[
  {
    "objectID": "chapters/fa/04_analisi_fattoriale_3.html",
    "href": "chapters/fa/04_analisi_fattoriale_3.html",
    "title": "22  Il modello multifattoriale",
    "section": "",
    "text": "22.1 Fattori ortogonali e teoria multifattoriale\nLa teoria dei due fattori di Spearman, secondo cui la prestazione in compiti cognitivi sarebbe spiegata da un fattore generale (g) comune a tutte le variabili e da un fattore specifico per ognuna di esse, ha influenzato gli studi sull’intelligenza per diversi anni. Tuttavia, con il passare del tempo, è emersa la necessità di spiegare in modo più articolato la covariazione tra più variabili osservabili. A questo scopo Thurstone (1945) propose la teoria multifattoriale, in base alla quale la covariazione tra le variabili manifeste (test, misure) non può essere riconducibile a un singolo fattore generale, ma deve essere spiegata dall’azione congiunta di diversi fattori comuni (ognuno relativo ad alcuni soltanto dei test).",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Il modello multifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/04_analisi_fattoriale_3.html#fattori-ortogonali-e-teoria-multifattoriale",
    "href": "chapters/fa/04_analisi_fattoriale_3.html#fattori-ortogonali-e-teoria-multifattoriale",
    "title": "22  Il modello multifattoriale",
    "section": "",
    "text": "22.1.1 Dal modello a due fattori al modello multifattoriale\nNel modello multifattoriale di Thurstone, si assume che ogni variabile manifesta \\(Y_i\\) dipenda da:\n\n\nUn insieme di \\(m\\) fattori comuni (\\(\\xi_1,\\dots,\\xi_m\\)), che spiegano la correlazione tra variabili diverse. Questi fattori sono detti comuni perché intervengono in più variabili manifeste.\n\nUn fattore specifico (o fattore unico, indicato con \\(\\delta_i\\)) proprio di ogni variabile manifesta. Questo fattore spiega la parte di varianza non condivisa con le altre variabili e viene spesso trattato come termine di errore o rumore statistico.\n\nIn presenza di \\(p\\) variabili manifeste \\(Y_1,\\dots,Y_p\\), l’ipotesi fondamentale è che vi siano molti meno fattori comuni (\\(m\\)) rispetto al numero di variabili (\\(p\\)), così da avere un modello parsimonioso: poche variabili latenti riescono a spiegare un gran numero di variabili osservate.\n\n22.1.2 Notazione\n\nLe variabili manifeste \\(Y\\) sono indicizzate da \\(i=1,\\dots,p\\).\nLe variabili latenti a fattore comune (\\(\\xi\\)) sono indicizzate da \\(j=1,\\dots,m\\).\nI fattori specifici (\\(\\delta\\)) sono indicizzati da \\(i=1,\\dots,p\\). Ciascun \\(\\delta_i\\) agisce soltanto su \\(Y_i\\).\nLe saturazioni fattoriali \\(\\lambda_{ij}\\) sono i parametri che quantificano l’importanza del fattore \\(\\xi_j\\) nella composizione della variabile osservabile \\(Y_i\\).\n\nInoltre, \\(\\mu_i\\) è la media della \\(i\\)-esima variabile manifesta \\(Y_i\\). Per semplicità, si assume che i fattori latenti abbiano media zero, il che permette di separare nettamente la componente sistematica (\\(\\xi_j\\)) dalla media \\(\\mu_i\\).\n\n22.1.3 Equazioni del modello multifattoriale\n\n\nCaso senza fattori comuni (solo fattori specifici):\n\\[\n\\begin{cases}\n  Y_{1k} = \\mu_1 + \\delta_{1k}, \\\\\n  \\vdots \\\\\n  Y_{ik} = \\mu_i + \\delta_{ik}, \\\\\n  \\vdots \\\\\n  Y_{pk} = \\mu_p + \\delta_{pk}.\n\\end{cases}\n\\]\nIn questo scenario, ogni variabile \\(Y_i\\) dipende solo dalla propria media \\(\\mu_i\\) e dal fattore specifico \\(\\delta_i\\). Poiché \\(\\delta_i\\) non è condiviso tra più variabili, le \\(Y_i\\) risulterebbero incorrelate.\n\n\nCaso con \\(m\\) fattori comuni (oltre ai fattori specifici):\n\\[\n\\begin{cases}\n  Y_1 - \\mu_1 &= \\lambda_{11}\\,\\xi_1 + \\dots + \\lambda_{1k}\\,\\xi_k + \\dots + \\lambda_{1m}\\,\\xi_m + \\delta_1, \\\\\n  \\vdots & \\\\\n  Y_i - \\mu_i &= \\lambda_{i1}\\,\\xi_1 + \\dots + \\lambda_{ik}\\,\\xi_k + \\dots + \\lambda_{im}\\,\\xi_m + \\delta_i, \\\\\n  \\vdots & \\\\\n  Y_p - \\mu_p &= \\lambda_{p1}\\,\\xi_1 + \\dots + \\lambda_{pk}\\,\\xi_k + \\dots + \\lambda_{pm}\\,\\xi_m + \\delta_p.\n\\end{cases}\n\\]\nOgni variabile manifesta \\(Y_i\\) viene dunque vista come combinazione lineare di tutti e soli i fattori comuni \\(\\xi_j\\) e di un fattore specifico \\(\\delta_i\\) che la riguarda esclusivamente.\n\n\nRiassumendo, nel modello multifattoriale:\n\n\n\\(\\xi_j\\) (con \\(j=1,\\dots,m\\)) è la \\(j\\)-esima variabile latente a fattore comune;\n\n\\(\\lambda_{ij}\\) è il peso fattoriale o saturazione che misura quanto il fattore \\(\\xi_j\\) contribuisce a definire la variabile osservabile \\(Y_i\\);\n\n\\(\\delta_i\\) è il fattore specifico, esclusivo della variabile \\(Y_i\\).\n\n22.1.4 Assunzioni del modello multifattoriale\nPer semplificare l’identificazione e l’interpretazione del modello, vengono poste alcune assunzioni chiave:\n\n\nMedia e varianza dei fattori comuni:\n\n\\(\\mathbb{E}(\\xi_j)=0\\) per ogni \\(j=1,\\dots,m\\).\n(Non avendo unità di misura proprie, si impone una media nulla per rendere il modello più semplice da stimare.)\n\\(\\mathbb{V}(\\xi_j)=1\\).\n(Analogamente, si normalizza la varianza dei fattori comuni a 1.)\n\n\nIncorrelazione tra i fattori comuni:\\[\n\\text{Cov}(\\xi_j, \\xi_h)=0 \\quad \\text{per} \\ j \\neq h.\n\\]\nSe i fattori sono incorrelati, si parla di fattori ortogonali. Questa è un’ipotesi tipica nelle prime formulazioni, ma può essere rilassata nei modelli a fattori obliqui, dove si ammette la possibilità che i fattori comuni siano correlati tra loro.\nIncorrelazione tra i fattori specifici:\\[\n\\text{Cov}(\\delta_i,\\delta_k)=0 \\quad \\text{per} \\ i \\neq k,\n\\]\ncon \\(\\mathbb{E}(\\delta_i) = 0\\) e \\(\\mathbb{V}(\\delta_i) = \\psi_{ii}\\). La quantità \\(\\psi_{ii}\\) è la varianza specifica (o unicità) di \\(Y_i\\).\nIncorrelazione tra fattori comuni e fattori specifici:\\[\n\\text{Cov}(\\xi_j, \\delta_i)=0\n\\quad \\text{per ogni} \\ i=1,\\dots,p \\ \\text{e} \\ j=1,\\dots,m.\n\\]\n\nIn sintesi, il modello multifattoriale di Thurstone propone che la relazione tra le variabili osservate sia spiegabile da un numero relativamente piccolo di fattori latenti ortogonali (per via dell’ipotesi di incorrelazione), ognuno dei quali influenza solo alcune delle variabili. Ciascuna variabile riceve inoltre un contributo unico da un fattore specifico. Grazie a queste ipotesi, si ottiene una struttura lineare parsimoniosa, utile sia in ambito di ricerca (per evidenziare eventuali dimensioni latenti comuni) sia in contesti applicativi (ad esempio nella costruzione e validazione di test psicologici).\n\nNota: Nella pratica, i fattori estratti con metodi come l’Analisi dei Fattori (Factor Analysis) possono anche essere obliqui (cioè correlati tra loro) qualora l’ipotesi di ortogonalità non sia realistica o non sia empiricamente supportata dai dati. L’ortogonalità non è dunque un dogma, bensì un’opzione che semplifica l’interpretazione ma che potrebbe non essere sempre adeguata a rappresentare la realtà psicologica sottostante.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Il modello multifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/04_analisi_fattoriale_3.html#interpretazione-dei-parametri-del-modello",
    "href": "chapters/fa/04_analisi_fattoriale_3.html#interpretazione-dei-parametri-del-modello",
    "title": "22  Il modello multifattoriale",
    "section": "\n22.2 Interpretazione dei parametri del modello",
    "text": "22.2 Interpretazione dei parametri del modello\n\n22.2.1 Covarianza tra variabili e fattori\nSupponiamo che le variabili manifeste \\(Y_i\\) abbiano media nulla \\(\\bigl(\\mathbb{E}(Y_i)=0\\bigr)\\). In questo caso, la covarianza tra una variabile \\(Y_i\\) e un fattore comune \\(\\xi_j\\) coincide esattamente con la corrispondente saturazione fattoriale \\(\\lambda_{ij}\\). Mostriamo questo risultato nel dettaglio:\n\\[\n\\begin{aligned}\n  \\text{Cov}(Y_i, \\xi_j) &= \\mathbb{E}(Y_i \\,\\xi_j)\\\\\n  &= \\mathbb{E}\\Bigl[\\bigl(\\lambda_{i1} \\,\\xi_1 + \\dots + \\lambda_{im} \\,\\xi_m + \\delta_i\\bigr)\\,\\xi_j \\Bigr]\\\\\n  &= \\lambda_{i1}\\,\\underbrace{\\mathbb{E}(\\xi_1\\,\\xi_j)}_{=0}\n     + \\dots\n     + \\lambda_{ij}\\,\\underbrace{\\mathbb{E}(\\xi_j^2)}_{=1}\n     + \\dots\n     + \\lambda_{im}\\,\\underbrace{\\mathbb{E}(\\xi_m\\,\\xi_j)}_{=0}\n     + \\underbrace{\\mathbb{E}(\\delta_i\\,\\xi_j)}_{=0} \\\\\n  &= \\lambda_{ij}.\n\\end{aligned}\n\\]\n\nLa prima e l’ultima eguaglianza seguono dall’ipotesi che i fattori comuni \\(\\xi_j\\) abbiano media zero e siano incorrelati tra loro (\\(\\text{Cov}(\\xi_j,\\xi_h) = 0\\) per \\(j \\neq h\\)) e con i fattori specifici \\(\\delta_i\\).\n\nInoltre, poiché \\(\\mathbb{V}(\\xi_j)=1\\), si ha \\(\\mathbb{E}(\\xi_j^2)=1\\).\n\nIn sintesi, le saturazioni fattoriali \\(\\lambda_{ij}\\) misurano la covarianza tra la variabile manifesta \\(Y_i\\) e il fattore latente \\(\\xi_j\\), a patto che \\(\\mathbb{E}(Y_i)=0\\).\n\n22.2.1.1 Saturazioni e correlazioni nel caso di variabili standardizzate\nSe le variabili \\(Y_i\\) sono ulteriormente standardizzate, ossia \\(\\mathbb{V}(Y_i)=1\\), allora la saturazione fattoriale \\(\\lambda_{ij}\\) diventa la correlazione tra \\(Y_i\\) e \\(\\xi_j\\). In tal caso, scriveremo\n\\[\nr_{ij} \\;=\\; \\lambda_{ij}.\n\\]\nCiò fornisce un’interpretazione ancora più immediata delle saturazioni fattoriali: in presenza di fattori latenti standardizzati, ciascuna \\(\\lambda_{ij}\\) riflette quanto il fattore \\(\\xi_j\\) è correlato con la variabile osservabile \\(Y_i\\).\n\n22.2.2 Espressione fattoriale della varianza\nNel modello multifattoriale, analogamente a quanto avviene nel modello monofattoriale, la varianza di ciascuna variabile manifesta \\(Y_i\\) può essere scomposta in due componenti:\n\nUna componente comune, detta comunalità, che riflette la porzione di varianza di \\(Y_i\\) spiegata dai fattori comuni.\nUna componente specifica, detta unicità, che rappresenta la porzione di varianza di \\(Y_i\\) non spiegata dai fattori comuni ed è attribuibile al fattore specifico \\(\\delta_i\\).\n\nSupponendo che \\(\\mathbb{E}(Y_i)=0\\) per ogni \\(i\\), la varianza della variabile \\(Y_i\\) è:\n\\[\n\\mathbb{V}(Y_i)\n= \\mathbb{E}\\Bigl[\\bigl(\\lambda_{i1}\\,\\xi_1 + \\dots + \\lambda_{im}\\,\\xi_m + \\delta_i\\bigr)^2\\Bigr].\n\\]\n\n22.2.2.1 Sviluppo del polinomio\nSviluppando il quadrato del termine tra parentesi, ricordiamo che il quadrato di una somma include:\n\nLa somma dei quadrati di tutti i termini.\nIl doppio prodotto di ogni termine con ciascuno degli altri termini successivi.\n\nIn formula:\n\\[\n(a + b + c)^2 \\;=\\; a^2 + b^2 + c^2 \\;+\\; 2ab + 2ac + 2bc.\n\\]\nApplicando lo stesso principio al nostro caso:\n\\[\n\\Bigl(\\lambda_{i1}\\,\\xi_1 + \\dots + \\lambda_{im}\\,\\xi_m + \\delta_i\\Bigr)^2\n= \\sum_{j=1}^m \\lambda_{ij}^2\\,\\xi_j^2 \\;+\\; \\delta_i^2\n  \\;+\\; \\text{(termini di doppio prodotto)}.\n\\]\n\n22.2.2.2 Valore atteso: contributo dei singoli fattori e delle loro interazioni\nPer calcolare \\(\\mathbb{V}(Y_i)\\), prendiamo il valore atteso di questo polinomio:\n\n\nTermini al quadrato dei fattori comuni:\nPoiché \\(\\mathbb{V}(\\xi_j) = 1\\) e \\(\\mathbb{E}(\\xi_j^2) = 1\\), il contributo di ciascun fattore comune \\(\\xi_j\\) è \\(\\lambda_{ij}^2\\).\n\nTermine al quadrato del fattore specifico:\\(\\mathbb{E}(\\delta_i^2) = \\psi_{ii}\\), dove \\(\\psi_{ii}\\) è la varianza specifica della variabile \\(Y_i\\).\n\nTermini di doppio prodotto:\nGrazie all’ipotesi di ortogonalità, la covarianza tra fattori comuni diversi è nulla (\\(\\mathbb{E}(\\xi_j\\,\\xi_h) = 0\\) per \\(j \\neq h\\)), e la covarianza tra fattori comuni e specifici è anch’essa nulla (\\(\\mathbb{E}(\\delta_i\\,\\xi_j) = 0\\)). Di conseguenza, tutti i doppi prodotti si annullano e non contribuiscono alla varianza totale.\n\n22.2.2.3 Risultato finale\nIn conclusione, la varianza di \\(Y_i\\) si ottiene sommando i contributi di tutti i fattori comuni e del fattore specifico:\n\\[\n\\mathbb{V}(Y_i)\n= \\lambda_{i1}^2 + \\lambda_{i2}^2 + \\dots + \\lambda_{im}^2 + \\psi_{ii}\n= \\sum_{j=1}^m \\lambda_{ij}^2 + \\psi_{ii}.\n\\]\n\n\nComunalità \\(\\,h_i^2 = \\sum_{j=1}^m \\lambda_{ij}^2\\): misura la quota di varianza di \\(Y_i\\) spiegata dai fattori comuni.\n\nUnicità \\(\\,\\psi_{ii}\\): rappresenta la varianza non spiegata dai fattori comuni, associata esclusivamente al fattore specifico \\(\\delta_i\\).\n\nIn sintesi, per ogni variabile \\(\\,Y_i\\), la somma di comunalità e unicità deve coincidere con la sua varianza totale, a conferma che l’analisi fattoriale scompone ciascuna variabile in una parte “comune” e una parte “specifica”.\n\n22.2.3 Espressione fattoriale della covarianza\nA titolo di esempio, consideriamo il caso di \\(p=5\\) variabili osservate e \\(m=2\\) fattori ortogonali. Assumiamo inoltre che le variabili manifeste siano state centrate (ossia abbiano media nulla), così da poter omettere i termini costanti. In queste condizioni, il modello multifattoriale si scrive:\n\\[\n\\begin{cases}\n  Y_1 = \\lambda_{11}\\,\\xi_1 + \\lambda_{12}\\,\\xi_2 + \\delta_1, \\\\\n  Y_2 = \\lambda_{21}\\,\\xi_1 + \\lambda_{22}\\,\\xi_2 + \\delta_2, \\\\\n  Y_3 = \\lambda_{31}\\,\\xi_1 + \\lambda_{32}\\,\\xi_2 + \\delta_3, \\\\\n  Y_4 = \\lambda_{41}\\,\\xi_1 + \\lambda_{42}\\,\\xi_2 + \\delta_4, \\\\\n  Y_5 = \\lambda_{51}\\,\\xi_1 + \\lambda_{52}\\,\\xi_2 + \\delta_5.\n\\end{cases}\n\\]\nRicordiamo che:\n\n\n\\(\\xi_1\\) e \\(\\xi_2\\) sono i due fattori comuni, con \\(\\mathbb{E}(\\xi_j) = 0\\), \\(\\mathbb{V}(\\xi_j)=1\\) e \\(\\mathrm{Cov}(\\xi_1,\\xi_2)=0\\).\n\n\\(\\delta_i\\) è il fattore specifico associato a \\(Y_i\\), con \\(\\mathbb{E}(\\delta_i)=0\\) e \\(\\mathrm{Cov}(\\delta_i,\\delta_k)=0\\) per \\(i \\neq k\\).\nI fattori comuni sono incorrelati con i fattori specifici (\\(\\mathrm{Cov}(\\xi_j,\\delta_i)=0\\)).\n\n\n22.2.3.1 Calcolo esplicito di una covarianza\nMostriamo, nello specifico, come si ottiene la covarianza \\(\\mathrm{Cov}(Y_1,Y_2)\\). Supponendo \\(\\mathbb{E}(Y_1)=\\mathbb{E}(Y_2)=0\\),\n\\[\n\\begin{aligned}\n  \\mathrm{Cov}(Y_1, Y_2)\n    &= \\mathbb{E}\\bigl(Y_1\\,Y_2\\bigr) \\\\\n    &= \\mathbb{E}\\Bigl[\n       (\\lambda_{11} \\,\\xi_1 + \\lambda_{12} \\,\\xi_2 + \\delta_1)\n       \\,(\\lambda_{21} \\,\\xi_1 + \\lambda_{22} \\,\\xi_2 + \\delta_2)\n       \\Bigr].\n\\end{aligned}\n\\]\nSviluppando il prodotto e facendo uso delle ipotesi di ortogonalità, si ottiene:\n\n\n\\(\\lambda_{11}\\,\\lambda_{21}\\,\\mathbb{E}(\\xi_1^2)\\). Qui, siccome \\(\\mathrm{Var}(\\xi_1)=1\\), abbiamo \\(\\mathbb{E}(\\xi_1^2)=1\\).\n\n\\(\\lambda_{11}\\,\\lambda_{22}\\,\\mathbb{E}(\\xi_1\\,\\xi_2)=0\\) perché \\(\\mathrm{Cov}(\\xi_1,\\xi_2)=0\\).\n\n\\(\\lambda_{11}\\,\\mathbb{E}(\\xi_1\\,\\delta_2)=0\\) perché i fattori comuni e i fattori specifici sono incorrelati.\n\n\\(\\lambda_{12}\\,\\lambda_{21}\\,\\mathbb{E}(\\xi_2\\,\\xi_1)=0\\) per la stessa ragione del punto 2.\n\n\\(\\lambda_{12}\\,\\lambda_{22}\\,\\mathbb{E}(\\xi_2^2)\\). Analogamente, \\(\\mathrm{Var}(\\xi_2)=1\\), quindi \\(\\mathbb{E}(\\xi_2^2)=1\\).\n\n\\(\\lambda_{12}\\,\\mathbb{E}(\\xi_2\\,\\delta_2)=0\\) (incorrelazione tra fattori).\n\n\\(\\lambda_{21}\\,\\mathbb{E}(\\xi_1\\,\\delta_1)=0\\) (stessa ragione).\n\n\\(\\lambda_{22}\\,\\mathbb{E}(\\xi_2\\,\\delta_1)=0\\) (stessa ragione).\n\n\\(\\mathbb{E}(\\delta_1\\,\\delta_2)=0\\) (i fattori specifici sono incorrelati tra loro).\n\nCon tutti i termini di doppio prodotto che si annullano, rimane solo la somma dei termini che includono \\(\\xi_1^2\\) e \\(\\xi_2^2\\):\n\\[\n\\mathrm{Cov}(Y_1, Y_2)\n= \\lambda_{11}\\,\\lambda_{21} \\,+\\, \\lambda_{12}\\,\\lambda_{22}.\n\\]\n\n22.2.3.2 Interpretazione\nIn generale, la covarianza tra due variabili manifeste \\(Y_\\ell\\) e \\(Y_m\\) in un modello multifattoriale con \\(m\\) fattori ortogonali si può interpretare come la somma dei prodotti tra le saturazioni nei fattori comuni condivisi:\n\\[\n\\mathrm{Cov}(Y_\\ell, Y_m)\n= \\sum_{j=1}^m \\lambda_{\\ell j}\\,\\lambda_{mj}.\n\\]\nQuesto vuol dire che due variabili \\(Y_\\ell\\) e \\(Y_m\\) risultano correlate soltanto nella misura in cui condividono (in senso letterale) gli stessi fattori comuni. Se in una determinata posizione \\(j\\) una variabile ha saturazione prossima a zero, il contributo di quel fattore alla covarianza tra le due variabili sarà molto basso (o nullo).\n\nQuesto esempio illustra in modo concreto come, nel modello multifattoriale, la covarianza osservata tra due variabili sia l’effetto cumulativo dei prodotti delle loro saturazioni nei fattori comuni. La struttura di queste saturazioni diventa quindi fondamentale per interpretare quali fattori latenti spiegano la correlazione tra le varie misure osservate.\n\nEsempio 22.1 Consideriamo, a titolo di esempio, i dati presentati da Brown (2015) relativi a un campione di 250 pazienti che hanno completato un programma di psicoterapia. Su ciascun soggetto sono state raccolte otto misure di personalità, corrispondenti ad altrettante scale:\n\n\nN1: anxiety\n\n\nN2: hostility\n\n\nN3: depression\n\n\nN4: self-consciousness\n\n\nE1: warmth\n\n\nE2: gregariousness\n\n\nE3: assertiveness\n\n\nE4: positive emotions\n\nLe prime quattro scale riflettono diverse sfaccettature del neuroticismo (N), mentre le ultime quattro sono riferite all’estroversione (E). Di seguito vengono mostrate le deviazioni standard di ciascuna scala, oltre alla matrice di correlazioni osservate (qui denominata psychot_cor_mat) e alla dimensione campionaria (\\(n=250\\)).\n\nvarnames &lt;- c(\"N1\", \"N2\", \"N3\", \"N4\", \"E1\", \"E2\", \"E3\", \"E4\")\nsds &lt;- '5.7  5.6  6.4  5.7  6.0  6.2  5.7  5.6'\n\ncors &lt;- '\n    1.000\n    0.767  1.000 \n    0.731  0.709  1.000 \n    0.778  0.738  0.762  1.000 \n    -0.351  -0.302  -0.356  -0.318  1.000 \n    -0.316  -0.280  -0.300  -0.267  0.675  1.000 \n    -0.296  -0.289  -0.297  -0.296  0.634  0.651  1.000 \n    -0.282  -0.254  -0.292  -0.245  0.534  0.593  0.566  1.000\n'\n\npsychot_cor_mat &lt;- getCov(cors, names = varnames)\nn_obs &lt;- 250\n\n\n22.2.4 Analisi fattoriale esplorativa\nApplichiamo un’analisi fattoriale esplorativa (EFA) con metodo di stima a massima verosimiglianza e ipotizziamo la presenza di due fattori comuni incorrelati. In R, il comando utilizzato è:\n\nfit_efa &lt;- factanal(\n  covmat = psychot_cor_mat,\n  factors = 2,\n  rotation = \"varimax\",\n  n.obs = n_obs\n)\n\nDalle saturazioni fattoriali (ottenute tramite fit_efa$loadings) emerge la presenza di due fattori:\n\nIl primo fattore satura principalmente sulle scale di neuroticismo (N1, N2, N3, N4).\n\nIl secondo fattore satura principalmente sulle scale di estroversione (E1, E2, E3, E4).\n\n22.2.5 Covarianze (o correlazioni) riprodotte dal modello\nUna volta stimati i fattori e le relative saturazioni \\(\\boldsymbol{\\Lambda}\\), possiamo confrontare la matrice di correlazioni osservate con quella riprodotta dal modello fattoriale.\n\n22.2.5.1 Esempio di correlazione riprodotta\nPer illustrare il concetto, consideriamo la correlazione riprodotta tra N1 (prima variabile) e N2 (seconda variabile), che secondo il modello a due fattori incorrelati si ottiene sommando i prodotti delle saturazioni sui singoli fattori:\n\\[\nr_{12}^{(\\text{modello})}\n= \\lambda_{11}\\,\\lambda_{21} + \\lambda_{12}\\,\\lambda_{22},\n\\]\ndove:\n\n\n\\(\\lambda_{1j}\\) è la saturazione della variabile \\(Y_1\\) sul fattore \\(j\\).\n\n\\(\\lambda_{2j}\\) è la saturazione della variabile \\(Y_2\\) sul fattore \\(j\\).\n\nNell’esempio in R, otteniamo le saturazioni fattoriali:\n\nlambda &lt;- fit_efa$loadings \nlambda\n#&gt; \n#&gt; Loadings:\n#&gt;    Factor1 Factor2\n#&gt; N1  0.854  -0.228 \n#&gt; N2  0.826  -0.194 \n#&gt; N3  0.811  -0.233 \n#&gt; N4  0.865  -0.186 \n#&gt; E1 -0.202   0.773 \n#&gt; E2 -0.139   0.829 \n#&gt; E3 -0.158   0.771 \n#&gt; E4 -0.147   0.684 \n#&gt; \n#&gt;                Factor1 Factor2\n#&gt; SS loadings      2.923   2.526\n#&gt; Proportion Var   0.365   0.316\n#&gt; Cumulative Var   0.365   0.681\n\nL’espressione\n\nlambda[1, 1] * lambda[2, 1] + lambda[1, 2] * lambda[2, 2]\n#&gt; [1] 0.7493\n\nrestituisce un valore molto simile alla correlazione empirica 0.767, confermando la bontà dell’adattamento del modello per questa coppia di variabili.\n\n22.2.5.2 Intera matrice di correlazioni riprodotte\nL’intera matrice di correlazioni riprodotte dal modello è data da:\n\\[\n\\mathbf{R}_{\\text{riprodotta}}\n=\n\\boldsymbol{\\Lambda}\\,\\boldsymbol{\\Lambda}^{\\mathsf{T}}\n+\n\\boldsymbol{\\Psi},\n\\]\ndove \\(\\boldsymbol{\\Lambda}\\) è la matrice delle saturazioni fattoriali (e righe corrispondono alle variabili, colonne ai fattori) e \\(\\boldsymbol{\\Psi}\\) è la matrice diagonale contenente le varianze specifiche (unicità) stimate. In R, possiamo calcolare questa matrice con:\n\nRr &lt;- lambda %*% t(lambda) + diag(fit_efa$uniq)\nRr |&gt; round(2)\n#&gt;       N1    N2    N3    N4    E1    E2    E3    E4\n#&gt; N1  1.00  0.75  0.75  0.78 -0.35 -0.31 -0.31 -0.28\n#&gt; N2  0.75  1.00  0.71  0.75 -0.32 -0.28 -0.28 -0.25\n#&gt; N3  0.75  0.71  1.00  0.74 -0.34 -0.31 -0.31 -0.28\n#&gt; N4  0.78  0.75  0.74  1.00 -0.32 -0.27 -0.28 -0.25\n#&gt; E1 -0.35 -0.32 -0.34 -0.32  1.00  0.67  0.63  0.56\n#&gt; E2 -0.31 -0.28 -0.31 -0.27  0.67  1.00  0.66  0.59\n#&gt; E3 -0.31 -0.28 -0.31 -0.28  0.63  0.66  1.00  0.55\n#&gt; E4 -0.28 -0.25 -0.28 -0.25  0.56  0.59  0.55  1.00\n\nConfrontando Rr con la matrice di correlazioni osservate (psychot_cor_mat), otteniamo l’errore di riproduzione, ovvero la differenza tra i valori osservati e quelli teorici previsti dal modello:\n\npsychot_cor_mat - Rr |&gt; round(3)\n#&gt;        N1     N2     N3     N4     E1     E2     E3     E4\n#&gt; N1  0.000  0.018 -0.014 -0.003 -0.003 -0.009  0.015 -0.001\n#&gt; N2  0.018  0.000 -0.006 -0.013  0.015 -0.004 -0.008  0.000\n#&gt; N3 -0.014 -0.006  0.000  0.017 -0.012  0.006  0.011 -0.013\n#&gt; N4 -0.003 -0.013  0.017  0.000  0.000  0.007 -0.016  0.009\n#&gt; E1 -0.003  0.015 -0.012  0.000  0.000  0.006  0.006 -0.024\n#&gt; E2 -0.009 -0.004  0.006  0.007  0.006  0.000 -0.010  0.006\n#&gt; E3  0.015 -0.008  0.011 -0.016  0.006 -0.010  0.000  0.016\n#&gt; E4 -0.001  0.000 -0.013  0.009 -0.024  0.006  0.016  0.000\n\nIdealmente, se il modello fattoriale si adatta bene ai dati, tale differenza risulterà piccola per tutte le coppie di variabili (osservate).\nIn sintesi, questo esempio dimostra come un modello con due fattori incorrelati sia in grado di spiegare le correlazioni tra le otto scale di personalità. Il primo fattore, caricato dalle scale di neuroticismo, e il secondo, caricato da quelle di estroversione, confermano così la struttura a due dimensioni attesa.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Il modello multifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/04_analisi_fattoriale_3.html#fattori-obliqui",
    "href": "chapters/fa/04_analisi_fattoriale_3.html#fattori-obliqui",
    "title": "22  Il modello multifattoriale",
    "section": "\n22.3 Fattori obliqui",
    "text": "22.3 Fattori obliqui\nNel modello fattoriale, può accadere che i fattori comuni non siano ortogonali, ma correlati tra loro. In tal caso si parla di fattori obliqui. Anche in questa situazione, è possibile esprimere:\n\nla covarianza teorica tra una variabile manifesta \\(Y_i\\) e un fattore comune \\(\\xi_j\\),\nla covarianza teorica tra due variabili manifeste,\nla comunalità di ciascuna variabile manifesta,\n\nma le formule diventano più complesse rispetto al caso ortogonale, perché occorre tener conto anche delle covarianze tra i fattori comuni.\n\n22.3.1 Covarianza teorica tra variabili manifeste e fattori comuni\nNel modello multifattoriale con \\(m\\) fattori comuni, ciascuna variabile manifesta è modellata come:\n\\[\nY_i = \\lambda_{i1} \\xi_1 + \\dots + \\lambda_{im} \\xi_m + \\delta_i .\n\\tag{22.1}\\]\nVogliamo calcolare la covarianza teorica tra la variabile \\(Y_i\\) e un fattore comune \\(\\xi_j\\). Sfruttando la linearità del valore atteso, abbiamo:\n\\[\n\\begin{aligned}\n\\mathrm{Cov}(Y_i, \\xi_j)\n&= \\mathbb{E}(Y_i \\cdot \\xi_j) \\\\\n&= \\mathbb{E}\\left[\n\\left(\\sum_{h=1}^{m} \\lambda_{ih} \\xi_h + \\delta_i \\right) \\xi_j\n\\right] \\\\\n&= \\sum_{h=1}^{m} \\lambda_{ih} \\cdot \\mathrm{Cov}(\\xi_h, \\xi_j)\n+ \\underbrace{\\mathrm{Cov}(\\delta_i, \\xi_j)}_{=0} \\\\\n&= \\sum_{h=1}^{m} \\lambda_{ih} \\cdot \\phi_{hj},\n\\end{aligned}\n\\]\ndove \\(\\phi_{hj} = \\mathrm{Cov}(\\xi_h, \\xi_j)\\) è l’elemento \\((h,j)\\) della matrice di covarianze tra i fattori comuni, denotata con \\(\\boldsymbol{\\Phi}\\).\nEsempio con 3 fattori comuni.\nLa covarianza tra \\(Y_1\\) e il primo fattore \\(\\xi_1\\) è:\n\\[\n\\mathrm{Cov}(Y_1, \\xi_1) = \\lambda_{11} + \\lambda_{12} \\cdot \\phi_{21} + \\lambda_{13} \\cdot \\phi_{31}.\n\\]\n\n22.3.2 Varianza teorica di una variabile manifesta\nPartendo dall’Equazione 22.1, vogliamo calcolare la varianza teorica di \\(Y_i\\). Sviluppiamo il quadrato:\n\\[\n\\mathrm{Var}(Y_i) = \\mathbb{E}\\left[Y_i^2\\right] = \\mathbb{E}\\left[\n\\left(\\sum_{j=1}^m \\lambda_{ij} \\xi_j + \\delta_i \\right)^2\n\\right].\n\\]\nSviluppando il quadrato e distribuendo il valore atteso:\n\\[\n\\mathrm{Var}(Y_i) =\n\\sum_{j=1}^m \\lambda_{ij}^2 +\n2 \\sum_{j&lt;k} \\lambda_{ij} \\lambda_{ik} \\cdot \\phi_{jk} +\n\\psi_{ii},\n\\]\ndove \\(\\psi_{ii} = \\mathrm{Var}(\\delta_i)\\) è l’unicità.\nEsempio con tre fattori. La varianza teorica di \\(Y_1\\) sarà:\n\\[\n\\begin{aligned}\n\\mathrm{Var}(Y_1) =\\; & \\lambda_{11}^2 + \\lambda_{12}^2 + \\lambda_{13}^2 + \\\\\n& 2 \\lambda_{11} \\lambda_{12} \\phi_{12} +\n  2 \\lambda_{11} \\lambda_{13} \\phi_{13} +\n  2 \\lambda_{12} \\lambda_{13} \\phi_{23} + \\\\\n& \\psi_{11}.\n\\end{aligned}\n\\]\n\n22.3.3 Covarianza teorica tra due variabili manifeste\nNel caso di due variabili \\(Y_1\\) e \\(Y_2\\) spiegate da due fattori obliqui \\(\\xi_1\\) e \\(\\xi_2\\), abbiamo:\n\\[\n\\begin{aligned}\n\\mathrm{Cov}(Y_1, Y_2) =\\; &\n\\lambda_{11} \\lambda_{21} +\n\\lambda_{12} \\lambda_{22} + \\\\\n& \\lambda_{11} \\lambda_{22} \\cdot \\phi_{12} +\n  \\lambda_{12} \\lambda_{21} \\cdot \\phi_{12}.\n\\end{aligned}\n\\]\nRaccogliendo \\(\\phi_{12}\\):\n\\[\n\\mathrm{Cov}(Y_1, Y_2) =\n\\lambda_{11} \\lambda_{21} +\n\\lambda_{12} \\lambda_{22} +\n\\phi_{12} (\\lambda_{11} \\lambda_{22} + \\lambda_{12} \\lambda_{21}).\n\\]\n\n22.3.4 Forma matriciale del modello\nNel caso generale, con \\(p\\) variabili e \\(m\\) fattori obliqui, la matrice di covarianze teoriche è:\n\\[\n\\boldsymbol{\\Sigma} =\n\\boldsymbol{\\Lambda} \\boldsymbol{\\Phi} \\boldsymbol{\\Lambda}^\\mathsf{T} +\n\\boldsymbol{\\Psi},\n\\]\ndove:\n\n\n\\(\\boldsymbol{\\Lambda}\\): matrice \\(p \\times m\\) delle saturazioni fattoriali;\n\n\\(\\boldsymbol{\\Phi}\\): matrice \\(m \\times m\\) di covarianze tra i fattori comuni (non più diagonale);\n\n\\(\\boldsymbol{\\Psi}\\): matrice diagonale \\(p \\times p\\) delle unicità.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Il modello multifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/04_analisi_fattoriale_3.html#applicazione-con-r-modello-con-fattori-obliqui",
    "href": "chapters/fa/04_analisi_fattoriale_3.html#applicazione-con-r-modello-con-fattori-obliqui",
    "title": "22  Il modello multifattoriale",
    "section": "\n22.4 Applicazione con R: modello con fattori obliqui",
    "text": "22.4 Applicazione con R: modello con fattori obliqui\nTorniamo ai dati di personalità esaminati in precedenza. Applichiamo ora un’analisi fattoriale con rotazione obliqua (oblimin), che consente ai due fattori di essere correlati:\n\nn_obs &lt;- 250\n\nefa_result &lt;- fa(\n  psychot_cor_mat,\n  nfactors = 2,\n  n.obs = n_obs,\n  rotate = \"oblimin\"\n)\n\nVisualizziamo la struttura del modello:\n\nfa.diagram(efa_result)\n\n\n\n\n\n\n\n\n22.4.1 Saturazioni fattoriali e parametri del modello\n\nlambda &lt;- matrix(efa_result$loadings[, 1:2], nrow = 8, ncol = 2)\n\nrownames(lambda) &lt;- c(\"N1\", \"N2\", \"N3\", \"N4\", \"E1\", \"E2\", \"E3\", \"E4\")\ncolnames(lambda) &lt;- c(\"Factor1\", \"Factor2\")\nlambda\n#&gt;      Factor1  Factor2\n#&gt; N1  0.877076 -0.01578\n#&gt; N2  0.852281  0.01128\n#&gt; N3  0.826584 -0.03685\n#&gt; N4  0.898763  0.03121\n#&gt; E1 -0.048589  0.77187\n#&gt; E2  0.034700  0.85566\n#&gt; E3  0.002815  0.79292\n#&gt; E4 -0.007885  0.69545\n\nMatrice di intercorrelazioni tra i fattori:\n\nPhi &lt;- efa_result$Phi\nPhi\n#&gt;         MR1     MR2\n#&gt; MR1  1.0000 -0.4314\n#&gt; MR2 -0.4314  1.0000\n\nUnicità:\n\nPsi &lt;- diag(efa_result$uniquenesses)\nround(Psi, 2)\n#&gt;      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n#&gt; [1,] 0.22 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n#&gt; [2,] 0.00 0.28 0.00 0.00 0.00 0.00 0.00 0.00\n#&gt; [3,] 0.00 0.00 0.29 0.00 0.00 0.00 0.00 0.00\n#&gt; [4,] 0.00 0.00 0.00 0.22 0.00 0.00 0.00 0.00\n#&gt; [5,] 0.00 0.00 0.00 0.00 0.37 0.00 0.00 0.00\n#&gt; [6,] 0.00 0.00 0.00 0.00 0.00 0.29 0.00 0.00\n#&gt; [7,] 0.00 0.00 0.00 0.00 0.00 0.00 0.37 0.00\n#&gt; [8,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.51\n\n\n22.4.2 Matrice delle correlazioni riprodotte\nCostruiamo la matrice di correlazioni riprodotte dal modello obliquo:\n\nR_hat &lt;- lambda %*% Phi %*% t(lambda) + Psi\nround(R_hat, 2)\n#&gt;       N1    N2    N3    N4    E1    E2    E3    E4\n#&gt; N1  1.00  0.75  0.75  0.78 -0.35 -0.31 -0.31 -0.28\n#&gt; N2  0.75  1.00  0.71  0.75 -0.32 -0.28 -0.28 -0.25\n#&gt; N3  0.75  0.71  1.00  0.74 -0.34 -0.31 -0.31 -0.28\n#&gt; N4  0.78  0.75  0.74  1.00 -0.32 -0.27 -0.28 -0.25\n#&gt; E1 -0.35 -0.32 -0.34 -0.32  1.00  0.67  0.63  0.55\n#&gt; E2 -0.31 -0.28 -0.31 -0.27  0.67  1.00  0.67  0.59\n#&gt; E3 -0.31 -0.28 -0.31 -0.28  0.63  0.67  1.00  0.55\n#&gt; E4 -0.28 -0.25 -0.28 -0.25  0.55  0.59  0.55  1.00\n\nDifferenza con la matrice osservata:\n\nround(psychot_cor_mat - R_hat, 2)\n#&gt;       N1    N2    N3    N4    E1    E2    E3    E4\n#&gt; N1  0.00  0.02 -0.01  0.00  0.00 -0.01  0.01  0.00\n#&gt; N2  0.02  0.00  0.00 -0.01  0.01  0.00 -0.01  0.00\n#&gt; N3 -0.01  0.00  0.00  0.02 -0.01  0.01  0.01 -0.01\n#&gt; N4  0.00 -0.01  0.02  0.00  0.00  0.01 -0.02  0.01\n#&gt; E1  0.00  0.01 -0.01  0.00  0.00  0.01  0.01 -0.02\n#&gt; E2 -0.01  0.00  0.01  0.01  0.01  0.00 -0.01  0.01\n#&gt; E3  0.01 -0.01  0.01 -0.02  0.01 -0.01  0.00  0.01\n#&gt; E4  0.00  0.00 -0.01  0.01 -0.02  0.01  0.01  0.00\n\n\n22.4.3 Correlazione riprodotta tra due variabili\nPer esempio, la correlazione tra N1 e N2 predetta dal modello è:\n\nlambda[1,1] * lambda[2,1] +\nlambda[1,2] * lambda[2,2] +\nlambda[1,1] * lambda[2,2] * Phi[1,2] +\nlambda[1,2] * lambda[2,1] * Phi[1,2]\n#&gt; [1] 0.7489\n\nQuesto valore dovrebbe essere molto vicino al valore osservato:\n\npsychot_cor_mat[1, 2]\n#&gt; [1] 0.767\n\nIn sintesi, nel modello a fattori obliqui, la struttura delle correlazioni tra variabili manifeste dipende non solo dalle saturazioni, ma anche dalla correlazione tra i fattori comuni. Questo tipo di modello è più flessibile e spesso più realistico in psicologia, dove i costrutti latenti tendono a essere interdipendenti.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Il modello multifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/04_analisi_fattoriale_3.html#riflessioni-conclusive",
    "href": "chapters/fa/04_analisi_fattoriale_3.html#riflessioni-conclusive",
    "title": "22  Il modello multifattoriale",
    "section": "\n22.5 Riflessioni Conclusive",
    "text": "22.5 Riflessioni Conclusive\nIn questo capitolo abbiamo esaminato il modello di analisi fattoriale comune, distinguendo tra l’ipotesi di fattori ortogonali e quella più generale di fattori obliqui. Abbiamo visto che, mentre i modelli ortogonali permettono una formulazione più semplice e interpretazioni più immediate, i modelli obliqui risultano più flessibili e realistici, poiché ammettono correlazioni tra i fattori latenti. Abbiamo inoltre analizzato come le covarianze tra le variabili osservate possano essere espresse in funzione delle saturazioni fattoriali, delle intercorrelazioni tra i fattori e delle unicità specifiche. Infine, abbiamo illustrato come queste relazioni teoriche si traducono concretamente nell’applicazione empirica dell’analisi fattoriale esplorativa, mediante l’uso del software R. Comprendere la struttura fattoriale sottostante a un insieme di variabili psicologiche osservate consente di sintetizzare l’informazione in modo più parsimonioso, rivelando le dimensioni latenti che organizzano il comportamento osservato.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Il modello multifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/04_analisi_fattoriale_3.html#session-info",
    "href": "chapters/fa/04_analisi_fattoriale_3.html#session-info",
    "title": "22  Il modello multifattoriale",
    "section": "\n22.6 Session Info",
    "text": "22.6 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.2\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] kableExtra_1.4.0  tidySEM_0.2.7     OpenMx_2.21.13    corrplot_0.95    \n#&gt;  [5] ggokabeito_0.1.0  see_0.11.0        MASS_7.3-65       viridis_0.6.5    \n#&gt;  [9] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n#&gt; [13] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n#&gt; [17] lavaan_0.6-19     psych_2.4.12      scales_1.3.0      markdown_1.13    \n#&gt; [21] knitr_1.50        lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n#&gt; [25] dplyr_1.1.4       purrr_1.0.4       readr_2.1.5       tidyr_1.3.1      \n#&gt; [29] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] splines_4.4.2         later_1.4.1           XML_3.99-0.18        \n#&gt;   [4] rpart_4.1.24          fastDummies_1.7.5     lifecycle_1.0.4      \n#&gt;   [7] Rdpack_2.6.3          rstatix_0.7.2         rprojroot_2.0.4      \n#&gt;  [10] StanHeaders_2.32.10   globals_0.16.3        lattice_0.22-6       \n#&gt;  [13] rockchalk_1.8.157     backports_1.5.0       magrittr_2.0.3       \n#&gt;  [16] openxlsx_4.2.8        Hmisc_5.2-3           rmarkdown_2.29       \n#&gt;  [19] httpuv_1.6.15         tmvnsim_1.0-2         qgraph_1.9.8         \n#&gt;  [22] zip_2.3.2             pkgbuild_1.4.6        pbapply_1.7-2        \n#&gt;  [25] minqa_1.2.8           multcomp_1.4-28       abind_1.4-8          \n#&gt;  [28] quadprog_1.5-8        nnet_7.3-20           TH.data_1.1-3        \n#&gt;  [31] sandwich_3.1-1        inline_0.3.21         listenv_0.9.1        \n#&gt;  [34] arm_1.14-4            proto_1.0.0           parallelly_1.42.0    \n#&gt;  [37] texreg_1.39.4         svglite_2.1.3         codetools_0.2-20     \n#&gt;  [40] xml2_1.3.8            tidyselect_1.2.1      farver_2.1.2         \n#&gt;  [43] lme4_1.1-36           matrixStats_1.5.0     stats4_4.4.2         \n#&gt;  [46] base64enc_0.1-3       jsonlite_1.9.1        progressr_0.15.1     \n#&gt;  [49] Formula_1.2-5         survival_3.8-3        emmeans_1.10.7       \n#&gt;  [52] systemfonts_1.2.1     dbscan_1.2.2          tools_4.4.2          \n#&gt;  [55] Rcpp_1.0.14           glue_1.8.0            mnormt_2.1.1         \n#&gt;  [58] xfun_0.51             MplusAutomation_1.1.1 loo_2.8.0            \n#&gt;  [61] withr_3.0.2           fastmap_1.2.0         boot_1.3-31          \n#&gt;  [64] digest_0.6.37         mi_1.1                timechange_0.3.0     \n#&gt;  [67] R6_2.6.1              mime_0.13             estimability_1.5.1   \n#&gt;  [70] colorspace_2.1-1      gtools_3.9.5          jpeg_0.1-10          \n#&gt;  [73] generics_0.1.3        data.table_1.17.0     corpcor_1.6.10       \n#&gt;  [76] httr_1.4.7            htmlwidgets_1.6.4     pkgconfig_2.0.3      \n#&gt;  [79] sem_3.1-16            gtable_0.3.6          bain_0.2.11          \n#&gt;  [82] htmltools_0.5.8.1     carData_3.0-5         blavaan_0.5-8        \n#&gt;  [85] png_0.1-8             reformulas_0.4.0      rstudioapi_0.17.1    \n#&gt;  [88] tzdb_0.5.0            reshape2_1.4.4        curl_6.2.1           \n#&gt;  [91] coda_0.19-4.1         checkmate_2.3.2       nlme_3.1-167         \n#&gt;  [94] nloptr_2.2.1          zoo_1.8-13            parallel_4.4.2       \n#&gt;  [97] miniUI_0.1.1.1        nonnest2_0.5-8        foreign_0.8-88       \n#&gt; [100] pillar_1.10.1         grid_4.4.2            vctrs_0.6.5          \n#&gt; [103] RANN_2.6.2            promises_1.3.2        car_3.1-3            \n#&gt; [106] xtable_1.8-4          cluster_2.1.8.1       GPArotation_2024.3-1 \n#&gt; [109] htmlTable_2.4.3       evaluate_1.0.3        pbivnorm_0.6.0       \n#&gt; [112] gsubfn_0.7            mvtnorm_1.3-3         cli_3.6.4            \n#&gt; [115] kutils_1.73           compiler_4.4.2        rlang_1.1.5          \n#&gt; [118] rstantools_2.4.0      future.apply_1.11.3   ggsignif_0.6.4       \n#&gt; [121] fdrtool_1.2.18        plyr_1.8.9            stringi_1.8.4        \n#&gt; [124] rstan_2.32.7          pander_0.6.6          QuickJSR_1.6.0       \n#&gt; [127] munsell_0.5.1         lisrelToR_0.3         CompQuadForm_1.4.3   \n#&gt; [130] V8_6.0.2              pacman_0.5.1          Matrix_1.7-3         \n#&gt; [133] hms_1.1.3             glasso_1.11           future_1.34.0        \n#&gt; [136] shiny_1.10.0          rbibutils_2.3         igraph_2.1.4         \n#&gt; [139] broom_1.0.7           RcppParallel_5.1.10\n\n\n\n\n\nBrown, T. A. (2015). Confirmatory factor analysis for applied research. Guilford publications.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Il modello multifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/05_factor_scores.html",
    "href": "chapters/fa/05_factor_scores.html",
    "title": "23  I punteggi fattoriali",
    "section": "",
    "text": "23.1 Introduzione\nUno dei passaggi più complessi e delicati nello sviluppo di un test psicometrico è rappresentato dall’interpretazione dei fattori. Mentre la verifica dell’affidabilità consente di stimare la precisione delle misure ottenute (ossia, quanto sono stabili e coerenti), essa non fornisce informazioni sul contenuto o sulla natura psicologica delle dimensioni che il test intende misurare.\nL’interpretazione dei fattori è un processo eminentemente concettuale e teorico, che non è guidato da regole univoche. Non esistono criteri oggettivi e definitivi per “leggere” i fattori: molto dipende dalla competenza del ricercatore, dalla sua conoscenza del dominio teorico, e dalla sua capacità di individuare un significato comune tra le variabili che saturano su ciascun fattore. È fondamentale, tuttavia, che tale interpretazione sia ancorata ai dati e non frutto di libere associazioni o intuizioni arbitrarie.\nVa inoltre ricordato che le scelte effettuate durante l’analisi fattoriale — ad esempio, il metodo di estrazione, il numero di fattori da mantenere, il tipo di rotazione (ortogonale o obliqua) — influenzano profondamente la soluzione ottenuta e, di conseguenza, anche la sua interpretazione. Questo implica che l’intero processo sia in parte soggettivo e debba essere condotto con cautela.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>I punteggi fattoriali</span>"
    ]
  },
  {
    "objectID": "chapters/fa/05_factor_scores.html#punteggi-fattoriali",
    "href": "chapters/fa/05_factor_scores.html#punteggi-fattoriali",
    "title": "23  I punteggi fattoriali",
    "section": "\n23.3 Punteggi fattoriali",
    "text": "23.3 Punteggi fattoriali\nFinora ci siamo concentrati sulla costruzione del modello fattoriale attraverso la stima delle saturazioni fattoriali e delle comunalità. Questi passaggi rappresentano la base per comprendere la struttura latente che organizza le variabili osservate.\nTuttavia, è possibile compiere un passo ulteriore: la stima dei punteggi fattoriali (factor scores). I punteggi fattoriali rappresentano una stima dei valori dei fattori latenti per ciascun partecipante nel campione. Sono utili sia per un’interpretazione individualizzata dell’analisi fattoriale, sia per ulteriori analisi statistiche (es. regressioni, classificazioni o confronti di gruppo).\n\n23.3.1 Metodi di stima\nEsistono diversi metodi per calcolare i punteggi fattoriali. I più comuni sono:\n\n\nMetodo di Thomson (regressione): stima i punteggi come valori previsti in un modello di regressione lineare multipla, basandosi sulla matrice delle correlazioni tra le variabili osservate e sulla matrice delle saturazioni fattoriali.\n\n\nMetodo di Bartlett: si basa su un approccio di massima verosimiglianza e produce stime meno correlate tra i fattori ma più influenzate dall’assunzione di assenza di errore.\n\nEntrambi i metodi sono implementati nella funzione factanal() di R. Il metodo di regressione si attiva specificando scores = \"regression\".",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>I punteggi fattoriali</span>"
    ]
  },
  {
    "objectID": "chapters/fa/05_factor_scores.html#session-info",
    "href": "chapters/fa/05_factor_scores.html#session-info",
    "title": "23  I punteggi fattoriali",
    "section": "\n23.7 Session Info",
    "text": "23.7 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.2\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] kableExtra_1.4.0  corrplot_0.95     ggokabeito_0.1.0  see_0.11.0       \n#&gt;  [5] MASS_7.3-65       viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n#&gt;  [9] ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1 \n#&gt; [13] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-19     psych_2.4.12     \n#&gt; [17] scales_1.3.0      markdown_1.13     knitr_1.50        lubridate_1.9.4  \n#&gt; [21] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.4      \n#&gt; [25] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1    \n#&gt; [29] tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.1      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.2.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] mime_0.13           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [25] Matrix_1.7-3        R6_2.6.1            fastmap_1.2.0      \n#&gt;  [28] rbibutils_2.3       shiny_1.10.0        digest_0.6.37      \n#&gt;  [31] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n#&gt;  [34] rprojroot_2.0.4     Hmisc_5.2-3         labeling_0.4.3     \n#&gt;  [37] timechange_0.3.0    abind_1.4-8         compiler_4.4.2     \n#&gt;  [40] withr_3.0.2         glasso_1.11         htmlTable_2.4.3    \n#&gt;  [43] backports_1.5.0     carData_3.0-5       ggsignif_0.6.4     \n#&gt;  [46] corpcor_1.6.10      gtools_3.9.5        tools_4.4.2        \n#&gt;  [49] pbivnorm_0.6.0      foreign_0.8-88      zip_2.3.2          \n#&gt;  [52] httpuv_1.6.15       nnet_7.3-20         glue_1.8.0         \n#&gt;  [55] quadprog_1.5-8      nlme_3.1-167        promises_1.3.2     \n#&gt;  [58] lisrelToR_0.3       grid_4.4.2          checkmate_2.3.2    \n#&gt;  [61] cluster_2.1.8.1     reshape2_1.4.4      generics_0.1.3     \n#&gt;  [64] gtable_0.3.6        tzdb_0.5.0          data.table_1.17.0  \n#&gt;  [67] hms_1.1.3           xml2_1.3.8          car_3.1-3          \n#&gt;  [70] sem_3.1-16          pillar_1.10.1       rockchalk_1.8.157  \n#&gt;  [73] later_1.4.1         splines_4.4.2       lattice_0.22-6     \n#&gt;  [76] survival_3.8-3      kutils_1.73         tidyselect_1.2.1   \n#&gt;  [79] miniUI_0.1.1.1      pbapply_1.7-2       reformulas_0.4.0   \n#&gt;  [82] svglite_2.1.3       stats4_4.4.2        xfun_0.51          \n#&gt;  [85] qgraph_1.9.8        arm_1.14-4          stringi_1.8.4      \n#&gt;  [88] yaml_2.3.10         pacman_0.5.1        boot_1.3-31        \n#&gt;  [91] evaluate_1.0.3      codetools_0.2-20    mi_1.1             \n#&gt;  [94] cli_3.6.4           RcppParallel_5.1.10 rpart_4.1.24       \n#&gt;  [97] systemfonts_1.2.1   xtable_1.8-4        Rdpack_2.6.3       \n#&gt; [100] munsell_0.5.1       Rcpp_1.0.14         coda_0.19-4.1      \n#&gt; [103] png_0.1-8           XML_3.99-0.18       parallel_4.4.2     \n#&gt; [106] jpeg_0.1-10         lme4_1.1-36         mvtnorm_1.3-3      \n#&gt; [109] openxlsx_4.2.8      rlang_1.1.5         multcomp_1.4-28    \n#&gt; [112] mnormt_2.1.1\n\n\n\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>I punteggi fattoriali</span>"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html",
    "href": "chapters/fa/06_constraints_on_parms.html",
    "title": "24  Attendibilità e modello fattoriale",
    "section": "",
    "text": "24.1 Introduzione\nIn questo capitolo affronteremo il tema della valutazione dell’affidabilità di uno strumento psicometrico tramite l’analisi fattoriale. Verranno introdotti tre modelli teorici che descrivono le diverse relazioni possibili tra gli indicatori osservati e un fattore latente comune: il modello congenerico, il modello tau-equivalente e il modello parallelo.\nPer ciascuno di questi modelli sarà discusso un indice specifico volto a quantificare l’affidabilità intesa come coerenza interna. Gli indici presentati saranno:\nSi mostrerà come l’impiego dell’indice alpha di Cronbach sia giustificato solo in presenza di specifiche condizioni, le quali risultano però raramente soddisfatte nei dati empirici. Per tale motivo, nella pratica applicativa, è spesso preferibile fare riferimento all’indice omega di McDonald, che fornisce una stima più accurata della coerenza interna.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html#teoria-classica-dei-test-e-analisi-fattoriale",
    "href": "chapters/fa/06_constraints_on_parms.html#teoria-classica-dei-test-e-analisi-fattoriale",
    "title": "24  Attendibilità e modello fattoriale",
    "section": "\n24.2 Teoria classica dei test e analisi fattoriale",
    "text": "24.2 Teoria classica dei test e analisi fattoriale\nCome illustrato da McDonald (2013), la teoria classica dei test (CTT) può essere messa in relazione diretta con il modello dell’analisi fattoriale confermativa. La figura seguente rappresenta, in termini fattoriali, il legame tra i punteggi osservati \\(Y\\) ottenuti da un test composto da cinque item e i corrispondenti punteggi veri.\n\n\n\n\n\nFigura 24.1: Diagramma di percorso del modello monofattoriale.\n\n\nQuando si somministra un unico test, esistono diverse strategie per stimarne l’affidabilità. In questo contesto, ci concentreremo su tre approcci implementabili tramite l’analisi fattoriale:\n\nil coefficiente \\(\\alpha\\) di Cronbach,\n\nil coefficiente \\(\\omega\\) di McDonald,\n\nl’indice \\(\\rho\\) derivato dalla formula di Spearman-Brown.\n\nTra questi, il coefficiente \\(\\alpha\\) di Cronbach è l’indice più diffuso per la stima dell’affidabilità come coerenza interna o omogeneità tra gli item. Tuttavia, approfondiremo come esso rappresenti una stima al ribasso dell’affidabilità del test, valida solo a patto che siano rispettate alcune ipotesi. In caso contrario, l’\\(\\alpha\\) può risultare uno stimatore distorto.\nPrima di analizzare nel dettaglio le tre metodologie, è necessario distinguere tra tre configurazioni possibili del modello unifattoriale, corrispondenti a:\n\nmodello con indicatori congenerici,\n\nmodello \\(\\tau\\)-equivalente,\n\nmodello parallelo.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html#modello-fattoriale-e-ctt",
    "href": "chapters/fa/06_constraints_on_parms.html#modello-fattoriale-e-ctt",
    "title": "32  Attendibilità e modello fattoriale",
    "section": "\n32.2 Modello fattoriale e CTT",
    "text": "32.2 Modello fattoriale e CTT\nConsiderando un insieme di item osservati \\(X_1, X_2, \\dots, X_p\\), con \\(p&gt;2\\), i punteggi ottenuti da questi item sono composti da due elementi distinti: una componente di punteggio vero e una componente di errore.\n\\[\n\\begin{equation}\n\\begin{aligned}\nX_1 &=T_1+E_1,\\notag\\\\\nX_2 &=T_2+E_2,\\notag\\\\\n&\\dots\\notag\\\\\nX_p &=T_p+E_p.\\notag\n\\end{aligned}\n\\end{equation}\n\\]\nIn linea con l’approccio delineato da McDonald (2013), questa decomposizione tra la componente vera e quella di errore può essere formalizzata mediante l’utilizzo dei parametri del modello fattoriale. L’equazione \\(X_i = T_i + E_i\\) può quindi essere riformulata come segue:\n\\[\nX_i = \\lambda_i \\xi + \\delta_i, \\quad{i=1, \\dots, p},\n\\]\nIn questa equazione, \\(X_i\\) rappresenta il punteggio osservato per l’item \\(i\\)-esimo (espresso in termini di scarti dalla media), \\(\\lambda_i\\) è il carico fattoriale associato all’item \\(i\\)-esimo, \\(\\xi\\) costituisce il fattore comune e \\(\\delta_i\\) è la componente residuale del punteggio osservato per l’item \\(i\\)-esimo. Tale formulazione si basa sulle assunzioni del modello monofattoriale. Nello specifico, si ipotizza che \\(\\xi\\) e \\(\\delta_i\\) siano incorrelati per ogni item \\(i\\), e che \\(\\delta_i\\) e \\(\\delta_k\\) siano incorrelati per ogni coppia \\(i \\neq k\\).",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html#classi-di-modelli",
    "href": "chapters/fa/06_constraints_on_parms.html#classi-di-modelli",
    "title": "24  Attendibilità e modello fattoriale",
    "section": "\n24.4 Classi di modelli",
    "text": "24.4 Classi di modelli\nNel contesto dei modelli monofattoriali, è possibile distinguere tre principali configurazioni teoriche, ciascuna caratterizzata da un diverso insieme di assunzioni riguardanti la relazione tra gli indicatori osservati e il fattore latente comune. Queste tre configurazioni si collocano lungo un continuum che va da una maggiore flessibilità a una maggiore restrizione strutturale.\n\n24.4.1 Modello congenerico\nIl modello con indicatori congenerici rappresenta la formulazione più generale e flessibile. In questo modello:\n\nogni indicatore è influenzato dal medesimo fattore latente,\n\ni carichi fattoriali (\\(\\lambda_i\\)) possono variare da un indicatore all’altro,\n\nanche le varianze degli errori (\\(\\delta_i\\)) sono libere di differire.\n\nIn altre parole, ciascun indicatore misura lo stesso costrutto latente, ma può farlo con una diversa intensità e con un diverso grado di errore. Questo modello riflette realisticamente la maggior parte delle situazioni empiriche e costituisce il punto di partenza per valutazioni più complesse.\n\n24.4.2 Modello tau-equivalente\nIl modello \\(\\tau\\)-equivalente è una restrizione del modello congenerico in cui si assume che:\n\n\ntutti gli indicatori abbiano lo stesso carico fattoriale (\\(\\lambda_i = \\lambda\\) per ogni \\(i\\)),\nle varianze degli errori possono comunque differire tra gli indicatori.\n\nQuesto implica che ogni indicatore contribuisce in egual misura alla misurazione del fattore latente, pur potendo avere un diverso grado di specificità residua. Il coefficiente \\(\\alpha\\) di Cronbach si basa proprio su questo modello e fornisce una stima attendibile solo se tale assunzione è soddisfatta.\n\n24.4.3 Modello parallelo\nIl modello con indicatori paralleli rappresenta il caso più restrittivo. In esso si assume che:\n\ntutti gli indicatori abbiano lo stesso carico fattoriale (\\(\\lambda_i = \\lambda\\)),\ne che le varianze degli errori siano identiche tra gli indicatori (\\(\\text{Var}(\\delta_i) = \\sigma^2\\) per ogni \\(i\\)).\n\nIn questa configurazione, gli indicatori sono considerati completamente equivalenti sia nella misura del fattore latente che nella quantità di errore associato. È il modello sottostante alla formula di Spearman-Brown, utilizzata, ad esempio, per prevedere l’affidabilità in funzione della lunghezza del test.\n\n24.4.4 Confronto tra i modelli\nI tre modelli possono essere letti come una gerarchia di assunzioni:\n\n\n\n\n\n\n\n\nModello\nCarichi fattoriali\nVarianze degli errori\nGrado di restrizione\n\n\n\nCongenerico\nLiberi\nLibere\nBasso\n\n\nTau-equivalente\nUguali\nLibere\nMedio\n\n\nParallelo\nUguali\nUguali\nAlto\n\n\n\n \nAll’aumentare dei vincoli imposti al modello, cresce la semplicità e la forza esplicativa teorica, ma diminuisce la flessibilità rispetto ai dati reali. È quindi essenziale scegliere il modello coerente con la struttura empirica degli item, poiché ogni indice di affidabilità presuppone uno di questi modelli:\n\n\nOmega di McDonald è coerente con il modello congenerico,\n\n\nAlpha di Cronbach assume il modello \\(\\tau\\)-equivalente,\n\n\nRho di Spearman-Brown richiede il modello parallelo.\n\nNelle sezioni successive esamineremo in dettaglio ciascuno di questi indici e illustreremo come stimarli e interpretarli correttamente in funzione della struttura del modello.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html#metodo-dei-minimi-quadrati-non-pesati",
    "href": "chapters/fa/06_constraints_on_parms.html#metodo-dei-minimi-quadrati-non-pesati",
    "title": "24  Attendibilità e modello fattoriale",
    "section": "\n24.6 Metodo dei minimi quadrati non pesati",
    "text": "24.6 Metodo dei minimi quadrati non pesati\nNel contesto del modello unifattoriale, la varianza osservata di ciascun indicatore può essere scomposta in due componenti principali:\n\nla varianza spiegata dal fattore latente comune, denotata con \\(\\sigma^2_T\\);\nla varianza residua o specifica, indicata con \\(\\psi\\).\n\nCome illustrato da McDonald (2013), è possibile stimare queste due componenti direttamente a partire dalla matrice di covarianza empirica degli item. Tali stime sono poi impiegate per calcolare indici di affidabilità interna come \\(\\alpha\\) di Cronbach e \\(\\omega\\) di McDonald.\nIn precedenza, abbiamo visto che la varianza del punteggio vero può essere interpretata come la covarianza tra due forme parallele dello stesso test:\n\\[\n\\sigma_T^2 = \\sigma_{XX'}.\n\\]\nNel caso specifico del modello \\(\\tau\\)-equivalente, la matrice teorica delle varianze e covarianze degli item assume la forma:\n\\[\n\\boldsymbol{\\Sigma} = \\begin{bmatrix}\n\\sigma_T^2 + \\psi_{11} & \\sigma_T^2 & \\dots & \\sigma_T^2 \\\\\n\\sigma_T^2 & \\sigma_T^2 + \\psi_{22} & \\dots & \\sigma_T^2 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_T^2 & \\sigma_T^2 & \\dots & \\sigma_T^2 + \\psi_{pp}\n\\end{bmatrix},\n\\]\ndove tutte le covarianze fuori diagonale sono uguali a \\(\\sigma_T^2\\), mentre le varianze diagonali variano in funzione delle specificità degli item.\n\n24.6.1 Stima della varianza del punteggio vero\nNel modello \\(\\tau\\)-equivalente, una stima della varianza del fattore comune, \\(\\hat{\\sigma}_T^2\\), può essere ottenuta calcolando la media delle covarianze osservate tra gli item, ovvero i valori fuori diagonale della matrice empirica \\(\\mathbf{S}\\):\n\\[\n\\hat{\\sigma}_T^2 = \\frac{1}{p(p-1)} \\sum_{i \\neq k} s_{ik}.\n\\tag{24.1}\\]\nQuesta stima è nota come metodo dei minimi quadrati non pesati (unweighted least squares), in quanto si basa su una media semplice delle covarianze, senza introdurre pesi differenziati tra item.\n\n24.6.2 Stima delle varianze specifiche\nUna volta stimata la varianza comune \\(\\hat{\\sigma}_T^2\\), è possibile ottenere la stima della varianza residua specifica per ciascun item sottraendo la parte comune dalla varianza totale osservata:\n\\[\n\\hat{\\psi}_{ii} = s_{ii} - \\hat{\\sigma}_T^2.\n\\]\nQuesta operazione va effettuata per ciascun item \\(i = 1, \\dots, p\\).\n\n24.6.3 Caso del modello parallelo\nNel modello parallelo, si assume che anche le varianze degli errori siano uguali tra tutti gli item. In tal caso, la stima della varianza comune \\(\\hat{\\sigma}_T^2\\) rimane invariata (è ancora la media delle covarianze osservate), ma la stima della varianza specifica \\(\\psi\\), essendo costante per tutti gli item, si ottiene come media delle differenze tra le varianze osservate e la varianza comune:\n\\[\n\\hat{\\psi} = \\frac{1}{p} \\sum_{i=1}^{p} (s_{ii} - \\hat{\\sigma}_T^2).\n\\tag{24.2}\\]",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html#varianza-del-punteggio-totale-di-un-test",
    "href": "chapters/fa/06_constraints_on_parms.html#varianza-del-punteggio-totale-di-un-test",
    "title": "24  Attendibilità e modello fattoriale",
    "section": "\n24.7 Varianza del punteggio totale di un test",
    "text": "24.7 Varianza del punteggio totale di un test\nConsideriamo ora un test composto da \\(p\\) item, e definiamo il punteggio totale come:\n\\[\nY = \\sum_{i=1}^{p} X_i.\n\\]\nNel contesto di un modello monofattoriale congenerico, ogni item è modellato come:\n\\[\nX_i = \\lambda_i \\xi + \\delta_i,\n\\]\ndove:\n\n\n\\(\\lambda_i\\) è il carico fattoriale dell’item \\(i\\) sul fattore comune \\(\\xi\\);\n\n\\(\\delta_i\\) è il residuo specifico, ossia la parte del punteggio non spiegata dal fattore.\n\nQuesta formulazione è coerente con la teoria classica dei test (\\(X_i = T_i + E_i\\)), dove il punteggio vero corrisponde alla componente \\(\\lambda_i \\xi\\) e l’errore di misura alla componente \\(\\delta_i\\).\n\n24.7.1 Decomposizione della varianza del punteggio totale\nPoiché il punteggio totale \\(Y\\) è la somma di tutti gli item, possiamo scriverlo come:\n\\[\nY = \\sum_{i=1}^{p} X_i = \\sum_{i=1}^{p} (\\lambda_i \\xi + \\delta_i).\n\\]\nLa varianza del punteggio totale, assumendo che \\(\\xi\\) abbia varianza unitaria e che sia incorrelato con i residui \\(\\delta_i\\), si calcola nel seguente modo:\n\\[\n\\begin{aligned}\n\\mathbb{V}(Y) &= \\mathbb{V} \\left[ \\sum_i (\\lambda_i \\xi + \\delta_i) \\right] \\\\\n&= \\mathbb{V} \\left[ \\left( \\sum_i \\lambda_i \\right) \\xi + \\sum_i \\delta_i \\right] \\\\\n&= \\left( \\sum_i \\lambda_i \\right)^2 \\mathbb{V}(\\xi) + \\sum_i \\mathbb{V}(\\delta_i) \\\\\n&= \\left( \\sum_i \\lambda_i \\right)^2 + \\sum_i \\psi_{ii}.\n\\end{aligned}\n\\tag{24.3}\\]\nQuesta equazione mostra chiaramente che la varianza del punteggio totale si scompone in:\n\nuna componente sistematica legata al fattore comune: \\((\\sum_i \\lambda_i)^2\\);\nuna componente casuale dovuta agli errori specifici: \\(\\sum_i \\psi_{ii}\\).\n\nLa proporzione della varianza totale attribuibile al fattore comune rappresenta, in ultima analisi, ciò che intendiamo per affidabilità del test, ed è su questa base che vengono costruiti gli indici \\(\\alpha\\), \\(\\omega\\) e \\(\\rho\\).",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html#stima-dellattendibilità",
    "href": "chapters/fa/06_constraints_on_parms.html#stima-dellattendibilità",
    "title": "24  Attendibilità e modello fattoriale",
    "section": "\n24.8 Stima dell’attendibilità",
    "text": "24.8 Stima dell’attendibilità\n\n24.8.1 Coefficiente \\(\\omega\\)\n\nCome visto in precedenza, la varianza del punteggio totale \\(Y\\) di un test costituito da \\(p\\) item, nel contesto di un modello monofattoriale congenerico, può essere scomposta in due componenti:\n\\[\n\\mathbb{V}(Y) = \\left( \\sum_{i=1}^{p} \\lambda_i \\right)^2 + \\sum_{i=1}^{p} \\psi_{ii}.\n\\]\nSulla base di questa decomposizione, McDonald (2013) propone il coefficiente \\(\\omega\\) come misura dell’affidabilità del test, intesa come proporzione della varianza totale spiegata dal fattore comune:\n\\[\n\\omega = \\frac{\\left( \\sum_{i=1}^{p} \\lambda_i \\right)^2}{\\left( \\sum_{i=1}^{p} \\lambda_i \\right)^2 + \\sum_{i=1}^{p} \\psi_{ii}}.\n\\tag{24.4}\\]\nQuesto coefficiente è coerente con il modello congenerico e consente di stimare l’affidabilità di un test a partire da una singola somministrazione, utilizzando i parametri dell’analisi fattoriale.\nIn termini interpretativi, \\(\\omega\\) indica quanto della varianza osservata nel punteggio totale è realmente riconducibile al costrutto latente che il test intende misurare.\n\n24.8.1.1 Esempio pratico: sottoscala Openness del dataset bfi\n\nUtilizziamo il pacchetto psych per caricare il dataset e ricodificare gli item invertiti.\n\ndata(bfi, package = \"psych\")\nbfi$O2r &lt;- 7 - bfi$O2\nbfi$O5r &lt;- 7 - bfi$O5\n\nEsaminiamo la matrice di correlazione tra gli item della scala Openness.\n\ncor(bfi[c(\"O1\", \"O2r\", \"O3\", \"O4\", \"O5r\")], use = \"pairwise.complete.obs\") |&gt;\n  round(2)\n#&gt;       O1  O2r   O3   O4  O5r\n#&gt; O1  1.00 0.21 0.40 0.18 0.24\n#&gt; O2r 0.21 1.00 0.26 0.07 0.32\n#&gt; O3  0.40 0.26 1.00 0.19 0.31\n#&gt; O4  0.18 0.07 0.19 1.00 0.18\n#&gt; O5r 0.24 0.32 0.31 0.18 1.00\n\nEseguiamo quindi l’analisi fattoriale confermativa con il pacchetto lavaan, specificando una soluzione monofattoriale standardizzata:\n\nmod &lt;- \"\n  f =~ NA*O1 + O2r + O3 + O4 + O5r\n  f ~~ 1*f\n\"\nfit &lt;- cfa(mod, data = bfi, std.ov = TRUE, std.lv = TRUE)\n\nEstraiamo le saturazioni fattoriali (\\(\\lambda_i\\)) e le varianze specifiche (\\(\\psi_{ii}\\)):\n\nlambda &lt;- inspect(fit, \"std\")$lambda\npsy &lt;- diag(inspect(fit, \"est\")$theta)\n\nApplichiamo ora la formula per calcolare \\(\\omega\\):\n\nsum(lambda)^2 / (sum(lambda)^2 + sum(psy))\n#&gt; [1] 0.6181\n\nPossiamo confrontare il risultato con quello ottenuto tramite la funzione compRelSEM() del pacchetto semTools, che calcola \\(\\omega\\) direttamente dal modello lavaan:\n\nsemTools::compRelSEM(fit, tau.eq = FALSE)\n#&gt;     f \n#&gt; 0.618\n\nNel nostro esempio, otteniamo \\(\\omega \\approx 0.62\\), il che implica che circa il 62% della varianza del punteggio totale nella scala Openness è spiegato dal fattore latente comune.\n\n24.8.2 Ipotesi del modello e possibili violazioni\nLa formula classica di \\(\\omega\\) si basa su una ipotesi fondamentale della teoria classica dei test: l’assenza di covarianza tra gli errori specifici degli item, ovvero:\n\\[\n\\psi_{ik} = 0 \\quad \\text{per ogni } i \\neq k.\n\\]\nTuttavia, nei dati reali questa assunzione può essere violata. Se esistono covarianze significative tra gli errori specifici, la stima classica di \\(\\omega\\) risulta sovrastimata. In questi casi, come sottolineato da Bollen (1980), è necessario utilizzare una formula corretta che tenga conto anche delle covarianze tra errori:\n\\[\n\\omega = \\frac{\\left( \\sum_{i=1}^{p} \\lambda_i \\right)^2}{\\left( \\sum_{i=1}^{p} \\lambda_i \\right)^2 + \\sum_{i=1}^{p} \\psi_{ii} + \\sum_{i \\neq k} \\psi_{ik}}.\n\\]\nPer valutare la presenza di covarianze spurie tra gli errori, è possibile consultare gli indici di modifica (modification indices) forniti dall’analisi fattoriale confermativa. Se le correlazioni residue tra item sono elevate, ciò può indicare la presenza di dimensioni latenti aggiuntive, suggerendo che il test non è unidimensionale.\n\n24.8.3 Interpretazioni del coefficiente \\(\\omega\\)\n\nIl coefficiente \\(\\omega\\) può essere interpretato da diversi punti di vista, tutti coerenti con la teoria classica dei test:\n\nCorrelazione quadrata tra punteggio totale e fattore comune:\\(\\omega\\) rappresenta il quadrato della correlazione tra il punteggio totale \\(Y\\) e il fattore latente \\(\\xi\\), cioè \\(\\rho_{Y\\xi}^2\\).\nCorrelazione tra forme parallele del test:\nIn un contesto ipotetico in cui si somministrano due versioni equivalenti del test, \\(\\omega\\) rappresenta la correlazione attesa tra i due punteggi totali.\nAffidabilità nel dominio:\\(\\omega\\) può essere visto come la correlazione tra il punteggio ottenuto su \\(p\\) item e quello che si otterrebbe da una somministrazione con un numero infinito di item tratti dallo stesso dominio latente (i.e., stesso costrutto).\n\nIn conclusione, il coefficiente \\(\\omega\\) rappresenta oggi una alternativa più robusta e teoricamente fondata rispetto al tradizionale \\(\\alpha\\) di Cronbach. A differenza di \\(\\alpha\\), \\(\\omega\\) non richiede l’assunzione di \\(\\tau\\)-equivalenza tra gli item ed è pertanto utilizzabile in un modello congenerico, più realistico nella maggior parte delle applicazioni empiriche.\nIn sintesi:\n\n\n\\(\\omega\\) misura la quota di varianza del punteggio totale attribuibile al costrutto latente;\nè più flessibile e accurato rispetto ad altri indici;\npuò essere stimato direttamente da un modello fattoriale confermativo, anche su una singola somministrazione del test.\n\n24.8.4 Coefficienti \\(\\omega\\) e \\(\\alpha\\) nel modello \\(\\tau\\)-equivalente\nNel contesto dei modelli monofattoriali, i coefficienti \\(\\omega\\) e \\(\\alpha\\) offrono due approcci distinti alla stima dell’affidabilità, differenziandosi in funzione delle assunzioni strutturali sugli item:\n\n\n\\(\\omega\\) è coerente con il modello congenerico, in cui i carichi fattoriali e le varianze residue possono variare tra item;\n\n\\(\\alpha\\) si basa sul modello \\(\\tau\\)-equivalente, che assume carichi fattoriali uguali ma consente varianze residue differenti.\n\nNel modello \\(\\tau\\)-equivalente, ciascun item ha la stessa carica fattoriale \\(\\lambda\\), e la varianza totale dell’item può essere scritta come:\n\\[\n\\sigma_{ii} = \\lambda^2 + \\psi_{ii} = \\sigma_T^2 + \\sigma^2_i.\n\\]\nLa varianza del punteggio totale \\(Y\\) (somma dei \\(p\\) item) sarà:\n\\[\n\\sigma_Y^2 = p^2 \\lambda^2 + \\sum_{i=1}^p \\psi_{ii}.\n\\]\nIn questo contesto, il coefficiente \\(\\omega\\) assume la forma semplificata:\n\\[\n\\omega = \\frac{p^2 \\lambda^2}{\\sigma_Y^2} = \\frac{p^2 \\sigma_T^2}{\\sigma_Y^2}.\n\\]\nApplicando il metodo dei minimi quadrati non pesati, possiamo stimare \\(\\omega\\) nel modo seguente:\n\\[\n\\hat{\\omega} = \\frac{p^2 \\hat{\\sigma}_T^2}{s_Y^2},\n\\]\ndove \\(\\hat{\\sigma}_T^2\\) è la media delle covarianze osservate tra item:\n\\[\n\\hat{\\sigma}_T^2 = \\frac{1}{p(p-1)} \\sum_{i \\neq k} s_{ik}.\n\\]\nSostituendo questa espressione, otteniamo:\n\\[\n\\hat{\\omega} = \\frac{p}{p-1} \\cdot \\frac{\\sum_{i \\neq k} s_{ik}}{s_Y^2}.\n\\]\nOppure, espressa in funzione delle varianze:\n\\[\n\\hat{\\omega} = \\frac{p}{p-1} \\left( 1 - \\frac{\\sum_{i=1}^p s_{ii}}{s_Y^2} \\right).\n\\tag{24.5}\\]\nQuesta formula coincide con quella classica per il coefficiente \\(\\alpha\\), che nei valori di popolazione si scrive:\n\\[\n\\alpha = \\frac{p}{p - 1} \\left( 1 - \\frac{\\sum_{i=1}^p \\sigma_{ii}}{\\sigma_Y^2} \\right)\n       = \\frac{p}{p - 1} \\cdot \\frac{\\sum_{i \\neq k} \\text{Cov}(X_i, X_k)}{\\mathbb{V}(Y)}.\n\\tag{24.6}\\]\nSotto le assunzioni del modello \\(\\tau\\)-equivalente, \\(\\alpha\\) e \\(\\omega\\) coincidono. Tuttavia, in presenza di carichi fattoriali disuguali (modello congenerico), \\(\\alpha\\) tende a sottostimare \\(\\omega\\), rendendolo un limite inferiore dell’affidabilità. Questa proprietà conservativa di \\(\\alpha\\) è stata spesso invocata come argomento a favore del suo utilizzo, ma essa non è garantita al di fuori del modello \\(\\tau\\)-equivalente.\n\n24.8.4.1 Esempio pratico: \\(\\alpha\\) per la sottoscala Openness\n\nCalcoliamo ora il coefficiente \\(\\alpha\\) a partire dalla matrice di covarianze empirica:\n\nC &lt;- cov(bfi[c(\"O1\", \"O2r\", \"O3\", \"O4\", \"O5r\")], use = \"pairwise.complete.obs\")\nround(C, 2)\n#&gt;       O1  O2r   O3   O4  O5r\n#&gt; O1  1.28 0.38 0.54 0.25 0.36\n#&gt; O2r 0.38 2.45 0.50 0.13 0.67\n#&gt; O3  0.54 0.50 1.49 0.29 0.50\n#&gt; O4  0.25 0.13 0.29 1.49 0.29\n#&gt; O5r 0.36 0.67 0.50 0.29 1.76\n\nApplichiamo la formula dell’\\(\\alpha\\) campionaria:\n\np &lt;- 5\nalpha &lt;- (p / (p - 1)) * (1 - tr(C) / sum(C))\nalpha\n#&gt; [1] 0.6002\n\n\n24.8.5 La formula “profetica” di Spearman-Brown\nLa formula profetica di Spearman-Brown è utilizzata per prevedere l’affidabilità di un test costruito da item paralleli, cioè:\n\ntutti gli item hanno lo stesso carico fattoriale \\(\\lambda\\);\ntutte le varianze residue sono uguali: \\(\\psi_{ii} = \\psi\\).\n\nIn questo caso, la varianza del punteggio totale è:\n\\[\n\\sigma_Y^2 = p^2 \\lambda^2 + p \\psi.\n\\]\nL’affidabilità del test è allora:\n\\[\n\\rho_p = \\frac{p^2 \\lambda^2}{p^2 \\lambda^2 + p \\psi}\n       = \\frac{p \\lambda^2}{p \\lambda^2 + \\psi}.\n\\]\nSe definiamo l’affidabilità di un singolo item come:\n\\[\n\\rho_1 = \\frac{\\lambda^2}{\\lambda^2 + \\psi},\n\\]\nla formula di Spearman-Brown esprime l’affidabilità del test come:\n\\[\n\\rho_p = \\frac{p \\rho_1}{(p - 1) \\rho_1 + 1}.\n\\tag{24.7}\\]\nQuesta formula è particolarmente utile quando si vuole prevedere l’effetto dell’aggiunta di nuovi item su un test esistente, ed è stata storicamente uno strumento fondamentale nello sviluppo dei test psicometrici.\nNel modello parallelo, i coefficienti \\(\\alpha\\), \\(\\omega\\) e \\(\\rho_p\\) coincidono, poiché tutte le assunzioni richieste per ciascuno sono soddisfatte simultaneamente.\n\n24.8.5.1 Esempio pratico: Spearman-Brown per la sottoscala Openness\n\nIpotizziamo che gli item della scala Openness siano paralleli. In questo caso, possiamo calcolare l’affidabilità del test a partire dalla correlazione media tra item:\n\nR &lt;- cor(bfi[c(\"O1\", \"O2r\", \"O3\", \"O4\", \"O5r\")], use = \"pairwise.complete.obs\")\nround(R, 3)\n#&gt;        O1   O2r    O3    O4   O5r\n#&gt; O1  1.000 0.214 0.395 0.178 0.239\n#&gt; O2r 0.214 1.000 0.262 0.068 0.325\n#&gt; O3  0.395 0.262 1.000 0.195 0.311\n#&gt; O4  0.178 0.068 0.195 1.000 0.179\n#&gt; O5r 0.239 0.325 0.311 0.179 1.000\n\nCalcoliamo la media delle correlazioni inter-item:\n\np &lt;- 5\nrr &lt;- NULL\nk &lt;- 1\nfor (i in 1:p) {\n  for (j in 1:p) {\n    if (j != i) rr[k] &lt;- R[i, j]\n    k &lt;- k + 1\n  }\n}\nro_1 &lt;- mean(rr, na.rm = TRUE)\nprint(ro_1)\n#&gt; [1] 0.2365\n\nApplichiamo la formula di Spearman-Brown:\n\n(p * ro_1) / ((p - 1) * ro_1 + 1) |&gt;\n  round(3)\n#&gt; [1] 0.6078\n\nIl risultato fornisce una stima dell’affidabilità complessiva del test sotto l’ipotesi di parallelismo degli item. Confrontare questo valore con quelli ottenuti tramite \\(\\omega\\) e \\(\\alpha\\) può essere utile per riflettere sulla validità delle assunzioni sottostanti al modello utilizzato.\n\n24.8.6 Conclusioni\n\n\n\n\n\n\n\n\nModello\nAssunzioni principali\nCoefficiente coerente\nFormula\n\n\n\nCongenerico\n\n\\(\\lambda_i\\) e \\(\\psi_{ii}\\) liberi\n\\(\\omega\\)\nVarianza spiegata / totale\n\n\n\n\\(\\tau\\)-equivalente\n\n\\(\\lambda_i = \\lambda\\); \\(\\psi_{ii}\\) liberi\n\n\\(\\alpha\\), \\(\\omega\\)\n\n\n\\(\\alpha\\) = \\(\\omega\\)\n\n\n\nParallelo\n\n\\(\\lambda_i = \\lambda\\); \\(\\psi_{ii} = \\psi\\)\n\n\n\\(\\rho\\), \\(\\omega\\), \\(\\alpha\\)\n\n\\(\\omega = \\alpha = \\rho\\)\n\n\n\n\nLa scelta dell’indice di affidabilità più appropriato dipende sempre dalle ipotesi del modello di misura. È quindi fondamentale:\n\ncomprendere la struttura del proprio strumento;\nvalutare empiricamente la bontà del modello (con CFA o EFA);\ninterpretare ciascun coefficiente alla luce delle assunzioni teoriche sottostanti.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html#commenti-e-considerazioni-conclusive",
    "href": "chapters/fa/06_constraints_on_parms.html#commenti-e-considerazioni-conclusive",
    "title": "32  Attendibilità e modello fattoriale",
    "section": "\n32.7 Commenti e considerazioni conclusive",
    "text": "32.7 Commenti e considerazioni conclusive\nIl coefficiente \\(\\alpha\\) di Cronbach è uno degli indici di affidabilità più diffusi in psicometria. Tuttavia, la sua efficacia dipende strettamente dalla \\(\\tau\\)-equivalenza degli item, che presuppongono un tratto latente unidimensionale. Nella pratica, questa condizione è spesso violata: molti test misurano più di un fattore, e le comunalità degli item non sono uniformi, mettendo in discussione la validità dell’ipotesi di \\(\\tau\\)-equivalenza. Se gli errori sono incorrelati, il coefficiente \\(\\alpha\\) può sottostimare l’affidabilità; se invece gli errori sono correlati, può sovrastimarla.\nData questa limitazione, l’utilizzo del coefficiente \\(\\omega\\) di McDonald è generalmente più consigliabile. Il coefficiente \\(\\omega\\) fornisce una stima più robusta dell’affidabilità in vari contesti, inclusi quelli con assunzioni meno restrittive rispetto alla \\(\\tau\\)-equivalenza. Altri indici come il \\(glb\\) (Greatest Lower Bound), discusso da Ten Berge e Sočan (2004), e l’indice \\(\\beta\\) di Revelle (1979), rappresentano alternative valide al coefficiente \\(\\alpha\\), offrendo diversi vantaggi metodologici a seconda delle specifiche esigenze di misurazione e delle caratteristiche dei dati analizzati.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html#session-info",
    "href": "chapters/fa/06_constraints_on_parms.html#session-info",
    "title": "24  Attendibilità e modello fattoriale",
    "section": "\n24.10 Session Info",
    "text": "24.10 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.2\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] modelsummary_2.3.0 ggokabeito_0.1.0   see_0.11.0        \n#&gt;  [4] MASS_7.3-65        viridis_0.6.5      viridisLite_0.4.2 \n#&gt;  [7] ggpubr_0.6.0       ggExtra_0.10.1     gridExtra_2.3     \n#&gt; [10] patchwork_1.3.0    bayesplot_1.11.1   semTools_0.5-6    \n#&gt; [13] semPlot_1.1.6      lavaan_0.6-19      psych_2.4.12      \n#&gt; [16] scales_1.3.0       markdown_1.13      knitr_1.50        \n#&gt; [19] lubridate_1.9.4    forcats_1.0.0      stringr_1.5.1     \n#&gt; [22] dplyr_1.1.4        purrr_1.0.4        readr_2.1.5       \n#&gt; [25] tidyr_1.3.1        tibble_3.2.1       ggplot2_3.5.1     \n#&gt; [28] tidyverse_2.0.0    here_1.0.1        \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.1      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.2.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] mime_0.13           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [25] Matrix_1.7-3        R6_2.6.1            fastmap_1.2.0      \n#&gt;  [28] rbibutils_2.3       shiny_1.10.0        digest_0.6.37      \n#&gt;  [31] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n#&gt;  [34] rprojroot_2.0.4     Hmisc_5.2-3         timechange_0.3.0   \n#&gt;  [37] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n#&gt;  [40] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n#&gt;  [43] carData_3.0-5       ggsignif_0.6.4      corpcor_1.6.10     \n#&gt;  [46] gtools_3.9.5        tools_4.4.2         pbivnorm_0.6.0     \n#&gt;  [49] foreign_0.8-88      zip_2.3.2           httpuv_1.6.15      \n#&gt;  [52] nnet_7.3-20         glue_1.8.0          quadprog_1.5-8     \n#&gt;  [55] nlme_3.1-167        promises_1.3.2      lisrelToR_0.3      \n#&gt;  [58] grid_4.4.2          checkmate_2.3.2     cluster_2.1.8.1    \n#&gt;  [61] reshape2_1.4.4      generics_0.1.3      gtable_0.3.6       \n#&gt;  [64] tzdb_0.5.0          data.table_1.17.0   hms_1.1.3          \n#&gt;  [67] car_3.1-3           tables_0.9.31       sem_3.1-16         \n#&gt;  [70] pillar_1.10.1       rockchalk_1.8.157   later_1.4.1        \n#&gt;  [73] splines_4.4.2       lattice_0.22-6      survival_3.8-3     \n#&gt;  [76] kutils_1.73         tidyselect_1.2.1    miniUI_0.1.1.1     \n#&gt;  [79] pbapply_1.7-2       reformulas_0.4.0    stats4_4.4.2       \n#&gt;  [82] xfun_0.51           qgraph_1.9.8        arm_1.14-4         \n#&gt;  [85] stringi_1.8.4       yaml_2.3.10         pacman_0.5.1       \n#&gt;  [88] boot_1.3-31         evaluate_1.0.3      codetools_0.2-20   \n#&gt;  [91] mi_1.1              cli_3.6.4           RcppParallel_5.1.10\n#&gt;  [94] rpart_4.1.24        xtable_1.8-4        Rdpack_2.6.3       \n#&gt;  [97] munsell_0.5.1       Rcpp_1.0.14         coda_0.19-4.1      \n#&gt; [100] png_0.1-8           XML_3.99-0.18       parallel_4.4.2     \n#&gt; [103] jpeg_0.1-10         lme4_1.1-36         mvtnorm_1.3-3      \n#&gt; [106] openxlsx_4.2.8      rlang_1.1.5         multcomp_1.4-28    \n#&gt; [109] mnormt_2.1.1\n\n\n\n\n\nMcDonald, R. P. (2013). Test theory: A unified treatment. Psychology Press.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/07_total_score.html",
    "href": "chapters/fa/07_total_score.html",
    "title": "25  Punteggio totale e modello fattoriale",
    "section": "",
    "text": "25.1 Introduzione\nIn questo capitolo affrontiamo una questione centrale nella costruzione e interpretazione dei test psicometrici: è corretto utilizzare il punteggio totale come misura del costrutto latente che il test intende rilevare?\nL’uso del punteggio totale (cioè la semplice somma dei punteggi degli item) è una pratica diffusissima nella ricerca psicologica e nelle applicazioni cliniche. Tuttavia, tale uso è giustificabile solo in circostanze ben precise. Come mostrano McNeish & Wolf (2020), è necessario verificare che i dati raccolti rispettino determinate assunzioni. In caso contrario, il punteggio totale può essere una misura fuorviante del costrutto.\nPer comprendere quando il punteggio totale può essere considerato una misura adeguata del costrutto latente, dobbiamo considerare il legame tra il punteggio totale e i modelli di misura fattoriali, in particolare il modello parallelo e il modello congenerico.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Punteggio totale e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/07_total_score.html#punteggio-totale-e-modello-fattoriale-parallelo",
    "href": "chapters/fa/07_total_score.html#punteggio-totale-e-modello-fattoriale-parallelo",
    "title": "25  Punteggio totale e modello fattoriale",
    "section": "\n25.2 Punteggio totale e modello fattoriale parallelo",
    "text": "25.2 Punteggio totale e modello fattoriale parallelo\n\n25.2.1 Il modello parallelo\nSecondo McNeish & Wolf (2020), il punteggio totale può essere considerato una misura valida del costrutto solo se gli item soddisfano i requisiti del modello fattoriale parallelo. Questo modello implica:\n\n\nsaturazioni fattoriali uguali per tutti gli item (ad esempio, tutte uguali a 1);\n\nvarianze residue uguali tra gli item.\n\nIn termini psicometrici, questo significa assumere che ogni item contribuisce nella stessa misura alla valutazione del costrutto, e che ha lo stesso livello di “rumore” o errore.\n\n25.2.2 Esempio: Dati di Holzinger e Swineford (1939)\nMcNeish & Wolf (2020) illustrano il problema usando i dati classici di Holzinger e Swineford. Gli item considerati sono:\n\nParagraph comprehension\n\nSentence completion\n\nWord definitions\n\nSpeeded addition\n\nSpeeded dot counting\n\nDiscrimination between curved and straight letters\n\nImportiamo i dati in R:\n\nd &lt;- rio::import(here::here(\"data\", \"1_Factor_Parallel.csv\"))\n\nSupponiamo ora di voler stimare il costrutto sottostante utilizzando il punteggio totale:\n\\[\n\\text{Punteggio totale} = \\text{Item 1 + Item 2 + Item 3 + Item 4 + Item 5 + Item 6}\n\\]\nQuesto equivale ad assumere che ogni item fornisca esattamente la stessa quantità di informazione sul costrutto. Formalmente, ciò può essere espresso da un modello parallelo nel quale tutte le saturazioni fattoriali sono fissate a 1 e tutte le varianze residue sono uguali.\n\nm_parallel &lt;- \"\n  f1 =~ 1*X4 + 1*X5 + 1*X6 + 1*X7 + 1*X8 + 1*X9\n  X4 ~~ theta*X4\n  X5 ~~ theta*X5\n  X6 ~~ theta*X6\n  X7 ~~ theta*X7\n  X8 ~~ theta*X8\n  X9 ~~ theta*X9\n\"\n\n\nfit_parallel &lt;- sem(m_parallel, data = d)\n\n\n25.2.3 Confronto tra punteggio totale e punteggio fattoriale\nCalcoliamo ora il punteggio totale e i punteggi fattoriali stimati dal modello:\n\nd$ts &lt;- with(d, X4 + X5 + X6 + X7 + X8 + X9)\nd$scores &lt;- as.numeric(lavPredict(fit_parallel, method=\"regression\"))\n\nVisualizziamo la relazione tra i due:\n\nggplot(d, aes(x = ts, y = scores)) + geom_point()\n\n\n\n\n\n\n\nSe il modello parallelo fosse corretto, il punteggio totale e il punteggio fattoriale dovrebbero essere perfettamente correlati. Tuttavia, l’output di lavaan mostra che il modello parallelo non si adatta bene ai dati:\n\nsummary(fit_parallel, fit.measures = TRUE, standardized = TRUE)\n#&gt; lavaan 0.6-19 ended normally after 13 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                         7\n#&gt;   Number of equality constraints                     5\n#&gt; \n#&gt;   Number of observations                           301\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                               325.899\n#&gt;   Degrees of freedom                                19\n#&gt;   P-value (Chi-square)                           0.000\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               568.519\n#&gt;   Degrees of freedom                                15\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.446\n#&gt;   Tucker-Lewis Index (TLI)                       0.562\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -2680.931\n#&gt;   Loglikelihood unrestricted model (H1)      -2517.981\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                5365.862\n#&gt;   Bayesian (BIC)                              5373.276\n#&gt;   Sample-size adjusted Bayesian (SABIC)       5366.933\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.232\n#&gt;   90 Percent confidence interval - lower         0.210\n#&gt;   90 Percent confidence interval - upper         0.254\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.000\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    1.000\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.206\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   f1 =~                                                                 \n#&gt;     X4                1.000                               0.633    0.551\n#&gt;     X5                1.000                               0.633    0.551\n#&gt;     X6                1.000                               0.633    0.551\n#&gt;     X7                1.000                               0.633    0.551\n#&gt;     X8                1.000                               0.633    0.551\n#&gt;     X9                1.000                               0.633    0.551\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .X4      (thet)    0.920    0.034   27.432    0.000    0.920    0.697\n#&gt;    .X5      (thet)    0.920    0.034   27.432    0.000    0.920    0.697\n#&gt;    .X6      (thet)    0.920    0.034   27.432    0.000    0.920    0.697\n#&gt;    .X7      (thet)    0.920    0.034   27.432    0.000    0.920    0.697\n#&gt;    .X8      (thet)    0.920    0.034   27.432    0.000    0.920    0.697\n#&gt;    .X9      (thet)    0.920    0.034   27.432    0.000    0.920    0.697\n#&gt;     f1                0.400    0.045    8.803    0.000    1.000    1.000\n\nIn questo caso, l’uso del punteggio totale non è giustificato: gli item non contribuiscono in modo uniforme alla misura del costrutto, e le ipotesi del modello parallelo non sono rispettate.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Punteggio totale e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/07_total_score.html#punteggio-totale-e-modello-fattoriale-congenerico",
    "href": "chapters/fa/07_total_score.html#punteggio-totale-e-modello-fattoriale-congenerico",
    "title": "33  Punteggio totale e modello fattoriale",
    "section": "\n33.2 Punteggio totale e modello fattoriale congenerico",
    "text": "33.2 Punteggio totale e modello fattoriale congenerico\nGli autori adattano ai dati un modello congenerico.\n\nm_congeneric &lt;- \n'\n  #all loadings are uniquely estimated\n  f1 =~ NA*X4 + X5 + X6 + X7 + X8 + X9\n  #constrain factor variance to 1\n  f1 ~~ 1*f1\n'\n\n\n# Fit above model\nfit_congeneric &lt;- sem(m_congeneric, data=d)\n\n\nparameterEstimates(fit_congeneric, standardized = TRUE) %&gt;%\n  dplyr::filter(op == \"=~\") %&gt;%\n  dplyr::select(\n    \"Latent Factor\" = lhs,\n    Indicator = rhs,\n    B = est,\n    SE = se,\n    Z = z,\n    \"p-value\" = pvalue,\n    Beta = std.all\n  ) %&gt;%\n  knitr::kable(\n    digits = 3, booktabs = TRUE, format = \"markdown\",\n    caption = \"Factor Loadings\"\n  )\n\n\nFactor Loadings\n\nLatent Factor\nIndicator\nB\nSE\nZ\np-value\nBeta\n\n\n\nf1\nX4\n0.963\n0.059\n16.274\n0.000\n0.824\n\n\nf1\nX5\n1.121\n0.067\n16.835\n0.000\n0.846\n\n\nf1\nX6\n0.894\n0.058\n15.450\n0.000\n0.792\n\n\nf1\nX7\n0.195\n0.071\n2.767\n0.006\n0.170\n\n\nf1\nX8\n0.185\n0.063\n2.938\n0.003\n0.180\n\n\nf1\nX9\n0.278\n0.065\n4.245\n0.000\n0.258\n\n\n\n\n\nSi noti che le saturazioni fattoriali sono molto diverse tra loro, suggerendo che il punteggio del costrutto si relaziona in modo diverso con ciascun item e che sarebbe inappropriato stimare il punteggio del costrutto assegnando un peso unitario agli item.\nMcNeish e Wolf (2020) calcolano poi i punteggi fattoriali del modello congenerico.\n\nscores_cong &lt;- lavPredict(fit_congeneric, method=\"regression\")\nd$scores_cong &lt;- as.numeric(scores_cong)\n\nIl grafico seguente mostra la relazione tra i punteggi fattoriali e il punteggio totale.\n\nd |&gt; \n  ggplot(aes(x=ts, y=scores_cong)) + \n  geom_point()\n\n\n\n\n\n\n\nNel caso presente, il coefficiente di determinazione tra punteggio totale e punteggi fattoriali è 0.77.\n\ncor(d$ts, d$scores_cong)^2\n#&gt; [1] 0.766\n\nSecondo gli autori, ciò significa che due persone con un punteggio totale identico potrebbero avere punteggi di modello congenerico potenzialmente diversi perché hanno raggiunto il loro particolare punteggio totale approvando item diversi. Poiché il modello congenerico assegna pesi diversi agli item, ciascun item contribuisce in modo diverso al punteggio fattoriale del modello congenerico, il che non è vero per il punteggio totale.\nSi noti che, per i dati di Holzinger and Swineford (1939), neppure un modello congenerico ad un fattore si dimostra adeguato.\n\nout = summary(fit_congeneric, fit.measures = TRUE, standardized = TRUE)\nprint(out)\n#&gt; lavaan 0.6-19 ended normally after 16 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        12\n#&gt; \n#&gt;   Number of observations                           301\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                               115.366\n#&gt;   Degrees of freedom                                 9\n#&gt;   P-value (Chi-square)                           0.000\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               568.519\n#&gt;   Degrees of freedom                                15\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.808\n#&gt;   Tucker-Lewis Index (TLI)                       0.680\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -2575.664\n#&gt;   Loglikelihood unrestricted model (H1)      -2517.981\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                5175.328\n#&gt;   Bayesian (BIC)                              5219.813\n#&gt;   Sample-size adjusted Bayesian (SABIC)       5181.756\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.198\n#&gt;   90 Percent confidence interval - lower         0.167\n#&gt;   90 Percent confidence interval - upper         0.231\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.000\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    1.000\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.129\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   f1 =~                                                                 \n#&gt;     X4                0.963    0.059   16.274    0.000    0.963    0.824\n#&gt;     X5                1.121    0.067   16.835    0.000    1.121    0.846\n#&gt;     X6                0.894    0.058   15.450    0.000    0.894    0.792\n#&gt;     X7                0.195    0.071    2.767    0.006    0.195    0.170\n#&gt;     X8                0.185    0.063    2.938    0.003    0.185    0.180\n#&gt;     X9                0.278    0.065    4.245    0.000    0.278    0.258\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;     f1                1.000                               1.000    1.000\n#&gt;    .X4                0.437    0.056    7.775    0.000    0.437    0.320\n#&gt;    .X5                0.500    0.071    6.998    0.000    0.500    0.285\n#&gt;    .X6                0.474    0.054    8.777    0.000    0.474    0.372\n#&gt;    .X7                1.278    0.105   12.211    0.000    1.278    0.971\n#&gt;    .X8                1.023    0.084   12.204    0.000    1.023    0.967\n#&gt;    .X9                1.080    0.089   12.132    0.000    1.080    0.933\n\nSe trascuriamo le considerazioni sulla struttura fattoriale e esaminiamo (per esempio) unicamente il coefficiente omega, finiamo per trovare una risposta accettabile, ma sbagliata.\n\npsych::omega(d[, 1:6])\n#&gt; Omega \n#&gt; Call: omegah(m = m, nfactors = nfactors, fm = fm, key = key, flip = flip, \n#&gt;     digits = digits, title = title, sl = sl, labels = labels, \n#&gt;     plot = plot, n.obs = n.obs, rotate = rotate, Phi = Phi, option = option, \n#&gt;     covar = covar)\n#&gt; Alpha:                 0.72 \n#&gt; G.6:                   0.76 \n#&gt; Omega Hierarchical:    0.55 \n#&gt; Omega H asymptotic:    0.65 \n#&gt; Omega Total            0.84 \n#&gt; \n#&gt; Schmid Leiman Factor loadings greater than  0.2 \n#&gt;       g  F1*  F2*   F3*   h2   h2   u2   p2  com\n#&gt; X4 0.73            0.68 1.00 1.00 0.00 0.53 1.99\n#&gt; X5 0.96                 0.92 0.92 0.08 1.00 1.01\n#&gt; X6 0.69            0.22 0.54 0.54 0.46 0.90 1.22\n#&gt; X7           0.56       0.33 0.33 0.67 0.03 1.15\n#&gt; X8           0.75       0.59 0.59 0.41 0.05 1.12\n#&gt; X9 0.22      0.49       0.29 0.29 0.71 0.16 1.41\n#&gt; \n#&gt; With Sums of squares  of:\n#&gt;    g  F1*  F2*  F3*   h2 \n#&gt; 2.02 0.00 1.11 0.54 2.67 \n#&gt; \n#&gt; general/max  0.75   max/min =   622.1\n#&gt; mean percent general =  0.44    with sd =  0.43 and cv of  0.97 \n#&gt; Explained Common Variance of the general factor =  0.55 \n#&gt; \n#&gt; The degrees of freedom are 0  and the fit is  0 \n#&gt; The number of observations was  301  with Chi Square =  0.03  with prob &lt;  NA\n#&gt; The root mean square of the residuals is  0 \n#&gt; The df corrected root mean square of the residuals is  NA\n#&gt; \n#&gt; Compare this with the adequacy of just a general factor and no group factors\n#&gt; The degrees of freedom for just the general factor are 9  and the fit is  0.48 \n#&gt; The number of observations was  301  with Chi Square =  142.3  with prob &lt;  3.5e-26\n#&gt; The root mean square of the residuals is  0.17 \n#&gt; The df corrected root mean square of the residuals is  0.21 \n#&gt; \n#&gt; RMSEA index =  0.222  and the 10 % confidence intervals are  0.191 0.255\n#&gt; BIC =  90.9 \n#&gt; \n#&gt; Measures of factor score adequacy             \n#&gt;                                                  g   F1*  F2*  F3*\n#&gt; Correlation of scores with factors            0.96  0.08 0.83 0.96\n#&gt; Multiple R square of scores with factors      0.93  0.01 0.68 0.91\n#&gt; Minimum correlation of factor score estimates 0.86 -0.99 0.36 0.83\n#&gt; \n#&gt;  Total, General and Subset omega for each subset\n#&gt;                                                  g  F1*  F2*  F3*\n#&gt; Omega total for total scores and subscales    0.84 0.92 0.66 0.86\n#&gt; Omega general for total scores and subscales  0.55 0.92 0.04 0.61\n#&gt; Omega group for total scores and subscales    0.27 0.00 0.61 0.25\n\n\n\n\n\n\n\nÈ invece necessario ipotizzare un modello congenerico a due fattori.\n\nm2f_cong &lt;- '\n  # all loadings are uniquely estimated on each factor\n  f1 =~ NA*X4 + X5 + X6\n  f2 =~ NA*X7 + X8 + X9\n  \n  # constrain factor variancse to 1\n  f1 ~~ 1*f1\n  f2 ~~ 1*f2\n  \n  # estimate factor covariance\n  f1 ~~ f2\n'\n\n\n# Fit above model\nfit_2f_congeneric &lt;- sem(m2f_cong, data=d)\n\nSolo questo modello fornisce un adattamento adeguato ai dati.\n\nout = summary(fit_2f_congeneric, fit.measures = TRUE, standardized = TRUE)\nprint(out)\n#&gt; lavaan 0.6-19 ended normally after 18 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        13\n#&gt; \n#&gt;   Number of observations                           301\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                14.736\n#&gt;   Degrees of freedom                                 8\n#&gt;   P-value (Chi-square)                           0.064\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               568.519\n#&gt;   Degrees of freedom                                15\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.988\n#&gt;   Tucker-Lewis Index (TLI)                       0.977\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -2525.349\n#&gt;   Loglikelihood unrestricted model (H1)      -2517.981\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                5076.698\n#&gt;   Bayesian (BIC)                              5124.891\n#&gt;   Sample-size adjusted Bayesian (SABIC)       5083.662\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.053\n#&gt;   90 Percent confidence interval - lower         0.000\n#&gt;   90 Percent confidence interval - upper         0.095\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.402\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.159\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.035\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   f1 =~                                                                 \n#&gt;     X4                0.965    0.059   16.296    0.000    0.965    0.826\n#&gt;     X5                1.123    0.067   16.845    0.000    1.123    0.847\n#&gt;     X6                0.895    0.058   15.465    0.000    0.895    0.793\n#&gt;   f2 =~                                                                 \n#&gt;     X7                0.659    0.080    8.218    0.000    0.659    0.575\n#&gt;     X8                0.733    0.077    9.532    0.000    0.733    0.712\n#&gt;     X9                0.599    0.075    8.025    0.000    0.599    0.557\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   f1 ~~                                                                 \n#&gt;     f2                0.275    0.072    3.813    0.000    0.275    0.275\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;     f1                1.000                               1.000    1.000\n#&gt;     f2                1.000                               1.000    1.000\n#&gt;    .X4                0.433    0.056    7.679    0.000    0.433    0.318\n#&gt;    .X5                0.496    0.072    6.892    0.000    0.496    0.282\n#&gt;    .X6                0.472    0.054    8.732    0.000    0.472    0.371\n#&gt;    .X7                0.881    0.100    8.807    0.000    0.881    0.670\n#&gt;    .X8                0.521    0.094    5.534    0.000    0.521    0.492\n#&gt;    .X9                0.798    0.087    9.162    0.000    0.798    0.689\n\nNel contesto di questi dati, l’utilizzo di un modello congenerico non è sufficiente a giustificare l’impiego del punteggio totale, che rappresenta la somma dei punteggi degli item. Questo perché, nel caso specifico, sommando i punteggi di tutti gli item, finiremmo per includere misurazioni di due costrutti distinti.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Punteggio totale e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/07_total_score.html#session-info",
    "href": "chapters/fa/07_total_score.html#session-info",
    "title": "25  Punteggio totale e modello fattoriale",
    "section": "\n25.6 Session Info",
    "text": "25.6 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.2\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] modelsummary_2.3.0 ggokabeito_0.1.0   see_0.11.0        \n#&gt;  [4] MASS_7.3-65        viridis_0.6.5      viridisLite_0.4.2 \n#&gt;  [7] ggpubr_0.6.0       ggExtra_0.10.1     gridExtra_2.3     \n#&gt; [10] patchwork_1.3.0    bayesplot_1.11.1   semTools_0.5-6    \n#&gt; [13] semPlot_1.1.6      lavaan_0.6-19      psych_2.4.12      \n#&gt; [16] scales_1.3.0       markdown_1.13      knitr_1.50        \n#&gt; [19] lubridate_1.9.4    forcats_1.0.0      stringr_1.5.1     \n#&gt; [22] dplyr_1.1.4        purrr_1.0.4        readr_2.1.5       \n#&gt; [25] tidyr_1.3.1        tibble_3.2.1       ggplot2_3.5.1     \n#&gt; [28] tidyverse_2.0.0    here_1.0.1        \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1    jsonlite_1.9.1       magrittr_2.0.3      \n#&gt;   [4] TH.data_1.1-3        estimability_1.5.1   farver_2.1.2        \n#&gt;   [7] nloptr_2.2.1         rmarkdown_2.29       vctrs_0.6.5         \n#&gt;  [10] minqa_1.2.8          base64enc_0.1-3      rstatix_0.7.2       \n#&gt;  [13] htmltools_0.5.8.1    broom_1.0.7          Formula_1.2-5       \n#&gt;  [16] htmlwidgets_1.6.4    plyr_1.8.9           sandwich_3.1-1      \n#&gt;  [19] rio_1.2.3            emmeans_1.10.7       zoo_1.8-13          \n#&gt;  [22] igraph_2.1.4         mime_0.13            lifecycle_1.0.4     \n#&gt;  [25] pkgconfig_2.0.3      Matrix_1.7-3         R6_2.6.1            \n#&gt;  [28] fastmap_1.2.0        rbibutils_2.3        shiny_1.10.0        \n#&gt;  [31] digest_0.6.37        OpenMx_2.21.13       fdrtool_1.2.18      \n#&gt;  [34] colorspace_2.1-1     rprojroot_2.0.4      Hmisc_5.2-3         \n#&gt;  [37] labeling_0.4.3       timechange_0.3.0     abind_1.4-8         \n#&gt;  [40] compiler_4.4.2       withr_3.0.2          glasso_1.11         \n#&gt;  [43] htmlTable_2.4.3      backports_1.5.0      carData_3.0-5       \n#&gt;  [46] R.utils_2.13.0       ggsignif_0.6.4       GPArotation_2024.3-1\n#&gt;  [49] corpcor_1.6.10       gtools_3.9.5         tools_4.4.2         \n#&gt;  [52] pbivnorm_0.6.0       foreign_0.8-88       zip_2.3.2           \n#&gt;  [55] httpuv_1.6.15        nnet_7.3-20          R.oo_1.27.0         \n#&gt;  [58] glue_1.8.0           quadprog_1.5-8       nlme_3.1-167        \n#&gt;  [61] promises_1.3.2       lisrelToR_0.3        grid_4.4.2          \n#&gt;  [64] checkmate_2.3.2      cluster_2.1.8.1      reshape2_1.4.4      \n#&gt;  [67] generics_0.1.3       gtable_0.3.6         tzdb_0.5.0          \n#&gt;  [70] R.methodsS3_1.8.2    data.table_1.17.0    hms_1.1.3           \n#&gt;  [73] car_3.1-3            tables_0.9.31        sem_3.1-16          \n#&gt;  [76] pillar_1.10.1        rockchalk_1.8.157    later_1.4.1         \n#&gt;  [79] splines_4.4.2        lattice_0.22-6       survival_3.8-3      \n#&gt;  [82] kutils_1.73          tidyselect_1.2.1     miniUI_0.1.1.1      \n#&gt;  [85] pbapply_1.7-2        reformulas_0.4.0     stats4_4.4.2        \n#&gt;  [88] xfun_0.51            qgraph_1.9.8         arm_1.14-4          \n#&gt;  [91] stringi_1.8.4        yaml_2.3.10          pacman_0.5.1        \n#&gt;  [94] boot_1.3-31          evaluate_1.0.3       codetools_0.2-20    \n#&gt;  [97] mi_1.1               cli_3.6.4            RcppParallel_5.1.10 \n#&gt; [100] rpart_4.1.24         xtable_1.8-4         Rdpack_2.6.3        \n#&gt; [103] munsell_0.5.1        Rcpp_1.0.14          coda_0.19-4.1       \n#&gt; [106] png_0.1-8            XML_3.99-0.18        parallel_4.4.2      \n#&gt; [109] jpeg_0.1-10          lme4_1.1-36          mvtnorm_1.3-3       \n#&gt; [112] openxlsx_4.2.8       rlang_1.1.5          multcomp_1.4-28     \n#&gt; [115] mnormt_2.1.1\n\n\n\n\n\nMcNeish, D., & Wolf, M. G. (2020). Thinking twice about sum scores. Behavior Research Methods, 52, 2287–2305.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Punteggio totale e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/01_val_matrici.html",
    "href": "chapters/extraction/01_val_matrici.html",
    "title": "26  Valutazione della matrice di correlazione",
    "section": "",
    "text": "26.1 Introduzione\nPrima di eseguire un’analisi fattoriale esplorativa (EFA), è fondamentale valutare se i dati soddisfano i presupposti minimi richiesti. Il primo passo consiste nell’esaminare la matrice di correlazione tra le variabili. Se il determinante di questa matrice è pari a zero, significa che esiste collinearità perfetta tra alcune variabili: in questo caso, l’analisi fattoriale non può essere effettuata perché non sarebbe possibile distinguere contributi distinti dei fattori latenti.\nTuttavia, anche se il determinante è diverso da zero, non è detto che i dati siano adatti all’EFA. È necessario che le variabili mostrino correlazioni sufficientemente elevate tra loro, altrimenti l’analisi potrebbe produrre soluzioni instabili o difficili da interpretare. Correlazioni troppo deboli indicano che non esistono dimensioni comuni sottostanti e quindi non ha senso estrarre dei fattori.\nPer valutare l’adeguatezza dei dati, possiamo:\nQuesti strumenti forniscono informazioni complementari sull’opportunità di procedere con un’analisi fattoriale.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Valutazione della matrice di correlazione</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/01_val_matrici.html#analisi-preliminari",
    "href": "chapters/extraction/01_val_matrici.html#analisi-preliminari",
    "title": "26  Valutazione della matrice di correlazione",
    "section": "\n26.2 Analisi Preliminari",
    "text": "26.2 Analisi Preliminari\nPer illustrare il procedimento, utilizziamo il dataset HolzingerSwineford1939, che contiene 301 osservazioni relative a punteggi di abilità mentale su diverse prove cognitive. In questa analisi considereremo le variabili x1–x9.\nCominciamo con l’importazione del dataset e la verifica dell’integrità dei dati.\n\n# Caricamento del dataset e visualizzazione iniziale\ndata(HolzingerSwineford1939)\nglimpse(HolzingerSwineford1939)\n#&gt; Rows: 301\n#&gt; Columns: 15\n#&gt; $ id     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, …\n#&gt; $ sex    &lt;int&gt; 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2,…\n#&gt; $ ageyr  &lt;int&gt; 13, 13, 13, 13, 12, 14, 12, 12, 13, 12, 12, 12, 12, 12, 12,…\n#&gt; $ agemo  &lt;int&gt; 1, 7, 1, 2, 2, 1, 1, 2, 0, 5, 2, 11, 7, 8, 6, 1, 11, 5, 8, …\n#&gt; $ school &lt;fct&gt; Pasteur, Pasteur, Pasteur, Pasteur, Pasteur, Pasteur, Paste…\n#&gt; $ grade  &lt;int&gt; 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,…\n#&gt; $ x1     &lt;dbl&gt; 3.333, 5.333, 4.500, 5.333, 4.833, 5.333, 2.833, 5.667, 4.5…\n#&gt; $ x2     &lt;dbl&gt; 7.75, 5.25, 5.25, 7.75, 4.75, 5.00, 6.00, 6.25, 5.75, 5.25,…\n#&gt; $ x3     &lt;dbl&gt; 0.375, 2.125, 1.875, 3.000, 0.875, 2.250, 1.000, 1.875, 1.5…\n#&gt; $ x4     &lt;dbl&gt; 2.333, 1.667, 1.000, 2.667, 2.667, 1.000, 3.333, 3.667, 2.6…\n#&gt; $ x5     &lt;dbl&gt; 5.75, 3.00, 1.75, 4.50, 4.00, 3.00, 6.00, 4.25, 5.75, 5.00,…\n#&gt; $ x6     &lt;dbl&gt; 1.2857, 1.2857, 0.4286, 2.4286, 2.5714, 0.8571, 2.8571, 1.2…\n#&gt; $ x7     &lt;dbl&gt; 3.391, 3.783, 3.261, 3.000, 3.696, 4.348, 4.696, 3.391, 4.5…\n#&gt; $ x8     &lt;dbl&gt; 5.75, 6.25, 3.90, 5.30, 6.30, 6.65, 6.20, 5.15, 4.65, 4.55,…\n#&gt; $ x9     &lt;dbl&gt; 6.361, 7.917, 4.417, 4.861, 5.917, 7.500, 4.861, 3.667, 7.3…\n\n# Selezione delle variabili di interesse\nhz &lt;- HolzingerSwineford1939 |&gt;\n  dplyr::select(x1:x9)\n\n# Visualizzazione delle prime 5 righe\nhz |&gt; slice(1:5)\n#&gt;      x1   x2    x3    x4   x5     x6    x7   x8    x9\n#&gt; 1 3.333 7.75 0.375 2.333 5.75 1.2857 3.391 5.75 6.361\n#&gt; 2 5.333 5.25 2.125 1.667 3.00 1.2857 3.783 6.25 7.917\n#&gt; 3 4.500 5.25 1.875 1.000 1.75 0.4286 3.261 3.90 4.417\n#&gt; 4 5.333 7.75 3.000 2.667 4.50 2.4286 3.000 5.30 4.861\n#&gt; 5 4.833 4.75 0.875 2.667 4.00 2.5714 3.696 6.30 5.917\n\n\n26.2.1 Valutazione dei Dati Mancanti\nPrima di esaminare le correlazioni, è importante verificare l’eventuale presenza di dati mancanti, poiché anche pochi valori mancanti possono alterare significativamente la matrice di correlazione.\n\n# Controllo dei dati mancanti\nhz |&gt; summarise(across(everything(), ~ sum(is.na(.))))\n#&gt;   x1 x2 x3 x4 x5 x6 x7 x8 x9\n#&gt; 1  0  0  0  0  0  0  0  0  0\n\nIn questo caso, non sono presenti dati mancanti. In presenza di dati mancanti, sarà necessario adottare strategie appropriate (es. imputazione, listwise deletion, ecc.) prima di procedere.\n\n🔍 Nota didattica: In genere, per l’analisi fattoriale è preferibile utilizzare una matrice di correlazione calcolata su dati completi, perché la presenza di dati mancanti può distorcere le relazioni tra le variabili.\n\n\n26.2.2 Esplorazione delle Distribuzioni\nUn controllo preliminare utile è l’esame della forma delle distribuzioni delle variabili, in particolare asimmetria (skewness) e curtosi (kurtosis), che possono indicare violazioni di normalità.\n\ndescribe(hz)\n#&gt;    vars   n mean   sd median trimmed  mad  min   max range  skew kurtosis\n#&gt; x1    1 301 4.94 1.17   5.00    4.96 1.24 0.67  8.50  7.83 -0.25     0.31\n#&gt; x2    2 301 6.09 1.18   6.00    6.02 1.11 2.25  9.25  7.00  0.47     0.33\n#&gt; x3    3 301 2.25 1.13   2.12    2.20 1.30 0.25  4.50  4.25  0.38    -0.91\n#&gt; x4    4 301 3.06 1.16   3.00    3.02 0.99 0.00  6.33  6.33  0.27     0.08\n#&gt; x5    5 301 4.34 1.29   4.50    4.40 1.48 1.00  7.00  6.00 -0.35    -0.55\n#&gt; x6    6 301 2.19 1.10   2.00    2.09 1.06 0.14  6.14  6.00  0.86     0.82\n#&gt; x7    7 301 4.19 1.09   4.09    4.16 1.10 1.30  7.43  6.13  0.25    -0.31\n#&gt; x8    8 301 5.53 1.01   5.50    5.49 0.96 3.05 10.00  6.95  0.53     1.17\n#&gt; x9    9 301 5.37 1.01   5.42    5.37 0.99 2.78  9.25  6.47  0.20     0.29\n#&gt;      se\n#&gt; x1 0.07\n#&gt; x2 0.07\n#&gt; x3 0.07\n#&gt; x4 0.07\n#&gt; x5 0.07\n#&gt; x6 0.06\n#&gt; x7 0.06\n#&gt; x8 0.06\n#&gt; x9 0.06\n\nIn questo caso, i valori di asimmetria e curtosi sono sufficientemente contenuti, quindi non è necessario trasformare le variabili. Tuttavia, se ci fossero valori molto estremi, una trasformazione (log, z-score, Box-Cox) potrebbe essere utile.\n\n26.2.3 Ispezione della Matrice di Correlazione\nUna matrice di correlazione è la base concettuale dell’analisi fattoriale: essa mostra quanto ogni variabile è associata con le altre. In presenza di gruppi di variabili altamente correlate tra loro e poco correlate con le altre, è plausibile che esistano fattori comuni sottostanti.\nVisualizziamo la matrice utilizzando il pacchetto corrr:\n\ncor_tb &lt;- correlate(hz)\n\ncor_tb |&gt;\n  rearrange() |&gt;\n  rplot(colors = c(\"red\", \"white\", \"blue\"))\n\n\n\n\n\n\n\nIl grafico mostra tre blocchi distinti:\n\n\nx4–x6: un primo gruppo fortemente correlato tra loro,\n\nx1–x3: un secondo gruppo coeso,\n\nx7–x9: un terzo gruppo distinto.\n\nQuesta struttura è coerente con l’idea che i punteggi derivino da tre fattori latenti diversi.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Valutazione della matrice di correlazione</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/01_val_matrici.html#sfericità-di-bartlett",
    "href": "chapters/extraction/01_val_matrici.html#sfericità-di-bartlett",
    "title": "26  Valutazione della matrice di correlazione",
    "section": "\n26.3 Sfericità di Bartlett",
    "text": "26.3 Sfericità di Bartlett\nIl test di sfericità di Bartlett verifica l’ipotesi che il campione provenga da una popolazione in cui le variabili non sono correlate. Formalmente, il test della sfericità di Bartlett verifica l’ipotesi \\(H_0 : \\boldsymbol{R} = \\boldsymbol{I}\\) tramite la formula:\n\\[\n\\chi^2 = -\\bigg[n -1 -\\frac{1}{6} (2p +5)\\bigg] \\ln |\\boldsymbol{R}|,\n\\]\nin cui \\(n\\) è il numero dei soggetti, \\(p\\) il numero delle variabili e \\(|\\boldsymbol{R}|\\) il determinante della matrice di correlazione.\nLa statistica del test di sfericità di Bartlett segue una distribuzione chi-quadro con \\(p(p - 1)/2\\) gradi di libertà. Un valore elevato della statistica indica che la matrice di correlazione R contiene valori di correlazione significativamente diversi da 0. Al contrario, un valore basso della statistica indica che le correlazioni sono basse e non si distinguono da 0.\nIl limite di questo test è che dipende dal numero delle variabili e dalla numerosità del campione, quindi tende a rigettare \\(H_0\\) all’aumentare del campione e del numero delle variabili, anche se le correlazioni sono piccole.\nApplichiamo il test di Bartlet per il dati dell’esempio in discussione.\n\ncor_mat &lt;- cor(hz)\n\nout = cortest.bartlett(R = cor_mat, n = 301)\nprint(out)\n#&gt; $chisq\n#&gt; [1] 904.1\n#&gt; \n#&gt; $p.value\n#&gt; [1] 1.912e-166\n#&gt; \n#&gt; $df\n#&gt; [1] 36\n\nIl risultato del test di Bartlett sui dati HolzingerSwineford1939 indica che esiste una correlazione tra le variabili.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Valutazione della matrice di correlazione</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/01_val_matrici.html#test-di-adeguatezza-campionaria-di-kaiser-meyer-olkin",
    "href": "chapters/extraction/01_val_matrici.html#test-di-adeguatezza-campionaria-di-kaiser-meyer-olkin",
    "title": "26  Valutazione della matrice di correlazione",
    "section": "\n26.4 Test di adeguatezza campionaria di Kaiser-Meyer-Olkin",
    "text": "26.4 Test di adeguatezza campionaria di Kaiser-Meyer-Olkin\nIl test di Kaiser-Meyer-Olkin (KMO) è uno strumento statistico che valuta l’adeguatezza dei dati per l’analisi fattoriale. Esso misura la proporzione di varianza tra le variabili che potrebbe essere attribuita a fattori comuni. Un valore KMO più alto indica una maggiore adattabilità dei dati all’analisi fattoriale.\nLa statistica di adeguatezza campionaria KMO è data da\n\\[\\text{KMO} = \\frac{\\sum_i\\sum_j r^2_{ij}}{\\sum_i\\sum_j r^2_{ij} +\\sum_i\\sum_jp^2_{ij}},\\]\ndove \\(r_{ij}\\) sono le correlazioni osservate e \\(p_{ij}\\) sono le correlazioni parzializzate su tutte le altre. Se le correlazioni parzializzate sono piccole, KMO tende a 1.\nSecondo Kaiser (1970), l’adeguatezza campionaria si valuta nel modo seguente:\n\nda 0.00 a 0.49: inaccettabile\nda 0.50 a 0.59: miserabile\nda 0.60 a 0.69: mediocre\nda 0.70 a 0.79: media\nda 0.80 a 0.89: meritevole\nda 0.90 a 1.00: meravigliosa.\n\nApplichiamo il test KMO ai dati HolzingerSwineford1939.\n\nout = KMO(cor_mat)\nprint(out)\n#&gt; Kaiser-Meyer-Olkin factor adequacy\n#&gt; Call: KMO(r = cor_mat)\n#&gt; Overall MSA =  0.75\n#&gt; MSA for each item = \n#&gt;   x1   x2   x3   x4   x5   x6   x7   x8   x9 \n#&gt; 0.81 0.78 0.73 0.76 0.74 0.81 0.59 0.68 0.79\n\nPer questi dati, il risultato del test KMO indica che l’adeguatezza campionaria è media.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Valutazione della matrice di correlazione</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/01_val_matrici.html#riflessioni-conclusive",
    "href": "chapters/extraction/01_val_matrici.html#riflessioni-conclusive",
    "title": "26  Valutazione della matrice di correlazione",
    "section": "\n26.5 Riflessioni conclusive",
    "text": "26.5 Riflessioni conclusive\nLe analisi preliminari condotte sul dataset HolzingerSwineford1939 indicano che:\n\nle correlazioni tra variabili sono presenti e coerenti con una struttura a più fattori,\nil test di Bartlett conferma che la matrice di correlazione è diversa da una matrice identità,\nl’indice KMO suggerisce un’adeguatezza campionaria soddisfacente, anche se non ottimale.\n\nQuesti risultati ci consentono di procedere con l’analisi fattoriale esplorativa, ma evidenziano anche la necessità di valutazioni critiche in fase interpretativa. L’uso combinato di più strumenti diagnostici consente di fondare l’analisi su basi solide e di trarre conclusioni più affidabili riguardo alla struttura latente dei dati.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Valutazione della matrice di correlazione</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/01_val_matrici.html#session-info",
    "href": "chapters/extraction/01_val_matrici.html#session-info",
    "title": "26  Valutazione della matrice di correlazione",
    "section": "\n26.6 Session Info",
    "text": "26.6 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.2\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] corrr_0.4.4       ggokabeito_0.1.0  see_0.11.0        MASS_7.3-65      \n#&gt;  [5] viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1   \n#&gt;  [9] gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6   \n#&gt; [13] semPlot_1.1.6     lavaan_0.6-19     psych_2.4.12      scales_1.3.0     \n#&gt; [17] markdown_1.13     knitr_1.50        lubridate_1.9.4   forcats_1.0.0    \n#&gt; [21] stringr_1.5.1     dplyr_1.1.4       purrr_1.0.4       readr_2.1.5      \n#&gt; [25] tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0  \n#&gt; [29] here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.1      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.2.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] iterators_1.0.14    mime_0.13           lifecycle_1.0.4    \n#&gt;  [25] pkgconfig_2.0.3     Matrix_1.7-3        R6_2.6.1           \n#&gt;  [28] fastmap_1.2.0       rbibutils_2.3       shiny_1.10.0       \n#&gt;  [31] digest_0.6.37       OpenMx_2.21.13      fdrtool_1.2.18     \n#&gt;  [34] colorspace_2.1-1    rprojroot_2.0.4     Hmisc_5.2-3        \n#&gt;  [37] seriation_1.5.7     labeling_0.4.3      timechange_0.3.0   \n#&gt;  [40] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n#&gt;  [43] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n#&gt;  [46] carData_3.0-5       ggsignif_0.6.4      corpcor_1.6.10     \n#&gt;  [49] gtools_3.9.5        tools_4.4.2         pbivnorm_0.6.0     \n#&gt;  [52] foreign_0.8-88      zip_2.3.2           httpuv_1.6.15      \n#&gt;  [55] nnet_7.3-20         glue_1.8.0          quadprog_1.5-8     \n#&gt;  [58] nlme_3.1-167        promises_1.3.2      lisrelToR_0.3      \n#&gt;  [61] grid_4.4.2          checkmate_2.3.2     cluster_2.1.8.1    \n#&gt;  [64] reshape2_1.4.4      generics_0.1.3      gtable_0.3.6       \n#&gt;  [67] tzdb_0.5.0          ca_0.71.1           data.table_1.17.0  \n#&gt;  [70] hms_1.1.3           car_3.1-3           sem_3.1-16         \n#&gt;  [73] foreach_1.5.2       pillar_1.10.1       rockchalk_1.8.157  \n#&gt;  [76] later_1.4.1         splines_4.4.2       lattice_0.22-6     \n#&gt;  [79] survival_3.8-3      kutils_1.73         tidyselect_1.2.1   \n#&gt;  [82] registry_0.5-1      miniUI_0.1.1.1      pbapply_1.7-2      \n#&gt;  [85] reformulas_0.4.0    stats4_4.4.2        xfun_0.51          \n#&gt;  [88] qgraph_1.9.8        arm_1.14-4          stringi_1.8.4      \n#&gt;  [91] yaml_2.3.10         pacman_0.5.1        boot_1.3-31        \n#&gt;  [94] evaluate_1.0.3      codetools_0.2-20    mi_1.1             \n#&gt;  [97] cli_3.6.4           RcppParallel_5.1.10 rpart_4.1.24       \n#&gt; [100] xtable_1.8-4        Rdpack_2.6.3        munsell_0.5.1      \n#&gt; [103] Rcpp_1.0.14         coda_0.19-4.1       png_0.1-8          \n#&gt; [106] XML_3.99-0.18       parallel_4.4.2      jpeg_0.1-10        \n#&gt; [109] lme4_1.1-36         mvtnorm_1.3-3       openxlsx_4.2.8     \n#&gt; [112] rlang_1.1.5         TSP_1.2-4           multcomp_1.4-28    \n#&gt; [115] mnormt_2.1.1\n\n\n\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Valutazione della matrice di correlazione</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html",
    "href": "chapters/extraction/02_estrazione.html",
    "title": "27  L’estrazione dei fattori",
    "section": "",
    "text": "27.1 Introduzione\nL’analisi fattoriale è una tecnica statistica multivariata utilizzata per identificare strutture latenti, ovvero fattori non osservabili, che spiegano le correlazioni tra variabili osservate.\nIn altre parole, quando abbiamo molte variabili (es. item di un questionario), l’analisi fattoriale cerca di scoprire quali gruppi di item misurano lo stesso costrutto psicologico sottostante, riducendo così la complessità dei dati.\nQuesta tecnica è particolarmente utile nelle scienze sociali e in psicologia, dove costrutti astratti come intelligenza, ansia o autostima non possono essere misurati direttamente, ma solo attraverso più item. L’analisi fattoriale aiuta a verificare se questi item misurano effettivamente un numero limitato di costrutti sottostanti, rendendo i dati più interpretabili.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html#introduzione",
    "href": "chapters/extraction/02_estrazione.html#introduzione",
    "title": "27  L’estrazione dei fattori",
    "section": "",
    "text": "27.1.1 Il modello statistico dell’analisi fattoriale\nIl modello matematico alla base dell’analisi fattoriale si esprime così:\n\\[\n\\boldsymbol{\\Sigma} = \\boldsymbol{\\Lambda} \\boldsymbol{\\Phi} \\boldsymbol{\\Lambda}^{\\mathsf{T}} + \\boldsymbol{\\Psi},\n\\]\ndove:\n\n\n\\(\\boldsymbol{\\Sigma}\\) è la matrice delle covarianze osservate tra le variabili;\n\n\\(\\boldsymbol{\\Lambda}\\) è la matrice dei carichi fattoriali: ciascun elemento rappresenta l’intensità della relazione tra una variabile osservata e un fattore latente;\n\n\\(\\boldsymbol{\\Phi}\\) è la matrice delle correlazioni tra i fattori (è l’identità se i fattori sono ortogonali, cioè non correlati);\n\n\\(\\boldsymbol{\\Psi}\\) è una matrice diagonale contenente le unicità (la porzione di varianza di ciascuna variabile che non è spiegata dai fattori comuni).\n\nQuesto modello riflette l’idea che ogni variabile osservata sia influenzata da uno o più fattori comuni, più una componente specifica e casuale (l’unicità).",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html#metodo-delle-componenti-principali",
    "href": "chapters/extraction/02_estrazione.html#metodo-delle-componenti-principali",
    "title": "27  L’estrazione dei fattori",
    "section": "\n27.3 Metodo delle Componenti Principali",
    "text": "27.3 Metodo delle Componenti Principali\n\n❗ Nota: Il metodo delle componenti principali (Principal Component Analysis, PCA) viene spesso utilizzato per ridurre la dimensionalità di un insieme di variabili osservate. Tuttavia, non è propriamente un metodo fattoriale: non distingue tra varianza comune e specifica e non assume un modello latente sottostante, come invece fa l’analisi fattoriale vera e propria.\n\n\n27.3.1 Obiettivo del metodo\nLa PCA cerca un insieme ridotto di componenti principali, che siano:\n\n\ncombinazioni lineari delle variabili osservate;\n\nortogonali (cioè non correlate tra loro);\ncapaci di spiegare la massima varianza possibile nei dati.\n\n27.3.2 Teorema spettrale e scomposizione della matrice\nIl punto di partenza teorico è il teorema spettrale, che afferma che ogni matrice simmetrica (come una matrice di correlazione \\(\\mathbf{R}\\)) può essere scomposta nella forma:\n\\[\n\\mathbf{R} = \\mathbf{C} \\mathbf{D} \\mathbf{C}^{\\mathsf{T}},\n\\]\ndove:\n\n\n\\(\\mathbf{C}\\) è una matrice ortogonale i cui vettori colonna sono gli autovettori di \\(\\mathbf{R}\\);\n\n\\(\\mathbf{D}\\) è una matrice diagonale contenente gli autovalori di \\(\\mathbf{R}\\);\n\n\\(\\mathbf{C}^{\\mathsf{T}}\\) è la trasposta di \\(\\mathbf{C}\\).\n\nQuesta è detta scomposizione spettrale di \\(\\mathbf{R}\\).\n\n27.3.3 Ricavare le saturazioni fattoriali\nA questo punto, vogliamo costruire una matrice di saturazioni fattoriali \\(\\hat{\\boldsymbol{\\Lambda}}\\) che, moltiplicata per la sua trasposta, approssimi la matrice \\(\\mathbf{R}\\):\n\\[\n\\mathbf{R} \\approx \\hat{\\boldsymbol{\\Lambda}} \\hat{\\boldsymbol{\\Lambda}}^\\mathsf{T}.\n\\]\nPer ottenere ciò, osserviamo che, poiché \\(\\mathbf{D}\\) è diagonale, essa può essere scritta come prodotto di due matrici diagonali identiche, che hanno sulla diagonale principale le radici quadrate degli autovalori:\n\\[\n\\mathbf{D} = \\mathbf{D}^{1/2} \\cdot \\mathbf{D}^{1/2}.\n\\]\nQuindi possiamo riscrivere la decomposizione spettrale come:\n\\[\n\\mathbf{R} = \\mathbf{C} \\mathbf{D}^{1/2} \\cdot \\mathbf{D}^{1/2} \\mathbf{C}^{\\mathsf{T}}.\n\\]\nOra definiamo:\n\\[\n\\hat{\\boldsymbol{\\Lambda}} = \\mathbf{C} \\mathbf{D}^{1/2},\n\\]\ne otteniamo:\n\\[\n\\hat{\\boldsymbol{\\Lambda}} \\hat{\\boldsymbol{\\Lambda}}^\\mathsf{T} = \\mathbf{C} \\mathbf{D}^{1/2} \\cdot (\\mathbf{D}^{1/2} \\mathbf{C}^{\\mathsf{T}}) = \\mathbf{C} \\mathbf{D} \\mathbf{C}^{\\mathsf{T}} = \\mathbf{R}.\n\\]\n\nLe saturazioni fattoriali nella PCA si ottengono moltiplicando ciascun autovettore per la radice quadrata del corrispondente autovalore perché ciò permette di ricostruire esattamente la matrice di correlazione \\(\\mathbf{R}\\).\n\nOgni elemento \\(l_{ij}\\) della matrice \\(\\hat{\\boldsymbol{\\Lambda}}\\) indica quanto la variabile \\(i\\) contribuisce alla componente \\(j\\).\nMaggiore è il valore assoluto di una saturazione, più forte è l’associazione tra la variabile e la componente.\nOvviamente, la semplice applicazione del teorema spettrale non può essere utilizzata direttamente, perché in questo modo avremmo semplicemente replicato i dati osservati, senza una riduzione di dimensionalità. Ma quando i primi \\(k \\leq p\\) autovettori spiegano una quota considerevole della varianza totale, allora, utilizzando solo \\(k\\) autovettori possiamo ottenere una soluzione fattoriale che spiega la maggior parte della varianza totale.\n\n27.3.4 Interpretazione\n\nGli autovalori indicano quanta varianza è spiegata da ciascuna componente.\nLe componenti principali sono combinazioni lineari delle variabili originali che massimizzano la varianza.\n\n27.3.5 Limiti per l’analisi fattoriale\n\nNon separa la varianza specifica da quella comune.\nNon ammette interpretazione in termini di fattori latenti.\nNon prevede un test di bontà dell’adattamento del modello.\n\n27.3.6 Quando usarlo\nIl metodo PCA è utile per riduzione della dimensionalità, ad esempio per creare punteggi compositi o visualizzare dati complessi. Non è però adeguato per esplorare strutture latenti teoriche (es. costrutti psicologici).",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html#metodo-dei-fattori-principali",
    "href": "chapters/extraction/02_estrazione.html#metodo-dei-fattori-principali",
    "title": "27  L’estrazione dei fattori",
    "section": "\n27.4 Metodo dei Fattori Principali",
    "text": "27.4 Metodo dei Fattori Principali\nIl metodo dei fattori principali (principal factor method) si differenzia dalla PCA perché considera solo la varianza comune tra le variabili, escludendo la varianza specifica e l’errore.\n\n27.4.1 Procedura\n\nSi stima la comunalità iniziale di ogni variabile (cioè la quota di varianza spiegata da fattori comuni).\nSi sostituiscono le varianze totali sulla diagonale della matrice \\(\\mathbf{R}\\) con le comunalità stimate.\nSi esegue la decomposizione spettrale sulla nuova matrice per ottenere autovettori e autovalori.\nSi costruisce la matrice dei carichi fattoriali.\n\n27.4.2 Stima delle comunalità iniziali\nPuò essere fatta, ad esempio:\n\nprendendo il massimo quadrato della correlazione della variabile con le altre;\noppure il \\(R^2\\) da una regressione multipla della variabile sulle altre.\n\n27.4.3 Vantaggi\n\nTiene conto della specificità delle variabili.\nÈ più coerente con il modello fattoriale classico.\n\n27.4.4 Limiti\n\nI risultati dipendono fortemente dalla stima iniziale delle comunalità.\nNon permette test di bontà dell’adattamento del modello.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html#metodo-dei-fattori-principali-iterato",
    "href": "chapters/extraction/02_estrazione.html#metodo-dei-fattori-principali-iterato",
    "title": "27  L’estrazione dei fattori",
    "section": "\n27.5 Metodo dei Fattori Principali Iterato",
    "text": "27.5 Metodo dei Fattori Principali Iterato\nQuesto metodo affina il precedente aggiornando iterativamente le comunalità:\n\nsi calcolano carichi fattoriali e comunalità iniziali;\nsi sostituiscono le nuove comunalità nella diagonale;\nsi ripete la procedura finché i valori convergono.\n\n\n27.5.1 Vantaggi\n\nFornisce stime più stabili delle comunalità.\nMigliora la qualità della rappresentazione se la struttura è forte.\n\n27.5.2 Limiti\n\nPuò generare soluzioni improprie (es. comunalità &gt; 1: problemi di Heywood).\nNon offre criteri interni per la scelta del numero di fattori.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html#metodo-di-massima-verosimiglianza",
    "href": "chapters/extraction/02_estrazione.html#metodo-di-massima-verosimiglianza",
    "title": "27  L’estrazione dei fattori",
    "section": "\n27.5 Metodo di Massima Verosimiglianza",
    "text": "27.5 Metodo di Massima Verosimiglianza\nIl metodo di massima verosimiglianza è particolarmente indicato quando si può ragionevolmente assumere che le variabili manifeste seguano una distribuzione normale multivariata. In tali condizioni, il metodo produce stime dei pesi fattoriali e delle specificità che sono quelle più verosimili date le correlazioni osservate. Questo metodo è spesso preferito rispetto ad altri, a patto che le sue ipotesi di base siano pienamente soddisfatte.\nIl metodo di massima verosimiglianza minimizza una funzione di discrepanza \\(F\\), che misura la distanza tra la matrice di covarianze osservata \\(\\textbf{S}\\) (o la matrice di correlazioni \\(\\textbf{R}\\)) e quella predetta dal modello \\(\\textbf{M}\\). La funzione \\(F\\) può essere espressa come:\n\\[\nF(\\boldsymbol{\\Lambda}, \\boldsymbol{\\Psi}) = \\text{discrepanza tra } \\textbf{S} \\text{ e } \\textbf{M}.\n\\]\nMinimizzando \\(F\\), uguagliando a zero le derivate di \\(F\\) rispetto ai parametri del modello \\(\\boldsymbol{\\Lambda}\\) (pesi fattoriali) e \\(\\boldsymbol{\\Psi}\\) (specificità), si ottengono le equazioni per le stime di massima verosimiglianza:\n\\[\n\\hat{\\boldsymbol{\\Lambda}}, \\hat{\\boldsymbol{\\Psi}} .\n\\]\nQueste equazioni non hanno una soluzione analitica diretta, quindi si ricorre a metodi numerici iterativi per trovare le stime dei parametri. Durante l’iterazione, si cerca di minimizzare la discrepanza tra la matrice osservata e quella predetta dal modello.\n\n27.5.1 Caratteristiche del Metodo\n\nPrecisione delle stime:\nSe le ipotesi sono rispettate, le stime di massima verosimiglianza sono asintoticamente efficienti, ossia hanno la minima varianza possibile tra gli stimatori.\nIndipendenza dall’unità di misura:\nLa soluzione non dipende dall’unità di misura delle variabili manifeste. Questo significa che si ottiene la stessa soluzione analizzando la matrice di covarianze \\(\\textbf{S}\\) o quella di correlazioni \\(\\textbf{R}\\).\nTest di bontà di adattamento:\nLe stime di massima verosimiglianza consentono di eseguire un test chi-quadrato per valutare se la matrice predetta dal modello è coerente con quella osservata. Questo test offre una misura formale della bontà di adattamento del modello ai dati.\n\nLimitazioni:\n\n\nProblemi di convergenza: In alcuni casi, il processo iterativo può non convergere, specialmente con dati problematici o ipotesi non rispettate.\n\nCasi di Heywood: Analogamente al metodo dei fattori principali iterato, possono verificarsi casi di Heywood in cui alcune comunalità stimate risultano maggiori di 1.\n\n\n\n27.5.2 Applicazione Pratica\nPer calcolare le stime di massima verosimiglianza in R, è possibile utilizzare la funzione factanal(), che implementa questo metodo direttamente.\nConsideriamo i dati dell’esempio precedente e calcoliamo i parametri di massima verosimiglianza:\n\nfactanal(\n    covmat = R,        # Matrice di correlazioni\n    factors = 2,       # Numero di fattori\n    rotation = \"none\", # Nessuna rotazione\n    n.obs = 225        # Numero di osservazioni\n)\n#&gt; \n#&gt; Call:\n#&gt; factanal(factors = 2, covmat = R, n.obs = 225, rotation = \"none\")\n#&gt; \n#&gt; Uniquenesses:\n#&gt;     K     I     H     L     J \n#&gt; 0.005 0.268 0.055 0.008 0.005 \n#&gt; \n#&gt; Loadings:\n#&gt;   Factor1 Factor2\n#&gt; K  0.955  -0.289 \n#&gt; I  0.528   0.673 \n#&gt; H  0.720  -0.653 \n#&gt; L  0.954  -0.287 \n#&gt; J  0.764   0.642 \n#&gt; \n#&gt;                Factor1 Factor2\n#&gt; SS loadings      3.203   1.457\n#&gt; Proportion Var   0.641   0.291\n#&gt; Cumulative Var   0.641   0.932\n#&gt; \n#&gt; Test of the hypothesis that 2 factors are sufficient.\n#&gt; The chi square statistic is 648.1 on 1 degree of freedom.\n#&gt; The p-value is 5.81e-143\n\n\nPesi fattoriali:\nI pesi fattoriali stimati \\(\\hat{\\boldsymbol{\\Lambda}}\\) saranno molto simili a quelli ottenuti con il metodo dei fattori principali iterato.\nSpecificità:\nLe specificità \\(\\hat{\\boldsymbol{\\Psi}}\\) rifletteranno la porzione di varianza non spiegata dai fattori comuni.\nTest di bontà di adattamento:\nIl risultato include un test chi-quadrato che valuta la coerenza tra il modello e i dati osservati. Un valore di p elevato indica un buon adattamento del modello.\n\n27.5.3 Confronto con Altri Metodi\nIl metodo di massima verosimiglianza si distingue per la sua capacità di fornire stime ottimali e una valutazione formale dell’adattamento del modello. Tuttavia, richiede che le variabili manifeste seguano una distribuzione normale multivariata, il che potrebbe non essere sempre soddisfatto nei dati reali. Quando le ipotesi di normalità sono violate, metodi alternativi come la stima mediante minimi quadrati ponderati (WLS) potrebbero essere più appropriati.\nIn conclusione, il metodo di massima verosimiglianza rappresenta uno standard per l’analisi fattoriale, offrendo stime efficienti e strumenti per valutare l’adattamento del modello ai dati. Tuttavia, l’efficacia del metodo dipende dalla validità delle ipotesi di normalità e dalla qualità dei dati utilizzati.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html#riflessioni-conclusive",
    "href": "chapters/extraction/02_estrazione.html#riflessioni-conclusive",
    "title": "27  L’estrazione dei fattori",
    "section": "\n27.9 Riflessioni Conclusive",
    "text": "27.9 Riflessioni Conclusive\nL’analisi fattoriale non è una procedura automatica, ma un processo di modellizzazione che richiede scelte motivate e valutazioni critiche a ogni passaggio. I metodi di estrazione dei fattori, pur essendo matematicamente diversi, riflettono concezioni differenti del ruolo della varianza nelle variabili osservate e, quindi, diverse filosofie di ricerca.\nAd esempio, la PCA tende a trattare le variabili come manifestazioni dirette della varianza totale, rendendola utile per scopi pratici come la riduzione della dimensionalità, ma meno adatta per inferenze teoriche su costrutti latenti. Al contrario, i metodi che stimano la varianza comune (come i fattori principali o la massima verosimiglianza) assumono che ci siano cause sottostanti e non osservabili che generano le covarianze tra le variabili.\nMa oltre agli aspetti tecnici, è utile considerare alcune domande chiave che dovrebbero guidare la scelta del metodo:\n\nQual è il mio obiettivo? Sintesi descrittiva, conferma di ipotesi teoriche, preparazione a un’analisi fattoriale confermativa?\nI dati rispettano le assunzioni richieste (normalità, dimensione del campione, struttura semplice)?\nQuanto voglio spingermi nell’interpretazione psicologica dei fattori?\n\nQuanto è affidabile la mia stima della varianza specifica o dell’errore di misura?\n\nInoltre, è importante non trascurare che l’analisi fattoriale, per quanto potente, è sensibile a molte scelte analitiche: dal numero di fattori estratti, al metodo di rotazione, fino alle decisioni su quali variabili includere o escludere. Ogni decisione ha impatto sull’interpretabilità, sulla stabilità delle soluzioni e sulla replicabilità dei risultati.\n\nUn buon analista fattoriale non cerca solo di “ottenere carichi elevati”, ma si interroga su cosa quei carichi rappresentano, se sono coerenti con la teoria, e se possono essere replicati in un altro campione.\n\nInfine, non dimentichiamo che l’analisi fattoriale non esaurisce l’indagine sulla struttura latente dei dati. È spesso il primo passo di un percorso più ampio che può includere:\n\n\nanalisi fattoriale confermativa (CFA);\n\nmodelli strutturali (SEM);\n\nanalisi di validità di costrutto.\n\nIn sintesi, padroneggiare i diversi metodi di estrazione non significa solo saperli applicare, ma anche comprendere le implicazioni epistemologiche e psicometriche delle scelte fatte. Il vero valore dell’analisi fattoriale non sta solo nella sintesi dei dati, ma nella sua capacità di collegare numeri e teoria, variabili osservate e costrutti latenti, statistica e psicologia.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/extraction/02_estrazione.html#informazioni-sullambiente-di-sviluppo",
    "title": "27  L’estrazione dei fattori",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.4\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] GPArotation_2024.3-1 ggokabeito_0.1.0     see_0.11.0          \n#&gt;  [4] MASS_7.3-65          viridis_0.6.5        viridisLite_0.4.2   \n#&gt;  [7] ggpubr_0.6.0         ggExtra_0.10.1       gridExtra_2.3       \n#&gt; [10] patchwork_1.3.0      bayesplot_1.11.1     semTools_0.5-7      \n#&gt; [13] semPlot_1.1.6        lavaan_0.6-19        psych_2.5.3         \n#&gt; [16] scales_1.3.0         markdown_2.0         knitr_1.50          \n#&gt; [19] lubridate_1.9.4      forcats_1.0.0        stringr_1.5.1       \n#&gt; [22] dplyr_1.1.4          purrr_1.0.4          readr_2.1.5         \n#&gt; [25] tidyr_1.3.1          tibble_3.2.1         ggplot2_3.5.2       \n#&gt; [28] tidyverse_2.0.0      here_1.0.1          \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_2.0.0      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.2.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.8         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.11.0      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] mime_0.13           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [25] Matrix_1.7-3        R6_2.6.1            fastmap_1.2.0      \n#&gt;  [28] rbibutils_2.3       shiny_1.10.0        numDeriv_2016.8-1.1\n#&gt;  [31] digest_0.6.37       OpenMx_2.21.13      fdrtool_1.2.18     \n#&gt;  [34] colorspace_2.1-1    rprojroot_2.0.4     Hmisc_5.2-3        \n#&gt;  [37] timechange_0.3.0    abind_1.4-8         compiler_4.4.2     \n#&gt;  [40] withr_3.0.2         glasso_1.11         htmlTable_2.4.3    \n#&gt;  [43] backports_1.5.0     carData_3.0-5       ggsignif_0.6.4     \n#&gt;  [46] corpcor_1.6.10      gtools_3.9.5        tools_4.4.2        \n#&gt;  [49] pbivnorm_0.6.0      foreign_0.8-90      zip_2.3.2          \n#&gt;  [52] httpuv_1.6.15       nnet_7.3-20         glue_1.8.0         \n#&gt;  [55] quadprog_1.5-8      nlme_3.1-168        promises_1.3.2     \n#&gt;  [58] lisrelToR_0.3       grid_4.4.2          checkmate_2.3.2    \n#&gt;  [61] cluster_2.1.8.1     reshape2_1.4.4      generics_0.1.3     \n#&gt;  [64] gtable_0.3.6        tzdb_0.5.0          data.table_1.17.0  \n#&gt;  [67] hms_1.1.3           car_3.1-3           sem_3.1-16         \n#&gt;  [70] pillar_1.10.2       rockchalk_1.8.157   later_1.4.2        \n#&gt;  [73] splines_4.4.2       lattice_0.22-7      survival_3.8-3     \n#&gt;  [76] kutils_1.73         tidyselect_1.2.1    miniUI_0.1.1.1     \n#&gt;  [79] pbapply_1.7-2       reformulas_0.4.0    stats4_4.4.2       \n#&gt;  [82] xfun_0.52           qgraph_1.9.8        arm_1.14-4         \n#&gt;  [85] stringi_1.8.7       pacman_0.5.1        boot_1.3-31        \n#&gt;  [88] evaluate_1.0.3      codetools_0.2-20    mi_1.1             \n#&gt;  [91] cli_3.6.4           RcppParallel_5.1.10 rpart_4.1.24       \n#&gt;  [94] xtable_1.8-4        Rdpack_2.6.4        munsell_0.5.1      \n#&gt;  [97] Rcpp_1.0.14         coda_0.19-4.1       png_0.1-8          \n#&gt; [100] XML_3.99-0.18       parallel_4.4.2      jpeg_0.1-11        \n#&gt; [103] lme4_1.1-37         mvtnorm_1.3-3       openxlsx_4.2.8     \n#&gt; [106] rlang_1.1.5         multcomp_1.4-28     mnormt_2.1.1\n\n\n\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html",
    "href": "chapters/extraction/03_numero_fattori.html",
    "title": "28  Determinare il numero dei fattori",
    "section": "",
    "text": "28.1 Introduzione\nL’Analisi Fattoriale Esplorativa (EFA) è uno strumento fondamentale nella costruzione e valutazione di test psicologici. Essa consente di esplorare la struttura latente sottostante a un insieme di variabili osservate. Uno degli aspetti più delicati dell’EFA è la determinazione del numero di fattori da estrarre, nota anche come factor retention. Una scelta errata può compromettere l’intera analisi, portando a:",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Determinare il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#introduzione",
    "href": "chapters/extraction/03_numero_fattori.html#introduzione",
    "title": "28  Determinare il numero dei fattori",
    "section": "",
    "text": "Sottofattorizzazione: sottostimare il numero di fattori, rischiando di distorcere la struttura latente e generare caricamenti spurii (cross-loadings);\n\nSovrafattorizzazione: estrarre più fattori del necessario, introducendo elementi poco interpretabili. Questo errore è meno grave, purché venga riconosciuto e gestito opportunamente.\n\n\n28.1.1 Tre domande fondamentali sulla dimensionalità di un test\n\n\nQuante dimensioni? Alcuni test misurano un solo costrutto latente, altri ne misurano diversi.\n\nLe dimensioni sono correlate? Se sì, è opportuno utilizzare metodi di rotazione obliqua.\n\nCosa significano le dimensioni? L’interpretazione teorica dei fattori è cruciale per l’applicazione pratica del test.\n\nQuesto capitolo presenta e confronta i principali approcci proposti per determinare il numero ottimale di fattori, illustrandone i presupposti, i punti di forza e le limitazioni.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Determinare il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#tre-domande-sulla-dimensionalità-del-test",
    "href": "chapters/extraction/03_numero_fattori.html#tre-domande-sulla-dimensionalità-del-test",
    "title": "28  Il numero dei fattori",
    "section": "28.2 Tre domande sulla dimensionalità del test",
    "text": "28.2 Tre domande sulla dimensionalità del test\nSono presenti almeno tre questioni rilevanti riguardo la dimensionalità di un test.\n\nNumero di Dimensioni:\n\nLa prima questione riguarda il numero di dimensioni rifletto dagli item del test. Alcuni test riflettono una sola dimensione, mentre altri ne riflettono due o più. Questa questione è importante poiché ogni dimensione del test è probabile che venga valutata separatamente, necessitando ciascuna una propria analisi psicometrica.\n\nCorrelazione tra Dimensioni:\n\nLa seconda questione indaga se, in un test con più di una dimensione, queste dimensioni siano correlate tra loro. Alcuni test presentano diverse dimensioni che sono in qualche modo correlate, mentre altri hanno dimensioni essenzialmente indipendenti e non correlate. Questa questione è rilevante, in parte, perché la natura delle associazioni tra le dimensioni di un test ha implicazioni per la significatività del “punteggio totale” del test.\n\nNatura delle Dimensioni:\n\nLa terza questione si chiede quali sono le dimensioni in un test con più di una dimensione, ovvero, quali attributi psicologici sono riflessi dalle dimensioni del test? Ad esempio, nel test della personalità con sei aggettivi descritto precedentemente, la prima dimensione riflette l’attributo psicologico dell’estroversione o qualche altro attributo? L’importanza di questa questione è evidente: per valutare ed interpretare efficacemente una dimensione di un test, è necessario comprendere il significato psicologico del punteggio.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#metodi-basati-sugli-autovalori",
    "href": "chapters/extraction/03_numero_fattori.html#metodi-basati-sugli-autovalori",
    "title": "28  Determinare il numero dei fattori",
    "section": "\n28.2 Metodi basati sugli autovalori",
    "text": "28.2 Metodi basati sugli autovalori\n\n28.2.1 Kaiser-Guttman Rule\nRetiene i fattori con autovalore &gt; 1 (solo per matrici di correlazione). Anche se molto diffuso, è fortemente sconsigliato: ha una tendenza sistematica alla sovrafattorizzazione. È inappropriato per dati non standardizzati o con bassa comunalità.\n\n28.2.2 Scree Test (Cattell)\nGrafico degli autovalori decrescenti: il numero di fattori corrisponde al punto prima del “gomito”. Soggettivo e a bassa affidabilità.\n\n28.2.3 Valore medio degli autovalori\nRetiene i fattori con autovalore maggiore della media degli autovalori. Alternativa alla regola di Kaiser, ma ancora euristica.\n\n28.2.4 Metodi Avanzati\n\n\nEmpirical Kaiser Criterion (EKC): corregge per numerosità campionaria e varianza cumulata. Buona performance solo in strutture semplici e unidimensionali.\n\nSTOC e STAF: automatizzano lo scree test via algoritmi (es. accelerazione della pendenza) – implementate in nFactors.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Determinare il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#metodi-basati-sul-confronto-tra-modelli",
    "href": "chapters/extraction/03_numero_fattori.html#metodi-basati-sul-confronto-tra-modelli",
    "title": "28  Il numero dei fattori",
    "section": "28.4 Metodi basati sul confronto tra modelli",
    "text": "28.4 Metodi basati sul confronto tra modelli\nIl confronto tra modelli può essere eseguito usando varie statistiche. Una scelta popolare per stimare il numero di fattori nella EFA è il Criterio d’Informazione Bayesiano (BIC; Schwarz, 1978), introdotto come un miglioramento rispetto al Criterio d’Informazione di Akaike (AIC; Akaike, 1973). Un’alternativa è l’indice RMSEA, che può essere considerato come una stima della mancanza di adattamento che tiene in considerazione i gradi di libertà del modello (Browne e Cudeck, 1992). Un altro metodo di questo tipo è il test Minimum Average Partial (MAP), che stima le correlazioni parziali residue medie per diversi numeri di fattori e seleziona quello con il valore più basso (Velicer, 1976).\n\n28.4.1 Test del rapporto di verosimiglianze\nIn questo test si confrontano due ipotesi: l’ipotesi nulla \\(H_0\\) e l’ipotesi alternativa \\(H_1\\), per valutare la bontà di adattamento di un modello fattoriale a una matrice di covarianza delle variabili oggetto di osservazione campionaria \\(Y\\). L’ipotesi nulla postula che la struttura di interdipendenza di \\(Y\\) può essere spiegata da \\(m\\) fattori comuni, mentre l’alternativa postula che i fattori comuni non sono sufficienti per spiegare la matrice di covarianza \\(\\boldsymbol{\\Sigma}\\).\nIl test si basa sulla statistica del chi-quadrato con gradi di libertà pari a \\(\\nu = \\frac{1}{2}[(p-m)^2-(p-m)]\\), dove \\(p\\) è il numero di variabili e \\(m\\) è il numero di fattori. In pratica, si inizia con \\(m^*=1\\) e si valuta l’ipotesi \\(H_0\\) per \\(m^*\\). Se \\(H_0\\) non viene rifiutata, il procedimento si arresta. In caso contrario, si considera \\(m^*+1\\) e si ripete il test finché \\(H_0\\) viene accettata o finché si raggiunge il valore minimo di gradi di libertà pari a zero.\nIl test del rapporto di verosimiglianze è particolarmente indicato quando il numero di osservazioni è grande, ma la sua applicazione è limitata dalle dimensioni del campione. In alternativa, è possibile utilizzare gli indici AIC, BIC e RMSEA per scegliere la soluzione con il valore più piccolo di tali statistiche. Tuttavia, questi indici non forniscono un test statistico per il confronto tra modelli.\nIn pratica, si può considerare il valore \\(m\\) indicato dal test come il limite superiore del numero di fattori che sono importanti dal punto di vista pratico.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#minimizzazione-dellout-of-sample-prediction-error",
    "href": "chapters/extraction/03_numero_fattori.html#minimizzazione-dellout-of-sample-prediction-error",
    "title": "28  Il numero dei fattori",
    "section": "28.5 Minimizzazione dell’out-of-sample prediction error",
    "text": "28.5 Minimizzazione dell’out-of-sample prediction error\nRecentemente è stato proposto un nuovo metodo per stimare il numero di fattori in EFA che affronta il problema come un problema di selezione del modello Haslbeck & Bork (2022). L’obiettivo è confrontare i modelli con 1, 2, …, p fattori, dove \\(p\\) è il numero di variabili, e selezionare il modello con l’errore di previsione atteso più basso nella popolazione. Tuttavia, questo è un compito non banale perché il modello che minimizza l’errore di previsione nel campione non minimizza sempre l’errore di previsione nella popolazione.\nIntuitivamente, questo problema viene affrontato suddividendo il campione di dati in due insiemi: un set di training e un set di test. Il set di training viene utilizzato per stimare i parametri del modello, le cui previsioni vengono poi verificate utilizzando i dati di test (non utilizzati per la stima dei parametri). Questo calcolo dell’errore di previsione fuori campione viene ripetuto diverse volte, suddividendo ogni volta in modo casuale il campione negli insiemi di training e test. Tale metodo per stimare il numero di fattori è implementato nel pacchetto R fspe.\nEsempio. Per confrontare i metodi discussi per la scelta del numero \\(m\\) di fattori usiamo una matrice di correlazioni calcolata sulle sottoscale della WAIS. Le 11 sottoscale del test sono le seguenti:\n\nX1 = Information\nX2 = Comprehension\nX3 = Arithmetic\nX4 = Similarities\nX5 = Digit.span\nX6 = Vocabulary\nX7 = Digit.symbol\nX8 = Picture.completion\nX9 = Block.design\nX10 = Picture.arrangement\nX11 = Object.\n\nI dati sono stati ottenuti dal manuale della III edizione.\n\nvarnames &lt;- c(\n    \"IN\", \"CO\", \"AR\", \"SI\", \"DS\", \"VO\", \"SY\", \"PC\",\n    \"BD\", \"PA\", \"OA\", \"AG\", \"ED\"\n)\ntemp &lt;- matrix(c(\n    1, 0.67, 0.62, 0.66, 0.47, 0.81, 0.47, 0.60, 0.49, 0.51, 0.41,\n    -0.07, 0.66, .67, 1, 0.54, 0.60, 0.39, 0.72, 0.40, 0.54, 0.45,\n    0.49, 0.38, -0.08, 0.52, .62, .54, 1, 0.51, 0.51, 0.58, 0.41,\n    0.46, 0.48, 0.43, 0.37, -0.08, 0.49, .66, .60, .51, 1, 0.41,\n    0.68, 0.49, 0.56, 0.50, 0.50, 0.41, -0.19, 0.55, .47, .39, .51,\n    .41, 1, 0.45, 0.45, 0.42, 0.39, 0.42, 0.31, -0.19, 0.43,\n    .81, .72, .58, .68, .45, 1, 0.49, 0.57, 0.46, 0.52, 0.40, -0.02,\n    0.62, .47, .40, .41, .49, .45, .49, 1, 0.50, 0.50, 0.52, 0.46,\n    -0.46, 0.57, .60, .54, .46, .56, .42, .57, .50, 1, 0.61, 0.59,\n    0.51, -0.28, 0.48, .49, .45, .48, .50, .39, .46, .50, .61, 1,\n    0.54, 0.59, -0.32, 0.44, .51, .49, .43, .50, .42, .52, .52, .59,\n    .54, 1, 0.46, -0.37, 0.49, .41, .38, .37, .41, .31, .40, .46, .51,\n    .59, .46, 1, -0.28, 0.40, -.07, -.08, -.08, -.19, -.19, -.02,\n    -.46, -.28, -.32, -.37, -.28, 1, -0.29, .66, .52, .49, .55, .43,\n    .62, .57, .48, .44, .49, .40, -.29, 1\n), nrow = 13, ncol = 13, byrow = TRUE)\n\ncolnames(temp) &lt;- varnames\nrownames(temp) &lt;- varnames\n\nwais_cor &lt;- temp[1:11, 1:11]\nwais_cor\n\n\nA matrix: 11 x 11 of type dbl\n\n\n\nIN\nCO\nAR\nSI\nDS\nVO\nSY\nPC\nBD\nPA\nOA\n\n\n\n\nIN\n1.00\n0.67\n0.62\n0.66\n0.47\n0.81\n0.47\n0.60\n0.49\n0.51\n0.41\n\n\nCO\n0.67\n1.00\n0.54\n0.60\n0.39\n0.72\n0.40\n0.54\n0.45\n0.49\n0.38\n\n\nAR\n0.62\n0.54\n1.00\n0.51\n0.51\n0.58\n0.41\n0.46\n0.48\n0.43\n0.37\n\n\nSI\n0.66\n0.60\n0.51\n1.00\n0.41\n0.68\n0.49\n0.56\n0.50\n0.50\n0.41\n\n\nDS\n0.47\n0.39\n0.51\n0.41\n1.00\n0.45\n0.45\n0.42\n0.39\n0.42\n0.31\n\n\nVO\n0.81\n0.72\n0.58\n0.68\n0.45\n1.00\n0.49\n0.57\n0.46\n0.52\n0.40\n\n\nSY\n0.47\n0.40\n0.41\n0.49\n0.45\n0.49\n1.00\n0.50\n0.50\n0.52\n0.46\n\n\nPC\n0.60\n0.54\n0.46\n0.56\n0.42\n0.57\n0.50\n1.00\n0.61\n0.59\n0.51\n\n\nBD\n0.49\n0.45\n0.48\n0.50\n0.39\n0.46\n0.50\n0.61\n1.00\n0.54\n0.59\n\n\nPA\n0.51\n0.49\n0.43\n0.50\n0.42\n0.52\n0.52\n0.59\n0.54\n1.00\n0.46\n\n\nOA\n0.41\n0.38\n0.37\n0.41\n0.31\n0.40\n0.46\n0.51\n0.59\n0.46\n1.00\n\n\n\n\n\nIl primo metodo per la determinazione di \\(m\\) richiede di estrarre tanti fattori quanti sono necessari per spiegare una quota predeterminata della varianza totale. Supponiamo di porre il criterio pari all’80% della varianza totale.\n\nout &lt;- eigen(wais_cor)\nsum(out$val[1:4]) / sum(out$val)\nsum(out$val[1:5]) / sum(out$val)\n\n0.765678107076221\n\n\n0.811885250571924\n\n\nLa soluzione ottenuta in questo modo ci porterebbe a mantenere \\(m=5\\) fattori.\nIl secondo metodo suggerisce di mantenere tutti gli autovalori superiori al valore medio degli autovalori (che, nel caso di R è uguale a \\(1\\)).\n\nprint(round(out$values, 3))\n\n [1] 6.074 1.015 0.746 0.587 0.508 0.431 0.423 0.377 0.351 0.310 0.177\n\n\nNel caso presente, \\(m=2\\).\nLo scree test può essere eseguito creando il grafico seguente.\n\nn &lt;- dim(wais_cor)[1]\nscree_tb &lt;- tibble(\n    x = 1:n,\n    y = sort(eigen(wais_cor)$value, decreasing = TRUE)\n)\n\nscree_plot &lt;- scree_tb |&gt;\n  ggplot(aes(x, y)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = 1:n) +\n  ggtitle(\"Scree plot\")\n\nscree_plot\n\n\n\n\n\n\n\n\nLo scree test suggerisce la presenza di un unico fattore comune.\nLa versione della Parallel Analysis può essere eseguita con la funzione paran() contenuta nel pacchetto paran.\n\nparan(wais_cor, graph = TRUE)\n\n\nUsing eigendecomposition of correlation matrix.\nComputing: 10%  20%  30%  40%  50%  60%  70%  80%  90%  100%\n\n\nResults of Horn's Parallel Analysis for component retention\n330 iterations, using the mean estimate\n\n-------------------------------------------------- \nComponent   Adjusted    Unadjusted    Estimated \n            Eigenvalue  Eigenvalue    Bias \n-------------------------------------------------- \n1           1.647667    3.765744      2.118077\n-------------------------------------------------- \n\nAdjusted eigenvalues &gt; 1 indicate dimensions to retain.\n(1 components retained)\n\n\n\n\n\n\n\n\n\n\nLa Parallel Analysis indica una soluzione a \\(m=1\\) fattore.\nIl test inferenziale relativo al numero di fattori basato sulla statistica \\(\\chi^2\\) può essere eseguito nel modo seguente.\n\nfactanal(covmat=wais_cor, factors=4, n.obs=933)\n\n\nCall:\nfactanal(factors = 4, covmat = wais_cor, n.obs = 933)\n\nUniquenesses:\n   IN    CO    AR    SI    DS    VO    SY    PC    BD    PA    OA \n0.229 0.387 0.005 0.416 0.645 0.137 0.005 0.375 0.331 0.492 0.519 \n\nLoadings:\n   Factor1 Factor2 Factor3 Factor4\nIN 0.758   0.306   0.279   0.157  \nCO 0.672   0.312   0.229   0.107  \nAR 0.368   0.247   0.886   0.120  \nSI 0.602   0.376   0.193   0.207  \nDS 0.315   0.288   0.331   0.252  \nVO 0.851   0.242   0.208   0.192  \nSY 0.238   0.359   0.144   0.888  \nPC 0.432   0.623   0.143   0.172  \nBD 0.237   0.733   0.217   0.168  \nPA 0.367   0.539   0.150   0.245  \nOA 0.207   0.620   0.133   0.190  \n\n               Factor1 Factor2 Factor3 Factor4\nSS loadings      2.826   2.264   1.233   1.137\nProportion Var   0.257   0.206   0.112   0.103\nCumulative Var   0.257   0.463   0.575   0.678\n\nTest of the hypothesis that 4 factors are sufficient.\nThe chi square statistic is 35.4 on 17 degrees of freedom.\nThe p-value is 0.00551 \n\n\n\nfactanal(covmat = wais_cor, factors = 5, n.obs = 933)\n\n\nCall:\nfactanal(factors = 5, covmat = wais_cor, n.obs = 933)\n\nUniquenesses:\n   IN    CO    AR    SI    DS    VO    SY    PC    BD    PA    OA \n0.235 0.389 0.117 0.419 0.600 0.109 0.277 0.308 0.334 0.472 0.456 \n\nLoadings:\n   Factor1 Factor2 Factor3 Factor4 Factor5\nIN  0.745   0.264   0.301   0.192   0.118 \nCO  0.667   0.278   0.244   0.129   0.111 \nAR  0.378   0.236   0.814   0.145         \nSI  0.591   0.332   0.207   0.252   0.121 \nDS  0.288   0.208   0.366   0.341   0.155 \nVO  0.865   0.216   0.207   0.229         \nSY  0.251   0.364   0.153   0.708         \nPC  0.425   0.548   0.156   0.216   0.375 \nBD  0.246   0.708   0.230   0.201   0.107 \nPA  0.355   0.457   0.163   0.325   0.245 \nOA  0.211   0.664   0.128   0.205         \n\n               Factor1 Factor2 Factor3 Factor4 Factor5\nSS loadings      2.799   1.986   1.176   1.043   0.280\nProportion Var   0.254   0.181   0.107   0.095   0.025\nCumulative Var   0.254   0.435   0.542   0.637   0.662\n\nTest of the hypothesis that 5 factors are sufficient.\nThe chi square statistic is 12.5 on 10 degrees of freedom.\nThe p-value is 0.256 \n\n\nIl test del \\(\\chi^2\\) indica una soluzione a sei fattori.\nPer concludere, si potrebbe usare il metodo basato sulla minimizzazione dell’errore di previsione. Tuttavia, non possiamo applicare tale metodo ai dati dell’esempio in quanto sarebbe necessario disporre dei dati grezzi (la matrice di correlazioni non è sufficiente). Allo scopo di illustrare la procedura relativa al metodo basato sulla minimizzazione dell’errore di previsione useremo qui un set di dati diverso, ovvero holzinger19.\n\ndata(holzinger19)\n\nsuppressWarnings(\n    fspe_out &lt;- fspe(\n        data = holzinger19,\n        maxK = 10,\n        nfold = 10,\n        rep = 10,\n        method = \"PE\"\n    )\n)\n\n  |                                                                  |   0%\n\n\n  |------------------------------------------------------------------| 100%\n\n\n\npar(mar=c(4,4,1,1))\nplot.new()\nplot.window(xlim=c(1, 10), ylim=c(.6, .8))\naxis(1, 1:10)\naxis(2, las=2)\nabline(h=min(fspe_out$PEs), col=\"grey\")\nlines(fspe_out$PEs, lty=2)\npoints(fspe_out$PEs, pch=20, cex=1.5)\ntitle(xlab=\"Number of Factors\", ylab=\"Prediction Error\")\n\n\n\n\n\n\n\n\nPer i dati holzinger19, il metodo di {cite:t}haslbeck2022estimating produce dunque una soluzione a 4 fattori.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#la-replicabilità-delle-strutture-fattoriali",
    "href": "chapters/extraction/03_numero_fattori.html#la-replicabilità-delle-strutture-fattoriali",
    "title": "28  Il numero dei fattori",
    "section": "28.6 La replicabilità delle strutture fattoriali",
    "text": "28.6 La replicabilità delle strutture fattoriali\nUn aspetto critico dell’analisi fattoriale è la sua capacità di produrre risultati replicabili. Ciò significa che la struttura fattoriale identificata in un campione dovrebbe essere sostanzialmente simile quando replicata in un campione indipendente. Tuttavia, la cross-validazione delle strutture fattoriali spesso presenta difficoltà.\nL’Analisi Fattoriale Confirmatoria (CFA) offre una soluzione a questo problema. La CFA permette di testare una struttura fattoriale predefinita in un nuovo campione, verificando se i dati empirici supportano la struttura teorica ipotizzata.\nLa replicabilità è un indicatore importante della validità di una struttura fattoriale. Essa fornisce evidenze sulla robustezza e sulla generalizzabilità dei risultati ottenuti. Una struttura fattoriale che si replica consistentemente in diversi campioni è più probabile che rifletta una vera struttura sottostante piuttosto che caratteristiche specifiche di un singolo campione.\nCi sono diversi approcci per valutare la replicabilità:\n\nReplicazione in campioni indipendenti: Applicare la stessa analisi fattoriale a campioni diversi e confrontare i risultati.\nValidazione incrociata: Dividere un ampio campione in sottocampioni e confrontare le strutture fattoriali ottenute.\nCFA: Utilizzare i risultati di un’analisi fattoriale esplorativa come base per un modello confermatorio da testare su nuovi dati.\nMetodi di ricampionamento: Tecniche come il bootstrap per valutare la stabilità della soluzione fattoriale.\n\nIn conclusione, la replicabilità è fondamentale per stabilire la validità e l’utilità di una struttura fattoriale. La CFA, insieme ad altri metodi, rappresenta uno strumento essenziale per valutare la replicabilità e la generalizzabilità dei risultati ottenuti con l’analisi fattoriale.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#cosa-fare-con-i-fattori",
    "href": "chapters/extraction/03_numero_fattori.html#cosa-fare-con-i-fattori",
    "title": "28  Il numero dei fattori",
    "section": "28.7 Cosa fare con i fattori",
    "text": "28.7 Cosa fare con i fattori\nUna volta identificati i fattori attraverso l’analisi fattoriale, sorge la domanda su come utilizzarli efficacemente. Questo aspetto è particolarmente rilevante nel contesto dei Modelli di Equazioni Strutturali (SEM), dove i fattori assumono un significato sostanziale e possono essere impiegati in vari ruoli all’interno del modello.\n\n28.7.1 Utilizzo dei fattori nei SEM\nNei Modelli di Equazioni Strutturali, i fattori possono essere utilizzati in diversi modi:\n\nPredittori: I fattori possono fungere da variabili indipendenti per predire altri costrutti nel modello.\nMediatori: Possono essere utilizzati per spiegare il meccanismo attraverso cui una variabile influenza un’altra.\nModeratori: Possono essere impiegati per esaminare come la relazione tra due variabili cambia in funzione del livello del fattore moderatore.\nEsiti: Possono rappresentare le variabili dipendenti del modello, influenzate da altri costrutti.\n\nUn vantaggio significativo dell’utilizzo dei fattori latenti nei SEM è la loro capacità di “disattenuare” le associazioni per l’errore di misurazione. Questo significa che le relazioni stimate tra i costrutti sono più accurate, poiché l’errore di misurazione viene esplicitamente modellato e separato dalla varianza “vera” del costrutto.\n\n\n28.7.2 Sfide nell’uso dei fattori al di fuori dei SEM\nQuando si cerca di utilizzare i fattori al di fuori del contesto dei SEM, emergono alcune problematiche:\n\nSomma o media semplice: Molti ricercatori, dopo aver identificato che tre variabili saturano su un Fattore A, tendono a combinarle semplicemente sommandole o calcolandone la media. Questa pratica, tuttavia, non è accurata poiché ignora i pesi fattoriali (factor loadings) e l’errore di misurazione.\nCompositi lineari ponderati: Un approccio più sofisticato consiste nel creare un composito lineare, sommando le variabili ponderate per i loro pesi fattoriali. Questo metodo preserva le differenze nelle correlazioni tra le variabili e il fattore, ma continua a ignorare l’errore stimato. Di conseguenza, potrebbe non essere pienamente generalizzabile o significativo.\nCompositi a pesi unitari: In alcuni casi, assegnare lo stesso peso a tutte le variabili (compositi a pesi unitari) potrebbe risultare più generalizzabile rispetto ai compositi ponderati. Questo perché parte della variabilità nei pesi fattoriali potrebbe riflettere errori di campionamento piuttosto che differenze reali nell’importanza delle variabili. Si noti che il risultato numerico è identico tra la somma semplice e il composito a pesi unitari – la media semplice differisce solo per una costante (la divisione per il numero di variabili), che non altera le relazioni tra i punteggi. La differenza principale sta nel ragionamento sottostante e nel contesto metodologico: La somma/media semplice è spesso usata senza considerare l’analisi fattoriale. I compositi a pesi unitari sono una scelta consapevole basata sui risultati dell’analisi fattoriale, decidendo di trattare tutte le variabili con uguale importanza.\n\n\n\n28.7.3 Considerazioni aggiuntive\n\nInterpretabilità: I fattori latenti nei SEM offrono una rappresentazione più “pura” del costrutto sottostante, ma possono essere meno intuitivi da interpretare rispetto a punteggi compositi.\nBilanciamento tra precisione e praticità: Mentre l’uso di fattori latenti nei SEM offre maggiore precisione, in alcuni contesti (ad esempio, nella pratica clinica o nella ricerca applicata) potrebbe essere necessario un compromesso tra precisione statistica e facilità d’uso.\nStabilità cross-campione: È importante valutare la stabilità della struttura fattoriale attraverso diversi campioni prima di utilizzare i fattori in analisi successive.\nMetodi avanzati: Tecniche come la regressione con variabili latenti o l’uso di punteggi fattoriali salvati dai SEM possono offrire un compromesso tra la precisione dei modelli SEM completi e la necessità di punteggi compositi.\n\nIn conclusione, mentre i fattori offrono potenti strumenti per la modellazione di costrutti latenti, il loro utilizzo richiede una comprensione approfondita delle loro proprietà statistiche e delle implicazioni delle diverse strategie di operazionalizzazione. La scelta del metodo più appropriato dipenderà dagli obiettivi specifici della ricerca, dal contesto di applicazione e dalle caratteristiche dei dati disponibili.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#riflessioni-conclusive",
    "href": "chapters/extraction/03_numero_fattori.html#riflessioni-conclusive",
    "title": "28  Determinare il numero dei fattori",
    "section": "\n28.15 Riflessioni Conclusive",
    "text": "28.15 Riflessioni Conclusive\nDeterminare il numero di fattori da estrarre in un’Analisi Fattoriale Esplorativa (EFA) rappresenta una delle sfide metodologiche più complesse nella costruzione di strumenti psicometrici. Nessun metodo è infallibile o universalmente valido: ogni tecnica presenta vantaggi, limiti e assunzioni specifiche. Per questo motivo, la triangolazione di più metodi e la riflessione teorica guidano le scelte più solide.\n\n28.15.1 1. La scelta del numero di fattori: tra tecniche classiche e moderne\nLe tecniche tradizionali, come la regola di Kaiser e lo scree test, sono state largamente impiegate per la loro semplicità, ma risultano oggi superate per via della loro tendenza sistematica alla sovra- o sotto-fattorizzazione. In particolare:\n\nLa Parallel Analysis (PA) si conferma il metodo più raccomandato, grazie alla sua capacità di distinguere fattori reali da quelli generati dal rumore;\nIl metodo Comparison Data (CD) si dimostra utile con strutture complesse o fattori correlati, ma va calibrato attentamente;\nI criteri informativi (AIC, BIC) e gli indici di adattamento del modello (RMSEA, CFI) aggiungono elementi preziosi per valutare la bontà della soluzione, specialmente quando integrati con la CFA;\nMetodi recenti come MAP, Hull, EGA o Factor Forest offrono soluzioni promettenti, soprattutto in contesti con comunalità basse, dati non normali o ordinali.\n\n28.15.2 2. Replicabilità: validare ciò che si scopre\nIdentificare una struttura fattoriale coerente è solo il primo passo. Per attribuire validità e generalizzabilità a una soluzione esplorativa, occorre valutarne la replicabilità:\n\n\nReplicazione in campioni indipendenti e validazione incrociata servono a testare la stabilità della struttura in sottogruppi differenti;\nLa CFA permette di verificare, in modo formale, se una struttura fattoriale individuata in EFA è confermata da nuovi dati;\nTecniche di ricampionamento (come il bootstrap) offrono ulteriori strumenti per stimare l’incertezza legata alla soluzione ottenuta.\n\nUna struttura replicabile è più probabilmente una rappresentazione fedele dei costrutti latenti piuttosto che un artefatto del campione specifico.\n\n28.15.3 3. Cosa fare con i fattori: applicazioni e criticità\nDopo aver identificato i fattori, è cruciale decidere come utilizzarli:\n\nNei Modelli di Equazioni Strutturali (SEM), i fattori possono essere usati come predittori, mediatori, moderatori o esiti, con il vantaggio di modellare esplicitamente l’errore di misura;\nAl di fuori dei SEM, l’uso di punteggi fattoriali (calcolati come media, somma o compositi ponderati) va considerato con cautela, poiché può trascurare errori di misura o differenze di importanza tra variabili;\nIn certi casi, compositi a pesi unitari possono essere preferibili a quelli ponderati, specialmente per garantire maggiore generalizzabilità e robustezza cross-campione.\n\nLa scelta operativa su come rappresentare e usare i fattori dovrebbe sempre essere coerente con le finalità della ricerca e con le caratteristiche dei dati.\n\n28.15.4 4. Una visione integrata\nIn definitiva, la determinazione del numero di fattori, la replicabilità della soluzione e il modo in cui i fattori vengono utilizzati sono aspetti interdipendenti di un processo rigoroso e cumulativo. Una buona pratica psicometrica non si limita a identificare la struttura che “funziona meglio” nel proprio campione, ma valuta la stabilità della soluzione, il suo significato teorico e la sua applicazione pratica.\n\n✏️ La validità di uno strumento psicologico non si esaurisce nella bontà del suo adattamento al campione corrente, ma nella sua capacità di riflettere stabilmente e in modo interpretabile i costrutti che intende misurare.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Determinare il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#session-info",
    "href": "chapters/extraction/03_numero_fattori.html#session-info",
    "title": "28  Determinare il numero dei fattori",
    "section": "\n28.16 Session Info",
    "text": "28.16 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.4\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] REFA_0.1.0         latentFactoR_0.0.6 EGAnet_2.2.0      \n#&gt;  [4] EFAfactors_1.2.1   EFAtools_0.4.6     nFactors_2.4.1.1  \n#&gt;  [7] lattice_0.22-7     fspe_0.1.2         paran_1.5.3       \n#&gt; [10] ggokabeito_0.1.0   see_0.11.0         MASS_7.3-65       \n#&gt; [13] viridis_0.6.5      viridisLite_0.4.2  ggpubr_0.6.0      \n#&gt; [16] ggExtra_0.10.1     gridExtra_2.3      patchwork_1.3.0   \n#&gt; [19] bayesplot_1.11.1   semTools_0.5-7     semPlot_1.1.6     \n#&gt; [22] lavaan_0.6-19      psych_2.5.3        scales_1.3.0      \n#&gt; [25] markdown_2.0       knitr_1.50         lubridate_1.9.4   \n#&gt; [28] forcats_1.0.0      stringr_1.5.1      dplyr_1.1.4       \n#&gt; [31] purrr_1.0.4        readr_2.1.5        tidyr_1.3.1       \n#&gt; [34] tibble_3.2.1       ggplot2_3.5.2      tidyverse_2.0.0   \n#&gt; [37] here_1.0.1        \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] splines_4.4.2          later_1.4.2            XML_3.99-0.18         \n#&gt;   [4] rpart_4.1.24           lifecycle_1.0.4        Rdpack_2.6.4          \n#&gt;   [7] rstatix_0.7.2          rprojroot_2.0.4        rockchalk_1.8.157     \n#&gt;  [10] backports_1.5.0        magrittr_2.0.3         openxlsx_4.2.8        \n#&gt;  [13] Hmisc_5.2-3            rmarkdown_2.29         yaml_2.3.10           \n#&gt;  [16] httpuv_1.6.15          qgraph_1.9.8           zip_2.3.2             \n#&gt;  [19] reticulate_1.42.0      RColorBrewer_1.1-3     pbapply_1.7-2         \n#&gt;  [22] minqa_1.2.8            multcomp_1.4-28        abind_1.4-8           \n#&gt;  [25] quadprog_1.5-8         nnet_7.3-20            TH.data_1.1-3         \n#&gt;  [28] sandwich_3.1-1         arm_1.14-4             codetools_0.2-20      \n#&gt;  [31] tidyselect_1.2.1       farver_2.1.2           lme4_1.1-37           \n#&gt;  [34] stats4_4.4.2           base64enc_0.1-3        ddpcr_1.15.2          \n#&gt;  [37] jsonlite_2.0.0         Formula_1.2-5          survival_3.8-3        \n#&gt;  [40] emmeans_1.11.0         tools_4.4.2            sna_2.8               \n#&gt;  [43] Rcpp_1.0.14            glue_1.8.0             mnormt_2.1.1          \n#&gt;  [46] xfun_0.52              ranger_0.17.0          withr_3.0.2           \n#&gt;  [49] fastmap_1.2.0          GGally_2.2.1           boot_1.3-31           \n#&gt;  [52] digest_0.6.37          mi_1.1                 timechange_0.3.0      \n#&gt;  [55] R6_2.6.1               mime_0.13              estimability_1.5.1    \n#&gt;  [58] colorspace_2.1-1       gtools_3.9.5           jpeg_0.1-11           \n#&gt;  [61] generics_0.1.3         data.table_1.17.0      corpcor_1.6.10        \n#&gt;  [64] htmlwidgets_1.6.4      ggstats_0.9.0          pkgconfig_2.0.3       \n#&gt;  [67] sem_3.1-16             gtable_0.3.6           SimCorMultRes_1.9.0   \n#&gt;  [70] htmltools_0.5.8.1      carData_3.0-5          parallelMap_1.5.1     \n#&gt;  [73] png_0.1-8              reformulas_0.4.0       rstudioapi_0.17.1     \n#&gt;  [76] tzdb_0.5.0             reshape2_1.4.4         statnet.common_4.11.0 \n#&gt;  [79] coda_0.19-4.1          checkmate_2.3.2        nlme_3.1-168          \n#&gt;  [82] nloptr_2.2.1           proxy_0.4-27           zoo_1.8-13            \n#&gt;  [85] parallel_4.4.2         miniUI_0.1.1.1         foreign_0.8-90        \n#&gt;  [88] pillar_1.10.2          grid_4.4.2             vctrs_0.6.5           \n#&gt;  [91] mlr_2.19.2             promises_1.3.2         car_3.1-3             \n#&gt;  [94] OpenMx_2.21.13         xtable_1.8-4           cluster_2.1.8.1       \n#&gt;  [97] GPArotation_2024.3-1   htmlTable_2.4.3        evaluate_1.0.3        \n#&gt; [100] BBmisc_1.13            pbivnorm_0.6.0         mvtnorm_1.3-3         \n#&gt; [103] cli_3.6.4              kutils_1.73            compiler_4.4.2        \n#&gt; [106] rlang_1.1.5            ggsignif_0.6.4         fdrtool_1.2.18        \n#&gt; [109] RcppArmadillo_14.4.1-1 plyr_1.8.9             stringi_1.8.7         \n#&gt; [112] network_1.19.0         munsell_0.5.1          lisrelToR_0.3         \n#&gt; [115] pacman_0.5.1           Matrix_1.7-3           ParamHelpers_1.14.2   \n#&gt; [118] hms_1.1.3              glasso_1.11            shiny_1.10.0          \n#&gt; [121] evd_2.3-7.1            rbibutils_2.3          ineq_0.2-13           \n#&gt; [124] igraph_2.1.4           broom_1.0.8            RcppParallel_5.1.10   \n#&gt; [127] fastmatch_1.1-6        xgboost_1.7.9.1\n\n\n\n\n\nGoretzko, D. (2025). How many factors to retain in exploratory factor analysis? A critical overview of factor retention methods. Psychological Methods.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Determinare il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/04_rotazione.html",
    "href": "chapters/extraction/04_rotazione.html",
    "title": "29  La rotazione fattoriale",
    "section": "",
    "text": "29.1 Introduzione\nUna volta stabilito il numero ottimale di fattori comuni (come illustrato nel Capitolo 28), ci si trova spesso davanti a una soluzione iniziale non ruotata, che può apparire poco chiara e di difficile interpretazione. A questo punto, per rendere più agevole l’identificazione di un pattern fattoriale interpretabile, è prassi comune procedere a una rotazione degli assi nello spazio dei fattori. Lo scopo principale di questa rotazione è semplificare la struttura delle saturazioni fattoriali, individuando gruppi di variabili che mostrino saturazioni elevate su un singolo fattore e saturazioni molto basse (o nulle) sugli altri.\nTale processo nasce dall’esigenza di affrontare la cosiddetta indeterminatezza rotazionale, ovvero il fatto che, fissato un determinato numero di fattori, sono possibili infinite soluzioni alternative ugualmente in grado di riprodurre la matrice di correlazioni osservata. Per scegliere in modo consapevole tra queste soluzioni si fa ricorso all’idea di “parsimonia”: i fattori vengono ruotati in modo da ottenere la cosiddetta struttura semplice, dove ogni fattore tende a “caricare” nettamente su un sottoinsieme limitato di variabili, facilitandone così l’interpretazione psicologica. In passato, questa operazione veniva eseguita a mano; oggi, si utilizzano metodi automatizzati che rendono molto più rapida l’individuazione di una configurazione fattoriale interpretabile.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>La rotazione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/04_rotazione.html#parsimonia-e-semplicità",
    "href": "chapters/extraction/04_rotazione.html#parsimonia-e-semplicità",
    "title": "29  La rotazione fattoriale",
    "section": "\n29.3 Parsimonia e semplicità",
    "text": "29.3 Parsimonia e semplicità\nPer ovviare al problema dell’indeterminatezza fattoriale, solitamente ci si affida a due criteri fondamentali:\n\n\nParsimonia: si preferisce il modello con minor numero di fattori che spieghi adeguatamente la covarianza tra le variabili.\n\nSe più soluzioni con un diverso numero di fattori riproducono parimenti la matrice di correlazione, si sceglie quella con il minor numero di fattori.\n\n\n\nSemplicità: tra diverse soluzioni fattoriali con lo stesso numero \\(m\\) di fattori, si preferisce la trasformazione (rotazione) che renda i fattori più interpretabili, ossia che abbia il maggior numero possibile di saturazioni nulle o prossime allo zero, e saturazioni elevate concentrate su poche variabili.\n\nLe rotazioni dei fattori possono essere di due tipi:\n\n\nortogonali (i fattori rimangono non correlati tra di loro);\n\noblique (i fattori possono essere correlati).\n\nIndipendentemente dal tipo di rotazione, l’obiettivo è rendere più agevole l’interpretazione dei fattori, identificando in maniera più chiara quali variabili “caricano” su ciascun fattore.\n\n29.3.1 Il Criterio della Struttura Semplice nell’Analisi Fattoriale\nThurstone (1947) introdusse il concetto di struttura semplice, un insieme di criteri che mirano a far emergere fattori facilmente interpretabili. In breve, l’ideale di struttura semplice è una configurazione in cui ogni variabile ha saturazioni elevate su un solo fattore (o su pochi fattori) e saturazioni nulle (o molto basse) sugli altri.\nLe condizioni fondamentali per una struttura semplice includono:\n\nciascuna variabile dovrebbe avere saturazioni prossime allo zero con la maggior parte dei fattori (ad eccezione di uno o pochi di essi);\nper ogni fattore, ci si aspetta di trovare almeno \\(m\\) saturazioni prossime allo zero (dove \\(m\\) è il numero di fattori).\n\nSe queste condizioni sono soddisfatte, i fattori risultano di immediata interpretazione: si possono raggruppare le variabili in base alle saturazioni elevate su ciascun fattore, interpretando poi i fattori come caratteristiche o dimensioni psicologiche comuni alle variabili che vi saturano.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>La rotazione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/04_rotazione.html#rotazione-nello-spazio-geometrico",
    "href": "chapters/extraction/04_rotazione.html#rotazione-nello-spazio-geometrico",
    "title": "29  La rotazione fattoriale",
    "section": "\n29.4 Rotazione nello Spazio Geometrico",
    "text": "29.4 Rotazione nello Spazio Geometrico\n\n29.4.1 Rotazione Ortogonale\nLa rotazione ortogonale può essere vista come una “rotazione rigida” degli assi in uno spazio cartesiano, in cui si mantengono inalterate le distanze tra i punti (cioè tra le variabili, rappresentate dalle loro saturazioni fattoriali). L’unica cosa che cambia è l’orientamento degli assi stessi.\nI metodi di rotazione ortogonale (come Varimax, Quartimax, Equamax, ecc.) cercano di massimizzare la “semplicità” della matrice ruotata, rendendo più evidenti i fattori. In particolare:\n\n\nVarimax massimizza la varianza dei quadrati dei loadings entro ciascun fattore (colonna);\n\n\nQuartimax massimizza la varianza dei loadings entro ogni variabile (riga);\n\n\nEquamax cerca un compromesso tra Varimax e Quartimax.\n\nQuesto tipo di rotazione si adatta bene quando si ritiene, sulla base di considerazioni teoriche, che i fattori siano incorrelati.\nNella rotazione ortogonale, essendo i fattori mantenuti a 90° l’uno rispetto all’altro, le comunalità rimangono inalterate (perché le distanze geometriche nel diagramma fattoriale non cambiano).\n\n29.4.2 Metodo Grafico per la Rotazione dei Fattori\nQuando il numero di fattori \\(m=2\\), è possibile rappresentare graficamente la soluzione di analisi fattoriale. Ogni variabile manifesta è un punto nello spazio bidimensionale, con coordinate date dalle saturazioni sui due fattori \\((\\hat{\\lambda}_{i1}, \\hat{\\lambda}_{i2})\\).\nEsempio (Brown, Williams e Barlow, 1984, discusso in Rencher (2002)):\nUna ragazza di dodici anni valuta sette suoi conoscenti su cinque attributi: gentilezza, intelligenza, felicità, simpatia e giustizia. Dalla matrice di correlazione \\(R\\) di questi attributi, si estrae una soluzione a due fattori con il metodo delle componenti principali, senza rotazione. Spesso, questa soluzione non ruotata ha un primo fattore che “assorbe” gran parte della varianza (saturazioni elevate su tutte le variabili) e un secondo fattore meno chiaro, con saturazioni positive o negative concentrate in modo non facilmente interpretabile.\nTramite una rotazione ortogonale si cerca un angolo \\(\\phi\\) (nel caso di due fattori) che “allinei” i nuovi assi ai punti che rappresentano i dati. Matematicamente, le saturazioni ruotate \\(\\hat{\\boldsymbol{\\Lambda}}^*\\) si ottengono moltiplicando \\(\\hat{\\boldsymbol{\\Lambda}}\\) per la matrice di rotazione ortogonale \\(\\mathbf{T}\\), ad esempio:\n\\[\n\\mathbf{T} =\n\\begin{bmatrix}\n\\cos{\\phi} & -\\sin{\\phi}\\\\\n\\sin{\\phi} & \\cos{\\phi}\n\\end{bmatrix}.\n\\]\nQuesto approccio grafico è molto intuitivo per \\(m=2\\), poiché consente di “ruotare” fisicamente un diagramma di dispersione di punti attorno all’origine, in modo da ricavare un sistema di assi più vicino alla cosiddetta “struttura semplice”.\n\n29.4.3 Metodi di rotazione ortogonale\n\nVarimax:\nMassimizza la varianza tra i quadrati dei loadings in ciascun fattore. Se i loadings di un fattore sono tutti simili, la varianza dei quadrati è bassa; se invece alcuni loadings sono prossimi a zero e altri prossimi a 1, la varianza è alta, e questo facilita l’interpretazione (un fattore “carica” fortemente solo su certe variabili).\nQuartimax:\nConcentra l’attenzione sulla semplificazione delle variabili (righe di \\(\\hat{\\boldsymbol{\\Lambda}}\\)) piuttosto che dei fattori (colonne).\n\nEntrambi i metodi, così come altri (Equamax, Orthomax, ecc.), sono disponibili in R attraverso diverse funzioni (factanal(), principal(), factor.pa(), ecc.).\n\n29.4.4 Metodi di Rotazione Obliqua\nSi parla di rotazione obliqua quando si consente ai fattori di correlare tra loro. Più precisamente, sarebbe più corretto il termine “trasformazione obliqua” (Rencher, 2002), perché una rotazione in senso geometrico implica il mantenimento dell’ortogonalità degli assi. Tuttavia, in letteratura il termine “rotazione obliqua” è universalmente accettato.\nNei metodi obliqui, gli assi che rappresentano i fattori possono non essere ad angolo retto, così da allinearsi meglio ai “cluster” di variabili che saturano su più fattori correlati. Esistono varie forme di rotazione obliqua (Direct Oblimin, Promax, Geomin, ecc.). Un vantaggio dell’obliquità è che può fornire soluzioni più realistiche in quei casi, molto comuni in psicologia, dove i costrutti latenti sono naturalmente correlati tra loro.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>La rotazione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/04_rotazione.html#matrice-dei-pesi-fattoriali-e-matrice-di-struttura",
    "href": "chapters/extraction/04_rotazione.html#matrice-dei-pesi-fattoriali-e-matrice-di-struttura",
    "title": "29  La rotazione fattoriale",
    "section": "\n29.5 Matrice dei Pesi Fattoriali e Matrice di Struttura",
    "text": "29.5 Matrice dei Pesi Fattoriali e Matrice di Struttura\n\n29.5.1 Rotazione Ortogonale\nIn presenza di fattori considerati non correlati, la matrice delle saturazioni fattoriali \\(\\hat{\\boldsymbol{\\Lambda}}\\) coincide, di fatto, con i coefficienti di correlazione tra i fattori e le variabili. Se i fattori sono ortogonali (\\(\\text{corr}(\\xi_1, \\xi_2) = 0\\)), ciascuna variabile manifesta \\(y_i\\) è collegata a ciascun fattore \\(\\xi_j\\) tramite un solo percorso: la saturazione fattoriale (in termini di path analysis, è come un coefficiente di regressione). Di conseguenza, le loadings fattoriali si possono interpretare direttamente come i cosiddetti “pesi beta” di regressione (Tabachnick & Fidell, 2001).\n\n29.5.2 Rotazione Obliqua\nSe i fattori sono correlati (\\(\\text{corr}(\\xi_1, \\xi_2) \\neq 0\\)), la situazione si complica perché ciascuna variabile manifesta si collegherà a ciascun fattore tramite percorsi diretti e percorsi indiretti (che passano attraverso la correlazione con l’altro fattore). In questo caso, la matrice delle saturazioni fattoriali \\(\\hat{\\boldsymbol{\\Lambda}}\\) non rappresenta più le semplici correlazioni tra fattori e variabili; occorre distinguere:\n\nMatrice Pattern (\\(\\hat{\\boldsymbol{\\Lambda}}\\)): contiene i coefficienti di regressione parziali delle variabili manifeste sui fattori. Ciascuna riga della matrice indica quanto una variabile è influenzata “direttamente” da ogni fattore, escludendo l’effetto dell’eventuale correlazione con gli altri fattori.\nMatrice di Struttura: contiene le correlazioni tra i fattori e le variabili, includendo sia gli effetti diretti sia quelli indiretti dovuti alla correlazione tra fattori.\nMatrice di Intercorrelazione Fattoriale (\\(\\hat{\\boldsymbol{\\Phi}}\\)): specifica il grado di correlazione tra fattori (per es. \\(\\phi_{12} = \\text{corr}(\\xi_1, \\xi_2)\\)).\n\nIn un contesto di rotazione obliqua, l’interpretazione dei fattori richiede di guardare sia alla matrice Pattern (per comprendere gli effetti diretti) sia alla matrice di Struttura (per valutare l’effetto congiunto diretto+indiretto dei fattori sulle variabili).\nDi seguito è riportato un esempio pratico per mettere a confronto, in modo sia numerico sia grafico, le soluzioni ortogonali e oblique in un’Analisi Fattoriale Esplorativa (EFA). L’obiettivo è mostrare come cambia la configurazione dei fattori e come, nella soluzione obliqua, i fattori risultino correlati, rendendo necessaria la distinzione tra matrice Pattern e matrice di Struttura.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>La rotazione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/04_rotazione.html#esempio-con-semtools",
    "href": "chapters/extraction/04_rotazione.html#esempio-con-semtools",
    "title": "29  La rotazione fattoriale",
    "section": "29.5 Esempio con semTools",
    "text": "29.5 Esempio con semTools\nPresento qui un esempio di uso di vari metodi di estrazione fattoriale. Tra tali metodi, la rotazione obliqua Geomin è molto popolare ed è il default di M-Plus.\nIniziamo a caricare il pacchetto semTools.\n\nsuppressPackageStartupMessages(library(\"semTools\")) \n\nEseguiamo l’analisi fattoriale esplorativa del classico set di dati di Holzinger e Swineford (1939) il quale è costituito dai punteggi dei test di abilità mentale di bambini di seconda e terza media di due scuole diverse (Pasteur e Grant-White). Nel set di dati originale (disponibile nel pacchetto MBESS), sono forniti i punteggi di 26 test. Tuttavia, un sottoinsieme più piccolo con 9 variabili è più ampiamente utilizzato in letteratura. Questi sono i dati qui usati.\nNel presente esempio, verrà eseguita l’analisi fattoriale esplorativa con l’estrazione di tre fattori. Il metodo di estrazione è mlr:\n\nmaximum likelihood estimation with robust (Huber-White) standard errors and a scaled test statistic that is (asymptotically) equal to the Yuan-Bentler test statistic. For both complete and incomplete data.\n\nLa soluzione iniziale non è ruotata.\n\nunrotated &lt;- efaUnrotate(\n    HolzingerSwineford1939, \n    nf = 3, \n    varList = paste0(\"x\", 1:9), \n    estimator = \"mlr\"\n)\nout &lt;- summary(unrotated)\nprint(out)\n\nlavaan 0.6-19 ended normally after 217 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        36\n\n  Number of observations                           301\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                22.897      23.864\n  Degrees of freedom                                12          12\n  P-value (Chi-square)                           0.029       0.021\n  Scaling correction factor                                  0.959\n    Yuan-Bentler correction (Mplus variant)                       \n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  factor1 =~                                          \n    x1      (l1_1)    0.653    0.083    7.909    0.000\n    x2      (l2_1)    0.353    0.079    4.481    0.000\n    x3      (l3_1)    0.415    0.086    4.832    0.000\n    x4      (l4_1)    0.926    0.067   13.762    0.000\n    x5      (l5_1)    1.014    0.067   15.176    0.000\n    x6      (l6_1)    0.868    0.062   13.887    0.000\n    x7      (l7_1)    0.283    0.091    3.113    0.002\n    x8      (l8_1)    0.340    0.083    4.095    0.000\n    x9      (l9_1)    0.460    0.078    5.881    0.000\n  factor2 =~                                          \n    x1      (l1_2)    0.349    0.124    2.815    0.005\n    x2      (l2_2)    0.242    0.159    1.523    0.128\n    x3      (l3_2)    0.497    0.132    3.767    0.000\n    x4      (l4_2)   -0.337    0.067   -5.058    0.000\n    x5      (l5_2)   -0.461    0.077   -6.009    0.000\n    x6      (l6_2)   -0.280    0.057   -4.908    0.000\n    x7      (l7_2)    0.372    0.188    1.976    0.048\n    x8      (l8_2)    0.510    0.133    3.831    0.000\n    x9      (l9_2)    0.489    0.066    7.416    0.000\n  factor3 =~                                          \n    x1      (l1_3)   -0.338    0.103   -3.275    0.001\n    x2      (l2_3)   -0.405    0.092   -4.401    0.000\n    x3      (l3_3)   -0.404    0.120   -3.355    0.001\n    x4      (l4_3)    0.049    0.098    0.503    0.615\n    x5      (l5_3)    0.122    0.105    1.154    0.248\n    x6      (l6_3)   -0.000    0.076   -0.003    0.998\n    x7      (l7_3)    0.609    0.125    4.863    0.000\n    x8      (l8_3)    0.409    0.143    2.853    0.004\n    x9      (l9_3)    0.112    0.123    0.915    0.360\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  factor1 ~~                                          \n    factor2           0.000                           \n    factor3           0.000                           \n  factor2 ~~                                          \n    factor3           0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    factor1           1.000                           \n    factor2           1.000                           \n    factor3           1.000                           \n   .x1                0.696    0.113    6.184    0.000\n   .x2                1.035    0.106    9.803    0.000\n   .x3                0.692    0.097    7.132    0.000\n   .x4                0.377    0.053    7.170    0.000\n   .x5                0.403    0.064    6.303    0.000\n   .x6                0.365    0.046    7.984    0.000\n   .x7                0.594    0.148    4.014    0.000\n   .x8                0.479    0.099    4.842    0.000\n   .x9                0.551    0.065    8.518    0.000\n\nConstraints:\n                                               |Slack|\n    0-(1_2*1_1+2_2*2_1+3_2*3_1+4_2*4_1+5_2*5_    0.000\n    0-(1_3*1_1+2_3*2_1+3_3*3_1+4_3*4_1+5_3*5_    0.000\n    0-(1_3*1_2+2_3*2_2+3_3*3_2+4_3*4_2+5_3*5_    0.000\n\n\n\nSi noti che, in assenza di rotazione, è impossibile assegnare un significato ai fattori comuni.\n\n29.5.1 Orthogonal varimax\nUtilizziamo ora la rotazione ortogonale Varimax.\n\nout_varimax &lt;- orthRotate(\n    unrotated, \n    method = \"varimax\"\n)\nout &lt;- summary(out_varimax, sort = FALSE, suppress = 0.3)\nprint(out)\n\nStandardized Rotated Factor Loadings\n\n\n   factor1 factor2 factor3\nx1  0.320*  0.607*        \nx2          0.481*        \nx3          0.662*        \nx4  0.838*                \nx5  0.867*                \nx6  0.815*                \nx7                  0.695*\nx8                  0.704*\nx9          0.409*  0.511*\n\nFactor Correlation\n        factor1 factor2 factor3\nfactor1       1       0       0\nfactor2       0       1       0\nfactor3       0       0       1\n\nMethod of rotation: varimax \n\nTest Statistics for Standardized Rotated Factor Loadings\n\n\n       lhs op rhs std.loading    se      z     p ci.lower ci.upper\n1  factor1 =~  x1       0.320 0.055  5.799 0.000    0.212    0.428\n2  factor1 =~  x2       0.135 0.063  2.151 0.031    0.012    0.259\n3  factor1 =~  x3       0.080 0.049  1.622 0.105   -0.017    0.176\n4  factor1 =~  x4       0.838 0.028 30.193 0.000    0.784    0.892\n5  factor1 =~  x5       0.867 0.024 36.189 0.000    0.820    0.914\n6  factor1 =~  x6       0.815 0.024 33.939 0.000    0.768    0.862\n7  factor1 =~  x7       0.102 0.049  2.058 0.040    0.005    0.199\n8  factor1 =~  x8       0.078 0.048  1.617 0.106   -0.016    0.172\n9  factor1 =~  x9       0.170 0.053  3.222 0.001    0.067    0.273\n10 factor2 =~  x1       0.607 0.075  8.138 0.000    0.461    0.753\n11 factor2 =~  x2       0.481 0.067  7.184 0.000    0.350    0.612\n12 factor2 =~  x3       0.662 0.058 11.499 0.000    0.549    0.775\n13 factor2 =~  x4       0.113 0.043  2.615 0.009    0.028    0.198\n14 factor2 =~  x5       0.032 0.040  0.802 0.422   -0.047    0.111\n15 factor2 =~  x6       0.162 0.042  3.855 0.000    0.079    0.244\n16 factor2 =~  x7      -0.062 0.047 -1.341 0.180   -0.154    0.029\n17 factor2 =~  x8       0.174 0.082  2.117 0.034    0.013    0.336\n18 factor2 =~  x9       0.409 0.079  5.173 0.000    0.254    0.564\n19 factor3 =~  x1       0.130 0.066  1.978 0.048    0.001    0.259\n20 factor3 =~  x2      -0.041 0.071 -0.578 0.563   -0.179    0.098\n21 factor3 =~  x3       0.113 0.049  2.324 0.020    0.018    0.209\n22 factor3 =~  x4       0.077 0.040  1.916 0.055   -0.002    0.155\n23 factor3 =~  x5       0.070 0.042  1.669 0.095   -0.012    0.153\n24 factor3 =~  x6       0.066 0.038  1.715 0.086   -0.009    0.141\n25 factor3 =~  x7       0.695 0.092  7.591 0.000    0.516    0.875\n26 factor3 =~  x8       0.704 0.083  8.520 0.000    0.542    0.865\n27 factor3 =~  x9       0.511 0.065  7.885 0.000    0.384    0.638\n       lhs op rhs std.loading    se      z     p ci.lower ci.upper\n1  factor1 =~  x1       0.320 0.055  5.799 0.000    0.212    0.428\n2  factor1 =~  x2       0.135 0.063  2.151 0.031    0.012    0.259\n3  factor1 =~  x3       0.080 0.049  1.622 0.105   -0.017    0.176\n4  factor1 =~  x4       0.838 0.028 30.193 0.000    0.784    0.892\n5  factor1 =~  x5       0.867 0.024 36.189 0.000    0.820    0.914\n6  factor1 =~  x6       0.815 0.024 33.939 0.000    0.768    0.862\n7  factor1 =~  x7       0.102 0.049  2.058 0.040    0.005    0.199\n8  factor1 =~  x8       0.078 0.048  1.617 0.106   -0.016    0.172\n9  factor1 =~  x9       0.170 0.053  3.222 0.001    0.067    0.273\n10 factor2 =~  x1       0.607 0.075  8.138 0.000    0.461    0.753\n11 factor2 =~  x2       0.481 0.067  7.184 0.000    0.350    0.612\n12 factor2 =~  x3       0.662 0.058 11.499 0.000    0.549    0.775\n13 factor2 =~  x4       0.113 0.043  2.615 0.009    0.028    0.198\n14 factor2 =~  x5       0.032 0.040  0.802 0.422   -0.047    0.111\n15 factor2 =~  x6       0.162 0.042  3.855 0.000    0.079    0.244\n16 factor2 =~  x7      -0.062 0.047 -1.341 0.180   -0.154    0.029\n17 factor2 =~  x8       0.174 0.082  2.117 0.034    0.013    0.336\n18 factor2 =~  x9       0.409 0.079  5.173 0.000    0.254    0.564\n19 factor3 =~  x1       0.130 0.066  1.978 0.048    0.001    0.259\n20 factor3 =~  x2      -0.041 0.071 -0.578 0.563   -0.179    0.098\n21 factor3 =~  x3       0.113 0.049  2.324 0.020    0.018    0.209\n22 factor3 =~  x4       0.077 0.040  1.916 0.055   -0.002    0.155\n23 factor3 =~  x5       0.070 0.042  1.669 0.095   -0.012    0.153\n24 factor3 =~  x6       0.066 0.038  1.715 0.086   -0.009    0.141\n25 factor3 =~  x7       0.695 0.092  7.591 0.000    0.516    0.875\n26 factor3 =~  x8       0.704 0.083  8.520 0.000    0.542    0.865\n27 factor3 =~  x9       0.511 0.065  7.885 0.000    0.384    0.638\n\n\n\n\n29.5.2 Orthogonal Quartimin\nUn metodo alternativo per la rotazione ortogonale è Quartimin.\n\nout_quartimin &lt;- orthRotate(\n    unrotated, \n    method = \"quartimin\"\n)\nout &lt;- summary(out_quartimin, sort = FALSE, suppress = 0.3)\nprint(out)\n\nStandardized Rotated Factor Loadings\n\n\n   factor1 factor2 factor3\nx1  0.353*  0.590*        \nx2          0.474*        \nx3          0.657*        \nx4  0.844*                \nx5  0.869*                \nx6  0.823*                \nx7                  0.692*\nx8                  0.702*\nx9          0.397*  0.508*\n\nFactor Correlation\n        factor1 factor2 factor3\nfactor1       1       0       0\nfactor2       0       1       0\nfactor3       0       0       1\n\nMethod of rotation: Quartimin \n\nTest Statistics for Standardized Rotated Factor Loadings\n\n\n       lhs op rhs std.loading    se      z     p ci.lower ci.upper\n1  factor1 =~  x1       0.353 0.062  5.720 0.000    0.232    0.473\n2  factor1 =~  x2       0.158 0.066  2.375 0.018    0.028    0.288\n3  factor1 =~  x3       0.115 0.057  2.015 0.044    0.003    0.226\n4  factor1 =~  x4       0.844 0.027 30.814 0.000    0.790    0.898\n5  factor1 =~  x5       0.869 0.023 37.224 0.000    0.823    0.914\n6  factor1 =~  x6       0.823 0.024 35.026 0.000    0.777    0.869\n7  factor1 =~  x7       0.116 0.054  2.161 0.031    0.011    0.222\n8  factor1 =~  x8       0.104 0.054  1.914 0.056   -0.003    0.210\n9  factor1 =~  x9       0.202 0.059  3.403 0.001    0.086    0.319\n10 factor2 =~  x1       0.590 0.078  7.523 0.000    0.436    0.743\n11 factor2 =~  x2       0.474 0.068  6.935 0.000    0.340    0.608\n12 factor2 =~  x3       0.657 0.059 11.204 0.000    0.542    0.771\n13 factor2 =~  x4       0.072 0.041  1.748 0.080   -0.009    0.152\n14 factor2 =~  x5      -0.010 0.039 -0.263 0.793   -0.087    0.066\n15 factor2 =~  x6       0.122 0.040  3.008 0.003    0.042    0.201\n16 factor2 =~  x7      -0.071 0.048 -1.465 0.143   -0.166    0.024\n17 factor2 =~  x8       0.167 0.090  1.849 0.064   -0.010    0.343\n18 factor2 =~  x9       0.397 0.087  4.581 0.000    0.227    0.567\n19 factor3 =~  x1       0.124 0.071  1.740 0.082   -0.016    0.264\n20 factor3 =~  x2      -0.042 0.074 -0.574 0.566   -0.187    0.102\n21 factor3 =~  x3       0.114 0.054  2.093 0.036    0.007    0.221\n22 factor3 =~  x4       0.056 0.037  1.503 0.133   -0.017    0.128\n23 factor3 =~  x5       0.048 0.038  1.263 0.207   -0.027    0.123\n24 factor3 =~  x6       0.046 0.034  1.347 0.178   -0.021    0.112\n25 factor3 =~  x7       0.692 0.093  7.477 0.000    0.511    0.874\n26 factor3 =~  x8       0.702 0.084  8.331 0.000    0.537    0.867\n27 factor3 =~  x9       0.508 0.070  7.298 0.000    0.371    0.644\n       lhs op rhs std.loading    se      z     p ci.lower ci.upper\n1  factor1 =~  x1       0.353 0.062  5.720 0.000    0.232    0.473\n2  factor1 =~  x2       0.158 0.066  2.375 0.018    0.028    0.288\n3  factor1 =~  x3       0.115 0.057  2.015 0.044    0.003    0.226\n4  factor1 =~  x4       0.844 0.027 30.814 0.000    0.790    0.898\n5  factor1 =~  x5       0.869 0.023 37.224 0.000    0.823    0.914\n6  factor1 =~  x6       0.823 0.024 35.026 0.000    0.777    0.869\n7  factor1 =~  x7       0.116 0.054  2.161 0.031    0.011    0.222\n8  factor1 =~  x8       0.104 0.054  1.914 0.056   -0.003    0.210\n9  factor1 =~  x9       0.202 0.059  3.403 0.001    0.086    0.319\n10 factor2 =~  x1       0.590 0.078  7.523 0.000    0.436    0.743\n11 factor2 =~  x2       0.474 0.068  6.935 0.000    0.340    0.608\n12 factor2 =~  x3       0.657 0.059 11.204 0.000    0.542    0.771\n13 factor2 =~  x4       0.072 0.041  1.748 0.080   -0.009    0.152\n14 factor2 =~  x5      -0.010 0.039 -0.263 0.793   -0.087    0.066\n15 factor2 =~  x6       0.122 0.040  3.008 0.003    0.042    0.201\n16 factor2 =~  x7      -0.071 0.048 -1.465 0.143   -0.166    0.024\n17 factor2 =~  x8       0.167 0.090  1.849 0.064   -0.010    0.343\n18 factor2 =~  x9       0.397 0.087  4.581 0.000    0.227    0.567\n19 factor3 =~  x1       0.124 0.071  1.740 0.082   -0.016    0.264\n20 factor3 =~  x2      -0.042 0.074 -0.574 0.566   -0.187    0.102\n21 factor3 =~  x3       0.114 0.054  2.093 0.036    0.007    0.221\n22 factor3 =~  x4       0.056 0.037  1.503 0.133   -0.017    0.128\n23 factor3 =~  x5       0.048 0.038  1.263 0.207   -0.027    0.123\n24 factor3 =~  x6       0.046 0.034  1.347 0.178   -0.021    0.112\n25 factor3 =~  x7       0.692 0.093  7.477 0.000    0.511    0.874\n26 factor3 =~  x8       0.702 0.084  8.331 0.000    0.537    0.867\n27 factor3 =~  x9       0.508 0.070  7.298 0.000    0.371    0.644\n\n\n\n\n29.5.3 Oblique Quartimin\nL’algoritmo Quartimin può anche essere usato per una soluzione obliqua.\n\nout_oblq &lt;- oblqRotate(\n    unrotated, \n    method = \"quartimin\"\n)\nout &lt;- summary(out_oblq, sort = FALSE, suppress = 0.3)\nprint(out)\n\nStandardized Rotated Factor Loadings\n\n\n   factor1 factor2 factor3\nx1          0.602*        \nx2          0.505*        \nx3          0.689*        \nx4  0.840*                \nx5  0.888*                \nx6  0.808*                \nx7                  0.723*\nx8                  0.702*\nx9          0.366*  0.463*\n\nFactor Correlation\n        factor1 factor2 factor3\nfactor1   1.000   0.326   0.216\nfactor2   0.326   1.000   0.270\nfactor3   0.216   0.270   1.000\n\nMethod of rotation: Quartimin \n\nTest Statistics for Standardized Rotated Factor Loadings\n\n\n       lhs op rhs std.loading    se      z     p ci.lower ci.upper\n1  factor1 =~  x1       0.191 0.064  2.965 0.003    0.065    0.317\n2  factor1 =~  x2       0.044 0.066  0.665 0.506   -0.085    0.172\n3  factor1 =~  x3      -0.070 0.034 -2.031 0.042   -0.137   -0.002\n4  factor1 =~  x4       0.840 0.033 25.622 0.000    0.776    0.905\n5  factor1 =~  x5       0.888 0.027 32.583 0.000    0.835    0.942\n6  factor1 =~  x6       0.808 0.028 28.441 0.000    0.752    0.863\n7  factor1 =~  x7       0.044 0.037  1.179 0.238   -0.029    0.116\n8  factor1 =~  x8      -0.033 0.036 -0.916 0.360   -0.103    0.037\n9  factor1 =~  x9       0.035 0.048  0.728 0.467   -0.059    0.129\n10 factor2 =~  x1       0.602 0.086  7.003 0.000    0.434    0.771\n11 factor2 =~  x2       0.505 0.071  7.163 0.000    0.367    0.644\n12 factor2 =~  x3       0.689 0.056 12.344 0.000    0.580    0.799\n13 factor2 =~  x4       0.022 0.045  0.483 0.629   -0.067    0.110\n14 factor2 =~  x5      -0.067 0.036 -1.890 0.059   -0.137    0.002\n15 factor2 =~  x6       0.078 0.041  1.887 0.059   -0.003    0.158\n16 factor2 =~  x7      -0.152 0.037 -4.059 0.000   -0.225   -0.078\n17 factor2 =~  x8       0.104 0.109  0.960 0.337   -0.109    0.317\n18 factor2 =~  x9       0.366 0.097  3.780 0.000    0.176    0.556\n19 factor3 =~  x1       0.031 0.062  0.500 0.617   -0.090    0.152\n20 factor3 =~  x2      -0.117 0.066 -1.776 0.076   -0.245    0.012\n21 factor3 =~  x3       0.023 0.039  0.587 0.557   -0.054    0.100\n22 factor3 =~  x4       0.005 0.042  0.128 0.898   -0.076    0.087\n23 factor3 =~  x5       0.008 0.035  0.216 0.829   -0.061    0.076\n24 factor3 =~  x6      -0.011 0.030 -0.362 0.717   -0.070    0.048\n25 factor3 =~  x7       0.723 0.087  8.328 0.000    0.553    0.893\n26 factor3 =~  x8       0.702 0.098  7.137 0.000    0.509    0.894\n27 factor3 =~  x9       0.463 0.075  6.211 0.000    0.317    0.609\n       lhs op rhs std.loading    se      z     p ci.lower ci.upper\n1  factor1 =~  x1       0.191 0.064  2.965 0.003    0.065    0.317\n2  factor1 =~  x2       0.044 0.066  0.665 0.506   -0.085    0.172\n3  factor1 =~  x3      -0.070 0.034 -2.031 0.042   -0.137   -0.002\n4  factor1 =~  x4       0.840 0.033 25.622 0.000    0.776    0.905\n5  factor1 =~  x5       0.888 0.027 32.583 0.000    0.835    0.942\n6  factor1 =~  x6       0.808 0.028 28.441 0.000    0.752    0.863\n7  factor1 =~  x7       0.044 0.037  1.179 0.238   -0.029    0.116\n8  factor1 =~  x8      -0.033 0.036 -0.916 0.360   -0.103    0.037\n9  factor1 =~  x9       0.035 0.048  0.728 0.467   -0.059    0.129\n10 factor2 =~  x1       0.602 0.086  7.003 0.000    0.434    0.771\n11 factor2 =~  x2       0.505 0.071  7.163 0.000    0.367    0.644\n12 factor2 =~  x3       0.689 0.056 12.344 0.000    0.580    0.799\n13 factor2 =~  x4       0.022 0.045  0.483 0.629   -0.067    0.110\n14 factor2 =~  x5      -0.067 0.036 -1.890 0.059   -0.137    0.002\n15 factor2 =~  x6       0.078 0.041  1.887 0.059   -0.003    0.158\n16 factor2 =~  x7      -0.152 0.037 -4.059 0.000   -0.225   -0.078\n17 factor2 =~  x8       0.104 0.109  0.960 0.337   -0.109    0.317\n18 factor2 =~  x9       0.366 0.097  3.780 0.000    0.176    0.556\n19 factor3 =~  x1       0.031 0.062  0.500 0.617   -0.090    0.152\n20 factor3 =~  x2      -0.117 0.066 -1.776 0.076   -0.245    0.012\n21 factor3 =~  x3       0.023 0.039  0.587 0.557   -0.054    0.100\n22 factor3 =~  x4       0.005 0.042  0.128 0.898   -0.076    0.087\n23 factor3 =~  x5       0.008 0.035  0.216 0.829   -0.061    0.076\n24 factor3 =~  x6      -0.011 0.030 -0.362 0.717   -0.070    0.048\n25 factor3 =~  x7       0.723 0.087  8.328 0.000    0.553    0.893\n26 factor3 =~  x8       0.702 0.098  7.137 0.000    0.509    0.894\n27 factor3 =~  x9       0.463 0.075  6.211 0.000    0.317    0.609\n\n\n\n\n29.5.4 Orthogonal Geomin\nConsideriamo ora la rotazione Geomin. L’algoritmo Geomin fornisce un metodo di rotazione che riduce al minimo la media geometrica delle saturazioni fattoriali innalzate al quadrato. Qui è usato per ottenere una soluzione ortogonale.\n\nout_geomin_orh &lt;- orthRotate(\n    unrotated, \n    method = \"geomin\"\n)\nout &lt;- summary(out_geomin_orh, sort = FALSE, suppress = 0.3)\nprint(out)\n\nStandardized Rotated Factor Loadings\n\n\n   factor1 factor2 factor3\nx1  0.315*         -0.621*\nx2                 -0.474*\nx3                 -0.671*\nx4  0.838*                \nx5  0.867*                \nx6  0.814*                \nx7          0.696*        \nx8          0.677*        \nx9          0.456* -0.468*\n\nFactor Correlation\n        factor1 factor2 factor3\nfactor1       1       0       0\nfactor2       0       1       0\nfactor3       0       0       1\n\nMethod of rotation: Geomin \n\nTest Statistics for Standardized Rotated Factor Loadings\n\n\n       lhs op rhs std.loading    se       z     p ci.lower ci.upper\n1  factor1 =~  x1       0.315 0.134   2.352 0.019    0.053    0.578\n2  factor1 =~  x2       0.130 0.118   1.102 0.271   -0.101    0.360\n3  factor1 =~  x3       0.074 0.123   0.599 0.549   -0.168    0.316\n4  factor1 =~  x4       0.838 0.036  23.402 0.000    0.767    0.908\n5  factor1 =~  x5       0.867 0.025  35.190 0.000    0.819    0.915\n6  factor1 =~  x6       0.814 0.040  20.427 0.000    0.736    0.892\n7  factor1 =~  x7       0.112 0.068   1.645 0.100   -0.021    0.245\n8  factor1 =~  x8       0.085 0.063   1.341 0.180   -0.039    0.209\n9  factor1 =~  x9       0.172 0.095   1.803 0.071   -0.015    0.359\n10 factor2 =~  x1       0.053 0.061   0.868 0.386   -0.067    0.173\n11 factor2 =~  x2      -0.099 0.070  -1.409 0.159   -0.237    0.039\n12 factor2 =~  x3       0.033 0.048   0.698 0.485   -0.060    0.127\n13 factor2 =~  x4       0.051 0.046   1.115 0.265   -0.039    0.140\n14 factor2 =~  x5       0.054 0.061   0.878 0.380   -0.066    0.173\n15 factor2 =~  x6       0.035 0.037   0.924 0.355   -0.039    0.108\n16 factor2 =~  x7       0.696 0.090   7.702 0.000    0.519    0.874\n17 factor2 =~  x8       0.677 0.088   7.660 0.000    0.504    0.850\n18 factor2 =~  x9       0.456 0.073   6.261 0.000    0.314    0.599\n19 factor3 =~  x1      -0.621 0.101  -6.148 0.000   -0.818   -0.423\n20 factor3 =~  x2      -0.474 0.074  -6.414 0.000   -0.619   -0.329\n21 factor3 =~  x3      -0.671 0.058 -11.658 0.000   -0.784   -0.558\n22 factor3 =~  x4      -0.129 0.161  -0.800 0.424   -0.445    0.187\n23 factor3 =~  x5      -0.048 0.174  -0.277 0.782   -0.389    0.293\n24 factor3 =~  x6      -0.176 0.164  -1.071 0.284   -0.497    0.146\n25 factor3 =~  x7      -0.021 0.059  -0.354 0.724   -0.137    0.095\n26 factor3 =~  x8      -0.257 0.094  -2.720 0.007   -0.442   -0.072\n27 factor3 =~  x9      -0.468 0.105  -4.472 0.000   -0.673   -0.263\n       lhs op rhs std.loading    se       z     p ci.lower ci.upper\n1  factor1 =~  x1       0.315 0.134   2.352 0.019    0.053    0.578\n2  factor1 =~  x2       0.130 0.118   1.102 0.271   -0.101    0.360\n3  factor1 =~  x3       0.074 0.123   0.599 0.549   -0.168    0.316\n4  factor1 =~  x4       0.838 0.036  23.402 0.000    0.767    0.908\n5  factor1 =~  x5       0.867 0.025  35.190 0.000    0.819    0.915\n6  factor1 =~  x6       0.814 0.040  20.427 0.000    0.736    0.892\n7  factor1 =~  x7       0.112 0.068   1.645 0.100   -0.021    0.245\n8  factor1 =~  x8       0.085 0.063   1.341 0.180   -0.039    0.209\n9  factor1 =~  x9       0.172 0.095   1.803 0.071   -0.015    0.359\n10 factor2 =~  x1       0.053 0.061   0.868 0.386   -0.067    0.173\n11 factor2 =~  x2      -0.099 0.070  -1.409 0.159   -0.237    0.039\n12 factor2 =~  x3       0.033 0.048   0.698 0.485   -0.060    0.127\n13 factor2 =~  x4       0.051 0.046   1.115 0.265   -0.039    0.140\n14 factor2 =~  x5       0.054 0.061   0.878 0.380   -0.066    0.173\n15 factor2 =~  x6       0.035 0.037   0.924 0.355   -0.039    0.108\n16 factor2 =~  x7       0.696 0.090   7.702 0.000    0.519    0.874\n17 factor2 =~  x8       0.677 0.088   7.660 0.000    0.504    0.850\n18 factor2 =~  x9       0.456 0.073   6.261 0.000    0.314    0.599\n19 factor3 =~  x1      -0.621 0.101  -6.148 0.000   -0.818   -0.423\n20 factor3 =~  x2      -0.474 0.074  -6.414 0.000   -0.619   -0.329\n21 factor3 =~  x3      -0.671 0.058 -11.658 0.000   -0.784   -0.558\n22 factor3 =~  x4      -0.129 0.161  -0.800 0.424   -0.445    0.187\n23 factor3 =~  x5      -0.048 0.174  -0.277 0.782   -0.389    0.293\n24 factor3 =~  x6      -0.176 0.164  -1.071 0.284   -0.497    0.146\n25 factor3 =~  x7      -0.021 0.059  -0.354 0.724   -0.137    0.095\n26 factor3 =~  x8      -0.257 0.094  -2.720 0.007   -0.442   -0.072\n27 factor3 =~  x9      -0.468 0.105  -4.472 0.000   -0.673   -0.263\n\n\n\n\n29.5.5 Oblique Geomin\nLa rotazione Geomin può anche essere usata per ottenere una soluzione obliqua.\n\nout_geomin_obl &lt;- oblqRotate(\n    unrotated, \n    method = \"geomin\"\n)\nout &lt;- summary(out_geomin_obl, sort = FALSE, suppress = 0.3)\nprint(out)\n\nStandardized Rotated Factor Loadings\n\n\n   factor1 factor2 factor3\nx1                 -0.604*\nx2                 -0.507*\nx3                 -0.691*\nx4  0.839*                \nx5  0.887*                \nx6  0.806*                \nx7          0.726*        \nx8          0.703*        \nx9          0.463* -0.368*\n\nFactor Correlation\n        factor1 factor2 factor3\nfactor1   1.000   0.230  -0.327\nfactor2   0.230   1.000  -0.278\nfactor3  -0.327  -0.278   1.000\n\nMethod of rotation: Geomin \n\nTest Statistics for Standardized Rotated Factor Loadings\n\n\n       lhs op rhs std.loading    se       z     p ci.lower ci.upper\n1  factor1 =~  x1       0.188 0.070   2.670 0.008    0.050    0.326\n2  factor1 =~  x2       0.044 0.054   0.806 0.420   -0.063    0.150\n3  factor1 =~  x3      -0.073 0.049  -1.467 0.142   -0.170    0.024\n4  factor1 =~  x4       0.839 0.032  26.467 0.000    0.777    0.901\n5  factor1 =~  x5       0.887 0.029  30.077 0.000    0.829    0.945\n6  factor1 =~  x6       0.806 0.030  26.717 0.000    0.747    0.865\n7  factor1 =~  x7       0.031 0.034   0.915 0.360   -0.036    0.099\n8  factor1 =~  x8      -0.045 0.048  -0.948 0.343   -0.139    0.048\n9  factor1 =~  x9       0.025 0.034   0.747 0.455   -0.041    0.091\n10 factor2 =~  x1       0.029 0.051   0.567 0.571   -0.071    0.129\n11 factor2 =~  x2      -0.119 0.072  -1.664 0.096   -0.260    0.021\n12 factor2 =~  x3       0.020 0.037   0.538 0.591   -0.053    0.092\n13 factor2 =~  x4       0.007 0.043   0.174 0.862   -0.076    0.091\n14 factor2 =~  x5       0.010 0.036   0.285 0.775   -0.060    0.080\n15 factor2 =~  x6      -0.009 0.030  -0.309 0.758   -0.068    0.049\n16 factor2 =~  x7       0.726 0.072  10.065 0.000    0.585    0.868\n17 factor2 =~  x8       0.703 0.118   5.963 0.000    0.472    0.934\n18 factor2 =~  x9       0.463 0.080   5.810 0.000    0.307    0.619\n19 factor3 =~  x1      -0.604 0.081  -7.438 0.000   -0.763   -0.445\n20 factor3 =~  x2      -0.507 0.073  -6.983 0.000   -0.649   -0.364\n21 factor3 =~  x3      -0.691 0.061 -11.373 0.000   -0.810   -0.572\n22 factor3 =~  x4      -0.024 0.034  -0.702 0.482   -0.091    0.043\n23 factor3 =~  x5       0.065 0.045   1.459 0.144   -0.022    0.153\n24 factor3 =~  x6      -0.080 0.048  -1.679 0.093   -0.173    0.013\n25 factor3 =~  x7       0.150 0.107   1.403 0.161   -0.060    0.360\n26 factor3 =~  x8      -0.106 0.164  -0.645 0.519   -0.428    0.216\n27 factor3 =~  x9      -0.368 0.133  -2.770 0.006   -0.629   -0.108\n       lhs op rhs std.loading    se       z     p ci.lower ci.upper\n1  factor1 =~  x1       0.188 0.070   2.670 0.008    0.050    0.326\n2  factor1 =~  x2       0.044 0.054   0.806 0.420   -0.063    0.150\n3  factor1 =~  x3      -0.073 0.049  -1.467 0.142   -0.170    0.024\n4  factor1 =~  x4       0.839 0.032  26.467 0.000    0.777    0.901\n5  factor1 =~  x5       0.887 0.029  30.077 0.000    0.829    0.945\n6  factor1 =~  x6       0.806 0.030  26.717 0.000    0.747    0.865\n7  factor1 =~  x7       0.031 0.034   0.915 0.360   -0.036    0.099\n8  factor1 =~  x8      -0.045 0.048  -0.948 0.343   -0.139    0.048\n9  factor1 =~  x9       0.025 0.034   0.747 0.455   -0.041    0.091\n10 factor2 =~  x1       0.029 0.051   0.567 0.571   -0.071    0.129\n11 factor2 =~  x2      -0.119 0.072  -1.664 0.096   -0.260    0.021\n12 factor2 =~  x3       0.020 0.037   0.538 0.591   -0.053    0.092\n13 factor2 =~  x4       0.007 0.043   0.174 0.862   -0.076    0.091\n14 factor2 =~  x5       0.010 0.036   0.285 0.775   -0.060    0.080\n15 factor2 =~  x6      -0.009 0.030  -0.309 0.758   -0.068    0.049\n16 factor2 =~  x7       0.726 0.072  10.065 0.000    0.585    0.868\n17 factor2 =~  x8       0.703 0.118   5.963 0.000    0.472    0.934\n18 factor2 =~  x9       0.463 0.080   5.810 0.000    0.307    0.619\n19 factor3 =~  x1      -0.604 0.081  -7.438 0.000   -0.763   -0.445\n20 factor3 =~  x2      -0.507 0.073  -6.983 0.000   -0.649   -0.364\n21 factor3 =~  x3      -0.691 0.061 -11.373 0.000   -0.810   -0.572\n22 factor3 =~  x4      -0.024 0.034  -0.702 0.482   -0.091    0.043\n23 factor3 =~  x5       0.065 0.045   1.459 0.144   -0.022    0.153\n24 factor3 =~  x6      -0.080 0.048  -1.679 0.093   -0.173    0.013\n25 factor3 =~  x7       0.150 0.107   1.403 0.161   -0.060    0.360\n26 factor3 =~  x8      -0.106 0.164  -0.645 0.519   -0.428    0.216\n27 factor3 =~  x9      -0.368 0.133  -2.770 0.006   -0.629   -0.108",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>La rotazione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/04_rotazione.html#interpretazione-dei-fattori-latenti-nellanalisi-fattoriale",
    "href": "chapters/extraction/04_rotazione.html#interpretazione-dei-fattori-latenti-nellanalisi-fattoriale",
    "title": "29  La rotazione fattoriale",
    "section": "\n29.8 Interpretazione dei fattori latenti nell’analisi fattoriale",
    "text": "29.8 Interpretazione dei fattori latenti nell’analisi fattoriale\nL’interpretazione dei fattori è un passaggio cruciale dell’analisi fattoriale. È importante comprendere la differenza tra:\n\n\nMatrice Pattern:\n\nmostra le saturazioni dirette, come coefficienti di regressione parziali;\n\nindica quanto un fattore influenza una data variabile, al netto degli altri fattori;\nè particolarmente utile per l’etichettamento dei fattori, perché fa vedere “quale fattore spiega cosa” in modo più puro.\n\n\n\nMatrice di Struttura:\n\nmostra le correlazioni tra fattori e variabili;\n\nincludendo anche gli effetti indiretti dovuti alle correlazioni tra i fattori (in caso di rotazione obliqua), fornisce un quadro della covariazione complessiva;\n\npuò essere più utile rispetto alla Pattern quando ci sono correlazioni tra fattori, ma non fornisce sempre l’interpretazione “pulita” delle relazioni dirette.\n\n\n\nIn molti manuali di psicologia e di analisi dei dati (Tabachnick & Fidell, 2001; Hair et al., 2010), si consiglia di utilizzare in primo luogo la matrice Pattern per assegnare le etichette fattoriali, poiché i loadings diretti riflettono il contributo specifico di ciascun fattore. In ogni caso, la matrice di Struttura rimane utile per capire come varia ciascuna variabile in relazione complessiva con ogni fattore, comprendendo anche eventuali effetti condivisi tra i fattori.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>La rotazione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/04_rotazione.html#session-info",
    "href": "chapters/extraction/04_rotazione.html#session-info",
    "title": "29  La rotazione fattoriale",
    "section": "\n29.10 Session Info",
    "text": "29.10 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.4\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] GPArotation_2024.3-1 ggokabeito_0.1.0     see_0.11.0          \n#&gt;  [4] MASS_7.3-65          viridis_0.6.5        viridisLite_0.4.2   \n#&gt;  [7] ggpubr_0.6.0         ggExtra_0.10.1       gridExtra_2.3       \n#&gt; [10] patchwork_1.3.0      bayesplot_1.11.1     semTools_0.5-7      \n#&gt; [13] semPlot_1.1.6        lavaan_0.6-19        psych_2.5.3         \n#&gt; [16] scales_1.3.0         markdown_2.0         knitr_1.50          \n#&gt; [19] lubridate_1.9.4      forcats_1.0.0        stringr_1.5.1       \n#&gt; [22] dplyr_1.1.4          purrr_1.0.4          readr_2.1.5         \n#&gt; [25] tidyr_1.3.1          tibble_3.2.1         ggplot2_3.5.2       \n#&gt; [28] tidyverse_2.0.0      here_1.0.1          \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_2.0.0      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.2.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.8         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.11.0      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] mime_0.13           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [25] Matrix_1.7-3        R6_2.6.1            fastmap_1.2.0      \n#&gt;  [28] rbibutils_2.3       shiny_1.10.0        numDeriv_2016.8-1.1\n#&gt;  [31] digest_0.6.37       OpenMx_2.21.13      fdrtool_1.2.18     \n#&gt;  [34] colorspace_2.1-1    rprojroot_2.0.4     Hmisc_5.2-3        \n#&gt;  [37] timechange_0.3.0    abind_1.4-8         compiler_4.4.2     \n#&gt;  [40] withr_3.0.2         glasso_1.11         htmlTable_2.4.3    \n#&gt;  [43] backports_1.5.0     carData_3.0-5       ggsignif_0.6.4     \n#&gt;  [46] corpcor_1.6.10      gtools_3.9.5        tools_4.4.2        \n#&gt;  [49] pbivnorm_0.6.0      foreign_0.8-90      zip_2.3.2          \n#&gt;  [52] httpuv_1.6.15       nnet_7.3-20         glue_1.8.0         \n#&gt;  [55] quadprog_1.5-8      nlme_3.1-168        promises_1.3.2     \n#&gt;  [58] lisrelToR_0.3       grid_4.4.2          checkmate_2.3.2    \n#&gt;  [61] cluster_2.1.8.1     reshape2_1.4.4      generics_0.1.3     \n#&gt;  [64] gtable_0.3.6        tzdb_0.5.0          data.table_1.17.0  \n#&gt;  [67] hms_1.1.3           car_3.1-3           sem_3.1-16         \n#&gt;  [70] pillar_1.10.2       rockchalk_1.8.157   later_1.4.2        \n#&gt;  [73] splines_4.4.2       lattice_0.22-7      survival_3.8-3     \n#&gt;  [76] kutils_1.73         tidyselect_1.2.1    miniUI_0.1.1.1     \n#&gt;  [79] pbapply_1.7-2       reformulas_0.4.0    stats4_4.4.2       \n#&gt;  [82] xfun_0.52           qgraph_1.9.8        arm_1.14-4         \n#&gt;  [85] stringi_1.8.7       pacman_0.5.1        boot_1.3-31        \n#&gt;  [88] evaluate_1.0.3      codetools_0.2-20    mi_1.1             \n#&gt;  [91] cli_3.6.4           RcppParallel_5.1.10 rpart_4.1.24       \n#&gt;  [94] xtable_1.8-4        Rdpack_2.6.4        munsell_0.5.1      \n#&gt;  [97] Rcpp_1.0.14         coda_0.19-4.1       png_0.1-8          \n#&gt; [100] XML_3.99-0.18       parallel_4.4.2      jpeg_0.1-11        \n#&gt; [103] lme4_1.1-37         mvtnorm_1.3-3       openxlsx_4.2.8     \n#&gt; [106] rlang_1.1.5         multcomp_1.4-28     mnormt_2.1.1\n\n\n\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.\n\n\nRencher, A. (2002). Methods of multivariate analysis. 2002. Wiley Publications.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>La rotazione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/05_val_soluzione.html",
    "href": "chapters/extraction/05_val_soluzione.html",
    "title": "30  Valutare e rifinire la soluzione fattoriale",
    "section": "",
    "text": "30.1 Valutazione della matrice pattern\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nLa maggior parte di strumenti usati nell’assessment psicologico e neuropsicologico non valuta una singola dimensione psicologica, ma piuttosto misura molteplici aspetti di un costrutto. Di conseguenza, l’analisi fattoriale produce solitamente una soluzione a più fattori. Idealmente, dopo la rotazione, ciascun item saturerà fortemente su un singolo fattore e debolmente sugli altri. In realtà, anche dopo la rotazione degli assi fattoriali, spesso si presentano item che saturano debolmente su tutti i fattori, oppure item che saturano fortemente su più di un fattore.\nUno dei primi passi da compiere per rifinire la soluzione fattoriale è quello di valutare la matrice struttura e intervenire utilizzando il criterio della “struttura semplice”, per poi valutare gli effetti delle azioni intraprese (es., eliminazione di alcuni item) nella matrice pattern. Ricordiamo che la matrice struttura contiene le correlazioni tra item e fattori, mentre la matrice pattern contiene le saturazioni fattoriali.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Valutare e rifinire la soluzione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/05_val_soluzione.html#valutazione-della-matrice-pattern",
    "href": "chapters/extraction/05_val_soluzione.html#valutazione-della-matrice-pattern",
    "title": "30  Valutare e rifinire la soluzione fattoriale",
    "section": "",
    "text": "30.1.1 Item con basse saturazioni su tutti i fattori\nPrima di procedere con l’analisi fattoriale è auspicabile esaminare la matrice di correlazioni tra gli item ed eliminare quegli item che sono insufficientemente correlati con gli altri item della matrice. Tuttavia, anche dopo questo screening iniziale, è possibile che vi siano item caratterizzati da saturazioni basse su tutti i fattori. Dal punto di vista pratico, si considerano “basse” le saturazioni il cui valore assoluto è minore di 0.30 (Hair et al., 1995). Hair e collaboratori suggeriscono due soluzioni nel caso di item con saturazioni basse su tutti i fattori:\n\neliminare gli item con basse saturazioni,\nvalutare le comunalità degli item problematici e il contributo specifico che forniscono allo strumento.\n\nSe un item ha una bassa comunalità, o se il contributo di un item nei confronti del significato generale dello strumento è di poca importanza, allora l’item dovrebbe essere eliminato. Dopo l’eliminazione degli item critici, si procede calcolando una nuova soluzione fattoriale e si esaminano i risultati ottenuti.\nSe vi sono degli item con basse saturazioni su tutti i fattori che però contribuiscono in maniera importante a determinare il significato della scala nel suo complesso, allora questi item dovrebbero essere mantenuti. Alle volte, per tali item è possibile creare delle sottoscale separate dalle altre.\n\n\n30.1.2 Item con saturazioni evevate su più di un fattore\nÈ comune anche il caso opposto, ovvero quello nel quale ci sono item che saturano su fattori multipli (con saturazioni fattoriali \\(&gt;\\) .30), specialmente nel caso di soluzioni fattoriali ottenutie dopo una rotazione obliqua. Kline (2000) suggerisce di eliminare tali item in quanto rendono difficile da interpretare il significato della scala che così si ottiene. Hair e collaboratori (1995) ritengono invece che tali item dovrebbero essere mantenuti, dato possono chiarire il significato dei fattori che la scala identifica.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Valutare e rifinire la soluzione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/05_val_soluzione.html#valutazione-dellattendibilità",
    "href": "chapters/extraction/05_val_soluzione.html#valutazione-dellattendibilità",
    "title": "30  Valutare e rifinire la soluzione fattoriale",
    "section": "30.2 Valutazione dell’attendibilità",
    "text": "30.2 Valutazione dell’attendibilità\nAll’interno del problema della costruzione di uno strumento vengono esaminati tre aspetti dell’attendibilità: la consistenza interna, la stabilità e l’equivalenza.\n\n30.2.1 Consistenza interna\n\n30.2.1.1 La procedura split-half\nLa consistenza interna misura il grado di coerenza tra gli item che costituiscono lo strumento o le sottoscale dello strumento. Se tutti gli item che costituiscono uno strumento o una sua sottoscala misurano la stessa cosa, allora saranno fortemente associati tra loro.\nÈ possibile misurare la consistenza interna con il metodo dello split-half, ovvero mediante la correlazione di Pearson tra i punteggi ottenuti utilizzando ciascuna delle due metà degli item dello strumento. Usando un software, è meglio trovare la media delle correlazioni inter-item ricavabili a partire da tutte le possibili divisioni a metà dell’insieme di item che costituiscono lo strumento. La correlazione trovata in questo modo viene poi corretta utilizzando la formula “profetica” di Spearman-Brown per tenere in considerazione il fatto che l’attendibilità è stata calcolata utilizzando soltanto metà degli item dello strumento.\nSi noti che la formula di Spearman-Brown è basata sull’assunzione che le due metà dello strumento siano parallele, ovvero che abbiano identici punteggi veri e uguali varianze d’errore (questa assunzione comporta la conseguenza per cui le due metà degli item devono producono punteggi aventi la stessa media e la stessa varianza). Se queste assunzioni molto stringenti non vengono soddisfatte, allora la procedura descritta sopra conduce ad una sovrastima dell’attendibilità quale consisenza interna della scala.\n\n\n30.2.1.2 L’analisi della varianza\nSe tutti gli item di uno strumento o di una sottoscala sono espressione dello stesso costrutto, allora ci dobbiamo aspettare che anche le medie dei punteggi sugli item siano uguali. Come è stato detto sopra, questa è infatti una delle assunzioni delle forme strettamente parallele di un test. È dunque possibile verificare questa assunzione mediante un’ANOVA che sottopone a test l’ipotesi nulla dell’uguaglianza delle medie di gruppi. Nel caso degli item di un test, dato che ciascun soggetto completa tutti gli item che costituiscono lo strumento, è appropriato usare un’ANOVA per misure ripetute che, nella sua declinazione più moderna, corrisponde ad un modello multi-livello (mixed-effect model).\n\n\n30.2.1.3 L’indice \\(\\alpha\\) di Cronbach\nL’indice \\(\\alpha\\) di Cronbach è comunque la misura più utilizzata per valutare l’attendibilità quale consistenza interna di uno strumento. L’\\(\\alpha\\) di Cronbach è stato interpretato come la proporzione di varianza della scala che può essere attribuita al fattore comune (DeVellis, 1991). Può anche essere interpretato come la correlazione stimata tra i punteggi della scala e un’altro strumento della stessa lunghezza tratto dall’universo degli item possibili che costituiscono il dominio del costrutto (Kline, 1986). La radice quadrata del coefficiente \\(\\alpha\\) di Cronbach rappresenta la correlazione stimata tra i punteggi ottenuti tramite lo strumento e i punteggi veri (Nunnally & Bernstein, 1994).\nIn precedenza abbiamo descritto una serie di limiti del coefficiente \\(\\alpha\\) di Cronbach. In generale, molti ricercatori suggeriscono di usare al suo posto l’indice \\(\\omega\\) di McDonald.\n\n\n\n30.2.2 Stabilità temporale\nLa stabilità temporale viene valutata attraverso la procedura di test-retest. La correlazione tra le misure ottenute in due momenti negli stessi rispondenti ci fornisce l’attendibilità di test-retest.\nKline (2000) ha messo in evidenza come l’attendibilità di test-retest sia influenzata da molteplici fattori, tra cui le caratteristiche del campione, la maturità dei rispondenti, i cambiamenti nello stato emozionale, le differenze nelle condizioni di somministrazione del test, la possibilità di ricordare le risposte date in precedenza, la difficoltà degli item, la grandezza del campione e le caratteristiche del costrutto (ad esempio, stato vs. tratto).\nParticolare attenzione deve essere rivolta all’intervallo temporale usato nella procedura di test-retest. Se il periodo di tempo che intercorre tra le due somministrazioni è troppo corto, i risultati possono risultare distorti a causa del fatto che i soggetti si ricordano le risposte date in precedenza. Questo può condurre ad una sovrastima dell’attendibilità test-retest (Pedhazur & Schmelkin, 1991). Un intervallo temporale troppo lungo tra le due somministrazioni ha invece come limite il fatto che, in questo caso, vi è un’alta possibilità che intervengano dei cambiamenti nei rispondenti rispetto al costrutto in esame. Alla luce di queste considerazioni è stato suggerito di utilizzare un intervallo temporale abbastanza breve, ovvero di una o due settimane (Nunnally & Bernstein, 1994; Pedhazur & Schmelkin, 1991). Se è necessario valutare la stabilità temporale nel corso di un lungo arco temporale, Nunnally e Bernstein (1994) suggeriscono di utilizzare un intervallo di sei mesi o maggiore.\n\n\n30.2.3 Equivalenza\nPer cercare di evitare i problemi associati all’attendibilità quale stabilità temporale, alcuni autori si sono posti il problema di esaminare la correlazione tra forme parallele (o equivalenti) dello strumento. La correlazione tra forme parallele di uno strumento va sotto il nome di coefficiente di equivalenza e fornisce una misura alternativa dell’attendibilità dello strumento (Burns & Grove, 2001; Pedhazur & Schmelkin, 1991; Polit & Hungler, 1999).\nNunnally e Bernstein (1994) suggeriscono di confrontare i risultati ottenuti con la somministrazione delle forme parallele lo stesso giorno con quelli ottenuti nel caso di un intervallo temporale di due settimane. Kline (2000) ritiene che l’attendibilità tra due forme parallele debba essere di almeno 0.9 perché, per valori inferiori, sarebbe difficile sostenere che le forme sono veramente parallele.\nÈ tuttavia molto oneroso predisporre due forme parallele di uno strumento. Per questa ragione, il coefficiente di equivalenza viene raramente usato.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Valutare e rifinire la soluzione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/05_val_soluzione.html#selezione-di-un-sottoinsieme-di-item",
    "href": "chapters/extraction/05_val_soluzione.html#selezione-di-un-sottoinsieme-di-item",
    "title": "30  Valutare e rifinire la soluzione fattoriale",
    "section": "30.3 Selezione di un sottoinsieme di item",
    "text": "30.3 Selezione di un sottoinsieme di item\nTipicamente, la costruzione di un test viene realizzata somministrando un grande numero di item per poi selezionare gli item “migliori” che andranno a fare parte del test vero e proprio. Si supponga di somministrare inizialmente \\(m\\) item, quando si desidera che il test finale sia costituito da \\(p &lt; m\\) item. Un modo di affrontare questo problema potrebbe essere quello di calcolare l’attendibilità del test (coefficiente \\(\\omega\\)) per tutti i possibili sottoinsiemi di \\(p\\) item, così da individuare il sottoinsieme migliore. Questo modo di procedere, però, è problematico perché richiede la valutazione di un elevatissimo numero di possibilità. Per esempio, da un insieme iniziale neanche troppo numeroso di 100 item, il numero di sottoinsiemi di 20 item è uguale a\n\\[\n\\binom{100}{20} = 5.36 \\times 10^{20}.\n\\]\nÈ dunque necessario trovare metodi alternativi che evitino una tale esplosione combinatoria. A questo fine, ovvero per procedere alla selezione del sottoinsieme dei “migliori” item, McDonald (2013) suggerisce di calcolare la quantità di informazione di ciascun item. La quantità di informazione di un item è definita come rapporto tra segnale/rumore, in relazione alla scomposizione della varianza dell’item:\n\\[\n\\frac{\\lambda_i^2}{\\psi_{ii}}.\n\\]\nMcDonald (2013) mostra come l’omissione di uno o più item produce sempre una riduzione dell’attendibilità del test (ovvero, una riduzione nel valore del coefficiente \\(\\omega\\)). Tuttavia, tale riduzione è tanto più piccola quanto più piccola è la quantità di informazione degli item omessi. Il processo di selezione degli item può dunque essere guidato da un semplice principio: si selezionano gli item aventi la quantità di informazione maggiore. Ovvero, in altre parole, si rimuovono gli item aventi la quantità di informazione più bassa.\nEsempio. Per fare un esempio, consideriamo nuovamente la matrice di varianze e di covarianze della scala SWLS.\n\nvarnames &lt;- c(\"Y1\", \"Y2\", \"Y3\", \"Y4\", \"Y5\")\nSWLS &lt;- matrix(c(\n  2.565, 1.424, 1.481, 1.328, 1.529,\n  1.424, 2.493, 1.267, 1.051, 1.308,\n  1.481, 1.267, 2.462, 1.093, 1.360,\n  1.328, 1.051, 1.093, 2.769, 1.128,\n  1.529, 1.308, 1.360, 1.128, 3.355\n),\nncol = 5, byrow = TRUE,\ndimnames = list(varnames, varnames)\n)\nSWLS\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nY1\nY2\nY3\nY4\nY5\n\n\n\n\nY1\n2.565\n1.424\n1.481\n1.328\n1.529\n\n\nY2\n1.424\n2.493\n1.267\n1.051\n1.308\n\n\nY3\n1.481\n1.267\n2.462\n1.093\n1.360\n\n\nY4\n1.328\n1.051\n1.093\n2.769\n1.128\n\n\nY5\n1.529\n1.308\n1.360\n1.128\n3.355\n\n\n\n\n\nUtilizzando la funzione cfa() contenuta nel pacchetto lavaan, il modello ad un fattore viene definito nel modo seguente.\n\nmod_1 &lt;- \"\n  F =~ Y1 + Y2 + Y3 + Y4 + Y5\n\"\n\nOtteniamo così una stima dei pesi fattoriali e delle unicità.\n\nfit &lt;- lavaan::cfa(\n  mod_1,\n  sample.cov = SWLS,\n  sample.nobs = 215,\n  std.lv = TRUE\n)\n\nCalcoliamo la quantità di informazione fornita da ciascun item. Iniziamo a estrarre dall’oggetto fit la matrice delle saturazioni fattoriali.\n\nlambda &lt;- inspect(fit, what=\"std\")$lambda\nlambda\n\n\nA lavaan.matrix: 5 x 1 of type dbl\n\n\n\nF\n\n\n\n\nY1\n0.8166956\n\n\nY2\n0.6941397\n\n\nY3\n0.7257827\n\n\nY4\n0.5905795\n\n\nY5\n0.6429117\n\n\n\n\n\nEstraiamo da fit le specificità.\n\ntheta &lt;- diag(inspect(fit, what=\"std\")$theta)\ntheta\n\nY10.333008280668232Y20.518170080229262Y30.473239478247548Y40.651215859909304Y50.586664570302436\n\n\nPossiamo ora calcolare quantità di informazione degli item facendo il rapporto tra ciascuna saturazione fattoriale innalzata al quadrato e la corrispondente specificità.\n\nfor (i in 1:5) {\n  print(lambda[i]^2 / theta[i])\n}\n\n      Y1 \n2.002928 \n       Y2 \n0.9298683 \n      Y3 \n1.113095 \n       Y4 \n0.5355891 \n       Y5 \n0.7045515 \n\n\nIl risultato ottenuto indica che il quarto item è il meno informativo e che il quinto item è il secondo meno informativo. Se un solo item deve essere eliminato, dunque, elimineremo il quarto item. Se devono essere eliminati due item, andranno eliminati il quarto e il quinto item.\n\n30.3.1 Attendibilità e numero di item\nDi quanto cambia l’attendibilità di uno strumento se viene variato il numero di item? Una risposta a questa domanda può essere fornita dalla formula profetica di Spearman-Brown. Supponiamo che nella formula di Spearman-Brown,\n\\[\n\\begin{equation}\n  \\rho_p = \\frac{p \\rho_1}{(p-1)\\rho_1 + 1},\n\\end{equation}\n\\tag{30.1}\\]\n\\(\\rho_1\\) rappresenti l’attendibilità di un test costituito da un certo numero di item. Se poniamo \\(p=2\\), l’Equazione 30.1 ci fornisce una stima dell’attendibilità che si otterrebbe raddoppiando il numero di item nel test. Valori di \\(p\\) minori di \\(1\\), invece, vengono usati per predire la diminuizione dell’attendibilità conseguente ad una diminuzione nel numero degli item del test.\nRicordiamo però che le predizioni della formula di Spearman-Brown sono accurate solo se la forma allungata o accorciata del test è parallela rispetto al test considerato. Per esempio, se ad un test con un coefficiente di attendibilità molto alto vengono aggiunti item aventi una bassa attendibilità, allora l’attendibilità del test allungato sarà minore di quella predetta dalla formula di Spearman-Brown.\nAnche se la formula di Spearman-Brown ha un ruolo centrale nella teoria classica dei test, si tenga conto che non rappresenta l’unico strumento che può essere utilizzato per valutare la relazione tra attendibilità e numero degli item del test. La quantità detta informazione dell’item (item information), formulata dai modelli IRT, consente di predire i cambiamenti nella qualità della misura a seguito dell’aggiunta o della cancellazione di un sottoinsieme di item.\nEsempio. Si consideri la scala SWLS. Chiediamoci come varia l’attendibilità della scala se il numero di item aumenta da 5 a 20. Poniamo che l’attendibilità della scala SWLS costituita da 5 item sia uguale a 0.824. Applicando la formula di Spearman-Brown otteniamo la stima seguente.\n\n(4 * 0.824) / ((4 - 1) * 0.824 + 1)\n\n0.949308755760369\n\n\nEsempio. Possiamo giungere al risultato precedente in un altro modo. Supponiamo che i 15 item aggiuntivi abbiano le stesse saturazioni fattoriali medie (\\(\\bar{\\lambda}\\)) e le stesse varianze specifiche medie (\\(\\bar{\\psi}\\)) rispetto agli item originali. Mediante gli item di cui disponiamo, stimiamo l’attendibilità di un “item medio” nel modo seguente\n\\[\n\\rho_1 = \\frac{\\bar{\\lambda}^2}{\\bar{\\lambda}^2 + \\bar{\\psi}},\n\\]\novvero otteniamo la stima di 0.48:\n\nrho_1 &lt;- mean(lambda)^2 / (mean(lambda)^2 + mean(theta)) \nrho_1\n\n0.484512352433458\n\n\nL’attendibilità predetta di un test costituito da 20 item sarà dunque uguale a\n\n(20 * rho_1) / ((20 - 1) * rho_1 + 1) \n\n0.949490393106468\n\n\nil che replica il risultato ottenuto precedentemente.\nEsempio. Un altro modo ancora per ottenere lo stesso risultato è quello di utilizzare un modello mono-fattoriale per item paralleli.\n\nmod_2 &lt;- \"\n  F =~ a*Y1 + a*Y2 + a*Y3 + a*Y4 + a*Y5\n  Y1 ~~ b*Y1\n  Y2 ~~ b*Y2\n  Y3 ~~ b*Y3\n  Y4 ~~ b*Y4\n  Y5 ~~ b*Y5\n\"\n\nAdattiamo il modello ai dati.\n\nfit2 &lt;- lavaan::cfa(\n  mod_2,\n  sample.cov = SWLS,\n  sample.nobs = 215,\n  std.lv = TRUE\n)\n\nEstraiamo dall’oggetto fit2 le saturazioni fattoriali.\n\nlambda &lt;- inspect(fit2, what=\"std\")$lambda\nlambda\n\n\nA lavaan.matrix: 5 x 1 of type dbl\n\n\n\nF\n\n\n\n\nY1\n0.6893938\n\n\nY2\n0.6893938\n\n\nY3\n0.6893938\n\n\nY4\n0.6893938\n\n\nY5\n0.6893938\n\n\n\n\n\nEstraiamo da fit2 le specificità.\n\ntheta &lt;- diag(inspect(fit2, what=\"std\")$theta)\ntheta\n\nY10.524736147775294Y20.524736147775294Y30.524736147775294Y40.524736147775294Y50.524736147775294\n\n\nCalcoliamo l’attendibilità dell’item “medio” usando \\(\\lambda\\) e \\(\\psi\\) (chiamato theta da lavaan).\n\nrho_1 &lt;- lambda[1]^2 / (lambda[1]^2 + theta[2])\nrho_1 \n\nY2: 0.475263852224706\n\n\nPosso ora applicare la formula di Spearman-Brown.\n\n(20 * rho_1) / ((20 - 1) * rho_1 + 1) \n\nY2: 0.94768340402785\n\n\nIl risultato è praticamente identico a quelli trovati in precedenza.\n\n\n30.3.2 Numero di item e affidabilità\nLa formula di Spearman-Brown può anche essere riarrangiata in maniera tale da consentirci di predire il numero degli item necessari per raggiungere un determinato livello di affidabilità:\n\\[\n\\begin{equation}\np = \\frac{\\rho_p (1-\\rho_1)}{\\rho_1(1-\\rho_p)},\n\\end{equation}\n\\tag{30.2}\\]\ndove \\(\\rho_1\\) è l’attendibilità stimata di un “item medio,” \\(\\rho_p\\) è il livello desiderato di attendibilità del test allungato e \\(p\\) è il numero di item del test allungato.\nEsempio. L’attendibilità della scala SWLS costituita da 5 item è \\(\\omega = 0.824\\). Quanti item devono essere aggiunti se si vuole raggiungere un livello di attendibilità pari a \\(0.95\\)?\nPonendo \\(\\rho_p = 0.95\\) e \\(\\rho_1= 0.479\\), in base alla {eq}eq-s-b-inv si ottiene che\n\n(.95 * (1 - rho_1)) / (rho_1 * (1 - .95))\n\nY2: 20.9777932006845\n\n\nil test dovrà essere costituito da 21 item.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Valutare e rifinire la soluzione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/05_val_soluzione.html#analisi-degli-item",
    "href": "chapters/extraction/05_val_soluzione.html#analisi-degli-item",
    "title": "30  Valutare e rifinire la soluzione fattoriale",
    "section": "30.4 Analisi degli item",
    "text": "30.4 Analisi degli item\nL’analisi degli item svolge un ruolo importante nello sviluppo e nella revisione dei test psicometrici. L’analisi degli item esamina le risposte fornite ai singoli item del questionario allo scopo di valutare la qualità degli item e del questionario nel suo complesso. Sotto al rubrica di analisi degli item possiamo raggruppare le procedure che possono essere utilizzate per descrivere la difficoltà degli item, le relazioni tra coppie di item, il punteggio totale del test, le relazioni tra gli item e il punteggio totale del test. Tali analisi statistiche vengono usate per la selezione degli item al fine di costruire un questionario omogeneo, attendibile e dotato di validità predittiva.\nLa selezione degli item di un test, però, non può essere svolta in maniera automatica usando soltanto criteri statistici quali quelli elencati sopra. La selezione degli item, invece, deve anche tenere includere considerazioni di ordine teorico basate sulla centralità degli item rispetto alla definizione del costrutto e considerazioni relative agli scopi della misurazione e al modo in cui l’item è stato formulato e costruito. Se alcuni aspetti di un costrutto non vengono rappresentanti da item che soddisfano i criteri statistici descritti sopra, o se c’è un numero insufficiente di item per produrre uno strumento attendibile, allora alcuni item dovranno essere riscritti. Nella riformulazione degli item, risultano utili le intuizioni che si sono guadagnate dalle analisi statistiche degli item che si sono dovuti scartare.\n\n30.4.1 Difficoltà degli item\nUna statistica comune da calcolare durante l’analisi degli item è la proporzione di esaminandi che rispondono correttamente ad ogni item. Questa è nota come difficoltà dell’item, p. La proporzione \\(p_j\\) di partecipanti che rispondono correttamente all’item \\(j\\)-esimo, o proporzione di partecipanti che si dichiarano in accordo con l’affermazione espressa dall’item, se il test non è di prestazione, fornisce una stima del livello di difficoltà \\(\\pi_j\\) dell’item.\nIn realtà, \\(p_j\\) dovrebbe essere chiamato “facilità dell’item” in quanto assume il suo valore maggiore (ovvero \\(1\\)) quando tutti i rispondenti rispondono correttamente all’item e il suo valore minimo (ovvero \\(0\\)) quando le risposte sono tutte sbagliate. Questo valore non va confuso con la difficoltà dell’item nella teoria della risposta agli item o con il valore-\\(p\\) dei test di ipotesi frequentisti.\nI valori \\(p_j\\) giocano un ruolo importante nelle procedure di selezione degli item. La difficoltà degli item deve essere interpretata in riferimento alla probabilità di indovinare la risposta corretta. Si suppone, infatti, che i rispondenti tirino ad indovinare quando non conoscono la risposta alla domanda di un questionario. Nel caso di item dicotomici, per esempio, ci possiamo aspettare un valore \\(p_j\\) pari a \\(0.50\\) sulla base del caso soltanto; nel caso di item a risposta multipla con quattro opzioni di scelta, invece, \\(p_j\\) assume un valore pari a \\(0.25\\) quando i rispondenti tirano ad indovinare.\nSe il test è composto per la maggior parte da item “facili”, allora il test non sarà in grado di discriminare tra rispondenti con diversi livelli di abilità, in quanto quasi tutti i rispondenti saranno in grado di fornire una risposta corretta alla maggioranza degli item. Lo stesso si può dire per un test composto da item “difficili”. Se il test è composto unicamente da item di difficoltà media, non potrà differenziare i rispondenti che hanno un grado di abilità media da quelli con abilità superiori alla media, dato che non ci sono item “difficili”, e neppure da quelli con abilità inferiori alla media, dato che non ci sono item “facili”.\nIn generale, dunque, è buona pratica costruire test composti da item che coprano tutti i livelli di difficoltà. La scelta che viene usualmente fatta è quella di una dispersione moderata e simmetrica del livello di difficoltà attorno ad un valore leggermente superiore al valore che sta a metà tra il livello del caso (\\(1.0\\) diviso per il numero di alternative) e il punteggio pieno (\\(1.0\\)).\nPer item che presentano cinque alternative di risposta, ad esempio, il livello del caso è pari a \\(1.0 / 5 = 0.20\\). Il livello ottimale di difficoltà è uguale a\n\\[\n0.20 + (1.0 - 0.20) / 2 = 0.60.\n\\]\nPer item dicotomici, il livello del caso è \\(1.0 / 2 = 0.50\\) e il livello ottimale di difficoltà è uguale a\n\\[\n0.50 + (1.00 - 0.50) / 2 = 0.75.\n\\]\nIn generale, item con livelli di difficoltà superiore a \\(0.90\\) o inferiore a \\(0.20\\) dovrebbero essere utilizzati con cautela.\nEsempio. Riporto qui sotto le proporzioni di risposte corrette (usando la correzione per il guessing) di 192 studenti di Psicometria nel primo parziale dell’AA 2021/2022. Il test aveva 16 item con 5 alternative di risposta ciascuno. Dunque la difficoltà media ottimale è pari a 0.6.\n\nitem_par_1 &lt;- c(\n  0.54255319, 0.76063830, 0.64361702, 0.65957447, 0.67021277, 0.12234043,\n  0.14361702, 0.18085106, 0.76063830, 0.82978723, 0.81914894, 0.84042553,\n  0.07978723, 0.07978723, 0.76063830, 0.79255319\n)\n\nNel compito, la difficoltà media è risultata essere un po’ inferiore.\n\nmean(item_par_1) %&gt;% \n  round(2)\n\n0.54\n\n\nLa distribuzione dei livelli di difficoltà degli item suggerisce che forse alcuni item “difficili” si sarebbero potuti sostituire con item di difficoltà media.\n\nplot(density(item_par_1))\n\n\n\n\n\n\n\n\n:::\nUn altro esempio riguarda il data set SAPA del pacchetto hemp. Per questi dati possiamo utilizzare la funzione colMeans per calcolare la difficoltà degli item. Poiché abbiamo dei partecipanti che hanno risposte mancanti su alcuni item, dobbiamo passare l’argomento na.rm = TRUE per ignorare i dati mancanti. In caso contrario, la funzione colMeans restituirebbe NA per gli item che hanno almeno un valore mancante. Per rendere più leggibili i valori di difficoltà degli item, arrotondiamo a tre decimali utilizzando la funzione round.\n\nitem_diff &lt;- colMeans(SAPA, na.rm = TRUE)\nround(item_diff, 3)\n\nreason.40.64reason.160.698reason.170.697reason.190.615letter.70.6letter.330.571letter.340.613letter.580.444matrix.450.526matrix.460.55matrix.470.614matrix.550.374rotate.30.194rotate.40.213rotate.60.299rotate.80.185\n\n\nL’output mostra che gli item reason.16 e reason.17 ottengono i livelli di difficoltà più alti, mentre rotate.8 ha il livello di difficoltà più basso. Circa il 70% degli studenti è stato in grado di rispondere correttamente a reason.16 e reason.17, mentre solo il 19% ha risposto correttamente a rotate.8.\n\n\n30.4.2 Correzione per guessing\nAlle volte i valori \\(p_j\\) sono calcolati introducendo una correzione per le risposte fornite casualmente dai soggetti (guessing). Si consideri un test a scelta multipla composto da item aventi ciascuno \\(C\\) alternative di risposta ed una sola risposta corretta. Si supponga che un rispondente risponda correttamente a \\(R\\) item e risponda in maniera sbagliata a \\(W\\) item.\nLa correzione per guessing si ottiene applicando una formula basata sul seguente ragionamento. Se assumiamo che un rispondente si limita a tirare ad indovinare allora, ogni \\(C\\) risposte, ci aspettiamo 1 risposta giusta e \\(C-1\\) risposte sbagliate. Per calcolare il punteggio totale del test in modo da eliminare il numero di risposte corrette ottenute tirando ad indovinare è necessario sottrarre 1 punto per ogni \\(C-1\\) item a cui è stata fornita una risposta corretta. Questo ragionamento conduce alla seguente formula:\n\\[\n\\begin{equation}\nFS = R - \\frac{W}{C - 1},\n\\end{equation}\n\\tag{30.3}\\]\ncon \\(R\\) = # risposte corrette, \\(W\\) = # risposte sbagliate, \\(C\\) = # alternative di risposta. Per esempio, se \\(C=5\\), allora è necessario sottrarre un punto ogni 4, il che è proprio quello che fa la {eq}eq-guessing.\nL’Equazione 30.3 produce un punteggio totale corretto per il guessing identico a quello che si otterrebbe assegnando 1 punto a ciascuna risposta corretta e assegnando \\(- \\frac{1}{C-1}\\) punti alle risposte sbagliate; le risposte non date non vengono considerate.\nLa correzione per guessing rappresenta il tentativo di scomporre il numero totale di risposte corrette in due componenti: le risposte corrette dovute alle conoscenze del soggetto, le risposte che risultano corrette come effetto del caso. La stessa formula può anche essere utilizzata per calcolare la difficoltà degli item corretta per il guessing (come è stato fatto nell’esempio del parziale di Psicometria).\n\n\n30.4.3 Discriminatività\nLa discriminatività è una misura di quanto ogni item è in grado di distinguere i soggetti con elevati livelli nel costrutto da quelli con un livello basso. L’indice di discriminatività \\(D\\) per i test di prestazione massima si trova nel modo seguente. Dopo avere calcolato il punteggio totale al test, si dividono i soggetti in due gruppi: soggetti con basso punteggio e soggetti con alto punteggio. Una volta definiti i due gruppi, l’indice di discriminatività \\(D\\) sarà dato da:\n\\[D = P(\\text{alto}) - P(\\text{basso}),\\]\ndove \\(P(\\text{alto}\\) è la proporzione di soggetti che ha risposto correttamente all’item nel gruppo con punteggi alti e \\(P(\\text{basso}\\) è la proporzione di soggetti che ha risposto correttamente all’item nel gruppo con punteggi bassi. Il valore di \\(D\\) può variare da -1 a +1. Nella tabella seguente sono fornite le linee guida per l’interpretazione di questo indice (Ebel, 1965).\n\n\n\nValore di \\(D\\)\nCommento\n\n\n\n\n\\(D \\geq 0.40\\)\nOttima, nessuna revisione\n\n\n\\(0.30 \\leq D &lt; 0.40\\)\nBuona, revisioni minime\n\n\n\\(0.20 \\leq D &lt; 0.30\\)\nSufficiente, revisioni parziali\n\n\n\\(D &lt; 0.20\\)\nInsufficiente, riformulazione o eliminazione\n\n\n\nLa discriminatività degli item di tipo Likert viene valutata con la medesima procedura degli item dei testi di prestazione massima, anche se cambiano le procedure statistiche da utilizzare. Si può dividere la distribuzione dei punteggi totali (o punteggi medi) in quartili e confrontare il punteggio medio o mediano del quartile superiore con quello del quartile inferiore, oppure, se il test è orientato al criterio e lo scopo è selezionare gli item che discriminano meglio due gruppi precostituiti di soggetto, eseguire i medesimi confronti tra il gruppo target (ad esempio, pazienti) e quello “di controllo” (per esempio, popolazione generale).\nÈ consigliabile valutare la dimensione dell’effetto, ad esempio attraverso l’indice \\(d\\) di Cohen. La dimensione dell’effetto dovrebbe essere almeno moderata (\\(d &gt; |0.50|\\)).\nEsempio. Per il primo parziale di Psicometria AA 2021/2022, l’indice \\(d\\) di Cohen calcolato sulla proporzione di risposte corrette per il gruppo di studenti con i punteggi più bassi (primo quartile) e il gruppo di studenti con i punteggi più alti (ultimo quartile) è stato di 4.76, 95% CI [4.0, 5.51]. L’indice complessivo di discriminatività sembra dunque adeguato. Sarebbe però necessario calcolare questo indice item per item.\n\n\n30.4.4 Potere discriminante dell’item e analisi fattoriale\nUn’altra statistica ampiamente utilizzata nell’analisi degli item è il potere discriminante degli item, che si riferisce alla capacità dell’item nel distinguere gli esaminandi con una alta abilità da quelli con una bassa abilità. Sebbene esistano molti modi per calcolare la discriminazione degli item, la forma più comune è la correlazione punto-biseriale tra le risposte degli esaminandi all’item e il loro punteggio totale nel test. Valori grandi e positivi indicano una forte relazione tra il rispondere correttamente all’item e avere un punteggio alto nel test, mentre valori vicini allo zero indicano nessuna relazione e valori negativi indicano che il rispondere correttamente all’item è associato a un punteggio complessivo del test più basso. Valori vicini allo zero o negativi suggeriscono che l’item potrebbe non funzionare correttamente. Alcune delle ragioni per ottenere una discriminazione degli item bassa o negativa potrebbero essere l’utilizzo di una chiave di risposta errata per l’item o l’assenza di risposte corrette. Indipendentemente dalla causa, gli item con correlazioni punto-biseriale basse o negative devono essere modificati, se il test/strumento è in fase di revisione, o rimossi dal test e dal punteggio.\nPer calcolare il potere discriminante dell’item per i dati SAPA, prima calcoliamo il punteggio totale del test utilizzando la funzione rowSums insieme all’opzione na.rm = TRUE e lo salviamo come total_score. Successivamente, correlaziamo gli item in SAPA con il punteggio totale del test utilizzando la funzione cor. Specificamente, usiamo l’argomento use = \"pairwise.complete.obs\" nella funzione cor a causa della presenza di risposte mancanti. Infine, salviamo la matrice di correlazione come item_discr e la stampiamo.\n\ntotal_score &lt;- rowSums(SAPA, na.rm = TRUE)\nitem_discr &lt;- cor(SAPA, total_score, use = \"pairwise.complete.obs\")\nround(item_discr, 2)\n\n\nA matrix: 16 x 1 of type dbl\n\n\nreason.4\n0.59\n\n\nreason.16\n0.53\n\n\nreason.17\n0.59\n\n\nreason.19\n0.56\n\n\nletter.7\n0.58\n\n\nletter.33\n0.56\n\n\nletter.34\n0.59\n\n\nletter.58\n0.58\n\n\nmatrix.45\n0.51\n\n\nmatrix.46\n0.51\n\n\nmatrix.47\n0.55\n\n\nmatrix.55\n0.45\n\n\nrotate.3\n0.51\n\n\nrotate.4\n0.56\n\n\nrotate.6\n0.55\n\n\nrotate.8\n0.48\n\n\n\n\n\nI risultati mostrano che tutti gli item del test SAPA sono moderatamente e positivamente correlati con il punteggio totale del test. Questo indica che tutti gli item funzionano correttamente e non fornisce informazioni salienti su quali item rimuovere o modificare.\nUn altro modo per calcolare il potere discriminante degli item consiste nel dividere i candidati in due gruppi (ad esempio, 1 = alto rendimento e 0 = basso rendimento) in base ai loro punteggi totali nel test e correlare questa variabile di raggruppamento con le risposte agli item. Questo è noto come indice di discriminazione degli item. Un’opzione per creare gruppi di alto e basso rendimento è selezionare il 25% più alto e il 25% più basso dei candidati in base ai loro punteggi totali nel test. Va notato che la decisione di utilizzare il 25% è arbitraria. Potremmo utilizzare un altro valore (ad esempio, il 10% o il 20%) per definire i gruppi di alto e basso rendimento. Dopo aver definito il punto di cut-off per i gruppi, calcoliamo la proporzione di candidati che hanno risposto correttamente all’elemento nei gruppi di alto e basso rendimento.\nNell’esempio seguente, calcoliamo l’indice di discriminazione dell’elemento reason.4 nel set di dati SAPA utilizzando la funzione idi del pacchetto hemp. Per specificare i gruppi di alto e basso rendimento, utilizziamo il valore perc_cut = .25 nella funzione idi.\n\nidi(SAPA, SAPA$reason.4, perc_cut = .25)\n\nUpper 25%0.805135951661631Lower 25%0.194864048338369\n\n\nAbbiamo scoperto che l’81% dei candidati nel gruppo di alto rendimento ha risposto correttamente all’item reason.4, mentre solo il 19% dei candidati nel gruppo di basso rendimento ha risposto correttamente. Questo suggerisce che l’item era più facile per i candidati di alto rendimento e più difficile per quelli di basso rendimento. Pertanto, possiamo dire che questo particolare item risulta utile per differenziare i due gruppi, ma non necessariamente all’interno di ciascun gruppo.\nSecondo McDondald (1999), la nozione di potere discriminante dell’item può essere trattata in maniera più precisa nell’ambito del modello monofattoriale. Se l’insieme di item a disposizione non è eccessivamente grande (200 o meno), infatti, è possibile procedere alla selezione degli item migliori tramite l’analisi fattoriale – ovvero, scegliendo gli item con le saturazioni maggiori.\n\n\n30.4.5 Punteggio sull’item e punteggio totale\nIl grado di associazione tra il punteggio sull’item e il punteggio totale viene considerato dalla teoria classica dei test come un indice che descrive il potere discriminante dell’item. Se il test fornisce una misura attendibile di un unico attributo, e se un item è fortemente associato al punteggio del test, allora l’item sarà in grado di distinguere tra rispondenti che ottengono un punteggio basso nel test e rispondenti che ottengono un punteggio alto nel test.\nNel caso di una forte associazione positiva tra il punteggio sull’item e il punteggio totale, la probabilità di risposta corretta sull’item è alta per rispondenti che ottengono un punteggio totale alto, e bassa per i rispondenti che ottengono un punteggio totale basso. Nel caso di una debole associazione tra il punteggio sull’item e il punteggio totale, invece, la probabilità di risposta corretta all’item non è predittiva del punteggio totale. Gli item con un basso potere discriminante dovrebbero dunque essere rimossi dal reattivo.\nÈ necessario distinguere i casi in cui gli item sono dicotomici dal caso di item continui. Nel caso di item dicotomici e di un test unidimensionale, il potere discriminante viene calcolato mediante la correlazione biseriale o punto-biseriale.\n\n\n30.4.6 Relazioni tra coppie di item\nLe relazioni tra coppie di item sono importanti sia per la costruzione sia per la validazione dei test psicometrici. La teoria classica dei test definisce l’attendibilità di un test (o di un item) come il rapporto tra la varianza del punteggio vero e la varianza del punteggio osservato. Il coefficiente di attendibilità può però essere calcolato anche trovando la correlazione tra due forme parallele di un test (o tra due item). Inoltre, è possibile interpretare la correlazione tra due forme parallele di un test (o tra due item) come il quadrato del coefficiente di correlazione tra i punteggi osservati e i punteggi veri di un test (o di un item).\nMolti indici sono disponibili per misurare il grado di associazione tra item. Per item quantitativi, possiamo usare la correlazione di Pearson o la covarianza. Per item qualitativi politomici ordinali, usiamo la correlazione policorica. Per item ordinali dicotomici, usiamo la correlazione tetracorica. Per item dicotomici usiamo, ad esempio, l’indice \\(\\phi\\).\n\n\n30.4.7 Ridondanza\nNel processo di raffinamento del test occorre anche tenere conto degli item ridondanti, ossia degli item che sono troppo associati tra loro. La ridondanza può essere valutata con indici statistici quali la correlazione: se due o più item hanno tra loro una correlazione maggiore di \\(|0.70|\\) viene mantenuto nell’item pool solo uno di essi, dato che gli altri item forniscono la stessa informazione.\n\n\n30.4.8 Massimizzazione della varianza del punteggio totale\nUno dei criteri che possono essere utilizzati per la selezionare degli item che andranno a costituire la versione finale di un test è la massimizzazione della varianza del punteggio totale. Più in particolare, si vuole massimizzare il rapporto tra la varianza del punteggio totale e la somma delle varianze dei punteggi dei \\(p\\) item. Dato che il coefficiente \\(\\alpha\\) di Cronbach ha la seguente forma:\n\\[\\alpha = \\frac{p}{p-1}\\left[1- \\frac{\\sum \\sigma^2_{Y_i}}{\\sigma^2_T} \\right],\\]\nla scelta di massimizzare il rapporto definito in precedenza avrà anche la conseguenza di massimizzare \\(\\alpha\\).\n{cite:t}mcdonald2013test fa notare che una procedura di selezione degli item basata sul principio della massimizzazione di \\(\\alpha\\) ha però dei limiti. In primo luogo, tale procedura è appropriata solo quando l’insieme di item è troppo grande per selezionare gli item in base all’esame delle saturazioni fattoriali ottenute applicando il modello mono-fattoriale. In secondo luogo, {cite:t}mcdonald2013test nota che la procedura di selezione basata sulla massimizzazione di \\(\\alpha\\) è adeguata solo nel caso di una struttura mono-fattoriale. La selezione degli item basata sulla massimizzazione di \\(\\alpha\\) deve dunque essere accompagnata da considerazione relative al contenuto e alla struttura del costrutto.\n\n\n30.4.9 Indice di affidabilità dell’item\nOltre agli indici di difficoltà e discriminazione degli item, un’altra statistica utile per l’analisi degli item è l’indice di affidabilità dell’item. L’indice di affidabilità dell’item (IRI) è definito come:\n\\[\nIRI = S_i \\cdot r_{i,tt},\n\\]\ndove \\(S_i\\) è la deviazione standard dell’item \\(i\\) e \\(r_{i,tt}\\) è la correlazione tra l’item \\(i\\) e il punteggio totale del test. L’IRI può teoricamente variare tra -0.5 e 0.5, con valori grandi e positivi indicativi di alta affidabilità.\nDi seguito calcoliamo l’IRI per tutti gli item nel set di dati SAPA. Possiamo farlo utilizzando la funzione iri in hemp.\n\niri(SAPA)\n\n\nA matrix: 16 x 1 of type dbl\n\n\nreason.4\n0.2820989\n\n\nreason.16\n0.2451971\n\n\nreason.17\n0.2692675\n\n\nreason.19\n0.2717135\n\n\nletter.7\n0.2865325\n\n\nletter.33\n0.2757209\n\n\nletter.34\n0.2897118\n\n\nletter.58\n0.2863221\n\n\nmatrix.45\n0.2544930\n\n\nmatrix.46\n0.2562540\n\n\nmatrix.47\n0.2668171\n\n\nmatrix.55\n0.2161230\n\n\nrotate.3\n0.2016459\n\n\nrotate.4\n0.2276081\n\n\nrotate.6\n0.2539219\n\n\nrotate.8\n0.1867207\n\n\n\n\n\nI risultati restituiti dalla funzione iri mostrano che l’IRI varia da circa 0.19 a 0.29 per il set di dati SAPA. Tutti questi sono valori ragionevoli per l’IRI (ovvero nessuno è negativo o vicino allo zero).\n\n\n30.4.10 Indice di validità dell’item\nQuando invece del punteggio totale del test viene utilizzato un criterio esterno, questo indice è noto come indice di validità dell’item (IVI). L’IVI può variare anche tra -0.5 e 0.5, con valori elevati (in valore assoluto) che indicano una validità maggiore. Valori negativi elevati indicano una maggiore validità quando ci si aspetta che gli elementi siano correlati in modo negativo con il criterio.\nNell’esempio seguente, utilizziamo la funzione ivi in hemp con “reason.17” come criterio esterno e “reason.4” come elemento di interesse e troviamo che l’IVI è 0.19.\n\nivi(item = SAPA$reason.4, crit = SAPA$reason.17)\n\n0.190321881267833\n\n\n\n\n30.4.11 Distrattori\nUn altro aspetto importante degli elementi che deve essere analizzato sono le opzioni di risposta. Nel contesto dei test a scelta multipla, le opzioni di risposta alternative (cioè sbagliate) vengono definite “distrattori”. I distrattori svolgono un ruolo importante in un elemento a scelta multipla. Per garantire elementi a scelta multipla di alta qualità, è cruciale includere distrattori plausibili e ben funzionanti che siano più probabili di attirare i candidati con conoscenze parziali. I distrattori non plausibili potrebbero dover essere riscritti o sostituiti con un distrattore migliore. La qualità dei distrattori viene tipicamente valutata attraverso l’analisi dei distrattori. L’analisi dei distrattori viene spesso condotta osservando la proporzione di candidati che scelgono un distrattore particolare.\nPer illustrare l’analisi dei distrattori, utilizziamo gli item del data set multiplechoice in hemp. Si tratta di un ipotetico test a scelta multipla composto da 27 item somministrati a 496 candidati. Le quattro opzioni di risposta sono codificate come 1, 2, 3 e 4 nel data set. Utilizziamo la funzione distract in hemp per calcolare la proporzione di candidati che selezionano ciascun distrattore.\n\ndistractors &lt;- distract(multiplechoice)\nhead(distractors)\n\n\nA matrix: 6 x 4 of type dbl\n\n\n\n1\n2\n3\n4\n\n\n\n\nitem1\n0.044\n0.058\n0.052\n0.845\n\n\nitem2\n0.109\n0.069\n0.792\n0.030\n\n\nitem3\n0.188\n0.562\n0.058\n0.192\n\n\nitem4\n0.034\n0.125\n0.742\n0.099\n\n\nitem5\n0.351\n0.254\n0.042\n0.353\n\n\nitem6\n0.081\n0.198\n0.558\n0.163\n\n\n\n\n\nNella tabella sopra, vediamo che molti item avevano distrattori selezionati circa il 5% delle volte o meno. Questi distrattori potrebbero essere candidati per una revisione in quanto sono stati approvati ad un livello così basso da suggerire che la maggior parte degli esaminandi non li ha considerati come opzioni plausibili. Per l’item 1, i distrattori funzionavano tutti più o meno allo stesso modo (ovvero circa il 5% delle volte ogniuno è stato approvato), suggerendo che funzionavano tutti bene rispetto l’uno all’altro, ma che l’item era troppo facile (la risposta corretta era l’opzione 4, selezionata dall’84.5% degli esaminandi). Al contrario, l’item 5 era un item più difficile, con la risposta corretta che ancora una volta era l’opzione 4. Le opzioni 1 e 2 erano molto probabilmente fraintendimenti, mentre l’opzione 3 potrebbe essere rivista o potenzialmente eliminata da questo item a causa del basso tasso di approvazione (solo il 4.2%). Dato l’approvazione molto alta dell’opzione 1 (35.1%), è molto probabile che anche questa opzione fosse corretta. Per ottenere una visione più completa del funzionamento dell’item, sarebbe consigliabile calcolare l’indice di discriminazione specifico per quell’item. Questo ci permetterebbe di ottenere ulteriori informazioni sulla capacità dell’item di distinguere tra candidati di alto e basso livello.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Valutare e rifinire la soluzione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/05_val_soluzione.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/extraction/05_val_soluzione.html#informazioni-sullambiente-di-sviluppo",
    "title": "30  Valutare e rifinire la soluzione fattoriale",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n\n\n\n\n\nMcDonald, R. P. (2013). Test theory: A unified treatment. Psychology Press.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Valutare e rifinire la soluzione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html",
    "href": "chapters/cfa/01_cfa.html",
    "title": "31  Analisti Fattoriale Confermativa",
    "section": "",
    "text": "31.1 Introduzione\nIn questo capitolo esamineremo la CFA per l’analisi dei modelli di misurazione con fattori comuni e indicatori continui. A differenza dell’analisi fattoriale esplorativa (EFA), nella CFA vengono analizzati modelli di misurazione vincolati. Ciò significa che il ricercatore specifica (1) il numero esatto di fattori; (2) il pattern dei carichi fattoriali, ossia la corrispondenza specifica tra i fattori e gli indicatori; e (3) la presenza di errori correlati, se presenti.\nLa seconda caratteristica menzionata sopra implica che un indicatore satura solo sui fattori specificati dal ricercatore, e tutte le saturazioni incrociate di quell’indicatore su altri fattori sono fissate a zero. Sebbene sia possibile specificare un numero esatto di fattori nella EFA, la tecnica analizza modelli di misurazione non restrittivi, in cui ciascun indicatore satura su tutti i fattori (ossia tutte le saturazioni incrociate sono liberamente stimate).\nUn’altra differenza è che i modelli EFA con più fattori sono identificati solo dopo aver specificato un metodo di rotazione dei fattori, come obliqua (i fattori possono covariare) oppure ortogonale (i fattori sono non correlati). Poiché la CFA richiede un modello identificato, non c’è una fase di rotazione e di solito è permesso ai fattori di covariare.\nNell’ambito dei requisiti per l’identificazione, è possibile stimare errori correlati nella CFA, ma è più difficile ottenere questo risultato nella EFA. Pertanto, la tecnica della CFA supporta meglio l’analisi delle strutture di covarianza degli errori rispetto alla EFA.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#limitazioni-dellapproccio-fattoriale",
    "href": "chapters/cfa/01_cfa.html#limitazioni-dellapproccio-fattoriale",
    "title": "31  Analisti Fattoriale Confermativa",
    "section": "\n31.2 Limitazioni dell’approccio fattoriale",
    "text": "31.2 Limitazioni dell’approccio fattoriale\nL’approccio classico dell’analisi fattoriale (EFA più rotazione fattoriale) ha rivelato avere diversi limiti. Nella ricerca iniziale, dibattiti teorici importanti, come il numero di fattori dell’intelligenza o della personalità, erano basati sui risultati di diverse rotazioni fattoriali. Questi dibattiti si sono rivelati essere semplici speculazioni, poiché conclusioni diverse potevano essere supportate a seconda dell’interpretazione dei dati. Per esempio, il dibattito tra Eysenck e Cattell sul numero di fattori della personalità (due o sedici) dipendeva dall’uso di rotazioni ortogonali o oblique sugli stessi dati.\nNella seconda metà del XX secolo, c’era una generale insoddisfazione verso l’analisi fattoriale a causa della sua apparente capacità di adattarsi a quasi qualsiasi soluzione. Furono raccomandati criteri rigorosi per il suo uso, come la necessità di grandi campioni, che spesso rendevano l’analisi impraticabile a quei tempi. Inoltre, furono introdotti vincoli relativi alle ipotesi del modello e al requisito che le variabili nella matrice di correlazione avessero varianze equivalenti, creando problemi pratici significativi, specialmente con dati binari spesso usati nei test psicometrici.\nSolo con l’introduzione di metodi psicometrici moderni, come l’analisi fattoriale confermativa (CFA) discussa in questo capitolo e la Teoria di Risposta all’Item (discussa in una sezione successiva), questi problemi sono stati risolti.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#efa-vs.-cfa-confronto-tra-analisi-fattoriale-esplorativa-e-confermativa",
    "href": "chapters/cfa/01_cfa.html#efa-vs.-cfa-confronto-tra-analisi-fattoriale-esplorativa-e-confermativa",
    "title": "31  Analisti Fattoriale Confermativa",
    "section": "\n31.3 EFA vs. CFA: Confronto tra Analisi Fattoriale Esplorativa e Confermativa",
    "text": "31.3 EFA vs. CFA: Confronto tra Analisi Fattoriale Esplorativa e Confermativa\n\n31.3.1 Fondamenti Comuni e Differenze\nSia l’Analisi Fattoriale Esplorativa (EFA) che quella Confermativa (CFA) si basano sul modello dei fattori comuni. Entrambe le tecniche presuppongono che la varianza degli indicatori osservati possa essere suddivisa in varianza comune e varianza unica. La varianza comune è quella condivisa tra gli indicatori e sottende le covarianze osservate, mentre la varianza unica comprende sia la varianza specifica delle variabili che l’errore di misurazione. I fattori estratti, detti fattori comuni, rappresentano le variabili latenti costruite da questa varianza comune.\nNell’EFA, la struttura dei fattori è indeterminata e viene esplorata senza ipotesi a priori riguardo al numero o alla natura dei fattori. L’EFA è quindi particolarmente utile nelle fasi iniziali di ricerca, quando la teoria è poco sviluppata o si sospetta la presenza di fattori inaspettati.\nAl contrario, la CFA si basa su un modello di fattori predefinito, che specifica a priori quali indicatori sono associati a ciascun fattore, rendendola idonea per confermare teorie esistenti o per validare strutture fattoriali precedentemente esplorate. In CFA, i carichi incrociati (indicatori che caricano su più di un fattore) sono generalmente vincolati a zero, stabilendo una relazione diretta e specifica tra fattori e indicatori.\n\n31.3.2 Indeterminatezza Fattoriale\nUn problema ricorrente in entrambe le tecniche è l’indeterminatezza fattoriale, dove i fattori comuni non possono essere definiti in modo univoco dai loro indicatori a causa della natura approssimativa delle stime. Questo si manifesta sia in EFA, con l’indeterminatezza della rotazione, che in CFA, dove l’analisi potrebbe non replicarsi in nuovi campioni a causa dell’uso dei medesimi dati per verificare il modello.\n\n31.3.3 Indeterminatezza dei Punteggi Fattoriali\nUn’altra complicazione è l’indeterminatezza dei punteggi fattoriali, che si verifica quando esistono infinite soluzioni valide per i punteggi fattoriali a partire dagli indicatori. Questo comporta che diversi metodi possono produrre ordinamenti differenti dei casi, un problema noto come indeterminatezza dei punteggi fattoriali (Grice, 2001).\n\n31.3.4 Rotazione e Specificazione del Modello\nLa EFA può presentare ambiguità a causa dell’infinita quantità di configurazioni dei carichi fattoriali che potrebbero adattarsi ai dati, un fenomeno meno pronunciato nella CFA dove la specifica del modello è più rigida.\n\n31.3.5 Applicazioni Pratiche\nL’EFA è spesso preferita in nuovi ambiti di ricerca, dove i fattori potrebbero non essere ben definiti, mentre la CFA è utilizzata per confermare le strutture fattoriali in studi di validazione o in seguito a revisioni di test esistenti.\n\n31.3.6 Problemi con l’Uso Combinato di EFA e CFA\nSi noti che l’applicazione della CFA immediatamente dopo la EFA nello stesso campione può essere problematica. Talvolta, l’uso congiunto non verifica né conferma i risultati dell’EFA. La restrittività dei modelli CFA, con i carichi incrociati impostati a zero, può portare a risultati che non sono coerenti con i dati analizzati nell’EFA.\n\n31.3.7 Nuove Approcci Intermedi\nMetodi come l’Analisi Strutturale Esplorativa (ESEM) offrono un approccio ibrido che combina la flessibilità dell’EFA con alcuni degli aspetti confirmatori della CFA. Questo permette una maggiore precisione nel testare l’adattamento del modello, pur mantenendo la capacità di esplorare nuove strutture fattoriali.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#raccomandazioni-per-la-selezione-degli-indicatori-nellanalisi-fattoriale",
    "href": "chapters/cfa/01_cfa.html#raccomandazioni-per-la-selezione-degli-indicatori-nellanalisi-fattoriale",
    "title": "31  Analisti Fattoriale Confermativa",
    "section": "\n31.4 Raccomandazioni per la Selezione degli Indicatori nell’Analisi Fattoriale",
    "text": "31.4 Raccomandazioni per la Selezione degli Indicatori nell’Analisi Fattoriale\nLa selezione accurata degli indicatori è cruciale per il successo dell’analisi fattoriale, sia essa Esplorativa (EFA) o Confermativa (CFA). Le linee guida suggerite da Fabrigar e Wegener (2012) e Little et al. (1999), come riassunto in Kline (2023), enfatizzano i seguenti punti chiave:\n\nDefinizione dei Concetti Teorici: È essenziale articolare i concetti teorici in modo dettagliato per delineare chiaramente ogni dominio di interesse. Ad esempio, se lo studio riguarda le dimensioni dell’ansia, è importante riferirsi a letteratura teorica ed empirica che discute vari aspetti come ansia di stato, ansia di tratto e ansia sociale.\nScelta degli Indicatori: Gli indicatori selezionati dovrebbero coprire adeguatamente i domini d’interesse senza affidarsi esclusivamente allo stesso metodo di misurazione, come i questionari di autovalutazione, per ridurre la varianza dovuta a metodi comuni. L’impiego di modelli CFA specializzati può aiutare a stimare questi effetti del metodo.\nGuida Teorica o Empirica Forte: Se esiste una solida base teorica o empirica, gli indicatori omogenei sono preferibili poiché forniscono stime più precise e meno distorte, specialmente in analisi di tipo più confermativo.\nAnalisi di Indicatori Meno Omogenei: Se la guida teorica è debole, può essere vantaggioso esaminare un insieme di indicatori meno omogenei che coprono un’ampia gamma del dominio d’interesse. Ciò evita di basarsi su approssimazioni che potrebbero non riflettere pienamente i concetti chiave.\nUso di Indicatori di Qualità Psicometrica Inferiore: Anche gli indicatori con minore qualità psicometrica possono essere utili se coprono ampiamente il costrutto, generano punteggi che riflettono ampie differenze individuali e sono analizzati attraverso metodi più confermativi.\nProblemi Tecnici: L’analisi potrebbe incontrare problemi come i casi Heywood o la mancata convergenza se alcuni fattori hanno un numero insufficiente di indicatori, specialmente in campioni piccoli. Un numero sicuro minimo di indicatori per ogni fattore previsto è di circa 3-5. Tuttavia, in alcuni casi, potrebbe essere vantaggioso utilizzare meno indicatori per fattore se questi sono psicometricamente solidi.\n\nHayduk e Littvay (2012) hanno sottolineato che non è sempre preferibile avere più indicatori per fattore; in certi contesti, un singolo indicatore ben scelto può essere sufficiente. Se gli indicatori sono altamente ridondanti, non aggiungono informazioni significative. L’idea di una “regola d’oro” di 3-5 indicatori per fattore è una guida generale, ma la scelta dovrebbe essere basata sulle ipotesi specifiche di ricerca piuttosto che su una regola arbitraria.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#fondamenti-dei-modelli-di-base-nella-cfa",
    "href": "chapters/cfa/01_cfa.html#fondamenti-dei-modelli-di-base-nella-cfa",
    "title": "31  Analisti Fattoriale Confermativa",
    "section": "\n31.5 Fondamenti dei Modelli di Base nella CFA",
    "text": "31.5 Fondamenti dei Modelli di Base nella CFA\nI modelli di base nella Confermative Factor Analysis (CFA) con più fattori sono caratterizzati da specifiche fondamentali che garantiscono una misurazione precisa delle variabili latenti. Ecco una sintesi delle caratteristiche principali di tali modelli:\n\nRelazione tra Indicatori e Fattori: Ogni indicatore è una variabile continua influenzata da due principali componenti: un fattore comune, che rappresenta la variabile latente che l’indicatore è inteso a misurare, e la varianza unica. Quest’ultima include sia l’errore di misurazione casuale sia la varianza specifica non spiegata dal fattore, entrambi rappresentati dal termine di errore.\nIndipendenza dei Termini di Errore: I termini di errore sono assunti come indipendenti l’uno dall’altro e dai fattori. Ciò implica l’assenza di confondenti non misurati per qualsiasi coppia di indicatori e l’indipendenza delle cause omesse dai fattori.\nLinearità e Covarianza: Le relazioni all’interno del modello sono lineari e i fattori possono covariare, il che significa che non esistono effetti causali diretti tra i fattori.\n\nQueste caratteristiche definiscono la misurazione unidimensionale, sottolineando che ciascun indicatore è pensato per misurare una sola dimensione e non condivide varianza con altri indicatori una volta controllati i fattori comuni. Tuttavia, è anche possibile specificare modelli CFA multidimensionali, dove alcuni indicatori possono caricare su più di un fattore o dove coppie di termini di errore possono essere correlati.\nInoltre, esistono metodi specializzati per analizzare relazioni non lineari tra fattori e indicatori continui, o tra i fattori stessi, come descritto da Amemiya e Yalcin (2001). Le relazioni tra indicatori categorici e fattori sono intrinsecamente non lineari, e questi scenari sono trattati nel CFA categorico.\nUn esempio di modello CFA di base con due fattori e sei indicatori viene presentato di seguito, dove tutti i carichi incrociati sono fissati a zero, Figura 31.1. Per esempio, il fattore B non ha un effetto causale diretto sull’indicatore X1, il quale è misurato da un altro fattore (A). Tuttavia, ciò non significa che X1 e il fattore B siano completamente scorrelati. La struttura del modello permette a X1 di covariare con B poiché B è correlato con A, che è una causa di X1 (l’altra causa è E1, il termine di errore di X1). In modo simile, si prevede che gli indicatori X1 e X4 covarino poiché sono influenzati dai fattori A e B, rispettivamente, i quali sono correlati.\nLe costanti di scala, indicate come (1) nel modello, definiscono le metriche per le variabili non misurate, inclusi i fattori comuni e i termini di errore degli indicatori, stabilendo così una base uniforme per la misurazione nel modello CFA.\n\n\n\n\n\nFigura 31.1: Modello di analisi fattoriali confermativa con due fattori comuni e sei indicatori. (Figura tratta da Kline (2023))",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#scalatura-dei-fattori-e-inclusione-delle-covariate-nei-modelli-cfa-di-base",
    "href": "chapters/cfa/01_cfa.html#scalatura-dei-fattori-e-inclusione-delle-covariate-nei-modelli-cfa-di-base",
    "title": "31  Analisti Fattoriale Confermativa",
    "section": "\n31.6 Scalatura dei Fattori e Inclusione delle Covariate nei Modelli CFA di Base",
    "text": "31.6 Scalatura dei Fattori e Inclusione delle Covariate nei Modelli CFA di Base\nNella rappresentazione di base dei modelli Confermative Factor Analysis (CFA), la scalatura dei fattori viene spesso eseguita utilizzando il metodo della variabile di riferimento, conosciuto anche come metodo della variabile marker o approccio di identificazione del carico di riferimento (Newsom, 2015). In questo approccio, un vincolo di Unit Loading Identification (ULI) è applicato al carico di un indicatore per ciascun fattore. Per esempio, nel modello illustrato, il carico di \\(X1\\) sul fattore \\(A\\) è fissato a 1.0, stabilendo così la varianza del fattore \\(A\\) sulla base della varianza comune dell’indicatore \\(X1\\), che funge da variabile di riferimento per \\(A\\). Analogamente, la varianza del fattore \\(B\\) è calibrata utilizzando \\(X4\\) come variabile marker.\nQuando più indicatori per lo stesso fattore presentano precisione equivalente e nessuno di essi è considerato particolarmente rappresentativo del concetto sottostante, la scelta dell’indicatore come variabile di riferimento diventa generalmente arbitraria. Questa selezione non influisce solitamente sull’adattamento globale del modello, sulla soluzione standardizzata, o sulle stime delle varianze di errore dell’indicatore nelle soluzioni non standardizzate. Le saturazioni fisse a 1.0 per le variabili di riferimento rimangono invariate nelle soluzioni non standardizzate e non sono soggette a test di significatività, poiché sono considerate costanti.\nUn potenziale svantaggio di questo metodo è l’assenza di test di significatività per le saturazioni fisse, il che può essere limitante se si desidera valutare la significatività di tutte le saturazioni. Metodi alternativi per scalare i fattori, che non richiedono la selezione di variabili di riferimento, saranno discussi nelle sezioni successive.\nNei modelli CFA di base, tutti i fattori sono considerati variabili esogene, il che significa che sono liberi di variare e covariare indipendentemente l’uno dall’altro. Tuttavia, è possibile includere variabili esterne, dette covariate, che si presume possano influenzare i fattori comuni. Ad esempio, l’età dei partecipanti potrebbe essere vista come una covariata che influisce sui fattori comuni in un modello CFA.\nL’inclusione di covariate trasforma i fattori comuni da variabili esogene a endogene, implicando che non sono più completamente liberi di variare in modo indipendente, ma possono essere direttamente influenzati dalle covariate. Questo richiede l’aggiunta di termini di disturbo nei fattori comuni per rappresentare l’effetto diretto delle covariate su di essi, integrando così l’effetto delle variabili esterne nel modello CFA.\n\n31.6.1 Parametri del Modello nella CFA\nIn un modello CFA con indicatori continui, quando le medie delle variabili non sono considerate, i parametri liberi includono varianze, covarianze di variabili esogene e gli effetti diretti (carichi) sulle variabili endogene. Ad esempio, analizzando il modello di base illustrato in figura, i parametri liberi possono essere suddivisi come segue:\n\n\nVarianze: Comprendono le varianze di due fattori e sei termini di errore associati agli indicatori, per un totale di otto varianze.\n\nCovarianza: È presente una covarianza tra i due fattori.\n\nEffetti Diretti (Carichi): Quattro carichi fattoriali rappresentano gli effetti diretti dei fattori sugli indicatori, specificamente per gli indicatori X2, X3, X5 e X6. Questi carichi non sono fissati, a differenza di quelli usati per scalare i fattori.\n\nSommando questi parametri, il totale dei parametri liberi nel modello è 13. Con \\(v = 6\\) variabili osservate, il numero totale di osservazioni statisticamente indipendenti, calcolato come \\(6(7)/2\\), è 21. Di conseguenza, i gradi di libertà per il modello presentato sono calcolati sottraendo i parametri liberi dalle osservazioni indipendenti, risultando in \\(21 - 13 = 8\\) gradi di libertà.\n\n31.6.1.1 Requisiti di Identificazione: Necessari ma Non Sufficienti per i Modelli di CFA\nPer assicurare che un modello di Confermative Factor Analysis (CFA) sia correttamente specificato e possa essere utilizzato per trarre conclusioni valide, è fondamentale soddisfare alcuni requisiti di identificazione essenziali. Questi requisiti sono necessari ma non sempre sufficienti; cioè, la loro soddisfazione non garantisce automaticamente l’identificazione completa del modello.\n\n\nGradi di Libertà (dfM) Maggiori o Uguali a Zero:\n\n\nCalcolo: I gradi di libertà di un modello CFA si determinano sottraendo il numero di parametri liberi (come varianze, covarianze, e carichi fattoriali) dal numero totale di osservazioni indipendenti disponibili, solitamente le varianze e covarianze degli indicatori.\n\nSignificato: Avere gradi di libertà positivi indica che ci sono sufficienti dati per stimare i parametri del modello e verificare il suo adattamento. Un modello con zero gradi di libertà è “saturato” e si adatterà perfettamente ai dati, ma non fornirà validazione ulteriore.\n\nImportanza: Mantenere dfM ≥ 0 è cruciale per evitare la sottospecificazione del modello, che potrebbe condurre a stime inaccurate e conclusioni fuorvianti.\n\n\n\nScalatura Corretta di Ogni Variabile Non Misurata:\n\n\nNecessità: È essenziale scalare ogni variabile latente, come i fattori, per definirne l’unità di misura. Senza una scalatura adeguata, parametri come i carichi fattoriali rimarrebbero indeterminati.\n\nMetodi: La scalatura può essere effettuata in vari modi, come fissando il carico di un indicatore per fattore a 1.0 (metodo della variabile di riferimento), fissando la varianza del fattore a un valore preciso, tipicamente 1.0 (metodo della standardizzazione della varianza), o applicando vincoli alle stime delle saturazioni fattoriali (metodo di codifica degli effetti).\n\n\n\nIn sintesi, pur essendo fondamentale soddisfare questi requisiti per stabilire una base identificabile e interpretabile per un modello CFA, l’identificazione completa del modello può richiedere considerazioni aggiuntive legate alla struttura specifica e alle ipotesi teoriche che sottendono al modello.\n\n\n\n\n\nFigura 31.2: Scalatura dei fattori nel metodo della variabile di riferimento con vincoli di identificazione del carico unitario (ULI) (a), metodo di standardizzazione della variabile con vincoli di identificazione della varianza unitaria (UVI) (b) e metodo di codifica degli effetti con vincoli di identificazione della codifica degli effetti (ECI) (a + b + c)/3 = (d + e + f)/3 = 1.0 (c). (Figura tratta da Kline (2023))\n\n\n\n31.6.1.2 Requisiti Sufficienti Aggiuntivi per l’Identificazione nei Modelli CFA\nOltre ai criteri base, esistono requisiti addizionali che favoriscono l’identificazione adeguata nei modelli di Confermative Factor Analysis (CFA):\n\nRegola dei Tre Indicatori per i Modelli a Singolo Fattore: Affinché un modello CFA con un solo fattore sia pienamente identificabile, è necessario che disponga di almeno tre indicatori. Questo è dovuto al fatto che con solamente due indicatori non si dispone di sufficiente informazione per separare accuratamente la varianza del fattore e i suoi carichi specifici dagli errori di misurazione. Un modello con esattamente tre indicatori ha zero gradi di libertà, il che significa che si adatterà perfettamente ai dati ma non permetterà ulteriori test o validazioni. Per garantire gradi di libertà positivi (dfM &gt; 0) e consentire un’efficace validazione del modello, è consigliabile utilizzare almeno quattro indicatori.\nRegola dei Due Indicatori per i Modelli con Più Fattori: Nei modelli CFA che coinvolgono più di un fattore, è essenziale che ogni fattore sia rappresentato da almeno due indicatori. Questa disposizione aiuta a definire chiaramente ogni fattore e a distinguerlo dagli altri fattori presenti nel modello. Tuttavia, i modelli che si limitano a due indicatori per fattore possono presentare problematiche, specialmente in campioni di dimensioni ridotte, poiché possono emergere instabilità nelle stime e complessità nell’interpretazione dei risultati.\n\nQuesti requisiti aggiuntivi sono fondamentali non solo per garantire che un modello CFA sia teoricamente valido (attraverso una corretta scalatura e definizione delle variabili latenti), ma anche per assicurare la sua utilità pratica, fornendo sufficienti gradi di libertà per consentire validazioni affidabili e interpretazioni significative del modello.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#oltre-i-requisiti-minimi-di-identificazione",
    "href": "chapters/cfa/01_cfa.html#oltre-i-requisiti-minimi-di-identificazione",
    "title": "31  Analisti Fattoriale Confermativa",
    "section": "\n31.7 Oltre i Requisiti Minimi di Identificazione",
    "text": "31.7 Oltre i Requisiti Minimi di Identificazione\nNel contesto della Confermative Factor Analysis (CFA), i requisiti di identificazione sono considerati necessari ma non sufficienti. Questo significa che, anche se il loro soddisfacimento è cruciale per una corretta stima dei parametri del modello, essi non garantiscono da soli che il modello sia il migliore possibile o pienamente identificato in termini di struttura e fondamenti teorici. Questa distinzione sottolinea l’importanza di andare oltre i criteri minimi per esplorare l’adeguatezza complessiva e la validità del modello all’interno del suo contesto teorico e applicativo.\n\n31.7.1 Perché sono Necessari\n\nGradi di Libertà (dfM) Maggiori o Uguali a Zero: Avere gradi di libertà non negativi è essenziale per assicurare che ci siano abbastanza dati per stimare i parametri del modello. Se i gradi di libertà sono negativi, indica che ci sono troppi parametri da stimare rispetto alle informazioni disponibili, il che rende il modello inidentificabile.\nScalatura Corretta di Ogni Variabile Non Misurata: La scalatura delle variabili latenti consente di stabilire un’unità di misura chiara, rendendo possibile l’interpretazione dei parametri come i carichi fattoriali. Senza una scalatura appropriata, i parametri del modello rimarrebbero indeterminati e potrebbero condurre a conclusioni ambigue.\n\n31.7.2 Perché Non Sono Sufficienti\nNonostante la soddisfazione di questi requisiti renda il modello tecnicamente identificabile e stima i parametri, ci sono altre considerazioni che possono influenzare l’adeguatezza del modello:\n\nAdeguamento del Modello: Anche se un modello ha gradi di libertà positivi e le variabili sono correttamente scalate, potrebbe non adattarsi bene ai dati. L’adeguatezza del modello è valutata attraverso statistiche di fit come il Chi-quadrato, RMSEA, CFI, e altri. Un modello può soddisfare i requisiti di identificazione ma avere un cattivo fit.\nValidità Teorica: Un modello può essere tecnicamente corretto ma non catturare accuratamente le relazioni teoriche tra le variabili. La costruzione del modello deve essere guidata da una solida base teorica che giustifica le relazioni tra i fattori e gli indicatori.\n\n31.7.3 Esempio Pratico\nImmaginiamo un modello CFA per misurare due concetti psicologici, come l’ansia e la depressione, con tre indicatori per ciascun fattore. Anche se il modello potrebbe avere gradi di libertà sufficienti e ogni fattore è correttamente scalato con un indicatore con carico fissato a 1.0, potrebbero sorgere problemi:\n\nCross-loadings: Gli indicatori per l’ansia potrebbero anche avere carichi significativi sulla depressione, il che non è catturato nel modello perché ogni indicatore è supposto misurare un solo fattore. Questo problema di validità del modello non è rilevato dai semplici criteri di identificazione.\nAdattamento del Modello: Il modello potrebbe mostrare un cattivo adattamento ai dati, suggerendo che la struttura ipotizzata dei fattori e degli indicatori non riflette accuratamente le relazioni tra le variabili osservate.\n\nIn conclusione, mentre i requisiti di identificazione sono critici per la fattibilità tecnica di un modello CFA, non garantiscono di per sé che il modello sia il migliore possibile o che rifletta accuratamente le dinamiche sottostanti. Ulteriori analisi e valutazioni sono necessarie per assicurare che il modello sia sia identificabile che valido.\n\n31.7.3.1 Altri Metodi per la Scalatura dei Fattori nei Modelli di CFA\nLa scalatura dei fattori è fondamentale per garantire una corretta identificazione e interpretazione dei fattori in un modello di Confermative Factor Analysis (CFA). Oltre al comune metodo della variabile di riferimento, esistono altri due approcci principali:\n\n\nMetodo di Standardizzazione della Varianza (Variance Standardization Method):\n\n\nDescrizione: Questo metodo fissa la varianza di ciascun fattore a 1.0, un approccio noto come unit variance identification (UVI).\n\nImplicazioni: La standardizzazione dei fattori implica che le loro varianze non sono stimate come parametri liberi. Invece, le covarianze tra i fattori sono liberamente stimate e interpretate come correlazioni di Pearson.\n\nCarichi degli Indicatori: Tutti i carichi degli indicatori sono considerati parametri liberi, il che permette di testarne la significatività statistica attraverso i loro errori standard.\n\nVantaggi e Limitazioni: Il principale vantaggio di questo metodo è la sua semplicità e l’assenza di necessità di selezionare una variabile di riferimento. Tuttavia, è generalmente più adatto per modellare fattori esogeni.\n\n\n\nMetodo di Codifica degli Effetti (Effects Coding Method):\n\n\nFunzionamento: A differenza dei metodi precedenti, questo non richiede la selezione di una variabile di riferimento e non implica la standardizzazione dei fattori.\n\nVincolo di Codifica degli Effetti (ECI): Si impone che la media dei carichi fattoriali per gli indicatori di un dato fattore sia uguale a 1.0. Questo obbliga il software SEM a trovare una combinazione ottimale di carichi che, in media, risultino in 1.0.\n\nStima della Varianza del Fattore: La varianza del fattore viene stimata come la varianza comune media calcolata attraverso tutti gli indicatori, considerando il loro contributo individuale alla misurazione del fattore.\n\nVantaggi: Questo metodo consente che tutti gli indicatori contribuiscano equamente alla scalatura del loro fattore comune. È particolarmente utile in studi longitudinali o quando si confrontano gruppi diversi, dato che le varianze dei fattori possono fornire informazioni preziose.\n\n\n\nOgni metodo di scalatura presenta vantaggi specifici e limitazioni, che devono essere considerati in base agli obiettivi della ricerca e alle caratteristiche del modello CFA:\n\nIl Metodo di Standardizzazione della Varianza offre una soluzione semplice e diretta, ma potrebbe non essere sempre il più appropriato, specialmente in contesti dove i fattori sono endogeni.\nIl Metodo di Codifica degli Effetti è vantaggioso per stabilire una scalatura equilibrata e stabile dei fattori, utile soprattutto in studi comparativi o longitudinali.\n\nLa scelta del metodo di scalatura dovrebbe essere guidata dalle necessità specifiche della ricerca, dalla struttura dei dati e dalle ipotesi teoriche del modello di CFA utilizzato.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#esempio-di-cfa-per-un-modello-di-abilità-cognitive-la-kaufman-assessment-battery-for-children",
    "href": "chapters/cfa/01_cfa.html#esempio-di-cfa-per-un-modello-di-abilità-cognitive-la-kaufman-assessment-battery-for-children",
    "title": "31  Analisti Fattoriale Confermativa",
    "section": "\n31.8 Esempio di CFA per un Modello di Abilità Cognitive: La Kaufman Assessment Battery for Children",
    "text": "31.8 Esempio di CFA per un Modello di Abilità Cognitive: La Kaufman Assessment Battery for Children\nLa Kaufman Assessment Battery for Children (KABC-I) è un test di valutazione delle abilità cognitive, somministrato individualmente a bambini dagli 2 anni e mezzo ai 12 anni e mezzo (Kaufman & Kaufman, 1983). Questo strumento è stato progettato per misurare due distinti fattori cognitivi attraverso otto indicatori.\nI primi tre compiti del test sono orientati all’elaborazione sequenziale e richiedono ai bambini di ricordare e ripetere stimoli uditivi (come nel Richiamo Numerico e nell’Ordine delle Parole) o visivi (come nei Movimenti della Mano) in un ordine specifico. Questi compiti sono pensati per riflettere la capacità di memoria a breve termine e di sequenziamento delle informazioni.\nGli altri cinque compiti, che includono la Chiusura Gestaltica e la Serie Fotografica, sono ritenuti misurare un tipo di ragionamento più olistico e meno sequenziale, associato all’elaborazione simultanea. Questi compiti valutano la capacità di integrare e sintetizzare le informazioni visuo-spaziali in un contesto più ampio, spesso indipendentemente dall’ordine in cui le informazioni sono presentate.\nKeith (1985) ha proposto delle denominazioni alternative per i fattori misurati dalla KABC-I, suggerendo i termini “memoria a breve termine” per sostituire “elaborazione sequenziale” e “ragionamento visuo-spaziale” per “elaborazione simultanea”. Queste etichette alternative riflettono una prospettiva leggermente diversa sui tipi di competenze cognitive che i due fattori intendono misurare.\nQuesto modello di CFA, utilizzando i compiti della KABC-I come indicatori, fornisce una struttura utile per comprendere come diversi tipi di elaborazione cognitiva possano essere categorizzati e valutati nei contesti educativi e diagnostici.\n\n# input the correlations in lower diagnonal form\nkabcLower.cor &lt;- \"\n 1.00\n .39 1.00\n .35  .67 1.00\n .21  .11  .16 1.00\n .32  .27  .29  .38 1.00\n .40  .29  .28  .30  .47 1.00\n .39  .32  .30  .31  .42  .41 1.00\n .39  .29  .37  .42  .58  .51  .42 1.00 \"\n\n# name the variables and convert to full correlation matrix\n# hm, hand movements; nr, number recall; wo, word order; gc, gestalt closure;\n# tr, triangles; sm, spatial memory; ma, matrix analogies; ps, photo series\nkabc.cor &lt;- lavaan::getCov(kabcLower.cor, names = c(\n    \"hm\", \"nr\", \"wo\",\n    \"gc\", \"tr\", \"sm\", \"ma\", \"ps\"\n))\n# display correlations\nkabc.cor\n#&gt;      hm   nr   wo   gc   tr   sm   ma   ps\n#&gt; hm 1.00 0.39 0.35 0.21 0.32 0.40 0.39 0.39\n#&gt; nr 0.39 1.00 0.67 0.11 0.27 0.29 0.32 0.29\n#&gt; wo 0.35 0.67 1.00 0.16 0.29 0.28 0.30 0.37\n#&gt; gc 0.21 0.11 0.16 1.00 0.38 0.30 0.31 0.42\n#&gt; tr 0.32 0.27 0.29 0.38 1.00 0.47 0.42 0.58\n#&gt; sm 0.40 0.29 0.28 0.30 0.47 1.00 0.41 0.51\n#&gt; ma 0.39 0.32 0.30 0.31 0.42 0.41 1.00 0.42\n#&gt; ps 0.39 0.29 0.37 0.42 0.58 0.51 0.42 1.00\n\n\n# add the standard deviations and convert to covariances\nkabc.cov &lt;- lavaan::cor2cov(kabc.cor, sds = c(3.40, 2.40, 2.90, 2.70, 2.70, 4.20, 2.80, 3.00))\n\n# display covariances\nkabc.cov\n#&gt;        hm     nr    wo     gc    tr     sm    ma    ps\n#&gt; hm 11.560 3.1824 3.451 1.9278 2.938  5.712 3.713 3.978\n#&gt; nr  3.182 5.7600 4.663 0.7128 1.750  2.923 2.150 2.088\n#&gt; wo  3.451 4.6632 8.410 1.2528 2.271  3.410 2.436 3.219\n#&gt; gc  1.928 0.7128 1.253 7.2900 2.770  3.402 2.344 3.402\n#&gt; tr  2.938 1.7496 2.271 2.7702 7.290  5.330 3.175 4.698\n#&gt; sm  5.712 2.9232 3.410 3.4020 5.330 17.640 4.822 6.426\n#&gt; ma  3.713 2.1504 2.436 2.3436 3.175  4.822 7.840 3.528\n#&gt; ps  3.978 2.0880 3.219 3.4020 4.698  6.426 3.528 9.000\n\n\n\n\n\n\nFigura 31.3: Modello CFA per la Kaufman Assessment Battery for Children. (Figura tratta da Kline (2023))\n\n\n\n31.8.0.1 Modello a Fattore Singolo\nNell’ambito della CFA, se il modello bersaglio ha due o più fattori, spesso il primo modello analizzato è un modello a fattore singolo. Se non si può rigettare un modello a fattore singolo, non ha molto senso valutare modelli con più fattori.\nSpecifichiamo il modello ad un fattore comune nella sintassi di lavaan.\n\n# single factor (general ability)\n# indicator hm automatically specified as reference variable\nkabc1.model &lt;- \"\n    General =~ hm + nr + wo + gc + tr + sm + ma + ps \n\"\n\nAdattiamo il modello ai dati.\n\n# variances calculated with N in the denominator, not N - 1\nkabc1 &lt;- lavaan::sem(\n    kabc1.model, \n    sample.cov = kabc.cov, \n    sample.nobs = 200\n)\n\nGeneriamo un modello di percorso.\n\nsemPlot::semPaths(kabc1,\n    what = \"col\", whatLabels = \"par\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\nEsaminiamo la soluzione non standardizzata.\n\nlavaan::parameterEstimates(kabc1) \n#&gt;        lhs op     rhs   est    se     z pvalue ci.lower ci.upper\n#&gt; 1  General =~      hm 1.000 0.000    NA     NA    1.000    1.000\n#&gt; 2  General =~      nr 0.636 0.111 5.708      0    0.418    0.854\n#&gt; 3  General =~      wo 0.805 0.136 5.910      0    0.538    1.072\n#&gt; 4  General =~      gc 0.659 0.123 5.361      0    0.418    0.900\n#&gt; 5  General =~      tr 0.963 0.138 6.984      0    0.693    1.233\n#&gt; 6  General =~      sm 1.433 0.211 6.796      0    1.019    1.846\n#&gt; 7  General =~      ma 0.883 0.137 6.459      0    0.615    1.151\n#&gt; 8  General =~      ps 1.166 0.159 7.324      0    0.854    1.478\n#&gt; 9       hm ~~      hm 7.812 0.863 9.049      0    6.120    9.504\n#&gt; 10      nr ~~      nr 4.240 0.456 9.294      0    3.345    5.134\n#&gt; 11      wo ~~      wo 5.975 0.650 9.195      0    4.702    7.249\n#&gt; 12      gc ~~      gc 5.652 0.599 9.432      0    4.478    6.827\n#&gt; 13      tr ~~      tr 3.831 0.468 8.186      0    2.914    4.748\n#&gt; 14      sm ~~      sm 9.979 1.179 8.463      0    7.668   12.290\n#&gt; 15      ma ~~      ma 4.925 0.558 8.822      0    3.831    6.020\n#&gt; 16      ps ~~      ps 3.936 0.531 7.410      0    2.895    4.977\n#&gt; 17 General ~~ General 3.690 0.921 4.008      0    1.885    5.494\n\nLa saturazione non standardizzata per il compito “Movimenti della Mano” è stato fissato automaticamente a 1.0 per scalare il singolo fattore comune. Le istruzioni seguenti consentono di estrarre dall’output di sem() solo le informazioni relative alle saturazioni fattoriali.\n\nparameterEstimates(kabc1, standardized = TRUE) %&gt;%\n    dplyr::filter(op == \"=~\") %&gt;%\n    dplyr::select(\n        \"Latent Factor\" = lhs,\n        Indicator = rhs,\n        B = est,\n        SE = se,\n        Z = z,\n        \"p-value\" = pvalue,\n        Beta = std.all\n    ) %&gt;%\n    knitr::kable(\n        digits = 3, booktabs = TRUE, format = \"markdown\",\n        caption = \"Factor Loadings\"\n    )\n\n\nFactor Loadings\n\nLatent Factor\nIndicator\nB\nSE\nZ\np-value\nBeta\n\n\n\nGeneral\nhm\n1.000\n0.000\nNA\nNA\n0.566\n\n\nGeneral\nnr\n0.636\n0.111\n5.708\n0\n0.510\n\n\nGeneral\nwo\n0.805\n0.136\n5.910\n0\n0.535\n\n\nGeneral\ngc\n0.659\n0.123\n5.361\n0\n0.470\n\n\nGeneral\ntr\n0.963\n0.138\n6.984\n0\n0.687\n\n\nGeneral\nsm\n1.433\n0.211\n6.796\n0\n0.657\n\n\nGeneral\nma\n0.883\n0.137\n6.459\n0\n0.607\n\n\nGeneral\nps\n1.166\n0.159\n7.324\n0\n0.749\n\n\n\n\n\nEsaminiamo le misure di bontà di adattamento.\n\nfitMeasures(kabc1, c(\"chisq\", \"df\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt; print()\n#&gt;   chisq      df     cfi     tli   rmsea    srmr \n#&gt; 105.427  20.000   0.818   0.746   0.146   0.084\n\nTroviamo i residui grezzi, ovvero la differenza tra la matrice di covarianza osservata e quella predetta dal modello.\n\nlavaan::residuals(kabc1, type = \"raw\") |&gt; print()\n#&gt; $type\n#&gt; [1] \"raw\"\n#&gt; \n#&gt; $cov\n#&gt;        hm     nr     wo     gc     tr     sm     ma     ps\n#&gt; hm  0.000                                                 \n#&gt; nr  0.820  0.000                                          \n#&gt; wo  0.462  2.751  0.000                                   \n#&gt; gc -0.513 -0.836 -0.711  0.000                            \n#&gt; tr -0.631 -0.519 -0.602  0.415  0.000                     \n#&gt; sm  0.397 -0.452 -0.863 -0.097  0.212  0.000              \n#&gt; ma  0.437  0.069 -0.199  0.186  0.022  0.131  0.000       \n#&gt; ps -0.345 -0.659 -0.263  0.550  0.530  0.229 -0.289  0.000\n\nSpecificando type = \"cor.bollen\" o type = \"cor\" otteniamo la differenza tra la matrice di correlazione osservata e quella predetta dal modello.\n\nlavaan::residuals(kabc1, type = \"cor.bollen\") |&gt; print()\n#&gt; $type\n#&gt; [1] \"cor.bollen\"\n#&gt; \n#&gt; $cov\n#&gt;        hm     nr     wo     gc     tr     sm     ma     ps\n#&gt; hm  0.000                                                 \n#&gt; nr  0.101  0.000                                          \n#&gt; wo  0.047  0.397  0.000                                   \n#&gt; gc -0.056 -0.130 -0.091  0.000                            \n#&gt; tr -0.069 -0.080 -0.077  0.057  0.000                     \n#&gt; sm  0.028 -0.045 -0.071 -0.009  0.019  0.000              \n#&gt; ma  0.046  0.010 -0.025  0.025  0.003  0.011  0.000       \n#&gt; ps -0.034 -0.092 -0.030  0.068  0.066  0.018 -0.035  0.000\n\nIn alternativa, possiamo ottenere i residui standardizzati alla maniera di Mplus (standardized.mplus), che vengono calcolati utilizzando la seguente formula:\n\\[\n    \\text{Residuo Standardizzato} = \\frac{\\text{Cov. Osservata} - \\text{Cov. Stimata}}{\\sqrt{\\text{Var. dell'Errore per X} \\times \\text{Var. dell'Errore per Y}}},\n\\]\ndove: - La covarianza osservata è il valore della covarianza tra due variabili nel set di dati. - La covarianza stimata è la covarianza tra le stesse due variabili, come previsto dal modello SEM. - La varianza dell’errore per la variabile X e Y sono le varianze degli errori per le due variabili in questione.\nI residui standardizzati misurano quanto la relazione osservata tra due variabili si discosta da quella prevista dal modello, in unità standardizzate. Un valore vicino a zero indica che il modello si adatta bene ai dati per quella specifica relazione. Valori più grandi in valore assoluto suggeriscono un cattivo adattamento in quella specifica parte del modello.\n\nlavaan::residuals(kabc1, type = \"standardized.mplus\") |&gt; print()\n#&gt; $type\n#&gt; [1] \"standardized.mplus\"\n#&gt; \n#&gt; $cov\n#&gt;        hm     nr     wo     gc     tr     sm     ma     ps\n#&gt; hm  0.000                                                 \n#&gt; nr  2.062  0.000                                          \n#&gt; wo  1.026  6.218  0.000                                   \n#&gt; gc -1.231 -2.727 -1.952  0.000                            \n#&gt; tr -2.200 -2.364 -2.355  1.379  0.000                     \n#&gt; sm  0.723 -1.188 -1.995 -0.210  0.596  0.000              \n#&gt; ma  1.086  0.237 -0.601  0.544  0.089  0.313  0.000       \n#&gt; ps -1.241 -3.422 -1.037  1.833  2.178  0.675 -1.375  0.000\n\nIl modello a fattore singolo mostra un rapporto elevato chi-quadro/df. Inoltre, i residui per questa analisi indicano che l’adattamento locale è scadente. Pertanto, il modello a fattore singolo per la KABC-I è rigettato.\n\n31.8.0.2 Modello a Due Fattori\nIn una seconda analisi, adattiamo ai dati il modello a due fattori rappresentato nella {numref}kline-14-3-fig.\n\nkabc2_model &lt;- \"\n    Sequent =~ hm + nr + wo\n    Simultan =~ gc + tr + sm + ma + ps \n\"\n\n\nkabc2 &lt;- lavaan::sem(kabc2_model, sample.cov = kabc.cov, sample.nobs = 200)\n\n\nsemPlot::semPaths(kabc2,\n    what = \"col\", whatLabels = \"par\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\n\nlavaan::parameterEstimates(kabc2) |&gt; print()\n#&gt;         lhs op      rhs   est    se     z pvalue ci.lower ci.upper\n#&gt; 1   Sequent =~       hm 1.000 0.000    NA     NA    1.000    1.000\n#&gt; 2   Sequent =~       nr 1.147 0.181 6.341  0.000    0.792    1.501\n#&gt; 3   Sequent =~       wo 1.388 0.219 6.340  0.000    0.959    1.817\n#&gt; 4  Simultan =~       gc 1.000 0.000    NA     NA    1.000    1.000\n#&gt; 5  Simultan =~       tr 1.445 0.227 6.352  0.000    0.999    1.890\n#&gt; 6  Simultan =~       sm 2.029 0.335 6.062  0.000    1.373    2.685\n#&gt; 7  Simultan =~       ma 1.212 0.212 5.717  0.000    0.797    1.628\n#&gt; 8  Simultan =~       ps 1.727 0.265 6.521  0.000    1.208    2.246\n#&gt; 9        hm ~~       hm 8.664 0.938 9.237  0.000    6.826   10.502\n#&gt; 10       nr ~~       nr 1.998 0.414 4.831  0.000    1.188    2.809\n#&gt; 11       wo ~~       wo 2.902 0.604 4.801  0.000    1.717    4.087\n#&gt; 12       gc ~~       gc 5.419 0.585 9.261  0.000    4.272    6.566\n#&gt; 13       tr ~~       tr 3.426 0.458 7.479  0.000    2.528    4.323\n#&gt; 14       sm ~~       sm 9.997 1.202 8.320  0.000    7.642   12.353\n#&gt; 15       ma ~~       ma 5.105 0.578 8.838  0.000    3.973    6.237\n#&gt; 16       ps ~~       ps 3.482 0.537 6.482  0.000    2.429    4.535\n#&gt; 17  Sequent ~~  Sequent 2.838 0.838 3.389  0.001    1.197    4.480\n#&gt; 18 Simultan ~~ Simultan 1.834 0.530 3.459  0.001    0.795    2.874\n#&gt; 19  Sequent ~~ Simultan 1.271 0.324 3.918  0.000    0.635    1.907\n\n\nstandardizedSolution(kabc2) |&gt; print()\n#&gt;         lhs op      rhs est.std    se      z pvalue ci.lower ci.upper\n#&gt; 1   Sequent =~       hm   0.497 0.062  8.031      0    0.376    0.618\n#&gt; 2   Sequent =~       nr   0.807 0.046 17.442      0    0.716    0.898\n#&gt; 3   Sequent =~       wo   0.808 0.046 17.478      0    0.718    0.899\n#&gt; 4  Simultan =~       gc   0.503 0.061  8.260      0    0.384    0.622\n#&gt; 5  Simultan =~       tr   0.726 0.044 16.462      0    0.640    0.813\n#&gt; 6  Simultan =~       sm   0.656 0.050 13.227      0    0.559    0.753\n#&gt; 7  Simultan =~       ma   0.588 0.055 10.716      0    0.480    0.695\n#&gt; 8  Simultan =~       ps   0.782 0.040 19.483      0    0.703    0.860\n#&gt; 9        hm ~~       hm   0.753 0.061 12.258      0    0.633    0.874\n#&gt; 10       nr ~~       nr   0.349 0.075  4.669      0    0.202    0.495\n#&gt; 11       wo ~~       wo   0.347 0.075  4.640      0    0.200    0.493\n#&gt; 12       gc ~~       gc   0.747 0.061 12.201      0    0.627    0.867\n#&gt; 13       tr ~~       tr   0.472 0.064  7.365      0    0.347    0.598\n#&gt; 14       sm ~~       sm   0.570 0.065  8.752      0    0.442    0.697\n#&gt; 15       ma ~~       ma   0.654 0.065 10.145      0    0.528    0.781\n#&gt; 16       ps ~~       ps   0.389 0.063  6.199      0    0.266    0.512\n#&gt; 17  Sequent ~~  Sequent   1.000 0.000     NA     NA    1.000    1.000\n#&gt; 18 Simultan ~~ Simultan   1.000 0.000     NA     NA    1.000    1.000\n#&gt; 19  Sequent ~~ Simultan   0.557 0.067  8.346      0    0.426    0.688\n\n\nfitMeasures(kabc2, c(\"chisq\", \"df\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt; print()\n#&gt;  chisq     df    cfi    tli  rmsea   srmr \n#&gt; 38.325 19.000  0.959  0.939  0.071  0.072\n\nUn modello di analisi fattoriale confermativa (CFA) che utilizza un singolo fattore può essere visto come un caso specifico o un “sottoinsieme” di modelli CFA più complessi con due o più fattori che impiegano gli stessi indicatori e lo stesso schema di covarianza degli errori, se presente. Questa struttura gerarchica tra i modelli a singolo fattore e quelli multifattoriali implica che i ricercatori possono applicare il test del chi-quadro per confrontare direttamente l’adattamento di un modello CFA a singolo fattore con quello di modelli CFA a più fattori. In pratica, ciò permette di valutare se l’introduzione di fattori aggiuntivi migliora significativamente l’adattamento del modello ai dati rispetto a un modello più semplice a singolo fattore. Questo tipo di analisi è fondamentale per determinare la complessità ottimale del modello in base alla struttura sottostante dei dati. Sebbene questo argomento verrà approfondito successivamente, è importante anticipare qui l’utilizzo del test del rapporto di verosimiglianza, che consente di confrontare i modelli in maniera quantitativa.\n\nlavTestLRT(kabc1, kabc2) |&gt; print()\n#&gt; \n#&gt; Chi-Squared Difference Test\n#&gt; \n#&gt;       Df  AIC  BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\n#&gt; kabc2 19 7592 7648  38.3                                    \n#&gt; kabc1 20 7657 7710 105.4       67.1 0.575       1    2.6e-16\n\nI risultati del test indicano che l’adattamento del modello con due fattori è statisticamente migliore rispetto a quello del modello a fattore singolo (il modello ad un fattore ha un valore \\(\\chi^2\\) superiore di 67.1 punti, con un grado di libertà).\nAnche se il test del rapporto tra verosimiglianze favorisce il modello a due fattori, possiamo notare che l’esame dei residui mostra un problema con l’indicatore hm.\n\nlavaan::residuals(kabc2, type = \"standardized.mplus\") |&gt; print()\n#&gt; $type\n#&gt; [1] \"standardized.mplus\"\n#&gt; \n#&gt; $cov\n#&gt;        hm     nr     wo     gc     tr     sm     ma     ps\n#&gt; hm  0.000                                                 \n#&gt; nr -0.591  0.000                                          \n#&gt; wo -3.790  1.539  0.000                                   \n#&gt; gc  1.126 -2.329 -1.315     NA                            \n#&gt; tr  2.046 -1.558 -1.001  0.429  0.000                     \n#&gt; sm  3.464 -0.112 -0.355 -0.784 -0.267  0.000              \n#&gt; ma  3.505  1.129  0.727  0.323 -0.245  0.664  0.008       \n#&gt; ps  2.991 -2.002  0.524  0.910  0.677 -0.144 -1.978  0.000\n\nPer affrontare questo problema, calcoliamo i modification indices che ci dicono quale parametro del modello ha l’effetto maggiore sulla misura di fit complessivo.\n\nmodindices(kabc2, sort = TRUE, maximum.number = 5) |&gt; print()\n#&gt;         lhs op rhs     mi    epc sepc.lv sepc.all sepc.nox\n#&gt; 25 Simultan =~  hm 20.097  1.054   1.428    0.421    0.421\n#&gt; 35       nr ~~  wo 20.097  4.741   4.741    1.969    1.969\n#&gt; 26 Simultan =~  nr  7.013 -0.510  -0.691   -0.289   -0.289\n#&gt; 29       hm ~~  wo  7.013 -1.746  -1.746   -0.348   -0.348\n#&gt; 32       hm ~~  sm  4.847  1.609   1.609    0.173    0.173\n\nI risultati degli indici di modifica (MI) indicano che il misfit del modello è principalmente attribuibile alla fissazione a zero del carico tra l’indicatore hm e il fattore comune Simulan, nonché alla fissazione a zero della covarianza tra le componenti residue di nr e wo. Per migliorare l’adattamento del modello, si propone quindi di modificare questi aspetti, iniziando con il primo, ovvero riconsiderando il carico di hm sul fattore Simulan.\n\nkabc3_model &lt;- \"\n    Sequent =~ hm + nr + wo\n    Simultan =~ hm + gc + tr + sm + ma + ps\n\"\n\n\nkabc3 &lt;- lavaan::sem(kabc3_model, sample.cov = kabc.cov, sample.nobs = 200)\n\n\nlavaan::parameterEstimates(kabc3) |&gt; print()\n#&gt;         lhs op      rhs   est    se     z pvalue ci.lower ci.upper\n#&gt; 1   Sequent =~       hm 1.000 0.000    NA     NA    1.000    1.000\n#&gt; 2   Sequent =~       nr 2.285 0.777 2.941  0.003    0.762    3.808\n#&gt; 3   Sequent =~       wo 2.767 0.941 2.939  0.003    0.922    4.612\n#&gt; 4  Simultan =~       hm 1.000 0.000    NA     NA    1.000    1.000\n#&gt; 5  Simultan =~       gc 1.014 0.255 3.979  0.000    0.515    1.514\n#&gt; 6  Simultan =~       tr 1.457 0.329 4.427  0.000    0.812    2.101\n#&gt; 7  Simultan =~       sm 2.103 0.483 4.354  0.000    1.157    3.050\n#&gt; 8  Simultan =~       ma 1.259 0.298 4.229  0.000    0.675    1.842\n#&gt; 9  Simultan =~       ps 1.752 0.391 4.486  0.000    0.987    2.518\n#&gt; 10       hm ~~       hm 7.851 0.845 9.291  0.000    6.195    9.507\n#&gt; 11       nr ~~       nr 1.899 0.487 3.896  0.000    0.944    2.854\n#&gt; 12       wo ~~       wo 2.750 0.713 3.856  0.000    1.352    4.148\n#&gt; 13       gc ~~       gc 5.444 0.585 9.297  0.000    4.296    6.591\n#&gt; 14       tr ~~       tr 3.521 0.457 7.702  0.000    2.625    4.417\n#&gt; 15       sm ~~       sm 9.767 1.179 8.287  0.000    7.457   12.077\n#&gt; 16       ma ~~       ma 5.013 0.569 8.815  0.000    3.898    6.127\n#&gt; 17       ps ~~       ps 3.554 0.529 6.718  0.000    2.517    4.591\n#&gt; 18  Sequent ~~  Sequent 0.734 0.490 1.499  0.134   -0.226    1.693\n#&gt; 19 Simultan ~~ Simultan 1.759 0.760 2.314  0.021    0.269    3.250\n#&gt; 20  Sequent ~~ Simultan 0.579 0.178 3.252  0.001    0.230    0.928\n\nIl modello così modificato fornisce un buon adattamento ai dati.\n\nfitMeasures(kabc3, c(\"chisq\", \"df\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt; print()\n#&gt;  chisq     df    cfi    tli  rmsea   srmr \n#&gt; 18.108 18.000  1.000  1.000  0.005  0.035\n\n\nlavaan::residuals(kabc3, type = \"standardized.mplus\") |&gt; print()\n#&gt; $type\n#&gt; [1] \"standardized.mplus\"\n#&gt; \n#&gt; $cov\n#&gt;        hm     nr     wo     gc     tr     sm     ma     ps\n#&gt; hm  0.000                                                 \n#&gt; nr  1.165  0.000                                          \n#&gt; wo -1.637  0.000  0.000                                   \n#&gt; gc -1.066 -1.919 -0.939  0.000                            \n#&gt; tr -1.710 -0.763 -0.247  0.603  0.000                     \n#&gt; sm  1.325  0.287  0.044 -0.867 -0.304  0.000              \n#&gt; ma  1.730  1.428  1.029  0.258 -0.298  0.338  0.000       \n#&gt; ps -0.512 -1.059  1.285  1.035  1.088 -0.361 -2.181  0.008\n\nEseguiamo il confronto tra questo terzo modello e il secondo.\n\nlavTestLRT(kabc2, kabc3) |&gt; print()\n#&gt; \n#&gt; Chi-Squared Difference Test\n#&gt; \n#&gt;       Df  AIC  BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\n#&gt; kabc3 18 7574 7633  18.1                                    \n#&gt; kabc2 19 7592 7648  38.3       20.2  0.31       1    6.9e-06\n\nIl test del rapporto tra verosimiglianze favorisce il modello nel quale hm satura su entrambi i fattori comuni.\n\nstandardizedSolution(kabc3) |&gt; print()\n#&gt;         lhs op      rhs est.std    se      z pvalue ci.lower ci.upper\n#&gt; 1   Sequent =~       hm   0.253 0.082  3.073  0.002    0.091    0.414\n#&gt; 2   Sequent =~       nr   0.818 0.053 15.336  0.000    0.713    0.922\n#&gt; 3   Sequent =~       wo   0.819 0.053 15.366  0.000    0.715    0.924\n#&gt; 4  Simultan =~       hm   0.391 0.079  4.938  0.000    0.236    0.546\n#&gt; 5  Simultan =~       gc   0.500 0.061  8.211  0.000    0.380    0.619\n#&gt; 6  Simultan =~       tr   0.717 0.044 16.192  0.000    0.631    0.804\n#&gt; 7  Simultan =~       sm   0.666 0.048 13.755  0.000    0.571    0.761\n#&gt; 8  Simultan =~       ma   0.598 0.054 11.114  0.000    0.492    0.703\n#&gt; 9  Simultan =~       ps   0.777 0.040 19.532  0.000    0.699    0.855\n#&gt; 10       hm ~~       hm   0.683 0.061 11.227  0.000    0.563    0.802\n#&gt; 11       nr ~~       nr   0.331 0.087  3.800  0.000    0.160    0.502\n#&gt; 12       wo ~~       wo   0.329 0.087  3.761  0.000    0.157    0.500\n#&gt; 13       gc ~~       gc   0.750 0.061 12.348  0.000    0.631    0.870\n#&gt; 14       tr ~~       tr   0.485 0.064  7.636  0.000    0.361    0.610\n#&gt; 15       sm ~~       sm   0.556 0.064  8.628  0.000    0.430    0.683\n#&gt; 16       ma ~~       ma   0.643 0.064  9.992  0.000    0.517    0.769\n#&gt; 17       ps ~~       ps   0.397 0.062  6.426  0.000    0.276    0.518\n#&gt; 18  Sequent ~~  Sequent   1.000 0.000     NA     NA    1.000    1.000\n#&gt; 19 Simultan ~~ Simultan   1.000 0.000     NA     NA    1.000    1.000\n#&gt; 20  Sequent ~~ Simultan   0.510 0.070  7.254  0.000    0.372    0.647\n\n\nsemPlot::semPaths(kabc3,\n    what = \"col\", whatLabels = \"std\", style = \"mx\",\n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\nNei modelli precedenti, abbiamo adottato un metodo di scalatura dei fattori comuni che fissava la saturazione fattoriale di uno degli indicatori per ciascun fattore comune a 1.0 come riferimento. Ora, esploreremo un diverso approccio di scalatura che prevede la standardizzazione della varianza delle variabili latenti.\nPer attuare questa procedura nel software lavaan, è necessario modificare la configurazione predefinita in cui la saturazione fattoriale del primo indicatore di ogni fattore comune è fissata a 1.0. Per fare ciò, useremo la sintassi NA* per indicare che la saturazione fattoriale del primo indicatore deve essere stimata. Questo si realizza inserendo NA* nell’istruzione che definisce la relazione tra le variabili latenti e gli indicatori (espresso tramite =~). Inoltre, è fondamentale specificare che la varianza delle variabili latenti sia fissata a 1.0, il che si attua mediante la sintassi 1* nell’istruzione che stabilisce la varianza di ciascun fattore comune (~~).\n\nkabc3alt_model &lt;- \"\n    Sequent =~ NA*hm + nr + wo\n    Simultan =~ NA*hm + gc + tr + sm + ma + ps\n\n    Sequent ~~ 1*Sequent\n    Simultan ~~ 1*Simultan\n\"\n\nAdattiamo il modello così parametrizzato ai dati.\n\nkabc3alt &lt;- lavaan::sem(\n    kabc3alt_model, sample.cov = kabc.cov, sample.nobs = 200, std.lv = TRUE\n)\n\nEsaminiamo la soluzione non standardizzata.\n\nsemPlot::semPaths(kabc3alt,\n    what = \"col\", whatLabels = \"par\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\nEsaminiamo la soluzione stanardizzata.\n\nloadings &lt;- standardizedSolution(kabc3alt)\nloadings |&gt; print()\n#&gt;         lhs op      rhs est.std    se      z pvalue ci.lower ci.upper\n#&gt; 1   Sequent =~       hm   0.253 0.082  3.073  0.002    0.091    0.414\n#&gt; 2   Sequent =~       nr   0.818 0.053 15.336  0.000    0.713    0.922\n#&gt; 3   Sequent =~       wo   0.819 0.053 15.366  0.000    0.715    0.924\n#&gt; 4  Simultan =~       hm   0.391 0.079  4.938  0.000    0.236    0.546\n#&gt; 5  Simultan =~       gc   0.500 0.061  8.211  0.000    0.380    0.619\n#&gt; 6  Simultan =~       tr   0.717 0.044 16.192  0.000    0.631    0.804\n#&gt; 7  Simultan =~       sm   0.666 0.048 13.755  0.000    0.571    0.761\n#&gt; 8  Simultan =~       ma   0.598 0.054 11.114  0.000    0.492    0.703\n#&gt; 9  Simultan =~       ps   0.777 0.040 19.532  0.000    0.699    0.855\n#&gt; 10  Sequent ~~  Sequent   1.000 0.000     NA     NA    1.000    1.000\n#&gt; 11 Simultan ~~ Simultan   1.000 0.000     NA     NA    1.000    1.000\n#&gt; 12       hm ~~       hm   0.683 0.061 11.227  0.000    0.563    0.802\n#&gt; 13       nr ~~       nr   0.331 0.087  3.800  0.000    0.160    0.502\n#&gt; 14       wo ~~       wo   0.329 0.087  3.761  0.000    0.157    0.500\n#&gt; 15       gc ~~       gc   0.750 0.061 12.347  0.000    0.631    0.870\n#&gt; 16       tr ~~       tr   0.485 0.064  7.636  0.000    0.361    0.610\n#&gt; 17       sm ~~       sm   0.556 0.064  8.628  0.000    0.430    0.683\n#&gt; 18       ma ~~       ma   0.643 0.064  9.992  0.000    0.517    0.769\n#&gt; 19       ps ~~       ps   0.397 0.062  6.426  0.000    0.276    0.518\n#&gt; 20  Sequent ~~ Simultan   0.510 0.070  7.254  0.000    0.372    0.647\n\n\nrelevant_loadings &lt;- loadings[loadings$op == \"=~\", c(\"lhs\", \"rhs\", \"est.std\")]\nrelevant_loadings |&gt; print()\n#&gt;        lhs rhs est.std\n#&gt; 1  Sequent  hm   0.253\n#&gt; 2  Sequent  nr   0.818\n#&gt; 3  Sequent  wo   0.819\n#&gt; 4 Simultan  hm   0.391\n#&gt; 5 Simultan  gc   0.500\n#&gt; 6 Simultan  tr   0.717\n#&gt; 7 Simultan  sm   0.666\n#&gt; 8 Simultan  ma   0.598\n#&gt; 9 Simultan  ps   0.777\n\nIdealmente, per sostenere l’ipotesi di validità convergente, un fattore dovrebbe spiegare almeno il 50% della varianza in ciascuno dei suoi indicatori continui, come sostengono Bagozzi e Yi (2012). Ciò implica che, per essere considerato adeguatamente rappresentativo del costrutto che intende misurare, tutti gli indicatori di un fattore dovrebbero mostrare che la maggior parte della loro varianza è spiegata dal fattore stesso. Un modo meno rigoroso ma ancora informativo per valutare la validità convergente è attraverso l’uso della Varianza Media Estratta (AVE), calcolata come la media dei quadrati dei carichi fattoriali standardizzati di tutti gli indicatori associati a un particolare fattore. Un valore AVE superiore a 0.50 indica che, in media, il fattore spiega più della metà della varianza degli indicatori rispetto alla varianza residua attribuibile agli errori di misurazione, come indicato da Hair et al. (2022).\nNell’ambito di un modello a due fattori, i risultati ottenuti dall’esempio in esame evidenziano alcune criticità in relazione al criterio più stringente: il modello non riesce a spiegare una variazione significativa (R^2 &gt; 0.50) per quattro dei otto indicatori, ossia la metà di essi. Tuttavia, se consideriamo l’AVE, i risultati migliorano leggermente per il fattore sequenziale, che spiega in media circa il 52% della varianza dei suoi tre indicatori (AVE = 0.517).\nNella pratica analitica reale, valori di R^2 inferiori a 0.50 sono spesso considerati accettabili. Comrey e Lee (1992) hanno proposto una scala di valutazione gradiente in cui un R^2 superiore a 0.50 è classificato come eccellente, mentre valori approssimativamente pari a 0.40, 0.30, 0.20 e 0.10 sono considerati molto buoni, buoni, sufficienti e scarsi, rispettivamente. Secondo queste linee guida più flessibili, i risultati per gli indicatori del modello CFA a due fattori della KABC-I sono classificati come “eccellenti” (R^2 &gt; 0.50) per tre degli otto indicatori, nessuno è giudicato “scarso” (R^2 circa 0.10), e i rimanenti cinque indicatori presentano valori intermedi. È essenziale sottolineare che queste linee guida non dovrebbero essere applicate in modo indiscriminato in tutti i contesti di CFA o con tutti i tipi di indicatori. Gli indicatori continui, come i punteggi totali nell’esempio citato, tendono a mostrare carichi fattoriali più elevati rispetto agli indicatori ordinali, come quelli basati su scale di risposta tipo Likert.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#riflessioni-conclusive",
    "href": "chapters/cfa/01_cfa.html#riflessioni-conclusive",
    "title": "31  Analisti Fattoriale Confermativa",
    "section": "\n31.9 Riflessioni Conclusive",
    "text": "31.9 Riflessioni Conclusive\nL’analisi fattoriale confermativa (CFA) rappresenta uno strumento cruciale nell’ambito delle ricerche psicologiche e sociali, in quanto consente di esaminare modelli di misurazione riflessiva. In questi modelli, i fattori comuni agiscono come proxy per le variabili teoriche. La CFA richiede che il ricercatore definisca preventivamente aspetti critici del modello, come il numero di fattori, l’assegnazione degli indicatori ai fattori e gli schemi di covarianza degli errori.\nNei modelli CFA base, ciascun indicatore continuo è associato a un unico fattore e si presume che gli errori siano indipendenti, formando così una struttura unidimensionale. L’analisi di modelli con più fattori permette di verificare le ipotesi di validità convergente e discriminante.\nÈ anche possibile esplorare modelli CFA che includono covarianze di errore o indicatori correlati a più fattori. Tuttavia, gestire tali modelli è più complesso, specialmente in termini di identificazione del modello. Problemi tecnici come la non convergenza delle soluzioni o risultati inammissibili sono più comuni nei campioni di dimensioni ridotte o quando i fattori sono definiti da soli due indicatori. L’aggiustamento del modello può diventare una sfida, considerata l’ampia varietà di modifiche potenziali.\nUn’altra questione critica è rappresentata dai modelli CFA equivalenti, i quali possono produrre risultati simili nonostante le loro differenze strutturali. Per affrontare queste sfide efficacemente, è essenziale fondare l’analisi più su basi teoriche che su meri calcoli statistici. L’efficacia della CFA, quindi, dipende notevolmente dal contesto teorico e dalla competenza metodologica del ricercatore, essendo cruciale un’approfondita comprensione del dominio di studio per guidare l’analisi.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#session-info",
    "href": "chapters/cfa/01_cfa.html#session-info",
    "title": "31  Analisti Fattoriale Confermativa",
    "section": "\n31.10 Session Info",
    "text": "31.10 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.2\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] ggokabeito_0.1.0  see_0.11.0        MASS_7.3-65       viridis_0.6.5    \n#&gt;  [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n#&gt;  [9] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n#&gt; [13] lavaan_0.6-19     psych_2.4.12      scales_1.3.0      markdown_1.13    \n#&gt; [17] knitr_1.50        lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n#&gt; [21] dplyr_1.1.4       purrr_1.0.4       readr_2.1.5       tidyr_1.3.1      \n#&gt; [25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.1      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.2.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] mime_0.13           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [25] Matrix_1.7-3        R6_2.6.1            fastmap_1.2.0      \n#&gt;  [28] rbibutils_2.3       shiny_1.10.0        digest_0.6.37      \n#&gt;  [31] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n#&gt;  [34] rprojroot_2.0.4     Hmisc_5.2-3         timechange_0.3.0   \n#&gt;  [37] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n#&gt;  [40] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n#&gt;  [43] carData_3.0-5       ggsignif_0.6.4      corpcor_1.6.10     \n#&gt;  [46] gtools_3.9.5        tools_4.4.2         pbivnorm_0.6.0     \n#&gt;  [49] foreign_0.8-88      zip_2.3.2           httpuv_1.6.15      \n#&gt;  [52] nnet_7.3-20         glue_1.8.0          quadprog_1.5-8     \n#&gt;  [55] nlme_3.1-167        promises_1.3.2      lisrelToR_0.3      \n#&gt;  [58] grid_4.4.2          checkmate_2.3.2     cluster_2.1.8.1    \n#&gt;  [61] reshape2_1.4.4      generics_0.1.3      gtable_0.3.6       \n#&gt;  [64] tzdb_0.5.0          data.table_1.17.0   hms_1.1.3          \n#&gt;  [67] car_3.1-3           sem_3.1-16          pillar_1.10.1      \n#&gt;  [70] rockchalk_1.8.157   later_1.4.1         splines_4.4.2      \n#&gt;  [73] lattice_0.22-6      survival_3.8-3      kutils_1.73        \n#&gt;  [76] tidyselect_1.2.1    miniUI_0.1.1.1      pbapply_1.7-2      \n#&gt;  [79] reformulas_0.4.0    stats4_4.4.2        xfun_0.51          \n#&gt;  [82] qgraph_1.9.8        arm_1.14-4          stringi_1.8.4      \n#&gt;  [85] yaml_2.3.10         pacman_0.5.1        boot_1.3-31        \n#&gt;  [88] evaluate_1.0.3      codetools_0.2-20    mi_1.1             \n#&gt;  [91] cli_3.6.4           RcppParallel_5.1.10 rpart_4.1.24       \n#&gt;  [94] xtable_1.8-4        Rdpack_2.6.3        munsell_0.5.1      \n#&gt;  [97] Rcpp_1.0.14         coda_0.19-4.1       png_0.1-8          \n#&gt; [100] XML_3.99-0.18       parallel_4.4.2      jpeg_0.1-10        \n#&gt; [103] lme4_1.1-36         mvtnorm_1.3-3       openxlsx_4.2.8     \n#&gt; [106] rlang_1.1.5         multcomp_1.4-28     mnormt_2.1.1\n\n\n\n\n\nKline, R. B. (2023). Principles and practice of structural equation modeling. Guilford publications.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/02_meanstructure.html",
    "href": "chapters/cfa/02_meanstructure.html",
    "title": "32  La struttura delle medie",
    "section": "",
    "text": "32.1 Introduzione\nL’Analisi Fattoriale Confermativa (CFA) condivide con l’analisi fattoriale tradizionale l’obiettivo di esaminare le relazioni di covarianza tra le variabili. Tuttavia, una caratteristica distintiva della CFA è la possibilità di includere nel modello anche le medie, sia delle variabili osservate che di quelle latenti. Questo approccio si rivela particolarmente utile in contesti come l’analisi fattoriale confermativa longitudinale, dove le ipotesi non si limitano alle covarianze, ma riguardano anche i cambiamenti nelle medie dei costrutti analizzati nel tempo.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>La struttura delle medie</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/02_meanstructure.html#interpretazione-delle-intercette-nei-modelli-cfa",
    "href": "chapters/cfa/02_meanstructure.html#interpretazione-delle-intercette-nei-modelli-cfa",
    "title": "32  La struttura delle medie",
    "section": "\n32.2 Interpretazione delle Intercette nei Modelli CFA",
    "text": "32.2 Interpretazione delle Intercette nei Modelli CFA\nNei modelli CFA o nei Modelli di Equazioni Strutturali (SEM), l’intercetta di una variabile indicatore, denotata con \\(\\tau\\), rappresenta la media stimata dell’indicatore quando il fattore latente a cui è associato assume il valore zero. Questo consente di integrare nel modello informazioni sia sulle relazioni tra variabili (covarianze) sia sulle loro medie.\nLa relazione generale per un indicatore \\(y\\) in un modello CFS (o SEM) è espressa dalla seguente equazione:\n\\[\ny = \\tau + \\lambda \\cdot \\text{fattore latente} + \\varepsilon,\n\\]\ndove:\n\n\n\\(y\\): è il punteggio osservato dell’indicatore.\n\n\\(\\tau\\): è l’intercetta, che rappresenta la media stimata dell’indicatore quando il fattore latente è zero.\n\n\\(\\lambda\\): è il carico fattoriale, che misura la relazione tra il fattore latente e l’indicatore, ovvero quanto l’indicatore è influenzato dal fattore latente.\n\n\\(\\varepsilon\\): è l’errore di misura, che cattura la varianza dell’indicatore non spiegata dal fattore latente.\n\n\n32.2.1 Struttura delle Medie nel Modello CFA\nLa struttura delle medie in un modello CFA può essere espressa attraverso la seguente formula:\n\\[\n\\text{media(variabile osservata)} = \\Lambda \\mu_{\\text{lat}} + \\tau,\n\\]\ndove:\n\n\n\\(\\Lambda\\): è la matrice dei carichi fattoriali, che collega i fattori latenti agli indicatori.\n\n\\(\\mu_{\\text{lat}}\\): è il vettore delle medie dei fattori latenti.\n\n\\(\\tau\\): è il vettore delle intercette degli indicatori, che rappresenta le medie degli indicatori indipendentemente dai fattori latenti.\n\n32.2.2 Interpretazione\n\nCosa rappresenta \\(\\tau\\) concretamente?\nL’intercetta \\(\\tau\\) rappresenta il valore atteso di un indicatore quando il fattore latente associato ha un valore pari a zero. Per esempio, immagina un indicatore che misura la performance in un test. Se il fattore latente (ad esempio, “abilità generale”) è zero, \\(\\tau\\) indica la media attesa della performance in quella condizione specifica.\nPerché \\(\\tau\\) è importante?\\(\\tau\\) è fondamentale per interpretare il livello base dell’indicatore, consentendo di separare la varianza spiegata dai fattori latenti da quella attribuibile ad altre cause, come il livello medio dell’indicatore stesso. Questo è particolarmente utile per comprendere i punti di partenza dei partecipanti o i livelli medi degli indicatori in un contesto specifico.\n\n\n32.2.2.1 Esempio Intuitivo\nSupponiamo di analizzare i risultati di un test di matematica. Se il fattore latente rappresenta “abilità matematica” e il carico fattoriale \\(\\lambda\\) è elevato, ciò significa che i punteggi del test sono fortemente influenzati dall’abilità matematica. Tuttavia, \\(\\tau\\) fornisce un’informazione aggiuntiva: indica il punteggio medio nel test per chi ha un’abilità matematica pari a zero.\nSe i dati sono stati centrati, l’intercetta \\(\\tau\\) rappresenta la performance media prevista per i partecipanti con un’abilità matematica media rispetto al gruppo di riferimento.\n\n32.2.2.2 Importanza di \\(\\tau\\) nei Modelli SEM\nL’intercetta \\(\\tau\\) assume particolare rilievo nei modelli SEM applicati a:\n\n\nStudi longitudinali: Le variazioni di \\(\\tau\\) nel tempo possono indicare cambiamenti nei livelli medi degli indicatori, come miglioramenti o peggioramenti in una competenza specifica.\n\nConfronti tra gruppi: Differenze significative nelle intercette tra gruppi possono evidenziare disuguaglianze nei livelli medi di un indicatore, fornendo informazioni utili per analisi comparative.\n\nIn sintesi, l’intercetta \\(\\tau\\) è uno strumento chiave per comprendere e interpretare il comportamento degli indicatori nei modelli SEM, offrendo una visione chiara delle loro relazioni con i fattori latenti e delle differenze a livello di gruppo o temporale.\n\n32.2.3 Utilizzo delle Medie nel Software lavaan\n\nNel software lavaan, utilizzato per l’analisi SEM, è possibile stimare le intercette inserendo l’opzione meanstructure = TRUE nella sintassi del modello. Questo comando permette di includere automaticamente una costante “1” in tutte le equazioni del modello, facilitando così il calcolo delle intercette per le variabili endogene. È necessario fornire i dati originali o una matrice di covarianza, insieme alle medie di tutte le variabili interessate.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>La struttura delle medie</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/02_meanstructure.html#un-esempio-pratico",
    "href": "chapters/cfa/02_meanstructure.html#un-esempio-pratico",
    "title": "32  La struttura delle medie",
    "section": "\n32.3 Un Esempio Pratico",
    "text": "32.3 Un Esempio Pratico\nUtilizziamo il dataset HolzingerSwineford1939 per costruire un modello di misurazione basato su tre costrutti latenti:\n\n\nVisual (visual): rappresenta abilità visive, misurate dagli indicatori x1, x2 e x3.\n\nTextual (textual): rappresenta abilità testuali, misurate dagli indicatori x4, x5 e x6.\n\nSpeed (speed): rappresenta velocità di elaborazione, misurata dagli indicatori x7, x8 e x9.\n\nVisualizziamo una panoramica del dataset:\n\ndata(HolzingerSwineford1939)\nglimpse(HolzingerSwineford1939)\n#&gt; Rows: 301\n#&gt; Columns: 15\n#&gt; $ id     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, …\n#&gt; $ sex    &lt;int&gt; 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2,…\n#&gt; $ ageyr  &lt;int&gt; 13, 13, 13, 13, 12, 14, 12, 12, 13, 12, 12, 12, 12, 12, 12,…\n#&gt; $ agemo  &lt;int&gt; 1, 7, 1, 2, 2, 1, 1, 2, 0, 5, 2, 11, 7, 8, 6, 1, 11, 5, 8, …\n#&gt; $ school &lt;fct&gt; Pasteur, Pasteur, Pasteur, Pasteur, Pasteur, Pasteur, Paste…\n#&gt; $ grade  &lt;int&gt; 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,…\n#&gt; $ x1     &lt;dbl&gt; 3.333, 5.333, 4.500, 5.333, 4.833, 5.333, 2.833, 5.667, 4.5…\n#&gt; $ x2     &lt;dbl&gt; 7.75, 5.25, 5.25, 7.75, 4.75, 5.00, 6.00, 6.25, 5.75, 5.25,…\n#&gt; $ x3     &lt;dbl&gt; 0.375, 2.125, 1.875, 3.000, 0.875, 2.250, 1.000, 1.875, 1.5…\n#&gt; $ x4     &lt;dbl&gt; 2.333, 1.667, 1.000, 2.667, 2.667, 1.000, 3.333, 3.667, 2.6…\n#&gt; $ x5     &lt;dbl&gt; 5.75, 3.00, 1.75, 4.50, 4.00, 3.00, 6.00, 4.25, 5.75, 5.00,…\n#&gt; $ x6     &lt;dbl&gt; 1.2857, 1.2857, 0.4286, 2.4286, 2.5714, 0.8571, 2.8571, 1.2…\n#&gt; $ x7     &lt;dbl&gt; 3.391, 3.783, 3.261, 3.000, 3.696, 4.348, 4.696, 3.391, 4.5…\n#&gt; $ x8     &lt;dbl&gt; 5.75, 6.25, 3.90, 5.30, 6.30, 6.65, 6.20, 5.15, 4.65, 4.55,…\n#&gt; $ x9     &lt;dbl&gt; 6.361, 7.917, 4.417, 4.861, 5.917, 7.500, 4.861, 3.667, 7.3…\n\n\n32.3.1 Specifica del Modello\nOgni costrutto è definito in relazione ai propri indicatori. Le varianze dei costrutti latenti sono fissate a 1 per garantirne la scalatura, mentre le loro medie sono fissate a 0.\nEcco il modello specificato:\n\nhs_model &lt;- \"\n    visual =~ NA*x1 + x2 + x3\n    textual =~ NA*x4 + x5 + x6\n    speed =~ NA*x7 + x8 + x9\n\n    visual ~~ 1*visual\n    textual ~~ 1*textual\n    speed ~~ 1*speed\n\"\n\n\n32.3.2 Stima del Modello\nRichiediamo la stima delle intercette degli indicatori impostando meanstructure = TRUE. Le intercette (\\(\\tau\\)) rappresentano il valore medio atteso per ciascun indicatore quando il rispettivo fattore latente è pari a zero. Adattiamo il modello ai dati:\n\nfit &lt;- cfa(hs_model,\n    data = HolzingerSwineford1939,\n    meanstructure = TRUE\n)\n\nEsaminiamo un riepilogo dei risultati, inclusi i carichi fattoriali standardizzati:\n\nsummary(fit, standardized = TRUE)\n#&gt; lavaan 0.6-19 ended normally after 20 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        30\n#&gt; \n#&gt;   Number of observations                           301\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                85.306\n#&gt;   Degrees of freedom                                24\n#&gt;   P-value (Chi-square)                           0.000\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   visual =~                                                             \n#&gt;     x1                0.900    0.081   11.128    0.000    0.900    0.772\n#&gt;     x2                0.498    0.077    6.429    0.000    0.498    0.424\n#&gt;     x3                0.656    0.074    8.817    0.000    0.656    0.581\n#&gt;   textual =~                                                            \n#&gt;     x4                0.990    0.057   17.474    0.000    0.990    0.852\n#&gt;     x5                1.102    0.063   17.576    0.000    1.102    0.855\n#&gt;     x6                0.917    0.054   17.082    0.000    0.917    0.838\n#&gt;   speed =~                                                              \n#&gt;     x7                0.619    0.070    8.903    0.000    0.619    0.570\n#&gt;     x8                0.731    0.066   11.090    0.000    0.731    0.723\n#&gt;     x9                0.670    0.065   10.305    0.000    0.670    0.665\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   visual ~~                                                             \n#&gt;     textual           0.459    0.064    7.189    0.000    0.459    0.459\n#&gt;     speed             0.471    0.073    6.461    0.000    0.471    0.471\n#&gt;   textual ~~                                                            \n#&gt;     speed             0.283    0.069    4.117    0.000    0.283    0.283\n#&gt; \n#&gt; Intercepts:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .x1                4.936    0.067   73.473    0.000    4.936    4.235\n#&gt;    .x2                6.088    0.068   89.855    0.000    6.088    5.179\n#&gt;    .x3                2.250    0.065   34.579    0.000    2.250    1.993\n#&gt;    .x4                3.061    0.067   45.694    0.000    3.061    2.634\n#&gt;    .x5                4.341    0.074   58.452    0.000    4.341    3.369\n#&gt;    .x6                2.186    0.063   34.667    0.000    2.186    1.998\n#&gt;    .x7                4.186    0.063   66.766    0.000    4.186    3.848\n#&gt;    .x8                5.527    0.058   94.854    0.000    5.527    5.467\n#&gt;    .x9                5.374    0.058   92.546    0.000    5.374    5.334\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;     visual            1.000                               1.000    1.000\n#&gt;     textual           1.000                               1.000    1.000\n#&gt;     speed             1.000                               1.000    1.000\n#&gt;    .x1                0.549    0.114    4.833    0.000    0.549    0.404\n#&gt;    .x2                1.134    0.102   11.146    0.000    1.134    0.821\n#&gt;    .x3                0.844    0.091    9.317    0.000    0.844    0.662\n#&gt;    .x4                0.371    0.048    7.779    0.000    0.371    0.275\n#&gt;    .x5                0.446    0.058    7.642    0.000    0.446    0.269\n#&gt;    .x6                0.356    0.043    8.277    0.000    0.356    0.298\n#&gt;    .x7                0.799    0.081    9.823    0.000    0.799    0.676\n#&gt;    .x8                0.488    0.074    6.573    0.000    0.488    0.477\n#&gt;    .x9                0.566    0.071    8.003    0.000    0.566    0.558\n\n\n32.3.3 Interpretazione dei Risultati\nCarichi Fattoriali (\\(\\lambda\\))\nI carichi fattoriali indicano quanto fortemente un indicatore è associato al costrutto latente. Ad esempio, per visual, il carico fattoriale di x1 è 0.90, indicando una forte relazione tra il costrutto “Visual” e l’indicatore x1.\nIntercette (\\(\\tau\\))\nLe intercette rappresentano la media predetta degli indicatori quando il fattore latente associato è zero. Nel nostro modello, le medie dei costrutti latenti sono fissate a zero, quindi le intercette corrispondono alle medie predette degli indicatori.\n\n32.3.4 Calcolo delle Medie Osservate e Predette\nPer comprendere meglio il ruolo delle intercette, calcoliamo le medie osservate e predette per gli indicatori x1, x2 e x3.\nEstraiamo le intercette stimate dal modello:\n\nparams &lt;- parameterEstimates(fit)\nintercepts &lt;- params$est[params$op == \"~1\"]\nintercepts\n#&gt;  [1] 4.936 6.088 2.250 3.061 4.341 2.186 4.186 5.527 5.374 0.000 0.000 0.000\n\n\n32.3.5 Calcolo della Media Predetta\nLa media predetta di ciascun indicatore in un modello CFA è fornita direttamente dalle intercette stimate (\\(\\tau\\)). Per gli indicatori x1, x2, x3, estraiamo le intercette dal modello:\n\nmean_predicted_scores &lt;- mean(intercepts[1:3])  # Intercette predette per x1, x2, x3\nmean_predicted_scores\n#&gt; [1] 4.425\n\n\n32.3.6 Calcolo della Media Osservata\nLa media osservata per gli stessi indicatori si ottiene calcolando la media aritmetica dei loro punteggi effettivi, come segue:\n\nmean_observed_scores &lt;- mean(\n  (HolzingerSwineford1939$x1 + HolzingerSwineford1939$x2 + HolzingerSwineford1939$x3) / 3\n)\nmean_observed_scores\n#&gt; [1] 4.425\n\nSe il modello si adatta bene ai dati, le due medie dovrebbero essere molto vicine. Questo riflette l’adeguatezza del modello nel rappresentare i dati. In questo caso, i due valori coincidono o differiscono solo leggermente, confermando che il modello rappresenta fedelmente i dati osservati.\nIn sommario, questo esempio illustra come i carichi fattoriali e le intercette nel modello CFA siano utilizzati per stimare le medie predette degli indicatori. L’allineamento tra medie osservate e predette riflette l’adeguatezza del modello nella rappresentazione dei dati.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>La struttura delle medie</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/02_meanstructure.html#medie-di-costrutti-latenti-non-zero",
    "href": "chapters/cfa/02_meanstructure.html#medie-di-costrutti-latenti-non-zero",
    "title": "32  La struttura delle medie",
    "section": "\n32.4 Medie di Costrutti Latenti Non Zero",
    "text": "32.4 Medie di Costrutti Latenti Non Zero\nQuando le medie dei costrutti latenti (\\(\\mu_{\\text{latente}}\\)) non sono fissate a zero, la media predetta di ciascun indicatore dipende sia dalle intercette (\\(\\tau\\)) che dai carichi fattoriali (\\(\\lambda\\)). In questo caso, l’equazione per la media predetta di un indicatore, ad esempio x1, è:\n\\[\n\\text{media predetta}(x1) = \\mu_{\\text{latente}} \\cdot \\lambda_{x1} + \\tau_{x1},\n\\]\ndove:\n\n\n\\(\\mu_{\\text{latente}}\\): è la media stimata del costrutto latente associato.\n\n\\(\\lambda_{x1}\\): è il carico fattoriale dell’indicatore x1, che misura quanto fortemente il costrutto latente influenza l’indicatore.\n\n\\(\\tau_{x1}\\): è l’intercetta stimata dell’indicatore x1.\n\nQuesta relazione evidenzia che, quando \\(\\mu_{\\text{latente}} \\neq 0\\), la media degli indicatori riflette non solo la loro intercetta, ma anche il contributo del costrutto latente, modulato dai carichi fattoriali.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>La struttura delle medie</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/02_meanstructure.html#modello-con-medie-dei-costrutti-latenti-non-zero",
    "href": "chapters/cfa/02_meanstructure.html#modello-con-medie-dei-costrutti-latenti-non-zero",
    "title": "32  La struttura delle medie",
    "section": "\n32.5 Modello con Medie dei Costrutti Latenti Non Zero",
    "text": "32.5 Modello con Medie dei Costrutti Latenti Non Zero\nPer esplorare questa configurazione, costruiamo un modello SEM in cui le medie dei costrutti latenti sono stimate liberamente. Per garantire l’identificabilità del modello, introduciamo due vincoli fondamentali:\n\n\nIntercette degli indicatori marker fissate a zero: Le variabili osservate x1, x4 e x7, che sono gli indicatori marker per ciascun fattore, avranno intercette impostate a zero.\n\nUna media latente fissata a zero: La media del costrutto latente visual è fissata a zero come riferimento, mentre le medie di textual e speed sono stimate liberamente.\n\n\n32.5.1 Specificazione del Modello\nIl modello viene specificatocome segue:\n\nhs_model &lt;- \"\n    # Definizione dei fattori latenti\n    visual =~ NA*x1 + x2 + x3\n    textual =~ NA*x4 + x5 + x6\n    speed =~ NA*x7 + x8 + x9\n\n    # Standardizzazione delle varianze latenti\n    visual ~~ 1*visual\n    textual ~~ 1*textual\n    speed ~~ 1*speed\n\n    # Vincoli sulle intercette degli indicatori marker\n    x1 ~ 0*1  # Intercetta di x1 fissata a zero\n    x4 ~ 0*1  # Intercetta di x4 fissata a zero\n    x7 ~ 0*1  # Intercetta di x7 fissata a zero\n\n    # Vincoli sulle medie latenti\n    visual ~ 0*1  # Media di visual fissata a zero\n    textual ~ 1   # Media di textual stimata liberamente\n    speed ~ 1     # Media di speed stimata liberamente\n\"\n\n\n32.5.2 Interpretazione delle Medie Predette\nLa caratteristica chiave di questo modello è che le medie predette degli indicatori sono una funzione di tre componenti:\n\nLa media del costrutto latente (\\(\\mu_{\\text{latente}}\\))\nIl carico fattoriale dell’indicatore (\\(\\lambda\\))\nL’intercetta dell’indicatore (\\(\\tau\\))\n\nQuesta relazione è espressa dalla formula:\n\\[\n\\text{media predetta(indicatore)} = \\mu_{\\text{latente}} \\cdot \\lambda + \\tau\n\\]\nPer esempio:\n\nPer l’indicatore marker x1 (fattore visual): \\[\n\\text{media predetta}(x1) = \\mu_{\\text{visual}} \\cdot \\lambda_{x1} + \\tau_{x1} = 0 \\cdot \\lambda_{x1} + 0 = 0\n\\]\nPer l’indicatore marker x4 (fattore textual): \\[\n\\text{media predetta}(x4) = \\mu_{\\text{textual}} \\cdot \\lambda_{x4} + \\tau_{x4}\n\\] dove \\(\\mu_{\\text{textual}}\\) è stimato liberamente e \\(\\tau_{x4} = 0\\)\n\nPer gli altri indicatori (es. x2, x3, x5, x6, x8, x9):\n\nLe intercette sono stimate liberamente\nLe medie predette includono sia il contributo della media latente che dell’intercetta\n\n\n\nQuesta struttura delle medie ci permette di:\n\nconfrontare le medie dei costrutti latenti tra gruppi diversi;\nvalutare i cambiamenti longitudinali nei costrutti latenti;\ninterpretare le differenze nelle medie degli indicatori in termini dei loro componenti strutturali.\n\n32.5.3 Adattamento del Modello\nApplichiamo il modello ai dati e esaminiamo i risultati:\n\nfit &lt;- cfa(hs_model, data = HolzingerSwineford1939, meanstructure = TRUE)\nparams &lt;- parameterEstimates(fit)\nparams\n#&gt;        lhs op     rhs    est    se       z pvalue ci.lower ci.upper\n#&gt; 1   visual =~      x1  4.983 0.212  23.538  0.000    4.568    5.398\n#&gt; 2   visual =~      x2  1.702 0.093  18.248  0.000    1.519    1.885\n#&gt; 3   visual =~      x3  2.243 0.106  21.099  0.000    2.035    2.451\n#&gt; 4  textual =~      x4  1.783 0.081  21.945  0.000    1.624    1.942\n#&gt; 5  textual =~      x5  1.985 0.090  22.007  0.000    1.808    2.162\n#&gt; 6  textual =~      x6  1.652 0.076  21.702  0.000    1.502    1.801\n#&gt; 7    speed =~      x7  1.136 0.072  15.882  0.000    0.996    1.277\n#&gt; 8    speed =~      x8  1.341 0.070  19.127  0.000    1.204    1.478\n#&gt; 9    speed =~      x9  1.229 0.068  17.979  0.000    1.095    1.363\n#&gt; 10  visual ~~  visual  1.000 0.000      NA     NA    1.000    1.000\n#&gt; 11 textual ~~ textual  1.000 0.000      NA     NA    1.000    1.000\n#&gt; 12   speed ~~   speed  1.000 0.000      NA     NA    1.000    1.000\n#&gt; 13      x1 ~1          0.000 0.000      NA     NA    0.000    0.000\n#&gt; 14      x4 ~1          0.000 0.000      NA     NA    0.000    0.000\n#&gt; 15      x7 ~1          0.000 0.000      NA     NA    0.000    0.000\n#&gt; 16  visual ~1          0.000 0.000      NA     NA    0.000    0.000\n#&gt; 17 textual ~1          0.885 0.054  16.405  0.000    0.779    0.990\n#&gt; 18   speed ~1          2.845 0.187  15.207  0.000    2.478    3.211\n#&gt; 19      x1 ~~      x1  0.890 0.257   3.457  0.001    0.385    1.394\n#&gt; 20      x2 ~~      x2  1.134 0.100  11.368  0.000    0.938    1.329\n#&gt; 21      x3 ~~      x3  0.844 0.087   9.719  0.000    0.674    1.015\n#&gt; 22      x4 ~~      x4  0.371 0.045   8.239  0.000    0.283    0.459\n#&gt; 23      x5 ~~      x5  0.446 0.055   8.116  0.000    0.338    0.554\n#&gt; 24      x6 ~~      x6  0.356 0.041   8.679  0.000    0.276    0.437\n#&gt; 25      x7 ~~      x7  0.799 0.077  10.411  0.000    0.649    0.950\n#&gt; 26      x8 ~~      x8  0.488 0.062   7.915  0.000    0.367    0.608\n#&gt; 27      x9 ~~      x9  0.566 0.062   9.132  0.000    0.445    0.688\n#&gt; 28  visual ~~ textual  0.870 0.016  53.410  0.000    0.838    0.902\n#&gt; 29  visual ~~   speed  0.877 0.019  47.025  0.000    0.840    0.913\n#&gt; 30 textual ~~   speed  0.783 0.027  28.490  0.000    0.729    0.837\n#&gt; 31      x2 ~1          4.460 0.064  69.660  0.000    4.335    4.586\n#&gt; 32      x3 ~1          0.106 0.058   1.814  0.070   -0.008    0.220\n#&gt; 33      x5 ~1          0.934 0.075  12.485  0.000    0.787    1.080\n#&gt; 34      x6 ~1         -0.649 0.064 -10.085  0.000   -0.775   -0.523\n#&gt; 35      x8 ~1          0.588 0.236   2.496  0.013    0.126    1.050\n#&gt; 36      x9 ~1          0.847 0.226   3.742  0.000    0.403    1.291\n\n\n32.5.4 Interpretazione dei Risultati\n\nMedie Latenti (\\(\\mu_{\\text{latente}}\\))\nLe medie stimate per i costrutti textual e speed riflettono il loro contributo alle medie predette degli indicatori associati. Per visual, la media latente è fissata a zero per identificare il modello.\nMedia Predetta degli Indicatori\nPer ciascun indicatore, la media predetta include sia l’intercetta (fissata a zero in questo esempio) sia il contributo del costrutto latente moltiplicato per il carico fattoriale.\n\nAd esempio, per x1:\n\\[\n\\text{media predetta}(x1) = \\mu_{\\text{visual}} \\cdot \\lambda_{x1} + \\tau_{x1} = 0 \\cdot \\lambda_{x1} + 0 = 0\n\\]\nMentre per un indicatore di textual (ad esempio x4), la media predetta sarà:\n\\[\n\\text{media predetta}(x4) = \\mu_{\\text{textual}} \\cdot \\lambda_{x4} + \\tau_{x4}.\n\\]\n\n32.5.5 Calcolo delle Medie Predette\nPer calcolare le medie predette:\n\n# Funzione per calcolare le medie predette\ncalc_predicted_means &lt;- function(params) {\n  # Estrai i carichi fattoriali (lambda)\n  lambdas &lt;- params[params$op == \"=~\", c(\"lhs\", \"rhs\", \"est\")]\n  \n  # Estrai le medie latenti (mu)\n  means &lt;- params[params$op == \"~1\" & params$lhs %in% c(\"visual\", \"textual\", \"speed\"), \n                 c(\"lhs\", \"est\")]\n  \n  # Estrai le intercette (tau)\n  intercepts &lt;- params[params$op == \"~1\" & params$lhs %in% paste0(\"x\", 1:9), \n                      c(\"lhs\", \"est\")]\n  \n  # Calcola le medie predette\n  predicted_means &lt;- data.frame(\n    indicator = character(),\n    predicted_mean = numeric(),\n    formula = character(),\n    stringsAsFactors = FALSE\n  )\n  \n  # Per ogni indicatore\n  for(i in 1:9) {\n    indicator &lt;- paste0(\"x\", i)\n    # Trova il fattore latente associato\n    factor &lt;- lambdas$lhs[lambdas$rhs == indicator]\n    lambda &lt;- lambdas$est[lambdas$rhs == indicator]\n    mu &lt;- means$est[means$lhs == factor]\n    tau &lt;- intercepts$est[intercepts$lhs == indicator]\n    \n    # Se l'intercetta non è presente nei risultati, assume 0\n    if(length(tau) == 0) tau &lt;- 0\n    \n    # Calcola la media predetta\n    pred_mean &lt;- mu * lambda + tau\n    \n    # Crea la formula come stringa\n    formula &lt;- sprintf(\"%.3f * %.3f + %.3f = %.3f\", \n                      mu, lambda, tau, pred_mean)\n    \n    # Aggiungi alla tabella dei risultati\n    predicted_means &lt;- rbind(predicted_means,\n                           data.frame(\n                             indicator = indicator,\n                             predicted_mean = pred_mean,\n                             formula = formula,\n                             stringsAsFactors = FALSE\n                           ))\n  }\n  \n  return(predicted_means)\n}\n\n\n# Calcola e mostra le medie predette\nresults &lt;- calc_predicted_means(params)\nprint(results)\n#&gt;   indicator predicted_mean                        formula\n#&gt; 1        x1         0.0000  0.000 * 4.983 + 0.000 = 0.000\n#&gt; 2        x2         4.4603  0.000 * 1.702 + 4.460 = 4.460\n#&gt; 3        x3         0.1055  0.000 * 2.243 + 0.106 = 0.106\n#&gt; 4        x4         1.5775  0.885 * 1.783 + 0.000 = 1.578\n#&gt; 5        x5         2.6894  0.885 * 1.985 + 0.934 = 2.689\n#&gt; 6        x6         0.8118 0.885 * 1.652 + -0.649 = 0.812\n#&gt; 7        x7         3.2331  2.845 * 1.136 + 0.000 = 3.233\n#&gt; 8        x8         4.4028  2.845 * 1.341 + 0.588 = 4.403\n#&gt; 9        x9         4.3436  2.845 * 1.229 + 0.847 = 4.344\n\nLe medie latenti riflettono il contributo di ciascun costrutto alla media predetta dei rispettivi indicatori. La formula \\(\\hat{Y} = \\mu \\cdot \\lambda + \\tau\\) ci consente di distinguere i diversi contributi strutturali alle medie osservate.\nIn sintesi, quando le medie dei costrutti latenti sono diverse da zero, il calcolo delle medie predette degli indicatori diventa più complesso, poiché include il contributo sia delle intercette sia dei carichi fattoriali ponderati dalle medie latenti. Questo approccio è particolarmente utile in contesti in cui è necessario confrontare medie tra gruppi o valutare cambiamenti longitudinali nei costrutti latenti.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>La struttura delle medie</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/02_meanstructure.html#riflessioni-conclusive",
    "href": "chapters/cfa/02_meanstructure.html#riflessioni-conclusive",
    "title": "32  La struttura delle medie",
    "section": "\n32.6 Riflessioni Conclusive",
    "text": "32.6 Riflessioni Conclusive\nIn questo capitolo abbiamo approfondito come l’inclusione delle medie nei modelli CFA consenta di ampliare la comprensione delle relazioni tra variabili, andando oltre le covarianze. Abbiamo evidenziato il ruolo cruciale delle intercette, che rappresentano il valore medio atteso degli indicatori quando i fattori latenti assumono valore zero, e come esse contribuiscano alla struttura delle medie predette degli indicatori. Infine, abbiamo esplorato l’utilità dei modelli con medie dei costrutti latenti non zero, che permettono di analizzare differenze tra gruppi e cambiamenti longitudinali, fornendo un quadro interpretativo più ricco e completo. Questo approccio è particolarmente rilevante per indagini che mirano a comprendere i livelli medi dei costrutti in relazione ai loro indicatori, sia in contesti trasversali che longitudinali.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>La struttura delle medie</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/02_meanstructure.html#session-info",
    "href": "chapters/cfa/02_meanstructure.html#session-info",
    "title": "32  La struttura delle medie",
    "section": "\n32.7 Session Info",
    "text": "32.7 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.2\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] ggokabeito_0.1.0  see_0.11.0        MASS_7.3-65       viridis_0.6.5    \n#&gt;  [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n#&gt;  [9] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n#&gt; [13] lavaan_0.6-19     psych_2.4.12      scales_1.3.0      markdown_1.13    \n#&gt; [17] knitr_1.50        lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n#&gt; [21] dplyr_1.1.4       purrr_1.0.4       readr_2.1.5       tidyr_1.3.1      \n#&gt; [25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.1      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.2.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] mime_0.13           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [25] Matrix_1.7-3        R6_2.6.1            fastmap_1.2.0      \n#&gt;  [28] rbibutils_2.3       shiny_1.10.0        digest_0.6.37      \n#&gt;  [31] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n#&gt;  [34] rprojroot_2.0.4     Hmisc_5.2-3         timechange_0.3.0   \n#&gt;  [37] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n#&gt;  [40] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n#&gt;  [43] carData_3.0-5       ggsignif_0.6.4      corpcor_1.6.10     \n#&gt;  [46] gtools_3.9.5        tools_4.4.2         pbivnorm_0.6.0     \n#&gt;  [49] foreign_0.8-88      zip_2.3.2           httpuv_1.6.15      \n#&gt;  [52] nnet_7.3-20         glue_1.8.0          quadprog_1.5-8     \n#&gt;  [55] nlme_3.1-167        promises_1.3.2      lisrelToR_0.3      \n#&gt;  [58] grid_4.4.2          checkmate_2.3.2     cluster_2.1.8.1    \n#&gt;  [61] reshape2_1.4.4      generics_0.1.3      gtable_0.3.6       \n#&gt;  [64] tzdb_0.5.0          data.table_1.17.0   hms_1.1.3          \n#&gt;  [67] car_3.1-3           sem_3.1-16          pillar_1.10.1      \n#&gt;  [70] rockchalk_1.8.157   later_1.4.1         splines_4.4.2      \n#&gt;  [73] lattice_0.22-6      survival_3.8-3      kutils_1.73        \n#&gt;  [76] tidyselect_1.2.1    miniUI_0.1.1.1      pbapply_1.7-2      \n#&gt;  [79] reformulas_0.4.0    stats4_4.4.2        xfun_0.51          \n#&gt;  [82] qgraph_1.9.8        arm_1.14-4          stringi_1.8.4      \n#&gt;  [85] yaml_2.3.10         pacman_0.5.1        boot_1.3-31        \n#&gt;  [88] evaluate_1.0.3      codetools_0.2-20    mi_1.1             \n#&gt;  [91] cli_3.6.4           RcppParallel_5.1.10 rpart_4.1.24       \n#&gt;  [94] xtable_1.8-4        Rdpack_2.6.3        munsell_0.5.1      \n#&gt;  [97] Rcpp_1.0.14         coda_0.19-4.1       png_0.1-8          \n#&gt; [100] XML_3.99-0.18       parallel_4.4.2      jpeg_0.1-10        \n#&gt; [103] lme4_1.1-36         mvtnorm_1.3-3       openxlsx_4.2.8     \n#&gt; [106] rlang_1.1.5         multcomp_1.4-28     mnormt_2.1.1\n\n\n\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>La struttura delle medie</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/03_cat_data.html",
    "href": "chapters/cfa/03_cat_data.html",
    "title": "41  Dati non gaussiani e categoriali",
    "section": "",
    "text": "41.1 Introduzione\nIn questa dispensa abbiamo già discusso l’uso dello stimatore di massima verosimiglianza (ML), largamente utilizzato nell’Analisi Fattoriale Confermativa (CFA) e nei modelli di Structural Equation Modeling (SEM). Questo metodo funziona bene quando i dati sono normalmente distribuiti, ovvero quando la distribuzione delle variabili è simmetrica e segue la classica “curva a campana”.\nTuttavia, se i dati si discostano molto dalla normalità, per esempio con una forte asimmetria o curtosi (distribuzioni molto appuntite o schiacciate), oppure se le variabili non sono su una scala numerica continua (ad esempio, dati binari o ordinali), lo stimatore ML potrebbe non essere adeguato. In questi casi, è meglio usare stimatori alternativi.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Dati non gaussiani e categoriali</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/03_cat_data.html#dati-non-normali-e-alternative-a-ml",
    "href": "chapters/cfa/03_cat_data.html#dati-non-normali-e-alternative-a-ml",
    "title": "41  Dati non gaussiani e categoriali",
    "section": "\n41.2 Dati non normali e alternative a ML",
    "text": "41.2 Dati non normali e alternative a ML\nAnche se ML è robusto a piccole deviazioni dalla normalità, in situazioni di marcata non normalità possono verificarsi i seguenti problemi:\n\n\nSovrastima della statistica chi-quadrato (\\(\\chi^2\\)): i test per valutare l’adattamento del modello ai dati possono risultare troppo severi.\n\nSottostima degli indici di bontà di adattamento: valori come il Tucker-Lewis Index (TLI) e il Comparative Fit Index (CFI) possono sembrare peggiori del reale.\n\nErrori standard sottostimati: le stime dei parametri del modello possono sembrare più precise di quanto non siano.\n\nQuesti problemi diventano più gravi con campioni piccoli. Per evitarli, si possono utilizzare i seguenti stimatori:\n\n41.2.1 1. GLS (Generalized Least Squares)\n\n\n\nQuando usarlo: Adatto se non ci sono dati mancanti.\n\nCome funziona: Valuta quanto la matrice di covarianza stimata dal modello differisce dalla matrice osservata. Più piccola è questa differenza, meglio il modello si adatta ai dati.\n\nInterpretazione: Un valore basso della funzione GLS indica un buon adattamento.\n\n41.2.2 2. WLS (Weighted Least Squares)\n\n\n\nQuando usarlo: Ideale per dati non normali o complessi (detto anche stimatore ADF - Asintoticamente Libero da Distribuzione).\n\nCome funziona: Pesa ogni elemento della matrice di covarianza in modo da considerare l’importanza relativa delle discrepanze.\n\nInterpretazione: Un valore basso indica che il modello si adatta bene, tenendo conto di questi pesi.\n\n41.2.3 3. DWLS (Diagonally Weighted Least Squares)\n\n\n\nQuando usarlo: Una versione semplificata di WLS.\n\nCome funziona: Utilizza pesi solo sugli elementi della diagonale della matrice di covarianza, semplificando i calcoli.\n\nInterpretazione: È più semplice di WLS, ma funziona bene per dati ordinali o binari.\n\n41.2.4 4. ULS (Unweighted Least Squares)\n\n\n\nQuando usarlo: Adatto a situazioni meno complesse.\n\nCome funziona: Tutti gli elementi della matrice hanno lo stesso peso.\n\nInterpretazione: È il metodo più semplice, ma meno sofisticato rispetto agli altri.\n\n41.2.5 ML Robusto: un compromesso per dati non normali\nOltre agli stimatori sopra descritti, esiste una variante di ML chiamata ML Robusto (Robust Maximum Likelihood). Questo metodo è pensato per gestire situazioni di forte non normalità:\n\n\nCorregge la statistica \\(\\chi^2\\): Evita che i risultati sembrino peggiori di quanto siano.\n\nErrori standard più precisi: Migliora la stima della precisione dei parametri.\n\nIndici di adattamento più affidabili: TLI e CFI risultano più accurati.\n\n\nIn sintesi, quando i dati non rispettano le condizioni di normalità, l’uso di stimatori come WLS, DWLS o ML Robusto può garantire risultati più affidabili. Questi metodi considerano le caratteristiche specifiche dei dati, come la distribuzione o la scala, e permettono di valutare meglio l’adattamento del modello. La scelta dello stimatore dipende dal tipo di dati e dal livello di complessità richiesto dall’analisi.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Dati non gaussiani e categoriali</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/03_cat_data.html#un-esempio-concreto",
    "href": "chapters/cfa/03_cat_data.html#un-esempio-concreto",
    "title": "41  Dati non gaussiani e categoriali",
    "section": "\n41.3 Un Esempio Concreto",
    "text": "41.3 Un Esempio Concreto\nPrendiamo in considerazione un caso pratico. Utilizzeremo i dati sintetici presentati da Brown (2015) nelle tabelle 9.5-9.7 come esempio.\n\nd &lt;- readRDS(here::here(\"data\", \"brown_table_9_5_data.RDS\"))\nhead(d)\n#&gt;   x1 x2 x3 x4 x5\n#&gt; 1  0  0  0  0  0\n#&gt; 2  0  0  0  0  0\n#&gt; 3  0  0  0  0  0\n#&gt; 4  4  2  2  1  1\n#&gt; 5  1  0  1  6  0\n#&gt; 6  0  0  0  0  0\n\nLe statistiche descrittive di questo campione di dati mostrano valori eccessivi di asimmetria e di curtosi.\n\npsych::describe(d)\n#&gt;    vars   n mean   sd median trimmed mad min max range skew kurtosis   se\n#&gt; x1    1 870 1.47 2.17      0    1.01   0   0   8     8 1.51     1.25 0.07\n#&gt; x2    2 870 0.82 1.60      0    0.42   0   0   8     8 2.40     5.67 0.05\n#&gt; x3    3 870 1.27 2.07      0    0.78   0   0   8     8 1.80     2.34 0.07\n#&gt; x4    4 870 1.03 1.93      0    0.54   0   0   8     8 2.16     3.98 0.07\n#&gt; x5    5 870 0.61 1.52      0    0.18   0   0   8     8 3.10     9.37 0.05\n\nDefiniamo un modello ad un fattore e, seguendo Brown (2015), aggiungiamo una correlazione residua tra gli indicatori X1 e X3:\n\nmodel &lt;- '\n  f1 =~ x1 + x2 + x3 + x4 + x5\n  x1 ~~ x3 \n'\n\nProcediamo alla stima dei parametri utilizzando uno stimatore di ML robusto. La sintassi lavaan è la seguente:\n\nfit &lt;- cfa(\n  model, \n  data = d, \n  mimic = \"MPLUS\", \n  estimator = \"MLM\"\n)\n\nPer esaminare la soluzione ottenuta ci focalizziamo sulla statistica \\(\\chi^2\\) – si consideri la soluzione robusta fornita nell’output.\n\nsummary(fit)\n#&gt; lavaan 0.6-19 ended normally after 28 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        16\n#&gt; \n#&gt;   Number of observations                           870\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                Standard      Scaled\n#&gt;   Test Statistic                                 25.913      10.356\n#&gt;   Degrees of freedom                                  4           4\n#&gt;   P-value (Chi-square)                            0.000       0.035\n#&gt;   Scaling correction factor                                   2.502\n#&gt;     Satorra-Bentler correction (Mplus variant)                     \n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                           Robust.sem\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;   f1 =~                                               \n#&gt;     x1                1.000                           \n#&gt;     x2                0.703    0.062   11.338    0.000\n#&gt;     x3                1.068    0.044   24.304    0.000\n#&gt;     x4                0.918    0.063   14.638    0.000\n#&gt;     x5                0.748    0.055   13.582    0.000\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;  .x1 ~~                                               \n#&gt;    .x3                0.655    0.143    4.579    0.000\n#&gt; \n#&gt; Intercepts:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;    .x1                1.470    0.074   19.968    0.000\n#&gt;    .x2                0.823    0.054   15.166    0.000\n#&gt;    .x3                1.266    0.070   18.043    0.000\n#&gt;    .x4                1.026    0.065   15.712    0.000\n#&gt;    .x5                0.607    0.051   11.790    0.000\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;    .x1                2.040    0.228    8.952    0.000\n#&gt;    .x2                1.241    0.124   10.019    0.000\n#&gt;    .x3                1.227    0.169    7.255    0.000\n#&gt;    .x4                1.458    0.177    8.233    0.000\n#&gt;    .x5                0.807    0.100    8.063    0.000\n#&gt;     f1                2.675    0.289    9.273    0.000\n\nPer fare un confronto, adattiamo lo stesso modello ai dati usando lo stimatore di ML.\n\nfit2 &lt;- cfa(model, data = d)\n\nNotiamo come il valore della statistica \\(\\chi^2\\) ora ottenuto sia molto maggiore di quello trovato in precedenza.\n\nsummary(fit2)\n#&gt; lavaan 0.6-19 ended normally after 28 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        11\n#&gt; \n#&gt;   Number of observations                           870\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                25.913\n#&gt;   Degrees of freedom                                 4\n#&gt;   P-value (Chi-square)                           0.000\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;   f1 =~                                               \n#&gt;     x1                1.000                           \n#&gt;     x2                0.703    0.035   20.133    0.000\n#&gt;     x3                1.068    0.034   31.730    0.000\n#&gt;     x4                0.918    0.042   21.775    0.000\n#&gt;     x5                0.748    0.033   22.416    0.000\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;  .x1 ~~                                               \n#&gt;    .x3                0.655    0.091    7.213    0.000\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;    .x1                2.040    0.128   15.897    0.000\n#&gt;    .x2                1.241    0.070   17.671    0.000\n#&gt;    .x3                1.227    0.095   12.942    0.000\n#&gt;    .x4                1.458    0.090   16.135    0.000\n#&gt;    .x5                0.807    0.053   15.119    0.000\n#&gt;     f1                2.675    0.220   12.154    0.000\n\n\n41.3.1 Dati Categoriali\nNella discussione precedente, abbiamo esaminato il modello CFA presupponendo che i dati fossero continui e normalmente distribuiti in maniera multivariata. Tuttavia, abbiamo anche trattato la stima robusta per dati non normalmente distribuiti. Ora, è fondamentale riconoscere che molti dei dati utilizzati nelle analisi fattoriali confermative (CFA) o SEM provengono da questionari e scale di tipo Likert, che producono dati categoriali, inclusi formati binari, ordinali e nominali. Questi dati sono di natura ordinale e non sono continui.\nL’uso del metodo di massima verosimiglianza (ML) ordinario non è raccomandato quando si analizzano dati con almeno un indicatore categoriale. Trattare tali variabili come se fossero continue può portare a varie conseguenze indesiderate, tra cui:\n\n\nStime Attenuate delle Relazioni: Le relazioni tra gli indicatori possono risultare attenuate, specialmente se influenzate da effetti di pavimento o soffitto.\n\nEmergenza di “Pseudo-Fattori”: La possibilità di identificare falsi fattori, che non rappresentano veri costrutti ma sono piuttosto artefatti del metodo statistico utilizzato.\n\nDistorsione degli Indici di Bontà di Adattamento e delle Stime degli Errori Standard: Questi indici, che valutano la qualità dell’adattamento del modello, possono essere distorti, così come le stime degli errori standard.\n\nStime Errate dei Parametri: I parametri del modello potrebbero essere stimati in modo inaccurato.\n\nPer mitigare questi problemi, esistono stimatori specifici per i dati categoriali, tra cui:\n\n\nWLS (Weighted Least Squares): Adatto per dati categoriali, considera il peso specifico di ciascuna osservazione.\n\nWLSMV (Weighted Least Squares Mean and Variance Adjusted): Una versione modificata di WLS che si adatta meglio alle peculiarità dei dati categoriali.\n\nULS (Unweighted Least Squares): Questo stimatore non prevede ponderazioni e può essere utile per dati categoriali senza presupporre pesi specifici.\n\nNelle sezioni seguenti, approfondiremo l’approccio CFA per dati categoriali, evidenziando le specificità e le migliori pratiche per gestire questo tipo di dati nelle analisi CFA. Questo ci permetterà di effettuare inferenze più accurate, preservando l’integrità e la validità delle conclusioni derivanti dalle analisi.\n\n41.3.2 Un esempio concreto\nNell’esempio discusso da Brown (2015), i ricercatori desiderano verificare un modello uni-fattoriale di dipendenza da alcol in un campione di 750 pazienti ambulatoriali. Gli indicatori di alcolismo sono item binari che riflettono la presenza/assenza di sei criteri diagnostici per l’alcolismo (0 = criterio non soddisfatto, 1 = criterio soddisfatto). I dati sono i seguenti:\n\nd1 &lt;- readRDS(here::here(\"data\", \"brown_table_9_9_data.RDS\"))\nhead(d1)\n#&gt;   y1 y2 y3 y4 y5 y6\n#&gt; 1  1  1  1  1  1  1\n#&gt; 2  1  1  1  1  1  1\n#&gt; 3  1  1  1  1  1  0\n#&gt; 4  1  1  1  1  1  1\n#&gt; 5  0  0  0  0  0  0\n#&gt; 6  1  1  0  1  1  1\n\nÈ possibile evidenziare la natura ordinale dei dati esaminando le tabelle bivariate che mostrano la frequenza di combinazioni specifiche tra due variabili.\n\nxtabs(~ y1 + y2, d1)\n#&gt;    y2\n#&gt; y1    0   1\n#&gt;   0 103  65\n#&gt;   1 156 426\n\n\nxtabs(~ y3 + y4, d1)\n#&gt;    y4\n#&gt; y3    0   1\n#&gt;   0  41  39\n#&gt;   1 119 551\n\n\nxtabs(~ y5 + y6, d1)\n#&gt;    y6\n#&gt; y5    0   1\n#&gt;   0  95 168\n#&gt;   1  60 427\n\nNelle tabelle precedenti, si osserva una maggiore frequenza di casi in cui entrambe le variabili assumono il valore 1, rispetto ai casi in cui entrambe sono 0 o in cui una è 1 e l’altra è 0. Questo suggerisce l’esistenza di una relazione ordinale tra le coppie di variabili nel dataset.\n\n41.3.3 Il Modello Basato sulle Soglie per Risposte Categoriali Ordinate\nIl modello basato sulle soglie per risposte categoriali ordinate si basa sull’idea che ogni risposta di una variabile categoriale possa essere vista come il risultato di una variabile continua non osservata, che è normalmente distribuita. Questa variabile nascosta, chiamata variabile latente, rappresenta la tendenza di una persona a rispondere in un determinato modo. Le risposte che vediamo, classificate in categorie, sono in realtà approssimazioni di questa variabile latente.\nImmaginiamo di utilizzare un questionario dove le risposte sono su una scala Likert a 7 punti. Questo crea una variabile categoriale con sette categorie ordinate. Se denotiamo con I un particolare item del questionario e con I* la sua corrispondente variabile latente non osservabile, possiamo descrivere il loro legame attraverso le seguenti equazioni, che mappano la variabile latente alle risposte osservabili:\n\\[\n\\begin{align*}\nI &= 1 \\quad \\text{se} \\quad -\\infty &lt; I^* \\leq t_1 \\\\\nI &= 2 \\quad \\text{se} \\quad t_1 &lt; I^* \\leq t_2 \\\\\nI &= 3 \\quad \\text{se} \\quad t_2 &lt; I^* \\leq t_3 \\\\\nI &= 4 \\quad \\text{se} \\quad t_3 &lt; I^* \\leq t_4 \\\\\nI &= 5 \\quad \\text{se} \\quad t_4 &lt; I^* \\leq t_5 \\\\\nI &= 6 \\quad \\text{se} \\quad t_5 &lt; I^* \\leq t_6 \\\\\nI &= 7 \\quad \\text{se} \\quad t_6 &lt; I^* &lt; \\infty\n\\end{align*}\n\\]\nIn queste equazioni, \\(t_i\\) (con i da 1 a 6) rappresenta le soglie che dividono l’intero spettro della variabile latente in sette categorie. Le soglie sono disposte in modo che \\(-\\infty &lt; t_1 &lt; t_2 &lt; t_3 &lt; t_4 &lt; t_5 &lt; t_6 &lt; \\infty\\). È importante notare che il numero di soglie è sempre uno in meno rispetto al numero di categorie, un po’ come il numero di variabili dummy usate nell’analisi di regressione per codificare una variabile categoriale.\nQuesto processo di categorizzazione può essere visualizzato come segue: si immagini una curva normale che rappresenta la distribuzione della variabile latente \\(I*\\). Le sei linee verticali nella figura rappresentano le soglie \\(t_1\\) a \\(t_6\\). Le risposte possibili vanno da I = 1 a I = 7, e la categoria specifica (I) dipende dall’intervallo, definito dalle soglie, in cui il valore di I* si trova.\n\n# Definire le soglie\nthresholds &lt;- c(-3, -2, -1, 0, 1, 2, 3)\n\n# Creare un dataframe per la curva normale\nx_values &lt;- seq(-4, 4, length.out = 300)\ny_values &lt;- dnorm(x_values)\ncurve_data &lt;- data.frame(x = x_values, y = y_values)\n\n# Creare il plot\nggplot(curve_data, aes(x = x, y = y)) +\n    geom_line() +\n    geom_vline(xintercept = thresholds, col = \"red\") +\n    scale_y_continuous(breaks = NULL) +\n    scale_x_continuous(\n      breaks = thresholds, labels = c(\"t1\", \"t2\", \"t3\", \"t4\", \"t5\", \"t6\", \"t7\")\n    ) +\n    labs(\n        title = \"Categorization of Latent Continuous Variable to Categorical Variable\",\n        x = \"Latent Continuous Variable I*\",\n        y = \"\"\n    ) \n\n\n\n\n\n\n\nLa conversione della variabile latente \\(I^*\\) in dati su una scala Likert comporta inevitabilmente degli errori di misurazione e campionamento. Come evidenziato da O’Brien (1985), questo processo di categorizzazione introduce due tipi principali di errore:\n\nErrore di categorizzazione: Questo errore deriva dalla segmentazione di una scala continua in una scala categoriale, dove la variabile latente viene divisa in categorie distinte.\nErrore di trasformazione: Questo errore emerge quando le categorie hanno larghezze disuguali, influenzando la fedeltà della rappresentazione delle misure originali della variabile latente.\n\nDi conseguenza, è fondamentale che le soglie siano stimate contemporaneamente agli altri parametri nel modello di equazioni strutturali per garantire che tali errori siano minimizzati e che l’analisi rifletta accuratamente la realtà sottostante.\n\n41.3.4 Modellazione di Variabili Categoriali nei Modelli CFA\nNell’ambito dei modelli CFA, le variabili categoriali ordinate vengono spesso modellate collegandole a una variabile latente sottostante, denominata \\(I^*\\). Questa variabile latente rappresenta una sorta di “propensione nascosta” che influisce sulle risposte osservate nelle variabili categoriali.\nPer esemplificare, consideriamo il seguente modello che esprime la variabile latente \\(I^*\\) attraverso una serie di predittori (x1, x2, …, xp), ognuno dei quali contribuisce all’esito con un effetto quantificato dai coefficienti \\(\\beta_1, \\beta_2, ..., \\beta_P\\):\n\\[\nI^*_i = \\beta_0 + \\beta_1 x_{1i} + \\dots + \\beta_P x_{Pi} + e_i.\n\\]\nIn questa equazione:\n\n\n\\(I^*_i\\) indica la propensione latente per l’osservatore $ i $.\n\n\\(\\beta_0\\) è un termine costante che agisce come intercetta.\n\n\\(\\beta_1, \\dots, \\beta_P\\) sono i coefficienti che misurano l’impatto di ciascun predittore sulla propensione latente.\n$ e_i$ è il termine di errore che rappresenta le variazioni non spiegate dai predittori.\n\nQuando la variabile categoriale $ I $ funge da indicatore di un fattore latente $ $ in un modello fattoriale confermativo, la formulazione dell’equazione si semplifica a:\n\\[\nI^*_i = \\beta_0 + \\beta_1 \\xi_i + e_i.\n\\]\nIn questa configurazione, \\(\\beta_1\\) rappresenta il carico fattoriale, indicando quanto fortemente il fattore latente \\(\\xi\\) influisce sulla variabile latente \\(I^*\\). Questo schema è analogo a quello usato per modellare indicatori di misurazione continui nei modelli SEM.\nQuesto approccio riflette l’idea che le risposte categoriali osservabili possono essere considerate come manifestazioni esterne di una propensione interna latente. Per la stima di tali modelli, il metodo dei minimi quadrati ponderati (WLS) è generalmente appropriato. Tuttavia, è importante tenere presente che la modellazione di risposte categoriali ordinate può richiedere considerazioni aggiuntive per gestire adeguatamente la loro natura ordinale, dettagli che verranno approfonditi nelle sezioni seguenti.\n\n41.3.5 Adattamento del Modello con lmer\n\nSpecifichiamo il modello nel modo seguente:\n\nmodel3 &lt;- '\n  etoh =~ y1 + y2 + y3 + y4 + y5 + y6\n'\n\nNell’analizzare dati ottenuti da scale ordinali, il software lavaan impiega un metodo specializzato per gestire la natura particolare dei dati categoriali. Questo approccio utilizza lo stimatore WLSMV (Weighted Least Squares Mean and Variance Adjusted). La stima dei parametri avviene tramite il metodo dei minimi quadrati ponderati diagonalmente (DWLS), che si concentra sulle componenti diagonali della matrice di peso. Questa specificità rende lo stimatore WLSMV particolarmente adatto per analizzare dati non normali.\nUna caratteristica importante dello stimatore WLSMV è la sua capacità di calcolare errori standard robusti. Questi sono determinati attraverso un metodo che mantiene l’affidabilità delle stime anche quando i dati non soddisfano le tradizionali assunzioni di normalità. Inoltre, le statistiche di test prodotte da WLSMV sono adeguatamente corrette per tenere conto delle variazioni nella media e nella varianza dei dati. Questo tipo di correzione è cruciale per garantire l’accuratezza e la validità delle statistiche di test, specialmente quando la distribuzione dei dati devia dalla normalità.\nIn conclusione, lavaan offre un approccio avanzato per la modellazione di dati categoriali utilizzando lo stimatore WLSMV, che è ottimizzato per rispondere alle esigenze specifiche di questi tipi di dati. Questo si traduce in stime più precise e statistiche di test affidabili, rendendo lavaan uno strumento molto appropriato per l’analisi di dati categoriali complessi.\n\nfit3 &lt;- cfa(\n  model3, \n  data = d1, \n  ordered = names(d1), \n  estimator = \"WLSMVS\", \n  mimic = \"mplus\"\n)\n\nEsaminiamo la soluzione ottenuta:\n\nsummary(fit3, fit.measures = TRUE)\n#&gt; lavaan 0.6-19 ended normally after 16 iterations\n#&gt; \n#&gt;   Estimator                                       DWLS\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        12\n#&gt; \n#&gt;   Number of observations                           750\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                   Standard      Scaled\n#&gt;   Test Statistic                                     5.651       9.540\n#&gt;   Degrees of freedom                                     9           9\n#&gt;   P-value (Chi-square)                               0.774       0.389\n#&gt;   Scaling correction factor                                      0.592\n#&gt;     mean and variance adjusted correction (WLSMV)                     \n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                              1155.845     694.433\n#&gt;   Degrees of freedom                                15           9\n#&gt;   P-value                                        0.000       0.000\n#&gt;   Scaling correction factor                                  1.664\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    1.000       0.999\n#&gt;   Tucker-Lewis Index (TLI)                       1.005       0.999\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.000       0.009\n#&gt;   90 Percent confidence interval - lower         0.000       0.000\n#&gt;   90 Percent confidence interval - upper         0.028       0.051\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.999       0.944\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.000       0.000\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.031       0.031\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Parameterization                               Delta\n#&gt;   Standard errors                           Robust.sem\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model        Unstructured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;   etoh =~                                             \n#&gt;     y1                1.000                           \n#&gt;     y2                0.822    0.072   11.392    0.000\n#&gt;     y3                0.653    0.092    7.097    0.000\n#&gt;     y4                1.031    0.075   13.703    0.000\n#&gt;     y5                1.002    0.072   13.861    0.000\n#&gt;     y6                0.759    0.076   10.011    0.000\n#&gt; \n#&gt; Thresholds:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;     y1|t1            -0.759    0.051  -14.890    0.000\n#&gt;     y2|t1            -0.398    0.047   -8.437    0.000\n#&gt;     y3|t1            -1.244    0.061  -20.278    0.000\n#&gt;     y4|t1            -0.795    0.051  -15.436    0.000\n#&gt;     y5|t1            -0.384    0.047   -8.148    0.000\n#&gt;     y6|t1            -0.818    0.052  -15.775    0.000\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;    .y1                0.399                           \n#&gt;    .y2                0.594                           \n#&gt;    .y3                0.744                           \n#&gt;    .y4                0.361                           \n#&gt;    .y5                0.397                           \n#&gt;    .y6                0.653                           \n#&gt;     etoh              0.601    0.063    9.596    0.000\n\nSi presti particolare attenzione alla seguente porzione dell’output:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    y1|t1            -0.759    0.051  -14.890    0.000\n    y2|t1            -0.398    0.047   -8.437    0.000\n    y3|t1            -1.244    0.061  -20.278    0.000\n    y4|t1            -0.795    0.051  -15.436    0.000\n    y5|t1            -0.384    0.047   -8.148    0.000\n    y6|t1            -0.818    0.052  -15.775    0.000\nIn questa porzione dell’output di lavaan sono presentati i risultati per le “soglie” (thresholds) relative alle variabili categoriali ordinate utilizzate nel modello SEM. Ecco una spiegazione dettagliata:\n\n\nThresholds (Soglie):\n\nOgni soglia rappresenta un punto di cutoff lungo la variabile continua latente (indicata in precedenza come I*), che determina le categorie della variabile categoriale osservata.\nNell’output, y1|t1, y2|t1, ecc., rappresentano soglie per le variabili rispettive (y1, y2, …, y6). Il termine “t1” si riferisce alla prima soglia per ciascuna di queste variabili.\n\n\n\nEstimate (Stima):\n\nQuesti valori indicano la posizione della soglia sulla scala della variabile continua latente. Per esempio, la soglia per y1 è a -0.759. Questo significa che la divisione tra le prime due categorie di y1 si verifica a -0.759 sulla scala della variabile latente.\n\n\n\nStd.Err (Errore Standard):\n\nL’errore standard della stima di ogni soglia. Ad esempio, per y1, l’errore standard è 0.051. Questo offre un’idea della variabilità o incertezza nella stima della soglia.\n\n\n\nz-value:\n\nIl valore z indica il rapporto tra la stima della soglia e il suo errore standard. Un valore z elevato suggerisce che la stima della soglia è significativamente diversa da zero (ovvero, la soglia è ben definita). Per esempio, per y1, il valore z è -14.890, che è statisticamente significativo.\n\n\n\nP(&gt;|z|):\n\nIl p-value associato al valore z. Un p-value basso (ad esempio, 0.000) indica che la stima della soglia è statisticamente significativa. Questo significa che possiamo essere abbastanza sicuri che la posizione della soglia sulla variabile latente sia accurata e non dovuta al caso.\n\n\n\nIn sintesi, queste soglie consentono di trasformare la variabile latente continua in una variabile categoriale osservata nel modello. La stima di queste soglie e la loro significatività statistica sono cruciali per comprendere come la variabile latente si traduce nelle categorie osservate.\nConfrontiamo ora la soluzione ottenuta con lo stimatore WLSMVS con quella ottenuta mediante lo stimatore ML.\n\nfit4 &lt;- cfa(\n  model3, \n  data = d1\n)\n\n\nsummary(fit4, fit.measures = TRUE)\n#&gt; lavaan 0.6-19 ended normally after 35 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        12\n#&gt; \n#&gt;   Number of observations                           750\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                14.182\n#&gt;   Degrees of freedom                                 9\n#&gt;   P-value (Chi-square)                           0.116\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               614.305\n#&gt;   Degrees of freedom                                15\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.991\n#&gt;   Tucker-Lewis Index (TLI)                       0.986\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -2087.600\n#&gt;   Loglikelihood unrestricted model (H1)      -2080.508\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                4199.199\n#&gt;   Bayesian (BIC)                              4254.640\n#&gt;   Sample-size adjusted Bayesian (SABIC)       4216.535\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.028\n#&gt;   90 Percent confidence interval - lower         0.000\n#&gt;   90 Percent confidence interval - upper         0.054\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.914\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.000\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.021\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;   etoh =~                                             \n#&gt;     y1                1.000                           \n#&gt;     y2                0.934    0.093   10.057    0.000\n#&gt;     y3                0.390    0.055    7.038    0.000\n#&gt;     y4                1.008    0.087   11.541    0.000\n#&gt;     y5                1.158    0.101   11.468    0.000\n#&gt;     y6                0.700    0.077    9.142    0.000\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;    .y1                0.109    0.007   14.692    0.000\n#&gt;    .y2                0.169    0.010   16.781    0.000\n#&gt;    .y3                0.085    0.005   18.483    0.000\n#&gt;    .y4                0.102    0.007   14.285    0.000\n#&gt;    .y5                0.140    0.010   14.506    0.000\n#&gt;    .y6                0.132    0.008   17.514    0.000\n#&gt;     etoh              0.065    0.009    7.664    0.000\n\nSi noti che la soluzione ottenuta mediante lo stimatore WLSMVS produce indici di bontà di adattamento migliori e errori standard dei parametri più piccoli.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Dati non gaussiani e categoriali</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/03_cat_data.html#riflessioni-conclusive",
    "href": "chapters/cfa/03_cat_data.html#riflessioni-conclusive",
    "title": "41  Dati non gaussiani e categoriali",
    "section": "\n41.4 Riflessioni Conclusive",
    "text": "41.4 Riflessioni Conclusive\nIn questo capitolo, abbiamo esplorato la modellazione CFA con dati non normalmente distribuiti. È essenziale riconoscere che, nella pratica analitica, incontrare dati non normalmente distribuiti dovrebbe essere considerato normale. Di conseguenza, si raccomanda l’utilizzo della massima verosimiglianza robusta (ML robusta) ogni volta che sorgono dubbi sulla normalità dei dati.\nCi sono alcune considerazioni importanti da tenere presente:\n\n\nStabilità delle stime di parametro: Anche se le versioni robuste di ML forniscono errori standard robusti e statistiche di test adattate, le stime dei parametri ottenute rimangono quelle della stima ML originale.\n\nRobustezza limitata: Gli aggiustamenti robusti compensano la violazione della normalità, ma non coprono la presenza di valori anomali, che richiedono un’analisi separata.\n\nLimitazioni degli aggiustamenti: Gli aggiustamenti robusti non trattano violazioni delle specifiche del modello, che è un altro argomento di discussione nella letteratura CFA e SEM.\n\nAbbiamo anche discusso l’uso dello stimatore WLSMV per dati categoriali, evidenziando come esso fornisca una stima dell’errore standard più precisa rispetto all’MLE standard e all’MLE robusta.\nVa notato che WLSMV è un metodo generale per dati categoriali nella CFA, ampiamente implementato in software come MPlus. In lavaan, l’uso di WLSMV può essere attivato semplicemente con lavaan(..., estimator = \"WLSMV\"), equivalente a lavaan(..., estimator = \"DWLS\", se = \"robust.sem\", test = \"scaled.shifted\").\nOltre al WLSMV, lavaan offre anche lo stimatore sperimentale di massima verosimiglianza marginale (MML), che, pur essendo preciso, può essere lento e più suscettibile a problemi di convergenza a causa della complessità dell’integrazione numerica. Un altro stimatore è l’ADF (estimator = “WLS”), che non assume specifiche distributive sui dati, ma richiede una dimensione campionaria molto grande (N &gt; 5000) per considerare affidabili le stime dei parametri, gli errori standard e le statistiche di test.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Dati non gaussiani e categoriali</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/03_cat_data.html#session-info",
    "href": "chapters/cfa/03_cat_data.html#session-info",
    "title": "41  Dati non gaussiani e categoriali",
    "section": "\n41.5 Session Info",
    "text": "41.5 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.2\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] BifactorIndicesCalculator_0.2.2 ggokabeito_0.1.0               \n#&gt;  [3] see_0.11.0                      MASS_7.3-65                    \n#&gt;  [5] viridis_0.6.5                   viridisLite_0.4.2              \n#&gt;  [7] ggpubr_0.6.0                    ggExtra_0.10.1                 \n#&gt;  [9] gridExtra_2.3                   patchwork_1.3.0                \n#&gt; [11] bayesplot_1.11.1                semTools_0.5-6                 \n#&gt; [13] semPlot_1.1.6                   lavaan_0.6-19                  \n#&gt; [15] psych_2.4.12                    scales_1.3.0                   \n#&gt; [17] markdown_1.13                   knitr_1.50                     \n#&gt; [19] lubridate_1.9.4                 forcats_1.0.0                  \n#&gt; [21] stringr_1.5.1                   dplyr_1.1.4                    \n#&gt; [23] purrr_1.0.4                     readr_2.1.5                    \n#&gt; [25] tidyr_1.3.1                     tibble_3.2.1                   \n#&gt; [27] ggplot2_3.5.1                   tidyverse_2.0.0                \n#&gt; [29] here_1.0.1                     \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.1      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.2.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] mime_0.13           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [25] Matrix_1.7-3        R6_2.6.1            fastmap_1.2.0      \n#&gt;  [28] rbibutils_2.3       shiny_1.10.0        digest_0.6.37      \n#&gt;  [31] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n#&gt;  [34] rprojroot_2.0.4     Hmisc_5.2-3         timechange_0.3.0   \n#&gt;  [37] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n#&gt;  [40] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n#&gt;  [43] carData_3.0-5       ggsignif_0.6.4      corpcor_1.6.10     \n#&gt;  [46] gtools_3.9.5        tools_4.4.2         pbivnorm_0.6.0     \n#&gt;  [49] foreign_0.8-88      zip_2.3.2           httpuv_1.6.15      \n#&gt;  [52] nnet_7.3-20         glue_1.8.0          quadprog_1.5-8     \n#&gt;  [55] nlme_3.1-167        promises_1.3.2      lisrelToR_0.3      \n#&gt;  [58] grid_4.4.2          checkmate_2.3.2     cluster_2.1.8.1    \n#&gt;  [61] reshape2_1.4.4      generics_0.1.3      gtable_0.3.6       \n#&gt;  [64] tzdb_0.5.0          data.table_1.17.0   hms_1.1.3          \n#&gt;  [67] car_3.1-3           sem_3.1-16          pillar_1.10.1      \n#&gt;  [70] rockchalk_1.8.157   later_1.4.1         splines_4.4.2      \n#&gt;  [73] lattice_0.22-6      survival_3.8-3      kutils_1.73        \n#&gt;  [76] tidyselect_1.2.1    miniUI_0.1.1.1      pbapply_1.7-2      \n#&gt;  [79] reformulas_0.4.0    stats4_4.4.2        xfun_0.51          \n#&gt;  [82] qgraph_1.9.8        arm_1.14-4          stringi_1.8.4      \n#&gt;  [85] pacman_0.5.1        boot_1.3-31         evaluate_1.0.3     \n#&gt;  [88] codetools_0.2-20    mi_1.1              cli_3.6.4          \n#&gt;  [91] RcppParallel_5.1.10 rpart_4.1.24        xtable_1.8-4       \n#&gt;  [94] Rdpack_2.6.3        munsell_0.5.1       Rcpp_1.0.14        \n#&gt;  [97] coda_0.19-4.1       png_0.1-8           XML_3.99-0.18      \n#&gt; [100] parallel_4.4.2      jpeg_0.1-10         lme4_1.1-36        \n#&gt; [103] mvtnorm_1.3-3       openxlsx_4.2.8      rlang_1.1.5        \n#&gt; [106] multcomp_1.4-28     mnormt_2.1.1\n\n\n\n\n\nBrown, T. A. (2015). Confirmatory factor analysis for applied research. Guilford publications.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Dati non gaussiani e categoriali</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/04_mmm.html",
    "href": "chapters/cfa/04_mmm.html",
    "title": "42  CFA per matrici multi-tratto multi-metodo",
    "section": "",
    "text": "42.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nLa validità descrive quanto accuratamente un metodo di misurazione riesce a quantificare ciò che è inteso misurare. Esistono diverse categorie di validità, ognuna delle quali si verifica attraverso metodi specifici. Una suddivisione convenzionale delle diverse tipologie di validità, che non riflette necessariamente gli sviluppi più recenti in questo campo, può essere descritta come segue (per ulteriori dettagli si rimanda al capitolo dedicato alla validità nella presente dispena):",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>CFA per matrici multi-tratto multi-metodo</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/04_mmm.html#introduzione",
    "href": "chapters/cfa/04_mmm.html#introduzione",
    "title": "42  CFA per matrici multi-tratto multi-metodo",
    "section": "",
    "text": "Validità di facciata: Valuta se gli item di un test appaiono appropriati e ragionevoli rispetto al costrutto che si intende misurare, sia agli occhi di chi partecipa al test sia di chi lo utilizza. Questo tipo di validità è basato sulla percezione esteriore della misura e si valuta tramite i giudizi di esperti sulla plausibilità delle misure.\nValidità di contenuto: Una misura possiede validità di contenuto quando i suoi indicatori rappresentano in modo esaustivo e accurato l’area di contenuto da misurare. Anche questa validità si basa sui giudizi di esperti.\nValidità di costrutto: Corrisponde alla definizione generale di validità e si riferisce alla capacità di uno strumento di misurare il costrutto che intende misurare. La validità di costrutto si verifica attraverso la correttezza con cui gli indicatori misurano i costrutti teorici di interesse e si convalida attraverso l’analisi delle relazioni tra il costrutto misurato e altri costrutti correlati, secondo modelli teorici specifici.\nValidità di criterio: Indica la capacità di uno strumento di fare previsioni accurate su un criterio esterno, valutando quanto bene la misura predice questo criterio.\nValidità concorrente: Si determina osservando quanto uno strumento di misurazione correla con altri strumenti considerati validi per misurare lo stesso attributo. Una forte correlazione è generalmente vista come una conferma della validità.\nValidità convergente: Si verifica confrontando e correlando i punteggi ottenuti con la misura da validare con quelli ottenuti da un altro costrutto teoricamente relazionato. La verifica di questa validità dipende dall’esistenza di misure valide per costrutti correlati.\nValidità discriminante: È l’opposto della validità convergente e si verifica quando la misura in esame non mostra correlazioni significative con le misure di costrutti teoricamente distinti.\n\n\n42.1.1 MTMM e CFA\nLa Matrice Multi-Tratto Multi-Metodo (MTMM) è un approccio utilizzato per valutare la validità di costrutto, esaminando la correlazione tra diversi costrutti misurati sia con gli stessi metodi sia con metodi differenti. La validità di costrutto è considerata alta quando la misura di un costrutto è indipendente dal metodo di misurazione utilizzato.\n\n\n42.1.2 Un esempio concreto\nNell’esempio discusso da {cite:t}brown2015confirmatory, il ricercatore desidera esaminare la validità del costrutto dei disturbi di personalità del Cluster A del DSM-IV, che sono pattern persistenti di sintomi caratterizzati da comportamenti strani o eccentrici (American Psychiatric Association, 1994). Il cluster A comprende tre costrutti di disturbo della personalità:\n\nparanoico (un pattern duraturo di sfiducia e sospetto tale che le motivazioni degli altri sono interpretate come malevole);\nschizoide (un pattern duraturo di distacco dalle relazioni sociali e una gamma ristretta di espressioni emotive);\nschizotipico (un pattern duraturo di disagio acuto nelle relazioni sociali, distorsioni cognitive e percettive ed eccentricità comportamentali).\n\nIn un campione di 500 pazienti, ciascuno di questi tre tratti è misurato mediante tre metodi di valutazione:\n\nun inventario di autovalutazione dei disturbi di personalità;\nvalutazioni dimensionali da un colloquio clinico strutturato sui disturbi della personalità;\nvalutazioni osservazionali effettuate da psicologi.\n\nI dati sono contenuti in una matrice 3 (T) × 3 (M), organizzata in modo tale che le correlazioni tra i diversi tratti (disturbi della personalità: paranoico, schizotipico, schizoide) siano annidate all’interno di ciascun metodo (tipo di valutazione: inventario, colloquio clinico, valutazioni degli osservatori).\nI dati sono riportati qui sotto.\n\nsds &lt;- c(3.61,  3.66,  3.59,  2.94,  3.03,  2.85,  2.22,  2.42,  2.04)\n\ncors &lt;- '\n  1.000 \n  0.290  1.000 \n  0.372  0.478  1.000 \n  0.587  0.238  0.209  1.000 \n  0.201  0.586  0.126  0.213  1.000 \n  0.218  0.281  0.681  0.195  0.096  1.000 \n  0.557  0.228  0.195  0.664  0.242  0.232  1.000 \n  0.196  0.644  0.146  0.261  0.641  0.248  0.383  1.000 \n  0.219  0.241  0.676  0.290  0.168  0.749  0.361  0.342  1.000'\n\ncovs &lt;- getCov(\n  cors, \n  sds = sds, \n  names = c(\"pari\", \"szti\", \"szdi\", \"parc\", \"sztc\", \"szdc\", \"paro\", \"szto\", \"szdo\")\n  )\n\nLa Matrice Multi-Tratto Multi-Metodo (MTMM) si organizza in due tipi di blocchi di coefficienti:\n\nBlocchi di mono-metodo: contengono le correlazioni tra indicatori che provengono dallo stesso metodo di misurazione. Questi blocchi esaminano come diversi indicatori del medesimo tratto si correlano tra loro quando misurati tramite lo stesso strumento.\nBlocchi di etero-metodo: includono le correlazioni tra indicatori misurati mediante metodi diversi. Particolarmente significativa è la “diagonale di validità” all’interno di questi blocchi, dove le correlazioni rappresentano stime di validità convergente. In altre parole, misure diverse di costrutti teoricamente simili dovrebbero mostrare forti correlazioni.\n\nNell’analisi MTMM, una forte correlazione tra metodi che misurano lo stesso tratto evidenzia la validità convergente. Per esempio, potrebbe risultare che diverse misure della personalità schizotipica mostrino correlazioni elevate, con coefficienti ( r ) che variano da 0.676 a 0.749. Al contrario, elementi al di fuori della diagonale nei blocchi di etero-metodo rivelano la validità discriminante, dove le misure di costrutti teoricamente distinti non dovrebbero essere altamente correlate. Questa validità è confermata quando tali correlazioni sono significativamente più basse rispetto a quelle della diagonale di validità, ad esempio, coefficienti che variano da 0.126 a 0.290.\nInoltre, è possibile rilevare gli effetti del metodo esaminando gli elementi al di fuori della diagonale nei blocchi di mono-metodo. Qui, la varianza nelle correlazioni tra diversi tratti misurati con lo stesso metodo, rispetto alle correlazioni tra gli stessi tratti misurati con metodi diversi, riflette l’entità degli effetti del metodo. Ad esempio, le valutazioni dell’osservatore dei tratti della personalità paranoica e schizotipica potrebbero essere più correlate (r = 0.383) rispetto alle loro misure con metodi diversi (ad esempio, la correlazione tra le misure di personalità paranoide e schizotipica, con l’uso rispettivamente dell’inventario e della valutazione dell’osservatore, è di 0.196).\nLa validità del costrutto è supportata quando i dati indicano alta validità convergente e discriminante con effetti del metodo trascurabili.\nIl modello CFA per analizzare la matrice MTMM può includere correlazioni residue tra le specificità di ciascun metodo, supponendo che ogni fattore comune (come paranoid, schizotypal, schizoid) sia identificato da item misurati con metodi diversi e che le specificità di ciascun metodo siano correlate tra loro.\n{cite:t}brown2015confirmatory mostra come sia possibile analizzare la matrice MTMM con un modello CFA nel quale si ipotizza che vi siano correlazioni residue tra le specificità di ciascun metodo. Il modello è dunque formulato nel modo seguente: ogni fattore comune (paranoid, schizotypal, schizoid) è identificato dagli item corrispondenti definiti da metodi diversi; le specificità di ciascun metodo, inoltre, sono correlate tra loro.\n\nmodel &lt;- '\n  paranoid    =~ pari + parc + paro\n  schizotypal =~ szti + sztc + szto\n  schizoid    =~ szdi + szdc + szdo\n  pari ~~ szti + szdi\n  szti ~~ szdi\n  parc ~~ sztc + szdc\n  sztc ~~ szdc\n  paro ~~ szto + szdo\n  szto ~~ szdo\n'  \n\nAdattiamo il modello ai dati.\n\nfit &lt;- cfa(\n  model, \n  sample.cov = covs, \n  sample.nobs = 500, \n  std.lv = TRUE\n)\n\nEsaminiamo la soluzione ottenuta.\n\nsummary(fit, fit.measures = TRUE, standardized = TRUE) |&gt;\n    print()\n\nlavaan 0.6.17 ended normally after 59 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        30\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                                14.371\n  Degrees of freedom                                15\n  P-value (Chi-square)                           0.498\n\nModel Test Baseline Model:\n\n  Test statistic                              2503.656\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.001\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -9879.996\n  Loglikelihood unrestricted model (H1)      -9872.811\n                                                      \n  Akaike (AIC)                               19819.992\n  Bayesian (BIC)                             19946.430\n  Sample-size adjusted Bayesian (SABIC)      19851.209\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.041\n  P-value H_0: RMSEA &lt;= 0.050                    0.989\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.025\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  paranoid =~                                                           \n    pari              2.588    0.145   17.833    0.000    2.588    0.712\n    parc              2.472    0.121   20.350    0.000    2.472    0.841\n    paro              1.747    0.088   19.946    0.000    1.747    0.788\n  schizotypal =~                                                        \n    szti              2.950    0.132   22.367    0.000    2.950    0.788\n    sztc              2.348    0.123   19.047    0.000    2.348    0.768\n    szto              2.047    0.089   22.905    0.000    2.047    0.843\n  schizoid =~                                                           \n    szdi              2.713    0.120   22.526    0.000    2.713    0.769\n    szdc              2.438    0.107   22.826    0.000    2.438    0.860\n    szdo              1.782    0.073   24.323    0.000    1.782    0.872\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .pari ~~                                                               \n   .szti              1.274    0.338    3.774    0.000    1.274    0.217\n   .szdi              2.537    0.329    7.703    0.000    2.537    0.441\n .szti ~~                                                               \n   .szdi              3.872    0.342   11.329    0.000    3.872    0.746\n .parc ~~                                                               \n   .sztc             -0.335    0.210   -1.597    0.110   -0.335   -0.107\n   .szdc             -0.608    0.176   -3.461    0.001   -0.608   -0.265\n .sztc ~~                                                               \n   .szdc             -0.933    0.188   -4.967    0.000   -0.933   -0.330\n .paro ~~                                                               \n   .szto              0.737    0.118    6.240    0.000    0.737    0.413\n   .szdo              0.505    0.096    5.274    0.000    0.505    0.368\n .szto ~~                                                               \n   .szdo              0.625    0.102    6.158    0.000    0.625    0.478\n  paranoid ~~                                                           \n    schizotypal       0.381    0.046    8.341    0.000    0.381    0.381\n    schizoid          0.359    0.046    7.856    0.000    0.359    0.359\n  schizotypal ~~                                                        \n    schizoid          0.310    0.047    6.666    0.000    0.310    0.310\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .pari              6.514    0.513   12.695    0.000    6.514    0.493\n   .parc              2.529    0.334    7.562    0.000    2.529    0.293\n   .paro              1.867    0.179   10.434    0.000    1.867    0.380\n   .szti              5.309    0.460   11.529    0.000    5.309    0.379\n   .sztc              3.846    0.330   11.654    0.000    3.846    0.411\n   .szto              1.704    0.175    9.742    0.000    1.704    0.289\n   .szdi              5.080    0.386   13.158    0.000    5.080    0.408\n   .szdc              2.085    0.230    9.047    0.000    2.085    0.260\n   .szdo              1.005    0.107    9.351    0.000    1.005    0.240\n    paranoid          1.000                               1.000    1.000\n    schizotypal       1.000                               1.000    1.000\n    schizoid          1.000                               1.000    1.000\n\n\n\n\neffectsize::interpret(fit) |&gt;\n    print()\n\n    Name      Value Threshold Interpretation\n1    GFI 0.99376810      0.95   satisfactory\n2   AGFI 0.98130431      0.90   satisfactory\n3    NFI 0.99425997      0.90   satisfactory\n4   NNFI 1.00061169      0.90   satisfactory\n5    CFI 1.00000000      0.90   satisfactory\n6  RMSEA 0.00000000      0.05   satisfactory\n7   SRMR 0.02482894      0.08   satisfactory\n8    RFI 0.98622392      0.90   satisfactory\n9   PNFI 0.41427499      0.50           poor\n10   IFI 1.00025272      0.90   satisfactory\n\n\nPer i dati considerati da {cite:t}brown2015confirmatory, l’adattamento del modello MTMM è eccellente. Ciò fornisce forti evidenze di validità di costrutto per i fattori Paranoico, Schizoide e Schizotipico che sono stati ipotizzati.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>CFA per matrici multi-tratto multi-metodo</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/04_mmm.html#session-info",
    "href": "chapters/cfa/04_mmm.html#session-info",
    "title": "42  CFA per matrici multi-tratto multi-metodo",
    "section": "42.2 Session Info",
    "text": "42.2 Session Info\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.4.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ggokabeito_0.1.0  viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n [5] ggExtra_0.10.1    bayesplot_1.11.1  gridExtra_2.3     patchwork_1.2.0  \n [9] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-17     psych_2.4.3      \n[13] scales_1.3.0      markdown_1.12     knitr_1.45        lubridate_1.9.3  \n[17] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[21] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.0    \n[25] tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.15.0  jsonlite_1.8.8     datawizard_0.9.1  \n  [4] magrittr_2.0.3     TH.data_1.1-2      estimability_1.5  \n  [7] nloptr_2.0.3       rmarkdown_2.26     vctrs_0.6.5       \n [10] minqa_1.2.6        effectsize_0.8.7   base64enc_0.1-3   \n [13] rstatix_0.7.2      htmltools_0.5.7    broom_1.0.5       \n [16] Formula_1.2-5      htmlwidgets_1.6.4  plyr_1.8.9        \n [19] sandwich_3.1-0     emmeans_1.10.0     zoo_1.8-12        \n [22] uuid_1.2-0         igraph_2.0.2       mime_0.12         \n [25] lifecycle_1.0.4    pkgconfig_2.0.3    Matrix_1.6-5      \n [28] R6_2.5.1           fastmap_1.1.1      shiny_1.8.0       \n [31] digest_0.6.35      OpenMx_2.21.11     fdrtool_1.2.17    \n [34] colorspace_2.1-0   rprojroot_2.0.4    Hmisc_5.1-1       \n [37] fansi_1.0.6        timechange_0.3.0   abind_1.4-5       \n [40] compiler_4.3.3     withr_3.0.0        glasso_1.11       \n [43] htmlTable_2.4.2    backports_1.4.1    carData_3.0-5     \n [46] performance_0.11.0 ggsignif_0.6.4     MASS_7.3-60.0.1   \n [49] corpcor_1.6.10     gtools_3.9.5       tools_4.3.3       \n [52] pbivnorm_0.6.0     foreign_0.8-86     zip_2.3.1         \n [55] httpuv_1.6.14      nnet_7.3-19        glue_1.7.0        \n [58] quadprog_1.5-8     nlme_3.1-164       promises_1.2.1    \n [61] lisrelToR_0.3      grid_4.3.3         pbdZMQ_0.3-11     \n [64] checkmate_2.3.1    cluster_2.1.6      reshape2_1.4.4    \n [67] generics_0.1.3     gtable_0.3.4       tzdb_0.4.0        \n [70] data.table_1.15.2  hms_1.1.3          car_3.1-2         \n [73] utf8_1.2.4         sem_3.1-15         pillar_1.9.0      \n [76] IRdisplay_1.1      rockchalk_1.8.157  later_1.3.2       \n [79] splines_4.3.3      lattice_0.22-5     survival_3.5-8    \n [82] kutils_1.73        tidyselect_1.2.0   miniUI_0.1.1.1    \n [85] pbapply_1.7-2      stats4_4.3.3       xfun_0.42         \n [88] qgraph_1.9.8       arm_1.13-1         stringi_1.8.3     \n [91] boot_1.3-29        evaluate_0.23      codetools_0.2-19  \n [94] mi_1.1             cli_3.6.2          RcppParallel_5.1.7\n [97] IRkernel_1.3.2     rpart_4.1.23       parameters_0.21.6 \n[100] xtable_1.8-4       repr_1.1.6         munsell_0.5.0     \n[103] Rcpp_1.0.12        coda_0.19-4.1      png_0.1-8         \n[106] XML_3.99-0.16.1    parallel_4.3.3     ellipsis_0.3.2    \n[109] bayestestR_0.13.2  jpeg_0.1-10        lme4_1.1-35.1     \n[112] mvtnorm_1.2-4      insight_0.19.10    openxlsx_4.2.5.2  \n[115] crayon_1.5.2       rlang_1.1.3        multcomp_1.4-25   \n[118] mnormt_2.1.1",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>CFA per matrici multi-tratto multi-metodo</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/05_bifactor.html",
    "href": "chapters/cfa/05_bifactor.html",
    "title": "43  Modello bifattoriale",
    "section": "",
    "text": "43.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nNumerose misure psicologiche sono progettate per valutare individui su un singolo costrutto. Tuttavia, caratteristiche psicologiche complesse come depressione e ansia spesso si manifestano in modi vari. Di conseguenza, è consigliabile includere item che coprono diverse aree tematiche per assicurare una validità di contenuto adeguata. Pertanto, molte scale di valutazione comunemente usate producono dati che si prestano a interpretazioni valide sia attraverso un modello unidimensionale, con un forte fattore generale, sia tramite un modello multidimensionale, che comprende due o più fattori correlati.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Modello bifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/05_bifactor.html#struttura-fattoriale",
    "href": "chapters/cfa/05_bifactor.html#struttura-fattoriale",
    "title": "43  Modello bifattoriale",
    "section": "43.2 Struttura Fattoriale",
    "text": "43.2 Struttura Fattoriale\nRecenti studi hanno evidenziato che, di fronte a misure che generano dati multidimensionali a causa di una diversificata struttura di contenuto, l’adozione di un modello di misurazione bifattoriale può essere particolarmente efficace per rappresentare la struttura sottostante. Questo modello suggerisce che le correlazioni tra gli item di un test possono essere spiegate attraverso due tipi di fattori: (a) un fattore generale che riflette la varianza condivisa tra tutti gli item, e (b) una serie di fattori di gruppo che catturano la varianza specifica non spiegata dal fattore generale e che è comune tra item simili in termini di contenuto. Generalmente, si ritiene che il fattore generale e i fattori di gruppo siano indipendenti.\nIl fattore generale rappresenta il costrutto principale che lo strumento si propone di misurare, mentre i fattori di gruppo individuano costrutti più specifici legati a sottodomini. I modelli bifattoriali sono utilizzati per diverse finalità importanti:\n\nAnalizzare la distribuzione della varianza quando si presume che uno strumento misuri sia varianza generale sia specifica di gruppo.\nGestire la multidimensionalità in modo che la misura risulti “essenzialmente unidimensionale”, pur presentando dimensioni secondarie.\nVerificare la presenza di un fattore generale sufficientemente robusto da giustificare l’uso di un modello di misurazione unidimensionale.\nDeterminare l’adeguatezza di un punteggio complessivo e valutare l’utilità di analizzare le sottoscale specifiche.\n\nQuesti approcci permettono una comprensione più profonda e una valutazione più accurata della struttura sottostante dei dati psicologici, offrendo agli specialisti gli strumenti per interpretare con maggiore precisione i risultati dei test psicologici.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Modello bifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/05_bifactor.html#un-esempio-pratico",
    "href": "chapters/cfa/05_bifactor.html#un-esempio-pratico",
    "title": "43  Modello bifattoriale",
    "section": "43.3 Un esempio pratico",
    "text": "43.3 Un esempio pratico\nConsideriamo i dati SRS_data forniti dal pacchetto BifactorIndicesCalculator. Il dataset contiene 500 risposte al test SRS-22r sulla qualità della vita legata alla scoliosi, composta da 20 item. La sottoscala “Function” è composta dagli item 5, 9, 12, 15 e 18. La sottoscala “Pain” è composta dagli item 1, 2, 8, 11 e 17. La sottoscala “SelfImage” è composta dagli item 4, 6, 10, 14 e 19. La sottoscala “MentalHealth” è composta dagli item 3, 7, 13, 16 e 20.\nIniziamo esaminando le statistiche descrittive a livello di item e le correlazioni tra gli item.\n\ndescribe(SRS_data)\n\n\nA psych: 20 x 13\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nSRS_1\n1\n500\n3.69\n1.065\n4\n3.78\n1.48\n1\n5\n4\n-0.5509\n-0.4413\n0.0476\n\n\nSRS_2\n2\n500\n3.81\n1.030\n4\n3.92\n1.48\n1\n5\n4\n-0.7148\n-0.0679\n0.0460\n\n\nSRS_3\n3\n500\n3.90\n1.090\n4\n4.04\n1.48\n1\n5\n4\n-0.8323\n-0.0397\n0.0487\n\n\nSRS_4\n4\n500\n3.23\n1.280\n3\n3.29\n1.48\n1\n5\n4\n-0.1059\n-1.0236\n0.0572\n\n\nSRS_5\n5\n500\n4.20\n0.894\n4\n4.30\n1.48\n2\n5\n3\n-0.7453\n-0.5385\n0.0400\n\n\nSRS_6\n6\n500\n3.91\n0.872\n4\n3.95\n1.48\n1\n5\n4\n-0.3767\n-0.3859\n0.0390\n\n\nSRS_7\n7\n500\n4.24\n1.011\n5\n4.40\n0.00\n1\n5\n4\n-1.2842\n1.0807\n0.0452\n\n\nSRS_8\n8\n500\n3.80\n1.132\n4\n3.91\n1.48\n1\n5\n4\n-0.5140\n-0.6993\n0.0506\n\n\nSRS_9\n9\n500\n4.41\n1.004\n5\n4.63\n0.00\n1\n5\n4\n-1.7706\n2.4517\n0.0449\n\n\nSRS_10\n10\n500\n3.70\n0.855\n4\n3.71\n1.48\n1\n5\n4\n-0.2302\n-0.2097\n0.0382\n\n\nSRS_11\n11\n500\n4.50\n0.839\n5\n4.69\n0.00\n1\n5\n4\n-2.0302\n4.0862\n0.0375\n\n\nSRS_12\n12\n500\n4.26\n1.028\n5\n4.44\n0.00\n1\n5\n4\n-1.3462\n1.1192\n0.0460\n\n\nSRS_13\n13\n500\n3.85\n0.927\n4\n3.94\n1.48\n1\n5\n4\n-0.6457\n0.0497\n0.0414\n\n\nSRS_14\n14\n500\n4.72\n0.676\n5\n4.89\n0.00\n1\n5\n4\n-2.8155\n8.2069\n0.0303\n\n\nSRS_15\n15\n500\n4.81\n0.629\n5\n4.99\n0.00\n1\n5\n4\n-4.1179\n18.0401\n0.0281\n\n\nSRS_16\n16\n500\n4.24\n0.960\n5\n4.40\n0.00\n1\n5\n4\n-1.2300\n0.9875\n0.0429\n\n\nSRS_17\n17\n500\n4.74\n0.860\n5\n4.99\n0.00\n1\n5\n4\n-3.4840\n11.2987\n0.0385\n\n\nSRS_18\n18\n500\n2.92\n0.923\n3\n2.92\n0.00\n1\n5\n4\n0.0515\n0.8364\n0.0413\n\n\nSRS_19\n19\n500\n3.60\n1.107\n4\n3.69\n1.48\n1\n5\n4\n-0.5442\n-0.2818\n0.0495\n\n\nSRS_20\n20\n500\n3.99\n0.900\n4\n4.09\n1.48\n1\n5\n4\n-0.8992\n0.7480\n0.0402\n\n\n\n\n\n\nround(cor(SRS_data, use = \"pairwise.complete.obs\"), 2)\n\n\nA matrix: 20 x 20 of type dbl\n\n\n\nSRS_1\nSRS_2\nSRS_3\nSRS_4\nSRS_5\nSRS_6\nSRS_7\nSRS_8\nSRS_9\nSRS_10\nSRS_11\nSRS_12\nSRS_13\nSRS_14\nSRS_15\nSRS_16\nSRS_17\nSRS_18\nSRS_19\nSRS_20\n\n\n\n\nSRS_1\n1.00\n0.88\n0.43\n0.36\n0.36\n0.39\n0.34\n0.70\n0.31\n0.37\n0.46\n0.59\n0.37\n0.36\n0.19\n0.40\n0.38\n0.30\n0.27\n0.33\n\n\nSRS_2\n0.88\n1.00\n0.40\n0.38\n0.35\n0.42\n0.35\n0.70\n0.34\n0.40\n0.49\n0.58\n0.35\n0.38\n0.20\n0.37\n0.39\n0.29\n0.31\n0.32\n\n\nSRS_3\n0.43\n0.40\n1.00\n0.32\n0.33\n0.39\n0.50\n0.46\n0.30\n0.32\n0.24\n0.46\n0.55\n0.34\n0.19\n0.55\n0.25\n0.25\n0.28\n0.41\n\n\nSRS_4\n0.36\n0.38\n0.32\n1.00\n0.23\n0.43\n0.32\n0.33\n0.19\n0.43\n0.20\n0.30\n0.30\n0.20\n0.23\n0.32\n0.10\n0.21\n0.54\n0.32\n\n\nSRS_5\n0.36\n0.35\n0.33\n0.23\n1.00\n0.33\n0.39\n0.33\n0.47\n0.34\n0.23\n0.52\n0.31\n0.29\n0.20\n0.42\n0.28\n0.31\n0.25\n0.40\n\n\nSRS_6\n0.39\n0.42\n0.39\n0.43\n0.33\n1.00\n0.48\n0.37\n0.25\n0.64\n0.28\n0.37\n0.41\n0.38\n0.22\n0.47\n0.22\n0.31\n0.62\n0.41\n\n\nSRS_7\n0.34\n0.35\n0.50\n0.32\n0.39\n0.48\n1.00\n0.40\n0.24\n0.37\n0.22\n0.42\n0.53\n0.44\n0.23\n0.78\n0.19\n0.39\n0.39\n0.56\n\n\nSRS_8\n0.70\n0.70\n0.46\n0.33\n0.33\n0.37\n0.40\n1.00\n0.30\n0.37\n0.36\n0.52\n0.40\n0.28\n0.14\n0.42\n0.31\n0.32\n0.28\n0.34\n\n\nSRS_9\n0.31\n0.34\n0.30\n0.19\n0.47\n0.25\n0.24\n0.30\n1.00\n0.36\n0.26\n0.49\n0.32\n0.29\n0.22\n0.32\n0.35\n0.27\n0.20\n0.31\n\n\nSRS_10\n0.37\n0.40\n0.32\n0.43\n0.34\n0.64\n0.37\n0.37\n0.36\n1.00\n0.26\n0.37\n0.30\n0.34\n0.22\n0.39\n0.19\n0.28\n0.54\n0.35\n\n\nSRS_11\n0.46\n0.49\n0.24\n0.20\n0.23\n0.28\n0.22\n0.36\n0.26\n0.26\n1.00\n0.42\n0.19\n0.33\n0.20\n0.23\n0.39\n0.18\n0.24\n0.19\n\n\nSRS_12\n0.59\n0.58\n0.46\n0.30\n0.52\n0.37\n0.42\n0.52\n0.49\n0.37\n0.42\n1.00\n0.43\n0.46\n0.23\n0.46\n0.44\n0.33\n0.32\n0.43\n\n\nSRS_13\n0.37\n0.35\n0.55\n0.30\n0.31\n0.41\n0.53\n0.40\n0.32\n0.30\n0.19\n0.43\n1.00\n0.35\n0.17\n0.57\n0.21\n0.30\n0.35\n0.60\n\n\nSRS_14\n0.36\n0.38\n0.34\n0.20\n0.29\n0.38\n0.44\n0.28\n0.29\n0.34\n0.33\n0.46\n0.35\n1.00\n0.33\n0.46\n0.38\n0.30\n0.32\n0.34\n\n\nSRS_15\n0.19\n0.20\n0.19\n0.23\n0.20\n0.22\n0.23\n0.14\n0.22\n0.22\n0.20\n0.23\n0.17\n0.33\n1.00\n0.27\n0.20\n0.17\n0.29\n0.24\n\n\nSRS_16\n0.40\n0.37\n0.55\n0.32\n0.42\n0.47\n0.78\n0.42\n0.32\n0.39\n0.23\n0.46\n0.57\n0.46\n0.27\n1.00\n0.19\n0.38\n0.36\n0.59\n\n\nSRS_17\n0.38\n0.39\n0.25\n0.10\n0.28\n0.22\n0.19\n0.31\n0.35\n0.19\n0.39\n0.44\n0.21\n0.38\n0.20\n0.19\n1.00\n0.25\n0.15\n0.16\n\n\nSRS_18\n0.30\n0.29\n0.25\n0.21\n0.31\n0.31\n0.39\n0.32\n0.27\n0.28\n0.18\n0.33\n0.30\n0.30\n0.17\n0.38\n0.25\n1.00\n0.25\n0.31\n\n\nSRS_19\n0.27\n0.31\n0.28\n0.54\n0.25\n0.62\n0.39\n0.28\n0.20\n0.54\n0.24\n0.32\n0.35\n0.32\n0.29\n0.36\n0.15\n0.25\n1.00\n0.41\n\n\nSRS_20\n0.33\n0.32\n0.41\n0.32\n0.40\n0.41\n0.56\n0.34\n0.31\n0.35\n0.19\n0.43\n0.60\n0.34\n0.24\n0.59\n0.16\n0.31\n0.41\n1.00\n\n\n\n\n\n\nSRS_UnidimensionalModel &lt;-\n\"\n    SRS =~ SRS_1 + SRS_2 + SRS_3 + SRS_4 + SRS_5 +\n    SRS_6 + SRS_7 + SRS_8 + SRS_9 + SRS_10 +\n    SRS_11 + SRS_12 + SRS_13 + SRS_14 + SRS_15 +\n    SRS_16 + SRS_17 + SRS_18 + SRS_19 + SRS_20\n\"\n\nSRS_Unidimensional &lt;- lavaan::cfa(SRS_UnidimensionalModel,\n    SRS_data,\n    ordered = paste0(\"SRS_\", 1:20),\n    orthogonal = TRUE\n)\n\n\nsemPaths(\n    SRS_Unidimensional, \n    intercepts = FALSE\n)\n\n\n\n\n\n\n\n\nEsaminiamo la bontà di adattamento.\n\nfit.subset &lt;- c(\n    \"chisq.scaled\", \"df\", \"pvalue.scaled\",\n    \"rmsea.scaled\", \"rmsea.pvalue.scale\",\n    \"rmsea.ci.lower.scaled\", \"rmsea.ci.upper.scaled\",\n    \"cfi\", \"tli\", \"srmr\"\n)\n\n\nfitmeasures(SRS_Unidimensional, fit.subset) |&gt; print()\n\n         chisq.scaled                    df         pvalue.scaled \n             2087.431               170.000                 0.000 \n         rmsea.scaled rmsea.ci.lower.scaled rmsea.ci.upper.scaled \n                0.150                 0.145                 0.156 \n                  cfi                   tli                  srmr \n                0.961                 0.956                 0.119 \n\n\n\nSRS_BifactorModel &lt;-\n\"\n    SRS =~ SRS_1 + SRS_2 + SRS_3 + SRS_4 + SRS_5 + \n           SRS_6 + SRS_7 + SRS_8 + SRS_9 + SRS_10 +\n           SRS_11 + SRS_12 + SRS_13 + SRS_14 + SRS_15 +\n           SRS_16 + SRS_17 + SRS_18 + SRS_19 + SRS_20\n    Function =~ SRS_5 + SRS_9 + SRS_12 + SRS_15 + SRS_18\n    Pain =~ SRS_1 + SRS_2 + SRS_8 + SRS_11 + SRS_17\n    SelfImage =~ SRS_4 + SRS_6 + SRS_10 + SRS_14 + SRS_19\n    MentalHealth =~ SRS_3 + SRS_7 + SRS_13 + SRS_16 + SRS_20\n\"\n\nSRS_bifactor &lt;- lavaan::cfa(SRS_BifactorModel,\n    SRS_data,\n    ordered = paste0(\"SRS_\", 1:20),\n    orthogonal = TRUE\n)\n\n\nsemPaths(\n    SRS_bifactor,\n    intercepts = FALSE\n)\n\n\n\n\n\n\n\n\nEsaminiamo la bontà di adattamento.\n\nfitmeasures(SRS_bifactor, fit.subset) |&gt; print()\n\n         chisq.scaled                    df         pvalue.scaled \n              468.648               150.000                 0.000 \n         rmsea.scaled rmsea.ci.lower.scaled rmsea.ci.upper.scaled \n                0.065                 0.059                 0.072 \n                  cfi                   tli                  srmr \n                0.997                 0.996                 0.055 \n\n\nConfrontiamo i due modelli.\n\nlavTestLRT(SRS_Unidimensional, SRS_bifactor) |&gt; print()\n\n\nScaled Chi-Squared Difference Test (method = \"satorra.2000\")\n\nlavaan-&gt;lavTestLRT():  \n   lavaan NOTE: The \"Chisq\" column contains standard test statistics, not \n   the robust test that should be reported per model. A robust difference \n   test is a function of two standard (not robust) statistics.\n                    Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq)    \nSRS_bifactor       150           309                                  \nSRS_Unidimensional 170          1965       1007      20     &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nConsideriamo ora gli indici specifici per un modello bifattoriale.\n\nbifactorIndices(SRS_bifactor, UniLambda = SRS_Unidimensional) |&gt; print()\n\n$ModelLevelIndices\n   ECV.SRS        PUC  Omega.SRS OmegaH.SRS       ARPB \n     0.673      0.789      0.942      0.834      0.121 \n\n$FactorLevelIndices\n             ECV_SS ECV_SG ECV_GS Omega OmegaH     H    FD\nSRS           0.673 0.6728  0.673 0.942 0.8338 0.943 0.952\nFunction      0.197 0.0415  0.803 0.799 0.0912 0.403 0.710\nPain          0.412 0.1115  0.588 0.882 0.3427 0.696 0.921\nSelfImage     0.328 0.0818  0.672 0.850 0.2025 0.591 0.830\nMentalHealth  0.342 0.0923  0.658 0.892 0.2930 0.615 0.854\n\n$ItemLevelIndices\n        IECV RelParBias\nSRS_1  0.510     0.3534\nSRS_2  0.498     0.3675\nSRS_3  0.798     0.0366\nSRS_4  0.612     0.0522\nSRS_5  0.635     0.0226\nSRS_6  0.637     0.0881\nSRS_7  0.632     0.1789\nSRS_8  0.682     0.1034\nSRS_9  0.588     0.0192\nSRS_10 0.649     0.0642\nSRS_11 0.633     0.1217\nSRS_12 0.938     0.1001\nSRS_13 0.624     0.1179\nSRS_14 0.999     0.0990\nSRS_15 1.000     0.1011\nSRS_16 0.601     0.1898\nSRS_17 0.750     0.0392\nSRS_18 0.999     0.0933\nSRS_19 0.458     0.1664\nSRS_20 0.691     0.1048\n\n\n\nI ModelLevelIndices possono essere spiegati nel modo seguente:\n\nECV.SRS: Questo indica la proporzione di varianza spiegata dal fattore generale nel modello bifattoriale. ECV sta per “Explained Common Variance” (Varianza Comune Spiegata). Un valore più alto indica che una maggiore parte della varianza totale nei dati è spiegata dal fattore generale.\nPUC: Questo è l’acronimo di “Percentage of Uniqueness in Common” (Percentuale di Unicità nel Comune). Indica quanto della varianza unica (cioè quella non spiegata dal fattore generale) è presente nei fattori di gruppo. Un valore basso indica che i fattori di gruppo spiegano una maggiore parte della varianza unica nei dati.\nOmega.SRS: Questo indice rappresenta il coefficiente di affidabilità del fattore generale del modello bifattoriale. Indica quanto sia affidabile il fattore generale nel catturare la varianza comune tra tutti gli item del test. Un valore più alto indica maggiore affidabilità.\nOmegaH.SRS: Questo indice rappresenta il coefficiente di affidabilità dei fattori di gruppo nel modello bifattoriale. Indica quanto sia affidabile l’insieme dei fattori di gruppo nel catturare la varianza condivisa tra gli item del gruppo specifico. Anche qui, un valore più alto indica maggiore affidabilità.\nARPB: Questo sta per “Average Reproducibility of Parameter Estimates” (Riproducibilità Media delle Stime dei Parametri). Rappresenta la riproducibilità media delle stime dei parametri del modello bifattoriale. In sostanza, valuta quanto le stime dei parametri del modello sono affidabili e riproducibili.\n\nLa sezione dell’output FactorLevelIndices riguarda gli indici a livello di fattore del modello bifattoriale.\n\nECV_SS, ECV_SG, ECV_GS: Questi rappresentano rispettivamente la proporzione di varianza spiegata dal Fattore Generale (GG), dal Fattore Specifico (SS) e dall’Interazione tra Fattore Generale e Fattore Specifico (GS) per ciascun fattore. Indicano quanto ciascun tipo di varianza contribuisce alla spiegazione della varianza totale nell’insieme dei dati del fattore.\nOmega: Questo indice rappresenta il coefficiente di affidabilità dell’estratto del Fattore Generale per ciascun fattore. Indica quanto sia affidabile il Fattore Generale nel catturare la varianza comune tra gli item di quel particolare fattore. Un valore più alto indica maggiore affidabilità.\nOmegaH: Questo indice rappresenta il coefficiente di affidabilità dell’estratto del Fattore Specifico per ciascun fattore. Indica quanto sia affidabile l’insieme dei Fattori Specifici nel catturare la varianza condivisa tra gli item del gruppo specifico. Anche qui, un valore più alto indica maggiore affidabilità.\nH: Questo indice rappresenta la quota della varianza unica spiegata dal Fattore Generale per ciascun fattore. Indica quanto della varianza unica è spiegata dal Fattore Generale piuttosto che da fattori specifici.\nFD: Questo indice rappresenta la distorsione fattoriale, che è una misura di quanto i dati si adattino bene al modello bifattoriale. Valori vicini a 1 indicano un buon adattamento.\n\nInfine, l’output ItemLevelIndices riguarda gli indici a livello di item in un modello bifattoriale.\n\nIECV: Questo indica la proporzione di varianza spiegata dal Fattore Generale per ciascun item. IECV sta per “Item Explained Common Variance” (Varianza Comune Spiegata dell’Item). Indica quanto della varianza totale dell’item può essere spiegata dal Fattore Generale del modello bifattoriale. Valori più alti indicano che il Fattore Generale contribuisce maggiormente a spiegare le variazioni osservate nell’item.\nRelParBias: Questo rappresenta il bias relativo dei parametri dell’item. Indica quanto i parametri dell’item sono influenzati dalla presenza del Fattore Generale e dai fattori specifici nel modello. Valori più alti indicano una maggiore influenza dei fattori specifici rispetto al Fattore Generale nell’item.\n\nPer i dati dell’esempio considerato, di seguito è riportata un’interpretazione succinta dei risultati chiave per ciascun gruppo principale di risultati:\n\n43.3.1 Model-Level Indices\n\nECV.SRS (Explained Common Variance): Il 67.28% della varianza osservata è spiegata dal modello.\nPUC (Percentage of Uncontaminated Correlations): Il 78.95% delle correlazioni tra gli item è “puro”, cioè non contaminato da altri fattori oltre al fattore generale.\nOmega.SRS: La consistenza interna complessiva del test è molto alta (0.942), indicando una buona affidabilità.\nOmegaH.SRS: Il 83.38% della varianza totale standardizzata è attribuibile al fattore generale, confermando che è un fattore dominante nel modello.\nARPB (Average Relative Parameter Bias): Un bias relativo medio basso (0.121) suggerisce che le stime dei parametri sono relativamente poco distorte.\n\n\n\n43.3.2 Factor-Level Indices\n\nECV (Explained Common Variance) per i fattori specifici:\n\nFunzione: 19.73% della varianza è spiegata dal fattore specifico “Funzione”, con l’80.27% attribuibile al fattore generale.\nDolore (Pain): 41.24% della varianza è spiegata dal fattore specifico “Dolore”.\nAutopercezione (SelfImage): 32.80% della varianza è spiegata dal fattore specifico “Autopercezione”.\nSalute Mentale (MentalHealth): 34.24% della varianza è spiegata dal fattore specifico “Salute Mentale”.\n\nOmega e OmegaH per ogni fattore specifico: Le misure Omega indicano la consistenza interna per ciascun sottogruppo di item, mentre OmegaH indica la proporzione della varianza attribuibile ai fattori specifici rispetto al fattore generale.\n\n\n\n43.3.3 Item-Level Indices\n\nIECV (Item Explained Common Variance): Valori come 0.937 per SRS_12 e quasi 1 per SRS_14, SRS_15 e SRS_18 indicano che questi item sono molto influenzati dal fattore generale.\nRelParBias (Relative Parameter Bias): La maggior parte degli item mostra un bias relativo basso, suggerendo che gli effetti dei fattori specifici su questi item sono correttamente rappresentati senza grande distorsione.\n\nIn sintesi, il modello bifattoriale sembra adattarsi bene ai dati, con un forte fattore generale che domina la struttura del test, supportato da alcuni fattori specifici che spiegano porzioni significative della varianza in diverse aree tematiche. Gli item individuati con alti valori di IECV sono particolarmente rappresentativi del fattore generale.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Modello bifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/05_bifactor.html#commenti-e-considerazioni-conclusive",
    "href": "chapters/cfa/05_bifactor.html#commenti-e-considerazioni-conclusive",
    "title": "43  Modello bifattoriale",
    "section": "43.4 Commenti e considerazioni conclusive",
    "text": "43.4 Commenti e considerazioni conclusive\nIn questo capitolo, abbiamo esplorato diversi indici derivati dall’analisi con un modello bifattoriale, ognuno dei quali rivela aspetti specifici delle proprietà psicometriche di uno strumento di misura. Questi indici sono di grande utilità per gli sviluppatori e i valutatori di scale, oltre a essere strumenti preziosi per i ricercatori e i professionisti che le impiegano nella pratica clinica e nella ricerca. Inoltre, contribuiscono allo sviluppo e alla comprensione dei costrutti psicologici che tali strumenti intendono misurare.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Modello bifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/05_bifactor.html#session-info",
    "href": "chapters/cfa/05_bifactor.html#session-info",
    "title": "43  Modello bifattoriale",
    "section": "43.5 Session Info",
    "text": "43.5 Session Info\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] BifactorIndicesCalculator_0.2.2 MASS_7.3-61                    \n [3] viridis_0.6.5                   viridisLite_0.4.2              \n [5] ggpubr_0.6.0                    ggExtra_0.10.1                 \n [7] gridExtra_2.3                   patchwork_1.3.0                \n [9] bayesplot_1.11.1                semTools_0.5-6                 \n[11] semPlot_1.1.6                   lavaan_0.6-19                  \n[13] psych_2.4.6.26                  scales_1.3.0                   \n[15] markdown_1.13                   knitr_1.49                     \n[17] lubridate_1.9.3                 forcats_1.0.0                  \n[19] stringr_1.5.1                   dplyr_1.1.4                    \n[21] purrr_1.0.2                     readr_2.1.5                    \n[23] tidyr_1.3.1                     tibble_3.2.1                   \n[25] ggplot2_3.5.1                   tidyverse_2.0.0                \n[27] here_1.0.1                     \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.17.1   jsonlite_1.8.9      magrittr_2.0.3     \n  [4] TH.data_1.1-2       estimability_1.5.1  farver_2.1.2       \n  [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n [10] Cairo_1.6-2         minqa_1.2.8         base64enc_0.1-3    \n [13] rstatix_0.7.2       htmltools_0.5.8.1   broom_1.0.7        \n [16] Formula_1.2-5       htmlwidgets_1.6.4   plyr_1.8.9         \n [19] sandwich_3.1-1      emmeans_1.10.5      zoo_1.8-12         \n [22] uuid_1.2-1          igraph_2.1.1        mime_0.12          \n [25] lifecycle_1.0.4     pkgconfig_2.0.3     Matrix_1.7-1       \n [28] R6_2.5.1            fastmap_1.2.0       shiny_1.9.1        \n [31] digest_0.6.37       OpenMx_2.21.13      fdrtool_1.2.18     \n [34] colorspace_2.1-1    rprojroot_2.0.4     Hmisc_5.2-0        \n [37] fansi_1.0.6         timechange_0.3.0    abind_1.4-8        \n [40] compiler_4.4.2      withr_3.0.2         glasso_1.11        \n [43] htmlTable_2.4.3     backports_1.5.0     carData_3.0-5      \n [46] ggsignif_0.6.4      corpcor_1.6.10      gtools_3.9.5       \n [49] tools_4.4.2         pbivnorm_0.6.0      foreign_0.8-87     \n [52] zip_2.3.1           httpuv_1.6.15       nnet_7.3-19        \n [55] glue_1.8.0          quadprog_1.5-8      promises_1.3.0     \n [58] nlme_3.1-166        lisrelToR_0.3       grid_4.4.2         \n [61] pbdZMQ_0.3-13       checkmate_2.3.2     cluster_2.1.6      \n [64] reshape2_1.4.4      generics_0.1.3      gtable_0.3.6       \n [67] tzdb_0.4.0          data.table_1.16.2   hms_1.1.3          \n [70] car_3.1-3           utf8_1.2.4          sem_3.1-16         \n [73] pillar_1.9.0        IRdisplay_1.1       rockchalk_1.8.157  \n [76] later_1.3.2         splines_4.4.2       cherryblossom_0.1.0\n [79] lattice_0.22-6      survival_3.7-0      kutils_1.73        \n [82] tidyselect_1.2.1    miniUI_0.1.1.1      pbapply_1.7-2      \n [85] airports_0.1.0      stats4_4.4.2        xfun_0.49          \n [88] qgraph_1.9.8        arm_1.14-4          stringi_1.8.4      \n [91] pacman_0.5.1        boot_1.3-31         evaluate_1.0.1     \n [94] codetools_0.2-20    mi_1.1              cli_3.6.3          \n [97] RcppParallel_5.1.9  IRkernel_1.3.2      rpart_4.1.23       \n[100] xtable_1.8-4        repr_1.1.7          munsell_0.5.1      \n[103] Rcpp_1.0.13-1       coda_0.19-4.1       png_0.1-8          \n[106] XML_3.99-0.17       parallel_4.4.2      usdata_0.3.1       \n[109] jpeg_0.1-10         lme4_1.1-35.5       mvtnorm_1.3-2      \n[112] openxlsx_4.2.7.1    crayon_1.5.3        openintro_2.5.0    \n[115] rlang_1.1.4         multcomp_1.4-26     mnormt_2.1.1",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Modello bifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/06_efa_lavaan.html",
    "href": "chapters/cfa/06_efa_lavaan.html",
    "title": "36  Exploratory Structural Equation Modeling",
    "section": "",
    "text": "36.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nL’Exploratory Structural Equation Modeling (ESEM) rappresenta un framework analitico innovativo che combina i vantaggi dell’Analisi Fattoriale Esplorativa (EFA) con il rigore della Confirmatory Factor Analysis (CFA). Questo approccio integrato consente di mantenere la flessibilità tipica dell’EFA, che permette di considerare le saturazioni incrociate tra i fattori, pur preservando la specificità e il controllo strutturale offerti dalla CFA. In particolare, l’ESEM si distingue per la sua capacità di bilanciare rigore metodologico e adattabilità, rendendolo adatto sia a contesti esplorativi che confermativi.\nUn aspetto chiave dell’ESEM è l’introduzione della rotazione target, una tecnica che facilita la definizione a priori dei carichi fattoriali principali, pur consentendo ai carichi incrociati di rimanere il più possibile vicini a zero, ma senza imporre vincoli rigidi. Questa caratteristica permette di applicare il modello in modo confermativo, basandosi su una struttura fattoriale predefinita, ma con una flessibilità tipicamente associata all’EFA. Di conseguenza, l’ESEM si presta efficacemente a contesti in cui è necessario convalidare ipotesi preesistenti, pur lasciando spazio all’esplorazione di relazioni inattese tra variabili.\nNella tradizionale Confirmatory Factor Analysis (CFA), ampiamente utilizzata in ambito psicologico, la struttura fattoriale è definita a priori: si assume che ogni indicatore carichi esclusivamente sul proprio fattore latente di riferimento, con saturazioni incrociate fissate a zero. Questo approccio, sebbene metodologicamente rigoroso, presenta limitazioni significative. In particolare, i modelli CFA tendono a essere eccessivamente restrittivi, presupponendo “fattori puri” in cui ogni item contribuisce solo al proprio costrutto latente. Tuttavia, nella pratica psicologica, molti item riflettono più di un costrutto, rendendo questa assunzione spesso irrealistica. Ignorare le saturazioni incrociate può portare a una rappresentazione distorta delle relazioni tra item e fattori, con conseguenti sovrastime delle statistiche di adattamento del modello e distorsioni positive nelle correlazioni tra fattori. Studi di simulazione hanno dimostrato che anche piccole saturazioni incrociate, se non considerate, possono alterare significativamente le stime dei parametri.\nUn ulteriore problema della CFA riguarda gli indici di bontà di adattamento, che risultano spesso troppo rigidi per strumenti psicologici multifattoriali. Questa rigidità rende difficile ottenere un adattamento soddisfacente senza apportare modifiche sostanziali ai modelli. Tuttavia, è importante notare che modelli con indici di adattamento non ottimali possono comunque presentare saturazioni ragionevoli e alti livelli di affidabilità quando analizzati a livello di item.\nProprio per superare queste limitazioni, l’ESEM si è affermato come un approccio più flessibile e robusto, in grado di cogliere la complessità delle misure psicologiche senza sacrificare il rigore metodologico. Grazie alla sua capacità di integrare i punti di forza dell’EFA e della CFA, l’ESEM offre un quadro analitico più realistico e adattabile, rendendolo uno strumento prezioso per la ricerca in ambito psicologico.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Exploratory Structural Equation Modeling</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/06_efa_lavaan.html#exploratory-structural-equation-modeling",
    "href": "chapters/cfa/06_efa_lavaan.html#exploratory-structural-equation-modeling",
    "title": "36  Exploratory Structural Equation Modeling",
    "section": "\n36.2 Exploratory Structural Equation Modeling",
    "text": "36.2 Exploratory Structural Equation Modeling\nL’ESEM combina elementi delle CFA e dell’Exploratory Factor Analysis (EFA) all’interno del tradizionale framework delle Equazioni Strutturali (SEM). Questo approccio rappresenta un compromesso tra la ricerca iterativa di soluzioni fattoriali ottimali, tipica dell’EFA, e la modellazione teorica restrittiva delle CFA.\nL’ESEM è essenzialmente un metodo confermativo che permette anche un’esplorazione attraverso l’uso di rotazioni mirate, mantenendo la presenza di caricamenti incrociati, seppur minimizzati. All’interno dell’ESEM, il ricercatore può prevedere a priori una struttura fattoriale, similmente a quanto avviene nelle CFA, ma con una maggiore flessibilità permessa dalla possibilità di modellare saturazioni incrociate.\nNell’ESEM, i fattori generali e specifici devono essere specificati come totalmente indipendenti, e le rotazioni ortogonali sono comuni nei modelli bifattoriali. I metodi di rotazione più usati nell’ESEM includono le rotazioni geomin e target, con rotazioni ortogonali adatte ai modelli più complessi.\nLe analisi di simulazione indicano che le correlazioni tra i fattori latenti ottenute con l’ESEM sono generalmente meno distorte e più vicine alle vere associazioni, rendendo i modelli ESEM più coerenti con le teorie sottostanti e le intenzioni degli strumenti psicometrici misurati.\nQuando un modello ESEM include solo una parte di misurazione, viene definito come “analisi fattoriale esplorativa” o EFA. Se il modello include anche una parte strutturale, come regressioni tra variabili latenti, è classificato come “modello di equazioni strutturali esplorativo” o ESEM.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Exploratory Structural Equation Modeling</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/06_efa_lavaan.html#un-esempio-pratico",
    "href": "chapters/cfa/06_efa_lavaan.html#un-esempio-pratico",
    "title": "36  Exploratory Structural Equation Modeling",
    "section": "\n36.3 Un Esempio Pratico",
    "text": "36.3 Un Esempio Pratico\nIn questo esempio pratico analizzeremo nuovamente i dati di Brown (2015), ovvero otto misure di personalità raccolte su un campione di 250 pazienti che hanno concluso un programma di psicoterapia. Utilizzeremo un’analisi EFA mediante la funzione efa() di lavaan.\nGli item sono i seguenti:\n\nanxiety (N1),\nhostility (N2),\ndepression (N3),\nself-consciousness (N4),\nwarmth (E1),\ngregariousness (E2),\nassertiveness (E3),\npositive emotions (E4).\n\n\nvarnames &lt;- c(\"N1\", \"N2\", \"N3\", \"N4\", \"E1\", \"E2\", \"E3\", \"E4\")\nsds &lt;- \"5.7  5.6  6.4  5.7  6.0  6.2  5.7  5.6\"\n\ncors &lt;- \"\n 1.000\n 0.767  1.000\n 0.731  0.709  1.000\n 0.778  0.738  0.762  1.000\n-0.351  -0.302  -0.356  -0.318  1.000\n-0.316  -0.280  -0.300  -0.267  0.675  1.000\n-0.296  -0.289  -0.297  -0.296  0.634  0.651  1.000\n-0.282  -0.254  -0.292  -0.245  0.534  0.593  0.566  1.000\"\n\npsychot_cor_mat &lt;- getCov(cors, names = varnames)\nn &lt;- 250\n\nDefiniamo un modello ad un solo fattore comune.\n\n# 1-factor model\nf1 &lt;- '\n    efa(\"efa\")*f1 =~ N1 + N2 + N3 + N4 + E1 + E2 + E3 + E4\n'\n\nDefiniamo un modello con due fattori comuni.\n\n# 2-factor model\nf2 &lt;- '\n    efa(\"efa\")*f1 +\n    efa(\"efa\")*f2 =~ N1 + N2 + N3 + N4 + E1 + E2 + E3 + E4\n'\n\nAdattiamo ai dati il modello ad un fattore comune.\n\nefa_f1 &lt;-cfa(\n    model = f1,\n    sample.cov = psychot_cor_mat,\n    sample.nobs = 250,\n    rotation = \"oblimin\"\n)\n\n\nsemPlot::semPaths(\n    efa_f1,\n    what = \"col\", whatLabels = \"no\", style = \"mx\",\n    layout = \"tree\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 5, sizeMan2 = 4\n)\n\n\n\n\n\n\n\nEsaminiamo la soluzione ottenuta.\n\nsummary(\n    efa_f1,\n    fit.measures = TRUE,\n    standardized = TRUE,\n    rsquare = TRUE\n) |&gt; \n    print()\n#&gt; lavaan 0.6-19 ended normally after 2 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        16\n#&gt; \n#&gt;   Rotation method                      OBLIMIN OBLIQUE\n#&gt;   Oblimin gamma                                      0\n#&gt;   Rotation algorithm (rstarts)                GPA (30)\n#&gt;   Standardized metric                             TRUE\n#&gt;   Row weights                                     None\n#&gt; \n#&gt;   Number of observations                           250\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                               375.327\n#&gt;   Degrees of freedom                                20\n#&gt;   P-value (Chi-square)                           0.000\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                              1253.791\n#&gt;   Degrees of freedom                                28\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.710\n#&gt;   Tucker-Lewis Index (TLI)                       0.594\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -2394.637\n#&gt;   Loglikelihood unrestricted model (H1)      -2206.974\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                4821.275\n#&gt;   Bayesian (BIC)                              4877.618\n#&gt;   Sample-size adjusted Bayesian (SABIC)       4826.897\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.267\n#&gt;   90 Percent confidence interval - lower         0.243\n#&gt;   90 Percent confidence interval - upper         0.291\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.000\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    1.000\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.187\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   f1 =~ efa                                                             \n#&gt;     N1                0.879    0.051   17.333    0.000    0.879    0.880\n#&gt;     N2                0.841    0.052   16.154    0.000    0.841    0.842\n#&gt;     N3                0.841    0.052   16.175    0.000    0.841    0.843\n#&gt;     N4                0.870    0.051   17.065    0.000    0.870    0.872\n#&gt;     E1               -0.438    0.062   -7.041    0.000   -0.438   -0.439\n#&gt;     E2               -0.398    0.063   -6.327    0.000   -0.398   -0.398\n#&gt;     E3               -0.398    0.063   -6.342    0.000   -0.398   -0.399\n#&gt;     E4               -0.364    0.063   -5.746    0.000   -0.364   -0.364\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .N1                0.224    0.028    7.915    0.000    0.224    0.225\n#&gt;    .N2                0.289    0.033    8.880    0.000    0.289    0.290\n#&gt;    .N3                0.288    0.032    8.866    0.000    0.288    0.289\n#&gt;    .N4                0.239    0.029    8.174    0.000    0.239    0.240\n#&gt;    .E1                0.804    0.073   10.963    0.000    0.804    0.807\n#&gt;    .E2                0.838    0.076   11.008    0.000    0.838    0.841\n#&gt;    .E3                0.837    0.076   11.007    0.000    0.837    0.841\n#&gt;    .E4                0.864    0.078   11.041    0.000    0.864    0.867\n#&gt;     f1                1.000                               1.000    1.000\n#&gt; \n#&gt; R-Square:\n#&gt;                    Estimate\n#&gt;     N1                0.775\n#&gt;     N2                0.710\n#&gt;     N3                0.711\n#&gt;     N4                0.760\n#&gt;     E1                0.193\n#&gt;     E2                0.159\n#&gt;     E3                0.159\n#&gt;     E4                0.133\n\n\nstandardizedSolution(efa_f1) |&gt; print()\n#&gt;    lhs op rhs est.std    se      z pvalue ci.lower ci.upper\n#&gt; 1   f1 =~  N1   0.880 0.018 48.290      0    0.845    0.916\n#&gt; 2   f1 =~  N2   0.842 0.022 38.609      0    0.800    0.885\n#&gt; 3   f1 =~  N3   0.843 0.022 38.757      0    0.800    0.886\n#&gt; 4   f1 =~  N4   0.872 0.019 45.875      0    0.835    0.909\n#&gt; 5   f1 =~  E1  -0.439 0.054 -8.181      0   -0.544   -0.334\n#&gt; 6   f1 =~  E2  -0.398 0.056 -7.140      0   -0.508   -0.289\n#&gt; 7   f1 =~  E3  -0.399 0.056 -7.161      0   -0.508   -0.290\n#&gt; 8   f1 =~  E4  -0.364 0.057 -6.347      0   -0.477   -0.252\n#&gt; 9   N1 ~~  N1   0.225 0.032  7.008      0    0.162    0.288\n#&gt; 10  N2 ~~  N2   0.290 0.037  7.896      0    0.218    0.362\n#&gt; 11  N3 ~~  N3   0.289 0.037  7.882      0    0.217    0.361\n#&gt; 12  N4 ~~  N4   0.240 0.033  7.230      0    0.175    0.305\n#&gt; 13  E1 ~~  E1   0.807 0.047 17.141      0    0.715    0.900\n#&gt; 14  E2 ~~  E2   0.841 0.044 18.930      0    0.754    0.928\n#&gt; 15  E3 ~~  E3   0.841 0.045 18.889      0    0.753    0.928\n#&gt; 16  E4 ~~  E4   0.867 0.042 20.724      0    0.785    0.949\n#&gt; 17  f1 ~~  f1   1.000 0.000     NA     NA    1.000    1.000\n\n\nlavaan::residuals(efa_f1, type = \"cor\") |&gt; print()\n#&gt; $type\n#&gt; [1] \"cor.bollen\"\n#&gt; \n#&gt; $cov\n#&gt;        N1     N2     N3     N4     E1     E2     E3     E4\n#&gt; N1  0.000                                                 \n#&gt; N2  0.025  0.000                                          \n#&gt; N3 -0.011 -0.001  0.000                                   \n#&gt; N4  0.010  0.003  0.027  0.000                            \n#&gt; E1  0.035  0.068  0.014  0.065  0.000                     \n#&gt; E2  0.035  0.056  0.036  0.080  0.500  0.000              \n#&gt; E3  0.055  0.047  0.040  0.052  0.459  0.492  0.000       \n#&gt; E4  0.039  0.053  0.015  0.073  0.374  0.448  0.421  0.000\n\nAdattiamo ai dati il modello a due fattori comuni.\n\nefa_f2 &lt;- cfa(\n    model = f2,\n    sample.cov = psychot_cor_mat,\n    sample.nobs = 250,\n    rotation = \"oblimin\"\n)\n\n\nsemPlot::semPaths(\n    efa_f2,\n    what = \"col\", whatLabels = \"no\", style = \"mx\",\n    layout = \"tree\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 5, sizeMan2 = 4\n)\n\n\n\n\n\n\n\nEsaminiamo la soluzione ottenuta.\n\nsummary(\n    efa_f2,\n    fit.measures = TRUE,\n    standardized = TRUE,\n    rsquare = TRUE\n) |&gt; print()\n#&gt; lavaan 0.6-19 ended normally after 1 iteration\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        25\n#&gt;   Row rank of the constraints matrix                 2\n#&gt; \n#&gt;   Rotation method                      OBLIMIN OBLIQUE\n#&gt;   Oblimin gamma                                      0\n#&gt;   Rotation algorithm (rstarts)                GPA (30)\n#&gt;   Standardized metric                             TRUE\n#&gt;   Row weights                                     None\n#&gt; \n#&gt;   Number of observations                           250\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                 9.811\n#&gt;   Degrees of freedom                                13\n#&gt;   P-value (Chi-square)                           0.709\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                              1253.791\n#&gt;   Degrees of freedom                                28\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    1.000\n#&gt;   Tucker-Lewis Index (TLI)                       1.006\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -2211.879\n#&gt;   Loglikelihood unrestricted model (H1)      -2206.974\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                4469.758\n#&gt;   Bayesian (BIC)                              4550.752\n#&gt;   Sample-size adjusted Bayesian (SABIC)       4477.840\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.000\n#&gt;   90 Percent confidence interval - lower         0.000\n#&gt;   90 Percent confidence interval - upper         0.048\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.957\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.001\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.010\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   f1 =~ efa                                                             \n#&gt;     N1                0.874    0.053   16.592    0.000    0.874    0.876\n#&gt;     N2                0.851    0.055   15.551    0.000    0.851    0.853\n#&gt;     N3                0.826    0.054   15.179    0.000    0.826    0.828\n#&gt;     N4                0.896    0.053   16.802    0.000    0.896    0.898\n#&gt;     E1               -0.046    0.040   -1.138    0.255   -0.046   -0.046\n#&gt;     E2                0.035    0.034    1.030    0.303    0.035    0.035\n#&gt;     E3                0.000    0.040    0.010    0.992    0.000    0.000\n#&gt;     E4               -0.006    0.049   -0.131    0.896   -0.006   -0.006\n#&gt;   f2 =~ efa                                                             \n#&gt;     N1               -0.017    0.032   -0.539    0.590   -0.017   -0.017\n#&gt;     N2                0.011    0.035    0.322    0.748    0.011    0.011\n#&gt;     N3               -0.035    0.036   -0.949    0.343   -0.035   -0.035\n#&gt;     N4                0.031    0.031    0.994    0.320    0.031    0.031\n#&gt;     E1                0.776    0.059   13.125    0.000    0.776    0.778\n#&gt;     E2                0.854    0.058   14.677    0.000    0.854    0.855\n#&gt;     E3                0.785    0.060   13.106    0.000    0.785    0.787\n#&gt;     E4                0.695    0.063   10.955    0.000    0.695    0.697\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   f1 ~~                                                                 \n#&gt;     f2               -0.432    0.059   -7.345    0.000   -0.432   -0.432\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .N1                0.218    0.028    7.790    0.000    0.218    0.219\n#&gt;    .N2                0.279    0.032    8.693    0.000    0.279    0.280\n#&gt;    .N3                0.287    0.032    8.907    0.000    0.287    0.289\n#&gt;    .N4                0.216    0.029    7.578    0.000    0.216    0.217\n#&gt;    .E1                0.361    0.044    8.226    0.000    0.361    0.362\n#&gt;    .E2                0.292    0.043    6.787    0.000    0.292    0.293\n#&gt;    .E3                0.379    0.046    8.315    0.000    0.379    0.381\n#&gt;    .E4                0.509    0.053    9.554    0.000    0.509    0.511\n#&gt;     f1                1.000                               1.000    1.000\n#&gt;     f2                1.000                               1.000    1.000\n#&gt; \n#&gt; R-Square:\n#&gt;                    Estimate\n#&gt;     N1                0.781\n#&gt;     N2                0.720\n#&gt;     N3                0.711\n#&gt;     N4                0.783\n#&gt;     E1                0.638\n#&gt;     E2                0.707\n#&gt;     E3                0.619\n#&gt;     E4                0.489\n\n\nstandardizedSolution(efa_f2) |&gt; print()\n#&gt;    lhs op rhs est.std    se      z pvalue ci.lower ci.upper\n#&gt; 1   f1 =~  N1   0.876 0.024 36.440  0.000    0.829    0.923\n#&gt; 2   f1 =~  N2   0.853 0.027 31.403  0.000    0.800    0.906\n#&gt; 3   f1 =~  N3   0.828 0.028 29.069  0.000    0.772    0.884\n#&gt; 4   f1 =~  N4   0.898 0.023 38.383  0.000    0.852    0.944\n#&gt; 5   f1 =~  E1  -0.046 0.040 -1.139  0.255   -0.125    0.033\n#&gt; 6   f1 =~  E2   0.035 0.034  1.031  0.303   -0.031    0.101\n#&gt; 7   f1 =~  E3   0.000 0.040  0.010  0.992   -0.078    0.079\n#&gt; 8   f1 =~  E4  -0.006 0.049 -0.131  0.896   -0.103    0.090\n#&gt; 9   f2 =~  N1  -0.017 0.032 -0.539  0.590   -0.079    0.045\n#&gt; 10  f2 =~  N2   0.011 0.035  0.322  0.748   -0.058    0.080\n#&gt; 11  f2 =~  N3  -0.035 0.037 -0.949  0.343   -0.106    0.037\n#&gt; 12  f2 =~  N4   0.031 0.031  0.994  0.320   -0.030    0.092\n#&gt; 13  f2 =~  E1   0.778 0.038 20.654  0.000    0.704    0.852\n#&gt; 14  f2 =~  E2   0.855 0.033 26.036  0.000    0.791    0.920\n#&gt; 15  f2 =~  E3   0.787 0.038 20.886  0.000    0.713    0.861\n#&gt; 16  f2 =~  E4   0.697 0.046 15.282  0.000    0.607    0.786\n#&gt; 17  N1 ~~  N1   0.219 0.032  6.905  0.000    0.157    0.281\n#&gt; 18  N2 ~~  N2   0.280 0.036  7.727  0.000    0.209    0.351\n#&gt; 19  N3 ~~  N3   0.289 0.036  7.909  0.000    0.217    0.360\n#&gt; 20  N4 ~~  N4   0.217 0.032  6.751  0.000    0.154    0.280\n#&gt; 21  E1 ~~  E1   0.362 0.047  7.673  0.000    0.269    0.454\n#&gt; 22  E2 ~~  E2   0.293 0.046  6.322  0.000    0.202    0.384\n#&gt; 23  E3 ~~  E3   0.381 0.049  7.816  0.000    0.285    0.476\n#&gt; 24  E4 ~~  E4   0.511 0.053  9.631  0.000    0.407    0.615\n#&gt; 25  f1 ~~  f1   1.000 0.000     NA     NA    1.000    1.000\n#&gt; 26  f2 ~~  f2   1.000 0.000     NA     NA    1.000    1.000\n#&gt; 27  f1 ~~  f2  -0.432 0.059 -7.345  0.000   -0.547   -0.317\n\nAnche se abbiamo introdotto finora soltanto la misura di bontà di adattamento del chi-quadrato, aggiungiamo qui il calcolo di altre misure di bontà di adattamento che discuteremo in seguito.\n\nfit_measures_robust &lt;- c(\n    \"chisq\", \"df\", \"pvalue\", \"cfi\", \"rmsea\", \"srmr\"\n)\n\nConfrontiamo le misure di bontà di adattamento del modello che ipotizza un solo fattore comune e il modello che ipotizza la presenza di due fattori comuni.\n\n# collect them for each model\nrbind(\n    fitmeasures(efa_f1, fit_measures_robust),\n    fitmeasures(efa_f2, fit_measures_robust)\n) |&gt;\n    # wrangle\n    data.frame() |&gt;\n    mutate(\n        chisq = round(chisq, digits = 0),\n        df = as.integer(df),\n        pvalue = ifelse(pvalue == 0, \"&lt; .001\", pvalue)\n    ) |&gt;\n    mutate_at(vars(cfi:srmr), ~ round(., digits = 3)) |&gt;\n    print()\n#&gt;   chisq df            pvalue  cfi rmsea  srmr\n#&gt; 1   375 20            &lt; .001 0.71 0.267 0.187\n#&gt; 2    10 13 0.709310449320098 1.00 0.000 0.010\n\n\nlavaan::residuals(efa_f2, type = \"cor\") |&gt; print()\n#&gt; $type\n#&gt; [1] \"cor.bollen\"\n#&gt; \n#&gt; $cov\n#&gt;        N1     N2     N3     N4     E1     E2     E3     E4\n#&gt; N1  0.000                                                 \n#&gt; N2  0.018  0.000                                          \n#&gt; N3 -0.014 -0.006  0.000                                   \n#&gt; N4 -0.003 -0.013  0.017  0.000                            \n#&gt; E1 -0.003  0.015 -0.012  0.000  0.000                     \n#&gt; E2 -0.009 -0.004  0.006  0.007  0.006  0.000              \n#&gt; E3  0.015 -0.008  0.011 -0.016  0.006 -0.010  0.000       \n#&gt; E4 -0.001  0.000 -0.013  0.009 -0.024  0.006  0.016  0.000\n\nL’evidenza empirica supporta la superiorità del modello a due fattori rispetto a quello ad un solo fattore comune. In particolare, l’analisi fattoriale esplorativa svolta mediante la funzione efa() evidenzia la capacità del modello a due fattori di fornire una descrizione adeguata della struttura dei dati e di distinguere in modo sensato tra i due fattori ipotizzati.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Exploratory Structural Equation Modeling</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/06_efa_lavaan.html#esem-within-cfa",
    "href": "chapters/cfa/06_efa_lavaan.html#esem-within-cfa",
    "title": "36  Exploratory Structural Equation Modeling",
    "section": "\n36.4 ESEM-within-CFA",
    "text": "36.4 ESEM-within-CFA\nUn approccio alternativo per eseguire un modello ESEM è quello proposto da Marsh et al. (2014), noto come ESEM-within-CFA. Questo metodo prevede di eseguire prima la parte esplorativa (EFA) e poi utilizzare i risultati ottenuti come valori iniziali per un modello CFA. In pratica, si combina la flessibilità dell’EFA con il rigore della CFA, sfruttando i vantaggi di entrambi gli approcci.\nPer illustrare questo metodo, seguiremo il tutorial di esemComp e utilizzeremo il dataset di Holzinger e Swineford (1939), disponibile nel pacchetto lavaan di R. Questo dataset contiene i risultati di 301 bambini in test che misurano tre abilità cognitive:\n\n\nAbilità visiva (item x1-x3),\n\n\nAbilità testuale (item x4-x6),\n\n\nAbilità di velocità (item x7-x9).\n\nPer iniziare, carichiamo il dataset e selezioniamo solo le colonne relative agli item di interesse. Questo ci permetterà di concentrarci sulle variabili rilevanti per l’analisi ESEM.\n\n#load full data\nhw_data &lt;- lavaan::HolzingerSwineford1939\n# keep all rows and only the item-columns\nhw_data &lt;- hw_data[, c(7:15)]\n\n#take a look\nhead(hw_data)\n#&gt;      x1   x2    x3    x4   x5     x6    x7   x8    x9\n#&gt; 1 3.333 7.75 0.375 2.333 5.75 1.2857 3.391 5.75 6.361\n#&gt; 2 5.333 5.25 2.125 1.667 3.00 1.2857 3.783 6.25 7.917\n#&gt; 3 4.500 5.25 1.875 1.000 1.75 0.4286 3.261 3.90 4.417\n#&gt; 4 5.333 7.75 3.000 2.667 4.50 2.4286 3.000 5.30 4.861\n#&gt; 5 4.833 4.75 0.875 2.667 4.00 2.5714 3.696 6.30 5.917\n#&gt; 6 5.333 5.00 2.250 1.000 3.00 0.8571 4.348 6.65 7.500\n\n\n36.4.1 Blocchi EFA (Analisi Fattoriale Esplorativa)\nPer eseguire un’Analisi Fattoriale Esplorativa (EFA) con una rotazione target, prima dobbiamo specificare la matrice di rotazione target. La funzione make_target() semplifica questo processo. Per far funzionare questa funzione, dobbiamo indicare la corrispondenza tra i fattori e i loro principali carichi (loadings), ovvero quali elementi ci aspettiamo che abbiano un carico elevato su ciascun fattore.\nQuesta informazione deve essere contenuta in una lista, dove il nome di ogni elemento è il nome del fattore, e il contenuto è un vettore numerico con il numero di colonna degli elementi che si riferiscono a quel fattore.\nSe controlliamo il dataset e i fattori, vediamo che la corrispondenza tra fattore e numero di colonna dell’elemento è piuttosto chiara in questo dataset. Le prime tre colonne si riferiscono agli elementi del primo fattore, le successive tre colonne sono gli elementi del secondo fattore, e così via. Tuttavia, questo potrebbe non essere il caso nel tuo dataset! Molte scale hanno elementi correlati a diversi fattori alternati, il che porta a elementi non sequenziali che si riferiscono allo stesso fattore.\nÈ importante ricordare che il numero dell’elemento per la matrice di rotazione si riferisce sempre alla posizione della colonna dell’elemento all’interno di un dataframe che contiene solo i dati degli elementi (ricorda che all’inizio di questa guida abbiamo creato un dataset separato contenente solo i dati degli elementi). Il numero più basso sarà sempre uno, e il numero più alto sarà il totale degli elementi.\n\n# list with mapping between factors and items\nmain_loadings_list &lt;- list(visual = c(1:3),\n                           textual = c(4:6),\n                           speed = c(7:9))\ntarget_rot &lt;- make_target(nitems = 9, mainloadings = main_loadings_list)\ntarget_rot\n#&gt;       visual textual speed\n#&gt;  [1,]     NA       0     0\n#&gt;  [2,]     NA       0     0\n#&gt;  [3,]     NA       0     0\n#&gt;  [4,]      0      NA     0\n#&gt;  [5,]      0      NA     0\n#&gt;  [6,]      0      NA     0\n#&gt;  [7,]      0       0    NA\n#&gt;  [8,]      0       0    NA\n#&gt;  [9,]      0       0    NA\n\nNella matrice di rotazione target, i valori NA indicano carichi (loadings) che non devono essere avvicinati a zero durante la procedura di rotazione, mentre gli zeri indicano il contrario, ovvero che quei carichi dovrebbero essere avvicinati a zero.\nÈ altresì possibile realizzare facilmente una rotazione target per un modello bifattoriale.\n\nbifactor_target_rot &lt;- make_target(nitems = 9,\n                                  mainloadings = main_loadings_list,\n                                  bifactor = TRUE)\nbifactor_target_rot\n#&gt;       visual textual speed  G\n#&gt;  [1,]     NA       0     0 NA\n#&gt;  [2,]     NA       0     0 NA\n#&gt;  [3,]     NA       0     0 NA\n#&gt;  [4,]      0      NA     0 NA\n#&gt;  [5,]      0      NA     0 NA\n#&gt;  [6,]      0      NA     0 NA\n#&gt;  [7,]      0       0    NA NA\n#&gt;  [8,]      0       0    NA NA\n#&gt;  [9,]      0       0    NA NA\n\nOra, per l’estrazione dei carichi (loadings) utilizzando la funzione esem_efa(), dobbiamo fornire i dati, il numero di fattori da estrarre e la matrice di rotazione target.\n\n# Specify the efa block.\n# Note that if we continued with the bifactor model 'nfactors' would then be specified as 4 and not 3 due to the G factor being added\n\nefa_block &lt;- esem_efa(data = hw_data,\n                      nfactors = 3,\n                      target = target_rot)\n#&gt; Loading required namespace: GPArotation\nefa_block\n#&gt; Factor Analysis using method =  pa\n#&gt; Call: psych::fa(r = data, nfactors = nfactors, rotate = targetAlgorithm, \n#&gt;     fm = fm, Target = target)\n#&gt; Standardized loadings (pattern matrix) based upon correlation matrix\n#&gt;      PA2   PA3   PA1   h2   u2 com\n#&gt; x1  0.18  0.08  0.58 0.48 0.52 1.2\n#&gt; x2  0.03 -0.09  0.52 0.26 0.74 1.1\n#&gt; x3 -0.08  0.08  0.67 0.45 0.55 1.1\n#&gt; x4  0.85  0.01  0.02 0.73 0.27 1.0\n#&gt; x5  0.89  0.00 -0.06 0.75 0.25 1.0\n#&gt; x6  0.80 -0.01  0.08 0.69 0.31 1.0\n#&gt; x7  0.04  0.75 -0.24 0.51 0.49 1.2\n#&gt; x8 -0.05  0.72  0.03 0.52 0.48 1.0\n#&gt; x9  0.01  0.50  0.32 0.46 0.54 1.7\n#&gt; \n#&gt;                        PA2  PA3  PA1\n#&gt; SS loadings           2.22 1.38 1.26\n#&gt; Proportion Var        0.25 0.15 0.14\n#&gt; Cumulative Var        0.25 0.40 0.54\n#&gt; Proportion Explained  0.46 0.28 0.26\n#&gt; Cumulative Proportion 0.46 0.74 1.00\n#&gt; \n#&gt;  With factor correlations of \n#&gt;      PA2  PA3  PA1\n#&gt; PA2 1.00 0.26 0.34\n#&gt; PA3 0.26 1.00 0.31\n#&gt; PA1 0.34 0.31 1.00\n#&gt; \n#&gt; Mean item complexity =  1.1\n#&gt; Test of the hypothesis that 3 factors are sufficient.\n#&gt; \n#&gt; df null model =  36  with the objective function =  3.05 with Chi Square =  904.1\n#&gt; df of  the model are 12  and the objective function was  0.08 \n#&gt; \n#&gt; The root mean square of the residuals (RMSR) is  0.02 \n#&gt; The df corrected root mean square of the residuals is  0.03 \n#&gt; \n#&gt; The harmonic n.obs is  301 with the empirical chi square  7.87  with prob &lt;  0.8 \n#&gt; The total n.obs was  301  with Likelihood Chi Square =  22.54  with prob &lt;  0.032 \n#&gt; \n#&gt; Tucker Lewis Index of factoring reliability =  0.963\n#&gt; RMSEA index =  0.054  and the 90 % confidence intervals are  0.016 0.088\n#&gt; BIC =  -45.95\n#&gt; Fit based upon off diagonal values = 1\n#&gt; Measures of factor score adequacy             \n#&gt;                                                    PA2  PA3  PA1\n#&gt; Correlation of (regression) scores with factors   0.94 0.86 0.84\n#&gt; Multiple R square of scores with factors          0.89 0.74 0.70\n#&gt; Minimum correlation of possible factor scores     0.78 0.48 0.40\n\nLa funzione esem_efa() è in realtà un wrapper intorno alla funzione fa() del pacchetto psych, utilizzata per l’analisi fattoriale esplorativa. Tutti i controlli disponibili in fa() possono essere specificati per una maggiore personalizzazione della procedura di estrazione dei fattori. Assicurati sempre di fornire gli argomenti della funzione originale utilizzando la sintassi nome = valore. Consulta la documentazione di fa() per ulteriori informazioni sui controlli e sui campi presenti nell’oggetto di output.\nPer impostazione predefinita, viene utilizzata una rotazione obliqua come rotazione target. L’utente può scegliere di passare a una rotazione ortogonale impostando il parametro targetAlgorithm su TargetT. Un’altra opzione è quella di eliminare completamente l’utilizzo della rotazione target e optare invece per una rotazione Geomin. In questo caso, basta non specificare il parametro target. Un’ultima possibilità è disponibile per i modelli bifattoriali: in questo caso, basta impostare bifactor = TRUE. Attualmente, i modelli bifattoriali sono supportati solo con la rotazione target.\nIl codice per ciascuno di questi casi è riportato di seguito (commentato).\n\n# geomin rotation\nesem_efa(data = hw_data,\n         nfactors = 3)\n#&gt; Factor Analysis using method =  pa\n#&gt; Call: psych::fa(r = data, nfactors = nfactors, rotate = \"geominQ\", \n#&gt;     fm = fm)\n#&gt; Standardized loadings (pattern matrix) based upon correlation matrix\n#&gt;      PA1   PA3   PA2   h2   u2 com\n#&gt; x1  0.19  0.59  0.03 0.48 0.52 1.2\n#&gt; x2  0.04  0.51 -0.12 0.26 0.74 1.1\n#&gt; x3 -0.07  0.69  0.02 0.45 0.55 1.0\n#&gt; x4  0.84  0.02  0.01 0.73 0.27 1.0\n#&gt; x5  0.88 -0.06  0.01 0.75 0.25 1.0\n#&gt; x6  0.80  0.08 -0.01 0.69 0.31 1.0\n#&gt; x7  0.03 -0.14  0.73 0.51 0.49 1.1\n#&gt; x8 -0.04  0.14  0.69 0.52 0.48 1.1\n#&gt; x9  0.02  0.39  0.45 0.46 0.54 2.0\n#&gt; \n#&gt;                        PA1  PA3  PA2\n#&gt; SS loadings           2.23 1.36 1.27\n#&gt; Proportion Var        0.25 0.15 0.14\n#&gt; Cumulative Var        0.25 0.40 0.54\n#&gt; Proportion Explained  0.46 0.28 0.26\n#&gt; Cumulative Proportion 0.46 0.74 1.00\n#&gt; \n#&gt;  With factor correlations of \n#&gt;      PA1  PA3  PA2\n#&gt; PA1 1.00 0.32 0.22\n#&gt; PA3 0.32 1.00 0.25\n#&gt; PA2 0.22 0.25 1.00\n#&gt; \n#&gt; Mean item complexity =  1.2\n#&gt; Test of the hypothesis that 3 factors are sufficient.\n#&gt; \n#&gt; df null model =  36  with the objective function =  3.05 with Chi Square =  904.1\n#&gt; df of  the model are 12  and the objective function was  0.08 \n#&gt; \n#&gt; The root mean square of the residuals (RMSR) is  0.02 \n#&gt; The df corrected root mean square of the residuals is  0.03 \n#&gt; \n#&gt; The harmonic n.obs is  301 with the empirical chi square  7.87  with prob &lt;  0.8 \n#&gt; The total n.obs was  301  with Likelihood Chi Square =  22.54  with prob &lt;  0.032 \n#&gt; \n#&gt; Tucker Lewis Index of factoring reliability =  0.963\n#&gt; RMSEA index =  0.054  and the 90 % confidence intervals are  0.016 0.088\n#&gt; BIC =  -45.95\n#&gt; Fit based upon off diagonal values = 1\n#&gt; Measures of factor score adequacy             \n#&gt;                                                    PA1  PA3  PA2\n#&gt; Correlation of (regression) scores with factors   0.94 0.84 0.85\n#&gt; Multiple R square of scores with factors          0.89 0.71 0.72\n#&gt; Minimum correlation of possible factor scores     0.78 0.42 0.44\n\n\n# orthogonal target rotation\nesem_efa(data = hw_data,\n         nfactors = 3,\n         target = target_rot,\n         targetAlgorithm = \"TargetT\")\n#&gt; Factor Analysis using method =  pa\n#&gt; Call: psych::fa(r = data, nfactors = nfactors, rotate = targetAlgorithm, \n#&gt;     fm = fm, Target = target)\n#&gt; Standardized loadings (pattern matrix) based upon correlation matrix\n#&gt;     PA2   PA3   PA1   h2   u2 com\n#&gt; x1 0.31  0.18  0.59 0.48 0.52 1.7\n#&gt; x2 0.12 -0.01  0.49 0.26 0.74 1.1\n#&gt; x3 0.07  0.16  0.65 0.45 0.55 1.2\n#&gt; x4 0.84  0.09  0.12 0.73 0.27 1.1\n#&gt; x5 0.86  0.08  0.05 0.75 0.25 1.0\n#&gt; x6 0.81  0.08  0.18 0.69 0.31 1.1\n#&gt; x7 0.10  0.70 -0.12 0.51 0.49 1.1\n#&gt; x8 0.07  0.71  0.13 0.52 0.48 1.1\n#&gt; x9 0.16  0.54  0.38 0.46 0.54 2.0\n#&gt; \n#&gt;                        PA2  PA3  PA1\n#&gt; SS loadings           2.26 1.36 1.24\n#&gt; Proportion Var        0.25 0.15 0.14\n#&gt; Cumulative Var        0.25 0.40 0.54\n#&gt; Proportion Explained  0.47 0.28 0.25\n#&gt; Cumulative Proportion 0.47 0.75 1.00\n#&gt; \n#&gt; Mean item complexity =  1.3\n#&gt; Test of the hypothesis that 3 factors are sufficient.\n#&gt; \n#&gt; df null model =  36  with the objective function =  3.05 with Chi Square =  904.1\n#&gt; df of  the model are 12  and the objective function was  0.08 \n#&gt; \n#&gt; The root mean square of the residuals (RMSR) is  0.02 \n#&gt; The df corrected root mean square of the residuals is  0.03 \n#&gt; \n#&gt; The harmonic n.obs is  301 with the empirical chi square  7.87  with prob &lt;  0.8 \n#&gt; The total n.obs was  301  with Likelihood Chi Square =  22.54  with prob &lt;  0.032 \n#&gt; \n#&gt; Tucker Lewis Index of factoring reliability =  0.963\n#&gt; RMSEA index =  0.054  and the 90 % confidence intervals are  0.016 0.088\n#&gt; BIC =  -45.95\n#&gt; Fit based upon off diagonal values = 1\n#&gt; Measures of factor score adequacy             \n#&gt;                                                    PA2  PA3  PA1\n#&gt; Correlation of (regression) scores with factors   0.94 0.84 0.81\n#&gt; Multiple R square of scores with factors          0.88 0.71 0.65\n#&gt; Minimum correlation of possible factor scores     0.75 0.42 0.30\n\n\n# bifactor model\nesem_efa(data = hw_data,\n         nfactors = 4,\n         target = bifactor_target_rot,\n         SMC=FALSE,\n         bifactor = TRUE)\n#&gt; Factor Analysis using method =  pa\n#&gt; Call: psych::fa(r = data, nfactors = nfactors, rotate = \"TargetT\", \n#&gt;     SMC = FALSE, fm = fm, Target = target)\n#&gt; Standardized loadings (pattern matrix) based upon correlation matrix\n#&gt;     PA4   PA2   PA3   PA1   h2    u2 com\n#&gt; x1 0.74  0.08 -0.02  0.00 0.55 0.449 1.0\n#&gt; x2 0.41  0.01 -0.06  0.90 0.97 0.029 1.4\n#&gt; x3 0.62 -0.12  0.01  0.10 0.41 0.593 1.1\n#&gt; x4 0.40  0.75  0.02 -0.03 0.73 0.273 1.5\n#&gt; x5 0.32  0.81  0.04  0.01 0.76 0.240 1.3\n#&gt; x6 0.42  0.71  0.02  0.02 0.69 0.312 1.6\n#&gt; x7 0.12  0.09  0.67 -0.10 0.48 0.519 1.1\n#&gt; x8 0.30  0.00  0.67  0.01 0.55 0.453 1.4\n#&gt; x9 0.53  0.01  0.42  0.02 0.46 0.543 1.9\n#&gt; \n#&gt;                        PA4  PA2  PA3  PA1\n#&gt; SS loadings           1.92 1.76 1.09 0.82\n#&gt; Proportion Var        0.21 0.20 0.12 0.09\n#&gt; Cumulative Var        0.21 0.41 0.53 0.62\n#&gt; Proportion Explained  0.34 0.31 0.19 0.15\n#&gt; Cumulative Proportion 0.34 0.66 0.85 1.00\n#&gt; \n#&gt; Mean item complexity =  1.4\n#&gt; Test of the hypothesis that 4 factors are sufficient.\n#&gt; \n#&gt; df null model =  36  with the objective function =  3.05 with Chi Square =  904.1\n#&gt; df of  the model are 6  and the objective function was  0.06 \n#&gt; \n#&gt; The root mean square of the residuals (RMSR) is  0.02 \n#&gt; The df corrected root mean square of the residuals is  0.04 \n#&gt; \n#&gt; The harmonic n.obs is  301 with the empirical chi square  5.13  with prob &lt;  0.53 \n#&gt; The total n.obs was  301  with Likelihood Chi Square =  17.95  with prob &lt;  0.0064 \n#&gt; \n#&gt; Tucker Lewis Index of factoring reliability =  0.917\n#&gt; RMSEA index =  0.081  and the 90 % confidence intervals are  0.04 0.126\n#&gt; BIC =  -16.29\n#&gt; Fit based upon off diagonal values = 1\n#&gt; Measures of factor score adequacy             \n#&gt;                                                    PA4  PA2  PA3  PA1\n#&gt; Correlation of (regression) scores with factors   0.85 0.90 0.81 0.95\n#&gt; Multiple R square of scores with factors          0.72 0.82 0.66 0.91\n#&gt; Minimum correlation of possible factor scores     0.45 0.64 0.32 0.81\n\nÈ certo utile poter eseguire queste analisi fattoriali utilizzando l’approccio ESEM (Exploratory Structural Equation Modeling), ma ciò non offre ancora la grande flessibilità ed estensibilità propria dei modelli CFA/SEM (Confirmatory Factor Analysis / Structural Equation Modeling). Per accedere a queste funzionalità, utilizzeremo successivamente l’approccio ESEM-all’interno-del-CFA.\n\n36.4.2 ESEM-all-interno-del-CFA\nUna volta ottenuta un’analisi fattoriale esplorativa (EFA) realizzata con l’approccio ESEM, è sufficiente utilizzare la funzione syntax_composer() per “comporre” il modello ESEM-all-interno-del-CFA utilizzando la sintassi di lavaan. Successivamente, con la sintassi generata, possiamo eseguire il fitting del modello in lavaan.\nLa funzione syntax_composer() richiede come primo argomento una soluzione EFA e come secondo argomento una lista denominata che indica i referenti (indicatori di riferimento) per ciascun fattore. Ogni voce della lista deve avere il formato fattore = \"nome_elemento\". È fondamentale che questa lista rispetti lo stesso ordine in cui i fattori appaiono nella matrice dei carichi fattoriali della soluzione EFA. Di solito, questo ordine non corrisponde a quello utilizzato nella lista per creare la rotazione target, poiché nella matrice EFA i fattori sono ordinati in base alla quantità di varianza spiegata, non in base all’ordine fornito dall’utente.\nPer esempio, controllando i carichi fattoriali, possiamo dedurre che nell’esempio in questione l’ordine nella matrice dei carichi fattoriali è “testuale, velocità, visivo”. Questo ordine non coincide con quello utilizzato in make_target(), dove abbiamo specificato “visivo, testuale, velocità”.\n\nefa_block$loadings\n#&gt; \n#&gt; Loadings:\n#&gt;    PA2    PA3    PA1   \n#&gt; x1  0.179         0.577\n#&gt; x2                0.515\n#&gt; x3                0.670\n#&gt; x4  0.845              \n#&gt; x5  0.886              \n#&gt; x6  0.803              \n#&gt; x7         0.745 -0.240\n#&gt; x8         0.724       \n#&gt; x9         0.504  0.317\n#&gt; \n#&gt;                  PA2   PA3   PA1\n#&gt; SS loadings    2.189 1.354 1.218\n#&gt; Proportion Var 0.243 0.150 0.135\n#&gt; Cumulative Var 0.243 0.394 0.529\n\nQuando esaminiamo la matrice dei carichi fattoriali, possiamo anche scegliere qual è il miglior referente (indicatore di riferimento) per ciascun fattore. Dovrebbe sempre essere un elemento che ha un carico elevato su un fattore e basso sugli altri. Quindi, per il fattore “testuale”, il referente sarà x5, per il fattore “velocità” sarà x8 e per il fattore “visivo” sarà x3. Creeremo la lista con questi elementi in quest’ordine.\n\n# create named character vector of referents\nhw_referents &lt;- list(textual = \"x5\",\n                     speed = \"x8\",\n                     visual = \"x3\")\n\nAlternativamente, è possibile utilizzare la funzione find_referents() per selezionare automaticamente i referenti (indicatori di riferimento). Gli input richiesti sono il risultato della funzione esem_efa() e un vettore di caratteri con i nomi desiderati per i fattori. Ancora una volta, i nomi devono corrispondere all’ordine in cui i fattori appaiono nella soluzione esplorativa.\n\nfind_referents(efa_block, c(\"textual\", \"speed\", \"visual\"))\n#&gt; $textual\n#&gt; [1] \"x5\"\n#&gt; \n#&gt; $speed\n#&gt; [1] \"x7\"\n#&gt; \n#&gt; $visual\n#&gt; [1] \"x3\"\n\nSi dovrebbe notare che i referenti scelti dalla funzione non sono esattamente gli stessi di quelli selezionati manualmente esaminando i carichi fattoriali; il referente per il fattore “velocità” differisce. Ciò accade perché l’attuale implementazione della funzione find_referents() cerca solo l’elemento con il carico più alto per ciascun fattore, senza considerare quanto bene tale elemento carichi su altri fattori.\nInfine, compiliamo la sintassi per lavaan utilizzando la funzione syntax_composer:\n\n# compose lavaan syntax\nmodel_syntax &lt;- syntax_composer(efa_object = efa_block,\n                                referents = hw_referents)\n\n# altenatively, if you plan fit the model with free factor variance parameters\nmodel_syntax_free_var &lt;- syntax_composer(efa_object = efa_block,\n                                referents = hw_referents,\n                                only_fix_crossloadings = FALSE)\n\nwriteLines(model_syntax)\n#&gt; textual =~ start(0.179)*x1+\n#&gt; start(0.03)*x2+\n#&gt; -0.082*x3+\n#&gt; start(0.845)*x4+\n#&gt; start(0.886)*x5+\n#&gt; start(0.803)*x6+\n#&gt; start(0.037)*x7+\n#&gt; -0.049*x8+\n#&gt; start(0.014)*x9 \n#&gt; \n#&gt; speed =~ start(0.081)*x1+\n#&gt; start(-0.085)*x2+\n#&gt; 0.075*x3+\n#&gt; start(0.01)*x4+\n#&gt; 0.003*x5+\n#&gt; start(-0.006)*x6+\n#&gt; start(0.745)*x7+\n#&gt; start(0.724)*x8+\n#&gt; start(0.504)*x9 \n#&gt; \n#&gt; visual =~ start(0.577)*x1+\n#&gt; start(0.515)*x2+\n#&gt; start(0.67)*x3+\n#&gt; start(0.016)*x4+\n#&gt; -0.064*x5+\n#&gt; start(0.08)*x6+\n#&gt; start(-0.24)*x7+\n#&gt; 0.035*x8+\n#&gt; start(0.317)*x9\n\nPossiamo confermare che ogni fattore ha due parametri fissati (i cross-loadings dagli altri fattori) e che tutti gli altri parametri hanno i carichi dell’EFA come punti di partenza.\n\ncfa_fit &lt;- lavaan::cfa(model = model_syntax, data = hw_data, std.lv =TRUE)\nlavaan::summary(cfa_fit, fit.measures = TRUE, std = TRUE)\n#&gt; lavaan 0.6-19 ended normally after 28 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        33\n#&gt; \n#&gt;   Number of observations                           301\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                22.897\n#&gt;   Degrees of freedom                                12\n#&gt;   P-value (Chi-square)                           0.029\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               918.852\n#&gt;   Degrees of freedom                                36\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.988\n#&gt;   Tucker-Lewis Index (TLI)                       0.963\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -3706.541\n#&gt;   Loglikelihood unrestricted model (H1)      -3695.092\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                7479.081\n#&gt;   Bayesian (BIC)                              7601.416\n#&gt;   Sample-size adjusted Bayesian (SABIC)       7496.758\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.055\n#&gt;   90 Percent confidence interval - lower         0.017\n#&gt;   90 Percent confidence interval - upper         0.089\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.365\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.120\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.017\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   textual =~                                                            \n#&gt;     x1                0.218    0.080    2.728    0.006    0.218    0.187\n#&gt;     x2                0.052    0.084    0.611    0.541    0.052    0.044\n#&gt;     x3               -0.082                              -0.082   -0.073\n#&gt;     x4                0.971    0.062   15.657    0.000    0.971    0.836\n#&gt;     x5                1.138    0.063   18.018    0.000    1.138    0.883\n#&gt;     x6                0.878    0.058   15.034    0.000    0.878    0.803\n#&gt;     x7                0.030    0.078    0.391    0.696    0.030    0.028\n#&gt;     x8               -0.049                              -0.049   -0.048\n#&gt;     x9                0.023    0.064    0.359    0.720    0.023    0.023\n#&gt;   speed =~                                                              \n#&gt;     x1                0.080    0.087    0.917    0.359    0.080    0.069\n#&gt;     x2               -0.105    0.093   -1.128    0.259   -0.105   -0.089\n#&gt;     x3                0.075                               0.075    0.066\n#&gt;     x4                0.006    0.062    0.104    0.917    0.006    0.006\n#&gt;     x5                0.003                               0.003    0.002\n#&gt;     x6               -0.008    0.060   -0.139    0.889   -0.008   -0.008\n#&gt;     x7                0.800    0.098    8.140    0.000    0.800    0.736\n#&gt;     x8                0.737    0.069   10.647    0.000    0.737    0.729\n#&gt;     x9                0.503    0.068    7.394    0.000    0.503    0.500\n#&gt;   visual =~                                                             \n#&gt;     x1                0.689    0.088    7.834    0.000    0.689    0.591\n#&gt;     x2                0.597    0.093    6.413    0.000    0.597    0.508\n#&gt;     x3                0.759    0.077    9.829    0.000    0.759    0.672\n#&gt;     x4                0.043    0.067    0.646    0.518    0.043    0.037\n#&gt;     x5               -0.064                              -0.064   -0.050\n#&gt;     x6                0.101    0.063    1.605    0.109    0.101    0.093\n#&gt;     x7               -0.236    0.100   -2.355    0.019   -0.236   -0.217\n#&gt;     x8                0.035                               0.035    0.035\n#&gt;     x9                0.318    0.072    4.384    0.000    0.318    0.315\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   textual ~~                                                            \n#&gt;     speed             0.258    0.087    2.977    0.003    0.258    0.258\n#&gt;     visual            0.303    0.092    3.307    0.001    0.303    0.303\n#&gt;   speed ~~                                                              \n#&gt;     visual            0.307    0.115    2.670    0.008    0.307    0.307\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .x1                0.696    0.087    8.038    0.000    0.696    0.513\n#&gt;    .x2                1.035    0.102   10.151    0.000    1.035    0.749\n#&gt;    .x3                0.692    0.097    7.134    0.000    0.692    0.543\n#&gt;    .x4                0.377    0.048    7.902    0.000    0.377    0.279\n#&gt;    .x5                0.403    0.061    6.590    0.000    0.403    0.243\n#&gt;    .x6                0.365    0.042    8.613    0.000    0.365    0.305\n#&gt;    .x7                0.594    0.106    5.624    0.000    0.594    0.502\n#&gt;    .x8                0.479    0.080    5.958    0.000    0.479    0.469\n#&gt;    .x9                0.551    0.060    9.132    0.000    0.551    0.543\n#&gt;     textual           1.000                               1.000    1.000\n#&gt;     speed             1.000                               1.000    1.000\n#&gt;     visual            1.000                               1.000    1.000\n\nSe hai bisogno di adattare un modello con varianze residue dei fattori libere, dovrai utilizzare la funzione fit_free_factor_var_esem(). Questa funzione è un wrapper intorno alla funzione lavaan(), con gli stessi parametri impostati nella funzione cfa(), eccetto per il fatto che le varianze dei fattori sono libere di essere stimati e i primi indicatori in ciascun fattore non vengono fissati automaticamente. Assumiamo che l’identificazione sia garantita dai referenti fissati nella sintassi del modello, il che dovrebbe essere il caso se hai impostato only_fix_crossloadings = FALSE durante la composizione della sintassi con syntax_composer.\n\ncfa_fit &lt;- fit_free_factor_var_esem(model_syntax_free_var, hw_data)\nlavaan::summary(cfa_fit, fit.measures = TRUE, std = TRUE)\n#&gt; lavaan 0.6-19 ended normally after 45 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        33\n#&gt; \n#&gt;   Number of observations                           301\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                22.897\n#&gt;   Degrees of freedom                                12\n#&gt;   P-value (Chi-square)                           0.029\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               918.852\n#&gt;   Degrees of freedom                                36\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.988\n#&gt;   Tucker-Lewis Index (TLI)                       0.963\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -3706.541\n#&gt;   Loglikelihood unrestricted model (H1)      -3695.092\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                7479.081\n#&gt;   Bayesian (BIC)                              7601.416\n#&gt;   Sample-size adjusted Bayesian (SABIC)       7496.758\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.055\n#&gt;   90 Percent confidence interval - lower         0.017\n#&gt;   90 Percent confidence interval - upper         0.089\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.365\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.120\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.017\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   textual =~                                                            \n#&gt;     x1                0.152    0.064    2.383    0.017    0.196    0.169\n#&gt;     x2                0.028    0.067    0.418    0.676    0.036    0.031\n#&gt;     x3               -0.082                              -0.106   -0.094\n#&gt;     x4                0.754    0.051   14.677    0.000    0.972    0.836\n#&gt;     x5                0.886                               1.143    0.887\n#&gt;     x6                0.680    0.048   14.197    0.000    0.878    0.802\n#&gt;     x7                0.018    0.061    0.293    0.769    0.023    0.021\n#&gt;     x8               -0.049                              -0.063   -0.063\n#&gt;     x9                0.004    0.051    0.073    0.942    0.005    0.005\n#&gt;   speed =~                                                              \n#&gt;     x1                0.080    0.086    0.933    0.351    0.082    0.070\n#&gt;     x2               -0.102    0.092   -1.100    0.271   -0.104   -0.088\n#&gt;     x3                0.075                               0.077    0.068\n#&gt;     x4                0.007    0.061    0.108    0.914    0.007    0.006\n#&gt;     x5                0.003                               0.003    0.002\n#&gt;     x6               -0.008    0.059   -0.134    0.893   -0.008   -0.007\n#&gt;     x7                0.785    0.137    5.739    0.000    0.802    0.737\n#&gt;     x8                0.724                               0.739    0.731\n#&gt;     x9                0.495    0.079    6.281    0.000    0.505    0.502\n#&gt;   visual =~                                                             \n#&gt;     x1                0.606    0.106    5.711    0.000    0.694    0.595\n#&gt;     x2                0.525    0.101    5.210    0.000    0.601    0.511\n#&gt;     x3                0.670                               0.767    0.679\n#&gt;     x4                0.032    0.060    0.527    0.598    0.036    0.031\n#&gt;     x5               -0.064                              -0.073   -0.057\n#&gt;     x6                0.083    0.058    1.442    0.149    0.095    0.087\n#&gt;     x7               -0.204    0.093   -2.196    0.028   -0.233   -0.215\n#&gt;     x8                0.035                               0.040    0.040\n#&gt;     x9                0.283    0.069    4.102    0.000    0.323    0.321\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   textual ~~                                                            \n#&gt;     speed             0.362    0.119    3.029    0.002    0.275    0.275\n#&gt;     visual            0.496    0.147    3.376    0.001    0.336    0.336\n#&gt;   speed ~~                                                              \n#&gt;     visual            0.361    0.133    2.719    0.007    0.309    0.309\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .x1                0.696    0.087    8.038    0.000    0.696    0.513\n#&gt;    .x2                1.035    0.102   10.151    0.000    1.035    0.749\n#&gt;    .x3                0.692    0.097    7.134    0.000    0.692    0.543\n#&gt;    .x4                0.377    0.048    7.902    0.000    0.377    0.279\n#&gt;    .x5                0.403    0.061    6.590    0.000    0.403    0.243\n#&gt;    .x6                0.365    0.042    8.613    0.000    0.365    0.305\n#&gt;    .x7                0.594    0.106    5.624    0.000    0.594    0.502\n#&gt;    .x8                0.479    0.080    5.958    0.000    0.479    0.469\n#&gt;    .x9                0.551    0.060    9.132    0.000    0.551    0.543\n#&gt;     textual           1.663    0.186    8.963    0.000    1.000    1.000\n#&gt;     speed             1.043    0.196    5.319    0.000    1.000    1.000\n#&gt;     visual            1.311    0.267    4.917    0.000    1.000    1.000\n\nUn diagramma di percorso si ottiene con la seguente istruzione:\n\nsemPlot::semPaths(\n    cfa_fit,\n    what = \"col\", whatLabels = \"no\", style = \"mx\",\n    layout = \"tree\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 5, sizeMan2 = 4\n)\n\n\n\n\n\n\n\n\n36.4.3 Omega di McDonald\nÈ possibile calcolare gli omega di McDonald utilizzando il modello adattato e la matrice di rotazione target.\n\nomega_esem(cfa_fit, target_rot)\n#&gt;  visual textual   speed \n#&gt;   0.639   0.885   0.719",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Exploratory Structural Equation Modeling</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/06_efa_lavaan.html#sintassi-esem-in-lavaan",
    "href": "chapters/cfa/06_efa_lavaan.html#sintassi-esem-in-lavaan",
    "title": "36  Exploratory Structural Equation Modeling",
    "section": "\n36.5 Sintassi ESEM in lavaan\n",
    "text": "36.5 Sintassi ESEM in lavaan\n\nSpecifichiamo lo stesso modello descritto in precedenza con la sintassi offerta da lavaan per i modelli ESEM.\n\nmodel &lt;- '\n    # EFA block\n    efa(\"efa1\")*visual + \n    efa(\"efa1\")*textual + \n    efa(\"efa1\")*speed =~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9\n'\n\nAdattiamo il modello ai dati.\n\nfit &lt;- sem(\n    model = model,\n    data = hw_data,\n    rotation = \"geomin\",  \n    rotation.args = list(\n        rstarts = 30,          # Number of random starts for rotation\n        algorithm = \"gpa\",      # Generalized Procrustes Analysis\n        std.ov = TRUE           # Standardize observed variables\n    )\n)\n\nEsaminiamo la soluzione ottenuta.\n\nsummary(fit, standardized = TRUE, fit.measures = TRUE)\n#&gt; lavaan 0.6-19 ended normally after 2 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        39\n#&gt;   Row rank of the constraints matrix                 6\n#&gt; \n#&gt;   Rotation method                       GEOMIN OBLIQUE\n#&gt;   Geomin epsilon                                 0.001\n#&gt;   Rotation algorithm (rstarts)                GPA (30)\n#&gt;   Standardized metric                             TRUE\n#&gt;   Row weights                                     None\n#&gt; \n#&gt;   Number of observations                           301\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                22.897\n#&gt;   Degrees of freedom                                12\n#&gt;   P-value (Chi-square)                           0.029\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               918.852\n#&gt;   Degrees of freedom                                36\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.988\n#&gt;   Tucker-Lewis Index (TLI)                       0.963\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -3706.541\n#&gt;   Loglikelihood unrestricted model (H1)      -3695.092\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                7479.081\n#&gt;   Bayesian (BIC)                              7601.416\n#&gt;   Sample-size adjusted Bayesian (SABIC)       7496.758\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.055\n#&gt;   90 Percent confidence interval - lower         0.017\n#&gt;   90 Percent confidence interval - upper         0.089\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.365\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.120\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.017\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   visual =~ efa1                                                        \n#&gt;     x1                0.712    0.092    7.771    0.000    0.712    0.611\n#&gt;     x2                0.628    0.104    6.063    0.000    0.628    0.534\n#&gt;     x3                0.796    0.096    8.255    0.000    0.796    0.705\n#&gt;     x4                0.011    0.011    0.944    0.345    0.011    0.009\n#&gt;     x5               -0.107    0.089   -1.203    0.229   -0.107   -0.083\n#&gt;     x6                0.076    0.073    1.028    0.304    0.076    0.069\n#&gt;     x7               -0.278    0.109   -2.538    0.011   -0.278   -0.255\n#&gt;     x8                0.012    0.008    1.371    0.170    0.012    0.011\n#&gt;     x9                0.314    0.076    4.142    0.000    0.314    0.312\n#&gt;   textual =~ efa1                                                       \n#&gt;     x1                0.198    0.103    1.917    0.055    0.198    0.170\n#&gt;     x2                0.039    0.092    0.424    0.672    0.039    0.033\n#&gt;     x3               -0.106    0.111   -0.963    0.335   -0.106   -0.094\n#&gt;     x4                0.981    0.058   16.850    0.000    0.981    0.844\n#&gt;     x5                1.153    0.074   15.545    0.000    1.153    0.895\n#&gt;     x6                0.886    0.062   14.338    0.000    0.886    0.810\n#&gt;     x7                0.011    0.012    0.923    0.356    0.011    0.010\n#&gt;     x8               -0.075    0.066   -1.135    0.256   -0.075   -0.074\n#&gt;     x9               -0.002    0.007   -0.315    0.753   -0.002   -0.002\n#&gt;   speed =~ efa1                                                         \n#&gt;     x1                0.015    0.048    0.302    0.762    0.015    0.012\n#&gt;     x2               -0.166    0.092   -1.813    0.070   -0.166   -0.141\n#&gt;     x3                0.002    0.048    0.036    0.971    0.002    0.002\n#&gt;     x4                0.004    0.047    0.091    0.927    0.004    0.004\n#&gt;     x5                0.012    0.036    0.322    0.747    0.012    0.009\n#&gt;     x6               -0.017    0.041   -0.409    0.683   -0.017   -0.015\n#&gt;     x7                0.843    0.105    7.999    0.000    0.843    0.775\n#&gt;     x8                0.752    0.076    9.893    0.000    0.752    0.744\n#&gt;     x9                0.484    0.070    6.954    0.000    0.484    0.481\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   visual ~~                                                             \n#&gt;     textual           0.373    0.118    3.173    0.002    0.373    0.373\n#&gt;     speed             0.432    0.097    4.465    0.000    0.432    0.432\n#&gt;   textual ~~                                                            \n#&gt;     speed             0.306    0.081    3.775    0.000    0.306    0.306\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .x1                0.696    0.087    8.038    0.000    0.696    0.513\n#&gt;    .x2                1.035    0.102   10.151    0.000    1.035    0.749\n#&gt;    .x3                0.692    0.097    7.134    0.000    0.692    0.543\n#&gt;    .x4                0.377    0.048    7.902    0.000    0.377    0.279\n#&gt;    .x5                0.403    0.061    6.590    0.000    0.403    0.243\n#&gt;    .x6                0.365    0.042    8.613    0.000    0.365    0.305\n#&gt;    .x7                0.594    0.106    5.624    0.000    0.594    0.502\n#&gt;    .x8                0.479    0.080    5.958    0.000    0.479    0.469\n#&gt;    .x9                0.551    0.060    9.132    0.000    0.551    0.543\n#&gt;     visual            1.000                               1.000    1.000\n#&gt;     textual           1.000                               1.000    1.000\n#&gt;     speed             1.000                               1.000    1.000\n\nEseguiamo un confronto tra le soluzioni fornite dai due metodi: ESEM specificato in lavaan vs. ESEM-within-CFA.\n\n\n\nIndici di Adattamento del Modello\n\nMetrica\nESEM\nCFA\n\n\n\nCFI\n0.988\n0.988\n\n\nTLI\n0.963\n0.963\n\n\nRMSEA\n0.055 (CI: 0.017–0.089)\n0.055 (CI: 0.017–0.089)\n\n\nSRMR\n0.017\n0.017\n\n\n\n\n\nConclusione: entrambi i modelli mostrano un ottimo adattamento (CFI e TLI &gt; 0.95, RMSEA accettabile).\n\n\n\nLog-Likelihood e Criteri Informativi\n\nMetrica\nESEM\nCFA\n\n\n\nLoglikelihood (H0)\n-3707\n-3707\n\n\nAIC\n7479\n7479\n\n\nBIC\n7601\n7601\n\n\n\n\n\nConclusione: nessuna differenza nei criteri informativi tra i due metodi.\n\n\n\nCaricamenti Fattoriali\n\nItem\nCaricamento_ESEM\nCaricamento_CFA\n\n\n\nx1\n0.712\n0.606\n\n\nx2\n0.628\n0.525\n\n\nx3\n0.796\n0.670\n\n\nx4\n0.011\n0.032\n\n\nx5\n-0.107\n-0.064\n\n\n\n\n\nOsservazione: l’ESEM permette maggior flessibilità nel gestire i caricamenti e le cross-loadings, ottenendo una separazione più chiara dei fattori.\n\n\n\nCovarianze tra i Fattori\n\nCovarianza\nStima_ESEM\nStima_CFA\n\n\n\nVisual ~ Textual\n0.373\n0.496\n\n\nVisual ~ Speed\n0.432\n0.361\n\n\n\n\n\nOsservazione: l’ESEM stima covarianze leggermente diverse, spesso più alte, rispetto alla CFA.\nInterpretazione dei risultati.\n\n\nESEM: offre una maggiore flessibilità e gestione delle cross-loadings, utile quando la struttura fattoriale non è chiara.\n\nCFA: più rigido, adatto quando la struttura dei fattori è ben definita.\n\nIn sintesi, i due metodi producono risultati simili. Tuttavia, il metodo ESEM disponibile nel pacchetto lavaan offre informazioni aggiuntive sulle saturazioni incrociate e sulle relazioni tra i fattori, rendendolo una scelta più solida e completa per analisi di tipo esplorativo.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Exploratory Structural Equation Modeling</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/06_efa_lavaan.html#riflessioni-conclusive",
    "href": "chapters/cfa/06_efa_lavaan.html#riflessioni-conclusive",
    "title": "36  Exploratory Structural Equation Modeling",
    "section": "\n36.6 Riflessioni Conclusive",
    "text": "36.6 Riflessioni Conclusive\nL’ESEM rappresenta un ponte significativo tra i modelli di misurazione tradizionali dell’Exploratory Factor Analysis (EFA) e il più esteso quadro del Confirmatory Factor Analysis/Structural Equation Modeling (CFA/SEM). Grazie a questo, l’ESEM combina i benefici dell’EFA con quelli del CFA/SEM, fornendo un approccio più flessibile e inclusivo nell’analisi dei dati. Tale integrazione ha segnato un progresso notevole nella ricerca statistica, evidenziando l’importanza dell’EFA che precedentemente era sottovalutata.\nL’ESEM e il quadro bifattoriale-ESEM, in particolare, offrono una rappresentazione più fedele e precisa della multidimensionalità dei costrutti psicometrici, che è spesso presente nelle misurazioni. Questo approccio riconosce e gestisce meglio la natura multidimensionale dei costrutti, a differenza dell’approccio tradizionale del CFA, che tende a sovrastimare le correlazioni tra i fattori quando non considera adeguatamente la loro natura gerarchica e interconnessa (Asparouhov et al., 2015; Morin et al., 2020).\nNonostante questi vantaggi, l’ESEM presenta alcune limitazioni che devono essere considerate:\n\n\nComplessità Computazionale: L’ESEM può essere più complesso e richiedere maggiori risorse computazionali rispetto agli approcci tradizionali, soprattutto quando si gestiscono grandi set di dati o modelli con molti fattori.\n\nInterpretazione dei Risultati: A causa della sua flessibilità, l’ESEM può produrre risultati che sono più difficili da interpretare. Ad esempio, la sovrapposizione tra i fattori può complicare l’interpretazione dei costrutti.\n\nRischio di Overfitting: La maggiore flessibilità dell’ESEM può anche portare a un rischio maggiore di overfitting, specialmente in campioni più piccoli o con modelli eccessivamente complessi.\n\nNecessità di Esperienza e Conoscenza: Per utilizzare efficacemente l’ESEM, è richiesta una comprensione approfondita della teoria sottostante e delle tecniche statistiche, che può essere una barriera per alcuni ricercatori.\n\nNonostante queste limitazioni, si prevede che i futuri sviluppi e le applicazioni dell’ESEM conducano a soluzioni più integrate e a un consenso più ampio sulle migliori pratiche nell’utilizzo di questo potente strumento statistico. Nel Capitolo 59 esploreremo il set-ESEM, una recente evoluzione di questa metodologia.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Exploratory Structural Equation Modeling</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/06_efa_lavaan.html#session-info",
    "href": "chapters/cfa/06_efa_lavaan.html#session-info",
    "title": "36  Exploratory Structural Equation Modeling",
    "section": "\n36.7 Session Info",
    "text": "36.7 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.2\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] kableExtra_1.4.0                esemComp_0.2                   \n#&gt;  [3] BifactorIndicesCalculator_0.2.2 ggokabeito_0.1.0               \n#&gt;  [5] see_0.11.0                      MASS_7.3-65                    \n#&gt;  [7] viridis_0.6.5                   viridisLite_0.4.2              \n#&gt;  [9] ggpubr_0.6.0                    ggExtra_0.10.1                 \n#&gt; [11] gridExtra_2.3                   patchwork_1.3.0                \n#&gt; [13] bayesplot_1.11.1                semTools_0.5-6                 \n#&gt; [15] semPlot_1.1.6                   lavaan_0.6-19                  \n#&gt; [17] psych_2.4.12                    scales_1.3.0                   \n#&gt; [19] markdown_1.13                   knitr_1.50                     \n#&gt; [21] lubridate_1.9.4                 forcats_1.0.0                  \n#&gt; [23] stringr_1.5.1                   dplyr_1.1.4                    \n#&gt; [25] purrr_1.0.4                     readr_2.1.5                    \n#&gt; [27] tidyr_1.3.1                     tibble_3.2.1                   \n#&gt; [29] ggplot2_3.5.1                   tidyverse_2.0.0                \n#&gt; [31] here_1.0.1                     \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1    jsonlite_1.9.1       magrittr_2.0.3      \n#&gt;   [4] TH.data_1.1-3        estimability_1.5.1   farver_2.1.2        \n#&gt;   [7] nloptr_2.2.1         rmarkdown_2.29       vctrs_0.6.5         \n#&gt;  [10] minqa_1.2.8          base64enc_0.1-3      rstatix_0.7.2       \n#&gt;  [13] htmltools_0.5.8.1    broom_1.0.7          Formula_1.2-5       \n#&gt;  [16] htmlwidgets_1.6.4    plyr_1.8.9           sandwich_3.1-1      \n#&gt;  [19] emmeans_1.10.7       zoo_1.8-13           igraph_2.1.4        \n#&gt;  [22] mime_0.13            lifecycle_1.0.4      pkgconfig_2.0.3     \n#&gt;  [25] Matrix_1.7-3         R6_2.6.1             fastmap_1.2.0       \n#&gt;  [28] rbibutils_2.3        shiny_1.10.0         numDeriv_2016.8-1.1 \n#&gt;  [31] digest_0.6.37        OpenMx_2.21.13       fdrtool_1.2.18      \n#&gt;  [34] colorspace_2.1-1     rprojroot_2.0.4      Hmisc_5.2-3         \n#&gt;  [37] timechange_0.3.0     abind_1.4-8          compiler_4.4.2      \n#&gt;  [40] withr_3.0.2          glasso_1.11          htmlTable_2.4.3     \n#&gt;  [43] backports_1.5.0      carData_3.0-5        ggsignif_0.6.4      \n#&gt;  [46] GPArotation_2024.3-1 corpcor_1.6.10       gtools_3.9.5        \n#&gt;  [49] tools_4.4.2          pbivnorm_0.6.0       foreign_0.8-88      \n#&gt;  [52] zip_2.3.2            httpuv_1.6.15        nnet_7.3-20         \n#&gt;  [55] glue_1.8.0           quadprog_1.5-8       nlme_3.1-167        \n#&gt;  [58] promises_1.3.2       lisrelToR_0.3        grid_4.4.2          \n#&gt;  [61] checkmate_2.3.2      cluster_2.1.8.1      reshape2_1.4.4      \n#&gt;  [64] generics_0.1.3       gtable_0.3.6         tzdb_0.5.0          \n#&gt;  [67] data.table_1.17.0    hms_1.1.3            xml2_1.3.8          \n#&gt;  [70] car_3.1-3            sem_3.1-16           pillar_1.10.1       \n#&gt;  [73] rockchalk_1.8.157    later_1.4.1          splines_4.4.2       \n#&gt;  [76] lattice_0.22-6       survival_3.8-3       kutils_1.73         \n#&gt;  [79] tidyselect_1.2.1     miniUI_0.1.1.1       pbapply_1.7-2       \n#&gt;  [82] reformulas_0.4.0     svglite_2.1.3        stats4_4.4.2        \n#&gt;  [85] xfun_0.51            qgraph_1.9.8         arm_1.14-4          \n#&gt;  [88] stringi_1.8.4        yaml_2.3.10          pacman_0.5.1        \n#&gt;  [91] boot_1.3-31          evaluate_1.0.3       codetools_0.2-20    \n#&gt;  [94] mi_1.1               cli_3.6.4            RcppParallel_5.1.10 \n#&gt;  [97] rpart_4.1.24         systemfonts_1.2.1    xtable_1.8-4        \n#&gt; [100] Rdpack_2.6.3         munsell_0.5.1        Rcpp_1.0.14         \n#&gt; [103] coda_0.19-4.1        png_0.1-8            XML_3.99-0.18       \n#&gt; [106] parallel_4.4.2       jpeg_0.1-10          lme4_1.1-36         \n#&gt; [109] mvtnorm_1.3-3        openxlsx_4.2.8       rlang_1.1.5         \n#&gt; [112] multcomp_1.4-28      mnormt_2.1.1",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Exploratory Structural Equation Modeling</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/06_efa_lavaan.html#bibliografia",
    "href": "chapters/cfa/06_efa_lavaan.html#bibliografia",
    "title": "36  Exploratory Structural Equation Modeling",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nBrown, T. A. (2015). Confirmatory factor analysis for applied research. Guilford publications.\n\n\nMarsh, H. W., Morin, A. J., Parker, P. D., & Kaur, G. (2014). Exploratory structural equation modeling: An integration of the best features of exploratory and confirmatory factor analysis. Annual Review of Clinical Psychology, 10(1), 85–110.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Exploratory Structural Equation Modeling</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html",
    "href": "chapters/cfa/07_fa_in_r.html",
    "title": "45  Strategia Integrata per un’Analisi Fattoriale",
    "section": "",
    "text": "45.1 Introduzione\nQuesto capitolo propone un tutorial, ispirato al lavoro di Saqr & López-Pernas (2024), su come condurre un’Analisi Fattoriale utilizzando R. Secondo Saqr & López-Pernas (2024), la distinzione tra Analisi Fattoriale Esplorativa (EFA) e Analisi Fattoriale Confermativa (CFA) non è sempre netta. Nella pratica, entrambe le tecniche vengono spesso impiegate all’interno dello stesso studio per ottenere una comprensione più completa dei costrutti latenti.\nIn questa sezione viene presentata una strategia integrata per combinare EFA e CFA, articolata in tre fasi che i ricercatori possono seguire quando i costrutti latenti giocano un ruolo centrale nello studio. Questo approccio è utile sia quando i costrutti latenti sono il fulcro dello strumento in esame, sia quando vengono utilizzati come predittori o esiti nell’analisi.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html#introduzione",
    "href": "chapters/cfa/07_fa_in_r.html#introduzione",
    "title": "45  Strategia Integrata per un’Analisi Fattoriale",
    "section": "",
    "text": "45.1.1 I tre passaggi principali:\n\nEsplorazione della struttura fattoriale\nIdentificare il numero e la natura dei fattori sottostanti attraverso l’EFA, per ottenere un modello iniziale della struttura dei dati.\nCostruzione e valutazione del modello fattoriale\nUtilizzare la CFA per confermare il modello individuato nell’EFA, valutando l’adattamento del modello ai dati raccolti.\nValutazione della generalizzabilità\nVerificare se la struttura fattoriale individuata è replicabile e stabile in campioni diversi o in contesti differenti.\n\nQuesto capitolo assume che il ricercatore abbia già completato una fase preliminare di sviluppo dello strumento, concentrandosi su un costrutto di interesse. Inoltre, si presuppone che i dati utilizzati provengano da un campione rappresentativo della popolazione target.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html#passo-1-esplorazione-della-struttura-fattoriale",
    "href": "chapters/cfa/07_fa_in_r.html#passo-1-esplorazione-della-struttura-fattoriale",
    "title": "45  Strategia Integrata per un’Analisi Fattoriale",
    "section": "\n45.2 Passo 1: Esplorazione della Struttura Fattoriale",
    "text": "45.2 Passo 1: Esplorazione della Struttura Fattoriale\nDopo aver selezionato le variabili di interesse e raccolto i relativi dati, il ricercatore dovrebbe avviare il processo con un’Analisi Fattoriale Esplorativa (EFA). Se si utilizza uno strumento già validato o si dispone di ipotesi solide sulla struttura fattoriale sottostante, l’obiettivo iniziale sarà verificare se il numero di fattori e le saturazioni degli indicatori sui fattori corrispondono ai risultati attesi. In questa fase, alcune domande fondamentali da porsi includono:\n- Le variabili ipotizzate come influenzate da uno stesso fattore caricano effettivamente su un unico fattore?\n- Se si presuppone l’esistenza di un unico fattore sottostante, le variabili mostrano effettivamente carichi elevati su quel fattore?\nNel caso di strumenti nuovi, l’EFA serve a valutare se la struttura fattoriale emergente è interpretabile. In questo caso, è utile chiedersi:\n- Le variabili che saturano principalmente su un fattore condividono effettivamente un contenuto comune?\n- Le variabili che saturano su fattori diversi riflettono differenze qualitative evidenti?\nAd esempio, in un test di matematica, potrebbe emergere che compiti di addizione, sottrazione, divisione e moltiplicazione saturano su quattro fattori distinti, interpretabili rispettivamente come abilità specifiche in ciascuna operazione.\nIn questa fase, potrebbero rendersi necessari aggiustamenti. Per esempio, variabili che non presentano carichi fattoriali sufficientemente elevati (ad esempio inferiori a 0.3) su alcuna dimensione potrebbero essere rimosse, seguite da una nuova esecuzione dell’EFA. Tuttavia, è fondamentale riflettere attentamente sulle ragioni di eventuali carichi fattoriali bassi, che potrebbero dipendere, ad esempio, da una formulazione poco chiara di un item. La rimozione di variabili dovrebbe essere guidata da una motivazione teorica solida, evitando decisioni arbitrarie.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html#passo-2-costruzione-del-modello-fattoriale-e-valutazione-delladattamento",
    "href": "chapters/cfa/07_fa_in_r.html#passo-2-costruzione-del-modello-fattoriale-e-valutazione-delladattamento",
    "title": "45  Strategia Integrata per un’Analisi Fattoriale",
    "section": "\n45.3 Passo 2: Costruzione del Modello Fattoriale e Valutazione dell’Adattamento",
    "text": "45.3 Passo 2: Costruzione del Modello Fattoriale e Valutazione dell’Adattamento\nDopo aver individuato un modello preliminare tramite l’EFA, il passo successivo consiste nel raffinare il modello e applicare la CFA per valutare quanto bene esso si adatti ai dati. Questo significa verificare se le covarianze previste dalla struttura fattoriale corrispondono alle covarianze osservate nel dataset. Nell’EFA, ogni variabile poteva caricare su tutti i fattori, ma con la CFA è possibile limitare i carichi fattoriali sulla base di considerazioni teoriche o empiriche.\nIn questa fase, è importante restringere il modello eliminando i carichi trasversali (cross-loadings) che non sono coerenti con la teoria o che risultano vicini allo zero. I carichi molto bassi possono essere rimossi senza introdurre problemi, ma quelli più alti richiedono una valutazione attenta. Anche se inizialmente sembrano privi di significato, la loro presenza potrebbe suggerire informazioni inattese sui dati. Pertanto, prima di rimuoverli, è fondamentale verificarne la coerenza con la teoria o le ipotesi iniziali. Se, dopo un’analisi approfondita, questi carichi possono essere giustificati teoricamente, è preferibile mantenerli. In caso contrario, si possono eliminare, procedendo poi a valutare l’adattamento del modello modificato ai dati.\nUna volta definite le relazioni tra variabili e fattori, si costruisce il modello CFA e lo si applica al dataset. Se l’adattamento del modello non risulta soddisfacente, è possibile tornare ai risultati dell’EFA per valutare l’inclusione di ulteriori carichi fattoriali o altre modifiche. Tuttavia, qualsiasi aggiunta o cambiamento deve essere giustificato teoricamente, evitando adattamenti puramente empirici.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html#passo-3-valutazione-della-generalizzabilità",
    "href": "chapters/cfa/07_fa_in_r.html#passo-3-valutazione-della-generalizzabilità",
    "title": "45  Strategia Integrata per un’Analisi Fattoriale",
    "section": "\n45.4 Passo 3: Valutazione della Generalizzabilità",
    "text": "45.4 Passo 3: Valutazione della Generalizzabilità\nDopo aver costruito e valutato il modello, l’obiettivo è verificarne la generalizzabilità. Questo passaggio è cruciale per garantire che il modello sia valido non solo per i dati attuali, ma anche per futuri studi sulla stessa popolazione. Tale verifica si effettua tramite la validazione incrociata, che consiste nel testare il modello su un dataset indipendente.\nIdealmente, sarebbe opportuno raccogliere un secondo dataset rappresentativo della stessa popolazione. Tuttavia, nella pratica, questa soluzione è spesso poco realizzabile a causa di limiti di tempo o risorse. Un’alternativa comune è dividere il dataset iniziale in due sottocampioni:\n\n\nCampione di sviluppo: utilizzato per eseguire i Passi 1 (EFA) e 2 (CFA).\n\n\nCampione di validazione: riservato al Passo 3 per testare la generalizzabilità.\n\nSe il modello CFA si adatta bene anche al campione di validazione, si ottiene una maggiore certezza sulla sua applicabilità in futuri studi. Se invece emergono problemi di adattamento, occorre analizzarne le cause, verificare eventuali incoerenze tra teoria e dati, e aggiornare di conseguenza il modello e le ipotesi.\n\n45.4.1 Considerazioni finali\nQuesta strategia, articolata in tre passaggi, rappresenta un approccio sistematico per l’analisi fattoriale in studi che utilizzano strumenti per misurare costrutti latenti. Anche quando si utilizza uno strumento già validato su una popolazione analoga, seguire questa procedura rimane una scelta prudente per evitare possibili distorsioni nei risultati.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html#analisi-fattoriale-in-r",
    "href": "chapters/cfa/07_fa_in_r.html#analisi-fattoriale-in-r",
    "title": "45  Strategia Integrata per un’Analisi Fattoriale",
    "section": "\n45.5 Analisi Fattoriale in R",
    "text": "45.5 Analisi Fattoriale in R\nSaqr & López-Pernas (2024) propone un tutorial dettagliato sui passaggi essenziali per condurre un’Analisi Fattoriale Esplorativa (EFA) e un’Analisi Fattoriale Confermativa (CFA) utilizzando R. Il tutorial affronta i seguenti aspetti chiave:\n\nverifica preliminare delle caratteristiche dei dati per valutarne l’idoneità all’EFA/CFA,\n\nscelta del numero di fattori,\n\nvalutazione dell’adattamento globale e locale del modello,\n\nverifica della generalizzabilità del modello fattoriale finale.\n\n\n45.5.1 Struttura del tutorial\nIl tutorial inizia con la preparazione dei dati: importazione, controllo della loro idoneità per l’analisi fattoriale e suddivisione del dataset per riservare un campione per la validazione incrociata. Successivamente, vengono descritti i passaggi per condurre:\n\nun’EFA per definire una struttura fattoriale preliminare (Passo 1),\n\nuna CFA per affinare e validare il modello (Passo 2),\n\nla verifica della generalizzabilità del modello tramite validazione incrociata (Passo 3).\n\n45.5.2 Preparazione\nIl dataset utilizzato da Saqr & López-Pernas (2024) raccoglie dati di un’indagine sul burnout degli insegnanti in Indonesia, con 876 rispondenti. Le domande sono organizzate in cinque ambiti teorici:\n\n\nConcetto di Sé dell’Insegnante (TSC): 5 item,\n\n\nEfficacia dell’Insegnante (TE): 5 item,\n\n\nEsaurimento Emotivo (EE): 5 item,\n\n\nDepersonalizzazione (DP): 3 item,\n\n\nRiduzione del Senso di Realizzazione Personale (RPA): 7 item.\n\nIn totale, il dataset include 25 variabili, ciascuna valutata su una scala Likert a 5 punti (da 1 = “mai” a 5 = “sempre”). Questa organizzazione rende il dataset ideale per un’analisi fattoriale, consentendo di esplorare la struttura latente delle dimensioni teoriche ipotizzate.\nPrima di procedere con l’EFA e la CFA, è necessario:\n\nverificare la sufficienza del campione (ad esempio, tramite il test di Kaiser-Meyer-Olkin, KMO),\n\ncontrollare la normalità delle distribuzioni o eventuali deviazioni,\n\nsuddividere il dataset in due sottocampioni, uno per lo sviluppo del modello e uno per la validazione.\n\nQuesto approccio organizzato fornisce una base solida per esplorare e confermare la struttura fattoriale, testandone infine la replicabilità su un campione indipendente. La chiarezza dei passaggi rende il tutorial applicabile a una vasta gamma di contesti di ricerca.\nCarichiamo le funzioni di supporto definite da Saqr & López-Pernas (2024):\n\n# Source 'sleasy' functions\nsource(here::here(\"code\", \"sleasy.R\"))\n\nImportiamo i dati:\n\ndataset &lt;- rio::import(\"https://github.com/lamethods/data/raw/main/4_teachersBurnout/2.%20Response.xlsx\")",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html#i-dati-sono-adatti-allanalisi-fattoriale",
    "href": "chapters/cfa/07_fa_in_r.html#i-dati-sono-adatti-allanalisi-fattoriale",
    "title": "45  Strategia Integrata per un’Analisi Fattoriale",
    "section": "\n45.6 I Dati Sono Adatti all’Analisi Fattoriale?",
    "text": "45.6 I Dati Sono Adatti all’Analisi Fattoriale?\nPer condurre un’Analisi Fattoriale Esplorativa (EFA) o una Analisi Fattoriale Confermativa (CFA), è fondamentale assicurarsi che i dati soddisfino determinati requisiti. Di seguito vengono descritti i principali aspetti da considerare.\n\n45.6.1 Variabili continue o categoriche\nIdealmente, le variabili dovrebbero essere continue. Sebbene raramente le variabili siano perfettamente continue, è accettabile trattarle come tali se sono misurate su una scala con almeno cinque categorie di risposta e presentano una distribuzione ragionevolmente simmetrica.\nSe le variabili sono categoriche (ad esempio, binarie o ordinali), è comunque possibile condurre un’analisi fattoriale utilizzando metodi di stima specifici per questo tipo di dati. Inoltre, tutte le variabili dovrebbero preferibilmente essere misurate sulla stessa scala. In caso contrario, oppure se le variabili presentano intervalli di punteggio molto diversi (ad esempio, alcune con valori da 1 a 5 e altre da 2 a 4), è opportuno trasformare le variabili per uniformare le scale prima dell’analisi.\nL’intervallo di ciascuna variabile può essere verificato con il comando seguente:\n\ndescribe(dataset)\n#&gt;      vars   n mean   sd median trimmed  mad min max range  skew kurtosis\n#&gt; TSC1    1 876 3.65 0.68      4    3.62 0.00   1   5     4 -0.09     0.06\n#&gt; TSC2    2 876 3.81 0.64      4    3.78 0.00   2   5     3 -0.07    -0.14\n#&gt; TSC3    3 876 3.73 0.64      4    3.71 0.00   2   5     3 -0.17    -0.02\n#&gt; TSC4    4 876 3.71 0.67      4    3.67 0.00   2   5     3 -0.03    -0.25\n#&gt; TSC5    5 876 3.82 0.65      4    3.79 0.00   2   5     3 -0.10    -0.13\n#&gt; TE1     6 876 4.06 0.71      4    4.10 0.00   1   5     4 -0.47     0.38\n#&gt; TE2     7 876 4.04 0.70      4    4.07 0.00   2   5     3 -0.22    -0.45\n#&gt; TE3     8 876 4.12 0.71      4    4.17 0.00   1   5     4 -0.72     1.60\n#&gt; TE4     9 876 4.11 0.69      4    4.15 0.00   1   5     4 -0.47     0.51\n#&gt; TE5    10 876 3.90 0.75      4    3.92 0.00   1   5     4 -0.41     0.16\n#&gt; EE1    11 876 3.81 0.76      4    3.81 0.00   1   5     4 -0.35     0.23\n#&gt; EE2    12 876 3.73 0.85      4    3.75 1.48   1   5     4 -0.37     0.12\n#&gt; EE3    13 876 3.88 0.83      4    3.91 1.48   1   5     4 -0.31    -0.40\n#&gt; EE4    14 876 3.69 0.80      4    3.67 1.48   1   5     4 -0.03    -0.41\n#&gt; EE5    15 876 3.99 0.81      4    4.03 1.48   1   5     4 -0.43    -0.27\n#&gt; DE1    16 876 3.92 0.68      4    3.93 0.00   1   5     4 -0.53     1.25\n#&gt; DE2    17 876 3.60 0.68      4    3.58 1.48   1   5     4 -0.22     0.64\n#&gt; DE3    18 876 3.82 0.70      4    3.79 0.00   1   5     4 -0.14     0.01\n#&gt; RPA1   19 876 3.93 0.83      4    3.97 1.48   1   5     4 -0.59     0.50\n#&gt; RPA2   20 876 3.94 0.80      4    3.99 0.00   1   5     4 -0.79     1.22\n#&gt; RPA3   21 876 3.88 0.79      4    3.91 0.00   1   5     4 -0.59     0.75\n#&gt; RPA4   22 876 3.87 0.76      4    3.89 0.00   1   5     4 -0.48     0.33\n#&gt; RPA5   23 876 3.84 0.79      4    3.86 0.00   1   5     4 -0.53     0.67\n#&gt;        se\n#&gt; TSC1 0.02\n#&gt; TSC2 0.02\n#&gt; TSC3 0.02\n#&gt; TSC4 0.02\n#&gt; TSC5 0.02\n#&gt; TE1  0.02\n#&gt; TE2  0.02\n#&gt; TE3  0.02\n#&gt; TE4  0.02\n#&gt; TE5  0.03\n#&gt; EE1  0.03\n#&gt; EE2  0.03\n#&gt; EE3  0.03\n#&gt; EE4  0.03\n#&gt; EE5  0.03\n#&gt; DE1  0.02\n#&gt; DE2  0.02\n#&gt; DE3  0.02\n#&gt; RPA1 0.03\n#&gt; RPA2 0.03\n#&gt; RPA3 0.03\n#&gt; RPA4 0.03\n#&gt; RPA5 0.03\n\nNel dataset in esame, le variabili sono misurate su scale Likert a 5 punti con intervalli simili, per cui possono essere trattate come continue senza ulteriori trasformazioni.\n\n45.6.2 Dimensione del campione\nLa dimensione del campione è un aspetto cruciale. Esistono diverse regole empiriche: - Una regola generale suggerisce un campione minimo di 200 osservazioni. - Per modelli semplici (pochi fattori, relazioni forti tra fattori e variabili), campioni più piccoli possono essere sufficienti. Per modelli complessi (molti fattori o relazioni più deboli), è necessario un campione più ampio. - Bentler e Chou raccomandano almeno 5 osservazioni per ogni parametro da stimare, mentre Jackson suggerisce almeno 10, preferibilmente 20 osservazioni per parametro.\nNel dataset di esempio, con 25 variabili che si presume misurino 5 costrutti latenti, i parametri da stimare includono:\n\n\n25 intercetti,\n\n\n25 varianze residue,\n\n\n125 carichi fattoriali (5 fattori × 25 variabili).\n\nIn totale, si devono stimare 175 parametri. La dimensione del campione è verificabile con:\n\nnrow(dataset)\n#&gt; [1] 876\n\nCon 876 osservazioni, il campione è sufficiente secondo Bentler e Chou (5 × 175 = 875) ma non soddisfa il criterio di Jackson per modelli più robusti. Pertanto, non è consigliabile suddividere il dataset per la validazione incrociata. Tuttavia, a scopo didattico, sarà mostrato come creare un campione di riserva.\n\n45.6.3 Correlazioni tra variabili\nUn presupposto fondamentale per l’analisi fattoriale è che le variabili siano correlate. Questo può essere verificato tramite il test di Bartlett, che controlla se la matrice di correlazione è una matrice identità (cioè con elementi fuori diagonale pari a zero). L’ipotesi nulla del test afferma che le variabili non sono correlate. Se l’ipotesi viene rifiutata, è possibile procedere con l’analisi fattoriale. Il comando seguente verifica il p-value del test:\n\nvar_names &lt;- colnames(dataset)\n\n\n(cortest.bartlett(\n    R = cor(dataset[, var_names]), \n    n = nrow(dataset)\n)$p.value) &lt; 0.05\n#&gt; [1] TRUE\n\nNel nostro esempio, il p-value è inferiore a 0,05, indicando che le variabili sono sufficientemente correlate.\n\n45.6.4 Adeguatezza della varianza comune\nUn altro requisito è che le variabili condividano una quantità sufficiente di varianza comune. Questo può essere valutato tramite il test di Kaiser-Meyer-Olkin (KMO), che misura la proporzione di varianza totale attribuibile a varianza comune. Secondo Kaiser, un valore KMO di almeno 0,8 è adeguato, mentre un valore di 0,9 o superiore è eccellente. Per calcolare il valore KMO, si utilizza:\n\nKMO(dataset)\n#&gt; Kaiser-Meyer-Olkin factor adequacy\n#&gt; Call: KMO(r = dataset)\n#&gt; Overall MSA =  0.94\n#&gt; MSA for each item = \n#&gt; TSC1 TSC2 TSC3 TSC4 TSC5  TE1  TE2  TE3  TE4  TE5  EE1  EE2  EE3  EE4  EE5 \n#&gt; 0.96 0.96 0.95 0.94 0.96 0.93 0.96 0.94 0.94 0.96 0.95 0.94 0.95 0.94 0.97 \n#&gt;  DE1  DE2  DE3 RPA1 RPA2 RPA3 RPA4 RPA5 \n#&gt; 0.87 0.86 0.92 0.91 0.91 0.95 0.94 0.96\n\nNel dataset in esame, il valore KMO è pari a 0.94, suggerendo un’eccellente adeguatezza per l’analisi fattoriale.\n\n45.6.5 Normalità e dati mancanti\nLe distribuzioni delle variabili devono essere valutate per verificare la presenza di eventuali deviazioni dalla normalità. Sebbene l’analisi fattoriale possa gestire deviazioni moderate, in caso di non-normalità è necessario utilizzare metodi di stima robusti. La normalità può essere esaminata tramite istogrammi, come nel comando seguente:\n\ndataset |&gt;\n    pivot_longer(2:ncol(dataset),\n        names_to = \"Variable\", values_to = \"Score\"\n    ) |&gt;\n    ggplot(aes(x = Score)) +\n    geom_histogram(bins = 6) +\n    scale_x_continuous(\n        limits = c(0, 6), breaks = c(1, 2, 3, 4, 5)\n    ) +\n    facet_wrap(\"Variable\", ncol = 6, scales = \"free\")\n\n\n\n\n\n\n\nInoltre, è necessario verificare la presenza di dati mancanti. La quantità di valori mancanti per variabile può essere calcolata con:\n\ncolSums(is.na(dataset)) \n#&gt; TSC1 TSC2 TSC3 TSC4 TSC5  TE1  TE2  TE3  TE4  TE5  EE1  EE2  EE3  EE4  EE5 \n#&gt;    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0 \n#&gt;  DE1  DE2  DE3 RPA1 RPA2 RPA3 RPA4 RPA5 \n#&gt;    0    0    0    0    0    0    0    0\n\nSe i dati mancanti sono presenti, è necessario adottare tecniche appropriate per gestirli, come l’imputazione o l’esclusione di osservazioni.\nQuesti controlli preliminari garantiscono che i dati siano adeguati per l’analisi fattoriale e pongono le basi per ottenere risultati affidabili e interpretabili.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html#separare-un-campione-di-riserva",
    "href": "chapters/cfa/07_fa_in_r.html#separare-un-campione-di-riserva",
    "title": "45  Strategia Integrata per un’Analisi Fattoriale",
    "section": "\n45.7 Separare un Campione di Riserva",
    "text": "45.7 Separare un Campione di Riserva\nDopo aver verificato che i dati siano adatti all’analisi fattoriale, è possibile considerare la creazione di un campione di riserva per valutare la generalizzabilità dei risultati. Tuttavia, questa decisione deve tenere conto della dimensione del campione. Come discusso in precedenza, la dimensione minima del campione deve essere almeno 5 volte il numero di parametri da stimare (preferibilmente 10 o 20 volte per modelli più robusti). È importante non suddividere il dataset se il campione disponibile non è sufficientemente ampio da soddisfare i requisiti per entrambe le parti (campione di costruzione e campione di riserva), poiché ciò potrebbe compromettere la qualità del modello. In questi casi, la validazione del modello dovrebbe essere rimandata a studi futuri.\nÈ utile notare che il numero di parametri da stimare in un modello CFA è generalmente inferiore rispetto a un modello EFA. Pertanto, il campione di riserva può essere leggermente più piccolo rispetto a quello utilizzato per costruire il modello.\n\n45.7.1 Considerazioni per il dataset di esempio\nNel nostro esempio, il campione totale di 876 osservazioni non è due volte la dimensione minima richiesta per un modello con 25 variabili e 5 fattori latenti. Tuttavia, a scopo illustrativo, procederemo comunque alla creazione di un campione di riserva. Dividiamo il dataset in due parti uguali:\n- 438 osservazioni per la costruzione del modello (campione di costruzione).\n- 438 osservazioni per la validazione (campione di riserva).\n\n45.7.2 Procedura per la suddivisione\nLa suddivisione avviene in modo casuale attraverso i seguenti passaggi:\n\nImpostazione del seed:\nIl seed viene impostato con set.seed() per garantire che la divisione casuale sia replicabile. Questo è fondamentale per assicurare la coerenza dei risultati.\nCreazione di un vettore di classificazione:\nSi genera un vettore chiamato ind, contenente le etichette “model.building” e “holdout” ripetute 438 volte ciascuna, in ordine casuale. Ogni riga del dataset sarà quindi assegnata a uno dei due gruppi.\nDivisione del dataset:\nUtilizzando la funzione split(), il dataset viene suddiviso in due sottoinsiemi. Le righe vengono assegnate al campione di costruzione o al campione di riserva in base al valore corrispondente nel vettore ind.\nEstrazione dei dataset finali:\nI due nuovi dataset vengono estratti dalla lista creata con split() e memorizzati in due oggetti: model.building e holdout.\n\nEcco il codice per eseguire la suddivisione:\n\n# Imposta il seed per garantire la replicabilità\nset.seed(19)\n\n# Crea il vettore di classificazione\nind &lt;- sample(\n    c(rep(\"model.building\", 438), rep(\"holdout\", 438))\n)\n\n# Suddividi il dataset in base al vettore di classificazione\ntmp &lt;- split(dataset, ind)\n\n# Estrai i due dataset finali\nmodel.building &lt;- tmp$model.building\nholdout &lt;- tmp$holdout\n\n\n45.7.3 Spiegazione dei passaggi\n\nImpostazione del seed:\nLa funzione set.seed(19) garantisce che la suddivisione casuale produca sempre lo stesso risultato, facilitando il controllo e la replicabilità.\nCreazione del vettore ind:\nIl vettore contiene un totale di 876 valori, con 438 assegnati a “model.building” e 438 a “holdout”, in ordine casuale.\nDivisione del dataset:\nLa funzione split() divide il dataset in base ai valori di ind, creando una lista contenente due sottoinsiemi: uno per il modello di costruzione (model.building) e uno per il campione di riserva (holdout).\nEstrazione dei dataset finali:\nI due sottoinsiemi vengono estratti dalla lista tmp e assegnati agli oggetti finali per l’analisi.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html#passo-1-esplorare-la-struttura-fattoriale",
    "href": "chapters/cfa/07_fa_in_r.html#passo-1-esplorare-la-struttura-fattoriale",
    "title": "45  Strategia Integrata per un’Analisi Fattoriale",
    "section": "\n45.8 Passo 1: Esplorare la Struttura Fattoriale",
    "text": "45.8 Passo 1: Esplorare la Struttura Fattoriale\nIl primo passo per esplorare la struttura fattoriale consiste nel determinare il numero di dimensioni sottostanti al costrutto di interesse. Questo processo può essere condotto utilizzando due approcci complementari: l’analisi parallela e il criterio di informazione bayesiano (BIC).\n\nL’analisi parallela fornisce un intervallo plausibile per il numero di dimensioni.\n\nIl BIC aiuta a scegliere il numero specifico di fattori che meglio si adatta ai dati, tenendo conto della parsimonia del modello.\n\n\n45.8.1 Analisi Parallela\nL’analisi parallela è un metodo basato su simulazioni che confronta la varianza spiegata da un certo numero di fattori nei dati reali con la varianza spiegata dagli stessi fattori in dataset simulati (privi di correlazioni tra le variabili, ma con la stessa dimensione e struttura).\nUn fattore viene considerato rilevante se la varianza spiegata nei dati reali supera quella osservata nei dati simulati, indicando che non si tratta di una struttura casuale. Il numero di fattori selezionato è quello in cui i valori osservati nei dati reali superano quelli simulati, fino a un punto in cui non si osserva più questa differenza.\nDettagli tecnici sull’analisi parallela e la sua implementazione sono disponibili nella documentazione della funzione fa.parallel().\n\n45.8.2 Applicazione dell’Analisi Parallela\nPer applicare l’analisi parallela:\n\nSpecificare i dati di costruzione del modello e le colonne corrispondenti alle variabili di interesse.\n\nUtilizzare l’argomento fa = \"fa\" per indicare che si desidera determinare il numero di fattori per l’analisi fattoriale (e non per l’analisi dei componenti principali).\n\nIl risultato include:\n\nUn messaggio che suggerisce il numero plausibile di fattori sottostanti.\nUn grafico che mostra come la varianza spiegata dai dati reali supera quella dei dati simulati fino a un certo numero di fattori.\n\nAd esempio, nel nostro caso, il messaggio indica che sono probabilmente presenti cinque fattori. Nel grafico, si osserva che oltre cinque fattori la varianza spiegata nei dati reali è inferiore a quella dei dati simulati.\nL’analisi parallela è un approccio data-driven: il numero di fattori suggerito è influenzato dal campione analizzato e deve essere considerato un punto di partenza. L’intervallo plausibile può includere più o meno un fattore rispetto a quello suggerito.\n\n45.8.3 Criterio di Informazione Bayesiano (BIC)\nDopo aver determinato un intervallo plausibile di fattori con l’analisi parallela, è necessario scegliere il numero finale utilizzando:\n\n\nL’interpretabilità teorica: valutare se le relazioni tra variabili e fattori sono coerenti con il costrutto di interesse.\n\n\nL’adattamento del modello: confrontare i modelli con diversi numeri di fattori utilizzando il BIC.\n\nIl BIC bilancia l’adattamento del modello ai dati con la semplicità del modello, penalizzando la complessità (cioè l’aggiunta di parametri). Un valore BIC più basso indica un migliore equilibrio tra adattamento e parsimonia.\nAd esempio, se il modello con cinque fattori presenta il BIC più basso, ciò fornisce un supporto per questa soluzione. Tuttavia, la decisione finale dovrebbe integrare il valore del BIC con considerazioni teoriche.\n\n45.8.4 Codice per l’analisi parallela\nDi seguito è riportato il comando per eseguire l’analisi parallela:\n\n# Determinare il numero di fattori con l'analisi parallela\nfa.parallel(x = model.building[, var_names], fa = \"fa\")\n#&gt; Parallel analysis suggests that the number of factors =  5  and the number of components =  NA\n\n\n\n\n\n\n\nQuesto comando genera un grafico e un output testuale, fornendo indicazioni sul numero plausibile di fattori.\nIn sintesi, l’analisi parallela e il BIC sono strumenti potenti e complementari per esplorare la struttura fattoriale. L’analisi parallela suggerisce un intervallo plausibile, mentre il BIC aiuta a identificare la soluzione più parsimoniosa. Integrare questi metodi con considerazioni teoriche è fondamentale per ottenere un modello fattoriale solido e interpretabile.\n\n45.8.5 Analisi Fattoriale Esplorativa\nL’Analisi Fattoriale Esplorativa (EFA) può essere eseguita utilizzando il comando seguente:\n\nEFA &lt;- efa(\n    data = model.building[, var_names],\n    nfactors = 4:6,\n    rotation = \"geomin\", \n    estimator = \"MLR\",\n    meanstructure = TRUE\n)\n\nLa funzione efa() appartiene al pacchetto lavaan e consente di esplorare il numero e la struttura dei fattori latenti nei dati. Di seguito vengono spiegati gli argomenti principali della funzione.\n\n45.8.6 Descrizione degli Argomenti\n\ndata\nSpecifica il dataset su cui eseguire l’EFA. In questo caso, include solo le colonne delle variabili di interesse.\nnfactors\nIndica l’intervallo di numeri di fattori da considerare. Qui, i modelli sono stimati con 4, 5 e 6 fattori.\n\nrotation\nQuesto argomento determina il metodo di rotazione utilizzato per identificare il modello.\n\nLa rotazione è necessaria nell’EFA, poiché, in assenza di restrizioni, esistono infinite soluzioni matematiche equivalenti. Ruotare la matrice dei carichi fattoriali consente di semplificare l’interpretazione, orientando gli assi dei fattori latenti.\n\nIn questo esempio, viene utilizzata la rotazione geomin, che permette ai fattori di essere correlati, una scelta realistica in contesti educativi e psicologici.\n\n\n\nestimator\nSpecifica il metodo di stima.\n\nIl valore predefinito è “ML” (massima verosimiglianza), ma qui viene utilizzato “MLR” (massima verosimiglianza robusta), che gestisce meglio eventuali violazioni della normalità nei dati.\n\n\n\nDati mancanti\n\nSe i dati contengono valori mancanti, è possibile utilizzare l’argomento missing = \"fiml\", che applica il metodo Full Information Maximum Likelihood (FIML). Questo approccio sfrutta tutte le informazioni disponibili ed è appropriato quando i dati mancanti sono MAR (Missing At Random).\n\n\nmeanstructure\nQuando impostato su TRUE, stima anche gli intercetti delle variabili osservate, oltre a varianze e covarianze. Se si utilizza missing = \"fiml\", l’opzione meanstructure è automaticamente attivata.\n\n45.8.7 Interpretazione dei Risultati\nPer identificare il modello migliore, è possibile ordinare i valori del BIC (Criterio di Informazione Bayesiano) in ordine crescente con il comando seguente:\n\nsort(fitMeasures(EFA)[\"bic\", ]) \n#&gt; nfactors = 5 nfactors = 4 nfactors = 6 \n#&gt;        18142        18167        18189\n\nL’output indica che il modello con cinque fattori è quello che ottiene il valore BIC più basso. Questo risultato è in linea sia con l’analisi parallela precedente sia con il numero di fattori atteso in base alla teoria (cinque fattori). Di conseguenza, il modello a cinque fattori è il più adatto per continuare l’analisi.\n\n45.8.8 Estrarre i Carichi Fattoriali\nI carichi fattoriali per il modello a cinque fattori possono essere estratti con il comando seguente:\n\nEFA$nf5\n#&gt; \n#&gt;          f1      f2      f3      f4      f5 \n#&gt; TSC1  0.584*                       *      .*\n#&gt; TSC2  0.487*                       *      .*\n#&gt; TSC3  0.637*                      .*       *\n#&gt; TSC4  0.578*      .*              .*       *\n#&gt; TSC5  0.547*                              . \n#&gt; TE1           0.728*              .         \n#&gt; TE2       .   0.672*                        \n#&gt; TE3           0.708*      .                 \n#&gt; TE4           0.651*              .*        \n#&gt; TE5           0.337*      .*      .*        \n#&gt; EE1               .   0.469*      .         \n#&gt; EE2       .*          0.689*                \n#&gt; EE3                   0.768*                \n#&gt; EE4       .*          0.732*              . \n#&gt; EE5               .   0.479*      .*        \n#&gt; DE1                  -0.353*  0.744*      . \n#&gt; DE2               .*          0.821*        \n#&gt; DE3                       .*  0.755*        \n#&gt; RPA1                                  0.851*\n#&gt; RPA2                                  0.906*\n#&gt; RPA3                                  0.624*\n#&gt; RPA4                      .       .   0.350*\n#&gt; RPA5                      .       .   0.338*\n\nL’output fornisce i carichi standardizzati, che possono essere interpretati come correlazioni tra le variabili osservate e i fattori latenti. Vengono mostrati solo i carichi assoluti superiori a 0.3.\n\n\nOsservazioni sulla struttura fattoriale\nI risultati indicano una struttura semplice, in cui ciascuna variabile carica su un solo fattore, ad eccezione della variabile DE1.\n\n\nDE1 presenta un cross-loading: un carico positivo sul fattore 4 (insieme alle altre variabili DE) e un carico negativo sul fattore 3 (insieme alle variabili EE).\n\nA parte questa eccezione, le variabili TSC, TE, EE, DE e RPA caricano rispettivamente su un unico fattore, confermando la coerenza con il modello teorico.\n\n\n\n45.8.9 Passaggi Successivi\nIl modello può ora essere affinato nella sezione CFA. Poiché la teoria non prevede il cross-loading di DE1, nel modello CFA verrà impostato a zero il carico di questa variabile sul fattore 3. Tuttavia, se il modello CFA non dovesse adattarsi bene, il ripristino di questo cross-loading sarà la prima modifica da considerare.\nQuesta procedura consente di integrare i risultati dell’EFA con la teoria e di preparare il modello per la successiva conferma tramite l’analisi fattoriale confermativa.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html#passo-2-costruire-il-modello-fattoriale-e-valutare-ladattamento",
    "href": "chapters/cfa/07_fa_in_r.html#passo-2-costruire-il-modello-fattoriale-e-valutare-ladattamento",
    "title": "45  Strategia Integrata per un’Analisi Fattoriale",
    "section": "\n45.9 Passo 2: Costruire il Modello Fattoriale e Valutare l’Adattamento",
    "text": "45.9 Passo 2: Costruire il Modello Fattoriale e Valutare l’Adattamento\nIl primo passo per costruire il modello fattoriale è definirlo utilizzando la sintassi di lavaan. Nel modello seguente, vengono specificati i 5 fattori (TSC, TE, EE, DE e RPA) in base alle variabili osservate identificate dall’EFA. Si includono inoltre le correlazioni tra fattori, come indicato dalla teoria e dai risultati precedenti. Gli intercetti non sono esplicitamente definiti, ma possono essere stimati impostando l’argomento meanstructure = TRUE.\n\nCFA_model &lt;- \"\n    # Relazioni tra variabili osservate e fattori\n    TSC =~ TSC1 + TSC2 + TSC3 + TSC5\n    TE =~ TE1 + TE2 + TE3 + TE5\n    EE =~ EE1 + EE2 + EE3 + EE4\n    DE =~ DE1 + DE2 + DE3\n    RPA =~ RPA1 + RPA2 + RPA3 + RPA4\n    # Correlazioni tra fattori\n    TSC ~~ TE + EE + DE + RPA\n    TE ~~ EE + DE + RPA\n    EE ~~ DE + RPA\n    DE ~~ RPA\n\"\n\nIl modello viene stimato con il comando seguente:\n\nCFA &lt;- cfa(\n    model = CFA_model, \n    data = model.building[, var_names],\n    estimator = \"MLR\", \n    std.lv = TRUE, \n    meanstructure = TRUE\n)\n\n\n\nestimator = \"MLR\": utilizza la massima verosimiglianza robusta, che gestisce eventuali deviazioni dalla normalità.\n\nstd.lv = TRUE: standardizza i fattori latenti, rendendo i carichi interpretabili come correlazioni.\n\nmeanstructure = TRUE: stima anche gli intercetti delle variabili osservate.\n\nL’adattamento del modello si valuta attraverso due livelli: adattamento globale e adattamento locale.\n\n45.9.1 Adattamento Globale\nL’adattamento globale verifica quanto bene l’intero modello rappresenti i dati. Le principali misure da considerare sono:\n\n\nTest Chi-quadro: verifica se il modello riproduce perfettamente le relazioni osservate. È sensibile alla dimensione del campione e tende a rifiutare l’adattamento perfetto con campioni ampi.\n\nIndice di adattamento comparativo (CFI): valuta l’adattamento relativo del modello rispetto a un modello nullo (senza correlazioni tra le variabili).\n\nErrore quadratico medio di approssimazione (RMSEA): quantifica l’adattamento approssimativo, penalizzando la complessità del modello.\n\nResiduo quadratico medio standardizzato (SRMR): rappresenta la discrepanza media tra la matrice di covarianza campionaria e quella del modello.\n\nLinee guida per l’interpretazione:\n\n\nChi-quadro: non significativo è preferibile, ma può essere ignorato in campioni ampi.\n\n\nCFI: &gt; 0.90 indica un buon adattamento; &gt; 0.95 è eccellente.\n\n\nRMSEA: &lt; 0.05 è ottimale; &lt; 0.08 è accettabile.\n\n\nSRMR: &lt; 0.08 è raccomandato.\n\nPuoi calcolare queste misure con il comando:\n\nglobalFit(CFA)\n#&gt; Results------------------------------------------------------------------------ \n#&gt;  \n#&gt; Chi-Square (142) = 318.8 with p-value\n#&gt;           = 1.332e-15\n#&gt; \n#&gt; CFI = 0.9477\n#&gt; \n#&gt; RMSEA = 0.05332; lower bound = 0.0459;\n#&gt;       upper bound = 0.06077\n#&gt; \n#&gt; SRMR = 0.04354\n#&gt; \n#&gt; Interpretations--------------------------------------------------------------- \n#&gt;  \n#&gt; The hypothesis of perfect fit *is* rejected according to the Chi-\n#&gt;           Square test statistics because the p-value is smaller than 0.05 \n#&gt;  \n#&gt; The hypothesis of approximate model fit *is not* rejected according\n#&gt;           to the CFI because the value is larger than 0.9. \n#&gt;  \n#&gt; The hypothesis of approximate model fit *is* rejected according\n#&gt;          to the RMSEA because the point estimate is larger or equal to\n#&gt;          0.05. \n#&gt;  \n#&gt; The hypothesis of approximate model fit *is not* rejected according\n#&gt;          to the SRMR because the value is smaller than 0.08. \n#&gt; \n\n\nIl test Chi-quadro rifiuta l’adattamento perfetto, ma le altre misure (CFI, RMSEA, SRMR) indicano un buon adattamento approssimativo.\n\nPoiché almeno tre misure supportano il modello, è possibile procedere senza ulteriori modifiche.\n\n45.9.2 Adattamento Locale\nL’adattamento locale verifica se ogni parte del modello si adatta bene ai dati. Ciò si ottiene confrontando le differenze assolute tra la matrice di covarianza campionaria e quella implicata dal modello. Questo consente di identificare problemi specifici, come variabili mal rappresentate.\nIl comando seguente calcola queste differenze per ogni coppia di variabili:\n\nlocalFit(CFA) \n#&gt; $local_misfit\n#&gt;       TSC1  TSC2  TSC3  TSC5   TE1   TE2   TE3   TE5   EE1   EE2   EE3\n#&gt; TSC1 0.000                                                            \n#&gt; TSC2 0.012 0.000                                                      \n#&gt; TSC3 0.007 0.012 0.000                                                \n#&gt; TSC5 0.007 0.002 0.010 0.000                                          \n#&gt; TE1  0.019 0.000 0.009 0.010 0.000                                    \n#&gt; TE2  0.025 0.014 0.031 0.021 0.011 0.000                              \n#&gt; TE3  0.013 0.010 0.048 0.005 0.003 0.008 0.000                        \n#&gt; TE5  0.025 0.028 0.032 0.022 0.012 0.026 0.005 0.000                  \n#&gt; EE1  0.013 0.010 0.004 0.016 0.042 0.044 0.001 0.072 0.000            \n#&gt; EE2  0.004 0.009 0.025 0.003 0.029 0.050 0.027 0.043 0.002 0.000      \n#&gt; EE3  0.013 0.015 0.039 0.013 0.021 0.042 0.006 0.081 0.012 0.001 0.000\n#&gt; EE4  0.002 0.002 0.000 0.013 0.042 0.021 0.006 0.039 0.017 0.017 0.010\n#&gt; DE1  0.011 0.019 0.015 0.002 0.010 0.026 0.011 0.036 0.010 0.048 0.042\n#&gt; DE2  0.014 0.018 0.030 0.011 0.008 0.025 0.032 0.059 0.058 0.031 0.012\n#&gt; DE3  0.000 0.008 0.041 0.021 0.023 0.006 0.012 0.019 0.048 0.015 0.022\n#&gt; RPA1 0.008 0.015 0.034 0.011 0.013 0.022 0.001 0.012 0.011 0.018 0.019\n#&gt; RPA2 0.006 0.008 0.044 0.007 0.021 0.004 0.009 0.008 0.015 0.016 0.002\n#&gt; RPA3 0.041 0.016 0.012 0.003 0.006 0.010 0.017 0.034 0.035 0.008 0.022\n#&gt; RPA4 0.020 0.000 0.003 0.031 0.001 0.027 0.031 0.039 0.042 0.035 0.031\n#&gt;        EE4   DE1   DE2   DE3  RPA1  RPA2  RPA3  RPA4\n#&gt; TSC1                                                \n#&gt; TSC2                                                \n#&gt; TSC3                                                \n#&gt; TSC5                                                \n#&gt; TE1                                                 \n#&gt; TE2                                                 \n#&gt; TE3                                                 \n#&gt; TE5                                                 \n#&gt; EE1                                                 \n#&gt; EE2                                                 \n#&gt; EE3                                                 \n#&gt; EE4  0.000                                          \n#&gt; DE1  0.040 0.000                                    \n#&gt; DE2  0.052 0.004 0.000                              \n#&gt; DE3  0.012 0.002 0.006 0.000                        \n#&gt; RPA1 0.041 0.008 0.006 0.002 0.000                  \n#&gt; RPA2 0.053 0.010 0.025 0.024 0.024 0.000            \n#&gt; RPA3 0.009 0.002 0.016 0.021 0.009 0.017 0.000      \n#&gt; RPA4 0.053 0.006 0.056 0.074 0.046 0.011 0.052 0.000\n#&gt; \n#&gt; $max_misfit\n#&gt; [1] 0.08052\n\nL’output mostra che la differenza massima tra le due matrici è 0,08, un valore trascurabile rispetto alla scala delle variabili. Non emergono problemi locali degni di nota.\n\n45.9.3 Affinare il Modello\nSe fossero emersi problemi di adattamento locale, si potrebbero apportare modifiche mirate, come aggiungere covarianze tra variabili. Tuttavia, ogni modifica dovrebbe avere una solida giustificazione teorica. Non introdurre parametri aggiuntivi solo per migliorare l’adattamento!\nNel caso specifico, poiché il modello attuale non presenta problemi di adattamento, si può proseguire con la valutazione dei carichi fattoriali.\n\n45.9.4 Esaminare i Carichi Fattoriali\nI carichi fattoriali standardizzati possono essere visualizzati con il seguente comando:\n\ninspect(object = CFA, what = \"std\")$lambda\n#&gt;        TSC    TE    EE    DE   RPA\n#&gt; TSC1 0.657 0.000 0.000 0.000 0.000\n#&gt; TSC2 0.692 0.000 0.000 0.000 0.000\n#&gt; TSC3 0.628 0.000 0.000 0.000 0.000\n#&gt; TSC5 0.726 0.000 0.000 0.000 0.000\n#&gt; TE1  0.000 0.789 0.000 0.000 0.000\n#&gt; TE2  0.000 0.745 0.000 0.000 0.000\n#&gt; TE3  0.000 0.788 0.000 0.000 0.000\n#&gt; TE5  0.000 0.649 0.000 0.000 0.000\n#&gt; EE1  0.000 0.000 0.739 0.000 0.000\n#&gt; EE2  0.000 0.000 0.802 0.000 0.000\n#&gt; EE3  0.000 0.000 0.786 0.000 0.000\n#&gt; EE4  0.000 0.000 0.760 0.000 0.000\n#&gt; DE1  0.000 0.000 0.000 0.665 0.000\n#&gt; DE2  0.000 0.000 0.000 0.640 0.000\n#&gt; DE3  0.000 0.000 0.000 0.738 0.000\n#&gt; RPA1 0.000 0.000 0.000 0.000 0.849\n#&gt; RPA2 0.000 0.000 0.000 0.000 0.854\n#&gt; RPA3 0.000 0.000 0.000 0.000 0.788\n#&gt; RPA4 0.000 0.000 0.000 0.000 0.587\n\nQuesti valori indicano la forza delle relazioni tra variabili osservate e fattori latenti. Carichi superiori a 0.3 (in valore assoluto) sono generalmente considerati rilevanti.\nIn sintesi, in questo passaggio, è stato costruito e valutato un modello CFA basato su teoria e risultati dell’EFA. Il modello presenta un buon adattamento sia globale sia locale, supportando la sua validità per rappresentare i dati. Il prossimo passo sarà interpretare e utilizzare i risultati del modello per ulteriori analisi o decisioni teoriche.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html#passo-3-valutare-la-generalizzabilità",
    "href": "chapters/cfa/07_fa_in_r.html#passo-3-valutare-la-generalizzabilità",
    "title": "45  Strategia Integrata per un’Analisi Fattoriale",
    "section": "\n45.10 Passo 3: Valutare la Generalizzabilità",
    "text": "45.10 Passo 3: Valutare la Generalizzabilità\nL’ultimo passo consiste nel valutare la generalizzabilità del modello CFA definito nel Passo 2, adattandolo al campione di riserva. Questo consente di verificare se il modello è applicabile a dati indipendenti, aumentando la fiducia nella sua capacità di rappresentare in modo affidabile la struttura sottostante del costrutto in studi futuri e in campioni diversi.\n\n45.10.1 Applicazione del Modello al Campione di Riserva\nIl modello viene applicato al campione di riserva utilizzando lo stesso codice del Passo 2, ma specificando il dataset di riserva nell’argomento data:\n\nCFA_holdout &lt;- cfa(\n    model = CFA_model, \n    data = holdout[, var_names],\n    estimator = \"MLR\", \n    std.lv = TRUE, \n    meanstructure = TRUE\n)\n\nCome nel Passo 2, le misure di adattamento globale possono essere calcolate con:\n\nglobalFit(CFA_holdout)\n#&gt; Results------------------------------------------------------------------------ \n#&gt;  \n#&gt; Chi-Square (142) = 339.8 with p-value\n#&gt;           = 0\n#&gt; \n#&gt; CFI = 0.943\n#&gt; \n#&gt; RMSEA = 0.05639; lower bound = 0.04899;\n#&gt;       upper bound = 0.06384\n#&gt; \n#&gt; SRMR = 0.04162\n#&gt; \n#&gt; Interpretations--------------------------------------------------------------- \n#&gt;  \n#&gt; The hypothesis of perfect fit *is* rejected according to the Chi-\n#&gt;           Square test statistics because the p-value is smaller than 0.05 \n#&gt;  \n#&gt; The hypothesis of approximate model fit *is not* rejected according\n#&gt;           to the CFI because the value is larger than 0.9. \n#&gt;  \n#&gt; The hypothesis of approximate model fit *is* rejected according\n#&gt;          to the RMSEA because the point estimate is larger or equal to\n#&gt;          0.05. \n#&gt;  \n#&gt; The hypothesis of approximate model fit *is not* rejected according\n#&gt;          to the SRMR because the value is smaller than 0.08. \n#&gt; \n\nDall’output, emerge che:\n\nIl test Chi-quadro rifiuta l’adattamento perfetto (un risultato atteso con campioni ampi).\n\nLe misure di adattamento approssimativo, come CFI e SRMR, confermano un buon adattamento anche nel campione di riserva.\n\nLe stime del RMSEA rientrano nei valori accettabili, indicando che il modello si adatta sufficientemente bene ai dati di riserva.\n\nQuesti risultati sono comparabili a quelli ottenuti con il campione di costruzione, suggerendo che il modello presenta una generalizzabilità adeguata.\nPer verificare l’adattamento locale, utilizziamo il comando seguente, come fatto nel Passo 2:\n\nlocalFit(CFA_holdout) \n#&gt; $local_misfit\n#&gt;       TSC1  TSC2  TSC3  TSC5   TE1   TE2   TE3   TE5   EE1   EE2   EE3\n#&gt; TSC1 0.000                                                            \n#&gt; TSC2 0.010 0.000                                                      \n#&gt; TSC3 0.012 0.015 0.000                                                \n#&gt; TSC5 0.007 0.007 0.005 0.000                                          \n#&gt; TE1  0.023 0.023 0.003 0.014 0.000                                    \n#&gt; TE2  0.027 0.012 0.019 0.008 0.008 0.000                              \n#&gt; TE3  0.012 0.010 0.024 0.008 0.012 0.002 0.000                        \n#&gt; TE5  0.019 0.014 0.008 0.002 0.046 0.006 0.013 0.000                  \n#&gt; EE1  0.037 0.023 0.009 0.005 0.035 0.009 0.002 0.011 0.000            \n#&gt; EE2  0.028 0.003 0.003 0.019 0.038 0.016 0.069 0.008 0.033 0.000      \n#&gt; EE3  0.032 0.047 0.012 0.017 0.024 0.017 0.004 0.071 0.026 0.019 0.000\n#&gt; EE4  0.006 0.033 0.003 0.002 0.027 0.002 0.002 0.048 0.015 0.020 0.004\n#&gt; DE1  0.056 0.005 0.007 0.007 0.005 0.020 0.003 0.032 0.037 0.072 0.020\n#&gt; DE2  0.005 0.029 0.032 0.061 0.012 0.014 0.046 0.006 0.024 0.038 0.034\n#&gt; DE3  0.012 0.019 0.022 0.002 0.034 0.016 0.014 0.005 0.057 0.032 0.050\n#&gt; RPA1 0.019 0.009 0.012 0.028 0.018 0.001 0.003 0.004 0.030 0.031 0.037\n#&gt; RPA2 0.009 0.020 0.023 0.001 0.017 0.016 0.018 0.004 0.003 0.045 0.008\n#&gt; RPA3 0.000 0.007 0.004 0.009 0.000 0.006 0.000 0.028 0.011 0.021 0.025\n#&gt; RPA4 0.018 0.015 0.006 0.014 0.021 0.019 0.040 0.036 0.049 0.023 0.046\n#&gt;        EE4   DE1   DE2   DE3  RPA1  RPA2  RPA3  RPA4\n#&gt; TSC1                                                \n#&gt; TSC2                                                \n#&gt; TSC3                                                \n#&gt; TSC5                                                \n#&gt; TE1                                                 \n#&gt; TE2                                                 \n#&gt; TE3                                                 \n#&gt; TE5                                                 \n#&gt; EE1                                                 \n#&gt; EE2                                                 \n#&gt; EE3                                                 \n#&gt; EE4  0.000                                          \n#&gt; DE1  0.036 0.000                                    \n#&gt; DE2  0.018 0.020 0.000                              \n#&gt; DE3  0.020 0.019 0.006 0.000                        \n#&gt; RPA1 0.003 0.014 0.029 0.004 0.000                  \n#&gt; RPA2 0.051 0.006 0.010 0.005 0.020 0.000            \n#&gt; RPA3 0.002 0.030 0.006 0.026 0.016 0.017 0.000      \n#&gt; RPA4 0.023 0.008 0.028 0.046 0.057 0.005 0.093 0.000\n#&gt; \n#&gt; $max_misfit\n#&gt; [1] 0.0931\n\nL’analisi mostra che la differenza massima tra la matrice di covarianza campionaria e quella del modello è pari a 0,09, un valore contenuto rispetto alla scala delle variabili osservate. Questo suggerisce che l’adattamento locale è soddisfacente anche per il campione di riserva, in linea con quanto osservato nel campione di costruzione.\n\n45.10.2 Confronto dei Carichi Fattoriali\nI carichi fattoriali del modello adattato al campione di riserva possono essere esaminati con:\n\ninspect(object = CFA_holdout, what = \"std\")$lambda \n#&gt;        TSC    TE    EE    DE   RPA\n#&gt; TSC1 0.679 0.000 0.000 0.000 0.000\n#&gt; TSC2 0.689 0.000 0.000 0.000 0.000\n#&gt; TSC3 0.691 0.000 0.000 0.000 0.000\n#&gt; TSC5 0.702 0.000 0.000 0.000 0.000\n#&gt; TE1  0.000 0.694 0.000 0.000 0.000\n#&gt; TE2  0.000 0.772 0.000 0.000 0.000\n#&gt; TE3  0.000 0.819 0.000 0.000 0.000\n#&gt; TE5  0.000 0.677 0.000 0.000 0.000\n#&gt; EE1  0.000 0.000 0.749 0.000 0.000\n#&gt; EE2  0.000 0.000 0.794 0.000 0.000\n#&gt; EE3  0.000 0.000 0.781 0.000 0.000\n#&gt; EE4  0.000 0.000 0.801 0.000 0.000\n#&gt; DE1  0.000 0.000 0.000 0.677 0.000\n#&gt; DE2  0.000 0.000 0.000 0.659 0.000\n#&gt; DE3  0.000 0.000 0.000 0.766 0.000\n#&gt; RPA1 0.000 0.000 0.000 0.000 0.851\n#&gt; RPA2 0.000 0.000 0.000 0.000 0.867\n#&gt; RPA3 0.000 0.000 0.000 0.000 0.700\n#&gt; RPA4 0.000 0.000 0.000 0.000 0.618\n\nI carichi fattoriali nel campione di riserva sono molto simili a quelli stimati nel campione di costruzione, confermando la stabilità della struttura fattoriale. Le differenze tra i due dataset sono trascurabili, supportando ulteriormente la generalizzabilità del modello.\nIn conclusione, il modello CFA si adatta bene sia al campione di costruzione sia a quello di riserva, con risultati simili in termini di misure di adattamento globale e locale, oltre che di carichi fattoriali. Questo supporta la conclusione che il modello ha una buona generalizzabilità e rappresenta in modo affidabile la struttura sottostante del costrutto analizzato.\nTuttavia, se il modello non si fosse adattato adeguatamente al campione di riserva (ad esempio, con misure di adattamento insufficienti o carichi fattoriali significativamente diversi), sarebbe stato necessario rivedere la struttura fattoriale. In tal caso:\n\nSi dovrebbero identificare le aree problematiche, come indicato dall’analisi dell’adattamento locale.\n\nEventuali modifiche al modello dovrebbero essere teoricamente giustificate.\nUna nuova raccolta di dati sarebbe necessaria per ripetere i tre passaggi, suddividendo i dati in un nuovo campione di costruzione e uno di riserva.\n\n45.10.3 Nota Pratica\nQuesto esempio dimostra l’importanza di suddividere i dati per testare la generalizzabilità. Tuttavia, nella pratica, raccogliere nuovi dati può essere complesso. Pertanto, è fondamentale progettare lo studio con un campione sufficientemente grande da consentire una suddivisione adeguata fin dall’inizio.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html#riflessioni-conclusive",
    "href": "chapters/cfa/07_fa_in_r.html#riflessioni-conclusive",
    "title": "45  Strategia Integrata per un’Analisi Fattoriale",
    "section": "\n45.11 Riflessioni conclusive",
    "text": "45.11 Riflessioni conclusive\nL’analisi fattoriale rappresenta uno strumento cruciale per lo studio e la comprensione di costrutti non direttamente osservabili, offrendo una metodologia rigorosa per identificare e validare strutture latenti nei dati. Questo approccio è particolarmente utile in campi come la psicologia, le scienze sociali, e molte altre discipline in cui i fenomeni di interesse non possono essere misurati direttamente, ma devono essere dedotti attraverso indicatori osservabili.\nLa versatilità dell’analisi fattoriale risiede nella sua capacità di sintetizzare informazioni complesse e di ridurre grandi insiemi di variabili a un numero limitato di fattori interpretabili. Questa caratteristica la rende particolarmente adatta per:\n\n\nCostruzione di strumenti di misura: Identificare e validare le dimensioni sottostanti a questionari e scale psicometriche.\n\nValidazione teorica: Verificare l’esistenza di costrutti teorici definiti o esplorarne di nuovi.\n\nComparazione tra gruppi: Esaminare se i costrutti latenti si manifestano allo stesso modo in diverse popolazioni o contesti.\n\nL’analisi fattoriale si estende oltre la semplice identificazione di fattori, includendo applicazioni avanzate come l’analisi fattoriale multigruppo, che consente di confrontare modelli in sottogruppi della popolazione, verificando l’invarianza di misura. Questo aspetto, fondamentale per garantire la comparabilità delle misurazioni, sarà discusso nel dettaglio nel capitolo successivo.\nNonostante i suoi punti di forza, l’analisi fattoriale presenta alcune sfide che richiedono attenzione:\n\n\nScelta del metodo appropriato: Decidere tra EFA e CFA dipende dal livello di conoscenza teorica del costrutto.\n\nAdeguatezza dei dati: La qualità dell’analisi dipende dalla dimensione del campione, dalla normalità dei dati e dalla presenza di correlazioni sufficienti tra le variabili.\n\nInterpretazione dei fattori: Sebbene i carichi fattoriali forniscano indicazioni sulla struttura latente, l’interpretazione richiede una solida base teorica e non deve essere puramente data-driven.\n\nValidazione incrociata: È essenziale testare la generalizzabilità del modello su campioni indipendenti per evitare di sovradattare il modello ai dati specifici.\n\nQuesto capitolo ha introdotto i concetti fondamentali dell’analisi fattoriale con l’obiettivo di stimolare interesse e fornire le basi per un’applicazione autonoma. Tuttavia, per sfruttare appieno il potenziale di questa metodologia, si suggerisce di approfondire:\n\n\nMetodi avanzati di rotazione: Come la rotazione obliqua o l’approccio procrustes per l’allineamento delle soluzioni.\n\nAnalisi fattoriale esplorativa a livello bayesiano: Un’alternativa moderna che incorpora incertezze nei modelli.\n\nModelli di equazioni strutturali: L’analisi fattoriale rappresenta il nucleo di questi modelli più complessi, che permettono di integrare relazioni causali tra fattori.\n\nIn conclusione, l’analisi fattoriale non si limita a essere un semplice metodo statistico, ma rappresenta un collegamento fondamentale tra la teoria e i dati, indispensabile per comprendere e validare costrutti complessi. Questo capitolo, pur introducendo solo i concetti di base, offre una solida piattaforma per approfondire le numerose applicazioni e potenzialità del metodo.\nUn tema cruciale, che sarà approfondito in un capitolo successivo, riguarda l’analisi fattoriale multigruppo. Questo approccio consente di esaminare l’invarianza di misura, un requisito essenziale per garantire che gli strumenti di misura siano equi e validi in contesti e gruppi diversi. Tale analisi sarà trattata in seguito all’interno del più ampio framework dei modelli di equazioni strutturali (SEM).",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html#session-info",
    "href": "chapters/cfa/07_fa_in_r.html#session-info",
    "title": "45  Strategia Integrata per un’Analisi Fattoriale",
    "section": "\n45.12 Session Info",
    "text": "45.12 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.2\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] devtools_2.4.5    usethis_3.1.0     effectsize_1.0.0  ggokabeito_0.1.0 \n#&gt;  [5] see_0.11.0        MASS_7.3-65       viridis_0.6.5     viridisLite_0.4.2\n#&gt;  [9] ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0  \n#&gt; [13] bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-19    \n#&gt; [17] psych_2.4.12      scales_1.3.0      markdown_1.13     knitr_1.50       \n#&gt; [21] lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n#&gt; [25] purrr_1.0.4       readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n#&gt; [29] ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] splines_4.4.2       later_1.4.1         cellranger_1.1.0   \n#&gt;   [4] R.oo_1.27.0         datawizard_1.0.1    XML_3.99-0.18      \n#&gt;   [7] rpart_4.1.24        lifecycle_1.0.4     Rdpack_2.6.3       \n#&gt;  [10] rstatix_0.7.2       rprojroot_2.0.4     lattice_0.22-6     \n#&gt;  [13] insight_1.1.0       rockchalk_1.8.157   backports_1.5.0    \n#&gt;  [16] magrittr_2.0.3      openxlsx_4.2.8      Hmisc_5.2-3        \n#&gt;  [19] rmarkdown_2.29      remotes_2.5.0       httpuv_1.6.15      \n#&gt;  [22] qgraph_1.9.8        zip_2.3.2           sessioninfo_1.2.3  \n#&gt;  [25] pkgbuild_1.4.6      pbapply_1.7-2       minqa_1.2.8        \n#&gt;  [28] multcomp_1.4-28     abind_1.4-8         pkgload_1.4.0      \n#&gt;  [31] quadprog_1.5-8      R.utils_2.13.0      nnet_7.3-20        \n#&gt;  [34] TH.data_1.1-3       sandwich_3.1-1      arm_1.14-4         \n#&gt;  [37] codetools_0.2-20    tidyselect_1.2.1    farver_2.1.2       \n#&gt;  [40] lme4_1.1-36         stats4_4.4.2        base64enc_0.1-3    \n#&gt;  [43] jsonlite_1.9.1      ellipsis_0.3.2      Formula_1.2-5      \n#&gt;  [46] survival_3.8-3      emmeans_1.10.7      tools_4.4.2        \n#&gt;  [49] rio_1.2.3           Rcpp_1.0.14         glue_1.8.0         \n#&gt;  [52] mnormt_2.1.1        xfun_0.51           numDeriv_2016.8-1.1\n#&gt;  [55] withr_3.0.2         fastmap_1.2.0       boot_1.3-31        \n#&gt;  [58] digest_0.6.37       mi_1.1              timechange_0.3.0   \n#&gt;  [61] R6_2.6.1            mime_0.13           estimability_1.5.1 \n#&gt;  [64] colorspace_2.1-1    gtools_3.9.5        jpeg_0.1-10        \n#&gt;  [67] R.methodsS3_1.8.2   generics_0.1.3      data.table_1.17.0  \n#&gt;  [70] corpcor_1.6.10      htmlwidgets_1.6.4   parameters_0.24.2  \n#&gt;  [73] pkgconfig_2.0.3     sem_3.1-16          gtable_0.3.6       \n#&gt;  [76] htmltools_0.5.8.1   carData_3.0-5       profvis_0.4.0      \n#&gt;  [79] png_0.1-8           reformulas_0.4.0    rstudioapi_0.17.1  \n#&gt;  [82] tzdb_0.5.0          reshape2_1.4.4      curl_6.2.1         \n#&gt;  [85] coda_0.19-4.1       checkmate_2.3.2     nlme_3.1-167       \n#&gt;  [88] nloptr_2.2.1        zoo_1.8-13          cachem_1.1.0       \n#&gt;  [91] parallel_4.4.2      miniUI_0.1.1.1      foreign_0.8-88     \n#&gt;  [94] pillar_1.10.1       grid_4.4.2          vctrs_0.6.5        \n#&gt;  [97] urlchecker_1.0.1    promises_1.3.2      car_3.1-3          \n#&gt; [100] OpenMx_2.21.13      xtable_1.8-4        cluster_2.1.8.1    \n#&gt; [103] htmlTable_2.4.3     evaluate_1.0.3      pbivnorm_0.6.0     \n#&gt; [106] mvtnorm_1.3-3       cli_3.6.4           kutils_1.73        \n#&gt; [109] compiler_4.4.2      rlang_1.1.5         ggsignif_0.6.4     \n#&gt; [112] labeling_0.4.3      fdrtool_1.2.18      plyr_1.8.9         \n#&gt; [115] fs_1.6.5            stringi_1.8.4       munsell_0.5.1      \n#&gt; [118] lisrelToR_0.3       bayestestR_0.15.2   pacman_0.5.1       \n#&gt; [121] Matrix_1.7-3        hms_1.1.3           glasso_1.11        \n#&gt; [124] shiny_1.10.0        rbibutils_2.3       igraph_2.1.4       \n#&gt; [127] broom_1.0.7         memoise_2.0.1       RcppParallel_5.1.10\n#&gt; [130] readxl_1.4.5\n\n\n\n\n\nSaqr, M., & López-Pernas, S. (2024). Learning analytics methods and tutorials: A practical guide using R. Springer Nature.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/E_01.html",
    "href": "chapters/cfa/E_01.html",
    "title": "46  ✏️ Esercizi",
    "section": "",
    "text": "source(\"../_common.R\")\nsuppressPackageStartupMessages({\n    library(\"lavaan\")\n    library(\"semTools\")\n})\nset.seed(42)\n\nE1. Si ripeta l’esercizio che abbiamo svolto in precedenza usando l’analisi fattoriale esplorativa, questa volta usando la CFA in lavaan. I dati sono forniti da Brown (2015) e riguardano a otto misure di personalità raccolte su un campione di 250 pazienti che hanno concluso un programma di psicoterapia:\n\nanxiety (N1),\nhostility (N2),\ndepression (N3),\nself-consciousness (N4),\nwarmth (E1),\ngregariousness (E2),\nassertiveness (E3),\npositive emotions (E4).\n\n\nvarnames &lt;- c(\"N1\", \"N2\", \"N3\", \"N4\", \"E1\", \"E2\", \"E3\", \"E4\")\nsds &lt;- \"5.7  5.6  6.4  5.7  6.0  6.2  5.7  5.6\"\n\ncors &lt;- \"\n 1.000\n 0.767  1.000\n 0.731  0.709  1.000\n 0.778  0.738  0.762  1.000\n-0.351  -0.302  -0.356  -0.318  1.000\n-0.316  -0.280  -0.300  -0.267  0.675  1.000\n-0.296  -0.289  -0.297  -0.296  0.634  0.651  1.000\n-0.282  -0.254  -0.292  -0.245  0.534  0.593  0.566  1.000\"\n\npsychot_cor_mat &lt;- getCov(cors, names = varnames)\nn &lt;- 250\n\nIl modello con due fattori ortogonali può essere adattato ai dati nel modo seguente.\n\ncfa_mod &lt;- \"\n  N =~ N1 + N2 + N3 + N4\n  E =~ E1 + E2 + E3 + E4\n\"\n\n\nfit_cfa &lt;- lavaan::cfa(\n    cfa_mod,\n    sample.cov = psychot_cor_mat,\n    sample.nobs = n,\n    orthogonal = TRUE,\n    std.lv = TRUE\n)\n\n\nsemPlot::semPaths(fit_cfa,\n    what = \"col\", whatLabels = \"par\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\n\nEsaminiamo le saturazioni fattoriali:\n\nparameterEstimates(fit_cfa, standardized = TRUE) %&gt;%\n    dplyr::filter(op == \"=~\") %&gt;%\n    dplyr::select(\n        \"Latent Factor\" = lhs,\n        Indicator = rhs,\n        B = est,\n        SE = se,\n        Z = z,\n        \"p-value\" = pvalue,\n        Beta = std.all\n    ) %&gt;%\n    knitr::kable(\n        digits = 3, booktabs = TRUE, format = \"markdown\",\n        caption = \"Factor Loadings\"\n    )\n\n\n\nTable: Factor Loadings\n\n|Latent Factor |Indicator |     B|    SE|      Z| p-value|  Beta|\n|:-------------|:---------|-----:|-----:|------:|-------:|-----:|\n|N             |N1        | 0.882| 0.051| 17.422|       0| 0.884|\n|N             |N2        | 0.847| 0.052| 16.340|       0| 0.849|\n|N             |N3        | 0.840| 0.052| 16.134|       0| 0.842|\n|N             |N4        | 0.882| 0.051| 17.432|       0| 0.884|\n|E             |E1        | 0.795| 0.056| 14.276|       0| 0.796|\n|E             |E2        | 0.838| 0.054| 15.369|       0| 0.839|\n|E             |E3        | 0.788| 0.056| 14.097|       0| 0.789|\n|E             |E4        | 0.697| 0.058| 11.942|       0| 0.699|\n\n\nIl risultato sembra sensato: le saturazioni su ciascun fattore sono molto alte. Tuttavia, la matrice delle correlazioni residue\n\ncor_table &lt;- residuals(fit_cfa, type = \"cor\")$cov\nknitr::kable(\n    cor_table,\n    digits = 3,\n    format = \"markdown\",\n    booktabs = TRUE\n)\n\n\n\n|   |     N1|     N2|     N3|     N4|     E1|     E2|     E3|     E4|\n|:--|------:|------:|------:|------:|------:|------:|------:|------:|\n|N1 |  0.000|  0.017| -0.013| -0.003| -0.351| -0.316| -0.296| -0.282|\n|N2 |  0.017|  0.000| -0.006| -0.012| -0.302| -0.280| -0.289| -0.254|\n|N3 | -0.013| -0.006|  0.000|  0.018| -0.356| -0.300| -0.297| -0.292|\n|N4 | -0.003| -0.012|  0.018|  0.000| -0.318| -0.267| -0.296| -0.245|\n|E1 | -0.351| -0.302| -0.356| -0.318|  0.000|  0.007|  0.006| -0.022|\n|E2 | -0.316| -0.280| -0.300| -0.267|  0.007|  0.000| -0.011|  0.007|\n|E3 | -0.296| -0.289| -0.297| -0.296|  0.006| -0.011|  0.000|  0.015|\n|E4 | -0.282| -0.254| -0.292| -0.245| -0.022|  0.007|  0.015|  0.000|\n\n\nrivela che il modello ipotizzato dall’analisi fattoriale confermativa non è adeguato.\n\nfit2_cfa &lt;- lavaan::cfa(\n    cfa_mod,\n    sample.cov = psychot_cor_mat,\n    sample.nobs = n,\n    orthogonal = FALSE,\n    std.lv = TRUE\n)\n\n\nsemPlot::semPaths(fit2_cfa,\n    what = \"col\", whatLabels = \"par\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\n\nEsaminiamo le saturazioni fattoriali.\n\nparameterEstimates(fit2_cfa, standardized = TRUE) %&gt;%\n    dplyr::filter(op == \"=~\") %&gt;%\n    dplyr::select(\n        \"Latent Factor\" = lhs,\n        Indicator = rhs,\n        B = est,\n        SE = se,\n        Z = z,\n        \"p-value\" = pvalue,\n        Beta = std.all\n    ) %&gt;%\n    knitr::kable(\n        digits = 3, booktabs = TRUE, format = \"markdown\",\n        caption = \"Factor Loadings\"\n    )\n\n\n\nTable: Factor Loadings\n\n|Latent Factor |Indicator |     B|    SE|      Z| p-value|  Beta|\n|:-------------|:---------|-----:|-----:|------:|-------:|-----:|\n|N             |N1        | 0.883| 0.051| 17.472|       0| 0.885|\n|N             |N2        | 0.847| 0.052| 16.337|       0| 0.849|\n|N             |N3        | 0.842| 0.052| 16.190|       0| 0.844|\n|N             |N4        | 0.880| 0.051| 17.381|       0| 0.882|\n|E             |E1        | 0.800| 0.055| 14.465|       0| 0.802|\n|E             |E2        | 0.832| 0.054| 15.294|       0| 0.834|\n|E             |E3        | 0.788| 0.056| 14.150|       0| 0.789|\n|E             |E4        | 0.698| 0.058| 11.974|       0| 0.699|\n\n\nEsaminiamo i residui.\n\ncor_table &lt;- residuals(fit2_cfa, type = \"cor\")$cov\nknitr::kable(\n    cor_table,\n    digits = 3,\n    format = \"markdown\",\n    booktabs = TRUE\n)\n\n\n\n|   |     N1|     N2|     N3|     N4|     E1|     E2|     E3|     E4|\n|:--|------:|------:|------:|------:|------:|------:|------:|------:|\n|N1 |  0.000|  0.016| -0.015| -0.002| -0.042|  0.005|  0.008| -0.013|\n|N2 |  0.016|  0.000| -0.007| -0.010| -0.006|  0.028|  0.002|  0.004|\n|N3 | -0.015| -0.007|  0.000|  0.018| -0.062|  0.006| -0.007| -0.035|\n|N4 | -0.002| -0.010|  0.018|  0.000| -0.010|  0.053|  0.007|  0.023|\n|E1 | -0.042| -0.006| -0.062| -0.010|  0.000|  0.006|  0.001| -0.027|\n|E2 |  0.005|  0.028|  0.006|  0.053|  0.006|  0.000| -0.007|  0.010|\n|E3 |  0.008|  0.002| -0.007|  0.007|  0.001| -0.007|  0.000|  0.014|\n|E4 | -0.013|  0.004| -0.035|  0.023| -0.027|  0.010|  0.014|  0.000|\n\n\nSistemiamo le saturazioni fattoriali in una matrice 8 \\(\\times\\) 2:\n\nlambda &lt;- inspect(fit2_cfa, what = \"std\")$lambda\nlambda\n\n\nA lavaan.matrix: 8 x 2 of type dbl\n\n\n\nN\nE\n\n\n\n\nN1\n0.8848214\n0.0000000\n\n\nN2\n0.8485128\n0.0000000\n\n\nN3\n0.8436432\n0.0000000\n\n\nN4\n0.8819736\n0.0000000\n\n\nE1\n0.0000000\n0.8018485\n\n\nE2\n0.0000000\n0.8337599\n\n\nE3\n0.0000000\n0.7894530\n\n\nE4\n0.0000000\n0.6990366\n\n\n\n\n\nOtteniamo la matrice di intercorrelazoni fattoriali.\n\nPhi &lt;- inspect(fit2_cfa, what = \"std\")$psi\nPhi\n\n\nA lavaan.matrix.symmetric: 2 x 2 of type dbl\n\n\n\nN\nE\n\n\n\n\nN\n1.000000\n-0.434962\n\n\nE\n-0.434962\n1.000000\n\n\n\n\n\nOtteniamo la matrice di varianze residue.\n\nPsi &lt;- inspect(fit2_cfa, what = \"std\")$theta\nPsi\n\n\nA lavaan.matrix.symmetric: 8 x 8 of type dbl\n\n\n\nN1\nN2\nN3\nN4\nE1\nE2\nE3\nE4\n\n\n\n\nN1\n0.217091\n0.0000000\n0.0000000\n0.0000000\n0.000000\n0.0000000\n0.000000\n0.0000000\n\n\nN2\n0.000000\n0.2800261\n0.0000000\n0.0000000\n0.000000\n0.0000000\n0.000000\n0.0000000\n\n\nN3\n0.000000\n0.0000000\n0.2882661\n0.0000000\n0.000000\n0.0000000\n0.000000\n0.0000000\n\n\nN4\n0.000000\n0.0000000\n0.0000000\n0.2221225\n0.000000\n0.0000000\n0.000000\n0.0000000\n\n\nE1\n0.000000\n0.0000000\n0.0000000\n0.0000000\n0.357039\n0.0000000\n0.000000\n0.0000000\n\n\nE2\n0.000000\n0.0000000\n0.0000000\n0.0000000\n0.000000\n0.3048445\n0.000000\n0.0000000\n\n\nE3\n0.000000\n0.0000000\n0.0000000\n0.0000000\n0.000000\n0.0000000\n0.376764\n0.0000000\n\n\nE4\n0.000000\n0.0000000\n0.0000000\n0.0000000\n0.000000\n0.0000000\n0.000000\n0.5113478\n\n\n\n\n\nMediante i parametri del modello la matrice di correlazione si riproduce nel modo seguente:\n\\[\n\\boldsymbol{\\Sigma} =\\boldsymbol{\\Lambda} \\boldsymbol{\\Phi} \\boldsymbol{\\Lambda}^{\\mathsf{T}} + \\boldsymbol{\\Psi}.\n\\]\nIn \\(\\textsf{R}\\) scriviamo:\n\nR_hat &lt;- lambda %*% Phi %*% t(lambda) + Psi\nR_hat %&gt;%\n    round(3)\n\n\nA lavaan.matrix.symmetric: 8 x 8 of type dbl\n\n\n\nN1\nN2\nN3\nN4\nE1\nE2\nE3\nE4\n\n\n\n\nN1\n1.000\n0.751\n0.746\n0.780\n-0.309\n-0.321\n-0.304\n-0.269\n\n\nN2\n0.751\n1.000\n0.716\n0.748\n-0.296\n-0.308\n-0.291\n-0.258\n\n\nN3\n0.746\n0.716\n1.000\n0.744\n-0.294\n-0.306\n-0.290\n-0.257\n\n\nN4\n0.780\n0.748\n0.744\n1.000\n-0.308\n-0.320\n-0.303\n-0.268\n\n\nE1\n-0.309\n-0.296\n-0.294\n-0.308\n1.000\n0.669\n0.633\n0.561\n\n\nE2\n-0.321\n-0.308\n-0.306\n-0.320\n0.669\n1.000\n0.658\n0.583\n\n\nE3\n-0.304\n-0.291\n-0.290\n-0.303\n0.633\n0.658\n1.000\n0.552\n\n\nE4\n-0.269\n-0.258\n-0.257\n-0.268\n0.561\n0.583\n0.552\n1.000\n\n\n\n\n\nLe correlazioni residue sono:\n\n(psychot_cor_mat - R_hat) %&gt;%\n    round(3)\n\n\nA lavaan.matrix.symmetric: 8 x 8 of type dbl\n\n\n\nN1\nN2\nN3\nN4\nE1\nE2\nE3\nE4\n\n\n\n\nN1\n0.000\n0.016\n-0.015\n-0.002\n-0.042\n0.005\n0.008\n-0.013\n\n\nN2\n0.016\n0.000\n-0.007\n-0.010\n-0.006\n0.028\n0.002\n0.004\n\n\nN3\n-0.015\n-0.007\n0.000\n0.018\n-0.062\n0.006\n-0.007\n-0.035\n\n\nN4\n-0.002\n-0.010\n0.018\n0.000\n-0.010\n0.053\n0.007\n0.023\n\n\nE1\n-0.042\n-0.006\n-0.062\n-0.010\n0.000\n0.006\n0.001\n-0.027\n\n\nE2\n0.005\n0.028\n0.006\n0.053\n0.006\n0.000\n-0.007\n0.010\n\n\nE3\n0.008\n0.002\n-0.007\n0.007\n0.001\n-0.007\n0.000\n0.014\n\n\nE4\n-0.013\n0.004\n-0.035\n0.023\n-0.027\n0.010\n0.014\n0.000\n\n\n\n\n\nCalcoliamo la correlazione predetta dal modello tra le variabili \\(Y_1\\) e \\(Y_2\\):\n\nlambda[1, 1] * lambda[2, 1] + lambda[1, 2] * lambda[2, 2] +\n    lambda[1, 1] * lambda[2, 2] * Phi[1, 2] +\n    lambda[1, 2] * lambda[2, 1] * Phi[1, 2]\n\n0.750782309575684\n\n\nQuesto risultato è molto simile al valore contenuto dell’elemento (1, 2) della matrice di correlazioni osservate:\n\npsychot_cor_mat[1, 2]\n\n0.767\n\n\nUsando le funzonalità di lavaan la matrice di correlazione predetta si ottiene con:\n\nfitted(fit2_cfa)$cov |&gt;\n    print()\n\n       N1     N2     N3     N4     E1     E2     E3     E4\nN1  0.996                                                 \nN2  0.748  0.996                                          \nN3  0.743  0.713  0.996                                   \nN4  0.777  0.745  0.741  0.996                            \nE1 -0.307 -0.295 -0.293 -0.306  0.996                     \nE2 -0.320 -0.306 -0.305 -0.319  0.666  0.996              \nE3 -0.303 -0.290 -0.289 -0.302  0.630  0.656  0.996       \nE4 -0.268 -0.257 -0.255 -0.267  0.558  0.580  0.550  0.996\n\n\nLa matrice dei residui è\n\nresid(fit2_cfa)$cov |&gt;\n    print()\n\n       N1     N2     N3     N4     E1     E2     E3     E4\nN1  0.000                                                 \nN2  0.016  0.000                                          \nN3 -0.015 -0.007  0.000                                   \nN4 -0.002 -0.010  0.018  0.000                            \nE1 -0.042 -0.006 -0.062 -0.010  0.000                     \nE2  0.005  0.028  0.006  0.053  0.006  0.000              \nE3  0.008  0.002 -0.007  0.007  0.001 -0.007  0.000       \nE4 -0.013  0.004 -0.035  0.023 -0.026  0.010  0.014  0.000\n\n\nLa matrice dei residui standardizzati è\n\nresid(fit2_cfa, type = \"standardized\")$cov |&gt;\n    print()\n\n       N1     N2     N3     N4     E1     E2     E3     E4\nN1  0.000                                                 \nN2  1.674  0.000                                          \nN3 -1.769 -0.569  0.000                                   \nN4 -0.350 -1.152  1.746  0.000                            \nE1 -1.214 -0.161 -1.646 -0.294  0.000                     \nE2  0.154  0.794  0.168  1.626  0.637  0.000              \nE3  0.219  0.062 -0.191  0.193  0.075 -0.693  0.000       \nE4 -0.314  0.092 -0.824  0.552 -1.481  0.624  0.690  0.000\n\n\nI valori precedenti possono essere considerati come punti z, dove i valori con un valore assoluto maggiore di 2 possono essere ritenuti problematici. Tuttavia, è importante considerare che in questo modo si stanno eseguendo molteplici confronti, pertanto, si dovrebbe considerare l’opportunità di applicare una qualche forma di correzione per i confronti multipli.\nE2. Si utilizzino i dati dass21.txt che corrispondono alla somministrazione del test DASS-21 a 334 partecipanti. Lo schema di codifica si può trovare seguendo questo link. Si adatti ai dati un modello a tre fattori usando l’analisi fattoriale esplorativa con la funzione lavaan::efa(). Usando le saturazioni fattoriali e la matrice di inter-correlazioni fattoriali, si trovi la matrice di correlazioni riprodotta dal modello. Senza usare l’albebra matriciale, si trovi la correlazione predetta tra gli indicatori DASS-1 e DASS-2.\n\n\n\n\nBrown, T. A. (2015). Confirmatory factor analysis for applied research. Guilford publications.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/E_02_bifactor.html",
    "href": "chapters/cfa/E_02_bifactor.html",
    "title": "47  ✏️ Esercizi",
    "section": "",
    "text": "47.1 Introduzione\nIn questo esercizio replicheremo la validazione della Strengths and Weaknesses of ADHD Symptoms and Normal Behavior Scale descritta nell’articolo di Blume et al. (2020).\nGli adulti con sintomi di disturbo da deficit di attenzione/iperattività (ADHD; American Psychiatric Association [APA], 2013) presentano sintomi di disattenzione (ad esempio, difficoltà a mantenere l’attenzione sul lavoro, durante compiti o attività), iperattività-impulsività (ad esempio, interrompere o intromettersi nelle conversazioni, parlare in modo eccessivo), o una combinazione di entrambi. Questi sintomi sono associati a compromissioni nel funzionamento accademico (ad esempio, tassi più bassi di diploma e laurea), lavorativo (ad esempio, redditi complessivamente inferiori) e sociale (ad esempio, meno amici, tassi di divorzio più alti).\nL’ADHD si manifesta inizialmente durante l’infanzia e persiste nell’età adulta in circa la metà dei casi, con una prevalenza stimata del 2.5% negli adulti. Clinicamente, l’ADHD si presenta in tre modalità principali:\nSwanson e colleghi (2012) hanno introdotto la scala Strengths and Weaknesses of ADHD-Symptoms and Normal-Behavior (SWAN), che valuta i sintomi di disattenzione e iperattività-impulsività nei bambini in età scolare tramite un report di terze parti. La scala, composta da 18 item, si basa sui criteri sintomatici definiti nel Diagnostic and Statistical Manual of Mental Disorders (DSM-IV; APA, 2000) e confermati nel DSM-5 (APA, 2013). La SWAN è stata progettata per valutare il comportamento dei bambini, concentrandosi su situazioni scolastiche, di gioco e domestiche. La scala utilizza un punteggio a 7 punti, con ancore che rappresentano gli estremi negativi (“molto al di sotto della media”) e positivi (“molto al di sopra della media”), confrontando il comportamento del bambino con quello di altri coetanei. La SWAN è stata la prima scala a valutare i sintomi dell’ADHD in modo realmente dimensionale.\nBlume et al. (2020) adattano la versione tedesca esistente, SWAN-DE (Schulz-Zhecheva et al., 2019), in una versione self-report per adulti, denominandola German Strengths and Weaknesses of ADHD and Normal-Behavior Scale Self-Report (SWAN-DE-SB).",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/E_02_bifactor.html#introduzione",
    "href": "chapters/cfa/E_02_bifactor.html#introduzione",
    "title": "47  ✏️ Esercizi",
    "section": "",
    "text": "Presentazione prevalentemente disattenta: predominano i sintomi di disattenzione;\nPresentazione prevalentemente iperattiva-impulsiva: predominano i sintomi di iperattività-impulsività;\nPresentazione combinata: sono presenti livelli significativi di entrambi i tipi di sintomi.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/E_02_bifactor.html#validazione",
    "href": "chapters/cfa/E_02_bifactor.html#validazione",
    "title": "47  ✏️ Esercizi",
    "section": "47.2 Validazione",
    "text": "47.2 Validazione\nDi seguito è fornita una parte dello script R fornito dagli autori per l’analisi statistica dai dati grezzi fino alla formulazione del modello bifattoriale.\n\ndata &lt;- rio::import(here::here(\"data\", \"blume_2024\", \"data_total_OSF.csv\"))\n\ndata$X &lt;- NULL # delete column without information\n\ndata$SW_mean &lt;- as.numeric(data$SW_mean) # convert from character to numeric\ndata$SW_AD_mean &lt;- as.numeric(data$SW_AD_mean)\ndata$SW_HI_mean &lt;- as.numeric(data$SW_HI_mean)\n\ndata$CA_mean &lt;- as.numeric(data$CA_mean)\ndata$CA_AD_mean &lt;- as.numeric(data$CA_AD_mean)\ndata$CA_HI_mean &lt;- as.numeric(data$CA_HI_mean)\n\ndata$HA_mean &lt;- as.numeric(data$HA_mean)\ndata$HA_AD_mean &lt;- as.numeric(data$HA_AD_mean)\ndata$HA_HI_mean &lt;- as.numeric(data$HA_HI_mean)\n\n\ndata_clin &lt;- rio::import(here::here(\"data\", \"blume_2024\", \"data_clinical_OSF.csv\"))\n\ndata_clin$X &lt;- NULL # delete column without information\n\ndata_clin$SW_mean &lt;- as.numeric(data_clin$SW_mean) # convert from character to numeric\ndata_clin$SW_AD_mean &lt;- as.numeric(data_clin$SW_AD_mean)\ndata_clin$SW_HI_mean &lt;- as.numeric(data_clin$SW_HI_mean)\n\ndata_clin$CA_mean &lt;- as.numeric(data_clin$CA_mean)\ndata_clin$CA_AD_mean &lt;- as.numeric(data_clin$CA_AD_mean)\ndata_clin$CA_HI_mean &lt;- as.numeric(data_clin$CA_HI_mean)\n\ndata_clin$HA_mean &lt;- as.numeric(data_clin$HA_mean)\ndata_clin$HA_AD_mean &lt;- as.numeric(data_clin$HA_AD_mean)\ndata_clin$HA_HI_mean &lt;- as.numeric(data_clin$HA_HI_mean)\n\n\n# Information on missing data in the general population sample\nSWAN_vars &lt;- colnames(data)[str_detect(colnames(data), \"SW01\")]\nsum(is.na(data[, SWAN_vars])) # 1 data point missing\nsum(!is.na(data[, SWAN_vars])) # 7163 not missing -&gt; 0.01% missing\n\n1\n\n\n7163\n\n\n\n# age\n\nsem_age1 &lt;- \"\n        SW_GF =~ SW01_01 + SW01_02 + SW01_03 + SW01_04 + SW01_05 + SW01_06\n                           + SW01_07 + SW01_08 + SW01_09 + SW01_10 + SW01_11 + SW01_12\n                           + SW01_13 + SW01_14 + SW01_15 + SW01_16 + SW01_17 + SW01_18;\n        SW_GF ~ age\n\"\nfit_age1 &lt;- sem(sem_age1, data = data)\n\n# Regressions:\n#                   Estimate  Std.Err  z-value  P(&gt;|z|)\n#  SW_GF ~\n#    age               0.001    0.003    0.285    0.775\n\n\nsem_age2 &lt;- \"\n        SW_AD =~ SW01_01 + SW01_02 + SW01_03 + SW01_04 + SW01_05 + SW01_06\n                + SW01_07 + SW01_08 + SW01_09;\n        SW_AD ~ age\n\"\nfit_age2 &lt;- sem(sem_age2, data = data)\n\n# Regressions:\n#                   Estimate  Std.Err  z-value  P(&gt;|z|)\n#  SW_AD ~\n#    age               0.004    0.003    1.018    0.309\n\nsem_age3 &lt;- \"\n        SW_HI =~ SW01_10 + SW01_11 + SW01_12 + SW01_13 + SW01_14 + SW01_15\n                + SW01_16 + SW01_17 + SW01_18;\n        SW_HI ~ age\n\"\nfit_age3 &lt;- sem(sem_age3, data = data)\n\n# Regressions:\n#                   Estimate  Std.Err  z-value  P(&gt;|z|)\n#  SW_HI ~\n#    age              -0.002    0.005   -0.530    0.596\n\n\nglimpse(data)\n\nRows: 398\nColumns: 84\n$ V1             &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, ~\n$ id             &lt;int&gt; 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,~\n$ gender         &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\n$ age            &lt;int&gt; 36, 26, 21, 21, 21, 19, 22, 25, 28, 20, 19, 32, 18,~\n$ diagnosis_ever &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ~\n$ diagnosis_now  &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ~\n$ medication     &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ~\n$ education      &lt;int&gt; 4, 3, 3, 3, 3, 3, 3, 4, 4, 3, 3, 4, 3, 3, 3, 4, 3, ~\n$ SW01_01        &lt;int&gt; 2, 4, 4, 4, 4, 6, 4, 6, 3, 4, 5, 6, 4, 4, 3, 5, 4, ~\n$ SW01_02        &lt;int&gt; 2, 6, 3, 3, 5, 5, 4, 4, 4, 1, 5, 5, 3, 4, 3, 5, 1, ~\n$ SW01_03        &lt;int&gt; 4, 6, 5, 3, 4, 6, 6, 6, 5, 4, 5, 5, 5, 6, 2, 5, 4, ~\n$ SW01_04        &lt;int&gt; 4, 6, 3, 4, 5, 6, 5, 5, 5, 3, 6, 5, 4, 5, 3, 5, 1, ~\n$ SW01_05        &lt;int&gt; 3, 3, 5, 4, 5, 6, 6, 6, 6, 5, 4, 6, 5, 5, 6, 5, 2, ~\n$ SW01_06        &lt;int&gt; 4, 3, 3, 3, 5, 6, 6, 5, 3, 1, 6, 5, 5, 5, 2, 5, 2, ~\n$ SW01_07        &lt;int&gt; 4, 4, 3, 3, 5, 6, 6, 6, 6, 3, 3, 5, 4, 5, 5, 5, 4, ~\n$ SW01_08        &lt;int&gt; 3, 3, 2, 3, 3, 4, 3, 5, 5, 1, 3, 4, 3, 2, 1, 2, 3, ~\n$ SW01_09        &lt;int&gt; 3, 4, 3, 4, 5, 4, 0, 6, 6, 5, 1, 5, 3, 6, 4, 4, 4, ~\n$ SW01_10        &lt;int&gt; 3, 5, 2, 3, 4, 3, 6, 3, 4, 5, 3, 5, 3, 4, 3, 3, 2, ~\n$ SW01_11        &lt;int&gt; 3, 6, 3, 3, 4, 5, 6, 6, 4, 6, 5, 5, 3, 6, 3, 3, 5, ~\n$ SW01_12        &lt;int&gt; 3, 3, 3, 3, 3, 6, 6, 6, 3, 6, 4, 5, 2, 3, 3, 3, 2, ~\n$ SW01_13        &lt;int&gt; 4, 6, 3, 3, 3, 6, 2, 6, 5, 2, 5, 6, 3, 5, 4, 3, 1, ~\n$ SW01_14        &lt;int&gt; 3, 1, 3, 3, 3, 6, 6, 6, 5, 6, 5, 4, 4, 5, 1, 2, 5, ~\n$ SW01_15        &lt;int&gt; 3, 4, 3, 3, 5, 6, 3, 6, 4, 4, 5, 5, 3, 4, 5, 2, 2, ~\n$ SW01_16        &lt;int&gt; 2, 6, 3, 3, 5, 5, 2, 6, 3, 6, 5, 5, 3, 4, 3, 2, 2, ~\n$ SW01_17        &lt;int&gt; 3, 4, 4, 3, 3, 5, 3, 6, 3, 6, 5, 5, 3, 4, 1, 1, 4, ~\n$ SW01_18        &lt;int&gt; 3, 4, 3, 3, 3, 6, 3, 6, 4, 2, 5, 5, 3, 1, 3, 2, 0, ~\n$ HA01_01        &lt;int&gt; 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 2, ~\n$ HA01_02        &lt;int&gt; 2, 1, 0, 1, 0, 0, 1, 0, 0, 2, 0, 2, 1, 0, 0, 0, 2, ~\n$ HA01_03        &lt;int&gt; 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 2, 0, 1, 1, 0, 1, ~\n$ HA01_04        &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, ~\n$ HA01_05        &lt;int&gt; 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, ~\n$ HA01_06        &lt;int&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ~\n$ HA01_07        &lt;int&gt; 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, ~\n$ HA01_08        &lt;int&gt; 1, 0, 1, 1, 0, 0, 1, 1, 0, 2, 1, 1, 1, 1, 2, 1, 2, ~\n$ HA01_09        &lt;int&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, ~\n$ HA01_10        &lt;int&gt; 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, ~\n$ HA01_11        &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\n$ HA01_12        &lt;int&gt; 1, 3, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 2, 0, 1, ~\n$ HA01_13        &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\n$ HA01_14        &lt;int&gt; 1, 2, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 1, 1, 2, 0, ~\n$ HA01_15        &lt;int&gt; 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 2, 2, ~\n$ HA01_16        &lt;int&gt; 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, ~\n$ HA01_17        &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, ~\n$ HA01_18        &lt;int&gt; 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, ~\n$ HA01_19        &lt;int&gt; 0, 1, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 1, 0, 1, 3, 2, ~\n$ HA01_20        &lt;int&gt; 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, ~\n$ HA01_21        &lt;int&gt; 1, 3, 0, 0, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 1, 1, ~\n$ HA01_22        &lt;int&gt; 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ~\n$ CA01_01        &lt;int&gt; 1, 0, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 2, 2, 2, ~\n$ CA01_02        &lt;int&gt; 1, 2, 1, 0, 0, 1, 2, 1, 3, 0, 0, 1, 0, 0, 1, 3, 0, ~\n$ CA01_03        &lt;int&gt; 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 2, ~\n$ CA01_04        &lt;int&gt; 1, 0, 0, 0, 2, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, ~\n$ CA01_05        &lt;int&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, ~\n$ CA01_06        &lt;int&gt; 2, 2, 2, 1, 2, 1, 0, 1, 1, 3, 2, 0, 1, 2, 0, 3, 0, ~\n$ CA01_07        &lt;int&gt; 2, 3, 2, 1, 1, 1, 1, 0, 1, 2, 1, 0, 0, 1, 1, 1, 0, ~\n$ CA01_08        &lt;int&gt; 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, ~\n$ CA01_09        &lt;int&gt; 0, 1, 2, 0, 0, 2, 0, 0, 0, 2, 1, 0, 1, 2, 1, 0, 1, ~\n$ CA01_10        &lt;int&gt; 1, 0, 1, 1, 1, 1, 2, 0, 1, 1, 0, 2, 1, 1, 1, 3, 0, ~\n$ CA01_11        &lt;int&gt; 1, 2, 2, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, ~\n$ CA01_12        &lt;int&gt; 2, 1, 3, 1, 0, 0, 1, 1, 0, 3, 1, 2, 0, 1, 2, 0, 1, ~\n$ CA01_13        &lt;int&gt; 2, 2, 2, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 1, 1, 0, 0, ~\n$ CA01_14        &lt;int&gt; 1, 1, 0, 0, 1, 0, 2, 0, 0, 2, 1, 2, 0, 0, 1, 1, 2, ~\n$ CA01_15        &lt;int&gt; 2, 3, 2, 2, 2, 1, 2, 0, 1, 1, 1, 2, 0, 1, 2, 3, 1, ~\n$ CA01_16        &lt;int&gt; 2, 3, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, ~\n$ CA01_17        &lt;int&gt; 2, 1, 2, 1, 2, 0, 0, 0, 0, 2, 0, 1, 0, 1, 1, 3, 3, ~\n$ CA01_18        &lt;int&gt; 2, 2, 2, 1, 1, 0, 0, 1, 1, 3, 0, 2, 1, 0, 1, 2, 3, ~\n$ CA01_19        &lt;int&gt; 0, 0, 2, 0, 1, 0, 1, 0, 1, 0, 2, 1, 0, 1, 0, 1, 1, ~\n$ CA01_20        &lt;int&gt; 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 2, 1, 0, 1, 1, 0, 1, ~\n$ CA01_21        &lt;int&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 3, 1, 2, 0, 0, 0, 2, 1, ~\n$ CA01_22        &lt;int&gt; 2, 1, 3, 0, 0, 0, 3, 0, 0, 1, 0, 2, 0, 0, 1, 0, 2, ~\n$ CA01_23        &lt;int&gt; 1, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ~\n$ CA01_24        &lt;int&gt; 0, 0, 1, 0, 0, 0, 2, 1, 0, 3, 0, 1, 0, 0, 0, 0, 0, ~\n$ CA01_25        &lt;int&gt; 1, 3, 3, 0, 1, 0, 0, 0, 1, 2, 0, 1, 0, 1, 3, 0, 3, ~\n$ CA01_26        &lt;int&gt; 1, 3, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ~\n$ diagnosis_type &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\n$ SW_mean        &lt;dbl&gt; 3.11, 4.33, 3.22, 3.22, 4.11, 5.39, 4.28, 5.56, 4.3~\n$ SW_AD_mean     &lt;dbl&gt; 3.22, 4.33, 3.44, 3.44, 4.56, 5.44, 4.44, 5.44, 4.7~\n$ SW_HI_mean     &lt;dbl&gt; 3.00, 4.33, 3.00, 3.00, 3.67, 5.33, 4.11, 5.67, 3.8~\n$ CA_mean        &lt;dbl&gt; 1.154, 1.308, 1.423, 0.462, 0.692, 0.538, 0.692, 0.~\n$ CA_AD_mean     &lt;dbl&gt; 1.0, 1.0, 0.8, 0.6, 1.0, 0.0, 0.2, 0.2, 0.2, 2.0, 0~\n$ CA_HI_mean     &lt;dbl&gt; 1.3, 1.1, 1.3, 0.5, 0.7, 0.8, 0.4, 0.1, 0.7, 1.1, 0~\n$ HA_mean        &lt;dbl&gt; 0.5000, 0.9091, 0.1364, 0.3636, 0.1364, 0.1364, 0.5~\n$ HA_AD_mean     &lt;dbl&gt; 0.778, 0.667, 0.111, 0.444, 0.222, 0.000, 0.333, 0.~\n$ HA_HI_mean     &lt;dbl&gt; 0.333, 0.778, 0.222, 0.333, 0.111, 0.333, 0.667, 0.~\n\n\n\n# education\nsem_education1 &lt;- \"\n        SW_GF =~ SW01_01 + SW01_02 + SW01_03 + SW01_04 + SW01_05 + SW01_06\n                           + SW01_07 + SW01_08 + SW01_09 + SW01_10 + SW01_11 + SW01_12\n                           + SW01_13 + SW01_14 + SW01_15 + SW01_16 + SW01_17 + SW01_18;\n        SW_GF ~ education\n\"\nfit_education1 &lt;- sem(sem_education1, data = data)\n\n# Regressions:\n#                   Estimate  Std.Err  z-value  P(&gt;|z|)\n#  SW_GF ~\n#    education         0.170    0.059    2.897    0.004\n\nsem_education2 &lt;- \"\n        SW_AD =~ SW01_01 + SW01_02 + SW01_03 + SW01_04 + SW01_05 + SW01_06\n                + SW01_07 + SW01_08 + SW01_09;\n        SW_AD ~ education\n\"\nfit_education2 &lt;- sem(sem_education2, data = data)\n\n# Regressions:\n#                   Estimate  Std.Err  z-value  P(&gt;|z|)\n#  SW_AD ~\n#    education         0.209    0.066    3.196    0.001\n\nsem_education3 &lt;- \"\n        SW_HI =~ SW01_10 + SW01_11 + SW01_12 + SW01_13 + SW01_14 + SW01_15\n                + SW01_16 + SW01_17 + SW01_18;\n        SW_HI ~ education\n\"\nfit_education3 &lt;- sem(sem_education3, data = data)\n\n# Regressions:\n#                   Estimate  Std.Err  z-value  P(&gt;|z|)\n#  SW_HI ~\n#    education         0.172    0.086    2.016    0.044\n\n# interactions\n\n\nSWAN_vars &lt;- colnames(data)[str_detect(colnames(data), \"SW01\")]\n\nSW_AD &lt;- colnames(data[, c(\n    \"SW01_01\",\n    \"SW01_02\",\n    \"SW01_03\",\n    \"SW01_04\",\n    \"SW01_05\",\n    \"SW01_06\",\n    \"SW01_07\",\n    \"SW01_08\",\n    \"SW01_09\"\n)])\n\nSW_HI &lt;- colnames(data[, c(\n    \"SW01_10\",\n    \"SW01_11\",\n    \"SW01_12\",\n    \"SW01_13\",\n    \"SW01_14\",\n    \"SW01_15\",\n    \"SW01_16\",\n    \"SW01_17\",\n    \"SW01_18\"\n)])\n\n# Cronbachs alphas\npsych::alpha(data[, SWAN_vars]) # 0.90\npsych::alpha(data[, SW_AD]) # 0.85\npsych::alpha(data[, SW_HI]) # 0.87\n\n\nReliability analysis   \nCall: psych::alpha(x = data[, SWAN_vars])\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean   sd median_r\n       0.9       0.9    0.92      0.33 8.8 0.0075  3.8 0.83     0.33\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.88   0.9  0.91\nDuhachek  0.88   0.9  0.91\n\n Reliability if an item is dropped:\n        raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r\nSW01_01      0.89      0.90    0.91      0.33 8.5   0.0078 0.014  0.34\nSW01_02      0.89      0.89    0.91      0.32 8.1   0.0081 0.013  0.32\nSW01_03      0.89      0.89    0.91      0.33 8.3   0.0080 0.014  0.32\nSW01_04      0.89      0.89    0.91      0.33 8.3   0.0080 0.013  0.33\nSW01_05      0.89      0.90    0.91      0.34 8.6   0.0077 0.012  0.33\nSW01_06      0.89      0.89    0.91      0.33 8.5   0.0078 0.014  0.33\nSW01_07      0.89      0.89    0.91      0.33 8.3   0.0080 0.013  0.33\nSW01_08      0.90      0.90    0.91      0.34 8.7   0.0077 0.013  0.34\nSW01_09      0.89      0.89    0.91      0.33 8.3   0.0080 0.014  0.32\nSW01_10      0.89      0.89    0.91      0.32 8.2   0.0081 0.014  0.32\nSW01_11      0.89      0.89    0.91      0.32 8.2   0.0081 0.013  0.32\nSW01_12      0.89      0.89    0.91      0.32 8.0   0.0082 0.012  0.32\nSW01_13      0.89      0.89    0.91      0.32 8.2   0.0081 0.013  0.32\nSW01_14      0.89      0.89    0.91      0.33 8.3   0.0080 0.014  0.33\nSW01_15      0.90      0.90    0.91      0.34 8.7   0.0077 0.012  0.33\nSW01_16      0.89      0.90    0.91      0.34 8.6   0.0077 0.011  0.33\nSW01_17      0.89      0.89    0.91      0.33 8.4   0.0079 0.013  0.33\nSW01_18      0.89      0.89    0.91      0.33 8.3   0.0080 0.013  0.33\n\n Item statistics \n          n raw.r std.r r.cor r.drop mean  sd\nSW01_01 398  0.54  0.54  0.51   0.48  3.9 1.3\nSW01_02 397  0.68  0.68  0.67   0.63  3.5 1.3\nSW01_03 398  0.63  0.64  0.61   0.58  4.4 1.2\nSW01_04 398  0.63  0.63  0.61   0.57  3.9 1.5\nSW01_05 398  0.53  0.52  0.50   0.45  4.1 1.5\nSW01_06 398  0.55  0.55  0.52   0.48  3.9 1.4\nSW01_07 398  0.63  0.63  0.62   0.57  4.1 1.3\nSW01_08 398  0.48  0.48  0.43   0.41  2.6 1.3\nSW01_09 398  0.62  0.61  0.58   0.55  3.6 1.5\nSW01_10 398  0.67  0.67  0.64   0.61  3.5 1.5\nSW01_11 398  0.66  0.67  0.65   0.61  4.4 1.3\nSW01_12 398  0.73  0.73  0.73   0.69  3.7 1.3\nSW01_13 398  0.67  0.67  0.66   0.61  4.0 1.4\nSW01_14 398  0.62  0.62  0.59   0.56  3.4 1.5\nSW01_15 398  0.50  0.50  0.46   0.43  3.8 1.4\nSW01_16 398  0.53  0.53  0.50   0.46  3.9 1.4\nSW01_17 398  0.58  0.58  0.55   0.52  3.5 1.3\nSW01_18 398  0.64  0.64  0.62   0.58  3.6 1.4\n\nNon missing response frequency for each item\n           0    1    2    3    4    5    6 miss\nSW01_01 0.01 0.05 0.10 0.17 0.31 0.31 0.06    0\nSW01_02 0.02 0.07 0.10 0.31 0.22 0.25 0.03    0\nSW01_03 0.01 0.02 0.06 0.17 0.23 0.36 0.17    0\nSW01_04 0.02 0.05 0.08 0.25 0.21 0.25 0.15    0\nSW01_05 0.01 0.06 0.09 0.17 0.21 0.26 0.20    0\nSW01_06 0.02 0.04 0.09 0.23 0.23 0.28 0.13    0\nSW01_07 0.01 0.03 0.07 0.23 0.26 0.27 0.14    0\nSW01_08 0.06 0.13 0.23 0.38 0.12 0.06 0.03    0\nSW01_09 0.03 0.07 0.10 0.28 0.25 0.17 0.11    0\nSW01_10 0.03 0.07 0.14 0.31 0.19 0.18 0.09    0\nSW01_11 0.01 0.02 0.03 0.24 0.22 0.24 0.25    0\nSW01_12 0.00 0.03 0.06 0.42 0.20 0.16 0.12    0\nSW01_13 0.00 0.04 0.08 0.28 0.20 0.23 0.17    0\nSW01_14 0.03 0.09 0.17 0.24 0.21 0.17 0.10    0\nSW01_15 0.01 0.06 0.10 0.23 0.24 0.25 0.12    0\nSW01_16 0.01 0.03 0.13 0.25 0.22 0.21 0.16    0\nSW01_17 0.01 0.04 0.13 0.37 0.21 0.18 0.06    0\nSW01_18 0.01 0.05 0.15 0.30 0.18 0.21 0.10    0\n\n\n\nReliability analysis   \nCall: psych::alpha(x = data[, SW_AD])\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r\n      0.85      0.85    0.85      0.38 5.5 0.012  3.8 0.92     0.38\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.82  0.85  0.87\nDuhachek  0.82  0.85  0.87\n\n Reliability if an item is dropped:\n        raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nSW01_01      0.84      0.84    0.83      0.39 5.1    0.012 0.0107  0.39\nSW01_02      0.82      0.82    0.81      0.36 4.4    0.014 0.0094  0.33\nSW01_03      0.83      0.83    0.83      0.38 5.0    0.013 0.0109  0.38\nSW01_04      0.82      0.82    0.82      0.36 4.6    0.014 0.0092  0.35\nSW01_05      0.83      0.83    0.82      0.38 4.9    0.013 0.0083  0.39\nSW01_06      0.83      0.83    0.83      0.39 5.1    0.012 0.0104  0.39\nSW01_07      0.82      0.82    0.81      0.36 4.5    0.014 0.0087  0.35\nSW01_08      0.84      0.85    0.84      0.41 5.5    0.012 0.0080  0.39\nSW01_09      0.83      0.83    0.83      0.38 5.0    0.013 0.0115  0.38\n\n Item statistics \n          n raw.r std.r r.cor r.drop mean  sd\nSW01_01 398  0.61  0.62  0.55   0.50  3.9 1.3\nSW01_02 397  0.77  0.78  0.76   0.69  3.5 1.3\nSW01_03 398  0.64  0.65  0.58   0.54  4.4 1.2\nSW01_04 398  0.74  0.73  0.70   0.64  3.9 1.5\nSW01_05 398  0.68  0.67  0.63   0.56  4.1 1.5\nSW01_06 398  0.63  0.63  0.56   0.51  3.9 1.4\nSW01_07 398  0.75  0.75  0.73   0.67  4.1 1.3\nSW01_08 398  0.53  0.54  0.44   0.40  2.6 1.3\nSW01_09 398  0.66  0.65  0.58   0.53  3.6 1.5\n\nNon missing response frequency for each item\n           0    1    2    3    4    5    6 miss\nSW01_01 0.01 0.05 0.10 0.17 0.31 0.31 0.06    0\nSW01_02 0.02 0.07 0.10 0.31 0.22 0.25 0.03    0\nSW01_03 0.01 0.02 0.06 0.17 0.23 0.36 0.17    0\nSW01_04 0.02 0.05 0.08 0.25 0.21 0.25 0.15    0\nSW01_05 0.01 0.06 0.09 0.17 0.21 0.26 0.20    0\nSW01_06 0.02 0.04 0.09 0.23 0.23 0.28 0.13    0\nSW01_07 0.01 0.03 0.07 0.23 0.26 0.27 0.14    0\nSW01_08 0.06 0.13 0.23 0.38 0.12 0.06 0.03    0\nSW01_09 0.03 0.07 0.10 0.28 0.25 0.17 0.11    0\n\n\n\nReliability analysis   \nCall: psych::alpha(x = data[, SW_HI])\n\n  raw_alpha std.alpha G6(smc) average_r S/N  ase mean   sd median_r\n      0.87      0.87    0.87      0.43 6.7 0.01  3.8 0.97     0.42\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.85  0.87  0.89\nDuhachek  0.85  0.87  0.89\n\n Reliability if an item is dropped:\n        raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nSW01_10      0.86      0.86    0.86      0.43 6.0    0.011 0.0073  0.41\nSW01_11      0.85      0.85    0.85      0.42 5.7    0.011 0.0059  0.41\nSW01_12      0.85      0.85    0.84      0.41 5.6    0.012 0.0059  0.41\nSW01_13      0.85      0.85    0.85      0.42 5.7    0.012 0.0069  0.41\nSW01_14      0.86      0.86    0.86      0.44 6.3    0.011 0.0079  0.44\nSW01_15      0.86      0.87    0.86      0.45 6.4    0.010 0.0065  0.44\nSW01_16      0.85      0.86    0.85      0.43 6.0    0.011 0.0082  0.41\nSW01_17      0.86      0.86    0.86      0.43 6.1    0.011 0.0083  0.43\nSW01_18      0.85      0.85    0.85      0.42 5.8    0.011 0.0086  0.40\n\n Item statistics \n          n raw.r std.r r.cor r.drop mean  sd\nSW01_10 398  0.69  0.69  0.63   0.59  3.5 1.5\nSW01_11 398  0.73  0.74  0.72   0.65  4.4 1.3\nSW01_12 398  0.77  0.77  0.75   0.70  3.7 1.3\nSW01_13 398  0.74  0.75  0.71   0.66  4.0 1.4\nSW01_14 398  0.66  0.65  0.58   0.54  3.4 1.5\nSW01_15 398  0.62  0.62  0.55   0.51  3.8 1.4\nSW01_16 398  0.70  0.70  0.65   0.60  3.9 1.4\nSW01_17 398  0.66  0.67  0.61   0.56  3.5 1.3\nSW01_18 398  0.73  0.73  0.69   0.64  3.6 1.4\n\nNon missing response frequency for each item\n           0    1    2    3    4    5    6 miss\nSW01_10 0.03 0.07 0.14 0.31 0.19 0.18 0.09    0\nSW01_11 0.01 0.02 0.03 0.24 0.22 0.24 0.25    0\nSW01_12 0.00 0.03 0.06 0.42 0.20 0.16 0.12    0\nSW01_13 0.00 0.04 0.08 0.28 0.20 0.23 0.17    0\nSW01_14 0.03 0.09 0.17 0.24 0.21 0.17 0.10    0\nSW01_15 0.01 0.06 0.10 0.23 0.24 0.25 0.12    0\nSW01_16 0.01 0.03 0.13 0.25 0.22 0.21 0.16    0\nSW01_17 0.01 0.04 0.13 0.37 0.21 0.18 0.06    0\nSW01_18 0.01 0.05 0.15 0.30 0.18 0.21 0.10    0\n\n\n\npsych::omega(data[SWAN_vars], nfactors = 2)\n\nOmega \nCall: omegah(m = m, nfactors = nfactors, fm = fm, key = key, flip = flip, \n    digits = digits, title = title, sl = sl, labels = labels, \n    plot = plot, n.obs = n.obs, rotate = rotate, Phi = Phi, option = option, \n    covar = covar)\nAlpha:                 0.9 \nG.6:                   0.92 \nOmega Hierarchical:    0.58 \nOmega H asymptotic:    0.63 \nOmega Total            0.91 \n\nSchmid Leiman Factor loadings greater than  0.2 \n           g   F1*   F2*   h2   h2   u2   p2  com\nSW01_01 0.40  0.34       0.28 0.28 0.72 0.56 2.06\nSW01_02 0.54  0.50       0.55 0.55 0.45 0.53 2.03\nSW01_03 0.49  0.32  0.20 0.38 0.38 0.62 0.62 2.12\nSW01_04 0.50  0.54       0.54 0.54 0.46 0.46 1.99\nSW01_05 0.40  0.52       0.44 0.44 0.56 0.37 1.95\nSW01_06 0.41  0.36       0.30 0.30 0.70 0.55 2.05\nSW01_07 0.50  0.55       0.55 0.55 0.45 0.45 1.98\nSW01_08 0.34  0.27       0.20 0.20 0.80 0.59 2.08\nSW01_09 0.46  0.35       0.36 0.36 0.64 0.60 2.10\nSW01_10 0.51        0.36 0.43 0.43 0.57 0.61 2.11\nSW01_11 0.52        0.46 0.49 0.49 0.51 0.55 2.05\nSW01_12 0.58        0.44 0.56 0.56 0.44 0.60 2.09\nSW01_13 0.52        0.49 0.51 0.51 0.49 0.53 2.03\nSW01_14 0.46        0.34 0.35 0.35 0.65 0.61 2.10\nSW01_15 0.35        0.44 0.32 0.32 0.68 0.38 1.95\nSW01_16 0.39        0.59 0.53 0.53 0.47 0.29 1.93\nSW01_17 0.44        0.43 0.38 0.38 0.62 0.50 2.01\nSW01_18 0.48        0.50 0.48 0.48 0.52 0.48 2.00\n\nWith Sums of squares  of:\n  g F1* F2*  h2 \n3.9 1.8 2.0 3.5 \n\ngeneral/max  1.13   max/min =   1.91\nmean percent general =  0.52    with sd =  0.09 and cv of  0.18 \nExplained Common Variance of the general factor =  0.51 \n\nThe degrees of freedom are 118  and the fit is  0.99 \nThe number of observations was  398  with Chi Square =  386  with prob &lt;  3.4e-30\nThe root mean square of the residuals is  0.05 \nThe df corrected root mean square of the residuals is  0.05\nRMSEA index =  0.075  and the 10 % confidence intervals are  0.067 0.084\nBIC =  -320\n\nCompare this with the adequacy of just a general factor and no group factors\nThe degrees of freedom for just the general factor are 135  and the fit is  2.52 \nThe number of observations was  398  with Chi Square =  980  with prob &lt;  3.5e-128\nThe root mean square of the residuals is  0.15 \nThe df corrected root mean square of the residuals is  0.16 \n\nRMSEA index =  0.125  and the 10 % confidence intervals are  0.118 0.133\nBIC =  172 \n\nMeasures of factor score adequacy             \n                                                 g  F1*  F2*\nCorrelation of scores with factors            0.76 0.73 0.74\nMultiple R square of scores with factors      0.58 0.54 0.54\nMinimum correlation of factor score estimates 0.17 0.08 0.09\n\n Total, General and Subset omega for each subset\n                                                 g  F1*  F2*\nOmega total for total scores and subscales    0.91 0.84 0.87\nOmega general for total scores and subscales  0.58 0.45 0.46\nOmega group for total scores and subscales    0.26 0.39 0.41\n\n\n\n\n\n\n\n\n\n\n# Correlation Matrix\ncorr.test(data[, c(SW_AD, \"SW_AD_mean\")]) # 0.53 - 0.77\ncorr.test(data[, c(SW_HI, \"SW_HI_mean\")]) # 0.62 - 0.77\ncorr.test(data[, c(SWAN_vars, \"SW_mean\")]) # 0.48 - 0.73\n\nCall:corr.test(x = data[, c(SW_AD, \"SW_AD_mean\")])\nCorrelation matrix \n           SW01_01 SW01_02 SW01_03 SW01_04 SW01_05 SW01_06 SW01_07 SW01_08\nSW01_01       1.00    0.49    0.32    0.31    0.38    0.36    0.32    0.22\nSW01_02       0.49    1.00    0.47    0.53    0.38    0.53    0.46    0.41\nSW01_03       0.32    0.47    1.00    0.44    0.32    0.31    0.40    0.23\nSW01_04       0.31    0.53    0.44    1.00    0.53    0.39    0.54    0.30\nSW01_05       0.38    0.38    0.32    0.53    1.00    0.28    0.61    0.20\nSW01_06       0.36    0.53    0.31    0.39    0.28    1.00    0.40    0.27\nSW01_07       0.32    0.46    0.40    0.54    0.61    0.40    1.00    0.33\nSW01_08       0.22    0.41    0.23    0.30    0.20    0.27    0.33    1.00\nSW01_09       0.33    0.40    0.41    0.39    0.34    0.28    0.48    0.29\nSW_AD_mean    0.61    0.77    0.64    0.74    0.68    0.63    0.75    0.53\n           SW01_09 SW_AD_mean\nSW01_01       0.33       0.61\nSW01_02       0.40       0.77\nSW01_03       0.41       0.64\nSW01_04       0.39       0.74\nSW01_05       0.34       0.68\nSW01_06       0.28       0.63\nSW01_07       0.48       0.75\nSW01_08       0.29       0.53\nSW01_09       1.00       0.66\nSW_AD_mean    0.66       1.00\nSample Size \n           SW01_01 SW01_02 SW01_03 SW01_04 SW01_05 SW01_06 SW01_07 SW01_08\nSW01_01        398     397     398     398     398     398     398     398\nSW01_02        397     397     397     397     397     397     397     397\nSW01_03        398     397     398     398     398     398     398     398\nSW01_04        398     397     398     398     398     398     398     398\nSW01_05        398     397     398     398     398     398     398     398\nSW01_06        398     397     398     398     398     398     398     398\nSW01_07        398     397     398     398     398     398     398     398\nSW01_08        398     397     398     398     398     398     398     398\nSW01_09        398     397     398     398     398     398     398     398\nSW_AD_mean     398     397     398     398     398     398     398     398\n           SW01_09 SW_AD_mean\nSW01_01        398        398\nSW01_02        397        397\nSW01_03        398        398\nSW01_04        398        398\nSW01_05        398        398\nSW01_06        398        398\nSW01_07        398        398\nSW01_08        398        398\nSW01_09        398        398\nSW_AD_mean     398        398\nProbability values (Entries above the diagonal are adjusted for multiple tests.) \n           SW01_01 SW01_02 SW01_03 SW01_04 SW01_05 SW01_06 SW01_07 SW01_08\nSW01_01          0       0       0       0       0       0       0       0\nSW01_02          0       0       0       0       0       0       0       0\nSW01_03          0       0       0       0       0       0       0       0\nSW01_04          0       0       0       0       0       0       0       0\nSW01_05          0       0       0       0       0       0       0       0\nSW01_06          0       0       0       0       0       0       0       0\nSW01_07          0       0       0       0       0       0       0       0\nSW01_08          0       0       0       0       0       0       0       0\nSW01_09          0       0       0       0       0       0       0       0\nSW_AD_mean       0       0       0       0       0       0       0       0\n           SW01_09 SW_AD_mean\nSW01_01          0          0\nSW01_02          0          0\nSW01_03          0          0\nSW01_04          0          0\nSW01_05          0          0\nSW01_06          0          0\nSW01_07          0          0\nSW01_08          0          0\nSW01_09          0          0\nSW_AD_mean       0          0\n\n To see confidence intervals of the correlations, print with the short=FALSE option\n\n\nCall:corr.test(x = data[, c(SW_HI, \"SW_HI_mean\")])\nCorrelation matrix \n           SW01_10 SW01_11 SW01_12 SW01_13 SW01_14 SW01_15 SW01_16 SW01_17\nSW01_10       1.00    0.49    0.58    0.42    0.44    0.30    0.33    0.34\nSW01_11       0.49    1.00    0.61    0.61    0.40    0.30    0.42    0.47\nSW01_12       0.58    0.61    1.00    0.55    0.50    0.34    0.39    0.41\nSW01_13       0.42    0.61    0.55    1.00    0.37    0.38    0.48    0.37\nSW01_14       0.44    0.40    0.50    0.37    1.00    0.32    0.32    0.36\nSW01_15       0.30    0.30    0.34    0.38    0.32    1.00    0.50    0.32\nSW01_16       0.33    0.42    0.39    0.48    0.32    0.50    1.00    0.46\nSW01_17       0.34    0.47    0.41    0.37    0.36    0.32    0.46    1.00\nSW01_18       0.44    0.38    0.49    0.51    0.37    0.44    0.49    0.47\nSW_HI_mean    0.69    0.73    0.77    0.74    0.66    0.62    0.70    0.66\n           SW01_18 SW_HI_mean\nSW01_10       0.44       0.69\nSW01_11       0.38       0.73\nSW01_12       0.49       0.77\nSW01_13       0.51       0.74\nSW01_14       0.37       0.66\nSW01_15       0.44       0.62\nSW01_16       0.49       0.70\nSW01_17       0.47       0.66\nSW01_18       1.00       0.73\nSW_HI_mean    0.73       1.00\nSample Size \n[1] 398\nProbability values (Entries above the diagonal are adjusted for multiple tests.) \n           SW01_10 SW01_11 SW01_12 SW01_13 SW01_14 SW01_15 SW01_16 SW01_17\nSW01_10          0       0       0       0       0       0       0       0\nSW01_11          0       0       0       0       0       0       0       0\nSW01_12          0       0       0       0       0       0       0       0\nSW01_13          0       0       0       0       0       0       0       0\nSW01_14          0       0       0       0       0       0       0       0\nSW01_15          0       0       0       0       0       0       0       0\nSW01_16          0       0       0       0       0       0       0       0\nSW01_17          0       0       0       0       0       0       0       0\nSW01_18          0       0       0       0       0       0       0       0\nSW_HI_mean       0       0       0       0       0       0       0       0\n           SW01_18 SW_HI_mean\nSW01_10          0          0\nSW01_11          0          0\nSW01_12          0          0\nSW01_13          0          0\nSW01_14          0          0\nSW01_15          0          0\nSW01_16          0          0\nSW01_17          0          0\nSW01_18          0          0\nSW_HI_mean       0          0\n\n To see confidence intervals of the correlations, print with the short=FALSE option\n\n\nCall:corr.test(x = data[, c(SWAN_vars, \"SW_mean\")])\nCorrelation matrix \n        SW01_01 SW01_02 SW01_03 SW01_04 SW01_05 SW01_06 SW01_07 SW01_08\nSW01_01    1.00    0.49    0.32    0.31    0.38    0.36    0.32    0.22\nSW01_02    0.49    1.00    0.47    0.53    0.38    0.53    0.46    0.41\nSW01_03    0.32    0.47    1.00    0.44    0.32    0.31    0.40    0.23\nSW01_04    0.31    0.53    0.44    1.00    0.53    0.39    0.54    0.30\nSW01_05    0.38    0.38    0.32    0.53    1.00    0.28    0.61    0.20\nSW01_06    0.36    0.53    0.31    0.39    0.28    1.00    0.40    0.27\nSW01_07    0.32    0.46    0.40    0.54    0.61    0.40    1.00    0.33\nSW01_08    0.22    0.41    0.23    0.30    0.20    0.27    0.33    1.00\nSW01_09    0.33    0.40    0.41    0.39    0.34    0.28    0.48    0.29\nSW01_10    0.34    0.43    0.34    0.38    0.23    0.30    0.29    0.28\nSW01_11    0.22    0.35    0.41    0.35    0.21    0.25    0.29    0.19\nSW01_12    0.30    0.38    0.38    0.42    0.28    0.35    0.37    0.26\nSW01_13    0.22    0.35    0.40    0.29    0.23    0.24    0.33    0.18\nSW01_14    0.24    0.32    0.33    0.31    0.26    0.20    0.29    0.34\nSW01_15    0.29    0.13    0.21    0.12    0.12    0.19    0.15    0.14\nSW01_16    0.17    0.18    0.28    0.09    0.03    0.17    0.09    0.15\nSW01_17    0.17    0.28    0.37    0.24    0.16    0.23    0.28    0.17\nSW01_18    0.25    0.34    0.30    0.21    0.17    0.28    0.25    0.28\nSW_mean    0.54    0.68    0.63    0.63    0.53    0.55    0.63    0.48\n        SW01_09 SW01_10 SW01_11 SW01_12 SW01_13 SW01_14 SW01_15 SW01_16\nSW01_01    0.33    0.34    0.22    0.30    0.22    0.24    0.29    0.17\nSW01_02    0.40    0.43    0.35    0.38    0.35    0.32    0.13    0.18\nSW01_03    0.41    0.34    0.41    0.38    0.40    0.33    0.21    0.28\nSW01_04    0.39    0.38    0.35    0.42    0.29    0.31    0.12    0.09\nSW01_05    0.34    0.23    0.21    0.28    0.23    0.26    0.12    0.03\nSW01_06    0.28    0.30    0.25    0.35    0.24    0.20    0.19    0.17\nSW01_07    0.48    0.29    0.29    0.37    0.33    0.29    0.15    0.09\nSW01_08    0.29    0.28    0.19    0.26    0.18    0.34    0.14    0.15\nSW01_09    1.00    0.35    0.33    0.38    0.36    0.37    0.20    0.23\nSW01_10    0.35    1.00    0.49    0.58    0.42    0.44    0.30    0.33\nSW01_11    0.33    0.49    1.00    0.61    0.61    0.40    0.30    0.42\nSW01_12    0.38    0.58    0.61    1.00    0.55    0.50    0.34    0.39\nSW01_13    0.36    0.42    0.61    0.55    1.00    0.37    0.38    0.48\nSW01_14    0.37    0.44    0.40    0.50    0.37    1.00    0.32    0.32\nSW01_15    0.20    0.30    0.30    0.34    0.38    0.32    1.00    0.50\nSW01_16    0.23    0.33    0.42    0.39    0.48    0.32    0.50    1.00\nSW01_17    0.27    0.34    0.47    0.41    0.37    0.36    0.32    0.46\nSW01_18    0.27    0.44    0.38    0.49    0.51    0.37    0.44    0.49\nSW_mean    0.62    0.67    0.66    0.73    0.67    0.62    0.50    0.53\n        SW01_17 SW01_18 SW_mean\nSW01_01    0.17    0.25    0.54\nSW01_02    0.28    0.34    0.68\nSW01_03    0.37    0.30    0.63\nSW01_04    0.24    0.21    0.63\nSW01_05    0.16    0.17    0.53\nSW01_06    0.23    0.28    0.55\nSW01_07    0.28    0.25    0.63\nSW01_08    0.17    0.28    0.48\nSW01_09    0.27    0.27    0.62\nSW01_10    0.34    0.44    0.67\nSW01_11    0.47    0.38    0.66\nSW01_12    0.41    0.49    0.73\nSW01_13    0.37    0.51    0.67\nSW01_14    0.36    0.37    0.62\nSW01_15    0.32    0.44    0.50\nSW01_16    0.46    0.49    0.53\nSW01_17    1.00    0.47    0.58\nSW01_18    0.47    1.00    0.64\nSW_mean    0.58    0.64    1.00\nSample Size \n        SW01_01 SW01_02 SW01_03 SW01_04 SW01_05 SW01_06 SW01_07 SW01_08\nSW01_01     398     397     398     398     398     398     398     398\nSW01_02     397     397     397     397     397     397     397     397\nSW01_03     398     397     398     398     398     398     398     398\nSW01_04     398     397     398     398     398     398     398     398\nSW01_05     398     397     398     398     398     398     398     398\nSW01_06     398     397     398     398     398     398     398     398\nSW01_07     398     397     398     398     398     398     398     398\nSW01_08     398     397     398     398     398     398     398     398\nSW01_09     398     397     398     398     398     398     398     398\nSW01_10     398     397     398     398     398     398     398     398\nSW01_11     398     397     398     398     398     398     398     398\nSW01_12     398     397     398     398     398     398     398     398\nSW01_13     398     397     398     398     398     398     398     398\nSW01_14     398     397     398     398     398     398     398     398\nSW01_15     398     397     398     398     398     398     398     398\nSW01_16     398     397     398     398     398     398     398     398\nSW01_17     398     397     398     398     398     398     398     398\nSW01_18     398     397     398     398     398     398     398     398\nSW_mean     398     397     398     398     398     398     398     398\n        SW01_09 SW01_10 SW01_11 SW01_12 SW01_13 SW01_14 SW01_15 SW01_16\nSW01_01     398     398     398     398     398     398     398     398\nSW01_02     397     397     397     397     397     397     397     397\nSW01_03     398     398     398     398     398     398     398     398\nSW01_04     398     398     398     398     398     398     398     398\nSW01_05     398     398     398     398     398     398     398     398\nSW01_06     398     398     398     398     398     398     398     398\nSW01_07     398     398     398     398     398     398     398     398\nSW01_08     398     398     398     398     398     398     398     398\nSW01_09     398     398     398     398     398     398     398     398\nSW01_10     398     398     398     398     398     398     398     398\nSW01_11     398     398     398     398     398     398     398     398\nSW01_12     398     398     398     398     398     398     398     398\nSW01_13     398     398     398     398     398     398     398     398\nSW01_14     398     398     398     398     398     398     398     398\nSW01_15     398     398     398     398     398     398     398     398\nSW01_16     398     398     398     398     398     398     398     398\nSW01_17     398     398     398     398     398     398     398     398\nSW01_18     398     398     398     398     398     398     398     398\nSW_mean     398     398     398     398     398     398     398     398\n        SW01_17 SW01_18 SW_mean\nSW01_01     398     398     398\nSW01_02     397     397     397\nSW01_03     398     398     398\nSW01_04     398     398     398\nSW01_05     398     398     398\nSW01_06     398     398     398\nSW01_07     398     398     398\nSW01_08     398     398     398\nSW01_09     398     398     398\nSW01_10     398     398     398\nSW01_11     398     398     398\nSW01_12     398     398     398\nSW01_13     398     398     398\nSW01_14     398     398     398\nSW01_15     398     398     398\nSW01_16     398     398     398\nSW01_17     398     398     398\nSW01_18     398     398     398\nSW_mean     398     398     398\nProbability values (Entries above the diagonal are adjusted for multiple tests.) \n        SW01_01 SW01_02 SW01_03 SW01_04 SW01_05 SW01_06 SW01_07 SW01_08\nSW01_01       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_02       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_03       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_04       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_05       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_06       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_07       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_08       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_09       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_10       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_11       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_12       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_13       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_14       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_15       0    0.01       0    0.02    0.02       0    0.00       0\nSW01_16       0    0.00       0    0.06    0.59       0    0.06       0\nSW01_17       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_18       0    0.00       0    0.00    0.00       0    0.00       0\nSW_mean       0    0.00       0    0.00    0.00       0    0.00       0\n        SW01_09 SW01_10 SW01_11 SW01_12 SW01_13 SW01_14 SW01_15 SW01_16\nSW01_01       0       0       0       0       0       0    0.00    0.01\nSW01_02       0       0       0       0       0       0    0.07    0.01\nSW01_03       0       0       0       0       0       0    0.00    0.00\nSW01_04       0       0       0       0       0       0    0.11    0.18\nSW01_05       0       0       0       0       0       0    0.11    0.59\nSW01_06       0       0       0       0       0       0    0.00    0.01\nSW01_07       0       0       0       0       0       0    0.02    0.18\nSW01_08       0       0       0       0       0       0    0.03    0.02\nSW01_09       0       0       0       0       0       0    0.00    0.00\nSW01_10       0       0       0       0       0       0    0.00    0.00\nSW01_11       0       0       0       0       0       0    0.00    0.00\nSW01_12       0       0       0       0       0       0    0.00    0.00\nSW01_13       0       0       0       0       0       0    0.00    0.00\nSW01_14       0       0       0       0       0       0    0.00    0.00\nSW01_15       0       0       0       0       0       0    0.00    0.00\nSW01_16       0       0       0       0       0       0    0.00    0.00\nSW01_17       0       0       0       0       0       0    0.00    0.00\nSW01_18       0       0       0       0       0       0    0.00    0.00\nSW_mean       0       0       0       0       0       0    0.00    0.00\n        SW01_17 SW01_18 SW_mean\nSW01_01    0.01    0.00       0\nSW01_02    0.00    0.00       0\nSW01_03    0.00    0.00       0\nSW01_04    0.00    0.00       0\nSW01_05    0.02    0.01       0\nSW01_06    0.00    0.00       0\nSW01_07    0.00    0.00       0\nSW01_08    0.01    0.00       0\nSW01_09    0.00    0.00       0\nSW01_10    0.00    0.00       0\nSW01_11    0.00    0.00       0\nSW01_12    0.00    0.00       0\nSW01_13    0.00    0.00       0\nSW01_14    0.00    0.00       0\nSW01_15    0.00    0.00       0\nSW01_16    0.00    0.00       0\nSW01_17    0.00    0.00       0\nSW01_18    0.00    0.00       0\nSW_mean    0.00    0.00       0\n\n To see confidence intervals of the correlations, print with the short=FALSE option\n\n\n\n# Model 1: Bifactor Model\n# Model specification\nswan_model_2 &lt;- \"\nSW_GF =~ NA*SW01_01 + SW01_02 + SW01_03 + SW01_04 + SW01_05 + SW01_06 + SW01_07 +\n    SW01_08 + SW01_09 + SW01_10 + SW01_11 + SW01_12 + SW01_13 + SW01_14 + SW01_15 +\n    SW01_16 + SW01_17 + SW01_18;\nSW_AD =~ NA*SW01_01 + SW01_02 + SW01_03 + SW01_04 + SW01_05 + SW01_06 + SW01_07 +\n    SW01_08 + SW01_09; SW_HI =~ NA*SW01_10 + SW01_11 + SW01_12 + SW01_13 + SW01_14 +\n    SW01_15 + SW01_16 + SW01_17 + SW01_18;\nSW_GF ~~ 1*SW_GF; SW_AD ~~ 1*SW_AD; SW_HI ~~ 1*SW_HI; SW_GF ~~ 0*SW_AD;\nSW_AD ~~ 0*SW_HI; SW_HI ~~ 0*SW_GF\n\"\n\n\n# Model calculation\nswan_m2_cfa &lt;- cfa(swan_model_2,\n    data = data,\n    std.lv = TRUE,\n    missing = \"fiml\",\n    estimator = \"MLR\"\n)\n# Summary\nsummary(swan_m2_cfa, standardized = TRUE, fit = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 66 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        72\n\n  Number of observations                           398\n  Number of missing patterns                         2\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                               352.858     289.028\n  Degrees of freedom                               117         117\n  P-value (Chi-square)                           0.000       0.000\n  Scaling correction factor                                  1.221\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              2928.538    2257.491\n  Degrees of freedom                               153         153\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.297\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.915       0.918\n  Tucker-Lewis Index (TLI)                       0.889       0.893\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.925\n  Robust Tucker-Lewis Index (TLI)                            0.902\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -11152.042  -11152.042\n  Scaling correction factor                                  1.231\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -10975.613  -10975.613\n  Scaling correction factor                                  1.225\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               22448.083   22448.083\n  Bayesian (BIC)                             22735.108   22735.108\n  Sample-size adjusted Bayesian (SABIC)      22506.649   22506.649\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.071       0.061\n  90 Percent confidence interval - lower         0.063       0.053\n  90 Percent confidence interval - upper         0.080       0.069\n  P-value H_0: RMSEA &lt;= 0.050                    0.000       0.014\n  P-value H_0: RMSEA &gt;= 0.080                    0.045       0.000\n                                                                  \n  Robust RMSEA                                               0.066\n  90 Percent confidence interval - lower                     0.056\n  90 Percent confidence interval - upper                     0.076\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.005\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.012\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.042       0.042\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  SW_GF =~                                                              \n    SW01_01           0.511    0.089    5.754    0.000    0.511    0.393\n    SW01_02           0.743    0.121    6.161    0.000    0.743    0.563\n    SW01_03           0.676    0.076    8.842    0.000    0.676    0.543\n    SW01_04           0.747    0.107    6.989    0.000    0.747    0.513\n    SW01_05           0.496    0.095    5.207    0.000    0.496    0.331\n    SW01_06           0.602    0.106    5.673    0.000    0.602    0.427\n    SW01_07           0.587    0.083    7.087    0.000    0.587    0.452\n    SW01_08           0.485    0.102    4.761    0.000    0.485    0.367\n    SW01_09           0.749    0.089    8.409    0.000    0.749    0.507\n    SW01_10           1.033    0.075   13.834    0.000    1.033    0.698\n    SW01_11           0.931    0.095    9.751    0.000    0.931    0.712\n    SW01_12           0.991    0.079   12.618    0.000    0.991    0.793\n    SW01_13           0.900    0.103    8.718    0.000    0.900    0.650\n    SW01_14           0.913    0.082   11.154    0.000    0.913    0.595\n    SW01_15           0.518    0.130    3.975    0.000    0.518    0.361\n    SW01_16           0.587    0.160    3.667    0.000    0.587    0.418\n    SW01_17           0.636    0.084    7.588    0.000    0.636    0.508\n    SW01_18           0.780    0.099    7.865    0.000    0.780    0.554\n  SW_AD =~                                                              \n    SW01_01           0.451    0.106    4.248    0.000    0.451    0.346\n    SW01_02           0.569    0.166    3.420    0.001    0.569    0.431\n    SW01_03           0.346    0.109    3.166    0.002    0.346    0.278\n    SW01_04           0.749    0.098    7.639    0.000    0.749    0.514\n    SW01_05           0.945    0.107    8.810    0.000    0.945    0.630\n    SW01_06           0.487    0.139    3.511    0.000    0.487    0.345\n    SW01_07           0.806    0.083    9.754    0.000    0.806    0.621\n    SW01_08           0.319    0.110    2.902    0.004    0.319    0.241\n    SW01_09           0.470    0.096    4.914    0.000    0.470    0.318\n  SW_HI =~                                                              \n    SW01_10           0.078    0.161    0.482    0.630    0.078    0.053\n    SW01_11           0.217    0.219    0.993    0.321    0.217    0.166\n    SW01_12           0.127    0.192    0.663    0.507    0.127    0.102\n    SW01_13           0.426    0.188    2.261    0.024    0.426    0.308\n    SW01_14           0.171    0.157    1.087    0.277    0.171    0.112\n    SW01_15           0.731    0.120    6.108    0.000    0.731    0.510\n    SW01_16           0.940    0.127    7.401    0.000    0.940    0.669\n    SW01_17           0.432    0.111    3.878    0.000    0.432    0.345\n    SW01_18           0.589    0.126    4.670    0.000    0.589    0.419\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  SW_GF ~~                                                              \n    SW_AD             0.000                               0.000    0.000\n  SW_AD ~~                                                              \n    SW_HI             0.000                               0.000    0.000\n  SW_GF ~~                                                              \n    SW_HI             0.000                               0.000    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .SW01_01           3.889    0.065   59.577    0.000    3.889    2.986\n   .SW01_02           3.529    0.066   53.265    0.000    3.529    2.674\n   .SW01_03           4.354    0.062   69.765    0.000    4.354    3.497\n   .SW01_04           3.920    0.073   53.716    0.000    3.920    2.693\n   .SW01_05           4.088    0.075   54.417    0.000    4.088    2.728\n   .SW01_06           3.942    0.071   55.693    0.000    3.942    2.792\n   .SW01_07           4.093    0.065   62.954    0.000    4.093    3.156\n   .SW01_08           2.643    0.066   39.895    0.000    2.643    2.000\n   .SW01_09           3.578    0.074   48.304    0.000    3.578    2.421\n   .SW01_10           3.457    0.074   46.630    0.000    3.457    2.337\n   .SW01_11           4.372    0.066   66.679    0.000    4.372    3.342\n   .SW01_12           3.741    0.063   59.687    0.000    3.741    2.992\n   .SW01_13           3.982    0.069   57.385    0.000    3.982    2.876\n   .SW01_14           3.440    0.077   44.746    0.000    3.440    2.243\n   .SW01_15           3.814    0.072   53.068    0.000    3.814    2.660\n   .SW01_16           3.897    0.070   55.295    0.000    3.897    2.772\n   .SW01_17           3.515    0.063   56.031    0.000    3.515    2.809\n   .SW01_18           3.606    0.071   51.085    0.000    3.606    2.561\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    SW_GF             1.000                               1.000    1.000\n    SW_AD             1.000                               1.000    1.000\n    SW_HI             1.000                               1.000    1.000\n   .SW01_01           1.232    0.103   11.957    0.000    1.232    0.726\n   .SW01_02           0.866    0.093    9.326    0.000    0.866    0.497\n   .SW01_03           0.974    0.082   11.901    0.000    0.974    0.628\n   .SW01_04           1.000    0.108    9.273    0.000    1.000    0.472\n   .SW01_05           1.107    0.187    5.905    0.000    1.107    0.493\n   .SW01_06           1.394    0.107   13.018    0.000    1.394    0.699\n   .SW01_07           0.689    0.110    6.242    0.000    0.689    0.409\n   .SW01_08           1.410    0.117   12.010    0.000    1.410    0.807\n   .SW01_09           1.401    0.123   11.414    0.000    1.401    0.642\n   .SW01_10           1.115    0.110   10.161    0.000    1.115    0.510\n   .SW01_11           0.798    0.087    9.133    0.000    0.798    0.466\n   .SW01_12           0.565    0.083    6.808    0.000    0.565    0.361\n   .SW01_13           0.925    0.086   10.779    0.000    0.925    0.482\n   .SW01_14           1.490    0.130   11.447    0.000    1.490    0.633\n   .SW01_15           1.254    0.153    8.218    0.000    1.254    0.610\n   .SW01_16           0.748    0.144    5.194    0.000    0.748    0.378\n   .SW01_17           0.976    0.097   10.010    0.000    0.976    0.623\n   .SW01_18           1.026    0.113    9.121    0.000    1.026    0.518\n\n\n\n\nmi &lt;- modindices(swan_m2_cfa, minimum.value = 10, sort = TRUE)\nprint(mi)\n\n        lhs op     rhs   mi    epc sepc.lv sepc.all sepc.nox\n120 SW01_02 ~~ SW01_06 28.3  0.326   0.326    0.297    0.297\n163 SW01_05 ~~ SW01_07 26.2  0.401   0.401    0.460    0.460\n226 SW01_11 ~~ SW01_13 24.3  0.252   0.252    0.293    0.293\n231 SW01_11 ~~ SW01_18 17.9 -0.227  -0.227   -0.251   -0.251\n113 SW01_01 ~~ SW01_15 17.9  0.288   0.288    0.232    0.232\n100 SW01_01 ~~ SW01_02 17.8  0.242   0.242    0.235    0.235\n119 SW01_02 ~~ SW01_05 15.5 -0.258  -0.258   -0.263   -0.263\n121 SW01_02 ~~ SW01_07 13.8 -0.198  -0.198   -0.256   -0.256\n126 SW01_02 ~~ SW01_12 12.6 -0.159  -0.159   -0.228   -0.228\n88    SW_AD =~ SW01_16 12.0 -0.261  -0.261   -0.185   -0.185\n162 SW01_05 ~~ SW01_06 11.8 -0.265  -0.265   -0.213   -0.213\n94    SW_HI =~ SW01_04 10.6 -0.235  -0.235   -0.162   -0.162\n105 SW01_01 ~~ SW01_07 10.3 -0.187  -0.187   -0.203   -0.203\n203 SW01_08 ~~ SW01_14 10.2  0.246   0.246    0.169    0.169\n\n\n\n# Model 2: Bifactor Model with Modification Items 5&7 and 2&6\n# Model specification\nswan_model_3 &lt;- \"\n    SW_GF =~ NA*SW01_01 + SW01_02 + SW01_03 + SW01_04 + SW01_05 + SW01_06 + \n        SW01_07 + SW01_08 + SW01_09 + SW01_10 + SW01_11 + SW01_12 + SW01_13 + \n        SW01_14 + SW01_15 + SW01_16 + SW01_17 + SW01_18; \n    SW_AD =~ NA*SW01_01 + SW01_02 + SW01_03 + SW01_04 + SW01_05 + SW01_06 + \n        SW01_07 + SW01_08 + SW01_09; \n    SW_HI =~ NA*SW01_10 + SW01_11 + SW01_12 + SW01_13 + SW01_14 + SW01_15 + \n        SW01_16 + SW01_17 + SW01_18; SW_GF ~~ 1*SW_GF; \n    SW_AD ~~ 1*SW_AD; \n    SW_HI ~~ 1*SW_HI; \n    SW_GF ~~ 0*SW_AD; \n    SW_AD ~~ 0*SW_HI; \n    SW_HI ~~ 0*SW_GF; \n    SW01_05 ~~ SW01_07; \n    SW01_02 ~~ SW01_06\n\"\n\n\n# Model calculation\nswan_m3_cfa &lt;- cfa(swan_model_3,\n    data = data,\n    std.lv = TRUE,\n    missing = \"fiml\",\n    estimator = \"MLR\"\n)\n\n# Summary\ns &lt;- summary(swan_m3_cfa, standardized = TRUE, fit = TRUE) # standardised factor loading is std.all\ns |&gt; print()\n\nlavaan 0.6-19 ended normally after 63 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        74\n\n  Number of observations                           398\n  Number of missing patterns                         2\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                               308.602     248.615\n  Degrees of freedom                               115         115\n  P-value (Chi-square)                           0.000       0.000\n  Scaling correction factor                                  1.241\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              2928.538    2257.491\n  Degrees of freedom                               153         153\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.297\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.930       0.937\n  Tucker-Lewis Index (TLI)                       0.907       0.916\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.941\n  Robust Tucker-Lewis Index (TLI)                            0.921\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -11129.914  -11129.914\n  Scaling correction factor                                  1.199\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -10975.613  -10975.613\n  Scaling correction factor                                  1.225\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               22407.828   22407.828\n  Bayesian (BIC)                             22702.825   22702.825\n  Sample-size adjusted Bayesian (SABIC)      22468.020   22468.020\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.065       0.054\n  90 Percent confidence interval - lower         0.056       0.046\n  90 Percent confidence interval - upper         0.074       0.062\n  P-value H_0: RMSEA &lt;= 0.050                    0.003       0.204\n  P-value H_0: RMSEA &gt;= 0.080                    0.002       0.000\n                                                                  \n  Robust RMSEA                                               0.059\n  90 Percent confidence interval - lower                     0.049\n  90 Percent confidence interval - upper                     0.070\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.069\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.040       0.040\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  SW_GF =~                                                              \n    SW01_01           0.490    0.081    6.054    0.000    0.490    0.376\n    SW01_02           0.700    0.085    8.216    0.000    0.700    0.531\n    SW01_03           0.661    0.071    9.378    0.000    0.661    0.531\n    SW01_04           0.728    0.094    7.717    0.000    0.728    0.500\n    SW01_05           0.505    0.096    5.254    0.000    0.505    0.337\n    SW01_06           0.572    0.085    6.726    0.000    0.572    0.405\n    SW01_07           0.595    0.082    7.255    0.000    0.595    0.459\n    SW01_08           0.466    0.091    5.122    0.000    0.466    0.353\n    SW01_09           0.741    0.085    8.714    0.000    0.741    0.501\n    SW01_10           1.026    0.075   13.696    0.000    1.026    0.694\n    SW01_11           0.947    0.075   12.631    0.000    0.947    0.724\n    SW01_12           1.009    0.061   16.426    0.000    1.009    0.807\n    SW01_13           0.922    0.079   11.671    0.000    0.922    0.666\n    SW01_14           0.919    0.079   11.582    0.000    0.919    0.599\n    SW01_15           0.542    0.111    4.860    0.000    0.542    0.378\n    SW01_16           0.614    0.138    4.449    0.000    0.614    0.437\n    SW01_17           0.650    0.072    9.054    0.000    0.650    0.519\n    SW01_18           0.794    0.089    8.875    0.000    0.794    0.564\n  SW_AD =~                                                              \n    SW01_01           0.498    0.093    5.342    0.000    0.498    0.383\n    SW01_02           0.656    0.108    6.056    0.000    0.656    0.497\n    SW01_03           0.399    0.091    4.398    0.000    0.399    0.321\n    SW01_04           0.778    0.095    8.199    0.000    0.778    0.535\n    SW01_05           0.803    0.112    7.197    0.000    0.803    0.536\n    SW01_06           0.495    0.098    5.071    0.000    0.495    0.351\n    SW01_07           0.691    0.096    7.156    0.000    0.691    0.532\n    SW01_08           0.368    0.092    4.002    0.000    0.368    0.278\n    SW01_09           0.492    0.091    5.391    0.000    0.492    0.333\n  SW_HI =~                                                              \n    SW01_10           0.055    0.131    0.421    0.674    0.055    0.037\n    SW01_11           0.174    0.167    1.041    0.298    0.174    0.133\n    SW01_12           0.078    0.134    0.582    0.560    0.078    0.063\n    SW01_13           0.385    0.141    2.736    0.006    0.385    0.278\n    SW01_14           0.141    0.127    1.110    0.267    0.141    0.092\n    SW01_15           0.712    0.120    5.959    0.000    0.712    0.497\n    SW01_16           0.929    0.140    6.624    0.000    0.929    0.661\n    SW01_17           0.412    0.098    4.193    0.000    0.412    0.329\n    SW01_18           0.569    0.119    4.768    0.000    0.569    0.404\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  SW_GF ~~                                                              \n    SW_AD             0.000                               0.000    0.000\n  SW_AD ~~                                                              \n    SW_HI             0.000                               0.000    0.000\n  SW_GF ~~                                                              \n    SW_HI             0.000                               0.000    0.000\n .SW01_05 ~~                                                            \n   .SW01_07           0.323    0.102    3.158    0.002    0.323    0.301\n .SW01_02 ~~                                                            \n   .SW01_06           0.263    0.082    3.198    0.001    0.263    0.243\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .SW01_01           3.889    0.065   59.577    0.000    3.889    2.986\n   .SW01_02           3.529    0.066   53.343    0.000    3.529    2.676\n   .SW01_03           4.354    0.062   69.765    0.000    4.354    3.497\n   .SW01_04           3.920    0.073   53.716    0.000    3.920    2.693\n   .SW01_05           4.088    0.075   54.417    0.000    4.088    2.728\n   .SW01_06           3.942    0.071   55.693    0.000    3.942    2.792\n   .SW01_07           4.093    0.065   62.954    0.000    4.093    3.156\n   .SW01_08           2.643    0.066   39.895    0.000    2.643    2.000\n   .SW01_09           3.578    0.074   48.304    0.000    3.578    2.421\n   .SW01_10           3.457    0.074   46.630    0.000    3.457    2.337\n   .SW01_11           4.372    0.066   66.679    0.000    4.372    3.342\n   .SW01_12           3.741    0.063   59.687    0.000    3.741    2.992\n   .SW01_13           3.982    0.069   57.385    0.000    3.982    2.876\n   .SW01_14           3.440    0.077   44.746    0.000    3.440    2.243\n   .SW01_15           3.814    0.072   53.068    0.000    3.814    2.660\n   .SW01_16           3.897    0.070   55.295    0.000    3.897    2.772\n   .SW01_17           3.515    0.063   56.031    0.000    3.515    2.809\n   .SW01_18           3.606    0.071   51.085    0.000    3.606    2.561\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    SW_GF             1.000                               1.000    1.000\n    SW_AD             1.000                               1.000    1.000\n    SW_HI             1.000                               1.000    1.000\n   .SW01_01           1.208    0.104   11.630    0.000    1.208    0.712\n   .SW01_02           0.820    0.104    7.848    0.000    0.820    0.471\n   .SW01_03           0.954    0.081   11.782    0.000    0.954    0.615\n   .SW01_04           0.983    0.121    8.150    0.000    0.983    0.464\n   .SW01_05           1.346    0.156    8.646    0.000    1.346    0.599\n   .SW01_06           1.422    0.107   13.257    0.000    1.422    0.713\n   .SW01_07           0.851    0.106    8.032    0.000    0.851    0.506\n   .SW01_08           1.395    0.117   11.930    0.000    1.395    0.798\n   .SW01_09           1.393    0.123   11.303    0.000    1.393    0.638\n   .SW01_10           1.132    0.112   10.084    0.000    1.132    0.518\n   .SW01_11           0.784    0.084    9.295    0.000    0.784    0.458\n   .SW01_12           0.540    0.069    7.828    0.000    0.540    0.345\n   .SW01_13           0.918    0.086   10.667    0.000    0.918    0.479\n   .SW01_14           1.487    0.130   11.479    0.000    1.487    0.632\n   .SW01_15           1.255    0.156    8.049    0.000    1.255    0.611\n   .SW01_16           0.736    0.162    4.547    0.000    0.736    0.373\n   .SW01_17           0.975    0.098    9.989    0.000    0.975    0.622\n   .SW01_18           1.028    0.117    8.795    0.000    1.028    0.519\n\n\n\n\n# Figure Structural Model (Model 3)\nm &lt;- matrix(nrow = 18, ncol = 3)\nm[, 1] &lt;- c(rep(0, 4), \"SW_A\", rep(0, 8), \"SW_H\", rep(0, 4))\nm[, 2] &lt;- c(SWAN_vars)\nm[, 3] &lt;- c(rep(0, 9), \"SW_G\", rep(0, 8))\n\nstr_model &lt;- semPaths(swan_m3_cfa,\n    layout = m,\n    intercepts = FALSE,\n    what = \"std\",\n    style = \"lisrel\",\n    edge.color = \"grey10\",\n    fade = FALSE,\n    edge.label.cex = 0.6,\n    sizeMan = 6,\n    sizeInt = 5,\n    sizeLat = 8,\n    sizeMan2 = 3,\n    esize = 1,\n    residuals = FALSE,\n    curvePivot = FALSE\n)\n\n\n\n\n\n\n\n\n\n\n\n\nBlume, F., Buhr, L., Kuehnhausen, J., Köpke, R., Weber, L. A., Fallgatter, A. J., Ethofer, T., & Gawrilow, C. (2020). Validation of the Self-Report Version of the German Strengths and Weaknesses of ADHD Symptoms and Normal Behavior Scale (SWAN-DE-SB). Assessment, 10731911241236699.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html",
    "href": "chapters/sem/01_sem_intro.html",
    "title": "48  Introduzione ai Modelli SEM",
    "section": "",
    "text": "48.1 Introduzione\nLa ricerca in psicologia si basa sull’indagine di costrutti teorici che, essendo non direttamente osservabili, possono essere studiati solo in modo indiretto attraverso le risposte dei partecipanti a indicatori osservabili (ad esempio, le risposte a item di un questionario). I capitoli precedenti sull’analisi fattoriale hanno illustrato come valutare la struttura fattoriale di questi costrutti latenti, identificando quali item siano buoni indicatori per misurare tali costrutti. Questo processo è fondamentale per sviluppare strumenti validi e affidabili volti a quantificare i costrutti latenti che caratterizzano la ricerca psicologica. Tuttavia, una buona misurazione non rappresenta quasi mai il fine ultimo: i ricercatori desiderano solitamente esplorare le relazioni tra costrutti, confrontare differenze medie o rispondere a domande del tipo: “La self-compassion è un fattore protettivo contro il burnout?”.\nQuando i ricercatori dispongono di sole variabili osservate come predittori e outcome, e desiderano esaminare gli effetti di uno o più predittori su un singolo outcome, possono utilizzare metodi di analisi familiari, come la regressione multipla. Tuttavia, se le domande di ricerca implicano costrutti latenti o richiedono di testare sistemi complessi di relazioni tra variabili, è necessario ricorrere a tecniche di analisi più flessibili in grado di modellare simultaneamente relazioni tra variabili osservate e latenti: questo è il campo dei Modelli di Equazioni Strutturali (SEM, Structural Equation Modeling).\nAlla base dei SEM si trova una combinazione di analisi fattoriale e analisi dei percorsi (path analysis). L’analisi dei percorsi può essere vista come un’estensione della regressione multipla, poiché consente di stimare e testare effetti diretti tra variabili. Tuttavia, a differenza della regressione multipla, che si concentra sugli effetti diretti di uno o più predittori su un unico outcome, l’analisi dei percorsi consente di analizzare sia effetti diretti sia indiretti tra interi insiemi di variabili predittive e di outcome in modo simultaneo. Questo approccio permette a una variabile di fungere contemporaneamente da predittore e da outcome: una variabile può essere prevista da una o più altre variabili, mentre a sua volta funge da predittore per altre variabili. In altre parole, l’analisi dei percorsi offre la possibilità di costruire modelli complessi, purché tutte le variabili siano osservate.\nL’inclusione di variabili latenti richiede però di andare oltre l’analisi dei percorsi tradizionale. Grazie al lavoro pionieristico di Jöreskog e Van Thillo, è stato possibile integrare variabili latenti nei modelli di percorsi, dando vita a quella che oggi conosciamo come SEM. Questo approccio è stato reso progressivamente più accessibile da software sempre più intuitivi, contribuendo alla diffusione dei SEM nella psicologia e nelle scienze sociali.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#introduzione",
    "href": "chapters/sem/01_sem_intro.html#introduzione",
    "title": "48  Introduzione ai Modelli SEM",
    "section": "",
    "text": "48.1.1 Struttura e Obiettivi dei SEM\nUn modello SEM tipico si compone di due parti principali:\n\n\nLa parte di misurazione, che collega i costrutti latenti a un insieme di variabili osservate o indicatori.\n\n\nLa parte strutturale, che modella le relazioni ipotizzate tra i costrutti latenti.\n\nL’obiettivo principale dei SEM è testare ipotesi teoriche specifiche tramite modelli che rappresentano le previsioni di tali ipotesi, utilizzando costrutti misurati attraverso variabili osservabili appropriate. I SEM fungono così da ponte tra teoria e osservazione, consentendo di tradurre concetti astratti in entità misurabili e di analizzarne le relazioni in modo sistematico e coerente con la teoria.\n\n48.1.2 Considerazioni Critiche sull’Uso dei SEM\nNonostante la loro potenza, i SEM richiedono un uso critico e consapevole. Ogni modello statistico è una semplificazione della realtà, come sottolineato dal celebre aforisma: “Tutti i modelli sono sbagliati, ma alcuni sono utili”. Questo implica che un buon adattamento ai dati non garantisce una rappresentazione accurata della realtà. Modelli intrinsecamente imprecisi possono adattarsi bene ai dati, portando a conclusioni errate. Pertanto, la scelta dei modelli non deve essere un semplice esercizio statistico, ma un processo orientato allo sviluppo e al raffinamento di teorie valide. La revisione critica dei modelli, basata su evidenze empiriche e solidi principi teorici, è essenziale per un progresso scientifico affidabile.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#modelli-di-regressione-e-introduzione-ai-sem",
    "href": "chapters/sem/01_sem_intro.html#modelli-di-regressione-e-introduzione-ai-sem",
    "title": "48  Introduzione ai Modelli SEM",
    "section": "\n48.2 Modelli di Regressione e Introduzione ai SEM",
    "text": "48.2 Modelli di Regressione e Introduzione ai SEM\nIn questo capitolo, i Modelli di Equazioni Strutturali (SEM) vengono introdotti partendo dal caso più semplice: il modello di regressione multipla, reinterpretato e rappresentato all’interno di un framework SEM. L’obiettivo è fornire un ripasso rigoroso del modello di regressione lineare, evidenziando come possa essere formalizzato e implementato come un caso particolare di un modello SEM. Questa rappresentazione permette di estendere il concetto di regressione multipla a modelli più complessi che includono variabili latenti e relazioni strutturali.\nUtilizzeremo dati empirici per illustrare l’approccio, focalizzandoci sulla Self-Sompassion Scale e sulle tre sottoscale del DASS-21: ansia, stress e depressione. Il campione analizzato comprende 526 studenti universitari iscritti a corsi di psicologia.\nLe sottoscale del DASS-21 rappresentano variabili osservate che misurano concetti teorici distinti ma correlati. L’obiettivo è esplorare come il punteggio totale della Self-Compassion possa essere predetto dalle tre sottoscale del DASS-21 utilizzando un approccio di regressione multipla. Successivamente, questo modello sarà riformulato e stimato come un caso specifico di SEM.\nL’implementazione in R mediante il pacchetto lavaan consentirà di confrontare i risultati della regressione tradizionale con quelli ottenuti dalla rappresentazione SEM, illustrando i vantaggi di quest’ultimo approccio, come la maggiore flessibilità e la possibilità di incorporare errori di misura nelle variabili osservate.\n\n48.2.1 Preliminari\nImportiamo i dati:\n\ndat &lt;- read.csv(\n    here::here(\"data\", \"dass_rosenberg_scs.csv\"),\n    header = TRUE\n)\ndat |&gt;\n    head()\n#&gt;   stress anxiety depression rosenberg self_kindness common_humanity\n#&gt; 1      7       6          4        31            17              16\n#&gt; 2      3       2          1        32            14              14\n#&gt; 3      1       0          1        31            20              16\n#&gt; 4     12      11         13        34            12               6\n#&gt; 5     10       6         12        25            16              17\n#&gt; 6      5       1          2        31            14              14\n#&gt;   mindfulness self_judgment isolation over_identification scs_ts\n#&gt; 1          16            11         8                  10     98\n#&gt; 2          16            16        11                  13     82\n#&gt; 3          16            13         6                   9    102\n#&gt; 4           6            10         7                  15     70\n#&gt; 5          13            17        16                  18     73\n#&gt; 6          10            12         8                  11     85\n\n\ndim(dat)\n#&gt; [1] 526  11\n\nSelezioniamo le variabili di interesse:\n\nd_mr &lt;- dat |&gt;\n    dplyr::select(stress, anxiety, depression, scs_ts)\n\nEsaminiamo i diagrammi di dispersione tra le varie misure per verificare che la relazione tra le variabili sia lineare.\n\npairs(d_mr)\n\n\n\n\n\n\n\nConvertiamo i dati in formato matriciale:\n\ny &lt;- d_mr$scs_ts |&gt; as.matrix()\ndim(y)\n#&gt; [1] 526   1",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#modello-di-regressione-lineare-multipla",
    "href": "chapters/sem/01_sem_intro.html#modello-di-regressione-lineare-multipla",
    "title": "48  Introduzione ai Modelli SEM",
    "section": "\n48.3 Modello di Regressione Lineare Multipla",
    "text": "48.3 Modello di Regressione Lineare Multipla\nIl modello generale di regressione lineare multipla (MLR) può essere espresso attraverso la seguente equazione:\n\\[\ny_i = \\beta_0 + \\beta_1 x_{1i} + \\cdots + \\beta_p x_{pi} + \\epsilon_i,\n\\]\ndove:\n\n\n\\(i = 1, \\ldots, N\\) identifica l’\\(i\\)-esima osservazione,\n\n\n\\(\\beta_0\\) è l’intercetta del modello,\n\n\n\\(\\beta_1, \\ldots, \\beta_p\\) sono i coefficienti di regressione associati alle variabili indipendenti,\n\n\n\\(\\epsilon_i\\) è il termine di errore per l’\\(i\\)-esima osservazione,\n\nSi assume che \\(\\epsilon_i\\) sia indipendente dalle variabili esplicative \\(x_{1i}, \\ldots, x_{pi}\\) e distribuito con media zero e varianza costante \\(\\sigma^2\\).\n\nIn questa formulazione, \\(y_i\\) rappresenta il valore della variabile dipendente per l’\\(i\\)-esima osservazione, mentre i coefficienti \\(\\beta\\) quantificano l’effetto delle variabili indipendenti \\(x_{1i}, \\ldots, x_{pi}\\) su \\(y_i\\). Il termine di errore \\(\\epsilon_i\\) cattura la varianza non spiegata dal modello lineare. Questa struttura consente di modellare relazioni lineari tra una variabile dipendente e più variabili indipendenti, fornendo una base per effettuare inferenze sui parametri \\(\\beta\\).\n\n48.3.1 Forma Matriciale del Modello\nIl modello MLR può essere rappresentato in forma matriciale come:\n\\[\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon},\n\\]\ndove:\n\n\n\\(\\mathbf{y}\\) è un vettore \\(N \\times 1\\) contenente i valori osservati della variabile dipendente,\n\n\n\\(\\mathbf{X}\\) è una matrice di progettazione \\(N \\times (p+1)\\) che include le \\(p\\) variabili indipendenti e una colonna di uni per l’intercetta,\n\n\n\\(\\boldsymbol{\\beta}\\) è un vettore \\((p+1) \\times 1\\) dei coefficienti di regressione (inclusa l’intercetta),\n\n\n\\(\\boldsymbol{\\epsilon}\\) è un vettore \\(N \\times 1\\) che rappresenta i termini di errore.\n\nLe componenti del modello sono definite come segue:\n\\[\n\\mathbf{y} =\n\\begin{pmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_N\n\\end{pmatrix}, \\quad\n\\boldsymbol{\\epsilon} =\n\\begin{pmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\vdots \\\\\n\\epsilon_N\n\\end{pmatrix}, \\quad\n\\mathbf{X} =\n\\begin{pmatrix}\n1 & x_{11} & \\cdots & x_{p1} \\\\\n1 & x_{12} & \\cdots & x_{p2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{1N} & \\cdots & x_{pN}\n\\end{pmatrix}.\n\\]\nOgni riga della matrice \\(\\mathbf{X}\\) rappresenta un’osservazione e include i valori delle variabili indipendenti per quella osservazione, oltre a un uno per l’intercetta.\n\n48.3.2 Metodo dei Minimi Quadrati\nIl metodo dei minimi quadrati (Least Squares Estimation, LSE) mira a stimare i parametri \\(\\boldsymbol{\\beta}\\) minimizzando la somma dei quadrati degli errori (SSE), definita come:\n\\[\n\\text{SSE} = \\boldsymbol{\\epsilon}'\\boldsymbol{\\epsilon} = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})'(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}).\n\\]\nEspandendo questa espressione:\n\\[\n\\text{SSE} = \\mathbf{y}'\\mathbf{y} - 2\\boldsymbol{\\beta}'\\mathbf{X}'\\mathbf{y} + \\boldsymbol{\\beta}'\\mathbf{X}'\\mathbf{X}\\boldsymbol{\\beta}.\n\\]\nMinimizzando la SSE rispetto a \\(\\boldsymbol{\\beta}\\) e ponendo la derivata prima pari a zero, si ottiene il sistema normale:\n\\[\n\\mathbf{X}'\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}'\\mathbf{y}.\n\\]\nSe la matrice \\(\\mathbf{X}'\\mathbf{X}\\) è invertibile, la soluzione per i coefficienti stimati è:\n\\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}.\n\\]\nQuesta soluzione fornisce le stime dei coefficienti di regressione che minimizzano la discrepanza tra i valori osservati \\(\\mathbf{y}\\) e quelli predetti \\(\\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\) dal modello. In tal modo, si ottengono le migliori stime lineari e non distorte dei parametri, sotto le ipotesi classiche di regressione.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#regressione-multipla-in-r",
    "href": "chapters/sem/01_sem_intro.html#regressione-multipla-in-r",
    "title": "48  Introduzione ai Modelli SEM",
    "section": "\n48.4 Regressione Multipla in R",
    "text": "48.4 Regressione Multipla in R\nApplichiamo il modello di regressione lineare multipla (MLR) ai dati disponibili, utilizzando sia la formulazione matriciale sia le funzioni predefinite di R. Come esempio, analizziamo le relazioni tra depressione, ansia, stress e una variabile dipendente (scs_ts) nei dati.\n\n48.4.1 Preparazione dei Dati\nSelezioniamo le variabili di interesse dal dataset:\n\ndass &lt;- d_mr |&gt;\n    dplyr::select(depression, anxiety, stress)\n\nCreiamo la matrice di progettazione \\(\\mathbf{X}\\), includendo una colonna di uni per l’intercetta:\n\nX &lt;- model.matrix(~ depression + anxiety + stress, data = dass)\nhead(X)\n#&gt;   (Intercept) depression anxiety stress\n#&gt; 1           1          4       6      7\n#&gt; 2           1          1       2      3\n#&gt; 3           1          1       0      1\n#&gt; 4           1         13      11     12\n#&gt; 5           1         12       6     10\n#&gt; 6           1          2       1      5\n\n\n48.4.2 Stima dei Coefficienti con la Formula Matriciale\nCalcoliamo i coefficienti \\(\\boldsymbol{\\beta}\\) utilizzando la formula dei minimi quadrati:\n\\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}.\n\\]\nIn R, il calcolo viene effettuato come segue:\n\ny &lt;- d_mr$scs_ts\nbeta &lt;- solve(t(X) %*% X) %*% t(X) %*% y\nbeta\n#&gt;                [,1]\n#&gt; (Intercept) 91.3611\n#&gt; depression  -1.4842\n#&gt; anxiety      1.0493\n#&gt; stress      -0.9733\n\n\n48.4.3 Verifica dei Risultati con lm()\n\nConfrontiamo i risultati ottenuti con la funzione lm():\n\nfm &lt;- lm(scs_ts ~ depression + anxiety + stress, data = d_mr)\ncoef(fm) \n#&gt; (Intercept)  depression     anxiety      stress \n#&gt;     91.3611     -1.4842      1.0493     -0.9733\n\nI coefficienti stimati con il metodo matriciale e quelli calcolati da lm() devono coincidere.\n\n48.4.4 Valori Predetti e Residui\nCalcoliamo i valori predetti \\(\\hat{y}\\) utilizzando i coefficienti stimati:\n\nyhat &lt;- X %*% beta\ncor(yhat, fm$fitted.values) \n#&gt;      [,1]\n#&gt; [1,]    1\n\nCalcoliamo i residui \\(e = \\mathbf{y} - \\hat{\\mathbf{y}}\\):\n\ne &lt;- d_mr$scs_ts - yhat\ncor(e, fm$residuals) \n#&gt;      [,1]\n#&gt; [1,]    1\n\n\n48.4.5 Somma dei Quadrati dei Residui\nLa somma dei quadrati dei residui (RSS) è definita come:\n\\[\n\\text{RSS} = \\mathbf{e}'\\mathbf{e}.\n\\]\nIn R:\n\nRSS &lt;- t(e) %*% e\nRSS \n#&gt;        [,1]\n#&gt; [1,] 128700\n\n\n48.4.6 Stima della Varianza dei Residui\nLa stima della varianza dei residui è data da:\n\\[\n\\hat{\\sigma}^2 = \\frac{\\text{RSS}}{N - (p+1)},\n\\]\ndove \\(N\\) è il numero di osservazioni e \\(p+1\\) è il numero di parametri stimati (inclusa l’intercetta):\n\nvar_e &lt;- RSS / (length(y) - dim(X)[2])\nvar_e \n#&gt;       [,1]\n#&gt; [1,] 246.6\n\n\n48.4.7 Errore Standard della Regressione\nL’errore standard della regressione, \\(\\hat{\\sigma}\\), è la radice quadrata della varianza dei residui:\n\nsqrt(var_e)\n#&gt;      [,1]\n#&gt; [1,] 15.7\n\nVerifichiamo questi risultati con il sommario del modello lm():\n\nsummary(fm)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = scs_ts ~ depression + anxiety + stress, data = d_mr)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -36.80 -12.00  -0.35  10.74  43.67 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)\n#&gt; (Intercept)   91.361      1.623   56.29  &lt; 2e-16\n#&gt; depression    -1.484      0.238   -6.25  8.7e-10\n#&gt; anxiety        1.049      0.210    4.99  8.2e-07\n#&gt; stress        -0.973      0.255   -3.81  0.00015\n#&gt; \n#&gt; Residual standard error: 15.7 on 522 degrees of freedom\n#&gt; Multiple R-squared:  0.247,  Adjusted R-squared:  0.243 \n#&gt; F-statistic: 57.1 on 3 and 522 DF,  p-value: &lt;2e-16\n\n\n48.4.8 Coefficiente di Determinazione \\(R^2\\)\n\nIl coefficiente di determinazione \\(R^2\\) misura la proporzione della varianza spiegata dal modello rispetto alla varianza totale:\n\\[\nR^2 = \\frac{\\sum (\\hat{y}_i - \\bar{y})^2}{\\sum (y_i - \\bar{y})^2}.\n\\]\nIn R, calcoliamo \\(R^2\\) come segue:\n\nR2 &lt;- (sum((yhat - mean(y))^2)) / (sum((y - mean(y))^2)) \nR2 \n#&gt; [1] 0.247\n\nIn conclusione, questo esempio dimostra come implementare un modello di regressione multipla sia utilizzando la formulazione matriciale sia ricorrendo a funzioni predefinite di R. Il confronto tra i due approcci evidenzia la coerenza dei risultati e offre un’utile comprensione del funzionamento interno dei metodi di regressione.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#modello-di-percorso",
    "href": "chapters/sem/01_sem_intro.html#modello-di-percorso",
    "title": "48  Introduzione ai Modelli SEM",
    "section": "\n48.5 Modello di Percorso",
    "text": "48.5 Modello di Percorso\nPassiamo ora al cuore di questo capitolo: la rappresentazione del modello di regressione multipla come un caso speciale di Modello di Equazioni Strutturali (SEM). In precedenza, il modello di regressione è stato descritto nei termini di un modello statistico stimato mediante il metodo della massima verosimiglianza (ML). Questo metodo, sotto l’ipotesi di normalità multivariata, coincide con il metodo dei minimi quadrati ordinari (OLS). Tuttavia, nei modelli SEM, le relazioni tra le variabili possono essere più complesse rispetto a quelle di un modello di regressione multipla. Di conseguenza, non esistono formule analitiche esplicite per stimare i coefficienti del modello.\n\n48.5.1 Stima nei Modelli SEM\nNei modelli di equazioni strutturali (SEM), la stima dei parametri non avviene attraverso l’applicazione diretta di formule analitiche (come abbiamo visto in precedenza), ma si basa su un processo di ottimizzazione numerica iterativa. È importante comprendere che, quando parliamo di massima verosimiglianza (ML) o minimi quadrati generalizzati (GLS), non ci riferiamo a soluzioni analitiche chiuse, ma a funzioni obiettivo che l’algoritmo cerca di ottimizzare.\nIl processo funziona così:\n\nSi parte da valori iniziali dei parametri (spesso basati su stime preliminari).\nL’algoritmo calcola la discrepanza tra la matrice di covarianza osservata (S) e quella predetta dal modello (Σ) con i parametri correnti.\nBasandosi su questa discrepanza, l’algoritmo aggiusta i parametri in una direzione che dovrebbe ridurre la differenza.\nSi ripetono i passi 2 e 3 finché la discrepanza non può essere ulteriormente ridotta in modo significativo.\n\nLa funzione di discrepanza (o funzione di costo) può essere basata su diversi criteri, come la verosimiglianza o la somma dei quadrati delle differenze, ma in tutti i casi l’obiettivo è trovare i valori dei parametri che la minimizzano attraverso successive approssimazioni. Non esiste una formula diretta per trovare questi valori - l’algoritmo “esplora” iterativamente lo spazio dei parametri cercando il punto di minimo della funzione di costo.\nQuesta natura iterativa del processo di stima ha importanti implicazioni pratiche:\n\nL’algoritmo potrebbe non convergere a una soluzione.\nPotrebbe convergere a un minimo locale invece che globale.\nIl tempo di calcolo aumenta con la complessità del modello.\nLa scelta dei valori iniziali può influenzare il risultato finale.\n\nLa differenza fondamentale rispetto a metodi analitici diretti (come la regressione lineare semplice) è che non esiste una formula chiusa per calcolare i parametri ottimali, ma si procede per successive approssimazioni guidate dalla riduzione di una funzione di costo.\n\n48.5.2 Rappresentazione di un Modello di Percorso con lavaan\n\nPer illustrare l’equivalenza tra un modello di regressione multipla e un SEM, riformuliamo il modello di regressione come un modello di percorso utilizzando la sintassi del pacchetto lavaan in R.\nDefiniamo il modello:\n\nmod_mr &lt;- \"\n  scs_ts ~ anxiety + depression + stress\n\"\n\nIn questo caso:\n\n\nscs_ts rappresenta la variabile dipendente (ad esempio, una misura di self-compassion),\n\n\nanxiety, depression, e stress sono le variabili predittive.\n\nAdattiamo il modello ai dati disponibili:\n\nfit_mr &lt;- lavaan::sem(mod_mr, d_mr)\n\nIl comando lavaan::sem() specifica che vogliamo stimare il modello utilizzando l’approccio SEM.\nEsaminiamo i parametri stimati:\n\nparameterEstimates(fit_mr) \n#&gt;           lhs op        rhs     est     se      z pvalue ci.lower ci.upper\n#&gt; 1      scs_ts  ~    anxiety   1.049  0.209  5.010      0    0.639    1.460\n#&gt; 2      scs_ts  ~ depression  -1.484  0.237 -6.271      0   -1.948   -1.020\n#&gt; 3      scs_ts  ~     stress  -0.973  0.254 -3.827      0   -1.472   -0.475\n#&gt; 4      scs_ts ~~     scs_ts 244.677 15.087 16.217      0  215.106  274.247\n#&gt; 5     anxiety ~~    anxiety  32.082  0.000     NA     NA   32.082   32.082\n#&gt; 6     anxiety ~~ depression  24.546  0.000     NA     NA   24.546   24.546\n#&gt; 7     anxiety ~~     stress  24.538  0.000     NA     NA   24.538   24.538\n#&gt; 8  depression ~~ depression  31.418  0.000     NA     NA   31.418   31.418\n#&gt; 9  depression ~~     stress  25.662  0.000     NA     NA   25.662   25.662\n#&gt; 10     stress ~~     stress  29.714  0.000     NA     NA   29.714   29.714\n\nI parametri stimati includono:\n\nI coefficienti di regressione che rappresentano le relazioni tra la variabile dipendente e i predittori.\n\nL’errore standard associato a ciascun parametro stimato.\n\nIl valore p (che può essere ignorato in un’analisi bayesiana o interpretato con cautela).\n\n48.5.3 Confronto con il Modello di Regressione Multipla\nI parametri stimati da lavaan risultano praticamente identici a quelli ottenuti con il metodo della massima verosimiglianza per il modello di regressione multipla. Questo è coerente con il fatto che il modello di regressione multipla è un caso particolare di SEM in cui tutte le variabili sono osservate e non vi sono variabili latenti o relazioni complesse.\nIn conclusione, rappresentare un modello di regressione multipla come un modello di percorso SEM evidenzia l’equivalenza metodologica tra i due approcci nei casi più semplici. Tuttavia, l’approccio SEM offre maggiore flessibilità, permettendo di includere variabili latenti, relazioni indirette, e modelli più complessi, che non possono essere gestiti con il semplice framework della regressione multipla. Questa flessibilità rende SEM uno strumento indispensabile per analisi avanzate in psicologia e scienze sociali.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#modelli-sem-e-scomposizione-della-covarianza",
    "href": "chapters/sem/01_sem_intro.html#modelli-sem-e-scomposizione-della-covarianza",
    "title": "48  Introduzione ai Modelli SEM",
    "section": "\n48.6 Modelli SEM e Scomposizione della Covarianza",
    "text": "48.6 Modelli SEM e Scomposizione della Covarianza\n\n48.6.1 Principio Fondamentale\nI modelli SEM mirano a spiegare le covarianze osservate nei dati attraverso una rete di relazioni causali dirette e indirette tra variabili. Questa rete è rappresentata da coefficienti di percorso che, opportunamente combinati, permettono di ricostruire la struttura di covarianza dei dati osservati.\n\n48.6.2 Esempio Pratico\nPrendiamo come esempio la relazione tra self-compassion (misurata dal punteggio totale) e ansia (misurata dal DASS-21). La covarianza osservata tra queste due variabili può essere scomposta in diversi percorsi causali:\n\n\nEffetto Diretto:\n\nIl coefficiente che rappresenta l’influenza diretta dell’ansia sulla self-compassion\n\n\n\nEffetti Indiretti:\n\nVia depressione: l’ansia è correlata con la depressione, che a sua volta influenza la self-compassion\nVia stress: l’ansia è correlata con lo stress, che a sua volta influenza la self-compassion\n\n\n\n48.6.3 Calcolo della Covarianza Predetta\nLa covarianza totale tra ansia e self-compassion viene calcolata combinando questi percorsi attraverso la seguente formula:\nCovarianza_Predetta = \n    (Effetto_Diretto × Varianza_Ansia) +\n    (Coefficiente_Depressione × Covarianza_Ansia_Depressione) +\n    (Coefficiente_Stress × Covarianza_Ansia_Stress)\nNel nostro modello specifico:\n\n# Coefficienti di percorso\nbeta_anxiety_scs_ts &lt;- 1.0493140    # Effetto diretto ansia → self-compassion\nbeta_depression_scs_ts &lt;- -1.4841573 # Effetto diretto depressione → self-compassion\nbeta_stress_scs_ts &lt;- -0.9733368    # Effetto diretto stress → self-compassion\n\n# Covarianze tra predittori\ncov_anxiety_depression &lt;- 24.5464225\ncov_anxiety_stress &lt;- 24.5381096\ncov_depression_stress &lt;- 25.6615608\n\n# Varianze dei predittori\nvar_anxiety &lt;- 32.0817418\nvar_depression &lt;- 31.4182365\nvar_stress &lt;- 29.7137880\n\n# Calcolo della covarianza predetta\npredicted_cov_anxiety_scs_ts &lt;- \n    beta_anxiety_scs_ts * var_anxiety +\n    beta_depression_scs_ts * cov_anxiety_depression +\n    beta_stress_scs_ts * cov_anxiety_stress\n\npredicted_cov_anxiety_scs_ts\n#&gt; [1] -26.65\n\n\n48.6.4 Verifica del Modello\nLa bontà del modello può essere verificata confrontando la covarianza predetta con quella osservata nei dati:\n\n# Covarianza osservata nei dati\ncov(d_mr$anxiety, d_mr$scs_ts)\n#&gt; [1] -26.7\n\nQuesto procedimento di scomposizione viene applicato a tutti gli elementi della matrice di varianza/covarianza, permettendo di:\n\nComprendere i meccanismi attraverso cui le variabili si influenzano reciprocamente\nQuantificare l’importanza relativa dei diversi percorsi causali\nValidare la struttura teorica del modello confrontando le covarianze predette con quelle osservate\n\nLa visualizzazione del modello attraverso il grafico semPaths aiuta a rappresentare questa rete di relazioni in modo intuitivo, mostrando i coefficienti di percorso stimati per ogni relazione.\n\nsemPaths(fit_mr,\n    whatLabels = \"est\",\n    sizeMan = 10,\n    edge.label.cex = 1.15,\n    style = \"mx\",\n    nCharNodes = 0, nCharEdges = 0\n)",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#errore-di-specificazione",
    "href": "chapters/sem/01_sem_intro.html#errore-di-specificazione",
    "title": "48  Introduzione ai Modelli SEM",
    "section": "\n48.7 Errore di Specificazione",
    "text": "48.7 Errore di Specificazione\nConosciuto spiritosamente come “heartbreak of L.O.V.E.” (Left-Out Variable Error; Mauro (1990)), l’errore di specificazione rappresenta una problematica fondamentale nei modelli di regressione, che deve essere sempre considerata attentamente durante l’interpretazione dei risultati.\nL’errore di specificazione si verifica quando una variabile viene esclusa dal modello di regressione, e tale variabile soddisfa entrambe le seguenti condizioni:\n\n\nÈ associata ad altre variabili incluse nel modello.\n\n\nHa un effetto diretto sulla variabile dipendente (\\(y\\)).\n\nQuando ciò accade, i coefficienti di regressione stimati per le variabili incluse nel modello risultano distorti in termini sia di intensità sia di segno. Questo fenomeno può portare a conclusioni errate sull’effetto delle variabili indipendenti sulla variabile dipendente.\n\n48.7.1 Un Esempio con Dati Simulati\nConsideriamo un esempio in cui la prestazione (performance) è positivamente associata alla motivazione (motivation) e negativamente all’ansia (anxiety). Inoltre, supponiamo che ansia e motivazione siano positivamente correlate. Vogliamo osservare come il coefficiente della variabile “motivazione” cambi se “ansia” viene esclusa dal modello.\nCreiamo i dati simulati:\n\nset.seed(123)\nn &lt;- 400\n\nanxiety &lt;- rnorm(n, 10, 1.5)\nmotivation &lt;- 4.0 * anxiety + rnorm(n, 0, 3.5)\ncor(anxiety, motivation)\n#&gt; [1] 0.8618\n\nLa variabile performance è definita come una combinazione lineare di motivation e anxiety, con un effetto positivo ma piccolo della motivazione e un effetto negativo marcato dell’ansia:\n\nperformance &lt;- 0.5 * motivation - 5.0 * anxiety + rnorm(n, 0, 3)\n\nSalviamo i dati in un data frame:\n\nsim_dat2 &lt;- tibble(performance, motivation, anxiety)\nsim_dat2 |&gt; head() |&gt; print()\n#&gt; # A tibble: 6 × 3\n#&gt;   performance motivation anxiety\n#&gt;         &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1       -26.5       36.4    9.16\n#&gt; 2       -33.0       34.5    9.65\n#&gt; 3       -35.6       47.1   12.3 \n#&gt; 4       -26.9       40.3   10.1 \n#&gt; 5       -28.6       43.1   10.2 \n#&gt; 6       -40.2       44.5   12.6\n\n\n48.7.1.1 Modello corretto\nAdattiamo un modello di regressione che includa entrambi i predittori (motivation e anxiety):\n\nfm1 &lt;- lm(performance ~ motivation + anxiety, sim_dat2)\ncoef(fm1) |&gt; print()\n#&gt; (Intercept)  motivation     anxiety \n#&gt;      1.3712      0.4954     -5.1052\n\nLe stime dei coefficienti di regressione riflettono correttamente i parametri utilizzati per generare i dati.\n\n48.7.1.2 Modello con specificazione errata\nOra escludiamo il predittore anxiety e stimiamo il modello solo con motivation:\n\nfm2 &lt;- lm(performance ~ motivation, sim_dat2)\nsummary(fm2) |&gt; print()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = performance ~ motivation, data = sim_dat2)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -13.501  -3.409   0.005   3.311  12.616 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)\n#&gt; (Intercept) -12.3972     1.4459   -8.57  2.2e-16\n#&gt; motivation   -0.4372     0.0355  -12.31  &lt; 2e-16\n#&gt; \n#&gt; Residual standard error: 4.87 on 398 degrees of freedom\n#&gt; Multiple R-squared:  0.276,  Adjusted R-squared:  0.274 \n#&gt; F-statistic:  151 on 1 and 398 DF,  p-value: &lt;2e-16\n\nIn questo caso, il segno del coefficiente di regressione per motivation è invertito rispetto al modello generatore dei dati. Questo è un tipico esempio di errore di specificazione.\n\n48.7.2 Spiegazione Matematica\nSupponiamo che il vero modello sia:\n\\[\ny = \\alpha + \\beta_1 X_1 + \\beta_2 X_2 + \\varepsilon,\n\\]\nstimato come:\n\\[\ny = a + b_1 X_1 + b_2 X_2 + e.\n\\]\nSe omettiamo erroneamente \\(X_2\\), il modello diventa:\n\\[\ny = a^\\prime + b_1^\\prime X_1 + e^\\prime,\n\\]\ndove \\(b_1^\\prime\\) è dato da:\n\\[\nb_1^\\prime = \\frac{\\text{Cov}(X_1, y)}{\\text{Var}(X_1)}.\n\\]\nSviluppando l’espressione:\n\\[\nb_1^\\prime = b_1 + b_2 \\frac{\\text{Cov}(X_1, X_2)}{\\text{Var}(X_1)}.\n\\]\nPertanto, il coefficiente \\(b_1^\\prime\\) stimato nel modello con specificazione errata è distorto dalla presenza di \\(b_2\\) e dalla correlazione \\(\\text{Cov}(X_1, X_2)\\).\n\n48.7.3 Verifica dell’Errore nei Dati Simulati\nCalcoliamo manualmente il coefficiente distorto \\(b_1^\\prime\\) utilizzando i risultati del modello completo:\n\nfm1$coef[2] + fm1$coef[3] * \n  cov(sim_dat2$motivation, sim_dat2$anxiety) / \n  var(sim_dat2$motivation)\n#&gt; motivation \n#&gt;    -0.4372\n\nIl valore calcolato coincide con quello stimato dal modello performance ~ motivation, confermando che il coefficiente è distorto.\nPossiamo trarre le seguenti conclusioni:\n\n\nIl coefficiente stimato \\(b_1^\\prime\\) è distorto se vengono omessi predittori rilevanti (\\(X_2\\)) correlati a quelli inclusi (\\(X_1\\)).\n\n\nLa distorsione è sistematica e non si riduce all’aumentare della numerosità campionaria, rendendo lo stimatore inconsistente.\n\n\nLa causa dell’errore è l’attribuzione degli effetti di \\(X_2\\) al predittore incluso, \\(X_1\\).\n\nL’errore di specificazione può essere evitato solo se:\n\nIl predittore omesso (\\(X_2\\)) non ha un effetto sulla variabile dipendente (\\(\\beta_2 = 0\\)).\n\nIl predittore omesso è incorrelato con i predittori inclusi (\\(\\text{Cov}(X_1, X_2) = 0\\)).\n\nUna corretta specificazione del modello è quindi essenziale per garantire risultati affidabili e interpretazioni corrette.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#errore-di-specificazione-e-modelli-sem",
    "href": "chapters/sem/01_sem_intro.html#errore-di-specificazione-e-modelli-sem",
    "title": "48  Introduzione ai Modelli SEM",
    "section": "\n48.8 Errore di Specificazione e Modelli SEM",
    "text": "48.8 Errore di Specificazione e Modelli SEM\nQui abbiamo descritto l’errore di specificazione in riferimento al modello di regressione. Tuttavia, dato che i Modelli di Equazioni Strutturali (SEM) possono essere considerati un’estensione del modello di regressione, le stesse considerazioni si applicano anche ai SEM.\nAnche se i SEM permettono di rappresentare e testare relazioni più complesse tra variabili osservate e latenti, le loro conclusioni rimangono condizionate a un’importante assunzione: che il modello sia specificato correttamente. In altre parole, le inferenze tratte da un SEM presuppongono che non vi siano variabili rilevanti omesse o erroneamente incluse nel modello. Tuttavia, questa ipotesi è spesso irrealistica. Dato che non possiamo sapere con certezza se esistano altre variabili correlate con quelle del modello e che, in generale, tale eventualità è altamente probabile, è inevitabile che le relazioni descritte da un SEM siano sempre influenzate, almeno in parte, dall’errore di specificazione.\n\n48.8.1 La Natura Condizionale delle Conclusioni nei SEM\nL’interpretazione dei risultati di un SEM è sempre condizionale all’assunzione che il modello rappresenti correttamente le relazioni tra le variabili. Questa condizionalità implica che:\n\nLe stime dei parametri e le inferenze derivate sono valide solo all’interno del contesto teorico e delle specifiche del modello scelto.\n\nLa validità delle conclusioni è compromessa se il modello è distorto da errori di specificazione, come l’omissione di variabili rilevanti o la presenza di relazioni spurie tra le variabili incluse.\n\n48.8.2 La Portata dell’Errore di Specificazione nei SEM\nL’errore di specificazione è intrinseco a tutti i modelli statistici, SEM inclusi. Questo deriva dal fatto che non possiamo conoscere tutte le variabili rilevanti né possiamo verificare in modo definitivo se il modello include tutte le relazioni pertinenti. Ne consegue che:\n\n\nL’errore di specificazione è inevitabile, ma non sempre rilevante. La sua gravità dipende da quanto le variabili omesse o erroneamente incluse distorcono le stime dei parametri del modello.\n\n\nNon è possibile quantificare con precisione la portata dell’errore di specificazione, poiché non possiamo osservare direttamente le variabili omesse.\n\n48.8.3 Utilità del Modello e il Dilemma della Verità\nTornando al celebre aforisma di George Box: “Tutti i modelli sono falsi, ma alcuni sono utili”, l’obiettivo dell’analisi con i SEM non è stabilire se il modello sia “vero”. Questa verifica è impossibile, dato che non possiamo mai essere certi di non aver omesso variabili rilevanti o di aver incluso solo relazioni valide. Il problema non è evitare l’errore di specificazione, bensì valutarne l’impatto e considerare l’utilità del modello nonostante la sua inevitabile falsità.\n\n48.8.4 Cosa Significa “Utilità” in un SEM?\nUn modello è utile se:\n\nConsente di rispondere a specifiche domande di ricerca, anche in presenza di alcune semplificazioni.\n\nFornisce una rappresentazione ragionevolmente coerente delle relazioni teoriche proposte, pur riconoscendo i limiti del contesto empirico.\n\nOffre predizioni interpretabili e comprensibili che possano guidare ulteriori ricerche, lo sviluppo teorico e decisioni pratiche.\n\nIn conclusione, riconoscere l’inevitabilità dell’errore di specificazione è essenziale per un utilizzo critico e consapevole dei SEM. Invece di mirare a modelli “veri” (un obiettivo irrealistico), il nostro scopo dovrebbe essere quello di costruire modelli sufficientemente validi e utili da consentire inferenze significative, pur accettandone le limitazioni. Questo approccio non elimina l’incertezza, ma permette di utilizzare i SEM come strumenti potenti per esplorare e chiarire le relazioni tra costrutti teorici complessi.\n\n48.8.5 Soppressione\nLe conseguenze dell’errore di specificazione possono manifestarsi sotto forma di soppressione (suppression), un fenomeno che si verifica quando le relazioni tra i predittori e la variabile criterio (dipendente) assumono caratteristiche inaspettate o controintuitive nell’analisi di regressione multipla. La soppressione si verifica in due situazioni principali:\n\nIl valore assoluto del peso \\(\\beta\\) di un predittore è maggiore della sua correlazione bivariata con la variabile criterio.\n\nIl peso \\(\\beta\\) e la correlazione bivariata hanno segni opposti.\n\nLa soppressione può essere suddivisa in tre categorie principali:\n\n48.8.5.1 1. Soppressione Negativa\nQuesto fenomeno si verifica quando un predittore ha una correlazione bivariata positiva con il criterio, ma riceve un peso \\(\\beta\\) negativo nell’analisi di regressione multipla.\nL’esempio precedente dell’errore di specificazione illustra proprio un caso di soppressione negativa: la variabile motivazione ha una correlazione bivariata positiva con la prestazione, ma, a causa della specificazione errata del modello (omissione della variabile ansia), il suo coefficiente \\(\\beta\\) diventa negativo. Questo tipo di soppressione è comune nei casi in cui predittori correlati negativamente tra loro competono per spiegare la stessa varianza della variabile criterio.\n\n48.8.5.2 2. Soppressione Classica\nNella soppressione classica, un predittore non ha alcuna correlazione bivariata con il criterio, ma riceve un peso \\(\\beta\\) diverso da zero nell’analisi di regressione multipla. Questo accade quando il predittore in questione contribuisce a ridurre la varianza non spiegata associata agli altri predittori, aumentando così il potere predittivo complessivo del modello.\nAd esempio, una variabile può essere utile per “sopprimere” la varianza irrilevante di un altro predittore, migliorando l’accuratezza delle stime.\n\n48.8.5.3 3. Soppressione Reciproca\nLa soppressione reciproca si verifica quando due predittori sono positivamente correlati con il criterio, ma sono negativamente correlati tra loro. In questi casi, l’inclusione di entrambi i predittori nel modello di regressione può aumentare i pesi \\(\\beta\\) di ciascuno, poiché ciascun predittore riduce la varianza non spiegata dell’altro, migliorando la spiegazione complessiva della varianza del criterio.\n\n48.8.6 Implicazioni della Soppressione\nLa soppressione, come conseguenza dell’errore di specificazione o della struttura dei dati, ha implicazioni significative per l’interpretazione dei modelli di regressione e SEM:\n\n\nDistorsione dei risultati: La soppressione può portare a interpretazioni controintuitive, come l’apparente effetto negativo di un predittore che ha una relazione positiva con il criterio.\n\n\nDipendenza dal modello: I pesi \\(\\beta\\) nei modelli di regressione e SEM sono sempre condizionali alle variabili incluse nel modello. Aggiungere o rimuovere predittori può alterare significativamente i pesi stimati.\n\n\nImportanza del controllo delle variabili: La soppressione evidenzia l’importanza di includere tutti i predittori rilevanti nel modello per evitare distorsioni e garantire stime accurate.\n\nIn conclusione, la soppressione è un fenomeno complesso ma inevitabile in analisi multivariate. Comprendere i meccanismi alla base della soppressione, e come essa può emergere a seguito di errori di specificazione, è cruciale per interpretare correttamente i risultati delle analisi statistiche.\n\n48.8.7 Regressione Stepwise\nNel contesto della regressione, è fondamentale comprendere che i predittori non dovrebbero essere selezionati basandosi esclusivamente sulle loro correlazioni bivariate con la variabile dipendente (criterio). Queste correlazioni, chiamate associazioni di ordine zero, non tengono conto dell’influenza reciproca tra i predittori e, di conseguenza, possono risultare fuorvianti quando si interpretano i coefficienti di regressione parziale.\nLa significatività statistica delle correlazioni bivariate non è un criterio affidabile per la selezione dei predittori, poiché non considera gli effetti congiunti di altri predittori nel modello. Questo punto è particolarmente rilevante nei modelli con più predittori correlati, dove le relazioni tra le variabili sono complesse e non lineari.\n\n48.8.7.1 Criticità delle Procedure Stepwise\nLe procedure automatiche di selezione dei predittori, come la regressione stepwise, possono sembrare attraenti per la loro semplicità ed efficienza. Tuttavia, queste tecniche presentano gravi limitazioni:\n- Sensibilità alla struttura dei dati: Piccole non-linearità o interazioni tra predittori possono alterare in modo significativo i coefficienti di regressione stimati.\n- Esposizione all’errore di specificazione: La rimozione o l’aggiunta automatica di variabili può distorcere il modello, attribuendo erroneamente effetti ad altri predittori.\n- Non replicabilità dei risultati: I modelli generati tramite procedure stepwise sono spesso specifici del campione di dati utilizzato, e i risultati tendono a non essere replicabili in altri campioni.\nPer queste ragioni, molte riviste scientifiche non accettano studi che utilizzano procedure stepwise. I risultati ottenuti con tali tecniche sono considerati inaffidabili e difficilmente generalizzabili.\n\n48.8.7.2 Selezione dei Predittori: Approccio Teorico vs. Statistico\nIn alternativa ai metodi automatici, i predittori dovrebbero essere scelti sulla base di considerazioni teoriche o di risultati empirici consolidati. Questo approccio ponderato aiuta a evitare distorsioni e garantisce che il modello rifletta le ipotesi di ricerca e il contesto teorico di riferimento.\nUna volta selezionati, i predittori possono essere inseriti nell’equazione di regressione con due strategie principali:\n1. Inserimento simultaneo: Tutti i predittori vengono inclusi contemporaneamente nel modello.\n2. Inserimento sequenziale: I predittori vengono aggiunti gradualmente, seguendo un ordine prestabilito, in base a criteri teorici o statistici.\n\n48.8.7.2.1 Regressione Gerarchica\nL’approccio teorico si traduce spesso nella regressione gerarchica, dove l’ordine di inserimento dei predittori è determinato da considerazioni razionali. Ad esempio, è comune includere prima le variabili demografiche e successivamente le variabili psicologiche di interesse. Questo approccio consente di:\n\nControllare gli effetti delle variabili demografiche.\n\nValutare il contributo unico delle variabili psicologiche, misurato tramite l’incremento del coefficiente di determinazione (\\(\\Delta R^2\\)).\n\n48.8.7.2.2 Regressione Stepwise\nL’approccio statistico, invece, è rappresentato dalla regressione stepwise, in cui il computer decide l’ordine di inserimento dei predittori in base ai valori di \\(p\\):\n- Nella stepwise forward inclusion, i predittori vengono aggiunti uno alla volta, scegliendo quello con il \\(p\\) più piccolo a ogni passo. Una volta aggiunti, i predittori rimangono nel modello.\n- Nella stepwise backward elimination, il modello parte con tutti i predittori e li elimina progressivamente in base ai loro valori di \\(p\\), fino a ottenere il modello finale.\n- Nella stepwise bidirezionale, predittori possono essere aggiunti o rimossi a ogni passo.\nQueste varianti si interrompono quando l’aggiunta o la rimozione di ulteriori predittori non migliora significativamente \\(\\Delta R^2\\). Tuttavia, come già sottolineato, tali procedure sono altamente problematiche e non raccomandate per la ricerca scientifica.\n\n48.8.8 La Tentazione di Rimuovere Predittori “Non Significativi”\nUn errore comune è la rimozione dal modello di predittori che non risultano “statisticamente significativi”. Questa pratica è controproducente per diverse ragioni:\n\n\nPotenza statistica insufficiente: In campioni piccoli, i test di significatività possono non rilevare effetti reali. Eliminare variabili potenzialmente rilevanti a causa della mancanza di significatività statistica può compromettere il modello.\n\n\nSpecificazione errata del modello: La rimozione di predittori senza una giustificazione teorica solida può introdurre errori di specificazione, distorcendo i coefficienti di regressione per le variabili rimanenti.\n\nSe esiste una motivazione teorica convincente per includere un predittore, questo dovrebbe essere mantenuto nel modello, indipendentemente dalla sua significatività statistica. Le decisioni relative ai predittori dovrebbero essere guidate da un approccio scientifico, non da criteri meccanici come i valori di \\(p\\).\nIn conclusione, le procedure di selezione dei predittori, in particolare quelle automatiche come la regressione stepwise, presentano gravi rischi di distorsione e non replicabilità. Al contrario, un approccio basato su solide considerazioni teoriche e supportato da prove empiriche garantisce modelli più robusti e utili Infine, è essenziale superare l’ossessione per la significatività statistica e focalizzarsi sulla coerenza teorica e sull’utilità del modello.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#oltre-la-regressione-multipla",
    "href": "chapters/sem/01_sem_intro.html#oltre-la-regressione-multipla",
    "title": "48  Introduzione ai Modelli SEM",
    "section": "\n48.9 Oltre la Regressione Multipla",
    "text": "48.9 Oltre la Regressione Multipla\nUna volta chiarita la relazione tra modello di regressione e modelli SEM, possiamo usare i modelli SEM per analizzare relazioni tra variabili che non possono essere descritte nei termini di un modello di regressione. Per fare un esempio, concludiamo il capitolo utilizzando il modello SEM per descrivere la relazione tra due costrutti:\nAnalizziamo un esempio in cui il modello di Equazioni Strutturali (SEM) viene impiegato per studiare la relazione tra autocompassione e disagio emotivo, utilizzando come indicatori le sotto-scale della DASS-21 (Depressione, Ansia, Stress) e della Self-Compassion Scale. In questo contesto, definiamo due variabili latenti: “disagio emotivo” e “autocompassione”. La variabile latente “disagio emotivo” è composta dalle tre sotto-scale della DASS-21, mentre la variabile “autocompassione” è formata dalle sei sotto-scale della Self-Compassion Scale.\nIl modello strutturale esplora la relazione tra queste due variabili latenti. L’autocompassione è considerata una variabile esogena, ipotizzata come un fattore di protezione che riduce il disagio emotivo, che a sua volta è trattato come variabile endogena. L’ipotesi principale del modello è che esista una relazione di regressione negativa tra autocompassione e disagio emotivo, indicando che livelli più elevati di autocompassione sono associati a minori livelli di disagio emotivo.\nUn elemento chiave dei modelli SEM è la gestione dell’errore di misurazione. Le variabili latenti sono progettate per riflettere il nucleo vero dei costrutti teorici, in questo caso autocompassione e disagio emotivo, isolando gli effetti degli errori di misurazione che possono affliggere gli indicatori osservati. Questo approccio consente di esaminare la “vera” relazione tra i costrutti, eliminando le distorsioni introdotte dagli errori di misurazione nelle misure osservate.\nLa capacità del modello SEM di separare la variabilità attribuibile ai costrutti latenti da quella dovuta agli errori di misurazione aumenta l’accuratezza e l’affidabilità dell’analisi. Questo è particolarmente vantaggioso in campi come la psicologia, dove i costrutti teorici non sono direttamente osservabili e devono essere inferiti attraverso misure potenzialmente errate.\n\nmod_sc &lt;- \"\n  ED =~ anxiety + depression + stress\n  SC =~ self_kindness   + common_humanity   + mindfulness   + \n        self_judgment   + isolation + over_identification\n  ED ~~ SC \n\"\n\nAdattiamo il modello ai dati.\n\nfit_sc &lt;- lavaan::sem(mod_sc, dat, std.lv = TRUE)\n\nEsaminiamo la soluzione ottenuta.\n\nstandardizedSolution(fit_sc) \n#&gt;                    lhs op                 rhs est.std    se       z pvalue\n#&gt; 1                   ED =~             anxiety   0.847 0.014  58.505      0\n#&gt; 2                   ED =~          depression   0.909 0.011  82.503      0\n#&gt; 3                   ED =~              stress   0.929 0.010  91.945      0\n#&gt; 4                   SC =~       self_kindness   0.757 0.022  33.984      0\n#&gt; 5                   SC =~     common_humanity   0.621 0.030  20.695      0\n#&gt; 6                   SC =~         mindfulness   0.689 0.026  26.235      0\n#&gt; 7                   SC =~       self_judgment  -0.770 0.022 -35.800      0\n#&gt; 8                   SC =~           isolation  -0.770 0.022 -35.822      0\n#&gt; 9                   SC =~ over_identification  -0.767 0.022 -35.415      0\n#&gt; 10                  ED ~~                  SC  -0.476 0.038 -12.385      0\n#&gt; 11             anxiety ~~             anxiety   0.282 0.025  11.495      0\n#&gt; 12          depression ~~          depression   0.173 0.020   8.633      0\n#&gt; 13              stress ~~              stress   0.136 0.019   7.246      0\n#&gt; 14       self_kindness ~~       self_kindness   0.427 0.034  12.647      0\n#&gt; 15     common_humanity ~~     common_humanity   0.615 0.037  16.527      0\n#&gt; 16         mindfulness ~~         mindfulness   0.525 0.036  14.522      0\n#&gt; 17       self_judgment ~~       self_judgment   0.407 0.033  12.285      0\n#&gt; 18           isolation ~~           isolation   0.407 0.033  12.280      0\n#&gt; 19 over_identification ~~ over_identification   0.411 0.033  12.360      0\n#&gt; 20                  ED ~~                  ED   1.000 0.000      NA     NA\n#&gt; 21                  SC ~~                  SC   1.000 0.000      NA     NA\n#&gt;    ci.lower ci.upper\n#&gt; 1     0.819    0.876\n#&gt; 2     0.888    0.931\n#&gt; 3     0.910    0.949\n#&gt; 4     0.713    0.801\n#&gt; 5     0.562    0.679\n#&gt; 6     0.637    0.740\n#&gt; 7    -0.812   -0.728\n#&gt; 8    -0.812   -0.728\n#&gt; 9    -0.810   -0.725\n#&gt; 10   -0.551   -0.400\n#&gt; 11    0.234    0.330\n#&gt; 12    0.134    0.212\n#&gt; 13    0.099    0.173\n#&gt; 14    0.361    0.493\n#&gt; 15    0.542    0.688\n#&gt; 16    0.454    0.596\n#&gt; 17    0.342    0.472\n#&gt; 18    0.342    0.472\n#&gt; 19    0.346    0.476\n#&gt; 20    1.000    1.000\n#&gt; 21    1.000    1.000\n\n\n\nSaturazioni Fattoriali (Loadings) per le Variabili Latenti:\n\n\nED: Le variabili osservate “anxiety”, “depression”, e “stress” hanno elevate saturazioni fattoriali sulla variabile latente “ED”. Questo suggerisce che ciascuna di queste misure è un buon indicatore della variabile latente “ED”.\n\nSC: Le variabili “self_kindness”, “common_humanity”, “mindfulness”, “self_judgment”, “isolation”, e “over_identification” hanno anch’esse significative saturazioni sulla variabile latente “SC”. Si noti che “self_judgment”, “isolation”, e “over_identification” hanno saturazioni negative, indicando che queste variabili sono inversamente associate con “SC”.\n\n\n\nCorrelazione tra Variabili Latenti:\n\nLa correlazione tra “ED” e “SC” mostra un coefficiente negativo (-0.476), il che indica una relazione inversa tra queste due variabili latenti. Questo significa che livelli più alti di “SC” sono associati a livelli più bassi di “ED”.\n\n\n\nVarianza delle Variabili Latenti:\n\nLa varianza di “ED” e “SC” indica quanto della variazione nelle variabili latenti è spiegata dai loro rispettivi indicatori. La varianza di “ED” (0.77) è relativamente alta, suggerendo che gli indicatori spiegano una buona parte della varianza in “ED”. La varianza di “SC” è fissata a 1, un approccio comune per identificare il modello.\n\n\n\nVarianze Residue degli Indicatori:\n\nLe varianze residue (ad esempio, “anxiety ~~ anxiety”) rappresentano la varianza non spiegata in ciascun indicatore dalle variabili latenti. Valori più bassi indicano che la variabile latente spiega una maggior parte della varianza dell’indicatore. Ad esempio, “anxiety” ha una varianza residua di 0.28, suggerendo che “ED” spiega una buona parte, ma non tutta, della varianza in “anxiety”.\n\n\n\nGeneriamo una rappresentazione grafica del modello.\n\nsemPaths(fit_sc,\n    whatLabels = \"std\",\n    sizeMan = 10,\n    edge.label.cex = 0.9,\n    style = \"mx\",\n    nCharNodes = 5, nCharEdges = 0, \n    fade=FALSE\n)\n\n\n\n\n\n\n\nI coefficienti stimati nel modello SEM appaiono coerenti e in linea con le aspettative, in particolare il coefficiente che descrive la correlazione tra il fattore dell’autocompassione e il disagio emotivo, che si attesta a -0.48. Questo valore negativo corrobora l’ipotesi secondo cui l’autocompassione svolge un ruolo di fattore protettivo contro il disagio emotivo. Tuttavia, prima di confermare definitivamente questa conclusione, è cruciale esaminare gli indici di bontà di adattamento del modello. Questi indici ci permetteranno di valutare quanto accuratamente il modello SEM si adatta ai dati osservati, fornendo un quadro più chiaro della validità delle nostre inferenze. In altre parole, sebbene il modello suggerisca una relazione negativa tra autocompassione e disagio emotivo, la conferma finale di questa associazione dipenderà dall’adeguatezza complessiva del modello rispetto ai dati. Questo argomento verrà affrontato nei prossimi capitoli.\n\n48.9.1 Vantaggi del Modello SEM\nÈ evidente che avremmo potuto calcolare la correlazione tra disagio emotivo e autocompassione in modo più semplice, utilizzando semplicemente la correlazione di Pearson tra il punteggio totale del DASS-21 (disagio emotivo) e il punteggio totale della Self-Compassion Scale (SCS). Per calcolare il punteggio totale del DASS-21, sommiamo le sottoscale di stress, ansia e depressione:\n\ndat &lt;- dat |&gt; \n  mutate(ed = stress + anxiety + depression)\n\nIl valore della correlazione così ottenuto è pari a 0.405:\n\ncor.test(dat$ed, dat$scs_ts)\n#&gt; \n#&gt;  Pearson's product-moment correlation\n#&gt; \n#&gt; data:  dat$ed and dat$scs_ts\n#&gt; t = -10, df = 524, p-value &lt;2e-16\n#&gt; alternative hypothesis: true correlation is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  -0.4742 -0.3311\n#&gt; sample estimates:\n#&gt;     cor \n#&gt; -0.4051\n\nTuttavia, questo valore differisce in maniera rilevante da quello calcolato mediante il modello SEM, dove la correlazione stimata è \\(r = -0.476\\). Questa differenza evidenzia una distinzione fondamentale tra i due approcci:\n\nCorrelazione di Pearson:\nLa correlazione di Pearson si basa sui punteggi osservati delle variabili, che includono sia il punteggio vero (il costrutto sottostante) sia l’errore di misurazione. Di conseguenza, la correlazione calcolata può essere distorta dall’errore associato alla misurazione di ciascun costrutto.\nModello SEM:\nNei modelli SEM, la correlazione è stimata tra i punteggi veri dei costrutti, al netto dell’errore di misurazione. Questo approccio fornisce una stima più accurata e affidabile della relazione tra i costrutti teorici, eliminando la varianza attribuibile agli errori di misurazione.\n\nIl principale vantaggio dei modelli SEM è la loro capacità di descrivere le associazioni tra le variabili in modo meno influenzato dagli errori di misurazione rispetto alle semplici correlazioni. Questo permette di ottenere stime più vicine alle relazioni reali tra i costrutti, migliorando la validità delle conclusioni.\nLa differenza tra i due valori di correlazione calcolati sottolinea l’importanza di utilizzare modelli SEM quando si vuole ottenere una rappresentazione più precisa delle relazioni tra costrutti latenti, in particolare in presenza di variabili soggette a errori di misurazione.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#impiego-delle-medie-nei-modelli-sem",
    "href": "chapters/sem/01_sem_intro.html#impiego-delle-medie-nei-modelli-sem",
    "title": "48  Introduzione ai Modelli SEM",
    "section": "\n48.10 Impiego delle Medie nei Modelli SEM",
    "text": "48.10 Impiego delle Medie nei Modelli SEM\nUn altro elemento importante da evidenziare nell’introduzione ai modelli SEM è il ruolo delle medie. Nei SEM, l’enfasi è tradizionalmente posta sull’analisi delle covarianze tra variabili. Tuttavia, a differenza dell’analisi fattoriale classica, i SEM permettono di includere anche le medie delle variabili osservate e latenti. Questo arricchisce l’analisi, fornendo informazioni importanti in contesti come i modelli longitudinali di analisi fattoriale confermativa (CFA), dove le ipotesi centrali possono riguardare cambiamenti o differenze nelle medie dei costrutti latenti nel tempo.\n\n48.10.1 Struttura delle Medie nei Modelli SEM\nLa struttura delle medie nei SEM può essere descritta con la seguente equazione:\n\\[\nE(\\mathbf{y}) = \\boldsymbol{\\mu}_y = \\boldsymbol{\\tau} + \\mathbf{\\Lambda} \\boldsymbol{\\alpha},\n\\]\ndove:\n\n\n\\(\\mathbf{y}\\) rappresenta i punteggi degli indicatori osservati,\n\n\n\\(E(\\mathbf{y})\\) è il vettore delle medie attese degli indicatori,\n\n\n\\(\\boldsymbol{\\mu}_y\\) è il vettore delle medie degli indicatori osservati (analogo a \\(\\mathbf{\\Sigma}\\) nelle strutture di covarianza),\n\n\n\\(\\boldsymbol{\\tau}\\) è il vettore delle intercette degli indicatori,\n\n\n\\(\\mathbf{\\Lambda}\\) è la matrice dei carichi fattoriali, che definisce la relazione tra gli indicatori e i costrutti latenti,\n\n\n\\(\\boldsymbol{\\alpha}\\) è il vettore delle medie dei costrutti latenti.\n\nIn un diagramma a percorsi, l’intercetta è rappresentata da un triangolo con il numero 1, che funge da costante di regressione. Questo simbolo indica l’origine della scala per la variabile e permette di stimare la media di una variabile quando viene regredita su questa costante.\n\n48.10.2 Interpretazione e Collegamento tra Medie e Carichi Fattoriali\nNei modelli SEM, gli indicatori con carichi fattoriali più elevati (\\(\\mathbf{\\Lambda}\\)) esercitano un impatto maggiore sulla media stimata del costrutto (\\(\\boldsymbol{\\alpha}\\)). Questo collegamento evidenzia che i carichi fattoriali non solo influenzano la struttura di covarianza, ma giocano un ruolo chiave anche nella determinazione delle medie latenti.\n\n48.10.3 Vincoli e Scalatura delle Medie\nCome per le strutture di covarianza, anche per le strutture delle medie è necessario imporre vincoli per definire la scala del modello. Spesso si utilizza lo zero come riferimento per le medie dei costrutti latenti. Questo vincolo serve a fissare un punto di partenza per le stime e a calcolare le differenze rispetto al riferimento, che possono assumere valori sia positivi sia negativi.\nAd esempio:\n\nNei modelli CFA longitudinali, i vincoli sulle medie possono essere utilizzati per studiare cambiamenti temporali nei costrutti latenti.\n\nNei modelli con gruppi multipli, i vincoli sulle medie permettono di confrontare le medie dei costrutti tra gruppi diversi.\n\n48.10.4 Stima delle Intercette con lavaan\n\nPer stimare le intercette e includere la struttura delle medie in un modello SEM, è necessario disporre non solo della matrice di covarianza, ma anche delle medie delle variabili osservate. Il software lavaan semplifica questo processo attraverso l’opzione meanstructure = TRUE. Quando questa opzione è attivata, lavaan integra automaticamente una costante “1” in tutte le equazioni del modello, consentendo di stimare:\n\nLe intercette per le variabili osservate.\n\nLe medie dei costrutti latenti (\\(\\boldsymbol{\\alpha}\\)).\n\nEsempio di codice in lavaan per includere le medie:\nmodel &lt;- \"\n  y1 + y2 + y3 =~ latent1\n  y4 + y5 + y6 =~ latent2\n\"\nfit &lt;- lavaan::sem(model, data = dat, meanstructure = TRUE)\nsummary(fit, standardize = TRUE)\nIn questo contesto, lavaan stimerà sia le strutture di covarianza sia le medie e le intercette per le variabili osservate e latenti. L’inclusione delle medie è specificata dall’uso dell’argomento meanstructure = TRUE.\n\n48.10.5 Importanza delle Medie nei Modelli SEM\nL’inclusione delle medie nei modelli SEM fornisce un livello aggiuntivo di informazione, permettendo di confrontare:\n\n\nLe medie stimate dai modelli SEM con le medie osservate nei dati.\n\nLe medie di costrutti latenti tra diversi gruppi o in momenti temporali differenti.\n\nIn conclusione, l’integrazione delle medie nei modelli SEM rappresenta un importante arricchimento rispetto all’analisi delle sole covarianze. Questo approccio consente di ottenere una visione più completa delle relazioni tra variabili, al netto dell’errore di misurazione, e di esplorare le differenze nelle medie dei costrutti in contesti complessi come studi longitudinali o analisi multigruppo. Di conseguenza, l’analisi delle medie nei SEM non solo amplia la gamma di domande di ricerca a cui è possibile rispondere, ma migliora anche la validità delle conclusioni tratte dai dati.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#riflessioni-conclusive",
    "href": "chapters/sem/01_sem_intro.html#riflessioni-conclusive",
    "title": "48  Introduzione ai Modelli SEM",
    "section": "\n48.11 Riflessioni Conclusive",
    "text": "48.11 Riflessioni Conclusive\nIn questo capitolo, abbiamo esplorato i Modelli di Equazioni Strutturali (SEM), evidenziando come questi modelli non si limitino a descrivere le correlazioni tra variabili osservabili, ma permettano anche di analizzare le relazioni tra variabili latenti. La forza dei SEM risiede nella loro capacità di integrare il modello di misurazione, che definisce le relazioni tra gli indicatori e le variabili latenti, con il modello strutturale, che esamina le interazioni tra le stesse variabili latenti.\nNei prossimi capitoli, approfondiremo vari aspetti della modellazione SEM. Esamineremo la bontà di adattamento del modello, un criterio fondamentale per verificare la fedeltà con cui il modello riflette la realtà osservata. Analizzeremo anche il confronto tra modelli alternativi, un passaggio cruciale per identificare il modello che migliora l’interpretazione dei dati.\nUn altro tema importante sarà l’analisi dell’applicabilità dei modelli a gruppi diversi, vitale per valutare la loro generalizzabilità e la pertinenza in contesti specifici. Inoltre, discuteremo le sfide metodologiche legate alla gestione di dati categoriali, all’implementazione di modelli SEM multilivello e alla gestione di dati mancanti. Questi approfondimenti ci permetteranno di comprendere meglio come i modelli SEM possono essere adattati e applicati efficacemente in diversi ambiti di ricerca.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#session-info",
    "href": "chapters/sem/01_sem_intro.html#session-info",
    "title": "48  Introduzione ai Modelli SEM",
    "section": "\n48.12 Session Info",
    "text": "48.12 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.2\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] effectsize_1.0.0   rsvg_2.6.1         DiagrammeRsvg_0.1 \n#&gt;  [4] mvnormalTest_1.0.0 lavaanExtra_0.2.1  ggokabeito_0.1.0  \n#&gt;  [7] see_0.11.0         MASS_7.3-65        viridis_0.6.5     \n#&gt; [10] viridisLite_0.4.2  ggpubr_0.6.0       ggExtra_0.10.1    \n#&gt; [13] gridExtra_2.3      patchwork_1.3.0    bayesplot_1.11.1  \n#&gt; [16] semTools_0.5-6     semPlot_1.1.6      lavaan_0.6-19     \n#&gt; [19] psych_2.4.12       scales_1.3.0       markdown_1.13     \n#&gt; [22] knitr_1.50         lubridate_1.9.4    forcats_1.0.0     \n#&gt; [25] stringr_1.5.1      dplyr_1.1.4        purrr_1.0.4       \n#&gt; [28] readr_2.1.5        tidyr_1.3.1        tibble_3.2.1      \n#&gt; [31] ggplot2_3.5.1      tidyverse_2.0.0    here_1.0.1        \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.1      datawizard_1.0.1   \n#&gt;   [4] magrittr_2.0.3      TH.data_1.1-3       estimability_1.5.1 \n#&gt;   [7] farver_2.1.2        nloptr_2.2.1        rmarkdown_2.29     \n#&gt;  [10] vctrs_0.6.5         minqa_1.2.8         base64enc_0.1-3    \n#&gt;  [13] rstatix_0.7.2       htmltools_0.5.8.1   curl_6.2.1         \n#&gt;  [16] broom_1.0.7         Formula_1.2-5       htmlwidgets_1.6.4  \n#&gt;  [19] plyr_1.8.9          sandwich_3.1-1      copula_1.1-6       \n#&gt;  [22] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [25] mime_0.13           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [28] Matrix_1.7-3        R6_2.6.1            fastmap_1.2.0      \n#&gt;  [31] rbibutils_2.3       shiny_1.10.0        numDeriv_2016.8-1.1\n#&gt;  [34] digest_0.6.37       OpenMx_2.21.13      fdrtool_1.2.18     \n#&gt;  [37] colorspace_2.1-1    rprojroot_2.0.4     Hmisc_5.2-3        \n#&gt;  [40] pspline_1.0-21      timechange_0.3.0    abind_1.4-8        \n#&gt;  [43] compiler_4.4.2      gsl_2.1-8           withr_3.0.2        \n#&gt;  [46] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n#&gt;  [49] carData_3.0-5       ggsignif_0.6.4      corpcor_1.6.10     \n#&gt;  [52] gtools_3.9.5        tools_4.4.2         pbivnorm_0.6.0     \n#&gt;  [55] foreign_0.8-88      zip_2.3.2           httpuv_1.6.15      \n#&gt;  [58] nnet_7.3-20         glue_1.8.0          quadprog_1.5-8     \n#&gt;  [61] stabledist_0.7-2    nlme_3.1-167        promises_1.3.2     \n#&gt;  [64] lisrelToR_0.3       grid_4.4.2          checkmate_2.3.2    \n#&gt;  [67] cluster_2.1.8.1     reshape2_1.4.4      generics_0.1.3     \n#&gt;  [70] gtable_0.3.6        nortest_1.0-4       tzdb_0.5.0         \n#&gt;  [73] data.table_1.17.0   hms_1.1.3           car_3.1-3          \n#&gt;  [76] sem_3.1-16          pillar_1.10.1       rockchalk_1.8.157  \n#&gt;  [79] later_1.4.1         splines_4.4.2       moments_0.14.1     \n#&gt;  [82] lattice_0.22-6      survival_3.8-3      kutils_1.73        \n#&gt;  [85] tidyselect_1.2.1    ADGofTest_0.3       miniUI_0.1.1.1     \n#&gt;  [88] pbapply_1.7-2       reformulas_0.4.0    V8_6.0.2           \n#&gt;  [91] stats4_4.4.2        xfun_0.51           qgraph_1.9.8       \n#&gt;  [94] arm_1.14-4          stringi_1.8.4       pacman_0.5.1       \n#&gt;  [97] boot_1.3-31         evaluate_1.0.3      codetools_0.2-20   \n#&gt; [100] mi_1.1              cli_3.6.4           RcppParallel_5.1.10\n#&gt; [103] rpart_4.1.24        parameters_0.24.2   xtable_1.8-4       \n#&gt; [106] Rdpack_2.6.3        munsell_0.5.1       Rcpp_1.0.14        \n#&gt; [109] coda_0.19-4.1       png_0.1-8           XML_3.99-0.18      \n#&gt; [112] parallel_4.4.2      bayestestR_0.15.2   jpeg_0.1-10        \n#&gt; [115] lme4_1.1-36         mvtnorm_1.3-3       insight_1.1.0      \n#&gt; [118] pcaPP_2.0-5         openxlsx_4.2.8      rlang_1.1.5        \n#&gt; [121] multcomp_1.4-28     mnormt_2.1.1\n\n\n\n\n\nMauro, R. (1990). Understanding LOVE (left out variables error): A method for estimating the effects of omitted variables. Psychological Bulletin, 108(2), 314–329.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.\n\n\nSaqr, M., & López-Pernas, S. (2024). Learning analytics methods and tutorials: A practical guide using R. Springer Nature.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/02_data_preparation.html",
    "href": "chapters/sem/02_data_preparation.html",
    "title": "49  Preparazione dei Dati",
    "section": "",
    "text": "49.1 Introduzione\nQuesto capitolo breve ma essenziale esplora diversi aspetti fondamentali della gestione dei dati nell’ambito della modellazione con Modelli di Equazioni Strutturali (SEM).",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Preparazione dei Dati</span>"
    ]
  },
  {
    "objectID": "chapters/sem/02_data_preparation.html#formati-dei-dati-di-input",
    "href": "chapters/sem/02_data_preparation.html#formati-dei-dati-di-input",
    "title": "49  Preparazione dei Dati",
    "section": "49.2 Formati dei Dati di Input",
    "text": "49.2 Formati dei Dati di Input\nI ricercatori spesso analizzano file di dati grezzi. Tuttavia, alcune analisi SEM possono essere eseguite anche con matrici di covarianze e medie. Se si utilizzano dati grezzi, il software SEM crea una propria matrice di covarianza per l’analisi. Talvolta, è necessario usare dati grezzi, come in casi di distribuzioni non normali, dati mancanti o variabili categoriali.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Preparazione dei Dati</span>"
    ]
  },
  {
    "objectID": "chapters/sem/02_data_preparation.html#definitezza-positiva",
    "href": "chapters/sem/02_data_preparation.html#definitezza-positiva",
    "title": "49  Preparazione dei Dati",
    "section": "49.3 Definitezza Positiva",
    "text": "49.3 Definitezza Positiva\nÈ fondamentale che la matrice di dati, sia quella inizialmente fornita come input che quella calcolata dal computer durante l’analisi, soddisfi i criteri di essere positiva definita. Questo concetto implica diverse proprietà chiave: innanzitutto, la matrice deve avere un inverso, il che significa che non è singolare e può essere invertita matematicamente. Inoltre, è necessario che tutti gli autovalori della matrice siano positivi, indicando che non esistono autovalori negativi che potrebbero causare problemi durante l’analisi. Inoltre, la matrice deve essere priva di correlazioni o covarianze al di fuori limite.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Preparazione dei Dati</span>"
    ]
  },
  {
    "objectID": "chapters/sem/02_data_preparation.html#dati-mancanti",
    "href": "chapters/sem/02_data_preparation.html#dati-mancanti",
    "title": "49  Preparazione dei Dati",
    "section": "49.4 Dati Mancanti",
    "text": "49.4 Dati Mancanti\nQuesto è un argomento complesso che richiede l’uso di metodi statistici moderni e sarà approfondito in un capitolo successivo.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Preparazione dei Dati</span>"
    ]
  },
  {
    "objectID": "chapters/sem/02_data_preparation.html#screening-dei-dati",
    "href": "chapters/sem/02_data_preparation.html#screening-dei-dati",
    "title": "49  Preparazione dei Dati",
    "section": "49.5 Screening dei Dati",
    "text": "49.5 Screening dei Dati\n\nCollinearità Estrema, Valori Anomali e Violazioni delle Assunzioni Distribuzionali: È importante gestire questi problemi per assicurare l’affidabilità dei risultati SEM. La collinearità estrema può essere rilevata tramite il fattore di inflazione della varianza (VIF), mentre i valori anomali e le violazioni delle ipotesi distribuzionali richiedono metodi specifici per essere identificati e gestiti.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Preparazione dei Dati</span>"
    ]
  },
  {
    "objectID": "chapters/sem/02_data_preparation.html#varianze-relative",
    "href": "chapters/sem/02_data_preparation.html#varianze-relative",
    "title": "49  Preparazione dei Dati",
    "section": "49.6 Varianze Relative",
    "text": "49.6 Varianze Relative\n\nGestione delle Varianze: La differenza eccessiva tra le varianze può complicare l’iterazione dei metodi di stima in SEM. Per mitigare questo aspetto, i dati con varianze molto basse o alte possono essere riscalati.\n\nIn sintesi, prima di procedere a qualunque analisi statistica è necessario affrontare diversi problemi relativi alla corretta preparazione e gestione dei dati. Questi aspetti sono fondamentali per assicurare l’accuratezza e l’affidabilità dei risultati delle analisi SEM.\n\n\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Preparazione dei Dati</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html",
    "href": "chapters/sem/03_gof.html",
    "title": "50  Test del Modello e Indicizzazione",
    "section": "",
    "text": "50.1 Introduzione\nIn questo capitolo, ci concentriamo sulle due principali categorie di statistiche per valutare l’adattamento globale nei modelli SEM: le statistiche di test del modello e gli indici di adattamento approssimativo. Queste due categorie si riferiscono rispettivamente al test di adattamento del modello e alla misurazione continua della sua bontà.\nUn aspetto critico da considerare è che, benché entrambe le categorie di statistiche valutino la corrispondenza media o generale tra modello e dati, possono non rilevare un cattivo adattamento locale. Questo si riferisce a specifiche coppie di variabili osservate per cui il modello potrebbe non spiegare adeguatamente le associazioni osservate. È fondamentale riconoscere che un modello con adattamento locale inadeguato non dovrebbe essere accettato, indipendentemente dalla sua bontà di adattamento globale.\nLa valutazione completa di un modello SEM segue una sequenza metodica: specificazione del modello, stima dei parametri, verifica dell’adattamento e dei parametri, e, se necessario, modifica del modello. Questo processo iterativo prosegue finché si identifica un modello ritenuto accettabile.\nInoltre, questo capitolo esplora due metodi fondamentali per pianificare la dimensione del campione nei modelli SEM: l’analisi della potenza e la stima della precisione dei parametri (precisione nella pianificazione). Questi approcci sono essenziali per assicurare che lo studio sia adeguatamente dimensionato e che i parametri siano stimati con massima precisione. La valutazione degli indici di bontà dell’adattamento, ampiamente utilizzati nella letteratura, rappresenterà un elemento chiave in questo contesto, fornendo una panoramica completa degli strumenti disponibili per giudicare l’efficacia dei modelli SEM.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#introduzione",
    "href": "chapters/sem/03_gof.html#introduzione",
    "title": "50  Test del Modello e Indicizzazione",
    "section": "",
    "text": "Statistiche di Test del Modello: Queste statistiche prevedono una decisione binaria, ossia stabilire se accettare o respingere le ipotesi nulle riguardanti il modello. La decisione si basa sui valori-p derivati dai test di significatività, con l’obiettivo di verificare se l’intero modello si adatti ai dati osservati.\nIndici di Adattamento Approssimativo: A differenza delle statistiche di test, gli indici di adattamento approssimativo forniscono una misura continua che esprime il grado di adattamento del modello ai dati. Questo approccio ricorda la stima dell’effetto quantitativo più che un test dicotomico, fornendo così una valutazione più dettagliata dell’adattamento e superando la semplice accettazione o rifiuto dell’ipotesi nulla.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#valutazione-della-bontà-di-adattamento-nel-modello-sem",
    "href": "chapters/sem/03_gof.html#valutazione-della-bontà-di-adattamento-nel-modello-sem",
    "title": "50  Test del Modello e Indicizzazione",
    "section": "\n50.2 Valutazione della Bontà di Adattamento nel Modello SEM",
    "text": "50.2 Valutazione della Bontà di Adattamento nel Modello SEM\nNel contesto dei modelli SEM (Structural Equation Modeling), la valutazione dell’adattamento del modello si basa sul confronto tra la matrice di varianze e covarianze stimata dal modello, \\(\\Sigma(\\hat{\\theta})\\), e la matrice di covarianza campionaria, \\(S\\). Il nostro obiettivo è verificare se la discrepanza tra queste due matrici indica possibili inadeguatezze nel modello proposto. Ecco alcuni aspetti rilevanti da considerare:\n\nModelli Saturi vs Modelli Ristretti: Un modello saturo include un numero di parametri in \\(\\theta\\) pari al numero di elementi distinti nella matrice di covarianza. In contrasto, un modello ristretto ha meno parametri rispetto al numero degli elementi distinti nella matrice di covarianza. La differenza tra questi due numeri corrisponde ai gradi di libertà del modello. Per esempio, in un modello saturo, se il numero dei parametri in \\(\\theta\\) e il numero degli elementi distinti nella matrice di covarianza sono entrambi 3, allora il modello ha zero gradi di libertà.\nPerfetto Adattamento dei Modelli Saturi: In un modello saturo, \\(\\Sigma(\\hat{\\theta})\\) coincide sempre con \\(S\\), poiché il modello ha abbastanza parametri per adattarsi perfettamente ai dati del campione. Tuttavia, ciò non implica necessariamente che il modello rappresenti fedelmente la popolazione più ampia. Le stime dei parametri in un modello saturo possono fornire informazioni sui pattern di relazione tra le variabili nel campione specifico, ma è cruciale interpretarle con cautela.\nStima e Identificabilità del Modello: Generalmente, la stima dei parametri non si basa sul semplice risolvere un sistema di equazioni matematiche. Invece, si utilizza una funzione di adattamento o discrepanza tra \\(\\Sigma(\\theta)\\) e \\(S\\), cercando il valore ottimale di \\(\\hat{\\theta}\\) attraverso tecniche di ottimizzazione numerica. Un modello SEM deve essere identificabile, il che significa che deve essere possibile stimare univocamente i parametri del modello. L’identificabilità implica che il numero di unità di informazione, come elementi nella matrice di covarianza, sia maggiore o uguale al numero di parametri da stimare.\n\n\n50.2.1 Gradi di Libertà e Identificabilità del Modello\nI gradi di libertà (dof) in un modello SEM sono calcolati come:\n\\[\ndof = \\# (\\text{unità di informazione}) - \\# (\\text{parametri da stimare})\n\\]\nPer una matrice di covarianza di ordine $ p $, il numero di unità di informazione è $ $. Per garantire l’identificabilità, è necessario soddisfare alcune condizioni:\n\nIn tutti i modelli, l’unità di misura delle variabili latenti deve essere specificata.\nIl numero di unità di informazione deve essere uguale o superiore al numero di parametri da stimare.\nIn modelli ad un fattore, è richiesto un minimo di tre indicatori per una soluzione “appena identificata”.\nIn modelli a più fattori, si raccomanda un minimo di tre indicatori per ogni variabile latente.\n\nUn modello è:\n\n\nNon identificato se \\(dof &lt; 0\\).\n\nAppena identificato o “saturo” se \\(dof = 0\\).\n\nSovra-identificato se \\(dof &gt; 0\\).\n\nÈ importante notare che un’analisi fattoriale con solo due indicatori per un fattore non è possibile, poiché ci sono meno unità di informazione rispetto ai parametri da stimare. Un modello con tre indicatori e un fattore è “appena identificato”, senza gradi di libertà per valutare la bontà dell’adattamento. Per modelli ad un solo fattore comune latente, è quindi necessario disporre di almeno quattro indicatori.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#funzione-di-discrepanza-e-valutazione-della-bontà-di-adattamento",
    "href": "chapters/sem/03_gof.html#funzione-di-discrepanza-e-valutazione-della-bontà-di-adattamento",
    "title": "50  Test del Modello e Indicizzazione",
    "section": "\n50.3 Funzione di Discrepanza e Valutazione della Bontà di Adattamento",
    "text": "50.3 Funzione di Discrepanza e Valutazione della Bontà di Adattamento\nLa funzione di discrepanza tra \\(S\\) (matrice di covarianza osservata) e \\(\\Sigma(\\theta)\\) (matrice di covarianza stimata dal modello in base ai parametri \\(\\theta\\)) misura l’adeguatezza con cui il modello rappresenta i dati. Derivata dalla log-verosimiglianza per una distribuzione normale multivariata, la funzione confronta le strutture di covarianza teoriche e osservate.\nLa formula per la discrepanza ML (Massima Verosimiglianza) è:\n\\[\nFML(S, \\Sigma(\\theta)) = \\log|\\Sigma(\\theta)| - \\log|S| + \\text{traccia}(S\\Sigma(\\theta)^{-1}) - p,\n\\]\ndove \\(|S|\\) e \\(|\\Sigma(\\theta)|\\) indicano i determinanti di \\(S\\) e \\(\\Sigma(\\theta)\\) rispettivamente, e \\(p\\) è la dimensione delle matrici. Vediamo ogni termine per comprenderne il significato.\n\n50.3.1 Componenti della Formula di Discrepanza\n\n\nLogaritmo del determinante della matrice stimata, \\(\\log|\\Sigma(\\theta)|\\):\n\nIl termine \\(\\log|\\Sigma(\\theta)|\\) rappresenta una misura della “dimensione” o “scala” della matrice \\(\\Sigma(\\theta)\\). Più precisamente, il determinante di una matrice di covarianza può essere visto come una misura del volume dello spazio descritto dalle variabili nel modello: maggiore è il determinante, più “ampio” è lo spazio che copre la distribuzione del modello. Il logaritmo del determinante di \\(\\Sigma(\\theta)\\) contribuisce quindi a quantificare la scala complessiva del modello.\n\n\n\nLogaritmo del determinante della matrice osservata, \\(\\log|S|\\):\n\nSimilmente, \\(\\log|S|\\) rappresenta la scala della matrice di covarianza osservata nei dati. Questo termine funge da riferimento per confrontare la scala dei dati con quella stimata dal modello. In altre parole, \\(|S|\\) ci dice quale sarebbe la “dimensione” dei dati se fossero perfettamente rappresentati solo da \\(S\\), la matrice di covarianza empirica.\n\n\n\nTraccia del prodotto \\(S\\Sigma(\\theta)^{-1}\\):\n\nLa traccia, ossia la somma degli elementi diagonali, del prodotto \\(S\\Sigma(\\theta)^{-1}\\) rappresenta la relazione tra \\(S\\) e l’inverso della matrice \\(\\Sigma(\\theta)\\). Se \\(S\\) e \\(\\Sigma(\\theta)\\) fossero perfettamente identiche, questa traccia sarebbe pari a \\(p\\), la dimensione delle matrici, perché il prodotto di una matrice con la propria inversa è la matrice identità, che ha una somma degli elementi diagonali pari alla dimensione. Un valore diverso da \\(p\\) indica discrepanze tra le covarianze osservate e quelle stimate.\n\n\n\nTermine di normalizzazione, \\(-p\\):\n\nSottrarre \\(p\\) serve a normalizzare la traccia in modo che, in assenza di discrepanze (ovvero quando \\(S = \\Sigma(\\theta)\\)), il valore complessivo della funzione di discrepanza sia zero. Questo termine fa sì che la discrepanza sia relativa a quanto \\(S\\) differisca da \\(\\Sigma(\\theta)\\) in una forma più bilanciata.\n\n\n\n50.3.2 Interpretazione Complessiva\nLa funzione di discrepanza combina queste tre componenti per ottenere una misura della distanza o della differenza tra \\(S\\) e \\(\\Sigma(\\theta)\\). Essa confronta sia la “dimensione” complessiva (tramite i termini log-determinante) sia la “forma” (tramite la traccia) delle due matrici. In sintesi, la funzione di discrepanza \\(FML(S, \\Sigma(\\theta))\\) ci indica quanto il modello con parametri \\(\\theta\\) si discosta dai dati osservati e consente di capire se il modello è una buona rappresentazione delle relazioni di covarianza presenti nei dati.\nSe questa funzione di discrepanza risulta elevata, significa che le covarianze stimate dal modello non rispecchiano adeguatamente quelle osservate, indicando una possibile necessità di migliorare il modello o di rivedere i parametri \\(\\theta\\).\n\n50.3.3 Distribuzione e Test di Adattamento\nLa discrepanza calcolata, sotto l’ipotesi di buon adattamento, si distribuisce asintoticamente come una variabile chi-quadrato (χ²), che permette un test statistico. I gradi di libertà sono dati dalla differenza tra il numero di elementi indipendenti nella matrice di covarianza e il numero di parametri del modello.\n\nSe il valore di discrepanza è minore del valore critico χ², l’ipotesi nulla di buon adattamento non viene rifiutata, suggerendo un buon modello.\nSe invece è maggiore, l’ipotesi viene rifiutata, indicando una necessità di revisione del modello.\n\nQuesto test fornisce un’indicazione quantitativa della bontà di adattamento, utile per valutare se le strutture teoriche catturano adeguatamente le relazioni nei dati.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#test-chi2",
    "href": "chapters/sem/03_gof.html#test-chi2",
    "title": "50  Test del Modello e Indicizzazione",
    "section": "\n50.4 Test \\(\\chi^2\\)\n",
    "text": "50.4 Test \\(\\chi^2\\)\n\nIl test del chi quadrato (\\(\\chi^2\\)) è utilizzato per determinare quanto bene un modello teorico si adatta ai dati osservati. La formula per calcolare la statistica \\(\\chi^2\\) è:\n\\[\n\\chi^2 = N \\times F_{\\text{min}},\n\\]\ndove: - \\(N\\) rappresenta la dimensione del campione. - \\(F_{\\text{min}}\\) è il valore minimo della funzione di discrepanza.\nLa funzione di discrepanza, \\(F\\), è una misura di quanto le covarianze (o le varianze) osservate nei dati differiscano da quelle previste dal modello. Durante il processo di stima dei parametri del modello, questa funzione viene minimizzata. Il valore di \\(F\\) al suo minimo, \\(F_{\\text{min}}\\), rappresenta la discrepanza minima tra i dati osservati e quelli previsti dal modello.\nNell’ambito dell’analisi strutturale di covarianza, il valore di \\(F_{\\text{min}}\\) è tipicamente ottenuto attraverso la stima di massima verosimiglianza (Maximum Likelihood, ML). Tuttavia, ci sono due modi comuni per calcolare \\(\\chi^2\\), che possono variare a seconda del software utilizzato:\n\n\\(\\chi^2 = (N - 1) \\times F_{\\text{min}}\\)\n\\(\\chi^2 = N \\times F_{\\text{min}}\\)\n\nLa scelta tra \\(N\\) e \\(N-1\\) dipende da come il software gestisce la normalizzazione e l’adattamento delle strutture di covarianza.\n\n50.4.1 Interpretazione del Test del \\(\\chi^2\\)\n\n\n\nIpotesi Nulla $ H_0 $: Il modello si adatta bene ai dati. Ciò significa che non c’è una differenza significativa tra le covarianze osservate e quelle previste dal modello.\n\nValore p: Un valore p basso (ad esempio, minore di 0.05) suggerisce che dovremmo rifiutare l’ipotesi nulla, indicando che il modello non si adatta bene ai dati.\n\n50.4.2 Limitazioni\nLa statistica \\(\\chi^2\\) è influenzata dalla dimensione del campione: con campioni ampi, anche lievi discrepanze tra il modello e i dati possono portare a un valore di \\(\\chi^2\\) elevato, risultando in un rifiuto ingiustificato di un modello valido. Inoltre, il test del \\(\\chi^2\\) presenta alcune limitazioni importanti:\n\n\nNon fornisce indicazioni sulla direzione o sulla natura della discrepanza: il test non specifica dove il modello si discosta dai dati o in che modo le discrepanze si manifestano.\n\nEfficacia ridotta in modelli complessi: per modelli con molteplici parametri, o in condizioni in cui le ipotesi fondamentali (ad esempio, la normalità multivariata) non sono soddisfatte, il test del \\(\\chi^2\\) potrebbe non essere affidabile.\n\nPer tali ragioni, è comune integrare il test del \\(\\chi^2\\) con altri indici di adattamento, come l’indice di adattamento comparativo (CFI) e la radice dell’errore quadratico medio di approssimazione (RMSEA), per ottenere una valutazione più accurata e robusta dell’adattamento del modello ai dati.\nIl test del \\(\\chi^2\\) resta dunque un utile strumento di valutazione, ma è fondamentale interpretarlo con cautela, tenendo conto delle dimensioni del campione e di altri fattori che possono influire sul risultato. Nonostante le sue limitazioni, la statistica \\(\\chi^2\\) ha un ruolo importante in contesti specifici, come:\n\n\nConfronto tra modelli nidificati: consente di valutare se aggiunte o modifiche migliorano significativamente l’adattamento del modello.\n\nCalcolo di altri indici di adattamento: come l’indice di Tucker-Lewis (TLI).\n\nRapporto tra \\(\\chi^2\\) e gradi di libertà: un rapporto basso è indicativo di un buon adattamento relativo del modello ai dati.\n\nIn conclusione, pur essendo utile, la statistica \\(\\chi^2\\) va affiancata da altri strumenti di valutazione per una comprensione più completa e bilanciata della bontà di adattamento del modello.\n\n50.4.3 Test di rapporto di verosimiglianza\nIl test del \\(\\chi^2\\) può essere impiegato come un test di rapporto di verosimiglianza per confrontare due modelli nidificati. In questo contesto, “nidificati” significa che uno dei modelli (considerato il modello più semplice o ristretto) è un caso speciale dell’altro (il modello più complesso), con meno parametri liberi da stimare. Questo tipo di test è particolarmente utile per valutare se l’aggiunta di parametri supplementari (rendendo il modello più complesso) migliora significativamente l’adattamento del modello ai dati.\nIl processo di confronto tra i due modelli avviene nel seguente modo:\n\nSi stima il modello più semplice e si calcola il suo valore di \\(\\chi^2\\).\nSi stima il modello più complesso e si calcola il suo valore di \\(\\chi^2\\).\nSi confrontano i due valori di \\(\\chi^2\\) per determinare se l’aggiunta di parametri aggiuntivi giustifica un miglioramento dell’adattamento del modello ai dati, dati i gradi di libertà aggiuntivi.\n\nSe il valore p associato al \\(\\chi^2\\) del modello più complesso è significativamente più basso rispetto a quello del modello più semplice, questo suggerisce che l’aggiunta dei parametri fornisce un miglioramento significativo nell’adattamento del modello. Al contrario, se non vi è un miglioramento significativo, si può concludere che il modello più semplice è preferibile in termini di parsimonia e adattamento.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#chi-quadrato-normalizzato-nc",
    "href": "chapters/sem/03_gof.html#chi-quadrato-normalizzato-nc",
    "title": "50  Test del Modello e Indicizzazione",
    "section": "\n50.5 Chi Quadrato Normalizzato (NC)",
    "text": "50.5 Chi Quadrato Normalizzato (NC)\nIl Chi Quadrato Normalizzato (NC) emerge come un tentativo di attenuare l’effetto della dimensione del campione sulla statistica del chi quadrato del modello (\\(\\chi^2\\)). Questa pratica, adottata da alcuni ricercatori, consiste nel dividere \\(\\chi^2\\) per il numero dei gradi di libertà del modello (dfM), risultando nella formula \\(\\frac{\\chi_{ML}}{dfM}\\). Nonostante l’intento di mitigare l’impatto della dimensione del campione (N), l’impiego di NC presenta limitazioni sostanziali:\n\n\nInfluenza di N sui Modelli Erronei: La statistica \\(\\chi_{ML}\\) è sensibile a N esclusivamente per i modelli non corretti. Questo implica che l’uso di NC per modelli veritieri potrebbe essere fuorviante.\n\nIndipendenza di dfM da N: I gradi di libertà del modello (dfM) non sono correlati con la dimensione del campione, rendendo la divisione di \\(\\chi_{ML}\\) per dfM arbitraria e priva di fondamento statistico.\n\nMancanza di Linee Guida: Non esistono criteri consolidati che definiscano i limiti “accettabili” per il valore di NC. Per esempio, non è chiaro se un valore massimo di NC debba essere inferiore a 2.0, 3.0, o altro.\n\nIn conclusione, data la mancanza di una solida giustificazione statistica o logica, Kline (2023) sconsiglia l’utilizzo del Chi Quadrato Normalizzato come strumento di valutazione della bontà di adattamento del modello.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#chi-quadrato-scalato-e-errori-standard-robusti-per-distribuzioni-non-normali",
    "href": "chapters/sem/03_gof.html#chi-quadrato-scalato-e-errori-standard-robusti-per-distribuzioni-non-normali",
    "title": "50  Test del Modello e Indicizzazione",
    "section": "\n50.6 Chi Quadrato Scalato e Errori Standard Robusti per Distribuzioni Non Normali",
    "text": "50.6 Chi Quadrato Scalato e Errori Standard Robusti per Distribuzioni Non Normali\nNell’ambito dell’analisi di massima verosimiglianza (ML), sia l’approccio ML standard che quello robusto forniscono le stesse stime dei parametri. Tuttavia, il ML robusto differisce nell’introduzione di chi quadrati scalati (\\(\\chi^2\\) scalati) e di errori standard robusti, i quali sono adattati per controbilanciare gli effetti della non normalità dei dati.\n\n50.6.1 Chi Quadrato Scalato di Satorra-Bentler\nUn metodo sviluppato da Satorra e Bentler, che si basa sull’utilizzo di dati completi, calcola il chi quadrato scalato (\\(\\chi_{SB}\\)) applicando un fattore di correzione di scala, indicato con \\(c\\), al valore del chi quadrato non scalato del modello (\\(\\chi_{ML}\\)). Questo fattore di scala \\(c\\) è determinato dalla curtosi multivariata media osservata nei dati grezzi. La formula specifica per il calcolo di \\(\\chi_{SB}\\) è:\n\\[\n\\chi_{SB} = \\frac{\\chi_{ML}}{c}.\n\\]\nQuesta formula evidenzia come il chi quadrato scalato di Satorra-Bentler modifica il chi quadrato tradizionale per tenere conto della curtosi nei dati, fornendo così una misura più affidabile della bontà di adattamento del modello in presenza di distribuzioni non normali.\nLe distribuzioni di \\(\\chi_{SB}\\) tendono ad avvicinarsi alle distribuzioni chi quadrato centrali, ma con una caratteristica fondamentale: le loro medie sono asintoticamente corrette. Questo significa che, su larga scala, \\(\\chi_{SB}\\) fornisce una stima media accurata della discrepanza tra i dati osservati e quelli previsti dal modello, correggendo per eventuali distorsioni causate dalla non normalità dei dati.\n\n50.6.2 Chi Quadrato Scalato di Asparouhov e Muthén\nUn altro tipo di chi quadrato, sviluppato da Asparouhov e Muthén, non si basa sul \\(\\chi_{ML}\\) standard. Invece, nei campioni di grandi dimensioni, il loro chi quadrato scalato corrisponde alla statistica T2* di Yuan e Bentler. Questa versione del chi quadrato è particolarmente adatta per gestire dati non normali o con valori mancanti. I gradi di libertà, sia per \\(\\chi_{SB}\\) che per T2*, sono rappresentati da dfM, indicando la flessibilità del modello in termini di numero di parametri stimabili.\n\n50.6.3 Altri Chi Quadrato Corretti\nAl di là di questi, esistono chi quadrati che sono corretti sia per la media che per la varianza. Questi chi quadrati utilizzano fattori di scala diversi e, in genere, seguono distribuzioni chi quadrato centrali con medie e varianze che sono corrette in modo asintotico. Sebbene questi metodi richiedano maggiori risorse computazionali rispetto ai metodi che correggono solo per la media, tendono ad essere più precisi, specialmente in campioni di grandi dimensioni. Questa precisione aggiuntiva è particolarmente utile quando si affrontano set di dati complessi o di ampie dimensioni, permettendo una stima più accurata della bontà di adattamento del modello.\n\n50.6.4 Metodi Robusti con lavaan\n\nIl pacchetto lavaan offre diverse opzioni per implementare metodi robusti di stima basati sulla massima verosimiglianza (ML). Questi metodi sono particolarmente utili in presenza di deviazioni dalle assunzioni di normalità multivariata. Ecco le principali opzioni disponibili:\n\n\nMLM: Utilizzato per dati completi, calcola un chi-quadrato scalato secondo il metodo di Satorra-Bentler basato sulla media.\n\nMLR: Applicabile sia a dati completi che incompleti, genera un chi-quadrato corretto per la media basato sulla statistica T2* di Yuan-Bentler. È particolarmente indicato per analisi con dati mancanti.\n\nMLMV: Per dati completi, produce un chi-quadrato scalato corretto per la media e la varianza.\n\nMLMVS: Adatto a dati completi, utilizza una correzione per eteroschedasticità basata sul metodo di Satterthwaite, calcolando un chi-quadrato corretto per media e varianza.\n\n\n50.6.4.1 La Matrice di Informazione in lavaan\n\nUn concetto centrale per i metodi ML robusti è la matrice di informazione, utilizzata per stimare gli errori standard dei parametri. Questa matrice rappresenta la varianza e la covarianza dei parametri stimati ed è cruciale per testare ipotesi e costruire intervalli di credibilità o confidenza. In lavaan, la matrice di informazione può essere calcolata in due modi:\n\nMatrice di Informazione Attesa: È l’opzione predefinita per il calcolo degli errori standard. Questa matrice si basa sulle aspettative teoriche delle varianze e covarianze dei parametri stimati, derivate dal modello e dai dati. È generalmente utilizzata in condizioni di dati completi e normali.\nMatrice di Informazione Osservata: Viene impiegata quando sono presenti dati mancanti. In questo caso, le varianze e covarianze vengono calcolate utilizzando i dati effettivamente osservati. Questo approccio può fornire stime degli errori standard più affidabili in presenza di incompletezza nei dati.\n\nGli utenti possono scegliere esplicitamente quale matrice utilizzare, ad esempio forzando l’uso della matrice attesa anche con dati incompleti, in base alle esigenze specifiche della loro analisi.\n\n50.6.4.2 Considerazioni Etiche e Metodologiche\nÈ essenziale utilizzare questi strumenti in modo rigoroso e trasparente. Selezionare metodi o combinazioni di chi-quadrati scalati ed errori standard robusti solo per ottenere risultati che meglio supportano le proprie ipotesi compromette l’integrità della ricerca. Per questo motivo, i ricercatori dovrebbero:\n\nDichiarare chiaramente i metodi utilizzati, incluso il tipo di matrice di informazione scelta.\nSegnalare eventuali variazioni nei risultati legate alla scelta del metodo.\nGarantire che le analisi siano guidate da principi metodologici, non da convenienze interpretative.\n\nQueste buone pratiche sono fondamentali per mantenere l’affidabilità e la credibilità delle analisi svolte.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#indicizzazione-delladattamento-del-modello",
    "href": "chapters/sem/03_gof.html#indicizzazione-delladattamento-del-modello",
    "title": "50  Test del Modello e Indicizzazione",
    "section": "\n50.7 Indicizzazione dell’Adattamento del Modello",
    "text": "50.7 Indicizzazione dell’Adattamento del Modello\nL’indicizzazione dell’adattamento del modello si basa sull’uso di indici di adattamento approssimati, i quali si differenziano dai test di significatività tradizionali. Invece di fornire una decisione dicotomica, come il rifiuto o l’accettazione di un’ipotesi nulla, questi indici offrono una misura continua di quanto bene un modello si adatta ai dati osservati. Non essendoci una separazione netta tra i limiti dell’errore di campionamento, gli indici di adattamento forniscono una valutazione più sfumata e graduale della bontà di adattamento.\nQuesti indici possono essere classificati in due categorie principali:\n\n\nStatistiche di Cattivo Adattamento: In questa categoria, valori più elevati indicano un peggior adattamento del modello ai dati. Un esempio tipico di questa categoria è il chi quadrato del modello, dove valori più alti suggeriscono una maggiore discrepanza tra il modello e i dati.\n\nStatistiche di Buon Adattamento: Al contrario, per gli indici in questa categoria, valori più alti segnalano un migliore adattamento del modello ai dati. Molti di questi indici sono normalizzati in modo che il loro intervallo varii da 0 a 1.0, dove 1.0 rappresenta l’adattamento ottimale del modello.\n\nA differenza del test del chi quadrato, che si basa su un framework teorico ben definito, l’interpretazione e l’applicazione degli indici di adattamento approssimati non sono guidate da un unico insieme di principi teorici consolidati. Questa situazione fa sì che la valutazione dell’adattamento del modello si allinei maggiormente a ciò che Little (2013) ha descritto come “scuola di modellazione”. Questo approccio contempla l’analisi di modelli statistici complessi in un contesto in cui le regole decisionali sono meno rigide e più soggette a interpretazione.\nLa natura flessibile di questo approccio rispecchia la varietà e la complessità dei modelli statistici, che devono essere personalizzati per rispondere a specifiche domande di ricerca. Questa flessibilità, tuttavia, porta con sé una certa ambiguità nelle regole di valutazione dei modelli statistici. Pur offrendo la possibilità di adattare l’analisi alle particolarità di ogni studio, questa mancanza di rigore teorico uniforme può talvolta non tradursi in pratiche ottimali di modellazione.\nLa questione filosofica relativa all’adattamento esatto dei modelli statistici solleva dubbi sull’idea di perfezione come standard per questi modelli. In effetti, è ampiamente riconosciuto che tutti i modelli statistici sono in qualche misura imperfetti; sono piuttosto strumenti di approssimazione che aiutano i ricercatori a organizzare e interpretare le loro osservazioni sui fenomeni di interesse. Un modello troppo semplificato, che non cattura la complessità del fenomeno, può essere inadeguato e quindi rifiutato. Allo stesso tempo, un modello eccessivamente complesso, che cerca di replicare fedelmente il fenomeno, può risultare di scarsa utilità scientifica a causa della sua complessità eccessiva.\nGeorge Box, nel suo influente lavoro del 1976, avanzò l’idea che nessun modello statistico potesse essere considerato perfettamente “corretto”. Questa visione nasce dalla consapevolezza che tutti i modelli hanno una certa dose di imperfezione intrinseca. Box suggeriva che lo scopo principale di uno scienziato dovrebbe essere la ricerca di una “descrizione economica” dei fenomeni naturali, cercando cioè di formulare modelli che siano semplici, ma al contempo efficaci, nella rappresentazione della realtà. Egli criticava la tendenza a sovraelaborare o sovraparametrizzare i modelli, considerandola un segno di mediocrità nella pratica scientifica. Box enfatizzava l’importanza di concentrarsi sugli errori sostantivi, o “tigri”, piuttosto che su piccole imperfezioni, o “topi”, affermando:\n\n“Since all models are wrong the scientist must be alert to what is importantly wrong. It is inappropriate to be concerned about mice when there are tigers abroad.”\n\nCiò implica che l’obiettivo nella modellazione statistica non dovrebbe essere una perfezione irraggiungibile, ma piuttosto lo sviluppo di modelli che, pur nella loro semplicità, riescano a cogliere gli aspetti fondamentali dei fenomeni analizzati. Questo richiede un equilibrio tra la complessità necessaria per una descrizione accurata e la semplicità che rende un modello pratico e interpretabile.\nHayduk (2014), nel commentare l’affermazione di Box, si focalizza specificatamente sul contesto della modellizzazione SEM (Structural Equation Modeling). Egli identifica le “tigri”, ovvero gli errori gravi nei modelli, come indicatori di una specificazione errata del modello. Hayduk sottolinea l’importanza critica di riconoscere e correggere gli errori significativi piuttosto che disperdere energie su dettagli minori. In sostanza, Hayduk rafforza l’idea che è essenziale distinguere tra errori minori e maggiori, questi ultimi potendo compromettere seriamente la validità e l’utilità di un modello statistico.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#tipologie-di-indici-di-adattamento-approssimati",
    "href": "chapters/sem/03_gof.html#tipologie-di-indici-di-adattamento-approssimati",
    "title": "50  Test del Modello e Indicizzazione",
    "section": "\n50.8 Tipologie di Indici di Adattamento Approssimati",
    "text": "50.8 Tipologie di Indici di Adattamento Approssimati\nGli indici di adattamento approssimati possono essere classificati in diverse categorie, che riflettono diversi aspetti della bontà di adattamento di un modello statistico ai dati. Sebbene questa classificazione non sia esaustiva né le categorie siano mutualmente esclusive, i tipi principali di indici di adattamento sono i seguenti:\n\nIndici di Adattamento Assoluto: Questi indici, come il GFI (Goodness of Fit Index), misurano quanto bene un modello spiega i dati senza riferimento ad altri modelli. Indicano l’abilità del modello di riprodurre i dati osservati.\nIndici di Adattamento Parsimonioso: Questi indici confrontano i gradi di libertà del modello (dfM) con il numero massimo possibile di gradi di libertà disponibili nei dati. Un esempio è l’AGFI (Adjusted Goodness of Fit Index), che incorpora una penalità per la complessità del modello, benché non sia un indice di adattamento parsimonioso come definito in questa categoria.\nIndici di Adattamento Incrementale (Relativo o Comparativo): Questi indici confrontano l’adattamento del modello del ricercatore con quello di un modello di base, tipicamente un modello di indipendenza che assume covarianze nulle tra le variabili osservate. È possibile scegliere un modello di base diverso, sebbene il calcolo manuale dell’indice possa essere necessario se il modello di base desiderato differisce da quello predefinito nel software.\nIndici di Adattamento Non Centrale: Stimano il grado in cui l’ipotesi di adattamento esatto è falsa, dati il modello e i dati. Questi indici approssimano parametri nelle distribuzioni chi quadrato non centrali, che descrivono anche le distribuzioni campionarie per gli indici di adattamento di questo tipo.\nIndici di Adattamento Predittivo (o basati sulla Teoria dell’Informazione): Derivati dalla teoria dell’informazione, stimano l’adattamento del modello in campioni di replica ipotetici della stessa dimensione, estratti casualmente dalla stessa popolazione del campione originale. Sono utilizzati principalmente per confrontare modelli alternativi basati sulle stesse variabili e adattati agli stessi dati, ma dove i modelli non sono gerarchicamente correlati.\n\nNon tutti gli indici di adattamento approssimati hanno resistito alla prova del tempo. Ad esempio, gli indici di adattamento parsimonioso non hanno mai raggiunto una popolarità significativa tra i ricercatori applicati, restando relativamente oscuri. Altri indici, come il GFI e l’AGFI, sono stati criticati per la loro sensibilità alla dimensione del campione e al numero di indicatori nei modelli di analisi fattoriale.\nI software moderni per la Structural Equation Modeling (SEM) presentano una notevole varietà nel numero di indici di adattamento approssimati forniti nei loro output. Programmi come Amos e LISREL elencano un numero elevato di indici (oltre 12), mentre altri come lavaan e Mplus ne includono un numero più limitato (circa 4-5). Questa abbondanza di indici può portare al rischio di “cherry-picking”, cioè la tendenza a selezionare e riportare solo quegli indici che mostrano risultati favorevoli al modello proposto dal ricercatore. Per mitigare questo rischio, è consigliabile limitarsi a un insieme essenziale di indici e prestare attenzione all’analisi dei residui.\n\n50.8.1 Modello Baseline\nIl modello baseline, noto anche come modello nullo, è un modello in cui tutte le covarianze sono impostate a zero, mentre le varianze sono stimate liberamente. In questo modello, non si stimano i carichi fattoriali; ci si limita invece a stimare le medie e le varianze osservate, eliminando tutte le covarianze tra le variabili.\nÈ utile pensare al modello nullo o baseline come il peggior modello possibile, da confrontare poi con il modello saturato, che rappresenta invece la migliore approssimazione ai dati. Teoricamente, il modello baseline è fondamentale per comprendere come vengono calcolati altri indici di adattamento del modello, in quanto fornisce un punto di riferimento iniziale per la valutazione della bontà di adattamento in un contesto di Modelli di Equazioni Strutturali.\n\n50.8.2 Set di Indici di Adattamento Consigliati\nKline (2023) suggerisce un insieme essenziale di soli tre indici di adattamento approssimati, che sono ampiamente utilizzati nei software SEM e frequentemente presenti negli studi pubblicati. Questi indici sono stati selezionati per le seguenti ragioni:\n\nAmpia Presenza nella Letteratura: Sono ampiamente riportati in numerosi studi SEM, rendendoli familiari sia ai ricercatori che ai revisori.\nStandardizzazione: Le scale di questi indici non dipendono dalle variabili osservate o latenti, fornendo così una misura standardizzata di adattamento.\nValidità Statistica Estesa: Almeno uno di questi indici, l’RMSEA, possiede un solido fondamento statistico e un quadro interpretativo più ampio per la stima degli intervalli, i test delle ipotesi e la pianificazione della dimensione del campione.\n\nNonostante la loro utilità, è fondamentale usare questi indici con attenzione. I ricercatori dovrebbero evitare l’uso acritico di soglie o punti di taglio, sia fissi sia variabili, che si suppone differenzino tra modelli con un buon o cattivo adattamento. L’applicazione di queste soglie può essere problematica, poiché non sono valide universalmente per tutti i tipi di modelli e set di dati. L’uso improprio di tali soglie può portare a decisioni errate, in particolare se si trascura l’analisi dei residui.\nIl gruppo principale di tre indici di adattamento approssimati raccomandato comprende:\n\nRoot Mean Square Error of Approximation (RMSEA) di Steiger-Lind (Steiger, 1990), accompagnato dal suo intervallo di confidenza al 90%. L’RMSEA valuta l’adattamento assoluto del modello, penalizzando la complessità del modello, ma non è un indice di adattamento parsimonioso. È un indice di cattivo adattamento dove il valore zero rappresenta l’adattamento ideale, senza un limite massimo teorico.\nComparative Fit Index (CFI) di Bentler (Bentler, 1990). Il CFI è un indice di adattamento incrementale e valuta la bontà di adattamento relativa del modello rispetto a un modello di base. Si estende su una scala da 0 a 1.0, dove 1.0 indica l’adattamento ottimale, e non impone penalità per la complessità del modello.\nStandardized Root Mean Square Residual (SRMR) (Jöreskog & Sörbom; 1981). L’SRMR è un indice di adattamento assoluto che misura la discrepanza tra le correlazioni osservate e quelle previste dal modello. Un valore di zero indica un adattamento perfetto.\n\nSia l’RMSEA sia il CFI incorporano il chi quadrato del modello e i suoi gradi di libertà nelle loro formule. Questo implica che condividono le stesse assunzioni distributive della corrispondente statistica di test. Se tali assunzioni non sono valide, i valori degli indici e della statistica di test (incluso il valore p) potrebbero non essere accurati. Entrambi gli indici sono stati inizialmente definiti per dati continui con distribuzioni normali analizzati tramite ML standard. Tuttavia, in presenza di dati significativamente non normali, i valori di chiML, RMSEA e CFI possono risultare distorti. Alcuni software SEM implementano correzioni ad hoc per la non normalità.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#misure-di-adeguamento-per-il-confronto",
    "href": "chapters/sem/03_gof.html#misure-di-adeguamento-per-il-confronto",
    "title": "50  Test del Modello e Indicizzazione",
    "section": "\n50.9 Misure di adeguamento per il confronto",
    "text": "50.9 Misure di adeguamento per il confronto\n\n50.9.1 CFI\nGli indici di adattamento comparativo [detti anche indici di adattamento incrementale; ad es. Hu & Bentler (1998)] valutano l’adattamento di una soluzione specificata dall’utente in relazione a un modello di base nidificato più ristretto. Tipicamente, il modello base è un modello “nullo” o “di indipendenza” in cui le covarianze tra tutti gli indicatori di input sono fissate a zero, ma nessun vincolo viene posto sulle varianze degli indicatori. Uno di questi indici, l’indice di adattamento comparativo (comparative fit index, CFI; Bentler, 1990). Il CFI si basa su un confronto relativo, situando il modello di interesse lungo un continuum che va dal modello peggiore (nullo) al modello perfetto (saturo).\nIl CFI valuta la riduzione relativa del parametro di non-centralità (\\(\\lambda\\)) tra il modello di interesse e il modello di riferimento (Bentler, 1990). Il parametro di non-centralità \\(\\lambda_m\\) rappresenta il grado di errore di specificazione del modello \\(m\\) ed è calcolato come:\n\\[\n\\lambda_m = \\chi^2_m - \\text{df}_m,\n\\]\ndove \\(\\chi^2_m\\) è il valore chi-quadro stimato per il modello e \\(\\text{df}_m\\) rappresenta i gradi di libertà. Più alto è \\(\\lambda_m\\), maggiore è la discrepanza tra il modello e i dati osservati. Il valore del CFI si basa sul rapporto tra i parametri di non-centralità del modello di interesse (\\(\\lambda_m\\)) e del modello nullo (\\(\\lambda_b\\)):\n\\[\nCFI(m, b) = 1 - \\frac{\\lambda_m}{\\lambda_b} = 1 - \\frac{\\chi^2_m - \\text{df}_m}{\\chi^2_b - \\text{df}_b}.\n\\]\nIl valore del CFI varia generalmente tra 0 e 1 (anche se in casi particolari può superare 1 o essere negativo), dove un valore vicino a 1 indica un buon adattamento del modello rispetto al modello nullo.\n\n50.9.1.1 Modello nullo come baseline\nIl modello nullo è un modello in cui tutte le variabili osservate sono considerate non correlate. Il CFI misura quindi quanto il modello di interesse riesce a migliorare l’adattamento rispetto a questo modello di riferimento, in modo analogo al concetto di \\(R^2\\) per la regressione lineare.\n\n50.9.1.2 Sensibilità ai dati e alle caratteristiche del modello\nIl comportamento del CFI è influenzato da tre fattori principali:\n\n\nDimensione del campione (\\(n\\)): Campioni più grandi aumentano il parametro di non-centralità del modello nullo (\\(\\lambda_b\\)), migliorando la capacità del CFI di distinguere tra modelli.\n\nNumero di variabili osservate (\\(p\\)): Un numero elevato di variabili può complicare l’interpretazione del CFI, poiché aumenta i gradi di libertà del modello nullo, riducendo la non-centralità \\(\\lambda_b\\).\n\nCorrelazione tra variabili (\\(R\\)): Maggiore è la correlazione tra le variabili, più il modello nullo differisce dai dati, e più il CFI può differenziare tra modelli.\n\n50.9.1.3 Regole empiriche\nValori del CFI superiori a 0.90 erano considerati accettabili in passato (Bentler & Bonett, 1980), mentre valori superiori a 0.95 sono oggi considerati indicativi di un buon adattamento (Hu & Bentler, 1999). Tuttavia, studi di simulazione più recenti, come quelli di Fan e Sivo nel 2005 e di Yuan nel 2005, hanno messo in dubbio l’universalità di un valore soglia specifico per il CFI, evidenziando che l’adeguatezza di tale valore può variare a seconda delle caratteristiche dei modelli e del grado di non normalità nei dati. Di conseguenza, è importante non applicare queste regole in modo meccanico, ma valutare il contesto specifico. Inoltre, Brosseau-Liard e Savalei (2014) hanno descritto delle versioni robuste del CFI adatte per dati non normali. Queste versioni del CFI sono calcolate e fornite dal software lavaan quando si utilizzano metodi di stima Maximum Likelihood (ML) robusti. Questo implica che, quando si lavora con dati che presentano deviazioni dalla normalità, queste versioni robuste del CFI possono offrire una misura più affidabile dell’adattamento del modello.\n\n50.9.1.4 Variabilità campionaria\nA livello di popolazione, un modello corretto dovrebbe avere un valore di CFI pari a 1. Tuttavia, la variabilità campionaria può influenzare il parametro di non-centralità \\(\\lambda_m\\) e \\(\\lambda_b\\), causando deviazioni rispetto alle aspettative teoriche. Questo fenomeno è particolarmente rilevante nei campioni piccoli o in presenza di modelli complessi.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#misure-di-adeguamento-parsimonioso",
    "href": "chapters/sem/03_gof.html#misure-di-adeguamento-parsimonioso",
    "title": "50  Test del Modello e Indicizzazione",
    "section": "\n50.10 Misure di adeguamento parsimonioso",
    "text": "50.10 Misure di adeguamento parsimonioso\n\n50.10.1 TLI\nUn indice che rientra nella degli indici di adeguamento parsimonioso è l’indice Tucker-Lewis (Tucker–Lewis index, TLI, anche chiamato indice di adattamento non normato). Il TLI si pone il problema di penalizzare la complessità del modello, ovvero include una funzione di penalizzazione per l’addizione di parametri che non migliorano in maniera sostanziale l’adattamento del modello. Il TLI è calcolato con la seguente formula:\n\\[\n\\begin{equation}\nTLI = \\frac{(\\chi^2_B / dof_B)–(\\chi^2_T / dof_T)}{(\\chi^2_B / dof_B) – 1},\n\\end{equation}\n\\]\ndove \\(\\chi^2_T\\) è il valore \\(\\chi^2\\) del modello target, \\(dof_T\\) sono i gradi di libertà del modello target, \\(\\chi^2_B\\) è il valore \\(\\chi^2\\) del modello baseline e \\(dof_B\\) sono i gradi di libertà del modello base.\nL’Indice di Tucker-Lewis (TLI) può, in teoria, assumere valori inferiori a zero se il modello di base, ovvero un modello diverso da quello studiato dal ricercatore, mostra un ottimo adattamento ai dati. Tuttavia, questa eventualità è rara nella pratica. Al contrario, il TLI può superare il valore di 1.0 se il modello analizzato dal ricercatore si adatta in modo particolarmente stretto ai dati. Marsh e Balla (1994) hanno evidenziato che la dimensione del campione influenza poco i valori del TLI.\nSecondo quanto osservato da Kenny (2020), si possono trarre due conclusioni importanti:\n\nIl Comparative Fit Index (CFI) e il TLI sono entrambi influenzati dall’entità delle correlazioni tra le variabili misurate. Ciò significa che valori medi di correlazione più elevati risultano in valori più alti sia per il CFI che per il TLI, e il contrario è vero per correlazioni medie più basse.\nI valori del CFI e del TLI mostrano una forte correlazione tra loro. Di conseguenza, è consigliabile riportare solo uno dei due indici per evitare ripetizioni e per mantenere la chiarezza del report. La scelta tra CFI e TLI dovrebbe basarsi su criteri specifici relativi al contesto e agli obiettivi dello studio in questione.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#misure-di-adeguamento-assoluto",
    "href": "chapters/sem/03_gof.html#misure-di-adeguamento-assoluto",
    "title": "50  Test del Modello e Indicizzazione",
    "section": "\n50.11 Misure di Adeguamento Assoluto",
    "text": "50.11 Misure di Adeguamento Assoluto\n\n50.11.1 Root Mean Square Error of Approximation (RMSEA)\nL’Errore Quadratico Medio di Approssimazione (RMSEA) misura quanto bene un modello statistico si adatta ai dati osservati, valutando l’adattamento in termini assoluti, piuttosto che confrontarlo con un modello di riferimento (come fanno indici quali CFI e TLI).\nIl calcolo del RMSEA si basa sul chi-quadrato (\\(\\chi^2\\)), che rappresenta la discrepanza tra la matrice di covarianza osservata e quella stimata dal modello. La discrepanza, indicata con \\(\\delta\\), è definita come:\n\\[\n\\delta = \\chi^2 - df,\n\\]\ndove \\(df\\) rappresenta i gradi di libertà del modello. Valori più alti di \\(\\delta\\) indicano una maggiore discrepanza, ossia un peggior adattamento del modello ai dati.\nLa formula generale per il calcolo del RMSEA è:\n\\[\n\\text{RMSEA} = \\sqrt{\\frac{\\max(0, \\delta)}{df \\cdot (n - 1)}},\n\\]\ndove \\(n\\) è il numero di osservazioni nel campione. Questo indice riflette l’errore di approssimazione del modello rispetto alla matrice di covarianza della popolazione, tenendo conto della parsimonia del modello (ossia del numero di gradi di libertà).\n\n50.11.2 Distribuzione del Chi-Quadrato e Non Centralità\nAssumendo che:\n\ni dati seguano una distribuzione normale multivariata;\nil modello sia corretto;\nil campione sia grande e casuale;\n\nil chi-quadrato del modello (\\(\\chi^2_{\\text{ML}}\\)) segue una distribuzione \\(\\chi^2\\) con \\(df_M\\) gradi di libertà. Tuttavia, se il modello non è corretto, il chi-quadrato segue una distribuzione non centrale \\(\\chi^2(df_M, \\lambda)\\), dove \\(\\lambda\\) rappresenta il parametro di non centralità, che indica il grado di discrepanza tra il modello e i dati.\nIl parametro di non centralità normalizzato è definito come:\n\\[\n\\delta_{\\text{norm}} = \\max(0, \\chi^2_{\\text{ML}} - df_M).\n\\]\nQuesto valore è utilizzato per stimare la discrepanza tra la matrice di covarianza osservata e quella della popolazione sotto il modello.\n\n50.11.3 Formula Finale del RMSEA\nIl valore finale del RMSEA, indicato spesso con \\(\\epsilon\\), è calcolato come:\n\\[\n\\epsilon = \\sqrt{\\frac{\\delta_{\\text{norm}}}{df_M \\cdot (n - 1)}}.\n\\]\nSebbene \\(\\epsilon\\) possa essere una stima distorta a causa della restrizione \\(\\epsilon \\geq 0\\), rappresenta una buona approssimazione dell’errore.\n\n50.11.4 Soglie Interpretative\nBrowne e Cudeck (1993) suggerirono che:\n\n\n\\(\\epsilon \\leq 0.05\\) indica un buon adattamento del modello;\n\n\\(0.05 &lt; \\epsilon \\leq 0.08\\) rappresenta un adattamento accettabile;\n\n\\(\\epsilon &gt; 0.10\\) segnala un cattivo adattamento.\n\nTuttavia, queste soglie non sono universali, e si consiglia di valutare anche il limite superiore dell’intervallo di confidenza di \\(\\epsilon\\) (indicato come \\(\\epsilon_U\\)) per un’interpretazione più accurata.\n\n50.11.5 Considerazioni e Versioni Robuste\n\nInterpretazione: L’interpretazione di \\(\\epsilon\\), \\(\\epsilon_L\\) (limite inferiore) e \\(\\epsilon_U\\) (limite superiore) è appropriata in campioni ampi e con modelli ben specificati. In campioni piccoli o con errori di specificazione significativi, è necessaria maggiore cautela.\nPenalità per Modelli Complessi: Studi di simulazione indicano che il RMSEA tende a penalizzare maggiormente i modelli con pochi gradi di libertà (ad esempio, modelli con poche variabili).\nVersioni Robuste: Versioni robuste del RMSEA, come quella basata sul chi-quadrato scalato di Satorra-Bentler, correggono gli effetti della non normalità e tendono a essere più accurate rispetto alla versione standard, che può sovrastimare l’indice in condizioni di non normalità.\nPotenza Statistica: Esistono metodi per calcolare la potenza statistica associata a ipotesi nulle basate sul RMSEA e per stimare la dimensione minima del campione necessaria a raggiungere determinati livelli di potenza.\n\nConcludendo, il RMSEA è uno strumento potente per valutare l’adattamento assoluto di un modello, ma il suo utilizzo richiede attenzione alle specifiche del modello, alla qualità dei dati e al contesto dell’analisi.\n\n50.11.6 Root Mean Square Residual (RMRS)\nA differenza del chi quadrato del modello e dei gradi di libertà, che valutano la bontà di adattamento di un modello in base a criteri di adattamento globale, l’indice RMRS (Root Mean Square Residual) si concentra esclusivamente sui residui del modello, ovvero le discrepanze tra le correlazioni osservate e quelle previste dal modello.\nLa formula per calcolare l’RMRS è la seguente:\n\\[\nRMRS = \\sqrt{ \\frac{2 \\sum_i\\sum_j(r_{ij} - \\hat{r}_{ij})^2}{p(p+1)}},\n\\]\ndove:\n\n\n\\(p\\) rappresenta il numero di item (variabili) nel modello,\n\n\\(r_{ij}\\) è la correlazione osservata tra le variabili \\(i\\) e \\(j\\),\n\n\\(\\hat{r}_{ij}\\) è la correlazione prevista dal modello tra le variabili \\(i\\) e \\(j\\).\n\nUn valore di RMRS pari a 0 indica un adattamento perfetto del modello, mentre valori crescenti indicano un adattamento meno preciso. In generale, un valore di SRMR inferiore a 0.08 è considerato favorevole (Hu e Bentler, 1999).\nTuttavia, è importante notare che il SRMR è una misura media e può nascondere variazioni significative tra i residui di correlazione individuali. Ad esempio, se il SRMR è 0.03, potrebbe sembrare un buon adattamento. Ma se i residui di correlazione variano da -0.12 a 0.18, con alcuni residui superiori a 0.10, potrebbe indicare problemi di adattamento locali più gravi.\nPertanto, quando si riportano i risultati in un report, per ottenere una comprensione più completa dell’adattamento del modello è consigliabile descrivere i residui di correlazione o, meglio ancora, presentare l’intera matrice dei residui, anziché basarsi esclusivamente su un valore medio come il SRMR.\n\n50.11.7 Interpretazione con lavaan\n\nL’interpretazione degli indici di bontà di adattamento trovati nella CFA o nella modellazione di equazioni strutturali può essere ottenuta usando le funzioni del pacchetto effectsize.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#adattamento-locale",
    "href": "chapters/sem/03_gof.html#adattamento-locale",
    "title": "50  Test del Modello e Indicizzazione",
    "section": "\n50.12 Adattamento Locale",
    "text": "50.12 Adattamento Locale\nI modelli SEM possono teoricamente superare i test di adattamento globale ma fallire nei test di adattamento locale. Questi dettagli, relativi all’adattamento del modello, sono esaminati direttamente nei test di adattamento locale. L’analisi dei residui (sia standardizzati che di correlazione) è quindi cruciale per una valutazione completa del modello (Maydeu-Olivares e Shi, 2017). Le recenti norme di reportistica per il SEM richiedono agli autori di descrivere sia l’adattamento globale che quello locale (Appelbaum et al., 2018); Greiff e Heene, 2017; Vernon e Eysenck, 2007).\n\n50.12.1 Residui di Covarianza, Residui Standardizzati, Residui Normalizzati\n\nResidui di Covarianza: Sono le differenze tra le covarianze osservate e quelle previste dal modello. Questi residui possono essere difficili da interpretare perché non sono standardizzati, ovvero la loro metrica dipende dalle scale delle variabili coinvolte. Pertanto, residui di covarianza per coppie di variabili diverse non sono direttamente confrontabili a meno che tutte le variabili non siano sulla stessa metrica.\nResidui Standardizzati: Sono versioni standardizzate dei residui di covarianza, interpretati come un test z in campioni grandi. Un residuo standardizzato significativamente diverso da zero indica una discrepanza tra modello e dati. Tuttavia, la significatività di questi residui può dipendere dalla dimensione del campione, con residui vicini allo zero che possono essere significativi in campioni grandi, mentre residui relativamente grandi potrebbero non essere significativi in campioni piccoli.\nResidui Normalizzati: Sono i rapporti tra i residui di covarianza e l’errore standard della covarianza campionaria. Sono generalmente più conservativi dei residui standardizzati in termini di test di significatività. In modelli complessi, quando non è possibile calcolare il denominatore di un residuo standardizzato, il residuo normalizzato fornisce un’alternativa più conservativa.\n\nNel software lavaan ci sono due opzioni principali per calcolare i residui di correlazione:\n\n\nOpzione cor.bollen: Questa specifica indica al computer di convertire separatamente le matrici di covarianza del campione e quelle implicite dal modello in matrici di correlazione prima di calcolare i residui. Questo processo comporta la standardizzazione di ciascuna matrice in base alle varianze (deviazioni standard quadrate) presenti nella diagonale principale di ciascuna matrice. Le varianze nella matrice di covarianza del campione sono osservate direttamente, mentre le varianze per le variabili endogene nella matrice di covarianza implicata dal modello sono previste dal modello e possono differire dalle varianze osservate corrispondenti.\n\nOpzione cor.bentler: Questa opzione standardizza sia la matrice di covarianza del campione che quella implicata dal modello basandosi sulle varianze presenti solo nella matrice di covarianza del campione. Poiché non tutti gli elementi della diagonale principale nella matrice di covarianza implicata dal modello sono varianze osservate, alcuni valori dei residui di correlazione del tipo Bentler potrebbero non essere pari a zero. Tuttavia, i valori dei residui fuori diagonale per entrambi i metodi sono generalmente simili.\n\nPer impostazione predefinita, lavaan utilizza il metodo cor.bollen per calcolare i residui di correlazione nelle sue analisi.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#esempio-empirico",
    "href": "chapters/sem/03_gof.html#esempio-empirico",
    "title": "50  Test del Modello e Indicizzazione",
    "section": "\n50.13 Esempio Empirico",
    "text": "50.13 Esempio Empirico\nNel capitolo precedente abbiamo formulato un modello SEM nel quale abbiamo definito una variabile latente con le sei sottoscale della Self-Compassion Scale e una seconda variabile latente con le tre sottoscale della DASS-21. Abbiamo ipotizzato che il fattore dell’autocompassione eserciti un effetto (protettivo) nei confronti del disagio psicologico misurato dal fattore definito dalle sottoscale della DASS-21.\nImportiamo i dati in R.\n\nd_sc &lt;- read.csv(\"../../data/dass_rosenberg_scs.csv\", header = TRUE)\n\nDefiniamo il modello SEM.\n\nmod_sc &lt;- \"\n  F =~ anxiety + depression + stress\n  SC =~ self_kindness   + common_humanity   + mindfulness   + \n        self_judgment   + isolation + over_identification\n  F ~ SC\n\"\n\nAdattiamo il modello.\n\nfit_sc &lt;- lavaan::sem(mod_sc, d_sc)\n\nCreiamo un diagramma di percorso.\n\nsemPlot::semPaths(fit_sc,\n    what = \"col\", whatLabels = \"std\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\nI coefficienti stimati nel modello SEM appaiono coerenti e in linea con le aspettative, in particolare il coefficiente che descrive l’effetto “causale” del fattore dell’autocompassione sul disagio emotivo, che si attesta a -0.48. Questo valore negativo corrobora l’ipotesi secondo cui l’autocompassione svolge un ruolo di fattore protettivo contro il disagio emotivo. Tuttavia, prima di confermare definitivamente questa conclusione, è cruciale esaminare gli indici di bontà di adattamento del modello. Questi indici ci permetteranno di valutare quanto accuratamente il modello SEM si adatta ai dati osservati, fornendo un quadro più chiaro della validità delle nostre inferenze. In altre parole, sebbene il modello suggerisca una relazione negativa tra autocompassione e disagio emotivo, la conferma finale di questa associazione dipenderà dall’adeguatezza complessiva del modello rispetto ai dati.\nCalcoliamo gli indici di bontà di adattamento.\n\nfitMeasures(fit_sc) \n#&gt;                  npar                  fmin                 chisq \n#&gt;                19.000                 0.427               449.141 \n#&gt;                    df                pvalue        baseline.chisq \n#&gt;                26.000                 0.000              3129.133 \n#&gt;           baseline.df       baseline.pvalue                   cfi \n#&gt;                36.000                 0.000                 0.863 \n#&gt;                   tli                  nnfi                   rfi \n#&gt;                 0.811                 0.811                 0.801 \n#&gt;                   nfi                  pnfi                   ifi \n#&gt;                 0.856                 0.619                 0.864 \n#&gt;                   rni                  logl     unrestricted.logl \n#&gt;                 0.863            -12308.490            -12083.920 \n#&gt;                   aic                   bic                ntotal \n#&gt;             24654.980             24736.021               526.000 \n#&gt;                  bic2                 rmsea        rmsea.ci.lower \n#&gt;             24675.710                 0.176                 0.162 \n#&gt;        rmsea.ci.upper        rmsea.ci.level          rmsea.pvalue \n#&gt;                 0.190                 0.900                 0.000 \n#&gt;        rmsea.close.h0 rmsea.notclose.pvalue     rmsea.notclose.h0 \n#&gt;                 0.050                 1.000                 0.080 \n#&gt;                   rmr            rmr_nomean                  srmr \n#&gt;                 1.203                 1.203                 0.071 \n#&gt;          srmr_bentler   srmr_bentler_nomean                  crmr \n#&gt;                 0.071                 0.071                 0.079 \n#&gt;           crmr_nomean            srmr_mplus     srmr_mplus_nomean \n#&gt;                 0.079                 0.071                 0.071 \n#&gt;                 cn_05                 cn_01                   gfi \n#&gt;                46.539                54.452                 0.846 \n#&gt;                  agfi                  pgfi                   mfi \n#&gt;                 0.733                 0.489                 0.669 \n#&gt;                  ecvi \n#&gt;                 0.926\n\nL’analisi degli indici di bontà di adattamento rivela alcune preoccupazioni significative riguardo alla validità del nostro modello SEM. Il rapporto \\(\\chi^2 / df\\) emerge come eccessivamente elevato, segnalando una possibile mancanza di adattamento:\n\n449.141 / 26\n#&gt; [1] 17.27\n\nAnalogamente, i valori di CFI e TLI sono inferiori al livello desiderato, suggerendo che il modello non rappresenta adeguatamente la struttura dei dati. In aggiunta, gli indici RMSEA e SRMR superano le soglie accettabili, indicando ulteriormente un’inadeguata aderenza del modello ai dati.\nDi fronte a questi risultati, è imprudente accettare la conclusione precedentemente formulata secondo cui l’autocompassione agisce come un fattore protettivo contro il disagio emotivo. Questa interpretazione, benché teoricamente fondata, non trova un solido supporto empirico nel contesto del modello attuale.\nIn questa situazione, un percorso costruttivo potrebbe essere quello di rivedere e potenzialmente modificare il modello. L’obiettivo sarebbe quello di esplorare alternative che potrebbero risultare in un migliore adattamento ai dati, mantenendo al contempo l’adeguatezza teorica. Ciò potrebbe includere la revisione delle assunzioni del modello, la riconsiderazione delle variabili incluse o la ristrutturazione delle relazioni ipotizzate tra di esse. Solo attraverso un modello che dimostra una bontà di adattamento adeguata possiamo affermare con maggiore sicurezza che i dati empirici sostengono l’ipotesi dell’effetto protettivo dell’autocompassione sul disagio emotivo.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#potere-statistico-e-precisione",
    "href": "chapters/sem/03_gof.html#potere-statistico-e-precisione",
    "title": "50  Test del Modello e Indicizzazione",
    "section": "\n50.14 Potere Statistico e Precisione",
    "text": "50.14 Potere Statistico e Precisione\nNell’ambito dei modelli di Structural Equation Modeling (SEM), l’analisi della potenza statistica è fondamentale per garantire l’affidabilità e la validità dei risultati. Esistono due approcci principali per quest’analisi: la potenza a priori (prospettica) e la potenza retrospettiva (post hoc, osservata).\n\nPotenza a priori (Prospettica): Questa analisi viene effettuata prima della raccolta dei dati e mira a stimare la probabilità che uno studio identifichi un effetto significativo, se presente nella popolazione. È cruciale nella pianificazione della ricerca per determinare la dimensione del campione necessaria, aumentando così l’efficienza dello studio e prevenendo l’uso di campioni eccessivamente grandi o inadeguati. In SEM, la potenza a priori si stima specificando nel software le caratteristiche del modello di popolazione, ipotesi nulle e alternative, il livello di significatività statistica e la dimensione campionaria prevista.\nPotenza Retrospettiva (Post Hoc, Osservata): A differenza dell’analisi a priori, questa viene condotta dopo la raccolta dei dati. Le statistiche campionarie vengono trattate come parametri reali della popolazione, ma questa pratica presenta limitazioni significative. Le stime possono essere distorte, e una maggiore potenza osservata non implica necessariamente una forte evidenza a favore delle ipotesi nulle non rifiutate. Inoltre, essendo una misura post hoc, non aiuta nella progettazione proattiva della ricerca.\n\nPer l’analisi della potenza in SEM, sono stati sviluppati diversi metodi, tra cui:\n\nIl metodo Satorra–Saris stima la potenza del test del rapporto di verosimiglianza per un singolo parametro.\nIl metodo MacCallum–RMSEA si basa sulla RMSEA di popolazione e sulle distribuzioni chi-quadrato non centrali.\nIl metodo di simulazione Monte Carlo è un’alternativa moderna e flessibile che non presuppone né risultati continui né stima ML predefinita.\n\nCon l’avanzamento degli strumenti informatici, l’analisi della potenza statistica in SEM è diventata più accessibile:\nSoftware SEM con Simulazione Monte Carlo: Software come Mplus e LISREL includono capacità di simulazione Monte Carlo, permettendo di generare dati campionari basati su ipotesi del modello e di valutare la frequenza con cui i risultati significativi vengono ottenuti.\nMetodo Kelley–Lai Precision: Calcola la dimensione campionaria minima necessaria per stimare parametri come l’indice RMSEA entro un margine di errore specificato.\nNel contesto di R, le funzioni semTools::findRMSEApower e semTools::findRMSEAsamplesize del pacchetto semTools facilitano queste analisi:\n\nsemTools::findRMSEApower: Determina la potenza di un test SEM data una dimensione specifica del campione, basandosi sull’RMSEA e altri parametri del test.\nsemTools::findRMSEAsamplesize: Calcola la dimensione del campione necessaria per raggiungere una specifica potenza statistica in un test SEM, considerando l’RMSEA e altri criteri come il livello di significatività e la potenza desiderata.\n\nQuesti strumenti sono importanti per ottimizzare la progettazione della ricerca SEM, garantendo campioni adeguati e potenza statistica sufficiente per rilevare gli effetti di interesse.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#riflessioni-conclusive",
    "href": "chapters/sem/03_gof.html#riflessioni-conclusive",
    "title": "50  Test del Modello e Indicizzazione",
    "section": "\n50.15 Riflessioni Conclusive",
    "text": "50.15 Riflessioni Conclusive\nNella letteratura SEM, sono state avanzate critiche significative all’uso di indici come RMSEA, CFI e TLI e ai loro valori di cutoff convenzionali (si veda, ad esempio, Barrett, 2007). Nonostante queste critiche, tali indici continuano a essere ampiamente utilizzati nella ricerca SEM, in assenza di alternative più accettate e praticabili. Come sottolineato da Xia & Yang (2019), l’attuale prassi considera valori più elevati di RMSEA e valori più bassi di CFI e TLI come indicativi di un peggior adattamento del modello. Questo ha portato molti ricercatori a modificare i loro modelli per ottimizzare tali indici, spesso spingendoli a concentrarsi esclusivamente su questi criteri.\nLa dipendenza eccessiva da RMSEA, CFI e TLI ha condotto a una situazione in cui gli indici di adattamento vengono utilizzati come unico criterio per accettare o rifiutare un modello ipotizzato. Ad esempio, se un modello raggiunge soglie considerate “pubblicabili” (ad es., RMSEA &lt; .06), viene spesso accettato senza ulteriori miglioramenti. Tuttavia, questa pratica è problematica: affermazioni come “poiché i valori di RMSEA, CFI e TLI indicano un buon adattamento, questo modello è stato scelto come modello finale” sono insufficienti e riduttive.\nIl raggiungimento di soglie desiderate per questi indici dovrebbe rappresentare solo uno dei fattori da considerare nel processo di selezione del modello. È essenziale che i ricercatori:\n\nValutino altre opzioni di miglioramento del modello: Analizzando se esistano modifiche che potrebbero migliorare l’adattamento senza compromettere la validità teorica o la parsimonia.\nGiustifichino le scelte adottate o scartate: Spiegando chiaramente perché determinate opzioni di miglioramento non sono state applicate e quali sono le implicazioni di tali decisioni.\nConsiderino le conseguenze scientifiche e pratiche: Valutando l’impatto delle scelte di modellazione sulle conclusioni scientifiche e, quando pertinente, sulle applicazioni cliniche.\n\nIn sintesi, affidarsi esclusivamente a soglie arbitrarie per RMSEA, CFI e TLI non è sufficiente per determinare la qualità di un modello. Un approccio più integrato e critico, che tenga conto di considerazioni teoriche, pratiche e metodologiche, è necessario per garantire che i modelli scelti siano solidi e utili per rispondere alle domande di ricerca.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/sem/03_gof.html#informazioni-sullambiente-di-sviluppo",
    "title": "50  Test del Modello e Indicizzazione",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.2\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] effectsize_1.0.0   rsvg_2.6.1         DiagrammeRsvg_0.1 \n#&gt;  [4] lme4_1.1-36        Matrix_1.7-3       mvnormalTest_1.0.0\n#&gt;  [7] lavaanPlot_0.8.1   lavaanExtra_0.2.1  ggokabeito_0.1.0  \n#&gt; [10] see_0.11.0         MASS_7.3-65        viridis_0.6.5     \n#&gt; [13] viridisLite_0.4.2  ggpubr_0.6.0       ggExtra_0.10.1    \n#&gt; [16] gridExtra_2.3      patchwork_1.3.0    bayesplot_1.11.1  \n#&gt; [19] semTools_0.5-6     semPlot_1.1.6      lavaan_0.6-19     \n#&gt; [22] psych_2.4.12       scales_1.3.0       markdown_1.13     \n#&gt; [25] knitr_1.50         lubridate_1.9.4    forcats_1.0.0     \n#&gt; [28] stringr_1.5.1      dplyr_1.1.4        purrr_1.0.4       \n#&gt; [31] readr_2.1.5        tidyr_1.3.1        tibble_3.2.1      \n#&gt; [34] ggplot2_3.5.1      tidyverse_2.0.0    here_1.0.1        \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] splines_4.4.2       later_1.4.1         datawizard_1.0.1   \n#&gt;   [4] XML_3.99-0.18       rpart_4.1.24        lifecycle_1.0.4    \n#&gt;   [7] Rdpack_2.6.3        rstatix_0.7.2       rprojroot_2.0.4    \n#&gt;  [10] lattice_0.22-6      insight_1.1.0       rockchalk_1.8.157  \n#&gt;  [13] backports_1.5.0     magrittr_2.0.3      openxlsx_4.2.8     \n#&gt;  [16] Hmisc_5.2-3         rmarkdown_2.29      httpuv_1.6.15      \n#&gt;  [19] qgraph_1.9.8        zip_2.3.2           pbapply_1.7-2      \n#&gt;  [22] minqa_1.2.8         RColorBrewer_1.1-3  ADGofTest_0.3      \n#&gt;  [25] multcomp_1.4-28     abind_1.4-8         quadprog_1.5-8     \n#&gt;  [28] pspline_1.0-21      nnet_7.3-20         TH.data_1.1-3      \n#&gt;  [31] sandwich_3.1-1      moments_0.14.1      nortest_1.0-4      \n#&gt;  [34] arm_1.14-4          codetools_0.2-20    tidyselect_1.2.1   \n#&gt;  [37] farver_2.1.2        stats4_4.4.2        base64enc_0.1-3    \n#&gt;  [40] jsonlite_1.9.1      Formula_1.2-5       survival_3.8-3     \n#&gt;  [43] emmeans_1.10.7      tools_4.4.2         Rcpp_1.0.14        \n#&gt;  [46] glue_1.8.0          mnormt_2.1.1        xfun_0.51          \n#&gt;  [49] withr_3.0.2         numDeriv_2016.8-1.1 fastmap_1.2.0      \n#&gt;  [52] boot_1.3-31         digest_0.6.37       mi_1.1             \n#&gt;  [55] timechange_0.3.0    R6_2.6.1            mime_0.13          \n#&gt;  [58] estimability_1.5.1  colorspace_2.1-1    gtools_3.9.5       \n#&gt;  [61] jpeg_0.1-10         copula_1.1-6        DiagrammeR_1.0.11  \n#&gt;  [64] generics_0.1.3      data.table_1.17.0   corpcor_1.6.10     \n#&gt;  [67] htmlwidgets_1.6.4   parameters_0.24.2   pkgconfig_2.0.3    \n#&gt;  [70] sem_3.1-16          gtable_0.3.6        pcaPP_2.0-5        \n#&gt;  [73] htmltools_0.5.8.1   carData_3.0-5       png_0.1-8          \n#&gt;  [76] reformulas_0.4.0    rstudioapi_0.17.1   tzdb_0.5.0         \n#&gt;  [79] reshape2_1.4.4      coda_0.19-4.1       visNetwork_2.1.2   \n#&gt;  [82] checkmate_2.3.2     nlme_3.1-167        curl_6.2.1         \n#&gt;  [85] nloptr_2.2.1        zoo_1.8-13          parallel_4.4.2     \n#&gt;  [88] miniUI_0.1.1.1      foreign_0.8-88      pillar_1.10.1      \n#&gt;  [91] grid_4.4.2          vctrs_0.6.5         promises_1.3.2     \n#&gt;  [94] car_3.1-3           OpenMx_2.21.13      xtable_1.8-4       \n#&gt;  [97] cluster_2.1.8.1     htmlTable_2.4.3     evaluate_1.0.3     \n#&gt; [100] pbivnorm_0.6.0      mvtnorm_1.3-3       cli_3.6.4          \n#&gt; [103] kutils_1.73         compiler_4.4.2      rlang_1.1.5        \n#&gt; [106] ggsignif_0.6.4      fdrtool_1.2.18      plyr_1.8.9         \n#&gt; [109] stringi_1.8.4       munsell_0.5.1       gsl_2.1-8          \n#&gt; [112] lisrelToR_0.3       bayestestR_0.15.2   pacman_0.5.1       \n#&gt; [115] V8_6.0.2            hms_1.1.3           stabledist_0.7-2   \n#&gt; [118] glasso_1.11         shiny_1.10.0        rbibutils_2.3      \n#&gt; [121] igraph_2.1.4        broom_1.0.7         RcppParallel_5.1.10\n\n\n\n\n\nBarrett, P. (2007). Structural equation modelling: Adjudging model fit. Personality and Individual Differences, 42(5), 815–824.\n\n\nHayduk, L. A. (2014). Shame for disrespecting evidence: The personal consequences of insufficient respect for structural equation model testing. BMC Medical Research Methodology, 14, 1–10.\n\n\nHu, L., & Bentler, P. M. (1998). Fit indices in covariance structure modeling: Sensitivity to underparameterized model misspecification. Psychological Methods, 3(4), 424--453.\n\n\nKline, R. B. (2023). Principles and practice of structural equation modeling. Guilford publications.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.\n\n\nXia, Y., & Yang, Y. (2019). RMSEA, CFI, and TLI in structural equation modeling with ordered categorical data: The story they tell depends on the estimation methods. Behavior Research Methods, 51(1), 409–428.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html",
    "href": "chapters/sem/04_mod_comp.html",
    "title": "43  Confronto tra modelli",
    "section": "",
    "text": "43.1 Introduzione\nI ricercatori spesso confrontano modelli alternativi di equazioni strutturali che includono le stesse variabili e sono adattati agli stessi dati. Il contesto più frequente si verifica quando un singolo modello iniziale viene testato attraverso una serie di passaggi. Ad ogni passaggio, il modello iniziale viene ridefinito aggiungendo uno o più parametri liberi, il che generalmente migliora l’adattamento, oppure eliminando (fissando a zero) uno o più parametri liberi, il che generalmente peggiora l’adattamento. Una coppia di modelli alternativi così specificata viene definita “modelli nidificati”, poiché il modello più semplice dei due, o modello vincolato, è un sottoinsieme proprio del modello più complesso, o modello non vincolato. Un contesto diverso si verifica quando ci sono due o più modelli iniziali tali che (1) ogni modello si basa su una teoria diversa e (2) i modelli alternativi non sono nidificati nella loro relazione l’uno con l’altro. In entrambi i contesti, la scelta tra modelli concorrenti dovrebbe essere guidata tanto da basi concettuali quanto da considerazioni statistiche.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html#confrontare-modelli-nel-sem",
    "href": "chapters/sem/04_mod_comp.html#confrontare-modelli-nel-sem",
    "title": "43  Confronto tra modelli",
    "section": "\n43.2 Confrontare Modelli nel SEM",
    "text": "43.2 Confrontare Modelli nel SEM\nNel contesto dei Modelli di Equazioni Strutturali (SEM), un aspetto critico è il confronto tra diversi modelli per determinare quale sia il più adeguato. Questo confronto si presenta frequentemente nella forma di analisi di modelli nidificati. In tale contesto, si confronta un modello considerato “pieno” o “meno restrittivo” con un altro modello che è “ridotto” o “più restrittivo”.\nIl modello pieno include un insieme più ampio di parametri e ipotesi, offrendo una rappresentazione più complessa delle relazioni tra le variabili. Al contrario, il modello ridotto è una versione più semplificata, con meno parametri e ipotesi, risultando in una struttura più contenuta e potenzialmente più parsimoniosa.\nQuesto tipo di confronto è cruciale per valutare l’adeguatezza dei modelli SEM, permettendo ai ricercatori di decidere se la complessità aggiuntiva del modello pieno sia giustificata rispetto al modello ridotto in termini di adattamento ai dati e coerenza con la teoria sottostante.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html#analisi-dei-modelli-nidificati",
    "href": "chapters/sem/04_mod_comp.html#analisi-dei-modelli-nidificati",
    "title": "43  Confronto tra modelli",
    "section": "\n43.3 Analisi dei Modelli Nidificati",
    "text": "43.3 Analisi dei Modelli Nidificati\nNell’ambito dei Modelli di Equazioni Strutturali (SEM), i modelli nidificati occupano un ruolo centrale. Due modelli sono definiti come nidificati quando soddisfano specifici criteri gerarchici, delineati come segue:\n\n\nFormazione del Modello Vincolato:\n\nSi crea un modello vincolato applicando una o più restrizioni a un modello non vincolato esistente. Questo processo aumenta i gradi di libertà del modello vincolato (C) rispetto a quelli del modello non vincolato (U), risultando in \\(\\text{df}_C &gt; \\text{df}_U\\).\n\n\n\nDifferenze nei Gradi di Libertà:\n\nLa differenza \\(\\text{df}_C - \\text{df}_U\\) rappresenta il numero di restrizioni imposte al modello non vincolato per creare il modello vincolato, che equivale alla variazione nel numero di parametri liberi tra i due modelli.\n\n\n\nParametri Liberi e Vincolati:\n\nI parametri liberi nel modello vincolato costituiscono un sottoinsieme di quelli presenti nel modello non vincolato. Allo stesso modo, i parametri fissi nel modello non vincolato formano un sottoinsieme di quelli nel modello vincolato.\n\n\n\nConfronto dei Valori di Chi-Quadro:\n\nTra i due modelli, il valore di \\(\\chi^2\\) è minore o uguale nel modello non vincolato rispetto a quello nel modello vincolato, ovvero \\(\\chi^2_U \\leq \\chi^2_C\\). Questo implica che le distribuzioni di probabilità possibili nel modello vincolato sono comprese anche nel modello non vincolato, che può tuttavia suggerire ulteriori distribuzioni non coerenti con il modello vincolato.\n\n\n\nQuesto tipo di relazione gerarchica, conosciuta come annidamento dei parametri, permette di valutare l’impatto di specifiche restrizioni o aggiunte di parametri. Ad esempio, un parametro libero in un modello non vincolato può essere fissato a zero, eliminando di conseguenza l’effetto corrispondente nel modello vincolato, oppure può essere sottoposto a un vincolo specificato dal ricercatore, riducendo così il numero di parametri liberi ma mantenendo l’effetto nel modello vincolato.\nConsideriamo, per esempio, un modello di percorso non vincolato U con effetti diretti: \\(X \\rightarrow Y_1 \\rightarrow Y_2\\) e \\(X \\rightarrow Y_2\\). Ridefinendo il percorso \\(X \\rightarrow Y_2 = 0\\), eliminiamo questa connessione dal modello U, generando così il modello vincolato C1, nidificato sotto U. Un’alternativa potrebbe essere imporre un vincolo di uguaglianza tra \\(X\\rightarrow Y_2\\) e \\(Y_1 \\rightarrow Y_2\\), indicando che gli effetti diretti non standardizzati di \\(X\\) e \\(Y_1\\) su \\(Y_2\\) sono identici. Questo produce un modello vincolato, C2, con un parametro libero in meno rispetto a U ma che include tutti i percorsi di U.\nNelle prossime sezioni, esamineremo come testare le ipotesi inerenti a questi modelli nidificati e come valutare la loro adeguatezza nel contesto SEM.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html#strategie-di-costruzione-e-potatura-nei-modelli-sem",
    "href": "chapters/sem/04_mod_comp.html#strategie-di-costruzione-e-potatura-nei-modelli-sem",
    "title": "43  Confronto tra modelli",
    "section": "\n43.4 Strategie di Costruzione e Potatura nei Modelli SEM",
    "text": "43.4 Strategie di Costruzione e Potatura nei Modelli SEM\n\n43.4.1 Costruzione Progressiva del Modello\nLa costruzione di modelli SEM inizia tipicamente con un modello iniziale semplice e vincolato, che riflette le ipotesi fondamentali basate su teorie sostanziali. Questo approccio, detto anche ricerca in avanti, implica l’aggiunta progressiva di parametri liberi che rappresentano ipotesi precedentemente escluse, in base alla loro importanza. Sebbene questo processo possa generalmente migliorare l’adattamento del modello (riduzione di chiML), un adattamento migliore non è necessariamente indicativo di una maggiore correttezza del modello. La costruzione del modello può teoricamente proseguire fino a quando non si raggiunge un modello perfettamente adatto ai dati (dfM = 0), ma è essenziale valutare la validità teorica e la parsimonia del modello in ogni passaggio.\n\n43.4.2 Potatura Retrograda del Modello\nIn contrasto, la potatura del modello, o ricerca all’indietro, inizia con un modello più complesso e non vincolato. In questa fase, il ricercatore semplifica il modello eliminando parametri liberi (fissandoli a zero) o imponendo vincoli di stima. Questo processo richiede di dare priorità alle ipotesi in ordine inverso di importanza. Il modello iniziale dovrebbe essere congruente con i dati, altrimenti non ha senso restringerlo ulteriormente. Tipicamente, come si procede con la potatura, l’adattamento complessivo del modello ai dati tende a peggiorare (aumento di chiML). Il criterio per arrestare la potatura si basa sull’adattamento del modello: si ferma quando ulteriori restrizioni peggiorerebbero significativamente l’adattamento ai dati.\n\n43.4.3 Obiettivi e Considerazioni\nL’obiettivo sia nella costruzione che nella potatura di modelli è identificare un modello con una struttura di covarianza (e, se presente, anche di media) correttamente specificata e teoricamente giustificata. Idealmente, entrambi gli approcci dovrebbero convergere verso lo stesso modello ottimale, benché ciò non sia garantito. È importante evitare il rischio di formulare ipotesi post hoc (HARKing), presentando modelli scoperti in modo esplorativo come se fossero stati ipotizzati a priori. Una soluzione a questo problema è la preregistrazione del piano di analisi.\n\n43.4.4 Punti di Forza Relativi\n\n\nCostruzione del Modello: Partire da un modello più semplice può essere vantaggioso, soprattutto per i neofiti del SEM, poiché facilita l’identificazione statistica e riduce il rischio di errori nella specificazione del modello.\n\nPotatura del Modello: Questo approccio può essere particolarmente efficace per i modelli di misurazione, dove le variabili osservate sono usate come indicatori di un numero limitato di fattori comuni. Un modello di misurazione correttamente specificato inizialmente può rendere la potatura più efficace rispetto alla costruzione.\n\nIn entrambi i casi, è cruciale basare le decisioni su solide basi teoriche oltre che su considerazioni statistiche, per assicurare che il modello finale sia non solo adatto ai dati ma anche coerente con il quadro teorico sottostante.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html#strategie-di-ridefinizione-dei-modelli-sem-approcci-teorici-ed-empirici",
    "href": "chapters/sem/04_mod_comp.html#strategie-di-ridefinizione-dei-modelli-sem-approcci-teorici-ed-empirici",
    "title": "43  Confronto tra modelli",
    "section": "\n43.5 Strategie di Ridefinizione dei Modelli SEM: Approcci Teorici ed Empirici",
    "text": "43.5 Strategie di Ridefinizione dei Modelli SEM: Approcci Teorici ed Empirici\n\n43.5.1 Approccio Teorico nella Ridefinizione dei Modelli\nNel processo di costruzione o potatura dei Modelli di Equazioni Strutturali (SEM), l’approccio teorico gioca un ruolo fondamentale. Qui, le modifiche al modello sono guidate da ipotesi teoriche predefinite e specifiche. Ad esempio, considerando un modello di percorso non vincolato U con le relazioni \\(X \\rightarrow Y_1 \\rightarrow Y_2\\) e \\(X \\rightarrow Y_2\\), un ricercatore potrebbe ipotizzare che l’effetto di X su Y2 sia esclusivamente indiretto attraverso Y1. Questa ipotesi può essere testata vincolando il coefficiente di \\(X \\rightarrow Y_2\\) a zero. Se l’adattamento del modello così modificato non è significativamente inferiore rispetto al modello non vincolato, l’ipotesi di un effetto indiretto viene supportata, a patto che la direzionalità delle relazioni sia corretta.\nQuesto approccio enfatizza che le modifiche al modello dovrebbero essere effettuate sulla base di solide basi teoriche e concettuali, piuttosto che su criteri puramente statistici, come evidenziato da Jöreskog (1969): “La decisione di smettere di aggiungere parametri non può basarsi solo su una base statistica; ciò dipende in gran parte dall’interpretazione dei dati da parte del ricercatore, basata su considerazioni teoriche e concettuali sostanziali.”\n\n43.5.2 Approccio Empirico nella Ridefinizione dei Modelli\nAl contrario, l’approccio empirico nella costruzione o potatura dei modelli SEM si basa su criteri statistici. In questo scenario, i parametri liberi vengono aggiunti o eliminati a seconda della loro significatività statistica o di altri indicatori empirici. Per esempio, se i percorsi sono eliminati solo perché i loro coefficienti non sono statisticamente significativi, la ridefinizione del modello è guidata da considerazioni puramente empiriche. Questo approccio è analogo alla tecnica di eliminazione all’indietro nella regressione multipla, dove il software sceglie quali predittori rimuovere in base a criteri di significatività statistica.\n\n43.5.3 Implicazioni per l’Interpretazione dei Modelli\nLa scelta tra un approccio teorico o empirico nella ridefinizione dei modelli SEM ha implicazioni significative per come interpretiamo i risultati. Un modello modificato in base a criteri teorici forti offre una maggiore fiducia nella validità delle sue conclusioni, mentre un modello costruito o potato basandosi principalmente su criteri empirici può essere soggetto a errori di Tipo I o II e può non essere replicabile in campioni diversi.\nÈ fondamentale che i ricercatori si avvicinino alla costruzione e alla potatura dei modelli SEM con un equilibrio tra intuizioni teoriche e risultati empirici, per assicurare che i modelli finali siano non solo statisticamente validi ma anche teoricamente giustificati e interpretativamente significativi.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html#test-della-differenza-chi-quadro-nel-sem",
    "href": "chapters/sem/04_mod_comp.html#test-della-differenza-chi-quadro-nel-sem",
    "title": "43  Confronto tra modelli",
    "section": "\n43.6 Test della Differenza Chi-Quadro nel SEM",
    "text": "43.6 Test della Differenza Chi-Quadro nel SEM\n\n43.6.1 Principi Fondamentali\nIl test della differenza Chi-Quadro (chiD) è una tecnica statistica essenziale nel contesto dei Modelli di Equazioni Strutturali (SEM) per valutare l’effetto delle modifiche ai parametri sui modelli. Questo test viene utilizzato sia nella potatura (restrizione dei parametri) che nella costruzione (aggiunta di parametri) dei modelli. Il valore chiD rappresenta la differenza tra i valori di chi-quadro (chi-quadro massima verosimiglianza, chiML) di due modelli nidificati. I gradi di libertà associati, dfD, sono determinati dalla differenza nei gradi di libertà dei due modelli (\\(\\text{df}_C - \\text{df}_U\\)).\n\n43.6.2 Applicazione del Test\nPer applicare il test della differenza chi-quadro, si seguono questi passaggi:\n\n\nDefinizione dei Modelli: Identificare il modello pieno (con tutti i parametri ritenuti rilevanti) e il modello ridotto (una versione semplificata del modello pieno con alcune restrizioni).\n\nStima dei Modelli: Utilizzare metodi di massima verosimiglianza per stimare entrambi i modelli.\n\nCalcolo del Rapporto di Verosimiglianze: Calcolare la differenza tra i logaritmi delle funzioni di verosimiglianza dei due modelli (\\(D = -2(\\ln(L_r) - \\ln(L_f))\\)).\n\nTest Statistico: Utilizzare la distribuzione chi-quadrato per determinare il p-value. Un p-value basso indica che il modello ridotto non si adatta ai dati così come il modello pieno.\n\n43.6.3 Interpretazione dei Risultati\nUn valore piccolo di chiD suggerisce che non c’è una differenza significativa nell’adattamento tra i due modelli, mentre un valore grande indica una differenza significativa. In termini di potatura, un grande chiD implica che il modello è stato eccessivamente vincolato. Nella costruzione, un grande chiD supporta la conservazione del parametro libero aggiunto. Tuttavia, prima di trarre conclusioni definitive, è cruciale considerare l’adattamento complessivo del modello, sia a livello globale che locale.\n\n43.6.4 Considerazioni nella Stima ML Robusta\nNel caso di stima ML robusta, la differenza tra i chi-quadri scalati non può essere interpretata come un test dell’ipotesi di adattamento uguale a causa delle distribuzioni non centrali in condizioni di non normalità. Sono disponibili metodi specifici per calcolare una statistica di differenza chi-quadro scalata che segue approssimativamente le distribuzioni chi-quadro.\n\n43.6.5 Implicazioni Teoriche ed Empiriche\nLa decisione di modificare un modello basandosi su un approccio teorico o empirico ha implicazioni significative. Ad esempio, l’eliminazione di percorsi non significativi su base puramente statistica può portare a conclusioni errate se non supportate da una solida base teorica. Inoltre, è importante essere consapevoli dei rischi associati alla capitalizzazione sul caso, come errori di Tipo I e II, e del pericolo di seguire “sentieri che si biforcano” nelle decisioni analitiche, che possono rendere i risultati specifici del campione e difficili da replicare. Per mitigare questi rischi, è consigliabile basare le modifiche del modello più su orientamenti teorici che sui risultati dei test di significatività.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html#test-della-differenza-chi-quadro-scalato",
    "href": "chapters/sem/04_mod_comp.html#test-della-differenza-chi-quadro-scalato",
    "title": "43  Confronto tra modelli",
    "section": "\n43.7 Test della Differenza Chi-Quadro Scalato",
    "text": "43.7 Test della Differenza Chi-Quadro Scalato\nIl metodo di Satorra e Bentler (2001) permette di calcolare manualmente una statistica di differenza chi-quadro scalata quando si confrontano due modelli gerarchici nella stima ML robusta. Si presume che il modello 1 sia più vincolato rispetto al modello 2 (cioè, dfM1 &gt; dfM2), i chi-quadri non scalati siano chiML e i chi-quadri scalati siano chiSB. La statistica di test Satorra-Bentler è definita come segue:\n\nCalcolare la Statistica di Differenza Chi-Quadro non Scalata e i suoi Gradi di Libertà:\n\n\nchiD = chiML1 - chiML2 e dfD = dfM1 - dfM2.\n\n\nRecuperare il Fattore di Correzione di Scala, c, per Ogni Modello:\n\n\nc1 = chiML1 / chiSB1 e c2 = chiML2 / chiSB2.\n\n\nCalcolare la Statistica di Differenza Chi-Quadro Scalata, chiSD:\n\n\nchiSD = chiD / ((c1 / dfM1 - c2 / dfM2) / dfD).\nLa probabilità per chiSD (dfD) in una distribuzione chi-quadro centrale rappresenta il p-value per il test di differenza chi-quadro scalato.\n\nIn campioni piccoli o quando il modello più vincolato è molto errato, il denominatore di chiSD può essere &lt; 0, invalidando il test. Questo test è implementato nella funzione lavTestLRT() in lavaan (Rosseel et al., 2023).\n\n43.7.1 Esempio\nConsideriamo nuovamente i dati discussi da Brown (2015) relativi al modello di misurazione per la depressione maggiore così come è definita nel DSM-IV. Ignoriamo qui le differenze di genere. Leggiamo i dati in \\(\\mathsf{R}\\):\n\nd_mdd &lt;- readRDS(here::here(\"data\", \"mdd_sex.RDS\"))\n\nConsideriamo il seguente modello:\n\nmodel_mdd &lt;- \"\n  MDD =~ mdd1 + mdd2 + mdd3 + mdd4 + mdd5 + mdd6 + mdd7 + mdd8 + mdd9\n\"\n\nAdattiamo il modello ai dati.\n\nfit_mdd &lt;- cfa(\n    model_mdd,\n    data = d_mdd\n)\n\n\nsemPaths(fit_mdd,\n    whatLabels = \"std\",\n    sizeMan = 8,\n    edge.label.cex = 0.7,\n    style = \"mx\",\n    nCharNodes = 0, nCharEdges = 0\n)\n\n\n\n\n\n\n\n\nfitMeasures(fit_mdd) \n#&gt;                  npar                  fmin                 chisq \n#&gt;                18.000                 0.074               110.272 \n#&gt;                    df                pvalue        baseline.chisq \n#&gt;                27.000                 0.000              1297.618 \n#&gt;           baseline.df       baseline.pvalue                   cfi \n#&gt;                36.000                 0.000                 0.934 \n#&gt;                   tli                  nnfi                   rfi \n#&gt;                 0.912                 0.912                 0.887 \n#&gt;                   nfi                  pnfi                   ifi \n#&gt;                 0.915                 0.686                 0.934 \n#&gt;                   rni                  logl     unrestricted.logl \n#&gt;                 0.934            -13747.192            -13692.056 \n#&gt;                   aic                   bic                ntotal \n#&gt;             27530.385             27613.546               750.000 \n#&gt;                  bic2                 rmsea        rmsea.ci.lower \n#&gt;             27556.389                 0.064                 0.052 \n#&gt;        rmsea.ci.upper        rmsea.ci.level          rmsea.pvalue \n#&gt;                 0.077                 0.900                 0.029 \n#&gt;        rmsea.close.h0 rmsea.notclose.pvalue     rmsea.notclose.h0 \n#&gt;                 0.050                 0.019                 0.080 \n#&gt;                   rmr            rmr_nomean                  srmr \n#&gt;                 0.191                 0.191                 0.044 \n#&gt;          srmr_bentler   srmr_bentler_nomean                  crmr \n#&gt;                 0.044                 0.044                 0.050 \n#&gt;           crmr_nomean            srmr_mplus     srmr_mplus_nomean \n#&gt;                 0.050                 0.044                 0.044 \n#&gt;                 cn_05                 cn_01                   gfi \n#&gt;               273.824               320.411                 0.964 \n#&gt;                  agfi                  pgfi                   mfi \n#&gt;                 0.940                 0.578                 0.946 \n#&gt;                  ecvi \n#&gt;                 0.195\n\nGli indici Comparative Fit Index (CFI) = 0.934 e Tucker-Lewis Index (TLI) = 0.912 sono superiori a 0.9, dunque sono almeno sufficienti per gli standard correnti. L’indice RMSEA = 0.064 è appena superiore alla soglia di 0.06. L’indice SRMR = 0.044 è inferiore alla soglia 0.05. Dunque, complessivamente, il modello sembra adeguato.\nAdattiamo ora il modello con la modifica proposta da {cite:t}brown2015confirmatory, ovvero\n\nmodel2_mdd &lt;- \"\n  MDD =~ mdd1 + mdd2 + mdd3 + mdd4 + mdd5 + mdd6 + mdd7 + mdd8 +  mdd9\n  mdd1 ~~ mdd2\n\"\n\nfit2_mdd &lt;- cfa(\n    model2_mdd,\n    data = d_mdd\n)\n\nEseguiamo il test del rapporto di verosimiglianze:\n\nlavTestLRT(fit_mdd, fit2_mdd)\n#&gt; \n#&gt; Chi-Squared Difference Test\n#&gt; \n#&gt;          Df   AIC   BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\n#&gt; fit2_mdd 26 27490 27577  67.6                                    \n#&gt; fit_mdd  27 27530 27614 110.3       42.7 0.236       1    6.3e-11\n\nIl test indica che il modello alternativo si adatta meglio ai dati del modello originale.\nEsaminiamo gli indici di bontà di adattamento.\n\neffectsize::interpret(fit2_mdd)\n#&gt;     Name   Value Threshold Interpretation\n#&gt; 1    GFI 0.97807      0.95   satisfactory\n#&gt; 2   AGFI 0.96205      0.90   satisfactory\n#&gt; 3    NFI 0.94794      0.90   satisfactory\n#&gt; 4   NNFI 0.95439      0.90   satisfactory\n#&gt; 5    CFI 0.96706      0.90   satisfactory\n#&gt; 6  RMSEA 0.04617      0.05   satisfactory\n#&gt; 7   SRMR 0.03675      0.08   satisfactory\n#&gt; 8    RFI 0.92791      0.90   satisfactory\n#&gt; 9   PNFI 0.68462      0.50   satisfactory\n#&gt; 10   IFI 0.96732      0.90   satisfactory\n\nGli indici Comparative Fit Index (CFI) = 0.967 e Tucker-Lewis Index (TLI) = 0.954 sono superiori a 0.95. L’indice RMSEA = 0.046. L’indice SRMR = 0.037.\nIl “costo” che si paga per questo miglioramento dell’adattamento è che indici di adattamento così buoni, probabilmente, non si replicheranno in un altro campione di dati, a meno che venga introdotto un qualche altro aggiustamento che, sicuramente, sarà diverso da quello usato nel campione corrente. Personalmente, non avrei introdotto il “miglioramento” proposto da {cite:t}brown2015confirmatory in quanto, anche senza un tale aggiustamento post-hoc, il modello produce un adattamento accettabile.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html#confronto-di-modelli-non-annidati",
    "href": "chapters/sem/04_mod_comp.html#confronto-di-modelli-non-annidati",
    "title": "43  Confronto tra modelli",
    "section": "\n43.8 Confronto di Modelli Non Annidati",
    "text": "43.8 Confronto di Modelli Non Annidati\nProseguendo, analizzeremo l’adattamento di due modelli distinti, entrambi costituiti dalle stesse variabili e applicati agli stessi dati. Tuttavia, a differenza di quanto precedentemente delineato, questi modelli non sono collegati gerarchicamente, ma si configurano come modelli non annidati. Questa situazione si presenta comunemente quando i ricercatori mettono a confronto modelli basati su teorie divergenti. È possibile effettuare un confronto informale dei valori del chi-quadrato derivanti da modelli non annidati, ma la loro differenza non va interpretata come una statistica di test valida. In altre parole, i test di differenza del chi-quadrato, sia in forma scalata che non, non sono appropriati in questo contesto. La ragione risiede nel fatto che la differenza tra le statistiche di test di modelli non annidati non segue una distribuzione chi-quadrato centrale. Sebbene siano stati compiuti sforzi per elaborare test di significatività adatti al confronto di modelli non annidati, questi metodi non hanno trovato un ampio utilizzo e spesso portano a complicazioni interpretative (Levy & Hancock, 2007).\nUna soluzione più pragmatica è rappresentata dalla famiglia degli indici di adattamento predittivo, conosciuti anche come criteri teorico-informativi. Questi indici non sono test di significatività, poiché le loro distribuzioni di probabilità variano ampiamente a seconda del tipo di modello e dei dati considerati e, pertanto, rimangono generalmente ignote. Piuttosto, essi riflettono sia la qualità dell’adattamento del modello sia la sua complessità, bilanciando questi due aspetti. Ciò implica l’applicazione di una penalità per la complessità del modello, che consente di regolare l’adattamento in funzione del numero di parametri liberi. Per esempio, nel caso di due modelli non annidati che mostrano un adattamento simile agli stessi dati, verrà privilegiato il modello meno complesso, in quanto considerato più probabile nella generalizzazione su campioni replicati. In questo scenario, il valore del criterio informativo sarà inferiore per il modello più semplice, dato che una penalità maggiore per la complessità viene applicata all’adattamento del modello più complesso. Di conseguenza, il modello con il criterio informativo più basso è da preferire. In questo capitolo, dopo aver introdotto un problema di ricerca, esploreremo due indici di adattamento predittivo ampiamente usati (AIC e BIC).\n\n\n\n\n\nFigura 43.1: Modelli alternativi non annidati di percorso ricorsivo per l’adattamento dopo un intervento chirurgico cardiaco. (Figura tratta da Kline (2023).)\n\n\nNella figura sono presentati due modelli di percorso che descrivono il recupero dei pazienti dopo un intervento chirurgico cardiaco (Romney et al., 1992) – si veda Kline (2023), cap. 11. Il modello psicosomatico rappresenta le ipotesi che il morale del paziente trasmetta gli effetti della disfunzione neurologica e dello stato socioeconomico ridotto (SES) su sintomi della malattia e scarse relazioni sociali. Il modello medico rappresenta un diverso schema di relazioni causali tra le stesse variabili. In particolare, sia i sintomi della malattia sia la disfunzione neurologica sono specificati come variabili esogene con effetti diretti sul SES ridotto, basso morale e scarse relazioni. Tra queste tre variabili endogene, si ipotizza che il SES ridotto influenzi indirettamente le scarse relazioni attraverso il suo impatto precedente sul basso morale. Ci sono ulteriori effetti indiretti nel modello medico convenzionale dalle variabili esogene a quelle endogene. I due modelli nella figura non sono annidati, quindi il test della differenza del chi-quadro non può essere utilizzato per confrontarli direttamente. È dunque necessario seguire un altro approccio.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html#aic-e-bic",
    "href": "chapters/sem/04_mod_comp.html#aic-e-bic",
    "title": "43  Confronto tra modelli",
    "section": "\n43.9 AIC e BIC",
    "text": "43.9 AIC e BIC\nUno degli indici di adattamento predittivo più noti basato sulla stima di massima verosimiglianza (ML) è il Criterio di Informazione di Akaike (AIC), che prende il nome dallo statistico Hirotugu Akaike. La formula per l’AIC di Akaike (1974, p. 719) è:\n\\[ \\text{AIC} = -2 \\ln L_0 + 2q \\]\ndove \\(L_0\\) è la funzione di verosimiglianza massimizzata nella stima ML per il modello del ricercatore e \\(q\\) è il numero di parametri liberi del modello. Si noti che la penalità per la complessità nell’equazione precedente, \\(2q\\), diventa relativamente più piccola all’aumentare della dimensione del campione (Mulaik, 2009b).\nUn diverso indice teorico-informativo che tiene direttamente conto della dimensione del campione è il Criterio di Informazione Bayesiano (BIC) (Raftery, 1993; Schwarz, 1978). La formula è\n\\[ \\text{BIC} = -2 \\ln L_0 + q \\ln N \\]\nConfrontato con l’AIC, il BIC impone una penalità relativa maggiore per la complessità del modello.\nSupponiamo che il numero di parametri stimati liberamente sia \\(q = 10\\) e che \\(N = 300\\). La penalità AIC equivale a \\(2(10)\\), ovvero 20.000 (Equazione 11.4), ma la penalità BIC per lo stesso modello è \\(10 (\\ln 300)\\), ovvero 50.038, più del doppio rispetto all’AIC. I valori relativi delle penalità BIC aumentano più lentamente all’aumentare della dimensione del campione; in altre parole, la sua penalità è asintotica su campioni sempre più grandi (Mulaik, 2009b).\nIn sostanza, sia l’AIC che il BIC sono strumenti per bilanciare l’adattamento del modello con la sua complessità, ma differiscono nel modo in cui valutano e penalizzano questa complessità, soprattutto in relazione alla dimensione del campione.\nUtilizzando lo script fornito da Kline (2023), iniziamo a importare i dati in R:\n\n# input the correlations in lower diagnonal form\nromneyLower.cor &lt;- \"\n 1.00\n  .53 1.00\n  .15  .18 1.00\n  .52  .29 -.05 1.00\n  .30  .34  .23  .09 1.00 \"\n\n# name the variables and convert to full correlation matrix\nromney.cor &lt;- lavaan::getCov(romneyLower.cor, names = c(\n    \"morale\", \"symptoms\",\n    \"dysfunction\", \"relations\", \"ses\"\n))\n\nEsaminiamo le matrici di correlazioni e covarianze:\n\n# display the correlations\nromney.cor\n#&gt;             morale symptoms dysfunction relations  ses\n#&gt; morale        1.00     0.53        0.15      0.52 0.30\n#&gt; symptoms      0.53     1.00        0.18      0.29 0.34\n#&gt; dysfunction   0.15     0.18        1.00     -0.05 0.23\n#&gt; relations     0.52     0.29       -0.05      1.00 0.09\n#&gt; ses           0.30     0.34        0.23      0.09 1.00\n\n\n# add the standard deviations and convert to covariances\nromney.cov &lt;- lavaan::cor2cov(romney.cor, sds = c(\n    3.75, 17.00, 19.50,\n    3.50, 24.70\n))\n\n# display the covariances\nromney.cov\n#&gt;             morale symptoms dysfunction relations    ses\n#&gt; morale      14.062    33.79      10.969     6.825  27.79\n#&gt; symptoms    33.788   289.00      59.670    17.255 142.77\n#&gt; dysfunction 10.969    59.67     380.250    -3.413 110.78\n#&gt; relations    6.825    17.25      -3.413    12.250   7.78\n#&gt; ses         27.787   142.77     110.779     7.780 610.09\n\nSpecifichiamo il modello psicosomatico.\n\nsomatic.model &lt;- \"\n    # regressions\n    morale ~ ses + dysfunction\n    relations ~ morale\n    symptoms ~ morale\n    # without the zero constraint listed next,\n    # lavaan automatically specifies correlated\n    # disturbances for symptoms and relations,\n    # but their disturbances are independent in\n    # figure 11.1\n    symptoms ~~ 0*relations\n    # unanalyzed association between ses and dysfunction\n    # automatically specified \n\"\n\nSpecifichiamo il modello medico convenzionale.\n\nmedical.model &lt;- \"\n    # regressions\n    ses ~ symptoms + dysfunction\n    morale ~ symptoms + ses\n    relations ~ dysfunction + morale\n    # unanalyzed association between symptoms and dysfunction\n    # automatically specified \n\"\n\nAdattiamo ai dati il modello psicosomatico.\n\nsomatic &lt;- lavaan::sem(somatic.model,\n    sample.cov = romney.cov,\n    sample.nobs = 469, fixed.x = FALSE, sample.cov.rescale = FALSE\n)\n\n\nsemPaths(somatic,\n    whatLabels = \"std\",\n    sizeMan = 10,\n    edge.label.cex = 1.0,\n    style = \"mx\",\n    nCharNodes = 0, nCharEdges = 0\n)\n\n\n\n\n\n\n\n\nlavaan::summary(somatic, fit.measures = TRUE, rsquare = TRUE) \n#&gt; lavaan 0.6-19 ended normally after 1 iteration\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        10\n#&gt; \n#&gt;   Number of observations                           469\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                40.488\n#&gt;   Degrees of freedom                                 5\n#&gt;   P-value (Chi-square)                           0.000\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               390.816\n#&gt;   Degrees of freedom                                 9\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.907\n#&gt;   Tucker-Lewis Index (TLI)                       0.833\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -8572.844\n#&gt;   Loglikelihood unrestricted model (H1)      -8552.599\n#&gt;                                                       \n#&gt;   Akaike (AIC)                               17165.687\n#&gt;   Bayesian (BIC)                             17207.193\n#&gt;   Sample-size adjusted Bayesian (SABIC)      17175.455\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.123\n#&gt;   90 Percent confidence interval - lower         0.090\n#&gt;   90 Percent confidence interval - upper         0.159\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.000\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.982\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.065\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Regressions:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;   morale ~                                            \n#&gt;     ses               0.043    0.007    6.217    0.000\n#&gt;     dysfunction       0.016    0.009    1.897    0.058\n#&gt;   relations ~                                         \n#&gt;     morale            0.485    0.037   13.184    0.000\n#&gt;   symptoms ~                                          \n#&gt;     morale            2.403    0.178   13.535    0.000\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;  .relations ~~                                        \n#&gt;    .symptoms          0.000                           \n#&gt;   ses ~~                                              \n#&gt;     dysfunction     110.779   22.821    4.854    0.000\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;    .morale           12.699    0.829   15.313    0.000\n#&gt;    .relations         8.938    0.584   15.313    0.000\n#&gt;    .symptoms        207.820   13.571   15.313    0.000\n#&gt;     ses             610.090   39.840   15.313    0.000\n#&gt;     dysfunction     380.250   24.831   15.313    0.000\n#&gt; \n#&gt; R-Square:\n#&gt;                    Estimate\n#&gt;     morale            0.097\n#&gt;     relations         0.270\n#&gt;     symptoms          0.281\n\nOra approfondiamo l’analisi dei residui. Nel contesto di un modello SEM, i residui sono derivati dalle differenze tra la matrice di correlazioni (o covarianze) osservata e quella prevista dal modello. Queste differenze sono elaborate attraverso specifiche funzioni per generare i residui. Utilizzando il pacchetto lavaan in R, possiamo accedere a tre principali tipi di residui: standardized.mplus, normalized, e cor.bollen.\n\nResidui Standardizzati (Standardized.mplus): Questo tipo di residuo è una versione standardizzata dei residui. I residui standardizzati sono ottenuti calcolando la differenza tra i valori osservati e quelli previsti dal modello, e dividendo questa differenza per uno stimatore della deviazione standard del residuo. Questo processo trasforma i residui in una scala in cui hanno una varianza approssimativamente uguale. I residui standardizzati sono utili per identificare punti dati che il modello non riesce a spiegare bene. In lavaan, type = \"standardized.mplus\" si riferisce a una particolare forma di standardizzazione dei residui, simile a quella utilizzata nel software Mplus.\nResidui Normalizzati (Normalized): I residui normalizzati sono un altro tipo di residui standardizzati. Sono calcolati come i residui standardizzati ma poi vengono normalizzati. La normalizzazione qui significa che i residui vengono ulteriormente trasformati in modo che la loro distribuzione si avvicini a una distribuzione normale. Questo è utile per verificare se i residui seguono una distribuzione normale, il che è un’assunzione comune in molti modelli statistici, inclusi quelli SEM.\nCorrelazione dei Residui secondo Bollen (Cor.bollen): Questo tipo di residuo si riferisce alla correlazione tra i residui di due variabili diverse nel modello. Il metodo cor.bollen calcola la correlazione tra i residui dopo che il modello è stato adattato ai dati. Questo tipo di analisi è utile per rilevare se ci sono correlazioni non modellate tra le variabili che potrebbero influenzare la validità del modello.\n\n\nlavaan::residuals(somatic, type = \"standardized.mplus\") \n#&gt; $type\n#&gt; [1] \"standardized.mplus\"\n#&gt; \n#&gt; $cov\n#&gt;             morale reltns symptm    ses dysfnc\n#&gt; morale       0.000                            \n#&gt; relations    0.000  0.000                     \n#&gt; symptoms     0.000  0.427  0.000              \n#&gt; ses          0.000 -1.776  4.542  0.000       \n#&gt; dysfunction  0.000 -3.291  2.549  0.000  0.000\n\n\nlavaan::residuals(somatic, type = \"normalized\") \n#&gt; $type\n#&gt; [1] \"normalized\"\n#&gt; \n#&gt; $cov\n#&gt;             morale reltns symptm    ses dysfnc\n#&gt; morale       0.000                            \n#&gt; relations    0.000  0.000                     \n#&gt; symptoms     0.000  0.300  0.000              \n#&gt; ses          0.000 -1.424  3.711  0.000       \n#&gt; dysfunction  0.000 -2.769  2.142  0.000  0.000\n\n\nlavaan::residuals(somatic, type = \"cor.bollen\")\n#&gt; $type\n#&gt; [1] \"cor.bollen\"\n#&gt; \n#&gt; $cov\n#&gt;             morale reltns symptm    ses dysfnc\n#&gt; morale       0.000                            \n#&gt; relations    0.000  0.000                     \n#&gt; symptoms     0.000  0.014  0.000              \n#&gt; ses          0.000 -0.066  0.181  0.000       \n#&gt; dysfunction  0.000 -0.128  0.100  0.000  0.000\n\nAdattiamo ai dati il modello medico convenzionale.\n\nmedical &lt;- lavaan::sem(medical.model,\n    sample.cov = romney.cov,\n    sample.nobs = 469, fixed.x = FALSE, sample.cov.rescale = FALSE\n)\n\n\nsemPaths(medical,\n    whatLabels = \"std\",\n    sizeMan = 10,\n    edge.label.cex = 1.15,\n    style = \"mx\",\n    nCharNodes = 0, nCharEdges = 0\n)\n\n\n\n\n\n\n\n\nlavaan::summary(medical, fit.measures = TRUE, rsquare = TRUE)  \n#&gt; lavaan 0.6-19 ended normally after 1 iteration\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        12\n#&gt; \n#&gt;   Number of observations                           469\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                 3.245\n#&gt;   Degrees of freedom                                 3\n#&gt;   P-value (Chi-square)                           0.355\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               400.859\n#&gt;   Degrees of freedom                                 9\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.999\n#&gt;   Tucker-Lewis Index (TLI)                       0.998\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -8554.222\n#&gt;   Loglikelihood unrestricted model (H1)      -8552.599\n#&gt;                                                       \n#&gt;   Akaike (AIC)                               17132.444\n#&gt;   Bayesian (BIC)                             17182.251\n#&gt;   Sample-size adjusted Bayesian (SABIC)      17144.166\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.013\n#&gt;   90 Percent confidence interval - lower         0.000\n#&gt;   90 Percent confidence interval - upper         0.080\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.742\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.050\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.016\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Regressions:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;   ses ~                                               \n#&gt;     symptoms          0.448    0.063    7.110    0.000\n#&gt;     dysfunction       0.221    0.055    4.019    0.000\n#&gt;   morale ~                                            \n#&gt;     symptoms          0.107    0.009   11.756    0.000\n#&gt;     ses               0.021    0.006    3.291    0.001\n#&gt;   relations ~                                         \n#&gt;     dysfunction      -0.024    0.007   -3.335    0.001\n#&gt;     morale            0.504    0.037   13.745    0.000\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;   symptoms ~~                                         \n#&gt;     dysfunction      59.670   15.553    3.836    0.000\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;    .ses             521.598   34.062   15.313    0.000\n#&gt;    .morale            9.884    0.645   15.313    0.000\n#&gt;    .relations         8.732    0.570   15.313    0.000\n#&gt;     symptoms        289.000   18.872   15.313    0.000\n#&gt;     dysfunction     380.250   24.831   15.313    0.000\n#&gt; \n#&gt; R-Square:\n#&gt;                    Estimate\n#&gt;     ses               0.145\n#&gt;     morale            0.297\n#&gt;     relations         0.290\n\n\nlavaan::residuals(medical, type = \"standardized.mplus\")\n#&gt; $type\n#&gt; [1] \"standardized.mplus\"\n#&gt; \n#&gt; $cov\n#&gt;                ses morale reltns symptm dysfnc\n#&gt; ses          0.000                            \n#&gt; morale       0.000  0.000                     \n#&gt; relations   -1.161 -1.818     NA              \n#&gt; symptoms     0.000  0.000  0.833  0.000       \n#&gt; dysfunction  0.000  0.842  0.862  0.000  0.000\n\n\nlavaan::residuals(medical, type = \"normalized\") \n#&gt; $type\n#&gt; [1] \"normalized\"\n#&gt; \n#&gt; $cov\n#&gt;                ses morale reltns symptm dysfnc\n#&gt; ses          0.000                            \n#&gt; morale       0.000  0.000                     \n#&gt; relations   -0.901 -0.080 -0.069              \n#&gt; symptoms     0.000  0.000  0.573  0.000       \n#&gt; dysfunction  0.000  0.680  0.370  0.000  0.000\n\n\nlavaan::residuals(medical, type = \"cor.bollen\") \n#&gt; $type\n#&gt; [1] \"cor.bollen\"\n#&gt; \n#&gt; $cov\n#&gt;                ses morale reltns symptm dysfnc\n#&gt; ses          0.000                            \n#&gt; morale       0.000  0.000                     \n#&gt; relations   -0.041 -0.003  0.000              \n#&gt; symptoms     0.000  0.000  0.028  0.000       \n#&gt; dysfunction  0.000  0.032  0.017  0.000  0.000\n\nIn conclusione, i valori i valori degli indici di adattamento predittivo per i due modelli alternativi di Romney et al. (1992) sono stati generati seguendo le istruzioni precedentemente descritte. Non sorprende che l’adattamento globale del modello medico convenzionale, più complesso (con $ dfM = 3 $), sia migliore rispetto a quello del modello psicosomatico, più semplice (con $ dfM = 5 $). Nonostante il modello medico convenzionale sia più complesso e quindi soggetto a una penalità maggiore per il minor numero di gradi di libertà, i valori ottenuti sia nell’AIC che nel BIC sono inferiori rispetto a quelli del modello psicosomatico. Questo indica che il vantaggio in termini di adattamento del modello medico convenzionale è sufficiente a superare la penalità per la sua maggiore complessità. In base a queste analisi, il modello medico convenzionale è considerato più adatto rispetto al modello psicosomatico, come evidenziato dai valori più bassi nei criteri AIC e BIC.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html#indici-di-modifica-e-statistiche-correlate-nel-sem",
    "href": "chapters/sem/04_mod_comp.html#indici-di-modifica-e-statistiche-correlate-nel-sem",
    "title": "43  Confronto tra modelli",
    "section": "\n43.10 Indici di Modifica e Statistiche Correlate nel SEM",
    "text": "43.10 Indici di Modifica e Statistiche Correlate nel SEM\nNell’elaborazione di modelli SEM (Structural Equation Modeling), esiste la possibilità di implementare modifiche attraverso processi automatizzati. Questi interventi, di carattere esplorativo, si basano sull’aggiunta o sulla rimozione di parametri seguendo criteri empirici, come la significatività statistica di un indice di modifica (MI) o di un test di punteggio. Gli indici di modifica, in particolare, sono calcolati per quei parametri che nel modello sono stati inizialmente vincolati e servono a stimare quanto il chi-quadrato del modello di massima verosimiglianza (chiML) si ridurrebbe se un dato parametro vincolato venisse liberato.\nIl meccanismo di modifica automatica opera liberando, ad ogni iterazione, il parametro vincolato che presenta il valore di MI più alto. Questo processo continua finché non si raggiunge un MI che è statisticamente significativo secondo i criteri stabiliti dal ricercatore. È cruciale, tuttavia, riconoscere che l’uso di questa metodologia, specialmente in campioni di piccole dimensioni, può portare alla formulazione di modelli che si basano eccessivamente sul caso. Di conseguenza, questi modelli potrebbero risultare poco robusti e difficilmente replicabili in studi successivi. L’automazione nel processo di ottimizzazione dei modelli SEM richiede dunque un’attenta valutazione del contesto e della dimensione del campione per garantire l’affidabilità e la validità dei risultati ottenuti.\nUn altro strumento di uso frequente nei SEM è il test di Wald, basato sulla statistica W. Questo test è progettato per valutare l’impatto che avrebbe la fissazione a zero di un parametro precedentemente stimato liberamente nel modello. In termini più tecnici, il test di Wald stima l’incremento che si verificherebbe nel chi-quadrato del modello di massima verosimiglianza (chiML) se tale parametro fosse “potato”, ovvero escluso dal modello.\nSimilmente ai processi di modifica automatica, l’efficacia del test di Wald può essere influenzata dalla casualità. Un aspetto cruciale da considerare è la sensibilità di questi test alla dimensione del campione. Infatti, anche piccole modifiche nella bontà di adattamento del modello possono assumere una significatività statistica notevole in campioni di ampie dimensioni.\nDi conseguenza, quando si valutano gli indici di modifica come il MI (Modifica Index), è essenziale che il ricercatore non si limiti a considerarne la sola significatività statistica. È importante anche valutare l’entità del cambiamento che si verificherebbe nel coefficiente del parametro se fosse liberato. Se il cambiamento previsto è minimo, la significatività statistica dell’indice di modifica potrebbe essere più indicativa della dimensione del campione che non della sostanziale rilevanza dell’effetto analizzato. Questa considerazione sottolinea l’importanza di un approccio olistico e critico nell’interpretazione dei risultati dei test diagnostici in SEM, specialmente in contesti dove la dimensione del campione può influenzare significativamente i risultati.\n\nfit_mdd &lt;- cfa(\n    model_mdd,\n    data = d_mdd\n)\n\nCalcoliamo gli indici di modifica.\n\nmodification_indices &lt;- lavInspect(fit_mdd, \"mi\")\nmodification_indices \n#&gt;     lhs op  rhs     mi    epc sepc.lv sepc.all sepc.nox\n#&gt; 20 mdd1 ~~ mdd2 46.877  0.732   0.732    0.560    0.560\n#&gt; 21 mdd1 ~~ mdd3  4.996 -0.211  -0.211   -0.108   -0.108\n#&gt; 22 mdd1 ~~ mdd4  4.625 -0.215  -0.215   -0.108   -0.108\n#&gt; 23 mdd1 ~~ mdd5  7.797 -0.266  -0.266   -0.137   -0.137\n#&gt; 24 mdd1 ~~ mdd6  6.434 -0.236  -0.236   -0.132   -0.132\n#&gt; 25 mdd1 ~~ mdd7  0.293  0.053   0.053    0.026    0.026\n#&gt; 26 mdd1 ~~ mdd8  5.046 -0.219  -0.219   -0.116   -0.116\n#&gt; 27 mdd1 ~~ mdd9  5.385  0.187   0.187    0.111    0.111\n#&gt; 28 mdd2 ~~ mdd3  1.435 -0.138  -0.138   -0.055   -0.055\n#&gt; 29 mdd2 ~~ mdd4 10.931 -0.402  -0.402   -0.158   -0.158\n#&gt; 30 mdd2 ~~ mdd5  5.835 -0.281  -0.281   -0.113   -0.113\n#&gt; 31 mdd2 ~~ mdd6  2.235 -0.169  -0.169   -0.074   -0.074\n#&gt; 32 mdd2 ~~ mdd7  0.394 -0.076  -0.076   -0.029   -0.029\n#&gt; 33 mdd2 ~~ mdd8  0.027 -0.019  -0.019   -0.008   -0.008\n#&gt; 34 mdd2 ~~ mdd9  2.070 -0.142  -0.142   -0.066   -0.066\n#&gt; 35 mdd3 ~~ mdd4 16.199  0.591   0.591    0.155    0.155\n#&gt; 36 mdd3 ~~ mdd5  3.298  0.258   0.258    0.069    0.069\n#&gt; 37 mdd3 ~~ mdd6  6.361  0.337   0.337    0.098    0.098\n#&gt; 38 mdd3 ~~ mdd7  0.499 -0.106  -0.106   -0.027   -0.027\n#&gt; 39 mdd3 ~~ mdd8  0.014 -0.017  -0.017   -0.005   -0.005\n#&gt; 40 mdd3 ~~ mdd9  2.860 -0.208  -0.208   -0.064   -0.064\n#&gt; 41 mdd4 ~~ mdd5 12.198  0.509   0.509    0.136    0.136\n#&gt; 42 mdd4 ~~ mdd6 17.435  0.573   0.573    0.165    0.165\n#&gt; 43 mdd4 ~~ mdd7  1.272 -0.174  -0.174   -0.043   -0.043\n#&gt; 44 mdd4 ~~ mdd8  0.975  0.143   0.143    0.039    0.039\n#&gt; 45 mdd4 ~~ mdd9  1.879 -0.173  -0.173   -0.053   -0.053\n#&gt; 46 mdd5 ~~ mdd6  7.502  0.364   0.364    0.107    0.107\n#&gt; 47 mdd5 ~~ mdd7  0.096  0.046   0.046    0.012    0.012\n#&gt; 48 mdd5 ~~ mdd8  4.217  0.288   0.288    0.080    0.080\n#&gt; 49 mdd5 ~~ mdd9  0.544 -0.090  -0.090   -0.028   -0.028\n#&gt; 50 mdd6 ~~ mdd7  2.046 -0.201  -0.201   -0.055   -0.055\n#&gt; 51 mdd6 ~~ mdd8  0.877  0.124   0.124    0.037    0.037\n#&gt; 52 mdd6 ~~ mdd9  2.479 -0.180  -0.180   -0.061   -0.061\n#&gt; 53 mdd7 ~~ mdd8  0.188  0.064   0.064    0.017    0.017\n#&gt; 54 mdd7 ~~ mdd9 13.527  0.474   0.474    0.139    0.139\n#&gt; 55 mdd8 ~~ mdd9  0.322  0.069   0.069    0.022    0.022\n\nNel modello che stiamo analizzando, l’indice di modifica (MI) più elevato si riferisce alla possibile modifica del parametro che governa la correlazione tra i residui degli indicatori mm1 e mm2. Nel modello model_mdd questa correlazione residua è impostata a zero, indicando l’assenza di una correlazione diretta tra questi residui. Tuttavia, l’indice di modifica suggerisce che se permettessimo a questa correlazione di essere stimata liberamente dal modello (anziché tenerla fissa a zero), si verificherebbe un miglioramento dell’adattamento del modello ai dati osservati. Questa osservazione è in linea con il modello alternativo che è stato proposto per questi dati proposto da Brown (2015).\nIncorporare una correlazione residua tra mm1 e mm2 significa riconoscere che, oltre alla variazione spiegata dalle variabili latenti comuni, esiste una relazione unica tra questi due indicatori che non è catturata dal modello. Tale relazione potrebbe essere dovuta a fattori specifici relativi a questi indicatori o a una misurazione comune non prevista dal modello originale.\nÈ importante sottolineare che ogni modifica al modello basata sugli indici di modifica dovrebbe essere attentamente valutata per assicurarsi che sia supportata sia da giustificazioni teoriche che empiriche. Aggiungere correlazioni residue può migliorare l’adattamento del modello, ma dovrebbe essere fatto con cautela per evitare di creare un modello eccessivamente complesso che potrebbe non essere generalizzabile al di fuori del campione di dati specifico utilizzato.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html#riflessioni-conclusive",
    "href": "chapters/sem/04_mod_comp.html#riflessioni-conclusive",
    "title": "43  Confronto tra modelli",
    "section": "\n43.11 Riflessioni Conclusive",
    "text": "43.11 Riflessioni Conclusive\nNel campo dei modelli SEM, è pratica comune selezionare il modello più appropriato da un insieme di alternative, tutte calibrate sugli stessi dati. È frequente il confronto tra modelli gerarchicamente collegati, in cui il modello più restrittivo è incluso, o annidato, in quello meno restrittivo. In queste situazioni, il test di differenza del chi-quadrato viene impiegato per valutare se i modelli hanno un adattamento equivalente. Utilizzare questo test e le statistiche diagnostiche correlate, come gli indici di modifica, richiede un approccio guidato dalla teoria, non solo da criteri empirici. Un eccessivo affidamento su criteri puramente empirici, come la significatività statistica, può portare a una dipendenza eccessiva dal caso.\nPer confrontare modelli non annidati, il test di differenza del chi-quadrato non è idoneo, ma si possono adottare gli indici di adattamento predittivo, basati sulla teoria dell’informazione, per la loro valutazione. Quando si decide di mantenere un modello, è cruciale prendere in considerazione anche altri modelli alternativi potenzialmente equivalenti. È importante fornire motivazioni solide su perché il modello selezionato dal ricercatore sia da preferire rispetto a queste alternative equivalenti.\nQuesto approccio consente una comprensione più profonda e una scelta più informata del modello, assicurando che la selezione sia fondata su basi teoriche solide e non solamente su risultati statistici.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html#session-info",
    "href": "chapters/sem/04_mod_comp.html#session-info",
    "title": "43  Confronto tra modelli",
    "section": "\n43.12 Session Info",
    "text": "43.12 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.2\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] effectsize_1.0.0   rsvg_2.6.1         DiagrammeRsvg_0.1 \n#&gt;  [4] mvnormalTest_1.0.0 lavaanPlot_0.8.1   lavaanExtra_0.2.1 \n#&gt;  [7] ggokabeito_0.1.0   see_0.11.0         MASS_7.3-65       \n#&gt; [10] viridis_0.6.5      viridisLite_0.4.2  ggpubr_0.6.0      \n#&gt; [13] ggExtra_0.10.1     gridExtra_2.3      patchwork_1.3.0   \n#&gt; [16] bayesplot_1.11.1   semTools_0.5-6     semPlot_1.1.6     \n#&gt; [19] lavaan_0.6-19      psych_2.4.12       scales_1.3.0      \n#&gt; [22] markdown_1.13      knitr_1.50         lubridate_1.9.4   \n#&gt; [25] forcats_1.0.0      stringr_1.5.1      dplyr_1.1.4       \n#&gt; [28] purrr_1.0.4        readr_2.1.5        tidyr_1.3.1       \n#&gt; [31] tibble_3.2.1       ggplot2_3.5.1      tidyverse_2.0.0   \n#&gt; [34] here_1.0.1        \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] splines_4.4.2       later_1.4.1         datawizard_1.0.1   \n#&gt;   [4] XML_3.99-0.18       rpart_4.1.24        lifecycle_1.0.4    \n#&gt;   [7] Rdpack_2.6.3        rstatix_0.7.2       rprojroot_2.0.4    \n#&gt;  [10] lattice_0.22-6      insight_1.1.0       rockchalk_1.8.157  \n#&gt;  [13] backports_1.5.0     magrittr_2.0.3      openxlsx_4.2.8     \n#&gt;  [16] Hmisc_5.2-3         rmarkdown_2.29      httpuv_1.6.15      \n#&gt;  [19] qgraph_1.9.8        zip_2.3.2           pbapply_1.7-2      \n#&gt;  [22] minqa_1.2.8         RColorBrewer_1.1-3  ADGofTest_0.3      \n#&gt;  [25] multcomp_1.4-28     abind_1.4-8         quadprog_1.5-8     \n#&gt;  [28] pspline_1.0-21      nnet_7.3-20         TH.data_1.1-3      \n#&gt;  [31] sandwich_3.1-1      moments_0.14.1      nortest_1.0-4      \n#&gt;  [34] arm_1.14-4          performance_0.13.0  codetools_0.2-20   \n#&gt;  [37] tidyselect_1.2.1    farver_2.1.2        lme4_1.1-36        \n#&gt;  [40] stats4_4.4.2        base64enc_0.1-3     jsonlite_1.9.1     \n#&gt;  [43] Formula_1.2-5       survival_3.8-3      emmeans_1.10.7     \n#&gt;  [46] tools_4.4.2         Rcpp_1.0.14         glue_1.8.0         \n#&gt;  [49] mnormt_2.1.1        xfun_0.51           withr_3.0.2        \n#&gt;  [52] numDeriv_2016.8-1.1 fastmap_1.2.0       boot_1.3-31        \n#&gt;  [55] digest_0.6.37       mi_1.1              timechange_0.3.0   \n#&gt;  [58] R6_2.6.1            mime_0.13           estimability_1.5.1 \n#&gt;  [61] colorspace_2.1-1    gtools_3.9.5        jpeg_0.1-10        \n#&gt;  [64] copula_1.1-6        DiagrammeR_1.0.11   generics_0.1.3     \n#&gt;  [67] data.table_1.17.0   corpcor_1.6.10      htmlwidgets_1.6.4  \n#&gt;  [70] parameters_0.24.2   pkgconfig_2.0.3     sem_3.1-16         \n#&gt;  [73] gtable_0.3.6        pcaPP_2.0-5         htmltools_0.5.8.1  \n#&gt;  [76] carData_3.0-5       png_0.1-8           reformulas_0.4.0   \n#&gt;  [79] rstudioapi_0.17.1   tzdb_0.5.0          reshape2_1.4.4     \n#&gt;  [82] coda_0.19-4.1       visNetwork_2.1.2    checkmate_2.3.2    \n#&gt;  [85] nlme_3.1-167        curl_6.2.1          nloptr_2.2.1       \n#&gt;  [88] zoo_1.8-13          parallel_4.4.2      miniUI_0.1.1.1     \n#&gt;  [91] foreign_0.8-88      pillar_1.10.1       grid_4.4.2         \n#&gt;  [94] vctrs_0.6.5         promises_1.3.2      car_3.1-3          \n#&gt;  [97] OpenMx_2.21.13      xtable_1.8-4        cluster_2.1.8.1    \n#&gt; [100] htmlTable_2.4.3     evaluate_1.0.3      pbivnorm_0.6.0     \n#&gt; [103] mvtnorm_1.3-3       cli_3.6.4           kutils_1.73        \n#&gt; [106] compiler_4.4.2      rlang_1.1.5         ggsignif_0.6.4     \n#&gt; [109] fdrtool_1.2.18      plyr_1.8.9          stringi_1.8.4      \n#&gt; [112] munsell_0.5.1       gsl_2.1-8           lisrelToR_0.3      \n#&gt; [115] bayestestR_0.15.2   pacman_0.5.1        V8_6.0.2           \n#&gt; [118] Matrix_1.7-3        hms_1.1.3           stabledist_0.7-2   \n#&gt; [121] glasso_1.11         shiny_1.10.0        rbibutils_2.3      \n#&gt; [124] igraph_2.1.4        broom_1.0.7         RcppParallel_5.1.10\n\n\n\n\n\nBrown, T. A. (2015). Confirmatory factor analysis for applied research. Guilford publications.\n\n\nKline, R. B. (2023). Principles and practice of structural equation modeling. Guilford publications.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/05_cfa_mod_comp.html",
    "href": "chapters/sem/05_cfa_mod_comp.html",
    "title": "52  CFA: confronto tra modelli",
    "section": "",
    "text": "52.1 Introduzione\nIn un modello CFA, i parametri possono essere stimati senza vincoli, possono essere fissi o possono essre stimati sulla base di alcuni vincoli. Un parametro libero è sconosciuto e il ricercatore consente all’algoritmo di stima di trovare il suo valore ottimale che, insime agli altri parametri del modello, riduce al minimo le differenze tra le matrici di varianze-covarianze osservate e quelle predette dal modello. Un parametro fisso è pre-specificato dal ricercatore ad un valore specifico, più comunemente 1.0 (ad esempio, per definire la metrica di una variabile latente) o 0 (ad esempio, l’assenza di saturazionoi fattoriali o di covarianze di errore). Come per un parametro libero, anche un parametro vincolato è sconosciuto; tuttavia, un tale parametro non può assumere un valore qualsiasi, ma deve rispettare le restrizioni su suoi valori che il ricercatore ha imposto. I vincoli più comuni sono i vincoli di uguaglianza, in cui i parametri non standardizzati devono assumere valori uguali (ad esempio, in diversi gruppi).\nConsideriamo un esempio discusso da Brown (2015). Viene qui esaminato un set di dati in cui le prime tre misure osservate (X1, X2, X3) sono indicatori di un costrutto latente corrispondente alla Memoria uditiva e il secondo insieme di misure (X4, X5, X6) sono indicatori di un altro costrutto latente, Memoria visiva. Le tre misure usate quali indicatori del costrutto di memoria uditiva sono:\nle tre misure usate come indicatori del costrutto di memoria visiva sono:\nI dati sono i seguenti:\nsds &lt;- '2.610  2.660  2.590  1.940  2.030  2.050'\n\ncors &lt;-'\n  1.000\n  0.661  1.000\n  0.630  0.643  1.000\n  0.270  0.300  0.268  1.000\n  0.297  0.265  0.225  0.805  1.000\n  0.290  0.287  0.248  0.796  0.779  1.000'\n\ncovs &lt;- getCov(cors, sds = sds, names = paste(\"x\", 1:6, sep = \"\"))\nprint(covs)\n#&gt;       x1    x2    x3    x4    x5    x6\n#&gt; x1 6.812 4.589 4.259 1.367 1.574 1.552\n#&gt; x2 4.589 7.076 4.430 1.548 1.431 1.565\n#&gt; x3 4.259 4.430 6.708 1.347 1.183 1.317\n#&gt; x4 1.367 1.548 1.347 3.764 3.170 3.166\n#&gt; x5 1.574 1.431 1.183 3.170 4.121 3.242\n#&gt; x6 1.552 1.565 1.317 3.166 3.242 4.202\nAdattiamo i cinque modelli discussi da Brown (2015).",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>CFA: confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/05_cfa_mod_comp.html#introduzione",
    "href": "chapters/sem/05_cfa_mod_comp.html#introduzione",
    "title": "52  CFA: confronto tra modelli",
    "section": "",
    "text": "X1 = memoria logica,\nX2 = associazione verbale a coppie,\nX3 = liste di parole;\n\n\n\nX4 = immagini di facce,\nX5 = foto di famiglia,\nX6 = generiche riproduzioni visive.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>CFA: confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/05_cfa_mod_comp.html#modello-congenerico",
    "href": "chapters/sem/05_cfa_mod_comp.html#modello-congenerico",
    "title": "52  CFA: confronto tra modelli",
    "section": "\n52.2 Modello congenerico",
    "text": "52.2 Modello congenerico\n\nmodel.congeneric &lt;- '\n  auditorymemory =~ x1 + x2 + x3\n  visualmemory   =~ x4 + x5 + x6\n'\n\n\nfit.congeneric &lt;- cfa(\n  model.congeneric, \n  sample.cov = covs, \n  sample.nobs = 200, \n  std.lv = TRUE\n)\n\nL’output si ottiene con:\n\nsummary(\n  fit.congeneric, \n  fit.measures = TRUE, \n  standardized = TRUE, \n  rsquare = TRUE\n)\n#&gt; lavaan 0.6-19 ended normally after 21 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        13\n#&gt; \n#&gt;   Number of observations                           200\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                 4.877\n#&gt;   Degrees of freedom                                 8\n#&gt;   P-value (Chi-square)                           0.771\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               719.515\n#&gt;   Degrees of freedom                                15\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    1.000\n#&gt;   Tucker-Lewis Index (TLI)                       1.008\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -2337.980\n#&gt;   Loglikelihood unrestricted model (H1)      -2335.541\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                4701.959\n#&gt;   Bayesian (BIC)                              4744.837\n#&gt;   Sample-size adjusted Bayesian (SABIC)       4703.652\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.000\n#&gt;   90 Percent confidence interval - lower         0.000\n#&gt;   90 Percent confidence interval - upper         0.057\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.929\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.010\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.012\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                     Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   auditorymemory =~                                                      \n#&gt;     x1                 2.101    0.166   12.663    0.000    2.101    0.807\n#&gt;     x2                 2.182    0.168   12.976    0.000    2.182    0.823\n#&gt;     x3                 2.013    0.166   12.124    0.000    2.013    0.779\n#&gt;   visualmemory =~                                                        \n#&gt;     x4                 1.756    0.108   16.183    0.000    1.756    0.907\n#&gt;     x5                 1.795    0.115   15.608    0.000    1.795    0.887\n#&gt;     x6                 1.796    0.117   15.378    0.000    1.796    0.878\n#&gt; \n#&gt; Covariances:\n#&gt;                     Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   auditorymemory ~~                                                      \n#&gt;     visualmemory       0.382    0.070    5.463    0.000    0.382    0.382\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .x1                2.366    0.372    6.365    0.000    2.366    0.349\n#&gt;    .x2                2.277    0.383    5.940    0.000    2.277    0.323\n#&gt;    .x3                2.621    0.373    7.027    0.000    2.621    0.393\n#&gt;    .x4                0.662    0.117    5.668    0.000    0.662    0.177\n#&gt;    .x5                0.877    0.134    6.554    0.000    0.877    0.214\n#&gt;    .x6                0.956    0.139    6.866    0.000    0.956    0.229\n#&gt;     auditorymemory    1.000                               1.000    1.000\n#&gt;     visualmemory      1.000                               1.000    1.000\n#&gt; \n#&gt; R-Square:\n#&gt;                    Estimate\n#&gt;     x1                0.651\n#&gt;     x2                0.677\n#&gt;     x3                0.607\n#&gt;     x4                0.823\n#&gt;     x5                0.786\n#&gt;     x6                0.771\n\nIl diagramma di percorso del modello è il seguente.\n\nsemPaths(\n  fit.congeneric,\n  \"std\",\n  posCol = c(\"black\"),\n  edge.label.cex = 1.2,\n  sizeMan = 7,\n  edge.width = 0.4, # Set a fixed width for all arrows\n  fade = FALSE # Disable fading of the arrows\n)",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>CFA: confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/05_cfa_mod_comp.html#modello-tau-equivalente",
    "href": "chapters/sem/05_cfa_mod_comp.html#modello-tau-equivalente",
    "title": "52  CFA: confronto tra modelli",
    "section": "\n52.3 Modello tau-equivalente",
    "text": "52.3 Modello tau-equivalente\nSolo memoria auditiva:\n\nmodel.tau.a &lt;- '\n  auditorymemory =~ x1 + v1*x1 + v1*x2 + v1*x3\n  visualmemory   =~ x4 + x5 + x6\n'\n\n\nfit.tau.a &lt;- cfa(\n  model.tau.a, \n  sample.cov = covs, \n  sample.nobs = 200, \n  std.lv = TRUE\n)\n\nMemoria auditiva e visiva:\n\nmodel.tau.av &lt;- '\n  auditorymemory =~ NA*x1 + v1*x1 + v1*x2 + v1*x3\n  visualmemory   =~ NA*x4 + v2*x4 + v2*x5 + v2*x6\n'\n\n\nfit.tau.av &lt;- cfa(\n  model.tau.av, \n  sample.cov = covs, \n  sample.nobs = 200, \n  std.lv = TRUE\n)\n\n\nsemPaths(\n  fit.tau.av,\n  \"std\",\n  posCol = c(\"black\"),\n  edge.label.cex = 1.2,\n  sizeMan = 7,\n  edge.width = 0.4, # Set a fixed width for all arrows\n  fade = FALSE # Disable fading of the arrows\n)",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>CFA: confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/05_cfa_mod_comp.html#modello-parallelo",
    "href": "chapters/sem/05_cfa_mod_comp.html#modello-parallelo",
    "title": "52  CFA: confronto tra modelli",
    "section": "\n52.4 Modello parallelo",
    "text": "52.4 Modello parallelo\nSolo memoria auditiva:\n\nmodel.parallel.a &lt;- '\n  auditorymemory =~ x1 + v1*x1 + v1*x2 + v1*x3\n  visualmemory   =~ x4 + v2*x4 + v2*x5 + v2*x6\n  x1 ~~ v3 * x1\n  x2 ~~ v3 * x2\n  x3 ~~ v3 * x3\n'\n\n\nfit.parallel.a &lt;- cfa(\n  model.parallel.a, \n  sample.cov = covs, \n  sample.nobs = 200, \n  std.lv = TRUE\n)\n\nMemoria auditiva e visiva:\n\nmodel.parallel.av &lt;- '\n  auditorymemory =~ x1 + v1*x1 + v1*x2 + v1*x3\n  visualmemory   =~ x4 + v2*x4 + v2*x5 + v2*x6\n  x1 ~~ v3 * x1\n  x2 ~~ v3 * x2\n  x3 ~~ v3 * x3\n \n  x4 ~~ v4 * x4\n  x5 ~~ v4 * x5\n  x6 ~~ v4 * x6\n'\n\n\nfit.parallel.av &lt;- cfa(\n  model.parallel.av, \n  sample.cov = covs, \n  sample.nobs = 200, \n  std.lv = TRUE\n)\n\n\nsemPaths(\n    fit.parallel.av,\n    \"std\",\n    posCol = c(\"black\"),\n    edge.label.cex = 1.2,\n    sizeMan = 7,\n    edge.width = 0.4, # Set a fixed width for all arrows\n    fade = FALSE # Disable fading of the arrows\n)",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>CFA: confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/05_cfa_mod_comp.html#il-test-del-chi2",
    "href": "chapters/sem/05_cfa_mod_comp.html#il-test-del-chi2",
    "title": "52  CFA: confronto tra modelli",
    "section": "\n52.5 Il test del \\(\\chi^2\\)\n",
    "text": "52.5 Il test del \\(\\chi^2\\)\n\nIl confronto tra modelli nidificati procede attraverso il test \\(\\chi^2\\). Tale test si basa su una proprietà delle variabili casuali distribuite come \\(\\chi^2\\): la differenza tra due v.c. \\(X_1\\) e \\(X_2\\) che seguono la distribuzione \\(\\chi^2\\), rispettivamente con \\(\\nu_1\\) e \\(\\nu_2\\), con \\(\\nu_1 &gt; \\nu_2\\), è una variabile causale che segue la distribuzione \\(\\chi^2\\) con gradi di libertà pari a \\(\\nu_1 - \\nu_2\\).\nUn modello nidificato è un modello che impone dei vincoli sui parametri del modello di partenza. L’imposizione di vincoli sui parametri ha la conseguenza che vi sarà un numero minore di parametri da stimare. Il confronto tra i modelli si esegue valutando in maniera relativa la bontà di adattamento di ciascun modello per mezzo della statistica chi-quadrato. La statistica così calcolata avrà un numero di gradi di libertà uguale alla differenza tra i gradi di libertà dei due modelli.\nNel caso dell’esempio in dicussione, abbiamo\n\nout = anova(\n  fit.congeneric, \n  fit.tau.a, \n  fit.tau.av, \n  fit.parallel.a, \n  fit.parallel.av, \n  test = \"chisq\"\n)\nprint(out)\n#&gt; \n#&gt; Chi-Squared Difference Test\n#&gt; \n#&gt;                 Df  AIC  BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\n#&gt; fit.congeneric   8 4702 4745  4.88                                    \n#&gt; fit.tau.a       10 4699 4735  5.66       0.78 0.000       2       0.68\n#&gt; fit.tau.av      12 4695 4725  5.88       0.22 0.000       2       0.90\n#&gt; fit.parallel.a  14 4691 4714  5.98       0.10 0.000       2       0.95\n#&gt; fit.parallel.av 16 4690 4707  9.28       3.30 0.057       2       0.19\n\nI test precedenti indicano come non vi sia una perdita di adattamento passando dal modello congenerico al modello più restrittivo (ovvero, il modello parallelo per entrambi i fattori). Per questi dati, dunque, può essere adottato il modello più semplice, cioè il modello parallelo.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>CFA: confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/05_cfa_mod_comp.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/sem/05_cfa_mod_comp.html#informazioni-sullambiente-di-sviluppo",
    "title": "52  CFA: confronto tra modelli",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.2\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] ggokabeito_0.1.0  see_0.11.0        MASS_7.3-65       viridis_0.6.5    \n#&gt;  [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n#&gt;  [9] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n#&gt; [13] lavaan_0.6-19     psych_2.4.12      scales_1.3.0      markdown_1.13    \n#&gt; [17] knitr_1.50        lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n#&gt; [21] dplyr_1.1.4       purrr_1.0.4       readr_2.1.5       tidyr_1.3.1      \n#&gt; [25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.1      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.2.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] mime_0.13           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [25] Matrix_1.7-3        R6_2.6.1            fastmap_1.2.0      \n#&gt;  [28] rbibutils_2.3       shiny_1.10.0        digest_0.6.37      \n#&gt;  [31] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n#&gt;  [34] rprojroot_2.0.4     Hmisc_5.2-3         timechange_0.3.0   \n#&gt;  [37] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n#&gt;  [40] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n#&gt;  [43] carData_3.0-5       ggsignif_0.6.4      corpcor_1.6.10     \n#&gt;  [46] gtools_3.9.5        tools_4.4.2         pbivnorm_0.6.0     \n#&gt;  [49] foreign_0.8-88      zip_2.3.2           httpuv_1.6.15      \n#&gt;  [52] nnet_7.3-20         glue_1.8.0          quadprog_1.5-8     \n#&gt;  [55] nlme_3.1-167        promises_1.3.2      lisrelToR_0.3      \n#&gt;  [58] grid_4.4.2          checkmate_2.3.2     cluster_2.1.8.1    \n#&gt;  [61] reshape2_1.4.4      generics_0.1.3      gtable_0.3.6       \n#&gt;  [64] tzdb_0.5.0          data.table_1.17.0   hms_1.1.3          \n#&gt;  [67] car_3.1-3           sem_3.1-16          pillar_1.10.1      \n#&gt;  [70] rockchalk_1.8.157   later_1.4.1         splines_4.4.2      \n#&gt;  [73] lattice_0.22-6      survival_3.8-3      kutils_1.73        \n#&gt;  [76] tidyselect_1.2.1    miniUI_0.1.1.1      pbapply_1.7-2      \n#&gt;  [79] reformulas_0.4.0    stats4_4.4.2        xfun_0.51          \n#&gt;  [82] qgraph_1.9.8        arm_1.14-4          stringi_1.8.4      \n#&gt;  [85] pacman_0.5.1        boot_1.3-31         evaluate_1.0.3     \n#&gt;  [88] codetools_0.2-20    mi_1.1              cli_3.6.4          \n#&gt;  [91] RcppParallel_5.1.10 rpart_4.1.24        xtable_1.8-4       \n#&gt;  [94] Rdpack_2.6.3        munsell_0.5.1       Rcpp_1.0.14        \n#&gt;  [97] coda_0.19-4.1       png_0.1-8           XML_3.99-0.18      \n#&gt; [100] parallel_4.4.2      jpeg_0.1-10         lme4_1.1-36        \n#&gt; [103] mvtnorm_1.3-3       openxlsx_4.2.8      rlang_1.1.5        \n#&gt; [106] multcomp_1.4-28     mnormt_2.1.1\n\n\n\n\n\nBrown, T. A. (2015). Confirmatory factor analysis for applied research. Guilford publications.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>CFA: confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html",
    "href": "chapters/sem/06_refine_solution.html",
    "title": "53  La revisione del modello",
    "section": "",
    "text": "53.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nI passi principali nella CFA e nei modelli SEM comprendono la specificazione del modello, la stima dei parametri, la valutazione del modello e dei parametri e la modificazione del modello. Questa sequenza può essere ripetuta molte volte fino a quando non si trovi un modello considerato accettabile.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html#stima-del-modello",
    "href": "chapters/sem/06_refine_solution.html#stima-del-modello",
    "title": "53  La revisione del modello",
    "section": "53.2 Stima del modello",
    "text": "53.2 Stima del modello\nConsideriamo qui un modello SEM con una sola variabile latente identificata da un insieme di indicatori, ovvero un modello CFA. L’obiettivo della CFA è ottenere stime per i parametro del modello (vale a dire, saturazioni fattoriali, varianze e covarianze fattoriali, varianze residue ed eventualmente covarianze degli errori) che sono in grado di produrre una matrice di covarianza prevista (denotata da \\(\\boldsymbol{\\Sigma}\\)) la quale è il più possibile simile alla matrice di covarianze campionarie (denotata da \\(\\boldsymbol{S}\\)). Questo processo di stima è basato sulla minimizzazione di una funzione che descrive la differenza tra \\(\\boldsymbol{\\Sigma}\\) e \\(\\boldsymbol{S}\\). Il metodo di stima più utilizzato nella CFA (e, in generale, nei modelli SEM) è la massima verosimiglianza (ML).",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html#massima-verosimiglianza",
    "href": "chapters/sem/06_refine_solution.html#massima-verosimiglianza",
    "title": "53  La revisione del modello",
    "section": "53.3 Massima verosimiglianza",
    "text": "53.3 Massima verosimiglianza\nL’equazione fondamentale dell’analisi fattoriale è\n\\[\n\\boldsymbol y = \\boldsymbol \\Lambda  \\boldsymbol x  + \\boldsymbol z,\n\\]\ndove \\(\\boldsymbol{y}\\) è un vettore di \\(p\\) componenti (i punteggi osservati nel del test), \\(\\boldsymbol{x}\\) è un vettore di \\(k &lt; p\\) componenti (i punteggi fattoriali), \\(\\boldsymbol{\\Lambda}\\) è una \\(p \\cdot k\\) matrice (di saturazioni fattoriali), e \\(\\boldsymbol{z}\\) è un vettore di \\(p\\) componenti (la componenti dei punteggi del test non dovute all’effetto causale delle variabili comuni latenti). Per l’item \\(i\\)-esimo, in precedenza abbiamo scritto l’equazione precedente come\n\\[\ny_i = \\lambda_{i1} \\xi_1 + \\dots + \\lambda_{ik} \\xi_k + \\delta_i.\n\\]\nDalle assunzioni del modello fattoriale deriva che\n\\[\n\\boldsymbol{\\Sigma} = \\boldsymbol{\\Lambda}\\boldsymbol{\\Phi}\\boldsymbol{\\Lambda}^\\prime + \\Psi,\n\\]\ndove \\(\\boldsymbol{\\Phi}\\) è la matrice delle inter-correlazioni fattoriali.\nSi assume che il vettore casuale \\(\\boldsymbol{y}\\) abbia una distribuzione normale multivariata con matrice di covarianza \\(\\boldsymbol{\\Sigma}\\) e che da tale distribuzione sia stato estratto un campione casuale di \\(n\\) osservazioni \\(y_l, y_2, \\dots, y_n\\). Il logaritmo della funzione di verosimiglianza per il campione è dato da\n\\[\n\\log L = \\frac{1}{2}n [\\log | \\boldsymbol{\\Sigma}| + tr(\\boldsymbol{\\boldsymbol{S} \\Sigma}^{-1})].\n\\]\nL’equazione precedente viene vista come funzione di \\(\\Lambda\\) e \\(\\Psi\\). Anziché massimizzare \\(\\log L\\), è equivalente e più conveniente minimizzare\n\\[\nF_{k}(\\Lambda, \\Psi) = \\log |\\boldsymbol{\\Sigma}| + tr[\\boldsymbol{S}\\boldsymbol{\\Sigma}^{-1}]  - \\log|\\boldsymbol{S}| – p,\n\\]\ndove \\(|\\boldsymbol{S}|\\) è il determinante della matrice di covarianza tra le variabili osservate, \\(|\\boldsymbol{\\Sigma}|\\) è il determinante della matrice di covarianza prevista e \\(p\\) è il numero di indicatori.\nL’obiettivo della stima di massima verosimiglianza della CFA è trovare le stime dei parametri che rendono più verosimili i dati osservati (o, al contrario, massimizzano la verosimiglianza dei parametri dati i dati). Le stime dei parametri in un modello CFA si ottengono con una procedura iterativa. Cioè, l’algoritmo inizia con una serie iniziale di stime dei parametri (denominate valori iniziali o stime iniziali, che possono essere generate automaticamente dal software o specificate dall’utente) e raffina ripetutamente queste stime nel tentativo di minimizzare la differenza tra \\(\\boldsymbol{\\Sigma}\\) e \\(\\boldsymbol{S}\\). Il programma effettua controlli interni per valutare i suoi progressi nell’ottenere stime dei parametri che al meglio riproducono \\(\\boldsymbol{S}\\). Si raggiunge la convergenza quando l’algoritmo produce una serie di stime dei parametri che non possono essere ulteriormente migliorate per ridurre la differenza tra \\(\\boldsymbol{\\Sigma}\\) e \\(\\boldsymbol{S}\\).",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html#identificabilità-del-modello",
    "href": "chapters/sem/06_refine_solution.html#identificabilità-del-modello",
    "title": "53  La revisione del modello",
    "section": "53.4 Identificabilità del modello",
    "text": "53.4 Identificabilità del modello\nUn modello CFA deve essere formulato in modo tale da garantire la risolvibilità matematica dello stesso, ovvero deve essere tale da consentire una stima univoca dei parametri del modello. Detto in altre parole, la specificazione del modello ne deve garantire l’dentificabilità.\nIl problema dell’identificazione richiede, innanzitutto, di chiarire il concetto di gradi di libertà (degrees of freedom). Nel presente contesto, per gradi di libertà (\\(dof\\)) intendiamo\n\\[\ndof = \\# (\\text{unità di informazione}) - \\# (\\text{parametri da stimare}).\n\\]\nI dati che vengono analizzati da un modello CFA sono contenuti in una matrice di covarianza. Per una matrice di covarianza di ordine \\(p\\), il numero di unità di informazione è\n\\[\n\\frac{p (p+1)}{2}.\n\\]\nAffinché il modello sia identificabile, devono essere soddisfatte le seguenti condizioni.\n\nIndipendentemente dalla complessità del modello (ad es. modelli ad un fattore rispetto a più fattori), l’unità di misura delle variabili latenti deve essere specificata (di solito fissandola a un valore di 1);\nIndipendentemente dalla complessità del modello, il numero di unità di informazione (es. la matrice di covarianza degli indicatori) deve essere uguale o superiore al numero di parametri da stimare (es. saturazioni fattoriali, specificità, covarianze degli errori dell’indicatore, covarianze tra i fattori);\nNel caso di modelli ad un fattore è richiesto un minimo di tre indicatori. Quando vengono utilizzati tre indicatori, la soluzione a un fattore si dice “appena identificata” (just-identified); in tali condizioni non è possibile valutare la bontà dell’adattamento.\nNel caso di modelli a due o più fattori e due indicatori per costrutto latente, la soluzione è sovraidentificata, a condizione che ogni variabile latente sia correlata con almeno un’altra variabile latente e gli errori tra gli indicatori siano tra loro incorrelati. Tuttavia, poiché tali soluzioni sono suscettibili di scarsa identificazione empirica, viene raccomandato un minimo di tre indicatori per variabile latente.\n\nIn conclusione, una semplice e necessaria condizione per l’identificazione di un modello CFA è che vi siano più unità di informazione che parametri da stimare. Dunque, abbiamo che:\n\nse \\(dof &lt; 0\\), il modello non è identificato e, in questo caso, non è possibile stimare i parametri;\nse \\(dof = 0\\), il modello è appena identificato o “saturo”; in questo caso, la matrice di covarianza riprodotta coincide con la matrice di covarianza delle variabili osservate e, di conseguenza, non esiste un residuo attraverso cui valutare la bontà dell’adattamento del modello;\nse \\(dof &gt; 0\\), il modello è sovra-identificato ed esistono le condizioni per valutare la bontà dell’adattamento.\n\nLe considerazioni precedenti ci fanno capire perché non si può fare un’analisi fattoriale con solo due indicatori e un fattore; in tali circostanze, infatti, ci sono \\((2 \\cdot 3)/2 = 3\\) gradi di libertà, ma 4 parametri da stimare (due saturazioni fattoriali e due specificità). Il caso di tre item e un fattore definisce un modello “appena identificato”, ovvero, il caso in cui ci sono zero gradi di libertà. In tali circostanze è possibile stimare i parametri (ricordiamo il metodo dell’annullamento della tetrade), ma non è possibile un test di bontà dell’adattamento. Questo vuol dire, in pratica, che per un modello SEM ad un solo fattore comune latente è necessario disporre di almeno quattro indicatori.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html#un-esempio-concreto",
    "href": "chapters/sem/06_refine_solution.html#un-esempio-concreto",
    "title": "53  La revisione del modello",
    "section": "53.5 Un Esempio Concreto",
    "text": "53.5 Un Esempio Concreto\nNell’approfondire la tematica dei Modelli di Equazioni Strutturali, è utile considerare alcune problematiche comuni che possono emergere nella fase di adattamento del modello ai dati. Facendo riferimento agli esempi discussi da Brown (2015) nel contesto dell’analisi fattoriale confermativa (CFA), possiamo identificare diverse potenziali cause di inadeguato adattamento. Queste cause possono essere di natura sia teorica che tecnica e spesso richiedono un’attenta riflessione e analisi per essere risolte. Esaminiamo alcune delle questioni più rilevanti:\n\nNumero Errato di Fattori Comuni Latenti:\n\nUno degli errori più comuni è ipotizzare un numero di fattori latenti che non riflette adeguatamente la struttura sottostante dei dati. Un numero insufficiente di fattori può portare a un modello semplificato eccessivamente, mentre un numero eccessivo può causare sovra-aggiustamento e complessità non necessaria.\n\nItem che Saturano su Fattori Multipli:\n\nIn alcuni casi, un item può essere erroneamente ipotizzato per saturare su un singolo fattore comune, mentre in realtà ha relazioni significative con più fattori. Questo errore nella specificazione del modello può portare a stime imprecise e a un adattamento inadeguato.\n\nAssegnazione Errata degli Item ai Fattori:\n\nUn’altra possibile causa di inadeguato adattamento riguarda l’errata assegnazione di un item al fattore comune sbagliato. Tale errore può derivare da una comprensione insufficiente delle dimensioni teoriche che si stanno misurando o da una cattiva interpretazione dei dati empirici.\n\nCorrelazioni Residue Non Considerate:\n\nInfine, le correlazioni residue non incorporate nel modello possono giocare un ruolo significativo nell’adattamento del modello. Queste correlazioni possono indicare relazioni non catturate dai fattori comuni, suggerendo la necessità di rivedere l’ipotesi di base del modello o di aggiungere percorsi specifici per accomodare queste correlazioni.\n\n\nIn sintesi, l’adattamento del modello SEM ai dati è un processo complesso che richiede una profonda comprensione sia della teoria sottostante che della natura dei dati. Ogni volta che un modello non si adatta adeguatamente, è essenziale esaminare criticamente questi e altri potenziali fattori per identificare e correggere le cause alla base di tale inadeguatezza. Questo processo non solo migliora l’adattamento del modello, ma può anche fornire intuizioni preziose sulla struttura dei dati e sulla validità delle teorie sottostanti.\nBrown (2015) mostra come il ricercatore possa usare i Modification Indices per valutare le cause del mancato adattamento del modello ai dati. I Modification Indices sono una misura utilizzata per identificare le covariate tra le variabili del modello che potrebbero migliorare l’aderenza del modello ai dati. I modification indices indicano quale sarebbe il miglioramento nell’aderenza del modello, ad esempio, se venisse permessa la correlazione tra due variabili che attualmente non sono considerate correlate. Ciò consente di identificare le relazioni nascoste tra le variabili e può aiutare a migliorare la precisione e l’accuratezza del modello.\nTuttavia, è importante tenere presente che i modification indices da soli non dovrebbero essere usati per prendere decisioni definitive sulle modifiche del modello. Invece, dovrebbero essere considerati insieme ad altre informazioni, come la conoscenza teorica, l’esperienza e altre tecniche di analisi dei dati per determinare se una modifica del modello è giustificata e in che modo.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html#un-numero-di-fattori-troppo-piccolo",
    "href": "chapters/sem/06_refine_solution.html#un-numero-di-fattori-troppo-piccolo",
    "title": "53  La revisione del modello",
    "section": "53.6 Un numero di fattori troppo piccolo",
    "text": "53.6 Un numero di fattori troppo piccolo\nUna delle possibili fonti di mancanza di adattamento del modello può dipendere dal fatto che è stato ipotizzato un numero insufficiente di fattori latenti comuni. Brown (2015) discute il caso nel quale si confrontano gli indici di bontà di adattamento di un modello ad un solo fattore comune e un modello a due fattori comuni. L’esempio riguarda i dati già in precedenza discussi e relativi relativi a otto misure di personalità raccolte su un campione di 250 pazienti che hanno concluso un programma di psicoterapia. Le scale sono le seguenti:\n\nanxiety (N1),\nhostility (N2),\ndepression (N3),\nself-consciousness (N4),\nwarmth (E1),\ngregariousness (E2),\nassertiveness (E3),\npositive emotions (E4).\n\nLeggiamo i dati in \\(\\mathsf{R}\\).\n\nvarnames &lt;- c(\"N1\", \"N2\", \"N3\", \"N4\", \"E1\", \"E2\", \"E3\", \"E4\")\n\nsds &lt;- c(5.7,  5.6,  6.4,  5.7,  6.0,  6.2,  5.7,  5.6)\n\ncors &lt;- '\n 1.000\n 0.767  1.000 \n 0.731  0.709  1.000 \n 0.778  0.738  0.762  1.000 \n-0.351  -0.302  -0.356  -0.318  1.000 \n-0.316  -0.280  -0.300  -0.267  0.675  1.000 \n-0.296  -0.289  -0.297  -0.296  0.634  0.651  1.000 \n-0.282  -0.254  -0.292  -0.245  0.534  0.593  0.566  1.000'\n\npsychot_cor_mat &lt;- getCov(cors, names = varnames)\n\nn &lt;- 250\n\nSupponiamo di adattare ai dati il modello “sbagliato” che include un unico fattore comune. Svolgiamo qui l’analisi fattoriale esplorativa usando la funzione sperimentale efa() di lavaan.\n\n# 1-factor model\nf1 &lt;- '\n  efa(\"efa\")*f1 =~ N1 + N2 + N3 + N4 + E1 + E2 + E3 + E4\n'\n\nAdattiamo il modello ai dati.\n\nefa_f1 &lt;-\n  cfa(\n    model = f1,\n    sample.cov = psychot_cor_mat,\n    sample.nobs = 250,\n    rotation = \"oblimin\"\n  )\n\nConsideriamo ora un modello a due fattori.\n\nf2 &lt;- '\n  efa(\"efa\")*f1 +\n  efa(\"efa\")*f2 =~ N1 + N2 + N3 + N4 + E1 + E2 + E3 + E4\n'\n\nAdattiamo il modello ai dati.\n\nefa_f2 &lt;-\n  cfa(\n    model = f2,\n    sample.cov = psychot_cor_mat,\n    sample.nobs = 250,\n    rotation = \"oblimin\"\n  )\n\nEsaminiamo gli indici di bontà di adattamento.\n\n# define the fit measures\nfit_measures_robust &lt;- c(\"chisq\", \"df\", \"pvalue\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")\n\n# collect them for each model\nrbind(\n  fitmeasures(efa_f1, fit_measures_robust),\n  fitmeasures(efa_f2, fit_measures_robust)\n) %&gt;%\n  # wrangle\n  data.frame() %&gt;%\n  mutate(\n    chisq = round(chisq, digits = 0),\n    df = as.integer(df),\n    pvalue = ifelse(pvalue == 0, \"&lt; .001\", pvalue)\n  ) %&gt;%\n  mutate_at(vars(cfi:srmr), ~ round(., digits = 3)) |&gt; print()\n\n  chisq df            pvalue  cfi   tli rmsea  srmr\n1   375 20            &lt; .001 0.71 0.594 0.267 0.187\n2    10 13 0.709310449320098 1.00 1.006 0.000 0.010\n\n\n\neffectsize::interpret(efa_f1) |&gt; print()\n\n    Name Value Threshold Interpretation\n1    GFI 0.671      0.95           poor\n2   AGFI 0.408      0.90           poor\n3    NFI 0.701      0.90           poor\n4   NNFI 0.594      0.90           poor\n5    CFI 0.710      0.90           poor\n6  RMSEA 0.267      0.05           poor\n7   SRMR 0.187      0.08           poor\n8    RFI 0.581      0.90           poor\n9   PNFI 0.500      0.50   satisfactory\n10   IFI 0.712      0.90           poor\n\n\n\neffectsize::interpret(efa_f2) |&gt; print()\n\n    Name   Value Threshold Interpretation\n1    GFI 0.99055      0.95   satisfactory\n2   AGFI 0.97384      0.90   satisfactory\n3    NFI 0.99217      0.90   satisfactory\n4   NNFI 1.00560      0.90   satisfactory\n5    CFI 1.00000      0.90   satisfactory\n6  RMSEA 0.00000      0.05   satisfactory\n7   SRMR 0.00991      0.08   satisfactory\n8    RFI 0.98315      0.90   satisfactory\n9   PNFI 0.46065      0.50           poor\n10   IFI 1.00257      0.90   satisfactory\n\n\nI risultati mostrano come, in un modello EFA, una soluzione a due fattori produca un adattamento adeguato, mentre ciò non si verifica con un modello ad un solo fattore.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html#specificazione-errata-delle-relazioni-tra-indicatori-e-fattori-latenti",
    "href": "chapters/sem/06_refine_solution.html#specificazione-errata-delle-relazioni-tra-indicatori-e-fattori-latenti",
    "title": "53  La revisione del modello",
    "section": "53.7 Specificazione errata delle relazioni tra indicatori e fattori latenti",
    "text": "53.7 Specificazione errata delle relazioni tra indicatori e fattori latenti\nUn’altra potenziale fonte di errata specificazione del modello CFA è una designazione errata delle relazioni tra indicatori e fattori latenti.\nIn questo esempio, un ricercatore ha sviluppato un questionario di 12 item (gli item sono valutati su scale da 0 a 8) progettato per valutare le motivazioni dei giovani adulti a consumare bevande alcoliche (Cooper, 1994). La misura aveva lo scopo di valutare tre aspetti di questo costrutto (4 item ciascuno): (1) motivazioni di coping (item 1–4), (2) motivazioni sociali (item 5–8) e (3) motivazioni di miglioramento (item 9 –12). I dati sono i seguenti.\n\nsds &lt;- c(2.06, 1.52, 1.92, 1.41, 1.73, 1.77, 2.49, 2.27, 2.68, 1.75, 2.57, 2.66)\n\ncors &lt;- '\n  1.000 \n  0.300  1.000 \n  0.229  0.261  1.000 \n  0.411  0.406  0.429  1.000 \n  0.172  0.252  0.218  0.481  1.000 \n  0.214  0.268  0.267  0.579  0.484  1.000 \n  0.200  0.214  0.241  0.543  0.426  0.492  1.000 \n  0.185  0.230  0.185  0.545  0.463  0.548  0.522  1.000 \n  0.134  0.146  0.108  0.186  0.122  0.131  0.108  0.151  1.000 \n  0.134  0.099  0.061  0.223  0.133  0.188  0.105  0.170  0.448  1.000 \n  0.160  0.131  0.158  0.161  0.044  0.124  0.066  0.061  0.370  0.350  1.000 \n  0.087  0.088  0.101  0.198  0.077  0.177  0.128  0.112  0.356  0.359  0.507  1.000'\n\ncovs &lt;- getCov(cors, sds = sds, names = paste(\"x\", 1:12, sep = \"\"))\n\nIniziamo con un modello che ipotizza tre fattori comuni latenti correlati, coerentemente con la motivazione che stava alla base della costruzione dello strumento.\n\nmodel1 &lt;- '\n  copingm  =~ x1 + x2 + x3 + x4\n  socialm  =~ x5 + x6 + x7 + x8\n  enhancem =~ x9 + x10 + x11 + x12\n'\n\nAdattiamo il modello ai dati.\n\nfit1 &lt;- cfa(\n  model1, \n  sample.cov = covs, \n  sample.nobs = 500, \n  mimic = \"mplus\"\n)\n\nEsaminando le misure di adattamento potremmo concludere che il modello è adeguato.\n\neffectsize::interpret(fit1) |&gt; print()\n\n    Name  Value Threshold Interpretation\n1    GFI 0.9701      0.95   satisfactory\n2   AGFI 0.9472      0.90   satisfactory\n3    NFI 0.9479      0.90   satisfactory\n4   NNFI 0.9710      0.90   satisfactory\n5    CFI 0.9776      0.90   satisfactory\n6  RMSEA 0.0375      0.05   satisfactory\n7   SRMR 0.0344      0.08   satisfactory\n8    RFI 0.9325      0.90   satisfactory\n9   PNFI 0.7324      0.50   satisfactory\n10   IFI 0.9778      0.90   satisfactory\n\n\nTuttavia, un esame più attento mette in evidenza un comportamento anomalo dell’item x4 e alcune caratteristiche anomale del modello in generale.\n\nstandardizedSolution(fit1) |&gt; print()\n\n        lhs op      rhs est.std    se     z pvalue ci.lower ci.upper\n1   copingm =~       x1   0.432 0.039 11.03   0.00    0.355    0.508\n2   copingm =~       x2   0.436 0.039 11.17   0.00    0.359    0.512\n3   copingm =~       x3   0.451 0.038 11.73   0.00    0.376    0.527\n4   copingm =~       x4   0.953 0.024 38.97   0.00    0.905    1.001\n5   socialm =~       x5   0.633 0.032 20.06   0.00    0.571    0.695\n6   socialm =~       x6   0.748 0.025 29.36   0.00    0.698    0.798\n7   socialm =~       x7   0.690 0.029 24.15   0.00    0.634    0.746\n8   socialm =~       x8   0.729 0.026 27.52   0.00    0.677    0.781\n9  enhancem =~       x9   0.602 0.039 15.58   0.00    0.526    0.678\n10 enhancem =~      x10   0.597 0.039 15.40   0.00    0.521    0.673\n11 enhancem =~      x11   0.661 0.037 17.98   0.00    0.589    0.733\n12 enhancem =~      x12   0.665 0.037 18.17   0.00    0.593    0.737\n13       x1 ~~       x1   0.814 0.034 24.09   0.00    0.747    0.880\n14       x2 ~~       x2   0.810 0.034 23.84   0.00    0.744    0.877\n15       x3 ~~       x3   0.796 0.035 22.94   0.00    0.728    0.864\n16       x4 ~~       x4   0.091 0.047  1.96   0.05    0.000    0.183\n17       x5 ~~       x5   0.599 0.040 14.98   0.00    0.521    0.677\n18       x6 ~~       x6   0.441 0.038 11.57   0.00    0.366    0.515\n19       x7 ~~       x7   0.524 0.039 13.29   0.00    0.447    0.601\n20       x8 ~~       x8   0.469 0.039 12.15   0.00    0.393    0.545\n21       x9 ~~       x9   0.638 0.047 13.71   0.00    0.546    0.729\n22      x10 ~~      x10   0.643 0.046 13.88   0.00    0.552    0.734\n23      x11 ~~      x11   0.563 0.049 11.61   0.00    0.468    0.659\n24      x12 ~~      x12   0.558 0.049 11.45   0.00    0.462    0.653\n25  copingm ~~  copingm   1.000 0.000    NA     NA    1.000    1.000\n26  socialm ~~  socialm   1.000 0.000    NA     NA    1.000    1.000\n27 enhancem ~~ enhancem   1.000 0.000    NA     NA    1.000    1.000\n28  copingm ~~  socialm   0.799 0.031 26.15   0.00    0.739    0.859\n29  copingm ~~ enhancem   0.322 0.051  6.34   0.00    0.222    0.422\n30  socialm ~~ enhancem   0.268 0.056  4.82   0.00    0.159    0.377\n31       x1 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n32       x2 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n33       x3 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n34       x4 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n35       x5 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n36       x6 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n37       x7 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n38       x8 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n39       x9 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n40      x10 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n41      x11 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n42      x12 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n43  copingm ~1            0.000 0.000    NA     NA    0.000    0.000\n44  socialm ~1            0.000 0.000    NA     NA    0.000    0.000\n45 enhancem ~1            0.000 0.000    NA     NA    0.000    0.000\n\n\nIn particolare, l’item x4 mostra una saturazione molto forte sul fattore Motivi di coping (.955) ed emerge una correlazione molto alta tra i fattori Motivi di coping e Motivi sociali (.798).\nBrown (2015) suggerisce di esaminare i Modification Indices. Tale esame mostra che il MI associato a x4 è molto alto, 18.916.\n\nmodindices(fit1) |&gt; print()\n\n         lhs op rhs     mi    epc sepc.lv sepc.all sepc.nox\n46   copingm =~  x5  0.030 -0.030  -0.027   -0.015   -0.015\n47   copingm =~  x6  0.484  0.127   0.113    0.064    0.064\n48   copingm =~  x7  0.780  0.220   0.196    0.079    0.079\n49   copingm =~  x8  1.962 -0.323  -0.287   -0.127   -0.127\n50   copingm =~  x9  0.101  0.044   0.039    0.015    0.015\n51   copingm =~ x10  2.016  0.129   0.114    0.065    0.065\n52   copingm =~ x11  1.870 -0.181  -0.161   -0.063   -0.063\n53   copingm =~ x12  0.040 -0.027  -0.024   -0.009   -0.009\n54   socialm =~  x1  6.927 -0.520  -0.569   -0.277   -0.277\n55   socialm =~  x2  0.052 -0.033  -0.036   -0.024   -0.024\n56   socialm =~  x3  2.058 -0.267  -0.292   -0.152   -0.152\n57   socialm =~  x4 18.916  1.300   1.423    1.010    1.010\n58   socialm =~  x9  0.338  0.067   0.073    0.027    0.027\n59   socialm =~ x10  2.884  0.128   0.140    0.080    0.080\n60   socialm =~ x11  4.357 -0.229  -0.251   -0.098   -0.098\n61   socialm =~ x12  0.001  0.004   0.004    0.002    0.002\n62  enhancem =~  x1  1.954  0.093   0.149    0.072    0.072\n63  enhancem =~  x2  0.863  0.045   0.073    0.048    0.048\n64  enhancem =~  x3  0.380  0.038   0.061    0.032    0.032\n65  enhancem =~  x4  3.102 -0.104  -0.168   -0.119   -0.119\n66  enhancem =~  x5  0.596 -0.039  -0.063   -0.036   -0.036\n67  enhancem =~  x6  2.495  0.078   0.125    0.071    0.071\n68  enhancem =~  x7  0.539 -0.052  -0.084   -0.034   -0.034\n69  enhancem =~  x8  0.093 -0.019  -0.031   -0.014   -0.014\n70        x1 ~~  x2 10.299  0.379   0.379    0.149    0.149\n71        x1 ~~  x3  0.986  0.147   0.147    0.046    0.046\n72        x1 ~~  x4  0.016 -0.015  -0.015   -0.019   -0.019\n73        x1 ~~  x5  0.452 -0.080  -0.080   -0.032   -0.032\n74        x1 ~~  x6  0.484 -0.078  -0.078   -0.036   -0.036\n75        x1 ~~  x7  0.290 -0.089  -0.089   -0.027   -0.027\n76        x1 ~~  x8  1.535 -0.181  -0.181   -0.063   -0.063\n77        x1 ~~  x9  0.468  0.133   0.133    0.034    0.034\n78        x1 ~~ x10  0.067  0.033   0.033    0.013    0.013\n79        x1 ~~ x11  4.030  0.364   0.364    0.102    0.102\n80        x1 ~~ x12  1.504 -0.229  -0.229   -0.062   -0.062\n81        x2 ~~  x3  3.508  0.205   0.205    0.088    0.088\n82        x2 ~~  x4  6.780 -0.229  -0.229   -0.393   -0.393\n83        x2 ~~  x5  1.449  0.106   0.106    0.058    0.058\n84        x2 ~~  x6  0.102  0.026   0.026    0.016    0.016\n85        x2 ~~  x7  1.144 -0.130  -0.130   -0.053   -0.053\n86        x2 ~~  x8  0.366 -0.065  -0.065   -0.031   -0.031\n87        x2 ~~  x9  1.877  0.196   0.196    0.067    0.067\n88        x2 ~~ x10  0.434 -0.062  -0.062   -0.032   -0.032\n89        x2 ~~ x11  1.599  0.169   0.169    0.064    0.064\n90        x2 ~~ x12  0.726 -0.117  -0.117   -0.043   -0.043\n91        x3 ~~  x4  0.107 -0.037  -0.037   -0.051   -0.051\n92        x3 ~~  x5  0.024  0.017   0.017    0.008    0.008\n93        x3 ~~  x6  0.211  0.048   0.048    0.024    0.024\n94        x3 ~~  x7  0.009  0.015   0.015    0.005    0.005\n95        x3 ~~  x8  5.281 -0.310  -0.310   -0.117   -0.117\n96        x3 ~~  x9  0.031  0.031   0.031    0.009    0.009\n97        x3 ~~ x10  3.545 -0.221  -0.221   -0.092   -0.092\n98        x3 ~~ x11  5.967  0.408   0.408    0.124    0.124\n99        x3 ~~ x12  0.055 -0.040  -0.040   -0.012   -0.012\n100       x4 ~~  x5  0.063 -0.016  -0.016   -0.028   -0.028\n101       x4 ~~  x6  0.052  0.015   0.015    0.029    0.029\n102       x4 ~~  x7  2.114  0.131   0.131    0.170    0.170\n103       x4 ~~  x8  0.208  0.037   0.037    0.057    0.057\n104       x4 ~~  x9  0.887 -0.091  -0.091   -0.100   -0.100\n105       x4 ~~ x10  1.063  0.065   0.065    0.109    0.109\n106       x4 ~~ x11  2.637 -0.149  -0.149   -0.181   -0.181\n107       x4 ~~ x12  0.169  0.039   0.039    0.046    0.046\n108       x5 ~~  x6  0.370  0.057   0.057    0.036    0.036\n109       x5 ~~  x7  0.292 -0.072  -0.072   -0.030   -0.030\n110       x5 ~~  x8  0.007  0.010   0.010    0.005    0.005\n111       x5 ~~  x9  0.822  0.133   0.133    0.047    0.047\n112       x5 ~~ x10  0.339  0.056   0.056    0.030    0.030\n113       x5 ~~ x11  1.126 -0.145  -0.145   -0.056   -0.056\n114       x5 ~~ x12  1.143 -0.151  -0.151   -0.057   -0.057\n115       x6 ~~  x7  2.528 -0.215  -0.215   -0.101   -0.101\n116       x6 ~~  x8  0.053  0.029   0.029    0.016    0.016\n117       x6 ~~  x9  1.056 -0.141  -0.141   -0.056   -0.056\n118       x6 ~~ x10  0.598  0.069   0.069    0.042    0.042\n119       x6 ~~ x11  0.248  0.064   0.064    0.028    0.028\n120       x6 ~~ x12  1.667  0.170   0.170    0.073    0.073\n121       x7 ~~  x8  1.431  0.206   0.206    0.074    0.074\n122       x7 ~~  x9  0.032 -0.036  -0.036   -0.009   -0.009\n123       x7 ~~ x10  1.521 -0.163  -0.163   -0.065   -0.065\n124       x7 ~~ x11  0.263 -0.097  -0.097   -0.028   -0.028\n125       x7 ~~ x12  0.637  0.156   0.156    0.044    0.044\n126       x8 ~~  x9  1.621  0.227   0.227    0.068    0.068\n127       x8 ~~ x10  1.311  0.134   0.134    0.061    0.061\n128       x8 ~~ x11  2.144 -0.244  -0.244   -0.081   -0.081\n129       x8 ~~ x12  0.591 -0.132  -0.132   -0.043   -0.043\n130       x9 ~~ x10 19.846  0.862   0.862    0.288    0.288\n131       x9 ~~ x11  2.908 -0.518  -0.518   -0.126   -0.126\n132       x9 ~~ x12  7.696 -0.876  -0.876   -0.207   -0.207\n133      x10 ~~ x11  7.331 -0.534  -0.534   -0.197   -0.197\n134      x10 ~~ x12  5.572 -0.484  -0.484   -0.174   -0.174\n135      x11 ~~ x12 26.947  1.711   1.711    0.447    0.447\n\n\nLe considerazioni precedenti, dunque, suggeriscono che il modello potrebbe non avere descritto in maniera adeguata le relazioni tra x4 e i fattori comuni latenti. In base a considerazioni teoriche, supponiamo che abbia senso pensare che x4 saturi non solo sul fattore Motivi di coping ma anche sul fattore di Motivi Sociali. Specifichiamo dunque un nuovo modello nel modo seguente.\n\nmodel2 &lt;- '\n  copingm  =~ x1 + x2 + x3 + x4\n  socialm  =~ x4 + x5 + x6 + x7 + x8\n  enhancem =~ x9 + x10 + x11 + x12\n'\n\nAdattiamo il modello.\n\nfit2 &lt;- cfa(\n  model2, \n  sample.cov = covs, \n  sample.nobs = 500, \n  mimic = \"mplus\"\n)\n\nEsaminiamo gli indici di bontà di adattamento.\n\neffectsize::interpret(fit2) |&gt; print()\n\n    Name  Value Threshold Interpretation\n1    GFI 0.9768      0.95   satisfactory\n2   AGFI 0.9583      0.90   satisfactory\n3    NFI 0.9583      0.90   satisfactory\n4   NNFI 0.9839      0.90   satisfactory\n5    CFI 0.9878      0.90   satisfactory\n6  RMSEA 0.0279      0.05   satisfactory\n7   SRMR 0.0289      0.08   satisfactory\n8    RFI 0.9449      0.90   satisfactory\n9   PNFI 0.7260      0.50   satisfactory\n10   IFI 0.9880      0.90   satisfactory\n\n\nLa bontà di adattamento è migliorata.\nEsaminiamo la soluzione standardizzata. Vediamo ora che sono scomparse le due anomalie trovate in precedenza.\n\nstandardizedSolution(fit2) |&gt; print()\n\n        lhs op      rhs est.std    se     z pvalue ci.lower ci.upper\n1   copingm =~       x1   0.514 0.043 12.03      0    0.430    0.597\n2   copingm =~       x2   0.515 0.043 12.07      0    0.431    0.599\n3   copingm =~       x3   0.516 0.043 12.11      0    0.432    0.600\n4   copingm =~       x4   0.538 0.062  8.66      0    0.416    0.660\n5   socialm =~       x4   0.439 0.061  7.20      0    0.320    0.558\n6   socialm =~       x5   0.632 0.032 20.00      0    0.570    0.694\n7   socialm =~       x6   0.746 0.025 29.28      0    0.696    0.796\n8   socialm =~       x7   0.691 0.028 24.23      0    0.635    0.746\n9   socialm =~       x8   0.731 0.026 27.76      0    0.679    0.782\n10 enhancem =~       x9   0.603 0.039 15.62      0    0.527    0.678\n11 enhancem =~      x10   0.595 0.039 15.31      0    0.519    0.671\n12 enhancem =~      x11   0.665 0.037 18.19      0    0.593    0.737\n13 enhancem =~      x12   0.663 0.037 18.10      0    0.591    0.735\n14       x1 ~~       x1   0.736 0.044 16.79      0    0.650    0.822\n15       x2 ~~       x2   0.735 0.044 16.73      0    0.649    0.821\n16       x3 ~~       x3   0.734 0.044 16.68      0    0.647    0.820\n17       x4 ~~       x4   0.230 0.037  6.29      0    0.158    0.301\n18       x5 ~~       x5   0.601 0.040 15.04      0    0.522    0.679\n19       x6 ~~       x6   0.443 0.038 11.63      0    0.368    0.517\n20       x7 ~~       x7   0.523 0.039 13.29      0    0.446    0.600\n21       x8 ~~       x8   0.466 0.038 12.11      0    0.390    0.541\n22       x9 ~~       x9   0.637 0.046 13.70      0    0.546    0.728\n23      x10 ~~      x10   0.646 0.046 13.99      0    0.556    0.737\n24      x11 ~~      x11   0.558 0.049 11.47      0    0.463    0.653\n25      x12 ~~      x12   0.561 0.049 11.55      0    0.465    0.656\n26  copingm ~~  copingm   1.000 0.000    NA     NA    1.000    1.000\n27  socialm ~~  socialm   1.000 0.000    NA     NA    1.000    1.000\n28 enhancem ~~ enhancem   1.000 0.000    NA     NA    1.000    1.000\n29  copingm ~~  socialm   0.610 0.057 10.74      0    0.498    0.721\n30  copingm ~~ enhancem   0.350 0.059  5.96      0    0.235    0.465\n31  socialm ~~ enhancem   0.265 0.055  4.79      0    0.156    0.373\n32       x1 ~1            0.000 0.045  0.00      1   -0.088    0.088\n33       x2 ~1            0.000 0.045  0.00      1   -0.088    0.088\n34       x3 ~1            0.000 0.045  0.00      1   -0.088    0.088\n35       x4 ~1            0.000 0.045  0.00      1   -0.088    0.088\n36       x5 ~1            0.000 0.045  0.00      1   -0.088    0.088\n37       x6 ~1            0.000 0.045  0.00      1   -0.088    0.088\n38       x7 ~1            0.000 0.045  0.00      1   -0.088    0.088\n39       x8 ~1            0.000 0.045  0.00      1   -0.088    0.088\n40       x9 ~1            0.000 0.045  0.00      1   -0.088    0.088\n41      x10 ~1            0.000 0.045  0.00      1   -0.088    0.088\n42      x11 ~1            0.000 0.045  0.00      1   -0.088    0.088\n43      x12 ~1            0.000 0.045  0.00      1   -0.088    0.088\n44  copingm ~1            0.000 0.000    NA     NA    0.000    0.000\n45  socialm ~1            0.000 0.000    NA     NA    0.000    0.000\n46 enhancem ~1            0.000 0.000    NA     NA    0.000    0.000\n\n\nEsaminando i MI, notiamo che il modello potrebbe migliorare se introduciamo una correlazione tra le specificità x11 e x12.\n\nmodindices(fit2) |&gt; print()\n\n         lhs op rhs     mi    epc sepc.lv sepc.all sepc.nox\n47   copingm =~  x5  0.076  0.032   0.034    0.020    0.020\n48   copingm =~  x6  1.413  0.143   0.151    0.086    0.086\n49   copingm =~  x7  0.245  0.083   0.088    0.035    0.035\n50   copingm =~  x8  3.668 -0.295  -0.311   -0.137   -0.137\n51   copingm =~  x9  0.243  0.066   0.069    0.026    0.026\n52   copingm =~ x10  0.566  0.065   0.069    0.040    0.040\n53   copingm =~ x11  0.119 -0.044  -0.046   -0.018   -0.018\n54   copingm =~ x12  0.598 -0.102  -0.108   -0.041   -0.041\n55   socialm =~  x1  1.948 -0.396  -0.245   -0.119   -0.119\n56   socialm =~  x2  0.718  0.177   0.110    0.072    0.072\n57   socialm =~  x3  0.298  0.144   0.089    0.047    0.047\n58   socialm =~  x9  0.316  0.114   0.071    0.026    0.026\n59   socialm =~ x10  3.169  0.236   0.146    0.084    0.084\n60   socialm =~ x11  4.927 -0.430  -0.266   -0.104   -0.104\n61   socialm =~ x12  0.017  0.026   0.016    0.006    0.006\n62  enhancem =~  x1  0.314  0.040   0.064    0.031    0.031\n63  enhancem =~  x2  0.003  0.003   0.004    0.003    0.003\n64  enhancem =~  x3  0.037 -0.013  -0.020   -0.011   -0.011\n65  enhancem =~  x4  0.106 -0.013  -0.021   -0.015   -0.015\n66  enhancem =~  x5  0.464 -0.034  -0.055   -0.032   -0.032\n67  enhancem =~  x6  2.703  0.079   0.128    0.072    0.072\n68  enhancem =~  x7  0.467 -0.048  -0.077   -0.031   -0.031\n69  enhancem =~  x8  0.095 -0.019  -0.031   -0.014   -0.014\n70        x1 ~~  x2  1.966  0.187   0.187    0.081    0.081\n71        x1 ~~  x3  2.042 -0.241  -0.241   -0.083   -0.083\n72        x1 ~~  x4  0.775  0.098   0.098    0.082    0.082\n73        x1 ~~  x5  0.238 -0.058  -0.058   -0.024   -0.024\n74        x1 ~~  x6  0.187 -0.048  -0.048   -0.023   -0.023\n75        x1 ~~  x7  0.019 -0.022  -0.022   -0.007   -0.007\n76        x1 ~~  x8  0.366 -0.087  -0.087   -0.032   -0.032\n77        x1 ~~  x9  0.155  0.076   0.076    0.020    0.020\n78        x1 ~~ x10  0.104  0.041   0.041    0.016    0.016\n79        x1 ~~ x11  2.019  0.255   0.255    0.075    0.075\n80        x1 ~~ x12  1.911 -0.257  -0.257   -0.073   -0.073\n81        x2 ~~  x3  0.035 -0.023  -0.023   -0.011   -0.011\n82        x2 ~~  x4  3.029 -0.144  -0.144   -0.163   -0.163\n83        x2 ~~  x5  2.503  0.138   0.138    0.079    0.079\n84        x2 ~~  x6  0.509  0.058   0.058    0.038    0.038\n85        x2 ~~  x7  0.471 -0.082  -0.082   -0.035   -0.035\n86        x2 ~~  x8  0.015  0.013   0.013    0.006    0.006\n87        x2 ~~  x9  1.289  0.161   0.161    0.058    0.058\n88        x2 ~~ x10  0.467 -0.064  -0.064   -0.035   -0.035\n89        x2 ~~ x11  0.338  0.077   0.077    0.031    0.031\n90        x2 ~~ x12  0.970 -0.135  -0.135   -0.052   -0.052\n91        x3 ~~  x4  1.095  0.109   0.109    0.098    0.098\n92        x3 ~~  x5  0.169  0.045   0.045    0.021    0.021\n93        x3 ~~  x6  0.681  0.085   0.085    0.044    0.044\n94        x3 ~~  x7  0.315  0.085   0.085    0.029    0.029\n95        x3 ~~  x8  3.075 -0.235  -0.235   -0.092   -0.092\n96        x3 ~~  x9  0.022 -0.026  -0.026   -0.008   -0.008\n97        x3 ~~ x10  3.825 -0.230  -0.230   -0.100   -0.100\n98        x3 ~~ x11  3.498  0.313   0.313    0.099    0.099\n99        x3 ~~ x12  0.079 -0.049  -0.049   -0.015   -0.015\n100       x4 ~~  x5  0.337 -0.037  -0.037   -0.041   -0.041\n101       x4 ~~  x6  0.033 -0.012  -0.012   -0.015   -0.015\n102       x4 ~~  x7  1.053  0.094   0.094    0.077    0.077\n103       x4 ~~  x8  0.071 -0.022  -0.022   -0.021   -0.021\n104       x4 ~~  x9  0.541 -0.070  -0.070   -0.048   -0.048\n105       x4 ~~ x10  1.128  0.066   0.066    0.070    0.070\n106       x4 ~~ x11  1.313 -0.102  -0.102   -0.079   -0.079\n107       x4 ~~ x12  0.322  0.052   0.052    0.039    0.039\n108       x5 ~~  x6  0.504  0.066   0.066    0.042    0.042\n109       x5 ~~  x7  0.262 -0.068  -0.068   -0.028   -0.028\n110       x5 ~~  x8  0.004  0.008   0.008    0.004    0.004\n111       x5 ~~  x9  0.850  0.135   0.135    0.047    0.047\n112       x5 ~~ x10  0.288  0.052   0.052    0.027    0.027\n113       x5 ~~ x11  1.019 -0.138  -0.138   -0.054   -0.054\n114       x5 ~~ x12  1.224 -0.157  -0.157   -0.059   -0.059\n115       x6 ~~  x7  2.404 -0.209  -0.209   -0.099   -0.099\n116       x6 ~~  x8  0.034  0.023   0.023    0.012    0.012\n117       x6 ~~  x9  0.978 -0.135  -0.135   -0.054   -0.054\n118       x6 ~~ x10  0.524  0.065   0.065    0.039    0.039\n119       x6 ~~ x11  0.341  0.074   0.074    0.033    0.033\n120       x6 ~~ x12  1.520  0.163   0.163    0.069    0.069\n121       x7 ~~  x8  1.171  0.186   0.186    0.067    0.067\n122       x7 ~~  x9  0.020 -0.028  -0.028   -0.007   -0.007\n123       x7 ~~ x10  1.593 -0.167  -0.167   -0.066   -0.066\n124       x7 ~~ x11  0.175 -0.079  -0.079   -0.023   -0.023\n125       x7 ~~ x12  0.586  0.149   0.149    0.042    0.042\n126       x8 ~~  x9  1.808  0.239   0.239    0.072    0.072\n127       x8 ~~ x10  1.267  0.131   0.131    0.060    0.060\n128       x8 ~~ x11  1.791 -0.222  -0.222   -0.075   -0.075\n129       x8 ~~ x12  0.595 -0.132  -0.132   -0.043   -0.043\n130       x9 ~~ x10 20.103  0.864   0.864    0.288    0.288\n131       x9 ~~ x11  3.658 -0.582  -0.582   -0.142   -0.142\n132       x9 ~~ x12  7.229 -0.845  -0.845   -0.199   -0.199\n133      x10 ~~ x11  7.617 -0.543  -0.543   -0.201   -0.201\n134      x10 ~~ x12  4.512 -0.431  -0.431   -0.154   -0.154\n135      x11 ~~ x12 26.071  1.680   1.680    0.440    0.440\n\n\nIl nuovo modello diventa dunque il seguente.\n\nmodel3 &lt;- '\n  copingm  =~ x1 + x2 + x3 + x4\n  socialm  =~ x4 + x5 + x6 + x7 + x8\n  enhancem =~ x9 + x10 + x11 + x12\n  x11 ~~ x12\n'\n\nAdattiamo il modello.\n\nfit3 &lt;- cfa(\n  model3, \n  sample.cov = covs, \n  sample.nobs = 500, \n  mimic = \"mplus\"\n)\n\nUn test basato sul rapporto di verosimiglianze conferma che il miglioramento di adattamento è sostanziale.\n\nlavTestLRT(fit2, fit3) |&gt; print()\n\n\nChi-Squared Difference Test\n\n     Df   AIC   BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)    \nfit3 49 23934 24107  45.0                                        \nfit2 50 23957 24125  69.4       24.5 0.217       1    7.5e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nEsaminiamo gli indici di bontà di adattamento.\n\nsummary(fit3, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 61 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        41\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                                44.955\n  Degrees of freedom                                49\n  P-value (Chi-square)                           0.638\n\nModel Test Baseline Model:\n\n  Test statistic                              1664.026\n  Degrees of freedom                                66\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.003\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -11926.170\n  Loglikelihood unrestricted model (H1)     -11903.692\n                                                      \n  Akaike (AIC)                               23934.339\n  Bayesian (BIC)                             24107.138\n  Sample-size adjusted Bayesian (SABIC)      23977.002\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.025\n  P-value H_0: RMSEA &lt;= 0.050                    1.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.023\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  copingm =~                                          \n    x1                1.000                           \n    x2                0.740    0.094    7.909    0.000\n    x3                0.933    0.118    7.903    0.000\n    x4                0.719    0.118    6.070    0.000\n  socialm =~                                          \n    x4                1.000                           \n    x5                1.771    0.273    6.485    0.000\n    x6                2.141    0.319    6.703    0.000\n    x7                2.784    0.421    6.611    0.000\n    x8                2.689    0.402    6.681    0.000\n  enhancem =~                                         \n    x9                1.000                           \n    x10               0.648    0.070    9.293    0.000\n    x11               0.776    0.093    8.340    0.000\n    x12               0.802    0.096    8.327    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n .x11 ~~                                              \n   .x12               1.460    0.300    4.873    0.000\n  copingm ~~                                          \n    socialm           0.398    0.071    5.603    0.000\n    enhancem          0.669    0.145    4.613    0.000\n  socialm ~~                                          \n    enhancem          0.320    0.084    3.783    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                0.000    0.092    0.000    1.000\n   .x2                0.000    0.068    0.000    1.000\n   .x3                0.000    0.086    0.000    1.000\n   .x4                0.000    0.063    0.000    1.000\n   .x5                0.000    0.077    0.000    1.000\n   .x6                0.000    0.079    0.000    1.000\n   .x7                0.000    0.111    0.000    1.000\n   .x8                0.000    0.101    0.000    1.000\n   .x9                0.000    0.120    0.000    1.000\n   .x10               0.000    0.078    0.000    1.000\n   .x11               0.000    0.115    0.000    1.000\n   .x12               0.000    0.119    0.000    1.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                3.117    0.230   13.546    0.000\n   .x2                1.694    0.125   13.527    0.000\n   .x3                2.705    0.200   13.536    0.000\n   .x4                0.454    0.070    6.502    0.000\n   .x5                1.794    0.130   13.835    0.000\n   .x6                1.384    0.115   12.015    0.000\n   .x7                3.240    0.248   13.089    0.000\n   .x8                2.393    0.194   12.352    0.000\n   .x9                3.958    0.400    9.895    0.000\n   .x10               1.710    0.170   10.063    0.000\n   .x11               4.657    0.371   12.545    0.000\n   .x12               4.997    0.398   12.561    0.000\n    copingm           1.118    0.217    5.158    0.000\n    socialm           0.380    0.110    3.469    0.001\n    enhancem          3.210    0.490    6.550    0.000\n\n\n\nGli indici di fit sono migliorati.\nEsaminiamo la soluzione standardizzata.\n\nstandardizedSolution(fit3) |&gt; print()\n\n        lhs op      rhs est.std    se     z pvalue ci.lower ci.upper\n1   copingm =~       x1   0.514 0.043 12.02      0    0.430    0.598\n2   copingm =~       x2   0.515 0.043 12.05      0    0.431    0.599\n3   copingm =~       x3   0.514 0.043 12.04      0    0.431    0.598\n4   copingm =~       x4   0.540 0.063  8.61      0    0.417    0.663\n5   socialm =~       x4   0.438 0.061  7.13      0    0.317    0.558\n6   socialm =~       x5   0.632 0.032 20.00      0    0.570    0.694\n7   socialm =~       x6   0.746 0.025 29.29      0    0.697    0.796\n8   socialm =~       x7   0.690 0.029 24.21      0    0.634    0.746\n9   socialm =~       x8   0.731 0.026 27.80      0    0.680    0.783\n10 enhancem =~       x9   0.669 0.041 16.39      0    0.589    0.749\n11 enhancem =~      x10   0.664 0.041 16.24      0    0.584    0.744\n12 enhancem =~      x11   0.542 0.045 12.12      0    0.454    0.629\n13 enhancem =~      x12   0.541 0.045 12.08      0    0.453    0.628\n14      x11 ~~      x12   0.303 0.050  6.10      0    0.205    0.400\n15       x1 ~~       x1   0.736 0.044 16.76      0    0.650    0.822\n16       x2 ~~       x2   0.735 0.044 16.70      0    0.649    0.821\n17       x3 ~~       x3   0.735 0.044 16.73      0    0.649    0.822\n18       x4 ~~       x4   0.229 0.037  6.22      0    0.157    0.301\n19       x5 ~~       x5   0.601 0.040 15.04      0    0.522    0.679\n20       x6 ~~       x6   0.443 0.038 11.64      0    0.368    0.517\n21       x7 ~~       x7   0.524 0.039 13.31      0    0.447    0.601\n22       x8 ~~       x8   0.465 0.038 12.10      0    0.390    0.541\n23       x9 ~~       x9   0.552 0.055 10.10      0    0.445    0.659\n24      x10 ~~      x10   0.559 0.054 10.31      0    0.453    0.666\n25      x11 ~~      x11   0.706 0.048 14.58      0    0.611    0.801\n26      x12 ~~      x12   0.708 0.048 14.62      0    0.613    0.802\n27  copingm ~~  copingm   1.000 0.000    NA     NA    1.000    1.000\n28  socialm ~~  socialm   1.000 0.000    NA     NA    1.000    1.000\n29 enhancem ~~ enhancem   1.000 0.000    NA     NA    1.000    1.000\n30  copingm ~~  socialm   0.610 0.057 10.73      0    0.499    0.721\n31  copingm ~~ enhancem   0.353 0.060  5.84      0    0.235    0.472\n32  socialm ~~ enhancem   0.289 0.056  5.14      0    0.179    0.399\n33       x1 ~1            0.000 0.045  0.00      1   -0.088    0.088\n34       x2 ~1            0.000 0.045  0.00      1   -0.088    0.088\n35       x3 ~1            0.000 0.045  0.00      1   -0.088    0.088\n36       x4 ~1            0.000 0.045  0.00      1   -0.088    0.088\n37       x5 ~1            0.000 0.045  0.00      1   -0.088    0.088\n38       x6 ~1            0.000 0.045  0.00      1   -0.088    0.088\n39       x7 ~1            0.000 0.045  0.00      1   -0.088    0.088\n40       x8 ~1            0.000 0.045  0.00      1   -0.088    0.088\n41       x9 ~1            0.000 0.045  0.00      1   -0.088    0.088\n42      x10 ~1            0.000 0.045  0.00      1   -0.088    0.088\n43      x11 ~1            0.000 0.045  0.00      1   -0.088    0.088\n44      x12 ~1            0.000 0.045  0.00      1   -0.088    0.088\n45  copingm ~1            0.000 0.000    NA     NA    0.000    0.000\n46  socialm ~1            0.000 0.000    NA     NA    0.000    0.000\n47 enhancem ~1            0.000 0.000    NA     NA    0.000    0.000\n\n\nNon ci sono ulteriori motivi di preoccupazione. Brown (2015) conclude che il modello più adeguato sia model3.\nNel caso presente, a mio parare, l’introduzione della correlazione residua tra x11 e x12 si sarebbe anche potuta evitare, dato che il modello model3 (con meno idiosincrasie legate al campione) si era già dimostrato adeguato.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html#saturazione-sul-fattore-sbagliato",
    "href": "chapters/sem/06_refine_solution.html#saturazione-sul-fattore-sbagliato",
    "title": "53  La revisione del modello",
    "section": "53.8 Saturazione sul fattore sbagliato",
    "text": "53.8 Saturazione sul fattore sbagliato\nBrown (2015) considera anche il caso opposto, ovvero quello nel quale il ricercatore ipotizza una saturazione spuria. Per i dati in discussione, si può avere la situazione presente.\n\nmodel4 &lt;- '\n  copingm  =~ x1 + x2 + x3 + x4\n  socialm  =~ x4 +x5 + x6 + x7 + x8 + x12\n  enhancem =~ x9 + x10 + x11\n'\n\nAdattiamo il modello ai dati.\n\nfit4 &lt;- cfa(\n  model4, \n  sample.cov = covs, \n  sample.nobs = 500, \n  mimic = \"mplus\"\n)\n\nEsaminiamo la soluzione ottenuta.\n\nsummary(fit4, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 59 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        40\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                               212.717\n  Degrees of freedom                                50\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              1664.026\n  Degrees of freedom                                66\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.898\n  Tucker-Lewis Index (TLI)                       0.866\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -12010.051\n  Loglikelihood unrestricted model (H1)     -11903.692\n                                                      \n  Akaike (AIC)                               24100.101\n  Bayesian (BIC)                             24268.685\n  Sample-size adjusted Bayesian (SABIC)      24141.723\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.081\n  90 Percent confidence interval - lower         0.070\n  90 Percent confidence interval - upper         0.092\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.554\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.073\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  copingm =~                                          \n    x1                1.000                           \n    x2                0.741    0.093    7.925    0.000\n    x3                0.932    0.118    7.906    0.000\n    x4                0.699    0.117    5.995    0.000\n  socialm =~                                          \n    x4                1.000                           \n    x5                1.725    0.260    6.634    0.000\n    x6                2.098    0.305    6.879    0.000\n    x7                2.717    0.401    6.775    0.000\n    x8                2.619    0.382    6.848    0.000\n    x12               0.900    0.236    3.818    0.000\n  enhancem =~                                         \n    x9                1.000                           \n    x10               0.638    0.076    8.408    0.000\n    x11               0.767    0.094    8.153    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  copingm ~~                                          \n    socialm           0.410    0.072    5.663    0.000\n    enhancem          0.661    0.148    4.456    0.000\n  socialm ~~                                          \n    enhancem          0.347    0.089    3.902    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                0.000    0.092    0.000    1.000\n   .x2                0.000    0.068    0.000    1.000\n   .x3                0.000    0.086    0.000    1.000\n   .x4                0.000    0.063    0.000    1.000\n   .x5                0.000    0.077    0.000    1.000\n   .x6                0.000    0.079    0.000    1.000\n   .x7                0.000    0.111    0.000    1.000\n   .x8                0.000    0.101    0.000    1.000\n   .x12               0.000    0.119    0.000    1.000\n   .x9                0.000    0.120    0.000    1.000\n   .x10               0.000    0.078    0.000    1.000\n   .x11               0.000    0.115    0.000    1.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                3.106    0.230   13.478    0.000\n   .x2                1.686    0.125   13.449    0.000\n   .x3                2.698    0.200   13.477    0.000\n   .x4                0.463    0.069    6.719    0.000\n   .x5                1.805    0.130   13.886    0.000\n   .x6                1.378    0.115   12.022    0.000\n   .x7                3.255    0.248   13.143    0.000\n   .x8                2.418    0.194   12.455    0.000\n   .x12               6.740    0.430   15.673    0.000\n   .x9                3.891    0.436    8.933    0.000\n   .x10               1.724    0.183    9.435    0.000\n   .x11               4.662    0.371   12.579    0.000\n    copingm           1.129    0.218    5.170    0.000\n    socialm           0.397    0.111    3.566    0.000\n    enhancem          3.277    0.524    6.258    0.000\n\n\n\nÈ chiaro che il modello model4 è inadeguato. Il problema emerge chiaramente anche esaminando i MI.\n\nmodindices(fit4) |&gt; print()\n\n         lhs op rhs      mi    epc sepc.lv sepc.all sepc.nox\n47   copingm =~  x5   0.090  0.036   0.038    0.022    0.022\n48   copingm =~  x6   0.554  0.090   0.096    0.054    0.054\n49   copingm =~  x7   0.107  0.055   0.059    0.024    0.024\n50   copingm =~  x8   3.919 -0.306  -0.325   -0.143   -0.143\n51   copingm =~ x12   6.109  0.499   0.530    0.199    0.199\n52   copingm =~  x9   0.390 -0.096  -0.102   -0.038   -0.038\n53   copingm =~ x10   0.027 -0.016  -0.017   -0.010   -0.010\n54   copingm =~ x11   0.823  0.123   0.131    0.051    0.051\n55   socialm =~  x1   1.990 -0.398  -0.251   -0.122   -0.122\n56   socialm =~  x2   0.638  0.166   0.105    0.069    0.069\n57   socialm =~  x3   0.372  0.160   0.101    0.053    0.053\n58   socialm =~  x9   0.315 -0.130  -0.082   -0.031   -0.031\n59   socialm =~ x10   1.423  0.179   0.113    0.064    0.064\n60   socialm =~ x11   0.520 -0.150  -0.094   -0.037   -0.037\n61  enhancem =~  x1   1.029  0.067   0.121    0.059    0.059\n62  enhancem =~  x2   0.232  0.023   0.042    0.028    0.028\n63  enhancem =~  x3   0.153 -0.024  -0.043   -0.023   -0.023\n64  enhancem =~  x4   0.745 -0.031  -0.056   -0.040   -0.040\n65  enhancem =~  x5   0.343 -0.028  -0.050   -0.029   -0.029\n66  enhancem =~  x6   0.103  0.015   0.027    0.015    0.015\n67  enhancem =~  x7   2.752 -0.110  -0.198   -0.080   -0.080\n68  enhancem =~  x8   0.129 -0.021  -0.038   -0.017   -0.017\n69  enhancem =~ x12 116.781  0.916   1.658    0.624    0.624\n70        x1 ~~  x2   1.709  0.177   0.177    0.077    0.077\n71        x1 ~~  x3   2.273 -0.257  -0.257   -0.089   -0.089\n72        x1 ~~  x4   0.850  0.103   0.103    0.086    0.086\n73        x1 ~~  x5   0.292 -0.064  -0.064   -0.027   -0.027\n74        x1 ~~  x6   0.188 -0.048  -0.048   -0.023   -0.023\n75        x1 ~~  x7   0.023 -0.025  -0.025   -0.008   -0.008\n76        x1 ~~  x8   0.419 -0.093  -0.093   -0.034   -0.034\n77        x1 ~~ x12   0.025 -0.034  -0.034   -0.007   -0.007\n78        x1 ~~  x9   0.011  0.020   0.020    0.006    0.006\n79        x1 ~~ x10   0.004  0.008   0.008    0.003    0.003\n80        x1 ~~ x11   1.804  0.259   0.259    0.068    0.068\n81        x2 ~~  x3   0.071 -0.034  -0.034   -0.016   -0.016\n82        x2 ~~  x4   2.979 -0.143  -0.143   -0.162   -0.162\n83        x2 ~~  x5   2.403  0.135   0.135    0.077    0.077\n84        x2 ~~  x6   0.551  0.060   0.060    0.040    0.040\n85        x2 ~~  x7   0.457 -0.081  -0.081   -0.035   -0.035\n86        x2 ~~  x8   0.012  0.011   0.011    0.006    0.006\n87        x2 ~~ x12   0.134 -0.058  -0.058   -0.017   -0.017\n88        x2 ~~  x9   1.033  0.145   0.145    0.056    0.056\n89        x2 ~~ x10   1.140 -0.100  -0.100   -0.058   -0.058\n90        x2 ~~ x11   0.323  0.081   0.081    0.029    0.029\n91        x3 ~~  x4   1.472  0.127   0.127    0.113    0.113\n92        x3 ~~  x5   0.140  0.041   0.041    0.019    0.019\n93        x3 ~~  x6   0.717  0.087   0.087    0.045    0.045\n94        x3 ~~  x7   0.317  0.086   0.086    0.029    0.029\n95        x3 ~~  x8   3.121 -0.237  -0.237   -0.093   -0.093\n96        x3 ~~ x12   0.001  0.006   0.006    0.001    0.001\n97        x3 ~~  x9   0.000  0.003   0.003    0.001    0.001\n98        x3 ~~ x10   4.165 -0.241  -0.241   -0.111   -0.111\n99        x3 ~~ x11   3.806  0.350   0.350    0.099    0.099\n100       x4 ~~  x5   0.316 -0.036  -0.036   -0.039   -0.039\n101       x4 ~~  x6   0.052 -0.015  -0.015   -0.019   -0.019\n102       x4 ~~  x7   1.182  0.099   0.099    0.081    0.081\n103       x4 ~~  x8   0.062 -0.021  -0.021   -0.020   -0.020\n104       x4 ~~ x12   0.033  0.020   0.020    0.011    0.011\n105       x4 ~~  x9   1.418 -0.115  -0.115   -0.086   -0.086\n106       x4 ~~ x10   0.914  0.061   0.061    0.068    0.068\n107       x4 ~~ x11   0.517 -0.068  -0.068   -0.047   -0.047\n108       x5 ~~  x6   0.611  0.073   0.073    0.046    0.046\n109       x5 ~~  x7   0.115 -0.045  -0.045   -0.019   -0.019\n110       x5 ~~  x8   0.079  0.034   0.034    0.016    0.016\n111       x5 ~~ x12   3.265 -0.302  -0.302   -0.087   -0.087\n112       x5 ~~  x9   0.203  0.066   0.066    0.025    0.025\n113       x5 ~~ x10   0.000  0.002   0.002    0.001    0.001\n114       x5 ~~ x11   2.312 -0.224  -0.224   -0.077   -0.077\n115       x6 ~~  x7   2.239 -0.200  -0.200   -0.094   -0.094\n116       x6 ~~  x8   0.073  0.033   0.033    0.018    0.018\n117       x6 ~~ x12   0.478  0.109   0.109    0.036    0.036\n118       x6 ~~  x9   1.251 -0.153  -0.153   -0.066   -0.066\n119       x6 ~~ x10   0.784  0.079   0.079    0.051    0.051\n120       x6 ~~ x11   0.370  0.083   0.083    0.033    0.033\n121       x7 ~~  x8   1.644  0.219   0.219    0.078    0.078\n122       x7 ~~ x12   0.433 -0.152  -0.152   -0.032   -0.032\n123       x7 ~~  x9   0.005 -0.015  -0.015   -0.004   -0.004\n124       x7 ~~ x10   1.836 -0.179  -0.179   -0.076   -0.076\n125       x7 ~~ x11   0.348 -0.119  -0.119   -0.031   -0.031\n126       x8 ~~ x12   2.680 -0.335  -0.335   -0.083   -0.083\n127       x8 ~~  x9   0.676  0.147   0.147    0.048    0.048\n128       x8 ~~ x10   0.337  0.068   0.068    0.033    0.033\n129       x8 ~~ x11   3.437 -0.330  -0.330   -0.098   -0.098\n130      x12 ~~  x9   7.051  0.713   0.713    0.139    0.139\n131      x12 ~~ x10   6.960  0.465   0.465    0.136    0.136\n132      x12 ~~ x11  68.717  2.238   2.238    0.399    0.399\n133       x9 ~~ x10   0.081  0.138   0.138    0.053    0.053\n134       x9 ~~ x11   0.166  0.209   0.209    0.049    0.049\n135      x10 ~~ x11   0.423 -0.211  -0.211   -0.075   -0.075\n\n\nIl MI relativo alla saturazione di x12 su enhancem è uguale a 116.781. Chiaramente, in una revisione del modello, questo problema dovrebbe essere affrontato.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html#riflessioni-conclusive",
    "href": "chapters/sem/06_refine_solution.html#riflessioni-conclusive",
    "title": "53  La revisione del modello",
    "section": "53.9 Riflessioni Conclusive",
    "text": "53.9 Riflessioni Conclusive\nGli esempi presentati da Brown (2015) mostrano come l’applicazione dei MI, combinata con l’esame delle soluzioni fattoriali, rappresenti un approccio utile per ottimizzare e perfezionare il modello proposto.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/sem/06_refine_solution.html#informazioni-sullambiente-di-sviluppo",
    "title": "53  La revisione del modello",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] effectsize_0.8.9  MASS_7.3-61       viridis_0.6.5     viridisLite_0.4.2\n [5] ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0  \n [9] bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-19    \n[13] psych_2.4.6.26    scales_1.3.0      markdown_1.13     knitr_1.49       \n[17] lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n[21] purrr_1.0.2       readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n[25] ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.17.1   jsonlite_1.8.9      datawizard_0.13.0  \n  [4] magrittr_2.0.3      TH.data_1.1-2       estimability_1.5.1 \n  [7] farver_2.1.2        nloptr_2.1.1        rmarkdown_2.29     \n [10] vctrs_0.6.5         minqa_1.2.8         base64enc_0.1-3    \n [13] rstatix_0.7.2       htmltools_0.5.8.1   broom_1.0.7        \n [16] Formula_1.2-5       htmlwidgets_1.6.4   plyr_1.8.9         \n [19] sandwich_3.1-1      emmeans_1.10.5      zoo_1.8-12         \n [22] uuid_1.2-1          igraph_2.1.1        mime_0.12          \n [25] lifecycle_1.0.4     pkgconfig_2.0.3     Matrix_1.7-1       \n [28] R6_2.5.1            fastmap_1.2.0       shiny_1.9.1        \n [31] numDeriv_2016.8-1.1 digest_0.6.37       OpenMx_2.21.13     \n [34] fdrtool_1.2.18      colorspace_2.1-1    rprojroot_2.0.4    \n [37] Hmisc_5.2-0         fansi_1.0.6         timechange_0.3.0   \n [40] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n [43] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n [46] carData_3.0-5       performance_0.12.4  ggsignif_0.6.4     \n [49] corpcor_1.6.10      gtools_3.9.5        tools_4.4.2        \n [52] pbivnorm_0.6.0      foreign_0.8-87      zip_2.3.1          \n [55] httpuv_1.6.15       nnet_7.3-19         glue_1.8.0         \n [58] quadprog_1.5-8      promises_1.3.0      nlme_3.1-166       \n [61] lisrelToR_0.3       grid_4.4.2          pbdZMQ_0.3-13      \n [64] checkmate_2.3.2     cluster_2.1.6       reshape2_1.4.4     \n [67] generics_0.1.3      gtable_0.3.6        tzdb_0.4.0         \n [70] data.table_1.16.2   hms_1.1.3           car_3.1-3          \n [73] utf8_1.2.4          sem_3.1-16          pillar_1.9.0       \n [76] IRdisplay_1.1       rockchalk_1.8.157   later_1.3.2        \n [79] splines_4.4.2       cherryblossom_0.1.0 lattice_0.22-6     \n [82] survival_3.7-0      kutils_1.73         tidyselect_1.2.1   \n [85] miniUI_0.1.1.1      pbapply_1.7-2       airports_0.1.0     \n [88] stats4_4.4.2        xfun_0.49           qgraph_1.9.8       \n [91] arm_1.14-4          stringi_1.8.4       pacman_0.5.1       \n [94] boot_1.3-31         evaluate_1.0.1      codetools_0.2-20   \n [97] mi_1.1              cli_3.6.3           RcppParallel_5.1.9 \n[100] IRkernel_1.3.2      rpart_4.1.23        parameters_0.23.0  \n[103] xtable_1.8-4        repr_1.1.7          munsell_0.5.1      \n[106] Rcpp_1.0.13-1       coda_0.19-4.1       png_0.1-8          \n[109] XML_3.99-0.17       parallel_4.4.2      usdata_0.3.1       \n[112] bayestestR_0.15.0   jpeg_0.1-10         lme4_1.1-35.5      \n[115] mvtnorm_1.3-2       insight_0.20.5      openxlsx_4.2.7.1   \n[118] crayon_1.5.3        openintro_2.5.0     rlang_1.1.4        \n[121] multcomp_1.4-26     mnormt_2.1.1       \n\n\n\n\n\n\nBrown, T. A. (2015). Confirmatory factor analysis for applied research. Guilford publications.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/07_group_invariance.html",
    "href": "chapters/sem/07_group_invariance.html",
    "title": "54  Invarianza di misura",
    "section": "",
    "text": "54.1 Introduzione\nNel campo della psicologia, non è sufficiente ottenere una buona misura delle variabili latenti. Molte ricerche mirano a rispondere a domande riguardanti relazioni o differenze tra più costrutti latenti. Per rispondere a queste domande, è fondamentale verificare preliminarmente l’invarianza di misura. Tale verifica garantisce che il modello fattoriale sottostante uno strumento sia applicabile in modo equivalente ai sottogruppi rilevanti del campione.\nL’invarianza di misura implica che, se si analizzano separatamente i sottogruppi con un’analisi fattoriale, si ottiene lo stesso modello in ognuno di essi. Questo aspetto è cruciale poiché l’interpretazione delle variabili latenti dipende dal modello fattoriale. Se il modello differisce, ad esempio, tra maschi e femmine, un’analisi combinata di entrambi i gruppi risulterebbe confusa e poco interpretabile. In altre parole, per confrontare gruppi, è necessario che il modello fattoriale abbia lo stesso numero di fattori e lo stesso schema di relazioni tra fattori e variabili osservate per tutti i gruppi. Questo livello minimo è chiamato invarianza configurale.\nTuttavia, molte domande di ricerca richiedono livelli più elevati di invarianza, in cui non solo la struttura del modello è uguale tra i gruppi, ma anche i valori di alcuni parametri, come carichi fattoriali e intercette. Solo dopo aver verificato l’invarianza di misura, è possibile confrontare in modo attendibile le medie o le relazioni tra costrutti nei sottogruppi.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Invarianza di misura</span>"
    ]
  },
  {
    "objectID": "chapters/sem/07_group_invariance.html#introduzione",
    "href": "chapters/sem/07_group_invariance.html#introduzione",
    "title": "54  Invarianza di misura",
    "section": "",
    "text": "54.1.1 Livelli di Invarianza Fattoriale\nL’invarianza di misura può essere suddivisa in tre livelli principali:\n\nInvarianza configurale: verifica se il modello fattoriale ha la stessa struttura di base (numero di fattori e pattern di saturazioni) tra i gruppi. È il livello minimo richiesto per confrontare i gruppi.\nInvarianza metrica: verifica se i carichi fattoriali (o saturazioni) sono uguali tra i gruppi. Questo livello è necessario per confrontare le relazioni tra i costrutti latenti nei gruppi.\nInvarianza scalare: verifica se le intercette delle variabili osservate sono uguali tra i gruppi. Questo livello è essenziale per confrontare le medie dei costrutti latenti.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Invarianza di misura</span>"
    ]
  },
  {
    "objectID": "chapters/sem/07_group_invariance.html#invarianza-nei-modelli-sem-multi-gruppo",
    "href": "chapters/sem/07_group_invariance.html#invarianza-nei-modelli-sem-multi-gruppo",
    "title": "54  Invarianza di misura",
    "section": "\n54.2 Invarianza nei Modelli SEM Multi-Gruppo",
    "text": "54.2 Invarianza nei Modelli SEM Multi-Gruppo\nL’analisi multi-gruppo in un modello di equazioni strutturali (SEM) estende le analisi condotte su un singolo gruppo, permettendo di verificare l’equivalenza del modello tra sottogruppi. I modelli SEM multi-gruppo richiedono di considerare due componenti principali:\n\nInvarianza nei modelli di misura: verifica se le relazioni tra i costrutti latenti e le variabili osservate sono equivalenti tra i gruppi. Questa analisi include la verifica dei carichi fattoriali, delle intercette e delle varianze residue.\nInvarianza nei modelli strutturali: verifica se le relazioni tra i costrutti latenti (ad esempio, i coefficienti di percorso, le medie e le covarianze) sono equivalenti tra i gruppi.\n\nIn generale, l’obiettivo è ottenere modelli parsimoniosi che garantiscano un buon adattamento ai dati. Tuttavia, modelli con livelli elevati di invarianza possono essere restrittivi, e il compromesso tra parsimonia e adattamento richiede decisioni basate sul contesto di ricerca.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Invarianza di misura</span>"
    ]
  },
  {
    "objectID": "chapters/sem/07_group_invariance.html#test-dellinvarianza-fattoriale",
    "href": "chapters/sem/07_group_invariance.html#test-dellinvarianza-fattoriale",
    "title": "54  Invarianza di misura",
    "section": "\n54.3 Test dell’Invarianza Fattoriale",
    "text": "54.3 Test dell’Invarianza Fattoriale\nPer verificare l’invarianza di misura, si utilizza l’Analisi Fattoriale Confermativa a Gruppi Multipli (MG-CFA), che permette di determinare se un modello fattoriale descrive le stesse relazioni tra variabili osservate e latenti in diversi gruppi. Ad esempio, in uno studio che confronta gruppi basati sul genere, è essenziale verificare se un test, come quello sull’autostima, misura lo stesso costrutto con la stessa struttura fattoriale in entrambi i gruppi. Solo se questa condizione è soddisfatta, è possibile confrontare validamente le medie o le relazioni tra le variabili nei due gruppi.\nSe l’invarianza non viene verificata, qualsiasi confronto tra gruppi può risultare fuorviante, in quanto si rischia di confrontare costrutti che non sono concettualmente equivalenti. In questo capitolo, analizzeremo l’invarianza di misura considerando sia variabili osservate continue che categoriali. Tale analisi rappresenta una base fondamentale per garantire validità e coerenza nelle analisi successive, come il confronto delle medie dei costrutti latenti o la stima di relazioni strutturali.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Invarianza di misura</span>"
    ]
  },
  {
    "objectID": "chapters/sem/07_group_invariance.html#indicatori-continui",
    "href": "chapters/sem/07_group_invariance.html#indicatori-continui",
    "title": "54  Invarianza di misura",
    "section": "\n54.4 Indicatori Continui",
    "text": "54.4 Indicatori Continui\n\n54.4.1 Intercette degli Item\nNei modelli di equazioni strutturali, si analizza tradizionalmente la matrice di covarianza delle variabili osservate. In precedenza, abbiamo introdotto il modello dell’analisi fattoriale nel seguente modo:\n\\[\ny_i = \\mu + \\lambda_j \\xi_k + \\delta_i ,\n\\]\ndove \\(y_i\\) rappresenta il valore osservato, \\(\\mu\\) è la media, \\(\\lambda_j\\) è il carico fattoriale, \\(\\xi_k\\) è il fattore latente, e \\(\\delta_i\\) è l’errore. Per semplicità, nelle analisi iniziali si esclude spesso la media \\(\\mu\\), considerando gli scarti dalla media (\\(y_i - \\mu\\)), poiché ciò non modifica la struttura delle covarianze.\nTuttavia, in alcune applicazioni, come la verifica dell’invarianza fattoriale, è cruciale includere anche le medie delle variabili osservate. In questo caso, le medie sono rappresentate dalle intercette degli item nel modello fattoriale.\n\n54.4.2 Specificare le Intercette con lavaan\n\nIn lavaan, le intercette possono essere incluse specificandole esplicitamente per ciascun item. Ad esempio, per un singolo indicatore, si utilizza la sintassi seguente:\n# Specifica di un'intercetta per una variabile manifesta\nmy_item ~ 1\nQui, my_item rappresenta il nome della variabile manifesta, e ~ 1 indica la presenza di un’intercetta nel modello.\n\n54.4.2.1 Modello a Due Fattori con Intercette\nPer un modello con due fattori e sei indicatori, è possibile includere esplicitamente le intercette per ogni variabile osservata:\n# Modello con intercette specificate\nmod1 &lt;- \"\n  # Modello a due fattori\n  f1 =~ x1 + x2 + x3\n  f2 =~ x4 + x5 + x6\n  # Intercette\n  x1 ~ 1\n  x2 ~ 1\n  x3 ~ 1\n  x4 ~ 1\n  x5 ~ 1\n  x6 ~ 1\n\"\nTuttavia, un approccio più efficiente consiste nell’omettere le intercette nella sintassi del modello e includerle automaticamente utilizzando l’argomento meanstructure = TRUE nella funzione cfa():\n# Modello senza intercette esplicite\nmod2 &lt;- \"\n  f1 =~ x1 + x2 + x3\n  f2 =~ x4 + x5 + x6\n\"\n\n# Stima del modello con struttura delle medie\nfit &lt;- cfa(\n  mod2,\n  data = d,\n  meanstructure = TRUE\n)\n\n54.4.3 Effetti della Specificazione delle Intercette\nL’aggiunta delle intercette non modifica l’adattamento complessivo del modello. Infatti, includendo la struttura delle medie, si aggiungono \\(p\\) dati (le medie degli \\(p\\) indicatori) ma si stimano anche \\(p\\) nuovi parametri (le intercette), lasciando invariata la bontà dell’adattamento in termini di chi-quadrato e gradi di libertà.\nL’unico motivo per specificare esplicitamente le intercette è la necessità di imporre vincoli nella stima di questi parametri. Ad esempio, in un’analisi multi-gruppo, si potrebbe voler confrontare le medie delle variabili latenti tra gruppi, imponendo vincoli di uguaglianza sulle intercette degli item.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Invarianza di misura</span>"
    ]
  },
  {
    "objectID": "chapters/sem/07_group_invariance.html#tipologie-di-invarianza",
    "href": "chapters/sem/07_group_invariance.html#tipologie-di-invarianza",
    "title": "54  Invarianza di misura",
    "section": "\n54.5 Tipologie di Invarianza",
    "text": "54.5 Tipologie di Invarianza\nNel contesto dell’analisi fattoriale confermativa (CFA) multigruppo, l’invarianza viene testata a diversi livelli, ciascuno con specifici vincoli sui parametri del modello. Ogni livello successivo rappresenta un passo avanti nella verifica dell’equivalenza delle misurazioni tra gruppi, aggiungendo nuovi vincoli ai parametri e includendo quelli stabiliti dai livelli precedenti. Ecco una descrizione dettagliata di ciascun livello:\n\nInvarianza Configurale\nL’invarianza configurale rappresenta il livello più basilare. Si verifica quando la stessa struttura fattoriale si adatta a tutti i gruppi. Questo implica che gli stessi fattori sono presenti in ogni gruppo e che gli stessi item misurano tali fattori in modo consistente. Tuttavia, non si assumono ancora uguaglianze nei carichi fattoriali, nelle intercette o in altri parametri. Questo livello garantisce che i gruppi condividano lo stesso modello concettuale, ma non permette ancora confronti quantitativi tra di essi.\nInvarianza Metrica\nL’invarianza metrica aggiunge un vincolo di uguaglianza ai carichi fattoriali (factor loadings) tra i gruppi, mantenendo la struttura configurale. Questo livello verifica se gli item hanno la stessa relazione con i fattori latenti in tutti i gruppi. L’invarianza metrica è essenziale per confrontare le relazioni tra fattori latenti e variabili osservate, poiché garantisce che la scala del fattore latente sia equivalente nei gruppi.\nInvarianza Scalare\nL’invarianza scalare introduce un ulteriore vincolo: le intercette degli item devono essere uguali tra i gruppi. Insieme all’invarianza metrica, questo livello permette di confrontare le medie dei fattori latenti tra gruppi. L’uguaglianza delle intercette implica che i gruppi interpretano i punteggi degli item in modo equivalente e che eventuali differenze nelle medie dei fattori latenti riflettono effettive differenze nei costrutti, non nelle modalità di risposta.\nInvarianza Residuale\nL’invarianza residuale richiede che le varianze degli errori delle variabili osservate siano uguali tra i gruppi. Questo implica che l’affidabilità degli item (in termini di errore di misurazione) sia la stessa in tutti i gruppi. È un livello più stringente e meno frequentemente richiesto per la maggior parte delle applicazioni, ma utile in contesti dove si vuole garantire una misura estremamente robusta.\nInvarianza delle Varianze dei Fattori Latenti\nA questo livello, si testa se le varianze dei fattori latenti sono uguali tra i gruppi. L’uguaglianza delle varianze dei fattori implica che la variabilità del costrutto latente è comparabile nei diversi gruppi, consentendo ulteriori confronti sui pattern di dispersione.\nInvarianza delle Medie dei Fattori Latenti\nQuesto è il livello più stringente di invarianza. Si verifica se le medie dei fattori latenti sono uguali tra i gruppi. L’invarianza delle medie dei fattori, combinata con i livelli precedenti, implica che i gruppi condividano non solo la struttura del modello, ma anche valori medi simili nei costrutti latenti.\n\n\n54.5.1 Confronto dei Modelli\nOgni livello successivo di invarianza viene valutato confrontando gli indici di adattamento del modello corrente con quelli del modello precedente. Questo confronto aiuta a stabilire se l’aggiunta di nuovi vincoli compromette l’adattamento del modello. Tra i metodi di confronto più comuni vi sono:\n\n\nChi-quadrato (\\(\\Delta \\chi^2\\)): Confronta direttamente i modelli vincolati e non vincolati. È sensibile alla dimensione del campione, per cui può risultare poco informativo con campioni ampi.\n\nCriteri di adattamento alternativi: Cambiamenti negli indici di adattamento, come CFI (\\(\\Delta CFI\\)), TLI, o RMSEA, sono spesso utilizzati per determinare se un modello con più vincoli mantiene un buon adattamento.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Invarianza di misura</span>"
    ]
  },
  {
    "objectID": "chapters/sem/07_group_invariance.html#un-esempio-concreto",
    "href": "chapters/sem/07_group_invariance.html#un-esempio-concreto",
    "title": "54  Invarianza di misura",
    "section": "\n54.6 Un esempio Concreto",
    "text": "54.6 Un esempio Concreto\nConsideriamo un esempio tratto da Brown (2015), che illustra un’analisi fattoriale confermativa (CFA) applicata alla misurazione della depressione maggiore, così come definita nel DSM-IV.\nIl modello CFA in questo caso è progettato per valutare un costrutto latente, la depressione maggiore, utilizzando un set di nove indicatori osservabili che rappresentano i criteri diagnostici del DSM-IV. Gli indicatori includono:\n\n\nMDD1: Umore depresso\n\n\nMDD2: Perdita di interesse nelle attività usuali\n\n\nMDD3: Cambiamenti di peso o appetito\n\n\nMDD4: Disturbi del sonno\n\n\nMDD5: Agitazione o rallentamento psicomotorio\n\n\nMDD6: Affaticamento o perdita di energia\n\n\nMDD7: Sentimenti di inutilità o senso di colpa\n\n\nMDD8: Difficoltà di concentrazione\n\n\nMDD9: Pensieri di morte o suicidari\n\nQuesto esempio illustra come il modello CFA possa essere utilizzato per verificare se i nove indicatori forniscono una rappresentazione valida e coerente del costrutto latente “depressione maggiore”. Inoltre, l’approccio permette di valutare se la struttura del modello è applicabile a diversi sottogruppi, ad esempio in base al genere, all’età o al contesto culturale, verificando così l’invarianza di misura.\nImportiamo i dati in \\(\\mathsf{R}\\):\n\nd &lt;- readRDS(\n  here::here(\"data\", \"mdd_sex.RDS\")\n)\n\n\nhead(d)\n#&gt;      sex mdd1 mdd2 mdd3 mdd4 mdd5 mdd6 mdd7 mdd8 mdd9\n#&gt; 1 female    5    4    1    6    5    6    5    4    2\n#&gt; 2 female    5    5    5    5    4    5    4    5    4\n#&gt; 3 female    4    5    4    2    6    6    0    0    0\n#&gt; 4 female    5    5    3    3    5    5    6    4    0\n#&gt; 5 female    5    5    0    5    0    4    6    0    0\n#&gt; 6 female    6    6    4    6    4    6    5    6    2\n\nNel caso presente, i gruppi corrispondono al genere. Confrontiamo le distribuzioni di densità empirica degli item tra i due gruppi.\n\nd_long &lt;- d |&gt;\n    pivot_longer(!sex, names_to = \"item\", values_to = \"value\")\n\nd_long |&gt;\n    ggplot(aes(value, col=sex)) +\n    geom_density(linewidth=1.5) +\n    facet_wrap(~item, nrow=3, scales=\"free\") +\n    labs(x=\" \", y=\"Density\")\n\n\n\n\n\n\n\nIn questo tutorial, affrontiamo il problema di verificare l’invarianza fattoriale rispetto al genere utilizzando un modello CFA multi-gruppo. Consideriamo il seguente modello, basato sugli indicatori della depressione maggiore:\n\nmodel_mdd &lt;- \"\n  MDD =~ mdd1 + mdd2 + mdd3 + mdd4 + mdd5 + mdd6 + mdd7 + mdd8 +\n         mdd9\n  mdd1 ~~ mdd2\n\"\n\nIn questa specificazione, il fattore latente MDD (depressione maggiore) è misurato da nove indicatori osservabili (mdd1–mdd9), e viene aggiunta una correlazione residua tra gli indicatori mdd1 (umore depresso) e mdd2 (perdita di interesse). Questo vincolo riflette un’eventuale sovrapposizione concettuale o statistica tra questi due item (Brown, 2015).\n\n54.6.1 Stima dei Parametri nel Contesto Multi-Gruppo\nIn precedenza, abbiamo analizzato i modelli CFA a singolo gruppo, in cui i parametri vengono stimati minimizzando una funzione di discrepanza. Questa funzione confronta la matrice di covarianza osservata con quella prevista dal modello, cercando il miglior insieme di stime che riduca al minimo tale differenza.\nQuando si passa all’analisi multi-gruppo, il metodo di stima si estende in modo naturale:\n\nDefinizione delle Funzioni di Discrepanza per Gruppo\nPer ogni gruppo (ad esempio, uomini e donne), si calcola una funzione di discrepanza separata. Questa funzione misura la discrepanza tra le covarianze osservate e quelle stimate per ciascun gruppo.\nFunzione di Discrepanza Complessiva\nLa funzione complessiva viene definita come la somma ponderata delle funzioni di discrepanza di ciascun gruppo. In altre parole, l’adattamento complessivo del modello tiene conto simultaneamente di tutti i gruppi.\nStima dei Parametri\nI parametri del modello vengono stimati minimizzando questa funzione di discrepanza complessiva. Questo approccio consente di ottenere stime congiunte che rappresentano il miglior adattamento possibile per tutti i gruppi, rispettando i vincoli imposti dal modello.\n\n54.6.2 Applicazione dell’Invarianza Fattoriale al Confronto di Genere\nL’analisi multi-gruppo permette di verificare se il costrutto latente MDD (depressione maggiore) è misurato in modo equivalente tra uomini e donne, attraverso una sequenza di modelli con vincoli progressivi. Ogni livello di invarianza aggiunge nuovi vincoli al modello precedente, garantendo un confronto sempre più dettagliato e rigoroso tra i gruppi.\nDi seguito, definiamo i modelli per ciascun livello di invarianza, utilizzando la sintassi del pacchetto lavaan, ampiamente utilizzato per l’analisi SEM. I vincoli vengono impostati attraverso l’argomento group.equal, che consente di specificare quali parametri devono essere uguali tra i gruppi.\n\n54.6.2.1 Modelli di Invarianza\n\n\nInvarianza Configurale\nQuesto modello verifica se la stessa struttura fattoriale è valida per entrambi i gruppi, senza imporre vincoli aggiuntivi sui parametri.\n\n\nfit_ef &lt;- cfa(\n  model_mdd,\n  data = d,\n  group = \"sex\",\n  meanstructure = TRUE\n)\n\n\n\nInvarianza Metrica\nQui si aggiunge il vincolo di uguaglianza dei carichi fattoriali (loadings), per verificare se gli item si relazionano ai fattori latenti nello stesso modo nei due gruppi.\n\n\nfit_efl &lt;- update(\n  fit_ef,\n  group.equal = c(\"loadings\")\n)\n\n\n\nInvarianza Scalare\nOltre ai carichi fattoriali, si vincolano le intercette degli item per garantire che i punteggi medi degli item siano interpretati allo stesso modo nei due gruppi.\n\n\nfit_eii &lt;- update(\n  fit_efl,\n  group.equal = c(\"loadings\", \"intercepts\")\n)\n\n\n\nInvarianza Residuale\nQuesto modello aggiunge il vincolo di uguaglianza delle varianze residue degli indicatori tra i gruppi, assicurando che la precisione di misurazione sia equivalente.\n\n\nfit_eir &lt;- update(\n  fit_eii,\n  group.equal = c(\"loadings\", \"intercepts\", \"residuals\")\n)\n\n\n\nInvarianza delle Varianze Latenti\nSi aggiunge il vincolo di uguaglianza delle varianze dei fattori latenti, verificando se la variabilità del costrutto latente è simile tra i gruppi.\n\n\nfit_fv &lt;- update(\n  fit_eir,\n  group.equal = c(\n    \"loadings\", \"intercepts\", \"residuals\",\n    \"lv.variances\"\n  )\n)\n\n\n\nInvarianza delle Medie Latenti\nInfine, si vincolano le medie latenti dei fattori, per verificare se i gruppi hanno medie equivalenti nel costrutto latente.\n\n\nfit_fm &lt;- update(\n  fit_fv,\n  group.equal = c(\n    \"loadings\", \"intercepts\", \"residuals\",\n    \"lv.variances\", \"means\"\n  )\n)\n\n\n54.6.2.2 Interpretazione dei Risultati\nOgni modello rappresenta un passo successivo nella verifica dell’invarianza fattoriale. Dopo aver adattato ciascun modello ai dati, è essenziale confrontare gli indici di adattamento—come il \\(\\chi^2\\), il CFI e il RMSEA—tra un modello e il successivo. Questo confronto consente di determinare se i vincoli introdotti a ciascun livello di invarianza sono coerenti con i dati e non compromettono l’adattamento complessivo del modello.\nPer eseguire il confronto tra modelli nidificati, utilizziamo il test del rapporto di verosimiglianze, che confronta direttamente la discrepanza tra i modelli:\n\nout &lt;- lavTestLRT(fit_ef, fit_efl, fit_eii, fit_eir, fit_fv, fit_fm)\nprint(out)\n#&gt; \n#&gt; Chi-Squared Difference Test\n#&gt; \n#&gt;         Df   AIC   BIC Chisq Chisq diff  RMSEA Df diff Pr(&gt;Chisq)\n#&gt; fit_ef  52 27526 27785  98.9                                     \n#&gt; fit_efl 60 27514 27735 102.8       3.93 0.0000       8       0.86\n#&gt; fit_eii 68 27510 27695 115.3      12.47 0.0386       8       0.13\n#&gt; fit_eir 77 27502 27645 125.0       9.71 0.0145       9       0.37\n#&gt; fit_fv  78 27501 27639 125.8       0.79 0.0000       1       0.37\n#&gt; fit_fm  79 27501 27635 127.7       1.92 0.0495       1       0.17\n\nIl confronto tra i modelli nidificati mostra che l’introduzione di vincoli sempre più stringenti—uguaglianza dei carichi fattoriali, delle intercette, delle varianze residue, delle varianze dei fattori latenti e delle medie latenti—non porta a una sostanziale perdita di adattamento del modello ai dati. In particolare:\n\nIl valore del test del rapporto di verosimiglianze (\\(\\Delta \\chi^2\\)) non risulta incompatibile con i dati, indicando che i vincoli aggiunti sono accettabili.\nGli indici di adattamento (ad esempio RMSEA) restano a livelli appropriati per ciascun modello.\n\nPer i dati esaminati da Brown (2015), possiamo concludere che vi sono forti evidenze di invarianza fattoriale tra uomini e donne rispetto al costrutto di depressione maggiore (MDD). Questo implica che:\n\nGli indicatori misurano il costrutto in modo equivalente nei due gruppi.\nLe differenze osservate tra i gruppi nei punteggi totali del test possono essere interpretate come reali differenze nel costrutto latente, e non come artefatti di misurazione.\n\nL’invarianza fattoriale stabilita giustifica il confronto tra le medie dei punteggi latenti nei due gruppi, permettendo un’interpretazione solida e valida dei risultati.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Invarianza di misura</span>"
    ]
  },
  {
    "objectID": "chapters/sem/07_group_invariance.html#analisi-multi-gruppo-con-indicatori-ordinali",
    "href": "chapters/sem/07_group_invariance.html#analisi-multi-gruppo-con-indicatori-ordinali",
    "title": "54  Invarianza di misura",
    "section": "\n54.7 Analisi Multi-Gruppo con Indicatori Ordinali",
    "text": "54.7 Analisi Multi-Gruppo con Indicatori Ordinali\nL’analisi multi-gruppo per dati ordinali presenta sfide specifiche legate alla natura di questo tipo di variabili. A differenza delle variabili continue, le variabili ordinali richiedono approcci particolari sia per il calcolo delle saturazioni fattoriali che per l’analisi statistica.\nLe variabili ordinali rappresentano categorie con un ordine intrinseco, ma senza garanzia di spaziature uniformi tra le categorie. Esempi comuni includono scale di risposta come:\n\n\n“Fortemente in disaccordo” - “Fortemente d’accordo”\n\n“Mai” - “Sempre”\n\nSebbene queste categorie abbiano un ordine logico, le distanze tra di esse non sono necessariamente equivalenti e non possono essere interpretate come grandezze quantitative. L’assegnazione di valori numerici è arbitraria e non rappresenta differenze reali di intensità o grandezza.\n\n54.7.1 Problematiche nell’Analisi di Dati Ordinali\n\n54.7.1.1 Correlazioni Policoriche\nUno degli aspetti chiave nell’analisi di dati ordinali è il calcolo delle correlazioni. Le correlazioni policoriche rappresentano una soluzione efficace, basandosi sull’ipotesi che le risposte ordinali derivino da una variabile latente continua e normalmente distribuita. In questo approccio:\n\nOgni categoria di risposta è associata a un intervallo specifico sulla variabile latente sottostante.\nI punti di taglio (o soglie, \\(\\tau_1, \\tau_2, \\dots, \\tau_k\\)) suddividono la distribuzione normale in sezioni che corrispondono alle frequenze osservate per ciascuna categoria.\n\nAd esempio, per una scala a quattro punti, le soglie dividono la distribuzione in quattro intervalli, ciascuno associato a una categoria di risposta (ad esempio, “Mai”, “A volte”, “Spesso”, “Sempre”).\n\n54.7.1.2 Invarianza delle Soglie\nNell’analisi multi-gruppo, un aspetto fondamentale è l’invarianza delle soglie (threshold invariance). Questo concetto presuppone che i punti di taglio che definiscono le categorie di risposta rimangano stabili tra i gruppi. La stabilità delle soglie è essenziale per garantire che le relazioni tra le categorie di risposta siano comparabili nei diversi gruppi. In assenza di invarianza delle soglie, eventuali differenze tra gruppi potrebbero riflettere variazioni nella struttura della misurazione, piuttosto che vere differenze nel costrutto latente.\n\n54.7.2 Stima delle Saturazioni Fattoriali\nPer i dati ordinali, lo stimatore dei Minimi Quadrati Ponderati (Weighted Least Squares, WLS) è la scelta preferita. Questo metodo è particolarmente adatto ai dati ordinali poiché:\n\nTiene conto della natura categoriale delle risposte.\nFornisce stime più affidabili delle saturazioni fattoriali rispetto agli stimatori progettati per dati continui, come il massimo di verosimiglianza.\n\nIn alternativa, si può utilizzare una variante ponderata del WLS, nota come WLSMV (Weighted Least Squares Mean and Variance adjusted), che introduce correzioni per migliorare la robustezza delle stime.\nIn sintesi, l’analisi di invarianza fattoriale con dati ordinali richiede un’attenzione particolare a due aspetti principali:\n\n\nUso di correlazioni policoriche, che consentono di modellare la relazione tra categorie ordinali assumendo una variabile latente continua.\n\nScelta di stimatori adeguati, come il WLS o il WLSMV, che rispettano la natura dei dati ordinali e garantiscono stime affidabili.\n\nAdottando questi approcci, è possibile ottenere risultati validi e interpretabili, garantendo che eventuali confronti tra gruppi riflettano differenze reali nei costrutti latenti e non artefatti derivanti dalla misurazione.\n\n54.7.3 Un Esempio Concreto\nWu & Estabrook (2016) evidenziano che la metodologia tradizionale per valutare l’invarianza fattoriale con dati continui richiede adattamenti significativi quando applicata a indicatori categoriali. Solitamente, l’analisi dell’invarianza fattoriale segue una sequenza standard: si parte dalla definizione di un modello configurale e si introducono gradualmente vincoli ai parametri del modello, come carichi fattoriali, intercette e varianze residue. Tuttavia, per i dati categoriali, questa procedura presenta criticità specifiche, in particolare a causa della dipendenza dalle soglie utilizzate per definire le correlazioni policoriche.\n\n54.7.3.1 La Proposta di Wu & Estabrook (2016)\n\nWu & Estabrook (2016) sottolineano l’importanza di valutare preliminarmente l’equivalenza delle soglie tra gruppi, proponendo un approccio alternativo che enfatizza la costruzione di un modello di soglia (threshold model). Questo modello serve a verificare se le soglie, che definiscono gli intervalli della variabile latente continua sottostante, siano stabili e comparabili tra i gruppi. Solo una volta stabilita questa coerenza, è opportuno procedere con la valutazione dell’invarianza delle saturazioni fattoriali e degli altri parametri.\nTale approccio si basa sull’assunto che, nei dati categoriali, le soglie sono un elemento cruciale per interpretare correttamente le relazioni tra indicatori e fattori latenti. L’ordine delle analisi suggerito da Wu & Estabrook (2016) garantisce che eventuali differenze osservate tra gruppi siano attribuibili a reali variazioni nei costrutti latenti, e non a discrepanze nella definizione delle categorie di risposta.\n\n54.7.3.2 Esempio Applicativo: Scala sul Bullismo\nPer illustrare l’approccio, riprendiamo l’esempio proposto da Svetina et al. (2020). I dati analizzati provengono da una scala Likert a 4 punti (da 0 = “mai” a 3 = “almeno una volta alla settimana”) che misura la frequenza di episodi di bullismo. La scala include quattro item, come ad esempio: “Mi prendevano in giro o mi insultavano”. I dati sono stati raccolti in tre paesi: Azerbaigian, Austria e Finlandia, coinvolgendo i seguenti campioni:\n\n\nAzerbaigian: 31 scuole, 3.808 partecipanti.\n\n\nAustria: 40 scuole, 4.457 partecipanti.\n\n\nFinlandia: 246 scuole, 4.520 partecipanti.\n\nOgni partecipante ha risposto agli item della scala valutando la frequenza di episodi di bullismo subiti. L’obiettivo è verificare se la scala misura il costrutto di bullismo in modo equivalente nei tre paesi.\nImportiamo i dati in R:\n\ndat &lt;- read.table(\"../../data/BULLY.dat\", header = FALSE)\nnames(dat) &lt;- c(\"IDCNTRY\", \"R09A\", \"R09B\", \"R09C\", \"R09D\")\nhead(dat)\n#&gt;   IDCNTRY R09A R09B R09C R09D\n#&gt; 1      31    3    3    0    0\n#&gt; 2      31    0    0    0    0\n#&gt; 3      31    3    2    1    3\n#&gt; 4      31    0    0    3    0\n#&gt; 5      31    0    0    0    0\n#&gt; 6      31    0    0    0    0\n\nLa matrice all.results viene creata per raccogliere e organizzare i risultati dei diversi modelli sottoposti a confronto. I modelli analizzati includono:\n\n\nBaseline: nessun vincolo tra i gruppi (invarianza configurale).\n\nProposition 4: vincolo di equivalenza delle soglie tra i gruppi (invarianza delle soglie).\n\nProposition 7: vincolo di equivalenza sia delle soglie che delle saturazioni fattoriali tra i gruppi (invarianza delle soglie e dei carichi fattoriali).\n\nGli indici di bontà di adattamento che saranno registrati per ciascun modello comprendono: il chi-quadrato (\\(\\chi^2\\)), i gradi di libertà (\\(df\\)), il valore \\(p\\), il RMSEA, il CFI e il TLI. Questi indici permettono di valutare la qualità dell’adattamento dei modelli ai dati e di confrontare l’effetto dei vincoli imposti tra i gruppi.\n\nall.results &lt;- matrix(NA, ncol = 6, nrow = 3)\n\n\n54.7.3.3 Procedura Analitica\n1. Modello Configurale (Baseline)\nNel modello configurale, non si impongono vincoli sui parametri tra gruppi. Questo rappresenta il livello più basilare di invarianza.\nUtilizziamo la funzione measEq.syntax per creare il modello di partenza:\n\nmod.cat &lt;- \"F1 =~ R09A + R09B + R09C + R09D\"\n\nbaseline &lt;- measEq.syntax(\n  configural.model = mod.cat,\n  data = dat,\n  ordered = c(\"R09A\", \"R09B\", \"R09C\", \"R09D\"),\n  parameterization = \"delta\",\n  ID.fac = \"std.lv\",\n  ID.cat = \"Wu.Estabrook.2016\",\n  group = \"IDCNTRY\",\n  group.equal = \"configural\"\n)\n\nIl modello viene adattato ai dati:\n\nmodel.baseline &lt;- as.character(baseline)\nfit.baseline &lt;- cfa(\n  model.baseline, \n  data = dat, \n  group = \"IDCNTRY\", \n  ordered = c(\"R09A\", \"R09B\", \"R09C\", \"R09D\")\n)\n\nI risultati di bontà di adattamento vengono salvati per il confronto con modelli successivi:\n\nall.results[1, ] &lt;- round(\n  fitmeasures(fit.baseline, c(\n    \"chisq.scaled\", \"df.scaled\", \"pvalue.scaled\", \n    \"rmsea.scaled\", \"cfi.scaled\", \"tli.scaled\"\n  )), 3\n)\n\n2. Modello con Invarianza delle Soglie (Threshold Invariance)\nPer testare l’invarianza delle soglie, aggiungiamo il vincolo di uguaglianza delle soglie tra i gruppi:\n\nprop4 &lt;- measEq.syntax(\n  configural.model = mod.cat,\n  data = dat,\n  ordered = c(\"R09A\", \"R09B\", \"R09C\", \"R09D\"),\n  parameterization = \"delta\",\n  ID.fac = \"std.lv\",\n  ID.cat = \"Wu.Estabrook.2016\",\n  group = \"IDCNTRY\",\n  group.equal = c(\"thresholds\")\n)\n\nAdattiamo il modello e salviamo i risultati:\n\nmodel.prop4 &lt;- as.character(prop4)\nfit.prop4 &lt;- cfa(\n  model.prop4,\n  data = dat,\n  group = \"IDCNTRY\",\n  ordered = c(\"R09A\", \"R09B\", \"R09C\", \"R09D\")\n)\n\nall.results[2, ] &lt;- round(\n  fitmeasures(fit.prop4, c(\n    \"chisq.scaled\", \"df.scaled\", \"pvalue.scaled\", \n    \"rmsea.scaled\", \"cfi.scaled\", \"tli.scaled\"\n  )), 3\n)\n\nEseguiamo il confronto tra il modello configurale e il modello con invarianza delle soglie:\n\nlavTestLRT(fit.baseline, fit.prop4)\n#&gt; \n#&gt; Scaled Chi-Squared Difference Test (method = \"satorra.2000\")\n#&gt; \n#&gt; lavaan-&gt;lavTestLRT():  \n#&gt;    lavaan NOTE: The \"Chisq\" column contains standard test statistics, not \n#&gt;    the robust test that should be reported per model. A robust difference \n#&gt;    test is a function of two standard (not robust) statistics.\n#&gt;              Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq)\n#&gt; fit.baseline  6          26.9                              \n#&gt; fit.prop4    14          42.2         61       8      3e-10\n\n3. Modello con Invarianza delle Soglie e dei Carichi Fattoriali (Threshold and Loading Invariance)\nAggiungiamo ora i vincoli di uguaglianza sia per le soglie che per i carichi fattoriali:\n\nprop7 &lt;- measEq.syntax(\n  configural.model = mod.cat,\n  data = dat,\n  ordered = c(\"R09A\", \"R09B\", \"R09C\", \"R09D\"),\n  parameterization = \"delta\",\n  ID.fac = \"std.lv\",\n  ID.cat = \"Wu.Estabrook.2016\",\n  group = \"IDCNTRY\",\n  group.equal = c(\"thresholds\", \"loadings\")\n)\n\nmodel.prop7 &lt;- as.character(prop7)\nfit.prop7 &lt;- cfa(\n  model.prop7, \n  data = dat, \n  group = \"IDCNTRY\", \n  ordered = c(\"R09A\", \"R09B\", \"R09C\", \"R09D\")\n)\n\nall.results[3, ] &lt;- round(\n  fitmeasures(fit.prop7, c(\n    \"chisq.scaled\", \"df.scaled\", \"pvalue.scaled\", \n    \"rmsea.scaled\", \"cfi.scaled\", \"tli.scaled\"\n  )), 3\n)\n\nEseguiamo il confronto tra i modelli:\n\nlavTestLRT(fit.prop4, fit.prop7)\n#&gt; \n#&gt; Scaled Chi-Squared Difference Test (method = \"satorra.2000\")\n#&gt; \n#&gt; lavaan-&gt;lavTestLRT():  \n#&gt;    lavaan NOTE: The \"Chisq\" column contains standard test statistics, not \n#&gt;    the robust test that should be reported per model. A robust difference \n#&gt;    test is a function of two standard (not robust) statistics.\n#&gt;           Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq)\n#&gt; fit.prop4 14          42.2                              \n#&gt; fit.prop7 20          93.1       73.7       6    7.1e-14\n\nRisultati Finali\nI risultati dei tre modelli vengono confrontati utilizzando gli indici di bontà di adattamento:\n\n\n\n\n\n\n\n\n\n\n\nModello\n\\(\\chi^2\\)\ndf\n\n\\(p\\)-value\nRMSEA\nCFI\nTLI\n\n\n\nBaseline\n50.9\n6\n0.000\n0.042\n0.997\n0.991\n\n\nProp4\n107.8\n14\n0.000\n0.040\n0.994\n0.992\n\n\nProp7\n186.5\n20\n0.000\n0.044\n0.989\n0.990\n\n\n\nNel caso presente, i risultati del test del rapporto di verosimiglianza indicano che l’invarianza delle soglie non viene rispettata. Di conseguenza, ulteriori confronti sui carichi fattoriali sono superflui, ma sono stati illustrati per completezza della procedura.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Invarianza di misura</span>"
    ]
  },
  {
    "objectID": "chapters/sem/07_group_invariance.html#vincoli-inter-gruppi",
    "href": "chapters/sem/07_group_invariance.html#vincoli-inter-gruppi",
    "title": "54  Invarianza di misura",
    "section": "\n54.8 Vincoli Inter-Gruppi",
    "text": "54.8 Vincoli Inter-Gruppi\nQuando si adatta un modello di equazioni strutturali (SEM) ai dati provenienti da più gruppi, è spesso utile imporre vincoli di uguaglianza inter-gruppi su alcuni parametri. Tali parametri possono includere effetti causali (diretti, indiretti o totali), varianze, covarianze, medie o intercette. La scelta dei vincoli da applicare dipende dalle ipotesi teoriche specifiche riguardo alle somiglianze o differenze tra i gruppi.\nSe un modello con vincoli inter-gruppi mostra un peggioramento significativo nell’adattamento rispetto a un modello senza vincoli, e quest’ultimo si adatta bene ai dati, si può concludere che i gruppi differiscono per i parametri oggetto del vincolo.\nUn’alternativa all’approccio multi-gruppo consiste nel rappresentare l’appartenenza al gruppo in un modello per un singolo gruppo che include variabili di interazione. In tale modello, variabili endogene vengono regresse su termini prodotto tra l’appartenenza al gruppo e altre variabili. Questo approccio consente di stimare effetti interattivi tra il gruppo e altre variabili causali, che possono essere interpretati come effetti indiretti condizionali. Tuttavia, un limite di questo approccio è l’assunzione di omoscedasticità tra i gruppi, che, se violata, può portare a stime inesatte. In contrasto, l’approccio multi-gruppo consente di testare esplicitamente ipotesi di omogeneità.\n\n54.8.1 Un Esempio Concreto: Studio di Lynam et al. (1993)\nLynam e colleghi (1993) hanno esaminato il ruolo dello status socioeconomico familiare (SES), del quoziente intellettivo verbale (QI verbale), della motivazione durante il test del QI, del rendimento scolastico e dei comportamenti delinquenziali in due gruppi di adolescenti maschi (bianchi e neri) di età compresa tra 12 e 13 anni. I partecipanti provenivano da scuole pubbliche urbane degli Stati Uniti e facevano parte di uno studio longitudinale su individui a rischio elevato di comportamenti delinquenziali.\n\n54.8.1.1 Il Modello Teorico\nIl modello teorico proposto da Lynam et al. è illustrato nella figura seguente:\n\n\n\n\n\nFigura 54.1: Diagramma di percorso proposto da Lynam et al. (1993).\n\n\nSecondo il modello, SES, motivazione e QI verbale influenzano i comportamenti delinquenziali sia direttamente che indirettamente, attraverso il rendimento scolastico. Ad esempio, abilità verbali limitate possono condurre all’abbandono scolastico, aumentando così la delinquenza a causa di opportunità di impiego ridotte o maggiore tempo libero non supervisionato.\n\n54.8.1.2 Critiche al Modello\nIl modello proposto è trasversale e non include una sequenza temporale chiara nelle misurazioni, il che rende discutibile la direzionalità ipotizzata. Block (1995) ha criticato questo approccio, suggerendo che l’impulsività potrebbe mediare l’effetto del QI verbale sulla delinquenza, sottolineando che non è la scarsa abilità verbale di per sé a determinare direttamente i comportamenti delinquenziali.\n\n54.8.2 Specificazione dei Dati\nLe correlazioni, deviazioni standard e medie per ciascun gruppo sono riportate nei seguenti blocchi di codice:\n\n# Correlazioni in forma diagonale inferiore\nblackLower.cor &lt;- \"\n 1.00\n  .08 1.00\n  .28  .30 1.00\n  .05  .21  .50 1.00\n -.11 -.17 -.26 -.33 1.00 \"\n\nwhiteLower.cor &lt;- \"\n 1.00\n  .25 1.00\n  .37  .40 1.00\n  .27  .28  .61 1.00\n -.11 -.20 -.31 -.21 1.00 \"\n\n# Covarianze e deviazioni standard\nblack.cor &lt;- lavaan::getCov(blackLower.cor, names = c(\n    \"ses\", \"effort\", \"viq\", \"achieve\", \"delinq\"\n))\nwhite.cor &lt;- lavaan::getCov(whiteLower.cor, names = c(\n    \"ses\", \"effort\", \"viq\", \"achieve\", \"delinq\"\n))\n\n# Deviazioni standard\nblack.cov &lt;- lavaan::cor2cov(black.cor, sds = c(10.58, 1.35, 13.62, .79, 1.63))\nwhite.cov &lt;- lavaan::cor2cov(white.cor, sds = c(11.53, 1.32, 16.32, .96, 1.45))\n\n# Medie di ciascun gruppo\nblack.mean &lt;- c(31.96, -.01, 93.76, 2.51, 1.40)\nwhite.mean &lt;- c(34.64, .05, 104.18, 2.88, 1.22)\n\n# Creazione degli oggetti per l'analisi\ncombined.cov &lt;- list(black = black.cov, white = white.cov)\ncombined.mean &lt;- list(black = black.mean, white = white.mean)\ncombined.n &lt;- list(black = 214, white = 181)\n\n\n54.8.3 Specificazione del Modello e Adattamento\nIl modello teorico è stato specificato come segue:\n\nlynam.model &lt;- \"\n achieve ~ ses + effort + viq\n delinq ~ achieve + ses + effort + viq \n\"\n\nL’adattamento dei modelli ai dati è stato eseguito con i seguenti vincoli:\n\n# Modello 1: Vincoli completi\nlynam1 &lt;- lavaan::sem(\n    lynam.model,\n    sample.cov = combined.cov,\n    sample.mean = combined.mean,\n    sample.nobs = combined.n,\n    group.equal = c(\"regressions\"),\n    fixed.x = FALSE, meanstructure = TRUE\n)\n\n# Modello 2: Vincoli parziali (achievement -&gt; delinquency libero)\nlynam2 &lt;- lavaan::sem(\n    lynam.model,\n    sample.cov = combined.cov,\n    sample.mean = combined.mean,\n    sample.nobs = combined.n,\n    group.equal = c(\"regressions\"),\n    group.partial = c(\"delinq ~ achieve\"),\n    fixed.x = FALSE, meanstructure = TRUE\n)\n\n# Modello 3: Vincoli sulle intercette del rendimento\nlynam3 &lt;- lavaan::sem(\n    lynam.model,\n    sample.cov = combined.cov,\n    sample.mean = combined.mean,\n    sample.nobs = combined.n,\n    group.equal = c(\"regressions\", \"intercepts\"),\n    group.partial = c(\"delinq ~ achieve\", \"delinq ~ 1\"),\n    fixed.x = FALSE, meanstructure = TRUE\n)\n\n# Modello 4: Vincoli sulle intercette della delinquenza\nlynam4 &lt;- lavaan::sem(\n    lynam.model,\n    sample.cov = combined.cov,\n    sample.mean = combined.mean,\n    sample.nobs = combined.n,\n    group.equal = c(\"regressions\", \"intercepts\"),\n    group.partial = c(\"delinq ~ achieve\", \"achieve ~ 1\"),\n    fixed.x = FALSE, meanstructure = TRUE\n)\n\n\n54.8.4 Valutazione dei Modelli\nLe statistiche di adattamento sono state calcolate per ciascun modello:\n\n# Specificare le statistiche di adattamento globale\nfit.stats &lt;- c(\n    \"chisq\", \"df\", \"pvalue\",\n    \"rmsea\", \"rmsea.ci.lower\", \"rmsea.ci.upper\", \"cfi\", \"srmr\"\n)\n\n# Stampare le statistiche per ogni modello\nlavaan::fitMeasures(lynam1, fit.stats) |&gt; print()\n#&gt;          chisq             df         pvalue          rmsea rmsea.ci.lower \n#&gt;         11.736          7.000          0.110          0.059          0.000 \n#&gt; rmsea.ci.upper            cfi           srmr \n#&gt;          0.115          0.975          0.036\nlavaan::fitMeasures(lynam2, fit.stats) |&gt; print()\n#&gt;          chisq             df         pvalue          rmsea rmsea.ci.lower \n#&gt;          6.107          6.000          0.411          0.010          0.000 \n#&gt; rmsea.ci.upper            cfi           srmr \n#&gt;          0.093          0.999          0.029\nlavaan::fitMeasures(lynam3, fit.stats) |&gt; print()\n#&gt;          chisq             df         pvalue          rmsea rmsea.ci.lower \n#&gt;          6.409          7.000          0.493          0.000          0.000 \n#&gt; rmsea.ci.upper            cfi           srmr \n#&gt;          0.083          1.000          0.030\nlavaan::fitMeasures(lynam4, fit.stats) |&gt; print()\n#&gt;          chisq             df         pvalue          rmsea rmsea.ci.lower \n#&gt;         10.237          7.000          0.176          0.048          0.000 \n#&gt; rmsea.ci.upper            cfi           srmr \n#&gt;          0.107          0.983          0.033\n\nI confronti tra modelli sono stati effettuati con il test del rapporto di verosimiglianza:\n\n# Confronti tra modelli\nlavaan::anova(lynam1, lynam2)\n#&gt; \n#&gt; Chi-Squared Difference Test\n#&gt; \n#&gt;        Df  AIC  BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\n#&gt; lynam2  6 9850 9985  6.11                                    \n#&gt; lynam1  7 9854 9985 11.74       5.63 0.153       1      0.018\n\n\n# Confronti tra modelli\nlavaan::anova(lynam2, lynam3)\n#&gt; \n#&gt; Chi-Squared Difference Test\n#&gt; \n#&gt;        Df  AIC  BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\n#&gt; lynam2  6 9850 9985  6.11                                    \n#&gt; lynam3  7 9848 9980  6.41      0.302     0       1       0.58\n\n\n# Confronti tra modelli\nlavaan::anova(lynam2, lynam4)\n#&gt; \n#&gt; Chi-Squared Difference Test\n#&gt; \n#&gt;        Df  AIC  BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\n#&gt; lynam2  6 9850 9985  6.11                                    \n#&gt; lynam4  7 9852 9984 10.24       4.13 0.126       1      0.042\n\n\n54.8.5 Interpretazione dei Risultati\n\n\nModello 1: I vincoli completi non si adattano ai dati, suggerendo differenze rilevanti tra i gruppi.\n\nModello 2: Rilasciare il vincolo sull’effetto del rendimento migliora in maniera notevole l’adattamento.\n\nModello 3: Vincolando l’intercetta del rendimento, l’adattamento peggiora leggermente rispetto al Modello 2, ma rimane coerente con i dati.\n\nModello 4: Vincolando anche l’intercetta della delinquenza, l’adattamento peggiora in maniera rilevante, indicando che questo vincolo non è supportato.\n\nIn conclusione, l’analisi conferma che il rendimento scolastico ha un effetto protettivo più forte contro la delinquenza nei giovani neri rispetto ai bianchi. Tuttavia, l’introduzione di vincoli aggiuntivi sulle intercette compromette l’adattamento, sottolineando la necessità di considerare le differenze di gruppo nel rappresentare accuratamente il fenomeno.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Invarianza di misura</span>"
    ]
  },
  {
    "objectID": "chapters/sem/07_group_invariance.html#riflessioni-conclusive",
    "href": "chapters/sem/07_group_invariance.html#riflessioni-conclusive",
    "title": "54  Invarianza di misura",
    "section": "\n54.9 Riflessioni Conclusive",
    "text": "54.9 Riflessioni Conclusive\nL’analisi dell’invarianza di misura rappresenta un pilastro fondamentale nella ricerca psicometrica contemporanea, andando oltre la semplice verifica statistica per toccare questioni di equità e validità nelle comparazioni tra gruppi. La sua implementazione sistematica permette di stabilire se le differenze osservate riflettono reali variazioni nel costrutto sottostante o sono invece artefatti dello strumento di misura.\nLa complessità dei moderni contesti di ricerca, caratterizzati da popolazioni sempre più diversificate, rende questa verifica particolarmente rilevante. L’invarianza configura infatti un prerequisito metodologico per qualsiasi studio comparativo, che si tratti di ricerche transculturali, analisi longitudinali o confronti tra sottogruppi demografici.\nUn aspetto spesso sottovalutato riguarda l’impatto dell’invarianza parziale: mentre tradizionalmente si cercava un’invarianza completa, la ricerca recente ha evidenziato come anche livelli parziali di invarianza possano supportare confronti significativi, purché adeguatamente modellati e interpretati. Questo approccio più sfumato riflette una maggiore consapevolezza della complessità dei costrutti psicologici e della loro manifestazione in diversi contesti.\nLa distinzione tra dati continui e ordinali nell’analisi dell’invarianza sottolinea inoltre l’importanza di adattare le procedure statistiche alla natura effettiva delle misurazioni psicologiche, evitando l’applicazione acritica di metodi sviluppati per variabili continue a scale Likert o altri formati di risposta ordinale.\nLe implicazioni di questi concetti si estendono oltre l’ambito accademico, influenzando la pratica clinica, la selezione del personale e la valutazione educativa. La garanzia di equità nelle misurazioni psicologiche diventa particolarmente cruciale in contesti decisionali ad alto impatto, dove interpretazioni non corrette potrebbero portare a decisioni discriminatorie o inappropriate.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Invarianza di misura</span>"
    ]
  },
  {
    "objectID": "chapters/sem/07_group_invariance.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/sem/07_group_invariance.html#informazioni-sullambiente-di-sviluppo",
    "title": "54  Invarianza di misura",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.2\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] ggokabeito_0.1.0  see_0.11.0        MASS_7.3-65       viridis_0.6.5    \n#&gt;  [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n#&gt;  [9] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n#&gt; [13] lavaan_0.6-19     psych_2.4.12      scales_1.3.0      markdown_1.13    \n#&gt; [17] knitr_1.50        lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n#&gt; [21] dplyr_1.1.4       purrr_1.0.4       readr_2.1.5       tidyr_1.3.1      \n#&gt; [25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.1      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.2.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] mime_0.13           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [25] Matrix_1.7-3        R6_2.6.1            fastmap_1.2.0      \n#&gt;  [28] rbibutils_2.3       shiny_1.10.0        digest_0.6.37      \n#&gt;  [31] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n#&gt;  [34] rprojroot_2.0.4     Hmisc_5.2-3         labeling_0.4.3     \n#&gt;  [37] timechange_0.3.0    abind_1.4-8         compiler_4.4.2     \n#&gt;  [40] withr_3.0.2         glasso_1.11         htmlTable_2.4.3    \n#&gt;  [43] backports_1.5.0     carData_3.0-5       ggsignif_0.6.4     \n#&gt;  [46] corpcor_1.6.10      gtools_3.9.5        tools_4.4.2        \n#&gt;  [49] pbivnorm_0.6.0      foreign_0.8-88      zip_2.3.2          \n#&gt;  [52] httpuv_1.6.15       nnet_7.3-20         glue_1.8.0         \n#&gt;  [55] quadprog_1.5-8      nlme_3.1-167        promises_1.3.2     \n#&gt;  [58] lisrelToR_0.3       grid_4.4.2          checkmate_2.3.2    \n#&gt;  [61] cluster_2.1.8.1     reshape2_1.4.4      generics_0.1.3     \n#&gt;  [64] gtable_0.3.6        tzdb_0.5.0          data.table_1.17.0  \n#&gt;  [67] hms_1.1.3           car_3.1-3           sem_3.1-16         \n#&gt;  [70] pillar_1.10.1       rockchalk_1.8.157   later_1.4.1        \n#&gt;  [73] splines_4.4.2       lattice_0.22-6      survival_3.8-3     \n#&gt;  [76] kutils_1.73         tidyselect_1.2.1    miniUI_0.1.1.1     \n#&gt;  [79] pbapply_1.7-2       reformulas_0.4.0    stats4_4.4.2       \n#&gt;  [82] xfun_0.51           qgraph_1.9.8        arm_1.14-4         \n#&gt;  [85] stringi_1.8.4       pacman_0.5.1        boot_1.3-31        \n#&gt;  [88] evaluate_1.0.3      codetools_0.2-20    mi_1.1             \n#&gt;  [91] cli_3.6.4           RcppParallel_5.1.10 rpart_4.1.24       \n#&gt;  [94] xtable_1.8-4        Rdpack_2.6.3        munsell_0.5.1      \n#&gt;  [97] Rcpp_1.0.14         coda_0.19-4.1       png_0.1-8          \n#&gt; [100] XML_3.99-0.18       parallel_4.4.2      jpeg_0.1-10        \n#&gt; [103] lme4_1.1-36         mvtnorm_1.3-3       openxlsx_4.2.8     \n#&gt; [106] rlang_1.1.5         multcomp_1.4-28     mnormt_2.1.1\n\n\n\n\n\nBrown, T. A. (2015). Confirmatory factor analysis for applied research. Guilford publications.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.\n\n\nSaqr, M., & López-Pernas, S. (2024). Learning analytics methods and tutorials: A practical guide using R. Springer Nature.\n\n\nSvetina, D., Rutkowski, L., & Rutkowski, D. (2020). Multiple-group invariance with categorical outcomes using updated guidelines: an illustration using M plus and the lavaan/semtools packages. Structural Equation Modeling: A Multidisciplinary Journal, 27(1), 111–130.\n\n\nWu, H., & Estabrook, R. (2016). Identification of confirmatory factor analysis models of different levels of invariance for ordered categorical outcomes. Psychometrika, 81(4), 1014–1045.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Invarianza di misura</span>"
    ]
  },
  {
    "objectID": "chapters/sem/08_multilevel_sem.html",
    "href": "chapters/sem/08_multilevel_sem.html",
    "title": "55  Modelli di Equazioni Strutturali Multilivello",
    "section": "",
    "text": "55.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nNella ricerca psicologica e nelle scienze sociali, i dati raccolti spesso mostrano strutture complesse a più livelli, in cui le informazioni sono organizzate in gruppi o cluster, e le osservazioni all’interno di ogni cluster tendono a essere correlate tra loro. Questo fenomeno rappresenta un aspetto cruciale poiché, in molte situazioni, i modelli classici di equazioni strutturali (SEM) non sono adatti per analizzare dati di questo tipo. La principale limitazione dei modelli SEM tradizionali risiede nella loro incapacità di tenere conto delle correlazioni intrinseche che caratterizzano i dati strutturati su più livelli, il che può portare a stime distorte e inefficaci.\nDi conseguenza, per analizzare i dati a struttura multilivello, è necessario estendere l’approccio SEM classico integrando una modellazione adatta a tale struttura. La modellazione delle equazioni strutturali multilivello (multilevel SEM) introduce variabili latenti progettate per catturare sia la variazione tra i cluster sia quella all’interno dei cluster. In questo modo, le variabili osservate sono influenzate da fattori latenti che operano sia a livello individuale che a livello di gruppo.\nQuesta strategia consente di distinguere tra la variazione sistematica tra i gruppi (variazione tra i cluster) e la variazione individuale all’interno dei gruppi (variazione intra-cluster), permettendo un’analisi più precisa e rappresentativa dei dati complessi tipici delle scienze sociali e psicologiche. Adottando questo approccio, si ottiene una visione più completa e informativa delle dinamiche che sottostanno ai fenomeni studiati, in quanto il modello multilevel SEM è in grado di cogliere sia le differenze tra gruppi sia le specificità individuali all’interno dei gruppi.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Modelli di Equazioni Strutturali Multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/08_multilevel_sem.html#concetto-generale-per-la-modellazione-a-equazioni-strutturali-multilivello",
    "href": "chapters/sem/08_multilevel_sem.html#concetto-generale-per-la-modellazione-a-equazioni-strutturali-multilivello",
    "title": "55  Modelli di Equazioni Strutturali Multilivello",
    "section": "55.2 Concetto Generale per la Modellazione a Equazioni Strutturali Multilivello",
    "text": "55.2 Concetto Generale per la Modellazione a Equazioni Strutturali Multilivello\nLa tipica implementazione della modellazione a equazioni strutturali multilivello (SEM) prevede la scomposizione dell’outcome osservato in due componenti, una per descrivere la varianza a livello “within” (all’interno) e l’altra a livello “between” (tra gruppi), come segue:\n\\[\n\\bar{y}_{ij} - \\bar{y}_{..} = (\\bar{y}_{ij} - \\bar{y}_{.j}) + (\\bar{y}_{.j} - \\bar{y}_{..}),\n\\]\ndove \\(j\\) è l’indicatore del cluster \\(j\\)-esimo (ad esempio, una scuola, come nell’esempio sopra), con \\(j = 1, \\dots, J\\) e \\(i\\) rappresenta l’indicatore dell’individuo \\(i\\)-esimo all’interno del cluster, con \\(i = 1, \\dots, n_j\\). Il termine \\(\\bar{y}_{.j}\\) indica la media a livello di cluster per il cluster \\(j\\), mentre \\(\\bar{y}_{..}\\) rappresenta la media complessiva.\nQuesta equazione corrisponde alla scomposizione della matrice di covarianza della popolazione in componenti “within” e “between”:\n\\[\n\\text{Cov}(y) = \\Sigma_T = \\Sigma_W + \\Sigma_B.\n\\]\nBasandosi su questa scomposizione, è possibile costruire una funzione di verosimiglianza per stimare i parametri associati ai pesi fattoriali, ai coefficienti di percorso e alle varianze residue nei modelli di equazioni strutturali. Gli errori standard e gli intervalli di credibilità si possono ottenere grazie alla teoria della stima di massima verosimiglianza per l’inferenza statistica. Questa stima viene implementata in lavaan basandosi sui metodi descritti da McDonald e Goldstein (1989).",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Modelli di Equazioni Strutturali Multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/08_multilevel_sem.html#un-esempio-pratico",
    "href": "chapters/sem/08_multilevel_sem.html#un-esempio-pratico",
    "title": "55  Modelli di Equazioni Strutturali Multilivello",
    "section": "55.3 Un Esempio Pratico",
    "text": "55.3 Un Esempio Pratico\nIn questo capitolo introdurremo l’implementazione in lavaan per l’analisi SEM multilivello seguendo il tutorial fornito sul sito di lavaan.org.\n\nUtilizzeremo un set di dati artificiali ricavato dal sito di MPlus.\n\n\ndat &lt;- read.table(\"http://statmodel.com/usersguide/chap9/ex9.6.dat\")\nnames(dat) &lt;- c(\"y1\", \"y2\", \"y3\", \"y4\", \"x1\", \"x2\", \"w\", \"clus\")\nhead(dat)\n\n\nA data.frame: 6 x 8\n\n\n\ny1\ny2\ny3\ny4\nx1\nx2\nw\nclus\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n\n\n\n\n1\n2.2033\n1.859\n1.7385\n2.245\n1.143\n-0.797\n-0.150\n1\n\n\n2\n1.9349\n2.128\n0.0831\n2.509\n1.949\n-0.123\n-0.150\n1\n\n\n3\n0.3220\n0.977\n-0.8354\n0.558\n-0.716\n-0.767\n-0.150\n1\n\n\n4\n0.0732\n-1.743\n-2.3103\n-1.514\n-2.649\n0.638\n-0.150\n1\n\n\n5\n-1.2149\n0.453\n0.3726\n-1.790\n-0.263\n0.303\n-0.150\n1\n\n\n6\n0.2983\n-1.820\n0.5613\n-2.091\n-0.945\n1.363\n0.319\n2\n\n\n\n\n\nIl data frame è costituito da 1000 righe:\n\ndim(dat) |&gt; print()\n\n[1] 1000    8\n\n\n\nCi sono 110 cluster (clus), il che significa che ci sono misure ripetute per ciascun cluster (possiamo immaginare che i cluster corrispondano ai soggetti):\n\nlength(unique(dat$clus))\n\n110\n\n\n\nAnalizzeremo questi dati mediante un modello di equazioni strutturali multilivello. Iniziamo a definire il modello SEM appropriato per questi dati.\n\nmodel &lt;- \"\n    level: 1\n        fw =~ y1 + y2 + y3 + y4\n        fw ~ x1 + x2\n\n    level: 2\n        fb =~ y1 + y2 + y3 + y4\n\n    # optional\n    y1 ~~ 0*y1\n    y2 ~~ 0*y2\n    y3 ~~ 0*y3\n    y4 ~~ 0*y4\n    fb ~ w\n\"\n\nQuesta sintassi del modello è strutturata su due livelli, uno per il livello 1 (intra-cluster) e uno per il livello 2 (inter-cluster). All’interno di ciascun livello, è possibile specificare un modello come nel caso a livello singolo, ma con una distinzione importante: ogni livello rappresenta fonti di variabilità differenti, in questo caso tra le misurazioni individuali (livello 1) e le differenze tra gruppi o cluster (livello 2).\n\n55.3.1 Spiegazione dei livelli\n\nLivello 1 (Intra-cluster):\n\nFattore latente fw: Al livello individuale, fw è un fattore latente che riflette la variazione tra le quattro variabili osservate y1, y2, y3 e y4. La sintassi fw =~ y1 + y2 + y3 + y4 indica che fw è il costrutto latente che sottende questi indicatori osservabili.\nEffetto dei predittori x1 e x2: Al livello individuale, fw è modellato in funzione dei predittori x1 e x2 (fw ~ x1 + x2), che rappresentano variabili a livello intra-cluster che possono influenzare il fattore latente fw. Questo permette di catturare come i predittori influenzano la variabilità nelle risposte individuali.\n\nLivello 2 (Inter-cluster):\n\nFattore latente fb: Al livello del cluster, fb rappresenta un secondo fattore latente che viene definito anch’esso dalle variabili y1, y2, y3 e y4, ma con una prospettiva inter-cluster. Questo livello considera quindi la variazione nei punteggi medi del cluster piuttosto che nelle risposte individuali.\nEffetto del predittore w: Al livello di cluster, fb è modellato come funzione del predittore w (fb ~ w), che rappresenta una variabile esplicativa per le differenze tra cluster.\n\n\n\n\n55.3.2 Parte opzionale\nLa sezione opzionale, che include espressioni come y1 ~~ 0*y1, specifica la varianza residua delle variabili osservate y1, y2, y3 e y4 al livello di cluster. L’uso di 0*y1, 0*y2, etc., indica che la varianza residua a livello inter-cluster viene fissata a zero, assumendo che tutta la varianza tra cluster sia spiegata da fb.\nIn sintesi, questo modello permette di distinguere come i fattori latenti (fw e fb) siano influenzati rispettivamente da variabili a livello individuale e cluster, consentendo di modellare simultaneamente la variabilità intra- e inter-cluster.\n\n\n55.3.3 Coefficiente di Correlazione Intraclasse (ICC)\nL’analisi SEM Multilivello permette di calcolare il Coefficiente di Correlazione Intraclasse (ICC), una misura statistica utile in studi dove i dati sono raggruppati in cluster o gruppi (come ad esempio soggetti all’interno di classi o pazienti all’interno di ospedali). L’ICC valuta il grado di somiglianza o omogeneità delle misurazioni all’interno di ciascun gruppo rispetto alla variazione totale nei dati.\nPiù precisamente, l’ICC quantifica la proporzione della varianza totale che può essere attribuita alle differenze tra i gruppi piuttosto che a quelle all’interno dei gruppi. Quando l’ICC è elevato, significa che una parte rilevante della varianza osservata nei dati deriva dalle differenze tra i gruppi. In questo caso, le misurazioni all’interno dello stesso gruppo tendono a essere più simili tra loro rispetto a quelle di gruppi differenti. Al contrario, un ICC basso indica che la varianza è in gran parte dovuta alle differenze individuali all’interno dei gruppi, suggerendo una scarsa influenza del raggruppamento.\nIn sintesi, l’ICC è un indice di quanto “forte” sia l’effetto del raggruppamento sulle misurazioni, informando sulla necessità di considerare la struttura multilivello dei dati nell’analisi.\n\n\n55.3.4 ICC nei Modelli SEM Multilivello\nIl calcolo dell’ICC (Intra-Class Correlation) per ciascuna variabile osservata in un modello SEM multilivello è essenziale per comprendere la struttura gerarchica dei dati e la variabilità tra i gruppi. L’ICC quantifica la proporzione della varianza totale di una variabile che può essere attribuita a differenze tra gruppi piuttosto che a variazioni individuali all’interno dei gruppi. Un ICC elevato per una variabile suggerisce che l’appartenenza a un determinato gruppo (come una scuola, una famiglia o un ospedale) ha un’influenza rilevante su quella variabile, indicando che una parte consistente della variazione osservata è dovuta alle differenze tra gruppi piuttosto che alle differenze tra individui all’interno di ciascun gruppo.\nIn pratica, l’ICC è un criterio utile per determinare l’adeguatezza di un modello multilivello. Un ICC basso suggerisce che la variabilità tra i gruppi è limitata e che potrebbe non essere necessario utilizzare un modello multilivello complesso, in quanto le differenze individuali rappresentano la maggior parte della variabilità. Al contrario, un ICC alto indica che la struttura a cluster dei dati è rilevante e che ignorarla potrebbe portare a stime distorte e a conclusioni potenzialmente errate.\nUtilizzando l’ICC come guida, i ricercatori possono decidere se e in che misura adottare un approccio multilivello per rappresentare in modo accurato e affidabile la varianza attribuibile a fattori tra e intra-gruppo, ottenendo così una comprensione più precisa dei fenomeni studiati.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Modelli di Equazioni Strutturali Multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/08_multilevel_sem.html#calcolo-dellicc-con-lmer",
    "href": "chapters/sem/08_multilevel_sem.html#calcolo-dellicc-con-lmer",
    "title": "55  Modelli di Equazioni Strutturali Multilivello",
    "section": "55.4 Calcolo dell’ICC con lmer",
    "text": "55.4 Calcolo dell’ICC con lmer\nPer iniziare, poniamoci il problema di calcolare l’ICC mediante un modello lineare multilivello. Svolgeremo questi calcoli con lmer. Il modello lmer considera ogni variabile osservata separatamente, fornendo un’analisi indipendente per ciascuna. Per y1, per esempio, abbiamo:\n\nmodel_lmer &lt;- lmer(y1 ~ 1 + (1 | clus), data = dat)\n\nCalcoliamo l’ICC:\n\n varianza_cluster &lt;- VarCorr(model_lmer)$clus[1]\n varianza_residua &lt;- attr(VarCorr(model_lmer), \"sc\")^2\n ICC &lt;- varianza_cluster / (varianza_cluster + varianza_residua)\n ICC\n\n0.129536196128802\n\n\n\nIl Coefficiente di Correlazione Intraclass (ICC) di 0.129 per la variabile y1 significa che circa il 12.9% della variazione totale in y1 è ascrivibile alle differenze tra gli studenti, considerati come unità separate o cluster individuali. Questa percentuale relativamente modesta della variazione totale suggerisce che le caratteristiche o i comportamenti individuali degli studenti spiegano solo una piccola parte della variazione osservata in y1.\nUn ICC di questo livello, che si può considerare relativamente basso, implica che la maggior parte della variazione nella variabile non è legata in modo sostanziale alle differenze tra gli studenti. Questo può essere indicativo del fatto che altri fattori, esterni alle caratteristiche individuali degli studenti, giocano un ruolo più determinante. In un contesto educativo, ad esempio, questo potrebbe suggerire che elementi come l’ambiente scolastico, le metodologie didattiche impiegate, o le specificità del programma di studi, hanno un impatto maggiore sulla variazione di y1 rispetto alle differenze individuali tra gli studenti.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Modelli di Equazioni Strutturali Multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/08_multilevel_sem.html#calcolo-dellicc-con-sem",
    "href": "chapters/sem/08_multilevel_sem.html#calcolo-dellicc-con-sem",
    "title": "55  Modelli di Equazioni Strutturali Multilivello",
    "section": "55.5 Calcolo dell’ICC con sem",
    "text": "55.5 Calcolo dell’ICC con sem\nOra calcoliamo l’ICC utilizzando un modello SEM multilivello. Per adattare questo modello, è necessario aggiungere l’argomento cluster= nella chiamata alla funzione sem di lavaan. Questo argomento specifica la variabile di raggruppamento, permettendo al modello di tenere conto della struttura a cluster dei dati e di stimare l’ICC per ciascuna variabile osservata.\n\n fit &lt;- sem(model,\n     data = dat,\n     cluster = \"clus\",\n     fixed.x = FALSE\n )\n\nL’argomento fixed.x controlla come vengono trattate le variabili predittive (o esogene) all’interno del modello.\nQuando fixed.x = FALSE, si sta indicando a lavaan di trattare le variabili esogene non come valori fissi, ma come variabili aleatorie con una propria varianza e covarianza da stimare nel modello. Questo significa che le variabili esogene non sono considerate “date” o senza errore, ma il modello tiene conto della loro variabilità.\n\nfixed.x = TRUE (impostazione predefinita in lavaan):\n\nLe variabili esogene sono considerate senza errore (cioè come dati “fissi”).\nNon si stima la varianza delle variabili esogene.\nQuesto approccio è tipico nei modelli di regressione classici, dove le variabili predittive sono trattate come note e prive di errore.\n\nfixed.x = FALSE:\n\nLe variabili esogene sono considerate come variabili aleatorie con errori di misurazione, quindi la loro varianza e covarianza vengono stimate.\nQuesto approccio è più realistico in molti contesti psicologici e sociali, dove è ragionevole assumere che anche le variabili esogene possano contenere errori.\n\n\nIn molte situazioni di ricerca, le variabili esogene (come i punteggi dei questionari o le misure di osservazione) non sono perfettamente accurate e possono contenere errore. Impostare fixed.x = FALSE consente di modellare questa incertezza, offrendo una rappresentazione più realistica dei dati.\n\nsummary(fit) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 27 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        26\n\n  Number of observations                          1000\n  Number of clusters [clus]                        110\n\nModel Test User Model:\n                                                      \n  Test statistic                                 3.863\n  Degrees of freedom                                17\n  P-value (Chi-square)                           1.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\n\nLevel 1 [within]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  fw =~                                               \n    y1                1.000                           \n    y2                0.999    0.033   30.735    0.000\n    y3                0.995    0.033   29.804    0.000\n    y4                1.017    0.033   30.364    0.000\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  fw ~                                                \n    x1                0.973    0.042   23.287    0.000\n    x2                0.510    0.038   13.422    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  x1 ~~                                               \n    x2                0.032    0.032    1.014    0.311\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    x1                0.007    0.031    0.222    0.825\n    x2                0.014    0.032    0.440    0.660\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .y1                0.981    0.057   17.151    0.000\n   .y2                0.948    0.056   17.015    0.000\n   .y3                1.070    0.060   17.700    0.000\n   .y4                1.014    0.059   17.182    0.000\n   .fw                0.980    0.071   13.888    0.000\n    x1                0.985    0.044   22.361    0.000\n    x2                1.017    0.045   22.361    0.000\n\n\nLevel 2 [clus]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  fb =~                                               \n    y1                1.000                           \n    y2                0.960    0.073   13.078    0.000\n    y3                0.924    0.074   12.452    0.000\n    y4                0.949    0.075   12.631    0.000\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  fb ~                                                \n    w                 0.344    0.078    4.429    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .y1               -0.083    0.074   -1.128    0.259\n   .y2               -0.077    0.071   -1.081    0.280\n   .y3               -0.045    0.071   -0.637    0.524\n   .y4               -0.030    0.072   -0.418    0.676\n    w                 0.006    0.086    0.070    0.944\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .y1                0.000                           \n   .y2                0.000                           \n   .y3                0.000                           \n   .y4                0.000                           \n   .fb                0.361    0.078    4.643    0.000\n    w                 0.815    0.110    7.416    0.000\n\n\n\n\nfitMeasures(fit) |&gt;\n    print()\n\n                 npar                  fmin                 chisq \n               26.000                 3.913                 3.863 \n                   df                pvalue        baseline.chisq \n               17.000                 1.000              3283.563 \n          baseline.df       baseline.pvalue                   cfi \n               24.000                 0.000                 1.000 \n                  tli                  nnfi                   rfi \n                1.006                 1.006                 0.998 \n                  nfi                  pnfi                   ifi \n                0.999                 0.707                 1.004 \n                  rni                  logl     unrestricted.logl \n                1.004             -9527.429             -9525.497 \n                  aic                   bic                ntotal \n            19106.857             19234.459              1000.000 \n                 bic2                 rmsea        rmsea.ci.lower \n            19151.882                 0.000                 0.000 \n       rmsea.ci.upper        rmsea.ci.level          rmsea.pvalue \n                0.000                 0.900                 1.000 \n       rmsea.close.h0 rmsea.notclose.pvalue     rmsea.notclose.h0 \n                0.050                 0.000                 0.080 \n                 srmr           srmr_within          srmr_between \n                0.022                 0.004                 0.017 \n\n\nQuando calcoliamo l’ICC utilizzando lavaan, otteniamo valori distinti per ciascuna variabile osservata all’interno del modello multilivello. Questi valori rappresentano la proporzione di varianza in ogni variabile che è attribuibile alle differenze tra i gruppi, considerando le relazioni specificate nel modello SEM. In altre parole, l’ICC di ciascun item riflette quanto le differenze tra i gruppi influenzano quella particolare variabile, nel contesto delle dipendenze definite dal modello.\nPer ottenere l’ICC per ciascuno dei quattro item, è possibile utilizzare il comando:\n\nlavInspect(fit, \"icc\") |&gt;\n    print()\n\n   y1    y2    y3    y4    x1    x2 \n0.125 0.121 0.106 0.115 0.000 0.000 \n\n\n\nNel caso di y1, la stima di ICC fornita dal modello SEM multilivello è molto simile al risultato ottenuto con lmer.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Modelli di Equazioni Strutturali Multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/08_multilevel_sem.html#riflessioni-conclusive",
    "href": "chapters/sem/08_multilevel_sem.html#riflessioni-conclusive",
    "title": "55  Modelli di Equazioni Strutturali Multilivello",
    "section": "55.6 Riflessioni Conclusive",
    "text": "55.6 Riflessioni Conclusive\nIn questo capitolo, abbiamo illustrato il modello di equazioni strutturali multilivello utilizzando lavaan. Come evidenziato dall’esempio, l’implementazione in lavaan è molto diretta, richiedendo solo l’inclusione dell’opzione cluster nella funzione sem. È importante sottolineare che, al momento, lavaan supporta solo modelli SEM a due livelli.\nNell’ambito dei modelli SEM multilivello, abbiamo visto come l’interpretazione dei coefficienti di correlazione intra-classe (ICC) possa fornire intuizioni significative sulla variazione dei dati all’interno di gruppi o cluster. Un ICC basso, come quello osservato nell’esempio (0.129), indica che una porzione minore della variazione totale è attribuibile alle differenze tra i cluster. Nel contesto specifico dei nostri dati, dove ogni studente è considerato un cluster individuale, ciò suggerisce che fattori esterni agli studenti stessi potrebbero giocare un ruolo più significativo nella variazione osservata rispetto alle caratteristiche individuali degli studenti.\nIn conclusione, la modellazione di equazioni strutturali multilivello è uno strumento potente e flessibile nell’analisi di dati strutturati gerarchicamente. lavaan, sebbene limitato ai modelli a due livelli, fornisce un approccio accessibile e diretto per questi tipi di analisi. Per modelli più complessi e a più livelli, Mplus offre soluzioni alternative che possono gestire una gamma più ampia di esigenze analitiche.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Modelli di Equazioni Strutturali Multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/08_multilevel_sem.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/sem/08_multilevel_sem.html#informazioni-sullambiente-di-sviluppo",
    "title": "55  Modelli di Equazioni Strutturali Multilivello",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lme4_1.1-35.5     Matrix_1.7-1      MASS_7.3-61       viridis_0.6.5    \n [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n [9] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n[13] lavaan_0.6-19     psych_2.4.6.26    scales_1.3.0      markdown_1.13    \n[17] knitr_1.49        lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1    \n[21] dplyr_1.1.4       purrr_1.0.2       readr_2.1.5       tidyr_1.3.1      \n[25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.17.1   jsonlite_1.8.9      magrittr_2.0.3     \n  [4] TH.data_1.1-2       estimability_1.5.1  farver_2.1.2       \n  [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n [19] emmeans_1.10.5      zoo_1.8-12          uuid_1.2-1         \n [22] igraph_2.1.1        mime_0.12           lifecycle_1.0.4    \n [25] pkgconfig_2.0.3     R6_2.5.1            fastmap_1.2.0      \n [28] shiny_1.9.1         digest_0.6.37       OpenMx_2.21.13     \n [31] fdrtool_1.2.18      colorspace_2.1-1    rprojroot_2.0.4    \n [34] Hmisc_5.2-0         fansi_1.0.6         timechange_0.3.0   \n [37] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n [40] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n [43] carData_3.0-5       ggsignif_0.6.4      corpcor_1.6.10     \n [46] gtools_3.9.5        tools_4.4.2         pbivnorm_0.6.0     \n [49] foreign_0.8-87      zip_2.3.1           httpuv_1.6.15      \n [52] nnet_7.3-19         glue_1.8.0          quadprog_1.5-8     \n [55] promises_1.3.0      nlme_3.1-166        lisrelToR_0.3      \n [58] grid_4.4.2          pbdZMQ_0.3-13       checkmate_2.3.2    \n [61] cluster_2.1.6       reshape2_1.4.4      generics_0.1.3     \n [64] gtable_0.3.6        tzdb_0.4.0          data.table_1.16.2  \n [67] hms_1.1.3           car_3.1-3           utf8_1.2.4         \n [70] sem_3.1-16          pillar_1.9.0        IRdisplay_1.1      \n [73] rockchalk_1.8.157   later_1.3.2         splines_4.4.2      \n [76] cherryblossom_0.1.0 lattice_0.22-6      survival_3.7-0     \n [79] kutils_1.73         tidyselect_1.2.1    miniUI_0.1.1.1     \n [82] pbapply_1.7-2       airports_0.1.0      stats4_4.4.2       \n [85] xfun_0.49           qgraph_1.9.8        arm_1.14-4         \n [88] stringi_1.8.4       pacman_0.5.1        boot_1.3-31        \n [91] evaluate_1.0.1      codetools_0.2-20    mi_1.1             \n [94] cli_3.6.3           RcppParallel_5.1.9  IRkernel_1.3.2     \n [97] rpart_4.1.23        xtable_1.8-4        repr_1.1.7         \n[100] munsell_0.5.1       Rcpp_1.0.13-1       coda_0.19-4.1      \n[103] png_0.1-8           XML_3.99-0.17       parallel_4.4.2     \n[106] usdata_0.3.1        jpeg_0.1-10         mvtnorm_1.3-2      \n[109] openxlsx_4.2.7.1    crayon_1.5.3        openintro_2.5.0    \n[112] rlang_1.1.4         multcomp_1.4-26     mnormt_2.1.1",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Modelli di Equazioni Strutturali Multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/09_structural_regr.html",
    "href": "chapters/sem/09_structural_regr.html",
    "title": "56  Modelli di Regressione Strutturale",
    "section": "",
    "text": "56.1 Introduzione\nUn modello comunemente utilizzato nell’analisi delle equazioni strutturali (SEM) è il modello di regressione strutturale (Sr), noto anche come modello di percorso con variabili latenti o modello LISREL completo. Questo tipo di modello combina due componenti principali:\nIl capitolo inizia affrontando la specificazione dei modelli Sr con indicatori continui, analizzando i requisiti per garantirne l’identificazione. In particolare, viene discusso come stabilire condizioni che consentano una stima coerente e interpretabile dei parametri del modello.\nSuccessivamente, vengono presentate due strategie distinte per analizzare modelli Sr completi, in cui tutte le variabili nel modello strutturale sono fattori comuni con molteplici indicatori. Queste strategie si concentrano su:\nInoltre, il capitolo esplora i modelli Sr parziali, in cui alcune variabili nella parte strutturale del modello sono rappresentate da indicatori singoli anziché da fattori comuni con molteplici indicatori. Viene illustrato un metodo specifico per gestire questo tipo di indicatori, permettendo di controllare esplicitamente gli errori di misurazione associati senza influire negativamente sull’adattamento globale del modello.\nIl capitolo evidenzia l’importanza di distinguere chiaramente tra le componenti di misurazione e strutturali nei modelli Sr. Questo approccio non solo migliora la comprensione delle relazioni tra variabili latenti, ma aiuta anche a mitigare il rischio di errori di specificazione, aumentando l’affidabilità dei risultati ottenuti dall’analisi SEM.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Modelli di Regressione Strutturale</span>"
    ]
  },
  {
    "objectID": "chapters/sem/09_structural_regr.html#introduzione",
    "href": "chapters/sem/09_structural_regr.html#introduzione",
    "title": "56  Modelli di Regressione Strutturale",
    "section": "",
    "text": "Parte strutturale: rappresenta le ipotesi sugli effetti diretti e indiretti tra le variabili latenti o osservate. Questa sezione descrive le relazioni causali tra i fattori comuni.\n\nParte di misurazione: rappresenta la relazione tra i fattori latenti e i loro indicatori osservati, definendo come i fattori comuni si manifestano attraverso le variabili misurate.\n\n\n\n\n\nIdentificazione delle fonti di errore di specificazione: analisi che separa la valutazione della parte di misurazione (validità degli indicatori) dall’analisi della parte strutturale (relazioni tra variabili latenti).\n\nValutazione del modello: tecniche per comprendere come gli errori nella parte di misurazione influenzino le inferenze strutturali.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Modelli di Regressione Strutturale</span>"
    ]
  },
  {
    "objectID": "chapters/sem/09_structural_regr.html#modello-di-regressione-strutturale-completo",
    "href": "chapters/sem/09_structural_regr.html#modello-di-regressione-strutturale-completo",
    "title": "56  Modelli di Regressione Strutturale",
    "section": "\n56.2 Modello di Regressione Strutturale Completo",
    "text": "56.2 Modello di Regressione Strutturale Completo\nNella Figura 56.1 (a) è rappresentato un modello di percorso con variabili manifeste. Viene assunto che la variabile esogena X1 sia misurata senza errore, sebbene questa assunzione sia spesso violata nella pratica. Le variabili endogene nel modello, come Y1 e Y4, possono avere errori casuali che si manifestano nelle loro perturbazioni.\n\n\n\n\n\nFigura 56.1: Esempi di un modello di percorso con variabili manifeste (a) e di un corrispondente modello di regressione strutturale completo con indicatori multipli per ogni fattore comune nella parte strutturale (b) (Figura tratta da Kline, 2023).\n\n\nLa Figura 56.1 (b) illustra un modello di Regressione Strutturale (SR) completo, che integra sia componenti strutturali sia di misurazione. In questo modello SR, a differenza del modello di percorso, ciascun indicatore (X1, Y1, Y4) è definito come uno tra numerosi indicatori associati a un fattore comune. Di conseguenza, tutte le variabili osservabili in questa figura includono termini di errore.\nNella parte strutturale del modello, presentata nella Figura 56.1 (b), si osserva la rappresentazione degli stessi schemi di effetti causali diretti e indiretti trovati nel modello di percorso mostrato nella 56.1, ma applicati ai fattori comuni.\nPer quanto riguarda l’analisi delle medie, le osservazioni e i parametri nei modelli SR sono trattati allo stesso modo di quelli nei modelli di percorso e nei modelli di Analisi Fattoriale Confermativa (CFA), conformemente alle regole precedentemente stabilite.\nL’identificazione di un modello SR completo avviene quando sia la sua componente di misurazione, riformulata come un modello CFA, sia la parte strutturale risultano identificate. La regola di identificazione in due fasi implica che, per determinare se un modello SR completo sia identificato, è necessario esaminare separatamente ciascuna delle sue parti, ovvero quelle di misurazione e strutturale.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Modelli di Regressione Strutturale</span>"
    ]
  },
  {
    "objectID": "chapters/sem/09_structural_regr.html#modellazione-in-due-fasi",
    "href": "chapters/sem/09_structural_regr.html#modellazione-in-due-fasi",
    "title": "56  Modelli di Regressione Strutturale",
    "section": "\n56.3 Modellazione in Due Fasi",
    "text": "56.3 Modellazione in Due Fasi\nImmaginiamo che un ricercatore abbia definito un modello di Regressione Strutturale (SR) completo, come mostrato nella Figura 56.1(a). Dopo aver raccolto i dati, il ricercatore adotta un approccio monofase per analizzare il modello, eseguendo una stima simultanea delle componenti di misurazione e strutturali. Tuttavia, i risultati rivelano che il modello non si adatta bene ai dati. Ciò solleva interrogativi sulla localizzazione del problema: è nella parte di misurazione, nella parte strutturale, o in entrambe? Identificare la fonte del problema con precisione può essere complesso usando un approccio monofase.\n\n\n\n\n\nFigura 56.2: Valutazione della regola in due fasi per l’identificazione di un modello di regressione strutturale completo presentato con simbolismo grafico compatto per i termini di errore degli indicatori nella parte di misurazione e le perturbazioni nella parte strutturale (Figura tratta da Kline, 2023).\n\n\nLa modellazione in due fasi, proposta da Anderson e Gerbing (1988), affronta questa difficoltà separando l’analisi in due momenti distinti.\nNel primo passaggio, il modello SR viene riformulato in un modello CFA per testare esclusivamente le relazioni tra i costrutti latenti e i loro indicatori. Un cattivo adattamento del modello CFA indica problemi nelle ipotesi sulla misurazione (ad esempio, carichi fattoriali errati o struttura del modello incoerente). Solo se il modello CFA è valido si passa al secondo passaggio, in cui vengono analizzate le relazioni strutturali tra i costrutti latenti. In questa fase, si confrontano il modello SR originale e varianti alternative per identificare la struttura più adatta.\n\n56.3.1 Limiti e Sfide della Modellazione in Due Fasi\nLa modellazione in due fasi presenta alcuni limiti. Il processo richiede numerose decisioni, come la riformulazione dei modelli CFA e SR, che possono generare un “giardino dei sentieri che si biforcano”, aumentando il rischio di interpretazioni errate. Inoltre, test ripetuti sullo stesso dataset possono portare a risultati influenzati da variazioni casuali.\nUn altro problema riguarda i modelli equivalenti. Ad esempio, un modello CFA e un modello SR con una parte strutturale appena identificata possono risultare indistinguibili statisticamente. In questi casi, la scelta tra i modelli deve basarsi su criteri teorici o sul design dello studio.\nInfine, se i carichi fattoriali cambiano tra modelli strutturali differenti, ciò indica che il modello di misurazione non è stabile, complicando ulteriormente l’interpretazione.\n\n56.3.2 Considerazioni Finali\nLa modellazione in due fasi offre un metodo utile per diagnosticare e risolvere problemi nei modelli SR, ma va utilizzata con attenzione. Le statistiche di adattamento, come il chi-quadro, il CFI o il RMSEA, sono spesso più sensibili alla parte di misurazione rispetto a quella strutturale. Inoltre, soglie standard per questi indici non sono sempre applicabili universalmente, e l’interpretazione dipende dal tipo di modello e dai dati.\nL’approccio bifase permette di isolare e analizzare separatamente i problemi nelle componenti di misurazione e strutturali, ma richiede un’analisi teoricamente solida per evitare di incorrere in errori metodologici o interpretativi.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Modelli di Regressione Strutturale</span>"
    ]
  },
  {
    "objectID": "chapters/sem/09_structural_regr.html#una-applicazione-concreta",
    "href": "chapters/sem/09_structural_regr.html#una-applicazione-concreta",
    "title": "56  Modelli di Regressione Strutturale",
    "section": "\n56.4 Una Applicazione Concreta",
    "text": "56.4 Una Applicazione Concreta\nLa Figura 56.3 illustra un modello SR (Regressione Strutturale) iniziale che esplora il rendimento scolastico e l’adattamento in aula di studenti di età media corrispondente ai gradi 7-8. Il modello considera l’influenza dell’abilità cognitiva generale e del livello di rischio di disturbi psicopatologici. Uno degli indicatori di rischio deriva dalla diagnosi di disturbi psichiatrici maggiori nei genitori, mentre il secondo è basato sul livello socio-economico (SES) della famiglia, con punteggi più alti che indicano un SES inferiore. Le abilità cognitive sono valutate tramite i punteggi in ragionamento verbale, analisi visivo-spaziale e memoria, ottenuti da un test di QI somministrato individualmente.\n\n\n\n\n\nFigura 56.3: Modello iniziale completo di regressione strutturale del rendimento scolastico e dell’adattamento in classe come funzione dell’abilità cognitiva e del rischio di psicopatologia (Figura tratta da Kline, 2023).\n\n\nIl modello comprende due fattori endogeni: il rendimento scolastico, valutato attraverso test standardizzati di lettura, aritmetica e ortografia, e l’adattamento in classe, misurato con tre indicatori forniti dagli insegnanti riguardo alla motivazione, stabilità emotiva e qualità delle relazioni sociali degli studenti. In questo modello strutturale, sia il rendimento scolastico sia l’adattamento in classe sono influenzati dall’abilità cognitiva e dal rischio, ma non vi è un effetto diretto o una covarianza delle perturbazioni tra questi due fattori endogeni, indicando che eventuali associazioni tra di essi sono attribuibili alle loro cause comuni, i fattori esogeni.\n\n# input the correlations in lower diagnonal form\nworlandLower.cor &lt;- \"\n1.00\n .70 1.00\n .65  .60 1.00\n .55  .50  .45 1.00\n .50  .45  .40  .70 1.00\n .35  .35  .30  .55  .50 1.00\n .30  .30  .30  .50  .45  .44 1.00\n .25  .20  .22  .41  .28  .34  .40 1.00\n .35  .32  .32  .48  .45  .42  .60  .45 1.00\n-.25 -.24 -.22 -.21 -.18 -.15 -.15 -.12 -.17 1.00\n-.22 -.26 -.30 -.25 -.22 -.18 -.17 -.14 -.20  .42 1.00 \"\n\n# name the variables and convert to full correlation matrix\nworland.cor &lt;- lavaan::getCov(worlandLower.cor, names = c(\n    \"verbal\", \"visual\",\n    \"memory\", \"read\", \"math\", \"spell\", \"motive\", \"harmony\", \"stable\", \"parent\", \"ses\"\n))\n\n# add the standard deviations and convert to covariances\nworland.cov &lt;- lavaan::cor2cov(worland.cor,\n    sds = c(\n        13.75, 14.80, 12.60, 14.90, 15.25, 13.85, 9.50, 11.10, 8.70,\n        12.00, 8.50\n    )\n)\n\nPrimo Passaggio nella modellazione in due fasi per un modello SR completo di rendimento scolastico e adattamento in classe\n\n# 4-factor CFA\nworlandCFA.model &lt;- \"\n    Cognitive =~ verbal + visual + memory\n    Achieve =~ read + math + spell\n    Adjust =~ motive + harmony + stable\n    Risk =~ parent + ses \n \"\n\n\nworlandCFA &lt;- lavaan::cfa(worlandCFA.model,\n    sample.cov = worland.cov,\n    sample.nobs = 158\n)\n\n\nsemPlot::semPaths(worlandCFA,\n    what = \"col\", whatLabels = \"par\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\n\nfitMeasures(worlandCFA, c(\"chisq\", \"df\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt; print()\n#&gt;  chisq     df    cfi    tli  rmsea   srmr \n#&gt; 16.212 38.000  1.000  1.049  0.000  0.023\n\n\n# predicted correlation matrix for factors\nlavaan::lavInspect(worlandCFA, \"cor.lv\") |&gt; print()\n#&gt;           Cogntv Achiev Adjust   Risk\n#&gt; Cognitive  1.000                     \n#&gt; Achieve    0.703  1.000              \n#&gt; Adjust     0.500  0.751  1.000       \n#&gt; Risk      -0.459 -0.401 -0.349  1.000\n\n\nlavaan::residuals(worlandCFA, type = \"standardized.mplus\") |&gt; print()\n#&gt; $type\n#&gt; [1] \"standardized.mplus\"\n#&gt; \n#&gt; $cov\n#&gt;         verbal visual memory   read   math  spell motive harmny stable\n#&gt; verbal   0.000                                                        \n#&gt; visual   0.000  0.000                                                 \n#&gt; memory   0.123 -0.130  0.000                                          \n#&gt; read     0.598  0.113 -0.285  0.000                                   \n#&gt; math     0.505  0.038 -0.377  0.597  0.000                            \n#&gt; spell   -0.952 -0.255 -0.704 -0.667 -0.206  0.000                     \n#&gt; motive  -0.821 -0.157  0.310 -0.042 -0.078  1.484  0.000              \n#&gt; harmony  0.117 -0.453  0.171  0.942 -1.179  1.196 -1.010  0.000       \n#&gt; stable   0.285  0.112  0.566 -1.893 -0.467  0.942  0.422  0.436  0.000\n#&gt; parent  -0.092 -0.260 -0.157  0.234  0.379  0.157  0.301  0.020 -0.022\n#&gt; ses      1.890 -0.247 -1.417 -0.370 -0.114 -0.112  0.213 -0.125 -0.362\n#&gt;         parent    ses\n#&gt; verbal               \n#&gt; visual               \n#&gt; memory               \n#&gt; read                 \n#&gt; math                 \n#&gt; spell                \n#&gt; motive               \n#&gt; harmony              \n#&gt; stable               \n#&gt; parent   0.000       \n#&gt; ses      0.000  0.000\n\n\nlavaan::residuals(worlandCFA, type = \"cor.bollen\") |&gt; print()\n#&gt; $type\n#&gt; [1] \"cor.bollen\"\n#&gt; \n#&gt; $cov\n#&gt;         verbal visual memory   read   math  spell motive harmny stable\n#&gt; verbal   0.000                                                        \n#&gt; visual   0.000  0.000                                                 \n#&gt; memory   0.001 -0.002  0.000                                          \n#&gt; read     0.015  0.003 -0.010  0.000                                   \n#&gt; math     0.017  0.001 -0.016  0.007  0.000                            \n#&gt; spell   -0.040 -0.012 -0.035 -0.010 -0.006  0.000                     \n#&gt; motive  -0.031 -0.007  0.016 -0.001 -0.003  0.075  0.000              \n#&gt; harmony  0.006 -0.026  0.010  0.041 -0.053  0.071 -0.027  0.000       \n#&gt; stable   0.011  0.005  0.028 -0.034 -0.015  0.045  0.005  0.012  0.000\n#&gt; parent  -0.003 -0.010 -0.007  0.008  0.017  0.009  0.015  0.001 -0.001\n#&gt; ses      0.050 -0.010 -0.068 -0.012 -0.005 -0.006  0.010 -0.008 -0.016\n#&gt;         parent    ses\n#&gt; verbal               \n#&gt; visual               \n#&gt; memory               \n#&gt; read                 \n#&gt; math                 \n#&gt; spell                \n#&gt; motive               \n#&gt; harmony              \n#&gt; stable               \n#&gt; parent   0.000       \n#&gt; ses      0.000  0.000\n\n\n# calculate factor reliability coefficients (semTools)\nsemTools::reliability(worlandCFA) |&gt; print()\n#&gt;        Cognitive Achieve Adjust   Risk\n#&gt; alpha     0.8463  0.8088 0.7249 0.5675\n#&gt; omega     0.8514  0.8208 0.7296 0.5770\n#&gt; omega2    0.8514  0.8208 0.7296 0.5770\n#&gt; omega3    0.8516  0.8228 0.7327 0.5770\n#&gt; avevar    0.6589  0.6101 0.4745 0.4095\n\nSecondo Passaggio nella modellazione in due fasi per un modello SR completo di rendimento scolastico e adattamento in classe\nI risultati del Passaggio 1 del metodo in due fasi, che si concentrava sul modello di misurazione, consentono di procedere all’analisi del modello SR originale, che prevede cinque percorsi nella Figura 56.3, nel Passaggio 2 del metodo. Anche questa seconda analisi ha portato a una soluzione ammissibile.\n\n# step 2a\n# 4-factor SR model with 5 paths among factors\n\n# by default, lavaan frees the disturbance covariance\n# between a pair of outcomes in a structural model\n# when there is no direct effect between them\n# thus, this parameter is explicitly fixed to zero\n# in this analysis\n\nworlandSRa_model &lt;- \"\n    # measurement part\n    Cognitive =~ verbal + visual + memory\n    Achieve =~ read + math + spell\n    Adjust =~ motive + harmony + stable\n    Risk =~ parent + ses\n    # structural part (5 paths)\n    Achieve ~ Cognitive + Risk\n    Adjust ~ Cognitive + Risk\n    # constrain disturbance covariance to zero\n    Adjust ~~ 0*Achieve \n\"\n\n\nworlandSRa &lt;- lavaan::sem(worlandSRa_model,\n    sample.cov = worland.cov,\n    sample.nobs = 158\n)\n\n\nsemPaths(\n    worlandSRa,\n    \"std\",\n    posCol = c(\"black\"),\n    edge.label.cex = 1.2,\n    sizeMan = 7,\n    edge.width = 0.4, # Set a fixed width for all arrows\n    fade = FALSE # Disable fading of the arrows\n)\n\n\n\n\n\n\n\n\nfitMeasures(worlandSRa, c(\"chisq\", \"df\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt; print()\n#&gt;  chisq     df    cfi    tli  rmsea   srmr \n#&gt; 37.320 39.000  1.000  1.004  0.000  0.045\n\n\nlavaan::residuals(worlandSRa, type = \"standardized.mplus\") |&gt; print()\n#&gt; $type\n#&gt; [1] \"standardized.mplus\"\n#&gt; \n#&gt; $cov\n#&gt;         verbal visual memory   read   math  spell motive harmny stable\n#&gt; verbal   0.002                                                        \n#&gt; visual   0.209  0.002                                                 \n#&gt; memory   0.204 -0.221  0.002                                          \n#&gt; read     0.671  0.115 -0.298  0.004                                   \n#&gt; math     0.562  0.043 -0.384  0.596  0.002                            \n#&gt; spell   -0.902 -0.252 -0.709 -0.670 -0.201  0.000                     \n#&gt; motive  -0.777 -0.158  0.304  0.030 -0.026  1.506  0.000              \n#&gt; harmony  0.132 -0.456  0.165  0.962 -1.151  1.208 -0.997  0.000       \n#&gt; stable   0.307  0.102  0.554 -1.797 -0.427  0.959  0.421  0.426  0.000\n#&gt; parent   0.229  0.003  0.057  0.544  0.602  0.308  0.446  0.118  0.158\n#&gt; ses      2.082  0.015 -1.204  0.009  0.133  0.043  0.362 -0.024 -0.143\n#&gt;         parent    ses\n#&gt; verbal               \n#&gt; visual               \n#&gt; memory               \n#&gt; read                 \n#&gt; math                 \n#&gt; spell                \n#&gt; motive               \n#&gt; harmony              \n#&gt; stable               \n#&gt; parent      NA       \n#&gt; ses      4.056  0.003\n\n\nlavaan::residuals(worlandSRa, type = \"cor.bollen\") |&gt; print()\n#&gt; $type\n#&gt; [1] \"cor.bollen\"\n#&gt; \n#&gt; $cov\n#&gt;         verbal visual memory   read   math  spell motive harmny stable\n#&gt; verbal   0.000                                                        \n#&gt; visual   0.002  0.000                                                 \n#&gt; memory   0.003 -0.004  0.000                                          \n#&gt; read     0.018  0.003 -0.010  0.000                                   \n#&gt; math     0.019  0.002 -0.016  0.007  0.000                            \n#&gt; spell   -0.038 -0.012 -0.036 -0.010 -0.006  0.000                     \n#&gt; motive  -0.029 -0.007  0.015  0.001 -0.001  0.076  0.000              \n#&gt; harmony  0.007 -0.026  0.010  0.042 -0.052  0.072 -0.026  0.000       \n#&gt; stable   0.012  0.004  0.027 -0.033 -0.013  0.046  0.005  0.012  0.000\n#&gt; parent   0.007  0.000  0.003  0.021  0.028  0.018  0.023  0.008  0.008\n#&gt; ses      0.059  0.001 -0.058  0.000  0.006  0.003  0.018 -0.002 -0.007\n#&gt;         parent    ses\n#&gt; verbal               \n#&gt; visual               \n#&gt; memory               \n#&gt; read                 \n#&gt; math                 \n#&gt; spell                \n#&gt; motive               \n#&gt; harmony              \n#&gt; stable               \n#&gt; parent   0.000       \n#&gt; ses      0.312  0.000\n\nSebbene gli indici di fit siano buoni, l’adattamento locale del modello con cinque percorsi tra i fattori è scarso. Ad esempio, i residui standardizzati per diverse coppie di indicatori dei fattori di rendimento e adattamento hanno spesso un valore maggiore di 2:\n\nLettura, Motivazione, 3.466\nOrtografia, Motivazione, 3.348\nLettura, Armonia, 2.903\n\nBasandosi su tutti questi risultati relativi all’adattamento globale e locale, il modello SR iniziale nella Figura 56.3 con cinque percorsi tra i fattori è rifiutato.\nEsaminiamo i modification indices.\n\nmodificationIndices(worlandSRa, sort = TRUE, minimum.value = 5)\n#&gt;         lhs op     rhs    mi    epc sepc.lv sepc.all sepc.nox\n#&gt; 125 Achieve  ~  Adjust 19.96 115.19   63.75   63.746   63.746\n#&gt; 120  parent ~~     ses 19.96  32.62   32.62    0.361    0.361\n#&gt; 126  Adjust  ~ Achieve 19.96  26.12   47.20   47.200   47.200\n\nI risultati dei modification indices mostrano che l’assenza di un percorso tra i fattori di rendimento e adattamento nella Figura 56.3 è chiaramente incoerente con i dati. Per aggiungere una covariazione tra i fattori di rendimento e adattamento abbiamo due opzioni: o aggiungere un effetto diretto tra i fattori o permettere alle loro perturbazioni di covariare. Ma sarebbe difficile giustificare un effetto diretto rispetto all’altro: scarse abilità scolastiche potrebbero peggiorare l’adattamento in classe tanto quanto i problemi comportamentali a scuola potrebbero influire negativamente sul rendimento. La specificazione di una causalità reciproca tra Rendimento e Adattamento renderebbe il modello strutturale non ricorsivo, ma il modello non sarebbe identificato senza imporre vincoli irrealistici. Riformuliamo dunque il modello della fig-kline-15-3 permettendo alle perturbazioni tra i fattori di rendimento e adattamento di covariare.\n\n# step 2b\n# 4-factor SR model with 6 paths among factors\n# this model is equivalent to the basic 4-factor\n# CFA measurement model analyzed in step 1\n\nworlandSRb_model &lt;- \"\n    # measurement part\n    Cognitive =~ verbal + visual + memory\n    Achieve =~ read + math + spell\n    Adjust =~ motive + harmony + stable\n    Risk =~ parent + ses\n    # structural part (6 paths)\n    Achieve ~ Cognitive + Risk\n    Adjust ~ Cognitive + Risk\n    Adjust ~~ Achieve \n\"\n\n\nworlandSRb &lt;- lavaan::sem(worlandSRb_model,\n    sample.cov = worland.cov,\n    sample.nobs = 158\n)\n\n\nsemPlot::semPaths(worlandSRb,\n    what = \"col\", whatLabels = \"par\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\n\nparameterEstimates(worlandSRb) |&gt; print()\n#&gt;          lhs op       rhs     est     se      z pvalue ci.lower ci.upper\n#&gt; 1  Cognitive =~    verbal   1.000  0.000     NA     NA    1.000    1.000\n#&gt; 2  Cognitive =~    visual   1.000  0.090 11.144  0.000    0.824    1.175\n#&gt; 3  Cognitive =~    memory   0.788  0.077 10.217  0.000    0.637    0.940\n#&gt; 4    Achieve =~      read   1.000  0.000     NA     NA    1.000    1.000\n#&gt; 5    Achieve =~      math   0.925  0.083 11.100  0.000    0.761    1.088\n#&gt; 6    Achieve =~     spell   0.678  0.080  8.480  0.000    0.521    0.835\n#&gt; 7     Adjust =~    motive   1.000  0.000     NA     NA    1.000    1.000\n#&gt; 8     Adjust =~   harmony   0.861  0.136  6.311  0.000    0.593    1.128\n#&gt; 9     Adjust =~    stable   0.940  0.114  8.231  0.000    0.716    1.164\n#&gt; 10      Risk =~    parent   1.000  0.000     NA     NA    1.000    1.000\n#&gt; 11      Risk =~       ses   0.773  0.224  3.445  0.001    0.333    1.212\n#&gt; 12   Achieve  ~ Cognitive   0.719  0.109  6.574  0.000    0.504    0.933\n#&gt; 13   Achieve  ~      Risk  -0.175  0.190 -0.922  0.357   -0.548    0.198\n#&gt; 14    Adjust  ~ Cognitive   0.261  0.070  3.743  0.000    0.124    0.398\n#&gt; 15    Adjust  ~      Risk  -0.146  0.127 -1.153  0.249   -0.395    0.102\n#&gt; 16   Achieve ~~    Adjust  36.374  7.967  4.565  0.000   20.758   51.989\n#&gt; 17    verbal ~~    verbal  46.253  9.782  4.728  0.000   27.080   65.426\n#&gt; 18    visual ~~    visual  76.171 12.153  6.268  0.000   52.351   99.991\n#&gt; 19    memory ~~    memory  69.732  9.717  7.177  0.000   50.688   88.776\n#&gt; 20      read ~~      read  51.273 11.238  4.562  0.000   29.246   73.299\n#&gt; 21      math ~~      math  86.341 13.049  6.616  0.000   60.764  111.917\n#&gt; 22     spell ~~     spell 112.797 14.078  8.012  0.000   85.205  140.389\n#&gt; 23    motive ~~    motive  37.737  6.463  5.839  0.000   25.071   50.404\n#&gt; 24   harmony ~~   harmony  83.953 10.590  7.927  0.000   63.197  104.709\n#&gt; 25    stable ~~    stable  29.306  5.386  5.441  0.000   18.749   39.863\n#&gt; 26    parent ~~    parent  88.005 18.343  4.798  0.000   52.053  123.958\n#&gt; 27       ses ~~       ses  38.896 10.208  3.810  0.000   18.889   58.904\n#&gt; 28 Cognitive ~~ Cognitive 141.617 22.098  6.409  0.000   98.307  184.928\n#&gt; 29   Achieve ~~   Achieve  84.317 16.324  5.165  0.000   52.322  116.312\n#&gt; 30    Adjust ~~    Adjust  38.008  8.188  4.642  0.000   21.960   54.055\n#&gt; 31      Risk ~~      Risk  55.079 19.989  2.755  0.006   15.902   94.257\n#&gt; 32 Cognitive ~~      Risk -40.517 12.448 -3.255  0.001  -64.914  -16.119\n\n\nfitMeasures(worlandSRb, c(\"chisq\", \"df\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt; print()\n#&gt;  chisq     df    cfi    tli  rmsea   srmr \n#&gt; 16.212 38.000  1.000  1.049  0.000  0.023\n\n\n# predicted correlation matrix for factors\nlavaan::lavInspect(worlandSRb, \"cor.lv\") |&gt; print()\n#&gt;           Cogntv Achiev Adjust   Risk\n#&gt; Cognitive  1.000                     \n#&gt; Achieve    0.703  1.000              \n#&gt; Adjust     0.500  0.751  1.000       \n#&gt; Risk      -0.459 -0.401 -0.349  1.000\n\n\nlavaan::residuals(worlandSRb, type = \"standardized.mplus\") |&gt; print()\n#&gt; $type\n#&gt; [1] \"standardized.mplus\"\n#&gt; \n#&gt; $cov\n#&gt;         verbal visual memory   read   math  spell motive harmny stable\n#&gt; verbal      NA                                                        \n#&gt; visual  -0.002     NA                                                 \n#&gt; memory   0.122 -0.131     NA                                          \n#&gt; read     0.597  0.113 -0.285     NA                                   \n#&gt; math     0.504  0.038 -0.377  0.597     NA                            \n#&gt; spell   -0.952 -0.255 -0.704 -0.667 -0.206     NA                     \n#&gt; motive  -0.821 -0.157  0.309 -0.042 -0.078  1.484     NA              \n#&gt; harmony  0.117 -0.453  0.171  0.942 -1.179  1.196 -1.010     NA       \n#&gt; stable   0.284  0.112  0.566 -1.893 -0.467  0.942  0.422  0.436     NA\n#&gt; parent  -0.093 -0.261 -0.157  0.234  0.378  0.157  0.300  0.019 -0.022\n#&gt; ses      1.891 -0.247 -1.417 -0.370 -0.114 -0.112  0.213 -0.125 -0.362\n#&gt;         parent    ses\n#&gt; verbal               \n#&gt; visual               \n#&gt; memory               \n#&gt; read                 \n#&gt; math                 \n#&gt; spell                \n#&gt; motive               \n#&gt; harmony              \n#&gt; stable               \n#&gt; parent   0.034       \n#&gt; ses      0.043  0.017\n\n\nlavaan::residuals(worlandSRb, type = \"cor.bollen\") |&gt; print()\n#&gt; $type\n#&gt; [1] \"cor.bollen\"\n#&gt; \n#&gt; $cov\n#&gt;         verbal visual memory   read   math  spell motive harmny stable\n#&gt; verbal   0.000                                                        \n#&gt; visual   0.000  0.000                                                 \n#&gt; memory   0.001 -0.002  0.000                                          \n#&gt; read     0.015  0.003 -0.010  0.000                                   \n#&gt; math     0.017  0.001 -0.016  0.007  0.000                            \n#&gt; spell   -0.040 -0.012 -0.036 -0.010 -0.006  0.000                     \n#&gt; motive  -0.031 -0.007  0.016 -0.001 -0.003  0.075  0.000              \n#&gt; harmony  0.006 -0.026  0.010  0.041 -0.053  0.071 -0.027  0.000       \n#&gt; stable   0.011  0.005  0.028 -0.034 -0.015  0.045  0.005  0.012  0.000\n#&gt; parent  -0.003 -0.011 -0.007  0.008  0.017  0.009  0.015  0.001 -0.001\n#&gt; ses      0.050 -0.010 -0.068 -0.012 -0.005 -0.006  0.010 -0.008 -0.016\n#&gt;         parent    ses\n#&gt; verbal               \n#&gt; visual               \n#&gt; memory               \n#&gt; read                 \n#&gt; math                 \n#&gt; spell                \n#&gt; motive               \n#&gt; harmony              \n#&gt; stable               \n#&gt; parent   0.000       \n#&gt; ses      0.000  0.000\n\nConfrontiamo i due modelli con il test del rapporto di verosimiglianza.\n\nlavTestLRT(worlandSRa, worlandSRb)\n#&gt; \n#&gt; Chi-Squared Difference Test\n#&gt; \n#&gt;            Df   AIC   BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\n#&gt; worlandSRb 38 12938 13024  16.2                                    \n#&gt; worlandSRa 39 12957 13040  37.3       21.1 0.357       1    4.3e-06\n\nL’adattamento del modello SR con 5 percorsi tra i fattori è significativamente peggiore rispetto a quello del modello CFA con 6 percorsi. Gli indici di fit del modello con 6 percorsi sono buoni così come il suo adattamento locale.\n\n# standardized estimates with standard errors\nlavaan::standardizedSolution(worlandSRb) |&gt; print()\n#&gt;          lhs op       rhs est.std    se      z pvalue ci.lower ci.upper\n#&gt; 1  Cognitive =~    verbal   0.868 0.032 27.075  0.000    0.805    0.931\n#&gt; 2  Cognitive =~    visual   0.806 0.037 21.720  0.000    0.733    0.879\n#&gt; 3  Cognitive =~    memory   0.747 0.043 17.470  0.000    0.663    0.831\n#&gt; 4    Achieve =~      read   0.876 0.031 28.212  0.000    0.815    0.937\n#&gt; 5    Achieve =~      math   0.791 0.038 20.777  0.000    0.717    0.866\n#&gt; 6    Achieve =~     spell   0.639 0.053 11.984  0.000    0.534    0.743\n#&gt; 7     Adjust =~    motive   0.761 0.049 15.552  0.000    0.665    0.857\n#&gt; 8     Adjust =~   harmony   0.561 0.065  8.657  0.000    0.434    0.688\n#&gt; 9     Adjust =~    stable   0.781 0.048 16.381  0.000    0.688    0.875\n#&gt; 10      Risk =~    parent   0.620 0.100  6.217  0.000    0.425    0.816\n#&gt; 11      Risk =~       ses   0.677 0.104  6.495  0.000    0.473    0.881\n#&gt; 12   Achieve  ~ Cognitive   0.657 0.079  8.357  0.000    0.503    0.811\n#&gt; 13   Achieve  ~      Risk  -0.100 0.107 -0.935  0.350   -0.310    0.110\n#&gt; 14    Adjust  ~ Cognitive   0.431 0.103  4.179  0.000    0.229    0.633\n#&gt; 15    Adjust  ~      Risk  -0.151 0.127 -1.186  0.236   -0.400    0.098\n#&gt; 16   Achieve ~~    Adjust   0.643 0.085  7.592  0.000    0.477    0.808\n#&gt; 17    verbal ~~    verbal   0.246 0.056  4.421  0.000    0.137    0.355\n#&gt; 18    visual ~~    visual   0.350 0.060  5.847  0.000    0.233    0.467\n#&gt; 19    memory ~~    memory   0.442 0.064  6.920  0.000    0.317    0.567\n#&gt; 20      read ~~      read   0.232 0.054  4.271  0.000    0.126    0.339\n#&gt; 21      math ~~      math   0.374 0.060  6.197  0.000    0.255    0.492\n#&gt; 22     spell ~~     spell   0.592 0.068  8.686  0.000    0.458    0.725\n#&gt; 23    motive ~~    motive   0.421 0.074  5.649  0.000    0.275    0.567\n#&gt; 24   harmony ~~   harmony   0.686 0.073  9.445  0.000    0.543    0.828\n#&gt; 25    stable ~~    stable   0.390 0.075  5.229  0.000    0.244    0.536\n#&gt; 26    parent ~~    parent   0.615 0.124  4.967  0.000    0.372    0.858\n#&gt; 27       ses ~~       ses   0.542 0.141  3.840  0.000    0.265    0.818\n#&gt; 28 Cognitive ~~ Cognitive   1.000 0.000     NA     NA    1.000    1.000\n#&gt; 29   Achieve ~~   Achieve   0.498 0.077  6.470  0.000    0.347    0.649\n#&gt; 30    Adjust ~~    Adjust   0.732 0.080  9.108  0.000    0.574    0.889\n#&gt; 31      Risk ~~      Risk   1.000 0.000     NA     NA    1.000    1.000\n#&gt; 32 Cognitive ~~      Risk  -0.459 0.098 -4.686  0.000   -0.651   -0.267\n\nLa correlazione stimata tra i fattori esogeni abilità cognitiva e rischio, –.459, è sensata: è negativa (più alto il rischio, minore l’abilità cognitiva). Questa correlazione non è prossima a -1.0, il che suggerisce che i due fattori sono distinti e non quasi identici, confermando così l’ipotesi di validità discriminante.\nAnalizzando gli impatti specifici, un incremento di un punto nel fattore cognitivo (misurato come varianza comune del ragionamento verbale) prevede un aumento di .719 punti nel rendimento scolastico (misurato come varianza comune dell’abilità di lettura), tenendo conto del fattore di rischio. In termini standardizzati, un aumento di una deviazione standard nell’abilità cognitiva si traduce in un aumento di .657 deviazioni standard nel rendimento scolastico, sempre controllando per il rischio.\nL’influenza del rischio sul rendimento scolastico è meno marcata: un incremento di un punto nel rischio (misurato come varianza comune del disturbo genitoriale) prevede una diminuzione di .175 punti nel rendimento scolastico. Standardizzando, un aumento di una deviazione standard nel rischio si associa a una diminuzione di .100 deviazioni standard nel rendimento, controllando per l’abilità cognitiva.\nLa correlazione di perturbazione di .643 misura la relazione tra il rendimento scolastico e l’adattamento in classe, dopo aver escluso l’influenza di altri fattori noti, in questo caso l’abilità cognitiva e il rischio di psicopatologia. In termini più semplici, la correlazione di perturbazione ci dice quanto sono correlati il rendimento scolastico e l’adattamento in classe quando si tiene conto (o si “controlla”) dell’effetto dell’abilità cognitiva e del rischio. Un valore di .643 indica una correlazione moderatamente forte, suggerendo che quando il rendimento scolastico di uno studente migliora (o peggiora), anche il suo adattamento in classe tende a migliorare (o peggiorare) in modo simile, indipendentemente dal suo livello di abilità cognitiva o dal grado di rischio di psicopatologia.\nLa presenza di questa correlazione parziale sostanziale implica che ci sono fattori non misurati nel modello che influenzano sia il rendimento scolastico sia l’adattamento in classe. Questi fattori non misurati potrebbero includere variabili come il sostegno familiare, la qualità dell’insegnamento, fattori ambientali o personalità dello studente. Importante è che questi fattori non misurati sono distinti sia dall’abilità cognitiva dello studente sia dal suo rischio di psicopatologia. In conclusione, il valore di .643 non solo mette in luce l’interdipendenza tra rendimento scolastico e adattamento in classe, ma suggerisce anche l’esistenza di altre variabili influenti che non sono state direttamente misurate o incluse nel modello. Questa informazione può essere preziosa per indirizzare ulteriori ricerche o interventi educativi.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Modelli di Regressione Strutturale</span>"
    ]
  },
  {
    "objectID": "chapters/sem/09_structural_regr.html#strategie-avanzate-di-modellazione-per-i-modelli-di-regressione-strutturale",
    "href": "chapters/sem/09_structural_regr.html#strategie-avanzate-di-modellazione-per-i-modelli-di-regressione-strutturale",
    "title": "56  Modelli di Regressione Strutturale",
    "section": "\n56.5 Strategie Avanzate di Modellazione per i Modelli di Regressione Strutturale",
    "text": "56.5 Strategie Avanzate di Modellazione per i Modelli di Regressione Strutturale\nOltre all’approccio tradizionale di modellazione in due fasi, esiste un metodo più complesso a quattro fasi per analizzare i modelli SR completi. Questa strategia, introdotta da Mulaik e Millsap nel 2000, amplia la modellazione bifase aggiungendo ulteriori analisi esplorative che possono portare a conclusioni più definitive in una serie più estesa di studi. Questo metodo prevede che ogni fattore comune abbia almeno quattro indicatori, numero ritenuto sufficiente per testare l’unidimensionalità con il test dell’annullamento della tetrade. I quattro indicatori rappresentano anche il numero minimo perché un modello CFA a singolo fattore sia considerato sovraidentificato. Il ricercatore testa quindi una serie di almeno quattro modelli gerarchicamente correlati, seguendo questi passaggi:\n\nPrimo Passaggio: Il modello iniziale meno restrittivo è un modello EFA, dove ogni indicatore satura su tutti i fattori. Il numero di fattori è lo stesso dei modelli analizzati nei passaggi successivi. Questo modello viene analizzato con lo stesso metodo di stima utilizzato nei passaggi successivi, ad esempio il metodo ML per indicatori continui e normalmente distribuiti. Alternativamente, si possono usare tecniche come ESEM o E/CFA al posto dell’EFA. Questo passaggio serve a testare la correttezza provvisoria delle ipotesi riguardo al numero di fattori.\nSecondo Passaggio: Corrisponde al Primo Passaggio della modellazione bifase. Qui, si specifica un modello CFA con alcuni carichi incrociati fissati a zero, identificando gli indicatori che non dipendono da certi fattori comuni. Se l’adattamento del modello CFA è ragionevole, si può procedere al test del modello SR; in caso contrario, il modello di misurazione va rivisto.\nTerzo Passaggio: Si specifica il modello SR target con lo stesso schema di carichi incrociati fissati a zero del modello CFA del Secondo Passaggio. Tipicamente, la parte strutturale del modello SR include meno effetti diretti rispetto al totale delle covarianze tra fattori nel modello CFA. Se la parte strutturale del modello SR ha tanti percorsi quanti il modello CFA, i due modelli saranno equivalenti e questo passaggio può essere omesso.\nQuarto Passaggio: Coinvolge test su ipotesi specifiche sui parametri definiti dall’inizio del processo. Questi test possono comportare l’applicazione di vincoli zero o altri, aumentando di uno dfM. I Passaggi 3 e 4 della modellazione a quattro fasi rappresentano una precisazione delle attività generali del Secondo Passaggio della modellazione bifase.\n\nUna delle critiche alla modellazione a quattro fasi riguarda la necessità di avere almeno quattro indicatori per fattore, condizione non sempre pratica o desiderabile, specialmente quando pochi indicatori, o anche un singolo indicatore ottimale, presentano migliori caratteristiche psicometriche rispetto a quattro. Tuttavia, Mulaik e Millsap hanno osservato che avere almeno quattro indicatori può compensare, in parte, le limitazioni di un campione più piccolo incrementando dfM.\nEntrambi gli approcci, bifase e quattro fasi, sfruttano la variazione casuale quando i modelli vengono testati e riformulati utilizzando gli stessi dati, e sono considerati migliori della modellazione monofase, dove non esiste una distinzione tra questioni di misurazione e struttura.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Modelli di Regressione Strutturale</span>"
    ]
  },
  {
    "objectID": "chapters/sem/09_structural_regr.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/sem/09_structural_regr.html#informazioni-sullambiente-di-sviluppo",
    "title": "56  Modelli di Regressione Strutturale",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.2\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] lme4_1.1-36       Matrix_1.7-3      ggokabeito_0.1.0  see_0.11.0       \n#&gt;  [5] MASS_7.3-65       viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n#&gt;  [9] ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1 \n#&gt; [13] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-19     psych_2.4.12     \n#&gt; [17] scales_1.3.0      markdown_1.13     knitr_1.50        lubridate_1.9.4  \n#&gt; [21] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.4      \n#&gt; [25] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1    \n#&gt; [29] tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.1      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.2.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] mime_0.13           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [25] R6_2.6.1            fastmap_1.2.0       rbibutils_2.3      \n#&gt;  [28] shiny_1.10.0        digest_0.6.37       OpenMx_2.21.13     \n#&gt;  [31] fdrtool_1.2.18      colorspace_2.1-1    rprojroot_2.0.4    \n#&gt;  [34] Hmisc_5.2-3         timechange_0.3.0    abind_1.4-8        \n#&gt;  [37] compiler_4.4.2      withr_3.0.2         glasso_1.11        \n#&gt;  [40] htmlTable_2.4.3     backports_1.5.0     carData_3.0-5      \n#&gt;  [43] ggsignif_0.6.4      corpcor_1.6.10      gtools_3.9.5       \n#&gt;  [46] tools_4.4.2         pbivnorm_0.6.0      foreign_0.8-88     \n#&gt;  [49] zip_2.3.2           httpuv_1.6.15       nnet_7.3-20        \n#&gt;  [52] glue_1.8.0          quadprog_1.5-8      nlme_3.1-167       \n#&gt;  [55] promises_1.3.2      lisrelToR_0.3       grid_4.4.2         \n#&gt;  [58] checkmate_2.3.2     cluster_2.1.8.1     reshape2_1.4.4     \n#&gt;  [61] generics_0.1.3      gtable_0.3.6        tzdb_0.5.0         \n#&gt;  [64] data.table_1.17.0   hms_1.1.3           car_3.1-3          \n#&gt;  [67] sem_3.1-16          pillar_1.10.1       rockchalk_1.8.157  \n#&gt;  [70] later_1.4.1         splines_4.4.2       lattice_0.22-6     \n#&gt;  [73] survival_3.8-3      kutils_1.73         tidyselect_1.2.1   \n#&gt;  [76] miniUI_0.1.1.1      pbapply_1.7-2       reformulas_0.4.0   \n#&gt;  [79] stats4_4.4.2        xfun_0.51           qgraph_1.9.8       \n#&gt;  [82] arm_1.14-4          stringi_1.8.4       yaml_2.3.10        \n#&gt;  [85] pacman_0.5.1        boot_1.3-31         evaluate_1.0.3     \n#&gt;  [88] codetools_0.2-20    mi_1.1              cli_3.6.4          \n#&gt;  [91] RcppParallel_5.1.10 rpart_4.1.24        xtable_1.8-4       \n#&gt;  [94] Rdpack_2.6.3        munsell_0.5.1       Rcpp_1.0.14        \n#&gt;  [97] coda_0.19-4.1       png_0.1-8           XML_3.99-0.18      \n#&gt; [100] parallel_4.4.2      jpeg_0.1-10         mvtnorm_1.3-3      \n#&gt; [103] openxlsx_4.2.8      rlang_1.1.5         multcomp_1.4-28    \n#&gt; [106] mnormt_2.1.1\n\n\n\n\n\nKline, R. B. (2023). Principles and practice of structural equation modeling. Guilford publications.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Modelli di Regressione Strutturale</span>"
    ]
  },
  {
    "objectID": "chapters/sem/10_missing_data.html",
    "href": "chapters/sem/10_missing_data.html",
    "title": "57  Dati mancanti",
    "section": "",
    "text": "57.1 Introduzione\nRaramente un ricercatore si trova nella situazione fortunata nella quale un’analisi statistica (di tipo CFA/SEM o altro) può essere condotta utilizzando un set di dati in cui tutte le variabili sono state osservate su tutte le unità statistiche: nella pratica ricerca i dati mancanti sono la norma piuttosto che l’eccezione.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Dati mancanti</span>"
    ]
  },
  {
    "objectID": "chapters/sem/10_missing_data.html#tipologie-di-dati-mancanti",
    "href": "chapters/sem/10_missing_data.html#tipologie-di-dati-mancanti",
    "title": "57  Dati mancanti",
    "section": "\n57.2 Tipologie di dati mancanti",
    "text": "57.2 Tipologie di dati mancanti\nCi sono molti motivi che possono stare alla base dei dati mancanti. Ad esempio, i dati possono mancare per disegno dello studio (“mancanza pianificata”), come ad esempio nei progetti di ricerca in cui i partecipanti al campione vengono selezionati casualmente per completare sottoinsiemi diversi della batteria di valutazione (una scelta di questo tipo viene motivata, ad esempio, a causa di considerazioni pratiche come i vincoli di tempo). In tali condizioni, si presume che i dati mancanti si distribuiscano in un modo completamente casuale rispetto a tutte le altre variabili nello studio.\nIn generale, i meccanismi che determinano la presenza di dati mancanti possono essere classificati in tre categorie:\n\n\nvalori mancanti completamente casuali (Missing Completely At Random, MCAR). La probabilità di dati mancanti su una variabile non è collegata né al valore mancante sulla variabile, né al valore di ogni altra variabile presente nella matrice dati che si sta analizzando;\n\nvalori mancanti casuali (Missing At Random, MAR). I valori mancanti sono indipendenti dal valore che viene a mancare, ma dipendono da altre variabili, cioè i dati sulla variabile sono mancanti per categorie di partecipanti che potrebbero essere identificati dai valori assunti dalle altre variabili presenti nello studio;\n\nvalori mancanti non ignorabili (Missing Not At Random, MNAR). La mancanza di un dato può dipendere sia dal valore del dato stesso che dalle altre variabili. Per esempio, se si studia la salute mentale e le persone depresse riferiscono meno volentieri informazioni riguardanti il loro stato di salute, allora i dati non sono mancanti per caso.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Dati mancanti</span>"
    ]
  },
  {
    "objectID": "chapters/sem/10_missing_data.html#la-gestione-dei-dati-mancanti",
    "href": "chapters/sem/10_missing_data.html#la-gestione-dei-dati-mancanti",
    "title": "57  Dati mancanti",
    "section": "\n57.3 La gestione dei dati mancanti",
    "text": "57.3 La gestione dei dati mancanti\nIl passo successivo dopo la definizione dei meccanismi è quello della gestione dei dati mancanti. Sostanzialmente le scelte possibili sono due: l’eliminazione dei casi o la sostituzione dei dati mancanti. Un metodo semplice, indicato solo nel caso in cui l’ammontare dei dati mancanti è limitato e questi sono mancanti completamente a caso (MCAR), è quello di rimuovere i casi con dati mancanti (case deletion).\nCi sono due metodi per eliminare le osservazioni con valori mancanti: listwise deletion e pairwise deletion. Nel primo caso si elimina dal campione ogni osservazione che contiene dati mancanti. Le analisi avverranno quindi solo sui casi che hanno valori validi su tutte le variabili in esame. In questo modo si ottiene una maggiore semplicità di trattazione nell’analisi statistica, tuttavia non si utilizza tutta l’informazione osservata (si riduce la numerosità campionaria e, quindi, l’informazione). Il secondo metodo è la pairwise deletion, che utilizza tutti i casi che hanno i dati validi su due variabili volta per volta. In questo modo si riesce a massimizzare la numerosità del campione da utilizzare, ma si tratta comunque di un metodo che presenta dei problemi, per esempio il fatto che con questo approccio i parametri del modello saranno basati su differenti insiemi di dati, con differenti numerosità campionarie e differenti errori standard.\nQuando i dati non sono MNAR è opportuno sostituirli con appropriate funzioni dei dati effettivamente osservati. Questa procedura è chiamata imputazione (imputation). Di seguito sono indicati alcuni metodi.\n\n\nMean Imputation. Il dato mancante viene sostituito con la media della variabile. Questo metodo, utilizzato troppo spesso per la sua semplicità, riducendo la variabilità dei dati, ha effetti importanti su molte analisi dei dati e, in generale, dovrebbe essere evitato.\n\nRegression Imputation. Si tratta di un approccio basato sulle informazioni disponibili sulle altre variabili. Si stima una equazione di regressione lineare per ogni variabile utilizzando le altre variabili come predittori. Questo metodo offre il vantaggio di poter utilizzare i rapporti esistenti tra le variabili per effettuare le valutazioni dei dati mancanti; tuttavia esso è usato raramente, in quanto amplifica le correlazioni tra le variabili; quindi, se le analisi si basano su regressioni o modelli SEM, questo metodo è sconsigliato.\n\nMultiple Imputation. La tecnica di multiple imputation, applicabile in caso di MAR, prevede che un dato mancante su una variabile sia sostituito, sulla base dei dati esistenti sulle altre variabili, con un valore che però comprende anche una componente di errore ricavata dalla distribuzione dei residui della variabile.\n\nExpectation-Maximization. Un altro approccio moderno del trattamento dei dati mancanti è l’applicazione dell’algoritmo Expectation Maximization (EM). La tecnica è quella di stimare i parametri sulla base dei dati osservati, e di stimare poi i dati mancanti sulla base di questi parametri (fase E). Poi i parametri vengono nuovamente stimati sulla base della nuova matrice di dati (fase M), e così via. Questo processo viene iterato fino a quando i valori stimati convergono. Tuttavia, una limitazione fondamentale dell’utilizzo dell’algoritmo EM per calcolare le matrici di input per le analisi CFA/SEM è che gli errori standard risultanti delle stime dei parametri non sono consistenti. Pertanto, gli intervalli di confidenza e i test di significatività possono risultare compromessi.\n\n\n57.3.1 Metodo Direct ML\nBenché i metodi precedenti vengano spesso usati, nella pratica concreta è preferibile usare il metodo Direct ML, conosciuto anche come “raw ML” o “full information ML” (FIML), in quanto è generalmente considerano come il metodo migliore per gestire i dati mancanti nella maggior parte delle applicazioni CFA e SEM. Il metodo full information ML è esente dai problemi associati all’utilizzo dell’algoritmo EM e produce stime consistenti sotto l’ipotesi di normalità multivariata per dati mancanti MAR.\nIntuitivamente, l’approccio utilizza la relazione tra le variabili per dedurre quali siano i valori mancanti con maggiore probabilità. Ad esempio, se due variabili, \\(X\\) e \\(Y\\), sono correlate positivamente, allora se, per alcune osservazioni \\(i\\), \\(X_i\\) è il valore più alto nella variabile, è probabile che anche il valore mancante \\(Y_i\\) sia un valore alto. FIML utilizza queste informazioni senza procedere all’imputazione dei valori mancanti, ma invece basandosi sulle stime più verosimili dei parametri della popolazione, ovvero massimizzando direttamente la verosimiglianza del modello specificato. Sotto l’assunzione di normalità multivariata, la funzione di verosimiglianza diventa\n\\[\nL(\\mu, \\Sigma) = \\prod_i f(y_i \\mid \\mu_i, \\Sigma_i),\n\\]\ndove \\(y_i\\) sono i dati, \\(\\mu_i\\) e \\(\\Sigma_i\\) sono i parametri della popolazione se gli elementi mancanti in \\(y_i\\) vengono rimossi. Si cercano i valori \\(\\mu\\) e \\(\\Sigma\\) che massimizzano la verosimiglianza.\nIn lavaan l’applicazione di tale metodo si ottiene specificando l’argomento missing = \"ml\".\n\n57.3.2 Un esempio concreto\nPer applicare il metodo direct ML, Brown (2015) prende in esame i dati reali di un questionario (un singolo fattore, quattro item, una covarianza di errore) caratterizzato dalla presenza di dati mancanti. Importiamo i dati in R:\n\nd &lt;- rio::import(here::here(\"data\", \"brown_table_9_1.csv\"))\nhead(d)\n#&gt;   subject s1 s2 s3 s4\n#&gt; 1    5760  2  0  1 NA\n#&gt; 2    5761  3  3  3 NA\n#&gt; 3    5763  2  4  4 NA\n#&gt; 4    5761  2  0  0 NA\n#&gt; 5    5769  2  1  1 NA\n#&gt; 6    5771  4  3  3 NA\n\nAbbiamo 650 osservazioni:\n\ndim(d)\n#&gt; [1] 650   5\n\nLe frequenze di dati mancanti vengono ottentute mediante la funzione summary()\n\nsummary(d)\n#&gt;     subject           s1             s2             s3             s4     \n#&gt;  Min.   :5756   Min.   :0.00   Min.   :0.00   Min.   :0.00   Min.   :0.0  \n#&gt;  1st Qu.:5934   1st Qu.:2.00   1st Qu.:2.00   1st Qu.:1.00   1st Qu.:2.0  \n#&gt;  Median :6102   Median :3.00   Median :3.00   Median :2.00   Median :3.0  \n#&gt;  Mean   :6104   Mean   :2.93   Mean   :2.56   Mean   :2.21   Mean   :2.4  \n#&gt;  3rd Qu.:6275   3rd Qu.:4.00   3rd Qu.:4.00   3rd Qu.:4.00   3rd Qu.:3.0  \n#&gt;  Max.   :6451   Max.   :4.00   Max.   :4.00   Max.   :4.00   Max.   :4.0  \n#&gt;                 NA's   :25     NA's   :25     NA's   :25     NA's   :190\n\nIl modello viene specificato come segue Brown (2015):\n\nmodel &lt;- '\n  esteem =~ s1 + s2 + s3 + s4\n  s2 ~~ s4\n'\n\nAdattiamo il modello ai dati specificanto l’utilizzo del metodo full information ML per la gestione dei dati mancanti:\n\nfit &lt;- cfa(model, data = d, missing = \"fiml\")\n\nÈ possibile identificare le configurazioni di risposte agli item che contengono dati mancanti:\n\nfit@Data@Mp[[1]]$npatterns\n#&gt; [1] 5\n\n\npats &lt;- fit@Data@Mp[[1]]$pat * 1L\ncolnames(pats) &lt;- fit@Data@ov.names[[1]]\nprint(pats)\n#&gt;      s1 s2 s3 s4\n#&gt; [1,]  1  1  1  1\n#&gt; [2,]  1  1  1  0\n#&gt; [3,]  0  1  1  1\n#&gt; [4,]  1  0  1  1\n#&gt; [5,]  1  1  0  1\n\nPossiamo esaminare la proporzione di dati disponibili per ciascun indicatore e per ciascuna coppia di indicatori:\n\ncoverage &lt;- fit@Data@Mp[[1]]$coverage\ncolnames(coverage) &lt;- rownames(coverage) &lt;- fit@Data@ov.names[[1]]\nprint(coverage)\n#&gt;        s1     s2     s3     s4\n#&gt; s1 0.9615 0.9231 0.9231 0.6692\n#&gt; s2 0.9231 0.9615 0.9231 0.6692\n#&gt; s3 0.9231 0.9231 0.9615 0.6692\n#&gt; s4 0.6692 0.6692 0.6692 0.7077\n\nAd esempio, consideriamo l’item s1; se moltiplichiamo la copertura di questo elemento per la numerosità campionaria possiamo concludere che questa variabile contiene 25 osservazioni mancanti; e così via.\n\n650 * 0.9615385\n#&gt; [1] 625\n\nProcediamo poi come sempre per esaminare la soluzione ottenuta.\n\neffectsize::interpret(fit)\n#&gt;     Name    Value Threshold Interpretation\n#&gt; 1    GFI 0.999449      0.95   satisfactory\n#&gt; 2   AGFI 0.992292      0.90   satisfactory\n#&gt; 3    NFI 0.999193      0.90   satisfactory\n#&gt; 4   NNFI 0.998978      0.90   satisfactory\n#&gt; 5    CFI 0.999830      0.90   satisfactory\n#&gt; 6  RMSEA 0.020238      0.05   satisfactory\n#&gt; 7   SRMR 0.004853      0.08   satisfactory\n#&gt; 8    RFI 0.995155      0.90   satisfactory\n#&gt; 9   PNFI 0.166532      0.50           poor\n#&gt; 10   IFI 0.999830      0.90   satisfactory\n\n\nstandardizedSolution(fit)\n#&gt;       lhs op    rhs est.std    se      z pvalue ci.lower ci.upper\n#&gt; 1  esteem =~     s1   0.737 0.020 37.086      0    0.698    0.776\n#&gt; 2  esteem =~     s2   0.920 0.013 68.651      0    0.894    0.947\n#&gt; 3  esteem =~     s3   0.880 0.013 66.432      0    0.854    0.906\n#&gt; 4  esteem =~     s4   0.905 0.016 55.400      0    0.873    0.937\n#&gt; 5      s2 ~~     s4  -0.886 0.216 -4.109      0   -1.309   -0.463\n#&gt; 6      s1 ~~     s1   0.456 0.029 15.554      0    0.399    0.514\n#&gt; 7      s2 ~~     s2   0.153 0.025  6.190      0    0.104    0.201\n#&gt; 8      s3 ~~     s3   0.225 0.023  9.636      0    0.179    0.271\n#&gt; 9      s4 ~~     s4   0.182 0.030  6.151      0    0.124    0.240\n#&gt; 10 esteem ~~ esteem   1.000 0.000     NA     NA    1.000    1.000\n#&gt; 11     s1 ~1          2.375 0.078 30.610      0    2.223    2.527\n#&gt; 12     s2 ~1          1.881 0.066 28.592      0    1.752    2.010\n#&gt; 13     s3 ~1          1.584 0.059 26.781      0    1.468    1.700\n#&gt; 14     s4 ~1          1.850 0.071 26.048      0    1.710    1.989\n#&gt; 15 esteem ~1          0.000 0.000     NA     NA    0.000    0.000",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Dati mancanti</span>"
    ]
  },
  {
    "objectID": "chapters/sem/10_missing_data.html#dati-mancanti-in-r",
    "href": "chapters/sem/10_missing_data.html#dati-mancanti-in-r",
    "title": "57  Dati mancanti",
    "section": "\n57.4 Dati mancanti in R",
    "text": "57.4 Dati mancanti in R\nPer completezza, aggiungiamo qualche breve accenno alla gestione dei dati mancanti in R.\nIn R, i valori mancanti vengono indicati dal codice NA, che significa not available — non disponibile.\nSe una variabile contiene valori mancanti, R non è in grado di applicare ad essa alcune funzioni, come ad esempio la media. Per questa ragione, la gran parte delle funzioni di R prevedono modi specifici per trattare i valori mancanti.\nCi sono diversi tipi di dati “mancanti” in R;\n\n\nNA - generico dato mancante;\n\nNaN - il codice NaN (Not a Number) indica i valori numerici impossibili, quali ad esempio un valore 0/0;\n\nInf e -Inf - Infinity, si verifca, ad esempio, quando si divide un numero per 0.\n\nLa funzione is.na() ritorna un output che indica con TRUE le celle che contengono NA o NaN.\nSi noti che\n\nse is.na(x) è TRUE, allora !is.na(x) è FALSE;\n\nall(!is.na(x)) ritorna TRUE se tutti i valori x sono NOT NA;\n\nany(is.na(x)) risponde alla domanda: c’è qualche valore NA (almeno uno) in x?;\n\ncomplete.cases(x) ritorna TRUE se ciascun elemento di x è is NOT NA; ritorna FALSE se almeno un elemento di x è NA;\n\nLe funzioni R is.nan() e is.infinite() si applicano ai tipi di dati NaN e Inf.\nPer esempio, consideriamo il seguente data.frame:\n\nd &lt;- tibble(\n  w = c(1, 2, NA, 3, NA), \n  x = 1:5, \n  y = 1, \n  z = x ^ 2 + y,\n  q = c(3, NA, 5, 1, 4)\n)\nd\n#&gt; # A tibble: 5 × 5\n#&gt;       w     x     y     z     q\n#&gt;   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1     1     1     2     3\n#&gt; 2     2     2     1     5    NA\n#&gt; 3    NA     3     1    10     5\n#&gt; 4     3     4     1    17     1\n#&gt; 5    NA     5     1    26     4\n\n\nis.na(d$w)\n#&gt; [1] FALSE FALSE  TRUE FALSE  TRUE\nis.na(d$x)\n#&gt; [1] FALSE FALSE FALSE FALSE FALSE\n\nPer creare un nuovo Dataframe senza valori mancanti:\n\nd_clean &lt;- d[complete.cases(d), ]\nd_clean\n#&gt; # A tibble: 2 × 5\n#&gt;       w     x     y     z     q\n#&gt;   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1     1     1     2     3\n#&gt; 2     3     4     1    17     1\n\nOppure, se vogliamo eliminare le righe con NA solo in una variabile:\n\nd1 &lt;- d[!is.na(d$q), ]\nd1\n#&gt; # A tibble: 4 × 5\n#&gt;       w     x     y     z     q\n#&gt;   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1     1     1     2     3\n#&gt; 2    NA     3     1    10     5\n#&gt; 3     3     4     1    17     1\n#&gt; 4    NA     5     1    26     4\n\nSe vogliamo esaminare le righe con i dati mancanti in qualunque colonna:\n\nd_na &lt;- d[!complete.cases(d), ]\nd_na\n#&gt; # A tibble: 3 × 5\n#&gt;       w     x     y     z     q\n#&gt;   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     2     2     1     5    NA\n#&gt; 2    NA     3     1    10     5\n#&gt; 3    NA     5     1    26     4",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Dati mancanti</span>"
    ]
  },
  {
    "objectID": "chapters/sem/10_missing_data.html#riflessioni-conclusive",
    "href": "chapters/sem/10_missing_data.html#riflessioni-conclusive",
    "title": "57  Dati mancanti",
    "section": "\n57.5 Riflessioni Conclusive",
    "text": "57.5 Riflessioni Conclusive\nIn conclusione, questo capitolo ha fornito una panoramica completa sui dati mancanti, affrontando le loro tipologie, le possibili cause e le strategie di gestione più appropriate in base al contesto e al meccanismo sottostante. Abbiamo esplorato tecniche tradizionali come la listwise e pairwise deletion, metodi di imputazione semplici e avanzati, e approcci più robusti come l’algoritmo Expectation-Maximization e il Full Information Maximum Likelihood (FIML), evidenziandone vantaggi e limiti.\nIn particolare, è stato sottolineato come il metodo FIML rappresenti una soluzione ottimale per molte applicazioni in CFA e SEM, grazie alla sua capacità di sfruttare tutte le informazioni disponibili senza introdurre i bias tipici dell’imputazione. Esempi pratici e codice R hanno illustrato come implementare queste tecniche, rendendo il capitolo una risorsa preziosa sia per comprendere i fondamenti teorici sia per affrontare casi applicativi concreti. La gestione adeguata dei dati mancanti non solo migliora l’affidabilità delle analisi, ma contribuisce a una migliore interpretazione dei risultati, garantendo robustezza e validità nelle conclusioni di ricerca.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Dati mancanti</span>"
    ]
  },
  {
    "objectID": "chapters/sem/10_missing_data.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/sem/10_missing_data.html#informazioni-sullambiente-di-sviluppo",
    "title": "57  Dati mancanti",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.2\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] ggokabeito_0.1.0  see_0.11.0        MASS_7.3-65       viridis_0.6.5    \n#&gt;  [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n#&gt;  [9] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n#&gt; [13] lavaan_0.6-19     psych_2.4.12      scales_1.3.0      markdown_1.13    \n#&gt; [17] knitr_1.50        lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n#&gt; [21] dplyr_1.1.4       purrr_1.0.4       readr_2.1.5       tidyr_1.3.1      \n#&gt; [25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.1      datawizard_1.0.1   \n#&gt;   [4] magrittr_2.0.3      TH.data_1.1-3       estimability_1.5.1 \n#&gt;   [7] farver_2.1.2        nloptr_2.2.1        rmarkdown_2.29     \n#&gt;  [10] vctrs_0.6.5         minqa_1.2.8         effectsize_1.0.0   \n#&gt;  [13] base64enc_0.1-3     rstatix_0.7.2       htmltools_0.5.8.1  \n#&gt;  [16] broom_1.0.7         Formula_1.2-5       htmlwidgets_1.6.4  \n#&gt;  [19] plyr_1.8.9          sandwich_3.1-1      rio_1.2.3          \n#&gt;  [22] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [25] mime_0.13           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [28] Matrix_1.7-3        R6_2.6.1            fastmap_1.2.0      \n#&gt;  [31] rbibutils_2.3       shiny_1.10.0        digest_0.6.37      \n#&gt;  [34] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n#&gt;  [37] rprojroot_2.0.4     Hmisc_5.2-3         timechange_0.3.0   \n#&gt;  [40] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n#&gt;  [43] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n#&gt;  [46] carData_3.0-5       performance_0.13.0  R.utils_2.13.0     \n#&gt;  [49] ggsignif_0.6.4      corpcor_1.6.10      gtools_3.9.5       \n#&gt;  [52] tools_4.4.2         pbivnorm_0.6.0      foreign_0.8-88     \n#&gt;  [55] zip_2.3.2           httpuv_1.6.15       nnet_7.3-20        \n#&gt;  [58] R.oo_1.27.0         glue_1.8.0          quadprog_1.5-8     \n#&gt;  [61] nlme_3.1-167        promises_1.3.2      lisrelToR_0.3      \n#&gt;  [64] grid_4.4.2          checkmate_2.3.2     cluster_2.1.8.1    \n#&gt;  [67] reshape2_1.4.4      generics_0.1.3      gtable_0.3.6       \n#&gt;  [70] tzdb_0.5.0          R.methodsS3_1.8.2   data.table_1.17.0  \n#&gt;  [73] hms_1.1.3           car_3.1-3           sem_3.1-16         \n#&gt;  [76] pillar_1.10.1       rockchalk_1.8.157   later_1.4.1        \n#&gt;  [79] splines_4.4.2       lattice_0.22-6      survival_3.8-3     \n#&gt;  [82] kutils_1.73         tidyselect_1.2.1    miniUI_0.1.1.1     \n#&gt;  [85] pbapply_1.7-2       reformulas_0.4.0    stats4_4.4.2       \n#&gt;  [88] xfun_0.51           qgraph_1.9.8        arm_1.14-4         \n#&gt;  [91] stringi_1.8.4       pacman_0.5.1        boot_1.3-31        \n#&gt;  [94] evaluate_1.0.3      codetools_0.2-20    mi_1.1             \n#&gt;  [97] cli_3.6.4           RcppParallel_5.1.10 rpart_4.1.24       \n#&gt; [100] parameters_0.24.2   xtable_1.8-4        Rdpack_2.6.3       \n#&gt; [103] munsell_0.5.1       Rcpp_1.0.14         coda_0.19-4.1      \n#&gt; [106] png_0.1-8           XML_3.99-0.18       parallel_4.4.2     \n#&gt; [109] bayestestR_0.15.2   jpeg_0.1-10         lme4_1.1-36        \n#&gt; [112] mvtnorm_1.3-3       insight_1.1.0       openxlsx_4.2.8     \n#&gt; [115] rlang_1.1.5         multcomp_1.4-28     mnormt_2.1.1\n\n\n\n\n\nBrown, T. A. (2015). Confirmatory factor analysis for applied research. Guilford publications.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.\n\n\nSaqr, M., & López-Pernas, S. (2024). Learning analytics methods and tutorials: A practical guide using R. Springer Nature.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Dati mancanti</span>"
    ]
  },
  {
    "objectID": "chapters/sem/11_small_samples.html",
    "href": "chapters/sem/11_small_samples.html",
    "title": "58  Modellizzazione SEM in Piccoli Campioni",
    "section": "",
    "text": "58.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nQuesto capitolo si concentra sull’applicazione dei modelli SEM in contesti caratterizzati da campioni di piccole dimensioni e sintetizza l’esposizione di Rosseel (2020) su questo argomento, mentre si avvale di esempi numerici tratti dai principi di Kline (2023).\nÈ risaputo che la maggior parte dei metodi di stima e inferenza in SEM si basa su presupposti asintotici, presupponendo la presenza di campioni casuali di grandi dimensioni. Tuttavia, nei casi di campioni più limitati, come quelli con N &lt; 200, emergono specifiche problematiche: i metodi iterativi possono non convergere, si possono verificare soluzioni non valide a causa dei casi di Heywood o di altri risultati anomali difficili da interpretare, e le stime dei parametri possono risultare fortemente distorte.\nInnanzitutto, i modelli strutturali possono diventare molto complessi, coinvolgendo numerose variabili (sia osservate che latenti), rendendo necessaria l’analisi di numerosi parametri e richiedendo un adeguato volume di dati per ottenere stime accurate. Inoltre, il framework statistico alla base della SEM tradizionale si fonda sulla teoria dei grandi campioni, suggerendo che una buona precisione nelle stime dei parametri e nell’inferenza sia garantita solo con campioni di dimensioni considerevoli. Alcuni studi di simulazione hanno addirittura suggerito che dimensioni del campione enormi siano necessarie per risultati affidabili, sebbene tali conclusioni siano rilevanti solo in specifici contesti e abbiano contribuito alla convinzione generalizzata che la SEM sia applicabile solo con campioni di dimensioni considerevoli (ad es. n &gt; 500) o addirittura molto grandi (n &gt; 2000).\nTuttavia, la realtà delle dimensioni ridotte dei campioni è una situazione comune per molte ragioni. In tali casi, molti ricercatori esitano ad utilizzare la SEM e si affidano a metodologie subottimali, come l’analisi di regressione o l’analisi di percorso basate su punteggi sommati. Tuttavia, è importante notare che il bias associato alle dimensioni ridotte del campione può essere ancora più accentuato in tecniche come la regressione multipla o l’analisi di percorso con variabili manifeste, soprattutto in assenza di considerazioni sull’errore di misurazione. Una strategia più efficace potrebbe essere quella di adottare l’approccio della SEM, pur cercando soluzioni per affrontare le sfide poste dalle dimensioni ridotte del campione.\nQuesto capitolo si propone di esplorare diverse strategie per superare tali sfide nell’utilizzo della SEM con campioni di piccole dimensioni. Sarà organizzato in tre sezioni: innanzitutto, verranno esaminate le problematiche comuni associate alle dimensioni ridotte del campione nella SEM. Successivamente, saranno presentati quattro approcci alternativi di stima che possono essere impiegati quando le dimensioni del campione sono limitate, anziché ricorrere alla SEM tradizionale. Infine, saranno discussi alcuni possibili correttivi per le statistiche di test e gli errori standard nelle situazioni di piccoli campioni. L’efficacia di alcune di queste tecniche sarà illustrata tramite l’analisi di un modello di fattore comune applicato a un campione di dimensioni ridotte.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Modellizzazione SEM in Piccoli Campioni</span>"
    ]
  },
  {
    "objectID": "chapters/sem/11_small_samples.html#problemi-con-le-piccole-dimensioni-del-campione-nella-sem",
    "href": "chapters/sem/11_small_samples.html#problemi-con-le-piccole-dimensioni-del-campione-nella-sem",
    "title": "58  Modellizzazione SEM in Piccoli Campioni",
    "section": "58.2 Problemi con le Piccole Dimensioni del Campione nella SEM",
    "text": "58.2 Problemi con le Piccole Dimensioni del Campione nella SEM\nConsideriamo un modello SEM con almeno 4 indicatori continui per ciascuna variabile latente. Se tutte le variabili osservate sono continue, lo stimatore di default nella maggior parte (se non in tutti) dei pacchetti software SEM è il metodo della massima verosimiglianza. Generalmente, lo stimatore della massima verosimiglianza è una buona scelta perché presenta molte proprietà statistiche desiderabili. Inoltre, l’approccio della massima verosimiglianza può essere adattato per gestire dati mancanti (sotto l’assunzione che i dati siano mancanti casualmente) e sono stati sviluppati errori standard e statistiche di test “robusti” per trattare dati non normali e modelli mal specificati.\nTuttavia, se la dimensione del campione è relativamente piccola (ad esempio, n &lt; 200), possono sorgere diversi problemi. Innanzitutto, il modello potrebbe non convergere, il che significa che l’ottimizzatore (l’algoritmo che cerca di trovare i valori dei parametri del modello che massimizzano la verosimiglianza dei dati) non è riuscito a trovare una soluzione che soddisfi uno o più criteri di convergenza. In rare occasioni, l’ottimizzatore potrebbe semplicemente sbagliare. In questo caso, modificare i criteri di convergenza, passare a un altro algoritmo di ottimizzazione o fornire valori iniziali migliori potrebbe risolvere il problema. Ma se la dimensione del campione è piccola, potrebbe benissimo essere che il set di dati non contenga informazioni sufficienti per trovare una soluzione unica per il modello.\nUn secondo problema potrebbe essere che il modello ottenga la convergenza ma produca una soluzione non ammissibile. Ciò significa che alcuni parametri assumono valori inamissibili. L’esempio più comune è una varianza negativa. Un altro esempio è un valore di correlazione che supera 1 (in valore assoluto). È importante rendersi conto che alcuni approcci di stima (sia frequentisti che bayesiani) potrebbero, per progettazione, non produrre mai soluzioni fuori gamma. Sebbene ciò possa sembrare una caratteristica desiderabile, maschera potenziali problemi con il modello o i dati. È importante che gli utenti notino varianze negative (o altri parametri fuori gamma). Le varianze negative sono spesso innocue, ma possono essere un sintomo di una cattiva specificazione strutturale.\nUn terzo problema riguarda il fatto che la massima verosimiglianza è una tecnica per grandi campioni. Questo implica che lavorare con piccole dimensioni del campione può portare a stime puntuali distorte, errori standard troppo piccoli, intervalli di confidenza non sufficientemente ampi e p-valori per test di ipotesi non affidabili.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Modellizzazione SEM in Piccoli Campioni</span>"
    ]
  },
  {
    "objectID": "chapters/sem/11_small_samples.html#soluzioni-possibili-per-la-stima-dei-parametri",
    "href": "chapters/sem/11_small_samples.html#soluzioni-possibili-per-la-stima-dei-parametri",
    "title": "58  Modellizzazione SEM in Piccoli Campioni",
    "section": "58.3 Soluzioni Possibili per la Stima dei Parametri",
    "text": "58.3 Soluzioni Possibili per la Stima dei Parametri\nIn questa sezione, consideriamo brevemente quattro approcci alternativi per stimare i parametri in un contesto SEM con dimensioni di campione piccole. Ci limitiamo ai metodi frequentisti e alle soluzioni disponibili in software gratuiti e open-source.\n\n58.3.1 Stima di Verosimiglianza Penalizzata\nI metodi di stima di verosimiglianza penalizzata (o metodi di regolarizzazione) sono stati sviluppati nella letteratura del machine learning statistico e sono particolarmente utili quando la dimensione del campione è piccola rispetto al numero di variabili nel modello. Questi metodi sono simili ai metodi di verosimiglianza ordinari (come la massima verosimiglianza) ma includono un termine di penalità aggiuntivo per controllare la complessità del modello. Il termine di penalità può essere formulato per incorporare conoscenze pregresse sui parametri o per scoraggiare valori dei parametri meno realistici (ad esempio, lontani da zero). Due termini di penalità popolari sono la penalità ridge e la penalità lasso (least absolute shrinkage and selection operator).\nPer illustrare come funziona questa penalizzazione, immaginate un modello di regressione univariata con un gran numero di predittori. Senza penalizzazione, tutti i coefficienti di regressione sono calcolati nel modo usuale. Tuttavia, il termine di penalità ridge ridurrà tutti i coefficienti verso zero, mentre la penalità lasso ridurrà ulteriormente i piccoli coefficienti fino a zero. In quest’ultimo approccio, sopravvivono solo i predittori “forti” (per i quali c’è un forte supporto nei dati), mentre i predittori “deboli” che possono essere difficilmente distinti dal rumore vengono eliminati. In generale, l’aggiunta di termini di penalità porta a modelli meno complessi, il che è particolarmente vantaggioso se la dimensione del campione è piccola.\nSebbene queste approcci di penalizzazione siano esistiti da alcuni decenni, sono stati applicati solo di recente alla SEM. Due esempi nel software R sono il pacchetto regsem (Jacobucci, Grimm, Brandmaier, Serang e Kievit, 2018) e il pacchetto lslx (Huang e Hu, 2018).\nUno svantaggio di questi metodi di penalizzazione è che l’utente deve indicare quali parametri richiedono la penalizzazione e in che misura. In un’analisi esplorativa, può essere utile e persino vantaggioso penalizzare i parametri verso zero se nel dati non si trova un forte supporto per essi. Tuttavia, la SEM è di solito un approccio confermativo, e l’utente deve assicurarsi che tutti i parametri inizialmente postulati nel modello non vengano rimossi dalla penalizzazione.\n\n\n58.3.2 Model-implied instrumental variables\nBollen (1996) ha proposto un approccio alternativo di stima per i modelli SEM basato sull’utilizzo di variabili strumentali implicite nel modello in combinazione con il metodo dei minimi quadrati a due stadi (MIIV-2SLS). In questo approccio, il modello viene tradotto in un insieme di equazioni (di regressione). Successivamente, ogni variabile latente in queste equazioni viene sostituita con il suo indicatore principale (solitamente il primo indicatore, dove il carico fattoriale è fissato a uno e l’intercetta a zero) meno il suo termine di errore residuo. Le equazioni risultanti non contengono più variabili latenti ma hanno una struttura dell’errore più complessa. È importante notare che la stima dei minimi quadrati ordinari non è più adatta per risolvere queste equazioni poiché alcuni predittori sono ora correlati con il termine di errore nell’equazione. Qui entrano in gioco le variabili strumentali (anche chiamate strumenti). Per ogni equazione, è necessario trovare un insieme di variabili strumentali. Una variabile strumentale deve essere non correlata con il termine di errore dell’equazione ma fortemente correlata con il predittore problematico. Di solito, le variabili strumentali sono ricercate al di fuori del modello, ma nell’approccio di Bollen, le variabili strumentali sono selezionate tra le variabili osservate che fanno parte del modello. Sono stati sviluppati diversi procedimenti automatizzati per trovare queste variabili strumentali all’interno del modello. Una volta selezionati gli strumenti, è necessaria una procedura di stima per stimare tutti i coefficienti delle equazioni, come il metodo dei minimi quadrati a due stadi (2SLS).\nUna motivazione principale per MIIV-2SLS è che è robusto: non si basa sulla normalità ed è meno probabile che diffonda il bias (che può derivare da errate specificazioni strutturali) in una parte del modello ad altre parti del modello. Un’altra caratteristica attraente di MIIV-2SLS è che non è iterativo. Ciò significa che non possono esserci problemi di convergenza e MIIV-2SLS può fornire una soluzione ragionevole per modelli in cui il massimo verosimigliante fallisce nella convergenza.\nSono necessarie ulteriori ricerche per valutare le prestazioni di questo stimatore in contesti in cui la dimensione del campione è (molto) piccola. L’approccio MIIV-2SLS è disponibile nel pacchetto R MIIVsem (Fisher, Bollen, Gates, & Rönkkö, 2017).\n\n\n58.3.3 Stima a Due Fasi\nNel metodo di stima a due fasi, si effettua una distinzione tra la parte di misurazione e la parte strutturale (di regressione) del modello, e la stima avviene in due passaggi distinti. Nel primo passo, vengono adattati uno per uno tutti i modelli di misurazione. Nel secondo passo, viene adattato il modello completo, inclusa la parte strutturale, ma i parametri dei modelli di misurazione vengono mantenuti fissi ai valori trovati nel primo passo. La principale motivazione per l’approccio a due fasi è quella di separare il modello (o i modelli) di misurazione dalla parte strutturale durante la stima in modo che non possano influenzarsi reciprocamente. Nel tradizionale framework della massima verosimiglianza, invece, tutti i parametri vengono adattati simultaneamente. Di conseguenza, errori nella specificazione del modello strutturale possono influenzare i pesi fattoriali stimati di uno o più modelli di misurazione, e ciò può causare problemi di interpretazione per le variabili latenti.\nL’approccio a due fasi è stato recentemente implementato nel pacchetto R lavaan (Rosseel, 2012).\n\n\n58.3.4 Regressione del Punteggio dei Fattori\nL’idea fondamentale della regressione del punteggio dei fattori è quella di sostituire tutte le variabili latenti con i loro punteggi. Questo processo è simile al metodo in due fasi, dove ciascun modello di misurazione viene adattato individualmente. Successivamente, si calcolano i punteggi dei fattori per tutte le variabili latenti nel modo consueto. Una volta che le variabili latenti sono sostituite dai loro punteggi, tutte le variabili diventano osservabili. In un passaggio finale, si stima la parte strutturale del modello. Questa stima può consistere in un’analisi di regressione o in un’analisi dei percorsi. Il termine “regressione del punteggio dei fattori” si riferisce a entrambi gli scenari.\nSe usata in modo ingenuo, questa regressione potrebbe portare a un notevole bias nelle stime dei parametri della parte strutturale, anche con campioni di grandi dimensioni. Questo si verifica perché i punteggi dei fattori vengono trattati come se fossero osservati senza errore di misurazione. Esistono però diversi metodi per correggere questo bias. Ad esempio, il metodo di Croon (2002) procede come segue: prima, si calcola la matrice di varianza-covarianza dei punteggi dei fattori. Poi, sulla base delle informazioni dei modelli di misurazione, gli elementi di questa matrice vengono corretti per approssimare le varianze e covarianze implicite dal modello delle variabili latenti. Questa matrice di varianza-covarianza corretta diventa poi l’input per un’analisi di regressione o dei percorsi regolare.\nSimile al metodo in due fasi, la regressione del punteggio dei fattori (combinata con la correzione di Croon) può essere un’alternativa utile per modelli piuttosto grandi in combinazione con una dimensione campionaria relativamente piccola. Inoltre, è possibile adattare i modelli di misurazione utilizzando un stimatore non iterativo, evitando problemi di convergenza. Tuttavia, la correzione di Croon può produrre una matrice di varianza-covarianza (per le variabili appartenenti alla parte strutturale) che non è definita positiva, specialmente se l’errore di misurazione è sostanziale. Pertanto, la correzione di Croon non è esente da problemi di stima. In questo caso, l’unica soluzione potrebbe essere quella di creare un punteggio somma per ogni variabile latente e stimare un modello in cui ogni variabile latente ha un unico indicatore (il punteggio somma) con la sua affidabilità fissata a un valore realistico fornito dall’utente.\n\n\n58.3.5 Discussione\nTutti i metodi descritti in questa sezione hanno vantaggi e svantaggi. L’approccio della verosimiglianza penalizzata è forse l’unico metodo specificamente progettato per gestire campioni (molto) piccoli. Gli altri tre metodi utilizzano un approccio di “divide et impera”; scompongono il modello completo in parti più piccole e stimano i parametri di ciascuna parte in successione. Oltre a ridurre la complessità e a essere meno vulnerabili a problemi di convergenza, gli ultimi tre metodi hanno il vantaggio di essere efficaci nel localizzare le parti problematiche all’interno di un modello ampio.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Modellizzazione SEM in Piccoli Campioni</span>"
    ]
  },
  {
    "objectID": "chapters/sem/11_small_samples.html#inferenze-per-modelli-sem-in-piccoli-campioni",
    "href": "chapters/sem/11_small_samples.html#inferenze-per-modelli-sem-in-piccoli-campioni",
    "title": "58  Modellizzazione SEM in Piccoli Campioni",
    "section": "58.4 Inferenze per Modelli SEM in Piccoli Campioni",
    "text": "58.4 Inferenze per Modelli SEM in Piccoli Campioni\nMolti autori hanno documentato che quando la dimensione del campione è piccola, il test del chi-quadrato porta ad un inflezione degli errori di Tipo I anche nelle circostanze ideali (cioè, modello correttamente specificato, dati normali). Allo stesso modo, gli errori standard sono spesso attenuati (troppo piccoli) e gli intervalli di confidenza non sono sufficientemente ampi.\nNelle due sottosezioni successive, {cite:t}rosseel2020small discute brevemente alcuni tentativi per affrontare questi problemi di inferenza su campioni piccoli nella modellizzazione SEM.\n\n58.4.1 Migliorare la statistica test del chi-quadrato\nSono state suggerite diverse correzioni per migliorare le prestazioni della statistica del test del chi-quadrato, come la correzione di Bartlett. I risultati di studi di simulazione su questo tema, però, non sono coerenti e, secondo {cite:t}rosseel2020small, per valutare i modelli quando la dimensione del campione è piccola, potrebbe essere opportuno abbandonare del tutto il test del chi-quadrato e esplorare approcci alternativi.\nUn approccio è quello di considerare gli intervalli di confidenza e i test di aderenza basati sull’indice SRMR (standardized root mean square residuals; Maydeu-Olivares, Shi, & Rosseel, 2018). Questi test sembrano funzionare bene anche quando n = 100 (la dimensione del campione più piccola considerata in Maydeu-Olivares et al., 2018) e il modello non è troppo grande. Questi test sono stati implementati come parte della funzione lavResiduals() del pacchetto lavaan.\n\n\n58.4.2 Una migliore stima degli Errori Standard e degli Intervalli di Confidenza\nIn generale, è ben noto che se si utilizza la teoria dei grandi campioni per costruire espressioni analitiche al fine di calcolare gli errori standard, questi possono avere prestazioni scadenti in campioni di piccole dimensioni.\nQuando le assunzioni alla base degli errori standard analitici non sono soddisfatte, spesso si suggerisce di utilizzare un approccio di resampling. Un metodo popolare è il bootstrap (Efron & Tibshirani, 1993): viene generato un campione bootstrap (o campione di replica) e si stima un nuovo set di parametri per questo campione bootstrap. Questo processo viene ripetuto un gran numero di volte (ad esempio, 1.000), e la deviazione standard di un parametro su tutti i campioni bootstrap replicati viene utilizzata come stima dell’errore standard per quel parametro. Purtroppo, nonostante molti altri vantaggi, sembra che il bootstrap non sia una soluzione affidabile quando la dimensione del campione è (molto) piccola (Yung & Bentler, 1996).\nIn alternativa, sono state sviluppate correzioni per dimensioni di campione piccole sugli errori standard (robusti) e sono state recentemente adattate al contesto SEM. Tuttavia, questa tecnologia non è ancora disponibile nei software SEM.\n\n\n58.4.3 Strategie Alternative\nOltre ai metodi precedenti suggeriti da {cite:t}rosseel2020small, altre strategie possibili sono stati indicate da {cite:t}kline2023principles.\n\nSelezione di Indicatori con Elevate Caratteristiche Psicometriche: {cite:t}kline2023principles consiglia di utilizzare indicatori che mostrino eccellenti proprietà psicometriche, idealmente con carichi standardizzati superiori a .70 per gli indicatori continui. Questa pratica riduce il rischio di incorrere in casi di Heywood.\nApplicazione di Restrizioni di Uguaglianza sui Carichi Non Standardizzati: Imponendo vincoli di uguaglianza sulle saturazioni degli indicatori relativi allo stesso fattore si possono evitare soluzioni inammissibili. Questo approccio è particolarmente valido quando gli indicatori sono sulla stessa scala. Un’alternativa valida consiste nel fissare le saturazioni degli indicatori dello stesso fattore a valori costanti non nulli, che riflettano le variazioni nelle loro deviazioni standard.\nSEM Basato su Compositi per Modelli Complessi in Campioni Piccoli: Questo approccio implica l’utilizzo di metodi basati su compositi, che sono combinazioni lineari di variabili osservate. In pratica, le variabili osservate vengono combinate in modi specifici per formare indicatori composti che rappresentano i costrutti latenti nel modello. Questi indicatori composti vengono quindi utilizzati per stimare i parametri del modello SEM. L’obiettivo è ottenere risultati per modelli complessi che richiederebbero campioni molto più ampi con il tradizionale approccio SEM. Tuttavia, è importante notare che i risultati del SEM basato su compositi possono essere soggetti a distorsioni anche nei campioni di piccole dimensioni.\nParceling: Questa strategia coinvolge la creazione di “parcel” o aggregati di due o più indicatori a livello di item attraverso la media dei singoli item. In altre parole, gli indicatori originali sono raggruppati in insiemi più piccoli e i loro valori sono combinati per creare nuovi indicatori aggregati. Questi nuovi indicatori vengono quindi utilizzati per rappresentare i costrutti latenti nel modello SEM. Sebbene il parceling possa ridurre la varianza dell’errore e migliorare la stabilità dei modelli, è importante considerare le sue limitazioni, tra cui la possibile perdita di informazioni e la sensibilità alle scelte fatte durante il processo di parceling. I risultati ottenuti con il parceling possono variare notevolmente in base alle decisioni prese dal ricercatore durante l’analisi.\n\n\n58.4.3.1 Parceling\nApprofondiamo brevemente la strategia del parceling. Il parceling è una strategia che comporta la suddivisione di un set di item in gruppi più piccoli, o “parcel”, per semplificare i modelli e migliorare la loro stima e adattamento.\nUn esempio, citato in {cite:t}kline2023principles, illustra come il parceling possa essere utilizzato in una CFA per ridurre il numero di indicatori e semplificare il modello. {cite:t}kline2023principles considera un questionario di 120 item diviso in tre gruppi distinti di 40 item ciascuno, ognuno mirato a misurare un dominio specifico di un costrutto. In un campione di 150 partecipanti, un’analisi fattoriale confermativa (CFA) con tre fattori e 40 indicatori per fattore, con 120 indicatori totali, può presentare sfide notevoli nella stima del modello a causa della ridotta dimensione del campione. Per affrontare questi problemi, il ricercatore può suddividere ogni gruppo di 40 item in 4 gruppi minori (o “parcel”) di 10 item ciascuno, sommando i punteggi all’interno di ogni “parcel”. Questi punteggi aggregati sostituiscono poi gli item singoli come indicatori in un modello CFA a 3 fattori che avrà quindi solo 12 indicatori in totale (4 indicatori parcellizzati per fattore). Se gli indicatori parcellizzati hanno una distribuzione normale, per la stima si può ricorrere al metodo dei minimi quadrati (ML); altrimenti, si può utilizzare un estimatore ML robusto.\nQuesto metodo è particolarmente utile in situazioni dove si hanno molti item e campioni di dimensioni ridotte, e offre diversi benefici, tra cui:\n\nMaggiore Affidabilità: Il parceling può aumentare l’affidabilità di una scala psicometrica, poiché gli item aggregati tendono ad avere maggior coerenza interna rispetto agli item singoli.\nRapporto Varianza Comune/Varianza Unica: Utilizzando il parceling, si può ottenere un rapporto più favorevole tra la varianza comune (quella spiegata dai fattori comuni) e la varianza unica (quella non spiegata).\nMinore Probabilità di Violazioni Distribuzionali: La pratica del parceling riduce la probabilità che le assunzioni distribuzionali siano violate, il che è importante per l’applicazione di certe tecniche statistiche.\n\nNonostante i suoi benefici, il parceling ha anche limitazioni. La metodologia utilizzata per formare i parcel può influenzare i risultati. Inoltre, il parceling non è consigliabile quando gli item all’interno di un parcel non sono unidimensionali, poiché ciò può distorcere i risultati. È fondamentale verificare l’unidimensionalità prima di procedere con il parceling.\nIn studi con campioni di piccole dimensioni, il parceling ha dimostrato diversi vantaggi, come mostrato nello studio di simulazione di Orçan e Yanyun (2016). Questi includono una riduzione della complessità del modello, tassi di errore di Tipo I più ragionevoli e tassi di errore di Tipo I più bassi quando si utilizza il metodo di stima della massima verosimiglianza con errori standard robusti (MLR) a livello di parcel.\nIn conclusione, il parceling è una tecnica utile che può migliorare l’affidabilità e la validità dei modelli psicometrici, specialmente in presenza di grandi set di item e campioni di piccole dimensioni. Tuttavia, è essenziale valutare attentamente la sua applicabilità e procedere con cautela, specialmente per quanto riguarda l’unidimensionalità dei parcel.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Modellizzazione SEM in Piccoli Campioni</span>"
    ]
  },
  {
    "objectID": "chapters/sem/11_small_samples.html#sem-in-un-piccolo-campione",
    "href": "chapters/sem/11_small_samples.html#sem-in-un-piccolo-campione",
    "title": "58  Modellizzazione SEM in Piccoli Campioni",
    "section": "58.5 SEM in un Piccolo Campione",
    "text": "58.5 SEM in un Piccolo Campione\n{cite:t}kline2023principles discute uno studio in cui è stato applicato un modello CFA a due fattori ad un campione di 103 donne, le quali hanno compilato questionari su esperienze di origine familiare e adattamento coniugale {cite:p}sabatelli2003family.\n\n# input the correlations in lower diagnonal form\nsabatelliLower.cor &lt;- \"\n 1.000\n  .740 1.000\n  .265  .422 1.000\n  .305  .401  .791 1.000\n  .315  .351  .662  .587 1.000 \"\n\n# name the variables and convert to full correlation matrix\nsabatelli.cor &lt;- lavaan::getCov(sabatelliLower.cor, names = c(\n    \"problems\", \"intimacy\", \"father\", \"mother\", \"both\"\n    )\n)\n\n# add the standard deviations and convert to covariances\nsabatelli.cov &lt;- lavaan::cor2cov(sabatelli.cor, sds = c(\n    32.936, 22.749, 13.390, 13.679, 14.382\n    )\n)\n\nIl modello proposto dagli autori è specificato di seguito:\n\nsabatelli_model &lt;- \"\n    # common factors\n    # loading for intimacy constrained to equal .691\n    Marital =~ problems + intimacy\n    FOE =~ father + mother + both\n\"\n\nIn riferimento al modello specificato sopra, la soluzione fornita da lavaan risulta inammissibile a causa di un caso di Heywood, evidenziato da una varianza d’errore negativa per la variabile “intimità”.\n\noriginal &lt;- lavaan::sem(sabatelli_model,\n    sample.cov = sabatelli.cov,\n    sample.nobs = 103\n)\n\n\nlavaan::summary(original,\n    fit.measures = TRUE, standardized = TRUE,\n    rsquare = TRUE\n) |&gt; print()\n\nlavaan 0.6.17 ended normally after 141 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        11\n\n  Number of observations                           103\n\nModel Test User Model:\n                                                      \n  Test statistic                                 4.688\n  Degrees of freedom                                 4\n  P-value (Chi-square)                           0.321\n\nModel Test Baseline Model:\n\n  Test statistic                               271.302\n  Degrees of freedom                                10\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.997\n  Tucker-Lewis Index (TLI)                       0.993\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -2087.964\n  Loglikelihood unrestricted model (H1)      -2085.620\n                                                      \n  Akaike (AIC)                                4197.928\n  Bayesian (BIC)                              4226.910\n  Sample-size adjusted Bayesian (SABIC)       4192.163\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.041\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.159\n  P-value H_0: RMSEA &lt;= 0.050                    0.448\n  P-value H_0: RMSEA &gt;= 0.080                    0.387\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.028\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Marital =~                                                            \n    problems          1.000                              23.362    0.713\n    intimacy          1.006    0.221    4.547    0.000   23.503    1.038\n  FOE =~                                                                \n    father            1.000                              12.488    0.937\n    mother            0.919    0.089   10.320    0.000   11.480    0.843\n    both              0.808    0.098    8.206    0.000   10.088    0.705\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Marital ~~                                                            \n    FOE             129.409   44.216    2.927    0.003    0.444    0.444\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .problems        528.472  130.514    4.049    0.000  528.472    0.492\n   .intimacy        -39.892  109.200   -0.365    0.715  -39.892   -0.078\n   .father           21.613   10.983    1.968    0.049   21.613    0.122\n   .mother           53.509   11.710    4.570    0.000   53.509    0.289\n   .both            103.075   16.168    6.375    0.000  103.075    0.503\n    Marital         545.776  169.103    3.227    0.001    1.000    1.000\n    FOE             155.939   26.732    5.833    0.000    1.000    1.000\n\nR-Square:\n                   Estimate\n    problems          0.508\n    intimacy             NA\n    father            0.878\n    mother            0.711\n    both              0.497\n\n\n\nPer ovviare ad un tale problema, in una nuova analisi del modello di adattamento coniugale è stato applicato un vincolo specifico ai carichi non standardizzati degli indicatori. A causa delle differenze sostanziali nelle metriche tra le due variabili, ovvero “intimacy” (con una deviazione standard di 22.749) e “problems” (con una deviazione standard di 32.936), sono state fissate le seguenti saturazioni fattoriali: il carico per la variabile “problemi” è stato fissato a 1, mentre il carico per la variabile “intimità” è stato fissato a 0.691. Questi valori sono stati calcolati in modo da riflettere proporzionalmente la differenza nelle deviazioni standard tra le due variabili.\n\n# analysis with constrained loadings for indicators of marital adjustment\n# model df = 5\n\n# standard deviations for both indicators\n# of the marital factor are listed next\n# intimacy, sd = 22.749\n# problems, sd = 32.936\n# ratio = 22.749/32.936 = .691\n\n# specify model with constrained loadings for problems, intimacy\n\nproportional.model &lt;- \"\n    # common factors\n    # loading for intimacy constrained to equal .691\n    Marital =~ 1*problems + .691*intimacy\n    FOE =~ father + mother + both \n\"\n\n\nproportional &lt;- lavaan::sem(proportional.model,\n    sample.cov = sabatelli.cov,\n    sample.nobs = 103\n)\n\n\nsemPlot::semPaths(proportional,\n    what = \"col\", whatLabels = \"par\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\n\n\nlavaan::summary(proportional,\n    fit.measures = TRUE, standardized = TRUE,\n    rsquare = TRUE\n) |&gt; print()\n\nlavaan 0.6.17 ended normally after 110 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n\n  Number of observations                           103\n\nModel Test User Model:\n                                                      \n  Test statistic                                 8.449\n  Degrees of freedom                                 5\n  P-value (Chi-square)                           0.133\n\nModel Test Baseline Model:\n\n  Test statistic                               271.302\n  Degrees of freedom                                10\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.987\n  Tucker-Lewis Index (TLI)                       0.974\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -2089.845\n  Loglikelihood unrestricted model (H1)      -2085.620\n                                                      \n  Akaike (AIC)                                4199.690\n  Bayesian (BIC)                              4226.037\n  Sample-size adjusted Bayesian (SABIC)       4194.449\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.082\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.174\n  P-value H_0: RMSEA &lt;= 0.050                    0.242\n  P-value H_0: RMSEA &gt;= 0.080                    0.584\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.045\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Marital =~                                                            \n    problems          1.000                              28.391    0.840\n    intimacy          0.691                              19.618    0.885\n  FOE =~                                                                \n    father            1.000                              12.373    0.929\n    mother            0.935    0.091   10.279    0.000   11.568    0.850\n    both              0.821    0.100    8.235    0.000   10.155    0.710\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Marital ~~                                                            \n    FOE             164.822   42.788    3.852    0.000    0.469    0.469\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .problems        335.923   80.080    4.195    0.000  335.923    0.294\n   .intimacy        106.214   34.373    3.090    0.002  106.214    0.216\n   .father           24.457   11.060    2.211    0.027   24.457    0.138\n   .mother           51.489   11.730    4.389    0.000   51.489    0.278\n   .both            101.712   16.070    6.329    0.000  101.712    0.497\n    Marital         806.040  132.946    6.063    0.000    1.000    1.000\n    FOE             153.095   26.669    5.741    0.000    1.000    1.000\n\nR-Square:\n                   Estimate\n    problems          0.706\n    intimacy          0.784\n    father            0.862\n    mother            0.722\n    both              0.503\n\n\n\n\nfitMeasures(proportional, c(\"chisq\", \"df\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt;\n    print()\n\nchisq    df   cfi   tli rmsea  srmr \n8.449 5.000 0.987 0.974 0.082 0.045 \n\n\n\nlavaan::parameterEstimates(proportional) |&gt; print()\n\n        lhs op      rhs     est      se      z pvalue ci.lower ci.upper\n1   Marital =~ problems   1.000   0.000     NA     NA    1.000    1.000\n2   Marital =~ intimacy   0.691   0.000     NA     NA    0.691    0.691\n3       FOE =~   father   1.000   0.000     NA     NA    1.000    1.000\n4       FOE =~   mother   0.935   0.091 10.279  0.000    0.757    1.113\n5       FOE =~     both   0.821   0.100  8.235  0.000    0.625    1.016\n6  problems ~~ problems 335.923  80.080  4.195  0.000  178.970  492.877\n7  intimacy ~~ intimacy 106.214  34.373  3.090  0.002   38.843  173.584\n8    father ~~   father  24.457  11.060  2.211  0.027    2.779   46.134\n9    mother ~~   mother  51.489  11.730  4.389  0.000   28.498   74.481\n10     both ~~     both 101.712  16.070  6.329  0.000   70.216  133.208\n11  Marital ~~  Marital 806.040 132.946  6.063  0.000  545.471 1066.608\n12      FOE ~~      FOE 153.095  26.669  5.741  0.000  100.825  205.365\n13  Marital ~~      FOE 164.822  42.788  3.852  0.000   80.959  248.684\n\n\n\nlavaan::standardizedSolution(proportional) |&gt; print()\n\n        lhs op      rhs est.std    se      z pvalue ci.lower ci.upper\n1   Marital =~ problems   0.840 0.036 23.439  0.000    0.770    0.910\n2   Marital =~ intimacy   0.885 0.037 24.040  0.000    0.813    0.957\n3       FOE =~   father   0.929 0.035 26.779  0.000    0.861    0.997\n4       FOE =~   mother   0.850 0.040 21.126  0.000    0.771    0.929\n5       FOE =~     both   0.710 0.055 12.800  0.000    0.601    0.818\n6  problems ~~ problems   0.294 0.060  4.884  0.000    0.176    0.412\n7  intimacy ~~ intimacy   0.216 0.065  3.317  0.001    0.088    0.344\n8    father ~~   father   0.138 0.064  2.139  0.032    0.012    0.264\n9    mother ~~   mother   0.278 0.068  4.065  0.000    0.144    0.412\n10     both ~~     both   0.497 0.079  6.313  0.000    0.342    0.651\n11  Marital ~~  Marital   1.000 0.000     NA     NA    1.000    1.000\n12      FOE ~~      FOE   1.000 0.000     NA     NA    1.000    1.000\n13  Marital ~~      FOE   0.469 0.091  5.177  0.000    0.292    0.647\n\n\nNonostante gli indici di bontà di adattamento siano eccellenti, la potenza di questa analisi statistica risulta estremamente limitata. Per valutare questa limitazione, è possibile utilizzare la funzione semTools::findRMSEAsamplesize(). Questa funzione calcola la dimensione del campione necessaria per rilevare una differenza significativa tra RMSEA_0 e RMSEA_A, considerando un modello con df gradi di libertà.\nPer esempio, se desideriamo distinguere tra RMSEA_0=0.05 e RMSEA_A=0.10 utilizzando il modello attuale con 5 gradi di libertà, la funzione ci indica che sono necessarie 561 osservazioni per ottenere una potenza statistica di 0.8:\n\nsemTools::findRMSEAsamplesize(0.05, .10, 5, .80, .05, 1)\n\n561\n\n\nPer creare un grafico che rappresenti la potenza statistica per rilevare la differenza tra RMSEA_0=0.05 e RMSEA_A=0.10 (utilizzati qui come esempio) al variare della dimensione del campione, è possibile seguire la seguente procedura:\n\nsemTools::plotRMSEApower(rmsea0 = .05, rmseaA = .10, df = 5, 50, 1000)\n\n\n\n\n\n\n\n\nQuesta analisi di potenza indica che la dimensione del campione utilizzato (\\(n\\) = 103) è del tutto inadeguata.\nPer migliorare il nostro giudizio sull’adattamento del modello consideriamo l’analisi dei residui.\n\nlavaan::residuals(proportional, type = \"standardized.mplus\") |&gt; print()\n\n$type\n[1] \"standardized.mplus\"\n\n$cov\n         prblms intmcy father mother   both\nproblems     NA                            \nintimacy     NA  0.918                     \nfather   -3.994  1.039  0.002              \nmother   -0.871  1.049  0.328  0.000       \nboth      0.407  0.930  0.352 -2.776     NA\n\n\n\n\nlavaan::lavResiduals(proportional, type = \"cor.bollen\", summary = TRUE) |&gt; print()\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n         prblms intmcy father mother   both\nproblems  0.000                            \nintimacy -0.004  0.000                     \nfather   -0.101  0.036  0.000              \nmother   -0.030  0.048  0.002  0.000       \nboth      0.035  0.056  0.003 -0.016  0.000\n\n$cov.z\n         prblms intmcy father mother   both\nproblems  0.000                            \nintimacy -2.524  0.000                     \nfather   -2.386  1.245  0.000              \nmother   -0.586  1.134  0.881  0.000       \nboth      0.523  0.936  0.507 -1.143  0.000\n\n$summary\n                           cov\ncrmr                     0.044\ncrmr.se                  0.015\ncrmr.exactfit.z          0.504\ncrmr.exactfit.pvalue     0.307\nucrmr                    0.023\nucrmr.se                 0.029\nucrmr.ci.lower          -0.024\nucrmr.cilupper           0.071\nucrmr.closefit.h0.value  0.050\nucrmr.closefit.z        -0.928\nucrmr.closefit.pvalue    0.823\n\n\n\nQuesti sono risultati relativamente scarsi per un modello così piccolo. Il computer non è stato in grado di calcolare tutti i residui standardizzati possibili, il che non è sorprendente in un campione così ridotto.\n\n58.5.1 Stimatore MIIV-2SLS\nUna seconda analisi viene condotta utilizzando lo stimatore MIIV-2SLS. Il pacchetto MIIVsem non calcola statistiche globali di bontà di adattamento. Al contrario, calcola il test di Sargan per ciascun indicatore previsto dal modello. Le statistiche del test di Sargan approssimano distribuzioni chi-quadro centrali con gradi di libertà equivalenti al numero di item meno uno, quindi df = 2. L’ipotesi nulla è che ogni insieme di strumenti multipli sia incorrelato con il termine di errore per l’equazione. Il mancato rifiuto dell’ipotesi nulla per il test di Sargan suggerisce una buona corrispondenza del modello con i dati.\n\nMIIVsem::miivs(sabatelli_model)\n\nModel Equation Information \n\n LHS        RHS        MIIVs                     \n intimacy   problems   father, mother, both      \n mother     father     problems, intimacy, both  \n both       father     problems, intimacy, mother\n\n\n\n\nsabatelli &lt;- MIIVsem::miive(sabatelli_model,\n    sample.cov = sabatelli.cov,\n    sample.nobs = 103, var.cov = TRUE\n)\n\nlavaan::summary(sabatelli, rsquare = TRUE) |&gt; print()\n\nMIIVsem (0.5.8) results \n\nNumber of observations                                                    103\nNumber of equations                                                         3\nEstimator                                                           MIIV-2SLS\nStandard Errors                                                      standard\nMissing                                                              listwise\n\n\nParameter Estimates:\n\n\nSTRUCTURAL COEFFICIENTS:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Sargan   df   P(Chi)\n  FOE =~                                                                     \n    father            1.000                                                  \n    mother            0.899    0.089   10.149    0.000    1.763    2    0.414\n    both              0.787    0.099    7.935    0.000    3.590    2    0.166\n  Marital =~                                                                 \n    problems          1.000                                                  \n    intimacy          0.805    0.155    5.195    0.000    4.980    2    0.083\n\nINTERCEPTS:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   \n    both              0.000                              \n    father            0.000                              \n    intimacy          0.000                              \n    mother            0.000                              \n    problems          0.000                              \n\nVARIANCES:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   \n    FOE             158.501                              \n    Marital         702.393                              \n    both            103.301                              \n    father           20.856                              \n    intimacy         50.871                              \n    mother           54.195                              \n    problems        422.427                              \n\nCOVARIANCES:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   \n  Marital ~~                                             \n    FOE             157.495                              \n\nR-SQUARE:\n                   Estimate\n    problems          0.624\n    intimacy          0.900\n    father            0.884\n    mother            0.703\n    both              0.487\nNULL\n\n\nSi noti che le stime standardizzate non sono calcolate nella versione del pacchetto MIIVsem utilizzata in questa analisi. I valori non standardizzati delle saturazioni fattoriali sono simili a quelli ottenuti in precedenza.\nIl pacchetto MIIVsem non fornisce né le correlazioni previste dal modello per gli indicatori né i residui di correlazione. Per ottenere i residui di correlazione per l’estimatore 2SLS, è possibile utilizzare il pacchetto lavaan per specificare nuovamente il modello precedentemente adattato, ma con l’importante modifica di fissare tutti i parametri non standardizzati in modo che siano identici alle loro controparti 2SLS. Successivamente, è possibile adattare nuovamente il modello con questi parametri fissati alla matrice di covarianza. La matrice di correlazione prevista in questa analisi si basa sulle stime dei parametri 2SLS, consentendo così di ottenere i residui di correlazione desiderati.\n\nsabatelliFixed.model &lt;- \"\n    # common factors\n    Marital =~ 1.0*problems + .805*intimacy\n    FOE =~ 1.0*father + .899*mother + .787*both\n    # factor variances, covariances\n    FOE ~~ 158.501*FOE\n    Marital ~~ 157.495*FOE\n    Marital ~~ 702.393*Marital\n    # indicator error variances\n    father ~~ 20.856*father\n    mother ~~ 54.195*mother\n    both ~~ 103.301*both\n    problems ~~ 422.427*problems\n    intimacy ~~ 50.781*intimacy \n \"\n\n\nsabatelliFixed &lt;- lavaan::sem(sabatelliFixed.model,\n    sample.cov = sabatelli.cov,\n    sample.nobs = 103\n)\n\n\n# standardized parameter \"estimates\" listed\n# next are fixed to nonzero constants, and\n# standard errors are undefined\nlavaan::parameterEstimates(sabatelliFixed)\n\n\nA lavaan.data.frame: 13 x 9\n\n\nlhs\nop\nrhs\nest\nse\nz\npvalue\nci.lower\nci.upper\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nMarital\n=~\nproblems\n1.000\n0\nNA\nNA\n1.000\n1.000\n\n\nMarital\n=~\nintimacy\n0.805\n0\nNA\nNA\n0.805\n0.805\n\n\nFOE\n=~\nfather\n1.000\n0\nNA\nNA\n1.000\n1.000\n\n\nFOE\n=~\nmother\n0.899\n0\nNA\nNA\n0.899\n0.899\n\n\nFOE\n=~\nboth\n0.787\n0\nNA\nNA\n0.787\n0.787\n\n\nFOE\n~~\nFOE\n158.501\n0\nNA\nNA\n158.501\n158.501\n\n\nMarital\n~~\nFOE\n157.495\n0\nNA\nNA\n157.495\n157.495\n\n\nMarital\n~~\nMarital\n702.393\n0\nNA\nNA\n702.393\n702.393\n\n\nfather\n~~\nfather\n20.856\n0\nNA\nNA\n20.856\n20.856\n\n\nmother\n~~\nmother\n54.195\n0\nNA\nNA\n54.195\n54.195\n\n\nboth\n~~\nboth\n103.301\n0\nNA\nNA\n103.301\n103.301\n\n\nproblems\n~~\nproblems\n422.427\n0\nNA\nNA\n422.427\n422.427\n\n\nintimacy\n~~\nintimacy\n50.781\n0\nNA\nNA\n50.781\n50.781\n\n\n\n\n\n\nlavaan::residuals(sabatelliFixed, type = \"standardized.mplus\") |&gt; print()\n\n$type\n[1] \"standardized.mplus\"\n\n$cov\n         prblms intmcy father mother   both\nproblems -0.338                            \nintimacy -0.180  0.092                     \nfather   -0.938  0.016 -0.073              \nmother   -0.120  0.293  0.043  0.116       \nboth      0.491  0.412  0.067  0.100  0.118\n\n\n\n\nlavaan::residuals(sabatelliFixed, type = \"cor.bollen\") |&gt; print()\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n         prblms intmcy father mother   both\nproblems  0.000                            \nintimacy -0.010  0.000                     \nfather   -0.086  0.001  0.000              \nmother   -0.008  0.026  0.003  0.000       \nboth      0.055  0.038  0.006  0.002  0.000\n\n\n\nSi noti che nessuno dei residui di correlazione assoluti basati sui risultati 2SLS supera lo 0.10, compreso il residuo per la coppia di indicatori “problems” e “father”. In termini di adattamento locale, dunque, in questo esempio i risultati dello stimatore 2SLS sono da preferire rispetto a quelli dello stimatore ML.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Modellizzazione SEM in Piccoli Campioni</span>"
    ]
  },
  {
    "objectID": "chapters/sem/11_small_samples.html#considerazioni-conclusive",
    "href": "chapters/sem/11_small_samples.html#considerazioni-conclusive",
    "title": "58  Modellizzazione SEM in Piccoli Campioni",
    "section": "58.6 Considerazioni Conclusive",
    "text": "58.6 Considerazioni Conclusive\nIn questo capitolo abbiamo discusso diversi problemi nel contesto della SEM quando le dimensioni del campione sono ridotte e vengono utilizzati metodi di stima standard (massima verosimiglianza), come la mancata convergenza, le soluzioni non ammissibili, il bias, le statistiche di test poco performanti e gli intervalli di confidenza e gli errori standard inaccurati. Come possibili soluzioni per ottenere stime puntuali migliori, {cite:t}rosseel2020small presenta quattro approcci alternativi alla stima: la stima della verosimiglianza penalizzata, le variabili strumentali derivanti dal modello, la stima a due fasi e la regressione dei punteggi fattoriali. Solo il primo metodo è stato specificamente progettato per gestire campioni ridotti. Gli altri approcci sono stati sviluppati con altre preoccupazioni in mente, ma potrebbero essere alternative valide per la stima quando le dimensioni del campione sono ridotte.\nPer quanto riguarda l’inferenza, {cite:t}rosseel2020small discute vari tentativi per migliorare le prestazioni della statistica del chi-quadro per valutare l’adattamento globale in presenza di campioni ridotti. Per quanto riguarda gli errori standard, sottolinea che il bootstrapping potrebbe non essere la soluzione che stiamo cercando. Per ottenere errori standard (e intervalli di confidenza) migliori nel contesto di campioni ridotti, {cite:t}rosseel2020small ritiene che sia necessario aspettare fino a quando nuove tecnologie saranno disponibili. Altri suggerimenti sono stati forniti da {cite:t}kline2023principles. La tecnica del “parceling” è stata presentata in relazione alla discussione fornita da {cite:t}rioux2020item.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Modellizzazione SEM in Piccoli Campioni</span>"
    ]
  },
  {
    "objectID": "chapters/sem/11_small_samples.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/sem/11_small_samples.html#informazioni-sullambiente-di-sviluppo",
    "title": "58  Modellizzazione SEM in Piccoli Campioni",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] MIIVsem_0.5.8     lme4_1.1-35.5     Matrix_1.7-0      ggokabeito_0.1.0 \n [5] viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1   \n [9] gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6   \n[13] semPlot_1.1.6     lavaan_0.6-18     psych_2.4.6.26    scales_1.3.0     \n[17] markdown_1.13     knitr_1.48        lubridate_1.9.3   forcats_1.0.0    \n[21] stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2       readr_2.1.5      \n[25] tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0  \n[29] here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.16.0  jsonlite_1.8.9     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5.1 farver_2.1.2      \n  [7] nloptr_2.1.1       rmarkdown_2.28     vctrs_0.6.5       \n [10] minqa_1.2.8        base64enc_0.1-3    rstatix_0.7.2     \n [13] htmltools_0.5.8.1  broom_1.0.6        Formula_1.2-5     \n [16] htmlwidgets_1.6.4  plyr_1.8.9         sandwich_3.1-1    \n [19] emmeans_1.10.4     zoo_1.8-12         uuid_1.2-1        \n [22] igraph_2.0.3       mime_0.12          lifecycle_1.0.4   \n [25] pkgconfig_2.0.3    R6_2.5.1           fastmap_1.2.0     \n [28] shiny_1.9.1        digest_0.6.37      OpenMx_2.21.12    \n [31] fdrtool_1.2.18     colorspace_2.1-1   rprojroot_2.0.4   \n [34] Hmisc_5.1-3        fansi_1.0.6        timechange_0.3.0  \n [37] abind_1.4-8        compiler_4.4.1     withr_3.0.1       \n [40] glasso_1.11        htmlTable_2.4.3    backports_1.5.0   \n [43] carData_3.0-5      ggsignif_0.6.4     MASS_7.3-61       \n [46] corpcor_1.6.10     gtools_3.9.5       tools_4.4.1       \n [49] pbivnorm_0.6.0     foreign_0.8-87     zip_2.3.1         \n [52] httpuv_1.6.15      nnet_7.3-19        glue_1.7.0        \n [55] quadprog_1.5-8     promises_1.3.0     nlme_3.1-166      \n [58] lisrelToR_0.3      grid_4.4.1         pbdZMQ_0.3-13     \n [61] checkmate_2.3.2    cluster_2.1.6      reshape2_1.4.4    \n [64] generics_0.1.3     gtable_0.3.5       tzdb_0.4.0        \n [67] data.table_1.16.0  hms_1.1.3          car_3.1-2         \n [70] utf8_1.2.4         sem_3.1-16         pillar_1.9.0      \n [73] IRdisplay_1.1      rockchalk_1.8.157  later_1.3.2       \n [76] splines_4.4.1      lattice_0.22-6     survival_3.7-0    \n [79] kutils_1.73        tidyselect_1.2.1   miniUI_0.1.1.1    \n [82] pbapply_1.7-2      stats4_4.4.1       xfun_0.47         \n [85] qgraph_1.9.8       arm_1.14-4         stringi_1.8.4     \n [88] pacman_0.5.1       boot_1.3-31        evaluate_1.0.0    \n [91] codetools_0.2-20   mi_1.1             cli_3.6.3         \n [94] RcppParallel_5.1.9 IRkernel_1.3.2     rpart_4.1.23      \n [97] xtable_1.8-4       repr_1.1.7         munsell_0.5.1     \n[100] Rcpp_1.0.13        coda_0.19-4.1      png_0.1-8         \n[103] XML_3.99-0.17      parallel_4.4.1     jpeg_0.1-10       \n[106] mvtnorm_1.3-1      openxlsx_4.2.7.1   crayon_1.5.3      \n[109] rlang_1.1.4        multcomp_1.4-26    mnormt_2.1.1      \n\n\n\n\n\n\nKline, R. B. (2023). Principles and practice of structural equation modeling. Guilford publications.\n\n\nRosseel, Y. (2020). Small sample solutions for structural equation modeling. In Small sample size solutions: A guide for applied researchers and practitioners (pp. 226–238). Routledge.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Modellizzazione SEM in Piccoli Campioni</span>"
    ]
  },
  {
    "objectID": "chapters/sem/13_esem.html",
    "href": "chapters/sem/13_esem.html",
    "title": "59  Exploratory structural equation modelling",
    "section": "",
    "text": "59.1 Introduzione\nL’Exploratory Structural Equation Modeling (ESEM) è una tecnica statistica innovativa che combina i punti di forza dell’analisi fattoriale esplorativa (EFA) e dell’analisi fattoriale confermativa (CFA). Introdotta da Asparouhov e Muthén nel 2009 e successivamente sviluppata da Marsh et al. (2009, 2014), l’ESEM consente di modellare strutture fattoriali complesse mantenendo una flessibilità analitica che supera i limiti tradizionali della CFA. Questa tecnica si rivela particolarmente vantaggiosa in presenza di item con fonti di varianza multiple, come sottolineato da Morin et al. (2013), grazie alla sua capacità di migliorare l’adattamento del modello, ridurre le correlazioni spurie tra fattori e rappresentare in modo realistico le saturazioni fattoriali incrociate.\nL’efficacia dell’ESEM è stata dimostrata in numerosi ambiti della psicologia, tra cui la psicologia clinica, educativa, industriale e della salute, dove spesso supera il CFA in termini di adattamento e interpretabilità del modello. Tuttavia, in contesti specifici, potrebbe essere necessario introdurre restrizioni al modello ESEM completamente libero. Questo ha portato all’evoluzione del set-ESEM (Marsh et al., 2020), una tecnica che integra in modo strategico elementi di ESEM e CFA in un quadro analitico unificato.\nIn questo capitolo, esploreremo i fondamenti e le applicazioni dell’Exploratory Structural Equation Modeling (ESEM), seguendo il tutorial proposto da Marsh & Alamer (2024).",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>Exploratory structural equation modelling</span>"
    ]
  },
  {
    "objectID": "chapters/sem/13_esem.html#efa-cfa-esem-e-set-esem",
    "href": "chapters/sem/13_esem.html#efa-cfa-esem-e-set-esem",
    "title": "59  Exploratory structural equation modelling",
    "section": "\n59.2 EFA, CFA, ESEM e Set-ESEM",
    "text": "59.2 EFA, CFA, ESEM e Set-ESEM\nL’analisi fattoriale esplorativa (EFA) e l’analisi fattoriale confermativa (CFA) rappresentano i due approcci principali per indagare le strutture latenti nei dati. L’EFA, introdotta da Spearman (1904) e sviluppata ulteriormente da Thurstone (1935, 1947), era inizialmente conosciuta semplicemente come “analisi fattoriale.” Solo con l’introduzione della CFA si è stabilita una distinzione tra l’approccio esplorativo (EFA) e quello confermativo (CFA). La CFA è diventata uno strumento centrale nella psicometria grazie alla sua capacità di valutare l’adattamento del modello, gestire dati mancanti con metodi avanzati e confrontare modelli teorici alternativi. Tuttavia, uno dei suoi principali limiti è l’ipotesi rigida che ogni item carichi su un solo fattore, ignorando potenziali carichi incrociati.\nQuesto limite ha portato allo sviluppo dell’Exploratory Structural Equation Modeling (ESEM) da parte di Asparouhov e Muthén (2009). L’ESEM combina la flessibilità dell’EFA con la potenza analitica della SEM, consentendo carichi incrociati tra i fattori e offrendo un migliore adattamento ai dati psicometrici complessi. Questo approccio si è dimostrato superiore alla CFA in numerosi studi, migliorando l’adattamento del modello e sostenendo la validità discriminante tra i fattori. Una meta-analisi recente condotta da Gegenfurtner (2022) su 158 studi ha confermato che l’ESEM supera la CFA sia per bontà di adattamento sia per validità discriminante.\nL’ESEM è ormai uno strumento consolidato nella psicometria moderna, dimostrando la sua efficacia nel modellare strutture fattoriali complesse. Grazie alla sua capacità di rappresentare accuratamente le correlazioni e le regressioni tra fattori e di utilizzare tutte le informazioni disponibili a livello degli indicatori, si è affermato come un’alternativa valida e spesso preferibile alla CFA.\nTuttavia, l’ESEM, pur essendo più flessibile, può risultare meno parsimonioso rispetto alla CFA in alcune situazioni. Per questo motivo è stato sviluppato il set-ESEM (Marsh et al., 2020), un’evoluzione che bilancia la flessibilità dell’ESEM con una struttura più rigorosa, tipica della CFA. Il set-ESEM utilizza tecniche di rotazione come la geomin rotation o il target rotation per limitare i carichi incrociati non essenziali, rendendo il modello più parsimonioso e adatto a specifiche esigenze empiriche.\n\n\n\n\n\nFigura 59.1: CFA, ESEM completo e set-ESEM. Nota: le linee tratteggiate indicano i carichi incrociati non target. [Figura tratta da Marsh et al., 2024]",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>Exploratory structural equation modelling</span>"
    ]
  },
  {
    "objectID": "chapters/sem/13_esem.html#situazioni-in-cui-il-set-esem-è-preferibile-allesem-completo",
    "href": "chapters/sem/13_esem.html#situazioni-in-cui-il-set-esem-è-preferibile-allesem-completo",
    "title": "59  Exploratory structural equation modelling",
    "section": "\n59.3 Situazioni in cui il Set-ESEM è Preferibile all’ESEM Completo",
    "text": "59.3 Situazioni in cui il Set-ESEM è Preferibile all’ESEM Completo\nIn alcune situazioni, l’ESEM completo potrebbe non essere l’approccio ottimale per un’analisi. Questo accade quando è necessario garantire che specifici insiemi di fattori e item siano distinti da altri insiemi non correlati. Per affrontare tali scenari, si può utilizzare il set-ESEM, un approccio introdotto da Marsh et al. (2020) che permette di creare sottoinsiemi di ESEM all’interno di un modello più ampio. Il set-ESEM bilancia la flessibilità dell’ESEM completo con la struttura più rigorosa della CFA, offrendo un compromesso ideale in termini di adattamento del modello, parsimonia e definizione chiara dei modelli di misurazione. Di seguito, descriviamo due situazioni reali in cui il set-ESEM può risultare preferibile, basandoci su dati empirici.\n\n59.3.1 1. Item relativi a costrutti teoricamente distinti\nLa prima situazione riguarda dataset che includono item derivati da costrutti concettualmente distinti o appartenenti a teorie differenti. Ad esempio, consideriamo un dataset che misura le tre necessità psicologiche di base—autonomia, competenza e relazionalità—utilizzando la scala BPN-L2 (Alamer, 2022), insieme a due costrutti di perseveranza nello sforzo e coerenza dell’interesse, derivati dalla teoria del grit (Duckworth et al., 2007) e misurati con la scala L2-grit (Alamer, 2021b). Poiché le necessità psicologiche di base e il grit si fondano su teorie con obiettivi e funzioni differenti, stimare carichi incrociati tra i loro item risulterebbe inappropriato. Per esempio, i fattori delle necessità psicologiche sono influenzati dal contesto sociale, mentre il grit è considerato un tratto stabile della personalità.\nIn queste circostanze, il set-ESEM consente di suddividere il modello in due blocchi: uno dedicato ai tre fattori delle necessità psicologiche, con carichi incrociati tra loro ma non con gli item del grit, e un secondo blocco per i due fattori del grit, senza carichi incrociati con le necessità psicologiche. Questo approccio mantiene una maggiore parsimonia, preservando sia la coerenza teorica sia l’accuratezza empirica.\n\n59.3.2 2. Costrutti rilevanti misurati in più momenti temporali\nIl secondo scenario in cui il set-ESEM è consigliato riguarda analisi longitudinali, in cui i dati provengono da costrutti misurati in più momenti temporali. In questi casi, i carichi incrociati dovrebbero essere stimati solo tra item relativi allo stesso momento temporale. Ad esempio, consideriamo un dataset che misura passione armoniosa, passione ossessiva e autonomia in due momenti distinti. Questi costrutti sono correlati concettualmente, rendendo ragionevoli i carichi incrociati all’interno dello stesso momento. Tuttavia, permettere carichi incrociati tra item di momenti diversi sarebbe teoricamente inappropriato e tecnicamente problematico, introducendo effetti di confondimento.\nInoltre, nelle analisi longitudinali SEM è consuetudine correlare i residui degli stessi item nel tempo (Marsh & Hau, 1996). Utilizzando il set-ESEM, è possibile preservare la flessibilità analitica dell’ESEM mantenendo il rigore strutturale necessario per evitare interpretazioni distorte.\n\n\n\n\n\nFigura 59.2: CFA, ESEM completo e set-ESEM. Nota: le linee tratteggiate indicano i carichi incrociati non target. (Figura tratta da Marsh & Alamer, 2024)",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>Exploratory structural equation modelling</span>"
    ]
  },
  {
    "objectID": "chapters/sem/13_esem.html#necessità-psicologiche-di-base-e-percezione-del-sé",
    "href": "chapters/sem/13_esem.html#necessità-psicologiche-di-base-e-percezione-del-sé",
    "title": "59  Exploratory structural equation modelling",
    "section": "\n59.4 Necessità Psicologiche di Base e Percezione del Sé",
    "text": "59.4 Necessità Psicologiche di Base e Percezione del Sé\nPer illustrare il primo scenario descritto in precedenza, esaminiamo uno studio condotto su 269 studenti sauditi che imparano l’inglese come seconda lingua (L2) in un’università pubblica saudita (Marsh & Alamer, 2024). I partecipanti, di età compresa tra 18 e 20 anni (M = 18.5), parlavano arabo come lingua madre e hanno completato un questionario online. Lo studio utilizza il set-ESEM per analizzare i dati, dimostrando come questo approccio possa superare sia l’ESEM completo che la CFA in termini di adattamento del modello e precisione nelle stime.\n\n59.4.1 Struttura del Modello\nLo studio si basa su due blocchi teorici di costrutti:\n\nNecessità Psicologiche di Base (BPN):\nQuesto blocco include autonomia, competenza e relazionalità, tre fattori derivati dalla teoria delle necessità psicologiche di base (Ryan & Deci, 2017; Noels, 2023). Gli item valutano la percezione degli studenti sull’insegnante come promotore di questi tre fattori.\nPercezione del Sé:\nQuesto blocco comprende senso di significato, senso di sicurezza e motivazione intrinseca, costrutti associati ai risultati positivi delle BPN. La letteratura suggerisce che quando gli studenti percepiscono l’insegnante come un promotore delle BPN, si osserva un aumento della motivazione intrinseca, del senso di significato e della sicurezza (Alamer, 2022; Alamer & Al Khateeb, 2023; Guay et al., 2015).\n\n59.4.2 Distinzione Concettuale\nAi partecipanti è stato chiesto di valutare sia la percezione dell’insegnante come promotore delle BPN, sia la loro percezione personale in termini di senso di significato, sicurezza e motivazione intrinseca. Questa distinzione concettuale giustifica l’assenza di carichi incrociati tra i due blocchi di item:\n\nGli item relativi alle BPN si concentrano sull’insegnante e riflettono l’interazione sociale.\n\nGli item relativi alla percezione di sé misurano costrutti soggettivi e individuali.\n\nConsentire carichi incrociati tra questi due domini sarebbe teoricamente ingiustificato, data la loro natura distinta.\n\n59.4.3 Variabile di Esito\nLo studio include anche l’intenzione di abbandonare il corso come variabile di esito. Questo costrutto rappresenta un indicatore pratico delle implicazioni educative delle percezioni relative all’insegnante e al sé.\n\n59.4.4 Analisi dei Modelli\nI modelli alternativi analizzati nello studio—uno basato su CFA e l’altro su set-ESEM—sono illustrati nella Figura 59.3. L’analisi dimostra che il set-ESEM bilancia efficacemente rigore teorico e flessibilità empirica, fornendo stime più affidabili delle relazioni tra variabili latenti e risultati migliori rispetto agli approcci tradizionali.\n\n\n\n\n\nFigura 59.3: Set-ESEM (modello A) e CFA (modello B). Nota: le linee tratteggiate indicano i carichi incrociati non target. (Figura tratta da Marsh & Alamer, 2024)\n\n\nStrumenti di Misura\nPer valutare i costrutti oggetto di studio, sono state utilizzate diverse scale validate, ognuna composta da specifici item rappresentativi.\n\n\nBPN-L2 (Alamer, 2022):\nQuesta scala misura le tre necessità psicologiche di base — autonomia, competenza e relazionalità — ciascuna con tre item. Esempi:\n\n\nAutonomia: “Il mio insegnante ci permette di scegliere i compiti di apprendimento linguistico” (ω = .75).\n\n\nCompetenza: “Il mio insegnante ci dice che siamo capaci di imparare l’inglese” (ω = .75).\n\n\nRelazionalità: “Il mio insegnante di inglese è amichevole e cordiale con noi” (ω = .91).\n\n\n\nMotivazione intrinseca (SDT-L2; Alamer, 2022):\nQuesto costrutto è stato misurato tramite tre item, come:\n\n\n“Imparo l’inglese perché mi piace” (ω = .91).\n\n\n\nSenso di sicurezza e senso di significato (Dörnyei & Ushioda, 2021; Dörnyei & Ryan, 2015):\n\n\nSicurezza: Tre item, ad esempio: “Credo nelle mie capacità di fare bene nel corso” (ω = .74).\n\n\nSignificato: Tre item, come: “So perché mi sono iscritto a questo corso” (ω = .91).\n\n\n\nIntenzione di abbandonare il corso (Lounsbury et al., 2004):\nQuesto costrutto è stato misurato con cinque item, ad esempio:\n\n\n“Non ho intenzione di continuare a studiare in questo settore” (ω = .90).\nTutte le misure adottano una scala Likert a cinque punti, con risposte che vanno da 1 (fortemente in disaccordo) a 5 (fortemente d’accordo), per valutare l’accordo o il disaccordo dei partecipanti con ciascun item.\n\n\n\nDati\nImportiamo i dati e esaminiamo le variabili.\n\nstudy1_dat &lt;- rio::import(\n    here::here(\n        \"data\", \"marsh_alamer\", \"Study_1_data.csv\"\n    )\n)\n\nglimpse(study1_dat)\n#&gt; Rows: 269\n#&gt; Columns: 23\n#&gt; $ Intent_to_withdraw1 &lt;int&gt; 2, 2, 4, 4, 5, 1, 4, 2, 1, 5, 2, 4, 5, 5, 3, 5…\n#&gt; $ Intent_to_withdraw2 &lt;int&gt; 2, 3, 4, 5, 5, 1, 3, 4, 1, 5, 2, 5, 5, 5, 4, 5…\n#&gt; $ Intent_to_withdraw3 &lt;int&gt; 1, 2, 1, 4, 5, 1, 3, 1, 1, 5, 2, 3, 4, 4, 3, 4…\n#&gt; $ Intent_to_withdraw4 &lt;int&gt; 2, 2, 3, 5, 5, 1, 3, 2, 2, 4, 2, 4, 4, 4, 5, 4…\n#&gt; $ Intent_to_withdraw5 &lt;int&gt; 3, 3, 4, 4, 4, 1, 4, 2, 1, 5, 2, 4, 4, 4, 4, 4…\n#&gt; $ T_relatedness1      &lt;int&gt; 4, 4, 2, 2, 2, 5, 1, 4, 4, 4, 4, 4, 1, 1, 1, 1…\n#&gt; $ T_relatedness2      &lt;int&gt; 3, 4, 2, 2, 1, 5, 1, 4, 4, 2, 4, 3, 1, 1, 1, 1…\n#&gt; $ T_relatedness3      &lt;int&gt; 3, 5, 2, 2, 2, 5, 1, 4, 4, 4, 4, 4, 1, 1, 3, 1…\n#&gt; $ T_competence1       &lt;int&gt; 4, 5, 2, 4, 3, 5, 5, 4, 2, 4, 3, 4, 2, 2, 3, 2…\n#&gt; $ T_competence2       &lt;int&gt; 4, 4, 2, 3, 3, 5, 5, 5, 2, 4, 4, 4, 2, 2, 4, 2…\n#&gt; $ T_competence3       &lt;int&gt; 4, 5, 1, 4, 3, 5, 4, 4, 2, 4, 3, 3, 2, 2, 4, 2…\n#&gt; $ T_autonomy1         &lt;int&gt; 4, 5, 2, 2, 4, 5, 3, 4, 4, 4, 4, 4, 2, 2, 2, 2…\n#&gt; $ T_autonomy2         &lt;int&gt; 3, 5, 2, 3, 3, 5, 1, 4, 4, 4, 4, 3, 3, 3, 2, 3…\n#&gt; $ T_autonomy3         &lt;int&gt; 3, 5, 2, 2, 3, 5, 1, 4, 4, 3, 3, 3, 1, 1, 4, 1…\n#&gt; $ S_meaning1          &lt;int&gt; 4, 5, 4, 1, 2, 5, 3, 4, 4, 3, 5, 2, 5, 5, 2, 5…\n#&gt; $ S_meaning2          &lt;int&gt; 3, 5, 4, 2, 2, 5, 1, 4, 4, 4, 5, 2, 5, 5, 1, 5…\n#&gt; $ S_meaning3          &lt;int&gt; 5, 4, 4, 2, 2, 5, 1, 4, 4, 4, 5, 2, 5, 5, 2, 5…\n#&gt; $ S_confidence1       &lt;int&gt; 4, 5, 5, 4, 4, 5, 5, 4, 2, 5, 5, 3, 5, 5, 4, 5…\n#&gt; $ S_confidence2       &lt;int&gt; 4, 5, 5, 4, 4, 5, 5, 4, 2, 5, 5, 4, 5, 5, 4, 5…\n#&gt; $ S_confidence3       &lt;int&gt; 4, 5, 5, 5, 5, 5, 5, 4, 1, 5, 5, 3, 5, 5, 4, 5…\n#&gt; $ S_Intrinsic1        &lt;int&gt; 4, 4, 2, 2, 2, 5, 3, 4, 2, 5, 3, 3, 2, 2, 1, 2…\n#&gt; $ S_Intrinsic2        &lt;int&gt; 4, 5, 2, 3, 3, 5, 4, 4, 2, 5, 5, 4, 1, 1, 2, 1…\n#&gt; $ S_Intrinsic3        &lt;int&gt; 4, 5, 1, 2, 3, 5, 4, 4, 2, 5, 4, 4, 1, 1, 2, 1…\n\nCodice lavaan per il modello ESEM\nDefiniamo il modello ESEM.\n\nesem1 &lt;- '\n\n  # the long format (more flexible) each factor is defined separately\n  efa(\"teacher\")*Teacher_autonomy =~ T_autonomy1 + T_autonomy2 + T_autonomy3 + T_competence1 + T_competence2 + T_competence3 + T_relatedness1 + T_relatedness2 + T_relatedness3\n  efa(\"teacher\")*Teacher_competence =~ T_autonomy1 + T_autonomy2 + T_autonomy3 + T_competence1 + T_competence2 + T_competence3 + T_relatedness1 + T_relatedness2 + T_relatedness3\n  efa(\"teacher\")*Teacher_relatedness =~ T_autonomy1 + T_autonomy2 + T_autonomy3 + T_competence1 + T_competence2 + T_competence3 + T_relatedness1 + T_relatedness2 + T_relatedness3\n\n  # the short format (less flexible) all factors defined in one instance (remove ”##” if you want to use this)\n  # efa(\"teacher\")*Teacher_autonomy +\n  # efa(\"teacher\")*Teacher_competence +\n  # efa(\"teacher\")*Teacher_relatedness =~ T_autonomy1 + T_autonomy2 + T_autonomy3 + T_competence1 + T_competence2 + T_competence3 + T_relatedness1 + T_relatedness2 + T_relatedness3\n\n  # defining the second ESEM block\n  efa(\"self\")*Self_Meaning =~ S_meaning1 + S_meaning2 + S_meaning3 + S_confidence1 + S_confidence2 + S_confidence3 + S_Intrinsic1 + S_Intrinsic2+S_Intrinsic3\n  efa(\"self\")*Self_Confidence =~ S_meaning1 + S_meaning2 + S_meaning3 + S_confidence1 + S_confidence2 + S_confidence3 + S_Intrinsic1 + S_Intrinsic2+S_Intrinsic3\n  efa(\"self\")*Intrinsic_Motivation =~ S_meaning1 + S_meaning2 + S_meaning3 + S_confidence1 + S_confidence2 + S_confidence3 + S_Intrinsic1 + S_Intrinsic2+S_Intrinsic3\n\n  # defining the outcome variable\n  Intent_to_Quit =~ Intent_to_withdraw1 + Intent_to_withdraw2 + Intent_to_withdraw3 + Intent_to_withdraw4 + Intent_to_withdraw5\n\n  # defining the structural part\n  Self_Meaning ~ Teacher_autonomy + Teacher_competence + Teacher_relatedness\n  Self_Confidence ~ Teacher_autonomy + Teacher_competence + Teacher_relatedness\n  Intrinsic_Motivation ~ Teacher_autonomy + Teacher_competence + Teacher_relatedness\n  Intent_to_Quit ~ Self_Meaning + Self_Confidence + Intrinsic_Motivation +\n                   Teacher_autonomy + Teacher_competence + Teacher_relatedness\n'\n\nAdattiamo il modello ai dati.\n\nout1 &lt;- sem(\n    model = esem1,\n    data = study1_dat,\n    estimator = \"MLR\", # verbose = TRUE, test = \"yuan.bentler\",\n    rotation = \"geomin\",\n    rotation.args = list(geomin.epsilon = 0.005)\n)\n\nCreiamo il diagramma di percorso.\n\nsemPlot::semPaths(\n    out1,\n    what = \"col\", whatLabels = \"no\", style = \"mx\",\n    layout = \"tree\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 5, sizeMan2 = 4\n)\n\n\n\n\n\n\n\nEsaminiamo la soluzione fattoriale.\n\nsummary(out1, standardized = TRUE, fit.measures = TRUE) \n#&gt; lavaan 0.6-19 ended normally after 67 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                       103\n#&gt;   Row rank of the constraints matrix                12\n#&gt; \n#&gt;   Rotation method                       GEOMIN OBLIQUE\n#&gt;   Geomin epsilon                                 0.005\n#&gt;   Rotation algorithm (rstarts)                GPA (30)\n#&gt;   Standardized metric                             TRUE\n#&gt;   Row weights                                     None\n#&gt; \n#&gt;   Number of observations                           269\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                               Standard      Scaled\n#&gt;   Test Statistic                               396.932     367.121\n#&gt;   Degrees of freedom                               185         185\n#&gt;   P-value (Chi-square)                           0.000       0.000\n#&gt;   Scaling correction factor                                  1.081\n#&gt;     Yuan-Bentler correction (Mplus variant)                       \n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                              4271.294    3518.139\n#&gt;   Degrees of freedom                               253         253\n#&gt;   P-value                                        0.000       0.000\n#&gt;   Scaling correction factor                                  1.214\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.947       0.944\n#&gt;   Tucker-Lewis Index (TLI)                       0.928       0.924\n#&gt;                                                                   \n#&gt;   Robust Comparative Fit Index (CFI)                         0.950\n#&gt;   Robust Tucker-Lewis Index (TLI)                            0.932\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -7894.627   -7894.627\n#&gt;   Scaling correction factor                                  1.378\n#&gt;       for the MLR correction                                      \n#&gt;   Loglikelihood unrestricted model (H1)      -7696.161   -7696.161\n#&gt;   Scaling correction factor                                  1.179\n#&gt;       for the MLR correction                                      \n#&gt;                                                                   \n#&gt;   Akaike (AIC)                               15971.254   15971.254\n#&gt;   Bayesian (BIC)                             16298.373   16298.373\n#&gt;   Sample-size adjusted Bayesian (SABIC)      16009.844   16009.844\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.065       0.060\n#&gt;   90 Percent confidence interval - lower         0.056       0.052\n#&gt;   90 Percent confidence interval - upper         0.074       0.069\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.003       0.025\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.003       0.000\n#&gt;                                                                   \n#&gt;   Robust RMSEA                                               0.063\n#&gt;   90 Percent confidence interval - lower                     0.053\n#&gt;   90 Percent confidence interval - upper                     0.072\n#&gt;   P-value H_0: Robust RMSEA &lt;= 0.050                         0.013\n#&gt;   P-value H_0: Robust RMSEA &gt;= 0.080                         0.001\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.045       0.045\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Sandwich\n#&gt;   Information bread                           Observed\n#&gt;   Observed information based on                Hessian\n#&gt; \n#&gt; Latent Variables:\n#&gt;                                  Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;   Teacher_autonomy =~ teacher                                       \n#&gt;     T_autonomy1                     0.787    0.128    6.159    0.000\n#&gt;     T_autonomy2                     1.156    0.077   14.918    0.000\n#&gt;     T_autonomy3                     0.971    0.104    9.323    0.000\n#&gt;     T_competence1                  -0.058    0.072   -0.806    0.420\n#&gt;     T_competence2                   0.288    0.164    1.753    0.080\n#&gt;     T_competence3                   0.123    0.219    0.564    0.573\n#&gt;     T_relatedness1                  0.423    0.149    2.845    0.004\n#&gt;     T_relatedness2                 -0.011    0.029   -0.391    0.696\n#&gt;     T_relatedness3                  0.332    0.246    1.348    0.178\n#&gt;   Teacher_competence =~ teacher                                     \n#&gt;     T_autonomy1                     0.261    0.119    2.184    0.029\n#&gt;     T_autonomy2                    -0.035    0.039   -0.901    0.368\n#&gt;     T_autonomy3                     0.087    0.085    1.022    0.307\n#&gt;     T_competence1                   1.220    0.073   16.735    0.000\n#&gt;     T_competence2                   0.943    0.147    6.433    0.000\n#&gt;     T_competence3                   0.622    0.169    3.678    0.000\n#&gt;     T_relatedness1                 -0.024    0.020   -1.247    0.212\n#&gt;     T_relatedness2                  0.049    0.053    0.919    0.358\n#&gt;     T_relatedness3                  0.061    0.151    0.404    0.686\n#&gt;   Teacher_relatedness =~ teacher                                    \n#&gt;     T_autonomy1                     0.042    0.061    0.684    0.494\n#&gt;     T_autonomy2                    -0.048    0.066   -0.729    0.466\n#&gt;     T_autonomy3                     0.078    0.104    0.752    0.452\n#&gt;     T_competence1                   0.029    0.054    0.536    0.592\n#&gt;     T_competence2                  -0.041    0.040   -1.043    0.297\n#&gt;     T_competence3                   0.179    0.111    1.608    0.108\n#&gt;     T_relatedness1                  0.832    0.156    5.346    0.000\n#&gt;     T_relatedness2                  1.129    0.086   13.160    0.000\n#&gt;     T_relatedness3                  0.316    0.198    1.593    0.111\n#&gt;   Self_Meaning =~ self                                              \n#&gt;     S_meaning1                      0.808    0.065   12.388    0.000\n#&gt;     S_meaning2                      1.065    0.060   17.818    0.000\n#&gt;     S_meaning3                      1.040    0.056   18.600    0.000\n#&gt;     S_confidence1                  -0.028    0.040   -0.697    0.486\n#&gt;     S_confidence2                   0.098    0.038    2.599    0.009\n#&gt;     S_confidence3                  -0.016    0.017   -0.900    0.368\n#&gt;     S_Intrinsic1                   -0.008    0.053   -0.157    0.875\n#&gt;     S_Intrinsic2                   -0.002    0.044   -0.057    0.955\n#&gt;     S_Intrinsic3                    0.009    0.037    0.231    0.818\n#&gt;   Self_Confidence =~ self                                           \n#&gt;     S_meaning1                      0.052    0.060    0.875    0.382\n#&gt;     S_meaning2                     -0.027    0.027   -0.996    0.319\n#&gt;     S_meaning3                     -0.002    0.026   -0.069    0.945\n#&gt;     S_confidence1                   0.609    0.074    8.255    0.000\n#&gt;     S_confidence2                   0.560    0.059    9.441    0.000\n#&gt;     S_confidence3                   0.553    0.065    8.445    0.000\n#&gt;     S_Intrinsic1                   -0.027    0.073   -0.374    0.708\n#&gt;     S_Intrinsic2                    0.107    0.062    1.737    0.082\n#&gt;     S_Intrinsic3                   -0.011    0.031   -0.354    0.723\n#&gt;   Intrinsic_Motivation =~ self                                      \n#&gt;     S_meaning1                      0.043    0.030    1.461    0.144\n#&gt;     S_meaning2                     -0.011    0.016   -0.665    0.506\n#&gt;     S_meaning3                     -0.014    0.016   -0.915    0.360\n#&gt;     S_confidence1                  -0.026    0.029   -0.905    0.366\n#&gt;     S_confidence2                  -0.004    0.010   -0.396    0.692\n#&gt;     S_confidence3                   0.028    0.024    1.194    0.232\n#&gt;     S_Intrinsic1                    0.449    0.047    9.469    0.000\n#&gt;     S_Intrinsic2                    0.498    0.076    6.581    0.000\n#&gt;     S_Intrinsic3                    0.634    0.078    8.120    0.000\n#&gt;   Intent_to_Quit =~                                                 \n#&gt;     Intnt_t_wthdr1                  1.000                           \n#&gt;     Intnt_t_wthdr2                  0.946    0.033   28.953    0.000\n#&gt;     Intnt_t_wthdr3                  1.017    0.031   32.987    0.000\n#&gt;     Intnt_t_wthdr4                  0.683    0.074    9.170    0.000\n#&gt;     Intnt_t_wthdr5                  0.665    0.053   12.648    0.000\n#&gt;    Std.lv  Std.all\n#&gt;                   \n#&gt;     0.787    0.612\n#&gt;     1.156    0.922\n#&gt;     0.971    0.774\n#&gt;    -0.058   -0.046\n#&gt;     0.288    0.227\n#&gt;     0.123    0.102\n#&gt;     0.423    0.324\n#&gt;    -0.011   -0.009\n#&gt;     0.332    0.250\n#&gt;                   \n#&gt;     0.261    0.203\n#&gt;    -0.035   -0.028\n#&gt;     0.087    0.069\n#&gt;     1.220    0.972\n#&gt;     0.943    0.744\n#&gt;     0.622    0.516\n#&gt;    -0.024   -0.019\n#&gt;     0.049    0.039\n#&gt;     0.061    0.046\n#&gt;                   \n#&gt;     0.042    0.033\n#&gt;    -0.048   -0.038\n#&gt;     0.078    0.062\n#&gt;     0.029    0.023\n#&gt;    -0.041   -0.033\n#&gt;     0.179    0.149\n#&gt;     0.832    0.636\n#&gt;     1.129    0.900\n#&gt;     0.316    0.238\n#&gt;                   \n#&gt;     0.890    0.738\n#&gt;     1.173    0.919\n#&gt;     1.146    0.927\n#&gt;    -0.031   -0.031\n#&gt;     0.107    0.140\n#&gt;    -0.017   -0.023\n#&gt;    -0.009   -0.008\n#&gt;    -0.003   -0.002\n#&gt;     0.009    0.007\n#&gt;                   \n#&gt;     0.055    0.045\n#&gt;    -0.028   -0.022\n#&gt;    -0.002   -0.002\n#&gt;     0.637    0.639\n#&gt;     0.586    0.763\n#&gt;     0.578    0.766\n#&gt;    -0.028   -0.024\n#&gt;     0.112    0.093\n#&gt;    -0.012   -0.009\n#&gt;                   \n#&gt;     0.070    0.058\n#&gt;    -0.018   -0.014\n#&gt;    -0.023   -0.019\n#&gt;    -0.043   -0.043\n#&gt;    -0.006   -0.008\n#&gt;     0.046    0.061\n#&gt;     0.725    0.619\n#&gt;     0.805    0.669\n#&gt;     1.025    0.815\n#&gt;                   \n#&gt;     1.170    0.955\n#&gt;     1.107    0.911\n#&gt;     1.190    0.875\n#&gt;     0.799    0.637\n#&gt;     0.778    0.648\n#&gt; \n#&gt; Regressions:\n#&gt;                           Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n#&gt;   Self_Meaning ~                                                      \n#&gt;     Teacher_autnmy          -0.040    0.139   -0.287    0.774   -0.036\n#&gt;     Teacher_cmptnc           0.271    0.125    2.172    0.030    0.246\n#&gt;     Teachr_rltdnss           0.287    0.120    2.387    0.017    0.260\n#&gt;   Self_Confidence ~                                                   \n#&gt;     Teacher_autnmy          -0.096    0.152   -0.630    0.529   -0.092\n#&gt;     Teacher_cmptnc           0.299    0.146    2.048    0.041    0.286\n#&gt;     Teachr_rltdnss          -0.306    0.159   -1.925    0.054   -0.293\n#&gt;   Intrinsic_Motivation ~                                              \n#&gt;     Teacher_autnmy           0.862    0.333    2.588    0.010    0.533\n#&gt;     Teacher_cmptnc           0.217    0.198    1.096    0.273    0.134\n#&gt;     Teachr_rltdnss           0.339    0.227    1.494    0.135    0.210\n#&gt;   Intent_to_Quit ~                                                    \n#&gt;     Self_Meaning            -0.123    0.078   -1.580    0.114   -0.115\n#&gt;     Self_Confidenc           0.056    0.072    0.774    0.439    0.050\n#&gt;     Intrinsc_Mtvtn           0.169    0.096    1.764    0.078    0.234\n#&gt;     Teacher_autnmy          -0.790    0.211   -3.752    0.000   -0.676\n#&gt;     Teacher_cmptnc           0.078    0.115    0.676    0.499    0.066\n#&gt;     Teachr_rltdnss          -0.208    0.156   -1.337    0.181   -0.178\n#&gt;   Std.all\n#&gt;          \n#&gt;    -0.036\n#&gt;     0.246\n#&gt;     0.260\n#&gt;          \n#&gt;    -0.092\n#&gt;     0.286\n#&gt;    -0.293\n#&gt;          \n#&gt;     0.533\n#&gt;     0.134\n#&gt;     0.210\n#&gt;          \n#&gt;    -0.115\n#&gt;     0.050\n#&gt;     0.234\n#&gt;    -0.676\n#&gt;     0.066\n#&gt;    -0.178\n#&gt; \n#&gt; Covariances:\n#&gt;                          Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n#&gt;   Teacher_autonomy ~~                                                \n#&gt;     Teacher_cmptnc          0.652    0.067    9.730    0.000    0.652\n#&gt;     Teachr_rltdnss          0.659    0.064   10.287    0.000    0.659\n#&gt;   Teacher_competence ~~                                              \n#&gt;     Teachr_rltdnss          0.545    0.084    6.495    0.000    0.545\n#&gt;  .Self_Meaning ~~                                                    \n#&gt;    .Self_Confidenc          0.195    0.083    2.346    0.019    0.195\n#&gt;    .Intrinsc_Mtvtn          0.071    0.105    0.678    0.498    0.071\n#&gt;  .Self_Confidence ~~                                                 \n#&gt;    .Intrinsc_Mtvtn          0.281    0.116    2.431    0.015    0.281\n#&gt;   Std.all\n#&gt;          \n#&gt;     0.652\n#&gt;     0.659\n#&gt;          \n#&gt;     0.545\n#&gt;          \n#&gt;     0.195\n#&gt;     0.071\n#&gt;          \n#&gt;     0.281\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .T_autonomy1       0.642    0.085    7.529    0.000    0.642    0.388\n#&gt;    .T_autonomy2       0.356    0.093    3.822    0.000    0.356    0.226\n#&gt;    .T_autonomy3       0.399    0.062    6.482    0.000    0.399    0.254\n#&gt;    .T_competence1     0.138    0.115    1.209    0.227    0.138    0.088\n#&gt;    .T_competence2     0.338    0.072    4.723    0.000    0.338    0.210\n#&gt;    .T_competence3     0.766    0.106    7.238    0.000    0.766    0.528\n#&gt;    .T_relatedness1    0.409    0.080    5.113    0.000    0.409    0.239\n#&gt;    .T_relatedness2    0.254    0.161    1.579    0.114    0.254    0.161\n#&gt;    .T_relatedness3    1.363    0.119   11.408    0.000    1.363    0.773\n#&gt;    .S_meaning1        0.600    0.084    7.099    0.000    0.600    0.412\n#&gt;    .S_meaning2        0.274    0.089    3.093    0.002    0.274    0.168\n#&gt;    .S_meaning3        0.231    0.052    4.420    0.000    0.231    0.151\n#&gt;    .S_confidence1     0.593    0.136    4.354    0.000    0.593    0.598\n#&gt;    .S_confidence2     0.218    0.053    4.138    0.000    0.218    0.370\n#&gt;    .S_confidence3     0.232    0.046    4.988    0.000    0.232    0.407\n#&gt;    .S_Intrinsic1      0.855    0.105    8.179    0.000    0.855    0.622\n#&gt;    .S_Intrinsic2      0.772    0.101    7.655    0.000    0.772    0.534\n#&gt;    .S_Intrinsic3      0.528    0.098    5.370    0.000    0.528    0.334\n#&gt;    .Intnt_t_wthdr1    0.133    0.031    4.277    0.000    0.133    0.088\n#&gt;    .Intnt_t_wthdr2    0.253    0.038    6.622    0.000    0.253    0.171\n#&gt;    .Intnt_t_wthdr3    0.432    0.057    7.549    0.000    0.432    0.234\n#&gt;    .Intnt_t_wthdr4    0.936    0.137    6.820    0.000    0.936    0.595\n#&gt;    .Intnt_t_wthdr5    0.836    0.104    8.004    0.000    0.836    0.580\n#&gt;     Teacher_autnmy    1.000                               1.000    1.000\n#&gt;     Teacher_cmptnc    1.000                               1.000    1.000\n#&gt;     Teachr_rltdnss    1.000                               1.000    1.000\n#&gt;    .Self_Meaning      1.000                               0.824    0.824\n#&gt;    .Self_Confidenc    1.000                               0.914    0.914\n#&gt;    .Intrinsc_Mtvtn    1.000                               0.383    0.383\n#&gt;    .Intent_to_Quit    0.786    0.093    8.477    0.000    0.574    0.574\n\nCodice lavaan per il modello CFA\nDefiniamo ora il modello CFA.\n\ncfa1 &lt;- ' ## Specify the measurement model\n\n  # \"teacher\" factors\n  Teacher_autonomy    =~    T_autonomy1 +    T_autonomy2 +    T_autonomy3\n  Teacher_competence  =~  T_competence1 +  T_competence2 +  T_competence3\n  Teacher_relatedness =~ T_relatedness1 + T_relatedness2 + T_relatedness3\n\n  # \"self\" factors\n  Self_Meaning         =~    S_meaning1 +    S_meaning2 +    S_meaning3\n  Self_Confidence      =~ S_confidence1 + S_confidence2 + S_confidence3\n  Intrinsic_Motivation =~  S_Intrinsic1 +  S_Intrinsic2 +  S_Intrinsic3\n\n  # defining the outcome variable\n  Intent_to_Quit =~ Intent_to_withdraw1 + Intent_to_withdraw2 + Intent_to_withdraw3 + Intent_to_withdraw4 + Intent_to_withdraw5\n\n  # specify the structural model\n  Self_Meaning ~ Teacher_autonomy + Teacher_competence + Teacher_relatedness\n  Self_Confidence ~ Teacher_autonomy + Teacher_competence + Teacher_relatedness\n  Intrinsic_Motivation ~ Teacher_autonomy + Teacher_competence + Teacher_relatedness\n  Intent_to_Quit ~ Self_Meaning + Self_Confidence + Intrinsic_Motivation +\n                   Teacher_autonomy + Teacher_competence + Teacher_relatedness\n\n  # residual covariances among mediating factors in Block 2 (\"self\")\n  # (not automatically estimated due to being predictors as well,\n  #  but ESEM rotation allows their covariances to be nonzero)\n  Self_Meaning    ~~ Self_Confidence + Intrinsic_Motivation\n  Self_Confidence ~~ Intrinsic_Motivation\n'\n\nAdattiamo il modello.\n\nfit1 &lt;- sem(\n    model = cfa1, data = study1_dat,\n    estimator = \"MLR\", std.lv = TRUE\n)\n\nCreiamo il diagramma di percorso.\n\nsemPlot::semPaths(\n    fit1,\n    what = \"col\", whatLabels = \"no\", style = \"mx\",\n    layout = \"tree\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 6, sizeMan2 = 4\n)\n\n\n\n\n\n\n\nEsaminiamo la soluzione fattoriale.\n\nsummary(fit1, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n#&gt; lavaan 0.6-19 ended normally after 53 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        67\n#&gt; \n#&gt;   Number of observations                           269\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                               Standard      Scaled\n#&gt;   Test Statistic                               459.107     419.040\n#&gt;   Degrees of freedom                               209         209\n#&gt;   P-value (Chi-square)                           0.000       0.000\n#&gt;   Scaling correction factor                                  1.096\n#&gt;     Yuan-Bentler correction (Mplus variant)                       \n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                              4271.294    3518.139\n#&gt;   Degrees of freedom                               253         253\n#&gt;   P-value                                        0.000       0.000\n#&gt;   Scaling correction factor                                  1.214\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.938       0.936\n#&gt;   Tucker-Lewis Index (TLI)                       0.925       0.922\n#&gt;                                                                   \n#&gt;   Robust Comparative Fit Index (CFI)                         0.942\n#&gt;   Robust Tucker-Lewis Index (TLI)                            0.930\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -7925.715   -7925.715\n#&gt;   Scaling correction factor                                  1.439\n#&gt;       for the MLR correction                                      \n#&gt;   Loglikelihood unrestricted model (H1)      -7696.161   -7696.161\n#&gt;   Scaling correction factor                                  1.179\n#&gt;       for the MLR correction                                      \n#&gt;                                                                   \n#&gt;   Akaike (AIC)                               15985.429   15985.429\n#&gt;   Bayesian (BIC)                             16226.275   16226.275\n#&gt;   Sample-size adjusted Bayesian (SABIC)      16013.842   16013.842\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.067       0.061\n#&gt;   90 Percent confidence interval - lower         0.058       0.053\n#&gt;   90 Percent confidence interval - upper         0.075       0.069\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.001       0.013\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.004       0.000\n#&gt;                                                                   \n#&gt;   Robust RMSEA                                               0.064\n#&gt;   90 Percent confidence interval - lower                     0.055\n#&gt;   90 Percent confidence interval - upper                     0.073\n#&gt;   P-value H_0: Robust RMSEA &lt;= 0.050                         0.006\n#&gt;   P-value H_0: Robust RMSEA &gt;= 0.080                         0.001\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.053       0.053\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Sandwich\n#&gt;   Information bread                           Observed\n#&gt;   Observed information based on                Hessian\n#&gt; \n#&gt; Latent Variables:\n#&gt;                           Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n#&gt;   Teacher_autonomy =~                                                 \n#&gt;     T_autonomy1              1.016    0.059   17.270    0.000    1.016\n#&gt;     T_autonomy2              1.066    0.050   21.119    0.000    1.066\n#&gt;     T_autonomy3              1.093    0.046   23.977    0.000    1.093\n#&gt;   Teacher_competence =~                                               \n#&gt;     T_competence1            1.129    0.053   21.180    0.000    1.129\n#&gt;     T_competence2            1.164    0.050   23.337    0.000    1.164\n#&gt;     T_competence3            0.819    0.074   11.027    0.000    0.819\n#&gt;   Teacher_relatedness =~                                              \n#&gt;     T_relatedness1           1.191    0.048   24.632    0.000    1.191\n#&gt;     T_relatedness2           1.053    0.052   20.124    0.000    1.053\n#&gt;     T_relatedness3           0.628    0.089    7.015    0.000    0.628\n#&gt;   Self_Meaning =~                                                     \n#&gt;     S_meaning1               0.839    0.061   13.822    0.000    0.920\n#&gt;     S_meaning2               1.061    0.060   17.603    0.000    1.163\n#&gt;     S_meaning3               1.039    0.058   17.821    0.000    1.139\n#&gt;   Self_Confidence =~                                                  \n#&gt;     S_confidence1            0.582    0.071    8.139    0.000    0.617\n#&gt;     S_confidence2            0.578    0.057   10.172    0.000    0.612\n#&gt;     S_confidence3            0.538    0.066    8.172    0.000    0.570\n#&gt;   Intrinsic_Motivation =~                                             \n#&gt;     S_Intrinsic1             0.443    0.050    8.881    0.000    0.719\n#&gt;     S_Intrinsic2             0.501    0.074    6.773    0.000    0.812\n#&gt;     S_Intrinsic3             0.634    0.069    9.250    0.000    1.028\n#&gt;   Intent_to_Quit =~                                                   \n#&gt;     Intnt_t_wthdr1           0.893    0.050   17.918    0.000    1.171\n#&gt;     Intnt_t_wthdr2           0.844    0.048   17.608    0.000    1.107\n#&gt;     Intnt_t_wthdr3           0.907    0.053   17.267    0.000    1.189\n#&gt;     Intnt_t_wthdr4           0.608    0.069    8.834    0.000    0.798\n#&gt;     Intnt_t_wthdr5           0.593    0.055   10.801    0.000    0.778\n#&gt;   Std.all\n#&gt;          \n#&gt;     0.790\n#&gt;     0.850\n#&gt;     0.871\n#&gt;          \n#&gt;     0.900\n#&gt;     0.918\n#&gt;     0.681\n#&gt;          \n#&gt;     0.911\n#&gt;     0.840\n#&gt;     0.473\n#&gt;          \n#&gt;     0.763\n#&gt;     0.911\n#&gt;     0.922\n#&gt;          \n#&gt;     0.619\n#&gt;     0.798\n#&gt;     0.756\n#&gt;          \n#&gt;     0.614\n#&gt;     0.675\n#&gt;     0.817\n#&gt;          \n#&gt;     0.956\n#&gt;     0.910\n#&gt;     0.875\n#&gt;     0.636\n#&gt;     0.648\n#&gt; \n#&gt; Regressions:\n#&gt;                          Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n#&gt;   Self_Meaning ~                                                     \n#&gt;     Teacher_autnmy         -0.170    0.192   -0.882    0.378   -0.155\n#&gt;     Teacher_cmptnc          0.335    0.160    2.096    0.036    0.305\n#&gt;     Teachr_rltdnss          0.308    0.157    1.963    0.050    0.281\n#&gt;   Self_Confidence ~                                                  \n#&gt;     Teacher_autnmy         -0.080    0.158   -0.511    0.610   -0.076\n#&gt;     Teacher_cmptnc          0.437    0.134    3.251    0.001    0.412\n#&gt;     Teachr_rltdnss         -0.403    0.160   -2.526    0.012   -0.380\n#&gt;   Intrinsic_Motivation ~                                             \n#&gt;     Teacher_autnmy          0.886    0.283    3.131    0.002    0.546\n#&gt;     Teacher_cmptnc          0.128    0.192    0.666    0.505    0.079\n#&gt;     Teachr_rltdnss          0.342    0.213    1.606    0.108    0.211\n#&gt;   Intent_to_Quit ~                                                   \n#&gt;     Self_Meaning           -0.151    0.087   -1.731    0.083   -0.126\n#&gt;     Self_Confidenc          0.051    0.085    0.596    0.551    0.041\n#&gt;     Intrinsc_Mtvtn          0.197    0.109    1.804    0.071    0.244\n#&gt;     Teacher_autnmy         -0.899    0.267   -3.368    0.001   -0.686\n#&gt;     Teacher_cmptnc          0.155    0.143    1.083    0.279    0.118\n#&gt;     Teachr_rltdnss         -0.259    0.191   -1.356    0.175   -0.197\n#&gt;   Std.all\n#&gt;          \n#&gt;    -0.155\n#&gt;     0.305\n#&gt;     0.281\n#&gt;          \n#&gt;    -0.076\n#&gt;     0.412\n#&gt;    -0.380\n#&gt;          \n#&gt;     0.546\n#&gt;     0.079\n#&gt;     0.211\n#&gt;          \n#&gt;    -0.126\n#&gt;     0.041\n#&gt;     0.244\n#&gt;    -0.686\n#&gt;     0.118\n#&gt;    -0.197\n#&gt; \n#&gt; Covariances:\n#&gt;                         Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n#&gt;  .Self_Meaning ~~                                                   \n#&gt;    .Self_Confidenc         0.241    0.081    2.970    0.003    0.241\n#&gt;    .Intrinsc_Mtvtn         0.093    0.106    0.876    0.381    0.093\n#&gt;  .Self_Confidence ~~                                                \n#&gt;    .Intrinsc_Mtvtn         0.334    0.119    2.801    0.005    0.334\n#&gt;   Teacher_autonomy ~~                                               \n#&gt;     Teacher_cmptnc         0.765    0.044   17.327    0.000    0.765\n#&gt;     Teachr_rltdnss         0.798    0.042   18.935    0.000    0.798\n#&gt;   Teacher_competence ~~                                             \n#&gt;     Teachr_rltdnss         0.662    0.049   13.510    0.000    0.662\n#&gt;   Std.all\n#&gt;          \n#&gt;     0.241\n#&gt;     0.093\n#&gt;          \n#&gt;     0.334\n#&gt;          \n#&gt;     0.765\n#&gt;     0.798\n#&gt;          \n#&gt;     0.662\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .T_autonomy1       0.623    0.086    7.243    0.000    0.623    0.376\n#&gt;    .T_autonomy2       0.435    0.063    6.871    0.000    0.435    0.277\n#&gt;    .T_autonomy3       0.380    0.062    6.153    0.000    0.380    0.241\n#&gt;    .T_competence1     0.301    0.063    4.791    0.000    0.301    0.191\n#&gt;    .T_competence2     0.254    0.058    4.380    0.000    0.254    0.158\n#&gt;    .T_competence3     0.778    0.116    6.726    0.000    0.778    0.537\n#&gt;    .T_relatedness1    0.291    0.077    3.752    0.000    0.291    0.170\n#&gt;    .T_relatedness2    0.464    0.066    7.012    0.000    0.464    0.295\n#&gt;    .T_relatedness3    1.369    0.121   11.283    0.000    1.369    0.777\n#&gt;    .S_meaning1        0.608    0.088    6.906    0.000    0.608    0.418\n#&gt;    .S_meaning2        0.278    0.089    3.115    0.002    0.278    0.171\n#&gt;    .S_meaning3        0.230    0.052    4.434    0.000    0.230    0.151\n#&gt;    .S_confidence1     0.612    0.139    4.413    0.000    0.612    0.616\n#&gt;    .S_confidence2     0.214    0.055    3.893    0.000    0.214    0.363\n#&gt;    .S_confidence3     0.244    0.049    4.982    0.000    0.244    0.429\n#&gt;    .S_Intrinsic1      0.857    0.105    8.146    0.000    0.857    0.624\n#&gt;    .S_Intrinsic2      0.786    0.108    7.292    0.000    0.786    0.544\n#&gt;    .S_Intrinsic3      0.528    0.090    5.836    0.000    0.528    0.333\n#&gt;    .Intnt_t_wthdr1    0.130    0.031    4.223    0.000    0.130    0.087\n#&gt;    .Intnt_t_wthdr2    0.254    0.038    6.685    0.000    0.254    0.172\n#&gt;    .Intnt_t_wthdr3    0.433    0.057    7.569    0.000    0.433    0.235\n#&gt;    .Intnt_t_wthdr4    0.937    0.137    6.830    0.000    0.937    0.595\n#&gt;    .Intnt_t_wthdr5    0.836    0.104    8.005    0.000    0.836    0.580\n#&gt;     Teacher_autnmy    1.000                               1.000    1.000\n#&gt;     Teacher_cmptnc    1.000                               1.000    1.000\n#&gt;     Teachr_rltdnss    1.000                               1.000    1.000\n#&gt;    .Self_Meaning      1.000                               0.832    0.832\n#&gt;    .Self_Confidenc    1.000                               0.889    0.889\n#&gt;    .Intrinsc_Mtvtn    1.000                               0.380    0.380\n#&gt;    .Intent_to_Quit    1.000                               0.582    0.582\n\nConfronto tra modelli\n\n# Extract model fit statistics from out1 and fit1\nfit_stats_out1 &lt;- fitMeasures(out1, c(\"chisq\", \"df\", \"rmsea\", \"rmsea.ci.lower\", \"rmsea.ci.upper\", \"cfi\", \"tli\"))\nfit_stats_fit1 &lt;- fitMeasures(fit1, c(\"chisq\", \"df\", \"rmsea\", \"rmsea.ci.lower\", \"rmsea.ci.upper\", \"cfi\", \"tli\"))\n\n# Create a tibble with the extracted fit statistics\nfit_table &lt;- tibble(\n    Model = c(\"CFA-based model\", \"Set-ESEM-based model\"),\n    chisq = c(fit_stats_fit1[\"chisq\"], fit_stats_out1[\"chisq\"]),\n    df = c(fit_stats_fit1[\"df\"], fit_stats_out1[\"df\"]),\n    RMSEA = c(fit_stats_fit1[\"rmsea\"], fit_stats_out1[\"rmsea\"]),\n    `RMSEA 90% CI` = c(\n        sprintf(\"(%.3f, %.3f)\", fit_stats_fit1[\"rmsea.ci.lower\"], fit_stats_fit1[\"rmsea.ci.upper\"]),\n        sprintf(\"(%.3f, %.3f)\", fit_stats_out1[\"rmsea.ci.lower\"], fit_stats_out1[\"rmsea.ci.upper\"])\n    ),\n    CFI = c(fit_stats_fit1[\"cfi\"], fit_stats_out1[\"cfi\"]),\n    TLI = c(fit_stats_fit1[\"tli\"], fit_stats_out1[\"tli\"])\n)\n\n# Convert numeric columns to formatted strings with three decimal places\nfit_table &lt;- fit_table %&gt;%\n    mutate(\n        across(where(is.numeric), ~ sprintf(\"%.3f\", .)),\n        chisq = sprintf(\"%.3f\", as.numeric(chisq)), # Ensure chisq is formatted correctly\n        df = as.character(df) # Convert df to character for consistent formatting\n    )\n\n# Calculate column widths for alignment\ncol_widths &lt;- fit_table %&gt;%\n    summarise(across(everything(), ~ max(nchar(.), na.rm = TRUE)))\n\n# Create text-based table output\nheader &lt;- paste(\n    str_pad(\"Model\", col_widths$Model, side = \"right\"),\n    str_pad(\"chisq\", col_widths$chisq, side = \"right\"),\n    str_pad(\"df\", col_widths$df, side = \"right\"),\n    str_pad(\"RMSEA\", col_widths$RMSEA, side = \"right\"),\n    str_pad(\"RMSEA 90% CI\", col_widths$`RMSEA 90% CI`, side = \"right\"),\n    str_pad(\"CFI\", col_widths$CFI, side = \"right\"),\n    str_pad(\"TLI\", col_widths$TLI, side = \"right\"),\n    sep = \" | \"\n)\nseparator &lt;- strrep(\"-\", nchar(header))\n\n# Print header and separator\ncat(header, \"\\n\")\n#&gt; Model                | chisq   | df      | RMSEA | RMSEA 90% CI   | CFI   | TLI\ncat(separator, \"\\n\")\n#&gt; ---------------------------------------------------------------------------------\n\n# Print each row formatted with aligned columns\nfit_table %&gt;%\n    mutate(\n        Model = str_pad(Model, col_widths$Model, side = \"right\"),\n        chisq = str_pad(chisq, col_widths$chisq, side = \"right\"),\n        df = str_pad(df, col_widths$df, side = \"right\"),\n        RMSEA = str_pad(RMSEA, col_widths$RMSEA, side = \"right\"),\n        `RMSEA 90% CI` = str_pad(`RMSEA 90% CI`, col_widths$`RMSEA 90% CI`, side = \"right\"),\n        CFI = str_pad(CFI, col_widths$CFI, side = \"right\"),\n        TLI = str_pad(TLI, col_widths$TLI, side = \"right\")\n    ) %&gt;%\n    rowwise() %&gt;%\n    mutate(row_text = paste(Model, chisq, df, RMSEA, `RMSEA 90% CI`, CFI, TLI, sep = \" | \")) %&gt;%\n    pull(row_text) %&gt;%\n    cat(sep = \"\\n\")\n#&gt; CFA-based model      | 459.107 | 209.000 | 0.067 | (0.058, 0.075) | 0.938 | 0.925\n#&gt; Set-ESEM-based model | 396.932 | 185.000 | 0.065 | (0.056, 0.074) | 0.947 | 0.928\n\nPer stimare i modelli, Marsh & Alamer (2024) utilizzano la versione robusta della massima verosimiglianza (MLR), che garantisce stime robuste rispetto a deviazioni dalla normalità multivariata (Yuan & Bentler, 2000). Per valutare la qualità dei modelli, sono presi in considerazione diversi indicatori di adattamento: il chi-quadro robusto (χ²) con i relativi gradi di libertà e valore p, il Comparative Fit Index (CFI), il Tucker-Lewis Index (TLI), e il Root Mean Square Error of Approximation (RMSEA) con il suo intervallo di confidenza al 90%. Coerentemente con l’approccio MLR, i valori di CFI, TLI e RMSEA riportati nei due esempi sono calcolati nella loro versione robusta.\nI risultati presentati nella tabella precedente indicano che sia il modello strutturale basato su CFA sia quello basato su set-ESEM mostrano un buon adattamento ai dati. Tuttavia, Marsh & Alamer (2024) si concentrano principalmente sulle differenze nelle relazioni strutturali tra i due modelli, tralasciando un’analisi dettagliata delle specifiche del modello di misura.\n\n59.4.5 Miglior adattamento del set-ESEM\nLa Tabella 2 dell’articolo di Marsh & Alamer (2024) riporta i coefficienti di percorso per entrambi i modelli. Sebbene entrambi mostrino un adattamento accettabile, il modello set-ESEM si distingue per un adattamento superiore. Questo è evidenziato da un incremento di +0.01 nei valori di TLI e CFI rispetto al modello CFA. Anche i criteri informativi confermano il vantaggio del set-ESEM:\n\n\nCFA: AIC = 15985.43, BIC = 16226.27, BIC corretto = 16013.84.\n\nSet-ESEM: AIC = 15971.25, BIC = 16298.37, BIC corretto = 16009.84.\n\nValori più bassi di AIC e BIC indicano un miglior adattamento complessivo per il modello set-ESEM.\n\n59.4.6 Riduzione delle correlazioni sovrastimate\nUn noto limite del CFA è la tendenza a sovrastimare le correlazioni tra variabili latenti esogene, il che può aumentare il rischio di collinearità e compromettere l’accuratezza dei parametri stimati (Shao et al., 2022). Il set-ESEM riduce notevolmente questo problema. Ad esempio:\n\nLa correlazione tra Autonomia_Insegnante e Relazionalità_Insegnante nel modello CFA è di .80, mentre nel set-ESEM scende a .51 (Δr = .29).\n\nQuesta riduzione migliora la validità discriminante e garantisce stime più attendibili.\n\n59.4.7 Sensibilità ai percorsi significativi\nIl set-ESEM dimostra una maggiore capacità di rilevare percorsi significativi rispetto al CFA. Due esempi illustrativi includono:\n\nIl percorso Competenza_Insegnante → Motivazione_Intrinseca è non significativo nel CFA (β = .08, p = .51), ma diventa significativo nel set-ESEM (β = .19, p = .03).\nIl percorso Relazionalità_Insegnante → Intenzione_di_Ritiro è non significativo nel CFA (β = −.20, p = .18), ma significativo nel set-ESEM (β = −.28, p = .01).\n\nQuesti risultati dimostrano come il set-ESEM possa rivelare relazioni importanti tra variabili latenti che il CFA potrebbe non individuare.\nIn conclusione, i risultati evidenziano che i modelli CFA e set-ESEM possono portare a interpretazioni diverse sulle relazioni tra variabili latenti. Tuttavia, il miglior adattamento del set-ESEM, unito alla riduzione delle correlazioni spurie e a una maggiore sensibilità ai percorsi significativi, suggerisce che i coefficienti stimati con questo approccio siano più affidabili. In contesti analitici complessi, il set-ESEM si dimostra un’opzione preferibile, garantendo un equilibrio ottimale tra flessibilità, parsimonia e validità delle stime.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>Exploratory structural equation modelling</span>"
    ]
  },
  {
    "objectID": "chapters/sem/13_esem.html#riflessioni-conclusive",
    "href": "chapters/sem/13_esem.html#riflessioni-conclusive",
    "title": "59  Exploratory structural equation modelling",
    "section": "\n59.5 Riflessioni Conclusive",
    "text": "59.5 Riflessioni Conclusive\nIn questo tutorial, Marsh & Alamer (2024) presentano un’introduzione approfondita all’ESEM, con particolare attenzione al set-ESEM. Sebbene questa tecnica sia spesso utilizzata per i modelli di misurazione, gli autori ne esplorano l’applicazione nei modelli strutturali, evidenziando i vantaggi concettuali ed empirici rispetto al CFA e all’ESEM completamente rilassato. Il set-ESEM, grazie alla possibilità di specificare “mini-set” indipendenti di ESEM all’interno di un unico modello (Marsh et al., 2020), rappresenta un compromesso ottimale tra flessibilità e parsimonia. Questo approccio è particolarmente utile quando l’ESEM completo risulta tecnicamente impraticabile o teoricamente non giustificato.\nGli esempi discussi dagli autori dimostrano che il set-ESEM offre una rappresentazione più accurata dei dati rispetto al CFA. Sebbene entrambi i modelli possano mostrare un adattamento accettabile, le correlazioni tra fattori esogeni nel CFA tendono a essere sistematicamente sovrastimate, come evidenziato in entrambi gli studi presentati. Questo porta a un rischio maggiore di multicollinearità, che può influenzare negativamente la stima dei coefficienti di percorso (Mai et al., 2018; Morin, 2023). Ad esempio, nello Studio 1, alcune relazioni tra variabili latenti risultano significative nel set-ESEM, ma non nel CFA. In particolare:\n\nIl percorso Competenza_Insegnante → Motivazione_Intrinseca, non significativo nel CFA (β = .08, p = .51), diventa significativo nel set-ESEM (β = .19, p = .03).\nIl percorso Relazionalità_Insegnante → Intenzione_di_Ritiro, non significativo nel CFA (β = −.20, p = .18), risulta significativo nel set-ESEM (β = −.28, p = .01).\n\nQueste differenze sottolineano come il set-ESEM riesca a identificare relazioni importanti tra variabili che il CFA potrebbe non rilevare. Simili variazioni emergono nello Studio 2 (non trattato qui), dove le differenze tra i due approcci influenzano le conclusioni sulle relazioni longitudinali.\nMarsh & Alamer (2024) identificano i principali vantaggi del set-ESEM rispetto a CFA ed ESEM completo:\n\n\nEquilibrio tra parsimonia e adattamento: Il set-ESEM è più parsimonioso dell’ESEM completo e si adatta spesso meglio del CFA.\n\nSeparazione dei costrutti teorici: Permette di includere costrutti teoricamente distinti in un unico modello, evitando carichi incrociati non giustificati.\n\nRotazione target: Consente un approccio confermativo, superando le limitazioni delle rotazioni meccaniche (es. geomin).\n\nGestione di modelli strutturali complessi: Rende possibile testare modelli strutturali che l’ESEM completo non può trattare.\n\nMigliore validità discriminante: Le correlazioni tra fattori risultano più realistiche rispetto al CFA.\n\nPrecisione dei coefficienti di percorso: Riduce l’attenuazione degli effetti, migliorando l’accuratezza delle stime e diminuendo i tassi di errore di tipo II.\n\nNonostante i suoi vantaggi, il set-ESEM rimane meno parsimonioso rispetto al CFA. Pertanto, quando i modelli CFA e set-ESEM mostrano correlazioni tra fattori e indici di adattamento simili, il CFA dovrebbe essere preferito per la sua semplicità. Tuttavia, in presenza di cross-loading teoricamente giustificati, il set-ESEM rappresenta l’opzione migliore.\nIn sintesi, il set-ESEM offre una soluzione metodologica efficace per superare i limiti del CFA e dell’ESEM completo. Gli esempi empirici mostrano come l’adozione del set-ESEM possa migliorare l’adattamento del modello e la precisione delle stime, evitando problemi di collinearità comuni nel CFA. L’analisi dei dati con il CFA, senza considerare il set-ESEM, potrebbe portare a interpretazioni parziali o errate delle relazioni tra variabili, con implicazioni importanti per la teoria e la pratica (Shao et al., 2022; Tabachnick & Fidell, 2023).\nIn definitiva, Marsh & Alamer (2024) raccomandano l’uso del set-ESEM per analisi strutturali e di misurazione, specialmente quando l’ESEM completo è troppo flessibile o il CFA troppo restrittivo. La loro analisi evidenzia che, in situazioni in cui i coefficienti di percorso differiscono significativamente tra CFA e set-ESEM, quest’ultimo fornisce risultati più affidabili e interpretabili.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>Exploratory structural equation modelling</span>"
    ]
  },
  {
    "objectID": "chapters/sem/13_esem.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/sem/13_esem.html#informazioni-sullambiente-di-sviluppo",
    "title": "59  Exploratory structural equation modelling",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.2\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] kableExtra_1.4.0  ggokabeito_0.1.0  see_0.11.0        MASS_7.3-65      \n#&gt;  [5] viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1   \n#&gt;  [9] gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6   \n#&gt; [13] semPlot_1.1.6     lavaan_0.6-19     psych_2.4.12      scales_1.3.0     \n#&gt; [17] markdown_1.13     knitr_1.50        lubridate_1.9.4   forcats_1.0.0    \n#&gt; [21] stringr_1.5.1     dplyr_1.1.4       purrr_1.0.4       readr_2.1.5      \n#&gt; [25] tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0  \n#&gt; [29] here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.1      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.2.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] rio_1.2.3           emmeans_1.10.7      zoo_1.8-13         \n#&gt;  [22] igraph_2.1.4        mime_0.13           lifecycle_1.0.4    \n#&gt;  [25] pkgconfig_2.0.3     Matrix_1.7-3        R6_2.6.1           \n#&gt;  [28] fastmap_1.2.0       rbibutils_2.3       shiny_1.10.0       \n#&gt;  [31] numDeriv_2016.8-1.1 digest_0.6.37       OpenMx_2.21.13     \n#&gt;  [34] fdrtool_1.2.18      colorspace_2.1-1    rprojroot_2.0.4    \n#&gt;  [37] Hmisc_5.2-3         timechange_0.3.0    abind_1.4-8        \n#&gt;  [40] compiler_4.4.2      withr_3.0.2         glasso_1.11        \n#&gt;  [43] htmlTable_2.4.3     backports_1.5.0     carData_3.0-5      \n#&gt;  [46] R.utils_2.13.0      ggsignif_0.6.4      corpcor_1.6.10     \n#&gt;  [49] gtools_3.9.5        tools_4.4.2         pbivnorm_0.6.0     \n#&gt;  [52] foreign_0.8-88      zip_2.3.2           httpuv_1.6.15      \n#&gt;  [55] nnet_7.3-20         R.oo_1.27.0         glue_1.8.0         \n#&gt;  [58] quadprog_1.5-8      nlme_3.1-167        promises_1.3.2     \n#&gt;  [61] lisrelToR_0.3       grid_4.4.2          checkmate_2.3.2    \n#&gt;  [64] cluster_2.1.8.1     reshape2_1.4.4      generics_0.1.3     \n#&gt;  [67] gtable_0.3.6        tzdb_0.5.0          R.methodsS3_1.8.2  \n#&gt;  [70] data.table_1.17.0   hms_1.1.3           xml2_1.3.8         \n#&gt;  [73] car_3.1-3           sem_3.1-16          pillar_1.10.1      \n#&gt;  [76] rockchalk_1.8.157   later_1.4.1         splines_4.4.2      \n#&gt;  [79] lattice_0.22-6      survival_3.8-3      kutils_1.73        \n#&gt;  [82] tidyselect_1.2.1    miniUI_0.1.1.1      pbapply_1.7-2      \n#&gt;  [85] reformulas_0.4.0    svglite_2.1.3       stats4_4.4.2       \n#&gt;  [88] xfun_0.51           qgraph_1.9.8        arm_1.14-4         \n#&gt;  [91] stringi_1.8.4       yaml_2.3.10         pacman_0.5.1       \n#&gt;  [94] boot_1.3-31         evaluate_1.0.3      codetools_0.2-20   \n#&gt;  [97] mi_1.1              cli_3.6.4           RcppParallel_5.1.10\n#&gt; [100] rpart_4.1.24        systemfonts_1.2.1   xtable_1.8-4       \n#&gt; [103] Rdpack_2.6.3        munsell_0.5.1       Rcpp_1.0.14        \n#&gt; [106] coda_0.19-4.1       png_0.1-8           XML_3.99-0.18      \n#&gt; [109] parallel_4.4.2      jpeg_0.1-10         lme4_1.1-36        \n#&gt; [112] mvtnorm_1.3-3       openxlsx_4.2.8      rlang_1.1.5        \n#&gt; [115] multcomp_1.4-28     mnormt_2.1.1\n\n\n\n\n\nMarsh, H., & Alamer, A. (2024). When and how to use set-exploratory structural equation modelling to test structural models: A tutorial using the R package lavaan. British Journal of Mathematical and Statistical Psychology.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>Exploratory structural equation modelling</span>"
    ]
  },
  {
    "objectID": "chapters/sem/14_sem_power.html",
    "href": "chapters/sem/14_sem_power.html",
    "title": "60  Dimensione Campionaria e Analisi della Potenza",
    "section": "",
    "text": "60.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nBuchberger et al. (2024) osservano che, nell’ultimo decennio, la questione della bassa potenza statistica e delle sue conseguenze sull’interpretazione dei risultati scientifici ha attirato un’attenzione crescente nella ricerca psicologica. La potenza statistica rappresenta la capacità di rilevare effetti reali, ovvero la probabilità di individuare correttamente un effetto quando è effettivamente presente. Questo valore dipende strettamente dalla dimensione campionaria, dall’entità dell’effetto che si intende rilevare e dall’affidabilità delle misurazioni.\nLa bassa potenza rappresenta un problema in quanto può limitare l’utilità scientifica dei risultati; inoltre, anche se si rilevano effetti significativi, questi potrebbero non riflettere associazioni reali. Per garantire che gli studi empirici siano adeguatamente progettati per rilevare gli effetti nei campioni esaminati, è cruciale calcolare in anticipo la potenza e stabilire la dimensione campionaria necessaria, evitando studi sotto-dimensionati.\nIn genere, in psicologia, si considera adeguata una potenza intorno a 0.80, anche se alcuni autori suggeriscono di puntare a valori più elevati, come 0.95, per bilanciare meglio gli errori di tipo I e tipo II.\nDiversi autori hanno proposto regole pratiche per stabilire la dimensione campionaria nei modelli SEM, come un numero minimo di osservazioni o un rapporto tra osservazioni e parametri da stimare. Tuttavia, queste regole empiriche possono essere fuorvianti, poiché la potenza nei SEM dipende anche da fattori quali l’entità dei carichi fattoriali e la complessità del modello. Inoltre, la dimensione campionaria necessaria dipende dalla specifica domanda di ricerca.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Dimensione Campionaria e Analisi della Potenza</span>"
    ]
  },
  {
    "objectID": "chapters/sem/14_sem_power.html#obiettivi-principali-dei-modelli-sem",
    "href": "chapters/sem/14_sem_power.html#obiettivi-principali-dei-modelli-sem",
    "title": "60  Dimensione Campionaria e Analisi della Potenza",
    "section": "60.2 Obiettivi Principali dei Modelli SEM",
    "text": "60.2 Obiettivi Principali dei Modelli SEM\nBuchberger et al. (2024) fanno notare che i due obiettivi principali dei SEM sono:\n\nRilevare un effetto specifico, come determinare se la correlazione tra due fattori latenti è diversa da zero.\nConfrontare modelli, verificando quale tra due o più modelli spiega meglio i dati.\n\nQuesti due obiettivi richiedono approcci diversi: il primo riguarda la potenza per rilevare un effetto mirato, mentre il secondo si concentra sulla potenza per individuare errori di specificazione del modello. Nei confronti tra modelli non nidificati (non derivabili l’uno dall’altro tramite restrizioni parametriche), i test basati sul \\(\\chi^2\\) non sono utilizzabili; è quindi necessario ricorrere a metodi alternativi, come le simulazioni Monte Carlo.\nMolti studi SEM mirano a determinare se un parametro specifico differisca da un valore atteso. In tal caso, si confronta un modello in cui il parametro è stimato liberamente con uno in cui è fissato a un valore specifico. Se il confronto tra i modelli suggerisce una differenza significativa, si può concludere che il parametro differisce effettivamente dal valore atteso.\nPer molti ricercatori, è rilevante individuare quale tra modelli teorici concorrenti descriva meglio i dati. Quando i modelli sono nidificati, è possibile usare metodi analitici basati sul \\(\\chi^2\\), mentre per modelli non nidificati si ricorre a tecniche di randomizzazione, come le simulazioni Monte Carlo. Questo approccio consente di stimare la potenza per confrontare modelli non nidificati senza compromettere le assunzioni teoriche del modello.\nIn conclusione, la determinazione della dimensione campionaria e l’analisi di potenza per i SEM richiedono una valutazione accurata della specificità della domanda di ricerca e delle ipotesi del modello, utilizzando approcci sia analitici che di simulazione per ottenere stime affidabili.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Dimensione Campionaria e Analisi della Potenza</span>"
    ]
  },
  {
    "objectID": "chapters/sem/14_sem_power.html#dimensione-campionaria-per-confronti-tra-gruppi",
    "href": "chapters/sem/14_sem_power.html#dimensione-campionaria-per-confronti-tra-gruppi",
    "title": "60  Dimensione Campionaria e Analisi della Potenza",
    "section": "60.3 Dimensione Campionaria per Confronti tra Gruppi",
    "text": "60.3 Dimensione Campionaria per Confronti tra Gruppi\nPer introdurre il metodo della simulazione Monte Carlo applicato all’analisi di potenza nei modelli SEM, seguiremo il tutorial di Chen & Yung (2023). Prima di esplorare l’uso della simulazione Monte Carlo per stimare la potenza in contesti SEM, è utile applicare questo metodo a un caso più semplice: il calcolo della dimensione campionaria necessaria per rilevare una differenza clinicamente rilevante tra due gruppi. Questo esempio introduttivo aiuterà a chiarire il funzionamento della simulazione Monte Carlo. Successivamente, applicheremo lo stesso approccio alla determinazione della potenza per i modelli SEM, un caso più complesso che richiede tecniche avanzate di simulazione per stimare accuratamente la potenza.\n\n60.3.1 Formula per la Dimensione Campionaria per Gruppo\nLa formula per determinare la dimensione campionaria per ciascun gruppo, per rilevare una differenza clinicamente significativa tra due gruppi, è:\n\\[\nn \\geq 2 \\left(\\frac{s^2}{\\delta^2}\\right) \\left[z_{1 - \\alpha/2} + z_{1 - \\beta}\\right]^2,\n\\tag{60.1}\\]\ndove:\n\n\\(s\\) è la stima della deviazione standard (assumendo varianze omogenee tra i gruppi);\n\\(\\delta\\) è la differenza clinicamente rilevante tra i gruppi, cioè l’effetto minimo che si desidera rilevare;\n\\(z_{1 - \\alpha/2}\\) e \\(z_{1 - \\beta}\\) sono i valori critici della distribuzione normale standard per i limiti degli errori di tipo I e tipo II.\n\n\n\n60.3.2 Derivazione\nPer calcolare la dimensione campionaria necessaria a confrontare le medie di due gruppi indipendenti, seguiamo i passaggi teorici necessari per derivare la formula finale. Il nostro interesse è quantificare la differenza tra le medie dei due gruppi, indicata come \\(\\mu_1 - \\mu_2\\). Supponiamo che le due medie siano stimate da campioni con deviazione standard comune \\(\\sigma\\).\nPoiché ciascuna media di campione (\\(\\bar{X}_1\\) per il gruppo 1 e \\(\\bar{X}_2\\) per il gruppo 2) ha varianza \\(\\frac{\\sigma^2}{n}\\) e i campioni sono indipendenti, la varianza della differenza \\(\\bar{X}_1 - \\bar{X}_2\\) è data dalla somma delle varianze:\n\\[\n\\text{Var}(\\bar{X}_1 - \\bar{X}_2) = \\frac{\\sigma^2}{n} + \\frac{\\sigma^2}{n} = \\frac{2\\sigma^2}{n}.\n\\]\nDa qui, la deviazione standard della differenza tra le medie è quindi:\n\\[\n\\text{Deviazione standard} = \\sqrt{\\frac{2\\sigma^2}{n}}.\n\\]\nPer il teorema del limite centrale, questa differenza tra le medie segue una distribuzione normale, dato che \\(n\\) è sufficientemente grande.\nPer determinare la potenza del test e la dimensione campionaria necessaria, definiamo:\n\nIpotesi nulla \\(H_0\\): \\(\\mu_1 - \\mu_2 = 0\\) (assenza di differenza significativa).\nIpotesi alternativa \\(H_1\\): \\(\\mu_1 - \\mu_2 = \\delta\\), dove \\(\\delta\\) rappresenta una differenza significativa che vogliamo rilevare.\nLivello di significatività \\(\\alpha\\): probabilità di rifiutare \\(H_0\\) quando è vera (errore di Tipo I).\nPotenza desiderata \\(1-\\beta\\): probabilità di rilevare una vera differenza \\(\\delta\\) (cioè non commettere un errore di Tipo II).\n\nSotto l’ipotesi nulla, la differenza tra le medie campionarie, \\(\\bar{X}_1 - \\bar{X}_2\\), ha media zero e deviazione standard \\(\\sqrt{\\frac{2\\sigma^2}{n}}\\). Possiamo quindi utilizzare la statistica \\(Z\\) per standardizzare questa differenza:\n\\[\nZ = \\frac{(\\bar{X}_1 - \\bar{X}_2) - 0}{\\sqrt{\\frac{2\\sigma^2}{n}}}.\n\\]\nIn un test a due code con livello di significatività \\(\\alpha\\), rifiutiamo \\(H_0\\) se il valore assoluto di \\(Z\\) supera \\(z_{1-\\alpha/2}\\), ovvero se:\n\\[\n|\\bar{X}_1 - \\bar{X}_2| &gt; z_{1-\\alpha/2} \\cdot \\sqrt{\\frac{2\\sigma^2}{n}}.\n\\]\nQuesto ci dice che il valore critico per rifiutare \\(H_0\\) è pari a \\(z_{1-\\alpha/2} \\cdot \\sqrt{\\frac{2\\sigma^2}{n}}\\). Tuttavia, poiché vogliamo assicurare una potenza del test pari a \\(1-\\beta\\), la differenza \\(\\delta\\) deve superare questo valore critico con probabilità \\(1 - \\beta\\).\nSotto l’ipotesi alternativa \\(H_1\\), la differenza attesa tra le medie campionarie è \\(\\delta\\), e quindi standardizziamo rispetto alla deviazione standard della differenza:\n\\[\nZ = \\frac{\\delta}{\\sqrt{\\frac{2\\sigma^2}{n}}}.\n\\]\nImponiamo ora che la somma dei valori critici \\(z_{1-\\alpha/2}\\) e \\(z_{1-\\beta}\\) sia uguale a questa statistica test, ottenendo:\n\\[\n\\delta = \\sqrt{\\frac{2\\sigma^2}{n}} (z_{1-\\alpha/2} + z_{1-\\beta}).\n\\]\nRisolviamo questa equazione per \\(n\\) al fine di ottenere la dimensione campionaria necessaria:\n\\[\nn \\geq \\frac{2\\sigma^2}{\\delta^2} (z_{1-\\alpha/2} + z_{1-\\beta})^2.\n\\]\nNella pratica, non sempre conosciamo \\(\\sigma^2\\); pertanto, lo sostituiamo con la stima \\(s^2\\), ottenendo:\n\\[\nn \\geq 2 \\left(\\frac{s^2}{\\delta^2}\\right) [z_{1-\\alpha/2} + z_{1-\\beta}]^2.\n\\]\nQuesta formula fornisce un modo pratico per calcolare la dimensione campionaria minima necessaria per garantire che il test abbia la potenza desiderata, tenendo conto della variabilità dei dati (\\(s^2\\)), dell’errore di Tipo I (\\(\\alpha\\)), dell’errore di Tipo II (\\(\\beta\\)), e della differenza minima rilevante \\(\\delta\\).\nIn conclusione, questa derivazione della dimensione campionaria è utile poiché:\n\nConsente di specificare il livello di controllo sugli errori di Tipo I e Tipo II.\nTiene conto della variabilità campionaria attraverso la stima \\(s^2\\).\nPermette di impostare una differenza minima rilevante da rilevare tra i gruppi.\nFornisce una stima della dimensione campionaria necessaria per garantire la potenza statistica richiesta.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Dimensione Campionaria e Analisi della Potenza</span>"
    ]
  },
  {
    "objectID": "chapters/sem/14_sem_power.html#esempio-pratico",
    "href": "chapters/sem/14_sem_power.html#esempio-pratico",
    "title": "60  Dimensione Campionaria e Analisi della Potenza",
    "section": "60.4 Esempio Pratico",
    "text": "60.4 Esempio Pratico\nConsideriamo i dati seguenti, ponendo \\(\\alpha\\) = 0.05 e la potenza \\(1-\\beta\\) = 0.80:\n\nmu2 &lt;- 1.2\nmu1 &lt;- 1\nsd &lt;- 0.5 # SD of each group\n\nalpha &lt;- 0.05\nbeta &lt;- 0.2\n\nCalcoliamo la dimensione minima campionaria per ciascun gruppo:\n\n# Mean difference\ndelta &lt;- mu2 - mu1\n\n# Required sample size\nn &lt;- 2 * sd^2 / delta^2 * (qnorm(1 - alpha / 2) + qnorm(1 - beta))^2\nn\n\n98.1109966793636",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Dimensione Campionaria e Analisi della Potenza</span>"
    ]
  },
  {
    "objectID": "chapters/sem/14_sem_power.html#collegamento-con-la-modellazione-a-equazioni-strutturali",
    "href": "chapters/sem/14_sem_power.html#collegamento-con-la-modellazione-a-equazioni-strutturali",
    "title": "60  Dimensione Campionaria e Analisi della Potenza",
    "section": "60.5 Collegamento con la Modellazione a Equazioni Strutturali",
    "text": "60.5 Collegamento con la Modellazione a Equazioni Strutturali\nIl confronto tra due gruppi è un caso relativamente semplice rispetto alla complessità dei modelli a equazioni strutturali (SEM), i quali spesso richiedono approcci avanzati per il calcolo della potenza, come il Monte Carlo Simulation-Based Approach (MCSB). Per comprendere meglio questo metodo, è utile applicare inizialmente il MCSB a un caso base come il confronto tra le medie di due gruppi. Successivamente, estenderemo questo approccio per includere i SEM, dove la complessità delle relazioni e dei parametri richiede strumenti di simulazione più sofisticati.\n\n60.5.1 Applicazione del Metodo MCSB\nPer implementare il metodo MCSB, dobbiamo definire due componenti principali:\n\nIl modello generativo dei dati, che descrive i parametri noti e serve per simulare i dati, come le differenze attese tra i gruppi e la variabilità delle misure.\nIl modello di stima dei parametri, che specifica come i parametri saranno stimati dai dati simulati, utilizzando un software di modellazione SEM come lavaan o simsem.\n\n\n\n60.5.2 Riformulazione del Confronto tra Trattamenti come Modello di Regressione\nPossiamo rappresentare il confronto tra gruppi anche come modello di regressione lineare:\n\\[\ny = \\beta_0 + \\beta_1 \\cdot TRT + \\epsilon,\n\\]\ndove:\n\n\\(y\\) è la variabile di esito,\n\\(TRT\\) è una variabile indicatrice che differenzia i gruppi (1 per il gruppo trattato, 0 per il gruppo di controllo),\n\\(\\beta_0\\) rappresenta la media del gruppo di controllo,\n\\(\\beta_1\\) rappresenta la differenza media tra i gruppi,\n\\(\\epsilon\\) è il termine di errore, distribuito normalmente con media zero.\n\n\n\n60.5.3 Implementazione in R dell’Approccio MCSB con simsem\nPer calcolare la potenza e stimare correttamente i parametri, è necessario determinare la dimensione dell’effetto e la varianza residua dell’esito \\(y\\) in questa equazione. Di seguito, i passaggi per implementare il metodo MCSB:\n\nCalcolo del \\(d\\) di Cohen:\nSupponiamo di avere una differenza clinicamente rilevante \\(\\delta = 0.2\\) e una deviazione standard \\(sd = 0.5\\). La dimensione dell’effetto standardizzata, o \\(d\\) di Cohen, è calcolata come:\n\\[\nd = \\frac{\\delta}{sd} = \\frac{0.2}{0.5} = 0.4.\n\\]\nConversione di \\(d\\) in Coefficiente di Correlazione \\(r\\):\nSpesso è utile esprimere la dimensione dell’effetto in termini di coefficiente di correlazione \\(r\\), che riflette la forza dell’associazione tra trattamento e esito. Il pacchetto compute.es in R include la funzione des() per convertire \\(d\\) in un valore di correlazione equivalente \\(r\\).\nUsando des(d, n.1 = n, n.2 = n), otteniamo un valore approssimativo di \\(r \\approx 0.2\\).\nCalcolo della Varianza di \\(y\\):\nIn questo contesto, il coefficiente \\(r = 0.2\\) rappresenta l’associazione tra il trattamento (TRT) e l’esito \\(y\\). La varianza totale di \\(y\\) può essere separata in una parte spiegata (dall’effetto di \\(TRT\\)) e una parte non spiegata. La varianza residua di \\(y\\), dopo aver tenuto conto del predittore, è data da \\(1 - r^2\\), poiché \\(r^2\\) rappresenta la proporzione della varianza di \\(y\\) spiegata dal trattamento:\n\\[\n\\text{var}(y) = 1 - r^2.\n\\]\nSostituendo \\(r = 0.2\\):\n\\[\n\\text{var}(y) = 1 - (0.2)^2 = 1 - 0.04 = 0.96.\n\\]\n\nQuesti passaggi possono essere implementati in R utilizzando il pacchetto compute.es, facilitando il calcolo e la configurazione della simulazione per l’analisi di potenza con simsem.\n\n# Calcolo dell'ES in termini di d\nd &lt;- delta / sd\n# print(d) # E.g., [1] 0.4\n\n# Conversione in altre misure di ES\nd2ES &lt;- des(d, n.1 = n, n.2 = n, verbose = FALSE)\n\n# Estrazione di r\nr &lt;- d2ES$r\nprint(r) # E.g., [1] 0.2\n\n# Calcolo della varianza di y\nvar.y &lt;- 1 - r**2\nprint(var.y) # E.g., [1] 0.96\n\n[1] 0.2\n[1] 0.96\n\n\nPer utilizzare simsem, è necessario specificare il processo di generazione dei dati. Per il caso presente abbiamo:\n\n# Modello di generazione dei dati\ndatMod &lt;- \"\n    # Regressione con correlazione nota di 0.2\n    y ~ 0.2*TRT\n    # Varianza dell'errore\n    y ~~ 0.96*y\n\"\n\nIl valore \\(0.2\\) in y ~ 0.2*TRT rappresenta tecnicamente un coefficiente di regressione. Tuttavia, se le variabili sono standardizzate, questo valore coincide con la correlazione tra la variabile predittore \\(TRT\\) e la variabile di risposta \\(y\\).\nIn questo contesto, considerare il coefficiente di correlazione è utile poiché ci permette di calcolare facilmente la varianza residua di \\(y\\). Conoscendo \\(r\\), infatti, possiamo stabilire la varianza residua come \\(1 - r^2\\), semplificando la specifica del modello.\nIl modello di stima è il seguente:\n\n# Modello di stima\nestMod &lt;- \"\n    # Regressione\n    y ~ TRT\n\"\n\nPossiamo ora eseguire la simulazione MCSB:\n\nsimOut &lt;- sim(\n    nRep = 1000, \n    generate = datMod, \n    model = estMod, \n    n = 198,\n    lavaanfun = \"sem\", \n    seed = 123, \n    silent = TRUE\n)\n\nNella simulazione eseguita con simOut, il parametro nRep indica il numero di repliche, che è stato impostato a 1000 per ridurre i tempi di esecuzione. Idealmente, sarebbe preferibile un numero più elevato (ad esempio &gt;10.000) per ottenere risultati più precisi. Il parametro generate specifica il modello di generazione dei dati (qui datMod), mentre model collega al modello di stima (qui estMod) utilizzando la funzione lavaanfun = \"sem\". La dimensione del campione, n, è impostata a 198 per il calcolo della potenza statistica, e seed consente di riprodurre la simulazione con gli stessi risultati. L’opzione silent = TRUE sopprime l’output intermedio per semplificare la visualizzazione.\nEsaminiamo l’output della simulazione.\n\nsummaryParam(simOut)\n\n\nA data.frame: 2 x 10\n\n\n\nEstimate Average\nEstimate SD\nAverage SE\nPower (Not equal 0)\nStd Est\nStd Est SD\nStd Ave SE\nAverage Param\nAverage Bias\nCoverage\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\ny~TRT\n0.202\n0.0693\n0.0698\n0.816\n0.201\n0.0675\n0.0671\n0.20\n0.00219\n0.953\n\n\ny~~y\n0.956\n0.0969\n0.0960\n1.000\n0.955\n0.0271\n0.0267\n0.96\n-0.00433\n0.942\n\n\n\n\n\nL’output mostra diverse colonne relative ai parametri stimati con due righe: la prima riga è per lo stimatore della correlazione tra \\(y\\) e TRT (cioè il coefficiente di regressione), mentre la seconda riga si riferisce alla varianza residua di \\(y\\) in y ~~ y.\n\nEstimate Average: È la media delle stime dei parametri nelle 1000 repliche. I valori ottenuti (0.202 per y ~ TRT e 0.956 per y ~~ y) sono molto vicini ai valori simulati, cioè \\(r = 0.2\\) e \\(\\text{var}(y) = 0.96\\), indicando che le stime sono coerenti con i parametri di partenza.\nEstimate SD: È la deviazione standard delle stime dei parametri tra le repliche. Rappresenta la variabilità delle stime nei vari set di dati simulati, e aiuta a capire la precisione delle stime.\nAverage SE: È la media degli errori standard delle stime in tutte le repliche, un’indicazione della variabilità stimata del parametro per ogni replica.\nPower (Not equal 0): Questa colonna indica la proporzione di repliche in cui i parametri sono risultati significativamente diversi da zero. Qui la potenza statistica per y ~ TRT è 0.816, vicina al valore target di 0.80. La lieve differenza potrebbe essere ridotta aumentando nRep.\nAverage Param: Rappresenta i valori medi dei parametri simulati. In questo caso, sono i valori di partenza utilizzati nel modello di generazione dei dati, cioè 0.20 per y ~ TRT e 0.96 per y ~~ y.\nAverage Bias: È la differenza tra i valori medi stimati e i parametri di partenza. Nel nostro caso, il bias è molto piccolo, indicando che le stime sono ben centrate attorno ai parametri di partenza.\nCoverage: Indica la percentuale di intervalli di confidenza che contengono i valori veri dei parametri. In generale, ci si aspetta una copertura intorno al 95% per un intervallo di confidenza al 95%, qui abbiamo coperture di 95.3% e 94.2%, il che è in linea con le aspettative.\n\nIn sintesi, i risultati della simulazione mostrano stime accurate e, con una numerosità campionaria totale di \\(n = 198\\), raggiungono una potenza statistica vicina all’obiettivo di 0.80, accompagnata da un bias minimo e una buona copertura.\nIn altre parole, questa dimostrazione evidenzia come la tecnica di simulazione MCSB riesca, nel caso del confronto tra le medie di due gruppi, a collegare la dimensione del campione alla potenza del test, tenendo conto dell’ampiezza dell’effetto e della variabilità campionaria, in linea con le aspettative teoriche.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Dimensione Campionaria e Analisi della Potenza</span>"
    ]
  },
  {
    "objectID": "chapters/sem/14_sem_power.html#analisi-di-potenza-nei-modelli-sem",
    "href": "chapters/sem/14_sem_power.html#analisi-di-potenza-nei-modelli-sem",
    "title": "60  Dimensione Campionaria e Analisi della Potenza",
    "section": "60.6 Analisi di Potenza nei Modelli SEM",
    "text": "60.6 Analisi di Potenza nei Modelli SEM\nDopo aver compreso l’approccio MCSB e la sua implementazione in simsem, possiamo ora illustrare come calcolare la potenza per modelli di equazioni strutturali (SEM) generali. Un esempio è offerto da Chen & Yung (2023), che utilizzano un modello di crescita latente (LGC) per analizzare i dati di Byrne (2013).\n\nIl dataset raccoglie i dati longitudinali raccolti da 405 donne di Hong Kong, sottoposte a valutazione post-chirurgica per il tumore al seno (Byrne, 2013).\n\nI dati, disponibili nel file hkcancer_red2.dat, includono 10 variabili:\n\nID: identificatore di ciascuna partecipante.\nMOOD1, MOOD4, MOOD8: valutazioni soggettive dello stato d’animo a 1, 4 e 8 mesi dall’intervento; punteggi più alti indicano un umore peggiore.\nSOCADJ1, SOCADJ4, SOCADJ8: misure di adattamento sociale a 1, 4 e 8 mesi; punteggi più alti indicano un miglior adattamento sociale.\nAge: età della partecipante al momento dell’intervento.\nAgeGrp: categoria di età (dichotomizzata) con ‘Younger’ (&lt; 50 anni) e ‘Older’ (&gt; 50 anni).\nSurgTX: tipo di intervento chirurgico, distinguendo tra lumpectomia e mastectomia.\n\n\n60.6.1 Modello di Crescita Latente (LGC)\nIl modello di crescita latente consente di modellare le traiettorie di crescita nel tempo e di confrontare differenze tra gruppi (ad esempio, tra gruppi di età o di intervento). Questo modello permette di analizzare sia le variazioni intra-individuali che quelle inter-individuali nel contesto delle traiettorie longitudinali. Nel caso del dataset sul tumore al seno, possiamo modellare sia il cambiamento longitudinale dello stato d’animo (MOOD) che dell’adattamento sociale (SOCADJ) lungo un periodo di 8 mesi, includendo le variazioni tra le partecipanti nelle loro traiettorie.\nChen & Yung (2023) affrontano la questione della determinazione della dimensione campionaria necessaria per rilevare un effetto significativo su “MOOD” con una potenza statistica pari a 0.80.\n\n\n60.6.2 Calcolo della Dimensione Campionaria\nPer determinare la dimensione campionaria necessaria a rilevare l’effetto dell’intervento chirurgico su “MOOD”, è necessario specificare il modello generativo dei dati, che comprende tutti i parametri del modello. Questi parametri descrivono il cambiamento atteso dello stato d’animo nel tempo in relazione al tipo di intervento.\nPrima di procedere con l’analisi di potenza, carichiamo i dati in R per adattare il modello in base alle osservazioni empiriche. Ecco come eseguire il caricamento dei dati:\n\ndCancer &lt;- rio::import(here::here(\"data\", \"hkcancer_red2.dat\"))\n\n\n# Replace all \"*\" with NA in dCancer\ndCancer[dCancer == \"*\"] &lt;- NA\nhead(dCancer)\n\n\nA data.frame: 6 x 11\n\n\n\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\nV10\nV11\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n3\n2\n15.000000\nNA\nNA\n95.906250\nNA\nNA\n70\n1\n1\n\n\n2\n2\n3\n16.000000\n25.000000\n22.000000\n114.888889\n105.111111\n90.444444\n47\n0\n1\n\n\n3\n2\n2\n37.000000\n26.000000\n25.000000\n80.666667\n95.333333\n95.333333\n47\n0\n1\n\n\n4\n2\n2\n19.000000\n16.000000\n15.000000\n112.838710\n108.580645\n99.000000\n52\n1\n1\n\n\n5\n1\n2\n13.000000\n16.000000\n14.000000\n115.000000\n105.000000\n101.000000\n43\n0\n1\n\n\n6\n1\n1\n21.000000\n28.000000\n19.000000\n106.451613\n114.967742\n107.516129\n34\n0\n0\n\n\n\n\n\n\nvar_names &lt;- c(\n\"X1\", \"X2\", \"MOOD1\", \"MOOD4\", \"MOOD8\", \"SOCADJ1\", \"SOCADJ4\", \"SOCADJ8\", \"Age\",  \n\"AgeGrp\", \"SurgTx\")\n\nnames(dCancer) &lt;- var_names\nhead(dCancer)\n\n\nA data.frame: 6 x 11\n\n\n\nX1\nX2\nMOOD1\nMOOD4\nMOOD8\nSOCADJ1\nSOCADJ4\nSOCADJ8\nAge\nAgeGrp\nSurgTx\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n3\n2\n15.000000\nNA\nNA\n95.906250\nNA\nNA\n70\n1\n1\n\n\n2\n2\n3\n16.000000\n25.000000\n22.000000\n114.888889\n105.111111\n90.444444\n47\n0\n1\n\n\n3\n2\n2\n37.000000\n26.000000\n25.000000\n80.666667\n95.333333\n95.333333\n47\n0\n1\n\n\n4\n2\n2\n19.000000\n16.000000\n15.000000\n112.838710\n108.580645\n99.000000\n52\n1\n1\n\n\n5\n1\n2\n13.000000\n16.000000\n14.000000\n115.000000\n105.000000\n101.000000\n43\n0\n1\n\n\n6\n1\n1\n21.000000\n28.000000\n19.000000\n106.451613\n114.967742\n107.516129\n34\n0\n0\n\n\n\n\n\nQuesta preparazione ci consentirà di configurare il modello LGC con simsem, impostando i parametri di interesse in modo da simulare la potenza con il metodo MCSB e verificare la capacità del modello di rilevare gli effetti desiderati su “MOOD”.\nDefiniamo il modello a crescita latente.\n\nmod4MOOD &lt;- \"\n    # Intercept and Slope with fixed-coefficients\n    iMOOD =~ 1*MOOD1 + 1*MOOD4 + 1*MOOD8\n    sMOOD =~ 0*MOOD1 + 1*MOOD4 + 2.33*MOOD8\n    \n    # Regression with a labeled for simulation\n    iMOOD ~ SurgTx\n    sMOOD ~ a*SurgTx\n\"\n\nQuesto modello di crescita latente è utilizzato per analizzare l’evoluzione dello stato d’animo (MOOD) di un gruppo di persone a tre momenti distinti nel tempo (1, 4 e 8 mesi dopo l’intervento chirurgico).\nIl modello ha due componenti principali: intercetta e pendenza. Queste rappresentano rispettivamente il valore iniziale e la velocità di cambiamento dello stato d’animo nel tempo.\n\nIntercetta (iMOOD): l’intercetta rappresenta il punto di partenza o il livello iniziale dello stato d’animo (MOOD). Nel modello, è definita come:\niMOOD =~ 1*MOOD1 + 1*MOOD4 + 1*MOOD8\nQui, iMOOD è una variabile latente che cattura il livello medio dello stato d’animo in ciascun momento, usando coefficienti fissati a 1 per indicare che ogni misura contribuisce allo stesso modo alla stima dell’intercetta. Questo implica che l’intercetta è il valore di base dello stato d’animo comune a tutti i partecipanti nei tre punti temporali.\nPendenza (sMOOD): la pendenza rappresenta la velocità e la direzione del cambiamento dello stato d’animo nel tempo.\nsMOOD =~ 0*MOOD1 + 1*MOOD4 + 2.33*MOOD8\nQui, sMOOD è una variabile latente che descrive come cambia lo stato d’animo nei diversi momenti temporali. I coefficienti (0, 1, 2.33) riflettono l’intervallo di tempo tra le misurazioni (ad esempio, da 1 a 8 mesi). Il coefficiente 2.33 per MOOD8 indica che l’effetto temporale si accumula, essendo il punto finale del periodo di osservazione.\n\nIl modello include anche delle regressioni che collegano l’intercetta e la pendenza a una variabile predittiva, il tipo di intervento chirurgico (SurgTx), che distingue tra due gruppi (lumpectomia e mastectomia).\n\niMOOD ~ SurgTx: questa regressione rappresenta l’effetto del tipo di intervento sul livello iniziale dello stato d’animo. In altre parole, cerca di vedere se esiste una differenza nello stato d’animo iniziale in base al tipo di intervento ricevuto.\nsMOOD ~ a*SurgTx: questa regressione, con un coefficiente etichettato a, rappresenta l’effetto del tipo di intervento sul tasso di cambiamento dello stato d’animo nel tempo. Il coefficiente a mostra se il tipo di intervento influisce sulla velocità di miglioramento o peggioramento dello stato d’animo lungo i mesi.\n\nIn sintesi, questo modello di crescita latente analizza sia il livello iniziale dello stato d’animo (intercetta) sia il cambiamento nel tempo (pendenza) e verifica se il tipo di intervento chirurgico influenza questi due aspetti.\nAdattiamo il modello ai dati.\n\n# Call growth function to fit the LGC\nfitMOOD &lt;- growth(mod4MOOD,\n    data = dCancer, \n    estimator = \"MLR\",\n    missing = \"fiml\"\n)\n\n\nsummary(fitMOOD) |&gt; print()\n\nlavaan 0.6-19 ended normally after 75 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n\n  Number of observations                           405\n  Number of missing patterns                         8\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                 2.022       2.077\n  Degrees of freedom                                 2           2\n  P-value (Chi-square)                           0.364       0.354\n  Scaling correction factor                                  0.974\n    Yuan-Bentler correction (Mplus variant)                       \n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  iMOOD =~                                            \n    MOOD1             1.000                           \n    MOOD4             1.000                           \n    MOOD8             1.000                           \n  sMOOD =~                                            \n    MOOD1             0.000                           \n    MOOD4             1.000                           \n    MOOD8             2.330                           \n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  iMOOD ~                                             \n    SurgTx           -0.118    0.758   -0.155    0.876\n  sMOOD ~                                             \n    SurgTx     (a)   -0.282    0.332   -0.850    0.395\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n .iMOOD ~~                                            \n   .sMOOD            -2.650    1.581   -1.676    0.094\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .iMOOD            10.550    0.671   15.715    0.000\n   .sMOOD            -0.389    0.298   -1.304    0.192\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .MOOD1            13.492    3.384    3.987    0.000\n   .MOOD4            18.792    2.415    7.782    0.000\n   .MOOD8             5.398    3.880    1.391    0.164\n   .iMOOD            25.827    3.574    7.226    0.000\n   .sMOOD             2.563    1.259    2.036    0.042\n\n\n\n\nsemPlot::semPaths(fitMOOD,\n    what = \"col\", \n    whatLabels = \"par\", \n    nCharNodes = 8,\n    shapeMan = \"rectangle\", \n    sizeMan = 8, \n    sizeMan2 = 7\n)\n\n\n\n\n\n\n\n\nUna volta ottenute le stime dei parametri del modello, possiamo utilizzarle per costruire la componente di generazione dei dati come segue (nel codice seguente, utilizzo i valori usati da Chen & Yung (2023) per riprodurre il loro risultato).\n\ndat.mod4MOOD &lt;- \"\n    # Intercept and Slope with MOOD\n    iMOOD =~ 1*MOOD1 + 1*MOOD4 + 1*MOOD8\n    sMOOD =~ 0*MOOD1 + 1*MOOD4 + 2.33*MOOD8\n    # residual variances for observed\n    MOOD1 ~~ 14.307*MOOD1\n    MOOD4 ~~ 18.637*MOOD4\n    MOOD8 ~~ 6.745*MOOD8\n    # Regression paths to covariates\n    iMOOD ~ (-0.116)*SurgTx\n    sMOOD ~ a*SurgTx + (-0.332)*SurgTx\n    # latent Intercepts\n    iMOOD ~ 21.683*1\n    sMOOD ~ 0.004*1\n    # latent variances/coVariances\n    iMOOD ~~ 25.826*iMOOD\n    sMOOD ~~ 2.272*sMOOD\n    iMOOD ~~ (-2.135)*sMOOD\n    # mean and variance for SurgTX\n    SurgTx ~ 0.5*1\n    SurgTx ~~ 0.25*SurgTx\n\"\n\nPossiamo ora usare la funzione sim() di simsem. Seguendo il tutorial di Chen & Yung (2023), fissiamo la dimensione campionaria complesiva a n = 405:\n\nsimOut1 &lt;- sim(\n    nRep = 1000, \n    generate = dat.mod4MOOD,\n    model = mod4MOOD, \n    n = 405, \n    lavaanfun = \"growth\",\n    seed = 123, \n    silent = TRUE\n)\n\n\n# simulation output\nsummary(simOut1)\n\nRESULT OBJECT\nModel Type\n[1] \"lavaan\"\n========= Fit Indices Cutoffs ============\n           Alpha\nFit Indices      0.1     0.05     0.01    0.001     Mean     SD\n      chisq    4.455    5.665    8.871   11.378    1.980  1.888\n      aic   7571.694 7588.105 7624.726 7667.762 7508.592 49.361\n      bic   7611.733 7628.144 7664.765 7707.801 7548.631 49.361\n      rmsea    0.055    0.067    0.092    0.108    0.016  0.025\n      cfi      0.994    0.991    0.983    0.975    0.998  0.004\n      tli      0.982    0.972    0.948    0.926    1.000  0.014\n      srmr     0.018    0.020    0.026    0.028    0.011  0.005\n========= Parameter Estimates and Standard Errors ============\n             Estimate Average Estimate SD Average SE Power (Not equal 0)\niMOOD~SurgTx           -0.088       0.614      0.607               0.056\na                      -0.337       0.244      0.242               0.287\nMOOD1~~MOOD1           14.312       2.882      2.816               1.000\nMOOD4~~MOOD4           18.563       1.831      1.846               1.000\nMOOD8~~MOOD8            6.843       3.477      3.401               0.536\niMOOD~~iMOOD           25.691       3.168      3.189               1.000\nsMOOD~~sMOOD            2.211       1.124      1.100               0.512\niMOOD~~sMOOD           -2.072       1.278      1.284               0.359\niMOOD~1                21.664       0.440      0.429               1.000\nsMOOD~1                 0.007       0.167      0.171               0.043\n             Std Est Std Est SD Std Ave SE Average Param Average Bias\niMOOD~SurgTx  -0.009      0.061      0.060        -0.116        0.028\na             -0.126      0.129      0.195        -0.332       -0.005\nMOOD1~~MOOD1   0.357      0.067      0.066        14.307        0.005\nMOOD4~~MOOD4   0.437      0.031      0.031        18.637       -0.074\nMOOD8~~MOOD8   0.195      0.099      0.096         6.745        0.098\niMOOD~~iMOOD   0.996      0.005      0.006        25.826       -0.135\nsMOOD~~sMOOD   0.968      0.147      0.311         2.272       -0.061\niMOOD~~sMOOD  -0.272      0.459     13.990        -2.135        0.063\niMOOD~1        4.291      0.282      0.282        21.683       -0.019\nsMOOD~1        0.005      0.146      0.194         0.004        0.003\n             Coverage\niMOOD~SurgTx    0.949\na               0.955\nMOOD1~~MOOD1    0.946\nMOOD4~~MOOD4    0.947\nMOOD8~~MOOD8    0.949\niMOOD~~iMOOD    0.956\nsMOOD~~sMOOD    0.940\niMOOD~~sMOOD    0.949\niMOOD~1         0.938\nsMOOD~1         0.956\n========= Correlation between Fit Indices ============\n       chisq    aic    bic  rmsea    cfi    tli   srmr\nchisq  1.000 -0.012 -0.012  0.955 -0.939 -0.995  0.957\naic   -0.012  1.000  1.000 -0.002 -0.014  0.016 -0.040\nbic   -0.012  1.000  1.000 -0.002 -0.014  0.016 -0.040\nrmsea  0.955 -0.002 -0.002  1.000 -0.932 -0.949  0.890\ncfi   -0.939 -0.014 -0.014 -0.932  1.000  0.941 -0.818\ntli   -0.995  0.016  0.016 -0.949  0.941  1.000 -0.957\nsrmr   0.957 -0.040 -0.040  0.890 -0.818 -0.957  1.000\n================== Replications =====================\nNumber of replications = 1000 \nNumber of converged replications = 947 \nNumber of nonconverged replications: \n   1. Nonconvergent Results = 0 \n   2. Nonconvergent results from multiple imputation = 0 \n   3. At least one SE were negative or NA = 0 \n   4. Nonpositive-definite latent or observed (residual) covariance matrix \n      (e.g., Heywood case or linear dependency) = 53 \n\n\nPossiamo osservare che la potenza associata al parametro \\(a\\) (cioè, l’effetto dell’intervento chirurgico) è 0.287, il che non sorprende, dato che sapevamo già che questo parametro non è statisticamente significativo con la dimensione campionaria attuale di 405.\nLa domanda successiva è quindi quale dimensione campionaria sia necessaria per ottenere una potenza pari a 0.80. A questo fine, è necessario applicare l’approccio MCSB per una serie di dimensioni campionarie, in modo da calcolare le rispettive potenze.\nChen & Yung (2023) costruiscono una curva che mostra la relazione tra dimensione campionaria e potenza, così da identificare la dimensione campionaria necessaria per raggiungere una potenza di 0.80. Per questo scopo, Chen & Yung (2023) utilizzano n = rep(seq(400, 2000, by = 200), 500), per eseguire l’MCSB su una sequenza di dimensioni campionarie da 400 a 2000, con incrementi di 200 (ovvero: 400, 600, 800, 1000, 1200, 1400, 1600, 1800, e 2000), ciascuna delle quali sarà utilizzata per 500 simulazioni.\n\n# Simulation for sequential sample size\nsimAll &lt;- sim(\n    nRep = NULL, \n    generate = dat.mod4MOOD,\n    model = mod4MOOD, \n    n = rep(seq(400, 2000, 200), 500),\n    lavaanfun = \"growth\", \n    seed = 123, \n    silent = TRUE\n    ) \n\n\n# Print the simulations\nsummary(simAll)\n\nRESULT OBJECT\nModel Type\n[1] \"lavaan\"\n========= Fit Indices Cutoffs ============\n     N chisq   aic   bic rmsea   cfi   tli  srmr\n1  400  6.19  7503  7545 0.061 0.993 0.978 0.019\n2  800  6.04 14933 14979 0.053 0.994 0.983 0.016\n3 1200  5.88 22363 22413 0.044 0.996 0.987 0.013\n4 1600  5.73 29793 29847 0.036 0.997 0.992 0.011\n5 2000  5.57 37223 37281 0.027 0.999 0.996 0.008\n========= Parameter Estimates and Standard Errors ============\n             Estimate Average Estimate SD Average SE Power (Not equal 0)\niMOOD~SurgTx           -0.122       0.400      0.387               0.066\na                      -0.327       0.163      0.154               0.607\nMOOD1~~MOOD1           14.298       1.831      1.794               1.000\nMOOD4~~MOOD4           18.662       1.230      1.178               1.000\nMOOD8~~MOOD8            6.700       2.223      2.163               0.855\niMOOD~~iMOOD           25.761       2.094      2.033               1.000\nsMOOD~~sMOOD            2.265       0.722      0.700               0.877\niMOOD~~sMOOD           -2.130       0.855      0.818               0.761\niMOOD~1                21.687       0.284      0.273               1.000\nsMOOD~1                 0.002       0.116      0.109               0.058\n             Std Est Std Est SD Std Ave SE Average Param Average Bias\niMOOD~SurgTx  -0.012      0.039      0.038        -0.116       -0.006\na             -0.113      0.079      0.329        -0.332        0.005\nMOOD1~~MOOD1   0.357      0.043      0.042        14.307       -0.009\nMOOD4~~MOOD4   0.439      0.020      0.020        18.637        0.025\nMOOD8~~MOOD8   0.191      0.063      0.061         6.745       -0.045\niMOOD~~iMOOD   0.998      0.003      0.003        25.826       -0.065\nsMOOD~~sMOOD   0.981      0.164      1.757         2.272       -0.007\niMOOD~~sMOOD  -0.272      0.097      0.134        -2.135        0.005\niMOOD~1        4.280      0.186      0.178        21.683        0.004\nsMOOD~1        0.000      0.086      0.169         0.004       -0.002\n             Coverage r_coef.n r_se.n\niMOOD~SurgTx    0.949   -0.008 -0.933\na               0.943   -0.022 -0.933\nMOOD1~~MOOD1    0.953   -0.007 -0.931\nMOOD4~~MOOD4    0.950   -0.022 -0.925\nMOOD8~~MOOD8    0.952    0.016 -0.933\niMOOD~~iMOOD    0.951   -0.001 -0.929\nsMOOD~~sMOOD    0.955   -0.004 -0.935\niMOOD~~sMOOD    0.948    0.012 -0.934\niMOOD~1         0.948   -0.003 -0.935\nsMOOD~1         0.942    0.012 -0.935\n========= Correlation between Fit Indices ============\n       chisq    aic    bic  rmsea    cfi    tli   srmr      n\nchisq  1.000 -0.007 -0.007  0.911 -0.780 -0.854  0.825 -0.007\naic   -0.007  1.000  1.000 -0.158  0.204  0.015 -0.427  1.000\nbic   -0.007  1.000  1.000 -0.158  0.204  0.015 -0.427  1.000\nrmsea  0.911 -0.158 -0.158  1.000 -0.875 -0.897  0.866 -0.158\ncfi   -0.780  0.204  0.204 -0.875  1.000  0.930 -0.776  0.204\ntli   -0.854  0.015  0.015 -0.897  0.930  1.000 -0.818  0.015\nsrmr   0.825 -0.427 -0.427  0.866 -0.776 -0.818  1.000 -0.427\nn     -0.007  1.000  1.000 -0.158  0.204  0.015 -0.427  1.000\n================== Replications =====================\nNumber of replications = 4500 \nNumber of converged replications = 4455 \nNumber of nonconverged replications: \n   1. Nonconvergent Results = 0 \n   2. Nonconvergent results from multiple imputation = 0 \n   3. At least one SE were negative or NA = 0 \n   4. Nonpositive-definite latent or observed (residual) covariance matrix \n      (e.g., Heywood case or linear dependency) = 45 \nNOTE: The sample size is varying.\n\n\nUtilizzando la funzione getPower, è quindi ottenere le stime di potenza per ciascuna dimensione campionaria come segue:\n\nLGC.N &lt;- getPower(simAll)\n# Find the samplesize for 80% power\nfindPower(LGC.N, \"N\", 0.8) |&gt; print()\n\niMOOD~SurgTx            a MOOD1~~MOOD1 MOOD4~~MOOD4 MOOD8~~MOOD8 \n          NA         1710          Inf          Inf          816 \niMOOD~~iMOOD sMOOD~~sMOOD iMOOD~~sMOOD      iMOOD~1      sMOOD~1 \n         Inf          748         1161          Inf           NA \n\n\nPertanto, in base alla simulazione di Chen & Yung (2023), sarebbe necessaria una dimensione campionaria di 1710 per ottenere una potenza di 0.80.\nQuesto risultato può essere mostrato graficamente nella figura seguente. In questa figura, la linea tratteggiata orizzontale indica la potenza a 0.80, mentre la linea con la freccia che va da questa linea orizzontale a \\(N\\) = 1710 indica la dimensione campionaria determinata dalla curva di potenza.\n\n# Call plotPower to plot the power to sample size\nplotPower(simAll, powerParam = \"a\")\n# Add a horizontal line of 0.80\nabline(h = 0.8, lwd = 2, lty = 8)\narrows(1710, 0.8, 1710, 0, lwd = 2)\ntext(\n    1710,\n    -0.018, \"N = 1710\"\n)",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Dimensione Campionaria e Analisi della Potenza</span>"
    ]
  },
  {
    "objectID": "chapters/sem/14_sem_power.html#riflessioni-conclusive",
    "href": "chapters/sem/14_sem_power.html#riflessioni-conclusive",
    "title": "60  Dimensione Campionaria e Analisi della Potenza",
    "section": "60.7 Riflessioni Conclusive",
    "text": "60.7 Riflessioni Conclusive\nIn questo capitolo, abbiamo illustrato come determinare la dimensione campionaria e calcolare la potenza statistica utilizzando l’approccio basato sulla simulazione Monte Carlo. Questo metodo rappresenta l’approccio più generale e flessibile per ottenere stime affidabili della dimensione campionaria e della potenza necessarie alla progettazione di studi in vari contesti di ricerca, inclusi i modelli a equazioni strutturali (SEM).\nLa simulazione Monte Carlo consente di affrontare situazioni complesse e realistiche che spesso non sono gestibili con metodi analitici tradizionali. Essa permette infatti di modellare vari scenari, includendo la variabilità dei parametri e le incertezze che caratterizzano i dati empirici. Inoltre, il metodo può essere applicato a una vasta gamma di modelli statistici, garantendo flessibilità nell’adattamento a diversi contesti di studio e domande di ricerca.\nIn conclusione, il metodo basato sulla simulazione Monte Carlo offre un potente strumento per pianificare studi empirici robusti e ben fondati, permettendo ai ricercatori di prendere decisioni informate riguardo alla dimensione del campione e alla configurazione del modello in relazione agli obiettivi specifici del loro studio.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Dimensione Campionaria e Analisi della Potenza</span>"
    ]
  },
  {
    "objectID": "chapters/sem/14_sem_power.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/sem/14_sem_power.html#informazioni-sullambiente-di-sviluppo",
    "title": "60  Dimensione Campionaria e Analisi della Potenza",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nRandom number generation:\n RNG:     L'Ecuyer-CMRG \n Normal:  Inversion \n Sample:  Rejection \n \nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] quantreg_5.99     compute.es_0.2-5  simsem_0.5-16     kableExtra_1.4.0 \n [5] MASS_7.3-61       viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n [9] ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1 \n[13] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-19     psych_2.4.6.26   \n[17] scales_1.3.0      markdown_1.13     knitr_1.49        lubridate_1.9.3  \n[21] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[25] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1    \n[29] tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] splines_4.4.2       later_1.3.2         pbdZMQ_0.3-13      \n  [4] R.oo_1.27.0         XML_3.99-0.17       rpart_4.1.23       \n  [7] lifecycle_1.0.4     rstatix_0.7.2       rprojroot_2.0.4    \n [10] lattice_0.22-6      rockchalk_1.8.157   backports_1.5.0    \n [13] magrittr_2.0.3      openxlsx_4.2.7.1    Hmisc_5.2-0        \n [16] rmarkdown_2.29      httpuv_1.6.15       qgraph_1.9.8       \n [19] zip_2.3.1           pbapply_1.7-2       minqa_1.2.8        \n [22] multcomp_1.4-26     abind_1.4-8         quadprog_1.5-8     \n [25] R.utils_2.12.3      nnet_7.3-19         TH.data_1.1-2      \n [28] sandwich_3.1-1      openintro_2.5.0     arm_1.14-4         \n [31] MatrixModels_0.5-3  airports_0.1.0      svglite_2.1.3      \n [34] codetools_0.2-20    xml2_1.3.6          tidyselect_1.2.1   \n [37] farver_2.1.2        lme4_1.1-35.5       stats4_4.4.2       \n [40] base64enc_0.1-3     jsonlite_1.8.9      Formula_1.2-5      \n [43] survival_3.7-0      emmeans_1.10.5      systemfonts_1.1.0  \n [46] tools_4.4.2         rio_1.2.3           Rcpp_1.0.13-1      \n [49] glue_1.8.0          mnormt_2.1.1        xfun_0.49          \n [52] IRdisplay_1.1       withr_3.0.2         fastmap_1.2.0      \n [55] boot_1.3-31         fansi_1.0.6         SparseM_1.84-2     \n [58] digest_0.6.37       mi_1.1              timechange_0.3.0   \n [61] R6_2.5.1            mime_0.12           estimability_1.5.1 \n [64] colorspace_2.1-1    Cairo_1.6-2         gtools_3.9.5       \n [67] jpeg_0.1-10         R.methodsS3_1.8.2   utf8_1.2.4         \n [70] generics_0.1.3      data.table_1.16.2   corpcor_1.6.10     \n [73] usdata_0.3.1        htmlwidgets_1.6.4   pkgconfig_2.0.3    \n [76] sem_3.1-16          gtable_0.3.6        htmltools_0.5.8.1  \n [79] carData_3.0-5       png_0.1-8           rstudioapi_0.17.1  \n [82] tzdb_0.4.0          reshape2_1.4.4      uuid_1.2-1         \n [85] coda_0.19-4.1       checkmate_2.3.2     nlme_3.1-166       \n [88] nloptr_2.1.1        repr_1.1.7          zoo_1.8-12         \n [91] parallel_4.4.2      miniUI_0.1.1.1      foreign_0.8-87     \n [94] pillar_1.9.0        grid_4.4.2          vctrs_0.6.5        \n [97] promises_1.3.0      car_3.1-3           OpenMx_2.21.13     \n[100] xtable_1.8-4        cluster_2.1.6       htmlTable_2.4.3    \n[103] evaluate_1.0.1      pbivnorm_0.6.0      mvtnorm_1.3-2      \n[106] cli_3.6.3           kutils_1.73         compiler_4.4.2     \n[109] rlang_1.1.4         crayon_1.5.3        ggsignif_0.6.4     \n[112] fdrtool_1.2.18      plyr_1.8.9          stringi_1.8.4      \n[115] munsell_0.5.1       lisrelToR_0.3       pacman_0.5.1       \n[118] Matrix_1.7-1        IRkernel_1.3.2      hms_1.1.3          \n[121] glasso_1.11         shiny_1.9.1         igraph_2.1.1       \n[124] broom_1.0.7         RcppParallel_5.1.9  cherryblossom_0.1.0\n\n\n\n\n\n\nBuchberger, E. S., Ngo, C. T., Peikert, A., Brandmaier, A. M., & Werkle-Bergner, M. (2024). Estimating statistical power for structural equation models in developmental cognitive science: A tutorial in R: Power simulation for SEMs. Behavior Research Methods, 1–18.\n\n\nByrne, B. M. (2013). Structural equation modeling with Mplus: Basic concepts, applications, and programming. routledge.\n\n\nChen, D.-G., & Yung, Y.-F. (2023). Structural Equation Modeling Using R/SAS: A Step-by-step Approach with Real Data Analysis. CRC Press.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Dimensione Campionaria e Analisi della Potenza</span>"
    ]
  },
  {
    "objectID": "chapters/sem/E_01.html",
    "href": "chapters/sem/E_01.html",
    "title": "61  ✏️ Esercizi",
    "section": "",
    "text": "Nello studio di {cite:t}weiss2018difficulties viene esaminata la relazione tra la difficiltà di regolare le emozioni positive e l’abuso di alcol e di sostanze. Gli autori propongono due modelli SEM. Si riproduca l’analisi svolta da {cite:t}weiss2018difficulties usando lavaan.\n\nsource(\"../_common.R\")\n\nsuppressPackageStartupMessages({\n    library(\"lavaan\")\n    library(\"lavaanExtra\")\n    library(\"lavaanPlot\")\n    library(\"psych\")\n    library(\"dplyr\") \n    library(\"tidyr\")\n    library(\"knitr\")\n    library(\"mvnormalTest\")\n    library(\"semPlot\")\n    library(\"DiagrammeRsvg\")\n    library(\"rsvg\")\n    library(\"effectsize\")\n})\nset.seed(42)\n\nNello studio di {cite:t}weiss2018difficulties\n\nLa difficoltà di regolare le emozioni positive viene misurata con la Difficulties in Emotion Regulation Scale – Positive (DERS-P; Weiss, Gratz, & Lavender, 2015), che comprende le sottoscale di Acceptance, Impulse, e Goals.\nL’abuso di sostanze viene misurato con la Drug Abuse Screening Test (DAST; Skinner, 1982).\nL’abuso di alcol viene misurato con la Alcohol Use Disorder Identification Test (AUDIT; Saunders, Aasland, Babor, De la Fuente, & Grant, 1993), con le sottoscale di Hazardous Consumption, Dependence, e Consequences.\n\nI dati di un campione di 284 partecipanti sono riportati nella forma di una matrice di correlazione.\n\nlower &lt;- \"\n   1\n   .38 1\n   .41 .64 1\n   .34 .44 .30 1\n   .29 .12 .27 .06 1\n   .29 .22 .20 .17 .54 1\n   .30 .15 .23 .09 .73 .69 1\n\"\n\n\ndat_cov &lt;- lavaan::getCov(\n    lower,\n    names = c(\"dmis\", \"con\", \"dep\", \"consu\", \"acc\", \"goal\", \"imp\")\n)\nprint(dat_cov)\n\n      dmis  con  dep consu  acc goal  imp\ndmis  1.00 0.38 0.41  0.34 0.29 0.29 0.30\ncon   0.38 1.00 0.64  0.44 0.12 0.22 0.15\ndep   0.41 0.64 1.00  0.30 0.27 0.20 0.23\nconsu 0.34 0.44 0.30  1.00 0.06 0.17 0.09\nacc   0.29 0.12 0.27  0.06 1.00 0.54 0.73\ngoal  0.29 0.22 0.20  0.17 0.54 1.00 0.69\nimp   0.30 0.15 0.23  0.09 0.73 0.69 1.00\n\n\nIn questo studio, gli autori adottano due modelli SEM distinti per analizzare i dati. Nel primo modello, si postula che la difficoltà nella regolazione delle emozioni positive funzioni come variabile esogena, influenzando sia l’abuso di sostanze sia l’abuso di alcol. Inoltre, si ipotizza una correlazione tra abuso di sostanze e abuso di alcol, suggerendo una possibile interdipendenza tra questi due comportamenti problematici.\nPer quanto riguarda le variabili latenti specifiche, la difficoltà di regolare le emozioni positive, indicata come drpe, è rappresentata da una variabile latente che si basa su tre indicatori.Parallelamente, l’abuso di alcol, etichettato come amis, è concepito come una seconda variabile latente, anch’essa identificata tramite tre indicatori distinti.\n\nmod &lt;- \"\n  drpe =~ NA*acc + goal + imp\n  amis =~ NA*con + dep + consu\n  amis ~ drpe\n  dmis ~ drpe\n  dmis ~~ amis\n  drpe ~~ 1*drpe\n  amis ~~ 1*amis\n\"\n\nAdattiamo il modello ai dati con sem().\n\nfit &lt;- lavaan::sem(mod, sample.cov = dat_cov, sample.nobs = 284)\n\nEsaminiamo i risultati.\n\nstandardizedSolution(fit) |&gt; print()\n\n     lhs op   rhs est.std    se      z pvalue ci.lower ci.upper\n1   drpe =~   acc   0.770 0.031 24.982  0.000    0.710    0.830\n2   drpe =~  goal   0.728 0.033 21.849  0.000    0.663    0.794\n3   drpe =~   imp   0.945 0.024 39.322  0.000    0.898    0.992\n4   amis =~   con   0.837 0.039 21.217  0.000    0.759    0.914\n5   amis =~   dep   0.756 0.041 18.420  0.000    0.676    0.837\n6   amis =~ consu   0.494 0.052  9.439  0.000    0.392    0.597\n7   amis  ~  drpe   0.254 0.066  3.863  0.000    0.125    0.383\n8   dmis  ~  drpe   0.334 0.056  6.001  0.000    0.225    0.443\n9   amis ~~  dmis   0.458 0.055  8.303  0.000    0.350    0.567\n10  drpe ~~  drpe   1.000 0.000     NA     NA    1.000    1.000\n11  amis ~~  amis   0.936 0.033 28.023  0.000    0.870    1.001\n12   acc ~~   acc   0.407 0.047  8.575  0.000    0.314    0.500\n13  goal ~~  goal   0.470 0.049  9.677  0.000    0.375    0.565\n14   imp ~~   imp   0.107 0.045  2.349  0.019    0.018    0.196\n15   con ~~   con   0.300 0.066  4.551  0.000    0.171    0.430\n16   dep ~~   dep   0.428 0.062  6.900  0.000    0.307    0.550\n17 consu ~~ consu   0.756 0.052 14.595  0.000    0.654    0.857\n18  dmis ~~  dmis   0.889 0.037 23.960  0.000    0.816    0.961\n\n\nCreiamo un path diagram.\n\nsemPaths(fit,\n    whatLabels = \"std\",\n    sizeMan = 10,\n    edge.label.cex = 0.9,\n    style = \"ram\",\n    nCharNodes = 0, nCharEdges = 0\n)\n\n\n\n\n\n\n\n\nGli autori esplorano un modello alternativo nel quale le relazioni causali vengono rovesciate: in questo caso è la difficoltà di regolazione delle emozioni positive ad essere la variabile esogena, e l’abuso di sostanze e l’abuso di alcol sono le variabili esogene.\n\nmod_alt &lt;- \"\n  drpe =~ NA*acc + goal + imp\n  amis =~ NA*con + dep + consu\n  drpe ~ amis + dmis\n  dmis ~~ amis\n  drpe ~~ 1*drpe\n  amis ~~ 1*amis\n\"\n\n\nfit_alt &lt;- sem(mod_alt, sample.cov = dat_cov, sample.nobs = 311)\n\n\nstandardizedSolution(fit_alt) |&gt; print()\n\n     lhs op   rhs est.std    se      z pvalue ci.lower ci.upper\n1   drpe =~   acc   0.770 0.029 26.143  0.000    0.712    0.828\n2   drpe =~  goal   0.728 0.032 22.864  0.000    0.666    0.791\n3   drpe =~   imp   0.945 0.023 41.149  0.000    0.900    0.990\n4   amis =~   con   0.837 0.038 22.203  0.000    0.763    0.910\n5   amis =~   dep   0.756 0.039 19.276  0.000    0.679    0.833\n6   amis =~ consu   0.494 0.050  9.877  0.000    0.396    0.592\n7   drpe  ~  amis   0.115 0.075  1.549  0.121   -0.031    0.261\n8   drpe  ~  dmis   0.276 0.066  4.189  0.000    0.147    0.405\n9   amis ~~  dmis   0.503 0.050 10.122  0.000    0.405    0.600\n10  drpe ~~  drpe   0.879 0.037 23.633  0.000    0.806    0.952\n11  amis ~~  amis   1.000 0.000     NA     NA    1.000    1.000\n12   acc ~~   acc   0.407 0.045  8.973  0.000    0.318    0.496\n13  goal ~~  goal   0.470 0.046 10.126  0.000    0.379    0.561\n14   imp ~~   imp   0.107 0.043  2.458  0.014    0.022    0.192\n15   con ~~   con   0.300 0.063  4.763  0.000    0.177    0.424\n16   dep ~~   dep   0.428 0.059  7.221  0.000    0.312    0.545\n17 consu ~~ consu   0.756 0.049 15.273  0.000    0.659    0.853\n18  dmis ~~  dmis   1.000 0.000     NA     NA    1.000    1.000\n\n\n\nsemPaths(fit_alt,\n    whatLabels = \"std\",\n    sizeMan = 10,\n    edge.label.cex = 0.9,\n    style = \"ram\",\n    nCharNodes = 0, nCharEdges = 0\n)\n\n\n\n\n\n\n\n\nVedremo in seguito come sia possibile eseguire un test statistico per stabilire quale di due modelli sia più appropriato. Anticipando qui tale discussione, applichiamo il test del rapporto di verosimiglianze.\n\nlavTestLRT(fit, fit_alt) |&gt; print()\n\n\nChi-Squared Difference Test\n\n        Df    AIC    BIC  Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\nfit     12 4963.6 5022.0 38.211                                    \nfit_alt 12 5433.1 5492.9 41.844     3.6327     0       0           \n\n\nI risultati di questo test suggeriscono che il primo modello è maggiormente appropriato per descrivere i dati raccolti da {cite:t}weiss2018difficulties.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/sem/15_prior_pred_mod_check.html",
    "href": "chapters/sem/15_prior_pred_mod_check.html",
    "title": "62  Prior Predictive Model Checking",
    "section": "",
    "text": "62.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nIn questo capitolo verrà discusso l’approccio del Bayesian prior predictive similarity checking proposto da Bonifay et al. (2024).",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Prior Predictive Model Checking</span>"
    ]
  },
  {
    "objectID": "chapters/sem/15_prior_pred_mod_check.html#replicabilità-e-gof",
    "href": "chapters/sem/15_prior_pred_mod_check.html#replicabilità-e-gof",
    "title": "62  Prior Predictive Model Checking",
    "section": "62.2 Replicabilità e GOF",
    "text": "62.2 Replicabilità e GOF\nIl progresso scientifico empirico si basa sulla replicabilità, ossia la capacità di riprodurre i risultati di uno studio precedente seguendo le stesse procedure con nuovi dati (Bollen et al., 2015). Le ricerche sulla replicabilità in psicologia si sono spesso concentrate sugli effetti sperimentali (e.g., Klein, 2014; Open Science Collaboration, 2015; Youyou et al., 2023), ma molti ambiti della disciplina si fondano sull’uso di modelli statistici piuttosto che su un disegno sperimentale. Anche in questi contesti, è essenziale verificare il grado di replicabilità dei modelli statistici utilizzati.\nUn modo per quantificare la replicabilità dei modelli (sia nell’analisi di regressione, nei modelli a equazioni strutturali (SEM), nella teoria della risposta al item, nei modelli di rete o in altri contesti di modellizzazione) è valutare la bontà di adattamento (GOF, goodness of fit) del modello ai dati osservati. Storicamente, molti ricercatori in psicologia hanno considerato la replicabilità di un modello principalmente come la capacità di riprodurre la bontà di adattamento di uno studio precedente: «[il] miglior adattamento del modello… ha replicato i risultati» di ricerche precedenti (Whiteman et al., 2022, p. 132), «il miglior adattamento… ha replicato i risultati precedenti» (Giuntoli et al., 2021, p. 1668), «un adattamento sostanzialmente migliore… ha replicato l’approccio classico» (Fernández de la Cruz et al., 2018, p. 608). Tuttavia, Bonifay et al. (2024) fanno notare come questa pratica meriti un’attenta considerazione, in quanto la mera replicazione di una buona bontà di adattamento non è sufficiente per confermare la validità del modello statistico originale e della teoria sottostante.\nBonifay et al. (2024) propongono il seguente esempio. Si considerino le matrici di covarianza simulate mostrate nella riga superiore della Figura 1. La matrice a sinistra (Pannello B) rappresenta le covarianze tra le variabili di uno studio originale, mentre le altre due (Pannelli C e D) rappresentano le covarianze delle stesse variabili in due dataset di replicazione. Questo scenario illustra il tipico caso di replicazione del modello, in cui la stessa struttura viene adattata a dataset differenti, lasciando liberi i parametri. Sebbene le differenze nei dati siano evidenti, un modello con due fattori correlati si adatta bene a ciascuna matrice di covarianza (indice di adattamento comparativo [CFI] elevato, i.e., ≥ 0.95). Una bontà di adattamento elevata indica che il modello rappresenta adeguatamente le covarianze all’interno del dataset originale e delle repliche, ma non informa sul fatto che il modello rifletta le stesse relazioni tra le variabili. Come mostrato in Figura 1, affidarsi esclusivamente alla bontà di adattamento può portare a ignorare differenze significative nei pattern di dati.\n\n# Load Correlation Matrices ----\ncor_mats &lt;- readRDS(\n    here::here(\"data\", \"Bonifay\", \"figure1_cormat.RDS\")\n)\n\n# Create and Save Correlation Plots ----\ncorrplot(cor_mats[[1]],\n    method = \"circle\", type = \"full\",\n    tl.col = \"black\", tl.cex = 2.5, cl.pos = \"n\"\n)\n\ncorrplot(cor_mats[[2]],\n    method = \"circle\", type = \"full\",\n    tl.col = \"black\", tl.cex = 2.5, cl.pos = \"n\"\n)\n\ncorrplot(cor_mats[[3]],\n    method = \"circle\", type = \"full\",\n    tl.col = \"black\", tl.cex = 2.5, cl.pos = \"n\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncor_mats\n\n\n    $original\n        \n\nA matrix: 6 x 6 of type dbl\n\n\n\ny1\ny2\ny3\ny4\ny5\ny6\n\n\n\n\ny1\n1.000\n0.682\n0.638\n0.185\n0.507\n0.571\n\n\ny2\n0.682\n1.000\n0.644\n0.275\n0.661\n0.705\n\n\ny3\n0.638\n0.644\n1.000\n0.189\n0.473\n0.532\n\n\ny4\n0.185\n0.275\n0.189\n1.000\n0.468\n0.477\n\n\ny5\n0.507\n0.661\n0.473\n0.468\n1.000\n0.730\n\n\ny6\n0.571\n0.705\n0.532\n0.477\n0.730\n1.000\n\n\n\n\n\n    $replication1\n        \n\nA matrix: 6 x 6 of type dbl\n\n\n\ny1\ny2\ny3\ny4\ny5\ny6\n\n\n\n\ny1\n1.000\n0.389\n0.503\n0.527\n0.501\n0.391\n\n\ny2\n0.389\n1.000\n0.536\n0.432\n0.420\n0.423\n\n\ny3\n0.503\n0.536\n1.000\n0.782\n0.707\n0.493\n\n\ny4\n0.527\n0.432\n0.782\n1.000\n0.766\n0.452\n\n\ny5\n0.501\n0.420\n0.707\n0.766\n1.000\n0.380\n\n\ny6\n0.391\n0.423\n0.493\n0.452\n0.380\n1.000\n\n\n\n\n\n    $replication2\n        \n\nA matrix: 6 x 6 of type dbl\n\n\n\ny1\ny2\ny3\ny4\ny5\ny6\n\n\n\n\ny1\n1.0000\n-0.2736\n-0.3580\n0.0224\n0.1361\n0.0264\n\n\ny2\n-0.2736\n1.0000\n0.5371\n0.0917\n0.0198\n-0.0611\n\n\ny3\n-0.3580\n0.5371\n1.0000\n-0.0743\n-0.0419\n-0.0680\n\n\ny4\n0.0224\n0.0917\n-0.0743\n1.0000\n0.3464\n0.4743\n\n\ny5\n0.1361\n0.0198\n-0.0419\n0.3464\n1.0000\n0.3296\n\n\ny6\n0.0264\n-0.0611\n-0.0680\n0.4743\n0.3296\n1.0000\n\n\n\n\n\n\n\n\n\nM &lt;- '\n    F1 =~ NA*y1 + y2 + y3\n    F2 =~ NA*y4 + y5 + y6\n    F1 ~~ 1*F1\n    F2 ~~ 1 * F2\n'\n\nfit_original &lt;- cfa(model = M, sample.cov = cor_mats$original, sample.nobs = 1000)\nparameterEstimates(fit_original)\n\n\nA lavaan.data.frame: 15 x 9\n\n\nlhs\nop\nrhs\nest\nse\nz\npvalue\nci.lower\nci.upper\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nF1\n=~\ny1\n0.772\n0.0278\n27.73\n0\n0.718\n0.827\n\n\nF1\n=~\ny2\n0.899\n0.0260\n34.56\n0\n0.848\n0.949\n\n\nF1\n=~\ny3\n0.732\n0.0284\n25.77\n0\n0.677\n0.788\n\n\nF2\n=~\ny4\n0.502\n0.0311\n16.11\n0\n0.441\n0.563\n\n\nF2\n=~\ny5\n0.823\n0.0272\n30.22\n0\n0.770\n0.876\n\n\nF2\n=~\ny6\n0.903\n0.0262\n34.53\n0\n0.852\n0.954\n\n\nF1\n~~\nF1\n1.000\n0.0000\nNA\nNA\n1.000\n1.000\n\n\nF2\n~~\nF2\n1.000\n0.0000\nNA\nNA\n1.000\n1.000\n\n\ny1\n~~\ny1\n0.403\n0.0224\n18.00\n0\n0.359\n0.447\n\n\ny2\n~~\ny2\n0.192\n0.0183\n10.49\n0\n0.156\n0.227\n\n\ny3\n~~\ny3\n0.463\n0.0243\n19.03\n0\n0.415\n0.511\n\n\ny4\n~~\ny4\n0.747\n0.0348\n21.45\n0\n0.679\n0.816\n\n\ny5\n~~\ny5\n0.322\n0.0206\n15.59\n0\n0.281\n0.362\n\n\ny6\n~~\ny6\n0.183\n0.0192\n9.53\n0\n0.146\n0.221\n\n\nF1\n~~\nF2\n0.835\n0.0156\n53.59\n0\n0.804\n0.865\n\n\n\n\n\n\nfitMeasures(fit_original, \"cfi\") |&gt; round(2)\n\ncfi: 0.96\n\n\n\nsemPaths(fit_original,\n    whatLabels = \"std\",\n    sizeMan = 8,\n    edge.label.cex = 0.7,\n    style = \"mx\",\n    nCharNodes = 0, nCharEdges = 0\n)\n\n\n\n\n\n\n\n\n\nfit_rep1 &lt;- cfa(model = M, sample.cov = cor_mats$replication1, sample.nobs = 1000)\nparameterEstimates(fit_rep1)\n\n\nA lavaan.data.frame: 15 x 9\n\n\nlhs\nop\nrhs\nest\nse\nz\npvalue\nci.lower\nci.upper\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nF1\n=~\ny1\n0.596\n0.0300\n19.88\n0\n0.537\n0.655\n\n\nF1\n=~\ny2\n0.566\n0.0303\n18.69\n0\n0.507\n0.626\n\n\nF1\n=~\ny3\n0.899\n0.0265\n33.95\n0\n0.847\n0.951\n\n\nF2\n=~\ny4\n0.904\n0.0254\n35.67\n0\n0.855\n0.954\n\n\nF2\n=~\ny5\n0.830\n0.0265\n31.34\n0\n0.778\n0.882\n\n\nF2\n=~\ny6\n0.524\n0.0306\n17.14\n0\n0.464\n0.584\n\n\nF1\n~~\nF1\n1.000\n0.0000\nNA\nNA\n1.000\n1.000\n\n\nF2\n~~\nF2\n1.000\n0.0000\nNA\nNA\n1.000\n1.000\n\n\ny1\n~~\ny1\n0.644\n0.0306\n21.02\n0\n0.584\n0.704\n\n\ny2\n~~\ny2\n0.678\n0.0320\n21.21\n0\n0.616\n0.741\n\n\ny3\n~~\ny3\n0.191\n0.0203\n9.41\n0\n0.151\n0.231\n\n\ny4\n~~\ny4\n0.181\n0.0154\n11.73\n0\n0.151\n0.211\n\n\ny5\n~~\ny5\n0.310\n0.0180\n17.23\n0\n0.274\n0.345\n\n\ny6\n~~\ny6\n0.724\n0.0336\n21.54\n0\n0.658\n0.790\n\n\nF1\n~~\nF2\n0.959\n0.0127\n75.76\n0\n0.934\n0.984\n\n\n\n\n\n\nfitMeasures(fit_rep1, \"cfi\") |&gt; round(2)\n\ncfi: 0.96\n\n\n\nfit_rep2 &lt;- cfa(model = M, sample.cov = cor_mats$replication2, sample.nobs = 1000)\nparameterEstimates(fit_rep2)\n\n\nA lavaan.data.frame: 15 x 9\n\n\nlhs\nop\nrhs\nest\nse\nz\npvalue\nci.lower\nci.upper\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nF1\n=~\ny1\n0.422\n0.0363\n11.62\n0.000000\n0.35098\n0.493\n\n\nF1\n=~\ny2\n-0.627\n0.0412\n-15.23\n0.000000\n-0.70783\n-0.546\n\n\nF1\n=~\ny3\n-0.855\n0.0479\n-17.86\n0.000000\n-0.94899\n-0.761\n\n\nF2\n=~\ny4\n0.701\n0.0418\n16.78\n0.000000\n0.61899\n0.783\n\n\nF2\n=~\ny5\n0.491\n0.0370\n13.26\n0.000000\n0.41836\n0.563\n\n\nF2\n=~\ny6\n0.676\n0.0411\n16.43\n0.000000\n0.59526\n0.757\n\n\nF1\n~~\nF1\n1.000\n0.0000\nNA\nNA\n1.00000\n1.000\n\n\nF2\n~~\nF2\n1.000\n0.0000\nNA\nNA\n1.00000\n1.000\n\n\ny1\n~~\ny1\n0.821\n0.0405\n20.27\n0.000000\n0.74139\n0.900\n\n\ny2\n~~\ny2\n0.606\n0.0463\n13.09\n0.000000\n0.51506\n0.696\n\n\ny3\n~~\ny3\n0.268\n0.0707\n3.79\n0.000152\n0.12918\n0.406\n\n\ny4\n~~\ny4\n0.508\n0.0496\n10.23\n0.000000\n0.41057\n0.605\n\n\ny5\n~~\ny5\n0.758\n0.0403\n18.81\n0.000000\n0.67902\n0.837\n\n\ny6\n~~\ny6\n0.542\n0.0477\n11.37\n0.000000\n0.44876\n0.636\n\n\nF1\n~~\nF2\n0.091\n0.0432\n2.11\n0.035024\n0.00639\n0.176\n\n\n\n\n\n\nfitMeasures(fit_rep2, \"cfi\") |&gt; round(2)\n\ncfi: 0.94\n\n\nA complicare ulteriormente le cose, l’adattamento dello stesso modello a ciascuna matrice di dati produce stime dei parametri molto variabili. Ad esempio, il fattore di carico standardizzato l21 (secondo indicatore sul primo fattore) è stimato a 0.90, 0.57 e -2.63 nelle tre matrici, mentre la correlazione tra i fattori (c21) varia da quasi indipendenza (0.09) a una sovrapposizione quasi totale (0.96).\nIn sintesi, la bontà di adattamento non ci fornisce alcuna indicazione sul grado di somiglianza tra il dataset di replicazione, i parametri del modello e quelli dello studio originale, mentre una forte somiglianza tra questi elementi è cruciale per valutare il successo della replicazione.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Prior Predictive Model Checking</span>"
    ]
  },
  {
    "objectID": "chapters/sem/15_prior_pred_mod_check.html#bontà-di-adattamento-e-replicazione",
    "href": "chapters/sem/15_prior_pred_mod_check.html#bontà-di-adattamento-e-replicazione",
    "title": "62  Prior Predictive Model Checking",
    "section": "62.3 Bontà di Adattamento e Replicazione",
    "text": "62.3 Bontà di Adattamento e Replicazione\nNel loro approfondimento critico sull’uso della bontà di adattamento come strumento per testare le teorie, Roberts & Pashler (2000) hanno sostenuto che «dimostrare che una teoria si adatta ai dati… è quasi privo di significato» (p. 361; vedi anche Vanpaemel, 2020). In particolare, hanno individuato tre limiti del GOF che ne impediscono l’uso come supporto teorico solido:\n\nNon chiarisce cosa predice una teoria.\n\nNon spiega la variabilità dei dati.\n\nNon considera la probabilità a priori che la teoria possa adattarsi a qualsiasi insieme di dati plausibili.\n\nDi conseguenza, Roberts & Pashler (2000) hanno concluso che il GOF fornisce un supporto convincente a una teoria solo quando sia i dati che la teoria sono ben vincolati, ovvero quando i dati non sono troppo variabili e la teoria non è troppo flessibile. Tuttavia, in un singolo studio, tali vincoli possono essere difficili da definire e applicare, anche per la mancanza di criteri di riferimento (ad esempio, cosa significa dire che i dati «non sono troppo variabili»? Variabili rispetto a cosa? E in che misura?).\nNel contesto delle repliche, però, il confronto con lo studio originale offre un chiaro riferimento per caratterizzare la variabilità dei dati e delle stime dei parametri del modello. Questo consente di estendere naturalmente le tre critiche di Roberts & Pashler (2000) al tema della replicazione:\n\nPrevisione limitata del risultato della replica\nLa bontà di adattamento dello studio originale non fornisce alcuna informazione sostanziale sull’esito della replica. Il fatto che un modello si sia adattato bene nello studio originale non implica che si replichino aspetti inferenziali più importanti, come i pattern dei dati o le stime dei parametri.\nAssenza di indicazioni sulla somiglianza tra i dati originali e quelli della replica\nDue set di dati possono presentare pattern nettamente distinti, potenzialmente derivanti da meccanismi di generazione diversi. Tuttavia, il modello potrebbe mascherare queste differenze, compromettendo l’accuratezza delle inferenze.\nTendenza intrinseca del modello ad adattarsi bene\nSe un modello possiede una forte predisposizione ad adattarsi bene ai dati (Bonifay & Cai, 2017; Falk & Muthukrishna, 2023), una buona bontà di adattamento per i dati originali e di replica non rappresenta una sorpresa né un valore scientifico. In tali casi, il GOF può replicarsi indipendentemente dai pattern specifici dei dati che il modello intende rappresentare.\n\nIdealmente, i ricercatori possono essere fiduciosi che i loro risultati offrano un supporto alla teoria alla base del modello statistico solo se dimostrano che i dati della replica non sono più variabili rispetto ai dati originali, e che le stime dei parametri nella replica non riflettono una maggiore flessibilità rispetto a quelle originali.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Prior Predictive Model Checking</span>"
    ]
  },
  {
    "objectID": "chapters/sem/15_prior_pred_mod_check.html#definire-lobiettivo-della-replica",
    "href": "chapters/sem/15_prior_pred_mod_check.html#definire-lobiettivo-della-replica",
    "title": "62  Prior Predictive Model Checking",
    "section": "62.4 Definire l’Obiettivo della Replica",
    "text": "62.4 Definire l’Obiettivo della Replica\nIn psicologia, ottenere una somiglianza perfetta tra studi è poco pratico e probabilmente non necessario, anche a causa dell’eterogeneità intrinseca della popolazione (McShane et al., 2019). Piuttosto, i ricercatori dovrebbero focalizzarsi sugli aspetti specifici dello studio originale che intendono replicare. A tal proposito, la Figura 2 illustra un approccio più ragionevole per definire obiettivi di replicazione.\n\n\n\n\n\n\nFigura 62.1: Definire Obiettivi Chiari è Essenziale per Indagare la Replicazione dei Modelli Statistici. Nota: Per andare oltre la semplice replicazione della bontà di adattamento (Goodness of Fit), i ricercatori devono mirare alle aree più significative e centrali del bersaglio. Questo richiede test più rigorosi dei dati e/o dei parametri del modello, che possono essere condotti utilizzando tecniche bayesiane di verifica della similarità predittiva a priori, con l’impiego di distribuzioni a priori sempre più informative, come illustrato nella parte inferiore della figura. La replicazione informata dalla teoria richiede, come minimo, di verificare ipotesi specifiche sui dati e/o sui parametri del modello. La replicazione empirica, sia approssimativa che ravvicinata, implica invece il controllo che i dati replicati e/o i parametri del modello siano rispettivamente approssimativamente o strettamente simili a quelli dello studio originale. (Figura tratta da Bonifay et al., 2024)\n\n\n\n\nCerchio esterno del bersaglio: rappresenta la pratica attuale nelle scienze sociali, spesso limitata a verificare che il modello originale abbia una buona bontà di adattamento ai dati della replica, senza considerare le caratteristiche empiriche dello studio originale. Questa pratica offre il supporto più debole alla teoria originale.\nCerchi interni: rappresentano obiettivi progressivamente più ambiziosi.\n\n\n62.4.1 Obiettivi di replica:\n\nReplicazione informata dalla teoria: un ricercatore interessato alle implicazioni teoriche più ampie dello studio originale può puntare alla teoria sottostante, formulando ipotesi specifiche sui dati o sulle stime dei parametri (es. “Per supportare l’associazione positiva tra x e y, il coefficiente di replica b deve avere un valore positivo”). Questo approccio supera la semplice verifica del GOF e merita studi dedicati.\nReplicazione empirica approssimativa: un ricercatore che desidera replicare direttamente i risultati empirici può puntare allo studio originale, testando la somiglianza approssimativa tra i dati e il modello della replica rispetto a quelli originali. Ad esempio, si può verificare se le covarianze tra i dati di replica riflettono quelle dello studio originale o se le stime dei parametri sono simili (es. “Per una replica approssimativa, b1 deve essere tra 0.4 e 0.7”).\nReplicazione empirica ravvicinata: è il test più rigoroso, in cui le stime devono essere estremamente simili a quelle originali (es. “Per una replica ravvicinata, b1 deve essere tra 0.52 e 0.58”). Riuscire a soddisfare tali criteri fornisce prove solide che il modello cattura lo stesso segnale in entrambi gli studi.\n\n\n\n62.4.2 Test progressivi e rischiosi\nLa struttura a cerchi concentrici del bersaglio rappresenta una sequenza di test sempre più stringenti. Come osservato da Roberts & Pashler (2000), i test di bontà di adattamento sono spesso troppo facili da superare, rendendoli deboli come prova di replica. La replicazione basata sul GOF è l’obiettivo più facile (e per alcuni modelli può comportare un rischio di fallimento praticamente nullo), offrendo il supporto più debole ai risultati originali.\n\nCerchi interni del bersaglio: man mano che ci si avvicina al centro, il rischio di fallimento aumenta, ma aumenta anche la forza delle prove a favore della replica. La replicazione informata dalla teoria è più rigorosa rispetto al GOF, offrendo supporto alla teoria sottostante. La replicazione empirica approssimativa è ancora più rischiosa, ma fornisce prove solide di somiglianza tra dati e parametri. Infine, la replicazione empirica ravvicinata è il test più rischioso, ma il suo successo rappresenta una prova molto forte della replica dei risultati originali.\n\nCome sottolineato da Waller & Meehl (2002), «i test rischiosi sono i mezzi più efficienti per valutare la solidità di una teoria».\nNella seconda parte dell’articolo, Bonifay et al. (2024) presentano un metodo statistico per quantificare la somiglianza tra i dati originali e replicati, oltre che tra le stime dei parametri. Discutono un esempio concreto nel contesto della modellizzazione della struttura latente della psicopatologia e forniscono raccomandazioni per futuri studi di replicazione.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Prior Predictive Model Checking</span>"
    ]
  },
  {
    "objectID": "chapters/sem/15_prior_pred_mod_check.html#verifica-predittiva-a-priori-dei-modelli",
    "href": "chapters/sem/15_prior_pred_mod_check.html#verifica-predittiva-a-priori-dei-modelli",
    "title": "62  Prior Predictive Model Checking",
    "section": "62.5 Verifica Predittiva a Priori dei Modelli",
    "text": "62.5 Verifica Predittiva a Priori dei Modelli\nPer indagare formalmente la somiglianza tra dati originali e di replica, nonché tra le stime dei parametri, Bonifay et al. (2024) propongono di utilizzare la verifica predittiva a priori bayesiana (Prior Predictive Model Checking, PrPMC; Box, 1980; Evans & Moshonov, 2006; Gelman et al., 2017). Questa tecnica sfrutta le distribuzioni a priori per valutare le implicazioni del modello prima di includere i dati osservati nell’analisi.\nIn un’analisi bayesiana, l’obiettivo principale è calcolare la distribuzione a posteriori \\(p(\\theta \\mid y)\\), combinando le informazioni sui dati osservati \\(y\\) e sul parametro sconosciuto \\(\\theta\\) tramite il teorema di Bayes:\n\\[\np(\\theta \\mid y) \\propto p(y \\mid \\theta)p(\\theta),\n\\]\ndove:\n\n\\(p(\\theta)\\) è la distribuzione a priori dei parametri.\n\\(p(y \\mid \\theta)\\) è la funzione di verosimiglianza dei dati dato il modello.\n\nLa PrPMC consiste nel generare campioni predittivi ipotetici per ciascuna variabile osservata, utilizzando esclusivamente le distribuzioni a priori definite sui parametri del modello. Questi campioni rappresentano scenari plausibili in base alle aspettative incorporate nei priori, permettendo una valutazione preliminare del modello.\nSupponiamo di voler stimare l’altezza media degli scalatori negli Stati Uniti, ipotizzando che sia vicina alla media nazionale di 168 cm (Fryar et al., 2021). Possiamo rappresentare questa aspettativa con una distribuzione normale \\(N(\\mu = 168, \\sigma = 10)\\). Generando campioni predittivi da questa distribuzione, otteniamo valori che variano plausibilmente attorno a questa media. Se la conoscenza sugli scalatori suggerisse un valore maggiore o minore, potremmo affinare i priori (ad esempio, aumentando \\(\\mu\\) o riducendo \\(\\sigma\\)) prima di raccogliere i dati.\nLa PrPMC può essere estesa per confrontare i dati osservati con i campioni predittivi. Questo confronto utilizza una statistica di test o una quantità di test per valutare la somiglianza tra i dati osservati e le aspettative predittive.\n\nStatistica di test: una proprietà statistica dei dati (es. mediana, range).\nQuantità di test: una proprietà dipendente dai dati e dal modello (es. stime dei parametri o indici di bontà di adattamento).\n\nAd esempio, per gli scalatori, potremmo confrontare la media delle altezze osservate con le medie dei campioni predittivi. Se la media osservata si trovasse agli estremi della distribuzione predittiva (ad esempio, \\(prpp \\leq 0.05\\) o \\(prpp \\geq 0.95\\)), ciò indicherebbe una discrepanza sistematica tra i dati osservati e le aspettative a priori.\nBonifay et al. (2024) propongono l’uso della PrPMC per verificare la somiglianza tra dati originali e di replica, così come tra le stime dei parametri del modello. Questo approccio, chiamato verifica di similarità predittiva a priori, consente di valutare:\n\nSomiglianza dei dati: confrontando la distribuzione dei dati replicati con le aspettative dei dati originali (es. intercorrelazioni tra item).\nSomiglianza dei parametri: confrontando le stime dei parametri derivanti dai dati di replica con quelle predette dal modello originale. Ad esempio, i caricamenti fattoriali stimati dal modello originale possono essere confrontati con quelli derivati da campioni predittivi a priori.\n\nCome illustrato nella Figura 62.1, i cerchi concentrici rappresentano diversi livelli di rischio e severità nei test di replica:\n\nCerchio esterno: distribuzioni a priori diffuse, che riflettono una bassa restrizione sui dati e sui parametri, portando a test meno rigorosi e meno significativi.\nCentro del bersaglio: distribuzioni a priori altamente informative, con restrizioni strette sui dati e sui parametri, producendo test più rigorosi e significativi.\n\nSe il valore \\(prpp\\) risultante si trova tra \\(0.05\\) e \\(0.95\\), possiamo concludere che i dati e/o i parametri replicati sono coerenti con le aspettative a priori. Questo approccio consente di condurre test progressivamente più stringenti e di acquisire prove più solide del successo della replica.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Prior Predictive Model Checking</span>"
    ]
  },
  {
    "objectID": "chapters/sem/15_prior_pred_mod_check.html#riflessioni-conclusive",
    "href": "chapters/sem/15_prior_pred_mod_check.html#riflessioni-conclusive",
    "title": "62  Prior Predictive Model Checking",
    "section": "62.6 Riflessioni Conclusive",
    "text": "62.6 Riflessioni Conclusive\nIn conclusione, la verifica predittiva a priori offre un metodo formale per quantificare la somiglianza tra studi originali e repliche, sia a livello di dati che di parametri. Implementando questa metodologia, i ricercatori possono definire obiettivi chiari e condurre analisi rigorose per valutare il successo della replicazione, migliorando così la robustezza delle conclusioni dello studio.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Prior Predictive Model Checking</span>"
    ]
  },
  {
    "objectID": "chapters/sem/15_prior_pred_mod_check.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/sem/15_prior_pred_mod_check.html#informazioni-sullambiente-di-sviluppo",
    "title": "62  Prior Predictive Model Checking",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nRandom number generation:\n RNG:     L'Ecuyer-CMRG \n Normal:  Inversion \n Sample:  Rejection \n \nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] quantreg_5.99     compute.es_0.2-5  simsem_0.5-16     kableExtra_1.4.0 \n [5] MASS_7.3-61       viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n [9] ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1 \n[13] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-19     psych_2.4.6.26   \n[17] scales_1.3.0      markdown_1.13     knitr_1.49        lubridate_1.9.3  \n[21] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[25] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1    \n[29] tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] splines_4.4.2       later_1.3.2         pbdZMQ_0.3-13      \n  [4] R.oo_1.27.0         XML_3.99-0.17       rpart_4.1.23       \n  [7] lifecycle_1.0.4     rstatix_0.7.2       rprojroot_2.0.4    \n [10] lattice_0.22-6      rockchalk_1.8.157   backports_1.5.0    \n [13] magrittr_2.0.3      openxlsx_4.2.7.1    Hmisc_5.2-0        \n [16] rmarkdown_2.29      httpuv_1.6.15       qgraph_1.9.8       \n [19] zip_2.3.1           pbapply_1.7-2       minqa_1.2.8        \n [22] multcomp_1.4-26     abind_1.4-8         quadprog_1.5-8     \n [25] R.utils_2.12.3      nnet_7.3-19         TH.data_1.1-2      \n [28] sandwich_3.1-1      openintro_2.5.0     arm_1.14-4         \n [31] MatrixModels_0.5-3  airports_0.1.0      svglite_2.1.3      \n [34] codetools_0.2-20    xml2_1.3.6          tidyselect_1.2.1   \n [37] farver_2.1.2        lme4_1.1-35.5       stats4_4.4.2       \n [40] base64enc_0.1-3     jsonlite_1.8.9      Formula_1.2-5      \n [43] survival_3.7-0      emmeans_1.10.5      systemfonts_1.1.0  \n [46] tools_4.4.2         rio_1.2.3           Rcpp_1.0.13-1      \n [49] glue_1.8.0          mnormt_2.1.1        xfun_0.49          \n [52] IRdisplay_1.1       withr_3.0.2         fastmap_1.2.0      \n [55] boot_1.3-31         fansi_1.0.6         SparseM_1.84-2     \n [58] digest_0.6.37       mi_1.1              timechange_0.3.0   \n [61] R6_2.5.1            mime_0.12           estimability_1.5.1 \n [64] colorspace_2.1-1    Cairo_1.6-2         gtools_3.9.5       \n [67] jpeg_0.1-10         R.methodsS3_1.8.2   utf8_1.2.4         \n [70] generics_0.1.3      data.table_1.16.2   corpcor_1.6.10     \n [73] usdata_0.3.1        htmlwidgets_1.6.4   pkgconfig_2.0.3    \n [76] sem_3.1-16          gtable_0.3.6        htmltools_0.5.8.1  \n [79] carData_3.0-5       png_0.1-8           rstudioapi_0.17.1  \n [82] tzdb_0.4.0          reshape2_1.4.4      uuid_1.2-1         \n [85] coda_0.19-4.1       checkmate_2.3.2     nlme_3.1-166       \n [88] nloptr_2.1.1        repr_1.1.7          zoo_1.8-12         \n [91] parallel_4.4.2      miniUI_0.1.1.1      foreign_0.8-87     \n [94] pillar_1.9.0        grid_4.4.2          vctrs_0.6.5        \n [97] promises_1.3.0      car_3.1-3           OpenMx_2.21.13     \n[100] xtable_1.8-4        cluster_2.1.6       htmlTable_2.4.3    \n[103] evaluate_1.0.1      pbivnorm_0.6.0      mvtnorm_1.3-2      \n[106] cli_3.6.3           kutils_1.73         compiler_4.4.2     \n[109] rlang_1.1.4         crayon_1.5.3        ggsignif_0.6.4     \n[112] fdrtool_1.2.18      plyr_1.8.9          stringi_1.8.4      \n[115] munsell_0.5.1       lisrelToR_0.3       pacman_0.5.1       \n[118] Matrix_1.7-1        IRkernel_1.3.2      hms_1.1.3          \n[121] glasso_1.11         shiny_1.9.1         igraph_2.1.1       \n[124] broom_1.0.7         RcppParallel_5.1.9  cherryblossom_0.1.0\n\n\n\n\n\n\nBonifay, W., Winter, S. D., Skoblow, H. F., & Watts, A. L. (2024). Good fit is weak evidence of replication: increasing rigor through prior predictive similarity checking. Assessment, 10731911241234118.\n\n\nRoberts, S., & Pashler, H. (2000). How persuasive is a good fit? A comment on theory testing. Psychological Review, 107(2), 358–367.\n\n\nWaller, N. G., & Meehl, P. E. (2002). Risky tests, verisimilitude, and path analysis. Psychological Methods, 7(3), 323–337. https://doi.org/10.1037/1082-989X.7.3.323",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Prior Predictive Model Checking</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_logistic_regr.html",
    "href": "chapters/mokken/01_logistic_regr.html",
    "title": "63  Modello di Regressione Logistica",
    "section": "",
    "text": "63.1 Introduzione\nPrima di affrontare i modelli parametrici e non parametrici della Teoria della Risposta all’Item (IRT), è fondamentale acquisire una solida comprensione del modello di regressione logistica. Questo modello, ampiamente utilizzato per l’analisi di dati categorici, rappresenta un punto di riferimento cruciale per comprendere i principi che i modelli IRT sviluppano ed estendono.\nLa regressione logistica consente di stimare la probabilità di un evento in funzione di una o più variabili predittive, fornendo una base teorica utile per interpretare la probabilità che un esaminando risponda correttamente a un item. Tuttavia, mentre il modello di regressione logistica si limita a considerare la relazione tra le variabili predittive e l’evento, i modelli IRT aggiungono un livello di complessità: integrano l’abilità individuale dell’esaminando e le caratteristiche specifiche degli item, permettendo un’analisi più articolata.\nNonostante alcune similitudini, i modelli IRT si differenziano profondamente dalla regressione logistica. Essi non solo modellano simultaneamente le proprietà degli item e le abilità degli individui, ma considerano anche le interdipendenze tra le risposte agli item e tra gli item stessi, superando l’assunzione di indipendenza tra le osservazioni tipica della regressione logistica.\nIn questo capitolo, esploreremo il modello di regressione logistica come punto di partenza per comprendere le basi teoriche e applicative dei modelli IRT, fornendo una transizione chiara e graduale verso l’approfondimento di questa teoria fondamentale.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_logistic_regr.html#modello-di-regressione-logistica-per-variabili-binarie",
    "href": "chapters/mokken/01_logistic_regr.html#modello-di-regressione-logistica-per-variabili-binarie",
    "title": "63  Modello di Regressione Logistica",
    "section": "\n63.2 Modello di Regressione Logistica per Variabili Binarie",
    "text": "63.2 Modello di Regressione Logistica per Variabili Binarie\nLa regressione logistica è uno strumento utile per analizzare la relazione tra una variabile dipendente dicotomica (che assume due valori, ad esempio “successo” e “fallimento”) e una o più variabili indipendenti (che possono essere sia quantitative che qualitative). È particolarmente utile per modellare situazioni in cui vogliamo stimare la probabilità che un evento si verifichi in base alle caratteristiche di un individuo o di una situazione.\n\n63.2.1 Definizione del Modello\nConsideriamo \\(n\\) osservazioni indipendenti, dove \\(Y_i\\) rappresenta l’osservazione \\(i\\)-esima della variabile dipendente (ad esempio, “successo” o “fallimento”), con \\(i = 1, \\dots, n\\). Ogni osservazione è associata a un insieme di variabili esplicative \\((x_{1}, \\dots, x_{p})\\). L’obiettivo è stimare la probabilità di successo, denotata con \\(\\pi_i\\), data la combinazione delle variabili esplicative:\n\\[\nP(Y = 1 \\mid X = x_i) = \\pi_i.\n\\]\nIn questo contesto, \\(Y\\) segue una distribuzione di Bernoulli, quindi può assumere solo due valori:\n\\[\ny_i =\n\\begin{cases}\n1 & \\text{se si verifica un successo (osservazione $i$)}, \\\\\n0 & \\text{se si verifica un fallimento.}\n\\end{cases}\n\\]\nLe probabilità associate sono:\n\n\n\\(\\pi_i\\) per il successo,\n\n\\(1 - \\pi_i\\) per il fallimento.\n\nLa media condizionata \\(\\mathbb{E}(Y \\mid X = x)\\) rappresenta la probabilità attesa di successo per un dato valore \\(x\\) delle variabili esplicative:\n\\[\n\\mathbb{E}(Y \\mid X = x) = \\pi_i.\n\\]\n\n63.2.2 Esempio Pratico\nImmaginiamo un dataset con 100 soggetti, in cui:\n\n\nage è una variabile esplicativa che indica l’età,\n\nchd è la variabile dipendente che indica la presenza (chd = 1) o l’assenza (chd = 0) di disturbi cardiaci.\n\nLa probabilità condizionata \\(\\pi_i\\) indica la probabilità di osservare disturbi cardiaci in un certo gruppo d’età:\n\\[\n\\pi_i = P(Y = 1 \\mid X = x).\n\\]\nPer valori discreti della variabile age, possiamo calcolare la proporzione di individui con \\(Y = 1\\) (cioè con disturbi cardiaci) per ogni gruppo di età. Queste proporzioni rappresentano una stima non parametrica della funzione di regressione tra chd e age.\n\n63.2.3 Visualizzazione dei Dati\nCon il dataset, possiamo calcolare queste proporzioni e rappresentarle graficamente:\n\nchdage &lt;- rio::import(\n    here::here(\"data\", \"logistic_reg\", \"chdage_dat.txt\")\n)\n\n# Calcolo delle proporzioni di Y = 1 (chd) per età\nprop_data &lt;- chdage %&gt;%\n    group_by(age) %&gt;%\n    summarise(prop_chd = mean(chd))\n\n# Creazione del grafico con smoothing\nggplot(prop_data, aes(x = age, y = prop_chd)) +\n    geom_point() + # Punti proporzione\n    geom_smooth(method = \"loess\", span = 0.7) + # Smoothing LOESS\n    labs(\n        title = \"Relazione tra Età e Disturbi Cardiaci\",\n        x = \"Età\",\n        y = \"Proporzione di CHD = 1\"\n    )\n\n\n\n\n\n\n\n\n63.2.4 Interpretazione della Relazione\nOsservando il grafico:\n\nPer valori bassi di age, la proporzione di Y = 1 (presenza di disturbi cardiaci) è vicina a 0.\nPer valori elevati di age, la proporzione di Y = 1 tende a 1.\nPer valori intermedi di age, la proporzione aumenta gradualmente, seguendo un andamento sigmoidale.\n\nQuesto andamento riflette la natura probabilistica del fenomeno: la probabilità di disturbi cardiaci cresce con l’età, ma non è una crescita lineare.\n\n63.2.5 Vantaggio del Modello Logistico\nSebbene la stima non parametrica (ad esempio LOESS) possa fornire un quadro generale, la regressione logistica permette di modellare questa relazione utilizzando una semplice funzione. Questo approccio è particolarmente vantaggioso quando ci sono più variabili esplicative, consentendo di quantificare come ciascuna contribuisce alla probabilità di un evento.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_logistic_regr.html#modello-lineare-nelle-probabilità",
    "href": "chapters/mokken/01_logistic_regr.html#modello-lineare-nelle-probabilità",
    "title": "63  Modello di Regressione Logistica",
    "section": "\n63.3 Modello Lineare nelle Probabilità",
    "text": "63.3 Modello Lineare nelle Probabilità\nIl modello lineare nelle probabilità rappresenta un primo approccio per descrivere la relazione tra una variabile dipendente binaria (ad esempio “successo” o “fallimento”) e una variabile indipendente continua o categoriale. La sua formulazione è data da:\n\\[\nY_i = \\alpha + \\beta X_i + \\varepsilon_i,\n\\]\ndove:\n\n\n\\(\\alpha\\) è l’intercetta,\n\n\\(\\beta\\) è il coefficiente di regressione,\n\n\\(\\varepsilon_i\\) rappresenta l’errore, che si assume distribuito normalmente con media 0 e varianza costante (\\(\\varepsilon_i \\sim \\mathcal{N}(0, 1)\\)).\n\nIl valore atteso di \\(Y_i\\) è quindi\n\\[\\mathbb{E}(Y_i) = \\alpha + \\beta X_i,\\]\nche porta alla stima della probabilità di successo \\(\\pi_i\\) come:\n\\[\n\\pi_i = \\alpha + \\beta X_i.\n\\]\n\n63.3.1 Limiti del Modello Lineare nelle Probabilità\nNonostante la semplicità, il modello lineare nelle probabilità presenta alcune problematiche.\n\nValori predetti fuori dall’intervallo [0,1]. Poiché \\(\\pi_i = \\alpha + \\beta X_i\\) è una funzione lineare, i valori predetti di \\(\\pi_i\\) possono essere negativi o superiori a 1, il che è incompatibile con l’interpretazione di \\(\\pi_i\\) come probabilità.\nAssunzione di normalità degli errori. La variabile dipendente \\(Y_i\\) è binaria (0 o 1), ma l’errore \\(\\varepsilon_i\\) non segue una distribuzione normale. Ad esempio, se \\(Y_i = 1\\), l’errore sarà:\n\n\\[\n\\varepsilon_i = 1 - \\mathbb{E}(Y_i) = 1 - (\\alpha + \\beta X_i) = 1 - \\pi_i.\n\\]\nAnalogamente, se \\(Y_i = 0\\), l’errore sarà:\n\\[\n\\varepsilon_i = 0 - \\mathbb{E}(Y_i) = 0 - (\\alpha + \\beta X_i) = - \\pi_i.\n\\]\nPertanto, gli errori sono dicotomici e non normali.\n\nProblemi di omoschedasticità.\n\nNel modello lineare nelle probabilità, la varianza degli errori dipende dalla media \\(\\pi_i\\), quindi non è costante. La varianza degli errori si calcola come:\n\\[\n\\mathbb{V}(\\varepsilon_i) = (1-\\pi_i)\\pi_i.\n\\] dove \\(\\pi_i\\) varia in funzione di \\(X_i\\). Dato che \\(\\pi_i\\) dipende da \\(x\\), ciò significa che la varianza non è costante in funzione di \\(x\\). Questa eteroschedasticità viola una delle assunzioni fondamentali del metodo dei minimi quadrati.\n\n\nLinearità irrealistica. La relazione tra \\(X_i\\) e la probabilità di successo non è sempre lineare nella realtà. Ad esempio, per valori estremi di \\(X_i\\), una relazione lineare può portare a predizioni improbabili (valori negativi o superiori a 1) e non cattura l’andamento sigmoidale tipico di molti fenomeni.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_logistic_regr.html#modello-lineare-nelle-probabilità-vincolato",
    "href": "chapters/mokken/01_logistic_regr.html#modello-lineare-nelle-probabilità-vincolato",
    "title": "63  Modello di Regressione Logistica",
    "section": "\n63.4 Modello Lineare nelle Probabilità Vincolato",
    "text": "63.4 Modello Lineare nelle Probabilità Vincolato\nUn tentativo di risolvere il problema dei valori predetti fuori dall’intervallo [0,1] consiste nell’introdurre vincoli:\n\\[\n\\pi=\n\\begin{cases}\n  0                           &\\text{se $\\alpha + \\beta X &lt; 0$},\\\\\n  \\alpha + \\beta X           &\\text{se $0 \\leq \\alpha + \\beta X \\leq 1$},\\\\\n  1 &\\text{se $\\alpha + \\beta X &gt; 1$}.\n\\end{cases}\n\\]\nTuttavia, questo approccio presenta diversi limiti:\n\n\nDipendenza critica dai valori estremi di \\(\\pi\\): I valori di \\(\\pi = 0\\) e \\(\\pi = 1\\) dipendono fortemente dai valori più bassi e più alti di \\(X_i\\), che possono variare tra campioni.\n\nCambiamenti bruschi nella pendenza: La curva di regressione subisce variazioni improvvise vicino agli estremi, risultando poco realistica.\n\nComplicazioni con più variabili esplicative: Quando il numero di variabili indipendenti aumenta, il modello diventa instabile e difficile da interpretare.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_logistic_regr.html#regressione-logistica",
    "href": "chapters/mokken/01_logistic_regr.html#regressione-logistica",
    "title": "63  Modello di Regressione Logistica",
    "section": "\n63.5 Regressione Logistica",
    "text": "63.5 Regressione Logistica\nLa regressione logistica offre una soluzione efficace per modellare probabilità, garantendo che i valori previsti siano sempre compresi nell’intervallo \\([0,1]\\). Invece di specificare un modello direttamente per le probabilità condizionate \\(\\pi_i\\), si definisce un modello lineare per una loro trasformazione: il logaritmo degli odds, noto come logit. Questa trasformazione risolve il problema del vincolo imposto dall’intervallo delle probabilità.\n\n63.5.1 Modello Logistico e Logit\nIl logit, definito come il logaritmo naturale del rapporto tra probabilità di successo e probabilità di fallimento, è dato da:\n\\[\n\\eta_i = \\log_e \\frac{\\pi_i}{1-\\pi_i} = \\alpha + \\beta x_i,\n\\]\ndove:\n\n\n\\(\\eta_i\\) è il logit, sempre un numero reale,\n\n\\(\\alpha\\) e \\(\\beta\\) sono i parametri del modello,\n\n\\(x_i\\) è la variabile esplicativa.\n\nQuesto approccio consente di modellare \\(\\pi_i\\) come una funzione non lineare di \\(x_i\\), espressa dalla funzione logistica:\n\\[\n\\pi_i = \\frac{1}{1 + e^{-(\\alpha + \\beta x_i)}}.\n\\]\n\n63.5.2 Caratteristiche del Modello Logistico\nIl modello logistico presenta i seguenti vantaggi:\n\n\nIntervallo limitato: garantisce che \\(\\pi_i\\) sia sempre compreso tra 0 e 1.\n\nRelazione sigmoidale: rappresenta una transizione fluida tra probabilità basse e alte in funzione di \\(x_i\\).\n\nAdatto a variabili dicotomiche: rispetta la natura della variabile dipendente.\n\n63.5.3 Relazione tra Probabilità, Odds e Logit\nLa relazione tra probabilità (\\(P\\)), odds (\\(O\\)) e logit (\\(L\\)) è illustrata nella tabella seguente:\n\n\n\n\n\n\n\nProbabilità (\\(P\\))\nOdds (\\(O = P / (1-P)\\))\nLogit (\\(L = \\ln(O)\\))\n\n\n\n0.01\n0.01 / 0.99 = 0.0101\n\\(-4.60\\)\n\n\n0.50\n0.50 / 0.50 = 1.0000\n\\(0.00\\)\n\n\n0.99\n0.99 / 0.01 = 99.0000\n\\(4.60\\)\n\n\n\nIl logit trasforma l’intervallo \\([0,1]\\) della probabilità in tutta la linea reale, semplificando l’uso di modelli lineari.\n\n63.5.4 Logit Empirici e Relazione Lineare\nPer visualizzare la relazione tra variabili trasformate, è possibile calcolare i logit empirici. Consideriamo un esempio con 8 intervalli della variabile age, calcolando il logit degli odds per ciascun gruppo. La relazione risultante è lineare, come mostrato dal seguente codice:\n\ndat1 &lt;- chdage %&gt;%\n    mutate(age_c = ntile(age, 8)) %&gt;%\n    group_by(age_c) %&gt;%\n    summarise(\n        age_bin_center = (min(age) + max(age)) / 2,\n        proportion_heart_disease = mean(chd)\n    )\n\nxc &lt;- dat1$age_bin_center\nyc &lt;- dat1$proportion_heart_disease\nlogit_y &lt;- log(yc / (1 - yc))\nfit &lt;- lm(logit_y ~ xc)\n\nplot(\n    xc, logit_y,\n    xlab = \"Età\", ylab = \"Logit(Y)\",\n    main = \"Relazione Lineare tra Logit e Età\", type = \"n\"\n)\npoints(xc, logit_y, cex = 2)\nabline(fit)\n\n\n\n\n\n\n\n\n63.5.5 Modello Logistico Applicato\nUtilizzando un modello logistico, possiamo rappresentare l’andamento sigmoidale della probabilità condizionata:\n\\[\n\\pi_i = \\frac{e^{\\alpha + \\beta x_i}}{1 + e^{\\alpha + \\beta x_i}}.\n\\]\nIn R, il modello può essere stimato come segue:\n\nfm &lt;- glm(chd ~ age, family = binomial(link = \"logit\"), data = chdage)\nlogit_hat &lt;- fm$coef[1] + fm$coef[2] * chdage$age\npi_hat &lt;- exp(logit_hat) / (1 + exp(logit_hat))\n\nplot(chdage$age, pi_hat,\n    xlab = \"Età\",\n    ylab = \"P(CHD)\",\n    main = \"Probabilità di Malattia Cardiaca\", type = \"n\"\n)\nlines(chdage$age, pi_hat)\npoints(dat1$age_bin_center, dat1$proportion_heart_disease, cex = 2)\n\n\n\n\n\n\n\nUn’alternativa per visualizzare i risultati è l’uso del pacchetto sjPlot:\n\nplot_model(fm, type = \"pred\", terms = \"age\") +\n  labs(y = \"Probabilità di Malattia Cardiaca\")\n\n\n\n\n\n\n\nIn conclusione, la regressione logistica rappresenta un metodo robusto e flessibile per modellare probabilità, superando i limiti del modello lineare nelle probabilità. La sua capacità di rappresentare relazioni non lineari e rispettare i vincoli probabilistici la rende ideale per l’analisi di variabili dipendenti binarie.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_logistic_regr.html#modelli-lineari-generalizzati-glm",
    "href": "chapters/mokken/01_logistic_regr.html#modelli-lineari-generalizzati-glm",
    "title": "63  Modello di Regressione Logistica",
    "section": "\n63.6 Modelli Lineari Generalizzati (GLM)",
    "text": "63.6 Modelli Lineari Generalizzati (GLM)\nI Modelli Lineari Generalizzati (GLM) rappresentano una potente estensione del modello lineare classico, progettata per affrontare i limiti che emergono con variabili risposta di natura non continua o con varianze non costanti. Nel caso di una variabile risposta binaria, come nel modello di regressione logistica, il modello lineare classico incontra diverse difficoltà:\n\n\nDistribuzione Binomiale: La variabile risposta \\(Y_i\\) segue una distribuzione binomiale (con parametro \\(n_i\\), tipicamente \\(n_i = 1\\) per dati individuali), incompatibile con l’assunzione di normalità.\n\nVincoli sulle Probabilità: Specificare un modello lineare come \\(\\pi_i = \\beta_0 + \\beta_1 x_i\\) può portare a stime di probabilità fuori dall’intervallo \\([0,1]\\).\n\nVarianze Non Costanti: La varianza dei residui, calcolata come \\(V(\\varepsilon_i) = \\pi_i (1 - \\pi_i)\\), varia in funzione di \\(\\pi_i\\).\n\nI GLM affrontano queste sfide consentendo di specificare una relazione tra la media attesa della variabile risposta e le variabili esplicative attraverso una funzione di legame. Questi modelli includono varianti come:\n\n\nRegressione Lineare: Per variabili dipendenti continue.\n\nRegressione Logistica: Per variabili risposta binarie.\n\nModello Loglineare di Poisson: Per conteggi o frequenze in tabelle di contingenza.\n\n\n63.6.1 Struttura dei GLM\nUn GLM si compone di tre elementi principali:\n\n\nComponente Aleatoria: Specifica la distribuzione della variabile risposta \\(Y_i\\), ad esempio:\n\nNormale per variabili continue,\nBinomiale per variabili binarie,\nPoisson per conteggi.\n\n\n\nComponente Sistematica: Definisce la relazione lineare tra le variabili esplicative e una trasformazione della media attesa della variabile risposta. È rappresentata dal predittore lineare:\n\\[\n\\eta_i = \\alpha + \\sum_{j} \\beta_j X_{ij}.\n\\]\n\nFunzione di Legame: Trasforma la media attesa \\(\\mathbb{E}(Y_i)\\) in modo che sia modellata linearmente rispetto a \\(\\eta_i\\). Ad esempio, nella regressione logistica, il legame è dato dal logit.\n\n\n\n\n\n\n\n\nComponente Aleatoria\nFunzione di Legame\nApplicazione\n\n\n\nGaussiana\nIdentità\nRegressione lineare\n\n\nBinomiale\nLogit\nRegressione logistica\n\n\nPoisson\nLogaritmo\nModello loglineare\n\n\n\n63.6.2 Componente Sistematica\nLa componente sistematica descrive come le variabili esplicative (\\(X_{ij}\\)) influenzano il predittore lineare \\(\\eta_i\\). Per \\(k\\) osservazioni e \\(p\\) variabili esplicative, il predittore lineare è definito come:\n\\[\n\\eta_i = \\alpha + \\sum_{j} \\beta_j X_{ij},\n\\]\ndove:\n\n\n\\(\\alpha\\) è l’intercetta,\n\n\\(\\beta_j\\) sono i coefficienti delle variabili esplicative.\n\n63.6.3 Componente Aleatoria\nLa componente aleatoria assume che le osservazioni \\(Y_i\\) siano realizzazioni indipendenti di una variabile casuale. Per una variabile risposta binaria:\n\\[\nY_i \\sim \\text{Bin}(n_i, \\pi_i),\n\\]\ndove \\(n_i = 1\\) per dati individuali.\n\n63.6.4 Funzione di Legame\nLa funzione di legame \\(g(\\cdot)\\) connette la media attesa \\(\\mathbb{E}(Y_i) = \\pi_i\\) alla componente sistematica \\(\\eta_i\\). Per la regressione logistica, il legame è dato dal logit:\n\\[\n\\eta_i = g(\\pi_i) = \\ln\\left(\\frac{\\pi_i}{1-\\pi_i}\\right).\n\\]\nLa funzione legame è invertibile, consentendo di esprimere la probabilità \\(\\pi_i\\) come funzione del predittore lineare:\n\\[\n\\pi_i = \\frac{e^{\\eta_i}}{1 + e^{\\eta_i}} = \\frac{e^{\\alpha + \\sum_j \\beta_j X_{ij}}}{1 + e^{\\alpha + \\sum_j \\beta_j X_{ij}}}.\n\\]\nQuesta relazione permette di ottenere un modello non lineare per le probabilità \\(\\pi_i\\).\n\n63.6.5 Visualizzazione della Funzione Logistica\nLa funzione logistica, che rappresenta il legame tra il predittore lineare \\(\\eta_i\\) e la probabilità \\(\\pi_i\\), ha un andamento sigmoidale:\n\nx &lt;- seq(-5, 5, length.out = 100)\nprob &lt;- plogis(x)  # Inversa del logit\nplot(x, prob, type = \"l\", \n     main = \"Funzione Logistica\", \n     ylab = \"Probabilità (\\u03c0)\", \n     xlab = \"Valori di \\u03b7\")\n\n\n\n\n\n\n\n\n63.6.6 Applicazioni dei GLM\nI GLM sono particolarmente utili in contesti in cui:\n\nLa variabile risposta non è continua (es. binaria o discreta),\nLe varianze non sono costanti,\nLa relazione tra media e predittore è non lineare.\n\nNella regressione logistica, la combinazione di queste componenti consente di descrivere in modo accurato la probabilità di successo in funzione delle variabili esplicative.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_logistic_regr.html#regressione-logistica-con-r",
    "href": "chapters/mokken/01_logistic_regr.html#regressione-logistica-con-r",
    "title": "63  Modello di Regressione Logistica",
    "section": "\n63.7 Regressione Logistica con R",
    "text": "63.7 Regressione Logistica con R\nLa regressione logistica può essere implementata in R utilizzando la funzione glm() (Generalized Linear Model). Questo metodo consente di stimare i parametri del modello, tenendo conto della distribuzione della variabile risposta e della funzione di legame appropriata.\nPer stimare i parametri del modello sui dati dell’esempio, si utilizza il seguente codice:\n\nfm &lt;- glm(chd ~ age,\n    family = binomial(link = \"logit\"),\n    data = chdage\n)\n\n\n\nfamily = binomial: Specifica che la variabile risposta segue una distribuzione binomiale (necessaria per una variabile binaria come chd).\n\nlink = \"logit\": Indica che la funzione di legame utilizzata è il logit.\n\nL’output del modello può essere visualizzato con la funzione summary():\n\nsummary(fm)\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = chd ~ age, family = binomial(link = \"logit\"), data = chdage)\n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)\n#&gt; (Intercept)  -5.3095     1.1337   -4.68  2.8e-06\n#&gt; age           0.1109     0.0241    4.61  4.0e-06\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 136.66  on 99  degrees of freedom\n#&gt; Residual deviance: 107.35  on 98  degrees of freedom\n#&gt; AIC: 111.4\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\nL’output mostra i coefficienti stimati (\\(\\alpha\\) e \\(\\beta\\)), i loro errori standard e altre statistiche. Per i dati dell’esempio, i risultati principali sono:\n\nIntercetta (\\(\\alpha = -5.309\\)): Indica il log-odds di sviluppare CHD a età 0.\nCoefficiente di Età (\\(\\beta = 0.111\\)): Indica come i log-odds di CHD cambiano per ogni anno aggiuntivo di età.\n\nLe equazioni risultanti sono:\n\nLogit stimato:\\[\n\\hat{\\eta}(x) = -5.309 + 0.111 \\cdot \\text{age}.\n\\]\nProbabilità stimata:\\[\n\\hat{\\pi}(x) = \\frac{e^{-5.309 + 0.111 \\cdot \\text{age}}}{1 + e^{-5.309 + 0.111 \\cdot \\text{age}}}.\n\\]\n\n\n63.7.1 Interpretazione dei Coefficienti\nLa comprensione dei coefficienti del modello di regressione logistica può essere suddivisa in tre livelli: log-odds, odds ratio e probabilità predette.\n\n63.7.2 Interpretazione Basata sui Log-Odds\n\n\nIntercetta (\\(-5.309\\)):\n\nRappresenta i log-odds di sviluppare CHD quando l’età è 0.\nUn valore negativo suggerisce che la probabilità di CHD è molto bassa a età 0.\n\n\n\nCoefficiente di Età (\\(0.111\\)):\n\nOgni anno aggiuntivo di età aumenta i log-odds di CHD di 0.111.\n\nUn coefficiente positivo indica che il rischio di CHD aumenta con l’età.\n\n\n\n63.7.3 Interpretazione Attraverso l’Odds Ratio\nPer una comprensione più intuitiva, il coefficiente di età può essere trasformato in un odds ratio esponenziando il valore del coefficiente:\n\\[\n\\text{Odds Ratio per Età} = e^{0.111} \\approx 1.12.\n\\]\n\n\nSignificato:\n\nUn odds ratio di 1.12 implica che per ogni anno di età in più, gli odds di sviluppare CHD aumentano del 12%.\nSe l’odds ratio fosse pari a 1, ciò indicherebbe che l’età non influisce sul rischio di CHD.\n\n\n\n63.7.4 Interpretazione Basata sulle Probabilità Predette\nIl modo più diretto per interpretare l’impatto delle variabili esplicative è attraverso le probabilità predette. Le probabilità mostrano come il rischio di CHD varia con l’età.\nPossiamo calcolare e visualizzare le probabilità predette per diverse età utilizzando il pacchetto jtools con la funzione effect_plot():\n\neffect_plot(fm,\n    pred = age, interval = TRUE, plot.points = TRUE,\n    jitter = 0.05\n)\n\n\n\n\n\n\n\n\n\npred = age: Indica che vogliamo calcolare le probabilità predette in funzione di age.\n\ninterval = TRUE: Aggiunge intervalli di confidenza per le stime.\n\nplot.points = TRUE: Mostra i punti osservati sui dati originali.\n\nQuesto grafico rappresenta la relazione sigmoidale tra età e probabilità di CHD, fornendo una rappresentazione intuitiva e accessibile anche a chi non ha una formazione avanzata in statistica.\nIn conclusione, la regressione logistica in R, tramite glm(), è uno strumento versatile per analizzare variabili binarie. L’interpretazione dei coefficienti attraverso log-odds, odds ratio e probabilità predette offre molteplici prospettive utili per comprendere l’effetto delle variabili esplicative e comunicare i risultati in modo chiaro ed efficace.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_logistic_regr.html#riflessioni-conclusive",
    "href": "chapters/mokken/01_logistic_regr.html#riflessioni-conclusive",
    "title": "63  Modello di Regressione Logistica",
    "section": "\n63.8 Riflessioni Conclusive",
    "text": "63.8 Riflessioni Conclusive\nNel caso di una variabile dipendente binaria \\(Y_i\\), il tradizionale modello di regressione lineare risulta inadatto, principalmente a causa della natura discreta di \\(Y_i\\), della varianza non costante e della necessità di vincolare i valori predetti all’intervallo \\([0, 1]\\). Queste limitazioni vengono superate applicando un modello lineare non direttamente alla probabilità \\(\\pi_i\\) (il valore atteso di \\(Y_i\\)), ma a una sua trasformazione: il logit.\nNel modello di regressione logistica, la componente sistematica esprime i logit, definiti come il logaritmo naturale degli odds, come una funzione lineare dei predittori:\n\\[\n\\ln \\frac{\\pi_i}{1-\\pi_i} = \\alpha + \\beta X_i.\n\\]\nQuesto rende il modello lineare nei logit, semplificando la relazione tra variabili esplicative e odds. La funzione logit è invertibile, e la trasformazione inversa (antilogit) consente di esprimere le probabilità \\(\\pi_i\\) in funzione del predittore lineare \\(\\eta_i = \\alpha + \\beta X_i\\):\n\\[\n\\pi_i = \\frac{\\exp(\\alpha + \\beta X_i)}{1 + \\exp(\\alpha + \\beta X_i)}.\n\\]\nQuesta relazione rende il modello non lineare rispetto alle probabilità, ma garantisce che i valori predetti rimangano nell’intervallo \\([0, 1]\\).\nNel contesto della regressione logistica, il valore atteso della variabile dipendente \\(Y_i\\), condizionato ai valori dei predittori, rappresenta la probabilità che \\(Y_i\\) assuma il valore 1:\n\\[\n\\mathbb{E}(Y \\mid x_i) = Pr(Y = 1 \\mid X = x_i) \\equiv \\pi_i.\n\\]\nQuesto valore può essere interpretato come la proporzione di individui nella popolazione con \\(Y = 1\\) per una data combinazione di valori \\(X = x_i\\).\nLa componente aleatoria del modello considera \\(Y_i\\) come una variabile aleatoria binomiale, con due scenari principali:\n\n\nDati raggruppati: Quando le osservazioni sono aggregate, la variabile risposta segue una distribuzione binomiale con parametro \\(n_i\\), dove \\(n_i\\) rappresenta il numero di osservazioni per ogni gruppo omogeneo di predittori.\n\nDati individuali: Quando ogni osservazione è indipendente, \\(n_i = 1\\) per tutte le unità.\n\nLa funzione logistica:\n\\[\n\\Lambda(\\eta) = \\frac{\\exp(\\eta)}{1 + \\exp(\\eta)},\n\\]\nè stata scelta come funzione di legame per trasformare il predittore lineare \\(\\eta_i = \\alpha + \\beta X_i\\) nelle probabilità \\(\\pi_i\\). Questa funzione è preferita per la sua semplicità interpretativa e per il fatto che produce un andamento sigmoidale, che descrive bene molte relazioni probabilistiche.\nIn conclusione, il modello di regressione logistica risolve elegantemente le limitazioni del modello lineare applicato a variabili binarie, fornendo un approccio flessibile e interpretabile:\n\n\nLineare nei logit: Il modello sfrutta la semplicità di una relazione lineare per descrivere log-odds.\n\nNon lineare nelle probabilità: La funzione logistica garantisce che le probabilità predette siano sempre comprese nell’intervallo \\([0, 1]\\).\n\nAdatto a variabili binarie: La componente aleatoria binomiale riflette la natura discreta della variabile dipendente.\n\nQuesto modello si dimostra particolarmente utile in ambiti dove le variabili risposta sono dicotomiche, offrendo interpretazioni intuitive tramite logit, odds e probabilità. La sua flessibilità consente di essere applicato sia a dati individuali sia a dati raggruppati, rendendolo uno strumento fondamentale per analisi statistiche moderne.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_logistic_regr.html#session-info",
    "href": "chapters/mokken/01_logistic_regr.html#session-info",
    "title": "63  Modello di Regressione Logistica",
    "section": "\n63.9 Session Info",
    "text": "63.9 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.2\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] sjPlot_2.8.17        jtools_2.3.0         effects_4.2-2       \n#&gt;  [4] gmodels_2.19.1       LaplacesDemon_16.1.6 car_3.1-3           \n#&gt;  [7] carData_3.0-5        ggokabeito_0.1.0     see_0.11.0          \n#&gt; [10] MASS_7.3-65          viridis_0.6.5        viridisLite_0.4.2   \n#&gt; [13] ggpubr_0.6.0         ggExtra_0.10.1       gridExtra_2.3       \n#&gt; [16] patchwork_1.3.0      bayesplot_1.11.1     semTools_0.5-6      \n#&gt; [19] semPlot_1.1.6        lavaan_0.6-19        psych_2.4.12        \n#&gt; [22] scales_1.3.0         markdown_1.13        knitr_1.50          \n#&gt; [25] lubridate_1.9.4      forcats_1.0.0        stringr_1.5.1       \n#&gt; [28] dplyr_1.1.4          purrr_1.0.4          readr_2.1.5         \n#&gt; [31] tidyr_1.3.1          tibble_3.2.1         ggplot2_3.5.1       \n#&gt; [34] tidyverse_2.0.0      here_1.0.1          \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] splines_4.4.2       later_1.4.1         R.oo_1.27.0        \n#&gt;   [4] datawizard_1.0.1    XML_3.99-0.18       rpart_4.1.24       \n#&gt;   [7] lifecycle_1.0.4     Rdpack_2.6.3        rstatix_0.7.2      \n#&gt;  [10] rprojroot_2.0.4     globals_0.16.3      lattice_0.22-6     \n#&gt;  [13] insight_1.1.0       rockchalk_1.8.157   backports_1.5.0    \n#&gt;  [16] survey_4.4-2        magrittr_2.0.3      openxlsx_4.2.8     \n#&gt;  [19] Hmisc_5.2-3         rmarkdown_2.29      httpuv_1.6.15      \n#&gt;  [22] qgraph_1.9.8        zip_2.3.2           RColorBrewer_1.1-3 \n#&gt;  [25] pbapply_1.7-2       DBI_1.2.3           minqa_1.2.8        \n#&gt;  [28] multcomp_1.4-28     abind_1.4-8         quadprog_1.5-8     \n#&gt;  [31] R.utils_2.13.0      nnet_7.3-20         TH.data_1.1-3      \n#&gt;  [34] sandwich_3.1-1      listenv_0.9.1       gdata_3.0.1        \n#&gt;  [37] arm_1.14-4          performance_0.13.0  parallelly_1.42.0  \n#&gt;  [40] codetools_0.2-20    tidyselect_1.2.1    ggeffects_2.2.1    \n#&gt;  [43] farver_2.1.2        lme4_1.1-36         broom.mixed_0.2.9.6\n#&gt;  [46] stats4_4.4.2        base64enc_0.1-3     jsonlite_1.9.1     \n#&gt;  [49] Formula_1.2-5       survival_3.8-3      emmeans_1.10.7     \n#&gt;  [52] tools_4.4.2         rio_1.2.3           Rcpp_1.0.14        \n#&gt;  [55] glue_1.8.0          mnormt_2.1.1        mgcv_1.9-1         \n#&gt;  [58] xfun_0.51           withr_3.0.2         fastmap_1.2.0      \n#&gt;  [61] mitools_2.4         boot_1.3-31         digest_0.6.37      \n#&gt;  [64] mi_1.1              timechange_0.3.0    R6_2.6.1           \n#&gt;  [67] mime_0.13           estimability_1.5.1  colorspace_2.1-1   \n#&gt;  [70] gtools_3.9.5        jpeg_0.1-10         R.methodsS3_1.8.2  \n#&gt;  [73] generics_0.1.3      data.table_1.17.0   corpcor_1.6.10     \n#&gt;  [76] htmlwidgets_1.6.4   pkgconfig_2.0.3     sem_3.1-16         \n#&gt;  [79] gtable_0.3.6        furrr_0.3.1         htmltools_0.5.8.1  \n#&gt;  [82] png_0.1-8           snakecase_0.11.1    reformulas_0.4.0   \n#&gt;  [85] rstudioapi_0.17.1   tzdb_0.5.0          reshape2_1.4.4     \n#&gt;  [88] coda_0.19-4.1       checkmate_2.3.2     nlme_3.1-167       \n#&gt;  [91] nloptr_2.2.1        zoo_1.8-13          sjlabelled_1.2.0   \n#&gt;  [94] parallel_4.4.2      miniUI_0.1.1.1      foreign_0.8-88     \n#&gt;  [97] pillar_1.10.1       grid_4.4.2          vctrs_0.6.5        \n#&gt; [100] promises_1.3.2      OpenMx_2.21.13      xtable_1.8-4       \n#&gt; [103] cluster_2.1.8.1     htmlTable_2.4.3     evaluate_1.0.3     \n#&gt; [106] pbivnorm_0.6.0      mvtnorm_1.3-3       cli_3.6.4          \n#&gt; [109] kutils_1.73         compiler_4.4.2      rlang_1.1.5        \n#&gt; [112] ggsignif_0.6.4      labeling_0.4.3      fdrtool_1.2.18     \n#&gt; [115] plyr_1.8.9          sjmisc_2.8.10       stringi_1.8.4      \n#&gt; [118] pander_0.6.6        munsell_0.5.1       lisrelToR_0.3      \n#&gt; [121] pacman_0.5.1        Matrix_1.7-3        sjstats_0.19.0     \n#&gt; [124] hms_1.1.3           glasso_1.11         future_1.34.0      \n#&gt; [127] shiny_1.10.0        haven_2.5.4         rbibutils_2.3      \n#&gt; [130] igraph_2.1.4        broom_1.0.7         RcppParallel_5.1.10",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_core_issues.html",
    "href": "chapters/mokken/02_core_issues.html",
    "title": "65  Analisi della Scala di Mokken",
    "section": "",
    "text": "65.1 Introduzione\nL’Analisi delle Scale di Mokken (MSA), sviluppata dal ricercatore olandese Robert J. Mokken, è un metodo psicometrico utilizzato per verificare se un test misura in modo coerente un costrutto psicologico latente, come l’ansia o l’autostima. Si basa sulla Teoria Non Parametrica della Risposta agli Item (NIRT), una branca della psicometria che analizza le risposte delle persone agli item di un test per dedurre caratteristiche non direttamente osservabili, chiamate costrutti latenti.\nUn costrutto latente, come l’ansia, non può essere osservato direttamente, ma viene stimato attraverso le risposte degli individui agli item di un test. L’idea principale è che queste risposte riflettano la posizione dell’individuo lungo un continuum (ad esempio, da “poco ansioso” a “molto ansioso”). Tuttavia, nella pratica, la relazione tra le risposte agli item e il costrutto latente può essere complessa e non seguire un andamento matematico preciso.\nLa MSA consente di verificare:\nA differenza dei modelli parametrici della Teoria della Risposta agli Item (IRT), la MSA adotta un approccio non parametrico, che offre maggiore flessibilità. Non richiede di assumere una relazione matematica specifica (ad esempio, una curva logistica) tra il costrutto latente e le risposte agli item. Questa caratteristica rende la MSA particolarmente utile quando:\nLa MSA amplia il modello di Guttman, basato sull’idea di perfetta cumulatività: una persona che risponde correttamente a un item più difficile dovrebbe rispondere correttamente anche a tutti gli item più semplici. Sebbene utile per creare scale gerarchiche, questo principio risulta spesso troppo rigido nella pratica, poiché non tiene conto delle variazioni naturali nei dati reali.\nLa MSA introduce un approccio probabilistico per affrontare queste complessità. Consente alcune deviazioni dalla perfetta cumulatività, rendendo il modello più realistico e adattabile a situazioni in cui le risposte ai test sono influenzate da fattori esterni, come la motivazione, la comprensione degli item o la stanchezza.\nUna caratteristica distintiva della MSA è che la forma della relazione tra il livello del tratto latente (\\(\\theta\\)) e la probabilità di rispondere correttamente a un item (nota come funzione di risposta all’item, IRF) non deve seguire una forma matematica specifica, come avviene nei modelli parametrici della Teoria della Risposta agli Item (IRT). Tuttavia, pur non imponendo vincoli rigidi alla forma delle IRF, la MSA introduce requisiti di ordinamento per item e rispondenti, che impongono alcune restrizioni sulle caratteristiche delle risposte agli item. Questi requisiti permettono di mantenere la coerenza nell’interpretazione del costrutto latente, pur lasciando spazio per variazioni nei dati reali.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_core_issues.html#introduzione",
    "href": "chapters/mokken/02_core_issues.html#introduzione",
    "title": "65  Analisi della Scala di Mokken",
    "section": "",
    "text": "Se gli item di un test sono coerenti tra loro e misurano lo stesso costrutto psicologico (concetto di omogeneità).\n\nSe è possibile ordinare sia le persone sia gli item lungo una scala, dalla persona con il punteggio più basso a quella con il punteggio più alto.\n\n\n\ni dati non soddisfano le ipotesi richieste dai modelli parametrici;\n\nsi preferisce un’analisi più semplice e robusta, senza ricorrere a formule matematiche complesse.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_core_issues.html#assunzioni-dellanalisi-delle-scale-di-mokken",
    "href": "chapters/mokken/02_core_issues.html#assunzioni-dellanalisi-delle-scale-di-mokken",
    "title": "65  Analisi della Scala di Mokken",
    "section": "\n65.2 Assunzioni dell’Analisi delle Scale di Mokken",
    "text": "65.2 Assunzioni dell’Analisi delle Scale di Mokken\nLe principali assunzioni della MSA sono descritte di seguito.\n\n65.2.1 Assunzione di Unidimensionalità\nL’assunzione di unidimensionalità richiede che le risposte agli item siano governate da un unico tratto latente, ovvero che tutti gli item della scala misurino lo stesso costrutto psicologico. Questo tratto rappresenta una variabile sottostante non osservabile che determina le risposte ai singoli item. Nella pratica, progettare scale che misurano un unico tratto latente facilita l’interpretazione dei punteggi, riduce la complessità analitica e aumenta la validità delle conclusioni.\n\n65.2.2 Assunzione di Indipendenza Locale\nL’indipendenza locale è un concetto fondamentale nella MSA e nella IRT. In parole semplici, questa assunzione dice che, una volta noto il livello del tratto latente \\(\\theta\\) (ad esempio, un’abilità o un tratto psicologico), le risposte di una persona ai diversi item di un test non devono influenzarsi a vicenda.\nImmagina un test composto da più domande, tutte progettate per misurare lo stesso tratto latente \\(\\theta\\), come la competenza matematica. L’indipendenza locale afferma che:\n\nle risposte alle singole domande dipendono solo dal livello di \\(\\theta\\) della persona;\nle risposte a una domanda non sono condizionate da come la persona ha risposto alle altre domande.\n\nIn pratica, ogni domanda misura esclusivamente \\(\\theta\\) senza essere influenzata dal contesto o da altre risposte.\nQuesta proprietà può essere descritta con una formula:\n\\[\nP(X = x \\mid \\theta) = \\prod_{i=1}^k P(X_i = x_i \\mid \\theta),\n\\]\ndove:\n\n\n\\(P(X = x \\mid \\theta)\\) è la probabilità che una persona risponda in un certo modo a tutte le domande, dato il suo livello di \\(\\theta\\).\n\n\\(P(X_i = x_i \\mid \\theta)\\) è la probabilità di rispondere a una singola domanda \\(i\\), dato il livello di \\(\\theta\\).\n\n\\(\\prod\\) indica che moltiplichiamo insieme le probabilità per tutte le domande del test.\n\nQuesta formula mostra che, dato \\(\\theta\\), le risposte sono indipendenti tra loro.\nL’indipendenza locale ha due implicazioni fondamentali:\n\nLe risposte non sono correlate (dato \\(\\theta\\)):\nUna volta noto il livello di \\(\\theta\\), non esiste relazione tra le risposte a domande diverse. Se osserviamo delle correlazioni, sono dovute esclusivamente alla variabilità di \\(\\theta\\).\n\nLe risposte riflettono solo \\(\\theta\\):\nLe risposte non devono essere influenzate da altri fattori, come:\n\nsimilarità tra i contenuti delle domande;\neffetti dell’ordine delle domande o stanchezza;\nindizi presenti in alcune domande che aiutano a rispondere ad altre.\n\n\n\nSe questa assunzione viene violata, il test potrebbe non rappresentare accuratamente il tratto latente \\(\\theta\\). Alcuni esempi comuni di violazione includono:\n\nDomande troppo simili possono creare correlazioni non legate a \\(\\theta\\). Ad esempio, se due item riguardano frazioni e decimali, la conoscenza di uno potrebbe influenzare l’altro.\nStanchezza, distrazione o l’ordine delle domande potrebbero modificare il modo in cui una persona risponde.\nRispondere a una domanda potrebbe fornire informazioni utili per rispondere a un’altra.\n\nL’indipendenza locale garantisce che il test misuri in modo accurato il tratto latente \\(\\theta\\). Quando questa assunzione è rispettata, possiamo essere certi che le risposte riflettano solo \\(\\theta\\) e non altri fattori esterni. Tuttavia, nella pratica, l’indipendenza locale può essere compromessa. Per questo motivo, è essenziale verificare che sia rispettata, così da assicurare la validità del modello psicometrico (sia in MSA che in IRT).\n\n65.2.3 Assunzione di Monotonicità Latente\nL’assunzione di monotonicità latente garantisce che, all’aumentare del tratto latente (\\(\\theta\\)), la probabilità di rispondere correttamente a un item (per domande dicotomiche) o di scegliere una categoria di risposta più alta (per domande politomiche) aumenti o rimanga costante, senza mai diminuire.\nMatematicamente, si esprime così:\n\\[\nP_i(\\theta_a) \\leq P_i(\\theta_b) \\quad \\text{se} \\quad \\theta_a \\leq \\theta_b ,\n\\]\ndove:\n\n\n\\(P_i(\\theta)\\) è la probabilità di rispondere correttamente o di scegliere una risposta più alta per l’item \\(i\\);\n\n\\(\\theta_a\\) e \\(\\theta_b\\) rappresentano due livelli del tratto latente.\n\nIn altre parole, questa proprietà assicura che individui con livelli più alti di \\(\\theta\\) abbiano una probabilità maggiore o uguale di ottenere punteggi migliori rispetto a quelli con livelli più bassi. Ciò garantisce una relazione coerente tra il tratto latente e le risposte al test, rendendo la scala interpretabile e robusta.\n\n65.2.3.1 Come verificare la monotonicità\nPer valutare la monotonicità si analizza la relazione tra il tratto latente (o una sua stima) e la probabilità di rispondere correttamente a un item. Un approccio comune utilizza il restscore, che rappresenta il punteggio totale di un partecipante sulle risposte agli altri item del test, escludendo l’item che si sta analizzando. Il restscore fornisce una stima indiretta del livello del tratto latente.\n\n65.2.3.2 Esempio di calcolo del restscore\nSupponiamo che un test abbia 10 item e che un partecipante abbia risposto così:\n\n\nItem\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nRisposta\n1\n1\n0\n1\n0\n1\n1\n1\n0\n1\n\n\nSe vogliamo calcolare il restscore per l’item 10, sommiamo le risposte agli altri 9 item:\n\\[\n\\text{Restscore} = 1 + 1 + 0 + 1 + 0 + 1 + 1 + 1 + 0 = 6.\n\\]\nIl restscore indica la prestazione complessiva del partecipante senza considerare l’item analizzato.\n\n65.2.3.3 Verifica della monotonicità con le Funzioni di Risposta agli Item (IRF)\nPer confermare la monotonicità, si costruiscono le Funzioni di Risposta agli Item (IRF), che mostrano graficamente come varia la probabilità di rispondere correttamente a un item al crescere del restscore. Una funzione monotona è non decrescente, cioè la probabilità aumenta o rimane costante.\n\n65.2.3.4 Esempio\nConsideriamo un test con 5 item e i seguenti dati relativi all’item 5:\n\n\n\n\n\n\n\n\nRestscore\nFrequenza Totale\nRisposte Corrette\nProbabilità Correttezza\n\n\n\n0\n5\n1\n\\(1/5 = 0.20\\)\n\n\n1\n10\n3\n\\(3/10 = 0.30\\)\n\n\n2\n15\n6\n\\(6/15 = 0.40\\)\n\n\n3\n10\n5\n\\(5/10 = 0.50\\)\n\n\n4\n5\n4\n\\(4/5 = 0.80\\)\n\n\n\nIn questo caso, la probabilità di risposta corretta aumenta (0.20, 0.30, 0.40, 0.50, 0.80), confermando la monotonicità. Questa relazione può essere visualizzata graficamente:\n\nrestscore &lt;- c(0, 1, 2, 3, 4)\nprob_correct &lt;- c(0.20, 0.30, 0.40, 0.50, 0.80)\n\n# Grafico\nplot(restscore, prob_correct,\n    type = \"b\", pch = 16, col = \"blue\",\n    xlab = \"Restscore\", ylab = \"Probabilità di Risposta Corretta\",\n    main = \"Funzione di Risposta all'Item (IRF)\"\n)\n\n\n\n\n\n\n\nIl grafico dovrebbe mostrare una curva crescente o piatta, segnalando che la monotonicità è rispettata.\n\n65.2.3.5 Aggregazione dei restscore\nSe il numero di partecipanti con un determinato restscore è troppo basso, le probabilità calcolate potrebbero essere instabili. In questi casi, è utile aggregare i restscore in gruppi.\nEsempio:\n\n\n\n\n\n\n\n\nGruppo Restscore\nFrequenza Totale\nRisposte Corrette\nProbabilità Correttezza\n\n\n\n0-2\n30\n10\n\\(10/30 = 0.33\\)\n\n\n3-4\n15\n9\n\\(9/15 = 0.60\\)\n\n\n\nQuesta aggregazione migliora l’affidabilità delle stime mantenendo il focus sulla relazione tra restscore e probabilità.\n\n65.2.3.6 Monotonicità e coefficienti di scalabilità\nNella MSA, la monotonicità viene analizzata anche tramite i coefficienti di scalabilità (\\(H_i\\) per i singoli item e \\(H_{ij}\\) per le coppie di item). Questi coefficienti verificano se gli item rispettano le assunzioni del modello, come l’omogeneità e, indirettamente, la monotonicità.\nNella MSA esistono tre principali tipi di coefficienti di scalabilità:\n\n\n\\(H_i\\): Coefficiente di scalabilità per un singolo item\nMisura quanto un item specifico contribuisce alla scala totale. Indica se le risposte a un dato item sono coerenti con il costrutto latente misurato dalla scala.\n\n\nInterpretazione:\n\n\n\\(H_i &gt; 0.30\\): l’item è considerato accettabile.\n\n\n\\(H_i &gt; 0.40\\): l’item è considerato buono.\n\n\n\\(H_i &gt; 0.50\\): l’item è considerato molto buono.\n\n\n\n\n\\(H_{ij}\\): Coefficiente di scalabilità per una coppia di item\nMisura la relazione tra due item specifici in termini di coerenza con il costrutto latente. Questo coefficiente verifica se le risposte a una coppia di item mostrano una relazione coerente con il tratto latente misurato dalla scala.\n\n\\(H\\): Coefficiente di scalabilità per l’intera scala\nÈ la media dei coefficienti di scalabilità di tutti gli item e misura quanto bene la scala complessiva rappresenta il costrutto latente.\n\n\nInterpretazione:\n\n\n\\(H &gt; 0.30\\): la scala è accettabile.\n\n\n\\(H &gt; 0.40\\): la scala è buona.\n\n\n\\(H &gt; 0.50\\): la scala è molto buona.\n\n\n\n\n\n65.2.3.7 Formula del coefficiente di scalabilità\nIl coefficiente di scalabilità per un item \\(i\\) (\\(H_i\\)) è calcolato come:\n\\[\nH_i = \\frac{\\sum_{j \\neq i} \\text{Cov}(X_i, X_j)}{\\sum_{j \\neq i} \\text{Var}(X_i)},\n\\]\ndove:\n\n\n\\(\\text{Cov}(X_i, X_j)\\) è la covarianza tra le risposte agli item \\(i\\) e \\(j\\);\n\n\\(\\text{Var}(X_i)\\) è la varianza delle risposte all’item \\(i\\);\nla sommatoria \\(\\sum_{j \\neq i}\\) indica che vengono considerati tutti gli item tranne \\(i\\).\n\nIn termini semplici, \\(H_i\\) misura quanto le risposte a un item sono correlate con le risposte agli altri item, rispetto alla variabilità interna dell’item stesso.\n\n65.2.3.8 Relazione tra monotonicità e coefficienti di scalabilità\nI coefficienti di scalabilità verificano l’omogeneità degli item (cioè, se misurano lo stesso costrutto latente), ma non garantiscono automaticamente la monotonicità. Ad esempio:\n\n\n\\(H_i &gt; 0.30\\) indica che un item contribuisce in modo rilevante alla scala, ma non assicura che la probabilità di rispondere correttamente all’item cresca monotonamente con il livello del tratto latente (\\(\\theta\\)).\n\n\nViolazioni della monotonicità possono verificarsi anche in presenza di coefficienti di scalabilità elevati. Pertanto, è necessario analizzare le Funzioni di Risposta agli Item (IRF) per verificare che la probabilità di risposte corrette o di scelte superiori aumenti in modo non decrescente con il tratto latente.\n\n65.2.3.9 Esempio pratico\nSupponiamo di avere una scala con tre item (\\(X_1\\), \\(X_2\\), \\(X_3\\)) e i seguenti coefficienti di scalabilità calcolati:\n\n\nItem\n\\(H_i\\)\nNote\n\n\n\n\\(X_1\\)\n0.45\nL’item contribuisce bene alla scala.\n\n\n\\(X_2\\)\n0.32\nL’item è accettabile.\n\n\n\\(X_3\\)\n0.28\nL’item potrebbe non essere adeguato.\n\n\n\nIn questo caso:\n\n\n\\(X_1\\) e \\(X_2\\) sono sufficientemente scalabili, mentre \\(X_3\\) potrebbe essere problematico.\n\nLa monotonicità deve comunque essere verificata per ciascun item, anche per \\(X_1\\), che ha un valore elevato di \\(H_i\\).\n\nIn conclusione, i coefficienti di scalabilità sono strumenti essenziali per valutare la qualità degli item in una scala, misurando la loro capacità di rappresentare il costrutto latente. Tuttavia, non garantiscono che la monotonicità sia rispettata. Per costruire una scala psicometrica valida e robusta, è necessario combinare l’analisi dei coefficienti di scalabilità con la verifica delle Funzioni di Risposta agli Item (IRF).\n\n65.2.4 Assunzione di Non-Intersezione delle Funzioni di Risposta\nL’assunzione di non-intersezione delle funzioni di risposta (IRF) prevede che le probabilità di successo su item più difficili non superino mai quelle relative a item più facili, per ogni livello del tratto latente. In altre parole, le IRF devono essere ordinate in modo che la probabilità di rispondere correttamente a un item più difficile sia sempre inferiore o uguale rispetto a un item meno difficile. Formalmente, questa proprietà può essere espressa come:\n\\[\nP_1(\\theta) \\leq P_2(\\theta) \\leq ... \\leq P_k(\\theta) \\quad \\text{per ogni} \\ \\theta .\n\\]\nL’intersezione delle IRF comporterebbe una violazione dell’ordinamento degli item, il che renderebbe difficile interpretare i risultati della scala.\nIn sintesi, l’Analisi delle Scale di Mokken si basa su assunzioni chiave simili a quelle dell’IRT, ma le implementa in un contesto non parametrico. La verifica di queste assunzioni garantisce la validità delle scale costruite e la corretta interpretazione dei risultati, rendendo l’MSA uno strumento potente per la costruzione di scale psicometriche robuste.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_core_issues.html#modelli-della-mokken-scale-analysis",
    "href": "chapters/mokken/02_core_issues.html#modelli-della-mokken-scale-analysis",
    "title": "65  Analisi della Scala di Mokken",
    "section": "\n65.3 Modelli della Mokken Scale Analysis",
    "text": "65.3 Modelli della Mokken Scale Analysis\nDalle suddette assunzioni derivano due modelli della MSA:\n\nModello di Monotonicità Omogenea (Mokken, 1971): rispetta le prime tre assunzioni (unidimensionalità, indipendenza locale e monotonicità latente). Questo modello permette di ordinare i rispondenti in base al tratto latente.\nModello di Doppia Monotonicità: rispetta tutte e quattro le assunzioni (unidimensionalità, indipendenza locale, monotonicità latente e non-intersezione). Consente di ordinare non solo i rispondenti, ma anche gli item in termini di difficoltà.\n\n\n65.3.1 Errori Standard nei Coefficienti di Scalabilità\nGli errori standard (SE) sono essenziali per interpretare i coefficienti di scalabilità (\\(H\\), \\(H_j\\), \\(H_{ij}\\)) nell’Analisi delle Scale di Mokken (MSA). Forniscono una misura dell’incertezza associata alle stime e aiutano a valutare quanto il valore stimato rappresenti accuratamente il coefficiente reale nella popolazione.\n\n65.3.1.1 Importanza degli Errori Standard\nUn errore standard elevato rispetto al coefficiente stimato indica maggiore incertezza. Ad esempio, se \\(H_j = 0.30\\) e \\(SE = 0.08\\), il coefficiente stimato potrebbe essere inferiore alla soglia accettabile di 0.30 nella popolazione, sollevando dubbi sulla scalabilità dell’item.\nGrazie agli errori standard, è possibile calcolare intervalli di confidenza (CI) per quantificare la precisione della stima. Per un livello di confidenza del 95%, la formula è:\n\\[\n\\text{95\\% CI} = H_j \\pm (1.96 \\times SE),\n\\]\ndove:\n\n\n\\(H_j\\) è il coefficiente stimato.\n\n\\(1.96\\) è il valore critico associato al 95% di confidenza.\n\n\\(SE\\) è l’errore standard.\n\nL’intervallo di confidenza descrive l’incertezza della stima e indica che, su un gran numero di campioni estratti dalla stessa popolazione, il 95% degli intervalli calcolati includerà il vero valore del coefficiente.\n\n65.3.1.2 Esempio di Calcolo del CI\nSupponiamo che un coefficiente di scalabilità abbia \\(H_j = 0.30\\) e un errore standard di \\(SE = 0.10\\). Il 95% CI è:\n\\[\n\\text{95\\% CI} = 0.30 \\pm (1.96 \\times 0.10) = [0.10, 0.50].\n\\]\nQuesto intervallo mostra che, se ripetessimo il campionamento molte volte, il valore reale di \\(H_j\\) sarebbe compreso tra 0.10 e 0.50 nel 95% dei casi. Tuttavia, un intervallo così ampio riflette una stima relativamente incerta, e la possibilità che \\(H_j\\) sia sotto la soglia accettabile di 0.30 richiede ulteriori considerazioni.\n\n65.3.2 Fattori che Influenzano l’Errore Standard\nDiversi elementi influenzano la dimensione dell’errore standard, tra cui:\n\n\nDimensione del campione:\n\nCampioni più grandi riducono l’errore standard, migliorando la precisione delle stime.\n\nNei campioni piccoli, l’incertezza è maggiore, rendendo difficile interpretare \\(H_j\\) con precisione.\n\n\n\nDistribuzione dei punteggi degli item:\n\nDistribuzioni sbilanciate (ad esempio, molte risposte estreme come tutto 0 o tutto 1) aumentano l’errore standard.\n\nUna distribuzione più uniforme dei punteggi garantisce una stima più stabile.\n\n\n\nEterogeneità degli item:\n\nSe gli item non misurano lo stesso costrutto o presentano deviazioni significative dal modello, i coefficienti di scalabilità diventano meno affidabili e i SE aumentano.\n\n\n\n65.3.3 Utilizzo Pratico degli Errori Standard\nGli errori standard offrono informazioni cruciali per decidere se un item contribuisce in modo adeguato alla scala. Se \\(H_j\\) è basso o il suo intervallo di confidenza include valori inferiori a 0.30, l’item potrebbe non essere idoneo.\n\n65.3.3.1 Interpretazione del CI\n\nSe il CI è interamente sopra 0.30:\nL’item è probabilmente scalabile e contribuisce in modo adeguato alla scala.\nSe il CI include valori sotto 0.30:\nC’è incertezza sulla scalabilità dell’item, e potrebbe essere necessaria una revisione.\nSe il CI è interamente sotto 0.30:\nL’item non è scalabile e dovrebbe essere considerato per la rimozione, a meno che non misuri aspetti unici e rilevanti del costrutto.\n\n65.3.4 Eliminazione degli Item con Bassi Coefficienti di Scalabilità\nMokken (1971) ha sottolineato che item con bassi coefficienti di scalabilità (\\(H_j &lt; 0.30\\)) spesso introducono errori nel modello, riducendo la coerenza della scala. Tuttavia, rimuovere automaticamente questi item può avere conseguenze sia positive sia negative:\n\n\nPro:\n\nMigliora la coerenza interna della scala.\n\nAumenta l’affidabilità psicometrica.\n\n\n\nContro:\n\nRiduce la copertura del costrutto.\n\nPuò eliminare aspetti teoricamente rilevanti del costrutto latente.\n\n\n\nPer evitare problemi, la decisione di eliminare un item dovrebbe considerare non solo i coefficienti di scalabilità e i SE, ma anche la validità teorica dell’item.\nIn conclusione, gli errori standard nei coefficienti di scalabilità sono strumenti essenziali per valutare l’incertezza delle stime e la qualità degli item in una scala psicometrica. Attraverso l’analisi di \\(SE\\), CI e coefficienti di scalabilità, i ricercatori possono prendere decisioni più informate sulla costruzione e sulla revisione delle scale, bilanciando le esigenze psicometriche con quelle teoriche.\n\nEsempio 65.1 Item con basso \\(H_j\\): se \\(H_j = 0.20\\) e il 95% CI è \\([0.05, 0.35]\\), il coefficiente è inferiore alla soglia accettabile di 0.30. Tuttavia, prima di eliminare l’item, si dovrebbe verificare se:\n\nl’item copre un aspetto importante del costrutto;\nla sua rimozione influisce sull’affidabilità della scala.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_core_issues.html#estensione-della-msa-agli-item-politomici",
    "href": "chapters/mokken/02_core_issues.html#estensione-della-msa-agli-item-politomici",
    "title": "65  Analisi della Scala di Mokken",
    "section": "\n65.4 Estensione della MSA agli Item Politomici",
    "text": "65.4 Estensione della MSA agli Item Politomici\nLa MSA, inizialmente sviluppata per item dicotomici (es. risposte “sì/no”), è stata ampliata da Molenaar (1982a, 1997) per includere item politomici, come quelli utilizzati nelle scale Likert. Questa estensione mantiene i principi di base della MSA ma li adatta per gestire la maggiore complessità degli item con più categorie di risposta.\nGli item politomici sono domande che offrono più di due opzioni di risposta ordinate, come una scala Likert a 5 punti (da “fortemente in disaccordo” a “fortemente d’accordo”). A differenza degli item dicotomici, gli item politomici introducono una struttura più articolata, dove le risposte non sono semplicemente giuste o sbagliate, ma riflettono livelli crescenti di accordo, preferenza o intensità.\nNel caso degli item politomici, la MSA non valuta solo l’item nel suo complesso, ma analizza anche i “passaggi” tra categorie di risposta consecutive. Questi passaggi rappresentano le transizioni da una categoria a quella successiva lungo la scala di risposta.\nAd esempio, in un item Likert a 5 punti (“fortemente in disaccordo”, “in disaccordo”, “neutrale”, “d’accordo”, “fortemente d’accordo”), ci sono 4 passaggi distinti:\n\nDa “fortemente in disaccordo” a “in disaccordo o superiore”.\nDa “in disaccordo” a “neutrale o superiore”.\nDa “neutrale” a “d’accordo o superiore”.\nDa “d’accordo” a “fortemente d’accordo”.\n\nOgni passaggio riflette una progressione lungo il continuum latente del tratto psicologico che si sta misurando.\n\n65.4.1 Funzioni di Risposta del Passaggio dell’Item\nPer analizzare i passaggi tra categorie, la MSA utilizza le Funzioni di Risposta del Passaggio dell’Item (Item Step Response Functions, ISRF). Le ISRF descrivono la probabilità che una persona con un certo livello di tratto latente (\\(\\theta\\)) scelga una determinata categoria o una categoria superiore.\n\n65.4.1.1 Caratteristiche delle ISRF:\n\nProbabilità di transizione:\nLe ISRF collegano la probabilità di scegliere una categoria (o una superiore) al livello di \\(\\theta\\) dell’individuo. Ad esempio, per un partecipante con \\(\\theta\\) medio-alto, la probabilità di passare a “d’accordo o superiore” dovrebbe essere maggiore rispetto a un partecipante con \\(\\theta\\) basso.\nContributo di ogni passaggio:\nOgni passaggio tra categorie viene analizzato separatamente, per verificare quanto contribuisce al progresso lungo il continuum latente. Questo permette di identificare eventuali problemi in specifiche transizioni.\n\n65.4.1.2 Requisito di monotonicità per le ISRF\nLe probabilità rappresentate dalle ISRF devono essere monotone crescenti. Ciò significa che, all’aumentare del livello di \\(\\theta\\), la probabilità di scegliere una categoria più alta (o una superiore) deve aumentare o rimanere costante.\n\n65.4.1.3 Esempio di Applicazione\nImmaginiamo un item Likert a 5 punti in una scala che misura l’autostima. Per un individuo con un basso livello di autostima (\\(\\theta\\) basso), le ISRF potrebbero mostrare una probabilità più alta di rimanere nelle categorie più basse (“fortemente in disaccordo”, “in disaccordo”). Al contrario, per un individuo con un alto livello di autostima (\\(\\theta\\) alto), le ISRF indicherebbero una maggiore probabilità di scegliere categorie superiori (“d’accordo”, “fortemente d’accordo”).\n\n65.4.1.4 Vantaggi dell’estensione agli item politomici\n\nMaggiore flessibilità:\nLa MSA può essere applicata a una gamma più ampia di test psicologici, inclusi quelli con risposte più sfumate e dettagliate.\nAnalisi dettagliata:\nEsaminando i passaggi tra categorie, è possibile individuare problemi specifici in alcune transizioni (ad esempio, una mancanza di progressione coerente tra categorie adiacenti).\nAdattamento alle scale ordinarie:\nGli item politomici sono comuni nei test psicologici e sociali; questa estensione rende la MSA più applicabile ai dati reali.\n\nIn conclusione, l’estensione della MSA agli item politomici rappresenta un importante avanzamento nell’analisi delle scale psicometriche. Analizzando non solo gli item nel loro complesso ma anche i singoli passaggi tra categorie, la MSA offre una visione più dettagliata di come gli item riflettono il tratto latente. Tuttavia, è fondamentale verificare che le probabilità rappresentate dalle ISRF rispettino la monotonicità, per garantire la validità e l’affidabilità della scala.\n\n65.4.2 Assunzioni Fondamentali per la MSA con Item Politomici\nL’estensione della MSA agli item politomici mantiene i principi fondamentali del modello originariamente sviluppato per gli item dicotomici, adattandoli ai singoli passaggi tra categorie. Ecco le principali assunzioni:\n\n\nMonotonicità delle ISRF:\n\nLa probabilità di scegliere una categoria \\(k\\) o superiore deve aumentare o rimanere costante all’aumentare del tratto latente \\(\\theta\\).\n\nQuesto garantisce che le categorie siano ordinate in modo coerente e riflettano livelli crescenti di \\(\\theta\\). Ad esempio, un individuo con un tratto latente più alto dovrebbe avere una maggiore probabilità di selezionare categorie superiori rispetto a chi ha un tratto latente più basso.\n\n\n\nOrdinamento coerente delle categorie:\n\nOgni categoria di risposta deve rappresentare un livello progressivamente più alto del tratto latente.\n\nAd esempio, il passaggio da “neutrale” a “d’accordo” dovrebbe indicare un incremento tangibile lungo il continuum di \\(\\theta\\), garantendo che le categorie siano interpretabili e coerenti.\n\n\n\nValidità del modello di omogeneità monotona:\n\nCome per gli item dicotomici, il modello presuppone che tutti gli item misurino lo stesso tratto latente in modo coerente e monotono.\n\nQuesta coerenza implica che le risposte degli individui siano influenzate principalmente dal livello di \\(\\theta\\), con minime interferenze da altri fattori.\n\n\n\n65.4.3 Vantaggi dell’Analisi Politomica con la MSA\nL’estensione della MSA agli item politomici offre strumenti utili per affrontare la complessità di scale con più categorie di risposta, come quelle basate su scale Likert. I principali vantaggi includono:\n\n\nMaggiore precisione nella misurazione:\n\nLa presenza di più categorie di risposta consente di catturare differenze più sottili tra i rispondenti rispetto agli item dicotomici.\n\nQuesto è particolarmente utile per misurare tratti psicologici che variano su un continuum, come atteggiamenti, opinioni o livelli di motivazione. Le categorie multiple offrono un modo per rappresentare una gamma più ampia di livelli del tratto latente.\n\n\n\nValutazione approfondita delle transizioni tra categorie:\n\nLa MSA politomica analizza le transizioni tra categorie consecutive (passaggi), permettendo di verificare se ciascun passaggio è coerente con un progresso lungo il tratto latente.\n\nQuesta analisi consente di identificare problemi come:\n\nCategorie che non rappresentano un aumento coerente del tratto latente.\n\nPassaggi con probabilità di transizione non monotone, che potrebbero indicare una cattiva formulazione dell’item o una scarsa capacità discriminativa.\n\n\n\n\n\nApplicabilità a scale con risposte articolate:\n\nLa MSA per item politomici è particolarmente adatta per scale frequentemente utilizzate in ambiti psicologici, educativi e sociali, dove le risposte graduali (ad esempio, “in disaccordo” a “d’accordo”) sono comuni.\n\nQuesto approccio consente di valutare con precisione la coerenza delle categorie di risposta rispetto al tratto latente e di identificare eventuali categorie ridondanti o poco utili.\n\n\n\n65.4.4 Implicazioni per la costruzione di scale\nL’analisi politomica con la MSA non si limita a verificare se gli item misurano un tratto latente, ma permette anche di ottimizzare la struttura della scala. Attraverso l’analisi dei passaggi, è possibile:\n\n\nGarantire che ogni categoria rappresenti un livello del tratto latente:\n\nOgni passaggio deve riflettere un progresso chiaro lungo il continuum latente, senza sovrapposizioni tra categorie.\n\n\n\nIndividuare anomalie nelle categorie:\n\nCategorie che non seguono un ordine coerente possono essere riformulate o combinate con altre per migliorare la qualità della scala.\n\n\n\nValutare l’utilità di ciascun passaggio:\n\nSe un passaggio tra categorie ha una bassa capacità discriminativa o non rispecchia adeguatamente il progresso lungo \\(\\theta\\), potrebbe essere necessario modificarlo o eliminarlo.\n\n\n\nIn conclusione, l’estensione della MSA agli item politomici offre strumenti analitici per verificare la coerenza e l’utilità delle categorie di risposta. Attraverso l’analisi dei passaggi tra categorie, si può ottenere una misurazione più dettagliata e precisa dei tratti latenti, migliorando la capacità delle scale di rappresentare accuratamente il costrutto d’interesse. Questo approccio è particolarmente rilevante in ambiti in cui le risposte articolate sono indispensabili per catturare la complessità dei fenomeni misurati.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_core_issues.html#mokken-reliability-coefficient",
    "href": "chapters/mokken/02_core_issues.html#mokken-reliability-coefficient",
    "title": "65  Analisi della Scala di Mokken",
    "section": "\n65.5 Mokken Reliability Coefficient",
    "text": "65.5 Mokken Reliability Coefficient\nL’affidabilità di un test psicometrico si riferisce alla sua capacità di fornire risultati coerenti nel tempo o attraverso somministrazioni ripetute. Sebbene l’alfa di Cronbach sia l’indicatore di affidabilità più diffuso, presenta alcune limitazioni, soprattutto quando gli item non sono omogenei o quando si lavora con scale ordinali. In questi casi, il coefficiente \\(\\rho\\) di Mokken (Mokken reliability coefficient o \\(\\rho_M\\)) rappresenta un’alternativa valida, basandosi su assunzioni meno restrittive e adattandosi meglio ai dati ordinali.\nIl coefficiente \\(\\rho\\) di Mokken è una misura di consistenza interna che deriva dalla teoria della scala non parametrica di Mokken, utilizzata principalmente per dati ordinali. A differenza dell’alfa di Cronbach, \\(\\rho\\) non richiede che gli item soddisfino rigorosi criteri di omogeneità o equivalenza, rendendolo particolarmente utile in contesti in cui i dati non rispettano pienamente i presupposti della teoria classica dei test (CTT).\nIl coefficiente \\(\\rho\\) di Mokken misura il rapporto tra la varianza spiegata dal punteggio totale (\\(S^2_T\\)) e la varianza totale osservata. La formula è la seguente:\n\\[\n\\rho = 1 - \\frac{\\sum_{i=1}^k S^2_i}{S^2_T},\n\\]\ndove:\n\n\n\\(k\\): numero di item del test,\n\n\\(S^2_i\\): varianza di ciascun item,\n\n\\(S^2_T\\): varianza totale del punteggio del test (somma dei punteggi degli item).\n\nInterpretazione di \\(\\rho\\):\n\n\n\\(\\rho\\) varia tra 0 e 1.\n\nvalori vicini a 1 indicano maggiore affidabilità.\n\n\n\\(\\rho &gt; 0.70\\): accettabile per la ricerca esplorativa.\n\n\n\\(\\rho &gt; 0.80\\): preferibile per applicazioni pratiche.\n\n\n\nVantaggi di \\(\\rho\\) di Mokken:\n\na differenza dell’alfa di Cronbach, che assume una scala intervallare, \\(\\rho\\) può essere utilizzato con dati ordinali come le scale Likert.\nnon richiede che tutti gli item abbiano correlazioni simili (omogeneità), né che riflettano lo stesso grado di relazione con la variabile latente.\npuò essere applicato a test che non rispettano i presupposti della teoria classica dei test o di modelli parametrici più rigidi.\n\nLimitazioni di \\(\\rho\\) di Mokken:\n\nse gli item non rispettano l’assunzione di monotonicità, l’interpretazione del coefficiente può risultare ambigua.\nil coefficiente è appropriato per scale unidimensionali; in presenza di più dimensioni, è necessario analizzare ogni sottoscala separatamente.\n\nIn conclusione, il coefficiente \\(\\rho\\) di Mokken valuta l’affidabilità di scale ordinali, offrendo un’alternativa all’alfa di Cronbach quando le assunzioni della teoria classica dei test non sono soddisfatte. Tuttavia, è essenziale verificare la monotonicità degli item e considerare la dimensionalità della scala per un’interpretazione accurata.\n\n65.5.1 Procedura di Selezione Automatica degli Item\nLa Procedura di Selezione Automatica degli Item (AISP) è una metodologia impiegata nella MSA per selezionare insiemi di item che rispettino le assunzioni del Modello di Mokken (MHM). A differenza di tecniche più comuni come l’analisi fattoriale o l’analisi parallela, l’AISP non determina esplicitamente la dimensionalità dei dati. Piuttosto, si basa sui coefficienti di scalabilità per identificare gruppi di item che misurano lo stesso costrutto latente, formando così una o più scale.\n\n65.5.1.1 Come Funziona l’AISP\nL’AISP segue un approccio iterativo che:\n\nSeleziona l’item iniziale, valutato come più rappresentativo di una dimensione, utilizzando il coefficiente di scalabilità individuale (\\(H_i\\)).\nSuccessivamente, analizza le coppie di item (\\(H_{ij}\\)) per identificare un insieme scalabile di item che misurano lo stesso costrutto.\n\nUn item viene incluso in una scala se:\n\nIl suo coefficiente di scalabilità individuale (\\(H_i\\)) supera una soglia predefinita (\\(c\\), solitamente pari a 0.30);\nLa covarianza tra ogni coppia di item (\\(H_{ij}\\)) è positiva e superiore alla stessa soglia.\n\nSe un item non soddisfa questi criteri, la procedura tenta di assegnarlo a una nuova scala. Questo processo iterativo continua finché tutti gli item non sono stati assegnati a una scala o esclusi come non scalabili.\n\n65.5.1.2 Soglia (\\(c\\)) e Implicazioni\nLa scelta della soglia \\(c\\) è cruciale:\n\n\nValore alto di \\(c\\): Maggiore precisione nella selezione degli item, ma rischio di escluderne troppi, riducendo la lunghezza della scala e la copertura del costrutto.\n\nValore basso di \\(c\\): Aumenta il numero di item inclusi, ma rischia di compromettere la coerenza e la validità della scala.\n\nUn valore comunemente utilizzato è \\(c = 0.30\\), che rappresenta un buon equilibrio tra inclusività e rigore. Tuttavia, il valore ottimale dipende dagli obiettivi dello studio e dalla natura dei dati.\n\n65.5.1.3 Limitazioni dell’AISP\n\n\nRelazioni tra dimensioni: L’AISP può risultare meno efficace in presenza di dimensioni fortemente correlate o di item che saturano su più dimensioni, poiché non distingue esplicitamente tali situazioni.\n\nDipendenza dai dati statistici: L’AISP si basa esclusivamente sui coefficienti di scalabilità e non considera le relazioni teoriche tra gli item. Questo può portare all’inclusione di item con scarso valore teorico o alla loro esclusione inappropriata.\n\nUnidimensionalità richiesta: Gli item selezionati da una scala devono riflettere un unico costrutto. In caso di multidimensionalità, occorre considerare scale separate.\n\nIn conclusione, l’AISP offre un’alternativa flessibile all’analisi fattoriale per la costruzione di scale psicometriche, particolarmente utile quando si lavora con dati ordinali. Sebbene non forzi soluzioni e tenga conto della scalabilità degli item, è fondamentale integrarla con considerazioni teoriche per garantire che le scale siano valide e utili nel contesto di applicazione.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_core_issues.html#ordinamento-invariante-degli-item-iio",
    "href": "chapters/mokken/02_core_issues.html#ordinamento-invariante-degli-item-iio",
    "title": "65  Analisi della Scala di Mokken",
    "section": "\n65.6 Ordinamento Invariante degli Item (IIO)",
    "text": "65.6 Ordinamento Invariante degli Item (IIO)\nL’Ordinamento Invariante degli Item (Invariant Item Ordering, IIO) è un concetto fondamentale nella MSA. Garantisce che l’ordine di difficoltà degli item rimanga costante per tutti i partecipanti, indipendentemente dal loro livello sul costrutto latente misurato (ad esempio, ansia o autostima).\nIn pratica, l’IIO richiede che:\n\nGli item abbiano sempre lo stesso ordine di difficoltà:\nUn item considerato “più difficile” deve essere percepito come tale da tutti i partecipanti, indipendentemente dal loro livello del tratto latente.\nAd esempio, in un test di matematica, un problema complesso deve risultare più difficile di uno semplice per tutti gli studenti, sia quelli più abili sia quelli meno abili.\nI punteggi totali siano interpretabili:\nSe un partecipante ha un punteggio totale più alto, deve avere una probabilità maggiore di rispondere positivamente (o correttamente) a ogni item rispetto a un partecipante con un punteggio più basso.\n\nL’IIO garantisce che le differenze nei punteggi totali riflettano effettivamente differenze nel costrutto latente, senza essere influenzate da variabili estranee come età, genere o interpretazioni diverse degli item.\n\n65.6.1 Perché l’IIO è Importante?\n\nUniformità nella misurazione:\nL’IIO assicura che gli item misurino lo stesso tratto latente per tutti i partecipanti.\nAd esempio, in una scala di depressione, un punteggio totale più alto deve indicare una maggiore gravità del disturbo, con una progressione coerente dagli item più “facili” (come stanchezza) ai più “difficili” (come pensieri suicidari).\nValidità dei punteggi:\nSe l’ordine degli item cambia tra sottogruppi di partecipanti, i punteggi totali potrebbero non rappresentare correttamente il costrutto latente, invalidando i confronti tra gruppi.\nIdentificazione di bias:\nLa verifica dell’IIO aiuta a individuare problemi come la Funzione Differenziale degli Item (Differential Item Functioning, DIF), che si verifica quando un item funziona diversamente per sottogruppi specifici (ad esempio, uomini e donne).\n\n65.6.2 Come Verificare l’IIO?\nUn metodo semplice per verificare l’IIO è il metodo dei gruppi di restscore. Ecco come funziona:\n\nCalcolo del restscore:\nPer ciascun partecipante, calcola il punteggio totale sugli item, escludendo l’item che stai analizzando. Questo punteggio si chiama restscore.\nCreazione di gruppi basati sul restscore:\nDividi i partecipanti in gruppi (ad esempio, punteggi bassi, medi e alti) in base al loro restscore.\n\nConfronto dell’ordine degli item:\nVerifica se l’ordine di difficoltà degli item rimane lo stesso tra i gruppi.\n\nAd esempio, se l’ordine teorico degli item è \\(A &lt; B &lt; C\\) (da più facile a più difficile), questo ordine deve essere confermato nei gruppi con restscore basso, medio e alto.\n\n\n\nSe l’ordine degli item cambia tra i gruppi, l’IIO non è rispettato. Questo potrebbe indicare che alcuni item non misurano il costrutto latente in modo coerente.\n\n65.6.3 Cosa Succede se l’IIO Non È Rispettato?\nSe l’IIO viene violato, si possono riscontrare i seguenti problemi:\n\nBias negli item:\nAlcuni item potrebbero misurare qualcosa di diverso dal costrutto latente (ad esempio, un item di depressione potrebbe essere influenzato dal genere o dalla cultura).\nPunteggi non interpretabili:\nI punteggi totali non riflettono accuratamente il livello del tratto latente, compromettendo l’utilità del test.\nInvalidità della scala:\nLa scala potrebbe non misurare un unico costrutto latente, rendendola poco affidabile per applicazioni pratiche.\n\nIn conclusione, l’IIO è essenziale per garantire che una scala misuri in modo coerente un costrutto latente. Verificare l’IIO consente di assicurarsi che:\n\ngli item mantengano un ordine di difficoltà stabile per tutti i partecipanti;\ni punteggi totali siano validi e rappresentino accuratamente il tratto latente;\n\nla scala sia priva di bias o funzionamenti differenziali tra sottogruppi.\n\nIl metodo dei gruppi di restscore offre un approccio intuitivo per valutare l’IIO e identificare eventuali problematiche nella costruzione o interpretazione della scala.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_core_issues.html#dimensione-del-campione-per-lmsa",
    "href": "chapters/mokken/02_core_issues.html#dimensione-del-campione-per-lmsa",
    "title": "65  Analisi della Scala di Mokken",
    "section": "\n65.7 Dimensione del Campione per l’MSA",
    "text": "65.7 Dimensione del Campione per l’MSA\nLa determinazione della dimensione del campione è essenziale per garantire l’affidabilità dei risultati nella MSA, ma non esistono linee guida univoche. Campioni troppo piccoli possono portare a:\n\n\nFalsi positivi, con scale erroneamente ritenute valide.\n\nFalsi negativi, con scale esistenti che non vengono rilevate.\n\nPrincipali Fattori che Influenzano la Dimensione del Campione\n\n\nCoefficiente di scalabilità (\\(H_i\\)):\n\nPer \\(H_i\\) basso (\\(\\approx 0.22\\)): sono necessari campioni molto grandi (750-2500 partecipanti).\n\nPer \\(H_i\\) moderato o alto (\\(\\approx 0.42\\)): campioni più piccoli (50-250 partecipanti) sono sufficienti.\n\n\nLunghezza del test:\nLa lunghezza del test ha un impatto minore, poiché test più lunghi non richiedono necessariamente campioni più grandi.\nCorrelazione tra dimensioni:\nScale multidimensionali possono necessitare di campioni più grandi a causa della complessità del modello.\n\nLinee Guida Generali\n\nPer $H_i : almeno 1000-2500 partecipanti per ottenere risultati accurati.\nPer $H_i : campioni di 50-250 partecipanti possono essere adeguati.\n\nStudi come quelli di Watson et al. (2018) suggeriscono che, sebbene i valori medi di \\(H\\) e \\(H_i\\) siano stabili anche con campioni più piccoli, i loro intervalli di confidenza sono molto ampi, aumentando il rischio di errori nelle decisioni sugli item.\nIn conclusione, campioni più grandi migliorano la precisione delle stime e la stabilità dei coefficienti di scalabilità. Tuttavia, per scale con \\(H_i\\) elevato, campioni moderati possono essere sufficienti per un’analisi accurata.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_core_issues.html#confronto-tra-la-ctt-e-la-msa",
    "href": "chapters/mokken/02_core_issues.html#confronto-tra-la-ctt-e-la-msa",
    "title": "65  Analisi della Scala di Mokken",
    "section": "\n65.8 Confronto tra la CTT e la MSA",
    "text": "65.8 Confronto tra la CTT e la MSA\nLa Classical Test Theory (CTT) e la MSA rappresentano due approcci distinti alla misurazione psicometrica, ognuno con vantaggi e limiti. Pur condividendo alcuni concetti di base, si differenziano per assunzioni, flessibilità e modalità di analisi dei dati.\n\n65.8.1 Somiglianze tra CTT e MSA\nEntrambi gli approcci analizzano le relazioni tra item e punteggi totali per valutare la qualità di un test. Alcuni indicatori delle due teorie sono concettualmente simili:\n\nIl coefficiente di scalabilità degli item (\\(H_i\\)) della MSA è paragonabile alla correlazione tra un item e il punteggio totale nella CTT.\nIl coefficiente di scalabilità tra coppie di item (\\(H_{ij}\\)) della MSA si avvicina alla correlazione tra coppie di item nella CTT.\nIl coefficiente complessivo di scalabilità (\\(H\\)) nella MSA è analogo all’indice medio di discriminazione degli item nella CTT.\n\nQueste somiglianze rendono entrambi gli approcci utili per descrivere le caratteristiche degli item e dei test.\n\n65.8.2 Differenze tra CTT e MSA\n\n\nAssunzioni e loro verifica:\n\nLa CTT si basa su assunzioni teoriche non direttamente verificabili dai dati, come l’indipendenza degli errori di misura e l’unidimensionalità del costrutto.\n\nLa MSA, invece, consente di testare empiricamente assunzioni fondamentali come la monotonicità, l’indipendenza locale e l’unidimensionalità. Per esempio, un coefficiente di scalabilità negativo (\\(H_i &lt; 0\\)) indicherebbe che i dati non sono compatibili con il modello.\n\n\n\nLivello di misurazione:\n\nLa CTT richiede dati a livello di scala ad intervalli e considera i punteggi totali come un’approssimazione diretta del costrutto latente.\n\nLa MSA è più adatta per dati ordinali (come risposte su una scala Likert) e non presuppone una relazione specifica tra il costrutto latente e le risposte agli item.\n\n\n\nIn conclusione, la CTT e la MSA non si escludono a vicenda, ma possono essere viste come approcci complementari. La CTT fornisce una base teorica solida e intuitiva per la misurazione, mentre la MSA permette un’analisi più rigorosa e adattabile alle caratteristiche dei dati. Utilizzare entrambi gli approcci in modo integrato può offrire una visione più completa della qualità e della validità di una scala psicometrica.\n\n65.8.3 Riflessioni Conclusive\nQuesto capitolo ha evidenziato l’importanza di non dare per scontata l’assunzione, tipica della Classical Test Theory (CTT), che i punteggi grezzi rappresentino dati ordinali validi. Tale presupposto, spesso implicito, necessita di una verifica empirica per garantire che le interpretazioni siano accurate e affidabili. In questo contesto, l’Analisi delle Scale di Mokken (MSA) offre strumenti efficaci per valutare empiricamente questa ipotesi.\nAttraverso i coefficienti di scalabilità (\\(H\\), \\(H_i\\), \\(H_{ij}\\)), la MSA permette di verificare se un test misura un costrutto unidimensionale in modo coerente, mentre il principio dell’Ordinamento Invariante degli Item (IIO) garantisce che l’ordine di difficoltà degli item sia stabile per tutti i livelli di abilità dei rispondenti. Questi strumenti forniscono un quadro analitico robusto per validare la qualità di una scala psicometrica, superando alcune limitazioni della CTT.\nLa MSA rappresenta un’evoluzione rispetto al modello di Guttman, introducendo maggiore flessibilità e adattabilità nella gestione delle deviazioni dalla cumulatività perfetta. Questo la rende particolarmente utile per l’analisi esplorativa di scale psicologiche, consentendo di indagare la struttura delle risposte agli item e la loro relazione con il costrutto latente.\nIn sintesi, l’MSA non solo integra ma amplia i tradizionali approcci della psicometria, offrendo strumenti analitici utili per costruire e validare scale che misurano costrutti complessi in modo accurato e interpretabile.\n\n\n\n\nWind, S. A. (2017). An instructional module on Mokken scale analysis. Educational Measurement: Issues and Practice, 36(2), 50–66.\n\n\nWind, S. A. (2024). Item-Explanatory Mokken Scale Analysis: Using Nonparametric Item Response Theory to Explore Item Attributes. The Journal of Experimental Education, 1–21.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/03_applications.html",
    "href": "chapters/mokken/03_applications.html",
    "title": "66  Applicazione Pratica",
    "section": "",
    "text": "66.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nIn questo capitolo, esamineremo l’applicazione pratica dei concetti e delle metodologie esplorate nel precedente capitolo, affrontando un’analisi dettagliata di un set di dati concreti. Il nostro focus è un caso di studio di grande rilevanza psicologica: l’indagine condotta dai ricercatori dell’ospedale Meyer, mirata a comprendere la capacità di coping delle famiglie di fronte alla diagnosi di una grave malattia in un figlio.\nQuesto lavoro non solo rappresenta un’opportunità per mettere in pratica le teorie e i metodi discussi, ma offre anche una finestra su questioni di vitale importanza nel campo della psicologia. Affrontare tematiche così delicate ci permette di esplorare le dinamiche familiari in situazioni di stress estremo, fornendo intuizioni preziose che possono guidare interventi psicosociali efficaci.\nPer garantire la massima accuratezza e rilevanza dei nostri risultati, iniziamo con un’attenta preparazione e pulizia dei dati. Questo passo ci assicura che l’analisi sia condotta su informazioni ben distribuite e rappresentative, eliminando gli item con eccessiva asimmetria e curtosi.\nAttraverso questa esplorazione approfondita, miriamo a dimostrare come le competenze metodologiche e analitiche possano essere efficacemente applicate a questioni di profondo impatto psicologico, evidenziando il potere dell’analisi statistica nel trasformare set di dati complessi in comprensioni approfondite e applicabili.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/03_applications.html#importazione-dei-dati",
    "href": "chapters/mokken/03_applications.html#importazione-dei-dati",
    "title": "66  Applicazione Pratica",
    "section": "66.2 Importazione dei dati",
    "text": "66.2 Importazione dei dati\n\ndf_tot &lt;- readRDS(\"../../data/fai_2022_11_20.rds\")\n\ntemp &lt;- df_tot |&gt; \n  dplyr::filter(FLAG == \"keep\")\ntemp$FLAG &lt;- NULL\n\nPer migliorare la qualità del nostro set di dati, rimuoveremo gli item che presentano livelli eccessivi di asimmetria (skewness) e curtosi. Questo passaggio è fondamentale per garantire che i nostri dati siano ben distribuiti e rappresentativi, migliorando così l’affidabilità e la validità delle nostre analisi. Gli item con asimmetria e curtosi estreme possono infatti distorcere i risultati degli analisi statistiche e influenzare negativamente le conclusioni tratte dallo studio.\n\nitems_stats &lt;- psych::describe(temp)\n\nitems_skew_kurt_bad &lt;- items_stats |&gt;\n    dplyr::filter(skew &gt; 2.5 | kurtosis &gt; 7.5) |&gt;\n    row.names()\nprint(items_skew_kurt_bad)\n\n [1] \"other\"                  \"child_birth_place\"      \"has_chronic_disease\"   \n [4] \"hospitalization_number\" \"emergency_care_number\"  \"divorce\"               \n [7] \"low_income\"             \"change_address\"         \"change_city\"           \n[10] \"is_mother_italian\"      \"is_father_italian\"      \"is_father_working\"     \n[13] \"is_child_italian\"       \"FAI_24\"                 \"FAI_32\"                \n[16] \"FAI_52\"                 \"FAI_53\"                 \"FAI_61\"                \n[19] \"FAI_74\"                 \"FAI_76\"                 \"FAI_77\"                \n[22] \"FAI_138\"                \"FAI_152\"                \"FAI_174\"               \n[25] \"FAI_175\"                \"FAI_182\"                \"FAI_193\"               \n\n\n\n# Select the strings starting with \"FAI_\"\nbad_fai_items &lt;- grep(\"^FAI_\", items_skew_kurt_bad, value = TRUE)\n\ndf &lt;- temp |&gt;\n    dplyr::select(!any_of(bad_fai_items))\n\nCi concentreremo qui su un sottoinsieme di item, ovvero quelli che riguardano l’area delle caratteristiche del bambino.\n\n# First subscale: items names.\nitem_subscale &lt;- c(\n    \"FAI_49\", \"FAI_106\", \"FAI_60\", \"FAI_124\", \"FAI_86\",\n    \"FAI_47\", \"FAI_121\", \"FAI_167\", \"FAI_99\",\n    \"FAI_63\", \"FAI_168\", \"FAI_5\", \"FAI_132\", \"FAI_85\", \"FAI_81\",\n    \"FAI_83\",\n    # \"FAI_152\",  \"FAI_175\",\n    \"FAI_57\", \"FAI_91\", \"FAI_135\", \"FAI_1\"\n)\n\n# Select only the items of this subscale.\nsubscale_data &lt;- df %&gt;%\n    dplyr::select(all_of(item_subscale))\ndim(subscale_data)\n\n\n45320",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/03_applications.html#statistiche-descrittive",
    "href": "chapters/mokken/03_applications.html#statistiche-descrittive",
    "title": "66  Applicazione Pratica",
    "section": "66.3 Statistiche descrittive",
    "text": "66.3 Statistiche descrittive\nEsaminiamo le statistiche descrittive degli item.\n\npsych::describe(subscale_data)\n\n\nA psych: 20 x 13\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nFAI_49\n1\n453\n0.5629139\n1.0320709\n0\n0.3085399\n0.0000\n0\n4\n4\n2.0665609\n3.6052375\n0.04849092\n\n\nFAI_106\n2\n453\n0.6710817\n0.9911011\n0\n0.4710744\n0.0000\n0\n4\n4\n1.6036473\n2.0835785\n0.04656599\n\n\nFAI_60\n3\n453\n0.9403974\n1.2169441\n0\n0.7272727\n0.0000\n0\n4\n4\n1.1792903\n0.2955897\n0.05717702\n\n\nFAI_124\n4\n453\n1.0463576\n1.1955175\n1\n0.8429752\n1.4826\n0\n4\n4\n1.1667416\n0.4658086\n0.05617031\n\n\nFAI_86\n5\n453\n1.1898455\n1.0171900\n1\n1.0606061\n1.4826\n0\n4\n4\n0.9491960\n0.6428214\n0.04779175\n\n\nFAI_47\n6\n453\n1.6247241\n1.1132778\n2\n1.5950413\n1.4826\n0\n4\n4\n0.1994007\n-0.7340689\n0.05230635\n\n\nFAI_121\n7\n453\n0.4900662\n0.7121035\n0\n0.3746556\n0.0000\n0\n4\n4\n1.8321794\n4.5725145\n0.03345754\n\n\nFAI_167\n8\n453\n1.9072848\n1.0306218\n2\n1.8815427\n1.4826\n0\n4\n4\n0.1974188\n-0.2695266\n0.04842284\n\n\nFAI_99\n9\n453\n2.0618102\n1.1543198\n2\n2.0220386\n1.4826\n0\n4\n4\n0.3447637\n-0.7883135\n0.05423468\n\n\nFAI_63\n10\n453\n1.1501104\n1.1615070\n1\n1.0192837\n1.4826\n0\n4\n4\n0.7295382\n-0.4985964\n0.05457236\n\n\nFAI_168\n11\n453\n0.7748344\n1.1298770\n0\n0.5399449\n0.0000\n0\n4\n4\n1.4492317\n1.1504289\n0.05308625\n\n\nFAI_5\n12\n453\n1.0132450\n1.1805209\n1\n0.8264463\n1.4826\n0\n4\n4\n1.0371531\n0.1238107\n0.05546571\n\n\nFAI_132\n13\n453\n1.4238411\n1.2360930\n1\n1.3002755\n1.4826\n0\n4\n4\n0.5953586\n-0.6157883\n0.05807672\n\n\nFAI_85\n14\n453\n1.3311258\n0.9027136\n1\n1.3002755\n1.4826\n0\n4\n4\n0.2930726\n-0.1115428\n0.04241319\n\n\nFAI_81\n15\n453\n1.7969095\n1.2685052\n2\n1.7465565\n1.4826\n0\n4\n4\n0.2930069\n-1.0004309\n0.05959957\n\n\nFAI_83\n16\n453\n1.1986755\n1.1825793\n1\n1.0468320\n1.4826\n0\n4\n4\n0.8458362\n-0.1601818\n0.05556242\n\n\nFAI_57\n17\n453\n0.3863135\n0.7123983\n0\n0.2341598\n0.0000\n0\n4\n4\n2.2165283\n5.6437160\n0.03347139\n\n\nFAI_91\n18\n453\n0.5320088\n0.9298228\n0\n0.3057851\n0.0000\n0\n4\n4\n2.1393688\n4.4338138\n0.04368689\n\n\nFAI_135\n19\n453\n0.4039735\n0.7686801\n0\n0.2231405\n0.0000\n0\n4\n4\n2.2914674\n5.8874641\n0.03611574\n\n\nFAI_1\n20\n453\n0.4547461\n0.7530503\n0\n0.2892562\n0.0000\n0\n4\n4\n1.8594704\n3.7153232\n0.03538139\n\n\n\n\n\nEsaminiamo la distribuzione del punteggio totale.\n\nscores &lt;- apply(subscale_data, 1, sum)\nhist(scores)",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/03_applications.html#automated-item-selection-procedure-aisp",
    "href": "chapters/mokken/03_applications.html#automated-item-selection-procedure-aisp",
    "title": "66  Applicazione Pratica",
    "section": "66.4 Automated Item Selection Procedure (AISP)",
    "text": "66.4 Automated Item Selection Procedure (AISP)\nIl primo passo nell’Analisi delle Scale Mokken (MSA) consiste nell’esecuzione dell’Automated Item Selection Procedure (AISP). Come precedentemente discusso, questa analisi ricerca insiemi di item (scale) che si conformano al modello di omogeneità monotona. Similmente all’analisi fattoriale esplorativa, l’AISP è un metodo per suddividere i dati in diverse sottoscale che soddisfano i criteri della MSA, includendo possibilmente anche l’individuazione di eventuali item non scalabili. Per eseguire la AISP, è necessario eseguire il codice seguente.\n\nsubscale_data &lt;- as.data.frame(subscale_data)\nscale &lt;- aisp(subscale_data, verbose = FALSE)\nprint(scale)\n\n        0.3\nFAI_49    1\nFAI_106   1\nFAI_60    1\nFAI_124   1\nFAI_86    0\nFAI_47    2\nFAI_121   0\nFAI_167   0\nFAI_99    0\nFAI_63    2\nFAI_168   0\nFAI_5     1\nFAI_132   2\nFAI_85    2\nFAI_81    1\nFAI_83    1\nFAI_57    2\nFAI_91    0\nFAI_135   0\nFAI_1     0\n\n\nNell’output precedente, FAI_* sono le etichette degli item considerati. Il valore .30 in alto indica il limite inferiore del coefficiente di scalabilità per la costruzione delle scale. ‘1’ indica che l’item appartiene alla scala 1 e 2 significa che l’item appartiene alla scala 2. ‘0’ indica che l’item non è scalabile. Dei 20 item di questa scala, sette item formano la scala 1 mentre altri cinque item formano la scala 2. Sette item risultano non scalabili.\nÈ possibile modificare sia il limite inferiore c (il limite inferiore predefinito è .30) sia il livello di alpha, che di default è .05. Per esempio:\n\nscale &lt;- aisp(subscale_data, lowerbound = 0.4, alpha = 0.1)\nprint(scale)\n\n        0.4\nFAI_49    1\nFAI_106   1\nFAI_60    1\nFAI_124   1\nFAI_86    0\nFAI_47    0\nFAI_121   0\nFAI_167   0\nFAI_99    0\nFAI_63    3\nFAI_168   0\nFAI_5     2\nFAI_132   0\nFAI_85    0\nFAI_81    2\nFAI_83    2\nFAI_57    3\nFAI_91    0\nFAI_135   0\nFAI_1     0\n\n\nSi noti che modificare il valore predefinito di c cambia la struttura della scala. Sijtsma e van der Ark (2017) hanno mostrato che il valore predefinito di .30 è quello che si dimostra più utile nella maggior parte delle applicazioni pratiche. Tuttavia, raccomandano di eseguire l’AISP 12 volte con c=0, .05, .10, .15, .20, .25, .30, .35, .40, .45, .50 e .55, così da potere esaminare le seguenti condizioni:\n\nNei dati unidimensionali, all’aumentare di c, si trova successivamente (a) la maggior parte o tutti gli item in una scala; (b) una scala più piccola; e (c) una o poche scale piccole e diversi item non scalabili. Viene raccomandato di prendere il risultato della fase (a) come finale.\nNei dati multidimensionali, all’aumentare di c, si trova successivamente (a) la maggior parte o tutti gli item in una scala; (b) due o più scale; e (c) due o più scale più piccole e diversi item non scalabili. Prendere il risultato della fase (b) come finale.\n\nLa decisione finale sulla struttura dei dati dovrebbe essere presa dal ricercatore sulla base di considerazioni teoriche e non sono statistiche.\nPer eseguire l’AISP con molteplici limiti inferiori, possiamo usare l’istruzione seguente:\n\naisp(\n    subscale_data, \n    lowerbound = c(.05, .10, .15, .20, .25, .30, .35, .40, .45, .50, .55), \n    verbose = FALSE\n)\n\n\nA matrix: 20 x 11 of type dbl\n\n\n\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n0.55\n\n\n\n\nFAI_49\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nFAI_106\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nFAI_60\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nFAI_124\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nFAI_86\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nFAI_47\n1\n1\n1\n1\n2\n2\n2\n0\n0\n0\n0\n\n\nFAI_121\n1\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n\n\nFAI_167\n1\n1\n1\n2\n0\n0\n0\n0\n0\n0\n0\n\n\nFAI_99\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nFAI_63\n1\n1\n1\n1\n2\n2\n2\n3\n3\n3\n0\n\n\nFAI_168\n1\n1\n1\n1\n3\n0\n0\n0\n0\n0\n0\n\n\nFAI_5\n1\n1\n1\n1\n1\n1\n0\n2\n0\n0\n0\n\n\nFAI_132\n1\n1\n1\n1\n2\n2\n0\n0\n0\n0\n0\n\n\nFAI_85\n2\n2\n0\n2\n2\n2\n2\n0\n0\n0\n0\n\n\nFAI_81\n1\n1\n1\n1\n1\n1\n1\n2\n2\n2\n2\n\n\nFAI_83\n1\n1\n1\n1\n1\n1\n1\n2\n2\n2\n2\n\n\nFAI_57\n1\n1\n1\n1\n2\n2\n2\n3\n3\n3\n0\n\n\nFAI_91\n1\n1\n1\n1\n3\n0\n0\n0\n0\n0\n0\n\n\nFAI_135\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nFAI_1\n2\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\nPer eseguire un’analisi della dimensionalità utilizzando un metodo diverso, ovvero l’algoritmo genetico (Straat, van der Ark & Sijtsma, 2013), si può utilizzare il seguente codice:\n\nscale &lt;- aisp(subscale_data, search = \"ga\")\nprint(scale)\n\n        0.3\nFAI_49    1\nFAI_106   1\nFAI_60    1\nFAI_124   1\nFAI_86    0\nFAI_47    2\nFAI_121   0\nFAI_167   0\nFAI_99    0\nFAI_63    2\nFAI_168   0\nFAI_5     1\nFAI_132   2\nFAI_85    2\nFAI_81    1\nFAI_83    1\nFAI_57    2\nFAI_91    0\nFAI_135   0\nFAI_1     0\n\n\nI risultati dell’algoritmo genetico sono equivalenti a quelli ottenuti utilizzando il limite inferiore raccomandato di 0.3.\nI risultati dell’analisi della dimensionalità ottenuti tramite l’AISP e l’algoritmo genetico (GA) dovrebbero essere replicabili in altri campioni. Pertanto, nelle procedure AISP e GA, la dimensione del campione è un fattore critico. Sijtsma e Molenaar (2002) affermano che l’AISP richiede almeno 100 partecipanti. Tuttavia, in studi basati su simulazioni di Monte Carlo, Straat, van der Ark e Sijtsma (2014) hanno dimostrato che sia l’AISP sia il GA necessitano di un campione compreso tra 250 e 500 partecipanti quando la qualità degli item (ovvero la loro capacità discriminante) è elevata, e tra 1250 e 1750 partecipanti quando la qualità degli item è scarsa.\nPossiamo selezionare gli item della scala 1 individuata dalla AISP nel modo seguente.\n\naisp.lb &lt;- aisp(subscale_data, lowerbound = .3)\ngood_items &lt;- subscale_data[, aisp.lb == 1]\nnames(good_items) |&gt; print()\n\n[1] \"FAI_49\"  \"FAI_106\" \"FAI_60\"  \"FAI_124\" \"FAI_5\"   \"FAI_81\"  \"FAI_83\"",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/03_applications.html#scalability-coefficients",
    "href": "chapters/mokken/03_applications.html#scalability-coefficients",
    "title": "66  Applicazione Pratica",
    "section": "66.5 Scalability Coefficients",
    "text": "66.5 Scalability Coefficients\nIl codice seguente ritorna i valori \\(H_{ij}\\), \\(H_j\\), e \\(H\\). Nelle parentesi tonde sono riportati gli errori standard.\n\nscal_coef &lt;- coefH(good_items, se = TRUE)\nscal_coef |&gt; print()\n\n$Hij\n        FAI_49  se      FAI_106 se      FAI_60  se      FAI_124 se      FAI_5  \nFAI_49                   0.561  (0.057)  0.608  (0.054)  0.601  (0.048)  0.212 \nFAI_106  0.561  (0.057)                  0.617  (0.042)  0.722  (0.039)  0.317 \nFAI_60   0.608  (0.054)  0.617  (0.042)                  0.688  (0.037)  0.280 \nFAI_124  0.601  (0.048)  0.722  (0.039)  0.688  (0.037)                  0.288 \nFAI_5    0.212  (0.056)  0.317  (0.055)  0.280  (0.052)  0.288  (0.052)        \nFAI_81   0.254  (0.060)  0.354  (0.056)  0.324  (0.050)  0.381  (0.053)  0.413 \nFAI_83   0.223  (0.057)  0.381  (0.056)  0.430  (0.047)  0.387  (0.048)  0.409 \n        se      FAI_81  se      FAI_83  se     \nFAI_49  (0.056)  0.254  (0.060)  0.223  (0.057)\nFAI_106 (0.055)  0.354  (0.056)  0.381  (0.056)\nFAI_60  (0.052)  0.324  (0.050)  0.430  (0.047)\nFAI_124 (0.052)  0.381  (0.053)  0.387  (0.048)\nFAI_5            0.413  (0.049)  0.409  (0.048)\nFAI_81  (0.049)                  0.551  (0.041)\nFAI_83  (0.048)  0.551  (0.041)                \n\n$Hi\n        Item H  se     \nFAI_49    0.410 (0.039)\nFAI_106   0.493 (0.032)\nFAI_60    0.484 (0.029)\nFAI_124   0.502 (0.029)\nFAI_5     0.323 (0.037)\nFAI_81    0.386 (0.034)\nFAI_83    0.404 (0.032)\n\n$H\nScale H      se \n  0.428 (0.026) \n\n$Hij\n        FAI_49  se      FAI_106 se      FAI_60  se      FAI_124 se      FAI_5  \nFAI_49                   0.561  (0.057)  0.608  (0.054)  0.601  (0.048)  0.212 \nFAI_106  0.561  (0.057)                  0.617  (0.042)  0.722  (0.039)  0.317 \nFAI_60   0.608  (0.054)  0.617  (0.042)                  0.688  (0.037)  0.280 \nFAI_124  0.601  (0.048)  0.722  (0.039)  0.688  (0.037)                  0.288 \nFAI_5    0.212  (0.056)  0.317  (0.055)  0.280  (0.052)  0.288  (0.052)        \nFAI_81   0.254  (0.060)  0.354  (0.056)  0.324  (0.050)  0.381  (0.053)  0.413 \nFAI_83   0.223  (0.057)  0.381  (0.056)  0.430  (0.047)  0.387  (0.048)  0.409 \n        se      FAI_81  se      FAI_83  se     \nFAI_49  (0.056)  0.254  (0.060)  0.223  (0.057)\nFAI_106 (0.055)  0.354  (0.056)  0.381  (0.056)\nFAI_60  (0.052)  0.324  (0.050)  0.430  (0.047)\nFAI_124 (0.052)  0.381  (0.053)  0.387  (0.048)\nFAI_5            0.413  (0.049)  0.409  (0.048)\nFAI_81  (0.049)                  0.551  (0.041)\nFAI_83  (0.048)  0.551  (0.041)                \n\n$Hi\n        Item H  se     \nFAI_49    0.410 (0.039)\nFAI_106   0.493 (0.032)\nFAI_60    0.484 (0.029)\nFAI_124   0.502 (0.029)\nFAI_5     0.323 (0.037)\nFAI_81    0.386 (0.034)\nFAI_83    0.404 (0.032)\n\n$H\nScale H      se \n  0.428 (0.026) \n\n$covHij\n               [,1]          [,2]          [,3]          [,4]          [,5]\n [1,]  3.256522e-03  1.972567e-03  1.673569e-03  6.850269e-04  0.0009303758\n [2,]  1.972567e-03  2.872632e-03  1.379662e-03  5.619181e-04  0.0007468355\n [3,]  1.673569e-03  1.379662e-03  2.324218e-03  7.130034e-04  0.0009755748\n [4,]  6.850269e-04  5.619181e-04  7.130034e-04  3.132843e-03  0.0014577910\n [5,]  9.303758e-04  7.468355e-04  9.755748e-04  1.457791e-03  0.0035999264\n [6,]  1.176059e-03  1.372448e-03  1.121228e-03  1.001618e-03  0.0007258314\n [7,]  1.143505e-03  6.796210e-04  6.075535e-04  3.611057e-04  0.0005325189\n [8,]  9.205140e-04  4.625925e-04  3.783489e-04  2.484545e-04  0.0003248843\n [9,]  1.281966e-04  2.342448e-04  1.519768e-04  1.410060e-03  0.0006882034\n[10,]  4.801610e-05  3.000776e-04  1.972389e-04  6.442391e-04  0.0016398978\n[11,]  6.330714e-05  1.877564e-04  2.994296e-04  4.332023e-04  0.0002887708\n[12,]  7.230668e-04  6.943860e-04  7.140702e-04  2.950943e-04  0.0003590450\n[13,]  3.194478e-04  4.391644e-04  3.080059e-04  1.808926e-03  0.0006665414\n[14,]  3.310593e-04  2.500548e-04  2.591961e-04  5.841647e-04  0.0016242307\n[15,] -2.626538e-05  1.143234e-04  3.461109e-04  5.949934e-04  0.0002896028\n[16,]  1.885976e-04  2.599218e-04  3.321093e-04  1.573800e-03  0.0006905796\n[17,]  1.132431e-04  1.511628e-04  5.240330e-05  5.614512e-04  0.0016082483\n[18,]  2.425335e-04  4.564482e-04  2.708591e-04  4.202745e-04  0.0002271944\n[19,]  6.016676e-05 -5.677038e-08  6.439974e-05  4.080167e-04  0.0003000318\n[20,] -6.323510e-05  2.770648e-04  9.115472e-05  4.564269e-04 -0.0000318649\n[21,] -1.452389e-04  4.045822e-05 -1.410212e-04 -8.502939e-05  0.0001437967\n               [,6]          [,7]          [,8]         [,9]        [,10]\n [1,]  1.176059e-03  1.143505e-03  9.205140e-04 1.281966e-04 0.0000480161\n [2,]  1.372448e-03  6.796210e-04  4.625925e-04 2.342448e-04 0.0003000776\n [3,]  1.121228e-03  6.075535e-04  3.783489e-04 1.519768e-04 0.0001972389\n [4,]  1.001618e-03  3.611057e-04  2.484545e-04 1.410060e-03 0.0006442391\n [5,]  7.258314e-04  5.325189e-04  3.248843e-04 6.882034e-04 0.0016398978\n [6,]  3.235792e-03  3.934074e-04  4.943276e-04 4.792224e-04 0.0002711309\n [7,]  3.934074e-04  1.765623e-03  7.886910e-04 3.994963e-04 0.0003576934\n [8,]  4.943276e-04  7.886910e-04  1.518517e-03 3.437684e-04 0.0004289051\n [9,]  4.792224e-04  3.994963e-04  3.437684e-04 3.075515e-03 0.0011512604\n[10,]  2.711309e-04  3.576934e-04  4.289051e-04 1.151260e-03 0.0030881212\n[11,]  1.580928e-03  7.872979e-04  6.366403e-04 1.439675e-03 0.0011535909\n[12,]  5.881401e-04  7.514867e-04  4.972229e-04 5.983421e-05 0.0002224072\n[13,]  7.426056e-04  4.490709e-04  9.903078e-05 1.401373e-03 0.0005654204\n[14,]  3.670699e-04  4.022907e-04  2.546547e-04 5.617317e-04 0.0015767924\n[15,]  1.394333e-03  3.075009e-04  2.545034e-04 4.954782e-04 0.0003710759\n[16,]  5.291520e-04  1.750076e-04  4.106679e-04 1.618941e-03 0.0007001615\n[17,]  2.446268e-04  2.201295e-04  2.059815e-04 6.070280e-04 0.0017308946\n[18,]  1.440363e-03  3.706182e-04  4.282926e-04 4.721104e-04 0.0005205689\n[19,] -2.408574e-05  7.725707e-05  8.859144e-05 7.086189e-04 0.0004992864\n[20,]  6.059513e-04  6.986203e-05 -5.566603e-05 8.917753e-04 0.0004276745\n[21,]  2.480873e-04 -1.299639e-05 -3.392847e-05 3.442378e-04 0.0005256114\n             [,11]         [,12]        [,13]        [,14]         [,15]\n [1,] 6.330714e-05  7.230668e-04 3.194478e-04 0.0003310593 -2.626538e-05\n [2,] 1.877564e-04  6.943860e-04 4.391644e-04 0.0002500548  1.143234e-04\n [3,] 2.994296e-04  7.140702e-04 3.080059e-04 0.0002591961  3.461109e-04\n [4,] 4.332023e-04  2.950943e-04 1.808926e-03 0.0005841647  5.949934e-04\n [5,] 2.887708e-04  3.590450e-04 6.665414e-04 0.0016242307  2.896028e-04\n [6,] 1.580928e-03  5.881401e-04 7.426056e-04 0.0003670699  1.394333e-03\n [7,] 7.872979e-04  7.514867e-04 4.490709e-04 0.0004022907  3.075009e-04\n [8,] 6.366403e-04  4.972229e-04 9.903078e-05 0.0002546547  2.545034e-04\n [9,] 1.439675e-03  5.983421e-05 1.401373e-03 0.0005617317  4.954782e-04\n[10,] 1.153591e-03  2.224072e-04 5.654204e-04 0.0015767924  3.710759e-04\n[11,] 3.100653e-03  3.797460e-04 5.558660e-04 0.0004499206  1.388343e-03\n[12,] 3.797460e-04  1.353425e-03 3.361125e-04 0.0003599263  3.711701e-04\n[13,] 5.558660e-04  3.361125e-04 2.679508e-03 0.0010149171  9.133291e-04\n[14,] 4.499206e-04  3.599263e-04 1.014917e-03 0.0025074061  7.814797e-04\n[15,] 1.388343e-03  3.711701e-04 9.133291e-04 0.0007814797  2.239613e-03\n[16,] 5.671980e-04  3.549025e-04 1.809069e-03 0.0006726168  7.152364e-04\n[17,] 5.230923e-04  8.662901e-05 5.994210e-04 0.0016280518  4.484829e-04\n[18,] 1.453794e-03  3.694086e-04 6.818643e-04 0.0005001348  1.390561e-03\n[19,] 3.721344e-04  8.121422e-05 6.065452e-04 0.0004767595  4.240890e-05\n[20,] 6.949490e-04  2.048871e-04 8.608315e-04 0.0001955560  4.525888e-04\n[21,] 3.341346e-04 -7.180751e-05 6.989798e-05 0.0004952616  1.117222e-04\n             [,16]        [,17]        [,18]         [,19]         [,20]\n [1,] 1.885976e-04 1.132431e-04 0.0002425335  6.016676e-05 -6.323510e-05\n [2,] 2.599218e-04 1.511628e-04 0.0004564482 -5.677038e-08  2.770648e-04\n [3,] 3.321093e-04 5.240330e-05 0.0002708591  6.439974e-05  9.115472e-05\n [4,] 1.573800e-03 5.614512e-04 0.0004202745  4.080167e-04  4.564269e-04\n [5,] 6.905796e-04 1.608248e-03 0.0002271944  3.000318e-04 -3.186490e-05\n [6,] 5.291520e-04 2.446268e-04 0.0014403628 -2.408574e-05  6.059513e-04\n [7,] 1.750076e-04 2.201295e-04 0.0003706182  7.725707e-05  6.986203e-05\n [8,] 4.106679e-04 2.059815e-04 0.0004282926  8.859144e-05 -5.566603e-05\n [9,] 1.618941e-03 6.070280e-04 0.0004721104  7.086189e-04  8.917753e-04\n[10,] 7.001615e-04 1.730895e-03 0.0005205689  4.992864e-04  4.276745e-04\n[11,] 5.671980e-04 5.230923e-04 0.0014537943  3.721344e-04  6.949490e-04\n[12,] 3.549025e-04 8.662901e-05 0.0003694086  8.121422e-05  2.048871e-04\n[13,] 1.809069e-03 5.994210e-04 0.0006818643  6.065452e-04  8.608315e-04\n[14,] 6.726168e-04 1.628052e-03 0.0005001348  4.767595e-04  1.955560e-04\n[15,] 7.152364e-04 4.484829e-04 0.0013905614  4.240890e-05  4.525888e-04\n[16,] 2.714269e-03 9.497205e-04 0.0009504702  8.252786e-04  9.374942e-04\n[17,] 9.497205e-04 2.781060e-03 0.0011345975  3.571314e-04  1.293712e-04\n[18,] 9.504702e-04 1.134597e-03 0.0023072463  5.416910e-05  4.234757e-04\n[19,] 8.252786e-04 3.571314e-04 0.0000541691  2.416128e-03  1.043338e-03\n[20,] 9.374942e-04 1.293712e-04 0.0004234757  1.043338e-03  2.288321e-03\n[21,] 8.680063e-05 4.749021e-04 0.0002621942  4.096861e-04  5.441687e-04\n              [,21]\n [1,] -1.452389e-04\n [2,]  4.045822e-05\n [3,] -1.410212e-04\n [4,] -8.502939e-05\n [5,]  1.437967e-04\n [6,]  2.480873e-04\n [7,] -1.299639e-05\n [8,] -3.392847e-05\n [9,]  3.442378e-04\n[10,]  5.256114e-04\n[11,]  3.341346e-04\n[12,] -7.180751e-05\n[13,]  6.989798e-05\n[14,]  4.952616e-04\n[15,]  1.117222e-04\n[16,]  8.680063e-05\n[17,]  4.749021e-04\n[18,]  2.621942e-04\n[19,]  4.096861e-04\n[20,]  5.441687e-04\n[21,]  1.663587e-03\n\n$covHi\n             [,1]         [,2]         [,3]         [,4]         [,5]\n[1,] 0.0014980127 0.0007465455 0.0007793789 0.0007134071 0.0005453263\n[2,] 0.0007465455 0.0010331381 0.0006387837 0.0006398164 0.0005589013\n[3,] 0.0007793789 0.0006387837 0.0008679826 0.0006400363 0.0005827344\n[4,] 0.0007134071 0.0006398164 0.0006400363 0.0008379947 0.0005613310\n[5,] 0.0005453263 0.0005589013 0.0005827344 0.0005613310 0.0013482139\n[6,] 0.0004596982 0.0005289703 0.0005019538 0.0005248655 0.0006078101\n[7,] 0.0004856933 0.0005415021 0.0005393324 0.0005410202 0.0005874255\n             [,6]         [,7]\n[1,] 0.0004596982 0.0004856933\n[2,] 0.0005289703 0.0005415021\n[3,] 0.0005019538 0.0005393324\n[4,] 0.0005248655 0.0005410202\n[5,] 0.0006078101 0.0005874255\n[6,] 0.0011488908 0.0004461349\n[7,] 0.0004461349 0.0010515910\n\n$covH\n             [,1]\n[1,] 0.0006624019\n\n\n\nPossiamo interpretare i coefficienti di scalabilità nel modo seguente.\n\nCoefficiente di Scalabilità tra Coppie di Item (Hij): Per ogni coppia di item (i, j), il coefficiente \\(H_{ij}\\) valuta l’efficacia con cui questi due item riflettono la variabile latente. Un coefficiente \\(H_{ij}\\) positivo per coppie di item appartenenti alla stessa scala di Mokken indica che questi item sono coerenti e misurano efficacemente la stessa variabile latente. Matematicamente, \\(H_{ij}\\) è definito per ogni coppia di item i e j, dove i, j = 1, …, J.\nCoeffiente di Scalabilità dell’Item (Hj): Il coefficiente \\(H_{j}\\) di un singolo item è analogo ai parametri di discriminazione nei modelli IRT parametrici. Esprime l’efficacia con cui un item distingue tra individui a diversi livelli della variabile latente. Per essere considerato efficace, \\(H_{j}\\) dovrebbe superare un certo limite inferiore, generalmente stabilito a c &gt; 0.3.\nCoefficiente di Scalabilità del Test (H): H rappresenta la scalabilità complessiva dell’intero insieme di item. L’interpretazione di H segue le seguenti soglie euristiche:\n\nDebole: se 0.3 ≤ H &lt; 0.4.\nModerato: se 0.4 ≤ H &lt; 0.5.\nForte: se H &gt; 0.5.\n\nQuesti valori indicano la forza con cui l’insieme di item misura la variabile latente. I coefficienti di scalabilità degli item forniscono indicazioni sulla discriminazione degli item e sulla loro aderenza al modello di omogeneità monotona. Item con bassa discriminazione non contribuiscono a un ordinamento affidabile degli esaminandi e dovrebbero essere scartati.\n\nSecondo Sijtsma e Molenaar (2002), le assunzioni di unidimensionalità, indipendenza locale e monotonicità implicano le seguenti restrizioni sui coefficienti di scalabilità: - 0 ≤ \\(H_{ij}\\) ≤ 1, per tutte le coppie di item i ≠ j. - 0 ≤ \\(H_{j}\\) ≤ 1, per tutti gli item j. - 0 ≤ \\(H\\) ≤ 1, per l’intero insieme di item.\nI coefficienti di scalabilità sono fondamentali per valutare quanto efficacemente un insieme di item lavori insieme per misurare una variabile latente. Valori alti di \\(H\\) suggeriscono che l’insieme di item è fortemente correlato e misura in modo affidabile la variabile latente, garantendo che l’analisi con la MSA sia valida e affidabile.\nÈ possibile ottenere il numero di valori negativi \\(H_{ij}\\) per ciascun item usando il codice seguente.\n\nscal_coef &lt;- coefH(good_items, se = FALSE)$Hij\napply(scal_coef, 1, function(x) sum(x &lt; 0)) |&gt; print()\n\n$Hij\n           FAI_49   FAI_106    FAI_60   FAI_124     FAI_5    FAI_81    FAI_83\nFAI_49  1.0000000 0.5613569 0.6075009 0.6005396 0.2118286 0.2539023 0.2229846\nFAI_106 0.5613569 1.0000000 0.6172766 0.7223965 0.3168352 0.3539449 0.3812326\nFAI_60  0.6075009 0.6172766 1.0000000 0.6879216 0.2795178 0.3243635 0.4303254\nFAI_124 0.6005396 0.7223965 0.6879216 1.0000000 0.2879511 0.3813744 0.3866023\nFAI_5   0.2118286 0.3168352 0.2795178 0.2879511 1.0000000 0.4132827 0.4085199\nFAI_81  0.2539023 0.3539449 0.3243635 0.3813744 0.4132827 1.0000000 0.5506000\nFAI_83  0.2229846 0.3812326 0.4303254 0.3866023 0.4085199 0.5506000 1.0000000\n\n$Hi\n   FAI_49   FAI_106    FAI_60   FAI_124     FAI_5    FAI_81    FAI_83 \n0.4096841 0.4929127 0.4838063 0.5019022 0.3231299 0.3862103 0.4040312 \n\n$H\n[1] 0.4279251\n\n FAI_49 FAI_106  FAI_60 FAI_124   FAI_5  FAI_81  FAI_83 \n      0       0       0       0       0       0       0 \n\n\nL’istruzione seguente esegue la procedura frequentista del test di ipotesi, con l’ipotesi alternativa che i coefficienti di scalabilità sono maggiori di zero. Il test è unidirezionale, dunque il valore soglia della statistica \\(z\\) è 1.65.\n\ncoefZ(good_items) |&gt; print()\n\n$Zij\n           FAI_49   FAI_106    FAI_60   FAI_124    FAI_5    FAI_81    FAI_83\nFAI_49   0.000000 11.122254 11.766708 11.519386 4.077595  4.303706  4.213676\nFAI_106 11.122254  0.000000 12.328013 14.049032 6.189922  6.313023  7.368202\nFAI_60  11.766708 12.328013  0.000000 14.039308 5.741312  6.231554  8.542324\nFAI_124 11.519386 14.049032 14.039308  0.000000 5.889676  7.265907  7.779304\nFAI_5    4.077595  6.189922  5.741312  5.889676 0.000000  8.141997  8.214378\nFAI_81   4.303706  6.313023  6.231554  7.265907 8.141997  0.000000 10.794877\nFAI_83   4.213676  7.368202  8.542324  7.779304 8.214378 10.794877  0.000000\n\n$Zi\n  FAI_49  FAI_106   FAI_60  FAI_124    FAI_5   FAI_81   FAI_83 \n18.87934 23.25525 23.49005 24.24297 15.73678 17.72590 19.40083 \n\n$Z\n[1] 37.97973\n\n\n\nNel caso presente, il test indica che i coefficienti di scalabilità di tutti gli item, sia considerati singolarmente sia considerati a coppie, sono maggiori di zero. Lo stesso si può dire per il coefficiente di scalabilità della scala nel suo complesso.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/03_applications.html#monotonicità",
    "href": "chapters/mokken/03_applications.html#monotonicità",
    "title": "66  Applicazione Pratica",
    "section": "66.6 Monotonicità",
    "text": "66.6 Monotonicità\nCome spiegato in precedenza, la monotonicità è un’importante assunzione della MSA. La probabilità di una risposta corretta dovrebbe aumentare con θ. La monotonicità può essere esaminata con il seguente codice:\n\nmonoton &lt;- check.monotonicity(good_items)\nsummary(monoton) |&gt; print()\n\n        ItemH #ac #vi #vi/#ac maxvi sum sum/#ac zmax #zsig crit\nFAI_49   0.41  20   0       0     0   0       0    0     0    0\nFAI_106  0.49  12   0       0     0   0       0    0     0    0\nFAI_60   0.48  18   0       0     0   0       0    0     0    0\nFAI_124  0.50  21   0       0     0   0       0    0     0    0\nFAI_5    0.32  24   0       0     0   0       0    0     0    0\nFAI_81   0.39  24   0       0     0   0       0    0     0    0\nFAI_83   0.40  21   0       0     0   0       0    0     0    0\n\n\nOgni riga dell’output rappresenta un item (ad esempio, FAI_49, FAI_106, ecc.). La spiegazione delle colonne è la seguente:\n\nItemH: Il coefficiente H per ogni item, che misura l’omogeneità dell’item. Un valore più alto indica una maggiore omogeneità. Nelle scale di Mokken, si cercano generalmente valori superiori a 0.3.\n#ac (Active pairs): Il numero di coppie attive, ovvero coppie di risposte che contribuiscono alla stima dell’H.\n#vi (Violations): Il numero di violazioni della monotonicità. La monotonicità implica che, man mano che aumenta il punteggio totale del test, la probabilità di una risposta positiva all’item non diminuisce.\n#vi/#ac: Il rapporto tra il numero di violazioni e il numero di coppie attive.\nmaxvi (Maximum violation): La massima violazione osservata.\nsum (Sum of violation size): La somma delle dimensioni delle violazioni.\nsum/#ac: Il rapporto tra la somma delle dimensioni delle violazioni e il numero di coppie attive.\nzmax: Il valore massimo della statistica Z per le violazioni.\n#zsig (Number of significant Z): Numero di statistiche Z significative.\ncrit: Un criterio per giudicare se le violazioni sono problematiche.\n\nNel caso presente, sembra che non ci siano violazioni della monotonicità per nessuno degli item elencati. Questo significa che per questi item, all’aumentare del punteggio totale, non si osserva una diminuzione della probabilità di una risposta positiva, mantenendo quindi una buona coerenza interna e validità per la scala.\nUn grafico della monotonicità per una coppia di item si ottiene nel modo seguente.\n\nplot(check.monotonicity(good_items), item = c(1, 2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa figura di questo esempio illustra i grafici di monotonicità per gli Item 49 e 106. Il grafico è diviso in due pannelli. Il pannello sul lato sinistro mostra le Funzioni di Risposta del Passaggio dell’Item (ISRFs), mentre il pannello sul lato destro mostra la Funzione di Risposta all’Item (IRF) complessiva per ciascun item. Il grafico evidenzia che sia l’IRF sia le ISRFs sono sempre non decrescenti per l’item 49; per l’item 106, invece, si osserva una piccola violazione della monotonicità.\nSe esaminiamo tutti gli item dell’area relativa alle caratteristiche del bambino (non solo quelli selezionati dalla procedura AISP) notiamo come, per alcuni item, si osserva un numero di violazioni maggiore di zero.\n\nmonoton2 &lt;- check.monotonicity(subscale_data)\nsummary(monoton2) |&gt; print()\n\n        ItemH #ac #vi #vi/#ac maxvi  sum sum/#ac zmax #zsig crit\nFAI_49   0.25  24   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_106  0.30  15   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_60   0.30  18   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_124  0.32  24   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_86   0.16  21   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_47   0.23  24   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_121  0.18  23   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_167  0.18  24   2    0.08  0.04 0.07  0.0031 1.07     0   31\nFAI_99   0.12  24   1    0.04  0.05 0.05  0.0023 1.58     0   31\nFAI_63   0.23  24   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_168  0.23  18   1    0.06  0.06 0.06  0.0032 0.73     0   25\nFAI_5    0.23  24   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_132  0.21  24   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_85   0.17  19   1    0.05  0.07 0.07  0.0035 1.08     0   30\nFAI_81   0.24  24   1    0.04  0.04 0.04  0.0015 0.52     0   17\nFAI_83   0.24  24   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_57   0.21  15   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_91   0.18  21   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_135  0.15  21   1    0.05  0.03 0.03  0.0016 0.39     0   22\nFAI_1    0.10  21   0    0.00  0.00 0.00  0.0000 0.00     0    0",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/03_applications.html#ordinamento-invariante-degli-item",
    "href": "chapters/mokken/03_applications.html#ordinamento-invariante-degli-item",
    "title": "66  Applicazione Pratica",
    "section": "66.7 Ordinamento Invariante degli Item",
    "text": "66.7 Ordinamento Invariante degli Item\nIl passo successivo nella MSA è indagare l’ordinamento invariante degli item (IIO) o la non intersezione delle Funzioni di Risposta all’Item (IRFs). È fondamentale determinare se l’ordine degli item sia lo stesso per tutti i rispondenti con diversi livelli del tratto. Esistono diversi metodi per esaminare l’IIO. Il metodo predefinito nel pacchetto R mokken è l’IIO manifesto o MIIO (Manifest IIO) (Ligtvoet, Van der Ark, Te Marvelde & Sijtsma, 2010). Per esaminare l’ordinamento invariante degli item, eseguiamo il seguente codice.\n\niio &lt;- check.iio(good_items)\nsummary(iio) |&gt; print()\n\n$method\n[1] \"MIIO\"\n\n$item.summary\n        ItemH #ac #vi #vi/#ac maxvi  sum sum/#ac tmax #tsig crit\nFAI_81   0.39  18   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_83   0.40  18   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_124  0.50  18   1    0.06  0.31 0.31  0.0171 3.50     1   83\nFAI_5    0.32  18   2    0.11  0.42 0.73  0.0407 3.50     2  145\nFAI_60   0.48  18   1    0.06  0.42 0.42  0.0236 2.49     1   97\nFAI_106  0.49  18   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_49   0.41  18   0    0.00  0.00 0.00  0.0000 0.00     0    0\n\n$backward.selection\n        step 1 step 2\nFAI_81       0      0\nFAI_83       0      0\nFAI_124      1      0\nFAI_5        2     NA\nFAI_60       1      0\nFAI_106      0      0\nFAI_49       0      0\n\n$HT\n[1] 0.3358101\n\n\n\nL’output include due tabelle principali. La prima tabella contiene le seguenti colonne. La prima colonna, “ItemH”, mostra il coefficiente di scalabilità Hi per ciascun item, ‘#ac’ indica il numero totale di coppie attive, ‘#vi’ segnala il numero totale di violazioni, ‘#vi/#ac’ mostra il numero medio di violazioni per coppia attiva, ‘maxvi’ indica la massima violazione, ‘sum’ rappresenta la somma di tutte le violazioni, ‘sum/#ac’ mostra la media delle violazioni per coppia attiva, ‘tmax’ indica la statistica di test massima, ‘#tsig’ il numero di violazioni significative, e il valore ‘crit’ è una somma ponderata di altri elementi come ‘itemH’, ‘#ac’, ecc. Valori elevati di ‘crit’ indicano item di scarsa qualità (il valore 0 è perfetto, valori più alti sono peggiori).\nL’output precedente mostra che l’Item FAI_5 ha 2 violazioni. In altre parole, la IRF per questo item si interseca con la IRF di altri due item, ed entrambe queste violazioni sono significative (#tsig per questo item è 2). Poiché ha il numero più alto di violazioni, è un buon candidato per essere rimosso dal test. Rimuovere questo item risolve l’intersezione degli altri item con questo item.\nPer esaminare graficamente queste intersezione, usiamo il codice seguente.\n\nplot(iio)",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/03_applications.html#non-intersezione-delle-funzioni-di-risposta-degli-step-degli-item",
    "href": "chapters/mokken/03_applications.html#non-intersezione-delle-funzioni-di-risposta-degli-step-degli-item",
    "title": "66  Applicazione Pratica",
    "section": "66.8 Non intersezione delle funzioni di risposta degli step degli item",
    "text": "66.8 Non intersezione delle funzioni di risposta degli step degli item\nPer indagare sulla non intersezione delle funzioni di risposta degli step degli item si possono impiegare le matrici P++ e P– (Molenaar & Sijtsma, 2000).\n\npmatrix &lt;- check.pmatrix(good_items)\nsummary(pmatrix) |&gt; print()\n\n        ItemH  #ac #vi #vi/#ac maxvi  sum sum/#ac zmax #zsig crit\nFAI_49   0.41 1920  17    0.01  0.05 0.65   3e-04 2.93    15   67\nFAI_106  0.49 1920  10    0.01  0.06 0.39   2e-04 4.21    10   59\nFAI_60   0.48 1920  23    0.01  0.06 0.94   5e-04 3.88    22   81\nFAI_124  0.50 1920  20    0.01  0.06 0.84   4e-04 4.60    20   80\nFAI_5    0.32 1920  25    0.01  0.06 1.07   6e-04 3.88    24   92\nFAI_81   0.39 1920  28    0.01  0.05 1.09   6e-04 3.60    27   89\nFAI_83   0.40 1920  17    0.01  0.06 0.68   4e-04 4.60    16   78\n\n\nCome spiegato nelle sezioni precedenti, la colonna ‘#vi’ indica il numero di violazioni delle ISRF. La colonna ‘#zsig’ mostra il numero di violazioni statisticamente significative. Le violazioni possono anche essere controllate graficamente utilizzando il seguente codice:\n\nplot(check.pmatrix(good_items), item = 5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa non intersezione delle ISRF può essere esaminata anche con il metodo del punteggio residuo.\n\nrestscore &lt;- check.restscore(good_items) \nsummary(restscore) |&gt; print()\n\n        ItemH #ac #vi #vi/#ac maxvi  sum sum/#ac zmax #zsig crit\nFAI_49   0.41 288   5    0.02  0.08 0.23  0.0008 1.19     0   18\nFAI_106  0.49 288   7    0.02  0.07 0.33  0.0011 2.16     2   34\nFAI_60   0.48 288   6    0.02  0.14 0.47  0.0016 2.19     4   50\nFAI_124  0.50 288   9    0.03  0.15 0.56  0.0019 2.78     1   44\nFAI_5    0.32 288  14    0.05  0.15 1.01  0.0035 2.78     5   74\nFAI_81   0.39 288   8    0.03  0.10 0.44  0.0015 1.70     1   37\nFAI_83   0.40 288   9    0.03  0.07 0.40  0.0014 2.07     1   36\n\n\nI risultati del metodo del punteggio residuo sono diversi da quelli delle matrici P++ e P–. L’output viene interpretato come abbiamo fatto in precedenza. La colonna ‘#vi’ indica il numero di violazioni delle ISRF. La colonna ‘#zsig’ mostra il numero di violazioni statisticamente significative. Questa colonna mostra che il numero di violazioni statisticamente significative è molto inferiore rispetto a quelle riportate dalle matrici P++ e P–. La violazione della non intersezione può essere esaminata graficamente nel modo seguente. Per esempio, consideriamo gli item FAI_49 e FAI_5.\n\nplot(restscore, item.pairs = c(4, 5))",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/03_applications.html#affidabilità",
    "href": "chapters/mokken/03_applications.html#affidabilità",
    "title": "66  Applicazione Pratica",
    "section": "66.9 Affidabilità",
    "text": "66.9 Affidabilità\nIl pacchetto Mokken calcola quattro diversi coefficienti di affidabilità: l’affidabilità della scala Mokken (MS) ρ, (Mokken, 1971), Lambda-2 (Guttman, 1945), l’alpha di Cronbach (Cronbach, 1951), e il coefficiente di affidabilità della classe latente (LCRC, van der Ark, van der Palm, & Sijtsma, 2011).\n\nMS (Molenaar-Sijtsma Method): Questo indice è basato sul metodo Molenaar-Sijtsma di stima dell’affidabilità per scale non parametriche, come quelle analizzate con l’analisi Mokken. Questo metodo considera la varianza tra gli item e la varianza totale per stimare l’affidabilità.\nAlpha (Cronbach’s Alpha): L’alpha di Cronbach è forse il più noto indice di affidabilità, utilizzato per valutare la consistenza interna degli item di un test. Misura fino a che punto gli item di un test sono correlati tra loro.\nLambda-2: Un altro indice di affidabilità, simile all’alpha di Cronbach, ma talvolta considerato più robusto poiché tiene conto delle correlazioni medie tra gli item.\nLCRC (Latent Class Reliability Coefficient): Questo è un indice di affidabilità che tiene conto dell’approccio delle classi latenti. È particolarmente utile quando gli item possono essere raggruppati in sottoscale che riflettono diversi costrutti o dimensioni.\n\nPer ottenere queste stime dell’affidabilità è possibile eseguire il seguente codice.\n\ncheck.reliability(good_items, LCRC = TRUE)\n\n\n    $MS\n        0.819110476790128\n    $alpha\n        0.81675073603741\n    $lambda.2\n        0.824148383038108\n    $LCRC\n        0.843559865692261\n\n\n\nIn generale, tutti i valori ottenuti indicano che la scala in considerazione ha un’alta affidabilità. Questo significa che è probabile che produca risultati coerenti nel tempo e che gli item che la compongono siano correlati tra loro in modo significativo, contribuendo tutti a misurare lo stesso costrutto o costrutti correlati.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/03_applications.html#identificazione-degli-outlier",
    "href": "chapters/mokken/03_applications.html#identificazione-degli-outlier",
    "title": "66  Applicazione Pratica",
    "section": "66.10 Identificazione degli Outlier",
    "text": "66.10 Identificazione degli Outlier\nGli outlier sono persone con modelli di risposta aberranti o molti errori di Guttman. Sijtsma e van der Ark (2017) raccomandano di rimuovere gli outlier e di rieseguire l’analisi senza di essi. Se c’è una differenza notevole nei risultati, allora la rimozione degli outlier è giustificata. Per ottenere il numero di errori di Guttman per ogni rispondente eseguiamo il seguente codice:\n\ngutt &lt;- check.errors(good_items)\nprint(gutt)\n\n$Gplus\n  [1]  2  6  0  7  2  4 23 30  2  7 15  0  1  9  2  0  5 24  3  3  0  0 11 51  0\n [26]  4  2 19  9  2 60  2  9 35  2 17 39  2  4  3  3 12  3  0 10  2  0  0 37  4\n [51] 68  0  2 31  2  0 12 36  1 70 12  1 28 27 25  3  4 10  9 16 28 22 81 26  1\n [76]  0 65 28  0 25  3 59 60 40  0  5  0  0  1  0  8  3  8  6 27 12 22 16 12  4\n[101] 12  9  0  0 20 25 25  6  4  6  0 47 20 18  4  7  6 33 14 25 12  4  0  1 65\n[126] 17 83 26 65  5  5 30  2  9 42  7 13  8 16  3 41  0 12 21  4  8 26 22  3 46\n[151]  7 14 38 12 12  1  6  4 32 11  2  6 17  1  8  8 27  8  3  5 58  2 62  3  0\n[176] 12 10 45 21  0  2 10 31 19 46 11 21  2 25  9 17  2  4 20 21 49 18 19 14 16\n[201] 44  5  7 14 41 24 44  0 18  9 31 12  9 46  8  6 11 14 23 41 24  1  7 39 23\n[226]  0 23  3  6  2  2  4  0  0  8 24 38  0 16 62  2  2 21  7 25  9  0  0  8 14\n[251] 10  4  1  0  2 18  0 21 19  0 12  0 41  8  7 16 25 20  3 41  3  0  6 18  0\n[276]  4  2  4  4 12  5 13  6  9  7  6  0  6  7  2  0  0 13 26  8 41 30  2  0  6\n[301]  6  0  5  2  0  0 39 48  0 43  8 15 10  0  4 25 16  9  0  5  2  2  8  2  8\n[326]  7  1  0 53 32  9  3  0  4 13  3  6 35 30  2 34  8 10 10  5 23 22 24 12  3\n[351]  0  9 16 21 39 27 47 19 28 32  7  2 16  0  3 26 34  6  4 11  7 25  1 15  6\n[376] 10 38 17  0  6  3 38  4  0 52  5  2  3  0  0 14  0 27 22 32  8 32 14  2 31\n[401] 14 29 52 12 11 32  2  3 17  2  0  5 62 86 34  5 24 39  7 10 31  0  0 25 10\n[426] 19 31 13  7 31 36  8  0  0 28  6 23 15 52  3  1  7  4 33 24 11 24  0  2  3\n[451]  0  1  0\n\n$UGplus\n$UGplus$U1Gplus\n[1] 54.5\n\n$UGplus$U2Gplus\n[1] 354.5259\n\n\n\n\nIl vettore $Gplus mostra il numero di errori di Guttman G+ per ogni persona. I valori 54.5 e 354.5259 sono le Tukey fences per il rilevamento degli outlier. Essi segnalano i casi in cui il numero di errori di Guttman G+ è oltre il Tukey fence della distribuzione di G+.\nIl primo valore, 54.5, è il limite per il numero di errori di Guttman se la distribuzione è approssimativamente normale. I rispondenti con un numero di errori di Guttman superiore al limite possono essere considerati sospetti. Il valore 354.5259 è un limite di soglia per il numero di errori di Guttman se la distribuzione è asimmetrica.\n\nhist(gutt$Gplus)\n\n\n\n\n\n\n\n\nPer il caso presente, l’istogramma del numero di errori di Gutmann mostra che la loro distribuzione è asimmetrica positiva. Pertanto, possiamo scegliere il secondo limite soglia. Per trovare le persone con valori G+ sopra il Tucky fence superiore, eseguiamo il seguente codice.\n\nerr &lt;- gutt$Gplus \nwhich(err &gt; 354.5259)\n\n\n\n\nI risultati indicano che, in base a questo secondo criterio, non ci sono modelli di risposta sospetti nei dati.\nSe questa procedura producesse invece come risultato l’individuazione di alcuni outlier, potremmo rimuoverli dai dati nel modo seguente:\ngood_items_clean &lt;- good_items[-which(err &gt; 354.5259), ]",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/03_applications.html#considerazioni-conclusive",
    "href": "chapters/mokken/03_applications.html#considerazioni-conclusive",
    "title": "66  Applicazione Pratica",
    "section": "66.11 Considerazioni conclusive",
    "text": "66.11 Considerazioni conclusive\nIn questo capitolo, abbiamo esplorato un approccio pratico all’analisi di un set di dati reali, utilizzando il caso di studio sull’indagine della capacità di coping delle famiglie di fronte alla diagnosi di una grave malattia in un figlio, condotta dai ricercatori del Meyer. Attraverso una serie di passaggi metodici e analitici rigorosi, siamo riusciti a trasformare un complesso insieme di dati in informazioni comprensibili e significative.\nAbbiamo iniziato importando i dati e conducendo un’accurata pulizia per rimuovere gli item con eccessiva asimmetria e curtosi. Successivamente, abbiamo implementato la Procedura di Selezione Automatica degli Item (AISP) nell’ambito dell’Analisi delle Scale Mokken (MSA) per identificare e selezionare scale omogenee e coerenti.\nUna volta stabilite le scale, abbiamo approfondito le caratteristiche degli item, esaminando le loro statistiche descrittive e la loro distribuzione. Abbiamo anche valutato la loro affidabilità attraverso vari coefficienti, tra cui l’Alpha di Cronbach e il Coefficiente di Affidabilità della Classe Latente (LCRC), rilevando un’alta coerenza interna.\nUlteriori analisi hanno incluso la verifica della monotonicità degli item e dell’ordinamento invariante degli item (IIO), fondamentali per garantire che la scala rispettasse i principi teorici sottostanti la MSA. Infine, abbiamo esaminato la presenza di outlier, utilizzando i Tukey fences per identificare e gestire i modelli di risposta aberranti.\nIn conclusione, questo capitolo dimostra come l’applicazione metodica e sistematica delle tecniche di analisi statistica possa fornire intuizioni preziose e comprensibili da un set di dati complesso. Ciò non solo rafforza la validità e l’affidabilità della ricerca, ma fornisce anche una base solida per ulteriori indagini e interpretazioni.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/03_applications.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/mokken/03_applications.html#informazioni-sullambiente-di-sviluppo",
    "title": "66  Applicazione Pratica",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats4    stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] mokken_3.1.2         poLCA_1.6.0.1        MASS_7.3-61         \n [4] scatterplot3d_0.3-44 mirt_1.42            lattice_0.22-6      \n [7] TAM_4.2-21           CDM_8.2-6            mvtnorm_1.3-1       \n[10] ggokabeito_0.1.0     viridis_0.6.5        viridisLite_0.4.2   \n[13] ggpubr_0.6.0         ggExtra_0.10.1       bayesplot_1.11.1    \n[16] gridExtra_2.3        patchwork_1.3.0      semTools_0.5-6      \n[19] semPlot_1.1.6        lavaan_0.6-18        psych_2.4.6.26      \n[22] scales_1.3.0         markdown_1.13        knitr_1.48          \n[25] lubridate_1.9.3      forcats_1.0.0        stringr_1.5.1       \n[28] dplyr_1.1.4          purrr_1.0.2          readr_2.1.5         \n[31] tidyr_1.3.1          tibble_3.2.1         ggplot2_3.5.1       \n[34] tidyverse_2.0.0      here_1.0.1          \n\nloaded via a namespace (and not attached):\n  [1] splines_4.4.1        later_1.3.2          pbdZMQ_0.3-13       \n  [4] R.oo_1.26.0          XML_3.99-0.17        rpart_4.1.23        \n  [7] lifecycle_1.0.4      rstatix_0.7.2        rprojroot_2.0.4     \n [10] globals_0.16.3       rockchalk_1.8.157    backports_1.5.0     \n [13] magrittr_2.0.3       openxlsx_4.2.7.1     Hmisc_5.1-3         \n [16] rmarkdown_2.28       httpuv_1.6.15        qgraph_1.9.8        \n [19] zip_2.3.1            sessioninfo_1.2.2    pbapply_1.7-2       \n [22] minqa_1.2.8          audio_0.1-11         multcomp_1.4-26     \n [25] abind_1.4-8          quadprog_1.5-8       R.utils_2.12.3      \n [28] nnet_7.3-19          TH.data_1.1-2        sandwich_3.1-1      \n [31] listenv_0.9.1        testthat_3.2.1.1     RPushbullet_0.3.4   \n [34] vegan_2.6-8          arm_1.14-4           parallelly_1.38.0   \n [37] permute_0.9-7        codetools_0.2-20     tidyselect_1.2.1    \n [40] farver_2.1.2         lme4_1.1-35.5        base64enc_0.1-3     \n [43] jsonlite_1.8.9       polycor_0.8-1        progressr_0.14.0    \n [46] Formula_1.2-5        survival_3.7-0       emmeans_1.10.4      \n [49] tools_4.4.1          snow_0.4-4           Rcpp_1.0.13         \n [52] glue_1.7.0           mnormt_2.1.1         xfun_0.47           \n [55] mgcv_1.9-1           admisc_0.36          IRdisplay_1.1       \n [58] withr_3.0.1          beepr_2.0            fastmap_1.2.0       \n [61] boot_1.3-31          fansi_1.0.6          digest_0.6.37       \n [64] mi_1.1               timechange_0.3.0     R6_2.5.1            \n [67] mime_0.12            estimability_1.5.1   colorspace_2.1-1    \n [70] gtools_3.9.5         jpeg_0.1-10          R.methodsS3_1.8.2   \n [73] utf8_1.2.4           generics_0.1.3       data.table_1.16.0   \n [76] corpcor_1.6.10       SimDesign_2.17.1     htmlwidgets_1.6.4   \n [79] pkgconfig_2.0.3      sem_3.1-16           gtable_0.3.5        \n [82] brio_1.1.5           htmltools_0.5.8.1    carData_3.0-5       \n [85] png_0.1-8            rstudioapi_0.16.0    tzdb_0.4.0          \n [88] reshape2_1.4.4       uuid_1.2-1           curl_5.2.3          \n [91] coda_0.19-4.1        checkmate_2.3.2      nlme_3.1-166        \n [94] nloptr_2.1.1         repr_1.1.7           zoo_1.8-12          \n [97] parallel_4.4.1       miniUI_0.1.1.1       foreign_0.8-87      \n[100] pillar_1.9.0         grid_4.4.1           vctrs_0.6.5         \n[103] promises_1.3.0       car_3.1-2            OpenMx_2.21.12      \n[106] xtable_1.8-4         Deriv_4.1.6          cluster_2.1.6       \n[109] dcurver_0.9.2        GPArotation_2024.3-1 htmlTable_2.4.3     \n[112] evaluate_1.0.0       pbivnorm_0.6.0       cli_3.6.3           \n[115] kutils_1.73          compiler_4.4.1       rlang_1.1.4         \n[118] crayon_1.5.3         future.apply_1.11.2  ggsignif_0.6.4      \n[121] fdrtool_1.2.18       plyr_1.8.9           stringi_1.8.4       \n[124] munsell_0.5.1        lisrelToR_0.3        pacman_0.5.1        \n[127] Matrix_1.7-0         IRkernel_1.3.2       hms_1.1.3           \n[130] glasso_1.11          future_1.34.0        shiny_1.9.1         \n[133] igraph_2.0.3         broom_1.0.6          RcppParallel_5.1.9",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_rasch_model.html",
    "href": "chapters/irt/01_rasch_model.html",
    "title": "67  Modello di Rasch",
    "section": "",
    "text": "67.1 Introduzione\nLa psicometria ha compiuto notevoli progressi con l’introduzione della Teoria della Risposta all’Item (IRT), un approccio innovativo che supera i limiti della Classica Teoria dei Test (CTT), particolarmente nella gestione e nella concettualizzazione degli errori di misurazione. A differenza della CTT, che si concentra sull’analisi del punteggio totale di un test, l’IRT esamina le risposte ai singoli item, fornendo una visione più dettagliata delle capacità individuali. Questa analisi si fonda su due elementi fondamentali: le caratteristiche degli item, che includono parametri come la difficoltà e la capacità discriminativa, e il livello di abilità del rispondente, che rappresenta la sua posizione su un continuum latente della capacità.\nIl nucleo dell’IRT è la modellizzazione della probabilità che un individuo risponda correttamente a un determinato item, espressa come funzione del livello di abilità del rispondente e delle caratteristiche dell’item stesso. Questa relazione viene rappresentata graficamente attraverso le Curve Caratteristiche degli Item, che illustrano come la probabilità di risposta corretta varia in base al livello di abilità.\nL’IRT presenta numerosi vantaggi. Offre una notevole flessibilità, consentendo di modellare sia risposte dicotomiche che categoriali o ordinali, molto comuni nei test psicometrici. L’analisi a livello di item permette di identificare item problematici, come quelli troppo facili o incapaci di discriminare tra rispondenti con diversi livelli di abilità. Inoltre, le stime di abilità non dipendono dagli item specifici somministrati, rendendo i test basati sull’IRT particolarmente adatti per applicazioni adattive. L’IRT fornisce anche una funzione di informazione del test che permette di valutare la precisione della misurazione per diversi livelli di abilità.\nTuttavia, l’IRT presenta anche alcune sfide. La stima affidabile dei parametri richiede campioni di dimensioni maggiori rispetto alla CTT, e i modelli, soprattutto quelli multidimensionali, necessitano di una padronanza di tecniche statistiche avanzate.\nNel contesto della psicometria moderna, è importante considerare anche la Mokken Scale Analysis (MSA), un approccio non parametrico per l’analisi di scale psicometriche. A differenza dell’IRT, la MSA non presuppone una forma specifica per le curve caratteristiche degli item, basandosi invece su criteri meno restrittivi come la monotonicità e l’indipendenza locale. La MSA risulta particolarmente utile per verificare la struttura della scala e individuare item problematici, rappresentando spesso un ottimo punto di partenza per analisi preliminari, specialmente con campioni di dimensioni ridotte.\nUn approccio efficace all’analisi psicometrica può prevedere l’integrazione di MSA e IRT. Si può iniziare con una valutazione preliminare degli item e della scala attraverso la MSA, per poi passare a un’analisi più approfondita mediante l’IRT. Questa combinazione permette di sfruttare i punti di forza di entrambi gli approcci: la MSA offre una valutazione iniziale della scalabilità e della coerenza interna degli item, mentre l’IRT fornisce una modellizzazione dettagliata degli item e delle abilità latenti, con una precisa valutazione della scala.\nIn conclusione, l’IRT rappresenta una pietra miliare nella psicometria moderna, offrendo strumenti sofisticati per l’analisi di item e abilità latenti. Tuttavia, la Mokken Scale Analysis mantiene la sua utilità, particolarmente in fasi esplorative o in contesti con dati limitati. Una comprensione integrata di entrambi gli approcci consente una valutazione più completa e robusta delle scale psicometriche, permettendo ai ricercatori di scegliere gli strumenti più appropriati in base alle specifiche esigenze del loro studio.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_rasch_model.html#curve-caratteristiche-degli-item",
    "href": "chapters/irt/01_rasch_model.html#curve-caratteristiche-degli-item",
    "title": "67  Modello di Rasch",
    "section": "\n67.2 Curve Caratteristiche degli Item",
    "text": "67.2 Curve Caratteristiche degli Item\nUn concetto fondamentale nell’IRT è la Curva Caratteristica dell’Item (Item Characteristic Curve, ICC), che rappresenta graficamente la relazione tra il livello di abilità latente di un individuo e la sua probabilità di rispondere correttamente a un determinato item o di manifestare un comportamento specifico. Questa relazione viene tipicamente modellata attraverso una funzione logistica che genera una curva a forma di S (sigmoide), dove la probabilità di risposta corretta varia in modo sistematico con il livello di abilità: è molto bassa per individui con scarse capacità, aumenta rapidamente nella fascia intermedia e tende asintoticamente al massimo per i livelli più elevati di abilità, senza mai raggiungerlo completamente.\nLe ICC rappresentano uno strumento prezioso per valutare la qualità degli item di un test, fornendo informazioni cruciali su diversi aspetti. Attraverso l’analisi della forma e dei parametri di una ICC, è possibile determinare la difficoltà dell’item, misurata dal livello di abilità necessario per ottenere una probabilità del 50% di risposta corretta. Si può inoltre valutare la discriminatività dell’item, ovvero la sua capacità di distinguere efficacemente tra individui con diversi livelli di abilità. Nei modelli più sofisticati, è anche possibile stimare la probabilità che un rispondente con bassa abilità fornisca una risposta corretta per caso.\nL’analisi delle ICC permette inoltre di identificare diverse problematiche comuni negli item. L’effetto soffitto si manifesta quando un item risulta troppo facile, portando quasi tutti i partecipanti a rispondere correttamente indipendentemente dal loro livello di abilità, rendendo così l’item poco utile per la misurazione del costrutto. Al contrario, l’effetto pavimento si verifica quando un item è talmente difficile che solo gli individui con abilità molto elevate riescono a rispondere correttamente. Un altro problema frequente è la bassa discriminatività, che si presenta quando l’item non riesce a differenziare efficacemente tra individui con diversi livelli di abilità.\nLe Curve Caratteristiche degli Item si rivelano quindi uno strumento essenziale nell’ambito dell’IRT, offrendo una rappresentazione chiara e dettagliata della relazione tra le abilità latenti e le prestazioni sugli item. La loro capacità di evidenziare le caratteristiche specifiche di ciascun item le rende fondamentali per il miglioramento della qualità e dell’affidabilità dei test psicometrici. Attraverso l’analisi delle ICC, è possibile ottimizzare la misurazione, assicurando che ogni item contribuisca in modo efficace alla valutazione del costrutto di interesse. Quando vengono identificati problemi rilevanti attraverso l’analisi delle ICC, gli item possono essere sottoposti a revisione o, se necessario, eliminati dal test, garantendo così la massima precisione nella misurazione dei costrutti psicologici.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_rasch_model.html#la-scala-di-guttman",
    "href": "chapters/irt/01_rasch_model.html#la-scala-di-guttman",
    "title": "67  Modello di Rasch",
    "section": "\n67.3 La Scala di Guttman",
    "text": "67.3 La Scala di Guttman\nPer comprendere meglio la teoria alla base delle ICC, è utile iniziare dalla Scala di Guttman, che stabilisce una relazione gerarchica tra la difficoltà degli item e le abilità degli individui. In una Scala di Guttman ideale, si assume che una persona con un determinato livello di abilità risponda correttamente a tutti gli item meno difficili e sbagli quelli più difficili. Questo modello si può rappresentare attraverso una matrice di risposte dove 1 indica una risposta corretta e 0 una risposta errata: in una scala perfetta, le risposte corrette si accumulano progressivamente man mano che il livello di abilità dell’individuo aumenta.\nLa seguente tabella mostra un esempio di una Scala di Guttman perfetta per cinque item.\n\n\n\n\n\n\n\n\n\n\nPattern di risposta\nItem 1\nItem 2\nItem 3\nItem 4\nItem 5\n\n\n\n1\n0\n0\n0\n0\n0\n\n\n2\n1\n0\n0\n0\n0\n\n\n3\n1\n1\n0\n0\n0\n\n\n4\n1\n1\n1\n0\n0\n\n\n5\n1\n1\n1\n1\n0\n\n\n6\n1\n1\n1\n1\n1\n\n\n\nIn questo modello ideale, le risposte corrette si accumulano man mano che il livello di abilità dell’individuo aumenta.\nGraficamente, la Scala di Guttman può essere rappresentata tramite curve caratteristiche degli item che mostrano, sull’asse verticale, la probabilità di rispondere correttamente a un item (che nel modello ideale è binaria: 1 o 0) e sull’asse orizzontale il livello di abilità dell’individuo. In questo modello ideale, un individuo risponde correttamente a tutti gli item con difficoltà inferiore o uguale al proprio livello di abilità, mentre sbaglia quelli con difficoltà superiore, creando così un modello di risposta perfettamente prevedibile.\nLe frecce nel grafico seguente rappresentano cinque individui con diversi livelli di abilità. Ogni freccia indica il punto in cui l’abilità di una persona interseca le curve caratteristiche degli item. Secondo il modello ideale, ogni persona dovrebbe rispondere correttamente a tutti gli item posizionati a sinistra della propria abilità sul grafico (item meno difficili) e sbagliare quelli a destra (item più difficili).\n\n\n\n\n\n\n\n\nTuttavia, questo modello deterministico raramente si osserva nei dati reali, dove intervengono numerosi fattori che introducono variabilità nelle risposte. Le persone con lo stesso livello di abilità possono rispondere diversamente agli stessi item, gli item possono presentare ambiguità nella loro formulazione, e fattori come motivazione, attenzione e contesto possono influenzare significativamente le risposte.\nÈ proprio da queste limitazioni del modello di Guttman che emerge la necessità di un approccio probabilistico, che viene sviluppato nel modello di Rasch. Quest’ultimo mantiene l’idea fondamentale della relazione gerarchica tra abilità e difficoltà, ma introduce una componente probabilistica che permette di gestire le deviazioni dal modello ideale, offrendo così una rappresentazione più realistica del processo di risposta agli item.\nIl modello di Rasch, che verrà approfondito nella prossima sezione, può essere visto come un’evoluzione naturale della Scala di Guttman, dove le transizioni nette tra risposta corretta e incorretta vengono sostituite da una curva logistica che descrive la probabilità di risposta corretta come una funzione continua dell’abilità. Questo passaggio da un modello deterministico a uno probabilistico rappresenta un avanzamento fondamentale nella teoria psicometrica, permettendo una modellizzazione più accurata e flessibile dei dati reali.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_rasch_model.html#il-modello-di-rasch",
    "href": "chapters/irt/01_rasch_model.html#il-modello-di-rasch",
    "title": "67  Modello di Rasch",
    "section": "\n67.4 Il Modello di Rasch",
    "text": "67.4 Il Modello di Rasch\nIl modello di Rasch rappresenta un’importante evoluzione nella psicometria, consentendo di superare i limiti dello scaling di Guttman. Questo modello si basa sul concetto di variabile latente, una caratteristica non direttamente osservabile ma inferita attraverso comportamenti misurabili, come le risposte a un test. Ad esempio, la competenza matematica può essere vista come una variabile latente stimata analizzando le risposte corrette e errate di un individuo in un test composto da più domande.\nSecondo il modello di Rasch, sia la competenza degli individui sia la difficoltà degli item sono rappresentate lungo un continuum latente. Gli individui con abilità più elevate si trovano verso l’estremità superiore del continuum, mentre quelli con abilità inferiori si collocano verso l’estremità inferiore. Analogamente, le domande del test (gli item) sono posizionate lungo il continuum in base alla loro difficoltà, indicata dal parametro \\(\\beta_i\\) per ciascun item \\(i\\). La posizione di una persona sul continuum è rappresentata dal parametro \\(\\theta_p\\), che riflette il livello di abilità latente dell’individuo \\(p\\).\nLa probabilità che un partecipante risponda correttamente a un determinato item dipende dalla differenza tra l’abilità del partecipante (\\(\\theta_p\\)) e la difficoltà dell’item (\\(\\beta_i\\)). Se \\(\\theta_p\\) è molto maggiore di \\(\\beta_i\\), la probabilità di una risposta corretta è alta; se \\(\\theta_p\\) è inferiore a \\(\\beta_i\\), la probabilità è bassa. Quando \\(\\theta_p\\) è circa uguale a \\(\\beta_i\\), la probabilità si avvicina al 50%, riflettendo incertezza nella risposta.\n\nEsercizio 67.1 Consideriamo un test con cinque domande (item) utilizzato per valutare la competenza matematica. Le risposte dei partecipanti (corrette o errate) costituiscono i dati osservabili. La difficoltà di ciascun item può essere stimata calcolando la proporzione di risposte corrette per ogni domanda.\nImportiamo i dati in R.\n\n# Importazione dei dati\nmath_dat &lt;- rio::import(here::here(\"data\", \"deAyala\", \"Math.txt\"))\nhead(math_dat)\n#&gt;   V1 V2 V3 V4 V5\n#&gt; 1  1  1  0  0  0\n#&gt; 2  1  1  1  0  0\n#&gt; 3  1  0  0  0  0\n#&gt; 4  1  1  1  0  0\n#&gt; 5  1  0  1  1  0\n#&gt; 6  1  1  1  0  0\n\n# Calcolo della difficoltà degli item\ncolMeans(math_dat)\n#&gt;     V1     V2     V3     V4     V5 \n#&gt; 0.8875 0.6440 0.5660 0.4270 0.3873\n\nGli item sono ordinati per difficoltà crescente: il primo è il più facile, mentre l’ultimo è il più difficile.\nPer analizzare la relazione tra il punteggio totale di ciascun partecipante e la proporzione di risposte corrette per ogni item, possiamo generare un grafico.\n\n# Calcolo dei punteggi totali\nmath_dat2 &lt;- math_dat\nmath_dat2$total_score &lt;- rowSums(math_dat2[, -1])\n\n# Preparazione dati per il grafico\nplot_data &lt;- lapply(names(math_dat2)[1:5], function(item) {\n    math_dat2 %&gt;%\n        group_by(total_score) %&gt;%\n        summarise(proportion = mean(get(item) == 1)) %&gt;%\n        mutate(item = item)\n})\n\nplot_data &lt;- do.call(rbind, plot_data)\n\n# Creazione del grafico\nggplot(\n  plot_data, \n  aes(x = total_score, y = proportion, group = item, color = item)\n  ) +\n    geom_line(linewidth = 1.5) +\n    labs(\n        x = \"Punteggio Totale\",\n        y = \"Proporzione di Risposte Corrette\",\n        title = \"Proporzione di Risposte Corrette\\nin Base al Punteggio Totale\"\n    )\n\n\n\n\n\n\n\n\n\n67.4.1 Curve Caratteristiche degli Item (ICC)\nLe curve caratteristiche degli item (ICC) forniscono una rappresentazione grafica della probabilità di risposta corretta in funzione del livello di abilità latente. Nel modello di Rasch, questa relazione è descritta da una funzione logistica:\n\\[\nP(X_{pi} = 1 | \\theta_p, \\beta_i) = \\frac{1}{1 + e^{-(\\theta_p - \\beta_i)}}.\n\\tag{67.1}\\]\nQuesta equazione mostra che la probabilità di una risposta corretta è determinata esclusivamente dalla differenza tra \\(\\theta_p\\) (abilità del partecipante) e \\(\\beta_i\\) (difficoltà dell’item). La forma sigmoide della curva riflette tre situazioni:\n\nquando \\(\\theta_p\\) è molto maggiore di \\(\\beta_i\\), la probabilità è vicina a 1;\nquando \\(\\theta_p\\) è molto minore di \\(\\beta_i\\), la probabilità è vicina a 0;\nquando \\(\\theta_p\\) è circa uguale a \\(\\beta_i\\), la probabilità è prossima a 0.5.\n\nNel grafico delle ICC, gli item facili (\\(\\beta_i &lt; 0\\)) mostrano alte probabilità di risposta corretta anche per partecipanti con abilità modeste, mentre gli item difficili (\\(\\beta_i &gt; 0\\)) richiedono abilità elevate per ottenere una risposta corretta.\n\n# Creazione del modello di Rasch e grafico delle ICC\nrasch_model &lt;- rasch(math_dat)\nplot(rasch_model, type = \"ICC\")\n\n\n\n\n\n\n\nLe curve mostrano che, nel modello di Rasch, la difficoltà degli item è il parametro principale che varia, mentre la pendenza delle curve rimane costante, confermando l’assunto fondamentale del modello.\n\n67.4.2 Interpretazione Pratica\nNel contesto dei test psicometrici, il modello di Rasch dell’Equazione 67.1 offre un approccio rigoroso per interpretare i dati dei test psicometrici. La stima delle difficoltà degli item (\\(\\beta_i\\)) e delle abilità dei partecipanti (\\(\\theta_p\\)) permette di verificare la coerenza tra gli item e di identificare eventuali problematiche, come:\n\n\nItem troppo facili: risolti correttamente da quasi tutti i partecipanti, indipendentemente dal livello di abilità.\n\nItem troppo difficili: risolti solo dai partecipanti con abilità molto elevate, contribuendo poco alla misurazione complessiva.\n\nLa relazione tra abilità e difficoltà è descritta dall’Equazione 67.1. Questa equazione trasforma la differenza \\(\\theta_p - \\beta_i\\), che teoricamente può variare da \\(-\\infty\\) a \\(+\\infty\\), in una probabilità compresa tra 0 e 1. In termini pratici:\n\nQuando \\(\\theta_p\\) è molto maggiore di \\(\\beta_i\\), la probabilità di una risposta corretta è vicina a 1.\nQuando \\(\\theta_p\\) è molto minore di \\(\\beta_i\\), la probabilità si avvicina a 0.\nQuando \\(\\theta_p \\approx \\beta_i\\), la probabilità è circa 0.5, indicando incertezza.\n\nSebbene i parametri \\(\\theta_p\\) (abilità) e \\(\\beta_i\\) (difficoltà) possano teoricamente assumere qualsiasi valore, nella pratica si collocano generalmente tra -3 e +3. Questo intervallo rappresenta una scala standardizzata, utile per interpretare i livelli di abilità e difficoltà:\n\n\nItem facili (\\(\\beta_i &lt; 0\\)): sono generalmente risolti correttamente anche da persone con abilità modeste.\n\nItem difficili (\\(\\beta_i &gt; 0\\)): richiedono abilità elevate per essere superati.\n\nItem intermedi (\\(\\beta_i \\approx 0\\)): massima capacità discriminativa vicino alla media.\n\nQuesta rappresentazione permette di identificare con precisione le caratteristiche di ciascun item e di valutare se contribuisce efficacemente alla misurazione del tratto latente. Ad esempio, item che risultano troppo facili o troppo difficili forniscono meno informazioni utili rispetto a quelli con difficoltà intermedia.\nIn sintesi, il modello di Rasch è uno strumento fondamentale per costruire test psicometrici affidabili, in grado di misurare con precisione abilità, atteggiamenti e tratti di personalità.\n\n67.4.3 Rappresentazioni Alternative della Funzione Logistica\nLa funzione logistica utilizzata nel modello di Rasch può essere scritta in due modi: con la funzione esponenziale sia al numeratore sia al denominatore (a sinistra), oppure equivalentemente con la funzione esponenziale solo al denominatore, seguita dal suo argomento negativo (a destra):\n\\[\n\\frac{\\exp(\\theta_p - \\beta_i)}{1 + \\exp(\\theta_p - \\beta_i)} = \\frac{1}{1 + \\exp(-(\\theta_p - \\beta_i))}\n\\]\nPer dimostrare l’equivalenza delle due espressioni della funzione logistica nel modello di Rasch, seguiamo i seguenti passaggi algebrici. Per semplificare il lato destro, utilizziamo la proprietà dell’esponenziale che afferma \\(e^{-x} = \\frac{1}{e^x}\\). Quindi, riscriviamo \\(\\exp(-(\\theta_p - \\beta_i))\\) come \\(\\frac{1}{\\exp(\\theta_p - \\beta_i)}\\):\n\\[ \\frac{1}{1 + \\frac{1}{\\exp(\\theta_p - \\beta_i)}} \\]\nIl denominatore del lato destro diventa \\(1 + \\frac{1}{\\exp(\\theta_p - \\beta_i)}\\). Per combinare i termini nel denominatore, otteniamo un denominatore comune:\n\\[ \\frac{1}{\\frac{\\exp(\\theta_p - \\beta_i) + 1}{\\exp(\\theta_p - \\beta_i)}} \\]\nSimplificando ulteriormente, il denominatore diventa \\(\\exp(\\theta_p - \\beta_i) + 1\\), quindi l’intera espressione diventa:\n\\[ \\frac{1}{\\exp(\\theta_p - \\beta_i) + 1} \\]\nPossiamo ora invertire la frazione per ottenere il lato sinistro dell’equazione originale:\n\\[ \\frac{\\exp(\\theta_p - \\beta_i)}{\\exp(\\theta_p - \\beta_i) + 1} \\]\nQuindi, abbiamo dimostrato che il lato sinistro e il lato destro dell’equazione originale sono effettivamente equivalenti.\n\nEsercizio 67.2 Per illustrare come il modello di Rasch venga utilizzato per calcolare i punti su una curva caratteristica dell’item, consideriamo il seguente problema. I valori dei parametri dell’item sono:\n\na = 1 è il parametro di discriminazione dell’item,\nb = -0.5 è il parametro di difficoltà dell’item.\n\nTroviamo la probabilità di rispondere correttamente a questo item al livello di abilità theta = 1.5.\n\nicc &lt;- function(a, b, theta) {\n    1 / (1 + exp(-a * (theta - b)))\n}\n\na = 1\nb = -0.5\ntheta = 1.5\nicc(a, b, theta)\n#&gt; [1] 0.8808\n\n\ntheta_range &lt;- seq(-3, 3, .1)\nplot(theta_range, icc(a, b, theta_range),\n    type = \"l\", xlim = c(-3, 3), ylim = c(0, 1),\n    xlab = \"Ability\", ylab = \"Probability of Correct Response\"\n)\npoints(theta, icc(a, b, theta), cex=2)\nsegments(-3, icc(a, b, theta), theta, icc(a, b, theta), lty = \"dashed\")\nsegments(theta, icc(a, b, theta), theta, 0, lty = \"dashed\")",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_rasch_model.html#la-proprietà-di-oggettività-specifica",
    "href": "chapters/irt/01_rasch_model.html#la-proprietà-di-oggettività-specifica",
    "title": "67  Modello di Rasch",
    "section": "\n67.5 La Proprietà di Oggettività Specifica",
    "text": "67.5 La Proprietà di Oggettività Specifica\nUna caratteristica distintiva del modello di Rasch è la oggettività specifica, che garantisce che la differenza tra i logit delle probabilità di rispondere correttamente a due item \\(i\\) e \\(j\\) sia costante per qualsiasi livello di abilità \\(\\theta\\). In altre parole, il confronto tra due item dipende esclusivamente dalla loro difficoltà e non dall’abilità del rispondente, realizzando così un principio fondamentale di misurazione oggettiva.\nNel modello di Rasch, la probabilità di rispondere correttamente a un item viene trasformata in logit, cioè il logaritmo delle quote tra la probabilità di una risposta corretta e quella di una risposta errata. Il logit è definito come:\n\\[\n\\log \\left( \\frac{\\text{Pr}(U_{pi} = 1 \\mid \\theta_p, \\beta_i)}{\\text{Pr}(U_{pi} = 0 \\mid \\theta_p, \\beta_i)} \\right) = \\theta_p - \\beta_i,\n\\]\ndove:\n\n\n\\(\\theta_p\\) è l’abilità latente del partecipante \\(p\\),\n\n\\(\\beta_i\\) è la difficoltà dell’item \\(i\\).\n\nLa probabilità \\(\\pi\\) di rispondere correttamente a un item è calcolata attraverso la funzione logistica:\n\\[\n\\pi = \\frac{\\exp(\\theta_p - \\beta_i)}{1 + \\exp(\\theta_p - \\beta_i)}.\n\\]\nIl complemento della probabilità, ossia la probabilità di rispondere in modo errato, è:\n\\[\n1 - \\pi = \\frac{1}{1 + \\exp(\\theta_p - \\beta_i)}.\n\\]\nLe quote \\(O\\) sono definite come il rapporto tra la probabilità di successo e quella di insuccesso:\n\\[\nO = \\exp(\\theta_p - \\beta_i).\n\\]\nIl logaritmo delle quote corrisponde direttamente al logit:\n\\[\n\\log(O) = \\theta_p - \\beta_i.\n\\]\nQuesta relazione evidenzia che i logit sono proporzionali alla differenza tra l’abilità del partecipante e la difficoltà dell’item.\nUn aumento della differenza \\(\\theta_p - \\beta_i\\) si traduce in una maggiore probabilità di successo (\\(\\pi\\)):\n\n\nValori alti di \\(\\theta_p - \\beta_i\\): indicano che l’abilità supera la difficoltà, con una probabilità di successo vicina a 1.\n\nValori bassi di \\(\\theta_p - \\beta_i\\): indicano che la difficoltà supera l’abilità, con una probabilità di successo vicina a 0.\n\nIn sintesi, la proprietà di oggettività specifica assicura che il modello di Rasch fornisca una misurazione consistente e comparabile degli item e delle abilità, indipendentemente dal contesto o dai partecipanti coinvolti.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_rasch_model.html#implicazioni-della-proprietà-di-oggettività-specifica",
    "href": "chapters/irt/01_rasch_model.html#implicazioni-della-proprietà-di-oggettività-specifica",
    "title": "67  Modello di Rasch",
    "section": "\n67.6 Implicazioni della Proprietà di Oggettività Specifica",
    "text": "67.6 Implicazioni della Proprietà di Oggettività Specifica\nL’oggettività specifica nel modello di Rasch significa che il confronto tra due item è indipendente dall’abilità dei rispondenti. Nella pratica, ciò si traduce nel fatto che le curve caratteristiche degli item (ICC) per diversi item sono parallele lungo la scala dei logit. Questa parallelismo deriva dal fatto che la differenza tra i logit di due item, \\(\\beta_i\\) e \\(\\beta_j\\), è costante per tutti i valori di abilità \\(\\theta_p\\).\nLe curve per item con diverse difficoltà\n\nhanno pendenze identiche, riflettendo che il tasso di variazione della probabilità rispetto a \\(\\theta_p\\) è uguale per tutti gli item;\nnon si intersecano mai lungo l’asse delle abilità, poiché ogni differenza tra le probabilità di risposta corretta è attribuibile esclusivamente alla differenza di difficoltà tra gli item (\\(\\beta_j - \\beta_i\\));\nsi spostano verticalmente lungo l’asse delle probabilità in base alla difficoltà dell’item, mantenendo una rappresentazione coerente del rapporto tra abilità e probabilità di risposta corretta.\n\nLa rappresentazione logit\n\nconsente il calcolo della probabilità di risposta corretta indipendentemente dal set di item somministrati, garantendo comparabilità tra diversi test.\nrende le differenze tra item facilmente interpretabili, attribuendo ogni variazione a una caratteristica misurabile (la difficoltà) e non a fattori confondenti come il livello di abilità dei partecipanti.\n\nQuesta struttura garantisce misure eque e precise, promuovendo un’interpretazione robusta dei dati raccolti.\n\n# Creazione di un dataframe con i valori di abilità (theta_p) e le difficoltà degli item (beta)\ntheta_p &lt;- seq(-3, 3, length.out = 100)\nbeta_i &lt;- -1\nbeta_j &lt;- 1\n\n# Calcolo dei logit per gli item i e j\nlogit_i &lt;- theta_p - beta_i\nlogit_j &lt;- theta_p - beta_j\n\ndata &lt;- data.frame(\n    Ability = c(theta_p, theta_p),\n    Logit = c(logit_i, logit_j),\n    Item = factor(c(rep(\"Item i (beta_i = -1)\", length(theta_p)), rep(\"Item j (beta_j = 1)\", length(theta_p))))\n)\n\nggplot(data, aes(x = Ability, y = Logit, color = Item)) +\n    geom_line() +\n    scale_color_manual(values = c(\"blue\", \"red\")) +\n    ggtitle(TeX(\"Parallel Lines for Item i and j in the Rasch Model\")) +\n    xlab(TeX(\"Ability ($\\\\theta_p$)\")) +\n    ylab(TeX(\"Logit Probability\"))",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_rasch_model.html#il-modello-di-rasch-e-lanalisi-fattoriale",
    "href": "chapters/irt/01_rasch_model.html#il-modello-di-rasch-e-lanalisi-fattoriale",
    "title": "67  Modello di Rasch",
    "section": "\n67.7 Il Modello di Rasch e l’Analisi Fattoriale",
    "text": "67.7 Il Modello di Rasch e l’Analisi Fattoriale\nPer comprendere meglio il Modello di Rasch, può essere utile confrontarlo con l’Analisi Fattoriale. Sebbene si basino su approcci metodologici differenti, entrambi condividono l’obiettivo di individuare le dimensioni latenti che influenzano le risposte osservate nei dati. Questo confronto evidenzia somiglianze e differenze, fornendo una prospettiva più chiara sui due metodi.\nNell’Analisi Fattoriale, il modello tipico è espresso come \\(Y_i = \\lambda_i \\xi + \\delta_i\\), dove \\(Y_i\\) è il punteggio osservato per l’item i-esimo, \\(\\lambda_i\\) rappresenta la saturazione fattoriale che indica quanto l’item è influenzato dal fattore latente \\(\\xi\\), e \\(\\delta_i\\) è il termine di errore specifico per quell’item. L’idea centrale è che, controllando per \\(\\xi\\), le correlazioni tra gli item \\(Y_i\\) diventano nulle, poiché qualsiasi associazione comune è spiegata dal fattore latente.\nIl Modello di Rasch, pur perseguendo lo stesso obiettivo generale, adotta un approccio diverso. Esso si concentra sull’analisi di risposte dicotomiche (0 o 1) e presuppone che la probabilità di una risposta corretta sia una funzione logistica dell’abilità del rispondente \\(\\theta\\) e della difficoltà dell’item \\(\\delta_i\\).\nLa differenza cruciale tra il Modello di Rasch e l’Analisi Fattoriale risiede nella trattazione dei parametri degli item:\n\n\nPotere discriminante:\n\nNell’Analisi Fattoriale, le saturazioni fattoriali (\\(\\lambda_i\\)) variano tra gli item, riflettendo differenze nella capacità degli item di rappresentare la dimensione latente.\nNel Modello di Rasch, tutti gli item hanno lo stesso potere discriminante. Si presume che siano ugualmente efficaci nel distinguere tra rispondenti con abilità diverse.\n\n\n\nFocus sui parametri:\n\nL’Analisi Fattoriale si concentra sull’identificazione delle saturazioni fattoriali (\\(\\lambda_i\\)) per ciascun item e sull’individuazione dei fattori latenti comuni.\nIl Modello di Rasch stima l’abilità dei rispondenti (\\(\\theta\\)) e la difficoltà degli item (\\(\\delta_i\\)), presupponendo un’equivalenza nella discriminazione tra gli item.\n\n\n\nIn conclusione, sia il Modello di Rasch che l’Analisi Fattoriale mirano a spiegare le risposte osservate attraverso una dimensione latente. Tuttavia:\n\nL’Analisi Fattoriale enfatizza le relazioni tra gli item, stimando saturazioni fattoriali per descrivere come ogni item contribuisce alla dimensione latente comune.\nIl Modello di Rasch si focalizza sull’interazione tra l’abilità del rispondente e la difficoltà dell’item, fornendo una rappresentazione dettagliata delle dinamiche che influenzano le risposte dicotomiche.\n\nQuesti due approcci, pur perseguendo finalità analoghe, si applicano a contesti e dati differenti, completandosi a vicenda nel panorama della psicometria.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_rasch_model.html#riflessioni-conclusive",
    "href": "chapters/irt/01_rasch_model.html#riflessioni-conclusive",
    "title": "67  Modello di Rasch",
    "section": "\n67.8 Riflessioni Conclusive",
    "text": "67.8 Riflessioni Conclusive\nIl modello di Rasch si differenzia notevolmente dalla Teoria Classica dei Test (CTT) grazie a una serie di caratteristiche fondamentali che ne fanno uno strumento di analisi psicometrica più sofisticato e dettagliato.\n\n67.8.1 Principali Differenze tra Rasch e CTT\n\n\nLivello di Analisi\n\nLa CTT si concentra sull’analisi aggregata dell’intero test, descrivendo il punteggio totale come \\(X = T + E\\), dove \\(T\\) rappresenta il punteggio vero e \\(E\\) l’errore di misurazione. Questo approccio si limita a una visione complessiva delle misure, senza approfondire le dinamiche specifiche degli item.\n\nAl contrario, il modello di Rasch analizza la probabilità di una risposta corretta per ciascun item, offrendo un’analisi dettagliata a livello di singolo item e consentendo una comprensione approfondita del funzionamento di ogni elemento del test.\n\n\n\nApproccio Focalizzato sugli Item\n\nMentre la CTT utilizza un approccio basato sulla somma delle risposte, il modello di Rasch, seguendo il paradigma proposto da Guttman, analizza direttamente le risposte osservate, modellandone le probabilità. Questo cambio di prospettiva migliora la precisione e riduce gli errori di misurazione.\n\n\n\n67.8.2 Vantaggi del Modello di Rasch\n\n\nPrecisione e Dettaglio\n\nL’analisi item per item permette di identificare specifiche aree di forza o debolezza sia nei rispondenti sia nei test stessi. Ciò consente di migliorare gli strumenti di misura e garantisce stime più accurate delle abilità individuali.\n\n\n\nSeparazione tra Attributi della Persona e Caratteristiche dell’Item\n\nIl modello di Rasch distingue nettamente tra l’abilità del rispondente (\\(\\theta_p\\)) e la difficoltà dell’item (\\(\\beta_i\\)). Questa separazione assicura che la difficoltà di un item sia una proprietà intrinseca e stabile, indipendente dal campione di rispondenti, migliorando la coerenza e la generalizzabilità delle misure.\n\n\n\nFlessibilità nei Modelli di Risposta\n\nIl modello di Rasch può essere applicato a una vasta gamma di formati di domanda, come domande dicotomiche, scale Likert e risposte aperte. Questa versatilità lo rende adatto a misurare una varietà di costrutti psicologici.\n\n\n\nValutazione Adattiva\n\nIntegrato nell’ambito dell’Item Response Theory (IRT), il modello di Rasch supporta valutazioni adattive, in cui gli item somministrati variano in base al livello di abilità del rispondente. Ciò riduce gli errori di misurazione e fornisce stime più precise e mirate.\n\n\n\nAnalisi Approfondita degli Item\n\nIl modello consente di valutare in dettaglio le caratteristiche degli item, come difficoltà, discriminazione e parametri di indovinamento, fornendo informazioni utili per migliorare la qualità dei test.\n\n\n\n67.8.3 Limiti e Critiche\nNonostante i numerosi vantaggi, il modello di Rasch è talvolta criticato per le sue assunzioni restrittive, che includono:\n\nLa presunzione di un potere discriminante uguale per tutti gli item.\nLa dipendenza da una funzione logistica semplice per modellare le risposte.\n\nQueste assunzioni possono ridurre la capacità del modello di catturare la complessità delle risposte in scenari reali. Tuttavia, le sue basi teoriche solide garantiscono un’analisi robusta, in grado di mantenere l’invarianza delle proprietà degli item e delle abilità dei rispondenti attraverso diversi contesti e campioni.\nIn conclusione, il modello di Rasch rappresenta un notevole passo avanti rispetto alla CTT, offrendo una misurazione più precisa e flessibile, capace di adattarsi a molteplici contesti psicometrici. Nonostante le critiche, rimane uno strumento essenziale per la costruzione, la valutazione e il miglioramento dei test psicologici, consentendo un’analisi dettagliata e affidabile delle risposte osservate.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_rasch_model.html#session-info",
    "href": "chapters/irt/01_rasch_model.html#session-info",
    "title": "67  Modello di Rasch",
    "section": "\n67.9 Session Info",
    "text": "67.9 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.2\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] grid      stats4    stats     graphics  grDevices utils     datasets \n#&gt; [8] methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] latex2exp_0.9.6   ggmirt_0.1.0      TAM_4.2-21        CDM_8.2-6        \n#&gt;  [5] mvtnorm_1.3-3     ltm_1.2-0         polycor_0.8-1     msm_1.8.2        \n#&gt;  [9] mirt_1.44.0       lattice_0.22-6    ggokabeito_0.1.0  see_0.11.0       \n#&gt; [13] MASS_7.3-65       viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n#&gt; [17] ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1 \n#&gt; [21] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-19     psych_2.4.12     \n#&gt; [25] scales_1.3.0      markdown_1.13     knitr_1.50        lubridate_1.9.4  \n#&gt; [29] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.4      \n#&gt; [33] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1    \n#&gt; [37] tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] splines_4.4.2        later_1.4.1          R.oo_1.27.0         \n#&gt;   [4] XML_3.99-0.18        rpart_4.1.24         lifecycle_1.0.4     \n#&gt;   [7] Rdpack_2.6.3         rstatix_0.7.2        rprojroot_2.0.4     \n#&gt;  [10] globals_0.16.3       rockchalk_1.8.157    backports_1.5.0     \n#&gt;  [13] magrittr_2.0.3       openxlsx_4.2.8       Hmisc_5.2-3         \n#&gt;  [16] rmarkdown_2.29       yaml_2.3.10          httpuv_1.6.15       \n#&gt;  [19] qgraph_1.9.8         zip_2.3.2            sessioninfo_1.2.3   \n#&gt;  [22] pbapply_1.7-2        minqa_1.2.8          multcomp_1.4-28     \n#&gt;  [25] abind_1.4-8          audio_0.1-11         expm_1.0-0          \n#&gt;  [28] quadprog_1.5-8       R.utils_2.13.0       nnet_7.3-20         \n#&gt;  [31] TH.data_1.1-3        sandwich_3.1-1       listenv_0.9.1       \n#&gt;  [34] testthat_3.2.3       vegan_2.6-10         arm_1.14-4          \n#&gt;  [37] parallelly_1.42.0    permute_0.9-7        codetools_0.2-20    \n#&gt;  [40] tidyselect_1.2.1     farver_2.1.2         lme4_1.1-36         \n#&gt;  [43] base64enc_0.1-3      jsonlite_1.9.1       progressr_0.15.1    \n#&gt;  [46] Formula_1.2-5        survival_3.8-3       emmeans_1.10.7      \n#&gt;  [49] tools_4.4.2          rio_1.2.3            Rcpp_1.0.14         \n#&gt;  [52] glue_1.8.0           mnormt_2.1.1         admisc_0.37         \n#&gt;  [55] xfun_0.51            mgcv_1.9-1           withr_3.0.2         \n#&gt;  [58] beepr_2.0            fastmap_1.2.0        boot_1.3-31         \n#&gt;  [61] digest_0.6.37        mi_1.1               timechange_0.3.0    \n#&gt;  [64] R6_2.6.1             mime_0.13            estimability_1.5.1  \n#&gt;  [67] colorspace_2.1-1     gtools_3.9.5         jpeg_0.1-10         \n#&gt;  [70] R.methodsS3_1.8.2    generics_0.1.3       data.table_1.17.0   \n#&gt;  [73] corpcor_1.6.10       SimDesign_2.19.1     htmlwidgets_1.6.4   \n#&gt;  [76] pkgconfig_2.0.3      sem_3.1-16           gtable_0.3.6        \n#&gt;  [79] brio_1.1.5           htmltools_0.5.8.1    carData_3.0-5       \n#&gt;  [82] png_0.1-8            reformulas_0.4.0     rstudioapi_0.17.1   \n#&gt;  [85] tzdb_0.5.0           reshape2_1.4.4       coda_0.19-4.1       \n#&gt;  [88] checkmate_2.3.2      nlme_3.1-167         nloptr_2.2.1        \n#&gt;  [91] zoo_1.8-13           parallel_4.4.2       miniUI_0.1.1.1      \n#&gt;  [94] foreign_0.8-88       pillar_1.10.1        vctrs_0.6.5         \n#&gt;  [97] promises_1.3.2       car_3.1-3            OpenMx_2.21.13      \n#&gt; [100] xtable_1.8-4         Deriv_4.1.6          cluster_2.1.8.1     \n#&gt; [103] dcurver_0.9.2        GPArotation_2024.3-1 htmlTable_2.4.3     \n#&gt; [106] evaluate_1.0.3       pbivnorm_0.6.0       cli_3.6.4           \n#&gt; [109] kutils_1.73          compiler_4.4.2       rlang_1.1.5         \n#&gt; [112] future.apply_1.11.3  ggsignif_0.6.4       labeling_0.4.3      \n#&gt; [115] fdrtool_1.2.18       plyr_1.8.9           stringi_1.8.4       \n#&gt; [118] munsell_0.5.1        lisrelToR_0.3        pacman_0.5.1        \n#&gt; [121] Matrix_1.7-3         hms_1.1.3            glasso_1.11         \n#&gt; [124] future_1.34.0        shiny_1.10.0         rbibutils_2.3       \n#&gt; [127] igraph_2.1.4         broom_1.0.7          RcppParallel_5.1.10\n\n\n\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/02_assumptions.html",
    "href": "chapters/irt/02_assumptions.html",
    "title": "68  Assunzioni e Proprietà del Modello di Rasch",
    "section": "",
    "text": "68.1 Introduzione\nQuesto capitolo esamina le proprietà distintive del modello di Rasch, che lo rendono uno strumento fondamentale nella misurazione psicometrica. Tra queste proprietà, l’oggettività specifica consente di confrontare abilità individuali e difficoltà degli item in modo indipendente dal campione e dalla selezione degli item, garantendo misurazioni stabili e affidabili. Inoltre, il modello sfrutta il concetto di statistiche sufficienti, che permettono di stimare abilità e difficoltà utilizzando informazioni aggregate, riducendo la necessità di analizzare ogni singola risposta. Un altro aspetto cruciale è la rappresentazione dei punteggi su una scala di intervallo, che consente confronti significativi tra le differenze di abilità e difficoltà, pur richiedendo convenzioni per definire il punto zero e l’unità di misura.\nIn questo capitolo analizzeremo come le tre assunzioni fondamentali del modello di Rasch — unidimensionalità, monotonicità e indipendenza locale — diano origine alle sue proprietà distintive. Approfondiremo l’applicazione del modello nella progettazione di test psicometrici equi e accurati, evidenziandone i punti di forza e discutendo le situazioni in cui le sue limitazioni rendono necessario il ricorso a estensioni multidimensionali o modelli alternativi.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>Assunzioni e Proprietà del Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/02_assumptions.html#statistiche-sufficienti",
    "href": "chapters/irt/02_assumptions.html#statistiche-sufficienti",
    "title": "68  Assunzioni e Proprietà del Modello di Rasch",
    "section": "68.2 Statistiche Sufficienti",
    "text": "68.2 Statistiche Sufficienti\nIniziamo a chiarire il concetto di “statistica sufficiente”. Una statistica è una funzione dei dati osservati, utilizzata per riassumere caratteristiche rilevanti di un insieme di dati. Ad esempio, la media campionaria è una statistica comunemente calcolata come:\n\\[ \\bar{x} = \\frac{1}{P} \\sum_{p=1}^{P} x_p, \\]\ndove \\(\\bar{x}\\) rappresenta la media dei valori \\(x_p\\) osservati per \\(P\\) individui. Questa statistica è spesso impiegata per stimare il valore atteso di una popolazione, in quanto fornisce una sintesi delle informazioni relative alla media del campione.\nOltre alla media campionaria, è possibile definire altre statistiche. Ad esempio, si potrebbe calcolare la media di un sottoinsieme di valori del campione, come:\n\\[ x^* = \\frac{1}{3} (x_1 + x_3 + x_5). \\]\nSebbene \\(x^*\\) sia un valido stimatore, risulta generalmente meno efficace di \\(\\bar{x}\\) perché utilizza solo una parte del campione (escludendo, ad esempio, \\(x_2, x_4\\), ecc.), riducendo la quantità di informazioni sfruttate.\nIl concetto di statistica sufficiente si applica quando una statistica, come \\(\\bar{x}\\), contiene tutte le informazioni necessarie per stimare il parametro di interesse (ad esempio, la media della popolazione) che sono presenti nei dati campionari. In altre parole, una statistica sufficiente cattura completamente l’informazione sui parametri senza richiedere ulteriori dettagli dai dati individuali.\nLe statistiche sufficienti sono particolarmente utili nelle analisi inferenziali perché, una volta calcolate, rendono superflua la conoscenza dei dati grezzi ai fini della stima del parametro.\n\n68.2.1 Applicazioni nel Modello di Rasch\nIl modello di Rasch permette di identificare statistiche sufficienti per i principali parametri:\n\nStatistica sufficiente per \\(\\theta_p\\) (abilità della persona):\nPer il parametro di abilità \\(\\theta_p\\) di una persona \\(p\\), la statistica sufficiente è il punteggio totale \\(r_p\\), calcolato sommando tutte le risposte corrette fornite dalla persona ai diversi item. Questo punteggio sintetizza l’informazione fondamentale sull’abilità di \\(p\\), senza richiedere un’analisi dettagliata delle singole risposte.\nStatistica sufficiente per \\(\\beta_i\\) (difficoltà dell’item):\nPer il parametro di difficoltà di un item \\(\\beta_i\\), la statistica sufficiente è il numero totale di risposte corrette \\(c_i\\) fornite da tutte le persone per quell’item. Questo valore concentra l’informazione necessaria per descrivere la difficoltà dell’item.\n\nLa probabilità che una persona \\(p\\) risponda correttamente all’item \\(i\\) è definita da una funzione logistica che dipende dalla differenza tra abilità e difficoltà:\n\\[\nP(Y_{pi} = 1 \\mid \\theta_p, \\beta_i) = \\frac{e^{\\theta_p - \\beta_i}}{1 + e^{\\theta_p - \\beta_i}}.\n\\]\nIn questo schema:\n\nIl punteggio totale \\(r_p\\) e il numero totale di risposte corrette \\(c_i\\) sono statistiche sufficienti, poiché contengono tutte le informazioni utili per stimare i parametri \\(\\theta_p\\) e \\(\\beta_i\\).\n\nLa indipendenza condizionale delle risposte, dato \\(\\theta_p\\) o \\(\\beta_i\\), permette di sintetizzare i dati attraverso i punteggi totali senza perdere informazioni rilevanti per l’inferenza.\n\nQuesta caratteristica del modello di Rasch facilita l’analisi statistica, poiché consente di stimare i parametri senza dover considerare l’intera matrice delle risposte individuali. Inoltre, l’uso di statistiche sufficienti migliora l’efficienza computazionale e rende i risultati più facilmente interpretabili.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>Assunzioni e Proprietà del Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/02_assumptions.html#assunzioni-del-modello-di-rasch",
    "href": "chapters/irt/02_assumptions.html#assunzioni-del-modello-di-rasch",
    "title": "68  Assunzioni e Proprietà del Modello di Rasch",
    "section": "68.3 Assunzioni del Modello di Rasch",
    "text": "68.3 Assunzioni del Modello di Rasch\nIl modello di Rasch si basa su tre fondamentali assunzioni che ne garantiscono la validità e l’applicabilità: unidimensionalità, monotonicità e indipendenza locale.\n\n68.3.1 Unidimensionalità nel Modello di Rasch\nL’assunzione di unidimensionalità è centrale nel modello di Rasch e implica che le risposte agli item di un test siano determinate prevalentemente da un unico tratto latente o dimensione. Questo tratto, definito come la dimensione target del test, ordina le persone secondo le loro abilità, escludendo l’influenza significativa di altre caratteristiche. In altre parole, un test unidimensionale misura esclusivamente una specifica abilità o attributo.\nAd esempio, un test di matematica ideale dovrebbe valutare unicamente la competenza matematica. Al contrario, test complessi come il SAT, che valutano sia abilità matematiche che verbali, non possono essere considerati unidimensionali perché ogni sezione misura una dimensione distinta.\n\n68.3.1.1 Unidimensionalità e Funzionamento Ideale\nUn test unidimensionale ben progettato assegna a ciascun partecipante un unico valore di abilità, riflettendo esclusivamente la competenza nella dimensione target. Tuttavia, se più dimensioni latenti influenzano le risposte, si possono verificare problematiche come il Funzionamento Differenziale degli Item (DIF). Il DIF emerge quando la difficoltà di specifici item varia tra gruppi di candidati non per differenze nella dimensione target, ma per l’influenza di dimensioni secondarie.\nAd esempio, in un test di matematica, la competenza linguistica potrebbe rappresentare una dimensione secondaria che influenza le risposte. Se un gruppo di candidati ha abilità linguistiche significativamente diverse rispetto a un altro, alcuni item potrebbero risultare più facili o difficili in modo sistematico, distorcendo così i risultati del test. Questo evidenzia un problema di parzialità, poiché il test non misura equamente la competenza matematica per tutti i partecipanti.\n\n\n68.3.1.2 Identificazione del DIF e Multidimensionalità\nPer rilevare il DIF e verificare l’unidimensionalità, si utilizzano test statistici specifici che esplorano la possibile influenza di dimensioni non previste dal modello. La presenza di DIF o di influenze multidimensionali può compromettere la validità del test, portando a valutazioni ingiuste o inaffidabili.\nMolti costrutti psicologici, tuttavia, sono intrinsecamente multidimensionali. Ad esempio, il modello dell’intelligenza proposto da Carroll (1993) descrive una struttura gerarchica che comprende dimensioni come l’intelligenza fluida e cristallizzata, dimostrando la complessità di tali costrutti.\n\n\n68.3.1.3 Estensioni del Modello di Rasch per Multidimensionalità\nIl modello di Rasch classico si basa sull’unidimensionalità, limitandosi alla misurazione di un’unica abilità. Tuttavia, per affrontare la complessità dei costrutti psicologici, sono state sviluppate estensioni multidimensionali del modello di Rasch. Queste versioni permettono di valutare simultaneamente più dimensioni, offrendo una rappresentazione più accurata e completa delle abilità o caratteristiche misurate.\nLe estensioni multidimensionali consentono di:\n\nIsolare dimensioni distinte: Identificare e misurare separatamente tratti diversi influenti sulle risposte.\nGestire costrutti complessi: Analizzare costrutti psicologici multidimensionali come l’intelligenza o la personalità.\nMigliorare l’equità del test: Ridurre la parzialità e il DIF, garantendo una valutazione più giusta per tutti i partecipanti.\n\nIn conclusione, l’unidimensionalità è un pilastro fondamentale per il modello di Rasch classico, cruciale per garantire la validità e l’affidabilità dei test psicometrici. Tuttavia, la realtà dei costrutti psicologici richiede spesso un approccio più flessibile che consideri la loro natura multidimensionale. Le estensioni multidimensionali del modello di Rasch rappresentano una risposta essenziale a questa sfida, migliorando la precisione delle misurazioni e l’equità delle valutazioni in contesti complessi.\n\n\n\n68.3.2 Monotonicità\nL’assunzione di monotonicità stabilisce che con l’incremento del tratto latente (\\(\\theta\\)), aumenta anche la probabilità di una risposta corretta. Ciò si allinea con l’intuizione generale nella misurazione: individui con un livello più elevato del tratto latente tendono a ottenere punteggi migliori nei test.\n\n\n68.3.3 Indipendenza Locale nel Modello di Rasch\nL’indipendenza locale è un’assunzione fondamentale nel modello di Rasch, secondo cui, una volta controllato il tratto latente (ad esempio, l’abilità di una persona), le risposte a due item distinti devono essere indipendenti. In altre parole, eventuali correlazioni tra risposte a diversi item sono interamente attribuibili al tratto latente, senza che una risposta influenzi o sia influenzata da un’altra.\n\n68.3.3.1 Il Concetto di Indipendenza Stocastica\nIn statistica, l’indipendenza stocastica implica che la probabilità di un evento non dipenda dall’esito di un altro. Questo principio semplifica il calcolo delle probabilità congiunte. Ad esempio, nel caso di due lanci di una moneta equilibrata, la probabilità di ottenere “testa” in entrambi i lanci si calcola come il prodotto delle probabilità individuali:\n\\[\n\\text{Pr}(\\text{testa, testa}) = \\text{Pr}(\\text{testa}) \\times \\text{Pr}(\\text{testa}) = 0.5 \\times 0.5 = 0.25.\n\\]\nNel modello di Rasch, questo principio si traduce nell’indipendenza condizionale delle risposte agli item, dato il parametro di abilità \\(\\theta_p\\).\n\n\n68.3.3.2 Applicazione dell’Indipendenza Locale nel Modello di Rasch\nL’indipendenza locale consente di calcolare la probabilità congiunta delle risposte a un test come il prodotto delle probabilità individuali. Per due item \\(i\\) e \\(j\\), la probabilità congiunta delle risposte è data da:\n\\[\n\\text{Pr}(U_{pi} = u_{pi}, U_{pj} = u_{pj} \\mid \\theta_p, \\beta_i, \\beta_j) = \\text{Pr}(U_{pi} = u_{pi} \\mid \\theta_p, \\beta_i) \\times \\text{Pr}(U_{pj} = u_{pj} \\mid \\theta_p, \\beta_j).\n\\]\nGeneralizzando a un test con \\(I\\) item, possiamo rappresentare la probabilità congiunta come:\n\\[\n\\text{Pr}(U_{p\\cdot} = u_{p\\cdot} \\mid \\theta_p, \\beta) = \\prod_{i=1}^{I} \\text{Pr}(U_{pi} = u_{pi} \\mid \\theta_p, \\beta_i),\n\\]\ndove \\(\\beta = (\\beta_1, \\dots, \\beta_I)\\) rappresenta il vettore dei parametri di difficoltà degli item e \\(U_{p\\cdot}\\) è il vettore delle risposte della persona \\(p\\). Questa formulazione semplifica notevolmente i calcoli e l’analisi statistica.\n\n\n68.3.3.3 Limitazioni dell’Indipendenza Locale\nNonostante la sua utilità, l’assunzione di indipendenza locale può essere violata in alcune situazioni, come:\n\nTest di matematica: La soluzione di un problema può dipendere dalla comprensione di item precedenti.\nTestlet: Gruppi di item che condividono un tema comune possono introdurre correlazioni tra le risposte.\n\nIn questi casi, le risposte non sono condizionatamente indipendenti, e l’applicazione del modello di Rasch può risultare inappropriata. Tali situazioni richiedono modelli alternativi, come quelli che incorporano dipendenze strutturali tra item.\nIn conclusione, l’indipendenza locale è una proprietà chiave che consente al modello di Rasch di calcolare in modo efficiente le probabilità congiunte delle risposte e di eseguire inferenze robuste. Tuttavia, è fondamentale valutare attentamente il contesto del test per verificare se questa assunzione sia valida. Nei casi in cui l’indipendenza locale non è rispettata, l’adozione di modelli più complessi può essere necessaria per garantire una valutazione accurata e priva di bias.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>Assunzioni e Proprietà del Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/02_assumptions.html#scala-di-misurazione-nel-modello-di-rasch",
    "href": "chapters/irt/02_assumptions.html#scala-di-misurazione-nel-modello-di-rasch",
    "title": "68  Assunzioni e Proprietà del Modello di Rasch",
    "section": "68.4 Scala di Misurazione nel Modello di Rasch",
    "text": "68.4 Scala di Misurazione nel Modello di Rasch\nEsaminiamo ora livello di misurazione dei punteggi ottenuti mediante il modello di Rash. Un aspetto cruciale è che i parametri del modello, come l’abilità delle persone (\\(\\theta_p\\)) e la difficoltà degli item (\\(\\beta_i\\)), sono rappresentati su una scala di intervallo. Questo livello di misurazione consente di confrontare differenze tra abilità e difficoltà, ma non definisce un punto zero assoluto né un’unità di misura intrinseca.\n\n68.4.1 Misurazione su una Scala di Intervallo\nNel modello di Rasch, i punteggi ottenuti non sono meri numeri ordinali (che stabiliscono solo un ordine), ma rappresentano intervalli misurabili. Questo significa che:\n\nLe differenze tra abilità o difficoltà hanno un significato costante e interpretabile.\nTuttavia, la scala non ha un punto zero assoluto; il valore “zero” è arbitrario e dipende dalla convenzione adottata.\n\nUn’analogia utile è il confronto con le scale di temperatura in gradi Celsius o Fahrenheit: mentre le differenze (ad esempio, 10°C contro 20°C) hanno un significato consistente, lo zero non rappresenta un’assenza di temperatura, ma è definito convenzionalmente.\n\n\n68.4.2 Trasformazioni e Ricalibrazione della Scala\nLe misure nel modello di Rasch possono essere trasformate senza alterare le probabilità di risposta corrette:\n\nTraslazione: Le abilità e le difficoltà possono essere traslate sottraendo un valore costante (\\(b\\)): \\[\n\\theta_p' = \\theta_p - b, \\quad \\beta_i' = \\beta_i - b.\n\\] Questa operazione mantiene invariata la funzione logistica che definisce le probabilità di risposta corretta.\nRiscalatura: La scala può essere modificata mediante una moltiplicazione o divisione per un fattore costante (\\(a\\)): \\[\n\\theta_p'' = \\frac{\\theta_p}{a}, \\quad \\beta_i'' = \\frac{\\beta_i}{a}.\n\\] Anche in questo caso, le probabilità restano invariate se la funzione logistica è adattata al nuovo fattore di scala.\n\nQueste trasformazioni mostrano che la scala è relativa: ciò che conta non è il valore assoluto delle misure, ma le differenze e il rapporto tra i parametri.\n\n\n68.4.3 Implicazioni per la Misurazione\nPoiché i parametri del modello di Rasch sono su una scala di intervallo, è necessario stabilire convenzioni per definire un punto zero e un’unità di misura. Comunemente, si adottano le seguenti strategie:\n\nFissare un riferimento: Ad esempio, assegnare la difficoltà di un item a zero.\nNormalizzazione: Imporre che la somma delle difficoltà degli item o delle abilità dei partecipanti sia pari a zero.\nStandardizzazione della pendenza: Impostare la scala della funzione logistica a 1, garantendo una coerenza nelle unità di misura.\n\nQueste scelte non influenzano la validità delle misure, ma permettono di ancorare i parametri a una scala interpretabile.\nIn conclusione, il modello di Rasch fornisce misurazioni robuste su una scala di intervallo, permettendo analisi precise delle differenze tra abilità e difficoltà. Tuttavia, la mancanza di un punto zero intrinseco e di un’unità di misura assoluta richiede la definizione di convenzioni per la calibratura della scala. Questo aspetto, sebbene tecnico, è cruciale per garantire la coerenza e l’interpretabilità dei risultati ottenuti nei contesti psicometrici.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>Assunzioni e Proprietà del Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/02_assumptions.html#oggettività-specifica",
    "href": "chapters/irt/02_assumptions.html#oggettività-specifica",
    "title": "68  Assunzioni e Proprietà del Modello di Rasch",
    "section": "68.5 Oggettività Specifica",
    "text": "68.5 Oggettività Specifica\nL’oggettività specifica è uno dei principi cardine del modello di Rasch e garantisce che i confronti tra individui siano equi e indipendenti dagli item specifici utilizzati nel test. Questo concetto assicura che le differenze tra le abilità delle persone si riflettano in modo coerente, senza essere influenzate da caratteristiche particolari degli item.\nL’oggettività specifica implica che:\n\nConfronti tra individui: Se una persona ha una probabilità maggiore di rispondere correttamente rispetto a un’altra, questa superiorità si manifesta uniformemente su tutti gli item.\nConfronti tra item: Se un item è più facile per una persona, sarà più facile per chiunque altro, indipendentemente dalle abilità specifiche.\n\nIn altre parole, il modello di Rasch garantisce che le probabilità di risposta corretta dipendano solo dalla differenza tra l’abilità della persona (\\(\\theta_p\\)) e la difficoltà dell’item (\\(\\beta_i\\)), preservando la coerenza nei confronti.\n\n68.5.1 Verifica dell’Oggettività Specifica\nUn indicatore chiave dell’oggettività specifica è rappresentato dalle Curve di Caratteristica dell’Item (ICC). Nel modello di Rasch, le ICC per diversi item non si incrociano: ciò indica che la relazione tra abilità e probabilità di risposta corretta rimane consistente per tutti gli item, rispettando l’oggettività specifica.\nL’oggettività specifica può essere descritta algebricamente attraverso i rapporti di probabilità. Per due persone \\(p\\) e \\(q\\) e un item \\(i\\), sia \\(P_{pi}\\) la probabilità che la persona \\(p\\) risponda correttamente all’item \\(i\\). L’oggettività specifica richiede che il rapporto di probabilità tra le due persone sia costante per tutti gli item:\n\\[\n\\frac{P_{pi}}{P_{qi}} = \\frac{P_{pj}}{P_{qj}}, \\quad \\forall i, j.\n\\]\nQuesto implica che:\n\\[\nP_{pi} \\cdot P_{qj} = P_{pj} \\cdot P_{qi}.\n\\]\nTale proprietà garantisce che le differenze tra individui siano indipendenti dagli specifici item somministrati.\nConsideriamo il seguente esempio pratico. Supponiamo che Marco e Cora affrontino un test composto da venti item. Se le probabilità di rispondere correttamente al primo item sono 20% per Marco e 80% per Cora, il rapporto tra le loro probabilità (4:1) deve rimanere lo stesso per tutti gli altri item del test. Questo equilibrio assicura che il confronto tra Marco e Cora non dipenda dagli item specifici ma esclusivamente dalle loro abilità relative.\n\n\n68.5.2 Limitazioni e Considerazioni\nSebbene l’oggettività specifica sia una proprietà potente, essa è soggetta a limitazioni:\n\nDipendenza dal contesto: La trasposizione di un test tra gruppi con caratteristiche culturali o professionali diverse (es. banchieri e ingegneri) potrebbe invalidare l’oggettività specifica se gli item vengono interpretati in modi differenti.\nNecessità di verifica empirica: L’oggettività specifica deve essere testata con i dati per confermare che le proprietà del modello siano rispettate nel contesto specifico.\n\nRasch stesso sottolineava l’importanza di validare questa proprietà ogni volta che si raccolgono nuovi dati.\nIn conclusione, l’oggettività specifica rappresenta il cuore del modello di Rasch, garantendo confronti equi e coerenti tra individui e item. Tuttavia, non è una proprietà intrinseca e universale, ma un’ipotesi di lavoro che deve essere verificata empiricamente in ogni applicazione. Questo principio, se rispettato, assicura che i test psicometrici siano strumenti affidabili per misurare abilità e caratteristiche, indipendentemente dal contesto o dagli item utilizzati.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>Assunzioni e Proprietà del Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/02_assumptions.html#riflessioni-conclusive",
    "href": "chapters/irt/02_assumptions.html#riflessioni-conclusive",
    "title": "68  Assunzioni e Proprietà del Modello di Rasch",
    "section": "68.6 Riflessioni Conclusive",
    "text": "68.6 Riflessioni Conclusive\nIl modello di Rasch rappresenta un approccio rigoroso alla misurazione psicometrica, basato su tre assunzioni fondamentali: unidimensionalità, monotonicità e indipendenza locale. Queste assunzioni costituiscono il fondamento del modello, garantendone validità e applicabilità. Tuttavia, la loro violazione può richiedere l’adozione di metodologie più avanzate o modelli alternativi per affrontare la complessità dei dati. In tali situazioni, un’analisi più approfondita dei dati o l’impiego di strumenti statistici sofisticati diventa indispensabile.\nUno degli aspetti distintivi del modello di Rasch è il principio di oggettività specifica, che consente di stimare la difficoltà degli item in modo indipendente dalle abilità dei partecipanti. Questo è reso possibile dall’uso della stima di massima verosimiglianza condizionale, che isola la difficoltà degli item basandosi unicamente sulle risposte specifiche a ciascun item, senza essere influenzata dal livello complessivo di abilità del campione.\nL’oggettività specifica assicura che i parametri di difficoltà degli item siano stabili e affidabili, indipendentemente dalla composizione del campione. Questo è analogo al concetto di invarianza in regressione lineare, dove i parametri della retta di regressione, come pendenza e intercetta, rimangono invariati rispetto al campione utilizzato per l’analisi. Nel modello di Rasch, l’invarianza dei parametri garantisce che la difficoltà degli item resti costante, anche quando i partecipanti hanno livelli di abilità diversi.\nUn aspetto particolarmente vantaggioso del modello di Rasch è che l’oggettività specifica elimina la necessità di utilizzare campioni normati o rappresentativi per calibrare gli item. Qualsiasi gruppo di partecipanti, purché presenti una sufficiente varietà nelle risposte, può essere impiegato per stimare i parametri di difficoltà. Questo contrasta con i metodi tradizionali, che spesso richiedono campioni rappresentativi per sviluppare tabelle normative basate su percentuali di risposte corrette.\nIl modello di Rasch offre un framework solido e trasparente per la misurazione psicometrica, distinguendosi per la precisione e la generalizzabilità delle sue stime. Tuttavia, il rispetto delle sue assunzioni fondamentali è cruciale per garantire risultati accurati ed equi. Verificare l’unidimensionalità, la monotonicità e l’indipendenza locale è essenziale per evitare bias e preservare l’integrità delle misurazioni.\nIn conclusione, il modello di Rasch si configura come uno strumento potente per sviluppare e validare strumenti di misura, combinando semplicità teorica con robustezza operativa. La sua capacità di produrre risultati indipendenti dalle caratteristiche del campione lo rende una scelta ideale per molte applicazioni psicometriche, purché le sue assunzioni siano rigorosamente testate e rispettate.\n\n\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>Assunzioni e Proprietà del Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/03_estimation.html",
    "href": "chapters/irt/03_estimation.html",
    "title": "69  Stima",
    "section": "",
    "text": "69.0.1 Introduzione\nPer utilizzare il modello di Rasch nella ricerca pratica, è essenziale comprendere come stimare i suoi parametri a partire dai dati osservati. In questa sezione verranno illustrati diversi metodi di stima, ognuno dei quali consente di calcolare sia i parametri degli item che quelli delle persone, differenziandosi però per l’approccio utilizzato.\nAlcuni metodi, come la massima verosimiglianza congiunta e l’inferenza bayesiana, stimano simultaneamente i parametri degli item e delle persone. Altri, come la massima verosimiglianza condizionale e la massima verosimiglianza marginale, separano il processo di stima: i parametri degli item vengono stimati per primi, seguiti dalla stima dei parametri delle persone in una fase successiva. Questa distinzione tra approcci permette di scegliere la metodologia più adatta al contesto e alle caratteristiche dei dati analizzati.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>Stima</span>"
    ]
  },
  {
    "objectID": "chapters/irt/03_estimation.html#la-funzione-di-verosimiglianza",
    "href": "chapters/irt/03_estimation.html#la-funzione-di-verosimiglianza",
    "title": "69  Stima",
    "section": "\n69.1 La Funzione di Verosimiglianza",
    "text": "69.1 La Funzione di Verosimiglianza\nLa stima dei parametri nel modello di Rasch si basa sulla funzione di verosimiglianza, che rappresenta la probabilità di osservare i dati disponibili dato un insieme di parametri sconosciuti. Nel contesto del modello, \\(U_{pi}\\) denota la risposta (corretta o errata) fornita dalla persona \\(p\\) all’item \\(i\\), dove una risposta corretta è codificata come 1 e una errata come 0. La probabilità condizionale che una persona con abilità \\(\\theta_p\\) risponda specificamente \\(u_{pi}\\) all’item \\(i\\), la cui difficoltà è \\(\\beta_i\\), è definita dalla formula:\n\\[\n\\text{Pr}(U_{pi} = u_{pi} | \\theta_p, \\beta_i) = \\frac{\\exp\\{u_{pi} \\cdot (\\theta_p - \\beta_i)\\}}{1 + \\exp(\\theta_p - \\beta_i)}.\n\\]\nQuesta equazione calcola la probabilità della risposta osservata, basandosi sulla differenza tra l’abilità della persona (\\(\\theta_p\\)) e la difficoltà dell’item (\\(\\beta_i\\)). Se l’abilità \\(\\theta_p\\) supera la difficoltà \\(\\beta_i\\), la probabilità di una risposta corretta (\\(u_{pi} = 1\\)) è elevata; al contrario, se \\(\\theta_p\\) è inferiore a \\(\\beta_i\\), tale probabilità sarà ridotta.\n\n69.1.1 Verosimiglianza Complessiva per una Persona\nLa verosimiglianza complessiva per una persona \\(p\\) rispetto a tutte le sue risposte agli item del test (\\(i = 1, \\dots, I\\)) si ottiene moltiplicando le probabilità condizionali di ciascuna risposta. La funzione di verosimiglianza totale è quindi espressa come:\n\\[\nL_{up}(\\theta_p, \\beta) = \\prod_{i=1}^{I} \\frac{\\exp\\{u_{pi} \\cdot (\\theta_p - \\beta_i)\\}}{1 + \\exp(\\theta_p - \\beta_i)}.\n\\]\nRiorganizzando per maggiore chiarezza, questa può essere riscritta come:\n\\[\nL_{up}(\\theta_p, \\beta) = \\frac{\\exp(r_p \\cdot \\theta_p - \\sum_{i=1}^{I} u_{pi} \\cdot \\beta_i)}{\\prod_{i=1}^{I} [1 + \\exp(\\theta_p - \\beta_i)]}, \\tag{1}\n\\]\ndove:\n\n\n\\(r_p = \\sum_{i=1}^{I} u_{pi}\\) rappresenta il punteggio grezzo della persona \\(p\\), ovvero il numero totale di risposte corrette.\n\nQuesta formulazione sintetizza come la funzione di verosimiglianza dipenda non solo dal parametro di abilità \\(\\theta_p\\) della persona, ma anche dai parametri di difficoltà \\(\\beta_i\\) degli item. La funzione è essenziale per stimare questi parametri e per interpretare la relazione tra abilità e difficoltà nel contesto del test.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>Stima</span>"
    ]
  },
  {
    "objectID": "chapters/irt/03_estimation.html#stima-dei-parametri-nel-modello-di-rasch",
    "href": "chapters/irt/03_estimation.html#stima-dei-parametri-nel-modello-di-rasch",
    "title": "69  Stima",
    "section": "\n69.2 Stima dei Parametri nel Modello di Rasch",
    "text": "69.2 Stima dei Parametri nel Modello di Rasch\nL’?eq-rasch-likelihood rappresenta la base comune per tutti i metodi di stima dei parametri nel modello di Rasch. Tuttavia, il metodo scelto per stimare i parametri influenzerà il modo in cui l’abilità delle persone (\\(\\theta_p\\)) e la difficoltà degli item (\\(\\beta_i\\)) vengono calcolate e interpretate:\n\n\nStima simultanea: Alcuni metodi, come la massima verosimiglianza congiunta e l’inferenza bayesiana, stimano simultaneamente \\(\\theta_p\\) e \\(\\beta_i\\).\n\nStima separata: Altri metodi, come la massima verosimiglianza condizionale e la massima verosimiglianza marginale, stimano \\(\\beta_i\\) in un primo passaggio, per poi derivare \\(\\theta_p\\).\n\nOgni approccio introduce assunzioni specifiche che influenzano le proprietà delle stime e la loro applicabilità in diversi contesti.\n\n69.2.1 Stima della Massima Verosimiglianza Congiunta\nLa stima della massima verosimiglianza congiunta (JML) mira a determinare simultaneamente i parametri delle persone (\\(\\theta_p\\)) e degli item (\\(\\beta_i\\)) che massimizzano la probabilità complessiva dei dati osservati, come descritto nella funzione di verosimiglianza del modello di Rasch. Questo approccio identifica il set di parametri più probabili che potrebbero aver generato il dataset analizzato.\nPunti di forza:\n\nMetodo diretto e intuitivo, che utilizza tutta l’informazione disponibile nei dati osservati.\n\nLimitazioni:\n\n\nStime inconsistenti: Nonostante la semplicità del metodo, JML non garantisce stime consistenti dei parametri degli item, anche con campioni di grandi dimensioni. Questo limita la sua affidabilità, specialmente in contesti che richiedono alta precisione e robustezza nelle stime.\n\nBias intrinseco: Le stime delle abilità (\\(\\theta_p\\)) e delle difficoltà (\\(\\beta_i\\)) possono essere influenzate l’una dall’altra, causando errori sistematici.\n\nImplementazione in R:\n\nJML è implementato nel pacchetto TAM, attraverso la funzione tam.jml(). Sebbene disponibile, il suo utilizzo è sconsigliato in analisi avanzate o quando la consistenza delle stime è critica.\n\n69.2.2 Stima della Massima Verosimiglianza Condizionale\nLa stima della massima verosimiglianza condizionale (CML) affronta le limitazioni della JML separando la stima dei parametri degli item da quella delle persone. Questo approccio procede in due fasi:\n\n\nStima dei parametri degli item:\n\nLa CML utilizza le statistiche sufficienti delle persone (ad esempio, i punteggi grezzi \\(r_p = \\sum u_{pi}\\)) per isolare i parametri degli item. In questa fase, le abilità delle persone (\\(\\theta_p\\)) non sono direttamente considerate, evitando il bias congiunto.\n\n\n\nStima dei parametri delle persone:\n\nUna volta stimati i parametri degli item (\\(\\beta_i\\)), si procede alla stima delle abilità (\\(\\theta_p\\)) basandosi sui dati individuali e sulle difficoltà stimate.\n\n\n\nVantaggi:\n\nFornisce stime consistenti dei parametri degli item.\nEvita il problema del bias associato alla stima simultanea di JML.\n\nLimitazioni:\n\nL’accuratezza dei parametri delle persone dipende dalla precisione delle stime degli item nella prima fase.\n\nImplementazione in R:\n\nLa CML è implementata nel pacchetto eRm tramite la funzione RM(), che consente di stimare i parametri degli item in modo robusto e separato.\n\n69.2.3 Stima della Massima Verosimiglianza Marginale\nLa stima della massima verosimiglianza marginale (MML) rappresenta un approccio avanzato che considera le abilità delle persone come una variabile casuale seguendo una distribuzione ipotizzata, tipicamente normale. Questo metodo differisce dalla CML trattando i parametri delle abilità (\\(\\theta_p\\)) come effetti casuali anziché fissi, e li integra nella funzione di verosimiglianza complessiva.\nCome funziona:\n\n\nDistribuzione marginale delle abilità:\n\nLa MML assume che le abilità (\\(\\theta_p\\)) siano distribuite nella popolazione secondo una distribuzione nota (ad esempio, una normale standard). Invece di stimare direttamente \\(\\theta_p\\), il metodo stima i parametri degli item (\\(\\beta_i\\)) tenendo conto di questa distribuzione.\n\n\n\nScoring individuale:\n\nDopo aver stimato i parametri degli item, si calcolano i punteggi individuali (\\(\\theta_p\\)) basandosi sulle risposte e sui parametri stimati.\n\n\n\nVantaggi:\n\nProduce stime più precise e realistiche dei parametri degli item rispetto alla JML.\nÈ particolarmente utile quando le abilità nella popolazione seguono una distribuzione continua e ipotizzabile.\n\nLimitazioni:\n\nLa validità delle stime dipende dalla correttezza dell’assunzione sulla distribuzione delle abilità (\\(\\theta_p\\)).\n\nImplementazione in R:\n\nLa MML è supportata dai pacchetti mirt e TAM. Ad esempio:\n\nFunzioni come mirt() in mirt permettono stime flessibili con distribuzioni marginali specificabili.\nAnche ltm (sebbene non più attivamente sviluppato) offre strumenti per la stima marginale.\n\n\n\n69.2.4 Confronto tra i Metodi\n\n\n\n\n\n\n\n\nMetodo\nCaratteristiche principali\nPro\nContro\n\n\n\nJML (Massima Verosimiglianza Congiunta)\nStima simultanea di \\(\\theta_p\\) e \\(\\beta_i\\).\nIntuitivo e diretto.\nStime inconsistenti; bias congiunto.\n\n\nCML (Massima Verosimiglianza Condizionale)\nStima separata in due fasi: prima \\(\\beta_i\\), poi \\(\\theta_p\\).\nStime consistenti per \\(\\beta_i\\); evita il bias.\nDipende dall’accuratezza delle stime iniziali degli item.\n\n\nMML (Massima Verosimiglianza Marginale)\nIntegra una distribuzione marginale per \\(\\theta_p\\); tratta \\(\\theta_p\\) come effetti casuali.\nStime realistiche e robuste; considera la distribuzione della popolazione.\nDipende dall’assunzione sulla distribuzione delle abilità.\n\n\n\nIn conclusione, ogni metodo presenta vantaggi e svantaggi che lo rendono più o meno adatto a specifici contesti di analisi. La JML è utile per analisi preliminari o semplici, ma è limitata dalla mancanza di consistenza. La CML e la MML offrono stime più robuste e realistiche, con la MML che si distingue per la sua flessibilità nell’incorporare distribuzioni di popolazione.\n\nEsercizio 69.1 Consideriamo ora la procedura di stima del livello di abilità \\(\\theta\\) di un individuo nel modello di Rasch attraverso l’uso della massima verosimiglianza marginale. La procedura per stimare la posizione di un individuo, dato un particolare pattern di risposte, può essere formulata con i seguenti passaggi.\n\nConsideriamo un determinato pattern di risposta. Per esempio, il pattern “11000” indica che un particolare individuo ha fornito due risposte corrette seguite da tre errate a cinque item, con un totale di \\(X = 2\\) risposte corrette.\nCalcoliamo le probabilità per ogni risposta. Utilizziamo l’Equazione 67.1 per calcolare la probabilità di ciascuna risposta nel pattern, in base a un dato livello di abilità \\(\\theta\\).\nDeterminiamo la probabilità del pattern di risposta. Questo passaggio si basa sull’assunzione di indipendenza condizionale (ovvero, per un dato \\(\\theta\\), le risposte sono indipendenti l’una dall’altra). Questa assunzione ci permette di applicare la regola di moltiplicazione per eventi indipendenti alle probabilità degli item per ottenere la probabilità complessiva del pattern di risposta per un dato \\(\\theta\\).\nRipetiamo i calcoli per diversi valori di \\(\\theta\\). Ripetiamo i passaggi 1 e 2 per una serie di valori di \\(\\theta\\). Nel nostro esempio, il range di \\(\\theta\\) va da \\(-3\\) a \\(3\\).\nDeterminiamo il valore di \\(\\theta\\) con la massima verosimiglianza. L’ultimo passaggio consiste nel determinare quale valore di \\(\\theta\\) tra quelli calcolati nel passaggio 3 abbia la più alta verosimiglianza di produrre il pattern “11000”. Per fare questo scegliamo il valore \\(\\theta\\) per cui la verosimiglianza è massima.\n\nDi seguito, esaminiamo uno script in R che implementa questa procedura.\n\n# Definiamo il pattern di risposta\nresponse_pattern &lt;- c(1, 1, 0, 0, 0)\n\n# Range di valori di theta da esplorare\ntheta_values &lt;- seq(-3, 3, by = 0.01)\n\n# Funzione per calcolare la probabilità di un singolo pattern di risposta\ncalculate_probability &lt;- function(theta, pattern) {\n    correct_probs &lt;- exp(theta) / (1 + exp(theta))\n    item_probs &lt;- ifelse(pattern == 1, correct_probs, 1 - correct_probs)\n    prod(item_probs)\n}\n# Per semplicità, assumiamo che il parametro di difficoltà (beta) sia zero per tutti gli item.\n\n# Calcoliamo le probabilità per ogni valore di theta. Usiamo sapply per applicare \n# la funzione calculate_probability a ciascun valore di theta nel range specificato.\nprobabilities &lt;- sapply(theta_values, calculate_probability, pattern = response_pattern)\n\n# Identifichiamo il valore di theta con la massima verosimiglianza\nbest_theta &lt;- theta_values[which.max(probabilities)]\n\nprint(paste(\"Valore di theta calcolato con la massima verosimiglianza:\", best_theta))\n#&gt; [1] \"Valore di theta calcolato con la massima verosimiglianza: -0.41\"\n\nQuesto script calcola la probabilità di ottenere il pattern di risposta “11000” per cinque item per un dato intervallo di valori di \\(\\theta\\) e identifica il valore di \\(\\theta\\) che massimizza questa probabilità. Si noti che il modello di Rasch prevede che tutti gli item abbiano la stessa discriminazione, quindi non è necessario specificare un parametro di discriminazione per ogni item. Abbiamo assunto inoltre che la difficoltà di tutti gli item sia uguale a zero.\nLa verosimiglianza di un pattern di risposta di un singolo rispondente a diversi item può essere rappresentata simbolicamente nel modo seguente. Se consideriamo \\(x\\) come il pattern di risposta di un rispondente (ad esempio, \\(x = 11000\\) indica che il rispondente ha risposto correttamente ai primi due item e ha dato risposte sbagliate agli ultimi tre), la verosimiglianza del vettore di risposta \\(x_i\\) della persona \\(i\\) è espressa come:\n\\[\n\\begin{equation}\nL(x_i) = \\prod_{j=1}^{L} p_{ij},\n\\end{equation}\n\\]\ndove \\(p_{ij} = p(x_{ij} = 1 \\mid \\theta_i, \\alpha_j, \\delta_j)\\) rappresenta la probabilità che la persona \\(i\\), con un livello di abilità \\(\\theta_i\\), risponda correttamente all’item \\(j\\). In questa formula, \\(\\alpha_j\\) è il parametro di discriminazione dell’item \\(j\\) e \\(\\delta_j\\) è il suo parametro di difficoltà. Il parametro \\(\\alpha_j\\) indica quanto bene l’item \\(j\\) è in grado di discriminare tra rispondenti di diversi livelli di abilità, mentre \\(\\delta_j\\) rappresenta il livello di abilità per cui la probabilità di una risposta corretta è del 50%. Il prodotto è calcolato su tutti gli \\(L\\) item a cui il rispondente ha risposto, e il simbolo \\(\\prod\\) rappresenta il prodotto di tutte queste probabilità individuali.\nIl calcolo diretto della verosimiglianza può diventare problematico all’aumentare del numero di item, poiché il prodotto di molteplici probabilità può risultare in valori molto piccoli, difficili da gestire con precisione in calcoli numerici. Pertanto, è spesso più pratico lavorare con la trasformazione logaritmica naturale della verosimiglianza, ovvero \\(\\log_e(L(x_i))\\) o \\(\\ln(L(x_i))\\). Questa trasformazione converte il prodotto in una somma, come segue:\n\\[\n\\begin{equation}\n\\ln L(x_i) = \\sum_{j=1}^{L} \\ln(p_{ij}).\n\\end{equation}\n\\]\nL’uso del logaritmo naturale trasforma quindi la verosimiglianza in una somma di logaritmi, semplificando il calcolo e riducendo i problemi di rappresentazione numerica nei calcoli complessi.\n\n# Definizione del pattern di risposta\nresponse_pattern &lt;- c(1, 1, 0, 0, 0)\n\n# Range di valori di theta da esplorare\ntheta_values &lt;- seq(-3, 3, by = 0.1)\n\n# Calcolo della log-verosimiglianza per ogni valore di theta\nlog_likelihoods &lt;- numeric(length(theta_values))\nfor (i in seq_along(theta_values)) {\n    theta &lt;- theta_values[i]\n    log_item_probs &lt;- numeric(length(response_pattern))\n\n    # Calcolo delle probabilità logaritmiche individuali per ogni item nel pattern\n    for (j in seq_along(response_pattern)) {\n        prob_correct &lt;- exp(theta) / (1 + exp(theta))\n        prob &lt;- ifelse(response_pattern[j] == 1, prob_correct, 1 - prob_correct)\n        log_item_probs[j] &lt;- log(prob)\n    }\n\n    # Calcolo della log-verosimiglianza\n    log_likelihoods[i] &lt;- sum(log_item_probs)\n}\n\n# Creazione di un dataframe per il plotting\nplot_data &lt;- data.frame(theta = theta_values, log_likelihood = log_likelihoods)\n\n# Rappresentazione grafica della log-verosimiglianza\nggplot(plot_data, aes(x = theta, y = log_likelihood)) +\n    geom_line() +\n    labs(\n        x = expression(theta), y = \"Log-likelihood\",\n        title = \"Log-likelihood Function for Response Pattern 11000\"\n    )",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>Stima</span>"
    ]
  },
  {
    "objectID": "chapters/irt/03_estimation.html#errore-standard-della-stima-e-informazione-dellitem",
    "href": "chapters/irt/03_estimation.html#errore-standard-della-stima-e-informazione-dellitem",
    "title": "69  Stima",
    "section": "\n69.3 Errore Standard della Stima e Informazione dell’Item",
    "text": "69.3 Errore Standard della Stima e Informazione dell’Item\nNel modello di Rasch, l’Errore Standard della Stima (EES) è un indicatore chiave che quantifica l’incertezza associata alla stima del livello di abilità di un individuo (\\(\\theta\\)). L’EES è fondamentale perché fornisce una misura della precisione con cui la stima di \\(\\theta\\) riflette l’abilità reale del rispondente. Un EES più basso indica una stima più precisa, mentre un EES più alto segnala una maggiore incertezza.\n\n69.3.1 Calcolo dell’EES\nL’EES è determinato dall’informazione totale dell’item a un dato livello di abilità \\(\\theta\\), indicata con \\(I(\\theta)\\). L’EES è definito come l’inverso della radice quadrata di \\(I(\\theta)\\):\n\\[\n\\text{EES}(\\theta) = \\frac{1}{\\sqrt{I(\\theta)}},\n\\]\ndove \\(I(\\theta)\\) rappresenta l’informazione totale accumulata dagli item del test a quel livello di abilità.\n\n69.3.2 Informazione dell’Item\nL’informazione dell’item misura il contributo di ciascun item alla precisione della stima di \\(\\theta\\). Per un dato livello di abilità, l’informazione fornita da un singolo item dipende dalla probabilità che il rispondente dia una risposta corretta (\\(p_{ij}\\)) e dalla probabilità di una risposta errata (\\(1 - p_{ij}\\)). La formula per calcolare l’informazione totale degli item è:\n\\[\nI(\\theta) = \\sum_{j=1}^{L} p_{ij}(1 - p_{ij}),\n\\]\ndove:\n\n\n\\(L\\) è il numero totale di item del test.\n\n\\(p_{ij}\\) è la probabilità che una persona con abilità \\(\\theta\\) risponda correttamente all’item \\(j\\).\n\nL’informazione fornita da un singolo item raggiunge il suo massimo quando la difficoltà dell’item (\\(\\delta_j\\)) è uguale al livello di abilità del rispondente (\\(\\theta\\)). In questa condizione, l’item discrimina al meglio tra rispondenti con livelli di abilità leggermente superiori o inferiori a \\(\\delta_j\\).\n\n69.3.3 Relazione tra Informazione e Precisione\n\n\nMassima informazione, minima incertezza: Quando \\(I(\\theta)\\) è alta, l’EES (\\(\\text{EES}(\\theta)\\)) è basso, indicando una stima precisa.\n\nBassa informazione, alta incertezza: Quando \\(I(\\theta)\\) è bassa, l’EES è alto, segnalando una maggiore incertezza nella stima di \\(\\theta\\).\n\nQuesta relazione evidenzia l’importanza di progettare test con item che siano informativi per il range di abilità di interesse.\n\n69.3.4 Curva di Informazione dell’Item\nL’informazione dell’item varia a seconda del livello di abilità del rispondente. Per visualizzare questa relazione, si traccia la curva di informazione dell’item, che rappresenta l’informazione fornita da un singolo item in funzione di \\(\\theta\\). Alcune caratteristiche della curva:\n\nHa una forma a campana.\nRaggiunge il picco quando \\(\\theta = \\delta_j\\), ossia quando l’abilità del rispondente corrisponde alla difficoltà dell’item.\nLarghezza e altezza della curva dipendono dalla discriminazione dell’item (nel modello Rasch, fissata a 1).\n\nLa somma delle curve di informazione dei singoli item produce la curva di informazione totale del test, che mostra la precisione complessiva del test a diversi livelli di abilità.\n\n69.3.5 Applicazioni pratiche\n\nProgettazione del test: La conoscenza dell’informazione degli item aiuta a creare test che siano più informativi per specifici livelli di abilità, riducendo l’EES per i range di interesse.\n\nInterpretazione dei risultati: L’EES permette di stimare intervalli di confidenza per \\(\\theta\\), fornendo una misura della precisione della stima:\n\\[\n\\text{Intervallo di confidenza per } \\theta = \\theta \\pm 1.96 \\cdot \\text{EES}(\\theta).\n\\]\n\n\nL’analisi dell’informazione dell’item e del test è quindi essenziale per garantire che le misurazioni ottenute siano affidabili e utili per l’interpretazione e il confronto delle abilità.\n\nEsercizio 69.2 Utilizzando il modello di Rasch, possiamo calcolare le probabilità di risposta corretta per diversi valori di abilità e, di conseguenza, la Funzione Informativa dell’Item (Item Information Function, IIF):\n\n# Definizione di un range di abilità\ntheta &lt;- seq(-4, 4, by = 0.1)\n\n# Definizione di un parametro di difficoltà dell'item\nbeta &lt;- 0\n\n# Calcolo delle probabilità di risposta corretta per ciascun valore di abilità usando la funzione logistica\nprob_correct &lt;- exp(theta - beta) / (1 + exp(theta - beta))\n\n# Calcolo dell'informazione dell'item\nitem_info &lt;- prob_correct * (1 - prob_correct)\n\n# Creazione della prima grafica (ICC)\nplot(theta, prob_correct,\n    type = \"l\", col = \"blue\", lwd = 2,\n    xlab = \"Abilita' theta\", ylab = \"Probabilita' di Risposta Corretta\",\n    main = \"Curva Caratteristica dell'Item (ICC) e Informazione dell'Item\"\n)\n\n# Aggiunta di un secondo asse y per l'informazione\npar(new = TRUE)\nplot(theta, item_info,\n    type = \"l\", col = \"red\", lwd = 2,\n    xlab = \"\", ylab = \"\", axes = FALSE, ann = FALSE\n)\n\n# Aggiungere l'asse y di destra per l'informazione\naxis(side = 4, at = pretty(range(item_info)))\nmtext(\"Informazione\", side = 4, line = 3)\n\n# Aggiunta della legenda\nlegend(\"topright\",\n    legend = c(\"ICC\", \"Informazione\"),\n    col = c(\"blue\", \"red\"), lty = 1, cex = 0.8\n)\n\n\n\n\n\n\n\nQuesta rappresentazione grafica in R mostra come l’informazione vari in funzione del livello di abilità. In generale, l’informazione è massima quando l’abilità dell’esaminando è vicina alla difficoltà dell’item e diminuisce man mano che ci si allontana da questo punto.\nIl concetto di informazione in IRT è fondamentale sia per la costruzione del test sia per la sua interpretazione. Indica quanto efficacemente ciascun item misura l’abilità a vari livelli e aiuta a determinare quali item sono più informativi per la stima dell’abilità degli esaminandi. Inoltre, fornisce indicazioni sulla precisione con cui l’abilità degli esaminandi può essere stimata a vari punti lungo la scala di abilità.\n\n\nEsercizio 69.3 Per dimostrare come calcolare la TIF in \\(\\mathsf{R}\\), possiamo estendere l’esempio precedente includendo più item e sommando le loro informazioni:\n\n# Definizione di parametri di difficoltà per diversi item\nbeta_items &lt;- c(-1, 0, 1) # Esempio di tre item con difficoltà diverse\n\n# Calcolo dell'informazione per ogni item e somma per ottenere la TIF\ntest_info &lt;- rep(0, length(theta))\nfor (beta in beta_items) {\n    prob_correct &lt;- exp(theta - beta) / (1 + exp(theta - beta))\n    item_info &lt;- prob_correct * (1 - prob_correct)\n    test_info &lt;- test_info + item_info\n}\n\n# Creazione del grafico della TIF\nplot(theta, test_info,\n    type = \"l\", col = \"blue\", lwd = 2,\n    xlab = \"Abilità theta\", ylab = \"Informazione del Test\",\n    main = \"Funzione di Informazione del Test (TIF)\"\n)\n\n\n\n\n\n\n\nIn questo esempio, calcoliamo e sommiamo le informazioni di tre item con diverse difficoltà per visualizzare la TIF di un test ipotetico. La TIF mostra in modo chiaro come il test nel suo insieme stima l’abilità degli esaminandi a vari livelli, fornendo così indicazioni preziose sulla costruzione e sull’utilizzo ottimale del test in diversi contesti.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>Stima</span>"
    ]
  },
  {
    "objectID": "chapters/irt/03_estimation.html#stima-dellabilità",
    "href": "chapters/irt/03_estimation.html#stima-dellabilità",
    "title": "69  Stima",
    "section": "\n69.4 Stima dell’Abilità",
    "text": "69.4 Stima dell’Abilità\nNel contesto dell’IRT, la stima dell’abilità di un esaminando (\\(\\theta\\)) viene effettuata utilizzando metodi iterativi, come la massima verosimiglianza, che sfruttano i dati del test e i parametri degli item. Questo processo consente di stimare il livello di abilità in modo personalizzato, tenendo conto del pattern di risposte specifico di ciascun esaminando.\n\n69.4.1 Procedura di Stima dell’Abilità\n\n\nPunto di partenza:\n\nLa stima inizia con un’ipotesi iniziale o un valore a priori per l’abilità dell’esaminando. Questo valore può essere scelto in base a considerazioni teoriche (ad esempio, \\(\\theta = 0\\), corrispondente alla media presunta dell’abilità) o determinato da informazioni preliminari.\n\n\n\nUtilizzo dei parametri degli item:\n\nI parametri noti degli item (ad esempio, difficoltà (_i) e discriminazione (a_i)) vengono utilizzati per calcolare la probabilità che l’esaminando risponda correttamente a ciascun item in base al livello di abilità iniziale ipotizzato. Questa probabilità è calcolata attraverso la funzione di risposta dell’item (IRF).\n\n\n\nIterazione per aggiustare la stima:\n\nIl livello di abilità viene aggiornato iterativamente. L’obiettivo di ogni iterazione è migliorare la corrispondenza tra le probabilità previste di risposta corretta (basate sul livello di abilità stimato) e il pattern effettivo di risposte fornite dall’esaminando.\nQuesto processo continua fino a quando le modifiche alla stima di \\(\\theta\\) diventano trascurabili, indicando che è stato raggiunto un punto di convergenza. Il risultato finale è una stima stabile e affidabile dell’abilità.\n\n\n\nStima personalizzata:\n\nIl processo viene ripetuto per ciascun esaminando, assicurando che ogni stima di \\(\\theta\\) sia basata esclusivamente sulle sue risposte.\n\n\n\n69.4.2 Metodi alternativi di stima\n\n\nStima simultanea:\n\nIn alternativa alla stima iterativa individuale, esistono approcci che stimano simultaneamente i livelli di abilità di tutti gli esaminandi. Questi metodi sono particolarmente utili in presenza di un ampio campione, ottimizzando il processo di calcolo.\n\n\n\nStima Bayesiana:\n\nLa stima bayesiana combina i dati del test con una distribuzione a priori sull’abilità (\\(\\theta\\)) per ottenere una stima posteriore. Questo approccio è particolarmente utile quando il numero di item è limitato o le risposte sono incomplete.\n\n\n\n69.4.3 Importanza della Stima dell’Abilità\nLa stima dell’abilità in IRT è fondamentale per due motivi principali:\n\n\nValutazione personalizzata:\n\nPermette di misurare l’abilità di ciascun esaminando in maniera individualizzata, considerando le interazioni specifiche tra il rispondente e gli item. Questa personalizzazione rende la stima più accurata rispetto ai punteggi grezzi, che non tengono conto delle caratteristiche degli item.\n\n\n\nAnalisi mirate:\n\nPoiché la stima dell’abilità è direttamente legata ai parametri degli item, consente di condurre analisi dettagliate sull’efficacia del test (ad esempio, quali item sono più informativi per specifici livelli di abilità) e sulle caratteristiche dei rispondenti.\n\n\n\nIn conclusione, la stima dell’abilità in IRT è un processo iterativo che utilizza i parametri degli item e il pattern di risposte individuali per fornire stime accurate e personalizzate del livello di abilità di ciascun esaminando. Grazie alla sua precisione, questa metodologia rappresenta una componente essenziale dell’IRT, sia per la valutazione degli esaminandi sia per l’ottimizzazione dei test.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>Stima</span>"
    ]
  },
  {
    "objectID": "chapters/irt/03_estimation.html#stima-bayesiana",
    "href": "chapters/irt/03_estimation.html#stima-bayesiana",
    "title": "69  Stima",
    "section": "\n69.5 Stima Bayesiana",
    "text": "69.5 Stima Bayesiana\nLa stima bayesiana sta diventando un metodo sempre più popolare per stimare i parametri del modello di Rasch. Come la stima della massima verosimiglianza congiunta, la stima bayesiana stima simultaneamente sia i parametri delle persone che quelli degli item. Tuttavia, mentre la stima della massima verosimiglianza congiunta trova i valori di \\(\\theta\\) e \\(\\beta\\) massimizzando la verosimiglianza congiunta, la stima bayesiana utilizza la regola di Bayes per trovare la densità a posteriori, \\(f(\\theta,\\beta \\mid u)\\).\nNel modello di Rasch, la regola di Bayes afferma che:\n\\[\nf(\\theta,\\beta \\mid u) = \\frac{\\text{Pr}(u \\mid \\theta,\\beta)f(\\theta,\\beta)}{\\text{Pr}(u)}.\n\\]\nIl primo termine nel numeratore, \\(\\text{Pr}(u \\mid \\theta, \\beta)\\), è la verosimiglianza congiunta. Il secondo è la distribuzione a priori congiunta per \\(\\theta\\) e \\(\\beta\\). Il denominatore è la probabilità media dei dati osservati rispetto alla distribuzione a priori congiunta.\nA differenza della stima della massima verosimiglianza, che si concentra sulla massimizzazione della verosimiglianza, la stima bayesiana integra le informazioni a priori con i dati osservati. La regola di Bayes combina la verosimiglianza dei dati osservati (la probabilità di osservare i dati dati i parametri) con la distribuzione a priori (le nostre credenze sui parametri prima di osservare i dati) per produrre una distribuzione a posteriori (le nostre credenze aggiornate sui parametri dopo aver osservato i dati). La densità a posteriori \\(f(\\theta,\\beta \\mid u)\\) ci fornisce una stima completa dei parametri, considerando sia i dati osservati sia le informazioni a priori.\nIn pratica, la stima bayesiana fornisce un approccio flessibile e informativo alla stima dei parametri nel modello di Rasch, consentendo l’integrazione di conoscenze pregresse e osservazioni attuali.\n\n69.5.1 Implementazione\nEsaminiamo un’applicazione della stima Bayesiana usando il linguaggio probabilistico Stan. Il modello di Rasch è implementato nel file rasch_model.stan utilizzando le distribuzioni a priori specificate da Debelak et al. (2022).\n\nstan_file &lt;- \"../../code/rasch_model.stan\"\nmod &lt;- cmdstan_model(stan_file)\nmod$print()\n#&gt; data {\n#&gt;   int&lt;lower=1&gt; num_person;\n#&gt;   int&lt;lower=1&gt; num_item;\n#&gt;   array[num_person, num_item] int&lt;lower=0, upper=1&gt; U;\n#&gt; }\n#&gt; parameters {\n#&gt;   vector[num_person] theta;\n#&gt;   vector[num_item] beta;\n#&gt;   real mu_beta;\n#&gt;   real&lt;lower=0&gt; sigma2_theta;\n#&gt;   real&lt;lower=0&gt; sigma2_beta;\n#&gt; }\n#&gt; transformed parameters {\n#&gt;   array[num_person, num_item] real&lt;lower=0, upper=1&gt; prob_solve;\n#&gt;   for (p in 1:num_person) \n#&gt;     for (i in 1:num_item) \n#&gt;       prob_solve[p, i] = inv_logit(theta[p] - beta[i]);\n#&gt; }\n#&gt; model {\n#&gt;   for (p in 1:num_person) \n#&gt;     for (i in 1:num_item) \n#&gt;       U[p, i] ~ bernoulli(prob_solve[p, i]);\n#&gt;   theta ~ normal(0, sqrt(sigma2_theta));\n#&gt;   beta ~ normal(mu_beta, sqrt(sigma2_beta));\n#&gt;   sigma2_theta ~ inv_chi_square(0.5);\n#&gt;   sigma2_beta ~ inv_chi_square(0.5);\n#&gt; }\n\nNella presente implementazione bayesiana del modello di Rasch, le sezioni “transformed parameters” e “model” hanno un ruolo centrale nel definire come i dati vengono processati e come il modello viene applicato. Vediamo dettagliatamente ciascuna sezione:\n\n69.5.1.1 Sezione Transformed Parameters\nNella sezione transformed parameters, viene definita la trasformazione dei parametri di base (i parametri theta per le abilità delle persone e beta per la difficoltà degli item) in una probabilità di risposta corretta per ogni coppia persona-item. Qui viene usata la funzione logistica inversa per convertire la differenza tra l’abilità della persona e la difficoltà dell’item in una probabilità:\ntransformed parameters {\n  array[num_person, num_item] real&lt;lower=0, upper=1&gt; prob_solve;\n  for (p in 1:num_person) \n    for (i in 1:num_item) \n      prob_solve[p, i] = inv_logit(theta[p] - beta[i]);\n}\nQuesta trasformazione serve a mappare la differenza tra l’abilità della persona (theta[p]) e la difficoltà dell’item (beta[i]) in un intervallo di probabilità tra 0 e 1. La funzione inv_logit è comunemente usata per questo scopo, essendo la funzione logistica inversa.\n\n69.5.1.2 Sezione Model\nNella sezione model, vengono definite le distribuzioni di probabilità per i dati osservati e i parametri del modello, che sono essenziali per la stima bayesiana. Questa parte del codice descrive come i dati sono generati, supponendo il modello di Rasch:\nmodel {\n  for (p in 1:num_person) \n    for (i in 1:num_item) \n      U[p, i] ~ bernoulli(prob_solve[p, i]);\n  theta ~ normal(0, sqrt(sigma2_theta));\n  beta ~ normal(mu_beta, sqrt(sigma2_beta));\n  sigma2_theta ~ inv_chi_square(0.5);\n  sigma2_beta ~ inv_chi_square(0.5);\n}\n\n\nU[p, i] ~ bernoulli(prob_solve[p, i]): ogni risposta U[p, i], che indica se la persona p ha risposto correttamente all’item i, segue una distribuzione di Bernoulli dove la probabilità di successo è data da prob_solve[p, i]. Questa è la vera verosimiglianza del modello, che collega i dati osservati alle probabilità calcolate tramite il modello logistico.\n\ntheta ~ normal(0, sqrt(sigma2_theta)) e beta ~ normal(mu_beta, sqrt(sigma2_beta)): le distribuzioni a priori per i parametri theta e beta sono normali. Questo significa che, in assenza di dati, si assume che queste variabili si distribuiscano normalmente con una media di 0 per theta e mu_beta per beta, e una deviazione standard derivata dai parametri di varianza sigma2_theta e sigma2_beta.\n\nsigma2_theta ~ inv_chi_square(0.5) e sigma2_beta ~ inv_chi_square(0.5): le varianze sigma2_theta e sigma2_beta hanno distribuzioni a priori che seguono una distribuzione chi quadrato inversa con parametro di forma 0.5. Questa è una scelta comune per imporre una distribuzione non informativa (vaga) sui parametri di scala.\n\nIn conclusione, la sezione transformed parameters calcola le probabilità di risposta corretta basate sui parametri di abilità e difficoltà, mentre la sezione model specifica come questi parametri e le risposte osservate interagiscono secondo il modello di Rasch, definendo così la struttura della verosimiglianza e delle priorità nel contesto bayesiano.\nCompiliamo il modello usando CmdStan:\n\nmod$compile()\n\nDefiniamo i dati nel formato appropriato per Stan:\n\ndata(data.fims.Aus.Jpn.scored, package = \"TAM\")\npeople &lt;- 1:400\nresponses &lt;- data.fims.Aus.Jpn.scored[people, 2:15]\nresponses &lt;- as.matrix(sapply(responses, as.integer))\ncolnames(responses) &lt;- gsub(\"M1PTI\", \"I\", colnames(responses))\n\nstan_data &lt;- list(\n    num_person = nrow(responses),\n    num_item = ncol(responses),\n    U = responses\n)\n\nEseguiamo il campionamento MCMC per ottenere la distribuzione a posteriori dei parametri.\n\nfit &lt;- mod$sample(\n    data = stan_data,\n    chains = 4, # Number of MCMC chains\n    parallel_chains = 2, # Number of chains to run in parallel \n    iter_warmup = 2000, # Number of warmup iterations per chain\n    iter_sampling = 2000, # Number of sampling iterations per chain\n    seed = 1234 # Set a seed for reproducibility\n)\n#&gt; Running MCMC with 4 chains, at most 2 in parallel...\n#&gt; \n#&gt; Chain 1 Iteration:    1 / 4000 [  0%]  (Warmup)\n#&gt; Chain 2 Iteration:    1 / 4000 [  0%]  (Warmup)\n#&gt; Chain 1 Iteration:  100 / 4000 [  2%]  (Warmup) \n#&gt; Chain 2 Iteration:  100 / 4000 [  2%]  (Warmup) \n#&gt; Chain 1 Iteration:  200 / 4000 [  5%]  (Warmup) \n#&gt; Chain 2 Iteration:  200 / 4000 [  5%]  (Warmup) \n#&gt; Chain 1 Iteration:  300 / 4000 [  7%]  (Warmup) \n#&gt; Chain 2 Iteration:  300 / 4000 [  7%]  (Warmup) \n#&gt; Chain 1 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#&gt; Chain 2 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#&gt; Chain 1 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#&gt; Chain 2 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#&gt; Chain 1 Iteration:  600 / 4000 [ 15%]  (Warmup) \n#&gt; Chain 2 Iteration:  600 / 4000 [ 15%]  (Warmup) \n#&gt; Chain 1 Iteration:  700 / 4000 [ 17%]  (Warmup) \n#&gt; Chain 2 Iteration:  700 / 4000 [ 17%]  (Warmup) \n#&gt; Chain 1 Iteration:  800 / 4000 [ 20%]  (Warmup) \n#&gt; Chain 2 Iteration:  800 / 4000 [ 20%]  (Warmup) \n#&gt; Chain 1 Iteration:  900 / 4000 [ 22%]  (Warmup) \n#&gt; Chain 2 Iteration:  900 / 4000 [ 22%]  (Warmup) \n#&gt; Chain 1 Iteration: 1000 / 4000 [ 25%]  (Warmup) \n#&gt; Chain 2 Iteration: 1000 / 4000 [ 25%]  (Warmup) \n#&gt; Chain 1 Iteration: 1100 / 4000 [ 27%]  (Warmup) \n#&gt; Chain 2 Iteration: 1100 / 4000 [ 27%]  (Warmup) \n#&gt; Chain 1 Iteration: 1200 / 4000 [ 30%]  (Warmup) \n#&gt; Chain 2 Iteration: 1200 / 4000 [ 30%]  (Warmup) \n#&gt; Chain 1 Iteration: 1300 / 4000 [ 32%]  (Warmup) \n#&gt; Chain 2 Iteration: 1300 / 4000 [ 32%]  (Warmup) \n#&gt; Chain 1 Iteration: 1400 / 4000 [ 35%]  (Warmup) \n#&gt; Chain 2 Iteration: 1400 / 4000 [ 35%]  (Warmup) \n#&gt; Chain 1 Iteration: 1500 / 4000 [ 37%]  (Warmup) \n#&gt; Chain 2 Iteration: 1500 / 4000 [ 37%]  (Warmup) \n#&gt; Chain 1 Iteration: 1600 / 4000 [ 40%]  (Warmup) \n#&gt; Chain 2 Iteration: 1600 / 4000 [ 40%]  (Warmup) \n#&gt; Chain 1 Iteration: 1700 / 4000 [ 42%]  (Warmup) \n#&gt; Chain 2 Iteration: 1700 / 4000 [ 42%]  (Warmup) \n#&gt; Chain 1 Iteration: 1800 / 4000 [ 45%]  (Warmup) \n#&gt; Chain 2 Iteration: 1800 / 4000 [ 45%]  (Warmup) \n#&gt; Chain 1 Iteration: 1900 / 4000 [ 47%]  (Warmup) \n#&gt; Chain 2 Iteration: 1900 / 4000 [ 47%]  (Warmup) \n#&gt; Chain 1 Iteration: 2000 / 4000 [ 50%]  (Warmup) \n#&gt; Chain 1 Iteration: 2001 / 4000 [ 50%]  (Sampling) \n#&gt; Chain 2 Iteration: 2000 / 4000 [ 50%]  (Warmup) \n#&gt; Chain 2 Iteration: 2001 / 4000 [ 50%]  (Sampling) \n#&gt; Chain 1 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#&gt; Chain 2 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#&gt; Chain 1 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#&gt; Chain 2 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#&gt; Chain 1 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#&gt; Chain 2 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#&gt; Chain 1 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#&gt; Chain 2 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#&gt; Chain 1 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#&gt; Chain 2 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#&gt; Chain 1 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#&gt; Chain 2 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#&gt; Chain 1 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#&gt; Chain 2 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#&gt; Chain 1 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#&gt; Chain 2 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#&gt; Chain 1 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#&gt; Chain 2 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#&gt; Chain 1 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#&gt; Chain 2 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#&gt; Chain 1 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#&gt; Chain 2 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#&gt; Chain 1 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#&gt; Chain 2 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#&gt; Chain 1 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#&gt; Chain 2 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#&gt; Chain 1 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#&gt; Chain 2 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#&gt; Chain 1 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#&gt; Chain 2 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#&gt; Chain 1 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#&gt; Chain 2 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#&gt; Chain 1 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#&gt; Chain 2 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#&gt; Chain 1 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#&gt; Chain 2 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#&gt; Chain 1 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#&gt; Chain 2 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#&gt; Chain 1 Iteration: 4000 / 4000 [100%]  (Sampling) \n#&gt; Chain 1 finished in 14.5 seconds.\n#&gt; Chain 3 Iteration:    1 / 4000 [  0%]  (Warmup)\n#&gt; Chain 2 Iteration: 4000 / 4000 [100%]  (Sampling) \n#&gt; Chain 2 finished in 14.7 seconds.\n#&gt; Chain 4 Iteration:    1 / 4000 [  0%]  (Warmup) \n#&gt; Chain 3 Iteration:  100 / 4000 [  2%]  (Warmup) \n#&gt; Chain 3 Iteration:  200 / 4000 [  5%]  (Warmup) \n#&gt; Chain 4 Iteration:  100 / 4000 [  2%]  (Warmup) \n#&gt; Chain 3 Iteration:  300 / 4000 [  7%]  (Warmup) \n#&gt; Chain 4 Iteration:  200 / 4000 [  5%]  (Warmup) \n#&gt; Chain 3 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#&gt; Chain 4 Iteration:  300 / 4000 [  7%]  (Warmup) \n#&gt; Chain 4 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#&gt; Chain 3 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#&gt; Chain 3 Iteration:  600 / 4000 [ 15%]  (Warmup) \n#&gt; Chain 4 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#&gt; Chain 3 Iteration:  700 / 4000 [ 17%]  (Warmup) \n#&gt; Chain 4 Iteration:  600 / 4000 [ 15%]  (Warmup) \n#&gt; Chain 3 Iteration:  800 / 4000 [ 20%]  (Warmup) \n#&gt; Chain 4 Iteration:  700 / 4000 [ 17%]  (Warmup) \n#&gt; Chain 3 Iteration:  900 / 4000 [ 22%]  (Warmup) \n#&gt; Chain 4 Iteration:  800 / 4000 [ 20%]  (Warmup) \n#&gt; Chain 3 Iteration: 1000 / 4000 [ 25%]  (Warmup) \n#&gt; Chain 4 Iteration:  900 / 4000 [ 22%]  (Warmup) \n#&gt; Chain 3 Iteration: 1100 / 4000 [ 27%]  (Warmup) \n#&gt; Chain 4 Iteration: 1000 / 4000 [ 25%]  (Warmup) \n#&gt; Chain 3 Iteration: 1200 / 4000 [ 30%]  (Warmup) \n#&gt; Chain 4 Iteration: 1100 / 4000 [ 27%]  (Warmup) \n#&gt; Chain 3 Iteration: 1300 / 4000 [ 32%]  (Warmup) \n#&gt; Chain 4 Iteration: 1200 / 4000 [ 30%]  (Warmup) \n#&gt; Chain 3 Iteration: 1400 / 4000 [ 35%]  (Warmup) \n#&gt; Chain 4 Iteration: 1300 / 4000 [ 32%]  (Warmup) \n#&gt; Chain 3 Iteration: 1500 / 4000 [ 37%]  (Warmup) \n#&gt; Chain 4 Iteration: 1400 / 4000 [ 35%]  (Warmup) \n#&gt; Chain 3 Iteration: 1600 / 4000 [ 40%]  (Warmup) \n#&gt; Chain 4 Iteration: 1500 / 4000 [ 37%]  (Warmup) \n#&gt; Chain 3 Iteration: 1700 / 4000 [ 42%]  (Warmup) \n#&gt; Chain 4 Iteration: 1600 / 4000 [ 40%]  (Warmup) \n#&gt; Chain 3 Iteration: 1800 / 4000 [ 45%]  (Warmup) \n#&gt; Chain 4 Iteration: 1700 / 4000 [ 42%]  (Warmup) \n#&gt; Chain 3 Iteration: 1900 / 4000 [ 47%]  (Warmup) \n#&gt; Chain 4 Iteration: 1800 / 4000 [ 45%]  (Warmup) \n#&gt; Chain 3 Iteration: 2000 / 4000 [ 50%]  (Warmup) \n#&gt; Chain 3 Iteration: 2001 / 4000 [ 50%]  (Sampling) \n#&gt; Chain 4 Iteration: 1900 / 4000 [ 47%]  (Warmup) \n#&gt; Chain 3 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#&gt; Chain 4 Iteration: 2000 / 4000 [ 50%]  (Warmup) \n#&gt; Chain 4 Iteration: 2001 / 4000 [ 50%]  (Sampling) \n#&gt; Chain 3 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#&gt; Chain 4 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#&gt; Chain 4 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#&gt; Chain 3 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#&gt; Chain 4 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#&gt; Chain 3 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#&gt; Chain 3 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#&gt; Chain 4 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#&gt; Chain 3 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#&gt; Chain 4 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#&gt; Chain 3 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#&gt; Chain 4 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#&gt; Chain 4 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#&gt; Chain 3 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#&gt; Chain 4 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#&gt; Chain 3 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#&gt; Chain 4 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#&gt; Chain 3 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#&gt; Chain 4 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#&gt; Chain 3 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#&gt; Chain 4 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#&gt; Chain 3 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#&gt; Chain 3 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#&gt; Chain 4 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#&gt; Chain 3 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#&gt; Chain 4 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#&gt; Chain 4 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#&gt; Chain 3 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#&gt; Chain 4 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#&gt; Chain 3 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#&gt; Chain 4 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#&gt; Chain 3 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#&gt; Chain 3 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#&gt; Chain 4 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#&gt; Chain 3 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#&gt; Chain 4 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#&gt; Chain 4 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#&gt; Chain 3 Iteration: 4000 / 4000 [100%]  (Sampling) \n#&gt; Chain 3 finished in 14.9 seconds.\n#&gt; Chain 4 Iteration: 4000 / 4000 [100%]  (Sampling) \n#&gt; Chain 4 finished in 15.0 seconds.\n#&gt; \n#&gt; All 4 chains finished successfully.\n#&gt; Mean chain execution time: 14.8 seconds.\n#&gt; Total execution time: 29.9 seconds.\n\nEsaminiamo le tracce per due parametri.\n\nfit_draws &lt;- fit$draws() # extract the posterior draws\nmcmc_trace(fit_draws, pars = c(\"beta[1]\"))\n\n\n\n\n\n\n\n\nmcmc_trace(fit_draws, pars = c(\"theta[1]\"))\n\n\n\n\n\n\n\nFocalizziamoci sulla stima dei parametri degli item.\n\nparameters &lt;- c(\n    \"beta[1]\", \"beta[2]\", \"beta[3]\", \"beta[4]\", \"beta[5]\",\n    \"beta[6]\", \"beta[7]\", \"beta[8]\", \"beta[9]\",\"beta[10]\",\n    \"beta[11]\", \"beta[12]\", \"beta[13]\", \"beta[14]\"\n)\n\nEsaminiamo la statistica rhat.\n\nrhats &lt;- rhat(fit_draws, pars = parameters)\nmcmc_rhat(rhats)\n\n\n\n\n\n\n\nEsaminiamo l’effect ratio:\n\neff_ratio &lt;- neff_ratio(fit, pars = parameters)\neff_ratio \n#&gt;  beta[1]  beta[2]  beta[3]  beta[4]  beta[5]  beta[6]  beta[7]  beta[8] \n#&gt;    1.132    1.189    1.162    1.098    1.357    1.160    1.176    1.082 \n#&gt;  beta[9] beta[10] beta[11] beta[12] beta[13] beta[14] \n#&gt;    1.301    1.208    1.263    1.235    1.392    1.306\n\n\nmcmc_neff(eff_ratio)\n\n\n\n\n\n\n\nEsaminiamo l’autocorrelazione.\n\nmcmc_acf(fit_draws, pars = parameters)\n\n\n\n\n\n\n\nOtteniamo le statistiche riassuntive delle distribuzioni a posteriori dei parametri degli item.\n\nfit$summary(\n    variables = parameters,\n    posterior::default_summary_measures(),\n    extra_quantiles = ~ posterior::quantile2(., probs = c(.0275, .975))\n)\n#&gt; # A tibble: 14 × 9\n#&gt;    variable    mean  median    sd   mad     q5    q95  q2.75  q97.5\n#&gt;    &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1 beta[1]  -1.10   -1.10   0.129 0.126 -1.31  -0.892 -1.35  -0.849\n#&gt;  2 beta[2]  -1.24   -1.24   0.130 0.130 -1.46  -1.03  -1.50  -0.997\n#&gt;  3 beta[3]  -2.03   -2.02   0.158 0.156 -2.30  -1.77  -2.34  -1.73 \n#&gt;  4 beta[4]  -0.0478 -0.0489 0.120 0.121 -0.241  0.149 -0.274  0.191\n#&gt;  5 beta[5]   2.51    2.51   0.183 0.185  2.22   2.82   2.18   2.88 \n#&gt;  6 beta[6]  -1.24   -1.24   0.132 0.131 -1.46  -1.03  -1.50  -0.989\n#&gt;  7 beta[7]   0.811   0.810  0.124 0.124  0.610  1.01   0.576  1.05 \n#&gt;  8 beta[8]  -0.493  -0.493  0.120 0.119 -0.688 -0.294 -0.723 -0.263\n#&gt;  9 beta[9]   1.32    1.32   0.135 0.138  1.11   1.55   1.07   1.58 \n#&gt; 10 beta[10] -0.395  -0.395  0.120 0.121 -0.592 -0.196 -0.622 -0.158\n#&gt; 11 beta[11]  2.05    2.05   0.158 0.159  1.80   2.31   1.75   2.37 \n#&gt; 12 beta[12]  1.75    1.74   0.148 0.146  1.50   1.99   1.47   2.04 \n#&gt; 13 beta[13]  2.40    2.39   0.172 0.172  2.12   2.69   2.07   2.74 \n#&gt; 14 beta[14] -1.92   -1.92   0.152 0.148 -2.18  -1.67  -2.22  -1.63\n\nI risultati ottenuti replicano quelli riportati da Debelak et al. (2022).",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>Stima</span>"
    ]
  },
  {
    "objectID": "chapters/irt/03_estimation.html#grandezza-del-campione",
    "href": "chapters/irt/03_estimation.html#grandezza-del-campione",
    "title": "69  Stima",
    "section": "\n69.6 Grandezza del Campione",
    "text": "69.6 Grandezza del Campione\nLa stima dei parametri degli item basata su un campione osservato di risposte è spesso definita come la calibrazione degli item. Generalmente, un campione di calibrazione più ampio consente una stima più accurata dei parametri degli item, sebbene altri fattori influenzino anch’essi l’accuratezza della stima. Ad esempio, la difficoltà di un item può essere stimata con maggiore precisione se l’item non è né troppo facile né troppo difficile per il campione di partecipanti al test. Pertanto, i fattori che influenzano l’accuratezza della stima includono l’allineamento e la forma delle distribuzioni dei parametri degli item e delle persone, il numero di item e la tecnica di stima utilizzata.\nDiverse pubblicazioni hanno affrontato la questione della dimensione del campione tipicamente necessaria per lavorare con il modello di Rasch e come questa sia influenzata da questi e altri fattori. Ad esempio, De Ayala (2009) fornisce la linea guida generale che un campione di calibrazione dovrebbe contenere almeno diverse centinaia di rispondenti e cita, tra le altre referenze, un articolo precedente di Wright (1977) che afferma che un campione di calibrazione di 500 sarebbe più che adeguato. De Ayala (2009) suggerisce anche che 250 o più rispondenti sono necessari per adattare un modello di Partial Credit. Poiché il modello di Partial Credit è una generalizzazione del modello di Rasch con più parametri degli item, ciò implica che la dimensione del campione suggerita di 250 dovrebbe essere sufficiente anche per adattare un modello di Rasch. Studi più recenti hanno indagato l’applicazione del modello di Rasch con dimensioni del campione di soli 100 rispondenti (ad esempio, Steinfeld & Robitzsch, 2021; Suárez-Falcón & Glas, 2003). Tali linee guida non devono essere interpretate come regole fisse, ma solo come indicazioni generali in quanto una dimensione del campione adeguata dipende dalle condizioni e dagli obiettivi dell’analisi.\nUn metodo più elaborato per determinare la dimensione del campione necessaria è l’analisi della potenza statistica. Qui, l’accuratezza della stima desiderata o il rischio di falsi positivi e falsi negativi devono essere formalizzati prima dell’analisi. La dimensione del campione necessaria viene quindi determinata in base a queste considerazioni.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>Stima</span>"
    ]
  },
  {
    "objectID": "chapters/irt/03_estimation.html#session-info",
    "href": "chapters/irt/03_estimation.html#session-info",
    "title": "69  Stima",
    "section": "\n69.7 Session Info",
    "text": "69.7 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.2\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats4    grid      stats     graphics  grDevices utils     datasets \n#&gt; [8] methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] effectsize_1.0.0    rsvg_2.6.1          rstan_2.32.7       \n#&gt;  [4] StanHeaders_2.32.10 posterior_1.6.1     cmdstanr_0.8.1     \n#&gt;  [7] ggmirt_0.1.0        TAM_4.2-21          CDM_8.2-6          \n#&gt; [10] mvtnorm_1.3-3       mirt_1.44.0         lattice_0.22-6     \n#&gt; [13] latex2exp_0.9.6     ggokabeito_0.1.0    see_0.11.0         \n#&gt; [16] MASS_7.3-65         viridis_0.6.5       viridisLite_0.4.2  \n#&gt; [19] ggpubr_0.6.0        ggExtra_0.10.1      gridExtra_2.3      \n#&gt; [22] patchwork_1.3.0     bayesplot_1.11.1    semTools_0.5-6     \n#&gt; [25] semPlot_1.1.6       lavaan_0.6-19       psych_2.4.12       \n#&gt; [28] scales_1.3.0        markdown_1.13       knitr_1.50         \n#&gt; [31] lubridate_1.9.4     forcats_1.0.0       stringr_1.5.1      \n#&gt; [34] dplyr_1.1.4         purrr_1.0.4         readr_2.1.5        \n#&gt; [37] tidyr_1.3.1         tibble_3.2.1        ggplot2_3.5.1      \n#&gt; [40] tidyverse_2.0.0     here_1.0.1         \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] splines_4.4.2        later_1.4.1          R.oo_1.27.0         \n#&gt;   [4] datawizard_1.0.1     XML_3.99-0.18        rpart_4.1.24        \n#&gt;   [7] lifecycle_1.0.4      Rdpack_2.6.3         rstatix_0.7.2       \n#&gt;  [10] rprojroot_2.0.4      processx_3.8.6       globals_0.16.3      \n#&gt;  [13] insight_1.1.0        rockchalk_1.8.157    backports_1.5.0     \n#&gt;  [16] magrittr_2.0.3       openxlsx_4.2.8       Hmisc_5.2-3         \n#&gt;  [19] rmarkdown_2.29       yaml_2.3.10          httpuv_1.6.15       \n#&gt;  [22] qgraph_1.9.8         zip_2.3.2            pkgbuild_1.4.6      \n#&gt;  [25] sessioninfo_1.2.3    pbapply_1.7-2        minqa_1.2.8         \n#&gt;  [28] multcomp_1.4-28      abind_1.4-8          audio_0.1-11        \n#&gt;  [31] quadprog_1.5-8       R.utils_2.13.0       tensorA_0.36.2.1    \n#&gt;  [34] nnet_7.3-20          TH.data_1.1-3        sandwich_3.1-1      \n#&gt;  [37] inline_0.3.21        listenv_0.9.1        testthat_3.2.3      \n#&gt;  [40] vegan_2.6-10         arm_1.14-4           parallelly_1.42.0   \n#&gt;  [43] permute_0.9-7        codetools_0.2-20     tidyselect_1.2.1    \n#&gt;  [46] farver_2.1.2         lme4_1.1-36          matrixStats_1.5.0   \n#&gt;  [49] base64enc_0.1-3      jsonlite_1.9.1       polycor_0.8-1       \n#&gt;  [52] progressr_0.15.1     Formula_1.2-5        survival_3.8-3      \n#&gt;  [55] emmeans_1.10.7       tools_4.4.2          Rcpp_1.0.14         \n#&gt;  [58] glue_1.8.0           mnormt_2.1.1         admisc_0.37         \n#&gt;  [61] xfun_0.51            mgcv_1.9-1           distributional_0.5.0\n#&gt;  [64] loo_2.8.0            withr_3.0.2          beepr_2.0           \n#&gt;  [67] fastmap_1.2.0        boot_1.3-31          digest_0.6.37       \n#&gt;  [70] mi_1.1               timechange_0.3.0     R6_2.6.1            \n#&gt;  [73] mime_0.13            estimability_1.5.1   colorspace_2.1-1    \n#&gt;  [76] gtools_3.9.5         jpeg_0.1-10          R.methodsS3_1.8.2   \n#&gt;  [79] utf8_1.2.4           generics_0.1.3       data.table_1.17.0   \n#&gt;  [82] corpcor_1.6.10       SimDesign_2.19.1     htmlwidgets_1.6.4   \n#&gt;  [85] parameters_0.24.2    pkgconfig_2.0.3      sem_3.1-16          \n#&gt;  [88] gtable_0.3.6         brio_1.1.5           htmltools_0.5.8.1   \n#&gt;  [91] carData_3.0-5        png_0.1-8            reformulas_0.4.0    \n#&gt;  [94] rstudioapi_0.17.1    tzdb_0.5.0           reshape2_1.4.4      \n#&gt;  [97] curl_6.2.1           coda_0.19-4.1        checkmate_2.3.2     \n#&gt; [100] nlme_3.1-167         nloptr_2.2.1         zoo_1.8-13          \n#&gt; [103] parallel_4.4.2       miniUI_0.1.1.1       foreign_0.8-88      \n#&gt; [106] pillar_1.10.1        vctrs_0.6.5          promises_1.3.2      \n#&gt; [109] car_3.1-3            OpenMx_2.21.13       xtable_1.8-4        \n#&gt; [112] Deriv_4.1.6          cluster_2.1.8.1      dcurver_0.9.2       \n#&gt; [115] GPArotation_2024.3-1 htmlTable_2.4.3      evaluate_1.0.3      \n#&gt; [118] pbivnorm_0.6.0       cli_3.6.4            kutils_1.73         \n#&gt; [121] compiler_4.4.2       rlang_1.1.5          future.apply_1.11.3 \n#&gt; [124] ggsignif_0.6.4       labeling_0.4.3       fdrtool_1.2.18      \n#&gt; [127] ps_1.9.0             plyr_1.8.9           stringi_1.8.4       \n#&gt; [130] QuickJSR_1.6.0       munsell_0.5.1        lisrelToR_0.3       \n#&gt; [133] bayestestR_0.15.2    V8_6.0.2             pacman_0.5.1        \n#&gt; [136] Matrix_1.7-3         hms_1.1.3            glasso_1.11         \n#&gt; [139] future_1.34.0        shiny_1.10.0         rbibutils_2.3       \n#&gt; [142] igraph_2.1.4         broom_1.0.7          RcppParallel_5.1.10\n\n\n\n\n\nDebelak, R., Strobl, C., & Zeigenfuse, M. D. (2022). An introduction to the rasch model with examples in r. CRC Press.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>Stima</span>"
    ]
  },
  {
    "objectID": "chapters/irt/04_1pl_2pl_3pl.html",
    "href": "chapters/irt/04_1pl_2pl_3pl.html",
    "title": "70  Modelli 1PL, 2PL e 3PL",
    "section": "",
    "text": "70.1 Introduzione\nAll’interno della teoria della risposta agli item (IRT), il modello di Rasch rappresenta l’approccio più restrittivo, poiché impone vincoli stringenti sulle relazioni tra abilità delle persone e difficoltà degli item. Questi vincoli garantiscono semplicità e proprietà matematiche utili, ma limitano la flessibilità del modello nel rappresentare dati complessi.\nProgressivamente, tali restrizioni possono essere allentate per definire modelli più flessibili:\nQuesta progressione da Rasch a 1PL, 2PL e 3PL permette una maggiore adattabilità del modello IRT, bilanciando semplicità e flessibilità a seconda delle esigenze specifiche dei dati e dell’analisi.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>Modelli 1PL, 2PL e 3PL</span>"
    ]
  },
  {
    "objectID": "chapters/irt/04_1pl_2pl_3pl.html#introduzione",
    "href": "chapters/irt/04_1pl_2pl_3pl.html#introduzione",
    "title": "70  Modelli 1PL, 2PL e 3PL",
    "section": "",
    "text": "Il modello 1PL (One-Parameter Logistic), che conserva l’assunzione di uguale discriminazione tra gli item ma non richiede tutte le proprietà rigorose del modello di Rasch.\n\nIl modello 2PL (Two-Parameter Logistic), che introduce un parametro aggiuntivo per descrivere la capacità discriminante degli item, consentendo una rappresentazione più accurata delle risposte.\n\nIl modello 3PL (Three-Parameter Logistic), che aggiunge un terzo parametro per tenere conto della probabilità di risposta corretta casuale (detta anche “guessing”).",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>Modelli 1PL, 2PL e 3PL</span>"
    ]
  },
  {
    "objectID": "chapters/irt/04_1pl_2pl_3pl.html#un-esempio-pratico",
    "href": "chapters/irt/04_1pl_2pl_3pl.html#un-esempio-pratico",
    "title": "70  Modelli 1PL, 2PL e 3PL",
    "section": "\n70.2 Un Esempio Pratico",
    "text": "70.2 Un Esempio Pratico\nIn questo capitolo, utilizzeremo nuovamente i dati che abbiamo esaminato in precedenza nel Capitolo 69.\n\ndata(data.fims.Aus.Jpn.scored, package = \"TAM\")\nfims &lt;- data.fims.Aus.Jpn.scored\n\nIl data set include 400 partecipanti. Per facilitare la manipolazione dei dati, cambiamo il nome delle colonne.\n\nresponses &lt;- fims[1:400, 2:15]\ncolnames(responses) &lt;- gsub(\"M1PTI\", \"I\", colnames(responses))\nglimpse(responses)\n#&gt; Rows: 400\n#&gt; Columns: 14\n#&gt; $ I1  &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,…\n#&gt; $ I2  &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,…\n#&gt; $ I3  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,…\n#&gt; $ I6  &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,…\n#&gt; $ I7  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n#&gt; $ I11 &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,…\n#&gt; $ I12 &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n#&gt; $ I14 &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,…\n#&gt; $ I17 &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,…\n#&gt; $ I18 &lt;dbl&gt; 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,…\n#&gt; $ I19 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n#&gt; $ I21 &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,…\n#&gt; $ I22 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,…\n#&gt; $ I23 &lt;dbl&gt; 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,…\n\nDefiniamo il fattore gender:\n\ngender &lt;- as.factor(fims$SEX[1:400])\nlevels(gender) &lt;- c(\"male\", \"female\")\n\ngender |&gt; table()\n#&gt; gender\n#&gt;   male female \n#&gt;    246    154",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>Modelli 1PL, 2PL e 3PL</span>"
    ]
  },
  {
    "objectID": "chapters/irt/04_1pl_2pl_3pl.html#modello-1pl",
    "href": "chapters/irt/04_1pl_2pl_3pl.html#modello-1pl",
    "title": "70  Modelli 1PL, 2PL e 3PL",
    "section": "\n70.3 Modello 1PL",
    "text": "70.3 Modello 1PL\nIl modello ad un parametro logistico (1PL) descrive la probabilità che un rispondente con un certo livello di abilità dia una risposta corretta a un item specifico. La formula del modello è:\n\\[\nP(X_i = 1 \\mid \\theta_v, \\alpha, \\delta_i) = \\frac{\\exp(\\alpha(\\theta_v - \\delta_i))}{1 + \\exp(\\alpha(\\theta_v - \\delta_i))} = \\frac{1}{1 + \\exp(-\\alpha(\\theta_v - \\delta_i))}, \\tag{1}\n\\]\ndove:\n\n\n\\(\\theta_v\\) è il livello di abilità del rispondente \\(v\\),\n\n\\(\\delta_i\\) è il parametro di difficoltà dell’item \\(i\\),\n\n\\(\\alpha\\) è il parametro di discriminazione dell’item, fissato e uguale per tutti gli item nel modello 1PL.\n\nL’assunzione fondamentale del modello 1PL è che \\(\\alpha\\) sia costante per tutti gli item, indicando che tutti gli item hanno la stessa capacità di discriminazione tra rispondenti con abilità diverse.\n\n70.3.1 Il Ruolo del Parametro \\(\\alpha\\)\n\nIl parametro \\(\\alpha\\) definisce la pendenza della curva caratteristica dell’item (ICC). Maggiore è \\(\\alpha\\), più ripida è la curva ICC, e maggiore è la capacità dell’item di discriminare tra rispondenti con abilità vicine alla difficoltà dell’item (\\(\\delta_i\\)).\n\n\n\\(\\alpha = 0\\): L’item non discrimina affatto; la probabilità di risposta corretta è costante e indipendente dal livello di abilità.\n\n\\(\\alpha &gt; 0\\): L’item discrimina, e la sua capacità discriminatoria cresce con l’aumento di \\(\\alpha\\).\n\n70.3.2 Esempio Pratico\nConsideriamo tre item (\\(i_1\\), \\(i_2\\), \\(i_3\\)) con la stessa difficoltà \\(\\delta = 0\\) e tre valori di discriminazione: \\(\\alpha_1 = 0.0\\), \\(\\alpha_2 = 1.0\\) e \\(\\alpha_3 = 2.0\\). Esaminiamo due rispondenti con livelli di abilità:\n\nRispondente A con \\(\\theta_A = -1\\),\nRispondente B con \\(\\theta_B = 1\\).\n\nItem con $= 0.0\nCon \\(\\alpha = 0\\), la probabilità di risposta corretta è costante, pari a 0.5 per tutti i livelli di abilità:\n\\[\nP(X_i = 1 \\mid \\theta, \\delta) = 0.5.\n\\]\nQuesto item non discrimina tra rispondenti con abilità diverse e non aggiunge alcuna informazione utile.\nItem con \\(\\alpha\\) = 1.0\nCon \\(\\alpha = 1.0\\), la probabilità di risposta corretta dipende dal livello di abilità:\n\\[\nP(X_i = 1 \\mid \\theta, \\delta) = \\frac{1}{1 + \\exp(-(\\theta - \\delta))}.\n\\]\n\nPer \\(\\theta_A = -1\\): \\(P(X_i = 1) \\approx 0.269\\),\nPer \\(\\theta_B = 1\\): \\(P(X_i = 1) \\approx 0.731\\).\n\nLa curva ICC è moderatamente ripida e l’item discrimina tra rispondenti con abilità diverse.\nItem con \\(\\alpha\\) = 2.0\nCon \\(\\alpha = 2.0\\), l’ICC diventa più ripida:\n\\[\nP(X_i = 1 \\mid \\theta, \\delta) = \\frac{1}{1 + \\exp(-2(\\theta - \\delta))}.\n\\]\n\nPer \\(\\theta_A = -1\\): \\(P(X_i = 1) \\approx 0.119\\),\nPer \\(\\theta_B = 1\\): \\(P(X_i = 1) \\approx 0.881\\).\n\nLa maggiore ripidità riflette una capacità discriminatoria più alta, permettendo di distinguere con maggiore precisione i rispondenti in base alle loro abilità.\nIn conclusione, nel modello 1PL, il parametro \\(\\alpha\\) controlla la capacità degli item di discriminare tra rispondenti con abilità diverse. Un aumento di \\(\\alpha\\) rende la curva ICC più ripida, migliorando la discriminazione, mentre un \\(\\alpha\\) più basso rende la curva piatta e riduce la capacità informativa dell’item. Tuttavia, nel 1PL, l’assunzione che \\(\\alpha\\) sia costante per tutti gli item rappresenta un limite rispetto ai modelli più complessi, come il 2PL, che permettono a ogni item di avere una discriminazione diversa.\n\n70.3.3 Modello di Rasch e Modello 1PL: Confronto e Differenze\nIl modello di Rasch e il modello 1PL (One-Parameter Logistic) sono due approcci alla misurazione che condividono una struttura matematica simile. Entrambi utilizzano un parametro di discriminazione (α) costante per tutti gli item, pur permettendo variazioni nei parametri di difficoltà (δᵢ).\nLa differenza tecnica principale sta nel valore del parametro α:\n\nNel modello di Rasch, α è sempre fissato a 1.0\nNel modello 1PL, α può assumere qualsiasi valore costante, anche diverso da 1.0\n\nMatematicamente, i due modelli sono equivalenti: è possibile convertire i parametri da un modello all’altro attraverso una semplice riscalatura, moltiplicando o dividendo θᵥ e δᵢ per α, mantenendo invariate le probabilità di risposta corretta.\nNonostante la loro equivalenza matematica, i due modelli si distinguono per filosofia e obiettivi:\nIl modello 1PL si concentra sull’adattamento ai dati empirici:\n\nMira a descrivere al meglio i dati osservati\nOffre flessibilità nella scelta del parametro α\nSi adatta ai dati esistenti\n\nIl modello di Rasch privilegia la misurazione oggettiva:\n\nPone l’enfasi sulla costruzione di misure stabili e generalizzabili\nConsidera il modello come uno standard di riferimento\nRichiede che i dati si conformino al modello, non viceversa\nSi pone come strumento per sviluppare misurazioni valide e oggettive\n\nIn sintesi, mentre il modello 1PL è più orientato alla descrizione statistica dei dati, il modello di Rasch si propone come standard per la costruzione di strumenti di misurazione oggettivi e universalmente applicabili.\n\nEsercizio 70.1 In \\(\\mathsf{R}\\), il modello di Rasch si implementa nel modo seguente:\n\nmirt_rm &lt;- mirt(responses, 1, \"Rasch\", verbose = FALSE)\n\nIl modello 1PL si implementa nel modo seguente:\n\nmirt_1pl &lt;- mirt(responses, 1, \"1PL\", verbose = FALSE)\n\nConfrontiamo i due modelli:\n\nanova(mirt_rm, mirt_1pl)\n#&gt;           AIC SABIC   HQ  BIC logLik     X2 df   p\n#&gt; mirt_rm  5663  5675 5687 5723  -2816              \n#&gt; mirt_1pl 5662  5673 5684 5718  -2817 -0.705 -1 NaN\n\nI modelli mirt_rm e mirt_1pl sono praticamente equivalenti in termini di adattamento ai dati. Il modello mirt_1pl mostra lievi miglioramenti nei criteri di informazione (AIC e BIC), ma la differenza è minima.\n\n\n70.3.4 Modello 2PL\nIl modello 2PL (Modello IRT a due parametri) rappresenta un’estensione del modello 1PL che consente una maggiore flessibilità, poiché permette alle Curve Caratteristiche degli Item (ICC) di avere pendenze diverse. Questo significa che, a differenza del modello 1PL e del modello di Rasch, le ICC degli item non sono necessariamente parallele. Nel modello 2PL, ogni item è descritto da due parametri fondamentali:\n\n\nParametro di difficoltà (\\(b\\)): Indica il livello di abilità (\\(\\theta\\)) a cui la probabilità di risposta corretta è del 50%. Determina il posizionamento della curva ICC lungo l’asse delle abilità.\n\nParametro di discriminazione (\\(a\\)): Regola la pendenza della curva ICC, rappresentando la capacità dell’item di distinguere tra rispondenti con abilità simili. Un valore più alto di \\(a\\) indica una maggiore sensibilità dell’item alle variazioni di abilità.\n\nLa formula generale per le ICC nel modello 2PL è:\n\\[\nP(X_i = 1 \\mid \\theta, a_i, b_i) = \\frac{1}{1 + \\exp(-a_i (\\theta - b_i))},\n\\]\ndove:\n\n\n\\(\\theta\\) rappresenta l’abilità del rispondente,\n\n\\(a_i\\) è il parametro di discriminazione per l’item \\(i\\),\n\n\\(b_i\\) è il parametro di difficoltà per l’item \\(i\\).\n\n70.3.5 Implementazione in R con il Pacchetto mirt\n\nUtilizziamo il pacchetto mirt per adattare il modello 2PL ai dati. Il comando mirt() permette di stimare i parametri specificando il modello 2PL:\n\nmirt_2pl &lt;- mirt(responses, 1, \"2PL\")\n#&gt; \nIteration: 1, Log-Lik: -2818.855, Max-Change: 0.66304\nIteration: 2, Log-Lik: -2772.597, Max-Change: 0.26013\nIteration: 3, Log-Lik: -2762.517, Max-Change: 0.12884\nIteration: 4, Log-Lik: -2760.353, Max-Change: 0.06587\nIteration: 5, Log-Lik: -2759.824, Max-Change: 0.03961\nIteration: 6, Log-Lik: -2759.675, Max-Change: 0.02169\nIteration: 7, Log-Lik: -2759.622, Max-Change: 0.01040\nIteration: 8, Log-Lik: -2759.609, Max-Change: 0.00642\nIteration: 9, Log-Lik: -2759.605, Max-Change: 0.00396\nIteration: 10, Log-Lik: -2759.602, Max-Change: 0.00160\nIteration: 11, Log-Lik: -2759.602, Max-Change: 0.00089\nIteration: 12, Log-Lik: -2759.602, Max-Change: 0.00064\nIteration: 13, Log-Lik: -2759.601, Max-Change: 0.00020\nIteration: 14, Log-Lik: -2759.601, Max-Change: 0.00009\n\nPer analizzare graficamente le Curve Caratteristiche degli Item, usiamo la funzione plot():\n\nplot(mirt_2pl, type = \"trace\")\n\n\n\n\n\n\n\nSe desideriamo visualizzare tutte le ICC in un unico grafico, senza separarle per item, aggiungiamo l’opzione facet_items = FALSE:\n\nplot(mirt_2pl, type = \"trace\", facet_items = FALSE)\n\n\n\n\n\n\n\nLa funzione coef() consente di ottenere le stime dei parametri degli item:\n\ncoef(mirt_2pl, IRTpars = TRUE, simplify = TRUE)\n#&gt; $items\n#&gt;          a      b g u\n#&gt; I1   1.147 -1.022 0 1\n#&gt; I2   1.769 -0.911 0 1\n#&gt; I3   1.372 -1.680 0 1\n#&gt; I6   1.479 -0.043 0 1\n#&gt; I7   1.071  2.441 0 1\n#&gt; I11  1.594 -0.957 0 1\n#&gt; I12  0.703  1.079 0 1\n#&gt; I14  0.771 -0.612 0 1\n#&gt; I17  0.707  1.758 0 1\n#&gt; I18  0.750 -0.502 0 1\n#&gt; I19  1.831  1.459 0 1\n#&gt; I21 -0.214 -7.076 0 1\n#&gt; I22  0.277  7.657 0 1\n#&gt; I23  1.521 -1.503 0 1\n#&gt; \n#&gt; $means\n#&gt; F1 \n#&gt;  0 \n#&gt; \n#&gt; $cov\n#&gt;    F1\n#&gt; F1  1\n\nQueste stime includono:\n\n\n\\(a\\), parametro di discriminazione,\n\n\\(b\\), parametro di difficoltà.\n\n70.3.6 Confronto tra Modello 1PL e Modello 2PL\nPer valutare quale modello si adatta meglio ai dati, confrontiamo il modello 1PL (discriminazione fissa) con il modello 2PL (discriminazione variabile) utilizzando la funzione anova():\n\nanova(mirt_rm, mirt_2pl)\n#&gt;           AIC SABIC   HQ  BIC logLik     X2 df p\n#&gt; mirt_rm  5663  5675 5687 5723  -2816            \n#&gt; mirt_2pl 5575  5598 5619 5687  -2760 113.77 13 0\n\nInterpretazione dei Risultati\n\n\nCriteri di Informazione (AIC e BIC): Il modello 2PL tipicamente mostra valori di AIC e BIC più bassi rispetto al modello 1PL, indicando un miglior adattamento ai dati.\n\nLog-Likelihood: Il modello 2PL presenta un log-likelihood superiore rispetto al modello 1PL, a indicare una maggiore probabilità di osservare i dati sotto il modello 2PL.\n\nTest di \\(X^2\\): Se il p-value associato è significativo (\\(p &lt; 0.05\\)), ciò suggerisce che il modello 2PL spiega significativamente più variazione rispetto al modello 1PL.\n\n70.3.7 Differenze Chiave tra Modello 1PL e Modello 2PL\n\n\n\n\n\n\n\nCaratteristica\nModello 1PL\nModello 2PL\n\n\n\nParametro di discriminazione (\\(a\\))\nFisso per tutti gli item (\\(a = \\alpha\\) costante)\nVariabile tra gli item (\\(a_i\\) specifico)\n\n\nCurva ICC\nTutte le curve ICC sono parallele\nLe curve ICC possono avere pendenze diverse\n\n\nAdattamento ai dati\nMeno flessibile, buono per dati uniformi\nPiù flessibile, cattura differenze di discriminazione\n\n\n\nIn conclusione, il modello 2PL è particolarmente utile quando gli item differiscono nella loro capacità di discriminare tra rispondenti con abilità simili. Questo lo rende una scelta preferibile rispetto al modello 1PL in situazioni in cui gli item non sono omogenei in termini di discriminazione. Tuttavia, la maggiore flessibilità del modello 2PL comporta una maggiore complessità e richiede un dataset con sufficiente variabilità per stimare accuratamente i parametri \\(a_i\\) e \\(b_i\\).",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>Modelli 1PL, 2PL e 3PL</span>"
    ]
  },
  {
    "objectID": "chapters/irt/04_1pl_2pl_3pl.html#modello-3pl",
    "href": "chapters/irt/04_1pl_2pl_3pl.html#modello-3pl",
    "title": "70  Modelli 1PL, 2PL e 3PL",
    "section": "\n70.4 Modello 3PL",
    "text": "70.4 Modello 3PL\nIl modello IRT a tre parametri (3PL) è un’estensione del modello 2PL che aggiunge un terzo parametro, il guessing (\\(g\\)), per tenere conto della probabilità di rispondere correttamente a un item semplicemente per caso. Questo parametro è particolarmente utile nei test a scelta multipla, dove i rispondenti con abilità molto bassa possono comunque selezionare la risposta corretta in modo casuale.\nLa probabilità di risposta corretta nel modello 3PL è espressa come:\n\\[\nP(X_i = 1 \\mid \\theta, a_i, b_i, g_i) = g_i + (1 - g_i) \\cdot \\frac{1}{1 + \\exp(-a_i (\\theta - b_i))},\n\\]\ndove:\n\n\n\\(\\theta\\): abilità latente del rispondente,\n\n\\(a_i\\): parametro di discriminazione dell’item \\(i\\) (controlla la pendenza della curva ICC),\n\n\\(b_i\\): parametro di difficoltà dell’item \\(i\\) (indica il livello di abilità richiesto per una probabilità del 50% di risposta corretta, escludendo il guessing),\n\n\\(g_i\\): parametro di guessing (probabilità minima di rispondere correttamente a un item, anche per rispondenti con abilità molto bassa).\n\n\n70.4.1 Caratteristiche del Modello 3PL\n\n\nParametro di Guessing (\\(g\\)):\n\nIntroduce un asintoto inferiore maggiore di zero nella curva caratteristica dell’item (ICC).\nAd esempio, un valore \\(g_i = 0.25\\) indica che, anche per abilità molto basse (\\(\\theta \\to -\\infty\\)), la probabilità di rispondere correttamente all’item è almeno del 25%. Questo valore è tipico per test a scelta multipla con quattro opzioni, dove c’è il 25% di probabilità di indovinare.\n\n\n\nRelazione con il Modello 2PL:\n\nIl modello 3PL generalizza il modello 2PL aggiungendo il parametro \\(g\\), che aumenta la flessibilità per rappresentare meglio il comportamento degli item in situazioni reali.\nMentre nel modello 2PL la probabilità di risposta corretta può scendere a zero per abilità molto basse, nel modello 3PL la probabilità minima è definita da \\(g\\).\n\n\n\nCurve Caratteristiche degli Item (ICC):\n\nLa presenza del parametro \\(g\\) modifica la forma della curva ICC, che non tocca mai lo zero ma si avvicina asintoticamente al valore di \\(g\\) per \\(\\theta\\) molto basso.\n\n\n\nComplessità del Modello:\n\nL’aggiunta del parametro \\(g\\) rende il modello più complesso rispetto al 2PL, aumentando il numero di parametri da stimare.\nPer ottenere stime affidabili, è necessario disporre di un dataset con un numero sufficiente di item e rispondenti.\n\n\n\n\nEsercizio 70.2 Utilizziamo il pacchetto mirt per stimare i parametri del modello 3PL:\n\nmirt_3pl &lt;- mirt(responses, 1, \"3PL\")\n\nLe curve ICC possono essere visualizzate con il comando:\n\nplot(mirt_3pl, type = \"trace\", facet_items = TRUE)\n\n\n\n\n\n\n\nUtilizziamo la funzione coef() per ottenere le stime dei parametri degli item (\\(a\\), \\(b\\), \\(g\\)):\n\ncoef(mirt_3pl, IRTpars = TRUE, simplify = TRUE)\n#&gt; $items\n#&gt;          a      b     g u\n#&gt; I1   1.410 -0.493 0.244 1\n#&gt; I2   2.667 -0.404 0.291 1\n#&gt; I3   1.445 -1.633 0.000 1\n#&gt; I6   1.514  0.009 0.019 1\n#&gt; I7   1.163  2.297 0.000 1\n#&gt; I11  1.487 -0.984 0.000 1\n#&gt; I12  2.131  1.275 0.207 1\n#&gt; I14  0.805 -0.586 0.000 1\n#&gt; I17  3.076  1.320 0.138 1\n#&gt; I18  0.724 -0.510 0.000 1\n#&gt; I19  1.844  1.449 0.000 1\n#&gt; I21 -5.264 -2.271 0.167 1\n#&gt; I22  9.179  1.990 0.090 1\n#&gt; I23  1.625 -1.452 0.000 1\n#&gt; \n#&gt; $means\n#&gt; F1 \n#&gt;  0 \n#&gt; \n#&gt; $cov\n#&gt;    F1\n#&gt; F1  1\n\nConfronto tra modelli 2PL e 3PL\n\nanova(mirt_2pl, mirt_3pl)\n#&gt;           AIC SABIC   HQ  BIC logLik     X2 df p\n#&gt; mirt_2pl 5575  5598 5619 5687  -2760            \n#&gt; mirt_3pl 5564  5599 5631 5732  -2740 38.784 14 0\n\n\nIl modello 3PL presenta un AIC inferiore rispetto al modello 2PL, suggerendo un miglior adattamento ai dati.\nTuttavia, il BIC penalizza maggiormente la complessità del modello, favorendo leggermente il modello 2PL.\nLa significatività del test \\(X^2\\) (\\(p = 0.0004\\)) indica che il modello 3PL offre un miglioramento significativo rispetto al modello 2PL.\n\nValutazione della bontà dell’adattamento\nPer verificare se il modello 3PL rappresenta adeguatamente i dati, utilizziamo la statistica \\(M2\\):\n\nM2(mirt_3pl)\n#&gt;          M2 df     p  RMSEA RMSEA_5 RMSEA_95   SRMSR    TLI    CFI\n#&gt; stats 76.06 63 0.125 0.0228       0  0.03944 0.04533 0.9751 0.9827\n\n\nIl valore \\(p = 0.125\\) indica che il modello 3PL non può essere rifiutato come rappresentazione adeguata dei dati.\nIl RMSEA inferiore a 0.05 (limite superiore: 0.039) suggerisce un buon adattamento.\n\nAdattamento degli item\nIl comando itemfit() calcola le statistiche di adattamento (fit) per ciascun item del modello 3PL, fornendo i valori di infit e outfit insieme ai relativi z-score che indicano quanto questi valori si discostano da quelli attesi secondo una distribuzione normale standardizzata.\n\nL’infit (Information-weighted fit) si concentra principalmente sui rispondenti con un livello di abilità simile alla difficoltà dell’item, ed è quindi particolarmente sensibile alle discrepanze nella “zona di interesse” dell’item, dove la probabilità di risposta corretta si aggira intorno al 50%.\nL’outfit (Outlier-sensitive fit) invece considera tutti i rispondenti, inclusi quelli con abilità molto diverse dalla difficoltà dell’item, risultando più sensibile a risposte inaspettate o estreme.\n\nPer entrambe le statistiche, valori compresi tra 0.7 e 1.3 indicano un buon adattamento dell’item al modello. Valori inferiori a 0.7 suggeriscono che l’item è troppo prevedibile o ridondante, mentre valori superiori a 1.3 indicano la presenza di risposte inaspettate. Per quanto riguarda gli z-score, valori con modulo inferiore a 2 sono considerati accettabili, mentre valori superiori potrebbero indicare problemi di adattamento.\n\nitemfit(mirt_3pl, \"infit\", method = \"ML\") # infit and outfit stats\n#&gt;    item outfit z.outfit infit z.infit\n#&gt; 1    I1  1.005    0.084 0.983  -0.273\n#&gt; 2    I2  0.865   -0.367 0.873  -2.016\n#&gt; 3    I3  0.876   -0.404 0.909  -0.940\n#&gt; 4    I6  0.981   -0.145 0.916  -1.503\n#&gt; 5    I7  0.783   -0.797 0.896  -0.899\n#&gt; 6   I11  1.462    2.383 0.886  -1.567\n#&gt; 7   I12  0.945   -0.929 0.942  -1.157\n#&gt; 8   I14  0.967   -0.565 1.009   0.223\n#&gt; 9   I17  0.784   -2.457 0.807  -2.709\n#&gt; 10  I18  1.025    0.539 1.013   0.349\n#&gt; 11  I19  0.590   -1.393 0.816  -2.165\n#&gt; 12  I21  0.915   -0.983 0.917  -0.963\n#&gt; 13  I22  0.860   -0.978 0.844  -1.160\n#&gt; 14  I23  0.778   -0.736 0.872  -1.370\n\nDall’analisi dei risultati emerge che la maggior parte degli item mostra un buon adattamento al modello. In particolare, l’item I1 presenta valori ottimali sia per outfit (1.005) che per infit (0.983), con z-score molto contenuti (0.084 e -0.273 rispettivamente). Anche gli item I3, I6, I14, I18 e I21 mostrano valori di adattamento soddisfacenti, rientrando negli intervalli di accettabilità sia per le statistiche di fit che per gli z-score.\nTuttavia, alcuni item presentano aspetti critici che meritano attenzione. L’item I11 mostra un outfit elevato (1.462) con uno z-score significativo (2.383), suggerendo la presenza di risposte anomale da parte di soggetti con livelli di abilità distanti dalla difficoltà dell’item. L’item I17, pur avendo valori di fit accettabili (outfit = 0.784, infit = 0.807), presenta uno z-score problematico per l’infit (-2.709), indicando possibili discrepanze significative per i rispondenti con abilità vicine alla difficoltà dell’item.\nUn caso particolare è rappresentato dall’item I19, che mostra un outfit inferiore alla soglia minima (0.590) e uno z-score dell’infit significativo (-2.165). Questi valori potrebbero indicare che l’item è troppo prevedibile o eccessivamente facile rispetto al livello atteso dal modello.\nNel complesso, sebbene la maggior parte degli item mostri un adattamento soddisfacente, potrebbe essere opportuno rivedere gli item I11, I17 e I19 per migliorare la qualità complessiva dello strumento di misura.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>Modelli 1PL, 2PL e 3PL</span>"
    ]
  },
  {
    "objectID": "chapters/irt/04_1pl_2pl_3pl.html#invarianza-di-gruppo-nella-item-response-theory",
    "href": "chapters/irt/04_1pl_2pl_3pl.html#invarianza-di-gruppo-nella-item-response-theory",
    "title": "70  Modelli 1PL, 2PL e 3PL",
    "section": "\n70.5 Invarianza di Gruppo nella Item Response Theory",
    "text": "70.5 Invarianza di Gruppo nella Item Response Theory\nL’invarianza di gruppo dei parametri degli item rappresenta una delle caratteristiche più importanti della IRT. Questo principio afferma che le proprietà misurate di un item - come la sua difficoltà, discriminazione e probabilità di indovinare la risposta corretta - sono caratteristiche intrinseche dell’item stesso e rimangono stabili indipendentemente dalla popolazione di riferimento.\nPer comprendere meglio questo concetto, consideriamo un esempio concreto. Immaginiamo di somministrare lo stesso test a due gruppi distinti di esaminandi:\n\nil primo gruppo è composto da individui con abilità relativamente bassa, con punteggi che variano tra -3 e -1 sulla scala di abilità (con una media di -2);\nil secondo gruppo invece include individui con abilità più elevata, con punteggi tra +1 e +3 (media +2).\n\nQuando analizziamo le risposte utilizzando il metodo della massima verosimiglianza, osserviamo un fenomeno notevole: per ogni item, otteniamo gli stessi parametri indipendentemente dal gruppo analizzato. Per esempio, se per un determinato item otteniamo un parametro di discriminazione a = 1.27 e un parametro di difficoltà b = 0.39 analizzando l’intero campione, ritroveremo sostanzialmente gli stessi valori anche analizzando separatamente il gruppo con abilità bassa o quello con abilità alta.\nQuesto risultato ha implicazioni pratiche molto importanti. Significa che:\n\nle caratteristiche dell’item rimangono stabili anche quando il test viene somministrato a popolazioni diverse;\npossiamo confrontare in modo valido le prestazioni di gruppi diversi sullo stesso item;\nle stime dei parametri dell’item sono robuste e generalizzabili;\nla calibrazione degli item può essere effettuata su un campione e poi applicata con fiducia a popolazioni diverse.\n\nL’invarianza di gruppo rappresenta quindi una proprietà fondamentale che distingue i modelli IRT dai modelli classici della teoria dei test, permettendo confronti più equi e interpretazioni più affidabili dei risultati dei test tra diverse popolazioni.\nQuesta proprietà è particolarmente utile in contesti pratici, come quando si devono confrontare gruppi culturali diversi, classi scolastiche di diverso livello, o quando si vuole verificare se un test funziona allo stesso modo per popolazioni diverse. L’invarianza garantisce che le differenze osservate riflettano reali differenze nelle abilità misurate, piuttosto che artefatti dovuti alle caratteristiche del campione utilizzato per la calibrazione.\n\nEsercizio 70.3 Questo esercizio utilizza una simulazione in R per dimostrare visivamente il principio di invarianza dei parametri degli item nella IRT. La funzione groupinv() simula le risposte di due gruppi distinti di esaminandi e visualizza le loro curve caratteristiche dell’item (ICC).\n\ngroupinv &lt;- function(mdl, t1l, t1u, t2l, t2u) {\n    if (missing(t1l)) t1l &lt;- -3\n    if (missing(t1u)) t1u &lt;- -1\n    if (missing(t2l)) t2l &lt;- 1\n    if (missing(t2u)) t2u &lt;- 3\n    theta &lt;- seq(-3, 3, .1875)\n    f &lt;- rep(21, length(theta))\n    wb &lt;- round(runif(1, -3, 3), 2)\n    wa &lt;- round(runif(1, 0.2, 2.8), 2)\n    wc &lt;- round(runif(1, 0, .35), 2)\n    if (mdl == 1 | mdl == 2) {\n        wc &lt;- 0\n    }\n    if (mdl == 1) {\n        wa &lt;- 1\n    }\n    for (g in 1:length(theta)) {\n        P &lt;- wc + (1 - wc) / (1 + exp(-wa * (theta - wb)))\n    }\n    p &lt;- rbinom(length(theta), f, P) / f\n    lowerg1 &lt;- 0\n    for (g in 1:length(theta)) {\n        if (theta[g] &lt;= t1l) {\n            lowerg1 &lt;- lowerg1 + 1\n        }\n    }\n    upperg1 &lt;- 0\n    for (g in 1:length(theta)) {\n        if (theta[g] &lt;= t1u) {\n            upperg1 &lt;- upperg1 + 1\n        }\n    }\n    theta1 &lt;- theta[lowerg1:upperg1]\n    p1 &lt;- p[lowerg1:upperg1]\n    lowerg2 &lt;- 0\n    for (g in 1:length(theta)) {\n        if (theta[g] &lt;= t2l) {\n            lowerg2 &lt;- lowerg2 + 1\n        }\n    }\n    upperg2 &lt;- 0\n    for (g in 1:length(theta)) {\n        if (theta[g] &lt;= t2u) {\n            upperg2 &lt;- upperg2 + 1\n        }\n    }\n    theta2 &lt;- theta[lowerg2:upperg2]\n    p2 &lt;- p[lowerg2:upperg2]\n    theta12 &lt;- c(theta1, theta2)\n    p12 &lt;- c(p1, p2)\n    par(lab = c(7, 5, 3))\n    plot(theta12, p12,\n        xlim = c(-3, 3), ylim = c(0, 1),\n        xlab = \"Ability\", ylab = \"Probability of Correct Response\"\n    )\n    if (mdl == 1) {\n        maintext &lt;- paste(\"Pooled Groups\", \"\\n\", \"b=\", wb)\n    }\n    if (mdl == 2) {\n        maintext &lt;- paste(\"Pooled Groups\", \"\\n\", \"a=\", wa, \"b=\", wb)\n    }\n    if (mdl == 3) {\n        maintext &lt;- paste(\n            \"Pooled Groups\", \"\\n\",\n            \"a=\", wa, \"b=\", wb, \"c=\", wc\n        )\n    }\n    par(new = \"T\")\n    plot(theta, P,\n        xlim = c(-3, 3), ylim = c(0, 1), type = \"l\",\n        xlab = \"\", ylab = \"\", main = maintext\n    )\n}\n\nset.seed(1)\ngroupinv(1, -3, -1, 1, 3)\n\n\n\n\n\n\n\nNel grafico risultante, osserviamo due segmenti di punti che rappresentano le risposte dei due gruppi. La linea continua mostra la curva ICC stimata utilizzando i dati di entrambi i gruppi. Il fatto che questa curva si adatti bene ai punti di entrambi i gruppi, nonostante la loro diversa distribuzione di abilità, dimostra visivamente il principio di invarianza: i parametri dell’item rimangono stabili indipendentemente dal gruppo considerato.\nQuesta visualizzazione è particolarmente efficace perché:\n\nmostra chiaramente la separazione tra i due gruppi di abilità;\npermette di verificare che la stessa curva ICC si adatta bene a entrambi i gruppi;\nconferma che le stime dei parametri (riportate nel titolo del grafico) sono valide per l’intero range di abilità.\n\nL’esercizio fornisce quindi una dimostrazione empirica dell’invarianza di gruppo, una delle proprietà fondamentali che rendono la IRT uno strumento robusto per la misurazione psicometrica.\n\n\n70.5.1 Confronto con la Teoria Classica dei Test\nLa differenza più notevole tra IRT e CTT riguarda proprio il modo in cui viene trattata l’invarianza dei parametri degli item rispetto ai gruppi esaminati. Questo aspetto emerge chiaramente analizzando come le due teorie definiscono e misurano la difficoltà degli item.\nNella CTT, la difficoltà di un item è definita come la proporzione di risposte corrette nel campione. Questa definizione rende il parametro di difficoltà intrinsecamente dipendente dalla popolazione esaminata: lo stesso item mostrerà una “difficoltà” diversa se somministrato a gruppi con differenti livelli di abilità. Per esempio, se somministriamo un test a due classi di livello diverso, nella CTT otterremo due stime di difficoltà diverse per lo stesso item, rendendo problematico qualsiasi confronto diretto tra i gruppi.\nL’IRT risolve questa limitazione fondamentale introducendo parametri che sono teoricamente invarianti rispetto alla popolazione. Il parametro di difficoltà (\\(\\beta\\)) in particolare rappresenta una proprietà intrinseca dell’item che rimane costante indipendentemente dal gruppo esaminato. Questo significa che, a differenza della CTT, l’IRT può fornire stime comparabili della difficoltà dell’item anche quando viene somministrato a popolazioni con distribuzioni di abilità molto diverse.\nL’invarianza nella IRT si manifesta nella curva caratteristica dell’item (ICC): la relazione tra abilità e probabilità di risposta corretta mantiene la stessa forma matematica indipendentemente dal gruppo considerato (come abbiamo osservato nell’esempio precedente). Questa proprietà ha importanti implicazioni pratiche:\n\npossiamo stimare i parametri dell’item utilizzando qualsiasi sottogruppo della popolazione e ottenere risultati coerenti;\nè possibile confrontare direttamente le prestazioni di gruppi diversi sullo stesso item;\nla calibrazione degli item può essere effettuata su un campione e poi applicata con fiducia ad altri gruppi.\n\nTuttavia, è importante notare che mentre nella CTT la dipendenza dalla popolazione è una limitazione intrinseca del modello, nell’IRT l’invarianza è una proprietà teorica che nella pratica può essere influenzata da vari fattori. Le stime empiriche dei parametri possono mostrare alcune variazioni dovute all’errore campionario, e l’invarianza è garantita solo quando l’item misura effettivamente lo stesso costrutto in tutti i gruppi considerati.\nQuesta differenza fondamentale tra CTT e IRT nell’approccio all’invarianza di gruppo rende l’IRT particolarmente adatta per applicazioni che richiedono confronti affidabili tra popolazioni diverse, come nel testing adattivo, negli studi longitudinali e nelle comparazioni tra gruppi culturali diversi.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>Modelli 1PL, 2PL e 3PL</span>"
    ]
  },
  {
    "objectID": "chapters/irt/04_1pl_2pl_3pl.html#riflessioni-conclusive",
    "href": "chapters/irt/04_1pl_2pl_3pl.html#riflessioni-conclusive",
    "title": "70  Modelli 1PL, 2PL e 3PL",
    "section": "\n70.6 Riflessioni Conclusive",
    "text": "70.6 Riflessioni Conclusive\nIn questo capitolo, abbiamo esplorato la progressione dei modelli IRT, partendo dal modello di Rasch fino al più flessibile modello 3PL. Ogni modello offre un equilibrio unico tra semplicità e adattabilità, consentendo di rispondere a esigenze diverse nell’ambito della misurazione psicometrica.\n\nIl modello di Rasch, con i suoi vincoli rigorosi, si distingue per la sua capacità di fornire misurazioni oggettive e stabili, risultando particolarmente utile nella costruzione di strumenti di misurazione.\nIl modello 1PL mantiene la semplicità del modello di Rasch ma introduce una maggiore flessibilità consentendo di variare il parametro di discriminazione a livello teorico.\nIl modello 2PL aggiunge un ulteriore livello di complessità, permettendo a ogni item di avere una discriminazione specifica, migliorando l’adattamento ai dati reali.\nIl modello 3PL completa questa progressione introducendo il parametro di guessing, necessario per tenere conto delle risposte corrette casuali, tipiche nei test a scelta multipla.\n\nQuesta evoluzione riflette l’importanza di adattare il modello alle caratteristiche dei dati e alle finalità dell’analisi. Abbiamo anche sottolineato l’importanza dell’invarianza di gruppo, una proprietà chiave che consente confronti equi tra popolazioni diverse, distinguendo l’IRT dalla Teoria Classica dei Test (CTT).\nIn definitiva, la scelta del modello dipende dall’obiettivo specifico dello studio e dalla complessità dei dati osservati. Il modello di Rasch offre rigore e semplicità, mentre i modelli 2PL e 3PL offrono flessibilità e precisione. Questa progressione dimostra la versatilità della IRT come framework per la misurazione psicometrica, supportando applicazioni che spaziano dalla ricerca accademica allo sviluppo di test standardizzati.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>Modelli 1PL, 2PL e 3PL</span>"
    ]
  },
  {
    "objectID": "chapters/irt/04_1pl_2pl_3pl.html#session-info",
    "href": "chapters/irt/04_1pl_2pl_3pl.html#session-info",
    "title": "70  Modelli 1PL, 2PL e 3PL",
    "section": "\n70.7 Session Info",
    "text": "70.7 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.2\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] grid      stats4    stats     graphics  grDevices utils     datasets \n#&gt; [8] methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] latex2exp_0.9.6   psychotools_0.7-4 ggmirt_0.1.0      TAM_4.2-21       \n#&gt;  [5] CDM_8.2-6         mvtnorm_1.3-3     mirt_1.44.0       lattice_0.22-6   \n#&gt;  [9] eRm_1.0-6         ggokabeito_0.1.0  see_0.11.0        MASS_7.3-65      \n#&gt; [13] viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1   \n#&gt; [17] gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6   \n#&gt; [21] semPlot_1.1.6     lavaan_0.6-19     psych_2.4.12      scales_1.3.0     \n#&gt; [25] markdown_1.13     knitr_1.50        lubridate_1.9.4   forcats_1.0.0    \n#&gt; [29] stringr_1.5.1     dplyr_1.1.4       purrr_1.0.4       readr_2.1.5      \n#&gt; [33] tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0  \n#&gt; [37] here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] splines_4.4.2        later_1.4.1          R.oo_1.27.0         \n#&gt;   [4] XML_3.99-0.18        rpart_4.1.24         lifecycle_1.0.4     \n#&gt;   [7] Rdpack_2.6.3         rstatix_0.7.2        rprojroot_2.0.4     \n#&gt;  [10] globals_0.16.3       rockchalk_1.8.157    backports_1.5.0     \n#&gt;  [13] magrittr_2.0.3       openxlsx_4.2.8       Hmisc_5.2-3         \n#&gt;  [16] rmarkdown_2.29       yaml_2.3.10          httpuv_1.6.15       \n#&gt;  [19] qgraph_1.9.8         zip_2.3.2            sessioninfo_1.2.3   \n#&gt;  [22] pbapply_1.7-2        minqa_1.2.8          multcomp_1.4-28     \n#&gt;  [25] abind_1.4-8          audio_0.1-11         quadprog_1.5-8      \n#&gt;  [28] R.utils_2.13.0       nnet_7.3-20          TH.data_1.1-3       \n#&gt;  [31] sandwich_3.1-1       listenv_0.9.1        testthat_3.2.3      \n#&gt;  [34] vegan_2.6-10         arm_1.14-4           parallelly_1.42.0   \n#&gt;  [37] permute_0.9-7        codetools_0.2-20     tidyselect_1.2.1    \n#&gt;  [40] farver_2.1.2         lme4_1.1-36          base64enc_0.1-3     \n#&gt;  [43] jsonlite_1.9.1       polycor_0.8-1        progressr_0.15.1    \n#&gt;  [46] Formula_1.2-5        survival_3.8-3       emmeans_1.10.7      \n#&gt;  [49] tools_4.4.2          Rcpp_1.0.14          glue_1.8.0          \n#&gt;  [52] mnormt_2.1.1         admisc_0.37          xfun_0.51           \n#&gt;  [55] mgcv_1.9-1           withr_3.0.2          beepr_2.0           \n#&gt;  [58] fastmap_1.2.0        boot_1.3-31          digest_0.6.37       \n#&gt;  [61] mi_1.1               timechange_0.3.0     R6_2.6.1            \n#&gt;  [64] mime_0.13            estimability_1.5.1   colorspace_2.1-1    \n#&gt;  [67] gtools_3.9.5         jpeg_0.1-10          R.methodsS3_1.8.2   \n#&gt;  [70] generics_0.1.3       data.table_1.17.0    corpcor_1.6.10      \n#&gt;  [73] SimDesign_2.19.1     htmlwidgets_1.6.4    pkgconfig_2.0.3     \n#&gt;  [76] sem_3.1-16           gtable_0.3.6         brio_1.1.5          \n#&gt;  [79] htmltools_0.5.8.1    carData_3.0-5        png_0.1-8           \n#&gt;  [82] reformulas_0.4.0     rstudioapi_0.17.1    tzdb_0.5.0          \n#&gt;  [85] reshape2_1.4.4       coda_0.19-4.1        checkmate_2.3.2     \n#&gt;  [88] nlme_3.1-167         nloptr_2.2.1         zoo_1.8-13          \n#&gt;  [91] parallel_4.4.2       miniUI_0.1.1.1       foreign_0.8-88      \n#&gt;  [94] pillar_1.10.1        vctrs_0.6.5          promises_1.3.2      \n#&gt;  [97] car_3.1-3            OpenMx_2.21.13       xtable_1.8-4        \n#&gt; [100] Deriv_4.1.6          cluster_2.1.8.1      dcurver_0.9.2       \n#&gt; [103] GPArotation_2024.3-1 htmlTable_2.4.3      evaluate_1.0.3      \n#&gt; [106] pbivnorm_0.6.0       cli_3.6.4            kutils_1.73         \n#&gt; [109] compiler_4.4.2       rlang_1.1.5          future.apply_1.11.3 \n#&gt; [112] ggsignif_0.6.4       fdrtool_1.2.18       plyr_1.8.9          \n#&gt; [115] stringi_1.8.4        munsell_0.5.1        lisrelToR_0.3       \n#&gt; [118] pacman_0.5.1         Matrix_1.7-3         hms_1.1.3           \n#&gt; [121] glasso_1.11          future_1.34.0        shiny_1.10.0        \n#&gt; [124] rbibutils_2.3        igraph_2.1.4         broom_1.0.7         \n#&gt; [127] RcppParallel_5.1.10\n\n\n\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>Modelli 1PL, 2PL e 3PL</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_implementation.html",
    "href": "chapters/irt/05_implementation.html",
    "title": "71  Implementazione",
    "section": "",
    "text": "71.1 Introduzione\nIn questo capitolo esamineremo il tutorial di Debelak et al. (2022).",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_implementation.html#un-esempio-pratico",
    "href": "chapters/irt/05_implementation.html#un-esempio-pratico",
    "title": "71  Implementazione",
    "section": "\n71.2 Un esempio pratico",
    "text": "71.2 Un esempio pratico\nIl set di dati data.fims.Aus.Jpn.scored contiene le risposte valutate per un sottoinsieme di item da parte di studenti australiani e giapponesi nello studio “First International Mathematics Study” (FIMS, Husén, 1967).\n\ndata(data.fims.Aus.Jpn.scored, package = \"TAM\")\nfims &lt;- data.fims.Aus.Jpn.scored\n\n\nglimpse(fims)\n#&gt; Rows: 6,371\n#&gt; Columns: 16\n#&gt; $ SEX     &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n#&gt; $ M1PTI1  &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1…\n#&gt; $ M1PTI2  &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1…\n#&gt; $ M1PTI3  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1…\n#&gt; $ M1PTI6  &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0…\n#&gt; $ M1PTI7  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#&gt; $ M1PTI11 &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1…\n#&gt; $ M1PTI12 &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#&gt; $ M1PTI14 &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1…\n#&gt; $ M1PTI17 &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0…\n#&gt; $ M1PTI18 &lt;dbl&gt; 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1…\n#&gt; $ M1PTI19 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#&gt; $ M1PTI21 &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0…\n#&gt; $ M1PTI22 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#&gt; $ M1PTI23 &lt;dbl&gt; 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1…\n#&gt; $ country &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n\nOltre alle risposte sui 14 item di matematica, il data set contiene anche informazioni sul genere del partecipate e sul paese d’origine.\n\nfims$SEX &lt;- as.factor(fims$SEX)\nlevels(fims$SEX) &lt;- c(\"male\", \"female\")\nfims$country &lt;- as.factor(fims$country)\nlevels(fims$country) &lt;- c(\"Australia\", \"Japan\")\n\n\nsummary(fims[, c(\"SEX\", \"country\")])\n#&gt;      SEX            country    \n#&gt;  male  :3319   Australia:4320  \n#&gt;  female:3052   Japan    :2051\n\nEsaminiamo le risposte dei primi 400 partecipanti. Con le seguenti istruzioni, per facilitare la manipolazione dei dati, cambiamo il nome delle colonne.\n\nresponses &lt;- fims[1:400, 2:15]\ncolnames(responses) &lt;- gsub(\"M1PTI\", \"I\", colnames(responses))\nglimpse(responses)\n#&gt; Rows: 400\n#&gt; Columns: 14\n#&gt; $ I1  &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,…\n#&gt; $ I2  &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,…\n#&gt; $ I3  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,…\n#&gt; $ I6  &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,…\n#&gt; $ I7  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n#&gt; $ I11 &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,…\n#&gt; $ I12 &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n#&gt; $ I14 &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,…\n#&gt; $ I17 &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,…\n#&gt; $ I18 &lt;dbl&gt; 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,…\n#&gt; $ I19 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n#&gt; $ I21 &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,…\n#&gt; $ I22 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,…\n#&gt; $ I23 &lt;dbl&gt; 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,…\n\n\ngender &lt;- as.factor(fims$SEX[1:400])\nlevels(gender) &lt;- c(\"male\", \"female\")",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_implementation.html#modello-di-rasch",
    "href": "chapters/irt/05_implementation.html#modello-di-rasch",
    "title": "71  Implementazione",
    "section": "\n71.3 Modello di Rasch",
    "text": "71.3 Modello di Rasch\nUn’analisi IRT può essere paragonata a un’analisi fattoriale. Dopo avere adattato il modello di Rasch ai dati usando mirt(), possiamo usare la funzione summary() per ottenere quella che viene definita “soluzione fattoriale”, che include i carichi fattoriali (F1) e le comunalità (h2). Le comunalità, essendo carichi fattoriali al quadrato, sono interpretate come la varianza spiegata in un item dal tratto latente. Nel caso presente, tutti gli item hanno una relazione sostanziale (saturazioni \\(\\approx\\) .50) con il tratto latente, indicando che il tratto latente è un buon indicatore della varianza osservata in quegli item. Questo suggerisce che il tratto latente è in grado di spiegare una porzione almento moderata della varianza nei punteggi degli item.\n\nmirt_rm &lt;- mirt(responses, 1, itemtype = \"Rasch\", verbose = FALSE)\nsummary(mirt_rm)\n#&gt;     F1    h2\n#&gt; I1     0.238\n#&gt; I2     0.238\n#&gt; I3     0.238\n#&gt; I6     0.238\n#&gt; I7     0.238\n#&gt; I11    0.238\n#&gt; I12    0.238\n#&gt; I14    0.238\n#&gt; I17    0.238\n#&gt; I18    0.238\n#&gt; I19    0.238\n#&gt; I21    0.238\n#&gt; I22    0.238\n#&gt; I23    0.238\n#&gt; \n#&gt; SS loadings:  0 \n#&gt; Proportion Var:  0 \n#&gt; \n#&gt; Factor correlations: \n#&gt; \n#&gt;    F1\n#&gt; F1  1\n\nNell’IRT, tuttavia, siamo generalmente più interessati ai parametri di discriminazione e difficoltà. Questi parametri possono essere estratti dall’oggetto creato da mirt() nel seguente modo:\n\nparams_rm &lt;- coef(mirt_rm, IRTpars = TRUE, simplify = TRUE)\nround(params_rm$items, 2) # g = c = guessing parameter\n#&gt;     a     b g u\n#&gt; I1  1 -1.10 0 1\n#&gt; I2  1 -1.25 0 1\n#&gt; I3  1 -2.04 0 1\n#&gt; I6  1 -0.05 0 1\n#&gt; I7  1  2.53 0 1\n#&gt; I11 1 -1.25 0 1\n#&gt; I12 1  0.81 0 1\n#&gt; I14 1 -0.50 0 1\n#&gt; I17 1  1.33 0 1\n#&gt; I18 1 -0.40 0 1\n#&gt; I19 1  2.06 0 1\n#&gt; I21 1  1.75 0 1\n#&gt; I22 1  2.41 0 1\n#&gt; I23 1 -1.93 0 1\n\n\n\n\\(a\\) (Discriminazione): Il parametro \\(a\\) (discriminazione) rappresenta la pendenza delle curve caratteristiche degli item (ICC - Item Characteristic Curves). Una pendenza elevata (valore alto di \\(a\\)) indica che l’item è molto efficace nel distinguere tra individui con livelli diversi del tratto latente (ad esempio, abilità). Questo significa che piccole variazioni nel tratto latente portano a grandi cambiamenti nella probabilità di rispondere correttamente all’item. Una pendenza bassa (valore basso di \\(a\\)) suggerisce che l’item non è altrettanto efficace nel discriminare tra livelli diversi del tratto latente. In questo caso, anche ampie variazioni nel tratto latente comportano solo piccoli cambiamenti nella probabilità di risposta corretta. Nel modello di Rasch si assume che tutti gli item abbiano la stessa pendenza (o potere discriminante), e quindi tutti i valori di \\(a\\) sono fissati allo stesso valore (ovvero 1).\n\n\\(b\\) (Difficoltà): Rappresenta il livello di abilità a cui un rispondente ha il 50% di probabilità di rispondere correttamente all’item. Un valore positivo indica un item più difficile (richiede un livello di abilità superiore per rispondere correttamente), mentre un valore negativo indica un item più facile. Ad esempio, I7 ha un valore di difficoltà di 2.53, il che significa che è relativamente difficile, mentre I3, con un valore di -2.04, è relativamente facile.\n\n\\(g\\) (Probabilità di Indovinare): In questo modello, la probabilità di indovinare è impostata a zero per tutti gli item, il che è coerente con il modello di Rasch, dove non si considera la possibilità di indovinare correttamente un item per caso.\n\nAdattiamo ora ai dati il modello di Rasch con la funzione eRm::RM()\n\nrm_sum0 &lt;- eRm::RM(responses)\n\n\nsummary(rm_sum0)\n#&gt; \n#&gt; Results of RM estimation: \n#&gt; \n#&gt; Call:  eRm::RM(X = responses) \n#&gt; \n#&gt; Conditional log-likelihood: -1887 \n#&gt; Number of iterations: 23 \n#&gt; Number of parameters: 13 \n#&gt; \n#&gt; Item (Category) Difficulty Parameters (eta): with 0.95 CI:\n#&gt;     Estimate Std. Error lower CI upper CI\n#&gt; I2    -1.420      0.121   -1.658   -1.183\n#&gt; I3    -2.210      0.145   -2.494   -1.926\n#&gt; I6    -0.215      0.108   -0.426   -0.004\n#&gt; I7     2.364      0.170    2.031    2.697\n#&gt; I11   -1.420      0.121   -1.658   -1.183\n#&gt; I12    0.642      0.113    0.422    0.863\n#&gt; I14   -0.663      0.110   -0.879   -0.448\n#&gt; I17    1.152      0.122    0.913    1.391\n#&gt; I18   -0.565      0.109   -0.778   -0.351\n#&gt; I19    1.889      0.146    1.602    2.175\n#&gt; I21    1.578      0.134    1.315    1.841\n#&gt; I22    2.244      0.163    1.925    2.564\n#&gt; I23   -2.103      0.141   -2.379   -1.827\n#&gt; \n#&gt; Item Easiness Parameters (beta) with 0.95 CI:\n#&gt;          Estimate Std. Error lower CI upper CI\n#&gt; beta I1     1.273      0.118    1.041    1.504\n#&gt; beta I2     1.420      0.121    1.183    1.658\n#&gt; beta I3     2.210      0.145    1.926    2.494\n#&gt; beta I6     0.215      0.108    0.004    0.426\n#&gt; beta I7    -2.364      0.170   -2.697   -2.031\n#&gt; beta I11    1.420      0.121    1.183    1.658\n#&gt; beta I12   -0.642      0.113   -0.863   -0.422\n#&gt; beta I14    0.663      0.110    0.448    0.879\n#&gt; beta I17   -1.152      0.122   -1.391   -0.913\n#&gt; beta I18    0.565      0.109    0.351    0.778\n#&gt; beta I19   -1.889      0.146   -2.175   -1.602\n#&gt; beta I21   -1.578      0.134   -1.841   -1.315\n#&gt; beta I22   -2.244      0.163   -2.564   -1.925\n#&gt; beta I23    2.103      0.141    1.827    2.379\n\nLa funzione RM() impone un vincolo sulle stime dei parametri di difficoltà degli item. Questo vincolo è che la media di questi parametri (beta) sia zero. Questo approccio è noto come “parametrizzazione ancorata” o “centrata”. Il vantaggio di questa parametrizzazione è che posiziona la scala di difficoltà degli item in un punto di riferimento fisso, facilitando il confronto tra diversi set di item o tra diverse applicazioni dello stesso test.\nVerifichiamo.\n\ncoef(rm_sum0) \n#&gt;  beta I1  beta I2  beta I3  beta I6  beta I7 beta I11 beta I12 beta I14 \n#&gt;   1.2726   1.4203   2.2098   0.2153  -2.3639   1.4203  -0.6424   0.6633 \n#&gt; beta I17 beta I18 beta I19 beta I21 beta I22 beta I23 \n#&gt;  -1.1517   0.5646  -1.8886  -1.5781  -2.2444   2.1029\n\n\nsum(rm_sum0$betapar)\n#&gt; [1] 8.882e-16\n\nNella parametrizzazione utilizzata da mirt(), i parametri di difficoltà vengono invece stimati senza un vincolo sulla loro media. Questo può portare a stime dei parametri di difficoltà che differiscono da quelle ottenute tramite RM(). Questa libertà nella stima dei parametri permette una flessibilità maggiore, specialmente in modelli IRT complessi o multidimensionali, ma può rendere più complesso il confronto diretto tra set di item o test differenti.\nDalla soluzione prodotta da eRm::RM() possiamo estrarre le stime sia nei termini della facilità che della difficoltà degli item.\n\ntab &lt;- data.frame(\n    item_score = colSums(responses),\n    easiness = coef(rm_sum0),\n    difficulty = -coef(rm_sum0)\n)\ntab[order(tab$item_score), ]\n#&gt;     item_score easiness difficulty\n#&gt; I7          40  -2.3639     2.3639\n#&gt; I22         44  -2.2444     2.2444\n#&gt; I19         58  -1.8886     1.8886\n#&gt; I21         73  -1.5781     1.5781\n#&gt; I17         98  -1.1517     1.1517\n#&gt; I12        134  -0.6424     0.6424\n#&gt; I6         204   0.2153    -0.2153\n#&gt; I18        233   0.5646    -0.5646\n#&gt; I14        241   0.6633    -0.6633\n#&gt; I1         287   1.2726    -1.2726\n#&gt; I2         297   1.4203    -1.4203\n#&gt; I11        297   1.4203    -1.4203\n#&gt; I23        336   2.1029    -2.1029\n#&gt; I3         341   2.2098    -2.2098\n\nIn alterativa, possiamo usare il pacchetto TAM. Come nel caso di mirt, anche in questo caso viene usata una procedura di stima di massima verosimiglianza marginale.\n\ntam_rm &lt;- tam.mml(responses)\n#&gt; ....................................................\n#&gt; Processing Data      2025-03-22 08:06:23.614344 \n#&gt;     * Response Data: 400 Persons and  14 Items \n#&gt;     * Numerical integration with 21 nodes\n#&gt;     * Created Design Matrices   ( 2025-03-22 08:06:23.615454 )\n#&gt;     * Calculated Sufficient Statistics   ( 2025-03-22 08:06:23.616819 )\n#&gt; ....................................................\n#&gt; Iteration 1     2025-03-22 08:06:23.617928\n#&gt; E Step\n#&gt; M Step Intercepts   |----\n#&gt;   Deviance = 5667.9246\n#&gt;   Maximum item intercept parameter change: 0.3147\n#&gt;   Maximum item slope parameter change: 0\n#&gt;   Maximum regression parameter change: 0\n#&gt;   Maximum variance parameter change: 0.123\n#&gt; ....................................................\n#&gt; Iteration 2     2025-03-22 08:06:23.618981\n#&gt; E Step\n#&gt; M Step Intercepts   |--\n#&gt;   Deviance = 5633.0417 | Absolute change: 34.88 | Relative change: 0.006193\n#&gt;   Maximum item intercept parameter change: 0.003284\n#&gt;   Maximum item slope parameter change: 0\n#&gt;   Maximum regression parameter change: 0\n#&gt;   Maximum variance parameter change: 0.007956\n#&gt; ....................................................\n#&gt; Iteration 3     2025-03-22 08:06:23.622938\n#&gt; E Step\n#&gt; M Step Intercepts   |--\n#&gt;   Deviance = 5633.0057 | Absolute change: 0.036 | Relative change: 6.39e-06\n#&gt;   Maximum item intercept parameter change: 0.002198\n#&gt;   Maximum item slope parameter change: 0\n#&gt;   Maximum regression parameter change: 0\n#&gt;   Maximum variance parameter change: 0.005837\n#&gt; ....................................................\n#&gt; Iteration 4     2025-03-22 08:06:23.62338\n#&gt; E Step\n#&gt; M Step Intercepts   |--\n#&gt;   Deviance = 5632.9887 | Absolute change: 0.017 | Relative change: 3.02e-06\n#&gt;   Maximum item intercept parameter change: 0.00151\n#&gt;   Maximum item slope parameter change: 0\n#&gt;   Maximum regression parameter change: 0\n#&gt;   Maximum variance parameter change: 0.004059\n#&gt; ....................................................\n#&gt; Iteration 5     2025-03-22 08:06:23.623682\n#&gt; E Step\n#&gt; M Step Intercepts   |--\n#&gt;   Deviance = 5632.9805 | Absolute change: 0.0081 | Relative change: 1.44e-06\n#&gt;   Maximum item intercept parameter change: 0.001036\n#&gt;   Maximum item slope parameter change: 0\n#&gt;   Maximum regression parameter change: 0\n#&gt;   Maximum variance parameter change: 0.002812\n#&gt; ....................................................\n#&gt; Iteration 6     2025-03-22 08:06:23.623967\n#&gt; E Step\n#&gt; M Step Intercepts   |--\n#&gt;   Deviance = 5632.9767 | Absolute change: 0.0039 | Relative change: 6.8e-07\n#&gt;   Maximum item intercept parameter change: 0.00071\n#&gt;   Maximum item slope parameter change: 0\n#&gt;   Maximum regression parameter change: 0\n#&gt;   Maximum variance parameter change: 0.001944\n#&gt; ....................................................\n#&gt; Iteration 7     2025-03-22 08:06:23.624269\n#&gt; E Step\n#&gt; M Step Intercepts   |--\n#&gt;   Deviance = 5632.9749 | Absolute change: 0.0018 | Relative change: 3.2e-07\n#&gt;   Maximum item intercept parameter change: 0.000486\n#&gt;   Maximum item slope parameter change: 0\n#&gt;   Maximum regression parameter change: 0\n#&gt;   Maximum variance parameter change: 0.001342\n#&gt; ....................................................\n#&gt; Iteration 8     2025-03-22 08:06:23.624569\n#&gt; E Step\n#&gt; M Step Intercepts   |--\n#&gt;   Deviance = 5632.974 | Absolute change: 0.0009 | Relative change: 1.5e-07\n#&gt;   Maximum item intercept parameter change: 0.000333\n#&gt;   Maximum item slope parameter change: 0\n#&gt;   Maximum regression parameter change: 0\n#&gt;   Maximum variance parameter change: 0.000925\n#&gt; ....................................................\n#&gt; Iteration 9     2025-03-22 08:06:23.624879\n#&gt; E Step\n#&gt; M Step Intercepts   |--\n#&gt;   Deviance = 5632.9736 | Absolute change: 0.0004 | Relative change: 7e-08\n#&gt;   Maximum item intercept parameter change: 0.000228\n#&gt;   Maximum item slope parameter change: 0\n#&gt;   Maximum regression parameter change: 0\n#&gt;   Maximum variance parameter change: 0.000637\n#&gt; ....................................................\n#&gt; Iteration 10     2025-03-22 08:06:23.625187\n#&gt; E Step\n#&gt; M Step Intercepts   |--\n#&gt;   Deviance = 5632.9734 | Absolute change: 0.0002 | Relative change: 3e-08\n#&gt;   Maximum item intercept parameter change: 0.000156\n#&gt;   Maximum item slope parameter change: 0\n#&gt;   Maximum regression parameter change: 0\n#&gt;   Maximum variance parameter change: 0.000439\n#&gt; ....................................................\n#&gt; Iteration 11     2025-03-22 08:06:23.625501\n#&gt; E Step\n#&gt; M Step Intercepts   |--\n#&gt;   Deviance = 5632.9733 | Absolute change: 0.0001 | Relative change: 2e-08\n#&gt;   Maximum item intercept parameter change: 0.000107\n#&gt;   Maximum item slope parameter change: 0\n#&gt;   Maximum regression parameter change: 0\n#&gt;   Maximum variance parameter change: 0.000302\n#&gt; ....................................................\n#&gt; Iteration 12     2025-03-22 08:06:23.625786\n#&gt; E Step\n#&gt; M Step Intercepts   |-\n#&gt;   Deviance = 5632.9733 | Absolute change: 0 | Relative change: 1e-08\n#&gt;   Maximum item intercept parameter change: 0.000073\n#&gt;   Maximum item slope parameter change: 0\n#&gt;   Maximum regression parameter change: 0\n#&gt;   Maximum variance parameter change: 0.000208\n#&gt; ....................................................\n#&gt; Iteration 13     2025-03-22 08:06:23.626048\n#&gt; E Step\n#&gt; M Step Intercepts   |-\n#&gt;   Deviance = 5632.9732 | Absolute change: 0 | Relative change: 0\n#&gt;   Maximum item intercept parameter change: 5e-05\n#&gt;   Maximum item slope parameter change: 0\n#&gt;   Maximum regression parameter change: 0\n#&gt;   Maximum variance parameter change: 0.000143\n#&gt; ....................................................\n#&gt; Iteration 14     2025-03-22 08:06:23.626315\n#&gt; E Step\n#&gt; M Step Intercepts   |-\n#&gt;   Deviance = 5632.9732 | Absolute change: 0 | Relative change: 0\n#&gt;   Maximum item intercept parameter change: 0.000034\n#&gt;   Maximum item slope parameter change: 0\n#&gt;   Maximum regression parameter change: 0\n#&gt;   Maximum variance parameter change: 0.000098\n#&gt; ....................................................\n#&gt; Item Parameters\n#&gt;    xsi.index xsi.label     est\n#&gt; 1          1        I1 -1.1029\n#&gt; 2          2        I2 -1.2505\n#&gt; 3          3        I3 -2.0422\n#&gt; 4          4        I6 -0.0480\n#&gt; 5          5        I7  2.5312\n#&gt; 6          6       I11 -1.2505\n#&gt; 7          7       I12  0.8132\n#&gt; 8          8       I14 -0.4952\n#&gt; 9          9       I17  1.3263\n#&gt; 10        10       I18 -0.3969\n#&gt; 11        11       I19  2.0639\n#&gt; 12        12       I21  1.7545\n#&gt; 13        13       I22  2.4145\n#&gt; 14        14       I23 -1.9345\n#&gt; ...................................\n#&gt; Regression Coefficients\n#&gt;      [,1]\n#&gt; [1,]    0\n#&gt; \n#&gt; Variance:\n#&gt;        [,1]\n#&gt; [1,] 0.9037\n#&gt; \n#&gt; \n#&gt; EAP Reliability:\n#&gt; [1] 0.656\n#&gt; \n#&gt; -----------------------------\n#&gt; Start:  2025-03-22 08:06:23.613484\n#&gt; End:  2025-03-22 08:06:23.628764 \n#&gt; Time difference of 0.01528 secs\n\nPossiamo ispezionare le stime dei parametri con\n\ntam_rm$item\n#&gt;     item   N      M xsi.item AXsi_.Cat1 B.Cat1.Dim1\n#&gt; I1    I1 400 0.7175 -1.10293   -1.10293           1\n#&gt; I2    I2 400 0.7425 -1.25048   -1.25048           1\n#&gt; I3    I3 400 0.8525 -2.04220   -2.04220           1\n#&gt; I6    I6 400 0.5100 -0.04803   -0.04803           1\n#&gt; I7    I7 400 0.1000  2.53123    2.53123           1\n#&gt; I11  I11 400 0.7425 -1.25048   -1.25048           1\n#&gt; I12  I12 400 0.3350  0.81319    0.81319           1\n#&gt; I14  I14 400 0.6025 -0.49523   -0.49523           1\n#&gt; I17  I17 400 0.2450  1.32632    1.32632           1\n#&gt; I18  I18 400 0.5825 -0.39687   -0.39687           1\n#&gt; I19  I19 400 0.1450  2.06394    2.06394           1\n#&gt; I21  I21 400 0.1825  1.75454    1.75454           1\n#&gt; I22  I22 400 0.1100  2.41452    2.41452           1\n#&gt; I23  I23 400 0.8400 -1.93454   -1.93454           1\n\nLe colonne di questo output possono essere interpretate come segue:\n\n\nitem indica il nome dell’item.\n\nN indica il numero di candidati che hanno risposto a ciascun item. In questo caso, tutti i 400 candidati hanno risposto a ogni item.\n\nM è la media delle risposte a ciascun item. Nel caso di un item con una media alta ciò significa che a tale item è stata fornita uba risposta corretta da una alta percentuale di candidati.\n\nxsi.item per il modello di Rasch è il parametro di difficoltà dell’item. Gli item con valori alti tendono ad essere più difficili.\n\nAXsi.Cat1 ripete la difficoltà dell’item per il modello di Rasch, ma permetterebbe l’inclusione di una matrice di design A, che non abbiamo usato qui. Per i modelli politomici, l’output includerà parametri dell’item per più di una categoria.\n\nB.Cat1.Dim1 è il parametro di discriminazione o pendenza dell’item. Per il modello di Rasch, la pendenza è 1 per ogni item.\n\nPossiamo mostrare solo la difficoltà e l’errore standard con:\n\ntam_rm$xsi\n#&gt;          xsi se.xsi\n#&gt; I1  -1.10293 0.1199\n#&gt; I2  -1.25048 0.1231\n#&gt; I3  -2.04220 0.1490\n#&gt; I6  -0.04803 0.1092\n#&gt; I7   2.53123 0.1742\n#&gt; I11 -1.25048 0.1231\n#&gt; I12  0.81319 0.1149\n#&gt; I14 -0.49523 0.1113\n#&gt; I17  1.32632 0.1250\n#&gt; I18 -0.39687 0.1105\n#&gt; I19  2.06394 0.1501\n#&gt; I21  1.75454 0.1378\n#&gt; I22  2.41452 0.1675\n#&gt; I23 -1.93454 0.1445\n\nLa parametrizzazione classica IRT si ottiene con:\n\ntam_rm$item_irt\n#&gt;    item alpha     beta\n#&gt; 1    I1     1 -1.10293\n#&gt; 2    I2     1 -1.25048\n#&gt; 3    I3     1 -2.04220\n#&gt; 4    I6     1 -0.04803\n#&gt; 5    I7     1  2.53123\n#&gt; 6   I11     1 -1.25048\n#&gt; 7   I12     1  0.81319\n#&gt; 8   I14     1 -0.49523\n#&gt; 9   I17     1  1.32632\n#&gt; 10  I18     1 -0.39687\n#&gt; 11  I19     1  2.06394\n#&gt; 12  I21     1  1.75454\n#&gt; 13  I22     1  2.41452\n#&gt; 14  I23     1 -1.93454",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_implementation.html#valutazione-del-test",
    "href": "chapters/irt/05_implementation.html#valutazione-del-test",
    "title": "71  Implementazione",
    "section": "\n71.4 Valutazione del Test",
    "text": "71.4 Valutazione del Test\nIl primo approccio per valutare i test consiste nell’utilizzo di metodi grafici. Tra questi, il primo strumento che esaminiamo è la mappa persona-item, utile per verificare se il campione di partecipanti copre l’intera gamma di difficoltà degli item e se, viceversa, gli item coprono l’intero spettro delle abilità del campione.\nIl secondo metodo riguarda il confronto tra le Curve Caratteristiche degli Item (ICC) teoriche ed empiriche, che consente di identificare eventuali item con scarsa aderenza al modello.\nInfine, il terzo approccio è il test grafico per il Funzionamento Differenziale degli Item (DIF), che offre un metodo visivo per rilevare discrepanze nel comportamento degli item tra gruppi di rispondenti.\n\n71.4.1 Mappa Persona-Item\nLa Mappa Persona-Item (nota anche come Wright Map o person-item map) è uno strumento grafico utile per valutare quanto efficacemente gli item coprono l’intervallo delle abilità latenti nel campione studiato. Questo strumento consente di rispondere alla domanda: Quanto bene gli item riflettono la gamma delle abilità latenti presenti nel campione?\nLa costruzione della mappa avviene in due fasi:\n\n\nDistribuzione delle abilità latenti: Si rappresenta graficamente la distribuzione delle abilità latenti (\\(\\theta\\)) del campione di persone.\n\nDifficoltà degli item: Si sovrappone la difficoltà di ciascun item sulla stessa scala di \\(\\theta\\) utilizzata per le abilità.\n\nL’allineamento di queste due rappresentazioni permette di valutare visivamente la corrispondenza tra le abilità del campione e le difficoltà degli item. Idealmente, le difficoltà degli item dovrebbero coprire l’intera gamma delle abilità delle persone, e viceversa. Questa corrispondenza è essenziale per garantire una stima precisa dei parametri degli item e delle abilità dei rispondenti.\nLa mappa persona-item fornisce quindi una panoramica intuitiva e immediata dell’adeguatezza del test rispetto al campione studiato, evidenziando eventuali gap nella copertura delle abilità o difficoltà non equilibrate.\nPer generare una mappa persona-item, è possibile utilizzare la funzione:\n\nitempersonMap(mirt_rm)\n\n\n\n\n\n\n\nQuesta funzione consente di visualizzare facilmente le distribuzioni e di valutare il bilanciamento tra le abilità delle persone e le difficoltà degli item.\nLa parte superiore della mappa persona-item mostra un istogramma delle stime dei parametri di abilità, mentre la parte inferiore mostra le stime delle difficoltà per ciascun item del test. Per ogni item, la stima della difficoltà è indicata dalla posizione del punto sulla linea tratteggiata corrispondente a quell’item. Ad esempio, la difficoltà stimata per l’item 1 corrisponde alla posizione del punto sulla linea tratteggiata più in alto. La mappa persona-item offre un controllo visivo di coerenza per le stime del nostro modello IRT (Teoria della Risposta all’Item). Le stime delle abilità sono più accurate quando cadono nel mezzo della distribuzione dei parametri degli item e viceversa. Pertanto, idealmente, l’istogramma delle abilità e le stime delle difficoltà dovrebbero essere centrate sullo stesso punto e mostrare un’ampia sovrapposizione. Nel nostro test, sembra essere questo il caso.\nIn alternativa, possiamo usare la funzione plotPImap() di eRm.\n\n71.4.2 ICC Empiriche\nLe Curve Caratteristiche degli Item (ICC) descrivono la relazione teorica tra l’abilità dei partecipanti al test e la probabilità di una risposta corretta che ci aspettiamo sotto il modello di Rasch per una data difficoltà. La ICC attesa per un item può essere tracciata dopo che la sua difficoltà è stata stimata. Oltre alle probabilità attese di una risposta corretta illustrate dall’ICC, possiamo anche tracciare le frequenze relative empiriche di una risposta corretta. Queste frequenze relative empiriche sono indicate nella figura come punti e vengono chiamate ICC empiriche.\nUsando eRm possimo generare le ECC empiriche nel modo seguente.\n\nplotICC(\n    rm_sum0, \n    item.subset = \"all\",\n    empICC = list(\"raw\"), \n    empCI = list()\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLe ICC empiriche sono rappresentate dai singoli punti, mentre la ICC attesa sotto il modello di Rasch è indicata dalla linea liscia. Dalle figure precedenti, per gli item 12 e 14 notiamo che in generale la forma dell’ICC empirica è molto ben allineata con l’ICC attesa, ma per l’item 12 l’ICC empirica mostra valori sopra zero anche per le abilità più basse a sinistra della dimensione latente. Questo potrebbe indicare una tendenza al tentativo di indovinare (guessing). Per l’item 19, l’ICC empirica appare più ripida dell’ICC attesa sotto il modello di Rasch. Mostra un salto molto più pronunciato tra la prima metà approssimativa dei punti e i punti rimanenti. Per l’item 21, al contrario, l’ICC empirica è molto più piatta rispetto a quella attesa. Confronteremo la nostra impressione visiva con le statistiche di adattamento degli item per questi item di seguito.\nÈ possibile visualizzare le ICC attese di tutti gli item del test in un unico grafico utilizzando la funzione plotjointICC(). Questo grafico ci permette di esaminare come la difficoltà influenzi la probabilità che un candidato risponda correttamente a un item. Ricordiamo che la difficoltà di un item è definita come il livello di abilità in cui una persona ha una probabilità del 50% di rispondere correttamente all’item. Abbiamo aggiunto una linea tratteggiata orizzontale alla probabilità di 0.5 usando il comando segments. Il punto in cui un’ICC interseca questa linea rappresenta la sua difficoltà. Questo ci permette di leggere facilmente le difficoltà relative degli item dal grafico. Spostandosi da sinistra a destra, il primo ICC intersecato dalla linea orizzontale corrisponde all’item meno difficile (in questo caso l’item 3, seguito da vicino dall’item 23, come indicato nell’ordine degli item nella legenda), e l’ultimo ICC intersecato dalla linea orizzontale è per l’item più difficile (in questo caso l’item 7).\nDa notare che le ICC attese nella figura sono parallele per definizione. Il modello di Rasch assume che le ICC siano parallele, quindi produrrà sempre ICC teoriche o attese parallele, anche quando gli item hanno in realtà pendenze o tassi di guessing diversi, come abbiamo visto in precedenza per le ICC empiriche.\n\neRm::plotjointICC(rm_sum0, cex = 0.7)\nsegments(-2.5, 0.5, 4, 0.5, lty = 2)\n\n\n\n\n\n\n\nIn alternativa, possiamo generare le ICC usando il pacchetto mirt:\n\nmirt_rm &lt;- mirt(responses, 1, \"Rasch\")\n#&gt; \nIteration: 1, Log-Lik: -2816.924, Max-Change: 0.03346\nIteration: 2, Log-Lik: -2816.650, Max-Change: 0.02041\nIteration: 3, Log-Lik: -2816.562, Max-Change: 0.01357\nIteration: 4, Log-Lik: -2816.520, Max-Change: 0.01027\nIteration: 5, Log-Lik: -2816.501, Max-Change: 0.00584\nIteration: 6, Log-Lik: -2816.493, Max-Change: 0.00395\nIteration: 7, Log-Lik: -2816.490, Max-Change: 0.00307\nIteration: 8, Log-Lik: -2816.488, Max-Change: 0.00174\nIteration: 9, Log-Lik: -2816.487, Max-Change: 0.00118\nIteration: 10, Log-Lik: -2816.487, Max-Change: 0.00094\nIteration: 11, Log-Lik: -2816.487, Max-Change: 0.00054\nIteration: 12, Log-Lik: -2816.487, Max-Change: 0.00036\nIteration: 13, Log-Lik: -2816.487, Max-Change: 0.00028\nIteration: 14, Log-Lik: -2816.487, Max-Change: 0.00016\nIteration: 15, Log-Lik: -2816.487, Max-Change: 0.00011\nIteration: 16, Log-Lik: -2816.487, Max-Change: 0.00008\nplot(mirt_rm, type = \"trace\")\n\n\n\n\n\n\n\nLe curve caratteristiche degli item offrono un quadro dettagliato e visuale di come ciascun item del test si comporta attraverso diversi livelli dell’abilità latente. Per esempio:\n\n\nVisualizzazione della Difficoltà e della Discriminazione:\n\nSupponiamo di avere un item che mostra una curva con una ripida salita in un punto specifico della scala di abilità. Questo indica che l’item ha una difficoltà concentrata attorno a quel punto e che discrimina efficacemente tra rispondenti con abilità appena al di sotto e al di sopra di quel livello.\nAl contrario, una curva più graduale suggerisce che l’item è meno discriminante, con una variazione più ampia nella probabilità di risposta corretta a seconda del livello di abilità.\n\n\n\nIdentificazione di Lacune nella Valutazione:\n\nVisualizzando le curve di più item, possiamo identificare se ci sono lacune nella copertura dell’abilità latente. Ad esempio, se tutti gli item hanno curve che si concentrano su livelli di abilità bassi, potrebbe esserci una mancanza di item difficili per misurare l’abilità ad alti livelli.\nInoltre, se le curve degli item si sovrappongono eccessivamente, potrebbe indicare ridondanza tra gli item, suggerendo che alcuni di essi non aggiungono informazioni uniche alla valutazione.\n\n\n\nConfronto tra Diversi Tipi di Item:\n\nPer esempio, gli item progettati per misurare concetti di base potrebbero avere curve che mostrano alta probabilità di risposta corretta anche a livelli di abilità bassi.\nAl contrario, item progettati per essere più impegnativi potrebbero mostrare probabilità elevate di risposta corretta solo a livelli di abilità più alti.\n\n\n\n71.4.3 Test Grafico\nIl test grafico del modello, basato sui principi di Rasch (1960), è un metodo intuitivo per valutare l’invarianza degli item in un test, confrontando i parametri degli item stimati per due gruppi di persone. Affinché il modello di Rasch sia considerato valido, è necessario che le stime dei parametri degli item per i diversi gruppi concordino, fino a una trasformazione lineare. In termini pratici, ciò si traduce nel fatto che, quando visualizzate in un grafico, le stime dei parametri degli item dei due gruppi dovrebbero allinearsi lungo una linea retta.\nPer complementare questa analisi, possiamo ricorrere al test del rapporto di verosimiglianza di Andersen (1973), un approccio ben consolidato per verificare l’adeguatezza del modello di Rasch nel rappresentare il comportamento dei partecipanti ai test. Il test di Andersen valuta se le stime dei parametri degli item rimangono consistenti tra diversi gruppi di partecipanti. Se i parametri degli item stimati individualmente per ciascun gruppo differiscono significativamente, ciò indica che il modello di Rasch potrebbe non essere un’adeguata rappresentazione del comportamento osservato nei test.\nA differenza del test grafico, il test del rapporto di verosimiglianza confronta il massimo della verosimiglianza condizionata sotto il modello di Rasch con il massimo della verosimiglianza condizionata quando i parametri degli item possono variare tra i gruppi. Questa metodologia offre un’indicazione di quanto efficacemente ciascun modello rappresenti il comportamento dei partecipanti.\nIl test del rapporto di verosimiglianza utilizza la statistica di test \\(T = −2 \\cdot log(LR)\\), che ha una distribuzione campionaria approssimativamente \\(\\chi^2\\) per campioni grandi. Valori del rapporto di verosimiglianza inferiori a 1, o valori elevati di T, suggeriscono una violazione del modello di Rasch.\nIl test di Andersen è implementato nel pacchetto eRm in R, offrendo uno strumento utile per l’analisi. Tuttavia, è importante notare che un risultato non significativo in questo test non può essere interpretato automaticamente come supporto per il modello di Rasch, specialmente se il modello più generale non descrive adeguatamente i dati. Inoltre, la capacità di rilevare differenze tra i gruppi specificati dipende dall’effettiva diversità dei parametri del modello tra questi gruppi. Sono stati messi a punto approcci più flessibili per rilevare le differenze nei parametri.\n\nlrt_mean_split &lt;- LRtest(rm_sum0, splitcr = \"mean\")\nlrt_mean_split\n#&gt; \n#&gt; Andersen LR-test: \n#&gt; LR-value: 79.71 \n#&gt; Chi-square df: 13 \n#&gt; p-value:  0\n\nL’output di questo test mostra una violazione significativa del modello di Rasch al livello \\(\\alpha\\) = 0.05.\n\nplotGOF(\n    lrt_mean_split,\n    tlab = \"item\", pos = 1,\n    main = \"Difficulty by Score (with Item Names)\",\n    conf = list(gamma = 0.95, col = 1)\n)\n\n\n\n\n\n\n\nOra possiamo tracciare le stime delle difficoltà di ciascun gruppo utilizzando la funzione plotGOF() per creare il test grafico. La funzione plotGOF() prende il risultato di LRtest() e traccia le stime dei parametri degli item per i due gruppi. Per facilitare la valutazione visiva, plotGOF() può opzionalmente etichettare gli item e aggiungere ellissi di confidenza.\nPer creare il grafico per il test grafico basato sulla divisione media, possiamo procedere in questo modo: ogni piccolo cerchio nella Figura mostra le stime delle difficoltà per un singolo item. La coordinata x di un cerchio indica la sua stima di difficoltà per i partecipanti al test con punteggi sotto la media e la sua coordinata y indica la stima di difficoltà per i partecipanti al test con punteggi sopra la media. La linea y = x è fornita come riferimento, poiché i punti che cadono su questa linea avrebbero la stessa stima in entrambi i gruppi. La distanza tra qualsiasi punto e la linea di riferimento y = x indica quanto le stime differiscono tra i due gruppi. Indica anche la direzione di questa differenza. Gli item sotto la linea sono più difficili per i partecipanti al test con punteggi sotto la media, mentre gli item sopra la linea sono più difficili per i partecipanti al test con punteggi sopra la media.\nGli assi orizzontali e verticali mostrano intervalli di confidenza per le stime per ciascun gruppo di partecipanti al test. La larghezza di ciascun intervallo di confidenza è determinata dall’elemento gamma della lista fornita a conf. L’impostazione predefinita gamma = .95 produce intervalli di confidenza al 95% per ciascun asse dell’ellisse. Quando un’ellisse di confidenza non incrocia la linea di riferimento, l’item rispettivo è diagnosticato come mostrante un significativo DIF.\nLa figura indica che gli item 2, 6, 21 e 22 differiscono significativamente tra le persone con punteggi sopra e sotto la media, poiché le loro ellissi di confidenza non incrociano la linea di riferimento. Gli item 21 e 22 sono più difficili per le persone con punteggi pari o superiori alla media, mentre gli item 2 e 6 sono più difficili per le persone con punteggi sotto la media. Tali violazioni del modello possono verificarsi quando le ICC osservate differiscono dalle ICC attese sotto il modello di Rasch per i partecipanti al test con abilità basse e alte. Questo può accadere, ad esempio, se è presente il tentativo di indovinare (guessing), o se la pendenza è più ripida o meno ripida di quanto previsto dal modello di Rasch.\nPossiamo anche fornire all’argomento splitcr una variabile che divide i partecipanti al test in gruppi. Ad esempio, possiamo testare se i parametri degli item differiscono in base al genere passando un vettore contenente le appartenenze di gruppo come argomento splitcr.\n\nlrt_gender &lt;- LRtest(rm_sum0, splitcr = gender)\nlrt_gender\n#&gt; \n#&gt; Andersen LR-test: \n#&gt; LR-value: 32.97 \n#&gt; Chi-square df: 13 \n#&gt; p-value:  0.002\n\nCome nel test precedente, anche il Test del Rapporto di Verosimiglianza (LRT) per il genere indica una violazione significativa del modello di Rasch al livello α = 0.05.\n\nplotGOF(\n    lrt_gender,\n    tlab = \"item\", pos = 1,\n    main = \"Difficulty by Score (with Item Names)\",\n    conf = list(gamma = 0.95, col = 1)\n)\n\n\n\n\n\n\n\nLa figura indica che gli item 2, 7 e 21 differiscono tra partecipanti al test femminili e maschili. Gli item 2 e 7 sono più difficili per i partecipanti femminili, mentre l’item 21 è più difficile per i partecipanti maschili.\n\n71.4.4 Test di Wald\nLe impostazioni del test del rapporto di verosimiglianza di Andersen (1973) e del test di Wald sono molto simili. Entrambi i test si basano sull’idea che il modello di Rasch sia un modello ragionevole per i dati dei test solo se i parametri degli item stimati non variano sistematicamente tra gruppi di persone. In entrambi i test, consideriamo le stime dei parametri degli item per ciascun gruppo di persone. A differenza del test del rapporto di verosimiglianza, tuttavia, il test di Wald confronta direttamente le stime dei parametri degli item dei gruppi. In sostanza, il test di Wald calcola la differenza tra la stima del primo gruppo della difficoltà dell’item i, β̂(1)i, e quella del secondo gruppo, β̂(2)i. Questa differenza viene divisa per il suo errore standard per tenere conto del fatto che tutte le stime sono soggette a rumore. Questo porta alla statistica di test per l’item i:\n\\[\nT_i = \\frac{\\hat{\\beta}^{(1)}_i - \\hat{\\beta}^{(2)}_i}{\\sqrt{se(\\hat{\\beta}^{(1)}_i)^2 + se(\\hat{\\beta}^{(2)}_i)^2}},\n\\]\ndove $ se(^{(1)}_i) $ e $ se(^{(2)}_i) $ indicano rispettivamente gli errori standard di $ ^{(1)}_i $ e $ ^{(2)}_i $.\nPer campioni di grandi dimensioni, $ T_i $ approssimativamente segue una distribuzione normale standard sotto l’ipotesi nulla che il vero parametro dell’item sia lo stesso per entrambi i gruppi. Valori estremi di $ T_i $ sono improbabili sotto la distribuzione normale. Quindi, un valore estremo di $ T_i $, con un piccolo valore p, indica che l’item i viola il modello di Rasch.\nEseguiamo il test con R:\n\nWaldtest(rm_sum0, splitcr = \"mean\")\n#&gt; \n#&gt; Wald test on item level (z-values):\n#&gt; \n#&gt;          z-statistic p-value\n#&gt; beta I1       -0.514   0.607\n#&gt; beta I2       -3.328   0.001\n#&gt; beta I3       -0.838   0.402\n#&gt; beta I6       -2.555   0.011\n#&gt; beta I7        0.210   0.834\n#&gt; beta I11      -1.773   0.076\n#&gt; beta I12       1.562   0.118\n#&gt; beta I14       1.821   0.069\n#&gt; beta I17       1.550   0.121\n#&gt; beta I18       0.333   0.739\n#&gt; beta I19      -1.827   0.068\n#&gt; beta I21       5.768   0.000\n#&gt; beta I22       4.106   0.000\n#&gt; beta I23      -1.560   0.119\n\nQuesti test indicano nuovamente che gli item 2, 6, 21 e 22 differiscono significativamente tra i partecipanti al test con punteggi sopra e sotto la media.\nPossiamo anche eseguire il test per la differenza tra maschi e femmine:\n\nWaldtest(rm_sum0, splitcr = gender)\n#&gt; \n#&gt; Wald test on item level (z-values):\n#&gt; \n#&gt;          z-statistic p-value\n#&gt; beta I1       -1.727   0.084\n#&gt; beta I2        2.543   0.011\n#&gt; beta I3       -1.020   0.308\n#&gt; beta I6        0.067   0.946\n#&gt; beta I7        3.089   0.002\n#&gt; beta I11      -1.978   0.048\n#&gt; beta I12      -0.861   0.389\n#&gt; beta I14      -0.673   0.501\n#&gt; beta I17       0.815   0.415\n#&gt; beta I18      -0.493   0.622\n#&gt; beta I19       0.583   0.560\n#&gt; beta I21      -2.305   0.021\n#&gt; beta I22      -0.030   0.976\n#&gt; beta I23      -1.019   0.308\n\nI risultati qui concordano in gran parte anche con la figura precedente. In linea con il test grafico, il test di Wald indica che gli item 2, 7 e 21 differiscono tra i gruppi.\n\n71.4.5 Ancoraggio\nL’ancoraggio è una procedura cruciale quando si confrontano le stime dei parametri degli item tra diversi gruppi, un passo fondamentale in test come il Wald e in metodi grafici. Tale processo necessita di particolare attenzione perché implica la restrizione di alcuni parametri degli item per allineare le scale latenti tra i gruppi. Ad esempio, fissare il parametro del primo item a zero in entrambi i gruppi crea un punto di riferimento comune, ma anche limitazioni.\nLa scelta degli item di ancoraggio è delicata: fissare un parametro in entrambi i gruppi significa non poter più valutare la differenza per quell’item specifico. La selezione dovrebbe essere guidata da un’attenta analisi dei dati e da considerazioni teoriche. Approcci guidati dai dati sono stati proposti per identificare item invarianti o escludere quelli con DIF, processo noto come purificazione. Tuttavia, occorre cautela: anche metodi ben progettati possono portare a conclusioni errate se gli item di ancoraggio scelti sono inappropriati.\nIn pratica, spesso si adotta una restrizione in cui la somma dei parametri degli item è zero per tutti i gruppi. Questo approccio, adottato da pacchetti software come eRm e difR in R, si basa sull’assunzione che eventuali DIF si annullino su tutti gli item. Ma se questa assunzione non è valida, o se l’ancoraggio include item con DIF, potremmo incorrere in errori interpretativi.\nIn sintesi, l’ancoraggio è una strategia potente ma che richiede un’attenta considerazione e un’analisi critica. È fondamentale non solo selezionare gli item di ancoraggio adeguati ma anche interpretare i risultati con una comprensione chiara delle ipotesi e delle potenziali limitazioni del metodo scelto.\n\nresp &lt;- as.matrix(responses)\nanchortest(\n    resp ~ gender,\n    class = \"constant\",\n    select = \"MPT\"\n)\n#&gt; Anchor items:\n#&gt; respI23, respI3, respI22, respI12\n#&gt; \n#&gt; Final DIF tests:\n#&gt; \n#&gt;   Simultaneous Tests for General Linear Hypotheses\n#&gt; \n#&gt; Linear Hypotheses:\n#&gt;              Estimate Std. Error z value Pr(&gt;|z|)\n#&gt; respI1 == 0   0.21972    0.29124    0.75   0.4506\n#&gt; respI2 == 0  -0.83344    0.29343   -2.84   0.0045\n#&gt; respI3 == 0   0.10283    0.26853    0.38   0.7018\n#&gt; respI6 == 0  -0.21887    0.27110   -0.81   0.4195\n#&gt; respI7 == 0  -1.79695    0.57084   -3.15   0.0016\n#&gt; respI11 == 0  0.29599    0.29835    0.99   0.3212\n#&gt; respI12 == 0 -0.00299    0.22722   -0.01   0.9895\n#&gt; respI14 == 0 -0.05134    0.27413   -0.19   0.8514\n#&gt; respI17 == 0 -0.41504    0.30706   -1.35   0.1765\n#&gt; respI18 == 0 -0.09294    0.27284   -0.34   0.7334\n#&gt; respI19 == 0 -0.38657    0.36100   -1.07   0.2842\n#&gt; respI21 == 0  0.42520    0.32052    1.33   0.1846\n#&gt; respI22 == 0 -0.19346    0.30037   -0.64   0.5195\n#&gt; (Univariate p values reported)\n\n\nanchortest(\n    resp ~ gender,\n    class = \"forward\",\n    select = \"MTT\"\n)\n#&gt; Anchor items:\n#&gt; respI23, respI12, respI18, respI14, respI6, respI1, respI11,\n#&gt; respI19, respI17\n#&gt; \n#&gt; Final DIF tests:\n#&gt; \n#&gt;   Simultaneous Tests for General Linear Hypotheses\n#&gt; \n#&gt; Linear Hypotheses:\n#&gt;              Estimate Std. Error z value Pr(&gt;|z|)\n#&gt; respI1 == 0    0.2818     0.2362    1.19   0.2328\n#&gt; respI2 == 0   -0.7714     0.2660   -2.90   0.0037\n#&gt; respI3 == 0    0.1649     0.3229    0.51   0.6097\n#&gt; respI6 == 0   -0.1568     0.2157   -0.73   0.4671\n#&gt; respI7 == 0   -1.7349     0.5574   -3.11   0.0019\n#&gt; respI11 == 0   0.3580     0.2432    1.47   0.1410\n#&gt; respI12 == 0   0.0591     0.2262    0.26   0.7940\n#&gt; respI14 == 0   0.0107     0.2188    0.05   0.9610\n#&gt; respI17 == 0  -0.3530     0.2514   -1.40   0.1603\n#&gt; respI18 == 0  -0.0309     0.2175   -0.14   0.8871\n#&gt; respI19 == 0  -0.3245     0.3030   -1.07   0.2841\n#&gt; respI21 == 0   0.4872     0.2952    1.65   0.0989\n#&gt; respI22 == 0  -0.1314     0.3717   -0.35   0.7237\n#&gt; (Univariate p values reported)\n\n\nanchortest(\n    resp ~ gender,\n    select = \"Gini\"\n)\n#&gt; Anchor items:\n#&gt; respI23\n#&gt; \n#&gt; Final DIF tests:\n#&gt; \n#&gt;   Simultaneous Tests for General Linear Hypotheses\n#&gt; \n#&gt; Linear Hypotheses:\n#&gt;              Estimate Std. Error z value Pr(&gt;|z|)\n#&gt; respI1 == 0   0.12610    0.38737    0.33   0.7448\n#&gt; respI2 == 0  -0.92706    0.38904   -2.38   0.0172\n#&gt; respI3 == 0   0.00921    0.42718    0.02   0.9828\n#&gt; respI6 == 0  -0.31249    0.37656   -0.83   0.4066\n#&gt; respI7 == 0  -1.89057    0.63210   -2.99   0.0028\n#&gt; respI11 == 0  0.20238    0.39228    0.52   0.6059\n#&gt; respI12 == 0 -0.09661    0.38679   -0.25   0.8028\n#&gt; respI14 == 0 -0.14496    0.37695   -0.38   0.7006\n#&gt; respI17 == 0 -0.50866    0.40738   -1.25   0.2118\n#&gt; respI18 == 0 -0.18656    0.37641   -0.50   0.6202\n#&gt; respI19 == 0 -0.48019    0.45080   -1.07   0.2868\n#&gt; respI21 == 0  0.33158    0.41809    0.79   0.4277\n#&gt; respI22 == 0 -0.28708    0.47621   -0.60   0.5466\n#&gt; (Univariate p values reported)\n\nGli output di R della funzione anchortest() elencano gli item di ancoraggio selezionati dai rispettivi approcci di selezione dell’ancoraggio, oltre ai risultati del test di Wald basati su questi item di ancoraggio. Tutti e tre gli approcci portano a risultati in cui solo gli item 2 e 7 mostrano DIF per genere, mentre il test grafico e il test di Wald in eRm hanno identificato anche l’item 21 e l’item 11 (al limite) come aventi DIF.\nRiesaminando il test grafico nella figura precedente, notiamo che gli item 2 e 7 mostrano DIF nella stessa direzione (sopra la diagonale), mentre gli item 21 e 11 sono orientati nella direzione opposta (sotto la diagonale) e in misura minore.\nConsiderando questi risultati nel loro insieme, si può concludere che potrebbe essere presente un DIF non bilanciato e che la diagonale usata nella Figura 6.4 non è ideale per valutare gli item. Per illustrare ciò, tracciamo manualmente una linea di riferimento alternativa attraverso la posizione dell’item 23, che è stato selezionato come item di ancoraggio (primario) dai tre approcci presentati in psychotools, utilizzando il comando abline.\n\n plotGOF(\n    lrt_gender,\n    tlab = \"item\", pos = 1,\n    main = \"Difficulty by Gender (with Item Names)\",\n    conf = list(gamma = 0.95, col = 1)\n)\nabline(-0.3, 1, lty=2)\n\n\n\n\n\n\n\nCome si può vedere nella figura risultante, basandoci sulla linea di riferimento alternativa, non troviamo più DIF negli item 11 e 21, ma gli item 2 e 7 mostrano ancora più chiaramente un DIF.\nPer questo set di dati, la stessa conclusione viene raggiunta in eRm quando si utilizza la funzione stepwiseIt(), che esegue diversi test di Wald e ad ogni passo esclude l’item singolo con la statistica di test più grande.\n\nstepwiseIt(rm_sum0, criterion = list(\"Waldtest\", gender))\n#&gt; Eliminated item - Step 1: I7\n#&gt; Eliminated item - Step 2: I2\n#&gt; \n#&gt; Results for stepwise item elimination:\n#&gt; Number of steps: 2 \n#&gt; Criterion: Waldtest\n#&gt; \n#&gt;            z-statistic p-value\n#&gt; Step 1: I7       3.089   0.002\n#&gt; Step 2: I2       3.059   0.002\n\nUtilizzando questo metodo, dopo l’esclusione degli item 7 e 2, che presentavano il DIF più marcato, non si rilevano più differenze significative nei test degli item rimanenti. Per visualizzare meglio questo processo, immaginiamo la figura precedente: inizialmente, la linea di riferimento corrisponde alla diagonale solida. Tuttavia, dopo aver eliminato l’item 7, questa linea si sposta verso quella tratteggiata nel secondo passaggio e, rimuovendo poi l’item 2, si allinea o si avvicina molto alla linea tratteggiata nel terzo passaggio. Di conseguenza, gli item restanti non mostrano più un DIF significativo.\nIn sintesi, mentre i test grafici e di Wald basati sulla restrizione della somma zero possono risultare ingannevoli in presenza di un DIF non bilanciato, l’impiego di metodi di ancoraggio avanzati e l’approccio di eliminazione graduale degli item possono offrire una visione più accurata e dettagliata della situazione.\n\n71.4.6 Rimozione di item\nSe questa analisi facesse parte della costruzione di un test reale, gli item che mostrano DIF (o altre anomalie nelle analisi successive) dovrebbero essere attentamente esaminati da esperti di contenuto per decidere se modificarli o rimuoverli dal test. Nella discussione seguente, tuttavia, non rimuoveremo gli item perché desideriamo mantenere il set di dati completo. Tuttavia, se si desiderasse rimuovere alcuni item (ovvero colonne) dal set di dati, ciò potrebbe essere fatto con i seguenti comandi.\n\nresponses_removeDIFitems &lt;- \n  responses[, -which(colnames(responses) %in% c(\"I2\", \"I7\"))]\ncolnames(responses_removeDIFitems)\n#&gt;  [1] \"I1\"  \"I3\"  \"I6\"  \"I11\" \"I12\" \"I14\" \"I17\" \"I18\" \"I19\" \"I21\" \"I22\" \"I23\"\n\nDopo aver rimosso degli item, l’intero processo dovrebbe ricominciare da capo, rifacendo il modello di Rasch e indagando sugli item rimanenti.\n\n71.4.7 Test di Martin-Löf\nNella sezione precedente, abbiamo visto che il test del rapporto di verosimiglianza di Andersen (1973) verifica l’ipotesi che i parametri degli item siano invarianti per vari gruppi di persone. Una ipotesi correlata riguarda l’invarianza dei parametri delle persone per diversi gruppi di item.\nQui, la domanda fondamentale è se diversi gruppi di item misurino tratti latenti differenti. Ciò rappresenterebbe una violazione del modello di Rasch, il quale implica un singolo tratto latente alla base di tutti gli item. Se questo tipo di violazione del modello viene rilevato, un modello IRT multidimensionale potrebbe essere più appropriato.\nUn metodo comune per valutare la dimensionalità in generale è l’analisi fattoriale esplorativa. Qui invece descriveremo il test di Martin-Löf che affronta l’ipotesi alternativa secondo cui gruppi di item misurano tratti latenti differenti ed è disponibile nel pacchetto eRm. Come il test del rapporto di verosimiglianza di Andersen, questo test si basa sul confronto di due verosimiglianze condizionate. La prima verosimiglianza condizionata Lu(r,β) è quella del modello di Rasch. La seconda verosimiglianza condizionata Lu(r1, r2, β) è nuovamente quella di un modello più generale che ora permette diversi parametri di persona per specifici gruppi di item. I gruppi di item devono essere definiti prima dell’analisi, il che può essere fatto in base alle loro difficoltà (cioè, testiamo item facili contro difficili) o in base a diverse dimensioni latenti che si sospetta siano misurate dai gruppi di item (cioè, il gruppo di item 1 è sospettato di misurare una dimensione latente diversa rispetto al gruppo di item 2). Se la seconda verosimiglianza è maggiore, ciò indica una violazione del modello di Rasch (analogamente al test del rapporto di verosimiglianza di Andersen).\nIl test di Martin-Löf è spesso descritto come un test per la unidimensionalità. Certi tipi di multidimensionalità possono anche manifestarsi come DIF. Per questa ragione, i test che mirano a rilevare il DIF, possono anche essere sensibili a certe violazioni della unidimensionalità.\n\nmloef_median &lt;- MLoef(rm_sum0, splitcr = \"median\")\nmloef_median\n#&gt; \n#&gt; Martin-Loef-Test (split criterion: median)\n#&gt; LR-value: 67.083 \n#&gt; Chi-square df: 48 \n#&gt; p-value: 0.036\n\nOtteniamo un valore p inferiore a 0.05. Ciò indica che le stime dei parametri delle persone ottenute dagli item facili e difficili differiscono in modo significativo, ovvero, una violazione del modello di Rasch.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_implementation.html#item-e-person-fit",
    "href": "chapters/irt/05_implementation.html#item-e-person-fit",
    "title": "71  Implementazione",
    "section": "\n71.5 Item e Person Fit",
    "text": "71.5 Item e Person Fit\n\n71.5.1 Tests e Statistiche di Bontà di Adattamento\nIn questa sezione esaminiamo una varietà di metodi per valutare l’adattamento dei dati di risposta agli item e al modello di Rasch. Alcuni di questi metodi sono test statistici formali, mentre altri sono statistiche descrittive per le quali sono stati suggeriti nella letteratura dei limiti critici empirici. Vedremo anche che esistono approcci per valutare l’adattamento a livello dell’intero test psicologico, così come approcci focalizzati sulla valutazione dell’adattamento di singoli item o individui.\n\n71.5.2 Test di Bontà di Adattamento χ2 e G2\nNella valutazione del modello di Rasch, esaminiamo due classi principali di test di bontà di adattamento: il test χ2 e il test G2, entrambi noti nell’analisi delle tabelle di contingenza. A differenza dei test del rapporto di verosimiglianza o dei test di Martin-Löf, il test χ2 non confronta l’adattamento relativo di due modelli. Piuttosto, esso valuta quanto accuratamente i modelli di risposta previsti dal modello di Rasch corrispondano ai modelli di risposta osservati. Questo avviene attraverso il confronto tra il numero di partecipanti che mostrano ciascun modello di risposta osservato e il numero previsto dal modello di Rasch.\nIl principio dei test di bontà di adattamento χ2 per il modello di Rasch è basato sull’analisi di tutti i possibili modelli di risposta (combinazioni di 0 e 1 per risposte errate e corrette). Definiamo Ou come il numero osservato di partecipanti con il modello di risposta u e Eu come il numero previsto sotto il modello di Rasch. La statistica del test χ2 è data da:\n\\[ T = \\sum_{u} \\frac{(O_u - E_u)^2}{E_u} \\]\nIn questa formula, le differenze tra osservazioni e previsioni sono elevate al quadrato e poi ponderate inversamente rispetto alla frequenza attesa. In campioni di grandi dimensioni, T segue approssimativamente una distribuzione χ2, se il modello di Rasch è appropriato. Valori alti di T indicano una cattiva adattazione del modello.\nTuttavia, il test χ2 richiede che ogni modello di risposta abbia una frequenza attesa sufficientemente alta, una condizione spesso non soddisfatta in test con molti item. In questi casi, il test χ2 non segue una distribuzione χ2 sotto l’ipotesi nulla, rendendolo poco pratico. Una soluzione potrebbe essere quella di raggruppare i modelli di risposta per aumentare le frequenze attese.\nParallelamente, la statistica del rapporto di verosimiglianza G2, anch’essa derivante dall’analisi dei dati categoriali, è calcolata come:\n\\[ G^2 = 2 \\sum_{u} O_u \\log \\left( \\frac{O_u}{E_u} \\right) \\]\nG2 confronta le frequenze osservate con quelle attese, anziché le verosimiglianze di due modelli. Se le frequenze attese sono vicine a quelle osservate, il rapporto \\(\\frac{O_u}{E_u}\\) si avvicina a 1, rendendo il logaritmo naturale \\(\\log\\left(\\frac{O_u}{E_u}\\right)\\) vicino a 0 e la statistica G2 tende a 0, indicando un buon adattamento. Anche G2 segue una distribuzione χ2 se il modello di Rasch è appropriato. Tuttavia, proprio come per il test χ2, G2 è praticabile solo con grandi frequenze attese, limitandone l’uso effettivo. Nonostante ciò, G2 è importante da comprendere poiché molte altre statistiche di test si basano su di esso.\n\n71.5.3 Statistica M2\nLa statistica M2, sviluppata da Maydeu-Olivares e Joe (2006), affronta il problema dei modelli di risposta rari che possono complicare i test χ2. Invece di confrontare le frequenze di interi modelli di risposta, la statistica M2 utilizza le informazioni provenienti dagli item individuali e dalle coppie di item. Specificatamente, confronta: 1. Le frequenze attese e osservate delle risposte corrette agli item individuali. 2. Le frequenze attese e osservate delle risposte corrette a entrambi gli item in una coppia di item.\nPer esempio, con due item, confronterebbe le frequenze osservate e attese per una risposta corretta al primo item, al secondo item e ad entrambi gli item insieme. Questo approccio è simile all’analisi delle tabelle di frequenza per le coppie di item. La statistica M2, come il test di bontà di adattamento χ2, implica un cattivo adattamento tra i dati e il modello di Rasch se produce un valore elevato o, equivalentemente, un valore p piccolo. Senza violazione del modello, la statistica M2 segue approssimativamente una distribuzione χ2 con gradi di libertà calcolati come $ k - d $, dove $ k $ è il numero di frequenze confrontate e $ d $ è il numero di parametri liberi del modello.\n\n71.5.4 Errore Quadratico Medio di Approssimazione (RMSEA)\nIl RMSEA deriva dalla statistica M2. Utilizza i gradi di libertà (nuovamente $ k - d $) e la dimensione del campione $ P $ per calcolare il valore RMSEA. La formula per il RMSEA è:\n\\[ \\text{RMSEA} = \\sqrt{\\frac{M2 - df}{P \\cdot df}} \\]\nValori di RMSEA vicini a 0 generalmente indicano un buon adattamento del modello ai dati. Sebbene non esistano linee guida universalmente accettate per interpretare il RMSEA, un valore intorno a 0,05 è spesso considerato indicativo di un buon adattamento del modello.\n\n71.5.5 Residuo Quadratico Medio Standardizzato (SRMSR)\nSRMSR è un’altra statistica di adattamento complessivo che confronta le correlazioni o le covarianze osservate tra tutte le coppie di item con quelle previste sotto il modello di Rasch (o un altro modello della teoria della risposta agli item). Valori vicini a 0 suggeriscono un buon adattamento del modello. Maydeu-Olivares (2013) raccomanda l’uso di un valore di soglia di 0.05 per SRMSR, simile al RMSEA.\nNel complesso, queste statistiche (M2, RMSEA e SRMSR) sono utili per valutare l’adattamento di un modello, come il modello di Rasch, a un dato insieme di dati di risposta agli item. Forniscono diverse prospettive attraverso le quali la congruenza tra i dati e il modello teorico può essere valutata, ognuna con il suo focus unico e metodo di calcolo.\n\nfit_rasch &lt;- mirt(responses, 1, itemtype = \"Rasch\", verbose = FALSE)\nfit_rasch\n#&gt; \n#&gt; Call:\n#&gt; mirt(data = responses, model = 1, itemtype = \"Rasch\", verbose = FALSE)\n#&gt; \n#&gt; Full-information item factor analysis with 1 factor(s).\n#&gt; Converged within 0.0001 tolerance after 16 EM iterations.\n#&gt; mirt version: 1.44.0 \n#&gt; M-step optimizer: nlminb \n#&gt; EM acceleration: Ramsay \n#&gt; Number of rectangular quadrature: 61\n#&gt; Latent density type: Gaussian \n#&gt; \n#&gt; Log-likelihood = -2816\n#&gt; Estimated parameters: 15 \n#&gt; AIC = 5663\n#&gt; BIC = 5723; SABIC = 5675\n#&gt; G2 (16368) = 1319, p = 1\n#&gt; RMSEA = 0, CFI = NaN, TLI = NaN\n\n\nM2(fit_rasch)\n#&gt;          M2 df p   RMSEA RMSEA_5 RMSEA_95   SRMSR    TLI    CFI\n#&gt; stats 277.6 90 0 0.07227 0.06262  0.08192 0.09414 0.7495 0.7522\n\nLa statistica M2 è alta e significativa, indicando che ci sono differenze preoccupanti tra il modello e i dati. Questo è ulteriormente supportato da un RMSEA troppo alto e da un CFA e TLI lontani da 1.\nRicordiamo il significato degli indici RMSEA, CFA e TLI.\nRMSEA (Root Mean Square Error of Approximation): - Il RMSEA è una misura di adattamento che valuta quanto bene un modello si adatta ai dati a livello di popolazione. - Un valore basso di RMSEA indica un buon adattamento, suggerendo che il modello approssima bene la realtà. - Generalmente, un RMSEA inferiore a 0.05 o 0.06 è considerato indicativo di un ottimo adattamento del modello.\nCFA (Comparative Fit Index): - Il CFA è un indice relativo di bontà di adattamento che confronta il modello specificato con un modello nullo o di base. - Valori più vicini a 1 indicano un adattamento migliore. Un CFA superiore a 0.90 o 0.95 è spesso considerato indicativo di un buon adattamento.\nTLI (Tucker-Lewis Index): - Simile al CFA, il TLI è un altro indice relativo di adattamento che tiene conto della complessità del modello. - Anche per il TLI, valori più vicini a 1 indicano un adattamento migliore. Valori superiori a 0.90 o 0.95 sono generalmente considerati buoni.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_implementation.html#valutare-ladattamento-degli-item",
    "href": "chapters/irt/05_implementation.html#valutare-ladattamento-degli-item",
    "title": "71  Implementazione",
    "section": "\n71.6 Valutare l’Adattamento degli Item",
    "text": "71.6 Valutare l’Adattamento degli Item\nTuttavia, nell’IRT, ci interessiamo maggiormente agli indici di adattamento degli item e delle persone. L’IRT ci consente di valutare quanto bene ogni item si adatti al modello e se i pattern di risposta individuali sono allineati con il modello.\nIniziamo con l’addattamento agli item. Sono stati proposti diversi indici per valutare l’adattamento degli item e possiamo utilizzare la funzione itemfit() per ottenere una varietà di questi indici. Di default, riceviamo l’S_X2 di Orlando e Thissen (2000) con i corrispondenti gradi di libertà (dfs), RMSEA e valori p. Questo test dovrebbe risultare non significativo per indicare un buon adattamento dell’item. Come vediamo qui sotto, diversi item mostra un cattivo adattamento.\n\nitemfit(fit_rasch)\n#&gt;    item   S_X2 df.S_X2 RMSEA.S_X2 p.S_X2\n#&gt; 1    I1  4.150       7      0.000  0.762\n#&gt; 2    I2 19.334       7      0.066  0.007\n#&gt; 3    I3  5.528       7      0.000  0.596\n#&gt; 4    I6 14.086       8      0.044  0.080\n#&gt; 5    I7  8.977       7      0.027  0.254\n#&gt; 6   I11 23.364       7      0.077  0.001\n#&gt; 7   I12 17.673       7      0.062  0.014\n#&gt; 8   I14  9.790       7      0.032  0.201\n#&gt; 9   I17 35.134       7      0.100  0.000\n#&gt; 10  I18  3.697       7      0.000  0.814\n#&gt; 11  I19 20.932       7      0.071  0.004\n#&gt; 12  I21 86.542       7      0.169  0.000\n#&gt; 13  I22 73.458       7      0.154  0.000\n#&gt; 14  I23  7.612       7      0.015  0.368\n\n\n71.6.1 Statistiche di Infit e Outfit\nNella sezione precedente abbiamo discusso i test χ2 e M2, basati sul confronto tra le frequenze osservate e quelle attese secondo il modello di Rasch. Le statistiche di adattamento presentate di seguito si basano su un approccio simile, utilizzando i residui di Rasch. Questi sono le differenze tra le risposte osservate (vale a dire, le risposte 0 o 1 per gli item dicotomici) e i loro valori attesi (cioè, le probabilità predette di una risposta corretta secondo il modello di Rasch). Tipicamente, questi valori attesi vengono calcolati in base alle stime dei parametri degli item e delle persone.\nGeneralmente, quando c’è un buon adattamento tra i dati e il modello, si può prevedere che i residui siano piccoli. Pertanto, è naturale che i residui di Rasch possano essere utilizzati per valutare l’adattamento del modello di Rasch. Vedremo che nell’analisi di Rasch non solo i casi in cui i residui sono più grandi del previsto possono essere motivo di preoccupazione, ma anche quelli in cui i residui sono più piccoli del previsto.\nUn approccio comune per verificare l’adattamento di singoli item usando i residui di Rasch consiste nel calcolare le statistiche di infit e outfit. Descriveremo i passaggi per calcolare queste statistiche prima di affrontarne l’interpretazione. Ci concentreremo sul caso in cui queste statistiche vengono calcolate per singoli item.\n\n71.6.1.1 Outfit\nLa funzione principale della statistica di outfit è quella di quantificare in che misura le risposte dei partecipanti si allontanano dalle previsioni del modello. Questo indice si calcola attraverso diversi passaggi, che mirano a stabilire la misura in cui le risposte individuali si discostano dalle aspettative teoriche.\n1. Definizione dei Residui di Rasch: Inizialmente, per ogni partecipante e per ciascun item del test, si calcola il residuo di Rasch. Un residuo è essenzialmente la differenza tra la risposta osservata di un individuo a un determinato item e la risposta prevista da quel partecipante per lo stesso item. La risposta prevista è calcolata sulla base della probabilità, fornita dal modello di Rasch, che il partecipante risponda correttamente all’item. Ad esempio, se il modello prevede che un partecipante abbia il 40% di probabilità di rispondere correttamente a un item e il partecipante risponde effettivamente correttamente, il residuo corrispondente sarà $ 1 - 0.40 = 0.60 $.\n2. Standardizzazione dei Residui di Rasch: Successivamente, questi residui vengono standardizzati. La standardizzazione implica l’adeguamento dei residui in modo che abbiano una media di zero e una varianza di uno. Ciò permette di confrontare i residui in maniera uniforme, indipendentemente dalle caratteristiche specifiche degli item o dei partecipanti.\n3. Calcolo dello Z-Score: Per ciascun residuo, si calcola lo z-score standardizzato, $ Z_{si} $, utilizzando la formula:\n\\[\n   Z_{si} = \\frac{X_{si} - E(X_{si})}{\\sqrt{Var(X_{si})}},\n   \\]\ndove $ Z_{si} $ rappresenta lo z-score del residuo per il partecipante $ s $ all’item $ i $, $ X_{si} $ è la risposta osservata, $ E(X_{si}) $ è la risposta attesa (basata sulla probabilità di una risposta corretta secondo il modello di Rasch), e $ Var(X_{si}) $ è la varianza della risposta attesa.\n4. Calcolo della Statistica di Outfit: Per calcolare la statistica di outfit mean square (MSQ) per un specifico item, si seguono questi passaggi: - Si elevano al quadrato gli z-score standardizzati di ogni partecipante per l’item in questione. - Si sommano tutti questi valori quadrati. - Si divide la somma ottenuta per il numero totale dei partecipanti.\nLa formula risultante per la statistica di outfit MSQ per l’item $ i $ è la seguente:\n\\[\n   \\text{Outfit MSQ}_i = \\frac{\\sum_{p=1}^{P} Z_{pi}^2}{P}.\n   \\]\nQuesta procedura fornisce una misura dell’adattamento delle risposte degli individui all’item specifico, rispetto alle previsioni del modello di Rasch. Un valore di MSQ significativamente alto o basso può indicare potenziali discrepanze tra le risposte osservate e quelle previste, suggerendo la necessità di ulteriori analisi o revisioni del modello o degli item del test.\nSecondo Wright e Masters (1990), questa statistica ha un valore atteso di 1 sotto il modello di Rasch. Valori superiori a 1 indicano residui di Rasch più grandi del previsto secondo il modello di Rasch, e quindi una possibile violazione del modello. Tali item vengono anche detti mostrare un underfit. Valori inferiori a 1 indicano che i residui sono inferiori al previsto. Ciò è considerato indicare un overfit delle risposte al modello di Rasch. In questo contesto, overfit significa che la deviazione tra i valori attesi e i dati empirici è minore del previsto.\nPossiamo inoltre ottenere una statistica di mean square pesata e standardizzata per ciascun item, tipicamente denotata da ti. Siano \\(\\sqrt[3]{\\text{MSQ}_i}\\) e sd(MSQ_i) il cubo radice e la deviazione standard attesa di Outfit MSQ_i, rispettivamente. Allora la statistica standardizzata ti è\n\\[\n\\text{Outfit ti} = \\left( \\sqrt[3]{\\text{MSQ}_i} - 1 \\right) \\left( \\frac{3}{\\text{sd(MSQ}_i)} \\right) + \\left( \\frac{\\text{sd(MSQ}_i)}{3} \\right).\n\\]\nQuesta statistica standardizzata ti è spesso presentata nei risultati del software in aggiunta alla statistica MSQ.\nItem che mostrano underfit e overfit possono anche essere identificati approssimativamente usando le loro ICC empiriche, come abbiamo già visto in precedenza. Gli item che mostrano underfit hanno ICC empiriche più piatte di quelle previste sotto il modello di Rasch. Gli item che mostrano overfit hanno ICC empiriche più ripide del previsto.\n\n71.6.1.2 Infit\nL’indice di infit è un altro indice critico nel modello di Rasch. A differenza dell’outfit, che è più influenzato da risposte casuali o outlier, l’infit è più sensibile alle risposte che sono incoerenti con il pattern generale del modello. L’infit è calcolato come una media ponderata dei residui standardizzati, dove i pesi sono inversamente proporzionali alla varianza degli item. Questo rende l’infit particolarmente utile per identificare problemi di adattamento del modello legati alla consistenza interna delle risposte.\nLa statistica di infit MSQ, come quella di outfit, serve a valutare l’adattamento delle risposte individuali rispetto alle aspettative teoriche del modello. Tuttavia, la statistica di infit differisce dall’outfit per il modo in cui tratta i residui.\n1. Ponderazione dei Residui di Rasch: Nella statistica di infit, i residui di Rasch delle risposte individuali vengono ponderati in base alla loro varianza attesa sotto il modello di Rasch. Ciò significa che i residui con varianze minori (che tendono a verificarsi quando c’è una grande distanza tra le abilità dei rispondenti e la difficoltà degli item) hanno un impatto relativamente minore sulla statistica di infit rispetto a quelli con varianze maggiori.\n2. Riduzione dell’Impatto degli Outlier: Questo approccio di ponderazione rende la statistica di infit meno sensibile agli outlier rispetto all’outfit. In altre parole, mentre la statistica di outfit è influenzata in maniera più uniforme da tutte le deviazioni dalle aspettative del modello, l’infit dà maggiore peso alle deviazioni che sono meno estreme o più prevedibili data la struttura del modello.\n3. Formula per la Statistica di Infit MSQ: La formula per calcolare l’Infit MSQ per un dato item $ i $ è la seguente:\n\\[\n   \\text{Infit MSQ}_i = \\frac{\\sum_{p=1}^{P} W_{pi} Z_{pi}^2}{\\sum_{p=1}^{P} W_{pi}},\n   \\]\ndove: - $ Z_{pi} $ rappresenta il residuo di Rasch standardizzato per il rispondente $ p $ all’item $ i $. - $ W_{pi} $ è la varianza attesa del residuo $ Z_{pi} $ sotto il modello di Rasch. - $ P $ è il numero totale dei rispondenti.\n4. Standardizzazione della Statistica Infit: Come per l’outfit, è anche possibile calcolare una versione standardizzata dell’Infit MSQ per ogni item. Questa versione standardizzata, nota come statistica Infit t, consente di confrontare più facilmente l’adattamento degli item in diverse situazioni o in diversi test, normalizzando i valori su una scala comune.\nIn sintesi, la statistica di infit MSQ offre un modo ponderato per valutare l’adattamento delle risposte ai singoli item in un test basato sul modello di Rasch, tenendo conto della varianza attesa delle risposte. Questo la rende particolarmente utile per identificare i casi in cui le risposte si discostano dalle previsioni del modello in modi meno estremi o più in linea con la struttura del modello stesso.\n\n71.6.1.3 Soglie\nPer entrambi i valori MSQ e t delle statistiche di infit e outfit, sono stati proposti vari valori di soglia. Bond e Fox (2007) e Engelhard (2013) menzionano valori di soglia di -2 e 2 per le statistiche t, mentre Paek e Cole (2020) suggeriscono -3 e 3. Analogamente, Bond e Fox (2007) danno 0.75 e 1.3 come valori di soglia per le statistiche MSQ, mentre DeMars (2010) menziona 0.6 e 1.5 come possibili alternative. Desjardins e Bulut (2018), d’altra parte, si oppongono all’uso di valori di soglia specifici per queste statistiche.\nPossiamo calcolare le statistiche infit e oputfit degli item usando il pacchetto eRm:\n\nrm_sum0 &lt;- RM(responses)\neRm::itemfit(person.parameter(rm_sum0))\n#&gt; \n#&gt; Itemfit Statistics: \n#&gt;     Chisq  df p-value Outfit MSQ Infit MSQ Outfit t Infit t Discrim\n#&gt; I1  325.4 397   0.996      0.818     0.904   -1.750  -1.666   0.418\n#&gt; I2  273.8 397   1.000      0.688     0.809   -2.928  -3.271   0.520\n#&gt; I3  289.2 397   1.000      0.727     0.869   -1.574  -1.505   0.371\n#&gt; I6  333.6 397   0.991      0.838     0.860   -2.317  -3.257   0.505\n#&gt; I7  272.9 397   1.000      0.686     0.838   -1.328  -1.423   0.279\n#&gt; I11 332.5 397   0.992      0.836     0.816   -1.432  -3.143   0.473\n#&gt; I12 458.7 397   0.018      1.152     0.972    1.538  -0.538   0.321\n#&gt; I14 395.8 397   0.508      0.994     1.023   -0.043   0.492   0.320\n#&gt; I17 524.1 397   0.000      1.317     0.936    2.290  -1.031   0.280\n#&gt; I18 432.9 397   0.104      1.088     1.019    1.121   0.424   0.314\n#&gt; I19 226.7 397   1.000      0.569     0.750   -2.579  -2.999   0.453\n#&gt; I21 906.0 397   0.000      2.276     1.246    5.846   2.986  -0.108\n#&gt; I22 728.0 397   0.000      1.829     0.985    2.918  -0.105   0.059\n#&gt; I23 275.9 397   1.000      0.693     0.852   -1.918  -1.812   0.412\n\nIn alternativa, è possibile usare la funzione mirt del pacchetto mirt:\n\nmirt_rm &lt;- mirt(responses, 1, \"Rasch\")\n#&gt; \nIteration: 1, Log-Lik: -2816.924, Max-Change: 0.03346\nIteration: 2, Log-Lik: -2816.650, Max-Change: 0.02041\nIteration: 3, Log-Lik: -2816.562, Max-Change: 0.01357\nIteration: 4, Log-Lik: -2816.520, Max-Change: 0.01027\nIteration: 5, Log-Lik: -2816.501, Max-Change: 0.00584\nIteration: 6, Log-Lik: -2816.493, Max-Change: 0.00395\nIteration: 7, Log-Lik: -2816.490, Max-Change: 0.00307\nIteration: 8, Log-Lik: -2816.488, Max-Change: 0.00174\nIteration: 9, Log-Lik: -2816.487, Max-Change: 0.00118\nIteration: 10, Log-Lik: -2816.487, Max-Change: 0.00094\nIteration: 11, Log-Lik: -2816.487, Max-Change: 0.00054\nIteration: 12, Log-Lik: -2816.487, Max-Change: 0.00036\nIteration: 13, Log-Lik: -2816.487, Max-Change: 0.00028\nIteration: 14, Log-Lik: -2816.487, Max-Change: 0.00016\nIteration: 15, Log-Lik: -2816.487, Max-Change: 0.00011\nIteration: 16, Log-Lik: -2816.487, Max-Change: 0.00008\nmirt::itemfit(mirt_rm, fit_stats = \"infit\", method = \"ML\")\n#&gt;    item outfit z.outfit infit z.infit\n#&gt; 1    I1  0.814   -1.683 0.904  -1.665\n#&gt; 2    I2  0.685   -2.793 0.809  -3.271\n#&gt; 3    I3  0.724   -1.510 0.870  -1.493\n#&gt; 4    I6  0.834   -2.272 0.860  -3.258\n#&gt; 5    I7  0.682   -1.357 0.831  -1.504\n#&gt; 6   I11  0.832   -1.381 0.816  -3.143\n#&gt; 7   I12  1.148    1.484 0.971  -0.554\n#&gt; 8   I14  0.990   -0.086 1.023   0.494\n#&gt; 9   I17  1.315    2.276 0.935  -1.040\n#&gt; 10  I18  1.083    1.011 1.019   0.426\n#&gt; 11  I19  0.569   -2.585 0.748  -3.014\n#&gt; 12  I21  2.279    5.852 1.244   2.960\n#&gt; 13  I22  1.823    2.911 0.978  -0.165\n#&gt; 14  I23  0.690   -1.831 0.853  -1.804\n\nLa tabella risultante inizia con le statistiche del test di adattamento χ2 approssimativo, i suoi gradi di libertà e i valori di p risultanti. Se il modello di Rasch è valido, la statistica di test risultante può essere approssimativamente descritta da una distribuzione χ2, il che porta ai valori di p presentati.\nLe colonne seguenti presentano le statistiche MSQ e t di infit e outfit. Per le statistiche MSQ di infit e outfit, valori vicini a 1 indicano un buon adattamento del modello, mentre per le statistiche t di infit e outfit, valori vicini a 0 indicano un buon adattamento. Valori più alti indicano che le risposte sono più casuali di quanto previsto dal modello di Rasch, segnalando un sottoadattamento (underfit); valori più bassi indicano che le risposte sono meno casuali del previsto, segnalando un sovradattamento (overfit).\nSeguendo una delle linee guida proposte, esamineremo ulteriormente quegli item i cui valori t di infit o outfit sono inferiori a -2 o superiori a 2 (ma esistono linee guida alternative). Troviamo che per gli item 2, 6, 11 e 19, almeno un valore t è inferiore a -2, indicando un sovradattamento. Per l’item 19 ciò è supportato dal fatto che la ICC empirica ha una pendenza più ripida rispetto alla ICC attesa.\nPer gli item 17, 21 e 22, invece, almeno un valore t per le statistiche di infit e outfit è superiore a 2, indicando un sottoadattamento. Questo è nuovamente in linea con l’esame delle ICC, dove abbiamo riscontrato che la ICC empirica per l’item 21 ha una pendenza inferiore rispetto alla ICC attesa.\n\nitemfitPlot(mirt_rm)\n\n\n\n\n\n\n\n\n71.6.2 Valutare l’Adattamento delle Persone\nPossiamo generare le stesse misure di adattamento per ogni persona per valutare quanto bene i pattern di risposta di ciascuno si allineano con il modello. Ragioniamo in questo modo: se una persona con un alto valore di \\(\\theta\\) (cioè alta abilità latente) non risponde correttamente a un item facile, questa persona non si adatta bene al modello. Al contrario, se una persona con bassa abilità risponde correttamente a una domanda molto difficile, anche questo non è conforme al modello. Nella pratica, è probabile che ci saranno alcune persone che non si adattano bene al modello. Tuttavia, finché il numero di rispondenti non conformi è basso, la situazione è accettabile. Di solito, ci concentriamo nuovamente sulle statistiche di infit e outfit. Se meno del 5% dei rispondenti presenta valori di infit e outfit superiori o inferiori a 1.96 e -1.96, possiamo considerare il modello adeguato.\nStimiamo gli indici infit e outfit delle persone usando eRm:\n\neRm::personfit(person.parameter(rm_sum0)) \n#&gt; \n#&gt; Personfit Statistics: \n#&gt;       Chisq df p-value Outfit MSQ Infit MSQ Outfit t Infit t\n#&gt; 1    13.191 13   0.433      0.942     1.080     0.07    0.35\n#&gt; 2     7.004 13   0.902      0.500     0.744    -0.44   -0.85\n#&gt; 3     7.564 13   0.871      0.540     0.799    -0.37   -0.62\n#&gt; 4    13.651 13   0.399      0.975     1.112     0.14    0.44\n#&gt; 5     4.024 13   0.991      0.287     0.354    -1.81   -2.20\n#&gt; 6    22.049 13   0.055      1.575     1.304     0.96    1.01\n#&gt; 7    22.823 13   0.044      1.630     1.623     1.02    1.83\n#&gt; 8    12.937 13   0.453      0.924     0.746     0.00   -0.66\n#&gt; 9    71.924 13   0.000      5.137     1.376     2.32    1.11\n#&gt; 10   22.507 13   0.048      1.608     1.065     1.12    0.30\n#&gt; 11    8.244 13   0.827      0.589     0.786    -0.83   -0.52\n#&gt; 12   17.925 13   0.160      1.280     0.916     0.67   -0.10\n#&gt; 13    5.629 13   0.959      0.402     0.667    -0.33   -1.01\n#&gt; 14   18.081 13   0.154      1.291     0.950     0.62   -0.07\n#&gt; 15    9.427 13   0.740      0.673     0.890    -0.50   -0.24\n#&gt; 16   49.152 13   0.000      3.511     1.562     1.53    1.20\n#&gt; 17   16.140 13   0.242      1.153     1.094     0.46    0.37\n#&gt; 18    7.400 13   0.880      0.529     0.785    -0.39   -0.68\n#&gt; 19    4.698 13   0.981      0.336     0.424    -1.70   -1.96\n#&gt; 20    4.698 13   0.981      0.336     0.424    -1.70   -1.96\n#&gt; 21   45.736 13   0.000      3.267     1.221     1.99    0.79\n#&gt; 22    5.441 13   0.964      0.389     0.501    -1.49   -1.60\n#&gt; 23    6.693 13   0.917      0.478     0.812    -0.21   -0.49\n#&gt; 24    9.043 13   0.770      0.646     0.957    -0.19   -0.05\n#&gt; 25   11.936 13   0.533      0.853     1.032    -0.01    0.21\n#&gt; 26    8.349 13   0.820      0.596     0.851    -0.49   -0.42\n#&gt; 27   28.139 13   0.009      2.010     1.853     1.40    2.35\n#&gt; 28    7.921 13   0.849      0.566     0.731    -0.84   -0.66\n#&gt; 29   65.841 13   0.000      4.703     1.677     1.83    1.38\n#&gt; 30    8.297 13   0.824      0.593     0.752    -0.70   -0.71\n#&gt; 31    6.582 13   0.922      0.470     0.660    -0.78   -1.18\n#&gt; 32   11.044 13   0.607      0.789     1.055    -0.12    0.28\n#&gt; 33   10.084 13   0.687      0.720     0.908    -0.39   -0.18\n#&gt; 34   13.555 13   0.406      0.968     1.375     0.25    1.23\n#&gt; 35   23.622 13   0.035      1.687     1.170     1.23    0.60\n#&gt; 36   10.893 13   0.620      0.778     0.863    -0.33   -0.28\n#&gt; 37    6.064 13   0.944      0.433     0.585    -1.15   -1.38\n#&gt; 38   28.190 13   0.009      2.014     1.700     1.77    1.73\n#&gt; 39   13.747 13   0.392      0.982     1.271     0.36    0.85\n#&gt; 40   34.164 13   0.001      2.440     1.211     1.50    0.76\n#&gt; 41   71.924 13   0.000      5.137     1.376     2.32    1.11\n#&gt; 42   10.846 13   0.624      0.775     0.840    -0.31   -0.32\n#&gt; 43    4.024 13   0.991      0.287     0.354    -1.81   -2.20\n#&gt; 44    6.981 13   0.903      0.499     0.661    -1.04   -0.90\n#&gt; 45   68.333 13   0.000      4.881     1.024     2.24    0.18\n#&gt; 46   15.795 13   0.260      1.128     1.141     0.41    0.55\n#&gt; 47   21.711 13   0.060      1.551     1.756     0.86    1.83\n#&gt; 48   13.778 13   0.390      0.984     1.094     0.25    0.37\n#&gt; 49   15.991 13   0.250      1.142     1.334     0.60    0.81\n#&gt; 50   10.989 13   0.612      0.785     1.008    -0.24    0.13\n#&gt; 51   12.656 13   0.475      0.904     1.053    -0.04    0.27\n#&gt; 52    4.166 13   0.989      0.298     0.804     0.40    0.00\n#&gt; 53    7.533 13   0.873      0.538     0.972    -0.17    0.04\n#&gt; 54    5.025 13   0.975      0.359     0.491    -1.08   -1.99\n#&gt; 55   11.323 13   0.584      0.809     1.061    -0.08    0.30\n#&gt; 56   42.848 13   0.000      3.061     1.148     1.39    0.46\n#&gt; 57   30.248 13   0.004      2.161     1.909     1.95    2.13\n#&gt; 58   17.432 13   0.180      1.245     1.137     0.57    0.47\n#&gt; 59    5.671 13   0.957      0.405     0.521    -1.43   -1.51\n#&gt; 60    4.093 13   0.990      0.292     0.390    -1.43   -2.02\n#&gt; 61   16.035 13   0.247      1.145     1.125     0.43    0.44\n#&gt; 62   18.197 13   0.150      1.300     1.095     0.65    0.37\n#&gt; 63   19.944 13   0.097      1.425     1.438     0.90    1.15\n#&gt; 64   37.689 13   0.000      2.692     2.222     1.74    2.65\n#&gt; 65   24.532 13   0.027      1.752     1.618     1.21    1.51\n#&gt; 66   12.174 13   0.513      0.870     0.973     0.09    0.04\n#&gt; 67   19.625 13   0.105      1.402     1.199     0.84    0.68\n#&gt; 68   12.566 13   0.482      0.898     0.877    -0.05   -0.24\n#&gt; 69   18.670 13   0.134      1.334     1.365     0.74    1.10\n#&gt; 70    7.604 13   0.868      0.543     0.755    -0.60   -0.78\n#&gt; 71    9.725 13   0.716      0.695     0.926    -0.50   -0.08\n#&gt; 72    5.025 13   0.975      0.359     0.491    -1.08   -1.99\n#&gt; 73    7.079 13   0.898      0.506     0.637    -1.02   -0.98\n#&gt; 74    7.604 13   0.868      0.543     0.755    -0.60   -0.78\n#&gt; 75    4.219 13   0.989      0.301     0.456    -0.98   -1.79\n#&gt; 76    5.671 13   0.957      0.405     0.521    -1.43   -1.51\n#&gt; 77    9.064 13   0.768      0.647     0.829    -0.38   -0.50\n#&gt; 78    8.268 13   0.826      0.591     0.796    -0.58   -0.46\n#&gt; 79   11.044 13   0.607      0.789     1.055    -0.12    0.28\n#&gt; 80    5.828 13   0.952      0.416     0.549    -1.31   -1.31\n#&gt; 81   10.485 13   0.654      0.749     0.917    -0.24   -0.10\n#&gt; 82   10.894 13   0.620      0.778     1.038    -0.04    0.22\n#&gt; 83   10.090 13   0.687      0.721     0.868    -0.30   -0.24\n#&gt; 84    6.768 13   0.914      0.483     0.619    -1.09   -1.05\n#&gt; 85    4.024 13   0.991      0.287     0.354    -1.81   -2.20\n#&gt; 86    6.471 13   0.927      0.462     0.649    -0.80   -1.22\n#&gt; 87    8.129 13   0.835      0.581     0.875    -0.36   -0.24\n#&gt; 88   12.393 13   0.496      0.885     0.903    -0.06   -0.14\n#&gt; 89    5.671 13   0.957      0.405     0.521    -1.43   -1.51\n#&gt; 90    5.123 13   0.972      0.366     0.513    -1.18   -1.46\n#&gt; 91    5.358 13   0.966      0.383     0.507    -1.32   -1.73\n#&gt; 92    4.093 13   0.990      0.292     0.390    -1.43   -2.02\n#&gt; 93    6.582 13   0.922      0.470     0.660    -0.78   -1.18\n#&gt; 94   12.089 13   0.520      0.864     0.995    -0.08    0.09\n#&gt; 95    5.629 13   0.959      0.402     0.667    -0.33   -1.01\n#&gt; 96   24.693 13   0.025      1.764     1.076     1.33    0.34\n#&gt; 97   15.117 13   0.300      1.080     1.025     0.32    0.19\n#&gt; 98    9.836 13   0.707      0.703     0.918    -0.43   -0.15\n#&gt; 99   10.893 13   0.620      0.778     0.863    -0.33   -0.28\n#&gt; 100   7.268 13   0.888      0.519     0.764    -0.41   -0.76\n#&gt; 101   8.268 13   0.826      0.591     0.796    -0.58   -0.46\n#&gt; 102  13.275 13   0.427      0.948     0.998     0.06    0.11\n#&gt; 103  20.669 13   0.080      1.476     1.181     0.98    0.58\n#&gt; 104  24.874 13   0.024      1.777     1.370     1.40    1.00\n#&gt; 105   6.582 13   0.922      0.470     0.660    -0.78   -1.18\n#&gt; 106   5.025 13   0.975      0.359     0.491    -1.08   -1.99\n#&gt; 107   5.159 13   0.972      0.368     0.485    -1.37   -1.84\n#&gt; 108  22.383 13   0.050      1.599     1.205     1.16    0.64\n#&gt; 109   4.024 13   0.991      0.287     0.354    -1.81   -2.20\n#&gt; 110   4.698 13   0.981      0.336     0.424    -1.70   -1.96\n#&gt; 111   5.025 13   0.975      0.359     0.491    -1.08   -1.99\n#&gt; 112   6.942 13   0.905      0.496     0.675    -0.81   -0.85\n#&gt; 113   5.358 13   0.966      0.383     0.507    -1.32   -1.73\n#&gt; 114   9.883 13   0.703      0.706     0.844    -0.51   -0.34\n#&gt; 115  18.178 13   0.151      1.298     1.534     0.63    1.62\n#&gt; 116   4.347 13   0.987      0.310     0.634    -0.11   -0.77\n#&gt; 117  10.403 13   0.661      0.743     1.095    -0.05    0.41\n#&gt; 118  26.269 13   0.016      1.876     1.226     1.46    0.75\n#&gt; 119   5.828 13   0.952      0.416     0.549    -1.31   -1.31\n#&gt; 120  40.540 13   0.000      2.896     1.718     1.51    1.87\n#&gt; 121   5.441 13   0.964      0.389     0.501    -1.49   -1.60\n#&gt; 122  28.230 13   0.008      2.016     1.666     1.41    1.93\n#&gt; 123   5.638 13   0.958      0.403     0.735    -0.38   -0.69\n#&gt; 124   5.671 13   0.957      0.405     0.521    -1.43   -1.51\n#&gt; 125  38.771 13   0.000      2.769     1.219     1.46    0.72\n#&gt; 126  45.617 13   0.000      3.258     1.202     1.98    0.73\n#&gt; 127 102.531 13   0.000      7.324     1.711     2.35    1.44\n#&gt; 128  40.524 13   0.000      2.895     1.439     1.51    1.26\n#&gt; 129   7.301 13   0.886      0.521     0.779    -0.40   -0.70\n#&gt; 130  26.975 13   0.013      1.927     1.304     1.52    0.95\n#&gt; 131   8.842 13   0.785      0.632     0.892    -0.42   -0.27\n#&gt; 132   9.427 13   0.740      0.673     0.890    -0.50   -0.24\n#&gt; 133  27.405 13   0.011      1.957     1.442     1.35    1.38\n#&gt; 134  19.833 13   0.099      1.417     1.647     0.72    1.92\n#&gt; 135  25.521 13   0.020      1.823     1.546     1.22    1.64\n#&gt; 136  11.116 13   0.601      0.794     1.081    -0.15    0.33\n#&gt; 137  17.812 13   0.165      1.272     1.014     0.67    0.16\n#&gt; 138  16.064 13   0.246      1.147     1.220     0.45    0.67\n#&gt; 139   5.739 13   0.955      0.410     0.644    -0.70   -1.01\n#&gt; 140   5.123 13   0.972      0.366     0.513    -1.18   -1.46\n#&gt; 141   7.079 13   0.898      0.506     0.637    -1.02   -0.98\n#&gt; 142  11.503 13   0.569      0.822     1.160     0.06    0.61\n#&gt; 143  10.950 13   0.615      0.782     0.932    -0.18   -0.06\n#&gt; 144   6.064 13   0.944      0.433     0.585    -1.15   -1.38\n#&gt; 145   4.917 13   0.977      0.351     0.633    -0.47   -1.06\n#&gt; 146   5.284 13   0.968      0.377     0.630    -0.37   -1.16\n#&gt; 147  24.053 13   0.031      1.718     1.470     1.37    1.26\n#&gt; 148  15.117 13   0.300      1.080     1.025     0.32    0.19\n#&gt; 149   5.441 13   0.964      0.389     0.501    -1.49   -1.60\n#&gt; 150   5.159 13   0.972      0.368     0.485    -1.37   -1.84\n#&gt; 151  11.637 13   0.558      0.831     0.940    -0.20   -0.05\n#&gt; 152   8.673 13   0.797      0.619     0.812    -0.44   -0.56\n#&gt; 153   6.263 13   0.936      0.447     0.628    -0.83   -1.32\n#&gt; 154  15.001 13   0.307      1.071     1.263     0.31    0.77\n#&gt; 155  17.014 13   0.199      1.215     1.418     0.55    1.23\n#&gt; 156   7.268 13   0.888      0.519     0.764    -0.41   -0.76\n#&gt; 157  12.366 13   0.498      0.883     1.136    -0.04    0.50\n#&gt; 158   8.924 13   0.779      0.637     0.938    -0.21   -0.11\n#&gt; 159   9.201 13   0.758      0.657     0.810    -0.59   -0.41\n#&gt; 160   5.441 13   0.964      0.389     0.501    -1.49   -1.60\n#&gt; 161  27.205 13   0.012      1.943     1.221     1.16    0.79\n#&gt; 162   5.123 13   0.972      0.366     0.513    -1.18   -1.46\n#&gt; 163  58.610 13   0.000      4.186     1.621     2.44    1.86\n#&gt; 164  10.699 13   0.636      0.764     0.896    -0.16   -0.26\n#&gt; 165  41.122 13   0.000      2.937     1.378     1.53    1.12\n#&gt; 166   5.159 13   0.972      0.368     0.485    -1.37   -1.84\n#&gt; 167  58.082 13   0.000      4.149     1.313     1.56    0.62\n#&gt; 168  17.858 13   0.163      1.276     1.502     0.60    1.54\n#&gt; 169 146.257 13   0.000     10.447     1.338     2.27    0.65\n#&gt; 170  21.252 13   0.068      1.518     1.420     1.00    1.24\n#&gt; 171  26.862 13   0.013      1.919     1.962     1.31    2.59\n#&gt; 172   4.219 13   0.989      0.301     0.456    -0.98   -1.79\n#&gt; 173   9.220 13   0.756      0.659     1.019     0.02    0.17\n#&gt; 174   7.856 13   0.853      0.561     0.817    -0.10   -0.47\n#&gt; 175  11.453 13   0.573      0.818     0.834    -0.21   -0.34\n#&gt; 176   7.466 13   0.877      0.533     0.799    -0.13   -0.53\n#&gt; 177   4.219 13   0.989      0.301     0.456    -0.98   -1.79\n#&gt; 178  10.699 13   0.636      0.764     0.896    -0.16   -0.26\n#&gt; 179  20.347 13   0.087      1.453     1.320     0.91    0.99\n#&gt; 180 123.641 13   0.000      8.832     2.158     3.23    2.72\n#&gt; 181   8.282 13   0.825      0.592     1.068     0.61    0.35\n#&gt; 182   8.856 13   0.784      0.633     0.808    -0.41   -0.58\n#&gt; 183  48.852 13   0.000      3.489     1.539     2.10    1.65\n#&gt; 184   4.093 13   0.990      0.292     0.390    -1.43   -2.02\n#&gt; 185  10.976 13   0.613      0.784     1.153     0.01    0.59\n#&gt; 186   4.219 13   0.989      0.301     0.456    -0.98   -1.79\n#&gt; 187   6.768 13   0.914      0.483     0.619    -1.09   -1.05\n#&gt; 188  46.013 13   0.000      3.287     1.243     2.00    0.86\n#&gt; 189  36.359 13   0.001      2.597     1.751     1.60    2.16\n#&gt; 190  20.042 13   0.094      1.432     1.448     0.88    1.30\n#&gt; 191   5.159 13   0.972      0.368     0.485    -1.37   -1.84\n#&gt; 192  58.333 13   0.000      4.167     1.599     2.43    1.80\n#&gt; 193   7.144 13   0.895      0.510     0.663    -0.92   -1.06\n#&gt; 194   5.671 13   0.957      0.405     0.521    -1.43   -1.51\n#&gt; 195  11.730 13   0.550      0.838     0.857    -0.16   -0.27\n#&gt; 196  13.872 13   0.383      0.991     0.924     0.14   -0.10\n#&gt; 197   9.064 13   0.768      0.647     0.829    -0.38   -0.50\n#&gt; 198   4.281 13   0.988      0.306     0.544    -0.56   -1.41\n#&gt; 199   5.739 13   0.955      0.410     0.644    -0.70   -1.01\n#&gt; 200  10.577 13   0.646      0.755     0.746    -0.35   -0.61\n#&gt; 201  17.316 13   0.185      1.237     1.268     0.60    0.78\n#&gt; 202   6.263 13   0.936      0.447     0.628    -0.83   -1.32\n#&gt; 203   9.807 13   0.710      0.700     0.767    -0.43   -0.66\n#&gt; 204   6.263 13   0.936      0.447     0.628    -0.83   -1.32\n#&gt; 205  10.485 13   0.654      0.749     0.917    -0.24   -0.10\n#&gt; 206   9.169 13   0.760      0.655     1.086    -0.01    0.35\n#&gt; 207  16.656 13   0.215      1.190     1.409     0.52    1.13\n#&gt; 208  11.758 13   0.548      0.840     0.980    -0.07    0.07\n#&gt; 209   5.828 13   0.952      0.416     0.549    -1.31   -1.31\n#&gt; 210  14.935 13   0.311      1.067     1.217     0.35    0.68\n#&gt; 211   5.348 13   0.967      0.382     0.570    -0.68   -1.62\n#&gt; 212   5.441 13   0.964      0.389     0.501    -1.49   -1.60\n#&gt; 213   5.671 13   0.957      0.405     0.521    -1.43   -1.51\n#&gt; 214   5.123 13   0.972      0.366     0.513    -1.18   -1.46\n#&gt; 215   5.638 13   0.958      0.403     0.735    -0.38   -0.69\n#&gt; 216   6.768 13   0.914      0.483     0.619    -1.09   -1.05\n#&gt; 217   5.775 13   0.954      0.413     0.860    -0.01   -0.18\n#&gt; 218  12.704 13   0.471      0.907     0.921    -0.01   -0.09\n#&gt; 219   5.920 13   0.949      0.423     0.747    -0.34   -0.65\n#&gt; 220   4.698 13   0.981      0.336     0.424    -1.70   -1.96\n#&gt; 221  27.073 13   0.012      1.934     1.200     1.15    0.73\n#&gt; 222   7.441 13   0.878      0.532     0.713    -0.86   -0.86\n#&gt; 223   5.629 13   0.959      0.402     0.667    -0.33   -1.01\n#&gt; 224   4.219 13   0.989      0.301     0.456    -0.98   -1.79\n#&gt; 225   9.128 13   0.763      0.652     0.859    -0.44   -0.27\n#&gt; 226  14.019 13   0.372      1.001     1.074     0.17    0.32\n#&gt; 227  10.950 13   0.615      0.782     0.932    -0.18   -0.06\n#&gt; 228  13.778 13   0.390      0.984     1.094     0.25    0.37\n#&gt; 229   5.671 13   0.957      0.405     0.521    -1.43   -1.51\n#&gt; 230   8.261 13   0.826      0.590     0.741    -0.77   -0.63\n#&gt; 231   6.768 13   0.914      0.483     0.619    -1.09   -1.05\n#&gt; 232  10.663 13   0.639      0.762     0.843    -0.37   -0.34\n#&gt; 233  62.458 13   0.000      4.461     2.648     3.13    3.89\n#&gt; 234   8.736 13   0.793      0.624     0.755    -0.50   -0.59\n#&gt; 235   7.856 13   0.853      0.561     0.817    -0.10   -0.47\n#&gt; 236   7.921 13   0.849      0.566     0.731    -0.84   -0.66\n#&gt; 237   7.921 13   0.849      0.566     0.731    -0.84   -0.66\n#&gt; 238   4.374 13   0.987      0.312     0.865     0.40    0.07\n#&gt; 239  18.149 13   0.152      1.296     1.223     0.92    0.53\n#&gt; 240  33.118 13   0.002      2.366     1.889     1.72    2.43\n#&gt; 241   4.024 13   0.991      0.287     0.354    -1.81   -2.20\n#&gt; 242   8.795 13   0.788      0.628     0.782    -0.71   -0.54\n#&gt; 243   7.441 13   0.878      0.532     0.713    -0.86   -0.86\n#&gt; 244   8.232 13   0.828      0.588     0.749    -0.78   -0.60\n#&gt; 245   5.629 13   0.959      0.402     0.667    -0.33   -1.01\n#&gt; 246   4.219 13   0.989      0.301     0.456    -0.98   -1.79\n#&gt; 247  22.642 13   0.046      1.617     0.967     1.00   -0.01\n#&gt; 248  16.010 13   0.249      1.144     1.334     0.44    0.96\n#&gt; 249   4.024 13   0.991      0.287     0.354    -1.81   -2.20\n#&gt; 250  18.843 13   0.128      1.346     1.182     0.65    0.62\n#&gt; 251   4.347 13   0.987      0.310     0.634    -0.11   -0.77\n#&gt; 252   9.201 13   0.758      0.657     0.810    -0.59   -0.41\n#&gt; 253   6.471 13   0.927      0.462     0.649    -0.80   -1.22\n#&gt; 254   9.300 13   0.750      0.664     0.822    -0.62   -0.41\n#&gt; 255   5.671 13   0.957      0.405     0.521    -1.43   -1.51\n#&gt; 256  18.159 13   0.152      1.297     1.362     0.70    0.99\n#&gt; 257  12.841 13   0.460      0.917     1.162     0.07    0.53\n#&gt; 258   5.358 13   0.966      0.383     0.507    -1.32   -1.73\n#&gt; 259   6.914 13   0.907      0.494     0.910     0.11   -0.05\n#&gt; 260   7.500 13   0.875      0.536     0.710    -0.98   -0.78\n#&gt; 261   6.981 13   0.903      0.499     0.661    -1.04   -0.90\n#&gt; 262   4.024 13   0.991      0.287     0.354    -1.81   -2.20\n#&gt; 263   7.784 13   0.857      0.556     1.094     0.58    0.37\n#&gt; 264   8.736 13   0.793      0.624     0.755    -0.50   -0.59\n#&gt; 265   9.302 13   0.750      0.664     1.038    -0.21    0.23\n#&gt; 266   8.484 13   0.811      0.606     0.901    -0.26   -0.24\n#&gt; 267   4.219 13   0.989      0.301     0.456    -0.98   -1.79\n#&gt; 268  20.660 13   0.080      1.476     1.214     0.84    0.76\n#&gt; 269   6.471 13   0.927      0.462     0.649    -0.80   -1.22\n#&gt; 270   8.367 13   0.819      0.598     0.829    -0.27   -0.51\n#&gt; 271  23.034 13   0.041      1.645     1.150     0.95    0.52\n#&gt; 272   8.666 13   0.798      0.619     0.756    -0.51   -0.58\n#&gt; 273   7.851 13   0.853      0.561     0.740    -0.78   -0.76\n#&gt; 274  19.063 13   0.121      1.362     1.210     0.66    0.70\n#&gt; 275   8.232 13   0.828      0.588     0.749    -0.78   -0.60\n#&gt; 276  14.978 13   0.309      1.070     1.236     0.31    0.78\n#&gt; 277  12.381 13   0.497      0.884     1.034    -0.08    0.21\n#&gt; 278   4.917 13   0.977      0.351     0.633    -0.47   -1.06\n#&gt; 279   5.123 13   0.972      0.366     0.513    -1.18   -1.46\n#&gt; 280  11.266 13   0.589      0.805     1.035     0.00    0.22\n#&gt; 281  30.642 13   0.004      2.189     1.642     1.39    1.61\n#&gt; 282  16.130 13   0.242      1.152     1.025     0.45    0.19\n#&gt; 283  10.799 13   0.628      0.771     1.136    -0.01    0.54\n#&gt; 284   5.441 13   0.964      0.389     0.501    -1.49   -1.60\n#&gt; 285  17.337 13   0.184      1.238     1.623     0.57    1.67\n#&gt; 286   5.894 13   0.950      0.421     0.769    -0.35   -0.58\n#&gt; 287  32.260 13   0.002      2.304     1.363     1.77    0.99\n#&gt; 288  23.818 13   0.033      1.701     1.277     1.00    0.82\n#&gt; 289   4.024 13   0.991      0.287     0.354    -1.81   -2.20\n#&gt; 290   9.693 13   0.719      0.692     0.728    -0.50   -0.67\n#&gt; 291  11.568 13   0.563      0.826     0.991    -0.21    0.09\n#&gt; 292  15.005 13   0.307      1.072     1.001     0.32    0.13\n#&gt; 293  12.043 13   0.524      0.860     0.736    -0.14   -0.69\n#&gt; 294   8.465 13   0.812      0.605     0.790    -0.47   -0.64\n#&gt; 296   5.739 13   0.955      0.410     0.644    -0.70   -1.01\n#&gt; 297  15.671 13   0.267      1.119     1.279     0.41    0.83\n#&gt; 298   7.784 13   0.857      0.556     1.094     0.58    0.37\n#&gt; 299  15.524 13   0.276      1.109     1.298     0.38    0.85\n#&gt; 300  32.032 13   0.002      2.288     1.795     2.03    1.84\n#&gt; 301   5.025 13   0.975      0.359     0.491    -1.08   -1.99\n#&gt; 302   5.358 13   0.966      0.383     0.507    -1.32   -1.73\n#&gt; 303  10.577 13   0.646      0.755     0.746    -0.35   -0.61\n#&gt; 304  12.076 13   0.521      0.863     0.975    -0.08    0.03\n#&gt; 305  14.767 13   0.322      1.055     0.934     0.27   -0.07\n#&gt; 306  18.873 13   0.127      1.348     1.290     0.78    0.83\n#&gt; 308  14.151 13   0.363      1.011     1.091     0.28    0.37\n#&gt; 309   5.739 13   0.955      0.410     0.644    -0.70   -1.01\n#&gt; 310  18.159 13   0.152      1.297     1.087     0.72    0.36\n#&gt; 311   5.348 13   0.967      0.382     0.570    -0.68   -1.62\n#&gt; 312  16.035 13   0.247      1.145     1.125     0.43    0.44\n#&gt; 313  23.034 13   0.041      1.645     1.150     0.95    0.52\n#&gt; 314   6.942 13   0.905      0.496     0.675    -0.81   -0.85\n#&gt; 315  11.315 13   0.584      0.808     1.024    -0.23    0.19\n#&gt; 316   7.471 13   0.876      0.534     0.633    -0.99   -1.06\n#&gt; 317  24.891 13   0.024      1.778     1.098     1.34    0.40\n#&gt; 318  18.175 13   0.151      1.298     0.811     0.70   -0.41\n#&gt; 319   6.024 13   0.945      0.430     0.903     0.01   -0.08\n#&gt; 320  13.667 13   0.398      0.976     1.072     0.11    0.32\n#&gt; 321   7.500 13   0.875      0.536     0.710    -0.98   -0.78\n#&gt; 322   8.736 13   0.793      0.624     0.755    -0.50   -0.59\n#&gt; 323  13.747 13   0.392      0.982     1.201     0.18    0.63\n#&gt; 324  28.435 13   0.008      2.031     1.448     1.03    1.04\n#&gt; 325   6.896 13   0.907      0.493     0.768    -0.53   -0.58\n#&gt; 326  15.660 13   0.268      1.119     1.228     0.39    0.69\n#&gt; 327   8.736 13   0.793      0.624     0.755    -0.50   -0.59\n#&gt; 328   4.024 13   0.991      0.287     0.354    -1.81   -2.20\n#&gt; 329   9.693 13   0.719      0.692     0.728    -0.50   -0.67\n#&gt; 330   5.441 13   0.964      0.389     0.501    -1.49   -1.60\n#&gt; 331   4.093 13   0.990      0.292     0.390    -1.43   -2.02\n#&gt; 332   7.441 13   0.878      0.532     0.713    -0.86   -0.86\n#&gt; 333   5.123 13   0.972      0.366     0.513    -1.18   -1.46\n#&gt; 334   8.049 13   0.840      0.575     0.762    -0.74   -0.68\n#&gt; 335  31.383 13   0.003      2.242     1.672     1.97    1.61\n#&gt; 336   4.698 13   0.981      0.336     0.424    -1.70   -1.96\n#&gt; 337  10.034 13   0.691      0.717     0.928    -0.13   -0.08\n#&gt; 338  11.779 13   0.546      0.841     1.183     0.09    0.68\n#&gt; 339  11.266 13   0.589      0.805     1.035     0.00    0.22\n#&gt; 340  30.471 13   0.004      2.176     1.461     1.65    1.20\n#&gt; 341   4.093 13   0.990      0.292     0.390    -1.43   -2.02\n#&gt; 342  17.006 13   0.199      1.215     1.125     0.54    0.45\n#&gt; 343   5.123 13   0.972      0.366     0.513    -1.18   -1.46\n#&gt; 344  12.126 13   0.517      0.866     0.920    -0.10   -0.09\n#&gt; 345  12.257 13   0.507      0.876     0.905     0.10   -0.15\n#&gt; 346  12.937 13   0.453      0.924     0.746     0.00   -0.66\n#&gt; 347  21.895 13   0.057      1.564     1.668     0.81    1.67\n#&gt; 348  13.515 13   0.409      0.965     0.934     0.10   -0.05\n#&gt; 349   5.739 13   0.955      0.410     0.644    -0.70   -1.01\n#&gt; 350  12.704 13   0.471      0.907     0.921    -0.01   -0.09\n#&gt; 351  23.700 13   0.034      1.693     1.714     1.14    1.69\n#&gt; 352   7.471 13   0.876      0.534     0.633    -0.99   -1.06\n#&gt; 353  26.390 13   0.015      1.885     1.588     1.60    1.51\n#&gt; 354  15.315 13   0.288      1.094     1.203     0.35    0.69\n#&gt; 355   4.024 13   0.991      0.287     0.354    -1.81   -2.20\n#&gt; 356   7.144 13   0.895      0.510     0.663    -0.92   -1.06\n#&gt; 357  13.627 13   0.401      0.973     1.200     0.11    0.64\n#&gt; 358   5.441 13   0.964      0.389     0.501    -1.49   -1.60\n#&gt; 359  18.068 13   0.155      1.291     1.289     0.62    0.97\n#&gt; 360  11.936 13   0.533      0.853     1.032    -0.01    0.21\n#&gt; 361   8.297 13   0.824      0.593     0.752    -0.70   -0.71\n#&gt; 362  21.689 13   0.060      1.549     1.504     1.09    1.28\n#&gt; 363   8.673 13   0.797      0.619     0.812    -0.44   -0.56\n#&gt; 364   8.349 13   0.820      0.596     0.851    -0.49   -0.42\n#&gt; 365  17.601 13   0.173      1.257     1.041     0.65    0.23\n#&gt; 366  20.608 13   0.081      1.472     1.003     0.97    0.13\n#&gt; 367   8.282 13   0.825      0.592     1.068     0.61    0.35\n#&gt; 368   4.567 13   0.984      0.326     0.848     0.42    0.06\n#&gt; 369  63.802 13   0.000      4.557     1.160     1.79    0.49\n#&gt; 370  38.911 13   0.000      2.779     1.507     1.46    1.42\n#&gt; 371  20.974 13   0.073      1.498     1.488     0.91    1.25\n#&gt; 372  31.487 13   0.003      2.249     1.150     1.62    0.57\n#&gt; 373   5.159 13   0.972      0.368     0.485    -1.37   -1.84\n#&gt; 374   7.466 13   0.877      0.533     0.799    -0.13   -0.53\n#&gt; 375  26.269 13   0.016      1.876     1.226     1.46    0.75\n#&gt; 376   7.268 13   0.888      0.519     0.764    -0.41   -0.76\n#&gt; 377   8.747 13   0.792      0.625     0.921    -0.23   -0.17\n#&gt; 378   6.389 13   0.931      0.456     0.873     0.07   -0.14\n#&gt; 379  19.958 13   0.096      1.426     1.151     0.79    0.58\n#&gt; 380  11.666 13   0.555      0.833     1.198     0.21    0.66\n#&gt; 381   9.056 13   0.769      0.647     0.959    -0.19   -0.04\n#&gt; 382   6.735 13   0.915      0.481     0.635    -1.01   -1.17\n#&gt; 383   5.671 13   0.957      0.405     0.521    -1.43   -1.51\n#&gt; 384  46.623 13   0.000      3.330     1.814     1.70    2.07\n#&gt; 385  27.073 13   0.012      1.934     1.200     1.15    0.73\n#&gt; 386   9.883 13   0.703      0.706     0.844    -0.51   -0.34\n#&gt; 387   9.512 13   0.733      0.679     0.828    -0.53   -0.36\n#&gt; 388  20.986 13   0.073      1.499     1.349     1.05    1.00\n#&gt; 389   8.214 13   0.829      0.587     0.737    -0.59   -0.64\n#&gt; 390   5.671 13   0.957      0.405     0.521    -1.43   -1.51\n#&gt; 391   7.400 13   0.880      0.529     0.785    -0.39   -0.68\n#&gt; 392   4.024 13   0.991      0.287     0.354    -1.81   -2.20\n#&gt; 393   9.024 13   0.771      0.645     0.802    -0.67   -0.47\n#&gt; 394   8.232 13   0.828      0.588     0.749    -0.78   -0.60\n#&gt; 395  19.666 13   0.104      1.405     1.465     0.90    1.25\n#&gt; 396   8.664 13   0.798      0.619     0.865    -0.24   -0.38\n#&gt; 397   5.358 13   0.966      0.383     0.507    -1.32   -1.73\n#&gt; 398   4.698 13   0.981      0.336     0.424    -1.70   -1.96\n#&gt; 399  14.347 13   0.350      1.025     1.217     0.26    0.77\n#&gt; 400   5.284 13   0.968      0.377     0.630    -0.37   -1.16\n\nCome per le statistiche di infit e outfit per i singoli item, individui con valori di t superiori a 2 mostrano un comportamento di risposta più casuale rispetto a quanto previsto dal modello di Rasch. Questo può indicare, ad esempio, comportamenti di risposta basati su supposizioni o scarsa attenzione. I modelli di risposta che portano a valori di t inferiori a -2 indicano un comportamento di risposta più deterministico rispetto a quello atteso. In questo esempio, le persone identificate con il numero 5 e 43 mostrano questo comportamento.\nOtteniamo le stime infit e outfit per le persone con mirt:\n\nhead(personfit(mirt_rm))\n#&gt;   outfit z.outfit  infit z.infit      Zh\n#&gt; 1 0.9312  0.04185 1.0903  0.3722 -0.1422\n#&gt; 2 0.5103 -0.64959 0.7279 -0.8983  0.8888\n#&gt; 3 0.5477 -0.56604 0.7821 -0.6818  0.7512\n#&gt; 4 0.9963  0.16858 1.1111  0.4302 -0.2316\n#&gt; 5 0.2893 -1.88739 0.3522 -2.2386  1.6538\n#&gt; 6 1.4647  0.88480 1.3150  1.0096 -1.0020\n\n\npersonfit(mirt_rm) %&gt;%\n    summarize(\n        infit.outside = prop.table(table(z.infit &gt; 1.96 | z.infit &lt; -1.96)),\n        outfit.outside = prop.table(table(z.outfit &gt; 1.96 | z.outfit &lt; -1.96))\n    ) # lower row = non-fitting people\n#&gt;   infit.outside outfit.outside\n#&gt; 1        0.9175           0.98\n#&gt; 2        0.0825           0.02\n\n\npersonfitPlot(mirt_rm)\n\n\n\n\n\n\n\nIn conclusione, nel caso dei dati in esame, meno del 5% dei rispondenti mostra valori di outfit che eccedono la soglia di 1.96 o che sono inferiori a -1.96. Invece, l’8% dei rispondenti mostra valori di infit che eccedono la soglia di 1.96 o che sono inferiori a -1.96. Questi risultati suggeriscono che il modello di Rasch non è del tutto coerente con i dati esaminati.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_implementation.html#curva-di-informazione-dellitem",
    "href": "chapters/irt/05_implementation.html#curva-di-informazione-dellitem",
    "title": "71  Implementazione",
    "section": "\n71.7 Curva di Informazione dell’Item",
    "text": "71.7 Curva di Informazione dell’Item\nUn altro modo per valutare la qualità di ciascun item è tramite la creazione delle cosiddette curve di informazione degli item. L’informazione è un concetto statistico che si riferisce alla capacità di un item di stimare con precisione i punteggi su theta. L’informazione a livello di item chiarisce quanto bene ogni item contribuisca alla precisione nella stima dei punteggi, con livelli più elevati di informazione che portano a stime dei punteggi più accurate.\nPer esempio:\n\nUn item con un’elevata informazione sarà molto utile per discriminare tra rispondenti con diversi livelli di abilità latente attorno a un certo punto della scala di theta. Questo significa che l’item fornisce dati affidabili e significativi sulla capacità o conoscenza che si sta misurando.\nAl contrario, un item con bassa informazione non aggiunge molto alla precisione della stima del punteggio. Questo potrebbe accadere se l’item è troppo facile o troppo difficile per la maggior parte dei rispondenti, o se non è strettamente correlato al tratto latente che si sta cercando di misurare.\n\nLa posizione delle Curve Caratteristiche degli Item (ICC) determina le regioni sul tratto latente dove ciascun item fornisce il massimo di informazione. Questo viene illustrato tramite il grafico dell’informazione dell’item.\n\nplotINFO(rm_sum0, type = \"item\", legpos = FALSE)\n\n\n\n\n\n\n\nQui vediamo che alcuni item forniscono maggiori informazioni sui livelli più bassi di \\(\\theta\\), altri a livelli medi di \\(\\theta\\) e altri ancora ai livelli alti di \\(\\theta\\).\n\n71.7.1 Informazione del Test\nIl concetto di “informazione” può essere applicato anche all’intera scala del test. La Test Information Curve (TIC) è una rappresentazione grafica che mostra quanta informazione un test fornisce a diversi livelli di abilità latente (\\(\\theta\\)). L’informazione è una misura della precisione con cui il test stima l’abilità di un individuo\nIn questo caso, osserviamo che la scala è molto efficace nel stimare i punteggi di theta tra -2 e 3, ma presenta una minore precisione nella stima dei punteggi di theta agli estremi. In altre parole, il test fornisce stime accurate per una vasta gamma di abilità medie e leggermente superiori alla media, ma diventa meno affidabile per valutare abilità molto basse o molto elevate.\nQuesta osservazione ha importanti implicazioni pratiche:\n\n\nValutazione Ottimale per la Maggior Parte dei Rispondenti: La scala è particolarmente adatta per valutare rispondenti il cui livello di abilità si trova all’interno dell’intervallo in cui il test è più informativo (-2 a 4).\n\nLimiti nella Valutazione degli Estremi: Per rispondenti con abilità molto al di sotto di -2 o molto al di sopra di 4, il test potrebbe non fornire stime di abilità così precise. Questo significa che per questi individui, il test potrebbe non essere in grado di discriminare efficacemente tra diversi livelli di abilità.\n\nLe curve di informazione del test aiutano a identificare dove il test è più efficace e dove potrebbe aver bisogno di miglioramenti o aggiustamenti, come l’aggiunta di item più difficili o più facili per estendere la sua precisione ai livelli estremi di abilità. Questa analisi consente di ottimizzare il test per una valutazione più accurata su tutta la gamma di abilità latente che si intende misurare.\nIl grafico dell’informazione del test può essere generato utilizzando eRm::plotINFO:\n\neRm::plotINFO(rm_sum0, type = \"test\")\n\n\n\n\n\n\n\nOppure possiamo usare l’output di mirt:\n\nplot(mirt_rm, type = \"info\")",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_implementation.html#errore-standard-di-misurazione-del-test",
    "href": "chapters/irt/05_implementation.html#errore-standard-di-misurazione-del-test",
    "title": "71  Implementazione",
    "section": "\n71.8 Errore Standard di Misurazione del Test",
    "text": "71.8 Errore Standard di Misurazione del Test\nIl Test Standard Error of Measurement (SEM) è una misura della precisione con cui un test stima il livello di abilità latente (\\(\\theta\\)) per una persona. In altre parole, rappresenta l’incertezza associata alla stima di \\(\\theta\\). Il grafico prodotto da plot(raschModel, type = \"SE\") mostra come il SEM varia in funzione del livello di abilità latente (\\(\\theta\\)).\n\nIl SEM è una stima dell’errore standard nella misurazione di \\(\\theta\\).\nÈ inversamente proporzionale alla quantità di informazione fornita dal test a un dato livello di \\(\\theta\\):\n\n\\[\nSEM(\\theta) = \\frac{1}{\\sqrt{\\text{Informazione}(\\theta)}}\n\\]\n\nIl SEM è espresso nella stessa scala di \\(\\theta\\).\nUn valore più basso del SEM implica una stima più precisa di \\(\\theta\\).\nIl SEM non è costante: varia in base al livello di \\(\\theta\\), riflettendo il fatto che il test è più informativo per alcune abilità rispetto ad altre.\n\n\nplot(mirt_rm, type = \"SE\")\n\n\n\n\n\n\n\nNel modello Rasch, il SEM dipende dalla distribuzione dei parametri di difficoltà (\\(b\\)) degli item:\n\nIl SEM minimo si verifica intorno ai valori di \\(\\theta\\) che corrispondono ai parametri di difficoltà (\\(b\\)) degli item.\nUn SEM alto si verifica a valori di \\(\\theta\\) lontani dal range dei parametri di difficoltà (\\(b\\)), poiché il test non discrimina bene a quei livelli di abilità.\n\nIn conclusione, il grafico del SEM ci aiuta a identificare i punti di forza e di debolezza del test in termini di precisione della stima di \\(\\theta\\). Il SEM ci permette di capire in quali range di \\(\\theta\\) il test fornisce stime più affidabili.\nLa funzione testInfoPlot() fornisce il grafico del SEM insieme alla curva di informazione del test:\n\ntestInfoPlot(mirt_rm, adj_factor = 2)\n\n\n\n\n\n\n\nL’informazione del test è maggiore attorno allo zero e, di conseguenza, gli errori standard aumentano allontanandosi dallo zero.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_implementation.html#stima-dei-parametri-delle-persone",
    "href": "chapters/irt/05_implementation.html#stima-dei-parametri-delle-persone",
    "title": "71  Implementazione",
    "section": "\n71.9 Stima dei Parametri delle Persone",
    "text": "71.9 Stima dei Parametri delle Persone\nLa stima dei parametri delle persone ottenuta con il metodo di massima verosimiglianza si ottiene nel modo seguente:\n\ntheta &lt;- eRm::person.parameter(rm_sum0)\ntheta\n#&gt; \n#&gt; Person Parameters:\n#&gt; \n#&gt;  Raw Score Estimate Std.Error\n#&gt;          1 -3.39916    1.0848\n#&gt;          2 -2.52178    0.8318\n#&gt;          3 -1.91598    0.7365\n#&gt;          4 -1.40926    0.6924\n#&gt;          5 -0.94518    0.6731\n#&gt;          6 -0.49650    0.6685\n#&gt;          7 -0.04741    0.6730\n#&gt;          8  0.41178    0.6831\n#&gt;          9  0.88780    0.6977\n#&gt;         10  1.38917    0.7203\n#&gt;         11  1.93510    0.7620\n#&gt;         12  2.57656    0.8513\n#&gt;         13  3.48324    1.0968\n#&gt;         14  4.45486        NA\n\nDa notare che questa tabella non mostra una stima per ogni persona. La stima dell’abilità di una persona dipende unicamente dal numero di item a cui ha risposto correttamente. Questo significa che dobbiamo calcolare una stima dell’abilità per ogni possibile punteggio totale (indicato come “punteggi grezzi” nella tabella) e possiamo assegnare tale stima a ciascuna persona che ottiene quel punteggio. Ad esempio, stimiamo che l’abilità di una persona che risponde correttamente a dieci item sia circa 1.39.\nVediamo che le stime dell’abilità aumentano con il punteggio grezzo. Questo ha senso, poiché un candidato ha maggiori probabilità di rispondere correttamente a un item se la sua abilità supera la difficoltà di quell’item. Più item vengono risposti correttamente, più è probabile che l’abilità del candidato sia elevata. Inoltre, vediamo che l’errore standard aumenta con la distanza da zero, come era prevedibile dalla mappa persona-item o dalle curve di informazione degli item e del test, dove abbiamo visto che la maggior parte degli item si trova intorno allo zero.\nLa mancanza di un errore standard per i candidati che rispondono correttamente a tutti i 14 item potrebbe lasciarci perplessi. La ragione di tale mancanza è che non esiste una stima di massima verosimiglianza per questo punteggio perfetto. Per gestire questo, la funzione person.parameter() utilizza un metodo chiamato interpolazione spline per produrre una stima dell’abilità, ma la procedura non fornisce stime dell’errore. Lo stesso sarebbe vero per i candidati che risolvono correttamente 0 item, ma in questo campione non si è verificato un punteggio di zero.\nPossiamo ottenere informazioni sulle stime dell’abilità dei singoli candidati utilizzando la funzione summary(), cioè,\n\nsummary(theta)\n#&gt; \n#&gt; Estimation of Ability Parameters\n#&gt; \n#&gt; Collapsed log-likelihood: -76.28 \n#&gt; Number of iterations: 10 \n#&gt; Number of parameters: 13 \n#&gt; \n#&gt; ML estimated ability parameters (without spline interpolated values): \n#&gt;           Estimate Std. Err.    2.5 %  97.5 %\n#&gt; theta 1   -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 2   -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 3   -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 4   -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 5    0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 6   -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 7   -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 8   -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 9   -1.91598    0.7365 -3.35958 -0.4724\n#&gt; theta 10  -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 11  -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 12   0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 13  -1.91598    0.7365 -3.35958 -0.4724\n#&gt; theta 14  -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 15  -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 16  -2.52178    0.8318 -4.15206 -0.8915\n#&gt; theta 17  -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 18  -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 19  -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 20  -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 21  -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 22  -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 23  -1.91598    0.7365 -3.35958 -0.4724\n#&gt; theta 24  -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 25  -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 26  -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 27  -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 28   0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 29  -2.52178    0.8318 -4.15206 -0.8915\n#&gt; theta 30  -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 31  -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 32  -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 33  -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 34  -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 35  -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 36  -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 37  -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 38  -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 39  -1.91598    0.7365 -3.35958 -0.4724\n#&gt; theta 40  -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 41  -1.91598    0.7365 -3.35958 -0.4724\n#&gt; theta 42   0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 43   0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 44   0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 45  -1.91598    0.7365 -3.35958 -0.4724\n#&gt; theta 46  -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 47   1.38917    0.7203 -0.02261  2.8010\n#&gt; theta 48   1.38917    0.7203 -0.02261  2.8010\n#&gt; theta 49  -2.52178    0.8318 -4.15206 -0.8915\n#&gt; theta 50  -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 51  -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 52  -3.39916    1.0848 -5.52537 -1.2730\n#&gt; theta 53   1.93510    0.7620  0.44155  3.4287\n#&gt; theta 54  -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 55  -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 56  -2.52178    0.8318 -4.15206 -0.8915\n#&gt; theta 57  -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 58   0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 59  -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 60   0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 61   0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 62   0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 63   0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 64   1.38917    0.7203 -0.02261  2.8010\n#&gt; theta 65   0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 66   1.38917    0.7203 -0.02261  2.8010\n#&gt; theta 67  -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 68  -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 69  -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 70  -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 71   0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 72  -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 73   0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 74  -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 75   1.38917    0.7203 -0.02261  2.8010\n#&gt; theta 76  -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 77  -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 78   0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 79  -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 80   0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 81   0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 82   1.38917    0.7203 -0.02261  2.8010\n#&gt; theta 83   0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 84   0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 85   0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 86  -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 87   1.38917    0.7203 -0.02261  2.8010\n#&gt; theta 88   0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 89  -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 90   0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 91  -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 92   0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 93  -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 94  -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 95  -1.91598    0.7365 -3.35958 -0.4724\n#&gt; theta 96  -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 97  -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 98  -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 99  -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 100 -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 101  0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 102 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 103  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 104  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 105 -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 106 -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 107 -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 108  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 109  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 110 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 111 -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 112  0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 113 -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 114 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 115 -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 116 -2.52178    0.8318 -4.15206 -0.8915\n#&gt; theta 117 -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 118 -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 119  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 120 -1.91598    0.7365 -3.35958 -0.4724\n#&gt; theta 121 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 122 -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 123  1.93510    0.7620  0.44155  3.4287\n#&gt; theta 124 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 125 -1.91598    0.7365 -3.35958 -0.4724\n#&gt; theta 126 -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 127 -2.52178    0.8318 -4.15206 -0.8915\n#&gt; theta 128 -1.91598    0.7365 -3.35958 -0.4724\n#&gt; theta 129 -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 130 -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 131 -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 132 -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 133 -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 134 -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 135 -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 136  0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 137 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 138  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 139  1.38917    0.7203 -0.02261  2.8010\n#&gt; theta 140  0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 141  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 142 -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 143  0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 144 -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 145  1.93510    0.7620  0.44155  3.4287\n#&gt; theta 146 -1.91598    0.7365 -3.35958 -0.4724\n#&gt; theta 147 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 148 -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 149 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 150 -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 151 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 152 -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 153 -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 154  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 155 -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 156 -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 157 -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 158 -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 159  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 160 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 161 -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 162  0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 163 -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 164 -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 165 -1.91598    0.7365 -3.35958 -0.4724\n#&gt; theta 166 -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 167 -3.39916    1.0848 -5.52537 -1.2730\n#&gt; theta 168 -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 169 -3.39916    1.0848 -5.52537 -1.2730\n#&gt; theta 170 -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 171 -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 172  1.38917    0.7203 -0.02261  2.8010\n#&gt; theta 173 -1.91598    0.7365 -3.35958 -0.4724\n#&gt; theta 174 -1.91598    0.7365 -3.35958 -0.4724\n#&gt; theta 175  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 176 -1.91598    0.7365 -3.35958 -0.4724\n#&gt; theta 177  1.38917    0.7203 -0.02261  2.8010\n#&gt; theta 178 -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 179 -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 180 -1.91598    0.7365 -3.35958 -0.4724\n#&gt; theta 181 -3.39916    1.0848 -5.52537 -1.2730\n#&gt; theta 182 -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 183 -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 184  0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 185 -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 186  1.38917    0.7203 -0.02261  2.8010\n#&gt; theta 187  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 188 -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 189 -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 190 -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 191 -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 192 -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 193 -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 194 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 195  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 196 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 197 -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 198  1.93510    0.7620  0.44155  3.4287\n#&gt; theta 199  1.38917    0.7203 -0.02261  2.8010\n#&gt; theta 200  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 201  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 202 -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 203 -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 204 -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 205  0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 206  1.93510    0.7620  0.44155  3.4287\n#&gt; theta 207  1.93510    0.7620  0.44155  3.4287\n#&gt; theta 208  0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 209  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 210  1.38917    0.7203 -0.02261  2.8010\n#&gt; theta 211 -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 212 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 213 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 214  0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 215  1.93510    0.7620  0.44155  3.4287\n#&gt; theta 216  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 217  2.57656    0.8513  0.90805  4.2451\n#&gt; theta 218  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 219  1.93510    0.7620  0.44155  3.4287\n#&gt; theta 220 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 221 -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 222 -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 223 -1.91598    0.7365 -3.35958 -0.4724\n#&gt; theta 224  1.38917    0.7203 -0.02261  2.8010\n#&gt; theta 225  0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 226 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 227  0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 228  1.38917    0.7203 -0.02261  2.8010\n#&gt; theta 229 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 230  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 231  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 232 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 233 -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 234  0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 235 -1.91598    0.7365 -3.35958 -0.4724\n#&gt; theta 236  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 237  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 238  3.48324    1.0968  1.33357  5.6329\n#&gt; theta 239 -3.39916    1.0848 -5.52537 -1.2730\n#&gt; theta 240 -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 241  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 242 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 243 -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 244  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 245 -1.91598    0.7365 -3.35958 -0.4724\n#&gt; theta 246  1.38917    0.7203 -0.02261  2.8010\n#&gt; theta 247 -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 248 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 249  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 250 -1.91598    0.7365 -3.35958 -0.4724\n#&gt; theta 251 -2.52178    0.8318 -4.15206 -0.8915\n#&gt; theta 252  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 253 -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 254 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 255 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 256  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 257  0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 258 -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 259 -2.52178    0.8318 -4.15206 -0.8915\n#&gt; theta 260 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 261  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 262  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 263  3.48324    1.0968  1.33357  5.6329\n#&gt; theta 264  0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 265  1.38917    0.7203 -0.02261  2.8010\n#&gt; theta 266 -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 267  1.38917    0.7203 -0.02261  2.8010\n#&gt; theta 268 -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 269 -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 270 -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 271  1.38917    0.7203 -0.02261  2.8010\n#&gt; theta 272  0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 273 -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 274 -1.91598    0.7365 -3.35958 -0.4724\n#&gt; theta 275  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 276 -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 277 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 278  1.93510    0.7620  0.44155  3.4287\n#&gt; theta 279  0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 280  1.38917    0.7203 -0.02261  2.8010\n#&gt; theta 281  1.38917    0.7203 -0.02261  2.8010\n#&gt; theta 282  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 283 -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 284 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 285 -1.91598    0.7365 -3.35958 -0.4724\n#&gt; theta 286  1.93510    0.7620  0.44155  3.4287\n#&gt; theta 287  0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 288  1.38917    0.7203 -0.02261  2.8010\n#&gt; theta 289  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 290  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 291 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 292  0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 293 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 294 -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 296  1.38917    0.7203 -0.02261  2.8010\n#&gt; theta 297  1.38917    0.7203 -0.02261  2.8010\n#&gt; theta 298  3.48324    1.0968  1.33357  5.6329\n#&gt; theta 299  0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 300  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 301 -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 302 -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 303  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 304 -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 305 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 306  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 308  1.38917    0.7203 -0.02261  2.8010\n#&gt; theta 309  1.38917    0.7203 -0.02261  2.8010\n#&gt; theta 310 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 311 -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 312  0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 313  1.38917    0.7203 -0.02261  2.8010\n#&gt; theta 314  0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 315  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 316 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 317 -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 318  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 319  2.57656    0.8513  0.90805  4.2451\n#&gt; theta 320 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 321 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 322  0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 323  0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 324  2.57656    0.8513  0.90805  4.2451\n#&gt; theta 325  1.38917    0.7203 -0.02261  2.8010\n#&gt; theta 326  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 327  0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 328  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 329  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 330 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 331  0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 332 -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 333  0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 334 -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 335  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 336 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 337  1.38917    0.7203 -0.02261  2.8010\n#&gt; theta 338 -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 339  1.38917    0.7203 -0.02261  2.8010\n#&gt; theta 340  0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 341  0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 342  1.93510    0.7620  0.44155  3.4287\n#&gt; theta 343  0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 344  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 345  1.38917    0.7203 -0.02261  2.8010\n#&gt; theta 346 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 347  1.93510    0.7620  0.44155  3.4287\n#&gt; theta 348  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 349  1.38917    0.7203 -0.02261  2.8010\n#&gt; theta 350  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 351  0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 352 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 353 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 354 -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 355  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 356 -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 357 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 358 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 359 -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 360 -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 361 -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 362  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 363 -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 364 -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 365 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 366  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 367 -3.39916    1.0848 -5.52537 -1.2730\n#&gt; theta 368 -3.39916    1.0848 -5.52537 -1.2730\n#&gt; theta 369 -2.52178    0.8318 -4.15206 -0.8915\n#&gt; theta 370 -1.91598    0.7365 -3.35958 -0.4724\n#&gt; theta 371  0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 372 -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 373 -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 374 -1.91598    0.7365 -3.35958 -0.4724\n#&gt; theta 375 -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 376 -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 377 -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 378 -2.52178    0.8318 -4.15206 -0.8915\n#&gt; theta 379 -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 380 -1.91598    0.7365 -3.35958 -0.4724\n#&gt; theta 381 -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 382 -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 383 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 384 -1.91598    0.7365 -3.35958 -0.4724\n#&gt; theta 385 -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 386 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 387  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 388 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 389  0.88780    0.6977 -0.47964  2.2552\n#&gt; theta 390 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 391 -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 392  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 393 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 394  0.41178    0.6831 -0.92699  1.7506\n#&gt; theta 395 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 396 -1.40926    0.6924 -2.76632 -0.0522\n#&gt; theta 397 -0.49650    0.6685 -1.80668  0.8137\n#&gt; theta 398 -0.04741    0.6730 -1.36640  1.2716\n#&gt; theta 399 -0.94518    0.6731 -2.26446  0.3741\n#&gt; theta 400 -1.91598    0.7365 -3.35958 -0.4724\n\nL’output – che qui abbiamo di nuovo troncato per risparmiare spazio – contiene stime ed errori standard, insieme ai limiti inferiori (2.5%) e superiori (97.5%) degli intervalli di confidenza al 95%, per tutti i candidati che hanno risposto correttamente fino a 13 item. I candidati che hanno risposto correttamente a tutti i 14 item (o a nessun item) sono omessi da questo output. Possiamo anche notare che alcuni candidati, ad esempio il secondo e il terzo, ricevono esattamente la stessa stima di abilità ed errore standard. Questo è dovuto al fatto che hanno lavorato sullo stesso set di item e hanno ottenuto lo stesso punteggio totale. In alternativa, possiamo utilizzare il comando coef(theta) per ottenere solo la stima dell’abilità per ciascun candidato.\nPossiamo anche stimare l’abilità dei candidati utilizzando la funzione mirt::fscores(). Per i modelli unidimensionali, gli argomenti più importanti di fscores() sono object e method. L’argomento object accetta il risultato della funzione mirt(). L’argomento method indica quale metodo utilizzare per stimare i parametri della persona. Per impostazione predefinita, method=\"EAP\", il che indica che il parametro della persona dovrebbe essere stimato utilizzando il metodo expected a posteriori (EAP). Possiamo calcolare le stime EAP per il modello di Rasch e stampare le sue prime sei voci inserendo:\n\ntheta_eap &lt;- fscores(mirt_rm)\nhead(theta_eap)\n#&gt;           F1\n#&gt; [1,] -0.2177\n#&gt; [2,] -0.8276\n#&gt; [3,] -0.8276\n#&gt; [4,] -0.2177\n#&gt; [5,]  0.3914\n#&gt; [6,] -0.5213\n\nPer impostazione predefinita mirt mostra solo le stime puntuali, ma è possibile aggiungere gli errori standard tramite l’opzione full.scores.SE = TRUE alla funzione fscores(). Gli errori standard dovrebbero essere esaminati prima di interpretare o riportare le stime dei parametri della persona.\nLa funzione fscores() fornisce anche stimatori di massima verosimiglianza (ML), massimo a posteriori (MAP) e likelihood ponderata (WLE). Ora confrontiamo i quattro tipi di stime dei parametri della persona fornite da mirt. Gli stimatori ML, MAP e WLE possono essere calcolati inserendo\n\ntheta_ml &lt;- fscores(mirt_rm, method = \"ML\", max_theta = 30)\ntheta_map &lt;- fscores(mirt_rm, method = \"MAP\")\ntheta_wle &lt;- fscores(mirt_rm, method = \"WLE\")\n\n\nests &lt;- cbind(theta_eap, theta_ml, theta_map, theta_wle)\ncolnames(ests) &lt;- c(\"EAP\", \"ML\", \"MAP\", \"WLE\")\npairs(ests, xlim = c(-3, 3), ylim = c(-3, 3))",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_implementation.html#affidabilità-condizionale",
    "href": "chapters/irt/05_implementation.html#affidabilità-condizionale",
    "title": "71  Implementazione",
    "section": "\n71.10 Affidabilità Condizionale",
    "text": "71.10 Affidabilità Condizionale\nIl concetto di affidabilità varia tra la CTT e la IRT. Nell’IRT, possiamo calcolare l’affidabilità condizionale, ossia l’affidabilità della scala a diversi livelli di theta.\n\nNella CTT, l’affidabilità è solitamente considerata come una proprietà fissa del test, indipendentemente dal livello di abilità dei rispondenti. Si misura spesso attraverso il coefficiente alfa di Cronbach o metodi simili.\nNell’IRT, invece, l’affidabilità è vista come una proprietà variabile che dipende dal livello di theta del rispondente. A diversi livelli di theta, la precisione con cui il test misura l’abilità può variare significativamente.\n\nL’affidabilità condizionale fornisce una misura più specifica e dettagliata di quanto affidabilmente un test misura l’abilità a diversi livelli di \\(\\theta\\).\n\nconRelPlot(mirt_rm)\n\n\n\n\n\n\n\n\nplot(mirt_rm, type = \"rxx\")\n\n\n\n\n\n\n\nNel caso presente,\n\na livelli medi di \\(\\theta\\): Il test mostra una buona affidabilità, indicando che è in grado di distinguere con precisione tra rispondenti con abilità medie.\nagli estremi di \\(\\theta\\): Il test mostra un’affidabilità più bassa, suggerendo che non è altrettanto efficace nel distinguere tra livelli di abilità molto alti o molto bassi.\n\nIn sostanza, l’affidabilità condizionale nell’IRT ci fornisce una comprensione più dettagliata di dove il test funziona bene e dove potrebbe richiedere miglioramenti per valutare con precisione l’abilità su tutta la gamma di theta.\nÈ comunque possibile calcolare un singolo valore di attendibilità:\n\nmarginal_rxx(mirt_rm)\n#&gt; [1] 0.6982\n\n\nIl valore riportato (\\(r_{xx} = 0.698\\)) indica che circa il 70% della varianza osservata nei punteggi stimati è attribuibile al punteggio vero (\\(\\theta\\)), mentre il restante 30% è dovuto all’errore di misura. Questo valore suggerisce che il test stima l’abilità latente \\(\\theta\\) con una precisione accettabile.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_implementation.html#curva-caratteristica-del-test",
    "href": "chapters/irt/05_implementation.html#curva-caratteristica-del-test",
    "title": "71  Implementazione",
    "section": "\n71.11 Curva Caratteristica del Test",
    "text": "71.11 Curva Caratteristica del Test\nUna proprietà aggiuntiva di un modello IRT è che il punteggio complessivo delle risposte corrette (la somma dei punteggi per le risposte corrette) risulta essere una stima efficace del tratto latente sottostante. Un grafico della cosiddetta curva caratteristica della scala (scale characteristic curve) permette di valutare visivamente questo aspetto tracciando la relazione tra theta e il punteggio di risposte corrette.\n\nQuesto tipo di grafico mostra come il punteggio totale delle risposte corrette si correla con il livello di abilità latente (theta) stimato dal modello IRT.\nAd esempio, se la curva mostra che punteggi più alti di risposte corrette corrispondono sistematicamente a livelli più alti di theta e viceversa, ciò indica che il punteggio totale è un buon indicatore del tratto latente.\nAl contrario, se la curva non mostra una relazione chiara o lineare tra punteggio totale e theta, ciò potrebbe suggerire che il punteggio totale non cattura completamente la complessità o le sfumature del tratto latente.\n\nIn sintesi, la curva caratteristica della scala fornisce una rappresentazione visiva di come il punteggio totale di risposte corrette rifletta l’abilità latente misurata dal test, offrendo una visione utile per valutare l’efficacia del punteggio totale come indicatore del tratto latente in questione.\n\nscaleCharPlot(mirt_rm)\n\n\n\n\n\n\n\n\nplot(mirt_rm, type = \"score\")\n\n\n\n\n\n\n\nQuesta curva di solito assume la forma di una S, poiché la relazione è più forte nel range medio di theta e meno precisa agli estremi (come già visto nella curva di informazione del test).\nPossiamo ovviamente testare anche questo con una semplice correlazione. Per prima cosa, estraiamo il punteggio latente IRT utilizzando la funzione fscores(). Quindi lo correliamo con il punteggio di risposte corrette.\n\nscore &lt;- fscores(mirt_rm)\nsumscore &lt;- rowSums(responses)\ncor.test(score, sumscore)\n#&gt; \n#&gt;  Pearson's product-moment correlation\n#&gt; \n#&gt; data:  score and sumscore\n#&gt; t = 1097, df = 398, p-value &lt;2e-16\n#&gt; alternative hypothesis: true correlation is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  0.9998 0.9999\n#&gt; sample estimates:\n#&gt;    cor \n#&gt; 0.9998\n\nNel caso presente, la correlazione è quasi perfetta.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_implementation.html#riflessioni-conclusive",
    "href": "chapters/irt/05_implementation.html#riflessioni-conclusive",
    "title": "71  Implementazione",
    "section": "\n71.12 Riflessioni Conclusive",
    "text": "71.12 Riflessioni Conclusive\nTradizionalmente, il punteggio totale ottenuto in un test psicologico è stato considerato come la misura più efficace dell’abilità o della predisposizione di una persona rispetto a un certo tratto di personalità. Tuttavia, la dipendenza del punteggio totale dalla difficoltà degli item presenta limitazioni significative. Ad esempio, due persone possono ottenere lo stesso punteggio totale rispondendo in modo diverso a item di varia difficoltà, il che non riflette accuratamente le loro abilità reali.\nNella Teoria Classica dei Test (CTT), l’enfasi è posta sul punteggio totale, ma questa prospettiva ignora le variazioni nella difficoltà degli item e assume che gli errori di misurazione si annullino reciprocamente attraverso la procedura di sommazione. Tuttavia, la CTT è limitata dalla sua assunzione di varianze di errore uniformi per tutti i rispondenti, dall’aspettativa di errori di misurazione nulli e dalla focalizzazione esclusiva sui punteggi totali, senza considerare l’adattamento di item e persone.\nAl contrario, la Teoria della Risposta all’Item (IRT) cambia il focus dai punteggi totali alle risposte a ciascun item, sfruttando le caratteristiche degli item. L’IRT descrive come attributi come abilità, atteggiamento o personalità, insieme alle caratteristiche degli item, influenzino la probabilità di fornire una risposta. Il Modello di Rasch, una forma semplice di IRT per risposte binarie, stabilisce una relazione diretta tra la probabilità di una risposta corretta e il livello di abilità del rispondente.\nLa stima dell’abilità in IRT non dipende dagli specifici item somministrati, permettendo di confrontare i risultati tra gruppi diversi con lo stesso set di item. Inoltre, la qualità degli item è valutata indipendentemente dal campione di rispondenti, rendendo le proprietà degli item costanti tra diversi gruppi con varie abilità.\nL’IRT supera i limiti della CTT stimando congiuntamente le proprietà degli item e il livello di abilità dei rispondenti. Le caratteristiche degli item diventano indipendenti dal campione di individui utilizzato per costruire il test, permettendo la creazione di insiemi di item equivalenti per misurare abilità latenti. Questo approccio offre maggiore precisione e affidabilità nelle misurazioni, assicurando la comparabilità tra diversi gruppi di individui. In conclusione, l’IRT rappresenta un metodo statistico avanzato e versatile per una valutazione più accurata e affidabile di tratti e abilità in contesti psicometrici.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_implementation.html#session-info",
    "href": "chapters/irt/05_implementation.html#session-info",
    "title": "71  Implementazione",
    "section": "\n71.13 Session Info",
    "text": "71.13 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.2\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] grid      stats4    stats     graphics  grDevices utils     datasets \n#&gt; [8] methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] psychotools_0.7-4 ggmirt_0.1.0      TAM_4.2-21        CDM_8.2-6        \n#&gt;  [5] mvtnorm_1.3-3     mirt_1.44.0       lattice_0.22-6    eRm_1.0-6        \n#&gt;  [9] ggokabeito_0.1.0  see_0.11.0        MASS_7.3-65       viridis_0.6.5    \n#&gt; [13] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n#&gt; [17] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n#&gt; [21] lavaan_0.6-19     psych_2.4.12      scales_1.3.0      markdown_1.13    \n#&gt; [25] knitr_1.50        lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n#&gt; [29] dplyr_1.1.4       purrr_1.0.4       readr_2.1.5       tidyr_1.3.1      \n#&gt; [33] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] splines_4.4.2        later_1.4.1          R.oo_1.27.0         \n#&gt;   [4] XML_3.99-0.18        rpart_4.1.24         lifecycle_1.0.4     \n#&gt;   [7] Rdpack_2.6.3         rstatix_0.7.2        rprojroot_2.0.4     \n#&gt;  [10] globals_0.16.3       rockchalk_1.8.157    backports_1.5.0     \n#&gt;  [13] magrittr_2.0.3       openxlsx_4.2.8       Hmisc_5.2-3         \n#&gt;  [16] rmarkdown_2.29       yaml_2.3.10          httpuv_1.6.15       \n#&gt;  [19] qgraph_1.9.8         zip_2.3.2            sessioninfo_1.2.3   \n#&gt;  [22] cowplot_1.1.3        pbapply_1.7-2        minqa_1.2.8         \n#&gt;  [25] multcomp_1.4-28      abind_1.4-8          audio_0.1-11        \n#&gt;  [28] quadprog_1.5-8       R.utils_2.13.0       nnet_7.3-20         \n#&gt;  [31] TH.data_1.1-3        sandwich_3.1-1       listenv_0.9.1       \n#&gt;  [34] testthat_3.2.3       vegan_2.6-10         arm_1.14-4          \n#&gt;  [37] parallelly_1.42.0    permute_0.9-7        codetools_0.2-20    \n#&gt;  [40] tidyselect_1.2.1     farver_2.1.2         lme4_1.1-36         \n#&gt;  [43] base64enc_0.1-3      jsonlite_1.9.1       polycor_0.8-1       \n#&gt;  [46] progressr_0.15.1     Formula_1.2-5        survival_3.8-3      \n#&gt;  [49] emmeans_1.10.7       tools_4.4.2          Rcpp_1.0.14         \n#&gt;  [52] glue_1.8.0           mnormt_2.1.1         admisc_0.37         \n#&gt;  [55] xfun_0.51            mgcv_1.9-1           withr_3.0.2         \n#&gt;  [58] beepr_2.0            fastmap_1.2.0        boot_1.3-31         \n#&gt;  [61] digest_0.6.37        mi_1.1               timechange_0.3.0    \n#&gt;  [64] R6_2.6.1             mime_0.13            estimability_1.5.1  \n#&gt;  [67] colorspace_2.1-1     gtools_3.9.5         jpeg_0.1-10         \n#&gt;  [70] R.methodsS3_1.8.2    generics_0.1.3       data.table_1.17.0   \n#&gt;  [73] corpcor_1.6.10       SimDesign_2.19.1     htmlwidgets_1.6.4   \n#&gt;  [76] pkgconfig_2.0.3      sem_3.1-16           gtable_0.3.6        \n#&gt;  [79] brio_1.1.5           htmltools_0.5.8.1    carData_3.0-5       \n#&gt;  [82] png_0.1-8            reformulas_0.4.0     rstudioapi_0.17.1   \n#&gt;  [85] tzdb_0.5.0           reshape2_1.4.4       coda_0.19-4.1       \n#&gt;  [88] checkmate_2.3.2      nlme_3.1-167         nloptr_2.2.1        \n#&gt;  [91] zoo_1.8-13           parallel_4.4.2       miniUI_0.1.1.1      \n#&gt;  [94] foreign_0.8-88       pillar_1.10.1        vctrs_0.6.5         \n#&gt;  [97] promises_1.3.2       car_3.1-3            OpenMx_2.21.13      \n#&gt; [100] xtable_1.8-4         Deriv_4.1.6          cluster_2.1.8.1     \n#&gt; [103] dcurver_0.9.2        GPArotation_2024.3-1 htmlTable_2.4.3     \n#&gt; [106] evaluate_1.0.3       pbivnorm_0.6.0       cli_3.6.4           \n#&gt; [109] kutils_1.73          compiler_4.4.2       rlang_1.1.5         \n#&gt; [112] future.apply_1.11.3  ggsignif_0.6.4       labeling_0.4.3      \n#&gt; [115] fdrtool_1.2.18       plyr_1.8.9           stringi_1.8.4       \n#&gt; [118] munsell_0.5.1        lisrelToR_0.3        pacman_0.5.1        \n#&gt; [121] Matrix_1.7-3         hms_1.1.3            glasso_1.11         \n#&gt; [124] future_1.34.0        shiny_1.10.0         rbibutils_2.3       \n#&gt; [127] igraph_2.1.4         broom_1.0.7          RcppParallel_5.1.10\n\n\n\n\n\nDebelak, R., Strobl, C., & Zeigenfuse, M. D. (2022). An introduction to the rasch model with examples in r. CRC Press.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/01_lgm_intro.html",
    "href": "chapters/lgm/01_lgm_intro.html",
    "title": "72  Curve di crescita latente",
    "section": "",
    "text": "72.1 Introduzione\nQuando si vuole studiare come le persone cambiano nel tempo, è necessario raccogliere dati longitudinali, ovvero misurazioni ripetute sullo stesso gruppo di individui. A differenza delle analisi tradizionali che confrontano le persone in un unico momento, i dati longitudinali permettono di tracciare le traiettorie individuali di cambiamento.\nLe caratteristiche distintive di questi dati, come la presenza di variazione sia tra gli individui sia all’interno degli stessi e la dipendenza tra le osservazioni ripetute, richiedono l’utilizzo di modelli statistici specifici.\nI modelli a curve di crescita latente (LGCM) sono stati sviluppati proprio per affrontare queste complessità. Questi modelli consentono di modellare la crescita e il cambiamento nel tempo, tenendo conto sia delle differenze individuali nelle traiettorie di sviluppo sia degli effetti di variabili esterne. Ad esempio, in uno studio sullo sviluppo cognitivo, un LGCM può essere utilizzato per analizzare come il quoziente intellettivo cambia dall’infanzia all’età adulta, tenendo conto di fattori come l’ambiente familiare e l’istruzione.\nUno dei principali vantaggi degli LGCM è la loro flessibilità. Possono essere applicati a una vasta gamma di dati e possono essere utilizzati per rispondere a diverse domande di ricerca. Inoltre, gli LGCM permettono di identificare i periodi di vita in cui il cambiamento è più rapido o più lento, e di valutare l’impatto di interventi specifici sulle traiettorie di sviluppo.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>72</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/01_lgm_intro.html#concettualizzazioni-del-tempo-in-studi-longitudinali",
    "href": "chapters/lgm/01_lgm_intro.html#concettualizzazioni-del-tempo-in-studi-longitudinali",
    "title": "72  Curve di crescita latente",
    "section": "72.2 Concettualizzazioni del Tempo in Studi Longitudinali",
    "text": "72.2 Concettualizzazioni del Tempo in Studi Longitudinali\nNegli studi longitudinali, ci sono principalmente cinque modalità di concettualizzare e analizzare il trascorrere del tempo:\n\nDisegno Trasversale (Cross-Sectional Design):\n\nQuesto metodo studia gruppi di persone di età diverse, ma in un unico momento temporale, senza misurazioni ripetute sugli stessi individui.\nAd esempio, si potrebbe confrontare le capacità cognitive di bambini di 6, 8 e 10 anni in un unico momento.\nQuesto approccio permette di ottenere dati preliminari e valutare le relazioni tra variabili, ma non può descrivere i processi evolutivi nel tempo, poiché le differenze tra gruppi di età potrebbero dipendere sia da fattori di sviluppo che da differenze tra coorti.\n\nDisegno Longitudinale di Singola Coorte:\n\nPrevede misurazioni ripetute sugli stessi individui nel tempo.\nAd esempio, si potrebbero valutare le stesse persone a 6, 8 e 10 anni per studiare il loro sviluppo cognitivo.\nQuesto permette di analizzare i cambiamenti intra-individuali nel tempo utilizzando modelli di panel, modelli di curva di crescita o altri modelli di cambiamento.\nI modelli di panel esaminano variazioni in sequenze di misurazioni, mentre i modelli di curva di crescita analizzano la variabilità nel cambiamento individuale.\n\nDisegno Cross-Sequenziale:\n\nCombina un disegno trasversale iniziale con una successione di misurazioni longitudinali.\nAd esempio, si potrebbero valutare gruppi di bambini di 6, 8 e 10 anni, e poi seguirli nel tempo con misurazioni successive.\nQuesto approccio permette di studiare sia gli effetti legati all’età che le differenze tra coorti, anche se può essere più complesso separare questi fattori.\n\nDisegno Sequenziale di Coorte:\n\nAvvia uno studio longitudinale con gruppi (coorti) di partecipanti della stessa età.\nOgni nuova coorte attraversa la stessa fascia di età nel tempo.\nAd esempio, si potrebbero valutare gruppi di bambini di 6 anni, 8 anni e 10 anni, seguendoli negli anni successivi.\nQuesto design aiuta a distinguere gli effetti legati all’età da quelli dovuti alle differenze tra coorti.\n\nDisegno Sequenziale Temporale:\n\nMeno comune, ma utile per separare gli effetti legati all’età da quelli legati al tempo di misurazione.\nMantiene invariata la fascia di età dei partecipanti, ma valuta nuove e vecchie coorti in diversi momenti temporali.\nAd esempio, si potrebbero valutare gruppi di bambini di 6-8 anni in diversi anni, per distinguere i cambiamenti legati all’età da quelli legati al passare del tempo.\nQuesto disegno però non permette di separare gli effetti di coorte dall’interazione tra età e tempo di misurazione.\n\n\nIn sintesi, ciascuno di questi approcci offre vantaggi e svantaggi nel comprendere l’impatto dell’età, delle coorti e del tempo di misurazione sui fenomeni di interesse negli studi longitudinali. La scelta del design dipende dagli obiettivi specifici della ricerca.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>72</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/01_lgm_intro.html#validità",
    "href": "chapters/lgm/01_lgm_intro.html#validità",
    "title": "72  Curve di crescita latente",
    "section": "72.3 Validità",
    "text": "72.3 Validità\nRischi per la validità includono la regressione verso la media, gli effetti del retest, effetti di selezione, attrito selettivo e effetti di strumentazione.\n\nLa regressione verso la media indica che i punteggi estremi tendono a spostarsi verso la media nelle misurazioni successive. È un fenomeno di inaffidabilità nelle misure ripetute e può essere mitigato utilizzando modelli SEM a variabili latenti.\nGli effetti del retest emergono quando una misura è sensibile all’esposizione ripetuta. Questi effetti possono essere stimati e corretti assegnando casualmente ai partecipanti la ricezione o meno di una misurazione, o utilizzando protocolli di mancata risposta pianificata.\nGli effetti di selezione si verificano quando il piano di campionamento non fornisce un campione rappresentativo della popolazione di interesse. L’attrito selettivo si riferisce alla perdita di partecipanti correlata a specifiche caratteristiche del campione.\nGli effetti di strumentazione possono alterare le proprietà di misurazione del fenomeno studiato. Misure sensibili al cambiamento sono cruciali per rilevare i processi di cambiamento.\n\nIn sintesi, gli studi longitudinali affrontano diverse sfide di validità, che richiedono metodi sofisticati per la misurazione e l’analisi dei dati. È fondamentale considerare come varie forze, come la regressione verso la media e gli effetti di retest, possano influenzare i risultati, e come strumenti di misurazione adeguati possano catturare in modo efficace i cambiamenti nel tempo.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>72</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/01_lgm_intro.html#dati-mancanti",
    "href": "chapters/lgm/01_lgm_intro.html#dati-mancanti",
    "title": "72  Curve di crescita latente",
    "section": "72.4 Dati Mancanti",
    "text": "72.4 Dati Mancanti\nIl problema dei dati mancanti nei disegni longitudinali rappresenta una sfida cruciale nella ricerca, specialmente nei campi della psicologia e delle scienze sociali. I dati mancanti portano a due conseguenze principali: la perdita di potenza statistica e l’introduzione di bias.\nLa perdita di potenza si verifica perché la riduzione dei dati comporta una minore capacità di rilevare effetti reali. Nei disegni longitudinali, questa perdita è particolarmente critica, poiché le misure ripetute nel tempo sono essenziali per comprendere le dinamiche e i cambiamenti. Una riduzione nel numero di osservazioni può rendere difficile individuare tendenze significative o effetti degli interventi.\nIl bias si introduce quando i dati mancanti non sono distribuiti casualmente. Se la mancanza di dati è legata a caratteristiche specifiche dei soggetti, i risultati possono non essere più rappresentativi della popolazione originale, portando a conclusioni errate o fuorvianti.\nI metodi tradizionali per gestire i dati mancanti, come l’eliminazione dei casi o l’utilizzo dell’ultimo punto disponibile, possono peggiorare la situazione, aumentando sia la perdita di potenza sia il bias. Al contrario, le tecniche moderne mirano a preservare il più possibile la potenza del dataset originale e, se usate correttamente, possono ridurre il bias selettivo.\nTra le tecniche moderne per la gestione dei dati mancanti troviamo l’imputazione multipla, che crea più set completi di dati imputando i valori mancanti in modo da riflettere l’incertezza associata a tali valori. L’uso di variabili ausiliarie appropriate nel modello di analisi e nel processo di imputazione contribuisce a minimizzare il bias.\nLe variabili ausiliarie sono fondamentali: se scelte correttamente, possono spiegare il meccanismo dei dati mancanti e ridurre il bias. Se assenti o selezionate in modo inappropriato, i risultati dell’analisi possono rimanere distorti e le conclusioni dello studio risultare compromesse.\nUn approccio basato sui dati, come l’uso dell’imputazione multipla con MICE (Multiple Imputation by Chained Equations) o missForest, e l’inclusione di tutte le variabili disponibili (comprese informazioni potenzialmente non lineari), permette di rappresentare al meglio il meccanismo di mancanza dei dati. Questo approccio presuppone generalmente che le relazioni tra variabili siano lineari, ma consente anche di includere informazioni non lineari rilevanti. Successivamente, quando viene selezionato un sottoinsieme di variabili per l’analisi, l’effetto della gestione dei dati mancanti è mantenuto, aumentando la generalizzabilità delle analisi considerate le variabili incluse nel protocollo.\nIn sintesi, la gestione dei dati mancanti nei disegni longitudinali richiede un’attenta considerazione del meccanismo di mancanza e l’applicazione di tecniche moderne che possano mitigare la perdita di potenza e il bias. Questo è essenziale per garantire l’affidabilità e la validità dei risultati della ricerca.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>72</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/01_lgm_intro.html#domande-della-ricerca",
    "href": "chapters/lgm/01_lgm_intro.html#domande-della-ricerca",
    "title": "72  Curve di crescita latente",
    "section": "72.5 Domande della ricerca",
    "text": "72.5 Domande della ricerca\nL’analisi di dati longitudinali nella ricerca psicologica è particolarmente interessata a valutare quanto le persone cambino in un particolare aspetto o tratto nel tempo (cioè la crescita media), quanto questa crescita varia rispetto agli altri individui (cioè la varianza della crescita), e come le persone cambiano rispetto a se stesse nel tempo (cioè i modelli di crescita intra-individuale). I modelli di crescita media (LGM) descrivono i primi due di questi elementi chiave, mentre i modelli di punteggio di cambiamento latente (LCSM) descrivono il terzo (come i valori precedenti prevedono i valori successivi nel tempo all’interno della stessa persona).\nIn ambito di ricerca psicologica, l’analisi di dati longitudinali si concentra sullo studio di come le persone cambiano in un particolare tratto o aspetto nel tempo, sulla variazione di questo cambiamento rispetto ad altre persone e su come le persone cambiano rispetto a se stesse nel tempo. I modelli di crescita media (LGM) e quelli di punteggio di cambiamento latente (LCSM) vengono utilizzati per descrivere questi aspetti.\nGrimm et al. (2016) identificano cinque motivi principali per cui questi modelli vengono utilizzati.\n\nIn primo luogo, l’analisi longitudinale consente di identificare direttamente il cambiamento e la stabilità intra-individuale. Ciò significa che è possibile valutare in che modo specifici attributi dell’individuo cambiano o rimangono gli stessi nel tempo, attraverso la misurazione ripetuta della stessa persona.\nIn secondo luogo, l’analisi longitudinale consente di identificare le differenze interindividuali nel cambiamento intra-individuale, ovvero se diversi individui cambiano in modi diversi, in quantità o direzioni diverse o se passano da uno stadio all’altro in momenti diversi.\nIn terzo luogo, l’analisi longitudinale consente di analizzare le interrelazioni nel cambiamento comportamentale, ovvero come i cambiamenti in una variabile influenzino i cambiamenti in un’altra variabile.\nIn quarto luogo, l’analisi longitudinale consente di analizzare le cause del cambiamento intra-individuale, ovvero di identificare i fattori e/o i meccanismi variabili nel tempo che influenzano i cambiamenti intra-individuali.\nInfine, in quinto luogo, l’analisi longitudinale consente di analizzare le cause delle differenze interindividuali nel cambiamento intra-individuale, ovvero di identificare le variabili invarianti nel tempo che sono correlate a specifici aspetti del cambiamento all’interno della persona, come le caratteristiche demografiche, gli interventi sperimentali e le caratteristiche dei contesti degli individui.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>72</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/01_lgm_intro.html#riflessioni-conclusive",
    "href": "chapters/lgm/01_lgm_intro.html#riflessioni-conclusive",
    "title": "72  Curve di crescita latente",
    "section": "72.6 Riflessioni Conclusive",
    "text": "72.6 Riflessioni Conclusive\nQuesto capitolo ha introdotto le peculiarità dei dati longitudinali e l’approccio necessario per studiare il cambiamento umano nel tempo. L’uso di disegni e modelli specifici, dalle curve di crescita latente alle tecniche di imputazione per i dati mancanti, è cruciale per comprendere come individui e gruppi evolvano nel tempo.\nGli studi longitudinali richiedono una gestione attenta delle complessità statistiche per evitare che i risultati siano distorti o di difficile generalizzazione. In particolare, il controllo di variabili di confondimento e la corretta modellazione della crescita intra- e interindividuale evidenziano quanto la qualità dei risultati dipenda dalla precisione nella progettazione dello studio e nella selezione delle metodologie analitiche.\nLa metodologia longitudinale ci ricorda che il cambiamento umano è complesso e multiforme. Sebbene ogni modello rappresenti un tentativo di cogliere questa complessità, nessun approccio è esaustivo: per cogliere appieno le sfumature del cambiamento, potrebbe essere necessario combinare più metodologie, integrando i punti di forza di ciascun approccio.\n\n\n\n\nGrimm, K. J., Ram, N., & Estabrook, R. (2016). Growth modeling: Structural equation and multilevel modeling approaches. Guilford Publications.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>72</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/02_lgm_prelims.html",
    "href": "chapters/lgm/02_lgm_prelims.html",
    "title": "73  Considerazioni Preliminari",
    "section": "",
    "text": "73.1 Strutture dei dati\nIn questo capitolo esamineremo alcune questioni pratiche e preliminari relative all’organizzazione di dati che provengono da studi longitudinali\nTradizionalmente, gli studi longitudinali venivano condotti con un numero relativamente basso di valutazioni ripetute (meno di 8) e un numero elevato di individui (più di 200). Tuttavia, i progressi nelle teorie statistica, che includono l’utilizzo di modelli non lineari, e nella tecnologia di raccolta dati, come i sondaggi basati sul web e gli smartphone, hanno notevolmente ampliato le possibilità di raccogliere e analizzare dati longitudinali. In particolare, Grimm et al. (2016) hanno discusso l’applicazione di modelli di crescita latente a dati longitudinali di grandi dimensioni, comprendenti fino a 50,000 individui e 1,000 valutazioni ripetute.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>73</span>  <span class='chapter-title'>Considerazioni Preliminari</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/02_lgm_prelims.html#strutture-dei-dati",
    "href": "chapters/lgm/02_lgm_prelims.html#strutture-dei-dati",
    "title": "73  Considerazioni Preliminari",
    "section": "",
    "text": "73.1.1 Formato Long e Wide\nI dati longitudinali tipicamente si presentano in due forme: long e wide. Nel formato long, la descrizione del tempo è sulle righe; nel formato wide le variabili relative ad ogni occasione temporale sono organizzate in colonne. È possibile trasformere i dati dal formato long in formato wide e viceversa usando le funzioni R pivot_wider() e pivot_longer(). La sintassi è spiegata nella pagina web tidyr.\nPer fare un esempio, esaminiamo il cambiamento nel rendimento in matematica dei bambini durante la scuola elementare e media utilizzando il set di dati NLSY-CYA. Questi dati sono stati analizzati da Grimm et al. (2016) e possono essere utilizzati per illustrare i concetti relativi alle analisi di cambiamento longitudinale.\nIniziamo a leggere i dati in R.\n\n# set filepath for data file\nfilepath &lt;- \"https://raw.githubusercontent.com/LRI-2/Data/main/GrowthModeling/nlsy_math_long_R.dat\"\n\n# read in the text data file using the url() function\ndat &lt;- read.table(\n  file = url(filepath),\n  na.strings = \".\"\n) # indicates the missing data designator\n# copy data with new name\nnlsy_math_long &lt;- dat\n\n# Add names the columns of the data set\nnames(nlsy_math_long) &lt;- c(\n  \"id\", \"female\", \"lb_wght\",\n  \"anti_k1\", \"math\", \"grade\",\n  \"occ\", \"age\", \"men\",\n  \"spring\", \"anti\"\n)\n\n# view the first few observations in the data set\nhead(nlsy_math_long) |&gt; \nprint()\n\n    id female lb_wght anti_k1 math grade occ age men spring anti\n1  201      1       0       0   38     3   2 111   0      1    0\n2  201      1       0       0   55     5   3 135   1      1    0\n3  303      1       0       1   26     2   2 121   0      1    2\n4  303      1       0       1   33     5   3 145   0      1    2\n5 2702      0       0       0   56     2   2 100  NA      1    0\n6 2702      0       0       0   58     4   3 125  NA      1    2\n\n\nI dati sono qui forniti nel formato long.\nContiamo il numero di partecipanti.\n\nnlsy_math_long |&gt;\n  distinct(id) |&gt;\n  count() |&gt; \n  print()\n\n    n\n1 932\n\n\nCon pivot_wider possiamo trasformare i dati in formato wide.\n\nnlsy_math_wide &lt;- nlsy_math_long |&gt; \n  pivot_wider(names_from = grade, values_from = math)\n\nnlsy_math_wide |&gt;\n  head() |&gt; \n  print()\n\n# A tibble: 6 x 16\n     id female lb_wght anti_k1   occ   age   men spring  anti   `3`\n  &lt;int&gt;  &lt;int&gt;   &lt;int&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1   201      1       0       0     2   111     0      1     0    38\n2   201      1       0       0     3   135     1      1     0    NA\n3   303      1       0       1     2   121     0      1     2    NA\n4   303      1       0       1     3   145     0      1     2    NA\n5  2702      0       0       0     2   100    NA      1     0    NA\n6  2702      0       0       0     3   125    NA      1     2    NA\n# i 6 more variables: `5` &lt;int&gt;, `2` &lt;int&gt;, `4` &lt;int&gt;, `8` &lt;int&gt;,\n#   `6` &lt;int&gt;, `7` &lt;int&gt;",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>73</span>  <span class='chapter-title'>Considerazioni Preliminari</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/02_lgm_prelims.html#visualizzazione-dei-dati-longitudinali",
    "href": "chapters/lgm/02_lgm_prelims.html#visualizzazione-dei-dati-longitudinali",
    "title": "73  Considerazioni Preliminari",
    "section": "73.2 Visualizzazione dei dati longitudinali",
    "text": "73.2 Visualizzazione dei dati longitudinali\nCome in qualsiasi analisi statistica, è importante esaminare attentamente i dati. Ciò include la produzione di sia riepiloghi quantitativi che visualizzazioni. Per fare un esempio di visualizzazione di dati longitudinali, esaminiamo il cambiamento nel rendimento in matematica dei bambini durante la scuola elementare e media utilizzando il set di dati NLSY-CYA (si veda Grimm et al., 2016). Da questi dati, selezioniamo solo il grado scolastico, il codice identificativo e il punteggio di matematica.\n\nnlsy_math_only_long &lt;- nlsy_math_long |&gt;\n    dplyr::select(id, grade, math)\n\nLe traiettorie di cambiamento intra-individuale possono essere prodotte nel modo seguente.\n\nnlsy_math_long |&gt; # data set\n  ggplot(aes(x = grade, y = math, group = id)) + # setting variables\n  geom_point(size = .5) + # adding points to plot\n  geom_line(alpha=0.2) + # adding lines to plot\n  # setting the x-axis with breaks and labels\n  scale_x_continuous(\n    limits = c(2, 8),\n    breaks = c(2, 3, 4, 5, 6, 7, 8),\n    name = \"Grade at Testing\"\n  ) +\n  # setting the y-axis with limits breaks and labels\n  scale_y_continuous(\n    limits = c(10, 90),\n    breaks = c(10, 30, 50, 70, 90),\n    name = \"PIAT Mathematics\"\n  )\n\n\n\n\n\n\n\n\nEsaminiamo le traiettorie di sviluppo dei primi 9 partecipanti.\n\nsubset_it &lt;- c(201, 303, 2702, 4303, 5002, 5005, 5701, 6102, 6801)\ntemp &lt;- nlsy_math_long[nlsy_math_long$id %in% subset_it, ]\n\ntemp |&gt;\n  ggplot(aes(x = grade, y = math)) +\n  geom_point() +\n  geom_line() +\n  # coord_cartesian(ylim = c(1, 4)) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~id)",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>73</span>  <span class='chapter-title'>Considerazioni Preliminari</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/02_lgm_prelims.html#data-screening",
    "href": "chapters/lgm/02_lgm_prelims.html#data-screening",
    "title": "73  Considerazioni Preliminari",
    "section": "73.3 Data screening",
    "text": "73.3 Data screening\nPrima di adattare i modelli di crescita ai dati longitudinali, è essenziale effettuare una valutazione preliminare dei dati e acquisire informazioni di base sulle variabili da utilizzare nell’analisi. Una delle prime fasi di questa valutazione preliminare è l’ispezione della distribuzione dei punteggi per ogni variabile, utilizzando le principali statistiche descrittive univariate, come la media, la mediana, la varianza (deviazione standard), l’asimmetria, la curtosi, il minimo, il massimo, l’intervallo e il numero di osservazioni per ogni variabile, in base alla metrica del tempo scelta. Le statistiche descrittive bivariate, come le correlazioni/covarianze e le tabelle di frequenza bivariate per variabili nominali o ordinali, possono fornire informazioni sui possibili schemi e relazioni non lineari, nonché sui potenziali valori anomali e codici errati.\nI dati longitudinali sono caratterizzati dall’ordinamento dei dati lungo una o più metriche del tempo (ad esempio, l’occasione di misurazione, l’età, la data, il tempo dall’evento, il numero di esposizioni, ecc.). È importante esaminare come la media, la varianza e il numero di casi disponibili cambiano attraverso le misure ripetute (ad esempio wght5, wght6, wght7). Va notato che la selezione della metrica del tempo influisce notevolmente sulla capacità di interpretare i risultati di qualsiasi modello di crescita specifico. Pertanto, durante la fase di selezione dei dati, è necessario considerare attentamente come varie proprietà dei dati longitudinali cambiano quando i dati sono organizzati in relazione a diverse metriche del tempo.\nPer i dati dell’esempio, le statistiche descrittive possono essere ottenute nel modo seguente.\n\ndescribe(nlsy_math_long) |&gt;\n    print()\n\n        vars    n      mean        sd median   trimmed       mad min\nid         1 2221 528449.15 327303.70 497403 515466.90 384144.63 201\nfemale     2 2221      0.49      0.50      0      0.49      0.00   0\nlb_wght    3 2221      0.08      0.27      0      0.00      0.00   0\nanti_k1    4 2221      1.42      1.50      1      1.19      1.48   0\nmath       5 2221     46.12     12.80     46     46.22     11.86  12\ngrade      6 2221      4.51      1.77      4      4.44      1.48   2\nocc        7 2221      2.84      0.79      3      2.77      1.48   2\nage        8 2221    126.90     22.06    126    126.28     25.20  82\nmen        9 1074      0.19      0.40      0      0.12      0.00   0\nspring    10 2221      0.65      0.48      1      0.69      0.00   0\nanti      11 2170      1.58      1.54      1      1.38      1.48   0\n            max   range  skew kurtosis      se\nid      1256601 1256400  0.30    -0.90 6945.07\nfemale        1       1  0.03    -2.00    0.01\nlb_wght       1       1  3.10     7.63    0.01\nanti_k1       8       8  1.14     1.14    0.03\nmath         81      69 -0.03    -0.18    0.27\ngrade         8       6  0.26    -0.92    0.04\nocc           5       3  0.55    -0.48    0.02\nage         175      93  0.19    -0.91    0.47\nmen           1       1  1.54     0.37    0.01\nspring        1       1 -0.63    -1.61    0.01\nanti          8       8  0.98     0.64    0.03\n\n\nEsaminiamo le statistiche descrittive bivariate.\n\n# Calcola la matrice di correlazione e arrotondala a 2 decimali\ncor_matrix &lt;- cor(nlsy_math_long, use = \"pairwise.complete.obs\") |&gt; round(2)\n\n# Imposta i valori al di sopra della diagonale principale a NA\ncor_matrix[!lower.tri(cor_matrix, diag = TRUE)] &lt;- NA\n\n# Stampa solo la matrice triangolare inferiore\nprint(cor_matrix, na.print = \"\")\n\n           id female lb_wght anti_k1  math grade  occ  age  men spring anti\nid       1.00                                                              \nfemale  -0.01   1.00                                                       \nlb_wght -0.01   0.06    1.00                                               \nanti_k1 -0.02  -0.09    0.03    1.00                                       \nmath    -0.22  -0.05   -0.03   -0.08  1.00                                 \ngrade   -0.01   0.00   -0.02   -0.03  0.59  1.00                           \nocc      0.01  -0.02   -0.03   -0.04  0.53  0.87 1.00                      \nage     -0.01  -0.04    0.01   -0.01  0.58  0.95 0.86 1.00                 \nmen     -0.02   0.02    0.04    0.01  0.30  0.62 0.57 0.64 1.00            \nspring  -0.11   0.04    0.03   -0.01  0.29  0.12 0.17 0.21 0.16   1.00     \nanti     0.01  -0.07    0.02    0.52 -0.05  0.04 0.04 0.06 0.13  -0.01    1\n\n\nScomponiamo i punteggi nelle componenti tra i soggetti ed entro i soggetti.\nEsaminiamo la distribuzione delle medie dei punteggi tra i soggetti.\n\ntmp &lt;- meanDecompose(math ~ id, data = nlsy_math_long)\n\n\nplot(\n    testDistribution(\n        tmp[[\"math by id\"]]$X,\n        extremevalues = \"theoretical\", ev.perc = .001\n    ),\n    varlab = \"Between Person Math Scores\"\n)\n\n\n\n\n\n\n\n\nEsaminiamo la distribuzione dei punteggi entro i soggetti.\n\nplot(\n    testDistribution(\n        tmp[[\"math by residual\"]]$X,\n        extremevalues = \"theoretical\", ev.perc = .001\n    ),\n    varlab = \"Within Person Math Scores\"\n)",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>73</span>  <span class='chapter-title'>Considerazioni Preliminari</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/02_lgm_prelims.html#attendibilità",
    "href": "chapters/lgm/02_lgm_prelims.html#attendibilità",
    "title": "73  Considerazioni Preliminari",
    "section": "73.4 Attendibilità",
    "text": "73.4 Attendibilità\nPer garantire la validità delle analisi, è necessario valutare l’affidabilità degli strumenti di misurazione utilizzati. Ciò è particolarmente importante in analisi longitudinali, in cui si studiano i cambiamenti all’interno di ciascun individuo nel tempo. Una delle metodologie utilizzate per valutare l’affidabilità degli strumenti di misurazione è il calcolo dell’indice di affidabilità \\(\\omega\\) di McDonald per ogni momento temporale in cui si effettua la misurazione. Tuttavia, è importante sottolineare che questo indice non è equivalente alle misure di affidabilità degli indici di cambiamento longitudinale. Quest’ultima è una problematica complessa, soprattutto in disegni longitudinali intensivi, e costituisce un tema di discussione nella letteratura metodologica contemporanea.\nMentre l’affidabilità dei cambiamenti intra-individuali è difficile da stimare, l’affidabilità delle differenze inter-individuali può essere trovata facilmente. L’affidabilità del cambiamento inter-individuale è la proporzione di varianza nei punteggi osservati tra gli individui che può essere attribuita alla varianza nei punteggi veri piuttosto che alla varianza dell’errore:\n\\[\n\\frac{\\text{varianza di interesse}}{\\text{varianza di interesse} + \\text{varianza d'errore}},\n\\]\nNel caso delle misure di differenza individuale ottenute tramite Intensive Longitudinal Designs (ILD), per valutare l’affidabilità delle medie intra-individuali \\(\\bar{y}\\) (cioè la varianza “vera” delle medie tra i soggetti) si può utilizzare il coefficiente di correlazione intraclasse (ICC). In sostanza, l’ICC indica la proporzione di varianza totale che è attribuibile alle differenze tra gli individui rispetto alla varianza totale, che comprende sia le differenze tra gli individui che le differenze all’interno di ciascun individuo nel tempo. L’ICC viene calcolato come un rapporto tra varianze:\n\\[\n\\rho^2_{\\bar{y}} = \\frac{\\hat{\\tau}^2_{\\mu}}{\\hat{\\tau}^2_{\\mu} + \\hat{\\tau}^2_{\\varepsilon}}.\n\\]\nIl coefficiente di correlazione intraclasse (ICC) può essere stimato utilizzano un modello misto lineare con intercetta casuale che tiene conto della clusterizzazione dei dati, cioè del fatto che le osservazioni sono raggruppate in base ai soggetti. L’ICC viene calcolato come il rapporto tra la varianza tra le medie dei cluster di raggruppamento dei dati e la varianza totale, che comprende la varianza tra i cluster e la varianza all’interno dei cluster. In altre parole, l’ICC rappresenta la proporzione di varianza totale che è dovuta alle differenze tra i soggetti rispetto alla varianza totale delle misure ripetute. Questo indice è utile per valutare l’affidabilità delle misure ripetute e la loro utilità per lo studio delle differenze individuali.\nConsideriamo nuovamente i dati nlsy_math_long, che rappresentano il cambiamento nel rendimento in matematica dei bambini tra i gradi scolastici 2 e 8.\nSelezioniamo solo le variabili di interesse.\n\nnlsy_math_only_long &lt;- nlsy_math_long %&gt;%\n    dplyr::select(id, grade, math)\n\nnlsy_math_only_long |&gt;\n    head()|&gt; \n    print()\n\n    id grade math\n1  201     3   38\n2  201     5   55\n3  303     2   26\n4  303     5   33\n5 2702     2   56\n6 2702     4   58\n\n\nIl coefficiente ICC può essere trovato, ad esempio, mediante la funzione iccMixed specificando un raggruppamento dei dati nei termini dei soggetti.\n\niccMixed(\n  dv = \"math\",\n  id = c(\"id\"),\n  data = nlsy_math_long\n) |&gt;\n  print()\n\n        Var     Sigma       ICC\n     &lt;char&gt;     &lt;num&gt;     &lt;num&gt;\n1:       id  47.02036 0.2872289\n2: Residual 116.68307 0.7127711\n\n\nPer capire meglio il significato dell’ICC calcolato in precedenza per i dati nlsy_math_long, possiamo replicare lo stesso risultato utilizzando il modello misto lineare lmer, che tiene conto dell’effetto casuale del soggetto.\n\nm &lt;- lmer(math ~ 1 + (1 | id), data = nlsy_math_only_long)\n\nIl modello misto decompone la varianza totale in due componenti: la varianza che dipende dalla differenze tra le medie di ciascun soggetto (tra i soggetti, o varianza delle intercette individuali, \\(\\tau_{00}\\)) e la varianza che dipende dalle variazioni di ciascun soggetto attorno alla sua media.\nCalcoliamo la varianza totale dei punteggi di matematica.\n\nvar(nlsy_math_only_long$math) |&gt;\n    print()\n\n[1] 163.8379\n\n\nEsaminiamo ora la scomposizione della varianza eseguita dal modello misto. Si noti che la somma delle due componenti è uguale alla varianza totale.\n\nVarCorr(m) |&gt;\n    print()\n\n Groups   Name        Std.Dev.\n id       (Intercept)  6.8571 \n Residual             10.8020 \n\n\n\n6.8571^2 + 10.8020^2\n\n163.70302441\n\n\nIl coefficiente ICC è data dal rapporto tra la varianza attribuibile alla variazione tra le medie dei soggetti e la varianza totale.\n\n6.8571^2 / (6.8571^2 + 10.8020^2)\n\n0.287226339155697\n\n\nNel contesto dei modelli di crescita latente (LGM), l’ICC può essere usato per stimare l’affidabilità delle medie intra-individuali dei fattori latenti, ma non è adatto a valutare l’affidabilità delle variazioni intra-individuali nelle traiettorie di sviluppo. Per queste ultime, è necessario ricorrere a diverse misure di affidabilità.\nAd esempio, il coefficiente di affidabilità test-retest può essere utilizzato per stimare l’affidabilità intra-individuale delle traiettorie di sviluppo calcolate in due momenti distinti. Tuttavia, questo richiede l’assunzione che i punteggi veri restino invariati nel tempo. Un’alternativa è l’uso di forme parallele di test per misurare l’affidabilità delle traiettorie intra-individuali, anche se tali forme sono raramente disponibili.\nPer stimare l’affidabilità della componente sistematica della variazione intra-individuale nelle traiettorie di sviluppo, sono necessari metodi di stima specifici, ancora oggetto di dibattito nella letteratura metodologica.\nPrima di approfondire i modelli di crescita latente, è utile considerare l’analisi dei dati longitudinali tramite modelli misti. In R, queste analisi possono essere condotte con la funzione lmer del pacchetto lme4.\n\n73.4.1 Analisi con lmer\nLa funzione lmer() accetta i seguenti argomenti:\n\nformula: una formula lineare a due lati che descrive sia gli effetti fissi che gli effetti casuali del modello, con la risposta a sinistra dell’operatore ~ e i predittori e gli effetti casuali sulla destra dell’operatore ~.\ndata: Un data.frame, che deve essere nel cosiddetto formato “lungo”, con una singola riga per osservazione.\n\nIniziamo a descrivere la sintassi che consente la specificazione di un modello misto. Gli effetti fissi sono specificati come segue.\n\n\n\n\n\n\n\nFormula\nDescription\n\n\n\n\na + b\nmain effects of a and b (and no interaction)\n\n\na:b\nonly interaction of a and b (and no main effects)\n\n\na * b\nmain effects and interaction of a and b (expands to: a + b + a:b)\n\n\n(a+b+c)^2\nmain effects and two-way interactions, but no three-way interaction (expands to: a + b + c + a:b + b:c + a:c)\n\n\n(a+b)*c\nall main effects and pairwise interactions between c and a or b (expands to: a + b + c + a:c + b:c)\n\n\n0 + a\n0 suppresses the intercept resulting in a model that has one parameter per level of a (identical to: a - 1)\n\n\n\nGli effetti random vengono aggiunti alla formula tra parentesi (). All’interno di queste parentesi si fornisce sul lato sinistro di un segno condizionale | la specifica degli effetti casuali relativi alle pendenze individuali da includere nel modello. Sul lato destro di questo segno condizionale, si specifica il fattore di raggruppamento o i fattori di raggruppamento da cui dipendono questi effetti casuali. I fattori di raggruppamento devono essere di classe factor (cioè non possono essere variabili numeriche).\nGli effetti random vengono specificati come segue.\n\n\n\n\n\n\n\nFormula\nDescription\n\n\n\n\n(1\\|s)\nrandom intercepts for unique level of the factor s\n\n\n(1\\|s) + (1\\|i)\nrandom intercepts for each unique level of s and for each unique level of i\n\n\n(1\\|s/i)\nrandom intercepts for factor s and i, where the random effects for i are nested in s. This expands to (1\\|s) + (1\\|s:i) , i.e. a random intercept for each level of s, and each unique combination of the levels of s and i. Nested random effects are used in so-called multilevel models. For example, s might refer to schools, and i to classrooms within those schools.\n\n\n(a\\|s)\nrandom intercepts and random slopes for a, for each level of s. Correlations between the intercept and slope effects are also estimated – identical to (a*b\\|s))\n\n\n(a*b\\|s)\nrandom intercepts and slopes for a, b, and the a:b interaction, for each level of s. Correlations between all the random effects are estimated.\n\n\n(0+a\\|s)\nrandom slopes for a for each level of s, but no random intercepts\n\n\n(a\\|\\|s)\nrandom intercepts and random slopes for a, for each level of s, but no correlations between the random effects (i.e. they are set to 0). This expands to: (0+a\\|s) + (1\\|s)\n\n\n\n\n\n73.4.2 Formulazione del modello\nIn precedenza abbiamo descritto la variazione media tra gli individui mediante un modello misto ad intercetta casuale. È possibile estendere questo modello a casi più complessi, per esempio quello che assume una retta di regressione con pendenza ed intercetta diversa per ciascun soggetto. Per i dati nlsy_math_long possiamo specificare un tale modello in lmer usando la sintassi seguente.\n\nm &lt;- lmer(math ~ grade + (1 + grade | id), data = nlsy_math_long)\n\nIn un modello misto, i coefficienti delle rette di regressione di ciascun soggetto sono considerati come componenti casuali di una distribuzione di coefficienti relativi all’intercetta e alla pendenza complessive del gruppo. Questi coefficienti casuali possono essere modellati specificando una componente casuale (1 + grado | id). In questo tipo di modello, esiste una correlazione tra i parametri delle intercette e quelli delle pendenze individuali. Ciò significa che le componenti di varianza attribuibili ai vari effetti del modello (fissi e casuali) non sono più indipendenti e la varianza totale non può essere scomposta in componenti indipendenti.\nPer estrarre le componenti di varianza di un modello misto, è possibile utilizzare le funzioni fornite dal pacchetto insight. Ad esempio, nel caso dell’esempio presentato, i risultati possono essere ottenuti attraverso l’oggetto creato dalla funzione lmer.\n\ninsight::get_variance(m) |&gt;\n    print()\n\n$var.fixed\n[1] 58.94602\n\n$var.random\n[1] 70.66767\n\n$var.residual\n[1] 36.23643\n\n$var.distribution\n[1] 36.23643\n\n$var.dispersion\n[1] 0\n\n$var.intercept\n      id \n68.40554 \n\n$var.slope\n id.grade \n0.7391598 \n\n$cor.slope_intercept\n        id \n-0.2353175 \n\n\n\nUna descrizione visiva della varianza delle varie componenti del modello può essere ottenuta mediante la funzione modelDiagnostics del pacchetto JWileymisc.\n\nmd &lt;- JWileymisc::modelDiagnostics(m, ev.perc = .001)\nplot(md, ask = FALSE, ncol = 2, nrow = 3)\n\n\n\n\n\n\n\n\nUna descrizione dei parametri del modello può essere ottenuta nel modo seguente.\n\nmt &lt;- modelTest(m)\nnames(mt)\nAPAStyler(mt) |&gt;\n    print()\n\n\n'FixedEffects''RandomEffects''EffectSizes''OverallModel'\n\n\n                        Term                     Est           Type\n                      &lt;char&gt;                  &lt;char&gt;         &lt;char&gt;\n 1:              (Intercept) 26.59*** [25.66, 27.51]  Fixed Effects\n 2:                    grade  4.34*** [ 4.17,  4.51]  Fixed Effects\n 3: cor_grade.(Intercept)|id                   -0.24 Random Effects\n 4:        sd_(Intercept)|id                    8.27 Random Effects\n 5:              sd_grade|id                    0.86 Random Effects\n 6:                    sigma                    6.02 Random Effects\n 7:                 Model DF                       6  Overall Model\n 8:               N (Groups)                id (932)  Overall Model\n 9:         N (Observations)                    2221  Overall Model\n10:                   logLik                -7968.69  Overall Model\n11:                      AIC                15949.39  Overall Model\n12:                      BIC                15983.62  Overall Model\n13:              Marginal R2                    0.36  Overall Model\n14:              Marginal F2                    0.55  Overall Model\n15:           Conditional R2                    0.78  Overall Model\n16:           Conditional F2                    3.57  Overall Model\n17:   grade (Fixed + Random)     0.55/2.26, p &lt; .001   Effect Sizes\n18:           grade (Random)     0.01/0.09, p = .002   Effect Sizes\n\n\nLa varianza spiegata dal modello viene ottenuta nel modo seguente.\n\nmodelPerformance(m) |&gt;\n    print()\n\n$Performance\n    Model Estimator N_Obs N_Groups      AIC      BIC        LL  LLDF\n   &lt;char&gt;    &lt;char&gt; &lt;num&gt;   &lt;char&gt;    &lt;num&gt;    &lt;num&gt;     &lt;num&gt; &lt;num&gt;\n1: merMod      REML  2221 id (932) 15952.99 15987.22 -7970.494     6\n     Sigma MarginalR2 ConditionalR2 MarginalF2 ConditionalF2\n     &lt;num&gt;      &lt;num&gt;         &lt;num&gt;      &lt;num&gt;         &lt;num&gt;\n1: 6.01967  0.3553143      0.781476  0.5511433      3.576157\n\nattr(,\"class\")\n[1] \"modelPerformance.merMod\" \"modelPerformance\"       \n\n\n\n\n\n\nGrimm, K. J., Ram, N., & Estabrook, R. (2016). Growth modeling: Structural equation and multilevel modeling approaches. Guilford Publications.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>73</span>  <span class='chapter-title'>Considerazioni Preliminari</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/06_lgm_mixed.html",
    "href": "chapters/lgm/06_lgm_mixed.html",
    "title": "74  LGM e modelli misti",
    "section": "",
    "text": "74.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nI modelli di crescita latente (LGM, Latent Growth Models) rappresentano una forma specializzata di analisi fattoriale confermativa (CFA) che si focalizza sull’evoluzione dei costrutti nel tempo. Questi modelli si distinguono per la loro capacità di fissare le saturazioni fattoriali a valori predefiniti. Questo significa che si imposta in anticipo come ciascun fattore contribuisca alla varianza osservata nei dati raccolti in diversi momenti. In molti casi, la traiettoria di crescita nel tempo può essere descritta utilizzando una funzione lineare o quadratica, permettendo di modellare diversi tipi di evoluzioni, come un aumento costante o un cambiamento accelerato.\nUn aspetto centrale dei LGM è il concetto di fattori di crescita, che rappresentano le differenze individuali all’interno dei dati longitudinali. Questi fattori di crescita sono rappresentati da variabili latenti continue, denominate growth factors. In pratica, permettono di catturare e quantificare variazioni individuali nel modo in cui i soggetti cambiano nel tempo, ad esempio, in termini di sviluppo delle competenze o dell’andamento di un sintomo.\nPer facilitare una comprensione più approfondita dei modelli LGM, nel presente capitolo si propone un confronto con i modelli misti {cite:p}hoffman2022catching. Questo confronto mira a evidenziare le differenze e le somiglianze tra i due approcci, aiutando a discernere quando e perché scegliere un modello rispetto all’altro. Mentre i modelli misti possono essere utilizzati per analizzare dati gerarchici o nidificati, i modelli LGM si concentrano specificamente sull’analisi della traiettoria di crescita nel tempo, rendendoli particolarmente utili in studi longitudinali dove l’interesse primario è capire come un costrutto si sviluppa o cambia nel corso del tempo.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>74</span>  <span class='chapter-title'>LGM e modelli misti</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/06_lgm_mixed.html#modelli-misti",
    "href": "chapters/lgm/06_lgm_mixed.html#modelli-misti",
    "title": "74  LGM e modelli misti",
    "section": "74.2 Modelli misti",
    "text": "74.2 Modelli misti\nI modelli lineari generalizzati classici presuppongono l’indipendenza delle osservazioni, un’ipotesi che può non essere valida in contesti dove si effettuano misurazioni ripetute sullo stesso soggetto nel tempo, portando a osservazioni correlate. In questi scenari, si ricorre all’uso di modelli lineari a effetti misti (o modelli gerarchici lineari), che consentono di gestire adeguatamente la correlazione intragruppo.\nNei modelli a effetti misti, si realizza un equilibrio tra i due approcci estremi di modellazione: la modellazione aggregata (o pooled) e la modellazione separata (o per gruppo). La modellazione aggregata tratta l’insieme dei dati come un unico gruppo, ignorando le differenze intergruppo e potenzialmente perdendo informazioni rilevanti sulla variabilità tra i gruppi. La modellazione separata, d’altra parte, adatta un modello distinto per ciascun gruppo, potendo portare a sovrapparametrizzazione e ridotta capacità di generalizzazione.\nI modelli a effetti misti superano queste limitazioni integrando elementi sia della modellazione aggregata sia della modellazione separata. Ciò consente di considerare sia le differenze tra gruppi (variabilità intergruppo) sia l’informazione comune (variabilità intragruppo), migliorando la precisione delle stime dei parametri. Tradizionalmente, i dati rilevati all’interno di un soggetto sono classificati come dati di Livello 1, mentre i dati raccolti tra soggetti diversi sono definiti dati di Livello 2.\nAnalogamente ai modelli di regressione lineari tradizionali, i modelli a effetti misti includono un’intercetta fissa, coefficienti fissi per i predittori e un termine di errore per la deviazione tra i valori osservati e quelli predetti dal modello. Tuttavia, a differenza dei modelli lineari standard, in un modello a effetti misti, l’intercetta e i coefficienti dei predittori possono variare tra le unità di analisi, permettendo una maggiore flessibilità e adattabilità nel rappresentare la struttura dei dati. Questo approccio rende i modelli a effetti misti particolarmente adatti per l’analisi di dati longitudinali o gerarchici, dove è necessario tener conto della correlazione tra osservazioni all’interno dello stesso gruppo o soggetto.\nIl modello lineare multilivello, applicato all’analisi di dati strutturati in gruppi o cluster, permette una comprensione dettagliata della variazione sia all’interno dei gruppi (within-group variation) sia tra i gruppi (between-group variation). Formalmente, consideriamo un’unità statistica \\(i\\) all’interno di un gruppo \\(j\\) (dove \\(i = 1, ..., n_j\\) e \\(j = 1, ..., N\\)), per un totale di \\(N\\) gruppi ciascuno con numerosità \\(n_j\\).\nPer una variabile dipendente \\(Y\\), una variabile indipendente a livello individuale \\(x\\) e una variabile di gruppo \\(z\\), il modello si articola su due livelli. Il primo livello è rappresentato dalla seguente equazione lineare:\n\\[\nY_{ij} = \\beta_{0j} + \\beta_{1j}x_{ij} + \\varepsilon_{ij}.\n\\]\nQuesta equazione descrive la relazione tra \\(Y\\) e \\(x\\) per ogni unità \\(i\\) nel gruppo \\(j\\), dove \\(\\beta_{0j}\\) è l’intercetta e \\(\\beta_{1j}\\) la pendenza, specifiche per ciascun gruppo \\(j\\). Il termine d’errore \\(\\varepsilon_{ij}\\) è assunto normalmente distribuito con media zero e varianza costante \\(\\sigma^2\\).\nIl secondo livello del modello esplicita come l’intercetta \\(\\beta_{0j}\\) e la pendenza \\(\\beta_{1j}\\) varino tra i gruppi in relazione alla variabile di gruppo \\(z\\)\n\\[\n\\beta_{0j} = \\gamma_{00} + \\gamma_{01}z_j + U_{0j}\n\\]\n\\[\n\\beta_{1j} = \\gamma_{10} + \\gamma_{11}z_j + U_{1j}\n\\]\ndove:\n\n\\(\\gamma_{00}\\) rappresenta l’intercetta media a livello di gruppo. In altre parole, è il valore previsto di \\(Y\\) quando tutte le variabili indipendenti (\\(x\\) e \\(z\\)) sono pari a zero. È una sorta di “punto di partenza” medio per i vari gruppi nel modello.\n\\(\\gamma_{01}\\) è il coefficiente di pendenza che descrive come l’intercetta varia tra i gruppi in relazione alla variabile di gruppo \\(z\\). In pratica, \\(\\gamma_{01}\\) moltiplica la variabile di gruppo \\(z_j\\) (per ogni gruppo \\(j\\)) per quantificare l’influenza di questa variabile sulla variazione dell’intercetta tra i gruppi. Un valore positivo di \\(\\gamma_{01}\\) indica che un aumento in \\(z_j\\) è associato a un aumento dell’intercetta di \\(Y\\), mentre un valore negativo indica il contrario.\n\\(\\gamma_{10}\\) rappresenta il valore medio della pendenza della relazione tra \\(Y\\) e \\(x\\) attraverso tutti i gruppi, quando la variabile di gruppo \\(z\\) è zero. Indica come, in media, la variabile indipendente a livello individuale \\(x\\) si relaziona con \\(Y\\) nei diversi gruppi.\n\\(\\gamma_{11}\\) modella come la relazione (pendenza) tra \\(Y\\) e \\(x\\) varia tra i gruppi in funzione della variabile di gruppo \\(z\\). Analogamente a \\(\\gamma_{01}\\), questo coefficiente moltiplica \\(z_j\\) per mostrare l’effetto di \\(z\\) sulla pendenza di \\(Y\\) rispetto a \\(x\\) tra i gruppi. Se \\(\\gamma_{11}\\) è significativo, indica che l’effetto di \\(x\\) su \\(Y\\) non è costante tra i gruppi, ma varia in base al valore di \\(z\\).\n\nIn sintesi, questi coefficienti permettono di comprendere non solo come varia la relazione tra \\(Y\\) e \\(x\\) all’interno di ciascun gruppo (grazie a \\(\\gamma_{10}\\)), ma anche come questa relazione sia influenzata dalla variabile di gruppo \\(z\\) (mediante \\(\\gamma_{11}\\)). Allo stesso modo, essi illustrano come l’intercetta di \\(Y\\) varia tra i gruppi in base a \\(z\\) (\\(\\gamma_{01}\\)), oltre a fornire un valore di intercetta medio (\\(\\gamma_{00}\\)).\nQueste equazioni legano le variazioni di \\(\\beta_{0j}\\) e \\(\\beta_{1j}\\) tra i gruppi alla variabile \\(z\\). I termini \\(U_{0j}\\) e \\(U_{1j}\\) rappresentano l’errore a livello di gruppo, anch’essi assunti normalmente distribuiti con media zero e varianze costanti \\(\\tau_0^2\\) e \\(\\tau_1^2\\), rispettivamente, e indipendenti dall’errore a livello individuale \\(\\varepsilon_{ij}\\).\nIl modello multilivello permette così di analizzare come le caratteristiche di gruppo (come \\(z\\)) influenzano non solo l’intercetta (il livello di base di \\(Y\\)) ma anche la relazione tra \\(Y\\) e \\(x\\) (la pendenza). In altre parole, consente di esplorare come la relazione tra una variabile dipendente e indipendente possa cambiare da un gruppo all’altro.\nUn elemento chiave di questo approccio è il coefficiente di correlazione intragruppo \\(\\rho(Y \\mid x)\\), definito come:\n\\[\n\\rho(Y \\mid x) = \\frac{\\tau_0^2}{\\tau_0^2 + \\sigma^2}\n\\]\nIl coefficiente di correlazione intragruppo misura la proporzione della varianza totale di \\(Y\\) attribuibile alle differenze tra i gruppi. Un valore di \\(\\rho\\) vicino a 1 indica che la maggior parte della varianza di \\(Y\\) è spiegata dalle differenze tra i gruppi, mentre un valore vicino a 0 suggerisce che la varianza è prevalentemente dovuta a differenze all’interno dei singoli gruppi. Questo coefficiente fornisce quindi una misura quantitativa dell’importanza relativa delle variazioni tra e all’interno dei gruppi nel modello.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>74</span>  <span class='chapter-title'>LGM e modelli misti</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/06_lgm_mixed.html#simulare-effetti-casuali",
    "href": "chapters/lgm/06_lgm_mixed.html#simulare-effetti-casuali",
    "title": "74  LGM e modelli misti",
    "section": "74.3 Simulare effetti casuali",
    "text": "74.3 Simulare effetti casuali\nEsaminiamo con una simulazione una dimostrazione del funzionamento dei modelli misti. Ciò ci permetterà di meglio comprendere i modelli a crescita latente. Simuleremo dei dati bilanciati, con punteggi su quattro rilevazioni temporali per 500 individui (soggetti). Esamineremo il tasso di crescita (‘growth’) e consentiremo la presenza di intercette e pendenze specifiche per i diversi soggetti.\nLe istruzioni seguenti generano i dati (per i nostri scopi, non è importante capire i dettagli di questa porzione di codice).\n\nset.seed(12345)\nn &lt;- 500\ntimepoints &lt;- 4\ntime &lt;- rep(0:3, times = n)\nsubject &lt;- rep(1:n, each = 4)\n\nintercept &lt;- .5\nslope &lt;- .25\nrandomEffectsCorr &lt;- matrix(c(1, .2, .2, 1), ncol = 2)\n\nrandomEffects &lt;- MASS::mvrnorm(\n  n,\n  mu = c(0, 0), Sigma = randomEffectsCorr, empirical = T\n) %&gt;%\n  data.frame()\n\ncolnames(randomEffects) &lt;- c(\"Int\", \"Slope\")\n\nNella simulazione, abbiamo impostato gli effetti fissi, che comprendono l’intercetta e la pendenza della regressione lineare standard, ai valori di 0.5 e 0.25 rispettivamente. Inoltre, è stata simulata una correlazione di 0.2 tra l’intercetta e la pendenza che sono specifiche per ogni singolo soggetto. A causa di questa correlazione, i dati sono stati generati utilizzando una distribuzione normale multivariata. In questo contesto, abbiamo assegnato una varianza di 1 sia per l’intercetta sia per la pendenza.\nProcediamo ora con l’analisi dei dati risultanti dalla simulazione. Questo passaggio è fondamentale per comprendere le implicazioni dei parametri scelti nella simulazione e per verificare se i dati generati rispecchiano le aspettative teoriche stabilite inizialmente.\nI dati prodotti fino ad ora sono i seguenti:\n\ndata.frame(\n    Subject = subject, \n    time = time, \n    randomEffects[subject, ]\n) |&gt;\n    head(10)\n\n\nA data.frame: 10 x 4\n\n\n\nSubject\ntime\nInt\nSlope\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1\n0\n-1.3322902\n-0.9548087\n\n\n1.1\n1\n1\n-1.3322902\n-0.9548087\n\n\n1.2\n1\n2\n-1.3322902\n-0.9548087\n\n\n1.3\n1\n3\n-1.3322902\n-0.9548087\n\n\n2\n2\n0\n-2.1261548\n-1.7813625\n\n\n2.1\n2\n1\n-2.1261548\n-1.7813625\n\n\n2.2\n2\n2\n-2.1261548\n-1.7813625\n\n\n2.3\n2\n3\n-2.1261548\n-1.7813625\n\n\n3\n3\n0\n0.4606242\n0.3039838\n\n\n3.1\n3\n1\n0.4606242\n0.3039838\n\n\n\n\n\nPer generare la variabile target, procediamo sommando gli effetti casuali, precedentemente calcolati, all’intercetta globale e applichiamo un analogo procedimento alle pendenze. In aggiunta, introduciamo un rumore gaussiano ai dati, caratterizzato da una deviazione standard \\(\\sigma\\) pari a 0.5. Questa operazione ha lo scopo di aggiungere un livello di variabilità realistica e di incertezza ai dati, rendendo la simulazione più vicina a scenari osservati nella realtà pratica.\n\nset.seed(12345)\nsigma &lt;- .5\ny1 &lt;- \n  (intercept + randomEffects$Int[subject]) + # random intercepts\n  (slope + randomEffects$Slope[subject]) * time + # random slopes\n  rnorm(n * timepoints, mean = 0, sd = sigma) # noise\n\nd &lt;- data.frame(subject, time, y1)\n\n\nd |&gt;\n  head(10) \n\n\nA data.frame: 10 x 3\n\n\n\nsubject\ntime\ny1\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1\n0\n-0.5395258\n\n\n2\n1\n1\n-1.1823659\n\n\n3\n1\n2\n-2.2965593\n\n\n4\n1\n3\n-3.1734649\n\n\n5\n2\n0\n-1.3232110\n\n\n6\n2\n1\n-4.0664952\n\n\n7\n2\n2\n-4.3738304\n\n\n8\n2\n3\n-6.3583342\n\n\n9\n3\n0\n0.8185443\n\n\n10\n3\n1\n1.0549470\n\n\n\n\n\nIl grafico seguente mostra le rette di regressione per ciascuno dei 500 soggetti.\n\nggplot(d, aes(x = time, y = y1)) +\n  geom_path(aes(group = subject), alpha = .1) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nAdattiamo ai dati un modello misto utilizzando la funzione lmer del pacchetto lme4. Si noti che questo è un modello in cui solo le intercette sono consentite variare tra i cluster.\n\nm0 &lt;- lmer(y1 ~ 1 + (1 | subject), data = d)\n\nLe componenti di varianza del modello si estraggono con la funzione VarCorr().\n\nVarCorr(m0)\n\n Groups   Name        Std.Dev.\n subject  (Intercept) 1.8306  \n Residual             1.4352  \n\n\nCalcoliamo il coefficiente di correlazione intraclasse.\n\n1.8306^2 / (1.8306^2 + 1.4352^2)\n\n0.619323810990691\n\n\nLaddove\n\n(1.8306^2 + 1.4352^2)\n\n5.4108954\n\n\nè uguale alla varianza della variabile risposta\n\nvar(d$y1)\n\n5.40584388269803\n\n\nLo stesso risultato si ottiene utilizzando una funzione R per il calcolo della correlazione intraclasse.\n\nmultilevelTools::iccMixed(\n  dv = \"y1\",\n  id = c(\"subject\"),\n  data = d\n) |&gt;\n  print()\n\n        Var    Sigma       ICC\n     &lt;char&gt;    &lt;num&gt;     &lt;num&gt;\n1:  subject 3.350987 0.6193062\n2: Residual 2.059886 0.3806938\n\n\n\n0.6193062 + 0.3806938\n\n1\n\n\nEsaminiamo ora un modello in cui sia le intercette sia le pendenze variano tra i cluster.\n\nmix_mod &lt;- lmer(y1 ~ time + (1 + time | subject), data = d)\nsummary(mix_mod) |&gt;\n    print()\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y1 ~ time + (1 + time | subject)\n   Data: d\n\nREML criterion at convergence: 5881.3\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.03499 -0.46249  0.00414  0.48241  2.74992 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n subject  (Intercept) 1.0245   1.0122       \n          time        1.0301   1.0149   0.15\n Residual             0.2412   0.4911       \nNumber of obs: 2000, groups:  subject, 500\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  0.50159    0.04885  10.267\ntime         0.25157    0.04644   5.417\n\nCorrelation of Fixed Effects:\n     (Intr)\ntime 0.072 \n\n\nGli effetti fissi che abbiamo ottenuto (\\(\\alpha\\) = 0.50159, \\(\\beta\\) = 0.25157) sono simili ai valori che abbiamo impostato nella simulazione per l’intercetta e la pendenza globale.\n\nVarCorr(mix_mod)\n\n Groups   Name        Std.Dev. Corr \n subject  (Intercept) 1.01217       \n          time        1.01494  0.150\n Residual             0.49108       \n\n\nLe varianze degli effetti casuali stimati (\\(1.0122^2\\), \\(1.0149^2\\)) sono molto simili al valore impostato di 1 nella simulazione, la correlazione (0.15) è simile al valore impostato di 0.2 e la deviazione standard dei residui (0.4911) è simile al valore impostato di 0.5.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>74</span>  <span class='chapter-title'>LGM e modelli misti</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/06_lgm_mixed.html#modello-di-crescita-latente",
    "href": "chapters/lgm/06_lgm_mixed.html#modello-di-crescita-latente",
    "title": "74  LGM e modelli misti",
    "section": "74.4 Modello di crescita latente",
    "text": "74.4 Modello di crescita latente\n\nEsploriamo ora l’analisi degli stessi dati tramite un modello di crescita latente (LGM). I modelli LGM possono essere considerati come un’estensione dei modelli CFA, in cui si ipotizzano due fattori latenti principali: il primo è una variabile latente associata alle intercette casuali, cioè rappresenta la variazione delle intercette individuali dei partecipanti; il secondo è una variabile latente relativa alle pendenze casuali, che descrive la variazione delle pendenze individuali dei partecipanti. Queste intercette e pendenze si riferiscono alla linea di regressione che descrive per ciascun partecipante la relazione tra la variabile in esame e il tempo.\nIn quanto il modello mira a spiegare la relazione tra le medie dei punteggi dei partecipanti nel tempo, è necessario analizzare i dati grezzi piuttosto che la matrice di covarianza campionaria. Ciò significa utilizzare le osservazioni individuali per ciascun partecipante come input.\nPer l’analisi, utilizzeremo nuovamente il software lavaan, ma con una sintassi differente per poter fissare le saturazioni fattoriali a valori specifici, come richiesto dai vincoli del modello LGM. Di conseguenza, l’output che otterremo sarà diverso da quello dei modelli SEM standard, poiché i parametri relativi alle saturazioni fattoriali sono fissi e non stimati.\nPer il fattore che rappresenta le intercette, le saturazioni fattoriali sono impostate a 1. Questo valore può essere interpretato come l’equivalente della colonna dell’intercetta nella matrice \\(\\boldsymbol{X}\\) di un modello di regressione multipla.\nLe saturazioni per il fattore che definisce le pendenze casuali sono stabilite in base alla sequenza temporale delle misurazioni \\(y\\), con valori \\(\\lambda\\) che vanno da 0 a 3. Questi riflettono gli intervalli temporali delle misurazioni. Iniziare la codifica da 0 conferisce un significato interpretabile allo zero, analogamente ai valori che, nella matrice \\(\\boldsymbol{X}\\) di un modello di regressione multipla, corrisponderebbero alla colonna della pendenza.\nIl modello di crescita latente (LGM) espresso dalla formula\n\\[\ny_j = \\alpha_0 + \\alpha_1 \\lambda_j + \\zeta_{00} + \\zeta_{11} \\lambda_j + \\epsilon_j\n\\]\npuò essere messo in relazione con il modello lineare ad effetti misti precedentemente descritto attraverso la comprensione della struttura dei due modelli e delle loro componenti.\n\nInterpretazione dei Componenti del Modello LGM:\n\n$ _0 $: Rappresenta l’intercetta media del modello di crescita.\n$ _1 _j $: Rappresenta il tasso medio di crescita nel tempo, dove $ _j $ sono i valori che descrivono l’intervallo temporale delle misurazioni.\n$ _{00} $: Indica la variazione delle intercette individuali tra i soggetti rispetto all’intercetta media $ _0 $.\n$ _{11} _j $: Esprime la variazione nelle pendenze individuali (tassi di crescita) tra i soggetti rispetto al tasso medio di crescita $ _1 $.\n$ _j $: Rappresenta l’errore di misurazione per ogni singolo soggetto.\n\nConfronto con il Modello Lineare ad Effetti Misti:\n\nNel modello lineare ad effetti misti, si considerano sia effetti fissi (come l’intercetta e la pendenza media del modello) sia effetti casuali (variazione delle intercette e delle pendenze tra i soggetti). In maniera simile, il modello LGM considera l’intercetta media e il tasso medio di crescita (effetti fissi) e permette la variazione individuale in queste componenti (effetti casuali).\nLa componente $ {00} $ nel modello LGM è analoga alla variazione casuale delle intercette nel modello ad effetti misti, mentre $ {11} $ corrisponde alla variazione casuale delle pendenze.\nEntrambi i modelli permettono di analizzare dati strutturati longitudinalmente, offrendo la flessibilità di modellare non solo la tendenza generale (effetti fissi) ma anche la variazione individuale intorno a questa tendenza (effetti casuali).\n\n\nIn sintesi, il modello LGM può essere visto come un caso speciale o un’estensione del modello lineare ad effetti misti, con un’enfasi particolare sulla modellazione del cambiamento nel tempo e sulla relazione di questa dinamica con variabili latenti. Entrambi i modelli sono strumenti potenti nell’analisi di dati longitudinali, permettendo di esaminare sia la tendenza centrale sia la variabilità individuale all’interno dei dati.\nUn requisito degli LGM è che i dati devono essere forniti del formato wide (mentre per il precedente modello misto abbiamo usato il formato long), il che significa che ogni colonna rappresenta la variabile di esito in un diverso momento nel tempo. Si presume che ogni osservazione o riga sia indipendente dalle altre; le colonne mostrano invece una dipendenza temporale. Trasformiamo dunque i dati nel formato richiesto.\n\ndwide &lt;- d %&gt;%\n  spread(time, y1) %&gt;%\n  rename_at(vars(-subject), function(x) paste0(\"y\", x))\nhead(dwide)\n\n\nA data.frame: 6 x 5\n\n\n\nsubject\ny0\ny1\ny2\ny3\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1\n-0.5395258\n-1.1823659\n-2.2965593\n-3.173465\n\n\n2\n2\n-1.3232110\n-4.0664952\n-4.3738304\n-6.358334\n\n\n3\n3\n0.8185443\n1.0549470\n2.0104678\n3.531232\n\n\n4\n4\n0.4469440\n-0.3162615\n-1.7896354\n-1.843919\n\n\n5\n5\n1.8959902\n5.5259110\n9.6045869\n12.546123\n\n\n6\n6\n2.1829579\n1.6287374\n-0.3136214\n-1.660328\n\n\n\n\n\nIl modello misto che abbiamo descritto in precedenza corrisponde dunque ad un modello fattoriale con due variabili latenti: un fattore (\\(\\eta_0\\)) che rappresenta il “punteggio vero” delle intercette individuali e un fattore (\\(\\eta_1\\)) che rappresenta il “punteggio vero” delle pendenze delle rette di regressione per i singoli individui.\nNella sintassi di lavaan il modello diventa:\n\nmodel &lt;- \"\n    i =~ 1*y0 + 1*y1 + 1*y2 + 1*y3\n    s =~ 0*y0 + 1*y1 + 2*y2 + 3*y3\n\"\n\nPossiamo adattare il modello ai dati usando una funzione specifica di lavaan, ovvero growth, che può essere usata per questa classe di modelli.\n\ngrowth_curve_model &lt;- growth(model, data = dwide)\n\n\nsummary(growth_curve_model) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 41 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         9\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                                 4.212\n  Degrees of freedom                                 5\n  P-value (Chi-square)                           0.519\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  i =~                                                \n    y0                1.000                           \n    y1                1.000                           \n    y2                1.000                           \n    y3                1.000                           \n  s =~                                                \n    y0                0.000                           \n    y1                1.000                           \n    y2                2.000                           \n    y3                3.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  i ~~                                                \n    s                 0.162    0.051    3.137    0.002\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    i                 0.501    0.049   10.263    0.000\n    s                 0.252    0.046    5.428    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .y0                0.268    0.042    6.356    0.000\n   .y1                0.237    0.022   10.713    0.000\n   .y2                0.209    0.029    7.262    0.000\n   .y3                0.299    0.066    4.556    0.000\n    i                 1.007    0.078   12.996    0.000\n    s                 1.021    0.068   14.953    0.000\n\n\n\n\ngrowth_curve_model |&gt;\n    semPaths(\n        style = \"ram\",\n        whatLabels = \"par\", edge.label.cex = .6,\n        label.prop = 0.9, edge.label.color = \"black\", rotation = 4,\n        equalizeManifests = FALSE, optimizeLatRes = TRUE,\n        node.width = 1.5,\n        edge.width = 0.5, shapeMan = \"rectangle\", shapeLat = \"ellipse\",\n        shapeInt = \"triangle\", sizeMan = 4, sizeInt = 2, sizeLat = 4,\n        curve = 2, unCol = \"#070b8c\"\n    )\n\n\n\n\n\n\n\n\nUsiamo l’oggetto creato da growth per creare un diagramma di percorso.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>74</span>  <span class='chapter-title'>LGM e modelli misti</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/06_lgm_mixed.html#comparazione-tra-modelli-a-effetti-misti-e-modelli-di-crescita-latente",
    "href": "chapters/lgm/06_lgm_mixed.html#comparazione-tra-modelli-a-effetti-misti-e-modelli-di-crescita-latente",
    "title": "74  LGM e modelli misti",
    "section": "74.5 Comparazione tra Modelli a Effetti Misti e Modelli di Crescita Latente",
    "text": "74.5 Comparazione tra Modelli a Effetti Misti e Modelli di Crescita Latente\nNell’output del metodo growth(), la sezione denominata Intercepts rappresenta in realtà gli effetti fissi all’interno del contesto di un modello a effetti misti:\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    i                 0.510    0.048   10.542    0.000\n    s                 0.234    0.046    5.133    0.000\nPuò apparire inizialmente non intuitivo riferirsi agli effetti fissi come ‘intercette’ in un modello a effetti misti. Tuttavia, questa terminologia diventa più chiara quando consideriamo la parametrizzazione del modello di crescita latente (LGM). In un modello LGM, ‘i’ rappresenta l’intercetta generale del modello (cioè, il punto di partenza medio per tutti i soggetti), mentre ‘s’ indica la pendenza media, ovvero il tasso di crescita o di cambiamento nel tempo.\nÈ interessante notare come le stime riportate qui siano molto vicine a quelle che si ottengono in un modello a effetti misti. Questa similitudine dimostra l’affinità tra i due approcci di modellazione: entrambi mirano a comprendere e quantificare sia gli effetti generali (come la tendenza media di crescita) sia le variazioni individuali all’interno di un insieme di dati longitudinali. In entrambi i casi, l’intercetta e la pendenza giocano ruoli cruciali nell’interpretazione dei modelli e nella comprensione di come i valori della variabile dipendente evolvano nel tempo.\n\nprint(fixef(mix_mod))\n\n(Intercept)        time \n  0.5015932   0.2515722 \n\n\nSi noti inoltre che le stime degli effetti fissi del modello misto sono identiche a quelle che vengono trovate usando un modello di regressione standard:\n\nlm(y1 ~ time, data = d)\n\n\nCall:\nlm(formula = y1 ~ time, data = d)\n\nCoefficients:\n(Intercept)         time  \n     0.5016       0.2516  \n\n\nConsideriamo ora le stime della varianza nel modello a crescita latente.\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  i ~~                                                \n    s                 0.220    0.050    4.371    0.000\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .y0                0.310    0.042    7.308    0.000\n   .y1                0.220    0.021   10.338    0.000\n   .y2                0.230    0.029    7.935    0.000\n   .y3                0.275    0.064    4.295    0.000\n    i                 0.973    0.076   12.854    0.000\n    s                 0.986    0.066   14.889    0.000\nConfrontiamo questi valori con quelli ottenuti dal modello misto.\n\nVarCorr(mix_mod) |&gt;\n    print()\n\n Groups   Name        Std.Dev. Corr \n subject  (Intercept) 1.01217       \n          time        1.01494  0.150\n Residual             0.49108       \n\n\nSi noti che il modello a crescita latente, per impostazione predefinita, assume una varianza eterogenea per ogni rilevazione temporale. I modelli misti, invece, per impostazione predefinita assumono la stessa varianza per ogni punto temporale. È però possibile specificare una stima separata della varianza nelle diverse rilevazioni temporali.\nSe vincoliamo le varianze ad essere uguali per ciascuna rilevazione temporale nel modello LGM, i due modelli producono delle stime identiche. La sintassi seguente viene utilizzata per forzare l’uguaglianza delle varianze in ciascuna rilevazione temporale.\n\nmodel &lt;- \"\n    # intercept and slope with fixed coefficients\n    i =~ 1*y0 + 1*y1 + 1*y2 + 1*y3\n    s =~ 0*y0 + 1*y1 + 2*y2 + 3*y3\n    y0 ~~ resvar*y0\n    y1 ~~ resvar*y1\n    y2 ~~ resvar*y2\n    y3 ~~ resvar*y3\n\"\n\nAdattiamo il nuovo modello ai dati.\n\ngrowth_curve_model &lt;- growth(model, data = dwide)\n\nEsaminiamo i risultati.\n\nsummary(growth_curve_model) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 27 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         9\n  Number of equality constraints                     3\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                                 6.180\n  Degrees of freedom                                 8\n  P-value (Chi-square)                           0.627\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  i =~                                                \n    y0                1.000                           \n    y1                1.000                           \n    y2                1.000                           \n    y3                1.000                           \n  s =~                                                \n    y0                0.000                           \n    y1                1.000                           \n    y2                2.000                           \n    y3                3.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  i ~~                                                \n    s                 0.154    0.051    3.034    0.002\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    i                 0.502    0.049   10.278    0.000\n    s                 0.252    0.046    5.423    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .y0      (rsvr)    0.241    0.011   22.361    0.000\n   .y1      (rsvr)    0.241    0.011   22.361    0.000\n   .y2      (rsvr)    0.241    0.011   22.361    0.000\n   .y3      (rsvr)    0.241    0.011   22.361    0.000\n    i                 1.022    0.076   13.502    0.000\n    s                 1.028    0.068   15.095    0.000\n\n\n\nPer lme4 abbiamo:\n\nprint(VarCorr(mix_mod), comp = \"Var\")\n\n Groups   Name        Variance Cov  \n subject  (Intercept) 1.02448       \n          time        1.03011  0.154\n Residual             0.24116       \n\n\nIn entrambi i casi, la varianza residua è uguale a 0.241 e la correlazione tra intercette e pendenze casuali è uguale a 0.154.\nInoltre, le stime dei coefficienti casuali del modello misto sono identiche a quelle delle variabili latenti.\n\ncoef(mix_mod)[[1]] |&gt; \n    head()\n\n\nA data.frame: 6 x 2\n\n\n\n(Intercept)\ntime\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n-0.3967486\n-0.9050565\n\n\n2\n-1.5328622\n-1.5945199\n\n\n3\n0.5429873\n0.8759706\n\n\n4\n0.3095727\n-0.7887484\n\n\n5\n2.0327226\n3.5319068\n\n\n6\n2.0645454\n-1.1411935\n\n\n\n\n\n\nlavPredict(growth_curve_model) |&gt;\n    head()\n\n\nA matrix: 6 x 2 of type dbl\n\n\ni\ns\n\n\n\n\n-0.3966515\n-0.9050631\n\n\n-1.5324914\n-1.5946260\n\n\n0.5430942\n0.8759036\n\n\n0.3094388\n-0.7886563\n\n\n2.0328124\n3.5317637\n\n\n2.0637121\n-1.1407804",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>74</span>  <span class='chapter-title'>LGM e modelli misti</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/06_lgm_mixed.html#riflessioni-conclusive",
    "href": "chapters/lgm/06_lgm_mixed.html#riflessioni-conclusive",
    "title": "74  LGM e modelli misti",
    "section": "74.6 Riflessioni Conclusive",
    "text": "74.6 Riflessioni Conclusive\nIn conclusione, abbiamo visto che, nel caso più semplice in cui viene assunta la stessa varianza per ogni punto temporale, i modelli LGM producono risultati identici ai modelli misti. Tuttavia, la concettualizzazione del cambiamento nei termini di un modello a crescita latente offre molti vantaggi rispetto alla descrizione dei dati nei termini dei modelli misti in quanto i modelli LGM sono più flessibili e consentono la verifica di ipotesi statistiche che non possono essere esaminate nel contesto dei modelli misti.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>74</span>  <span class='chapter-title'>LGM e modelli misti</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/03_time_effects.html",
    "href": "chapters/lgm/03_time_effects.html",
    "title": "75  Dati longitudinali",
    "section": "",
    "text": "75.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nL’obiettivo di questo capitolo è esaminare come è possibile estendere i modelli SEM per adattarli alle particolarità dei dati longitudinali. Per semplificare, cominciamo concentrandoci su due misurazioni temporali consecutive.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>75</span>  <span class='chapter-title'>Dati longitudinali</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/03_time_effects.html#misurare-il-cambiamento",
    "href": "chapters/lgm/03_time_effects.html#misurare-il-cambiamento",
    "title": "75  Dati longitudinali",
    "section": "75.2 Misurare il Cambiamento",
    "text": "75.2 Misurare il Cambiamento\nIl modo più semplice per valutare il cambiamento individuale tra due momenti temporali consiste nel calcolare la differenza tra i punteggi ottenuti nelle due occasioni. Tuttavia, questa strategia ha un limite significativo: non disponiamo del punteggio “vero” dell’individuo in ciascun momento, ma solo di una misura influenzata dall’errore di misurazione. L’errore di misurazione può ridurre notevolmente la precisione delle stime sulle differenze individuali, compromettendo l’interpretazione del cambiamento.\nPer superare questo problema negli studi longitudinali, vengono impiegati i modelli di crescita latente (Latent Growth Models, LGM), appartenenti alla famiglia dei modelli a equazioni strutturali (SEM). Questi modelli permettono di stimare traiettorie di cambiamento per ciascun individuo, separando le componenti latenti dal rumore delle misurazioni.\n\n75.2.1 Componenti del Modello di Crescita Latente\nNei LGM, si assume che ogni individuo segua una propria traiettoria di cambiamento nel tempo. I dati osservati possono essere scomposti in tre componenti principali:\n\nPunteggi latenti: rappresentano il livello individuale del costrutto in un dato momento.\nPunteggi di cambiamento latenti: indicano il cambiamento individuale nel tempo.\nCaratteristiche uniche non osservate: includono gli errori di misurazione specifici per ogni momento.\n\nL’equazione generale del modello SEM è espressa come:\n\\[\n\\Sigma = \\Lambda \\Psi \\Lambda' + \\Theta,\n\\]\ndove:\n\n$ $ rappresenta la matrice delle varianze e covarianze teoriche.\n$ $ è la matrice dei carichi fattoriali, che descrive le relazioni tra indicatori e costrutti latenti.\n$ $ indica le varianze e covarianze tra i fattori latenti.\n$ $ rappresenta le varianze residue e covarianze tra gli errori di misura.\n\n\n\n75.2.2 Struttura del Modello di Misurazione Longitudinale\nIn un modello longitudinale, si definiscono tre fattori latenti principali:\n\nUn fattore che rappresenta il livello di base del costrutto in un dato momento.\nUn fattore che rappresenta il cambiamento nel costrutto tra momenti temporali.\nUn fattore che rappresenta l’errore di misurazione specifico per ciascun momento.\n\nPer illustrare come funziona questo modello, consideriamo un costrutto misurato in due occasioni. Il punteggio osservato di un individuo in un tempo specifico può essere descritto dalla formula:\n\\[\nx_{it} = \\tau_i + (1)\\xi_1 + (t)\\xi_2 + \\delta_{it},\n\\]\ndove:\n\n\\(\\tau_i\\) è il livello iniziale dell’individuo \\(i\\),\n\\(\\xi_1\\) rappresenta il livello latente al tempo \\(t_1\\),\n\\(\\xi_2\\) rappresenta il cambiamento latente tra i due momenti,\n\\(\\delta_{it}\\) è l’errore di misurazione specifico per l’individuo \\(i\\) al tempo \\(t\\).\n\n\n\n75.2.3 Modello per Più Occasioni di Misurazione\nQuando vengono utilizzati più indicatori in ciascun momento, la struttura del modello può essere rappresentata come segue:\n\\[\n\\begin{align}\nx_{1} &= 0 + (1)\\xi_{1} + (0)\\xi_{2} + \\delta_{1} \\notag\\\\\nx_{2} &= 0 + (1)\\xi_{1} + (1)\\xi_{2} + \\delta_{2} \\notag\\\\\nx_{3} &= 0 + (1)\\xi_{1} + (2)\\xi_{2} + \\delta_{3} \\notag\\\\\nx_{4} &= 0 + (1)\\xi_{1} + (4)\\xi_{2} + \\delta_{4} \\notag\\\\\nx_{5} &= 0 + (1)\\xi_{1} + (5)\\xi_{2} + \\delta_{5} \\notag\n\\end{align}\n\\]\nInoltre, nel modello si ipotizza una correlazione tra \\(\\xi_1\\) e \\(\\xi_2\\), rappresentata dalla matrice di intercorrelazione dei fattori:\n\\[\n\\mathbf{\\Phi} =\n\\begin{bmatrix}\n\\phi_{11} & \\\\\n\\phi_{21} & \\phi_{22}\n\\end{bmatrix}\n\\]\ndove:\n\n\\(\\phi_{11}\\) è la varianza dell’intercetta latente (livello di base),\n\\(\\phi_{22}\\) è la varianza della pendenza latente (cambiamento nel tempo),\n\\(\\phi_{21}\\) è la covarianza tra intercetta e pendenza, utile per comprendere come il livello iniziale sia associato alla velocità di cambiamento.\n\n\n\n75.2.4 Correlazioni tra Varianze Residue\nUn aspetto distintivo dei modelli longitudinali è la possibilità di correlare le varianze residue degli stessi indicatori misurati in momenti diversi (ad esempio, la correlazione tra X1 al Tempo 1 e X1 al Tempo 2). Questo consente di distinguere tra le informazioni stabili del costrutto nel tempo e le variazioni specifiche di ciascun indicatore a ogni misurazione.\n\n\n75.2.5 Interpretazione e Utilità\nQuesto approccio permette di esaminare in modo approfondito lo sviluppo o il cambiamento di un costrutto latente e dei suoi indicatori nel tempo, fornendo un quadro dettagliato delle dinamiche individuali e collettive di cambiamento.\nIl modello di crescita latente definito da queste equazioni produce previsioni sulla struttura delle medie e delle covarianze dei dati osservati. Queste previsioni sono utilizzate nel contesto della modellizzazione delle equazioni strutturali per stimare i parametri e valutare l’adattamento del modello ai dati. La struttura delle covarianze prevista dal modello è:\n\\[\n\\boldsymbol{\\Sigma} = \\boldsymbol{\\Lambda} \\boldsymbol{\\Phi} \\boldsymbol{\\Lambda}' + \\boldsymbol{\\Theta}.\n\\]\nLa figura Figura 75.1 rappresenta graficamente il percorso del modello di crescita latente (LGM) che stiamo analizzando.\n\n\n\n\n\n\nFigura 75.1: Modello di crescita latente.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>75</span>  <span class='chapter-title'>Dati longitudinali</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/03_time_effects.html#la-variazione-temporale-di-positive-affect",
    "href": "chapters/lgm/03_time_effects.html#la-variazione-temporale-di-positive-affect",
    "title": "75  Dati longitudinali",
    "section": "75.3 La Variazione Temporale di Positive Affect",
    "text": "75.3 La Variazione Temporale di Positive Affect\nApplichiamo questo modello al caso in cui tre indicatori di Positive Affect (Glad, Cheerful, Happy) vengono misurati in due momenti del tempo (si veda Little (2023)).\nImportiamo i dati.\n\ndat &lt;- read.table(\n    file = \"../../data/grade7and8.long.823.dat\",\n    col.names = c(\n        \"PAT1P1\", \"PAT1P2\", \"PAT1P3\", \"NAT1P1\", \"NAT1P2\", \"NAT1P3\",\n        \"PAT2P1\", \"PAT2P2\", \"PAT2P3\", \"NAT2P1\", \"NAT2P2\", \"NAT2P3\",\n        \"PAT3P1\", \"PAT3P2\", \"PAT3P3\", \"NAT3P1\", \"NAT3P2\", \"NAT3P3\",\n        \"grade\", \"female\", \"black\", \"hispanic\", \"other\"\n    )\n)\nglimpse(dat)\n\nRows: 823\nColumns: 23\n$ PAT1P1   &lt;dbl&gt; 1.50000, 2.98116, 3.50000, 3.00000, 3.00000, 3.00000, 3.0~\n$ PAT1P2   &lt;dbl&gt; 1.50000, 2.98284, 4.00000, 3.50000, 2.50000, 2.50000, 2.5~\n$ PAT1P3   &lt;dbl&gt; 2.00000, 2.98883, 4.00000, 2.50000, 3.00000, 3.00000, 4.0~\n$ NAT1P1   &lt;dbl&gt; 2.50000, 1.56218, 1.50000, 1.50000, 1.00000, 1.50000, 1.0~\n$ NAT1P2   &lt;dbl&gt; 3.50000, 1.45688, 1.00000, 2.00000, 1.00000, 2.50000, 1.0~\n$ NAT1P3   &lt;dbl&gt; 3.00000, 1.65477, 1.00000, 1.50000, 1.00000, 2.50000, 1.0~\n$ PAT2P1   &lt;dbl&gt; 4.00000, 4.00000, 4.00000, 2.95942, 3.17170, 2.00000, 3.0~\n$ PAT2P2   &lt;dbl&gt; 4.00000, 4.00000, 2.50000, 2.99083, 2.87806, 2.00000, 3.0~\n$ PAT2P3   &lt;dbl&gt; 4.00000, 4.00000, 4.00000, 3.06670, 3.11031, 3.00000, 4.0~\n$ NAT2P1   &lt;dbl&gt; 2.00000, 1.00000, 1.00000, 1.65159, 1.65777, 2.00000, 1.0~\n$ NAT2P2   &lt;dbl&gt; 1.00000, 1.00000, 1.00000, 1.42599, 1.44804, 2.00000, 1.0~\n$ NAT2P3   &lt;dbl&gt; 2.00000, 1.00000, 1.00000, 1.67184, 1.56296, 2.00000, 1.0~\n$ PAT3P1   &lt;dbl&gt; 4.00000, 4.00000, 4.00000, 4.00000, 2.67109, 3.00000, 2.5~\n$ PAT3P2   &lt;dbl&gt; 4.00000, 4.00000, 4.00000, 3.50000, 2.85851, 2.00000, 2.0~\n$ PAT3P3   &lt;dbl&gt; 4.00000, 4.00000, 3.48114, 3.50000, 3.28099, 2.50000, 3.5~\n$ NAT3P1   &lt;dbl&gt; 1.00000, 1.00000, 1.18056, 1.00000, 1.19869, 2.00000, 1.0~\n$ NAT3P2   &lt;dbl&gt; 1.00000, 1.00000, 1.00000, 1.50000, 1.00000, 2.00000, 1.0~\n$ NAT3P3   &lt;dbl&gt; 2.50000, 1.00000, 1.62051, 1.00000, 1.00000, 3.00000, 1.0~\n$ grade    &lt;int&gt; 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ~\n$ female   &lt;int&gt; 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, ~\n$ black    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ~\n$ hispanic &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ~\n$ other    &lt;int&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ~\n\n\nLa specificazione del modello SEM longitudinale per questi dati in lavaan può essere formulata in modo simile a un modello CFA per un singolo momento del tempo. In questo caso, ci sono due fattori comuni, che chiameremo Fattore_T1 e Fattore_T2, che vengono identificati dagli indicatori misurati nei due momenti del tempo. Questi due fattori comuni sono correlati tra loro.\nTuttavia, la differenza chiave rispetto ai casi precedenti è che i fattori specifici di ciascun indicatore nei due momenti del tempo sono anche correlati tra loro. Questo significa che, oltre alla correlazione tra i fattori comuni Fattore_T1 e Fattore_T2, dobbiamo anche specificare la correlazione tra i fattori specifici dei singoli indicatori nei due momenti del tempo.\n\nmod_1 &lt;- \"\n  # Definizione dei fattori latenti al tempo T1\n  Fattore_T1 =~ NA*PAT1P1 + PAT1P2 + PAT1P3\n\n  # Definizione dei fattori latenti al tempo T2\n  Fattore_T2 =~ NA*PAT2P1 + PAT2P2 + PAT2P3\n\n  # Varianza dei fattori latenti\n  Fattore_T1 ~~ 1*Fattore_T1\n  Fattore_T2 ~~ 1*Fattore_T2\n\n  # Covarianza tra i fattori latenti\n  Fattore_T1 ~~ Fattore_T2\n\n  # Definizione degli errori di misurazione per gli indicatori al tempo T1\n  PAT1P1 ~~ PAT1P1\n  PAT1P2 ~~ PAT1P2\n  PAT1P3 ~~ PAT1P3\n\n  # Definizione degli errori di misurazione per gli indicatori al tempo T2\n  PAT2P1 ~~ PAT2P1\n  PAT2P2 ~~ PAT2P2\n  PAT2P3 ~~ PAT2P3\n\n  # Covarianze tra i residui degli item tra T1 e T2\n  PAT1P1 ~~ PAT2P1\n  PAT1P2 ~~ PAT2P2\n  PAT1P3 ~~ PAT2P3\n\n  # Opzionale: Specifica delle medie degli indicatori (intercette)\n  PAT1P1 ~ 1\n  PAT1P2 ~ 1\n  PAT1P3 ~ 1\n  PAT2P1 ~ 1\n  PAT2P2 ~ 1\n  PAT2P3 ~ 1\n\"\n\nLe covarianze tra gli errori degli indicatori corrispondenti tra T1 e T2 sono stimate, indicando potenziali correlazioni tra gli errori degli stessi indicatori nei due momenti temporali.\nIn questo modello, i carichi fattoriali e le intercettazioni non sono ancora eguagliati nel tempo, il che significa che ogni set di indicatori è libero di avere relazioni uniche con il proprio fattore latente in ciascun momento temporale.\nQuesto modello è definito “configural-invariant” perché mantiene la stessa struttura fattoriale (o configurazione) nel tempo, ma non impone ancora l’equivalenza dei parametri tra i due momenti temporali.\nIl modello configural-invariant è spesso il punto di partenza per testare l’invarianza longitudinale in SEM, poiché stabilisce una base di confronto prima di imporre vincoli più stringenti come l’invarianza dei carichi fattoriali o delle intercette nei modelli successivi.\nAdattiamo il modello ai dati.\n\nfit_1 &lt;- lavaan::sem(mod_1, data = dat, meanstructure = TRUE)\n\n\nparameterEstimates(fit_1) |&gt; print()\n\n          lhs op        rhs   est    se       z pvalue ci.lower ci.upper\n1  Fattore_T1 =~     PAT1P1 0.670 0.022  30.862  0.000    0.628    0.713\n2  Fattore_T1 =~     PAT1P2 0.661 0.021  31.241  0.000    0.619    0.702\n3  Fattore_T1 =~     PAT1P3 0.643 0.021  29.979  0.000    0.601    0.685\n4  Fattore_T2 =~     PAT2P1 0.689 0.021  32.994  0.000    0.648    0.730\n5  Fattore_T2 =~     PAT2P2 0.680 0.021  33.049  0.000    0.639    0.720\n6  Fattore_T2 =~     PAT2P3 0.639 0.021  31.155  0.000    0.598    0.679\n7  Fattore_T1 ~~ Fattore_T1 1.000 0.000      NA     NA    1.000    1.000\n8  Fattore_T2 ~~ Fattore_T2 1.000 0.000      NA     NA    1.000    1.000\n9  Fattore_T1 ~~ Fattore_T2 0.552 0.027  20.141  0.000    0.498    0.606\n10     PAT1P1 ~~     PAT1P1 0.135 0.010  12.919  0.000    0.114    0.155\n11     PAT1P2 ~~     PAT1P2 0.121 0.010  12.308  0.000    0.102    0.141\n12     PAT1P3 ~~     PAT1P3 0.145 0.010  14.046  0.000    0.125    0.165\n13     PAT2P1 ~~     PAT2P1 0.102 0.008  12.160  0.000    0.086    0.119\n14     PAT2P2 ~~     PAT2P2 0.098 0.008  11.997  0.000    0.082    0.114\n15     PAT2P3 ~~     PAT2P3 0.125 0.009  14.711  0.000    0.108    0.142\n16     PAT1P1 ~~     PAT2P1 0.012 0.006   1.946  0.052    0.000    0.025\n17     PAT1P2 ~~     PAT2P2 0.005 0.006   0.884  0.377   -0.006    0.017\n18     PAT1P3 ~~     PAT2P3 0.011 0.006   1.781  0.075   -0.001    0.024\n19     PAT1P1 ~1            2.992 0.027 112.316  0.000    2.940    3.044\n20     PAT1P2 ~1            2.896 0.026 111.210  0.000    2.845    2.947\n21     PAT1P3 ~1            3.112 0.026 119.527  0.000    3.061    3.163\n22     PAT2P1 ~1            3.002 0.026 113.400  0.000    2.950    3.054\n23     PAT2P2 ~1            2.909 0.026 111.532  0.000    2.858    2.960\n24     PAT2P3 ~1            3.127 0.025 122.862  0.000    3.077    3.177\n25 Fattore_T1 ~1            0.000 0.000      NA     NA    0.000    0.000\n26 Fattore_T2 ~1            0.000 0.000      NA     NA    0.000    0.000\n\n\n\nsemPaths(fit_1,\n    whatLabels = \"std\",\n    sizeMan = 10,\n    edge.label.cex = 0.9,\n    style = \"mx\",\n    nCharNodes = 0, nCharEdges = 0\n)\n\n\n\n\n\n\n\n\n\nfitMeasures(fit_1, c(\"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt; print()\n\n  cfi   tli rmsea  srmr \n1.000 0.999 0.016 0.010 \n\n\nPotremmo pensare che modello di baseline (con cui possono essere confrontati i modelli che descrivono il cambiamento temporale) sia semplicemente il modello in cui non sono permesse covarianze.\n\nmod_2 &lt;- \"\n  # Definizione dei fattori latenti al tempo T1\n  Fattore_T1 =~ NA*PAT1P1 + PAT1P2 + PAT1P3\n\n  # Definizione dei fattori latenti al tempo T2\n  Fattore_T2 =~ NA*PAT2P1 + PAT2P2 + PAT2P3\n\n  # Varianza dei fattori latenti\n  Fattore_T1 ~~ 1*Fattore_T1\n  Fattore_T2 ~~ 1*Fattore_T2\n\n  # Covarianza tra i fattori latenti\n  Fattore_T1 ~~ 0*Fattore_T2\n\n  # Definizione degli errori di misurazione per gli indicatori al tempo T1\n  PAT1P1 ~~ PAT1P1\n  PAT1P2 ~~ PAT1P2\n  PAT1P3 ~~ PAT1P3\n\n  # Definizione degli errori di misurazione per gli indicatori al tempo T2\n  PAT2P1 ~~ PAT2P1\n  PAT2P2 ~~ PAT2P2\n  PAT2P3 ~~ PAT2P3\n\n  # Covarianze tra i residui degli item tra T1 e T2\n  # PAT1P1 ~~ PAT2P1\n  # PAT1P2 ~~ PAT2P2\n  # PAT1P3 ~~ PAT2P3\n\n  # Opzionale: Specifica delle medie degli indicatori (intercette)\n  PAT1P1 ~ 1\n  PAT1P2 ~ 1\n  PAT1P3 ~ 1\n  PAT2P1 ~ 1\n  PAT2P2 ~ 1\n  PAT2P3 ~ 1\n\"\n\n\nfit_2 &lt;- lavaan::sem(mod_2, data = dat, meanstructure = TRUE)\n\n\nsemPaths(fit_2,\n    whatLabels = \"std\",\n    sizeMan = 10,\n    edge.label.cex = 0.9,\n    style = \"mx\",\n    nCharNodes = 0, nCharEdges = 0\n)\n\n\n\n\n\n\n\n\n\nparameterEstimates(fit_2) |&gt; print()\n\n          lhs op        rhs   est    se       z pvalue ci.lower ci.upper\n1  Fattore_T1 =~     PAT1P1 0.670 0.022  30.720      0    0.627    0.713\n2  Fattore_T1 =~     PAT1P2 0.658 0.021  30.943      0    0.616    0.700\n3  Fattore_T1 =~     PAT1P3 0.647 0.021  30.105      0    0.605    0.689\n4  Fattore_T2 =~     PAT2P1 0.689 0.021  32.917      0    0.648    0.730\n5  Fattore_T2 =~     PAT2P2 0.681 0.021  33.073      0    0.641    0.722\n6  Fattore_T2 =~     PAT2P3 0.636 0.021  30.961      0    0.596    0.676\n7  Fattore_T1 ~~ Fattore_T1 1.000 0.000      NA     NA    1.000    1.000\n8  Fattore_T2 ~~ Fattore_T2 1.000 0.000      NA     NA    1.000    1.000\n9  Fattore_T1 ~~ Fattore_T2 0.000 0.000      NA     NA    0.000    0.000\n10     PAT1P1 ~~     PAT1P1 0.135 0.011  12.641      0    0.114    0.156\n11     PAT1P2 ~~     PAT1P2 0.125 0.010  12.310      0    0.105    0.145\n12     PAT1P3 ~~     PAT1P3 0.141 0.010  13.503      0    0.120    0.161\n13     PAT2P1 ~~     PAT2P1 0.102 0.009  11.872      0    0.085    0.119\n14     PAT2P2 ~~     PAT2P2 0.097 0.008  11.609      0    0.080    0.113\n15     PAT2P3 ~~     PAT2P3 0.127 0.009  14.716      0    0.110    0.144\n16     PAT1P1 ~1            2.992 0.027 112.344      0    2.940    3.044\n17     PAT1P2 ~1            2.896 0.026 111.246      0    2.845    2.947\n18     PAT1P3 ~1            3.112 0.026 119.412      0    3.061    3.163\n19     PAT2P1 ~1            3.002 0.026 113.387      0    2.950    3.054\n20     PAT2P2 ~1            2.909 0.026 111.468      0    2.858    2.960\n21     PAT2P3 ~1            3.127 0.025 123.031      0    3.077    3.177\n22 Fattore_T1 ~1            0.000 0.000      NA     NA    0.000    0.000\n23 Fattore_T2 ~1            0.000 0.000      NA     NA    0.000    0.000\n\n\n\nfitMeasures(fit_2, c(\"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt; print()\n\n  cfi   tli rmsea  srmr \n0.931 0.886 0.187 0.253 \n\n\nTuttavia, Little (2023) fa notare che, nel contesto dei disegni longitudinali, il modello di base adeguato prevede che vengano aggiunte al modello nullo delle aspettative aggiuntive, specificatamente che le medie e le varianze rimangano invariate nel tempo. Questa specificazione ampliata del modello nullo fornisce il confronto appropriato per analizzare e interpretare i dati longitudinali.\n\nmod_3 &lt;- \"\n  # Definizione dei fattori latenti al tempo T1\n  Fattore_T1 =~ NA*PAT1P1 + b1*PAT1P2 + b2*PAT1P3\n\n  # Definizione dei fattori latenti al tempo T2\n  Fattore_T2 =~ NA*PAT2P1 + b1*PAT2P2 + b2*PAT2P3\n\n  # Varianza dei fattori latenti\n  Fattore_T1 ~~ c1*Fattore_T1\n  Fattore_T2 ~~ c1*Fattore_T2\n\n  # Covarianza tra i fattori latenti (assumendo che sia 0)\n  Fattore_T1 ~~ 0*Fattore_T2\n\n  # Definizione degli errori di misurazione per gli indicatori al tempo T1\n  PAT1P1 ~~ a1*PAT1P1\n  PAT1P2 ~~ a2*PAT1P2\n  PAT1P3 ~~ a3*PAT1P3\n\n  # Definizione degli errori di misurazione per gli indicatori al tempo T2\n  PAT2P1 ~~ a1*PAT2P1\n  PAT2P2 ~~ a2*PAT2P2\n  PAT2P3 ~~ a3*PAT2P3\n\n  # Specifica delle medie degli indicatori (intercettazioni) uguali tra i due tempi\n  # PAT1P1 ~ m1\n  # PAT1P2 ~ m2\n  # PAT1P3 ~ m3\n  # PAT2P1 ~ m1\n  # PAT2P2 ~ m2\n  # PAT2P3 ~ m3\n\"\n\n\nfit_3 &lt;- lavaan::sem(mod_3, data = dat, meanstructure = TRUE)\n\n\nsemPaths(fit_3,\n    whatLabels = \"std\",\n    sizeMan = 10,\n    edge.label.cex = 0.9,\n    style = \"mx\",\n    nCharNodes = 0, nCharEdges = 0\n)\n\n\n\n\n\n\n\n\n\nparameterEstimates(fit_3) |&gt; print()\n\n          lhs op        rhs label   est    se       z pvalue ci.lower\n1  Fattore_T1 =~     PAT1P1       0.863 0.018  47.730      0    0.828\n2  Fattore_T1 =~     PAT1P2    b1 0.854 0.012  71.968      0    0.831\n3  Fattore_T1 =~     PAT1P3    b2 0.818 0.012  66.282      0    0.794\n4  Fattore_T2 =~     PAT2P1       0.871 0.018  48.118      0    0.835\n5  Fattore_T2 =~     PAT2P2    b1 0.854 0.012  71.968      0    0.831\n6  Fattore_T2 =~     PAT2P3    b2 0.818 0.012  66.282      0    0.794\n7  Fattore_T1 ~~ Fattore_T1    c1 0.614 0.014  44.667      0    0.587\n8  Fattore_T2 ~~ Fattore_T2    c1 0.614 0.014  44.667      0    0.587\n9  Fattore_T1 ~~ Fattore_T2       0.000 0.000      NA     NA    0.000\n10     PAT1P1 ~~     PAT1P1    a1 0.119 0.007  17.380      0    0.105\n11     PAT1P2 ~~     PAT1P2    a2 0.111 0.007  16.933      0    0.098\n12     PAT1P3 ~~     PAT1P3    a3 0.134 0.007  19.941      0    0.121\n13     PAT2P1 ~~     PAT2P1    a1 0.119 0.007  17.380      0    0.105\n14     PAT2P2 ~~     PAT2P2    a2 0.111 0.007  16.933      0    0.098\n15     PAT2P3 ~~     PAT2P3    a3 0.134 0.007  19.941      0    0.121\n16     PAT1P1 ~1                  2.992 0.026 113.076      0    2.940\n17     PAT1P2 ~1                  2.896 0.026 111.098      0    2.844\n18     PAT1P3 ~1                  3.112 0.026 120.907      0    3.062\n19     PAT2P1 ~1                  3.002 0.027 112.656      0    2.949\n20     PAT2P2 ~1                  2.909 0.026 111.616      0    2.858\n21     PAT2P3 ~1                  3.127 0.026 121.471      0    3.076\n22 Fattore_T1 ~1                  0.000 0.000      NA     NA    0.000\n23 Fattore_T2 ~1                  0.000 0.000      NA     NA    0.000\n   ci.upper\n1     0.898\n2     0.878\n3     0.842\n4     0.906\n5     0.878\n6     0.842\n7     0.641\n8     0.641\n9     0.000\n10    0.132\n11    0.123\n12    0.147\n13    0.132\n14    0.123\n15    0.147\n16    3.044\n17    2.947\n18    3.163\n19    3.054\n20    2.960\n21    3.177\n22    0.000\n23    0.000\n\n\n\nfitMeasures(fit_3, c(\"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt; print()\n\n  cfi   tli rmsea  srmr \n0.927 0.916 0.160 0.254 \n\n\nPossiamo ora fare il confronto tra il modello di cambiamento latente e l’appropriato modello di confronto.\n\nlavTestLRT(fit_1, fit_3) |&gt; print()\n\n\nChi-Squared Difference Test\n\n      Df    AIC    BIC    Chisq Chisq diff  RMSEA Df diff Pr(&gt;Chisq)    \nfit_1  5 7427.8 7531.4   6.0645                                         \nfit_3 13 7693.5 7759.5 287.8078     281.74 0.2039       8  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nÈ evidente che, nel contesto di questi dati, un modello che presuma l’assenza di qualsiasi cambiamento è completamente inadeguato.\n\n\n\n\nLittle, T. D. (2023). Longitudinal structural equation modeling. Guilford Press.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>75</span>  <span class='chapter-title'>Dati longitudinali</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/05_intro_panel.html",
    "href": "chapters/lgm/05_intro_panel.html",
    "title": "76  Specificare e Interpretare un Modello Longitudinale",
    "section": "",
    "text": "76.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nNel presente capitolo esploreremo i modelli panel longitudinali tradizionali, con un’attenzione particolare alla struttura simplex e alle tecniche per ottimizzare, interpretare e arricchire questi modelli con covariate e analisi degli effetti indiretti.\nI modelli panel e i modelli SEM longitudinali si concentrano sulle relazioni predittive tra variabili latenti (ad esempio, atteggiamenti o abilità) e sulle loro variazioni nel tempo. Sebbene spesso usati come sinonimi, i modelli panel differiscono dai modelli di Confirmatory Factor Analysis (CFA) longitudinali: mentre i CFA analizzano la stabilità dei livelli medi dei costrutti, i modelli panel esplorano le interazioni dinamiche tra variabili nel corso del tempo. Un’importante distinzione va fatta anche con i modelli di crescita latente (LGM), che hanno l’obiettivo di mappare l’evoluzione temporale dei livelli medi di un costrutto, come il monitoraggio dello sviluppo di una competenza specifica.\nInoltre, le relazioni di regressione nei modelli panel suggeriscono un’interpretazione causale, ma questa deve essere affrontata con cautela. La causalità in questi modelli è implicata quando si osservano effetti predittivi coerenti nel tempo, ma è essenziale che i dati siano raccolti con rigore per permettere inferenze causali. Un elemento fondamentale è il controllo delle variabili confondenti, che permette di ridurre il rischio di bias e di migliorare la robustezza delle inferenze.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>76</span>  <span class='chapter-title'>Specificare e Interpretare un Modello Longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/05_intro_panel.html#il-processo-di-cambiamento-simplex",
    "href": "chapters/lgm/05_intro_panel.html#il-processo-di-cambiamento-simplex",
    "title": "76  Specificare e Interpretare un Modello Longitudinale",
    "section": "76.2 Il Processo di Cambiamento Simplex",
    "text": "76.2 Il Processo di Cambiamento Simplex\nUn modello efficace per rappresentare il cambiamento continuo e graduale nel tempo è la struttura simplex. Questa struttura si basa sull’assunto che gli individui cambino a un ritmo stabile, con influenze esterne minime. Nel modello simplex, la correlazione tra punti temporali decresce in modo prevedibile, secondo una progressione graduale. La Tabella 1 illustra una struttura di correlazione simplex in cui la stabilità decresce col passare del tempo.\nTabella 1.  Esempi di Strutture di Correlazione Simplex.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nT1\nT2\nT3\nT4\nT5\nT6\nT7\nT8\n\n\n\n\nT1\n–\n.800\n.640\n.512\n.410\n.328\n.262\n.210\n\n\nT2\n.528\n–\n.800\n.640\n.512\n.410\n.328\n.262\n\n\nT3\n.279\n.528\n–\n.800\n.640\n.512\n.410\n.328\n\n\nT4\n.147\n.279\n.528\n–\n.800\n.640\n.512\n.410\n\n\nT5\n.078\n.147\n.279\n.528\n–\n.800\n.640\n.512\n\n\nT6\n.041\n.078\n.147\n.279\n.528\n–\n.800\n.640\n\n\nT7\n.022\n.041\n.078\n.147\n.279\n.528\n–\n.800\n\n\nT8\n.011\n.022\n.041\n.078\n.147\n.279\n.528\n–\n\n\n\nNota: Le correlazioni sopra la diagonale sono basate sull’analogia del mescolamento delle carte con una correlazione autoregressiva di .528. Le correlazioni sotto la diagonale si basano su una stabilità iniziale più elevata (.80), indicando una persistenza più forte nel tempo.\nNel modello simplex, il cambiamento graduale e stabile è rappresentato da un coefficiente di stabilità costante tra punti temporali consecutivi, simile alla correlazione tra l’ordine delle carte in un mazzo dopo diverse mescolate.\nL’esempio del mazzo di carte mescolato è una metafora utile per comprendere come funziona una struttura di correlazione simplex e il concetto di autoregressività nei modelli longitudinali. Immaginiamo di avere un mazzo di carte perfettamente ordinato, in cui ogni carta ha una posizione specifica. Ogni volta che mescoliamo il mazzo, l’ordine cambia, ma non in modo totalmente casuale: la disposizione iniziale ha ancora una certa influenza sull’ordine risultante dopo la mescolata.\n\n76.2.1 Mescolamento e Correlazione\nSe consideriamo l’ordine delle carte prima e dopo una singola mescolata, possiamo calcolare la correlazione tra la posizione delle carte iniziale e quella dopo il mescolamento. Una sola mescolata modifica la disposizione delle carte, ma mantiene una certa somiglianza con l’ordine iniziale: diciamo, per esempio, che la correlazione è di 0.528. Questa correlazione rappresenta la stabilità del cambiamento: dopo una singola mescolata, le carte non sono ancora completamente in un ordine casuale.\nOgni successiva mescolata riduce ulteriormente questa correlazione. Dopo una seconda mescolata, la correlazione tra l’ordine originale e il nuovo ordine sarà inferiore, ad esempio 0.279. Con il terzo mescolamento, la correlazione continua a decrescere, e così via. Dopo circa sette mescolate perfette, l’ordine diventa quasi del tutto casuale, con una correlazione vicino a 0 rispetto all’ordine iniziale.\n\n\n76.2.2 Cosa Rappresenta nel Contesto dei Modelli Longitudinali\nIn un modello longitudinale con struttura simplex, ogni “mescolata” rappresenta un passaggio temporale in cui un fenomeno cambia gradualmente ma in modo prevedibile. La correlazione tra i punti temporali successivi diminuisce man mano che ci si allontana dal punto di partenza, proprio come la correlazione dell’ordine delle carte diminuisce con ogni mescolata.\n\nCorrelazione tra punti temporali consecutivi: rappresenta la stabilità immediata del costrutto. Più è alta la correlazione tra misurazioni consecutive, maggiore è la stabilità del fenomeno nel tempo.\nCorrelazione tra punti temporali distanti: rappresenta quanto il fenomeno rimanga stabile su periodi più lunghi. Una diminuzione graduale della correlazione, come nell’esempio del mazzo di carte, è tipica di processi che cambiano in modo costante ma senza grandi sconvolgimenti improvvisi.\n\nIn conclusione, l’esempio del mazzo di carte ci aiuta a visualizzare come un modello simplex cattura il cambiamento graduale e prevedibile in un processo. Ogni passaggio temporale influenza il successivo, ma con il tempo questa influenza diminuisce, portando a una correlazione minore tra i punti temporali distanti.\nQuesta struttura è utile nei modelli panel longitudinali perché descrive una dinamica di cambiamento continua e coerente, tipica di molti fenomeni psicologici e sociali che evolvono in modo graduale e prevedibile nel tempo.\n\n\n\n\n\n\nFigura 76.1: Stime dei parametri standardizzati dal modello simplex di mazzi di carte mescolati consecutivamente. Nota. Queste stime dei parametri provengono da un modello adattato ai dati nella tabella precedente. Questo modello ha 21 gradi di libertà e un adattamento perfetto del modello. Le correlazioni tra i mazzi separati da più di una mescolata sono riprodotte tracciando i percorsi di regressione tra ciascun mazzo consecutivo. Le linee tratteggiate mostrano le correlazioni riprodotte con l’ordine iniziale del mazzo. (Figura tratta da Little, 2023)\n\n\n\n\n\n76.2.3 Modelli Simplex e Modelli Autoregressivi (AR1 e AR2)\nIn termini formali, possiamo dire che la struttura simplex può essere vista come un’istanza di un modello autoregressivo di primo ordine (AR1), in cui ogni punto temporale è correlato solo con il precedente. Nei modelli AR1, l’effetto di ogni variabile dipende unicamente dalla sua osservazione immediatamente precedente. Per processi che mostrano maggiore persistenza nel tempo, si può invece adottare un modello autoregressivo di secondo ordine (AR2), dove ogni punto è influenzato non solo dal precedente, ma anche dal punto ancora precedente.\nIl modello AR2 suggerisce che l’influenza persiste per due passaggi temporali, implicando una stabilità più duratura rispetto al modello AR1. Questo approccio è utile per rappresentare processi in cui l’effetto di un evento non si dissipa immediatamente, ma ha un’influenza estesa nel tempo.\n\n\n76.2.4 Applicazioni della Struttura Simplex nella Ricerca Psicologica\nIn psicologia e scienze sociali, il modello simplex è frequentemente usato per studiare processi di cambiamento in campioni longitudinali. La semplicità di questa struttura la rende una scelta ideale per rappresentare fenomeni evolutivi graduali, come lo sviluppo di competenze o il cambiamento di atteggiamenti. La struttura simplex può anche essere estesa con l’inclusione di variabili contestuali, il che ne aumenta la flessibilità senza compromettere la chiarezza.\nPer comprendere la natura di un processo di cambiamento, è essenziale che il modello predittivo catturi correttamente il ritmo del cambiamento stesso. Una frequenza di misurazione adeguata permette di rilevare con precisione la velocità e la consistenza delle variazioni, aumentando la validità delle inferenze che si possono trarre.\nIn sintesi, i modelli panel longitudinali e la struttura simplex offrono potenti strumenti per analizzare il cambiamento e le relazioni temporali nei dati longitudinali. La struttura simplex, in particolare, è una rappresentazione versatile ed efficace dei processi di cambiamento graduale, utile per studiare fenomeni psicologici ed evolutivi in modo teoricamente informato e statisticamente robusto.\n\n\n76.2.5 Modello Simplex per il Mescolamento di Carte\nEsaminiamo qui di seguto l’implementazione del modello Simplex proposta da Little (2023) per i dati artificiali relativi all’esempio del mazzo di carte discusso in precedenza.\n\ntri_corr &lt;- c(\n    1, rep(0, 7),\n    0.523, 1, rep(0, 6),\n    0.279, 0.523, 1, rep(0, 5),\n    0.147, 0.279, 0.523, 1, rep(0, 4),\n    0.078, 0.147, 0.279, 0.523, 1, rep(0, 3),\n    0.041, 0.078, 0.147, 0.279, 0.523, 1, rep(0, 2),\n    0.022, 0.041, 0.078, 0.147, 0.279, 0.523, 1, 0,\n    0.011, 0.022, 0.041, 0.078, 0.147, 0.279, 0.523, 1\n)\nupper &lt;- matrix(tri_corr, 8, byrow = FALSE)\nlower &lt;- matrix(tri_corr, 8, byrow = TRUE)\nmycorr &lt;- upper + lower - diag(8)\n\nrownames(mycorr) &lt;- c(\"Time1\", \"Time2\", \"Time3\", \"Time4\", \"Time5\", \"Time6\", \"Time7\", \"Time8\")\ncolnames(mycorr) &lt;- c(\"Time1\", \"Time2\", \"Time3\", \"Time4\", \"Time5\", \"Time6\", \"Time7\", \"Time8\")\nmynob &lt;- 166\n\nmycorr |&gt; print()\n\n      Time1 Time2 Time3 Time4 Time5 Time6 Time7 Time8\nTime1 1.000 0.523 0.279 0.147 0.078 0.041 0.022 0.011\nTime2 0.523 1.000 0.523 0.279 0.147 0.078 0.041 0.022\nTime3 0.279 0.523 1.000 0.523 0.279 0.147 0.078 0.041\nTime4 0.147 0.279 0.523 1.000 0.523 0.279 0.147 0.078\nTime5 0.078 0.147 0.279 0.523 1.000 0.523 0.279 0.147\nTime6 0.041 0.078 0.147 0.279 0.523 1.000 0.523 0.279\nTime7 0.022 0.041 0.078 0.147 0.279 0.523 1.000 0.523\nTime8 0.011 0.022 0.041 0.078 0.147 0.279 0.523 1.000\n\n\n\nmod6_2 &lt;- \"\n    Time2 ~ Time1\n    Time3 ~ Time2\n    Time4 ~ Time3\n    Time5 ~ Time4\n    Time6 ~ Time5\n    Time7 ~ Time6\n    Time8 ~ Time7\n\n    Time1 ~~ 1*Time1\n    Time2 ~~ Time2\n    Time3 ~~ Time3\n    Time4 ~~ Time4\n    Time5 ~~ Time5\n    Time6 ~~ Time6\n    Time7 ~~ Time7\n    Time8 ~~ Time8\n\"\n\n\nfit6_2 &lt;- lavaan(mod6_2, sample.cov = mycorr, sample.nobs = mynob, fixed.x = FALSE)\n\n\nparameterEstimates(fit6_2) |&gt; print()\n\n     lhs op   rhs   est    se     z pvalue ci.lower ci.upper\n1  Time2  ~ Time1 0.523 0.066 7.930      0    0.394    0.652\n2  Time3  ~ Time2 0.523 0.066 7.912      0    0.393    0.653\n3  Time4  ~ Time3 0.523 0.066 7.908      0    0.393    0.653\n4  Time5  ~ Time4 0.523 0.066 7.906      0    0.393    0.653\n5  Time6  ~ Time5 0.523 0.066 7.906      0    0.393    0.653\n6  Time7  ~ Time6 0.523 0.066 7.906      0    0.393    0.653\n7  Time8  ~ Time7 0.523 0.066 7.906      0    0.393    0.653\n8  Time1 ~~ Time1 1.000 0.000    NA     NA    1.000    1.000\n9  Time2 ~~ Time2 0.722 0.079 9.110      0    0.567    0.877\n10 Time3 ~~ Time3 0.722 0.079 9.110      0    0.567    0.877\n11 Time4 ~~ Time4 0.722 0.079 9.110      0    0.567    0.877\n12 Time5 ~~ Time5 0.722 0.079 9.110      0    0.567    0.877\n13 Time6 ~~ Time6 0.722 0.079 9.110      0    0.567    0.877\n14 Time7 ~~ Time7 0.722 0.079 9.110      0    0.567    0.877\n15 Time8 ~~ Time8 0.722 0.079 9.110      0    0.567    0.877\n\n\nNel commentare il modello Simplex specificato, si può osservare che, per i dati artificiali in questione, la stima della correlazione tra costrutti latenti in momenti successivi risulta costante, con un valore di 0.523. Questo dato è in linea con i risultati ottenuti da Little (2023), che riporta una correlazione di 0.528. È importante notare la consistenza in queste stime, indicativa di una relazione stabile nel tempo tra i costrutti.\nInoltre, il modello mostra che la varianza delle variabili latenti rimane relativamente costante nel tempo, con un valore di 0.722. Questo suggerisce che, nonostante il passare del tempo e i possibili cambiamenti nei costrutti, la quantità di varianza che essi spiegano rimane simile. Un’eccezione a questo schema si trova nella varianza al Tempo 1, che è stata fissata a 1. Questa scelta metodologica è comune in molti modelli di serie temporali per stabilire un punto di riferimento o una scala di misurazione per le varianze nei tempi successivi.\nRifocalizziamoci sulle correlazioni nella parte inferiore della diagonale della Tabella 1, dove possiamo osservare una stabilità piuttosto elevata tra punti temporali successivi, con una correlazione di 0.80 tra ciascun punto temporale e il successivo. Tuttavia, all’aumentare dell’intervallo tra le misurazioni, la correlazione tra punti temporali distanti diminuisce in modo graduale e prevedibile. Ad esempio, nella tabella, la correlazione tra Tempo 1 e Tempo 3 è di 0.64, lo stesso valore che troviamo tra Tempo 2 e Tempo 4, tra Tempo 5 e Tempo 7, e così via per ogni coppia di punti temporali separati da uno spazio temporale intermedio. Alla massima distanza, la correlazione tra Tempo 1 e Tempo 8 è ancora leggermente positiva, pari a 0.210; con un numero crescente di punti temporali, questa correlazione si avvicinerebbe gradualmente a zero.\nQuesto schema di correlazioni evidenzia che, mentre la stabilità a breve termine (tra punti temporali adiacenti) è elevata, essa diminuisce all’aumentare della distanza tra le misurazioni. Questo riflette una riduzione dell’influenza o della connessione tra i costrutti latenti misurati a intervalli temporali più lunghi.\nNel triangolo superiore della tabella, la stabilità tra punti temporali adiacenti è più bassa, con una correlazione di 0.528. Qui, i punti temporali bi-contigui (separati da un intervallo intermedio) si correlano a 0.279 e la correlazione tra i punti più distanti, da Tempo 1 a Tempo 8, scende a 0.011, praticamente nulla. Entrambi questi schemi riflettono un tasso costante di cambiamento e sono ben rappresentati da un modello autoregressivo simplex.\nUn modello autoregressivo simplex è in grado di riprodurre tutte queste correlazioni attraverso effetti indiretti: nel modello, l’influenza del Tempo 1 sul Tempo 8 viene trasmessa indirettamente attraverso una sequenza di influenze dirette da un punto temporale al successivo (es., da Tempo 1 a Tempo 2, da Tempo 2 a Tempo 3, e così via fino a Tempo 8). Questo passaggio continuo di influenze permette di riprodurre lo schema di correlazioni osservato nella Tabella 1. Utilizzando le regole di tracciamento dei percorsi in un modello autoregressivo, possiamo osservare come questo schema di correlazioni diminuisca progressivamente, evidenziando il modo in cui il modello rappresenta il declino della connessione tra i punti temporali man mano che aumenta la distanza tra essi.\n\n0.523^{2:7} |&gt;\n    round(3) |&gt;\n    print()\n\n[1] 0.274 0.143 0.075 0.039 0.020 0.011\n\n\nIl modello presentato nella (little-fig-simplex?) è un modello Simplex univariato perché include un solo costrutto, rappresentato in più punti temporali. È importante notare che, in un modello come questo, si verifica e si garantisce la forte invarianza fattoriale del modello di misurazione. Inoltre, i residui corrispondenti presentano unicità correlate nel tempo per ogni occorrenza dello stesso indicatore. I coefficienti di percorso in questo modello riproducono perfettamente le correlazioni nella parte superiore della Tabella 1.\nLo stesso modello, quando applicato all’altro insieme di correlazioni nella Tabella 1, riprodurrebbe altrettanto perfettamente il pattern di correlazione. Gli effetti diretti in ciascun punto temporale adiacente sarebbero di .8, mentre l’effetto indiretto sarebbe il prodotto multiplo dei coefficienti di percorso diretti. Questo approccio mette in evidenza come le correlazioni tra punti temporali più lontani siano il risultato di una serie di influenze dirette che si susseguono nel tempo.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>76</span>  <span class='chapter-title'>Specificare e Interpretare un Modello Longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/05_intro_panel.html#basi-di-un-modello-panel",
    "href": "chapters/lgm/05_intro_panel.html#basi-di-un-modello-panel",
    "title": "76  Specificare e Interpretare un Modello Longitudinale",
    "section": "76.3 Basi di un Modello Panel",
    "text": "76.3 Basi di un Modello Panel\nConsideriamo ora il modello panel. Nella costruzione di un modello panel, le associazioni tra variabili osservate in momenti diversi sono spesso trasformate in percorsi di regressione direzionali. Le covarianze tra variabili al Tempo 1 sono generalmente viste come associazioni “a ordine zero,” ovvero associazioni non controllate per altre variabili. Nei momenti successivi, le covarianze tra variabili rappresentano invece varianze residue, o “fattori di disturbo,” ossia componenti di varianza non spiegate dagli effetti temporali precedenti.\nUn concetto chiave nei modelli panel è il percorso autoregressivo, che rappresenta la relazione predittiva tra lo stesso costrutto in momenti successivi. In aggiunta, i modelli panel possono includere effetti incrociati ritardati (o cross-lagged), dove una variabile predice un’altra variabile in un momento futuro. Entrambi questi tipi di percorso permettono di osservare le dinamiche temporali e la persistenza di influenze tra variabili.\n\n\n\n\n\n\nFigura 76.2: Etichette dei parametri per tre punti temporali con affetto positivo e affetto negativo: Un’analisi di base del modello panel direzionale. Nota. Si consente l’associazione delle varianze residue tra gli indicatori corrispondenti nel tempo. (Figura adattata da Little, 2023)\n\n\n\n\n76.3.1 Modello Panel per Affetto Positivo e Negativo\nLa figura (little-fig-simplex?) approfondisce l’uso dei modelli longitudinali tramite un esempio di modello CFA per studiare l’affetto positivo e negativo negli adolescenti. A differenza di un’analisi limitata a soli due punti temporali, questo esempio illustra una configurazione più complessa, in cui i due costrutti (affetto positivo e negativo) vengono misurati in tre momenti distinti. Questa struttura a più punti temporali permette di esaminare come i livelli di affetto positivo e negativo cambiano nel tempo e di osservare le interazioni tra i costrutti lungo diverse fasi della misurazione.\nI dati sono i seguenti.\n\ndat &lt;- read.table(\n    file = \"../../data/grade7and8.long.823.dat\",\n    col.names = c(\n        \"PAT1P1\", \"PAT1P2\", \"PAT1P3\", \"NAT1P1\", \"NAT1P2\", \"NAT1P3\",\n        \"PAT2P1\", \"PAT2P2\", \"PAT2P3\", \"NAT2P1\", \"NAT2P2\", \"NAT2P3\",\n        \"PAT3P1\", \"PAT3P2\", \"PAT3P3\", \"NAT3P1\", \"NAT3P2\", \"NAT3P3\",\n        \"grade\", \"female\", \"black\", \"hispanic\", \"other\"\n    )\n)\n\n\npsych::describe(dat[, 1:18])\n\n\nA psych: 18 x 13\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nPAT1P1\n1\n823\n2.991885\n0.7644692\n3.00000\n3.042751\n0.7413000\n1\n4\n3\n-0.4591316\n-0.3486015\n0.02664772\n\n\nPAT1P2\n2\n823\n2.895543\n0.7471531\n3.00000\n2.919860\n0.7413000\n1\n4\n3\n-0.2986814\n-0.3732379\n0.02604412\n\n\nPAT1P3\n3\n823\n3.112289\n0.7481646\n3.00000\n3.175932\n0.7413000\n1\n4\n3\n-0.5761073\n-0.2344880\n0.02607938\n\n\nNAT1P1\n4\n823\n1.706311\n0.7057569\n1.50000\n1.596773\n0.7413000\n1\n4\n3\n1.2112142\n1.2584152\n0.02460114\n\n\nNAT1P2\n5\n823\n1.450148\n0.6576346\n1.00000\n1.312657\n0.0000000\n1\n4\n3\n1.7700996\n3.1319596\n0.02292370\n\n\nNAT1P3\n6\n823\n1.453063\n0.6678318\n1.00000\n1.311948\n0.0000000\n1\n4\n3\n1.8185647\n3.3558790\n0.02327915\n\n\nPAT2P1\n7\n823\n3.001628\n0.7599034\n3.00000\n3.044683\n0.7413000\n1\n4\n3\n-0.4055187\n-0.5083138\n0.02648857\n\n\nPAT2P2\n8\n823\n2.909043\n0.7491414\n3.00000\n2.936239\n0.7413000\n1\n4\n3\n-0.2667719\n-0.5077469\n0.02611343\n\n\nPAT2P3\n9\n823\n3.126799\n0.7295410\n3.09211\n3.189561\n0.8778623\n1\n4\n3\n-0.6282730\n-0.1499079\n0.02543020\n\n\nNAT2P1\n10\n823\n1.695210\n0.6614440\n1.50000\n1.606124\n0.7413000\n1\n4\n3\n1.1219909\n1.2251472\n0.02305649\n\n\nNAT2P2\n11\n823\n1.537798\n0.6225102\n1.50000\n1.429653\n0.7413000\n1\n4\n3\n1.3883623\n1.9372133\n0.02169934\n\n\nNAT2P3\n12\n823\n1.580027\n0.6499109\n1.50000\n1.471439\n0.7413000\n1\n4\n3\n1.3401267\n1.8529497\n0.02265447\n\n\nPAT3P1\n13\n823\n2.886528\n0.7823545\n3.00000\n2.917427\n0.7413000\n1\n4\n3\n-0.2178390\n-0.7223282\n0.02727116\n\n\nPAT3P2\n14\n823\n2.849560\n0.7624570\n3.00000\n2.868214\n0.7413000\n1\n4\n3\n-0.1687695\n-0.6306593\n0.02657758\n\n\nPAT3P3\n15\n823\n3.056508\n0.7484883\n3.00000\n3.107101\n0.7413000\n1\n4\n3\n-0.4511290\n-0.5120333\n0.02609066\n\n\nNAT3P1\n16\n823\n1.723787\n0.6912895\n1.50000\n1.623666\n0.7413000\n1\n4\n3\n1.2328507\n1.5654947\n0.02409684\n\n\nNAT3P2\n17\n823\n1.575689\n0.6600865\n1.50000\n1.469134\n0.7413000\n1\n4\n3\n1.3962112\n2.0916581\n0.02300917\n\n\nNAT3P3\n18\n823\n1.641652\n0.6980201\n1.50000\n1.533856\n0.7413000\n1\n4\n3\n1.1666457\n1.0690468\n0.02433145\n\n\n\n\n\n\nplots_list &lt;- list()\n\n# Creazione della lista di grafici con i pannelli più grandi\nplots_list &lt;- list()\n\nfor (i in 1:16) {\n    col_name &lt;- names(dat)[i]\n    p &lt;- ggplot(dat, aes(x = !!sym(col_name))) +\n        geom_density(fill = \"blue\", color = \"black\", alpha = 0.5) +\n        ggtitle(col_name)\n    plots_list[[i]] &lt;- p\n}\n\n# Organizza e visualizza i grafici con pannelli più grandi\ndo.call(grid.arrange, c(plots_list, ncol = 4)) \n\n\n\n\n\n\n\n\nIniziamo a specificare il modello nullo.\n\nmod_null &lt;- \"\n    PAT1P1 ~~ V1*PAT1P1\n    PAT1P2 ~~ V2*PAT1P2\n    PAT1P3 ~~ V3*PAT1P3\n    NAT1P1 ~~ V4*NAT1P1\n    NAT1P2 ~~ V5*NAT1P2\n    NAT1P3 ~~ V6*NAT1P3\n\n    PAT2P1 ~~ V1*PAT2P1\n    PAT2P2 ~~ V2*PAT2P2\n    PAT2P3 ~~ V3*PAT2P3\n    NAT2P1 ~~ V4*NAT2P1\n    NAT2P2 ~~ V5*NAT2P2\n    NAT2P3 ~~ V6*NAT2P3\n\n    PAT3P1 ~~ V1*PAT3P1\n    PAT3P2 ~~ V2*PAT3P2\n    PAT3P3 ~~ V3*PAT3P3\n    NAT3P1 ~~ V4*NAT3P1\n    NAT3P2 ~~ V5*NAT3P2\n    NAT3P3 ~~ V6*NAT3P3\n\n    PAT1P1 ~ T1*1\n    PAT1P2 ~ T2*1\n    PAT1P3 ~ T3*1\n    NAT1P1 ~ T4*1\n    NAT1P2 ~ T5*1\n    NAT1P3 ~ T6*1\n\n    PAT2P1 ~ T1*1\n    PAT2P2 ~ T2*1\n    PAT2P3 ~ T3*1\n    NAT2P1 ~ T4*1\n    NAT2P2 ~ T5*1\n    NAT2P3 ~ T6*1\n\n    PAT3P1 ~ T1*1\n    PAT3P2 ~ T2*1\n    PAT3P3 ~ T3*1\n    NAT3P1 ~ T4*1\n    NAT3P2 ~ T5*1\n    NAT3P3 ~ T6*1\n\"\n\nIl modello nullo (baseline) è usato da Little (2023) come punto di partenza nell’analisi SEM e per i confronti con modelli più complessi. Il modello nullo specifica sei variabili osservate (PAT1P1, PAT1P2, PAT1P3, NAT1P1, NAT1P2, NAT1P3) misurate in tre punti temporali distinti. Questo implica che ci sono 18 variabili osservate in totale. Ogni variabile osservata ha la propria varianza unica che è stimata nel modello. Le medie delle 6 variabili misurate in ciascuno dei tre punti temporali sono assunte non variare in funzione del tempo. Nonostante il modello prenda in considerazione misurazioni ripetute nel tempo, non vi è alcuna specificazione di correlazioni o percorsi causali tra queste misure nel tempo, come sarebbe tipico per i modelli longitudinali. Essendo un modello nullo, non vengono specificate relazioni tra le variabili (varianze e medie) diverse dai loro effetti unici.\n\nfit_null &lt;- lavaan(mod_null, data = dat, orthogonal = TRUE)\n\n\nsummary(fit_null, standardized = T, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 32 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        36\n  Number of equality constraints                    24\n\n  Number of observations                           823\n\nModel Test User Model:\n                                                       \n  Test statistic                              11213.103\n  Degrees of freedom                                177\n  P-value (Chi-square)                            0.000\n\nModel Test Baseline Model:\n\n  Test statistic                             11131.067\n  Degrees of freedom                               153\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.000\n  Tucker-Lewis Index (TLI)                       0.131\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -15975.145\n  Loglikelihood unrestricted model (H1)     -10368.594\n                                                      \n  Akaike (AIC)                               31974.291\n  Bayesian (BIC)                             32030.846\n  Sample-size adjusted Bayesian (SABIC)      31992.739\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.275\n  90 Percent confidence interval - lower         0.271\n  90 Percent confidence interval - upper         0.280\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.328\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    PAT1P1    (T1)    2.960    0.016  190.946    0.000    2.960    3.843\n    PAT1P2    (T2)    2.885    0.015  190.376    0.000    2.885    3.831\n    PAT1P3    (T3)    3.099    0.015  207.418    0.000    3.099    4.174\n    NAT1P1    (T4)    1.708    0.014  123.730    0.000    1.708    2.490\n    NAT1P2    (T5)    1.521    0.013  116.519    0.000    1.521    2.345\n    NAT1P3    (T6)    1.558    0.014  114.474    0.000    1.558    2.304\n    PAT2P1    (T1)    2.960    0.016  190.946    0.000    2.960    3.843\n    PAT2P2    (T2)    2.885    0.015  190.376    0.000    2.885    3.831\n    PAT2P3    (T3)    3.099    0.015  207.418    0.000    3.099    4.174\n    NAT2P1    (T4)    1.708    0.014  123.730    0.000    1.708    2.490\n    NAT2P2    (T5)    1.521    0.013  116.519    0.000    1.521    2.345\n    NAT2P3    (T6)    1.558    0.014  114.474    0.000    1.558    2.304\n    PAT3P1    (T1)    2.960    0.016  190.946    0.000    2.960    3.843\n    PAT3P2    (T2)    2.885    0.015  190.376    0.000    2.885    3.831\n    PAT3P3    (T3)    3.099    0.015  207.418    0.000    3.099    4.174\n    NAT3P1    (T4)    1.708    0.014  123.730    0.000    1.708    2.490\n    NAT3P2    (T5)    1.521    0.013  116.519    0.000    1.521    2.345\n    NAT3P3    (T6)    1.558    0.014  114.474    0.000    1.558    2.304\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    PAT1P1    (V1)    0.593    0.017   35.135    0.000    0.593    1.000\n    PAT1P2    (V2)    0.567    0.016   35.135    0.000    0.567    1.000\n    PAT1P3    (V3)    0.551    0.016   35.135    0.000    0.551    1.000\n    NAT1P1    (V4)    0.471    0.013   35.135    0.000    0.471    1.000\n    NAT1P2    (V5)    0.421    0.012   35.135    0.000    0.421    1.000\n    NAT1P3    (V6)    0.457    0.013   35.135    0.000    0.457    1.000\n    PAT2P1    (V1)    0.593    0.017   35.135    0.000    0.593    1.000\n    PAT2P2    (V2)    0.567    0.016   35.135    0.000    0.567    1.000\n    PAT2P3    (V3)    0.551    0.016   35.135    0.000    0.551    1.000\n    NAT2P1    (V4)    0.471    0.013   35.135    0.000    0.471    1.000\n    NAT2P2    (V5)    0.421    0.012   35.135    0.000    0.421    1.000\n    NAT2P3    (V6)    0.457    0.013   35.135    0.000    0.457    1.000\n    PAT3P1    (V1)    0.593    0.017   35.135    0.000    0.593    1.000\n    PAT3P2    (V2)    0.567    0.016   35.135    0.000    0.567    1.000\n    PAT3P3    (V3)    0.551    0.016   35.135    0.000    0.551    1.000\n    NAT3P1    (V4)    0.471    0.013   35.135    0.000    0.471    1.000\n    NAT3P2    (V5)    0.421    0.012   35.135    0.000    0.421    1.000\n    NAT3P3    (V6)    0.457    0.013   35.135    0.000    0.457    1.000\n\n\n\n\n\n76.3.2 Modello SEM Iniziale\nNel modello SEM iniziale, Little (2023) definisce sei variabili latenti (Pos1, Pos2, Pos3, Neg1, Neg2, Neg3) che rappresentano costrutti psicologici positivi e negativi misurati in tre diversi momenti temporali. Ogni variabile latente è identificato da tre indicatori (per esempio, Pos1 è identificato da PAT1P1, PAT1P2, PAT1P3), con saturazioni fattoriali L1, L2, L3 che quantificano la relazione tra la variabile latenti e i suoi indicatori. Il modello stima la varianza di ciascuna variabile latente e la covarianza tra variabili latenti diverse. Le medie delle variabili latenti sono impostate a 1, indicando che sono considerate fisse. Il modello include stime per la varianza e la covarianza degli indicatori attraverso il tempo, suggerendo l’esistenza di correlazioni temporali tra gli stessi indicatori misurati in momenti diversi. Ci sono percorsi di regressione che collegano le variabili latenti nel tempo (ad esempio, Pos2 è influenzata da Pos1). Il modello impone alcuni vincoli sulle saturazioni fattoriali e sulle intercette degli indicatori.\nQuesto modello mira a esplorare le relazioni dinamiche e temporali tra variabili latenti, diversamente da un modello di invarianza configurale, che è più orientato alla valutazione della costanza della struttura fattoriale.\n\nSEMmod &lt;- \"\n    Pos1 =~ L1*PAT1P1 + L2*PAT1P2 + L3*PAT1P3\n    Neg1 =~ L4*NAT1P1 + L5*NAT1P2 + L6*NAT1P3\n    Pos2 =~ L1*PAT2P1 + L2*PAT2P2 + L3*PAT2P3\n    Neg2 =~ L4*NAT2P1 + L5*NAT2P2 + L6*NAT2P3\n    Pos3 =~ L1*PAT3P1 + L2*PAT3P2 + L3*PAT3P3\n    Neg3 =~ L4*NAT3P1 + L5*NAT3P2 + L6*NAT3P3\n\n    ## Latent factor variance and covariance (Psi matrix)\n    Pos1 ~~ Pos1 ##Psi 1,1\n    Pos1 ~~ Neg1 ##Psi 1,2\n    Neg1 ~~ Neg1 ##Psi 2,2\n\n    Pos2 ~~ Pos2 ##Psi 3,3\n    Pos2 ~~ Neg2 ##Psi 3,4\n    Neg2 ~~ Neg2 ##Psi 4,4\n\n    Pos3 ~~ Pos3  ##Psi 5,5\n    Pos3 ~~ Neg3  ##Psi 5,6\n    Neg3 ~~ Neg3  ##Psi 6,6\n\n    ## Latent means (Alpha matrix)\n    Pos1 ~ 1\n    Neg1 ~ 1\n    Pos2 ~ 1\n    Neg2 ~ 1\n    Pos3 ~ 1\n    Neg3 ~ 1\n\n    ## indicator resduals var-covar (Theta-Epsilon matrix)\n    ## Time1\n    PAT1P1 ~~ PAT1P1\n    PAT1P1 ~~ PAT2P1\n    PAT1P1 ~~ PAT3P1\n\n    PAT1P2 ~~ PAT1P2\n    PAT1P2 ~~ PAT2P2\n    PAT1P2 ~~ PAT3P2\n\n    PAT1P3 ~~ PAT1P3\n    PAT1P3 ~~ PAT2P3\n    PAT1P3 ~~ PAT3P3\n\n    NAT1P1 ~~ NAT1P1\n    NAT1P1 ~~ NAT2P1\n    NAT1P1 ~~ NAT3P1\n\n    NAT1P2 ~~ NAT1P2\n    NAT1P2 ~~ NAT2P2\n    NAT1P2 ~~ NAT3P2\n\n    NAT1P3 ~~ NAT1P3\n    NAT1P3 ~~ NAT2P3\n    NAT1P3 ~~ NAT3P3\n\n    #Time2\n    PAT2P1 ~~ PAT2P1\n    PAT2P1 ~~ PAT3P1\n\n    PAT2P2 ~~ PAT2P2\n    PAT2P2 ~~ PAT3P2\n\n    PAT2P3 ~~ PAT2P3\n    PAT2P3 ~~ PAT3P3\n\n    NAT2P1 ~~ NAT2P1\n    NAT2P1 ~~ NAT3P1\n\n    NAT2P2 ~~ NAT2P2\n    NAT2P2 ~~ NAT3P2\n\n    NAT2P3 ~~ NAT2P3\n    NAT2P3 ~~ NAT3P3\n\n    ## Time3\n    PAT3P1  ~~ PAT3P1\n    PAT3P2 ~~ PAT3P2\n    PAT3P3 ~~ PAT3P3\n    NAT3P1 ~~ NAT3P1\n    NAT3P2 ~~ NAT3P2\n    NAT3P3 ~~ NAT3P3\n\n    ##Indicator means/intercepts (Tau vector)\n    PAT1P1 ~ t1*1\n    PAT1P2 ~ t2*1\n    PAT1P3 ~ t3*1\n    NAT1P1 ~ t4*1\n    NAT1P2 ~ t5*1\n    NAT1P3 ~ t6*1\n\n    PAT2P1 ~ t1*1\n    PAT2P2 ~ t2*1\n    PAT2P3 ~ t3*1\n    NAT2P1 ~ t4*1\n    NAT2P2 ~ t5*1\n    NAT2P3 ~ t6*1\n\n    PAT3P1 ~ t1*1\n    PAT3P2 ~ t2*1\n    PAT3P3 ~ t3*1\n    NAT3P1 ~ t4*1\n    NAT3P2 ~ t5*1\n    NAT3P3 ~ t6*1\n\n    ##Regression paths here\n    Pos2 ~ Pos1\n    Pos3 ~ Pos1 + Pos2\n    Neg2 ~ Neg1\n    Neg3 ~ Neg1 + Neg2\n\n    ## Constraints\n    L1 == 3 - L2 - L3\n    L4 == 3 - L5 - L6\n    t1 == 0 - t2 - t3\n    t4 == 0 - t5 - t6\n\"\n\n\nfitSEM &lt;- lavaan(SEMmod, data = dat, meanstructure = TRUE)\n\n\nsummary(fitSEM, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 129 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        93\n  Number of equality constraints                    28\n\n  Number of observations                           823\n\nModel Test User Model:\n                                                      \n  Test statistic                               441.520\n  Degrees of freedom                               124\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                             11131.067\n  Degrees of freedom                               153\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.971\n  Tucker-Lewis Index (TLI)                       0.964\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -10589.354\n  Loglikelihood unrestricted model (H1)     -10368.594\n                                                      \n  Akaike (AIC)                               21308.709\n  Bayesian (BIC)                             21615.051\n  Sample-size adjusted Bayesian (SABIC)      21408.635\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.056\n  90 Percent confidence interval - lower         0.050\n  90 Percent confidence interval - upper         0.061\n  P-value H_0: RMSEA &lt;= 0.050                    0.044\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.045\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Pos1 =~                                                               \n    PAT1P1    (L1)    1.040    0.010  103.232    0.000    0.684    0.884\n    PAT1P2    (L2)    0.997    0.010   98.450    0.000    0.655    0.881\n    PAT1P3    (L3)    0.963    0.010   93.311    0.000    0.633    0.856\n  Neg1 =~                                                               \n    NAT1P1    (L4)    0.957    0.012   78.547    0.000    0.553    0.757\n    NAT1P2    (L5)    0.999    0.011   90.776    0.000    0.578    0.891\n    NAT1P3    (L6)    1.044    0.011   94.041    0.000    0.604    0.906\n  Pos2 =~                                                               \n    PAT2P1    (L1)    1.040    0.010  103.232    0.000    0.692    0.910\n    PAT2P2    (L2)    0.997    0.010   98.450    0.000    0.663    0.900\n    PAT2P3    (L3)    0.963    0.010   93.311    0.000    0.641    0.877\n  Neg2 =~                                                               \n    NAT2P1    (L4)    0.957    0.012   78.547    0.000    0.525    0.816\n    NAT2P2    (L5)    0.999    0.011   90.776    0.000    0.548    0.873\n    NAT2P3    (L6)    1.044    0.011   94.041    0.000    0.573    0.890\n  Pos3 =~                                                               \n    PAT3P1    (L1)    1.040    0.010  103.232    0.000    0.699    0.900\n    PAT3P2    (L2)    0.997    0.010   98.450    0.000    0.671    0.864\n    PAT3P3    (L3)    0.963    0.010   93.311    0.000    0.648    0.856\n  Neg3 =~                                                               \n    NAT3P1    (L4)    0.957    0.012   78.547    0.000    0.566    0.837\n    NAT3P2    (L5)    0.999    0.011   90.776    0.000    0.591    0.888\n    NAT3P3    (L6)    1.044    0.011   94.041    0.000    0.617    0.869\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Pos2 ~                                                                \n    Pos1              0.550    0.033   16.667    0.000    0.544    0.544\n  Pos3 ~                                                                \n    Pos1              0.340    0.039    8.822    0.000    0.333    0.333\n    Pos2              0.372    0.038    9.848    0.000    0.368    0.368\n  Neg2 ~                                                                \n    Neg1              0.445    0.033   13.494    0.000    0.468    0.468\n  Neg3 ~                                                                \n    Neg1              0.285    0.038    7.470    0.000    0.279    0.279\n    Neg2              0.408    0.040   10.113    0.000    0.379    0.379\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Pos1 ~~                                                               \n    Neg1             -0.063    0.015   -4.293    0.000   -0.166   -0.166\n .Pos2 ~~                                                               \n   .Neg2             -0.050    0.011   -4.503    0.000   -0.183   -0.183\n .Pos3 ~~                                                               \n   .Neg3             -0.074    0.011   -6.758    0.000   -0.288   -0.288\n .PAT1P1 ~~                                                             \n   .PAT2P1            0.010    0.006    1.630    0.103    0.010    0.089\n   .PAT3P1            0.007    0.007    1.039    0.299    0.007    0.058\n .PAT1P2 ~~                                                             \n   .PAT2P2            0.007    0.006    1.236    0.217    0.007    0.065\n   .PAT3P2            0.013    0.007    1.922    0.055    0.013    0.097\n .PAT1P3 ~~                                                             \n   .PAT2P3            0.012    0.006    1.866    0.062    0.012    0.088\n   .PAT3P3            0.012    0.007    1.723    0.085    0.012    0.081\n .NAT1P1 ~~                                                             \n   .NAT2P1            0.027    0.007    3.603    0.000    0.027    0.151\n   .NAT3P1            0.009    0.007    1.230    0.219    0.009    0.052\n .NAT1P2 ~~                                                             \n   .NAT2P2            0.001    0.005    0.105    0.916    0.001    0.006\n   .NAT3P2            0.006    0.005    1.169    0.242    0.006    0.066\n .NAT1P3 ~~                                                             \n   .NAT2P3           -0.001    0.005   -0.178    0.859   -0.001   -0.011\n   .NAT3P3           -0.008    0.006   -1.410    0.159   -0.008   -0.081\n .PAT2P1 ~~                                                             \n   .PAT3P1           -0.000    0.006   -0.068    0.946   -0.000   -0.004\n .PAT2P2 ~~                                                             \n   .PAT3P2            0.005    0.006    0.776    0.438    0.005    0.039\n .PAT2P3 ~~                                                             \n   .PAT3P3            0.010    0.006    1.538    0.124    0.010    0.072\n .NAT2P1 ~~                                                             \n   .NAT3P1            0.011    0.006    1.810    0.070    0.011    0.081\n .NAT2P2 ~~                                                             \n   .NAT3P2            0.008    0.005    1.621    0.105    0.008    0.088\n .NAT2P3 ~~                                                             \n   .NAT3P3           -0.004    0.006   -0.674    0.500   -0.004   -0.037\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    Pos1              3.000    0.024  124.676    0.000    4.564    4.564\n    Neg1              1.520    0.021   71.493    0.000    2.629    2.629\n   .Pos2              1.361    0.101   13.447    0.000    2.047    2.047\n   .Neg2              0.929    0.053   17.429    0.000    1.693    1.693\n   .Pos3              0.787    0.107    7.373    0.000    1.171    1.171\n   .Neg3              0.558    0.064    8.746    0.000    0.944    0.944\n   .PAT1P1    (t1)   -0.140    0.031   -4.557    0.000   -0.140   -0.181\n   .PAT1P2    (t2)   -0.088    0.031   -2.855    0.004   -0.088   -0.118\n   .PAT1P3    (t3)    0.228    0.031    7.238    0.000    0.228    0.308\n   .NAT1P1    (t4)    0.174    0.021    8.467    0.000    0.174    0.239\n   .NAT1P2    (t5)   -0.068    0.018   -3.706    0.000   -0.068   -0.105\n   .NAT1P3    (t6)   -0.106    0.018   -5.760    0.000   -0.106   -0.159\n   .PAT2P1    (t1)   -0.140    0.031   -4.557    0.000   -0.140   -0.184\n   .PAT2P2    (t2)   -0.088    0.031   -2.855    0.004   -0.088   -0.120\n   .PAT2P3    (t3)    0.228    0.031    7.238    0.000    0.228    0.312\n   .NAT2P1    (t4)    0.174    0.021    8.467    0.000    0.174    0.271\n   .NAT2P2    (t5)   -0.068    0.018   -3.706    0.000   -0.068   -0.108\n   .NAT2P3    (t6)   -0.106    0.018   -5.760    0.000   -0.106   -0.165\n   .PAT3P1    (t1)   -0.140    0.031   -4.557    0.000   -0.140   -0.180\n   .PAT3P2    (t2)   -0.088    0.031   -2.855    0.004   -0.088   -0.114\n   .PAT3P3    (t3)    0.228    0.031    7.238    0.000    0.228    0.301\n   .NAT3P1    (t4)    0.174    0.021    8.467    0.000    0.174    0.258\n   .NAT3P2    (t5)   -0.068    0.018   -3.706    0.000   -0.068   -0.102\n   .NAT3P3    (t6)   -0.106    0.018   -5.760    0.000   -0.106   -0.150\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    Pos1              0.432    0.024   18.356    0.000    1.000    1.000\n    Neg1              0.334    0.018   18.207    0.000    1.000    1.000\n   .Pos2              0.311    0.018   17.618    0.000    0.704    0.704\n   .Neg2              0.235    0.014   17.253    0.000    0.781    0.781\n   .Pos3              0.281    0.017   16.941    0.000    0.621    0.621\n   .Neg3              0.237    0.014   16.908    0.000    0.679    0.679\n   .PAT1P1            0.131    0.010   13.048    0.000    0.131    0.219\n   .PAT1P2            0.124    0.009   13.290    0.000    0.124    0.224\n   .PAT1P3            0.147    0.010   14.824    0.000    0.147    0.268\n   .NAT1P1            0.228    0.013   17.665    0.000    0.228    0.427\n   .NAT1P2            0.086    0.007   11.609    0.000    0.086    0.206\n   .NAT1P3            0.080    0.008   10.338    0.000    0.080    0.180\n   .PAT2P1            0.099    0.008   12.300    0.000    0.099    0.171\n   .PAT2P2            0.104    0.008   13.261    0.000    0.104    0.191\n   .PAT2P3            0.124    0.008   14.911    0.000    0.124    0.231\n   .NAT2P1            0.139    0.009   16.035    0.000    0.139    0.335\n   .NAT2P2            0.094    0.007   13.123    0.000    0.094    0.238\n   .NAT2P3            0.086    0.007   11.805    0.000    0.086    0.208\n   .PAT3P1            0.115    0.010   11.900    0.000    0.115    0.190\n   .PAT3P2            0.153    0.011   14.459    0.000    0.153    0.253\n   .PAT3P3            0.153    0.010   14.904    0.000    0.153    0.267\n   .NAT3P1            0.136    0.009   15.347    0.000    0.136    0.299\n   .NAT3P2            0.094    0.008   12.323    0.000    0.094    0.212\n   .NAT3P3            0.123    0.009   13.525    0.000    0.123    0.244\n\nConstraints:\n                                               |Slack|\n    L1 - (3-L2-L3)                               0.000\n    L4 - (3-L5-L6)                               0.000\n    t1 - (0-t2-t3)                               0.000\n    t4 - (0-t5-t6)                               0.000",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>76</span>  <span class='chapter-title'>Specificare e Interpretare un Modello Longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/05_intro_panel.html#invarianza",
    "href": "chapters/lgm/05_intro_panel.html#invarianza",
    "title": "76  Specificare e Interpretare un Modello Longitudinale",
    "section": "76.4 Invarianza",
    "text": "76.4 Invarianza\nL’invarianza nei modelli SEM panel è fondamentale per garantire che le misurazioni di un costrutto siano comparabili nel tempo. Nei modelli longitudinali, l’invarianza implica che il significato e la struttura di un costrutto rimangano stabili attraverso diverse occasioni di misurazione. Senza l’invarianza, qualsiasi cambiamento osservato potrebbe riflettere variazioni nella misurazione stessa, piuttosto che un cambiamento reale nel costrutto. Testare l’invarianza nei modelli SEM panel consente quindi di distinguere tra cambiamenti reali e artefatti della misurazione, supportando un’interpretazione valida delle traiettorie di sviluppo.\n\n76.4.1 Modello di Invarianza Configurale\nIn un modello di invarianza configurale, ci si aspetta che la struttura fattoriale, cioè il numero di fattori e il pattern di carichi fattoriali, sia la stessa in tutti i gruppi o momenti temporali considerati.\nOgni variabile latente (Pos1, Pos2, Pos3, Neg1, Neg2, Neg3) è misurata da un set specifico di indicatori in ciascuno dei tre momenti temporali. Ad esempio, Pos1 è misurata da PAT1P1, PAT1P2, e PAT1P3. I carichi fattoriali (L1, L2, L3, ecc.) sono specificati separatamente per ogni momento temporale. I vincoli imposti (ad esempio, L1 == 3 - L2 - L3) indicano che ci sono alcune restrizioni nella relazione tra i carichi fattoriali. Questi vincoli sono utilizzati per testare l’uguaglianza dei carichi attraverso i diversi tempi.\nIl modello stima separatamente la varianza di ciascun indicatore e di ciascuna variabile latente in ogni momento temporale. Il modello include covarianze sia tra gli indicatori in diversi momenti temporali sia tra le variabili latenti, suggerendo che esiste una correlazione tra le misurazioni nel tempo. Le medie delle variabili latenti e degli indicatori sono specificate imponendo alcuni vincoli (ad esempio, t1 == 0 - t2 - t3).\nI vincoli imposti sui carichi fattoriali e sulle medie degli indicatori permettono di testare se la struttura fattoriale è consistente nel tempo, che è l’essenza dell’invarianza configurale.\n\nmod_config &lt;- \"\n    Pos1 =~ L1*PAT1P1 + L2*PAT1P2 + L3*PAT1P3\n    Neg1 =~ L4*NAT1P1 + L5*NAT1P2 + L6*NAT1P3\n    Pos2 =~ L7*PAT2P1 + L8*PAT2P2 + L9*PAT2P3\n    Neg2 =~ L10*NAT2P1 + L11*NAT2P2 + L12*NAT2P3\n    Pos3 =~ L13*PAT3P1 + L14*PAT3P2 + L15*PAT3P3\n    Neg3 =~ L16*NAT3P1 + L17*NAT3P2 + L18*NAT3P3\n    L1 == 3 - L2 - L3\n    L4 == 3 - L5 - L6\n    L7 == 3 - L8 - L9\n    L10== 3 - L11- L12\n    L13== 3 - L14- L15\n    L16== 3 - L17- L18\n\n    PAT1P1~~PAT1P1\n    PAT1P2~~PAT1P2\n    PAT1P3~~PAT1P3\n    NAT1P1~~NAT1P1\n    NAT1P2~~NAT1P2\n    NAT1P3~~NAT1P3\n    PAT2P1~~PAT2P1\n    PAT2P2~~PAT2P2\n    PAT2P3~~PAT2P3\n    NAT2P1~~NAT2P1\n    NAT2P2~~NAT2P2\n    NAT2P3~~NAT2P3\n    PAT3P1~~PAT3P1\n    PAT3P2~~PAT3P2\n    PAT3P3~~PAT3P3\n    NAT3P1~~NAT3P1\n    NAT3P2~~NAT3P2\n    NAT3P3~~NAT3P3\n\n    Pos1~~Pos1\n    Neg1~~Neg1\n    Pos2~~Pos2\n    Neg2~~Neg2\n    Pos3~~Pos3\n    Neg3~~Neg3\n\n    PAT1P1~~PAT2P1 + PAT3P1\n    PAT2P1~~PAT3P1\n    PAT1P2~~PAT2P2 + PAT3P2\n    PAT2P2~~PAT3P2\n    PAT1P3~~PAT2P3 + PAT3P3\n    PAT2P3~~PAT3P3\n    NAT1P1~~NAT2P1 + NAT3P1\n    NAT2P1~~NAT3P1\n    NAT1P2~~NAT2P2 + NAT3P2\n    NAT2P2~~NAT3P2\n    NAT1P3~~NAT2P3 + NAT3P3\n    NAT2P3~~NAT3P3\n\n    Pos1~~Pos2 + Pos3 + Neg1 + Neg2 + Neg3\n    Pos2~~Pos3 + Neg1 + Neg2 + Neg3\n    Pos3~~Neg1 + Neg2 + Neg3\n    Neg1~~Neg2 + Neg3\n    Neg2~~Neg3\n\n    Pos1~NA*1\n    Neg1~NA*1\n    Pos2~NA*1\n    Neg2~NA*1\n    Pos3~NA*1\n    Neg3~NA*1\n\n    PAT1P1~t1*1\n    PAT1P2~t2*1\n    PAT1P3~t3*1\n    NAT1P1~t4*1\n    NAT1P2~t5*1\n    NAT1P3~t6*1\n    PAT2P1~t7*1\n    PAT2P2~t8*1\n    PAT2P3~t9*1\n    NAT2P1~t10*1\n    NAT2P2~t11*1\n    NAT2P3~t12*1\n    PAT3P1~t13*1\n    PAT3P2~t14*1\n    PAT3P3~t15*1\n    NAT3P1~t16*1\n    NAT3P2~t17*1\n    NAT3P3~t18*1\n\n    t1 == 0 - t2 - t3\n    t4 == 0 - t5 - t6\n    t7 == 0 - t8 - t9\n    t10== 0 - t11- t12\n    t13== 0 - t14- t15\n    t16== 0 - t17- t18\n\"\n\n\nfit_config &lt;- lavaan(mod_config, data = dat, meanstructure = TRUE)\n\n\nsummary(fit_config, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 160 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        99\n  Number of equality constraints                    12\n\n  Number of observations                           823\n\nModel Test User Model:\n                                                      \n  Test statistic                               352.232\n  Degrees of freedom                               102\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                             11131.067\n  Degrees of freedom                               153\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.977\n  Tucker-Lewis Index (TLI)                       0.966\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -10544.710\n  Loglikelihood unrestricted model (H1)     -10368.594\n                                                      \n  Akaike (AIC)                               21263.420\n  Bayesian (BIC)                             21673.447\n  Sample-size adjusted Bayesian (SABIC)      21397.168\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.055\n  90 Percent confidence interval - lower         0.048\n  90 Percent confidence interval - upper         0.061\n  P-value H_0: RMSEA &lt;= 0.050                    0.108\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.035\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Pos1 =~                                                               \n    PAT1P1    (L1)    1.021    0.018   57.311    0.000    0.672    0.878\n    PAT1P2    (L2)    0.999    0.018   57.022    0.000    0.658    0.882\n    PAT1P3    (L3)    0.980    0.018   54.726    0.000    0.644    0.862\n  Neg1 =~                                                               \n    NAT1P1    (L4)    0.911    0.023   40.011    0.000    0.521    0.740\n    NAT1P2    (L5)    1.032    0.019   53.312    0.000    0.591    0.899\n    NAT1P3    (L6)    1.057    0.020   54.058    0.000    0.605    0.907\n  Pos2 =~                                                               \n    PAT2P1    (L7)    1.031    0.015   67.338    0.000    0.690    0.909\n    PAT2P2    (L8)    1.010    0.015   66.147    0.000    0.676    0.905\n    PAT2P3    (L9)    0.958    0.016   60.988    0.000    0.641    0.877\n  Neg2 =~                                                               \n    NAT2P1   (L10)    0.973    0.020   49.201    0.000    0.537    0.824\n    NAT2P2   (L11)    0.976    0.019   52.312    0.000    0.538    0.866\n    NAT2P3   (L12)    1.052    0.019   55.900    0.000    0.581    0.894\n  Pos3 =~                                                               \n    PAT3P1   (L13)    1.065    0.018   60.101    0.000    0.709    0.907\n    PAT3P2   (L14)    0.981    0.018   53.971    0.000    0.653    0.857\n    PAT3P3   (L15)    0.954    0.018   52.524    0.000    0.636    0.849\n  Neg3 =~                                                               \n    NAT3P1   (L16)    0.994    0.019   52.539    0.000    0.586    0.852\n    NAT3P2   (L17)    0.989    0.018   54.815    0.000    0.583    0.884\n    NAT3P3   (L18)    1.017    0.019   53.509    0.000    0.600    0.861\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .PAT1P1 ~~                                                             \n   .PAT2P1            0.011    0.006    1.701    0.089    0.011    0.091\n   .PAT3P1            0.008    0.007    1.133    0.257    0.008    0.064\n .PAT2P1 ~~                                                             \n   .PAT3P1           -0.000    0.006   -0.039    0.969   -0.000   -0.002\n .PAT1P2 ~~                                                             \n   .PAT2P2            0.007    0.006    1.251    0.211    0.007    0.066\n   .PAT3P2            0.012    0.007    1.782    0.075    0.012    0.089\n .PAT2P2 ~~                                                             \n   .PAT3P2            0.005    0.006    0.732    0.464    0.005    0.037\n .PAT1P3 ~~                                                             \n   .PAT2P3            0.011    0.006    1.740    0.082    0.011    0.083\n   .PAT3P3            0.013    0.007    1.851    0.064    0.013    0.087\n .PAT2P3 ~~                                                             \n   .PAT3P3            0.010    0.006    1.524    0.127    0.010    0.071\n .NAT1P1 ~~                                                             \n   .NAT2P1            0.029    0.007    3.893    0.000    0.029    0.163\n   .NAT3P1            0.010    0.007    1.412    0.158    0.010    0.061\n .NAT2P1 ~~                                                             \n   .NAT3P1            0.009    0.006    1.506    0.132    0.009    0.069\n .NAT1P2 ~~                                                             \n   .NAT2P2            0.000    0.005    0.006    0.996    0.000    0.000\n   .NAT3P2            0.005    0.005    1.009    0.313    0.005    0.058\n .NAT2P2 ~~                                                             \n   .NAT3P2            0.008    0.005    1.660    0.097    0.008    0.087\n .NAT1P3 ~~                                                             \n   .NAT2P3            0.000    0.005    0.002    0.998    0.000    0.000\n   .NAT3P3           -0.006    0.006   -1.016    0.310   -0.006   -0.057\n .NAT2P3 ~~                                                             \n   .NAT3P3           -0.004    0.006   -0.729    0.466   -0.004   -0.039\n  Pos1 ~~                                                               \n    Pos2              0.243    0.019   12.892    0.000    0.552    0.552\n    Pos3              0.230    0.019   12.278    0.000    0.525    0.525\n    Neg1             -0.062    0.015   -4.217    0.000   -0.164   -0.164\n    Neg2             -0.059    0.014   -4.149    0.000   -0.162   -0.162\n    Neg3             -0.014    0.015   -0.913    0.361   -0.035   -0.035\n  Pos2 ~~                                                               \n    Pos3              0.241    0.019   12.725    0.000    0.542    0.542\n  Neg1 ~~                                                               \n    Pos2             -0.058    0.015   -3.963    0.000   -0.152   -0.152\n  Pos2 ~~                                                               \n    Neg2             -0.090    0.014   -6.254    0.000   -0.244   -0.244\n    Neg3             -0.028    0.015   -1.851    0.064   -0.071   -0.071\n  Neg1 ~~                                                               \n    Pos3             -0.010    0.015   -0.717    0.473   -0.027   -0.027\n  Neg2 ~~                                                               \n    Pos3             -0.047    0.014   -3.296    0.001   -0.128   -0.128\n  Pos3 ~~                                                               \n    Neg3             -0.078    0.015   -5.087    0.000   -0.199   -0.199\n  Neg1 ~~                                                               \n    Neg2              0.149    0.013   11.184    0.000    0.472    0.472\n    Neg3              0.149    0.014   10.586    0.000    0.441    0.441\n  Neg2 ~~                                                               \n    Neg3              0.167    0.014   11.998    0.000    0.514    0.514\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    Pos1              3.000    0.024  124.563    0.000    4.560    4.560\n    Neg1              1.537    0.021   72.416    0.000    2.684    2.684\n    Pos2              3.012    0.024  124.258    0.000    4.503    4.503\n    Neg2              1.604    0.020   78.945    0.000    2.907    2.907\n    Pos3              2.931    0.024  120.101    0.000    4.401    4.401\n    Neg3              1.647    0.022   75.997    0.000    2.793    2.793\n   .PAT1P1    (t1)   -0.071    0.054   -1.303    0.192   -0.071   -0.093\n   .PAT1P2    (t2)   -0.103    0.054   -1.916    0.055   -0.103   -0.138\n   .PAT1P3    (t3)    0.174    0.055    3.171    0.002    0.174    0.232\n   .NAT1P1    (t4)    0.307    0.037    8.291    0.000    0.307    0.436\n   .NAT1P2    (t5)   -0.136    0.031   -4.349    0.000   -0.136   -0.206\n   .NAT1P3    (t6)   -0.172    0.031   -5.455    0.000   -0.172   -0.257\n   .PAT2P1    (t7)   -0.106    0.047   -2.243    0.025   -0.106   -0.139\n   .PAT2P2    (t8)   -0.134    0.047   -2.860    0.004   -0.134   -0.180\n   .PAT2P3    (t9)    0.240    0.048    4.960    0.000    0.240    0.328\n   .NAT2P1   (t10)    0.135    0.033    4.058    0.000    0.135    0.207\n   .NAT2P2   (t11)   -0.027    0.031   -0.874    0.382   -0.027   -0.044\n   .NAT2P3   (t12)   -0.108    0.031   -3.419    0.001   -0.108   -0.166\n   .PAT3P1   (t13)   -0.234    0.053   -4.418    0.000   -0.234   -0.299\n   .PAT3P2   (t14)   -0.026    0.054   -0.481    0.631   -0.026   -0.034\n   .PAT3P3   (t15)    0.260    0.054    4.779    0.000    0.260    0.347\n   .NAT3P1   (t16)    0.087    0.033    2.663    0.008    0.087    0.127\n   .NAT3P2   (t17)   -0.054    0.031   -1.724    0.085   -0.054   -0.081\n   .NAT3P3   (t18)   -0.033    0.033   -1.019    0.308   -0.033   -0.048\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .PAT1P1            0.133    0.010   12.941    0.000    0.133    0.228\n   .PAT1P2            0.123    0.010   12.691    0.000    0.123    0.222\n   .PAT1P3            0.144    0.010   14.093    0.000    0.144    0.257\n   .NAT1P1            0.224    0.013   17.831    0.000    0.224    0.452\n   .NAT1P2            0.083    0.008   10.067    0.000    0.083    0.193\n   .NAT1P3            0.078    0.008    9.243    0.000    0.078    0.176\n   .PAT2P1            0.100    0.008   12.156    0.000    0.100    0.174\n   .PAT2P2            0.102    0.008   12.582    0.000    0.102    0.182\n   .PAT2P3            0.124    0.008   14.727    0.000    0.124    0.231\n   .NAT2P1            0.137    0.009   15.465    0.000    0.137    0.322\n   .NAT2P2            0.097    0.007   13.132    0.000    0.097    0.251\n   .NAT2P3            0.084    0.008   10.906    0.000    0.084    0.200\n   .PAT3P1            0.109    0.010   10.536    0.000    0.109    0.178\n   .PAT3P2            0.154    0.011   14.284    0.000    0.154    0.265\n   .PAT3P3            0.156    0.011   14.776    0.000    0.156    0.279\n   .NAT3P1            0.129    0.009   14.221    0.000    0.129    0.274\n   .NAT3P2            0.095    0.008   12.052    0.000    0.095    0.218\n   .NAT3P3            0.125    0.009   13.536    0.000    0.125    0.258\n    Pos1              0.433    0.024   18.354    0.000    1.000    1.000\n    Neg1              0.328    0.018   17.933    0.000    1.000    1.000\n    Pos2              0.448    0.024   18.745    0.000    1.000    1.000\n    Neg2              0.305    0.017   18.153    0.000    1.000    1.000\n    Pos3              0.444    0.024   18.323    0.000    1.000    1.000\n    Neg3              0.348    0.019   18.211    0.000    1.000    1.000\n\nConstraints:\n                                               |Slack|\n    L1 - (3-L2-L3)                               0.000\n    L4 - (3-L5-L6)                               0.000\n    L7 - (3-L8-L9)                               0.000\n    L10 - (3-L11-L12)                            0.000\n    L13 - (3-L14-L15)                            0.000\n    L16 - (3-L17-L18)                            0.000\n    t1 - (0-t2-t3)                               0.000\n    t4 - (0-t5-t6)                               0.000\n    t7 - (0-t8-t9)                               0.000\n    t10 - (0-t11-t12)                            0.000\n    t13 - (0-t14-t15)                            0.000\n    t16 - (0-t17-t18)                            0.000\n\n\n\n\n\n76.4.2 Modello di Invarianza Debole\nIl modello di invarianza debole, o invarianza metrica, è un passo oltre l’invarianza configurale nella SEM per testare l’uguaglianza di costrutti psicologici nel tempo. Mentre l’invarianza configurale si concentra sulla struttura fattoriale (cioè, la presenza e il pattern dei carichi fattoriali), l’invarianza debole agginge il vincolo dell’uguaglianza dei carichi fattoriali nei diversi momenti temporali.\nNel modello successivo, i carichi fattoriali per gli indicatori corrispondenti sono mantenuti costanti nelle tre rilevazioni temporali. Ad esempio, lo stesso valore per L1 è utilizzato per PAT1P1, PAT2P1 e PAT3P1 in tutti e tre i momenti temporali. Questo significa che questo modello verifica se la relazione tra le variabili latenti (Pos e Neg) e i loro indicatori (PAT e NAT) è la stessa nel tempo.\nIl modello stima separatamente la varianza di ciascun indicatore e di ciascuna variabile latente in ogni momento temporale. Questo è simile all’invarianza configurale.\nIl modello include covarianze sia tra gli indicatori in diversi momenti temporali sia tra le variabili latenti, permettendo di catturare la correlazione tra le misurazioni nel tempo.\nViene fatta un’ipotesi sulle medie degli indicatori, come mostrato nelle equazioni PAT1P1 ~ t1*1, ecc. I vincoli sulle medie degli indicatori (ad esempio, t1 == 0 - t2 - t3) suggeriscono che ci sono alcune restrizioni matematiche imposte sulle medie degli indicatori. Prendendo l’equazione t1 == 0 - t2 - t3 come esempio, questa impone una relazione diretta tra tre medie degli indicatori. In pratica, afferma che la media di un indicatore (rappresentata da t1) è definita come l’opposto della somma delle medie di altri due indicatori (t2 e t3). Questo tipo di vincolo può essere interpretato come un meccanismo di bilanciamento. Se t2 e t3 aumentano, allora t1 diminuisce di conseguenza, mantenendo una relazione bilanciata tra queste tre medie.\nMentre l’invarianza configurale richiede solo che la stessa struttura fattoriale sia presente attraverso i gruppi o nel tempo (ad esempio, gli stessi fattori con gli stessi indicatori), l’invarianza debole richiede anche che i carichi fattoriali siano gli stessi. Questo è un test più rigoroso dell’invarianza poiché non solo assume che le stesse variabili latenti siano misurate, ma anche che la forza della relazione tra le variabili latenti e i loro indicatori sia costante.\n\nmod_weak &lt;- \"\n    Pos1 =~ L1*PAT1P1 + L2*PAT1P2 + L3*PAT1P3\n    Neg1 =~ L4*NAT1P1 + L5*NAT1P2 + L6*NAT1P3\n    Pos2 =~ L1*PAT2P1 + L2*PAT2P2 + L3*PAT2P3\n    Neg2 =~ L4*NAT2P1 + L5*NAT2P2 + L6*NAT2P3\n    Pos3 =~ L1*PAT3P1 + L2*PAT3P2 + L3*PAT3P3\n    Neg3 =~ L4*NAT3P1 + L5*NAT3P2 + L6*NAT3P3\n\n    ## indicator residual variances (Theta-Epsilon matrix)\n    PAT1P1 ~~ PAT1P1\n    PAT1P2 ~~ PAT1P2\n    PAT1P3 ~~ PAT1P3\n    NAT1P1 ~~ NAT1P1\n    NAT1P2 ~~ NAT1P2\n    NAT1P3 ~~ NAT1P3\n    PAT2P1 ~~ PAT2P1\n    PAT2P2 ~~ PAT2P2\n    PAT2P3 ~~ PAT2P3\n    NAT2P1 ~~ NAT2P1\n    NAT2P2 ~~ NAT2P2\n    NAT2P3 ~~ NAT2P3\n    PAT3P1 ~~ PAT3P1\n    PAT3P2 ~~ PAT3P2\n    PAT3P3 ~~ PAT3P3\n    NAT3P1 ~~ NAT3P1\n    NAT3P2 ~~ NAT3P2\n    NAT3P3 ~~ NAT3P3\n\n    ### cross-time correlated residuals\n    ## correlate residuals of indicators with themselves across time\n    PAT1P1 ~~ PAT2P1 + PAT3P1\n    PAT2P1 ~~ PAT3P1\n    PAT1P2 ~~ PAT2P2 + PAT3P2\n    PAT2P2 ~~ PAT3P2\n    PAT1P3 ~~ PAT2P3 + PAT3P3\n    PAT2P3 ~~ PAT3P3\n    NAT1P1 ~~ NAT2P1 + NAT3P1\n    NAT2P1 ~~ NAT3P1\n    NAT1P2 ~~ NAT2P2 + NAT3P2\n    NAT2P2 ~~ NAT3P2\n    NAT1P3 ~~ NAT2P3 + NAT3P3\n    NAT2P3 ~~ NAT3P3\n\n    ## indicator intercepts (Tau vector), include labels for model constraints\n    PAT1P1 ~ t1*1\n    PAT1P2 ~ t2*1\n    PAT1P3 ~ t3*1\n    NAT1P1 ~ t4*1\n    NAT1P2 ~ t5*1\n    NAT1P3 ~ t6*1\n\n    PAT2P1 ~ t7*1\n    PAT2P2 ~ t8*1\n    PAT2P3 ~ t9*1\n    NAT2P1 ~ t10*1\n    NAT2P2 ~ t11*1\n    NAT2P3 ~ t12*1\n\n    PAT3P1 ~ t13*1\n    PAT3P2 ~ t14*1\n    PAT3P3 ~ t15*1\n    NAT3P1 ~ t16*1\n    NAT3P2 ~ t17*1\n    NAT3P3 ~ t18*1\n\n    ### latent factor variance (Psi matrix)\n    Pos1 ~~ Pos1\n    Neg1 ~~ Neg1\n    Pos2 ~~ Pos2\n    Neg2 ~~ Neg2\n    Pos3 ~~ Pos3\n    Neg3 ~~ Neg3\n\n    ### factor covariance\n    Pos1 ~~ Pos2 + Pos3 + Neg1 + Neg2 + Neg3\n    Pos2 ~~ Pos3 + Neg1 + Neg2 + Neg3\n    Pos3 ~~ Neg1 + Neg2 + Neg3\n    Neg1 ~~ Neg2 + Neg3\n    Neg2 ~~ Neg3\n\n    ## latent means (Alpha matrix)\n    Pos1 ~ 1\n    Neg1 ~ 1\n    Pos2 ~ 1\n    Neg2 ~ 1\n    Pos3 ~ 1\n    Neg3 ~ 1\n\n    ## model constraints\n    L1 == 3 - L2 - L3\n    L4 == 3 - L5 - L6\n    t1 == 0 - t2 - t3\n    t4 == 0 - t5 - t6\n    t7 == 0 - t8 - t9\n    t10 == 0 - t11 - t12\n    t13 == 0 - t14 - t15\n    t16 == 0 - t17 - t18\n\"\n\n\nfit_wk &lt;- lavaan(mod_weak, data = dat, meanstructure = TRUE)\n#### Did not converge on first run, used final estimates on starting values for next run\nfit_weak &lt;- lavaan(mod_weak, data = dat, meanstructure = TRUE, start = fit_wk)\n\n\nsummary(fit_weak, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 4 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        99\n  Number of equality constraints                    20\n\n  Number of observations                           823\n\nModel Test User Model:\n                                                      \n  Test statistic                               366.562\n  Degrees of freedom                               110\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                             11131.067\n  Degrees of freedom                               153\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.977\n  Tucker-Lewis Index (TLI)                       0.967\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -10551.875\n  Loglikelihood unrestricted model (H1)     -10368.594\n                                                      \n  Akaike (AIC)                               21261.750\n  Bayesian (BIC)                             21634.074\n  Sample-size adjusted Bayesian (SABIC)      21383.200\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.053\n  90 Percent confidence interval - lower         0.047\n  90 Percent confidence interval - upper         0.059\n  P-value H_0: RMSEA &lt;= 0.050                    0.182\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.035\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Pos1 =~                                                               \n    PAT1P1    (L1)    1.038    0.010  102.905    0.000    0.683    0.884\n    PAT1P2    (L2)    0.998    0.010   98.574    0.000    0.656    0.881\n    PAT1P3    (L3)    0.963    0.010   93.200    0.000    0.633    0.856\n  Neg1 =~                                                               \n    NAT1P1    (L4)    0.964    0.012   79.273    0.000    0.558    0.766\n    NAT1P2    (L5)    0.997    0.011   90.664    0.000    0.578    0.891\n    NAT1P3    (L6)    1.039    0.011   93.757    0.000    0.602    0.906\n  Pos2 =~                                                               \n    PAT2P1    (L1)    1.038    0.010  102.905    0.000    0.695    0.911\n    PAT2P2    (L2)    0.998    0.010   98.574    0.000    0.668    0.901\n    PAT2P3    (L3)    0.963    0.010   93.200    0.000    0.645    0.878\n  Neg2 =~                                                               \n    NAT2P1    (L4)    0.964    0.012   79.273    0.000    0.532    0.820\n    NAT2P2    (L5)    0.997    0.011   90.664    0.000    0.550    0.873\n    NAT2P3    (L6)    1.039    0.011   93.757    0.000    0.574    0.889\n  Pos3 =~                                                               \n    PAT3P1    (L1)    1.038    0.010  102.905    0.000    0.693    0.898\n    PAT3P2    (L2)    0.998    0.010   98.574    0.000    0.666    0.864\n    PAT3P3    (L3)    0.963    0.010   93.200    0.000    0.643    0.853\n  Neg3 =~                                                               \n    NAT3P1    (L4)    0.964    0.012   79.273    0.000    0.567    0.840\n    NAT3P2    (L5)    0.997    0.011   90.664    0.000    0.587    0.886\n    NAT3P3    (L6)    1.039    0.011   93.757    0.000    0.612    0.868\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .PAT1P1 ~~                                                             \n   .PAT2P1            0.010    0.006    1.643    0.100    0.010    0.090\n   .PAT3P1            0.008    0.007    1.171    0.242    0.008    0.065\n .PAT2P1 ~~                                                             \n   .PAT3P1            0.000    0.006    0.046    0.963    0.000    0.003\n .PAT1P2 ~~                                                             \n   .PAT2P2            0.008    0.006    1.304    0.192    0.008    0.068\n   .PAT3P2            0.012    0.007    1.761    0.078    0.012    0.089\n .PAT2P2 ~~                                                             \n   .PAT3P2            0.005    0.006    0.796    0.426    0.005    0.040\n .PAT1P3 ~~                                                             \n   .PAT2P3            0.011    0.006    1.784    0.074    0.011    0.084\n   .PAT3P3            0.013    0.007    1.780    0.075    0.013    0.084\n .PAT2P3 ~~                                                             \n   .PAT3P3            0.010    0.006    1.482    0.138    0.010    0.069\n .NAT1P1 ~~                                                             \n   .NAT2P1            0.029    0.007    3.931    0.000    0.029    0.166\n   .NAT3P1            0.011    0.007    1.474    0.141    0.011    0.063\n .NAT2P1 ~~                                                             \n   .NAT3P1            0.010    0.006    1.625    0.104    0.010    0.073\n .NAT1P2 ~~                                                             \n   .NAT2P2           -0.000    0.005   -0.038    0.970   -0.000   -0.002\n   .NAT3P2            0.005    0.005    1.052    0.293    0.005    0.059\n .NAT2P2 ~~                                                             \n   .NAT3P2            0.008    0.005    1.654    0.098    0.008    0.089\n .NAT1P3 ~~                                                             \n   .NAT2P3           -0.000    0.005   -0.079    0.937   -0.000   -0.005\n   .NAT3P3           -0.006    0.006   -1.103    0.270   -0.006   -0.063\n .NAT2P3 ~~                                                             \n   .NAT3P3           -0.004    0.006   -0.780    0.435   -0.004   -0.042\n  Pos1 ~~                                                               \n    Pos2              0.243    0.019   12.903    0.000    0.552    0.552\n    Pos3              0.231    0.019   12.304    0.000    0.526    0.526\n    Neg1             -0.062    0.015   -4.210    0.000   -0.163   -0.163\n    Neg2             -0.059    0.014   -4.163    0.000   -0.162   -0.162\n    Neg3             -0.014    0.015   -0.921    0.357   -0.036   -0.036\n  Pos2 ~~                                                               \n    Pos3              0.243    0.019   12.757    0.000    0.544    0.544\n  Neg1 ~~                                                               \n    Pos2             -0.059    0.015   -3.979    0.000   -0.153   -0.153\n  Pos2 ~~                                                               \n    Neg2             -0.091    0.014   -6.280    0.000   -0.246   -0.246\n    Neg3             -0.028    0.015   -1.875    0.061   -0.072   -0.072\n  Neg1 ~~                                                               \n    Pos3             -0.011    0.015   -0.731    0.465   -0.028   -0.028\n  Neg2 ~~                                                               \n    Pos3             -0.047    0.014   -3.305    0.001   -0.128   -0.128\n  Pos3 ~~                                                               \n    Neg3             -0.079    0.015   -5.118    0.000   -0.200   -0.200\n  Neg1 ~~                                                               \n    Neg2              0.152    0.014   11.288    0.000    0.477    0.477\n    Neg3              0.151    0.014   10.634    0.000    0.443    0.443\n  Neg2 ~~                                                               \n    Neg3              0.167    0.014   12.005    0.000    0.514    0.514\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .PAT1P1    (t1)   -0.123    0.032   -3.855    0.000   -0.123   -0.160\n   .PAT1P2    (t2)   -0.100    0.032   -3.107    0.002   -0.100   -0.134\n   .PAT1P3    (t3)    0.223    0.033    6.799    0.000    0.223    0.301\n   .NAT1P1    (t4)    0.225    0.022   10.149    0.000    0.225    0.309\n   .NAT1P2    (t5)   -0.081    0.019   -4.220    0.000   -0.081   -0.126\n   .NAT1P3    (t6)   -0.144    0.019   -7.423    0.000   -0.144   -0.217\n   .PAT2P1    (t7)   -0.127    0.032   -3.989    0.000   -0.127   -0.166\n   .PAT2P2    (t8)   -0.099    0.032   -3.094    0.002   -0.099   -0.133\n   .PAT2P3    (t9)    0.225    0.033    6.910    0.000    0.225    0.307\n   .NAT2P1   (t10)    0.149    0.022    6.793    0.000    0.149    0.230\n   .NAT2P2   (t11)   -0.061    0.020   -3.101    0.002   -0.061   -0.098\n   .NAT2P3   (t12)   -0.087    0.020   -4.399    0.000   -0.087   -0.136\n   .PAT3P1   (t13)   -0.157    0.031   -5.018    0.000   -0.157   -0.203\n   .PAT3P2   (t14)   -0.077    0.032   -2.426    0.015   -0.077   -0.099\n   .PAT3P3   (t15)    0.234    0.032    7.256    0.000    0.234    0.310\n   .NAT3P1   (t16)    0.136    0.022    6.076    0.000    0.136    0.202\n   .NAT3P2   (t17)   -0.066    0.020   -3.251    0.001   -0.066   -0.100\n   .NAT3P3   (t18)   -0.070    0.021   -3.384    0.001   -0.070   -0.100\n    Pos1              3.000    0.024  124.639    0.000    4.563    4.563\n    Neg1              1.537    0.021   71.637    0.000    2.652    2.652\n    Pos2              3.012    0.024  124.219    0.000    4.502    4.502\n    Neg2              1.604    0.020   78.947    0.000    2.908    2.908\n    Pos3              2.931    0.024  119.853    0.000    4.391    4.391\n    Neg3              1.647    0.022   76.092    0.000    2.797    2.797\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .PAT1P1            0.130    0.010   13.072    0.000    0.130    0.219\n   .PAT1P2            0.124    0.009   13.267    0.000    0.124    0.223\n   .PAT1P3            0.147    0.010   14.837    0.000    0.147    0.268\n   .NAT1P1            0.220    0.013   17.540    0.000    0.220    0.414\n   .NAT1P2            0.087    0.007   11.767    0.000    0.087    0.207\n   .NAT1P3            0.079    0.008   10.385    0.000    0.079    0.179\n   .PAT2P1            0.099    0.008   12.350    0.000    0.099    0.170\n   .PAT2P2            0.104    0.008   13.257    0.000    0.104    0.188\n   .PAT2P3            0.123    0.008   14.898    0.000    0.123    0.229\n   .NAT2P1            0.138    0.009   15.933    0.000    0.138    0.327\n   .NAT2P2            0.094    0.007   13.203    0.000    0.094    0.237\n   .NAT2P3            0.087    0.007   11.961    0.000    0.087    0.209\n   .PAT3P1            0.115    0.010   11.939    0.000    0.115    0.193\n   .PAT3P2            0.150    0.010   14.356    0.000    0.150    0.253\n   .PAT3P3            0.154    0.010   14.973    0.000    0.154    0.272\n   .NAT3P1            0.134    0.009   15.181    0.000    0.134    0.294\n   .NAT3P2            0.094    0.008   12.427    0.000    0.094    0.215\n   .NAT3P3            0.123    0.009   13.604    0.000    0.123    0.247\n    Pos1              0.432    0.024   18.358    0.000    1.000    1.000\n    Neg1              0.336    0.018   18.230    0.000    1.000    1.000\n    Pos2              0.448    0.024   18.754    0.000    1.000    1.000\n    Neg2              0.304    0.017   18.188    0.000    1.000    1.000\n    Pos3              0.446    0.024   18.349    0.000    1.000    1.000\n    Neg3              0.347    0.019   18.230    0.000    1.000    1.000\n\nConstraints:\n                                               |Slack|\n    L1 - (3-L2-L3)                               0.000\n    L4 - (3-L5-L6)                               0.000\n    t1 - (0-t2-t3)                               0.000\n    t4 - (0-t5-t6)                               0.000\n    t7 - (0-t8-t9)                               0.000\n    t10 - (0-t11-t12)                            0.000\n    t13 - (0-t14-t15)                            0.000\n    t16 - (0-t17-t18)                            0.000\n\n\n\n\n\n76.4.3 Modello di Invarianza Forte\nIl modello di invarianza forte è un passaggio ulteriore nell’analisi dell’invarianza in un contesto di modellazione SEM longitudinale. Mentre l’invarianza configurale si concentra sulla struttura fattoriale e l’invarianza debole aggiunge l’uguaglianza dei carichi fattoriali, l’invarianza forte va oltre per includere anche l’uguaglianza delle medie degli indicatori.\nCome nei modelli di invarianza debole, i carichi fattoriali (L1, L2, L3, L4, L5, L6) sono mantenuti uguali attraverso i diversi momenti temporali, indicando che la forza della relazione tra le variabili latenti e i loro indicatori è costante.\nIl modello impone che le medie degli indicatori siano uguali attraverso i diversi momenti temporali. Questo è indicato dalle equazioni come PAT1P1 ~ t1*1, PAT2P1 ~ t1*1, e PAT3P1 ~ t1*1, dove t1 è lo stesso in tutti e tre i momenti temporali.\nIl modello continua a stimare separatamente la varianza degli indicatori e la covarianza sia tra gli indicatori in diversi momenti temporali sia tra le variabili latenti.\nSono imposti alcuni vincoli, come t1 == 0 - t2 - t3, che servono a identificare il modello e riflettono ipotesi teoriche sulle relazioni tra gli indicatori.\nL’invarianza forte è fondamentale per garantire che le misure di un costrutto siano completamente comparabili nel tempo o tra i gruppi. Se un modello dimostra invarianza forte, significa che non solo la relazione tra le variabili latenti e i loro indicatori è costante, ma anche che il livello di base di ciascun indicatore è lo stesso. Questo è cruciale per confronti delle medie latenti o per esaminare i cambiamenti nel tempo.\n\nmod_strong &lt;- \"\n    ### loadings\n    Pos1 =~ L1*PAT1P1 + L2*PAT1P2 + L3*PAT1P3\n    Neg1 =~ L4*NAT1P1 + L5*NAT1P2 + L6*NAT1P3\n    Pos2 =~ L1*PAT2P1 + L2*PAT2P2 + L3*PAT2P3\n    Neg2 =~ L4*NAT2P1 + L5*NAT2P2 + L6*NAT2P3\n    Pos3 =~ L1*PAT3P1 + L2*PAT3P2 + L3*PAT3P3\n    Neg3 =~ L4*NAT3P1 + L5*NAT3P2 + L6*NAT3P3\n    L1 == 3 - L2 - L3\n    L4 == 3 - L5 - L6\n\n    ### factor variance\n    Pos1 ~~ Pos1\n    Neg1 ~~ Neg1\n    Pos2 ~~ Pos2\n    Neg2 ~~ Neg2\n    Pos3 ~~ Pos3\n    Neg3 ~~ Neg3\n\n    ### factor covariance\n    Pos1 ~~ Pos2 + Pos3 + Neg1 + Neg2 + Neg3\n    Pos2 ~~ Pos3 + Neg1 + Neg2 + Neg3\n    Pos3 ~~ Neg1 + Neg2 + Neg3\n    Neg1 ~~ Neg2 + Neg3\n    Neg2 ~~ Neg3\n\n    ### residual variance\n    PAT1P1 ~~ PAT1P1\n    PAT1P2 ~~ PAT1P2\n    PAT1P3 ~~ PAT1P3\n    NAT1P1 ~~ NAT1P1\n    NAT1P2 ~~ NAT1P2\n    NAT1P3 ~~ NAT1P3\n    PAT2P1 ~~ PAT2P1\n    PAT2P2 ~~ PAT2P2\n    PAT2P3 ~~ PAT2P3\n    NAT2P1 ~~ NAT2P1\n    NAT2P2 ~~ NAT2P2\n    NAT2P3 ~~ NAT2P3\n    PAT3P1 ~~ PAT3P1\n    PAT3P2 ~~ PAT3P2\n    PAT3P3 ~~ PAT3P3\n    NAT3P1 ~~ NAT3P1\n    NAT3P2 ~~ NAT3P2\n    NAT3P3 ~~ NAT3P3\n\n    ### cross-time correlated residuals\n    PAT1P1 ~~ PAT2P1 + PAT3P1\n    PAT2P1 ~~ PAT3P1\n    PAT1P2 ~~ PAT2P2 + PAT3P2\n    PAT2P2 ~~ PAT3P2\n    PAT1P3 ~~ PAT2P3 + PAT3P3\n    PAT2P3 ~~ PAT3P3\n    NAT1P1 ~~ NAT2P1 + NAT3P1\n    NAT2P1 ~~ NAT3P1\n    NAT1P2 ~~ NAT2P2 + NAT3P2\n    NAT2P2 ~~ NAT3P2\n    NAT1P3 ~~ NAT2P3 + NAT3P3\n    NAT2P3 ~~ NAT3P3\n\n    ## latent mean\n    Pos1 ~ 1\n    Neg1 ~ 1\n    Pos2 ~ 1\n    Neg2 ~ 1\n    Pos3 ~ 1\n    Neg3 ~ 1\n\n    ## intercept\n    PAT1P1 ~ t1*1\n    PAT1P2 ~ t2*1\n    PAT1P3 ~ t3*1\n    NAT1P1 ~ t4*1\n    NAT1P2 ~ t5*1\n    NAT1P3 ~ t6*1\n    PAT2P1 ~ t1*1\n    PAT2P2 ~ t2*1\n    PAT2P3 ~ t3*1\n    NAT2P1 ~ t4*1\n    NAT2P2 ~ t5*1\n    NAT2P3 ~ t6*1\n    PAT3P1 ~ t1*1\n    PAT3P2 ~ t2*1\n    PAT3P3 ~ t3*1\n    NAT3P1 ~ t4*1\n    NAT3P2 ~ t5*1\n    NAT3P3 ~ t6*1\n\n    t1 == 0 - t2 - t3\n    t4 == 0 - t5 - t6\n    \"\n\n\nfit_strong &lt;- lavaan(mod_strong, data = dat, meanstructure = TRUE)\n\n\nsummary(fit_strong, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 148 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        99\n  Number of equality constraints                    28\n\n  Number of observations                           823\n\nModel Test User Model:\n                                                      \n  Test statistic                               418.737\n  Degrees of freedom                               118\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                             11131.067\n  Degrees of freedom                               153\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.973\n  Tucker-Lewis Index (TLI)                       0.964\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -10577.963\n  Loglikelihood unrestricted model (H1)     -10368.594\n                                                      \n  Akaike (AIC)                               21297.925\n  Bayesian (BIC)                             21632.545\n  Sample-size adjusted Bayesian (SABIC)      21407.076\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.056\n  90 Percent confidence interval - lower         0.050\n  90 Percent confidence interval - upper         0.061\n  P-value H_0: RMSEA &lt;= 0.050                    0.052\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.037\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Pos1 =~                                                               \n    PAT1P1    (L1)    1.040    0.010  103.200    0.000    0.684    0.884\n    PAT1P2    (L2)    0.997    0.010   98.587    0.000    0.656    0.881\n    PAT1P3    (L3)    0.963    0.010   93.301    0.000    0.633    0.856\n  Neg1 =~                                                               \n    NAT1P1    (L4)    0.957    0.012   78.488    0.000    0.553    0.757\n    NAT1P2    (L5)    0.999    0.011   90.830    0.000    0.578    0.891\n    NAT1P3    (L6)    1.044    0.011   94.108    0.000    0.604    0.906\n  Pos2 =~                                                               \n    PAT2P1    (L1)    1.040    0.010  103.200    0.000    0.696    0.911\n    PAT2P2    (L2)    0.997    0.010   98.587    0.000    0.667    0.900\n    PAT2P3    (L3)    0.963    0.010   93.301    0.000    0.644    0.878\n  Neg2 =~                                                               \n    NAT2P1    (L4)    0.957    0.012   78.488    0.000    0.527    0.816\n    NAT2P2    (L5)    0.999    0.011   90.830    0.000    0.550    0.873\n    NAT2P3    (L6)    1.044    0.011   94.108    0.000    0.576    0.891\n  Pos3 =~                                                               \n    PAT3P1    (L1)    1.040    0.010  103.200    0.000    0.694    0.898\n    PAT3P2    (L2)    0.997    0.010   98.587    0.000    0.665    0.864\n    PAT3P3    (L3)    0.963    0.010   93.301    0.000    0.642    0.853\n  Neg3 =~                                                               \n    NAT3P1    (L4)    0.957    0.012   78.488    0.000    0.563    0.836\n    NAT3P2    (L5)    0.999    0.011   90.830    0.000    0.588    0.887\n    NAT3P3    (L6)    1.044    0.011   94.108    0.000    0.614    0.868\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Pos1 ~~                                                               \n    Pos2              0.243    0.019   12.901    0.000    0.552    0.552\n    Pos3              0.231    0.019   12.306    0.000    0.527    0.527\n    Neg1             -0.062    0.015   -4.219    0.000   -0.164   -0.164\n    Neg2             -0.059    0.014   -4.163    0.000   -0.162   -0.162\n    Neg3             -0.014    0.015   -0.919    0.358   -0.035   -0.035\n  Pos2 ~~                                                               \n    Pos3              0.243    0.019   12.758    0.000    0.544    0.544\n  Neg1 ~~                                                               \n    Pos2             -0.059    0.015   -3.971    0.000   -0.152   -0.152\n  Pos2 ~~                                                               \n    Neg2             -0.091    0.014   -6.291    0.000   -0.246   -0.246\n    Neg3             -0.028    0.015   -1.882    0.060   -0.072   -0.072\n  Neg1 ~~                                                               \n    Pos3             -0.011    0.015   -0.722    0.470   -0.028   -0.028\n  Neg2 ~~                                                               \n    Pos3             -0.047    0.014   -3.309    0.001   -0.128   -0.128\n  Pos3 ~~                                                               \n    Neg3             -0.079    0.015   -5.140    0.000   -0.201   -0.201\n  Neg1 ~~                                                               \n    Neg2              0.152    0.013   11.258    0.000    0.475    0.475\n    Neg3              0.150    0.014   10.606    0.000    0.442    0.442\n  Neg2 ~~                                                               \n    Neg3              0.166    0.014   11.985    0.000    0.513    0.513\n .PAT1P1 ~~                                                             \n   .PAT2P1            0.010    0.006    1.662    0.096    0.010    0.091\n   .PAT3P1            0.007    0.007    1.089    0.276    0.007    0.061\n .PAT2P1 ~~                                                             \n   .PAT3P1           -0.000    0.006   -0.017    0.986   -0.000   -0.001\n .PAT1P2 ~~                                                             \n   .PAT2P2            0.008    0.006    1.325    0.185    0.008    0.069\n   .PAT3P2            0.012    0.007    1.749    0.080    0.012    0.088\n .PAT2P2 ~~                                                             \n   .PAT3P2            0.005    0.006    0.780    0.435    0.005    0.039\n .PAT1P3 ~~                                                             \n   .PAT2P3            0.011    0.006    1.785    0.074    0.011    0.084\n   .PAT3P3            0.013    0.007    1.782    0.075    0.013    0.084\n .PAT2P3 ~~                                                             \n   .PAT3P3            0.010    0.006    1.486    0.137    0.010    0.070\n .NAT1P1 ~~                                                             \n   .NAT2P1            0.028    0.007    3.711    0.000    0.028    0.156\n   .NAT3P1            0.009    0.007    1.149    0.251    0.009    0.049\n .NAT2P1 ~~                                                             \n   .NAT3P1            0.011    0.006    1.798    0.072    0.011    0.080\n .NAT1P2 ~~                                                             \n   .NAT2P2            0.000    0.005    0.038    0.970    0.000    0.002\n   .NAT3P2            0.006    0.005    1.169    0.243    0.006    0.066\n .NAT2P2 ~~                                                             \n   .NAT3P2            0.008    0.005    1.642    0.101    0.008    0.089\n .NAT1P3 ~~                                                             \n   .NAT2P3           -0.001    0.005   -0.176    0.861   -0.001   -0.011\n   .NAT3P3           -0.008    0.006   -1.350    0.177   -0.008   -0.077\n .NAT2P3 ~~                                                             \n   .NAT3P3           -0.004    0.006   -0.713    0.476   -0.004   -0.039\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    Pos1              3.000    0.024  124.671    0.000    4.563    4.563\n    Neg1              1.520    0.021   71.482    0.000    2.629    2.629\n    Pos2              3.012    0.024  124.262    0.000    4.502    4.502\n    Neg2              1.605    0.020   79.147    0.000    2.912    2.912\n    Pos3              2.929    0.024  119.856    0.000    4.389    4.389\n    Neg3              1.647    0.022   76.221    0.000    2.800    2.800\n   .PAT1P1    (t1)   -0.140    0.031   -4.557    0.000   -0.140   -0.181\n   .PAT1P2    (t2)   -0.089    0.031   -2.892    0.004   -0.089   -0.120\n   .PAT1P3    (t3)    0.229    0.031    7.275    0.000    0.229    0.309\n   .NAT1P1    (t4)    0.174    0.021    8.461    0.000    0.174    0.238\n   .NAT1P2    (t5)   -0.068    0.018   -3.691    0.000   -0.068   -0.104\n   .NAT1P3    (t6)   -0.107    0.018   -5.781    0.000   -0.107   -0.160\n   .PAT2P1    (t1)   -0.140    0.031   -4.557    0.000   -0.140   -0.183\n   .PAT2P2    (t2)   -0.089    0.031   -2.892    0.004   -0.089   -0.120\n   .PAT2P3    (t3)    0.229    0.031    7.275    0.000    0.229    0.312\n   .NAT2P1    (t4)    0.174    0.021    8.461    0.000    0.174    0.270\n   .NAT2P2    (t5)   -0.068    0.018   -3.691    0.000   -0.068   -0.107\n   .NAT2P3    (t6)   -0.107    0.018   -5.781    0.000   -0.107   -0.165\n   .PAT3P1    (t1)   -0.140    0.031   -4.557    0.000   -0.140   -0.181\n   .PAT3P2    (t2)   -0.089    0.031   -2.892    0.004   -0.089   -0.116\n   .PAT3P3    (t3)    0.229    0.031    7.275    0.000    0.229    0.304\n   .NAT3P1    (t4)    0.174    0.021    8.461    0.000    0.174    0.259\n   .NAT3P2    (t5)   -0.068    0.018   -3.691    0.000   -0.068   -0.102\n   .NAT3P3    (t6)   -0.107    0.018   -5.781    0.000   -0.107   -0.151\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    Pos1              0.432    0.024   18.357    0.000    1.000    1.000\n    Neg1              0.334    0.018   18.210    0.000    1.000    1.000\n    Pos2              0.448    0.024   18.753    0.000    1.000    1.000\n    Neg2              0.304    0.017   18.183    0.000    1.000    1.000\n    Pos3              0.445    0.024   18.343    0.000    1.000    1.000\n    Neg3              0.346    0.019   18.219    0.000    1.000    1.000\n   .PAT1P1            0.130    0.010   13.046    0.000    0.130    0.218\n   .PAT1P2            0.124    0.009   13.297    0.000    0.124    0.224\n   .PAT1P3            0.147    0.010   14.841    0.000    0.147    0.268\n   .NAT1P1            0.228    0.013   17.679    0.000    0.228    0.427\n   .NAT1P2            0.086    0.007   11.639    0.000    0.086    0.206\n   .NAT1P3            0.080    0.008   10.321    0.000    0.080    0.179\n   .PAT2P1            0.099    0.008   12.317    0.000    0.099    0.170\n   .PAT2P2            0.104    0.008   13.290    0.000    0.104    0.189\n   .PAT2P3            0.123    0.008   14.903    0.000    0.123    0.229\n   .NAT2P1            0.139    0.009   16.054    0.000    0.139    0.333\n   .NAT2P2            0.094    0.007   13.168    0.000    0.094    0.237\n   .NAT2P3            0.086    0.007   11.823    0.000    0.086    0.206\n   .PAT3P1            0.115    0.010   11.933    0.000    0.115    0.193\n   .PAT3P2            0.151    0.010   14.387    0.000    0.151    0.254\n   .PAT3P3            0.154    0.010   14.968    0.000    0.154    0.272\n   .NAT3P1            0.136    0.009   15.342    0.000    0.136    0.301\n   .NAT3P2            0.094    0.008   12.327    0.000    0.094    0.213\n   .NAT3P3            0.123    0.009   13.539    0.000    0.123    0.246\n\nConstraints:\n                                               |Slack|\n    L1 - (3-L2-L3)                               0.000\n    L4 - (3-L5-L6)                               0.000\n    t1 - (0-t2-t3)                               0.000\n    t4 - (0-t5-t6)                               0.000\n\n\n\n\n\n76.4.4 Confronto tra Modelli\nUn confronto tra i modelli precedenti può essere eseguito mediante il test del rapporto tra verosimiglianze.\n\nout &lt;- compareFit(fit_null, fitSEM, fit_config, fit_weak, fit_strong)\nsummary(out) |&gt; \n    print()\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n            Df   AIC   BIC    Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)\nfit_config 102 21263 21673   352.23                                      \nfit_weak   110 21262 21634   366.56       14.3 0.03101       8  0.0735478\nfit_strong 118 21298 21633   418.74       52.2 0.08191       8  1.557e-08\nfitSEM     124 21309 21615   441.52       22.8 0.05830       6  0.0008723\nfit_null   177 31974 32031 11213.10    10771.6 0.49571      53  &lt; 2.2e-16\n              \nfit_config    \nfit_weak   .  \nfit_strong ***\nfitSEM     ***\nfit_null   ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n####################### Model Fit Indices ###########################\n                     chisq  df pvalue        rmsea          cfi\nfit_config 352.232&lt;U+2020&gt; 102   .000        .055  .977&lt;U+2020&gt;\nfit_weak          366.562  110   .000 .053&lt;U+2020&gt;        .977 \nfit_strong        418.737  118   .000        .056         .973 \nfitSEM            441.520  124   .000        .056         .971 \nfit_null        11213.103  177   .000        .275         .000 \n                    tli         srmr               aic               bic\nfit_config        .966         .035         21263.420         21673.447 \nfit_weak   .967&lt;U+2020&gt; .035&lt;U+2020&gt; 21261.750&lt;U+2020&gt;        21634.074 \nfit_strong        .964         .037         21297.925         21632.545 \nfitSEM            .964         .045         21308.709  21615.051&lt;U+2020&gt;\nfit_null          .131         .328         31974.291         32030.846 \n\n################## Differences in Fit Indices #######################\n                      df  rmsea    cfi    tli  srmr       aic       bic\nfit_weak - fit_config  8 -0.001 -0.001  0.002 0.000    -1.670   -39.373\nfit_strong - fit_weak  8  0.002 -0.004 -0.003 0.002    36.175    -1.529\nfitSEM - fit_strong    6  0.000 -0.002  0.000 0.008    10.783   -17.494\nfit_null - fitSEM     53  0.219 -0.971 -0.833 0.282 10665.582 10415.795\n\nThe following lavaan models were compared:\n    fit_config\n    fit_weak\n    fit_strong\n    fitSEM\n    fit_null\nTo view results, assign the compareFit() output to an object and  use the summary() method; see the class?FitDiff help page.\n\n\nI valori di Chisq indicano il grado di adattamento dei modelli ai dati. Valori più bassi indicano un migliore adattamento. Dall’elenco, fit_config ha il valore di Chi-square più basso, suggerendo il miglior adattamento tra i modelli confrontati. Un valore di Chisq diff significativo (basso valore p) suggerisce che il modello più vincolato ha un adattamento significativamente peggiore.Chisq diff di 14.3 con un valore p di 0.0735 indica che non c’è una differenza statisticamente significativa nel fit tra i modelli configurale e debole. Questo suggerisce che l’aggiunta dell’invarianza debole (uguaglianza dei carichi fattoriali) non peggiora significativamente il fit. Chisq diff è 52.2 con un valore p molto basso (1.557e-08), indica che l’aggiunta dell’invarianza forte (uguaglianza delle medie) peggiora significativamente il fit rispetto al modello debole. Una differenza di 22.8 nel Chi-square e un valore p basso (0.0008723) suggeriscono che il modello forte ha un fit significativamente peggiore rispetto al modello SEM base. Il modello nullo ha un valore molto alto di Chi-square, indicando, come previsto, un adattamento molto scarso. Questo è normale per i modelli nulli e serve come riferimento estremo.\nIl RMSEA è un indice di bontà di adattamento che considera la complessità del modello. Valori inferiori a 0.05 indicano un buon adattamento, valori tra 0.05 e 0.08 indicano un adattamento accettabile, e valori superiori a 0.10 sono generalmente considerati inaccettabili. In questo caso, il RMSEA aumenta da fit_config a fit_strong, suggerendo un peggioramento dell’adattamento con l’aggiunta di vincoli più forti.\nIn conclusione, i risultati indicano che l’aggiunta di vincoli di invarianza debole non peggiora significativamente il fit, mentre l’aggiunta di vincoli di invarianza forte riduce in modo significativo la bontà di adattamento del modello. Questo suggerisce che, mentre i carichi fattoriali possono essere considerati invarianti tra i gruppi o nel tempo, le medie degli indicatori potrebbero non esserlo.\nLittle (2023) nota che, con un campione così grande, disponiamo di un livello di potere statistico sufficiente anche per rilevare differenze minuscole. Quindi, i risultati dei test statistici precedenti vanno presi con un grano di sale. In particolare, Little (2023) nota che il modello di invarianza forte fornisce evidenze di un adattamento soddisfacente e che il peggioramento dell’adattamento rispetto al modello di invarianza debole è, quantitativamente, estremamente piccolo se esaminato rispetto alle dimensioni di CFI, TLI, RMSEA, e SRMR. Per queste ragioni, Little (2023) conclude affermando che il modello di invarianza forte risulta giustificato da questi dati. I criteri per determinare una perdita eccessiva dell’adattamento, data la potenza della dimensione del campione, sono un valore p inferiore a .001, un cambiamento nel CFI superiore a .002, o una stima puntuale dell’RMSEA che cade al di fuori dell’intervallo di confidenza del modello di invarianza forte.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>76</span>  <span class='chapter-title'>Specificare e Interpretare un Modello Longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/05_intro_panel.html#riflessioni-conclusive",
    "href": "chapters/lgm/05_intro_panel.html#riflessioni-conclusive",
    "title": "76  Specificare e Interpretare un Modello Longitudinale",
    "section": "76.5 Riflessioni Conclusive",
    "text": "76.5 Riflessioni Conclusive\nIn questo capitolo, abbiamo affrontato i passaggi e molte delle problematiche associate all’adattamento di un modello panel standard ai dati longitudinali. Nei dati longitudinali, il continuum lungo il quale sono ordinati i costrutti è rappresentato dal tempo.\nI modelli panel di base che abbiamo esaminato qui sono solo alcuni tra i vari tipi di modelli che possono essere adattati ai dati panel. In questo contesto, l’analisi dei dati longitudinali implica un approccio sistematico per esaminare come determinati costrutti o variabili cambiano nel corso del tempo. Questo può includere l’analisi di tendenze, cicli o pattern nei dati raccolti in diversi momenti.\nAdattare un modello panel a dati longitudinali richiede una comprensione approfondita sia della natura dei dati sia delle tecniche statistiche utilizzate. Questo processo può comportare sfide specifiche, come la gestione di dati mancanti, l’accounting per la variabilità sia tra i soggetti che all’interno dello stesso soggetto nel tempo, e la scelta del modello statistico più appropriato in base alla struttura dei dati e agli obiettivi della ricerca.\nI modelli panel di base, come quelli discussi in questo capitolo, sono un punto di partenza fondamentale. Tuttavia, esistono molte altre varianti e approfondimenti di questi modelli che possono essere esplorati per adattarsi meglio a scenari complessi o per rispondere a specifiche domande di ricerca. Questi includono modelli panel più avanzati che possono tener conto di effetti casuali, effetti fissi, o che possono essere usati per analizzare le interazioni tra variabili nel tempo.\nL’obiettivo finale di questi modelli è di fornire una rappresentazione accurata di come i costrutti si evolvono nel tempo, permettendo ai ricercatori di trarre conclusioni affidabili dai loro dati.\n\n\n\n\nLittle, T. D. (2023). Longitudinal structural equation modeling. Guilford Press.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>76</span>  <span class='chapter-title'>Specificare e Interpretare un Modello Longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/07_growth_1.html",
    "href": "chapters/lgm/07_growth_1.html",
    "title": "77  Curve di crescita latente",
    "section": "",
    "text": "77.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nNel capitolo precedente, abbiamo esplorato come i modelli di Crescita Latente (LGM) possano essere correlati e confrontati con i modelli lineari ad effetti misti attraverso l’uso di dati simulati. In particolare, abbiamo osservato come sia possibile strutturare un modello LGM che incorpori un fattore latente per la variazione delle intercette individuali, il quale cattura le dinamiche del cambiamento delle medie nel tempo, e un secondo fattore latente che riflette le variazioni individuali nelle pendenze delle rette di regressione.\nAbbiamo esaminato in dettaglio il processo di definizione di questi fattori latenti, mettendo in atto una serie di vincoli sugli indicatori che identificano le variabili latenti. In particolare, abbiamo visto come l’applicazione dei vincoli 0, 1, 2, 3 alle saturazioni fattoriali per il fattore “pendenza” determini una relazione lineare tra la media del costrutto e il tempo. Questo approccio è particolarmente efficace quando le misurazioni del costrutto sono state effettuate a intervalli regolari.\nIn questo capitolo, ci dedicheremo all’approfondimento di questo argomento, spostando la nostra attenzione dall’ambito teorico e simulato all’analisi di un set di dati reali. Questo passaggio ci fornirà una visione più chiara e concreta di come questi modelli possano essere impiegati nell’analisi di dati longitudinali reali, con tutti le loro sfaccettature e sfide.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>77</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/07_growth_1.html#una-applicazione-concreta",
    "href": "chapters/lgm/07_growth_1.html#una-applicazione-concreta",
    "title": "77  Curve di crescita latente",
    "section": "77.2 Una applicazione concreta",
    "text": "77.2 Una applicazione concreta\nEsaminiamo l’adattamento di un modello LGM ad un campione di dati reali. In questo tutorial, considereremo il cambiamento nel rendimento in matematica dei bambini durante la scuola elementare e media utilizzando il set di dati NLSY-CYA {cite:p}grimm2016growth. Iniziamo a leggere i dati.\n\n#set filepath for data file\nfilepath &lt;- \"https://raw.githubusercontent.com/LRI-2/Data/main/GrowthModeling/nlsy_math_wide_R.dat\"\n\n# read in the text data file using the url() function\ndat &lt;- read.table(\n  file = url(filepath),\n  na.strings = \".\"\n) # indicates the missing data designator\n\n# copy data with new name\nnlsy_math_wide &lt;- dat\n\n# Give the variable names\nnames(nlsy_math_wide) &lt;- c(\n  \"id\", \"female\", \"lb_wght\", \"anti_k1\",\n  \"math2\", \"math3\", \"math4\", \"math5\", \"math6\", \"math7\", \"math8\",\n  \"age2\", \"age3\", \"age4\", \"age5\", \"age6\", \"age7\", \"age8\",\n  \"men2\", \"men3\", \"men4\", \"men5\", \"men6\", \"men7\", \"men8\",\n  \"spring2\", \"spring3\", \"spring4\", \"spring5\", \"spring6\", \"spring7\", \"spring8\",\n  \"anti2\", \"anti3\", \"anti4\", \"anti5\", \"anti6\", \"anti7\", \"anti8\"\n)\n\n# view the first few observations (and columns) in the data set\nhead(nlsy_math_wide[, 1:11], 10)\n\n\nA data.frame: 10 x 11\n\n\n\nid\nfemale\nlb_wght\nanti_k1\nmath2\nmath3\nmath4\nmath5\nmath6\nmath7\nmath8\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n201\n1\n0\n0\nNA\n38\nNA\n55\nNA\nNA\nNA\n\n\n2\n303\n1\n0\n1\n26\nNA\nNA\n33\nNA\nNA\nNA\n\n\n3\n2702\n0\n0\n0\n56\nNA\n58\nNA\nNA\nNA\n80\n\n\n4\n4303\n1\n0\n0\nNA\n41\n58\nNA\nNA\nNA\nNA\n\n\n5\n5002\n0\n0\n4\nNA\nNA\n46\nNA\n54\nNA\n66\n\n\n6\n5005\n1\n0\n0\n35\nNA\n50\nNA\n60\nNA\n59\n\n\n7\n5701\n0\n0\n2\nNA\n62\n61\nNA\nNA\nNA\nNA\n\n\n8\n6102\n0\n0\n0\nNA\nNA\n55\n67\nNA\n81\nNA\n\n\n9\n6801\n1\n0\n0\nNA\n54\nNA\n62\nNA\n66\nNA\n\n\n10\n6802\n0\n0\n0\nNA\n55\nNA\n66\nNA\n68\nNA\n\n\n\n\n\nIl nostro interesse specifico riguarda il cambiamento relativo alle misure ripetute di matematica, da math2 a math8. Selezioniamo dunque le variabili di interesse.\n\nnlsy_math_sub &lt;- nlsy_math_wide |&gt;\n    dplyr::select(\"id\", \"math2\", \"math3\", \"math4\", \"math5\", \"math6\", \"math7\", \"math8\")\n\nTrasformiamo i dati in formato long.\n\nnlsy_math_long &lt;- reshape(\n  data = nlsy_math_sub,\n  timevar = c(\"grade\"),\n  idvar = \"id\",\n  varying = c(\n    \"math2\", \"math3\", \"math4\",\n    \"math5\", \"math6\", \"math7\", \"math8\"\n  ),\n  direction = \"long\", sep = \"\"\n)\n\nOrdiniamo i dati in base alle variabili id e grade.\n\nnlsy_math_long &lt;- nlsy_math_long[order(nlsy_math_long$id, nlsy_math_long$grade), ]\n\nRimuoviamo gli NA dalla variabile math per potere generare il grafico con le traiettorie individuali di sviluppo.\n\nnlsy_math_long &lt;- nlsy_math_long[which(is.na(nlsy_math_long$math) == FALSE), ]\n\nEsaminiamo i dati grezzi.\n\nnlsy_math_long |&gt;\n  ggplot(aes(x = grade, y = math)) +\n  geom_point(\n    size = 1.2,\n    alpha = .8,\n    # to add some random noise for plotting purposes\n    position = \"jitter\"\n  ) +\n  labs(title = \"PAT Mathematics as a function of Grade\")\n\n\n\n\n\n\n\n\nAggiungiamo al grafico le retta dei minimi quadrati calcolata su tutti i dati (ignorando il ragruppamento dei dati in funzione dei partecipanti).\n\nnlsy_math_long |&gt;\n    ggplot(aes(x = grade, y = math)) +\n    geom_point(\n        size = 1.2,\n        alpha = .8,\n        # to add some random noise for plotting purposes\n        position = \"jitter\"\n    ) +\n    geom_smooth(\n        method = lm,\n        se = FALSE,\n        col = \"blue\",\n        linewidth = 1.5,\n        alpha = .8\n    ) + # to add regression line\n    labs(title = \"PAT Mathematics as a function of Grade\")\n\n\n\n\n\n\n\n\nEsaminiamo le traiettorie di cambiamento intra-individuale.\n\n# intraindividual change trajetories\nnlsy_math_long |&gt;\n  ggplot(\n    aes(x = grade, y = math, group = id)\n  ) + # setting variables\n  geom_point(size = .5) + # adding points to plot\n  geom_line(alpha = 0.3) + # adding lines to plot\n  # setting the x-axis with breaks and labels\n  scale_x_continuous(\n    limits = c(2, 8),\n    breaks = c(2, 3, 4, 5, 6, 7, 8),\n    name = \"Grade at Testing\"\n  ) +\n  # setting the y-axis with limits breaks and labels\n  scale_y_continuous(\n    limits = c(10, 90),\n    breaks = c(10, 30, 50, 70, 90),\n    name = \"PIAT Mathematics\"\n  )",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>77</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/07_growth_1.html#modello-di-assenza-di-crescita",
    "href": "chapters/lgm/07_growth_1.html#modello-di-assenza-di-crescita",
    "title": "77  Curve di crescita latente",
    "section": "77.3 Modello di assenza di crescita",
    "text": "77.3 Modello di assenza di crescita\nDall’analisi dei grafici precedenti, si osserva che i punteggi di matematica mostrano un incremento sistematico nel tempo. Per iniziare l’analisi, adotteremo un modello di assenza di crescita come benchmark di base per il confronto con modelli più complessi successivi.\nIn questo modello si assume che i punteggi di matematica degli studenti rimangano invariati nel corso del tempo. Esso mira a stimare, per ogni studente, il “valore vero” dei loro punteggi in matematica, senza prendere in considerazione eventuali variazioni nel tempo. Poiché non contempla la dinamica temporale dei punteggi, questo modello rappresenta una situazione di stallo o assenza di sviluppo, risultando spesso di limitato interesse e pertanto generalmente non viene adottato in analisi più approfondite.\nIl modello di assenza della crescita è caratterizzato dalla presenza di una variabile latente e di un’intercetta, la quale rappresenta un livello medio di performance che si mantiene costante nel tempo. Questa configurazione del modello permette di stabilire un punto di partenza per comprendere se e in che misura i punteggi di matematica variano effettivamente nel corso del tempo, quando confrontati con modelli che considerano la crescita o l’evoluzione dei punteggi.\nPer definire il modello di assenza di crescita, utilizziamo la seguente sintassi di lavaan.\n\nng_math_lavaan_model &lt;- ' \n  # latent variable definitions\n      #intercept\n      eta_1 =~ 1*math2\n      eta_1 =~ 1*math3\n      eta_1 =~ 1*math4\n      eta_1 =~ 1*math5\n      eta_1 =~ 1*math6\n      eta_1 =~ 1*math7\n      eta_1 =~ 1*math8\n\n  # factor variances\n      eta_1 ~~ eta_1\n\n  # covariances among factors \n      #none (only 1 factor)\n\n  # factor means \n      eta_1 ~ start(30)*1\n\n  # manifest variances (made equivalent by naming theta)\n      math2 ~~ theta*math2\n      math3 ~~ theta*math3\n      math4 ~~ theta*math4\n      math5 ~~ theta*math5\n      math6 ~~ theta*math6\n      math7 ~~ theta*math7\n      math8 ~~ theta*math8\n  # manifest means (fixed at zero)\n      math2 ~ 0*1\n      math3 ~ 0*1\n      math4 ~ 0*1\n      math5 ~ 0*1\n      math6 ~ 0*1\n      math7 ~ 0*1\n      math8 ~ 0*1\n' #end of model definition\n\nAdattiamo il modello ai dati.\n\nng_math_lavaan_fit &lt;- sem(ng_math_lavaan_model,\n  data = nlsy_math_wide,\n  meanstructure = TRUE,\n  estimator = \"ML\",\n  missing = \"fiml\"\n)\n\nNel codice fornito, l’opzione missing = \"fiml\" utilizzata nella funzione sem() specifica il metodo “Full Information Maximum Likelihood” (FIML) per gestire i dati mancanti nel dataset. FIML è un approccio sofisticato per la gestione dei dati mancanti in analisi statistiche complesse come i modelli SEM. A differenza di metodi più semplici come l’eliminazione lista per lista o l’imputazione media, FIML utilizza tutte le informazioni disponibili nel dataset, inclusi i pattern dei dati mancanti, per produrre stime dei parametri. Questo metodo è particolarmente utile quando si lavora con dataset longitudinali o complessi dove i dati mancanti sono comuni. FIML è considerato un approccio più accurato e meno distorto rispetto ad altri metodi, in quanto non si limita a utilizzare solo i casi completi, ma incorpora l’intero insieme di dati disponibili, comprese le osservazioni parziali.\nEsaminiamo la soluzione.\n\nsummary(ng_math_lavaan_fit, fit.measures = TRUE, standardized = TRUE) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 18 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         9\n  Number of equality constraints                     6\n\n                                                  Used       Total\n  Number of observations                           932         933\n  Number of missing patterns                        60            \n\nModel Test User Model:\n                                                      \n  Test statistic                              1759.002\n  Degrees of freedom                                32\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               862.334\n  Degrees of freedom                                21\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.000\n  Tucker-Lewis Index (TLI)                      -0.347\n                                                      \n  Robust Comparative Fit Index (CFI)             0.000\n  Robust Tucker-Lewis Index (TLI)                0.093\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -8745.952\n  Loglikelihood unrestricted model (H1)      -7866.451\n                                                      \n  Akaike (AIC)                               17497.903\n  Bayesian (BIC)                             17512.415\n  Sample-size adjusted Bayesian (SABIC)      17502.888\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.241\n  90 Percent confidence interval - lower         0.231\n  90 Percent confidence interval - upper         0.250\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n                                                      \n  Robust RMSEA                                   0.467\n  90 Percent confidence interval - lower         0.402\n  90 Percent confidence interval - upper         0.534\n  P-value H_0: Robust RMSEA &lt;= 0.050             0.000\n  P-value H_0: Robust RMSEA &gt;= 0.080             1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.480\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  eta_1 =~                                                              \n    math2             1.000                               6.850    0.536\n    math3             1.000                               6.850    0.536\n    math4             1.000                               6.850    0.536\n    math5             1.000                               6.850    0.536\n    math6             1.000                               6.850    0.536\n    math7             1.000                               6.850    0.536\n    math8             1.000                               6.850    0.536\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    eta_1            45.915    0.324  141.721    0.000    6.703    6.703\n   .math2             0.000                               0.000    0.000\n   .math3             0.000                               0.000    0.000\n   .math4             0.000                               0.000    0.000\n   .math5             0.000                               0.000    0.000\n   .math6             0.000                               0.000    0.000\n   .math7             0.000                               0.000    0.000\n   .math8             0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    eta_1            46.917    4.832    9.709    0.000    1.000    1.000\n   .math2   (thet)  116.682    4.548   25.656    0.000  116.682    0.713\n   .math3   (thet)  116.682    4.548   25.656    0.000  116.682    0.713\n   .math4   (thet)  116.682    4.548   25.656    0.000  116.682    0.713\n   .math5   (thet)  116.682    4.548   25.656    0.000  116.682    0.713\n   .math6   (thet)  116.682    4.548   25.656    0.000  116.682    0.713\n   .math7   (thet)  116.682    4.548   25.656    0.000  116.682    0.713\n   .math8   (thet)  116.682    4.548   25.656    0.000  116.682    0.713\n\n\n\nGeneriamo il diagramma di percorso.\n\nsemPaths(ng_math_lavaan_fit, what = \"path\", whatLabels = \"par\")\n\n\n\n\n\n\n\n\nCalcoliamo le traiettorie predette.\n\n#obtaining predicted factor scores for individuals\nnlsy_math_predicted &lt;- as.data.frame(cbind(nlsy_math_wide$id,lavPredict(ng_math_lavaan_fit)))\n\n#naming columns\nnames(nlsy_math_predicted) &lt;- c(\"id\", \"eta_1\")\n\n#looking at data\nhead(nlsy_math_predicted) \n\n\nA data.frame: 6 x 2\n\n\n\nid\neta_1\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n201\n46.17558\n\n\n2\n303\n38.59816\n\n\n3\n2702\n56.16725\n\n\n4\n4303\n47.51278\n\n\n5\n5002\n51.06429\n\n\n6\n5005\n49.05038\n\n\n\n\n\n\n# calculating implied manifest scores\nnlsy_math_predicted$math2 &lt;- 1 * nlsy_math_predicted$eta_1\nnlsy_math_predicted$math3 &lt;- 1 * nlsy_math_predicted$eta_1\nnlsy_math_predicted$math4 &lt;- 1 * nlsy_math_predicted$eta_1\nnlsy_math_predicted$math5 &lt;- 1 * nlsy_math_predicted$eta_1\nnlsy_math_predicted$math6 &lt;- 1 * nlsy_math_predicted$eta_1\nnlsy_math_predicted$math7 &lt;- 1 * nlsy_math_predicted$eta_1\nnlsy_math_predicted$math8 &lt;- 1 * nlsy_math_predicted$eta_1\n\n# reshaping wide to long\nnlsy_math_predicted_long &lt;- reshape(\n  data = nlsy_math_predicted,\n  timevar = c(\"grade\"),\n  idvar = \"id\",\n  varying = c(\n    \"math2\", \"math3\", \"math4\",\n    \"math5\", \"math6\", \"math7\", \"math8\"\n  ),\n  direction = \"long\", sep = \"\"\n)\n# sorting for easy viewing\n# order by id and time\nnlsy_math_predicted_long &lt;- nlsy_math_predicted_long[order(nlsy_math_predicted_long$id, nlsy_math_predicted_long$grade), ]\n\n# intraindividual change trajetories\nggplot(\n  data = nlsy_math_predicted_long, # data set\n  aes(x = grade, y = math, group = id)\n) + # setting variables\n  # geom_point(size=.5) + #adding points to plot\n  geom_line(alpha = 0.1) + # adding lines to plot\n  # setting the x-axis with breaks and labels\n  scale_x_continuous(\n    limits = c(2, 8),\n    breaks = c(2, 3, 4, 5, 6, 7, 8),\n    name = \"Grade at Testing\"\n  ) +\n  # setting the y-axis with limits breaks and labels\n  scale_y_continuous(\n    limits = c(10, 90),\n    breaks = c(10, 30, 50, 70, 90),\n    name = \"Predicted PIAT Mathematics\"\n  )\n\n\n\n\n\n\n\n\nDal grafico risulta evidente che il modello impiegato genera una serie di linee orizzontali, ognuna rappresentante la traiettoria statica dell’abilità matematica per ogni individuo. In questo modello, l’intercetta associata a ciascuna di queste linee orizzontali corrisponde al “valore vero” dell’abilità matematica di ogni bambino. Conformemente alle ipotesi del modello, questo valore si mantiene invariato nel corso del tempo, suggerendo che, secondo il modello, l’abilità matematica di ciascun individuo non subisce variazioni o sviluppi significativi nel periodo osservato.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>77</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/07_growth_1.html#modello-di-crescita-lineare",
    "href": "chapters/lgm/07_growth_1.html#modello-di-crescita-lineare",
    "title": "77  Curve di crescita latente",
    "section": "77.4 Modello di crescita lineare",
    "text": "77.4 Modello di crescita lineare\nNella discussione dei modelli di crescita, il modello di assenza di crescita viene sempre seguito dall’esame del modello di crescita lineare. Infatti, i modelli di crescita lineare rappresentano spesso il punto di partenza quando si cerca di comprendere il cambiamento all’interno dell’individuo. Successivamente, possono essere considerati anche modelli di crescita non lineare. Procediamo dunque all’implementazione di un modello di crescita latente lineare.\n\nlg_math_lavaan_model &lt;- '\n  # latent variable definitions\n      #intercept (note intercept is a reserved term)\n      eta_1 =~ 1*math2\n      eta_1 =~ 1*math3\n      eta_1 =~ 1*math4\n      eta_1 =~ 1*math5\n      eta_1 =~ 1*math6\n      eta_1 =~ 1*math7\n      eta_1 =~ 1*math8\n\n      #linear slope \n      eta_2 =~ 0*math2\n      eta_2 =~ 1*math3\n      eta_2 =~ 2*math4\n      eta_2 =~ 3*math5\n      eta_2 =~ 4*math6\n      eta_2 =~ 5*math7\n      eta_2 =~ 6*math8\n\n  # factor variances\n      eta_1 ~~ eta_1\n      eta_2 ~~ eta_2\n\n  # covariances among factors \n      eta_1 ~~ eta_2\n\n  # factor means \n      eta_1 ~ 1\n      eta_2 ~ 1\n\n  # manifest variances (made equivalent by naming theta)\n      math2 ~~ theta*math2\n      math3 ~~ theta*math3\n      math4 ~~ theta*math4\n      math5 ~~ theta*math5\n      math6 ~~ theta*math6\n      math7 ~~ theta*math7\n      math8 ~~ theta*math8\n  # manifest means (fixed at zero)\n      math2 ~ 0*1\n      math3 ~ 0*1\n      math4 ~ 0*1\n      math5 ~ 0*1\n      math6 ~ 0*1\n      math7 ~ 0*1\n      math8 ~ 0*1\n' #end of model definition\n\nAdattiamo il modello ai dati.\n\nlg_math_lavaan_fit &lt;- sem(lg_math_lavaan_model,\n  data = nlsy_math_wide,\n  meanstructure = TRUE,\n  estimator = \"ML\",\n  missing = \"fiml\"\n)\n\nEsaminiamo il risultato ottenuto.\n\nsummary(lg_math_lavaan_fit, fit.measures = TRUE, standardized = TRUE) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 38 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        12\n  Number of equality constraints                     6\n\n                                                  Used       Total\n  Number of observations                           932         933\n  Number of missing patterns                        60            \n\nModel Test User Model:\n                                                      \n  Test statistic                               204.484\n  Degrees of freedom                                29\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               862.334\n  Degrees of freedom                                21\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.791\n  Tucker-Lewis Index (TLI)                       0.849\n                                                      \n  Robust Comparative Fit Index (CFI)             0.896\n  Robust Tucker-Lewis Index (TLI)                0.925\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -7968.693\n  Loglikelihood unrestricted model (H1)      -7866.451\n                                                      \n  Akaike (AIC)                               15949.386\n  Bayesian (BIC)                             15978.410\n  Sample-size adjusted Bayesian (SABIC)      15959.354\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.081\n  90 Percent confidence interval - lower         0.070\n  90 Percent confidence interval - upper         0.091\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.550\n                                                      \n  Robust RMSEA                                   0.134\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.233\n  P-value H_0: Robust RMSEA &lt;= 0.050             0.136\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.792\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.121\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  eta_1 =~                                                              \n    math2             1.000                               8.035    0.800\n    math3             1.000                               8.035    0.799\n    math4             1.000                               8.035    0.792\n    math5             1.000                               8.035    0.779\n    math6             1.000                               8.035    0.762\n    math7             1.000                               8.035    0.742\n    math8             1.000                               8.035    0.719\n  eta_2 =~                                                              \n    math2             0.000                               0.000    0.000\n    math3             1.000                               0.856    0.085\n    math4             2.000                               1.712    0.169\n    math5             3.000                               2.568    0.249\n    math6             4.000                               3.424    0.325\n    math7             5.000                               4.279    0.395\n    math8             6.000                               5.135    0.459\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  eta_1 ~~                                                              \n    eta_2            -0.181    1.150   -0.158    0.875   -0.026   -0.026\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    eta_1            35.267    0.355   99.229    0.000    4.389    4.389\n    eta_2             4.339    0.088   49.136    0.000    5.070    5.070\n   .math2             0.000                               0.000    0.000\n   .math3             0.000                               0.000    0.000\n   .math4             0.000                               0.000    0.000\n   .math5             0.000                               0.000    0.000\n   .math6             0.000                               0.000    0.000\n   .math7             0.000                               0.000    0.000\n   .math8             0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    eta_1            64.562    5.659   11.408    0.000    1.000    1.000\n    eta_2             0.733    0.327    2.238    0.025    1.000    1.000\n   .math2   (thet)   36.230    1.867   19.410    0.000   36.230    0.359\n   .math3   (thet)   36.230    1.867   19.410    0.000   36.230    0.358\n   .math4   (thet)   36.230    1.867   19.410    0.000   36.230    0.352\n   .math5   (thet)   36.230    1.867   19.410    0.000   36.230    0.341\n   .math6   (thet)   36.230    1.867   19.410    0.000   36.230    0.326\n   .math7   (thet)   36.230    1.867   19.410    0.000   36.230    0.309\n   .math8   (thet)   36.230    1.867   19.410    0.000   36.230    0.290\n\n\n\n\nprint(fitMeasures(lg_math_lavaan_fit, c(\"chisq\", \"df\", \"pvalue\", \"cfi\", \"rmsea\")))\n\n  chisq      df  pvalue     cfi   rmsea \n204.484  29.000   0.000   0.791   0.081 \n\n\nGeneriamo un diagramma di percorso.\n\nsemPaths(lg_math_lavaan_fit, what = \"path\", whatLabels = \"par\")\n\n\n\n\n\n\n\n\nEsaminiamo le traiettorie di crescita.\n\nnlsy_math_predicted &lt;- as.data.frame(\n    cbind(nlsy_math_wide$id, lavPredict(lg_math_lavaan_fit))\n)\n\n#naming columns\nnames(nlsy_math_predicted) &lt;- c(\"id\", \"eta_1\", \"eta_2\")\n\nhead(nlsy_math_predicted)\n\n\nA data.frame: 6 x 3\n\n\n\nid\neta_1\neta_2\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n201\n36.94675\n4.534084\n\n\n2\n303\n26.03589\n4.050780\n\n\n3\n2702\n49.70187\n4.594149\n\n\n4\n4303\n41.04200\n4.548064\n\n\n5\n5002\n37.01240\n4.496746\n\n\n6\n5005\n37.68809\n4.324198\n\n\n\n\n\n\n#calculating implied manifest scores\nnlsy_math_predicted$math2 &lt;- 1 * nlsy_math_predicted$eta_1 + 0 * nlsy_math_predicted$eta_2\nnlsy_math_predicted$math3 &lt;- 1 * nlsy_math_predicted$eta_1 + 1 * nlsy_math_predicted$eta_2\nnlsy_math_predicted$math4 &lt;- 1 * nlsy_math_predicted$eta_1 + 2 * nlsy_math_predicted$eta_2\nnlsy_math_predicted$math5 &lt;- 1 * nlsy_math_predicted$eta_1 + 3 * nlsy_math_predicted$eta_2\nnlsy_math_predicted$math6 &lt;- 1 * nlsy_math_predicted$eta_1 + 4 * nlsy_math_predicted$eta_2\nnlsy_math_predicted$math7 &lt;- 1 * nlsy_math_predicted$eta_1 + 5 * nlsy_math_predicted$eta_2\nnlsy_math_predicted$math8 &lt;- 1 * nlsy_math_predicted$eta_1 + 6 * nlsy_math_predicted$eta_2\n\n\n# reshaping wide to long\nnlsy_math_predicted_long &lt;- reshape(\n  data = nlsy_math_predicted,\n  timevar = c(\"grade\"),\n  idvar = \"id\",\n  varying = c(\n    \"math2\", \"math3\", \"math4\", \"math5\", \"math6\", \"math7\", \"math8\"\n  ),\n  direction = \"long\", sep = \"\"\n)\n\n\nhead(nlsy_math_predicted_long)\n\n\nA data.frame: 6 x 5\n\n\n\nid\neta_1\neta_2\ngrade\nmath\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n201.2\n201\n36.94675\n4.534084\n2\n36.94675\n\n\n303.2\n303\n26.03589\n4.050780\n2\n26.03589\n\n\n2702.2\n2702\n49.70187\n4.594149\n2\n49.70187\n\n\n4303.2\n4303\n41.04200\n4.548064\n2\n41.04200\n\n\n5002.2\n5002\n37.01240\n4.496746\n2\n37.01240\n\n\n5005.2\n5005\n37.68809\n4.324198\n2\n37.68809\n\n\n\n\n\n\n# sorting for easy viewing\n# order by id and time\nnlsy_math_predicted_long &lt;-\n  nlsy_math_predicted_long[order(nlsy_math_predicted_long$id, nlsy_math_predicted_long$grade), ]\n\n\nhead(nlsy_math_predicted_long)\n\n\nA data.frame: 6 x 5\n\n\n\nid\neta_1\neta_2\ngrade\nmath\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n201.2\n201\n36.94675\n4.534084\n2\n36.94675\n\n\n201.3\n201\n36.94675\n4.534084\n3\n41.48083\n\n\n201.4\n201\n36.94675\n4.534084\n4\n46.01492\n\n\n201.5\n201\n36.94675\n4.534084\n5\n50.54900\n\n\n201.6\n201\n36.94675\n4.534084\n6\n55.08309\n\n\n201.7\n201\n36.94675\n4.534084\n7\n59.61717\n\n\n\n\n\n\nggplot(\n  data = nlsy_math_predicted_long, # data set\n  aes(x = grade, y = math, group = id)\n) + # setting variables\n  # geom_point(size=.5) + #adding points to plot\n  geom_line(alpha = 0.15) + # adding lines to plot\n  # setting the x-axis with breaks and labels\n  scale_x_continuous(\n    limits = c(2, 8),\n    breaks = c(2, 3, 4, 5, 6, 7, 8),\n    name = \"Grade at Testing\"\n  ) +\n  # setting the y-axis with limits breaks and labels\n  scale_y_continuous(\n    limits = c(10, 90),\n    breaks = c(10, 30, 50, 70, 90),\n    name = \"Predicted PIAT Mathematics\"\n  )\n\n\n\n\n\n\n\n\nIl modello di crescita latente lineare descrive la traiettoria di sviluppo di ogni bambino attraverso una linea retta, mettendo in luce le variazioni individuali nelle competenze matematiche nel corso del tempo. Il grafico illustra che, per ciascun bambino, si registra un incremento “reale” di circa 5 punti nell’abilità matematica per ogni anno scolastico. Questo modello, quindi, non solo traccia la progressione lineare delle competenze matematiche, ma rivela anche un pattern di crescita coerente e uniforme tra i bambini nel periodo considerato.\n\n77.4.1 Sintassi alternativa\nPer semplificare la scrittura del modello possiamo usare la funzione growth. Tuttavia, per il modello discusso in precedenza, è necessario specificare un parametro aggiuntivo rispetto ai default di growth: vogliamo che le varianze residue di math siano costanti nel tempo.\n\nm1 &lt;-   '\n  i =~ 1*math2 + 1*math3 + 1*math4 + 1*math5 + 1*math6 + 1*math7 + 1*math8  \n  s =~ 0 * math2 + 1 * math3 + 2 * math4 + 3 * math5 + 4 * math6 + 5 * math7 + 6 * math8\n  \n  # manifest variances (made equivalent by naming theta)\n  math2 ~~ theta*math2\n  math3 ~~ theta*math3\n  math4 ~~ theta*math4\n  math5 ~~ theta*math5\n  math6 ~~ theta*math6\n  math7 ~~ theta*math7\n  math8 ~~ theta*math8\n'\n\nAdattiamo il modello.\n\nfit_m1 &lt;- growth(\n  m1,\n  data = nlsy_math_wide,\n  estimator = \"ML\",\n  missing = \"fiml\"\n)\n\nOtteniamo in questo modo lo stesso risultato trovato con la precedente specificazione del modello.\n\nprint(fitMeasures(fit_m1, c(\"chisq\", \"df\", \"pvalue\", \"cfi\", \"rmsea\")))\n\n  chisq      df  pvalue     cfi   rmsea \n204.484  29.000   0.000   0.791   0.081 \n\n\n\n\n77.4.2 Interpretazione dei Parametri del Modello\nNell’output relativo alla sezione Intercepts, il parametro eta_1, con un valore di 35.267, rappresenta la previsione del punteggio in matematica al tempo iniziale $ t_0 $. Questo valore indica la media iniziale dei punteggi in matematica per gli studenti. Per quanto riguarda il parametro eta_2, il suo valore di 4.339 suggerisce che, ad ogni incremento unitario nell’arco temporale considerato, ci si aspetta un aumento medio di 4.339 punti nel punteggio predetto di matematica.\nPassando alla sezione Variances, il valore di eta_1 pari a 64.562 indica la varianza tra gli studenti nelle intercette, cioè la variabilità dei valori iniziali di matematica tra i diversi studenti. Il valore di eta_2, pari a 0.733, rappresenta invece la varianza tra gli studenti nelle pendenze, ossia la variabilità dei tassi di crescita dei punteggi in matematica tra gli studenti. Calcolando l’intervallo $ 35.267 $ e assumendo una distribuzione normale, otteniamo una stima dell’intervallo al 95% per i valori plausibili delle medie dei punteggi in matematica tra gli studenti. Questo intervallo non rappresenta un intervallo di fiducia frequentista, ma piuttosto un intervallo attorno alla stima del valore vero. Analogamente, l’intervallo $ 4.339 $ fornisce una stima dell’intervallo al 95% per i valori plausibili delle pendenze dei punteggi in matematica tra gli studenti.\nLa covarianza stimata di -0.181 (con SE = 1.150) suggerisce che non vi è una relazione significativa tra intercette e pendenze. Se la covarianza fosse stata positiva, avremmo potuto interpretarla come un’indicazione che studenti con un punteggio iniziale più alto in matematica tendono a mostrare un maggiore incremento dei punteggi nel tempo. Al contrario, una covarianza negativa tra intercetta e pendenza implicherebbe che studenti con punteggi iniziali più alti tendono a mostrare un aumento meno marcato dei punteggi nel tempo.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>77</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/07_growth_1.html#confronto-con-il-modello-a-effetti-misti",
    "href": "chapters/lgm/07_growth_1.html#confronto-con-il-modello-a-effetti-misti",
    "title": "77  Curve di crescita latente",
    "section": "77.5 Confronto con il Modello a Effetti Misti",
    "text": "77.5 Confronto con il Modello a Effetti Misti\nProcediamo ora all’analisi degli stessi dati impiegando un modello a effetti misti. Dobbiamo però tenere presente che, in questo contesto, non saremo in grado di replicare esattamente gli stessi risultati ottenuti con il modello di crescita latente (LGM), a causa della presenza di dati mancanti. Nel modello LGM, abbiamo adottato l’approccio della massima verosimiglianza (ML) per la stima dei parametri, gestendo i dati mancanti attraverso l’uso del metodo fiml (Full Information Maximum Likelihood) implementato nel software lavaan. Questo metodo non comporta l’imputazione dei dati mancanti, ma sfrutta le informazioni disponibili in ciascun caso per stimare i parametri secondo il criterio della massima verosimiglianza.\nTuttavia, quando si tratta di modelli a effetti misti, il metodo FIML non è generalmente una strategia applicabile. Di conseguenza, per procedere con l’analisi in questo contesto, adotteremo una soluzione alternativa, consistente nell’eliminazione dei casi che presentano dati mancanti. Questo approccio, sebbene meno sofisticato rispetto al FIML, ci permetterà di procedere con l’analisi del modello a effetti misti, pur con una certa limitazione dovuta alla riduzione del campione di dati disponibili.\nIn formato long, i dati sono i seguenti.\n\nnlsy_math_long |&gt;\n    head()\n\n\nA data.frame: 6 x 3\n\n\n\nid\ngrade\nmath\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n\n\n\n\n201.3\n201\n3\n38\n\n\n201.5\n201\n5\n55\n\n\n303.2\n303\n2\n26\n\n\n303.5\n303\n5\n33\n\n\n2702.2\n2702\n2\n56\n\n\n2702.4\n2702\n4\n58\n\n\n\n\n\nSottraiamo 2 dalla variabile grade in modo che il valore 0 corrisponda alla prima rilevazione temporale. In questo modo, l’intercetta rappresenterà il valore atteso del punteggio di matematica per la prima rilevazione temporale (quando grade è pari a 2).\n\nnlsy_math_long$grade_c2 &lt;- nlsy_math_long$grade - 2\n\nNel contesto del modello a effetti misti, utilizziamo la funzione lmer per adattare il modello. In questa configurazione ((1 | id)), adottiamo un modello con intercette casuali che prevede una pendenza uniforme per tutti gli individui, implicando un tasso di crescita costante per ciascuno. Questa scelta è coerente con le traiettorie di crescita illustrate nella figura precedente.\nL’utilizzo dell’opzione REML = FALSE nel modello specifica che stiamo applicando il metodo della massima verosimiglianza (ML) per la stima dei parametri, anziché l’approccio REML (Restricted Maximum Likelihood), che è il metodo predefinito nella funzione lmer.\nIn aggiunta, l’opzione na.action = na.exclude viene utilizzata per indicare che le osservazioni contenenti dati mancanti saranno escluse dall’analisi. Questo significa che tali osservazioni non contribuiranno alla stima dei parametri del modello, permettendoci di procedere con l’analisi nonostante la presenza di dati incompleti. Questo approccio, benché pratico, può avere implicazioni sulla rappresentatività e sulla generalizzabilità dei risultati, specialmente se la quantità di dati mancanti è sostanziale.\n\nfit_lmer &lt;- lmer(\n    math ~ 1 + grade_c2 + (1 | id),\n    data = nlsy_math_long,\n    REML = FALSE,\n    na.action = na.exclude\n)\n\n\nsummary(fit_lmer) |&gt;\n    print()\n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: math ~ 1 + grade_c2 + (1 | id)\n   Data: nlsy_math_long\n\n     AIC      BIC   logLik deviance df.resid \n 15957.7  15980.5  -7974.8  15949.7     2217 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2082 -0.5265  0.0081  0.5456  2.5651 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n id       (Intercept) 67.30    8.204   \n Residual             39.31    6.270   \nNumber of obs: 2221, groups:  id, 932\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 35.33081    0.36264   97.43\ngrade_c2     4.29352    0.08266   51.94\n\nCorrelation of Fixed Effects:\n         (Intr)\ngrade_c2 -0.555\n\n\nDall’output vediamo che il punteggio di matematica in corrispondenza del secondo grado scolastico (codificato qui con 0) è uguale a 35.33 (0.36). Il tasso di crescita, ovvero l’aumento atteso dei punteggi di matematica per ciascun grado scolastico è uguale a 4.29 (0.08).\nUna rappresentazione grafica dei punteggi predetti dal modello misto può essere ottenuta nel modo seguente.\n\ngr &lt;- emmeans::ref_grid(fit_lmer, cov.keep= c('grade_c2'))\nemm &lt;- emmeans(gr, spec= c('grade_c2'), level= 0.95)\n\n\nnlsy_math_long |&gt;\n    ggplot(aes(x= grade_c2, y= math)) +\n        geom_ribbon(\n            data= data.frame(emm), \n            aes(ymin= lower.CL, ymax= upper.CL, y= NULL), fill= 'grey80'\n        ) +\n        geom_line(data= data.frame(emm), aes(y= emmean)) +\n        geom_point() \n\n\n\n\n\n\n\n\nQuesti risultati, ottenuti escludendo tutte le osservazioni con dati mancanti, sono comunque molto simili ai risultati ottenuti usando lavaan (si veda la figura con le traiettorie di crescita del modello LGM).",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>77</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/07_growth_1.html#modello-di-crescita-non-lineare",
    "href": "chapters/lgm/07_growth_1.html#modello-di-crescita-non-lineare",
    "title": "77  Curve di crescita latente",
    "section": "77.6 Modello di Crescita Non Lineare",
    "text": "77.6 Modello di Crescita Non Lineare\nIn alcuni casi, può essere utile esplorare la possibilità che il cambiamento osservato segua una traiettoria non lineare. Il metodo adottato per costruire un modello di crescita non lineare può essere paragonato all’utilizzo di variabili dummy in un modello di regressione lineare. Tuttavia, in questo contesto, apportiamo una modifica specifica ai carichi fattoriali associati alla variabile latente che rappresenta la pendenza.\nNel modello di crescita non lineare, fissiamo il primo carico fattoriale a 0 e l’ultimo a 1. Questa configurazione implica che il primo punto temporale rappresenta il punto di partenza, mentre l’ultimo indica la conclusione dell’intervallo temporale considerato. I carichi fattoriali intermedi, invece, non sono fissi e vengono stimati liberamente dal modello. Questa impostazione permette di interpretare la pendenza come l’entità complessiva del cambiamento che si verifica tra l’inizio e la fine dell’intervallo temporale considerato.\nI carichi fattoriali che vengono stimati rappresentano la proporzione del cambiamento complessivo che si è verificato fino a quel particolare punto temporale, rispetto al cambiamento totale osservato durante l’intero intervallo. In altre parole, questi carichi fattoriali intermedi offrono una misura di quanto il cambiamento si sia sviluppato a ogni punto temporale intermedio, in rapporto al cambiamento totale che si è verificato dall’inizio alla fine del periodo considerato.\nAttraverso questo approccio, il modello di crescita non lineare fornisce una comprensione più dettagliata e flessibile della dinamica del cambiamento, permettendo di catturare traiettorie che potrebbero non essere adeguatamente descritte da un modello lineare.\n\nmod_nl &lt;- \"\n    i =~ 1*math2 + 1*math3 + 1*math4 + 1*math5 + 1*math6 + 1*math7 + 1*math8\n    s = ~ 0 * math2 + math3 + math4 + math5 + math6 + math7 + 1*math8\n    math2 ~~ theta*math2\n    math3 ~~ theta*math3\n    math4 ~~ theta*math4\n    math5 ~~ theta*math5\n    math6 ~~ theta*math6\n    math7 ~~ theta*math7\n    math8 ~~ theta*math8\n\"\n\nAdattiamo il modello ai dati.\n\nfit_nl &lt;- growth(\n  mod_nl,\n  data = nlsy_math_wide, \n  estimator = \"ML\",\n  missing = \"fiml\"\n)\n\nEsaminiamo la soluzione.\n\nsummary(fit_nl, fit.measures = TRUE, standardized = TRUE) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 128 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        17\n  Number of equality constraints                     6\n\n                                                  Used       Total\n  Number of observations                           932         933\n  Number of missing patterns                        60            \n\nModel Test User Model:\n                                                      \n  Test statistic                                52.947\n  Degrees of freedom                                24\n  P-value (Chi-square)                           0.001\n\nModel Test Baseline Model:\n\n  Test statistic                               862.334\n  Degrees of freedom                                21\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.966\n  Tucker-Lewis Index (TLI)                       0.970\n                                                      \n  Robust Comparative Fit Index (CFI)             1.000\n  Robust Tucker-Lewis Index (TLI)                1.023\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -7892.924\n  Loglikelihood unrestricted model (H1)      -7866.451\n                                                      \n  Akaike (AIC)                               15807.848\n  Bayesian (BIC)                             15861.059\n  Sample-size adjusted Bayesian (SABIC)      15826.124\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.036\n  90 Percent confidence interval - lower         0.023\n  90 Percent confidence interval - upper         0.049\n  P-value H_0: RMSEA &lt;= 0.050                    0.961\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n                                                      \n  Robust RMSEA                                   0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.177\n  P-value H_0: Robust RMSEA &lt;= 0.050             0.644\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.284\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.094\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i =~                                                                  \n    math2             1.000                               8.509    0.839\n    math3             1.000                               8.509    0.857\n    math4             1.000                               8.509    0.851\n    math5             1.000                               8.509    0.840\n    math6             1.000                               8.509    0.823\n    math7             1.000                               8.509    0.808\n    math8             1.000                               8.509    0.791\n  s =~                                                                  \n    math2             0.000                               0.000    0.000\n    math3             0.295    0.019   15.783    0.000    1.849    0.186\n    math4             0.533    0.019   28.588    0.000    3.346    0.335\n    math5             0.664    0.021   31.083    0.000    4.167    0.411\n    math6             0.799    0.022   36.470    0.000    5.016    0.485\n    math7             0.901    0.030   30.314    0.000    5.656    0.537\n    math8             1.000                               6.276    0.583\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i ~~                                                                  \n    s               -13.303    7.281   -1.827    0.068   -0.249   -0.249\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    i                32.400    0.474   68.399    0.000    3.808    3.808\n    s                25.539    0.731   34.916    0.000    4.070    4.070\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .math2   (thet)   30.502    1.678   18.182    0.000   30.502    0.296\n   .math3   (thet)   30.502    1.678   18.182    0.000   30.502    0.310\n   .math4   (thet)   30.502    1.678   18.182    0.000   30.502    0.305\n   .math5   (thet)   30.502    1.678   18.182    0.000   30.502    0.297\n   .math6   (thet)   30.502    1.678   18.182    0.000   30.502    0.286\n   .math7   (thet)   30.502    1.678   18.182    0.000   30.502    0.275\n   .math8   (thet)   30.502    1.678   18.182    0.000   30.502    0.264\n    i                72.408    6.590   10.988    0.000    1.000    1.000\n    s                39.385   11.371    3.464    0.001    1.000    1.000\n\n\n\nEffettuiamo il test del rapporto di verosimiglianze per confrontare il modello di crescita lineare con quello che assume una crescita non lineare.\n\nlavTestLRT(fit_m1, fit_nl) |&gt;\n    print()\n\n\nChi-Squared Difference Test\n\n       Df   AIC   BIC   Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)    \nfit_nl 24 15808 15861  52.947                                          \nfit_m1 29 15949 15978 204.484     151.54 0.17733       5  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIl risultato indica che il modello che assume un cambiamento non lineare si adatta meglio ai dati. Possiamo visualizzare il cambiamento nel modo seguente.\n\n# extract just th eloadings of the slopes\nloadings &lt;- parameterestimates(fit_nl) %&gt;% # get estimates\n  filter(lhs == \"s\", op == \"=~\") %&gt;% # filter the rows we want\n  .[[\"est\"]] # extract \"est\" variable\n# print result\nprint(loadings)\n\n[1] 0.0000000 0.2946469 0.5331544 0.6640287 0.7992433 0.9012769 1.0000000\n\n\n\n# predict scores\npred_lgm3 &lt;- predict(fit_nl)\n# create long data for each individual\npred_lgm3_long &lt;- map(loadings, # loop over time\n                      function(x) pred_lgm3[, 1] + \n                        x * pred_lgm3[, 2]) %&gt;% \n  reduce(cbind) %&gt;% # bring together the wave predictions \n  as.data.frame()\n\n\n# predict scores\npred_lgm3 &lt;- predict(fit_nl)\n# create long data for each individual\npred_lgm3_long &lt;- map(loadings, # loop over time\n                      function(x) pred_lgm3[, 1] + \n                        x * pred_lgm3[, 2]) %&gt;% \n  reduce(cbind) %&gt;% # bring together the wave predictions \n  as.data.frame() %&gt;% # make data frame\n  setNames(str_c(\"Grade \", 1:7)) %&gt;% # give names to variables\n  mutate(id = row_number()) %&gt;% # make unique id\n  gather(-id, key = grade, value = pred) # make long format\npred_lgm3_long %&gt;% \n  ggplot(aes(grade, pred, group = id)) + # what variables to plot?\n  geom_line(alpha = 0.05) + # add a transparent line for each person\n  stat_summary( # add average line\n    aes(group = 1),\n    fun = mean,\n    geom = \"line\",\n    size = 1.5,\n    color = \"green\"\n  ) + \n  stat_summary(data = pred_lgm3_long, # add average from linear model\n               aes(group = 1),\n               fun = mean,\n               geom = \"line\",\n               size = 1.5,\n               color = \"red\",\n               alpha = 0.5\n  ) +\n  stat_summary(data = pred_lgm3_long, # add average from squared model\n               aes(group = 1),\n               fun = mean,\n               geom = \"line\",\n               size = 1.5,\n               color = \"blue\",\n               alpha = 0.5\n  ) +\n  labs(y = \"Predicted PIAT Mathematics\", # labels\n       x = \"Grade at Testing\")",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>77</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/07_growth_1.html#riflessioni-finali",
    "href": "chapters/lgm/07_growth_1.html#riflessioni-finali",
    "title": "77  Curve di crescita latente",
    "section": "77.7 Riflessioni Finali",
    "text": "77.7 Riflessioni Finali\nQuesto capitolo ha esplorato l’implementazione e l’adattamento dei modelli di crescita lineare all’interno del framework della modellizzazione delle equazioni strutturali (SEM), utilizzando il pacchetto lavaan in R. Abbiamo illustrato come calcolare e visualizzare graficamente le traiettorie di crescita predette da questi modelli.\nI modelli di crescita lineare rappresentano un punto di partenza essenziale per analizzare il cambiamento individuale nel tempo. Tuttavia, possono non essere sempre in grado di descrivere accuratamente il processo di cambiamento. Per questa ragione, è opportuno valutare anche altri modelli e, eventualmente, esaminare le variazioni tra diversi gruppi. L’impiego dei modelli di crescita all’interno dei framework SEM e dei modelli a effetti misti presenta sia vantaggi sia limitazioni. Ad esempio, i modelli SEM offrono indici di adattamento globale quali RMSEA, CFI e TLI, che non sono disponibili nell’approccio dei modelli a effetti misti, i quali si basano piuttosto su criteri come AIC e BIC e su strumenti diagnostici quali i grafici dei residui.\nUn aspetto cruciale nell’adattamento dei modelli di crescita lineare è la scelta della metrica temporale. Nel nostro esempio, abbiamo utilizzato il grado scolastico come indicatore temporale, ma esistono altre opzioni possibili. Ad esempio, l’età al momento del test potrebbe essere una metrica più appropriata, in quanto potrebbe riflettere più accuratamente gli intervalli tra le misurazioni. Si deve inoltre considerare che l’utilizzo del grado scolastico può avere delle limitazioni, ad esempio in casi di studenti che ripetono o saltano un anno.\nLa posizione dell’intercetta può essere scelta in qualsiasi punto del continuum temporale. Nel nostro esempio, abbiamo centrato l’intercetta sulla valutazione della seconda elementare, in quanto era il primo dato disponibile. Tuttavia, è importante selezionare un punto di origine che sia significativo per lo studio specifico. Per esempio, posizionare l’intercetta alla fine dell’ottava elementare potrebbe essere rilevante per studi che mirano a valutare la preparazione degli studenti per la scuola superiore.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>77</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/08_growth_cont.html",
    "href": "chapters/lgm/08_growth_cont.html",
    "title": "78  Il tempo su una metrica continua",
    "section": "",
    "text": "78.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nQuando si affronta lo studio del cambiamento in un individuo, un aspetto cruciale è la selezione di una scala temporale adeguata per osservare questo cambiamento. Nel capitolo precedente, ad esempio, abbiamo adottato il grado scolastico come nostra scala temporale di riferimento, strutturando le osservazioni su questa base. Tuttavia, il grado scolastico non è l’unica scala temporale applicabile a tali dati. Altre scale significative potrebbero essere l’età o le specifiche occasioni in cui si effettuano le misurazioni.\nCi sono scale temporali che rappresentano intervalli discreti, come le occasioni di misurazione, dove i valori assunti sono specifici e comuni tra i partecipanti. In questo contesto, però, potrebbe non essere possibile valutare ogni partecipante ad ogni occasione di misurazione. D’altra parte, esistono scale temporali più fluide, come l’età, dove i valori sono unici per ciascun partecipante e non condivisi.\nInteressante è notare come la stessa scala temporale possa essere impiegata sia in un contesto discreto che continuo. Ad esempio, l’età può essere approssimata all’anno più vicino, mentre il grado scolastico può essere definito più precisamente, considerando l’anno scolastico e il numero di giorni trascorsi dall’inizio dell’anno scolastico.\nIn questo capitolo, ci concentreremo sulle tecniche per modellare la crescita individuale utilizzando una scala temporale continua, esplorando come questa possa fornire una comprensione più dettagliata e sfumata del cambiamento all’interno della persona.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>78</span>  <span class='chapter-title'>Il tempo su una metrica continua</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/08_growth_cont.html#lapplicazione-della-finestra-temporale",
    "href": "chapters/lgm/08_growth_cont.html#lapplicazione-della-finestra-temporale",
    "title": "78  Il tempo su una metrica continua",
    "section": "78.2 L’Applicazione della Finestra Temporale",
    "text": "78.2 L’Applicazione della Finestra Temporale\nProseguendo nell’analisi delle metriche del tempo, un approccio interessante è quello della finestra temporale, particolarmente utile per dati che presentano occasioni di misurazione variabili individualmente. Questa metodologia cerca di standardizzare la variabilità temporale individuale su una scala temporale discreta. Un esempio pratico di questo può essere visto nell’arrotondamento dell’età o del tempo al semestre o al quarto d’anno più vicino.\nQuesto metodo, benché utile, rappresenta ancora un’approssimazione della realtà temporale. Riducendo la dimensione delle finestre temporali si può aumentare la precisione, ma questo può comportare una maggiore dispersione dei dati, rendendo così più complessa l’accuratezza delle stime.\nNell’applicazione pratica di questo esempio, definiamo le finestre temporali in termini di semestri. Pertanto, lavoriamo con i dati in formato long, arrotondando l’età al semestre più vicino, e successivamente convertiamo questi dati in formato wide. Questo consente la loro integrazione nel framework SEM, facilitando l’analisi e l’interpretazione dei cambiamenti individuali nel tempo.\nPer questo esempio considereremo i dati di abilità matematica NLSY-CYA Long Data [si veda {cite:t}grimm2016growth]. Iniziamo a leggere i dati.\n\n#set filepath for data file\nfilepath &lt;- \"https://raw.githubusercontent.com/LRI-2/Data/main/GrowthModeling/nlsy_math_long_R.dat\"\n#read in the text data file using the url() function\ndat &lt;- read.table(file=url(filepath),\n                  na.strings = \".\")  #indicates the missing data designator\n#copy data with new name \nnlsy_math_long &lt;- dat  \n\n#Add names the columns of the data set\nnames(nlsy_math_long) = c('id'     , 'female', 'lb_wght', \n                          'anti_k1', 'math'  , 'grade'  ,\n                          'occ'    , 'age'   , 'men'    ,\n                          'spring' , 'anti')\n\n#subset to the variables of interest\nnlsy_math_long &lt;- nlsy_math_long[ ,c(\"id\", \"math\", \"grade\", \"age\")]\n#view the first few observations in the data set \nhead(nlsy_math_long, 10)\n\n\nA data.frame: 10 x 4\n\n\n\nid\nmath\ngrade\nage\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n201\n38\n3\n111\n\n\n2\n201\n55\n5\n135\n\n\n3\n303\n26\n2\n121\n\n\n4\n303\n33\n5\n145\n\n\n5\n2702\n56\n2\n100\n\n\n6\n2702\n58\n4\n125\n\n\n7\n2702\n80\n8\n173\n\n\n8\n4303\n41\n3\n115\n\n\n9\n4303\n58\n4\n135\n\n\n10\n5002\n46\n4\n117\n\n\n\n\n\n\n#intraindividual change trajetories\nggplot(data=nlsy_math_long,                    #data set\n       aes(x = age, y = math, group = id)) + #setting variables\n  geom_point(size=.5) + #adding points to plot\n  geom_line(alpha = 0.5) +  #adding lines to plot\n  #setting the x-axis with breaks and labels\n  scale_x_continuous(#limits=c(2,8),\n                     #breaks = c(2,3,4,5,6,7,8), \n                     name = \"Age at Testing\") +    \n  #setting the y-axis with limits breaks and labels\n  scale_y_continuous(limits=c(10,90), \n                     breaks = c(10,30,50,70,90), \n                     name = \"PIAT Mathematics\")\n\n\n\n\n\n\n\n\nImplementiamo il metodo della finestra temporale e ricodifichiamo i dati in formato wide.\n\n# creating new age variable scaled in years\nnlsy_math_long$ageyr &lt;- (nlsy_math_long$age / 12)\nhead(nlsy_math_long)\n\n\nA data.frame: 6 x 5\n\n\n\nid\nmath\ngrade\nage\nageyr\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n201\n38\n3\n111\n9.250000\n\n\n2\n201\n55\n5\n135\n11.250000\n\n\n3\n303\n26\n2\n121\n10.083333\n\n\n4\n303\n33\n5\n145\n12.083333\n\n\n5\n2702\n56\n2\n100\n8.333333\n\n\n6\n2702\n58\n4\n125\n10.416667\n\n\n\n\n\n\n# rounding to nearest half-year\n# multiplied by 10 to remove decimal for easy conversion to wide\nnlsy_math_long$agewindow &lt;- plyr::round_any(nlsy_math_long$ageyr * 10, 5)\nhead(nlsy_math_long)\n\n\nA data.frame: 6 x 6\n\n\n\nid\nmath\ngrade\nage\nageyr\nagewindow\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n201\n38\n3\n111\n9.250000\n90\n\n\n2\n201\n55\n5\n135\n11.250000\n110\n\n\n3\n303\n26\n2\n121\n10.083333\n100\n\n\n4\n303\n33\n5\n145\n12.083333\n120\n\n\n5\n2702\n56\n2\n100\n8.333333\n85\n\n\n6\n2702\n58\n4\n125\n10.416667\n105\n\n\n\n\n\n\n# reshaping long to wide (just variables of interest)\nnlsy_math_wide &lt;- reshape(\n  data = nlsy_math_long[, c(\"id\", \"math\", \"agewindow\")],\n  timevar = c(\"agewindow\"),\n  idvar = c(\"id\"),\n  v.names = c(\"math\"),\n  direction = \"wide\", sep = \"\"\n)\n\n# reordering columns for easy viewing\nnlsy_math_wide &lt;- nlsy_math_wide[, c(\n  \"id\", \"math70\", \"math75\", \"math80\", \"math85\", \"math90\", \"math95\", \"math100\", \"math105\", \"math110\", \"math115\", \"math120\", \"math125\", \"math130\", \"math135\", \"math140\", \"math145\"\n)]\n# looking at the data\nhead(nlsy_math_wide)\n\n\nA data.frame: 6 x 17\n\n\n\nid\nmath70\nmath75\nmath80\nmath85\nmath90\nmath95\nmath100\nmath105\nmath110\nmath115\nmath120\nmath125\nmath130\nmath135\nmath140\nmath145\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n201\nNA\nNA\nNA\nNA\n38\nNA\nNA\nNA\n55\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n3\n303\nNA\nNA\nNA\nNA\nNA\nNA\n26\nNA\nNA\nNA\n33\nNA\nNA\nNA\nNA\nNA\n\n\n5\n2702\nNA\nNA\nNA\n56\nNA\nNA\nNA\n58\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n80\n\n\n8\n4303\nNA\nNA\nNA\nNA\nNA\n41\nNA\nNA\n58\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n10\n5002\nNA\nNA\nNA\nNA\nNA\nNA\n46\nNA\nNA\nNA\n54\nNA\nNA\nNA\n66\nNA\n\n\n13\n5005\nNA\nNA\n35\nNA\nNA\n50\nNA\nNA\nNA\n60\nNA\nNA\nNA\n59\nNA\nNA\n\n\n\n\n\nSpecifichiamo il modello SEM.\n\nlg_math_age_lavaan_model &lt;- '\n  # latent variable definitions\n      #intercept (note intercept is a reserved term)\n      eta_1 =~  1*math70 +\n                1*math75 +\n                1*math80 +\n                1*math85 +\n                1*math90 +\n                1*math95 +\n                1*math100 +\n                1*math105 +\n                1*math110 +\n                1*math115 +\n                1*math120 +\n                1*math125 +\n                1*math130 +\n                1*math135 +\n                1*math140 +\n                1*math145 \n\n      #linear slope (note intercept is a reserved term)\n      eta_2 =~ -1*math70 +\n               -0.5*math75 +\n                0*math80 +\n                0.5*math85 +\n                1*math90 +\n                1.5*math95 +\n                2*math100 +\n                2.5*math105 +\n                3*math110 +\n                3.5*math115 +\n                4*math120 +\n                4.5*math125 +\n                5*math130 +\n                5.5*math135 +\n                6*math140 +\n                6.5*math145\n\n  # factor variances\n      eta_1 ~~ start(65)*eta_1\n      eta_2 ~~ start(.75)*eta_2\n\n  # covariances among factors \n      eta_1 ~~ start(1.2)*eta_2\n\n  # manifest variances (made equivalent by naming theta)\n      math70 ~~ start(35)*theta*math70\n      math75 ~~ theta*math75\n      math80 ~~ theta*math80\n      math85 ~~ theta*math85\n      math90 ~~ theta*math90\n      math95 ~~ theta*math95\n      math100 ~~ theta*math100\n      math105 ~~ theta*math105\n      math110 ~~ theta*math110\n      math115 ~~ theta*math115\n      math120 ~~ theta*math120\n      math125 ~~ theta*math125\n      math130 ~~ theta*math130\n      math135 ~~ theta*math135\n      math140 ~~ theta*math140\n      math145 ~~ theta*math145\n      \n  # manifest means (fixed at zero)\n      math70 ~ 0*1\n      math75 ~ 0*1\n      math80 ~ 0*1\n      math85 ~ 0*1\n      math90 ~ 0*1\n      math95 ~ 0*1\n      math100 ~ 0*1\n      math105 ~ 0*1\n      math110 ~ 0*1\n      math115 ~ 0*1\n      math120 ~ 0*1\n      math125 ~ 0*1\n      math130 ~ 0*1\n      math135 ~ 0*1\n      math140 ~ 0*1\n      math145 ~ 0*1\n\n  # factor means (estimated freely)\n      eta_1 ~ start(35)*1\n      eta_2 ~ start(4)*1\n' #end of model definition\n\nIn questo modello, si definiscono due variabili latenti: l’intercetta latente (eta_1) e la pendenza lineare (eta_2). La scelta dei coefficienti per eta_2 consente di modellare una traiettoria di crescita lineare nel tempo. Ogni coefficiente corrisponde al peso assegnato a ciascuna misura di matematica (math70, math75, …, math145) nell’espressione della pendenza lineare.\n\nI coefficienti vanno da -1 a 6.5, aumentando di 0.5 ad ogni passaggio. Questa progressione rappresenta l’aumento lineare nel tempo. Ad esempio, math70 ha un coefficiente di -1, math75 ha un coefficiente di -0.5, e così via fino a math145, che ha un coefficiente di 6.5.\nI coefficienti sono scelti per mantenere una distanza temporale costante tra ogni punto di misurazione. Ad esempio, la differenza di 0.5 tra i coefficienti di math70 e math75 implica che il lasso di tempo tra queste due misurazioni è costante rispetto alle altre misurazioni.\nÈ interessante notare che il coefficiente per math80 è 0. Questo implica che math80 è stato scelto come punto di riferimento o centro per la pendenza lineare. I valori negativi e positivi dei coefficienti rappresentano misurazioni prima e dopo questo punto di riferimento, rispettivamente.\n\nAdattiamo il modello ai dati.\n\n#estimating the model using sem() function\nlg_math_age_lavaan_fit &lt;- sem(lg_math_age_lavaan_model,\n  data = nlsy_math_wide,\n  meanstructure = TRUE,\n  estimator = \"ML\",\n  missing = \"fiml\"\n)\n\nEsaminiamo la soluzione.\n\nsummary(lg_math_age_lavaan_fit, fit.measures = TRUE) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 40 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        21\n  Number of equality constraints                    15\n\n  Number of observations                           932\n  Number of missing patterns                       139\n\nModel Test User Model:\n                                                      \n  Test statistic                               295.028\n  Degrees of freedom                               146\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              1053.342\n  Degrees of freedom                               120\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.840\n  Tucker-Lewis Index (TLI)                       0.869\n                                                      \n  Robust Comparative Fit Index (CFI)             0.003\n  Robust Tucker-Lewis Index (TLI)                0.181\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -7928.559\n  Loglikelihood unrestricted model (H1)      -7781.045\n                                                      \n  Akaike (AIC)                               15869.117\n  Bayesian (BIC)                             15898.141\n  Sample-size adjusted Bayesian (SABIC)      15879.086\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.033\n  90 Percent confidence interval - lower         0.028\n  90 Percent confidence interval - upper         0.039\n  P-value H_0: RMSEA &lt;= 0.050                    1.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n                                                      \n  Robust RMSEA                                   4.193\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.000\n  P-value H_0: Robust RMSEA &lt;= 0.050               NaN\n  P-value H_0: Robust RMSEA &gt;= 0.080               NaN\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.314\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 =~                                            \n    math70            1.000                           \n    math75            1.000                           \n    math80            1.000                           \n    math85            1.000                           \n    math90            1.000                           \n    math95            1.000                           \n    math100           1.000                           \n    math105           1.000                           \n    math110           1.000                           \n    math115           1.000                           \n    math120           1.000                           \n    math125           1.000                           \n    math130           1.000                           \n    math135           1.000                           \n    math140           1.000                           \n    math145           1.000                           \n  eta_2 =~                                            \n    math70           -1.000                           \n    math75           -0.500                           \n    math80            0.000                           \n    math85            0.500                           \n    math90            1.000                           \n    math95            1.500                           \n    math100           2.000                           \n    math105           2.500                           \n    math110           3.000                           \n    math115           3.500                           \n    math120           4.000                           \n    math125           4.500                           \n    math130           5.000                           \n    math135           5.500                           \n    math140           6.000                           \n    math145           6.500                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 ~~                                            \n    eta_2             1.157    1.010    1.146    0.252\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .math70            0.000                           \n   .math75            0.000                           \n   .math80            0.000                           \n   .math85            0.000                           \n   .math90            0.000                           \n   .math95            0.000                           \n   .math100           0.000                           \n   .math105           0.000                           \n   .math110           0.000                           \n   .math115           0.000                           \n   .math120           0.000                           \n   .math125           0.000                           \n   .math130           0.000                           \n   .math135           0.000                           \n   .math140           0.000                           \n   .math145           0.000                           \n    eta_1            35.236    0.347  101.512    0.000\n    eta_2             4.229    0.081   51.910    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1            65.063    5.503   11.824    0.000\n    eta_2             0.725    0.277    2.616    0.009\n   .math70  (thet)   32.337    1.695   19.083    0.000\n   .math75  (thet)   32.337    1.695   19.083    0.000\n   .math80  (thet)   32.337    1.695   19.083    0.000\n   .math85  (thet)   32.337    1.695   19.083    0.000\n   .math90  (thet)   32.337    1.695   19.083    0.000\n   .math95  (thet)   32.337    1.695   19.083    0.000\n   .math100 (thet)   32.337    1.695   19.083    0.000\n   .math105 (thet)   32.337    1.695   19.083    0.000\n   .math110 (thet)   32.337    1.695   19.083    0.000\n   .math115 (thet)   32.337    1.695   19.083    0.000\n   .math120 (thet)   32.337    1.695   19.083    0.000\n   .math125 (thet)   32.337    1.695   19.083    0.000\n   .math130 (thet)   32.337    1.695   19.083    0.000\n   .math135 (thet)   32.337    1.695   19.083    0.000\n   .math140 (thet)   32.337    1.695   19.083    0.000\n   .math145 (thet)   32.337    1.695   19.083    0.000\n\n\n\n\nparameterEstimates(lg_math_age_lavaan_fit) |&gt;\n    print()\n\n       lhs op     rhs label    est    se       z pvalue ci.lower ci.upper\n1    eta_1 =~  math70        1.000 0.000      NA     NA    1.000    1.000\n2    eta_1 =~  math75        1.000 0.000      NA     NA    1.000    1.000\n3    eta_1 =~  math80        1.000 0.000      NA     NA    1.000    1.000\n4    eta_1 =~  math85        1.000 0.000      NA     NA    1.000    1.000\n5    eta_1 =~  math90        1.000 0.000      NA     NA    1.000    1.000\n6    eta_1 =~  math95        1.000 0.000      NA     NA    1.000    1.000\n7    eta_1 =~ math100        1.000 0.000      NA     NA    1.000    1.000\n8    eta_1 =~ math105        1.000 0.000      NA     NA    1.000    1.000\n9    eta_1 =~ math110        1.000 0.000      NA     NA    1.000    1.000\n10   eta_1 =~ math115        1.000 0.000      NA     NA    1.000    1.000\n11   eta_1 =~ math120        1.000 0.000      NA     NA    1.000    1.000\n12   eta_1 =~ math125        1.000 0.000      NA     NA    1.000    1.000\n13   eta_1 =~ math130        1.000 0.000      NA     NA    1.000    1.000\n14   eta_1 =~ math135        1.000 0.000      NA     NA    1.000    1.000\n15   eta_1 =~ math140        1.000 0.000      NA     NA    1.000    1.000\n16   eta_1 =~ math145        1.000 0.000      NA     NA    1.000    1.000\n17   eta_2 =~  math70       -1.000 0.000      NA     NA   -1.000   -1.000\n18   eta_2 =~  math75       -0.500 0.000      NA     NA   -0.500   -0.500\n19   eta_2 =~  math80        0.000 0.000      NA     NA    0.000    0.000\n20   eta_2 =~  math85        0.500 0.000      NA     NA    0.500    0.500\n21   eta_2 =~  math90        1.000 0.000      NA     NA    1.000    1.000\n22   eta_2 =~  math95        1.500 0.000      NA     NA    1.500    1.500\n23   eta_2 =~ math100        2.000 0.000      NA     NA    2.000    2.000\n24   eta_2 =~ math105        2.500 0.000      NA     NA    2.500    2.500\n25   eta_2 =~ math110        3.000 0.000      NA     NA    3.000    3.000\n26   eta_2 =~ math115        3.500 0.000      NA     NA    3.500    3.500\n27   eta_2 =~ math120        4.000 0.000      NA     NA    4.000    4.000\n28   eta_2 =~ math125        4.500 0.000      NA     NA    4.500    4.500\n29   eta_2 =~ math130        5.000 0.000      NA     NA    5.000    5.000\n30   eta_2 =~ math135        5.500 0.000      NA     NA    5.500    5.500\n31   eta_2 =~ math140        6.000 0.000      NA     NA    6.000    6.000\n32   eta_2 =~ math145        6.500 0.000      NA     NA    6.500    6.500\n33   eta_1 ~~   eta_1       65.063 5.503  11.824  0.000   54.278   75.849\n34   eta_2 ~~   eta_2        0.725 0.277   2.616  0.009    0.182    1.268\n35   eta_1 ~~   eta_2        1.157 1.010   1.146  0.252   -0.822    3.136\n36  math70 ~~  math70 theta 32.337 1.695  19.083  0.000   29.016   35.658\n37  math75 ~~  math75 theta 32.337 1.695  19.083  0.000   29.016   35.658\n38  math80 ~~  math80 theta 32.337 1.695  19.083  0.000   29.016   35.658\n39  math85 ~~  math85 theta 32.337 1.695  19.083  0.000   29.016   35.658\n40  math90 ~~  math90 theta 32.337 1.695  19.083  0.000   29.016   35.658\n41  math95 ~~  math95 theta 32.337 1.695  19.083  0.000   29.016   35.658\n42 math100 ~~ math100 theta 32.337 1.695  19.083  0.000   29.016   35.658\n43 math105 ~~ math105 theta 32.337 1.695  19.083  0.000   29.016   35.658\n44 math110 ~~ math110 theta 32.337 1.695  19.083  0.000   29.016   35.658\n45 math115 ~~ math115 theta 32.337 1.695  19.083  0.000   29.016   35.658\n46 math120 ~~ math120 theta 32.337 1.695  19.083  0.000   29.016   35.658\n47 math125 ~~ math125 theta 32.337 1.695  19.083  0.000   29.016   35.658\n48 math130 ~~ math130 theta 32.337 1.695  19.083  0.000   29.016   35.658\n49 math135 ~~ math135 theta 32.337 1.695  19.083  0.000   29.016   35.658\n50 math140 ~~ math140 theta 32.337 1.695  19.083  0.000   29.016   35.658\n51 math145 ~~ math145 theta 32.337 1.695  19.083  0.000   29.016   35.658\n52  math70 ~1                0.000 0.000      NA     NA    0.000    0.000\n53  math75 ~1                0.000 0.000      NA     NA    0.000    0.000\n54  math80 ~1                0.000 0.000      NA     NA    0.000    0.000\n55  math85 ~1                0.000 0.000      NA     NA    0.000    0.000\n56  math90 ~1                0.000 0.000      NA     NA    0.000    0.000\n57  math95 ~1                0.000 0.000      NA     NA    0.000    0.000\n58 math100 ~1                0.000 0.000      NA     NA    0.000    0.000\n59 math105 ~1                0.000 0.000      NA     NA    0.000    0.000\n60 math110 ~1                0.000 0.000      NA     NA    0.000    0.000\n61 math115 ~1                0.000 0.000      NA     NA    0.000    0.000\n62 math120 ~1                0.000 0.000      NA     NA    0.000    0.000\n63 math125 ~1                0.000 0.000      NA     NA    0.000    0.000\n64 math130 ~1                0.000 0.000      NA     NA    0.000    0.000\n65 math135 ~1                0.000 0.000      NA     NA    0.000    0.000\n66 math140 ~1                0.000 0.000      NA     NA    0.000    0.000\n67 math145 ~1                0.000 0.000      NA     NA    0.000    0.000\n68   eta_1 ~1               35.236 0.347 101.512  0.000   34.556   35.917\n69   eta_2 ~1                4.229 0.081  51.910  0.000    4.069    4.389\n\n\n\ninspect(lg_math_age_lavaan_fit, what=\"est\") |&gt;\n    print()\n\n$lambda\n        eta_1 eta_2\nmath70      1  -1.0\nmath75      1  -0.5\nmath80      1   0.0\nmath85      1   0.5\nmath90      1   1.0\nmath95      1   1.5\nmath100     1   2.0\nmath105     1   2.5\nmath110     1   3.0\nmath115     1   3.5\nmath120     1   4.0\nmath125     1   4.5\nmath130     1   5.0\nmath135     1   5.5\nmath140     1   6.0\nmath145     1   6.5\n\n$theta\n        math70 math75 math80 math85 math90 math95 mth100 mth105 mth110\nmath70  32.337                                                        \nmath75   0.000 32.337                                                 \nmath80   0.000  0.000 32.337                                          \nmath85   0.000  0.000  0.000 32.337                                   \nmath90   0.000  0.000  0.000  0.000 32.337                            \nmath95   0.000  0.000  0.000  0.000  0.000 32.337                     \nmath100  0.000  0.000  0.000  0.000  0.000  0.000 32.337              \nmath105  0.000  0.000  0.000  0.000  0.000  0.000  0.000 32.337       \nmath110  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 32.337\nmath115  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\nmath120  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\nmath125  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\nmath130  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\nmath135  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\nmath140  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\nmath145  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n        mth115 mth120 mth125 mth130 mth135 mth140 mth145\nmath70                                                  \nmath75                                                  \nmath80                                                  \nmath85                                                  \nmath90                                                  \nmath95                                                  \nmath100                                                 \nmath105                                                 \nmath110                                                 \nmath115 32.337                                          \nmath120  0.000 32.337                                   \nmath125  0.000  0.000 32.337                            \nmath130  0.000  0.000  0.000 32.337                     \nmath135  0.000  0.000  0.000  0.000 32.337              \nmath140  0.000  0.000  0.000  0.000  0.000 32.337       \nmath145  0.000  0.000  0.000  0.000  0.000  0.000 32.337\n\n$psi\n       eta_1  eta_2\neta_1 65.063       \neta_2  1.157  0.725\n\n$nu\n        intrcp\nmath70       0\nmath75       0\nmath80       0\nmath85       0\nmath90       0\nmath95       0\nmath100      0\nmath105      0\nmath110      0\nmath115      0\nmath120      0\nmath125      0\nmath130      0\nmath135      0\nmath140      0\nmath145      0\n\n$alpha\n      intrcp\neta_1 35.236\neta_2  4.229\n\n\n\nCreiamo un diagramma di percorso.\n\nlg_math_age_lavaan_fit |&gt;\n    semPaths(\n        style = \"ram\",\n        whatLabels = \"par\", edge.label.cex = .6,\n        label.prop = 0.9, edge.label.color = \"black\", rotation = 4,\n        equalizeManifests = FALSE, optimizeLatRes = TRUE, node.width = 1.5,\n        edge.width = 0.5, shapeMan = \"rectangle\", shapeLat = \"ellipse\",\n        shapeInt = \"triangle\", sizeMan = 4, sizeInt = 2, sizeLat = 4,\n        curve = 2, unCol = \"#070b8c\"\n    )",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>78</span>  <span class='chapter-title'>Il tempo su una metrica continua</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/09_time_inv_cov.html",
    "href": "chapters/lgm/09_time_inv_cov.html",
    "title": "79  Covariate indipendenti dal tempo",
    "section": "",
    "text": "79.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nNell’ambito degli studi sui modelli di crescita lineare, una questione rilevante è capire in che modo le differenze individuali nelle traiettorie di cambiamento siano influenzate da altre variabili. Il presente capitolo si dedica all’integrazione di covariate invarianti nel tempo in questi modelli di crescita.\nLe covariate invarianti nel tempo sono quelle variabili che rimangono costanti per ogni individuo durante il periodo di studio. Esempi tipici includono il genere, le condizioni sperimentali, lo stato socio-economico, e altri attributi che non subiscono modifiche nel tempo. In termini di modellazione, queste variabili vengono trattate come fattori indipendenti in un modello di regressione multipla, dove l’intercetta e la pendenza del modello di crescita lineare fungono da variabili dipendenti. Le covariate possono assumere vari formati: possono essere continue (es. età), ordinali (es. livelli di istruzione) o categoriche (es. genere).\nIncludere covariate invarianti nel tempo permette di esplorare come le differenze individuali nella traiettoria di crescita (sia in termini di intercetta che di pendenza) siano associate a queste variabili. Questo approccio offre la possibilità di indagare le ragioni sottostanti le diverse modalità di cambiamento tra gli individui.\nIn sintesi, l’integrazione di covariate invarianti nel tempo nei modelli di crescita lineare fornisce una visione più dettagliata delle dinamiche individuali e del modo in cui vari fattori possono influenzare le traiettorie di crescita. Questo approccio arricchisce la comprensione dei fenomeni studiati, pur richiedendo un’interpretazione cauta e informata dei risultati ottenuti.\nPer questo esempio considereremo i dati di prestazione matematica dal data set NLSY-CYA Long Data [si veda {cite:t}grimm2016growth]. Iniziamo a leggere i dati.\n# set filepath for data file\nfilepath &lt;- \"https://raw.githubusercontent.com/LRI-2/Data/main/GrowthModeling/nlsy_math_long_R.dat\"\n# read in the text data file using the url() function\ndat &lt;- read.table(\n  file = url(filepath),\n  na.strings = \".\"\n) # indicates the missing data designator\n# copy data with new name\nnlsy_math_long &lt;- dat\n\n# Add names the columns of the data set\nnames(nlsy_math_long) &lt;- c(\n  \"id\", \"female\", \"lb_wght\",\n  \"anti_k1\", \"math\", \"grade\",\n  \"occ\", \"age\", \"men\",\n  \"spring\", \"anti\"\n)\n\n# view the first few observations in the data set\nhead(nlsy_math_long, 10)\n\n\nA data.frame: 10 x 11\n\n\n\nid\nfemale\nlb_wght\nanti_k1\nmath\ngrade\nocc\nage\nmen\nspring\nanti\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n201\n1\n0\n0\n38\n3\n2\n111\n0\n1\n0\n\n\n2\n201\n1\n0\n0\n55\n5\n3\n135\n1\n1\n0\n\n\n3\n303\n1\n0\n1\n26\n2\n2\n121\n0\n1\n2\n\n\n4\n303\n1\n0\n1\n33\n5\n3\n145\n0\n1\n2\n\n\n5\n2702\n0\n0\n0\n56\n2\n2\n100\nNA\n1\n0\n\n\n6\n2702\n0\n0\n0\n58\n4\n3\n125\nNA\n1\n2\n\n\n7\n2702\n0\n0\n0\n80\n8\n4\n173\nNA\n1\n2\n\n\n8\n4303\n1\n0\n0\n41\n3\n2\n115\n0\n0\n1\n\n\n9\n4303\n1\n0\n0\n58\n4\n3\n135\n0\n1\n2\n\n\n10\n5002\n0\n0\n4\n46\n4\n2\n117\nNA\n1\n4\nnlsy_math_long |&gt;\n  ggplot(\n    aes(grade, math, group = id)\n  ) +\n  geom_line(alpha = 0.3) + # add individual line with transparency\n  stat_summary( # add average line\n    aes(group = 1),\n    fun = mean,\n    geom = \"line\",\n    linewidth = 1.5,\n    color = \"blue\"\n  ) +\n  labs(x = \"Grade at testing\", y = \"PAT Mathematics\")\nPer ottenere una visione più dettagliata dei cambiamenti a livello individuale, possiamo selezionare casualmente un campione di 20 individui e registrare, per ciascuno di essi, l’evoluzione dei loro punteggi in matematica nel tempo.\n# sample 20 ids\npeople &lt;- unique(nlsy_math_long$id) %&gt;% sample(20)\n# do separate graph for each individual\nnlsy_math_long %&gt;% \n  filter(id %in% people) %&gt;%  # filter only sampled cases\n  ggplot(aes(grade, math, group = 1)) +\n  geom_line() +\n  facet_wrap(~id) + # a graph for each individual\n  labs(x = \"Grade at testing\", y = \"PAT Mathematics\")\nPer semplicità, leggiamo gli stessi dati in formato wide da un file.\n# set filepath for data file\nfilepath &lt;- \"https://raw.githubusercontent.com/LRI-2/Data/main/GrowthModeling/nlsy_math_wide_R.dat\"\n\n# read in the text data file using the url() function\ndat &lt;- read.table(\n  file = url(filepath),\n  na.strings = \".\"\n) # indicates the missing data designator\n\n# copy data with new name\nnlsy_math_wide &lt;- dat\n\n# Give the variable names\nnames(nlsy_math_wide) &lt;- c(\n  \"id\", \"female\", \"lb_wght\", \"anti_k1\",\n  \"math2\", \"math3\", \"math4\", \"math5\", \"math6\", \"math7\", \"math8\",\n  \"age2\", \"age3\", \"age4\", \"age5\", \"age6\", \"age7\", \"age8\",\n  \"men2\", \"men3\", \"men4\", \"men5\", \"men6\", \"men7\", \"men8\",\n  \"spring2\", \"spring3\", \"spring4\", \"spring5\", \"spring6\", \"spring7\", \"spring8\",\n  \"anti2\", \"anti3\", \"anti4\", \"anti5\", \"anti6\", \"anti7\", \"anti8\"\n)\n\n\n# view the first few observations (and columns) in the data set\nhead(nlsy_math_wide[, 1:11], 10)\n\n\nA data.frame: 10 x 11\n\n\n\nid\nfemale\nlb_wght\nanti_k1\nmath2\nmath3\nmath4\nmath5\nmath6\nmath7\nmath8\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n201\n1\n0\n0\nNA\n38\nNA\n55\nNA\nNA\nNA\n\n\n2\n303\n1\n0\n1\n26\nNA\nNA\n33\nNA\nNA\nNA\n\n\n3\n2702\n0\n0\n0\n56\nNA\n58\nNA\nNA\nNA\n80\n\n\n4\n4303\n1\n0\n0\nNA\n41\n58\nNA\nNA\nNA\nNA\n\n\n5\n5002\n0\n0\n4\nNA\nNA\n46\nNA\n54\nNA\n66\n\n\n6\n5005\n1\n0\n0\n35\nNA\n50\nNA\n60\nNA\n59\n\n\n7\n5701\n0\n0\n2\nNA\n62\n61\nNA\nNA\nNA\nNA\n\n\n8\n6102\n0\n0\n0\nNA\nNA\n55\n67\nNA\n81\nNA\n\n\n9\n6801\n1\n0\n0\nNA\n54\nNA\n62\nNA\n66\nNA\n\n\n10\n6802\n0\n0\n0\nNA\n55\nNA\n66\nNA\n68\nNA\nSpecifichiamo il modello SEM (si noti che, anche in questo caso, la scrittura del modello può essere semplificata usando la funzione growth).\nI covarianti invarianti nel tempo valutati qui includono lb_wght, una variabile dicotomica codificata come dummy che indica se il bambino aveva un peso alla nascita normale (codificato 0) o basso (codificato 1), e anti_k1, una variabile continua con valori che variano da 0 a 8 indicando il grado in cui il bambino manifestava comportamenti antisociali all’asilo o in prima elementare (punteggi più alti indicano un comportamento più antisociale).\n#writing out linear growth model with tic in full SEM way \nlg_math_tic_lavaan_model &lt;- '\n    #latent variable definitions\n            #intercept\n              eta1 =~ 1*math2+\n                      1*math3+\n                      1*math4+\n                      1*math5+\n                      1*math6+\n                      1*math7+\n                      1*math8\n            #linear slope\n              eta2 =~ 0*math2+\n                      1*math3+\n                      2*math4+\n                      3*math5+\n                      4*math6+\n                      5*math7+\n                      6*math8\n\n          #factor variances\n            eta1 ~~ eta1\n            eta2 ~~ eta2\n\n          #factor covariance\n            eta1 ~~ eta2\n\n          #manifest variances (set equal by naming theta)\n            math2 ~~ theta*math2\n            math3 ~~ theta*math3\n            math4 ~~ theta*math4\n            math5 ~~ theta*math5\n            math6 ~~ theta*math6\n            math7 ~~ theta*math7\n            math8 ~~ theta*math8\n\n          #latent means (freely estimated)\n            eta1 ~ 1\n            eta2 ~ 1\n\n          #manifest means (fixed to zero)\n            math2 ~ 0*1\n            math3 ~ 0*1\n            math4 ~ 0*1\n            math5 ~ 0*1\n            math6 ~ 0*1\n            math7 ~ 0*1\n            math8 ~ 0*1\n\n        #Time invariant covaraite\n        #regression of time-invariant covariate on intercept and slope factors\n            eta1 ~ lb_wght + anti_k1\n            eta2 ~ lb_wght + anti_k1\n\n        #variance of TIV covariates\n            lb_wght ~~ lb_wght\n            anti_k1 ~~ anti_k1\n\n        #covariance of TIV covariates\n            lb_wght ~~ anti_k1\n\n        #means of TIV covariates (freely estimated)\n            lb_wght ~ 1\n            anti_k1 ~ 1\n' #end of model definition\nAdattiamo il modello ai dati.\nlg_math_tic_lavaan_fit &lt;- sem(lg_math_tic_lavaan_model,\n  data = nlsy_math_wide,\n  meanstructure = TRUE,\n  estimator = \"ML\",\n  missing = \"fiml\"\n)\nEsaminiamo la soluzione ottenuta.\nsummary(lg_math_tic_lavaan_fit, fit.measures = TRUE) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 110 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        21\n  Number of equality constraints                     6\n\n  Number of observations                           933\n  Number of missing patterns                        61\n\nModel Test User Model:\n                                                      \n  Test statistic                               220.221\n  Degrees of freedom                                39\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               892.616\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.788\n  Tucker-Lewis Index (TLI)                       0.805\n                                                      \n  Robust Comparative Fit Index (CFI)             0.920\n  Robust Tucker-Lewis Index (TLI)                0.926\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -9785.085\n  Loglikelihood unrestricted model (H1)      -9674.975\n                                                      \n  Akaike (AIC)                               19600.171\n  Bayesian (BIC)                             19672.747\n  Sample-size adjusted Bayesian (SABIC)      19625.108\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.071\n  90 Percent confidence interval - lower         0.062\n  90 Percent confidence interval - upper         0.080\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.046\n                                                      \n  Robust RMSEA                                   0.100\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.183\n  P-value H_0: Robust RMSEA &lt;= 0.050             0.218\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.654\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.097\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta1 =~                                             \n    math2             1.000                           \n    math3             1.000                           \n    math4             1.000                           \n    math5             1.000                           \n    math6             1.000                           \n    math7             1.000                           \n    math8             1.000                           \n  eta2 =~                                             \n    math2             0.000                           \n    math3             1.000                           \n    math4             2.000                           \n    math5             3.000                           \n    math6             4.000                           \n    math7             5.000                           \n    math8             6.000                           \n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta1 ~                                              \n    lb_wght          -2.716    1.294   -2.099    0.036\n    anti_k1          -0.551    0.232   -2.369    0.018\n  eta2 ~                                              \n    lb_wght           0.625    0.333    1.873    0.061\n    anti_k1          -0.019    0.059   -0.327    0.743\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n .eta1 ~~                                             \n   .eta2             -0.078    1.145   -0.068    0.945\n  lb_wght ~~                                          \n    anti_k1           0.007    0.014    0.548    0.584\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .eta1             36.290    0.497   73.052    0.000\n   .eta2              4.315    0.122   35.420    0.000\n   .math2             0.000                           \n   .math3             0.000                           \n   .math4             0.000                           \n   .math5             0.000                           \n   .math6             0.000                           \n   .math7             0.000                           \n   .math8             0.000                           \n    lb_wght           0.080    0.009    9.031    0.000\n    anti_k1           1.454    0.050   29.216    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .eta1             63.064    5.609   11.242    0.000\n   .eta2              0.713    0.326    2.185    0.029\n   .math2   (thet)   36.257    1.868   19.406    0.000\n   .math3   (thet)   36.257    1.868   19.406    0.000\n   .math4   (thet)   36.257    1.868   19.406    0.000\n   .math5   (thet)   36.257    1.868   19.406    0.000\n   .math6   (thet)   36.257    1.868   19.406    0.000\n   .math7   (thet)   36.257    1.868   19.406    0.000\n   .math8   (thet)   36.257    1.868   19.406    0.000\n    lb_wght           0.074    0.003   21.599    0.000\n    anti_k1           2.312    0.107   21.599    0.000\nInterpretazione dei risultati per eta1 (Intercetta).\nInterpretazione dei risultati per eta2 (Pendenza).\nIn sintesi, il peso alla nascita basso e i comportamenti antisociali sembrano influenzare negativamente il valore iniziale (intercetta) del costrutto misurato. Il peso alla nascita e i comportamenti antisociali non sembrano avere un impatto significativo sulla pendenza.\nCreiamo un diagramma di percorso.\nlg_math_tic_lavaan_fit |&gt;\n    semPaths(\n        style = \"lisrel\",\n        whatLabels = \"std\", edge.label.cex = .6,\n        label.prop = 0.9, edge.label.color = \"black\", rotation = 4,\n        equalizeManifests = FALSE, optimizeLatRes = TRUE, node.width = 1.5,\n        edge.width = 0.5, shapeMan = \"rectangle\", shapeLat = \"ellipse\",\n        shapeInt = \"triangle\", sizeMan = 4, sizeInt = 2, sizeLat = 4,\n        curve = 2, unCol = \"#070b8c\"\n    )",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>79</span>  <span class='chapter-title'>Covariate indipendenti dal tempo</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/09_time_inv_cov.html#introduzione",
    "href": "chapters/lgm/09_time_inv_cov.html#introduzione",
    "title": "79  Covariate indipendenti dal tempo",
    "section": "",
    "text": "È importante sottolineare che, pur fornendo insights significativi, i risultati ottenuti da questi modelli non implicano relazioni causali. Questi modelli hanno limitazioni simili a quelle dei modelli di regressione standard in termini di inferenza causale.\nÈ cruciale considerare il contesto nel quale le covariate invarianti nel tempo sono state raccolte. Se queste provengono da un contesto sperimentale con assegnazione casuale, le inferenze potrebbero essere più robuste. Tuttavia, nel caso di dati osservazionali, è necessario un’attenta considerazione per evitare interpretazioni errate o eccessivamente assertive riguardo la causalità.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlb_wght: L’effetto stimato di lb_wght sull’intercetta (eta1) è di -2.716 con uno standard error di 1.294. Questo valore di -2.716 indica che avere un peso alla nascita basso (codificato come 1) è associato ad una riduzione media di 2.716 unità nel valore iniziale di eta1, rispetto a un peso alla nascita normale (codificato come 0). Questo risultato è statisticamente significativo, come indicato dal valore di P (0.036), che è inferiore a 0.05.\nanti_k1: Per anti_k1, l’effetto stimato sull’intercetta è di -0.551 con uno standard error di 0.232. Questo suggerisce che per ogni unità di aumento nel punteggio di comportamento antisociale (anti_k1), il valore iniziale di eta1 diminuisce in media di 0.551 unità. Anche questo risultato è statisticamente significativo, con un valore di P di 0.018.\n\n\n\nlb_wght: Per la pendenza (eta2), l’effetto stimato di lb_wght è di 0.625 con uno standard error di 0.333. Ciò indica che avere un peso alla nascita basso è associato ad un aumento medio di 0.625 unità nella pendenza di eta2, rispetto a un peso normale alla nascita. Tuttavia, questo risultato non è statisticamente significativo al livello del 5%, dato che il valore di P è 0.061, che è leggermente superiore a 0.05.\nanti_k1: L’effetto stimato di anti_k1 sulla pendenza è molto piccolo (-0.019) e non è statisticamente significativo (valore di P = 0.743), suggerendo che non c’è una relazione chiara tra il comportamento antisociale in età precoce e il cambiamento nel tempo di eta2.\n\n\n\n\n\n79.1.1 Valutare il contributo delle covariate\nUna domanda comune in questo approccio per comprendere le associazioni tra covarianti invarianti nel tempo e traiettorie individuali è se l’aggiunta dei covarianti invarianti nel tempo sia stata utile. Nel framework di modellazione multilivello, i -2LL ottenuti quando si adattano modelli con e senza i covarianti invarianti nel tempo possono essere confrontati direttamente (se nessun partecipante è stato escluso dall’analisi a causa di dati incompleti sui covarianti invarianti nel tempo), fornendo un modo per valutare l’adattamento relativo del modello. Specificamente, possiamo esaminare la differenza tra i -2LL rispetto alla differenza nel numero di parametri stimati (o differenza nei gradi di libertà).\n\n#writing out linear growth model with tic in full SEM way \nlg_math_ticZERO_lavaan_model &lt;- '\n    #latent variable definitions\n            #intercept\n              eta1 =~ 1*math2+\n                      1*math3+\n                      1*math4+\n                      1*math5+\n                      1*math6+\n                      1*math7+\n                      1*math8\n            #linear slope\n              eta2 =~ 0*math2+\n                      1*math3+\n                      2*math4+\n                      3*math5+\n                      4*math6+\n                      5*math7+\n                      6*math8\n\n          #factor variances\n            eta1 ~~ eta1\n            eta2 ~~ eta2\n\n          #factor covariance\n            eta1 ~~ eta2\n\n          #manifest variances (set equal by naming theta)\n            math2 ~~ theta*math2\n            math3 ~~ theta*math3\n            math4 ~~ theta*math4\n            math5 ~~ theta*math5\n            math6 ~~ theta*math6\n            math7 ~~ theta*math7\n            math8 ~~ theta*math8\n\n          #latent means (freely estimated)\n            eta1 ~ 1\n            eta2 ~ 1\n\n          #manifest means (fixed to zero)\n            math2 ~ 0*1\n            math3 ~ 0*1\n            math4 ~ 0*1\n            math5 ~ 0*1\n            math6 ~ 0*1\n            math7 ~ 0*1\n            math8 ~ 0*1\n\n        #Time invariant covaraite\n          #regression of time-invariant covariate on intercept and slope factors\n          #FIXED to 0\n            eta1 ~ 0*lb_wght + 0*anti_k1\n            eta2 ~ 0*lb_wght + 0*anti_k1\n\n        #variance of TIV covariates\n            lb_wght ~~ lb_wght\n            anti_k1 ~~ anti_k1\n\n        #covariance of TIV covaraites\n            lb_wght ~~ anti_k1\n\n        #means of TIV covariates (freely estimated)\n            lb_wght ~ 1\n            anti_k1 ~ 1\n' #end of model definition\n\nAdattiamo il modello ai dati.\n\nlg_math_ticZERO_lavaan_fit &lt;- sem(lg_math_ticZERO_lavaan_model,\n  data = nlsy_math_wide,\n  meanstructure = TRUE,\n  estimator = \"ML\",\n  missing = \"fiml\"\n)\n\nEsaminiamo il risultato ottenuto.\n\nsummary(lg_math_ticZERO_lavaan_fit, fit.measures = TRUE) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 85 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        17\n  Number of equality constraints                     6\n\n  Number of observations                           933\n  Number of missing patterns                        61\n\nModel Test User Model:\n                                                      \n  Test statistic                               234.467\n  Degrees of freedom                                43\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               892.616\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.776\n  Tucker-Lewis Index (TLI)                       0.813\n                                                      \n  Robust Comparative Fit Index (CFI)             0.917\n  Robust Tucker-Lewis Index (TLI)                0.931\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -9792.208\n  Loglikelihood unrestricted model (H1)      -9674.975\n                                                      \n  Akaike (AIC)                               19606.416\n  Bayesian (BIC)                             19659.638\n  Sample-size adjusted Bayesian (SABIC)      19624.703\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.069\n  90 Percent confidence interval - lower         0.061\n  90 Percent confidence interval - upper         0.078\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.020\n                                                      \n  Robust RMSEA                                   0.097\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.173\n  P-value H_0: Robust RMSEA &lt;= 0.050             0.208\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.646\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.103\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta1 =~                                             \n    math2             1.000                           \n    math3             1.000                           \n    math4             1.000                           \n    math5             1.000                           \n    math6             1.000                           \n    math7             1.000                           \n    math8             1.000                           \n  eta2 =~                                             \n    math2             0.000                           \n    math3             1.000                           \n    math4             2.000                           \n    math5             3.000                           \n    math6             4.000                           \n    math7             5.000                           \n    math8             6.000                           \n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta1 ~                                              \n    lb_wght           0.000                           \n    anti_k1           0.000                           \n  eta2 ~                                              \n    lb_wght           0.000                           \n    anti_k1           0.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n .eta1 ~~                                             \n   .eta2             -0.181    1.150   -0.158    0.875\n  lb_wght ~~                                          \n    anti_k1           0.007    0.014    0.548    0.584\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .eta1             35.267    0.355   99.229    0.000\n   .eta2              4.339    0.088   49.136    0.000\n   .math2             0.000                           \n   .math3             0.000                           \n   .math4             0.000                           \n   .math5             0.000                           \n   .math6             0.000                           \n   .math7             0.000                           \n   .math8             0.000                           \n    lb_wght           0.080    0.009    9.031    0.000\n    anti_k1           1.454    0.050   29.216    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .eta1             64.562    5.659   11.408    0.000\n   .eta2              0.733    0.327    2.238    0.025\n   .math2   (thet)   36.230    1.867   19.410    0.000\n   .math3   (thet)   36.230    1.867   19.410    0.000\n   .math4   (thet)   36.230    1.867   19.410    0.000\n   .math5   (thet)   36.230    1.867   19.410    0.000\n   .math6   (thet)   36.230    1.867   19.410    0.000\n   .math7   (thet)   36.230    1.867   19.410    0.000\n   .math8   (thet)   36.230    1.867   19.410    0.000\n    lb_wght           0.074    0.003   21.599    0.000\n    anti_k1           2.312    0.107   21.599    0.000\n\n\n\nGeneriamo il diagramma di percorso.\n\nlg_math_ticZERO_lavaan_fit |&gt;\n    semPaths(\n        style = \"lisrel\",\n        whatLabels = \"std\", edge.label.cex = .6,\n        label.prop = 0.9, edge.label.color = \"black\", rotation = 4,\n        equalizeManifests = FALSE, optimizeLatRes = TRUE, node.width = 1.5,\n        edge.width = 0.5, shapeMan = \"rectangle\", shapeLat = \"ellipse\",\n        shapeInt = \"triangle\", sizeMan = 4, sizeInt = 2, sizeLat = 4,\n        curve = 2, unCol = \"#070b8c\"\n    )\n\n\n\n\n\n\n\n\nEseguiamo il confronto tra i due modelli mediante il test del rapporto tra verosimiglianze.\n\nlavTestLRT(lg_math_tic_lavaan_fit, lg_math_ticZERO_lavaan_fit) |&gt;\n    print()\n\n\nChi-Squared Difference Test\n\n                           Df   AIC   BIC  Chisq Chisq diff    RMSEA\nlg_math_tic_lavaan_fit     39 19600 19673 220.22                    \nlg_math_ticZERO_lavaan_fit 43 19606 19660 234.47     14.245 0.052395\n                           Df diff Pr(&gt;Chisq)   \nlg_math_tic_lavaan_fit                          \nlg_math_ticZERO_lavaan_fit       4   0.006552 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNel nostro esempio, il -2LL per il modello di crescita lineare era 15.937 e il -2LL per il modello di crescita lineare con due predittori invarianti nel tempo (basso peso alla nascita e comportamenti antisociali) dell’intercetta e della pendenza era 15.923, una differenza di 14. La differenza nel numero di parametri stimati era (19 – 15) = 4. Quindi, il miglioramento dell’adattamento era significativo (χ2(4) = 14, p &lt; .01), indicando che basso peso alla nascita e comportamenti antisociali erano predittori utili. Parallelamente alle differenze nei -2LL, le differenze in AIC e BIC hanno anche indicato un miglioramento nell’adattamento del modello (criteri di informazione più bassi indicano un migliore adattamento) quando i covarianti invarianti nel tempo erano inclusi nel modello.\nNel framework di modellazione delle equazioni strutturali, è tipico valutare l’adattamento globale (ad esempio, RMSEA, CFI, TLI); tuttavia, raramente l’aggiunta di covarianti invarianti nel tempo cambia significativamente questi indici. L’adattamento relativo dei modelli può essere informativo come nel framework di modellazione multilivello. Come sopra, le differenze nei -2LL (o χ2) possono essere calcolate per testare se l’inclusione dei covarianti invarianti nel tempo ha migliorato significativamente l’adattamento del modello. Si noti che, nel framework di modellazione delle equazioni strutturali, il modello di confronto (baseline) non è semplicemente un modello senza i covarianti invarianti nel tempo. Piuttosto, è un modello che include i covarianti invarianti nel tempo ma vincola i loro effetti sull’intercetta e sulla pendenza a 0.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>79</span>  <span class='chapter-title'>Covariate indipendenti dal tempo</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/09_time_inv_cov.html#confronto-con-il-modello-misto",
    "href": "chapters/lgm/09_time_inv_cov.html#confronto-con-il-modello-misto",
    "title": "79  Covariate indipendenti dal tempo",
    "section": "79.2 Confronto con il modello misto",
    "text": "79.2 Confronto con il modello misto\nEseguiamo ora l’analisi statistica utilizzando un modello misto con intercetta e pendenza casuale. Confronteremo un modello ridotto, che include solo l’effetto del tempo, con un modello completo che include le covariate esaminate in precedenza. Il modello completo include gli effetti principali delle covariate e l’interazione tra le covariate e il tempo.\nAdattiamo il modello “completo”.\n\nnlsy_math_long$grade_c2 &lt;- nlsy_math_long$grade-2\n\nfit2_lmer &lt;- lmer(\n    math ~ 1 + grade_c2 + lb_wght + anti_k1 + I(grade_c2 * lb_wght) + I(grade_c2 * anti_k1) +\n        (1 + grade_c2 | id),\n    data = nlsy_math_long,\n    REML = FALSE,\n    na.action = na.exclude\n)\n\nEsaminiamo i risultati ottenuti.\n\nsummary(fit2_lmer)\n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: math ~ 1 + grade_c2 + lb_wght + anti_k1 + I(grade_c2 * lb_wght) +  \n    I(grade_c2 * anti_k1) + (1 + grade_c2 | id)\n   Data: nlsy_math_long\n\n     AIC      BIC   logLik deviance df.resid \n 15943.1  16000.2  -7961.6  15923.1     2211 \n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.07986 -0.52517 -0.00867  0.53079  2.53455 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n id       (Intercept) 63.0653  7.941         \n          grade_c2     0.7141  0.845    -0.01\n Residual             36.2543  6.021         \nNumber of obs: 2221, groups:  id, 932\n\nFixed effects:\n                      Estimate Std. Error t value\n(Intercept)           36.28983    0.49630  73.120\ngrade_c2               4.31521    0.12060  35.782\nlb_wght               -2.71621    1.29359  -2.100\nanti_k1               -0.55087    0.23246  -2.370\nI(grade_c2 * lb_wght)  0.62463    0.33314   1.875\nI(grade_c2 * anti_k1) -0.01930    0.05886  -0.328\n\nCorrelation of Fixed Effects:\n            (Intr) grd_c2 lb_wgh ant_k1 I(_2*l\ngrade_c2    -0.529                            \nlb_wght     -0.194  0.096                     \nanti_k1     -0.671  0.358 -0.025              \nI(grd_2*l_)  0.091 -0.168 -0.532  0.026       \nI(gr_2*a_1)  0.343 -0.660  0.026 -0.529 -0.055\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.00577269 (tol = 0.002, component 1)\n\n\nAdattiamo un modello misto vincolato senza covariate, utilizzando un modello con intercetta e pendenza casuale.\n\nfit3_lmer &lt;- lmer(\n    math ~ 1 + grade_c2 + (1 + grade_c2 | id),\n    data = nlsy_math_long,\n    REML = FALSE,\n    na.action = na.exclude\n)\n\nConfrontiamo i due modelli utilizzando il test del rapporto di verosimiglianza.\n\nanova(fit2_lmer, fit3_lmer) |&gt; \n    print()\n\nData: nlsy_math_long\nModels:\nfit3_lmer: math ~ 1 + grade_c2 + (1 + grade_c2 | id)\nfit2_lmer: math ~ 1 + grade_c2 + lb_wght + anti_k1 + I(grade_c2 * lb_wght) + I(grade_c2 * anti_k1) + (1 + grade_c2 | id)\n          npar   AIC   BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)   \nfit3_lmer    6 15949 15984 -7968.7    15937                        \nfit2_lmer   10 15943 16000 -7961.6    15923 14.245  4   0.006552 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLa differenza nei -2LL tra questi due modelli era 14, con una differenza di 4 gradi di libertà, χ2(4) = 14, p &lt; .01. Questa differenza è identica a quella ottenuta confrontando i modelli nel framework di modellazione LGM. Giungiamo alla stessa conclusione riguardo l’importanza del basso peso alla nascita e dei comportamenti antisociali nel framework di modellazione multilivello e quando esaminiamo le differenze nelle traiettorie matematiche dei bambini.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>79</span>  <span class='chapter-title'>Covariate indipendenti dal tempo</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/09_time_inv_cov.html#considerazioni-importanti",
    "href": "chapters/lgm/09_time_inv_cov.html#considerazioni-importanti",
    "title": "79  Covariate indipendenti dal tempo",
    "section": "79.3 Considerazioni Importanti",
    "text": "79.3 Considerazioni Importanti\nLa maggior parte delle ricerche nel modello di crescita cerca di comprendere come le caratteristiche interpersonali (ovvero, i covarianti invarianti nel tempo) siano associate a differenze interpersonali nei cambiamenti intrapersonali catturati dai dati longitudinali. Nel nostro esempio illustrativo, ci siamo limitati a due covarianti invarianti nel tempo per semplicità. Tuttavia, è possibile includere contemporaneamente nel modello diversi covarianti invarianti nel tempo e le interazioni tra di essi. Come in tutte le analisi di regressione, è essenziale un’adeguata scala e centratura dei covarianti invarianti nel tempo per ottenere stime di parametri sostanzialmente significative. Tutte le pratiche comuni nella regressione, come l’esame delle interazioni (moderazione) tra i covarianti invarianti nel tempo, relazioni non lineari e test di mediazione, sono possibili e implementate in modi tipici. Ad esempio, è possibile calcolare variabili prodotto, includerle nel dataset e inserirle come predittori aggiuntivi per esaminare effetti interattivi. Inoltre, gli effetti dei covarianti invarianti nel tempo possono essere aggiunti al modello in modo gerarchico per isolare se il loro inserimento ha migliorato significativamente l’adattamento del modello (simile all’esame del cambiamento significativo in R^2).\n\n79.3.1 Varianza Spiegata\nOltre a valutare l’importanza dei covarianti invarianti nel tempo, i ricercatori vogliono anche sapere quanto della varianza nell’intercetta e nella pendenza sia stata spiegata dai covarianti invarianti nel tempo. In altre parole, quale proporzione delle differenze interpersonali nell’intercetta e nelle pendenze è stata spiegata dai covarianti invarianti nel tempo. Nei framework di modellazione multilivello e di equazioni strutturali, è possibile confrontare le stime di varianza dell’intercetta e della pendenza ottenute in modelli con e senza covarianti invarianti nel tempo. Nel nostro esempio, ad esempio, la stima della varianza dell’intercetta era 64.562 per il modello di crescita lineare senza covarianti invarianti nel tempo e 63.064 quando il basso peso alla nascita e i comportamenti antisociali erano inclusi come covarianti invarianti nel tempo. La differenza tra le varianze stimate era di 1.498. Convertendo questa differenza in una proporzione della varianza originale, troviamo che i covarianti invarianti nel tempo hanno spiegato lo 0.023 (1.498/64.562) ovvero il 2.3% delle differenze interpersonali nell’intercetta. Calcoli simili per la pendenza hanno prodotto una varianza spiegata dello 0.027 (2.7%).\n\n\n79.3.2 Coefficienti Standardizzati\nNell’ambito della ricerca, oltre alla valutazione della varianza spiegata, può essere utile calcolare i coefficienti standardizzati. Questi aiutano a determinare l’importanza di ciascun predittore e funzionano come una misura della grandezza dell’effetto. I coefficienti di regressione di secondo livello (percorsi), che partono dai covarianti invarianti nel tempo verso l’intercetta e la pendenza, sono inizialmente non standardizzati. Per ottenere i coefficienti standardizzati, è necessario moltiplicare il coefficiente non standardizzato per il rapporto tra la deviazione standard del predittore (cioè, il covariante invariante nel tempo) e quella del risultato (cioè, l’intercetta o la pendenza).\nLa formula generale per il calcolo di un coefficiente standardizzato è:\n\\[ \\beta^* = \\frac{b \\times \\sigma_{\\text{predittore}}}{\\sigma_{\\text{risultato}}}, \\]\ndove:\n\n\\(\\beta^*\\) rappresenta il coefficiente standardizzato.\n\\(b\\) è il coefficiente non standardizzato.\n\\(\\sigma_{\\text{predittore}}\\) è la deviazione standard del predittore, come i comportamenti antisociali nel nostro caso.\n\\(\\sigma_{\\text{risultato}}\\) è la deviazione standard del risultato, come l’intercetta nel nostro caso.\n\nApplicando questa formula, il coefficiente standardizzato per l’effetto dei comportamenti antisociali sull’intercetta è dato da:\n\\[\n\\beta^* = \\frac{-0.551 \\times \\sigma_{\\text{anti\\_k1}}}{\\sqrt{63.065 + (0.080 \\times \\sigma_{\\text{X1}} \\times 0.080) + (-0.551 \\times \\sigma_{\\text{anti\\_k1}} \\times -0.551) + 2 \\times (0.080 \\times \\sigma_{\\text{X1,anti\\_k1}} \\times -0.551)}}\n\\]\nQui, \\(\\sigma_{\\text{anti\\_k1}}\\) rappresenta la deviazione standard dei comportamenti antisociali, \\(\\sigma_{\\text{X1}}\\) è la deviazione standard di un altro predittore, se presente, e \\(\\sigma_{\\text{X1,anti\\_k1}}\\) è la covarianza tra i due predittori.\nIn conclusione, il calcolo produce:\n\\[\n-0.551 \\times 2.312 / \\sqrt{63.065 + (0.080 \\times 0.074 \\times 0.080) + (-0.551 \\times 2.312 \\times -0.551) + 2 \\times (0.080 \\times 0.007 \\times -0.551)} = -0.105\n\\]\nQuindi, l’effetto dei comportamenti antisociali sui punteggi di matematica di seconda elementare è risultato essere di piccola entità.\n\n\n79.3.3 Riflessioni Conclusive\nIn questo capitolo, abbiamo esaminato il modello di crescita lineare con covarianti invarianti nel tempo, un modello spesso utilizzato per esaminare le differenze individuali nella crescita e nel cambiamento. L’uso di questo modello implica una serie di assunzioni. Innanzitutto, il modello presume l’invarianza della struttura del cambiamento per tutte le persone. Ciò significa che si assume che tutti i bambini, indipendentemente dai loro punteggi sui covarianti invarianti nel tempo, seguano una traiettoria di crescita lineare. Inoltre, abbiamo ipotizzato che la grandezza della varianza residua nell’intercetta e nella pendenza, così come la covarianza residua tra l’intercetta e la pendenza, siano le stesse per i bambini con valori diversi sui covarianti invarianti nel tempo.\nAssumiamo anche che la varianza residua dei punteggi osservati sia equivalente per tutti i bambini. In altre parole, indipendentemente dai valori dei covarianti invarianti nel tempo, l’inadeguatezza del modello lineare è identica. Nel nostro esempio, abbiamo ipotizzato che la grandezza delle fluttuazioni annuali nelle prestazioni matematiche dei bambini con livelli inferiori o superiori di comportamento antisociale fosse equivalente. Dato che queste assunzioni potrebbero essere vere o meno, esse dovrebbero essere attentamente considerate prima di intraprendere tali analisi.\nNel prossimo capitolo, discuteremo i modelli di crescita per gruppi multipli che facilitano un esame approfondito di queste assunzioni per determinati tipi di covarianti invarianti nel tempo, in particolare quelli che sono variabili categoriche, ordinali o variabili continue che sono state categorizzate (ad esempio, tramite uno split mediano). Questo approccio permette una verifica più accurata e specifica delle ipotesi del modello in contesti diversi e con differenti tipologie di dati.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>79</span>  <span class='chapter-title'>Covariate indipendenti dal tempo</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/10_growth_groups.html",
    "href": "chapters/lgm/10_growth_groups.html",
    "title": "80  Modelli di crescita latenti a gruppi multipli",
    "section": "",
    "text": "80.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nNel capitolo precedente, abbiamo approfondito i modelli di crescita con covarianti invarianti nel tempo. In questo capitolo, esploreremo un approccio alternativo per studiare le differenze individuali nel cambiamento: l’analisi comparativa tra gruppi (McArdle, 1989; McArdle & Hamagami, 1996). Sebbene i modelli di crescita con covarianti invarianti nel tempo siano efficaci nell’analizzare le differenze nelle traiettorie medie di crescita, questi modelli presentano limitazioni nell’indagare altri aspetti dei cambiamenti intrapersonali e delle differenze interpersonali in tali processi.\nSenza adeguati ampliamenti, i modelli basati esclusivamente su covarianti invarianti nel tempo non forniscono informazioni sulle variazioni nelle varianze e covarianze tra i fattori di crescita, né sulla variabilità residua e sulla dinamica dei cambiamenti intrapersonali. Nel presente capitolo, dimostreremo come l’approccio di confronto tra gruppi possa essere impiegato per esaminare le differenze in qualsiasi aspetto del modello di crescita. Questa flessibilità metodologica ci permette di acquisire una comprensione più profonda su come e perché gli individui mostrino percorsi di sviluppo diversificati.\nPer i nostri esempi, utilizziamo i punteggi di rendimento in matematica dai dati NLSY-CYA [si veda {cite:t}grimm2016growth]. Importiamo i dati.\n# set filepath for data file\nfilepath &lt;- \"https://raw.githubusercontent.com/LRI-2/Data/main/GrowthModeling/nlsy_math_long_R.dat\"\n\n# read in the text data file using the url() function\ndat &lt;- read.table(\n  file = url(filepath),\n  na.strings = \".\"\n) # indicates the missing data designator\n\n# copy data with new name\nnlsy_math_long &lt;- dat\n\n# Add names the columns of the data set\nnames(nlsy_math_long) &lt;- c(\n  \"id\", \"female\", \"lb_wght\",\n  \"anti_k1\", \"math\", \"grade\",\n  \"occ\", \"age\", \"men\",\n  \"spring\", \"anti\"\n)\n\n# reducing to variables of interest\nnlsy_math_long &lt;- nlsy_math_long[, c(\"id\", \"grade\", \"math\", \"lb_wght\")]\n\n# adding another dummy code variable for normal birth weight that coded the opposite of the low brithweight variable.\nnlsy_math_long$nb_wght &lt;- 1 - nlsy_math_long$lb_wght\n\n# view the first few observations in the data set\nhead(nlsy_math_long, 10) |&gt;\n  print()\n\n     id grade math lb_wght nb_wght\n1   201     3   38       0       1\n2   201     5   55       0       1\n3   303     2   26       0       1\n4   303     5   33       0       1\n5  2702     2   56       0       1\n6  2702     4   58       0       1\n7  2702     8   80       0       1\n8  4303     3   41       0       1\n9  4303     4   58       0       1\n10 5002     4   46       0       1\nEsaminiamo le curve di crescita nei due gruppi.\n# intraindividual change trajetories\nggplot(\n  data = nlsy_math_long, # data set\n  aes(x = grade, y = math, group = id)\n) + # setting variables\n  geom_point(size = .5) + # adding points to plot\n  geom_line(alpha=0.3) + # adding lines to plot\n  # setting the x-axis with breaks and labels\n  scale_x_continuous(\n    limits = c(2, 8),\n    breaks = c(2, 3, 4, 5, 6, 7, 8),\n    name = \"Grade at Testing\"\n  ) +\n  # setting the y-axis with limits breaks and labels\n  scale_y_continuous(\n    limits = c(10, 90),\n    breaks = c(10, 30, 50, 70, 90),\n    name = \"PIAT Mathematics\"\n  ) +\n  facet_wrap(~lb_wght)\nPer semplicità, carichiamo di nuovo i dati già trasformati in formato wide.\n# set filepath for data file\nfilepath &lt;- \"https://raw.githubusercontent.com/LRI-2/Data/main/GrowthModeling/nlsy_math_wide_R.dat\"\n# read in the text data file using the url() function\ndat &lt;- read.table(\n  file = url(filepath),\n  na.strings = \".\"\n) # indicates the missing data designator\n# copy data with new name\nnlsy_math_wide &lt;- dat\n\n# Give the variable names\nnames(nlsy_math_wide) &lt;- c(\n  \"id\", \"female\", \"lb_wght\", \"anti_k1\",\n  \"math2\", \"math3\", \"math4\", \"math5\", \"math6\", \"math7\", \"math8\",\n  \"age2\", \"age3\", \"age4\", \"age5\", \"age6\", \"age7\", \"age8\",\n  \"men2\", \"men3\", \"men4\", \"men5\", \"men6\", \"men7\", \"men8\",\n  \"spring2\", \"spring3\", \"spring4\", \"spring5\", \"spring6\", \"spring7\", \"spring8\",\n  \"anti2\", \"anti3\", \"anti4\", \"anti5\", \"anti6\", \"anti7\", \"anti8\"\n)\n\n# view the first few observations (and columns) in the data set\nhead(nlsy_math_wide[, 1:11], 10)\n\n\nA data.frame: 10 x 11\n\n\n\nid\nfemale\nlb_wght\nanti_k1\nmath2\nmath3\nmath4\nmath5\nmath6\nmath7\nmath8\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n201\n1\n0\n0\nNA\n38\nNA\n55\nNA\nNA\nNA\n\n\n2\n303\n1\n0\n1\n26\nNA\nNA\n33\nNA\nNA\nNA\n\n\n3\n2702\n0\n0\n0\n56\nNA\n58\nNA\nNA\nNA\n80\n\n\n4\n4303\n1\n0\n0\nNA\n41\n58\nNA\nNA\nNA\nNA\n\n\n5\n5002\n0\n0\n4\nNA\nNA\n46\nNA\n54\nNA\n66\n\n\n6\n5005\n1\n0\n0\n35\nNA\n50\nNA\n60\nNA\n59\n\n\n7\n5701\n0\n0\n2\nNA\n62\n61\nNA\nNA\nNA\nNA\n\n\n8\n6102\n0\n0\n0\nNA\nNA\n55\n67\nNA\n81\nNA\n\n\n9\n6801\n1\n0\n0\nNA\n54\nNA\n62\nNA\n66\nNA\n\n\n10\n6802\n0\n0\n0\nNA\n55\nNA\n66\nNA\n68\nNA",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>80</span>  <span class='chapter-title'>Modelli di crescita latenti a gruppi multipli</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/10_growth_groups.html#invarianza-tra-gruppi",
    "href": "chapters/lgm/10_growth_groups.html#invarianza-tra-gruppi",
    "title": "80  Modelli di crescita latenti a gruppi multipli",
    "section": "80.2 Invarianza tra gruppi",
    "text": "80.2 Invarianza tra gruppi\nDefiniamo il modello di crescita latente per i due gruppi.\n\n# writing out linear growth model in full SEM way\nmg_math_lavaan_model &lt;- \"\n  # latent variable definitions\n      #intercept (note intercept is a reserved term)\n      eta_1 =~ 1*math2\n      eta_1 =~ 1*math3\n      eta_1 =~ 1*math4\n      eta_1 =~ 1*math5\n      eta_1 =~ 1*math6\n      eta_1 =~ 1*math7\n      eta_1 =~ 1*math8\n\n      #linear slope\n      eta_2 =~ 0*math2\n      eta_2 =~ 1*math3\n      eta_2 =~ 2*math4\n      eta_2 =~ 3*math5\n      eta_2 =~ 4*math6\n      eta_2 =~ 5*math7\n      eta_2 =~ 6*math8\n\n  # factor variances\n      eta_1 ~~ eta_1\n      eta_2 ~~ eta_2\n\n  # covariances among factors\n      eta_1 ~~ eta_2\n\n  # factor means\n      eta_1 ~ start(35)*1\n      eta_2 ~ start(4)*1\n\n  # manifest variances (made equivalent by naming theta)\n      math2 ~~ theta*math2\n      math3 ~~ theta*math3\n      math4 ~~ theta*math4\n      math5 ~~ theta*math5\n      math6 ~~ theta*math6\n      math7 ~~ theta*math7\n      math8 ~~ theta*math8\n  # manifest means (fixed at zero)\n      math2 ~ 0*1\n      math3 ~ 0*1\n      math4 ~ 0*1\n      math5 ~ 0*1\n      math6 ~ 0*1\n      math7 ~ 0*1\n      math8 ~ 0*1\n\" # end of model definition\n\nAdattiamo il modello ai dati specificando la separazione delle osservazioni in due gruppi e introducendo i vincoli di eguaglianza tra gruppi sulle saturazioni fattoriali, le medie, le varianze, le covarianze, e i residui. In questo modello, sostanzialmente, non c’è alcune differenza tra gruppi.\n\nmg_math_lavaan_fitM1 &lt;- sem(mg_math_lavaan_model,\n  data = nlsy_math_wide,\n  meanstructure = TRUE,\n  estimator = \"ML\",\n  missing = \"fiml\",\n  group = \"lb_wght\", # to separate groups\n  group.equal = c(\n    \"loadings\", # for constraints\n    \"means\",\n    \"lv.variances\",\n    \"lv.covariances\",\n    \"residuals\"\n  )\n)\n\nEsaminiamo i risultati.\n\nsummary(mg_math_lavaan_fitM1, fit.measures = TRUE) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 24 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        24\n  Number of equality constraints                    18\n\n  Number of observations per group:               Used       Total\n    0                                              857         858\n    1                                               75          75\n  Number of missing patterns per group:                           \n    0                                               60            \n    1                                               25            \n\nModel Test User Model:\n                                                      \n  Test statistic                               249.111\n  Degrees of freedom                                64\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    0                                          191.954\n    1                                           57.156\n\nModel Test Baseline Model:\n\n  Test statistic                               887.887\n  Degrees of freedom                                42\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.781\n  Tucker-Lewis Index (TLI)                       0.856\n                                                      \n  Robust Comparative Fit Index (CFI)             1.000\n  Robust Tucker-Lewis Index (TLI)                0.346\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -7968.693\n  Loglikelihood unrestricted model (H1)      -7844.138\n                                                      \n  Akaike (AIC)                               15949.386\n  Bayesian (BIC)                             15978.410\n  Sample-size adjusted Bayesian (SABIC)      15959.354\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.079\n  90 Percent confidence interval - lower         0.069\n  90 Percent confidence interval - upper         0.089\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.436\n                                                      \n  Robust RMSEA                                   0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.000\n  P-value H_0: Robust RMSEA &lt;= 0.050             1.000\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.128\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\n\nGroup 1 [0]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 =~                                            \n    math2             1.000                           \n    math3             1.000                           \n    math4             1.000                           \n    math5             1.000                           \n    math6             1.000                           \n    math7             1.000                           \n    math8             1.000                           \n  eta_2 =~                                            \n    math2             0.000                           \n    math3             1.000                           \n    math4             2.000                           \n    math5             3.000                           \n    math6             4.000                           \n    math7             5.000                           \n    math8             6.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 ~~                                            \n    eta_2   (.17.)   -0.181    1.150   -0.158    0.875\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1   (.18.)   35.267    0.355   99.229    0.000\n    eta_2   (.19.)    4.339    0.088   49.136    0.000\n   .math2             0.000                           \n   .math3             0.000                           \n   .math4             0.000                           \n   .math5             0.000                           \n   .math6             0.000                           \n   .math7             0.000                           \n   .math8             0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1   (.15.)   64.562    5.659   11.408    0.000\n    eta_2   (.16.)    0.733    0.327    2.238    0.025\n   .math2   (thet)   36.230    1.867   19.410    0.000\n   .math3   (thet)   36.230    1.867   19.410    0.000\n   .math4   (thet)   36.230    1.867   19.410    0.000\n   .math5   (thet)   36.230    1.867   19.410    0.000\n   .math6   (thet)   36.230    1.867   19.410    0.000\n   .math7   (thet)   36.230    1.867   19.410    0.000\n   .math8   (thet)   36.230    1.867   19.410    0.000\n\n\nGroup 2 [1]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 =~                                            \n    math2             1.000                           \n    math3             1.000                           \n    math4             1.000                           \n    math5             1.000                           \n    math6             1.000                           \n    math7             1.000                           \n    math8             1.000                           \n  eta_2 =~                                            \n    math2             0.000                           \n    math3             1.000                           \n    math4             2.000                           \n    math5             3.000                           \n    math6             4.000                           \n    math7             5.000                           \n    math8             6.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 ~~                                            \n    eta_2   (.17.)   -0.181    1.150   -0.158    0.875\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1   (.18.)   35.267    0.355   99.229    0.000\n    eta_2   (.19.)    4.339    0.088   49.136    0.000\n   .math2             0.000                           \n   .math3             0.000                           \n   .math4             0.000                           \n   .math5             0.000                           \n   .math6             0.000                           \n   .math7             0.000                           \n   .math8             0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1   (.15.)   64.562    5.659   11.408    0.000\n    eta_2   (.16.)    0.733    0.327    2.238    0.025\n   .math2   (thet)   36.230    1.867   19.410    0.000\n   .math3   (thet)   36.230    1.867   19.410    0.000\n   .math4   (thet)   36.230    1.867   19.410    0.000\n   .math5   (thet)   36.230    1.867   19.410    0.000\n   .math6   (thet)   36.230    1.867   19.410    0.000\n   .math7   (thet)   36.230    1.867   19.410    0.000\n   .math8   (thet)   36.230    1.867   19.410    0.000\n\n\n\nCreiamo il diagramma di percorso.\n\nsemPaths(mg_math_lavaan_fitM1, what = \"path\", whatLabels = \"par\")",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>80</span>  <span class='chapter-title'>Modelli di crescita latenti a gruppi multipli</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/10_growth_groups.html#vincoli-sulle-medie",
    "href": "chapters/lgm/10_growth_groups.html#vincoli-sulle-medie",
    "title": "80  Modelli di crescita latenti a gruppi multipli",
    "section": "80.3 Vincoli sulle medie",
    "text": "80.3 Vincoli sulle medie\nTrasformiamo ora il modello restrittivo specificato in precedenza allentando via via i vincoli che abbiamo introdotto. In questo modello rendiamo possibile la differenza tra le medie nei due gruppi.\n\nmg_math_lavaan_fitM2 &lt;- sem(mg_math_lavaan_model,\n  data = nlsy_math_wide,\n  meanstructure = TRUE,\n  estimator = \"ML\",\n  missing = \"fiml\",\n  group = \"lb_wght\", # to separate groups\n  group.equal = c(\n    \"loadings\", # for constraints\n    # \"means\", commented out so can differ\n    \"lv.variances\",\n    \"lv.covariances\",\n    \"residuals\"\n  )\n)\n\nEsaminiamo i risultati.\n\nsummary(mg_math_lavaan_fitM2, fit.measures = TRUE) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 31 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        24\n  Number of equality constraints                    16\n\n  Number of observations per group:               Used       Total\n    0                                              857         858\n    1                                               75          75\n  Number of missing patterns per group:                           \n    0                                               60            \n    1                                               25            \n\nModel Test User Model:\n                                                      \n  Test statistic                               243.910\n  Degrees of freedom                                62\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    0                                          191.440\n    1                                           52.470\n\nModel Test Baseline Model:\n\n  Test statistic                               887.887\n  Degrees of freedom                                42\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.785\n  Tucker-Lewis Index (TLI)                       0.854\n                                                      \n  Robust Comparative Fit Index (CFI)             1.000\n  Robust Tucker-Lewis Index (TLI)                0.326\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -7966.093\n  Loglikelihood unrestricted model (H1)      -7844.138\n                                                      \n  Akaike (AIC)                               15948.185\n  Bayesian (BIC)                             15986.884\n  Sample-size adjusted Bayesian (SABIC)      15961.477\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.079\n  90 Percent confidence interval - lower         0.069\n  90 Percent confidence interval - upper         0.090\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.472\n                                                      \n  Robust RMSEA                                   0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.000\n  P-value H_0: Robust RMSEA &lt;= 0.050             1.000\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.127\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\n\nGroup 1 [0]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 =~                                            \n    math2             1.000                           \n    math3             1.000                           \n    math4             1.000                           \n    math5             1.000                           \n    math6             1.000                           \n    math7             1.000                           \n    math8             1.000                           \n  eta_2 =~                                            \n    math2             0.000                           \n    math3             1.000                           \n    math4             2.000                           \n    math5             3.000                           \n    math6             4.000                           \n    math7             5.000                           \n    math8             6.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 ~~                                            \n    eta_2   (.17.)   -0.035    1.144   -0.031    0.975\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1            35.488    0.369   96.080    0.000\n    eta_2             4.292    0.092   46.898    0.000\n   .math2             0.000                           \n   .math3             0.000                           \n   .math4             0.000                           \n   .math5             0.000                           \n   .math6             0.000                           \n   .math7             0.000                           \n   .math8             0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1   (.15.)   63.704    5.637   11.301    0.000\n    eta_2   (.16.)    0.699    0.325    2.147    0.032\n   .math2   (thet)   36.321    1.871   19.413    0.000\n   .math3   (thet)   36.321    1.871   19.413    0.000\n   .math4   (thet)   36.321    1.871   19.413    0.000\n   .math5   (thet)   36.321    1.871   19.413    0.000\n   .math6   (thet)   36.321    1.871   19.413    0.000\n   .math7   (thet)   36.321    1.871   19.413    0.000\n   .math8   (thet)   36.321    1.871   19.413    0.000\n\n\nGroup 2 [1]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 =~                                            \n    math2             1.000                           \n    math3             1.000                           \n    math4             1.000                           \n    math5             1.000                           \n    math6             1.000                           \n    math7             1.000                           \n    math8             1.000                           \n  eta_2 =~                                            \n    math2             0.000                           \n    math3             1.000                           \n    math4             2.000                           \n    math5             3.000                           \n    math6             4.000                           \n    math7             5.000                           \n    math8             6.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 ~~                                            \n    eta_2   (.17.)   -0.035    1.144   -0.031    0.975\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1            32.733    1.244   26.314    0.000\n    eta_2             4.905    0.320   15.320    0.000\n   .math2             0.000                           \n   .math3             0.000                           \n   .math4             0.000                           \n   .math5             0.000                           \n   .math6             0.000                           \n   .math7             0.000                           \n   .math8             0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1   (.15.)   63.704    5.637   11.301    0.000\n    eta_2   (.16.)    0.699    0.325    2.147    0.032\n   .math2   (thet)   36.321    1.871   19.413    0.000\n   .math3   (thet)   36.321    1.871   19.413    0.000\n   .math4   (thet)   36.321    1.871   19.413    0.000\n   .math5   (thet)   36.321    1.871   19.413    0.000\n   .math6   (thet)   36.321    1.871   19.413    0.000\n   .math7   (thet)   36.321    1.871   19.413    0.000\n   .math8   (thet)   36.321    1.871   19.413    0.000\n\n\n\nEseguiamo il confronto tra i due modelli.\n\nlavTestLRT(mg_math_lavaan_fitM1, mg_math_lavaan_fitM2) |&gt;\n    print()\n\n\nChi-Squared Difference Test\n\n                     Df   AIC   BIC  Chisq Chisq diff    RMSEA Df diff\nmg_math_lavaan_fitM2 62 15948 15987 243.91                            \nmg_math_lavaan_fitM1 64 15949 15978 249.11     5.2005 0.058601       2\n                     Pr(&gt;Chisq)  \nmg_math_lavaan_fitM2             \nmg_math_lavaan_fitM1    0.07425 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNon vi è evidenza che consentire una differenza tra medie tra gruppi migliori l’adattamento del modello.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>80</span>  <span class='chapter-title'>Modelli di crescita latenti a gruppi multipli</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/10_growth_groups.html#vincoli-sulle-varianzecovarianze",
    "href": "chapters/lgm/10_growth_groups.html#vincoli-sulle-varianzecovarianze",
    "title": "80  Modelli di crescita latenti a gruppi multipli",
    "section": "80.4 Vincoli sulle varianze/covarianze",
    "text": "80.4 Vincoli sulle varianze/covarianze\nNel modello M3 consentiamo che anche le varianza e le covarianza differiscano tra gruppi, oltre alle medie.\n\nmg_math_lavaan_fitM3 &lt;- sem(mg_math_lavaan_model,\n  data = nlsy_math_wide,\n  meanstructure = TRUE,\n  estimator = \"ML\",\n  missing = \"fiml\",\n  group = \"lb_wght\", # to separate groups\n  group.equal = c(\n    \"loadings\", # for constraints\n    # \"means\", commented out so can differ\n    # \"lv.variances\",\n    # \"lv.covariances\",\n    \"residuals\"\n  )\n)\n\nEsaminiamo i risultati.\n\nsummary(mg_math_lavaan_fitM3, fit.measures = TRUE) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 57 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        24\n  Number of equality constraints                    13\n\n  Number of observations per group:               Used       Total\n    0                                              857         858\n    1                                               75          75\n  Number of missing patterns per group:                           \n    0                                               60            \n    1                                               25            \n\nModel Test User Model:\n                                                      \n  Test statistic                               241.182\n  Degrees of freedom                                59\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    0                                          191.157\n    1                                           50.024\n\nModel Test Baseline Model:\n\n  Test statistic                               887.887\n  Degrees of freedom                                42\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.785\n  Tucker-Lewis Index (TLI)                       0.847\n                                                      \n  Robust Comparative Fit Index (CFI)             1.000\n  Robust Tucker-Lewis Index (TLI)                0.320\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -7964.728\n  Loglikelihood unrestricted model (H1)      -7844.138\n                                                      \n  Akaike (AIC)                               15951.457\n  Bayesian (BIC)                             16004.668\n  Sample-size adjusted Bayesian (SABIC)      15969.732\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.081\n  90 Percent confidence interval - lower         0.071\n  90 Percent confidence interval - upper         0.092\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.598\n                                                      \n  Robust RMSEA                                   0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.000\n  P-value H_0: Robust RMSEA &lt;= 0.050             1.000\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.124\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\n\nGroup 1 [0]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 =~                                            \n    math2             1.000                           \n    math3             1.000                           \n    math4             1.000                           \n    math5             1.000                           \n    math6             1.000                           \n    math7             1.000                           \n    math8             1.000                           \n  eta_2 =~                                            \n    math2             0.000                           \n    math3             1.000                           \n    math4             2.000                           \n    math5             3.000                           \n    math6             4.000                           \n    math7             5.000                           \n    math8             6.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 ~~                                            \n    eta_2             0.243    1.147    0.212    0.832\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1            35.485    0.365   97.271    0.000\n    eta_2             4.293    0.091   47.089    0.000\n   .math2             0.000                           \n   .math3             0.000                           \n   .math4             0.000                           \n   .math5             0.000                           \n   .math6             0.000                           \n   .math7             0.000                           \n   .math8             0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1            61.062    5.692   10.727    0.000\n    eta_2             0.663    0.326    2.033    0.042\n   .math2   (thet)   36.276    1.870   19.402    0.000\n   .math3   (thet)   36.276    1.870   19.402    0.000\n   .math4   (thet)   36.276    1.870   19.402    0.000\n   .math5   (thet)   36.276    1.870   19.402    0.000\n   .math6   (thet)   36.276    1.870   19.402    0.000\n   .math7   (thet)   36.276    1.870   19.402    0.000\n   .math8   (thet)   36.276    1.870   19.402    0.000\n\n\nGroup 2 [1]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 =~                                            \n    math2             1.000                           \n    math3             1.000                           \n    math4             1.000                           \n    math5             1.000                           \n    math6             1.000                           \n    math7             1.000                           \n    math8             1.000                           \n  eta_2 =~                                            \n    math2             0.000                           \n    math3             1.000                           \n    math4             2.000                           \n    math5             3.000                           \n    math6             4.000                           \n    math7             5.000                           \n    math8             6.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 ~~                                            \n    eta_2            -3.801    4.912   -0.774    0.439\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1            32.850    1.413   23.241    0.000\n    eta_2             4.881    0.341   14.332    0.000\n   .math2             0.000                           \n   .math3             0.000                           \n   .math4             0.000                           \n   .math5             0.000                           \n   .math6             0.000                           \n   .math7             0.000                           \n   .math8             0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1            95.283   24.221    3.934    0.000\n    eta_2             1.297    1.315    0.986    0.324\n   .math2   (thet)   36.276    1.870   19.402    0.000\n   .math3   (thet)   36.276    1.870   19.402    0.000\n   .math4   (thet)   36.276    1.870   19.402    0.000\n   .math5   (thet)   36.276    1.870   19.402    0.000\n   .math6   (thet)   36.276    1.870   19.402    0.000\n   .math7   (thet)   36.276    1.870   19.402    0.000\n   .math8   (thet)   36.276    1.870   19.402    0.000\n\n\n\nConfrontiamo il modello M2 con il modello M3.\n\nlavTestLRT(mg_math_lavaan_fitM2, mg_math_lavaan_fitM3) |&gt;\n    print()\n\n\nChi-Squared Difference Test\n\n                     Df   AIC   BIC  Chisq Chisq diff RMSEA Df diff\nmg_math_lavaan_fitM3 59 15952 16005 241.18                         \nmg_math_lavaan_fitM2 62 15948 15987 243.91     2.7283     0       3\n                     Pr(&gt;Chisq)\nmg_math_lavaan_fitM3           \nmg_math_lavaan_fitM2     0.4354\n\n\nNon ci sono evidenze che una differenza nelle varianze e nelle covarianze tra gruppi migliori la bontà dell’adattamento del modello ai dati.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>80</span>  <span class='chapter-title'>Modelli di crescita latenti a gruppi multipli</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/10_growth_groups.html#vincoli-sui-residui",
    "href": "chapters/lgm/10_growth_groups.html#vincoli-sui-residui",
    "title": "80  Modelli di crescita latenti a gruppi multipli",
    "section": "80.5 Vincoli sui residui",
    "text": "80.5 Vincoli sui residui\nEsaminiamo ora il vincolo sulle covarianze residue. Iniziamo a specificare il modello in una nuova forma.\n\nmg_math_lavaan_model4 &lt;- \"\n  # latent variable definitions\n      #intercept (note intercept is a reserved term)\n      eta_1 =~ 1*math2\n      eta_1 =~ 1*math3\n      eta_1 =~ 1*math4\n      eta_1 =~ 1*math5\n      eta_1 =~ 1*math6\n      eta_1 =~ 1*math7\n      eta_1 =~ 1*math8\n\n      #linear slope\n      eta_2 =~ 0*math2\n      eta_2 =~ 1*math3\n      eta_2 =~ 2*math4\n      eta_2 =~ 3*math5\n      eta_2 =~ 4*math6\n      eta_2 =~ 5*math7\n      eta_2 =~ 6*math8\n\n  # factor variances\n      eta_1 ~~ start(60)*eta_1\n      eta_2 ~~ start(.75)*eta_2\n\n  # covariances among factors\n      eta_1 ~~ eta_2\n\n  # factor means\n      eta_1 ~ start(35)*1\n      eta_2 ~ start(4)*1\n\n  # manifest variances (made equivalent by naming theta)\n      math2 ~~ c(theta1,theta2)*math2\n      math3 ~~ c(theta1,theta2)*math3\n      math4 ~~ c(theta1,theta2)*math4\n      math5 ~~ c(theta1,theta2)*math5\n      math6 ~~ c(theta1,theta2)*math6\n      math7 ~~ c(theta1,theta2)*math7\n      math8 ~~ c(theta1,theta2)*math8\n  # manifest means (fixed at zero)\n      math2 ~ 0*1\n      math3 ~ 0*1\n      math4 ~ 0*1\n      math5 ~ 0*1\n      math6 ~ 0*1\n      math7 ~ 0*1\n      math8 ~ 0*1\n\" # end of model definition\n\nAdattiamo il modello ai dati.\n\nmg_math_lavaan_fitM4 &lt;- sem(mg_math_lavaan_model4,\n  data = nlsy_math_wide,\n  meanstructure = TRUE,\n  estimator = \"ML\",\n  missing = \"fiml\",\n  group = \"lb_wght\", # to separate groups\n  group.equal = c(\"loadings\")\n) # for constraints\n\nEsaminiamo i risulati.\n\nsummary(mg_math_lavaan_fitM4, fit.measures = TRUE) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 62 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        24\n  Number of equality constraints                    12\n\n  Number of observations per group:               Used       Total\n    0                                              857         858\n    1                                               75          75\n  Number of missing patterns per group:                           \n    0                                               60            \n    1                                               25            \n\nModel Test User Model:\n                                                      \n  Test statistic                               237.836\n  Degrees of freedom                                58\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    0                                          190.833\n    1                                           47.004\n\nModel Test Baseline Model:\n\n  Test statistic                               887.887\n  Degrees of freedom                                42\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.787\n  Tucker-Lewis Index (TLI)                       0.846\n                                                      \n  Robust Comparative Fit Index (CFI)             1.000\n  Robust Tucker-Lewis Index (TLI)                0.294\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -7963.056\n  Loglikelihood unrestricted model (H1)      -7844.138\n                                                      \n  Akaike (AIC)                               15950.111\n  Bayesian (BIC)                             16008.159\n  Sample-size adjusted Bayesian (SABIC)      15970.048\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.082\n  90 Percent confidence interval - lower         0.071\n  90 Percent confidence interval - upper         0.092\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.607\n                                                      \n  Robust RMSEA                                   0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.000\n  P-value H_0: Robust RMSEA &lt;= 0.050             1.000\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.124\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\n\nGroup 1 [0]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 =~                                            \n    math2             1.000                           \n    math3             1.000                           \n    math4             1.000                           \n    math5             1.000                           \n    math6             1.000                           \n    math7             1.000                           \n    math8             1.000                           \n  eta_2 =~                                            \n    math2             0.000                           \n    math3             1.000                           \n    math4             2.000                           \n    math5             3.000                           \n    math6             4.000                           \n    math7             5.000                           \n    math8             6.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 ~~                                            \n    eta_2            -0.063    1.161   -0.054    0.957\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1            35.481    0.365   97.257    0.000\n    eta_2             4.297    0.091   47.145    0.000\n   .math2             0.000                           \n   .math3             0.000                           \n   .math4             0.000                           \n   .math5             0.000                           \n   .math6             0.000                           \n   .math7             0.000                           \n   .math8             0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1            62.287    5.728   10.873    0.000\n    eta_2             0.774    0.334    2.314    0.021\n   .math2   (tht1)   35.172    1.904   18.473    0.000\n   .math3   (tht1)   35.172    1.904   18.473    0.000\n   .math4   (tht1)   35.172    1.904   18.473    0.000\n   .math5   (tht1)   35.172    1.904   18.473    0.000\n   .math6   (tht1)   35.172    1.904   18.473    0.000\n   .math7   (tht1)   35.172    1.904   18.473    0.000\n   .math8   (tht1)   35.172    1.904   18.473    0.000\n\n\nGroup 2 [1]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 =~                                            \n    math2             1.000                           \n    math3             1.000                           \n    math4             1.000                           \n    math5             1.000                           \n    math6             1.000                           \n    math7             1.000                           \n    math8             1.000                           \n  eta_2 =~                                            \n    math2             0.000                           \n    math3             1.000                           \n    math4             2.000                           \n    math5             3.000                           \n    math6             4.000                           \n    math7             5.000                           \n    math8             6.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 ~~                                            \n    eta_2             0.745    5.522    0.135    0.893\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1            32.800    1.407   23.314    0.000\n    eta_2             4.873    0.341   14.298    0.000\n   .math2             0.000                           \n   .math3             0.000                           \n   .math4             0.000                           \n   .math5             0.000                           \n   .math6             0.000                           \n   .math7             0.000                           \n   .math8             0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1            79.627   25.629    3.107    0.002\n    eta_2            -0.157    1.477   -0.106    0.915\n   .math2   (tht2)   48.686    8.444    5.766    0.000\n   .math3   (tht2)   48.686    8.444    5.766    0.000\n   .math4   (tht2)   48.686    8.444    5.766    0.000\n   .math5   (tht2)   48.686    8.444    5.766    0.000\n   .math6   (tht2)   48.686    8.444    5.766    0.000\n   .math7   (tht2)   48.686    8.444    5.766    0.000\n   .math8   (tht2)   48.686    8.444    5.766    0.000\n\n\n\nFacciamo un confronto tra la bontà di adattamento del modello M3 e del modello M4.\n\nlavTestLRT(mg_math_lavaan_fitM3, mg_math_lavaan_fitM4) |&gt;\n    print()\n\n\nChi-Squared Difference Test\n\n                     Df   AIC   BIC  Chisq Chisq diff    RMSEA Df diff\nmg_math_lavaan_fitM4 58 15950 16008 237.84                            \nmg_math_lavaan_fitM3 59 15952 16005 241.18     3.3457 0.070948       1\n                     Pr(&gt;Chisq)  \nmg_math_lavaan_fitM4             \nmg_math_lavaan_fitM3    0.06738 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnche in questo caso non otteniamo un risultato che fornisce evidenza di differenze tra i due gruppi.\nIn sintesi, possiamo dire che le evidenze presenti suggeriscono che i modelli di crescita latente per di due gruppi hanno parametri uguali per ciò che concerne le saturazioni fattoriali, le medie, le varianze, le covarianze e i residui.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>80</span>  <span class='chapter-title'>Modelli di crescita latenti a gruppi multipli</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/10_growth_groups.html#riflessioni-conclusive",
    "href": "chapters/lgm/10_growth_groups.html#riflessioni-conclusive",
    "title": "80  Modelli di crescita latenti a gruppi multipli",
    "section": "80.6 Riflessioni Conclusive",
    "text": "80.6 Riflessioni Conclusive\nL’approccio a gruppi multipli, utilizzato per studiare le differenze interpersonali nei cambiamenti intrapersonali, è estremamente efficace. Qui ci siamo concentrati sui modelli di crescita lineare per descrivere i cambiamenti intrapersonali e le differenze interpersonali in questi cambiamenti, ma è anche possibile considerare modelli non lineari più complessi. Per esempio, in alcune situazioni, un gruppo potrebbe seguire una traiettoria di crescita lineare (ad esempio, un gruppo di controllo), mentre un altro potrebbe seguire una crescita esponenziale (ad esempio, un gruppo di intervento).\nQuando consideriamo l’ipotesi che gruppi diversi di individui possano seguire traiettorie di cambiamento intrapersonale differenti, l’utilità del framework a gruppi multipli diventa ancora più evidente. Abbiamo presentato il framework a gruppi multipli come alternativa all’approccio con covarianti invarianti nel tempo, tuttavia i due approcci possono essere integrati. Come descritto nel capitolo precedente, i covarianti invarianti nel tempo sono utilizzati per spiegare le differenze interpersonali nell’intercetta e nella pendenza. Includendo i covarianti invarianti nel tempo nel framework a gruppi multipli, possiamo spiegare la variabilità nell’intercetta e nella pendenza all’interno di ciascun gruppo e testare se le relazioni tra i covarianti invarianti nel tempo e l’intercetta e la pendenza differiscono tra i gruppi.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>80</span>  <span class='chapter-title'>Modelli di crescita latenti a gruppi multipli</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/11_lgm_wais.html",
    "href": "chapters/lgm/11_lgm_wais.html",
    "title": "\n81  Lo Sviluppo dell’Intelligenza\n",
    "section": "",
    "text": "81.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nQuesta discussione riproduce il tutorial presentato nel Workshop on Latent Growth Modeling in Lavaan tenuto al Donders Institute nel novembre 2024. Questo tutorial riprende in un unico studio i concetti che avevamo esaminato nei capitoli precedenti. Verranno utilizzati dei dati longitudinali relativi al WISC-V forniti dagli autori a 6, 7, 9 e 11 anni.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>81</span>  <span class='chapter-title'>Lo Sviluppo dell'Intelligenza</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/11_lgm_wais.html#dati",
    "href": "chapters/lgm/11_lgm_wais.html#dati",
    "title": "\n81  Lo Sviluppo dell’Intelligenza\n",
    "section": "\n81.2 Dati",
    "text": "81.2 Dati\nIl WISC-V Test (Wechsler Intelligence Scale for Children) è un test del QI somministrato a bambini di età compresa tra 6 e 16 anni. Fornisce cinque punteggi indici principali, ovvero Indice di Comprensione Verbale, Indice Visuo-Spaziale, Indice di Ragionamento Fluido, Indice di Memoria di Lavoro e Indice di Velocità di Elaborazione. Nel workshop gli autori discutono su un sottoinsieme contenente: Indice di Comprensione Verbale, Indice di Velocità di Elaborazione e il totale.\n\nwisc &lt;- rio::import(\n  here::here(\n    \"data\", \"wisc.csv\"\n  )\n)[,-1]\n\nhead(wisc)         #first 6 rows\n#&gt;   ID Verbal_T6 Verbal_T7 Verbal_T9 Verbal_T11 Pspeed_T6 Pspeed_T7 Pspeed_T9\n#&gt; 1  0     24.42     26.98     39.61      55.64    19.836     22.97     43.90\n#&gt; 2  1     12.44     14.38     21.92      37.81     5.899     13.44     18.29\n#&gt; 3  2     32.43     33.51     34.30      50.18    27.638     45.02     46.99\n#&gt; 4  3     22.69     28.39     42.16      44.72    33.158     29.68     45.97\n#&gt; 5  4     28.23     37.81     41.06      70.95    27.638     44.42     65.48\n#&gt; 6  5     16.06     20.12     38.02      39.94     8.446     15.78     26.99\n#&gt;   Pspeed_T11 Total_6 Total_7 Total_9 Total_11 age_T6 sex race mo_edu\n#&gt; 1      44.19   22.13   24.97   41.76    49.91  5.833   1    1      4\n#&gt; 2      40.38    9.17   13.91   20.10    39.10  5.917   2    2      6\n#&gt; 3      77.72   30.03   39.27   40.65    63.95  6.333   1    1      2\n#&gt; 4      61.66   27.93   29.03   44.06    53.19  6.333   2    1      2\n#&gt; 5      64.22   27.93   41.12   53.27    67.59  6.167   1    1      3\n#&gt; 6      39.08   12.25   17.95   32.50    39.51  5.667   1    1      2\n#&gt;   mo_educat fa_edu fa_educat\n#&gt; 1         0      4         0\n#&gt; 2         0      5         0\n#&gt; 3         2      3         1\n#&gt; 4         2      2         2\n#&gt; 5         1      3         1\n#&gt; 6         2      2         2\n\n\ndim(wisc)          #number of rows and columns\n#&gt; [1] 204  20\n\nGli autori si concentrano sull’analisi dei dati del subtest verbale.\n\nwisc_verbal &lt;- wisc[,c(\"ID\",\"Verbal_T6\",\"Verbal_T7\",\"Verbal_T9\",\"Verbal_T11\")]\nglimpse(wisc_verbal)\n#&gt; Rows: 204\n#&gt; Columns: 5\n#&gt; $ ID         &lt;int&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 14, 15, 16, 17, 21, 2…\n#&gt; $ Verbal_T6  &lt;dbl&gt; 24.420, 12.440, 32.426, 22.693, 28.229, 16.057, 8.497, …\n#&gt; $ Verbal_T7  &lt;dbl&gt; 26.98, 14.38, 33.51, 28.39, 37.81, 20.12, 16.49, 20.92,…\n#&gt; $ Verbal_T9  &lt;dbl&gt; 39.61, 21.92, 34.30, 42.16, 41.06, 38.02, 28.71, 21.53,…\n#&gt; $ Verbal_T11 &lt;dbl&gt; 55.64, 37.81, 50.18, 44.72, 70.95, 39.94, 40.83, 25.68,…\n\nI dati vanno trasformati nel formato long.\n\nwisc_verbal_long &lt;- wisc_verbal %&gt;% \n  pivot_longer(!ID, names_to = \"wave\", values_to = \"verbal\")\n\nwisc_verbal_long |&gt; head()\n#&gt; # A tibble: 6 × 3\n#&gt;      ID wave       verbal\n#&gt;   &lt;int&gt; &lt;chr&gt;       &lt;dbl&gt;\n#&gt; 1     0 Verbal_T6    24.4\n#&gt; 2     0 Verbal_T7    27.0\n#&gt; 3     0 Verbal_T9    39.6\n#&gt; 4     0 Verbal_T11   55.6\n#&gt; 5     1 Verbal_T6    12.4\n#&gt; 6     1 Verbal_T7    14.4\n\nUn grafico dei dati si ottiene nel modo seguente.\n\nwisc_verbal_long$wave = factor(wisc_verbal_long$wave, levels=c(\"Verbal_T6\",\"Verbal_T7\",\"Verbal_T9\",\"Verbal_T11\"))\n\nggplot(wisc_verbal_long, aes(wave, verbal, group=ID, fill=ID, color=ID)) +\n  geom_point() + \n  geom_line() +\n  theme_classic(base_size = 15) + # adding a classic theme; https://ggplot2.tidyverse.org/reference/ggtheme.html\n  theme(legend.position = \"none\") + # getting rid of legend\n  labs(x = \"Wave\", y = \"Score on Verbal Subtest\")",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>81</span>  <span class='chapter-title'>Lo Sviluppo dell'Intelligenza</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/11_lgm_wais.html#modello-lineare",
    "href": "chapters/lgm/11_lgm_wais.html#modello-lineare",
    "title": "\n81  Lo Sviluppo dell’Intelligenza\n",
    "section": "\n81.3 Modello lineare",
    "text": "81.3 Modello lineare\nIl modello più semplice è quello di crescita lineare.\n\n# Create LGM\nlinear_growth_model &lt;- '\n  i =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11\n  s =~ 0*Verbal_T6 + 1*Verbal_T7 + 2*Verbal_T9 + 3*Verbal_T11'\n\nAdattiamo il modello ai dati ed esaminiamo i risultati.\n\n# Fit LGM\nfit_linear_growth_model &lt;- growth(linear_growth_model, data=wisc_verbal,missing='fiml')\n# Output results\nsummary(fit_linear_growth_model, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)\n#&gt; lavaan 0.6-19 ended normally after 65 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                         9\n#&gt; \n#&gt;   Number of observations                           204\n#&gt;   Number of missing patterns                         1\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                               100.756\n#&gt;   Degrees of freedom                                 5\n#&gt;   P-value (Chi-square)                           0.000\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               585.906\n#&gt;   Degrees of freedom                                 6\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.835\n#&gt;   Tucker-Lewis Index (TLI)                       0.802\n#&gt;                                                       \n#&gt;   Robust Comparative Fit Index (CFI)             0.835\n#&gt;   Robust Tucker-Lewis Index (TLI)                0.802\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -2530.194\n#&gt;   Loglikelihood unrestricted model (H1)      -2479.816\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                5078.388\n#&gt;   Bayesian (BIC)                              5108.251\n#&gt;   Sample-size adjusted Bayesian (SABIC)       5079.736\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.306\n#&gt;   90 Percent confidence interval - lower         0.256\n#&gt;   90 Percent confidence interval - upper         0.360\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.000\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    1.000\n#&gt;                                                       \n#&gt;   Robust RMSEA                                   0.306\n#&gt;   90 Percent confidence interval - lower         0.256\n#&gt;   90 Percent confidence interval - upper         0.360\n#&gt;   P-value H_0: Robust RMSEA &lt;= 0.050             0.000\n#&gt;   P-value H_0: Robust RMSEA &gt;= 0.080             1.000\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.113\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Observed\n#&gt;   Observed information based on                Hessian\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   i =~                                                                  \n#&gt;     Verbal_T6         1.000                               4.354    0.775\n#&gt;     Verbal_T7         1.000                               4.354    0.681\n#&gt;     Verbal_T9         1.000                               4.354    0.583\n#&gt;     Verbal_T11        1.000                               4.354    0.417\n#&gt;   s =~                                                                  \n#&gt;     Verbal_T6         0.000                               0.000    0.000\n#&gt;     Verbal_T7         1.000                               1.251    0.196\n#&gt;     Verbal_T9         2.000                               2.502    0.335\n#&gt;     Verbal_T11        3.000                               3.752    0.360\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   i ~~                                                                  \n#&gt;     s                 5.081    1.079    4.709    0.000    0.933    0.933\n#&gt; \n#&gt; Intercepts:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;     i                18.760    0.391   48.006    0.000    4.308    4.308\n#&gt;     s                 7.291    0.192   38.007    0.000    5.829    5.829\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .Verbal_T6        12.600    2.175    5.793    0.000   12.600    0.399\n#&gt;    .Verbal_T7        10.213    1.463    6.982    0.000   10.213    0.250\n#&gt;    .Verbal_T9        10.243    1.941    5.277    0.000   10.243    0.184\n#&gt;    .Verbal_T11       45.410    5.781    7.855    0.000   45.410    0.417\n#&gt;     i                18.961    3.154    6.012    0.000    1.000    1.000\n#&gt;     s                 1.565    0.658    2.379    0.017    1.000    1.000\n#&gt; \n#&gt; R-Square:\n#&gt;                    Estimate\n#&gt;     Verbal_T6         0.601\n#&gt;     Verbal_T7         0.750\n#&gt;     Verbal_T9         0.816\n#&gt;     Verbal_T11        0.583\n\nIl modello non si adatta bene ai dati.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>81</span>  <span class='chapter-title'>Lo Sviluppo dell'Intelligenza</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/11_lgm_wais.html#crescita-non-lineare",
    "href": "chapters/lgm/11_lgm_wais.html#crescita-non-lineare",
    "title": "\n81  Lo Sviluppo dell’Intelligenza\n",
    "section": "\n81.4 Crescita non lineare",
    "text": "81.4 Crescita non lineare\nNell’analisi precedente, abbiamo modellato un modello di crescita lineare. Tuttavia, è anche possibile modellare una crescita non lineare in lavaan come una traiettoria quadratica. Per fare ciò, è necessario aggiungere un terzo parametro chiamato termine quadratico che avrà gli stessi loadings del coefficiente angolare, ma al quadrato.\nPer fare questo, è necessario specificare un’altra variabile latente nel modello chiamata termine quadratico. Al termine quadratico vengono assegnati loadings che sono i quadrati dei loadings del coefficiente angolare.\n\n\n# Create quadratic growth model\nquad_growth_model &lt;- 'i =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11\n                      s =~ 0*Verbal_T6 + 1*Verbal_T7 + 2*Verbal_T9 + 3*Verbal_T11\n                      q =~ 0*Verbal_T6 + 1*Verbal_T7 + 4*Verbal_T9 + 9*Verbal_T11'\n# Fit model\nfit_quad_growth_model &lt;- growth(quad_growth_model, data=wisc_verbal,missing='fiml')\n# Output results\nsummary(fit_quad_growth_model, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)\n#&gt; lavaan 0.6-19 ended normally after 99 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        13\n#&gt; \n#&gt;   Number of observations                           204\n#&gt;   Number of missing patterns                         1\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                 6.176\n#&gt;   Degrees of freedom                                 1\n#&gt;   P-value (Chi-square)                           0.013\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               585.906\n#&gt;   Degrees of freedom                                 6\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.991\n#&gt;   Tucker-Lewis Index (TLI)                       0.946\n#&gt;                                                       \n#&gt;   Robust Comparative Fit Index (CFI)             0.991\n#&gt;   Robust Tucker-Lewis Index (TLI)                0.946\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -2482.904\n#&gt;   Loglikelihood unrestricted model (H1)      -2479.816\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                4991.808\n#&gt;   Bayesian (BIC)                              5034.943\n#&gt;   Sample-size adjusted Bayesian (SABIC)       4993.755\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.159\n#&gt;   90 Percent confidence interval - lower         0.059\n#&gt;   90 Percent confidence interval - upper         0.289\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.039\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.910\n#&gt;                                                       \n#&gt;   Robust RMSEA                                   0.159\n#&gt;   90 Percent confidence interval - lower         0.059\n#&gt;   90 Percent confidence interval - upper         0.289\n#&gt;   P-value H_0: Robust RMSEA &lt;= 0.050             0.039\n#&gt;   P-value H_0: Robust RMSEA &gt;= 0.080             0.910\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.023\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Observed\n#&gt;   Observed information based on                Hessian\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   i =~                                                                  \n#&gt;     Verbal_T6         1.000                               4.883    0.843\n#&gt;     Verbal_T7         1.000                               4.883    0.800\n#&gt;     Verbal_T9         1.000                               4.883    0.668\n#&gt;     Verbal_T11        1.000                               4.883    0.459\n#&gt;   s =~                                                                  \n#&gt;     Verbal_T6         0.000                                  NA       NA\n#&gt;     Verbal_T7         1.000                                  NA       NA\n#&gt;     Verbal_T9         2.000                                  NA       NA\n#&gt;     Verbal_T11        3.000                                  NA       NA\n#&gt;   q =~                                                                  \n#&gt;     Verbal_T6         0.000                                  NA       NA\n#&gt;     Verbal_T7         1.000                                  NA       NA\n#&gt;     Verbal_T9         4.000                                  NA       NA\n#&gt;     Verbal_T11        9.000                                  NA       NA\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   i ~~                                                                  \n#&gt;     s                -0.564    6.937   -0.081    0.935   -0.064   -0.064\n#&gt;     q                 2.014    1.811    1.112    0.266    0.738    0.738\n#&gt;   s ~~                                                                  \n#&gt;     q                 1.518    1.719    0.883    0.377    1.500    1.500\n#&gt; \n#&gt; Intercepts:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;     i                19.697    0.409   48.124    0.000    4.033    4.033\n#&gt;     s                 4.051    0.354   11.439    0.000       NA       NA\n#&gt;     q                 1.284    0.130    9.861    0.000       NA       NA\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .Verbal_T6         9.730    6.209    1.567    0.117    9.730    0.290\n#&gt;    .Verbal_T7        11.059    2.297    4.814    0.000   11.059    0.297\n#&gt;    .Verbal_T9         9.542    2.677    3.564    0.000    9.542    0.179\n#&gt;    .Verbal_T11       29.417   11.518    2.554    0.011   29.417    0.260\n#&gt;     i                23.848    6.558    3.636    0.000    1.000    1.000\n#&gt;     s                -3.277    7.053   -0.465    0.642       NA       NA\n#&gt;     q                -0.312    0.656   -0.476    0.634       NA       NA\n#&gt; \n#&gt; R-Square:\n#&gt;                    Estimate\n#&gt;     Verbal_T6         0.710\n#&gt;     Verbal_T7         0.703\n#&gt;     Verbal_T9         0.821\n#&gt;     Verbal_T11        0.740\n\nÈ anche possibile modellare una crescita non lineare in lavaan senza alcuna ipotesi sulla forma. Per farlo, si fissano i loadings della prima e dell’ultima misurazione, ma si stimano liberamente quelli intermedi.\n\n# Create non-linear growth model\nbasis_growth_model &lt;- 'i =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11\n                       s =~ 0*Verbal_T6 + Verbal_T7 + Verbal_T9 + 1*Verbal_T11'\n# Fit model\nfit_basis_growth_model &lt;- growth(basis_growth_model, data=wisc_verbal,missing='fiml')\n# Output results\nsummary(fit_basis_growth_model, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)\n#&gt; lavaan 0.6-19 ended normally after 109 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        11\n#&gt; \n#&gt;   Number of observations                           204\n#&gt;   Number of missing patterns                         1\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                 5.893\n#&gt;   Degrees of freedom                                 3\n#&gt;   P-value (Chi-square)                           0.117\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               585.906\n#&gt;   Degrees of freedom                                 6\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.995\n#&gt;   Tucker-Lewis Index (TLI)                       0.990\n#&gt;                                                       \n#&gt;   Robust Comparative Fit Index (CFI)             0.995\n#&gt;   Robust Tucker-Lewis Index (TLI)                0.990\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -2482.763\n#&gt;   Loglikelihood unrestricted model (H1)      -2479.816\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                4987.525\n#&gt;   Bayesian (BIC)                              5024.024\n#&gt;   Sample-size adjusted Bayesian (SABIC)       4989.173\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.069\n#&gt;   90 Percent confidence interval - lower         0.000\n#&gt;   90 Percent confidence interval - upper         0.151\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.275\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.491\n#&gt;                                                       \n#&gt;   Robust RMSEA                                   0.069\n#&gt;   90 Percent confidence interval - lower         0.000\n#&gt;   90 Percent confidence interval - upper         0.151\n#&gt;   P-value H_0: Robust RMSEA &lt;= 0.050             0.275\n#&gt;   P-value H_0: Robust RMSEA &gt;= 0.080             0.491\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.043\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Observed\n#&gt;   Observed information based on                Hessian\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   i =~                                                                  \n#&gt;     Verbal_T6         1.000                               4.610    0.825\n#&gt;     Verbal_T7         1.000                               4.610    0.731\n#&gt;     Verbal_T9         1.000                               4.610    0.620\n#&gt;     Verbal_T11        1.000                               4.610    0.446\n#&gt;   s =~                                                                  \n#&gt;     Verbal_T6         0.000                               0.000    0.000\n#&gt;     Verbal_T7         0.237    0.012   20.223    0.000    1.300    0.206\n#&gt;     Verbal_T9         0.536    0.012   43.295    0.000    2.937    0.395\n#&gt;     Verbal_T11        1.000                               5.484    0.531\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   i ~~                                                                  \n#&gt;     s                15.072    3.133    4.811    0.000    0.596    0.596\n#&gt; \n#&gt; Intercepts:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;     i                19.634    0.391   50.221    0.000    4.259    4.259\n#&gt;     s                24.180    0.565   42.773    0.000    4.409    4.409\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .Verbal_T6         9.974    1.645    6.063    0.000    9.974    0.319\n#&gt;    .Verbal_T7         9.704    1.307    7.422    0.000    9.704    0.244\n#&gt;    .Verbal_T9         9.217    1.513    6.093    0.000    9.217    0.167\n#&gt;    .Verbal_T11       25.340    4.317    5.870    0.000   25.340    0.237\n#&gt;     i                21.255    2.898    7.335    0.000    1.000    1.000\n#&gt;     s                30.076    6.788    4.430    0.000    1.000    1.000\n#&gt; \n#&gt; R-Square:\n#&gt;                    Estimate\n#&gt;     Verbal_T6         0.681\n#&gt;     Verbal_T7         0.756\n#&gt;     Verbal_T9         0.833\n#&gt;     Verbal_T11        0.763\n\n\n# Compare model fit\nanova(fit_linear_growth_model, fit_quad_growth_model)\n#&gt; \n#&gt; Chi-Squared Difference Test\n#&gt; \n#&gt;                         Df  AIC  BIC  Chisq Chisq diff RMSEA Df diff\n#&gt; fit_quad_growth_model    1 4992 5035   6.18                         \n#&gt; fit_linear_growth_model  5 5078 5108 100.76       94.6 0.333       4\n#&gt;                         Pr(&gt;Chisq)\n#&gt; fit_quad_growth_model             \n#&gt; fit_linear_growth_model     &lt;2e-16\n\nIl modello non lineare in lavaan senza alcuna ipotesi sulla forma e il modello quadratico non sono annidati. Pertanto un test del rapporto di verosimiglianza non è possibile. Tuttavia, gli indici di bontà di adattamento del modello senza ipotesi sulla forma sono migliori del modello quadratico, per cui sarà quello il modello prescelto.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>81</span>  <span class='chapter-title'>Lo Sviluppo dell'Intelligenza</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/11_lgm_wais.html#predizioni",
    "href": "chapters/lgm/11_lgm_wais.html#predizioni",
    "title": "\n81  Lo Sviluppo dell’Intelligenza\n",
    "section": "\n81.5 Predizioni",
    "text": "81.5 Predizioni\nSi potrebbe essere interessati a ciò che predice i punteggi di base e/o il cambiamento. Per valutare questo, si possono aggiungere predittori nel modello di crescita. Un’ipotesi potrebbe essere che il livello di istruzione della madre predica lo sviluppo della comprensione verbale.\n\n# Specify model\nbasis_growth_model_cov &lt;- ' \n  i =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11\n  s =~ 0*Verbal_T6 + Verbal_T7 + Verbal_T9 + 1*Verbal_T11 \n  s~mo_edu\n  i~mo_edu\n  '\n\n\n# Fit model\nfit_basis_growth_model_cov &lt;- growth(basis_growth_model_cov, data=wisc,missing='fiml')\n# Output results\nsummary(fit_basis_growth_model_cov, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)\n#&gt; lavaan 0.6-19 ended normally after 118 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        13\n#&gt; \n#&gt;   Number of observations                           204\n#&gt;   Number of missing patterns                         1\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                 6.498\n#&gt;   Degrees of freedom                                 5\n#&gt;   P-value (Chi-square)                           0.261\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               650.266\n#&gt;   Degrees of freedom                                10\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.998\n#&gt;   Tucker-Lewis Index (TLI)                       0.995\n#&gt;                                                       \n#&gt;   Robust Comparative Fit Index (CFI)             0.998\n#&gt;   Robust Tucker-Lewis Index (TLI)                0.995\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -2450.885\n#&gt;   Loglikelihood unrestricted model (H1)      -2447.636\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                4927.770\n#&gt;   Bayesian (BIC)                              4970.906\n#&gt;   Sample-size adjusted Bayesian (SABIC)       4929.718\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.038\n#&gt;   90 Percent confidence interval - lower         0.000\n#&gt;   90 Percent confidence interval - upper         0.110\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.520\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.210\n#&gt;                                                       \n#&gt;   Robust RMSEA                                   0.038\n#&gt;   90 Percent confidence interval - lower         0.000\n#&gt;   90 Percent confidence interval - upper         0.110\n#&gt;   P-value H_0: Robust RMSEA &lt;= 0.050             0.520\n#&gt;   P-value H_0: Robust RMSEA &gt;= 0.080             0.210\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.038\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Observed\n#&gt;   Observed information based on                Hessian\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   i =~                                                                  \n#&gt;     Verbal_T6         1.000                               4.615    0.826\n#&gt;     Verbal_T7         1.000                               4.615    0.732\n#&gt;     Verbal_T9         1.000                               4.615    0.619\n#&gt;     Verbal_T11        1.000                               4.615    0.447\n#&gt;   s =~                                                                  \n#&gt;     Verbal_T6         0.000                               0.000    0.000\n#&gt;     Verbal_T7         0.237    0.012   20.312    0.000    1.306    0.207\n#&gt;     Verbal_T9         0.535    0.012   43.171    0.000    2.949    0.396\n#&gt;     Verbal_T11        1.000                               5.508    0.534\n#&gt; \n#&gt; Regressions:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   s ~                                                                   \n#&gt;     mo_edu           -1.724    0.414   -4.165    0.000   -0.313   -0.394\n#&gt;   i ~                                                                   \n#&gt;     mo_edu           -1.943    0.259   -7.503    0.000   -0.421   -0.531\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;  .i ~~                                                                  \n#&gt;    .s                 9.676    2.721    3.556    0.000    0.489    0.489\n#&gt; \n#&gt; Intercepts:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .i                26.302    0.958   27.446    0.000    5.700    5.700\n#&gt;    .s                30.098    1.527   19.710    0.000    5.464    5.464\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .Verbal_T6         9.923    1.622    6.117    0.000    9.923    0.318\n#&gt;    .Verbal_T7         9.607    1.281    7.500    0.000    9.607    0.242\n#&gt;    .Verbal_T9         9.443    1.501    6.291    0.000    9.443    0.170\n#&gt;    .Verbal_T11       24.956    4.288    5.820    0.000   24.956    0.234\n#&gt;    .i                15.298    2.309    6.624    0.000    0.718    0.718\n#&gt;    .s                25.619    6.352    4.033    0.000    0.844    0.844\n#&gt; \n#&gt; R-Square:\n#&gt;                    Estimate\n#&gt;     Verbal_T6         0.682\n#&gt;     Verbal_T7         0.758\n#&gt;     Verbal_T9         0.830\n#&gt;     Verbal_T11        0.766\n#&gt;     i                 0.282\n#&gt;     s                 0.156\n\nI risultati indicano come il livello di educazione della madre influenza sia il valore di base delle abilità verbali del bambino, sia il tasso di crescita.\nAggiungiamo ora la velocità di elaborazione a 11 anni come esito dei cambiamenti nella comprensione verbale. In altre parole, verifichiamo se le pendenze del cambiamento verbale predicono il livello di velocità di elaborazione a 11.\n\n# Specify model\nbasis_growth_model_covO &lt;- ' \n  i =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11\n  s =~ 0*Verbal_T6 + Verbal_T7 + Verbal_T9 + 1*Verbal_T11 \n  Pspeed_T11~s\n  Pspeed_T11~1\n'\n\n# Fit model\nfit_basis_growth_model_covO &lt;- growth(basis_growth_model_covO, data=wisc,missing='fiml')\n# Output results\nsummary(fit_basis_growth_model_covO, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)\n#&gt; lavaan 0.6-19 ended normally after 142 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        14\n#&gt; \n#&gt;   Number of observations                           204\n#&gt;   Number of missing patterns                         1\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                14.016\n#&gt;   Degrees of freedom                                 6\n#&gt;   P-value (Chi-square)                           0.029\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               685.769\n#&gt;   Degrees of freedom                                10\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.988\n#&gt;   Tucker-Lewis Index (TLI)                       0.980\n#&gt;                                                       \n#&gt;   Robust Comparative Fit Index (CFI)             0.988\n#&gt;   Robust Tucker-Lewis Index (TLI)                0.980\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -3240.780\n#&gt;   Loglikelihood unrestricted model (H1)      -3233.772\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                6509.560\n#&gt;   Bayesian (BIC)                              6556.014\n#&gt;   Sample-size adjusted Bayesian (SABIC)       6511.657\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.081\n#&gt;   90 Percent confidence interval - lower         0.024\n#&gt;   90 Percent confidence interval - upper         0.137\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.151\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.566\n#&gt;                                                       \n#&gt;   Robust RMSEA                                   0.081\n#&gt;   90 Percent confidence interval - lower         0.024\n#&gt;   90 Percent confidence interval - upper         0.137\n#&gt;   P-value H_0: Robust RMSEA &lt;= 0.050             0.151\n#&gt;   P-value H_0: Robust RMSEA &gt;= 0.080             0.566\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.043\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Observed\n#&gt;   Observed information based on                Hessian\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   i =~                                                                  \n#&gt;     Verbal_T6         1.000                               4.476    0.804\n#&gt;     Verbal_T7         1.000                               4.476    0.714\n#&gt;     Verbal_T9         1.000                               4.476    0.598\n#&gt;     Verbal_T11        1.000                               4.476    0.434\n#&gt;   s =~                                                                  \n#&gt;     Verbal_T6         0.000                               0.000    0.000\n#&gt;     Verbal_T7         0.237    0.012   19.886    0.000    1.220    0.195\n#&gt;     Verbal_T9         0.534    0.013   42.125    0.000    2.752    0.367\n#&gt;     Verbal_T11        1.000                               5.157    0.500\n#&gt; \n#&gt; Regressions:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   Pspeed_T11 ~                                                          \n#&gt;     s                 1.683    0.219    7.690    0.000    8.680    0.697\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   i ~~                                                                  \n#&gt;     s                17.195    2.513    6.842    0.000    0.745    0.745\n#&gt; \n#&gt; Intercepts:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .Pspeed_T11       10.214    5.368    1.903    0.057   10.214    0.820\n#&gt;     i                19.648    0.390   50.431    0.000    4.389    4.389\n#&gt;     s                24.194    0.554   43.672    0.000    4.691    4.691\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .Verbal_T6        10.997    1.451    7.578    0.000   10.997    0.354\n#&gt;    .Verbal_T7         9.658    1.299    7.436    0.000    9.658    0.246\n#&gt;    .Verbal_T9        10.154    1.529    6.640    0.000   10.154    0.181\n#&gt;    .Verbal_T11       25.254    3.694    6.836    0.000   25.254    0.238\n#&gt;    .Pspeed_T11       79.657   10.996    7.244    0.000   79.657    0.514\n#&gt;     i                20.036    2.643    7.580    0.000    1.000    1.000\n#&gt;     s                26.598    5.701    4.666    0.000    1.000    1.000\n#&gt; \n#&gt; R-Square:\n#&gt;                    Estimate\n#&gt;     Verbal_T6         0.646\n#&gt;     Verbal_T7         0.754\n#&gt;     Verbal_T9         0.819\n#&gt;     Verbal_T11        0.762\n#&gt;     Pspeed_T11        0.486\n\nI dati mostrano come le pendenze del cambiamento verbale effettivamente predicono il livello di velocità di elaborazione a 11 anni.\nI predittori tempo-invarianti sono predittori delle differenze individuali nelle intercette e nelle pendenze. Sono spesso misurati al basale (ad esempio, reddito familiare) o sono caratteristiche specifiche della persona il cui valore è costante nel tempo (ad esempio, sesso biologico, paese di origine). Ad esempio, nelle analisi precedenti, il livello di istruzione della madre e la velocità di elaborazione a 6 anni sono predittori tempo-invarianti.\nI predittori tempo-varianti sono predittori dell’esito in ogni punto temporale. Nel nostro esempio, ad esempio, avremmo bisogno di misurazioni a T6, T7, T9 e T11.\nIn questo ultimo modello useremo la velocità di elaborazione come predittore tempo-variante della misurazione verbale in ogni punto temporale. Ci chiediamo le seguenti domande. Come sono l’intercetta e la pendenza delle misure verbali? La velocità di elaborazione predice le misure verbali allo stesso modo in tutti i punti temporali?\n\n# Specify model\nbasis_growth_model_tvp &lt;- ' \n  i =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11\n  s =~ 0*Verbal_T6 + Verbal_T7 + Verbal_T9 + 1*Verbal_T11 \n  Verbal_T6~Pspeed_T6\n  Verbal_T7~Pspeed_T7\n  Verbal_T9~Pspeed_T9\n  Verbal_T11~Pspeed_T11\n  '\n# Fit LGM\nfit_basis_growth_model_tvp &lt;- growth(basis_growth_model_tvp, data=wisc,missing='fiml')\n# Output results\nsummary(fit_basis_growth_model_tvp, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)\n#&gt; lavaan 0.6-19 ended normally after 96 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        15\n#&gt; \n#&gt;   Number of observations                           204\n#&gt;   Number of missing patterns                         1\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                90.277\n#&gt;   Degrees of freedom                                15\n#&gt;   P-value (Chi-square)                           0.000\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               754.793\n#&gt;   Degrees of freedom                                22\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.897\n#&gt;   Tucker-Lewis Index (TLI)                       0.849\n#&gt;                                                       \n#&gt;   Robust Comparative Fit Index (CFI)             0.897\n#&gt;   Robust Tucker-Lewis Index (TLI)                0.849\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -2440.511\n#&gt;   Loglikelihood unrestricted model (H1)      -2395.373\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                4911.022\n#&gt;   Bayesian (BIC)                              4960.794\n#&gt;   Sample-size adjusted Bayesian (SABIC)       4913.269\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.157\n#&gt;   90 Percent confidence interval - lower         0.127\n#&gt;   90 Percent confidence interval - upper         0.189\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.000\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    1.000\n#&gt;                                                       \n#&gt;   Robust RMSEA                                   0.157\n#&gt;   90 Percent confidence interval - lower         0.127\n#&gt;   90 Percent confidence interval - upper         0.189\n#&gt;   P-value H_0: Robust RMSEA &lt;= 0.050             0.000\n#&gt;   P-value H_0: Robust RMSEA &gt;= 0.080             1.000\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.194\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Observed\n#&gt;   Observed information based on                Hessian\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   i =~                                                                  \n#&gt;     Verbal_T6         1.000                               3.628    0.710\n#&gt;     Verbal_T7         1.000                               3.628    0.627\n#&gt;     Verbal_T9         1.000                               3.628    0.535\n#&gt;     Verbal_T11        1.000                               3.628    0.386\n#&gt;   s =~                                                                  \n#&gt;     Verbal_T6         0.000                               0.000    0.000\n#&gt;     Verbal_T7         0.297    0.071    4.209    0.000    1.337    0.231\n#&gt;     Verbal_T9         0.703    0.108    6.531    0.000    3.161    0.466\n#&gt;     Verbal_T11        1.000                               4.498    0.479\n#&gt; \n#&gt; Regressions:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   Verbal_T6 ~                                                           \n#&gt;     Pspeed_T6         0.243    0.038    6.467    0.000    0.243    0.397\n#&gt;   Verbal_T7 ~                                                           \n#&gt;     Pspeed_T7         0.230    0.034    6.853    0.000    0.230    0.397\n#&gt;   Verbal_T9 ~                                                           \n#&gt;     Pspeed_T9         0.220    0.036    6.130    0.000    0.220    0.332\n#&gt;   Verbal_T11 ~                                                          \n#&gt;     Pspeed_T11        0.319    0.039    8.233    0.000    0.319    0.423\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   i ~~                                                                  \n#&gt;     s                 5.873    2.723    2.157    0.031    0.360    0.360\n#&gt; \n#&gt; Intercepts:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;     i                15.271    0.754   20.245    0.000    4.209    4.209\n#&gt;     s                12.341    1.994    6.190    0.000    2.744    2.744\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .Verbal_T6         8.814    1.705    5.170    0.000    8.814    0.338\n#&gt;    .Verbal_T7         9.806    1.311    7.478    0.000    9.806    0.292\n#&gt;    .Verbal_T9         9.583    1.978    4.846    0.000    9.583    0.208\n#&gt;    .Verbal_T11       27.351    4.282    6.387    0.000   27.351    0.310\n#&gt;     i                13.165    2.347    5.609    0.000    1.000    1.000\n#&gt;     s                20.235    6.197    3.265    0.001    1.000    1.000\n#&gt; \n#&gt; R-Square:\n#&gt;                    Estimate\n#&gt;     Verbal_T6         0.662\n#&gt;     Verbal_T7         0.708\n#&gt;     Verbal_T9         0.792\n#&gt;     Verbal_T11        0.690",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>81</span>  <span class='chapter-title'>Lo Sviluppo dell'Intelligenza</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/11_lgm_wais.html#interazione-tra-pendenza-e-intercetta",
    "href": "chapters/lgm/11_lgm_wais.html#interazione-tra-pendenza-e-intercetta",
    "title": "\n81  Lo Sviluppo dell’Intelligenza\n",
    "section": "\n81.6 Interazione tra pendenza e intercetta",
    "text": "81.6 Interazione tra pendenza e intercetta\nOra che sappiamo come stimare la traiettoria di una variabile, siamo in grado di stimare la traiettoria di due variabili e vedere come interagiscono.\nNell’analisi successiva, creiamo due modelli di crescita non lineari, uno per la comprensione verbale e uno per la velocità di elaborazione. Correliamo i cambiamenti delle due metriche e ci chiediamo se loro pendenze sono correlate.\n\n# Specify model\nbasis_growth_model_cor_ver_pro &lt;- ' \n  i_verbal =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11\n  s_verbal =~ 0*Verbal_T6 + Verbal_T7 + Verbal_T9 + 1*Verbal_T11 \n  i_processpeed =~ 1*Pspeed_T6 + 1*Pspeed_T7 + 1*Pspeed_T9 + 1*Pspeed_T11\n  s_processpeed =~ 0*Pspeed_T6 + Pspeed_T7 + Pspeed_T9 + 1*Pspeed_T11 \n  s_verbal ~~ s_processpeed\n'\n\n# Fit LGM\nfit_basis_growth_model_cor_ver_pro &lt;- growth(basis_growth_model_cor_ver_pro, data=wisc,missing='fiml')\n# Output results\nsummary(fit_basis_growth_model_cor_ver_pro, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)\n#&gt; lavaan 0.6-19 ended normally after 211 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        26\n#&gt; \n#&gt;   Number of observations                           204\n#&gt;   Number of missing patterns                         1\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                29.305\n#&gt;   Degrees of freedom                                18\n#&gt;   P-value (Chi-square)                           0.045\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                              1423.083\n#&gt;   Degrees of freedom                                28\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.992\n#&gt;   Tucker-Lewis Index (TLI)                       0.987\n#&gt;                                                       \n#&gt;   Robust Comparative Fit Index (CFI)             0.992\n#&gt;   Robust Tucker-Lewis Index (TLI)                0.987\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -5124.285\n#&gt;   Loglikelihood unrestricted model (H1)      -5109.632\n#&gt;                                                       \n#&gt;   Akaike (AIC)                               10300.570\n#&gt;   Bayesian (BIC)                             10386.841\n#&gt;   Sample-size adjusted Bayesian (SABIC)      10304.465\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.055\n#&gt;   90 Percent confidence interval - lower         0.009\n#&gt;   90 Percent confidence interval - upper         0.091\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.367\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.137\n#&gt;                                                       \n#&gt;   Robust RMSEA                                   0.055\n#&gt;   90 Percent confidence interval - lower         0.009\n#&gt;   90 Percent confidence interval - upper         0.091\n#&gt;   P-value H_0: Robust RMSEA &lt;= 0.050             0.367\n#&gt;   P-value H_0: Robust RMSEA &gt;= 0.080             0.137\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.048\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Observed\n#&gt;   Observed information based on                Hessian\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   i_verbal =~                                                           \n#&gt;     Verbal_T6         1.000                               4.637    0.832\n#&gt;     Verbal_T7         1.000                               4.637    0.734\n#&gt;     Verbal_T9         1.000                               4.637    0.617\n#&gt;     Verbal_T11        1.000                               4.637    0.451\n#&gt;   s_verbal =~                                                           \n#&gt;     Verbal_T6         0.000                               0.000    0.000\n#&gt;     Verbal_T7         0.237    0.012   20.495    0.000    1.349    0.213\n#&gt;     Verbal_T9         0.533    0.012   43.283    0.000    3.037    0.404\n#&gt;     Verbal_T11        1.000                               5.694    0.554\n#&gt;   i_processpeed =~                                                      \n#&gt;     Pspeed_T6         1.000                               7.604    0.902\n#&gt;     Pspeed_T7         1.000                               7.604    0.799\n#&gt;     Pspeed_T9         1.000                               7.604    0.713\n#&gt;     Pspeed_T11        1.000                               7.604    0.617\n#&gt;   s_processpeed =~                                                      \n#&gt;     Pspeed_T6         0.000                               0.000    0.000\n#&gt;     Pspeed_T7         0.298    0.011   26.220    0.000    1.841    0.194\n#&gt;     Pspeed_T9         0.648    0.012   53.375    0.000    4.005    0.376\n#&gt;     Pspeed_T11        1.000                               6.183    0.502\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   s_verbal ~~                                                           \n#&gt;     s_processpeed    16.813    4.806    3.499    0.000    0.478    0.478\n#&gt;   i_verbal ~~                                                           \n#&gt;     s_verbal         14.606    3.112    4.694    0.000    0.553    0.553\n#&gt;     i_processpeed    26.537    3.572    7.430    0.000    0.753    0.753\n#&gt;     s_processpeed     2.520    3.154    0.799    0.424    0.088    0.088\n#&gt;   s_verbal ~~                                                           \n#&gt;     i_processpeed    25.796    4.875    5.291    0.000    0.596    0.596\n#&gt;   i_processpeed ~~                                                      \n#&gt;     s_processpeed    14.974    5.451    2.747    0.006    0.319    0.319\n#&gt; \n#&gt; Intercepts:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;     i_verbal         19.642    0.390   50.360    0.000    4.236    4.236\n#&gt;     s_verbal         24.200    0.561   43.138    0.000    4.250    4.250\n#&gt;     i_processpeed    17.949    0.590   30.419    0.000    2.360    2.360\n#&gt;     s_processpeed    32.986    0.615   53.609    0.000    5.335    5.335\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .Verbal_T6         9.574    1.581    6.055    0.000    9.574    0.308\n#&gt;    .Verbal_T7         9.698    1.269    7.643    0.000    9.698    0.243\n#&gt;    .Verbal_T9        10.149    1.491    6.806    0.000   10.149    0.180\n#&gt;    .Verbal_T11       22.419    4.039    5.551    0.000   22.419    0.212\n#&gt;    .Pspeed_T6        13.286    2.911    4.565    0.000   13.286    0.187\n#&gt;    .Pspeed_T7        20.338    2.534    8.026    0.000   20.338    0.225\n#&gt;    .Pspeed_T9        20.430    2.945    6.937    0.000   20.430    0.180\n#&gt;    .Pspeed_T11       25.840    5.200    4.969    0.000   25.840    0.170\n#&gt;     i_verbal         21.502    2.893    7.432    0.000    1.000    1.000\n#&gt;     s_verbal         32.425    6.990    4.639    0.000    1.000    1.000\n#&gt;     i_processpeed    57.821    6.889    8.393    0.000    1.000    1.000\n#&gt;     s_processpeed    38.226    8.968    4.263    0.000    1.000    1.000\n#&gt; \n#&gt; R-Square:\n#&gt;                    Estimate\n#&gt;     Verbal_T6         0.692\n#&gt;     Verbal_T7         0.757\n#&gt;     Verbal_T9         0.820\n#&gt;     Verbal_T11        0.788\n#&gt;     Pspeed_T6         0.813\n#&gt;     Pspeed_T7         0.775\n#&gt;     Pspeed_T9         0.820\n#&gt;     Pspeed_T11        0.830\n\n\n# Specify model\nbasis_growth_model_pred_ver_pro &lt;- ' \n  i_verbal =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11\n  s_verbal =~ 0*Verbal_T6 + Verbal_T7 + Verbal_T9 + 1*Verbal_T11 \n  i_processpeed =~ 1*Pspeed_T6 + 1*Pspeed_T7 + 1*Pspeed_T9 + 1*Pspeed_T11\n  s_processpeed =~ 0*Pspeed_T6 + Pspeed_T7 + Pspeed_T9 + 1*Pspeed_T11 \n  s_verbal ~ i_processpeed\n  s_processpeed ~ i_verbal'\n\n# Fit LGM\nfit_basis_growth_model_pred_ver_pro &lt;- growth(basis_growth_model_pred_ver_pro, data=wisc,missing='fiml')\n# Output results\nsummary(fit_basis_growth_model_pred_ver_pro, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)\n#&gt; lavaan 0.6-19 ended normally after 175 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        24\n#&gt; \n#&gt;   Number of observations                           204\n#&gt;   Number of missing patterns                         1\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                38.158\n#&gt;   Degrees of freedom                                20\n#&gt;   P-value (Chi-square)                           0.008\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                              1423.083\n#&gt;   Degrees of freedom                                28\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.987\n#&gt;   Tucker-Lewis Index (TLI)                       0.982\n#&gt;                                                       \n#&gt;   Robust Comparative Fit Index (CFI)             0.987\n#&gt;   Robust Tucker-Lewis Index (TLI)                0.982\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -5128.711\n#&gt;   Loglikelihood unrestricted model (H1)      -5109.632\n#&gt;                                                       \n#&gt;   Akaike (AIC)                               10305.422\n#&gt;   Bayesian (BIC)                             10385.057\n#&gt;   Sample-size adjusted Bayesian (SABIC)      10309.018\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.067\n#&gt;   90 Percent confidence interval - lower         0.033\n#&gt;   90 Percent confidence interval - upper         0.099\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.181\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.268\n#&gt;                                                       \n#&gt;   Robust RMSEA                                   0.067\n#&gt;   90 Percent confidence interval - lower         0.033\n#&gt;   90 Percent confidence interval - upper         0.099\n#&gt;   P-value H_0: Robust RMSEA &lt;= 0.050             0.181\n#&gt;   P-value H_0: Robust RMSEA &gt;= 0.080             0.268\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.055\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Observed\n#&gt;   Observed information based on                Hessian\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   i_verbal =~                                                           \n#&gt;     Verbal_T6         1.000                               4.809    0.856\n#&gt;     Verbal_T7         1.000                               4.809    0.760\n#&gt;     Verbal_T9         1.000                               4.809    0.647\n#&gt;     Verbal_T11        1.000                               4.809    0.477\n#&gt;   s_verbal =~                                                           \n#&gt;     Verbal_T6         0.000                               0.000    0.000\n#&gt;     Verbal_T7         0.238    0.011   21.065    0.000    1.421    0.225\n#&gt;     Verbal_T9         0.534    0.012   43.708    0.000    3.189    0.429\n#&gt;     Verbal_T11        1.000                               5.977    0.593\n#&gt;   i_processpeed =~                                                      \n#&gt;     Pspeed_T6         1.000                               7.861    0.929\n#&gt;     Pspeed_T7         1.000                               7.861    0.831\n#&gt;     Pspeed_T9         1.000                               7.861    0.756\n#&gt;     Pspeed_T11        1.000                               7.861    0.662\n#&gt;   s_processpeed =~                                                      \n#&gt;     Pspeed_T6         0.000                               0.000    0.000\n#&gt;     Pspeed_T7         0.299    0.011   26.953    0.000    2.078    0.220\n#&gt;     Pspeed_T9         0.648    0.012   53.898    0.000    4.495    0.432\n#&gt;     Pspeed_T11        1.000                               6.940    0.585\n#&gt; \n#&gt; Regressions:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   s_verbal ~                                                            \n#&gt;     i_processpeed     0.408    0.071    5.782    0.000    0.537    0.537\n#&gt;   s_processpeed ~                                                       \n#&gt;     i_verbal          0.143    0.142    1.013    0.311    0.099    0.099\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   i_verbal ~~                                                           \n#&gt;     i_processpeed    26.674    3.649    7.310    0.000    0.706    0.706\n#&gt;  .s_verbal ~~                                                           \n#&gt;    .s_processpeed    10.954    5.052    2.168    0.030    0.315    0.315\n#&gt; \n#&gt; Intercepts:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;     i_verbal         19.630    0.393   49.926    0.000    4.082    4.082\n#&gt;    .s_verbal         16.877    1.368   12.333    0.000    2.823    2.823\n#&gt;     i_processpeed    17.944    0.592   30.305    0.000    2.283    2.283\n#&gt;    .s_processpeed    30.167    2.843   10.612    0.000    4.347    4.347\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .Verbal_T6         8.445    1.407    6.000    0.000    8.445    0.267\n#&gt;    .Verbal_T7         9.715    1.287    7.547    0.000    9.715    0.243\n#&gt;    .Verbal_T9        10.375    1.493    6.951    0.000   10.375    0.188\n#&gt;    .Verbal_T11       21.001    3.922    5.355    0.000   21.001    0.207\n#&gt;    .Pspeed_T6         9.777    2.479    3.943    0.000    9.777    0.137\n#&gt;    .Pspeed_T7        21.117    2.640    8.000    0.000   21.117    0.236\n#&gt;    .Pspeed_T9        21.159    3.017    7.014    0.000   21.159    0.196\n#&gt;    .Pspeed_T11       23.241    5.106    4.552    0.000   23.241    0.165\n#&gt;     i_verbal         23.129    2.847    8.125    0.000    1.000    1.000\n#&gt;    .s_verbal         25.426    5.629    4.517    0.000    0.712    0.712\n#&gt;     i_processpeed    61.801    6.889    8.971    0.000    1.000    1.000\n#&gt;    .s_processpeed    47.685    8.194    5.820    0.000    0.990    0.990\n#&gt; \n#&gt; R-Square:\n#&gt;                    Estimate\n#&gt;     Verbal_T6         0.733\n#&gt;     Verbal_T7         0.757\n#&gt;     Verbal_T9         0.812\n#&gt;     Verbal_T11        0.793\n#&gt;     Pspeed_T6         0.863\n#&gt;     Pspeed_T7         0.764\n#&gt;     Pspeed_T9         0.804\n#&gt;     Pspeed_T11        0.835\n#&gt;     s_verbal          0.288\n#&gt;     s_processpeed     0.010",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>81</span>  <span class='chapter-title'>Lo Sviluppo dell'Intelligenza</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/12_temp_reliability.html",
    "href": "chapters/lgm/12_temp_reliability.html",
    "title": "82  Affidabilità longitudinale",
    "section": "",
    "text": "82.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nNel capitolo (interrater-reliability?), abbiamo discusso il calcolo dell’affidabilità delle misure in disegni longitudinali utilizzando la teoria della generalizzabilità. In questo capitolo, affronteremo lo stesso problema attraverso i modelli di equazioni strutturali (SEM).\nNegli ultimi anni, i progressi tecnologici hanno trasformato i metodi di raccolta dei dati longitudinali intensivi, consentendo la raccolta di informazioni in modo meno invasivo e riducendo le difficoltà per i partecipanti. Tradizionalmente, i dati longitudinali venivano raccolti con un numero limitato di misurazioni ripetute e intervalli di tempo lunghi tra una misurazione e l’altra. Oggi, invece, è possibile ottenere dati con un numero elevato di misurazioni ravvicinate nel tempo, grazie all’uso di strumenti come applicazioni per smartphone e tablet. Questi dati longitudinali intensivi permettono di esaminare la dinamica di processi psicologici che variano nel tempo, come i cambiamenti giornalieri negli stati psicologici.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>82</span>  <span class='chapter-title'>Affidabilità longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/12_temp_reliability.html#la-struttura-annidata-dei-dati-longitudinali",
    "href": "chapters/lgm/12_temp_reliability.html#la-struttura-annidata-dei-dati-longitudinali",
    "title": "82  Affidabilità longitudinale",
    "section": "82.2 La Struttura Annidata dei Dati Longitudinali",
    "text": "82.2 La Struttura Annidata dei Dati Longitudinali\nI dati raccolti con misure quotidiane presentano una struttura annidata, poiché le varie occasioni di misurazione sono raggruppate all’interno dello stesso individuo. Per analizzare l’affidabilità in questo contesto, si ricorre comunemente a due approcci: la teoria della generalizzabilità e l’approccio fattoriale.\n\nTeoria della Generalizzabilità: Questo approccio scompone la varianza totale in componenti di tempo, item e persona, permettendo di valutare l’affidabilità dei cambiamenti nel tempo a livello individuale. Tuttavia, la teoria della generalizzabilità si basa su alcune assunzioni che possono non adattarsi completamente ai dati raccolti.\nApproccio Fattoriale: Questo metodo è più flessibile e consente di modellare le associazioni tra gli item e il punteggio vero, oltre a gestire le varianze degli errori. Nelle sezioni precedenti abbiamo esaminato come l’analisi fattoriale confermativa multilivello (MCFA) possa stimare l’ICC delle singole variabili in contesti con dati annidati (cioè con misurazioni multiple per lo stesso partecipante). Ora, utilizzeremo la MCFA per determinare l’affidabilità sia a livello intra-individuale che inter-individuale nei casi di misure ripetute nel tempo.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>82</span>  <span class='chapter-title'>Affidabilità longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/12_temp_reliability.html#studio-di-caso-affidabilità-con-dati-longitudinali-intensivi",
    "href": "chapters/lgm/12_temp_reliability.html#studio-di-caso-affidabilità-con-dati-longitudinali-intensivi",
    "title": "82  Affidabilità longitudinale",
    "section": "82.3 Studio di Caso: Affidabilità con Dati Longitudinali Intensivi",
    "text": "82.3 Studio di Caso: Affidabilità con Dati Longitudinali Intensivi\nPer guidare la discussione, ci baseremo sull’articolo di Alphen et al. (2022), che offre un tutorial per valutare l’affidabilità dei dati longitudinali intensivi raccolti quotidianamente. In questo studio, gli autori utilizzano dati empirici relativi al livello di stress lavorativo giornaliero tra insegnanti di scuola secondaria, mostrando come calcolare l’affidabilità tramite MCFA e confrontandola con gli indici di affidabilità derivati dalla teoria della generalizzabilità.\nGrazie a questa comparazione, è possibile comprendere i vantaggi e le differenze tra i due approcci, evidenziando come l’analisi fattoriale multilivello consenta una rappresentazione più dettagliata delle variazioni intra-individuali e inter-individuali nei dati longitudinali.\n\n82.3.1 Affidabilità nei Modelli Fattoriali a Livello Singolo\nIn psicologia, la Confermatory Factor Analysis (CFA) è ormai lo standard per valutare la dimensionalità e l’affidabilità dei punteggi. Quando si lavora con dati a livello singolo, l’affidabilità può essere misurata attraverso diversi indici. Tra questi, l’indice \\(\\omega\\) offre un’alternativa al tradizionale coefficiente di consistenza interna \\(\\alpha\\), poiché non richiede che i carichi fattoriali degli item contribuiscano in egual misura al costrutto latente.\nI valori di \\(\\omega\\) spaziano tra zero e uno, con valori prossimi a uno che indicano una maggiore affidabilità della scala. Questo indice rappresenta la proporzione di varianza nei punteggi della scala spiegata dal fattore latente comune a tutti gli indicatori.\n\n\n82.3.2 Definizione dell’Affidabilità Composita \\(\\omega\\)\nL’affidabilità composita \\(\\omega\\) per un costrutto misurato con \\(p\\) item è calcolata come segue:\n\\[\n\\omega = \\frac{\\sum_{i=1}^{p} \\lambda_i^2 \\Phi}{\\sum_{i=1}^{p} \\lambda_i^2 \\Phi + \\sum_{i=1}^{p} \\theta_i},\n\\]\ndove:\n\n\\(i\\) indica ciascun item,\n\\(\\lambda\\) rappresenta il carico fattoriale dell’item sul costrutto latente,\n\\(\\Phi\\) è la varianza del fattore latente,\n\\(\\theta\\) è la varianza residua dell’item.\n\nQuesto indice considera i diversi contributi degli item al costrutto latente, fornendo una stima dell’affidabilità che riflette meglio la struttura fattoriale del punteggio rispetto agli approcci che assumono contributi uniformi.\nPer chiarire, Alphen et al. (2022) propongono un esempio concreto. Supponiamo di avere un modello a singolo fattore, in cui la varianza del fattore è fissata a 1 per l’identificazione del modello, e i carichi fattoriali sui tre indicatori sono pari a 0.7, 0.8 e 0.9. Di conseguenza, le specificità degli item saranno rispettivamente 0.51, 0.36 e 0.19.\nInserendo questi valori nell’equazione per \\(\\omega\\), otteniamo un’affidabilità composita della scala pari a 0.84:\n\\[\n\\omega = \\frac{\\left(0.70 + 0.80 + 0.90\\right)^{2} \\cdot 1}{\\left(0.70 + 0.80 + 0.90\\right)^{2} \\cdot 1 + \\left(0.51 + 0.36 + 0.19\\right)} = 0.84.\n\\]\nQuesto risultato indica che l’84% della varianza totale nei punteggi della scala è attribuibile al fattore comune, il che riflette un’elevata affidabilità della misura.\n\n\n82.3.3 Affidabilità nei Modelli Fattoriali Multilivello\nIn psicologia, i dati spesso presentano una struttura annidata, in cui le unità a un livello inferiore sono raggruppate in unità di livello superiore. Ad esempio, gli studenti sono annidati nelle classi, e i pazienti negli ospedali. Con misurazioni ripetute sugli stessi individui, come nei dati raccolti giornalmente, le occasioni di misurazione sono annidate negli individui. Nel caso illustrato qui, i dati empirici provengono da misurazioni ripetute su insegnanti durante 15 occasioni di raccolta, creando una struttura annidata in cui le occasioni sono raggruppate per ogni insegnante.\nL’analisi fattoriale multilivello consente di rappresentare varianze e covarianze distinte per le differenze intra-individuali e inter-individuali (Muthén, 1994). Nell’esempio discusso, Alphen et al. (2022) si focalizzano su strutture a due livelli: le occasioni di misurazione (Livello 1, o livello intra-individuale) e gli individui (Livello 2, o livello inter-individuale).\nLa Figura 82.1 illustra un modello fattoriale multilivello. In una confermatory factor analysis (CFA) a due livelli, i punteggi degli item vengono suddivisi in componenti latenti intra- e inter-individuali. La componente inter-individuale modella la struttura di covarianza tra individui, spiegando le differenze tra di essi e fornendo un’interpretazione simile a quella di una CFA a livello singolo. La componente intra-individuale modella la covarianza tra le misurazioni ripetute per ciascun individuo, riflettendo le variazioni all’interno degli individui nei diversi momenti temporali. In questo contesto, il livello intra-individuale rappresenta caratteristiche di stato (condizioni fluttuanti nel tempo), mentre il livello inter-individuale riflette caratteristiche di tratto, offrendo una misura aggregata più stabile nel tempo, simile a un’indicazione della personalità.\n\n\n\n\n\n\nFigura 82.1: Un modello configurale multilivello con i carichi fattoriali, varianze residuali e varianza del fattoriali dell’esempio discusso da Alphen et al. (2022). (Figura tratta da Alphen et al., 2022)\n\n\n\nGeldhof et al. (2014) hanno ampliato il metodo per calcolare \\(\\omega\\) adattandolo ai modelli a due livelli, ottenendo così indici di affidabilità distinti per il livello intra-individuale (\\(\\omega_w\\)) e inter-individuale (\\(\\omega_b\\)). Questo approccio è stato successivamente sviluppato ulteriormente da Lai (2021).",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>82</span>  <span class='chapter-title'>Affidabilità longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/12_temp_reliability.html#calcolo-dellaffidabilità-della-scala-con-dati-giornalieri",
    "href": "chapters/lgm/12_temp_reliability.html#calcolo-dellaffidabilità-della-scala-con-dati-giornalieri",
    "title": "82  Affidabilità longitudinale",
    "section": "82.4 Calcolo dell’Affidabilità della Scala con Dati Giornalieri",
    "text": "82.4 Calcolo dell’Affidabilità della Scala con Dati Giornalieri\nAlphen et al. (2022) illustrano l’analisi dell’affidabilità utilizzando un dataset con misure longitudinali intensive giornaliere sullo stress degli insegnanti. In questo contesto, il fattore comune a livello esterno può essere interpretato come la componente stabile dello stress, mentre il fattore a livello interno rappresenta la variabilità dello stress nel tempo. Quando si modellano componenti interne ed esterne dello stesso fattore, il modello fattoriale multilivello prende il nome di modello configurale (Stapleton et al., 2016).\nNel modello configurale multilivello, i fattori a livello interno ed esterno riflettono componenti diverse della stessa variabile latente, con una struttura fattoriale identica per entrambi i livelli e carichi fattoriali uguali (Asparouhov & Muthen, 2012). Lai (2021) ha fornito le formule per calcolare gli indici di affidabilità a livello intra-individuale (\\(\\omega_w\\)) e inter-individuale (\\(\\omega_b\\)) in questi modelli configurali. Alphen et al. (2022) illustrano come calcolare questi indici di affidabilità.\n\n82.4.1 Affidabilità a Livello Intra-Individuale\nPer determinare l’affidabilità a livello intra-individuale, utilizziamo la seguente formula:\n\\[\n\\omega_w = \\frac{\\sum (\\lambda_i^2 \\Phi_w)}{\\sum (\\lambda_i^2 \\Phi_w) + \\sum (\\theta_w)},\n\\]\ndove il pedice \\(w\\) si riferisce al livello intra-individuale. I carichi fattoriali (\\(\\lambda\\)) non hanno un pedice di livello specifico poiché sono vincolati ad essere identici a entrambi i livelli. In questo contesto, \\(\\Phi_w\\) rappresenta la varianza del fattore a livello intra-individuale e \\(\\theta_w\\) rappresenta la varianza residua al livello interno.\nInserendo i valori di esempio dalla Figura 82.1, otteniamo un’affidabilità intra-individuale di 0.84:\n\\[\n\\omega_w = \\frac{(0.70 + 0.80 + 0.90)^2}{(0.70 + 0.80 + 0.90)^2 + (0.51 + 0.36 + 0.19)} = 0.84.\n\\]\nQuesto valore indica che il fattore comune a livello interno spiega l’84% della varianza nelle deviazioni a livello intra-individuale nei punteggi della scala.\n\n\n82.4.2 Affidabilità a Livello Inter-Individuale\nPer il calcolo dell’affidabilità a livello inter-individuale, l’equazione è la seguente:\n\\[\n\\omega_b = \\frac{\\sum (\\lambda_i^2 \\Phi_b)}{\\sum (\\lambda_i^2 (\\Phi_b + \\Phi_w/n)) + \\sum (\\theta_b + \\theta_w/n)},\n\\]\ndove \\(n\\) è il numero di occasioni di misurazione (in questo caso, 15). La presenza di \\(\\Phi_w/n\\) e \\(\\theta_w/n\\) tiene conto della varianza dell’errore di campionamento delle medie osservate a livello di persona.\nInserendo i valori di esempio di Alphen et al. (2022) e impostando \\(n = 15\\), otteniamo un’affidabilità inter-individuale di 0.90:\n\\[\n\\omega_b = \\frac{(0.70 + 0.80 + 0.90)^2}{\n    (0.70 + 0.80 + 0.90)^2(0.90 + \\frac{1}{15}) + (0.05 + 0.05 + 0.05) \\\\\n    + \\frac{(0.51 + 0.36 + 0.19)}{15}\n} = 0.90\n\\]\nQuesto valore indica che il fattore comune a livello esterno spiega il 90% della varianza nelle medie osservate a livello di persona.\n\n\n82.4.3 Interpretazione degli Indici di Affidabilità\nQueste formule permettono di calcolare l’affidabilità delle componenti intra- e inter-individuali di una scala in studi longitudinali intensivi. I risultati consentono ai ricercatori di distinguere tra variazioni stabili (livello inter-individuale) e temporanee (livello intra-individuale), offrendo una visione dettagliata della dinamica dei fenomeni psicologici misurati.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>82</span>  <span class='chapter-title'>Affidabilità longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/12_temp_reliability.html#confronto-con-la-teoria-della-generalizzabilità",
    "href": "chapters/lgm/12_temp_reliability.html#confronto-con-la-teoria-della-generalizzabilità",
    "title": "82  Affidabilità longitudinale",
    "section": "82.5 Confronto con la Teoria della Generalizzabilità",
    "text": "82.5 Confronto con la Teoria della Generalizzabilità\nAlphen et al. (2022) hanno anche derivato le componenti di varianza per il calcolo del punteggio di affidabilità a livello interno e a livello esterno utilizzando la teoria della generalizzabilità. Per questi dati, Alphen et al. (2022) trovano che la stima dell’affidabilità a livello interno è .87, molto simile alla stima ottenuta con l’approccio CFA multilivello. Tuttavia, la stima a livello esterno ottenuta con il metodo della generalizzabilità è 0.99, che è .11 più alta rispetto all’approccio analitico fattoriale. Questa differenza potrebbe essere causata dalle assunzioni più rigide fatte dal metodo della teoria della generalizzabilità. Tuttavia, Alphen et al. (2022) notano che questi risultati sono specifici al dataset utilizzato e sarebbe necessario uno studio di simulazione per valutare, in generale, quali sono le differenze sistematiche tra le stime di affidabilità ottenute con i due diversi metodi.\nQui sotto viene presentato il metodo SEM per il calcolo dell’affidabilità inter- e intra-persona usando gli script R forniti da Alphen et al. (2022).",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>82</span>  <span class='chapter-title'>Affidabilità longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/12_temp_reliability.html#metodo-a-cinque-passi",
    "href": "chapters/lgm/12_temp_reliability.html#metodo-a-cinque-passi",
    "title": "82  Affidabilità longitudinale",
    "section": "82.6 Metodo a Cinque Passi",
    "text": "82.6 Metodo a Cinque Passi\nPer studiare l’affidabilità delle misure longitudinali intensive, Alphen et al. (2022) propongono una procedura in cinque passi. Questi passaggi sono stati ideati per evitare bias e problemi di specificazione del modello e comprendono:\n\nIspezione delle Correlazioni Intraclasse: Questo primo passo verifica la proporzione di varianza attribuibile alle differenze tra i gruppi, utile per comprendere la struttura annidata dei dati.\nVerifica della Varianza e Covarianza al Livello tra-Persone: Si testa la presenza di varianza e covarianza significative tra le persone per valutare la necessità di un approccio multilivello, dove le differenze tra persone giocano un ruolo importante.\nSpecifica Progressiva del Modello di Misura a ciascun Livello: Il modello di misura viene definito gradualmente per ciascun livello, assicurando che la struttura del modello rappresenti adeguatamente le relazioni tra variabili.\nVerifica della Invarianza di Misura tra Livelli: Si testa l’invarianza della misura per assicurarsi che la struttura del modello sia simile nei vari livelli, il che è fondamentale per confrontare interpretazioni tra livelli.\nCalcolo degli Indici di Affidabilità a Livello Intra-Personale (ωw) e Inter-Personale (ωb): Infine, si calcolano gli indici di affidabilità a livello intra-personale (ωw) e inter-personale (ωb) per quantificare la stabilità delle misure rispettivamente entro e tra persone.\n\nQuesta procedura strutturata permette di valutare l’affidabilità delle misure in studi longitudinali intensivi, garantendo che il modello rispetti le caratteristiche dei dati e fornisca stime affidabili di variabilità e stabilità a livello intra- e inter-personale.\nIniziamo ad importare i dati dell’esempio di Alphen et al. (2022).\n\nvan_alphen &lt;- read.table(\"../../data/van_alphen.dat\", na.strings = \"9999\")\ncolnames(van_alphen) &lt;- c(\"day\", \"school\", \"ID\", \"str1\", \"str2\", \"str3\", \"str4\")\nvan_alphen |&gt; head()\n\n\nA data.frame: 6 x 7\n\n\n\nday\nschool\nID\nstr1\nstr2\nstr3\nstr4\n\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n1\n1\n1\n26\nNA\n24\n78\n\n\n2\n1\n1\n30\nNA\n24\n24\n50\n\n\n3\n1\n1\n55\nNA\n36\n70\n72\n\n\n4\n1\n1\n92\nNA\n37\n34\n41\n\n\n5\n1\n2\n20\n24\nNA\n24\n36\n\n\n6\n1\n2\n22\n12\nNA\n18\n39\n\n\n\n\n\n\nvan_alphen |&gt; tail()\n\n\nA data.frame: 6 x 7\n\n\n\nday\nschool\nID\nstr1\nstr2\nstr3\nstr4\n\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1264\n15\n6\n62\nNA\n66\n57\n61\n\n\n1265\n15\n6\n87\nNA\n42\n59\n83\n\n\n1266\n15\n6\n115\n53\nNA\n53\n45\n\n\n1267\n15\n6\n118\nNA\n16\n32\n16\n\n\n1268\n15\n6\n123\n23\n22\n23\nNA\n\n\n1269\n15\n6\n143\n64\n64\n65\nNA\n\n\n\n\n\nEseguiamo una procedura di imputazione multipla per gestire il problema dei dati mancanti. In Mplus, questo passaggio può essere eseguito direttamente durante la procedura di fit, ma in R non è possibile. Pertanto, utilizziamo missRanger per imputare i dati prima di adattare il modello.\n\nimp &lt;- missRanger(van_alphen, num.trees = 100)\n\n\nVariables to impute:        str3, str4, str1, str2\nVariables used to impute:   day, school, ID, str1, str2, str3, str4\n\niter 1 \n  |============================================================| 100%\niter 2 \n  |============================================================| 100%\niter 3 \n  |============================================================| 100%\niter 4 \n  |============================================================| 100%\niter 5 \n  |============================================================| 100%\niter 6 \n  |============================================================| 100%\n\n\n\nimp |&gt; summary()\n\n     day                school            ID              str1       \n Length:1269        Min.   :1.000   Min.   :  1.00   Min.   :  0.00  \n Class :character   1st Qu.:2.000   1st Qu.: 43.00   1st Qu.:  7.00  \n Mode  :character   Median :4.000   Median : 81.00   Median : 28.09  \n                    Mean   :3.779   Mean   : 78.53   Mean   : 33.63  \n                    3rd Qu.:5.000   3rd Qu.:115.00   3rd Qu.: 57.05  \n                    Max.   :6.000   Max.   :151.00   Max.   :100.00  \n      str2               str3             str4       \n Min.   :  0.0000   Min.   :  0.00   Min.   :  0.00  \n 1st Qu.:  0.7818   1st Qu.:  7.00   1st Qu.:  7.00  \n Median : 21.0000   Median : 30.00   Median : 34.00  \n Mean   : 26.6310   Mean   : 34.71   Mean   : 34.99  \n 3rd Qu.: 45.0000   3rd Qu.: 61.00   3rd Qu.: 59.00  \n Max.   :100.0000   Max.   :100.00   Max.   :100.00  \n\n\n\n82.6.1 Passo 1: Ispezione delle Correlazioni Intraclasse\nIl modello multilivello è appropriato se una quota rilevante della varianza può essere attribuita al livello tra-persone. Il coefficiente di correlazione intraclasse (ICC) di una variabile permette di quantificare l’entità di questa proporzione (Snijders & Bosker, 2012). Pertanto, il primo passo consiste nel verificare se è presente una varianza significativa al livello tra-persone attraverso l’ispezione dell’ICC, calcolato come:\n\\[\n\\text{ICC} = \\frac{\\sigma_b}{\\sigma_b + \\sigma_w},\n\\]\ndove \\(\\sigma_b\\) e \\(\\sigma_w\\) rappresentano, rispettivamente, la varianza dell’indicatore al livello tra-persone e al livello entro-persona, ottenute adattando modelli saturi a entrambi i livelli.\n\nmodel1 &lt;- \"\n        level: 1\n            str1 ~~ str2 + str3 + str4\n            str2 ~~ str3 + str4\n            str3 ~~ str4\n\n        level: 2\n            str1 ~~ str2 + str3 + str4\n            str2 ~~ str3 + str4\n            str3 ~~ str4\n\n          # means\n            str1 + str2 + str3 + str4  ~ 1\n    \"\n\n\nfit.step1 &lt;- lavaan(\n    model = model1, data = imp, cluster = \"ID\",\n    auto.var = TRUE\n)\n\nsummary(fit.step1) |&gt; print()\n\nlavaan 0.6-19 ended normally after 363 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        24\n\n  Number of observations                          1269\n  Number of clusters [ID]                          151\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\n\nLevel 1 [within]:\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  str1 ~~                                             \n    str2            342.261   16.268   21.039    0.000\n    str3            406.537   19.297   21.067    0.000\n    str4            359.195   17.888   20.081    0.000\n  str2 ~~                                             \n    str3            305.064   16.314   18.699    0.000\n    str4            307.366   15.847   19.396    0.000\n  str3 ~~                                             \n    str4            347.263   18.496   18.775    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    str1            471.690   19.872   23.737    0.000\n    str2            383.080   16.154   23.714    0.000\n    str3            538.863   22.705   23.733    0.000\n    str4            489.494   20.666   23.686    0.000\n\n\nLevel 2 [ID]:\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  str1 ~~                                             \n    str2            275.295   38.963    7.066    0.000\n    str3            278.127   40.489    6.869    0.000\n    str4            292.286   41.776    6.996    0.000\n  str2 ~~                                             \n    str3            251.097   37.105    6.767    0.000\n    str4            271.965   39.062    6.962    0.000\n  str3 ~~                                             \n    str4            282.024   40.920    6.892    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    str1             34.296    1.579   21.727    0.000\n    str2             27.161    1.492   18.207    0.000\n    str3             35.551    1.561   22.779    0.000\n    str4             35.744    1.611   22.186    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    str1            298.145   42.327    7.044    0.000\n    str2            270.910   37.919    7.144    0.000\n    str3            281.366   41.412    6.794    0.000\n    str4            310.574   44.373    6.999    0.000\n\n\n\n\n# see ICC\nlavInspect(fit.step1, \"icc\") |&gt; print()\n\n str1  str2  str3  str4 \n0.387 0.414 0.343 0.388 \n\n\n\n\n82.6.2 Passo 2: Verifica della Varianza e Covarianza al Livello tra-Persone\nIn questo secondo passaggio, verifichiamo se: a) la varianza al livello tra-persone è significativa e b) se esistono covarianze significative a questo livello (Hox, Moerbeek, & van der Schoot, 2017). Il test delle covarianze ci permette di valutare se esistono effettivamente relazioni che potrebbero essere modellate con un fattore comune al livello tra-persone.\nPer prima cosa, per testare la significatività della varianza al livello tra-persone (passo 2a), adattiamo ai dati un modello nullo per questo livello. Un modello nullo è un modello in cui tutte le varianze (e le covarianze) sono fissate a zero. Al livello entro-persona specifichiamo invece un modello saturo, in cui tutti gli item sono correlati, assicurando così una perfetta aderenza ai dati. In questo modo, ogni eventuale discrepanza nel fit del modello deriva esclusivamente dal livello tra-persone, dove le varianze sono fissate a zero.\nQuando si testa il fit del modello, un test χ² significativo indica che il modello differisce in modo significativo rispetto a un modello che si adatterebbe perfettamente ai dati (Kline, 2011). Pertanto, se il test χ² respinge il modello nullo, possiamo concludere che è presente una varianza significativa al livello tra-persone.\nSuccessivamente, per verificare se esiste una covarianza significativa al livello tra-persone (passo 2b), rilasciamo il vincolo sulle varianze in modo da stimarle liberamente, mantenendo però le covarianze fissate a zero. La specificazione del modello saturo al livello entro-persona rimane invariata. Anche in questo caso, un test χ² significativo di fit del modello indica che questo modello differisce in modo significativo rispetto a un modello che si adatterebbe perfettamente ai dati. Poiché non abbiamo modellato alcuna relazione tra gli item a livello tra-persone, un test χ² significativo indica che le covarianze dovrebbero essere prese in considerazione. In altre parole, un test χ² significativo suggerisce la presenza di covarianze significative, che potrebbero essere spiegate con un modello fattoriale nei passaggi successivi.\n\nmodel2a &lt;- \"\n        level: 1\n            str1 ~~ str2 + str3 + str4\n            str2 ~~ str3 + str4\n            str3 ~~ str4\n\n        level: 2\n            str1 ~~ 0*str2 + 0*str3 + 0*str4\n            str2 ~~ 0*str3 + 0*str4\n            str3 ~~ 0*str4\n\n            str1 ~~ 0*str1\n            str2 ~~ 0*str2\n            str3 ~~ 0*str3\n            str4 ~~ 0*str4\n\n          # means\n            str1 + str2 + str3 + str4  ~ 1\n    \"\n\n\nfit.step2a &lt;- lavaan(\n    model = model2a, data = imp, cluster = \"ID\",\n    auto.var = TRUE\n)\n\nsummary(fit.step2a) |&gt; print()\n\nlavaan 0.6-19 ended normally after 118 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        14\n\n  Number of observations                          1269\n  Number of clusters [ID]                          151\n\nModel Test User Model:\n                                                      \n  Test statistic                               565.401\n  Degrees of freedom                                10\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\n\nLevel 1 [within]:\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  str1 ~~                                             \n    str2            623.313   26.624   23.412    0.000\n    str3            685.932   29.343   23.376    0.000\n    str4            659.716   29.075   22.690    0.000\n  str2 ~~                                             \n    str3            558.874   25.832   21.635    0.000\n    str4            588.081   26.523   22.172    0.000\n  str3 ~~                                             \n    str4            633.186   28.998   21.835    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    str1            774.610   30.685   25.244    0.000\n    str2            660.240   26.257   25.145    0.000\n    str3            817.829   32.239   25.368    0.000\n    str4            809.702   32.482   24.928    0.000\n\n\nLevel 2 [ID]:\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  str1 ~~                                             \n    str2              0.000                           \n    str3              0.000                           \n    str4              0.000                           \n  str2 ~~                                             \n    str3              0.000                           \n    str4              0.000                           \n  str3 ~~                                             \n    str4              0.000                           \n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    str1             33.560    0.781   42.954    0.000\n    str2             26.659    0.721   36.960    0.000\n    str3             34.732    0.803   43.264    0.000\n    str4             34.988    0.799   43.802    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    str1              0.000                           \n    str2              0.000                           \n    str3              0.000                           \n    str4              0.000                           \n\n\n\n\nmodel2b &lt;- \"\n        level: 1\n            str1 ~~ str2 + str3 + str4\n            str2 ~~ str3 + str4\n            str3 ~~ str4\n\n        level: 2\n            str1 ~~ 0*str2 + 0*str3 + 0*str4\n            str2 ~~ 0*str3 + 0*str4\n            str3 ~~ 0*str4\n\n          # means\n            str1 + str2 + str3 + str4  ~ 1\n    \"\n\n\nfit.step2b &lt;- lavaan(\n    model = model2b, data = imp, cluster = \"ID\",\n    auto.var = TRUE\n)\n\nsummary(fit.step2b) |&gt; print()\n\nlavaan 0.6-19 ended normally after 240 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        18\n\n  Number of observations                          1269\n  Number of clusters [ID]                          151\n\nModel Test User Model:\n                                                      \n  Test statistic                               425.971\n  Degrees of freedom                                 6\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\n\nLevel 1 [within]:\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  str1 ~~                                             \n    str2            583.494   26.975   21.631    0.000\n    str3            671.583   30.182   22.251    0.000\n    str4            623.698   29.159   21.390    0.000\n  str2 ~~                                             \n    str3            532.195   25.884   20.561    0.000\n    str4            534.900   26.062   20.524    0.000\n  str3 ~~                                             \n    str4            598.950   29.051   20.617    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    str1            750.987   32.247   23.288    0.000\n    str2            589.951   26.541   22.228    0.000\n    str3            790.466   33.639   23.498    0.000\n    str4            738.518   32.940   22.420    0.000\n\n\nLevel 2 [ID]:\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  str1 ~~                                             \n    str2              0.000                           \n    str3              0.000                           \n    str4              0.000                           \n  str2 ~~                                             \n    str3              0.000                           \n    str4              0.000                           \n  str3 ~~                                             \n    str4              0.000                           \n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    str1             33.600    0.803   41.857    0.000\n    str2             26.507    0.803   32.993    0.000\n    str3             34.830    0.874   39.851    0.000\n    str4             35.056    0.878   39.911    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    str1              6.520    3.333    1.956    0.050\n    str2             23.657    5.371    4.405    0.000\n    str3             17.924    5.397    3.321    0.001\n    str4             24.375    6.655    3.663    0.000\n\n\n\n\nanova(fit.step2a, fit.step2b)\n\n\nA anova: 2 x 8\n\n\n\nDf\nAIC\nBIC\nChisq\nChisq diff\nRMSEA\nDf diff\nPr(&gt;Chisq)\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nfit.step2b\n6\n42786.09\n42878.72\n425.9705\nNA\nNA\nNA\nNA\n\n\nfit.step2a\n10\n42917.52\n42989.56\n565.4010\n139.4305\n0.1633418\n4\n3.737341e-29\n\n\n\n\n\n\n\n82.6.3 Passo 3: Definizione di un Modello di Misura al Livello Entro-Persona\nIn questo terzo passo, esaminiamo se gli item possono essere rappresentati da un unico fattore al livello entro-persona. Per farlo, definiamo un modello di misura per il livello entro-persona, mantenendo invece un modello saturo a livello tra-persone. La bontà di adattamento di questo modello, e dei modelli successivi, può essere valutata utilizzando il test χ². Se il test χ² risulta significativo, dobbiamo rifiutare l’adattamento perfetto del modello. Con campioni di grandi dimensioni, anche piccole discrepanze nel modello possono portare a rifiutarlo (Marsh, Balla, & McDonald, 1988).\nPer questo motivo, oltre al test χ², consideriamo anche indici di adattamento approssimato: un RMSEA inferiore a 0.05 e un CFI superiore a 0.95 indicano un buon adattamento (Browne & Cudeck, 1992), mentre un RMSEA inferiore a 0.08 e un CFI superiore a 0.90 indicano un adattamento accettabile (Hu & Bentler, 1999).\nSe il modello non si adatta adeguatamente ai dati, possono essere intrapresi passi aggiuntivi per affrontare le cause di tale discrepanza prima di procedere. In questi casi, l’ispezione degli indici di modifica o dei residui di correlazione può fornire informazioni preziose su eventuali discrepanze locali del modello. È importante che le modifiche al modello siano sempre guidate da considerazioni teoriche, poiché seguire esclusivamente i risultati statistici può portare a modelli che non si generalizzano ad altri campioni (MacCallum, 1986).\nSolo quando il modello si adatta adeguatamente ai dati e ha senso teorico, è opportuno passare al passaggio successivo.\n\nmodel3 &lt;- \"\n        level: 1\n            stress =~ str1 + str2 + str3 + str4\n            stress ~~ 1*stress\n\n        level: 2\n            str1 ~~ str2 + str3 + str4\n            str2 ~~ str3 + str4\n            str3 ~~ str4\n\n          # means\n            str1 + str2 + str3 + str4  ~ 1\n    \"\n\n\nfit.step3 &lt;- lavaan(\n    model = model3, data = imp, cluster = \"ID\",\n    auto.var = TRUE\n)\n\n\nfitMeasures(fit.step3) |&gt; print()\n\n                 npar                  fmin                 chisq \n               22.000                13.036                50.423 \n                   df                pvalue        baseline.chisq \n                2.000                 0.000              4286.653 \n          baseline.df       baseline.pvalue                   cfi \n               12.000                 0.000                 0.989 \n                  tli                  nnfi                   rfi \n                0.932                 0.932                 0.929 \n                  nfi                  pnfi                   ifi \n                0.988                 0.165                 0.989 \n                  rni                  logl     unrestricted.logl \n                0.989            -21207.330            -21182.119 \n                  aic                   bic                ntotal \n            42458.661             42571.872              1269.000 \n                 bic2                 rmsea        rmsea.ci.lower \n            42501.990                 0.138                 0.107 \n       rmsea.ci.upper        rmsea.ci.level          rmsea.pvalue \n                0.172                 0.900                 0.000 \n       rmsea.close.h0 rmsea.notclose.pvalue     rmsea.notclose.h0 \n                0.050                 0.999                 0.080 \n                 srmr           srmr_within          srmr_between \n                0.023                 0.019                 0.003 \n\n\n\n\n82.6.4 Passo 4: Adattamento di un Modello a Due Livelli con Vincoli tra Livelli\nNel modello configurale, desideriamo che il costrutto abbia un significato comparabile a entrambi i livelli. Ad esempio, vorremmo che la “nervosità” rappresenti il sentimento generale degli individui come indicatore di stress al livello tra-persone e che, al livello entro-persona, esprima le variazioni quotidiane di quella stessa emozione per indicare le fluttuazioni giornaliere dello stress. Per permettere questa interpretazione, è necessario vincolare i carichi fattoriali affinché siano uguali tra il livello entro-persona e tra-persona.\nIn questo modello, la varianza del fattore al livello tra-persone deve essere stimata liberamente, poiché il vincolo sui carichi fattoriali già identifica la scala del fattore a livello tra-persona quando la varianza del fattore al livello entro-persona è fissata (Jak et al., 2014).\n\nmodel4 &lt;- \"\n        level: 1\n            stress =~ L1*str1 + L2*str2 + L3*str3 + L4*str4\n            stress ~~ 1*stress\n\n        level: 2\n            stress =~ L1*str1 + L2*str2 + L3*str3 + L4*str4\n            stress ~~ fb*stress\n\n          # means\n            str1 + str2 + str3 + str4  ~ 1\n    \"\n\n\nfit.step4 &lt;- lavaan(\n    model = model4, data = imp, cluster = \"ID\",\n    auto.var = TRUE\n)\n\n\nfitMeasures(fit.step4) |&gt; print()\n\n                 npar                  fmin                 chisq \n               17.000                13.049                83.701 \n                   df                pvalue        baseline.chisq \n                7.000                 0.000              4286.653 \n          baseline.df       baseline.pvalue                   cfi \n               12.000                 0.000                 0.982 \n                  tli                  nnfi                   rfi \n                0.969                 0.969                 0.967 \n                  nfi                  pnfi                   ifi \n                0.980                 0.572                 0.982 \n                  rni                  logl     unrestricted.logl \n                0.982            -21223.969            -21182.119 \n                  aic                   bic                ntotal \n            42481.938             42569.420              1269.000 \n                 bic2                 rmsea        rmsea.ci.lower \n            42515.419                 0.093                 0.076 \n       rmsea.ci.upper        rmsea.ci.level          rmsea.pvalue \n                0.111                 0.900                 0.000 \n       rmsea.close.h0 rmsea.notclose.pvalue     rmsea.notclose.h0 \n                0.050                 0.894                 0.080 \n                 srmr           srmr_within          srmr_between \n                0.032                 0.018                 0.014 \n\n\n\n\n82.6.5 Passo 5: Calcolo degli Indici di Affidabilità\nSe il modello ottenuto al Passo 4 presenta un buon adattamento, l’ultimo passo consiste nel calcolare gli indici di affidabilità. Utilizziamo le stime dei parametri ottenute per calcolare ωb (affidabilità a livello tra-persone) e ωw (affidabilità a livello entro-persona) seguendo le formule presentate.\nQuesti indici quantificano la stabilità e la coerenza delle misure rispettivamente tra e entro persone, offrendo una valutazione completa dell’affidabilità delle misure a ciascun livello del modello.\n\nmodel5 &lt;- \"\n        level: 1\n            stress =~ L1*str1 + L2*str2 + L3*str3 + L4*str4\n            stress ~~ 1*stress\n\n            str1 ~~ tw1*str1\n            str2 ~~ tw2*str2\n            str3 ~~ tw3*str3\n            str4 ~~ tw4*str4\n\n        level: 2\n            stress =~ L1*str1 + L2*str2 + L3*str3 + L4*str4\n            stress ~~ fb*stress\n\n            str1 ~~ tb1*str1\n            str2 ~~ tb2*str2\n            str3 ~~ tb3*str3\n            str4 ~~ tb4*str4\n\n          # means\n            str1 + str2 + str3 + str4  ~ 1\n\n          # reliability calculations\n          lambda := L1 + L2 + L3 + L4\n          thetaw := tw1 + tw2 + tw3 + tw4\n          thetab := tb1 + tb2 + tb3 + tb4\n          omega_w := lambda^2 / (lambda^2 + thetaw)\n          omega_b := (lambda^2 * fb) / (lambda^2 * (1/15 + fb) + fb + thetaw/15)\n    \"\n\n\nfit.step5 &lt;- lavaan(\n    model = model5, data = imp, cluster = \"ID\",\n    auto.var = TRUE\n)\n\n\nsummary(fit.step5) |&gt; print()\n\nlavaan 0.6-19 ended normally after 93 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        21\n  Number of equality constraints                     4\n\n  Number of observations                          1269\n  Number of clusters [ID]                          151\n\nModel Test User Model:\n                                                      \n  Test statistic                                83.701\n  Degrees of freedom                                 7\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\n\nLevel 1 [within]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  stress =~                                           \n    str1      (L1)   20.206    0.482   41.904    0.000\n    str2      (L2)   17.076    0.458   37.289    0.000\n    str3      (L3)   19.223    0.526   36.513    0.000\n    str4      (L4)   18.338    0.520   35.246    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    stress            1.000                           \n   .str1     (tw1)   53.776    5.062   10.623    0.000\n   .str2     (tw2)  110.015    5.823   18.895    0.000\n   .str3     (tw3)  160.900    8.107   19.847    0.000\n   .str4     (tw4)  172.111    8.652   19.894    0.000\n\n\nLevel 2 [ID]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  stress =~                                           \n    str1      (L1)   20.206    0.482   41.904    0.000\n    str2      (L2)   17.076    0.458   37.289    0.000\n    str3      (L3)   19.223    0.526   36.513    0.000\n    str4      (L4)   18.338    0.520   35.246    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .str1             34.417    1.589   21.659    0.000\n   .str2             27.133    1.415   19.179    0.000\n   .str3             35.533    1.591   22.335    0.000\n   .str4             35.740    1.537   23.251    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    stress    (fb)    0.745    0.112    6.644    0.000\n   .str1     (tb1)   -0.800    2.973   -0.269    0.788\n   .str2     (tb2)   18.555    4.380    4.236    0.000\n   .str3     (tb3)   20.302    5.575    3.641    0.000\n   .str4     (tb4)   23.640    6.723    3.516    0.000\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    lambda           74.843    1.708   43.824    0.000\n    thetaw          496.802   12.485   39.790    0.000\n    thetab           61.697    9.488    6.502    0.000\n    omega_w           0.919    0.004  232.932    0.000\n    omega_b           0.911    0.012   75.486    0.000\n\n\n\nI risultati sono simili a quelli riportati da Alphen et al. (2022), sebbene nel loro caso sia stata utilizzata una diversa procedura di imputazione e il modello sia stato adattato con Mplus.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>82</span>  <span class='chapter-title'>Affidabilità longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/12_temp_reliability.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/lgm/12_temp_reliability.html#informazioni-sullambiente-di-sviluppo",
    "title": "82  Affidabilità longitudinale",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lme4_1.1-35.5     Matrix_1.7-0      ggokabeito_0.1.0  viridis_0.6.5    \n [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n [9] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n[13] lavaan_0.6-18     psych_2.4.6.26    scales_1.3.0      markdown_1.13    \n[17] knitr_1.48        lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1    \n[21] dplyr_1.1.4       purrr_1.0.2       readr_2.1.5       tidyr_1.3.1      \n[25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.16.0  jsonlite_1.8.9     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5.1 farver_2.1.2      \n  [7] nloptr_2.1.1       rmarkdown_2.28     vctrs_0.6.5       \n [10] minqa_1.2.8        base64enc_0.1-3    rstatix_0.7.2     \n [13] htmltools_0.5.8.1  broom_1.0.6        Formula_1.2-5     \n [16] htmlwidgets_1.6.4  plyr_1.8.9         sandwich_3.1-1    \n [19] emmeans_1.10.4     zoo_1.8-12         uuid_1.2-1        \n [22] igraph_2.0.3       mime_0.12          lifecycle_1.0.4   \n [25] pkgconfig_2.0.3    R6_2.5.1           fastmap_1.2.0     \n [28] shiny_1.9.1        digest_0.6.37      OpenMx_2.21.12    \n [31] fdrtool_1.2.18     colorspace_2.1-1   rprojroot_2.0.4   \n [34] Hmisc_5.1-3        fansi_1.0.6        timechange_0.3.0  \n [37] abind_1.4-8        compiler_4.4.1     withr_3.0.1       \n [40] glasso_1.11        htmlTable_2.4.3    backports_1.5.0   \n [43] carData_3.0-5      ggsignif_0.6.4     MASS_7.3-61       \n [46] corpcor_1.6.10     gtools_3.9.5       tools_4.4.1       \n [49] pbivnorm_0.6.0     foreign_0.8-87     zip_2.3.1         \n [52] httpuv_1.6.15      nnet_7.3-19        glue_1.7.0        \n [55] quadprog_1.5-8     promises_1.3.0     nlme_3.1-166      \n [58] lisrelToR_0.3      grid_4.4.1         pbdZMQ_0.3-13     \n [61] checkmate_2.3.2    cluster_2.1.6      reshape2_1.4.4    \n [64] generics_0.1.3     gtable_0.3.5       tzdb_0.4.0        \n [67] data.table_1.16.0  hms_1.1.3          car_3.1-2         \n [70] utf8_1.2.4         sem_3.1-16         pillar_1.9.0      \n [73] IRdisplay_1.1      rockchalk_1.8.157  later_1.3.2       \n [76] splines_4.4.1      lattice_0.22-6     survival_3.7-0    \n [79] kutils_1.73        tidyselect_1.2.1   miniUI_0.1.1.1    \n [82] pbapply_1.7-2      stats4_4.4.1       xfun_0.47         \n [85] qgraph_1.9.8       arm_1.14-4         stringi_1.8.4     \n [88] pacman_0.5.1       boot_1.3-31        evaluate_1.0.0    \n [91] codetools_0.2-20   mi_1.1             cli_3.6.3         \n [94] RcppParallel_5.1.9 IRkernel_1.3.2     rpart_4.1.23      \n [97] xtable_1.8-4       repr_1.1.7         munsell_0.5.1     \n[100] Rcpp_1.0.13        coda_0.19-4.1      png_0.1-8         \n[103] XML_3.99-0.17      parallel_4.4.1     jpeg_0.1-10       \n[106] mvtnorm_1.3-1      openxlsx_4.2.7.1   crayon_1.5.3      \n[109] rlang_1.1.4        multcomp_1.4-26    mnormt_2.1.1      \n\n\n\n\n\n\nAlphen, T. van, Jak, S., Jansen in de Wal, J., Schuitema, J., & Peetsma, T. (2022). Determining reliability of daily measures: An illustration with data on teacher stress. Applied Measurement in Education, 35(1), 63–79.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>82</span>  <span class='chapter-title'>Affidabilità longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/prediction/01_prediction.html",
    "href": "chapters/prediction/01_prediction.html",
    "title": "83  Predizione",
    "section": "",
    "text": "83.1 Introduzione\nLe predizioni rappresentano un aspetto cruciale in numerosi ambiti. Possono riguardare sia dati categoriali, valutabili attraverso strumenti come matrici di confusione e modelli di regressione logistica, sia dati continui, analizzabili tramite regressioni multiple o modelli più complessi come quelli misti o a equazioni strutturali.\nIl capitolo si concentra sulla valutazione delle predizioni in contesti categoriali, approfondendo l’utilizzo della curva ROC e dell’AUC (Area Under the Curve) per misurare la qualità dei modelli predittivi.",
    "crumbs": [
      "Predizione",
      "<span class='chapter-number'>83</span>  <span class='chapter-title'>Predizione</span>"
    ]
  },
  {
    "objectID": "chapters/prediction/01_prediction.html#probabilità-di-una-condizione-medica",
    "href": "chapters/prediction/01_prediction.html#probabilità-di-una-condizione-medica",
    "title": "83  Predizione",
    "section": "\n83.2 Probabilità di una condizione medica",
    "text": "83.2 Probabilità di una condizione medica\nUn esempio classico, discusso da Petersen (2024), illustra il problema di calcolare la probabilità di avere l’HIV (\\(P(HIV \\mid Test+)\\)) partendo da un risultato positivo a un test diagnostico. Le informazioni di base sono:\n\n\nTasso di base dell’HIV (\\(P(HIV)\\)): 0.3% (0.003), ovvero la probabilità che una persona nella popolazione generale abbia l’HIV.\n\nSensibilità del test (\\(P(Test+ \\mid HIV)\\)): 95% (0.95), ovvero la probabilità che il test sia positivo quando la persona ha l’HIV.\n\nSpecificità del test (\\(P(Test- \\mid \\neg HIV)\\)): 99.28% (0.9928), ovvero la probabilità che il test sia negativo quando la persona non ha l’HIV.\n\nUtilizziamo il teorema di Bayes per calcolare \\(P(HIV \\mid Test+)\\):\n\\[\nP(HIV \\mid Test+) = \\frac{P(Test+ \\mid HIV) \\cdot P(HIV)}{P(Test+)},\n\\]\ndove il denominatore \\(P(Test+)\\) rappresenta la probabilità complessiva di un test positivo, somma di:\n\n\nVeri positivi: \\(P(Test+ \\mid HIV) \\cdot P(HIV)\\),\n\nFalsi positivi: \\(P(Test+ \\mid \\neg HIV) \\cdot P(\\neg HIV)\\).\n\nCon \\(P(Test+ \\mid \\neg HIV) = 1 - P(Test- \\mid \\neg HIV) = 1 - 0.9928 = 0.0072\\), e \\(P(\\neg HIV) = 1 - P(HIV) = 0.997\\), possiamo calcolare:\n\\[\nP(Test+) = (0.95 \\cdot 0.003) + (0.0072 \\cdot 0.997) \\approx 0.010027.\n\\]\nInserendo i valori nella formula di Bayes:\n\\[\nP(HIV \\mid Test+) = \\frac{0.95 \\cdot 0.003}{0.010027} \\approx 0.2844 \\quad (28.44\\%).\n\\]\nUn primo test positivo aumenta quindi la probabilità di avere l’HIV al 28.44%.\nDopo un primo test positivo, la probabilità di avere l’HIV è aumentata al 28.44%. Consideriamo ora l’effetto di un secondo test positivo e calcoliamo la probabilità aggiornata di avere l’HIV.\nLa probabilità di ottenere un secondo test positivo (\\(P(\\text{Secondo Test+})\\)) si calcola considerando due scenari:\n\n\nLa persona ha l’HIV:\n\nProbabilità: \\(P(HIV \\mid Test+) = 0.2844\\),\nSensibilità del test: \\(P(Test+ \\mid HIV) = 0.95\\).\n\n\n\nLa persona non ha l’HIV:\n\nProbabilità: \\(P(\\neg HIV \\mid Test+) = 1 - P(HIV \\mid Test+) = 0.7156\\),\nTasso di falsi positivi: \\(P(Test+ \\mid \\neg HIV) = 0.0072\\).\n\n\n\nLa probabilità totale è data da:\n\\[\nP(\\text{Secondo Test+}) = P(Test+ \\mid HIV) \\cdot P(HIV \\mid Test+) + P(Test+ \\mid \\neg HIV) \\cdot P(\\neg HIV \\mid Test+).\n\\]\nSostituendo i valori:\n\\[\nP(\\text{Secondo Test+}) = (0.95 \\cdot 0.2844) + (0.0072 \\cdot 0.7156) \\approx 0.2753.\n\\]\nAggiorniamo la probabilità di avere l’HIV dopo un secondo test positivo usando nuovamente il teorema di Bayes:\n\\[\nP(HIV \\mid \\text{Secondo Test+}) = \\frac{P(Test+ \\mid HIV) \\cdot P(HIV \\mid Test+)}{P(\\text{Secondo Test+})}.\n\\]\nSostituendo i valori:\n\\[\nP(HIV \\mid \\text{Secondo Test+}) = \\frac{0.95 \\cdot 0.2844}{0.2753} \\approx 0.981 \\quad (98.1\\%).\n\\]\nL’esempio presentato da Petersen (2024) è rilevante per il problema generale della predizione, in quanto illustra come il ragionamento bayesiano consenta di integrare informazioni iniziali e successive per migliorare la precisione delle stime. In particolare, i risultati evidenziano tre aspetti fondamentali:\n\nTasso di base come punto di partenza cruciale: La probabilità iniziale (prior) di avere l’HIV, pari allo 0.3%, sottolinea quanto sia importante considerare il contesto epidemiologico e demografico nella fase iniziale della predizione. Questo valore guida l’intero processo di aggiornamento e mostra che una condizione rara richiede prove forti per modificarne la probabilità.\nAggiornamento incrementale delle probabilità: Il passaggio da una probabilità del 28.44% dopo un primo test positivo a una probabilità del 98.1% dopo un secondo test positivo evidenzia la potenza del teorema di Bayes nel combinare evidenze successive. Ogni risultato positivo aggiunge informazioni che riducono l’incertezza iniziale, migliorando progressivamente la qualità della predizione.\nValore aggiunto dei test ripetuti: L’analisi dimostra che l’efficacia diagnostica cresce con l’accumularsi di evidenze. Test ripetuti consentono di discriminare meglio tra casi veri positivi e falsi positivi, fornendo stime più affidabili e utili per decisioni cliniche.\n\nQuesti risultati mettono in luce l’importanza del ragionamento bayesiano non solo per valutare la probabilità di una condizione medica, ma anche per affrontare una vasta gamma di problemi di predizione in cui l’incertezza iniziale può essere ridotta integrando dati nuovi. L’approccio evidenzia come sia possibile arrivare a conclusioni robuste anche in contesti caratterizzati da bassi tassi di base e test diagnostici non perfetti.",
    "crumbs": [
      "Predizione",
      "<span class='chapter-number'>83</span>  <span class='chapter-title'>Predizione</span>"
    ]
  },
  {
    "objectID": "chapters/prediction/01_prediction.html#accuratezza-delle-predizioni",
    "href": "chapters/prediction/01_prediction.html#accuratezza-delle-predizioni",
    "title": "83  Predizione",
    "section": "\n83.3 Accuratezza delle Predizioni",
    "text": "83.3 Accuratezza delle Predizioni\nDopo l’introduzione sull’uso del teorema di Bayes per la predizione, Petersen (2024) affronta il tema delle predizioni con una tabella 2 × 2. Per spiegare questo caso, Petersen (2024) presenta un esempio adattato da Meehl & Rosen (1955).\nImmaginiamo che l’esercito americano utilizzi un test per escludere i candidati con basse probabilità di completare l’addestramento di base. Per analizzare l’accuratezza delle predizioni effettuate, possiamo utilizzare una matrice di confusione, che confronta le predizioni del test con i risultati reali.\n\n\n\n\n\n\n\n\n\nDecisione (Predizione)\nAdattamento Effettivo (Scarso)\nAdattamento Effettivo (Buono)\nTotale Predetto\nRapporto di Selezione (SR)\n\n\n\nEscludere\nTP = 86 (0.043)\nFP = 422 (0.211)\n508\nSR = 0.254\n\n\nTrattenere\nFN = 14 (0.007)\nTN = 1.478 (0.739)\n1,492\n1 − SR = 0.746\n\n\nTotale Effettivo\n100\n1,900\nN = 2,000\n\n\n\nTasso di Base (BR)\nBR = 0.05\n1 − BR = 0.95\n\n\n\n\n\n\n83.3.1 La Matrice di Confusione\nLa matrice di confusione è uno strumento che mette in relazione le predizioni di un modello con i risultati osservati. Nel caso di una predizione binaria (es. sì/no o positivo/negativo), la matrice è organizzata in quattro categorie:\n\n\nVero Positivo (TP): La predizione corretta che identifica una persona con la caratteristica (es. cattivo adattamento).\n\nVero Negativo (TN): La predizione corretta che identifica una persona senza la caratteristica.\n\nFalso Positivo (FP): L’errore in cui si predice la presenza della caratteristica quando in realtà non è presente.\n\nFalso Negativo (FN): L’errore in cui si predice l’assenza della caratteristica quando in realtà è presente.\n\nQuesti quattro risultati consentono di valutare l’accuratezza di un modello.\n\n83.3.2 Tassi Marginali e Indicatori Chiave\nDalla matrice di confusione possiamo calcolare alcuni tassi e indicatori utili:\n\nTasso di Base (BR): La probabilità che una persona abbia la caratteristica di interesse. Ad esempio: \\[\nBR = \\frac{FN + TP}{N} = \\frac{100}{2000} = 0.05\n\\] Ciò significa che solo il 5% dei candidati ha un cattivo adattamento.\nRapporto di Selezione (SR): La probabilità di essere esclusi dal programma: \\[\nSR = \\frac{TP + FP}{N} = \\frac{508}{2000} = 0.254\n\\] In questo caso, il 25,4% dei candidati è stato escluso.\nPercentuale di Accuratezza: Rappresenta la proporzione di predizioni corrette sul totale: \\[\n\\text{Accuratezza} = 100 \\times \\frac{TP + TN}{N} = 100 \\times \\frac{86 + 1478}{2000} = 78\\%\n\\]\nAccuratezza per Caso: Misura la precisione che si otterrebbe effettuando predizioni casuali basate solo sulle probabilità marginali (BR e SR). Per esempio: \\[\nP(TP) = BR \\times SR = 0.05 \\times 0.254 = 0.0127\n\\] \\[\nP(TN) = (1 − BR) \\times (1 − SR) = 0.95 \\times 0.746 = 0.7087\n\\] \\[\n\\text{Accuratezza per Caso} = P(TP) + P(TN) = 0.0127 + 0.7087 = 0.7214 \\, (72,14\\%)\n\\]\n\nConfrontando il 78% di accuratezza del modello con il 72.14% ottenibile per caso, il modello fornisce un miglioramento del 6%.\n\n83.3.3 L’Importanza del Tasso di Base\nQuando il tasso di base è molto basso (come in questo caso, BR = 0.05), l’accuratezza complessiva può essere ingannevole. Se predicessimo che nessuno ha un cattivo adattamento, otterremmo un’accuratezza del 95%, ma il modello non identificherebbe alcun caso di cattivo adattamento.\n\n\n\n\n\n\n\n\nDecisione (Predizione)\nAdattamento Effettivo (Scarso)\nAdattamento Effettivo (Buono)\nTotale Predetto\n\n\n\nEscludere\nTP = 0\nFP = 0\n0\n\n\nTrattenere\nFN = 100\nTN = 1,900\n2,000\n\n\nTotale Effettivo\n100\n1,900\nN = 2,000\n\n\n\nIn questo caso, l’accuratezza complessiva sarebbe: \\[\nP(\\text{Accuratezza}) = \\frac{TP + TN}{N} = \\frac{0 + 1900}{2000} = 95\\%.\n\\]\nQuesto esempio evidenzia che un’elevata accuratezza complessiva non garantisce un buon modello, specialmente quando il tasso di base è sbilanciato.\nIn conclusione, l’analisi di una matrice di confusione richiede attenzione ai tassi di base e agli errori, poiché l’accuratezza globale può essere fuorviante. È essenziale confrontare il valore del modello con ciò che si otterrebbe per caso o con strategie alternative, come basarsi solo sul tasso di base. Inoltre, occorre considerare il peso relativo degli errori (falsi positivi e falsi negativi) in base al contesto applicativo. Questi aspetti saranno discussi nel prossimo paragrafo.\n\n83.3.4 Diversi Tipi di Errori e i loro Costi\nIn un processo di classificazione, non tutti gli errori hanno lo stesso costo. Esistono due tipi principali di errori: i falsi positivi e i falsi negativi, ciascuno con implicazioni diverse che dipendono dal contesto della predizione.\nSpesso, l’accuratezza complessiva può essere aumentata affidandosi semplicemente al tasso di base, ma in molte situazioni può essere preferibile utilizzare uno strumento di screening, anche a costo di una minore accuratezza complessiva, se ciò consente di minimizzare errori specifici che hanno costi elevati. Ad esempio:\n\nScreening medico: Consideriamo uno strumento di screening per l’HIV. I falsi positivi (classificare erroneamente una persona come a rischio) comportano costi come la necessità di test di conferma e, talvolta, ansia temporanea per l’individuo. Tuttavia, un falso negativo (non identificare una persona effettivamente a rischio) ha costi molto più alti, poiché potrebbe portare a un mancato intervento precoce, con conseguenze gravi per la salute. In questo caso, i costi associati ai falsi negativi superano di gran lunga quelli dei falsi positivi, rendendo lo screening preferibile nonostante una diminuzione dell’accuratezza complessiva.\nSelezione del personale in situazioni di rischio: La CIA, ad esempio, ha utilizzato strumenti di selezione per identificare potenziali spie durante periodi di guerra. Un falso positivo in questo contesto (considerare erroneamente una persona come una spia) potrebbe risultare nell’esclusione di un candidato innocente. Un falso negativo (assumere una persona che è effettivamente una spia) comporta rischi molto più gravi, rendendo cruciale l’identificazione corretta delle spie, anche a costo di più falsi positivi.\n\nIl modo in cui i costi degli errori vengono valutati dipende fortemente dal contesto. Alcuni potenziali costi dei falsi positivi includono trattamenti medici non necessari o il rischio di incarcerare una persona innocente. Al contrario, i falsi negativi possono portare al rilascio di una persona pericolosa, alla mancata individuazione di una malattia grave, o al mancato riconoscimento di un rischio imminente.\n\n83.3.5 Importanza del Rapporto di Selezione e del Tasso di Base\nIl costo degli errori può variare a seconda di come si imposta il rapporto di selezione (cioè, quanto rigorosamente si applica il criterio per accettare o escludere un individuo). La scelta di un rapporto di selezione meno restrittivo o più restrittivo influisce sulla probabilità di incorrere in falsi positivi e falsi negativi e può dipendere dal contesto e dai costi associati agli errori.\n\n\nCriterio meno rigido: Se escludere candidati è costoso, ad esempio quando si ha la necessità di assumere molte persone, potrebbe essere più utile un criterio di selezione permissivo, che accetta anche persone con un rischio potenziale.\n\nCriterio più rigido: In contesti in cui non è necessario accettare molti individui, si può adottare un criterio di selezione più rigido per ridurre i rischi, scartando un numero maggiore di candidati sospetti.\n\nQuando il rapporto di selezione differisce dal tasso di base degli esiti negativi effettivi, inevitabilmente si generano errori:\n\nSe, ad esempio, il rapporto di selezione prevede di escludere il 25% dei candidati, ma solo il 5% risulta effettivamente “non idoneo,” il risultato sarà un numero elevato di falsi positivi.\nD’altro canto, se si esclude solo l’1% dei candidati mentre il tasso di non idoneità è del 5%, si finirà per includere molti falsi negativi.\n\n83.3.6 Predizioni e Affidabilità in Condizioni di Basso Tasso di Base\nFare predizioni accurate diventa particolarmente complesso quando il tasso di base è basso, come nel caso di eventi rari (ad esempio, il suicidio). In questi casi, il numero di casi positivi reali è molto ridotto, rendendo difficile identificare correttamente i pochi eventi positivi senza generare numerosi falsi positivi o falsi negativi.\nQuesta difficoltà può essere compresa in relazione alla teoria classica dei test, che definisce l’affidabilità come il rapporto tra la varianza del punteggio vero e la varianza del punteggio osservato. Con un tasso di base molto basso, la varianza del punteggio vero è ridotta, il che abbassa l’affidabilità della misura e rende più complessa una predizione accurata.\n\n83.3.7 Sensibilità, Specificità, PPV e NPV\nCome abbiamo visto, la percentuale di accuratezza da sola non è sufficiente per valutare l’efficacia di un modello, poiché è molto influenzata dai tassi di base. Ad esempio, se il tasso di base è basso, potremmo ottenere un’alta percentuale di accuratezza semplicemente affermando che nessuno ha la condizione; se è alto, affermando che tutti ce l’hanno. Perciò, è essenziale considerare altre metriche di accuratezza, come sensibilità (SN), specificità (SP), valore predittivo positivo (PPV) e valore predittivo negativo (NPV).\nQueste metriche, che si possono calcolare dalla matrice di confusione, ci aiutano a valutare se il modello è efficace nel rilevare la condizione senza includere erroneamente i casi negativi. Analizziamole in dettaglio:\n\n\nSensibilità (SN): indica la capacità del test di identificare correttamente i veri positivi, cioè le persone con la condizione. Si calcola come la proporzione di veri positivi (\\(\\text{TP}\\)) rispetto al totale di persone con la condizione (\\(\\text{TP} + \\text{FN}\\)):\n\\[\n\\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{86}{86 + 14} = 0.86\n\\]\n\n\nSpecificità (SP): misura la capacità del test di identificare correttamente i veri negativi, ossia le persone senza la condizione. Si calcola come la proporzione di veri negativi (\\(\\text{TN}\\)) rispetto al totale di persone senza la condizione (\\(\\text{TN} + \\text{FP}\\)):\n\\[\n\\frac{\\text{TN}}{\\text{TN} + \\text{FP}} = \\frac{1,478}{1,478 + 422} = 0.78\n\\]\n\n\nValore Predittivo Positivo (PPV): indica la probabilità che una persona classificata come positiva abbia effettivamente la condizione. Si calcola come la proporzione di veri positivi (\\(\\text{TP}\\)) sul totale dei positivi stimati (\\(\\text{TP} + \\text{FP}\\)):\n\\[\n\\frac{\\text{TP}}{\\text{TP} + \\text{FP}} = \\frac{86}{86 + 422} = 0.17\n\\]\n\n\nValore Predittivo Negativo (NPV): rappresenta la probabilità che una persona classificata come negativa non abbia effettivamente la condizione. Si calcola come la proporzione di veri negativi (\\(\\text{TN}\\)) sul totale dei negativi stimati (\\(\\text{TN} + \\text{FN}\\)):\n\\[\n\\frac{\\text{TN}}{\\text{TN} + \\text{FN}} = \\frac{1,478}{1,478 + 14} = 0.99\n\\]\n\n\nOgni misura è espressa come una proporzione, variando da 0 a 1, dove valori più alti indicano una maggiore accuratezza per ciascun aspetto specifico. Usando queste metriche otteniamo un quadro dettagliato dell’efficacia dello strumento a un determinato cutoff.\nIn questo caso, il nostro strumento mostra:\n\n\nAlta sensibilità (0.86): è efficace nel rilevare chi ha la condizione.\n\nBassa specificità (0.78): classifica erroneamente come positivi molti casi che non hanno la condizione.\n\nBasso PPV (0.17): la maggior parte dei casi classificati come positivi sono in realtà negativi, indicando una frequenza elevata di falsi positivi.\n\nAlto NPV (0.99): quasi tutti i casi classificati come negativi non hanno la condizione.\n\nQuindi, pur avendo una buona capacità di rilevare i positivi (alta sensibilità), il modello è meno efficace nel limitare i falsi positivi (basso PPV). Questo potrebbe essere accettabile se l’obiettivo è identificare tutti i potenziali casi positivi, anche a costo di includere molti falsi positivi, ma potrebbe non essere ideale se il costo degli errori di falsa positività è elevato.",
    "crumbs": [
      "Predizione",
      "<span class='chapter-number'>83</span>  <span class='chapter-title'>Predizione</span>"
    ]
  },
  {
    "objectID": "chapters/prediction/01_prediction.html#stime-di-accuratezza-e-cutoff",
    "href": "chapters/prediction/01_prediction.html#stime-di-accuratezza-e-cutoff",
    "title": "83  Predizione",
    "section": "\n83.4 Stime di Accuratezza e Cutoff",
    "text": "83.4 Stime di Accuratezza e Cutoff\nSensibilità, specificità, PPV e NPV variano in base al cutoff (ovvero, la soglia) per la classificazione. Consideriamo il seguente esempio. Degli alieni visitano la Terra e sviluppano un test per determinare se una bacca è commestibile o non commestibile.\n\nsampleSize &lt;- 1000\n\nedibleScores &lt;- rnorm(sampleSize, 50, 15)\ninedibleScores &lt;- rnorm(sampleSize, 100, 15)\n\nedibleData &lt;- data.frame(score = c(edibleScores, inedibleScores), type = c(rep(\"edible\", sampleSize), rep(\"inedible\", sampleSize)))\n\ncutoff &lt;- 75\n\nhist_edible &lt;- density(edibleScores, from = 0, to = 150) %$%\n    data.frame(x = x, y = y) %&gt;%\n    mutate(area = x &gt;= cutoff)\n\nhist_edible$type[hist_edible$area == TRUE] &lt;- \"edible_FP\"\nhist_edible$type[hist_edible$area == FALSE] &lt;- \"edible_TN\"\n\nhist_inedible &lt;- density(inedibleScores, from = 0, to = 150) %$%\n    data.frame(x = x, y = y) %&gt;%\n    mutate(area = x &lt; cutoff)\n\nhist_inedible$type[hist_inedible$area == TRUE] &lt;- \"inedible_FN\"\nhist_inedible$type[hist_inedible$area == FALSE] &lt;- \"inedible_TP\"\n\ndensity_data &lt;- bind_rows(hist_edible, hist_inedible)\n\ndensity_data$type &lt;- factor(density_data$type, levels = c(\"edible_TN\", \"inedible_TP\", \"edible_FP\", \"inedible_FN\"))\n\nLa figura successiva mostra le distribuzioni dei punteggi in base al tipo di bacca. Si può notare come ci sono due distribuzioni distinte, ma con una certa sovrapposizione. Pertanto, qualsiasi cutoff selezionato comporterà almeno alcune classificazioni errate. L’entità della sovrapposizione delle distribuzioni riflette la quantità di errore di misurazione dello strumento rispetto alla caratteristica di interesse.\n\nggplot(data = edibleData, aes(x = score, ymin = 0, fill = type)) +\n    geom_density(alpha = .5) +\n    scale_fill_manual(\n      name = \"Tipo di Bacca\", values = c(viridis(2)[1], viridis(2)[2])\n    ) +\n    scale_y_continuous(name = \"Frequenza\") \n\n\n\n\n\n\n\nLa figura successiva mostra le distribuzioni dei punteggi in base al tipo di bacca con un cutoff. La linea rossa indica il cutoff: il livello al di sopra del quale le bacche vengono classificate come non commestibili. Ci sono errori su entrambi i lati del cutoff. Sotto il cutoff, ci sono dei falsi negativi (blu): bacche non commestibili erroneamente classificate come commestibili. Sopra il cutoff, ci sono dei falsi positivi (verde): bacche commestibili erroneamente classificate come non commestibili. I costi dei falsi negativi potrebbero includere malattia o morte derivanti dal consumo di bacche non commestibili, mentre i costi dei falsi positivi potrebbero includere maggiore tempo per trovare cibo, insufficienza di cibo e fame.\n\nggplot(data = density_data, aes(x = x, ymin = 0, ymax = y, fill = type)) +\n    geom_ribbon(alpha = 1) +\n    scale_fill_manual(\n        name = \"Tipo di Bacca\",\n        values = c(viridis(4)[4], viridis(4)[1], viridis(4)[3], viridis(4)[2]),\n        breaks = c(\"edible_TN\", \"inedible_TP\", \"edible_FP\", \"inedible_FN\"),\n        labels = c(\n          \"Commestibile: TN\", \"Non Commestibile: TP\", \n          \"Commestibile: FP\", \"Non Commestibile: FN\")\n    ) +\n    geom_line(aes(y = y)) +\n    geom_vline(xintercept = cutoff, color = \"red\", linewidth = 2) +\n    scale_x_continuous(name = \"Punteggio\") +\n    scale_y_continuous(name = \"Frequenza\") \n\n\n\n\n\n\n\nA seconda dei nostri obiettivi di valutazione, potremmo voler usare un diverso rapporto di selezione modificando il cutoff. La Figura mostra le distribuzioni dei punteggi quando si aumenta il cutoff. Ora ci sono più falsi negativi (blu) e meno falsi positivi (verde). Se alziamo il cutoff per essere più conservativi, il numero di falsi negativi aumenta, mentre il numero di falsi positivi diminuisce. Di conseguenza, aumentando il cutoff, la sensibilità e il valore predittivo negativo (NPV) diminuiscono, mentre la specificità e il valore predittivo positivo (PPV) aumentano. Un cutoff più alto potrebbe essere ottimale se i costi dei falsi positivi sono considerati superiori a quelli dei falsi negativi. Ad esempio, se gli alieni non possono rischiare di mangiare bacche non commestibili perché sono fatali, e ci sono abbastanza bacche commestibili per nutrire la colonia aliena.\n\n# Raise the cutoff\ncutoff &lt;- 85\n\nggplot(data = density_data, aes(x = x, ymin = 0, ymax = y, fill = type)) +\n    geom_ribbon(alpha = 1) +\n    scale_fill_manual(\n        name = \"Tipo di Bacca\",\n        values = c(viridis(4)[4], viridis(4)[1], viridis(4)[3], viridis(4)[2]),\n        breaks = c(\"edible_TN\", \"inedible_TP\", \"edible_FP\", \"inedible_FN\"),\n        labels = c(\n          \"Commestibile: TN\", \"Non Commestibile: TP\", \n          \"Commestibile: FP\", \"Non Commestibile: FN\")\n    ) +\n    geom_line(aes(y = y)) +\n    geom_vline(xintercept = cutoff, color = \"red\", linewidth = 2) +\n    scale_x_continuous(name = \"Punteggio\") +\n    scale_y_continuous(name = \"Frequenza\") +\n    theme(\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank()\n    )\n\n\n\n\n\n\n\nIn alternativa, possiamo abbassare il cutoff per essere più liberali. La Figura seguente mostra le distribuzioni dei punteggi quando abbassiamo il cutoff. Ora ci sono meno falsi negativi (blu) e più falsi positivi (verde). Abbassando il cutoff, la sensibilità e il NPV aumentano, mentre la specificità e il PPV diminuiscono. Un cutoff più basso potrebbe essere ottimale se i costi dei falsi negativi sono considerati superiori a quelli dei falsi positivi. Ad esempio, se gli alieni non possono rischiare di perdere bacche commestibili perché sono scarse, e mangiare bacche non commestibili comporta solo disagi temporanei.\n\n# Lower the cutoff\ncutoff &lt;- 65\n\nggplot(data = density_data, aes(x = x, ymin = 0, ymax = y, fill = type)) +\n    geom_ribbon(alpha = 1) +\n    scale_fill_manual(\n        name = \"Tipo di Bacca\",\n        values = c(viridis(4)[4], viridis(4)[1], viridis(4)[3], viridis(4)[2]),\n        breaks = c(\"edible_TN\", \"inedible_TP\", \"edible_FP\", \"inedible_FN\"),\n        labels = c(\n          \"Commestibile: TN\", \"Non Commestibile: TP\", \n          \"Commestibile: FP\", \"Non Commestibile: FN\")\n    ) +\n    geom_line(aes(y = y)) +\n    geom_vline(xintercept = cutoff, color = \"red\", linewidth = 2) +\n    scale_x_continuous(name = \"Punteggio\") +\n    scale_y_continuous(name = \"Frequenza\") +\n    theme(\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank()\n    )\n\n\n\n\n\n\n\nIn sintesi, sensibilità e specificità variano in base al cutoff utilizzato per la classificazione. Se aumentiamo il cutoff, la specificità e il PPV aumentano, mentre la sensibilità e il NPV diminuiscono. Se abbassiamo il cutoff, la sensibilità e il NPV aumentano, mentre la specificità e il PPV diminuiscono. Pertanto, il cutoff ottimale dipende dai costi associati ai falsi negativi e ai falsi positivi. Se i falsi negativi sono più costosi, dovremmo impostare un cutoff basso; se i falsi positivi sono più costosi, dovremmo impostare un cutoff alto.",
    "crumbs": [
      "Predizione",
      "<span class='chapter-number'>83</span>  <span class='chapter-title'>Predizione</span>"
    ]
  },
  {
    "objectID": "chapters/prediction/01_prediction.html#teoria-della-detezione-del-segnale",
    "href": "chapters/prediction/01_prediction.html#teoria-della-detezione-del-segnale",
    "title": "83  Predizione",
    "section": "\n83.5 Teoria della Detezione del Segnale",
    "text": "83.5 Teoria della Detezione del Segnale\nLa Teoria della Detezione del Segnale (Signal Detection Theory, SDT) è un approccio probabilistico che descrive la capacità di rilevare uno stimolo target (segnale) in un contesto di stimoli non target (rumore). Sviluppata durante la Seconda Guerra Mondiale per ottimizzare le prestazioni di radar e sonar, questa teoria distingue tra due aspetti fondamentali: sensibilità e bias.\n\nSensibilità\nLa sensibilità quantifica l’abilità di discriminare tra segnale e rumore, indipendentemente dalle tendenze decisionali. È una misura della qualità del sistema di rilevamento.\nBias\nIl bias riflette la tendenza dell’osservatore a rispondere in modo conservativo o liberale, cioè a sovrastimare o sottostimare la presenza del segnale, influenzando la soglia decisionale.\n\nOriginariamente impiegata per la selezione e l’addestramento degli operatori radar, la SDT ha trovato applicazioni in numerosi settori moderni. In medicina, ad esempio, viene utilizzata per valutare la capacità di diagnosticare patologie come tumori o infezioni. In psicologia, la SDT ha contribuito a studiare processi percettivi e decisionali, come la percezione sociale, evidenziando differenze sistematiche nella sensibilità e nel bias tra individui o gruppi.\n\n83.5.1 Metriche Principali\nLa SDT introduce metriche specifiche per quantificare sensibilità e bias:\n\n\n\\(d'\\): misura la sensibilità, rappresentando la separazione statistica tra distribuzioni di segnale e rumore.\n\n\n\\(\\beta\\), \\(c\\), \\(b\\): diverse misure del bias decisionale, che descrivono l’inclinazione del soggetto verso risposte più conservative o liberali.",
    "crumbs": [
      "Predizione",
      "<span class='chapter-number'>83</span>  <span class='chapter-title'>Predizione</span>"
    ]
  },
  {
    "objectID": "chapters/prediction/01_prediction.html#curva-roc-e-auc",
    "href": "chapters/prediction/01_prediction.html#curva-roc-e-auc",
    "title": "83  Predizione",
    "section": "\n83.6 Curva ROC e AUC",
    "text": "83.6 Curva ROC e AUC\nLa curva ROC (Receiver Operating Characteristic) è uno strumento essenziale per l’analisi delle prestazioni di modelli di classificazione binaria, particolarmente utile per visualizzare il compromesso tra sensibilità e specificità. Questa curva è strettamente connessa alla Signal Detection Theory (SDT), che fornisce una base teorica per comprendere i processi decisionali sotto incertezza.\nMentre la SDT descrive i principi fondamentali del processo decisionale, la curva ROC rappresenta un’applicazione pratica che consente di analizzare come la variazione della soglia decisionale influenzi le prestazioni del modello. Questa analisi è particolarmente utile per identificare soglie che ottimizzino il bilanciamento tra errori di classificazione (falsi positivi e falsi negativi).\n\n83.6.1 Definizione e Significato\nLa curva ROC visualizza il compromesso tra sensibilità (True Positive Rate, TPR) e 1-specificità (False Positive Rate, FPR) al variare della soglia decisionale. Ogni punto della curva rappresenta una combinazione specifica di questi due indicatori.\n\n\nSensibilità (TPR): proporzione di casi positivi correttamente classificati rispetto al totale dei veri positivi.\n\n1-Specificità (FPR): proporzione di falsi positivi rispetto al totale dei veri negativi.\n\n\n83.6.1.1 Costruzione della Curva ROC\n\n\nCalcolo di TPR e FPR: Si definiscono diverse soglie per il modello e, per ciascuna, si calcolano i valori di sensibilità e 1-specificità.\n\nGrafico bidimensionale: La sensibilità è rappresentata sull’asse y, mentre 1-specificità è riportata sull’asse x.\n\nInterpretazione: Una curva che si avvicina all’angolo superiore sinistro indica prestazioni elevate, con alta sensibilità e specificità. Una curva vicina alla diagonale principale rappresenta invece un modello privo di capacità discriminativa.\n\n83.6.2 Area Sotto la Curva (AUC)\nL’Area Under the Curve (AUC) è una misura aggregata della capacità del modello di distinguere tra classi. È definita come l’area sotto la curva ROC e può assumere valori compresi tra 0.5 (discriminazione casuale) e 1 (classificazione perfetta).\n\n83.6.2.1 Interpretazione dei Valori di AUC\n\n\nAUC = 1: Il modello distingue perfettamente tra classi positive e negative.\n\n0.9 ≤ AUC &lt; 1: Prestazioni eccellenti.\n\n0.75 ≤ AUC &lt; 0.9: Buone prestazioni.\n\n0.5 ≤ AUC &lt; 0.75: Prestazioni moderate.\n\nAUC = 0.5: Nessuna capacità discriminativa (classificazione casuale).\n\n83.6.3 Limiti della Curva ROC e dell’AUC\nSebbene la curva ROC e l’AUC siano strumenti utili, presentano alcune limitazioni che devono essere considerate per una corretta interpretazione.\n\n83.6.3.1 Problema dei Base Rate\nI base rate rappresentano la prevalenza di ciascuna classe nella popolazione. In dataset sbilanciati, l’AUC può essere fuorviante, poiché misura le prestazioni globali senza distinguere tra le difficoltà nella classificazione delle diverse classi.\nAd esempio, un modello potrebbe ottenere un AUC elevato classificando correttamente quasi tutti i casi della classe più frequente, trascurando però le prestazioni sulla classe meno rappresentata. Questo problema è particolarmente rilevante in applicazioni critiche, come la diagnosi di malattie rare, dove l’accuratezza della classe minoritaria è cruciale.\n\n83.6.4 La Base Rate Fallacy e i Limiti dell’AUC\nLa Base Rate Fallacy si manifesta quando i tassi di prevalenza di un evento (i cosiddetti base rate) vengono ignorati nell’interpretazione delle probabilità o nella valutazione delle prestazioni di un modello. Questo errore è particolarmente rilevante nei contesti in cui una delle due classi è molto meno frequente dell’altra.\nAd esempio, consideriamo un test diagnostico con un’accuratezza del 90% per una malattia rara che colpisce solo l’1% della popolazione. Anche se il test sembra altamente affidabile, il numero di falsi positivi potrebbe essere molto elevato rispetto ai veri positivi, a causa della bassa prevalenza della malattia. Di conseguenza, il valore predittivo positivo (probabilità che un individuo con test positivo sia realmente malato) sarebbe molto basso, rendendo il test di scarsa utilità pratica nonostante un AUC apparentemente elevato.\n\n83.6.4.1 Limiti dell’AUC\nPur essendo una misura aggregata utile per valutare le prestazioni complessive di un modello, l’AUC presenta alcune importanti limitazioni:\n\nInsensibilità ai Base Rate\nL’AUC non tiene conto della distribuzione delle classi, il che può portare a valutazioni fuorvianti in presenza di classi sbilanciate. Un modello potrebbe ottenere un AUC elevato classificando correttamente quasi tutti i casi della classe dominante, ignorando però quelli della classe meno rappresentata.\nMancanza di Dettagli sulle Soglie\nEssendo una misura aggregata, l’AUC non fornisce informazioni sulle prestazioni del modello a una soglia decisionale specifica. Questo può rappresentare un limite pratico quando è necessario scegliere un cutoff ottimale per massimizzare le prestazioni in un determinato contesto applicativo.\nTrattamento Simmetrico degli Errori\nL’AUC considera falsi positivi e falsi negativi come equivalenti. Tuttavia, in molti contesti reali (ad esempio, in ambito medico), questi errori hanno conseguenze molto diverse, e il loro bilanciamento può essere cruciale.\nSovrastima delle Prestazioni in Dataset Sbilanciati\nNei dataset con classi fortemente sbilanciate, l’AUC può mascherare le difficoltà del modello nel trattare correttamente la classe meno rappresentata, portando a una sopravvalutazione delle sue prestazioni globali.\n\nIn sintesi, sebbene la curva ROC e l’AUC siano strumenti fondamentali per l’analisi delle prestazioni dei modelli di classificazione, devono essere utilizzati con consapevolezza dei loro limiti. In contesti caratterizzati da base rate sbilanciati, è importante integrare queste metriche con altre, come i valori predittivi positivo e negativo, o analisi mirate delle soglie decisionali.\nUna valutazione completa delle prestazioni di un modello richiede di considerare il contesto applicativo, i costi relativi degli errori di classificazione e le implicazioni pratiche delle decisioni prese sulla base dei risultati del modello. Solo così è possibile ottimizzare il processo decisionale e garantire l’utilità del modello nelle applicazioni reali.\n\n83.6.5 Esempio in R\nIn questo esempio, utilizziamo un dataset simulato che rappresenta i punteggi ottenuti da un questionario psicologico sullo stress percepito e la loro associazione con l’appartenenza a due gruppi: basso stress e alto stress.\nCreiamo un dataset con 200 partecipanti, suddivisi equamente tra i due gruppi:\n\n# Generazione del dataset simulato\nset.seed(123)\nn &lt;- 200\n# Creazione di variabili\ngroup &lt;- factor(\n  rep(c(\"Basso\", \"Alto\"), each = n / 2), \n  levels = c(\"Basso\", \"Alto\")\n)\nstress_score &lt;- c(\n  rnorm(n / 2, mean = 50, sd = 10), # Gruppo basso stress\n  rnorm(n / 2, mean = 70, sd = 10)  # Gruppo alto stress\n)\n# Creazione del dataframe\ndata_stress &lt;- data.frame(\n  group = group,\n  stress_score = stress_score\n)\n# Visualizzazione delle prime righe\nhead(data_stress)\n#&gt;   group stress_score\n#&gt; 1 Basso        44.40\n#&gt; 2 Basso        47.70\n#&gt; 3 Basso        65.59\n#&gt; 4 Basso        50.71\n#&gt; 5 Basso        51.29\n#&gt; 6 Basso        67.15\n\nIl dataset contiene:\n\n\ngroup: Gruppo di appartenenza (Basso o Alto stress percepito).\n\nstress_score: Punteggio del questionario sullo stress percepito (variabile continua).\n\n\n83.6.5.1 Generazione della Curva ROC\nUtilizziamo la curva ROC per valutare la capacità del questionario di distinguere tra i due gruppi.\n\n# Generazione della curva ROC\nrocCurve &lt;- roc(data_stress$group, data_stress$stress_score)\n\nVisualizziamo il grafico della curva ROC e calcoliamo l’AUC:\n# Visualizzazione della curva ROC\nplot(rocCurve, legacy.axes = TRUE, print.auc = TRUE)\n\n83.6.5.2 Calcolo di Punti sulla Curva ROC per Soglie Specifiche\nFissiamo una serie di soglie per classificare i partecipanti in base al punteggio di stress percepito. Classifichiamo i partecipanti come “Alto stress” se il loro punteggio è superiore o uguale alla soglia, altrimenti come “Basso stress”.\n\n# Definiamo diverse soglie\nthresholds &lt;- seq(0, 100, by = 5)\n\n# Funzione per calcolare sensibilità e specificità\ncalc_metrics &lt;- function(threshold, data) {\n  # Predizione basata sulla soglia\n  predicted &lt;- factor(\n    ifelse(data$stress_score &lt;= threshold, \"Alto\", \"Basso\"),\n    levels = c(\"Alto\", \"Basso\")\n  )\n  \n  # Gruppo osservato\n  actual &lt;- factor(data$group, levels = c(\"Alto\", \"Basso\"))\n  \n  # Matrice di confusione\n  conf_matrix &lt;- table(Predicted = predicted, Actual = actual)\n  \n  # Estrarre metriche dalla matrice di confusione\n  TP &lt;- conf_matrix[\"Alto\", \"Alto\"]\n  FP &lt;- conf_matrix[\"Alto\", \"Basso\"]\n  TN &lt;- conf_matrix[\"Basso\", \"Basso\"]\n  FN &lt;- conf_matrix[\"Basso\", \"Alto\"]\n  \n  # Calcolo di sensibilità e specificità\n  sensitivity &lt;- TP / (TP + FN)\n  specificity &lt;- TN / (TN + FP)\n  \n  # Restituzione dei risultati\n  return(list(\n    threshold = threshold,\n    sensitivity = sensitivity,\n    specificity = specificity,\n    fpr = 1 - specificity\n  ))\n}\n\n# Calcolo metriche per ogni soglia\nresults &lt;- lapply(thresholds, function(t) calc_metrics(t, data_stress))\n\n# Estrazione dei valori per il plotting\nfpr_values &lt;- sapply(results, function(x) x$fpr)\nsens_values &lt;- sapply(results, function(x) x$sensitivity)\n\n# Invertire y per calcolare i punti richiesti\npoints_inverted &lt;- list(x = fpr_values, y = 1 - sens_values)\n\n# Visualizzazione della curva ROC\nplot(rocCurve, legacy.axes = TRUE, print.auc = TRUE)\n\n# Plot dei punti con colori appropriati\npoints(points_inverted$x, points_inverted$y, col = \"green\", pch = 19, cex = 0.8)\n\n# Aggiunta di etichette per soglie selezionate\nselected_thresholds &lt;- seq(0, 100, by = 20)\nfor (threshold in selected_thresholds) {\n  idx &lt;- which(thresholds == threshold)\n  if (length(idx) &gt; 0) {\n    text(\n      points_inverted$x[idx],\n      points_inverted$y[idx],\n      labels = threshold,\n      pos = 3,\n      cex = 0.7\n    )\n  }\n}\n\n\n\n\n\n\n\nLa curva ROC permette di valutare l’abilità di un modello di distinguere tra due gruppi (ad esempio, “Basso stress” e “Alto stress”). Ogni punto sulla curva ROC corrisponde a una soglia specifica utilizzata per classificare i partecipanti. Per calcolare questi punti, seguiamo i seguenti passaggi:\n1. Definizione delle Soglie. Le soglie sono valori specifici del punteggio continuo (stress_score) che separano i gruppi. Per ogni soglia:\n\nI partecipanti con punteggio ≥ soglia vengono classificati come “Alto stress”.\nI partecipanti con punteggio &lt; soglia vengono classificati come “Basso stress”.\n\n2. Sensibilità e Specificità. Per ogni soglia, calcoliamo due metriche fondamentali:\n\n\nSensibilità (True Positive Rate, TPR): proporzione di partecipanti del gruppo “Alto stress” classificati correttamente.\n\nSpecificità: proporzione di partecipanti del gruppo “Basso stress” classificati correttamente. La False Positive Rate (FPR) è complementare alla specificità: \\(\\text{FPR} = 1 - \\text{Specificità}\\).\n\n3. Logica della Funzione calc_metrics. La funzione prende una soglia e il dataset come input. I passaggi principali sono:\n\n\nClassificazione: per ogni partecipante, confrontiamo il punteggio con la soglia per assegnare la classe predetta.\n\nMatrice di Confusione: costruiamo una tabella che confronta le classificazioni predette con quelle osservate per calcolare i seguenti valori:\n\n\nTP (True Positives): numero di “Alto stress” predetti correttamente.\n\nFP (False Positives): numero di “Basso stress” classificati erroneamente come “Alto stress”.\n\nTN (True Negatives): numero di “Basso stress” classificati correttamente.\n\nFN (False Negatives): numero di “Alto stress” classificati erroneamente come “Basso stress”.\n\n\n\nMetriche: calcoliamo sensibilità, specificità e FPR.\n\n4. Calcolo e Visualizzazione dei Punti.\n\nPer ciascuna soglia nella sequenza definita (seq(0, 100, by = 5)), la funzione calcola le metriche.\nI valori di FPR (asse \\(x\\)) e sensibilità (asse \\(y\\)) vengono estratti per costruire i punti ROC.\nLa curva ROC viene tracciata utilizzando la funzione roc, mentre i punti calcolati manualmente vengono aggiunti al grafico per confrontarli.\n\nQuesta parte del codice calcola e traccia i punti ROC per ogni soglia:\n# Calcolo delle metriche per ogni soglia\nresults &lt;- lapply(thresholds, function(t) calc_metrics(t, data_stress))\n\n# Estrazione di FPR e sensibilità per il plotting\nfpr_values &lt;- sapply(results, function(x) x$fpr)\nsens_values &lt;- sapply(results, function(x) x$sensitivity)\n\n# Visualizzazione dei punti ROC sul grafico\npoints(fpr_values, 1 - sens_values, col = \"green\", pch = 19, cex = 0.8)\n\n# Aggiunta di etichette alle soglie selezionate\nselected_thresholds &lt;- seq(0, 100, by = 20)\nfor (threshold in selected_thresholds) {\n  idx &lt;- which(thresholds == threshold)\n  if (length(idx) &gt; 0) {\n    text(\n      fpr_values[idx], \n      1 - sens_values[idx], \n      labels = threshold, \n      pos = 3, \n      cex = 0.7\n    )\n  }\n}\n\n\nInversione di \\(y\\): per allineare i punti calcolati alla curva ROC, si usa \\(1 - \\text{Sensibilità}\\) sull’asse \\(y\\). Questo perché la curva ROC calcola direttamente la sensibilità, mentre i punti manuali la confrontano come \\(1 - \\text{Specificità}\\) sull’asse \\(x\\).\n\nPunti e Etichette: i punti calcolati vengono evidenziati in verde e annotati con i valori delle soglie.\n\nIn sintesi, il grafico finale mostra la curva ROC completa generata automaticamente e i punti calcolati manualmente. Le etichette sulle soglie forniscono informazioni su come il comportamento del modello varia al variare della soglia. Questo approccio permette di comprendere i concetti dietro la costruzione della curva ROC e come ogni soglia influenzi le metriche di classificazione.\nPer una rappresentazione più fluida, applichiamo una lisciatura:\n\n# Curva ROC lisciata\nplot(\n  roc(data_stress$group, data_stress$stress_score, smooth = TRUE),\n  legacy.axes = TRUE, print.auc = TRUE\n)\n\n\n\n\n\n\n\nNel nostro esempio, il valore di AUC ≈ 0.92 indica che il questionario sullo stress percepito ha un’ottima capacità di distinguere tra i due gruppi, essendo molto vicino al valore massimo di 1. Questo risultato evidenzia che il questionario possiede una combinazione di elevata sensibilità e specificità, rendendolo uno strumento efficace per discriminare tra persone con basso e alto stress percepito.\nIn sintesi, l’analisi dimostra che:\n\nil questionario è un valido strumento diagnostico;\n\nl’approccio ROC è utile per valutare l’efficacia di un test psicologico nella distinzione tra gruppi con caratteristiche diverse, come livelli di stress, depressione o ansia.\n\nQuesto esempio illustra l’applicazione pratica della curva ROC nel contesto psicologico, sottolineando il suo valore nell’analisi dell’accuratezza di strumenti diagnostici e questionari.",
    "crumbs": [
      "Predizione",
      "<span class='chapter-number'>83</span>  <span class='chapter-title'>Predizione</span>"
    ]
  },
  {
    "objectID": "chapters/prediction/01_prediction.html#riflessioni-conclusive",
    "href": "chapters/prediction/01_prediction.html#riflessioni-conclusive",
    "title": "83  Predizione",
    "section": "\n83.7 Riflessioni Conclusive",
    "text": "83.7 Riflessioni Conclusive\nL’analisi delle prestazioni dei modelli predittivi è un aspetto centrale in numerosi ambiti applicativi, dalla psicologia fino all’intelligenza artificiale. Questo capitolo ha esplorato strumenti e concetti fondamentali per valutare l’accuratezza e l’affidabilità di modelli di classificazione, sottolineando come ciascuno di essi possa contribuire a migliorare la qualità delle decisioni in condizioni di incertezza.\nUno dei temi chiave affrontati riguarda l’importanza di metriche come sensibilità, specificità, valore predittivo positivo (PPV) e valore predittivo negativo (NPV), che offrono una descrizione dettagliata delle prestazioni di un modello in relazione ai suoi errori (falsi positivi e falsi negativi). Queste metriche, derivate dalla matrice di confusione, non solo consentono di misurare l’accuratezza complessiva, ma permettono anche di adattare l’interpretazione del modello alle esigenze specifiche del contesto applicativo:\n\n\nSensibilità e specificità descrivono rispettivamente la capacità del modello di rilevare correttamente i positivi e di escludere correttamente i negativi.\n\nPPV e NPV forniscono informazioni sulla probabilità che una predizione del modello corrisponda alla realtà, tenendo conto della prevalenza delle classi (tasso di base).\n\nLa matrice di confusione rappresenta una base analitica essenziale, da cui derivare tassi marginali e indicatori chiave. Tuttavia, è stato evidenziato come l’accuratezza globale possa risultare fuorviante in presenza di tassi di base sbilanciati, poiché essa può essere dominata dalla classe più frequente. Per questo motivo, l’interpretazione delle prestazioni di un modello richiede sempre un’analisi approfondita, che includa sia gli errori commessi che i costi associati a tali errori.\nLa curva ROC (Receiver Operating Characteristic) e l’AUC (Area Under the Curve) costituiscono strumenti avanzati per analizzare le prestazioni dei modelli predittivi, offrendo un quadro completo della capacità discriminativa del modello al variare delle soglie decisionali. L’AUC, in particolare, sintetizza in un singolo valore la qualità del modello, rendendo possibile il confronto tra diversi approcci anche in presenza di tassi di base molto diversi. Tuttavia, è stato sottolineato come il valore aggregato dell’AUC debba essere interpretato con cautela, in quanto non fornisce informazioni dettagliate sulle prestazioni a soglie specifiche.\nUn altro tema affrontato riguarda il teorema di Bayes, applicato a contesti diagnostici per aggiornare la probabilità di una condizione sulla base di nuove evidenze. Questo approccio ha evidenziato l’importanza di integrare informazioni iniziali (priori) con nuove osservazioni (dati) per ridurre l’incertezza e migliorare la precisione delle stime. L’esempio del test diagnostico per l’HIV ha illustrato come l’aggiornamento progressivo delle probabilità consenta di ottenere stime più affidabili anche in contesti caratterizzati da bassi tassi di base e test diagnostici imperfetti.\nInfine, sono state discusse le implicazioni pratiche legate alla scelta delle soglie decisionali, al bilanciamento tra diversi tipi di errori e alla gestione delle distribuzioni sbilanciate. In particolare, è stato evidenziato come la selezione del cutoff ottimale debba essere guidata non solo dalle prestazioni globali del modello, ma anche dai costi associati agli errori di classificazione in un dato contesto.\nIn sintesi, questo capitolo ha fornito una panoramica delle principali metodologie per la valutazione delle predizioni, combinando strumenti tradizionali come la matrice di confusione e il teorema di Bayes con approcci avanzati come la curva ROC e l’AUC. Questi strumenti, utilizzati in modo complementare, consentono di affrontare con rigore e flessibilità le sfide della classificazione, guidando verso decisioni informate e ottimizzate in base alle esigenze specifiche di ciascun contesto applicativo.",
    "crumbs": [
      "Predizione",
      "<span class='chapter-number'>83</span>  <span class='chapter-title'>Predizione</span>"
    ]
  },
  {
    "objectID": "chapters/prediction/01_prediction.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/prediction/01_prediction.html#informazioni-sullambiente-di-sviluppo",
    "title": "83  Predizione",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.2\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] grid      stats     graphics  grDevices utils     datasets  methods  \n#&gt; [8] base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] tinytex_0.56            MOTE_1.0.2              ggrepel_0.9.6          \n#&gt;  [4] car_3.1-3               carData_3.0-5           msir_1.3.3             \n#&gt;  [7] ResourceSelection_0.3-6 rms_7.0-0               Hmisc_5.2-3            \n#&gt; [10] ROCR_1.0-11             pROC_1.18.5             magrittr_2.0.3         \n#&gt; [13] petersenlab_1.1.0       ggokabeito_0.1.0        see_0.11.0             \n#&gt; [16] MASS_7.3-65             viridis_0.6.5           viridisLite_0.4.2      \n#&gt; [19] ggpubr_0.6.0            ggExtra_0.10.1          gridExtra_2.3          \n#&gt; [22] patchwork_1.3.0         bayesplot_1.11.1        semTools_0.5-6         \n#&gt; [25] semPlot_1.1.6           lavaan_0.6-19           psych_2.4.12           \n#&gt; [28] scales_1.3.0            markdown_1.13           knitr_1.50             \n#&gt; [31] lubridate_1.9.4         forcats_1.0.0           stringr_1.5.1          \n#&gt; [34] dplyr_1.1.4             purrr_1.0.4             readr_2.1.5            \n#&gt; [37] tidyr_1.3.1             tibble_3.2.1            ggplot2_3.5.1          \n#&gt; [40] tidyverse_2.0.0         here_1.0.1             \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] splines_4.4.2       later_1.4.1         polspline_1.1.25   \n#&gt;   [4] XML_3.99-0.18       rpart_4.1.24        lifecycle_1.0.4    \n#&gt;   [7] Rdpack_2.6.3        rstatix_0.7.2       rprojroot_2.0.4    \n#&gt;  [10] lattice_0.22-6      rockchalk_1.8.157   backports_1.5.0    \n#&gt;  [13] openxlsx_4.2.8      rmarkdown_2.29      yaml_2.3.10        \n#&gt;  [16] httpuv_1.6.15       qgraph_1.9.8        zip_2.3.2          \n#&gt;  [19] pbapply_1.7-2       DBI_1.2.3           minqa_1.2.8        \n#&gt;  [22] RColorBrewer_1.1-3  multcomp_1.4-28     abind_1.4-8        \n#&gt;  [25] quadprog_1.5-8      nnet_7.3-20         TH.data_1.1-3      \n#&gt;  [28] sandwich_3.1-1      arm_1.14-4          MatrixModels_0.5-3 \n#&gt;  [31] codetools_0.2-20    tidyselect_1.2.1    farver_2.1.2       \n#&gt;  [34] lme4_1.1-36         stats4_4.4.2        base64enc_0.1-3    \n#&gt;  [37] jsonlite_1.9.1      Formula_1.2-5       survival_3.8-3     \n#&gt;  [40] emmeans_1.10.7      tools_4.4.2         Rcpp_1.0.14        \n#&gt;  [43] glue_1.8.0          mnormt_2.1.1        xfun_0.51          \n#&gt;  [46] mgcv_1.9-1          withr_3.0.2         fastmap_1.2.0      \n#&gt;  [49] mitools_2.4         boot_1.3-31         SparseM_1.84-2     \n#&gt;  [52] digest_0.6.37       mi_1.1              timechange_0.3.0   \n#&gt;  [55] R6_2.6.1            mime_0.13           estimability_1.5.1 \n#&gt;  [58] colorspace_2.1-1    mix_1.0-13          gtools_3.9.5       \n#&gt;  [61] jpeg_0.1-10         generics_0.1.3      data.table_1.17.0  \n#&gt;  [64] corpcor_1.6.10      htmlwidgets_1.6.4   pkgconfig_2.0.3    \n#&gt;  [67] sem_3.1-16          gtable_0.3.6        htmltools_0.5.8.1  \n#&gt;  [70] png_0.1-8           reformulas_0.4.0    rstudioapi_0.17.1  \n#&gt;  [73] tzdb_0.5.0          reshape2_1.4.4      coda_0.19-4.1      \n#&gt;  [76] checkmate_2.3.2     nlme_3.1-167        nloptr_2.2.1       \n#&gt;  [79] zoo_1.8-13          parallel_4.4.2      miniUI_0.1.1.1     \n#&gt;  [82] foreign_0.8-88      pillar_1.10.1       reshape_0.8.9      \n#&gt;  [85] vctrs_0.6.5         promises_1.3.2      OpenMx_2.21.13     \n#&gt;  [88] xtable_1.8-4        cluster_2.1.8.1     htmlTable_2.4.3    \n#&gt;  [91] evaluate_1.0.3      pbivnorm_0.6.0      ez_4.4-0           \n#&gt;  [94] mvtnorm_1.3-3       cli_3.6.4           kutils_1.73        \n#&gt;  [97] compiler_4.4.2      rlang_1.1.5         ggsignif_0.6.4     \n#&gt; [100] labeling_0.4.3      fdrtool_1.2.18      mclust_6.1.1       \n#&gt; [103] plyr_1.8.9          stringi_1.8.4       munsell_0.5.1      \n#&gt; [106] MBESS_4.9.3         lisrelToR_0.3       pacman_0.5.1       \n#&gt; [109] quantreg_6.1        Matrix_1.7-3        hms_1.1.3          \n#&gt; [112] glasso_1.11         shiny_1.10.0        rbibutils_2.3      \n#&gt; [115] igraph_2.1.4        broom_1.0.7         RcppParallel_5.1.10\n\n\n\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "Predizione",
      "<span class='chapter-number'>83</span>  <span class='chapter-title'>Predizione</span>"
    ]
  },
  {
    "objectID": "99-references.html",
    "href": "99-references.html",
    "title": "Bibliografia",
    "section": "",
    "text": "Allen, M. J., & Yen, W. M. (2001). Introduction to measurement\ntheory. Waveland Press.\n\n\nAlphen, T. van, Jak, S., Jansen in de Wal, J., Schuitema, J., &\nPeetsma, T. (2022). Determining reliability of daily measures: An\nillustration with data on teacher stress. Applied Measurement in\nEducation, 35(1), 63–79.\n\n\nAmerican Educational Research Association, American Psychological\nAssociation, & National Council on Measurement in Education. (2014).\nStandards for educational and psychological testing. American\nEducational Research Association.\n\n\nArias, A. (2024). A short tutorial on validation in educational and\npsychological assessment. Teaching Quantitative Methods\nVignettes, 20(3).\n\n\nBandalos, D. L. (2018). Measurement theory and applications for the\nsocial sciences. Guilford Publications.\n\n\nBarbeau, K., Boileau, K., Sarr, F., & Smith, K. (2019). Path\nanalysis in mplus: A tutorial using a conceptual model of psychological\nand behavioral antecedents of bulimic symptoms in young adults. The\nQuantitative Methods for Psychology, 15(1), 38–53.\n\n\nBarrett, P. (2007). Structural equation modelling: Adjudging model fit.\nPersonality and Individual Differences, 42(5),\n815–824.\n\n\nBates, D., Mächler, M., Bolker, B., & Walker, S. (2014). Fitting\nlinear mixed-effects models using lme4. arXiv Preprint\narXiv:1406.5823.\n\n\nBlampied, N. M. (2022). Reliable change and the reliable change index:\nStill useful after all these years? The Cognitive Behaviour\nTherapist, 15, e50.\n\n\nBlume, F., Buhr, L., Kuehnhausen, J., Köpke, R., Weber, L. A.,\nFallgatter, A. J., Ethofer, T., & Gawrilow, C. (2020). Validation of\nthe self-report version of the german strengths and weaknesses of ADHD\nsymptoms and normal behavior scale (SWAN-DE-SB). Assessment,\n10731911241236699.\n\n\nBollen, K., & Lennox, R. (1991). Conventional wisdom on measurement:\nA structural equation perspective. Psychological Bulletin,\n110(2), 305–314.\n\n\nBonifay, W., Winter, S. D., Skoblow, H. F., & Watts, A. L. (2024).\nGood fit is weak evidence of replication: Increasing rigor through prior\npredictive similarity checking. Assessment, 10731911241234118.\n\n\nBrown, A. (2023). Psychometrics in exercises using r and RStudio:\nTextbook and data resource. https://bookdown.org/annabrown/psychometricsR\n\n\nBrown, T. A. (2015). Confirmatory factor analysis for applied\nresearch. Guilford publications.\n\n\nBuchberger, E. S., Ngo, C. T., Peikert, A., Brandmaier, A. M., &\nWerkle-Bergner, M. (2024). Estimating statistical power for structural\nequation models in developmental cognitive science: A tutorial in r:\nPower simulation for SEMs. Behavior Research Methods, 1–18.\n\n\nByrne, B. M. (2013). Structural equation modeling with mplus: Basic\nconcepts, applications, and programming. routledge.\n\n\nCaudek, C., & Luccio, R. (2001). Statistica per psicologi\n(III rist. 2023, Vol. 11, p. 320). Laterza.\n\n\nCharter, R. A. (1996). Revisiting the standard errors of measurement,\nestimate, and prediction and their application to test scores.\nPerceptual and Motor Skills, 82(3), 1139–1144.\n\n\nChen, D.-G., & Yung, Y.-F. (2023). Structural equation modeling\nusing r/SAS: A step-by-step approach with real data analysis. CRC\nPress.\n\n\nClement, L. M., & Bradley-Garcia, M. (2022). A step-by-step tutorial\nfor performing a moderated mediation analysis using PROCESS. The\nQuantitative Methods for Psychology, 18(3), 258–271.\n\n\nDebelak, R., Strobl, C., & Zeigenfuse, M. D. (2022). An\nintroduction to the rasch model with examples in r. CRC Press.\n\n\nDomini, F., & Caudek, C. (2009). The intrinsic constraint model and\nfechnerian sensory scaling. Journal of Vision, 9(2),\n25–25.\n\n\nGrimm, K. J., Ram, N., & Estabrook, R. (2016). Growth modeling:\nStructural equation and multilevel modeling approaches. Guilford\nPublications.\n\n\nHargrave, T. D., & Hammer, M. Y. (2016). Restoration of\nrelationships after affairs. In Techniques for the couple\ntherapist (pp. 190–193). Routledge.\n\n\nHaslbeck, J., & Bork, R. van. (2022). Estimating the number of\nfactors in exploratory factor analysis via out-of-sample prediction\nerrors. Psychological Methods.\n\n\nHayduk, L. A. (2014). Shame for disrespecting evidence: The personal\nconsequences of insufficient respect for structural equation model\ntesting. BMC Medical Research Methodology, 14, 1–10.\n\n\nHu, L., & Bentler, P. M. (1998). Fit indices in covariance structure\nmodeling: Sensitivity to underparameterized model misspecification.\nPsychological Methods, 3(4), 424--453.\n\n\nJohn, O. P., & Benet-Martinez, V. (2014). Measurement: Reliability,\nconstruct validation, and scale construction. In H. T. Reis & C. M.\nJudd (Eds.), Handbook of research methods in social and personality\npsychology (2nd ed., pp. 473–503). Cambridge University Press.\n\n\nKan, K.-J., Maas, H. L. van der, & Levine, S. Z. (2019). Extending\npsychometric network analysis: Empirical evidence against g in favor of\nmutualism? Intelligence, 73, 52–62.\n\n\nKelley, T. L. (1921). The reliability of test scores. The Journal of\nEducational Research, 3(5), 370–379.\n\n\nKline, P. (2013). Handbook of psychological testing. Routledge.\n\n\nKline, R. B. (2023). Principles and practice of structural equation\nmodeling. Guilford publications.\n\n\nLittle, T. D. (2023). Longitudinal structural equation\nmodeling. Guilford Press.\n\n\nLord, F. M., & Novick, M. R. (1968). Statistical theories of\nmental test scores. Addison-Wesley.\n\n\nMarsh, H. W., Morin, A. J., Parker, P. D., & Kaur, G. (2014).\nExploratory structural equation modeling: An integration of the best\nfeatures of exploratory and confirmatory factor analysis. Annual\nReview of Clinical Psychology, 10(1), 85–110.\n\n\nMarsh, H., & Alamer, A. (2024). When and how to use set-exploratory\nstructural equation modelling to test structural models: A tutorial\nusing the r package lavaan. British Journal of Mathematical and\nStatistical Psychology.\n\n\nMauro, R. (1990). Understanding LOVE (left out\nvariables error): A method for estimating the effects of omitted\nvariables. Psychological Bulletin, 108(2),\n314–329.\n\n\nMcDonald, R. P. (2013). Test theory: A unified treatment.\nPsychology Press.\n\n\nNunnally, J. C. (1994). Psychometric theory. McGraw-Hill.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With\napplied examples in r. CRC Press.\n\n\nRandall, J. (2021). “Color-neutral” is not a thing:\nRedefining construct definition and representation through a\njustice-oriented critical antiracist lens. Educational Measurement:\nIssues and Practice, 40(4), 82–90.\n\n\nRandall, J., Slomp, D., Poe, M., & Oliveri, E. (2023). Disrupting\nwhite supremacy in assessment: Toward a justice-oriented, antiracist\nvalidity framework. In Twin pandemics (pp. 78–86). Routledge.\n\n\nRencher, A. (2002). Methods of multivariate analysis. 2002.\nWiley Publications.\n\n\nReynolds, C. R., & Livingston, R. (2021). Mastering modern\npsychological testing. Springer.\n\n\nRoberts, S., & Pashler, H. (2000). How persuasive is a good fit? A\ncomment on theory testing. Psychological Review,\n107(2), 358–367.\n\n\nRosseel, Y. (2020). Small sample solutions for structural equation\nmodeling. In Small sample size solutions: A guide for applied\nresearchers and practitioners (pp. 226–238). Routledge.\n\n\nSaqr, M., & López-Pernas, S. (2024). Learning analytics methods\nand tutorials: A practical guide using r. Springer Nature.\n\n\nSpearman, C. (1904). General intelligence objectively determined and\nmeasured. American Journal of Psychology, 15, 201–293.\n\n\nSvetina, D., Rutkowski, L., & Rutkowski, D. (2020). Multiple-group\ninvariance with categorical outcomes using updated guidelines: An\nillustration using m plus and the lavaan/semtools packages.\nStructural Equation Modeling: A Multidisciplinary Journal,\n27(1), 111–130.\n\n\nTimmerman, M. E., Voncken, L., & Albers, C. J. (2021). A tutorial on\nregression-based norming of psychological tests with GAMLSS.\nPsychological Methods, 26(3), 357–373.\n\n\nWaller, N. G., & Meehl, P. E. (2002). Risky tests, verisimilitude,\nand path analysis. Psychological Methods, 7(3),\n323–337. https://doi.org/10.1037/1082-989X.7.3.323\n\n\nWechsler, D. (2008). Wechsler adult intelligence scale–fourth edition\n(WAIS–IV). San Antonio, TX: NCS Pearson, 22(498),\n816–827.\n\n\nWind, S. A. (2017). An instructional module on mokken scale analysis.\nEducational Measurement: Issues and Practice, 36(2),\n50–66.\n\n\nWind, S. A. (2024). Item-explanatory mokken scale analysis: Using\nnonparametric item response theory to explore item attributes. The\nJournal of Experimental Education, 1–21.\n\n\nWright, S. (1934). The method of path coefficients. The Annals of\nMathematical Statistics, 5(3), 161–215.\n\n\nWu, H., & Estabrook, R. (2016). Identification of confirmatory\nfactor analysis models of different levels of invariance for ordered\ncategorical outcomes. Psychometrika, 81(4), 1014–1045.\n\n\nXia, Y., & Yang, Y. (2019). RMSEA, CFI, and TLI in structural\nequation modeling with ordered categorical data: The story they tell\ndepends on the estimation methods. Behavior Research Methods,\n51(1), 409–428.",
    "crumbs": [
      "Bibliografia"
    ]
  },
  {
    "objectID": "chapters/appendix/a1_intro_r.html",
    "href": "chapters/appendix/a1_intro_r.html",
    "title": "Appendice A — Linguaggio R",
    "section": "",
    "text": "\\(\\mathsf{R}\\) è un linguaggio di programmazione per l’analisi dei dati, il calcolo e la visualizzazione grafica. È open source ed estensibile, il che significa che il codice sorgente è disponibile per essere esaminato e riutilizzato. Può essere scaricato gratuitamente dal sito web del Comprehensive R Archive Network (CRAN) ed è disponibile per PC, MacOS e sistemi operativi Linux/Unix. Gran parte del core-R è scritto in Fortran o C++, ma molti pacchetti per \\(\\mathsf{R}\\) sono scritti in \\(\\mathsf{R}\\) stesso. Chiunque può aggiungere pacchetti al CRAN o ad altri repository come GitHub o BioConductor. CRAN ha test di garanzia della qualità per garantire che i programmi contribuiti abbiano una documentazione coerente e non falliscano durante l’esecuzione degli esempi forniti. Al momento ci sono migliaia di pacchetti disponibili per \\(\\mathsf{R}\\) e questo numero aumenta quotidianamente.\nPer programmare in \\(\\mathsf{R}\\), è importante seguire le regole sintattiche del linguaggio. Se una riga di codice non è scritta correttamente, l’interprete di \\(\\mathsf{R}\\) segnalerà un errore. Questo può essere difficile per i principianti, ma ci sono due vantaggi nell’atto di scrivere codice.\n\nPrima di poter risolvere un problema con il codice, è necessario comprenderlo e analizzarlo in modo preciso. Questo aiuta a sviluppare una comprensione più profonda del problema e a identificare soluzioni efficaci.\nInoltre, se un programma non funziona correttamente, il programmatore che lo ha scritto è l’unico responsabile. Questo aiuta i programmatori a sviluppare una maggiore autoconsapevolezza e responsabilità nei confronti del proprio lavoro.\n\nInoltre, su Internet è disponibile una vasta gamma di materiali utili per avvicinarsi all’ambiente \\(\\mathsf{R}\\) e aiutare l’utente nell’apprendimento di questo software statistico. Tra le tante introduzioni al linguaggio \\(\\mathsf{R}\\), si veda ad esempio, Introduction to R di Venables, Smith, and the R development core team (2023).",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Linguaggio R</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a2_sums.html",
    "href": "chapters/appendix/a2_sums.html",
    "title": "Appendice B — Simbolo di somma",
    "section": "",
    "text": "B.1 Manipolazione di somme\nLe somme si incontrano costantemente in svariati contesti matematici e statistici quindi abbiamo bisogno di una notazione adeguata che ci consenta di gestirle. La somma dei primi \\(n\\) numeri interi può essere scritta come \\(1+2+\\dots+(n-1)+n\\), dove `\\(\\dots\\)’ ci dice di completare la sequenza definita dai termini che vengono prima e dopo. Ovviamente, una notazione come \\(1+7+\\dots+73.6\\) non avrebbe alcun senso senza qualche altro tipo di precisazione. In generale, nel seguito incontreremo delle somme nella forma\n\\[\nx_1+x_2+\\dots+x_n,\n\\]\ndove \\(x_n\\) è un numero che è stato definito altrove. La notazione precedente, che fa uso dei tre puntini di sospensione, è utile in alcuni contesti ma in altri risulta ambigua. Pertanto la notazione di uso corrente è del tipo\n\\[\n\\sum_{i=1}^n x_i\n\\] e si legge “sommatoria per \\(i\\) che va da \\(1\\) a \\(n\\) di \\(x_i\\)”. Il simbolo \\(\\sum\\) (lettera sigma maiuscola dell’alfabeto greco) indica l’operazione di somma, il simbolo \\(x_i\\) indica il generico addendo della sommatoria, le lettere \\(1\\) ed \\(n\\) indicano i cosiddetti estremi della sommatoria, ovvero l’intervallo (da \\(1\\) fino a \\(n\\) estremi inclusi) in cui deve variare l’indice \\(i\\) allorché si sommano gli addendi \\(x_i\\). Solitamente l’estremo inferiore è \\(1\\) ma potrebbe essere qualsiasi altri numero \\(m &lt; n\\). Quindi\n\\[\n\\sum_{i=1}^n x_i = x_1 + x_{2} + \\dots + x_{n}.\n\\]\nPer esempio, se i valori \\(x\\) sono \\(\\{3, 11, 4, 7\\}\\), si avrà\n\\[\n\\sum_{i=1}^4 x_i = 3+11+4+7 = 25\n\\]\nladdove \\(x_1 = 3\\), \\(x_2 = 11\\), eccetera. La quantità \\(x_i\\) nella formula precedente si dice l’argomento della sommatoria, mentre la variabile \\(i\\), che prende i valori naturali successivi indicati nel simbolo, si dice indice della sommatoria.\nLa notazione di sommatoria può anche essere fornita nella forma seguente\n\\[\n\\sum_{P(i)} x_i\n\\]\ndove \\(P(i)\\) è qualsiasi proposizione riguardante \\(i\\) che può essere vera o falsa. Quando è ovvio che si vogliono sommare tutti i valori di \\(n\\) osservazioni, la notazione può essere semplificata nel modo seguente: \\(\\sum_{i} x_i\\) oppure \\(\\sum x_i\\). Al posto di \\(i\\) si possono trovare altre lettere: \\(k, j, l, \\dots\\),.\nÈ conveniente utilizzare le seguenti regole per semplificare i calcoli che coinvolgono l’operatore della sommatoria.",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Simbolo di somma</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a2_sums.html#manipolazione-di-somme",
    "href": "chapters/appendix/a2_sums.html#manipolazione-di-somme",
    "title": "Appendice B — Simbolo di somma",
    "section": "",
    "text": "B.1.1 Proprietà 1\nLa sommatoria di \\(n\\) valori tutti pari alla stessa costante \\(a\\) è pari a \\(n\\) volte la costante stessa:\n\\[\n\\sum_{i=1}^{n} a = \\underbrace{a + a + \\dots + a} = {n\\text{ volte } a} = n a.\n\\]\n\n\nB.1.2 Proprietà 2 (proprietà distributiva)\nNel caso in cui l’argomento contenga una costante, è possibile riscrivere la sommatoria. Ad esempio con\n\\[\n\\sum_{i=1}^{n} a x_i = a x_1 + a x_2 + \\dots + a x_n\n\\]\nè possibile raccogliere la costante \\(a\\) e fare \\(a(x_1 +x_2 + \\dots + x_n)\\). Quindi possiamo scrivere\n\\[\n\\sum_{i=1}^{n} a x_i = a \\sum_{i=1}^{n} x_i.\n\\]\n\n\nB.1.3 Proprietà 3 (proprietà associativa)\nNel caso in cui\n\\[\n\\sum_{i=1}^{n} (a + x_i) = (a + x_1) + (a + x_1) + \\dots  (a + x_n)\n\\]\nsi ha che\n\\[\n\\sum_{i=1}^{n} (a + x_i) = n a + \\sum_{i=1}^{n} x_i.\n\\]\nÈ dunque chiaro che in generale possiamo scrivere\n\\[\n\\sum_{i=1}^{n} (x_i + y_i) = \\sum_{i=1}^{n} x_i + \\sum_{i=1}^{n} y_i.\n\\]\n\n\nB.1.4 Proprietà 4\nSe deve essere eseguita un’operazione algebrica (innalzamento a potenza, logaritmo, ecc.) sull’argomento della sommatoria, allora tale operazione algebrica deve essere eseguita prima della somma. Per esempio,\n\\[\n\\sum_{i=1}^{n} x_i^2 = x_1^2 + x_2^2 + \\dots + x_n^2 \\neq \\left(\\sum_{i=1}^{n} x_i \\right)^2.\n\\]\n\n\nB.1.5 Proprietà 5\nNel caso si voglia calcolare \\(\\sum_{i=1}^{n} x_i y_i\\), il prodotto tra i punteggi appaiati deve essere eseguito prima e la somma dopo:\n\\[\n\\sum_{i=1}^{n} x_i y_i = x_1 y_1 + x_2 y_2 + \\dots + x_n y_n,\n\\]\ninfatti, \\(a_1 b_1 + a_2 b_2 \\neq (a_1 + a_2)(b_1 + b_2)\\).",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Simbolo di somma</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a2_sums.html#doppia-sommatoria",
    "href": "chapters/appendix/a2_sums.html#doppia-sommatoria",
    "title": "Appendice B — Simbolo di somma",
    "section": "B.2 Doppia sommatoria",
    "text": "B.2 Doppia sommatoria\nÈ possibile incontrare la seguente espressione in cui figurano una doppia sommatoria e un doppio indice:\n\\[\n\\sum_{i=1}^{n}\\sum_{j=1}^{m} x_{ij}.\n\\]\nLa doppia sommatoria comporta che per ogni valore dell’indice esterno, \\(i\\) da \\(1\\) ad \\(n\\), occorre sviluppare la seconda sommatoria per \\(j\\) da \\(1\\) ad \\(m\\). Quindi,\n\\[\n\\sum_{i=1}^{3}\\sum_{j=4}^{6} x_{ij} = (x_{1, 4} + x_{1, 5} + x_{1, 6}) + (x_{2, 4} + x_{2, 5} + x_{2, 6}) + (x_{3, 4} + x_{3, 5} + x_{3, 6}).\n\\]\nUn caso particolare interessante di doppia sommatoria è il seguente:\n\\[\n\\sum_{i=1}^{n}\\sum_{j=1}^{n} x_i y_j\n\\]\nSi può osservare che nella sommatoria interna (quella che dipende dall’indice \\(j\\)), la quantità \\(x_i\\) è costante, ovvero non dipende dall’indice (che è \\(j\\)). Allora possiamo estrarre \\(x_i\\) dall’operatore di sommatoria interna e scrivere\n\\[\n\\sum_{i=1}^{n} \\left( x_i \\sum_{j=1}^{n} y_j \\right).\n\\]\nAllo stesso modo si può osservare che nell’argomento della sommatoria esterna la quantità costituita dalla sommatoria in \\(j\\) non dipende dall’indice \\(i\\) e quindi questa quantità può essere estratta dalla sommatoria esterna. Si ottiene quindi\n\\[\n\\sum_{i=1}^{n}\\sum_{j=1}^{n} x_i y_j = \\sum_{i=1}^{n} \\left( x_i \\sum_{j=1}^{n} y_j \\right) = \\sum_{i=1}^{n} x_i \\sum_{j=1}^{n} y_j.\n\\]\nFacciamo un esercizio. Verifichiamo quanto detto sopra nel caso particolare di \\(x = \\{2, 3, 1\\}\\) e \\(y = \\{1, 4, 9\\}\\), svolgendo prima la doppia sommatoria per poi verificare che quanto così ottenuto sia uguale al prodotto delle due sommatorie.\n\\[\n\\begin{align}\n\\sum_{i=1}^3 \\sum_{j=1}^3 x_i y_j &= x_1y_1 + x_1y_2 + x_1y_3 +\nx_2y_1 + x_2y_2 + x_2y_3 +\nx_3y_1 + x_3y_2 + x_3y_3 \\notag\\\\\n&= 2 \\times (1+4+9) + 3 \\times (1+4+9) + 2 \\times (1+4+9) = 84,\\notag\n\\end{align}\n\\]\novvero\n\\[\n(2 + 3 + 1) \\times (1+4+9) = 84.\n\\]",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Simbolo di somma</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a3_calculus.html",
    "href": "chapters/appendix/a3_calculus.html",
    "title": "Appendice C — Per liberarvi dai terrori preliminari",
    "section": "",
    "text": "C.1 Introduzione ai logaritmi\nFornisco qui la traduzione del primo capitolo di Calculus made easy.\nIl terrore preliminare, che impedisce alla maggior parte dei ragazzi di quinta anche solo di tentare di imparare l’analisi, può essere abolito una volta per tutte semplicemente affermando qual è il significato – in termini di buon senso – dei due simboli principali che sono usati nell’analisi matematica.\nQuesti terribili simboli sono:\nÈ tutto.\nIl logaritmo è una funzione matematica che risponde alla domanda: “quante volte devo moltiplicare un certo numero (chiamato”base”) per ottenere un altro numero?” Matematicamente, questo è espresso come:\n\\[\n\\log_b(a) = x \\iff b^x = a\n\\]\nAd esempio, \\(\\log_2(8) = 3\\) perché \\(2^3 = 8\\).\nNel contesto dei logaritmi, i valori molto piccoli (compresi tra 0 e 1) diventano più grandi (in termini assoluti) e negativi quando applichiamo una funzione logaritmica. Questo è utile per stabilizzare i calcoli, specialmente quando lavoriamo con prodotti di numeri molto piccoli che potrebbero portare a problemi di underflow.\nPer esempio: - \\(\\log(1) = 0\\) - \\(\\log(0.1) = -1\\) - \\(\\log(0.01) = -2\\) - \\(\\log(0.001) = -3\\)\nCome si può vedere, i valori assoluti dei logaritmi crescono man mano che il numero originale si avvicina a zero.\nUna delle proprietà più utili dei logaritmi è che consentono di trasformare un prodotto in una somma:\n\\[\n\\log_b(a \\times c) = \\log_b(a) + \\log_b(c)\n\\]\nQuesta proprietà è estremamente utile in calcoli complessi, come nella statistica bayesiana, dove il prodotto di molte probabilità potrebbe diventare un numero molto piccolo e causare problemi numerici.\nUn’altra proprietà utile dei logaritmi è che un rapporto tra due numeri diventa la differenza dei loro logaritmi:\n\\[\n\\log_b\\left(\\frac{a}{c}\\right) = \\log_b(a) - \\log_b(c)\n\\]\nAnche questa proprietà è molto utilizzata in matematica, specialmente in situazioni in cui è necessario normalizzare i dati.\nIn sintesi, i logaritmi sono strumenti potenti per semplificare e stabilizzare i calcoli matematici. Essi consentono di lavorare più agevolmente con numeri molto grandi o molto piccoli e di trasformare operazioni complesse come prodotti e divisioni in somme e differenze, rendendo i calcoli più gestibili e meno inclini a errori numerici.",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Per liberarvi dai terrori preliminari</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/solutions_probability.html",
    "href": "chapters/appendix/solutions_probability.html",
    "title": "Appendice D — Probabilità",
    "section": "",
    "text": "# Standard library imports\nimport os\n\n# Third-party imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport arviz as az\nimport scipy.stats as stats\nfrom scipy.special import expit  # Funzione logistica\nimport math\nfrom cmdstanpy import cmdstan_path, CmdStanModel\n\n# Configuration\nseed = sum(map(ord, \"stan_poisson_regression\"))\nrng = np.random.default_rng(seed=seed)\naz.style.use(\"arviz-darkgrid\")\n%config InlineBackend.figure_format = \"retina\"\n\n# Define directories\nhome_directory = os.path.expanduser(\"~\")\nproject_directory = f\"{home_directory}/_repositories/psicometria\"\n\n# Print project directory to verify\nprint(f\"Project directory: {project_directory}\")\n\nProject directory: /Users/corradocaudek/_repositories/psicometria\n\n\n\n?sec-prob-on-general-spaces\n?exr-prob-on-general-spaces-1\nPer calcolare questa probabilità in maniera analitica, utilizziamo la seguente uguaglianza:\n\\[\nP(\\text{almeno 2 psicologi clinici}) = 1 - P(\\text{nessun psicologo clinico}) - P(\\text{1 psicologo clinico}).\n\\]\nIl numero totale di modi per selezionare 5 persone dal gruppo di 20 è dato da:\n\\[\n\\binom{20}{5} = \\frac{20!}{5!(15!)} = 15,504.\n\\]\nIl numero di modi per avere nessun psicologo clinico nella commissione (ovvero, selezionare solo psicologi del lavoro) è:\n\\[\n\\binom{10}{0} \\times \\binom{10}{5} = 1 \\times 252 = 252.\n\\]\nQuindi, la probabilità di avere nessun psicologo clinico è:\n\\[\nP(\\text{nessun psicologo clinico}) = \\frac{252}{15,504} \\approx 0.016.\n\\]\nIl numero di modi per avere esattamente 1 psicologo clinico nella commissione è:\n\\[\n\\binom{10}{1} \\times \\binom{10}{4} = 10 \\times 210 = 2,100.\n\\]\nQuindi, la probabilità di avere esattamente 1 psicologo clinico è:\n\\[\nP(\\text{1 psicologo clinico}) = \\frac{2,100}{15,504} \\approx 0.135.\n\\]\nLa probabilità di avere almeno 2 psicologi clinici nella commissione è quindi:\n\\[\n\\begin{align}\nP(\\text{almeno 2 psicologi clinici}) &= 1 - P(\\text{nessun psicologo clinico}) - P(\\text{1 psicologo clinico}) \\notag\\\\\n&= 1 - 0.016 - 0.135 \\notag\\\\\n&= 0.848.\\notag\n\\end{align}\n\\]\nQuindi, la probabilità che almeno 2 psicologi clinici siano nella commissione è circa 0.848.\n\n# Funzione per calcolare le combinazioni\ndef nCk(n, k):\n    return math.factorial(n) // (math.factorial(k) * math.factorial(n - k))\n\n\n# Calcolo delle probabilità per il problema della commissione\ntotal_ways = nCk(20, 5)\nno_clinical = nCk(10, 0) * nCk(10, 5)\none_clinical = nCk(10, 1) * nCk(10, 4)\n\np_no_clinical = no_clinical / total_ways\np_one_clinical = one_clinical / total_ways\n\np_at_least_two_clinical = 1 - p_no_clinical - p_one_clinical\n\nprint(f\"Probabilità di almeno 2 psicologi clinici: {p_at_least_two_clinical:.3f}\")\n\nProbabilità di almeno 2 psicologi clinici: 0.848\n\n\nIn maniera più intuitiva, possiamo risolvere il problema con una simulazione Monte Carlo.\n\nimport random\n\n# Numero di simulazioni\nsimulations = 1000000\n\n# Numero di successi (almeno 2 psicologi clinici nella commissione)\nsuccess_count = 0\n\n# Creiamo una lista che rappresenta il gruppo di 20 persone\n# 1 rappresenta un psicologo clinico, 0 rappresenta un psicologo del lavoro\ngroup = [1] * 10 + [0] * 10\n\n# Simulazione Monte Carlo\nfor _ in range(simulations):\n    # Estrai casualmente 5 persone dal gruppo\n    committee = random.sample(group, 5)\n\n    # Conta quanti psicologi clinici ci sono nella commissione\n    num_clinical_psychologists = sum(committee)\n\n    # Verifica se ci sono almeno 2 psicologi clinici\n    if num_clinical_psychologists &gt;= 2:\n        success_count += 1\n\n# Calcola la probabilità\nprobability = success_count / simulations\n\n# Mostra il risultato\nprint(\n    f\"La probabilità che almeno 2 psicologi clinici siano nella commissione è: {probability:.4f}\"\n)\n\nLa probabilità che almeno 2 psicologi clinici siano nella commissione è: 0.8482\n\n\n\n\n?sec-simulations\n?exr-prob-simulation-1\nPer calcolare le deviazioni standard delle distribuzioni gaussiane date le percentuali di studenti che ottengono meno di 18, possiamo utilizzare le proprietà della distribuzione normale e i quantili della distribuzione normale standard (distribuzione normale con media 0 e deviazione standard 1).\nLe distribuzioni normali hanno la proprietà che possiamo trasformare qualsiasi valore \\(X\\) della distribuzione \\(N(\\mu, \\sigma)\\) nella distribuzione normale standard \\(N(0, 1)\\) tramite la formula:\n\\[ Z = \\frac{X - \\mu}{\\sigma}, \\]\ndove \\(Z\\) è il quantile standardizzato.\nPer trovare il valore di \\(\\sigma\\) dato un certo percentile, utilizziamo l’inverso della funzione di distribuzione cumulativa (CDF) della distribuzione normale standard. Per un dato percentile \\(p\\), \\(z_p\\) è tale che:\n\\[ p = P(Z \\leq z_p) \\]\nQuindi possiamo trovare \\(\\sigma\\) risolvendo per \\(\\sigma\\) nella formula:\n\\[ z_p = \\frac{X - \\mu}{\\sigma}, \\]\n\\[ \\sigma = \\frac{X - \\mu}{z_p}, \\]\ndove:\n\n\\(X\\) è il punteggio di soglia (18 in questo caso).\n\\(\\mu\\) è la media della distribuzione.\n\\(z_p\\) è il quantile della distribuzione normale standard per il percentile \\(p\\).\n\nI quantili della distribuzione normale standard per i percentili desiderati sono:\n\nPer il 15%, il quantile è \\(z_{0.15} \\approx -1.036\\).\nPer il 10%, il quantile è \\(z_{0.10} \\approx -1.281\\).\nPer il 5%, il quantile è \\(z_{0.05} \\approx -1.645\\).\n\nPrima Prova\n\nMedia: \\(\\mu = 24\\)\nPercentuale che ottiene meno di 18: 15%\nQuantile: \\(z_{0.15} = -1.036\\)\nSoglia: \\(X = 18\\)\n\n\\[ \\sigma_1 = \\frac{24 - 18}{1.036} \\approx 5.79 \\]\nSeconda Prova\n\nMedia: \\(\\mu = 25\\)\nPercentuale che ottiene meno di 18: 10%\nQuantile: \\(z_{0.10} = -1.281\\)\nSoglia: \\(X = 18\\)\n\n\\[ \\sigma_2 = \\frac{25 - 18}{1.281} \\approx 5.46 \\]\nTerza Prova\n\nMedia: \\(\\mu = 26\\)\nPercentuale che ottiene meno di 18: 5%\nQuantile: \\(z_{0.05} = -1.645\\)\nSoglia: \\(X = 18\\)\n\n\\[ \\sigma_3 = \\frac{26 - 18}{1.645} \\approx 4.86 \\]\n\n# Funzione per calcolare la deviazione standard data la media, la soglia e il quantile\ndef calculate_std(mean, threshold, quantile):\n    return abs((mean - threshold) / quantile)\n\n\n# Parametri delle distribuzioni gaussiane per le tre prove\nmean_test1 = 24\nstd_test1 = calculate_std(mean_test1, 18, -1.036)\nmean_test2 = 25\nstd_test2 = calculate_std(mean_test2, 18, -1.281)\nmean_test3 = 26\nstd_test3 = calculate_std(mean_test3, 18, -1.645)\n\n# Numero di studenti\nn_students = 220\n\n# Percentuale di studenti che non fa le prove\ndrop_test1 = 0.10\ndrop_test2 = 0.05\n\n# Seed per il generatore di numeri casuali basato sulla stringa \"simulation\"\nseed = sum(map(ord, \"simulation\"))\nrng = np.random.default_rng(seed=seed)\n\n# Generazione dei voti per le tre prove\n# Genera i voti solo per gli studenti che partecipano alla prova\ntest1_scores = np.where(\n    rng.random(n_students) &gt; drop_test1,\n    rng.normal(mean_test1, std_test1, n_students),\n    np.nan,\n)\ntest2_scores = np.where(\n    rng.random(n_students) &gt; drop_test2,\n    rng.normal(mean_test2, std_test2, n_students),\n    np.nan,\n)\ntest3_scores = rng.normal(mean_test3, std_test3, n_students)\n\n# Calcola il voto finale solo per gli studenti che hanno partecipato a tutte e tre le prove\nfinal_scores = np.nanmean(\n    np.column_stack((test1_scores, test2_scores, test3_scores)), axis=1\n)\n\n# Filtra gli studenti che non hanno partecipato a tutte e tre le prove\nvalid_final_scores = final_scores[~np.isnan(final_scores)]\n\n# Visualizzazione della distribuzione finale dei voti\nplt.hist(valid_final_scores, bins=30, edgecolor=\"black\")\nplt.title(\"Distribuzione dei voti finali\")\nplt.xlabel(\"Voto finale\")\nplt.ylabel(\"Frequenza\")\nplt.show()\n\n# Statistiche descrittive dei voti finali\nmean_final_score = np.mean(valid_final_scores)\nmedian_final_score = np.median(valid_final_scores)\nstd_final_score = np.std(valid_final_scores)\n\nprint(f\"Media dei voti finali: {mean_final_score:.2f}\")\nprint(f\"Mediana dei voti finali: {median_final_score:.2f}\")\nprint(f\"Deviazione standard dei voti finali: {std_final_score:.2f}\")",
    "crumbs": [
      "Appendici",
      "Soluzioni degli esercizi",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Probabilità</span>"
    ]
  },
  {
    "objectID": "chapters/fa/05_factor_scores.html#introduzione",
    "href": "chapters/fa/05_factor_scores.html#introduzione",
    "title": "23  I punteggi fattoriali",
    "section": "",
    "text": "23.1.1 Passaggi per l’interpretazione di una matrice fattoriale ruotata\nLa procedura per interpretare una matrice dei carichi fattoriali dopo una rotazione può essere articolata nei seguenti passaggi:\n\nDefinizione di una soglia per i carichi fattoriali\nSi stabilisce un valore soglia (ad esempio, |.40|) al di sotto del quale le saturazioni vengono considerate troppo deboli per contribuire significativamente alla definizione del fattore. Valori più alti (es. .50 o .60) o più bassi possono essere usati in base al numero di item e alla chiarezza della soluzione.\nOrdinamento dei carichi\nLe saturazioni (in valore assoluto) vengono ordinate in senso decrescente per ciascun fattore, partendo da quelle più forti fino alla soglia stabilita.\nIdentificazione delle variabili\nA ciascun carico si associa la descrizione della variabile o dell’item corrispondente, per facilitare l’analisi del contenuto.\nAttribuzione di un’etichetta teorica al fattore\nConsiderando il contenuto degli item, il dominio teorico di riferimento e i risultati di studi precedenti, si cerca di individuare un tratto psicologico comune che li unisca. Tale tratto definisce l’etichetta interpretativa del fattore. Gli item con carichi più elevati contribuiscono maggiormente alla definizione di questo tratto.\nInterpretazione del segno delle saturazioni\nIl segno negativo di un carico indica semplicemente una direzione opposta rispetto alle saturazioni positive. Il tratto psicologico rappresentato dal fattore può essere visto come un continuum: le variabili con segno opposto si trovano alle estremità opposte di tale continuum. È spesso utile iniziare l’interpretazione considerando le saturazioni con il segno più frequente come “positive”, per poi interpretare le altre come opposte.\nFattori non interpretabili\nSe non si riesce a identificare un tratto comune tra le variabili che saturano su un fattore, si può concludere che il fattore non è interpretabile. Questo può essere dovuto a errori di campionamento, rumore nella misurazione, o alla presenza di variabili che non condividono un contenuto psicologico coerente. È comune che i primi fattori estratti siano interpretabili, mentre gli ultimi — soprattutto se si sono estratti molti fattori o se la matrice delle correlazioni è debole — risultino ambigui o saturi di una sola variabile, diventando di fatto fattori “specifici”. Se molti fattori non risultano interpretabili, è preferibile rivalutare l’intera analisi fattoriale.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>I punteggi fattoriali</span>"
    ]
  },
  {
    "objectID": "chapters/fa/05_factor_scores.html#esempio-di-interpretazione-la-scala-wisc-iii",
    "href": "chapters/fa/05_factor_scores.html#esempio-di-interpretazione-la-scala-wisc-iii",
    "title": "23  I punteggi fattoriali",
    "section": "\n23.2 Esempio di interpretazione: la scala WISC-III",
    "text": "23.2 Esempio di interpretazione: la scala WISC-III\nIl WISC-III (Wechsler Intelligence Scale for Children – III) è uno strumento standardizzato per la valutazione dell’intelligenza nei bambini e adolescenti dai 6 ai 16 anni e 11 mesi. I subtest che lo compongono sono progettati per misurare differenti abilità cognitive, contribuendo alla stima del Quoziente Intellettivo globale.\nAlcuni subtest richiedono abilità verbali e di ragionamento astratto, altri valutano la memoria a breve termine, altri ancora sollecitano competenze visuo-percettive e di coordinazione motoria.\n\n23.2.1 Matrice di correlazione tra i subtest\nConsideriamo ora la matrice di correlazione tra i subtest del WISC-III (tratta dal manuale italiano):\n\nlower &lt;- '\n1\n.66      1\n.57 .55      1\n.70 .69 .54       1\n.56 .59 .47 .64      1\n.34 .34 .43 .35 .29      1\n.47 .45 .39 .45 .38 .25      1\n.21 .20 .27 .26 .25 .23 .18      1\n.40 .39 .35 .40 .35 .20 .37 .28      1\n.48 .49 .52 .46 .40 .32 .52 .27 .41      1\n.41 .42 .39 .41 .34 .26 .49 .24 .37 .61      1\n.35 .35 .41 .35 .34 .28 .33 .53 .36 .45 .38      1\n.18 .18 .22 .17 .17 .14 .24 .15 .23 .31 .29 .24     1\n'\n\n\nwisc_III_cov &lt;- getCov(\n  lower,\n  names = c(\n    \"INFO\", \"SIM\", \"ARITH\", \"VOC\", \"COMP\", \"DIGIT\", \"PICTCOM\",\n    \"CODING\", \"PICTARG\", \"BLOCK\", \"OBJECT\", \"SYMBOL\", \"MAZES\"\n  )\n)\nwisc_III_cov\n#&gt;         INFO  SIM ARITH  VOC COMP DIGIT PICTCOM CODING PICTARG BLOCK OBJECT\n#&gt; INFO    1.00 0.66  0.57 0.70 0.56  0.34    0.47   0.21    0.40  0.48   0.41\n#&gt; SIM     0.66 1.00  0.55 0.69 0.59  0.34    0.45   0.20    0.39  0.49   0.42\n#&gt; ARITH   0.57 0.55  1.00 0.54 0.47  0.43    0.39   0.27    0.35  0.52   0.39\n#&gt; VOC     0.70 0.69  0.54 1.00 0.64  0.35    0.45   0.26    0.40  0.46   0.41\n#&gt; COMP    0.56 0.59  0.47 0.64 1.00  0.29    0.38   0.25    0.35  0.40   0.34\n#&gt; DIGIT   0.34 0.34  0.43 0.35 0.29  1.00    0.25   0.23    0.20  0.32   0.26\n#&gt; PICTCOM 0.47 0.45  0.39 0.45 0.38  0.25    1.00   0.18    0.37  0.52   0.49\n#&gt; CODING  0.21 0.20  0.27 0.26 0.25  0.23    0.18   1.00    0.28  0.27   0.24\n#&gt; PICTARG 0.40 0.39  0.35 0.40 0.35  0.20    0.37   0.28    1.00  0.41   0.37\n#&gt; BLOCK   0.48 0.49  0.52 0.46 0.40  0.32    0.52   0.27    0.41  1.00   0.61\n#&gt; OBJECT  0.41 0.42  0.39 0.41 0.34  0.26    0.49   0.24    0.37  0.61   1.00\n#&gt; SYMBOL  0.35 0.35  0.41 0.35 0.34  0.28    0.33   0.53    0.36  0.45   0.38\n#&gt; MAZES   0.18 0.18  0.22 0.17 0.17  0.14    0.24   0.15    0.23  0.31   0.29\n#&gt;         SYMBOL MAZES\n#&gt; INFO      0.35  0.18\n#&gt; SIM       0.35  0.18\n#&gt; ARITH     0.41  0.22\n#&gt; VOC       0.35  0.17\n#&gt; COMP      0.34  0.17\n#&gt; DIGIT     0.28  0.14\n#&gt; PICTCOM   0.33  0.24\n#&gt; CODING    0.53  0.15\n#&gt; PICTARG   0.36  0.23\n#&gt; BLOCK     0.45  0.31\n#&gt; OBJECT    0.38  0.29\n#&gt; SYMBOL    1.00  0.24\n#&gt; MAZES     0.24  1.00\n\n\n23.2.2 Analisi fattoriale\nApplichiamo un’analisi delle componenti principali con rotazione Varimax (ortogonale), e scegliamo di estrarre 3 fattori:\n\nf_pc &lt;- psych::principal(wisc_III_cov, nfactors = 3, rotate = \"varimax\")\nprint(f_pc)\n#&gt; Principal Components Analysis\n#&gt; Call: psych::principal(r = wisc_III_cov, nfactors = 3, rotate = \"varimax\")\n#&gt; Standardized loadings (pattern matrix) based upon correlation matrix\n#&gt;           RC1  RC3  RC2   h2   u2 com\n#&gt; INFO     0.80 0.25 0.09 0.72 0.28 1.2\n#&gt; SIM      0.81 0.25 0.08 0.72 0.28 1.2\n#&gt; ARITH    0.65 0.26 0.28 0.57 0.43 1.7\n#&gt; VOC      0.83 0.19 0.13 0.75 0.25 1.2\n#&gt; COMP     0.75 0.14 0.16 0.60 0.40 1.2\n#&gt; DIGIT    0.45 0.06 0.36 0.34 0.66 2.0\n#&gt; PICTCOM  0.43 0.61 0.02 0.56 0.44 1.8\n#&gt; CODING   0.10 0.09 0.88 0.79 0.21 1.0\n#&gt; PICTARG  0.34 0.45 0.27 0.39 0.61 2.6\n#&gt; BLOCK    0.41 0.66 0.22 0.66 0.34 1.9\n#&gt; OBJECT   0.31 0.71 0.14 0.62 0.38 1.5\n#&gt; SYMBOL   0.23 0.32 0.74 0.70 0.30 1.6\n#&gt; MAZES   -0.06 0.71 0.11 0.51 0.49 1.1\n#&gt; \n#&gt;                        RC1  RC3  RC2\n#&gt; SS loadings           3.80 2.37 1.74\n#&gt; Proportion Var        0.29 0.18 0.13\n#&gt; Cumulative Var        0.29 0.47 0.61\n#&gt; Proportion Explained  0.48 0.30 0.22\n#&gt; Cumulative Proportion 0.48 0.78 1.00\n#&gt; \n#&gt; Mean item complexity =  1.5\n#&gt; Test of the hypothesis that 3 components are sufficient.\n#&gt; \n#&gt; The root mean square of the residuals (RMSR) is  0.07 \n#&gt; \n#&gt; Fit based upon off diagonal values = 0.97\n\n\n23.2.3 Interpretazione dei fattori\nDai risultati:\n\nI primi cinque subtest (“Informazioni”, “Somiglianze”, “Aritmetica”, “Vocabolario”, “Comprensione”) presentano saturazioni elevate (oltre .60) sul primo fattore. Questi item implicano la comprensione e produzione di linguaggio, e richiedono un buon livello di ragionamento verbale. È quindi coerente denominare questo fattore Comprensione Verbale.\nI subtest “Cifrario” e “Ricerca di simboli” saturano sul secondo fattore. Entrambi implicano rapidità nella codifica di simboli visivi e attenzione sostenuta. Il fattore può essere chiamato Velocità di Elaborazione.\nI subtest “Completamento di figure”, “Disegno con i cubi”, “Riordinamento di storie figurate” e “Labirinti” saturano sul terzo fattore. Questi subtest richiedono abilità visuo-spaziali e organizzazione percettiva. Il fattore può essere denominato Organizzazione Percettiva.\n\n23.2.4 Calcolo delle comunalità\nNel caso di una rotazione ortogonale, la comunalità di ciascuna variabile è data dalla somma dei quadrati dei suoi carichi sui fattori. Calcoliamole:\n\nh2 &lt;- rep(0, 13)\nfor (i in 1:13) {\n  h2[i] &lt;- sum(f_pc$loadings[i, ]^2)\n}\nround(h2, 2)\n#&gt;  [1] 0.72 0.72 0.57 0.75 0.60 0.34 0.56 0.79 0.39 0.66 0.62 0.70 0.51\n\nLe comunalità rappresentano la proporzione della varianza di ciascun subtest spiegata dai fattori estratti. Valori elevati indicano che il fattore riesce a spiegare bene il comportamento della variabile. Questi risultati riproducono quelli riportati nel manuale del WISC-III, confermando la validità dell’interpretazione proposta.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>I punteggi fattoriali</span>"
    ]
  },
  {
    "objectID": "chapters/fa/05_factor_scores.html#dimostrazione-di-thurstone",
    "href": "chapters/fa/05_factor_scores.html#dimostrazione-di-thurstone",
    "title": "23  I punteggi fattoriali",
    "section": "\n23.4 Dimostrazione di Thurstone",
    "text": "23.4 Dimostrazione di Thurstone\nPer illustrare in modo intuitivo il significato dei punteggi fattoriali, riprendiamo un esempio proposto da Thurstone (1947) (citato in Loehlin, 1987), poi ripreso da Jennrich (2007). L’idea è di simulare un contesto in cui i fattori latenti sono noti, così da poter valutare la bontà della stima ottenuta dai punteggi fattoriali.\nSupponiamo di avere 1000 scatole le cui dimensioni reali (\\(x\\), \\(y\\), \\(z\\)) sono note:\n\nset.seed(123)\nn &lt;- 1e3\nx &lt;- rnorm(n, 100, 1.5)\ny &lt;- rnorm(n, 200, 1.5)\nz &lt;- rnorm(n, 300, 1.5)\n\nOra immaginiamo di disporre solo di misurazioni indirette e rumorose delle scatole. Queste misurazioni sono derivate da trasformazioni non lineari e prodotti tra le dimensioni, più un errore casuale:\n\ns &lt;- 40\ny1 &lt;- rnorm(n, mean(x), s)\ny2 &lt;- rnorm(n, mean(y), s)\ny3 &lt;- rnorm(n, mean(z), s)\ny4 &lt;- x * y + rnorm(n, 0, s)\ny5 &lt;- x * z + rnorm(n, 0, s)\ny6 &lt;- y * z + rnorm(n, 0, s)\ny7 &lt;- x^2 * y + rnorm(n, 0, s)\ny8 &lt;- x * y^2 + rnorm(n, 0, s)\ny9 &lt;- x^2 * z + rnorm(n, 0, s)\ny10 &lt;- x * z^2 + rnorm(n, 0, s)\ny11 &lt;- y^2 * z + rnorm(n, 0, s)\ny12 &lt;- y * z^2 + rnorm(n, 0, s)\ny13 &lt;- y^2 * z + rnorm(n, 0, s)\ny14 &lt;- y * z^2 + rnorm(n, 0, s)\ny15 &lt;- x / y + rnorm(n, 0, s)\ny16 &lt;- y / x + rnorm(n, 0, s)\ny17 &lt;- x / z + rnorm(n, 0, s)\ny18 &lt;- z / x + rnorm(n, 0, s)\ny19 &lt;- y / z + rnorm(n, 0, s)\ny20 &lt;- z / y + rnorm(n, 0, s)\ny21 &lt;- 2 * x + 2 * y + rnorm(n, 0, s)\ny22 &lt;- 2 * x + 2 * z + rnorm(n, 0, s)\ny23 &lt;- 2 * y + 2 * z + rnorm(n, 0, s)\n\nMettiamo insieme tutte queste variabili in una matrice:\n\nY &lt;- cbind(\n  y1, y2, y3, y4, y5, y6, y7, y8, y9, \n  y10, y11, y12, y13, y14, y15, y16, \n  y17, y18, y19, y20, y21, y22, y23\n)\n\n\n23.4.1 Analisi fattoriale\nEseguiamo ora un’analisi fattoriale con tre fattori, utilizzando il metodo di regressione per la stima dei punteggi:\n\nfa &lt;- factanal(\n  Y, \n  factors = 3, \n  scores = \"regression\",\n  lower = 0.01\n)\n\nCon scores = \"regression\" chiediamo la stima dei punteggi fattoriali secondo il metodo di Thomson. Inoltre, poiché factanal() utilizza per default una rotazione ortogonale Varimax, i punteggi stimati risultano incorrelati tra loro (fatto che possiamo verificare con la matrice delle correlazioni tra fattori):\n\ncor(fa$scores) %&gt;% round(3)\n#&gt;         Factor1 Factor2 Factor3\n#&gt; Factor1   1.000   0.002  -0.001\n#&gt; Factor2   0.002   1.000   0.005\n#&gt; Factor3  -0.001   0.005   1.000\n\n\n23.4.2 Validazione dei punteggi: confronto con i fattori noti\nSe la procedura ha funzionato, ci aspettiamo che:\n\nciascun punteggio fattoriale sia fortemente correlato con una sola delle dimensioni originarie delle scatole (\\(x\\), \\(y\\), \\(z\\)),\nle altre correlazioni siano molto più deboli.\n\nPossiamo visualizzare questi rapporti con dei diagrammi di dispersione:\n\n# Primo fattore\np1 &lt;- ggplot(tibble(x, fs1 = fa$scores[, 1]), aes(x, fs1)) + geom_point(alpha = 0.2)\np2 &lt;- ggplot(tibble(y, fs1 = fa$scores[, 1]), aes(y, fs1)) + geom_point(alpha = 0.2)\np3 &lt;- ggplot(tibble(z, fs1 = fa$scores[, 1]), aes(z, fs1)) + geom_point(alpha = 0.2)\n\n# Secondo fattore\np4 &lt;- ggplot(tibble(x, fs2 = fa$scores[, 2]), aes(x, fs2)) + geom_point(alpha = 0.2)\np5 &lt;- ggplot(tibble(y, fs2 = fa$scores[, 2]), aes(y, fs2)) + geom_point(alpha = 0.2)\np6 &lt;- ggplot(tibble(z, fs2 = fa$scores[, 2]), aes(z, fs2)) + geom_point(alpha = 0.2)\n\n# Terzo fattore\np7 &lt;- ggplot(tibble(x, fs3 = fa$scores[, 3]), aes(x, fs3)) + geom_point(alpha = 0.2)\np8 &lt;- ggplot(tibble(y, fs3 = fa$scores[, 3]), aes(y, fs3)) + geom_point(alpha = 0.2)\np9 &lt;- ggplot(tibble(z, fs3 = fa$scores[, 3]), aes(z, fs3)) + geom_point(alpha = 0.2)\n\n\n(p1 | p2 | p3) /\n(p4 | p5 | p6) /\n(p7 | p8 | p9) \n\n\n\n\n\n\n\n\n23.4.3 Interpretazione\nI grafici confermano l’ipotesi: ciascun punteggio fattoriale è chiaramente associato a una sola delle dimensioni originali (\\(x\\), \\(y\\), o \\(z\\)), a conferma del fatto che l’analisi fattoriale è stata in grado di recuperare le dimensioni latenti a partire da misurazioni indirette e rumorose.\nQuesto esempio illustra molto bene:\n\nil significato dei punteggi fattoriali,\nla loro interpretabilità,\ne il modo in cui possono essere utilizzati per stimare valori latenti individuali.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>I punteggi fattoriali</span>"
    ]
  },
  {
    "objectID": "chapters/fa/05_factor_scores.html#il-metodo-della-regressione-per-la-stima-dei-punteggi-fattoriali",
    "href": "chapters/fa/05_factor_scores.html#il-metodo-della-regressione-per-la-stima-dei-punteggi-fattoriali",
    "title": "23  I punteggi fattoriali",
    "section": "\n23.5 Il Metodo della Regressione per la stima dei punteggi fattoriali",
    "text": "23.5 Il Metodo della Regressione per la stima dei punteggi fattoriali\nUna volta stimato un modello fattoriale (ovvero, una soluzione con un certo numero di fattori e una matrice dei pesi fattoriali \\(\\hat{\\boldsymbol{\\Lambda}}\\)), è possibile stimare per ciascun individuo i punteggi fattoriali: si tratta di stime dei livelli dei fattori latenti per ogni rispondente. Il metodo della regressione, proposto da Thomson, è uno dei più diffusi per questo scopo.\n\n23.5.1 L’idea di base\nIl metodo della regressione parte da un concetto semplice ma potente: possiamo considerare ciascun fattore latente come una funzione lineare delle variabili osservate. Cioè, immaginiamo che ciascun punteggio fattoriale derivi da una regressione delle variabili osservate sul fattore. Per esempio, per il fattore \\(F_j\\) (il fattore \\(j\\)-esimo), possiamo scrivere:\n\\[\nF_j = \\beta_{1j} y_1 + \\beta_{2j} y_2 + \\dots + \\beta_{pj} y_p + \\varepsilon_j,\n\\]\ndove:\n\n\n\\(y_1, y_2, \\dots, y_p\\) sono le variabili osservate standardizzate (cioè con media 0 e deviazione standard 1),\n\n\\(\\beta_{ij}\\) sono i coefficienti di regressione da stimare,\n\n\\(\\varepsilon_j\\) è l’errore di stima sul fattore \\(j\\).\n\nQuesta equazione indica che il fattore \\(F_j\\) può essere predetto a partire da una combinazione lineare delle variabili osservate.\n\n23.5.2 Forma matriciale del modello\nPer semplicità, possiamo scrivere tutto in forma matriciale. Supponiamo di avere \\(p\\) variabili osservate e \\(m\\) fattori.\n\n\n\\(\\mathbf{y}\\) è il vettore riga delle variabili osservate standardizzate (lunghezza \\(p\\)),\n\n\\(\\mathbf{F}\\) è il vettore riga dei fattori (lunghezza \\(m\\)),\n\n\\(\\mathbf{B}\\) è la matrice dei coefficienti di regressione (dimensione \\(p \\times m\\)),\n\n\\(\\boldsymbol{\\varepsilon}\\) è il vettore degli errori di regressione.\n\nIl modello diventa:\n\\[\n\\mathbf{F} = \\mathbf{y} \\mathbf{B} + \\boldsymbol{\\varepsilon}\n\\]\nIl nostro obiettivo è stimare \\(\\mathbf{B}\\), cioè i pesi che ci permettono di calcolare i punteggi fattoriali a partire dalle osservazioni.\n\n23.5.3 Stima della matrice dei coefficienti di regressione\nPer stimare \\(\\mathbf{B}\\) si usa una formula ben nota dell’algebra della regressione lineare:\n\\[\n\\hat{\\mathbf{B}} = \\mathbf{R}_{yy}^{-1} \\mathbf{R}_{yf}\n\\]\ndove:\n\n\n\\(\\mathbf{R}_{yy}\\) è la matrice delle correlazioni tra le variabili osservate (dimensione \\(p \\times p\\)),\n\n\\(\\mathbf{R}_{yf}\\) è la matrice delle correlazioni tra le variabili osservate e i fattori (dimensione \\(p \\times m\\)).\n\nQuest’ultima matrice, \\(\\mathbf{R}_{yf}\\), è nota anche come matrice di struttura fattoriale, e coincide con la matrice dei carichi fattoriali \\(\\hat{\\boldsymbol{\\Lambda}}\\) se i fattori sono ortogonali (cioè incorrelati tra loro, come nel caso di una rotazione Varimax).\nPertanto, in presenza di fattori ortogonali:\n\\[\n\\hat{\\mathbf{B}} = \\mathbf{R}^{-1} \\hat{\\boldsymbol{\\Lambda}},\n\\]\ndove \\(\\mathbf{R}\\) è la matrice delle correlazioni tra le variabili osservate.\n\n23.5.4 Calcolo dei punteggi fattoriali\nUna volta ottenuti i coefficienti di regressione \\(\\hat{\\mathbf{B}}\\), possiamo calcolare i punteggi fattoriali per ciascun individuo applicando la formula del modello di regressione:\n\\[\n\\hat{\\mathbf{F}} = \\mathbf{y} \\hat{\\mathbf{B}} = \\mathbf{y} \\mathbf{R}^{-1} \\hat{\\boldsymbol{\\Lambda}}\n\\]\ndove:\n\n\n\\(\\mathbf{y}\\) è la matrice delle variabili osservate standardizzate (righe = individui, colonne = variabili),\n\n\\(\\hat{\\mathbf{F}}\\) è la matrice dei punteggi fattoriali (righe = individui, colonne = fattori).\n\n23.5.5 Interpretazione\n\nOgni riga di \\(\\hat{\\mathbf{F}}\\) contiene i valori stimati dei fattori latenti per un individuo.\nCiascun valore rappresenta la posizione dell’individuo su uno specifico fattore, espressa su una scala con media zero (poiché ottenuta da variabili standardizzate).\n\n23.5.6 Vantaggi del metodo della regressione\n\n\nSemplice da calcolare: basta conoscere la matrice delle correlazioni e i carichi fattoriali.\n\nPunteggi stimati per tutti i soggetti: anche se il numero di soggetti è diverso dal numero di variabili.\n\nUtilizzabile in ulteriori analisi: ad esempio per confrontare gruppi sui fattori latenti, per costruire profili individuali o per analisi predittive.\n\n23.5.7 Limiti\n\nI punteggi stimati non sono indipendenti dagli errori di misura: il metodo minimizza l’errore medio quadratico, ma non fornisce stime “pure” dei fattori.\nSe i fattori sono obliqui (cioè correlati), la formula deve essere modificata e il calcolo diventa più complesso.\n\n\nEsempio 23.1 Esaminiamo ora un esempio pratico in cui calcoliamo i punteggi fattoriali del fattore Comprensione Verbale del WISC-III seguendo due strade:\n\n\nutilizzando direttamente la formula della regressione in forma matriciale: \\(\\hat{\\mathbf{F}} = \\mathbf{Y} \\, \\mathbf{R}^{-1} \\hat{\\boldsymbol{\\Lambda}}\\);\n\nconfrontando il risultato con quello ottenuto da factanal() in R usando scores = \"regression\".\n\nUtilizzeremo solo le prime 5 variabili del test WISC-III, che saturano principalmente sul primo fattore (Comprensione Verbale): INFO, SIM, ARITH, VOC, COMP.\n\nwisc_names &lt;- c(\n    \"INFO\", \"SIM\", \"ARITH\", \"VOC\", \"COMP\", \"DIGIT\", \"PICTCOM\",\n    \"CODING\", \"PICTARG\", \"BLOCK\", \"OBJECT\", \"SYMBOL\", \"MAZES\"\n  )\n\nwisc_III_cov &lt;- getCov(lower, names = wisc_names)\n\n# solo le prime 5 variabili (Comprensione Verbale)\nR &lt;- wisc_III_cov[1:5, 1:5]\nR\n#&gt;       INFO  SIM ARITH  VOC COMP\n#&gt; INFO  1.00 0.66  0.57 0.70 0.56\n#&gt; SIM   0.66 1.00  0.55 0.69 0.59\n#&gt; ARITH 0.57 0.55  1.00 0.54 0.47\n#&gt; VOC   0.70 0.69  0.54 1.00 0.64\n#&gt; COMP  0.56 0.59  0.47 0.64 1.00\n\nSimuliamo dati osservati standardizzati (media 0, sd 1):\n\nset.seed(123)\nn &lt;- 100  # 100 soggetti\nY_std &lt;- mvrnorm(n = n, mu = rep(0, 5), Sigma = R)  # dati simulati coerenti con R\n\nY_std |&gt; head()\n#&gt;        INFO     SIM   ARITH     VOC     COMP\n#&gt; [1,] 0.4526 -0.4134 -0.5654 -0.1982 -1.70021\n#&gt; [2,] 0.1469 -0.3958 -0.6263  0.5940 -0.78535\n#&gt; [3,] 1.4065  0.7505  1.4028  1.6446  1.23083\n#&gt; [4,] 0.5866 -0.3431  0.1102  0.1921 -0.27661\n#&gt; [5,] 0.3441 -0.1589  0.7462 -0.3431  0.02788\n#&gt; [6,] 0.7813  1.3285  1.5152  2.1020  1.34216\n\nStandardizzazione ulteriore (già è standardizzato, ma è buono verificare):\n\ncolnames(Y_std) &lt;- colnames(R)\nY_std &lt;- scale(Y_std)  \n\nEstraiamo 1 solo fattore (Comprensione Verbale):\n\nfa &lt;- factanal(Y_std, factors = 1, scores = \"regression\")\n\nLa matrice dei carichi fattoriali:\n\nLambda &lt;- fa$loadings[, 1, drop = FALSE]  # matrice dei carichi (5x1)\nrownames(Lambda) &lt;- colnames(R)\nLambda\n#&gt;       Factor1\n#&gt; INFO   0.7394\n#&gt; SIM    0.7503\n#&gt; ARITH  0.6688\n#&gt; VOC    0.8495\n#&gt; COMP   0.7369\n\nCalcolo manuale: B = R⁻¹ Λ:\n\nB_manual &lt;- solve(R) %*% Lambda  # (5x5)^(-1) %*% (5x1) = (5x1)\nB_manual\n#&gt;       Factor1\n#&gt; INFO   0.1113\n#&gt; SIM    0.1466\n#&gt; ARITH  0.1894\n#&gt; VOC    0.4214\n#&gt; COMP   0.2294\n\nCalcolo manuale dei punteggi fattoriali:\n\nF_manual &lt;- Y_std %*% B_manual  # (100x5) %*% (5x1) = (100x1)\ncolnames(F_manual) &lt;- \"Fattore1\"\n\nF_manual |&gt; head()\n#&gt;      Fattore1\n#&gt; [1,] -0.68947\n#&gt; [2,] -0.17506\n#&gt; [3,]  1.50731\n#&gt; [4,] -0.01497\n#&gt; [5,] -0.04504\n#&gt; [6,]  1.76837\n\nConfronto con punteggi ottenuti da factanal()\n\nF_factanal &lt;- fa$scores[, 1, drop = FALSE]\ncolnames(F_factanal) &lt;- \"Fattore1_factanal\"\n\nF_factanal |&gt; head()\n#&gt;      Fattore1_factanal\n#&gt; [1,]         -0.605846\n#&gt; [2,]         -0.183356\n#&gt; [3,]          1.520967\n#&gt; [4,]         -0.002501\n#&gt; [5,]         -0.055315\n#&gt; [6,]          1.732971\n\n\ncor(F_manual, F_factanal)\n#&gt;          Fattore1_factanal\n#&gt; Fattore1            0.9963\n\nIl coefficiente di correlazione tra i due metodi sarà molto vicino a 1, indicando che la formula \\(\\hat{\\mathbf{F}} = \\mathbf{Y} \\mathbf{R}^{-1} \\hat{\\boldsymbol{\\Lambda}}\\) è equivalente (entro errori numerici minimi) al metodo implementato da factanal() con l’opzione scores = \"regression\".\nIn sintesi,\n\nabbiamo visto come calcolare manualmente i punteggi fattoriali utilizzando le formule matriciali di regressione;\nabbiamo confrontato i risultati con quelli ottenuti da un pacchetto statistico (factanal() in R);\nl’equivalenza tra i due risultati conferma la validità teorica delle formule presentate nel metodo della regressione per la stima dei punteggi fattoriali.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>I punteggi fattoriali</span>"
    ]
  },
  {
    "objectID": "chapters/fa/05_factor_scores.html#riflessioni-conclusive",
    "href": "chapters/fa/05_factor_scores.html#riflessioni-conclusive",
    "title": "23  I punteggi fattoriali",
    "section": "\n23.6 Riflessioni Conclusive",
    "text": "23.6 Riflessioni Conclusive\nL’analisi fattoriale non è solo una tecnica per ridurre la dimensionalità dei dati, ma un vero e proprio strumento per costruire modelli psicologici latenti a partire da manifestazioni osservabili. Se interpretata con rigore teorico e supportata da una stima accurata dei punteggi fattoriali, essa può favorire una comprensione più profonda dei costrutti psicologici e della loro struttura. Tuttavia, va ricordato che ogni decisione presa lungo il processo – dalla scelta del numero di fattori, al metodo di estrazione, alla rotazione e alla stima dei punteggi – introduce un certo grado di arbitrarietà. È proprio in questo spazio di arbitrarietà che la psicometria incontra la teoria: l’analisi fattoriale è tanto uno strumento tecnico quanto una pratica epistemica, in cui i dati non parlano da soli, ma vengono interrogati alla luce di ipotesi e modelli. Per questo motivo, lo sviluppo di test psicometrici validi richiede non solo padronanza tecnica, ma anche capacità critica e riflessività teorica.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>I punteggi fattoriali</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Testing Psicologico",
    "section": "",
    "text": "Informazioni Generali",
    "crumbs": [
      "Informazioni Generali"
    ]
  },
  {
    "objectID": "index.html#informazioni-generali",
    "href": "index.html#informazioni-generali",
    "title": "Testing Psicologico",
    "section": "",
    "text": "Codice: B033288 - Testing Psicologico\nModulo: B033288 - Testing Psicologico (Cognomi L-Z)\nCorso di laurea: Corso di Laurea Magistrale Psicologia Clinica e della Salute e Neuropsicologia dell’Università degli Studi di Firenze.\nAnno Accademico: 2024-2025\nCalendario: Il corso si terrà dal 4 marzo al 31 maggio 2025.\nOrario delle lezioni: Le lezioni si svolgeranno il martedì dalle 10:30 alle 13:30 e il giovedì dalle 8:30 alle 11:30.\nLuogo: Le lezioni si terranno presso il Plesso didattico La Torretta.\nModalità di svolgimento della didattica: Le lezioni ed esercitazioni saranno svolte in modalità frontale.",
    "crumbs": [
      "Informazioni Generali"
    ]
  },
  {
    "objectID": "index.html#panoramica-del-corso",
    "href": "index.html#panoramica-del-corso",
    "title": "Testing Psicologico",
    "section": "Panoramica del Corso",
    "text": "Panoramica del Corso\nL’insegnamento di Testing Psicologico si propone quale stimolo e guida per l’apprendimento delle basi dell’assessment psicologico. Vengono affrontati temi come la teoria classica dei test, l’analisi dei percorsi, l’analisi fattoriale, la PCA, i modelli SEM, lo scaling Mokken e la teoria di risposta all’item (IRT). Il corso integra questi argomenti in un percorso didattico che combina lezioni teoriche, esercitazioni pratiche e momenti di riflessione critica. Gli studenti saranno così preparati ad applicare l’analisi dei dati sia in contesti teorici che pratici.\n\n\n\n\n\n\nQuesto sito web è la fonte ufficiale per tutte le informazioni relative al programma dell’insegnamento B033288 - Testing Psicologico (Cognomi A-K) per l’A.A. 2024-2025 e le modalità d’esame.",
    "crumbs": [
      "Informazioni Generali"
    ]
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "Testing Psicologico",
    "section": "Syllabus",
    "text": "Syllabus\nIl Syllabus esteso può essere scaricato utilizzando questo link.",
    "crumbs": [
      "Informazioni Generali"
    ]
  },
  {
    "objectID": "index.html#licenza-duso",
    "href": "index.html#licenza-duso",
    "title": "Testing Psicologico",
    "section": "Licenza d’Uso",
    "text": "Licenza d’Uso\n\n\n\nCC BY 4.0\n\n\nI materiali sono rilasciati con licenza CC BY 4.0. È consentito qualsiasi utilizzo previa attribuzione. Per usi commerciali o derivati, consultare le linee guida complete.",
    "crumbs": [
      "Informazioni Generali"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html#introduzione",
    "href": "chapters/fa/06_constraints_on_parms.html#introduzione",
    "title": "24  Attendibilità e modello fattoriale",
    "section": "",
    "text": "l’indice omega di McDonald,\n\nl’indice alpha di Cronbach,\n\nl’indice rho, derivato dalla formula “profetica” di Spearman-Brown.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html#modello-fattoriale-e-teoria-classica-dei-test",
    "href": "chapters/fa/06_constraints_on_parms.html#modello-fattoriale-e-teoria-classica-dei-test",
    "title": "24  Attendibilità e modello fattoriale",
    "section": "\n24.3 Modello fattoriale e teoria classica dei test",
    "text": "24.3 Modello fattoriale e teoria classica dei test\nConsideriamo un insieme di \\(p\\) item osservati, denotati con \\(X_1, X_2, \\dots, X_p\\) (dove \\(p &gt; 2\\)). Secondo la teoria classica dei test, ciascun punteggio osservato \\(X_i\\) può essere scomposto in due componenti:\n\nil punteggio vero \\(T_i\\),\n\nun errore casuale \\(E_i\\):\n\n\\[\n\\begin{aligned}\nX_1 &= T_1 + E_1, \\\\\nX_2 &= T_2 + E_2, \\\\\n&\\dots \\\\\nX_p &= T_p + E_p.\n\\end{aligned}\n\\]\nSeguendo l’approccio proposto da McDonald (2013), questa decomposizione può essere reinterpretata nel contesto dell’analisi fattoriale. La relazione tra punteggio vero ed errore viene descritta nel seguente modo:\n\\[\nX_i = \\lambda_i \\xi + \\delta_i, \\quad \\text{per } i = 1, \\dots, p,\n\\]\ndove:\n\n\n\\(X_i\\) è il punteggio osservato dell’item \\(i\\)-esimo (espresso come scarto dalla media),\n\n\\(\\lambda_i\\) è il carico fattoriale, che rappresenta il contributo del fattore comune \\(\\xi\\) all’item \\(i\\),\n\n\\(\\xi\\) è il fattore latente comune (ipotizzato con media zero e varianza unitaria),\n\n\\(\\delta_i\\) è l’errore specifico (residuo) associato all’item \\(i\\).\n\nQuesta formulazione si basa sulle ipotesi classiche del modello monofattoriale:\n\nil fattore comune \\(\\xi\\) è incorrelato con ciascun errore \\(\\delta_i\\),\n\ngli errori \\(\\delta_i\\) sono mutuamente incorrelati (\\(\\text{Cov}(\\delta_i, \\delta_j) = 0\\) per ogni \\(i \\neq j\\)).\n\nQuesta struttura consente di collegare direttamente i concetti della teoria classica dei test con il formalismo dell’analisi fattoriale e di derivare in modo coerente gli indici di affidabilità basati su modelli fattoriali.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html#modelli-monofattoriali-indicatori-congenerici-τ-equivalenti-e-paralleli",
    "href": "chapters/fa/06_constraints_on_parms.html#modelli-monofattoriali-indicatori-congenerici-τ-equivalenti-e-paralleli",
    "title": "24  Attendibilità e modello fattoriale",
    "section": "\n24.5 Modelli monofattoriali: indicatori congenerici, τ-equivalenti e paralleli",
    "text": "24.5 Modelli monofattoriali: indicatori congenerici, τ-equivalenti e paralleli\n\n24.5.1 Indicatori congenerici\n\nGli indicatori congenerici rappresentano misure di uno stesso costrutto latente, ma non è richiesto che lo riflettano con la stessa intensità né con lo stesso grado di precisione. Nel modello monofattoriale congenerico, non vengono imposti vincoli né sui carichi fattoriali né sulle varianze degli errori specifici:\n\\[\n\\lambda_1 \\neq \\lambda_2 \\neq \\dots \\neq \\lambda_p, \\quad \\psi_{11} \\neq \\psi_{22} \\neq \\dots \\neq \\psi_{pp}.\n\\]\nIl modello è descritto, come già visto, dall’equazione:\n\\[\nX_i = \\lambda_i \\xi + \\delta_i, \\quad i = 1, \\dots, p.\n\\]\nLa matrice di varianze e covarianze riprodotta dal modello è:\n\\[\n\\boldsymbol{\\Sigma} = \\begin{bmatrix}\n\\sigma_{11} & \\sigma_{12} & \\dots & \\sigma_{1p} \\\\\n\\sigma_{21} & \\sigma_{22} & \\dots & \\sigma_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_{p1} & \\sigma_{p2} & \\dots & \\sigma_{pp}\n\\end{bmatrix},\n\\]\ndove ogni elemento può assumere un valore diverso. Le covarianze tra gli item sono tutte positive (poiché condividono il fattore comune), ma non necessariamente uguali tra loro, e lo stesso vale per le varianze.\nQuesto è il modello più flessibile, adatto a situazioni empiriche in cui gli item non sono perfettamente equivalenti ma riflettono lo stesso costrutto. Il coefficiente omega di McDonald è coerente con questo modello.\n\n24.5.2 Indicatori τ-equivalenti\n\nIl modello con indicatori \\(\\tau\\)-equivalenti introduce un vincolo importante: tutti gli item presentano lo stesso carico fattoriale. Tuttavia, le varianze residue possono ancora differire:\n\\[\n\\lambda_1 = \\lambda_2 = \\dots = \\lambda_p = \\lambda, \\quad \\psi_{11} \\neq \\psi_{22} \\neq \\dots \\neq \\psi_{pp}.\n\\]\nL’equazione del modello diventa quindi:\n\\[\nX_i = \\lambda \\xi + \\delta_i,\n\\]\noppure, definendo \\(\\tau = \\lambda \\xi\\) come componente comune scalata nell’unità dell’indicatore:\n\\[\nX_i = \\tau + \\delta_i.\n\\]\nIn questo modello, le covarianze tra gli item sono tutte uguali, poiché dipendono solo dalla varianza della componente comune:\n\\[\n\\sigma_{ik} = \\lambda^2 = \\sigma_T^2, \\quad \\text{per } i \\neq k.\n\\]\nInvece, le varianze degli item possono differire a causa delle varianze residue:\n\\[\n\\sigma_{ii} = \\lambda^2 + \\psi_{ii} = \\sigma_T^2 + \\psi_{ii}.\n\\]\nLa matrice delle varianze e covarianze risultante è dunque:\n\\[\n\\boldsymbol{\\Sigma} = \\begin{bmatrix}\n\\sigma_T^2 + \\psi_{11} & \\sigma_T^2 & \\dots & \\sigma_T^2 \\\\\n\\sigma_T^2 & \\sigma_T^2 + \\psi_{22} & \\dots & \\sigma_T^2 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_T^2 & \\sigma_T^2 & \\dots & \\sigma_T^2 + \\psi_{pp}\n\\end{bmatrix}.\n\\]\nIl coefficiente alpha di Cronbach assume implicitamente che gli item soddisfino questa struttura. Tuttavia, in molte applicazioni empiriche questa assunzione è violata, rendendo l’\\(\\alpha\\) un sottostimatore distorto dell’affidabilità reale.\n\n24.5.3 Indicatori paralleli\n\nIl modello con indicatori paralleli rappresenta il caso più restrittivo. Oltre a imporre carichi fattoriali uguali, richiede anche che tutte le varianze residue siano identiche:\n\\[\n\\lambda_1 = \\lambda_2 = \\dots = \\lambda_p = \\lambda, \\quad \\psi_{11} = \\psi_{22} = \\dots = \\psi_{pp} = \\psi.\n\\]\nDi conseguenza, tutte le varianze osservate risultano uguali:\n\\[\n\\sigma_{ii} = \\lambda^2 + \\psi = \\sigma_T^2 + \\sigma^2, \\quad \\text{per ogni } i.\n\\]\nAnche tutte le covarianze tra item restano uguali:\n\\[\n\\sigma_{ik} = \\lambda^2 = \\sigma_T^2, \\quad \\text{per } i \\neq k.\n\\]\nLa matrice \\(\\boldsymbol{\\Sigma}\\) assume quindi la seguente forma simmetrica e omogenea:\n\\[\n\\boldsymbol{\\Sigma} = \\begin{bmatrix}\n\\sigma_T^2 + \\sigma^2 & \\sigma_T^2 & \\dots & \\sigma_T^2 \\\\\n\\sigma_T^2 & \\sigma_T^2 + \\sigma^2 & \\dots & \\sigma_T^2 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_T^2 & \\sigma_T^2 & \\dots & \\sigma_T^2 + \\sigma^2\n\\end{bmatrix}.\n\\]\nQuesto modello si colloca all’estremo del continuum di restrizione e viene assunto dalla formula di Spearman-Brown, impiegata per stimare l’effetto dell’aumento del numero di item sull’affidabilità.\n\n24.5.4 Riepilogo concettuale\n\n\n\n\n\n\n\n\n\n\nModello\nCarichi fattoriali\nVarianze errori\nVarianze osservate\nCovarianze\nIndice coerente\n\n\n\nCongenerico\nDiversi\nDiverse\nDiverse\nDiverse\nOmega\n\n\nTau-equivalente\nUguali\nDiverse\nDiverse\nUguali\nAlpha\n\n\nParallelo\nUguali\nUguali\nUguali\nUguali\nRho\n\n\n\n \nComprendere queste tre configurazioni è fondamentale per scegliere l’indice di affidabilità appropriato e per valutare la validità delle assunzioni nei modelli di misura psicometrica.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html#riflessioni-conclusive",
    "href": "chapters/fa/06_constraints_on_parms.html#riflessioni-conclusive",
    "title": "24  Attendibilità e modello fattoriale",
    "section": "\n24.9 Riflessioni Conclusive",
    "text": "24.9 Riflessioni Conclusive\nNel corso di questo capitolo abbiamo analizzato tre principali coefficienti di affidabilità interna – \\(\\alpha\\), \\(\\omega\\), e \\(\\rho\\) – ciascuno associato a un diverso modello di misura monofattoriale:\n\n\n\n\n\n\n\n\n\n\nCoefficiente\nModello sottostante\nIpotesi sui carichi \\(\\lambda\\)\n\nIpotesi sugli errori \\(\\psi_{ii}\\)\n\nCorrelazioni residue tra errori\nInterpretabile come…\n\n\n\n\\(\\omega\\)\nCongenerico\nLiberi\nLiberi\nNessuna (idealmente zero)\nVarianza spiegata / totale\n\n\n\\(\\alpha\\)\n\n\\(\\tau\\)-equivalente\nUguali\nLiberi\nNessuna\nLimite inferiore di \\(\\omega\\)\n\n\n\n\\(\\rho\\)\nParallelo\nUguali\nUguali\nNessuna\nPredizione su test allungato\n\n\n\n\n24.9.1 Scelte operative\n\n\nIl coefficiente \\(\\alpha\\) di Cronbach è il più diffuso in ambito psicometrico per la sua semplicità computazionale. Tuttavia, è valido solo quando gli item sono \\(\\tau\\)-equivalenti, ovvero misurano lo stesso costrutto con intensità uguale ma con varianze d’errore potenzialmente differenti.\nNella pratica, tale ipotesi è raramente soddisfatta: spesso gli item mostrano carichi diversi, oppure strutture multidimensionali latenti. In questi casi, \\(\\alpha\\) può sottostimare l’affidabilità se gli errori sono incorrelati, oppure sovrastimarla se gli errori sono correlati.\n\nIl coefficiente \\(\\omega\\) di McDonald costituisce un’alternativa più generale e robusta, poiché richiede ipotesi meno restrittive. È compatibile con il modello congenerico, che riflette con maggiore realismo la struttura empirica di molti test. In questo senso, \\(\\omega\\) rappresenta la scelta raccomandata per stimare l’affidabilità interna in presenza di carichi fattoriali disuguali.\nLa formula di Spearman-Brown (\\(\\rho\\)) trova il suo uso ideale quando si assume il modello più restrittivo con item paralleli. Sebbene meno flessibile, essa è utile per stimare l’impatto della lunghezza del test sull’affidabilità, rispondendo alla domanda: “Cosa succederebbe se raddoppiassi il numero di item?”\n\n24.9.2 Indici alternativi\nOltre a \\(\\alpha\\) e \\(\\omega\\), la letteratura psicometrica propone altri indici più sofisticati:\n\nIl GLB (Greatest Lower Bound) di Ten Berge e Sočan (2004), che fornisce il limite inferiore teoricamente più alto dell’affidabilità. Tuttavia, è computazionalmente più complesso e sensibile alla struttura dei dati.\nL’indice \\(\\beta\\) di Revelle (1979), che misura la coerenza del sottoinsieme peggiore di item (worst split-half), ed è utile per rilevare problemi di dimensionalità.\n\nQuesti indici possono essere utili in situazioni in cui si sospetta che l’unidimensionalità sia violata o in cui si desidera esplorare diverse prospettive sulla coerenza interna di una scala.\n\n24.9.3 Considerazioni finali\nIn sintesi:\n\nIl coefficiente \\(\\alpha\\) dovrebbe essere usato con cautela, solo quando le condizioni teoriche del modello \\(\\tau\\)-equivalente sono empiricamente verificate.\nIl coefficiente \\(\\omega\\) è da preferire nella maggior parte delle applicazioni, in quanto fornisce una misura più realistica e flessibile dell’affidabilità.\nNessun coefficiente è “migliore” in senso assoluto: la scelta dipende sempre dalla struttura latente del test, dalla qualità dei dati e dallo scopo dell’analisi.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/07_total_score.html#punteggio-totale-e-modello-congenerico",
    "href": "chapters/fa/07_total_score.html#punteggio-totale-e-modello-congenerico",
    "title": "25  Punteggio totale e modello fattoriale",
    "section": "\n25.3 Punteggio totale e modello congenerico",
    "text": "25.3 Punteggio totale e modello congenerico\n\n25.3.1 Il modello congenerico\nIl modello congenerico è una generalizzazione del modello parallelo. Qui si ammette che gli item possano avere saturazioni fattoriali differenti e varianze residue differenti. In pratica, si riconosce che alcuni item sono più informativi di altri.\n\nm_congeneric &lt;- \"\n  f1 =~ NA*X4 + X5 + X6 + X7 + X8 + X9\n  f1 ~~ 1*f1\n\"\n\n\nfit_congeneric &lt;- sem(m_congeneric, data = d)\n\nAnalizziamo le saturazioni fattoriali:\n\nparameterEstimates(fit_congeneric, standardized = TRUE) %&gt;%\n  dplyr::filter(op == \"=~\") %&gt;%\n  dplyr::select(\n    \"Latent Factor\" = lhs,\n    Indicator = rhs,\n    B = est,\n    SE = se,\n    Z = z,\n    \"p-value\" = pvalue,\n    Beta = std.all\n  ) %&gt;%\n  knitr::kable(\n    digits = 3, booktabs = TRUE, format = \"markdown\",\n    caption = \"Factor Loadings\"\n  )\n\n\nFactor Loadings\n\nLatent Factor\nIndicator\nB\nSE\nZ\np-value\nBeta\n\n\n\nf1\nX4\n0.963\n0.059\n16.274\n0.000\n0.824\n\n\nf1\nX5\n1.121\n0.067\n16.835\n0.000\n0.846\n\n\nf1\nX6\n0.894\n0.058\n15.450\n0.000\n0.792\n\n\nf1\nX7\n0.195\n0.071\n2.767\n0.006\n0.170\n\n\nf1\nX8\n0.185\n0.063\n2.938\n0.003\n0.180\n\n\nf1\nX9\n0.278\n0.065\n4.245\n0.000\n0.258\n\n\n\n\n\nCome si osserva, le saturazioni sono eterogenee. Ciò significa che il costrutto latente si riflette diversamente nei vari item. In tali casi, il punteggio totale—che attribuisce lo stesso peso a ciascun item—è una misura distorta del costrutto.\n\n25.3.2 Confronto con i punteggi fattoriali del modello congenerico\nEsaminiamo la relazione tra i punteggi fattoriali del modello congenerico e il punteggio totale del test.\n\nd$scores_cong &lt;- as.numeric(lavPredict(fit_congeneric, method=\"regression\"))\n\nd |&gt; \n  ggplot(aes(x = ts, y = scores_cong)) + \n  geom_point()\n\ncor(d$ts, d$scores_cong)^2\n#&gt; [1] 0.766\n\n\n\n\n\n\n\nQui il coefficiente di determinazione è circa 0.77, il che implica che due persone con lo stesso punteggio totale possono avere punteggi fattoriali molto diversi, a seconda degli item approvati. Questo è un limite importante del punteggio totale, che non tiene conto del contributo specifico di ciascun item.\nSe ignoriamo le assunzioni del modello e guardiamo solo a un indice globale come l’omega, possiamo essere tratti in inganno:\n\npsych::omega(d[, 1:6])\n#&gt; Omega \n#&gt; Call: omegah(m = m, nfactors = nfactors, fm = fm, key = key, flip = flip, \n#&gt;     digits = digits, title = title, sl = sl, labels = labels, \n#&gt;     plot = plot, n.obs = n.obs, rotate = rotate, Phi = Phi, option = option, \n#&gt;     covar = covar)\n#&gt; Alpha:                 0.72 \n#&gt; G.6:                   0.76 \n#&gt; Omega Hierarchical:    0.55 \n#&gt; Omega H asymptotic:    0.65 \n#&gt; Omega Total            0.84 \n#&gt; \n#&gt; Schmid Leiman Factor loadings greater than  0.2 \n#&gt;       g  F1*  F2*   F3*   h2   h2   u2   p2  com\n#&gt; X4 0.73            0.68 1.00 1.00 0.00 0.53 1.99\n#&gt; X5 0.96                 0.92 0.92 0.08 1.00 1.01\n#&gt; X6 0.69            0.22 0.54 0.54 0.46 0.90 1.22\n#&gt; X7           0.56       0.33 0.33 0.67 0.03 1.15\n#&gt; X8           0.75       0.59 0.59 0.41 0.05 1.12\n#&gt; X9 0.22      0.49       0.29 0.29 0.71 0.16 1.41\n#&gt; \n#&gt; With Sums of squares  of:\n#&gt;    g  F1*  F2*  F3*   h2 \n#&gt; 2.02 0.00 1.11 0.54 2.67 \n#&gt; \n#&gt; general/max  0.75   max/min =   622.1\n#&gt; mean percent general =  0.44    with sd =  0.43 and cv of  0.97 \n#&gt; Explained Common Variance of the general factor =  0.55 \n#&gt; \n#&gt; The degrees of freedom are 0  and the fit is  0 \n#&gt; The number of observations was  301  with Chi Square =  0.03  with prob &lt;  NA\n#&gt; The root mean square of the residuals is  0 \n#&gt; The df corrected root mean square of the residuals is  NA\n#&gt; \n#&gt; Compare this with the adequacy of just a general factor and no group factors\n#&gt; The degrees of freedom for just the general factor are 9  and the fit is  0.48 \n#&gt; The number of observations was  301  with Chi Square =  142.3  with prob &lt;  3.5e-26\n#&gt; The root mean square of the residuals is  0.17 \n#&gt; The df corrected root mean square of the residuals is  0.21 \n#&gt; \n#&gt; RMSEA index =  0.222  and the 10 % confidence intervals are  0.191 0.255\n#&gt; BIC =  90.9 \n#&gt; \n#&gt; Measures of factor score adequacy             \n#&gt;                                                  g   F1*  F2*  F3*\n#&gt; Correlation of scores with factors            0.96  0.08 0.83 0.96\n#&gt; Multiple R square of scores with factors      0.93  0.01 0.68 0.91\n#&gt; Minimum correlation of factor score estimates 0.86 -0.99 0.36 0.83\n#&gt; \n#&gt;  Total, General and Subset omega for each subset\n#&gt;                                                  g  F1*  F2*  F3*\n#&gt; Omega total for total scores and subscales    0.84 0.92 0.66 0.86\n#&gt; Omega general for total scores and subscales  0.55 0.92 0.04 0.61\n#&gt; Omega group for total scores and subscales    0.27 0.00 0.61 0.25\n\n\n\n\n\n\n\nL’omega può risultare “accettabile” (Omega Total  0.84), ma ciò non garantisce che il punteggio totale sia una misura valida del costrutto.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Punteggio totale e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/07_total_score.html#necessità-di-un-modello-a-due-fattori",
    "href": "chapters/fa/07_total_score.html#necessità-di-un-modello-a-due-fattori",
    "title": "25  Punteggio totale e modello fattoriale",
    "section": "\n25.4 Necessità di un modello a due fattori",
    "text": "25.4 Necessità di un modello a due fattori\nI dati suggeriscono invece la presenza di due costrutti distinti. Adattiamo quindi un modello congenerico a due fattori:\n\nm2f_cong &lt;- \"\n  f1 =~ NA*X4 + X5 + X6\n  f2 =~ NA*X7 + X8 + X9\n  f1 ~~ 1*f1\n  f2 ~~ 1*f2\n  f1 ~~ f2\n\"\nfit_2f_congeneric &lt;- sem(m2f_cong, data = d)\nsummary(fit_2f_congeneric, fit.measures = TRUE, standardized = TRUE)\n#&gt; lavaan 0.6-19 ended normally after 18 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        13\n#&gt; \n#&gt;   Number of observations                           301\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                14.736\n#&gt;   Degrees of freedom                                 8\n#&gt;   P-value (Chi-square)                           0.064\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               568.519\n#&gt;   Degrees of freedom                                15\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.988\n#&gt;   Tucker-Lewis Index (TLI)                       0.977\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -2525.349\n#&gt;   Loglikelihood unrestricted model (H1)      -2517.981\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                5076.698\n#&gt;   Bayesian (BIC)                              5124.891\n#&gt;   Sample-size adjusted Bayesian (SABIC)       5083.662\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.053\n#&gt;   90 Percent confidence interval - lower         0.000\n#&gt;   90 Percent confidence interval - upper         0.095\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.402\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.159\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.035\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   f1 =~                                                                 \n#&gt;     X4                0.965    0.059   16.296    0.000    0.965    0.826\n#&gt;     X5                1.123    0.067   16.845    0.000    1.123    0.847\n#&gt;     X6                0.895    0.058   15.465    0.000    0.895    0.793\n#&gt;   f2 =~                                                                 \n#&gt;     X7                0.659    0.080    8.218    0.000    0.659    0.575\n#&gt;     X8                0.733    0.077    9.532    0.000    0.733    0.712\n#&gt;     X9                0.599    0.075    8.025    0.000    0.599    0.557\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   f1 ~~                                                                 \n#&gt;     f2                0.275    0.072    3.813    0.000    0.275    0.275\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;     f1                1.000                               1.000    1.000\n#&gt;     f2                1.000                               1.000    1.000\n#&gt;    .X4                0.433    0.056    7.679    0.000    0.433    0.318\n#&gt;    .X5                0.496    0.072    6.892    0.000    0.496    0.282\n#&gt;    .X6                0.472    0.054    8.732    0.000    0.472    0.371\n#&gt;    .X7                0.881    0.100    8.807    0.000    0.881    0.670\n#&gt;    .X8                0.521    0.094    5.534    0.000    0.521    0.492\n#&gt;    .X9                0.798    0.087    9.162    0.000    0.798    0.689\n\nIl modello mostra un buon adattamento. In questo scenario, il punteggio totale aggrega item che misurano costrutti diversi, perdendo così validità come indicatore di un singolo costrutto.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Punteggio totale e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/07_total_score.html#conclusione",
    "href": "chapters/fa/07_total_score.html#conclusione",
    "title": "25  Punteggio totale e modello fattoriale",
    "section": "\n25.5 Conclusione",
    "text": "25.5 Conclusione\nL’uso del punteggio totale come stima del costrutto latente è una semplificazione che deve essere giustificata empiricamente:\n\né giustificato solo se il modello parallelo è supportato dai dati;\nnel caso in cui gli item abbiano saturazioni diverse (modello congenerico), il punteggio totale perde validità;\nse esistono più costrutti latenti, l’uso del punteggio totale può introdurre errori sistematici.\n\nIn sintesi: prima di usare o interpretare un punteggio totale, è essenziale testare il modello fattoriale sottostante. I punteggi totali, per quanto comodi, possono essere ingannevoli (McNeish & Wolf, 2020).",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Punteggio totale e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "cal_testing_psic_2025.html",
    "href": "cal_testing_psic_2025.html",
    "title": "Calendario Didattico e Programma del Corso",
    "section": "",
    "text": "Struttura degli Incontri\nIl calendario didattico prevede 14 incontri di 3 ore ciascuno. Nell’ultimo tratto del corso, ci sarà una verifica tramite Quiz Moodle (1 ora) e le presentazioni finali degli studenti negli ultimi due incontri.",
    "crumbs": [
      "Calendario",
      "Calendario Didattico e Programma del Corso"
    ]
  },
  {
    "objectID": "cal_testing_psic_2025.html#struttura-degli-incontri",
    "href": "cal_testing_psic_2025.html#struttura-degli-incontri",
    "title": "Calendario Didattico e Programma del Corso",
    "section": "",
    "text": "Incontro 1\n\nData: 4 marzo 2025\n\nOrario: 10:30-13:30\n\nArgomenti:\n\nPresentazione del corso\n\nIntroduzione a \\(\\mathsf{R}\\)\n\n\nCommenti: Incontro introduttivo con panoramica sul corso e introduzione ai fondamenti di R.\n\n\n\n\nIncontro 2\n\nData: 6 marzo 2025\n\nOrario: 8:30-11:30\n\nArgomenti:\n\nPunteggi e scale: punteggi derivati, scaling Likert, codifica inversa\n\nImputazione e calcolo del punteggio totale\n\nOttimizzazione dello scoring dei dati ordinali\n\nScaling di Thurstone e sviluppo dello strumento\n\nEquating nei test psicologici\n\nIntroduzione al modello lineare\n\n\nCommenti: Approfondimento su metodi di scoring psicometrico e scaling.\n\n\n\n\nIncontro 3\n\nData: 11 marzo 2025\n\nOrario: 10:30-13:30\n\nArgomenti:\n\nTeoria Classica dei Test\n\nRelazione con il modello lineare\n\nErrore standard della misurazione\n\nMisure congeneriche, tau-equivalenti e parallele\n\nAffidabilità\n\nStima del punteggio vero ed errore standard della stima\n\nApplicazioni pratiche\n\n\nCommenti: Panoramica completa sulla Teoria Classica dei Test.\n\n\n\n\nIncontro 4\n\nData: 13 marzo 2025\n\nOrario: 8:30-11:30\n\nArgomenti:\n\nPath Analysis\n\nTutorial di Clement & Bradley-Garcia (2022)\nNetwork Analysis\n\n\nCommenti: Approfondimento su Path Analysis e Network Analysis.\n\n\n\n\nIncontro 5\n\nData: 18 marzo 2025\n\nOrario: 10:30-13:30\n\nArgomenti:\n\nElementi di algebra lineare\n\nAnalisi delle componenti principali (PCA)\n\n\nCommenti: Fondamenti di algebra lineare e analisi delle componenti principali.\n\n\n\n\nIncontro 6\n\nData: 20 marzo 2025\n\nOrario: 8:30-11:30\n\nArgomenti:\n\nAnalisi fattoriale esplorativa (EFA)\n\nIl modello statistico dell’analisi fattoriale\n\n\nCommenti: Introduzione all’analisi fattoriale esplorativa e al modello statistico.\n\n\n\n\nIncontro 7\n\nData: 25 marzo 2025\n\nOrario: 10:30-13:30\n\nArgomenti:\n\nEstrazione dei fattori\n\nRotazione dei fattori\n\n\nCommenti: Tecniche di estrazione e rotazione dei fattori.\n\n\n\n\nIncontro 8\n\nData: 27 marzo 2025\n\nOrario: 8:30-11:30\n\nArgomenti:\n\nAnalisi fattoriale confermativa (CFA)\n\n\nCommenti: Introduzione all’analisi fattoriale confermativa e alle sue applicazioni.\n\n\n\n\nIncontro 9\n\nData: 1 aprile 2025\n\nOrario: 10:30-13:30\n\nArgomenti:\n\nIntroduzione ai modelli di equazioni strutturali (SEM)\n\n\nCommenti: Panoramica sui modelli di equazioni strutturali.\n\n\n\n\nIncontro 10\n\nData: 3 aprile 2025\n\nOrario: 8:30-11:30\n\nArgomenti:\n\nModelli multilivello\n\nAttendibilità dei giudici\n\nModelli di crescita latente (LGM)\n\n\nCommenti: Discussione sui modelli multilivello e sulla crescita latente.\n\n\n\n\nIncontro 11\n\nData: 8 aprile 2025\n\nOrario: 10:30-13:30\n\nArgomenti:\n\nModello di regressione logistica\n\nMokken Scale Analysis\n\nPredizione\n\n\nCommenti: Introduzione ai modelli di regressione logistica e analisi di Mokken.\n\n\n\n\nIncontro 12\n\nData: 10 aprile 2025\n\nOrario: 8:30-11:30\n\nArgomenti:\n\nItem Response Theory (IRT)\n\nConcetti di validità\n\n\nCommenti: Introduzione alla IRT e alla validità psicometrica.\n\n\n\n\nIncontro 13\n\nData: 15 aprile 2025\n\nOrario: 10:30-13:30\n\nArgomenti:\n\nVerifica tramite Quiz Moodle (1 ora)\n\nPresentazioni finali degli studenti\n\n\nCommenti: Quiz di valutazione e presentazioni finali.\n\n\n\n\nIncontro 14\n\nData: 17 aprile 2025\n\nOrario: 8:30-11:30\n\nArgomenti:\n\nPresentazioni finali degli studenti\n\n\nCommenti: Conclusione del corso con le presentazioni finali degli studenti.\n\n\n\n\n\n\nClement, L. M., & Bradley-Garcia, M. (2022). A step-by-step tutorial for performing a moderated mediation analysis using PROCESS. The Quantitative Methods for Psychology, 18(3), 258–271.",
    "crumbs": [
      "Calendario",
      "Calendario Didattico e Programma del Corso"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html",
    "href": "chapters/networks/01_networks.html",
    "title": "24  Network psicologici",
    "section": "",
    "text": "24.1 Introduzione\nQuesto capitolo costituisce un riassunto semplificato del capitolo Psychological Networks: A Modern Approach to Analysis of Learning and Complex Learning Processes del testo di Saqr & López-Pernas (2024).",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#la-network-analysis",
    "href": "chapters/networks/01_networks.html#la-network-analysis",
    "title": "24  Network psicologici",
    "section": "\n24.2 La Network Analysis",
    "text": "24.2 La Network Analysis\nL’analisi delle reti è un potente strumento per i ricercatori, utile per mappare relazioni, individuare connessioni e identificare cluster o comunità tra elementi interagenti. Questa metodologia si è affermata come una delle più importanti per comprendere sistemi complessi. In psicologia, l’analisi delle reti non è più limitata allo studio delle interazioni sociali, ma viene sempre più utilizzata per esplorare processi astratti come quelli cognitivi, emotivi e comportamentali. In questo ambito, le reti probabilistiche svolgono un ruolo centrale, rappresentando i nodi come variabili psicologiche (ad esempio, punteggi di scale o indicatori di costrutti) e gli archi come associazioni probabilistiche tra di esse.\nUn esempio di spicco è il Gaussian Graphical Model (GGM), in cui i nodi rappresentano costrutti psicologici come emozioni, comportamenti o tratti, mentre gli archi riflettono correlazioni parziali. La correlazione parziale rappresenta la relazione tra due variabili controllando l’effetto di tutte le altre variabili nella rete, secondo il principio del ceteris paribus.\nAd esempio, in una rete in cui i nodi rappresentano motivazione, successo accademico, coinvolgimento, autoregolazione e benessere, un arco tra benessere e successo accademico indica che il benessere è associato al successo accademico, indipendentemente dagli effetti delle altre variabili. L’assenza di un arco, invece, suggerisce che due nodi sono condizionalmente indipendenti, una volta considerati gli effetti delle altre variabili.\n\n24.2.1 Vantaggi delle Reti Psicologiche\nL’analisi delle reti offre diversi strumenti per valutare la robustezza e la precisione delle stime:\n\n\nAccuratezza delle stime: Tecniche come il bootstrapping permettono di valutare la stabilità dei pesi degli archi.\n\nCentralità: Misure come la forza, vicinanza e intermediazione dei nodi permettono di identificare elementi centrali o influenti nella rete.\n\nSimulazioni: Analisi basate su campioni simulati consentono di stimare la replicabilità dei risultati.\n\nConsideriamo, ad esempio, una rete che rappresenta le relazioni tra emozioni negative (ansia, tristezza, rabbia), pensieri disfunzionali e strategie di coping. Un arco tra ansia e pensieri disfunzionali potrebbe indicare che l’ansia è strettamente associata ai pensieri disfunzionali, controllando per l’effetto di tristezza, rabbia e coping. Questo tipo di analisi non solo chiarisce le interazioni tra variabili, ma suggerisce interventi mirati: ad esempio, rafforzare le strategie di coping per ridurre l’impatto delle emozioni negative sui pensieri disfunzionali.\nIn sintesi, la Network Analysis rappresenta un approccio innovativo e rigoroso per indagare i sistemi psicologici complessi, offrendo una visione dettagliata delle interazioni tra variabili e potenziali punti di intervento.",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#tutorial-con-r",
    "href": "chapters/networks/01_networks.html#tutorial-con-r",
    "title": "24  Network psicologici",
    "section": "\n24.3 Tutorial con R",
    "text": "24.3 Tutorial con R\nQuesto capitolo presenta un tutorial passo-passo su come utilizzare le reti psicologiche applicandole a dati raccolti in indagini trasversali, così come discusso da Saqr & López-Pernas (2024). Il dataset utilizzato contiene le risposte di 6071 studenti a un questionario che indaga le caratteristiche psicologiche legate al loro benessere durante la pandemia di COVID-19, condotto in Finlandia e Austria.\nLe domande del questionario riguardano i bisogni psicologici fondamentali degli studenti (relazionalità, autonomia e competenza percepita), l’apprendimento autoregolato, le emozioni positive e la motivazione intrinseca verso l’apprendimento. Inoltre, il dataset include variabili demografiche come paese di residenza, genere ed età.\nNel tutorial, mostreremo come costruire e visualizzare una rete che rappresenti le relazioni tra le diverse caratteristiche psicologiche. Successivamente, interpreteremo e valuteremo queste relazioni e confronteremo le differenze nelle reti tra gruppi demografici. Questo approccio consente di esplorare in modo visivo e quantitativo i collegamenti tra i costrutti psicologici, identificando eventuali differenze tra le categorie demografiche.",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#importazione-e-preparazione-dei-dati",
    "href": "chapters/networks/01_networks.html#importazione-e-preparazione-dei-dati",
    "title": "24  Network psicologici",
    "section": "\n24.4 Importazione e Preparazione dei Dati",
    "text": "24.4 Importazione e Preparazione dei Dati\nLa prima fase prevede l’importazione dei dati e la loro preparazione, eliminando risposte mancanti o incomplete per garantire un dataset coerente e utilizzabile nelle analisi successive.\nI dati vengono importati direttamente da una fonte online. Utilizziamo la funzione import() per caricare il file e drop_na() per rimuovere eventuali righe contenenti valori mancanti.\n\nURL &lt;- (\"https://raw.githubusercontent.com/lamethods/data/main/11_universityCovid/data.sav\") \ndf &lt;- import(URL) |&gt;\n    drop_na()\n\nPer rappresentare ciascun costrutto misurato dal questionario, combiniamo le colonne relative agli item dello stesso costrutto calcolando la media delle risposte. Questo approccio permette di ottenere una sintesi per ogni costrutto psicologico (ad esempio Competence, Autonomy, ecc.).\n\naggregated &lt;- df |&gt; rowwise() |&gt; mutate(\n    Competence = rowMeans(cbind\n    (comp1.rec , comp2.rec, comp3.rec),\n    na.rm = T),\n    Autonomy = rowMeans(cbind\n    (auto1.rec , auto2.rec, auto3.rec),\n    na.rm = T),\n    Motivation = rowMeans(cbind\n    (lm1.rec , lm2.rec, lm3.rec),\n    na.rm = T),\n    Emotion = rowMeans(cbind\n    (pa1.rec , pa2.rec, pa3.rec),\n    na.rm = T),\n    Relatedness = rowMeans(cbind\n    (sr1.rec , sr2.rec, sr3.rec),\n    na.rm = T),\n    SRL = rowMeans(cbind\n    (gp1.rec , gp2.rec, gp3.rec),\n    na.rm = T)\n)\n\nDopo aver calcolato i valori medi per ciascun costrutto, manteniamo solo le colonne appena generate. Questo rende il dataset più pulito e specifico per l’analisi.\nInoltre, creiamo dei sottoinsiemi di dati in base al genere (un dataset per i maschi e uno per le femmine) e al paese (un dataset per l’Austria e un altro per la Finlandia). Utilizzeremo questi dataset successivamente per confronti tra generi e paesi.\n\ncols &lt;- c(\n    \"Relatedness\", \"Competence\", \"Autonomy\",\n    \"Emotion\", \"Motivation\", \"SRL\"\n)\ndplyr::filter(aggregated, country == 1) |&gt;\n    dplyr::select(all_of(cols)) -&gt; finlandData\ndplyr::filter(aggregated, country == 0) |&gt;\n    dplyr::select(all_of(cols)) -&gt; austriaData\ndplyr::filter(aggregated, gender == 1) |&gt;\n    dplyr::select(all_of(cols)) -&gt; femaleData\ndplyr::filter(aggregated, gender == 2) |&gt;\n    dplyr::select(all_of(cols)) -&gt; maleData\ndplyr::select(aggregated, all_of(cols)) -&gt; allData\n\nInfine, utilizziamo glimpse() per visualizzare una panoramica del dataset finale, verificando che i dati siano stati preparati correttamente.\n\nallData |&gt; glimpse()\n#&gt; Rows: 7,160\n#&gt; Columns: 6\n#&gt; Rowwise: \n#&gt; $ Relatedness &lt;dbl&gt; 3.000, 5.000, 4.667, 4.333, 5.000, 4.000, 4.667, 2.333…\n#&gt; $ Competence  &lt;dbl&gt; 2.333, 3.000, 4.000, 3.333, 4.000, 2.667, 4.333, 3.667…\n#&gt; $ Autonomy    &lt;dbl&gt; 2.333, 1.667, 2.667, 3.000, 3.000, 2.667, 3.333, 2.667…\n#&gt; $ Emotion     &lt;dbl&gt; 2.000, 2.333, 4.000, 3.333, 4.000, 3.333, 4.667, 5.000…\n#&gt; $ Motivation  &lt;dbl&gt; 1.333, 2.000, 3.000, 2.667, 3.000, 2.667, 4.333, 1.333…\n#&gt; $ SRL         &lt;dbl&gt; 2.667, 4.333, 4.333, 2.667, 4.333, 5.000, 4.667, 2.667…",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#controllo-delle-assunzioni",
    "href": "chapters/networks/01_networks.html#controllo-delle-assunzioni",
    "title": "24  Network psicologici",
    "section": "\n24.5 Controllo delle Assunzioni",
    "text": "24.5 Controllo delle Assunzioni\nPrima di procedere con l’analisi, è essenziale verificare alcune assunzioni per assicurarsi che il dataset e la rete stimata siano appropriati e robusti.\n\n24.5.1 Matrice di Correlazione Definita Positiva\nLa matrice di correlazione deve essere definita positiva, il che implica che le variabili incluse non devono essere combinazioni lineari tra loro. In termini pratici, ogni variabile deve aggiungere informazioni uniche al modello. Per controllare questa proprietà, utilizziamo la funzione is.positive.definite() del pacchetto matrixcalc.\nSe la matrice di correlazione non fosse definita positiva, è possibile utilizzare metodi alternativi, come l’opzione cor_auto, per ottenere una matrice che soddisfi questa condizione. Nel nostro caso, la matrice risulta già definita positiva. Inoltre, specifichiamo l’argomento use = \"pairwise.complete.obs\" per includere tutte le osservazioni disponibili per ogni coppia di variabili.\n\ncorrelationMatrix &lt;- cor(\n  x = allData, use = c(\"pairwise.complete.obs\")\n)\nis.positive.definite(correlationMatrix)\n#&gt; [1] TRUE\n\nSe la funzione restituisce TRUE, possiamo procedere con l’analisi; in caso contrario, sarà necessario intervenire per correggere il problema.\n\n24.5.2 Assenza di ridondanza tra le variabili\nLa seconda verifica consiste nell’assicurarci che non ci siano variabili altamente correlate al punto da risultare ridondanti. Questo è fondamentale per garantire che ogni variabile rappresenti un costrutto unico e non una semplice sovrapposizione di altri costrutti già inclusi.\nUtilizziamo l’algoritmo goldbricker, che identifica pattern di correlazione fortemente simili tra coppie di variabili. I criteri utilizzati per individuare la ridondanza sono:\n\nCorrelazione alta: \\(r &gt; 0.50\\)\n\nFrazione significativa: almeno il 25% delle variabili altamente correlate.\np-value: 0.05 (significatività statistica).\n\nEseguiamo il controllo con il codice seguente:\n\ngoldbricker(allData,\n    p = 0.05, method = \"hittner2003\",\n    threshold = 0.25, corMin = 0.5, \n    progressbar = FALSE\n)\n#&gt; Suggested reductions: Less than 25 % of correlations are significantly different for the following pairs: \n#&gt; [1] \"No suggested reductions\"\n\nSe vengono identificate variabili ridondanti, sarà necessario rivedere il dataset, eliminando o modificando alcune variabili per ridurre la sovrapposizione.",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#stima-della-rete",
    "href": "chapters/networks/01_networks.html#stima-della-rete",
    "title": "24  Network psicologici",
    "section": "\n24.6 Stima della Rete",
    "text": "24.6 Stima della Rete\nUna volta verificato che i dati soddisfano le assunzioni necessarie, possiamo procedere alla stima della rete. Questo processo consiste nel quantificare le associazioni tra le variabili per analizzare come i loro valori si influenzano reciprocamente. Nelle reti psicologiche, le associazioni più comuni sono le correlazioni, che permettono di individuare relazioni dirette o condizionate tra le variabili (ad esempio, se alti livelli di motivazione sono associati ad alti livelli di coinvolgimento).\nLe associazioni possono essere stimate attraverso diverse misure, tra cui covarianze, correlazioni semplici, correlazioni parziali e modelli basati su regressioni. In questo contesto, ci concentriamo sulla stima delle correlazioni parziali regolarizzate, una tecnica molto utilizzata nelle analisi di rete psicologica.\n\n24.6.1 Correlazioni Parziali Regolarizzate\nLe correlazioni parziali regolarizzate offrono numerosi vantaggi e rappresentano uno standard nell’analisi delle reti psicologiche. Queste consentono di ottenere una struttura della rete interpretabile, evidenziando associazioni condizionate tra le variabili.\nVantaggi Principali:\n\n\nRecupero della struttura reale: Le correlazioni parziali regolarizzate aiutano a identificare le relazioni condizionate effettive, eliminando interferenze di altre variabili.\n\nSparsità: Forniscono una rete più chiara e leggibile, mantenendo solo gli archi più rilevanti.\n\nLa correlazione parziale misura l’associazione tra due variabili controllando l’effetto di tutte le altre variabili nella rete (ceteris paribus). Ad esempio, possiamo stimare l’associazione tra motivazione e coinvolgimento, escludendo l’influenza di variabili come successo accademico, ansia o benessere. Questo approccio permette di concentrarsi su relazioni specifiche e significative.\n\n24.6.2 Regolarizzazione\nLa regolarizzazione introduce una penalità per semplificare la complessità del modello di rete, contribuendo a eliminare associazioni spurie e a migliorare l’interpretabilità del risultato.\nVantaggi della Regolarizzazione:\n\n\nRiduzione degli archi spuri: Elimina associazioni false o deboli causate da rumore statistico o sovrapposizione.\n\nChiarezza: Imposta a zero i pesi degli archi trascurabili, producendo una rete meno densa e più interpretabile.\n\nAffidabilità: Riduce il rischio di errori di Tipo 1 (falsi positivi), mantenendo solo le associazioni più forti e significative.\n\nLa tecnica più comune per applicare la regolarizzazione è il LASSO (Least Absolute Shrinkage and Selection Operator), che penalizza la complessità della rete eliminando gli archi di importanza marginale. Questo approccio garantisce che la rete rappresenti fedelmente le relazioni chiave tra le variabili, riducendo il rumore e semplificando l’interpretazione.\nIn sintesi, la stima delle correlazioni parziali regolarizzate consente di costruire una rete psicologica robusta, chiara e focalizzata sulle relazioni essenziali tra variabili. Questo metodo combina l’efficacia delle correlazioni parziali con l’efficienza della regolarizzazione, fornendo una rappresentazione affidabile e interpretabile delle interazioni tra costrutti psicologici.",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#procedura-di-stima-della-rete",
    "href": "chapters/networks/01_networks.html#procedura-di-stima-della-rete",
    "title": "24  Network psicologici",
    "section": "\n24.7 Procedura di Stima della Rete",
    "text": "24.7 Procedura di Stima della Rete\nIl processo di stima utilizza la funzione estimateNetwork() del pacchetto bootnet, che richiede tre elementi fondamentali:\n\nIl dataset di input contenente le variabili da analizzare.\nIl metodo “EBICglasso” per la regolarizzazione della rete.\nIl calcolo automatico delle correlazioni tra variabili.\n\nLa funzione opera attraverso questi passaggi:\n\nGenera 100 modelli di rete differenti.\nValuta ogni modello utilizzando l’Extended Bayesian Information Criterion (EBIC).\nSeleziona il modello ottimale che bilancia precisione e parsimonia.\n\nIl parametro gamma (γ) controlla il livello di parsimonia della rete:\n\nγ = 0: produce una rete più densa con molte connessioni.\nγ = 0.5 (valore consigliato): mantiene solo le connessioni più rilevanti.\n\n\nallNetwork &lt;- estimateNetwork(\n    allData,\n    default = \"EBICglasso\",\n    corMethod = \"cor_auto\",\n    tuning = 0.5\n)\n\nQuesto codice produrrà una rete statistica ottimizzata che evidenzia le relazioni più importanti tra le variabili, eliminando le connessioni spurie o meno rilevanti.\n\n# Visualizzazione del risultato\nsummary(allNetwork)\n#&gt; \n#&gt; === Estimated network ===\n#&gt; Number of nodes: 6 \n#&gt; Number of non-zero edges: 15 / 15 \n#&gt; Mean weight: 0.1397 \n#&gt; Network stored in object$graph \n#&gt;  \n#&gt; Default set used: EBICglasso \n#&gt;  \n#&gt; Use plot(object) to plot estimated network \n#&gt; Use bootnet(object) to bootstrap edge weights and centrality indices \n#&gt; \n#&gt; Relevant references:\n#&gt; \n#&gt;      Friedman, J. H., Hastie, T., & Tibshirani, R. (2008). Sparse inverse covariance estimation with the graphical lasso. Biostatistics, 9 (3), 432-441.\n#&gt;  Foygel, R., & Drton, M. (2010). Extended Bayesian information criteria for Gaussian graphical models. \n#&gt;  Friedman, J. H., Hastie, T., & Tibshirani, R. (2014). glasso: Graphical lasso estimation of gaussian graphical models. Retrieved from https://CRAN.R-project.org/package=glasso\n#&gt;  Epskamp, S., Cramer, A., Waldorp, L., Schmittmann, V. D., & Borsboom, D. (2012). qgraph: Network visualizations of relationships in psychometric data. Journal of Statistical Software, 48 (1), 1-18.\n#&gt;  Epskamp, S., Borsboom, D., & Fried, E. I. (2016). Estimating psychological networks and their accuracy: a tutorial paper. arXiv preprint, arXiv:1604.08462.\n\nIn sintesi, la funzione estimateNetwork() consente di costruire una rete psicologica affidabile e parsimoniosa, sfruttando il metodo delle correlazioni parziali regolarizzate con penalizzazione LASSO. Questo approccio garantisce un equilibrio ottimale tra complessità e accuratezza, rendendo la rete uno strumento efficace per analizzare le relazioni tra variabili.",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#creazione-del-grafico-della-rete",
    "href": "chapters/networks/01_networks.html#creazione-del-grafico-della-rete",
    "title": "24  Network psicologici",
    "section": "\n24.8 Creazione del Grafico della Rete",
    "text": "24.8 Creazione del Grafico della Rete\nLa visualizzazione della rete è un passaggio fondamentale per interpretare le relazioni tra variabili. Con la funzione plot(), possiamo creare un grafico chiaro e informativo. Per impostazione predefinita, il grafico utilizza un tema inclusivo per daltonici e rappresenta le associazioni condizionate tra i nodi con le seguenti caratteristiche:\n\n\nArchi blu: rappresentano correlazioni positive.\n\n\nArchi rossi: rappresentano correlazioni negative.\n\n\nSpessore degli archi: proporzionale alla magnitudine delle correlazioni parziali regolarizzate.\n\nAd esempio, il grafico può mostrare una forte associazione tra motivazione, autonomia e competenza, mentre le emozioni risultano strettamente legate alla competenza. Tutte le relazioni visualizzate nel grafico sono condizionate, ovvero tengono conto degli effetti di tutte le altre variabili nella rete, analogamente a quanto avviene in un’analisi di regressione.\n\nallDataPlot &lt;- plot(allNetwork)\nLX &lt;- allDataPlot$layout\n\n\n\n\n\n\n\nÈ possibile salvare il grafico in un oggetto R, come allDataPlot. Questo oggetto non solo memorizza il grafico, ma contiene anche informazioni utili, tra cui:\n\nLa matrice di correlazione.\nI parametri di configurazione del grafico.\nIl layout della rete (disposizione dei nodi), salvato in allDataPlot$layout. Questo layout può essere riutilizzato per mantenere una disposizione visiva coerente tra grafici di reti diverse, agevolando i confronti.\n\nPer migliorare la leggibilità e l’interpretazione, è utile personalizzare il grafico. Ecco alcuni esempi di opzioni disponibili:\n\n\nTitolo: Utilizzare l’argomento title per aggiungere un titolo descrittivo.\n\nDimensione dei nodi: Regolare con l’opzione vsize per migliorare la visibilità.\n\n\nPesi degli archi: Impostare edge.labels = TRUE per visualizzare i valori numerici delle correlazioni sul grafico.\n\n\nSoglia di visibilità degli archi:\n\n\ncut = 0.10: evidenzia gli archi con correlazioni superiori a 0.10, mentre gli altri saranno rappresentati con colori più tenui.\n\n\nminimum = 0.05: nasconde archi con valori inferiori a 0.05, riducendo il rumore visivo senza eliminarli dalla rete.\n\n\n\n\nLayout: Specificare un layout, ad esempio \"spring\", per posizionare automaticamente i nodi in modo intuitivo.\n\nEsempio di grafico personalizzato:\n\nallDataPlot &lt;- plot(\n    allNetwork,\n    title = \"Both countries combined\",\n    vsize = 9,\n    edge.labels = TRUE,\n    cut = 0.10,\n    minimum = 0.05,\n    layout = \"spring\"\n)\n\n\n\n\n\n\n\nPersonalizzare il grafico permette di:\n\nMigliorare la leggibilità, evidenziando le connessioni più rilevanti.\n\nRidurre il rumore visivo, concentrandosi su associazioni più robuste.\n\nGarantire coerenza visiva tra grafici di reti diverse, facilitando analisi comparative.\n\nLa possibilità di salvare e riutilizzare il layout o altre configurazioni consente di ottimizzare la comunicazione visiva, rendendo i risultati più chiari e facilmente interpretabili.",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#predicibilità-dei-nodi",
    "href": "chapters/networks/01_networks.html#predicibilità-dei-nodi",
    "title": "24  Network psicologici",
    "section": "\n24.9 Predicibilità dei Nodi",
    "text": "24.9 Predicibilità dei Nodi\nLa predicibilità di un nodo rappresenta la proporzione di varianza che può essere spiegata dalle connessioni di quel nodo con gli altri nodi della rete. È una misura chiave per comprendere l’influenza reciproca delle variabili all’interno della rete e per identificare nodi centrali o meno integrati.\nLa predicibilità si basa su una regressione lineare in cui:\n\nOgni nodo viene considerato come variabile dipendente.\nGli altri nodi della rete agiscono come predittori.\nSi calcola il coefficiente di determinazione (\\(R^2\\)), che indica la percentuale di varianza spiegata.\n\nLa predicibilità viene calcolata per ciascun nodo. Valori di \\(R^2\\) vicini a 0 indicano che il nodo è poco influenzato dalle sue connessioni, mentre valori vicini a 1 suggeriscono che il nodo è fortemente spiegato dalle sue relazioni.\n\n24.9.0.1 Interpretazione di \\(R^2\\)\n\n\n\n\\(R^2 = 0\\): Il nodo non è spiegato dalle sue connessioni. Questo potrebbe indicare che la variabile è marginale nella rete o mal misurata.\n\n\\(R^2 &gt; 0\\): Indica una connessione significativa con altri nodi, proporzionata al valore di \\(R^2\\).\n\n\\(R^2\\) molto alto (vicino a 1): Potrebbe suggerire una ridondanza del nodo rispetto ad altri oppure un modello sovrastimato.\n\n24.9.1 Predicibilità e Controllabilità\nLa predicibilità è strettamente collegata al concetto di controllabilità, cioè la capacità di influenzare un nodo tramite le sue connessioni con altri nodi. Un nodo con alta predicibilità è particolarmente sensibile ai cambiamenti delle variabili ad esso connesse, rendendolo un potenziale target per interventi mirati in ambiti come la psicologia clinica o l’educazione.\n\n24.9.2 Calcolo con il Pacchetto mgm\n\nPer stimare la predicibilità utilizziamo il pacchetto mgm, che permette di specificare il tipo di variabili incluse (es. gaussiane per variabili continue).\n\nfitAllData &lt;- mgm(\n  as.matrix(allData), \n  type = rep('g', 6) # Variabili gaussiane\n)\n#&gt; \n  |                                                                        \n  |                                                                  |   0%\n  |                                                                        \n  |-----------                                                       |  17%\n  |                                                                        \n  |----------------------                                            |  33%\n  |                                                                        \n  |---------------------------------                                 |  50%\n  |                                                                        \n  |--------------------------------------------                      |  67%\n  |                                                                        \n  |-------------------------------------------------------           |  83%\n  |                                                                        \n  |------------------------------------------------------------------| 100%\n#&gt; Note that the sign of parameter estimates is stored separately; see ?mgm\n\nSuccessivamente, possiamo calcolare la predicibilità per ciascun nodo:\n\npredictAll &lt;- predict(fitAllData, na.omit(allData))\npredictAll$errors$R2\n#&gt; [1] 0.139 0.518 0.442 0.315 0.458 0.126\n\nPer valutare la qualità esplicativa complessiva della rete, calcoliamo la predicibilità media:\n\nmean(predictAll$errors$R2)\n#&gt; [1] 0.333\n\nOltre a \\(R^2\\), possiamo esaminare l’Errore Quadratico Medio (RMSE), che misura la discrepanza tra i valori osservati e quelli previsti:\n\nmean(predictAll$errors$RMSE)\n#&gt; [1] 0.8113\n\n\n24.9.3 Interpretazione dei Risultati\nL’analisi della predicibilità evidenzia quanto ciascun nodo sia integrato nella rete e fornisce indicazioni utili sul ruolo di ogni variabile:\n\nNodi con Alta Predicibilità: Variabili come competenza, motivazione e autonomia presentano valori di \\(R^2\\) elevati. Questi nodi sono fortemente influenzati dalle loro connessioni, indicando che sono centrali nella rete e ben integrati nel modello.\n\nNodi con Bassa Predicibilità: Variabili come apprendimento autoregolato (SRL) e relazionalità hanno \\(R^2\\) più bassi. Questi nodi potrebbero essere meno influenti o scarsamente connessi, suggerendo la necessità di ulteriori verifiche, ad esempio:\n\nLa variabile è meno centrale nel sistema studiato.\nMancano connessioni significative con altri nodi.\nPossono esserci problemi nella misurazione o nella definizione della variabile.\n\n\n\nLa predicibilità può anche essere rappresentata graficamente, con il valore di \\(R^2\\) visualizzato come grafici a torta all’interno dei nodi:\n\nallDataPlot &lt;- plot(\n    allNetwork,\n    title = \"Both countries combined\",\n    vsize = 9,\n    edge.labels = TRUE,\n    cut = 0.10,\n    minimum = 0.05,\n    pie = predictAll$errors$R2\n)\n\n\n\n\n\n\n\nL’analisi della predicibilità offre informazioni utili per:\n\n\nIdentificare target di intervento: Nodi con alta predicibilità (es. competenza e motivazione) sono particolarmente sensibili alle connessioni, rendendoli strategici per interventi mirati.\n\nRivedere nodi marginali: Nodi con bassa predicibilità (es. SRL e relazionalità) richiedono un’ulteriore esplorazione per comprendere meglio il loro ruolo nel sistema.\n\nLa combinazione di \\(R^2\\), RMSE e la rappresentazione grafica permette di ottenere una visione completa della rete, aiutando a individuare punti di forza e aree da approfondire.",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#inferenza-sulla-rete-misure-di-centralità",
    "href": "chapters/networks/01_networks.html#inferenza-sulla-rete-misure-di-centralità",
    "title": "24  Network psicologici",
    "section": "\n24.10 Inferenza sulla Rete: Misure di Centralità",
    "text": "24.10 Inferenza sulla Rete: Misure di Centralità\nLe misure di centralità consentono di identificare i nodi più influenti e importanti all’interno di una rete psicologica. Queste misure forniscono informazioni sul ruolo di ciascun nodo nella rete, aiutando a individuare variabili chiave che potrebbero rappresentare bersagli strategici per interventi o analisi approfondite.\n\n24.10.1 Principali Misure di Centralità\nTra le numerose misure disponibili, le seguenti sono le più comunemente utilizzate per le reti psicologiche grazie alla loro interpretazione chiara e al valore pratico:\n\n\nGrado di centralità (Degree Centrality): Indica il numero di connessioni dirette di un nodo, ignorando il peso degli archi.\n\nForza di centralità (Strength Centrality): Somma i pesi assoluti di tutte le connessioni di un nodo, evidenziando l’intensità complessiva delle sue relazioni.\n\nInfluenza attesa (Expected Influence): Somma i pesi grezzi (positivi e negativi) delle connessioni di un nodo, fornendo un’indicazione dell’effetto complessivo delle sue relazioni.\n\nAd esempio, per un nodo con connessioni 0.3, -0.1 e 0.5:\n\n\nDegree Centrality: 3 (numero di connessioni).\n\n\nStrength Centrality: \\(|0.3| + |-0.1| + |0.5| = 0.9\\) (somma dei pesi assoluti).\n\n\nExpected Influence: \\(0.3 + (-0.1) + 0.5 = 0.7\\) (somma dei pesi grezzi).\n\nSe nella rete non sono presenti archi negativi, la Strength Centrality e l’Expected Influence coincidono.\nAltre misure, come closeness, betweenness ed eigenvector centrality, possono essere calcolate, ma spesso hanno un’interpretazione meno diretta nel contesto delle reti psicologiche e non sono generalmente raccomandate per un’analisi standard.\n\n24.10.2 Calcolo e Visualizzazione delle Misure di Centralità\nPer stimare e visualizzare le misure di centralità, utilizziamo il pacchetto bootnet. La funzione centralityPlot() consente di creare un grafico con le misure selezionate, standardizzate come z-score per una migliore interpretazione visiva.\n\n24.10.2.1 Esempio di Visualizzazione\n\ncentralityPlot(\n    allNetwork,\n    include = c(\"ExpectedInfluence\", \"Strength\"),\n    scale = \"z-scores\"\n)\n\n\n\n\n\n\n\nSe si desiderano i valori numerici delle centralità, è possibile utilizzare la funzione centralityTable():\n\ncentralityTable(allNetwork)\n#&gt;      graph type        node           measure   value\n#&gt; 1  graph 1   NA Relatedness       Betweenness -0.6172\n#&gt; 2  graph 1   NA  Competence       Betweenness  0.7715\n#&gt; 3  graph 1   NA    Autonomy       Betweenness  1.6973\n#&gt; 4  graph 1   NA     Emotion       Betweenness -0.6172\n#&gt; 5  graph 1   NA  Motivation       Betweenness -0.6172\n#&gt; 6  graph 1   NA         SRL       Betweenness -0.6172\n#&gt; 7  graph 1   NA Relatedness         Closeness -1.1443\n#&gt; 8  graph 1   NA  Competence         Closeness  0.7205\n#&gt; 9  graph 1   NA    Autonomy         Closeness  1.2060\n#&gt; 10 graph 1   NA     Emotion         Closeness -0.2021\n#&gt; 11 graph 1   NA  Motivation         Closeness  0.5782\n#&gt; 12 graph 1   NA         SRL         Closeness -1.1584\n#&gt; 13 graph 1   NA Relatedness          Strength -1.0738\n#&gt; 14 graph 1   NA  Competence          Strength  1.2241\n#&gt; 15 graph 1   NA    Autonomy          Strength  0.6340\n#&gt; 16 graph 1   NA     Emotion          Strength -0.3095\n#&gt; 17 graph 1   NA  Motivation          Strength  0.6949\n#&gt; 18 graph 1   NA         SRL          Strength -1.1697\n#&gt; 19 graph 1   NA Relatedness ExpectedInfluence -1.0026\n#&gt; 20 graph 1   NA  Competence ExpectedInfluence  1.2197\n#&gt; 21 graph 1   NA    Autonomy ExpectedInfluence  0.6490\n#&gt; 22 graph 1   NA     Emotion ExpectedInfluence -0.3711\n#&gt; 23 graph 1   NA  Motivation ExpectedInfluence  0.7079\n#&gt; 24 graph 1   NA         SRL ExpectedInfluence -1.2029\n\n\n24.10.3 Misure Avanzate con NetworkToolbox\n\nIl pacchetto NetworkToolbox offre un’ampia gamma di misure di centralità, tra cui:\n\n\nDegree Centrality: Numero di connessioni dirette di un nodo.\n\nStrength Centrality: Intensità complessiva delle connessioni.\n\nCloseness Centrality: Misura della vicinanza di un nodo a tutti gli altri.\n\nEigenvector Centrality: Valuta l’importanza di un nodo considerando anche l’importanza dei suoi vicini.\n\nLeverage Centrality: Peso relativo delle connessioni di un nodo rispetto ai suoi vicini.\n\n\nDegree &lt;- degree(allNetwork$graph)\nStrength &lt;- strength(allNetwork$graph)\nBetweenness &lt;- betweenness(allNetwork$graph)\nCloseness &lt;- closeness(allNetwork$graph)\nEigenvector &lt;- eigenvector(allNetwork$graph)\nLeverage &lt;- leverage(allNetwork$graph)\n\ndata.frame(\n    Variable = names(Degree),\n    Degree,\n    Strength,\n    Betweenness,\n    Closeness,\n    Eigenvector,\n    Leverage\n)\n#&gt;                Variable Degree Strength Betweenness Closeness Eigenvector\n#&gt; Relatedness Relatedness      5     0.40           0     1.887       0.217\n#&gt; Competence   Competence      5     1.07           6     3.267       0.553\n#&gt; Autonomy       Autonomy      5     0.90          10     3.626       0.475\n#&gt; Emotion         Emotion      5     0.62           0     2.584       0.363\n#&gt; Motivation   Motivation      5     0.91           0     3.162       0.495\n#&gt; SRL                 SRL      5     0.37           0     1.877       0.211\n#&gt;             Leverage\n#&gt; Relatedness   -3.429\n#&gt; Competence     1.349\n#&gt; Autonomy       1.066\n#&gt; Emotion       -0.425\n#&gt; Motivation     1.105\n#&gt; SRL           -5.379\n\nInterpretazione delle misure:\n\n\nGrado e forza di centralità: Indicazioni semplici e dirette del numero e della forza delle connessioni di un nodo.\n\nInfluenza attesa: Misura più raffinata che considera il bilancio complessivo delle connessioni, includendo sia pesi positivi sia negativi.\n\nMisure avanzate: Strumenti utili in analisi specifiche, ma da usare con cautela in contesti psicologici standard, poiché la loro interpretazione può risultare meno intuitiva.\n\n24.10.4 Implicazioni Pratiche\nLe misure di centralità sono strumenti cruciali per:\n\n\nIndividuare target di intervento: Nodi con elevata forza o influenza attesa possono rappresentare variabili chiave su cui focalizzarsi.\n\nValutare l’integrazione dei nodi nella rete: Nodi con basse misure di centralità possono essere marginali o meno influenti, suggerendo potenziali problemi nella definizione o nella misurazione della variabile.\n\nLe misure calcolate forniscono una visione approfondita della struttura della rete, guidando analisi teoriche e applicative in contesti come la psicologia clinica, l’educazione e la ricerca sui sistemi complessi.",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#altre-opzioni-per-la-stima-delle-reti",
    "href": "chapters/networks/01_networks.html#altre-opzioni-per-la-stima-delle-reti",
    "title": "24  Network psicologici",
    "section": "\n24.11 Altre Opzioni per la Stima delle Reti",
    "text": "24.11 Altre Opzioni per la Stima delle Reti\nCome accennato in precedenza, oltre alle reti basate su correlazioni parziali regolarizzate, esistono diverse altre opzioni di stima delle reti. Di seguito ne presentiamo alcune, ma per ulteriori dettagli si consiglia di consultare le pagine del manuale della funzione estimateNetwork().\n\n24.11.1 Rete di Associazione\nLa rete di associazione (correlation network) si basa sulle correlazioni semplici tra le variabili. Questo tipo di rete è utile principalmente per esplorazioni preliminari dei dati, ma non è generalmente raccomandata per analisi definitive, poiché tende a produrre reti molto dense con molteplici connessioni non significative.\n\nallNetwork_cor &lt;- estimateNetwork(allData,\n    default = \"cor\", verbose = FALSE\n)\n\n\n24.11.2 Metodo ggmModSelect()\n\nIl metodo ggmModSelect() è particolarmente indicato per dataset di grandi dimensioni con un numero ridotto di nodi. Funziona in questo modo:\n\nParte da una rete regolarizzata come punto di riferimento iniziale.\nStima tutte le possibili reti non regolarizzate.\nSeleziona il modello migliore in base al criterio EBIC (Extended Bayesian Information Criterion), scegliendo quello con il valore più basso.\n\nQuesto approccio combina i vantaggi della regolarizzazione con la flessibilità di reti non regolarizzate, risultando in un modello più accurato per dataset specifici.\n\nallNetwork_mgm &lt;- estimateNetwork(allData,\n    default = \"ggmModSelect\", verbose = FALSE\n)\n\n\n24.11.3 Rete di Importanza Relativa\nLa rete di importanza relativa (relimp; Relative Importance Network) stima una rete direzionale basata sull’importanza relativa dei predittori in un modello di regressione lineare. In questa rete:\n\nGli archi rappresentano la magnitudine dell’importanza relativa di ciascun predittore.\nLa direzione degli archi indica come ciascuna variabile influenza le altre, secondo i risultati della regressione.\n\nQuesta rete è utile per identificare relazioni causali teoriche o per evidenziare come alcune variabili predicono altre all’interno del dataset.\n\nallNetwork_relimp &lt;- estimateNetwork(allData,\n    default = \"relimp\", verbose = FALSE\n)\n\n\n24.11.4 Confronto tra i Modelli\n\nLa rete ggmModSelect() produce risultati molto simili alla rete regolarizzata standard, con differenze minime nelle connessioni più deboli.\nLa rete di associazione è invece molto densa, poiché include tutte le correlazioni, senza applicare alcuna penalizzazione.\nLa rete di importanza relativa è direzionale e fornisce informazioni aggiuntive su come una variabile influenza le altre.\n\n\nplot(\n    allNetwork_cor, title = \"Correlation\", vsize = 18, edge.labels = TRUE, \n    cut = 0.10, minimum = 0.05, layout = LX\n)\n\n\n\n\n\n\n\n\nplot(\n    allNetwork, title = \"EBICglasso\", vsize = 18, edge.labels = TRUE,\n    cut = 0.10, minimum = 0.05, layout = LX\n)\n\n\n\n\n\n\n\n\nplot(\n    allNetwork_mgm, title = \"ggmModSelect\", vsize = 18, edge.labels = TRUE,\n    cut = 0.10, minimum = 0.05, layout = LX\n)\n\n\n\n\n\n\n\n\nplot(\n    allNetwork_relimp, title = \"Relative importance\", vsize = 18, \n    edge.labels = TRUE, cut = 0.10, minimum = 0.05, layout = LX\n)",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#confronto-tra-reti",
    "href": "chapters/networks/01_networks.html#confronto-tra-reti",
    "title": "24  Network psicologici",
    "section": "\n24.12 Confronto tra Reti",
    "text": "24.12 Confronto tra Reti\nDopo aver illustrato i passaggi fondamentali per stimare una singola rete, procediamo ora al confronto tra diverse reti. Le reti psicologiche offrono metodi per confrontare le reti nel loro complesso, i pesi degli archi (edge weights) e le misure di centralità.\nNel nostro caso, disponendo di dati provenienti da due paesi (Finlandia e Austria), possiamo stimare due reti separate, una per ciascun paese, e confrontarle per osservare come differiscono.\nPer confrontare le reti, dobbiamo innanzitutto stimarle separatamente per ciascun paese. I passaggi di stima sono gli stessi descritti in precedenza. Per ciascun paese, iniziamo con i passi seguenti:\n\nControllo delle assunzioni per ciascun dataset (ad esempio, verifica che la matrice di correlazione sia positiva definita).\nVerifica con l’algoritmo goldbricker per identificare nodi altamente simili che potrebbero necessitare di essere combinati o ridotti. In questo caso, i risultati mostrano che:\nLe matrici di correlazione di entrambe le reti sono già positive definite.\nNon ci sono nodi altamente simili che richiedono riduzioni.\n\n\n### Check the assumptions\n## Finland\n# check for positive definitiveness\ncorrelationMatrix &lt;- cor(\n    x = finlandData, \n    use = c(\"pairwise.complete.obs\")\n)\n\nis.positive.definite(correlationMatrix)\n#&gt; [1] TRUE\n# check for redundancy\ngoldbricker(finlandData,\n    p = 0.05, method = \"hittner2003\",\n    threshold = 0.25, corMin = 0.5, progressbar = FALSE\n)\n#&gt; Suggested reductions: Less than 25 % of correlations are significantly different for the following pairs: \n#&gt; [1] \"No suggested reductions\"\n\n\n## Austria\n# check for positive definitiveness\ncorrelationMatrix &lt;- cor(\n    x = austriaData, \n    use = c(\"pairwise.complete.obs\")\n)\n\nis.positive.definite(correlationMatrix)\n#&gt; [1] TRUE\n# check for redundancy\ngoldbricker(austriaData,\n    p = 0.05, method = \"hittner2003\",\n    threshold = 0.25, corMin = 0.5, progressbar = FALSE\n)\n#&gt; Suggested reductions: Less than 25 % of correlations are significantly different for the following pairs: \n#&gt; [1] \"No suggested reductions\"\n\nStima dei networks:\n\n# Estimate the networks\nfinlandNetwork &lt;- estimateNetwork(\n    finlandData,\n    default = \"EBICglasso\", corMethod = \"cor_auto\", tuning = 0.5\n)\n\naustriaNetwork &lt;- estimateNetwork(\n    austriaData,\n    default = \"EBICglasso\", corMethod = \"cor_auto\", tuning = 0.5\n)\n\nCalcoliamo la predicibilità per ciascun paese.\n\n# Compute the predictability\nfitFinland &lt;- mgm(\n    as.matrix(finlandData), # data\n    c(\"g\", \"g\", \"g\", \"g\", \"g\", \"g\"),\n    # distribution for each var\n    verbatim = TRUE, # hide warnings and progress bar\n    signInfo = FALSE # hide message about signs\n)\n\n\npredictFinland &lt;- predict(fitFinland, na.omit(finlandData))\nmean(predictFinland$errors$R2)\n#&gt; [1] 0.3085\nmean(predictFinland$errors$RMSE)\n#&gt; [1] 0.8283\n\n\nfitAustria &lt;- mgm(\n    as.matrix(austriaData), # data\n    c(\"g\", \"g\", \"g\", \"g\", \"g\", \"g\"),\n    # distribution for each var\n    verbatim = TRUE, # hide warnings and progress bar\n    signInfo = FALSE # hide message about signs\n)\n\n\npredictAustria &lt;- predict(fitAustria, na.omit(austriaData))\nmean(predictAustria$errors$R2)\n#&gt; [1] 0.3437\nmean(predictAustria$errors$RMSE)\n#&gt; [1] 0.8038\n\nDopo aver stimato le reti, possiamo visualizzarle per facilitare il confronto. Questo approccio consente di identificare rapidamente le differenze e le somiglianze tra le reti dei due paesi.\n\nAverageLayout &lt;- averageLayout(finlandNetwork, austriaNetwork)\n\n\nplot(finlandNetwork, # input network\n    title = \"Finland\", # plot title\n    vsize = 19, # size of the nodes\n    edge.labels = TRUE, # label the edge weights\n    cut = 0.10, # saturate edges &gt; .10\n    minimum = 0.05, # remove edges &lt; .05\n    pie = predictFinland$errors$R2, # put R2 as pie\n    layout = LX\n) # set the layout\n\n\n\n\n\n\n\n\nplot(austriaNetwork, # input network\n    title = \"Austria\", # plot title\n    vsize = 19, # size of the nodes\n    edge.labels = TRUE, # label the edge weights\n    cut = 0.10, # saturate edges &gt; .10\n    minimum = 0.05, # remove edges &lt; .05\n    pie = predictAustria$errors$R2, # put R2 as pie\n    layout = LX\n) # set the layout\n\n\n\n\n\n\n\nPossiamo rappresentare graficamente la differenza tra le due reti usando qgraph(). La funzione qgraph() richiede come input una rete stimata o una matrice. Per creare una rete di differenza, è necessario sottrarre le due matrici delle connessioni delle reti (ad esempio, finlandNetwork$graph - austriaNetwork$graph).\nIl seguente codice mostra come visualizzare la rete di differenza che evidenzia le variazioni nei pesi delle connessioni tra le due reti.\n\nqgraph(finlandNetwork$graph - abs(austriaNetwork$graph),\n    title = \"Difference\", # plot title\n    theme = allDataPlot$Arguments$theme,\n    vsize = 19, # size of the nodes\n    edge.labels = TRUE, # label the edge weights\n    labels = allDataPlot$Arguments$labels, # node labels\n    cut = 0.10, # saturate edges &gt; .10\n    layout = LX\n) # set the layout\n\n\n\n\n\n\n\nIl confronto tra le reti evidenzia differenze tra i due paesi:\n\n\nFinlandia:\n\nConnessione più forte tra competenza ed emozione.\nConnessione più forte tra motivazione e relazionalità.\n\n\n\nAustria:\n\nConnessioni più forti tra:\n\n\nMotivazione e competenza.\n\nMotivazione ed emozione.\n\nCompetenza e autonomia.\n\nAutonomia e relazionalità.\n\n\n\n\n\nQueste differenze suggeriscono che le relazioni psicologiche tra le variabili possono essere influenzate da fattori specifici di ciascun contesto culturale o sociale. La rete di differenza rappresenta un utile strumento visivo per evidenziare queste variazioni e guidare l’interpretazione.\nUn confronto visivo delle centralità può essere effettuato nello stesso modo descritto in precedenza. Per farlo, forniamo le reti che vogliamo confrontare come una lista e specifichiamo le misure di centralità da calcolare.\nI risultati mostrano che:\n\nNella rete dell’Austria, la variabile con il valore di centralità più alto è la motivazione, indicando che la motivazione è il fattore principale che guida la connettività nella rete.\nNella rete della Finlandia, la variabile più centrale è la competenza, che risulta essere il driver principale della connettività della rete.\n\nQuesto confronto mette in evidenza come le variabili centrali differiscano tra i due contesti, suggerendo che fattori culturali o ambientali possono influenzare il ruolo delle variabili psicologiche nella struttura della rete.\n\ncentralityPlot(\n    list(\n        Finland = finlandNetwork,\n        Austria = austriaNetwork\n    ),\n    include = c(\"ExpectedInfluence\", \"Strength\")\n)",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#confronto-statistico-tra-reti",
    "href": "chapters/networks/01_networks.html#confronto-statistico-tra-reti",
    "title": "24  Network psicologici",
    "section": "\n24.13 Confronto Statistico tra Reti",
    "text": "24.13 Confronto Statistico tra Reti\nPer confrontare in modo rigoroso le reti, è necessario utilizzare un test statistico che permetta di stabilire quali differenze nei pesi degli archi o nelle misure di centralità siano significative e non dovute al caso. Il Network Comparison Test (NCT) è uno strumento per effettuare un confronto dettagliato della struttura delle reti, dei pesi degli archi e delle centralità.\nL’NCT utilizza un approccio basato su permutazioni:\n\nGenera un grande numero di reti permutate a partire dalle reti originali, creando una distribuzione di riferimento.\nConfronta le reti originali con quelle permutate per determinare se le differenze osservate sono statisticamente significative.\n\nPer eseguire il test, dobbiamo:\n\nFornire le due reti da confrontare.\nSpecificare il numero di iterazioni (almeno 1000 è raccomandato per risultati affidabili).\nTestare gli archi con test.edges = TRUE e edges = 'all' per verificare tutte le connessioni.\nTestare le centralità con test.centrality = TRUE (poiché non vengono testate di default).\n\n\nset.seed(1337)\n\nCompared &lt;- NCT(\n    finlandNetwork, # network 1\n    austriaNetwork, # network 2\n    verbose = FALSE, # hide warnings and progress bar\n    it = 1000, # number of iterations\n    abs = TRUE,\n    binary.data = FALSE, # set data distribution\n    test.edges = TRUE, # test edge differences\n    edges = 'all', # which edges to test\n    test.centrality = TRUE, # test centrality\n    progressbar = FALSE # progress bar\n)\n\n\nCompared$glstrinv.sep # Separate global strength values of the individual networks\n#&gt; [1] 2.148 2.172\n\n\nCompared$einv.pvals # Holm-Bonferroni adjusted p-values for each edge\n#&gt;           Var1       Var2  p-value Test statistic E\n#&gt; 7  Relatedness Competence 0.017982          0.06813\n#&gt; 13 Relatedness   Autonomy 0.001998          0.09520\n#&gt; 14  Competence   Autonomy 0.000999          0.13479\n#&gt; 19 Relatedness    Emotion 0.165834          0.04143\n#&gt; 20  Competence    Emotion 0.000999          0.17131\n#&gt; 21    Autonomy    Emotion 0.009990          0.07668\n#&gt; 25 Relatedness Motivation 0.000999          0.13714\n#&gt; 26  Competence Motivation 0.000999          0.16045\n#&gt; 27    Autonomy Motivation 0.003996          0.08758\n#&gt; 28     Emotion Motivation 0.001998          0.12874\n#&gt; 31 Relatedness        SRL 0.077922          0.04673\n#&gt; 32  Competence        SRL 0.000999          0.12582\n#&gt; 33    Autonomy        SRL 0.688312          0.01228\n#&gt; 34     Emotion        SRL 0.073926          0.05299\n#&gt; 35  Motivation        SRL 0.301698          0.03063\n\n\nCompared$diffcen.real # Difference in centralities\n#&gt;             strength expectedInfluence\n#&gt; Relatedness  0.10477           0.10477\n#&gt; Competence   0.07001           0.07001\n#&gt; Autonomy    -0.22861          -0.22861\n#&gt; Emotion      0.12670           0.21367\n#&gt; Motivation  -0.20900          -0.20900\n#&gt; SRL          0.08803           0.17499\n\n\nCompared$diffcen.pval # Holm-Bonferroni adjusted p-values for each centrality\n#&gt;             strength expectedInfluence\n#&gt; Relatedness 0.006993          0.006993\n#&gt; Competence  0.127872          0.127872\n#&gt; Autonomy    0.000999          0.000999\n#&gt; Emotion     0.009990          0.000999\n#&gt; Motivation  0.000999          0.000999\n#&gt; SRL         0.132867          0.000999\n\n\n24.13.1 Interpretazione dei Risultati\nForza Globale delle Reti. Finlandia: 2.15, Austria: 2.17. La differenza tra le due reti (\\(\\Delta = 0.024\\)) non è statisticamente significativa, indicando che le reti hanno una connettività complessiva simile.\nDifferenze nei Pesi degli Archi. Gli archi Competence-Autonomy, Competence-Emotion, Competence-Motivation, Relatedness-Competence e Relatedness-Motivation mostrano differenze statisticamente significative (\\(p &lt; 0.05\\)). Archi come Relatedness-Emotion e Autonomy-SRL non mostrano differenze significative.\nDifferenze nelle Centralità. Autonomy e Motivation hanno differenze significative sia in strength che in expected influence (\\(p &lt; 0.001\\)), suggerendo che il loro ruolo nella rete varia notevolmente tra i due paesi. Relatedness e Emotion mostrano differenze significative (\\(p &lt; 0.01\\)). Competence e SRL non presentano differenze significative nelle centralità.\nIn conclusione, le reti di Finlandia e Austria hanno una forza globale simile, ma mostrano differenze significative in alcune connessioni chiave e centralità. Competence, Autonomy, e Motivation giocano ruoli diversi nei due contesti culturali. Le differenze nei pesi degli archi e nelle centralità suggeriscono influenze specifiche di fattori contestuali o culturali.",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#la-rete-della-variabilità",
    "href": "chapters/networks/01_networks.html#la-rete-della-variabilità",
    "title": "24  Network psicologici",
    "section": "\n24.14 La Rete della Variabilità",
    "text": "24.14 La Rete della Variabilità\nLa rete della variabilità fornisce un’indicazione di come i pesi degli archi (le connessioni tra nodi) variano tra le reti. In altre parole, questa rete riflette il grado di variabilità o le differenze individuali presenti nella popolazione analizzata.\n\n\nArchi con bassa variabilità: Indicano che le connessioni sono simili tra le reti, ovvero stabili e consistenti.\n\nArchi con alta variabilità: Indicano che le connessioni differiscono significativamente tra le reti, suggerendo la presenza di differenze individuali o contestuali.\n\nPer costruire la rete della variabilità, calcoliamo la deviazione standard dei pesi degli archi tra le due reti. Il processo include:\n\nLa creazione di due matrici, una per ciascuna rete.\nUn ciclo che calcola la deviazione standard per ogni arco tra le due reti.\n\n\n# Construct a network where edges are standard deviations across edge weights of networks\nedgeMeanJoint &lt;- matrix(0, 6, 6)\nedgeSDJoint &lt;- matrix(0, 6, 6)\nfor (i in 1:6) {\n    for (j in 1:6) {\n        vector &lt;- c(getWmat(finlandNetwork)[i, j], getWmat\n        (austriaNetwork)[i, j])\n        edgeMeanJoint[i, j] &lt;- mean(vector)\n        edgeSDJoint[i, j] &lt;- sd(vector)\n    }\n}\n\nSuccessivamente, tracciamo le reti in cui i pesi degli archi rappresentano le deviazioni standard di tutti gli archi.\n\nqgraph(edgeSDJoint,\n    layout = LX, edge.labels = TRUE,\n    labels = allDataPlot$Arguments$labels, vsize = 9,\n    cut = 0.09, minimum = 0.01, theme = \"colorblind\"\n)\n\n\n\n\n\n\n\nAllo stesso modo in cui abbiamo confrontato i paesi, possiamo confrontare i generi. Come mostrato nel seguente blocco di codice, stimiamo la rete per il gruppo maschile, quella per il gruppo femminile e la rete di differenza. Le differenze risultano molto piccole o addirittura trascurabili.\n\nmaleNetwork &lt;- estimateNetwork(maleData, default = \"EBICglasso\")\nfemaleNetwork &lt;- estimateNetwork(femaleData, default = \"EBICglasso\")\n\nplot(maleNetwork,\n    title = \"Male\", vsize = 9, edge.labels = TRUE,\n    cut = 0.10, minimum = 0.05, layout = LX\n)\n\nplot(femaleNetwork,\n    title = \"Female\", vsize = 9,\n    edge.labels = TRUE,\n    cut = 0.10, minimum = 0.05, layout = LX\n)\n\nqgraph(femaleNetwork$graph - maleNetwork$graph,\n    title =\n        \"Difference\", cut = 0.1,\n    labels = allDataPlot$Arguments$labels, vsize = 9,\n    minimum = 0.01,\n    edge.labels = TRUE, layout = LX, theme =\n        \"colorblind\"\n)\n\nDi seguito eseguiamo il Network Comparison Test (NCT) e osserviamo che i valori \\(p\\) relativi alle differenze tra tutti gli archi non sono statisticamente significativi.\n\nComparedGender &lt;- NCT(\nmaleNetwork, # network 1\nfemaleNetwork, # network 2\nverbose = FALSE, # hide warnings and progress bar\nit = 1000, # number of iterations\nabs = T, # test strength or expected influence?\nbinary.data = FALSE, # set data distribution\ntest.edges = TRUE, # test edge differences\nedges = 'all', # which edges to test\nprogressbar = FALSE) # progress bar\nComparedGender$einv.pvals # Holm-Bonferroni adjusted p-values for each edge",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#valutazione-della-robustezza-e-accuratezza",
    "href": "chapters/networks/01_networks.html#valutazione-della-robustezza-e-accuratezza",
    "title": "24  Network psicologici",
    "section": "\n24.15 Valutazione della Robustezza e Accuratezza",
    "text": "24.15 Valutazione della Robustezza e Accuratezza\nIl metodo più comune per valutare la stabilità e l’accuratezza delle reti stimate è il bootstrapping. Questo procedimento prevede la creazione di un grande numero di reti bootstrappate (almeno 1000) a partire dai dati originali.\nPassaggi del Bootstrapping:\n\nSi generano un numero elevato di reti bootstrappate basate sui dati originali.\nSi calcolano i pesi degli archi per ciascuna di queste reti.\nSi utilizzano i pesi degli archi delle reti bootstrappate per costruire intervalli di confidenza che rappresentano l’accuratezza degli archi.\n\nInterpretazione:\n\nPer ogni arco nella rete stimata, i pesi vengono confrontati con gli intervalli di confidenza generati dalle reti bootstrappate.\nUn arco è considerato statisticamente significativo se i limiti superiore e inferiore dell’intervallo di confidenza non includono lo zero.\nAl contrario, un arco è non significativo se uno dei limiti dell’intervallo di confidenza attraversa la linea dello zero.\n\nQuesto approccio consente di identificare quali archi sono robusti e quali potrebbero essere il risultato di variabilità casuale nei dati.\n\nnCores &lt;- parallel::detectCores() - 1\n# Non-parametric bootstrap for stability of edges and of edge differences\n\nallBoot &lt;- bootnet(\n    allNetwork, # network input\n    default = \"EBICglasso\", # method\n    nCores = nCores, # number of cores for parallelization\n    computeCentrality = FALSE, # estimate centrality?\n    statistics = \"edge\" # what statistics do we want?\n)\n\n\nplot(allBoot,\n    plot = \"area\", order = \"sample\", legend = FALSE\n)\n\n\n\n\n\n\n\nCome mostrato nella Figura precedente, solo gli archi autonomy-emotion ed emotion-SRL attraversano la linea dello zero e, pertanto, non sono significativi.\nPossiamo inoltre tracciare il grafico delle differenze tra gli archi, che verifica se i pesi degli archi differiscono significativamente tra loro.\n\nplot(allBoot,\n    plot = \"difference\", order = \"sample\",\n    onlyNonZero = FALSE, labels = TRUE\n)\n\nInterpretazione del Grafico delle Differenze tra gli Archi\n\n\nQuadrati grigi: Indicano che l’intervallo di confidenza al 95% ottenuto dal bootstrapping per la differenza tra due archi attraversa la linea dello zero, suggerendo che la differenza non è statisticamente significativa.\n\nQuadrati neri: Indicano che l’intervallo di confidenza non attraversa lo zero, quindi la differenza tra i due archi è significativa.\n\nAd esempio:\n\nGli archi autonomy-emotion ed emotion-SRL presentano un quadrato grigio, indicando una differenza non significativa.\nGli archi emotion-SRL e relatedness-SRL presentano un quadrato nero, indicando che i due archi differiscono significativamente.\n\nL’accuratezza delle misure di centralità viene valutata tramite il case dropping test. In questo test, vengono eliminate diverse proporzioni di casi dai dati, e si calcola la correlazione tra la misura di centralità osservata e quella ottenuta dai dati ridotti. Se la correlazione diminuisce significativamente dopo l’eliminazione di un piccolo sottoinsieme di casi, la misura di centralità è considerata non affidabile.\n\nset.seed(1)\ncentBoot &lt;- bootnet(\n    allNetwork, # network input\n    default = \"EBICglasso\", # method\n    type = \"case\", # method for testing centrality stability\n    nCores = nCores, # number of cores\n    computeCentrality = TRUE, # compute centrality\n    statistics = c(\"strength\", \"expectedInfluence\"),\n    nBoots = 19000, # number of bootstraps\n    caseMin = .05, # min cases to drop\n    caseMax = .95 # max cases to drop\n)\n\nIl coefficiente di stabilità della correlazione è una metrica utilizzata per valutare la stabilità delle misure di centralità attraverso il case dropping test. Esso viene stimato come la massima proporzione di casi che può essere eliminata mantenendo una correlazione di almeno 0.7 con il campione originale.\n\ncorStability(centBoot)\n#&gt; === Correlation Stability Analysis === \n#&gt; \n#&gt; Sampling levels tested:\n#&gt;    nPerson Drop%    n\n#&gt; 1      358    95 1948\n#&gt; 2     1074    85 1876\n#&gt; 3     1790    75 1900\n#&gt; 4     2506    65 1802\n#&gt; 5     3222    55 1947\n#&gt; 6     3938    45 1927\n#&gt; 7     4654    35 1959\n#&gt; 8     5370    25 1861\n#&gt; 9     6086    15 1890\n#&gt; 10    6802     5 1890\n#&gt; \n#&gt; Maximum drop proportions to retain correlation of 0.7 in at least 95% of the samples:\n#&gt; \n#&gt; expectedInfluence: 0.95 (CS-coefficient is highest level tested)\n#&gt;   - For more accuracy, run bootnet(..., caseMin = 0.85, caseMax = 1) \n#&gt; \n#&gt; strength: 0.95 (CS-coefficient is highest level tested)\n#&gt;   - For more accuracy, run bootnet(..., caseMin = 0.85, caseMax = 1) \n#&gt; \n#&gt; Accuracy can also be increased by increasing both 'nBoots' and 'caseN'.\n\nSe tracciamo i risultati, possiamo osservare che il coefficiente di stabilità della correlazione è pari a 0.95, un valore molto elevato che indica un’alta stabilità degli archi.\n\nplot(centBoot)",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#riflessioni-conclusive",
    "href": "chapters/networks/01_networks.html#riflessioni-conclusive",
    "title": "24  Network psicologici",
    "section": "\n24.16 Riflessioni Conclusive",
    "text": "24.16 Riflessioni Conclusive\nIl campo delle reti psicologiche è in continua evoluzione, con metodi sempre più raffinati e applicazioni in costante crescita. In questo capitolo abbiamo esplorato i passaggi fondamentali per analizzare una rete psicologica, visualizzarne i risultati e confrontare diverse reti, oltre a introdurre tecniche statistiche robuste come il bootstrapping per valutare l’accuratezza delle reti stimate.\nUn tema rilevante è il confronto tra le reti psicologiche e altri approcci analitici. Ad esempio, l’Epistemic Network Analysis (ENA) presenta alcune limitazioni rispetto all’analisi delle reti psicologiche:\n\nNon offre strumenti per verificare se i pesi degli archi si discostano dal caso.\nNon include metodi rigorosi per confrontare reti o valutare i pesi degli archi.\nNon prevede misure di centralità o altre metriche tipiche delle reti.\n\nAllo stesso modo, il process mining, che genera reti di transizione, è limitato nella sua capacità di fornire test statistici per confermare la validità dei modelli o confrontarli con altri. L’analisi delle reti sociali (SNA), pur essendo più affine alle reti psicologiche, si concentra prevalentemente su archi non direzionati e positivi, utilizzati per analizzare interazioni sociali o semantiche, risultando meno flessibile per studiare le dipendenze complesse tra variabili psicologiche.\nLe reti psicologiche si distinguono per la loro capacità di rappresentare in modo dettagliato le interazioni e le dipendenze tra variabili. Grazie a una vasta gamma di metodi di stima e tecniche di ottimizzazione, offrono una prospettiva unica e ricca di possibilità analitiche. La comunità scientifica che sostiene questo approccio è particolarmente dinamica, contribuendo con continue innovazioni.\nUn ulteriore punto di forza è la flessibilità: le reti psicologiche non richiedono una teoria predefinita o assunzioni rigide sulle variabili. Questo le rende strumenti estremamente versatili, adatti sia per esplorazioni teoriche sia per analisi applicate. Come sottolineato da Borsboom et al., le reti psicologiche costituiscono un “ponte naturale tra l’analisi dei dati e la formulazione di teorie basate sui principi della scienza delle reti,” aprendo la strada alla generazione di ipotesi causali.\nIn conclusione, le reti psicologiche rappresentano uno strumento potente e innovativo per l’analisi dei sistemi complessi. Offrono nuove modalità per esplorare le interazioni tra variabili psicologiche, superando molte delle limitazioni di altri approcci. La loro capacità di integrare analisi empiriche e teorie emergenti le rende un contributo fondamentale per il progresso della ricerca psicologica, favorendo la scoperta di nuovi schemi interpretativi e la formulazione di ipotesi teoriche di grande impatto.\nRisorse Raccomandate\n\nEpskamp, S., Borsboom, D., & Fried, E. I. (2018). Estimating psychological networks and their accuracy: A tutorial paper. Behavior Research Methods, 50(1), 195–212. https://doi.org/10.3758/s13428-017-0862-1\nEpskamp, S., & Fried, E. I. (2018). A tutorial on regularized partial correlation networks. Psychological Methods, 23(4), 617–634. https://doi.org/10.1037/met0000167\nVan Borkulo, C. D., Van Bork, R., Boschloo, L., Kossakowski, J. J., Tio, P., Schoevers, R. A., & Waldorp, L. J. (2022). Comparing network structures on three aspects: A permutation test. Psychological Methods. https://doi.org/10.1037/met0000427\nBorsboom, D., Deserno, M. K., Rhemtulla, M., Epskamp, S., Fried, E. I., McNally, R. J., & Waldorp, L. J. (2021). Network analysis of multivariate data in psychological science. Nature Reviews Methods Primers, 1(1), 58. https://doi.org/10.1038/s43586-021-00055-w\nBringmann, L. F., Elmer, T., Epskamp, S., Krause, R. W., Schoch, D., Wichers, M., & Snippe, E. (2019). What do centrality measures measure in psychological networks? Journal of Abnormal Psychology, 128(8), 892–903. https://doi.org/10.1037/abn0000446",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/networks/01_networks.html#informazioni-sullambiente-di-sviluppo",
    "title": "24  Network psicologici",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.2\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] matrixcalc_1.0-6            mgm_1.2-15                 \n#&gt;  [3] qgraph_1.9.8                NetworkComparisonTest_2.2.2\n#&gt;  [5] NetworkToolbox_1.4.2        networktools_1.5.2         \n#&gt;  [7] bootnet_1.6                 rio_1.2.3                  \n#&gt;  [9] ggokabeito_0.1.0            see_0.11.0                 \n#&gt; [11] MASS_7.3-65                 viridis_0.6.5              \n#&gt; [13] viridisLite_0.4.2           ggpubr_0.6.0               \n#&gt; [15] ggExtra_0.10.1              gridExtra_2.3              \n#&gt; [17] patchwork_1.3.0             bayesplot_1.11.1           \n#&gt; [19] semTools_0.5-6              semPlot_1.1.6              \n#&gt; [21] lavaan_0.6-19               psych_2.4.12               \n#&gt; [23] scales_1.3.0                markdown_1.13              \n#&gt; [25] knitr_1.50                  lubridate_1.9.4            \n#&gt; [27] forcats_1.0.0               stringr_1.5.1              \n#&gt; [29] dplyr_1.1.4                 purrr_1.0.4                \n#&gt; [31] readr_2.1.5                 tidyr_1.3.1                \n#&gt; [33] tibble_3.2.1                ggplot2_3.5.1              \n#&gt; [35] tidyverse_2.0.0             here_1.0.1                 \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] splines_4.4.2       later_1.4.1         R.oo_1.27.0        \n#&gt;   [4] XML_3.99-0.18       rpart_4.1.24        lifecycle_1.0.4    \n#&gt;   [7] Rdpack_2.6.3        rstatix_0.7.2       doParallel_1.0.17  \n#&gt;  [10] rprojroot_2.0.4     lattice_0.22-6      survey_4.4-2       \n#&gt;  [13] rockchalk_1.8.157   backports_1.5.0     magrittr_2.0.3     \n#&gt;  [16] openxlsx_4.2.8      Hmisc_5.2-3         rmarkdown_2.29     \n#&gt;  [19] plotrix_3.8-4       yaml_2.3.10         IsingFit_0.4       \n#&gt;  [22] httpuv_1.6.15       zip_2.3.2           DBI_1.2.3          \n#&gt;  [25] pbapply_1.7-2       minqa_1.2.8         RColorBrewer_1.1-3 \n#&gt;  [28] multcomp_1.4-28     abind_1.4-8         quadprog_1.5-8     \n#&gt;  [31] R.utils_2.13.0      nnet_7.3-20         TH.data_1.1-3      \n#&gt;  [34] sandwich_3.1-1      relaimpo_2.2-7      gdata_3.0.1        \n#&gt;  [37] ellipse_0.5.0       arm_1.14-4          codetools_0.2-20   \n#&gt;  [40] tidyselect_1.2.1    shape_1.4.6.1       farver_2.1.2       \n#&gt;  [43] IsingSampler_0.2.3  lme4_1.1-36         stats4_4.4.2       \n#&gt;  [46] base64enc_0.1-3     eigenmodel_1.11     jsonlite_1.9.1     \n#&gt;  [49] e1071_1.7-16        mitml_0.4-5         Formula_1.2-5      \n#&gt;  [52] survival_3.8-3      iterators_1.0.14    emmeans_1.10.7     \n#&gt;  [55] foreach_1.5.2       tools_4.4.2         snow_0.4-4         \n#&gt;  [58] Rcpp_1.0.14         glue_1.8.0          mnormt_2.1.1       \n#&gt;  [61] pan_1.9             xfun_0.51           withr_3.0.2        \n#&gt;  [64] fastmap_1.2.0       mitools_2.4         boot_1.3-31        \n#&gt;  [67] digest_0.6.37       mi_1.1              timechange_0.3.0   \n#&gt;  [70] R6_2.6.1            mime_0.13           estimability_1.5.1 \n#&gt;  [73] mice_3.17.0         colorspace_2.1-1    gtools_3.9.5       \n#&gt;  [76] jpeg_0.1-10         weights_1.0.4       R.methodsS3_1.8.2  \n#&gt;  [79] generics_0.1.3      data.table_1.17.0   corpcor_1.6.10     \n#&gt;  [82] class_7.3-23        htmlwidgets_1.6.4   pkgconfig_2.0.3    \n#&gt;  [85] sem_3.1-16          gtable_0.3.6        htmltools_0.5.8.1  \n#&gt;  [88] carData_3.0-5       cocor_1.1-4         png_0.1-8          \n#&gt;  [91] wordcloud_2.6       reformulas_0.4.0    rstudioapi_0.17.1  \n#&gt;  [94] tzdb_0.5.0          reshape2_1.4.4      curl_6.2.1         \n#&gt;  [97] coda_0.19-4.1       checkmate_2.3.2     nlme_3.1-167       \n#&gt; [100] nloptr_2.2.1        proxy_0.4-27        zoo_1.8-13         \n#&gt; [103] parallel_4.4.2      miniUI_0.1.1.1      foreign_0.8-88     \n#&gt; [106] pillar_1.10.1       grid_4.4.2          vctrs_0.6.5        \n#&gt; [109] promises_1.3.2      car_3.1-3           OpenMx_2.21.13     \n#&gt; [112] jomo_2.7-6          xtable_1.8-4        cluster_2.1.8.1    \n#&gt; [115] htmlTable_2.4.3     evaluate_1.0.3      pbivnorm_0.6.0     \n#&gt; [118] mvtnorm_1.3-3       cli_3.6.4           kutils_1.73        \n#&gt; [121] compiler_4.4.2      rlang_1.1.5         smacof_2.1-7       \n#&gt; [124] ggsignif_0.6.4      labeling_0.4.3      fdrtool_1.2.18     \n#&gt; [127] plyr_1.8.9          stringi_1.8.4       nnls_1.6           \n#&gt; [130] munsell_0.5.1       lisrelToR_0.3       glmnet_4.1-8       \n#&gt; [133] pacman_0.5.1        Matrix_1.7-3        hms_1.1.3          \n#&gt; [136] glasso_1.11         shiny_1.10.0        haven_2.5.4        \n#&gt; [139] rbibutils_2.3       igraph_2.1.4        broom_1.0.7        \n#&gt; [142] RcppParallel_5.1.10 polynom_1.4-1\n\n\n\n\n\nSaqr, M., & López-Pernas, S. (2024). Learning analytics methods and tutorials: A practical guide using R. Springer Nature.",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/pca/01_linear_algebra.html",
    "href": "chapters/pca/01_linear_algebra.html",
    "title": "\n25  Elementi di algebra lineare\n",
    "section": "",
    "text": "25.1 Introduzione\nQuesto capitolo presenta alcune nozioni di base dell’algebra lineare, una branca della matematica essenziale per la comprensione e l’analisi dei modelli di regressione lineare.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/pca/01_linear_algebra.html#rappresentazione-dei-vettori",
    "href": "chapters/pca/01_linear_algebra.html#rappresentazione-dei-vettori",
    "title": "\n25  Elementi di algebra lineare\n",
    "section": "\n25.2 Rappresentazione dei Vettori",
    "text": "25.2 Rappresentazione dei Vettori\nNell’algebra lineare, un vettore, che rappresenta una lista ordinata di scalari, è solitamente indicato con una lettera minuscola in grassetto, come \\(\\mathbf{v}\\). Gli elementi di un vettore sono generalmente indicati con un indice, ad esempio \\(\\mathbf{v}_1\\) si riferisce al primo elemento del vettore \\(\\mathbf{v}\\).\nUn vettore \\(\\mathbf{v}\\) di \\(n\\) elementi può essere rappresentato sia come una colonna che come una riga, a seconda della convenzione scelta. Ad esempio, un vettore colonna di \\(n\\) elementi è scritto come:\n\\[\n\\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix},\n\\]\nmentre un vettore riga appare come:\n\\[\n\\mathbf{v} = \\begin{bmatrix} v_1 & v_2 & \\cdots & v_n \\end{bmatrix}.\n\\]\nQuesta notazione consente di visualizzare chiaramente i singoli elementi del vettore e di riferirsi a ciascuno di essi in modo specifico.\nUna lista di \\(n\\) scalari organizzata in un vettore \\(\\mathbf{v}\\) è chiamata “dimensione” del vettore. Formalmente, si esprime come \\(\\mathbf{v} \\in \\mathbb{R}^n\\), indicando che il vettore \\(\\mathbf{v}\\) appartiene all’insieme di tutti i vettori reali di dimensione \\(n\\).",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/pca/01_linear_algebra.html#visualizzazione-geometrica-dei-vettori",
    "href": "chapters/pca/01_linear_algebra.html#visualizzazione-geometrica-dei-vettori",
    "title": "\n25  Elementi di algebra lineare\n",
    "section": "\n25.3 Visualizzazione Geometrica dei Vettori",
    "text": "25.3 Visualizzazione Geometrica dei Vettori\nI vettori possono essere rappresentati come frecce in uno spazio \\(n\\)-dimensionale, con l’origine come punto di partenza e la punta della freccia che corrisponde alle coordinate specificate dal vettore. La norma \\(L_2\\) (o lunghezza) di un vettore, denotata come \\(\\|\\mathbf{v}\\|\\), rappresenta la distanza euclidea dall’origine alla punta del vettore.\nPer un vettore \\(\\mathbf{v} = [v_1, v_2, \\ldots, v_n]\\), la norma è definita come:\n\\[\n\\|\\mathbf{v}\\| = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2}.\n\\]\n\n25.3.1 Esempio Numerico\nConsideriamo un vettore in uno spazio bidimensionale, ad esempio \\(\\mathbf{v} = [3, 4]\\). Geometricamente, questo vettore parte dall’origine \\((0, 0)\\) e termina nel punto \\((3, 4)\\) del piano cartesiano.\nPer calcolare la norma \\(L_2\\) di questo vettore, applichiamo la formula:\n\\[\n\\|\\mathbf{v}\\| = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5.\n\\]\nQuindi, la norma del vettore \\(\\mathbf{v} = [3, 4]\\) è 5, che rappresenta la lunghezza della freccia dal punto di origine \\((0, 0)\\) al punto \\((3, 4)\\) nello spazio bidimensionale.\n\n25.3.2 Rappresentazione Geometrica\ny\n^\n|       * (3, 4)\n|      /\n|     /\n|    /\n|   /\n|  /\n| / \n|/____________&gt; x\n(0, 0)\nIn questo diagramma, il punto * rappresenta la fine del vettore \\(\\mathbf{v}\\) e la linea inclinata mostra il vettore stesso che parte dall’origine. L’altezza della linea fino al punto (3, 4) rappresenta visivamente la norma del vettore, che è la distanza di 5 unità dall’origine.\nQuesto esempio illustra chiaramente la relazione tra la rappresentazione numerica di un vettore e la sua interpretazione geometrica, facilitando la comprensione della lunghezza del vettore e della sua direzione nello spazio bidimensionale.\nSebbene noi siamo principalmente limitati a ragionare su spazi bidimensionali (2D) e tridimensionali (3D), i dati che raccogliamo spesso risiedono in spazi di dimensioni superiori. L’algebra lineare permette di ragionare e sviluppare intuizioni su vettori e spazi di dimensioni molto più elevate, superando i limiti della visualizzazione diretta.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/pca/01_linear_algebra.html#operazioni-di-base-sui-vettori",
    "href": "chapters/pca/01_linear_algebra.html#operazioni-di-base-sui-vettori",
    "title": "\n25  Elementi di algebra lineare\n",
    "section": "\n25.4 Operazioni di Base sui Vettori",
    "text": "25.4 Operazioni di Base sui Vettori\n\n25.4.1 1. Moltiplicazione di un Vettore per uno Scalare\nLa moltiplicazione di un vettore per uno scalare produce un nuovo vettore. Questa operazione può essere interpretata come una “scalatura” del vettore nello spazio: il vettore risultante mantiene la stessa direzione dell’originale, ma la sua lunghezza viene modificata in base allo scalare.\nSe \\(\\mathbf{v} = [v_1, v_2, \\ldots, v_n]\\) è un vettore e \\(c\\) è uno scalare, la moltiplicazione del vettore per lo scalare è data da:\n\\[\nc\\mathbf{v} = [cv_1, cv_2, \\ldots, cv_n]\n\\]\n\n25.4.2 2. Addizione di Vettori\nÈ possibile sommare due vettori della stessa dimensione. La somma vettoriale si ottiene sommando gli elementi corrispondenti di ciascun vettore.\nSe \\(\\mathbf{u} = [u_1, u_2, \\ldots, u_n]\\) e \\(\\mathbf{v} = [v_1, v_2, \\ldots, v_n]\\) sono due vettori di dimensione \\(n\\), la loro somma è:\n\\[\n\\mathbf{u} + \\mathbf{v} = [u_1 + v_1, u_2 + v_2, \\ldots, u_n + v_n]\n\\]\n\n25.4.3 3. Prodotto Scalare (o Prodotto Interno)\nIl prodotto scalare tra due vettori della stessa dimensione è uno scalare che fornisce informazioni sull’angolo tra i vettori nello spazio. Formalmente, il prodotto scalare di \\(\\mathbf{u} = [u_1, u_2, \\ldots, u_n]\\) e \\(\\mathbf{v} = [v_1, v_2, \\ldots, v_n]\\) è definito come:\n\\[\n\\mathbf{u} \\cdot \\mathbf{v} = u_1v_1 + u_2v_2 + \\cdots + u_nv_n\n\\]\nQuesto prodotto scalare può anche essere espresso in termini dell’angolo \\(\\theta\\) tra i vettori:\n\\[\n\\mathbf{u} \\cdot \\mathbf{v} = \\|\\mathbf{u}\\| \\|\\mathbf{v}\\| \\cos(\\theta)\n\\]\nSe due vettori sono ortogonali, ovvero formano un angolo di \\(90^\\circ\\) tra loro, il loro prodotto scalare è zero: \\(\\mathbf{u} \\cdot \\mathbf{v} = 0\\).\n\n25.4.4 4. Prodotto Scalare di un Vettore con Se Stesso\nIl prodotto scalare di un vettore con se stesso fornisce il quadrato della sua lunghezza. Se \\(\\mathbf{v} = [v_1, v_2, \\ldots, v_n]\\), allora:\n\\[\n\\mathbf{v} \\cdot \\mathbf{v} = v_1^2 + v_2^2 + \\cdots + v_n^2 = \\|\\mathbf{v}\\|^2\n\\]\nQueste operazioni di base sui vettori sono fondamentali per molte applicazioni in matematica, fisica, informatica e altre scienze, fornendo una struttura potente per analizzare e risolvere problemi in spazi multidimensionali.\n\n25.4.5 Vettori in R\nIn R, possiamo creare un vettore con tre elementi usando la funzione c():\n\n# Creazione di un vettore\nv &lt;- c(1, 2, 3)\nv\n#&gt; [1] 1 2 3\n\nIn questo esempio, v è un vettore con tre elementi: 1, 2 e 3.\nPer eseguire il prodotto tra un vettore e uno scalare, possiamo semplicemente moltiplicare il vettore per lo scalare. Questo moltiplica ogni elemento del vettore per lo scalare:\n\n# Scalari e vettori\na &lt;- 5\n\n# Prodotto vettore-scalare\nva &lt;- v * a\nva\n#&gt; [1]  5 10 15\n\nIl risultato sarà [5, 10, 15].\nIl prodotto interno (o prodotto scalare) tra due vettori si può calcolare con la funzione sum() per ottenere la somma dei prodotti degli elementi corrispondenti:\n\n# Un altro vettore\nv2 &lt;- c(4, 5, 6)\n\n# Prodotto interno\nprodotto_interno &lt;- sum(v * v2)\nprodotto_interno\n#&gt; [1] 32\n\nIl risultato sarà 32, dato che il prodotto interno è calcolato come \\(1*4 + 2*5 + 3*6 = 32\\).\nIn alternativa, si può utilizzare la funzione crossprod() che calcola il prodotto interno in modo efficiente:\n\n# Prodotto interno con crossprod\nprodotto_interno2 &lt;- crossprod(v, v2)\nprodotto_interno2\n#&gt;      [,1]\n#&gt; [1,]   32\n\nLa funzione crossprod() restituisce una matrice \\(1 \\times 1\\), quindi il risultato sarà simile.\nIl prodotto esterno tra due vettori produce una matrice dove ogni elemento è il prodotto degli elementi corrispondenti dei due vettori. In R, possiamo usare la funzione outer():\n\n# Prodotto esterno\nprodotto_esterno &lt;- outer(v, v2)\nprodotto_esterno\n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]    4    5    6\n#&gt; [2,]    8   10   12\n#&gt; [3,]   12   15   18\n\nIl risultato sarà una matrice in cui ogni elemento è il prodotto dei corrispondenti elementi dei vettori v e v2.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/pca/01_linear_algebra.html#matrici",
    "href": "chapters/pca/01_linear_algebra.html#matrici",
    "title": "\n25  Elementi di algebra lineare\n",
    "section": "\n25.5 Matrici",
    "text": "25.5 Matrici\nUna matrice è una struttura matematica bidimensionale costituita da elementi disposti in righe e colonne. Formalmente, una matrice \\(\\mathbf{A}\\) di dimensioni \\(m \\times n\\) (si legge “m per n”) è un array rettangolare di numeri reali o complessi, denotato come:\n\\[ \\mathbf{A} = (a_{ij})_{m \\times n} = \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix} \\]\ndove \\(a_{ij}\\) rappresenta l’elemento nella \\(i\\)-esima riga e \\(j\\)-esima colonna della matrice.\nLe matrici sono comunemente indicate con lettere maiuscole in grassetto, come \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), \\(\\mathbf{C}\\), etc. Una matrice con \\(m\\) righe e \\(n\\) colonne si dice di ordine \\(m \\times n\\).\nIn molte matrici di dati, ogni elemento \\(a_{ij}\\) è uno scalare che rappresenta il valore della \\(j\\)-esima variabile del \\(i\\)-esimo campione. Formalmente, possiamo indicare \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\), il che significa che la matrice \\(\\mathbf{A}\\) ha \\(m\\) righe e \\(n\\) colonne. Si dice comunemente che la “dimensione” di \\(\\mathbf{A}\\) è \\(m \\times n\\).\n\n25.5.1 Matrici come Collezioni di Vettori Colonna\nLe matrici possono essere interpretate come collezioni di vettori colonna. Ad esempio, una matrice di dati può essere rappresentata come:\n\\[\n\\mathbf{A} = \\begin{bmatrix} \\mathbf{a}_1 & \\mathbf{a}_2 & \\cdots & \\mathbf{a}_n \\end{bmatrix}\n\\]\nIn questo caso, \\(\\mathbf{A}\\) è composta da una sequenza di \\(n\\) vettori colonna \\(\\mathbf{a}_1, \\mathbf{a}_2, \\ldots, \\mathbf{a}_n\\), ciascuno dei quali è un vettore di dimensione \\(m\\). Più precisamente, ogni vettore colonna \\(\\mathbf{a}_j\\) rappresenta i dati di tutti i campioni per la \\(j\\)-esima variabile o feature.\n\n25.5.2 Matrici come Collezioni di Vettori Riga\nIn alternativa, una matrice può essere vista come una collezione di vettori riga. In questo contesto, ogni riga di \\(\\mathbf{A}\\) rappresenta tutte le variabili misurate per un dato campione:\n\\[\n\\mathbf{A} = \\begin{bmatrix}\n\\mathbf{a}_1^T \\\\\n\\mathbf{a}_2^T \\\\\n\\vdots \\\\\n\\mathbf{a}_m^T\n\\end{bmatrix}\n\\]\nQui, la matrice \\(\\mathbf{A}\\) è composta da \\(m\\) vettori riga, denotati come \\(\\mathbf{a}_i^T\\). Ognuno di questi vettori riga \\(\\mathbf{a}_i^T\\) è di dimensione \\(n\\), indicando che ciascun campione ha \\(n\\) variabili o feature associate.\n\n25.5.3 Trasposta di una Matrice\nIl simbolo \\(T\\) rappresenta la trasposta di una matrice. La trasposta di una matrice, denotata con un apice \\(T\\) (es. \\(\\mathbf{A}^T\\)), è un’operazione che trasforma ciascuna delle righe di \\(\\mathbf{A}\\) in colonne di \\(\\mathbf{A}^T\\). In altre parole, se \\(\\mathbf{A}\\) ha dimensione \\(m \\times n\\), allora \\(\\mathbf{A}^T\\) avrà dimensione \\(n \\times m\\):\n\\[\n\\mathbf{A}^T = \\begin{bmatrix}\na_{11} & a_{21} & \\cdots & a_{m1} \\\\\na_{12} & a_{22} & \\cdots & a_{m2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{1n} & a_{2n} & \\cdots & a_{mn}\n\\end{bmatrix}\n\\]\nCon la trasposta, le variabili misurate diventano colonne e i campioni diventano righe. Essenzialmente, i vettori riga sono le trasposte dei vettori colonna. Questo concetto è molto utile in algebra lineare, poiché permette di passare facilmente da una rappresentazione dei dati a un’altra.\n\n25.5.4 Matrici in R\nIn R, una matrice può essere creata utilizzando la funzione matrix(). Per esempio, possiamo creare una matrice 3x4 fornendo un vettore di elementi e specificando il numero di righe e colonne.\nEcco come definire una matrice 3x4:\n\n# Definizione della matrice 3x4\nM &lt;- matrix(c(\n    1, 2, 3, 4, \n    5, 6, 7, 8, \n    9, 10, 11, 12\n    ), \n    nrow = 3, ncol = 4, byrow = TRUE)\n\nprint(\"Matrice originale:\")\n#&gt; [1] \"Matrice originale:\"\nprint(M)\n#&gt;      [,1] [,2] [,3] [,4]\n#&gt; [1,]    1    2    3    4\n#&gt; [2,]    5    6    7    8\n#&gt; [3,]    9   10   11   12\n\nQui, byrow = TRUE indica che i dati vengono inseriti riga per riga. Se si utilizza byrow = FALSE, i dati vengono inseriti colonna per colonna.\nIn R, puoi calcolare la trasposta di una matrice utilizzando la funzione t():\n\n# Calcolo della trasposta\ntrasposta &lt;- t(M)\ntrasposta\n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]    1    5    9\n#&gt; [2,]    2    6   10\n#&gt; [3,]    3    7   11\n#&gt; [4,]    4    8   12",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/pca/01_linear_algebra.html#moltiplicazione-tra-matrici",
    "href": "chapters/pca/01_linear_algebra.html#moltiplicazione-tra-matrici",
    "title": "\n25  Elementi di algebra lineare\n",
    "section": "\n25.6 Moltiplicazione tra Matrici",
    "text": "25.6 Moltiplicazione tra Matrici\nLa moltiplicazione tra matrici è un’operazione fondamentale nell’algebra lineare. Per poter moltiplicare due matrici, è necessario che siano conformabili, il che significa che il numero di colonne della prima matrice deve essere uguale al numero di righe della seconda matrice.\nSe abbiamo una matrice \\(\\mathbf{A}\\) di dimensioni \\(m \\times n\\) (cioè, \\(m\\) righe e \\(n\\) colonne) e una matrice \\(\\mathbf{B}\\) di dimensioni \\(n \\times p\\) (cioè, \\(n\\) righe e \\(p\\) colonne), allora il prodotto delle due matrici \\(\\mathbf{A} \\mathbf{B}\\) sarà una matrice \\(\\mathbf{C}\\) di dimensioni \\(m \\times p\\).\nIl prodotto tra due matrici \\(\\mathbf{A}\\) e \\(\\mathbf{B}\\) si ottiene calcolando il prodotto interno tra le righe della prima matrice e le colonne della seconda matrice.\nPer ciascun elemento \\(c_{ij}\\) della matrice risultante \\(\\mathbf{C}\\), si esegue il seguente calcolo:\n\\[\nc_{ij} = \\sum_{k=1}^{n} a_{ik} b_{kj}.\n\\]\nQuesto significa che l’elemento \\(c_{ij}\\) è il risultato del prodotto interno tra la \\(i\\)-esima riga della matrice \\(\\mathbf{A}\\) e la \\(j\\)-esima colonna della matrice \\(\\mathbf{B}\\).\nLa moltiplicazione di una matrice per un vettore è un caso particolare della moltiplicazione tra matrici, dove il vettore può essere visto come una matrice con una delle dimensioni uguale a 1.\nSe \\(\\mathbf{A}\\) è una matrice \\(m \\times n\\) e \\(\\mathbf{x}\\) è un vettore di dimensione \\(n\\) (cioè una matrice di dimensione \\(n \\times 1\\)), allora il prodotto \\(\\mathbf{A} \\mathbf{x}\\) è un vettore di dimensione \\(m\\). Ogni elemento del vettore risultante è il prodotto interno tra una riga della matrice \\(\\mathbf{A}\\) e il vettore \\(\\mathbf{x}\\).\nConsideriamo le seguenti matrici:\n\\[\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix}, \\quad \\mathbf{B} = \\begin{bmatrix}\n7 & 8 \\\\\n9 & 10 \\\\\n11 & 12\n\\end{bmatrix}\n\\]\n\n\n\\(\\mathbf{A}\\) è una matrice \\(2 \\times 3\\).\n\n\\(\\mathbf{B}\\) è una matrice \\(3 \\times 2\\).\n\nIl prodotto \\(\\mathbf{A} \\mathbf{B}\\) è una matrice \\(2 \\times 2\\) calcolata come segue:\n\\[\n\\mathbf{C} = \\mathbf{A} \\mathbf{B} = \\begin{bmatrix}\n(1 \\cdot 7 + 2 \\cdot 9 + 3 \\cdot 11) & (1 \\cdot 8 + 2 \\cdot 10 + 3 \\cdot 12) \\\\\n(4 \\cdot 7 + 5 \\cdot 9 + 6 \\cdot 11) & (4 \\cdot 8 + 5 \\cdot 10 + 6 \\cdot 12)\n\\end{bmatrix}\n\\]\nCalcolando ogni elemento:\n\\[\n\\mathbf{C} = \\begin{bmatrix}\n58 & 64 \\\\\n139 & 154\n\\end{bmatrix}\n\\]\nIn questo esempio, ogni elemento della matrice risultante \\(\\mathbf{C}\\) è stato ottenuto calcolando il prodotto interno tra le righe di \\(\\mathbf{A}\\) e le colonne di \\(\\mathbf{B}\\).\n\n25.6.1 Calcoli con Matrici in R\nIn R, il prodotto tra matrici può essere calcolato utilizzando l’operatore %*%.\n\n# Definizione della matrice A (2x3)\nA &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2, ncol = 3, byrow = TRUE)\n\n# Definizione della matrice B (3x2)\nB &lt;- matrix(c(7, 8, 9, 10, 11, 12), nrow = 3, ncol = 2, byrow = TRUE)\n\n# Calcolo del prodotto A * B\nprodotto_AB &lt;- A %*% B\nprint(\"Prodotto A * B usando l'operatore %*%:\")\n#&gt; [1] \"Prodotto A * B usando l'operatore %*%:\"\nprodotto_AB\n#&gt;      [,1] [,2]\n#&gt; [1,]   58   64\n#&gt; [2,]  139  154\n\nIn R, %*% è l’operatore per il prodotto matriciale.\n\n25.6.2 Matrice Identità e Matrice Inversa\n\n25.6.3 Matrice Identità\nLa matrice identità, denotata come \\(\\mathbf{I}_n\\), è una matrice quadrata di dimensione \\(n \\times n\\) con tutti gli elementi sulla diagonale principale uguali a 1 e tutti gli altri elementi uguali a 0. Ad esempio, una matrice identità 3x3 è:\n\\[\n\\mathbf{I}_3 = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\]\nIn generale, una matrice identità di dimensione \\(n \\times n\\) è:\n\\[\n\\mathbf{I}_n = \\begin{bmatrix}\n1 & 0 & \\cdots & 0 \\\\\n0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 1\n\\end{bmatrix}\n\\]\nLa matrice identità ha la proprietà fondamentale di essere l’elemento neutro per la moltiplicazione matriciale. Per qualsiasi matrice \\(\\mathbf{A}\\) di dimensioni \\(n \\times n\\):\n\\[\n\\mathbf{A} \\mathbf{I}_n = \\mathbf{A} \\quad \\text{e} \\quad \\mathbf{I}_n \\mathbf{A} = \\mathbf{A}.\n\\]\nIn R, puoi creare una matrice identità utilizzando la funzione diag():\n\n# Creazione della matrice identità 3x3\nI &lt;- diag(3)\n\nprint(\"Matrice identità 3x3:\")\n#&gt; [1] \"Matrice identità 3x3:\"\nprint(I)\n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]    1    0    0\n#&gt; [2,]    0    1    0\n#&gt; [3,]    0    0    1\n\nLa funzione diag(3) crea una matrice identità 3x3 con 1 lungo la diagonale principale e 0 altrove.\n\n25.6.4 Determinante di una Matrice\nIl determinante è un numero associato a una matrice quadrata che fornisce informazioni essenziali sulle proprietà della matrice stessa. È uno scalare che può indicare se una matrice è invertibile, se un sistema di equazioni lineari ha una soluzione unica, e molto altro.\nIl determinante di una matrice può essere interpretato in diversi modi:\n\nIn termini geometrici, il determinante di una matrice \\(2 \\times 2\\) o \\(3 \\times 3\\) rappresenta rispettivamente l’area o il volume del parallelogramma o del parallelepipedo definito dai vettori delle righe (o colonne) della matrice. Un determinante pari a zero indica che i vettori sono linearmente dipendenti e che l’area o il volume è nullo, suggerendo che la matrice non ha un’inversa.\nAlgebraicamente, il determinante di una matrice quadrata può dirci se la matrice è invertibile. Se il determinante è diverso da zero, la matrice è invertibile, cioè esiste una matrice inversa tale che il prodotto delle due sia la matrice identità. Se il determinante è zero, la matrice non è invertibile.\nNel contesto dei sistemi di equazioni lineari, se il determinante del coefficiente della matrice associata a un sistema è zero, il sistema può non avere soluzioni o avere un numero infinito di soluzioni. Se è diverso da zero, il sistema ha una soluzione unica.\n\n\n25.6.4.1 Calcolo del Determinante per una Matrice 2x2\nPer una matrice \\(2 \\times 2\\):\n\\[\n\\mathbf{A} = \\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\n\\]\nil determinante è calcolato come:\n\\[\n\\det(\\mathbf{A}) = ad - bc\n\\]\nQuesto semplice calcolo deriva dalla differenza tra il prodotto degli elementi della diagonale principale (dall’angolo superiore sinistro all’angolo inferiore destro) e il prodotto degli elementi della diagonale secondaria (dall’angolo superiore destro all’angolo inferiore sinistro).\n\n25.6.4.2 Calcolo del Determinante per Matrici di Dimensioni Superiori\nPer matrici di dimensioni superiori a \\(2 \\times 2\\), il calcolo del determinante diventa più complesso. Un metodo comune per calcolare il determinante di matrici più grandi è l’espansione di Laplace o espansione per cofattori. Questo metodo si basa sulla ricorsione, calcolando il determinante attraverso una somma pesata di determinanti di matrici più piccole (minori) che si ottengono eliminando una riga e una colonna dalla matrice originale.\n\n25.6.5 Calcolo del Determinante in R\nPer calcolare il determinante di una matrice quadrata in R, puoi usare la funzione det(). Questa funzione funziona per matrici quadrate di qualsiasi dimensione.\n\n25.6.5.1 Esempio con una matrice \\(2 \\times 2\\)\n\n\n# Definizione di una matrice 2x2\nA &lt;- matrix(c(1, 2, 3, 4), nrow = 2, ncol = 2, byrow = TRUE)\n\n# Calcolo del determinante\ndeterminante_A &lt;- det(A)\ndeterminante_A\n#&gt; [1] -2\n\n\n25.6.5.2 Esempio con una matrice \\(3 \\times 3\\)\n\n\n# Definizione di una matrice 3x3\nB &lt;- matrix(c(6, 1, 1, 4, -2, 5, 2, 8, 7), nrow = 3, ncol = 3, byrow = TRUE)\n\n# Calcolo del determinante\ndeterminante_B &lt;- det(B)\ndeterminante_B\n#&gt; [1] -306\n\nIn R, come in Python, il determinante è uno strumento fondamentale per comprendere le proprietà di una matrice. Può essere utilizzato per determinare:\n\n\nInvertibilità: Se il determinante è \\(0\\), la matrice non è invertibile.\n\nTrasformazioni geometriche: Il valore del determinante descrive il fattore di scala della trasformazione rappresentata dalla matrice.\n\nSistemi lineari: Il determinante aiuta a identificare la singolarità dei sistemi di equazioni.\n\n25.6.6 Inversa di una Matrice\nL’inversa di una matrice quadrata \\(\\mathbf{A}\\), denotata come \\(\\mathbf{A}^{-1}\\), è una matrice che, moltiplicata per \\(\\mathbf{A}\\), restituisce la matrice identità \\(\\mathbf{I}_n\\). L’inversa di una matrice esiste solo per matrici quadrate non singolari, ovvero matrici il cui determinante è diverso da zero.\nLa proprietà fondamentale dell’inversa è:\n\\[\n\\mathbf{A} \\mathbf{A}^{-1} = \\mathbf{I}_n \\quad \\text{e} \\quad \\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_n.\n\\]\ndove \\(\\mathbf{I}_n\\) è la matrice identità di dimensione \\(n \\times n\\).\n\n25.6.6.1 Esempio: Calcolo dell’Inversa di una Matrice \\(2 \\times 2\\)\n\nPer una matrice \\(2 \\times 2\\):\n\\[\n\\mathbf{A} = \\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\n\\]\nl’inversa, se esiste, è data dalla formula:\n\\[\n\\mathbf{A}^{-1} = \\frac{1}{ad-bc} \\begin{bmatrix}\nd & -b \\\\\n-c & a\n\\end{bmatrix}\n\\]\ndove \\(ad-bc\\) è il determinante della matrice \\(\\mathbf{A}\\). L’inversa esiste solo se questo determinante è diverso da zero (cioè, se \\(\\mathbf{A}\\) è non singolare).\n\n25.6.7 Utilizzo dell’Inversa di una Matrice\nL’inversa di una matrice è particolarmente utile per risolvere sistemi di equazioni lineari. Ad esempio, consideriamo un sistema rappresentato in forma matriciale come \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\), dove \\(\\mathbf{A}\\) è la matrice dei coefficienti, \\(\\mathbf{x}\\) è il vettore delle variabili incognite e \\(\\mathbf{b}\\) è il vettore dei termini noti.\nSe \\(\\mathbf{A}\\) è una matrice invertibile, possiamo risolvere per \\(\\mathbf{x}\\) moltiplicando entrambi i lati dell’equazione per \\(\\mathbf{A}^{-1}\\):\n\\[\n\\mathbf{A}^{-1} \\mathbf{A} \\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}.\n\\]\nPoiché \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}\\), otteniamo:\n\\[\n\\mathbf{I} \\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b},\n\\]\n\\[\n\\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}.\n\\]\nQuesta proprietà è utile anche per altre applicazioni, come nella derivazione della formula per i coefficienti della regressione lineare.\nEcco come calcolare l’inversa di una matrice in R utilizzando la funzione solve():\n\n25.6.8 Calcolo dell’Inversa in R\nIn R, possiamo calcolare l’inversa di una matrice quadrata (se invertibile) utilizzando la funzione solve(). È importante verificare che la matrice abbia un determinante diverso da zero, altrimenti non è invertibile.\n\n# Definizione di una matrice 2x2\nA &lt;- matrix(c(1, 2, 3, 4), nrow = 2, ncol = 2, byrow = TRUE)\n\n# Calcolo dell'inversa\nA_inv &lt;- solve(A)\n\ncat(\"Inversa di A:\\n\")\n#&gt; Inversa di A:\nprint(A_inv)\n#&gt;      [,1] [,2]\n#&gt; [1,] -2.0  1.0\n#&gt; [2,]  1.5 -0.5\n\nPer verificare che l’inversa sia stata calcolata correttamente, possiamo moltiplicare la matrice originale \\(\\mathbf{A}\\) per la sua inversa \\(\\mathbf{A}^{-1}\\) e verificare che il risultato sia la matrice identità:\n\n# Prodotto di A e A_inv\nidentita &lt;- A %*% A_inv\n\ncat(\"Prodotto di A e A_inv (matrice identità):\\n\")\n#&gt; Prodotto di A e A_inv (matrice identità):\nprint(identita)\n#&gt;      [,1]     [,2]\n#&gt; [1,]    1 1.11e-16\n#&gt; [2,]    0 1.00e+00\n\nIn conclusione, l’inversa di una matrice è uno strumento potente e utile per diverse applicazioni, come:\n\nRisoluzione di sistemi di equazioni lineari\nTrasformazioni geometriche\nAnalisi di modelli lineari\n\nIn R, solve() rende semplice e veloce il calcolo dell’inversa, a patto che la matrice sia:\n\n\nQuadrata: Deve avere lo stesso numero di righe e colonne.\n\nInvertibile: Il determinante della matrice deve essere diverso da zero, altrimenti solve() restituirà un errore.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/pca/01_linear_algebra.html#regressione-lineare-e-stima-dei-coefficienti",
    "href": "chapters/pca/01_linear_algebra.html#regressione-lineare-e-stima-dei-coefficienti",
    "title": "\n25  Elementi di algebra lineare\n",
    "section": "\n25.7 Regressione Lineare e Stima dei Coefficienti",
    "text": "25.7 Regressione Lineare e Stima dei Coefficienti\nLa regressione lineare è una tecnica statistica utilizzata per modellare la relazione tra una variabile dipendente (o risposta) e una o più variabili indipendenti (o predittori). È possibile rappresentare questo modello in termini di algebra matriciale per semplificare il calcolo dei coefficienti.\n\n25.7.1 Regressione Lineare Semplice\nLa regressione lineare semplice descrive una relazione lineare tra una variabile indipendente \\(x\\) e una variabile dipendente \\(y\\). Quando abbiamo un campione di \\(n\\) osservazioni, il modello assume la seguente forma:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + e_i, \\quad \\text{per} \\; i = 1, 2, \\ldots, n,\n\\]\ndove:\n\n\n\\(y_i\\) è il valore osservato della variabile dipendente per l’osservazione \\(i\\),\n\n\\(\\beta_0\\) è l’intercetta, che rappresenta il valore di \\(y\\) quando \\(x = 0\\),\n\n\\(\\beta_1\\) è il coefficiente di regressione, che indica quanto varia \\(y\\) per una variazione unitaria di \\(x\\),\n\n\\(x_i\\) è il valore della variabile indipendente per l’osservazione \\(i\\),\n\n\\(e_i\\) è l’errore o residuo per l’osservazione \\(i\\), rappresenta la differenza tra il valore osservato \\(y_i\\) e il valore previsto \\(\\hat{y}_i = \\beta_0 + \\beta_1 x_i\\).\n\nPer un campione di \\(n\\) osservazioni, possiamo rappresentare la regressione lineare in forma matriciale, che rende il modello più compatto e facilita i calcoli statistici. La rappresentazione matriciale del modello di regressione lineare è:\n\\[\n\\mathbf{y} = \\mathbf{Xb} + \\mathbf{e},\n\\]\ndove:\n\n\n\\(\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\\) è il vettore delle osservazioni della variabile dipendente,\n\n\\(\\mathbf{X} = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix}\\) è la matrice di design, in cui la prima colonna è costituita da 1 per includere l’intercetta \\(\\beta_0\\),\n\n\\(\\mathbf{b} = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}\\) è il vettore dei coefficienti del modello,\n\n\\(\\mathbf{e} = \\begin{bmatrix} e_1 \\\\ e_2 \\\\ \\vdots \\\\ e_n \\end{bmatrix}\\) è il vettore degli errori o residui.\n\nQuesta forma matriciale sintetizza tutte le \\(n\\) equazioni del modello di regressione lineare semplice in un’unica espressione compatta, che rappresenta la relazione tra le osservazioni della variabile dipendente \\(y\\) e le corrispondenti osservazioni della variabile indipendente \\(x\\), tenendo conto degli errori di previsione.\n\n25.7.2 Regressione Lineare Multipla\nLa regressione lineare multipla estende la regressione lineare semplice includendo più variabili indipendenti, consentendo di modellare la relazione tra una variabile dipendente e diverse variabili indipendenti. Il modello di regressione lineare multipla per un campione di \\(n\\) osservazioni con \\(p\\) variabili indipendenti può essere scritto come:\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip} + e_i, \\quad \\text{per} \\; i = 1, 2, \\ldots, n,\n\\]\ndove:\n\n\n\\(y_i\\) è il valore osservato della variabile dipendente per l’osservazione \\(i\\),\n\n\\(\\beta_0\\) è l’intercetta del modello,\n\n\\(\\beta_1, \\beta_2, \\ldots, \\beta_p\\) sono i coefficienti di regressione associati alle variabili indipendenti,\n\n\\(x_{i1}, x_{i2}, \\ldots, x_{ip}\\) sono i valori delle variabili indipendenti per l’osservazione \\(i\\),\n\n\\(e_i\\) è l’errore o residuo per l’osservazione \\(i\\), che rappresenta la differenza tra il valore osservato \\(y_i\\) e il valore previsto \\(\\hat{y}_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip}\\).\n\nIn termini matriciali, il modello di regressione lineare multipla può essere scritto come:\n\\[\n\\mathbf{y} = \\mathbf{Xb} + \\mathbf{e},\n\\]\ndove:\n\n\n\\(\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\\) è il vettore delle osservazioni della variabile dipendente, di dimensione \\(n \\times 1\\),\n\n\\(\\mathbf{X} = \\begin{bmatrix} 1 & x_{11} & x_{12} & \\cdots & x_{1p} \\\\ 1 & x_{21} & x_{22} & \\cdots & x_{2p} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n1} & x_{n2} & \\cdots & x_{np} \\end{bmatrix}\\) è la matrice di design, di dimensione \\(n \\times (p+1)\\), dove la prima colonna è composta da 1 per includere l’intercetta \\(\\beta_0\\),\n\n\\(\\mathbf{b} = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}\\) è il vettore dei coefficienti, di dimensione \\((p+1) \\times 1\\),\n\n\\(\\mathbf{e} = \\begin{bmatrix} e_1 \\\\ e_2 \\\\ \\vdots \\\\ e_n \\end{bmatrix}\\) è il vettore degli errori o residui, di dimensione \\(n \\times 1\\).\n\nL’equazione in forma matriciale esplicita per il campione di \\(n\\) osservazioni con \\(p\\) variabili indipendenti è:\n\\[\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & x_{11} & x_{12} & \\cdots & x_{1p} \\\\\n1 & x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n1} & x_{n2} & \\cdots & x_{np}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\beta_2 \\\\\n\\vdots \\\\\n\\beta_p\n\\end{bmatrix} +\n\\begin{bmatrix}\ne_1 \\\\\ne_2 \\\\\n\\vdots \\\\\ne_n\n\\end{bmatrix}.\n\\]\nIn questa rappresentazione:\n\nIl prodotto \\(\\mathbf{Xb}\\) rappresenta i valori previsti (o stimati) del modello come combinazione lineare delle colonne della matrice di design \\(\\mathbf{X}\\), ponderata dai coefficienti \\(\\mathbf{b}\\).\nIl vettore \\(\\mathbf{e}\\) rappresenta gli errori o residui, che sono le differenze tra i valori osservati \\(\\mathbf{y}\\) e i valori previsti \\(\\mathbf{Xb}\\).\n\nQuesta forma compatta e ordinata consente un’efficiente analisi statistica e facilita i calcoli necessari per stimare i coefficienti del modello di regressione (Caudek & Luccio, 2001).",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/pca/01_linear_algebra.html#stima-dei-coefficienti-con-il-metodo-dei-minimi-quadrati",
    "href": "chapters/pca/01_linear_algebra.html#stima-dei-coefficienti-con-il-metodo-dei-minimi-quadrati",
    "title": "\n25  Elementi di algebra lineare\n",
    "section": "\n25.8 Stima dei Coefficienti con il Metodo dei Minimi Quadrati",
    "text": "25.8 Stima dei Coefficienti con il Metodo dei Minimi Quadrati\nPer ogni osservazione \\(i\\), l’errore (o residuo) è definito come la differenza tra il valore osservato \\(y_i\\) e il valore predetto \\(\\hat{y}_i\\) dal modello:\n\\[\ne_i = y_i - \\hat{y}_i,\n\\]\ndove:\n\n\n\\(y_i\\) è il valore osservato dell’output per l’osservazione \\(i\\),\n\n\\(\\hat{y}_i\\) è il valore predetto dal modello per l’osservazione \\(i\\).\n\nIn forma matriciale, possiamo rappresentare l’errore per tutte le \\(n\\) osservazioni come segue:\n\\[\n\\mathbf{e} = \\mathbf{y} - \\hat{\\mathbf{y}},\n\\]\ndove:\n\n\n\\(\\mathbf{e} = \\begin{bmatrix} e_1 \\\\ e_2 \\\\ \\vdots \\\\ e_n \\end{bmatrix}\\) è il vettore degli errori o residui,\n\n\\(\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\\) è il vettore delle osservazioni della variabile dipendente,\n\n\\(\\hat{\\mathbf{y}} = \\begin{bmatrix} \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\vdots \\\\ \\hat{y}_n \\end{bmatrix} = \\mathbf{Xb}\\) è il vettore dei valori predetti dal modello.\n\nL’equazione matriciale esplicita per il vettore degli errori \\(\\mathbf{e}\\) è quindi:\n\\[\n\\mathbf{e} = \\mathbf{y} - \\mathbf{Xb}.\n\\]\nQuesta equazione mostra che il vettore degli errori \\(\\mathbf{e}\\) è la differenza tra il vettore delle osservazioni \\(\\mathbf{y}\\) e il vettore dei valori predetti \\(\\hat{\\mathbf{y}} = \\mathbf{Xb}\\). In altre parole, ogni elemento \\(e_i\\) del vettore degli errori rappresenta la differenza tra il valore osservato \\(y_i\\) e il valore predetto \\(\\hat{y}_i\\) per l’osservazione \\(i\\).\nL’obiettivo della regressione lineare è minimizzare la somma degli errori quadrati (\\(SSE\\), Sum of Squared Errors) per tutte le osservazioni. Questa somma è data da:\n\\[\n\\text{SSE} = \\sum_{i=1}^m e_i^2 = \\sum_{i=1}^m (y_i - \\hat{y}_i)^2.\n\\]\nUtilizzando la notazione matriciale, possiamo esprimere la somma degli errori quadrati come:\n\\[\n\\text{SSE} = \\mathbf{e}^T \\mathbf{e} = (\\mathbf{y} - \\mathbf{X} \\mathbf{b})^T (\\mathbf{y} - \\mathbf{X} \\mathbf{b}).\n\\]\nIl problema di ottimizzazione per minimizzare la somma degli errori quadrati si traduce in:\n\\[\n\\min_{\\mathbf{b}} (\\mathbf{y} - \\mathbf{X} \\mathbf{b})^T (\\mathbf{y} - \\mathbf{X} \\mathbf{b}),\n\\]\ndove:\n\n\n\\(\\mathbf{b}\\) è il vettore dei coefficienti da stimare.\n\n\\(\\mathbf{X}\\) è la matrice di design che include tutte le osservazioni delle variabili indipendenti.\n\n\\(\\mathbf{y}\\) è il vettore delle osservazioni della variabile dipendente.\n\nPer trovare i coefficienti ottimali \\(\\mathbf{b}\\), calcoliamo la derivata parziale dell’errore quadratico totale rispetto a \\(\\mathbf{b}\\) e la impostiamo a zero:\n\\[\n\\frac{\\partial}{\\partial \\mathbf{b}} (\\mathbf{y} - \\mathbf{X} \\mathbf{b})^T (\\mathbf{y} - \\mathbf{X} \\mathbf{b}) = -2 \\mathbf{X}^T (\\mathbf{y} - \\mathbf{X} \\mathbf{b}).\n\\]\nImpostando questa derivata uguale a zero, otteniamo:\n\\[\n-2 \\mathbf{X}^T (\\mathbf{y} - \\mathbf{X} \\mathbf{b}) = 0.\n\\]\nSemplificando, possiamo riscrivere l’equazione come:\n\\[\n\\mathbf{X}^T \\mathbf{y} = \\mathbf{X}^T \\mathbf{X} \\mathbf{b}.\n\\]\nAssumendo che la matrice \\(\\mathbf{X}^T \\mathbf{X}\\) sia invertibile, risolviamo per \\(\\mathbf{b}\\):\n\\[\n\\mathbf{b} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}.\n\\]\nPer gli scopi presenti, non è necessario comprendere la derivazione formale in dettaglio. Tuttavia, possiamo fare un parallelo con il metodo dei minimi quadrati per il caso univariato per ottenere un’intuizione geometrica su cosa stiamo facendo.\nNel caso della regressione lineare semplice (univariata), minimizzare la somma degli errori quadrati significa trovare la retta che meglio si adatta ai dati in uno spazio bidimensionale (2D). Dal punto di vista geometrico, questo processo equivale a calcolare la derivata della funzione di errore rispetto ai coefficienti della retta, quindi impostando la derivata a zero per trovare il punto in cui la pendenza della tangente è piatta. In altre parole, cerchiamo il punto in cui la pendenza della funzione di errore è zero, che corrisponde a un minimo della funzione.\nNel caso della regressione lineare multipla, invece di lavorare in uno spazio bidimensionale, stiamo operando in uno spazio multidimensionale. Ogni dimensione aggiuntiva rappresenta una variabile indipendente (regressore) nel nostro modello. Quando prendiamo la derivata dell’errore quadratico totale rispetto ai coefficienti \\(\\mathbf{b}\\) e la impostiamo a zero, stiamo essenzialmente cercando il punto in questo spazio multidimensionale in cui tutte le “pendenze” (derivate parziali) sono zero. Questo punto rappresenta il minimo dell’errore quadratico totale e corrisponde alla migliore stima dei coefficienti del nostro modello di regressione lineare, minimizzando l’errore di previsione su tutti i dati.\nQuindi, mentre nel caso univariato minimizzare l’errore quadratico trova la migliore linea retta che si adatta ai dati in 2D, nel caso multivariato troviamo il miglior piano o iperpiano che si adatta ai dati in uno spazio di dimensioni superiori.\n\n25.8.1 Stima dei Coefficienti OLS\nQuesta formula:\n\\[\n\\mathbf{b} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n\\]\nè conosciuta come stima dei minimi quadrati ordinari (Ordinary Least Squares, OLS) per i coefficienti della regressione lineare multivariata. Essa fornisce i valori dei coefficienti \\(\\mathbf{b}\\) che minimizzano la somma degli errori quadrati e, quindi, rappresenta la migliore approssimazione lineare dei dati osservati.\n\n25.8.2 Simulazione di una Regressione Lineare Semplice in R\n\n25.8.2.1 Definizione dei dati e calcolo dei coefficienti dei minimi quadrati\n\n# Scegli valori per i coefficienti veri\nb &lt;- c(3.4, 12.35)  # Intercetta e pendenza\n\n# Simula n punti dati\nset.seed(123)  # Per riproducibilità\nn &lt;- 30\ndata_mean &lt;- 0\ndata_sd &lt;- 1\ndata &lt;- rnorm(n, mean = data_mean, sd = data_sd)  # Variabile indipendente\n\n# Aggiungi una colonna di 1s per la matrice di design\nx &lt;- cbind(1, data)  # Matrice di design\n\n# Aggiungi rumore gaussiano\nnoise_mean &lt;- 0\nnoise_sd &lt;- 5\ne &lt;- rnorm(n, mean = noise_mean, sd = noise_sd)\n\n\n# Simula i valori di y\ny &lt;- x %*% b + e\n\n\n# Calcola i coefficienti stimati (minimi quadrati)\nb_hat &lt;- solve(t(x) %*% x) %*% t(x) %*% y\n\n\ncat(\"Valori veri di b:\\n\")\n#&gt; Valori veri di b:\nprint(b)\n#&gt; [1]  3.40 12.35\n\n\ncat(\"Stima di b:\\n\")\n#&gt; Stima di b:\nprint(b_hat)\n#&gt;       [,1]\n#&gt;       4.26\n#&gt; data 11.68\n\n\n25.8.2.2 Calcolo dei valori predetti e del coefficiente di determinazione (\\(R^2\\))\n\n# Valori predetti\ny_hat &lt;- x %*% b_hat\n\n\n# Calcola R^2\nSS_res &lt;- sum((y - y_hat)^2)  # Somma dei residui al quadrato\nSS_tot &lt;- sum((y - mean(y))^2)  # Somma totale dei quadrati\nr2 &lt;- 1 - (SS_res / SS_tot)\n\ncat(\"Coefficiente di determinazione (R^2):\", r2, \"\\n\")\n#&gt; Coefficiente di determinazione (R^2): 0.8853\n\n\n25.8.2.3 Rappresentazione Grafica dei Dati e della Regressione\n\n# Costruiamo un data.frame\ndf &lt;- data.frame(x = data, y = y)\n\n# Data frame per le due linee\nlines_df &lt;- data.frame(\n  intercept = c(b[1], b_hat[1]),\n  slope     = c(b[2], b_hat[2]),\n  tipo      = c(\"Valori veri\", \"Stima\")\n)\n\nggplot(df, aes(x = x, y = y)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_abline(data = lines_df,\n              aes(intercept = intercept, slope = slope, \n                  color = tipo, linetype = tipo),\n              linewidth = 1) +\n  scale_color_manual(values = c(\"Valori veri\" = \"black\", \"Stima\" = \"red\")) +\n  scale_linetype_manual(values = c(\"Valori veri\" = \"dashed\", \"Stima\" = \"solid\")) +\n  labs(\n    title = \"Regressione Lineare Semplice\",\n    x = \"x\",\n    y = \"y\",\n    color = \"Linea\",\n    linetype = \"Linea\"\n  ) +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    legend.position = \"bottom\",                 \n    legend.box = \"horizontal\",                 \n    legend.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n25.8.2.4 Analisi con il Pacchetto lm\n\n\n# Modello di regressione con lm()\nlm_model &lt;- lm(y ~ data)\nsummary(lm_model)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y ~ data)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt;  -8.04  -2.53  -1.08   3.47  10.06 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)\n#&gt; (Intercept)    4.260      0.767    5.55  6.1e-06\n#&gt; data          11.680      0.794   14.70  1.1e-14\n#&gt; \n#&gt; Residual standard error: 4.2 on 28 degrees of freedom\n#&gt; Multiple R-squared:  0.885,  Adjusted R-squared:  0.881 \n#&gt; F-statistic:  216 on 1 and 28 DF,  p-value: 1.07e-14\n\n\nLa matrice x è la matrice di design, che include una colonna di 1 per l’intercetta.\nLa funzione solve() calcola i coefficienti dei minimi quadrati usando l’equazione \\((X'X)^{-1}X'Y\\).\nLa somma dei quadrati residui (\\(SS_{res}\\)) e la somma totale dei quadrati (\\(SS_{tot}\\)) sono calcolate manualmente per derivare \\(R^2\\).\nLa funzione lm() offre un modo alternativo e diretto per ottenere il modello di regressione e i relativi output statistici.\n\n25.8.3 Traccia di una matrice\nSi definisce traccia di una matrice quadrata \\(\\boldsymbol{A}\\) \\(n \\times n\\), e si denota con \\(tr(\\boldsymbol{A})\\) la somma degli elementi sulla diagonale principale di \\(\\boldsymbol{A}\\):\n\\[\ntr(\\boldsymbol{A}) = \\sum_{i=1}^{n} a_{ii}.\n\\]\nLa traccia gode delle seguenti proprietà:\n\\[\n\\begin{aligned}\n&tr(\\rho \\boldsymbol{A}) = \\rho tr( \\boldsymbol{A}) \\notag \\\\\n&tr(\\boldsymbol{A} + \\boldsymbol{B}) =  tr( \\boldsymbol{A})+tr( \\boldsymbol{B}) \\notag \\\\\n&tr(\\boldsymbol{A}') =  tr( \\boldsymbol{A}) \\notag \\\\\n&tr(\\boldsymbol{AB}) =  tr( \\boldsymbol{BA}) \\notag\\end{aligned}\n\\]\nPer esempio, sia\n\\[\n\\boldsymbol{A} =  \\left[ \\begin{array}{c c c}\n7 & 1 & 2\\\\\n1 & 8 & 3\\\\\n2 & 3 & 9 \\end{array} \\right]\n\\]\nallora\n\\[\ntr(\\boldsymbol{A}) = 7 + 8 + 9 = 24.\n\\]\n\nA &lt;- matrix(\n  c(7,1, 2, 1, 8, 3, 2, 3, 9),\n  nrow = 3,\n  byrow = TRUE\n)\nA |&gt; print()\n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]    7    1    2\n#&gt; [2,]    1    8    3\n#&gt; [3,]    2    3    9\n\n\nsum(diag(A)) |&gt; print()\n#&gt; [1] 24\n\n\n25.8.4 Dipendenza lineare\nSi consideri la matrice\n\\[\n\\boldsymbol{A}=\n\\left(%\n\\begin{array}{ccc}\n  1 & 1 & 1 \\\\\n  3 & 1 & 5 \\\\\n  2 & 3 & 1 \\\\\n\\end{array}%\n\\right).\n\\]\nSiano \\(\\boldsymbol{c}_1\\), \\(\\boldsymbol{c}_2\\), \\(\\boldsymbol{c}_3\\) le colonne di \\(\\boldsymbol{A}\\). Si noti che\n\\[\n2\\boldsymbol{c}_1 + -\\boldsymbol{c}_2 + - \\boldsymbol{c}_3 =\n\\boldsymbol{0}\n\\]\ndove \\(\\boldsymbol{0}\\) è un vettore (\\(3 \\times 1\\)) di zeri.\nDato che le 3 colonne di \\(\\boldsymbol{A}\\) possono essere combinate linearmente in modo da produrre un vettore \\(\\boldsymbol{0}\\) vi è chiaramente una qualche forma di relazione, o dipendenza, tra le informazioni nelle colonne. Detto in un altro modo, sembra esserci una qualche duplicazione delle informazione nelle colonne. In generale, si dice che \\(k\\) colonne \\(\\boldsymbol{c}_1, \\boldsymbol{c}_2,\n\\dots \\boldsymbol{c}_k\\) di una matrice sono linearmente dipendenti se esiste un insieme di valori scalari \\(\\lambda_1,\n\\dots, \\lambda_k\\) tale per cui\n\\[\n\\lambda_1 \\boldsymbol{c}_1 + \\dots + \\lambda_k \\boldsymbol{c}_k=\\boldsymbol{0}\n\\]\ne almeno uno dei valori \\(\\lambda_i\\) non è uguale a 0.\nLa dipendenza lineare implica che ciascun vettore colonna è una combinazione degli altri. Per esempio\n\\[\n\\boldsymbol{c}_k= -(\\lambda_1 \\boldsymbol{c}_1 + \\dots + \\lambda_{k-1}\n   \\boldsymbol{c}_{k-1})/\\lambda_k.\n\\]\nQuesto implica che tutta “l’informazione” della matrice è contenuta in un sottoinsieme delle colonne – se \\(k-1\\) colonne sono conosciute, l’ultima resta determinata. È in questo senso che abbiamo detto che l’informazione della matrice veniva “duplicata”.\nSe l’unico insieme di valori scalari \\(\\lambda_i\\) che soddisfa l’equazione\n\\[\n\\lambda_1 \\boldsymbol{c}_1 + \\dots + \\lambda_k \\boldsymbol{c}_k=\\boldsymbol{0}\n\\]\nè un vettore di zeri, allora questo significa che non vi è alcuna relazione tra le colonne della matrice. Le colonne si dicono linearmente indipendenti, nel senso che non contengono alcuna “duplicazione” di informazione.\n\n25.8.5 Rango di una matrice\nIl rango della matrice è il massimo numero di vettori colonna linearmente indipendenti che possono essere selezionati dalla matrice. In maniera equivalente, il rango di una matrice può essere definito come il massimo numero di vettori riga linermente indipendenti. Il rango minimo di una matrice è 1, il che significa che vi è una colonna tale per cui le altre colonne sono dei multipli di questa. Per l’esempio precedente, il rango della matrice \\(\\boldsymbol{A}\\) è 2.\nSe la matrice è quadrata, \\(\\boldsymbol{A}_{n \\times n}\\), ed è costituita da vettori tutti indipendenti tra di loro, allora il suo rango è \\(n\\). Se, invece, la matrice è rettangolare, \\(\\boldsymbol{A}_{m \\times n}\\), allora il suo rango può essere al massimo il più piccolo tra i due valori m ed n, cioè:\n\\[\nr(\\boldsymbol{A}_{m \\times n}) \\leq min(m,n).\n\\]",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/pca/01_linear_algebra.html#radici-e-vettori-latenti",
    "href": "chapters/pca/01_linear_algebra.html#radici-e-vettori-latenti",
    "title": "\n25  Elementi di algebra lineare\n",
    "section": "\n25.9 Radici e vettori latenti",
    "text": "25.9 Radici e vettori latenti\nDal determinante di una matrice si possono ricavare le radici latenti o autovalori (denotati da \\(\\lambda_i\\)) e i vettori latenti o autovettori della matrice. Alle nozioni di autovalore e autovettore verrà qui fornita un’interpretazione geometrica.\nSimuliamo di dati di due variabili associate tra loro:\n\nset.seed(123456)\n\nnpoints &lt;- 20\nx &lt;- as.numeric(scale(rnorm(npoints, 0, 1)))\ny &lt;- as.numeric(scale(3 * x + rnorm(npoints, 0, 2)))\nmean(x) |&gt; print()\n#&gt; [1] -2.776e-17\nmean(y) |&gt; print()\n#&gt; [1] -7.772e-17\ncor(x, y) |&gt; print()\n#&gt; [1] 0.8291\n\nDisegnamo il diagramma di dispersione con un ellisse che contiene la nube di punti:\n\nY &lt;- cbind(x, y)\ncar::dataEllipse(\n  Y[, 1], Y[, 2],\n  levels = 0.95,\n  lty = 2,\n  ylim = c(-3, 3),\n  xlim = c(-3, 3)\n)\n\n\n\n\n\n\n\nSe racchiudiamo le osservazioni (\\(v_1, v_2\\)) con un’ellisse, allora la lunghezza dei semiassi maggiori e minori dell’ellisse sarà proporzionale a \\(\\sqrt{\\lambda_1}\\) e \\(\\sqrt{\\lambda_2}\\). L’asse maggiore è la linea passante per il punto (\\(\\bar{v_1}, \\bar{v_2}\\)) nella direzione determinata dal primo autovettore \\(\\boldsymbol{a}_1'\\) con pendenza uguale a \\(a_{12}/a_{11}\\). L’asse minore è la linea passante per il punto (\\(\\bar{v_1}, \\bar{v_2}\\)) nella direzione determinata dal secondo autovettore \\(\\boldsymbol{a}_2\\).\nCalcoliamo ora gli autovettori e gli autovalori:\n\ns &lt;- cov(Y)\nee &lt;- eigen(s)\nee |&gt; print()\n#&gt; eigen() decomposition\n#&gt; $values\n#&gt; [1] 1.8291 0.1709\n#&gt; \n#&gt; $vectors\n#&gt;        [,1]    [,2]\n#&gt; [1,] 0.7071 -0.7071\n#&gt; [2,] 0.7071  0.7071\n\n\n# First eigenvector \nev_1 &lt;- ee$vectors[, 1]\n\n# Slope of the first eigenvector\nev1_m &lt;- ev_1[2] / ev_1[1]\n\n# Second eigenvector \nev_2 &lt;- ee$vectors[, 2]\n\n# Slope of the second eigenvector\nev2_m &lt;- ev_2[2] / ev_2[1]\n\n\n# Scatter plot showing the span of both eigenvectors \ndata.frame(zx=scale(x), zy=scale(y))  |&gt;\nggplot(aes(x = zx, y = zy)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_vline(xintercept = 0, linewidth = .5) +\n  geom_hline(yintercept = 0, linewidth = .5) +\n  geom_abline(slope = ev1_m, color = \"blue\", linewidth = 0.7) +\n  geom_abline(slope = ev2_m, color = \"red\", linewidth = 0.7) \n\n\n\n\n\n\n\nGli autovettori sono ortogonali:\n\n# Multiply both eigenvectors \nprint(ev_1 %*% ev_2)\n#&gt;           [,1]\n#&gt; [1,] 2.237e-17\n\nGeneriamo uno Scree Plot.\n\n# Calculate the estimated variance for each eigenvalue\ne_var &lt;- ee$values / (length(x) - 1)\n\n# Data frame with variance percentages\nvar_per &lt;- tibble(\n  PC  = c(\"PC1\", \"PC2\"),\n  PER = c(e_var) * 100 / sum(e_var) # Calculate the percentage\n    )\n\n# Scree plot \nggplot(var_per, aes(x = PC, y = PER)) +\n  geom_col(width = 0.5, color = \"black\") +\n  xlab(\"Principal component\") +\n  ylab(\"Percentage of variation (%)\") \n\n\n\n\n\n\n\nVerifichiamo che la somma degli autovalori sia uguale alla varianza totale.\n\nvar(x) + var(y)\n#&gt; [1] 2\n\n\nee$values |&gt; sum()\n#&gt; [1] 2\n\nGli autovettori ottenuti utilizzando la funzione eigen() sono normalizzati. Ciò significa che la loro lunghezza è uguale a 1:\n\nt(as.matrix(ee$vectors[, 1])) %*% as.matrix(ee$vectors[, 1]) \n#&gt;      [,1]\n#&gt; [1,]    1\n\nUtilizziamo le informazioni degli autovettori e degli autovalori per disegnare gli assi dell’ellisse:\n\ngli autovettori determinano la direzione degli assi;\nla radice quadrata degli autovalori è proporzionale alla lunghezza degli assi dell’ellisse.\n\n\ncar::dataEllipse(\n  Y[, 1], Y[, 2],\n  levels = 0.95,\n  lty = 2,\n  ylim = c(-3, 3),\n  xlim = c(-3, 3)\n)\nk &lt;- 2.65\narrows(\n  0, 0, \n  k * sqrt(ee$values[1]) * ee$vectors[1],\n  k * sqrt(ee$values[1]) * ee$vectors[2],\n  code = 2, \n  col = \"red\", \n  lwd = 2\n)\narrows(\n  0, 0, \n  k * sqrt(ee$values[2]) * ee$vectors[1],\n  k * sqrt(ee$values[2]) * -ee$vectors[2],\n  code = 2, \n  col = \"red\", \n  lwd = 2\n)\n\n\n\n\n\n\n\nTale analisi si può estendere a qualunque numero di variabili. Per esempio, nel caso di tre variabili, possiamo pensare di disegnare un ellisoide attorno ad una nube di punti nello spazio tridimensionale. Anche in questo caso, gli autovalori e gli associati autovettori corrisponderanno agli assi dell’elissoide.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/pca/01_linear_algebra.html#scomposizione-spettrale-di-una-matrice",
    "href": "chapters/pca/01_linear_algebra.html#scomposizione-spettrale-di-una-matrice",
    "title": "\n25  Elementi di algebra lineare\n",
    "section": "\n25.10 Scomposizione spettrale di una matrice",
    "text": "25.10 Scomposizione spettrale di una matrice\nData una matrice quadrata e simmetrica di dimensione \\(n\\), \\(\\boldsymbol{A}\\), esistono una matrice diagonale \\(\\boldsymbol{\\Lambda}\\) e una matrice ortogonale \\(\\boldsymbol{V}\\) tali che\n\\[\\boldsymbol{A} =\\boldsymbol{V} \\boldsymbol{\\Lambda} \\boldsymbol{V}',\\] dove\n\n\n\\(\\boldsymbol{\\Lambda}\\) è una matrice diagonale i cui elementi sono gli autovalori di \\(\\boldsymbol{A}\\): \\(\\boldsymbol{\\Lambda} = diag(\\lambda_1, \\lambda_2,\n    \\dots, \\lambda_n)\\);\n\n\\(\\boldsymbol{V}\\) è una matrice ortogonale le cui colonne \\((v_1, v_2, \\dots, v_p)\\) sono gli autovettori di \\(\\boldsymbol{A}\\) associati ai rispettivi autovalori.\n\nIn maniera equivalente\n\\[\\boldsymbol{A} \\boldsymbol{V} =  \\boldsymbol{\\Lambda} \\boldsymbol{V}'.\\]\nPremoltiplicando entrambi i membri per \\(\\boldsymbol{V}'\\) si ottiene\n\\[\\boldsymbol{V}'\\boldsymbol{A} \\boldsymbol{V} =\n\\boldsymbol{\\Lambda},\\]\nda cui l’affermazione che la matrice degli autovettori diagonalizza \\(\\boldsymbol{A}\\).\nPer esempio,\n\nsigma &lt;- matrix(\n  data = c(1, 0.5, 0.5, 1.25), \n  nrow = 2, \n  ncol = 2\n)\nsigma |&gt; print()\n#&gt;      [,1] [,2]\n#&gt; [1,]  1.0 0.50\n#&gt; [2,]  0.5 1.25\n\n\nout &lt;- eigen(sigma)\nout |&gt; print()\n#&gt; eigen() decomposition\n#&gt; $values\n#&gt; [1] 1.6404 0.6096\n#&gt; \n#&gt; $vectors\n#&gt;        [,1]    [,2]\n#&gt; [1,] 0.6154 -0.7882\n#&gt; [2,] 0.7882  0.6154\n\n\nLambda &lt;- diag(out$values)\nLambda |&gt; print()\n#&gt;      [,1]   [,2]\n#&gt; [1,] 1.64 0.0000\n#&gt; [2,] 0.00 0.6096\n\n\nU &lt;- out$vectors\nU |&gt; print()\n#&gt;        [,1]    [,2]\n#&gt; [1,] 0.6154 -0.7882\n#&gt; [2,] 0.7882  0.6154\n\n\nU %*% Lambda %*% t(U) |&gt; print()\n#&gt;      [,1] [,2]\n#&gt; [1,]  1.0 0.50\n#&gt; [2,]  0.5 1.25\n\n\n25.10.1 Autovalori e determinante\nIl determinante di una matrice è il prodotto degli autovalori:\n\\[\\begin{aligned}\n    |\\boldsymbol{A}| &= \\prod_{i=1}^{p} \\lambda_i. \\notag\n    \\end{aligned}\\]\nLa traccia di una matrice è uguale alla somma degli autovalori:\n\\[\\begin{aligned}\n    tr(\\boldsymbol{A}) &= \\sum_{i=1}^{p} \\lambda_i. \\notag\n    \\end{aligned}\\]\n\nsigma &lt;- matrix(data = c(1, 0.5, 0.5, 2), nrow = 2, ncol = 2)\nsigma |&gt; print()\n#&gt;      [,1] [,2]\n#&gt; [1,]  1.0  0.5\n#&gt; [2,]  0.5  2.0\n\nout &lt;- eigen(sigma)\nout |&gt; print()\n#&gt; eigen() decomposition\n#&gt; $values\n#&gt; [1] 2.2071 0.7929\n#&gt; \n#&gt; $vectors\n#&gt;        [,1]    [,2]\n#&gt; [1,] 0.3827 -0.9239\n#&gt; [2,] 0.9239  0.3827\n\nLa traccia di una matrice è uguale alla somma degli autovalori:\n\nsum(out$values) |&gt; print()\n#&gt; [1] 3\n\nIl determinante di una matrice è il prodotto degli autovalori:\n\ndet(sigma) |&gt; print()\n#&gt; [1] 1.75\n(out$values[1] * out$values[2]) |&gt; print()\n#&gt; [1] 1.75\n\nGli autovalori di \\(\\boldsymbol{A}^{-1}\\) sono i reciproci degli autovalori di \\(\\boldsymbol{A}\\); gli autovettori sono coincidenti.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/pca/01_linear_algebra.html#la-distanza-euclidea",
    "href": "chapters/pca/01_linear_algebra.html#la-distanza-euclidea",
    "title": "\n25  Elementi di algebra lineare\n",
    "section": "\n25.11 La distanza euclidea",
    "text": "25.11 La distanza euclidea\nPer calcolare la distanza euclidea tra due punti utilizzando l’algebra matriciale, consideriamo i punti come vettori in uno spazio euclideo.\n\n25.11.1 In due dimensioni:\nSiano dati i vettori:\n\n\nx = \\(\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\\),\n\ny = \\(\\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix}\\).\n\nLa distanza euclidea tra x e y è la norma del vettore differenza (\\(\\mathbf{x} - \\mathbf{y}\\)):\n\\[ d(\\mathbf{x}, \\mathbf{y}) = \\| \\mathbf{x} - \\mathbf{y} \\| \\].\nIn termini di algebra matriciale, questa norma è calcolata come:\n\\[ d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{ (\\mathbf{x} - \\mathbf{y})^\\top (\\mathbf{x} - \\mathbf{y}) } \\]\nPassaggi dettagliati:\n\n\nCalcolo del vettore differenza:\n\\[\n\\mathbf{x} - \\mathbf{y} = \\begin{bmatrix} x_1 - y_1 \\\\ x_2 - y_2 \\end{bmatrix}\n\\]\n\n\nCalcolo del prodotto scalare:\n\\[\n(\\mathbf{x} - \\mathbf{y})^\\top (\\mathbf{x} - \\mathbf{y}) = (x_1 - y_1)^2 + (x_2 - y_2)^2\n\\]\n\n\nCalcolo della radice quadrata:\n\\[\nd(\\mathbf{x}, \\mathbf{y}) = \\sqrt{ (x_1 - y_1)^2 + (x_2 - y_2)^2 }\n\\]\n\n\nQuesto risultato corrisponde alla formula classica per la distanza tra due punti nel piano cartesiano.\n\n25.11.2 Estensione a più dimensioni\nPer vettori in uno spazio $ n $-dimensionale:\n\n\nx = \\(\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}\\)\n\n\ny = \\(\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\\)\n\n\nLa distanza euclidea diventa:\n\\[ d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{ (\\mathbf{x} - \\mathbf{y})^\\top (\\mathbf{x} - \\mathbf{y}) } \\]\nChe si espande in:\n\\[ d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{ \\sum_{i=1}^{n} (x_i - y_i)^2 } \\]\nIn conclusione:\nl’utilizzo dell’algebra matriciale permette di esprimere in modo compatto e generalizzato il calcolo della distanza euclidea tra due punti in qualsiasi dimensione, sfruttando operazioni matriciali come la trasposizione e il prodotto scalare.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/pca/01_linear_algebra.html#la-distanza-di-mahalanobis",
    "href": "chapters/pca/01_linear_algebra.html#la-distanza-di-mahalanobis",
    "title": "\n25  Elementi di algebra lineare\n",
    "section": "\n25.12 La distanza di Mahalanobis",
    "text": "25.12 La distanza di Mahalanobis\nLa distanza euclidea presuppone che le variabili siano non correlate e su scale comparabili. Tuttavia, in molti casi, le variabili possono avere scale diverse e possono essere correlate tra loro. La distanza di Mahalanobis tiene conto di queste differenze utilizzando la matrice di covarianza, permettendo una misurazione della distanza che considera sia la scala che la correlazione tra le variabili.\n\n25.12.1 Definizione\nLa distanza di Mahalanobis tra due vettori \\(\\mathbf{x}\\) e \\(\\mathbf{y}\\) è definita come:\n\\[\nd_M(\\mathbf{x}, \\mathbf{y}) = \\sqrt{ (\\mathbf{x} - \\mathbf{y})^\\top \\mathbf{S}^{-1} (\\mathbf{x} - \\mathbf{y}) }\n\\]\ndove:\n\n\n\\(\\mathbf{S}\\) è la matrice di covarianza delle variabili.\n\n\\(\\mathbf{S}^{-1}\\) è l’inversa della matrice di covarianza.\n\n25.12.2 Perché è necessaria la matrice di covarianza?\n\n\nScala delle variabili: Se le variabili hanno varianze diverse, la matrice di covarianza normalizza queste differenze, evitando che variabili con varianze maggiori dominino la misura della distanza.\n\nCorrelazione tra variabili: La matrice di covarianza tiene conto delle correlazioni tra le variabili, riducendo l’influenza delle variabili altamente correlate sulla distanza totale.\n\n25.12.3 Esempio numerico\nSupponiamo di avere due punti in uno spazio bidimensionale:\n\n\nPunto A: \\(\\mathbf{x} = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}\\)\n\n\nPunto B: \\(\\mathbf{y} = \\begin{bmatrix} 5 \\\\ 7 \\end{bmatrix}\\)\n\n\nE una matrice di covarianza stimata:\n\\[\n\\mathbf{S} = \\begin{bmatrix}\n4 & 2 \\\\\n2 & 3\n\\end{bmatrix}\n\\]\n\n25.12.3.1 Passaggi per il calcolo\n\n\nCalcolo del vettore differenza \\(\\mathbf{d}\\):\n\\[\n\\mathbf{d} = \\mathbf{x} - \\mathbf{y} = \\begin{bmatrix} 2 - 5 \\\\ 3 - 7 \\end{bmatrix} = \\begin{bmatrix} -3 \\\\ -4 \\end{bmatrix}\n\\]\n\n\nCalcolo dell’inversa della matrice di covarianza \\(\\mathbf{S}^{-1}\\):\n\n\nDeterminante di \\(\\mathbf{S}\\):\n\\[\n\\det(\\mathbf{S}) = (4)(3) - (2)(2) = 12 - 4 = 8\n\\]\n\n\nMatrice aggiunta (comatrice trasposta) di \\(\\mathbf{S}\\):\n\\[\n\\text{adj}(\\mathbf{S}) = \\begin{bmatrix}\n3 & -2 \\\\\n-2 & 4\n\\end{bmatrix}\n\\]\n\n\nInversa di \\(\\mathbf{S}\\):\n\\[\n\\mathbf{S}^{-1} = \\frac{1}{\\det(\\mathbf{S})} \\text{adj}(\\mathbf{S}) = \\frac{1}{8} \\begin{bmatrix}\n3 & -2 \\\\\n-2 & 4\n\\end{bmatrix} = \\begin{bmatrix}\n0.375 & -0.25 \\\\\n-0.25 & 0.5\n\\end{bmatrix}\n\\]\n\n\n\n\nCalcolo della distanza di Mahalanobis:\n\\[\nd_M(\\mathbf{x}, \\mathbf{y}) = \\sqrt{ \\mathbf{d}^\\top \\mathbf{S}^{-1} \\mathbf{d} }\n\\]\n\n\nCalcolo del prodotto \\(\\mathbf{S}^{-1} \\mathbf{d}\\):\n\\[\n\\mathbf{S}^{-1} \\mathbf{d} = \\begin{bmatrix}\n0.375 & -0.25 \\\\\n-0.25 & 0.5\n\\end{bmatrix} \\begin{bmatrix} -3 \\\\ -4 \\end{bmatrix} = \\begin{bmatrix}\n(-0.375 \\times 3) + (0.25 \\times 4) \\\\\n(0.25 \\times 3) + (-0.5 \\times 4)\n\\end{bmatrix} = \\begin{bmatrix}\n-0.125 \\\\\n-1.25\n\\end{bmatrix}\n\\]\n\n\nCalcolo del prodotto scalare:\n\\[\n\\mathbf{d}^\\top (\\mathbf{S}^{-1} \\mathbf{d}) = \\begin{bmatrix} -3 & -4 \\end{bmatrix} \\begin{bmatrix} -0.125 \\\\ -1.25 \\end{bmatrix} = (-3)(-0.125) + (-4)(-1.25) = 0.375 + 5 = 5.375\n\\]\n\n\nCalcolo della distanza:\n\\[\nd_M(\\mathbf{x}, \\mathbf{y}) = \\sqrt{5.375} \\approx 2.318\n\\]\n\n\n\n\n25.12.4 Confronto con la distanza euclidea\nLa distanza euclidea tra gli stessi punti è:\n\\[\nd_E(\\mathbf{x}, \\mathbf{y}) = \\sqrt{ (-3)^2 + (-4)^2 } = \\sqrt{9 + 16} = \\sqrt{25} = 5\n\\]\nCome si può notare, la distanza di Mahalanobis (\\(\\approx 2.318\\)) è diversa dalla distanza euclidea (5) a causa della considerazione delle varianze e delle correlazioni tra le variabili.\n\n25.12.5 Interpretazione\n\n\nVarianze diverse: Se una variabile ha una varianza elevata, le differenze lungo quella direzione avranno meno peso nella distanza totale.\n\nCorrelazioni: Se due variabili sono altamente correlate, la distanza di Mahalanobis riduce l’importanza delle differenze lungo la direzione in cui le variabili sono correlate.\n\nIn conclusione, la distanza di Mahalanobis è particolarmente utile in contesti multivariati dove le variabili hanno scale diverse e possono essere correlate. Essa fornisce una misura di distanza che è invariante rispetto alle trasformazioni lineari dei dati, rendendola ideale per l’analisi di dati statistici e il rilevamento di outlier.\nNota: È importante assicurarsi che la matrice di covarianza \\(\\mathbf{S}\\) sia non singolare (invertibile). In pratica, quando si lavora con campioni di dati, \\(\\mathbf{S}\\) viene stimata dai dati stessi.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/pca/01_linear_algebra.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/pca/01_linear_algebra.html#informazioni-sullambiente-di-sviluppo",
    "title": "\n25  Elementi di algebra lineare\n",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.2\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] ggokabeito_0.1.0  see_0.11.0        MASS_7.3-65       viridis_0.6.5    \n#&gt;  [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n#&gt;  [9] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n#&gt; [13] lavaan_0.6-19     psych_2.4.12      scales_1.3.0      markdown_1.13    \n#&gt; [17] knitr_1.50        lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n#&gt; [21] dplyr_1.1.4       purrr_1.0.4       readr_2.1.5       tidyr_1.3.1      \n#&gt; [25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.1      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.2.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] mime_0.13           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [25] Matrix_1.7-3        R6_2.6.1            fastmap_1.2.0      \n#&gt;  [28] rbibutils_2.3       shiny_1.10.0        digest_0.6.37      \n#&gt;  [31] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n#&gt;  [34] rprojroot_2.0.4     Hmisc_5.2-3         labeling_0.4.3     \n#&gt;  [37] timechange_0.3.0    abind_1.4-8         compiler_4.4.2     \n#&gt;  [40] withr_3.0.2         glasso_1.11         htmlTable_2.4.3    \n#&gt;  [43] backports_1.5.0     carData_3.0-5       ggsignif_0.6.4     \n#&gt;  [46] corpcor_1.6.10      gtools_3.9.5        tools_4.4.2        \n#&gt;  [49] pbivnorm_0.6.0      foreign_0.8-88      zip_2.3.2          \n#&gt;  [52] httpuv_1.6.15       nnet_7.3-20         glue_1.8.0         \n#&gt;  [55] quadprog_1.5-8      nlme_3.1-167        promises_1.3.2     \n#&gt;  [58] lisrelToR_0.3       grid_4.4.2          checkmate_2.3.2    \n#&gt;  [61] cluster_2.1.8.1     reshape2_1.4.4      generics_0.1.3     \n#&gt;  [64] gtable_0.3.6        tzdb_0.5.0          data.table_1.17.0  \n#&gt;  [67] hms_1.1.3           car_3.1-3           sem_3.1-16         \n#&gt;  [70] pillar_1.10.1       rockchalk_1.8.157   later_1.4.1        \n#&gt;  [73] splines_4.4.2       lattice_0.22-6      survival_3.8-3     \n#&gt;  [76] kutils_1.73         tidyselect_1.2.1    miniUI_0.1.1.1     \n#&gt;  [79] pbapply_1.7-2       reformulas_0.4.0    stats4_4.4.2       \n#&gt;  [82] xfun_0.51           qgraph_1.9.8        arm_1.14-4         \n#&gt;  [85] stringi_1.8.4       yaml_2.3.10         pacman_0.5.1       \n#&gt;  [88] boot_1.3-31         evaluate_1.0.3      codetools_0.2-20   \n#&gt;  [91] mi_1.1              cli_3.6.4           RcppParallel_5.1.10\n#&gt;  [94] rpart_4.1.24        xtable_1.8-4        Rdpack_2.6.3       \n#&gt;  [97] munsell_0.5.1       Rcpp_1.0.14         coda_0.19-4.1      \n#&gt; [100] png_0.1-8           XML_3.99-0.18       parallel_4.4.2     \n#&gt; [103] jpeg_0.1-10         lme4_1.1-36         mvtnorm_1.3-3      \n#&gt; [106] openxlsx_4.2.8      rlang_1.1.5         multcomp_1.4-28    \n#&gt; [109] mnormt_2.1.1\n\n\n\n\n\nCaudek, C., & Luccio, R. (2001). Statistica per psicologi (III rist. 2023, Vol. 11, p. 320). Laterza.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/pca/02_pca.html",
    "href": "chapters/pca/02_pca.html",
    "title": "18  Analisi delle componenti principali",
    "section": "",
    "text": "18.1 Introduzione\nL’Analisi delle Componenti Principali (PCA) è una tecnica statistica utilizzata per ridurre la dimensionalità di un insieme di dati. Il suo obiettivo è quello di semplificare analisi complesse conservando quanta più informazione possibile, cioè spiegando la maggior parte della varianza presente nei dati originali.\nLa PCA trasforma un insieme di variabili iniziali, spesso correlate tra loro, in un nuovo insieme di variabili non correlate dette componenti principali. Queste componenti sono combinazioni lineari delle variabili originali e sono ordinate in base alla quantità di varianza che riescono a spiegare.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Analisi delle componenti principali</span>"
    ]
  },
  {
    "objectID": "chapters/pca/02_pca.html#introduzione",
    "href": "chapters/pca/02_pca.html#introduzione",
    "title": "26  Analisi delle componenti principali",
    "section": "",
    "text": "ridurre il numero di variabili in studi con molti questionari o test psicometrici;\nidentificare le dimensioni sottostanti a un set di item (ad esempio, esplorare le dimensioni latenti di una scala);\npreparare i dati per analisi successive (ad esempio, in regressioni o modelli strutturali).\n\n\n26.1.1 Perché Usare la PCA in Psicologia?\nQuando un grande numero di variabili è fortemente correlato, può essere difficile interpretare i dati. In questi casi, la PCA permette di semplificare l’analisi mantenendo gran parte dell’informazione originale:\n\nle componenti principali catturano la varianza condivisa tra le variabili, fornendo un riepilogo efficace dei dati;\nse le prime componenti principali spiegano una quota sostanziale della varianza totale, possiamo ridurre il numero di variabili senza perdere significative informazioni.\n\n26.1.2 Cos’è la Varianza Totale?\nLa varianza totale rappresenta la quantità complessiva di variabilità nei dati. Nella PCA, è definita come la somma delle varianze delle variabili originali. Ad esempio, se abbiamo un dataset con tre variabili, la varianza totale è:\n\\[\n\\text{Varianza Totale} = \\sigma_1^2 + \\sigma_2^2 + \\sigma_3^2 ,\n\\]\ndove \\(\\sigma_i^2\\) è la varianza della variabile \\(i\\)-esima.\nNella PCA:\n\ngli autovalori (eigenvalues) rappresentano la varianza spiegata da ciascuna componente principale;\nla somma degli autovalori corrisponde alla varianza totale dei dati:\n\n\\[\n\\text{Somma degli autovalori} = \\text{Varianza Totale}\n\\]\n\n26.1.3 Un Nuovo Sistema di Coordinate\nLa PCA può essere interpretata come una ridescrizione dei dati in un nuovo sistema di assi coordinati. Questi nuovi assi (le componenti principali) sono calcolati come segue:\n\nle componenti principali sono orientate lungo le direzioni di massima varianza nei dati;\nla prima componente principale (PC1) è la direzione che spiega la massima quantità di varianza;\nla seconda componente principale (PC2) è ortogonale alla prima e spiega la successiva maggiore quantità di varianza, e così via.\n\nQuesto significa che la PCA non elimina le variabili, ma le ricombina in modo tale da rappresentare i dati in un sistema più semplice e interpretabile.\n\nEsempio 26.1 Supponiamo di avere 10 variabili in un questionario psicologico, molte delle quali sono fortemente correlate. Con la PCA, potremmo scoprire che le prime due componenti principali spiegano l’80% della varianza totale. In questo caso, potremmo ridurre l’analisi a queste due componenti, semplificando notevolmente l’interpretazione.\n\n\n26.1.4 Riduzione della Dimensionalità\nL’obiettivo principale della PCA è dunque quello di identificare il minor numero di componenti che spiegano la maggior parte della varianza nei dati. In psicologia, questo è particolarmente utile quando:\n\nsi vuole ridurre il numero di variabili per facilitare l’interpretazione;\nsi cerca di individuare dimensioni sottostanti (ad esempio, in uno studio sui tratti di personalità).\n\nAd esempio, in uno studio sui Big Five, la PCA potrebbe ridurre centinaia di item iniziali alle cinque dimensioni principali.\n\n26.1.5 Interpretazione dei Risultati\nLa PCA produce due risultati principali:\n\nPunteggi delle Componenti Principali: Ogni osservazione ottiene un punteggio per ciascuna componente principale, che rappresenta la sua posizione nel nuovo spazio.\nVarianza Spiegata: La proporzione di varianza spiegata da ciascuna componente principale è un indicatore della sua importanza: \\(\\text{Varianza Spiegata per PC} = \\text{Autovalore della PC} / \\text{Somma degli autovalori}\\).\n\nSe, ad esempio, la PC1 spiega il 60% della varianza e la PC2 il 20%, possiamo concludere che le prime due componenti rappresentano l’80% della variabilità nei dati.\nIn sintesi, la PCA è uno strumento potente per semplificare e interpretare dataset complessi in psicologia, soprattutto quando ci troviamo di fronte a molte variabili correlate. Questo metodo non solo facilita l’analisi, ma può anche fornire una nuova prospettiva sulle relazioni tra le variabili, evidenziando dimensioni latenti che altrimenti potrebbero non essere immediatamente evidenti.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Analisi delle componenti principali</span>"
    ]
  },
  {
    "objectID": "chapters/pca/02_pca.html#tutorial",
    "href": "chapters/pca/02_pca.html#tutorial",
    "title": "26  Analisi delle componenti principali",
    "section": "\n26.2 Tutorial",
    "text": "26.2 Tutorial\nEsaminiamo qui di seguito l’analisi delle componenti principali passo passo.\n\n26.2.1 Passo 1: Creare un dataset\nPer cominciare, generiamo un dataset di esempio per applicare la PCA.\n\n# Generare un dataset con due variabili correlate\nset.seed(123)\nX &lt;- data.frame(\n  x1 = rnorm(100, mean = 5, sd = 2),\n  x2 = rnorm(100, mean = 10, sd = 3)\n)\nX$x2 &lt;- 0.8 * X$x1 + rnorm(100, mean = 0, sd = 1)  # Introduciamo correlazione\n\n\n26.2.2 Passo 2: Standardizzare i dati\nPrima di calcolare la PCA, è importante standardizzare le variabili (sottrarre la media e dividere per la deviazione standard) per garantire che abbiano lo stesso peso.\n\n# Centrare e scalare le variabili\nX_scaled &lt;- scale(X)\n\n# Plot con aspect ratio 1\nplot(X_scaled[, 1], X_scaled[, 2], asp = 1, \n     col = \"blue\", pch = 19, \n     main = \"Dati standardizzati con aspect ratio = 1\",\n     xlab = \"Variabile x1 standardizzata\",\n     ylab = \"Variabile x2 standardizzata\")\n\n\n\n\n\n\n\n\ncar::dataEllipse(\n  X_scaled[, 1], X_scaled[, 2],\n  levels = 0.95,\n  lty = 2,\n  ylim = c(-3, 3),\n  xlim = c(-3, 3),\n  asp = 1, \n)\n\n\n\n\n\n\n\n\n26.2.3 Passo 3: Calcolare la matrice di covarianza\nLa PCA utilizza la matrice di covarianza per calcolare le componenti principali.\n\ncov_matrix &lt;- cov(X_scaled)\nprint(cov_matrix)\n#&gt;        x1     x2\n#&gt; x1 1.0000 0.8177\n#&gt; x2 0.8177 1.0000\n\n\n26.2.4 Passo 4: Calcolare autovalori e autovettori\nUtilizziamo l’algebra lineare per ottenere gli autovalori e gli autovettori della matrice di covarianza.\n\neigen_decomp &lt;- eigen(cov_matrix)\neigenvalues &lt;- eigen_decomp$values       # Autovalori\neigenvectors &lt;- eigen_decomp$vectors     # Autovettori\nprint(eigenvalues)\n#&gt; [1] 1.8177 0.1823\n\n\nprint(eigenvectors)\n#&gt;        [,1]    [,2]\n#&gt; [1,] 0.7071 -0.7071\n#&gt; [2,] 0.7071  0.7071\n\nGli autovalori rappresentano la varianza spiegata dalle componenti principali, mentre gli autovettori indicano le direzioni delle componenti principali.\n\n# First eigenvector \nev_1 &lt;- eigen_decomp$vectors[, 1]\n\n# Slope of the first eigenvector\nev1_m &lt;- ev_1[2] / ev_1[1]\n\n# Second eigenvector \nev_2 &lt;- eigen_decomp$vectors[, 2]\n\n# Slope of the second eigenvector\nev2_m &lt;- ev_2[2] / ev_2[1]\n\n\n# Scatter plot showing the span of both eigenvectors \ndata.frame(zx=X_scaled[, 1], zy= X_scaled[, 2])  |&gt;\nggplot(aes(x = zx, y = zy)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_vline(xintercept = 0, linewidth = .5) +\n  geom_hline(yintercept = 0, linewidth = .5) +\n  geom_abline(slope = ev1_m, color = \"blue\", linewidth = 0.7) +\n  geom_abline(slope = ev2_m, color = \"red\", linewidth = 0.7) \n\n\n\n\n\n\n\nGli autovettori sono ortogonali:\n\nprint(ev_1 %*% ev_2)\n#&gt;           [,1]\n#&gt; [1,] 2.237e-17\n\nGeneriamo uno Scree Plot.\n\n# Calculate the estimated variance for each eigenvalue\ne_var &lt;- eigen_decomp$values / (length(X_scaled[, 1]) - 1)\n\n# Data frame with variance percentages\nvar_per &lt;- tibble(\n  PC  = c(\"PC1\", \"PC2\"),\n  PER = c(e_var) * 100 / sum(e_var) # Calculate the percentage\n    )\n\n# Scree plot \nggplot(var_per, aes(x = PC, y = PER)) +\n  geom_col(width = 0.5, color = \"black\") +\n  xlab(\"Principal component\") +\n  ylab(\"Percentage of variation (%)\") \n\n\n\n\n\n\n\nVerifichiamo che la somma degli autovalori sia uguale alla varianza totale.\n\nvar(X_scaled[, 1]) + var(X_scaled[, 2])\n#&gt; [1] 2\n\n\neigen_decomp$values |&gt; sum()\n#&gt; [1] 2\n\nGli autovettori ottenuti utilizzando la funzione eigen() sono normalizzati. Ciò significa che la loro lunghezza è uguale a 1:\n\nt(as.matrix(eigen_decomp$vectors[, 1])) %*% \n  as.matrix(eigen_decomp$vectors[, 1]) \n#&gt;      [,1]\n#&gt; [1,]    1\n\nUtilizziamo le informazioni degli autovettori e degli autovalori per disegnare gli assi dell’ellisse:\n\ngli autovettori determinano la direzione degli assi;\nla radice quadrata degli autovalori è proporzionale alla lunghezza degli assi dell’ellisse.\n\n\ncar::dataEllipse(\n  X_scaled[, 1], X_scaled[, 2],\n  levels = 0.95,\n  lty = 2,\n  ylim = c(-3, 3),\n  xlim = c(-3, 3), \n  asp = 1\n)\nk &lt;- 2.5\narrows(\n  0, 0, \n  k * sqrt(eigen_decomp$values[1]) * eigen_decomp$vectors[1],\n  k * sqrt(eigen_decomp$values[1]) * eigen_decomp$vectors[2],\n  code = 2, \n  col = \"red\", \n  lwd = 2\n)\narrows(\n  0, 0, \n  k * sqrt(eigen_decomp$values[2]) * eigen_decomp$vectors[1],\n  k * sqrt(eigen_decomp$values[2]) * -eigen_decomp$vectors[2],\n  code = 2, \n  col = \"red\", \n  lwd = 2\n)\n\n\n\n\n\n\n\nTale analisi si può estendere a qualunque numero di variabili. Per esempio, nel caso di tre variabili, possiamo pensare di disegnare un ellisoide attorno ad una nube di punti nello spazio tridimensionale. Anche in questo caso, gli autovalori e gli associati autovettori corrisponderanno agli assi dell’elissoide.\n\n26.2.5 Passo 5: Proiettare i dati sulle componenti principali\nPer calcolare i punteggi delle Componenti Principali, dobbiamo proiettare ortogonalmente i punti originali del dataset sulle nuove coordinate, definite dalle direzioni principali (autovettori). Questo processo ci permette di rappresentare ogni osservazione nello spazio delle componenti principali.\nNell’algebra lineare, la proiezione ortogonale consiste nel trovare la posizione di un punto su una retta o un piano, in modo che il vettore risultante sia perpendicolare alla direzione di proiezione.\nNel contesto della PCA:\n\nGli autovettori rappresentano le direzioni principali (componenti principali) lungo cui la varianza dei dati è massimizzata.\nProiettare un punto sui componenti principali significa calcolare la sua posizione lungo queste nuove direzioni.\n\n\n26.2.5.1 Formulazione Matematica\nConsideriamo le seguenti matrici:\n\n\n\\(\\mathbf{X}_{\\text{scaled}}\\): la matrice dei dati standardizzati, in cui ogni riga rappresenta un’osservazione e ogni colonna una variabile.\n\n\\(\\mathbf{V}\\): la matrice degli autovettori, le cui colonne rappresentano le nuove direzioni principali.\n\nLa proiezione dei dati nello spazio delle componenti principali si calcola come:\n\\[\n\\mathbf{Z} = \\mathbf{X}_{\\text{scaled}} \\cdot \\mathbf{V}\n\\]\ndove:\n\n\n\\(\\mathbf{Z}\\) è la matrice dei punteggi delle componenti principali.\nOgni riga di \\(\\mathbf{Z}\\) rappresenta un’osservazione trasformata nello spazio delle componenti principali.\nOgni colonna di \\(\\mathbf{Z}\\) corrisponde a una componente principale (ad esempio, PC1, PC2).\n\n26.2.5.2 Implementazione in R\nIn R, questo calcolo può essere realizzato attraverso il prodotto matrice-matrice. Ecco il codice per calcolare i punteggi delle componenti principali:\n\n# Calcolo dei punteggi delle componenti principali\npc_scores &lt;- as.matrix(X_scaled) %*% eigenvectors\ncolnames(pc_scores) &lt;- c(\"PC1\", \"PC2\")  # Etichettare le componenti principali\n\nPer verificare i risultati, possiamo visualizzare i primi punteggi calcolati:\n\n# Stampare i primi punteggi delle componenti principali\nprint(head(pc_scores))\n#&gt;           PC1     PC2\n#&gt; [1,] -0.05606  0.9523\n#&gt; [2,]  0.04512  0.5418\n#&gt; [3,]  1.98607 -0.2887\n#&gt; [4,]  0.15352  0.1844\n#&gt; [5,] -0.17413 -0.2344\n#&gt; [6,]  2.12408 -0.3930\n\n\n26.2.5.3 Interpretazione dei Punteggi\nOgni valore in pc_scores rappresenta la posizione dell’osservazione nello spazio trasformato delle componenti principali:\n\nLa PC1 è la direzione lungo cui si osserva la massima varianza dei dati.\nLa PC2 è la direzione ortogonale successiva con la seconda massima varianza, e così via.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Analisi delle componenti principali</span>"
    ]
  },
  {
    "objectID": "chapters/pca/02_pca.html#passo-6-confrontare-con-loutput-di-prcomp",
    "href": "chapters/pca/02_pca.html#passo-6-confrontare-con-loutput-di-prcomp",
    "title": "26  Analisi delle componenti principali",
    "section": "\n26.3 Passo 6: Confrontare con l’output di prcomp\n",
    "text": "26.3 Passo 6: Confrontare con l’output di prcomp\n\nUtilizziamo la funzione prcomp di R per confermare i risultati.\n\npca &lt;- prcomp(X, scale. = TRUE)\nprint(pca)\n#&gt; Standard deviations (1, .., p=2):\n#&gt; [1] 1.348 0.427\n#&gt; \n#&gt; Rotation (n x k) = (2 x 2):\n#&gt;       PC1     PC2\n#&gt; x1 0.7071  0.7071\n#&gt; x2 0.7071 -0.7071\n\n\n# Confronto tra i punteggi calcolati manualmente e quelli di prcomp\nprint(head(pca$x))\n#&gt;           PC1     PC2\n#&gt; [1,] -0.05606 -0.9523\n#&gt; [2,]  0.04512 -0.5418\n#&gt; [3,]  1.98607  0.2887\n#&gt; [4,]  0.15352 -0.1844\n#&gt; [5,] -0.17413  0.2344\n#&gt; [6,]  2.12408  0.3930\n\n\n26.3.1 Passo 7: Visualizzare la proiezione dei dati\nPossiamo visualizzare i punti originali proiettati sulle componenti principali.\n\n# Grafico del dataset originale\nplot(\n  X_scaled, \n  col = \"blue\", pch = 19, \n  main = \"Dati originali e componenti principali\",\n  asp = 1\n)\nabline(0, eigenvectors[2,1] / eigenvectors[1,1], col = \"red\", lwd = 2)  \n# Prima componente\nabline(0, eigenvectors[2,2] / eigenvectors[1,2], col = \"green\", lwd = 2)  \n# Seconda componente\n\n\n\n\n\n\n\n\n# Grafico delle componenti principali\nplot(\n  pc_scores, \n  col = \"blue\", pch = 19, \n  main = \"Punteggi delle componenti principali\",\n  asp = 1)",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Analisi delle componenti principali</span>"
    ]
  },
  {
    "objectID": "chapters/pca/02_pca.html#biplot",
    "href": "chapters/pca/02_pca.html#biplot",
    "title": "26  Analisi delle componenti principali",
    "section": "\n26.4 Biplot",
    "text": "26.4 Biplot\nIl biplot è uno strumento grafico che combina la visualizzazione dei punteggi delle componenti principali e delle variabili originali in un unico grafico. Questo permette di:\n\nInterpretare la relazione tra le variabili originali.\nVisualizzare come le osservazioni (campioni) si distribuiscono nello spazio delle componenti principali.\nIdentificare cluster, outlier, o pattern nei dati.\n\nUn biplot combina due tipi di informazioni:\n\n\nI punteggi delle componenti principali (proiezioni delle osservazioni sulle componenti principali), rappresentati come punti.\n\nI carichi delle variabili originali sulle componenti principali (autovettori), rappresentati come frecce.\n\nLe frecce indicano:\n\nLa direzione della variabilità spiegata da ciascuna variabile.\nLa correlazione tra le variabili e le componenti principali.\n\n\n26.4.1 Come Creare un Biplot in R\nPer creare un biplot in R possiamo utilizzare prcomp. Supponiamo di avere già calcolato la PCA con la funzione prcomp:\n\n# PCA con prcomp\npca &lt;- prcomp(X, scale. = TRUE)\n\nIl biplot si visualizza nel modo seguente.\n\n# Creare un biplot\nbiplot(\n  pca, scale = 0, \n  main = \"Biplot delle Componenti Principali\", \n  xlab = \"PC1\", ylab = \"PC2\"\n)\n\n\n\n\n\n\n\n\n\nscale = 0: Evita di ridimensionare le frecce e i punteggi per semplificare l’interpretazione.\n\nNel grafico:\n\n\nI punti rappresentano le osservazioni, proiettate sulle componenti principali.\n\nLe frecce rappresentano le variabili originali, con:\n\nLa lunghezza della freccia che indica la forza della correlazione con le componenti principali.\nL’angolo tra due frecce che rappresenta la correlazione tra le due variabili:\n\nUn angolo piccolo indica una correlazione positiva.\nUn angolo di 90° indica una correlazione nulla.\nUn angolo ampio (vicino a 180°) indica una correlazione negativa.\n\n\n\n\n\n26.4.2 Interpretazione\nIn psicologia, il biplot è particolarmente utile per:\n\n\nIdentificare pattern nei dati: Ad esempio, come i partecipanti si distribuiscono lungo dimensioni psicologiche latenti (es. tratti di personalità).\n\nEsaminare le relazioni tra variabili: Le frecce possono evidenziare cluster di variabili correlate che rappresentano dimensioni psicologiche (es. ansia, stress, depressione).\n\nValutare l’adeguatezza della PCA: Se le frecce delle variabili sono lunghe e ben distribuite lungo le componenti principali, ciò suggerisce che la PCA sta spiegando bene la varianza delle variabili.\n\nIn sostanza, il biplot è uno strumento grafico che semplifica l’interpretazione della PCA. Combina in un unico diagramma sia le informazioni sulle variabili originali che sulla loro proiezione nello spazio delle componenti principali, offrendo una visione d’insieme chiara e immediata dei dati.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Analisi delle componenti principali</span>"
    ]
  },
  {
    "objectID": "chapters/pca/02_pca.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/pca/02_pca.html#informazioni-sullambiente-di-sviluppo",
    "title": "18  Analisi delle componenti principali",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.2\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] ggokabeito_0.1.0  see_0.11.0        MASS_7.3-65       viridis_0.6.5    \n#&gt;  [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n#&gt;  [9] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n#&gt; [13] lavaan_0.6-19     psych_2.4.12      scales_1.3.0      markdown_1.13    \n#&gt; [17] knitr_1.50        lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n#&gt; [21] dplyr_1.1.4       purrr_1.0.4       readr_2.1.5       tidyr_1.3.1      \n#&gt; [25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.1      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.2.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] mime_0.13           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [25] Matrix_1.7-3        R6_2.6.1            fastmap_1.2.0      \n#&gt;  [28] rbibutils_2.3       shiny_1.10.0        digest_0.6.37      \n#&gt;  [31] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n#&gt;  [34] rprojroot_2.0.4     Hmisc_5.2-3         labeling_0.4.3     \n#&gt;  [37] timechange_0.3.0    abind_1.4-8         compiler_4.4.2     \n#&gt;  [40] withr_3.0.2         glasso_1.11         htmlTable_2.4.3    \n#&gt;  [43] backports_1.5.0     carData_3.0-5       ggsignif_0.6.4     \n#&gt;  [46] corpcor_1.6.10      gtools_3.9.5        tools_4.4.2        \n#&gt;  [49] pbivnorm_0.6.0      foreign_0.8-88      zip_2.3.2          \n#&gt;  [52] httpuv_1.6.15       nnet_7.3-20         glue_1.8.0         \n#&gt;  [55] quadprog_1.5-8      nlme_3.1-167        promises_1.3.2     \n#&gt;  [58] lisrelToR_0.3       grid_4.4.2          checkmate_2.3.2    \n#&gt;  [61] cluster_2.1.8.1     reshape2_1.4.4      generics_0.1.3     \n#&gt;  [64] gtable_0.3.6        tzdb_0.5.0          data.table_1.17.0  \n#&gt;  [67] hms_1.1.3           car_3.1-3           sem_3.1-16         \n#&gt;  [70] pillar_1.10.1       rockchalk_1.8.157   later_1.4.1        \n#&gt;  [73] splines_4.4.2       lattice_0.22-6      survival_3.8-3     \n#&gt;  [76] kutils_1.73         tidyselect_1.2.1    miniUI_0.1.1.1     \n#&gt;  [79] pbapply_1.7-2       reformulas_0.4.0    stats4_4.4.2       \n#&gt;  [82] xfun_0.51           qgraph_1.9.8        arm_1.14-4         \n#&gt;  [85] stringi_1.8.4       yaml_2.3.10         pacman_0.5.1       \n#&gt;  [88] boot_1.3-31         evaluate_1.0.3      codetools_0.2-20   \n#&gt;  [91] mi_1.1              cli_3.6.4           RcppParallel_5.1.10\n#&gt;  [94] rpart_4.1.24        xtable_1.8-4        Rdpack_2.6.3       \n#&gt;  [97] munsell_0.5.1       Rcpp_1.0.14         coda_0.19-4.1      \n#&gt; [100] png_0.1-8           XML_3.99-0.18       parallel_4.4.2     \n#&gt; [103] jpeg_0.1-10         lme4_1.1-36         mvtnorm_1.3-3      \n#&gt; [106] openxlsx_4.2.8      rlang_1.1.5         multcomp_1.4-28    \n#&gt; [109] mnormt_2.1.1\n\n\n\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Analisi delle componenti principali</span>"
    ]
  },
  {
    "objectID": "chapters/pca/02_pca.html#perché-usare-la-pca-in-psicologia",
    "href": "chapters/pca/02_pca.html#perché-usare-la-pca-in-psicologia",
    "title": "18  Analisi delle componenti principali",
    "section": "\n18.2 Perché Usare la PCA in Psicologia?",
    "text": "18.2 Perché Usare la PCA in Psicologia?\nIn psicologia, ci troviamo spesso a lavorare con dati ad alta dimensionalità: questionari con decine di item, batterie di test, o set di dati raccolti tramite studi longitudinali. La PCA è utile perché:\n\n\nsemplifica l’interpretazione dei dati, riducendo molte variabili a poche dimensioni latenti;\n\nelimina ridondanze: se due o più variabili sono altamente correlate, la PCA può rappresentarle con un’unica componente;\n\nfavorisce la visualizzazione, soprattutto nei casi in cui si riescano a ridurre i dati a 2 o 3 componenti principali;\n\nprepara i dati per analisi successive come regressioni o modelli strutturali, evitando collinearità tra predittori.\n\n\n🧩 Esempio tipico: un questionario su tratti di personalità con 50 item può essere ridotto a 5 componenti principali che riflettono le dimensioni dei Big Five.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Analisi delle componenti principali</span>"
    ]
  },
  {
    "objectID": "chapters/pca/02_pca.html#cosè-la-varianza-totale",
    "href": "chapters/pca/02_pca.html#cosè-la-varianza-totale",
    "title": "18  Analisi delle componenti principali",
    "section": "\n18.3 Cos’è la Varianza Totale?",
    "text": "18.3 Cos’è la Varianza Totale?\nLa varianza totale rappresenta la somma della variabilità presente in ciascuna variabile del dataset.\nSe abbiamo tre variabili, la varianza totale sarà:\n\\[\n\\text{Varianza Totale} = \\sigma_1^2 + \\sigma_2^2 + \\sigma_3^2 ,\n\\]\ndove \\(\\sigma_i^2\\) è la varianza della variabile \\(i\\)-esima.\nNella PCA:\n\nogni autovalore (o eigenvalue) rappresenta la quantità di varianza spiegata da una componente principale;\nla somma degli autovalori è pari alla varianza totale del dataset (dopo la standardizzazione, essa sarà uguale al numero di variabili).",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Analisi delle componenti principali</span>"
    ]
  },
  {
    "objectID": "chapters/pca/02_pca.html#un-nuovo-sistema-di-coordinate",
    "href": "chapters/pca/02_pca.html#un-nuovo-sistema-di-coordinate",
    "title": "18  Analisi delle componenti principali",
    "section": "\n18.4 Un Nuovo Sistema di Coordinate",
    "text": "18.4 Un Nuovo Sistema di Coordinate\nLa PCA può essere intesa come una rotazione del sistema di riferimento nello spazio delle variabili:\n\nla prima componente principale (PC1) è la direzione lungo cui la varianza dei dati è massima;\nla seconda componente (PC2) è perpendicolare alla prima (cioè ortogonale) e spiega la massima varianza residua;\nle successive componenti seguono lo stesso principio.\n\nQuesto nuovo sistema è costruito in modo tale che le componenti siano non correlate tra loro (ortogonali) e spiegano, progressivamente, meno varianza.\n\n💡 Nota importante: la PCA non elimina variabili, ma le riorganizza in modo da ridurre la complessità informativa.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Analisi delle componenti principali</span>"
    ]
  },
  {
    "objectID": "chapters/pca/02_pca.html#riduzione-della-dimensionalità",
    "href": "chapters/pca/02_pca.html#riduzione-della-dimensionalità",
    "title": "18  Analisi delle componenti principali",
    "section": "\n18.5 Riduzione della Dimensionalità",
    "text": "18.5 Riduzione della Dimensionalità\nLa PCA consente di rappresentare i dati originali in un nuovo spazio, più compatto, ma informativamente ricco.\n\n18.5.1 Cosa significa “ridurre la dimensionalità”?\n\n\nRiduzione: da p variabili iniziali (es. 30 item), possiamo ottenere k componenti principali (es. 3 o 5), dove k &lt; p;\n\nObiettivo: mantenere una soglia prefissata di varianza spiegata, ad esempio il 70% o l’80%.\n\n\n🎯 Esempio concreto: In uno studio sui Big Five, la PCA può ridurre un set di 100 item a 5 componenti, ciascuna interpretabile come una delle dimensioni di personalità.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Analisi delle componenti principali</span>"
    ]
  },
  {
    "objectID": "chapters/pca/02_pca.html#interpretazione-dei-risultati-della-pca",
    "href": "chapters/pca/02_pca.html#interpretazione-dei-risultati-della-pca",
    "title": "18  Analisi delle componenti principali",
    "section": "\n18.6 Interpretazione dei Risultati della PCA",
    "text": "18.6 Interpretazione dei Risultati della PCA\nLa PCA produce due insiemi principali di risultati:\n\nPunteggi delle componenti principali (scores)\nPer ogni partecipante (o unità osservata), si ottiene un punteggio per ciascuna componente. Questi punteggi possono essere utilizzati come nuove variabili sintetiche.\nVarianza spiegata (eigenvalues)\nL’importanza di ciascuna componente è misurata dalla proporzione di varianza che essa spiega:\n\n\\[\n\\text{Proporzione di varianza spiegata dalla PC}_i = \\frac{\\lambda_i}{\\sum_{j=1}^{p} \\lambda_j} ,\n\\]\ndove \\(\\lambda_i\\) è l’autovalore della componente i.\n\n🔍 Interpretazione tipica: Se la PC1 spiega il 50% della varianza e la PC2 un ulteriore 30%, possiamo dire che le prime due componenti spiegano l’80% della variabilità totale.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Analisi delle componenti principali</span>"
    ]
  },
  {
    "objectID": "chapters/pca/02_pca.html#geometria-della-pca-assi-autovalori-e-autovettori",
    "href": "chapters/pca/02_pca.html#geometria-della-pca-assi-autovalori-e-autovettori",
    "title": "18  Analisi delle componenti principali",
    "section": "\n18.7 Geometria della PCA: Assi, Autovalori e Autovettori",
    "text": "18.7 Geometria della PCA: Assi, Autovalori e Autovettori\nLa PCA si basa su concetti fondamentali dell’algebra lineare:\n\n\nautovalori (eigenvalues): quantificano la varianza spiegata da ogni componente;\n\nautovettori (eigenvectors): definiscono le direzioni lungo cui si osserva la varianza massima nei dati.\n\nNel piano bidimensionale:\n\nogni autovettore è un asse di una nuova base ortogonale;\nla lunghezza dell’asse è proporzionale alla radice quadrata dell’autovalore corrispondente;\nle osservazioni vengono proiettate ortogonalmente su questi assi per ottenere i punteggi delle componenti principali.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Analisi delle componenti principali</span>"
    ]
  },
  {
    "objectID": "chapters/pca/02_pca.html#visualizzazione-scree-plot-e-biplot",
    "href": "chapters/pca/02_pca.html#visualizzazione-scree-plot-e-biplot",
    "title": "18  Analisi delle componenti principali",
    "section": "\n18.8 Visualizzazione: Scree Plot e Biplot",
    "text": "18.8 Visualizzazione: Scree Plot e Biplot\n\n18.8.1 Scree Plot\nLo Scree Plot è un grafico che mostra gli autovalori ordinati per componente:\n\npermette di determinare quante componenti mantenere;\nun “gomito” nel grafico indica il punto oltre il quale le componenti aggiuntive spiegano poca varianza.\n\n18.8.2 Biplot\nIl biplot mostra simultaneamente:\n\n\ni punteggi delle osservazioni (punti);\n\ni contributi delle variabili originali (frecce).\n\n\n🎓 Guida all’interpretazione del biplot:\n- Correlazione positiva: variabili con frecce vicine tra loro (angolo piccolo tra i vettori);\n- Correlazione negativa: variabili con frecce dirette in versi opposti (angolo ≈ 180°);\n- Nessuna correlazione: variabili con frecce quasi perpendicolari (angolo ≈ 90°).\nOgni angolo riflette l’intensità della relazione tra le variabili analizzate.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Analisi delle componenti principali</span>"
    ]
  },
  {
    "objectID": "chapters/pca/02_pca.html#tutorial-in-r",
    "href": "chapters/pca/02_pca.html#tutorial-in-r",
    "title": "18  Analisi delle componenti principali",
    "section": "\n18.9 Tutorial in R",
    "text": "18.9 Tutorial in R\n\n18.9.1 Obiettivo del tutorial\nApplicare la PCA passo dopo passo su un dataset simulato con due variabili correlate, comprendendo ogni fase del processo:\n\ncreazione dei dati e visualizzazione;\nstandardizzazione;\ncalcolo della PCA con algebra lineare;\ninterpretazione geometrica (autovalori, autovettori);\nvisualizzazioni: ellissi, componenti, scree plot;\nproiezione dei dati nello spazio delle componenti;\nverifica con prcomp();\ncostruzione del biplot.\n\n18.9.2 Passo 1: Creare un dataset\nGeneriamo un dataset con due variabili correlate.\n\n# Generiamo due variabili correlate\nset.seed(123)\nX &lt;- data.frame(\n  x1 = rnorm(100, mean = 5, sd = 2),\n  x2 = rnorm(100, mean = 10, sd = 3)\n)\n\n# Aggiungiamo una correlazione lineare tra x1 e x2\nX$x2 &lt;- 0.8 * X$x1 + rnorm(100, mean = 0, sd = 1)\n\n\n18.9.3 Passo 2: Standardizzare i dati\nLa standardizzazione è essenziale nella PCA quando le variabili hanno scale diverse. Dopo la standardizzazione, ogni variabile ha media = 0 e deviazione standard = 1.\n\n# Standardizziamo le variabili\nX_scaled &lt;- scale(X)\n\nVisualizziamo i dati standardizzati:\n\nggplot(X_scaled, aes(x = x1, y = x2)) +\n  geom_point(shape = 19) +\n  coord_fixed(ratio = 1) + # Imposta l'aspect ratio a 1\n  labs(\n    x = \"x1 standardizzata\",\n    y = \"x2 standardizzata\",\n    title = \"Dati standardizzati (asp = 1)\"\n  )\n\n\n\n\n\n\n\nAggiungiamo un’ellisse di confidenza per mostrare la distribuzione:\n\ncar::dataEllipse(\n  X_scaled[, 1], X_scaled[, 2],\n  levels = 0.95, lty = 2,\n  asp = 1,\n  xlab = \"x1\", ylab = \"x2\"\n)\n\n\n\n\n\n\n\n\n18.9.4 Passo 3: Calcolare la matrice di covarianza\n\ncov_matrix &lt;- cov(X_scaled)\ncov_matrix\n#&gt;        x1     x2\n#&gt; x1 1.0000 0.8177\n#&gt; x2 0.8177 1.0000\n\n\n18.9.5 Passo 4: Calcolare autovalori e autovettori\n\neigen_decomp &lt;- eigen(cov_matrix)\neigenvalues &lt;- eigen_decomp$values       # Varianza spiegata (autovalori)\neigenvectors &lt;- eigen_decomp$vectors     # Direzioni principali (autovettori)\n\nStampiamo i risultati:\n\neigenvalues\n#&gt; [1] 1.8177 0.1823\n\n\neigenvectors\n#&gt;        [,1]    [,2]\n#&gt; [1,] 0.7071 -0.7071\n#&gt; [2,] 0.7071  0.7071\n\nVerifichiamo che gli autovettori siano ortogonali (prodotto scalare = 0):\n\nt(eigenvectors[,1]) %*% eigenvectors[,2]\n#&gt;           [,1]\n#&gt; [1,] 2.237e-17\n\n\n18.9.6 Visualizzare le direzioni principali\nCalcoliamo le pendenze degli autovettori:\n\nev1_slope &lt;- eigenvectors[2, 1] / eigenvectors[1, 1]\nev2_slope &lt;- eigenvectors[2, 2] / eigenvectors[1, 2]\n\nVisualizziamo il grafico con gli autovettori sovrapposti:\n\ndata.frame(zx = X_scaled[, 1], zy = X_scaled[, 2]) |&gt;\n  ggplot(aes(x = zx, y = zy)) +\n  geom_point(size = 2) +\n  geom_vline(xintercept = 0, linewidth = .5) +\n  geom_hline(yintercept = 0, linewidth = .5) +\n  geom_abline(slope = ev1_slope, color = \"red\", linewidth = 1.2) +\n  geom_abline(slope = ev2_slope, color = \"blue\", linewidth = 1.2) +\n  ggtitle(\"Autovettori: direzioni delle componenti principali\") \n\n\n\n\n\n\n\n\n18.9.7 Scree Plot – Percentuale di varianza spiegata\n\n# Percentuale di varianza spiegata\nvar_per &lt;- tibble(\n  PC = c(\"PC1\", \"PC2\"),\n  Percent = eigenvalues / sum(eigenvalues) * 100\n)\n\nggplot(var_per, aes(x = PC, y = Percent)) +\n  geom_col(fill = \"skyblue\", color = \"black\", width = 0.5) +\n  ylab(\"Varianza spiegata (%)\") +\n  xlab(\"Componente Principale\") +\n  ggtitle(\"Scree Plot\") \n\n\n\n\n\n\n\nVerifica: somma degli autovalori ≈ somma delle varianze\n\nsum(eigenvalues)\n#&gt; [1] 2\nsum(apply(X_scaled, 2, var))  # Varianza totale\n#&gt; [1] 2\n\n\n18.9.8 Passo 5: Visualizzazione geometrica (ellisse + assi principali)\n\ncar::dataEllipse(\n  X_scaled[, 1], X_scaled[, 2],\n  levels = 0.95, lty = 2,\n  xlim = c(-3, 3), ylim = c(-3, 3),\n  asp = 1,\n  xlab = \"x1\", ylab = \"x2\"\n)\n\n# Disegniamo gli assi in base agli autovettori\nk &lt;- 2.5\narrows(0, 0,\n       k * sqrt(eigenvalues[1]) * eigenvectors[1, 1],\n       k * sqrt(eigenvalues[1]) * eigenvectors[2, 1],\n       col = \"red\", lwd = 2, code = 2)\n\narrows(0, 0,\n       k * sqrt(eigenvalues[2]) * eigenvectors[1, 2],\n       k * sqrt(eigenvalues[2]) * eigenvectors[2, 2],\n       col = \"red\", lwd = 2, code = 2)\n\n\n\n\n\n\n\n\n18.9.9 Passo 6: Proiezione dei dati (calcolo dei punteggi)\n\npc_scores &lt;- as.matrix(X_scaled) %*% eigenvectors\ncolnames(pc_scores) &lt;- c(\"PC1\", \"PC2\")\n\nhead(pc_scores)  # Mostra le prime osservazioni nel nuovo spazio\n#&gt;           PC1     PC2\n#&gt; [1,] -0.05606  0.9523\n#&gt; [2,]  0.04512  0.5418\n#&gt; [3,]  1.98607 -0.2887\n#&gt; [4,]  0.15352  0.1844\n#&gt; [5,] -0.17413 -0.2344\n#&gt; [6,]  2.12408 -0.3930\n\nVisualizziamo le osservazioni nello spazio delle componenti:\n\npc_df &lt;- as.data.frame(pc_scores)\n\n# Grafico ggplot dei punteggi delle componenti principali\nggplot(pc_df, aes(x = PC1, y = PC2)) +\n  geom_point(color = \"blue\", size = 2) +\n  labs(\n    title = \"Punteggi delle Componenti Principali\",\n    x = \"PC1\",\n    y = \"PC2\"\n  ) +\n  coord_fixed()\n\n\n\n\n\n\n\n\n18.9.10 Passo 7: Confronto con prcomp\n\n\n# Calcolo automatico della PCA\npca_auto &lt;- prcomp(X, scale. = TRUE)\nsummary(pca_auto)\n#&gt; Importance of components:\n#&gt;                          PC1    PC2\n#&gt; Standard deviation     1.348 0.4270\n#&gt; Proportion of Variance 0.909 0.0912\n#&gt; Cumulative Proportion  0.909 1.0000\n\nVerifica: confronta punteggi calcolati a mano con quelli di prcomp\n\nhead(pca_auto$x)  # Punteggi calcolati da prcomp\n#&gt;           PC1     PC2\n#&gt; [1,] -0.05606 -0.9523\n#&gt; [2,]  0.04512 -0.5418\n#&gt; [3,]  1.98607  0.2887\n#&gt; [4,]  0.15352 -0.1844\n#&gt; [5,] -0.17413  0.2344\n#&gt; [6,]  2.12408  0.3930\n\n\nhead(pc_scores)   # Punteggi calcolati a mano\n#&gt;           PC1     PC2\n#&gt; [1,] -0.05606  0.9523\n#&gt; [2,]  0.04512  0.5418\n#&gt; [3,]  1.98607 -0.2887\n#&gt; [4,]  0.15352  0.1844\n#&gt; [5,] -0.17413 -0.2344\n#&gt; [6,]  2.12408 -0.3930\n\n\n18.9.11 Passo 8: Biplot\nIl biplot permette di visualizzare contemporaneamente:\n\nle osservazioni (rappresentate da numeri), proiettate nello spazio delle componenti principali;\nle variabili originali (x1, x2), rappresentate come frecce rosse, che indicano come ciascuna variabile contribuisce alla definizione delle componenti.\n\n\nbiplot(pca_auto, scale = 0,\n       main = \"Biplot delle Componenti Principali\",\n       xlab = \"PC1\", ylab = \"PC2\")\n\n\n\n\n\n\n\nDistribuzione delle osservazioni\n\nI numeri neri rappresentano le 100 osservazioni.\nLe osservazioni si distribuiscono principalmente lungo la direzione della prima componente principale (PC1), che si estende orizzontalmente.\nC’è relativamente poca variabilità lungo la seconda componente (PC2), che è verticale. Questo conferma che quasi tutta la varianza è catturata da PC1, come osservato nei passaggi precedenti (Scree Plot, autovalori).\n\nFrecce delle variabili originali\nNel biplot vediamo due frecce:\n\nla freccia x1 punta verso l’alto a destra;\nla freccia x2 punta verso il basso a destra;\nentrambe le frecce sono allineate in parte con l’asse orizzontale (PC1), ma puntano in direzioni opposte lungo PC2.\n\nInterpretazione geometrica:\n\n\nEntrambe le variabili contribuiscono positivamente a PC1, perché le componenti orizzontali delle frecce sono entrambe &gt; 0.\nLa componente verticale di x1 è positiva, quella di x2 è negativa, quindi le due variabili sono positivamente correlate in generale, ma divergono leggermente lungo PC2.\n\nQuesto pattern è coerente con i coefficienti degli autovettori:\npca_auto$rotation\nEs.:\n           PC1      PC2\nx1       0.71     0.71\nx2       0.71    -0.71\nCosa significa?\n\nLa PC1 è la somma bilanciata di x1 e x2, e rappresenta la dimensione comune tra le due variabili.\nLa PC2 è la loro differenza: rappresenta una direzione lungo cui x1 e x2 si muovono in modo opposto. Ma in questo caso, la varianza lungo PC2 è molto piccola → questo secondo asse è poco informativo.\n\nLunghezza delle frecce:\n\nle frecce hanno lunghezza simile → entrambe le variabili sono ben rappresentate dallo spazio PC1–PC2;\nin un biplot, la lunghezza di una freccia indica quanto bene quella variabile è spiegata dalle componenti principali.\n\nConclusioni sull’interpretazione del biplot.\n\nIl biplot mostra che la struttura dei dati può essere riassunta da una sola dimensione latente (PC1).\nLe due variabili x1 e x2 sono entrambe fortemente associate a PC1, e debolmente differenziate da PC2.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Analisi delle componenti principali</span>"
    ]
  },
  {
    "objectID": "chapters/pca/02_pca.html#passo-9-interpretazione-del-biplot",
    "href": "chapters/pca/02_pca.html#passo-9-interpretazione-del-biplot",
    "title": "26  Analisi delle componenti principali",
    "section": "\n26.10 🧭 Passo 9: Interpretazione del Biplot",
    "text": "26.10 🧭 Passo 9: Interpretazione del Biplot\nIl biplot consente di visualizzare simultaneamente:\n\nle osservazioni proiettate nello spazio delle componenti principali (come punti);\nle variabili originali (come frecce), che mostrano il contributo di ciascuna variabile alle componenti principali.\n\nNel nostro caso, abbiamo due variabili originali: x1 e x2.\n\nbiplot(pca_auto, scale = 0,\n       main = \"Biplot delle Componenti Principali\",\n       xlab = \"PC1\", ylab = \"PC2\")\n\n\n\n\n\n\n\n\n26.10.1 🧠 Come leggere il biplot\n\n\nI punti blu rappresentano le osservazioni (le righe del dataset), posizionate nel nuovo spazio definito dalle componenti principali.\n\nLe frecce rosse indicano la direzione e la forza della relazione tra le variabili originali (x1 e x2) e le componenti principali (PC1, PC2).\n\n26.10.2 🔍 Cosa osserviamo nel biplot dei nostri dati\n\n\nDistribuzione delle osservazioni:\n\nI punti si distribuiscono lungo una direzione diagonale, che coincide con la prima componente principale (PC1).\nQuesto riflette il fatto che la maggior parte della varianza nei dati è spiegata da PC1 (come già osservato negli autovalori).\n\n\n\nFrecce delle variabili originali:\n\nLe frecce di x1 e x2 sono orientate nella stessa direzione, formando un angolo molto stretto tra loro.\nQuesto indica che x1 e x2 sono fortemente correlate positivamente (infatti abbiamo generato i dati con una correlazione artificiale di circa 0.8).\n\n\n\nAngolo tra le frecce\nInterpretazione\n\n\n\n≈ 0°\nCorrelazione positiva forte\n\n\n≈ 90°\nNessuna correlazione\n\n\n≈ 180°\nCorrelazione negativa forte\n\n\n\n\n\nLunghezza delle frecce:\n\nLe frecce sono abbastanza lunghe, suggerendo che entrambe le variabili sono ben rappresentate dallo spazio delle prime due componenti principali.\nIn generale, più una freccia è lunga, più la varianza di quella variabile è spiegata dalla combinazione di PC1 e PC2.\n\n\n\nComposizione delle componenti: Possiamo visualizzare i coefficienti delle componenti principali tramite:\n\npca_auto$rotation\n#&gt;       PC1     PC2\n#&gt; x1 0.7071  0.7071\n#&gt; x2 0.7071 -0.7071\n\nIl risultato (approssimativo) sarà:\n         PC1     PC2\nx1     0.707   0.707\nx2     0.707  -0.707\n\nLa PC1 è una somma bilanciata di x1 e x2: rappresenta la tendenza comune delle due variabili.\nLa PC2 è una differenza tra le due variabili: rappresenta ciò che le distingue, che in questo caso è poco (quindi PC2 spiega poca varianza).\n\n\n\n26.10.3 ✅ Conclusioni sull’interpretazione del biplot\n\n\nIl biplot conferma visivamente ciò che abbiamo già osservato nei passaggi precedenti:\n\nLe variabili sono positivamente correlate.\nLa prima componente spiega quasi tutta la varianza.\n\n\nLe due variabili possono essere riassunte in un’unica dimensione latente (PC1), senza perdita significativa di informazione.\n\nIl biplot è quindi uno strumento utile per:\n\n\nVisualizzare la struttura dei dati;\n\nEsplorare correlazioni tra variabili;\n\nIndividuare pattern o gruppi tra le osservazioni.\n\n\n\n26.10.4 ✅ Riflessioni conclusive\nAbbiamo completato un’analisi PCA manuale e automatica, verificando ogni passaggio attraverso:\n\nalgebra lineare (autovalori/autovettori);\nvisualizzazione geometrica (ellisse, autovettori);\nscree plot;\nproiezione dei dati;\nbiplot.\n\nQuesta struttura permette di collegare i concetti teorici alla loro implementazione pratica, fornendo agli studenti una comprensione più profonda della PCA.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Analisi delle componenti principali</span>"
    ]
  },
  {
    "objectID": "chapters/pca/02_pca.html#suggerimenti-per-gli-studenti",
    "href": "chapters/pca/02_pca.html#suggerimenti-per-gli-studenti",
    "title": "18  Analisi delle componenti principali",
    "section": "\n18.11 ✍️ Suggerimenti per gli Studenti",
    "text": "18.11 ✍️ Suggerimenti per gli Studenti\n\nQuando standardizzare? Sempre, se le variabili sono su scale diverse (es. punteggi da 0 a 10, da 1 a 100);\nQuando usare la PCA? Quando l’obiettivo è descrittivo, esplorare la struttura latente o semplificare l’analisi;\nQuando non usarla? Se le variabili non sono correlate: la PCA non sarà utile;\nCome interpretare le componenti? Serve analizzare i coefficienti degli autovettori per capire il contributo delle variabili originali a ciascuna componente.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Analisi delle componenti principali</span>"
    ]
  },
  {
    "objectID": "chapters/pca/02_pca.html#riflessioni-conclusive",
    "href": "chapters/pca/02_pca.html#riflessioni-conclusive",
    "title": "18  Analisi delle componenti principali",
    "section": "\n18.10 Riflessioni Conclusive",
    "text": "18.10 Riflessioni Conclusive\nAbbiamo completato un’analisi PCA manuale e automatica, verificando ogni passaggio attraverso:\n\nalgebra lineare (autovalori/autovettori);\nvisualizzazione geometrica (ellisse, autovettori);\nscree plot;\nproiezione dei dati;\nbiplot.\n\nQuesta struttura permette di collegare i concetti teorici alla loro implementazione pratica, fornendo agli studenti una comprensione più profonda della PCA.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Analisi delle componenti principali</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/01_val_matrici.html#introduzione",
    "href": "chapters/extraction/01_val_matrici.html#introduzione",
    "title": "26  Valutazione della matrice di correlazione",
    "section": "",
    "text": "ispezionare visivamente la matrice di correlazione,\ncalcolare il test di sfericità di Bartlett,\ncalcolare l’indice di adeguatezza campionaria KMO (Kaiser-Meyer-Olkin).",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Valutazione della matrice di correlazione</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/01_val_matrici.html#test-di-sfericità-di-bartlett",
    "href": "chapters/extraction/01_val_matrici.html#test-di-sfericità-di-bartlett",
    "title": "26  Valutazione della matrice di correlazione",
    "section": "\n26.3 Test di Sfericità di Bartlett",
    "text": "26.3 Test di Sfericità di Bartlett\nIl test di sfericità di Bartlett verifica se la matrice di correlazione differisce significativamente da una matrice identità (cioè una matrice in cui tutte le variabili sono incorrelate).\nIpotesi nulla: le variabili non sono correlate tra loro.Ipotesi alternativa: esistono correlazioni significative tra le variabili.\nLa formula del test è:\n\\[\n\\chi^2 = -\\left[n - 1 - \\frac{1}{6}(2p + 5)\\right] \\ln |\\boldsymbol{R}|,\n\\]\ndove:\n\n\n\\(n\\) = numerosità campionaria,\n\n\\(p\\) = numero di variabili,\n\n\\(|\\boldsymbol{R}|\\) = determinante della matrice di correlazione.\n\nIl test restituisce una statistica \\(\\chi^2\\) con \\(p(p - 1)/2\\) gradi di libertà.\n\ncor_mat &lt;- cor(hz)\n\nout = cortest.bartlett(R = cor_mat, n = 301)\nprint(out)\n#&gt; $chisq\n#&gt; [1] 904.1\n#&gt; \n#&gt; $p.value\n#&gt; [1] 1.912e-166\n#&gt; \n#&gt; $df\n#&gt; [1] 36\n\nRisultato: la statistica è altamente significativa → possiamo rifiutare l’ipotesi nulla. Le variabili sono sufficientemente correlate per proseguire con l’analisi fattoriale.\n\n📌 Nota: Il test di Bartlett è molto sensibile alla numerosità campionaria: con campioni ampi, anche correlazioni deboli possono risultare statisticamente “significative”. È quindi opportuno integrare il test con altri indici, come il KMO.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Valutazione della matrice di correlazione</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/01_val_matrici.html#indice-kmo-kaiser-meyer-olkin",
    "href": "chapters/extraction/01_val_matrici.html#indice-kmo-kaiser-meyer-olkin",
    "title": "26  Valutazione della matrice di correlazione",
    "section": "\n26.4 Indice KMO (Kaiser-Meyer-Olkin)",
    "text": "26.4 Indice KMO (Kaiser-Meyer-Olkin)\nL’indice KMO misura quanto le correlazioni osservate siano spiegabili da fattori latenti comuni, piuttosto che da correlazioni parziali tra le variabili (che rappresentano associazioni “spuriate”).\nLa formula è:\n\\[\n\\text{KMO} = \\frac{\\sum_i \\sum_j r^2_{ij}}{\\sum_i \\sum_j r^2_{ij} + \\sum_i \\sum_j p^2_{ij}},\n\\]\ndove \\(r_{ij}\\) sono le correlazioni osservate, e \\(p_{ij}\\) le correlazioni parziali.\nValori possibili:\n\n\n0.90–1.00: eccellente (meravigliosa)\n\n0.80–0.89: molto buona (meritevole)\n\n0.70–0.79: buona (media)\n\n0.60–0.69: discreta (mediocre)\n\n0.50–0.59: scarsa (miserabile)\n\n&lt; 0.50: inadeguata (inaccettabile)\n\n\nout = KMO(cor_mat)\nprint(out)\n#&gt; Kaiser-Meyer-Olkin factor adequacy\n#&gt; Call: KMO(r = cor_mat)\n#&gt; Overall MSA =  0.75\n#&gt; MSA for each item = \n#&gt;   x1   x2   x3   x4   x5   x6   x7   x8   x9 \n#&gt; 0.81 0.78 0.73 0.76 0.74 0.81 0.59 0.68 0.79\n\nNel nostro caso, il valore KMO è attorno a 0.70, quindi l’adeguatezza è buona, ma non eccellente. Possiamo proseguire con l’analisi, pur restando consapevoli che la qualità dei dati potrebbe essere migliorata (es. con la revisione degli item).",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Valutazione della matrice di correlazione</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html#estrazione-dei-fattori-panoramica-dei-metodi",
    "href": "chapters/extraction/02_estrazione.html#estrazione-dei-fattori-panoramica-dei-metodi",
    "title": "27  L’estrazione dei fattori",
    "section": "\n27.2 Estrazione dei Fattori: Panoramica dei Metodi",
    "text": "27.2 Estrazione dei Fattori: Panoramica dei Metodi\nL’estrazione dei fattori consiste nel stimare i parametri del modello (soprattutto \\(\\boldsymbol{\\Lambda}\\)), sulla base della matrice di correlazioni o covarianze. I diversi metodi si distinguono per:\n\nle assunzioni statistiche (es. normalità dei dati);\nil tipo di informazione utilizzata (es. varianza totale o varianza comune);\nla possibilità di testare l’adattamento del modello ai dati.\n\n\n\n\n\n\n\n\n\nMetodo\nTiene conto della specificità?\nRichiede normalità?\nPermette test di bontà del modello?\n\n\n\nComponenti principali (PCA)\n❌ No\n❌ No\n❌ No\n\n\nFattori principali\n✅ Sì\n❌ No\n❌ No\n\n\nFattori principali iterato\n✅ Sì (con aggiornamenti)\n❌ No\n❌ No\n\n\nMassima verosimiglianza (ML)\n✅ Sì\n✅ Sì\n✅ Sì\n\n\n\nVediamo ora in dettaglio ciascun metodo.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html#metodo-di-massima-verosimiglianza-ml",
    "href": "chapters/extraction/02_estrazione.html#metodo-di-massima-verosimiglianza-ml",
    "title": "27  L’estrazione dei fattori",
    "section": "\n27.8 Metodo di Massima Verosimiglianza (ML)",
    "text": "27.8 Metodo di Massima Verosimiglianza (ML)\nQuesto è il metodo più coerente con l’analisi fattoriale teorica: assume normalità multivariata e consente test formali di adattamento.\nEsecuzione con factanal().\n\nml &lt;- factanal(\n  covmat = R,\n  factors = 2,\n  rotation = \"none\",\n  n.obs = 225  # necessario per il test chi-quadro\n)\nml\n#&gt; \n#&gt; Call:\n#&gt; factanal(factors = 2, covmat = R, n.obs = 225, rotation = \"none\")\n#&gt; \n#&gt; Uniquenesses:\n#&gt;     K     I     H     L     J \n#&gt; 0.005 0.268 0.055 0.008 0.005 \n#&gt; \n#&gt; Loadings:\n#&gt;   Factor1 Factor2\n#&gt; K  0.955  -0.289 \n#&gt; I  0.528   0.673 \n#&gt; H  0.720  -0.653 \n#&gt; L  0.954  -0.287 \n#&gt; J  0.764   0.642 \n#&gt; \n#&gt;                Factor1 Factor2\n#&gt; SS loadings      3.203   1.457\n#&gt; Proportion Var   0.641   0.291\n#&gt; Cumulative Var   0.641   0.932\n#&gt; \n#&gt; Test of the hypothesis that 2 factors are sufficient.\n#&gt; The chi square statistic is 648.1 on 1 degree of freedom.\n#&gt; The p-value is 5.81e-143\n\n\n\nloadings: saturazioni fattoriali\n\n\nuniquenesses: varianza specifica di ciascuna variabile\n\ntest statistic: test chi-quadro per valutare se i fattori estratti spiegano sufficientemente la correlazione tra le variabili\n\nIl p-value indica se il modello a 2 fattori è adeguato:\n\np alto → il modello spiega bene i dati (non c’è differenza significativa con la matrice osservata).\n\np basso → il modello è insufficiente (i residui sono troppo grandi).\n\nSe vogliamo usare lavaan dobbiamo introdurre una rotazione obliqua:\n\nml2 &lt;- factanal(\n  covmat = R,\n  factors = 2,\n  rotation = \"oblimin\",\n  n.obs = 225  \n)\nml2\n#&gt; \n#&gt; Call:\n#&gt; factanal(factors = 2, covmat = R, n.obs = 225, rotation = \"oblimin\")\n#&gt; \n#&gt; Uniquenesses:\n#&gt;     K     I     H     L     J \n#&gt; 0.005 0.268 0.055 0.008 0.005 \n#&gt; \n#&gt; Loadings:\n#&gt;   Factor1 Factor2\n#&gt; K  0.927   0.192 \n#&gt; I          0.875 \n#&gt; H  1.011  -0.257 \n#&gt; L  0.925   0.193 \n#&gt; J  0.116   0.959 \n#&gt; \n#&gt;                Factor1 Factor2\n#&gt; SS loadings      2.759   1.827\n#&gt; Proportion Var   0.552   0.365\n#&gt; Cumulative Var   0.552   0.917\n#&gt; \n#&gt; Factor Correlations:\n#&gt;         Factor1 Factor2\n#&gt; Factor1   1.000  -0.277\n#&gt; Factor2  -0.277   1.000\n#&gt; \n#&gt; Test of the hypothesis that 2 factors are sufficient.\n#&gt; The chi square statistic is 648.1 on 1 degree of freedom.\n#&gt; The p-value is 5.81e-143\n\nReplichiamo ora i risultati con lavaan:\n\nfit &lt;- efa(sample.cov = R, \n           sample.nobs = 225,\n           nfactors = 1:2,\n           rotation = \"geomin\",\n           rotation.args = list(geomin.epsilon = 0.01, rstarts = 1))\n\nsummary(fit, nd = 3L, cutoff = 0.2, dot.cutoff = 0.05)\n#&gt; This is lavaan 0.6-19 -- running exploratory factor analysis\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Rotation method                       GEOMIN OBLIQUE\n#&gt;   Geomin epsilon                                  0.01\n#&gt;   Rotation algorithm (rstarts)                 GPA (1)\n#&gt;   Standardized metric                             TRUE\n#&gt;   Row weights                                     None\n#&gt; \n#&gt;   Number of observations                           225\n#&gt; \n#&gt; Overview models:\n#&gt;                 aic  bic sabic  chisq df pvalue   cfi rmsea\n#&gt;   nfactors = 1 1734 1768  1737 1227.0  5      0 0.546 1.042\n#&gt;   nfactors = 2 1122 1170  1126  607.1  1      0 0.775 1.641\n#&gt; \n#&gt; Eigenvalues correlation matrix:\n#&gt; \n#&gt;        ev1        ev2        ev3        ev4        ev5 \n#&gt;   3.263377   1.538382   0.167969   0.030030   0.000242 \n#&gt; \n#&gt; Number of factors:  1 \n#&gt; \n#&gt; Standardized loadings: (* = significant at 1% level)\n#&gt; \n#&gt;       f1       unique.var   communalities\n#&gt; K  1.000            0.000           1.000\n#&gt; I  0.296*           0.912           0.088\n#&gt; H  0.881*           0.224           0.776\n#&gt; L  0.995*           0.010           0.990\n#&gt; J  0.545*           0.703           0.297\n#&gt; \n#&gt;                            f1\n#&gt; Sum of squared loadings 3.151\n#&gt; Proportion of total     1.000\n#&gt; Proportion var          0.630\n#&gt; Cumulative var          0.630\n#&gt; \n#&gt; Number of factors:  2 \n#&gt; \n#&gt; Standardized loadings: (* = significant at 1% level)\n#&gt; \n#&gt;       f1      f2       unique.var   communalities\n#&gt; K  0.967*      .*           0.000           1.000\n#&gt; I      .*  0.876*           0.263           0.737\n#&gt; H  1.031* -0.360*           0.049           0.951\n#&gt; L  0.961*      .*           0.010           0.990\n#&gt; J      .*  0.933*           0.000           1.000\n#&gt; \n#&gt;                               f1    f2 total\n#&gt; Sum of sq (obliq) loadings 2.922 1.755 4.678\n#&gt; Proportion of total        0.625 0.375 1.000\n#&gt; Proportion var             0.584 0.351 0.936\n#&gt; Cumulative var             0.584 0.936 0.936\n#&gt; \n#&gt; Factor correlations: (* = significant at 1% level)\n#&gt; \n#&gt;        f1      f2 \n#&gt; f1  1.000         \n#&gt; f2  0.325   1.000\nfitMeasures(fit, fit.measures = \"all\")\n#&gt;                         nfct=1   nfct=2\n#&gt; npar                     9.000   14.000\n#&gt; fmin                     2.727    1.349\n#&gt; chisq                 1226.984  607.148\n#&gt; df                       5.000    1.000\n#&gt; pvalue                   0.000    0.000\n#&gt; baseline.chisq        2700.426 2700.426\n#&gt; baseline.df             10.000   10.000\n#&gt; baseline.pvalue          0.000    0.000\n#&gt; cfi                      0.546    0.775\n#&gt; tli                      0.092   -1.253\n#&gt; nnfi                     0.092   -1.253\n#&gt; rfi                      0.091    1.000\n#&gt; nfi                      0.546    0.775\n#&gt; pnfi                     0.273    0.078\n#&gt; ifi                      0.547    0.775\n#&gt; rni                      0.546    0.775\n#&gt; logl                  -857.079 -547.161\n#&gt; unrestricted.logl     -243.587 -243.587\n#&gt; aic                   1734.159 1122.323\n#&gt; bic                   1768.320 1170.148\n#&gt; ntotal                 225.000  225.000\n#&gt; bic2                  1736.628 1125.779\n#&gt; rmsea                    1.042    1.641\n#&gt; rmsea.ci.lower           0.994    1.533\n#&gt; rmsea.ci.upper           1.092    1.752\n#&gt; rmsea.ci.level           0.900    0.900\n#&gt; rmsea.pvalue             0.000    0.000\n#&gt; rmsea.close.h0           0.050    0.050\n#&gt; rmsea.notclose.pvalue    1.000    1.000\n#&gt; rmsea.notclose.h0        0.080    0.080\n#&gt; rmr                      0.209    0.016\n#&gt; rmr_nomean               0.209    0.016\n#&gt; srmr                     0.210    0.016\n#&gt; srmr_bentler             0.210    0.016\n#&gt; srmr_bentler_nomean      0.210    0.016\n#&gt; crmr                     0.257    0.020\n#&gt; crmr_nomean              0.257    0.020\n#&gt; srmr_mplus               0.210    0.016\n#&gt; srmr_mplus_nomean        0.210    0.016\n#&gt; cn_05                    3.030    2.424\n#&gt; cn_01                    3.766    3.459\n#&gt; gfi                      0.552    0.777\n#&gt; agfi                    -0.345   -2.341\n#&gt; pgfi                     0.184    0.052\n#&gt; mfi                      0.066    0.260\n#&gt; ecvi                     5.533    2.823\n\nIn sintesi:\n\n\n\n\n\n\n\n\n\nMetodo\nObiettivo principale\nInclude specificità?\nTest del modello?\nQuando usarlo\n\n\n\nPCA\nRidurre dimensionalità\n❌ No\n❌ No\nSintesi descrittiva\n\n\nFattori principali\nIsolare la varianza comune\n✅ Sì\n❌ No\nAnalisi esplorativa\n\n\nFattori iterato\nRaffinare le comunalità\n✅ Sì\n❌ No\nSoluzioni più stabili\n\n\nML\nTestare modello fattoriale\n✅ Sì\n✅ Sì\nVerifica ipotesi psicologiche\n\n\n\n🎓 Suggerimento:\n\nse lo scopo è identificare costrutti teorici (es. “l’autostima ha due dimensioni?”), si preferisce il metodo ML.\n\nse invece lo scopo è solo quello di riassumendo dati (es. da un questionario), anche la PCA può andare bene.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html#esempio-pratico-in-r",
    "href": "chapters/extraction/02_estrazione.html#esempio-pratico-in-r",
    "title": "27  L’estrazione dei fattori",
    "section": "\n27.7 Esempio pratico in R",
    "text": "27.7 Esempio pratico in R\nSupponiamo che una ragazza abbia valutato 7 persone su 5 tratti personali: Kind, Intelligent, Happy, Likeable, Just (Rencher, 2002). La matrice di correlazione tra i tratti è la seguente:\n\nR &lt;- matrix(c(\n  1.000, .296, .881, .995, .545,\n  .296, 1.000, -.022, .326, .837,\n  .881, -.022, 1.000, .867, .130,\n  .995, .326, .867, 1.000, .544,\n  .545, .837, .130, .544, 1.000\n),\nncol = 5, byrow = TRUE,\ndimnames = list(c(\"K\", \"I\", \"H\", \"L\", \"J\"),\n                c(\"K\", \"I\", \"H\", \"L\", \"J\")))\nR\n#&gt;       K      I      H     L     J\n#&gt; K 1.000  0.296  0.881 0.995 0.545\n#&gt; I 0.296  1.000 -0.022 0.326 0.837\n#&gt; H 0.881 -0.022  1.000 0.867 0.130\n#&gt; L 0.995  0.326  0.867 1.000 0.544\n#&gt; J 0.545  0.837  0.130 0.544 1.000\n\n\n27.7.1 Metodo delle Componenti Principali (PCA)\n\n27.7.1.1 Calcolo degli autovalori e autovettori\n\ne &lt;- eigen(R)\ne\n#&gt; eigen() decomposition\n#&gt; $values\n#&gt; [1] 3.2633766 1.5383821 0.1679693 0.0300298 0.0002422\n#&gt; \n#&gt; $vectors\n#&gt;         [,1]    [,2]     [,3]    [,4]    [,5]\n#&gt; [1,] -0.5367 -0.1860 -0.18992 -0.1248  0.7910\n#&gt; [2,] -0.2875  0.6506  0.68489 -0.1198  0.1034\n#&gt; [3,] -0.4344 -0.4737  0.40695  0.6137 -0.2116\n#&gt; [4,] -0.5374 -0.1693 -0.09533 -0.6294 -0.5266\n#&gt; [5,] -0.3897  0.5377 -0.56583  0.4442 -0.2037\n\n\nGli autovalori indicano la varianza spiegata da ciascuna componente.\nGli autovettori sono i coefficienti delle combinazioni lineari delle variabili.\n\n27.7.1.2 Ricostruzione della matrice R\nVerifichiamo che \\(\\textbf{R} = \\textbf{C}\\textbf{D}\\textbf{C}^{\\mathsf{T}}\\):\n\ne$vectors %*% diag(e$values) %*% t(e$vectors)\n#&gt;       [,1]   [,2]   [,3]  [,4]  [,5]\n#&gt; [1,] 1.000  0.296  0.881 0.995 0.545\n#&gt; [2,] 0.296  1.000 -0.022 0.326 0.837\n#&gt; [3,] 0.881 -0.022  1.000 0.867 0.130\n#&gt; [4,] 0.995  0.326  0.867 1.000 0.544\n#&gt; [5,] 0.545  0.837  0.130 0.544 1.000\n\n\n27.7.1.3 Varianza spiegata dai primi due fattori\n\n(e$values[1] + e$values[2]) / 5\n#&gt; [1] 0.9604\n\n\n🔎 Interpretazione: i primi due autovalori spiegano circa il 96% della varianza → possiamo ridurre da 5 a 2 dimensioni senza perdere molte informazioni.\n\n\n27.7.1.4 Calcolo delle saturazioni fattoriali (̂\\(\\lambda\\))\nApprossimiamo la matrice con rango 2:\n\nL &lt;- cbind(\n  e$vectors[, 1] * sqrt(e$values[1]),\n  e$vectors[, 2] * sqrt(e$values[2])\n)\nround(L, 3)\n#&gt;        [,1]   [,2]\n#&gt; [1,] -0.970 -0.231\n#&gt; [2,] -0.519  0.807\n#&gt; [3,] -0.785 -0.588\n#&gt; [4,] -0.971 -0.210\n#&gt; [5,] -0.704  0.667\n\nQuesta matrice rappresenta le saturazioni fattoriali: i valori mostrano quanto ogni variabile satura su ciascuna componente.\n\n27.7.1.5 Matrice riprodotta e matrice residua\n\nR_hat &lt;- round(L %*% t(L), 3)  # matrice riprodotta\nR - R_hat                      # residui\n#&gt;        K      I      H      L      J\n#&gt; K  0.007 -0.021 -0.015  0.005  0.016\n#&gt; I -0.021  0.079  0.045 -0.009 -0.067\n#&gt; H -0.015  0.045  0.039 -0.018 -0.030\n#&gt; L  0.005 -0.009 -0.018  0.013  0.001\n#&gt; J  0.016 -0.067 -0.030  0.001  0.060\n\nSe i residui sono piccoli, la soluzione è soddisfacente.\n\n27.7.2 Metodo dei Fattori Principali\nA differenza della PCA, il metodo dei fattori principali tiene conto della varianza specifica ed estrae solo la varianza comune tra le variabili.\n\n27.7.2.1 Stima iniziale delle comunalità\nNel caso in cui si lavori con la matrice di correlazione, la comunalità di ogni variabile può essere stimata come:\n\nR_inv &lt;- solve(R)             # inversa di R\ndiag(1 - 1 / diag(R_inv))     # stima delle comunalità iniziali\n#&gt;        [,1]   [,2]  [,3]   [,4]   [,5]\n#&gt; [1,] 0.9996 0.0000 0.000 0.0000 0.0000\n#&gt; [2,] 0.0000 0.9791 0.000 0.0000 0.0000\n#&gt; [3,] 0.0000 0.0000 0.995 0.0000 0.0000\n#&gt; [4,] 0.0000 0.0000 0.000 0.9991 0.0000\n#&gt; [5,] 0.0000 0.0000 0.000 0.0000 0.9944\n\nOppure, in modo più semplice, si può usare il massimo valore assoluto per riga (approssimazione):\n\nh.hat &lt;- apply(abs(R), 1, max)\nh.hat\n#&gt; K I H L J \n#&gt; 1 1 1 1 1\n\nSostituiamo queste stime sulla diagonale di R per ottenere la matrice ridotta:\n\nR1 &lt;- R\ndiag(R1) &lt;- h.hat\nR1\n#&gt;       K      I      H     L     J\n#&gt; K 1.000  0.296  0.881 0.995 0.545\n#&gt; I 0.296  1.000 -0.022 0.326 0.837\n#&gt; H 0.881 -0.022  1.000 0.867 0.130\n#&gt; L 0.995  0.326  0.867 1.000 0.544\n#&gt; J 0.545  0.837  0.130 0.544 1.000\n\n\n27.7.2.2 Autovalori e saturazioni fattoriali\n\nee &lt;- eigen(R1)\nround(ee$values, 3)  # autovalori\n#&gt; [1] 3.263 1.538 0.168 0.030 0.000\n\n# saturazioni fattoriali per i primi due fattori\nL &lt;- ee$vectors[, 1:2] %*% sqrt(diag(ee$values[1:2]))\nround(L, 3)\n#&gt;        [,1]   [,2]\n#&gt; [1,] -0.970 -0.231\n#&gt; [2,] -0.519  0.807\n#&gt; [3,] -0.785 -0.588\n#&gt; [4,] -0.971 -0.210\n#&gt; [5,] -0.704  0.667\n\nQueste sono le saturazioni fattoriali stimate con il metodo dei fattori principali.\n\n27.7.3 Metodo dei Fattori Principali Iterato\nQuesto metodo migliora iterativamente le stime delle comunalità fino alla convergenza.\n\n27.7.3.1 Esecuzione in R con il pacchetto psych\n\n\npa &lt;- psych::fa(R, nfactors = 2, rotate = \"none\", fm = \"pa\")\npa\n#&gt; Factor Analysis using method =  pa\n#&gt; Call: psych::fa(r = R, nfactors = 2, rotate = \"none\", fm = \"pa\")\n#&gt; Standardized loadings (pattern matrix) based upon correlation matrix\n#&gt;    PA1   PA2   h2     u2 com\n#&gt; K 0.98 -0.21 1.01 -0.008 1.1\n#&gt; I 0.48  0.74 0.77  0.230 1.7\n#&gt; H 0.78 -0.56 0.92  0.085 1.8\n#&gt; L 0.98 -0.19 0.99  0.010 1.1\n#&gt; J 0.69  0.69 0.95  0.049 2.0\n#&gt; \n#&gt;                        PA1  PA2\n#&gt; SS loadings           3.22 1.41\n#&gt; Proportion Var        0.64 0.28\n#&gt; Cumulative Var        0.64 0.93\n#&gt; Proportion Explained  0.70 0.30\n#&gt; Cumulative Proportion 0.70 1.00\n#&gt; \n#&gt; Mean item complexity =  1.5\n#&gt; Test of the hypothesis that 2 factors are sufficient.\n#&gt; \n#&gt; df null model =  10  with the objective function =  12\n#&gt; df of  the model are 1  and the objective function was  5.6 \n#&gt; \n#&gt; The root mean square of the residuals (RMSR) is  0.01 \n#&gt; The df corrected root mean square of the residuals is  0.04 \n#&gt; \n#&gt; Fit based upon off diagonal values = 1\n\n\n\nfm = \"pa\" specifica il metodo dei fattori principali.\nIl risultato mostra le saturazioni, le unicità, e l’adeguatezza del modello.\n\n\n🔍 Se le unicità sono negative o &gt; 1, si è verificato un caso di Heywood, cioè una soluzione impropria.\n\n\n27.7.4 Metodo di Massima Verosimiglianza\nQuesto metodo assume che le variabili seguano una distribuzione normale multivariata. Consente di testare formalmente l’adattamento del modello.\n\n27.7.4.1 Esecuzione in R con factanal()\n\n\nfactanal(\n  covmat = R,        # matrice di correlazione\n  factors = 2,       # numero di fattori\n  rotation = \"none\", # nessuna rotazione\n  n.obs = 225        # numero di osservazioni\n)\n#&gt; \n#&gt; Call:\n#&gt; factanal(factors = 2, covmat = R, n.obs = 225, rotation = \"none\")\n#&gt; \n#&gt; Uniquenesses:\n#&gt;     K     I     H     L     J \n#&gt; 0.005 0.268 0.055 0.008 0.005 \n#&gt; \n#&gt; Loadings:\n#&gt;   Factor1 Factor2\n#&gt; K  0.955  -0.289 \n#&gt; I  0.528   0.673 \n#&gt; H  0.720  -0.653 \n#&gt; L  0.954  -0.287 \n#&gt; J  0.764   0.642 \n#&gt; \n#&gt;                Factor1 Factor2\n#&gt; SS loadings      3.203   1.457\n#&gt; Proportion Var   0.641   0.291\n#&gt; Cumulative Var   0.641   0.932\n#&gt; \n#&gt; Test of the hypothesis that 2 factors are sufficient.\n#&gt; The chi square statistic is 648.1 on 1 degree of freedom.\n#&gt; The p-value is 5.81e-143\n\n\n📌 Nota: factanal() richiede il numero di osservazioni per poter eseguire il test chi-quadro di bontà dell’adattamento.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html#riflessioni-conclusive-1",
    "href": "chapters/extraction/02_estrazione.html#riflessioni-conclusive-1",
    "title": "27  L’estrazione dei fattori",
    "section": "\n27.9 Riflessioni conclusive",
    "text": "27.9 Riflessioni conclusive\n\nLa PCA spiega la varianza totale, ma non distingue tra varianza comune e specifica.\nIl metodo dei fattori principali stima solo la varianza comune, ma le comunalità sono inizialmente stimate.\nIl metodo iterato migliora le stime di comunalità in modo automatico e produce risultati più stabili.\nIl metodo ML è il più rigoroso: permette di testare formalmente l’adattamento del modello, ma richiede normalità e può soffrire di problemi di convergenza.\n\nOgni metodo ha vantaggi e limiti, e la scelta va guidata dalla teoria e dalla qualità dei dati.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html#metodo-delle-componenti-principali-pca",
    "href": "chapters/extraction/02_estrazione.html#metodo-delle-componenti-principali-pca",
    "title": "27  L’estrazione dei fattori",
    "section": "\n27.3 Metodo delle Componenti Principali (PCA)",
    "text": "27.3 Metodo delle Componenti Principali (PCA)\n\n❗ Importante: Sebbene sia molto diffuso, il metodo delle componenti principali (Principal Component Analysis, PCA) non è un vero metodo fattoriale. Non fa distinzione tra varianza comune (quella condivisa tra variabili) e varianza specifica (quella unica di ogni variabile), e non assume l’esistenza di fattori latenti. Per questo motivo, in psicometria, viene usato per riduzione della dimensionalità, non per identificare costrutti teorici.\n\n\n27.3.1 Obiettivo\nLa PCA costruisce un numero ridotto di componenti principali:\n\nsono combinazioni lineari delle variabili originali;\nsono ortogonali (cioè non correlate tra loro);\nspiegano progressivamente la massima varianza possibile nei dati.\n\n27.3.2 Fondamento teorico: il teorema spettrale\nLa PCA si basa sul teorema spettrale, che dice che ogni matrice simmetrica (come la matrice di correlazione \\(\\mathbf{R}\\)) può essere scomposta come:\n\\[\n\\mathbf{R} = \\mathbf{C} \\mathbf{D} \\mathbf{C}^{\\mathsf{T}},\n\\]\ndove:\n\n\n\\(\\mathbf{C}\\) è la matrice i cui vettori colonna sono gli autovettori (direzioni principali) di \\(\\mathbf{R}\\);\n\n\\(\\mathbf{D}\\) è una matrice diagonale con gli autovalori (quantità di varianza spiegata da ciascuna direzione);\n\n\\(\\mathbf{C}^{\\mathsf{T}}\\) è la trasposta di \\(\\mathbf{C}\\).\n\nQuesta è la scomposizione spettrale della matrice \\(\\mathbf{R}\\).\n\n27.3.3 Costruzione delle saturazioni (carichi)\nVogliamo una matrice \\(\\hat{\\boldsymbol{\\Lambda}}\\) che approssimi la matrice \\(\\mathbf{R}\\):\n\\[\n\\mathbf{R} \\approx \\hat{\\boldsymbol{\\Lambda}} \\hat{\\boldsymbol{\\Lambda}}^\\mathsf{T}.\n\\]\nPoiché \\(\\mathbf{D}\\) è diagonale, possiamo scriverla come:\n\\[\n\\mathbf{D} = \\mathbf{D}^{1/2} \\cdot \\mathbf{D}^{1/2},\n\\]\ndove \\(\\mathbf{D}^{1/2}\\) ha sulla diagonale le radici quadrate degli autovalori. Allora:\n\\[\n\\mathbf{R} = \\mathbf{C} \\mathbf{D}^{1/2} \\cdot \\mathbf{D}^{1/2} \\mathbf{C}^{\\mathsf{T}}.\n\\]\nDefiniamo:\n\\[\n\\hat{\\boldsymbol{\\Lambda}} = \\mathbf{C} \\mathbf{D}^{1/2},\n\\]\ne otteniamo:\n\\[\n\\hat{\\boldsymbol{\\Lambda}} \\hat{\\boldsymbol{\\Lambda}}^\\mathsf{T} = \\mathbf{R}.\n\\]\n\n💡 Le saturazioni si ottengono moltiplicando ogni autovettore per la radice quadrata dell’autovalore corrispondente. Questo consente di ricostruire esattamente la matrice \\(\\mathbf{R}\\).\n\nOgni elemento \\(l_{ij}\\) di \\(\\hat{\\boldsymbol{\\Lambda}}\\) indica quanto la variabile \\(i\\) contribuisce alla componente \\(j\\).\nQuando si selezionano solo i primi \\(k\\) autovalori e autovettori (cioè quelli che spiegano più varianza), si ottiene una rappresentazione semplificata dei dati, utile per la riduzione della dimensionalità.\n\n27.3.4 Interpretazione\n\nGli autovalori indicano quanta varianza è spiegata da ciascuna componente.\nLe componenti principali sono nuove variabili non osservate, che sintetizzano l’informazione contenuta nelle variabili originali.\n\n27.3.5 Limiti della PCA come analisi fattoriale\n\nNon separa varianza comune da varianza specifica.\nNon assume fattori latenti.\nNon consente di valutare l’adattamento del modello ai dati.\n\n27.3.6 Quando usarla\nLa PCA è utile quando:\n\nsi vuole ridurre il numero di variabili mantenendo la massima varianza;\nsi desidera costruire indici sintetici (es. punteggi compositi);\nsi vuole esplorare la struttura dei dati in modo preliminare.\n\nNon è invece adatta quando l’obiettivo è identificare costrutti latenti teorici.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html#metodo-della-massima-verosimiglianza-ml",
    "href": "chapters/extraction/02_estrazione.html#metodo-della-massima-verosimiglianza-ml",
    "title": "27  L’estrazione dei fattori",
    "section": "\n27.6 Metodo della Massima Verosimiglianza (ML)",
    "text": "27.6 Metodo della Massima Verosimiglianza (ML)\nIl metodo di massima verosimiglianza assume che i dati provengano da una distribuzione normale multivariata. Si basa sulla stima dei parametri che rendono massimamente probabile l’osservazione dei dati dati i parametri.\n\n27.6.1 Caratteristiche\n\nPermette di stimare carichi, unicità e correlazioni tra fattori.\nFornisce un test statistico di bontà dell’adattamento (test chi-quadro).\nPermette confronti tra modelli alternativi (usando AIC, BIC, etc.).\n\n27.6.2 Vantaggi\n\nÈ il più coerente con un’interpretazione psicometrica.\nConsente analisi inferenziali e confronti tra ipotesi.\n\n27.6.3 Limiti\n\nSensibile alle violazioni della normalità.\nRichiede campioni sufficientemente numerosi.\nPuò non convergere in presenza di dati problematici.\n\n\n🎓 Suggerimento:\nSe l’obiettivo è identificare costrutti psicologici latenti, scegliete metodi coerenti con il modello fattoriale, come la massima verosimiglianza.\nSe invece volete solo ridurre le dimensioni dei dati per scopi descrittivi o pratici, allora la PCA può essere sufficiente.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html#esempio-pratico-in-r-confronto-tra-metodi-di-estrazione",
    "href": "chapters/extraction/02_estrazione.html#esempio-pratico-in-r-confronto-tra-metodi-di-estrazione",
    "title": "27  L’estrazione dei fattori",
    "section": "\n27.7 Esempio pratico in R: Confronto tra metodi di estrazione",
    "text": "27.7 Esempio pratico in R: Confronto tra metodi di estrazione\nPer illustrare i principali metodi di estrazione dei fattori, useremo un semplice esempio tratto da Rencher (2010). Una ragazza ha valutato 7 persone su 5 tratti personali:\n\n\nK = Kind (Gentile)\n\n\nI = Intelligent (Intelligente)\n\n\nH = Happy (Felice)\n\n\nL = Likeable (Simpatica)\n\n\nJ = Just (Giusta)\n\nLa matrice di correlazione tra i tratti è la seguente:\n\nR &lt;- matrix(c(\n  1.000, .296, .881, .995, .545,\n  .296, 1.000, -.022, .326, .837,\n  .881, -.022, 1.000, .867, .130,\n  .995, .326, .867, 1.000, .544,\n  .545, .837, .130, .544, 1.000\n),\nncol = 5, byrow = TRUE,\ndimnames = list(c(\"K\", \"I\", \"H\", \"L\", \"J\"),\n                c(\"K\", \"I\", \"H\", \"L\", \"J\")))\nR\n#&gt;       K      I      H     L     J\n#&gt; K 1.000  0.296  0.881 0.995 0.545\n#&gt; I 0.296  1.000 -0.022 0.326 0.837\n#&gt; H 0.881 -0.022  1.000 0.867 0.130\n#&gt; L 0.995  0.326  0.867 1.000 0.544\n#&gt; J 0.545  0.837  0.130 0.544 1.000",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html#metodo-delle-componenti-principali-pca-1",
    "href": "chapters/extraction/02_estrazione.html#metodo-delle-componenti-principali-pca-1",
    "title": "27  L’estrazione dei fattori",
    "section": "\n27.8 Metodo delle Componenti Principali (PCA)",
    "text": "27.8 Metodo delle Componenti Principali (PCA)\n1. Calcolo degli autovalori e autovettori.\n\ne &lt;- eigen(R)\ne$values       # varianza spiegata da ciascuna componente\n#&gt; [1] 3.2633766 1.5383821 0.1679693 0.0300298 0.0002422\ne$vectors      # coefficienti delle combinazioni lineari\n#&gt;         [,1]    [,2]     [,3]    [,4]    [,5]\n#&gt; [1,] -0.5367 -0.1860 -0.18992 -0.1248  0.7910\n#&gt; [2,] -0.2875  0.6506  0.68489 -0.1198  0.1034\n#&gt; [3,] -0.4344 -0.4737  0.40695  0.6137 -0.2116\n#&gt; [4,] -0.5374 -0.1693 -0.09533 -0.6294 -0.5266\n#&gt; [5,] -0.3897  0.5377 -0.56583  0.4442 -0.2037\n\n\nGli autovalori indicano quanta varianza spiega ciascuna componente.\nGli autovettori sono le “direzioni” lungo cui le componenti combinano le variabili.\n\n2. Verifica della decomposizione spettrale.\n\nround(e$vectors %*% diag(e$values) %*% t(e$vectors), 3)\n#&gt;       [,1]   [,2]   [,3]  [,4]  [,5]\n#&gt; [1,] 1.000  0.296  0.881 0.995 0.545\n#&gt; [2,] 0.296  1.000 -0.022 0.326 0.837\n#&gt; [3,] 0.881 -0.022  1.000 0.867 0.130\n#&gt; [4,] 0.995  0.326  0.867 1.000 0.544\n#&gt; [5,] 0.545  0.837  0.130 0.544 1.000\n\nQuesta moltiplicazione ricostruisce la matrice di correlazione originale:\\(\\mathbf{R} = \\mathbf{C} \\mathbf{D} \\mathbf{C}^{\\mathsf{T}}\\)\n3. Varianza spiegata dai primi 2 fattori.\n\nsum(e$values[1:2]) / sum(e$values)\n#&gt; [1] 0.9604\n\nInterpretazione: Se i primi due autovalori spiegano, ad esempio, il 96% della varianza totale, possiamo ridurre da 5 a 2 dimensioni con perdita minima di informazione.\n4. Calcolo delle saturazioni fattoriali (matrice \\(\\hat{\\Lambda}\\)).\n\nL &lt;- cbind(\n  e$vectors[, 1] * sqrt(e$values[1]),\n  e$vectors[, 2] * sqrt(e$values[2])\n)\nround(L, 3)\n#&gt;        [,1]   [,2]\n#&gt; [1,] -0.970 -0.231\n#&gt; [2,] -0.519  0.807\n#&gt; [3,] -0.785 -0.588\n#&gt; [4,] -0.971 -0.210\n#&gt; [5,] -0.704  0.667\n\n\nOgni colonna rappresenta una componente.\nOgni riga rappresenta una variabile.\nGli elementi indicano quanto una variabile satura su una componente.\n\n5. Matrice riprodotta e residui.\n\nR_hat &lt;- round(L %*% t(L), 3)\nresidui &lt;- round(R - R_hat, 3)\nresidui\n#&gt;        K      I      H      L      J\n#&gt; K  0.007 -0.021 -0.015  0.005  0.016\n#&gt; I -0.021  0.079  0.045 -0.009 -0.067\n#&gt; H -0.015  0.045  0.039 -0.018 -0.030\n#&gt; L  0.005 -0.009 -0.018  0.013  0.001\n#&gt; J  0.016 -0.067 -0.030  0.001  0.060\n\nSe i residui (cioè la differenza tra \\(\\mathbf{R}\\) e \\(\\hat{\\Lambda} \\hat{\\Lambda}^\\mathsf{T}\\)) sono piccoli, la soluzione a 2 fattori è soddisfacente.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html#metodo-dei-fattori-principali-1",
    "href": "chapters/extraction/02_estrazione.html#metodo-dei-fattori-principali-1",
    "title": "27  L’estrazione dei fattori",
    "section": "\n27.9 Metodo dei Fattori Principali",
    "text": "27.9 Metodo dei Fattori Principali\nIl metodo dei fattori principali estrae solo la varianza comune, escludendo la varianza specifica.\n1. Stima iniziale delle comunalità.\nUna stima semplice: il valore massimo della correlazione per ogni variabile (approssimazione):\n\nh.hat &lt;- apply(abs(R), 1, max)\nround(h.hat, 3)\n#&gt; K I H L J \n#&gt; 1 1 1 1 1\n\n2. Matrice ridotta.\nSostituiamo le varianze sulla diagonale con le comunalità stimate:\n\nR1 &lt;- R\ndiag(R1) &lt;- h.hat\nR1\n#&gt;       K      I      H     L     J\n#&gt; K 1.000  0.296  0.881 0.995 0.545\n#&gt; I 0.296  1.000 -0.022 0.326 0.837\n#&gt; H 0.881 -0.022  1.000 0.867 0.130\n#&gt; L 0.995  0.326  0.867 1.000 0.544\n#&gt; J 0.545  0.837  0.130 0.544 1.000\n\n3. Decomposizione della matrice ridotta.\n\nee &lt;- eigen(R1)\nround(ee$values, 3)  # autovalori\n#&gt; [1] 3.263 1.538 0.168 0.030 0.000\n\n4. Saturazioni fattoriali.\n\nL &lt;- ee$vectors[, 1:2] %*% sqrt(diag(ee$values[1:2]))\nround(L, 3)\n#&gt;        [,1]   [,2]\n#&gt; [1,] -0.970 -0.231\n#&gt; [2,] -0.519  0.807\n#&gt; [3,] -0.785 -0.588\n#&gt; [4,] -0.971 -0.210\n#&gt; [5,] -0.704  0.667\n\nLe saturazioni qui rappresentano la relazione tra variabili e fattori latenti, tenendo conto solo della varianza comune.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html#metodo-dei-fattori-principali-iterato-1",
    "href": "chapters/extraction/02_estrazione.html#metodo-dei-fattori-principali-iterato-1",
    "title": "27  L’estrazione dei fattori",
    "section": "\n27.10 Metodo dei Fattori Principali Iterato",
    "text": "27.10 Metodo dei Fattori Principali Iterato\nQuesto metodo aggiorna iterativamente le stime delle comunalità finché le saturazioni non cambiano più (convergenza).\nEsecuzione in R con il pacchetto psych.\n\npa &lt;- psych::fa(R, nfactors = 2, rotate = \"none\", fm = \"pa\")\npa\n#&gt; Factor Analysis using method =  pa\n#&gt; Call: psych::fa(r = R, nfactors = 2, rotate = \"none\", fm = \"pa\")\n#&gt; Standardized loadings (pattern matrix) based upon correlation matrix\n#&gt;    PA1   PA2   h2     u2 com\n#&gt; K 0.98 -0.21 1.01 -0.008 1.1\n#&gt; I 0.48  0.74 0.77  0.230 1.7\n#&gt; H 0.78 -0.56 0.92  0.085 1.8\n#&gt; L 0.98 -0.19 0.99  0.010 1.1\n#&gt; J 0.69  0.69 0.95  0.049 2.0\n#&gt; \n#&gt;                        PA1  PA2\n#&gt; SS loadings           3.22 1.41\n#&gt; Proportion Var        0.64 0.28\n#&gt; Cumulative Var        0.64 0.93\n#&gt; Proportion Explained  0.70 0.30\n#&gt; Cumulative Proportion 0.70 1.00\n#&gt; \n#&gt; Mean item complexity =  1.5\n#&gt; Test of the hypothesis that 2 factors are sufficient.\n#&gt; \n#&gt; df null model =  10  with the objective function =  12\n#&gt; df of  the model are 1  and the objective function was  5.6 \n#&gt; \n#&gt; The root mean square of the residuals (RMSR) is  0.01 \n#&gt; The df corrected root mean square of the residuals is  0.04 \n#&gt; \n#&gt; Fit based upon off diagonal values = 1\n\n\n\nfm = \"pa\": specifica il metodo dei fattori principali.\nL’output include:\n\nsaturazioni fattoriali;\nunicità (\\(1 - h^2\\));\nvarianza spiegata da ciascun fattore.\n\n\n\n⚠️ Se una unicità &gt; 1 o negativa → soluzione impropria (caso di Heywood).",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#metodi-basati-sulla-simulazione",
    "href": "chapters/extraction/03_numero_fattori.html#metodi-basati-sulla-simulazione",
    "title": "28  Determinare il numero dei fattori",
    "section": "\n28.3 Metodi Basati sulla Simulazione",
    "text": "28.3 Metodi Basati sulla Simulazione\n\n28.3.1 Parallel Analysis (PA)\nConsiderata lo “standard aureo”. Confronta gli autovalori empirici con quelli derivati da dati casuali:\n\nvariante standard: usa la media degli autovalori simulati;\nvariante di Glorfeld: usa il 95° percentile per essere più conservativa;\nfunzioni: fa.parallel() del pacchetto psych.\n\n28.3.2 Comparison Data (CD)\nUsa bootstrap e riproduce la matrice di correlazione. Confronta soluzioni adiacenti con un test di Mann-Whitney sugli RMSE. Utile con fattori correlati, ma tende a sovrafattorizzare se non ben calibrato. Implementato in EFAtools.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Determinare il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#approcci-basati-sul-confronto-tra-modelli",
    "href": "chapters/extraction/03_numero_fattori.html#approcci-basati-sul-confronto-tra-modelli",
    "title": "28  Determinare il numero dei fattori",
    "section": "\n28.4 Approcci basati sul confronto tra modelli",
    "text": "28.4 Approcci basati sul confronto tra modelli\n\n28.4.1 Criteri Informativi: AIC, BIC\nUtilizzano la verosimiglianza e penalizzano la complessità del modello.\n\nAIC: tende a selezionare modelli più complessi;\nBIC: più conservativo.\n\n28.4.2 Indici di Fit:\nGli indicici di fit come RMSEA, CFI, SRMR, ecc. sono usati più comunemente in CFA, ma sono meno affidabili in EFA a causa della dipendenza da dimensione campionaria e altri fattori.\n\n28.4.3 Metodo Hull\nIl metodo Hull (Lorenzo-Seva, Timmerman, & Kiers, 2011) è un approccio grafico per la determinazione del numero ottimale di fattori. L’idea di base è bilanciare bontà dell’adattamento e parsimonia del modello (cioè la semplicità).\nCome funziona:\n\nsi adattano diversi modelli fattoriali con un numero crescente di fattori;\nper ciascun modello, si registra un indice di fit (es. CFI) e i gradi di libertà;\nsi costruisce il “convex hull”, ovvero il contorno convesso che racchiude i punti CFI ~ gradi di libertà;\nsi identifica il punto sul contorno del hull che rappresenta il miglior compromesso tra fit accettabile e modello semplice (cioè con più gradi di libertà).\n\nVantaggi:\n\ntende a evitare la sovrafattorizzazione, comune in altri metodi;\nha buone prestazioni quando il modello è ben sovradeterminato (cioè ogni fattore è misurato da molte variabili);\nè adatto anche in presenza di fattori correlati.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Determinare il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#minimum-average-partial-map",
    "href": "chapters/extraction/03_numero_fattori.html#minimum-average-partial-map",
    "title": "28  Determinare il numero dei fattori",
    "section": "\n28.5 Minimum Average Partial (MAP)",
    "text": "28.5 Minimum Average Partial (MAP)\nTest che valuta la media delle correlazioni parziali residue dopo estrazione di i componenti. Retiene il numero di componenti che minimizza questa media. È implementato in vss() del pacchetto psych.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Determinare il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#approcci-moderni",
    "href": "chapters/extraction/03_numero_fattori.html#approcci-moderni",
    "title": "28  Determinare il numero dei fattori",
    "section": "\n28.6 Approcci moderni",
    "text": "28.6 Approcci moderni\n\n28.6.1 Analisi Esplorativa della Rete (EGA)\nL’Exploratory Graph Analysis (EGA) è un metodo alternativo all’analisi fattoriale esplorativa (EFA), che non si basa sull’ipotesi di fattori latenti comuni, ma sull’identificazione di comunità di variabili all’interno di un modello a rete.\n\nIn EGA, le variabili osservate (es. item di un test) sono rappresentate come nodi di una rete.\nLe connessioni (archi) tra i nodi riflettono correlazioni parziali standardizzate, cioè relazioni tra due variabili controllando per tutte le altre.\nIl modello statistico di base è il Gaussian Graphical Model (GGM), stimato con un metodo di massima verosimiglianza penalizzata (regularization), che tende ad annullare le correlazioni più deboli, producendo reti sparse (con pochi collegamenti).\nAll’interno di questa rete, le variabili fortemente collegate tra loro tendono a raggrupparsi in comunità (clusters), che vengono interpretate come fattori.\n\nVantaggi dell’EGA:\n\nè particolarmente utile in condizioni in cui:\n\nle comunalità sono basse (cioè le variabili condividono poca varianza comune),\ni dati sono ordinali o non normalmente distribuiti.\n\n\nrispetto all’EFA tradizionale, EGA è più robusto a strutture complesse o deboli.\n\nCome viene determinato il numero di fattori?\nIl numero di comunità (e quindi di fattori) viene identificato attraverso algoritmi di rilevamento delle comunità, come il walktrap algorithm, che cerca sottogruppi fortemente interconnessi all’interno della rete.\nIn sintesi, EGA fornisce una rappresentazione grafica e interpretabile della struttura fattoriale dei dati, e può essere usato per decidere quanti fattori estrarre in un’analisi esplorativa.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Determinare il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#implementazione-in-r",
    "href": "chapters/extraction/03_numero_fattori.html#implementazione-in-r",
    "title": "28  Determinare il numero dei fattori",
    "section": "\n28.8 Implementazione in R",
    "text": "28.8 Implementazione in R\nPer confrontare i metodi discussi per la scelta del numero \\(m\\) di fattori usiamo una matrice di correlazioni calcolata sulle sottoscale della WAIS. Le 11 sottoscale del test sono le seguenti:\n\nX1 = Information\nX2 = Comprehension\nX3 = Arithmetic\nX4 = Similarities\nX5 = Digit.span\nX6 = Vocabulary\nX7 = Digit.symbol\nX8 = Picture.completion\nX9 = Block.design\nX10 = Picture.arrangement\nX11 = Object.\n\nI dati sono stati ottenuti dal manuale della III edizione.\n\nvarnames &lt;- c(\n    \"IN\", \"CO\", \"AR\", \"SI\", \"DS\", \"VO\", \"SY\", \"PC\",\n    \"BD\", \"PA\", \"OA\", \"AG\", \"ED\"\n)\ntemp &lt;- matrix(c(\n    1, 0.67, 0.62, 0.66, 0.47, 0.81, 0.47, 0.60, 0.49, 0.51, 0.41,\n    -0.07, 0.66, .67, 1, 0.54, 0.60, 0.39, 0.72, 0.40, 0.54, 0.45,\n    0.49, 0.38, -0.08, 0.52, .62, .54, 1, 0.51, 0.51, 0.58, 0.41,\n    0.46, 0.48, 0.43, 0.37, -0.08, 0.49, .66, .60, .51, 1, 0.41,\n    0.68, 0.49, 0.56, 0.50, 0.50, 0.41, -0.19, 0.55, .47, .39, .51,\n    .41, 1, 0.45, 0.45, 0.42, 0.39, 0.42, 0.31, -0.19, 0.43,\n    .81, .72, .58, .68, .45, 1, 0.49, 0.57, 0.46, 0.52, 0.40, -0.02,\n    0.62, .47, .40, .41, .49, .45, .49, 1, 0.50, 0.50, 0.52, 0.46,\n    -0.46, 0.57, .60, .54, .46, .56, .42, .57, .50, 1, 0.61, 0.59,\n    0.51, -0.28, 0.48, .49, .45, .48, .50, .39, .46, .50, .61, 1,\n    0.54, 0.59, -0.32, 0.44, .51, .49, .43, .50, .42, .52, .52, .59,\n    .54, 1, 0.46, -0.37, 0.49, .41, .38, .37, .41, .31, .40, .46, .51,\n    .59, .46, 1, -0.28, 0.40, -.07, -.08, -.08, -.19, -.19, -.02,\n    -.46, -.28, -.32, -.37, -.28, 1, -0.29, .66, .52, .49, .55, .43,\n    .62, .57, .48, .44, .49, .40, -.29, 1\n), nrow = 13, ncol = 13, byrow = TRUE)\n\ncolnames(temp) &lt;- varnames\nrownames(temp) &lt;- varnames\n\nwais_cor &lt;- temp[1:11, 1:11]\nwais_cor\n#&gt;      IN   CO   AR   SI   DS   VO   SY   PC   BD   PA   OA\n#&gt; IN 1.00 0.67 0.62 0.66 0.47 0.81 0.47 0.60 0.49 0.51 0.41\n#&gt; CO 0.67 1.00 0.54 0.60 0.39 0.72 0.40 0.54 0.45 0.49 0.38\n#&gt; AR 0.62 0.54 1.00 0.51 0.51 0.58 0.41 0.46 0.48 0.43 0.37\n#&gt; SI 0.66 0.60 0.51 1.00 0.41 0.68 0.49 0.56 0.50 0.50 0.41\n#&gt; DS 0.47 0.39 0.51 0.41 1.00 0.45 0.45 0.42 0.39 0.42 0.31\n#&gt; VO 0.81 0.72 0.58 0.68 0.45 1.00 0.49 0.57 0.46 0.52 0.40\n#&gt; SY 0.47 0.40 0.41 0.49 0.45 0.49 1.00 0.50 0.50 0.52 0.46\n#&gt; PC 0.60 0.54 0.46 0.56 0.42 0.57 0.50 1.00 0.61 0.59 0.51\n#&gt; BD 0.49 0.45 0.48 0.50 0.39 0.46 0.50 0.61 1.00 0.54 0.59\n#&gt; PA 0.51 0.49 0.43 0.50 0.42 0.52 0.52 0.59 0.54 1.00 0.46\n#&gt; OA 0.41 0.38 0.37 0.41 0.31 0.40 0.46 0.51 0.59 0.46 1.00",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Determinare il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#metodi-basati-sugli-autovalori-1",
    "href": "chapters/extraction/03_numero_fattori.html#metodi-basati-sugli-autovalori-1",
    "title": "28  Determinare il numero dei fattori",
    "section": "\n28.9 Metodi basati sugli autovalori",
    "text": "28.9 Metodi basati sugli autovalori\n\n# Calcola gli autovalori della matrice di correlazione WAIS\nwais_eigen &lt;- eigen(wais_cor)\neigenvalues &lt;- wais_eigen$values\nprint(eigenvalues)\n#&gt;  [1] 6.0745 1.0150 0.7462 0.5868 0.5083 0.4312 0.4233 0.3766 0.3510 0.3101\n#&gt; [11] 0.1770\n\n\n28.9.1 Kaiser-Guttman Rule\n\nkaiser_rule &lt;- sum(eigenvalues &gt; 1)\ncat(\"Numero di fattori secondo la regola di Kaiser:\", kaiser_rule, \"\\n\")\n#&gt; Numero di fattori secondo la regola di Kaiser: 2\n\nSpiegazione: la regola di Kaiser suggerisce di mantenere i fattori con autovalori maggiori di 1.Problema: è noto che sovrastima il numero di fattori, specialmente in campioni piccoli o quando le comunalità sono basse.\n\n28.9.2 Scree Plot (Cattell)\n\n# Scree plot\nplot(eigenvalues, type = \"b\", pch = 19, main = \"Scree plot (Cattell)\", \n     xlab = \"Numero di fattori\", ylab = \"Autovalore\")\nabline(h = 1, col = \"red\", lty = 2)\n\n\n\n\n\n\n\nSpiegazione: Il numero ottimale di fattori corrisponde al punto prima del “gomito” nella curva degli autovalori decrescenti.Problema: il metodo è visivo e soggettivo, quindi ha bassa affidabilità.\n\n28.9.3 Regola del valore medio degli autovalori\n\nmean_val &lt;- mean(eigenvalues)\nmean_rule &lt;- sum(eigenvalues &gt; mean_val)\ncat(\"Numero di fattori secondo la regola del valore medio:\", mean_rule, \"\\n\")\n#&gt; Numero di fattori secondo la regola del valore medio: 2\n\nSpiegazione: mantiene solo i fattori con autovalori superiori alla media.\nQuesta è una variante della regola di Kaiser, meno estrema, ma comunque euristica.\n\n28.9.4 Metodi avanzati con il pacchetto nFactors\n\n\n# Metodo EKC (Empirical Kaiser Criterion)\nekc_result &lt;- efa.ekc(sample.cov = wais_cor, sample.nobs = 300)  # Specificare N = numerosità stimata\nekc_result\n#&gt; \n#&gt;  Empirical Kaiser Criterion suggests 4 factors.\n#&gt;  Traditional Kaiser Criterion suggests 2 factors.\n#&gt; \n#&gt;    Sample   Ref\n#&gt; 1   6.074 1.420\n#&gt; 2   1.015 0.699\n#&gt; 3   0.746 0.617\n#&gt; 4   0.587 0.562\n#&gt; 5   0.508 0.523\n#&gt; 6   0.431 0.490\n#&gt; 7   0.423 0.465\n#&gt; 8   0.377 0.431\n#&gt; 9   0.351 0.397\n#&gt; 10  0.310 0.346\n#&gt; 11  0.177 0.251\n\n\n\n\n\n\n\nSpiegazione: EKC è una versione empiricamente corretta della regola di Kaiser, che tiene conto del campione, della forma della distribuzione, e della varianza spiegata cumulativa.\nÈ più affidabile, soprattutto in strutture semplici.\n\n28.9.5 STOC e STAF (versioni automatizzate dello Scree Test)\n\n# Calcola autovalori simulati\nnfac &lt;- nFactors::nScree(x = eigenvalues)\nsummary(nfac)\n#&gt; Report For a nScree Class \n#&gt; \n#&gt; Details: components \n#&gt; \n#&gt;    Eigenvalues Prop Cumu Par.Analysis Pred.eig     OC Acc.factor     AF\n#&gt; 1            6    1    1            1        1                NA (&lt; AF)\n#&gt; 2            1    0    1            1        1 (&lt; OC)          5       \n#&gt; 3            1    0    1            1        1                 0       \n#&gt; 4            1    0    1            1        1                 0       \n#&gt; 5            1    0    1            1        0                 0       \n#&gt; 6            0    0    1            1        0                 0       \n#&gt; 7            0    0    1            1        0                 0       \n#&gt; 8            0    0    1            1        0                 0       \n#&gt; 9            0    0    1            1        0                 0       \n#&gt; 10           0    0    1            1       NA                 0       \n#&gt; 11           0    0    1            1       NA                NA       \n#&gt; \n#&gt; \n#&gt;  Number of factors retained by index \n#&gt; \n#&gt;   noc naf nparallel nkaiser\n#&gt; 1   2   1         2       2\n\n# Plot per confronto\nplotnScree(nfac)\n\n\n\n\n\n\n\nSpiegazione:\n\n\nSTOC = Optimal Coordinate\n\nSTAF = Acceleration Factor\nSono versioni statistiche dello scree test, che usano variazioni nella pendenza degli autovalori.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Determinare il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#metodi-basati-sulla-simulazione-1",
    "href": "chapters/extraction/03_numero_fattori.html#metodi-basati-sulla-simulazione-1",
    "title": "28  Determinare il numero dei fattori",
    "section": "\n28.10 Metodi basati sulla simulazione",
    "text": "28.10 Metodi basati sulla simulazione\n\n28.10.1 Parallel Analysis (PA)\n\n# Variante standard: confronto con media degli autovalori simulati\nset.seed(123)  # Per replicabilità\nfa.parallel(wais_cor, n.obs = 300, fa = \"fa\", fm = \"ml\", \n            main = \"Parallel Analysis (media simulata)\")\n#&gt; Parallel analysis suggests that the number of factors =  2  and the number of components =  NA\n\n\n\n\n\n\n\n\n28.10.2 Spiegazione\n\nConfronta gli autovalori osservati con quelli ottenuti da dati casuali.\nSe l’autovalore osservato &gt; simulato → mantieni il fattore.\nIl metodo è molto affidabile, specie se il numero di soggetti (n.obs) è corretto.\nLa variante di Glorfeld (non mostrata) è più conservativa (riduce il rischio di sovrafattorizzare).\n\n28.10.3 Comparison Data (CD)\n\n# Metodo Comparison Data\n# Richiede che i dati siano in formato \"raw\" (non solo matrice di correlazione)\n# Quindi, simuliamo dati coerenti con la matrice di correlazione per scopi didattici:\n\nset.seed(123)\nN &lt;- 300  # ipotetica numerosità campionaria\nwais_sim &lt;- mvrnorm(N, mu = rep(0, 11), Sigma = wais_cor)\ncolnames(wais_sim) &lt;- colnames(wais_cor)\n\n# Applica il metodo Comparison Data\ncd_result &lt;- EFAtools::CD(\n  x = wais_sim,\n  n_factors_max = 6,        # Numero massimo di fattori da testare\n  N_pop = 10000,             # Dimensione della popolazione simulata\n  N_samples = 500,           # Numero di campioni bootstrap\n  alpha = 0.3,               # Soglia per il test di Mann-Whitney\n  use = \"pairwise.complete.obs\",  # Gestione dei dati mancanti\n  cor_method = \"pearson\",    # Metodo di correlazione\n  max_iter = 50              # Iterazioni massime\n)\n\n# Mostra il riepilogo dei risultati\ncd_result\n#&gt; The number of factors suggested by CD is .\n\n\n28.10.4 Spiegazione\n\nIl metodo Comparison Data (CD) simula set di dati “riprodotti” con un certo numero di fattori.\nConfronta il RMSE delle soluzioni successive con un test di Mann-Whitney.\nIl numero ottimale di fattori è quello oltre il quale non si osserva un miglioramento significativo.\n\nAttenzione: può sovrafattorizzare se max_factors è troppo alto o se i dati sono rumorosi.\nMolto utile con fattori correlati e strutture complesse.\n\nIn sintesi:\n\n\nPA (Parallel Analysis) è il metodo di riferimento, raccomandato dalla maggior parte delle linee guida (es. Fabrigar et al., 1999).\n\nCD (Comparison Data) è utile in presenza di fattori obliqui o bassa comunalità, ma può richiedere parametri aggiustati per una stima più accurata.\nEvita di usare un solo criterio: combina i risultati con quelli basati sugli autovalori e sulle analisi di bontà di adattamento (RMSEA, BIC, ecc.).",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Determinare il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#indici-di-informazione",
    "href": "chapters/extraction/03_numero_fattori.html#indici-di-informazione",
    "title": "28  Determinare il numero dei fattori",
    "section": "\n28.11 Indici di informazione",
    "text": "28.11 Indici di informazione\nConsideriamo ora un’implementazione in R per determinare il numero di fattori da estrarre dalla matrice di correlazione WAIS utilizzando metodi basati sugli indici di informazione.\n\ndim(wais_cor)  # Dovrebbe essere 11x11\n#&gt; [1] 11 11\n\n\n\nCriteri Informativi AIC e BIC:\n\nCalcolo di AIC e BIC per modelli con 1-5 fattori.\nVisualizzazione grafica per identificare il punto di minimo.\n\n\n\n\n# Calcolo il numero di fattori usando AIC e BIC\nfa_fit &lt;- function(nfactors, x, n.obs = 100) {\n  fit &lt;- fa(x, nfactors = nfactors, fm = \"ml\", n.obs = n.obs)\n  chi &lt;- fit$STATISTIC\n  df &lt;- fit$dof\n  pval &lt;- fit$PVAL\n  aic &lt;- chi - 2 * df\n  bic &lt;- chi - df * log(n.obs)\n  list(nfactors = nfactors, chi = chi, df = df, pval = pval, aic = aic, bic = bic)\n}\n\n\n# Assumiamo una dimensione campionaria di 100 \nn.obs &lt;- 100\n\n# Calcoliamo AIC e BIC per diversi numeri di fattori\nresults &lt;- data.frame()\nfor (i in 1:5) {\n  res &lt;- fa_fit(i, wais_cor, n.obs)\n  results &lt;- rbind(results, data.frame(\n    nfactors = i,\n    chi_square = res$chi,\n    df = res$df,\n    p_value = res$pval,\n    aic = res$aic,\n    bic = res$bic\n  ))\n}\n\n\n# Visualizziamo i risultati\nprint(results)\n#&gt;   nfactors chi_square df  p_value    aic     bic\n#&gt; 1        1     69.589 44 0.008288 -18.41 -133.04\n#&gt; 2        2     15.998 34 0.996287 -52.00 -140.58\n#&gt; 3        3      8.244 25 0.999341 -41.76 -106.89\n#&gt; 4        4      3.515 17 0.999787 -30.48  -74.77\n#&gt; 5        5      1.229 10 0.999561 -18.77  -44.82\n\n\n# Grafici per AIC e BIC\npar(mfrow = c(1, 2))\nplot(results$nfactors, results$aic, type = \"b\", main = \"AIC per numero di fattori\", \n     xlab = \"Numero di fattori\", ylab = \"AIC\", xaxt = \"n\")\naxis(1, at = 1:5)\nabline(v = which.min(results$aic), col = \"red\", lty = 2)\n\nplot(results$nfactors, results$bic, type = \"b\", main = \"BIC per numero di fattori\", \n     xlab = \"Numero di fattori\", ylab = \"BIC\", xaxt = \"n\")\naxis(1, at = 1:5)\nabline(v = which.min(results$bic), col = \"red\", lty = 2)\npar(mfrow = c(1, 1))\n\n\n\n\n\n\n\n\n\nIndici di Fit da CFA:\n\nImplementazione di CFI, TLI, RMSEA e SRMR.\nGrafici per valutare quando questi indici raggiungono valori accettabili.\nCriteri di riferimento: CFI &gt; 0.95, RMSEA &lt; 0.05.\n\n\n\n\n# Definiamo i modelli CFA per diversi numeri di fattori\nfit_indices &lt;- data.frame()\n\nfor (i in 1:5) {\n  # Estraiamo prima i fattori con analisi fattoriale esplorativa\n  fa_result &lt;- fa(wais_cor, nfactors = i, fm = \"ml\", rotate = \"varimax\")\n  \n  # Creiamo il modello CFA basato sui loadings più alti\n  model_syntax &lt;- \"\"\n  for (j in 1:i) {\n    # Seleziona le variabili con i loadings più alti per ciascun fattore\n    vars &lt;- names(sort(abs(fa_result$loadings[, j]), decreasing = TRUE)[1:ceiling(11/i)])\n    model_syntax &lt;- paste0(model_syntax, \"F\", j, \" =~ \", paste(vars, collapse = \" + \"), \"\\n\")\n  }\n  \n  # Eseguiamo la CFA\n  try({\n    fit &lt;- cfa(model_syntax, sample.cov = wais_cor, sample.nobs = n.obs)\n    indices &lt;- fitMeasures(fit, c(\"chisq\", \"df\", \"pvalue\", \"cfi\", \"tli\", \"rmsea\", \"srmr\", \"aic\", \"bic\"))\n    \n    fit_indices &lt;- rbind(fit_indices, data.frame(\n      nfactors = i, \n      chisq = indices[\"chisq\"], \n      df = indices[\"df\"], \n      pvalue = indices[\"pvalue\"],\n      cfi = indices[\"cfi\"], \n      tli = indices[\"tli\"], \n      rmsea = indices[\"rmsea\"], \n      srmr = indices[\"srmr\"],\n      aic = indices[\"aic\"], \n      bic = indices[\"bic\"]\n    ))\n  }, silent = TRUE)\n}\n\n\n# Visualizziamo gli indici di fit\nprint(fit_indices)\n#&gt;        nfactors  chisq df   pvalue    cfi    tli  rmsea    srmr  aic  bic\n#&gt; chisq         1 74.162 44 0.002985 0.9476 0.9345 0.0828 0.05991 2598 2655\n#&gt; chisq1        2 15.429 32 0.994044 1.0000 1.0430 0.0000 0.02935 2302 2362\n#&gt; chisq2        3 16.482 30 0.978282 1.0000 1.0383 0.0000 0.03198 2319 2384\n#&gt; chisq3        4 13.998 27 0.981271 1.0000 1.0429 0.0000 0.02705 2348 2420\n#&gt; chisq4        5  5.737 20 0.999209 1.0000 1.0635 0.0000 0.01502 2353 2444\n\n\n# Visualizziamo graficamente gli indici di fit\nif (nrow(fit_indices) &gt; 0) {\n  par(mfrow = c(2, 2))\n  plot(fit_indices$nfactors, fit_indices$cfi, type = \"b\", main = \"CFI per numero di fattori\", \n       xlab = \"Numero di fattori\", ylab = \"CFI\", xaxt = \"n\")\n  axis(1, at = 1:5)\n  abline(h = 0.95, col = \"red\", lty = 2)\n  \n  plot(fit_indices$nfactors, fit_indices$rmsea, type = \"b\", main = \"RMSEA per numero di fattori\", \n       xlab = \"Numero di fattori\", ylab = \"RMSEA\", xaxt = \"n\")\n  axis(1, at = 1:5)\n  abline(h = 0.05, col = \"red\", lty = 2)\n  \n  plot(fit_indices$nfactors, fit_indices$aic, type = \"b\", main = \"AIC (CFA) per numero di fattori\", \n       xlab = \"Numero di fattori\", ylab = \"AIC\", xaxt = \"n\")\n  axis(1, at = 1:5)\n  \n  plot(fit_indices$nfactors, fit_indices$bic, type = \"b\", main = \"BIC (CFA) per numero di fattori\", \n       xlab = \"Numero di fattori\", ylab = \"BIC\", xaxt = \"n\")\n  axis(1, at = 1:5)\n  par(mfrow = c(1, 1))\n}\n\n\n\n\n\n\n\n\n\nTest del Chi-Quadrato:\n\nConfronto incrementale tra modelli con diverso numero di fattori.\nTest della significatività della differenza di fit.\n\n\n\n\n# Calcoliamo la differenza di chi-quadrato tra modelli consecutivi\nif (nrow(results) &gt; 1) {\n  chi_diff &lt;- data.frame(\n    comparison = character(),\n    chi_diff = numeric(),\n    df_diff = numeric(),\n    p_value = numeric()\n  )\n  \n  for (i in 2:nrow(results)) {\n    chi_diff_val &lt;- results$chi_square[i-1] - results$chi_square[i]\n    df_diff_val &lt;- results$df[i-1] - results$df[i]\n    p_val &lt;- 1 - pchisq(chi_diff_val, df_diff_val)\n    \n    chi_diff &lt;- rbind(chi_diff, data.frame(\n      comparison = paste(i-1, \"vs\", i),\n      chi_diff = chi_diff_val,\n      df_diff = df_diff_val,\n      p_value = p_val\n    ))\n  }\n  \n  print(\"Test del chi-quadrato per confronto di modelli:\")\n  print(chi_diff)\n}\n#&gt; [1] \"Test del chi-quadrato per confronto di modelli:\"\n#&gt;   comparison chi_diff df_diff   p_value\n#&gt; 1     1 vs 2   53.591      10 5.782e-08\n#&gt; 2     2 vs 3    7.754       9 5.591e-01\n#&gt; 3     3 vs 4    4.729       8 7.861e-01\n#&gt; 4     4 vs 5    2.286       7 9.423e-01\n\n\n\nSintesi dei Risultati:\n\nRiepilogo delle indicazioni dai vari indici.\nRaccomandazione sul numero ottimale di fattori.\n\n\n\n\ncat(\"\\nSintesi dei risultati:\\n\")\n#&gt; \n#&gt; Sintesi dei risultati:\ncat(\"Numero di fattori suggerito da AIC:\", which.min(results$aic), \"\\n\")\n#&gt; Numero di fattori suggerito da AIC: 2\ncat(\"Numero di fattori suggerito da BIC:\", which.min(results$bic), \"\\n\")\n#&gt; Numero di fattori suggerito da BIC: 2\n\nif (nrow(fit_indices) &gt; 0) {\n  # Per CFI vogliamo valori &gt; 0.95\n  good_cfi &lt;- which(fit_indices$cfi &gt; 0.95)\n  if (length(good_cfi) &gt; 0) {\n    cat(\"Numero minimo di fattori con CFI &gt; 0.95:\", min(good_cfi), \"\\n\")\n  }\n  \n  # Per RMSEA vogliamo valori &lt; 0.05\n  good_rmsea &lt;- which(fit_indices$rmsea &lt; 0.05)\n  if (length(good_rmsea) &gt; 0) {\n    cat(\"Numero minimo di fattori con RMSEA &lt; 0.05:\", min(good_rmsea), \"\\n\")\n  }\n}\n#&gt; Numero minimo di fattori con CFI &gt; 0.95: 2 \n#&gt; Numero minimo di fattori con RMSEA &lt; 0.05: 2\n\nif (nrow(chi_diff) &gt; 0) {\n  # Per il test chi-quadrato, cerchiamo il primo confronto non significativo\n  non_sig &lt;- which(chi_diff$p_value &gt; 0.05)\n  if (length(non_sig) &gt; 0) {\n    cat(\"Basato sul test del chi-quadrato, il numero ottimale di fattori è:\", as.numeric(substr(chi_diff$comparison[min(non_sig)], 1, 1)), \"\\n\")\n  }\n}\n#&gt; Basato sul test del chi-quadrato, il numero ottimale di fattori è: 2",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Determinare il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#metodo-hull-1",
    "href": "chapters/extraction/03_numero_fattori.html#metodo-hull-1",
    "title": "28  Determinare il numero dei fattori",
    "section": "\n28.12 Metodo Hull",
    "text": "28.12 Metodo Hull\nIl metodo è implementato nel pacchetto EFAtools in R (Steiner & Gruber, 2020).\nNel grafico risultante, si osserva la curva dei valori di CFI in funzione dei gradi di libertà. Il metodo Hull seleziona il punto “di gomito”, dove il modello ha ancora un buon fit ma con la massima parsimonia. In alcuni casi, il metodo può suggerire ad esempio che una soluzione a un fattore è preferibile, se l’aggiunta di ulteriori fattori non migliora significativamente l’adattamento.\n\nHull(\n  wais_sim,\n  fa = \"fa\",\n  nfact = 6,\n  cor.type = \"pearson\",\n  use = \"pairwise.complete.obs\",\n  vis = TRUE,\n  plot = TRUE\n)\n#&gt; The number of factors suggested by Hull is 1 .\n#&gt; The number of factors suggested by Hull is 1 .",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Determinare il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#metodo-map",
    "href": "chapters/extraction/03_numero_fattori.html#metodo-map",
    "title": "28  Determinare il numero dei fattori",
    "section": "\n28.13 Metodo MAP",
    "text": "28.13 Metodo MAP\nIl MAP test valuta, per ogni possibile numero di componenti estratti, quanto rimane di correlazione “spuria” nei residui. Il numero ottimale è quello che minimizza la media delle correlazioni parziali residue, cioè quello che riesce a “pulire” meglio la matrice di correlazione iniziale.\n\n# Applica il metodo MAP con la funzione vss()\nvss_map &lt;- vss(\n  x = wais_cor,\n  n = 6,          # numero massimo di fattori/componenti da testare\n  n.obs = 100,     # numero di osservazioni\n  rotate = \"none\", # nessuna rotazione per mantenere interpretabilità\n  plot = FALSE     # non mostrare il grafico automaticamente\n)\nvss_map\n#&gt; \n#&gt; Very Simple Structure\n#&gt; Call: vss(x = wais_cor, n = 6, rotate = \"none\", n.obs = 100, plot = FALSE)\n#&gt; VSS complexity 1 achieves a maximimum of 0.92  with  2  factors\n#&gt; VSS complexity 2 achieves a maximimum of 0.95  with  5  factors\n#&gt; \n#&gt; The Velicer MAP achieves a minimum of 0.03  with  2  factors \n#&gt; BIC achieves a minimum of  -140.3  with  2  factors\n#&gt; Sample Size adjusted BIC achieves a minimum of  -32.93  with  2  factors\n#&gt; \n#&gt; Statistics by number of factors \n#&gt;   vss1 vss2   map dof chisq   prob sqresid  fit RMSEA  BIC SABIC complex\n#&gt; 1 0.92 0.00 0.033  44 72.00 0.0049     3.2 0.92 0.079 -131   8.3     1.0\n#&gt; 2 0.92 0.94 0.029  34 16.27 0.9956     2.3 0.94 0.000 -140 -32.9     1.2\n#&gt; 3 0.83 0.95 0.044  25  8.40 0.9992     1.7 0.96 0.000 -107 -27.8     1.4\n#&gt; 4 0.92 0.95 0.071  17  3.94 0.9995     1.7 0.96 0.000  -74 -20.7     1.4\n#&gt; 5 0.92 0.95 0.107  10  1.46 0.9990     1.5 0.96 0.000  -45 -13.0     1.5\n#&gt; 6 0.92 0.95 0.150   4  0.31 0.9894     1.4 0.96 0.000  -18  -5.5     1.6\n#&gt;   eChisq   SRMR eCRMS eBIC\n#&gt; 1 41.515 0.0614 0.069 -161\n#&gt; 2  8.201 0.0273 0.035 -148\n#&gt; 3  2.926 0.0163 0.024 -112\n#&gt; 4  1.115 0.0101 0.018  -77\n#&gt; 5  0.326 0.0054 0.013  -46\n#&gt; 6  0.082 0.0027 0.010  -18\n\n\n28.13.1 Sintesi dei principali risultati\n\n\n\n\n\n\n\nMetodo\nNumero ottimale di fattori\nValore ottimale\n\n\n\nMAP (Velicer)\n2\n0.03 (minimo)\n\n\nBIC\n2\n-140.3 (minimo)\n\n\nBIC corretto per n (SABIC)\n2\n-32.93 (minimo)\n\n\nVSS complessità 1\n2\n0.92 (massimo)\n\n\nVSS complessità 2\n5\n0.95 (massimo)\n\n\n\n28.13.2 Velicer MAP\n\nValuta la media delle correlazioni parziali residue.\nL’obiettivo è minimizzare la varianza residua non spiegata dai fattori.\n\nRisultato: minimo a 2 fattori, con valore 0.03 → suggerisce 2 fattori.\n\n28.13.3 BIC e SABIC\n\nCriteri informativi che bilanciano bontà del fit e parsimonia.\nPiù basso è il valore, meglio è.\nEntrambi i criteri (sia BIC classico che SABIC) raggiungono il minimo a 2 fattori.\n\n28.13.4 SS (Very Simple Structure)\n\nMisura quanto bene una struttura semplice (con pochi caricamenti per variabile) si adatta ai dati.\nDue versioni:\n\n\nComplessità 1: solo il caricamento maggiore per ogni variabile.\n\nComplessità 2: primi due caricamenti per variabile.\n\n\nComplessità 1 → massimo a 2 fattori (0.92)\nComplessità 2 → massimo a 5 fattori (0.95)\n\n🔎 Nota: VSS complessità 2 è più permissiva e tende a favorire strutture più complesse.\nInterpretazione complessiva.\nTutti i criteri basati su residui o penalizzazione della complessità (MAP, BIC, SABIC, VSS-1) concordano nel suggerire una soluzione a 2 fattori.\nSolo VSS-2 (più permissivo) suggerisce 5 fattori, ma questa soluzione è meno parsimoniosa e più soggetta a sovrafattorizzazione.\nIn sintesi, sulla base di criteri oggettivi e parsimoniosi come MAP, BIC, SABIC, e VSS a complessità 1, una soluzione a 2 fattori sembra ottimale per questi dati WAIS. L’adozione di criteri informativi e basati sui residui, come MAP e BIC, è fortemente raccomandata rispetto a metodi più soggettivi o sovraestimanti come Kaiser o Scree test.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Determinare il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#exploratory-graph-analysis-ega",
    "href": "chapters/extraction/03_numero_fattori.html#exploratory-graph-analysis-ega",
    "title": "28  Determinare il numero dei fattori",
    "section": "\n28.14 Exploratory Graph Analysis (EGA)",
    "text": "28.14 Exploratory Graph Analysis (EGA)\nL’Exploratory Graph Analysis (EGA) è un metodo innovativo per identificare la struttura latente dei dati basato su modelli a rete, piuttosto che sui tradizionali modelli fattoriali. È implementato nel pacchetto EGAnet, sviluppato da Golino e colleghi. Il metodo è stato introdotto da Golino & Epskamp (2017) e perfezionato in studi successivi (Christensen, Golino & Silvia, 2020; Golino et al., 2020).\n\n28.14.1 Caratteristiche principali di EGA:\n\nUtilizza il graphical lasso per stimare le correlazioni parziali tra variabili, costruendo così una rete sparsa (solo le relazioni più forti restano).\nIdentifica comunità di variabili all’interno della rete, che corrispondono a fattori latenti.\nÈ particolarmente utile quando:\n\nle comunalità sono basse,\nla struttura fattoriale non è ben definita,\nsi lavora con dati ordinali o non normali.\n\n\n\nApplichiamo l’EGA alla matrice di correlazione delle 11 sottoscale della WAIS.\n\n# Applica l'EGA alla matrice di correlazione WAIS\nega_result &lt;- EGA(\n  data = wais_cor,        # Matrice di correlazione tra le 11 sottoscale\n  n = 300,                # Numero di soggetti nel campione\n  model = \"glasso\",       # Metodo di stima: graphical lasso\n  type = \"correlation\",   # Specifica che stiamo passando una matrice di correlazione\n  plot.EGA = TRUE         # Visualizza il grafo delle comunità (dimensioni)\n)\n\n\n\n\n\n\n\n\n\nmodel = \"glasso\": applica una penalizzazione (lasso) per ridurre il numero di connessioni deboli tra variabili.\n\nplot.EGA = TRUE: mostra un grafo con le sottoscale collegate in base alla loro correlazione condizionale.\n\nega_result$wc: contiene l’assegnazione di ciascuna variabile a una comunità, interpretata come un fattore latente.\n\nInterpretazione dei risultati:\n\n# Riepilogo generale\nsummary(ega_result)\n#&gt; Model: GLASSO (EBIC with gamma = 0.5)\n#&gt; Correlations: auto\n#&gt; Lambda: 0.081 (n = 100, ratio = 0.1)\n#&gt; \n#&gt; Number of nodes: 11\n#&gt; Number of edges: 48\n#&gt; Edge density: 0.873\n#&gt; \n#&gt; Non-zero edge weights: \n#&gt;      M    SD   Min   Max\n#&gt;  0.099 0.088 0.003 0.421\n#&gt; \n#&gt; ----\n#&gt; \n#&gt; Algorithm:  Louvain\n#&gt; \n#&gt; Number of communities:  1\n#&gt; \n#&gt; IN CO AR SI DS VO SY PC BD PA OA \n#&gt;  1  1  1  1  1  1  1  1  1  1  1 \n#&gt; \n#&gt; ----\n#&gt; \n#&gt; Unidimensional Method: Louvain\n#&gt; Unidimensional: Yes\n#&gt; \n#&gt; ----\n#&gt; \n#&gt; TEFI: 0\n\n\n# Numero di dimensioni individuate (cioè di comunità)\nlength(unique(ega_result$wc))\n#&gt; [1] 1\n\nVantaggi:\n\n\nnon richiede ipotesi forti sulla distribuzione dei dati;\n\npiù robusto dei metodi classici (EFA, PA) in presenza di comunalità basse;\noffre una visualizzazione intuitiva delle relazioni tra variabili.\n\n\n💡 EGA può essere utilizzato sia per esplorare la dimensionalità di un set di item sia per decidere quanti fattori mantenere prima di una conferma con CFA.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Determinare il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#metodi-basati-su-machine-learning",
    "href": "chapters/extraction/03_numero_fattori.html#metodi-basati-su-machine-learning",
    "title": "28  Determinare il numero dei fattori",
    "section": "\n28.7 Metodi Basati su Machine Learning",
    "text": "28.7 Metodi Basati su Machine Learning\n\n28.7.1 Factor Forest (ML)\nApproccio machine learning addestrato su dati simulati. Molto preciso, ma dipende da modelli preaddestrati. Implementato in latentFactoR.\n\n28.7.2 Comparison Data Forest (CDF)\nVersione più leggera del Factor Forest basata su CD + Random Forest. Meno accurata ma più accessibile. Implementazione disponibile su OSF.\n\n28.7.3 Regularized EFA (REFA)\nUtilizza penalizzazioni (LASSO, Ridge, MC+) per ottenere strutture sparse. Può essere utile per l’identificazione automatica dei fattori. Implementazioni: fanc, regsem, lslx.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Determinare il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html",
    "href": "chapters/ctt/01_ctt_1.html",
    "title": "8  Fondamenti teorici",
    "section": "",
    "text": "8.1 Introduzione\nPrerequisiti\nPreparazione del Notebook\nLa Teoria Classica dei Test (CTT), sviluppata inizialmente da Spearman (1904) e formalizzata in seguito da Lord & Novick (1968), costituisce un pilastro della psicometria per la valutazione di caratteristiche psicologiche mediante strumenti quali test e questionari. Il principio fondante della CTT risiede nell’assunzione che il punteggio osservato di un individuo derivi dalla combinazione di due elementi: il punteggio vero, corrispondente al livello effettivo del tratto misurato, e un errore casuale insito nel processo di misurazione. Questo framework teorico offre un modello per stimare la discrepanza tra performance reale e risultato empirico, oltre a fornire strumenti per valutarne la precisione e l’affidabilità. Nonostante l’emergere di approcci avanzati come la Teoria della Risposta all’Item (IRT) e la Teoria della Generalizzabilità, la CTT mantiene un ruolo centrale nella progettazione, nell’interpretazione e nella validazione dei test psicometrici.\nFondamenti concettuali\nSecondo la CTT, ogni risposta a un test è riconducibile all’equazione:\n\\[\nX = T + E,\n\\tag{8.1}\\]\ndove \\(X\\) rappresenta il punteggio osservato, \\(T\\) il punteggio vero (costante teorica) ed \\(E\\) l’errore di misurazione, inteso come componente stocastica a media nulla. Il punteggio vero riflette la competenza o il tratto latente del soggetto, mentre l’errore incorpora fattori contingenti (es. distrazioni, variabilità ambientali, limitazioni dello strumento) che influenzano la performance in modo non sistematico.\nImplicazioni metodologiche\nLa teoria si concentra sull’analisi della varianza dei punteggi: attraverso indicatori statistici quali deviazione standard, coefficienti di affidabilità (es. alpha di Cronbach) e correlazioni tra item, quantifica la proporzione di variabilità attribuibile al costrutto rispetto al rumore sperimentale. Questa analisi guida la standardizzazione dei test, l’ottimizzazione del numero di item e la stima dell’intervallo di confidenza per i punteggi veri. Pur non modellando esplicitamente la difficoltà degli item o i tratti latenti (come avviene nell’IRT), la CTT rimane indispensabile per la valutazione psicometrica di base, garantendo rigore nella costruzione degli strumenti e nell’interpretazione dei dati.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html#costrutti-in-psicometria",
    "href": "chapters/ctt/01_ctt_1.html#costrutti-in-psicometria",
    "title": "8  Fondamenti teorici",
    "section": "\n8.2 Costrutti in Psicometria",
    "text": "8.2 Costrutti in Psicometria\nUn costrutto è un concetto teorico, tipicamente latente e non direttamente osservabile, che rappresenta una caratteristica psicologica, sociale o comportamentale (Petersen, 2024). Esempi emblematici includono depressione, intelligenza o autostima: questi fenomeni non possono essere misurati in modo diretto, ma vengono inferiti attraverso indicatori osservabili (es. risposte a item di un test, comportamenti registrati). La validità di un costrutto dipende dalla sua definizione teorica e dalla capacità degli indicatori di rappresentarlo in modo coerente.\n\n8.2.1 Tipologie di Costrutti\nLa letteratura distingue due categorie fondamentali, con implicazioni metodologiche divergenti (Bollen & Lennox, 1991):\n\n8.2.1.1 Costrutti Riflessivi\n\n\nDefinizione: Il costrutto latente causa le manifestazioni osservate. Gli indicatori riflettono variazioni nel tratto sottostante.\n\n\nEsempio: L’estroversione si manifesta attraverso comportamenti come “partecipare attivamente a eventi sociali” o “preferire conversazioni con sconosciuti”.\n\n\nProprietà:\n\n\nCorrelazioni attese: Gli indicatori mostrano covarianze sistematiche (es. alta consistenza interna, misurata tramite alpha di Cronbach).\n\n\nIntercambiabilità: Gli item sono replicabili (rimuovere un indicatore non compromette la definizione del costrutto).\n\n\nModelli statistici: Richiedono approcci che catturino la varianza comune (analisi fattoriale confirmatoria, modelli SEM, IRT).\n\n\n\n8.2.1.2 Costrutti Formativi\n\n\nDefinizione: Gli indicatori definiscono il costrutto, agendo come cause dirette. Il costrutto è una sintesi delle sue componenti.\n\n\nEsempio: Lo status socioeconomico (SES) emerge dalla combinazione di educazione, reddito e prestigio occupazionale.\n\n\nProprietà:\n\n\nAssenza di correlazioni: Gli indicatori possono essere indipendenti (es. reddito e educazione non necessariamente covariano).\n\n\nCompletezza: L’omissione di un indicatore altera la definizione del costrutto (es. escludere il reddito distorce la misura del SES).\n\n\nRischi statistici: Alte correlazioni tra indicatori generano multicollinearità, compromettendo regressioni o modelli a equazioni strutturali.\n\n\nModelli statistici: Punteggi sommativi, indici pesati o modelli SEM con relazioni causali esplicite.\n\n\n\n8.2.2 Confronto Sistematico\n\n\n\n\n\n\n\nCaratteristica\nCostrutti Riflessivi\nCostrutti Formativi\n\n\n\nNatura causale\nCostrutto → Indicatori\nIndicatori → Costrutto\n\n\nCorrelazioni\nAttese e desiderabili\nNon necessarie, talvolta indesiderate\n\n\nCampionamento\nItem intercambiabili\nItem essenziali e non ridondanti\n\n\nObiettivo statistico\nMassimizzare varianza comune\nSintetizzare informazioni multiple\n\n\n\n8.2.3 Implicazioni per la Stima\nLa scelta del metodo di misurazione deve allinearsi alla natura teorica del costrutto:\n- Per costrutti riflessivi, modelli basati sulla covarianza (es. analisi fattoriale) identificano la struttura latente sottostante.\n- Per costrutti formativi, approcci compositi (es. punteggi sommativi) o modelli causali (SEM formativo) sono più appropriati.\nSintesi critica: L’ambiguità nella classificazione di un costrutto (riflessivo vs. formativo) può portare a errori metodologici gravi, come l’uso improprio dell’alpha di Cronbach per costrutti formativi. La chiarezza concettuale e il rigore teorico sono prerequisiti per una misurazione valida.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html#lequazione-fondamentale-della-ctt",
    "href": "chapters/ctt/01_ctt_1.html#lequazione-fondamentale-della-ctt",
    "title": "8  Fondamenti teorici",
    "section": "\n8.3 L’Equazione Fondamentale della CTT",
    "text": "8.3 L’Equazione Fondamentale della CTT\nLa Teoria Classica dei Test (CTT) descrive il modo in cui i punteggi ottenuti da un test psicometrico siano legati a un costrutto latente che si intende misurare. Un concetto centrale nella CTT è quello dell’affidabilità, ovvero la capacità del test di produrre risultati coerenti e stabili nel tempo e in differenti condizioni di misurazione. Tale concetto è chiarito dall’equazione fondamentale della CTT, l’Equazione 8.1.\n\n8.3.1 Le due componenti del punteggio osservato\nL’equazione fondamentale evidenzia che ogni punteggio osservato (\\(X\\)) può essere scomposto in due componenti principali:\n\n\nPunteggio vero (\\(T\\)): rappresenta il valore reale, non influenzato da errori, della caratteristica o del costrutto misurato.\n\nErrore di misurazione (\\(E\\)): rappresenta tutte le variazioni casuali che influenzano la misura e impediscono al punteggio osservato di coincidere esattamente con quello vero.\n\n8.3.2 Errore standard di misurazione\nUn aspetto di particolare importanza nella CTT riguarda la varianza dell’errore di misurazione. Più grande è tale varianza, minore sarà la precisione con cui il punteggio vero è rappresentato dal punteggio osservato. Idealmente, se l’errore di misurazione fosse nullo, ogni punteggio osservato corrisponderebbe esattamente al punteggio vero. Tuttavia, nella realtà, gli errori di misurazione sono inevitabili.\nLa deviazione standard dell’errore di misurazione è chiamata errore standard di misurazione e indicata con \\(\\sigma_E\\). Questa quantità misura la dispersione dei punteggi osservati intorno al punteggio vero dovuta agli errori casuali di misurazione. Uno degli obiettivi principali della CTT è proprio stimare \\(\\sigma_E\\) per valutare la qualità di una scala psicometrica.\n\n8.3.3 Affidabilità e precisione del test\nL’affidabilità nella CTT è definita come il rapporto tra la varianza dei punteggi veri e la varianza totale dei punteggi osservati. Un’alta affidabilità indica una bassa incidenza degli errori casuali di misurazione (ossia un piccolo valore di \\(\\sigma_E\\)), garantendo quindi che il punteggio osservato (\\(X\\)) sia una misura accurata del punteggio vero (\\(T\\)). Al contrario, una bassa affidabilità implica una grande incertezza nella misura, evidenziando una significativa discrepanza tra il punteggio osservato e il punteggio vero.\nNelle sezioni successive, esploreremo come il concetto di affidabilità nella CTT sia correlato al coefficiente di determinazione (\\(R^2\\)) utilizzato nella regressione lineare. Inoltre, vedremo come l’errore standard di misurazione nella CTT sia analogo all’errore standard della stima nel modello statistico della regressione.\n\n8.3.4 Il punteggio vero\nSecondo l’Equazione 8.1, il punteggio osservato risulta dalla combinazione di due componenti: una sistematica, cioè il punteggio vero, e una aleatoria, rappresentata dall’errore di misurazione. Ma cosa si intende esattamente con il concetto di punteggio vero? La Teoria Classica dei Test (CTT) propone diverse interpretazioni complementari.\n\nDal punto di vista psicologico, la CTT interpreta un test come una selezione casuale di domande estratte da un insieme più ampio che riflette il costrutto che si vuole misurare (Kline, 2013; Nunnally, 1994). In questa prospettiva, il punteggio vero corrisponde al risultato che un partecipante otterrebbe rispondendo all’intero insieme di domande. L’errore di misurazione rappresenta dunque quanto le specifiche domande selezionate casualmente si discostano dalla totalità delle domande relative al costrutto.\nIn modo analogo, il punteggio vero può essere visto come il risultato della misurazione ideale, cioè non influenzata da fattori estranei al costrutto di interesse, come l’apprendimento durante il test, la fatica, la memoria o la motivazione del soggetto. Poiché la componente aleatoria (errore) è considerata un processo casuale, essa non introduce alcun bias sistematico; infatti, per definizione, la media di \\(E\\) è uguale a zero.\nDa un punto di vista statistico, il punteggio vero è una quantità teorica, non osservabile direttamente, che rappresenta il valore atteso (media) delle misurazioni ottenute somministrando ripetutamente il test allo stesso individuo nelle medesime condizioni:\n\n\\[\nT = \\mathbb{E}(X) \\equiv \\mu_X \\equiv \\mu_{T}.\n\\]\nIntegrando le definizioni precedenti, Lord & Novick (1968) definiscono il punteggio vero come il punteggio medio che un individuo otterrebbe se il test fosse somministrato ripetutamente nelle stesse condizioni, escludendo ogni effetto di apprendimento o affaticamento.\n\n8.3.5 Somministrazioni ripetute\nNella Teoria Classica dei Test (CTT) è possibile distinguere due approcci sperimentali complementari: uno in cui ciascun individuo costituisce un’unità di osservazione che varia casualmente all’interno di un campione, e un altro in cui il punteggio ottenuto da un singolo individuo viene trattato come una variabile casuale derivante da ripetute somministrazioni teoriche dello stesso test. Combinare questi due approcci consente di generalizzare i risultati della CTT—originariamente formulata ipotizzando molteplici somministrazioni immaginarie dello stesso test allo stesso individuo, in condizioni identiche—al caso pratico di una singola somministrazione del test a un campione di individui (Allen & Yen, 2001).\nQuesta generalizzazione si basa sull’assunzione ergodica, la quale consente di considerare la variabilità derivante da somministrazioni ripetute dello stesso test su un singolo individuo come rappresentativa della variabilità tra individui differenti in un unico momento temporale. L’assunzione ergodica è valida se sono soddisfatte le seguenti condizioni:\n\nOmogeneità: tutti gli individui nel campione condividono le stesse proprietà fondamentali del costrutto misurato. Ciò implica che le differenze osservate tra individui non riflettono variazioni sostanziali del costrutto, bensì derivano principalmente dalla variabilità casuale o dall’errore di misurazione.\nStabilità: le caratteristiche individuali misurate dal test rimangono costanti nel tempo durante le ipotetiche somministrazioni ripetute. Pertanto, eventuali variazioni osservate in misurazioni successive dello stesso individuo sono attribuibili esclusivamente all’errore di misura e non a effettivi cambiamenti nel costrutto.\n\nQuando queste condizioni sono rispettate, le principali quantità definite dalla CTT acquisiscono un’interpretazione empirica valida anche in una singola somministrazione a un campione di individui. In particolare:\n\n\n\\(\\sigma^2_X\\) corrisponde alla varianza dei punteggi osservati nel campione o nella popolazione;\n\n\\(\\sigma^2_T\\) corrisponde alla varianza dei punteggi veri, rappresentando la reale differenza tra individui nella popolazione;\n\n\\(\\sigma^2_E\\) rappresenta la varianza degli errori di misurazione nella popolazione.\n\nL’assunzione ergodica, quindi, permette di inferire proprietà della popolazione partendo dalla variabilità osservata in singoli individui, e viceversa, considerando che la variabilità intra-individuale (somministrazioni ripetute su un singolo individuo) e quella inter-individuale (tra individui diversi) siano comparabili sotto le condizioni descritte.\n\n8.3.6 Le assunzioni sul punteggio ottenuto\nLa Teoria Classica dei Test (CTT) assume che il valore atteso del punteggio osservato \\(X\\) sia uguale al valore atteso del punteggio vero \\(T\\):\n\\[\n\\mu_X \\equiv \\mu_{T},\n\\tag{8.2}\\]\nCiò significa che, in media, il punteggio osservato fornisce una stima accurata del punteggio vero (abilità latente). Tuttavia, nella pratica, ogni singolo punteggio osservato sarà influenzato da un errore di misurazione, per cui non coinciderà mai esattamente con il punteggio vero:\n\\[\nE \\equiv X - T.\n\\]\nData l’assunzione che i punteggi osservati siano mediamente uguali ai punteggi veri, ne consegue che l’errore di misurazione ha valore atteso pari a zero:\n\\[\n\\mathbb{E}(E) = \\mathbb{E}(X - T) = \\mathbb{E}(X) - \\mathbb{E}(T) = \\mu_{T} - \\mu_{T} = 0.\n\\]\nIn altre parole, gli errori di misurazione non introducono alcun bias sistematico, ma solo variazione casuale.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html#assiomi-della-teoria-classica",
    "href": "chapters/ctt/01_ctt_1.html#assiomi-della-teoria-classica",
    "title": "8  Fondamenti teorici",
    "section": "\n8.4 Assiomi della Teoria Classica",
    "text": "8.4 Assiomi della Teoria Classica\nLa CTT assume che gli errori siano delle variabili casuali incorrelate tra loro\n\\[\n\\rho(E_i, E_k \\mid T) = 0, \\qquad\\text{con}\\; i \\neq k,\n\\]\ne incorrelate con il punteggio vero,\n\\[\n\\rho(E, T) = 0,\n\\]\nle quali seguono una distribuzione gaussiana con media zero e deviazione standard pari a \\(\\sigma_E\\):\n\\[\nE \\sim \\mathcal{N}(0, \\sigma_E).\n\\]\nLa quantità \\(\\sigma_E\\) è appunto l’errore standard della misurazione. Sulla base di tali assunzioni la CTT deriva la formula dell’attendibilità di un test. Si noti che le assunzioni della CTT hanno una corrispondenza puntuale con le assunzioni su cui si basa il modello di regressione lineare.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html#simulazione-empirica",
    "href": "chapters/ctt/01_ctt_1.html#simulazione-empirica",
    "title": "8  Fondamenti teorici",
    "section": "\n8.5 Simulazione empirica",
    "text": "8.5 Simulazione empirica\nPer chiarire ulteriormente queste idee, consideriamo la seguente simulazione realizzata con il software \\(\\textsf{R}\\). In questa simulazione, i punteggi veri (\\(T\\)) e gli errori di misurazione (\\(E\\)) sono generati rispettando le condizioni della CTT, ovvero:\n\n\n\\(T\\) ed \\(E\\) sono variabili casuali Gaussiane;\n\n\\(T\\) ed \\(E\\) sono tra loro indipendenti (covarianza nulla);\nIl valore atteso di \\(E\\) è zero.\n\nGeneriamo un campione di 100 osservazioni con i seguenti parametri:\n\n\\(T \\sim \\mathcal{N}(\\mu_T = 12, \\sigma^2_T = 6)\\)\n\\(E \\sim \\mathcal{N}(\\mu_E = 0, \\sigma^2_E = 3)\\)\n\n\nset.seed(8394)\n\nn &lt;- 100\nSigma &lt;- matrix(c(6, 0, 0, 3), byrow = TRUE, ncol = 2)\nmu &lt;- c(12, 0)\ndat &lt;- mvrnorm(n, mu, Sigma, empirical = TRUE)\nT &lt;- dat[, 1]\nE &lt;- dat[, 2]\n\nUtilizzando l’opzione empirical = TRUE, otteniamo un campione in cui le medie e le covarianze sono esattamente quelle specificate. Possiamo quindi interpretare questo insieme di dati come rappresentativo di una “popolazione”.\nIl punteggio osservato viene simulato come \\(X = T + E\\):\n\nX &lt;- T + E\n\nLe prime sei osservazioni sono le seguenti:\n\ntibble(X, T, E) |&gt; head()\n#&gt; # A tibble: 6 × 3\n#&gt;       X     T      E\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 15.7  16.8  -1.07 \n#&gt; 2 13.7  12.2   1.41 \n#&gt; 3  6.73  7.85 -1.12 \n#&gt; 4 14.6  14.2   0.388\n#&gt; 5 10.6  10.2   0.420\n#&gt; 6 12.4  13.3  -0.960\n\nIl seguente diagramma di dispersione mostra la relazione tra punteggi veri e osservati:\n\ntibble(X, T) |&gt;\nggplot(aes(T, X)) +\n    geom_point(position = position_jitter(w = .3, h = .3)) +\n    geom_abline(col = \"blue\")\n\n\n\n\n\n\n\nVerifichiamo le assunzioni della CTT per i dati dell’esempio:\n\ncor(E, T)\n#&gt; [1] -4.229e-17\n\n\nplot(density(E))\ncurve(dnorm(x, mean(E), sd(E)), add = TRUE, col = \"red\")\n\n\n\n\n\n\n\nVerifichiamo che la media dei punteggi veri e dei punteggi osservati siano uguali:\n\nmean(T) == mean(X)\n#&gt; [1] TRUE\n\nVerifichiamo anche che l’errore di misurazione abbia valore medio zero:\n\nmean(E)\n#&gt; [1] -8.882e-17\n\nLe varianze dei punteggi veri, dei punteggi osservati e degli errori sono rispettivamente\n\nc(var(T), var(X), var(E))\n#&gt; [1] 6 9 3\n\ncoerentemente con i valori usati nella simulazione.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html#lerrore-standard-della-misurazione-sigma_e",
    "href": "chapters/ctt/01_ctt_1.html#lerrore-standard-della-misurazione-sigma_e",
    "title": "8  Fondamenti teorici",
    "section": "\n8.6 L’errore standard della misurazione (\\(\\sigma_E\\))",
    "text": "8.6 L’errore standard della misurazione (\\(\\sigma_E\\))\nL’errore standard della misurazione (\\(\\sigma_E\\)) è un concetto fondamentale della CTT. Si tratta della deviazione standard degli errori di misurazione e indica la precisione con cui un test riflette il punteggio vero di un individuo. In termini pratici, \\(\\sigma_E\\) rappresenta la quantità media di variazione attesa nei punteggi osservati qualora il test fosse somministrato ripetutamente allo stesso individuo, nelle medesime condizioni (senza effetti di apprendimento o affaticamento).\nUn test con un valore basso di \\(\\sigma_E\\) sarà considerato altamente preciso, poiché i punteggi osservati tenderanno a variare poco intorno al punteggio vero. Viceversa, un valore elevato di \\(\\sigma_E\\) indica una maggiore variabilità dei punteggi osservati, riflettendo una minore precisione nella misura.\nLa stima di questo errore è uno degli obiettivi principali della CTT, perché permette di quantificare l’affidabilità del test.\nNel nostro esempio, l’errore standard della misurazione viene calcolato come segue:\n\nsqrt(var(E))\n#&gt; [1] 1.732\n\nQuesto valore rappresenta quindi la quantità attesa di variazione del punteggio osservato nel caso di somministrazioni ripetute dello stesso test alla medesima persona, nelle stesse condizioni.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html#quattro-livelli-di-misurazione-nella-ctt",
    "href": "chapters/ctt/01_ctt_1.html#quattro-livelli-di-misurazione-nella-ctt",
    "title": "8  Fondamenti teorici",
    "section": "\n8.7 Quattro Livelli di Misurazione nella CTT",
    "text": "8.7 Quattro Livelli di Misurazione nella CTT\nNella Teoria Classica dei Test (CTT), le misure della stessa entità (item, sottoscale o test) possono essere classificate secondo quattro livelli gerarchici di similarità, dal più al meno restrittivo: misure parallele, misure τ-equivalenti, misure essenzialmente τ-equivalenti e misure congeneriche. Questa gerarchia riflette i vincoli progressivamente minori sulle caratteristiche delle misure.\nLa similarità tra due misure è definita dalla relazione tra i punteggi veri (\\(t_i\\) e \\(t_j\\)):\n\\[\nt_i = a_{ij} + b_{ij} t_j ,\n\\]\ndove:\n\n\n\\(a_{ij}\\): differenza sistematica nelle medie tra le misure (intercetta);\n\n\\(b_{ij}\\): differenza di scala rispetto al costrutto latente misurato (pendenza).\n\n\n8.7.1 Misure parallele\nLe misure parallele soddisfano tre condizioni fondamentali:\n\nMedie dei punteggi veri uguali (\\(a_{ij} = 0\\)).\nVarianze dei punteggi veri uguali (\\(b_{ij} = 1\\)).\nVarianze degli errori di misura uguali.\n\n\n\n\n\n\n\nApprofondimento\n\n\n\n\n\nPerché la differenza di intercetta (\\(a_{ij}\\)) corrisponde a una differenza di media?\nQuando aggiungi un termine costante \\(a_{ij}\\) a un punteggio latente \\(t_j\\), stai semplicemente spostando in alto o in basso tutti i valori di \\(t_j\\) della stessa quantità. Questo spostamento costante equivale a cambiare la “posizione” della distribuzione dei punteggi, senza però modificarne la forma (cioè senza modificare deviazione standard o varianza).\n\nSe \\(a_{ij} = 0\\), non c’è spostamento: le due misure hanno la stessa media.\nSe \\(a_{ij} \\neq 0\\), la media di \\(t_i\\) risulterà traslata rispetto alla media di \\(t_j\\) proprio di \\(a_{ij}\\).\n\nIn formula, la media di \\(t_i\\) diventa:\n\\[\n\\mathbb{E}[t_i] \\;=\\; \\mathbb{E}[a_{ij} + b_{ij}\\,t_j]\n\\;=\\; a_{ij} + b_{ij} \\, \\mathbb{E}[t_j].\n\\]\nSe \\(b_{ij} = 1\\), allora \\(\\mathbb{E}[t_i] = a_{ij} + \\mathbb{E}[t_j]\\), e \\(a_{ij}\\) è precisamente lo scostamento nella media.\nPerché la differenza di pendenza (\\(b_{ij}\\)) corrisponde a una differenza di varianza?\nLa pendenza \\(b_{ij}\\) agisce come un fattore di scala sul punteggio \\(t_j\\). Quando un punteggio viene moltiplicato per un coefficiente \\(b_{ij}\\), l’ampiezza (o dispersione) della distribuzione cambia. Più formalmente, la varianza di \\(t_i\\) è data da:\n\\[\n\\mathrm{Var}(t_i)\n\\;=\\; \\mathrm{Var}(a_{ij} + b_{ij}\\, t_j)\n\\;=\\; \\mathrm{Var}(b_{ij}\\, t_j)\n\\;=\\; b_{ij}^2 \\,\\mathrm{Var}(t_j),\n\\]\nperché l’aggiunta di un termine costante (\\(a_{ij}\\)) non influisce mai sulla varianza, mentre la moltiplicazione per una costante (\\(b_{ij}\\)) la cambia in modo proporzionale al quadrato di tale costante (\\(b_{ij}^2\\)).\n\nSe \\(b_{ij} = 1\\), la varianza di \\(t_i\\) coincide esattamente con la varianza di \\(t_j\\): non c’è alcun cambiamento di scala.\nSe \\(b_{ij} \\neq 1\\), la varianza di \\(t_i\\) viene “riscalata” di \\(b_{ij}^2\\). Di conseguenza, le due misure avranno una diversa dispersione (cioè una diversa “ampiezza” nella distribuzione dei punteggi veri).\n\nConclusioni.\n\n\n\\(a_{ij}\\) rappresenta uno “shift” costante: indica quanto le due misure differiscono in termini di media (posizione della distribuzione).\n\n\\(b_{ij}\\) rappresenta un fattore di scala: indica quanto le due misure differiscono in termini di varianza (grandezza o dispersione della distribuzione).\n\nQuesti due aspetti sono centrali nella definizione dei livelli di similarità fra misure in CTT:\n\n\nMisure parallele richiedono nessuna differenza di media (quindi \\(a_{ij}=0\\)) e nessuna differenza di varianza (quindi \\(b_{ij}=1\\)), oltre a errori di misura uguali.\n\nStrutture meno restrittive (τ-equivalenti, essenzialmente τ-equivalenti, congeneriche) permettono sempre più libertà nei valori di \\(a_{ij}\\) e \\(b_{ij}\\).\n\nEcco perché, nel caso delle misure parallele, il requisito è che non ci sia né uno shift costante (\\(a_{ij} = 0\\)) né un fattore di riscalamento (\\(b_{ij} = 1\\)), in modo che i due test condividano media e varianza vera, oltre ad avere la stessa precisione di misura (varianza d’errore).\n\n\n\nSimulazione in R:\n\nset.seed(123)\nn &lt;- 1000\nt1 &lt;- rnorm(n, mean = 20, sd = 5)\ne1 &lt;- rnorm(n, mean = 0, sd = 2)\nx1 &lt;- t1 + e1\n\nt2 &lt;- t1\ne2 &lt;- rnorm(n, mean = 0, sd = 2)\nx2 &lt;- t2 + e2\n\ncor(x1, x2)\n#&gt; [1] 0.869\n\n\n8.7.2 Misure τ-equivalenti\nLe misure τ-equivalenti soddisfano:\n\nMedie dei punteggi veri uguali (\\(a_{ij} = 0\\)).\nVarianze dei punteggi veri uguali (\\(b_{ij} = 1\\)).\nPossibile differenza nelle varianze degli errori di misura.\n\nSimulazione in R:\n\nset.seed(123)\nt1 &lt;- rnorm(n, mean = 20, sd = 5)\ne1 &lt;- rnorm(n, mean = 0, sd = 2)\nx1 &lt;- t1 + e1\n\nt2 &lt;- t1\ne2 &lt;- rnorm(n, mean = 0, sd = 4)\nx2 &lt;- t2 + e2\n\ncor(x1, x2)\n#&gt; [1] 0.733\n\n\n8.7.3 Misure essenzialmente τ-equivalenti\nLe misure essenzialmente τ-equivalenti permettono:\n\nDifferenze sistematiche nelle medie dei punteggi veri (\\(a_{ij} \\neq 0\\)).\nVarianze dei punteggi veri uguali (\\(b_{ij} = 1\\)).\nDifferenze nelle varianze degli errori di misura.\n\nSimulazione in R:\n\nset.seed(123)\nt1 &lt;- rnorm(n, mean = 20, sd = 5)\ne1 &lt;- rnorm(n, mean = 0, sd = 2)\nx1 &lt;- t1 + e1\n\nt2 &lt;- 5 + t1\ne2 &lt;- rnorm(n, mean = 0, sd = 4)\nx2 &lt;- t2 + e2\n\ncor(x1, x2)\n#&gt; [1] 0.733\n\n\n8.7.4 Misure congeneriche\nLe misure congeneriche sono il modello più generale e consentono:\n\nDifferenze sistematiche nelle medie dei punteggi veri (\\(a_{ij} \\neq 0\\)).\nDifferenze nella scala dei punteggi veri (\\(b_{ij} \\neq 1\\)).\nDifferenze nelle varianze degli errori di misura.\n\nSimulazione in R:\n\nset.seed(123)\nt1 &lt;- rnorm(n, mean = 20, sd = 5)\ne1 &lt;- rnorm(n, mean = 0, sd = 2)\nx1 &lt;- t1 + e1\n\nt2 &lt;- 5 + 0.8 * t1\ne2 &lt;- rnorm(n, mean = 0, sd = 4)\nx2 &lt;- t2 + e2\n\ncor(x1, x2)\n#&gt; [1] 0.6638\n\nQueste simulazioni permettono di comprendere chiaramente le implicazioni pratiche e teoriche dei diversi livelli di similarità nella misurazione secondo la CTT.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html#riflessioni-conclusive",
    "href": "chapters/ctt/01_ctt_1.html#riflessioni-conclusive",
    "title": "8  Fondamenti teorici",
    "section": "\n8.8 Riflessioni Conclusive",
    "text": "8.8 Riflessioni Conclusive\nQuesto capitolo ha offerto una panoramica dei concetti chiave della teoria classica dei test (CTT) e ha introdotto quattro tipi di misure psicometriche. Le misure parallele si distinguono per l’elevata somiglianza nei punteggi veri, garantendo che le varianze siano uguali per tutte le misure. Le misure τ-equivalenti condividono questa equivalenza nelle varianze dei punteggi veri, ma non richiedono una somiglianza così stretta come le misure parallele. Le misure essenzialmente τ-equivalenti tollerano una maggiore variabilità nei punteggi veri, pur mantenendo la coerenza dei risultati. Infine, le misure congeneriche presentano le minori restrizioni tra le quattro tipologie, consentendo differenze sia nelle medie sia nelle varianze dei punteggi veri.\nComprendere le differenze tra queste tipologie di misure è fondamentale per valutare l’affidabilità e la validità di un test e per interpretare in modo accurato i risultati. Nelle prossime sezioni della dispensa, approfondiremo l’applicazione pratica della CTT nello sviluppo e nella valutazione dei test psicometrici. Per un’esplorazione più dettagliata, si rimanda alle letture di riferimento: McDonald (2013) e Lord & Novick (1968).",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html#session-info",
    "href": "chapters/ctt/01_ctt_1.html#session-info",
    "title": "8  Fondamenti teorici",
    "section": "\n8.9 Session Info",
    "text": "8.9 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.2\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] modelsummary_2.3.0 ggokabeito_0.1.0   see_0.11.0        \n#&gt;  [4] MASS_7.3-65        viridis_0.6.5      viridisLite_0.4.2 \n#&gt;  [7] ggpubr_0.6.0       ggExtra_0.10.1     gridExtra_2.3     \n#&gt; [10] patchwork_1.3.0    bayesplot_1.11.1   semTools_0.5-6    \n#&gt; [13] semPlot_1.1.6      lavaan_0.6-19      psych_2.4.12      \n#&gt; [16] scales_1.3.0       markdown_1.13      knitr_1.50        \n#&gt; [19] lubridate_1.9.4    forcats_1.0.0      stringr_1.5.1     \n#&gt; [22] dplyr_1.1.4        purrr_1.0.4        readr_2.1.5       \n#&gt; [25] tidyr_1.3.1        tibble_3.2.1       ggplot2_3.5.1     \n#&gt; [28] tidyverse_2.0.0    here_1.0.1        \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.1      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.2.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] mime_0.13           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [25] Matrix_1.7-3        R6_2.6.1            fastmap_1.2.0      \n#&gt;  [28] rbibutils_2.3       shiny_1.10.0        digest_0.6.37      \n#&gt;  [31] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n#&gt;  [34] rprojroot_2.0.4     Hmisc_5.2-3         labeling_0.4.3     \n#&gt;  [37] timechange_0.3.0    abind_1.4-8         compiler_4.4.2     \n#&gt;  [40] withr_3.0.2         glasso_1.11         htmlTable_2.4.3    \n#&gt;  [43] backports_1.5.0     carData_3.0-5       ggsignif_0.6.4     \n#&gt;  [46] corpcor_1.6.10      gtools_3.9.5        tools_4.4.2        \n#&gt;  [49] pbivnorm_0.6.0      foreign_0.8-88      zip_2.3.2          \n#&gt;  [52] httpuv_1.6.15       nnet_7.3-20         glue_1.8.0         \n#&gt;  [55] quadprog_1.5-8      nlme_3.1-167        promises_1.3.2     \n#&gt;  [58] lisrelToR_0.3       grid_4.4.2          checkmate_2.3.2    \n#&gt;  [61] cluster_2.1.8.1     reshape2_1.4.4      generics_0.1.3     \n#&gt;  [64] gtable_0.3.6        tzdb_0.5.0          data.table_1.17.0  \n#&gt;  [67] hms_1.1.3           car_3.1-3           tables_0.9.31      \n#&gt;  [70] sem_3.1-16          pillar_1.10.1       rockchalk_1.8.157  \n#&gt;  [73] later_1.4.1         splines_4.4.2       lattice_0.22-6     \n#&gt;  [76] survival_3.8-3      kutils_1.73         tidyselect_1.2.1   \n#&gt;  [79] miniUI_0.1.1.1      pbapply_1.7-2       reformulas_0.4.0   \n#&gt;  [82] stats4_4.4.2        xfun_0.51           qgraph_1.9.8       \n#&gt;  [85] arm_1.14-4          stringi_1.8.4       yaml_2.3.10        \n#&gt;  [88] pacman_0.5.1        boot_1.3-31         evaluate_1.0.3     \n#&gt;  [91] codetools_0.2-20    mi_1.1              cli_3.6.4          \n#&gt;  [94] RcppParallel_5.1.10 rpart_4.1.24        xtable_1.8-4       \n#&gt;  [97] Rdpack_2.6.3        munsell_0.5.1       Rcpp_1.0.14        \n#&gt; [100] coda_0.19-4.1       png_0.1-8           XML_3.99-0.18      \n#&gt; [103] parallel_4.4.2      jpeg_0.1-10         lme4_1.1-36        \n#&gt; [106] mvtnorm_1.3-3       openxlsx_4.2.8      rlang_1.1.5        \n#&gt; [109] multcomp_1.4-28     mnormt_2.1.1\n\n\n\n\n\nAllen, M. J., & Yen, W. M. (2001). Introduction to measurement theory. Waveland Press.\n\n\nBollen, K., & Lennox, R. (1991). Conventional wisdom on measurement: A structural equation perspective. Psychological bulletin, 110(2), 305–314.\n\n\nKline, P. (2013). Handbook of psychological testing. Routledge.\n\n\nLord, F. M., & Novick, M. R. (1968). Statistical theories of mental test scores. Addison-Wesley.\n\n\nMcDonald, R. P. (2013). Test theory: A unified treatment. Psychology Press.\n\n\nNunnally, J. C. (1994). Psychometric theory. McGraw-Hill.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.\n\n\nSpearman, C. (1904). General intelligence objectively determined and measured. American Journal of Psychology, 15, 201–293.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html",
    "href": "chapters/fa/02_analisi_fattoriale_1.html",
    "title": "20  Il modello unifattoriale",
    "section": "",
    "text": "20.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nL’analisi fattoriale è una tecnica statistica che permette di spiegare le correlazioni tra variabili osservate attraverso la loro dipendenza da uno o più fattori latenti. In questo modello, le \\(p\\) variabili osservate (ad esempio, item di un questionario o indicatori comportamentali) sono considerate condizionalmente indipendenti, dato l’insieme di \\(m\\) fattori comuni non osservabili. L’obiettivo principale è interpretare tali fattori come costrutti teorici latenti sottostanti alle risposte osservate.\nAd esempio, si può utilizzare l’analisi fattoriale per spiegare le correlazioni tra le prestazioni di un gruppo di individui in diversi compiti cognitivi attraverso un singolo fattore latente come l’intelligenza generale. In questo modo, l’analisi fattoriale consente di identificare i costrutti a cui gli item si riferiscono e di quantificare quanto ciascun item contribuisca a rappresentarli.\nIl modello può prevedere uno (modello unifattoriale, \\(m = 1\\)) o più fattori latenti (modello multifattoriale, \\(m &gt; 1\\)). In questo capitolo ci concentreremo sul modello unifattoriale, che assume l’esistenza di un unico fattore comune che influenza tutte le variabili osservate.\nNel contesto dell’analisi fattoriale, le variabili latenti rappresentano costrutti teorici non direttamente osservabili, come abilità cognitive o tratti psicologici. Queste variabili riflettono le comunanze sottostanti a un insieme di indicatori osservabili, chiamati variabili manifeste. Le prime sono rappresentate nei diagrammi di percorso come cerchi, mentre le seconde come quadrati.\nIl legame tra fattore latente e variabili manifeste è descritto tramite i carichi fattoriali (\\(\\lambda\\)), che quantificano l’intensità dell’influenza esercitata dal fattore latente su ciascuna variabile osservata. Il valore di \\(\\lambda\\) indica la proporzione di varianza della variabile osservata che è spiegata dal fattore comune: più alto è il carico, maggiore è la rappresentatività dell’item rispetto al costrutto latente.\nDal punto di vista matematico, ciascuna misura osservabile \\(y\\) è modellata come una combinazione lineare del fattore latente \\(\\xi\\), ponderato dal carico \\(\\lambda\\), più un termine di errore specifico \\(\\delta\\), che rappresenta la componente di varianza non spiegata dal fattore comune.\nUn esempio intuitivo può chiarire il significato di questa decomposizione: immaginate di usare una bilancia imprecisa per misurare il peso corporeo. Ogni misurazione (\\(y\\)) rifletterà in parte il peso reale della persona (\\(\\xi\\)), ma anche un errore casuale (\\(\\delta\\)) dovuto all’inaffidabilità dello strumento.\nQuando si dispone di più variabili osservabili \\(y\\) che fanno riferimento a un medesimo costrutto latente \\(\\xi\\), diventa possibile stimare con maggiore precisione sia il punteggio latente sia la componente di errore. Questo consente di ottenere una rappresentazione più affidabile del costrutto teorico di interesse e di migliorare l’interpretazione psicometrica dei dati.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#modello-monofattoriale",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#modello-monofattoriale",
    "title": "20  Il modello unifattoriale",
    "section": "\n20.2 Modello monofattoriale",
    "text": "20.2 Modello monofattoriale\nNel caso di un solo fattore comune e \\(p\\) variabili manifeste \\(y_i\\), il modello assume la forma:\n\\[\n\\begin{equation}\ny_i = \\mu_i + \\lambda_{i} \\xi + 1 \\cdot \\delta_i \\qquad i = 1, \\dots, p,\n\\end{equation}\n\\tag{20.1}\\]\ndove:\n\n\n\\(\\mu_i\\) è la media della variabile osservata \\(y_i\\),\n\n\\(\\xi\\) è il fattore latente comune a tutte le variabili osservate,\n\n\\(\\lambda_i\\) è la saturazione fattoriale, ovvero il peso del fattore comune \\(\\xi\\) sulla variabile \\(y_i\\),\n\n\\(\\delta_i\\) è il fattore specifico (o errore unico) associato a \\(y_i\\), indipendente da \\(\\xi\\).\n\nSi assume che:\n\nil fattore comune \\(\\xi\\) abbia media zero e varianza unitaria,\ni fattori specifici \\(\\delta_i\\) abbiano media zero, varianza \\(\\psi_i\\), e siano indipendenti tra loro e dal fattore comune.\n\nIn questo modello, ciascuna variabile osservata \\(y_i\\) è influenzata da una componente condivisa (\\(\\xi\\)) e da una componente specifica (\\(\\delta_i\\)), che rappresenta la parte di varianza unica della variabile non spiegata dal fattore comune.\nIl modello di analisi fattoriale può ricordare formalmente il modello di regressione lineare, ma esistono alcune differenze fondamentali. Innanzitutto, sia il fattore comune \\(\\xi\\) sia i fattori specifici \\(\\delta_i\\) sono variabili latenti, cioè non osservabili direttamente. Di conseguenza, tutti i termini presenti nel lato destro dell’equazione sono incogniti. Inoltre, i due modelli perseguono obiettivi differenti: la regressione lineare mira a identificare variabili indipendenti osservabili che spiegano la varianza di una variabile dipendente, mentre l’analisi fattoriale cerca di individuare una o più variabili latenti che rendano conto della covarianza tra un insieme di variabili osservate.\nPer semplicità, si assume spesso che le variabili osservate siano centrate, cioè che la loro media sia pari a zero (\\(\\mu_i = 0\\)). Questo equivale a considerare ogni \\(y_i\\) come uno scarto rispetto alla propria media. Il modello unifattoriale può quindi essere riscritto nella forma:\n\\[\n\\begin{equation}\ny_i - \\mu_i = \\lambda_i \\xi + 1 \\cdot \\delta_i,\n\\end{equation}\n\\tag{20.2}\\]\ndove:\n\n\n\\(\\lambda_i\\) è la saturazione (o carico) fattoriale della variabile \\(y_i\\) sul fattore comune \\(\\xi\\),\n\n\\(\\delta_i\\) rappresenta la componente specifica della variabile \\(y_i\\), ovvero la parte di varianza non condivisa con le altre variabili.\n\nSi assume che:\n\nil fattore comune \\(\\xi\\) abbia media zero e varianza unitaria (\\(\\mathbb{E}[\\xi] = 0\\), \\(\\mathrm{Var}(\\xi) = 1\\)),\nciascun fattore specifico \\(\\delta_i\\) abbia media zero, varianza \\(\\psi_i\\) e sia incorrelato con \\(\\xi\\) e con gli altri \\(\\delta_j\\) (per \\(j \\ne i\\)).\n\nSotto queste assunzioni, l’interdipendenza tra le variabili osservate è interamente spiegata dalla loro dipendenza dal fattore comune. I fattori specifici, invece, rendono conto della varianza residua non condivisa.\nQuesta formulazione permette di derivare espressioni analitiche per quantità fondamentali come:\n\nla covarianza tra una variabile osservata \\(y_i\\) e il fattore comune \\(\\xi\\),\nla varianza della variabile osservata \\(y_i\\),\nla covarianza tra due variabili osservate \\(y_i\\) e \\(y_k\\).\n\nL’obiettivo di questo capitolo è analizzare nel dettaglio tali quantità e mostrare come esse riflettano la struttura latente imposta dal modello unifattoriale.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#correlazione-parziale",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#correlazione-parziale",
    "title": "20  Il modello unifattoriale",
    "section": "\n20.3 Correlazione parziale",
    "text": "20.3 Correlazione parziale\nPrima di introdurre formalmente il modello statistico dell’analisi fattoriale, è utile chiarire il concetto di correlazione parziale, centrale per comprendere la logica della separazione tra fattori comuni e specifici.\nL’analisi fattoriale è spesso fatta risalire agli studi di Charles Spearman. Nel 1904, Spearman pubblicò un articolo intitolato “General Intelligence, Objectively Determined and Measured”, nel quale propose la Teoria dei Due Fattori. In quel lavoro, mostrò che era possibile identificare un fattore latente a partire da una matrice di correlazioni osservate, utilizzando la tecnica dell’annullamento della tetrade (tetrad differences). Questa tecnica si basa sul principio della correlazione parziale e mira a verificare se, una volta controllati gli effetti di variabili latenti (i fattori \\(\\xi_j\\)), le correlazioni tra le variabili osservate \\(Y_i\\) risultino nulle.\nPer illustrare il concetto, consideriamo un esempio con tre variabili: \\(Y_1\\), \\(Y_2\\) e una variabile comune \\(F\\). La correlazione semplice \\(r_{12}\\) tra \\(Y_1\\) e \\(Y_2\\) può riflettere l’influenza condivisa di \\(F\\) su entrambe. La correlazione parziale tra \\(Y_1\\) e \\(Y_2\\) al netto di \\(F\\) misura invece la relazione diretta tra \\(Y_1\\) e \\(Y_2\\) una volta rimosso l’effetto lineare di \\(F\\) da entrambe le variabili.\nPer farlo, si calcolano i residui delle regressioni di \\(Y_1\\) e \\(Y_2\\) su \\(F\\), ovvero le componenti ortogonali a \\(F\\). Ad esempio, nel caso di \\(Y_1\\):\n\\[\nY_1 = b_{01} + b_{11}F + E_1,\n\\tag{20.3}\\]\ndove \\(E_1\\) è la parte di \\(Y_1\\) linearmente indipendente da \\(F\\). Ripetendo l’operazione per \\(Y_2\\) si ottiene un residuo \\(E_2\\), anch’esso ortogonale a \\(F\\). La correlazione di Pearson tra \\(E_1\\) ed \\(E_2\\) rappresenta la correlazione parziale tra \\(Y_1\\) e \\(Y_2\\) dato \\(F\\).\nLa stessa quantità può essere calcolata direttamente a partire dalle correlazioni semplici tra le tre variabili, tramite la formula:\n\\[\n\\begin{equation}\nr_{1,2 \\mid F} = \\frac{r_{12} - r_{1F}r_{2F}}{\\sqrt{(1-r_{1F}^2)(1-r_{2F}^2)}}.\n\\end{equation}\n\\tag{20.4}\\]\nQuesta formula mostra che la correlazione parziale è ottenuta sottraendo al valore osservato di \\(r_{12}\\) il prodotto delle correlazioni tra \\(Y_1\\) e \\(F\\) e tra \\(Y_2\\) e \\(F\\), e normalizzando per la varianza residua non spiegata da \\(F\\).\n\n20.3.1 Esempio numerico\nSupponiamo di avere una variabile \\(f\\) generata da una distribuzione normale:\n\nset.seed(123)\nn &lt;- 1000\nf &lt;- rnorm(n, 24, 12)\n\nCostruiamo due variabili, \\(y_1\\) e \\(y_2\\), come combinazioni lineari di \\(f\\) più un errore casuale:\n\ny1 &lt;- 10 + 7 * f + rnorm(n, 0, 50)\ny2 &lt;- 3  + 2 * f + rnorm(n, 0, 50)\n\nLe tre variabili sono correlate; in particolare \\(y_1\\) e \\(y_2\\) hanno una correlazione semplice \\(r_{12} = 0.380\\):\n\nY &lt;- cbind(y1, y2, f)\ncor(Y) |&gt;\n    round(3)\n#&gt;       y1    y2     f\n#&gt; y1 1.000 0.380 0.867\n#&gt; y2 0.380 1.000 0.423\n#&gt; f  0.867 0.423 1.000\n\nPer calcolare la correlazione parziale tra \\(y_1\\) e \\(y_2\\) al netto di \\(f\\), eseguiamo due modelli di regressione lineare:\n\nfm1 &lt;- lm(y1 ~ f)\nfm2 &lt;- lm(y2 ~ f)\n\nOgni osservazione viene così scomposta in due componenti: i valori adattati \\(\\hat{y}_i\\) (dipendenti da \\(f\\)) e i residui \\(e_i\\) (indipendenti da \\(f\\)):\n\ncbind(y1, y1.hat=fm1$fit, e=fm1$res, sum=fm1$fit+fm1$res) |&gt;\n    head() |&gt;\n    round(3)\n#&gt;       y1 y1.hat        e    sum\n#&gt; 1  81.13  130.5  -49.375  81.13\n#&gt; 2 106.67  159.7  -53.037 106.67\n#&gt; 3 308.03  317.8   -9.813 308.03\n#&gt; 4 177.31  186.3   -8.971 177.31\n#&gt; 5  61.39  191.5 -130.089  61.39\n#&gt; 6 374.09  331.7   42.426 374.09\n\nLa correlazione parziale tra \\(y_1\\) e \\(y_2\\) è quindi calcolabile come la correlazione tra i residui:\n\ncor(fm1$res, fm2$res)\n#&gt; [1] 0.02829\n\nNel nostro esempio, la correlazione parziale tra \\(y_1\\) e \\(y_2\\) al netto di \\(f\\) è \\(r_{12|f} = 0.028\\), praticamente nulla. Questo indica che la correlazione osservata tra \\(y_1\\) e \\(y_2\\) (\\(r = 0.380\\)) era dovuta esclusivamente all’influenza condivisa di \\(f\\) su entrambe.\nIn altre parole, una volta eliminato l’effetto di \\(f\\), non rimane alcuna associazione lineare diretta tra \\(y_1\\) e \\(y_2\\). Le due variabili sono quindi condizionalmente indipendenti dato \\(f\\): le componenti di \\(y_1\\) e \\(y_2\\) non spiegate da \\(f\\) risultano tra loro incorrelate.\nInfine, possiamo verificare che il valore ottenuto con la formula della correlazione parziale coincide con quello calcolato sui residui:\n\nR &lt;- cor(Y)\n\n(R[1, 2] - R[1, 3] * R[2, 3]) / \n  sqrt((1 - R[1, 3]^2) * (1- R[2, 3]^2)) |&gt;\n  round(3)\n#&gt; [1] 0.02828\n\nIl risultato conferma la coerenza tra il calcolo algebrico e il metodo dei residui. Questo esempio evidenzia come la correlazione parziale sia uno strumento fondamentale per isolare le relazioni dirette tra variabili, rimuovendo l’influenza di fattori comuni non osservabili.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#principio-base-dellanalisi-fattoriale",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#principio-base-dellanalisi-fattoriale",
    "title": "20  Il modello unifattoriale",
    "section": "\n20.4 Principio base dell’analisi fattoriale",
    "text": "20.4 Principio base dell’analisi fattoriale\nOggi, l’inferenza statistica in ambito fattoriale si basa prevalentemente su metodi di stima per massima verosimiglianza, ottenuti attraverso procedure iterative. Tuttavia, nelle fasi iniziali dello sviluppo dell’analisi fattoriale, l’estrazione dei fattori si fondava su proprietà invarianti che il modello fattoriale impone alla matrice di covarianza (o di correlazione) delle variabili osservate. Tra queste proprietà, la più nota è quella dell’annullamento della tetrade, caratteristica distintiva dei modelli a un solo fattore.\nUna tetrade è una combinazione lineare di quattro correlazioni tra variabili osservate. Se le correlazioni tra le variabili possono essere spiegate da un singolo fattore latente comune, allora è possibile costruire combinazioni di correlazioni la cui differenza si annulla. In altri termini, il modello a un fattore impone vincoli strutturali tali da rendere nulle alcune espressioni algebriche (le tetradi), che dipendono unicamente dalla matrice di correlazione.\nL’analisi fattoriale può dunque essere formulata come un problema di ricerca di un insieme ristretto di variabili latenti (\\(m &lt; p\\)), tali che, una volta controllato il loro effetto, tutte le correlazioni parziali tra le variabili osservate \\(y_i\\) risultino nulle. Se l’annullamento delle correlazioni parziali è confermato, lo psicologo può concludere che esistono \\(m\\) fattori latenti capaci di spiegare la struttura di covarianza del sistema osservato.\nPer chiarire il principio, consideriamo la seguente matrice di correlazione, costruita in modo da riflettere un modello con un unico fattore latente \\(\\xi\\):\n\n\n\n\\(\\xi\\)\n\\(y_1\\)\n\\(y_2\\)\n\\(y_3\\)\n\\(y_4\\)\n\\(y_5\\)\n\n\n\n\\(\\xi\\)\n1.00\n\n\n\n\n\n\n\n\\(y_1\\)\n0.90\n1.00\n\n\n\n\n\n\n\\(y_2\\)\n0.80\n0.72\n1.00\n\n\n\n\n\n\\(y_3\\)\n0.70\n0.63\n0.56\n1.00\n\n\n\n\n\\(y_4\\)\n0.60\n0.54\n0.48\n0.42\n1.00\n\n\n\n\\(y_5\\)\n0.50\n0.45\n0.40\n0.35\n0.30\n1.00\n\n\n\nIn questa matrice, ogni variabile \\(y_i\\) ha una correlazione positiva con \\(\\xi\\), e le correlazioni tra le \\(y_i\\) sono coerenti con un modello in cui \\(\\xi\\) agisce come unica fonte comune di covarianza. Per esempio, la correlazione parziale tra \\(y_3\\) e \\(y_5\\) al netto di \\(\\xi\\) risulta:\n\\[\n\\begin{align}\n  r_{35 \\mid \\xi} &= \\frac{r_{35} - r_{3\\xi}r_{5\\xi}}{\\sqrt{(1-r_{3\\xi}^2)(1-r_{5\\xi}^2)}} \\notag \\\\[12pt]\n  &= \\frac{0.35 - 0.7 \\times 0.5}{\\sqrt{(1 - 0.7^2)(1 - 0.5^2)}} = 0. \\notag\n\\end{align}\n\\]\nLo stesso vale per qualunque altra coppia di variabili: tutte le correlazioni parziali condizionate a \\(\\xi\\) sono nulle, cioè \\(r_{ij \\mid \\xi} = 0\\) per ogni \\(i \\ne j\\).\nIn questa matrice, ci sono \\(p(p-1)/2 = 5(5-1)/2 = 10\\) correlazioni tra le variabili osservate, tutte spiegate dal singolo fattore \\(\\xi\\). Questo non sorprende, poiché la matrice è stata costruita appositamente per rispettare questa proprietà.\nTuttavia, immaginiamo ora di trovarci in una situazione reale, in cui osserviamo soltanto le variabili \\(y_1, \\dots, y_5\\), ma non abbiamo accesso diretto a \\(\\xi\\). Possiamo allora porci la seguente domanda: esiste una variabile latente \\(\\xi\\) tale che, se ne controllassimo l’effetto, tutte le correlazioni parziali tra le variabili osservate risulterebbero nulle?\nSe una tale variabile esiste, e riesce a spiegare completamente le interdipendenze osservate tra le \\(y_i\\), essa assume lo status di fattore.\n\nDefinizione 20.1 Un fattore è una variabile latente non osservabile che, una volta controllata, rende significativamente nulle tutte le correlazioni parziali tra le variabili manifeste.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#vincoli-sulle-correlazioni",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#vincoli-sulle-correlazioni",
    "title": "20  Il modello unifattoriale",
    "section": "\n20.5 Vincoli sulle correlazioni",
    "text": "20.5 Vincoli sulle correlazioni\nCome possiamo determinare se esiste una variabile latente \\(\\xi\\) in grado di spiegare tutte le correlazioni osservate tra le variabili manifeste, rendendo nulle le loro correlazioni parziali? Possiamo partire dalla formula della correlazione parziale, già introdotta in precedenza (vedi Equazione 20.4), e riscriverla per esprimere la correlazione parziale tra due variabili osservate \\(y_i\\) e \\(y_j\\) condizionata a \\(\\xi\\):\n\\[\n\\begin{align}\nr_{ij \\mid \\xi} = \\frac{r_{ij} - r_{i\\xi} \\, r_{j\\xi}}{\\sqrt{(1 - r_{i\\xi}^2)(1 - r_{j\\xi}^2)}}.\n\\end{align}\n\\]\nAffinché la correlazione parziale \\(r_{ij \\mid \\xi}\\) sia uguale a zero — cioè affinché \\(y_i\\) e \\(y_j\\) risultino condizionalmente indipendenti dato \\(\\xi\\) — il numeratore della frazione deve annullarsi. Questa condizione si traduce nella seguente equazione:\n\\[\nr_{ij} = r_{i\\xi} \\cdot r_{j\\xi}.\n\\tag{20.5}\\]\nIn altri termini, la correlazione tra ogni coppia di variabili osservate deve essere pari al prodotto delle correlazioni tra ciascuna variabile e il fattore comune \\(\\xi\\). Questa relazione vincola la struttura della matrice di correlazione osservata e costituisce una delle implicazioni fondamentali del modello fattoriale unifattoriale.\nIl principio è il seguente: se un unico fattore latente \\(\\xi\\) è in grado di spiegare tutte le correlazioni tra le variabili osservate \\(y_i\\), allora ciascuna correlazione \\(r_{ij}\\) deve poter essere scritta come il prodotto \\(r_{i\\xi} \\cdot r_{j\\xi}\\).\nQuesta condizione è il cuore del modello fattoriale a un fattore: tutte le interdipendenze tra le variabili manifeste sono attribuibili alla loro relazione con una variabile latente comune. Se la struttura delle correlazioni osservate viola questa condizione, il modello a un fattore non è compatibile con i dati, e sarà necessario considerare un modello con due o più fattori.\nDal punto di vista pratico, questo principio può essere utilizzato per valutare se una matrice di correlazione è compatibile con un modello fattoriale semplice. Ad esempio, per ogni terna di variabili si possono calcolare differenze di tetradi, cioè differenze tra prodotti incrociati di correlazioni. Se tali differenze risultano sistematicamente prossime a zero, ciò suggerisce che la struttura osservata potrebbe essere spiegata da un unico fattore comune.\nNei prossimi paragrafi analizzeremo in dettaglio la procedura per stimare formalmente le relazioni tra fattori e variabili manifeste.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#teoria-dei-due-fattori",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#teoria-dei-due-fattori",
    "title": "20  Il modello unifattoriale",
    "section": "\n20.6 Teoria dei Due Fattori",
    "text": "20.6 Teoria dei Due Fattori\nPer illustrare in modo concreto il principio dell’annullamento della tetrade, consideriamo un esempio tratto dallo studio originale di Spearman (1904). In uno dei suoi primi lavori, Spearman raccolse una serie di misurazioni delle capacità intellettive su un campione di studenti, includendo sia prestazioni scolastiche sia abilità percettive.\nLe sei variabili considerate furono:\n\nClassics: rendimento nello studio dei classici;\nFrench: rendimento in lingua francese;\nEnglish: rendimento in letteratura inglese;\nMath: rendimento in matematica;\nPitch: abilità nella discriminazione dell’altezza dei suoni;\nMusic: competenza musicale.\n\nPer semplicità, nella discussione seguente considereremo solo tre materie scolastiche (studi classici, \\(c\\), letteratura inglese, \\(e\\), e matematica, \\(m\\)) e la discriminazione dell’altezza dei suoni, \\(p\\). Nel suo studio, Spearman riportò le seguenti correlazioni:\n\\[\n\\begin{array}{ccccc}\n  \\hline\n    & y_c & y_e & y_m & y_p \\\\\n  \\hline\n  y_c & 1.00 & 0.78 & 0.70 & 0.66 \\\\\n  y_e &      & 1.00 & 0.64 & 0.54 \\\\\n  y_m &      &      & 1.00 & 0.45 \\\\\n  y_p &      &      &      & 1.00 \\\\\n  \\hline\n\\end{array}\n\\]\nSecondo la Teoria dei Due Fattori, proposta da Spearman, ogni prestazione intellettiva può essere scomposta in due componenti:\n\nun fattore generale (g), comune a tutti i compiti cognitivi;\nun fattore specifico (s), unico per ciascun compito.\n\nIl fattore \\(g\\) rappresenta la componente stabile e condivisa dell’intelligenza, mentre ciascun fattore \\(s\\) spiega la varianza residua specifica della singola prova. La domanda centrale è: esiste un’unica variabile latente in grado di spiegare le covarianze osservate tra le variabili manifeste?\nPer rispondere, Spearman utilizzò il metodo dell’annullamento della tetrade, che si basa sulle implicazioni della correlazione parziale. Abbiamo visto in precedenza che, se una variabile latente \\(\\xi\\) è in grado di rendere nulle le correlazioni parziali tra le variabili osservate, allora:\n\\[\nr_{ij} = r_{i\\xi} \\cdot r_{j\\xi}.\n\\]\nNel contesto dei dati di Spearman, ciò significa ad esempio che la correlazione osservata tra “studi classici” (\\(c\\)) e “letteratura inglese” (\\(e\\)) deve essere uguale al prodotto delle loro correlazioni con il fattore comune \\(\\xi\\): \\(r_{ec} = \\lambda_e \\cdot \\lambda_c\\). Lo stesso vale per tutte le altre coppie di variabili.\nLe correlazioni tra le variabili manifeste e il fattore latente sono chiamate saturazioni fattoriali e vengono denotate con \\(\\lambda\\). Il vincolo fondamentale del modello fattoriale è che ogni correlazione osservata può essere scomposta come prodotto di due saturazioni.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#annullamento-della-tetrade",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#annullamento-della-tetrade",
    "title": "20  Il modello unifattoriale",
    "section": "\n20.7 Annullamento della tetrade",
    "text": "20.7 Annullamento della tetrade\nIl metodo dell’annullamento della tetrade si basa sull’assunto centrale del modello fattoriale unifattoriale: se un singolo fattore comune spiega tutte le covarianze tra le variabili osservate, allora la correlazione tra ciascuna coppia di variabili può essere espressa come il prodotto delle loro saturazioni fattoriali:\n\\[\nr_{ij} = \\lambda_i \\cdot \\lambda_j.\n\\]\nQuesta relazione consente di tradurre le correlazioni osservate in un sistema di equazioni non lineari, in cui le incognite sono le saturazioni fattoriali \\(\\lambda_i\\).\nAd esempio, se consideriamo tre variabili — ad esempio Classics (\\(c\\)), Math (\\(m\\)) e English (\\(e\\)) — e assumiamo che siano tutte influenzate dallo stesso fattore latente \\(\\xi\\), possiamo scrivere:\n\\[\n\\begin{align}\nr_{cm} &= \\lambda_c \\cdot \\lambda_m, \\notag \\\\\nr_{em} &= \\lambda_e \\cdot \\lambda_m, \\\\\nr_{ce} &= \\lambda_c \\cdot \\lambda_e. \\notag\n\\end{align}\n\\]\nRisolvendo questo sistema di equazioni, otteniamo un’espressione per ciascuna saturazione in funzione delle sole correlazioni osservate. Ad esempio, isolando \\(\\lambda_m\\):\n\\[\n\\lambda_m = \\sqrt{ \\frac{r_{cm} \\cdot r_{em}}{r_{ce}} }.\n\\tag{20.6}\\]\nQuesta equazione mostra come si possa stimare il contributo di una variabile al fattore latente partendo esclusivamente dalle correlazioni empiriche tra le variabili osservate.\n\n20.7.1 Esempio con i Dati di Spearman\nUtilizzando i dati riportati da Spearman nel 1904, e in particolare le variabili Classics, Math ed English, abbiamo:\n\\[\nr_{cm} = 0.70, \\quad r_{em} = 0.64, \\quad r_{ce} = 0.78.\n\\]\nSostituendo nella formula:\n\\[\n\\hat{\\lambda}_{\\text{Math}} = \\sqrt{ \\frac{0.70 \\cdot 0.64}{0.78} } = \\sqrt{0.5733} \\approx 0.76.\n\\]\nQuesto risultato rappresenta la saturazione stimata della variabile Math nel fattore comune latente \\(\\xi\\).\nIl principio dell’annullamento della tetrade fornisce quindi un ponte diretto tra le correlazioni osservate e la struttura latente sottostante, assumendo che un singolo fattore sia responsabile delle covarianze tra le variabili.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#metodo-del-centroide",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#metodo-del-centroide",
    "title": "20  Il modello unifattoriale",
    "section": "\n20.9 Metodo del centroide",
    "text": "20.9 Metodo del centroide\nUna volta accettata l’ipotesi che le stime multiple siano sufficientemente simili, ci si può chiedere come ottenere un’unica stima sintetica per ciascuna saturazione.\nUn primo approccio consiste semplicemente nel calcolare la media aritmetica delle stime ottenute:\n\\[\n\\bar{\\lambda}_m = \\frac{0.76 + 0.69 + 0.73}{3} \\approx 0.73.\n\\]\nUn’alternativa più robusta è rappresentata dal metodo del centroide, che consiste nel calcolare una media pesata dei prodotti numeratori e dei denominatori coinvolti nelle diverse stime:\n\\[\n\\hat{\\lambda}_m = \\sqrt{\n\\frac{0.70 \\cdot 0.64 + 0.78 \\cdot 0.45 + 0.64 \\cdot 0.45}{0.78 + 0.66 + 0.54}\n} = \\sqrt{ \\frac{1.031}{1.98} } \\approx 0.73.\n\\]\nIn questo caso, entrambi i metodi producono un risultato identico. Applicando lo stesso procedimento alle altre variabili, otteniamo:\n\\[\n\\hat{\\lambda}_c = 0.97, \\quad \\hat{\\lambda}_e = 0.84, \\quad \\hat{\\lambda}_p = 0.65,\n\\]\ne quindi il vettore delle saturazioni fattoriali stimate è:\n\\[\n\\boldsymbol{\\hat{\\Lambda}}' =\n(\\hat{\\lambda}_c, \\hat{\\lambda}_e, \\hat{\\lambda}_m, \\hat{\\lambda}_p) = (0.97, 0.84, 0.73, 0.65).\n\\]\nIn sintesi, l’esempio tratto dai dati di Spearman mostra come, a partire da una semplice matrice di correlazione tra variabili osservate, sia possibile ricostruire una struttura latente assumendo l’esistenza di un unico fattore comune. Il metodo dell’annullamento della tetrade fornisce sia un criterio per verificare la coerenza del modello, sia una procedura per stimare le saturazioni fattoriali.\nSebbene oggi l’analisi fattoriale si avvalga di metodi più avanzati, come la stima per massima verosimiglianza o i modelli bayesiani, il principio introdotto da Spearman conserva tutta la sua rilevanza: la covarianza tra variabili osservate può essere interpretata come riflesso di costrutti latenti condivisi. È proprio questo passaggio — dal livello osservabile al livello teorico — che rende l’analisi fattoriale uno strumento fondamentale per la psicometria e le scienze psicologiche.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#introduzione-a-lavaan",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#introduzione-a-lavaan",
    "title": "20  Il modello unifattoriale",
    "section": "\n20.10 Introduzione a lavaan\n",
    "text": "20.10 Introduzione a lavaan\n\nAttualmente, l’analisi fattoriale viene svolta mediante software. Il pacchetto R più ampiamente utilizzato per condurre l’analisi fattoriale è lavaan.\n\n20.10.1 Sintassi del modello\nAl cuore del pacchetto lavaan si trova la “sintassi del modello”. La sintassi del modello è una descrizione del modello da stimare. In questa sezione, spieghiamo brevemente gli elementi della sintassi del modello lavaan.\nNell’ambiente R, una formula di regressione ha la seguente forma:\ny ~ x1 + x2 + x3 + x4\nIn questa formula, la tilde (“~”) è l’operatore di regressione. Sul lato sinistro dell’operatore, abbiamo la variabile dipendente (y), e sul lato destro abbiamo le variabili indipendenti, separate dall’operatore “+” . In lavaan, un modello tipico è semplicemente un insieme (o sistema) di formule di regressione, in cui alcune variabili (che iniziano con una ‘f’ qui sotto) possono essere latenti. Ad esempio:\ny ~ f1 + f2 + x1 + x2\nf1 ~ f2 + f3\nf2 ~ f3 + x1 + x2\nSe abbiamo variabili latenti in una qualsiasi delle formule di regressione, dobbiamo “definirle” elencando i loro indicatori (manifesti o latenti). Lo facciamo utilizzando l’operatore speciale “=~”, che può essere letto come “è misurato da”. Ad esempio, per definire le tre variabili latenti f1, f2 e f3, possiamo usare la sintassi seguente:\nf1 =~ y1 + y2 + y3\nf2 =~ y4 + y5 + y6\nf3 =~ y7 + y8 + y9 + y10\nInoltre, le varianze e le covarianze sono specificate utilizzando un operatore “doppia tilde”, ad esempio:\ny1 ~~ y1 # varianza\ny1 ~~ y2 # covarianza\nf1 ~~ f2 # covarianza\nE infine, le intercette per le variabili osservate e latenti sono semplici formule di regressione con solo una intercetta (esplicitamente indicato dal numero “1”) come unico predittore:\ny1 ~ 1\nf1 ~ 1\nUtilizzando questi quattro tipi di formule, è possibile descrivere una vasta gamma di modelli di variabili latenti. L’attuale insieme di tipi di formula è riassunto nella tabella sottostante.\n\n\ntipo di formula\noperatore\nmnemonic\n\n\n\ndefinizione variabile latente\n=~\nè misurato da\n\n\nregressione\n~\nviene regredito su\n\n\n(co)varianza (residuale)\n~~\nè correlato con\n\n\nintercetta\n~ 1\nintercetta\n\n\n\nUna sintassi completa del modello lavaan è semplicemente una combinazione di questi tipi di formule, racchiusi tra virgolette singole. Ad esempio:\nmy_model &lt;- ' \n  # regressions\n  y1 + y2 ~ f1 + f2 + x1 + x2\n  f1 ~ f2 + f3\n  f2 ~ f3 + x1 + x2\n\n  # latent variable definitions \n  f1 =~ y1 + y2 + y3 \n  f2 =~ y4 + y5 + y6 \n  f3 =~ y7 + y8 + y9\n  \n  # variances and covariances \n  y1 ~~ y1 \n  y1 ~~ y2 \n  f1 ~~ f2\n\n  # intercepts \n  y1 ~ 1 \n  f1 ~ 1\n'\nPer adattare il modello ai dati usiamo la seguente sintassi.\nfit &lt;- cfa(model = my_model, data = my_data)",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#diagrammi-di-percorso",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#diagrammi-di-percorso",
    "title": "20  Il modello unifattoriale",
    "section": "\n20.12 Diagrammi di percorso",
    "text": "20.12 Diagrammi di percorso\nIl pacchetto semPlot consente di disegnare diagrammi di percorso per vari modelli SEM. La funzione semPaths prende in input un oggetto creato da lavaan e disegna il diagramma, con diverse opzioni disponibili. Il diagramma prodotto controlla le dimensioni dei caratteri/etichette, la visualizzazione dei residui e il colore dei percorsi/coefficienti. Sono disponibili queste e molte altre opzioni di controllo.\n\nsemPaths(\n    fit1,\n    \"std\",\n    posCol = c(\"black\"),\n    edge.label.cex = 1.2,\n    whatLabels = \"std\", \n    edge.width = 0.3, # Imposta lo spessore delle linee \n    fade = FALSE # Disabilita il fading\n)\n\n\n\n\n\n\n\nIl calcolo delle saturazioni fattoriali con il metodo del centroide aveva prodotto il seguente risultato:\n\nclassici (Cls): 0.97\ninglese (Eng): 0.84\nmatematica (Mth): 0.73\npitch discrimination (Ptc): 0.65\n\nSi noti la somiglianza con i valori ottenuti mediante il metodo di massima verosimiglianza riportati nella figura.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#analisi-fattoriale-esplorativa",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#analisi-fattoriale-esplorativa",
    "title": "20  Il modello unifattoriale",
    "section": "\n20.13 Analisi fattoriale esplorativa",
    "text": "20.13 Analisi fattoriale esplorativa\nQuando abbiamo un’unica variabile latente, l’analisi fattoriale confermativa si riduce al caso dell’analisi fattoriale esplorativa. Esaminiamo qui sotto la sintassi per l’analisi fattoriale esplorativa in lavaan.\nSpecifichiamo il modello.\n\nefa_model &lt;- '\n    efa(\"efa\")*g =~ Classics + French + English + Math + Pitch + Music\n'\n\nAdattiamo il modello ai dati.\n\nfit2 &lt;- lavaan::cfa(\n  efa_model,\n  sample.cov = spearman_cor_mat,\n  sample.nobs = n,\n  std.lv = TRUE\n)\n\nEsaminiamo la soluzione ottenuta.\n\nsummary(fit2, standardized = TRUE) \n#&gt; lavaan 0.6-19 ended normally after 3 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        12\n#&gt; \n#&gt;   Rotation method                       GEOMIN OBLIQUE\n#&gt;   Geomin epsilon                                 0.001\n#&gt;   Rotation algorithm (rstarts)                GPA (30)\n#&gt;   Standardized metric                             TRUE\n#&gt;   Row weights                                     None\n#&gt; \n#&gt;   Number of observations                            33\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                 2.913\n#&gt;   Degrees of freedom                                 9\n#&gt;   P-value (Chi-square)                           0.968\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   g =~ efa                                                              \n#&gt;     Classics          0.942    0.129    7.314    0.000    0.942    0.956\n#&gt;     French            0.857    0.137    6.239    0.000    0.857    0.871\n#&gt;     English           0.795    0.143    5.545    0.000    0.795    0.807\n#&gt;     Math              0.732    0.149    4.923    0.000    0.732    0.743\n#&gt;     Pitch             0.678    0.153    4.438    0.000    0.678    0.689\n#&gt;     Music             0.643    0.155    4.142    0.000    0.643    0.653\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .Classics          0.083    0.051    1.629    0.103    0.083    0.086\n#&gt;    .French            0.234    0.072    3.244    0.001    0.234    0.242\n#&gt;    .English           0.338    0.094    3.610    0.000    0.338    0.349\n#&gt;    .Math              0.434    0.115    3.773    0.000    0.434    0.447\n#&gt;    .Pitch             0.510    0.132    3.855    0.000    0.510    0.526\n#&gt;    .Music             0.556    0.143    3.893    0.000    0.556    0.573\n#&gt;     g                 1.000                               1.000    1.000\n\n\nsemPaths(\n    fit2,\n    \"std\",\n    posCol = c(\"black\"),\n    edge.label.cex = 1.2,\n    whatLabels = \"std\",\n    edge.width = 0.3, # Imposta lo spessore delle linee\n    fade = FALSE # Disabilita il fading\n)",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#riflessioni-conclusive",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#riflessioni-conclusive",
    "title": "20  Il modello unifattoriale",
    "section": "\n20.14 Riflessioni Conclusive",
    "text": "20.14 Riflessioni Conclusive\nIn questo capitolo, abbiamo introdotto il metodo dell’annullamento della tetrade, che permette di stimare le saturazioni in un modello monofattoriale. Abbiamo anche illustrato come questo metodo sia, in effetti, un’applicazione del concetto di correlazione parziale.\nUn aspetto fondamentale nella costruzione dei test psicologici riguarda la determinazione del numero di fattori o tratti sottostanti al set di indicatori in esame. La teoria classica dei test presuppone che un test sia monofattoriale, cioè che gli indicatori riflettano un unico tratto latente. La mancata monodimensionalità introduce difficoltà nell’applicare i principi della teoria classica ai punteggi di un test che non soddisfa tale proprietà.\nL’analisi della dimensionalità di un insieme di indicatori rappresenta, quindi, una fase cruciale nel processo di costruzione di un test. Solitamente, questa valutazione viene effettuata attraverso l’analisi fattoriale. In questo capitolo, abbiamo descritto le proprietà di base del modello unifattoriale, gettando le fondamenta per una comprensione più approfondita della dimensionalità e dell’influenza di un singolo tratto latente sugli indicatori.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#session-info",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#session-info",
    "title": "20  Il modello unifattoriale",
    "section": "\n20.15 Session Info",
    "text": "20.15 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.2\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] lavaanExtra_0.2.1 lavaanPlot_0.8.1  kableExtra_1.4.0  corrplot_0.95    \n#&gt;  [5] ggokabeito_0.1.0  see_0.11.0        MASS_7.3-65       viridis_0.6.5    \n#&gt;  [9] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n#&gt; [13] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n#&gt; [17] lavaan_0.6-19     psych_2.5.3       scales_1.3.0      markdown_2.0     \n#&gt; [21] knitr_1.50        lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n#&gt; [25] dplyr_1.1.4       purrr_1.0.4       readr_2.1.5       tidyr_1.3.1      \n#&gt; [29] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] RColorBrewer_1.1-3  rstudioapi_0.17.1   jsonlite_1.9.1     \n#&gt;   [4] magrittr_2.0.3      TH.data_1.1-3       estimability_1.5.1 \n#&gt;   [7] farver_2.1.2        nloptr_2.2.1        rmarkdown_2.29     \n#&gt;  [10] vctrs_0.6.5         minqa_1.2.8         base64enc_0.1-3    \n#&gt;  [13] rstatix_0.7.2       htmltools_0.5.8.1   broom_1.0.7        \n#&gt;  [16] Formula_1.2-5       htmlwidgets_1.6.4   plyr_1.8.9         \n#&gt;  [19] sandwich_3.1-1      emmeans_1.11.0      zoo_1.8-13         \n#&gt;  [22] igraph_2.1.4        mime_0.13           lifecycle_1.0.4    \n#&gt;  [25] pkgconfig_2.0.3     Matrix_1.7-3        R6_2.6.1           \n#&gt;  [28] fastmap_1.2.0       rbibutils_2.3       shiny_1.10.0       \n#&gt;  [31] numDeriv_2016.8-1.1 digest_0.6.37       OpenMx_2.21.13     \n#&gt;  [34] fdrtool_1.2.18      colorspace_2.1-1    rprojroot_2.0.4    \n#&gt;  [37] Hmisc_5.2-3         labeling_0.4.3      timechange_0.3.0   \n#&gt;  [40] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n#&gt;  [43] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n#&gt;  [46] carData_3.0-5       ggsignif_0.6.4      corpcor_1.6.10     \n#&gt;  [49] gtools_3.9.5        tools_4.4.2         pbivnorm_0.6.0     \n#&gt;  [52] foreign_0.8-89      zip_2.3.2           httpuv_1.6.15      \n#&gt;  [55] nnet_7.3-20         glue_1.8.0          quadprog_1.5-8     \n#&gt;  [58] DiagrammeR_1.0.11   nlme_3.1-167        promises_1.3.2     \n#&gt;  [61] lisrelToR_0.3       grid_4.4.2          checkmate_2.3.2    \n#&gt;  [64] cluster_2.1.8.1     reshape2_1.4.4      generics_0.1.3     \n#&gt;  [67] gtable_0.3.6        tzdb_0.5.0          data.table_1.17.0  \n#&gt;  [70] hms_1.1.3           xml2_1.3.8          car_3.1-3          \n#&gt;  [73] sem_3.1-16          pillar_1.10.1       rockchalk_1.8.157  \n#&gt;  [76] later_1.4.1         splines_4.4.2       lattice_0.22-6     \n#&gt;  [79] survival_3.8-3      kutils_1.73         tidyselect_1.2.1   \n#&gt;  [82] miniUI_0.1.1.1      pbapply_1.7-2       reformulas_0.4.0   \n#&gt;  [85] svglite_2.1.3       stats4_4.4.2        xfun_0.51          \n#&gt;  [88] qgraph_1.9.8        arm_1.14-4          visNetwork_2.1.2   \n#&gt;  [91] stringi_1.8.4       pacman_0.5.1        boot_1.3-31        \n#&gt;  [94] evaluate_1.0.3      codetools_0.2-20    mi_1.1             \n#&gt;  [97] cli_3.6.4           RcppParallel_5.1.10 rpart_4.1.24       \n#&gt; [100] systemfonts_1.2.1   xtable_1.8-4        Rdpack_2.6.3       \n#&gt; [103] munsell_0.5.1       Rcpp_1.0.14         coda_0.19-4.1      \n#&gt; [106] png_0.1-8           XML_3.99-0.18       parallel_4.4.2     \n#&gt; [109] jpeg_0.1-11         lme4_1.1-36         mvtnorm_1.3-3      \n#&gt; [112] openxlsx_4.2.8      rlang_1.1.5         multcomp_1.4-28    \n#&gt; [115] mnormt_2.1.1\n\n\n\n\n\nKan, K.-J., Maas, H. L. van der, & Levine, S. Z. (2019). Extending psychometric network analysis: Empirical evidence against g in favor of mutualism? Intelligence, 73, 52–62.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.\n\n\nSpearman, C. (1904). General intelligence objectively determined and measured. American Journal of Psychology, 15, 201–293.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html",
    "href": "chapters/fa/03_analisi_fattoriale_2.html",
    "title": "21  Il modello statistico dell’analisi fattoriale",
    "section": "",
    "text": "21.1 Modello monofattoriale\nL’analisi fattoriale esplorativa (AFE) parte da una matrice di dimensioni \\(p \\times p\\) (dove \\(p\\) è il numero di variabili osservate) che contiene i coefficienti di correlazione (o, in alternativa, di covarianza) fra tali variabili. L’obiettivo dell’AFE è ottenere una matrice di dimensioni \\(p \\times k\\) (dove \\(k\\) è il numero di fattori comuni) i cui elementi – chiamati saturazioni fattoriali – descrivono la relazione tra ciascun fattore comune e ogni variabile osservata.\nNel caso più semplice, quello monofattoriale, si ipotizza l’esistenza di un unico fattore latente, \\(\\xi\\). In presenza di \\(p\\) variabili manifeste \\(Y_i\\), il modello matematico di un solo fattore comune si può esprimere nel modo seguente:\n\\[\nY_i = \\mu_i + \\lambda_{i} \\,\\xi + \\delta_i ,\n\\quad\\text{per}\\quad\ni=1, \\dots, p,\n\\tag{21.1}\\]\ndove:\nPer semplificare l’analisi, si assume spesso che \\(\\mu_i = 0\\), considerando le \\(Y_i\\) già centrate (cioè prive della loro media). Questa convenzione rende possibile riscrivere il modello come:\n\\[\nY_i = \\lambda_i \\,\\xi + \\delta_i.\n\\tag{21.2}\\]\nIn aggiunta a questa ipotesi di centratura, si stabilisce che:\nDate queste ipotesi, l’interdipendenza (cioè le correlazioni) fra le variabili osservate \\(Y_i\\) e \\(Y_k\\) è interamente spiegata dal singolo fattore comune \\(\\xi\\). I termini \\(\\delta_i\\) riguardano solo la varianza non condivisa di ciascuna variabile.\nSulla base di queste assunzioni, è possibile:\nTali derivazioni consentono di comprendere a fondo come il fattore latente \\(\\xi\\) contribuisca a spiegare le relazioni fra le variabili osservate e quanta parte della varianza di ciascuna variabile sia invece imputabile a fattori specifici (non condivisi). Questo concetto è alla base di tutte le procedure di stima e di interpretazione nell’analisi fattoriale esplorativa con un solo fattore.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html#modello-monofattoriale",
    "href": "chapters/fa/03_analisi_fattoriale_2.html#modello-monofattoriale",
    "title": "21  Il modello statistico dell’analisi fattoriale",
    "section": "",
    "text": "\\(\\xi\\) è il fattore comune (o fattore latente), condiviso da tutte le variabili \\(Y_i\\);\n\n\\(\\delta_i\\) è il fattore specifico (o fattore unico) associato alla \\(i\\)-esima variabile osservata, cioè una componente di varianza che non è condivisa con le altre variabili;\n\n\\(\\lambda_i\\) è la saturazione fattoriale (o peso) della \\(i\\)-esima variabile, ossia il coefficiente che quantifica il peso esercitato dal fattore comune \\(\\xi\\) su \\(Y_i\\).\n\n\n\n\n\nil fattore comune \\(\\xi\\) abbia media nulla, \\(\\mathbb{E}(\\xi) = 0\\), e varianza unitaria, \\(\\mathbb{V}(\\xi) = 1\\);\ni fattori specifici \\(\\delta_i\\) abbiano media nulla, \\(\\mathbb{E}(\\delta_i)=0\\), e varianza \\(\\psi_i\\), cioè \\(\\mathbb{V}(\\delta_i) = \\psi_i\\);\ni fattori specifici siano tra loro incorrelati: \\(\\mathbb{E}(\\delta_i \\,\\delta_k) = 0\\) per \\(i \\neq k\\);\ni fattori specifici siano incorrelati con il fattore comune: \\(\\mathbb{E}(\\delta_i\\,\\xi) = 0\\) per ogni \\(i\\).\n\n\n\n\ncalcolare la covarianza tra \\(Y_i\\) e il fattore comune \\(\\xi\\);\ndeterminare la varianza di ciascuna variabile \\(Y_i\\);\nottenere la covarianza tra due variabili manifeste \\(Y_i\\) e \\(Y_k\\).",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html#covarianza-tra-un-indicatore-e-il-fattore-comune",
    "href": "chapters/fa/03_analisi_fattoriale_2.html#covarianza-tra-un-indicatore-e-il-fattore-comune",
    "title": "21  Il modello statistico dell’analisi fattoriale",
    "section": "\n21.2 Covarianza tra un indicatore e il fattore comune",
    "text": "21.2 Covarianza tra un indicatore e il fattore comune\nNel modello monofattoriale, vogliamo determinare la covarianza teorica tra una variabile manifesta \\(Y_i\\) e il fattore comune \\(\\xi\\). La definizione di covarianza è:\n\\[\n\\mathrm{Cov}(Y_i, \\xi)\n= \\mathbb{E}(Y_i \\, \\xi)\n  - \\mathbb{E}(Y_i)\\,\\mathbb{E}(\\xi).\n\\]\nPoiché per semplicità assumiamo \\(\\mathbb{E}(\\xi) = 0\\), la formula si riduce a:\n\\[\n\\mathrm{Cov}(Y_i, \\xi)\n= \\mathbb{E}(Y_i \\,\\xi).\n\\]\nUsando il modello monofattoriale \\(Y_i = \\lambda_i \\xi + \\delta_i\\), si ottiene:\n\\[\n\\mathrm{Cov}(Y_i, \\xi)\n= \\mathbb{E}\\bigl((\\lambda_i \\,\\xi + \\delta_i)\\xi\\bigr)\n= \\mathbb{E}(\\lambda_i\\,\\xi^2 + \\delta_i\\,\\xi).\n\\]\nIl termine \\(\\lambda_i\\) è una costante (la saturazione fattoriale), perciò si può portare fuori dall’aspettazione:\n\\[\n= \\lambda_i \\,\\mathbb{E}(\\xi^2) + \\mathbb{E}(\\delta_i \\,\\xi).\n\\]\nA questo punto, valgono due ipotesi fondamentali:\n\n\n\\(\\mathbb{E}(\\xi^2) = \\mathbb{V}(\\xi) = 1\\), cioè il fattore comune ha varianza unitaria.\n\n\\(\\mathrm{Cov}(\\delta_i,\\xi) = \\mathbb{E}(\\delta_i \\,\\xi) = 0\\), poiché il fattore specifico \\(\\delta_i\\) è incorrelato con il fattore comune \\(\\xi\\).\n\nApplicando queste ipotesi si ha:\n\\[\n\\mathrm{Cov}(Y_i, \\xi)\n= \\lambda_i \\cdot 1 + 0\n= \\lambda_i.\n\\]\nIn sintesi: in un modello a singolo fattore, la saturazione \\(\\lambda_i\\) coincide con la covarianza tra la variabile manifesta \\(Y_i\\) e il fattore comune \\(\\xi\\). Inoltre, se ogni variabile \\(Y_i\\) è stata standardizzata, ossia ha varianza pari a 1, allora \\(\\lambda_i = \\mathrm{Corr}(Y_i,\\xi)\\). In tal caso, \\(\\lambda_i\\) esprime direttamente la correlazione tra la variabile \\(Y_i\\) e il fattore comune \\(\\xi\\).",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html#espressione-fattoriale-della-varianza",
    "href": "chapters/fa/03_analisi_fattoriale_2.html#espressione-fattoriale-della-varianza",
    "title": "21  Il modello statistico dell’analisi fattoriale",
    "section": "\n21.3 Espressione fattoriale della varianza",
    "text": "21.3 Espressione fattoriale della varianza\nSotto l’ipotesi che \\(\\mathbb{E}(Y_i) = 0\\), la varianza di \\(Y_i\\) è:\n\\[\n\\mathbb{V}(Y_i) = \\mathbb{E}(Y_i^2) - [\\mathbb{E}(Y_i)]^2\n                = \\mathbb{E}(Y_i^2).\n\\]\nUsando di nuovo il modello monofattoriale \\(Y_i = \\lambda_i \\xi + \\delta_i\\):\n\\[\n\\mathbb{V}(Y_i)\n= \\mathbb{E}\\bigl((\\lambda_i \\,\\xi + \\delta_i)^2\\bigr).\n\\]\nSviluppando il quadrato:\n\\[\n= \\mathbb{E}\\bigl(\\lambda_i^2 \\,\\xi^2\n                  + 2\\,\\lambda_i\\,\\xi\\,\\delta_i\n                  + \\delta_i^2\\bigr).\n\\]\nDistinguiamo i tre termini all’interno dell’aspettazione:\n\n\n\\(\\mathbb{E}(\\lambda_i^2 \\,\\xi^2) = \\lambda_i^2 \\,\\mathbb{E}(\\xi^2)\\) poiché \\(\\lambda_i^2\\) è costante e \\(\\mathbb{E}(\\xi^2) = \\mathbb{V}(\\xi) = 1\\). Pertanto questo termine diventa \\(\\lambda_i^2\\).\n\n\\(\\mathbb{E}(2\\,\\lambda_i\\,\\xi\\,\\delta_i) = 2\\,\\lambda_i\\,\\mathbb{E}(\\xi\\,\\delta_i)\\). Ma \\(\\mathrm{Cov}(\\xi, \\delta_i) = 0\\), dunque \\(\\mathbb{E}(\\xi\\,\\delta_i) = 0\\). Di conseguenza questo termine è nullo.\n\n\\(\\mathbb{E}(\\delta_i^2) = \\mathbb{V}(\\delta_i) = \\psi_i\\), dato che il fattore specifico \\(\\delta_i\\) ha varianza \\(\\psi_i\\).\n\nMettendo insieme questi risultati, otteniamo:\n\\[\n\\mathbb{V}(Y_i)\n= \\lambda_i^2 + \\psi_i.\n\\]\n\n\n\\(\\lambda_i^2\\) è detta comunalità della variabile \\(Y_i\\) e indica la parte di varianza spiegata dal fattore comune \\(\\xi\\).\n\n\\(\\psi_i\\) rappresenta la parte di varianza non spiegata dal fattore comune, detta unicità di \\(Y_i\\).\n\nNel caso in cui le \\(Y_i\\) siano state standardizzate (quindi abbiano \\(\\mathbb{V}(Y_i) = 1\\)), si ottiene:\n\\[\n1 = \\lambda_i^2 + \\psi_i,\n\\]\nda cui\n\\[\n\\psi_i = 1 - \\lambda_i^2.\n\\]\nIn questo scenario, la comunalità \\(\\lambda_i^2\\) indica esattamente la percentuale di varianza di \\(Y_i\\) spiegata dal fattore comune, mentre \\(\\psi_i\\) indica la percentuale rimanente, legata a fattori specifici o ad altri errori di misura.\nIn sintesi, la varianza di una variabile osservata \\(Y_i\\) può essere scomposta in:\n\n\n\\(\\lambda_i^2\\), la parte comune che la variabile condivide con tutte le altre (ossia la porzione di varianza attribuibile al fattore comune \\(\\xi\\), chiamata comunalità);\n\n\\(\\psi_i\\), la parte specifica o residua, non spiegata dal fattore comune (chiamata unicità).\n\nNei modelli fattoriali, l’obiettivo principale è proprio stimare correttamente \\(\\lambda_i\\) e \\(\\psi_i\\) per capire in che misura un fattore latente unico (\\(\\xi\\)) spiega le relazioni tra le diverse variabili manifeste \\(Y_1, Y_2, \\dots, Y_p\\).\n\nEsempio 21.1 Riprendiamo l’analisi della matrice di correlazioni di Spearman.\n\nSpearman &lt;- matrix(c(\n  1.0, .78, .70, .66,\n  .78, 1.0, .64, .54,\n  .70, .64, 1.0, .45,\n  .66, .54, .45, 1.0\n),\nbyrow = TRUE, ncol = 4\n)\nrownames(Spearman) &lt;- c(\"C\", \"E\", \"M\", \"P\")\ncolnames(Spearman) &lt;- c(\"C\", \"E\", \"M\", \"P\")\nSpearman |&gt;\n  print()\n#&gt;      C    E    M    P\n#&gt; C 1.00 0.78 0.70 0.66\n#&gt; E 0.78 1.00 0.64 0.54\n#&gt; M 0.70 0.64 1.00 0.45\n#&gt; P 0.66 0.54 0.45 1.00\n\nQuando eseguiamo una analisi fattoriale con la funzione factanal(), nello stesso output compare la quantità denominata SS loadings.\n\nfm &lt;- factanal(covmat = Spearman, factors = 1)\nfm\n#&gt; \n#&gt; Call:\n#&gt; factanal(factors = 1, covmat = Spearman)\n#&gt; \n#&gt; Uniquenesses:\n#&gt;     C     E     M     P \n#&gt; 0.086 0.329 0.460 0.539 \n#&gt; \n#&gt; Loadings:\n#&gt;   Factor1\n#&gt; C 0.956  \n#&gt; E 0.819  \n#&gt; M 0.735  \n#&gt; P 0.679  \n#&gt; \n#&gt;                Factor1\n#&gt; SS loadings      2.587\n#&gt; Proportion Var   0.647\n#&gt; \n#&gt; The degrees of freedom for the model is 2 and the fit was 0.023\n\nQuesta quantità indica quanta parte della varianza totale delle quattro variabili manifeste è spiegata dal fattore comune.\nRicordiamo che, per varianza totale in statistica multivariata, si intende la somma delle varianze delle variabili osservate (cioè la traccia della matrice di covarianza). Se le variabili sono standardizzate, ciascuna contribuisce con 1 alla varianza complessiva, quindi, con quattro variabili, la varianza totale risulta 4.\nLa quota della varianza totale spiegata dal modello fattoriale a un fattore è data dalla somma delle comunalità di ogni variabile, ossia dalle saturazioni fattoriali (loadings) al quadrato, sommate tra loro. Nell’esempio, il valore ottenuto è 2.587; perciò la proporzione di varianza spiegata è\n\\[\n\\frac{2.587}{4} \\approx 0.647,\n\\]\nche factanal() riporta come Proportion Var.\nLa parte di varianza di ciascuna variabile non spiegata dal fattore comune prende il nome di unicità (in inglese uniqueness). Nel risultato di factanal(), l’unicità di ogni variabile si ottiene con fm$uniqueness. La comunalità (ovvero la quota di varianza spiegata dal fattore comune) si ricava da 1 - fm$uniqueness, oppure calcolando direttamente il quadrato di ogni saturazione fattoriale.\n\nL &lt;- c(fm$load[1], fm$load[2], fm$load[3], fm$load[4])\nprint(L)\n#&gt; [1] 0.9563 0.8194 0.7350 0.6790\n\nEseguendo il prodotto interno t(L) %*% L, infatti, si ottiene la somma dei quadrati delle saturazioni (le cosiddette squared loadings), che fornisce la comunalità totale spiegata dal fattore per l’insieme delle variabili.\n\nt(L) %*% L \n#&gt;       [,1]\n#&gt; [1,] 2.587",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html#covarianza-tra-due-variabili-manifeste",
    "href": "chapters/fa/03_analisi_fattoriale_2.html#covarianza-tra-due-variabili-manifeste",
    "title": "21  Il modello statistico dell’analisi fattoriale",
    "section": "\n21.4 Covarianza tra due variabili manifeste",
    "text": "21.4 Covarianza tra due variabili manifeste\nConsideriamo due variabili manifeste \\(Y_i\\) e \\(Y_k\\) con media nulla, ossia \\(\\mathbb{E}(Y_i) = \\mathbb{E}(Y_k) = 0\\). In questa ipotesi, la loro covarianza è data da:\n\\[\n\\mathrm{Cov}(Y_i, Y_k)\n= \\mathbb{E}(Y_i \\, Y_k)\n  - \\mathbb{E}(Y_i)\\,\\mathbb{E}(Y_k)\n= \\mathbb{E}(Y_i \\, Y_k).\n\\]\nNel modello monofattoriale, ogni variabile si esprime come \\(Y_i = \\lambda_i \\,\\xi + \\delta_i\\), dove \\(\\xi\\) è il fattore comune e \\(\\delta_i\\) è il fattore specifico. Sostituendo queste espressioni nella formula della covarianza, otteniamo:\n\\[\n\\mathrm{Cov}(Y_i, Y_k)\n= \\mathbb{E}\\bigl((\\lambda_i \\,\\xi + \\delta_i)\n                  (\\lambda_k \\,\\xi + \\delta_k)\\bigr).\n\\]\nEspandendo il prodotto dentro l’aspettazione:\n\\[\n= \\mathbb{E}\\bigl(\\lambda_i\\,\\lambda_k \\,\\xi^2\n                + \\lambda_i\\,\\xi\\,\\delta_k\n                + \\lambda_k\\,\\delta_i\\,\\xi\n                + \\delta_i\\,\\delta_k\\bigr).\n\\]\nA questo punto, si applicano le ipotesi del modello:\n\n\n\\(\\mathbb{E}(\\xi^2) = \\mathbb{V}(\\xi) = 1\\).\n\n\\(\\mathrm{Cov}(\\xi, \\delta_i) = 0\\), dunque \\(\\mathbb{E}(\\xi \\,\\delta_i) = 0\\).\n\n\\(\\mathrm{Cov}(\\delta_i, \\delta_k) = 0\\), cioè \\(\\mathbb{E}(\\delta_i \\,\\delta_k) = 0\\).\n\nApplicandole ai termini sopra, abbiamo:\n\\[\n\\mathrm{Cov}(Y_i, Y_k)\n= \\lambda_i\\,\\lambda_k\\,\\underbrace{\\mathbb{E}(\\xi^2)}_{=1}\n  + \\lambda_i \\,\\underbrace{\\mathbb{E}(\\xi\\,\\delta_k)}_{=0}\n  + \\lambda_k \\,\\underbrace{\\mathbb{E}(\\delta_i\\,\\xi)}_{=0}\n  + \\underbrace{\\mathbb{E}(\\delta_i\\,\\delta_k)}_{=0}\n= \\lambda_i \\,\\lambda_k.\n\\]\nIn sintesi, in un modello a singolo fattore, la covarianza tra due variabili manifeste \\(Y_i\\) e \\(Y_k\\) è interamente spiegata dal fattore comune \\(\\xi\\) e risulta pari al prodotto delle rispettive saturazioni fattoriali \\(\\lambda_i\\) e \\(\\lambda_k\\).",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html#correlazioni-osservate-e-correlazioni-riprodotte-dal-modello",
    "href": "chapters/fa/03_analisi_fattoriale_2.html#correlazioni-osservate-e-correlazioni-riprodotte-dal-modello",
    "title": "21  Il modello statistico dell’analisi fattoriale",
    "section": "\n21.5 Correlazioni osservate e correlazioni riprodotte dal modello",
    "text": "21.5 Correlazioni osservate e correlazioni riprodotte dal modello\nNel modello monofattoriale, l’ipotesi di base è che il fattore comune spieghi tutta la covarianza tra le variabili osservate. In altre parole, ci aspettiamo che, una volta noto il valore del fattore comune \\(\\xi\\), ogni variabile \\(Y_i\\) sia incorrelata con le altre \\((Y_k)\\). Formalmente, ciò si traduce nell’uguaglianza:\n\\[\n\\mathrm{Cov}(Y_i, Y_k \\,\\mid\\, \\xi) = 0\n\\quad\\text{per}\\quad\ni \\neq k.\n\\]\nSe questa condizione risulta soddisfatta, il modello monofattoriale riproduce correttamente le correlazioni osservate fra le variabili. Con il termine \\(\\boldsymbol{\\Sigma}\\) si indica la matrice di correlazioni riprodotte dal modello, che in forma matriciale si esprime come:\n\\[\n\\boldsymbol{\\Sigma}\n= \\boldsymbol{\\Lambda}\\,\\boldsymbol{\\Lambda}^{\\prime}\n  + \\boldsymbol{\\Psi},\n\\]\ndove \\(\\boldsymbol{\\Lambda}\\) è la matrice delle saturazioni fattoriali (loadings) e \\(\\boldsymbol{\\Psi}\\) è la matrice delle unicità (cioè le varianze specifiche non spiegate dal fattore comune).\nIl modello monofattoriale si considera adeguato se la differenza tra la matrice di correlazioni empiricamente osservate e la matrice \\(\\boldsymbol{\\Sigma}\\) prodotta dal modello risulta trascurabile. Quando tale differenza (chiamata spesso misura di scostamento o misfit) è prossima allo zero, possiamo concludere che il fattore comune riesce a spiegare in modo soddisfacente i rapporti di correlazione tra le variabili del nostro insieme di dati.\n\nEsempio 21.2 Per i dati di Spearman, le correlazioni riprodotte dal modello ad un fattore sono\n\nround(L %*% t(L) + diag(fm$uniq), 3)\n#&gt;       [,1]  [,2]  [,3]  [,4]\n#&gt; [1,] 1.000 0.784 0.703 0.649\n#&gt; [2,] 0.784 1.000 0.602 0.556\n#&gt; [3,] 0.703 0.602 1.000 0.499\n#&gt; [4,] 0.649 0.556 0.499 1.000\n\nLa matrice delle differenze tra le correlazioni campionarie e quelle riprodotte è\n\nround(Spearman - (L %*% t(L) + diag(fm$uniq)), 3) \n#&gt;        C      E      M      P\n#&gt; C  0.000 -0.004 -0.003  0.011\n#&gt; E -0.004  0.000  0.038 -0.016\n#&gt; M -0.003  0.038  0.000 -0.049\n#&gt; P  0.011 -0.016 -0.049  0.000\n\nLo scarto maggiore tra le correlazioni campionarie e quelle riprodotte è uguale a 0.049. Si può dunque concludere che il modello monofattoriale spiega in maniera ragionevole i dati di Spearman.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html#bontà-di-adattamento-del-modello-ai-dati",
    "href": "chapters/fa/03_analisi_fattoriale_2.html#bontà-di-adattamento-del-modello-ai-dati",
    "title": "21  Il modello statistico dell’analisi fattoriale",
    "section": "\n21.6 Bontà di adattamento del modello ai dati",
    "text": "21.6 Bontà di adattamento del modello ai dati\nUn aspetto fondamentale nell’analisi fattoriale è valutare se la matrice di correlazioni (o covarianze) prevista dal modello rispecchia adeguatamente i dati empirici. A tal scopo, si conduce un test statistico che confronta la matrice di correlazioni/covarianze osservata con quella predetta dal modello fattoriale.\n\n21.6.1 L’ipotesi nulla del test\nL’ipotesi nulla (\\(H_0\\)) afferma che le differenze tra le correlazioni osservate e quelle riprodotte dal modello siano dovute soltanto agli errori di campionamento. In altre parole, il modello è considerato “corretto” a livello di popolazione, ossia \\(\\boldsymbol{\\Sigma}(\\theta) = \\boldsymbol{\\Sigma}\\), dove:\n\n\n\\(\\boldsymbol{\\Sigma}\\) è la matrice di correlazioni (o covarianze) nella popolazione;\n\n\\(\\boldsymbol{\\Sigma}(\\theta)\\) è la matrice di correlazioni (o covarianze) riprodotta dal modello in base ai parametri \\(\\theta\\).\n\n21.6.2 La statistica \\(\\chi^2\\)\n\nLa statistica usata per il test, indicata come \\(v\\) (o più comunemente \\(\\chi^2\\)), è funzione della differenza tra \\(\\boldsymbol{S}\\) (la matrice osservata) e \\(\\boldsymbol{S}(\\theta)\\) (la matrice riprodotta dal modello):\n\\[\nv = f\\bigl[\\boldsymbol{S}(\\theta) - \\boldsymbol{S}\\bigr].\n\\tag{21.3}\\]\nQuando l’ipotesi nulla è vera (cioè la discrepanza tra le due matrici è solo casuale), \\(v\\) si distribuisce approssimativamente come una \\(\\chi^2\\) con \\(\\nu\\) gradi di libertà, dove\n\\[\n\\nu = \\frac{p(p+1)}{2} \\;-\\; q.\n\\tag{21.4}\\]\n\n\n\\(p\\) è il numero di variabili manifeste;\n\n\\(q\\) è il numero di parametri stimati dal modello (ad esempio, \\(\\lambda\\) e \\(\\psi\\) nel modello monofattoriale).\n\nIl valore di \\(v\\) è tanto maggiore quanto più le correlazioni/covarianze previste dal modello differiscono da quelle effettivamente osservate. Se \\(v=0\\), i parametri del modello ricostruiscono esattamente la matrice di correlazioni della popolazione.\n\n21.6.3 Interpretazione del test\nIl test \\(\\chi^2\\) di adattamento del modello fattoriale segue la logica inversa rispetto ai test più comuni (dove un risultato significativo indica evidenza per l’ipotesi alternativa):\n\nse il test non è significativo (es., \\(p\\geq 0{,}05\\)), non si può escludere che la discrepanza tra matrice osservata e matrice stimata sia dovuta al caso: in tal caso, il modello è considerato adeguato;\nal contrario, un risultato significativo (es., \\(p &lt; 0{,}05\\)) indica che la differenza non si spiega solo con l’errore di campionamento e che il modello presenta carenze di adattamento ai dati.\n\n21.6.4 Assunzioni e limiti\n\n\nNormalità multivariata: il test \\(\\chi^2\\) richiede che le variabili siano distribuite (almeno approssimativamente) come un campione casuale tratto da una distribuzione normale multivariata. Nella pratica, non sempre questa condizione è soddisfatta.\n\nDimensioni campionarie: la statistica \\(\\chi^2\\) è sensibile al numero di osservazioni. Con campioni di grandi dimensioni, anche piccole discrepanze tra il modello e i dati tendono a produrre risultati statisticamente significativi, suggerendo un falso cattivo adattamento.\n\nPer questi motivi, la bontà di adattamento del modello non si giudica solo in base alla significatività del test \\(\\chi^2\\). Un criterio alternativo è ad esempio valutare il rapporto \\(\\chi^2/\\nu\\), dove \\(\\nu\\) sono i gradi di libertà del test. Valori di \\(\\chi^2/\\nu \\leq 3\\) (o talvolta \\(\\leq 4\\)) sono spesso considerati indicativi di un adattamento accettabile. Inoltre, in letteratura esistono molti altri indici (fit indices) che completano la valutazione della bontà di adattamento del modello fattoriale.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html#lerrore-standard-della-misurazione-nel-modello-fattoriale",
    "href": "chapters/fa/03_analisi_fattoriale_2.html#lerrore-standard-della-misurazione-nel-modello-fattoriale",
    "title": "21  Il modello statistico dell’analisi fattoriale",
    "section": "\n21.7 L’errore standard della misurazione nel modello fattoriale",
    "text": "21.7 L’errore standard della misurazione nel modello fattoriale\nIn questa sezione, mostriamo come il concetto di errore standard di misurazione, tipico della CTT, possa essere reinterpretato tramite il modello fattoriale.\n\n21.7.1 Collegamento tra CTT e analisi fattoriale\nSecondo la CTT, il punteggio osservato \\(X\\) di un test si scompone in due parti:\n\\[\nX = T + E,\n\\]\ndove:\n\n\n\\(T\\) è il valore vero del soggetto,\n\n\\(E\\) è l’errore di misurazione, considerato una variabile casuale con media zero, indipendente da \\(T\\).\n\nSe consideriamo un modello fattoriale monofattoriale con \\(p\\) item (variabili osservate), la relazione che descrive ciascun item \\(j\\) per il soggetto \\(i\\) è:\n\\[\nY_{ji} = \\lambda_j \\,\\xi_i + \\delta_{ji},\n\\]\ndove:\n\n\n\\(Y_{ji}\\) è il punteggio osservato nell’item \\(j\\),\n\n\\(\\xi_i\\) è il fattore comune (latente) per il soggetto \\(i\\),\n\n\\(\\lambda_j\\) è il carico fattoriale dell’item \\(j\\) sul fattore \\(\\xi\\),\n\n\\(\\delta_{ji}\\) è l’errore unico (o fattore specifico) relativo all’item \\(j\\).\n\nIl punteggio totale \\(X_i\\) del soggetto \\(i\\) si ottiene sommando i punteggi dei singoli item:\n\\[\nX_i\n= \\sum_{j=1}^p Y_{ji}\n= \\sum_{j=1}^p (\\lambda_j \\,\\xi_i + \\delta_{ji})\n= \\biggl(\\sum_{j=1}^p \\lambda_j\\biggr)\\,\\xi_i\n  + \\sum_{j=1}^p \\delta_{ji}.\n\\]\nNotiamo che questa formula rispecchia la struttura della CTT (McDonald, 2013):\n\\[\nX_i = T_i + E_i,\n\\]\ndove il valore vero \\(T_i\\) è \\(\\bigl(\\sum_{j=1}^p \\lambda_j\\bigr)\\,\\xi_i\\) (la parte del punteggio dovuta al fattore comune) e l’errore \\(E_i\\) è \\(\\sum_{j=1}^p \\delta_{ji}\\) (la somma degli errori unici).\n\n21.7.2 Decomposizione della varianza\nNella CTT, la varianza del punteggio osservato \\(\\sigma^2_{X_i}\\) si scompone nella varianza del valore vero (\\(\\sigma^2_{T_i}\\)) e nella varianza dell’errore (\\(\\sigma^2_{E_i}\\)):\n\\[\n\\sigma^2_{X_i}\n= \\sigma^2_{T_i} + \\sigma^2_{E_i}.\n\\]\nAll’interno del modello fattoriale, la varianza del valore vero (\\(\\sigma^2_{T_i}\\)) corrisponde alla varianza del termine \\(\\bigl(\\sum_{j=1}^p \\lambda_j\\bigr)\\,\\xi_i\\). Poiché \\(\\xi_i\\) ha varianza unitaria (\\(\\mathbb{V}(\\xi_i)=1\\)), si ha:\n\\[\n\\sigma^2_{T_i}\n= \\mathbb{V}\\Biggl[\\biggl(\\sum_{j=1}^p \\lambda_j\\biggr)\\,\\xi_i\\Biggr]\n= \\biggl(\\sum_{j=1}^p \\lambda_j\\biggr)^2 \\,\\mathbb{V}(\\xi_i)\n= \\biggl(\\sum_{j=1}^p \\lambda_j\\biggr)^2.\n\\]\nLa varianza dell’errore (\\(\\sigma^2_{E_i}\\)), invece, è la varianza della somma degli errori unici \\(\\sum_{j=1}^p \\delta_{ji}\\). Nel modello fattoriale, si assume che gli errori \\(\\delta_{ji}\\) siano incorrelati tra loro, per cui:\n\\[\n\\sigma^2_{E_i}\n= \\mathbb{V}\\Biggl(\\sum_{j=1}^p \\delta_{ji}\\Biggr)\n= \\sum_{j=1}^p \\mathbb{V}(\\delta_{ji})\n= \\sum_{j=1}^p \\Psi_j,\n\\]\ndove \\(\\Psi_j\\) è la varianza (unicità) associata all’errore dell’item \\(j\\).\n\n21.7.3 Errore standard di misurazione\nNella CTT, l’errore standard di misurazione di un test quantifica, in media, quanto il punteggio osservato può differire dal valore vero \\(T_i\\). Nel modello fattoriale, l’errore standard di misurazione del punteggio totale è la radice quadrata della somma delle varianze degli errori unici:\n\\[\n\\sigma_E\n= \\sqrt{\\sigma^2_{E_i}}\n= \\sqrt{\\sum_{j=1}^p \\Psi_j}.\n\\]\nQuesto fornisce una visione fattoriale dell’errore di misurazione: la precisione di un test (intesa come minore ampiezza dell’errore) aumenta al diminuire della somma delle unicità dei singoli item.\nIn sintesi, il modello fattoriale arricchisce il tradizionale concetto di errore standard di misurazione della CTT, mostrando che l’errore (specifico o unico) di ciascun item influisce cumulativamente sulla precisione del punteggio totale. In altre parole, gli item che presentano carichi fattoriali elevati contribuiscono a ridurre l’errore complessivo di misurazione, mentre le loro corrispettive unicità (varianze specifiche non spiegate dal fattore comune) aumentano l’incertezza del punteggio totale. Questo legame tra CTT e analisi fattoriale offre un’ottica ulteriore per comprendere e quantificare la precisione dei test psicometrici.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html#un-esempio-concreto",
    "href": "chapters/fa/03_analisi_fattoriale_2.html#un-esempio-concreto",
    "title": "21  Il modello statistico dell’analisi fattoriale",
    "section": "\n21.8 Un esempio concreto",
    "text": "21.8 Un esempio concreto\nVediamo ora come applicare i concetti del modello monofattoriale e dell’errore standard di misurazione a un caso reale. Utilizzeremo i dati raccolti per la validazione italiana del Cognitive Style Questionnaire – Short Form (CSQ-SF, Meins et al. 2012). Questo questionario, volto a misurare la vulnerabilità all’ansia e alla depressione, comprende cinque sottoscale: Internality (I), Globality (G), Stability (S), Negative consequences (N) e Self-worth (W). Sebbene la sottoscala di Internality risulti problematica, la includiamo nell’analisi per illustrare la procedura completa.\n\n21.8.1 Lettura e ispezione preliminare dei dati\nIn \\(\\textsf{R}\\), carichiamo il dataset e ne verifichiamo la dimensione (\\(n\\), numero di partecipanti). Per una prima esplorazione, calcoliamo le statistiche descrittive (psych::describe(...)) e visualizziamo la matrice di correlazione con psych::pairs.panels(...).\n\ncsq &lt;- rio::import(here::here(\"data\", \"csq540.csv\"))\nn &lt;- nrow(csq)                 # Numero di partecipanti\npsych::describe(csq, type = 2) # Statistiche descrittive\n#&gt;   vars   n  mean    sd median trimmed   mad min max range  skew kurtosis\n#&gt; I    1 540 47.76  5.78     48   47.87  4.45  21  64    43 -0.31     1.07\n#&gt; G    2 540 45.00 11.94     42   44.55 11.86  16  78    62  0.34    -0.70\n#&gt; S    3 540 44.60 12.18     42   44.24 13.34  16  77    61  0.27    -0.77\n#&gt; N    4 540 22.01  6.92     21   21.86  7.41   8  39    31  0.21    -0.74\n#&gt; W    5 540 44.05 13.10     43   43.66 13.34  16  79    63  0.31    -0.53\n#&gt;     se\n#&gt; I 0.25\n#&gt; G 0.51\n#&gt; S 0.52\n#&gt; N 0.30\n#&gt; W 0.56\n\n\npsych::pairs.panels(csq)       # Matrice di correlazione\n\n\n\n\n\n\n\nNe emerge che la sottoscala Internality (I) presenta alcune criticità — aspetto già segnalato in letteratura.\n\n21.8.2 Specifica e stima di un modello unifattoriale\nPer analizzare i dati secondo un modello fattoriale a un solo fattore, usiamo la sintassi del pacchetto lavaan:\n\nmod_csq &lt;- \"\n  F =~ NA*I + G + S + N + W\n  F ~~ 1*F\n\"\nfit &lt;- lavaan:::cfa(mod_csq, data = csq)\n\n\nNella prima riga, F =~ NA*I + G + S + N + W indichiamo che il fattore F è definito dai cinque item/sottoscale (I, G, S, N, W), con parametro NA per lasciare libera la stima della prima saturazione.\nNella seconda riga, F ~~ 1*F impone varianza unitaria per il fattore F.\n\nOtteniamo il resoconto completo con:\n\nsummary(fit, standardized = TRUE, fit.measures = TRUE)\n#&gt; lavaan 0.6-19 ended normally after 26 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        10\n#&gt; \n#&gt;   Number of observations                           540\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                46.716\n#&gt;   Degrees of freedom                                 5\n#&gt;   P-value (Chi-square)                           0.000\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                              2361.816\n#&gt;   Degrees of freedom                                10\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.982\n#&gt;   Tucker-Lewis Index (TLI)                       0.965\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -8741.781\n#&gt;   Loglikelihood unrestricted model (H1)      -8718.423\n#&gt;                                                       \n#&gt;   Akaike (AIC)                               17503.562\n#&gt;   Bayesian (BIC)                             17546.478\n#&gt;   Sample-size adjusted Bayesian (SABIC)      17514.734\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.124\n#&gt;   90 Percent confidence interval - lower         0.093\n#&gt;   90 Percent confidence interval - upper         0.158\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.000\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.989\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.033\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   F =~                                                                  \n#&gt;     I                 0.725    0.253    2.867    0.004    0.725    0.126\n#&gt;     G               -11.322    0.384  -29.481    0.000  -11.322   -0.949\n#&gt;     S               -11.342    0.398  -28.513    0.000  -11.342   -0.932\n#&gt;     N                -6.163    0.233  -26.398    0.000   -6.163   -0.891\n#&gt;     W               -11.598    0.444  -26.137    0.000  -11.598   -0.886\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;     F                 1.000                               1.000    1.000\n#&gt;    .I                32.840    2.000   16.420    0.000   32.840    0.984\n#&gt;    .G                14.038    1.473    9.532    0.000   14.038    0.099\n#&gt;    .S                19.508    1.718   11.353    0.000   19.508    0.132\n#&gt;    .N                 9.847    0.725   13.573    0.000    9.847    0.206\n#&gt;    .W                36.892    2.685   13.737    0.000   36.892    0.215\n\ne le stime dei parametri con:\n\nparameterEstimates(fit)\n#&gt;    lhs op rhs     est    se       z pvalue ci.lower ci.upper\n#&gt; 1    F =~   I   0.725 0.253   2.867  0.004    0.229    1.220\n#&gt; 2    F =~   G -11.322 0.384 -29.481  0.000  -12.075  -10.569\n#&gt; 3    F =~   S -11.342 0.398 -28.513  0.000  -12.122  -10.563\n#&gt; 4    F =~   N  -6.163 0.233 -26.398  0.000   -6.621   -5.705\n#&gt; 5    F =~   W -11.598 0.444 -26.137  0.000  -12.467  -10.728\n#&gt; 6    F ~~   F   1.000 0.000      NA     NA    1.000    1.000\n#&gt; 7    I ~~   I  32.840 2.000  16.420  0.000   28.920   36.759\n#&gt; 8    G ~~   G  14.038 1.473   9.532  0.000   11.151   16.924\n#&gt; 9    S ~~   S  19.508 1.718  11.353  0.000   16.140   22.876\n#&gt; 10   N ~~   N   9.847 0.725  13.573  0.000    8.425   11.269\n#&gt; 11   W ~~   W  36.892 2.685  13.737  0.000   31.628   42.155\n\nIn particolare, ci soffermiamo sulle unicità (varianze specifiche di ciascuna sottoscala), accessibili da:\n\npsi &lt;- parameterEstimates(fit)$est[7:11]\npsi\n#&gt; [1] 32.840 14.038 19.508  9.847 36.892\n\n\n21.8.3 Stima dell’errore standard di misurazione tramite il modello fattoriale\nNel modello fattoriale monofattoriale, l’errore standard di misurazione di un punteggio totale è la radice quadrata della somma delle varianze specifiche (unicità). Con le stime ottenute da lavaan, basta sommare i valori \\(\\psi_j\\) e prenderne la radice quadrata:\n\nsqrt(sum(psi))\n#&gt; [1] 10.64\n\nQuesto valore rappresenta l’errore standard complessivo del punteggio totale calcolato sui cinque item (sottoscale).\n\n21.8.4 Confronto con la formula della CTT\nRicordiamo la formula classica per l’errore standard di misurazione nella Classical Test Theory (CTT):\n\\[\n\\sigma_E = \\sigma_X \\sqrt{1 - \\rho_{XX'}},\n\\]\ndove \\(\\sigma_X\\) è la deviazione standard del punteggio totale e \\(\\rho_{XX'}\\) è l’attendibilità (affidabilità) del test.\n\n\nCalcoliamo il punteggio totale come somma delle sottoscale:\n\ntot_score &lt;- rowSums(csq)\n\n\n\nOtteniamo la deviazione standard di tot_score:\n\nsigma &lt;- sd(tot_score)\nsigma\n#&gt; [1] 41.26\n\n\n\nStimiamo l’attendibilità \\(\\rho_{XX'}\\) (qui indicata da \\(\\Omega\\)) con la funzione semTools::reliability() applicata all’oggetto fit prodotto da lavaan:\n\nrel &lt;- semTools::reliability(fit)\nrel\n#&gt;             F\n#&gt; alpha  0.8507\n#&gt; omega  0.9330\n#&gt; omega2 0.9330\n#&gt; omega3 0.9273\n#&gt; avevar 0.7917\n\nIl valore di \\(\\Omega\\) è tipicamente riportato nella seconda riga di rel.\n\n\nApplichiamo la formula:\n\nsigma * sqrt(1 - rel[2])\n#&gt; [1] 10.68\n\n\n\nIl valore ottenuto risulta molto simile all’errore standard di misurazione calcolato con la formula di derivazione fattoriale \\(\\sqrt{\\sum_{j=1}^p \\Psi_j}\\), a conferma della coerenza tra il modello fattoriale e la CTT.\n\n21.8.5 Correlazioni riprodotte dal modello\nPer ispezionare come il modello unifattoriale “ricostruisce” le correlazioni tra le variabili, possiamo estrarre:\n\n\nLa matrice di correlazione riprodotta:\n\ncor_mat &lt;- lavInspect(fit, \"cor.ov\")\ncor_mat\n#&gt;        I      G      S      N      W\n#&gt; I  1.000                            \n#&gt; G -0.119  1.000                     \n#&gt; S -0.117  0.885  1.000              \n#&gt; N -0.112  0.846  0.830  1.000       \n#&gt; W -0.111  0.841  0.825  0.789  1.000\n\n\n\nLe saturazioni fattoriali standardizzate (loadings):\n\nl &lt;- inspect(fit, what=\"std\")$lambda\nl\n#&gt;        F\n#&gt; I  0.126\n#&gt; G -0.949\n#&gt; S -0.932\n#&gt; N -0.891\n#&gt; W -0.886\n\n\n\nNel modello monofattoriale, la correlazione predetta tra due variabili manifeste (ad esempio I e G) è data dal prodotto delle loro saturazioni. Se l[1] è la saturazione di I e l[2] quella di G, allora:\n\nl[1] * l[2]\n#&gt; [1] -0.1192\n\nrestituisce la correlazione stimata per queste due sottoscale. Per visualizzare la matrice completa delle correlazioni riprodotte dal modello, calcoliamo:\n\nl %*% t(l) |&gt; round(3)\n#&gt;        I      G      S      N      W\n#&gt; I  0.016 -0.119 -0.117 -0.112 -0.111\n#&gt; G -0.119  0.901  0.885  0.846  0.841\n#&gt; S -0.117  0.885  0.868  0.830  0.825\n#&gt; N -0.112  0.846  0.830  0.794  0.789\n#&gt; W -0.111  0.841  0.825  0.789  0.785\n\n\n21.8.6 Varianza riprodotta di una variabile\nPrendiamo a esempio la variabile \\(`W`\\) e confrontiamo:\n\n\nLa varianza osservata:\n\nvar(csq$W)\n#&gt; [1] 171.7\n\n\n\nLa varianza riprodotta dal modello: somma della varianza spiegata dal fattore e della varianza residua. Poiché il caricamento fattoriale standardizzato per W (ad esempio -11.598 nel caso di non standardizzazione, o un valore \\(\\lambda_W\\) in quello standardizzato) misura l’effetto di F su W, la parte spiegata è \\(\\lambda_W^2\\). Sommando poi la specificità (residuo), ricostruiamo la varianza totale. Se prendiamo la proporzione di varianza residua rispetto a quella osservata:\n\n1 - (l^2 / var(csq$W))\n#&gt;       F\n#&gt; I 1.000\n#&gt; G 0.995\n#&gt; S 0.995\n#&gt; N 0.995\n#&gt; W 0.995\n\notteniamo un valore simile all’unicità stimata per W.\n\n\nProcedendo nello stesso modo per le altre sottoscale (G, I, ecc.) possiamo verificare la corrispondenza fra la varianza empirica e la varianza spiegata dal modello unifattoriale.\nIn sintesi, questo esempio illustra come, in un modello unifattoriale, sia possibile collegare:\n\n\nerrori standard di misurazione (CTT) e somme di unicità degli item (analisi fattoriale);\n\ncorrelazioni osservate e correlazioni riprodotte dal modello mediante il prodotto dei carichi fattoriali;\n\nvarianza osservata e varianza ricostruita come somma di varianza comune (fattore) e varianza residua (unicità).\n\nLa coerenza dei risultati fra CTT e analisi fattoriale ribadisce la loro stretta complementarità nello studio dell’affidabilità e della struttura latente di un test psicometrico.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html#correlazione-tra-variabili-manifeste-e-fattore-comune",
    "href": "chapters/fa/03_analisi_fattoriale_2.html#correlazione-tra-variabili-manifeste-e-fattore-comune",
    "title": "21  Il modello statistico dell’analisi fattoriale",
    "section": "\n21.9 Correlazione tra variabili manifeste e fattore comune",
    "text": "21.9 Correlazione tra variabili manifeste e fattore comune\nNel modello unifattoriale, la saturazione fattoriale \\(\\lambda_i\\) della variabile manifesta \\(Y_i\\) corrisponde teoricamente alla correlazione tra \\(Y_i\\) e il fattore comune \\(\\xi\\). Tuttavia, perché ciò risulti evidente in un’applicazione reale, è necessario poter disporre di una stima dei punteggi fattoriali per ciascun partecipante, cioè i valori (stimati) del fattore comune \\(\\xi_i\\). In \\(\\textsf{R}\\), con lavaan possiamo ricavare i punteggi fattoriali usando la funzione lavPredict(fit).\n\nhead(lavPredict(fit)) |&gt; print()\n#&gt;            F\n#&gt; [1,]  0.2694\n#&gt; [2,] -0.9111\n#&gt; [3,]  0.1871\n#&gt; [4,] -0.3316\n#&gt; [5,]  0.8307\n#&gt; [6,]  1.1535\ndim(lavPredict(fit))\n#&gt; [1] 540   1\n\n\n\nhead(lavPredict(fit)) mostra le prime righe dei punteggi fattoriali stimati per i soggetti.\n\ndim(lavPredict(fit)) conferma che abbiamo un punteggio per ognuno dei 540 soggetti nel dataset.\n\nPer verificare in concreto la relazione tra \\(\\lambda_i\\) e la correlazione di \\(Y_i\\) con il fattore comune, calcoliamo la correlazione tra i valori osservati su ciascuna sottoscala del CSQ e le stime dei punteggi fattoriali (ossia \\(\\hat{\\xi}\\)):\n\nc(\n  cor(csq$I, lavPredict(fit)),\n  cor(csq$G, lavPredict(fit)),\n  cor(csq$S, lavPredict(fit)),\n  cor(csq$N, lavPredict(fit)),\n  cor(csq$W, lavPredict(fit))\n) |&gt; \n  round(3)\n#&gt; [1]  0.128 -0.970 -0.952 -0.910 -0.905\n\nI risultati ottenuti sono molto simili (ma non necessariamente identici) alle saturazioni fattoriali riportate in:\n\ninspect(fit, what=\"std\")$lambda\n#&gt;        F\n#&gt; I  0.126\n#&gt; G -0.949\n#&gt; S -0.932\n#&gt; N -0.891\n#&gt; W -0.886\n\nLa piccola differenza riscontrata è dovuta al fatto che i punteggi fattoriali \\(\\hat{\\xi}_i\\) sono stime e non i valori reali del fattore latente, quindi non coincidono esattamente con \\(\\xi_i\\). Ciò nonostante, se il modello è ben specificato e i dati si adattano in modo soddisfacente, le correlazioni tra punteggi osservati e fattore stimato risulteranno prossime alle saturazioni fattoriali teoriche.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html#session-info",
    "href": "chapters/fa/03_analisi_fattoriale_2.html#session-info",
    "title": "21  Il modello statistico dell’analisi fattoriale",
    "section": "\n21.10 Session Info",
    "text": "21.10 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.2\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] dagitty_0.3-4     ggdag_0.2.13      ggokabeito_0.1.0  see_0.11.0       \n#&gt;  [5] MASS_7.3-65       viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n#&gt;  [9] ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1 \n#&gt; [13] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-19     psych_2.4.12     \n#&gt; [17] scales_1.3.0      markdown_1.13     knitr_1.50        lubridate_1.9.4  \n#&gt; [21] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.4      \n#&gt; [25] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1    \n#&gt; [29] tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.1      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.2.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   curl_6.2.1          broom_1.0.7        \n#&gt;  [16] Formula_1.2-5       htmlwidgets_1.6.4   plyr_1.8.9         \n#&gt;  [19] sandwich_3.1-1      rio_1.2.3           emmeans_1.10.7     \n#&gt;  [22] zoo_1.8-13          igraph_2.1.4        mime_0.13          \n#&gt;  [25] lifecycle_1.0.4     pkgconfig_2.0.3     Matrix_1.7-3       \n#&gt;  [28] R6_2.6.1            fastmap_1.2.0       rbibutils_2.3      \n#&gt;  [31] shiny_1.10.0        digest_0.6.37       OpenMx_2.21.13     \n#&gt;  [34] fdrtool_1.2.18      colorspace_2.1-1    rprojroot_2.0.4    \n#&gt;  [37] Hmisc_5.2-3         timechange_0.3.0    abind_1.4-8        \n#&gt;  [40] compiler_4.4.2      withr_3.0.2         glasso_1.11        \n#&gt;  [43] htmlTable_2.4.3     backports_1.5.0     carData_3.0-5      \n#&gt;  [46] R.utils_2.13.0      ggsignif_0.6.4      corpcor_1.6.10     \n#&gt;  [49] gtools_3.9.5        tools_4.4.2         pbivnorm_0.6.0     \n#&gt;  [52] foreign_0.8-88      zip_2.3.2           httpuv_1.6.15      \n#&gt;  [55] nnet_7.3-20         R.oo_1.27.0         glue_1.8.0         \n#&gt;  [58] quadprog_1.5-8      nlme_3.1-167        promises_1.3.2     \n#&gt;  [61] lisrelToR_0.3       grid_4.4.2          checkmate_2.3.2    \n#&gt;  [64] cluster_2.1.8.1     reshape2_1.4.4      generics_0.1.3     \n#&gt;  [67] gtable_0.3.6        tzdb_0.5.0          R.methodsS3_1.8.2  \n#&gt;  [70] data.table_1.17.0   hms_1.1.3           tidygraph_1.3.1    \n#&gt;  [73] car_3.1-3           sem_3.1-16          pillar_1.10.1      \n#&gt;  [76] rockchalk_1.8.157   later_1.4.1         splines_4.4.2      \n#&gt;  [79] lattice_0.22-6      survival_3.8-3      kutils_1.73        \n#&gt;  [82] tidyselect_1.2.1    miniUI_0.1.1.1      pbapply_1.7-2      \n#&gt;  [85] reformulas_0.4.0    V8_6.0.2            stats4_4.4.2       \n#&gt;  [88] xfun_0.51           qgraph_1.9.8        arm_1.14-4         \n#&gt;  [91] stringi_1.8.4       yaml_2.3.10         pacman_0.5.1       \n#&gt;  [94] boot_1.3-31         evaluate_1.0.3      codetools_0.2-20   \n#&gt;  [97] mi_1.1              cli_3.6.4           RcppParallel_5.1.10\n#&gt; [100] rpart_4.1.24        xtable_1.8-4        Rdpack_2.6.3       \n#&gt; [103] munsell_0.5.1       Rcpp_1.0.14         coda_0.19-4.1      \n#&gt; [106] png_0.1-8           XML_3.99-0.18       parallel_4.4.2     \n#&gt; [109] jpeg_0.1-10         lme4_1.1-36         mvtnorm_1.3-3      \n#&gt; [112] openxlsx_4.2.8      rlang_1.1.5         multcomp_1.4-28    \n#&gt; [115] mnormt_2.1.1\n\n\n\n\n\nMcDonald, R. P. (2013). Test theory: A unified treatment. Psychology Press.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/01_intro_fa.html",
    "href": "chapters/fa/01_intro_fa.html",
    "title": "19  Introduzione all’analisi fattoriale",
    "section": "",
    "text": "19.1 Analisi Fattoriale Esplorativa (EFA)\nL’analisi fattoriale è una tecnica statistica utilizzata per identificare la struttura latente sottostante a un insieme di variabili osservate, con lo scopo di ridurre la complessità del dato e individuare costrutti latenti che spiegano le relazioni tra le variabili. Le due principali forme di analisi fattoriale sono l’analisi fattoriale esplorativa (EFA) e l’analisi fattoriale confermativa (CFA), che differiscono nel loro scopo e nel grado di definizione a priori della struttura da parte del ricercatore.\nL’EFA viene utilizzata quando il ricercatore non ha ipotesi a priori su come un gruppo di variabili si strutturi. Il suo scopo è identificare empiricamente il modello che meglio si adatta ai dati, bilanciando precisione e semplicità. Questa tecnica esplora la struttura sottostante ai dati, permettendo di individuare fattori latenti che spiegano la varianza comune tra le variabili osservate. È particolarmente utile nei primi stadi di sviluppo di test psicometrici, quando si desidera identificare le dimensioni latenti sottostanti a un nuovo insieme di item. Tuttavia, la scelta di parametri e metodi di estrazione influisce pesantemente sul risultato finale.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Introduzione all'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/01_intro_fa.html#analisi-fattoriale-confermativa-cfa",
    "href": "chapters/fa/01_intro_fa.html#analisi-fattoriale-confermativa-cfa",
    "title": "19  Introduzione all’analisi fattoriale",
    "section": "19.2 Analisi Fattoriale Confermativa (CFA)",
    "text": "19.2 Analisi Fattoriale Confermativa (CFA)\nL’CFA è un approccio utilizzato quando il ricercatore ha un modello teorico ben definito e desidera valutare quanto questo modello ipotizzato si adatti ai dati osservati. La CFA consente di confrontare modelli teorici alternativi e valutare quale meglio spiega i dati, tenendo conto di vari fattori come i carichi fattoriali, gli errori e le covarianze. In psicometria, viene comunemente utilizzata per verificare la validità strutturale di un test o questionario, valutando se i dati empirici supportano il modello teorico ipotizzato.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Introduzione all'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/01_intro_fa.html#struttura-e-componenti-dellanalisi-fattoriale",
    "href": "chapters/fa/01_intro_fa.html#struttura-e-componenti-dellanalisi-fattoriale",
    "title": "19  Introduzione all’analisi fattoriale",
    "section": "19.3 Struttura e Componenti dell’Analisi Fattoriale",
    "text": "19.3 Struttura e Componenti dell’Analisi Fattoriale\nIndipendentemente dal tipo di analisi, l’analisi fattoriale si basa sulla distinzione tra variabili osservate (o manifeste) e variabili latenti (o fattori). Le variabili latenti rappresentano costrutti teorici non direttamente osservabili, mentre le variabili osservate sono i punteggi effettivi ottenuti da misure dirette. Un modello fattoriale può includere carichi fattoriali, errori, covarianze e percorsi di regressione.\nUn carico fattoriale (o saturazione fattoriale) rappresenta la forza della relazione tra una variabile osservata e il fattore latente, mentre il residuo o errore rappresenta la varianza non spiegata dal fattore latente. Le covarianze esprimono le relazioni tra le variabili o tra i fattori latenti. L’equazione generale di un indicatore osservato \\(X\\) in relazione a un fattore latente \\(F\\) può essere espressa come:\n\\[\nX = \\text{Intercetta} + \\lambda \\cdot F + \\text{Errore}\n\\]\ndove:\n\n\\(X\\) è il valore osservato dell’indicatore;\n\\(\\lambda\\) è il carico fattoriale;\n\\(F\\) è il valore del fattore latente;\nl’intercetta è il valore atteso dell’indicatore quando il fattore latente è zero;\nl’errore è la parte di varianza non spiegata dal fattore latente.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Introduzione all'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/01_intro_fa.html#modelli-fattoriali-gerarchici-e-bifattoriali",
    "href": "chapters/fa/01_intro_fa.html#modelli-fattoriali-gerarchici-e-bifattoriali",
    "title": "19  Introduzione all’analisi fattoriale",
    "section": "19.4 Modelli Fattoriali Gerarchici e Bifattoriali",
    "text": "19.4 Modelli Fattoriali Gerarchici e Bifattoriali\nEsistono varianti più avanzate dell’analisi fattoriale, come i modelli gerarchici e i modelli bifattoriali, che permettono di rappresentare strutture latenti più complesse. In particolare, i modelli bifattoriali sono utili quando si ritiene che un insieme di variabili possa essere spiegato sia da un fattore generale che da fattori specifici. Ad esempio, nel contesto della misurazione dell’intelligenza, un modello bifattoriale potrebbe includere un fattore generale (\\(g\\)) che spiega la varianza comune tra tutte le variabili, e fattori specifici che spiegano varianze più circoscritte a singoli domini cognitivi.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Introduzione all'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/01_intro_fa.html#sviluppo-storico-dellanalisi-fattoriale",
    "href": "chapters/fa/01_intro_fa.html#sviluppo-storico-dellanalisi-fattoriale",
    "title": "19  Introduzione all’analisi fattoriale",
    "section": "19.5 Sviluppo Storico dell’Analisi Fattoriale",
    "text": "19.5 Sviluppo Storico dell’Analisi Fattoriale\nL’analisi fattoriale è stata sviluppata all’inizio del XX secolo da Charles Spearman per studiare la struttura dell’intelligenza (Spearman, 1904). Spearman introdusse il concetto di fattore generale (g), che rappresentava la dimensione comune che spiegava la covarianza tra diverse abilità cognitive. Successivamente, psicologi come Thurstone criticarono il modello unifattoriale di Spearman e proposero un modello multifattoriale, che permetteva di individuare più fattori specifici, ciascuno dei quali spiegava una dimensione distinta dell’intelligenza.\nNegli anni ’60 e ’70, l’analisi fattoriale subì una trasformazione con lo sviluppo dei modelli di equazioni strutturali (SEM), che combinavano l’analisi fattoriale con la path analysis per rappresentare relazioni più complesse tra variabili osservate e latenti. Questo sviluppo permise ai ricercatori di verificare ipotesi teoriche più articolate riguardanti la struttura di costrutti psicologici complessi.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Introduzione all'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/01_intro_fa.html#applicazioni-dellanalisi-fattoriale",
    "href": "chapters/fa/01_intro_fa.html#applicazioni-dellanalisi-fattoriale",
    "title": "19  Introduzione all’analisi fattoriale",
    "section": "19.6 Applicazioni dell’Analisi Fattoriale",
    "text": "19.6 Applicazioni dell’Analisi Fattoriale\nL’analisi fattoriale è ampiamente utilizzata nello sviluppo di strumenti psicometrici, come i test di intelligenza, le scale di personalità e i questionari di auto-valutazione. Viene utilizzata per valutare la validità di costrutto, ovvero la capacità di uno strumento di misurare effettivamente il costrutto teorico che si propone di valutare. Inoltre, l’analisi fattoriale può essere impiegata per esaminare la validità discriminante, ovvero la capacità di uno strumento di distinguere tra costrutti correlati ma distinti.\nInfine, l’analisi fattoriale è uno strumento fondamentale per individuare variabili latenti sottostanti e semplificare i dati complessi, consentendo ai ricercatori di ridurre grandi set di variabili osservate a un insieme più ristretto di fattori interpretabili. La sua applicazione, tuttavia, richiede attenzione nella scelta dei parametri e delle assunzioni, poiché le decisioni prese nel processo di analisi possono influenzare significativamente i risultati finali.\n\n\n\n\nSpearman, C. (1904). General intelligence objectively determined and measured. American Journal of Psychology, 15, 201–293.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Introduzione all'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#introduzione",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#introduzione",
    "title": "20  Il modello unifattoriale",
    "section": "",
    "text": "Nota sulla indipendenza condizionale\n\n\n\n\n\nIn un modello fattoriale, si dice che le variabili osservate \\(y_1, y_2, \\dots, y_p\\) sono condizionalmente indipendenti dato il fattore latente \\(\\xi\\) quando, una volta noto il valore di \\(\\xi\\), la conoscenza di una qualunque delle variabili osservate non fornisce informazioni aggiuntive sulle altre. Formalmente, per ogni coppia \\(i \\neq j\\) si ha:\n\\[\nP(y_i, y_j \\mid \\xi) = P(y_i \\mid \\xi) \\cdot P(y_j \\mid \\xi).\n\\]\nQuesta proprietà implica che tutte le covarianze tra le variabili osservate sono spiegate esclusivamente dal fattore comune. In assenza di tale fattore, le variabili osservate risulterebbero tra loro incorrelate. L’indipendenza condizionale è una delle assunzioni centrali del modello fattoriale, e giustifica la possibilità di interpretare la covarianza tra item come effetto della loro dipendenza condivisa da un unico costrutto latente. Un esempio numerico è fornito nella Sezione 20.3.1.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#conclusione",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#conclusione",
    "title": "20  Il modello unifattoriale",
    "section": "\n20.9 Conclusione",
    "text": "20.9 Conclusione\nL’esempio di Spearman mostra in modo chiaro come, a partire da una semplice matrice di correlazione, sia possibile stimare le saturazioni fattoriali sotto l’assunzione di un singolo fattore latente. Il metodo dell’annullamento della tetrade permette di verificare empiricamente la compatibilità dei dati con un modello unifattoriale e fornisce una strategia concreta per l’identificazione dei carichi fattoriali.\nAnche se oggi l’analisi fattoriale si avvale di tecniche computazionali più sofisticate (come la stima per massima verosimiglianza), lo schema logico proposto da Spearman resta fondamentale: ogni covarianza tra variabili osservate viene interpretata come il riflesso di una fonte comune latente. Il contributo di Spearman ha dunque gettato le basi teoriche per l’intera tradizione dell’analisi fattoriale, ponendo al centro del discorso psicometrico la ricerca di dimensioni latenti condivise.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#verifica-del-modello",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#verifica-del-modello",
    "title": "20  Il modello unifattoriale",
    "section": "\n20.8 Verifica del modello",
    "text": "20.8 Verifica del modello\nRipetendo il procedimento per diverse terne di variabili osservate, è possibile ottenere stime multiple per la stessa saturazione fattoriale. Nel caso dei dati di Spearman, ad esempio, la saturazione \\(\\lambda_m\\) relativa alla variabile Math può essere stimata in almeno tre modi diversi, utilizzando combinazioni alternative di variabili:\n\\[\n\\begin{align}\n  \\hat{\\lambda}_m &= \\sqrt{ \\frac{0.70 \\cdot 0.64}{0.78} } = 0.76, \\notag \\\\\n  \\hat{\\lambda}_m &= \\sqrt{ \\frac{0.78 \\cdot 0.45}{0.66} } = 0.69, \\notag \\\\\n  \\hat{\\lambda}_m &= \\sqrt{ \\frac{0.64 \\cdot 0.45}{0.54} } = 0.73. \\notag\n\\end{align}\n\\]\nQuesta molteplicità di stime evidenzia un aspetto fondamentale del modello fattoriale unifattoriale: se il modello è corretto, ossia se un unico fattore comune spiega le correlazioni tra tutte le variabili, allora tutte le stime ottenute devono essere coerenti tra loro, entro un margine di errore accettabile. Se, al contrario, le stime risultano incoerenti o divergenti, ciò suggerisce che il modello a un solo fattore non è compatibile con i dati.\nIn altri termini, il modello di Spearman è verificabile empiricamente: è possibile confrontare le previsioni del modello (in termini di relazioni tra saturazioni e correlazioni osservate) con i dati raccolti. Sebbene Spearman non abbia formalizzato un test statistico per valutare la bontà del modello, ha introdotto un principio centrale nell’analisi fattoriale moderna: non tutte le matrici di correlazione giustificano l’ipotesi dell’esistenza di un unico fattore latente.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#un-esempio-concreto",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#un-esempio-concreto",
    "title": "20  Il modello unifattoriale",
    "section": "\n20.11 Un esempio concreto",
    "text": "20.11 Un esempio concreto\nAnalizziamo nuovamente i dati di Spearman che abbiamo esaminato in precedenza usando lavaan. La matrice completa dei dati di Spearman è messa a disposizione da Kan et al. (2019).\nSpecifichiamo il nome delle variabili manifeste\n\nvarnames &lt;- c(\n  \"Classics\", \"French\", \"English\", \"Math\", \"Pitch\", \"Music\"\n)\n\ne il loro numero\n\nny &lt;- length(varnames)\n\nCreiamo la matrice di correlazione:\n\nspearman_cor_mat &lt;- matrix(\n  c(\n    1.00,  .83,  .78,  .70,  .66,  .63,\n     .83, 1.00,  .67,  .67,  .65,  .57,\n     .78,  .67, 1.00,  .64,  .54,  .51,\n     .70,  .67,  .64, 1.00,  .45,  .51,\n     .66,  .65,  .54,  .45, 1.00,  .40,\n     .63,  .57,  .51,  .51,  .40, 1.00\n  ),\n  ny, ny,\n  byrow = TRUE,\n  dimnames = list(varnames, varnames)\n)\nspearman_cor_mat\n#&gt;          Classics French English Math Pitch Music\n#&gt; Classics     1.00   0.83    0.78 0.70  0.66  0.63\n#&gt; French       0.83   1.00    0.67 0.67  0.65  0.57\n#&gt; English      0.78   0.67    1.00 0.64  0.54  0.51\n#&gt; Math         0.70   0.67    0.64 1.00  0.45  0.51\n#&gt; Pitch        0.66   0.65    0.54 0.45  1.00  0.40\n#&gt; Music        0.63   0.57    0.51 0.51  0.40  1.00\n\nSpecifichiamo l’ampiezza campionaria:\n\nn &lt;- 33\n\nDefiniamo il modello unifattoriale in lavaan. L’operatore =~ si può leggere dicendo che la variabile latente a sinistra dell’operatore viene identificata dalle variabili manifeste elencate a destra dell’operatore e separate dal segno +. Per il caso presente, il modello dei due fattori di Spearman può essere specificato come segue.\n\nspearman_mod &lt;- \"\n  g =~ Classics + French + English + Math + Pitch + Music\n\"\n\nAdattiamo il modello ai dati con la funzione cfa():\n\nfit1 &lt;- lavaan::cfa(\n  spearman_mod,\n  sample.cov = spearman_cor_mat,\n  sample.nobs = n,\n  std.lv = TRUE\n)\n\nLa funzione cfa() del pacchetto lavaan serve per stimare un modello di analisi fattoriale confermativa (CFA), cioè un modello che verifica se un insieme di variabili osservate può essere spiegato da una o più variabili latenti (cioè non direttamente osservabili).\nVediamo passo passo cosa significano gli argomenti specificati:\n🔹 spearman_mod: è il modello CFA specificato dall’utente. Di solito è scritto come stringa e indica quali variabili osservate sono collegate a quali fattori latenti. Per esempio, g =~ Classics + French + English + Math + Pitch + Music significa che tutte le variabili (Classics, French, …) sono indicatori del fattore latente g.\n🔹 sample.cov = spearman_cor_mat: qui non stiamo fornendo direttamente i dati grezzi, ma una matrice di correlazioni tra le variabili osservate. Questo approccio è comune quando si lavora con dati standardizzati o quando si vuole fare una CFA direttamente sulla matrice di correlazione.\n🔹 sample.nobs = n: è il numero di osservazioni (cioè i soggetti) usato per costruire la matrice di correlazioni. Serve a lavaan per calcolare gli errori standard e gli indici di bontà di adattamento del modello.\n🔹 std.all = TRUE: questa opzione chiede a lavaan di standardizzare le variabili latenti nel modello. Ciò significa che:\n\nle variabili latenti (in questo caso il fattore g) vengono fissate con varianza = 1;\nquesto approccio fornisce una scala alle variabili latenti, che altrimenti non avrebbero una metrica naturale;\ni coefficienti stimati (saturazioni fattoriali) saranno espressi in una scala più facilmente interpretabile;\nrappresenta una convenzione standard nell’analisi fattoriale confermativa.\n\nQuesto è utile perché:\n\nsemplifica l’interpretazione dei risultati;\nconsente di confrontare più facilmente l’importanza relativa delle variabili osservate nel definire il fattore;\nfornisce una scala di riferimento coerente per il fattore latente.\n\n\n20.11.1 Esaminare i risultati del modello con summary()\n\nUna volta adattato il modello CFA, possiamo esaminarne i risultati principali usando la funzione summary(). Ad esempio:\n\nout &lt;- summary(\n  fit1, \n  fit.measures = TRUE, \n  standardized = TRUE\n)\nout\n#&gt; lavaan 0.6-19 ended normally after 23 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        12\n#&gt; \n#&gt;   Number of observations                            33\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                 2.913\n#&gt;   Degrees of freedom                                 9\n#&gt;   P-value (Chi-square)                           0.968\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               133.625\n#&gt;   Degrees of freedom                                15\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    1.000\n#&gt;   Tucker-Lewis Index (TLI)                       1.086\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)               -212.547\n#&gt;   Loglikelihood unrestricted model (H1)       -211.091\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                 449.094\n#&gt;   Bayesian (BIC)                               467.052\n#&gt;   Sample-size adjusted Bayesian (SABIC)        429.622\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.000\n#&gt;   90 Percent confidence interval - lower         0.000\n#&gt;   90 Percent confidence interval - upper         0.000\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.976\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.016\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.025\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   g =~                                                                  \n#&gt;     Classics          0.942    0.129    7.314    0.000    0.942    0.956\n#&gt;     French            0.857    0.137    6.239    0.000    0.857    0.871\n#&gt;     English           0.795    0.143    5.545    0.000    0.795    0.807\n#&gt;     Math              0.732    0.149    4.923    0.000    0.732    0.743\n#&gt;     Pitch             0.678    0.153    4.438    0.000    0.678    0.689\n#&gt;     Music             0.643    0.155    4.142    0.000    0.643    0.653\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .Classics          0.083    0.051    1.629    0.103    0.083    0.086\n#&gt;    .French            0.234    0.072    3.244    0.001    0.234    0.242\n#&gt;    .English           0.338    0.094    3.610    0.000    0.338    0.349\n#&gt;    .Math              0.434    0.115    3.773    0.000    0.434    0.447\n#&gt;    .Pitch             0.510    0.132    3.855    0.000    0.510    0.526\n#&gt;    .Music             0.556    0.143    3.893    0.000    0.556    0.573\n#&gt;     g                 1.000                               1.000    1.000\n\nVediamo nel dettaglio cosa fanno i singoli argomenti:\n📌 summary(fit1, ...): Questa funzione fornisce un riassunto dettagliato del modello stimato (in questo caso, fit1). L’output include:\n\ninformazioni generali sul modello (tipo di stimatore, ottimizzatore, numero di osservazioni, ecc.),\nle stime dei parametri (es. carichi fattoriali, varianze residue, covarianze),\ne, se richiesto, anche gli indici di bontà dell’adattamento e le stime standardizzate.\n\n📏 fit.measures = TRUE: Questa opzione richiede a lavaan di includere nella stampa una sezione con gli indici di bontà di adattamento del modello, tra cui:\n\n\nChi-quadrato del modello e p-value associato,\n\nCFI (Comparative Fit Index),\n\nTLI (Tucker-Lewis Index),\n\nRMSEA (Root Mean Square Error of Approximation),\n\nSRMR (Standardized Root Mean Residual),\n\ne altri.\n\nQuesti indici aiutano a valutare quanto bene il modello riproduce le relazioni osservate nei dati. Sono particolarmente utili per confrontare modelli alternativi e per verificare se il modello è coerente con la struttura teorica ipotizzata.\n🔁 standardized = TRUE: Questo argomento dice a lavaan di aggiungere le stime standardizzate dei parametri. Verranno incluse due colonne in più nell’output:\n🔹 Std.lv (standardizzato rispetto alla variabile latente)\n\nOgni carico fattoriale è standardizzato considerando che la variabile latente ha varianza 1, ma senza standardizzare le variabili osservate.\nQuesta colonna mostra quanto una variabile osservata cambia in media per un aumento di 1 deviazione standard nel fattore latente, mantenendo le osservate nella loro scala originale.\nSe hai usato std.lv = TRUE nella stima (come nel nostro esempio), allora Estimate e Std.lv saranno identici, perché la varianza della latente è già fissata a 1.\n\n🔹 Std.all (completamente standardizzato)\n\nQuesta colonna mostra i carichi completamente standardizzati, cioè sia la variabile latente che le variabili osservate sono standardizzate (varianza = 1).\nI valori possono essere letti come correlazioni tra ogni indicatore osservato e il fattore latente.\nÈ la forma più comunemente riportata negli articoli scientifici, perché:\n\nfacilita il confronto tra variabili;\nsemplifica l’interpretazione (tutti i parametri sono su una scala comparabile);\nconsente di capire quali indicatori sono più rappresentativi del costrutto.\n\n\n\nℹ️ Nota sulla sezione Varianze\n\nLe righe con un punto davanti al nome (es. .x1) indicano variabili osservate: il valore rappresenta la varianza residua, cioè la parte non spiegata dal fattore.\nLe variabili latenti (es. g, F1, ecc.) non hanno il punto iniziale: il valore indicato rappresenta la loro varianza totale.\n\n📋 Come semplificare l’output\nPer estrarre solo la tabella delle stime:\n\ncoef(fit1)  # solo le stime grezze (non standardizzate)\n#&gt;        g=~Classics          g=~French         g=~English            g=~Math \n#&gt;              0.942              0.857              0.795              0.732 \n#&gt;           g=~Pitch           g=~Music Classics~~Classics     French~~French \n#&gt;              0.678              0.643              0.083              0.234 \n#&gt;   English~~English         Math~~Math       Pitch~~Pitch       Music~~Music \n#&gt;              0.338              0.434              0.510              0.556\n\nOppure, per una tabella completa:\n\nparameterEstimates(fit1, standardized = TRUE)\n#&gt;         lhs op      rhs   est    se     z pvalue ci.lower ci.upper std.lv\n#&gt; 1         g =~ Classics 0.942 0.129 7.314  0.000    0.689    1.194  0.942\n#&gt; 2         g =~   French 0.857 0.137 6.239  0.000    0.588    1.127  0.857\n#&gt; 3         g =~  English 0.795 0.143 5.545  0.000    0.514    1.076  0.795\n#&gt; 4         g =~     Math 0.732 0.149 4.923  0.000    0.441    1.024  0.732\n#&gt; 5         g =~    Pitch 0.678 0.153 4.438  0.000    0.379    0.978  0.678\n#&gt; 6         g =~    Music 0.643 0.155 4.142  0.000    0.339    0.948  0.643\n#&gt; 7  Classics ~~ Classics 0.083 0.051 1.629  0.103   -0.017    0.183  0.083\n#&gt; 8    French ~~   French 0.234 0.072 3.244  0.001    0.093    0.376  0.234\n#&gt; 9   English ~~  English 0.338 0.094 3.610  0.000    0.154    0.522  0.338\n#&gt; 10     Math ~~     Math 0.434 0.115 3.773  0.000    0.208    0.659  0.434\n#&gt; 11    Pitch ~~    Pitch 0.510 0.132 3.855  0.000    0.251    0.769  0.510\n#&gt; 12    Music ~~    Music 0.556 0.143 3.893  0.000    0.276    0.836  0.556\n#&gt; 13        g ~~        g 1.000 0.000    NA     NA    1.000    1.000  1.000\n#&gt;    std.all\n#&gt; 1    0.956\n#&gt; 2    0.871\n#&gt; 3    0.807\n#&gt; 4    0.743\n#&gt; 5    0.689\n#&gt; 6    0.653\n#&gt; 7    0.086\n#&gt; 8    0.242\n#&gt; 9    0.349\n#&gt; 10   0.447\n#&gt; 11   0.526\n#&gt; 12   0.573\n#&gt; 13   1.000\n\nPuoi anche filtrare e formattare l’output per visualizzare solo le saturazioni fattoriali:\n\nparameterEstimates(fit1, standardized = TRUE) |&gt;\n  dplyr::filter(op == \"=~\") |&gt;\n  dplyr::select(\n    \"Fattore latente\" = lhs,\n    Indicatore = rhs,\n    B = est,\n    SE = se,\n    Z = z,\n    \"p-value\" = pvalue,\n    Beta = std.all\n  )\n#&gt;   Fattore.latente Indicatore     B    SE     Z p.value  Beta\n#&gt; 1               g   Classics 0.942 0.129 7.314       0 0.956\n#&gt; 2               g     French 0.857 0.137 6.239       0 0.871\n#&gt; 3               g    English 0.795 0.143 5.545       0 0.807\n#&gt; 4               g       Math 0.732 0.149 4.923       0 0.743\n#&gt; 5               g      Pitch 0.678 0.153 4.438       0 0.689\n#&gt; 6               g      Music 0.643 0.155 4.142       0 0.653\n\n\n20.11.1.1 Analisi dei residui\nPer valutare quanto il modello riesce a riprodurre le correlazioni osservate, possiamo esaminare la matrice delle correlazioni residue:\n\nresiduals(fit1, type = \"cor\")$cov\n#&gt;          Clsscs French Englsh   Math  Pitch  Music\n#&gt; Classics  0.000                                   \n#&gt; French   -0.003  0.000                            \n#&gt; English   0.008 -0.033  0.000                     \n#&gt; Math     -0.011  0.023  0.040  0.000              \n#&gt; Pitch     0.001  0.050 -0.016 -0.062  0.000       \n#&gt; Music     0.005  0.001 -0.017  0.024 -0.050  0.000\n\nPossiamo visualizzare graficamente i residui, ad esempio con un Q-Q plot:\n\nres1 &lt;- residuals(fit1, type = \"cor\")$cov\nres1[upper.tri(res1, diag = TRUE)] &lt;- NA  # Consideriamo solo i residui non ridondanti\nv1 &lt;- as.vector(res1)\nv2 &lt;- v1[!is.na(v1)]\n\ntibble(v2) %&gt;%\n  ggplot(aes(sample = v2)) + \n  stat_qq() + \n  stat_qq_line()\n\n\n\n\n\n\n\nQuesto grafico serve a verificare se le correlazioni residue seguono una distribuzione normale: deviazioni dalla linea indicano coppie di variabili mal rappresentate dal modello fattoriale.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#analisi-fattoriale-esplorativa-efa-in-lavaan",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#analisi-fattoriale-esplorativa-efa-in-lavaan",
    "title": "20  Il modello unifattoriale",
    "section": "\n20.13 Analisi Fattoriale Esplorativa (EFA) in lavaan\n",
    "text": "20.13 Analisi Fattoriale Esplorativa (EFA) in lavaan\n\nL’analisi fattoriale esplorativa (EFA) è una tecnica utilizzata per esplorare la struttura latente di un insieme di variabili osservate, senza imporre vincoli a priori su quali variabili siano associate a quali fattori. Anche in presenza di un solo fattore, EFA e CFA non coincidono, poiché nel CFA i legami tra fattori e indicatori sono specificati dall’utente, mentre nell’EFA sono liberamente stimati dal modello.\nIn lavaan, possiamo specificare un modello EFA utilizzando la sintassi efa(\"nome\")* nel modello.\nSpecificazione del modello EFA:\n\nefa_model &lt;- '\n  efa(\"efa\")*g =~ Classics + French + English + Math + Pitch + Music\n'\n\nIn questo esempio:\n\n\ng è un fattore esplorativo (non confermativo);\nle saturazioni fattoriali saranno stimate senza vincoli a priori.\n\nAdattamento del modello ai dati:\n\nfit2 &lt;- lavaan::cfa(\n  efa_model,\n  sample.cov = spearman_cor_mat,\n  sample.nobs = n,\n  std.lv = TRUE\n)\n\nNota: anche se usiamo la funzione cfa(), il modello è trattato come EFA perché abbiamo specificato efa() nella sintassi del modello.\nEsame della soluzione ottenuta:\n\nsummary(fit2, standardized = TRUE)\n#&gt; lavaan 0.6-19 ended normally after 3 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        12\n#&gt; \n#&gt;   Rotation method                       GEOMIN OBLIQUE\n#&gt;   Geomin epsilon                                 0.001\n#&gt;   Rotation algorithm (rstarts)                GPA (30)\n#&gt;   Standardized metric                             TRUE\n#&gt;   Row weights                                     None\n#&gt; \n#&gt;   Number of observations                            33\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                 2.913\n#&gt;   Degrees of freedom                                 9\n#&gt;   P-value (Chi-square)                           0.968\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   g =~ efa                                                              \n#&gt;     Classics          0.942    0.129    7.314    0.000    0.942    0.956\n#&gt;     French            0.857    0.137    6.239    0.000    0.857    0.871\n#&gt;     English           0.795    0.143    5.545    0.000    0.795    0.807\n#&gt;     Math              0.732    0.149    4.923    0.000    0.732    0.743\n#&gt;     Pitch             0.678    0.153    4.438    0.000    0.678    0.689\n#&gt;     Music             0.643    0.155    4.142    0.000    0.643    0.653\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .Classics          0.083    0.051    1.629    0.103    0.083    0.086\n#&gt;    .French            0.234    0.072    3.244    0.001    0.234    0.242\n#&gt;    .English           0.338    0.094    3.610    0.000    0.338    0.349\n#&gt;    .Math              0.434    0.115    3.773    0.000    0.434    0.447\n#&gt;    .Pitch             0.510    0.132    3.855    0.000    0.510    0.526\n#&gt;    .Music             0.556    0.143    3.893    0.000    0.556    0.573\n#&gt;     g                 1.000                               1.000    1.000\n\nL’output riporterà:\n\nle saturazioni fattoriali esplorative;\nle varianze residue;\nle stime standardizzate, tra cui Std.all, interpretabili come correlazioni tra fattore e variabili osservate.\n\nVisualizzazione del modello (diagramma di percorso):\n\nsemPaths(\n  fit2,\n  \"std\",\n  posCol = c(\"black\"),\n  edge.label.cex = 1.2,\n  whatLabels = \"std\",\n  edge.width = 0.3,\n  fade = FALSE\n)\n\n\n\n\n\n\n\nQuesto comando crea un diagramma in cui:\n\nle variabili osservate sono collegate al fattore esplorativo;\nle etichette sui bordi mostrano i carichi standardizzati (Std.all).\n\n💡 Osservazione importante\nSe confronti la soluzione ottenuta con il modello CFA specificato come:\ncfa_model &lt;- '\n  g =~ Classics + French + English + Math + Pitch + Music\n'\npotresti ottenere valori simili, ma la differenza concettuale rimane:\n\nnella CFA, i legami tra fattore e indicatori sono vincolati (nessun cross-loading);\nnell’EFA (via efa()), i carichi sono liberamente stimati: è il modello a decidere “quanto” ogni variabile è legata al fattore.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/04_rotazione.html#indeterminatezza-della-soluzione-fattoriale",
    "href": "chapters/extraction/04_rotazione.html#indeterminatezza-della-soluzione-fattoriale",
    "title": "29  La rotazione fattoriale",
    "section": "\n29.2 Indeterminatezza della soluzione fattoriale",
    "text": "29.2 Indeterminatezza della soluzione fattoriale\nLa necessità di effettuare la rotazione deriva dal fatto che la matrice delle saturazioni non possiede un’unica soluzione. Attraverso trasformazioni matematiche, è possibile ottenere infinite matrici dello stesso ordine, tutte in grado di riprodurre allo stesso modo la matrice di correlazioni originale. Questo fenomeno prende il nome di indeterminatezza della soluzione fattoriale.\nIn altri termini, la matrice delle saturazioni fattoriali \\(\\boldsymbol{\\Lambda}\\) non è univoca, perché una singola matrice di correlazioni \\(\\boldsymbol{R}\\) può dar luogo a più configurazioni fattoriali alternative. Ciò significa che, a parità di numero di fattori, si possono ottenere differenti configurazioni delle saturazioni fattoriali, oppure, in alcuni casi, soluzioni con un diverso numero di fattori comuni ma ugualmente capaci di riprodurre la matrice di correlazioni (o di covarianza) osservata.\n\n29.2.1 Esempio di indeterminatezza con lo stesso numero di fattori\nDi seguito, viene mostrato come due diverse matrici di saturazioni (\\(\\boldsymbol{\\Lambda}_1\\) e \\(\\boldsymbol{\\Lambda}_2\\)) possano restituire la medesima matrice di correlazioni riprodotte, pur avendo la stessa dimensionalità (cioè lo stesso numero di fattori).\n\n# Matrice di saturazioni fattoriali Lambda_1\nl1 &lt;- matrix(\n  c(\n    0.766,  -0.232,\n    0.670,  -0.203,\n    0.574,  -0.174,\n    0.454,   0.533,\n    0.389,   0.457,\n    0.324,   0.381\n  ),\n  byrow = TRUE, ncol = 2\n)\n\n# Matrice di saturazioni fattoriali Lambda_2\nl2 &lt;- matrix(\n  c(\n    0.783,  0.163,\n    0.685,  0.143,\n    0.587,  0.123,\n    0.143,  0.685,\n    0.123,  0.587,\n    0.102,  0.489\n  ),\n  byrow = TRUE, ncol = 2\n)\n\n# Matrici di correlazioni riprodotte da Lambda_1 e Lambda_2\nl1 %*% t(l1) |&gt; round(2)\n#&gt;      [,1] [,2] [,3] [,4] [,5] [,6]\n#&gt; [1,] 0.64 0.56 0.48 0.22 0.19 0.16\n#&gt; [2,] 0.56 0.49 0.42 0.20 0.17 0.14\n#&gt; [3,] 0.48 0.42 0.36 0.17 0.14 0.12\n#&gt; [4,] 0.22 0.20 0.17 0.49 0.42 0.35\n#&gt; [5,] 0.19 0.17 0.14 0.42 0.36 0.30\n#&gt; [6,] 0.16 0.14 0.12 0.35 0.30 0.25\nl2 %*% t(l2) |&gt; round(2)\n#&gt;      [,1] [,2] [,3] [,4] [,5] [,6]\n#&gt; [1,] 0.64 0.56 0.48 0.22 0.19 0.16\n#&gt; [2,] 0.56 0.49 0.42 0.20 0.17 0.14\n#&gt; [3,] 0.48 0.42 0.36 0.17 0.14 0.12\n#&gt; [4,] 0.22 0.20 0.17 0.49 0.42 0.35\n#&gt; [5,] 0.19 0.17 0.14 0.42 0.36 0.30\n#&gt; [6,] 0.16 0.14 0.12 0.35 0.30 0.25\n\nEntrambe le matrici di correlazioni riprodotte risultano identiche, pur derivando da matrici di saturazioni diverse.\n\n29.2.2 Esempio di indeterminatezza con un diverso numero di fattori\nÈ possibile mostrare che la stessa matrice di correlazioni riprodotte può derivare anche da soluzioni con un diverso numero di fattori comuni. Consideriamo due matrici \\(\\boldsymbol{\\Lambda}_1\\) e \\(\\boldsymbol{\\Lambda}_2\\) con lo stesso numero di righe (cioè, con lo stesso numero di variabili manifeste) ma un numero di colonne differente (quindi, un diverso numero di fattori):\n\n# Matrice di saturazioni a 1 fattore\nl1 &lt;- matrix(\n  c(\n    0.9,\n    0.7,\n    0.5,\n    0.3\n  ),\n  byrow = TRUE, ncol = 1\n)\n\n# Matrice di saturazioni a 2 fattori\nl2 &lt;- matrix(\n  c(\n    0.78, 0.45,\n    0.61, 0.35,\n    0.43, 0.25,\n    0.25, 0.15\n  ),\n  byrow = TRUE, ncol = 2\n)\n\n# Matrici di correlazioni riprodotte\nl1 %*% t(l1) |&gt; round(2)\n#&gt;      [,1] [,2] [,3] [,4]\n#&gt; [1,] 0.81 0.63 0.45 0.27\n#&gt; [2,] 0.63 0.49 0.35 0.21\n#&gt; [3,] 0.45 0.35 0.25 0.15\n#&gt; [4,] 0.27 0.21 0.15 0.09\nl2 %*% t(l2) |&gt; round(2)\n#&gt;      [,1] [,2] [,3] [,4]\n#&gt; [1,] 0.81 0.63 0.45 0.26\n#&gt; [2,] 0.63 0.49 0.35 0.20\n#&gt; [3,] 0.45 0.35 0.25 0.14\n#&gt; [4,] 0.26 0.20 0.14 0.08\n\nAnche in questo caso, il prodotto delle due nuove matrici restituisce la stessa matrice di correlazioni riprodotte, nonostante \\(\\boldsymbol{\\Lambda}_1\\) preveda un solo fattore comune mentre \\(\\boldsymbol{\\Lambda}_2\\) ne preveda due.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>La rotazione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/04_rotazione.html#esempio-con-metodi-ortogonali-e-obliqui-in-r",
    "href": "chapters/extraction/04_rotazione.html#esempio-con-metodi-ortogonali-e-obliqui-in-r",
    "title": "29  La rotazione fattoriale",
    "section": "\n29.6 Esempio con Metodi Ortogonali e Obliqui in R",
    "text": "29.6 Esempio con Metodi Ortogonali e Obliqui in R\nDi seguito utilizziamo una piccola matrice di correlazione tratta da un esempio proposto da Rencher (2002). Consideriamo un caso studiato da Brown, Williams e Barlow (1984), analizzato in Rencher (2002). Ad una ragazza di dodici anni è stato chiesto di valutare sette suoi conoscenti su cinque attributi: gentilezza, intelligenza, felicità, simpatia e giustizia. Di seguito è presentata la matrice di correlazione tra le variabili misurate:\n\nR &lt;- matrix(\n  c(\n    1.00, .296, .881, .995, .545,\n    .296, 1.000, -.022, .326, .837,\n    .881, -.022, 1.000, .867, .130,\n    .995, .326, .867, 1.000, .544,\n    .545, .837, .130, .544, 1.00\n  ),\n  ncol = 5, byrow = TRUE, dimnames = list(\n    c(\"K\", \"I\", \"H\", \"L\", \"J\"), c(\"K\", \"I\", \"H\", \"L\", \"J\")\n  )\n)\n\nprint(R)\n#&gt;       K      I      H     L     J\n#&gt; K 1.000  0.296  0.881 0.995 0.545\n#&gt; I 0.296  1.000 -0.022 0.326 0.837\n#&gt; H 0.881 -0.022  1.000 0.867 0.130\n#&gt; L 0.995  0.326  0.867 1.000 0.544\n#&gt; J 0.545  0.837  0.130 0.544 1.000\n\nEseguiamo l’analisi fattoriale con 2 fattori con il metodo principale (principal). Imponiamo una rotazione ortogonale Varimax:\n\nf1_pc &lt;- principal(R, 2, rotate = \"varimax\")\nf1_pc\n#&gt; Principal Components Analysis\n#&gt; Call: principal(r = R, nfactors = 2, rotate = \"varimax\")\n#&gt; Standardized loadings (pattern matrix) based upon correlation matrix\n#&gt;    RC1   RC2   h2     u2 com\n#&gt; K 0.95  0.30 0.99 0.0067 1.2\n#&gt; I 0.03  0.96 0.92 0.0792 1.0\n#&gt; H 0.97 -0.10 0.96 0.0391 1.0\n#&gt; L 0.94  0.32 0.99 0.0135 1.2\n#&gt; J 0.26  0.93 0.94 0.0597 1.2\n#&gt; \n#&gt;                        RC1  RC2\n#&gt; SS loadings           2.81 1.99\n#&gt; Proportion Var        0.56 0.40\n#&gt; Cumulative Var        0.56 0.96\n#&gt; Proportion Explained  0.58 0.42\n#&gt; Cumulative Proportion 0.58 1.00\n#&gt; \n#&gt; Mean item complexity =  1.1\n#&gt; Test of the hypothesis that 2 components are sufficient.\n#&gt; \n#&gt; The root mean square of the residuals (RMSR) is  0.03 \n#&gt; \n#&gt; Fit based upon off diagonal values = 1\n\nLe saturazioni fattoriali (loadings) mostrano quanto ciascuna variabile sia associata ai due fattori, ma l’angolo tra i fattori è mantenuto a 90°.\nConsideriamo ora una rotazione obliqua (Oblimin):\n\npr_oblimin &lt;- principal(R, 2, rotate = \"oblimin\")\npr_oblimin\n#&gt; Principal Components Analysis\n#&gt; Call: principal(r = R, nfactors = 2, rotate = \"oblimin\")\n#&gt; Standardized loadings (pattern matrix) based upon correlation matrix\n#&gt;     TC1   TC2   h2     u2 com\n#&gt; K  0.94  0.17 0.99 0.0067 1.1\n#&gt; I -0.08  0.98 0.92 0.0792 1.0\n#&gt; H  1.01 -0.25 0.96 0.0391 1.1\n#&gt; L  0.93  0.19 0.99 0.0135 1.1\n#&gt; J  0.16  0.92 0.94 0.0597 1.1\n#&gt; \n#&gt;                        TC1  TC2\n#&gt; SS loadings           2.84 1.96\n#&gt; Proportion Var        0.57 0.39\n#&gt; Cumulative Var        0.57 0.96\n#&gt; Proportion Explained  0.59 0.41\n#&gt; Cumulative Proportion 0.59 1.00\n#&gt; \n#&gt;  With component correlations of \n#&gt;      TC1  TC2\n#&gt; TC1 1.00 0.26\n#&gt; TC2 0.26 1.00\n#&gt; \n#&gt; Mean item complexity =  1.1\n#&gt; Test of the hypothesis that 2 components are sufficient.\n#&gt; \n#&gt; The root mean square of the residuals (RMSR) is  0.03 \n#&gt; \n#&gt; Fit based upon off diagonal values = 1\n\nQui i fattori possono correlare tra loro: la matrice pr_oblimin$Phi mostra i valori di correlazione tra i due fattori, che risultano diversi da zero. Di conseguenza, la matrice Pattern (che contiene i coefficienti di regressione parziali dei fattori sulle variabili) non coincide con la matrice di Struttura (che riporta le correlazioni fattore-variabile, considerando anche gli effetti indiretti).\nPer approfondire:\n\n\nMatrice Pattern (saturazioni parziali)\n\ncbind(pr_oblimin$load[, 1], pr_oblimin$load[, 2])\n#&gt;       [,1]    [,2]\n#&gt; K  0.94009  0.1685\n#&gt; I -0.08425  0.9777\n#&gt; H  1.01399 -0.2460\n#&gt; L  0.92799  0.1887\n#&gt; J  0.15540  0.9182\n\n\n\nMatrice di inter-correlazione fattoriale (\\(\\boldsymbol{\\Phi}\\))\n\npr_oblimin$Phi\n#&gt;        TC1    TC2\n#&gt; TC1 1.0000 0.2562\n#&gt; TC2 0.2562 1.0000\n\n\n\nMatrice di Struttura (correlazioni fattore-variabile)\n\npr_oblimin$load %*% pr_oblimin$Phi %&gt;% round(3)\n#&gt;     TC1   TC2\n#&gt; K 0.983 0.409\n#&gt; I 0.166 0.956\n#&gt; H 0.951 0.014\n#&gt; L 0.976 0.426\n#&gt; J 0.391 0.958\n\n\n\nDi seguito presento due esempi grafici che mostrano come variano i punti (le saturazioni fattoriali) e gli assi quando si effettua una rotazione ortogonale e una rotazione obliqua. In entrambi i casi, si parte dalla stessa soluzione “non ruotata” (unrotated), in cui si dispone dei loadings iniziali su due fattori. Nel grafico, gli assi originali (quelli della soluzione non ruotata) vengono mantenuti e, successivamente, aggiunti gli assi della soluzione ruotata, in modo da evidenziare il cambiamento di orientamento.\nPer queste variabili, la matrice di correlazione \\(R\\) è stata analizzata per estrarre due fattori mediante il metodo delle componenti principali, senza rotazione iniziale. Si osserva che i fattori risultano difficili da interpretare: il primo fattore mostra alte saturazioni positive su tutte le variabili manifeste, mentre il secondo fattore si caratterizza per alte saturazioni positive su una variabile e negative sulle altre.\n\nf.pc &lt;- principal(R, 2, rotate = FALSE)\nf.pc\n#&gt; Principal Components Analysis\n#&gt; Call: principal(r = R, nfactors = 2, rotate = FALSE)\n#&gt; Standardized loadings (pattern matrix) based upon correlation matrix\n#&gt;    PC1   PC2   h2     u2 com\n#&gt; K 0.97 -0.23 0.99 0.0067 1.1\n#&gt; I 0.52  0.81 0.92 0.0792 1.7\n#&gt; H 0.78 -0.59 0.96 0.0391 1.9\n#&gt; L 0.97 -0.21 0.99 0.0135 1.1\n#&gt; J 0.70  0.67 0.94 0.0597 2.0\n#&gt; \n#&gt;                        PC1  PC2\n#&gt; SS loadings           3.26 1.54\n#&gt; Proportion Var        0.65 0.31\n#&gt; Cumulative Var        0.65 0.96\n#&gt; Proportion Explained  0.68 0.32\n#&gt; Cumulative Proportion 0.68 1.00\n#&gt; \n#&gt; Mean item complexity =  1.6\n#&gt; Test of the hypothesis that 2 components are sufficient.\n#&gt; \n#&gt; The root mean square of the residuals (RMSR) is  0.03 \n#&gt; \n#&gt; Fit based upon off diagonal values = 1\n\nIn un grafico delle saturazioni fattoriali, i punti rappresentano le cinque coppie di saturazioni (una per ciascun fattore):\n\nplot(\n  f.pc$load[, 1], f.pc$load[, 2],\n  bty = \"n\", xaxt = \"n\",\n  xlab = \"Primo Fattore\", ylab = \"Secondo Fattore\",\n  ylim = c(-.6, 1), xlim = c(0, 1), pch = 19, asp = 1\n)\nabline(0, 0)\n\n\n\n\n\n\n\nRencher (2002) suggerisce che una rotazione ortogonale di \\(-35^\\circ\\) avvicinerebbe efficacemente gli assi ai punti nel diagramma di dispersione. Per verificarlo, si può disegnare i nuovi assi nel grafico dopo una rotazione di \\(-35^\\circ\\).\n\nplot(\n  f.pc$load[, 1], f.pc$load[, 2],\n  bty = \"n\", xaxt = \"n\",\n  xlab = \"Primo Fattore\", ylab = \"Secondo Fattore\",\n  ylim = c(-.6, 1), xlim = c(0, 1), pch = 19, asp = 1\n)\naxis(1, pos = c(0, 0))\nabline(0, 0)\n\nar &lt;- matrix(c(\n  0, 0,\n  0, 1,\n  0, 0,\n  1, 0\n), ncol = 2, byrow = TRUE)\n\nangle &lt;- 35\nrad &lt;- angle * pi / 180\nT &lt;- matrix(c(\n  cos(rad), -sin(rad),\n  sin(rad),  cos(rad)\n), ncol = 2, byrow = TRUE)\n\nround(ar %*% T, 3)\n#&gt;       [,1]   [,2]\n#&gt; [1,] 0.000  0.000\n#&gt; [2,] 0.574  0.819\n#&gt; [3,] 0.000  0.000\n#&gt; [4,] 0.819 -0.574\n\narrows(0, 0, 0.574, 0.819, lwd = 2)\narrows(0, 0, 0.819, -0.574, lwd = 2)\n\n\n\n\n\n\n\nNella figura, le due frecce rappresentano gli assi ruotati. La rotazione di \\(-35^{\\circ}\\) ha effettivamente avvicinato gli assi ai punti del diagramma. Se usiamo dunque il valore \\(\\phi = -35^{\\circ}\\) nella matrice di rotazione, possiamo calcolare le saturazioni fattoriali della soluzione ruotata \\(\\hat{\\boldsymbol{\\Lambda}}^* = \\hat{\\boldsymbol{\\Lambda}} \\textbf{T}\\).\nLe saturazioni fattoriali ruotate corrispondono alla proiezione ortogonale dei punti sugli assi ruotati:\n\nangle &lt;- -35\nrad &lt;- angle * pi / 180\nT &lt;- matrix(c(\n  cos(rad), -sin(rad),\n  sin(rad),  cos(rad)\n), ncol = 2, byrow = TRUE)\nround(f.pc$load %*% T, 3)\n#&gt;     [,1]   [,2]\n#&gt; K  0.927  0.367\n#&gt; I -0.037  0.959\n#&gt; H  0.980 -0.031\n#&gt; L  0.916  0.385\n#&gt; J  0.194  0.950\n\nLa soluzione ottenuta in questo modo riproduce quanto riportato da Rencher (2002).\nNella rotazione obliqua, invece, gli assi dei fattori non sono più perpendicolari: si inclinano per adattarsi meglio ai dati, consentendo ai fattori di essere correlati. Il grafico può quindi mostrare assi non ortogonali.\n\n# Estrai i loadings non ruotati\nf_unrot &lt;- principal(R, nfactors = 2, rotate = \"none\")\nL_unrot &lt;- unclass(f_unrot$loadings)\n\n# Calcola la rotazione obliqua e ottieni la matrice T\nrot_result &lt;- oblimin(L_unrot) # usa GPArotation\nTmat &lt;- rot_result$Th # matrice di trasformazione obliqua\n\n# Ruota i due assi canonici\ne1 &lt;- c(1, 0) # asse F1\ne2 &lt;- c(0, 1) # asse F2\n\n# Rotazione degli assi\naxis1_rot &lt;- Tmat %*% e1\naxis2_rot &lt;- Tmat %*% e2\n\n# Punti: loadings non ruotati\nx &lt;- L_unrot[, 1]\ny &lt;- L_unrot[, 2]\n\n# Plot\nplot(\n  x, y,\n  xlab = \"Primo Fattore (non ruotato)\",\n  ylab = \"Secondo Fattore (non ruotato)\",\n  xlim = c(-1, 1), ylim = c(-1, 1),\n  asp = 1, pch = 19, bty = \"n\"\n)\ntext(x, y, labels = rownames(R), pos = 3)\n\n# Assi originali\narrows(0, 0, 1, 0, col = \"gray\", lty = 2)\narrows(0, 0, 0, 1, col = \"gray\", lty = 2)\n\n# Assi ruotati\narrows(0, 0, axis1_rot[1], axis1_rot[2], col = \"red\", lwd = 2)\narrows(0, 0, axis2_rot[1], axis2_rot[2], col = \"blue\", lwd = 2)\n\nlegend(\"bottomright\",\n  legend = c(\"Asse F1 ruotato (obliquo)\", \"Asse F2 ruotato (obliquo)\"),\n  col = c(\"red\", \"blue\"), lwd = 2, bty = \"n\"\n)\n\n\n\n\n\n\n\nNel caso presente, l’angolo tra gli assi si discosta poco da 90 gradi\n\naxis1_rot &lt;- Tmat %*% c(1, 0) # asse F1 ruotato\naxis2_rot &lt;- Tmat %*% c(0, 1) # asse F2 ruotato\n\n# Calcola l’angolo tra i due vettori ruotati\ncos_theta &lt;- sum(axis1_rot * axis2_rot) /\n  (sqrt(sum(axis1_rot^2)) * sqrt(sum(axis2_rot^2)))\n\n# Assicura che il valore sia nel range [-1, 1] (per evitare errori numerici)\ncos_theta &lt;- max(min(cos_theta, 1), -1)\n\n# Calcola l’angolo in radianti\ntheta_rad &lt;- acos(cos_theta)\n\n# Converti in gradi\ntheta_deg &lt;- theta_rad * 180 / pi\n\n# Mostra il risultato\ntheta_deg\n#&gt; [1] 75.15\n\nma in altri casi la differenza può essere sostanziale.\n\n29.6.1 Osservazioni\n\n\nNel caso ortogonale, i fattori restano a 90°: la rotazione è una semplice “rotazione rigida” dello spazio fattoriale. Le comunalità delle variabili non cambiano, e i fattori rimangono incorrelati.\n\n\nNel caso obliquo, i fattori possono acquisire correlazioni. La matrice di inter-correlazione fattoriale \\(\\boldsymbol{\\Phi}\\) presenta valori \\(\\neq 0\\). In questa situazione:\n\nLa matrice Pattern (coeff. di regressione parziali) diverge dalla matrice di Struttura (correlazioni fattore-variabile).\n\nGeometricamente, gli assi non risultano più ortogonali, né necessariamente di lunghezza unitaria.\n\n\n\nCon questi due esempi, si evidenzia in modo sia numerico sia grafico la differenza fra una rotazione ortogonale e una rotazione obliqua: nella prima i fattori vengono “ruotati” ma restano indipendenti, nella seconda emerge la possibilità di correlazione fra i fattori, dando spesso una soluzione più aderente alla realtà psicologica (dove i costrutti latenti sono raramente del tutto indipendenti).\nIn sintesi, il confronto tra Varimax (ortogonale) e Oblimin (obliqua) mette in luce come l’angolo tra i fattori e le saturazioni delle variabili cambino a seconda che si ipotizzi o meno la presenza di una correlazione tra fattori. Nella soluzione obliqua:\n\ni fattori risultano correlati, come evidenziato dalla matrice \\(\\boldsymbol{\\Phi}\\) con valori \\(\\neq 0\\);\nla matrice Pattern differisce dalla matrice di Struttura, perché le correlazioni tra variabili e fattori includono non solo l’effetto diretto del fattore su una variabile, ma anche gli effetti “indiretti” mediati dalla correlazione con altri fattori.\n\nIn pratica, l’uso di una rotazione obliqua è più appropriato quando i costrutti che i fattori misurano sono ragionevolmente attesi come correlati (evento frequente in psicologia). Al contrario, una rotazione ortogonale può risultare utile se si ritiene che i fattori siano realmente indipendenti o se, per esigenze di interpretazione, si preferisce mantenerli tali.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>La rotazione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/04_rotazione.html#esempio-con-semtools-geomin-quartimin-varimax-ecc.",
    "href": "chapters/extraction/04_rotazione.html#esempio-con-semtools-geomin-quartimin-varimax-ecc.",
    "title": "29  La rotazione fattoriale",
    "section": "\n29.7 Esempio con semTools (Geomin, Quartimin, Varimax, ecc.)",
    "text": "29.7 Esempio con semTools (Geomin, Quartimin, Varimax, ecc.)\nQui si mostra un esempio di uso del pacchetto semTools per l’analisi fattoriale esplorativa (EFA) del classico dataset di Holzinger e Swineford (1939), contenente i punteggi di test di abilità mentale di bambini di seconda e terza media di due diverse scuole. In letteratura, spesso si utilizza un subset di 9 variabili.\nNel nostro esempio, estraiamo 3 fattori utilizzando il metodo mlr:\n\nMaximum likelihood estimation with robust (Huber-White) standard errors and a scaled test statistic that is (asymptotically) equal to the Yuan-Bentler test statistic. For both complete and incomplete data.\n\n\n\nSoluzione non ruotata:\n\n\nunrotated &lt;- efaUnrotate(\n  HolzingerSwineford1939,\n  nf = 3,\n  varList = paste0(\"x\", 1:9),\n  estimator = \"mlr\"\n)\nout &lt;- summary(unrotated)\nprint(out)\n#&gt; lavaan 0.6-19 ended normally after 217 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        36\n#&gt; \n#&gt;   Number of observations                           301\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                               Standard      Scaled\n#&gt;   Test Statistic                                22.897      23.864\n#&gt;   Degrees of freedom                                12          12\n#&gt;   P-value (Chi-square)                           0.029       0.021\n#&gt;   Scaling correction factor                                  0.959\n#&gt;     Yuan-Bentler correction (Mplus variant)                       \n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Sandwich\n#&gt;   Information bread                           Observed\n#&gt;   Observed information based on                Hessian\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;   factor1 =~                                          \n#&gt;     x1      (l1_1)    0.653    0.083    7.909    0.000\n#&gt;     x2      (l2_1)    0.353    0.079    4.481    0.000\n#&gt;     x3      (l3_1)    0.415    0.086    4.832    0.000\n#&gt;     x4      (l4_1)    0.926    0.067   13.762    0.000\n#&gt;     x5      (l5_1)    1.014    0.067   15.176    0.000\n#&gt;     x6      (l6_1)    0.868    0.062   13.887    0.000\n#&gt;     x7      (l7_1)    0.283    0.091    3.113    0.002\n#&gt;     x8      (l8_1)    0.340    0.083    4.095    0.000\n#&gt;     x9      (l9_1)    0.460    0.078    5.881    0.000\n#&gt;   factor2 =~                                          \n#&gt;     x1      (l1_2)    0.349    0.124    2.815    0.005\n#&gt;     x2      (l2_2)    0.242    0.159    1.523    0.128\n#&gt;     x3      (l3_2)    0.497    0.132    3.767    0.000\n#&gt;     x4      (l4_2)   -0.337    0.067   -5.058    0.000\n#&gt;     x5      (l5_2)   -0.461    0.077   -6.009    0.000\n#&gt;     x6      (l6_2)   -0.280    0.057   -4.908    0.000\n#&gt;     x7      (l7_2)    0.372    0.188    1.976    0.048\n#&gt;     x8      (l8_2)    0.510    0.133    3.831    0.000\n#&gt;     x9      (l9_2)    0.489    0.066    7.416    0.000\n#&gt;   factor3 =~                                          \n#&gt;     x1      (l1_3)   -0.338    0.103   -3.275    0.001\n#&gt;     x2      (l2_3)   -0.405    0.092   -4.401    0.000\n#&gt;     x3      (l3_3)   -0.404    0.120   -3.355    0.001\n#&gt;     x4      (l4_3)    0.049    0.098    0.503    0.615\n#&gt;     x5      (l5_3)    0.122    0.105    1.154    0.248\n#&gt;     x6      (l6_3)   -0.000    0.076   -0.003    0.998\n#&gt;     x7      (l7_3)    0.609    0.125    4.863    0.000\n#&gt;     x8      (l8_3)    0.409    0.143    2.853    0.004\n#&gt;     x9      (l9_3)    0.112    0.123    0.915    0.360\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;   factor1 ~~                                          \n#&gt;     factor2           0.000                           \n#&gt;     factor3           0.000                           \n#&gt;   factor2 ~~                                          \n#&gt;     factor3           0.000                           \n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;     factor1           1.000                           \n#&gt;     factor2           1.000                           \n#&gt;     factor3           1.000                           \n#&gt;    .x1                0.696    0.113    6.184    0.000\n#&gt;    .x2                1.035    0.106    9.803    0.000\n#&gt;    .x3                0.692    0.097    7.132    0.000\n#&gt;    .x4                0.377    0.053    7.170    0.000\n#&gt;    .x5                0.403    0.064    6.303    0.000\n#&gt;    .x6                0.365    0.046    7.984    0.000\n#&gt;    .x7                0.594    0.148    4.014    0.000\n#&gt;    .x8                0.479    0.099    4.842    0.000\n#&gt;    .x9                0.551    0.065    8.518    0.000\n#&gt; \n#&gt; Constraints:\n#&gt;                                                |Slack|\n#&gt;     0-(1_2*1_1+2_2*2_1+3_2*3_1+4_2*4_1+5_2*5_    0.000\n#&gt;     0-(1_3*1_1+2_3*2_1+3_3*3_1+4_3*4_1+5_3*5_    0.000\n#&gt;     0-(1_3*1_2+2_3*2_2+3_3*3_2+4_3*4_2+5_3*5_    0.000\n\n\n\nRotazione ortogonale Varimax:\n\n\nout_varimax &lt;- orthRotate(\n  unrotated,\n  method = \"varimax\"\n)\nsummary(out_varimax, sort = FALSE, suppress = 0.3)\n#&gt; Standardized Rotated Factor Loadings\n#&gt;    factor1 factor2 factor3\n#&gt; x1  0.320*  0.607*        \n#&gt; x2          0.481*        \n#&gt; x3          0.662*        \n#&gt; x4  0.838*                \n#&gt; x5  0.867*                \n#&gt; x6  0.815*                \n#&gt; x7                  0.695*\n#&gt; x8                  0.704*\n#&gt; x9          0.409*  0.511*\n#&gt; \n#&gt; Factor Correlation\n#&gt;         factor1 factor2 factor3\n#&gt; factor1       1       0       0\n#&gt; factor2       0       1       0\n#&gt; factor3       0       0       1\n#&gt; \n#&gt; Method of rotation:  varimax \n#&gt; \n#&gt; Test Statistics for Standardized Rotated Factor Loadings\n#&gt;        lhs op rhs std.loading    se      z     p ci.lower ci.upper\n#&gt; 1  factor1 =~  x1       0.320 0.055  5.799 0.000    0.212    0.428\n#&gt; 2  factor1 =~  x2       0.135 0.063  2.151 0.031    0.012    0.259\n#&gt; 3  factor1 =~  x3       0.080 0.049  1.622 0.105   -0.017    0.176\n#&gt; 4  factor1 =~  x4       0.838 0.028 30.193 0.000    0.784    0.892\n#&gt; 5  factor1 =~  x5       0.867 0.024 36.189 0.000    0.820    0.914\n#&gt; 6  factor1 =~  x6       0.815 0.024 33.939 0.000    0.768    0.862\n#&gt; 7  factor1 =~  x7       0.102 0.049  2.058 0.040    0.005    0.199\n#&gt; 8  factor1 =~  x8       0.078 0.048  1.617 0.106   -0.016    0.172\n#&gt; 9  factor1 =~  x9       0.170 0.053  3.222 0.001    0.067    0.273\n#&gt; 10 factor2 =~  x1       0.607 0.075  8.138 0.000    0.461    0.753\n#&gt; 11 factor2 =~  x2       0.481 0.067  7.184 0.000    0.350    0.612\n#&gt; 12 factor2 =~  x3       0.662 0.058 11.499 0.000    0.549    0.775\n#&gt; 13 factor2 =~  x4       0.113 0.043  2.615 0.009    0.028    0.198\n#&gt; 14 factor2 =~  x5       0.032 0.040  0.802 0.422   -0.047    0.111\n#&gt; 15 factor2 =~  x6       0.162 0.042  3.855 0.000    0.079    0.244\n#&gt; 16 factor2 =~  x7      -0.062 0.047 -1.341 0.180   -0.154    0.029\n#&gt; 17 factor2 =~  x8       0.174 0.082  2.117 0.034    0.013    0.336\n#&gt; 18 factor2 =~  x9       0.409 0.079  5.173 0.000    0.254    0.564\n#&gt; 19 factor3 =~  x1       0.130 0.066  1.978 0.048    0.001    0.259\n#&gt; 20 factor3 =~  x2      -0.041 0.071 -0.578 0.563   -0.179    0.098\n#&gt; 21 factor3 =~  x3       0.113 0.049  2.324 0.020    0.018    0.209\n#&gt; 22 factor3 =~  x4       0.077 0.040  1.916 0.055   -0.002    0.155\n#&gt; 23 factor3 =~  x5       0.070 0.042  1.669 0.095   -0.012    0.153\n#&gt; 24 factor3 =~  x6       0.066 0.038  1.715 0.086   -0.009    0.141\n#&gt; 25 factor3 =~  x7       0.695 0.092  7.591 0.000    0.516    0.875\n#&gt; 26 factor3 =~  x8       0.704 0.083  8.520 0.000    0.542    0.865\n#&gt; 27 factor3 =~  x9       0.511 0.065  7.885 0.000    0.384    0.638\n\n\n\nRotazione ortogonale Quartimin:\n\n\nout_quartimin &lt;- orthRotate(\n  unrotated,\n  method = \"quartimin\"\n)\nsummary(out_quartimin, sort = FALSE, suppress = 0.3)\n#&gt; Standardized Rotated Factor Loadings\n#&gt;    factor1 factor2 factor3\n#&gt; x1  0.353*  0.590*        \n#&gt; x2          0.474*        \n#&gt; x3          0.657*        \n#&gt; x4  0.844*                \n#&gt; x5  0.869*                \n#&gt; x6  0.823*                \n#&gt; x7                  0.692*\n#&gt; x8                  0.702*\n#&gt; x9          0.397*  0.508*\n#&gt; \n#&gt; Factor Correlation\n#&gt;         factor1 factor2 factor3\n#&gt; factor1       1       0       0\n#&gt; factor2       0       1       0\n#&gt; factor3       0       0       1\n#&gt; \n#&gt; Method of rotation:  Quartimin \n#&gt; \n#&gt; Test Statistics for Standardized Rotated Factor Loadings\n#&gt;        lhs op rhs std.loading    se      z     p ci.lower ci.upper\n#&gt; 1  factor1 =~  x1       0.353 0.062  5.720 0.000    0.232    0.473\n#&gt; 2  factor1 =~  x2       0.158 0.066  2.375 0.018    0.028    0.288\n#&gt; 3  factor1 =~  x3       0.115 0.057  2.015 0.044    0.003    0.226\n#&gt; 4  factor1 =~  x4       0.844 0.027 30.814 0.000    0.790    0.898\n#&gt; 5  factor1 =~  x5       0.869 0.023 37.224 0.000    0.823    0.914\n#&gt; 6  factor1 =~  x6       0.823 0.024 35.026 0.000    0.777    0.869\n#&gt; 7  factor1 =~  x7       0.116 0.054  2.161 0.031    0.011    0.222\n#&gt; 8  factor1 =~  x8       0.104 0.054  1.914 0.056   -0.003    0.210\n#&gt; 9  factor1 =~  x9       0.202 0.059  3.403 0.001    0.086    0.319\n#&gt; 10 factor2 =~  x1       0.590 0.078  7.523 0.000    0.436    0.743\n#&gt; 11 factor2 =~  x2       0.474 0.068  6.935 0.000    0.340    0.608\n#&gt; 12 factor2 =~  x3       0.657 0.059 11.204 0.000    0.542    0.771\n#&gt; 13 factor2 =~  x4       0.072 0.041  1.748 0.080   -0.009    0.152\n#&gt; 14 factor2 =~  x5      -0.010 0.039 -0.263 0.793   -0.087    0.066\n#&gt; 15 factor2 =~  x6       0.122 0.040  3.008 0.003    0.042    0.201\n#&gt; 16 factor2 =~  x7      -0.071 0.048 -1.465 0.143   -0.166    0.024\n#&gt; 17 factor2 =~  x8       0.167 0.090  1.849 0.064   -0.010    0.343\n#&gt; 18 factor2 =~  x9       0.397 0.087  4.581 0.000    0.227    0.567\n#&gt; 19 factor3 =~  x1       0.124 0.071  1.740 0.082   -0.016    0.264\n#&gt; 20 factor3 =~  x2      -0.042 0.074 -0.574 0.566   -0.187    0.102\n#&gt; 21 factor3 =~  x3       0.114 0.054  2.093 0.036    0.007    0.221\n#&gt; 22 factor3 =~  x4       0.056 0.037  1.503 0.133   -0.017    0.128\n#&gt; 23 factor3 =~  x5       0.048 0.038  1.263 0.207   -0.027    0.123\n#&gt; 24 factor3 =~  x6       0.046 0.034  1.347 0.178   -0.021    0.112\n#&gt; 25 factor3 =~  x7       0.692 0.093  7.477 0.000    0.511    0.874\n#&gt; 26 factor3 =~  x8       0.702 0.084  8.331 0.000    0.537    0.867\n#&gt; 27 factor3 =~  x9       0.508 0.070  7.298 0.000    0.371    0.644\n\n\n\nRotazione obliqua Quartimin:\n\n\nout_oblq &lt;- oblqRotate(\n  unrotated,\n  method = \"quartimin\"\n)\nsummary(out_oblq, sort = FALSE, suppress = 0.3)\n#&gt; Standardized Rotated Factor Loadings\n#&gt;    factor1 factor2 factor3\n#&gt; x1          0.602*        \n#&gt; x2          0.505*        \n#&gt; x3          0.689*        \n#&gt; x4  0.840*                \n#&gt; x5  0.888*                \n#&gt; x6  0.808*                \n#&gt; x7                  0.723*\n#&gt; x8                  0.702*\n#&gt; x9          0.366*  0.463*\n#&gt; \n#&gt; Factor Correlation\n#&gt;         factor1 factor2 factor3\n#&gt; factor1  1.0000  0.3258  0.2164\n#&gt; factor2  0.3258  1.0000  0.2705\n#&gt; factor3  0.2164  0.2705  1.0000\n#&gt; \n#&gt; Method of rotation:  Quartimin \n#&gt; \n#&gt; Test Statistics for Standardized Rotated Factor Loadings\n#&gt;        lhs op rhs std.loading    se      z     p ci.lower ci.upper\n#&gt; 1  factor1 =~  x1       0.191 0.064  2.965 0.003    0.065    0.317\n#&gt; 2  factor1 =~  x2       0.044 0.066  0.665 0.506   -0.085    0.172\n#&gt; 3  factor1 =~  x3      -0.070 0.034 -2.031 0.042   -0.137   -0.002\n#&gt; 4  factor1 =~  x4       0.840 0.033 25.622 0.000    0.776    0.905\n#&gt; 5  factor1 =~  x5       0.888 0.027 32.583 0.000    0.835    0.942\n#&gt; 6  factor1 =~  x6       0.808 0.028 28.441 0.000    0.752    0.863\n#&gt; 7  factor1 =~  x7       0.044 0.037  1.179 0.238   -0.029    0.116\n#&gt; 8  factor1 =~  x8      -0.033 0.036 -0.916 0.360   -0.103    0.037\n#&gt; 9  factor1 =~  x9       0.035 0.048  0.728 0.467   -0.059    0.129\n#&gt; 10 factor2 =~  x1       0.602 0.086  7.003 0.000    0.434    0.771\n#&gt; 11 factor2 =~  x2       0.505 0.071  7.163 0.000    0.367    0.644\n#&gt; 12 factor2 =~  x3       0.689 0.056 12.344 0.000    0.580    0.799\n#&gt; 13 factor2 =~  x4       0.022 0.045  0.483 0.629   -0.067    0.110\n#&gt; 14 factor2 =~  x5      -0.067 0.036 -1.890 0.059   -0.137    0.002\n#&gt; 15 factor2 =~  x6       0.078 0.041  1.887 0.059   -0.003    0.158\n#&gt; 16 factor2 =~  x7      -0.152 0.037 -4.059 0.000   -0.225   -0.078\n#&gt; 17 factor2 =~  x8       0.104 0.109  0.960 0.337   -0.109    0.317\n#&gt; 18 factor2 =~  x9       0.366 0.097  3.780 0.000    0.176    0.556\n#&gt; 19 factor3 =~  x1       0.031 0.062  0.500 0.617   -0.090    0.152\n#&gt; 20 factor3 =~  x2      -0.117 0.066 -1.776 0.076   -0.245    0.012\n#&gt; 21 factor3 =~  x3       0.023 0.039  0.587 0.557   -0.054    0.100\n#&gt; 22 factor3 =~  x4       0.005 0.042  0.128 0.898   -0.076    0.087\n#&gt; 23 factor3 =~  x5       0.008 0.035  0.216 0.829   -0.061    0.076\n#&gt; 24 factor3 =~  x6      -0.011 0.030 -0.362 0.717   -0.070    0.048\n#&gt; 25 factor3 =~  x7       0.723 0.087  8.328 0.000    0.553    0.893\n#&gt; 26 factor3 =~  x8       0.702 0.098  7.137 0.000    0.509    0.894\n#&gt; 27 factor3 =~  x9       0.463 0.075  6.211 0.000    0.317    0.609\n\n\n\nRotazione ortogonale Geomin:\n\n\nout_geomin_orh &lt;- orthRotate(\n  unrotated,\n  method = \"geomin\"\n)\nsummary(out_geomin_orh, sort = FALSE, suppress = 0.3)\n#&gt; Standardized Rotated Factor Loadings\n#&gt;    factor1 factor2 factor3\n#&gt; x1  0.315*         -0.621*\n#&gt; x2                 -0.474*\n#&gt; x3                 -0.671*\n#&gt; x4  0.838*                \n#&gt; x5  0.867*                \n#&gt; x6  0.814*                \n#&gt; x7          0.696*        \n#&gt; x8          0.677*        \n#&gt; x9          0.456* -0.468*\n#&gt; \n#&gt; Factor Correlation\n#&gt;         factor1 factor2 factor3\n#&gt; factor1       1       0       0\n#&gt; factor2       0       1       0\n#&gt; factor3       0       0       1\n#&gt; \n#&gt; Method of rotation:  Geomin \n#&gt; \n#&gt; Test Statistics for Standardized Rotated Factor Loadings\n#&gt;        lhs op rhs std.loading    se       z     p ci.lower ci.upper\n#&gt; 1  factor1 =~  x1       0.315 0.134   2.352 0.019    0.053    0.578\n#&gt; 2  factor1 =~  x2       0.130 0.118   1.102 0.271   -0.101    0.360\n#&gt; 3  factor1 =~  x3       0.074 0.123   0.599 0.549   -0.168    0.316\n#&gt; 4  factor1 =~  x4       0.838 0.036  23.402 0.000    0.767    0.908\n#&gt; 5  factor1 =~  x5       0.867 0.025  35.190 0.000    0.819    0.915\n#&gt; 6  factor1 =~  x6       0.814 0.040  20.427 0.000    0.736    0.892\n#&gt; 7  factor1 =~  x7       0.112 0.068   1.645 0.100   -0.021    0.245\n#&gt; 8  factor1 =~  x8       0.085 0.063   1.341 0.180   -0.039    0.209\n#&gt; 9  factor1 =~  x9       0.172 0.095   1.803 0.071   -0.015    0.359\n#&gt; 10 factor2 =~  x1       0.053 0.061   0.868 0.386   -0.067    0.173\n#&gt; 11 factor2 =~  x2      -0.099 0.070  -1.409 0.159   -0.237    0.039\n#&gt; 12 factor2 =~  x3       0.033 0.048   0.698 0.485   -0.060    0.127\n#&gt; 13 factor2 =~  x4       0.051 0.046   1.115 0.265   -0.039    0.140\n#&gt; 14 factor2 =~  x5       0.054 0.061   0.878 0.380   -0.066    0.173\n#&gt; 15 factor2 =~  x6       0.035 0.037   0.924 0.355   -0.039    0.108\n#&gt; 16 factor2 =~  x7       0.696 0.090   7.702 0.000    0.519    0.874\n#&gt; 17 factor2 =~  x8       0.677 0.088   7.660 0.000    0.504    0.850\n#&gt; 18 factor2 =~  x9       0.456 0.073   6.261 0.000    0.314    0.599\n#&gt; 19 factor3 =~  x1      -0.621 0.101  -6.148 0.000   -0.818   -0.423\n#&gt; 20 factor3 =~  x2      -0.474 0.074  -6.414 0.000   -0.619   -0.329\n#&gt; 21 factor3 =~  x3      -0.671 0.058 -11.658 0.000   -0.784   -0.558\n#&gt; 22 factor3 =~  x4      -0.129 0.161  -0.800 0.424   -0.445    0.187\n#&gt; 23 factor3 =~  x5      -0.048 0.174  -0.277 0.782   -0.389    0.293\n#&gt; 24 factor3 =~  x6      -0.176 0.164  -1.071 0.284   -0.497    0.146\n#&gt; 25 factor3 =~  x7      -0.021 0.059  -0.354 0.724   -0.137    0.095\n#&gt; 26 factor3 =~  x8      -0.257 0.094  -2.720 0.007   -0.442   -0.072\n#&gt; 27 factor3 =~  x9      -0.468 0.105  -4.472 0.000   -0.673   -0.263\n\n\n\nRotazione obliqua Geomin:\n\n\nout_geomin_obl &lt;- oblqRotate(\n  unrotated,\n  method = \"geomin\"\n)\nsummary(out_geomin_obl, sort = FALSE, suppress = 0.3)\n#&gt; Standardized Rotated Factor Loadings\n#&gt;    factor1 factor2 factor3\n#&gt; x1                 -0.604*\n#&gt; x2                 -0.507*\n#&gt; x3                 -0.691*\n#&gt; x4  0.839*                \n#&gt; x5  0.887*                \n#&gt; x6  0.806*                \n#&gt; x7          0.726*        \n#&gt; x8          0.703*        \n#&gt; x9          0.463* -0.368*\n#&gt; \n#&gt; Factor Correlation\n#&gt;         factor1 factor2 factor3\n#&gt; factor1  1.0000  0.2296 -0.3272\n#&gt; factor2  0.2296  1.0000 -0.2777\n#&gt; factor3 -0.3272 -0.2777  1.0000\n#&gt; \n#&gt; Method of rotation:  Geomin \n#&gt; \n#&gt; Test Statistics for Standardized Rotated Factor Loadings\n#&gt;        lhs op rhs std.loading    se       z     p ci.lower ci.upper\n#&gt; 1  factor1 =~  x1       0.188 0.070   2.670 0.008    0.050    0.326\n#&gt; 2  factor1 =~  x2       0.044 0.054   0.806 0.420   -0.063    0.150\n#&gt; 3  factor1 =~  x3      -0.073 0.049  -1.467 0.142   -0.170    0.024\n#&gt; 4  factor1 =~  x4       0.839 0.032  26.467 0.000    0.777    0.901\n#&gt; 5  factor1 =~  x5       0.887 0.029  30.077 0.000    0.829    0.945\n#&gt; 6  factor1 =~  x6       0.806 0.030  26.717 0.000    0.747    0.865\n#&gt; 7  factor1 =~  x7       0.031 0.034   0.915 0.360   -0.036    0.099\n#&gt; 8  factor1 =~  x8      -0.045 0.048  -0.948 0.343   -0.139    0.048\n#&gt; 9  factor1 =~  x9       0.025 0.034   0.747 0.455   -0.041    0.091\n#&gt; 10 factor2 =~  x1       0.029 0.051   0.567 0.571   -0.071    0.129\n#&gt; 11 factor2 =~  x2      -0.119 0.072  -1.664 0.096   -0.260    0.021\n#&gt; 12 factor2 =~  x3       0.020 0.037   0.538 0.591   -0.053    0.092\n#&gt; 13 factor2 =~  x4       0.007 0.043   0.174 0.862   -0.076    0.091\n#&gt; 14 factor2 =~  x5       0.010 0.036   0.285 0.775   -0.060    0.080\n#&gt; 15 factor2 =~  x6      -0.009 0.030  -0.309 0.758   -0.068    0.049\n#&gt; 16 factor2 =~  x7       0.726 0.072  10.065 0.000    0.585    0.868\n#&gt; 17 factor2 =~  x8       0.703 0.118   5.963 0.000    0.472    0.934\n#&gt; 18 factor2 =~  x9       0.463 0.080   5.810 0.000    0.307    0.619\n#&gt; 19 factor3 =~  x1      -0.604 0.081  -7.438 0.000   -0.763   -0.445\n#&gt; 20 factor3 =~  x2      -0.507 0.073  -6.983 0.000   -0.649   -0.364\n#&gt; 21 factor3 =~  x3      -0.691 0.061 -11.373 0.000   -0.810   -0.572\n#&gt; 22 factor3 =~  x4      -0.024 0.034  -0.702 0.482   -0.091    0.043\n#&gt; 23 factor3 =~  x5       0.065 0.045   1.459 0.144   -0.022    0.153\n#&gt; 24 factor3 =~  x6      -0.080 0.048  -1.679 0.093   -0.173    0.013\n#&gt; 25 factor3 =~  x7       0.150 0.107   1.403 0.161   -0.060    0.360\n#&gt; 26 factor3 =~  x8      -0.106 0.164  -0.645 0.519   -0.428    0.216\n#&gt; 27 factor3 =~  x9      -0.368 0.133  -2.770 0.006   -0.629   -0.108\n\nLa rotazione Geomin è molto popolare perché minimizza la media geometrica dei quadrati delle saturazioni fattoriali, forzando in un certo senso i loadings a polarizzarsi (alti o bassi), favorendo nuovamente la struttura semplice. In Mplus, per esempio, Geomin è il default per la rotazione obliqua.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>La rotazione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/04_rotazione.html#riflessioni-conclusive",
    "href": "chapters/extraction/04_rotazione.html#riflessioni-conclusive",
    "title": "29  La rotazione fattoriale",
    "section": "\n29.9 Riflessioni Conclusive",
    "text": "29.9 Riflessioni Conclusive\n\n\nL’indeterminatezza della soluzione fattoriale implica che non esiste una sola matrice di saturazioni in grado di spiegare una data matrice di correlazioni.\nPer gestire l’indeterminatezza, si adottano i criteri di parsimonia (minimo numero di fattori adeguati a spiegare la struttura dei dati) e di semplicità (rotazione per favorire un pattern di saturazioni più interpretabile).\nLe rotazioni ortogonali (es. Varimax) mantengono i fattori non correlati, semplificando l’interpretazione ma a volte restringendo eccessivamente il modello se in realtà i costrutti sono correlati.\nLe rotazioni oblique (es. Promax, Oblimin, Geomin) consentono ai fattori di correlare, spesso fornendo soluzioni più aderenti alla realtà psicologica, ma che richiedono di distinguere tra la matrice Pattern (saturazioni dirette) e la matrice di Struttura (correlazioni globali).\nL’interpretazione dei fattori dovrebbe basarsi prevalentemente sui coefficienti diretti della matrice Pattern, ricordando comunque di consultare la matrice di Struttura per una comprensione più ampia dell’influenza dei fattori sulle variabili.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>La rotazione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html#esempio-in-r-confronto-tra-metodi-di-estrazione",
    "href": "chapters/extraction/02_estrazione.html#esempio-in-r-confronto-tra-metodi-di-estrazione",
    "title": "27  L’estrazione dei fattori",
    "section": "\n27.7 Esempio in R: Confronto tra Metodi di Estrazione",
    "text": "27.7 Esempio in R: Confronto tra Metodi di Estrazione\nPer illustrare i principali metodi di estrazione dei fattori, useremo un semplice esempio tratto da Rencher (2010). Una ragazza ha valutato 7 persone su 5 tratti personali:\n\n\nK = Kind (Gentile)\n\n\nI = Intelligent (Intelligente)\n\n\nH = Happy (Felice)\n\n\nL = Likeable (Simpatica)\n\n\nJ = Just (Giusta)\n\nLa matrice di correlazione tra i tratti è la seguente:\n\nR &lt;- matrix(c(\n  1.000, .296, .881, .995, .545,\n  .296, 1.000, -.022, .326, .837,\n  .881, -.022, 1.000, .867, .130,\n  .995, .326, .867, 1.000, .544,\n  .545, .837, .130, .544, 1.000\n),\nncol = 5, byrow = TRUE,\ndimnames = list(c(\"K\", \"I\", \"H\", \"L\", \"J\"),\n                c(\"K\", \"I\", \"H\", \"L\", \"J\")))\nR\n#&gt;       K      I      H     L     J\n#&gt; K 1.000  0.296  0.881 0.995 0.545\n#&gt; I 0.296  1.000 -0.022 0.326 0.837\n#&gt; H 0.881 -0.022  1.000 0.867 0.130\n#&gt; L 0.995  0.326  0.867 1.000 0.544\n#&gt; J 0.545  0.837  0.130 0.544 1.000\n\n\n27.7.1 Metodo delle Componenti Principali (PCA)\n1. Calcolo degli autovalori e autovettori.\n\ne &lt;- eigen(R)\ne$values       # varianza spiegata da ciascuna componente\n#&gt; [1] 3.2633766 1.5383821 0.1679693 0.0300298 0.0002422\ne$vectors      # coefficienti delle combinazioni lineari\n#&gt;         [,1]    [,2]     [,3]    [,4]    [,5]\n#&gt; [1,] -0.5367 -0.1860 -0.18992 -0.1248  0.7910\n#&gt; [2,] -0.2875  0.6506  0.68489 -0.1198  0.1034\n#&gt; [3,] -0.4344 -0.4737  0.40695  0.6137 -0.2116\n#&gt; [4,] -0.5374 -0.1693 -0.09533 -0.6294 -0.5266\n#&gt; [5,] -0.3897  0.5377 -0.56583  0.4442 -0.2037\n\n\nGli autovalori indicano quanta varianza spiega ciascuna componente.\nGli autovettori sono le “direzioni” lungo cui le componenti combinano le variabili.\n\n2. Verifica della decomposizione spettrale.\n\nround(e$vectors %*% diag(e$values) %*% t(e$vectors), 3)\n#&gt;       [,1]   [,2]   [,3]  [,4]  [,5]\n#&gt; [1,] 1.000  0.296  0.881 0.995 0.545\n#&gt; [2,] 0.296  1.000 -0.022 0.326 0.837\n#&gt; [3,] 0.881 -0.022  1.000 0.867 0.130\n#&gt; [4,] 0.995  0.326  0.867 1.000 0.544\n#&gt; [5,] 0.545  0.837  0.130 0.544 1.000\n\nQuesta moltiplicazione ricostruisce la matrice di correlazione originale:\\(\\mathbf{R} = \\mathbf{C} \\mathbf{D} \\mathbf{C}^{\\mathsf{T}}\\)\n3. Varianza spiegata dai primi 2 fattori.\n\nsum(e$values[1:2]) / sum(e$values)\n#&gt; [1] 0.9604\n\nInterpretazione: Se i primi due autovalori spiegano, ad esempio, il 96% della varianza totale, possiamo ridurre da 5 a 2 dimensioni con perdita minima di informazione.\n4. Calcolo delle saturazioni fattoriali (matrice \\(\\hat{\\Lambda}\\)).\n\nL &lt;- cbind(\n  e$vectors[, 1] * sqrt(e$values[1]),\n  e$vectors[, 2] * sqrt(e$values[2])\n)\nround(L, 3)\n#&gt;        [,1]   [,2]\n#&gt; [1,] -0.970 -0.231\n#&gt; [2,] -0.519  0.807\n#&gt; [3,] -0.785 -0.588\n#&gt; [4,] -0.971 -0.210\n#&gt; [5,] -0.704  0.667\n\n\nOgni colonna rappresenta una componente.\nOgni riga rappresenta una variabile.\nGli elementi indicano quanto una variabile satura su una componente.\n\n5. Matrice riprodotta e residui.\n\nR_hat &lt;- round(L %*% t(L), 3)\nresidui &lt;- round(R - R_hat, 3)\nresidui\n#&gt;        K      I      H      L      J\n#&gt; K  0.007 -0.021 -0.015  0.005  0.016\n#&gt; I -0.021  0.079  0.045 -0.009 -0.067\n#&gt; H -0.015  0.045  0.039 -0.018 -0.030\n#&gt; L  0.005 -0.009 -0.018  0.013  0.001\n#&gt; J  0.016 -0.067 -0.030  0.001  0.060\n\nSe i residui (cioè la differenza tra \\(\\mathbf{R}\\) e \\(\\hat{\\Lambda} \\hat{\\Lambda}^\\mathsf{T}\\)) sono piccoli, la soluzione a 2 fattori è soddisfacente.\n\n27.7.2 Metodo dei Fattori Principali\nIl metodo dei fattori principali (o principal factor method) mira a estrarre solo la varianza comune tra le variabili, escludendo la varianza specifica (quella unica di ciascuna variabile).\nStima iniziale delle comunalità.\nPer avere una stima iniziale delle comunalità, qui si propone di prendere, per ciascuna variabile, il massimo valore di correlazione in valore assoluto, escludendo la diagonale principale della matrice delle correlazioni (che rappresenta la correlazione di ogni variabile con sé stessa, pari a 1). In R si può fare così:\n\n# Copiamo la matrice di correlazione\nR_no_diag &lt;- R\n\n# Mettiamo 0 sulla diagonale (anziché 1)\ndiag(R_no_diag) &lt;- 0\n\n# Calcoliamo il massimo (in valore assoluto) per ogni riga\nh.hat &lt;- apply(abs(R_no_diag), 1, max)\n\n# Arrotondiamo a 3 cifre decimali\nround(h.hat, 3)\n#&gt;     K     I     H     L     J \n#&gt; 0.995 0.837 0.881 0.995 0.837\n\nQuesta stima della comunalità (per ogni variabile) è volutamente molto semplificata e potrebbe risultare meno accurata di altre procedure (ad esempio, correlazione multipla al quadrato). Tuttavia, si usa spesso a scopo didattico per illustrare l’algoritmo.\nMatrice ridotta.\nCreiamo la matrice ridotta sostituendo la diagonale di \\(R\\) (che vale 1, in quanto matrice di correlazione) con i valori stimati di comunalità:\n\nR1 &lt;- R\ndiag(R1) &lt;- h.hat\nR1\n#&gt;       K      I      H     L     J\n#&gt; K 0.995  0.296  0.881 0.995 0.545\n#&gt; I 0.296  0.837 -0.022 0.326 0.837\n#&gt; H 0.881 -0.022  0.881 0.867 0.130\n#&gt; L 0.995  0.326  0.867 0.995 0.544\n#&gt; J 0.545  0.837  0.130 0.544 0.837\n\nDecomposizione della matrice ridotta.\nCalcoliamo gli autovalori e gli autovettori della matrice ridotta:\n\nee &lt;- eigen(R1)\nround(ee$values, 3)  # autovalori\n#&gt; [1]  3.202  1.394  0.029  0.000 -0.080\n\nSaturazioni fattoriali.\nInfine, estraiamo le saturazioni fattoriali per i fattori desiderati (ad esempio, i primi due). Le saturazioni (\\(\\mathbf{L}\\)) indicano la relazione tra le variabili originali e i fattori latenti, considerando solo la varianza comune:\n\nL &lt;- ee$vectors[, 1:2] %*% sqrt(diag(ee$values[1:2]))\nround(L, 3)\n#&gt;       [,1]   [,2]\n#&gt; [1,] 0.981 -0.209\n#&gt; [2,] 0.487  0.774\n#&gt; [3,] 0.772 -0.544\n#&gt; [4,] 0.982 -0.187\n#&gt; [5,] 0.667  0.648\n\nConclusione: questa procedura illustra il metodo dei fattori principali e mostra come escludere correttamente la diagonale principale dal calcolo delle comunalità iniziali, utilizzando il massimo (in valore assoluto) delle correlazioni di ogni variabile con le altre.\n\n27.7.3 Metodo dei Fattori Principali Iterato\nQuesto metodo aggiorna iterativamente le stime delle comunalità finché le saturazioni non cambiano più (convergenza).\nEsecuzione in R con il pacchetto psych.\n\npa &lt;- psych::fa(R, nfactors = 2, rotate = \"none\", fm = \"pa\")\npa\n#&gt; Factor Analysis using method =  pa\n#&gt; Call: psych::fa(r = R, nfactors = 2, rotate = \"none\", fm = \"pa\")\n#&gt; Standardized loadings (pattern matrix) based upon correlation matrix\n#&gt;    PA1   PA2   h2     u2 com\n#&gt; K 0.98 -0.21 1.01 -0.008 1.1\n#&gt; I 0.48  0.74 0.77  0.230 1.7\n#&gt; H 0.78 -0.56 0.92  0.085 1.8\n#&gt; L 0.98 -0.19 0.99  0.010 1.1\n#&gt; J 0.69  0.69 0.95  0.049 2.0\n#&gt; \n#&gt;                        PA1  PA2\n#&gt; SS loadings           3.22 1.41\n#&gt; Proportion Var        0.64 0.28\n#&gt; Cumulative Var        0.64 0.93\n#&gt; Proportion Explained  0.70 0.30\n#&gt; Cumulative Proportion 0.70 1.00\n#&gt; \n#&gt; Mean item complexity =  1.5\n#&gt; Test of the hypothesis that 2 factors are sufficient.\n#&gt; \n#&gt; df null model =  10  with the objective function =  12\n#&gt; df of  the model are 1  and the objective function was  5.6 \n#&gt; \n#&gt; The root mean square of the residuals (RMSR) is  0.01 \n#&gt; The df corrected root mean square of the residuals is  0.04 \n#&gt; \n#&gt; Fit based upon off diagonal values = 1\n\n\n\nfm = \"pa\": specifica il metodo dei fattori principali.\nL’output include:\n\nsaturazioni fattoriali;\nunicità (\\(1 - h^2\\));\nvarianza spiegata da ciascun fattore.\n\n\n\n⚠️ Se una unicità &gt; 1 o negativa → soluzione impropria (caso di Heywood).",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  }
]