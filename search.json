[
  {
    "objectID": "chapters/measurement/01_scores_scales.html",
    "href": "chapters/measurement/01_scores_scales.html",
    "title": "1  Punteggi e scale",
    "section": "",
    "text": "1.1 Punteggi Grezzi e Trasformati\nNell’ambito dei test psicometrici, il punteggio grezzo costituisce la valutazione più immediata e si basa sulla somma delle risposte categorizzate, come quelle corrette o errate, o vero o falso. Nonostante la sua immediatezza, il punteggio grezzo presenta limitazioni interpretative, poiché non considera fattori contestuali quali il numero totale di domande o il livello di difficoltà di queste.\nPer mitigare queste limitazioni, i punteggi grezzi vengono spesso convertiti in formati che permettono un’interpretazione più contestualizzata, quali i punteggi standardizzati o scalati. Queste trasformazioni facilitano l’interpretazione dei risultati ottenuti.\nL’interpretazione dei risultati dei test necessita di un riferimento comparativo. A seconda del contesto, può essere utile confrontare le prestazioni con una norma di riferimento o con criteri specifici.\nLe interpretazioni basate sulla norma confrontano la performance di un individuo con quella di un gruppo di riferimento o normativo, offrendo una valutazione relativa alla prestazione tipica o “normale”. Un esempio è rappresentato dai test di intelligenza. Al contrario, le interpretazioni basate sul criterio valutano le prestazioni rispetto a un livello di competenza specifico, indipendentemente dalla performance altrui.\nUn altro approccio interpretativo è offerto dalla Teoria della Risposta agli Item (IRT), che fornisce un’analisi avanzata delle prestazioni nei test, permettendo un’esplorazione dettagliata delle risposte individuali.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#interpretazioni-basate-sulla-norma-norm-referenced",
    "href": "chapters/measurement/01_scores_scales.html#interpretazioni-basate-sulla-norma-norm-referenced",
    "title": "1  Punteggi e scale",
    "section": "1.2 Interpretazioni Basate sulla Norma (Norm-Referenced)",
    "text": "1.2 Interpretazioni Basate sulla Norma (Norm-Referenced)\nPer valutare la performance in un test psicologico, può essere utile confrontarla con quella di un gruppo predefinito. I punteggi grezzi acquisiscono significato quando messi a confronto con le prestazioni di un gruppo normativo. In questo contesto, i punteggi grezzi vengono trasformati in punteggi derivati basati sulle performance di un gruppo normativo specifico.\nUn aspetto cruciale in queste interpretazioni è la pertinenza del gruppo di riferimento. È fondamentale che questo gruppo sia rappresentativo degli individui ai quali il test è destinato o con cui il partecipante viene confrontato.\nLa selezione del campione normativo, chiamato anche campione di standardizzazione, segue il principio del campionamento casuale stratificato proporzionale, assicurando che il campione rifletta proporzionalmente le caratteristiche demografiche nazionali. Tale rappresentatività è vitale per l’interpretazione basata sulla norma, rendendo necessaria l’accurata selezione e descrizione del campione da parte degli sviluppatori del test.\nQuando si utilizzano questi test, è cruciale valutare se il campione di standardizzazione è rappresentativo per l’uso previsto e se le caratteristiche demografiche del campione corrispondono a quelle dei soggetti testati. La pertinenza e l’attualità del campione, insieme alla sua dimensione, sono fattori chiave per garantire interpretazioni valide e affidabili.\nUna considerazione finale riguardante le interpretazioni basate sulla norma è l’importanza della standardizzazione nella somministrazione. È fondamentale che il campione di riferimento venga sottoposto al test nelle stesse condizioni e secondo le stesse procedure amministrative che saranno utilizzate nella pratica effettiva. Di conseguenza, quando il test viene somministrato in contesti clinici, è cruciale che l’utente del test segua attentamente le procedure amministrative prescritte. Ad esempio, nel caso di test standardizzati, è essenziale leggere le istruzioni testuali esattamente come sono fornite e rispettare rigorosamente i limiti di tempo. Sarebbe irragionevole confrontare la performance dell’esaminando in un test a tempo con quella di un campione di standardizzazione che ha avuto più o meno tempo per completare gli item. Questa necessità di seguire procedure standardizzate si applica a tutti i test standardizzati, sia quelli con interpretazioni basate sulla norma che quelli basati sul criterio.\n\n1.2.1 Punteggi Derivati\nIn ambito psicometrico, i punteggi derivati da test possono assumere diverse forme, ciascuna con implicazioni specifiche per l’interpretazione dei dati. Esploreremo le tipologie più comuni:\n\nPunteggi Standardizzati:\n\nQuesti punteggi trasformano i punteggi grezzi (ad esempio, il numero di risposte corrette) in misure standardizzate. Ciò permette di ottenere valori invarianti rispetto a variabili come l’età dell’individuo.\nSi calcolano stabilendo una media e una deviazione standard specifiche a priori.\nEsempi:\n\nz-scores: Misurano la distanza di un punteggio dalla media, espressa in deviazioni standard. Hanno una media di 0 e una deviazione standard di 1.\nT-scores: Trasformano i punteggi in valori positivi, con una media di 50 e una deviazione standard di 10.\nPunteggi di QI: Tipici delle scale di intelligenza, hanno una media di 100 e una deviazione standard di 15.\n\n\nPunteggi Standardizzati Normalizzati:\n\nQuando i punteggi originali non seguono una distribuzione normale, si utilizzano trasformazioni non lineari per normalizzarli.\nEsempi:\n\nStanine: Suddividono i punteggi in 9 categorie (da 1 a 9).\nPunteggi scalati di Wechsler: Utilizzati nei test di intelligenza di Wechsler.\nEquivalenti della Curva Normale (NCE): Esprimono la posizione di un punteggio rispetto alla distribuzione normale.\n\n\nRanghi Percentili:\n\nVanno da 1 a 99 e indicano la posizione relativa di un soggetto rispetto alla popolazione.\nAd esempio, un punteggio al 75° percentile significa che il soggetto ha ottenuto un risultato migliore del 75% della popolazione.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#interpretazioni-basate-su-criteri",
    "href": "chapters/measurement/01_scores_scales.html#interpretazioni-basate-su-criteri",
    "title": "1  Punteggi e scale",
    "section": "1.3 Interpretazioni Basate su Criteri",
    "text": "1.3 Interpretazioni Basate su Criteri\nL’approccio delle valutazioni basate su criteri specifici è diventato sempre più rilevante nel mondo dell’educazione e della psicometria a partire dagli anni Sessanta. Questo approccio, noto anche come valutazione basata su contenuti, dominio o obiettivi, si concentra sulla misurazione delle competenze individuali rispetto a standard definiti, piuttosto che sul confronto con le prestazioni di un gruppo di riferimento.\nEcco alcune metodologie e applicazioni comuni:\n\nPercentuale di Risposte Corrette:\n\nQuesto metodo fornisce un’indicazione diretta delle competenze di uno studente.\nAd esempio, se uno studente risponde correttamente all’85% delle domande di matematica, l’insegnante può valutare le sue abilità in modo specifico.\n\nTest di Padronanza:\n\nQuesti test determinano se uno studente ha acquisito una competenza specifica.\nAd esempio, gli esami per la patente di guida valutano se lo studente ha raggiunto il livello di padronanza richiesto.\n\nValutazioni Basate su Standard:\n\nQueste valutazioni classificano i risultati in categorie di prestazione (ad esempio, base, competente, avanzato).\nSpesso, i punteggi vengono correlati a voti letterali basati su una percentuale di correttezza.\n\n\nI punti di forza delle valutazioni basate su criteri includono:\n\nComparazione con Standard Predefiniti:\n\nValutano il raggiungimento di competenze o obiettivi specifici, indipendentemente dalle prestazioni altrui.\nQuesto approccio evita il bias derivante dal confronto con altri studenti.\n\nFocalizzazione su Competenze Specifiche:\n\nQuesti test richiedono una definizione precisa dell’area di conoscenza o abilità valutata.\nSono ideali per valutare aree di contenuto specifiche.\n\n\n\n1.3.0.1 Benefici\n\nValutazione Mirata delle Competenze: Fornisce una verifica concreta del conseguimento delle conoscenze e abilità delineate dal programma di studi.\nPersonalizzazione dell’Insegnamento: Identifica le aree di debolezza, consentendo un approccio didattico più focalizzato e personalizzato.\n\nIn conclusione, le valutazioni basate su criteri rappresentano un’alternativa preziosa ai metodi di valutazione tradizionali, specialmente in contesti in cui è fondamentale misurare le competenze individuali. Questo approccio è in crescente adozione in ambiti educativi e formativi, enfatizzando l’importanza dell’acquisizione di conoscenze e abilità mirate.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#analisi-comparativa-tra-valutazioni-normative-e-basate-su-criteri",
    "href": "chapters/measurement/01_scores_scales.html#analisi-comparativa-tra-valutazioni-normative-e-basate-su-criteri",
    "title": "1  Punteggi e scale",
    "section": "1.4 Analisi Comparativa tra Valutazioni Normative e Basate su Criteri",
    "text": "1.4 Analisi Comparativa tra Valutazioni Normative e Basate su Criteri\nLa distinzione tra valutazioni normative (norm-referenced) e basate su criteri (criterion-referenced) è fondamentale per interpretare le prestazioni individuali nei test. Sebbene un test possa teoricamente adottare entrambi gli approcci interpretativi, di solito si orienta verso uno dei due, a seconda dell’obiettivo specifico.\nEcco una panoramica delle differenze:\n\nValutazioni Normative:\n\nVersatilità: Si applicano a test che valutano una vasta gamma di dimensioni, come attitudini, risultati scolastici, interessi, atteggiamenti e comportamenti.\nAmpio Quadro: Ideali per esplorare costrutti generali come l’attitudine generale o l’intelligenza.\nSelezione delle Domande: Preferiscono domande di difficoltà intermedia, evitando quelle troppo semplici o complesse.\n\nValutazioni Basate su Criteri:\n\nSpecificità: Associate principalmente a test che mirano a valutare conoscenze o competenze specifiche.\nFocalizzazione: Concentrate su abilità e competenze ben definite.\nCalibrazione delle Domande: La difficoltà delle domande è tarata in base alle conoscenze o abilità specifiche da valutare.\n\n\nÈ importante notare che queste interpretazioni non sono mutuamente esclusive. Alcuni test offrono sia valutazioni normative che basate su criteri, fornendo una visione completa delle prestazioni relative rispetto a un gruppo di riferimento e del livello di competenza in un ambito specifico. Questa dualità interpretativa è preziosa in vari contesti.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#analisi-dei-punteggi-secondo-la-teoria-della-risposta-agli-item",
    "href": "chapters/measurement/01_scores_scales.html#analisi-dei-punteggi-secondo-la-teoria-della-risposta-agli-item",
    "title": "1  Punteggi e scale",
    "section": "1.5 Analisi dei Punteggi secondo la Teoria della Risposta agli Item",
    "text": "1.5 Analisi dei Punteggi secondo la Teoria della Risposta agli Item\nLa Teoria della Risposta agli Item (IRT) rappresenta un notevole avanzamento nel campo della psicometria, fornendo strumenti essenziali per valutare con precisione le capacità e i tratti latenti degli individui.\nFondamenti e Principi dell’IRT: L’IRT si basa sull’assunto che ogni persona possieda un livello di un tratto latente, come l’intelligenza, che è indipendente dalle specifiche domande del test o dal metodo di valutazione utilizzato. Attraverso l’applicazione di modelli matematici complessi, l’IRT consente di posizionare ogni individuo su un continuum di tratto latente, offrendo una misurazione delle capacità più precisa rispetto ai tradizionali punteggi grezzi.\nVantaggi dei Punteggi basati sull’IRT: I punteggi derivati dall’IRT presentano significativi vantaggi. Essi sono trattati come punteggi a intervalli costanti, consentendo comparazioni valide tra le performance di soggetti o gruppi diversi. Inoltre, questi punteggi mantengono una deviazione standard uniforme attraverso diverse fasce d’età, rendendoli particolarmente adatti per monitorare l’evoluzione o il progresso delle abilità nel tempo.\nApplicazioni Pratiche e Prospettive Future dell’IRT: Una delle applicazioni più innovative dell’IRT è lo sviluppo dei test adattivi computerizzati (CAT), in cui le domande vengono selezionate dinamicamente in base alle risposte precedenti del candidato. Questo metodo consente valutazioni precise ed efficienti delle abilità in tempo reale. Ad esempio, i punteggi IRT, come i W-scores nel Woodcock-Johnson IV, vengono utilizzati per analizzare variazioni nelle capacità cognitive legate ai processi di apprendimento o ai declini cognitivi.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#quali-tipi-di-punteggi-usare",
    "href": "chapters/measurement/01_scores_scales.html#quali-tipi-di-punteggi-usare",
    "title": "1  Punteggi e scale",
    "section": "1.6 Quali tipi di punteggi usare?",
    "text": "1.6 Quali tipi di punteggi usare?",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#la-selezione-del-punteggio-appropriato-per-la-valutazione",
    "href": "chapters/measurement/01_scores_scales.html#la-selezione-del-punteggio-appropriato-per-la-valutazione",
    "title": "1  Punteggi e scale",
    "section": "1.7 La Selezione del Punteggio Appropriato per la Valutazione",
    "text": "1.7 La Selezione del Punteggio Appropriato per la Valutazione\nDeterminare il tipo di punteggio più adeguato per un test è essenziale per ottenere informazioni specifiche e pertinenti dalla valutazione. Le diverse categorie di punteggi forniscono risposte a domande distinte riguardo alle prestazioni degli esaminandi:\n\nPunteggi Grezzi:\n\nRappresentano la quantità totale di risposte corrette accumulate da un individuo.\nOffrono una visione immediata del livello di prestazione e permettono di stabilire un ordine tra i partecipanti.\nSono utili per identificare rapidamente il posizionamento relativo di un individuo all’interno di un gruppo.\n\nPunteggi Norm-Referenced Standard:\n\nForniscono un confronto diretto tra le prestazioni di un individuo e quelle di un gruppo normativo.\nConsentono di interpretare la prestazione su una scala relativa, facilitando la comprensione del rendimento in termini di posizione all’interno di una popolazione di riferimento.\n\nPunteggi Criterion-Referenced:\n\nIndicano se un individuo ha raggiunto un determinato standard di competenza.\nSono particolarmente indicati per valutare il conseguimento di obiettivi specifici o competenze chiave.\n\nPunteggi Basati sull’IRT (Inclusi i Punteggi Rasch):\n\nOffrono una misurazione su scala a intervalli costanti, riflettendo la posizione di un individuo su un continuum di un tratto latente.\nSono ideali per tracciare il progresso nel tempo o confrontare le prestazioni attraverso diverse valutazioni di un medesimo tratto.\n\n\nAd esempio, nel caso di Giovanni, che ha beneficiato di un programma di supporto alla lettura: - Punteggi Norm-Referenced: Fornirebbero insight su come le capacità di lettura di Giovanni si confrontano con quelle dei suoi coetanei dopo l’intervento. - Punteggi Rasch o IRT: Consentirebbero di valutare l’evoluzione precisa delle competenze di lettura di Giovanni, misurando il progresso a partire dal suo livello iniziale. - Punteggi Grezzi: Darebbero indicazioni sul miglioramento assoluto, sebbene privi della capacità di riflettere le variazioni in termini di difficoltà degli item o di altri fattori. - Punteggi Criterion-Referenced: Stabilirebbero se Giovanni ha raggiunto specifici obiettivi di competenza in lettura definiti a priori.\nIn contesti educativi, l’uso di punteggi norm-referenced standardizzati per età può essere preferibile per determinare se uno studente sta progredendo adeguatamente rispetto ai suoi pari. In contesti clinici, come nella gestione della depressione, i punteggi criterion-referenced possono offrire una valutazione mirata del raggiungimento di soglie di miglioramento clinico significativo.\nIn conclusione, la scelta del tipo di punteggio da utilizzare è guidata dal contesto di valutazione e dall’obiettivo specifico della misurazione. Diverse tipologie di punteggi illuminano aspetti distinti delle prestazioni, rendendoli più o meno adatti a seconda delle esigenze informative della valutazione.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#significato-e-applicazione-delle-norme-e-dei-punteggi-standardizzati",
    "href": "chapters/measurement/01_scores_scales.html#significato-e-applicazione-delle-norme-e-dei-punteggi-standardizzati",
    "title": "1  Punteggi e scale",
    "section": "1.8 Significato e Applicazione delle Norme e dei Punteggi Standardizzati",
    "text": "1.8 Significato e Applicazione delle Norme e dei Punteggi Standardizzati\nPer chiarire questi concetti, esaminiamo i dati della Tabella 2.1 di {cite:t}bandalos2018measurement. Con degli esempi numerici, analizzeremo vari tipi di punteggi normativi, tra cui:\n\nPunteggi Percentili: Che indicano la posizione relativa di un individuo all’interno del gruppo normativo.\nPunteggi Standardizzati e Normalizzati: Che trasformano i punteggi grezzi in una scala standard per facilitare il confronto tra diversi individui o gruppi.\nStanini: Un metodo di punteggio che divide i punteggi in intervalli standardizzati.\nEquivalenti alla Curva Normale: Che adattano i punteggi a una distribuzione normale.\n\nNei capitoli successivi esamineremo come calcolare i punteggi basati sulla teoria IRT.\nIniziamo a leggere i dati.\n\nraw_score &lt;- c(\n    26, 25, 33, 31, 26, 34, 29, 36, 25, 29, 28, 32, 25,\n    30, 27, 31, 30, 30, 35, 30, 27, 26, 34, 32, 26, 34,\n    30, 28, 28, 31, 30, 27, 26, 29, 29, 33, 27, 35, 26,\n    27, 28, 29, 28, 27, 34, 36, 26, 26, 34, 30, 34, 27\n)\n\n\n1.8.1 Distribuzione di frequenze\n\nfreq &lt;- table(raw_score) # frequency\ncumfreq &lt;- cumsum(freq) # cumulative frequency\nperc &lt;- prop.table(freq) * 100 # percentage\ncumperc &lt;- cumsum(perc) # cumulative percentage\npr &lt;- (cumperc - 0.5 * perc) # percentile rank\ncbind(freq, cumfreq, perc, cumperc, pr)\n\n\nA matrix: 12 x 5 of type dbl\n\n\n\nfreq\ncumfreq\nperc\ncumperc\npr\n\n\n\n\n25\n3\n3\n5.769231\n5.769231\n2.884615\n\n\n26\n8\n11\n15.384615\n21.153846\n13.461538\n\n\n27\n7\n18\n13.461538\n34.615385\n27.884615\n\n\n28\n5\n23\n9.615385\n44.230769\n39.423077\n\n\n29\n5\n28\n9.615385\n53.846154\n49.038462\n\n\n30\n7\n35\n13.461538\n67.307692\n60.576923\n\n\n31\n3\n38\n5.769231\n73.076923\n70.192308\n\n\n32\n2\n40\n3.846154\n76.923077\n75.000000\n\n\n33\n2\n42\n3.846154\n80.769231\n78.846154\n\n\n34\n6\n48\n11.538462\n92.307692\n86.538462\n\n\n35\n2\n50\n3.846154\n96.153846\n94.230769\n\n\n36\n2\n52\n3.846154\n100.000000\n98.076923\n\n\n\n\n\n\n\n1.8.2 Punteggi Percentili\nI punteggi percentili sono un modo efficace per interpretare e confrontare i punteggi di un individuo con quelli di un campione normativo. Un punteggio percentile indica la posizione relativa di un individuo all’interno di un gruppo normativo. Più specificamente, un punteggio percentile mostra la percentuale di persone nel campione normativo che ha ottenuto un punteggio uguale o inferiore a quello dell’individuo in questione.\nPer esemplificare il concetto, consideriamo il calcolo di un quantile di ordine 0.74. Questo significa che stiamo cercando il valore al di sotto del quale si trova il 74% dei punteggi nel campione normativo. In altre parole, un individuo con un punteggio corrispondente a questo quantile ha superato il 74% delle persone nel gruppo normativo.\nIl calcolo dei punteggi percentili può essere effettuato attraverso l’analisi statistica dei dati di un campione rappresentativo. Questi dati vengono ordinati in modo crescente, e si identifica il punteggio che corrisponde al percentile desiderato. Nel caso del quantile 0.74, si cerca il punteggio che si trova alla posizione che corrisponde al 74% della lunghezza totale dell’elenco ordinato dei punteggi.\n\n# P74\nquantile(raw_score, .74)\n\n74%: 31.74\n\n\n\n# Use a different type (see https://en.wikipedia.org/wiki/Quantile#Estimating_quantiles_from_a_sample)\nquantile(raw_score, .74, type = 6)\n\n74%: 32\n\n\nI punteggi percentili sono particolarmente utili perché offrono una comprensione intuitiva della posizione di un individuo rispetto agli altri. Tuttavia, è importante notare che essi rappresentano una scala ordinale e, pertanto, le differenze tra i punteggi percentili non sono necessariamente uniformi o proporzionali attraverso l’intera gamma di punteggi.\nIn conclusione, i punteggi percentili sono uno strumento fondamentale nella valutazione psicologica e educativa, poiché forniscono un modo diretto e facilmente interpretabile per valutare le prestazioni di un individuo in confronto a un campione normativo.\n\n\n1.8.3 Punteggi Standardizzati\nI punteggi standardizzati rappresentano una trasformazione essenziale nel campo della psicometria, che consente di convertire i punteggi grezzi ottenuti in un test in una scala unificata. Questa trasformazione permette di confrontare i risultati di individui o gruppi in maniera equa e coerente, superando le variazioni di scala o di difficoltà tra diversi test.\n\n1.8.3.1 Principi Fondamentali dei Punteggi Standardizzati\n\nMedia e Deviazione Standard Predefinite: I punteggi standardizzati sono calcolati in modo tale da avere una media e una deviazione standard specifiche, stabilite in anticipo. Per esempio, spesso si utilizza una media di 100 e una deviazione standard di 15 (come nei test di intelligenza) o una media di 0 e una deviazione standard di 1 (come negli z-score).\nRisultati Confrontabili: Attraverso questa standardizzazione, i punteggi diventano direttamente confrontabili. Un punteggio standardizzato rispetto a una media di 100 e una deviazione standard di 15, ad esempio, permette di valutare rapidamente se un punteggio è al di sopra, al di sotto o vicino alla media del campione normativo.\n\n\n\n1.8.3.2 Come Funziona la Trasformazione\nIl processo di standardizzazione implica la sottrazione della media del campione normativo dal punteggio grezzo di un individuo, seguita dalla divisione del risultato per la deviazione standard del campione normativo. In termini matematici, se $ X $ è un punteggio grezzo, $ $ è la media del campione normativo e $ $ è la deviazione standard del campione normativo, allora il punteggio standardizzato $ Z $ è calcolato come:\n\\[\nZ = \\frac{X - \\mu}{\\sigma}.\n\\]\n\n\n1.8.3.3 Utilità dei Punteggi Standardizzati\n\nComparabilità: Rendono i punteggi ottenuti da test diversi o da campioni diversi direttamente comparabili.\nInterpretazione Facilitata: Forniscono un modo semplice per interpretare i punteggi individuali in termini di posizione relativa rispetto alla media del campione normativo.\nAdattabilità: Sono utili in una varietà di contesti, da test educativi a valutazioni cliniche.\n\nIn conclusione, i punteggi standardizzati sono uno strumento cruciale nella psicometria e nella valutazione educativa. Trasformando i punteggi grezzi in una scala comune con media e deviazione standard specifiche, facilitano il confronto e l’interpretazione dei risultati dei test, rendendo più accessibile l’analisi e la valutazione delle prestazioni individuali e di gruppo.\nNel caso dell’esempio, i calcoli si svolgono in R nel modo seguente:\n\nz_score &lt;- (raw_score - mean(raw_score)) / sd(raw_score)\nc(mean = mean(z_score), sd = sd(z_score))\n\nmean-5.61516645146954e-16sd1\n\n\n\n\n1.8.3.4 Punteggi T\nI punteggi T sono una forma specifica di punteggi standardizzati, utilizzati frequentemente nella psicometria per rendere più accessibili e interpretabili i risultati dei test. A differenza dei punteggi z, che tipicamente hanno una media di 0 e una deviazione standard di 1, i punteggi T sono trasformati in modo da avere una media fissata a 50 e una deviazione standard di 10.\n\n\n1.8.3.5 Caratteristiche Principali dei Punteggi T\n\nMedia e Deviazione Standard: La media fissata a 50 e la deviazione standard di 10 sono scelte per offrire una scala più intuitiva e di facile lettura rispetto agli z-score. Questa trasformazione sposta la scala degli z-score in una gamma numericamente più familiare e più semplice da interpretare per la maggior parte delle persone.\nCalcolo dei Punteggi T: Il calcolo dei punteggi T avviene trasformando prima i punteggi grezzi in z-score e poi convertendo questi z-score nella scala dei punteggi T. Matematicamente, se $ Z $ è lo z-score, il punteggio T corrispondente $ T $ è calcolato come:\n\\[\nT = 50 + 10 \\times Z.\n\\]\nQuesta formula adatta lo z-score in una scala che inizia da 50 e si allarga in entrambe le direzioni con incrementi standard di 10 per ogni deviazione standard.\n\n\n\n1.8.3.6 Utilizzo dei Punteggi T\n\nFacilità di Interpretazione: I punteggi T sono particolarmente utili quando si desidera presentare i risultati dei test in un formato che sia immediatamente comprensibile, senza la necessità di ulteriori calcoli o trasformazioni.\nComparabilità: Consentono di confrontare i risultati di test diversi in modo più diretto, grazie alla loro scala standardizzata.\nAmpio Utilizzo: Sono ampiamente usati in vari ambiti della valutazione psicologica, inclusi l’educazione, la ricerca e la pratica clinica.\n\nIn sintesi, i punteggi T offrono un modo efficace e standardizzato per interpretare i risultati dei test, rendendo i dati più accessibili e immediatamente comprensibili. La loro trasformazione da z-score a una scala con media 50 e deviazione standard 10 facilita la comprensione e la comparazione dei punteggi tra diversi test e diversi individui.\nSvolgendo i calcoli in R otteniamo\n\nT_score &lt;- z_score * 10 + 50\nc(mean = mean(T_score), sd = sd(T_score))\n\nmean50sd10\n\n\n\n\n\n1.8.4 Punteggi Stanini\nI punteggi Stanini rappresentano un metodo standardizzato per categorizzare i risultati dei test in psicometria, utilizzando una scala di nove intervalli. Ogni intervallo è definito da un cut-off di 10 punti, rendendo questo sistema particolarmente intuitivo e facile da interpretare. La scala Stanini è progettata per fornire una visione chiara e semplificata della posizione relativa di un individuo all’interno di un gruppo normativo.\n\n1.8.4.1 Caratteristiche dei Punteggi Stanini\n\nIntervalli di Scala: La scala Stanini è divisa in nove intervalli, con ogni intervallo corrispondente a un punteggio specifico. Ad esempio, un punteggio Stanini di 1 indica i punteggi più bassi, mentre un punteggio di 9 rappresenta i punteggi più alti.\nFacilità di Interpretazione: I punteggi Stanini offrono un modo semplice e diretto per comprendere i risultati dei test, permettendo una rapida valutazione del livello relativo di prestazione di un individuo rispetto alla popolazione di riferimento.\n\n\n\n1.8.4.2 Calcolo dei Punteggi Stanini\nPer calcolare i punteggi Stanini, è necessario seguire alcuni passaggi:\n\nDeterminare Media e Deviazione Standard: Inizialmente, si calcolano la media e la deviazione standard dei dati del campione normativo.\nApplicare la Formula dei Punteggi Stanini: Per ogni punteggio grezzo, si applica la seguente formula per calcolare il punteggio Stanini corrispondente:\n\\[\n\\text{Stanine} = \\left( \\frac{\\text{Punteggio Grezzo} - \\text{Media}}{\\text{Deviazione Standard}} \\right) \\times 2 + 5.\n\\]\nQuesta formula trasforma il punteggio grezzo in un valore sulla scala Stanini.\nArrotondare al Numero Intero Più Vicino: Infine, si arrotonda il risultato al numero intero più vicino per ottenere il punteggio Stanini finale.\n\n\n\n1.8.4.3 Applicazioni dei Punteggi Stanini\n\nSemplificazione dell’Analisi: I punteggi Stanini sono particolarmente utili in contesti educativi e di ricerca, dove è necessario semplificare l’analisi e la comunicazione dei risultati.\nValutazione Rapida: Forniscono agli insegnanti, ai clinici e ai ricercatori uno strumento rapido per valutare il posizionamento di un individuo rispetto ai coetanei o alla popolazione di riferimento.\n\nIn sintesi, i punteggi Stanini offrono un metodo efficace e semplificato per interpretare i risultati dei test psicometrici. La loro struttura a intervalli permette una categorizzazione chiara dei punteggi, rendendo più agevole la comprensione e la comparazione delle prestazioni relative degli individui.\nPer l’esempio presente abbiamo:\n\nmean_score &lt;- mean(raw_score)\nsd_score &lt;- sd(raw_score)\n\nstanine_scores &lt;- round((raw_score - mean_score) / sd_score * 2 + 5)\nprint(stanine_scores)\n\n [1] 3 2 7 6 3 8 5 9 2 5 4 7 2 5 3 6 5 5 8 5 3 3 8 7 3 8 5 4 4 6 5 3 3 5 5 7 3 8\n[39] 3 3 4 5 4 3 8 9 3 3 8 5 8 3\n\n\nÈ importante ricordare che la trasformazione in punti z non cambia la forma della distribuzione.\n\nplot(density(raw_score))\n\n\n\n\n\n\n\n\n\nplot(density(z_score))\n\n\n\n\n\n\n\n\n\nplot(density(T_score))\n\n\n\n\n\n\n\n\n\nplot(density(stanine_scores))\n\n\n\n\n\n\n\n\n\n\n\n1.8.5 Equivalenti alla Curva Normale (NCE)\nGli Equivalenti alla Curva Normale, noti come NCE (dall’inglese “Normal Curve Equivalents”), sono un tipo di punteggio standardizzato utilizzato in ambito psicometrico. Questi punteggi vengono calcolati per trasformare i punteggi grezzi ottenuti in un test in una scala che rifletta una distribuzione approssimativamente normale. L’obiettivo principale dei punteggi NCE è quello di rendere i punteggi di diverse misure o test direttamente confrontabili, mantenendo una distribuzione che si allinea strettamente con una curva normale standard.\n\n1.8.5.1 Caratteristiche dei Punteggi NCE\n\nDistribuzione Normalizzata: I punteggi NCE sono progettati per aderire a una distribuzione normale. Ciò significa che, a differenza di altri tipi di punteggi, i NCE si allineano più da vicino con le caratteristiche di una curva di distribuzione gaussiana, con la maggior parte dei punteggi concentrati intorno alla media e una distribuzione simmetrica verso gli estremi.\nFacilità di Comparazione: Grazie alla loro standardizzazione, i punteggi NCE consentono un confronto diretto e significativo tra le prestazioni in diversi test o misure. Questo è particolarmente utile in contesti educativi e clinici dove è necessario interpretare e confrontare i risultati di diversi test.\n\n\n\n1.8.5.2 Calcolo e Utilizzo dei Punteggi NCE\nIl calcolo dei punteggi NCE si basa sulla trasformazione dei punteggi grezzi in modo che si adattino a una distribuzione normalizzata. Questo processo implica l’uso di formule matematiche che riallineano i dati grezzi su una scala standard, considerando la media e la deviazione standard del campione normativo.\nUna volta calcolati, i punteggi NCE offrono una visione chiara e immediata delle prestazioni relative di un individuo o di un gruppo, rispetto a un campione normativo. Questo tipo di punteggio è particolarmente utile quando i punteggi grezzi provengono da distribuzioni che non seguono una curva normale, consentendo così un’interpretazione più accurata e standardizzata dei risultati.\n\n\n1.8.5.3 Applicazioni Pratiche dei Punteggi NCE\nI punteggi NCE trovano impiego in una varietà di contesti, tra cui:\n\nValutazioni Educative: In ambito scolastico, per confrontare le prestazioni degli studenti in test diversi.\nRicerca Psicologica: Per analizzare e confrontare i risultati di diversi studi o misure psicometriche.\nPratica Clinica: Nella valutazione di clienti o pazienti utilizzando diversi strumenti diagnostici.\n\nIn conclusione, i punteggi Equivalenti alla Curva Normale rappresentano uno strumento psicometrico potente per standardizzare e confrontare efficacemente i risultati di diversi test o misure, assicurando che questi siano interpretati all’interno di un quadro coerente e comparabile.\nPer i dati dell’esempio abbiamo:\n\n# Using normal quantile\nqnorm_pr &lt;- qnorm(pr / 100)\n# Convert raw scores\nnormalized_zscore &lt;- as.vector(qnorm_pr[as.character(raw_score)])\n\nIn alternativa, è possibile usare la trasformazione di Box-Cox, che è una tecnica parametrica che cerca di correggere le asimmetrie e trasformare i dati in una forma che approssima una distribuzione normale. È efficace per i dati positivi. La trasformazione è definita come segue:\n\\[\ny(\\lambda) = \\begin{cases} \\frac{x^\\lambda - 1}{\\lambda} & \\text{se } \\lambda \\neq 0 \\\\ \\log(x) & \\text{se } \\lambda = 0 \\end{cases},\n\\]\ndove \\(x\\) è il valore originale e \\(\\lambda\\) è il parametro di trasformazione che viene spesso trovato attraverso la massimizzazione della verosimiglianza.\nSupponiamo di voler utilizzare la trasformazione di Box-Cox sul nostro set di dati. La procedura è la seguente. Questo codice utilizza la funzione boxcox dal pacchetto MASS per trovare il valore di \\(\\lambda\\) che massimizza la log-verosimiglianza della trasformazione di Box-Cox applicata ai dati. Poi, utilizza questo \\(\\lambda\\) per trasformare i dati.\n\n# Dati di esempio\nset.seed(123) # Per rendere l'esempio riproducibile\ndata &lt;- raw_score\n\n# Trova il miglior lambda per la trasformazione di Box-Cox\nbc &lt;- boxcox(data ~ 1, lambda = seq(-2, 2, by = 0.1))\n\n# Calcola la trasformazione di Box-Cox con il lambda ottimale\nlambda_opt &lt;- bc$x[which.max(bc$y)]\ndata_transformed &lt;- (data^lambda_opt - 1) / lambda_opt\n\n\n\n\n\n\n\n\nAvendo trovato i dati trasformati con la procedura Box-Cox, li confrontiamo con gli Equivalenti alla Curva Normale (NCE) calcolati con la procedura usuale.\n\nplot(normalized_zscore, data_transformed)\n\n\n\n\n\n\n\n\n\nplot(density(normalized_zscore)) # the shape will be closer to normal\n\n\n\n\n\n\n\n\n\nplot(density((data_transformed - mean(data_transformed)) / sd(data_transformed)))\n\n\n\n\n\n\n\n\nSi osservi che i dati NCE presentano una distribuzione più simile a una curva a forma campanulare rispetto ai dati grezzi.\n\nlillie.test(normalized_zscore)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  normalized_zscore\nD = 0.085801, p-value = 0.4426\n\n\n\nlillie.test(data_transformed)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  data_transformed\nD = 0.12456, p-value = 0.04274",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#riflessioni-finali-sui-metodi-di-trasformazione-dei-punteggi",
    "href": "chapters/measurement/01_scores_scales.html#riflessioni-finali-sui-metodi-di-trasformazione-dei-punteggi",
    "title": "1  Punteggi e scale",
    "section": "1.9 Riflessioni Finali sui Metodi di Trasformazione dei Punteggi",
    "text": "1.9 Riflessioni Finali sui Metodi di Trasformazione dei Punteggi\nLa trasformazione dei punteggi grezzi in formati più interpretabili è una pratica cruciale in psicometria. Due sono i principali approcci utilizzati per attribuire significato ai punteggi di un test: il riferimento normativo e il riferimento criteriale.\n\n1.9.1 Riferimento Normativo\nNel riferimento normativo, si confronta il punteggio di un individuo con quello medio del gruppo normativo, ovvero gli altri soggetti che hanno svolto lo stesso test. Ci sono diversi tipi di punteggi normati, ciascuno con i suoi specifici vantaggi e limitazioni:\n\nPunteggi Percentili: Questi punteggi sono intuitivi e offrono un’indicazione immediata della posizione relativa di un individuo all’interno di un gruppo. Tuttavia, sono una scala ordinale e non si prestano bene a calcoli matematici più complessi.\nPunteggi Standardizzati: Gli z-score e i T-scores rientrano in questa categoria. Sono scalari a intervallo, quindi adatti a operazioni matematiche. Mantengono la forma distributiva originale dei punteggi grezzi, rendendo più agevole la loro elaborazione statistica.\n\n\n\n1.9.2 Riferimento Criteriale\nAl contrario del riferimento normativo, il riferimento criteriale confronta i punteggi di un individuo con uno standard prestabilito o un criterio specifico, piuttosto che con i punteggi di altri individui.\n\n\n1.9.3 Trasformazioni per la Normalizzazione\nPer ottenere una distribuzione dei punteggi più vicina alla curva normale, si possono utilizzare trasformazioni come gli stanini o gli NCE (Normal Curve Equivalents). Questi metodi di normalizzazione aiutano a standardizzare la distribuzione dei punteggi, facilitando così l’interpretazione e l’analisi dei dati psicometrici.\nIn conclusione, la scelta del metodo di trasformazione dei punteggi dipende dagli obiettivi specifici della valutazione e dall’interpretazione desiderata. La comprensione di queste diverse tecniche è essenziale per una corretta interpretazione dei risultati dei test e per l’analisi psicometrica più generale.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#considerazioni-conclusive",
    "href": "chapters/measurement/01_scores_scales.html#considerazioni-conclusive",
    "title": "1  Punteggi e scale",
    "section": "1.10 Considerazioni Conclusive",
    "text": "1.10 Considerazioni Conclusive\nQuesto capitolo fornisce una panoramica sui diversi tipi di punteggi dei test e il loro significato. Iniziamo notando che i punteggi grezzi, sebbene facili da calcolare, di solito forniscono poche informazioni utili sul rendimento di un esaminando in un test. Di conseguenza, di solito trasformiamo i punteggi grezzi in punteggi derivati, che possono essere di riferimento normativo o al criterio. I punteggi di riferimento normativo confrontano il rendimento di un esaminando con quello di altre persone nel campione di standardizzazione, mentre quelli al criterio confrontano il rendimento con un livello di competenza specificato. È importante valutare l’adeguatezza del campione di standardizzazione quando si utilizzano punteggi di riferimento normativo.\nPer interpretazioni basate sui punteggi di riferimento normativo, è utile conoscere la distribuzione normale e i punteggi standard di riferimento. Questi ultimi hanno una media predefinita e una deviazione standard. Esistono anche punteggi normalizzati quando i punteggi non seguono una distribuzione normale. Altri tipi di punteggi di riferimento normativo includono il rango percentile e i punteggi basati su età o livello di scolarità. Tuttavia, questi ultimi sono da evitare, se possibile, a favore di punteggi standard e ranghi percentile.\nI punteggi al criterio confrontano il rendimento con un livello specifico di competenza. Sono utili per valutare abilità in domini specifici, ma richiedono una chiara definizione del dominio. A volte, un test può produrre entrambi i tipi di punteggi. Forniamo anche una panoramica dei punteggi basati sulla teoria della risposta agli item (IRT), che sono utili per misurare i cambiamenti nel tempo.\nIn conclusione, i diversi tipi di punteggi dei test forniscono informazioni per rispondere a diverse domande e devono essere scelti in base alle esigenze specifiche dell’analisi dei dati del test.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#esercizi",
    "href": "chapters/measurement/01_scores_scales.html#esercizi",
    "title": "1  Punteggi e scale",
    "section": "1.11 Esercizi",
    "text": "1.11 Esercizi\nBandalos, capitolo 2, E1, E2, E5, E6, E7, E8, E9, E12",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#session-info",
    "href": "chapters/measurement/01_scores_scales.html#session-info",
    "title": "1  Punteggi e scale",
    "section": "1.12 Session Info",
    "text": "1.12 Session Info\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] nortest_1.0-4     MASS_7.3-61       ggokabeito_0.1.0  viridis_0.6.5    \n [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    bayesplot_1.11.1 \n [9] gridExtra_2.3     patchwork_1.3.0   semTools_0.5-6    semPlot_1.1.6    \n[13] lavaan_0.6-18     psych_2.4.6.26    scales_1.3.0      markdown_1.13    \n[17] knitr_1.48        lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1    \n[21] dplyr_1.1.4       purrr_1.0.2       readr_2.1.5       tidyr_1.3.1      \n[25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.16.0  jsonlite_1.8.9     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5.1 farver_2.1.2      \n  [7] nloptr_2.1.1       rmarkdown_2.28     vctrs_0.6.5       \n [10] minqa_1.2.8        base64enc_0.1-3    rstatix_0.7.2     \n [13] htmltools_0.5.8.1  broom_1.0.6        Formula_1.2-5     \n [16] htmlwidgets_1.6.4  plyr_1.8.9         sandwich_3.1-1    \n [19] emmeans_1.10.4     zoo_1.8-12         uuid_1.2-1        \n [22] igraph_2.0.3       mime_0.12          lifecycle_1.0.4   \n [25] pkgconfig_2.0.3    Matrix_1.7-0       R6_2.5.1          \n [28] fastmap_1.2.0      shiny_1.9.1        digest_0.6.37     \n [31] OpenMx_2.21.12     fdrtool_1.2.18     colorspace_2.1-1  \n [34] rprojroot_2.0.4    Hmisc_5.1-3        fansi_1.0.6       \n [37] timechange_0.3.0   abind_1.4-8        compiler_4.4.1    \n [40] withr_3.0.1        glasso_1.11        htmlTable_2.4.3   \n [43] backports_1.5.0    carData_3.0-5      ggsignif_0.6.4    \n [46] corpcor_1.6.10     gtools_3.9.5       tools_4.4.1       \n [49] pbivnorm_0.6.0     foreign_0.8-87     zip_2.3.1         \n [52] httpuv_1.6.15      nnet_7.3-19        glue_1.7.0        \n [55] quadprog_1.5-8     promises_1.3.0     nlme_3.1-166      \n [58] lisrelToR_0.3      grid_4.4.1         pbdZMQ_0.3-13     \n [61] checkmate_2.3.2    cluster_2.1.6      reshape2_1.4.4    \n [64] generics_0.1.3     gtable_0.3.5       tzdb_0.4.0        \n [67] data.table_1.16.0  hms_1.1.3          car_3.1-2         \n [70] utf8_1.2.4         sem_3.1-16         pillar_1.9.0      \n [73] IRdisplay_1.1      rockchalk_1.8.157  later_1.3.2       \n [76] splines_4.4.1      lattice_0.22-6     survival_3.7-0    \n [79] kutils_1.73        tidyselect_1.2.1   miniUI_0.1.1.1    \n [82] pbapply_1.7-2      stats4_4.4.1       xfun_0.47         \n [85] qgraph_1.9.8       arm_1.14-4         stringi_1.8.4     \n [88] boot_1.3-31        evaluate_1.0.0     codetools_0.2-20  \n [91] mi_1.1             cli_3.6.3          RcppParallel_5.1.9\n [94] IRkernel_1.3.2     rpart_4.1.23       xtable_1.8-4      \n [97] repr_1.1.7         munsell_0.5.1      Rcpp_1.0.13       \n[100] coda_0.19-4.1      png_0.1-8          XML_3.99-0.17     \n[103] parallel_4.4.1     jpeg_0.1-10        lme4_1.1-35.5     \n[106] mvtnorm_1.3-1      openxlsx_4.2.7.1   crayon_1.5.3      \n[109] rlang_1.1.4        multcomp_1.4-26    mnormt_2.1.1",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E1_likert.html",
    "href": "chapters/measurement/E1_likert.html",
    "title": "2  ✏️ Esercizi",
    "section": "",
    "text": "2.1 Scaling Likert\nIn questo tutorial esamineremo i dati di un questionario ordinale. Gli obiettivi saranno il punteggio totale e lo scaling normativo.\nIl Strengths and Difficulties Questionnaire (SDQ) è un breve questionario di screening comportamentale riguardante bambini e adolescenti di età compresa tra 3 e 16 anni. Esiste in diverse versioni consultabili su http://www.sdqinfo.org/. Dal sito è possibile scaricare il questionario, il metodo di scoring e le norme del test.\nLa versione autovalutativa (SDQ Pupil) include 25 item che misurano 5 scale (faccette), con 5 item ciascuna:\nAi partecipanti viene chiesto di valutare ciascuna domanda utilizzando le seguenti opzioni di risposta: 0 = “Non vero” 1 = “Un po’ vero” 2 = “Certamente vero”\nNOTA che alcuni item del SDQ sono reverse: item a punteggio invertito – punteggi più alti della scala corrispondono a punteggi inferiori degli item. Ad esempio, l’item “Di solito faccio quello che mi dicono” (variabile ubbidisce) è reverse dei Problemi di Condotta. Ci sono 5 item di questo tipo nel SDQ; sono contrassegnati nella tabella sopra con asterischi (*).\nI partecipanti a questo studio sono alunni di seconda media della stessa scuola (N=228). Si tratta di un campione di comunità e non ci aspettiamo che molti bambini abbiano punteggi al di sopra delle soglie cliniche. Il SDQ è stato somministrato due volte, la prima volta quando i bambini hanno appena iniziato la scuola secondaria (erano in anno 7), e un anno dopo (erano in anno 8).",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E1_likert.html#scaling-likert",
    "href": "chapters/measurement/E1_likert.html#scaling-likert",
    "title": "2  ✏️ Esercizi",
    "section": "",
    "text": "Sintomi Emotivi somatico preoccupazioni triste attaccamento paura\nProblemi di Condotta scatti ubbidisce* litiga mente ruba\nIperattività irrequietezza agitato distratto riflessivo* attento*\nProblemi con i Peer solitario amico* popolare* vittima di bullismo vecchio migliore amico\nPro-sociale prendersi cura condivide gentilezza aiuta",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E1_likert.html#emotional-symptoms-scale",
    "href": "chapters/measurement/E1_likert.html#emotional-symptoms-scale",
    "title": "2  ✏️ Esercizi",
    "section": "2.2 Emotional Symptoms scale",
    "text": "2.2 Emotional Symptoms scale\nQuesta scala non contiene item reverse.\nImportiamo i dati in R.\n\nload(\"../../data/data_sdq/SDQ.RData\")\nglimpse(SDQ)\n\nRows: 228\nColumns: 51\n$ Gender   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~\n$ consid   &lt;dbl&gt; 1, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2~\n$ restles  &lt;dbl&gt; 2, 0, 0, 0, 1, 0, 2, 1, 2, 0, 1, 1, 0, 1, 0, 2, 0, 1, 1, 1, 0~\n$ somatic  &lt;dbl&gt; 2, 2, 0, 0, 2, 1, 0, 0, 1, 0, 0, 2, 0, 0, 1, 2, 1, 1, 1, 1, 1~\n$ shares   &lt;dbl&gt; 1, 1, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2~\n$ tantrum  &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 2, 0, 2, 0, 0, 1, 0, 1, 1, 2, 0, 1, 1, 1, 0~\n$ loner    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 1, 0, 0, 0, 1, 0, 0, 2, 2, 0~\n$ obeys    &lt;dbl&gt; 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, 2, 2~\n$ worries  &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 1, 2, 0, 1, 1, 2, 1, 0~\n$ caring   &lt;dbl&gt; 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2~\n$ fidgety  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0~\n$ friend   &lt;dbl&gt; 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2~\n$ fights   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~\n$ unhappy  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 2, 1, 0~\n$ popular  &lt;dbl&gt; 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, 1, 2~\n$ distrac  &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 2, 0, 0, 1, 0, 0~\n$ clingy   &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 2, 0, 0, 0, 0, 1, 0, 2, 2, 1, 2, 0, 2, 2, 0~\n$ kind     &lt;dbl&gt; 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2~\n$ lies     &lt;dbl&gt; 0, 0, 0, 0, 2, 0, 1, 0, 2, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0~\n$ bullied  &lt;dbl&gt; 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0~\n$ helpout  &lt;dbl&gt; 2, 1, 2, 2, 0, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2~\n$ reflect  &lt;dbl&gt; 1, 1, 2, 2, 0, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2~\n$ steals   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0~\n$ oldbest  &lt;dbl&gt; 1, 0, 2, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1~\n$ afraid   &lt;dbl&gt; 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 2, 2, 0, 1, 1, 1, 0, 1, 1, 0~\n$ attends  &lt;dbl&gt; 2, 2, 1, 2, 0, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 2~\n$ consid2  &lt;dbl&gt; 1, 2, 2, 2, NA, 2, 2, 2, 2, 2, NA, 1, NA, 2, 2, NA, 1, 2, 2, ~\n$ restles2 &lt;dbl&gt; 0, 1, 2, 1, NA, 0, 1, 1, 0, 0, NA, 2, NA, 0, 1, NA, 1, 1, 2, ~\n$ somatic2 &lt;dbl&gt; 0, 1, 1, 0, NA, 0, 0, 0, 0, 0, NA, 2, NA, 0, 1, NA, 0, 1, 2, ~\n$ shares2  &lt;dbl&gt; 1, 2, 2, 1, NA, 2, 1, 2, 2, 2, NA, 2, NA, 2, 2, NA, 1, 2, 1, ~\n$ tantrum2 &lt;dbl&gt; 0, 1, 2, 0, NA, 0, 2, 0, 0, 0, NA, 2, NA, 0, 1, NA, 1, 0, 2, ~\n$ loner2   &lt;dbl&gt; 0, 0, 1, 0, NA, 0, 0, 0, 0, 0, NA, 1, NA, 1, 0, NA, 0, 0, 1, ~\n$ obeys2   &lt;dbl&gt; 2, 1, 2, 1, NA, 2, 2, 2, 2, 1, NA, 1, NA, 2, 1, NA, 1, 2, 1, ~\n$ worries2 &lt;dbl&gt; 0, 0, 1, 0, NA, NA, 1, 0, 0, 0, NA, 1, NA, 1, 2, NA, 0, 0, 2,~\n$ caring2  &lt;dbl&gt; 2, 2, 1, 2, NA, 2, 2, 2, 2, 2, NA, 2, NA, 2, 2, NA, 1, 2, 2, ~\n$ fidgety2 &lt;dbl&gt; 0, 1, 0, 0, NA, 0, 1, 0, 0, 0, NA, 2, NA, 0, 0, NA, 1, 0, 2, ~\n$ friend2  &lt;dbl&gt; 2, 2, 1, 2, NA, 2, 2, 2, 2, 2, NA, 2, NA, 1, 2, NA, 2, 2, 2, ~\n$ fights2  &lt;dbl&gt; 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA, 2, NA, 0, 0, NA, 0, 0, 0, ~\n$ unhappy2 &lt;dbl&gt; 0, 0, 1, 0, NA, 0, 0, 0, 0, 0, NA, 1, NA, 0, 0, NA, 0, 0, 1, ~\n$ popular2 &lt;dbl&gt; 2, 1, 1, 2, NA, 2, 1, 2, 2, 2, NA, 2, NA, 2, 2, NA, 1, 2, 1, ~\n$ distrac2 &lt;dbl&gt; 0, 0, 0, 2, NA, 0, 2, 1, 0, 0, NA, 1, NA, 0, 1, NA, 1, 0, 2, ~\n$ clingy2  &lt;dbl&gt; 1, 1, 1, 0, NA, 1, 1, 1, 0, 0, NA, 1, NA, 0, 0, NA, 2, 0, 2, ~\n$ kind2    &lt;dbl&gt; 2, 2, 2, 2, NA, 2, 2, 2, 2, 2, NA, 2, NA, 2, 2, NA, 1, 2, 2, ~\n$ lies2    &lt;dbl&gt; 1, 0, 0, 0, NA, 0, 1, 0, 1, 0, NA, 1, NA, 0, 0, NA, 1, 0, 0, ~\n$ bullied2 &lt;dbl&gt; 0, 0, 0, 0, NA, 0, 2, 0, 0, 0, NA, 0, NA, 0, 0, NA, 0, 0, 0, ~\n$ helpout2 &lt;dbl&gt; 1, 1, 1, 2, NA, 2, 2, 1, 2, 1, NA, 2, NA, 2, 1, NA, 0, 2, 1, ~\n$ reflect2 &lt;dbl&gt; 1, 1, 2, 1, NA, 2, 1, 2, 1, 2, NA, 1, NA, 2, 1, NA, 1, 2, 1, ~\n$ steals2  &lt;dbl&gt; 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA, 2, NA, 0, 0, NA, 0, 0, 0, ~\n$ oldbest2 &lt;dbl&gt; 0, 0, 1, 0, NA, 1, 0, 1, 1, 0, NA, 1, NA, 0, 0, NA, 0, 0, 1, ~\n$ afraid2  &lt;dbl&gt; 0, 1, 0, 0, NA, 0, 0, 0, 0, 0, NA, 2, NA, 0, 0, NA, 0, 0, 2, ~\n$ attends2 &lt;dbl&gt; 1, 1, 2, 0, NA, 2, 2, 2, 2, 1, NA, 1, NA, 2, 2, NA, 1, 1, 0, ~\n\n\nSelezioniamo solo gli item della Emotional Symptoms scale.\n\nitems_emotion &lt;- c(\"somatic\", \"worries\", \"unhappy\", \"clingy\", \"afraid\")\nsdq_emo &lt;- SDQ[, items_emotion]  \nsdq_emo |&gt;\n    head()\n\n\nA tibble: 6 x 5\n\n\nsomatic\nworries\nunhappy\nclingy\nafraid\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n2\n1\n0\n1\n0\n\n\n2\n0\n0\n1\n0\n\n\n0\n0\n0\n0\n1\n\n\n0\n0\n0\n1\n1\n\n\n2\n1\n0\n1\n0\n\n\n1\n0\n0\n1\n0\n\n\n\n\n\nCalcoliamo il punteggio della scala.\n\nrowSums(sdq_emo) |&gt; print()\n\n  [1]  4  3  1  2  4  2  4  0  1  1  0  8  2  3  7  4  5  2  8  6  1  4  9  4  5\n [26]  9  0  3  3  1  0  2  6  3  9  4  4  0  7  1  3  6  4  5  4  1  4  1  0  5\n [51]  1  2  2  4  4  4  6  1  8  3  2  2  4  1  1  0  2  2  7  5  0 NA NA  1  1\n [76]  7  4  1  8  3  5  0  5  4  0  1  1  5  3  6  1  3  2  6  6  0  2  4  5  3\n[101]  3  1  1  7  2  3  5  5 NA  0  4  0  4  1  1  1  1  0  2  7  0  3  8  4  6\n[126] NA  2  4  7  1  0  0  1  0  4  3  0 10  5  2  1  6  1  2  1  0  1 NA  4  4\n[151]  2  4  7  5  6  1  0  5  3  1  3  3  6  4  2  3  1  0  3  3  0  3  0  0  0\n[176]  2  2  2  0  1  5  3  3  1  4  3  1  6  2  4  2 NA  0  2  5  5  0  2  2  3\n[201]  4  0  2  4  2  2  1  3  2  0  1  0  0  8  1  1  2  1  2  2  4  0  0  1  2\n[226]  2  1  6\n\n\nNotiamo che ci sono diversi punteggi mancanti, denotati da NA.\nUn primo metodo per affrontare i dati mancanti è semplicemente quello di ignorarli:\n\nrowSums(sdq_emo, na.rm = TRUE) |&gt; print()\n\n  [1]  4  3  1  2  4  2  4  0  1  1  0  8  2  3  7  4  5  2  8  6  1  4  9  4  5\n [26]  9  0  3  3  1  0  2  6  3  9  4  4  0  7  1  3  6  4  5  4  1  4  1  0  5\n [51]  1  2  2  4  4  4  6  1  8  3  2  2  4  1  1  0  2  2  7  5  0  2  7  1  1\n [76]  7  4  1  8  3  5  0  5  4  0  1  1  5  3  6  1  3  2  6  6  0  2  4  5  3\n[101]  3  1  1  7  2  3  5  5  4  0  4  0  4  1  1  1  1  0  2  7  0  3  8  4  6\n[126]  0  2  4  7  1  0  0  1  0  4  3  0 10  5  2  1  6  1  2  1  0  1  4  4  4\n[151]  2  4  7  5  6  1  0  5  3  1  3  3  6  4  2  3  1  0  3  3  0  3  0  0  0\n[176]  2  2  2  0  1  5  3  3  1  4  3  1  6  2  4  2  4  0  2  5  5  0  2  2  3\n[201]  4  0  2  4  2  2  1  3  2  0  1  0  0  8  1  1  2  1  2  2  4  0  0  1  2\n[226]  2  1  6\n\n\nTuttavia, questa non è una buona idea. Anche per il fatto che, in questo modo non verrà calcolato il punteggio totale di 7 partecipanti. Possiamo identificare le colonne in cui ci sono dei valori mancanti usando summary().\n\nsummary(sdq_emo)\n\n    somatic          worries          unhappy           clingy      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :1.0000  \n Mean   :0.6106   Mean   :0.6211   Mean   :0.3172   Mean   :0.8421  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :2.0000   Max.   :2.0000   Max.   :2.0000   Max.   :2.0000  \n NA's   :2        NA's   :1        NA's   :1                        \n     afraid    \n Min.   :0.00  \n 1st Qu.:0.00  \n Median :0.00  \n Mean   :0.48  \n 3rd Qu.:1.00  \n Max.   :2.00  \n NA's   :3     \n\n\n\nsdq_emo &lt;- sdq_emo %&gt;%\n    mutate_at(vars(somatic:afraid), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .))\n\nQuesta istruzione utilizza la funzione mutate_at del pacchetto dplyr per applicare una trasformazione a colonne specifiche (da somatic a afraid). All’interno della funzione di trasformazione, essa controlla se ogni valore è mancante (NA). Se lo è, lo sostituisce con la media della colonna usando mean(., na.rm = TRUE), che calcola la media escludendo eventuali valori mancanti.\nPossiamo ora calcolare il punteggio della scala per ciascun partecipante.\n\nSDQ$s_emotion &lt;- rowSums(sdq_emo)\nSDQ$s_emotion |&gt; print()\n\n  [1]  4  3  1  2  4  2  4  0  1  1  0  8  2  3  7  4  5  2  8  6  1  4  9  4  5\n [26]  9  0  3  3  1  0  2  6  3  9  4  4  0  7  1  3  6  4  5  4  1  4  1  0  5\n [51]  1  2  2  4  4  4  6  1  8  3  2  2  4  1  1  0  2  2  7  5  0 NA NA  1  1\n [76]  7  4  1  8  3  5  0  5  4  0  1  1  5  3  6  1  3  2  6  6  0  2  4  5  3\n[101]  3  1  1  7  2  3  5  5 NA  0  4  0  4  1  1  1  1  0  2  7  0  3  8  4  6\n[126] NA  2  4  7  1  0  0  1  0  4  3  0 10  5  2  1  6  1  2  1  0  1 NA  4  4\n[151]  2  4  7  5  6  1  0  5  3  1  3  3  6  4  2  3  1  0  3  3  0  3  0  0  0\n[176]  2  2  2  0  1  5  3  3  1  4  3  1  6  2  4  2 NA  0  2  5  5  0  2  2  3\n[201]  4  0  2  4  2  2  1  3  2  0  1  0  0  8  1  1  2  1  2  2  4  0  0  1  2\n[226]  2  1  6\n\n\nUn istogramma si ottiene nel modo seguente.\n\nSDQ |&gt;\n    ggplot(aes(x = s_emotion)) +\n    geom_histogram(bins = 10)\n\nWarning message:\n\"Removed 6 rows containing non-finite outside the scale range (`stat_bin()`).\"\n\n\n\n\n\n\n\n\n\n\nhist(SDQ$s_emotion)\n\n\n\n\n\n\n\n\nPiù utile è un KDE plot.\n\nSDQ |&gt;\n    ggplot(aes(x = s_emotion)) +\n    geom_density()\n\nWarning message:\n\"Removed 6 rows containing non-finite outside the scale range\n(`stat_density()`).\"\n\n\n\n\n\n\n\n\n\nPossiamo ottenere le statistiche descrittive della scala usando la funzione describe del pacchetto psych.\n\ndescribe(SDQ$s_emotion)\n\n\nA psych: 1 x 13\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nX1\n1\n222\n2.837838\n2.301054\n2\n2.61236\n2.9652\n0\n10\n10\n0.7454236\n-0.08749808\n0.1544367\n\n\n\n\n\nCome si può vedere, la mediana (il punteggio al di sotto del quale si trova la metà del campione) di s_emotion è 2, mentre la media è più alta e pari a 2.87. Questo perché la distribuione dei punteggi è asimmetrica positiva; in questo caso, la mediana è più rappresentativa della tendenza centrale. Queste statistiche sono coerenti con la nostra osservazione dell’istogramma, che mostra un forte floor effect.\nDi seguito sono riportati i valori di soglia per i casi “Normali”, “Borderline” e “Anormali” per i Sintomi Emotivi forniti dal publisher del test (vedi https://sdqinfo.org/). Questi sono i punteggi che distinguono i casi probabilmente borderline e anormali dai casi “normali”.\nNormale: 0-5 Borderline: 6 Anormale: 7-10\n\ntable(SDQ$s_emotion &lt;= 5)\n\n\nFALSE  TRUE \n   33   195 \n\n\nIn questo campione, dunque, l’85% dei partecipanti è classificato nell’intervallo Normale.\n\ntable(SDQ$s_emotion &lt;= 5)[2] / length(SDQ$s_emotion)\n\nTRUE: 0.855263157894737\n\n\nIn maniera equivalente otteniamo\n\ntable(SDQ$s_emotion == 6)[2] / length(SDQ$s_emotion)\n\nTRUE: 0.0570175438596491\n\n\n\ntable(SDQ$s_emotion &gt;= 7)[2] / length(SDQ$s_emotion)\n\nTRUE: 0.0833333333333333",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E1_likert.html#item-reverse",
    "href": "chapters/measurement/E1_likert.html#item-reverse",
    "title": "2  ✏️ Esercizi",
    "section": "2.3 Item reverse",
    "text": "2.3 Item reverse\nLa scala Conduct Problems contiene item reverse. Esaminiamo lo scoring di questo tipo di item.\n\nitems_conduct &lt;- c(\"tantrum\", \"obeys\", \"fights\", \"lies\", \"steals\")\n\nPer i Problemi di Condotta, abbiamo solo un item reverse, obeys.\ntantrum    obeys*      fights       lies       steals\nPer invertire il codice di questo item, useremo una funzione dedicata del pacchetto psych, reverse.code(). Questa funzione ha la forma generale reverse.code(keys, items,…). L’argomento keys è un vettore di valori 1 o -1, dove -1 implica l’inversione dell’item. L’argomento items sono i nomi delle variabili che vogliamo valutare.\n\nR_conduct &lt;- reverse.code(keys = c(1, -1, 1, 1, 1), SDQ[, items_conduct]) |&gt; as.data.frame()\nR_conduct |&gt; head()\n\n\nA data.frame: 6 x 5\n\n\n\ntantrum\nobeys-\nfights\nlies\nsteals\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n0\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n\n\n3\n0\n0\n0\n0\n0\n\n\n4\n0\n0\n0\n0\n0\n\n\n5\n1\n2\n0\n2\n0\n\n\n6\n0\n0\n0\n0\n0\n\n\n\n\n\n\nSDQ[, items_conduct] |&gt; head()\n\n\nA tibble: 6 x 5\n\n\ntantrum\nobeys\nfights\nlies\nsteals\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n0\n2\n0\n0\n0\n\n\n0\n2\n0\n0\n0\n\n\n0\n2\n0\n0\n0\n\n\n0\n2\n0\n0\n0\n\n\n1\n0\n0\n2\n0\n\n\n0\n2\n0\n0\n0\n\n\n\n\n\nAnche in questo caso ci sono dei dati mancanti.\n\nsummary(R_conduct)\n\n    tantrum           obeys-           fights           lies       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000  \n Median :0.0000   Median :1.0000   Median :0.000   Median :0.0000  \n Mean   :0.5708   Mean   :0.5789   Mean   :0.193   Mean   :0.5442  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.000   3rd Qu.:1.0000  \n Max.   :2.0000   Max.   :2.0000   Max.   :2.000   Max.   :2.0000  \n NA's   :2                                         NA's   :2       \n     steals     \n Min.   :0.000  \n 1st Qu.:0.000  \n Median :0.000  \n Mean   :0.185  \n 3rd Qu.:0.000  \n Max.   :2.000  \n NA's   :1      \n\n\n\nR_conduct &lt;- R_conduct %&gt;%\n    mutate_at(vars(tantrum:steals), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .))\n\nCalcoliamo ora il punteggio totale.\n\nSDQ$s_conduct &lt;- rowMeans(R_conduct)\n\n\nSDQ |&gt;\n    ggplot(aes(x = s_conduct)) +\n    geom_histogram(bins = 10)",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E1_likert.html#session-info",
    "href": "chapters/measurement/E1_likert.html#session-info",
    "title": "2  ✏️ Esercizi",
    "section": "2.4 Session Info",
    "text": "2.4 Session Info\n\nsessionInfo() \n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ggokabeito_0.1.0  viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n [5] ggExtra_0.10.1    bayesplot_1.11.1  gridExtra_2.3     patchwork_1.3.0  \n [9] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-18     psych_2.4.6.26   \n[13] scales_1.3.0      markdown_1.13     knitr_1.48        lubridate_1.9.3  \n[17] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[21] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1    \n[25] tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.16.0  jsonlite_1.8.9     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5.1 farver_2.1.2      \n  [7] nloptr_2.1.1       rmarkdown_2.28     vctrs_0.6.5       \n [10] minqa_1.2.8        base64enc_0.1-3    rstatix_0.7.2     \n [13] htmltools_0.5.8.1  broom_1.0.6        Formula_1.2-5     \n [16] htmlwidgets_1.6.4  plyr_1.8.9         sandwich_3.1-1    \n [19] emmeans_1.10.4     zoo_1.8-12         uuid_1.2-1        \n [22] igraph_2.0.3       mime_0.12          lifecycle_1.0.4   \n [25] pkgconfig_2.0.3    Matrix_1.7-0       R6_2.5.1          \n [28] fastmap_1.2.0      shiny_1.9.1        digest_0.6.37     \n [31] OpenMx_2.21.12     fdrtool_1.2.18     colorspace_2.1-1  \n [34] rprojroot_2.0.4    Hmisc_5.1-3        labeling_0.4.3    \n [37] fansi_1.0.6        timechange_0.3.0   abind_1.4-8       \n [40] compiler_4.4.1     withr_3.0.1        glasso_1.11       \n [43] htmlTable_2.4.3    backports_1.5.0    carData_3.0-5     \n [46] ggsignif_0.6.4     MASS_7.3-61        corpcor_1.6.10    \n [49] gtools_3.9.5       tools_4.4.1        pbivnorm_0.6.0    \n [52] foreign_0.8-87     zip_2.3.1          httpuv_1.6.15     \n [55] nnet_7.3-19        glue_1.7.0         quadprog_1.5-8    \n [58] promises_1.3.0     nlme_3.1-166       lisrelToR_0.3     \n [61] grid_4.4.1         pbdZMQ_0.3-13      checkmate_2.3.2   \n [64] cluster_2.1.6      reshape2_1.4.4     generics_0.1.3    \n [67] gtable_0.3.5       tzdb_0.4.0         data.table_1.16.0 \n [70] hms_1.1.3          car_3.1-2          utf8_1.2.4        \n [73] sem_3.1-16         pillar_1.9.0       IRdisplay_1.1     \n [76] rockchalk_1.8.157  later_1.3.2        splines_4.4.1     \n [79] lattice_0.22-6     survival_3.7-0     kutils_1.73       \n [82] tidyselect_1.2.1   miniUI_0.1.1.1     pbapply_1.7-2     \n [85] stats4_4.4.1       xfun_0.47          qgraph_1.9.8      \n [88] arm_1.14-4         stringi_1.8.4      boot_1.3-31       \n [91] evaluate_1.0.0     codetools_0.2-20   mi_1.1            \n [94] cli_3.6.3          RcppParallel_5.1.9 IRkernel_1.3.2    \n [97] rpart_4.1.23       xtable_1.8-4       repr_1.1.7        \n[100] munsell_0.5.1      Rcpp_1.0.13        coda_0.19-4.1     \n[103] png_0.1-8          XML_3.99-0.17      parallel_4.4.1    \n[106] jpeg_0.1-10        lme4_1.1-35.5      mvtnorm_1.3-1     \n[109] openxlsx_4.2.7.1   crayon_1.5.3       rlang_1.1.4       \n[112] multcomp_1.4-26    mnormt_2.1.1",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E2_optimal_scoring.html",
    "href": "chapters/measurement/E2_optimal_scoring.html",
    "title": "3  ✏️ Esercizi",
    "section": "",
    "text": "3.1 Ottimizzazione dello scoring dei dati di questionari ordinali\nPrecedentemente, nell’Esercizio sulla scala Likert, abbiamo effettuato lo scoring del questionario Strengths and Difficulties Questionnaire (SDQ) utilizzando l’approccio chiamato “Likert scaling”, in cui le categorie di risposta “non vero”, “un po’ vero” e “certamente vero” sono state assegnate ai numeri interi consecutivi 0-1-2. Oltre a riflettere il presunto grado crescente di accordo in queste opzioni di risposta, l’assegnazione dei numeri interi era arbitraria, poiché non c’era una particolare ragione per cui abbiamo assegnato 0-1-2 anziché, ad esempio, 1-2-3. Un tale modo arbitrario di valutare le risposte degli item è chiamato anche “measurement by fiat”. In questo tutorial, cercheremo di trovare punteggi “ottimali” per le risposte ordinate al SDQ. “Ottimale” significa che i punteggi che assegniamo alle risposte non sono solo dei punteggi qualsiasi, ma sono i “migliori” tra tutti gli altri possibili punteggi in base a qualche criterio statistico.\nEsistono molti modi per “ottimizzare” i punteggi degli item; qui, massimizzeremo il rapporto tra la varianza del punteggio totale e la somma delle varianze dei punteggi degli item. In psicometria, il soddisfacimento di questo criterio porta alla massimizzazione della somma delle correlazioni degli item (e quindi della “coerenza interna” del punteggio del test misurata dall’alpha di Cronbach).\nsource(\"../../code/_common.R\")\nlibrary(\"aspect\")\nImportiamo nuovamente i dati del Strengths and Difficulties Questionnaire (SDQ).\nload(\"../data/data_sdq/SDQ.RData\")\nglimpse(SDQ)\n\nRows: 228\nColumns: 51\n$ Gender   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ consid   &lt;dbl&gt; 1, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2…\n$ restles  &lt;dbl&gt; 2, 0, 0, 0, 1, 0, 2, 1, 2, 0, 1, 1, 0, 1, 0, 2, 0, 1, 1, 1, 0…\n$ somatic  &lt;dbl&gt; 2, 2, 0, 0, 2, 1, 0, 0, 1, 0, 0, 2, 0, 0, 1, 2, 1, 1, 1, 1, 1…\n$ shares   &lt;dbl&gt; 1, 1, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2…\n$ tantrum  &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 2, 0, 2, 0, 0, 1, 0, 1, 1, 2, 0, 1, 1, 1, 0…\n$ loner    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 1, 0, 0, 0, 1, 0, 0, 2, 2, 0…\n$ obeys    &lt;dbl&gt; 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, 2, 2…\n$ worries  &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 1, 2, 0, 1, 1, 2, 1, 0…\n$ caring   &lt;dbl&gt; 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2…\n$ fidgety  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0…\n$ friend   &lt;dbl&gt; 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2…\n$ fights   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ unhappy  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 2, 1, 0…\n$ popular  &lt;dbl&gt; 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, 1, 2…\n$ distrac  &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 2, 0, 0, 1, 0, 0…\n$ clingy   &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 2, 0, 0, 0, 0, 1, 0, 2, 2, 1, 2, 0, 2, 2, 0…\n$ kind     &lt;dbl&gt; 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2…\n$ lies     &lt;dbl&gt; 0, 0, 0, 0, 2, 0, 1, 0, 2, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0…\n$ bullied  &lt;dbl&gt; 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0…\n$ helpout  &lt;dbl&gt; 2, 1, 2, 2, 0, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2…\n$ reflect  &lt;dbl&gt; 1, 1, 2, 2, 0, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2…\n$ steals   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0…\n$ oldbest  &lt;dbl&gt; 1, 0, 2, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1…\n$ afraid   &lt;dbl&gt; 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 2, 2, 0, 1, 1, 1, 0, 1, 1, 0…\n$ attends  &lt;dbl&gt; 2, 2, 1, 2, 0, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 2…\n$ consid2  &lt;dbl&gt; 1, 2, 2, 2, NA, 2, 2, 2, 2, 2, NA, 1, NA, 2, 2, NA, 1, 2, 2, …\n$ restles2 &lt;dbl&gt; 0, 1, 2, 1, NA, 0, 1, 1, 0, 0, NA, 2, NA, 0, 1, NA, 1, 1, 2, …\n$ somatic2 &lt;dbl&gt; 0, 1, 1, 0, NA, 0, 0, 0, 0, 0, NA, 2, NA, 0, 1, NA, 0, 1, 2, …\n$ shares2  &lt;dbl&gt; 1, 2, 2, 1, NA, 2, 1, 2, 2, 2, NA, 2, NA, 2, 2, NA, 1, 2, 1, …\n$ tantrum2 &lt;dbl&gt; 0, 1, 2, 0, NA, 0, 2, 0, 0, 0, NA, 2, NA, 0, 1, NA, 1, 0, 2, …\n$ loner2   &lt;dbl&gt; 0, 0, 1, 0, NA, 0, 0, 0, 0, 0, NA, 1, NA, 1, 0, NA, 0, 0, 1, …\n$ obeys2   &lt;dbl&gt; 2, 1, 2, 1, NA, 2, 2, 2, 2, 1, NA, 1, NA, 2, 1, NA, 1, 2, 1, …\n$ worries2 &lt;dbl&gt; 0, 0, 1, 0, NA, NA, 1, 0, 0, 0, NA, 1, NA, 1, 2, NA, 0, 0, 2,…\n$ caring2  &lt;dbl&gt; 2, 2, 1, 2, NA, 2, 2, 2, 2, 2, NA, 2, NA, 2, 2, NA, 1, 2, 2, …\n$ fidgety2 &lt;dbl&gt; 0, 1, 0, 0, NA, 0, 1, 0, 0, 0, NA, 2, NA, 0, 0, NA, 1, 0, 2, …\n$ friend2  &lt;dbl&gt; 2, 2, 1, 2, NA, 2, 2, 2, 2, 2, NA, 2, NA, 1, 2, NA, 2, 2, 2, …\n$ fights2  &lt;dbl&gt; 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA, 2, NA, 0, 0, NA, 0, 0, 0, …\n$ unhappy2 &lt;dbl&gt; 0, 0, 1, 0, NA, 0, 0, 0, 0, 0, NA, 1, NA, 0, 0, NA, 0, 0, 1, …\n$ popular2 &lt;dbl&gt; 2, 1, 1, 2, NA, 2, 1, 2, 2, 2, NA, 2, NA, 2, 2, NA, 1, 2, 1, …\n$ distrac2 &lt;dbl&gt; 0, 0, 0, 2, NA, 0, 2, 1, 0, 0, NA, 1, NA, 0, 1, NA, 1, 0, 2, …\n$ clingy2  &lt;dbl&gt; 1, 1, 1, 0, NA, 1, 1, 1, 0, 0, NA, 1, NA, 0, 0, NA, 2, 0, 2, …\n$ kind2    &lt;dbl&gt; 2, 2, 2, 2, NA, 2, 2, 2, 2, 2, NA, 2, NA, 2, 2, NA, 1, 2, 2, …\n$ lies2    &lt;dbl&gt; 1, 0, 0, 0, NA, 0, 1, 0, 1, 0, NA, 1, NA, 0, 0, NA, 1, 0, 0, …\n$ bullied2 &lt;dbl&gt; 0, 0, 0, 0, NA, 0, 2, 0, 0, 0, NA, 0, NA, 0, 0, NA, 0, 0, 0, …\n$ helpout2 &lt;dbl&gt; 1, 1, 1, 2, NA, 2, 2, 1, 2, 1, NA, 2, NA, 2, 1, NA, 0, 2, 1, …\n$ reflect2 &lt;dbl&gt; 1, 1, 2, 1, NA, 2, 1, 2, 1, 2, NA, 1, NA, 2, 1, NA, 1, 2, 1, …\n$ steals2  &lt;dbl&gt; 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA, 2, NA, 0, 0, NA, 0, 0, 0, …\n$ oldbest2 &lt;dbl&gt; 0, 0, 1, 0, NA, 1, 0, 1, 1, 0, NA, 1, NA, 0, 0, NA, 0, 0, 1, …\n$ afraid2  &lt;dbl&gt; 0, 1, 0, 0, NA, 0, 0, 0, 0, 0, NA, 2, NA, 0, 0, NA, 0, 0, 2, …\n$ attends2 &lt;dbl&gt; 1, 1, 2, 0, NA, 2, 2, 2, 2, 1, NA, 1, NA, 2, 2, NA, 1, 1, 0, …\nPer analizzare solo gli item che misurano i Sintomi Emotivi, è conveniente creare un nuovo data frame.\nitems_emotion &lt;- c(\"somatic\", \"worries\", \"unhappy\", \"clingy\", \"afraid\")\nsdq_emo &lt;- SDQ[, items_emotion]\nsdq_emo |&gt;\n    head()\n\n\nA tibble: 6 × 5\n\n\nsomatic\nworries\nunhappy\nclingy\nafraid\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n2\n1\n0\n1\n0\n\n\n2\n0\n0\n1\n0\n\n\n0\n0\n0\n0\n1\n\n\n0\n0\n0\n1\n1\n\n\n2\n1\n0\n1\n0\n\n\n1\n0\n0\n1\n0\nAffrontiamo il problema dei dati mancanti come discusso in precedenza.\nsdq_emo &lt;- sdq_emo %&gt;%\n    mutate_at(vars(somatic:afraid), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .))\nEliminiamo i valori decimali.\nsdq_emo &lt;- round(sdq_emo)\nemotional_symptoms &lt;- c(\"somatic\", \"worries\", \"unhappy\", \"clingy\", \"afraid\")\nresult &lt;- lapply(emotional_symptoms, function(x) sort(unique(sdq_emo[[x]])))\nresult |&gt; print()\n\n[[1]]\n[1] 0 1 2\n\n[[2]]\n[1] 0 1 2\n\n[[3]]\n[1] 0 1 2\n\n[[4]]\n[1] 0 1 2\n\n[[5]]\n[1] 0 1 2\nTrasformiamo il data frame in una matrice.\nM &lt;- sdq_emo |&gt; as.matrix()\nM\n\n\nA matrix: 228 × 5 of type dbl\n\n\nsomatic\nworries\nunhappy\nclingy\nafraid\n\n\n\n\n2\n1\n0\n1\n0\n\n\n2\n0\n0\n1\n0\n\n\n0\n0\n0\n0\n1\n\n\n0\n0\n0\n1\n1\n\n\n2\n1\n0\n1\n0\n\n\n1\n0\n0\n1\n0\n\n\n0\n1\n1\n2\n0\n\n\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n1\n\n\n0\n0\n0\n0\n0\n\n\n2\n2\n1\n1\n2\n\n\n0\n0\n0\n0\n2\n\n\n0\n1\n0\n2\n0\n\n\n1\n2\n1\n2\n1\n\n\n2\n0\n0\n1\n1\n\n\n1\n1\n0\n2\n1\n\n\n1\n1\n0\n0\n0\n\n\n1\n2\n2\n2\n1\n\n\n1\n1\n1\n2\n1\n\n\n1\n0\n0\n0\n0\n\n\n2\n1\n0\n0\n1\n\n\n2\n2\n2\n1\n2\n\n\n0\n1\n1\n1\n1\n\n\n1\n2\n0\n0\n2\n\n\n2\n2\n2\n2\n1\n\n\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n2\n0\n\n\n1\n0\n1\n1\n0\n\n\n0\n0\n0\n1\n0\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n1\n0\n0\n1\n0\n\n\n1\n0\n0\n1\n1\n\n\n1\n1\n0\n1\n1\n\n\n0\n0\n0\n0\n0\n\n\n0\n2\n0\n0\n0\n\n\n1\n1\n1\n0\n1\n\n\n1\n0\n0\n1\n0\n\n\n1\n0\n0\n1\n0\n\n\n0\n0\n0\n1\n0\n\n\n1\n0\n0\n2\n0\n\n\n0\n1\n0\n1\n0\n\n\n0\n0\n0\n0\n0\n\n\n0\n1\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n\n\n1\n2\n1\n2\n2\n\n\n0\n1\n0\n0\n0\n\n\n0\n0\n0\n1\n0\n\n\n0\n0\n0\n1\n1\n\n\n1\n0\n0\n0\n0\n\n\n0\n0\n0\n2\n0\n\n\n1\n0\n0\n0\n1\n\n\n1\n0\n1\n1\n1\n\n\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n\n\n0\n1\n0\n0\n0\n\n\n1\n0\n0\n0\n1\n\n\n0\n1\n0\n1\n0\n\n\n0\n1\n0\n0\n0\n\n\n2\n1\n1\n1\n1\nImplementiamo lo scaling ottimale con la funzione corAspect().\nopt &lt;- corAspect(M, aspect = \"aspectSum\", level = \"ordinal\")\nEsaminiamo il risultato ottenuto.\nattributes(opt)\n\n\n    $names\n        \n'loss''catscores''cormat''eigencor''indmat''scoremat''data''burtmat''niter''call'\n\n    $class\n        'aspect'\nopt$scoremat\n\n\nA matrix: 228 × 5 of type dbl\n\n\n\nsomatic\nworries\nunhappy\nclingy\nafraid\n\n\n\n\n1\n1.9720960\n0.4454555\n-0.6009399\n0.2369782\n-0.7685934\n\n\n2\n1.9720960\n-0.8540365\n-0.6009399\n0.2369782\n-0.7685934\n\n\n3\n-0.9013969\n-0.8540365\n-0.6009399\n-1.1988603\n1.0085914\n\n\n4\n-0.9013969\n-0.8540365\n-0.6009399\n0.2369782\n1.0085914\n\n\n5\n1.9720960\n0.4454555\n-0.6009399\n0.2369782\n-0.7685934\n\n\n6\n0.5862099\n-0.8540365\n-0.6009399\n0.2369782\n-0.7685934\n\n\n7\n-0.9013969\n0.4454555\n1.3931034\n1.6170823\n-0.7685934\n\n\n8\n-0.9013969\n-0.8540365\n-0.6009399\n-1.1988603\n-0.7685934\n\n\n9\n0.5862099\n-0.8540365\n-0.6009399\n-1.1988603\n-0.7685934\n\n\n10\n-0.9013969\n-0.8540365\n-0.6009399\n-1.1988603\n1.0085914\n\n\n11\n-0.9013969\n-0.8540365\n-0.6009399\n-1.1988603\n-0.7685934\n\n\n12\n1.9720960\n2.0964116\n1.3931034\n0.2369782\n1.9509426\n\n\n13\n-0.9013969\n-0.8540365\n-0.6009399\n-1.1988603\n1.9509426\n\n\n14\n-0.9013969\n0.4454555\n-0.6009399\n1.6170823\n-0.7685934\n\n\n15\n0.5862099\n2.0964116\n1.3931034\n1.6170823\n1.0085914\n\n\n16\n1.9720960\n-0.8540365\n-0.6009399\n0.2369782\n1.0085914\n\n\n17\n0.5862099\n0.4454555\n-0.6009399\n1.6170823\n1.0085914\n\n\n18\n0.5862099\n0.4454555\n-0.6009399\n-1.1988603\n-0.7685934\n\n\n19\n0.5862099\n2.0964116\n2.6586118\n1.6170823\n1.0085914\n\n\n20\n0.5862099\n0.4454555\n1.3931034\n1.6170823\n1.0085914\n\n\n21\n0.5862099\n-0.8540365\n-0.6009399\n-1.1988603\n-0.7685934\n\n\n22\n1.9720960\n0.4454555\n-0.6009399\n-1.1988603\n1.0085914\n\n\n23\n1.9720960\n2.0964116\n2.6586118\n0.2369782\n1.9509426\n\n\n24\n-0.9013969\n0.4454555\n1.3931034\n0.2369782\n1.0085914\n\n\n25\n0.5862099\n2.0964116\n-0.6009399\n-1.1988603\n1.9509426\n\n\n26\n1.9720960\n2.0964116\n2.6586118\n1.6170823\n1.0085914\n\n\n27\n-0.9013969\n-0.8540365\n-0.6009399\n-1.1988603\n-0.7685934\n\n\n28\n0.5862099\n-0.8540365\n-0.6009399\n1.6170823\n-0.7685934\n\n\n29\n0.5862099\n-0.8540365\n1.3931034\n0.2369782\n-0.7685934\n\n\n30\n-0.9013969\n-0.8540365\n-0.6009399\n0.2369782\n-0.7685934\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n199\n0.5862099\n-0.8540365\n-0.6009399\n0.2369782\n-0.7685934\n\n\n200\n0.5862099\n-0.8540365\n-0.6009399\n0.2369782\n1.0085914\n\n\n201\n0.5862099\n0.4454555\n-0.6009399\n0.2369782\n1.0085914\n\n\n202\n-0.9013969\n-0.8540365\n-0.6009399\n-1.1988603\n-0.7685934\n\n\n203\n-0.9013969\n2.0964116\n-0.6009399\n-1.1988603\n-0.7685934\n\n\n204\n0.5862099\n0.4454555\n1.3931034\n-1.1988603\n1.0085914\n\n\n205\n0.5862099\n-0.8540365\n-0.6009399\n0.2369782\n-0.7685934\n\n\n206\n0.5862099\n-0.8540365\n-0.6009399\n0.2369782\n-0.7685934\n\n\n207\n-0.9013969\n-0.8540365\n-0.6009399\n0.2369782\n-0.7685934\n\n\n208\n0.5862099\n-0.8540365\n-0.6009399\n1.6170823\n-0.7685934\n\n\n209\n-0.9013969\n0.4454555\n-0.6009399\n0.2369782\n-0.7685934\n\n\n210\n-0.9013969\n-0.8540365\n-0.6009399\n-1.1988603\n-0.7685934\n\n\n211\n-0.9013969\n0.4454555\n-0.6009399\n-1.1988603\n-0.7685934\n\n\n212\n-0.9013969\n-0.8540365\n-0.6009399\n-1.1988603\n-0.7685934\n\n\n213\n-0.9013969\n-0.8540365\n-0.6009399\n-1.1988603\n-0.7685934\n\n\n214\n0.5862099\n2.0964116\n1.3931034\n1.6170823\n1.9509426\n\n\n215\n-0.9013969\n0.4454555\n-0.6009399\n-1.1988603\n-0.7685934\n\n\n216\n-0.9013969\n-0.8540365\n-0.6009399\n0.2369782\n-0.7685934\n\n\n217\n-0.9013969\n-0.8540365\n-0.6009399\n0.2369782\n1.0085914\n\n\n218\n0.5862099\n-0.8540365\n-0.6009399\n-1.1988603\n-0.7685934\n\n\n219\n-0.9013969\n-0.8540365\n-0.6009399\n1.6170823\n-0.7685934\n\n\n220\n0.5862099\n-0.8540365\n-0.6009399\n-1.1988603\n1.0085914\n\n\n221\n0.5862099\n-0.8540365\n1.3931034\n0.2369782\n1.0085914\n\n\n222\n-0.9013969\n-0.8540365\n-0.6009399\n-1.1988603\n-0.7685934\n\n\n223\n-0.9013969\n-0.8540365\n-0.6009399\n-1.1988603\n-0.7685934\n\n\n224\n-0.9013969\n0.4454555\n-0.6009399\n-1.1988603\n-0.7685934\n\n\n225\n0.5862099\n-0.8540365\n-0.6009399\n-1.1988603\n1.0085914\n\n\n226\n-0.9013969\n0.4454555\n-0.6009399\n0.2369782\n-0.7685934\n\n\n227\n-0.9013969\n0.4454555\n-0.6009399\n-1.1988603\n-0.7685934\n\n\n228\n1.9720960\n0.4454555\n1.3931034\n0.2369782\n1.0085914\nEsaminiamo la relazione tra lo scoring basato sul metodo Likert con lo scoring ottimale.\nplot(opt$scoremat[, 1], sdq_emo$somatic)\nplot(opt$scoremat[, 5], sdq_emo$afraid)\nGuardando ai grafici ottenuti, si può notare che 1) i punteggi per le categorie successive aumentano quasi linearmente; 2) le categorie sono approssimativamente equidistanti. Concludiamo che per la valutazione degli item ordinali nella scala dei Sintomi Emotivi del SDQ, la scala Likert è appropriata, e non si può ottenere molto di più dall’ottimizzazione della scala rispetto alla semplice scala Likert di base.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E2_optimal_scoring.html#session-info",
    "href": "chapters/measurement/E2_optimal_scoring.html#session-info",
    "title": "3  ✏️ Esercizi",
    "section": "3.2 Session Info",
    "text": "3.2 Session Info\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: x86_64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.3.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] aspect_1.0-6       ggokabeito_0.1.0   viridis_0.6.5      viridisLite_0.4.2 \n [5] ggpubr_0.6.0       ggExtra_0.10.1     bayesplot_1.11.1   gridExtra_2.3     \n [9] patchwork_1.2.0    semTools_0.5-6.920 semPlot_1.1.6      lavaan_0.6-17     \n[13] psych_2.4.1        scales_1.3.0       markdown_1.12      knitr_1.45        \n[17] lubridate_1.9.3    forcats_1.0.0      stringr_1.5.1      dplyr_1.1.4       \n[21] purrr_1.0.2        readr_2.1.5        tidyr_1.3.1        tibble_3.2.1      \n[25] ggplot2_3.5.0      tidyverse_2.0.0    here_1.0.1        \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.15.0  jsonlite_1.8.8     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5   nloptr_2.0.3      \n  [7] rmarkdown_2.26     vctrs_0.6.5        minqa_1.2.6       \n [10] base64enc_0.1-3    rstatix_0.7.2      htmltools_0.5.7   \n [13] broom_1.0.5        Formula_1.2-5      htmlwidgets_1.6.4 \n [16] plyr_1.8.9         sandwich_3.1-0     emmeans_1.10.0    \n [19] zoo_1.8-12         uuid_1.2-0         igraph_2.0.2      \n [22] mime_0.12          lifecycle_1.0.4    pkgconfig_2.0.3   \n [25] Matrix_1.6-5       R6_2.5.1           fastmap_1.1.1     \n [28] shiny_1.8.0        digest_0.6.34      OpenMx_2.21.11    \n [31] fdrtool_1.2.17     colorspace_2.1-0   rprojroot_2.0.4   \n [34] Hmisc_5.1-1        fansi_1.0.6        timechange_0.3.0  \n [37] abind_1.4-5        compiler_4.3.3     withr_3.0.0       \n [40] glasso_1.11        htmlTable_2.4.2    backports_1.4.1   \n [43] carData_3.0-5      ggsignif_0.6.4     MASS_7.3-60.0.1   \n [46] corpcor_1.6.10     gtools_3.9.5       tools_4.3.3       \n [49] pbivnorm_0.6.0     foreign_0.8-86     zip_2.3.1         \n [52] httpuv_1.6.14      nnet_7.3-19        glue_1.7.0        \n [55] quadprog_1.5-8     nlme_3.1-164       promises_1.2.1    \n [58] lisrelToR_0.3      grid_4.3.3         pbdZMQ_0.3-11     \n [61] checkmate_2.3.1    cluster_2.1.6      reshape2_1.4.4    \n [64] generics_0.1.3     gtable_0.3.4       tzdb_0.4.0        \n [67] data.table_1.15.2  hms_1.1.3          car_3.1-2         \n [70] utf8_1.2.4         sem_3.1-15         pillar_1.9.0      \n [73] IRdisplay_1.1      rockchalk_1.8.157  later_1.3.2       \n [76] splines_4.3.3      lattice_0.22-5     survival_3.5-8    \n [79] kutils_1.73        tidyselect_1.2.0   miniUI_0.1.1.1    \n [82] pbapply_1.7-2      stats4_4.3.3       xfun_0.42         \n [85] qgraph_1.9.8       arm_1.13-1         stringi_1.8.3     \n [88] boot_1.3-29        evaluate_0.23      codetools_0.2-19  \n [91] mi_1.1             cli_3.6.2          RcppParallel_5.1.7\n [94] IRkernel_1.3.2     rpart_4.1.23       xtable_1.8-4      \n [97] repr_1.1.6         munsell_0.5.0      Rcpp_1.0.12       \n[100] coda_0.19-4.1      png_0.1-8          XML_3.99-0.16.1   \n[103] parallel_4.3.3     ellipsis_0.3.2     jpeg_0.1-10       \n[106] lme4_1.1-35.1      mvtnorm_1.2-4      openxlsx_4.2.5.2  \n[109] crayon_1.5.2       rlang_1.1.3        multcomp_1.4-25   \n[112] mnormt_2.1.1",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E3_thurstone.html",
    "href": "chapters/measurement/E3_thurstone.html",
    "title": "4  ✏️ Esercizi",
    "section": "",
    "text": "4.1 Introduzione allo Scaling di Thurstone\nLo scaling di Thurstone, sviluppato da Louis Leon Thurstone nel 1931, è un approccio statistico che mira a modellare dati di ranking soggettivo. I dati di ranking soggettivo si producono quando le persone ordinano un insieme di elementi o stimoli secondo un criterio particolare. Questo tipo di dati è particolarmente utile quando è più semplice per i partecipanti esprimere una preferenza relativa piuttosto che stime quantitative precise.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E3_thurstone.html#introduzione-allo-scaling-di-thurstone",
    "href": "chapters/measurement/E3_thurstone.html#introduzione-allo-scaling-di-thurstone",
    "title": "4  ✏️ Esercizi",
    "section": "",
    "text": "4.1.1 Il Modello Thurstoniano\nIl modello Thurstoniano rappresenta un approccio statistico per analizzare e interpretare le preferenze o i ranking individuali rispetto a vari oggetti o stimoli. Questo modello si basa sull’idea che esista una scala latente, ovvero una dimensione non direttamente osservabile, attorno alla quale si distribuiscono i ranking individuali. In altre parole, ogni individuo assegna un punteggio ad ogni oggetto basandosi su criteri personali, ma queste valutazioni individuali sono influenzate da una percezione collettiva o aggregata che può essere descritta su una scala continua latente.\nIl principale obiettivo del modello Thurstoniano è di trasformare queste medie di ranking latenti aggregati, che esistono su una scala continua, in un ranking discreto che possiamo interpretare più facilmente. Per farlo, il modello si avvale di alcune ipotesi chiave:\n\nDistribuzione Gaussiana: Si assume che il ranking latente per ciascun oggetto possa essere descritto da una distribuzione gaussiana.\nMedia Differenziata, Varianza Costante: Il modello presuppone che le distribuzioni gaussiane dei ranking per ciascun oggetto differiscano tra loro solo per la media, mantenendo costante la varianza (scaling di Thurstone caso V). Questo implica che, sebbene gli oggetti possano avere livelli di preferenza medi diversi (alcuni potrebbero essere generalmente preferiti ad altri), la variabilità delle valutazioni (quanto le opinioni dei rispondenti differiscono tra loro) è la stessa per tutti gli oggetti.\n\nPer posizionare gli oggetti sulla scala di Thurstone, si procede nel seguente modo:\n\nSi calcola la proporzione di rispondenti che preferiscono un oggetto rispetto a ciascuno degli altri.\nSi determinano i corrispondenti percentile (z-scores) della distribuzione cumulativa normale, che ci dicono quante deviazioni standard un valore è distante dalla media.\nSi calcola la media di questi z-scores per ciascun oggetto.\n\nEsempio Pratico:\nImmaginiamo di avere tre oggetti: A, B e C. Dopo aver raccolto le preferenze, scopriamo che:\n\nIl 70% dei rispondenti preferisce A rispetto a B e C.\nIl 50% dei rispondenti preferisce B rispetto ad A, ma solo il 30% lo preferisce a C.\nIl 80% dei rispondenti preferisce C rispetto a B, ma solo il 50% lo preferisce ad A.\n\nTrasformando queste percentuali in z-scores, possiamo ottenere una misura della “distanza” di ciascun oggetto dalla media sulla scala latente. Mediando questi z-scores, possiamo creare un ranking discreto che riflette le preferenze medie aggregate, permettendoci di interpretare quali oggetti sono generalmente preferiti rispetto ad altri secondo il modello Thurstoniano.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E3_thurstone.html#studio-sulle-preferenze-riguardanti-le-caratteristiche-delloccupazione-ideale",
    "href": "chapters/measurement/E3_thurstone.html#studio-sulle-preferenze-riguardanti-le-caratteristiche-delloccupazione-ideale",
    "title": "4  ✏️ Esercizi",
    "section": "4.2 Studio sulle preferenze riguardanti le caratteristiche dell’occupazione ideale",
    "text": "4.2 Studio sulle preferenze riguardanti le caratteristiche dell’occupazione ideale\nI dati utilizzati in questo studio sono stati raccolti nell’ambito di una ricerca sulla motivazione lavorativa condotta da Ilke Inceoglu. Nel corso di questa indagine, 1079 partecipanti sono stati invitati a classificare nove aspetti lavorativi in base all’importanza che desideravano che fossero presenti nella loro occupazione ideale:\n\nAmbiente di Supporto (Supporto)\nLavoro Stimolante (Sfida)\nProgressione di Carriera (Carriera)\nLavoro Etico (Etica)\nControllo sul Lavoro, Impatto Personale (Autonomia)\nSviluppo (Sviluppo)\nInterazione Sociale (Interazione)\nAmbiente Competitivo (Competizione)\nAmbiente Piacevole e Sicuro (Sicurezza)\n\nUn punteggio di 1 attribuito a qualsiasi aspetto lavorativo indica che tale aspetto era il più importante per quel partecipante, mentre un punteggio di 9 indica che era il meno importante.\n\nJobFeatures &lt;- rio::import(\"../data/JobFeatures.txt\")\nglimpse(JobFeatures)\n\nRows: 1,079\nColumns: 9\n$ Support     &lt;int&gt; 8, 7, 5, 7, 1, 6, 5, 1, 1, 7, 6, 8, 5, 9, 8, 1, 6, 7, 4, 2~\n$ Challenge   &lt;int&gt; 3, 5, 8, 6, 4, 1, 4, 9, 3, 4, 2, 1, 4, 8, 6, 7, 4, 4, 1, 3~\n$ Career      &lt;int&gt; 4, 1, 1, 8, 8, 3, 7, 2, 7, 6, 3, 4, 6, 1, 3, 5, 8, 3, 5, 4~\n$ Ethics      &lt;int&gt; 5, 6, 9, 9, 3, 7, 2, 8, 4, 1, 9, 3, 7, 5, 9, 6, 7, 5, 9, 8~\n$ Autonomy    &lt;int&gt; 2, 2, 6, 3, 9, 8, 3, 7, 9, 8, 4, 6, 3, 7, 5, 2, 3, 8, 2, 1~\n$ Development &lt;int&gt; 6, 8, 2, 4, 2, 5, 6, 5, 2, 5, 1, 2, 2, 6, 1, 3, 1, 2, 6, 5~\n$ Interaction &lt;int&gt; 1, 3, 3, 2, 6, 2, 1, 4, 6, 9, 5, 5, 1, 4, 2, 8, 2, 6, 3, 6~\n$ Competition &lt;int&gt; 7, 9, 4, 5, 7, 4, 9, 6, 8, 3, 7, 7, 9, 2, 7, 9, 9, 1, 7, 9~\n$ Safety      &lt;int&gt; 9, 4, 7, 1, 5, 9, 8, 3, 5, 2, 8, 9, 8, 3, 4, 4, 5, 9, 8, 7~\n\n\nConsideriamo i dati del primo rispondente:\n\nJobFeatures[1, ]\n\n\nA data.frame: 1 x 9\n\n\n\nSupport\nChallenge\nCareer\nEthics\nAutonomy\nDevelopment\nInteraction\nCompetition\nSafety\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n8\n3\n4\n5\n2\n6\n1\n7\n9\n\n\n\n\n\nQuesto rispondente ha risposto assegnando la caratteristica più importante dell’impego a “Interaction”, seguita da “Autonomy”. L’ultima preferenza è “Safety”.\nEseguiamo lo scaling di Thurstone usando la funzione thurstone del pacchetto psych:\n\nscaling &lt;- psych::thurstone(JobFeatures, ranks = TRUE)\n\nGli attributi dell’oggetto scaling prodotto da thurstone() possono essere elencati nel modo seguente.\n\nattributes(scaling)\n\n\n    $names\n        \n'scale''GF''choice''residual''Call'\n\n    $class\n        \n'psych''thurstone'\n\n\n\n\nI risultati dello scaling si ottengono nel modo seguente. Sono elencati nell’ordine fornito sopra, ovvero Support, Challenge, Career, Ethics, Autonomy, Development, Interaction, Competition e Safety.\nUna media alta indica che i partecipanti attribuiscono un alto valore a questo aspetto lavorativo rispetto agli altri. Tuttavia, poiché le preferenze sono sempre relative, è impossibile identificare in maniera univoca tutte le medie. Pertanto, una delle medie deve essere fissata a un valore arbitrario. È consuetudine fissare la media dell’aspetto meno preferito a 0. Quindi, tutte le altre medie sono positive.\n\nscaling$scale |&gt; print()\n\n[1] 0.97 0.93 0.91 0.92 0.60 1.04 0.63 0.00 0.23\n\n\nLa media più bassa (0.0) corrisponde all’8° aspetto, Competizione, mentre la media più alta (1.04) corrisponde al 6° aspetto, Sviluppo. Ciò significa che l’ambiente competitivo era il meno desiderato, mentre le opportunità di sviluppo personale erano le più desiderate dalle persone nel loro lavoro ideale. Gli altri aspetti sono stati valutati come aventi un’importanza relativa intermedia a queste due, con Sicurezza che ha una media bassa (0.23) - appena superiore a 0 per la Competizione, mentre Supporto, Sfida, Carriera ed Etica hanno medie simili (intorno a 0.9). Autonomia e Interazione hanno medie moderate simili intorno a 0.6.\nL’istruzione seguente produce una matrice 9x9 contenente le proporzioni dei partecipanti nel campione che hanno preferito l’aspetto nella colonna rispetto all’aspetto nella riga. Nella matrice risultante, le righe e le colonne seguono l’ordine delle variabili nel file originale.\n\nscaling$choice |&gt; round(2)\n\n\nA matrix: 9 x 9 of type dbl\n\n\n0.50\n0.47\n0.47\n0.47\n0.36\n0.51\n0.36\n0.20\n0.23\n\n\n0.53\n0.50\n0.47\n0.49\n0.38\n0.53\n0.36\n0.17\n0.28\n\n\n0.53\n0.53\n0.50\n0.50\n0.38\n0.52\n0.39\n0.19\n0.26\n\n\n0.53\n0.51\n0.50\n0.50\n0.36\n0.52\n0.38\n0.19\n0.26\n\n\n0.64\n0.62\n0.62\n0.64\n0.50\n0.67\n0.51\n0.29\n0.34\n\n\n0.49\n0.47\n0.48\n0.48\n0.33\n0.50\n0.29\n0.15\n0.20\n\n\n0.64\n0.64\n0.61\n0.62\n0.49\n0.71\n0.50\n0.22\n0.30\n\n\n0.80\n0.83\n0.81\n0.81\n0.71\n0.85\n0.78\n0.50\n0.61\n\n\n0.77\n0.72\n0.74\n0.74\n0.66\n0.80\n0.70\n0.39\n0.50\n\n\n\n\n\nIl valore maggiore è\n\nmax(scaling$choice)\n\n0.852641334569045\n\n\nQuesto valore, 0.8526, rappresenta la proporzione di partecipanti che hanno preferito l’8° aspetto, Competizione, al 6° aspetto, Sviluppo, ed è il valore più grande nella matrice precedente: questa coppia di caratteristiche ha la preferenza più decisa per un aspetto rispetto all’altro.\nLa preferenza più decisa in termini di proporzioni di persone che scelgono un aspetto rispetto all’altro deve avere la maggiore distanza/differenza sulla scala delle preferenze soggettive (il 6° aspetto, Sviluppo, deve avere una preferenza percepita media molto più alta dell’8° aspetto, Competizione). Questo risultato è effettivamente in linea con i risultati per le medie di utilità, dove la media dello Sviluppo è la più alta con un valore di 1.04 e la Competizione è la più bassa con un valore di 0.\nConsideriamo i residui del modello:\n\nscaling$residual\n\n\nA matrix: 9 x 9 of type dbl\n\n\n0.000000000\n0.0133479521\n0.001732662\n0.0077151838\n-0.005872352\n0.020341170\n0.008084655\n-0.036657121\n0.007039854\n\n\n-0.013347952\n0.0000000000\n0.022201895\n0.0003878924\n-0.005625238\n0.011115950\n0.018649360\n0.009672656\n-0.040301391\n\n\n-0.001732662\n-0.0222018946\n0.000000000\n0.0073806440\n0.004543318\n0.028230409\n0.001106985\n-0.004628757\n-0.008671076\n\n\n-0.007715184\n-0.0003878924\n-0.007380644\n0.0000000000\n0.010887738\n0.027845520\n0.007400479\n-0.012284446\n-0.010646760\n\n\n0.005872352\n0.0056252384\n-0.004543318\n-0.0108877381\n0.000000000\n-0.005140288\n0.006785811\n-0.012984302\n0.017008932\n\n\n-0.020341170\n-0.0111159497\n-0.028230409\n-0.0278455199\n0.005140288\n0.000000000\n0.049380030\n0.002756144\n0.015651117\n\n\n-0.008084655\n-0.0186493603\n-0.001106985\n-0.0074004791\n-0.006785811\n-0.049380030\n0.000000000\n0.042051948\n0.041174300\n\n\n0.036657121\n-0.0096726558\n0.004628757\n0.0122844463\n0.012984302\n-0.002756144\n-0.042051948\n0.000000000\n-0.021145626\n\n\n-0.007039854\n0.0403013911\n0.008671076\n0.0106467596\n-0.017008932\n-0.015651117\n-0.041174300\n0.021145626\n0.000000000\n\n\n\n\n\nL’istruzione precedente produce una matrice 9x9 contenente le differenze tra le proporzioni osservate (la matrice delle scelte) e le proporzioni attese (proporzioni che preferiscono l’aspetto nella riga rispetto all’aspetto nella colonna, che sarebbe atteso in base alle distribuzioni normali standard delle preferenze soggettive intorno alle medie scalate come sopra). Gli scarti tra i valori attesi e quelli osservati sono il modo più diretto di misurare se un modello (in questo caso, il modello proposto da Thurstone) “si adatta” ai dati osservati. Gli scarti piccoli (vicini allo zero) indicano che ci sono piccole discrepanze tra le scelte osservate e le scelte previste dal modello; il che significa che il modello che abbiamo adottato è piuttosto buono.\nInfine, esaminiamo un indice di bontà di adattamento:\n\nscaling$GF\n\n0.998754828963899\n\n\nIl valore GF (Goodness of Fit) viene calcolato come 1 meno la somma dei residui al quadrato divisi per i valori osservati al quadrato. Quando i residui sono quasi zero, i loro rapporti al quadrato rispetto alle proporzioni osservate dovrebbero anch’essi avvicinarsi a zero. Di conseguenza, l’indice di bontà di adattamento di un modello ben adattato dovrebbe essere vicino a 1.\nNella nostra analisi, tutti i residui sono notevolmente piccoli, indicando una stretta corrispondenza tra le scelte osservate (proporzioni di preferenze per una caratteristica rispetto a un’altra). Questo allineamento preciso si riflette nell’indice GF, che è quasi 1, suggerendo che il modello di Thurstone cattura adeguatamente le proprietà dei dati relativi alle caratteristiche dell’occupazione ideale.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E3_thurstone.html#considerazioni-conclusive",
    "href": "chapters/measurement/E3_thurstone.html#considerazioni-conclusive",
    "title": "4  ✏️ Esercizi",
    "section": "4.3 Considerazioni conclusive",
    "text": "4.3 Considerazioni conclusive\nQuesta metodologia, introdotta da Louis Leon Thurstone negli anni ’20, rappresenta una delle forme più semplici e intuitive di scaling, dove per “scaling” si intende il processo di costruzione di un ordinamento di valori lungo un continuum psicologico. Lo scaling thurstoniano si basa sulla premessa che sia possibile ordinare stimoli o concetti secondo il grado in cui incarnano una certa proprietà psicologica, creando così una scala di misura che riflette le percezioni, le attitudini o i giudizi degli individui.\nUno degli aspetti centrali dello scaling di Thurstone, in particolare il caso V della sua legge del giudizio comparativo, è l’assunzione che le distribuzioni di ranking degli stimoli abbiano varianze uguali. Questa ipotesi, pur facilitando la modellizzazione matematica e l’interpretazione dei dati, è stata oggetto di critiche poiché difficilmente riscontrabile nella pratica. Le varianze possono variare significativamente tra gli stimoli a seconda della coerenza dei giudizi degli individui e della natura degli stimoli stessi. Questa limitazione ha stimolato lo sviluppo e l’adozione di metodi alternativi più flessibili per affrontare la complessità dello scaling psicologico.\nNel panorama contemporaneo, l’approccio più diffuso e metodologicamente avanzato per lo scaling psicologico deriva dalla Teoria della Risposta all’Item (IRT). L’IRT supera alcune delle limitazioni intrinseche allo scaling thurstoniano offrendo un quadro teorico e metodologico che considera la probabilità di una certa risposta a un item in funzione delle caratteristiche dell’item stesso e del livello dell’attributo psicologico del rispondente. Questo approccio permette di gestire in modo più efficace la varianza tra gli stimoli e di fornire stime più accurate delle proprietà psicometriche degli items e delle caratteristiche degli individui.\nIn conclusione, mentre lo scaling thurstoniano ha rappresentato un passo fondamentale nello sviluppo degli strumenti di misurazione in psicologia, l’evoluzione metodologica e teorica ha portato a preferire approcci basati sull’IRT. Questo non diminuisce il valore storico e didattico dello scaling di Thurstone, che continua a essere un esempio introduttivo prezioso per comprendere i concetti fondamentali dello scaling psicologico. Tuttavia, è nell’ambito della IRT che attualmente si trovano le soluzioni più robuste e sofisticate per affrontare le sfide della misurazione psicologica, guidando la ricerca e l’applicazione pratica nel campo della psicometria contemporanea.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E3_thurstone.html#sesssion-info",
    "href": "chapters/measurement/E3_thurstone.html#sesssion-info",
    "title": "4  ✏️ Esercizi",
    "section": "4.4 Sesssion Info",
    "text": "4.4 Sesssion Info\n\nsessionInfo()\n\nR version 4.3.2 (2023-10-31)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.3.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] rio_1.0.1         ggokabeito_0.1.0  viridis_0.6.5     viridisLite_0.4.2\n [5] ggpubr_0.6.0      ggExtra_0.10.1    bayesplot_1.11.0  gridExtra_2.3    \n [9] patchwork_1.2.0   semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-17    \n[13] psych_2.4.1       scales_1.3.0      markdown_1.12     knitr_1.45       \n[17] lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n[21] purrr_1.0.2       readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n[25] ggplot2_3.4.4     tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.15.0  jsonlite_1.8.8     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.4.1 nloptr_2.0.3      \n  [7] rmarkdown_2.25     vctrs_0.6.5        minqa_1.2.6       \n [10] base64enc_0.1-3    rstatix_0.7.2      htmltools_0.5.7   \n [13] broom_1.0.5        Formula_1.2-5      htmlwidgets_1.6.4 \n [16] plyr_1.8.9         sandwich_3.1-0     emmeans_1.10.0    \n [19] zoo_1.8-12         uuid_1.2-0         igraph_2.0.1.1    \n [22] mime_0.12          lifecycle_1.0.4    pkgconfig_2.0.3   \n [25] Matrix_1.6-5       R6_2.5.1           fastmap_1.1.1     \n [28] shiny_1.8.0        digest_0.6.34      OpenMx_2.21.11    \n [31] fdrtool_1.2.17     colorspace_2.1-0   rprojroot_2.0.4   \n [34] Hmisc_5.1-1        fansi_1.0.6        timechange_0.3.0  \n [37] abind_1.4-5        compiler_4.3.2     withr_3.0.0       \n [40] glasso_1.11        htmlTable_2.4.2    backports_1.4.1   \n [43] carData_3.0-5      R.utils_2.12.3     ggsignif_0.6.4    \n [46] MASS_7.3-60.0.1    corpcor_1.6.10     gtools_3.9.5      \n [49] tools_4.3.2        pbivnorm_0.6.0     foreign_0.8-86    \n [52] httpuv_1.6.14      zip_2.3.1          nnet_7.3-19       \n [55] R.oo_1.26.0        glue_1.7.0         quadprog_1.5-8    \n [58] promises_1.2.1     nlme_3.1-164       lisrelToR_0.3     \n [61] grid_4.3.2         pbdZMQ_0.3-11      checkmate_2.3.1   \n [64] cluster_2.1.6      reshape2_1.4.4     generics_0.1.3    \n [67] gtable_0.3.4       tzdb_0.4.0         R.methodsS3_1.8.2 \n [70] data.table_1.15.0  hms_1.1.3          car_3.1-2         \n [73] utf8_1.2.4         sem_3.1-15         pillar_1.9.0      \n [76] IRdisplay_1.1      rockchalk_1.8.157  later_1.3.2       \n [79] splines_4.3.2      lattice_0.22-5     survival_3.5-7    \n [82] kutils_1.73        tidyselect_1.2.0   miniUI_0.1.1.1    \n [85] pbapply_1.7-2      stats4_4.3.2       xfun_0.42         \n [88] qgraph_1.9.8       arm_1.13-1         stringi_1.8.3     \n [91] boot_1.3-28.1      evaluate_0.23      codetools_0.2-19  \n [94] mi_1.1             cli_3.6.2          RcppParallel_5.1.7\n [97] IRkernel_1.3.2     rpart_4.1.23       xtable_1.8-4      \n[100] repr_1.1.6         munsell_0.5.0      Rcpp_1.0.12       \n[103] coda_0.19-4.1      png_0.1-8          XML_3.99-0.16.1   \n[106] parallel_4.3.2     ellipsis_0.3.2     jpeg_0.1-10       \n[109] lme4_1.1-35.1      mvtnorm_1.2-4      openxlsx_4.2.5.2  \n[112] crayon_1.5.2       rlang_1.1.3        multcomp_1.4-25   \n[115] mnormt_2.1.1",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html",
    "href": "chapters/ctt/01_ctt_1.html",
    "title": "5  Fondamenti teorici",
    "section": "",
    "text": "5.1 Introduzione\nLa teoria classica dei test (Classic Test Theory, CTT) è una teoria fondamentale utilizzata in psicometria per valutare e misurare le caratteristiche psicologiche degli individui attraverso l’uso di test e questionari. Secondo questa teoria, il punteggio ottenuto da un individuo in un test è influenzato da due componenti principali: il punteggio vero dell’individuo sulla caratteristica misurata e l’errore casuale di misurazione.\nIl punteggio vero rappresenta la misura effettiva della caratteristica che si intende valutare nel soggetto. Tuttavia, a causa di vari fattori come l’errore di misurazione, le distrazioni o l’incertezza dell’individuo durante il test, il punteggio osservato può deviare dal punteggio vero. Questa discrepanza tra il punteggio vero e il punteggio osservato viene definita errore di misurazione.\nLa teoria classica dei test si focalizza sulla quantificazione della relazione tra il punteggio vero, il punteggio osservato e l’errore di misurazione. Attraverso l’uso di statistiche come la media, la deviazione standard e il coefficiente di affidabilità, questa teoria fornisce una base concettuale per la costruzione dei test, l’interpretazione dei risultati e l’analisi dell’affidabilità del test stesso.\nÈ importante notare che negli ultimi anni sono state sviluppate altre teorie, come la teoria della risposta all’item o la teoria della generalizzabilità (Generalizability Theory), che cercano di superare alcune delle limitazioni della teoria classica dei test. Tuttavia, nonostante queste nuove teorie, la teoria classica dei test continua ad essere ampiamente utilizzata e costituisce ancora una base fondamentale per la valutazione e la misurazione delle caratteristiche psicologiche degli individui.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html#tipi-di-costrutti-e-modelli-latenti",
    "href": "chapters/ctt/01_ctt_1.html#tipi-di-costrutti-e-modelli-latenti",
    "title": "5  Fondamenti teorici",
    "section": "5.2 Tipi di Costrutti e Modelli Latenti",
    "text": "5.2 Tipi di Costrutti e Modelli Latenti\nUn costrutto è un concetto, spesso considerato una idea latente o fenomeno non direttamente osservabile (Petersen 2024). Ad esempio, la depressione può essere un costrutto perché non possiamo misurare direttamente il livello di depressione di una persona, ma lo inferiamo attraverso indicatori indiretti come umore basso, perdita di interesse, difficoltà nel sonno, ecc. Gli indicatori sono misure che riflettono il costrutto.\nEsistono due tipi principali di costrutti: costrutti riflessivi e costrutti formativi.\n\n5.2.1 1. Costrutto Riflessivo\nIn un costrutto riflessivo, il costrutto causa le misure e gli indicatori riflettono il costrutto (Bollen e Lennox 1991). Ad esempio, l’estroversione è un costrutto riflessivo perché la risposta agli indicatori come “piacere di parlare con sconosciuti” o “andare a feste” riflette il livello di estroversione della persona. Gli indicatori sono correlati perché riflettono tutti un unico costrutto latente. La consistenza interna tra gli indicatori è quindi attesa.\n\n\n5.2.2 2. Costrutto Formativo\nIn un costrutto formativo, invece, le misure causano il costrutto (Bollen e Lennox 1991). Ad esempio, lo stato socioeconomico (SES) può essere formato dall’educazione, dal reddito e dal prestigio occupazionale di una persona. Gli indicatori formano il costrutto e potrebbero non essere correlati tra loro, in contrasto con i costrutti riflessivi.\n\n\n5.2.3 Differenze tra Costrutti Riflessivi e Formativi\n\nCorrelazioni tra indicatori: Gli indicatori riflessivi sono correlati, mentre quelli formativi non lo devono necessariamente essere.\nCampionamento degli indicatori: Nei costrutti formativi è essenziale campionare tutti gli aspetti del costrutto, mentre nei riflessivi gli indicatori possono essere intercambiabili.\nCorrelazioni ottimali: Nei costrutti riflessivi, alte correlazioni tra indicatori sono desiderabili, mentre nei formativi correlazioni troppo alte possono creare multicollinearità.\n\n\n\n5.2.4 Come Stimare\n\nCostrutti riflessivi: Devono essere stimati con modelli latenti come SEM, analisi fattoriale o teoria della risposta dell’item (IRT).\nCostrutti formativi: Possono essere stimati con medie pesate o tramite SEM.\n\n\n\n5.2.5 Conclusione\nPrima di stimare un costrutto, è importante comprendere la sua natura teorica. I costrutti riflessivi richiedono modelli che riflettano la varianza comune tra gli indicatori, mentre i costrutti formativi possono essere stimati con medie o punteggi sommativi. La teoria è essenziale per guidare la scelta del metodo di stima.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html#la-teoria-classica",
    "href": "chapters/ctt/01_ctt_1.html#la-teoria-classica",
    "title": "5  Fondamenti teorici",
    "section": "5.3 La Teoria Classica",
    "text": "5.3 La Teoria Classica\nLa Teoria Classica dei Test (CTT), originariamente sviluppata da Spearman (1904) e successivamente formalizzata da Lord e Novick (1968), offre un quadro teorico che descrive come i punteggi ottenuti da un test psicometrico siano legati a un costrutto latente.\nUno dei concetti fondamentali all’interno della CTT riguarda l’affidabilità dei punteggi ottenuti dai test. L’affidabilità, in questo contesto, indica la capacità del test di produrre risultati coerenti e stabili in diverse occasioni. Questo concetto può essere compreso attraverso l’equazione fondamentale della CTT:\n\\[\nX = T + E,\n\\tag{5.1}\\]\ndove \\(X\\) rappresenta il punteggio osservato nel test, \\(T\\) è il punteggio vero (ovvero la rappresentazione della variabile latente di interesse), e \\(E\\) rappresenta l’errore di misurazione.\nUn aspetto di particolare rilevanza all’interno della CTT riguarda la varianza dell’errore. Maggiore è questa varianza, minore sarà la precisione con cui il punteggio vero si riflette nei punteggi osservati. In un contesto ideale, gli errori di misurazione sarebbero tutti nulli, garantendo punteggi esatti per ogni partecipante. Tuttavia, a causa delle inevitabili imperfezioni, si verifica una certa variazione negli errori. La deviazione standard associata a questi errori è chiamata errore standard di misurazione e viene indicata con \\(\\sigma_E\\). Uno degli obiettivi principali della CTT è stimare \\(\\sigma_E\\) al fine di valutare la qualità di una scala psicometrica.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html#le-due-componenti-del-punteggio-osservato",
    "href": "chapters/ctt/01_ctt_1.html#le-due-componenti-del-punteggio-osservato",
    "title": "5  Fondamenti teorici",
    "section": "5.4 Le due componenti del punteggio osservato",
    "text": "5.4 Le due componenti del punteggio osservato\nL’Equazione 5.1 rappresenta il cuore del modello, sottolineando che il punteggio osservato è il risultato dell’addizione del punteggio vero e dell’errore di misurazione.\nL’obiettivo principale della CTT è quantificare l’errore di misurazione (rappresentato da \\(\\sigma_E\\)) per valutare l’affidabilità del test e ottenere una stima dell’errore standard di misurazione. L’affidabilità del test riflette la precisione con cui il test può misurare il punteggio vero (Coaley, 2014). Si calcola come il rapporto tra la varianza dei punteggi veri e la varianza dei punteggi osservati. Un’alta affidabilità indica una ridotta incertezza dovuta all’errore di misurazione (\\(\\sigma_E\\)), indicando che il punteggio osservato (\\(X\\)) fornisce una misura accurata del punteggio vero (\\(T\\)). Al contrario, una bassa affidabilità indica un elevato errore di misurazione (\\(\\sigma_E\\)) e una significativa discrepanza tra il punteggio osservato e il punteggio vero.\nLa stima dell’errore standard di misurazione comporta il calcolo della deviazione standard della variabile casuale \\(E\\) (ossia \\(\\sigma_E\\)), che rappresenta l’errore di misurazione influente sui punteggi veri. Questa stima offre un’indicazione della dispersione dei punteggi osservati attorno ai punteggi veri, causata dall’errore di misurazione.\nNelle prossime sezioni, esploreremo come il concetto chiave di attendibilità nella CTT possa essere collegato al coefficiente di determinazione nel contesto del modello statistico di regressione lineare. Inoltre, vedremo come l’errore standard di misurazione della CTT possa essere associato all’errore standard nella regressione.\n\n5.4.1 Il punteggio vero\nL’Equazione 5.1 ci spiega che il punteggio osservato è il risultato della combinazione di due componenti: una componente sistematica (il punteggio vero) e una componente aleatoria (l’errore di misurazione). Ma cosa rappresenta esattamente il punteggio vero? La Teoria Classica dei Test (CTT) attribuisce diverse interpretazioni al concetto di punteggio vero.\n\nDa un punto di vista psicologico, la CTT considera il test come una selezione casuale di domande da un insieme più ampio di domande che riflettono il costrutto da misurare (Nunnally 1994; Kline 2013). In questo contesto, il punteggio vero rappresenta il punteggio che un partecipante otterrebbe se rispondesse a tutte le domande dell’insieme completo. L’errore di misurazione riflette quindi quanto le domande selezionate rappresentano l’intero insieme di domande relative al costrutto.\nIn modo equivalente, il punteggio vero può essere considerato come il punteggio non influenzato da fattori esterni al costrutto, come effetti di apprendimento, fatica, memoria, motivazione, e così via. Poiché è concepito come un processo completamente casuale, la componente aleatoria non introduce alcun bias nella tendenza centrale della misurazione (la media di \\(E\\) è assunta essere uguale a 0).\nDal punto di vista statistico, il punteggio vero è un punteggio inosservabile che rappresenta il valore atteso di infinite misurazioni del punteggio ottenute:\n\n\\[\nT = \\mathbb{E}(X) \\equiv \\mu_X \\equiv \\mu_{T}.\n\\]\nCombinando le definizioni presentate sopra, Lord e Novick (1968) concepiscono il punteggio vero come la media dei punteggi che un soggetto otterrebbe se il test venisse somministrato ripetutamente nelle stesse condizioni, senza effetti di apprendimento o fatica.\n\n\n5.4.2 Somministrazioni ripetute\nNel modello della Teoria Classica dei Test (CTT) possiamo distinguere due tipi di esperimenti casuali: uno in cui l’unità di osservazione (l’individuo) è considerata come una variabile campionaria, e l’altro in cui il punteggio per un determinato individuo è trattato come una variabile casuale.\nUn importante risultato è dato dalla combinazione di questi due esperimenti casuali, dimostrando che i risultati della CTT, sviluppata ipotizzando somministrazioni ripetute immaginarie del test allo stesso individuo nelle stesse condizioni, si estendono al caso di una singola somministrazione del test su un campione di individui (Allen e Yen 2001). Questo risultato ci permette di attribuire un significato empirico alle quantità discusse dalla CTT quando consideriamo la somministrazione del test a una popolazione di individui:\n\n\\(\\sigma^2_X\\) rappresenta la varianza del punteggio osservato nella popolazione,\n\\(\\sigma^2_T\\) rappresenta la varianza del punteggio vero nella popolazione,\n\\(\\sigma^2_E\\) rappresenta la varianza della componente d’errore nella popolazione.\n\n\n\n5.4.3 Le assunzioni sul punteggio ottenuto\nLa CTT assume che la media del punteggio osservato \\(X\\) sia uguale alla media del punteggio vero,\n\\[\n\\mu_X \\equiv \\mu_{T},\n\\tag{5.2}\\]\nin altri termini, assume che il punteggio osservato fornisca una stima statisticamente corretta dell’abilità latente (punteggio vero).\nIn pratica, il punteggio osservato non sarà mai uguale all’abilità latente, ma corrisponde solo ad uno dei possibili punteggi che il soggetto può ottenere, subordinatamente alla sua abilità latente. L’errore della misura è la differenza tra il punteggio osservato e il punteggio vero:\n\\[\nE \\equiv X - T.\n\\]\nIn base all’assunzione secondo cui il valore atteso dei punteggi è uguale alla media del valore vero, segue che\n\\[\n\\mathbb{E}(E) = \\mathbb{E}(X - T) = \\mathbb{E}(X) - \\mathbb{E}(T) = \\mu_{T} - \\mu_{T} = 0,\n\\]\novvero, il valore atteso degli errori è uguale a zero.\nPer dare un contenuto concreto alle affermazioni precedenti, consideriamo la seguente simulazione svolta in \\(\\textsf{R}\\). In tale simulazione il punteggio vero \\(T\\) e l’errore \\(E\\) sono creati in modo tale da soddisfare i vincoli della CTT: \\(T\\) e \\(E\\) sono variabili casuali gaussiane tra loro incorrelate. Nella simulazione generiamo 100 coppie di valori \\(X\\) e \\(T\\) con i seguenti parametri: \\(T \\sim \\mathcal{N}(\\mu_T = 12, \\sigma^2_T = 6)\\), \\(E \\sim \\mathcal{N}(\\mu_E = 0, \\sigma^2_T = 3)\\):\n\nset.seed(8394)\n\nn &lt;- 100\nSigma &lt;- matrix(c(6, 0, 0, 3), byrow = TRUE, ncol = 2)\nmu &lt;- c(12, 0)\ndat &lt;- mvrnorm(n, mu, Sigma, empirical = TRUE)\nT &lt;- dat[, 1]\nE &lt;- dat[, 2]\n\nLe istruzioni precedenti (empirical = TRUE) creano un campione di valori nei quali le medie e la matrice di covarianze assumono esattamente i valori richiesti. Possiamo dunque immaginare tale insieme di dati come la “popolazione”.\nSecondo la CTT, il punteggio osservato è \\(X = T + E\\). Simuliamo dunque il punteggio osservato \\(X\\) come:\n\nX &lt;- T + E\n\nLe prime 6 osservazioni così ottenute sono:\n\ntibble(X, T, E) |&gt; head()\n\n\nA tibble: 6 x 3\n\n\nX\nT\nE\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n15.698623\n16.765359\n-1.0667358\n\n\n13.657503\n12.248096\n1.4094073\n\n\n6.731979\n7.852136\n-1.1201563\n\n\n14.621813\n14.233699\n0.3881133\n\n\n10.606647\n10.187035\n0.4196115\n\n\n12.370288\n13.329971\n-0.9596831\n\n\n\n\n\nUn diagramma di dispersione è fornito nella figura seguente:\n\ntibble(X, T) |&gt;\nggplot(aes(T, X)) +\n    geom_point(position = position_jitter(w = .3, h = .3)) +\n    geom_abline(col = \"blue\")\n\n\n\n\n\n\n\n\nSecondo la CTT, il valore atteso di \\(T\\) è uguale al valore atteso di \\(X\\). Verifichiamo questa assunzione nei nostri dati\n\nmean(T) == mean(X)\n\nTRUE\n\n\nL’errore deve avere media zero:\n\nmean(E)\n\n-1.33660443824013e-17\n\n\nLe varianze dei punteggi veri, dei punteggi osservati e degli errori sono rispettivamente uguali a:\n\nc(var(T), var(X), var(E))  \n\n[1] 6 9 3",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html#lerrore-standard-della-misurazione-sigma_e",
    "href": "chapters/ctt/01_ctt_1.html#lerrore-standard-della-misurazione-sigma_e",
    "title": "5  Fondamenti teorici",
    "section": "5.5 L’errore standard della misurazione \\(\\sigma_E\\)",
    "text": "5.5 L’errore standard della misurazione \\(\\sigma_E\\)\nLa radice quadrata della varianza degli errori di misurazione, ovvero la deviazione standard degli errori, \\(\\sigma_E\\), è la quantità fondamentale della CTT ed è chiamata errore standard della misurazione. La stima dell’errore standard della misurazione costituisce uno degli obiettivi più importanti della CTT.\nNel caso presente, abbiamo:\n\nsqrt(var(E))\n\n1.73205080756888\n\n\nRicordiamo che la deviazione standard indica quanto i dati di una distribuzione si discostano dalla media di quella distribuzione. È simile allo scarto tipico, ovvero la distanza media tra i valori della distribuzione e la loro media. Possiamo dunque utilizzare questa proprietà per descrivere il modo in cui la CTT interpreta la quantità \\(\\sigma_E\\): l’errore standard della misurazione \\(\\sigma_E\\) ci dice qual è, approssimativamente, la quantità attesa di variazione del punteggio osservato, se il test venisse somministrato ripetute volte al medesimo rispondente sotto le stesse condizioni (ovvero, in assenza di effetti di apprendimento o di fatica).",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html#assiomi-della-teoria-classica",
    "href": "chapters/ctt/01_ctt_1.html#assiomi-della-teoria-classica",
    "title": "5  Fondamenti teorici",
    "section": "5.6 Assiomi della Teoria Classica",
    "text": "5.6 Assiomi della Teoria Classica\nLa CTT assume che gli errori siano delle variabili casuali incorrelate tra loro\n\\[\n\\rho(E_i, E_k \\mid T) = 0, \\qquad\\text{con}\\; i \\neq k,\n\\]\ne incorrelate con il punteggio vero,\n\\[\n\\rho(E, T) = 0,\n\\]\nle quali seguono una distribuzione gaussiana con media zero e deviazione standard pari a \\(\\sigma_E\\):\n\\[\nE \\sim \\mathcal{N}(0, \\sigma_E).\n\\]\nLa quantità \\(\\sigma_E\\) è appunto l’errore standard della misurazione. Sulla base di tali assunzioni la CTT deriva la formula dell’attendibilità di un test. Si noti che le assunzioni della CTT hanno una corrispondenza puntuale con le assunzioni su cui si basa il modello di regressione lineare.\nVerifichiamo le assunzioni per i dati dell’esempio.\n\ncor(E, T)\n\n-4.22920527591589e-17\n\n\n\nplot(density(E))\ncurve(dnorm(x, mean(E), sd(E)), add = TRUE, col = \"red\")",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html#misure-parallele-tau-equivalenti-essenzialmente-tau-equivalenti-e-congenetiche",
    "href": "chapters/ctt/01_ctt_1.html#misure-parallele-tau-equivalenti-essenzialmente-tau-equivalenti-e-congenetiche",
    "title": "5  Fondamenti teorici",
    "section": "5.7 Misure parallele, \\(\\tau\\)-equivalenti, essenzialmente \\(\\tau\\)-equivalenti e congenetiche",
    "text": "5.7 Misure parallele, \\(\\tau\\)-equivalenti, essenzialmente \\(\\tau\\)-equivalenti e congenetiche\nNell’ambito della CTT, le misure della stessa entità (che possono essere item, sottoscale o test) possono essere classificate in base al loro livello di similarità. In questa sezione, verranno definiti quattro livelli di similarità: misure parallele, \\(\\tau\\)-equivalenti, essenzialmente \\(\\tau\\)-equivalenti e congeneriche. È importante notare che questi livelli sono gerarchici nel senso che il livello più alto (misure parallele) richiede la maggiore similarità, mentre i livelli inferiori nella gerarchia consentono una minore similarità nelle proprietà del test. Ad esempio, le misure parallele devono avere varianze di vero punteggio uguali, mentre le misure congenetiche non richiedono questa condizione.\nUn modo utile per comprendere questi livelli è riflettere sulle relazioni tra i punteggi veri di coppie di misure {cite:p}komaroff1997effect. Nella CTT, la relazione tra i punteggi veri su due misure (\\(t_i\\) e \\(t_j\\)) è espressa come:\n\\[\nt_i = a_{ij }+ b_{ij} * t_{j}.\n\\]\nSe il termine \\(b_{ij}\\) non è zero, le due misure hanno quote di punteggio vero differenti. Se il termine \\(a_{ij}\\) non è zero, le medie del punteggio vero delle misure sono diverse. I termini \\(a_{ij}\\) e \\(b_{ij}\\) sono sottoscritti per entrambe le misure (\\(i\\) e \\(j\\)), indicando che queste costanti possono variare tra le coppie di misure (anche se possono essere uguali tra tutte le coppie di misure).\n\n5.7.1 Misure parallele\nLe misure parallele rappresentano il tipo di similarità più forte tra le misure. Per le misure parallele, \\(a_{ij}\\) = 0 e \\(b_{ij}\\) = 1 per tutte le coppie di misure. Ciò implica che i punteggi veri di tutte le misure sono esattamente uguali. Di conseguenza, le varianze dei punteggi veri delle misure saranno anch’esse uguali. Le misure parallele presentano anche varianze di errore uguali. Queste proprietà (medie dei punteggi veri uguali, varianze dei punteggi veri uguali e varianze di errore uguali) implicano che le misure parallele avranno anche medie dei punteggi osservati uguali e varianze dei punteggi osservati uguali. Inoltre, i punteggi osservati avranno correlazioni uguali tra loro. Quest’ultima proprietà deriva dal fatto che i punteggi osservati sulle misure parallele sono perfettamente correlati linearmente. Le misure parallele, quindi, catturano un costrutto comune e misurano tale costrutto con la stessa precisione.\nSimuliamo i punteggi di due test paralleli in R.\n\nset.seed(2237) # setting the seed ensure reproducibility\nnum_person &lt;- 1000 # number of respondents\n# True scores for Test 1\nt1 &lt;- rnorm(num_person, mean = 20, sd = 5)\n# Error scores for Test 1\ne1 &lt;- rnorm(num_person, mean = 0, sd = 2)\n# Observed scores for Test 1\nx1 &lt;- t1 + e1\n# True scores for Test 2\nt2 &lt;- t1 # parallel tests have equal true scores\n# Error scores for Test 2\ne2 &lt;- rnorm(num_person, mean = 0, sd = 2)\n# Observed scores for Test 2\nx2 &lt;- t2 + e2\n\n\n# Merge into a data frame\ntest_df &lt;- data.frame(x1, x2)\n# Get means and variances\nmv &lt;- datasummary(x1 + x2 ~ Mean + Var,\n    data = test_df,\n    output = \"data.frame\"\n)\nmv\n\n\nA data.frame: 2 x 3\n\n\n\nMean\nVar\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\nx1\n20.41\n29.20\n\n\nx2\n20.31\n30.27\n\n\n\n\n\n\ncor(test_df$x1, test_df$x2)\n\n0.865310361839848\n\n\nNel caso di due test paralleli, le medie e le varianze dei punteggi osservati sono (teoricamente) uguali; la correlazione descrive l’affidabilità del test.\n\n\n5.7.2 Misure \\(\\tau\\)-equivalenti\nLe misure \\(\\tau\\)-equivalenti, talvolta chiamate misure con equivalenza dei punteggi veri, presentano una forma di similarità leggermente più debole rispetto alle misure parallele. Come le misure parallele, le misure \\(\\tau\\)-equivalenti hanno \\(a_{ij}\\) = 0 e \\(b_{ij}\\) = 1 per tutte le coppie di misure; ciò significa che hanno varianze dei punteggi veri uguali. Tuttavia, le misure \\(\\tau\\)-equivalenti non sono obbligate ad avere varianze di errore uguali. È importante notare che la \\(\\tau\\)-equivalenza non richiede necessariamente varianze di errore diverse, ma semplicemente consente la possibilità di varianze di errore diverse. La \\(\\tau\\)-equivalenza, quindi, rilassa il vincolo che le varianze di errore debbano essere uguali. Pertanto, sebbene le misure \\(\\tau\\)-equivalenti debbano avere varianze dei punteggi veri uguali, possono o meno avere varianze di punteggio osservato uguali. Le misure \\(\\tau\\)-equivalenti presentano anche covarianze dei punteggi veri (e dei punteggi osservati) uguali tra loro.\nSimuliamo due misure \\(\\tau\\)-equivalenti.\n\nset.seed(2237) # setting the seed ensure reproducibility\nnum_person &lt;- 1000 # number of respondents\n# True scores for Test 1\nt1 &lt;- rnorm(num_person, mean = 20, sd = 5)\n# Error scores for Test 1\ne1 &lt;- rnorm(num_person, mean = 0, sd = 2)\n# Observed scores for Test 1\nx1 &lt;- t1 + e1\n# True scores for Test 2\nt2 &lt;- t1 # parallel tests have equal true scores\n# Error scores for Test 2\ne2 &lt;- rnorm(num_person, mean = 0, sd = 2)\n# Observed scores for Test 2\nx2 &lt;- t2 + e2\n\nSe conosciamo i punteggi veri, le stime dell’affidabilità di x1 e x2 sono:\n\n# Reliability for x1\nvar(t1) / var(x1)\n\n0.878424313030747\n\n\n\n# Reliability for x2\nvar(t2) / var(x2)\n\n0.847351804948915\n\n\n\n# Merge into a data frame\ntest_df &lt;- data.frame(x1, x2)\n# Get means and variances\nmv &lt;- datasummary(x1 + x2 ~ Mean + Var,\n    data = test_df,\n    output = \"data.frame\"\n)\nmv\n\n\nA data.frame: 2 x 3\n\n\n\nMean\nVar\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\nx1\n20.41\n29.20\n\n\nx2\n20.31\n30.27\n\n\n\n\n\n\ncor(test_df$x1, test_df$x2)\n\n0.865310361839848\n\n\nIn conclusione, nel caso di due test \\(\\tau\\)-equivalenti, le medie e le varianze dei punteggi osservati sono (teoricamente) uguali. Anche in questo caso, la correlazione descrive l’affidabilità del test.\n\n\n5.7.3 Misure essenzialmente \\(\\tau\\)-equivalenti\nLe misure essenzialmente \\(\\tau\\)-equivalenti sono una forma leggermente più debole di \\(\\tau\\)-equivalenza in cui \\(a_{ij} \\neq 0\\) ma \\(b_{ij} = 1\\). Ciò significa che i punteggi veri delle misure essenzialmente \\(\\tau\\)-equivalenti possono differire per una costante additiva. Ad esempio, in una coppia di misure essentially \\(\\tau\\)-equivalenti, un punteggio vero potrebbe essere 2 in più dell’altro. Sebbene la \\(\\tau\\)-equivalenza essenziale ammetta differenze costanti tra i punteggi veri, ciò non è obbligatorio. Alcuni elementi di un insieme \\(\\tau\\)-equivalente possono avere medie di punteggio vero che differiscono per una costante e altri no. È importante notare che se due punteggi veri differiscono per una costante, sarebbero comunque perfettamente correlati linearmente. E sebbene tali punteggi veri possano avere medie diverse, non avrebbero varianze diverse. Pertanto, le loro varianze di punteggio vero sarebbero uguali, anche se le varianze di punteggio osservato potrebbero essere diverse poiché le misure essenzialmente \\(\\tau\\)-equivalenti possono avere varianze di errore diverse. I punteggi veri delle misure essenzialmente \\(\\tau\\)-equivalenti sono perfettamente correlati linearmente, quindi queste misure avrebbero covarianze di punteggio vero uguali tra loro. Tuttavia, né i punteggi \\(\\tau\\)-equivalenti né quelli essenazialmente \\(\\tau\\)-equivalenti avranno correlazioni uguali. Questo perché possono avere varianze di punteggio osservato e deviazioni standard diverse.\n\n# True scores for Test 3\nt3 &lt;- 5 + t1 # essentially tau-equivalent tests\n# Error scores for Test 3 (larger error SDs)\ne3 &lt;- rnorm(num_person, mean = 0, sd = 4)\n# Observed scores for Test 2\nx3 &lt;- t3 + e3\n\n\n# Merge into a data frame\ntest_df2 &lt;- data.frame(x1, x3)\n# Get means and variances\nmv &lt;- datasummary(x1 + x3 ~ Mean + Var,\n    data = test_df2,\n    output = \"data.frame\"\n)\nmv\n\n\nA data.frame: 2 x 3\n\n\n\nMean\nVar\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\nx1\n20.41\n29.20\n\n\nx3\n25.41\n41.50\n\n\n\n\n\nSe conosciamo i punteggi veri, la stima dell’affidabilità di x3 è:\n\n# Reliability for x3\nvar(t3) / var(x3)\n\n0.618012243898734\n\n\nIn conclusione, nel caso di test essenzialmente \\(\\tau\\)-equivalenti, le medie e le varianze dei punteggi osservati sono diverse; la correlazione non è uguale all’affidabilità.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html#misure-congeneriche",
    "href": "chapters/ctt/01_ctt_1.html#misure-congeneriche",
    "title": "5  Fondamenti teorici",
    "section": "5.8 Misure congeneriche",
    "text": "5.8 Misure congeneriche\nInfine, per le misure congeneriche, si ha \\(a_{ij} \\neq 0\\) e \\(b_{ij} \\neq 1\\). Le misure congeneriche non sono soggette a nessuna delle restrizioni precedenti. Non è richiesto che abbiano varianze di errore, varianze di punteggio vero, varianze di punteggio osservato, covarianze di punteggio osservato, correlazioni di punteggio osservato o medie uguali tra di loro. Le misure congeneriche hanno quindi le ipotesi meno restrittive e, di conseguenza, possono differire maggiormente tra loro rispetto alle altre tipologie.\n\n# True scores for Test 4\nt4 &lt;- 2 + 0.8 * t1\n# Error scores for Test 4 (larger error SDs)\ne4 &lt;- rnorm(num_person, mean = 0, sd = 3)\n# Observed scores for Test 2\nx4 &lt;- t4 + e4\n\n\n# Merge into a data frame\ntest_df3 &lt;- data.frame(x1, x4)\n# Get means and variances\nmv &lt;- datasummary(x1 + x4 ~ Mean + Var,\n    data = test_df3,\n    output = \"data.frame\"\n)\nmv\n\n\nA data.frame: 2 x 3\n\n\n\nMean\nVar\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\nx1\n20.41\n29.20\n\n\nx4\n18.27\n24.23\n\n\n\n\n\nSe conosciamo i punteggi veri, la stima dell’affidabilità di x4 è:\n\n# Reliability for x4\nvar(t4) / var(x4)\n\n0.677398252481377\n\n\nNel caso di test congenerici, le medie e le varianze dei punteggi osservati sono diverse; la correlazione non è uguale all’affidabilità. Per distinguere test congenerici dai test essenzialmente \\(\\tau\\)-equivalenti sono necessari più di due test.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html#considerazioni-conclusive",
    "href": "chapters/ctt/01_ctt_1.html#considerazioni-conclusive",
    "title": "5  Fondamenti teorici",
    "section": "5.9 Considerazioni conclusive",
    "text": "5.9 Considerazioni conclusive\nIn conclusione, il presente capitolo ha fornito una panoramica dei concetti fondamentali della CTT e ha introdotto quattro tipologie di misure. Le misure parallele sono caratterizzate da una forte somiglianza tra i punteggi veri di tutte le misure, mentre le misure \\(\\tau\\)-equivalenti mostrano un’equivalenza nelle varianze dei punteggi veri. Le misure essenzialmente \\(\\tau\\)-equivalenti, invece, consentono una certa variabilità nei punteggi veri, mentre le misure congeneriche presentano la minore restrizione tra le quattro tipologie, consentendo differenze sia nelle medie che nelle varianze dei punteggi veri. Comprendere queste differenze tra i tipi di misure è essenziale per valutare l’affidabilità e la validità di un test, nonché per interpretare correttamente i risultati ottenuti. Nelle prossime sezioni del corso, approfondiremo ulteriormente questi concetti e affronteremo l’applicazione pratica della CTT nello sviluppo e nella valutazione dei test psicometrici. Per un’approfondimento più dettagliato su questi temi, si consiglia di consultare McDonald (2013) e Lord e Novick (1968).",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html#session-info",
    "href": "chapters/ctt/01_ctt_1.html#session-info",
    "title": "5  Fondamenti teorici",
    "section": "5.10 Session Info",
    "text": "5.10 Session Info\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] MASS_7.3-61        modelsummary_2.2.0 ggokabeito_0.1.0   viridis_0.6.5     \n [5] viridisLite_0.4.2  ggpubr_0.6.0       ggExtra_0.10.1     bayesplot_1.11.1  \n [9] gridExtra_2.3      patchwork_1.3.0    semTools_0.5-6     semPlot_1.1.6     \n[13] lavaan_0.6-18      psych_2.4.6.26     scales_1.3.0       markdown_1.13     \n[17] knitr_1.48         lubridate_1.9.3    forcats_1.0.0      stringr_1.5.1     \n[21] dplyr_1.1.4        purrr_1.0.2        readr_2.1.5        tidyr_1.3.1       \n[25] tibble_3.2.1       ggplot2_3.5.1      tidyverse_2.0.0    here_1.0.1        \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.16.0  jsonlite_1.8.9     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5.1 farver_2.1.2      \n  [7] nloptr_2.1.1       rmarkdown_2.28     vctrs_0.6.5       \n [10] minqa_1.2.8        base64enc_0.1-3    rstatix_0.7.2     \n [13] htmltools_0.5.8.1  broom_1.0.6        Formula_1.2-5     \n [16] htmlwidgets_1.6.4  plyr_1.8.9         sandwich_3.1-1    \n [19] emmeans_1.10.4     zoo_1.8-12         uuid_1.2-1        \n [22] igraph_2.0.3       mime_0.12          lifecycle_1.0.4   \n [25] pkgconfig_2.0.3    Matrix_1.7-0       R6_2.5.1          \n [28] fastmap_1.2.0      shiny_1.9.1        digest_0.6.37     \n [31] OpenMx_2.21.12     fdrtool_1.2.18     colorspace_2.1-1  \n [34] rprojroot_2.0.4    Hmisc_5.1-3        labeling_0.4.3    \n [37] fansi_1.0.6        timechange_0.3.0   abind_1.4-8       \n [40] compiler_4.4.1     withr_3.0.1        glasso_1.11       \n [43] htmlTable_2.4.3    backports_1.5.0    carData_3.0-5     \n [46] ggsignif_0.6.4     corpcor_1.6.10     gtools_3.9.5      \n [49] tools_4.4.1        pbivnorm_0.6.0     foreign_0.8-87    \n [52] zip_2.3.1          httpuv_1.6.15      nnet_7.3-19       \n [55] glue_1.7.0         quadprog_1.5-8     promises_1.3.0    \n [58] nlme_3.1-166       lisrelToR_0.3      grid_4.4.1        \n [61] pbdZMQ_0.3-13      checkmate_2.3.2    cluster_2.1.6     \n [64] reshape2_1.4.4     generics_0.1.3     gtable_0.3.5      \n [67] tzdb_0.4.0         data.table_1.16.0  hms_1.1.3         \n [70] car_3.1-2          utf8_1.2.4         tables_0.9.31     \n [73] sem_3.1-16         pillar_1.9.0       IRdisplay_1.1     \n [76] rockchalk_1.8.157  later_1.3.2        splines_4.4.1     \n [79] lattice_0.22-6     survival_3.7-0     kutils_1.73       \n [82] tidyselect_1.2.1   miniUI_0.1.1.1     pbapply_1.7-2     \n [85] stats4_4.4.1       xfun_0.47          qgraph_1.9.8      \n [88] arm_1.14-4         stringi_1.8.4      boot_1.3-31       \n [91] evaluate_1.0.0     codetools_0.2-20   mi_1.1            \n [94] cli_3.6.3          RcppParallel_5.1.9 IRkernel_1.3.2    \n [97] rpart_4.1.23       xtable_1.8-4       repr_1.1.7        \n[100] munsell_0.5.1      Rcpp_1.0.13        coda_0.19-4.1     \n[103] png_0.1-8          XML_3.99-0.17      parallel_4.4.1    \n[106] jpeg_0.1-10        lme4_1.1-35.5      mvtnorm_1.3-1     \n[109] insight_0.20.4     openxlsx_4.2.7.1   crayon_1.5.3      \n[112] rlang_1.1.4        multcomp_1.4-26    mnormt_2.1.1      \n\n\n\n\n\n\nAllen, Mary J, e Wendy M Yen. 2001. Introduction to measurement theory. Waveland Press.\n\n\nBollen, Kenneth, e Richard Lennox. 1991. «Conventional wisdom on measurement: A structural equation perspective.» Psychological bulletin 110 (2): 305–14.\n\n\nKline, Paul. 2013. Handbook of psychological testing. Routledge.\n\n\nLord, Frederic M, e Melvin R Novick. 1968. Statistical theories of mental test scores. Addison-Wesley.\n\n\nMcDonald, Roderick P. 2013. Test theory: A unified treatment. Psychology Press.\n\n\nNunnally, Jum C. 1994. Psychometric theory. McGraw-Hill.\n\n\nPetersen, Isaac T. 2024. Principles of psychological assessment: With applied examples in R. CRC Press.\n\n\nSpearman, C. 1904. «General intelligence objectively determined and measured». American Journal of Psychology 15: 201–93.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/02_ctt_2.html",
    "href": "chapters/ctt/02_ctt_2.html",
    "title": "6  L’affidabilità del test",
    "section": "",
    "text": "6.1 Introduzione\nL’affidabilità è un principio fondamentale nella teoria della misurazione, essenziale per garantire coerenza, stabilità e precisione nelle misurazioni effettuate in vari contesti. Nell’ambito del testing psicologico, è cruciale che i punteggi mostrino un grado di consistenza accettabile per essere considerati significativi. Questo concetto è particolarmente rilevante, poiché i punteggi possono variare a seconda delle specifiche condizioni di misurazione, rendendo necessario l’impiego di diversi metodi per valutare l’affidabilità di un test.\nEsploriamo i vari metodi utilizzati per valutare l’affidabilità:\nCentrali in queste misurazioni sono l’errore di misurazione e l’errore standard di misurazione. Quest’ultimo offre un modo per quantificare l’errore di misurazione. La teoria della generalizzabilità e la teoria della risposta all’item, entrambe moderne teorie dei test, forniscono strumenti sofisticati per analizzare e interpretare l’errore di misurazione, migliorando così la precisione dei test.\nIn questo capitolo, ci concentreremo sulla comprensione dell’affidabilità secondo la Teoria Classica dei Test (CTT), esaminando come misurare la consistenza e garantire l’accuratezza delle valutazioni psicologiche. Questa approfondita analisi dell’affidabilità è fondamentale per assicurare la validità e l’efficacia delle misurazioni psicologiche.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>L'affidabilità del test</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/02_ctt_2.html#introduzione",
    "href": "chapters/ctt/02_ctt_2.html#introduzione",
    "title": "6  L’affidabilità del test",
    "section": "",
    "text": "Affidabilità Test-Retest (Coefficiente di Stabilità): Questo metodo valuta la coerenza dei punteggi nel tempo. Si somministra lo stesso test a un gruppo di individui in due momenti distinti, e la correlazione tra i punteggi ottenuti fornisce un’indicazione dell’affidabilità. Tuttavia, la limitazione di questo metodo risiede nel fatto che variazioni nelle condizioni ambientali o psicologiche possono influenzare i risultati nel tempo.\nAffidabilità delle Forme Alternative (Coefficiente di Equivalenza): Utile quando esistono diverse versioni di un test, questo metodo serve a verificare se queste versioni producono punteggi simili e coerenti. È cruciale per assicurare che le diverse forme del test siano equivalenti in termini di difficoltà e contenuto.\nConsistenza Interna: Questo metodo misura il grado in cui gli elementi individuali di un test sono correlati tra loro. È particolarmente importante in test che includono molteplici item, dove la consistenza interna alta suggerisce che gli item misurano lo stesso costrutto in modo uniforme.\nAffidabilità Inter-Osservatori: Questo approccio valuta la coerenza tra le valutazioni di diversi osservatori. È fondamentale in situazioni dove più persone valutano lo stesso fenomeno, assicurando che i loro giudizi siano uniformi e affidabili.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>L'affidabilità del test</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/02_ctt_2.html#lattendibilità-del-test",
    "href": "chapters/ctt/02_ctt_2.html#lattendibilità-del-test",
    "title": "6  L’affidabilità del test",
    "section": "6.2 L’attendibilità del test",
    "text": "6.2 L’attendibilità del test\nPer definire l’attendibilità, la CTT si basa su due informazioni chiave:\n\nLa varianza dei punteggi osservati.\nLa correlazione tra il punteggio osservato e il punteggio vero.\n\nVedremo come ottenere queste informazioni utilizzando le assunzioni del modello statistico alla base della CTT.\n\n6.2.1 La varianza del punteggio osservato\nLa varianza del punteggio osservato \\(X\\) è uguale alla somma della varianza del punteggio vero e della varianza dell’errore di misurazione:\n\\[\n\\sigma^2_X =   \\sigma_T^2 + \\sigma_E^2.\n\\tag{6.1}\\]\nLa dimostrazione è la seguente. La varianza del punteggio osservato è uguale a\n\\[\n\\sigma^2_X =  \\mathbb{V}(T+E) =  \\sigma_T^2 + \\sigma_E^2 + 2 \\sigma_{TE}.\n\\tag{6.2}\\]\nDato che \\(\\sigma_{TE}=\\rho_{TE}\\sigma_T \\sigma_E=0\\), in quanto \\(\\rho_{TE}=0\\), ne segue che\n\\[\n\\sigma^2_X =   \\sigma_T^2 + \\sigma_E^2.\n\\]\nPer fare un esempio concreto, riprendiamo la simulazione del capitolo precedente.\n\nset.seed(8394)\n\nn &lt;- 100\nSigma &lt;- matrix(c(6, 0, 0, 3), byrow = TRUE, ncol = 2)\nmu &lt;- c(12, 0)\ndat &lt;- mvrnorm(n, mu, Sigma, empirical = TRUE)\nT &lt;- dat[, 1]\nE &lt;- dat[, 2]\nX &lt;- T + E\n\ntibble(X, T, E) |&gt; head()\n\n\nA tibble: 6 × 3\n\n\nX\nT\nE\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n15.698623\n16.765359\n-1.0667358\n\n\n13.657503\n12.248096\n1.4094073\n\n\n6.731979\n7.852136\n-1.1201563\n\n\n14.621813\n14.233699\n0.3881133\n\n\n10.606647\n10.187035\n0.4196115\n\n\n12.370288\n13.329971\n-0.9596831\n\n\n\n\n\n\nvar(X) == var(T) + var(E)\n\nTRUE\n\n\n\n\n6.2.2 La covarianza tra punteggio osservato e punteggio vero\nLa covarianza tra punteggio osservato \\(X\\) e punteggio vero \\(T\\) è uguale alla varianza del punteggio vero:\n\\[\n\\sigma_{X T} = \\sigma_T^2.\n\\tag{6.3}\\]\nLa dimostrazione è la seguente. La covarianza tra punteggio osservato e punteggio vero è uguale a\n\\[\n\\begin{aligned}\n\\sigma_{X T} &= \\mathbb{E}(XT) - \\mathbb{E}(X)\\mathbb{E}(T)\\notag\\\\\n&=  \\mathbb{E}[(T+E)T] - \\mathbb{E}(T+E)\\mathbb{E}(T)\\notag\\\\\n&=  \\mathbb{E}(T^2) + \\underbrace{\\mathbb{E}(ET)}_{=0} - [\\mathbb{E}(T)]^2 -  \\underbrace{\\mathbb{E}(E)}_{=0} \\mathbb{E}(T)\\notag\\\\\n&=\\mathbb{E}(T^2) - [\\mathbb{E}(T)]^2\\notag \\\\\n&= \\sigma_T^2.\n\\end{aligned}\n\\]\nVerifichiamo per i dati dell’esempio.\n\ncov(X, T) == var(T)\n\nTRUE\n\n\n\n\n6.2.3 Correlazione tra punteggio osservato e punteggio vero\nLa correlazione tra punteggio osservato \\(X\\) e punteggio vero \\(T\\) è uguale al rapporto tra la covarianza tra \\(X\\) e \\(T\\) divisa per il prodotto delle due deviazioni standard:\n\\[\n\\rho_{XT} = \\frac{\\sigma_{XT}}{\\sigma_X \\sigma_T} = \\frac{\\sigma^2_{T}}{\\sigma_X \\sigma_T} = \\frac{\\sigma_{T}}{\\sigma_X}.\n\\tag{6.4}\\]\nDunque, la correlazione tra il punteggio osservato e il punteggio vero è uguale al rapporto tra la deviazione standard dei punteggi veri e la deviazione standard dei punteggi osservati.\nVerifichiamo per i dati dell’esempio.\n\ncor(X, T) \n\n0.816496580927726\n\n\n\nsd(T) / sd(X)\n\n0.816496580927726\n\n\n\n\n6.2.4 Definizione e significato dell’attendibilità\nSulla base dell’Equazione 6.4, possiamo giungere alla definizione di attendibilità. La Teoria della Misurazione Classica (CTT) definisce l’attendibilità di un test (o di un singolo elemento) come il rapporto tra la varianza del punteggio vero e la varianza del punteggio osservato. In altre parole, l’attendibilità rappresenta il quadrato della correlazione tra il punteggio osservato \\(X\\) e il punteggio vero \\(T\\):\n\\[\n\\begin{equation}\n\\rho_{XT}^2 = \\frac{\\sigma_{T}^2}{\\sigma_{X}^2}.\n\\end{equation}\n\\]\nQuesta formula è il concetto fondamentale della CTT e misura il livello di variazione del punteggio vero rispetto alla variazione del punteggio osservato.\nAdesso possiamo procedere a verificare questa relazione utilizzando i dati forniti nell’esempio.\n\ncor(X, T)^2\n\n0.666666666666667\n\n\n\nvar(T) / var(X)\n\n0.666666666666667\n\n\nDato che \\(\\sigma^2_X = \\sigma_T^2 + \\sigma_E^2\\), in base alla {ref}eq-reliability-1 possiamo scrivere\n\\[\n\\begin{equation}\n\\rho_{XT}^2 = \\frac{\\sigma_{T}^2}{\\sigma_X^2} =\\frac{\\sigma_{X}^2 - \\sigma^2_E}{\\sigma_X^2} = 1-\\frac{\\sigma_{E}^2}{\\sigma_{X}^2}.\n\\end{equation}\n\\tag{6.5}\\]\n\n1 - (var(E) / var(X))\n\n0.666666666666667\n\n\nDall’Equazione 6.5, possiamo dedurre che il coefficiente di affidabilità assume il valore di \\(1\\) quando la varianza degli errori \\(\\sigma_{E}^2\\) è nulla, e assume il valore di \\(0\\) quando la varianza degli errori è uguale alla varianza del punteggio osservato. Quindi, il coefficiente di affidabilità è un valore assoluto situato nell’intervallo tra \\(0\\) e \\(1\\).",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>L'affidabilità del test</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/02_ctt_2.html#attendibilità-e-modello-di-regressione-lineare",
    "href": "chapters/ctt/02_ctt_2.html#attendibilità-e-modello-di-regressione-lineare",
    "title": "6  L’affidabilità del test",
    "section": "6.3 Attendibilità e modello di regressione lineare",
    "text": "6.3 Attendibilità e modello di regressione lineare\nIn parole semplici, la CTT si basa sul modello di regressione lineare, dove i punteggi osservati sono considerati come variabile dipendente e i punteggi veri come variabile indipendente. Il coefficiente di attendibilità \\(\\rho_{XT}^2\\) rappresenta la proporzione di varianza nella variabile dipendente spiegata dalla variabile indipendente in un modello di regressione lineare con una pendenza unitaria e un’intercetta di zero. In altre parole, il coefficiente di attendibilità è equivalente al coefficiente di determinazione del modello di regressione.\nPer rendere questo concetto più chiaro, possiamo tornare a considerare i dati simulati come esempio.\nLa motivazione di questa simulazione è quella di mettere in relazione il coefficiente di attendibilità, calcolato con la formula della CTT (come abbiamo fatto sopra), con il modello di regressione lineare. Analizziamo dunque i dati della simulazione mediante il seguente modello di regressione lineare:\n\\[\nX = a + b T + E.\n\\]\nUsando \\(\\textsf{R}\\) otteniamo:\n\nfm &lt;- lm(X ~ T)\nsummary(fm)\n\n\nCall:\nlm(formula = X ~ T)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.4343 -0.9720 -0.0865  1.0803  3.7347 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 9.948e-15  8.746e-01       0        1    \nT           1.000e+00  7.143e-02      14   &lt;2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.741 on 98 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6633 \nF-statistic:   196 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nSi noti che la retta di regressione ha intercetta 0 e pendenza 1. Questo è coerente con l’assunzione \\(\\mathbb{E}(X) = \\mathbb{E}(T)\\). Ma il risultato più importante di questa simulazione è che il coefficiente di determinazione (\\(R^2\\) = 0.67) del modello di regressione \\(X = 0 + 1 \\times T + E\\) è identico al coefficiente di attendibilità calcolato con la formula \\(\\rho_{XT}^2 = \\frac{\\sigma_{T}^2}{\\sigma_X^2}\\):\n\nvar(T) / var(X)\n\n0.666666666666667\n\n\nQuesti risultati ci permettono di interpretare il coefficiente di affidabilità nel seguente modo: l’affidabilità di un test rappresenta la porzione di varianza presente nel punteggio osservato \\(X\\) che viene spiegata dalla regressione di \\(X\\) rispetto al punteggio vero \\(T\\). Questo risultato è stato ottenuto mediante una regressione lineare, dove il coefficiente angolare \\(\\beta\\) è uguale a 1 e l’intercetta \\(\\alpha\\) è uguale a 0.\nInoltre, ricordiamo che la radice quadrata della varianza degli errori è l’errore standard della misurazione, \\(\\sigma_E\\). La quantità \\(\\sqrt{\\sigma_E^2}\\) fornisce una misura della dispersione del punteggio osservato attorno al valore vero, nella condizione ipotetica di ripetute somministrazioni del test:\n\nsqrt(var(E) * 99 / 98)\n\n1.74086537242199\n\n\nL’output della funzione lm() rende chiaro che l’errore standard della misurazione della CTT è identico all’errore standard della regressione nel caso di un modello di regressione definito come abbiamo fatto sopra.\nNel codice precedente è stato incluso il termine correttivo 99/98. Questa correzione è necessaria poiché, mentre R calcola la deviazione standard con \\(n-1\\) al denominatore, l’errore standard della regressione richiede \\(n-2\\) al denominatore.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>L'affidabilità del test</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/02_ctt_2.html#misurazioni-parallele-e-affidabilità",
    "href": "chapters/ctt/02_ctt_2.html#misurazioni-parallele-e-affidabilità",
    "title": "6  L’affidabilità del test",
    "section": "6.4 Misurazioni parallele e affidabilità",
    "text": "6.4 Misurazioni parallele e affidabilità\nL’equazione \\(\\rho_{XT}^2 = \\frac{\\sigma_{T}^2}{\\sigma_X^2}\\) definisce il coefficiente di affidabilità, ma non ci fornisce gli strumenti pratici per calcolarlo direttamente. Questo perché la varianza del punteggio reale \\(\\sigma_{T}^2\\) rappresenta un valore sconosciuto. Il metodo utilizzato dalla CTT per ottenere una stima empirica dell’attendibilità è quello delle forme parallele del test. In pratica, se è possibile creare versioni alternative del test che siano equivalenti in termini di contenuto, modalità di risposta e caratteristiche statistiche, allora diventa possibile ottenere una stima empirica del coefficiente di affidabilità.\nSecondo la CTT, due test \\(X=T+E\\) e \\(X^\\prime=T^\\prime+E^\\prime\\) sono considerati misurazioni parallele della stessa abilità latente quando:\n\n\\(T = T^\\prime\\),\n\\(\\mathbb{V}(E) = \\mathbb{V}(E^\\prime)\\).\n\nQueste premesse implicano che \\(\\mathbb{E}(X) = \\mathbb{E}(X^\\prime)\\).\nLa dimostrazione procede come segue. Considerando che \\(\\mathbb{E}(X) = T\\) e \\(\\mathbb{E}(X^\\prime) = T\\), è evidente che \\(\\mathbb{E}(X) =\\mathbb{E}(X^\\prime)\\) poiché \\(\\mathbb{E}(E) = \\mathbb{E}(E^\\prime) = 0\\).\nIn modo analogo, l’uguaglianza delle varianze nei punteggi osservati delle due misurazioni parallele deve essere verificata, cioè \\(\\mathbb{V}(X) = \\mathbb{V}(X^\\prime)\\).\nQuesta dimostrazione si sviluppa come segue. Per \\(X\\), possiamo scrivere\n\\[\\mathbb{V}(X) = \\mathbb{V}(T + E) = \\mathbb{V}(T) + \\mathbb{V}(E);\\]\nmentre per \\(X^\\prime\\) possiamo scrivere\n\\[\\mathbb{V}(X^\\prime) = \\mathbb{V}(T^\\prime + E^\\prime) = \\mathbb{V}(T^\\prime) + \\mathbb{V}(E^\\prime).\\]\nPoiché sappiamo che \\(\\mathbb{V}(E) = \\mathbb{V}(E^\\prime)\\) e che \\(T = T^\\prime\\), possiamo dedurre che \\(\\mathbb{V}(X) = \\mathbb{V}(X^\\prime)\\).\nIn aggiunta, è importante notare che per costruzione gli errori \\(E\\) e \\(E^\\prime\\) sono incorrelati sia con \\(T\\) che tra di loro.\n\n6.4.1 La correlazione tra due forme parallele del test\nOra procediamo a dimostrare che, secondo le ipotesi della Teoria della CTT, la correlazione tra due versioni parallele di un test è effettivamente equivalente al rapporto tra la varianza del punteggio reale e la varianza del punteggio osservato. Come discusso nel capitolo precedente, le misurazioni parallele rappresentano il grado più elevato di somiglianza tra due diverse versioni di un test.\nLa dimostrazione è la seguente. Consideriamo, senza perdita di generalità, che \\(\\mathbb{E}(X) = \\mathbb{E}(X') = \\mathbb{E}(T) = 0\\). Questa scelta ci consente di scrivere:\n\\[\n\\begin{aligned}\n\\rho_{X X^\\prime} &= \\frac{\\sigma(X, X^\\prime)}{\\sigma(X) \\sigma(X^\\prime)} \\\\\n&= \\frac{\\mathbb{E}(XX^\\prime)}{\\sigma(X) \\sigma(X^\\prime)} \\\\\n&= \\frac{\\mathbb{E}[(T+E)(T+E^\\prime)]}{\\sigma(X) \\sigma(X^\\prime)} \\\\\n&= \\frac{\\mathbb{E}(T^2) + \\mathbb{E}(TE^\\prime) + \\mathbb{E}(TE) + \\mathbb{E}(EE^\\prime)}{\\sigma(X) \\sigma(X^\\prime)}.\n\\end{aligned}\n\\]\nTuttavia, sappiamo che \\(\\mathbb{E}(TE) = \\mathbb{E}(TE^\\prime) = \\mathbb{E}(EE^\\prime) = 0\\). Inoltre, \\(\\sigma(X) = \\sigma(X^\\prime) = \\sigma_X\\). Pertanto, giungiamo a:\n\\[\n\\rho_{X X^\\prime} = \\frac{\\mathbb{E}(T^2)}{\\sigma_X \\sigma_X} = \\frac{\\sigma^2_T}{\\sigma^2_X}.\n\\] {#eq:3-3-5}\nNotiamo che il risultato ottenuto, insieme all’equazione che definisce il coefficiente di affidabilità \\(\\rho_{XT}^2 = \\frac{\\sigma_{T}^2}{\\sigma_X^2}\\), presentano entrambi la stessa espressione a destra del segno di uguale. Questo conduce a un risultato cruciale: il coefficiente di affidabilità, ossia il quadrato della correlazione tra il punteggio osservato e il punteggio reale, è identico alla correlazione tra i punteggi osservati di due versioni parallele del test:\n\\[\n\\rho^2_{XT} =  \\rho_{XX^\\prime}.\n\\tag{6.6}\\]\nQuesta conclusione è di notevole importanza in quanto consente di esprimere la variabile inosservabile \\(\\rho^2_{XT}\\) in termini della variabile osservabile \\(\\rho_{XX^\\prime}\\), la quale può essere calcolata in base ai punteggi osservati delle due forme parallele del test. Fondamentalmente, la stima di \\(\\rho^2_{XT}\\) si semplifica nella stima di \\(\\rho^2_{XX^\\prime}\\). Questo spiega l’importanza dell’equazione {eq}eq-rho2xt-rhoxx nella CTT. Inoltre, è da sottolineare che l’equazione {ref}eq:rho2xt-rhoxx fornisce una giustificazione per l’utilizzo della correlazione split-half come misura di affidabilità.\n\n\n6.4.2 La correlazione tra punteggio osservato e punteggio vero\nEsaminiamo adesso la correlazione tra il punteggio osservato e il punteggio reale. L’Equazione 6.6 può essere riformulata come segue:\n\\[\n\\rho_{XT} = \\sqrt{\\rho_{XX^\\prime}}.\n\\]\nIn altre parole, la radice quadrata del coefficiente di affidabilità equivale alla correlazione tra il punteggio osservato e il punteggio reale.\nProcediamo ora a verificare questa relazione utilizzando i dati dell’esempio.\n\nsqrt(var(T) / var(X))\n\n0.816496580927726\n\n\n\ncor(X, T)\n\n0.816496580927726\n\n\n\n\n6.4.3 I fattori che influenzano l’attendibilità\nConsiderando le tre equazioni:\n\\[\n\\rho^2_{XT} = \\rho_{XX'},\\quad\n\\rho_{XT}^2 = \\frac{\\sigma_{T}^2}{\\sigma_X^2}, \\quad\n\\rho_{XT}^2 = 1-\\frac{\\sigma_{E}^2}{\\sigma_X^2},\n\\]\npossiamo affermare che esistono tre modi equivalenti per giungere alla conclusione che l’attendibilità di un test è elevata. L’attendibilità di un test è considerata alta quando si verificano le seguenti condizioni:\n\nLa correlazione tra le forme parallele del test è elevata.\nLa varianza del punteggio vero è ampia rispetto alla varianza del punteggio osservato.\nLa varianza dell’errore di misurazione è ridotta rispetto alla varianza del punteggio osservato.\n\nQueste considerazioni rivestono un’importanza fondamentale nella progettazione di un test. In particolare, l’equazione \\(\\rho^2_{XT} = \\rho_{XX'}\\) fornisce un criterio per la selezione degli item da includere nel test. Se interpretiamo \\(\\rho_{XX'}\\) come la correlazione tra due item, allora gli item che presentano la correlazione più elevata tra di loro dovrebbero essere inclusi nel test. In questo modo, l’attendibilità del test aumenta, poiché gli item selezionati risultano fortemente correlati con il punteggio vero.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>L'affidabilità del test</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/02_ctt_2.html#conclusioni",
    "href": "chapters/ctt/02_ctt_2.html#conclusioni",
    "title": "6  L’affidabilità del test",
    "section": "6.5 Conclusioni",
    "text": "6.5 Conclusioni\nL’affidabilità costituisce un concetto fondamentale all’interno della teoria della misurazione, poiché si riferisce alla coerenza dei punteggi in varie situazioni, come diverse configurazioni di item, versioni del test o momenti di somministrazione. Nel corso di questo capitolo, abbiamo esplorato le basi teoriche dell’affidabilità. All’interno della CTT, l’affidabilità è definita come la correlazione tra il punteggio vero e il punteggio osservato, oppure, equivalentemente, come uno meno la correlazione tra il punteggio di errore e il punteggio osservato. Dal momento che il punteggio vero non è direttamente osservabile, è necessario ricorrere a metodi alternativi per stimare l’affidabilità. Il metodo proposto dalla CTT per ottenere tale stima è quello della correlazione dei punteggi ottenuti da due test paralleli.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>L'affidabilità del test</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/02_ctt_2.html#session-info",
    "href": "chapters/ctt/02_ctt_2.html#session-info",
    "title": "6  L’affidabilità del test",
    "section": "6.6 Session Info",
    "text": "6.6 Session Info\n\nsessionInfo()\n\nR version 4.3.2 (2023-10-31)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.3.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] MASS_7.3-60.0.1    modelsummary_1.4.5 ggokabeito_0.1.0   viridis_0.6.5     \n [5] viridisLite_0.4.2  ggpubr_0.6.0       ggExtra_0.10.1     bayesplot_1.11.1  \n [9] gridExtra_2.3      patchwork_1.2.0    semTools_0.5-6     semPlot_1.1.6     \n[13] lavaan_0.6-17      psych_2.4.1        scales_1.3.0       markdown_1.12     \n[17] knitr_1.45         lubridate_1.9.3    forcats_1.0.0      stringr_1.5.1     \n[21] dplyr_1.1.4        purrr_1.0.2        readr_2.1.5        tidyr_1.3.1       \n[25] tibble_3.2.1       ggplot2_3.4.4      tidyverse_2.0.0    here_1.0.1        \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.15.0  jsonlite_1.8.8     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5   nloptr_2.0.3      \n  [7] rmarkdown_2.25     vctrs_0.6.5        minqa_1.2.6       \n [10] base64enc_0.1-3    rstatix_0.7.2      htmltools_0.5.7   \n [13] broom_1.0.5        Formula_1.2-5      htmlwidgets_1.6.4 \n [16] plyr_1.8.9         sandwich_3.1-0     emmeans_1.10.0    \n [19] zoo_1.8-12         uuid_1.2-0         igraph_2.0.2      \n [22] mime_0.12          lifecycle_1.0.4    pkgconfig_2.0.3   \n [25] Matrix_1.6-5       R6_2.5.1           fastmap_1.1.1     \n [28] shiny_1.8.0        digest_0.6.34      OpenMx_2.21.11    \n [31] fdrtool_1.2.17     colorspace_2.1-0   rprojroot_2.0.4   \n [34] Hmisc_5.1-1        fansi_1.0.6        timechange_0.3.0  \n [37] abind_1.4-5        compiler_4.3.2     withr_3.0.0       \n [40] glasso_1.11        htmlTable_2.4.2    backports_1.4.1   \n [43] carData_3.0-5      ggsignif_0.6.4     corpcor_1.6.10    \n [46] gtools_3.9.5       tools_4.3.2        pbivnorm_0.6.0    \n [49] foreign_0.8-86     zip_2.3.1          httpuv_1.6.14     \n [52] nnet_7.3-19        glue_1.7.0         quadprog_1.5-8    \n [55] nlme_3.1-164       promises_1.2.1     lisrelToR_0.3     \n [58] grid_4.3.2         pbdZMQ_0.3-11      checkmate_2.3.1   \n [61] cluster_2.1.6      reshape2_1.4.4     generics_0.1.3    \n [64] gtable_0.3.4       tzdb_0.4.0         data.table_1.15.0 \n [67] hms_1.1.3          car_3.1-2          utf8_1.2.4        \n [70] tables_0.9.17      sem_3.1-15         pillar_1.9.0      \n [73] IRdisplay_1.1      rockchalk_1.8.157  later_1.3.2       \n [76] splines_4.3.2      lattice_0.22-5     survival_3.5-8    \n [79] kutils_1.73        tidyselect_1.2.0   miniUI_0.1.1.1    \n [82] pbapply_1.7-2      stats4_4.3.2       xfun_0.42         \n [85] qgraph_1.9.8       arm_1.13-1         stringi_1.8.3     \n [88] boot_1.3-29        evaluate_0.23      codetools_0.2-19  \n [91] mi_1.1             cli_3.6.2          RcppParallel_5.1.7\n [94] IRkernel_1.3.2     rpart_4.1.23       xtable_1.8-4      \n [97] repr_1.1.6         munsell_0.5.0      Rcpp_1.0.12       \n[100] coda_0.19-4.1      png_0.1-8          XML_3.99-0.16.1   \n[103] parallel_4.3.2     ellipsis_0.3.2     jpeg_0.1-10       \n[106] lme4_1.1-35.1      mvtnorm_1.2-4      insight_0.19.8    \n[109] openxlsx_4.2.5.2   crayon_1.5.2       rlang_1.1.3       \n[112] multcomp_1.4-25    mnormt_2.1.1",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>L'affidabilità del test</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/03_ctt_3.html",
    "href": "chapters/ctt/03_ctt_3.html",
    "title": "7  Metodi di stima dell’affidabilità",
    "section": "",
    "text": "7.1 Introduzione\nNel capitolo precedente abbiamo evidenziato come i punteggi dei test possano variare in diverse situazioni, ad esempio, in relazione alle differenze tra insiemi di item o alle variazioni tra diverse somministrazioni o valutazioni. A tal fine, sono stati sviluppati vari metodi per valutare l’affidabilità, o quanto i risultati del test siano stabili e coerenti, considerando diversi tipi di errori casuali.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Metodi di stima dell'affidabilità</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/03_ctt_3.html#la-definizione-di-attendibilità",
    "href": "chapters/ctt/03_ctt_3.html#la-definizione-di-attendibilità",
    "title": "7  Metodi di stima dell’affidabilità",
    "section": "7.2 La Definizione di Attendibilità",
    "text": "7.2 La Definizione di Attendibilità\nLa Teoria Classica dei Test (CTT) assume che il “punteggio osservato” \\(X\\) sia composto da due componenti: il “punteggio vero” \\(T\\) e l’errore di misurazione \\(E\\). Formalmente, questa relazione può essere espressa come \\(X = T + E\\), dove \\(E\\) rappresenta l’errore di misurazione che si sovrappone al “punteggio vero” \\(T\\).\nSecondo questa definizione, l’errore per una singola misurazione è dato dalla differenza tra il “punteggio osservato” e il “punteggio vero”, cioè \\(E = X - T\\). È importante notare che l’errore atteso è zero (\\(E(X) = 0\\)), e che non vi è alcuna correlazione o covarianza tra l’errore e il “punteggio vero” (\\(Cov(E,T) = 0\\)).\nCon queste premesse, l’affidabilità può essere definita come il quadrato della correlazione tra il “punteggio vero” e il “punteggio osservato”, ossia \\(\\rho^2_{XT}\\). In altre parole, rappresenta la frazione di varianza del “punteggio osservato” che è spiegata dal “punteggio vero”. Un’alta affidabilità (\\(\\rho^2_{XT} = 1\\)) indica che l’errore di misurazione è praticamente assente, mentre un valore inferiore indica la presenza di un errore significativo.\nUn’altra prospettiva per comprendere l’affidabilità è considerare la relazione tra le varianze del “punteggio osservato”, del “punteggio vero” e dell’errore. Secondo l’assunzione di indipendenza tra l’errore e il “punteggio vero”, le varianze osservate, vere e di errore sono correlate secondo l’equazione \\(\\sigma^2_X = \\sigma^2_T + \\sigma^2_E\\). Di conseguenza, \\(\\rho^2_{XT}\\) può essere interpretato come la proporzione di varianza del “punteggio osservato” spiegata dal “punteggio vero”, o come 1 meno il rapporto della varianza dell’errore rispetto alla varianza del “punteggio osservato”.\nIn sintesi, l’affidabilità di un test può essere concepita in diversi modi, riflettendo la sua relazione con il concetto di “punteggio vero” e l’errore di misurazione. La sfida successiva è quella di stimare l’affidabilità in modo accurato, tenendo conto di queste considerazioni.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Metodi di stima dell'affidabilità</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/03_ctt_3.html#approcci-per-stimare-laffidabilità",
    "href": "chapters/ctt/03_ctt_3.html#approcci-per-stimare-laffidabilità",
    "title": "7  Metodi di stima dell’affidabilità",
    "section": "7.3 Approcci per Stimare l’Affidabilità",
    "text": "7.3 Approcci per Stimare l’Affidabilità\nPer stimare l’affidabilità (\\(\\rho_{TT'}\\)), ci troviamo di fronte alla sfida di dover stimare una delle due componenti non direttamente osservabili: il punteggio vero o la varianza dell’errore. Ma come possiamo affrontare questa sfida? La risposta è complessa e dipende da come intendiamo concettualizzare la varianza dell’errore (\\(\\sigma^2_E\\)).\n\nAffidabilità delle Forme Parallele: Se il nostro interesse principale è misurare quanto accuratamente possiamo stimare il punteggio vero dai dati osservati, potrebbe essere più appropriato considerare \\(\\sigma^2_E\\) come l’incertezza nella nostra stima attraverso ripetute somministrazioni di una misura equivalente. Questo approccio ci porta alla definizione di affidabilità delle forme parallele.\nConsistenza Interna: Se invece vogliamo valutare se più elementi su una scala riflettono lo stesso costrutto sottostante, possiamo utilizzare un concetto simile all’Alpha di Cronbach (\\(\\alpha\\)). Questo ci porta alla definizione di affidabilità come consistenza interna.\nCoerenza Temporale (Affidabilità Test-Retest): Se ci interessa la coerenza di una misura nel tempo, allora \\(\\sigma^2_E\\) potrebbe essere meglio interpretato come la varianza non comune attraverso diverse somministrazioni della stessa misura su un periodo di tempo arbitrario. Questo concetto ci conduce alla definizione di coerenza temporale o affidabilità test-retest.\n\nIn sostanza, le equazioni dell’affidabilità presentate in precedenza possono essere applicate a ciascuno dei tre tipi di affidabilità descritti sopra. La differenza fondamentale risiede nella nostra concezione e nel calcolo di \\(\\sigma^2_E\\), che varia a seconda del contesto e degli obiettivi specifici dell’analisi.\n\n7.3.1 Affidabilità come Consistenza Interna\nIniziamo esaminando tre scenari distinti che illustrano le possibili relazioni tra gli item di un test: quelli con indicatori congenerici, tau-equivalenti e paralleli. Nell’ambito della CTT, sono disponibili due indicatori principali per valutare l’affidabilità in termini di coerenza interna, a seconda del tipo di relazione tra gli item presunta: l’indice alpha di Cronbach per gli item tau-equivalenti e l’indice di Spearman-Brown per gli item paralleli.\nOltre alla consistenza interna, esistono altre misure di affidabilità, tra cui la affidabilità test-retest, la affidabilità tra forme alternative, la affidabilità tra valutatori, la affidabilità dei punteggi compositi e la affidabilità dei punteggi delle differenze.\nAl centro della misurazione dell’affidabilità c’è l’errore di misurazione, e in precedenza abbiamo esaminato come lo standard error of measurement sia uno dei metodi per valutare l’errore di misurazione.\nVa notato che ci riferiamo all’affidabilità come una stima, poiché l’affidabilità assoluta o precisa dei risultati della valutazione non può essere conosciuta con certezza. Proprio come ci sono sempre degli errori nei punteggi dei test, ci sono anche degli errori nei nostri tentativi di misurare l’affidabilità. Tuttavia, i metodi di stima dell’affidabilità che discuteremo sono considerati stime conservative e rappresentano il limite inferiore della vera affidabilità dei punteggi dei test. In altre parole, l’affidabilità effettiva dei punteggi dei test è almeno altrettanto alta, se non superiore, rispetto all’affidabilità stimata (Reynolds, 1999).\n\n7.3.1.1 Coefficienti di consistenza interna\nLa CTT presenta il metodo delle forme parallele come un approccio parziale per stimare l’attendibilità dei test. Questo metodo prevede la somministrazione di due test distinti, indicati come \\(X\\) e \\(X^\\prime\\), che valutano lo stesso costrutto, a un campione di individui nello stesso momento. In questo contesto, la correlazione tra i punteggi totali dei due test, \\(\\rho^2_{XT} = \\rho_{XX^\\prime}\\), rappresenta l’indicatore principale dell’attendibilità. Tuttavia, è cruciale che le due versioni del test siano effettivamente parallele, secondo la definizione fornita dalla teoria classica dei test, affinché questa relazione sia valida.\nNella pratica, risulta impraticabile somministrare lo stesso test due volte agli stessi partecipanti “nelle stesse condizioni”, come richiesto dal metodo delle forme parallele. Di conseguenza, la stima dell’attendibilità deve basarsi sui dati raccolti attraverso una singola somministrazione del test. La CTT risponde a questa sfida introducendo specifici indicatori di coerenza interna, mirati a valutare l’affidabilità.\nQuesti indicatori di coerenza interna costituiscono la soluzione proposta dalla CTT per affrontare tale problematica. La loro logica si basa sull’idea che una correlazione tra i punteggi di diversi item che misurano lo stesso costrutto rifletta la varianza condivisa del punteggio reale, anziché la varianza condivisa dell’errore. Considerando che gli errori casuali dovrebbero mancare di una varianza condivisa, i coefficienti di coerenza interna riflettono la correlazione tra gli item all’interno del test, offrendo così un’indicazione dell’affidabilità generale della scala di misurazione.\nOltre a questo, gli item stessi possono rappresentare una fonte di errore nei punteggi dei test. Problemi come formulazioni confuse, item non coerenti con il costrutto, linguaggio poco comprensibile o item con risposte ambigue possono emergere quando gli item non sono formulati in modo adeguato. Tali problemi possono portare a risposte inconsistenti per due ragioni: innanzitutto, i partecipanti potrebbero reagire in modi diversi agli item problematici; in secondo luogo, tali item interferiscono con la capacità dei partecipanti di esprimere il loro reale livello del costrutto.\nPer valutare la coerenza delle risposte tra gli item all’interno di una scala, vengono impiegati i coefficienti di consistenza interna. Questi coefficienti si basano sull’assunto che una correlazione tra due punteggi osservati, che misurano lo stesso costrutto, rifletta la varianza condivisa del punteggio reale, non la varianza condivisa dell’errore. Dal momento che gli errori casuali dovrebbero mancare di varianza condivisa, i coefficienti di consistenza interna riflettono la correlazione tra gli item del test e forniscono un’indicazione dell’affidabilità complessiva della scala.\nQuando si valuta l’attendibilità con una singola somministrazione del test, sono disponibili vari approcci. In questo capitolo, esamineremo due metodi proposti dalla CTT: l’indice \\(\\alpha\\) di Cronbach e il metodo di Spearman-Brown. L’indice \\(\\alpha\\) è l’indicatore più comunemente usato per valutare l’attendibilità in termini di coerenza interna o omogeneità. Analizzeremo come questo indice rappresenta il valore minimo possibile dell’attendibilità di un test, sotto determinate ipotesi soddisfatte, e come, allo stesso tempo, può fornire una valutazione distorta dell’attendibilità se le assunzioni che delineeremo non sono rispettate.\nTuttavia, prima di esplorare dettagliatamente questi due diversi metodi di stima dell’attendibilità come coerenza interna, è essenziale distinguere tra tre diverse tipologie di relazioni tra gli item: item congenerici, item \\(\\tau\\)-equivalenti e item paralleli.\n\n\n7.3.1.2 Test paralleli\nSimuliamo i punteggi di due test paralleli.\n\nset.seed(2237) # setting the seed ensure reproducibility\nnum_person &lt;- 1000 # number of respondents\n# True scores for Test 1\nt1 &lt;- rnorm(num_person, mean = 20, sd = 5)\n# Error scores for Test 1\ne1 &lt;- rnorm(num_person, mean = 0, sd = 2)\n# Observed scores for Test 1\nx1 &lt;- t1 + e1\n# True scores for Test 2\nt2 &lt;- t1 # parallel tests have equal true scores\n# Error scores for Test 2\ne2 &lt;- rnorm(num_person, mean = 0, sd = 2)\n# Observed scores for Test 2\nx2 &lt;- t2 + e2\n\n# Merge into a data frame\ntest_df &lt;- data.frame(x1, x2)\n\nmv &lt;- datasummary(x1 + x2 ~ Mean + Var,\n    data = test_df,\n    output = \"data.frame\"\n)\nmv\n\n\nA data.frame: 2 x 3\n\n\n\nMean\nVar\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\nx1\n20.41\n29.20\n\n\nx2\n20.31\n30.27\n\n\n\n\n\n\n# Correlation\ncor(test_df) |&gt;\n    round(2)\n\n\nA matrix: 2 x 2 of type dbl\n\n\n\nx1\nx2\n\n\n\n\nx1\n1.00\n0.87\n\n\nx2\n0.87\n1.00\n\n\n\n\n\n\nvar(t1) / var(x1)\n\n0.878424313030747\n\n\n\nvar(t2) / var(x2)\n\n0.847351804948915\n\n\nIn conclusione, per test paralleli: - le medie e le varianze dei punteggi osservati sono statisticamente uguali; - la correlazione è uguale all’attendibilità.\n\n\n7.3.1.3 Test \\(\\tau\\)-equivalenti\n\n# True scores for Test 3\nt3 &lt;- 5 + t1 # essentially tau-equivalent tests\n# Error scores for Test 3 (larger error SDs)\ne3 &lt;- rnorm(num_person, mean = 0, sd = 4)\n# Observed scores for Test 2\nx3 &lt;- t3 + e3\n\n# Merge into a data frame\ntest_df2 &lt;- data.frame(x1, x3)\n# Get means and variances\nmv &lt;- datasummary(x1 + x3 ~ Mean + Var,\n    data = test_df2,\n    output = \"data.frame\"\n)\nmv\n\n\nA data.frame: 2 x 3\n\n\n\nMean\nVar\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\nx1\n20.41\n29.20\n\n\nx3\n25.41\n41.50\n\n\n\n\n\n\n# Correlation\ncor(test_df2) |&gt;\n    round(2)\n\n\nA matrix: 2 x 2 of type dbl\n\n\n\nx1\nx3\n\n\n\n\nx1\n1.00\n0.72\n\n\nx3\n0.72\n1.00\n\n\n\n\n\nSe conosciamo i punteggi veri, l’attendibilità di X3 si trova come\n\n# Reliability for x3\nvar(t3) / var(x3)\n\n0.618012243898734\n\n\nIn conclusione, per test tau-equivalenti: - le medie e le varianze dei punteggi osservati sono diverse; - correlazione \\(\\neq\\) attendibilità.\n\n\n7.3.1.4 Test congenerici\n\n# True scores for Test 4\nt4 &lt;- 2 + 0.8 * t1\n# Error scores for Test 4 (larger error SDs)\ne4 &lt;- rnorm(num_person, mean = 0, sd = 3)\n# Observed scores for Test 2\nx4 &lt;- t4 + e4\n\n# Merge into a data frame\ntest_df3 &lt;- data.frame(x1, x4)\n# Get means and variances\nmv &lt;- datasummary(x1 + x4 ~ Mean + Var,\n    data = test_df3,\n    output = \"data.frame\"\n)\nmv\n\n\nA data.frame: 2 x 3\n\n\n\nMean\nVar\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\nx1\n20.41\n29.20\n\n\nx4\n18.27\n24.23\n\n\n\n\n\n\n# Correlation\ncor(test_df3) |&gt;\n    round(2)\n\n\nA matrix: 2 x 2 of type dbl\n\n\n\nx1\nx4\n\n\n\n\nx1\n1.00\n0.73\n\n\nx4\n0.73\n1.00\n\n\n\n\n\nSe conosciamo i punteggi veri, l’attendibilità di X4 si trova come\n\n# Reliability for x4\nvar(t4) / var(x4)\n\n0.677398252481377\n\n\nIn conclusione, per test congenerici: - le medie e le varianze dei punteggi osservati sono diverse; - correlazione \\(\\neq\\) attendibilità; - sono necessari più di due test per distinguere test congenerici e test \\(\\tau\\)-equivalenti.\n\n\n7.3.1.5 Coefficiente \\(\\alpha\\) di Cronbach\nIl coefficiente \\(\\alpha\\) consente la stima dell’affidabilità nel contesto di indicatori \\(\\tau\\)-equivalenti. In queste circostanze, l’attendibilità viene valutata utilizzando l’equazione:\n\\[\n\\alpha = \\frac{{k}}{{k-1}} \\left( 1 - \\frac{{\\sum_{i=1}^{k} \\sigma_{X_i}^{2}}}{{\\sigma_{X}^{2}}} \\right)\n\\]\ndove: - \\(k\\) è il numero di item nel test, - \\(\\sigma_{i}^{2}\\) rappresenta la varianza del punteggio dell’item \\(i\\), - \\(\\sigma_{X}^{2}\\) è la varianza totale dei punteggi del test.\nUna derivazione della formula del coefficiente alpha di Cronbach è fornita nel capitolo {ref}reliability-fa-notebook.\nFu Guttman nel 1945 a scoprire questo coefficiente, anche se erroneamente attribuito a Cronbach. È spesso noto come coefficiente \\(\\alpha\\) di Guttman-Cronbach o G-C \\(\\alpha\\).\nQuando il modello di \\(\\tau\\)-equivalenza è applicabile, il coefficiente \\(\\alpha\\) costituisce un limite inferiore dell’affidabilità, in altri termini, il coefficiente \\(\\alpha\\) offre una stima prudente dell’affidabilità. Questa caratteristica è considerata uno dei principali vantaggi di questo indice. Tuttavia, è fondamentale notare che questa natura conservativa del coefficiente \\(\\alpha\\) vale solo se le ipotesi del modello \\(\\tau\\)-equivalente sono rispettate.\nIl coefficiente di attendibilità \\(\\alpha\\) è ampiamente utilizzato nell’ambito della psicometria. Tuttavia, come menzionato in precedenza, quando l’assunzione di \\(\\tau\\)-equivalenza non è valida, \\(\\alpha\\) può perdere la sua proprietà conservativa e sovrastimare l’attendibilità del test (Sijtsma, 2009). In tal caso, è necessario valutare attentamente l’adeguatezza dell’utilizzo del coefficiente \\(\\alpha\\) come indicatore di affidabilità.\nEsempio. Per illustrare la procedura di calcolo del coefficiente \\(\\alpha\\), useremo i dati bfi contenuti nel pacchetto psych. Il dataframe bfi comprende 25 item di autovalutazione della personalità. Sono riportati i dati di 2800 soggetti. Ci concentreremo qui sulla sottoscala Openness. - O1: Am full of ideas; - O2: Avoid difficult reading material; - O3: Carry the conversation to a higher level; - O4: Spend time reflecting on things; - O5: Will not probe deeply into a subject.\nLeggiamo i dati in R.\n\ndata(bfi, package = \"psych\")\nhead(bfi[c(\"O1\", \"O2\", \"O3\", \"O4\", \"O5\")])\n\n\nA data.frame: 6 x 5\n\n\n\nO1\nO2\nO3\nO4\nO5\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n61617\n3\n6\n3\n4\n3\n\n\n61618\n4\n2\n4\n3\n3\n\n\n61620\n4\n2\n5\n5\n2\n\n\n61621\n3\n3\n4\n3\n5\n\n\n61622\n3\n3\n4\n3\n3\n\n\n61623\n4\n3\n5\n6\n1\n\n\n\n\n\nEsaminiamo la correlazione tra gli item della sottoscale Openness.\n\ncor(bfi[c(\"O1\", \"O2\", \"O3\", \"O4\", \"O5\")], use = \"pairwise.complete.obs\") |&gt;\n    round(2)\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nO1\nO2\nO3\nO4\nO5\n\n\n\n\nO1\n1.00\n-0.21\n0.40\n0.18\n-0.24\n\n\nO2\n-0.21\n1.00\n-0.26\n-0.07\n0.32\n\n\nO3\n0.40\n-0.26\n1.00\n0.19\n-0.31\n\n\nO4\n0.18\n-0.07\n0.19\n1.00\n-0.18\n\n\nO5\n-0.24\n0.32\n-0.31\n-0.18\n1.00\n\n\n\n\n\nÈ necessario ricodificare due item.\n\nbfi$O2r &lt;- 7 - bfi$O2\nbfi$O5r &lt;- 7 - bfi$O5\n\n\ncor(bfi[c(\"O1\", \"O2r\", \"O3\", \"O4\", \"O5r\")], use = \"pairwise.complete.obs\") |&gt;\n    round(2)\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nO1\nO2r\nO3\nO4\nO5r\n\n\n\n\nO1\n1.00\n0.21\n0.40\n0.18\n0.24\n\n\nO2r\n0.21\n1.00\n0.26\n0.07\n0.32\n\n\nO3\n0.40\n0.26\n1.00\n0.19\n0.31\n\n\nO4\n0.18\n0.07\n0.19\n1.00\n0.18\n\n\nO5r\n0.24\n0.32\n0.31\n0.18\n1.00\n\n\n\n\n\nConsideriamo la matrice di varianze e covarianze della sottoscala Openness.\n\nC &lt;- cov(bfi[c(\"O1\", \"O2r\", \"O3\", \"O4\", \"O5r\")], use = \"pairwise.complete.obs\")\nC |&gt; round(2)\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nO1\nO2r\nO3\nO4\nO5r\n\n\n\n\nO1\n1.28\n0.38\n0.54\n0.25\n0.36\n\n\nO2r\n0.38\n2.45\n0.50\n0.13\n0.67\n\n\nO3\n0.54\n0.50\n1.49\n0.29\n0.50\n\n\nO4\n0.25\n0.13\n0.29\n1.49\n0.29\n\n\nO5r\n0.36\n0.67\n0.50\n0.29\n1.76\n\n\n\n\n\nCalcoliamo alpha:\n\np &lt;- 5\nalpha &lt;- (p / (p - 1)) * (1 - tr(C) / sum(C))\nalpha\n\n0.600172514820215\n\n\nLo stesso risultato si ottiene utilizzando la funzione alpha() contenuta nel pacchetto psych:\n\npsych::alpha(C)\n\n\nReliability analysis   \nCall: psych::alpha(x = C)\n\n  raw_alpha std.alpha G6(smc) average_r S/N median_r\n       0.6      0.61    0.57      0.24 1.5     0.23\n\n    95% confidence boundaries \n      lower alpha upper\nFeldt -0.49   0.6  0.95\n\n Reliability if an item is dropped:\n    raw_alpha std.alpha G6(smc) average_r S/N  var.r med.r\nO1       0.53      0.53    0.48      0.22 1.1 0.0092  0.23\nO2r      0.57      0.57    0.51      0.25 1.3 0.0076  0.22\nO3       0.50      0.50    0.44      0.20 1.0 0.0071  0.20\nO4       0.61      0.62    0.56      0.29 1.6 0.0044  0.29\nO5r      0.51      0.53    0.47      0.22 1.1 0.0115  0.20\n\n Item statistics \n       r r.cor r.drop\nO1  0.65  0.52   0.39\nO2r 0.60  0.43   0.33\nO3  0.69  0.59   0.45\nO4  0.52  0.29   0.22\nO5r 0.66  0.52   0.42\n\n\n\n\n7.3.1.6 Metodi alternativi per la stima del coefficiente di attendibilità\nCi sono altri coefficienti di consistenza interna oltre al coefficiente alpha di Cronbach. Alcuni esempi includono il coefficiente KR-20 e il coefficiente KR-21, che vengono utilizzati con item dicotomici (ossia con risposte a due alternative, come vero/falso).\n\n\n7.3.1.7 Coefficiente KR-20\nLa formula di Kuder-Richardson-20 (KR-20) è un caso particolare del coefficiente α. Se ogni item è dicotomico, il coefficiente α diventa il KR-20. Il coefficiente Coefficiente KR-20 si calcola con la formula:\n\\[\nKR\\_20 = \\frac{{k}}{{k-1}} \\left( 1 - \\frac{{p(1-p)}}{{\\sigma_{X}^{2}}} \\right)\n\\]\ndove: - \\(k\\) è il numero di item nel test, - \\(p\\) è la proporzione di individui che rispondono correttamente all’item, - \\(\\sigma_{X}^{2}\\) è la varianza totale dei punteggi del test.\nEsempio. Per fare un esempio, consideriamo il data-set LSAT contenuto nel pacchetto ltm.\n\nKR20 &lt;- function(responses) {\n    # Get number of items (N) and individuals\n    n.items &lt;- ncol(responses)\n    n.persons &lt;- nrow(responses)\n    # get p_j for each item\n    p &lt;- colMeans(responses)\n    # Get total scores (X)\n    x &lt;- rowSums(responses)\n    # observed score variance\n    var.x &lt;- var(x) * (n.persons - 1) / n.persons\n    # Apply KR-20 formula\n    rel &lt;- (n.items / (n.items - 1)) * (1 - sum(p * (1 - p)) / var.x)\n    return(rel)\n}\n\n\ndata(LSAT)\nhead(LSAT)\n\n\nA data.frame: 6 x 5\n\n\n\nItem 1\nItem 2\nItem 3\nItem 4\nItem 5\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n0\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n\n\n3\n0\n0\n0\n0\n0\n\n\n4\n0\n0\n0\n0\n1\n\n\n5\n0\n0\n0\n0\n1\n\n\n6\n0\n0\n0\n0\n1\n\n\n\n\n\n\nKR20(LSAT)\n\n0.294997192215955\n\n\n\n\n7.3.1.8 Coefficiente KR-21\nIl coefficiente Coefficiente KR-21 si calcola con la formula:\n\\[\nKR\\_21 = \\frac{{k}}{{k-1}} \\left( 1 - \\frac{{\\frac{{\\sum_{i=1}^{k} p_{i}(1-p_{i})}}{{\\sigma_{X}^{2}}}}}{{1 - \\frac{{\\sum_{i=1}^{k} p_{i}}}{k}}} \\right)\n\\]\ndove: - \\(k\\) è il numero di item nel test, - \\(p_{i}\\) è la proporzione di individui che rispondono correttamente all’item \\(i\\), - \\(\\sigma_{X}^{2}\\) è la varianza totale dei punteggi del test.\n\n\n7.3.1.9 La formula “profetica” di Spearman-Brown\nL’indice di Spearman-Brown stima l’attendibilità nel caso di \\(p\\) indicatori paralleli:\n\\[\n\\begin{equation}\n  \\rho_p = \\frac{p \\rho_1}{(p-1)\\rho_1 + 1},\n\\end{equation}\n\\] (eq-spearman-brown-der)\ndove \\(\\rho_1\\) rappresenta l’attendibilità di un singolo elemento.\nUna derivazione della formula Spearman-Brown è fornita nel capitolo {ref}reliability-fa-notebook.\nL’equazione {eq}eq-spearman-brown-der esprime l’attendibilità \\(\\rho_p\\) di un test composto da \\(p\\) elementi paralleli in termini dell’attendibilità di un singolo elemento. Questa equazione è universalmente riconosciuta come la formula “profetica” di Spearman-Brown (Spearman-Brown prophecy formula).\nPer fare un esempio concreto, poniamoci il problema di calcolare l’attendibilità della sottoscala Openness utilizzando la formula di Spearman-Brown. Ipotizziamo dunque che gli item della scala Openness siano paralleli. La matrice di correlazione è:\n\nR &lt;- cor(bfi[c(\"O1\", \"O2r\", \"O3\", \"O4\", \"O5r\")], use = \"pairwise.complete.obs\")\nR |&gt; round(2)\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nO1\nO2r\nO3\nO4\nO5r\n\n\n\n\nO1\n1.00\n0.21\n0.40\n0.18\n0.24\n\n\nO2r\n0.21\n1.00\n0.26\n0.07\n0.32\n\n\nO3\n0.40\n0.26\n1.00\n0.19\n0.31\n\n\nO4\n0.18\n0.07\n0.19\n1.00\n0.18\n\n\nO5r\n0.24\n0.32\n0.31\n0.18\n1.00\n\n\n\n\n\nSupponiamo di calcolare l’attendibilità di un singolo item (\\(\\rho_1\\)) come la correlazione media tra gli item:\n\nrr &lt;- NULL\np &lt;- 5\nk &lt;- 1\nfor (i in 1:p) {\n    for (j in 1:p) {\n        if (j != i) {\n            rr[k] &lt;- R[i, j]\n        }\n        k &lt;- k + 1\n    }\n}\nro_1 &lt;- mean(rr, na.rm = TRUE)\nro_1\n\n0.236538319550858\n\n\nApplicando la formula di Spearman-Brown, la stima dell’attendibilità del test diventa pari a\n\n(p * ro_1) / ((p - 1) * ro_1 + 1)\n\n0.607707322439719\n\n\n\n\n\n7.3.2 Forme parallele del test\nIn alcune situazioni, è possibile avere a disposizione diverse versioni di un test che sono progettate per essere interscambiabili, in modo tale che la specifica versione del test non influenzi i punteggi ottenuti dai partecipanti. Queste forme alternative del test sono comuni soprattutto nel campo dell’educazione, dove spesso vengono preparate diverse versioni al fine di prevenire frodi o imbrogli. Inoltre, anche i ricercatori possono adottare forme alternative in studi che coinvolgono pre-test e post-test, al fine di evitare che i partecipanti beneficiino degli effetti di pratica o memoria. Tuttavia, è di fondamentale importanza determinare se i punteggi ottenuti da queste diverse versioni sono coerenti, poiché la mancanza di equivalenza tra le forme potrebbe condurre a conclusioni errate riguardo alle variazioni dei punteggi.\nLe principali fonti di errore di misurazione per le forme alternative di test cognitivi derivano dalle differenze nei contenuti, nella difficoltà e nella complessità cognitiva degli item. Per quanto riguarda i test non-cognitivi, le differenze nei contenuti e nell’intensità degli item sono motivo di attenzione. Gli sviluppatori di forme alternative adottano diverse procedure al fine di garantire l’equivalenza tra le varie versioni, basandosi sulla stessa tabella di specifiche che stabilisce la proporzione di item per i diversi domini di contenuto e i livelli cognitivi o non-cognitivi. Inoltre, vengono appaiati gli item in base alla loro difficoltà e alla loro capacità discriminante.\nI coefficienti di equivalenza, noti anche come affidabilità delle forme alternative, valutano la similitudine tra due o più versioni di un test. Per calcolare questi coefficienti, le diverse forme vengono somministrate agli stessi partecipanti e i punteggi ottenuti vengono correlati. Tuttavia, vi sono alcune considerazioni legate alla somministrazione dei test e alla possibile fatica dei partecipanti. Al fine di affrontare tali problematiche, possono essere adottate strategie come il bilanciamento dell’ordine di somministrazione e l’introduzione di un breve intervallo di tempo tra le diverse versioni. Inoltre, è importante considerare gli effetti della pratica o della memoria, i quali potrebbero influenzare i punteggi ottenuti nel secondo test somministrato. L’impiego del bilanciamento tra gruppi può contribuire a controllare tali effetti.\n\n\n7.3.3 Attendibilità test-retest\nInfine, esaminiamo il concetto di “affidabilità test-retest”, che si riferisce alla coerenza o stabilità dei punteggi di un test in diverse occasioni nel corso del tempo. Questo tipo di affidabilità riveste una particolare importanza nelle situazioni in cui i punteggi vengono ottenuti in momenti diversi e confrontati, come nel caso di test effettuati prima e dopo un intervento. Inoltre, è di rilievo quando i punteggi del test vengono utilizzati per prendere decisioni diagnostiche, di selezione o di collocazione. Tuttavia, è importante sottolineare che l’affidabilità test-retest non è adatta per valutare costrutti che non sono noti per la loro stabilità nel tempo. Ciò deriva dal fatto che l’analisi della stabilità di un test potrebbe essere influenzata da effettivi cambiamenti nei livelli veri del costrutto tra i partecipanti. Di conseguenza, è essenziale che i ricercatori siano consapevoli in anticipo della stabilità del costrutto che intendono misurare. È importante notare che molti costrutti di interesse nelle scienze sociali sono generalmente considerati stabili nel tempo, come ad esempio la creatività, l’abilità cognitiva e alcune caratteristiche della personalità.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Metodi di stima dell'affidabilità</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/03_ctt_3.html#affidabilità-dei-punteggi-compositi",
    "href": "chapters/ctt/03_ctt_3.html#affidabilità-dei-punteggi-compositi",
    "title": "7  Metodi di stima dell’affidabilità",
    "section": "7.4 Affidabilità dei punteggi compositi",
    "text": "7.4 Affidabilità dei punteggi compositi\nL’affidabilità dei punteggi compositi si riferisce alla misura in cui più punteggi ottenuti da diverse fonti possono essere combinati per creare un punteggio complessivo. Ad esempio, nella valutazione educativa, la determinazione delle votazioni spesso si basa su un punteggio complessivo ottenuto da diverse prove e altre valutazioni somministrate durante un periodo di valutazione o un semestre. Molti test psicologici standardizzati includono diverse sottoscale che vengono combinate per formare un punteggio complessivo.\nIl vantaggio dei punteggi compositi è che la loro affidabilità è generalmente maggiore rispetto a quella dei punteggi individuali delle sottoscale (o item) che contribuiscono al punteggio composto. Più precisamente, l’affidabilità di un punteggio composto è il risultato del numero di punteggi inclusi nel composto, dell’affidabilità dei punteggi individuali e della correlazione tra questi punteggi. Più punteggi sono inclusi nel composto, più alta è la correlazione tra di essi e maggiore è l’affidabilità individuale, maggiore è l’affidabilità del composto. Come abbiamo notato in precedenza, i test rappresentano semplicemente dei campioni del dominio che si intende misurare, e la combinazione di misurazioni multiple è analoga all’aumento del numero di osservazioni o della dimensione del campione.\nPer fare un esempio, supponiamo di avere due variabili aleatorie, $ X $ e $ Y $, che rappresentano i punteggi di due subtest diversi. L’affidabilità (indicata come $ $) di un test è legata alla varianza del test stesso. Un modo per esprimere l’affidabilità è attraverso il rapporto tra la varianza del vero punteggio (quello che il test intende misurare) e la varianza totale del test. Supponendo che il vero punteggio e l’errore di misura siano indipendenti, la varianza totale del test è la somma della varianza del vero punteggio e della varianza dell’errore.\nQuando combiniamo più subtest in un punteggio composito, stiamo in effetti aumentando la varianza del vero punteggio (poiché stiamo combinando più misurazioni del costrutto che vogliamo misurare) mentre l’errore di misura, supposto indipendente tra i subtest, si somma meno che proporzionalmente.\nPer rendere queste affermazioni più concrete, consideriamo un esempio numerico nel quale supponiamo che i subtest siano correlati (il che è spesso il caso in psicometria, dove diversi subtest possono misurare aspetti correlati di un costrutto più ampio).\n\n7.4.1 Calcolo per il Puniteggio Composito\nPer esempio, dati due subtest con una varianza del vero punteggio di 25 ciascuno e una covarianza di 15 (dovuta al vero punteggio), la varianza del vero punteggio nel composito è data da:\n\\[ \\text{Var}(Z_{vero}) = 25 + 25 + 2 \\cdot 15 = 80 \\]\nLa varianza totale nel composito, tenendo conto anche della varianza dell’errore di misura, sarà:\n\\[ \\text{Var}(Z_{totale}) = 35 + 35 + 2 \\cdot 15 = 100 \\]\nIl rapporto tra la varianza del vero punteggio e la varianza totale nel composito è:\n\\[ \\text{Rapporto} = \\frac{\\text{Var}(Z_{vero})}{\\text{Var}(Z_{totale})} = \\frac{80}{100} = 0.8 \\]\n\n\n7.4.2 Confronto con un Singolo Subtest\nLa varianza del vero punteggio in un singolo subtest è data (come da ipotesi) da 25.\nLa varianza totale in un singolo subtest è la somma della varianza del vero punteggio e quella dell’errore di misura, quindi 35 (25 di vero punteggio + 10 di errore).\nIl rapporto tra la varianza del vero punteggio e la varianza totale in un singolo subtest è:\n\\[ \\text{Rapporto} = \\frac{\\text{Var}(X_{vero})}{\\text{Var}(X_{totale})} = \\frac{25}{35} \\approx 0.714 \\]\nIl confronto mostra che l’affidabilità del punteggio composito (0.8) è maggiore di quella di un singolo subtest (circa 0.714). Questo esemplifica come la correlazione positiva tra i subtest possa effettivamente aumentare l’affidabilità del punteggio composito rispetto ai subtest individuali.\nQuindi, il vantaggio di combinare i punteggi dai subtest in un punteggio composito emerge principalmente quando i subtest sono in qualche modo correlati e/o quando la varianza dell’errore di misura è ridotta rispetto alla varianza del vero punteggio. In pratica, l’uso di punteggi compositi è spesso giustificato dall’idea che essi forniscono una misura più completa e rappresentativa del costrutto di interesse, riducendo l’impatto dell’errore di misura specifico di ciascun subtest.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Metodi di stima dell'affidabilità</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/03_ctt_3.html#laffidabilità-dei-punteggi-differenza",
    "href": "chapters/ctt/03_ctt_3.html#laffidabilità-dei-punteggi-differenza",
    "title": "7  Metodi di stima dell’affidabilità",
    "section": "7.5 L’affidabilità dei Punteggi Differenza",
    "text": "7.5 L’affidabilità dei Punteggi Differenza\nCi sono numerose situazioni in cui ricercatori e clinici vogliono considerare la differenza tra due punteggi. Qui, la variabile di interesse è un punteggio differenza che viene calcolato come:\n\\[ D = X - Y, \\]\ndove X è il punteggio su un test e Y su un altro. Ad esempio, un approccio alla diagnosi delle difficoltà di apprendimento prevede il calcolo dei punteggi differenza sottraendo il punteggio di un esaminando in un test di rendimento (ad esempio, comprensione della lettura) dal suo QI. Si presume che se la discrepanza è negativa e sufficientemente ampia (ad esempio, due o più deviazioni standard), l’esaminando non sta dimostrando un rendimento accademico commisurato all’attitudine. Se ulteriori valutazioni escludono una serie di spiegazioni come opportunità educative inadeguate o problemi sensoriali (ad esempio, problemi visivi o uditivi), la discrepanza potrebbe riflettere una difficoltà di apprendimento intrinseca.\nUn altro esempio comune dell’utilizzo dei punteggi differenza si ha quando uno psicologo vuole considerare i guadagni (o le perdite) nella performance di un test nel tempo. Ad esempio, un ricercatore potrebbe voler determinare se un trattamento specifico ha portato a un miglioramento nelle prestazioni su un determinato compito. Ciò è spesso realizzato somministrando test prima e dopo l’intervento.\nIn queste situazioni, la variabile di interesse è un punteggio differenza. Quando si trattano punteggi differenza, è però importante ricordare che l’affidabilità dei punteggi differenza è tipicamente considerevolmente inferiore rispetto alle affidabilità dei punteggi individuali. Come regola generale, l’affidabilità dei punteggi differenza diminuisce all’aumentare della correlazione tra le misure individuali.\nLa formula per l’affidabilità dei punteggi differenza è data da:\n\\[\nr_{dd} = \\frac{0.5 (r_{xx} + r_{yy}) - r_{xy}}{1 - r_{xy}}\n\\],\ndove \\(r_{xx}\\) e \\(r_{yy}\\) sono le affidabilità delle due componenti della differenza e \\(r_{xy}\\) è la loro correlazione. Facciamo un esempio numerico varianza la correlazione tra le due componenti.\n\nrdd &lt;- function(rxx, ryy, rxy) {\n    (0.5 * (rxx + ryy) - rxy) / (1 - rxy)\n}\n\nseq(0.01, 0.81, by = 0.1)\n\n\n0.010.110.210.310.410.510.610.710.81\n\n\n\nrxx &lt;- 0.9\nryy &lt;- 0.8\n\nrdd(rxx, ryy, seq(0.01, 0.81, by = 0.1))\n\n\n0.8484848484848490.8314606741573040.8101265822784810.7826086956521740.7457627118644070.6938775510204080.6153846153846160.4827586206896550.210526315789474\n\n\nSi vede che, all’aumentare di \\(r_{xy}\\), l’affidabilità del punteggio differenza diminuisce.\nIn sintesi, si dovrebbe essere cauti nell’interpretare i punteggi differenza. L’affidabilità dei punteggi differenza è tipicamente considerevolmente inferiore rispetto alle affidabilità dei punteggi individuali. Per aggravare il problema, i punteggi differenza sono spesso calcolati utilizzando punteggi che hanno correlazioni piuttosto forti tra loro (ad esempio, punteggi di QI e di rendimento; punteggi pre e post test).",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Metodi di stima dell'affidabilità</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/03_ctt_3.html#scelta-del-coefficiente-di-affidabilità-in-funzione-del-contesto",
    "href": "chapters/ctt/03_ctt_3.html#scelta-del-coefficiente-di-affidabilità-in-funzione-del-contesto",
    "title": "7  Metodi di stima dell’affidabilità",
    "section": "7.6 Scelta del Coefficiente di Affidabilità in Funzione del Contesto",
    "text": "7.6 Scelta del Coefficiente di Affidabilità in Funzione del Contesto\nLa selezione di un coefficiente di affidabilità adeguato dipende da vari fattori, inclusa la natura del costrutto psicologico e l’utilizzo previsto dei punteggi del test. È essenziale considerare il contesto specifico in cui il test verrà impiegato per determinare l’indice di affidabilità più appropriato.\n\n7.6.1 Affidabilità Test-Retest\nL’affidabilità test-retest è rilevante quando un test viene somministrato ripetutamente agli stessi soggetti. Questo tipo di affidabilità misura la stabilità dei punteggi nel tempo, rendendola particolarmente utile per test che sono sensibili agli errori di misurazione legati al tempo. Ad esempio, se un test è impiegato per predire il comportamento futuro di un individuo, l’affidabilità test-retest fornisce una stima significativa dell’errore dovuto alla variabilità temporale.\n\n\n7.6.2 Affidabilità della Coerenza Interna\nQuando un test è previsto per essere somministrato una sola volta, è più pertinente considerare la coerenza interna. Ci sono due approcci principali:\n\nAffidabilità Split-Half: Questo metodo stima l’errore dovuto alla varianza del campionamento del contenuto. È adatto per test con contenuti eterogenei, dove l’eterogeneità è intenzionale. Ad esempio, in un test che valuta più costrutti psicologici (come depressione, ansia, rabbia, impulsività), l’approccio split-half può essere preferibile. Qui, si divide idealmente il test in due parti, con un numero equo di item per ogni tratto o caratteristica in ciascuna metà.\nCoefficienti Alfa e KR-20: Questi coefficienti stimano l’errore dovuto sia al campionamento del contenuto che all’eterogeneità di questo. Sono applicabili quando il test misura un’area di conoscenza omogenea o un singolo tratto. Per esempio, un test che valuta specificamente l’umore depressivo potrebbe avvalersi efficacemente del coefficiente alfa o del KR-20, poiché si focalizza su un dominio omogeneo.\n\n\n\n7.6.3 Affidabilità delle Forme Alternate\nSe esistono diverse forme di un test, è importante stimare l’affidabilità delle forme alternate. Questo approccio misura la consistenza dei punteggi tra diverse versioni del test, garantendo che le varie forme siano equivalenti e affidabili.\n\n\n7.6.4 Affidabilità Intervalutatori\nNel caso in cui il test richieda un giudizio soggettivo da parte dei valutatori, diventa cruciale considerare l’affidabilità intervalutatori. Questo tipo di affidabilità valuta la consistenza dei giudizi tra diversi valutatori, assicurando che le valutazioni siano obiettive e non dipendano significativamente dall’interpretazione individuale.\nIn sintesi, la scelta del coefficiente di affidabilità dipende dal contesto specifico del test, dalla natura del costrutto da misurare e dall’uso previsto dei risultati del test. È fondamentale valutare attentamente questi fattori per garantire la validità e l’accuratezza delle misurazioni psicologiche.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Metodi di stima dell'affidabilità</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/03_ctt_3.html#linee-guida-sulla-scelta-e-valutazione-dei-coefficienti-di-affidabilità",
    "href": "chapters/ctt/03_ctt_3.html#linee-guida-sulla-scelta-e-valutazione-dei-coefficienti-di-affidabilità",
    "title": "7  Metodi di stima dell’affidabilità",
    "section": "7.7 Linee Guida sulla Scelta e Valutazione dei Coefficienti di Affidabilità",
    "text": "7.7 Linee Guida sulla Scelta e Valutazione dei Coefficienti di Affidabilità\nLa scelta e la valutazione dei coefficienti di affidabilità in ambito psicometrico sono processi complessi influenzati da diversi fattori.\n\n7.7.1 Significato e Importanza dei Coefficienti di Affidabilità\nI coefficienti di affidabilità possono essere interpretati come la proporzione della varianza dei punteggi di un test attribuibile a differenze reali tra gli individui nel costrutto valutato. Idealmente, vorremmo che questi coefficienti raggiungessero il valore di 1.0, indicando che il 100% della varianza dei punteggi di test è dovuto a vere differenze tra gli individui. Tuttavia, a causa dell’errore di misurazione, una misura perfettamente affidabile non esiste. Non vi è una risposta univoca su quale sia un livello accettabile di affidabilità; ciò dipende da vari fattori come il costrutto misurato, il tempo disponibile per il test, l’uso dei punteggi e il metodo di stima dell’affidabilità.\n\n\n7.7.2 Fattori da Considerare nella Valutazione dell’Affidabilità\n\n7.7.2.1 Il Costrutto\nAlcuni costrutti sono più difficili da misurare rispetto ad altri a causa della complessità del dominio degli item. Ad esempio, le variabili di personalità sono generalmente più difficili da misurare rispetto alle abilità cognitive. Pertanto, un livello di affidabilità accettabile per un test sulla “dipendenza” potrebbe essere inadeguato per un test sull’intelligenza. È importante considerare la natura del costrutto e la sua difficoltà di misurazione.\n\n\n7.7.2.2 Tempo Disponibile per il Test\nSe il tempo disponibile per il test è limitato, ciò può influenzare l’affidabilità. Un numero limitato di item può aumentare l’errore nel campionamento del dominio del test. Ad esempio, test rapidi per lo screening dei problemi di lettura richiederanno standard di affidabilità diversi rispetto a test più lunghi, come quelli per l’intelligenza.\n\n\n7.7.2.3 Uso dei Punteggi del Test\nL’uso previsto dei punteggi del test è un altro fattore cruciale. Test diagnostici che influenzano decisioni importanti su un individuo richiedono standard di affidabilità più elevati rispetto ai test utilizzati per ricerche di gruppo o screening. Ad esempio, test sull’intelligenza utilizzati nella diagnosi di disabilità intellettuali richiedono un’alta affidabilità.\n\n\n7.7.2.4 Metodo di Stima dell’Affidabilità\nIl metodo scelto per stimare l’affidabilità può influenzare la grandezza dei coefficienti di affidabilità. Alcuni metodi, come KR-20 e il coefficiente alfa, tendono a produrre stime di affidabilità minori rispetto al metodo split-half. È importante considerare il metodo utilizzato quando si valutano e si confrontano l’affidabilità di diversi test.\n\n\n\n7.7.3 Linee Guida Generali per i Coefficienti di Affidabilità\nSebbene molti fattori meritino considerazione, possiamo fornire alcune linee guida generali:\n\nPer decisioni importanti che hanno un impatto significativo e non facilmente reversibile sugli individui, si dovrebbero aspettare coefficienti di affidabilità di 0.90 o addirittura 0.95.\nStime di affidabilità di 0.80 o superiori sono generalmente accettabili per molti test di rendimento e personalità.\nPer test creati da insegnanti o usati per lo screening, si aspettano stime di affidabilità di almeno 0.70.\nAlcuni suggeriscono che coefficienti di affidabilità fino a 0.60 possano essere accettabili per ricerche di gruppo, ma è consigliabile cautela nell’usare test con affidabilità al di sotto di 0.70.\n\nIn sintesi, la valutazione dell’affidabilità di un test psicometrico richiede un’attenta considerazione di diversi fattori chiave, e gli standard di accettabilità variano in base al contesto specifico del test e al suo uso previsto.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Metodi di stima dell'affidabilità</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/03_ctt_3.html#considerazioni-conclusive",
    "href": "chapters/ctt/03_ctt_3.html#considerazioni-conclusive",
    "title": "7  Metodi di stima dell’affidabilità",
    "section": "7.8 Considerazioni conclusive",
    "text": "7.8 Considerazioni conclusive\nIn conclusione, la valutazione dell’affidabilità di un test richiede l’impiego di diversi coefficienti che tengono conto delle varie fonti di errore. I coefficienti di consistenza interna si concentrano sull’errore derivante dalle fluttuazioni delle risposte tra gli item, mentre quelli di equivalenza esaminano la coerenza dei punteggi tra diverse versioni del test. I coefficienti di stabilità misurano la coerenza dei punteggi nel corso del tempo. È di fondamentale importanza selezionare il tipo di affidabilità appropriato in base allo scopo del test, al fine di ottenere informazioni affidabili e utili per le decisioni basate sui punteggi ottenuti dal test.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Metodi di stima dell'affidabilità</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/03_ctt_3.html#session-info",
    "href": "chapters/ctt/03_ctt_3.html#session-info",
    "title": "7  Metodi di stima dell’affidabilità",
    "section": "7.9 Session Info",
    "text": "7.9 Session Info\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ltm_1.2-0          polycor_0.8-1      msm_1.7.1          MASS_7.3-60.0.1   \n [5] modelsummary_1.4.5 ggokabeito_0.1.0   viridis_0.6.5      viridisLite_0.4.2 \n [9] ggpubr_0.6.0       ggExtra_0.10.1     bayesplot_1.11.1   gridExtra_2.3     \n[13] patchwork_1.2.0    semTools_0.5-6     semPlot_1.1.6      lavaan_0.6-17     \n[17] psych_2.4.1        scales_1.3.0       markdown_1.12      knitr_1.45        \n[21] lubridate_1.9.3    forcats_1.0.0      stringr_1.5.1      dplyr_1.1.4       \n[25] purrr_1.0.2        readr_2.1.5        tidyr_1.3.1        tibble_3.2.1      \n[29] ggplot2_3.5.0      tidyverse_2.0.0    here_1.0.1        \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.15.0  jsonlite_1.8.8     magrittr_2.0.3    \n  [4] nloptr_2.0.3       rmarkdown_2.26     vctrs_0.6.5       \n  [7] minqa_1.2.6        base64enc_0.1-3    rstatix_0.7.2     \n [10] htmltools_0.5.7    broom_1.0.5        Formula_1.2-5     \n [13] htmlwidgets_1.6.4  plyr_1.8.9         uuid_1.2-0        \n [16] admisc_0.35        igraph_2.0.2       mime_0.12         \n [19] lifecycle_1.0.4    pkgconfig_2.0.3    Matrix_1.6-5      \n [22] R6_2.5.1           fastmap_1.1.1      shiny_1.8.0       \n [25] digest_0.6.34      OpenMx_2.21.11     fdrtool_1.2.17    \n [28] colorspace_2.1-0   rprojroot_2.0.4    Hmisc_5.1-1       \n [31] fansi_1.0.6        timechange_0.3.0   abind_1.4-5       \n [34] compiler_4.3.3     withr_3.0.0        glasso_1.11       \n [37] htmlTable_2.4.2    backports_1.4.1    carData_3.0-5     \n [40] ggsignif_0.6.4     corpcor_1.6.10     gtools_3.9.5      \n [43] tools_4.3.3        pbivnorm_0.6.0     foreign_0.8-86    \n [46] zip_2.3.1          httpuv_1.6.14      nnet_7.3-19       \n [49] glue_1.7.0         quadprog_1.5-8     nlme_3.1-164      \n [52] promises_1.2.1     lisrelToR_0.3      grid_4.3.3        \n [55] pbdZMQ_0.3-11      checkmate_2.3.1    cluster_2.1.6     \n [58] reshape2_1.4.4     generics_0.1.3     gtable_0.3.4      \n [61] tzdb_0.4.0         data.table_1.15.2  hms_1.1.3         \n [64] car_3.1-2          utf8_1.2.4         tables_0.9.17     \n [67] sem_3.1-15         pillar_1.9.0       IRdisplay_1.1     \n [70] rockchalk_1.8.157  later_1.3.2        splines_4.3.3     \n [73] lattice_0.22-5     survival_3.5-8     kutils_1.73       \n [76] tidyselect_1.2.0   miniUI_0.1.1.1     pbapply_1.7-2     \n [79] stats4_4.3.3       xfun_0.42          expm_0.999-9      \n [82] qgraph_1.9.8       arm_1.13-1         stringi_1.8.3     \n [85] boot_1.3-29        evaluate_0.23      mi_1.1            \n [88] cli_3.6.2          RcppParallel_5.1.7 IRkernel_1.3.2    \n [91] rpart_4.1.23       xtable_1.8-4       repr_1.1.6        \n [94] munsell_0.5.0      Rcpp_1.0.12        coda_0.19-4.1     \n [97] png_0.1-8          XML_3.99-0.16.1    parallel_4.3.3    \n[100] ellipsis_0.3.2     jpeg_0.1-10        lme4_1.1-35.1     \n[103] mvtnorm_1.2-4      insight_0.19.8     openxlsx_4.2.5.2  \n[106] crayon_1.5.2       rlang_1.1.3        mnormt_2.1.1",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Metodi di stima dell'affidabilità</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/04_err_std_mis.html",
    "href": "chapters/ctt/04_err_std_mis.html",
    "title": "8  L’errore standard della misurazione",
    "section": "",
    "text": "8.1 Introduzione\nI coefficienti di affidabilità che abbiamo discusso nel capitolo precedente rappresentano una misura proporzionale della varianza osservata di un test che è attribuibile alla varianza reale. Questi coefficienti sono fondamentali per confrontare l’affidabilità dei punteggi ottenuti da diverse procedure di valutazione. In generale, preferiremo selezionare il test che produce i punteggi con la migliore affidabilità. Tuttavia, una volta scelto il test, il nostro focus si sposta sull’interpretazione dei punteggi.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>L'errore standard della misurazione</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/04_err_std_mis.html#errore-standard-di-misurazione-sem",
    "href": "chapters/ctt/04_err_std_mis.html#errore-standard-di-misurazione-sem",
    "title": "8  L’errore standard della misurazione",
    "section": "8.2 Errore Standard di Misurazione (SEM)",
    "text": "8.2 Errore Standard di Misurazione (SEM)\nL’Errore Standard di Misurazione (SEM) diventa una statistica più pratica quando l’attenzione è rivolta all’interpretazione dei punteggi di un test. Il SEM è definito come la deviazione standard della distribuzione dei punteggi che un individuo otterrebbe se fosse sottoposto a un numero infinito di forme parallele del test, costituite da item campionati casualmente dallo stesso dominio di contenuto.\nPer comprendere meglio, immaginiamo di creare un numero infinito di forme parallele di un test e di far svolgere queste forme alla stessa persona, senza che vi siano effetti di trasferimento. La presenza dell’errore di misurazione impedirebbe alla persona di ottenere sempre lo stesso punteggio. Anche se ogni test rappresenta ugualmente bene il dominio di contenuto, il candidato potrebbe ottenere risultati migliori in alcuni test e peggiori in altri, semplicemente a causa di errori casuali (ad esempio, la fortuna nel conoscere le risposte agli item selezionati per una versione del test ma non per un’altra). Prendendo i punteggi ottenuti in tutti questi test, si otterrebbe una distribuzione di punteggi. La media di questa distribuzione rappresenta il punteggio vero (T) dell’individuo, mentre il SEM è la deviazione standard di questa distribuzione di punteggi di errore.\nOvviamente, non è possibile attuare questi procedimenti nella realtà, quindi dobbiamo stimare il SEM utilizzando le informazioni disponibili. Esamineremo qui l’approccio utilizzato dalla Teoria Classica dei Test (CTT) per raggiungere questo obiettivo.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>L'errore standard della misurazione</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/04_err_std_mis.html#stima-di-sem",
    "href": "chapters/ctt/04_err_std_mis.html#stima-di-sem",
    "title": "8  L’errore standard della misurazione",
    "section": "8.3 Stima di SEM",
    "text": "8.3 Stima di SEM\nSecondo Lord (1968), l’errore \\(E = X - T\\) rappresenta la variabile aleatoria di interesse primario nella CTT. L’obiettivo della CTT è stimare il punteggio vero di ogni rispondente e confrontare le stime ottenute per rispondenti diversi. La grandezza dell’errore \\(E\\) fornisce informazioni essenziali in questo contesto. La discrepanza tra il punteggio osservato e il punteggio vero può essere misurata utilizzando la deviazione standard degli errori \\(E\\), conosciuta appunto come “Errore Standard della Misurazione” o SEM. Il SEM è quindi lo strumento impiegato dalla CTT per stimare in che misura un punteggio osservato differisce dal punteggio vero.\nNel presente capitolo esploreremo come sia possibile stimare la deviazione standard dell’errore (\\(\\sigma_E\\)) in un campione di osservazioni. Questo consente di comprendere meglio la precisione dei punteggi ottenuti attraverso un test psicometrico e di interpretare in modo più accurato i risultati.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>L'errore standard della misurazione</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/04_err_std_mis.html#lincertezza-della-misura",
    "href": "chapters/ctt/04_err_std_mis.html#lincertezza-della-misura",
    "title": "8  L’errore standard della misurazione",
    "section": "8.4 L’incertezza della misura",
    "text": "8.4 L’incertezza della misura\nIn base alla CTT, è possibile stimare l’errore standard della misurazione utilizzando una formula che dipende dalla deviazione standard della distribuzione dei punteggi del test e dall’attendibilità del test. Mediante questa formula, è possibile ottenere una stima dell’errore standard associato a un singolo punteggio, il quale indica quanto il punteggio osservato può variare rispetto al vero punteggio di un individuo:\n\\[\n\\sigma_E = \\sigma_X \\sqrt{1 -\\rho_{XX^\\prime}},\n\\] (eq-err-stnd-mis)\ndove \\(\\sigma_X\\) rappresenta la deviazione standard dei punteggi ottenuti da un campione di soggetti e \\(\\rho_{XX^\\prime}\\) è il coefficiente di attendibilità. Attraverso questo calcolo, si ottiene l’errore standard della misurazione sottraendo l’attendibilità del test da 1, quindi calcolando la radice quadrata del risultato e moltiplicandolo per la deviazione standard dei punteggi del test.\nLa logica alla base dell’errore standard della misurazione si fonda sull’assunzione che se una persona dovesse sostenere numerosi test equivalenti, i punteggi ottenuti seguirebbero una distribuzione normale con il vero punteggio dell’individuo come media. In altre parole, possiamo immaginare che l’individuo affronti ripetutamente versioni identiche del test, in circostanze simili e senza ricordare le risposte precedenti. In tale contesto ipotetico, l’errore standard della misurazione rappresenterebbe la deviazione standard tra queste misurazioni ripetute.\nLa formula sopra indicata evidenzia come l’errore standard della misurazione (\\(\\sigma_E\\)) sia strettamente correlato all’attendibilità del test: all’aumentare dell’attendibilità del test, l’errore standard della misurazione diminuisce. Se l’attendibilità del test si avvicina a 0, l’errore standard della misurazione tende a diventare uguale alla deviazione standard dei punteggi osservati del test. In contrasto, se l’attendibilità del test raggiunge 1, l’errore standard della misurazione si riduce a zero: in una situazione di perfetta affidabilità, in cui non vi è alcun errore di misurazione, \\(\\sigma_E\\) assume valore zero.\n\n8.4.1 Interpretazione\nLa Teoria Classica dei Test (CTT) postula che, se un individuo dovesse ripetere un test un numero infinito di volte, mantenendo inalterate le condizioni di somministrazione, i punteggi ottenuti si distribuirebbero in maniera normale attorno al suo vero punteggio. L’errore standard di misura (SEM) viene quindi definito come la stima della deviazione standard di questa distribuzione ipotetica di punteggi. Di conseguenza, un SEM elevato indica una maggiore incertezza nell’utilizzo del test per valutare l’abilità latente dell’individuo.\nSecondo McDonald, invece, il termine di errore (E) segue una distribuzione di propensione, che riflette le variazioni casuali nelle prestazioni di un individuo nel tempo a causa di test. Queste variazioni possono essere influenzate da fattori quali lo stato d’animo, la motivazione e altre variabili contestuali. L’errore standard di misura, in questo contesto, fornisce una quantificazione della deviazione standard dei punteggi attesi per un individuo, se fosse possibile testarlo un numero infinito di volte (o attraverso test equivalenti) in condizioni identiche, assumendo che il suo vero punteggio rimanga invariato.\nIl coefficiente di attendibilità, la varianza dell’errore e l’errore standard di misura rappresentano metriche che riflettono la precisione di un test psicometrico, ciascuna fornendo un tipo di insight specifico sulla precisione:\n\nL’errore standard di misura (SEM) offre una stima della precisione di un punteggio osservato per un individuo, offrendo una base per inferenze riguardo l’affidabilità di quel punteggio specifico. Al contrario, il coefficiente di attendibilità non si presta a una interpretazione così diretta in relazione ai punteggi individuali.\nIl SEM è calcolato nell’unità di misura dei punteggi del test, facilitando la comprensione e l’interpretazione della variabilità attorno al punteggio osservato di un individuo. Diversamente, la varianza dell’errore è espressa come il quadrato delle unità di misura del punteggio, rendendola meno intuitiva per interpretazioni dirette riguardanti la precisione del punteggio.\nIl coefficiente di attendibilità quantifica il rapporto tra la varianza dei punteggi veri e la varianza totale dei punteggi osservati, risultando in un indice senza unità di misura (adimensionale). Questo lo distingue dal SEM e dalla varianza dell’errore, in quanto l’attendibilità valuta la consistenza relativa dei punteggi all’interno dell’intero test piuttosto che la precisione di un singolo punteggio osservato.\n\nEsempio 1. Consideriamo un esempio in cui un test di intelligenza fornisce un punteggio medio di 100 con una deviazione standard di 15. Supponiamo inoltre che l’attendibilità di questo test sia pari a 0.73. Vogliamo calcolare l’errore standard della misurazione.\nUtilizzando la formula dell’errore standard della misurazione, otteniamo:\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\sigma_E &= \\sigma_X \\sqrt{1 -\\rho_{XX^\\prime}} \\notag\\\\\n&= 15 \\sqrt{1 - 0.73} \\notag\\\\\n&= 7.79.\\notag\n\\end{aligned}\n\\end{equation}\n\\]\nIl valore 7.79 rappresenta l’errore standard atteso nei punteggi ottenuti da un singolo individuo se il test fosse somministrato più volte sotto identiche condizioni. In altre parole, ci aspettiamo che i punteggi variino in media di circa 8 punti tra diverse somministrazioni del test.\nInoltre, possiamo utilizzare l’errore standard della misurazione per calcolare un intervallo di confidenza intorno al vero punteggio del rispondente. Utilizzando la proprietà della distribuzione gaussiana, possiamo stimare che il 95% dei punteggi ottenuti da ripetute somministrazioni del test si troveranno nell’intervallo:\n\\[\n\\text{punteggio vero del rispondente} \\pm 1.96 \\cdot \\text{errore standard della misurazione}.\n\\]\nNel nostro caso, questo intervallo sarebbe pari a \\(2 \\cdot 1.96 \\cdot 7.79 = 30.54\\) punti. Quindi, ci aspettiamo che i punteggi del QI di un singolo rispondente varino all’interno di un intervallo di 30 punti se il test fosse somministrato molte volte sotto le stesse condizioni.\nQuesto esempio dimostra che se un test ha un’attendibilità di 0.73 e una deviazione standard dei punteggi di 15, la misurazione del test su un singolo individuo risulterebbe poco affidabile a causa dell’ampio errore di misurazione. A titolo di confronto, la Full Scale IQ (FSIQ) della WAIS-IV {cite:p}wechsler2008wechsler ha un’attendibilità split-half di 0.98 e un errore standard di misurazione di 2.16.\nL’errore standard della misurazione può anche essere calcolato utilizzando la funzione SE.Means() del pacchetto psychometric.\n\nSE.Meas(15, .73)\n\n7.79422863405995\n\n\nEsempio 2. Continuando con l’esempio precedente, per gli ipotetici dati riportati sopra, poniamoci ora la seguente domanda: qual è la probabilità che un rispondente ottenga un punteggio minore o uguale a 116 nel test, se il suo punteggio vero fosse uguale a 120?\nIl problema si risolve rendendosi conto che i punteggi del rispondente si distribuiscono normalmente attorno al punteggio vero di 120, con una deviazione standard uguale a 7.79. Dobbiamo dunque trovare l’area sottesa alla normale \\(\\mathcal{N}(120, 7.79)\\) nell’intervallo \\([-\\infty, 116]\\). Utilizzando R, la soluzione si trova nel modo seguente:\n\npnorm(116, 120, 7.79)\n\n0.303808211691303\n\n\nSe la variabile aleatoria che corrisponde al punteggio osservato segue una distribuzione \\(\\mathcal{N}(120, 7.79)\\), la probabilità che il rispondente ottenga un punteggio minore o uguale a 116 è dunque uguale a 0.30.\nEsempio 3. Sempre per l’esempio discusso, poniamoci ora la seguente domanda: quale intervallo di valori centrato sul punteggio vero contiene, con una probabilità di 0.95, i punteggi che il rispondente otterrebbe in ipotetiche somministrazioni ripetute del test sotto le stesse identiche condizioni?\nDobbiamo trovare i quantili della distribuzione \\(\\mathcal{N}(120, 7.79)\\) a cui sono associate le probabilità di 0.025 e 0.975. La soluzione è data da:\n\nqnorm(c(.025, .975), 120, 7.79)\n\n\n104.731880560433135.268119439567\n\n\nL’intervallo cercato è dunque \\([104.7, 135.3]\\).\nEsempio 4. Calcoliamo ora l’errore standard di misurazione utilizzando un campione di dati grezzi. Esamineremo un set di dati discusso da {cite:t}brown2015confirmatory. Il set di dati grezzi contiene 9 indicatori utilizzati per misurare la depressione maggiore così come è definita nel DSM-IV:\n\nMDD1: depressed mood;\nMDD2: loss of interest in usual activities;\nMDD3: weight/appetite change;\nMDD4: sleep disturbance;\nMDD5: psychomotor agitation/retardation;\nMDD6: fatigue/loss of energy;\nMDD7: feelings of worthlessness/guilt;\nMDD8: concentration difficulties;\nMDD9: thoughts of death/suicidality.\n\nImportiamo i dati:\n\ndf &lt;- readRDS(\n    here::here(\"data\", \"mdd_sex.RDS\")\n) |&gt;\n    dplyr::select(-sex)\n\nCi sono 750 osservazioni:\n\ndim(df) |&gt; print()\n\n[1] 750   9\n\n\n\nhead(df)\n\n\nA data.frame: 6 x 9\n\n\n\nmdd1\nmdd2\nmdd3\nmdd4\nmdd5\nmdd6\nmdd7\nmdd8\nmdd9\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n5\n4\n1\n6\n5\n6\n5\n4\n2\n\n\n2\n5\n5\n5\n5\n4\n5\n4\n5\n4\n\n\n3\n4\n5\n4\n2\n6\n6\n0\n0\n0\n\n\n4\n5\n5\n3\n3\n5\n5\n6\n4\n0\n\n\n5\n5\n5\n0\n5\n0\n4\n6\n0\n0\n\n\n6\n6\n6\n4\n6\n4\n6\n5\n6\n2\n\n\n\n\n\nCalcoliamo il coefficiente di attendibilità \\(\\alpha\\) di Cronbach con la funzione alpha() del pacchetto psych.\n\nres &lt;- psych::alpha(df)\nalpha &lt;- res$total$raw_alpha\nalpha\n\n0.753150463775787\n\n\nCalcoliamo un vettore che contiene il punteggio totale del test per ciascun individuo:\n\ntotal_score &lt;- rowSums(df)\n\nTroviamo l’errore standard di misurazione:\n\nsd(total_score) * sqrt(1 - alpha)\n\n5.29643177867088\n\n\nConfrontiamo il risultato con quello ottenuto con la funzione SE.Meas():\n\nSE.Meas(sd(total_score), alpha)\n\n5.29643177867088",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>L'errore standard della misurazione</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/04_err_std_mis.html#dimostrazione",
    "href": "chapters/ctt/04_err_std_mis.html#dimostrazione",
    "title": "8  L’errore standard della misurazione",
    "section": "8.5 Dimostrazione",
    "text": "8.5 Dimostrazione\nEsaminiamo ora la derivazione della formula per l’errore standard di misurazione, \\(\\sigma_E = \\sigma_X \\sqrt{1 - \\rho_{XX^\\prime}}\\). Per arrivare a questa formula, seguiremo due passaggi chiave: innanzitutto, calcoleremo la varianza del punteggio vero e successivamente rappresenteremo il punteggio osservato come la somma della varianza del punteggio vero e la varianza dell’errore.\nIniziamo definendo il coefficiente di attendibilità come \\(\\rho_{XX^\\prime} = \\frac{\\sigma^2_T}{\\sigma^2_X}\\), in cui \\(\\sigma^2_T\\) è la varianza del punteggio vero e \\(\\sigma^2_X\\) è la varianza del punteggio osservato. Utilizzando questa definizione, possiamo riscrivere \\(\\sigma^2_T\\) come \\(\\sigma^2_T = \\rho_{XX^\\prime} \\sigma^2_X\\), considerando che \\(X\\) e \\(X^\\prime\\) sono forme parallele di un test.\nDato che \\(\\sigma_X = \\sigma_{X^\\prime}\\), possiamo scrivere l’equazione precedente come \\(\\sigma^2_T = \\rho_{XX^\\prime} \\sigma_X \\sigma_{X^\\prime}\\). Inoltre, la covarianza tra \\(X\\) e \\(X^\\prime\\) è definita come \\(\\sigma_{XX^\\prime} = \\rho_{XX^\\prime} \\sigma_X \\sigma_{X^\\prime}\\). Da qui, possiamo affermare che \\(\\sigma^2_T = \\sigma_{XX^\\prime}\\), ovvero che la varianza del punteggio vero equivale alla covarianza tra due misurazioni parallele.\nOra, passiamo a calcolare la varianza dell’errore, \\(\\sigma^2_E\\). La varianza del punteggio osservato è espressa come \\(\\sigma^2_X = \\sigma^2_T + \\sigma^2_E\\). Utilizzando la definizione di attendibilità, possiamo riscrivere questa equazione come \\(\\sigma^2_X = \\rho_{XX^\\prime} \\sigma^2_X + \\sigma^2_E\\), da cui otteniamo:\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\sigma^2_E &= \\sigma^2_X - \\sigma^2_X \\rho_{XX^\\prime} \\\\\n&= \\sigma^2_X (1 - \\rho_{XX^\\prime}).\n\\end{aligned}\n\\end{equation}\n\\]\nDi conseguenza, la varianza dell’errore di misurazione, \\(\\sigma^2_E\\), può essere espressa come il prodotto di due fattori: il primo rappresenta la varianza del punteggio osservato, mentre il secondo equivale a uno meno la correlazione tra le due forme parallele del test (\\(\\rho_{XX^\\prime}\\)). In conclusione, abbiamo calcolato l’incognita \\(\\sigma^2_E\\) in termini di due quantità osservabili, \\(\\sigma^2_X\\) e \\(\\rho_{XX^\\prime}\\).",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>L'errore standard della misurazione</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/04_err_std_mis.html#relazione-tra-affidabilità-e-sem",
    "href": "chapters/ctt/04_err_std_mis.html#relazione-tra-affidabilità-e-sem",
    "title": "8  L’errore standard della misurazione",
    "section": "8.6 Relazione tra Affidabilità e SEM",
    "text": "8.6 Relazione tra Affidabilità e SEM\nSi osserva che, all’aumentare dell’affidabilità di un test, l’Errore Standard di Misurazione (SEM) diminuisce. Questa relazione inversa è coerente con il fatto che il coefficiente di affidabilità riflette la proporzione della varianza dei punteggi osservati dovuta alla varianza dei punteggi veri, e il SEM è una stima dell’errore presente nei punteggi del test. Quindi, maggiore è l’affidabilità dei punteggi di un test, minore è il SEM, e maggior fiducia possiamo avere nella precisione dei punteggi del test. Viceversa, minore è l’affidabilità di un test, maggiore è il SEM, e minore è la nostra fiducia nella precisione dei punteggi del test.\nPer esempio, con un coefficiente di affidabilità perfetto pari a 1.0, il SEM sarebbe uguale a 0, indicando l’assenza di errore nella misurazione e che il punteggio ottenuto rappresenta il punteggio vero. Un coefficiente di affidabilità pari a 0, invece, produrrebbe un SEM uguale alla deviazione standard (SD) dei punteggi ottenuti, indicando che tutta la varianza dei punteggi del test è dovuta a errori.\nIl SEM è tradizionalmente utilizzato nel calcolo di intervalli o bande intorno ai punteggi osservati, all’interno dei quali ci si aspetta che cada il punteggio vero. Ora passeremo a questa applicazione del SEM.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>L'errore standard della misurazione</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/04_err_std_mis.html#intervallo-di-confidenza-e-errore-standard-di-misurazione-sem",
    "href": "chapters/ctt/04_err_std_mis.html#intervallo-di-confidenza-e-errore-standard-di-misurazione-sem",
    "title": "8  L’errore standard della misurazione",
    "section": "8.7 Intervallo di Confidenza e Errore Standard di Misurazione (SEM)",
    "text": "8.7 Intervallo di Confidenza e Errore Standard di Misurazione (SEM)\nL’intervallo di confidenza rappresenta un range di punteggi che include il vero punteggio di un individuo con una probabilità prescritta. Generalmente, utilizziamo il SEM per calcolare gli intervalli di confidenza. Il SEM fornisce informazioni sulla distribuzione dei punteggi osservati intorno ai punteggi veri.\nAd esempio, se un individuo ha un punteggio vero di 70 in un test con un SEM di 3, ci aspetteremmo che ottenga punteggi tra 67 e 73 due terzi delle volte, a patto che non ci siano cambiamenti nelle prestazioni a causa della ripetizione del test.\n\n# Definiamo il punteggio vero e il SEM\npunteggio_vero &lt;- 70\nSEM &lt;- 3\n\npnorm(73, 70, 3) - pnorm(67, 70, 3)\n\n0.682689492137086\n\n\nPer ottenere un intervallo di confidenza del 95%, determiniamo il numero di deviazioni standard che comprendono il 95% dei punteggi in una distribuzione. Con un punteggio vero di 70 e un SEM di 3, l’intervallo di confidenza del 95% sarebbe 70 ± 3(1.96), ovvero 70 ± 5.88. Quindi, in questa situazione, ci aspetteremmo che il punteggio osservato dell’individuo sia tra 64.12 e 75.88, il 95% delle volte.\n\n# Calcoliamo il valore critico Z per il livello di confidenza del 95%\nlivello_confidenza &lt;- 0.95\nz_critico &lt;- qnorm((1 + livello_confidenza) / 2)\n\n# Calcoliamo l'errore standard dell'intervallo\nerrore_standard_intervallo &lt;- SEM * z_critico\n\n# Calcoliamo l'intervallo di confidenza\nintervallo_confidenza_inf &lt;- punteggio_vero - errore_standard_intervallo\nintervallo_confidenza_sup &lt;- punteggio_vero + errore_standard_intervallo\n\n# Stampiamo l'intervallo di confidenza\ncat(\"L'intervallo di confidenza al 95% e' [\", intervallo_confidenza_inf, \", \", intervallo_confidenza_sup, \"]\\n\")\n\nL'intervallo di confidenza al 95% e' [ 64.12011 ,  75.87989 ]\n\n\n\n8.7.1 Relazione tra Affidabilità, SEM e Intervalli di Confidenza\nÈ utile notare la relazione tra l’affidabilità di un punteggio di test, il SEM e gli intervalli di confidenza. Ricordiamo che all’aumentare dell’affidabilità dei punteggi, il SEM diminuisce. La stessa relazione esiste tra l’affidabilità dei punteggi di test e gli intervalli di confidenza. Man mano che l’affidabilità dei punteggi di test aumenta (denotando meno errore di misurazione), gli intervalli di confidenza diventano più piccoli (denotando maggiore precisione nella misurazione).\n\n\n8.7.2 Vantaggio del SEM e dell’Uso degli Intervalli di Confidenza\nIl SEM e l’utilizzo degli intervalli di confidenza forniscono ci ricordano che l’errore di misurazione è un elemento intrinseco a tutti i punteggi e che dovremmo interpretare tali punteggi con cautela. Troppo spesso, si tende a interpretare un singolo punteggio numerico come se fosse assolutamente preciso, trascurando la presenza di errori associati.\nPer esempio, se si riporta che Alice ha un QI totale di 113, i suoi genitori potrebbero essere inclini a interpretare questo dato come un’indicazione precisa del QI di Alice, assumendo che sia esattamente 113. Tuttavia, anche quando si utilizzano test di alta qualità per misurare il QI, i punteggi ottenuti non sono privi di errore. Il SEM e gli intervalli di confidenza sono strumenti utili che ci consentono di quantificare e illustrare questa inevitabile incertezza associata ai punteggi di misurazione. Essi ci avvertono che ogni punteggio contiene una certa dose di errore e ci invitano a considerare i risultati con una visione più prudente e completa.\n\n\n8.7.3 Problema nel Calcolare l’Intervallo di Confidenza\nUn problema potenziale con l’approccio descritto sopra è che non conosciamo il vero punteggio dell’esaminato, ma solo il punteggio osservato. È comune usare il SEM per stabilire intervalli di confidenza intorno ai punteggi ottenuti. Tuttavia, è importante sottolineare che questa pratica non è corretta {cite:p}charter1996revisiting.\n\nIn spite of {cite:t}dudek1979continuing’s reminder that the SEM should not be used to construct confidence intervals, many test manuals, computer-scoring programs, and texts in psychology and education continue to do so. Because authors of many textbooks and manuals make these errors, it is understandable that those who learned from and look to these sources for guidance also make these errors. In summary, the SEM should not be used to construct confidence intervals for test scores (p. 1141).\n\nÈ invece possibile costruire gli intervalli di confidenza basati su punteggi veri stimati e sull’errore standard della stima (SEE). Questo approccio verrà descritto nel prossimo capitolo.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>L'errore standard della misurazione</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/04_err_std_mis.html#considerazioni-conclusive",
    "href": "chapters/ctt/04_err_std_mis.html#considerazioni-conclusive",
    "title": "8  L’errore standard della misurazione",
    "section": "8.8 Considerazioni conclusive",
    "text": "8.8 Considerazioni conclusive\nNel contesto della CTT, le stime di affidabilità si rivelano uno strumento fondamentale per valutare la coerenza dei test. Tuttavia, quando si affrontano decisioni relative al singolo individuo, come ad esempio determinare se un candidato supera un esame, diventa più vantaggioso fare riferimento all’errore standard di misurazione (SEM). Il SEM rende evidente quanto i punteggi di un test siano suscettibili di fluttuazioni casuali se lo stesso test venisse ripetuto più volte dallo stesso esaminando. In generale, un SEM più ridotto corrisponde a un intervallo di fluttuazioni casuali più stretto. Ciò implica che, grazie a un SEM più basso, i punteggi rifletteranno in modo più coerente le vere capacità dell’esaminando.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>L'errore standard della misurazione</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/04_err_std_mis.html#session-info",
    "href": "chapters/ctt/04_err_std_mis.html#session-info",
    "title": "8  L’errore standard della misurazione",
    "section": "8.9 Session Info",
    "text": "8.9 Session Info\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] psychometric_2.4  multilevel_2.7    MASS_7.3-60.0.1   nlme_3.1-164     \n [5] ggokabeito_0.1.0  viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n [9] ggExtra_0.10.1    bayesplot_1.11.1  gridExtra_2.3     patchwork_1.2.0  \n[13] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-17     psych_2.4.1      \n[17] scales_1.3.0      markdown_1.12     knitr_1.45        lubridate_1.9.3  \n[21] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[25] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.0    \n[29] tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.15.0  jsonlite_1.8.8     magrittr_2.0.3    \n  [4] nloptr_2.0.3       rmarkdown_2.26     vctrs_0.6.5       \n  [7] minqa_1.2.6        base64enc_0.1-3    rstatix_0.7.2     \n [10] htmltools_0.5.7    broom_1.0.5        Formula_1.2-5     \n [13] htmlwidgets_1.6.4  plyr_1.8.9         uuid_1.2-0        \n [16] igraph_2.0.2       mime_0.12          lifecycle_1.0.4   \n [19] pkgconfig_2.0.3    Matrix_1.6-5       R6_2.5.1          \n [22] fastmap_1.1.1      shiny_1.8.0        digest_0.6.34     \n [25] OpenMx_2.21.11     fdrtool_1.2.17     colorspace_2.1-0  \n [28] rprojroot_2.0.4    Hmisc_5.1-1        fansi_1.0.6       \n [31] timechange_0.3.0   abind_1.4-5        compiler_4.3.3    \n [34] withr_3.0.0        glasso_1.11        htmlTable_2.4.2   \n [37] backports_1.4.1    carData_3.0-5      ggsignif_0.6.4    \n [40] corpcor_1.6.10     gtools_3.9.5       tools_4.3.3       \n [43] pbivnorm_0.6.0     foreign_0.8-86     zip_2.3.1         \n [46] httpuv_1.6.14      nnet_7.3-19        glue_1.7.0        \n [49] quadprog_1.5-8     promises_1.2.1     lisrelToR_0.3     \n [52] grid_4.3.3         pbdZMQ_0.3-11      checkmate_2.3.1   \n [55] cluster_2.1.6      reshape2_1.4.4     generics_0.1.3    \n [58] gtable_0.3.4       tzdb_0.4.0         data.table_1.15.2 \n [61] hms_1.1.3          car_3.1-2          utf8_1.2.4        \n [64] sem_3.1-15         pillar_1.9.0       IRdisplay_1.1     \n [67] rockchalk_1.8.157  later_1.3.2        splines_4.3.3     \n [70] lattice_0.22-5     kutils_1.73        tidyselect_1.2.0  \n [73] miniUI_0.1.1.1     pbapply_1.7-2      stats4_4.3.3      \n [76] xfun_0.42          qgraph_1.9.8       arm_1.13-1        \n [79] stringi_1.8.3      boot_1.3-29        evaluate_0.23     \n [82] mi_1.1             cli_3.6.2          RcppParallel_5.1.7\n [85] IRkernel_1.3.2     rpart_4.1.23       xtable_1.8-4      \n [88] repr_1.1.6         munsell_0.5.0      Rcpp_1.0.12       \n [91] coda_0.19-4.1      png_0.1-8          XML_3.99-0.16.1   \n [94] parallel_4.3.3     ellipsis_0.3.2     jpeg_0.1-10       \n [97] lme4_1.1-35.1      openxlsx_4.2.5.2   crayon_1.5.2      \n[100] rlang_1.1.3        mnormt_2.1.1",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>L'errore standard della misurazione</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/05_err_std_stima.html",
    "href": "chapters/ctt/05_err_std_stima.html",
    "title": "9  La stima del punteggio vero",
    "section": "",
    "text": "9.1 Introduzione\nUno dei principali scopi della valutazione psicologica è stimare il punteggio vero del soggetto. Il punteggio osservato \\(X\\) differisce dal punteggio vero \\(T\\) a causa dell’errore di misurazione: \\(X = T + E\\). Ora poniamoci l’obiettivo di utilizzare i concetti della Teoria Classica per stimare il punteggio vero di un soggetto, utilizzando il suo punteggio osservato e l’affidabilità del test. Questa stima risulta particolarmente utile quando è necessario costruire un intervallo di confidenza per il punteggio vero del soggetto.\nPer costruire l’intervallo di confidenza del vero punteggio, sono necessarie due misurazioni:\nCominciamo affrontando il problema della stima del vero punteggio.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>La stima del punteggio vero</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/05_err_std_stima.html#introduzione",
    "href": "chapters/ctt/05_err_std_stima.html#introduzione",
    "title": "9  La stima del punteggio vero",
    "section": "",
    "text": "Una stima del vero punteggio.\nL’errore standard della stima (ossia, una stima della deviazione standard della distribuzione delle stime del punteggio vero che si otterrebbero se il test venisse somministrato infinite volte nelle stesse condizioni).",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>La stima del punteggio vero</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/05_err_std_stima.html#il-paradosso-di-kelley",
    "href": "chapters/ctt/05_err_std_stima.html#il-paradosso-di-kelley",
    "title": "9  La stima del punteggio vero",
    "section": "9.2 Il paradosso di Kelley",
    "text": "9.2 Il paradosso di Kelley\nNegli anni ’20, Kelly ha dimostrato come sia possibile stimare il punteggio vero del rispondente utilizzando un modello di regressione. La formula di Kelley si fonda sull’equivalenza algebrica che lega l’attendibilità al quadrato del coefficiente di correlazione tra i punteggi osservati e quelli veri. Pertanto, la stima del punteggio vero di un rispondente può essere calcolata nel seguente modo:\n\\[\n\\begin{equation}\n\\hat{T} = \\mu_x + \\rho  (X - \\mu_x),\n\\end{equation}\n\\] (eq-kelly)\ndove \\(X\\) rappresenta il punteggio osservato, \\(\\mu_x\\) è la media dei punteggi ottenuti da tutti i partecipanti nel campione, e \\(\\rho\\) è l’attendibilità del test.\nQuando l’attendibilità è perfetta (\\(\\rho = 1\\)), il punteggio vero coincide con il punteggio osservato. Nel caso di attendibilità nulla (dove tutta la varianza è attribuibile all’errore di misurazione), la stima più accurata del punteggio vero è semplicemente la media del campione. Per valori di \\(\\rho\\) compresi tra 0 e 1, la stima del punteggio vero si discosta dal punteggio osservato in direzione della media campionaria. In questo modo, la stima del punteggio vero illustra il concetto di regressione verso la media dei punteggi osservati, considerando l’attendibilità del test.\nLa formula del punteggio vero può essere interpretata nel modo seguente: per stimare il vero punteggio di un individuo, si parte dalla media della distribuzione di tutti i partecipanti e si procede in direzione del punteggio osservato. Tuttavia, il punteggio osservato non viene raggiunto completamente; l’entità dello spostamento è proporzionale all’attendibilità del test. Ciò implica che la stima del punteggio vero di un individuo, a seconda del valore di \\(\\rho\\), tiene conto anche della sua posizione rispetto alla media del gruppo. Se il soggetto si trova al di sotto della media, la stima del punteggio vero sarà maggiorata e viceversa. Questo fenomeno è noto come il “paradosso di Kelley”.\nÈ cruciale mettere in evidenza una discrepanza tra la formula di Kelley e l’intuizione comune che suggerisce il punteggio osservato come una stima accurata del vero punteggio (cioè \\(\\hat{T} = X\\)). Tuttavia, questo punto di vista è valido solo quando la misura è perfettamente attendibile (\\(\\rho = 1\\)). Al contrario, quando \\(\\rho = 0\\), la formula di Kelley suggerisce di utilizzare la media dei punteggi osservati come stima del vero punteggio, implicando che il punteggio osservato non rifletta necessariamente il vero punteggio, ma solo l’errore di misurazione.\nIn pratica, è estremamente improbabile che \\(\\rho\\) sia esattamente uguale a zero. Invece, con valori di \\(\\rho\\) compresi tra 0 e 1, la stima del punteggio vero si troverà in una posizione intermedia tra il punteggio osservato e la media della popolazione. Per una comprensione più dettagliata di questo concetto, possiamo fare riferimento a Kelley (1947), il quale ha osservato che:\n\nThis is an interesting equation in that it expresses the estimate of true ability as the weighted sum of two separate estimates, – one based upon the individual’s observed score, \\(X_1\\) (\\(X\\) nella notazione corrente) and the other based upon the mean of the group to which he belongs, \\(M_1\\) (\\(\\mu_x\\) nella notazione corrente). If the test is highly reliable, much weight is given to the test score and little to the group mean, and vice versa.\n\nPer chiarire l’eq. {eq}eq-kelly e la sua derivazione in termini di predizione del punteggio vero a partire dal punteggio osservato, iniziamo esaminando il modello di base di regressione lineare semplice. Questo modello stabilisce una relazione diretta tra il punteggio osservato \\(X\\) e il punteggio vero \\(T\\), una relazione che inizialmente abbiamo descritto con la formula \\(X = 0 + 1 \\cdot T + E\\). Il nostro interesse specifico qui, però, si sposta verso la predizione del punteggio vero \\(T\\) utilizzando il punteggio osservato \\(X\\), attraverso un modello di regressione. La formula per questa predizione assume la forma:\n\\[\nT = \\alpha + \\beta X + \\varepsilon.\n\\]\nRiorganizzando le variabili in termini di deviazioni dalla loro media (\\(x = X - \\bar{X}\\) e \\(\\tau = T - \\mathbb{E}(T)\\)), e considerando l’intercetta \\(\\alpha\\) come 0, il modello si semplifica in \\(\\hat{\\tau} = \\beta x\\), dove \\(\\hat{\\tau}\\) rappresenta la nostra stima del punteggio vero come deviazione dalla media. La questione centrale diventa quindi il calcolo di \\(\\beta\\), che è la pendenza della retta di regressione nel nostro modello semplificato.\nIl valore di \\(\\beta\\) viene definito come \\(\\beta = \\frac{\\sigma_{\\tau x}}{\\sigma^2_x}\\), che ci permette di esprimere il modello come:\n\\[\n\\hat{\\tau} = \\frac{\\sigma_{\\tau x}}{\\sigma^2_x} x.\n\\]\nIntroducendo la correlazione tra \\(x\\) (o \\(X\\)) e \\(\\tau\\) (o \\(T\\)), denotata \\(\\rho_{\\tau x}\\), e sostituendo la covarianza con il prodotto della correlazione per le deviazioni standard dei punteggi, riscriviamo l’equazione come:\n\\[\n\\hat{\\tau} = \\rho_{\\tau x}\\frac{\\sigma_{\\tau}}{\\sigma_x} x.\n\\]\nQuesto ci porta a considerare la definizione di attendibilità, secondo cui la varianza del punteggio vero può essere espressa come \\(\\sigma^2_{\\tau} = \\sigma^2_x \\rho_{xx^\\prime}\\). La stima del punteggio vero, in termini di deviazioni dalla media, diventa quindi una funzione del coefficiente di attendibilità e della deviazione del punteggio osservato dalla sua media:\n\\[\n\\hat{\\tau} = \\rho_{\\tau x} \\sqrt{\\rho_{xx^\\prime}} x.\n\\]\nAvendo dimostrato che \\(\\rho^2_{\\tau x} = \\rho_{xx^\\prime}\\), possiamo ulteriormente semplificare la nostra stima del punteggio vero come:\n\\[\n\\hat{\\tau} = \\rho_{xx^\\prime} x.\n\\]\nQuesto ci indica che la stima del punteggio vero (in termini di deviazioni dalla media) si ottiene moltiplicando il punteggio osservato, anch’esso espresso come deviazione dalla media, per il coefficiente di attendibilità.\nPer ritornare alla formula in termini di punteggi grezzi, aggiungiamo la media dei punteggi osservati \\(\\bar{X}\\) alla nostra equazione, ottenendo così la stima del punteggio vero grezzo \\(\\hat{T}\\):\n\\[\n\\hat{T} = \\rho_{XX^\\prime} (X - \\bar{X}) + \\bar{X}.\n\\]\nEspandendo e riorganizzando l’equazione, arriviamo a una forma che chiarisce la relazione tra la media dei punteggi osservati, il coefficiente di attendibilità, e il punteggio osservato grezzo:\n\\[\n\\hat{T} = \\bar{X} + \\rho_{XX^\\prime} (X - \\bar{X}).\n\\]\nQuesta equazione finale dimostra come la stima del punteggio vero grezzo possa essere calcolata regolando il punteggio osservato per la media dei punteggi e il coefficiente di attendibilità.\nNel contesto di dati campionari, dove il coefficiente di attendibilità popolazionale \\(\\rho_{XX^\\prime}\\) viene sostituito con il suo corrispettivo campionario \\(r_{XX^\\prime}\\), la formula diventa:\n\\[\n\\hat{T} = \\bar{X} + r_{XX^\\prime} (X - \\bar{X}),\n\\]\noffrendo un metodo pratico per stimare il punteggio vero di un individuo a partire dal suo punteggio osservato, con l’aggiunta della media dei punteggi osservati e il coefficiente di attendibilità campionario.\nEsercizio. Posto un coefficiente di attendibilità pari a 0.80 e una media del test pari a \\(\\bar{X} = 100\\), si trovi una stima del punteggio vero per un rispondente con un punteggio osservato uguale a \\(X\\) = 115.\nLa stima del punteggio vero \\(\\hat{T}\\) è uguale a\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{T} &= \\bar{X} + r_{XX^\\prime}  (X - \\bar{X})\\notag\\\\\n&= 100 + 0.80 \\cdot (115 - 100) = 112.\n\\end{aligned}\n\\end{equation}\n\\]\nIn alternativa, possiamo usare la funzione Est.true del pacchetto psychometric.\n\nEst.true(115, 100, .8)\n\n112",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>La stima del punteggio vero</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/05_err_std_stima.html#lerrore-standard-della-stima-nel-modello-di-regressione-di-kelley",
    "href": "chapters/ctt/05_err_std_stima.html#lerrore-standard-della-stima-nel-modello-di-regressione-di-kelley",
    "title": "9  La stima del punteggio vero",
    "section": "9.3 L’Errore Standard della Stima nel Modello di Regressione di Kelley",
    "text": "9.3 L’Errore Standard della Stima nel Modello di Regressione di Kelley\nNell’ambito del modello di regressione di Kelley, uno strumento fondamentale per valutare la precisione delle stime del punteggio vero ottenute dai punteggi osservati è l’errore standard della stima. Questo indice quantifica quanto le nostre stime del punteggio vero possano variare se ripetessimo il test più volte sotto le stesse condizioni. Denotiamo l’errore standard della stima con \\(\\sigma_{\\hat{T}}\\), dove \\(\\hat{T}\\) rappresenta la stima del valore vero.\n\n9.3.1 Importanza dell’Errore Standard della Stima\nL’errore standard della stima è cruciale per comprendere quanto sia affidabile la stima del punteggio vero. Un errore standard più piccolo indica una stima più precisa. Matematicamente, l’errore standard della stima si calcola come:\n\\[\n\\sigma_{\\hat{T}} = \\sigma_X \\sqrt{\\rho_{XX^\\prime} (1 -\\rho_{XX^\\prime})},\n\\] (eq-std-err-estimate)\ndove \\(\\sigma_X\\) è la deviazione standard dei punteggi osservati, e \\(\\rho_{XX^\\prime}\\) rappresenta il coefficiente di correlazione tra i punteggi osservati e i punteggi veri. Questa formula assume una distribuzione normale dei punteggi e una relazione lineare tra i punteggi osservati e i punteggi veri.\nPer dati campionari, utilizziamo una formula leggermente modificata per calcolare l’errore standard della stima:\n\\[\ns_{\\hat{T}} = s_X \\sqrt{r_{XX^\\prime} (1-r_{XX^\\prime})},\n\\]\nqui \\(s_X\\) indica la deviazione standard campionaria, e \\(r_{XX^\\prime}\\) è il coefficiente di affidabilità campionario.\n\n\n9.3.2 Derivazione dell’Errore Standard della Stima\nPer derivare l’equazione dell’errore standard di stima, {eq}eq-std-err-estimate, iniziamo con la definizione dell’errore \\(\\varepsilon\\), che rappresenta la discrepanza tra il punteggio reale \\(T\\) e il punteggio stimato \\(\\hat{T}\\), come illustrato nella formula:\n\\[\n\\varepsilon = T - \\hat{T}.\n\\]\nSi sottolinea la distinzione tra l’errore di misurazione, indicato con \\(E = X - T\\) (dove \\(E\\) quantifica l’errore di misurazione, ovvero la differenza tra il punteggio osservato \\(X\\) e il punteggio reale \\(T\\)), e l’errore \\(\\varepsilon = T - \\hat{T}\\) (che esprime la discrepanza tra il punteggio reale \\(T\\) e la sua stima \\(\\hat{T}\\)).\nAdottando la formula \\(\\hat{T} = \\bar{X} + \\rho_{XX^\\prime} (X - \\bar{X})\\) per esprimere \\(\\hat{T}\\), si può calcolare la varianza di \\(\\varepsilon\\) come segue:\n\\[\n\\begin{aligned}\n\\mathbb{V}(\\varepsilon) &=  \\mathbb{V}(T - \\hat{T}) \\\\\n&= \\mathbb{V}(T - \\bar{X} - \\rho_{XX^\\prime} X + \\rho_{XX^\\prime}\\bar{X}).\n\\end{aligned}\n\\]\nDato che l’aggiunta di una costante non altera la varianza di una variabile aleatoria, possiamo semplificare l’espressione a:\n\\[\n\\mathbb{V}(\\varepsilon) = \\mathbb{V}(T - \\rho_{XX^\\prime}X).\n\\]\nSfruttando la regola della varianza per la somma di variabili aleatorie, incluso il caso in cui una variabile è moltiplicata per una costante, arriviamo a:\n\\[\n\\begin{aligned}\n\\mathbb{V}(\\varepsilon) &= \\mathbb{V}(T) + \\rho_{XX^\\prime}^2 \\mathbb{V}(X) - 2 \\rho_{XX^\\prime} \\mbox{Cov}(X,T) \\\\\n&= \\sigma^2_T + \\rho_{XX^\\prime}^2 \\sigma^2_X - 2 \\rho_{XX^\\prime} \\sigma_{XT}.\n\\end{aligned}\n\\]\nCon \\(\\sigma_{XT} = \\sigma^2_T\\), possiamo ridurre ulteriormente l’espressione a:\n\\[\n\\sigma^2_{\\varepsilon} = \\sigma^2_T + \\left(\\frac{\\sigma_T^2}{\\sigma_X^2}\\right)^2 \\sigma^2_X - 2 \\frac{\\sigma_T^2}{\\sigma_X^2} \\sigma_{XT}.\n\\]\nSemplificando, otteniamo:\n\\[\n\\begin{aligned}\n\\sigma^2_{\\varepsilon} &= \\sigma^2_T + \\frac{\\sigma_T^4}{\\sigma_X^4} \\sigma^2_X - 2 \\frac{\\sigma_T^2}{\\sigma_X^2} \\sigma_{XT} \\\\\n&= \\sigma^2_T \\left(1 + \\frac{\\sigma_T^2}{\\sigma_X^2} - 2 \\frac{\\sigma_{XT}}{\\sigma_X^2}\\right).\n\\end{aligned}\n\\]\nInfine, con \\(\\sigma_{XT} = \\sigma^2_T\\), semplifichiamo a:\n\\[\n\\begin{aligned}\n\\sigma^2_{\\varepsilon} &= \\sigma^2_T \\left(1 - \\frac{\\sigma_{T}^2}{\\sigma_X^2}\\right).\n\\end{aligned}\n\\]\nQuesto ci porta alla formula dell’errore standard della stima \\(\\sigma_{\\varepsilon}\\):\n\\[\n\\begin{aligned}\n\\sigma_{\\varepsilon} &= \\sigma_T \\sqrt{1 - \\frac{\\sigma^2_T}{\\sigma^2_X}} \\\\\n&= \\sigma_T \\sqrt{\\frac{\\sigma^2_X - \\sigma^2_T}{\\sigma^2_X}} \\\\\n&= \\frac{\\sigma_T}{\\sigma_X} \\sqrt{\\sigma^2_X - \\sigma^2_T}.\n\\end{aligned}\n\\]\nConsiderando che \\(\\sigma^2_X = \\sigma^2_T + \\sigma^2_E\\), l’errore standard di stima diventa:\n\\[\n\\begin{aligned}\n\\sigma_{\\varepsilon} &= \\frac{\\sigma_T}{\\sigma_X} \\sqrt{\\sigma^2_E } \\\\\n&= \\frac{\\sigma_T}{\\sigma_X} \\sigma_E \\\\\n&= \\sqrt{\\rho_{XX^\\prime}} \\sigma_E.\n\\end{aligned}\n\\]\nDato che l’errore standard di misurazione è definito come \\(\\sigma_E = \\sigma_X \\sqrt{1 - \\rho_{XX^\\prime}}\\), possiamo concludere che:\n\\[\n\\begin{aligned}\n\\sigma_{\\varepsilon} &= \\sqrt{\\rho_{XX^\\prime}} \\sigma_E \\\\\n&= \\sqrt{\\rho_{XX^\\prime}} \\sigma_X \\sqrt{1-\\rho_{XX^\\prime}} \\\\\n&= \\sigma_X \\sqrt{\\rho_{XX^\\prime} (1 - \\rho_{XX^\\prime})}.\n\\end{aligned}\n\\]\nQuest’ultima espressione dimostra come l’errore standard della stima sia determinato dalla deviazione standard dei punteggi osservati, modulata dal coefficiente di correlazione tra i punteggi osservati e i punteggi veri.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>La stima del punteggio vero</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/05_err_std_stima.html#intervallo-di-confidenza-per-il-punteggio-vero",
    "href": "chapters/ctt/05_err_std_stima.html#intervallo-di-confidenza-per-il-punteggio-vero",
    "title": "9  La stima del punteggio vero",
    "section": "9.4 Intervallo di confidenza per il punteggio vero",
    "text": "9.4 Intervallo di confidenza per il punteggio vero\nSiamo ora finalmente nelle condizioni di potere calcolare l’intervallo di confidenza per il punteggio vero. Conoscendo l’errore standard della stima \\(\\sigma_{\\hat{T}}\\), l’intervallo di confidenza per il punteggio vero è dato da:\n\\[\n\\hat{T} \\pm z  \\sigma_{\\hat{T}},\n\\]\nladdove \\(\\hat{T}\\) è la stima del punteggio vero e \\(z\\) è il quantile della normale standardizzata al livello di probabilità desiderato. Se il campione è piccolo (minore di 30) è opportuno usare \\(t\\) anziché \\(z\\).\n\n9.4.1 Interpretazione\nNotiamo che l’intervallo \\(\\hat{T} \\pm z \\sigma_{\\hat{T}}\\) è centrato sulla stima puntuale del valore vero e la sua ampiezza dipende sia dal livello di confidenza desiderato (rappresentato dal quantile \\(z_{\\frac{\\alpha}{2}}\\)), sia dal grado di precisione dello stimatore, misurato dall’errore standard della stima, \\(\\sigma_{\\hat{T}} = \\sigma_X \\sqrt{\\rho_{XX^\\prime} (1 -\\rho_{XX^\\prime})}\\). È importante notare che l’errore standard della stima diventa sempre più grande man mano che diminuisce l’attendibilità \\(\\rho_{XX^\\prime}\\) del test.\nL’intervallo di confidenza indica quanto l’imprecisione della misura influisce sull’interpretazione dei dati. Più l’intervallo di confidenza è ampio, maggiore è l’incertezza nella valutazione dei risultati.\nEsercizio. {cite:t}charter1996revisiting ha esaminato l’effetto della variazione dell’attendibilità del test sull’ampiezza dell’intervallo di confidenza per il punteggio vero. Utilizzando come esempio i punteggi di QI (\\(\\mu\\) = 100, \\(\\sigma\\) = 15), Charter ha immaginato di variare il coefficiente di attendibilità del test utilizzato per la misurazione del QI. I valori presi in considerazione sono 0.55, 0.65, 0.75, 0.85 e 0.95. Ad esempio, supponiamo di avere un punteggio osservato pari a QI = 120 e un coefficiente di attendibilità del test \\(\\rho_{xx^\\prime}\\) pari a 0.65. In tali circostanze, la stima del punteggio vero è pari a\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{T} &= \\bar{X} + r_{XX^\\prime}  (X - \\bar{X}) \\notag\\\\\n&= 100 + 0.65 (120 - 100)\\notag\\\\\n&= 113.\\notag\n\\end{aligned}\n\\end{equation}\n\\]\nL’errore standard della stima è uguale a\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\sigma_{\\hat{T}} &= \\sigma_{X} \\sqrt{r_{XX^\\prime} (1 - r_{XX^\\prime})} \\notag\\\\\n&= 15 \\sqrt{0.65 (1 - 0.65)}\\notag\\\\\n&= 7.15.\\notag\n\\end{aligned}\n\\end{equation}\n\\]\nL’intervallo di confidenza al 95% per la stima del punteggio vero diventa pertanto uguale a\n\\[\n113 \\pm 1.96 \\cdot 7.15 = [98.98, 127.02].\n\\]\nSi noti che si può calcolare l’errore standard della stima con la funzione SE.Est() del pacchetto psychometric.\n\nSE.Est(15, .65)\n\n7.15454401062709\n\n\nInoltre, la funzione CI.tscore() restituisce sia la stima del punteggio vero sia l’intervallo di fiducia al livello desiderato di significatività.\n\nCI.tscore(120, 100, 15, 0.65, level = 0.95)\n\n\nA data.frame: 1 x 4\n\n\nSE.Est\nLCL\nT.Score\nUCL\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n7.154544\n98.97735\n113\n127.0226",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>La stima del punteggio vero</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/05_err_std_stima.html#cut-off",
    "href": "chapters/ctt/05_err_std_stima.html#cut-off",
    "title": "9  La stima del punteggio vero",
    "section": "9.5 Cut-off",
    "text": "9.5 Cut-off\nGli intervalli di confidenza per il punteggio vero possono essere utilizzati per confrontare i limiti dell’intervallo con un cut-off. Ci sono tre possibili esiti: il limite inferiore dell’intervallo di confidenza è maggiore del cut-off, il limite superiore dell’intervallo è minore del cut-off, o il valore del cut-off è compreso all’interno dell’intervallo. Nel primo caso, lo psicologo può affermare, con un grado di certezza \\(1-\\alpha\\), che il valore vero del rispondente è superiore al cut-off. Nel secondo caso, lo psicologo può affermare, con un grado di certezza \\(1-\\alpha\\), che il valore vero del rispondente è inferiore al cut-off. Nel terzo caso, lo psicologo non può concludere né che il valore vero sia inferiore né che sia superiore al cut-off, con un certo grado di incertezza.\n\nSi considerino i punteggi del QI, per cui \\(\\bar{X}\\) = 100 e \\(s_X\\) = 15. Sia l’attendibilità del test \\(\\rho_{XX^\\prime}\\) = 0.95. Supponiamo che il rispondente abbia un QI = 130. Poniamo che il cut-off per ammettere il rispondente ad un corso avanzato sia 120. Ci sono tre alternative: il valore vero del rispondente è sicuramente maggiore di 120; il valore vero del rispondente è sicuramente inferiore di 120; le evidenze disponibili ci lasciano in dubbio se il punteggio vero sia maggiore o minore di 120. Svolgiamo i calcoli per trovare l’intervallo di confidenza al livello di certezza del 95%:\n\nxm &lt;- 100\nsx &lt;- 15\nrho &lt;- .95\nx &lt;- 130\nt.hat &lt;- xm + rho * (x - xm)\nt.hat\nse.t &lt;- sx * sqrt(rho * (1 - rho))\nse.t\nt.hat + c(1, -1) * qnorm(.025, 0, 1) * se.t\n\n128.5\n\n\n3.26917420765551\n\n\n\n122.092536293808134.907463706192\n\n\nDato che il limite inferiore dell’intervallo di confidenza è maggiore del cut-off, lo psicologo conclude che il punteggio vero del rispondente è maggiore di 120. Quindi, raccomanda che il rispondente sia ammesso al corso avanzato.\nContinuando con l’esempio precedente, supponiamo che l’attendibilità del test abbia un valore simile a quello che solitamente si ottiene empiricamente, ovvero 0.80.\n\nxm &lt;- 100\nsx &lt;- 15\nrho &lt;- .8\nx &lt;- 130\nt.hat &lt;- xm + rho * (x - xm)\nt.hat\nse.t &lt;- sx * sqrt(rho * (1 - rho))\nse.t\nt.hat + c(1, -1) * qnorm(.025, 0, 1) * se.t\n\n124\n\n\n6\n\n\n\n112.24021609276135.75978390724\n\n\nIn questo secondo esempio, l’intervallo di confidenza al 95% è \\([112.24,\n135.76]\\) e contiene il valore del cut-off. Dunque, la decisione dello psicologo è che non vi sono evidenze sufficienti che il vero valore del rispondente sia superiore al cut-off. Si noti come la diminuzione dell’attendibilità del test porta all’aumento delle dimensioni dell’intervallo di confidenza.\n\n9.6 Conclusioni\nLa teoria classica del punteggio vero si basa su un modello additivo, in cui il punteggio osservato \\(X\\) è considerato come la somma di due componenti: il punteggio vero stabile \\(T\\) e il punteggio di errore casuale \\(E\\). Si suppone che i punteggi di errore all’interno di un test non siano correlati né con i punteggi veri di quel test, né con i punteggi veri o di errore di altri test. I test paralleli hanno gli stessi punteggi veri e le stesse varianze di errore. I test che sono considerati “sostanzialmente equivalenti” o \\(\\tau\\)-equivalenti, differiscono solo per una costante additiva nei punteggi veri. Tuttavia, queste assunzioni possono essere violate in presenza di diverse condizioni che influenzano i punteggi dei test. Tuttavia, poiché non possiamo osservare direttamente \\(T\\) ed \\(E\\), non possiamo verificare direttamente l’adeguatezza di queste assunzioni, e possiamo solo fare delle supposizioni su quando sarebbero appropriate.\nÈ importante tenere a mente che i punteggi veri e quelli di errore sono concetti teorici e non osservabili. Ciò che possiamo osservare sono solamente i punteggi \\(X\\). Quando parliamo di punteggi veri, è essenziale considerare che il “punteggio vero”, cioè la media dei punteggi su ripetuti test indipendenti con lo stesso test, è un’astrazione teorica. Questo punteggio potrebbe non riflettere completamente l’attributo “vero” di interesse, a meno che il test non abbia una precisione perfetta, cioè che misuri esattamente ciò che afferma di misurare.\nL’approccio della teoria classica dei test (CTT) nel processo di sviluppo dei test presenta diversi vantaggi. In primo luogo, i concetti della CTT sono ampiamente diffusi e comprensibili. Inoltre, sono relativamente accessibili sia per l’apprendimento che per l’applicazione. Le statistiche descrittive dei test (come la media, la deviazione standard, l’intervallo, ecc.) e le analisi degli item (in particolare la facilità e la discriminazione degli item) possono essere calcolate facilmente. Inoltre, il modello CTT risponde a varie esigenze di misurazione, specialmente nello sviluppo di valutazioni di competenze e collocazione, utili per decisioni di ammissione, confronti tra programmi e valutazioni in vari contesti lavorativi. Infine, il modello CTT permette l’interpretazione dei punteggi degli esaminati sia al 0% che al 100% e delle stime di facilità degli item da 0.0 a 1.0, riflettendo risultati realistici. Tuttavia, queste interpretazioni non sono comuni nei modelli di teoria della risposta agli item (IRT).\nTuttavia, l’adozione della CTT presenta anche alcune limitazioni. In primo luogo, i test basati sulla CTT tendono a essere lunghi e composti da elementi omogenei. In secondo luogo, gli individui che svolgono test sviluppati con il metodo CTT potrebbero essere confrontati con item troppo facili o troppo difficili per le loro abilità. In terzo luogo, i risultati dei test CTT si applicano solo al campione considerato o a campioni molto simili. In quarto luogo, tali risultati si applicano solo alla selezione corrente di item. In quinto luogo, a causa della dipendenza dalla distribuzione normale, la CTT è adatta solo per lo sviluppo di test normativi. In sesto luogo, a causa della correlazione tra discriminazione degli item, affidabilità e alcune stime di validità, gli item e i test basati sulla CTT possono risultare sensibili alle differenze agli estremi della scala. Infine, sebbene gli errori di misurazione nei test CTT varino lungo tutto il range dei possibili punteggi (ossia, l’errore standard di misurazione è minore vicino alla media e aumenta man mano che i punteggi si discostano dalla media in entrambe le direzioni), l’errore standard di misurazione stimato nei CTT rappresenta una media su tutto questo intervallo.\n\n\n9.7 Session Info\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] psychometric_2.4  multilevel_2.7    MASS_7.3-60.0.1   nlme_3.1-164     \n [5] ggokabeito_0.1.0  viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n [9] ggExtra_0.10.1    bayesplot_1.11.1  gridExtra_2.3     patchwork_1.2.0  \n[13] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-17     psych_2.4.1      \n[17] scales_1.3.0      markdown_1.12     knitr_1.45        lubridate_1.9.3  \n[21] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[25] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.0    \n[29] tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.15.0  jsonlite_1.8.8     magrittr_2.0.3    \n  [4] nloptr_2.0.3       rmarkdown_2.26     vctrs_0.6.5       \n  [7] minqa_1.2.6        base64enc_0.1-3    rstatix_0.7.2     \n [10] htmltools_0.5.7    broom_1.0.5        Formula_1.2-5     \n [13] htmlwidgets_1.6.4  plyr_1.8.9         uuid_1.2-0        \n [16] igraph_2.0.2       mime_0.12          lifecycle_1.0.4   \n [19] pkgconfig_2.0.3    Matrix_1.6-5       R6_2.5.1          \n [22] fastmap_1.1.1      shiny_1.8.0        digest_0.6.34     \n [25] OpenMx_2.21.11     fdrtool_1.2.17     colorspace_2.1-0  \n [28] rprojroot_2.0.4    Hmisc_5.1-1        fansi_1.0.6       \n [31] timechange_0.3.0   abind_1.4-5        compiler_4.3.3    \n [34] withr_3.0.0        glasso_1.11        htmlTable_2.4.2   \n [37] backports_1.4.1    carData_3.0-5      ggsignif_0.6.4    \n [40] corpcor_1.6.10     gtools_3.9.5       tools_4.3.3       \n [43] pbivnorm_0.6.0     foreign_0.8-86     zip_2.3.1         \n [46] httpuv_1.6.14      nnet_7.3-19        glue_1.7.0        \n [49] quadprog_1.5-8     promises_1.2.1     lisrelToR_0.3     \n [52] grid_4.3.3         pbdZMQ_0.3-11      checkmate_2.3.1   \n [55] cluster_2.1.6      reshape2_1.4.4     generics_0.1.3    \n [58] gtable_0.3.4       tzdb_0.4.0         data.table_1.15.2 \n [61] hms_1.1.3          car_3.1-2          utf8_1.2.4        \n [64] sem_3.1-15         pillar_1.9.0       IRdisplay_1.1     \n [67] rockchalk_1.8.157  later_1.3.2        splines_4.3.3     \n [70] lattice_0.22-5     kutils_1.73        tidyselect_1.2.0  \n [73] miniUI_0.1.1.1     pbapply_1.7-2      stats4_4.3.3      \n [76] xfun_0.42          qgraph_1.9.8       arm_1.13-1        \n [79] stringi_1.8.3      boot_1.3-29        evaluate_0.23     \n [82] mi_1.1             cli_3.6.2          RcppParallel_5.1.7\n [85] IRkernel_1.3.2     rpart_4.1.23       xtable_1.8-4      \n [88] repr_1.1.6         munsell_0.5.0      Rcpp_1.0.12       \n [91] coda_0.19-4.1      png_0.1-8          XML_3.99-0.16.1   \n [94] parallel_4.3.3     ellipsis_0.3.2     jpeg_0.1-10       \n [97] lme4_1.1-35.1      openxlsx_4.2.5.2   crayon_1.5.2      \n[100] rlang_1.1.3        mnormt_2.1.1",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>La stima del punteggio vero</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/05_err_std_stima.html#conclusioni",
    "href": "chapters/ctt/05_err_std_stima.html#conclusioni",
    "title": "9  La stima del punteggio vero",
    "section": "9.6 Conclusioni",
    "text": "9.6 Conclusioni\nLa teoria classica del punteggio vero si basa su un modello additivo, in cui il punteggio osservato \\(X\\) è considerato come la somma di due componenti: il punteggio vero stabile \\(T\\) e il punteggio di errore casuale \\(E\\). Si suppone che i punteggi di errore all’interno di un test non siano correlati né con i punteggi veri di quel test, né con i punteggi veri o di errore di altri test. I test paralleli hanno gli stessi punteggi veri e le stesse varianze di errore. I test che sono considerati “sostanzialmente equivalenti” o \\(\\tau\\)-equivalenti, differiscono solo per una costante additiva nei punteggi veri. Tuttavia, queste assunzioni possono essere violate in presenza di diverse condizioni che influenzano i punteggi dei test. Tuttavia, poiché non possiamo osservare direttamente \\(T\\) ed \\(E\\), non possiamo verificare direttamente l’adeguatezza di queste assunzioni, e possiamo solo fare delle supposizioni su quando sarebbero appropriate.\nÈ importante tenere a mente che i punteggi veri e quelli di errore sono concetti teorici e non osservabili. Ciò che possiamo osservare sono solamente i punteggi \\(X\\). Quando parliamo di punteggi veri, è essenziale considerare che il “punteggio vero”, cioè la media dei punteggi su ripetuti test indipendenti con lo stesso test, è un’astrazione teorica. Questo punteggio potrebbe non riflettere completamente l’attributo “vero” di interesse, a meno che il test non abbia una precisione perfetta, cioè che misuri esattamente ciò che afferma di misurare.\nL’approccio della teoria classica dei test (CTT) nel processo di sviluppo dei test presenta diversi vantaggi. In primo luogo, i concetti della CTT sono ampiamente diffusi e comprensibili. Inoltre, sono relativamente accessibili sia per l’apprendimento che per l’applicazione. Le statistiche descrittive dei test (come la media, la deviazione standard, l’intervallo, ecc.) e le analisi degli item (in particolare la facilità e la discriminazione degli item) possono essere calcolate facilmente. Inoltre, il modello CTT risponde a varie esigenze di misurazione, specialmente nello sviluppo di valutazioni di competenze e collocazione, utili per decisioni di ammissione, confronti tra programmi e valutazioni in vari contesti lavorativi. Infine, il modello CTT permette l’interpretazione dei punteggi degli esaminati sia al 0% che al 100% e delle stime di facilità degli item da 0.0 a 1.0, riflettendo risultati realistici. Tuttavia, queste interpretazioni non sono comuni nei modelli di teoria della risposta agli item (IRT).\nTuttavia, l’adozione della CTT presenta anche alcune limitazioni. In primo luogo, i test basati sulla CTT tendono a essere lunghi e composti da elementi omogenei. In secondo luogo, gli individui che svolgono test sviluppati con il metodo CTT potrebbero essere confrontati con item troppo facili o troppo difficili per le loro abilità. In terzo luogo, i risultati dei test CTT si applicano solo al campione considerato o a campioni molto simili. In quarto luogo, tali risultati si applicano solo alla selezione corrente di item. In quinto luogo, a causa della dipendenza dalla distribuzione normale, la CTT è adatta solo per lo sviluppo di test normativi. In sesto luogo, a causa della correlazione tra discriminazione degli item, affidabilità e alcune stime di validità, gli item e i test basati sulla CTT possono risultare sensibili alle differenze agli estremi della scala. Infine, sebbene gli errori di misurazione nei test CTT varino lungo tutto il range dei possibili punteggi (ossia, l’errore standard di misurazione è minore vicino alla media e aumenta man mano che i punteggi si discostano dalla media in entrambe le direzioni), l’errore standard di misurazione stimato nei CTT rappresenta una media su tutto questo intervallo.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>La stima del punteggio vero</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/05_err_std_stima.html#session-info",
    "href": "chapters/ctt/05_err_std_stima.html#session-info",
    "title": "9  La stima del punteggio vero",
    "section": "9.7 Session Info",
    "text": "9.7 Session Info\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] psychometric_2.4  multilevel_2.7    MASS_7.3-60.0.1   nlme_3.1-164     \n [5] ggokabeito_0.1.0  viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n [9] ggExtra_0.10.1    bayesplot_1.11.1  gridExtra_2.3     patchwork_1.2.0  \n[13] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-17     psych_2.4.1      \n[17] scales_1.3.0      markdown_1.12     knitr_1.45        lubridate_1.9.3  \n[21] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[25] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.0    \n[29] tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.15.0  jsonlite_1.8.8     magrittr_2.0.3    \n  [4] nloptr_2.0.3       rmarkdown_2.26     vctrs_0.6.5       \n  [7] minqa_1.2.6        base64enc_0.1-3    rstatix_0.7.2     \n [10] htmltools_0.5.7    broom_1.0.5        Formula_1.2-5     \n [13] htmlwidgets_1.6.4  plyr_1.8.9         uuid_1.2-0        \n [16] igraph_2.0.2       mime_0.12          lifecycle_1.0.4   \n [19] pkgconfig_2.0.3    Matrix_1.6-5       R6_2.5.1          \n [22] fastmap_1.1.1      shiny_1.8.0        digest_0.6.34     \n [25] OpenMx_2.21.11     fdrtool_1.2.17     colorspace_2.1-0  \n [28] rprojroot_2.0.4    Hmisc_5.1-1        fansi_1.0.6       \n [31] timechange_0.3.0   abind_1.4-5        compiler_4.3.3    \n [34] withr_3.0.0        glasso_1.11        htmlTable_2.4.2   \n [37] backports_1.4.1    carData_3.0-5      ggsignif_0.6.4    \n [40] corpcor_1.6.10     gtools_3.9.5       tools_4.3.3       \n [43] pbivnorm_0.6.0     foreign_0.8-86     zip_2.3.1         \n [46] httpuv_1.6.14      nnet_7.3-19        glue_1.7.0        \n [49] quadprog_1.5-8     promises_1.2.1     lisrelToR_0.3     \n [52] grid_4.3.3         pbdZMQ_0.3-11      checkmate_2.3.1   \n [55] cluster_2.1.6      reshape2_1.4.4     generics_0.1.3    \n [58] gtable_0.3.4       tzdb_0.4.0         data.table_1.15.2 \n [61] hms_1.1.3          car_3.1-2          utf8_1.2.4        \n [64] sem_3.1-15         pillar_1.9.0       IRdisplay_1.1     \n [67] rockchalk_1.8.157  later_1.3.2        splines_4.3.3     \n [70] lattice_0.22-5     kutils_1.73        tidyselect_1.2.0  \n [73] miniUI_0.1.1.1     pbapply_1.7-2      stats4_4.3.3      \n [76] xfun_0.42          qgraph_1.9.8       arm_1.13-1        \n [79] stringi_1.8.3      boot_1.3-29        evaluate_0.23     \n [82] mi_1.1             cli_3.6.2          RcppParallel_5.1.7\n [85] IRkernel_1.3.2     rpart_4.1.23       xtable_1.8-4      \n [88] repr_1.1.6         munsell_0.5.0      Rcpp_1.0.12       \n [91] coda_0.19-4.1      png_0.1-8          XML_3.99-0.16.1   \n [94] parallel_4.3.3     ellipsis_0.3.2     jpeg_0.1-10       \n [97] lme4_1.1-35.1      openxlsx_4.2.5.2   crayon_1.5.2      \n[100] rlang_1.1.3        mnormt_2.1.1",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>La stima del punteggio vero</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/06_ctt_applications.html",
    "href": "chapters/ctt/06_ctt_applications.html",
    "title": "10  Applicazioni della CTT",
    "section": "",
    "text": "10.1 Introduzione\nQuesto capitolo si focalizza sull’esplorazione di diverse applicazioni della Teoria Classica dei Test (CTT). Innanzitutto, verrà analizzato il metodo per determinare il numero di item necessari al fine di ottenere un livello specifico di affidabilità. Successivamente, si approfondirà il concetto di correlazione disattenuata e si esaminerà il metodo proposto per mitigare tale disattenuazione. Infine, verrà presentato l’utilizzo del metodo di Kelly per migliorare la stima dei punteggi reali a livello individuale, e sarà esaminato come i modelli bayesiani gerarchici rappresentino un’alternativa più moderna a tale approccio.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Applicazioni della CTT</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/06_ctt_applications.html#affidabilità-e-lunghezza-del-test",
    "href": "chapters/ctt/06_ctt_applications.html#affidabilità-e-lunghezza-del-test",
    "title": "10  Applicazioni della CTT",
    "section": "10.2 Affidabilità e lunghezza del test",
    "text": "10.2 Affidabilità e lunghezza del test\nL’affidabilità può essere utilizzata per determinare la lunghezza di un test. La formula di Spearman-Brown può essere adattata per calcolare il numero di item necessari al fine di raggiungere una specifica affidabilità:\n\\[\n\\begin{equation}\np = \\frac{\\rho_p(1 - \\rho_1)}{\\rho_1(1 - \\rho_p)},\n\\end{equation}\n\\] (eq-spearman-brown-item-number)\ndove \\(\\rho_1\\) rappresenta l’affidabilità stimata di un “item medio”, \\(\\rho_p\\) è il livello desiderato di affidabilità complessiva del test, e \\(p\\) è il numero di item nel test esteso.\nPer esempio, supponiamo che l’attendibilità di un test composto da 5 item sia 0.824, e che \\(\\rho_1\\) sia 0.479. Possiamo chiederci quanti item debbano essere aggiunti per raggiungere un livello di affidabilità pari a 0.95.\nPonendo \\(\\rho_p\\) a 0.95 e \\(\\rho_1\\) a 0.479, in base all’equazione {eq}eq-spearman-brown-item-number, otteniamo che:\n\nrho_1 &lt;- 0.479\n(.95 * (1 - rho_1)) / (rho_1 * (1 - .95))\n\n20.6659707724426\n\n\nPertanto, per ottenere un livello di affidabilità pari a 0.95 sono necessari almeno 21 item.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Applicazioni della CTT</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/06_ctt_applications.html#attenuazione",
    "href": "chapters/ctt/06_ctt_applications.html#attenuazione",
    "title": "10  Applicazioni della CTT",
    "section": "10.3 Attenuazione",
    "text": "10.3 Attenuazione\n\n10.3.1 Attenuazione e Correlazioni Disattenuate\nUn aspetto cruciale nell’analisi statistica riguarda il fenomeno dell’attenuazione, che si verifica quando l’incremento dell’errore di misurazione porta a una riduzione della correlazione osservata tra due variabili. Questo errore di misurazione tende a “nascondere” la vera associazione esistente tra le variabili, generando quello che è noto come effetto di attenuazione.\nLord e Novick (1967) hanno sottolineato che, nel tentativo di esplorare la relazione tra due costrutti, gli psicologi spesso ricorrono allo sviluppo di scale di misura. Se esiste una relazione lineare tra queste scale, è possibile calcolare il grado di correlazione attraverso il coefficiente di correlazione. Tuttavia, dato che le scale includono inevitabilmente un certo livello di errore, la correlazione empiricamente osservata tra di esse risulta inferiore rispetto alla correlazione “vera” tra i costrutti. In queste circostanze, è possibile ricorrere a formule specifiche per stimare la correlazione corretta tra i tratti latenti.\nSi può dimostrare che la correlazione tra i punteggi veri di due costrutti, \\(T_X\\) e \\(T_Y\\), può essere calcolata utilizzando la correlazione \\(\\rho_{XY}\\) tra i punteggi osservati \\(X\\) e \\(Y\\), e i coefficienti di affidabilità \\(\\rho_{XX'}\\) e \\(\\rho_{YY'}\\) dei due test, come segue:\n\\[\n\\begin{equation}\n\\rho(T_X, T_Y)  = \\frac{\\rho_{XY}}{\\sqrt{\\rho_{XX^\\prime} \\rho_{YY^\\prime}}}.\n\\end{equation}\n\\](eq-3-9-6)\nAnalogamente, la correlazione tra i punteggi osservati di un test e i punteggi veri di un secondo test può essere espressa attraverso la correlazione tra i punteggi osservati dei due test e il coefficiente di affidabilità del secondo test:\n\\[\n\\begin{equation}\n\\rho(X, T_Y)  = \\frac{\\rho_{XY}}{\\sqrt{\\rho_{YY^\\prime}}}.\n\\end{equation}\n\\](eq-3-9-7)\nQueste equazioni forniscono gli strumenti per calcolare le correlazioni disattenuate secondo la Teoria Classica dei Test (CTT).\nIl calcolo degli intervalli di confidenza per la correlazione corretta richiede un approccio che tenga conto dell’attenuazione dell’affidabilità. Applicando la formula di disattenuazione agli estremi dell’intervallo di confidenza osservato, possiamo ottenere stime più precise degli intervalli di confidenza per la correlazione tra i punteggi veri.\nPer fare un esempio, supponiamo di avere una correlazione osservata di 0.5 tra due misure, con affidabilità di 0.7 per la prima misura e 0.8 per la seconda misura. Vogliamo calcolare la correlazione disattenuata e il relativo intervallo di confidenza.\n\n# Parametri\nr_osservata &lt;- 0.5\nrho_X &lt;- 0.7\nrho_Y &lt;- 0.8\n\n# Calcolo della correlazione disattenuata\nr_corretta &lt;- r_osservata / sqrt(rho_X * rho_Y)\n\n# Stampa della correlazione disattenuata\nprint(paste(\"Correlazione disattenuata:\", r_corretta))\n\n# Calcolo approssimativo dell'intervallo di confidenza (per semplificazione)\n# NOTA: Questo è un esempio semplificato e non riflette il calcolo preciso degli intervalli di confidenza.\nCI_lower_observed &lt;- 0.4 # Limite inferiore osservato\nCI_upper_observed &lt;- 0.6 # Limite superiore osservato\n\nCI_lower_corrected &lt;- CI_lower_observed / sqrt(rho_X * rho_Y)\nCI_upper_corrected &lt;- CI_upper_observed / sqrt(rho_X * rho_Y)\n\n# Stampa dell'intervallo di confidenza corretto\nprint(paste(\"Intervallo di confidenza corretto: da\", CI_lower_corrected, \"a\", CI_upper_corrected))\n\n[1] \"Correlazione disattenuata: 0.668153104781061\"\n[1] \"Intervallo di confidenza corretto: da 0.534522483824849 a 0.801783725737273\"\n\n\n\n\n10.3.2 L’impiego delle Correlazioni Disattenuate\nL’uso delle correlazioni disattenuate risale al 1904 con Spearman, che le applicò in uno studio in cui \\(X\\) misurava la discriminazione dell’altezza del suono e \\(Y\\) l’intelligenza valutata da un insegnante. La correlazione tra queste due misure era \\(\\hat{\\rho}_{XY} = 0.38\\), con affidabilità di \\(\\hat{\\rho}_{XX'} = 0.25\\) e \\(\\hat{\\rho}_{YY'} = 0.55\\). Utilizzando le formule sopra citate, la correlazione predetta tra i punteggi veri di discriminazione del suono e l’intelligenza risultava essere \\(\\hat{\\rho}(X, T_Y) = 0.76\\), mentre tra i punteggi veri dei due costrutti era \\(\\hat{\\rho}(T_X, T_Y) = 1.025\\).\nQuesto esempio evidenzia come l’uso delle correlazioni disattenuate possa portare a stime eccessive, una problematica già rilevata nell’interazione tra Spearman e Karl Pearson. Spearman, attraverso l’applicazione della sua formula, sottolineò come le correlazioni empiriche basse proposte da Pearson potessero essere sottostimate a causa dell’errore di misurazione. Tuttavia, Pearson non accolse queste osservazioni, rimanendo scettico riguardo alla possibilità che la formula di Spearman generasse correlazioni superiori a 1 e rigettando l’idea di quantità non osservabili.\nNonostante queste controversie, Spearman proseguì nello studio delle variabili psicologiche, trovando in numerosi casi che le correlazioni disattenuate si avvicinavano all’unità, suggerendo un’associazione stretta tra variabili indicative dello stesso fenomeno. Queste osservazioni lo portarono a sviluppare ulteriormente l’analisi fattoriale.\nMcDonald (1999) avverte sull’utilizzo delle correlazioni disattenuate, evidenziando la necessità di cautela. Propone come alternativa più affidabile l’uso di modelli di equazioni strutturali per calcolare le correlazioni tra variabili latenti, ovvero quelle non influenzate da errori di misurazione, consentendo un’esplorazione diretta e più accurata delle ipotesi, inclusa la correlazione tra variabili latenti.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Applicazioni della CTT</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/06_ctt_applications.html#usare-laffidabilità-per-migliorare-linferenza-a-livello-individuale",
    "href": "chapters/ctt/06_ctt_applications.html#usare-laffidabilità-per-migliorare-linferenza-a-livello-individuale",
    "title": "10  Applicazioni della CTT",
    "section": "10.4 Usare l’Affidabilità per Migliorare l’Inferenza a Livello Individuale",
    "text": "10.4 Usare l’Affidabilità per Migliorare l’Inferenza a Livello Individuale\nUn altro uso importante dell’affidabilità è quello che ci consente di migliorare la nostra inferenza sui punteggi veri a livello individuale.\nKelley ha dimostrato – già nel 1920 (vedi Kelley, 1947) – che possiamo stimare i punteggi veri per ciascun individuo, regredendo i punteggi osservati sulla stima dell’affidabilità:\n\\[ \\hat{T} = \\bar{X} + r_{xx'}(X - \\bar{X}). \\]\nQui, \\(\\bar{X}\\) è la media dei punteggi osservati su tutti i soggetti, data da:\n\\[ \\bar{X} = \\frac{1}{N} \\sum_{i=1}^{N} X_i. \\]\nIntuitivamente, il punteggio vero per ciascun soggetto è stimato avvicinando il loro punteggio osservato verso la media dei punteggi a livello di gruppo in proporzione all’affidabilità della stima a livello individuale.\nIn aggiunta alla sua teoria che i punteggi osservati tendono ad essere regolati verso la media del gruppo quando si stima il vero punteggio, Kelley ha evidenziato come l’errore standard della stima del vero punteggio sia ridotto secondo la formula:\n\\[\n\\sigma_{\\hat{T}} = \\sigma_X \\sqrt{\\rho_{XX^\\prime} (1 -\\rho_{XX^\\prime})}.\n\\]\nQui, \\(\\sigma_{\\hat{T}}\\) rappresenta l’errore standard della stima del vero punteggio, \\(\\sigma_X\\) è la deviazione standard dei punteggi osservati, e \\(\\rho_{XX^\\prime}\\) indica il coefficiente di affidabilità tra i punteggi osservati e quelli veri. Questo errore standard per le stime dei punteggi veri è inferiore rispetto all’errore standard dei punteggi osservati, espresso come:\n\\[\nSE_{X} = \\sigma_{X} \\sqrt{1 - \\rho_{XX'}}.\n\\]\nIl confronto tra le due formule rivela che l’errore standard della stima del vero punteggio include un fattore aggiuntivo, \\(\\rho_{XX'}\\), che rappresenta il coefficiente di affidabilità. Questo evidenzia l’importanza del coefficiente di affidabilità nell’influenzare la precisione della stima del vero punteggio: un alto coefficiente di affidabilità contribuisce a ridurre l’errore standard della stima, migliorando così la precisione della stima del vero punteggio.\nLe equazioni di Kelley, scoperte nel 1920, anticipano di molti anni i principi alla base degli stimatori di James-Stein, che analogamente aggiustano le stime individuali avvicinandole alla media del gruppo. Questa affinità storica evidenzia un precedente significativo alla comprensione moderna di come le stime possano essere migliorate mediante l’incorporazione di informazioni aggiuntive.\nLa relazione tra le equazioni di Kelley e i concetti bayesiani offre una prospettiva ancora più profonda. Assumendo che i punteggi veri seguano una distribuzione a priori normale e che esista una distribuzione normale dei punteggi veri intorno ai punteggi osservati, l’approccio bayesiano empirico genera medie posteriori che corrispondono alle stime di Kelley dei punteggi veri. Questa equivalenza, come discussa da de Gruijter e van der Kamp nel 2008, stabilisce un ponte concettuale tra la psicometria classica e l’inferenza bayesiana, sottolineando come l’incorporazione di presupposti a priori possa affinare le nostre stime.\nQuesta connessione è ulteriormente rafforzata dall’uso di tecniche simili alla stima bayesiana empirica nei software di modellazione multilivello, come ad esempio il pacchetto lmer in R. Questi software si avvalgono della potenza dell’inferenza bayesiana per integrare informazioni di gruppo, migliorando così la precisione delle inferenze a livello individuale. La pratica di utilizzare informazioni a livello di gruppo per affinare le stime individuali non solo ha radici storiche profonde ma continua a essere una componente essenziale nell’evoluzione delle tecniche statistiche, dimostrando il suo valore nell’arricchire l’accuratezza e l’affidabilità delle inferenze statistiche.\nPer illustrare in modo pratico come avviene la stima dei punteggi veri, ossia il processo di pooling, eseguiremo una simulazione basata sul codice R di Nathaniel Haines. Questa simulazione genera dati seguendo una distribuzione binomiale per 20 soggetti, con una probabilità media di successo di 0.7. La simulazione considera tre diversi set di item: 10, 30 e 100, al fine di esaminare come le variazioni nel numero di item influenzino l’affidabilità ottenuta e, di conseguenza, gli effetti del pooling.\nIl codice inizia definendo il numero di soggetti e la varietà delle dimensioni degli item. Successivamente, genera un campione casuale di “punteggi veri” intorno a 0.7 per ogni soggetto. Viene poi definita una funzione per stimare l’errore standard della misurazione (al quadrato), basata sulla probabilità di successo per ogni item.\nPer ogni set di item, il codice simula i dati osservati per ogni soggetto utilizzando il suo “punteggio vero”. Calcola quindi la media del gruppo per i punteggi osservati, la affidabilità, e l’errore standard di misurazione, utilizzando l’approccio basato sulla varianza. Infine, stima i punteggi veri e gli errori standard associati sia per i punteggi osservati sia per quelli stimati.\nI risultati della simulazione vengono visualizzati in un grafico, che confronta i punteggi veri, osservati e stimati per ogni soggetto, evidenziando come la precisione della stima vari in funzione del numero di item. Il grafico include anche intervalli di confidenza al 95% per i punteggi osservati e stimati, e una linea orizzontale che rappresenta la media del gruppo per i punteggi osservati, offrendo una rappresentazione visiva dell’efficacia del processo di pooling nel recuperare i punteggi veri a partire da dati osservati affetti da errore di misurazione.\n\nset.seed(43202)\n\n# Number of subjects and items\nn_subj &lt;- 20\nn_items &lt;- c(10, 30, 100)\n\n# Random sample of \"true\" scores around .7\ntheta &lt;- rnorm(n_subj, .7, .1)\n\n# Estimate standard error of measurement (squared)\nest_se2 &lt;- function(x) {\n    # Success and failure probability\n    n &lt;- length(x)\n    p &lt;- mean(x)\n    q &lt;- 1 - p\n\n    sig2_ep_i &lt;- (p * q) / (n - 1)\n\n    return(sig2_ep_i)\n}\n\n# Estimate observed and true score\ndis_dat &lt;- foreach(i = seq_along(n_items), .combine = \"rbind\") %do% {\n    # Generate observed data for each subject using \"true\" score\n    X_all &lt;- foreach(t = seq_along(theta), .combine = \"rbind\") %do% {\n        rbinom(n_items[i], 1, prob = theta[t])\n    }\n\n    # group average observed score\n    X_bar &lt;- mean(rowMeans(X_all))\n\n    # Reliability\n    X &lt;- rowMeans(X_all)\n\n    # Standard arror of measurement approach\n    sig2_ep &lt;- mean(apply(X_all, 1, est_se2))\n    sig2_X &lt;- var(X)\n    rho &lt;- 1 - (sig2_ep / sig2_X)\n\n    foreach(t = seq_along(theta), .combine = \"rbind\") %do% {\n        # Using observed scores from parallel form 1\n        X_obs &lt;- X_all[t, ]\n        X_i &lt;- mean(X_obs)\n\n        data.frame(\n            subj_num = t,\n            n_items = n_items[i],\n            theta = theta[t],\n            rho = rho,\n            X = X_i,\n            se_obs = sd(X) * sqrt(1 - rho),\n            se_hat = sd(X) * sqrt(1 - rho) * sqrt(rho),\n            theta_hat = (1 - rho) * X_bar + rho * X_i\n        )\n    }\n}\n\n# Plot true, observed, and estimated true scores\ndis_dat %&gt;%\n    mutate(subj_num = reorder(subj_num, theta)) %&gt;%\n    ggplot(aes(x = subj_num, y = theta)) +\n    geom_point(color = I(\"black\")) +\n    geom_point(aes(x = subj_num, y = X),\n        color = I(\"#DCBCBC\"),\n        position = position_jitter(width = .2, height = 0, seed = 1)\n    ) +\n    geom_linerange(\n        aes(\n            x = subj_num,\n            ymin = X - 1.96 * se_obs,\n            ymax = X + 1.96 * se_obs\n        ),\n        color = I(\"#DCBCBC\"),\n        position = position_jitter(width = .2, height = 0, seed = 1)\n    ) +\n    geom_point(aes(x = subj_num, y = theta_hat),\n        color = I(\"#8F2727\"),\n        position = position_jitter(width = .2, height = 0, seed = 2)\n    ) +\n    geom_linerange(\n        aes(\n            x = subj_num,\n            ymin = theta_hat - 1.96 * se_hat,\n            ymax = theta_hat + 1.96 * se_hat\n        ),\n        color = I(\"#8F2727\"),\n        position = position_jitter(width = .2, height = 0, seed = 2)\n    ) +\n    geom_hline(yintercept = X_bar, linetype = 2, color = I(\"gray\")) +\n    annotate(\"text\",\n        x = 15, y = .4, label = expression(\"True\" ~ theta[i]),\n        color = \"black\", size = 5\n    ) +\n    annotate(\"text\",\n        x = 15, y = .3, label = expression(\"Obs\" ~ X[i]),\n        color = \"#DCBCBC\", size = 5\n    ) +\n    annotate(\"text\",\n        x = 15, y = .2, label = expression(\"Est\" ~ hat(theta)[i]),\n        color = \"#8F2727\", size = 5\n    ) +\n    facet_wrap(c(\"n_items\"), nrow = 1) +\n    ggtitle(\"Regression-Based True Score Estimates\") +\n    xlab(\"Subject\") +\n    ylab(\"Value\") +\n    theme_minimal(base_size = 15) +\n    theme(\n        panel.grid = element_blank(),\n        axis.text.x.bottom = element_blank()\n    )\n\nWarning message in is.na(x):\n\"is.na() applicato ad un oggetto di tipo 'expression' (ne lista, ne vettore)\"\nWarning message in is.na(x):\n\"is.na() applicato ad un oggetto di tipo 'expression' (ne lista, ne vettore)\"\nWarning message in is.na(x):\n\"is.na() applicato ad un oggetto di tipo 'expression' (ne lista, ne vettore)\"\n\n\n\n\n\n\n\n\n\nSi notino tre risultati di questa simulazione:\n\nle stime puntuali basate sulla regressione di Kelley (i punti neri nel grafico) risultano più vicine alla media a livello di gruppo (rappresentata dalla linea tratteggiata grigia orizzontale) di quanto lo siano le stime individuali “non corrette” (punti grigi);\nquesto effetto di “pooling” è tanto maggiore quanto minore è l’attendibilità (in questa simulazione l’attendibilità è stata manipolata variando il numero di item);\ngli intervalli di confidenza per i punteggi veri stimati sono più stretti rispetto a quelli dei punteggi osservati.\n\n\n10.4.1 Approccio Bayesiano\nNella seguente simulazione mostreremo come i risultati raggiunti con la regressione di Kelley possano essere replicati se i dati vengono analizzati con un modello gerarchico bayesiano.\nQuando si analizzano dati provenienti da questionari con risposte dicotomiche (ad esempio, vero/falso o corretto/errato), è possibile applicare la distribuzione di Bernoulli. In questo contesto, ogni risposta data a un item del questionario può essere vista come il risultato di un esperimento di Bernoulli. Se indichiamo con \\(X\\) una variabile casuale che segue tale distribuzione, la probabilità di ottenere un successo (ad esempio, una risposta corretta) è espressa come:\n\\[\n\\Pr(X=1) = p, \\quad \\text{e quindi} \\quad \\Pr(X=0) = 1 - p = q,\n\\]\ndove \\(p\\) indica la probabilità di successo e \\(q\\) quella di insuccesso.\nIntroduciamo il modello di Bernoulli tramite l’equazione logistica:\n\\[\np = \\frac{1}{1 + e^{-\\theta}}.\n\\]\nQuesta formula ci permette di modellare \\(p\\) in termini di \\(\\theta\\), un parametro che riflette una caratteristica o “abilità” dell’individuo. Il modello logistico assicura che \\(p\\), la probabilità di successo, sia sempre compresa nell’intervallo \\([0, 1]\\). Il parametro \\(\\theta\\) viene definito come:\n\\[\n\\theta = \\log\\left(\\frac{p}{1-p}\\right),\n\\]\ne può variare tra \\(-\\infty\\) e \\(+\\infty\\). Attraverso la trasformazione logistica, \\(\\theta\\) viene mappato in un valore di \\(p\\) che rispetta i limiti di una probabilità. Questa funzione di collegamento permette di interpretare il legame tra \\(\\theta\\) e \\(p\\).\nIl modello descritto sopra può essere considerato una forma estremamente semplificata della Teoria della Risposta all’Item (IRT), dove ogni persona è caratterizzata da un unico parametro di abilità (\\(\\theta\\)), e tutti gli item del test sono assunti avere uguale difficoltà e capacità di discriminazione, fissate convenzionalmente a 1.\nIl nostro obiettivo principale nell’analisi dei dati è quindi stimare il parametro \\(\\theta\\) per ogni individuo. La relazione tra \\(\\theta\\) e \\(p\\) è fondamentale: \\(\\theta\\) determina il valore di \\(p\\) attraverso la funzione logistica, che trasforma i valori di \\(\\theta\\) in probabilità \\(p\\) comprese tra 0 e 1. La stima di \\(\\theta\\) ci fornisce, di conseguenza, una misura della probabilità di successo di un individuo in risposta agli item del questionario.\nPer approfondire la nostra comprensione su come emergono le risposte osservate, è fondamentale definire la modalità con cui i parametri \\(\\theta\\) vengono generati per ogni individuo. Similmente a quanto avviene nella teoria classica dei test, dove si presume l’esistenza di una distribuzione di campionamento a livello di popolazione, nell’ambito della modellazione generativa bayesiana si postula una distribuzione generativa per il gruppo. In termini pratici, possiamo ipotizzare che i parametri \\(\\theta\\) individuali derivino da una distribuzione normale standardizzata:\n\\[\n\\theta \\sim \\mathcal{N}(0, 1)\n\\]\nNel contesto bayesiano, questa distribuzione di gruppo viene comunemente identificata come una distribuzione a priori per \\(\\theta\\). In alternativa, possiamo dedurre questi parametri direttamente dai dati:\n\\[\n\\theta \\sim \\mathcal{N}(\\mu, \\sigma)\n\\]\nDi conseguenza, si introduce un’ipotesi generativa riguardante i parametri di media \\(\\mu\\) e deviazione standard \\(\\sigma\\) del gruppo, che potrebbero essere descritti, in termini bayesiani tradizionali, come a priori del gruppo. Nel nostro esempio, supponiamo \\(\\mu = 0\\) e \\(\\sigma \\sim \\text{HalfNormal}(1)\\), dove \\(\\text{HalfNormal}(1)\\) rappresenta una distribuzione normale limitata ai valori positivi, coerente con il principio che le deviazioni standard debbano essere positive.\nQuesto approccio introduce un modello gerarchico: durante l’adattamento del modello, i parametri individuali influenzano quelli di gruppo, che a loro volta modellano nuovamente quelli individuali. Analogamente alle stime dei punteggi “veri” ottenuti tramite regressione nella teoria classica dei test, i nostri parametri individuali verranno regolati (“pooled”) verso la media di gruppo, portando a una riduzione degli intervalli di incertezza per le stime individuali.\nPer facilitare la comprensione di come queste assunzioni generative si traducano in pratica, eseguiamo la seguente simulazione.\n\nfile &lt;- file.path(\"hbern.stan\")\n\n\nmod &lt;- cmdstan_model(file)\n\n\nmod$print()\n\ndata {\n  int&lt;lower=0&gt; N;      // Number of subjects\n  int&lt;lower=0&gt; N_items; // Number of timepoints\n  array[N, N_items] int Y; // Binary responses for each subject and item\n}\n\nparameters {\n  real&lt;lower=0&gt; sigma_theta; // SD of individual effects\n  real mu_theta; // Mean of individual effects\n  \n  vector[N] theta_pr; // Non-centered individual-level parameters\n}\n\ntransformed parameters {\n  vector[N] theta = mu_theta + sigma_theta * theta_pr; // Individual-level effects\n}\n\nmodel {\n  // Priors\n  mu_theta ~ normal(0, 1);\n  sigma_theta ~ normal(0, 1);\n  theta_pr ~ normal(0, 1);\n  \n  // Likelihood\n  for (i in 1:N) {\n    for (j in 1:N_items) {\n      Y[i, j] ~ bernoulli_logit(theta[i]);\n    }\n  }\n}\n\ngenerated quantities {\n  array[N] real p; // Success probability estimate for each individual\n  \n  for (i in 1:N) {\n    p[i] = inv_logit(theta[i]);\n  }\n}\n\n\nIl codice Stan presentato adotta una parametrizzazione non centrata (non-centered parameterization) per la parte di modello a livello di gruppo, una scelta motivata per migliorare l’efficienza computazionale e facilitare la convergenza degli algoritmi di stima, come il campionamento Hamiltoniano Monte Carlo (HMC) usato da Stan. Questa scelta di design è matematicamente equivalente al modello generativo descritto dalle equazioni precedenti, pur offrendo vantaggi pratici significativi in fase di implementazione.\nLa parametrizzazione non centrata è una strategia avanzata nella modellazione bayesiana, specialmente utile nei modelli gerarchici o multilivello. Essa differisce dalla parametrizzazione centrata, nella quale i parametri di gruppo sono direttamente definiti dai parametri individuali. Invece, con la parametrizzazione non centrata, i parametri individuali sono inizialmente espressi come variazioni indipendenti rispetto alla media e deviazione standard di gruppo, per poi essere trasformati.\nImplementazione nel codice Stan:\n\nDefinizione dei Parametri:\n\nsigma_theta denota la deviazione standard degli effetti individuali, indicando la variabilità dei parametri \\(\\theta\\) a livello personale.\nmu_theta rappresenta la media degli effetti individuali.\ntheta_pr corrisponde ai parametri individuali nella forma non centrata, esprimendo le deviazioni rispetto alla media di gruppo in unità standardizzate.\n\nTrasformazione dei Parametri:\n\nGli effetti individuali effettivi (theta) sono ottenuti trasformando theta_pr per allinearli attorno a mu_theta e adattarli alla scala definita da sigma_theta. Questo processo è sintetizzato dall’equazione theta = mu_theta + sigma_theta * theta_pr, che trasla e scala theta_pr per ottenere valori centrati e proporzionati correttamente.\n\nApplicazione nel Modello:\n\nAll’interno del modello, sia mu_theta che sigma_theta sono sottoposti a priori normali (normal(0, 1)), presupponendo una distribuzione iniziale per questi parametri a livello di gruppo. Anche theta_pr è assoggettato a una distribuzione normale standard come priori, rispecchiando l’approccio di considerare le variazioni in termini standardizzati.\nLa verosimiglianza del modello è calcolata usando una distribuzione di Bernoulli con una funzione di collegamento logit, basata sui valori di theta trasformati, per analizzare le risposte binarie Y fornite da ogni soggetto per ogni item.\n\n\nAttraverso questa struttura, il modello mira a una stima più stabile e accurata dei parametri, beneficiando della maggiore efficienza computazionale e della riduzione dei problemi di convergenza che spesso accompagnano la modellazione bayesiana gerarchica.\nSimuliamo i dati di un singolo soggetto.\n\n# Initialize parameters for a single subject\nn_subj &lt;- 1\nn_items &lt;- 30 # Example with 30 items for simplicity\n\n# Generate \"true\" theta for the subject\ntheta &lt;- rnorm(n_subj, .7, .1)\n\n# Generate observed data for the subject using \"true\" theta\nY &lt;- rbinom(n_items, 1, prob = theta)\n\nAdattiamo il modello gerarchico bayesiano ai dati.\n\nfit_bernoulli &lt;- mod$sample(\n    data = list(\n        N = n_subj,\n        N_items = n_items,\n        Y = matrix(Y, nrow = 1) # Ensure Y is a matrix even for a single subject\n    ),\n    iter_sampling = 2500,\n    iter_warmup = 500,\n    chains = 4,\n    parallel_chains = 4,\n    seed = 43202\n)\n\nCalcoliamo la media a posteriori di \\(\\theta\\) e l’intervallo di confidenza al 95%:\n\n# Extract posterior samples for parameter 'p'\nbayes_est &lt;- fit_bernoulli$draws(variables = \"p\")\n\nbayes_est_p &lt;- as.vector(bayes_est)\n\n# Calculate the mean of the Bayesian estimates for 'p'\nbayes_theta_est &lt;- mean(bayes_est_p)\n\n# Calculate the 95% HDI using quantiles for the flattened vector\nhdi_bounds &lt;- quantile(bayes_est_p, probs = c(0.025, 0.975))\n\n# Prepare the results with a single HDI for 'p'\nresults &lt;- data.frame(\n    subj_num = 1,\n    n_items = n_items,\n    theta = theta,\n    bayes_theta = bayes_theta_est,\n    bayes_lo = hdi_bounds[1], # Lower bound of HDI\n    bayes_hi = hdi_bounds[2] # Upper bound of HDI\n)\n\n# Print the corrected results\nprint(results)\n\n     subj_num n_items     theta bayes_theta  bayes_lo  bayes_hi\n2.5%        1      30 0.6185681   0.5909482 0.4230871 0.7531208\n\n\nAdesso svolgiamo la stessa simulazione considerando però 20 soggetti e facendo variare il numero di item del questionario (10, 30, 100).\n\nset.seed(43202)\n\nn_subj &lt;- 20\nn_items_vec &lt;- c(10, 30, 100)\n\n# Placeholder for results\nresults &lt;- list()\n\nfor (n_items in n_items_vec) {\n    for (subj in 1:n_subj) {\n        # Generate \"true\" theta for the subject\n        theta &lt;- rnorm(1, .7, .1)\n\n        # Generate observed data for the subject using \"true\" theta\n        Y &lt;- rbinom(n_items, 1, prob = theta)\n\n        # Fit the model\n        fit_bernoulli &lt;- mod$sample(\n            data = list(\n                N = 1,\n                N_items = n_items,\n                Y = matrix(Y, nrow = 1) # Ensure Y is a matrix\n            ),\n            iter_sampling = 2500,\n            iter_warmup = 500,\n            chains = 4,\n            parallel_chains = 4,\n            seed = 43202\n        )\n\n        # Extract and process posterior samples for 'p'\n        bayes_est_p &lt;- as.vector(fit_bernoulli$draws(variables = \"p\"))\n        bayes_theta_est &lt;- mean(bayes_est_p)\n        hdi_bounds &lt;- quantile(bayes_est_p, probs = c(0.025, 0.975))\n\n        # Collect results\n        results[[paste(subj, n_items)]] &lt;- data.frame(\n            subj_num = subj,\n            n_items = n_items,\n            theta = theta,\n            bayes_theta = bayes_theta_est,\n            bayes_lo = hdi_bounds[1],\n            bayes_hi = hdi_bounds[2]\n        )\n    }\n}\n\nCombiniamo tutti i risultati in un singolo data frame.\n\nall_results &lt;- bind_rows(results)\nall_results |&gt; head()\n\n\nA data.frame: 6 x 6\n\n\n\nsubj_num\nn_items\ntheta\nbayes_theta\nbayes_lo\nbayes_hi\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n2.5%...1\n1\n10\n0.6369682\n0.5763993\n0.3105686\n0.8190624\n\n\n2.5%...2\n2\n10\n0.7167035\n0.5004720\n0.2411716\n0.7542886\n\n\n2.5%...3\n3\n10\n0.7412891\n0.7314809\n0.4723178\n0.9255873\n\n\n2.5%...4\n4\n10\n0.6369442\n0.5767456\n0.3153395\n0.8209660\n\n\n2.5%...5\n5\n10\n0.6903750\n0.4990059\n0.2371964\n0.7604131\n\n\n2.5%...6\n6\n10\n0.6933872\n0.6540551\n0.3854844\n0.8781372\n\n\n\n\n\nCreiamo un grafico con i risultati ottenuti.\n\nggplot(all_results, aes(x = theta, y = bayes_theta)) +\n    geom_point() +\n    geom_errorbar(aes(ymin = bayes_lo, ymax = bayes_hi), width = 0.02) +\n    geom_hline(yintercept = 0.7, linetype = \"dashed\", color = \"gray\") + # Add dashed line at y = 0.7\n    # facet_wrap(~n_items, scales = \"free_x\", ncol = 1) + # Separate panels for each n_items, with a common y-axis\n    facet_wrap(c(\"n_items\"), nrow = 1) +\n    theme_minimal() +\n    labs(x = \"True Theta\", y = \"Estimated p\") +\n    ggtitle(\"Estimated p vs. True Theta for Different Numbers of Items\") +\n    theme(\n        panel.grid = element_blank(),\n        axis.text.x.bottom = element_blank()\n    )\n\n\n\n\n\n\n\n\nLo scopo di questa simulazione è quello di confrontare i risultati del modello gerarchico bayesiano con i risultati ottenuti mediante la tecnica di Kelly. Per gli stessi dati utilizzati nel modello gerarchico bayesiamo, calcoliamo dunque la stima dei punteggi veri e gli intervalli di confidenza al 95% secondo il metodo di Kelley.\nLa formula di Kelley per stimare i punteggi veri dai punteggi osservati coinvolge l’affidabilità del test e la media e la deviazione standard dei punteggi osservati:\n\\[\n\\text{Punteggio Vero} = \\text{Media} + (\\text{Affidabilità}) \\times (\\text{Punteggio Osservato} - \\text{Media}).\n\\]\nPer calcolare il CI al 95% per i punteggi veri, dobbiamo tener conto dell’errore standard di misurazione, che deriva dall’affidabilità del test:\n\\[\n\\text{SEM} = \\sigma \\times \\sqrt{1 - \\text{Affidabilità}},\n\\]\ndove $ $ è la deviazione standard dei punteggi osservati.\nDate la stima di SEM, l’intervallo di confidenza al 95% per il punteggio vero di un individuo può essere calcolato come segue:\n\\[\n\\text{CI} = \\text{Punteggio Vero} \\pm (1.96 \\times \\text{SEM}).\n\\]\nSvolgiamo ora i calcoli in R.\n\n# Assuming a reliability coefficient\nr_xx &lt;- 0.8\nZ_alpha &lt;- qnorm(0.975) # For a 95% CI\n\n# Calculate estimated true scores and CIs\nall_results$kelley_true_score &lt;- all_results$bayes_theta\nall_results$kelley_lo &lt;- all_results$bayes_theta - (Z_alpha * sqrt(1 - r_xx) * sd(all_results$bayes_theta))\nall_results$kelley_hi &lt;- all_results$bayes_theta + (Z_alpha * sqrt(1 - r_xx) * sd(all_results$bayes_theta))\n\nA questo punto possiamo generare un grarico che contiene sia la stima del punteggio vero basata sul metodo di Kelley, insieme all’intervallo di confidenza al 95% (colore grigio), sia le stime bayesiane trovate in precedenza (colore blue).\nPer semplicità, ho solo considerato il caso in cui la stima di Kelley si riferisce al caso di 100 items.\n\nggplot() +\n    geom_point(data = all_results, aes(x = theta - 0.02, y = bayes_theta, color = \"Bayesian Estimate\")) +\n    geom_errorbar(data = all_results, aes(x = theta - 0.02, ymin = bayes_lo, ymax = bayes_hi, color = \"Bayesian Estimate\"), width = 0.02) +\n    geom_point(data = all_results, aes(x = theta, y = kelley_true_score, color = \"Kelley's Estimate\")) +\n    geom_errorbar(data = all_results, aes(x = theta, ymin = kelley_lo, ymax = kelley_hi, color = \"Kelley's Estimate\"), width = 0.02) +\n    geom_hline(yintercept = 0.7, linetype = \"dashed\", color = \"gray\") +\n    facet_wrap(c(\"n_items\"), nrow = 1) +\n    theme_minimal() +\n    labs(x = \"True Theta\", y = \"Estimated Score\") +\n    ggtitle(\"Estimated Scores vs. True Theta for Different Numbers of Items\") +\n    theme(\n        panel.grid = element_blank(),\n        legend.position = \"bottom\"\n    ) + # Aggiunta della legenda in basso\n    scale_color_manual(values = c(\"Bayesian Estimate\" = \"blue\", \"Kelley's Estimate\" = \"darkgray\"))\n\n\n\n\n\n\n\n\nI risultati della simulazione completa sono riportati nella figura seguente.\n\n\n\n```pjyrdkwt ../images/haynes_kelley.png\n\n\n\n\nheight: 350px\n\n\nname: haynes-kelley-fig\n\n\n\nStime dei punteggi veri basate sul metodo della regressione di Kelley e sulla regressione gerarchica bayesiana.\n\nI risultati della simulazione indicano che le stime medie a posteriori del modello bayesiano, così come gli intervalli di credibilità al 95% (definiti come intervalli di densità di probabilità più elevata), mostrano una notevole congruenza con le stime corrispondenti dei punteggi veri ottenute mediante la regressione di Kelley, insieme ai relativi intervalli di confidenza al 95%. Le stime puntuali prodotte da entrambi i metodi risultano quasi sovrapponibili. Considerando che i punteggi veri derivanti dalla regressione di Kelley posseggono un'interpretazione bayesiana, la similitudine tra i risultati non dovrebbe sorprendere eccessivamente. Tuttavia, una conferma empirica di questa corrispondenza fornisce una validazione più robusta.\n\nQuesto esempio illustra come i modelli bayesiani gerarchici siano capaci di generare stime dei \"punteggi veri\" comparabili a quelle prodotte dalla teoria classica dei test, offrendo l'ulteriore vantaggio di non richiedere il calcolo dell'affidabilità per giungere a tali stime. Al contrario, l'approccio bayesiano si basa sull'adozione di assunzioni generative e distribuzionali riguardo le relazioni sia tra i parametri del modello a diversi livelli (ad esempio, la struttura gerarchica delinea le connessioni tra i parametri individuali e quelli di gruppo) sia con i dati osservati. In questo modo, adottando la media posteriore come stima dell'aspettativa dei parametri a livello individuale, siamo in grado di ottenere le stime più accurate dei parametri reali che sottendono la generazione dei dati osservati.\n\n## Commenti e considerazioni conclusive\n\nIn questo capitolo, abbiamo analizzato diverse applicazioni pratiche della CTT. Ci siamo concentrati sulla comprensione dei concetti di attenuazione e sul metodo per determinare il numero di item necessari per ottenere un livello desiderato di affidabilità. Inoltre, abbiamo esaminato come stimare i punteggi veri individuali utilizzando due approcci differenti: la regressione di Kelley basata sulla CTT e la regressione gerarchica bayesiana. Approfondire questi argomenti ci ha permesso di ottenere una visione più completa e concreta sull'utilizzo e sull'applicazione della CTT, migliorando la nostra comprensione dei concetti chiave e delle implicazioni pratiche della teoria.\n\n## Session Info\n\n::: {#a719648c .cell vscode='{\"languageId\":\"r\"}' execution_count=18}\n``` {.r .cell-code}\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] parallel  stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] doParallel_1.0.17  iterators_1.0.14   cmdstanr_0.7.1     truncnorm_1.0-9   \n [5] ggridges_0.5.6     foreach_1.5.2      modelsummary_1.4.5 ggokabeito_0.1.0  \n [9] viridis_0.6.5      viridisLite_0.4.2  ggpubr_0.6.0       ggExtra_0.10.1    \n[13] bayesplot_1.11.1   gridExtra_2.3      patchwork_1.2.0    semTools_0.5-6    \n[17] semPlot_1.1.6      lavaan_0.6-17      psych_2.4.1        scales_1.3.0      \n[21] markdown_1.12      knitr_1.45         lubridate_1.9.3    forcats_1.0.0     \n[25] stringr_1.5.1      dplyr_1.1.4        purrr_1.0.2        readr_2.1.5       \n[29] tidyr_1.3.1        tibble_3.2.1       ggplot2_3.5.0      tidyverse_2.0.0   \n[33] here_1.0.1        \n\nloaded via a namespace (and not attached):\n  [1] tensorA_0.36.2.1     rstudioapi_0.15.0    jsonlite_1.8.8      \n  [4] magrittr_2.0.3       farver_2.1.1         nloptr_2.0.3        \n  [7] rmarkdown_2.26       vctrs_0.6.5          minqa_1.2.6         \n [10] base64enc_0.1-3      rstatix_0.7.2        htmltools_0.5.7     \n [13] distributional_0.4.0 broom_1.0.5          Formula_1.2-5       \n [16] htmlwidgets_1.6.4    plyr_1.8.9           uuid_1.2-0          \n [19] igraph_2.0.2         mime_0.12            lifecycle_1.0.4     \n [22] pkgconfig_2.0.3      Matrix_1.6-5         R6_2.5.1            \n [25] fastmap_1.1.1        shiny_1.8.0          digest_0.6.34       \n [28] OpenMx_2.21.11       fdrtool_1.2.17       colorspace_2.1-0    \n [31] ps_1.7.6             rprojroot_2.0.4      Hmisc_5.1-1         \n [34] labeling_0.4.3       fansi_1.0.6          timechange_0.3.0    \n [37] abind_1.4-5          compiler_4.3.3       withr_3.0.0         \n [40] glasso_1.11          htmlTable_2.4.2      backports_1.4.1     \n [43] carData_3.0-5        ggsignif_0.6.4       MASS_7.3-60.0.1     \n [46] corpcor_1.6.10       gtools_3.9.5         tools_4.3.3         \n [49] pbivnorm_0.6.0       foreign_0.8-86       zip_2.3.1           \n [52] httpuv_1.6.14        nnet_7.3-19          glue_1.7.0          \n [55] quadprog_1.5-8       nlme_3.1-164         promises_1.2.1      \n [58] lisrelToR_0.3        grid_4.3.3           pbdZMQ_0.3-11       \n [61] checkmate_2.3.1      cluster_2.1.6        reshape2_1.4.4      \n [64] generics_0.1.3       gtable_0.3.4         tzdb_0.4.0          \n [67] data.table_1.15.2    hms_1.1.3            car_3.1-2           \n [70] utf8_1.2.4           tables_0.9.17        sem_3.1-15          \n [73] pillar_1.9.0         IRdisplay_1.1        rockchalk_1.8.157   \n [76] posterior_1.5.0      later_1.3.2          splines_4.3.3       \n [79] lattice_0.22-5       kutils_1.73          tidyselect_1.2.0    \n [82] miniUI_0.1.1.1       pbapply_1.7-2        stats4_4.3.3        \n [85] xfun_0.42            qgraph_1.9.8         arm_1.13-1          \n [88] stringi_1.8.3        boot_1.3-29          codetools_0.2-19    \n [91] evaluate_0.23        mi_1.1               cli_3.6.2           \n [94] RcppParallel_5.1.7   IRkernel_1.3.2       rpart_4.1.23        \n [97] xtable_1.8-4         processx_3.8.3       repr_1.1.6          \n[100] munsell_0.5.0        Rcpp_1.0.12          coda_0.19-4.1       \n[103] png_0.1-8            XML_3.99-0.16.1      ellipsis_0.3.2      \n[106] jpeg_0.1-10          lme4_1.1-35.1        insight_0.19.8      \n[109] openxlsx_4.2.5.2     crayon_1.5.2         rlang_1.1.3         \n[112] mnormt_2.1.1        \n\n:::",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Applicazioni della CTT</span>"
    ]
  },
  {
    "objectID": "chapters/raters/01_multilevel.html",
    "href": "chapters/raters/01_multilevel.html",
    "title": "11  Modelli multilivello",
    "section": "",
    "text": "11.1 Introduzione\nI modelli multilivello, noti anche come modelli gerarchici o a effetti misti, rappresentano una metodologia statistica potente e flessibile, particolarmente adatta per analizzare dati strutturati in più livelli. Questi modelli permettono di considerare simultaneamente variazioni a livelli diversi, come quello individuale e quello di gruppo, fornendo una comprensione più dettagliata e accurata dei fenomeni studiati.\nIn psicologia i modelli multilivello assumono un’importanza cruciale. Gli psicologi utilizzano frequentemente dati raccolti in contesti complessi, dove gli effetti individuali e contestuali si intrecciano. Ad esempio, nella valutazione delle prestazioni cognitive o della risposta emotiva, i modelli multilivello consentono di distinguere tra variazioni dovute a caratteristiche individuali (come l’abilità cognitiva o la personalità) e quelle derivanti da fattori esterni (come l’ambiente scolastico o familiare).\nI modelli multilivello sono particolarmente utili per: - Analizzare dati longitudinali, dove le misurazioni ripetute sugli stessi soggetti introducono correlazioni naturali. - Studiare l’impatto di fattori contestuali su variabili psicologiche, consentendo di esaminare come l’ambiente influenzi i comportamenti o gli stati mentali. - Gestire la variabilità intra-individuale e inter-individuale in modo più efficace, offrendo una rappresentazione più realistica della complessità dei fenomeni psicologici.\nIn questo capitolo, ci focalizzeremo sull’analisi di un’indagine sperimentale condotta per studiare l’impatto della deprivazione del sonno sulle prestazioni psicomotorie. I dati utilizzati provengono dallo studio di Belenky et al. (2003) sugli effetti della deprivazione del sonno. Questi dati sono accessibili nel dataset sleepstudy, incluso nel pacchetto lme4 di R {cite:t}bates2014fitting.\nImportiamo i dati.\ndata(sleepstudy)\nIl data frame comprende 180 righe (osservazioni) e tre variabili:\nQuesti dati forniscono un esempio di dati multilivello, caratterizzati da misurazioni ripetute su una stessa variabile dipendente - in questo caso, il tempo medio di reazione (RT) - raccolte dai medesimi partecipanti per un periodo di dieci giorni. Tale struttura di dati è molto diffusa in psicologia, dove spesso si valutano le variazioni delle risposte o dei comportamenti di individui nel tempo.\nIl dataset esaminato focalizza su diciotto partecipanti, sottoposti a una condizione di sonno limitato a tre ore. Nell’arco di dieci giorni, questi partecipanti hanno partecipato quotidianamente a un “test di vigilanza psicomotoria” della durata di dieci minuti. Durante il test, era richiesto loro di monitorare uno schermo e di premere un pulsante quanto più rapidamente possibile alla comparsa di uno stimolo. La variabile dipendente principale dello studio è il tempo medio di risposta (RT) di ciascun partecipante.\nPer analizzare questi dati, è utile iniziare con una rappresentazione grafica. Se ci concentriamo sui dati di un singolo soggetto, questo ci permette di osservare le tendenze e le variazioni nel tempo di reazione di quel particolare individuo, fornendo insight su come la restrizione del sonno possa influire sulle sue prestazioni nel corso dei dieci giorni dello studio.\njust_308 &lt;- sleepstudy |&gt;\n    filter(Subject == \"308\")\nggplot(just_308, aes(x = Days, y = Reaction)) +\n    geom_point(size = 2.5) +\n    scale_x_continuous(breaks = 0:9)\nEsaminiamo ora i dati di tutti i 18 soggetti.\nggplot(sleepstudy, aes(x = Days, y = Reaction)) +\n    geom_point() +\n    scale_x_continuous(breaks = 0:9) +\n    facet_wrap(~Subject)",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modelli multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/raters/01_multilevel.html#introduzione",
    "href": "chapters/raters/01_multilevel.html#introduzione",
    "title": "11  Modelli multilivello",
    "section": "",
    "text": "Reaction: Average reaction time (ms)\nDays: Number of days of sleep deprivation\nSubject: Subject number on which the observation was made.\n\n\n\n\n\n\n\n\n\n11.1.0.1 Descrizione del Disegno Sperimentale\n\nFase di Adattamento e Baseline: I primi tre giorni dello studio (T1, T2 e B) sono stati utilizzati per l’adattamento e l’addestramento (T1 e T2), seguiti da una misurazione baseline (B). Durante questo periodo, ai soggetti è stato chiesto di rimanere a letto per 8 ore (dalle 23:00 alle 07:00).\nCondizioni di Sonno: Dal quarto giorno in poi, per sette giorni (E1-E7), i soggetti hanno sperimentato diverse condizioni di sonno, variando la durata del tempo a letto (TIB) da 3 a 9 ore.\n\nI primi due giorni (codificati come 0 e 1) sono stati dedicati all’adattamento e all’addestramento, mentre il terzo giorno (codificato come 2) ha visto la misurazione baseline. L’analisi dovrebbe idealmente partire dal giorno di baseline per riflettere l’effetto della restrizione del sonno sulle prestazioni. Per evitare che l’adattamento influenzi i risultati, i giorni 0 e 1 devono dunque essere esclusi dall’analisi, poiché qualsiasi variazione di prestazione in questi giorni è attribuibile all’addestramento piuttosto che alla restrizione del sonno.\n\n\n11.1.0.2 Preparazione dei Dati\n\nRimozione delle Osservazioni Iniziali: Dal dataset, eliminiamo le osservazioni dove la variabile “Days” è codificata come 0 o 1.\nCreazione di una Nuova Variabile “days_deprived”: Creiamo una nuova variabile basata su “Days”, iniziando la sequenza dal giorno 2. In questa nuova variabile, il giorno 2 viene ricodificato come 0, il giorno 3 come 1, e così via. Questa variabile rappresenta il numero di giorni di privazione del sonno. Salviamo il dataset modificato con il nome sleep2, che ora riflette accuratamente il periodo di restrizione del sonno per l’analisi.\n\n\nsleep2 &lt;- sleepstudy |&gt;\n    filter(Days &gt;= 2L) |&gt;\n    mutate(days_deprived = Days - 2L)\n\n\nsleep2 |&gt;\n    count(days_deprived, Days)\n\n\nA data.frame: 8 x 3\n\n\ndays_deprived\nDays\nn\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n\n\n\n\n0\n2\n18\n\n\n1\n3\n18\n\n\n2\n4\n18\n\n\n3\n5\n18\n\n\n4\n6\n18\n\n\n5\n7\n18\n\n\n6\n8\n18\n\n\n7\n9\n18\n\n\n\n\n\n\nggplot(sleep2, aes(x = days_deprived, y = Reaction)) +\n    geom_point() +\n    scale_x_continuous(breaks = 0:7) +\n    facet_wrap(~Subject) +\n    labs(y = \"Reaction Time\", x = \"Days deprived of sleep (0 = baseline)\")\n\n\n\n\n\n\n\n\n\n\n11.1.1 Analisi della Relazione tra Tempo di Reazione e Privazione del Sonno\nNel contesto dello studio sulla privazione del sonno, l’analisi dei dati suggerisce che, a parte una singola eccezione (il soggetto 335), il tempo di reazione medio tende ad aumentare progressivamente con ogni giorno aggiuntivo di privazione del sonno. Questo pattern indica che potrebbe essere utile descrivere le prestazioni di ciascun partecipante attraverso un modello di regressione lineare.\nLa regressione lineare è rappresentata dall’equazione generale:\n\\[ E(Y) = \\beta_0 + \\beta_1 X, \\]\ndove $ Y $ è la variabile dipendente (in questo caso, il tempo di reazione), $ _0 $ rappresenta l’intercetta (il tempo di reazione medio al giorno zero, prima dell’inizio della privazione del sonno) e $ _1 $ è la pendenza (la variazione del tempo di reazione per ogni giorno aggiuntivo di privazione del sonno). Questi parametri ($ _0 $ e $ _1 $) sono stimati dai dati.\nQuando si modellano i dati di ciascun partecipante, emergono diverse domande: dobbiamo adattare lo stesso modello di regressione lineare a tutti i partecipanti, o sarebbe più appropriato utilizzare un modello diverso per ogni soggetto? Oppure esiste un approccio intermedio che bilancia questi estremi?\nPer rispondere a queste domande, esploriamo tre approcci differenti, come illustrato da {cite:t}McElreath_rethinking:\n\nComplete Pooling: Questo approccio implica l’utilizzo di un unico modello di regressione lineare per tutti i partecipanti. Significa che assumiamo la stessa relazione lineare (stessa intercetta e pendenza) per tutti, ignorando le differenze individuali.\nNo Pooling: In questo approccio, ogni partecipante ha un proprio modello di regressione lineare individuale, con intercetta e pendenza uniche. Qui si riconosce che ogni individuo può rispondere diversamente alla privazione del sonno, e quindi il modello è personalizzato per ciascun soggetto.\nPartial Pooling: Questo approccio intermedio cerca di bilanciare gli estremi dei due metodi precedenti. Include alcuni elementi comuni tra i soggetti (ad esempio, una pendenza media) ma permette anche una certa variazione individuale.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modelli multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/raters/01_multilevel.html#complete-pooling",
    "href": "chapters/raters/01_multilevel.html#complete-pooling",
    "title": "11  Modelli multilivello",
    "section": "11.2 Complete pooling",
    "text": "11.2 Complete pooling\nL’approccio di “complete pooling” in analisi statistica implica l’utilizzo di un modello che calcola un’unica intercetta e una sola pendenza per l’intero dataset. Questo metodo si basa sull’ipotesi che tutti i soggetti nel dataset condividano le stesse caratteristiche di base riguardo alla relazione tra la variabile dipendente e indipendente.\n\n11.2.0.1 Caratteristiche del Complete Pooling\n\nUnicità delle Stime: Il modello stima un singolo set di parametri (intercetta e pendenza) per tutti i dati, considerando l’intero campione come un’unità omogenea.\nIgnorare le Variazioni Individuali: Questo approccio non tiene conto delle possibili differenze individuali nelle intercette o nelle pendenze tra i diversi soggetti. Ad esempio, ignorara come ciascun soggetto reagisce in modo diverso alla privazione del sonno.\n\n\n\n11.2.0.2 Limitazioni dell’Approccio di Complete Pooling\nDall’analisi preliminare dei dati, abbiamo notato che l’approccio di complete pooling potrebbe non essere adatto per il nostro studio. La visualizzazione dei dati suggerisce che ogni partecipante potrebbe avere una propria relazione unica tra il tempo di reazione e i giorni di privazione del sonno, indicando la necessità di valori individuali per le intercette e le pendenze.\n\n\n11.2.0.3 Modello di Regressione Lineare in Complete Pooling\nIl modello generale lineare (GLM) per l’approccio di complete pooling è formulato come segue:\n\\[Y_{sd} = \\beta_0 + \\beta_1 X_{sd} + e_{sd},\\]\ndove: - $ Y_{sd} $ rappresenta il tempo di reazione medio del soggetto $ s $ nel giorno $ d $. - $ X_{sd} $ è il numero di giorni di privazione del sonno (variabile “days_deprived”), che varia da 0 a 7. - $ e_{sd} $ è il termine di errore, che rappresenta le fluttuazioni casuali non spiegate dal modello.\n\n\n11.2.0.4 Implementazione in R\nPer adattare questo modello in R, si utilizza la funzione lm():\n\ncp_model &lt;- lm(Reaction ~ days_deprived, sleep2)\nsummary(cp_model)\n\n\nCall:\nlm(formula = Reaction ~ days_deprived, data = sleep2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-112.284  -26.732    2.143   27.734  140.453 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    267.967      7.737  34.633  &lt; 2e-16 ***\ndays_deprived   11.435      1.850   6.183 6.32e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 50.85 on 142 degrees of freedom\nMultiple R-squared:  0.2121,    Adjusted R-squared:  0.2066 \nF-statistic: 38.23 on 1 and 142 DF,  p-value: 6.316e-09\n\n\n\n\n11.2.1 Interpretazione del Modello di Regressione e Visualizzazione Grafica\nIl modello di regressione che abbiamo considerato offre una stima del tempo di risposta medio per i soggetti allo studio al Giorno 0 (prima della privazione del sonno) e la variazione media del tempo di risposta per ogni giorno aggiuntivo di privazione. Secondo questo modello, il tempo di risposta medio iniziale è stimato essere di circa 268 millisecondi, con un incremento medio di circa 11 millisecondi per ogni giorno successivo di privazione del sonno.\n\n11.2.1.1 Considerazioni sui Limiti del Modello\nÈ importante notare, tuttavia, che questo modello potrebbe avere delle limitazioni nella sua applicabilità: - Assunzione di Indipendenza: Il modello assume che tutte le osservazioni siano indipendenti. Questa assunzione potrebbe non essere valida nel nostro studio, dato che le osservazioni provengono da misurazioni ripetute sugli stessi soggetti. - Errori Standard dei Coefficienti: La presunta indipendenza delle osservazioni implica che gli errori standard dei coefficienti di regressione potrebbero non essere completamente affidabili.\n\n\n11.2.1.2 Aggiunta delle Previsioni al Grafico\nPer visualizzare meglio questi risultati, possiamo aggiungere le previsioni del modello al grafico che abbiamo già creato. Utilizziamo la funzione geom_abline() di R per tracciare la linea di regressione stimata direttamente sul grafico esistente: - Utilizzo di geom_abline(): Questa funzione ci permette di aggiungere una linea di regressione al grafico, specificando l’intercetta e la pendenza. - Coefficienti del Modello: Utilizziamo coef(cp_model) per ottenere i coefficienti di regressione (intercetta e pendenza) dal nostro modello. Questa funzione restituisce un vettore con due elementi corrispondenti all’intercetta e alla pendenza, che possono essere poi utilizzati per definire la linea nel grafico.\nIn conclusione, l’aggiunta delle previsioni del modello al grafico fornisce una rappresentazione visiva dell’effetto stimato della privazione del sonno sul tempo di risposta. Tuttavia, è essenziale tenere presente le limitazioni e le assunzioni del modello, in particolare l’indipendenza delle osservazioni, che potrebbe non essere una rappresentazione accurata dei dati nel contesto di misurazioni ripetute.\n\nggplot(sleep2, aes(x = days_deprived, y = Reaction)) +\n    geom_abline(\n        intercept = coef(cp_model)[1],\n        slope = coef(cp_model)[2],\n        color = \"blue\"\n    ) +\n    geom_point() +\n    scale_x_continuous(breaks = 0:7) +\n    facet_wrap(~Subject) +\n    labs(y = \"Reaction Time\", x = \"Days deprived of sleep (0 = baseline)\")\n\n\n\n\n\n\n\n\nDall’analisi effettuata, emerge che il modello attuale non si adatta in modo ottimale ai dati raccolti. Questa situazione indica la necessità di esplorare un approccio diverso per modellare in modo più accurato le relazioni presenti nei dati.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modelli multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/raters/01_multilevel.html#approccio-di-no-pooling",
    "href": "chapters/raters/01_multilevel.html#approccio-di-no-pooling",
    "title": "11  Modelli multilivello",
    "section": "11.3 Approccio di No Pooling",
    "text": "11.3 Approccio di No Pooling\nIn alternativa al modello di “complete pooling”, consideriamo l’approccio di “no pooling”. Questo approccio si basa sull’idea di adattare modelli di regressione separati per ogni partecipante, trattando ogni individuo come un’entità distinta.\n\n11.3.1 Caratteristiche del No Pooling\n\nIndipendenza delle Stime: In questo approccio, ogni partecipante ha il proprio set di stime per l’intercetta e la pendenza. Le stime relative a un partecipante non sono influenzate dalle stime degli altri.\nStime Individualizzate: Si stima separatamente una coppia di intercetta/pendenza per ciascuno dei 18 partecipanti, riconoscendo la possibilità di variazioni significative nelle risposte individuali.\n\n\n\n11.3.2 Implementazione del Modello di No Pooling\nEsistono due modi principali per implementare questo approccio: 1. Regressioni Separate per Ogni Partecipante: Eseguire una serie di regressioni lineari individuali, una per ogni soggetto. 2. Modello di Regressione Unificato con Effetti Principali e Interazione: Utilizzare un unico modello di regressione che includa sia gli effetti principali sia l’interazione tra le variabili Subject (soggetto) e Day (giorno). Questo metodo permette di includere tutte le stime in un unico modello.\nPer il secondo approccio, è necessario considerare le seguenti fasi: - Creazione di Variabili Dummy per il Fattore Subject: Poiché Subject ha 18 livelli, saranno necessarie 17 variabili dummy per rappresentare questi livelli. In R, questo può essere fatto automaticamente definendo Subject come un fattore. - Includere Subject come Fattore nel Modello: Aggiungere Subject, definito come un fattore, come predittore nel modello. L’inclusione dell’interazione tra Subject e days_deprived permette variazioni nelle intercette e nelle pendenze tra i soggetti.\n\n\n11.3.3 Verifica del Fattore Subject\nPrima di procedere, è importante assicurarsi che Subject sia definito correttamente come un fattore. Questo può essere verificato utilizzando la funzione summary() in R, che fornisce una sintesi delle caratteristiche della variabile, compreso se è trattata come un fattore.\n\nsleep2 %&gt;% \n    summary()\n\n    Reaction          Days         Subject   days_deprived \n Min.   :203.0   Min.   :2.00   308    : 8   Min.   :0.00  \n 1st Qu.:265.2   1st Qu.:3.75   309    : 8   1st Qu.:1.75  \n Median :303.2   Median :5.50   310    : 8   Median :3.50  \n Mean   :308.0   Mean   :5.50   330    : 8   Mean   :3.50  \n 3rd Qu.:347.7   3rd Qu.:7.25   331    : 8   3rd Qu.:5.25  \n Max.   :466.4   Max.   :9.00   332    : 8   Max.   :7.00  \n                                (Other):96                 \n\n\nLa funzione pull() viene utilizzata per estrarre una specifica colonna da un data frame. Con le seguenti istruzioni verifichiamo se la colonna Subject è codificata come factor.\n\nsleep2 |&gt;\n    pull(Subject) |&gt;\n    is.factor()\n\nTRUE\n\n\nAdattiamo il modello di regressione ai dati. Si noti che la sintassi seguente può essere semplificata utilizzando Reaction ~ days_deprived * Subject.\n\nnp_model &lt;- lm(Reaction ~ days_deprived + Subject + days_deprived:Subject,\n    data = sleep2\n)\n\nsummary(np_model)\n\n\nCall:\nlm(formula = Reaction ~ days_deprived + Subject + days_deprived:Subject, \n    data = sleep2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-106.521   -8.541    1.143    8.889  128.545 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              288.2175    16.4772  17.492  &lt; 2e-16 ***\ndays_deprived             21.6905     3.9388   5.507 2.49e-07 ***\nSubject309               -87.9262    23.3023  -3.773 0.000264 ***\nSubject310               -62.2856    23.3023  -2.673 0.008685 ** \nSubject330               -14.9533    23.3023  -0.642 0.522422    \nSubject331                 9.9658    23.3023   0.428 0.669740    \nSubject332                27.8157    23.3023   1.194 0.235215    \nSubject333                -2.7581    23.3023  -0.118 0.906000    \nSubject334               -50.2051    23.3023  -2.155 0.033422 *  \nSubject335               -25.3429    23.3023  -1.088 0.279207    \nSubject337                24.6143    23.3023   1.056 0.293187    \nSubject349               -59.2183    23.3023  -2.541 0.012464 *  \nSubject350               -40.2023    23.3023  -1.725 0.087343 .  \nSubject351               -24.2467    23.3023  -1.041 0.300419    \nSubject352                43.0655    23.3023   1.848 0.067321 .  \nSubject369               -21.5040    23.3023  -0.923 0.358154    \nSubject370               -53.3072    23.3023  -2.288 0.024107 *  \nSubject371               -30.4896    23.3023  -1.308 0.193504    \nSubject372                 2.4772    23.3023   0.106 0.915535    \ndays_deprived:Subject309 -17.3334     5.5703  -3.112 0.002380 ** \ndays_deprived:Subject310 -17.7915     5.5703  -3.194 0.001839 ** \ndays_deprived:Subject330 -13.6849     5.5703  -2.457 0.015613 *  \ndays_deprived:Subject331 -16.8231     5.5703  -3.020 0.003154 ** \ndays_deprived:Subject332 -19.2947     5.5703  -3.464 0.000765 ***\ndays_deprived:Subject333 -10.8151     5.5703  -1.942 0.054796 .  \ndays_deprived:Subject334  -3.5745     5.5703  -0.642 0.522423    \ndays_deprived:Subject335 -25.8995     5.5703  -4.650 9.47e-06 ***\ndays_deprived:Subject337   0.7518     5.5703   0.135 0.892895    \ndays_deprived:Subject349  -5.2644     5.5703  -0.945 0.346731    \ndays_deprived:Subject350   1.6007     5.5703   0.287 0.774382    \ndays_deprived:Subject351 -13.1681     5.5703  -2.364 0.019867 *  \ndays_deprived:Subject352 -14.4019     5.5703  -2.585 0.011057 *  \ndays_deprived:Subject369  -7.8948     5.5703  -1.417 0.159273    \ndays_deprived:Subject370  -1.0495     5.5703  -0.188 0.850912    \ndays_deprived:Subject371  -9.3443     5.5703  -1.678 0.096334 .  \ndays_deprived:Subject372 -10.6041     5.5703  -1.904 0.059613 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 25.53 on 108 degrees of freedom\nMultiple R-squared:  0.849, Adjusted R-squared:  0.8001 \nF-statistic: 17.35 on 35 and 108 DF,  p-value: &lt; 2.2e-16\n\n\nPer chiarire, il soggetto di riferimento è il 308; in R, la modalità predefinita è quella di ordinare i livelli del fattore in ordine alfabetico e di scegliere il primo come soggetto di riferimento. Questo significa che l’intercetta e la pendenza per il soggetto 308 sono rappresentate rispettivamente da (Intercept) e days_deprived, poiché tutte le altre 17 variabili dummy saranno nulle per il soggetto 308.\nTutti i coefficienti di regressione degli altri soggetti sono rappresentati come scostamenti da questo soggetto di riferimento. Se desideriamo calcolare l’intercetta e la pendenza per un dato soggetto, dobbiamo semplicemente sommare gli scostamenti corrispondenti. Pertanto, abbiamo:\nIntercetta per 308: 288.217\nPendenza per 308: 21.69\nIntercetta per 335: (Intercept) + Subject335 = 288.217 + -25.343 = 262.874\nPendenza per 335: days_deprived + days_deprived:Subject335 = 21.69 + -25.899 = -4.209\nE così via.\nNel modello “no pooling”, non viene stimata un’intercetta e una pendenza complessive per l’intera popolazione; in questo caso, (Intercept) e days_deprived sono stime dell’intercetta e della pendenza per il soggetto 308, che è stato scelto (arbitrariamente) come soggetto di riferimento. Per ottenere stime per l’intera popolazione, è possibile procedere con una seconda fase dell’analisi statistica in cui calcoliamo le medie delle intercette e delle pendenze individuali.\n\ncoef(np_model) |&gt; as.data.frame()\n\n\nA data.frame: 36 x 1\n\n\n\ncoef(np_model)\n\n\n\n&lt;dbl&gt;\n\n\n\n\n(Intercept)\n288.2174667\n\n\ndays_deprived\n21.6904952\n\n\nSubject309\n-87.9262083\n\n\nSubject310\n-62.2856250\n\n\nSubject330\n-14.9532917\n\n\nSubject331\n9.9658167\n\n\nSubject332\n27.8157333\n\n\nSubject333\n-2.7581167\n\n\nSubject334\n-50.2051167\n\n\nSubject335\n-25.3428833\n\n\nSubject337\n24.6143167\n\n\nSubject349\n-59.2183250\n\n\nSubject350\n-40.2022833\n\n\nSubject351\n-24.2467083\n\n\nSubject352\n43.0654583\n\n\nSubject369\n-21.5040417\n\n\nSubject370\n-53.3072333\n\n\nSubject371\n-30.4895750\n\n\nSubject372\n2.4772250\n\n\ndays_deprived:Subject309\n-17.3333512\n\n\ndays_deprived:Subject310\n-17.7915000\n\n\ndays_deprived:Subject330\n-13.6848524\n\n\ndays_deprived:Subject331\n-16.8231262\n\n\ndays_deprived:Subject332\n-19.2946845\n\n\ndays_deprived:Subject333\n-10.8151060\n\n\ndays_deprived:Subject334\n-3.5745167\n\n\ndays_deprived:Subject335\n-25.8994869\n\n\ndays_deprived:Subject337\n0.7517702\n\n\ndays_deprived:Subject349\n-5.2643643\n\n\ndays_deprived:Subject350\n1.6007274\n\n\ndays_deprived:Subject351\n-13.1680905\n\n\ndays_deprived:Subject352\n-14.4018881\n\n\ndays_deprived:Subject369\n-7.8948310\n\n\ndays_deprived:Subject370\n-1.0494833\n\n\ndays_deprived:Subject371\n-9.3442786\n\n\ndays_deprived:Subject372\n-10.6041357\n\n\n\n\n\nCalcoliamo le intercette individuali:\n\nall_intercepts &lt;- c(\n    coef(np_model)[\"(Intercept)\"],\n    coef(np_model)[3:19] + coef(np_model)[\"(Intercept)\"]\n)\n\nCalcliamo le pendenze individuali:\n\nall_slopes &lt;- c(\n    coef(np_model)[\"days_deprived\"],\n    coef(np_model)[20:36] + coef(np_model)[\"days_deprived\"]\n)\n\nCreiamo un DataFrame con le colonne Subject, intercept e slope:\n\nids &lt;- sleep2 |&gt;\n    pull(Subject) |&gt;\n    levels() |&gt;\n    factor()\nprint(ids)\n\n [1] 308 309 310 330 331 332 333 334 335 337 349 350 351 352 369 370 371 372\n18 Levels: 308 309 310 330 331 332 333 334 335 337 349 350 351 352 369 ... 372\n\n\n\n# make a tibble with the data extracted above\nnp_coef &lt;- tibble(\n    Subject = ids,\n    intercept = all_intercepts,\n    slope = all_slopes\n)\n\nprint(np_coef)\n\n# A tibble: 18 x 3\n   Subject intercept slope\n   &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1 308          288. 21.7 \n 2 309          200.  4.36\n 3 310          226.  3.90\n 4 330          273.  8.01\n 5 331          298.  4.87\n 6 332          316.  2.40\n 7 333          285. 10.9 \n 8 334          238. 18.1 \n 9 335          263. -4.21\n10 337          313. 22.4 \n11 349          229. 16.4 \n12 350          248. 23.3 \n13 351          264.  8.52\n14 352          331.  7.29\n15 369          267. 13.8 \n16 370          235. 20.6 \n17 371          258. 12.3 \n18 372          291. 11.1 \n\n\nEsaminiamo l’adattamento di questo modello ai dati.\n\nggplot(sleep2, aes(x = days_deprived, y = Reaction)) +\n    geom_abline(\n        data = np_coef,\n        mapping = aes(\n            intercept = intercept,\n            slope = slope\n        ),\n        color = \"blue\"\n    ) +\n    geom_point() +\n    scale_x_continuous(breaks = 0:7) +\n    facet_wrap(~Subject) +\n    labs(y = \"Reaction Time\", x = \"Days deprived of sleep (0 = baseline)\")\n\n\n\n\n\n\n\n\nQuesta situazione è notevolmente migliorata rispetto al modello di pooling completo. Se desideriamo testare l’ipotesi nulla secondo cui la pendenza della retta di regressione è uguale a zero, possiamo farlo eseguendo un test \\(t\\) di Student sul campione di pendenze individuali.\n\nnp_coef |&gt;\n    pull(slope) |&gt;\n    t.test()\n\n\n    One Sample t-test\n\ndata:  pull(np_coef, slope)\nt = 6.1971, df = 17, p-value = 9.749e-06\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n  7.542244 15.328613\nsample estimates:\nmean of x \n 11.43543 \n\n\nQuesto test suggerisce che la pendenza media di 11.435 è diversa da zero, t(17) = 6.20 p &lt; .001.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modelli multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/raters/01_multilevel.html#approccio-di-partial-pooling",
    "href": "chapters/raters/01_multilevel.html#approccio-di-partial-pooling",
    "title": "11  Modelli multilivello",
    "section": "11.4 Approccio di Partial Pooling",
    "text": "11.4 Approccio di Partial Pooling\nNell’ambito dell’analisi dei dati psicologici, i ricercatori si trovano spesso a dover gestire un delicato equilibrio. Da un lato, vi è l’approccio di complete pooling, che tratta tutti i dati come se appartenessero a un unico gruppo omogeneo, e dall’altro, l’approccio di no-pooling, che considera i dati di ciascun soggetto in modo isolato, senza sfruttare le informazioni aggregate. Entrambi gli estremi hanno limitazioni significative: il complete pooling può mascherare le variazioni individuali, mentre il no-pooling può non sfruttare appieno le informazioni disponibili dall’intero set di dati.\nPer superare queste limitazioni, emerge l’approccio di partial pooling. Questo metodo utilizza i modelli lineari a effetti misti, che rappresentano una via di mezzo tra i due estremi menzionati. Il partial pooling permette di trarre vantaggio dalle informazioni provenienti dall’insieme dei partecipanti, migliorando così le stime dei soggetti individuali. Attraverso questo approccio, si ottengono stime che non solo tengono conto delle peculiarità di ciascun individuo ma sono anche informate dalle tendenze generali osservate nel gruppo più ampio.\nL’approccio di partial pooling permette di separare più efficacemente le tendenze generali dagli errori casuali in ciascun partecipante e si dimostra più adatto per generalizzare i risultati a una popolazione più ampia, al di là dei soggetti specifici coinvolti nello studio.\n\n11.4.1 Implementazione dei Modelli a Effetti Misti\n\nTrattare i Soggetti come Fattori Casuali: Nel partial pooling, i soggetti vengono considerati come un fattore casuale anziché fisso. Ciò implica che i livelli del fattore (i soggetti nel nostro caso) sono visti come un campione casuale da una popolazione più ampia.\nModello Lineare a Effetti Misti: Questo tipo di modello statistico consente di includere i fattori casuali nell’analisi. In un modello misto, le stime per ogni soggetto sono “informate” o influenzate dalle informazioni aggregate degli altri soggetti.\nShrinkage o Restringimento: Il fenomeno dello shrinkage indica che le stime per ciascun soggetto vengono regolate o “spostate” verso le stime medie della popolazione, permettendo una valutazione più equilibrata e meno influenzata da variazioni estreme o casuali.\n\n\n\n11.4.2 Articolazione e Applicazione del Modello Multilivello\nIl modello multilivello che analizziamo è strutturato per cogliere le relazioni dinamiche tra variabili a livelli diversi. Esaminiamo i dettagli di ogni livello e il loro significato nel contesto del modello.\n\n11.4.2.1 Livello 1: Modellazione della Relazione Individuale\nIl primo livello del modello esprime la relazione lineare individuale tra la variabile di risposta (tempo di reazione) e i predittori (giorni di privazione del sonno):\n\\[\nY_{sd} = \\beta_{0s} + \\beta_{1s} X_{sd} + e_{sd},\n\\]\ndove \\(Y_{sd}\\) è il tempo di reazione del soggetto \\(s\\) al giorno \\(d\\), \\(\\beta_{0s}\\) e \\(\\beta_{1s}\\) sono i parametri individuali di intercetta e pendenza, e \\(e_{sd}\\) rappresenta l’errore per ogni soggetto e giorno. I parametri \\(\\beta_{0s}\\) e \\(\\beta_{1s}\\) sono considerati derivati, poiché dipendono dalle variabili a Livello 2.\n\n\n11.4.2.2 Livello 2: Modellazione delle Variazioni Tra Soggetti\nAl secondo livello, definiamo come l’intercetta e la pendenza cambiano tra i soggetti:\n\\[\n\\beta_{0s} = \\gamma_{0} + S_{0s},\n\\]\n\\[\n\\beta_{1s} = \\gamma_{1} + S_{1s}.\n\\]\nQui, \\(\\gamma_0\\) e \\(\\gamma_1\\) sono gli effetti fissi che rappresentano l’intercetta e la pendenza medie della popolazione, mentre \\(S_{0s}\\) e \\(S_{1s}\\) sono gli effetti casuali che permettono variazioni individuali.\n\n\n11.4.2.3 Componenti di Varianza\nLe componenti di varianza nel modello sono espresse come:\n\\[\n\\langle S_{0s}, S_{1s} \\rangle \\sim N(\\langle 0, 0 \\rangle, \\mathbf{\\Sigma}),\n\\]\n\\[\n\\mathbf{\\Sigma} = \\begin{pmatrix}{\\tau_{00}}^2 & \\rho\\tau_{00}\\tau_{11} \\\\ \\rho\\tau_{00}\\tau_{11} & {\\tau_{11}}^2 \\end{pmatrix},\n\\]\n\\[\ne_{sd} \\sim N(0, \\sigma^2).\n\\]\nLa matrice \\(\\mathbf{\\Sigma}\\) determina la distribuzione degli effetti casuali, con \\({\\tau_{00}}^2\\) e \\({\\tau_{11}}^2\\) che indicano le varianze delle intercette e delle pendenze casuali, mentre \\(\\rho\\) è la correlazione tra questi effetti.\n\n\n11.4.2.4 Interpretazione Complessiva del Modello\nUnendo le equazioni di Livello 1 e Livello 2, otteniamo una visione completa del modello:\n\\[\nY_{sd} = \\gamma_0 + S_{0s} + (\\gamma_1 + S_{1s}) X_{sd} + e_{sd},\n\\]\ndove\n\n$ Y_{sd} $: È il valore osservato della variabile di risposta per il soggetto $ s $ al giorno $ d $.\n$ _0 $ e $ _1 $: Sono gli effetti fissi. $ _0 $ rappresenta l’intercetta generale, ovvero la media della popolazione per la variabile di risposta quando la variabile predittiva $ X $ è zero. $ _1 $ rappresenta la pendenza generale, che indica come la variabile di risposta cambia in media con un’unità di incremento della variabile predittiva $ X $.\n$ S_{0s} $ e $ S_{1s} $: Sono gli effetti casuali. $ S_{0s} $ è l’effetto casuale dell’intercetta per il soggetto $ s $, mentre $ S_{1s} $ è l’effetto casuale della pendenza. Questi termini rappresentano come ciascun soggetto si discosta dalla media della popolazione.\n$ X_{sd} $: È la variabile predittiva per il soggetto $ s $ al giorno $ d $.\n$ e_{sd} $: È l’errore casuale associato a ciascuna osservazione.\n\n\n\n11.4.2.5 Interpretazione degli Effetti Fissi e Casuali\n\nGli effetti fissi ($ _0 $ e $ _1 $) sono costanti per tutti i soggetti e riflettono le caratteristiche della popolazione generale.\nGli effetti casuali ($ S_{0s} $ e $ S_{1s} $) variano da soggetto a soggetto e sono modellati come distribuzioni normali centrate attorno a zero. Questo significa che la distribuzione degli effetti casuali è centrata attorno agli effetti fissi della popolazione.\n\n\n\n11.4.2.6 Uso di Campioni per Informare sulla Popolazione\nTrattare i soggetti come variabili casuali anziché fisse ci consente di usare i dati del campione per inferire sulla popolazione più ampia. Invece di stimare valori specifici per ogni soggetto, il modello stima la distribuzione da cui questi valori sono estratti, fornendo una visione più generalizzata e applicabile a livello di popolazione.\n\n\n\n11.4.3 Spiegazione della Matrice di Varianza-Covarianza nel Modello Multilivello\nLa matrice di varianza-covarianza nel modello multilivello gioca un ruolo cruciale nel definire come gli effetti casuali variano e sono correlati tra loro all’interno della popolazione studiata. Questa matrice è rappresentata come segue:\n\\[\n\\Sigma =\n\\begin{pmatrix}\n\\tau_{00}^2 & \\rho \\tau_{00} \\tau_{11} \\\\\n\\rho \\tau_{00} \\tau_{11} & \\tau_{11}^2\n\\end{pmatrix},\n\\]\ndove \\(\\Sigma\\) denota la matrice di varianza-covarianza per gli effetti casuali \\(\\langle S_{0s}, S_{1s} \\rangle\\), che sono modellati come normalmente distribuiti con una media di zero e una varianza-covarianza data da \\(\\Sigma\\).\n\n11.4.3.1 Componenti della Matrice\n\nVarianze degli Effetti Casuali:\n\n\\(\\tau_{00}^2\\): Varianza dell’effetto casuale sull’intercetta (\\(S_{0s}\\)). Questo parametro indica quanto i soggetti differiscono nella loro reazione iniziale (tempo di reazione al Giorno 0) prima di qualsiasi privazione del sonno.\n\\(\\tau_{11}^2\\): Varianza dell’effetto casuale sulla pendenza (\\(S_{1s}\\)). Misura la variazione tra i soggetti nella loro reattività agli effetti della privazione del sonno.\n\nCovarianze:\n\n\\(\\rho \\tau_{00} \\tau_{11}\\): Covarianza tra l’intercetta e la pendenza casuali. Questo termine riflette come la reazione iniziale di un soggetto (intercetta) sia correlata con la variazione della sua reattività al sonno (pendenza). Un valore positivo di \\(\\rho\\) indica che soggetti con un tempo di reazione inizialmente più lento tendono a mostrare un aumento più marcato del tempo di reazione con la privazione del sonno, e viceversa.\n\n\n\n\n11.4.3.2 Significato e Importanza di \\(\\Sigma\\)\n\nDistribuzione degli Effetti Casuali: La matrice \\(\\Sigma\\) determina le probabilità di estrarre una coppia specifica di effetti casuali (\\(S_{0s}, S_{1s}\\)) per un soggetto dal pool più ampio della popolazione.\nAnalisi delle Variazioni Individuali: Attraverso \\(\\Sigma\\), possiamo comprendere meglio quanto e in che modo i soggetti variano sia nella loro reazione iniziale sia nella loro risposta alla privazione del sonno.\nInterpretazione dei Risultati: La comprensione di \\(\\Sigma\\) aiuta a interpretare i risultati del modello in termini di varianza e covarianza tra i soggetti, permettendo di trarre conclusioni più precise sulla popolazione studiata.\n\nLa matrice di varianza-covarianza \\(\\Sigma\\) è fondamentale nel modello multilivello perché fornisce un quadro dettagliato della varianza e della covarianza degli effetti casuali, consentendo di cogliere le sottili variazioni e correlazioni tra i soggetti. Questa comprensione arricchisce l’analisi e rende possibili conclusioni più accurate sulla popolazione.\n\n\n\n11.4.4 Tabella delle Variabili\n\n\n\n\n\n\n\n\nVariabile\nTipo\nDescrizione\n\n\n\n\n\\(Y_{sd}\\)\nOsservata\nValore di Reaction per il soggetto \\(s\\) al giorno \\(d\\)\n\n\n\\(X_{sd}\\)\nOsservata\nValore di days_deprived (0-7) per il soggetto \\(s\\) al giorno \\(d\\)\n\n\n\\(\\beta_{0s}\\)\nDerivata\nParametro di intercetta di livello 1 per il soggetto \\(s\\)\n\n\n\\(\\beta_{1s}\\)\nDerivata\nParametro di pendenza di livello 1 per il soggetto \\(s\\)\n\n\n\\(e_{sd}\\)\nDerivata\nErrore per il soggetto \\(s\\) al giorno \\(d\\)\n\n\n\\(\\gamma_0\\)\nFissa\nIntersezione generale (media di \\(\\beta_{0s}\\) nella popolazione)\n\n\n\\(\\gamma_1\\)\nFissa\nPendenza generale (media di \\(\\beta_{1s}\\) nella popolazione)\n\n\n\\(S_{0s}\\)\nCasuale\nEffetto random di intercetta per il soggetto \\(s\\)\n\n\n\\(S_{1s}\\)\nCasuale\nEffetto random di pendenza per il soggetto \\(s\\)\n\n\n\\(\\mathbf{\\Sigma}\\)\nCasuale\nMatrice di varianza-covarianza\n\n\n\\({\\tau_{00}}^2\\)\nCasuale\nVarianza degli effetti random di intercetta\n\n\n\\(\\rho\\)\nCasuale\nCorrelazione tra intercetta e pendenza\n\n\n\\({\\tau_{11}}^2\\)\nCasuale\nVarianza degli effetti random di pendenza\n\n\n\\(\\sigma^2\\)\nCasuale\nVarianza dell’errore\n\n\n\n\n11.4.4.1 Spiegazione delle Categorie di Variabili nel Modello Multilivello\nNella tabella delle variabili del modello multilivello, utilizziamo tre categorie specifiche per classificare le diverse variabili: fisso, casuale e derivato. Queste categorie ci aiutano a comprendere il ruolo e la natura di ciascuna variabile all’interno del modello:\n\n11.4.4.1.1 Variabili Fisse\n\nDefinizione: Le variabili fisse sono quelle che assumiamo costanti attraverso il campione e la popolazione. Sono parametri che rappresentano le caratteristiche generali della popolazione da cui il campione è tratto.\nEsempi: Nella tabella, \\(\\gamma_0\\) (intercetta generale) e \\(\\gamma_1\\) (pendenza generale) sono variabili fisse.\nRuolo nel Modello: Riflettono le tendenze centrali o gli effetti medi nella popolazione oggetto di studio.\n\n\n\n11.4.4.1.2 Variabili Casuali\n\nDefinizione: Le variabili casuali indicano i parametri che possono variare tra i soggetti o altre unità di analisi. Questi parametri sono concepiti come estratti da una distribuzione più ampia.\nEsempi: \\(S_{0s}\\) (intercetta casuale per soggetto) e \\(S_{1s}\\) (pendenza casuale per soggetto) sono esempi di variabili casuali.\nRuolo nel Modello: Consentono di modellare e comprendere la varianza e la covarianza all’interno del campione, riflettendo la variabilità individuale o di gruppo.\n\n\n\n11.4.4.1.3 Variabili Derivate\n\nDefinizione: ‘Derivato’ non è un termine standard nella modellistica statistica, ma lo utilizziamo qui per distinguere le variabili che non sono direttamente stimabili, ma piuttosto calcolate o derivate da altre variabili nel modello.\nEsempi: \\(\\beta_{0s}\\) (intercetta di livello 1 per soggetto) e \\(\\beta_{1s}\\) (pendenza di livello 1 per soggetto) sono variabili derivate, calcolate combinando variabili fisse e casuali.\nRuolo nel Modello: Le variabili derivate rappresentano i parametri specifici per ciascun soggetto o unità di analisi, derivati dalla combinazione delle influenze fisse e casuali.\n\nIn conclusione, il modello misto permette di tenere conto della variazione sia a livello di popolazione sia a livello individuale. Questa capacità di distinguere tra variazioni generali e specifiche dei soggetti è cruciale in molte situazioni, come quando si analizzano effetti di trattamenti o condizioni sperimentali diverse su gruppi di soggetti.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modelli multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/raters/01_multilevel.html#stimare-i-parametri-del-modello",
    "href": "chapters/raters/01_multilevel.html#stimare-i-parametri-del-modello",
    "title": "11  Modelli multilivello",
    "section": "11.5 Stimare i parametri del modello",
    "text": "11.5 Stimare i parametri del modello\nPer stimare i parametri del modello usando R, utilizzeremo la funzione lmer() del pacchetto lme4 {cite:t}bates2014fitting. Questa funzione permette di specificare sia i fattori fissi (come i giorni di deprivazione del sonno) sia i fattori casuali (come i soggetti), ottenendo un modello che bilancia efficacemente le informazioni individuali con quelle aggregate.\nLa sintassi base di lmer() è la seguente:\n\\[ \\text{lmer(formula, data, ...)}, \\]\ndove formula esprime la struttura del modello sottostante in un formato compatto e data è il data frame in cui si trovano le variabili menzionate nella formula.\nIl formato generale della formula del modello per N effetti fissi (fix) e K effetti casuali (ran) è:\n\\[ \\text{DV ~ fix1 + fix2 + ... + fixN + (ran1 + ran2 + ... + ranK | random\\_factor1)} \\]\nLe interazioni tra i fattori A e B possono essere specificate utilizzando sia A * B (interazione ed effetti principali) che A:B (solo l’interazione).\nUna differenza chiave dalla sintassi standard dei modelli R è la presenza di un termine di effetto casuale, racchiuso tra parentesi, ad esempio (ran1 + ran2 + ... + ranK | random_factor). Ogni espressione tra parentesi rappresenta gli effetti casuali associati a un singolo fattore casuale. È possibile avere più di un termine di effetti casuali in una singola formula (fattori casuali incrociati). I termini relativi agli effetti casuali forniscono istruzioni a lmer() su come costruire le matrici di varianza-covarianza.\nSul lato sinistro della barra | vengono elencati gli effetti che vogliamo fare variare tra i livelli del fattore casuale indicato sul lato destro. Di solito, la variabile sul lato destro è una variabile che identifica i soggetti (ad esempio, subject_id).\nConsideriamo le seguenti possibili formule di modello per i dati sleep2 e le matrici di varianza-covarianza che esse costruiscono.\n\n\n\n\n\n\n\nmodel\nsyntax\n\n\n\n\n1. random intercepts only\nReaction ~ days_deprived + (1 | Subject)\n\n\n2. random intercepts and slopes\nReaction ~ days_deprived + (1 + days_deprived | Subject)\n\n\n3. model 2 alternate syntax\nReaction ~ days_deprived + (days_deprived | Subject)\n\n\n4. random slopes only\nReaction ~ days_deprived + (0 + days_deprived | Subject)\n\n\n5. model 2 + zero-covariances\nReaction ~ days_deprived + (days_deprived || Subject)\n\n\n\nModello 1:\n\\[ \\mathbf{\\Sigma} =\n\\begin{pmatrix}\n\\tau_{00}^2 & 0 \\\\\n0 & 0\n\\end{pmatrix}\n\\]\nModelli 2 e 3:\n\\[  \\mathbf{\\Sigma} =\n\\begin{pmatrix}\n\\tau_{00}^2 & \\rho \\tau_{00} \\tau_{11} \\\\\n\\rho \\tau_{00} \\tau_{11} & \\tau_{11}^2\n\\end{pmatrix}\n\\]\nModello 4:\n\\[  \\mathbf{\\Sigma} =\n\\begin{pmatrix}\n0 & 0 \\\\\n0 & \\tau_{11}^2\n\\end{pmatrix}\n\\]\nModello 5:\n\\[  \\mathbf{\\Sigma} =\n\\begin{pmatrix}\n\\tau_{00}^2 & 0 \\\\\n0 & \\tau_{11}^2\n\\end{pmatrix}\n\\]\nIl modello più ragionevole per i dati dell’esempio è il Modello 2, quindi useremo quello.\n\npp_mod &lt;- lmer(\n    Reaction ~ 1 + days_deprived + (1 + days_deprived | Subject), \n    data = sleep2\n)\nsummary(pp_mod)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Reaction ~ 1 + days_deprived + (1 + days_deprived | Subject)\n   Data: sleep2\n\nREML criterion at convergence: 1404.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.0157 -0.3541  0.0069  0.4681  5.0732 \n\nRandom effects:\n Groups   Name          Variance Std.Dev. Corr\n Subject  (Intercept)   958.35   30.957       \n          days_deprived  45.78    6.766   0.18\n Residual               651.60   25.526       \nNumber of obs: 144, groups:  Subject, 18\n\nFixed effects:\n              Estimate Std. Error t value\n(Intercept)    267.967      8.266  32.418\ndays_deprived   11.435      1.845   6.197\n\nCorrelation of Fixed Effects:\n            (Intr)\ndays_deprvd -0.062\n\n\nPrima di discutere come interpretare l’output, iniziamo col rappresentare graficamente i dati rispetto alle previsioni del nostro modello. Possiamo ottenere le previsioni del modello utilizzando la funzione predict() (vedi ?predict.merMod per informazioni sull’uso con modelli a effetti misti).\nPer prima cosa, creaiamo un nuovo dataframe con i valori dei predittori per Subject e days_deprived.\n\nhead(sleep2)\n\n\nA data.frame: 6 x 4\n\n\n\nReaction\nDays\nSubject\ndays_deprived\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;fct&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n250.8006\n2\n308\n0\n\n\n2\n321.4398\n3\n308\n1\n\n\n3\n356.8519\n4\n308\n2\n\n\n4\n414.6901\n5\n308\n3\n\n\n5\n382.2038\n6\n308\n4\n\n\n6\n290.1486\n7\n308\n5\n\n\n\n\n\n\nnewdata &lt;- crossing(\n    Subject = sleep2 |&gt; \n    pull(Subject) |&gt; \n    levels() |&gt; \n    factor(),\n    days_deprived = 0:7\n)\n\nhead(newdata, 17)\n\n\nA tibble: 17 x 2\n\n\nSubject\ndays_deprived\n\n\n&lt;fct&gt;\n&lt;int&gt;\n\n\n\n\n308\n0\n\n\n308\n1\n\n\n308\n2\n\n\n308\n3\n\n\n308\n4\n\n\n308\n5\n\n\n308\n6\n\n\n308\n7\n\n\n309\n0\n\n\n309\n1\n\n\n309\n2\n\n\n309\n3\n\n\n309\n4\n\n\n309\n5\n\n\n309\n6\n\n\n309\n7\n\n\n310\n0\n\n\n\n\n\nIl codice precedente crea un nuovo data frame chiamato newdata utilizzando la funzione crossing() da dplyr.\nSubject = sleep2 |&gt; pull(Subject) |&gt; levels() |&gt; factor(): Questa parte del codice estrae la colonna “Subject” dal data frame “sleep2”, quindi applica una serie di operazioni successive utilizzando l’operatore |&gt; (pipe) per manipolare i dati nella colonna. - sleep2 |&gt; pull(Subject): Inizia estraendo la colonna “Subject” dal data frame “sleep2”. - levels(): Successivamente, applica la funzione “levels()” per ottenere i livelli unici della colonna “Subject”. Questo è utile quando si ha a che fare con variabili categoriche (come un “factor”). - factor(): Infine, trasforma i livelli ottenuti in un “factor”. Questo è importante perché la funzione “crossing()” richiede che le variabili categoriche siano di tipo “factor”.\n\ndays_deprived = 0:7: Questa parte del codice crea una nuova variabile chiamata “days_deprived” che contiene una sequenza da 0 a 7. Questo rappresenta i giorni di privazione.\ncrossing(...): Infine, la funzione “crossing()” viene utilizzata per creare un nuovo data frame chiamato “newdata” combinando tutte le possibili combinazioni di valori tra la variabile “Subject” (con i suoi livelli unici) e la variabile “days_deprived” (con i valori da 0 a 7).\n\nUtilizziamo predict() per trovare le rette di regressione per ciascun soggetto.\n\nnewdata2 &lt;- newdata |&gt;\n    mutate(Reaction = predict(pp_mod, newdata))\n\n\nhead(newdata2)\n\n\nA tibble: 6 x 3\n\n\nSubject\ndays_deprived\nReaction\n\n\n&lt;fct&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\n308\n0\n292.4667\n\n\n308\n1\n312.5041\n\n\n308\n2\n332.5416\n\n\n308\n3\n352.5790\n\n\n308\n4\n372.6164\n\n\n308\n5\n392.6539\n\n\n\n\n\nOra possiamo creare il grafico con le predizioni del modello.\n\nggplot(sleep2, aes(x = days_deprived, y = Reaction)) +\n    geom_line(\n        data = newdata2,\n        color = \"blue\"\n    ) +\n    geom_point() +\n    scale_x_continuous(breaks = 0:7) +\n    facet_wrap(~Subject) +\n    labs(y = \"Reaction Time\", x = \"Days deprived of sleep (0 = baseline)\")",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modelli multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/raters/01_multilevel.html#interpretare-loutput-di-lmer-ed-estrarre-le-stime",
    "href": "chapters/raters/01_multilevel.html#interpretare-loutput-di-lmer-ed-estrarre-le-stime",
    "title": "11  Modelli multilivello",
    "section": "11.6 Interpretare l’output di lmer() ed estrarre le stime",
    "text": "11.6 Interpretare l’output di lmer() ed estrarre le stime\nLa chiamata a lmer() restituisce un oggetto della classe “lmerMod”.\n\n11.6.1 Effetti fissi\nLa sezione dell’output chiamata “Effetti fissi:” è simile a ciò che si vede nell’output per un modello lineare semplice adattato con lm().\nFixed effects:\n              Estimate Std. Error t value\n(Intercept)    267.967      8.266  32.418\ndays_deprived   11.435      1.845   6.197\nL’output precedente indica che il tempo di reazione medio stimato per i partecipanti al Giorno 0 era di circa 268 millisecondi, con ogni giorno di privazione del sonno che aggiungeva mediamente ulteriori 11 millisecondi al tempo di risposta.\nSe dobbiamo ottenere gli effetti fissi dal modello, possiamo estrarli utilizzando la funzione fixef().\n\nfixef(pp_mod) |&gt; \n    print()\n\n  (Intercept) days_deprived \n    267.96742      11.43543 \n\n\nGli errori standard ci forniscono stime della variabilità di questi parametri dovuta all’errore di campionamento. Possiamo utilizzarli per calcolare i valori \\(t\\) o derivare gli intervalli di confidenza. Per estrarli, utilizziamo vcov(pp_mod), che restituirà una matrice di varianza-covarianza (non quella associata agli effetti casuali), quindi estraiamo la diagonale utilizzando diag() e calcoliamo infine la radice quadrata utilizzando sqrt().\n\nvcov(pp_mod)\n\n2 x 2 Matrix of class \"dpoMatrix\"\n              (Intercept) days_deprived\n(Intercept)     68.325030     -0.949732\ndays_deprived   -0.949732      3.405105\n\n\n\nvcov(pp_mod) |&gt; \n    diag() |&gt; \n    sqrt() |&gt; \n    print()\n\n  (Intercept) days_deprived \n     8.265896      1.845293 \n\n\nSi noti che, nell’output di lmer, i valori \\(t\\) non sono accompagnati dai valori \\(p\\), come avviene di solito nei contesti di modellazione più semplici. Esistono molteplici approcci per ottenere i valori \\(p\\) da modelli a effetti misti, ciascuno con vantaggi e svantaggi (si veda, ad esempio, Luke (2017) per un’analisi delle opzioni disponibili). I valori \\(t\\) non vengono accompagnati dai gradi di libertà, poiché i gradi di libertà in un modello a effetti misti non sono ben definiti. Spesso i ricercatori trattano i valori \\(t\\) come valori \\(z\\) di Wald, ossia come osservazioni provenienti da una distribuzione normale standard. Poiché la distribuzione \\(t\\) si avvicina alla distribuzione normale standard all’aumentare del numero di osservazioni, questa pratica “t-as-z” è legittima se il numero di osservazioni campionarie è sufficientemente grande.\nPer calcolare i valori \\(z\\) di Wald, basta dividere la stima dell’effetto fisso per il suo errore standard:\n\ntvals &lt;- fixef(pp_mod) / sqrt(diag(vcov(pp_mod)))\n\ntvals |&gt; \n    print()\n\n  (Intercept) days_deprived \n    32.418437      6.197082 \n\n\nI valori-\\(p\\) si ottengono nel modo seguente:\n\nprint(2 * (1 - pnorm(abs(tvals))))\n\n  (Intercept) days_deprived \n  0.00000e+00   5.75197e-10 \n\n\nQuesto fornisce una forte evidenza contro l’ipotesi nulla \\(H_0: \\gamma_1 = 0\\). Sembra che la privazione del sonno aumenti effettivamente il tempo di risposta.\nÈ possibile ottenere gli intervalli di confidenza per le stime utilizzando la funzione confint() (questa tecnica utilizza il bootstrap parametrico).\n\nconfint(pp_mod) |&gt; \n    print()\n\nComputing profile confidence intervals ...\n\n\n\n                    2.5 %      97.5 %\n.sig01         19.0980993  46.3366669\n.sig02         -0.4051075   0.8059258\n.sig03          4.0079284  10.2487353\n.sigma         22.4666029  29.3494508\n(Intercept)   251.3443396 284.5904989\ndays_deprived   7.7245247  15.1463328\n\n\n\n\n11.6.2 Effetti random\nRandom effects:\n Groups   Name          Variance Std.Dev. Corr\n Subject  (Intercept)   958.35   30.957       \n          days_deprived  45.78    6.766   0.18\n Residual               651.60   25.526       \nNumber of obs: 144, groups:  Subject, 18\nLa parte relativa agli effetti casuali dell’output di summary() ci fornisce una tabella con informazioni sulle diverse componenti della varianza: la matrice di varianza-covarianza (o matrici, se ci sono più fattori casuali) e la varianza residua.\nCominciamo con la riga Residual. Questo ci indica che la varianza residua, \\(\\sigma^2\\), è stata stimata a circa 651.6. Il valore nella colonna successiva, 25.526, è la deviazione standard, \\(\\sigma\\), che è la radice quadrata della varianza.\nEstraiamo la deviazione standard dei residui utilizzando la funzione sigma().\n\nsigma(pp_mod) # residual\n\n25.526404577851\n\n\nLe due righe sopra la riga Residual ci forniscono informazioni sulla matrice di varianza-covarianza per il fattore casuale “Subject”.\nRandom effects:\n Groups   Name          Variance Std.Dev. Corr\n Subject  (Intercept)   958.35   30.957       \n          days_deprived  45.78    6.766   0.18\nI valori nella colonna “Variance” ci forniscono la diagonale principale della matrice, mentre i valori nella colonna “Std.Dev.” rappresentano semplicemente le radici quadrate di questi valori. La colonna “Corr” indica la correlazione tra l’intercetta e la pendenza.\nPossiamo estrarre questi valori dall’oggetto adattato pp_mod utilizzando la funzione VarCorr(). Questa funzione restituisce una lista nominata, con un elemento per ciascun fattore casuale. Nel nostro caso, “Subject” è l’unico fattore casuale, quindi la lista avrà lunghezza 1.\n\n# variance-covariance matrix for random factor Subject\nVarCorr(pp_mod)[[\"Subject\"]] |&gt; \n    print() # oppure: VarCorr(pp_mod)[[1]]\n\n              (Intercept) days_deprived\n(Intercept)      958.3517      37.20460\ndays_deprived     37.2046      45.77766\nattr(,\"stddev\")\n  (Intercept) days_deprived \n    30.957255      6.765919 \nattr(,\"correlation\")\n              (Intercept) days_deprived\n(Intercept)     1.0000000     0.1776263\ndays_deprived   0.1776263     1.0000000\n\n\nLe prime righe rappresentano la matrice di varianza-covarianza. Le varianze sono riportate sulla diagonale principale. correlation indica la correlazione tra la stima della pendenza e la stima dell’intercetta.\nPossiamo estrarre gli effetti casuali stimati (BLUPS) utilizzando la funzione ranef().\n\nranef(pp_mod)[[\"Subject\"]] |&gt; \n    print()\n\n    (Intercept) days_deprived\n308  24.4992891     8.6020000\n309 -59.3723102    -8.1277534\n310 -39.4762764    -7.4292365\n330   1.3500428    -2.3845976\n331  18.4576169    -3.7477340\n332  30.5270040    -4.8936899\n333  13.3682027     0.2888639\n334 -18.1583020     3.8436686\n335 -16.9737887   -12.0702333\n337  44.5850842    10.1760837\n349 -26.6839022     2.1946699\n350  -5.9657957     8.1758613\n351  -5.5710355    -2.3718494\n352  46.6347253    -0.5616377\n369   0.9616395     1.7385130\n370 -18.5216778     5.6317534\n371  -7.3431320     0.2729282\n372  17.6826159     0.6623897\n\n\nPossiamo ottenere i valori stimati dal modello utilizzando fitted() e i residui utilizzando residuals().\n\nmutate(sleep2,\n    fit = fitted(pp_mod),\n    resid = residuals(pp_mod)\n) |&gt;\n    group_by(Subject) %&gt;%\n    slice(c(1, 10)) %&gt;%\n    print(n = +Inf)\n\n# A tibble: 18 x 6\n# Groups:   Subject [18]\n   Reaction  Days Subject days_deprived   fit  resid\n      &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;           &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1     251.     2 308                 0  292. -41.7 \n 2     203.     2 309                 0  209.  -5.62\n 3     234.     2 310                 0  228.   5.83\n 4     284.     2 330                 0  269.  14.5 \n 5     302.     2 331                 0  286.  15.4 \n 6     273.     2 332                 0  298. -25.5 \n 7     277.     2 333                 0  281.  -4.57\n 8     243.     2 334                 0  250.  -6.44\n 9     254.     2 335                 0  251.   3.50\n10     292.     2 337                 0  313. -20.9 \n11     239.     2 349                 0  241.  -2.36\n12     256.     2 350                 0  262.  -5.80\n13     270.     2 351                 0  262.   7.50\n14     327.     2 352                 0  315.  12.3 \n15     257.     2 369                 0  269. -11.7 \n16     239.     2 370                 0  249. -10.5 \n17     278.     2 371                 0  261.  17.3 \n18     298.     2 372                 0  286.  11.9 \n\n\nInfine, possiamo ottenere previsioni per nuovi dati utilizzando predict(), come abbiamo fatto in precedenza.\n\n## create the table with new predictor values\nndat &lt;- crossing(\n    Subject = sleep2 %&gt;% pull(Subject) %&gt;% levels() %&gt;% factor(),\n    days_deprived = 8:10\n) %&gt;%\n    mutate(Reaction = predict(pp_mod, newdata = .))\n\nndat |&gt; \n    head()\n\n\nA tibble: 6 x 3\n\n\nSubject\ndays_deprived\nReaction\n\n\n&lt;fct&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\n308\n8\n452.7661\n\n\n308\n9\n472.8036\n\n\n308\n10\n492.8410\n\n\n309\n8\n235.0565\n\n\n309\n9\n238.3642\n\n\n309\n10\n241.6719\n\n\n\n\n\n\nggplot(sleep2, aes(x = days_deprived, y = Reaction)) +\n    geom_line(\n        data = bind_rows(newdata2, ndat),\n        color = \"blue\"\n    ) +\n    geom_point() +\n    scale_x_continuous(breaks = 0:10) +\n    facet_wrap(~Subject) +\n    labs(y = \"Reaction Time\", x = \"Days deprived of sleep (0 = baseline)\")",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modelli multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/raters/01_multilevel.html#considerazioni-conclusive",
    "href": "chapters/raters/01_multilevel.html#considerazioni-conclusive",
    "title": "11  Modelli multilivello",
    "section": "11.7 Considerazioni Conclusive",
    "text": "11.7 Considerazioni Conclusive\nIl presente capitolo presenta un’analisi approfondita dei modelli statistici multilivello discutendo l’esempio relativo all’effetto della deprivazione del sonno sulle prestazioni psicomotorie. Utilizzando il dataset sleepstudy, abbiamo introdotto concetti chiave quali “complete pooling”, “no pooling” e “partial pooling”. Questi approcci sono stati esplorati per evidenziare le loro implicazioni nella modellazione dei dati che contengono misure ripetute.\nL’analisi si focalizza sull’applicazione pratica dei modelli multilivello, dettagliando le variabili fisse e casuali e la loro importanza nel contesto dei modelli. È stata posta particolare enfasi sulla matrice di varianza-covarianza, essenziale per comprendere le relazioni all’interno dei modelli multilivello.\nI modelli multilivello sono di vitale importanza per l’assessment psicologico e la psicometria, in quanto permettono di analizzare dati complessi tenendo conto delle variazioni individuali e di gruppo. Questi modelli forniscono strumenti flessibili per esaminare l’influenza di fattori contestuali e individuali sul comportamento e le prestazioni psicologiche, un aspetto cruciale nell’assessment psicologico.\nIl capitolo anticipa l’utilizzo dei modelli multilivello nel calcolo dell’affidabilità tra giudici, un tema che sarà approfondito nel capitolo successivo. Inoltre, i modelli multilivello sono presentati come un’introduzione o un’alternativa ai modelli di crescita latente, che verranno esplorati nei capitoli successivi. Questo collegamento sottolinea la continuità e l’importanza di tali modelli nell’ambito più ampio della ricerca psicologica e psicometrica.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modelli multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/raters/01_multilevel.html#session-info",
    "href": "chapters/raters/01_multilevel.html#session-info",
    "title": "11  Modelli multilivello",
    "section": "11.8 Session Info",
    "text": "11.8 Session Info\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] kableExtra_1.4.0  repr_1.1.7        lme4_1.1-35.5     Matrix_1.7-0     \n [5] car_3.1-2         carData_3.0-5     ggokabeito_0.1.0  viridis_0.6.5    \n [9] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    bayesplot_1.11.1 \n[13] gridExtra_2.3     patchwork_1.3.0   semTools_0.5-6    semPlot_1.1.6    \n[17] lavaan_0.6-18     psych_2.4.6.26    scales_1.3.0      markdown_1.13    \n[21] knitr_1.48        lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1    \n[25] dplyr_1.1.4       purrr_1.0.2       readr_2.1.5       tidyr_1.3.1      \n[29] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.16.0  jsonlite_1.8.9     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5.1 farver_2.1.2      \n  [7] nloptr_2.1.1       rmarkdown_2.28     vctrs_0.6.5       \n [10] minqa_1.2.8        base64enc_0.1-3    rstatix_0.7.2     \n [13] htmltools_0.5.8.1  broom_1.0.6        Formula_1.2-5     \n [16] htmlwidgets_1.6.4  plyr_1.8.9         sandwich_3.1-1    \n [19] emmeans_1.10.4     zoo_1.8-12         uuid_1.2-1        \n [22] igraph_2.0.3       mime_0.12          lifecycle_1.0.4   \n [25] pkgconfig_2.0.3    R6_2.5.1           fastmap_1.2.0     \n [28] shiny_1.9.1        digest_0.6.37      OpenMx_2.21.12    \n [31] fdrtool_1.2.18     colorspace_2.1-1   rprojroot_2.0.4   \n [34] Hmisc_5.1-3        labeling_0.4.3     fansi_1.0.6       \n [37] timechange_0.3.0   abind_1.4-8        compiler_4.4.1    \n [40] withr_3.0.1        glasso_1.11        htmlTable_2.4.3   \n [43] backports_1.5.0    ggsignif_0.6.4     MASS_7.3-61       \n [46] corpcor_1.6.10     gtools_3.9.5       tools_4.4.1       \n [49] pbivnorm_0.6.0     foreign_0.8-87     zip_2.3.1         \n [52] httpuv_1.6.15      nnet_7.3-19        glue_1.7.0        \n [55] quadprog_1.5-8     promises_1.3.0     nlme_3.1-166      \n [58] lisrelToR_0.3      grid_4.4.1         pbdZMQ_0.3-13     \n [61] checkmate_2.3.2    cluster_2.1.6      reshape2_1.4.4    \n [64] generics_0.1.3     gtable_0.3.5       tzdb_0.4.0        \n [67] data.table_1.16.0  hms_1.1.3          xml2_1.3.6        \n [70] utf8_1.2.4         sem_3.1-16         pillar_1.9.0      \n [73] IRdisplay_1.1      rockchalk_1.8.157  later_1.3.2       \n [76] splines_4.4.1      lattice_0.22-6     survival_3.7-0    \n [79] kutils_1.73        tidyselect_1.2.1   miniUI_0.1.1.1    \n [82] pbapply_1.7-2      svglite_2.1.3      stats4_4.4.1      \n [85] xfun_0.47          qgraph_1.9.8       arm_1.14-4        \n [88] stringi_1.8.4      pacman_0.5.1       boot_1.3-31       \n [91] evaluate_1.0.0     codetools_0.2-20   mi_1.1            \n [94] cli_3.6.3          RcppParallel_5.1.9 IRkernel_1.3.2    \n [97] rpart_4.1.23       systemfonts_1.1.0  xtable_1.8-4      \n[100] munsell_0.5.1      Rcpp_1.0.13        coda_0.19-4.1     \n[103] png_0.1-8          XML_3.99-0.17      parallel_4.4.1    \n[106] jpeg_0.1-10        mvtnorm_1.3-1      openxlsx_4.2.7.1  \n[109] crayon_1.5.3       rlang_1.1.4        multcomp_1.4-26   \n[112] mnormt_2.1.1",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modelli multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/raters/02_interrater_reliability.html",
    "href": "chapters/raters/02_interrater_reliability.html",
    "title": "12  L’affidabilità tra giudici",
    "section": "",
    "text": "12.1 Introduzione\nSe la valutazione di un test dipende da giudizi soggettivi, è essenziale valutare il grado di accordo tra diversi individui che assegnano i punteggi. Questo tipo di affidabilità è conosciuto come affidabilità inter-valutatore (o inter-scorer, inter-rater reliability in inglese). L’affidabilità inter-valutatore si riferisce alla coerenza o concordanza nelle valutazioni tra due o più giudici che esaminano lo stesso fenomeno o campione. Questo aspetto è fondamentale, in quanto indica il livello di similitudine nei risultati ottenuti da diversi valutatori quando si trovano a esprimere giudizi o a effettuare misurazioni in contesti simili.\nIn pratica, l’affidabilità inter-valutatore si misura calcolando quanto siano vicini tra loro i punteggi o le valutazioni fornite da diversi osservatori. Una forte affidabilità inter-valutatore indica che il test o lo strumento di valutazione produce risultati consistenti e riproducibili, nonostante la presenza di diversi valutatori. Questo è cruciale per garantire che le conclusioni tratte dai risultati del test siano valide e affidabili.\nL’affidabilità tra giudici è un aspetto fondamentale in molti ambiti della psicologia. Consideriamo il caso di una diagnosi psicologica. Quando diversi psicologi valutano i sintomi di un paziente, è essenziale che le loro valutazioni siano coerenti per garantire una diagnosi accurata. Ad esempio, nella valutazione dei disturbi dell’umore, gli psicologi devono giungere a conclusioni simili basandosi su sintomi osservabili e risposte del paziente a questionari standardizzati. Un esempio nella ricerca potrebbe essere uno studio sull’efficacia di una nuova terapia per l’ansia. Qui, vari terapeuti valutano il livello di ansia dei partecipanti prima e dopo il trattamento. L’affidabilità tra le valutazioni di questi terapeuti è cruciale per validare l’efficacia del trattamento.\nQuesto capitolo esamina l’affidabilità tra giudici in due contesti: con due giudici e con giudici molteplici. Nel primo caso, analizzeremo l’accordo nominale e il Kappa di Cohen. Nel secondo caso, useremo l’ICC, l’ANOVA ad una via e l’ANOVA a due vie. In questo contesto, i modelli a effetti misti si dimostrano fondamentali per gestire disegni con giudici molteplici, catturando le variazioni tra i giudici e tra i partecipanti e fornendo una visione completa dell’affidabilità delle valutazioni.\nNella discussione seguente useremo i dati dello studio The Reliability and Validity of Speech-Language Pathologists’ Estimations of Intelligibility in Dysarthria di Hirsch et al. (2022). In questo studio, i giudici valutano la “Percentuale di Intelligibilità del Discorso” di persone affette da disartria. Questa misura soggettiva descrive la percentuale di discorso compreso durante una conversazione con un parlante che soffre di disartria, una condizione che influisce sulla chiarezza e sull’intelligibilità della parola. I giudici ascoltano e valutano le registrazioni vocali dei partecipanti affetti da disartria e stimano la percentuale di parole o frasi che sono in grado di comprendere chiaramente. Questa valutazione fornisce una misura importante dell’impatto della disartria sulla comunicazione verbale quotidiana.\nslp_dat &lt;- read.csv(\"https://osf.io/download/p9gqk/\")\nslp_vas_wide &lt;- slp_dat |&gt;\n    select(slpID, Speaker, slp_VAS) |&gt;\n    pivot_wider(names_from = slpID, values_from = slp_VAS)\nhead(slp_vas_wide)\n\n\nA tibble: 6 x 22\n\n\nSpeaker\nslp10\nslp11\nslp13\nslp14\nslp15\nslp16\nslp17\nslp18\nslp19\n...\nslp21\nslp22\nslp23\nslp25\nslp3\nslp4\nslp5\nslp6\nslp8\nslp9\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n...\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nAF1\n96.28\n100.00\n96.56\n99.30\n100.00\n12.58\n100.00\n88.16\n52.58\n...\n77.42\n73.64\n100.00\n100.00\n37.82\n73.24\n55.01\n100.00\n53.94\n85.07\n\n\nAF9\n12.32\n34.68\n0.86\n3.52\n12.58\n0.00\n6.69\n0.00\n7.42\n...\n13.55\n0.00\n0.00\n2.90\n0.00\n14.51\n1.72\nNA\n6.76\n10.28\n\n\nALSF6\nNA\n30.84\n5.73\n73.94\n73.87\n4.52\n45.13\n41.92\n26.13\n...\n25.81\n8.31\n18.46\n86.77\n0.00\n26.20\n3.72\nNA\n9.86\n37.75\n\n\nALSF7\n93.98\n96.31\n61.32\n64.37\n100.00\n7.74\n69.92\n88.16\n37.42\n...\n59.35\n34.10\n90.63\n53.23\n99.71\n63.10\n56.73\n88.83\n64.93\n53.80\n\n\nALSF9\n29.51\n52.20\n43.55\n46.76\n60.65\n23.87\n25.21\n91.64\n37.10\n...\n57.74\n29.51\n50.14\n64.84\n0.00\n31.13\n40.11\n16.91\n5.92\n50.00\n\n\nALSM1\n96.28\nNA\n29.23\n69.44\n99.35\n82.26\n70.19\n79.72\n78.39\n...\n83.87\n84.24\n92.29\n100.00\n100.00\n18.03\n58.74\n66.48\n51.83\n60.28",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>L'affidabilità tra giudici</span>"
    ]
  },
  {
    "objectID": "chapters/raters/02_interrater_reliability.html#due-giudici",
    "href": "chapters/raters/02_interrater_reliability.html#due-giudici",
    "title": "12  L’affidabilità tra giudici",
    "section": "12.2 Due giudici",
    "text": "12.2 Due giudici\nSelezioniamo unicamente due giudici.\n\nslp_2rater &lt;- slp_vas_wide |&gt;\n    select(slp14, slp15)\nhead(slp_2rater)\n\n\nA tibble: 6 x 2\n\n\nslp14\nslp15\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n99.30\n100.00\n\n\n3.52\n12.58\n\n\n73.94\n73.87\n\n\n64.37\n100.00\n\n\n46.76\n60.65\n\n\n69.44\n99.35\n\n\n\n\n\n\n12.2.1 Accordo nominale\nPer calcolare l’accordo nominale, è necessario analizzare quali valutazioni tra i due giudici corrispondono in modo perfettamente uguale. Utilizzando questa definizione, si ottiene:\n\nwith(slp_2rater, mean(slp14 == slp15))\n\n0.1\n\n\nSe invece modifichiamo leggermente la definizione e affermiamo che l’accordo è raggiunto quando rimane invariato anche dopo l’arrotondamento, si ottiene:\n\nslp_2round &lt;- round(slp_2rater / 10)\nslp_2round |&gt; print()\n\n   slp14 slp15\n1     10    10\n2      0     1\n3      7     7\n4      6    10\n5      5     6\n6      7    10\n7      8     9\n8      8     9\n9      9     8\n10    10    10\n11    10     7\n12    10    10\n13     6     5\n14    10    10\n15     6    10\n16    10    10\n17    10    10\n18     9    10\n19     9    10\n20    10    10\n\n\n\nslp_2round &lt;- lapply(slp_2round, FUN = factor, levels = 0:10)\n# Contingency table\ntable(slp_2round)\n\n     slp15\nslp14 0 1 2 3 4 5 6 7 8 9 10\n   0  0 1 0 0 0 0 0 0 0 0  0\n   1  0 0 0 0 0 0 0 0 0 0  0\n   2  0 0 0 0 0 0 0 0 0 0  0\n   3  0 0 0 0 0 0 0 0 0 0  0\n   4  0 0 0 0 0 0 0 0 0 0  0\n   5  0 0 0 0 0 0 1 0 0 0  0\n   6  0 0 0 0 0 1 0 0 0 0  2\n   7  0 0 0 0 0 0 0 1 0 0  1\n   8  0 0 0 0 0 0 0 0 0 2  0\n   9  0 0 0 0 0 0 0 0 1 0  2\n   10 0 0 0 0 0 0 0 1 0 0  7\n\n\nIl codice precedente fa uso di due funzioni principali: lapply() e table(). Esso opera su una lista o su un dataframe chiamato slp_2round, applicando una trasformazione ai dati e successivamente creando una tabella di contingenza.\n\nslp_2round &lt;- lapply(slp_2round, FUN = factor, levels = 0:10)\n\n\nlapply(): questa funzione appartiene alla famiglia delle funzioni “*apply” in R, che sono utilizzate per applicare una funzione a ciascun elemento di una lista o di un vettore. lapply() restituisce sempre una lista come risultato, mantenendo la lunghezza dell’input originale.\nslp_2round: è la lista o il dataframe su cui si sta operando. Il codice modifica questa variabile, sostituendola con il risultato dell’applicazione di lapply().\nFUN = factor: specifica che la funzione da applicare a ciascun elemento di slp_2round è factor(). La funzione factor() è utilizzata per codificare un vettore come un fattore, che è utile in R per rappresentare dati categorici.\nlevels = 0:10: indica i livelli del fattore, che in questo caso sono i numeri da 0 a 10. Questo parametro assicura che i fattori creati abbiano esattamente questi livelli, anche se alcuni di essi non appaiono nei dati. Questo è particolarmente utile per garantire la coerenza dei livelli dei fattori attraverso vari elementi di slp_2round, soprattutto se si intende confrontarli o combinarli in seguito.\n\n\ntable(slp_2round)\n\n\ntable(): questa funzione crea una tabella di contingenza dai dati forniti. Una tabella di contingenza è un tipo di tabella in formato matriciale che conta la frequenza di combinazioni specifiche di valori all’interno di una o più variabili categoriche.\n\nDopo l’applicazione di lapply(), slp_2round diventa una lista di elementi, ciascuno dei quali è un fattore con livelli da 0 a 10. Quando si passa questa lista modificata alla funzione table(), viene generata una tabella di contingenza. Questa tabella riassume le frequenze dei vari livelli dei fattori attraverso gli elementi della lista slp_2round.\n\np0 &lt;- with(slp_2round, mean(slp14 == slp15))\np0\n\n0.4\n\n\nQui, l’arrotondamento serve a ridurre la specificità delle valutazioni, permettendo un confronto più ampio e potenzialmente più flessibile tra le valutazioni dei due giudici​​. Questo approccio riflette una visione più tollerante dell’accordo, riconoscendo che piccole discrepanze nelle valutazioni potrebbero non essere significative in un contesto pratico.\nNel contesto specifico descritto, l’arrotondamento implica dividere le valutazioni per 10 e poi arrotondarle al numero intero più vicino. Questo processo rende i valori leggermente diversi equivalenti. Ad esempio, se un giudice assegna una valutazione di 75 e l’altro assegna 78, dividendo per 10 otteniamo 7.5 e 7.8; arrotondando, entrambi i valori diventano 8. Così, valutazioni originalmente diverse vengono arrotondate allo stesso numero intero, permettendo un accordo più ampio tra i giudici. Questo metodo riconosce l’accordo anche in presenza di piccole variazioni nelle valutazioni, riflettendo una visione più flessibile dell’accordo tra i giudici.\n\n\n12.2.2 Kappa di Cohen\nL’indice di concordanza Kappa di Cohen è definito nel modo seguente:\n\\[\n\\kappa = \\frac{p_o - p_c}{1 - p_c},\n\\]\ndove:\n\n\\(\\kappa\\) rappresenta il coefficiente di Kappa di Cohen.\n\\(p_o\\) è l’indice di concordanza osservato tra gli osservatori.\n\\(p_c\\) è l’indice di concordanza attesa per caso.\n\n\\(p_o\\) (indice di concordanza osservato tra gli osservatori):\n\\[\np_o = \\frac{{\\text{Numero di accordi osservati}}}{{\\text{Numero totale di confronti}}}\n\\]\nIn altre parole, \\(p_o\\) è il rapporto tra il numero di volte in cui gli osservatori sono concordi (hanno dato la stessa valutazione) e il numero totale di confronti effettuati.\n\\(p_c\\) è l’indice di concordanza attesa per caso:\n\\[\np_c = \\frac{{\\sum (\\text{Riga i-esima del totale}) \\times (\\text{Colonna i-esima del totale})}}{{\\text{Numero totale di confronti}}^2}\n\\]\nIn altre parole, \\(p_c\\) è il rapporto atteso di concordanza tra gli osservatori nel caso in cui le loro valutazioni siano indipendenti.\nQueste formule vengono utilizzate per calcolare il coefficiente di Kappa di Cohen, che misura la concordanza tra osservatori che stanno misurando una variabile categoriale, tenendo conto della concordanza casuale attesa. Un valore di \\(\\kappa\\) vicino a 1 indica un’alta concordanza, mentre un valore vicino a 0 indica una concordanza casuale e un valore negativo indica una concordanza peggiore di quella casuale.\n\nslp_2tab &lt;- table(slp_2round)\npc &lt;- sum(colSums(slp_2tab) * rowSums(slp_2tab)) / sum(slp_2tab)^2\n(kappa &lt;- (p0 - pc) / (1 - pc))\n\n0.166666666666667\n\n\nSvantaggio: Il coefficiente di Kappa tende a essere basso quando i punteggi sono distribuiti in maniera disomogenea (ad esempio, quando la maggior parte dei punteggi appartiene a determinate categorie). Questa situazione corrisponde esattamente al caso dell’esempio presente.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>L'affidabilità tra giudici</span>"
    ]
  },
  {
    "objectID": "chapters/raters/02_interrater_reliability.html#giudici-multipli",
    "href": "chapters/raters/02_interrater_reliability.html#giudici-multipli",
    "title": "12  L’affidabilità tra giudici",
    "section": "12.3 Giudici multipli",
    "text": "12.3 Giudici multipli\n\n12.3.1 Coefficiente \\(\\alpha\\)\nIl coefficiente \\(\\alpha\\) di Cronbach è comunemente utilizzato per misurare l’affidabilità interna di un test, cioè la coerenza con cui gli item all’interno del test misurano un costrutto comune. Tuttavia, può essere utilizzato anche per valutare l’affidabilità tra giudici in uno studio in cui più valutatori assegnano punteggi o giudizi a una serie di item.\nNel contesto dell’affidabilità tra giudici, il coefficiente \\(\\alpha\\) di Cronbach misura quanto consistentemente i diversi giudici valutano gli stessi item. Un valore di \\(\\alpha\\) vicino a 1 indica un alto livello di consistenza (o affidabilità) tra i giudici, suggerendo che stanno valutando gli item in modo simile. Un valore più basso indica una minore coerenza nelle valutazioni tra i giudici.\nPer calcolare il coefficiente \\(\\alpha\\) di Cronbach in questo contesto, si considerano i punteggi assegnati da ciascun giudice a ciascun item come se fossero item individuali di un test. Quindi, si applica la formula standard del coefficiente \\(\\alpha\\) per valutare la coerenza interna di questi “item” (in questo caso, le valutazioni dei giudici).\n\npsych::alpha(slp_vas_wide[-1])\n\nNumber of categories should be increased  in order to count frequencies. \n\nWarning message in cor.smooth(r):\n\"Matrix was not positive definite, smoothing was done\"\nIn smc, smcs &gt; 1 were set to 1.0\n\nIn smc, smcs &gt; 1 were set to 1.0\n\nIn smc, smcs &gt; 1 were set to 1.0\n\nIn smc, smcs &gt; 1 were set to 1.0\n\nIn smc, smcs &lt; 0 were set to .0\n\nIn smc, smcs &lt; 0 were set to .0\n\nIn smc, smcs &lt; 0 were set to .0\n\nIn smc, smcs &lt; 0 were set to .0\n\nIn smc, smcs &gt; 1 were set to 1.0\n\nIn smc, smcs &lt; 0 were set to .0\n\nIn smc, smcs &gt; 1 were set to 1.0\n\nIn smc, smcs &gt; 1 were set to 1.0\n\nIn smc, smcs &gt; 1 were set to 1.0\n\nIn smc, smcs &gt; 1 were set to 1.0\n\nIn smc, smcs &lt; 0 were set to .0\n\nIn smc, smcs &gt; 1 were set to 1.0\n\nIn smc, smcs &lt; 0 were set to .0\n\nIn smc, smcs &gt; 1 were set to 1.0\n\nIn smc, smcs &lt; 0 were set to .0\n\nIn smc, smcs &gt; 1 were set to 1.0\n\nIn smc, smcs &gt; 1 were set to 1.0\n\nIn smc, smcs &gt; 1 were set to 1.0\n\nIn smc, smcs &gt; 1 were set to 1.0\n\nIn smc, smcs &gt; 1 were set to 1.0\n\nIn smc, smcs &lt; 0 were set to .0\n\nIn smc, smcs &gt; 1 were set to 1.0\n\n\n\n\nReliability analysis   \nCall: psych::alpha(x = slp_vas_wide[-1])\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean sd median_r\n      0.98      0.98       1      0.69  46 0.0076   62 25      0.7\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.96  0.98  0.99\nDuhachek  0.96  0.98  0.99\n\n Reliability if an item is dropped:\n      raw_alpha std.alpha G6(smc) average_r S/N var.r med.r\nslp10      0.97      0.98    0.99      0.68  43 0.014  0.70\nslp11      0.97      0.98    0.99      0.68  43 0.014  0.70\nslp13      0.97      0.98    0.99      0.69  44 0.015  0.70\nslp14      0.97      0.98    0.99      0.69  44 0.015  0.70\nslp15      0.97      0.98    0.99      0.69  44 0.014  0.70\nslp16      0.98      0.98    0.99      0.70  46 0.014  0.71\nslp17      0.97      0.98    0.99      0.68  43 0.015  0.69\nslp18      0.97      0.98    0.99      0.69  45 0.013  0.70\nslp19      0.97      0.98    0.99      0.69  45 0.014  0.71\nslp2       0.97      0.98    1.00      0.69  45 0.014  0.71\nslp20      0.97      0.98    0.99      0.68  43 0.015  0.69\nslp21      0.97      0.98    0.99      0.68  43 0.014  0.69\nslp22      0.97      0.98    0.99      0.69  44 0.015  0.70\nslp23      0.97      0.98    0.99      0.68  43 0.015  0.69\nslp25      0.97      0.98    0.99      0.69  45 0.014  0.71\nslp3       0.98      0.98    0.99      0.70  46 0.014  0.71\nslp4       0.97      0.98    0.99      0.69  45 0.014  0.71\nslp5       0.97      0.98    0.99      0.69  45 0.015  0.70\nslp6       0.98      0.98    0.99      0.70  47 0.010  0.71\nslp8       0.97      0.98    0.99      0.69  44 0.015  0.70\nslp9       0.97      0.98    1.00      0.68  43 0.014  0.69\n\n Item statistics \n       n raw.r std.r r.cor r.drop mean sd\nslp10 18  0.89  0.89  0.90   0.86   81 28\nslp11 18  0.89  0.90  0.90   0.90   69 33\nslp13 19  0.86  0.86  0.85   0.86   54 33\nslp14 20  0.82  0.84  0.84   0.81   80 25\nslp15 20  0.81  0.83  0.83   0.80   85 22\nslp16 20  0.77  0.76  0.76   0.73   50 42\nslp17 20  0.90  0.91  0.91   0.90   63 34\nslp18 20  0.82  0.80  0.77   0.79   70 30\nslp19 19  0.83  0.82  0.82   0.81   56 25\nslp2  20  0.83  0.82  0.82   0.80   64 32\nslp20 20  0.90  0.90  0.90   0.89   49 25\nslp21 19  0.91  0.91  0.90   0.91   59 25\nslp22 20  0.86  0.86  0.86   0.84   50 27\nslp23 20  0.89  0.89  0.89   0.86   71 32\nslp25 20  0.79  0.79  0.79   0.77   79 28\nslp3  19  0.76  0.75  0.76   0.74   52 40\nslp4  20  0.79  0.79  0.78   0.77   47 27\nslp5  19  0.82  0.82  0.82   0.81   43 23\nslp6  13  0.80  0.71  0.71   0.54   76 23\nslp8  20  0.86  0.87  0.87   0.86   53 31\nslp9  19  0.89  0.90  0.90   0.90   58 23\n\n\nNell’esempio in discussione, il valore del coefficiente \\(\\alpha\\) di Cronbach è 0.98. Questo indica un’altissima affidabilità interna, suggerendo che i giudici coinvolti nello studio hanno fornito delle valutazioni molto consistenti tra loro. Un valore così elevato di \\(\\alpha\\) è raro e suggerisce che ci sia un’alta coerenza nelle valutazioni tra i giudici. In termini pratici, questo significa che si può avere un’elevata fiducia nel fatto che le valutazioni fornite dai diversi giudici siano intercambiabili e riflettano in modo affidabile la “Percentuale di Intelligibilità del Discorso” che viene valutata.\n\n\n12.3.2 Coefficiente di correlazione intraclasse\nIl Coefficiente di Correlazione Intraclasse (ICC) quantifica il grado di somiglianza tra le unità all’interno dello stesso gruppo. A differenza della maggior parte delle altre misure di correlazione, l’ICC viene impiegato con dati organizzati in gruppi anziché con coppie di osservazioni. Questo lo rende particolarmente idoneo per valutare la concordanza tra giudici che stanno valutando lo stesso insieme di individui o item.\nL’ICC si basa sul framework del modello a effetti misti. Questi modelli statistici consentono di analizzare le variazioni dei punteggi sia all’interno dei gruppi (ovvero le differenze tra le osservazioni all’interno dello stesso gruppo) che tra i gruppi (ovvero le differenze tra i gruppi stessi). Pertanto, l’ICC tiene conto di due fonti di varianza: la varianza all’interno dei gruppi (varianza delle valutazioni dei giudici all’interno dello stesso gruppo) e la varianza tra i gruppi (varianza delle valutazioni dei giudici tra gruppi diversi). Inoltre, viene considerato l’errore casuale presente nelle valutazioni.\nIn sintesi, l’ICC valuta la consistenza delle misurazioni quando queste vengono effettuate da diversi osservatori su un insieme di soggetti o item. Un valore elevato di ICC segnala una forte concordanza tra le valutazioni degli osservatori, indicando che le misurazioni sono affidabili e riproducibili. Al contrario, un ICC basso riflette una discrepanza significativa tra gli osservatori, suggerendo che le valutazioni sono meno consistenti.\nIl calcolo dell’ICC si basa sull’uso di un modello statistico ad effetti misti, in cui gli osservatori (giudici) fungono da variabili indipendenti e i punteggi assegnati rappresentano la variabile dipendente. Questo modello consente di esaminare e quantificare diverse fonti di variazione nei dati, tra cui:\n\nLa variazione attribuibile ai soggetti (o item) valutati (la varianza di interesse principale).\nLa variazione attribuibile agli osservatori (giudici).\nLa variazione dovuta all’interazione tra i soggetti (o item) valutati e gli osservatori.\nLa variazione causata da errore casuale.\n\nL’ICC si calcola dividendo la varianza relativa ai soggetti valutati (o item) per la somma di questa varianza più la varianza dovuta all’errore casuale. Questo rapporto esprime la proporzione della varianza totale dei punteggi che può essere attribuita alla differenziazione tra i soggetti valutati (o item), offrendo un indicatore diretto dell’affidabilità delle misurazioni. Pertanto, un ICC elevato indica che la maggior parte della varianza nei punteggi deriva dalle differenze tra i soggetti valutati (o item), confermando l’affidabilità delle valutazioni fornite dagli osservatori.\nTuttavia, ciò che viene considerato come errore dipende da diverse considerazioni, tra cui:\n\nSe il disegno è “crossed” (cioè tutti i giudici valutano ciascun individuo o item) o “nested” (diversi gruppi di giudici valutano ciascun individuo o item). Nel primo caso, l’errore sarà associato alle differenze tra i giudici all’interno dello stesso gruppo di individui (o item) valutati, mentre nel secondo caso l’errore sarà relativo alle differenze tra i gruppi di giudici che valutano individui (o item) diversi.\nSe i giudici sono considerati “fixed” (non generalizzabili) o “random” (generalizzabili a una “popolazione di giudici”). Nel caso in cui i giudici siano considerati “fixed”, si presume che i giudici specifici coinvolti nello studio siano l’unica popolazione rilevante, e l’interesse è focalizzato sulla concordanza tra di loro. Nel caso in cui i giudici siano considerati “random”, si ammette che i giudici coinvolti nello studio siano solo un campione di una popolazione più ampia di giudici, e l’interesse è sulla concordanza che può essere generalizzata a tale popolazione più vasta.\nLa distinzione tra “consistenza” (decisioni relative; viene giudicato di interesse solo l’ordinamento dei punteggi) e “accordo assoluto” (decisioni assolute; viene giudicato di interesse il valore effettivo del punteggio). Nel contesto della valutazione dei giudici, la consistenza si riferisce al grado in cui i giudici sono concordi nell’ordine di classificazione degli individui valutati, indipendentemente dai valori numerici dei punteggi. D’altra parte, l’accordo assoluto misura il grado di concordanza tra i giudici riguardo ai valori numerici dei punteggi assegnati agli individui valutati.\n\nQueste considerazioni sono fondamentali quando si analizzano e interpretano i risultati dell’ICC o di altri metodi di valutazione dell’affidabilità tra giudici o osservatori. La scelta del modello statistico e la definizione dell’errore dipendono da queste varie considerazioni, e la comprensione di questi aspetti è essenziale per una valutazione accurata della concordanza e dell’affidabilità nelle misurazioni effettuate da più giudici o osservatori.\n\n\n12.3.3 ANOVA ad una via per disegni nidificati\nConsideriamo il caso in cui i dati rappresentano da due valutazioni assegnate a ciascuna persona da giudici diversi.\n\nslp_vas_nested &lt;- slp_dat |&gt;\n    mutate(SpeakerID = as.numeric(as.factor(Speaker))) |&gt;\n    # Select only 10 speakers\n    filter(SpeakerID &lt;= 10) |&gt;\n    group_by(Speaker) |&gt;\n    # Filter specific raters\n    filter(row_number() %in% (SpeakerID[1] * 2 - (1:0)))\n\nhead(slp_vas_nested)\n\n\nA grouped_df: 6 x 7\n\n\nslpID\nSpeaker\nslp_VAS\nslp_VAS.Rel\nslp_EST\nslp_EST.Rel\nSpeakerID\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nslp10\nAF1\n96.28\nNA\n90\nNA\n1\n\n\nslp11\nAF1\n100.00\nNA\n80\nNA\n1\n\n\nslp13\nAF9\n0.86\nNA\n10\nNA\n2\n\n\nslp14\nAF9\n3.52\nNA\n10\nNA\n2\n\n\nslp15\nALSF6\n73.87\nNA\n10\nNA\n3\n\n\nslp16\nALSF6\n4.52\n9.03\n5\nNA\n3\n\n\n\n\n\nVerifichiamo che ogni giudice abbia fornito due giudizi per soggetto:\n\nslp_vas_nested %&gt;%\n  group_by(Speaker) %&gt;%\n  summarise(Count = n())\n\n\nA tibble: 10 x 2\n\n\nSpeaker\nCount\n\n\n&lt;chr&gt;\n&lt;int&gt;\n\n\n\n\nAF1\n2\n\n\nAF9\n2\n\n\nALSF6\n2\n\n\nALSF7\n2\n\n\nALSF9\n2\n\n\nALSM1\n2\n\n\nALSM4\n2\n\n\nAM1\n2\n\n\nAM3\n2\n\n\nAM5\n2\n\n\n\n\n\nCi sono 20 giudici:\n\nlength(slp_vas_nested$Speaker)\n\n20\n\n\nIn questo studio, analizziamo dati costituiti da due valutazioni fornite a ciascun individuo da giudici diversi. Il nostro obiettivo è identificare e separare due principali fonti di variazione: le differenze tra i giudici e le variazioni casuali all’interno delle valutazioni di ciascun giudice.\nLa sfida risiede nel distinguere queste fonti di variazione, essenziale per determinare la proporzione di variabilità nei punteggi attribuibile a reali differenze tra i giudici, rispetto a quella derivante da errori casuali. Per affrontare questo problema, applichiamo un’analisi della varianza a effetti casuali (Random-Effects ANOVA), utilizzando il modello lineare misto implementato attraverso la funzione lmer() di R. Questa metodologia ci permette di stimare separatamente la varianza dovuta alle differenze tra i giudici e quella associata all’errore casuale.\nImplementiamo il modello lmer(slp_VAS ~ 1 + (1 | Speaker), data = slp_vas_nested), dove il termine Speaker rappresenta i giudici. Attraverso questo modello, trattiamo le intercette associate a ciascun giudice come effetti casuali, consentendoci di catturare le specifiche variazioni tra i giudici al di là delle fluttuazioni casuali.\n\nm1 &lt;- lmer(slp_VAS ~ 1 + (1 | Speaker), data = slp_vas_nested)\nsummary(m1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: slp_VAS ~ 1 + (1 | Speaker)\n   Data: slp_vas_nested\n\nREML criterion at convergence: 179.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.50049 -0.77609 -0.05948  0.87584  1.35589 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Speaker  (Intercept) 308.9    17.58   \n Residual             816.8    28.58   \nNumber of obs: 19, groups:  Speaker, 10\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   53.613      8.627   6.214\n\n\nDall’analisi emergono due componenti principali di varianza: - La varianza attribuibile a differenze intrinseche tra i soggetti valutati (slpID), che riflette la variabilità naturale tra gli individui. - La varianza attribuibile ai giudici (Speaker), che misura l’estensione del bias sistematico, ossia la tendenza di alcuni giudici a fornire valutazioni sistematicamente più alte o più basse rispetto ad altri.\nIn aggiunta, abbiamo la varianza residua, che include l’errore di misurazione e altre fonti di variabilità non spiegate dal modello.\n\nvc_m1 &lt;- as.data.frame(VarCorr(m1))\nvc_m1\n\n\nA data.frame: 2 x 5\n\n\ngrp\nvar1\nvar2\nvcov\nsdcor\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nSpeaker\n(Intercept)\nNA\n308.9062\n17.57573\n\n\nResidual\nNA\nNA\n816.8141\n28.57996\n\n\n\n\n\nPer valutare l’affidabilità delle valutazioni, calcoliamo l’Intraclass Correlation Coefficient (ICC) utilizzando i dati estratti dal modello. Questo coefficiente quantifica la proporzione della varianza totale attribuibile alle differenze tra i soggetti valutati, offrendoci un indice dell’affidabilità delle valutazioni in contesti dove sono coinvolti giudizi o misurazioni ripetute.\nUn ICC prossimo a 1 indica che la maggior parte della variabilità nei dati è dovuta a differenze reali tra i soggetti valutati, mentre un valore più basso suggerisce un’influenza significativa di variazioni casuali o di bias dei giudici sulla variabilità totale osservata.\nCalcoliamo l’ICC per una singola valutazione:\n\\[\nICC = \\frac{\\sigma^2_{\\text{giudici}}} {\\sigma^2_{\\text{giudici}} + \\sigma^2_E}.\n\\]\nNella formula, l’ICC misura la proporzione della varianza totale del punteggio slp_VAS che è attribuibile alla variazione tra i gruppi dei giudici (Speaker) rispetto alla variazione residua o errore.\n\nvc_m1$vcov[1] / (vc_m1$vcov[1] + vc_m1$vcov[2])\n\n0.274407605476794\n\n\nUtilizziamo la funzione icc() del pacchetto performance:\n\nperformance::icc(m1)\n\n\nA icc: 1 x 3\n\n\nICC_adjusted\nICC_conditional\nICC_unadjusted\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n0.2744076\n0.2744076\n0.2744076\n\n\n\n\n\nQuesta formula calcola l’affidabilità per una singola valutazione da un singolo giudice. La formula considera la varianza tra i soggetti rispetto alla varianza totale, che include sia la varianza tra i soggetti sia l’errore di misurazione.\nCalcoliamo ora l’ICC per la valutazione media:\n\\[\nICC = \\frac{\\sigma^2_{\\text{giudici}}} {\\sigma^2_{\\text{giudici}} + \\sigma^2_E / k},\n\\]\ndove \\(k\\) rappresenta il numero di giudizi assegnati a ciascun soggetto da ciascun giudice.\nLa formula calcola l’ICC basandosi sulla varianza tra i gruppi (in questo caso, la varianza attribuibile ai giudici, \\(\\sigma^2_{\\text{giudici}}\\)) e la varianza entro i gruppi (l’errore casuale, \\(\\sigma^2_E\\)), aggiustando per il numero di giudizi (\\(k\\)) dati per ogni soggetto. L’inserimento di \\(k\\) nel denominatore serve a normalizzare l’effetto dell’errore casuale in base al numero di giudizi, assumendo che più giudizi per soggetto possano ridurre l’impatto dell’errore casuale sulla stima dell’affidabilità.\nQuindi, se l’ICC è vicino a 1, ciò indica che la maggior parte della varianza nei dati è dovuta a differenze reali tra i soggetti valutati, piuttosto che a variazioni casuali o a differenze tra i giudici. In questo modo, l’ICC fornisce un indice utile per valutare l’affidabilità delle valutazioni in studi in cui sono coinvolti giudizi o misurazioni ripetute.\nNel caso presente, ogni giudice fornisce due valutazioni per ogni soggetto:\n\nvc_m1$vcov[1] / (vc_m1$vcov[1] + vc_m1$vcov[2] / 2)\n\n0.430643389599249\n\n\nQuesta seconda formula calcola l’affidabilità per la media delle valutazioni di più giudici. È pertinente quando si ha a che fare con più valutazioni per ciascun soggetto e si vuole sapere l’affidabilità della media di queste valutazioni. Questa formula differisce dalla prima perché considera la riduzione dell’errore di misurazione che si verifica quando si calcola la media di più valutazioni.\n\n\n12.3.4 ANOVA a due vie e l’Impatto del Bias dei Giudici\nL’ANOVA a due vie è uno strumento statistico cruciale per esaminare gli effetti di più fattori, inclusi i giudici, sulla variabile dipendente in contesti di valutazione. Per interpretare accuratamente i risultati, è fondamentale distinguere tra il bias sistematico introdotto dai giudici e la variabilità intrinseca nelle valutazioni.\n\nBias dei Giudici: Questo si verifica quando i giudici hanno una tendenza costante a fornire valutazioni più alte o più basse rispetto agli altri, influenzando potenzialmente l’equità delle valutazioni.\nVariabilità Intrinseca: Si riferisce alle differenze naturali nelle valutazioni che emergono dalle percezioni individuali o dalle interpretazioni soggettive dei giudici.\n\nIn termini di Contesto di Valutazione: - Nelle analisi di Consistenza, dove l’attenzione è sull’ordine relativo delle valutazioni, il bias dei giudici non è considerato problematico poiché non altera questo ordine. - Nelle analisi di Accordo, che si concentrano sulla concordanza dei valori assoluti delle misurazioni, il bias dei giudici è trattato come una fonte di errore significativa.\nL’applicazione dell’ANOVA a due vie consente di distinguere l’effetto del bias dei giudici rispetto ad altre fonti di varianza, offrendo così un’importante metodologia per valutare l’affidabilità delle valutazioni e l’impatto del bias in diversi contesti.\nUtilizzando lmer(slp_VAS ~ 1 + (1 | Speaker) + (1 | slpID), data = slp_dat), analizziamo i dati considerando due principali fonti di variazione: - Effetti Casuali dei Giudici ((1 | Speaker)): Valutiamo il bias dei giudici come fonte di variazione, distinguendo tra analisi di consistenza e di accordo a seconda di come questo bias influisce sui risultati. - Variabilità Intrinseca tra i Soggetti ((1 | slpID)): Esploriamo le differenze naturali tra i soggetti valutati, fondamentali per comprendere la diversità delle risposte individuali.\nQuesto approccio ci permette di facilitare la distinzione tra l’effetto del bias dei giudici e altre fonti di varianza, fornendo una metodologia preziosa per valutare l’affidabilità delle valutazioni e l’impatto del bias in diversi contesti.\nEseguiamo l’analisi con lmer():\n\nm2 &lt;- lmer(slp_VAS ~ 1 + (1 | Speaker) + (1 | slpID), data = slp_dat)\n\nQuesta formula considera due principali fonti di variazione:\n\nEffetti Casuali dei Giudici ((1 | Speaker)): Valutiamo il bias dei giudici come una fonte di variazione, differenziando tra le analisi di consistenza e di accordo a seconda dell’impatto di questo bias sui risultati.\nVariabilità Intrinseca tra i Soggetti ((1 | slpID)): Esploriamo le differenze naturali tra i soggetti valutati, cruciali per comprendere la diversità delle risposte individuali.\n\n\nsummary(m2)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: slp_VAS ~ 1 + (1 | Speaker) + (1 | slpID)\n   Data: slp_dat\n\nREML criterion at convergence: 3544.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.3281 -0.5813  0.0862  0.6611  2.5747 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n slpID    (Intercept) 132.8    11.53   \n Speaker  (Intercept) 585.7    24.20   \n Residual             291.2    17.07   \nNumber of obs: 403, groups:  slpID, 21; Speaker, 20\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   61.882      6.028   10.27\n\n\nEstraiamo le fonti di varianza:\n\nvc_m2 &lt;- as.data.frame(VarCorr(m2))\nvc_m2\n\n\nA data.frame: 3 x 5\n\n\ngrp\nvar1\nvar2\nvcov\nsdcor\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nslpID\n(Intercept)\nNA\n132.8257\n11.52500\n\n\nSpeaker\n(Intercept)\nNA\n585.6568\n24.20035\n\n\nResidual\nNA\nNA\n291.2401\n17.06576\n\n\n\n\n\n\n\n12.3.5 Componenti di Varianza\n\nVariazione dovuta a slpID (Intercept): Indica la variabilità attribuibile alle differenze tra i soggetti.\nVariazione dovuta a Speaker (Intercept): Riflette la variabilità nelle valutazioni dovuta al bias dei giudici.\nResidui (Residual): Comprende l’errore di misurazione e altre fonti di variabilità non spiegate.\n\n\n\n12.3.6 Calcolo dell’ICC\nPer calcolare l’ICC, dobbiamo distinguere tra le diverse versioni dell’ICC basate sulle definizioni generali. Tuttavia, il calcolo specifico dell’ICC può variare a seconda della struttura del modello e dell’interpretazione desiderata. Nella versione più semplice e comune, si considera la formula:\n\\[\n\\text{ICC} = \\frac{\\sigma^2_{\\text{tra gruppo}}}{\\sigma^2_{\\text{tra gruppo}} + \\sigma^2_{\\text{errore}}} \\]\nDove $ ^2_{} $ è la varianza attribuita agli effetti casuali tra gruppi, e $ ^2_{} $ è la varianza residua.\nNel modello precedente, ci sono due componenti di varianza tra gruppi (slpID e Speaker), quindi è possibile considerare la somma di queste due come la varianza totale tra gruppo. Così, l’ICC adjusted può essere calcolato come:\n\\[\n\\text{ICC}_{\\text{adjusted}} = \\frac{\\sigma^2_{slpID} + \\sigma^2_{Speaker}}{\\sigma^2_{slpID} + \\sigma^2_{Speaker} + \\sigma^2_{Residual}}\n\\]\nSostituendo i valori ottenuti:\n\\[ \\text{ICC}_{\\text{adjusted}} = \\frac{132.8257 + 585.6568}{132.8257 + 585.6568 + 291.2401} \\]\nQuesto calcolo assume che tutte le componenti di varianza contribuiscano al calcolo dell’ICC in modo equivalente e che l’ICC adjusted consideri la somma delle varianze tra gruppi (in questo caso, slpID e Speaker) rispetto alla varianza totale (inclusa la varianza residua).\nCalcoliamo ora questo valore usando Python per ottenere il risultato esatto.\n\n(vc_m2$vcov[1] + vc_m2$vcov[2]) / (vc_m2$vcov[1] + vc_m2$vcov[2] + vc_m2$vcov[3])\n\n0.711564270978534\n\n\nQuesto valore riproduce quello trovato da performance::icc():\n\nperformance::icc(m2)\n\n\nA icc: 1 x 3\n\n\nICC_adjusted\nICC_conditional\nICC_unadjusted\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n0.7115643\n0.7115643\n0.7115643\n\n\n\n\n\nL’Intraclass Correlation Coefficient (ICC) di 0.7115643 offre una misura dell’affidabilità o della coerenza delle valutazioni all’interno dei gruppi definiti nel tuo modello (in questo caso, slpID e Speaker).\n\nValore dell’ICC: Il valore di 0.71 indica un livello relativamente alto di coerenza o omogeneità tra le misurazioni all’interno dei gruppi rispetto alla variabilità totale. In altre parole, una proporzione sostanziale della varianza totale nei dati è attribuibile alle differenze tra i gruppi piuttosto che alle variazioni casuali all’interno dei gruppi o agli errori di misurazione.\nAffidabilità delle Misure: Un ICC più vicino a 1 suggerisce che le misure sono molto affidabili, poiché indica che la maggior parte della varianza nei dati può essere attribuita alle differenze sistematiche tra i gruppi piuttosto che al rumore casuale o agli errori. Nel tuo caso, un ICC di circa 0.71 può essere interpretato come indicativo di un’alta affidabilità, suggerendo che le differenze tra i gruppi (ad esempio, tra diversi Speaker o differenti slpID) sono significative e consistenti.\nImplicazioni per la Ricerca: Per la ricerca, un ICC alto come questo implica che il disegno dello studio e la misurazione utilizzata sono adeguatamente sensibili per distinguere tra gli individui o le unità all’interno dei gruppi definiti. Questo è particolarmente rilevante quando si studiano gli effetti di interventi o trattamenti specifici su gruppi distinti o si valuta la consistenza delle risposte tra i membri di un gruppo.\nContesto e Benchmark: L’interpretazione dell’ICC dipende dal contesto specifico e dal campo di studio. Alcuni campi potrebbero considerare un ICC di 0.71 come eccellente, mentre altri potrebbero considerarlo solo moderatamente buono. È importante confrontare questo valore con benchmark o standard specifici del campo di interesse.\nStruttura del Modello e Dati: L’interpretazione dovrebbe anche tenere conto della struttura specifica del modello e della natura dei dati. Diversi tipi di ICC possono essere calcolati a seconda degli obiettivi dello studio e della configurazione del modello misto, quindi è cruciale assicurarsi che l’ICC calcolato sia il più appropriato per il tuo specifico contesto di ricerca.\n\nIn sintesi, un ICC di 0.7115643 indica un’alta affidabilità nelle misure all’interno dei gruppi definiti nel tuo modello, suggerendo che le variazioni osservate sono significativamente influenzate dalle differenze tra i gruppi piuttosto che dalla varianza casuale o dall’errore di misurazione.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>L'affidabilità tra giudici</span>"
    ]
  },
  {
    "objectID": "chapters/raters/02_interrater_reliability.html#conclusioni",
    "href": "chapters/raters/02_interrater_reliability.html#conclusioni",
    "title": "12  L’affidabilità tra giudici",
    "section": "12.4 Conclusioni",
    "text": "12.4 Conclusioni\nL’Intraclass Correlation Coefficient (ICC) può essere interpretato come “la proporzione di varianza spiegata dalla struttura di raggruppamento nella popolazione”. Questa struttura di raggruppamento implica che le misurazioni siano organizzate in gruppi (ad esempio, i punteggi di un test in una scuola possono essere raggruppati per classe, se ci sono più classi e a ciascuna è stato somministrato lo stesso test), e l’ICC indica quanto fortemente le misurazioni nello stesso gruppo si assomiglino. Questo indice varia da 0, se il raggruppamento non trasmette alcuna informazione, a 1, se tutte le osservazioni in un gruppo sono identiche (Gelman e Hill, 2007, p. 258). In altre parole, l’ICC - a volte concettualizzato come ripetibilità della misurazione - “può anche essere interpretato come la correlazione attesa tra due unità estratte casualmente che si trovano nello stesso gruppo” (Hox 2010), sebbene questa definizione potrebbe non applicarsi a modelli misti con strutture di effetti casuali più complesse. L’ICC può aiutare a determinare se un modello misto è necessario: un ICC pari a zero (o molto vicino a zero) significa che le osservazioni all’interno dei cluster non sono più simili tra loro rispetto a osservazioni di cluster diversi, e quindi considerarlo come un fattore casuale potrebbe non essere necessario.\n\nAlto ICC significa che i dati sono altamente raggruppati, il che significa che i risultati dipendono fortemente dai gruppi a cui appartengono le osservazioni.\nBasso ICC significa che i dati non sono altamente (o per niente) raggruppati, il che significa che i risultati non dipendono molto (o per niente) dai gruppi a cui appartengono le osservazioni.\n\nDifferenza con R^2\nIl coefficiente di determinazione R^2 quantifica la proporzione di varianza spiegata da un modello statistico, ma la sua definizione in modelli misti è complessa (da qui l’esistenza di diversi metodi per calcolare un’approssimazione). L’ICC è associato a R^2 poiché entrambi sono rapporti di componenti di varianza. Più precisamente, R^2 rappresenta la proporzione della varianza spiegata (del modello completo), mentre l’ICC è la proporzione della varianza spiegata attribuibile agli effetti casuali.\nCalcolo\nL’ICC viene calcolato dividendo la varianza degli effetti casuali, σ^2_i, per la varianza totale, cioè la somma della varianza degli effetti casuali e della varianza residua, σ^2_ε.\nICC aggiustato e non aggiustato\nLa funzione icc() calcola un ICC aggiustato e uno non aggiustato, che tengono conto di tutte le fonti di incertezza (cioè di tutti gli effetti casuali). Mentre l’ICC aggiustato si riferisce solo agli effetti casuali, l’ICC non aggiustato considera anche le varianze degli effetti fissi, aggiungendo più precisamente la varianza degli effetti fissi al denominatore della formula per calcolare l’ICC (vedi Nakagawa et al. 2017). Tipicamente, l’ICC aggiustato è di interesse quando l’analisi degli effetti casuali è di interesse. icc() restituisce un valore ICC anche per strutture di effetti casuali più complesse, come modelli con pendenze casuali o design nidificato (più di due livelli) ed è applicabile per modelli con distribuzioni diverse dalla gaussiana.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>L'affidabilità tra giudici</span>"
    ]
  },
  {
    "objectID": "chapters/raters/02_interrater_reliability.html#session-info",
    "href": "chapters/raters/02_interrater_reliability.html#session-info",
    "title": "12  L’affidabilità tra giudici",
    "section": "12.5 Session Info",
    "text": "12.5 Session Info\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lme4_1.1-35.1     Matrix_1.6-5      ggokabeito_0.1.0  viridis_0.6.5    \n [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    bayesplot_1.11.1 \n [9] gridExtra_2.3     patchwork_1.2.0   semTools_0.5-6    semPlot_1.1.6    \n[13] lavaan_0.6-17     psych_2.4.1       scales_1.3.0      markdown_1.12    \n[17] knitr_1.45        lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1    \n[21] dplyr_1.1.4       purrr_1.0.2       readr_2.1.5       tidyr_1.3.1      \n[25] tibble_3.2.1      ggplot2_3.5.0     tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.15.0  jsonlite_1.8.8     magrittr_2.0.3    \n  [4] nloptr_2.0.3       rmarkdown_2.26     vctrs_0.6.5       \n  [7] minqa_1.2.6        base64enc_0.1-3    rstatix_0.7.2     \n [10] htmltools_0.5.7    broom_1.0.5        Formula_1.2-5     \n [13] htmlwidgets_1.6.4  plyr_1.8.9         uuid_1.2-0        \n [16] igraph_2.0.2       mime_0.12          lifecycle_1.0.4   \n [19] pkgconfig_2.0.3    R6_2.5.1           fastmap_1.1.1     \n [22] shiny_1.8.0        digest_0.6.35      OpenMx_2.21.11    \n [25] fdrtool_1.2.17     colorspace_2.1-0   rprojroot_2.0.4   \n [28] Hmisc_5.1-1        fansi_1.0.6        timechange_0.3.0  \n [31] abind_1.4-5        compiler_4.3.3     withr_3.0.0       \n [34] glasso_1.11        htmlTable_2.4.2    backports_1.4.1   \n [37] carData_3.0-5      ggsignif_0.6.4     MASS_7.3-60.0.1   \n [40] corpcor_1.6.10     gtools_3.9.5       tools_4.3.3       \n [43] pbivnorm_0.6.0     foreign_0.8-86     zip_2.3.1         \n [46] httpuv_1.6.14      nnet_7.3-19        glue_1.7.0        \n [49] quadprog_1.5-8     nlme_3.1-164       promises_1.2.1    \n [52] lisrelToR_0.3      grid_4.3.3         pbdZMQ_0.3-11     \n [55] checkmate_2.3.1    cluster_2.1.6      reshape2_1.4.4    \n [58] generics_0.1.3     gtable_0.3.4       tzdb_0.4.0        \n [61] data.table_1.15.2  hms_1.1.3          car_3.1-2         \n [64] utf8_1.2.4         sem_3.1-15         pillar_1.9.0      \n [67] IRdisplay_1.1      rockchalk_1.8.157  later_1.3.2       \n [70] splines_4.3.3      lattice_0.22-5     kutils_1.73       \n [73] tidyselect_1.2.0   miniUI_0.1.1.1     pbapply_1.7-2     \n [76] stats4_4.3.3       xfun_0.42          qgraph_1.9.8      \n [79] arm_1.13-1         stringi_1.8.3      boot_1.3-29       \n [82] evaluate_0.23      mi_1.1             cli_3.6.2         \n [85] RcppParallel_5.1.7 IRkernel_1.3.2     rpart_4.1.23      \n [88] xtable_1.8-4       repr_1.1.6         munsell_0.5.0     \n [91] Rcpp_1.0.12        coda_0.19-4.1      png_0.1-8         \n [94] XML_3.99-0.16.1    parallel_4.3.3     ellipsis_0.3.2    \n [97] jpeg_0.1-10        openxlsx_4.2.5.2   crayon_1.5.2      \n[100] rlang_1.1.3        mnormt_2.1.1      \n\n\n\n\n\n\nHirsch, Micah E, Austin Thompson, Yunjung Kim, e Kaitlin L Lansford. 2022. «The Reliability and Validity of Speech-Language Pathologists’ Estimations of Intelligibility in Dysarthria». Brain Sciences 12 (8): 1011.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>L'affidabilità tra giudici</span>"
    ]
  },
  {
    "objectID": "chapters/raters/E1_irr.html",
    "href": "chapters/raters/E1_irr.html",
    "title": "13  ✏️ Esercizi",
    "section": "",
    "text": "13.1 Dati categoriali\nIniziamo con dati categoriali e utilizziamo un sottoinsieme dei dati forniti dal data frame diagnoses del pacchetto irr.\ndata(diagnoses)\nglimpse(diagnoses)\n\nRows: 30\nColumns: 6\n$ rater1 &lt;fct&gt; 4. Neurosis, 2. Personality Disorder, 2. Personality Disorder, ~\n$ rater2 &lt;fct&gt; 4. Neurosis, 2. Personality Disorder, 3. Schizophrenia, 5. Othe~\n$ rater3 &lt;fct&gt; 4. Neurosis, 2. Personality Disorder, 3. Schizophrenia, 5. Othe~\n$ rater4 &lt;fct&gt; 4. Neurosis, 5. Other, 3. Schizophrenia, 5. Other, 4. Neurosis,~\n$ rater5 &lt;fct&gt; 4. Neurosis, 5. Other, 3. Schizophrenia, 5. Other, 4. Neurosis,~\n$ rater6 &lt;fct&gt; 4. Neurosis, 5. Other, 5. Other, 5. Other, 4. Neurosis, 3. Schi~\ndat &lt;- diagnoses[, 1:3]\nhead(dat)\n\n\nA data.frame: 6 x 3\n\n\n\nrater1\nrater2\nrater3\n\n\n\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;fct&gt;\n\n\n\n\n1\n4. Neurosis\n4. Neurosis\n4. Neurosis\n\n\n2\n2. Personality Disorder\n2. Personality Disorder\n2. Personality Disorder\n\n\n3\n2. Personality Disorder\n3. Schizophrenia\n3. Schizophrenia\n\n\n4\n5. Other\n5. Other\n5. Other\n\n\n5\n2. Personality Disorder\n2. Personality Disorder\n2. Personality Disorder\n\n\n6\n1. Depression\n1. Depression\n3. Schizophrenia\nIniziamo considerando il caso di due giudici. Per questi dati calcoleremo il Kappa di Cohen.\nkappa2(dat[, c(1, 2)], \"unweighted\")\n\n Cohen's Kappa for 2 Raters (Weights: unweighted)\n\n Subjects = 30 \n   Raters = 2 \n    Kappa = 0.651 \n\n        z = 7 \n  p-value = 2.63e-12\nIl Kappa di Cohen valuta la concordanza tra due valutatori su una scala categoriale. Può essere usato quando i valutatori assegnano categorie, non punteggi numerici, agli oggetti di interesse. Un valore di Kappa maggiore di 0 indica una maggiore concordanza tra i valutatori rispetto a quella che ci si aspetterebbe per caso, mentre un valore di 0 indica che la concordanza non è maggiore di quella casuale.\nNel caso presente, l’output indica che:\nL’output fornisce anche un test statistico per valutare se la concordanza osservata è significativamente maggiore di quella che ci si aspetterebbe per caso:\nIn sintesi, l’output indica una concordanza sostanziale tra i due valutatori sulla scala categorica utilizzata, e questa concordanza è statisticamente significativa. Questo suggerisce che i giudizi dei due valutatori sono affidabili e consistenti tra loro oltre quello che ci si aspetterebbe semplicemente per caso.\nSe ci sono più di due giudici, usiamo il Kappa di Fleiss.\nkappam.fleiss(dat)\n\n Fleiss' Kappa for m Raters\n\n Subjects = 30 \n   Raters = 3 \n    Kappa = 0.534 \n\n        z = 9.89 \n  p-value = 0\nÈ anche possibile utilizzare il Kappa esatto di Conger (1980).\nkappam.fleiss(dat, exact = TRUE)\n\n Fleiss' Kappa for m Raters (exact value)\n\n Subjects = 30 \n   Raters = 3 \n    Kappa = 0.55\nNel caso di 3 giudici, otteniamo un valore di Kappa di Fleiss pari a 0.53, il che indica un’accordo moderato tra gli osservatori. Questo significa che c’è una certa concordanza tra di loro, ma non c’è né una forte né una perfetta corrispondenza. In generale, un Kappa di Fleiss compreso tra 0.41 e 0.60 può essere considerato come un’accordo moderato.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/raters/E1_irr.html#dati-categoriali",
    "href": "chapters/raters/E1_irr.html#dati-categoriali",
    "title": "13  ✏️ Esercizi",
    "section": "",
    "text": "Subjects = 30: Ci sono 30 oggetti o soggetti che sono stati valutati dai giudici.\nRaters = 2: Due valutatori hanno partecipato alla valutazione.\nKappa = 0.651: Il valore di Kappa, 0.651, suggerisce una concordanza sostanziale tra i due valutatori. Secondo le linee guida generalmente accettate per l’interpretazione del coefficiente di Kappa, un valore tra 0.61 e 0.80 indica una concordanza sostanziale.\n\n\n\nz = 7: Il valore z del test statistico è 7. Questo indica la distanza della statistica di test (Kappa osservato) dal valore di Kappa atteso sotto l’ipotesi nulla (nessuna concordanza oltre il caso), misurata in unità di deviazione standard. Un valore elevato indica una forte evidenza contro l’ipotesi nulla.\np-value = 2.63e-12: Il p-value è estremamente basso, molto inferiore a qualsiasi soglia di significatività comune (ad esempio, 0.05 o 0.01). Questo suggerisce che la probabilità di osservare un valore di Kappa come quello ottenuto (o più estremo) se in realtà non ci fosse concordanza tra i valutatori (oltre quella casuale) è estremamente bassa. In altre parole, c’è una forte evidenza statistica che la concordanza tra i due valutatori è significativamente maggiore di quella che ci si aspetterebbe per caso.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/raters/E1_irr.html#dati-ordinali",
    "href": "chapters/raters/E1_irr.html#dati-ordinali",
    "title": "13  ✏️ Esercizi",
    "section": "13.2 Dati ordinali",
    "text": "13.2 Dati ordinali\nSe i dati sono ordinali, è necessario utilizzare un Kappa ponderato. Ad esempio, se i valori possibili sono basso, medio e alto, allora se un caso viene valutato come medio da un codificatore e alto dall’altro, essi sarebbero in un accordo maggiore rispetto a una situazione in cui le valutazioni fossero basso e alto.\nPer chiarire ulteriormente, il concetto di Kappa ponderato si basa sull’idea che non tutte le discrepanze tra i codificatori siano ugualmente gravi. Nell’esempio dato, la differenza tra le valutazioni medio e alto è considerata meno significativa rispetto alla differenza tra basso e alto, perché le categorie sono vicine l’una all’altra nell’ordine. Il Kappa ponderato introduce quindi dei pesi per riflettere questa differenza di gravità nelle discrepanze, valutando le discrepanze minori (come tra medio e alto) meno severamente delle discrepanze maggiori (come tra basso e alto). Questo approccio è particolarmente utile in contesti dove l’ordine e la distanza tra le categorie sono informativi e importanti per l’analisi.\nUsiamo i dati forniti dal data frame anxiety del pacchetto irr.\n\ndata(anxiety)\n\ndfa &lt;- anxiety[, c(1, 2)]\ndfa\n\n\nA data.frame: 20 x 2\n\n\n\nrater1\nrater2\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n3\n3\n\n\n2\n3\n6\n\n\n3\n3\n4\n\n\n4\n4\n6\n\n\n5\n5\n2\n\n\n6\n5\n4\n\n\n7\n2\n2\n\n\n8\n3\n4\n\n\n9\n5\n3\n\n\n10\n2\n3\n\n\n11\n2\n2\n\n\n12\n6\n3\n\n\n13\n1\n3\n\n\n14\n5\n3\n\n\n15\n2\n2\n\n\n16\n2\n2\n\n\n17\n1\n1\n\n\n18\n2\n3\n\n\n19\n4\n3\n\n\n20\n3\n4\n\n\n\n\n\nPossiamo calcolare il Kappa ponderato sui giudizi di due valutatori. È possibile usare pesi lineari o quadrati delle differenze.\n\nkappa2(dfa, \"squared\")\n\n Cohen's Kappa for 2 Raters (Weights: squared)\n\n Subjects = 20 \n   Raters = 2 \n    Kappa = 0.297 \n\n        z = 1.34 \n  p-value = 0.18 \n\n\nQuesto valore di Kappa suggerisce che c’è un livello di accordo “equo” tra i due valutatori, considerando che i pesi delle discrepanze tra le valutazioni sono calcolati al quadrato. La scala convenzionale per interpretare Kappa è la seguente: valori ≤ 0 indicano nessun accordo, 0.01–0.20 leggero, 0.21–0.40 equo, 0.41–0.60 moderato, 0.61–0.80 sostanziale, e 0.81–1.00 quasi perfetto.\nIl valore di z è una misura della distanza statistica del valore di Kappa dal valore nullo (nessun accordo oltre il caso), espresso in termini di deviazioni standard. Un p-value di 0.18 indica che non c’è una significatività statistica per rifiutare l’ipotesi nulla di nessun accordo oltre il caso, al livello di significatività convenzionale di 0.05.\n\nkappa2(dfa, \"equal\")\n\n Cohen's Kappa for 2 Raters (Weights: equal)\n\n Subjects = 20 \n   Raters = 2 \n    Kappa = 0.189 \n\n        z = 1.42 \n  p-value = 0.157 \n\n\nCon pesi uguali per le discrepanze tra le valutazioni, il valore di Kappa scende a 0.189, indicando sempre un livello di accordo “leggero” verso “equo” tra i valutatori. Questo suggerisce che l’accordo non è molto forte e che le valutazioni differiscono più di quanto non facciano con i pesi al quadrato.\nCon un valore di z leggermente più alto rispetto al primo caso e un p-value di 0.157, anche qui non c’è evidenza statistica sufficiente per rifiutare l’ipotesi nulla. Il risultato implica che l’accordo osservato potrebbe ancora essere dovuto al caso, anche se il p-value è leggermente più basso qui, suggerendo una tendenza (anche se non significativa) verso un accordo maggiore rispetto al caso.\nIn conclusione, entrambi gli output indicano un livello di accordo che va da leggero a equo tra i due valutatori, con nessuna delle due misure che raggiunge la significatività statistica. Ciò suggerisce che, mentre c’è qualche grado di accordo oltre la coincidenza casuale, non è forte. La scelta dei pesi influisce leggermente sui risultati, con i pesi al quadrato che mostrano un livello di accordo leggermente superiore rispetto ai pesi uguali. Questo potrebbe riflettere la natura delle discrepanze nelle valutazioni: l’utilizzo di pesi al quadrato penalizza di più le grandi discrepanze rispetto ai pesi uguali.\n\nkappa2(dfa, \"unweighted\")\n\n Cohen's Kappa for 2 Raters (Weights: unweighted)\n\n Subjects = 20 \n   Raters = 2 \n    Kappa = 0.119 \n\n        z = 1.16 \n  p-value = 0.245 \n\n\n\nKappa = 0.119: Questo valore rappresenta il livello più basso di accordo tra i tre casi analizzati, indicando un accordo molto debole tra i valutatori. La mancanza di pesi implica che tutti i disaccordi sono stati trattati uniformemente, indipendentemente dalla loro gravità o distanza.\nz = 1.16, p-value = 0.245: Questo risultato conferma ulteriormente che l’accordo tra i valutatori non è statisticamente significativo, con un p-value ancora più alto rispetto ai casi precedenti, suggerendo una forte possibilità che qualsiasi accordo osservato sia casuale.\n\nI diversi valori di Kappa nei tre scenari riflettono l’effetto della ponderazione (o mancanza di essa) sulla valutazione dell’accordo. L’accordo sembra diminuire man mano che si passa da pesi quadrati a pesi uguali e infine a nessuna ponderazione, indicando che la severità dei disaccordi ha un impatto notevole sull’accordo percepito.\nNessuno dei tre scenari ha mostrato un accordo statisticamente significativo tra i valutatori, come indicato dai p-value superiori a 0.05. Ciò suggerisce che, indipendentemente dal metodo di ponderazione utilizzato, l’accordo osservato tra i valutatori potrebbe non essere distintamente migliore di quello che ci si aspetterebbe per caso.\nLa scelta del metodo di ponderazione può influenzare fortemente la stima dell’accordo tra i valutatori. In contesti dove la gravità dei disaccordi è importante, i pesi possono fornire una misura più accurata dell’accordo effettivo. Tuttavia, la mancanza di significatività statistica in tutti e tre i casi solleva questioni sulla coerenza delle valutazioni o sulla possibile necessità di formazione aggiuntiva per i valutatori per migliorare l’affidabilità delle loro valutazioni.\nI dati usati in precedenza erano numerici, ma il Kappa ponderato può anche essere calcolato nel caso di dati categoriali. In R i dati categoriali sono codificati in termini di fattori. Si noti che i livelli dei fattori devono essere nell’ordine corretto, altrimenti i risultati saranno errati.\n\ndfa2 &lt;- dfa\ndfa2$rater1 &lt;- factor(dfa2$rater1, levels = 1:6, labels = LETTERS[1:6])\ndfa2$rater2 &lt;- factor(dfa2$rater2, levels = 1:6, labels = LETTERS[1:6])\ndfa2 |&gt; head()\n\n\nA data.frame: 6 x 2\n\n\n\nrater1\nrater2\n\n\n\n&lt;fct&gt;\n&lt;fct&gt;\n\n\n\n\n1\nC\nC\n\n\n2\nC\nF\n\n\n3\nC\nD\n\n\n4\nD\nF\n\n\n5\nE\nB\n\n\n6\nE\nD\n\n\n\n\n\n\nlevels(dfa2$rater1) |&gt; print()\n\n[1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\"\n\n\n\nlevels(dfa2$rater2) |&gt; print()\n\n[1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\"\n\n\nI risultati sono uguali a quelli ottenuti con i dati numerici.\n\nkappa2(dfa2, \"squared\")\n\n Cohen's Kappa for 2 Raters (Weights: squared)\n\n Subjects = 20 \n   Raters = 2 \n    Kappa = 0.297 \n\n        z = 1.34 \n  p-value = 0.18 \n\n\n\nkappa2(dfa2, \"equal\")\n\n Cohen's Kappa for 2 Raters (Weights: equal)\n\n Subjects = 20 \n   Raters = 2 \n    Kappa = 0.189 \n\n        z = 1.42 \n  p-value = 0.157",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/raters/E1_irr.html#dati-continui",
    "href": "chapters/raters/E1_irr.html#dati-continui",
    "title": "13  ✏️ Esercizi",
    "section": "13.3 Dati continui",
    "text": "13.3 Dati continui\nQuando ci si confronta con variabili continue, è necessario calcolare il Coefficiente di Correlazione Intraclasse (ICC) per valutare l’accordo tra le misurazioni. La selezione dell’ICC appropriato richiede l’attenzione su diversi aspetti (Shrout e Fleiss, 1979). Questi includono:\n\nSelezione del Modello: È cruciale determinare se trattare esclusivamente i soggetti come effetti casuali, seguendo il modello “oneway” (impostazione predefinita), oppure se sia soggetti che valutatori sono stati scelti casualmente da un pool più ampio, adottando il modello “twoway”. Tale decisione dipende dalla struttura del disegno di studio e dall’obiettivo dell’analisi.\nInteresse per le Differenze: Nel caso in cui si desideri esaminare le differenze nei punteggi medi tra i valutatori, è preferibile calcolare l’“accordo” anziché la “coerenza” (quest’ultima è l’opzione predefinita). Questa scelta è cruciale quando le differenze sistematiche tra i valutatori sono rilevanti per l’analisi.\nUnità di Analisi: È necessario decidere se l’unità di analisi dovrebbe essere la media di più punteggi o se mantenere l’analisi su valori singoli (impostazione predefinita, unità=“singola”). La trasformazione dell’unità di analisi in “media” può essere appropriata quando l’interesse è concentrato sulla stima aggregata delle misurazioni, mentre l’opzione “singola” è generalmente preferita per esaminare la variabilità tra misurazioni individuali.\n\nLa scelta tra queste opzioni dovrebbe essere guidata dagli obiettivi specifici della ricerca, dalla natura dei dati e dal contesto in cui le misurazioni sono state raccolte. Tali decisioni influenzano notevolmente l’interpretazione del Coefficiente di Correlazione Intraclasse e, di conseguenza, le conclusioni che possono essere tratte riguardo all’accordo o alla variabilità delle misurazioni all’interno del campione studiato.\nPer illustrare questi concetti, useremo il set di dati anxiety dal pacchetto “irr”.\n\ndata(anxiety)\nanxiety |&gt; head()\n\n\nA data.frame: 6 x 3\n\n\n\nrater1\nrater2\nrater3\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n3\n3\n2\n\n\n2\n3\n6\n1\n\n\n3\n3\n4\n4\n\n\n4\n4\n6\n4\n\n\n5\n5\n2\n3\n\n\n6\n5\n4\n2\n\n\n\n\n\n\nicc(anxiety, model = \"twoway\", type = \"agreement\")\n\n Single Score Intraclass Correlation\n\n   Model: twoway \n   Type : agreement \n\n   Subjects = 20 \n     Raters = 3 \n   ICC(A,1) = 0.198\n\n F-Test, H0: r0 = 0 ; H1: r0 &gt; 0 \n F(19,39.7) = 1.83 , p = 0.0543 \n\n 95%-Confidence Interval for ICC Population Values:\n  -0.039 &lt; ICC &lt; 0.494\n\n\nPossiamo replicare questo risultato usando un modello ad effetti misti. Per fare questo dobbiamo prima trasformare i dati in formato long.\n\nanxiety_long &lt;- anxiety %&gt;%\n    mutate(subject = row_number()) %&gt;%\n    pivot_longer(\n        cols = starts_with(\"rater\"),\n        names_to = \"rater\",\n        values_to = \"score\",\n        names_prefix = \"rater\"\n    )\nglimpse(anxiety_long)\n\nRows: 60\nColumns: 3\n$ subject &lt;int&gt; 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7,~\n$ rater   &lt;chr&gt; \"1\", \"2\", \"3\", \"1\", \"2\", \"3\", \"1\", \"2\", \"3\", \"1\", \"2\", \"3\", \"1~\n$ score   &lt;int&gt; 3, 3, 2, 3, 6, 1, 3, 4, 4, 4, 6, 4, 5, 2, 3, 5, 4, 2, 2, 2, 1,~\n\n\nPossiamo ora adattare il modello misto ai dati.\n\nmodel &lt;- lmer(score ~ 1 + (1 | subject) + (1 | rater), data = anxiety_long)\n\nIl procedimento utilizzato da lmer per modellare i dati e calcolare componenti di varianza, che poi possono essere utilizzati per stimare l’Intraclass Correlation Coefficient (ICC), si basa sull’analisi degli effetti misti. Ecco una spiegazione dettagliata del processo:\n\nDefinizione del Modello: lmer(score ~ 1 + (1 | subject) + (1 | rater), data = anxiety_long) specifica un modello lineare misto, dove score è la variabile dipendente. La notazione (1 | subject) indica che si vuole considerare un effetto casuale per ogni soggetto (subject) che corrisponde unicamente all’intercetta. Analogamente, (1 | rater) indica un effetto casuale, corrispondente all’intercetta, per ogni valutatore (rater), permettendo che ogni valutatore possa avere una sua propria tendenza generale nella valutazione. Il termine 1 rappresenta l’intercetta fissa comune a tutti i dati.\nEffetti Fissi e Casuali: In questo modello, l’unico effetto fisso è l’intercetta globale (il termine 1 nel modello), che rappresenta la media generale dei punteggi di ansia. Gli effetti casuali sono rappresentati dalle varianze associate a ciascun soggetto e valutatore, indicando che si riconosce la presenza di variazioni individuali nei punteggi di ansia attribuibili a questi due fattori.\nInterpretazione delle Componenti di Varianza: Le componenti di varianza estratte dal modello rappresentano la quantità di variazione nei punteggi di ansia attribuibili a variazioni tra soggetti (var_subject), a variazioni tra valutatori (var_rater), e alla variazione residua non spiegata dal modello (var_residual).\n\nDopo aver stimato le componenti di varianza, l’ICC può essere calcolato come una proporzione della varianza tra soggetti e tra giudici rispetto alla varianza totale.\nEsaminiamo le componenti di varianza:\n\nvc &lt;- as.data.frame(VarCorr(model))\nvc\n\n\nA data.frame: 3 x 5\n\n\ngrp\nvar1\nvar2\nvcov\nsdcor\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nsubject\n(Intercept)\nNA\n0.3991227\n0.6317616\n\n\nrater\n(Intercept)\nNA\n0.1684205\n0.4103906\n\n\nResidual\nNA\nNA\n1.4482458\n1.2034308\n\n\n\n\n\nCalcolo dell’ICC:\n\n(vc$vcov[1] + vc$vcov[2]) / (vc$vcov[1] + vc$vcov[2] + vc$vcov[3])\n\n0.281548906954477\n\n\nQuesto risultato riproduce quello trovato da icc():\n\nperformance::icc(model)\n\n\nA icc: 1 x 3\n\n\nICC_adjusted\nICC_conditional\nICC_unadjusted\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n0.2815489\n0.2815489\n0.2815489",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/validity/01_validity.html",
    "href": "chapters/validity/01_validity.html",
    "title": "14  La validità del test",
    "section": "",
    "text": "14.1 Minacce alla Validità\nLa validità di un test può essere compromessa quando non misura integralmente o accuratamente il costrutto di interesse, o quando valuta elementi estranei a tale costrutto. Anche test con alta affidabilità possono cadere in queste insidie, portando a interpretazioni errate dei risultati. Ci concentreremo sui tipi di validità e le relative evidenze, esaminando come integrare diverse fonti di prova per costruire un solido argomento di validità per il test.\nSotto-Rappresentazione del Costrutto: Questa si verifica quando un test non misura aspetti importanti del costrutto specificato. Ad esempio, un test di matematica di terza elementare che valuta solo la divisione non rappresenta adeguatamente l’intero spettro di competenze matematiche richieste a quel livello. Per risolvere questo problema, il contenuto del test dovrebbe essere ampliato per riflettere tutte le abilità insegnate nel curriculum di matematica di terza elementare.\nVarianza Estranea al Costrutto: Si presenta quando il test misura caratteristiche, contenuti o competenze non collegati al costrutto del test. Un esempio potrebbe essere un test di matematica che richiede elevate competenze di comprensione del testo, misurando così anche la capacità di lettura invece che solo la matematica. Per affrontare questo problema, il design del test dovrebbe minimizzare le istruzioni scritte e assicurarsi che il livello di lettura sia adeguato.\nAltri Fattori Che Influenzano la Validità: Oltre alle caratteristiche del test stesso, fattori esterni possono influenzare la validità delle interpretazioni dei risultati. Questi includono:\nInoltre, la validità delle interpretazioni norm-referenced è influenzata dall’adeguatezza del gruppo di riferimento.\nLe diverse minacce alla validità richiedono un’attenta valutazione e una gestione strategica per assicurare che le interpretazioni dei risultati del test siano affidabili e appropriate.",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>La validità del test</span>"
    ]
  },
  {
    "objectID": "chapters/validity/01_validity.html#minacce-alla-validità",
    "href": "chapters/validity/01_validity.html#minacce-alla-validità",
    "title": "14  La validità del test",
    "section": "",
    "text": "Caratteristiche dell’Esaminando:\n\nFattori personali, come l’ansia o la bassa motivazione, possono compromettere la validità.\n\nProcedura di Amministrazione e Valutazione:\n\nDeviazioni dalle procedure standard possono ridurre la validità. Anche le variazioni per accomodare esigenze speciali devono essere gestite con attenzione per mantenere la validità.\n\nIstruzione e Coaching:\n\nIstruzioni o coaching pre-test possono influenzare la validità, specialmente se gli esaminandi vengono addestrati specificamente a rispondere alle domande del test.",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>La validità del test</span>"
    ]
  },
  {
    "objectID": "chapters/validity/01_validity.html#tipologie-di-validità-rispetto-a-tipologie-di-prove-di-validità-nel-contesto-dei-test-educativi-e-psicologici",
    "href": "chapters/validity/01_validity.html#tipologie-di-validità-rispetto-a-tipologie-di-prove-di-validità-nel-contesto-dei-test-educativi-e-psicologici",
    "title": "14  La validità del test",
    "section": "14.2 “Tipologie di Validità” rispetto a “Tipologie di Prove di Validità” nel Contesto dei Test Educativi e Psicologici",
    "text": "14.2 “Tipologie di Validità” rispetto a “Tipologie di Prove di Validità” nel Contesto dei Test Educativi e Psicologici\n\n14.2.1 Contestualizzazione Storica e Evoluzione Terminologica\nI documenti guida nella creazione e nell’uso dei test educativi e psicologici, come gli Standard dell’AERA e altri (2014), hanno subito significative evoluzioni nel corso degli anni. Originariamente, la validità nei test era distinta in tre categorie principali: validità di contenuto, criteriale e di costrutto, come delineato da Messick (1989).\n\nValidità di Contenuto: Questa misura la pertinenza e la rappresentatività del contenuto del test rispetto al dominio del costrutto. Essa si basa su giudizi professionali riguardo l’adeguatezza dei contenuti del test.\nValidità Criteriale: Implica l’esame delle relazioni tra i punteggi del test e variabili esterne direttamente legate al costrutto, utilizzando metodi come l’analisi di correlazione o di regressione.\nValidità di Costrutto: Si concentra sull’integrazione di diverse evidenze relative al significato e all’interpretazione dei punteggi del test.\n\nQuesti tipi di validità, inizialmente ampiamente accettati, hanno successivamente ceduto il passo a un approccio unitario alla validità. Questa visione olistica considera la validità non come categorie distinte, ma come modi differenti di raccogliere prove a sostegno delle interpretazioni dei punteggi del test. Gli Standard del 1985 (APA et al., 1985) hanno introdotto il termine “tipi di prove di validità”, sostituendo la precedente nomenclatura.\n\n\n14.2.2 Standard del 2014 e Categorie di Prove di Validità\nGli Standard più recenti {cite:p}AERA-APA-NCME2014 hanno ulteriormente sviluppato questo concetto, definendo la validità come un grado in cui tutte le evidenze supportano l’interpretazione intesa dei punteggi del test per l’uso specificato. Le cinque categorie di prove di validità sono:\n\nProve Basate sul Contenuto del Test: Comprendono analisi del contenuto del test, tipologie di domande o compiti, e linee guida per somministrazione e correzione.\nProve Basate sui Processi di Risposta: Includono analisi dei processi cognitivi e comportamentali coinvolti nelle risposte agli item del test.\nProve Basate sulla Struttura Interna: Riguardano le relazioni tra elementi e componenti del test.\nProve Basate sulle Relazioni con Altre Variabili: Si concentrano sull’esame delle correlazioni tra le prestazioni nel test e variabili esterne.\nProve Basate sulle Conseguenze del Test: Considerano le implicazioni attese e non attese derivanti dall’uso del test.\n\nLa selezione e la valutazione delle prove pertinenti dipendono da fattori come il costrutto misurato, l’intento d’uso dei punteggi del test e la popolazione valutata.\n\n\n14.2.3 Rilevanza nella Pratica e nella Ricerca\nQuesta evoluzione terminologica non è solo un cambiamento superficiale, ma riflette un mutamento profondo nella comprensione della validità. È cruciale per i professionisti, gli sviluppatori e gli utenti di test aderire a queste linee guida, sia per ragioni legali che etiche. La letteratura più recente tende a utilizzare la nuova nomenclatura, ma è importante riconoscere e comprendere anche la terminologia storica, specialmente quando si esaminano manuali di test più datati e si valutano le proprietà psicometriche di un test.",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>La validità del test</span>"
    ]
  },
  {
    "objectID": "chapters/validity/01_validity.html#ottenere-evidenze-per-la-validità",
    "href": "chapters/validity/01_validity.html#ottenere-evidenze-per-la-validità",
    "title": "14  La validità del test",
    "section": "14.3 Ottenere evidenze per la validità",
    "text": "14.3 Ottenere evidenze per la validità\nLa validità è definita da {cite:p}AERA-APA-NCME2014 come “il grado in cui le evidenze e la teoria sostengono le interpretazioni dei punteggi del test per gli utilizzi proposti dei test”. Questa definizione implica che non è possibile ottenere prove a sostegno di tutte le possibili interpretazioni o utilizzi di un test. I test sono interpretati e utilizzati in molti modi, alcuni dei quali sono giustificabili e altri no. Il primo passo nella validazione del test, quindi, è specificare le interpretazioni e gli utilizzi intesi dei punteggi del test.\nConsideriamo dunque nel dettaglio i cinque tipi di prove di validità delineati negli Standard (2014): (1) prove basate sul contenuto del test, (2) prove basate sui processi di risposta, (3) prove basate sulla struttura interna, (4) prove basate sulle relazioni con altre variabili e (5) prove basate sulle conseguenze del test.\n\n14.3.1 Evidenza di Validità Basata sul Contenuto del Test\nSecondo quanto definito negli {cite:t}AERA-APA-NCME2014, l’evidenza di validità basata sul contenuto si riferisce alla misura in cui il contenuto di un test rappresenta adeguatamente il dominio che intende misurare. È importante riconoscere che spesso, nei contesti di test, non è possibile includere ogni elemento del dominio del costrutto. Questo può portare a una sotto-rappresentazione del costrutto o a una varianza non pertinente al costrutto, che minacciano la validità del test. Per mitigare queste minacce, i test devono essere attentamente pianificati per garantire che il loro contenuto rifletta equilibratamente e appropriatamente il dominio del costrutto.\nGli standard {cite:p}AERA-APA-NCME2014 enfatizzano l’importanza di esaminare la relazione tra il contenuto del test e il costrutto o dominio che il test è progettato per misurare. La validità basata sul contenuto si concentra su quanto bene gli elementi del test campionano i comportamenti o la materia di studio che il test è destinato a misurare. Questo tipo di evidenza di validità era in passato comunemente raggruppato sotto l’etichetta di “validità del contenuto”.\nNelle fasi iniziali dello sviluppo di un test, è essenziale definire chiaramente il costrutto o il dominio del contenuto da misurare. Successivamente, si sviluppa una tabella delle specifiche che funge da guida per lo sviluppo del test, delineando gli argomenti e gli obiettivi da coprire e la loro importanza relativa.\nDurante la fase di revisione del test, esperti del settore valutano sistematicamente il test per giudicare la corrispondenza tra il contenuto del test e il suo costrutto o dominio. Questi esperti affrontano due questioni principali: la rilevanza degli elementi e la copertura del contenuto. La rilevanza degli elementi si riferisce alla valutazione di ciascun elemento del test per determinare se rifletta contenuti essenziali nel dominio specificato. La copertura del contenuto valuta se gli elementi del test nel loro insieme coprono adeguatamente il dominio specificato.\nL’evidenza di validità basata sul contenuto è tipicamente qualitativa, ma può essere riportata in modo più quantitativo, come il numero e le qualifiche degli esperti coinvolti, il numero di revisioni effettuate e il loro grado di accordo su questioni relative al contenuto.\nQuesto tipo di evidenza di validità è particolarmente importante per i test di rendimento accademico e per i test utilizzati nella selezione e classificazione dei dipendenti, in quanto sono progettati per fornire un campione rappresentativo della conoscenza, del comportamento o delle abilità misurate.\n\n14.3.1.1 Validità di Faccia\nVa inoltre distinta la validità di faccia dalla validità basata sul contenuto. La validità di faccia si riferisce all’apparenza di un test e alla sua capacità di sembrare valido per persone non esperte, ma non riguarda ciò che il test misura effettivamente. Un test può sembrare valido, ma non essere tale alla luce di un’analisi tecnica approfondita del suo contenuto. Tuttavia, un buon grado di validità di faccia può aumentare la cooperazione degli esaminandi e la percezione pubblica dei risultati come significativi. In alcuni contesti, come quelli forensi, la validità di faccia può essere indesiderabile, ad esempio per evitare che gli esaminandi simulino risposte patologiche.\n\n\n\n14.3.2 Evidenza basata sui processi di risposta\nL’evidenza di validità basata sui processi di risposta riguarda l’analisi di come le risposte fornite dagli esaminandi corrispondano al costrutto che il test intende valutare. Questo tipo di evidenza di validità esamina se gli esaminandi utilizzino effettivamente i processi cognitivi previsti per rispondere alle domande del test. Ad esempio, in un test che misura la capacità di ragionamento matematico, è importante verificare che gli esaminandi stiano effettivamente applicando analisi e ragionamento piuttosto che ricorrendo a algoritmi matematici meccanici.\nQuesto tipo di evidenza è raccolta attraverso varie metodologie, come intervistare gli esaminandi sui loro processi di risposta e strategie, registrare indicatori comportamentali come tempi di risposta e movimenti oculari o analizzare i tipi di errori commessi. Se gli esaminandi non impiegano i processi cognitivi attesi, possono emergere dubbi sulla validità delle misurazioni ottenute e sulla capacità del test di valutare in modo attendibile il costrutto di interesse.\nInoltre, le indagini sui processi di risposta non si limitano solo agli individui che prendono il test, ma possono anche includere i professionisti dell’assessment che amministrano o valutano i test. È fondamentale che le loro azioni o processi siano in linea con il costrutto misurato. Molti test forniscono criteri specifici o rubriche intesi a guidare il processo di valutazione. Per esempio, il Wechsler Individual Achievement Test—Terza Edizione (WIAT-III) include un compito che richiede all’esaminando di scrivere un breve saggio. Per facilitare la valutazione, gli autori includono una rubrica di valutazione analitica che copre diverse categorie valutative, come l’uso di più paragrafi, un’introduzione (compresa una tesi e un riassunto), transizioni che mostrano le relazioni tra le idee, ragioni che supportano la tesi, elaborazioni che supportano ogni ragione e una conclusione che include una tesi e un riassunto delle ragioni presentate. Queste rubriche aiutano a garantire la coerenza della valutazione da parte di coloro che valutano i saggi e aiutano a evitare di attribuire credito a fattori irrilevanti che non indicano la capacità dell’esaminando di scrivere buoni saggi.\nIn sintesi, l’evidenza di validità basata sui processi di risposta è un aspetto cruciale per garantire che il test misuri effettivamente il costrutto previsto e che le risposte degli esaminandi riflettano i processi cognitivi appropriati. Questo approccio, sebbene non abbia ricevuto tanta attenzione quanto altre forme di evidenza di validità, ha un notevole potenziale e viene classificato tradizionalmente sotto la validità di costrutto.\n\n\n14.3.3 Evidenza basata sulla struttura interna\nL’evidenza basata sulla struttura interna di un test si focalizza sulla coerenza degli elementi del test con le dimensioni teoriche previste. Questo tipo di valutazione è cruciale per determinare se i punteggi di un test rappresentino accuratamente le dimensioni o i costrutti che si prevede di misurare.\nAlcuni test sono progettati per valutare una singola dimensione o un aspetto specifico, come l’estroversione o l’apertura mentale, mentre altri mirano a misurare costrutti più ampi e multidimensionali, come la personalità generale. L’analisi della struttura interna di un test permette di verificare se le relazioni tra gli elementi del test (o, nel caso di batterie di test, tra i test componenti) sono coerenti con il costrutto che il test è progettato per misurare.\nAd esempio, in un test di personalità generale, ci si aspetta che gli elementi relativi a diverse dimensioni della personalità siano correlati tra loro, contribuendo alla misurazione complessiva della personalità. Se gli elementi di un test non mostrano questa coerenza con la struttura multidimensionale ipotizzata, ciò può sollevare dubbi sulla validità delle interpretazioni dei punteggi ottenuti.\nUno strumento statistico spesso utilizzato per esaminare la struttura interna di un test è l’analisi fattoriale. Questa procedura sofisticata aiuta a determinare il numero di fattori o dimensioni concettualmente distinti che sottendono un test o una batteria di test. L’analisi fattoriale può quindi confermare se la struttura effettiva del test è coerente con la struttura ipotizzata del costrutto che misura.\nIn sintesi, l’evidenza basata sulla struttura interna è essenziale per valutare la validità di un test e assicurare che i suoi punteggi riflettano fedelmente le dimensioni o i costrutti teorizzati. Questa valutazione fornisce una base solida per l’interpretazione dei risultati del test, permettendo di stabilire in che misura il test possa essere utilizzato come misura affidabile delle dimensioni o dei costrutti desiderati.\n\n\n14.3.4 Evidenza di Validità Basata sulle Conseguenze del Testing\nL’evidenza di validità basata sulle conseguenze del testing è un aspetto cruciale nella valutazione della validità di un test psicometrico, secondo la prospettiva degli Standards {cite:p}AERA-APA-NCME2014. Questo tipo di evidenza si concentra sugli effetti che l’uso del test ha sulle persone testate e sugli esiti diretti delle misurazioni. È importante considerare non solo le conseguenze intenzionali, ovvero gli effetti voluti o previsti che il test dovrebbe generare, ma anche le conseguenze non intenzionali, come effetti imprevisti o negativi che potrebbero manifestarsi.\n\n14.3.4.1 Conseguenze Intenzionali e Non Intenzionali\nLe conseguenze intenzionali del testing sono legate agli scopi e agli utilizzi del test. Ad esempio, se un test viene utilizzato per la selezione di candidati per un ruolo specifico, l’obiettivo è identificare le persone più idonee. Invece, le conseguenze non intenzionali si riferiscono a effetti imprevisti, come discriminazioni ingiuste o influenze distorte sui partecipanti, che possono emergere dall’uso del test.\nL’analisi approfondita di queste conseguenze aiuta a garantire che il test non causi effetti dannosi e sia utilizzato in modo etico e appropriato. Questa valutazione è fondamentale per assicurare che il test fornisca misurazioni accurate e significative per gli scopi previsti.\n\n\n14.3.4.2 Consequentialità della Validità\nLa validità consequenziale si riferisce ai benefici specifici attesi dall’uso dei test. Ad esempio, l’uso di un test per l’assunzione di personale dovrebbe portare a migliori decisioni di assunzione, come minori costi di formazione e turnover. Questo tipo di validità pone la domanda: “Questi benefici vengono effettivamente raggiunti?” È particolarmente applicabile a test progettati per la selezione e la promozione.\n\n\n14.3.4.3 Valutazione delle Conseguenze Sociali e Politiche\nAlcuni autori hanno proposto un concetto di validità più ampio che incorpora questioni sociali e valori. Tuttavia, questa posizione è stata oggetto di critiche, poiché l’inclusione di questioni sociali e di valore potrebbe complicare il concetto di validità. Gli Standard AERA et al. (2014) sembrano evitare questa concezione più ampia della validità, distinguendo tra evidenze consequenziali direttamente legate al concetto di validità ed evidenze relative alla politica sociale.\n\n\n14.3.4.4 Considerazione delle Alternative all’Uso dei Test\nÈ anche importante considerare le conseguenze di non utilizzare i test. Anche se l’uso dei test può produrre alcuni effetti avversi, questi devono essere confrontati con gli effetti positivi e negativi delle alternative all’uso dei test psicologici. L’adozione di approcci più soggettivi al processo decisionale può aumentare la probabilità di pregiudizi culturali, etnici e di genere.\nIn conclusione, l’evidenza di validità basata sulle conseguenze del testing è un aspetto essenziale da considerare nella valutazione complessiva della validità di un test psicometrico. È necessario esaminare gli effetti sia voluti che non voluti dei test e garantire che il loro utilizzo sia eticamente appropriato e non causi danni, contribuendo così all’utilizzo responsabile e informato dei test nella pratica professionale.\n\n\n\n14.3.5 Evidenza di Validità Basata sulle Relazioni con Altre Variabili\nL’evidenza di validità basata sulle relazioni con altre variabili riguarda la correlazione tra i punteggi del test e altre variabili rilevanti. Questa forma di validità può includere le correlazioni dei punteggi del test con variabili teoricamente correlate, la capacità del test di predire risultati specifici, le differenze tra gruppi differenziati dal costrutto misurato e studi volti a identificare possibili contaminazioni dovute agli effetti del metodo.\n\n14.3.5.1 Relazioni Test-Criterio\nMolti test sono progettati per predire le prestazioni su una variabile definita come criterio. Il criterio può essere il rendimento accademico, la performance lavorativa o altri risultati importanti per l’utente del test. Questa tipologia di validità include studi predittivi, in cui si amministra il test, si attende un intervallo di tempo e poi si misura il criterio, e studi concorrenti, in cui il test e il criterio vengono misurati contemporaneamente.\nAd esempio, per validare il SAT come predittore del successo universitario, si potrebbe correlare il punteggio SAT degli studenti delle superiori con il loro GPA universitario dopo il primo anno. Questo tipo di studio può utilizzare il coefficiente di correlazione, noto come coefficiente di validità. La scelta tra studi predittivi e concorrenti dipende dall’obiettivo della valutazione e dal contesto di utilizzo del test.\n\n\n14.3.5.2 Confronto di Gruppi\nLa teoria può servire come base per raccogliere evidenze di validità esaminando come gruppi diversi (identificati da un criterio esterno) dovrebbero differire nel costrutto misurato dal test. Ad esempio, nella validazione di un nuovo test di intelligenza, si potrebbero confrontare i punteggi di gruppi con disabilità intellettuali e quelli con abilità intellettuali tipiche.\n\n\n14.3.5.3 Sensibilità e Specificità\nQuesti concetti sono importanti quando si utilizza un punteggio di test per classificare individui o caratteristiche in gruppi. La sensibilità di una misura si riferisce alla sua capacità di rilevare la presenza di una condizione, mentre la specificità si riferisce alla sua capacità di determinare l’assenza di una condizione.\n\n\n14.3.5.4 Validità convergente e Divergente\nL’evidenza di validità convergente si ottiene correlando un test con altri test che misurano costrutti simili o identici. Invece, l’evidenza di validità discriminante si ottiene correlando un test con misure di costrutti dissimili.\n\n\n14.3.5.5 Generalizzazione della Validità\nSi riferisce al grado in cui le relazioni test-criterio possono essere generalizzate a nuove situazioni senza ulteriori studi. La meta-analisi ha mostrato che la variabilità osservata nei coefficienti di validità è spesso dovuta a artefatti statistici, suggerendo che i coefficienti di validità possono essere generalizzati più di quanto si pensasse in passato.\nIn conclusione, l’evidenza di validità basata sulle relazioni con altre variabili è cruciale per stabilire la validità di un test. Essa comprende l’esame delle relazioni test-criterio, lo studio di gruppi contrastanti, la considerazione di modelli di teoria delle decisioni, e l’analisi di sensibilità e specificità, oltre alla ricerca di evidenze convergenti e divergenti. Queste approfondite indagini aiutano a garantire che i punteggi di un test riflettano accuratamente il costrutto che intendono misurare e siano utili nel contesto specifico in cui vengono applicati.",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>La validità del test</span>"
    ]
  },
  {
    "objectID": "chapters/validity/01_validity.html#integrazione-delle-prove-di-validità",
    "href": "chapters/validity/01_validity.html#integrazione-delle-prove-di-validità",
    "title": "14  La validità del test",
    "section": "14.4 Integrazione delle Prove di Validità",
    "text": "14.4 Integrazione delle Prove di Validità\nGli Standards {cite:p}AERA-APA-NCME2014 definiscono la validazione come un processo di costruzione e valutazione di argomentazioni a favore e contro l’interpretazione intesa dei punteggi dei test e la loro rilevanza per l’uso proposto. Lo sviluppo di un argomento di validità comporta tipicamente l’integrazione di numerose linee di evidenza in un commento coerente. Diversi tipi di prove di validità sono più applicabili a diversi tipi di test. Ecco una breve rassegna delle applicazioni prominenti di diversi tipi di prove di validità:\n\nProve Basate sul Contenuto del Test: Sono spesso riportate con test di rendimento accademico e test utilizzati nella selezione dei dipendenti.\nProve Basate sulle Relazioni con Altre Variabili: Possono includere (1) relazioni test-criterio, applicabili quando i test sono usati per predire le prestazioni su un criterio esterno; (2) evidenze convergenti e discriminanti, utili con una varietà di test, inclusi test di intelligenza, di rendimento, test di personalità, ecc.; (3) evidenze di generalizzazione della validità, utili quando gli stessi o simili test sono usati ripetutamente in applicazioni simili.\nProve Basate sulla Struttura Interna: Utili con una varietà di test, ma tradizionalmente applicate con test che misurano costrutti teorici come la personalità o l’intelligenza.\nProve Basate sui Processi di Risposta: Utili con praticamente qualsiasi test che richiede agli esaminandi di impegnarsi in attività cognitive o comportamentali.\nProve Basate sulle Conseguenze del Testing: Applicabili soprattutto ai test progettati per la selezione e la promozione, ma utili con un’ampia gamma di test.\n\nLa maggior parte dei tipi di prove di validità ha applicazioni per una vasta gamma di test, il che è appropriato. L’integrazione di molteplici linee di ricerca o tipi di evidenza fornisce un argomento di validità più convincente. È importante ricordare che ogni interpretazione o uso inteso di un test deve essere validato. Se un test viene utilizzato per applicazioni diverse, ogni uso o applicazione deve essere validato, richiedendo diversi tipi di prove di validità.\nLa validità di un’interpretazione dei punteggi di un test dipende da tutte le prove disponibili relative alla qualità tecnica di un sistema di test. Componenti importanti di questa evidenza includono la costruzione attenta del test, l’affidabilità adeguata dei punteggi, l’amministrazione e la valutazione appropriate del test, la scalatura accurata dei punteggi e l’attenzione alla giustizia per gli esaminandi, come appropriato per l’interpretazione del test in questione.\nIn sintesi, la validità di un test è un strumento ben sviluppato e tecnicamente solido. Nel prossimo capitolo, forniremo ulteriori indicazioni pratiche per garantire la validità delle interpretazioni dei punteggi dei test. Questo processo inizia quando si inizia a pensare allo sviluppo di un test.\nInfine, lo sviluppo di un argomento di validità è un processo continuo che tiene conto delle ricerche esistenti e incorpora nuove scoperte scientifiche. Mentre gli sviluppatori di test sono tenuti a fornire prove iniziali della validità delle interpretazioni dei punteggi che propongono, la ricerca di ricercatori indipendenti successiva al rilascio del test è essenziale. Riviste professionali eccellenti pubblicano regolarmente articoli di ricerca empirica che coprono le proprietà psicometriche di diversi test. Inoltre, coloro che utilizzano i test sono tenuti a valutare le prove di validità e a formulare i propri giudizi sulla loro appropriateness nel proprio contesto e ambiente. Ciò pone i professionisti clinici che utilizzano i test psicologici nel ruolo finale e più responsabile nel processo di validazione.",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>La validità del test</span>"
    ]
  },
  {
    "objectID": "chapters/validity/01_validity.html#conclusione",
    "href": "chapters/validity/01_validity.html#conclusione",
    "title": "14  La validità del test",
    "section": "14.5 Conclusione",
    "text": "14.5 Conclusione\nNella psicometria, la validità emerge come un concetto dinamico e sfaccettato, che richiede un’analisi approfondita e l’integrazione di varie forme di evidenza. Nel corso di questo capitolo, abbiamo esaminato le diverse dimensioni della validità, sottolineando l’importanza di un’analisi olistica nell’interpretazione dei punteggi dei test. È essenziale andare oltre la mera coerenza del contenuto del test con il costrutto target, includendo anche un esame dettagliato della sua struttura interna, dei processi cognitivi e comportamentali evocati nei rispondenti, e delle conseguenze, sia attese che inattese, derivanti dal suo impiego.\nConformemente agli Standards {cite:p}AERA-APA-NCME2014, la validità trascende la semplice analisi statistica per abbracciare il significato e la pertinenza delle interpretazioni dei punteggi dei test all’interno dei contesti specifici di utilizzo. Ciò implica un processo di indagine e aggiornamento continuo, che assimila nuove ricerche e avanzamenti nel settore. L’uso dei test psicometrici, pertanto, deve essere condotto con una consapevolezza critica delle loro limitazioni e potenzialità, assicurando che le decisioni basate sui loro risultati siano non solo accurate, ma anche etiche e ben fondate.\nIn conclusione, la validazione di un test psicometrico si configura come un processo dinamico, che sollecita una costante attenzione critica e un approccio responsabile da parte degli psicologi. La validità, dunque, non deve essere percepita come un attributo statico e inalterabile del test, ma piuttosto come un’analisi continua e progressiva della sua efficacia nel fornire interpretazioni dei risultati che siano pertinenti e rilevanti all’interno di diversi contesti di applicazione.",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>La validità del test</span>"
    ]
  },
  {
    "objectID": "chapters/validity/02_other_variables.html",
    "href": "chapters/validity/02_other_variables.html",
    "title": "15  Relazioni test-criterio",
    "section": "",
    "text": "15.1 Analisi della Relazione Test-Criterio mediante la Regressione Logistica\nIn questo capitolo, esploriamo approfonditamente la relazione tra i punteggi dei test e vari criteri esterni, focalizzandoci su come i test possono predire o differenziare fenomeni specifici. Questo tipo di analisi è cruciale per valutare la validità dei test in contesti pratici e teorici.",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Relazioni test-criterio</span>"
    ]
  },
  {
    "objectID": "chapters/validity/02_other_variables.html#analisi-della-relazione-test-criterio-mediante-la-regressione-logistica",
    "href": "chapters/validity/02_other_variables.html#analisi-della-relazione-test-criterio-mediante-la-regressione-logistica",
    "title": "15  Relazioni test-criterio",
    "section": "",
    "text": "15.1.1 Categorie di Evidenze basate su Relazioni con Altre Variabili\nIn psicometria, vengono utilizzate diverse categorie di prove per valutare le relazioni dei punteggi dei test con altre variabili:\n\nRelazioni Test-Criterio:\n\nQueste si concentrano sull’uso dei punteggi dei test per prevedere il rendimento o lo stato attuale in vari ambiti, come l’adattamento accademico o lavorativo.\n\nDifferenze tra Gruppi:\n\nAnalizziamo se i test evidenziano differenze significative nei punteggi tra gruppi definiti da criteri specifici, come la presenza o assenza di una diagnosi clinica.\n\nProve di Convergenza e Discriminazione:\n\nEsploriamo se i punteggi di un test sono correlati a quelli di altri test che misurano costrutti simili (validità convergente) e se sono meno correlati a test che misurano costrutti diversi (validità discriminante).\n\n\nQuando si affronta la questione delle relazioni test-criterio, un aspetto cruciale è la selezione di un criterio appropriato e la valutazione quantitativa della relazione tra il test e il criterio. In situazioni dove il criterio è di natura categorica, come il superamento o il fallimento di un test, la regressione logistica diventa uno strumento essenziale.\nLa regressione logistica è un metodo statistico usato per analizzare la relazione tra una variabile dipendente binaria e una o più variabili indipendenti. Questa tecnica stima la probabilità che un’osservazione appartenga a una delle categorie della variabile dipendente, basandosi sui valori delle variabili indipendenti.\nSupponiamo che una variabile risposta \\(Y_i\\) (per l’osservazione \\(i\\)-esima, con \\(i = 1, \\dots, n\\)) assuma due modalità, definite come successo e insuccesso. Ogni osservazione può essere associata a un vettore di variabili esplicative (\\(x_1, \\dots, x_p\\)), ma per semplicità, considereremo il caso di una singola variabile indipendente.\nNella regressione logistica, il logaritmo delle probabilità delle due categorie è modellato come una funzione lineare delle variabili indipendenti. Questo ci permette di esaminare come i cambiamenti nelle variabili indipendenti influenzino la probabilità di successo o insuccesso.\nNel contesto dei test psicometrici, la regressione logistica può essere usata per determinare il grado in cui i punteggi di un test sono predittivi di un risultato categorico. Ad esempio, possiamo valutare la probabilità che gli studenti con determinati punteggi ad un test di ammissione universitario riescano o meno nel loro primo anno accademico.\nQuesto metodo fornisce un quadro più chiaro della validità di un test nel predire risultati specifici, migliorando così l’affidabilità e l’applicabilità dei test in vari contesti. Attraverso l’uso della regressione logistica, possiamo quindi formulare interpretazioni più precise e informate dei risultati dei test, contribuendo a una migliore comprensione della validità di uno strumento psicometrico.\nNello specifico, possiamo dire che l’obiettivo della regressione logistica è studiare la relazione tra la probabilità di risposta\n\\[\nPr(Y=1 | X=x_i) \\equiv Pr(Y_i) \\equiv \\pi_i\n\\]\ne la variabile esplicativa \\(x\\).\nLa componente sistematica del modello è espressa come funzione lineare del predittore\n\\[\n\\eta_i = logit(\\pi_i) = \\alpha + \\beta x_i.\n\\]\nLa componente aleatoria del modello suppone l’esistenza di \\(k\\) osservazioni indipendenti \\(y_1, y_2, \\dots, y_k\\), ciascuna delle quali viene trattata come la realizzazione di una variabile casuale \\(Y_i\\). Nel caso di dati ragruppati, si assume che \\(Y_i\\) abbia una distribuzione binomiale\n\\[\nY_i \\sim Bin(n_i, \\pi_i),\n\\]\ncon parametri \\(n_i\\) e \\(\\pi_i\\). Per dati individuali (uno per ciascun valore \\(x_i\\)), \\(n_i=1, \\forall i\\).\nLa funzione link del modello trasforma il predittore lineare nel valore atteso della variabile risposta condizionato alla variabile \\(X\\):\n\\[\n\\begin{equation}\n\\pi_i = g(\\eta_i) = \\frac{e^{\\alpha + \\beta x_i}}{1+e^{\\alpha + \\beta x_i}}.\n\\end{equation}\n\\] (eq-reg-logistic-prob)\nIn sintesi, la regressione logistica è una tecnica statistica che stima la probabilità che l’evento \\(Y = 1\\) si verifichi per ciascun valore della variabile indipendente \\(X\\).\n\n\n15.1.2 Un Esempio Pratico\nPer fare un esempio pratico, consideriamo i dati dello studio Pitfalls When Using Area Under the Curve to Evaluate Item Content for Early Screening Tests for Autism di Lucas, Brewer e Young (2022).\nIl campione raccolto da Nah et al. (2018) include 270 bambini di età compresa tra 12 e 36 mesi (M = 25.4, SD = 7.0). Sulla base di una diagnosi clinica eseguita secondo il DSM-5, 106 bambini sono stati diagnosticati con ASD (Autism Spectrum Disorder, Disturbo dello Spettro Autistico), 86 erano in sviluppo non tipico (non-TD) e 78 erano considerati in sviluppo tipico (TD). Qui considereremo solo i gruppi ASD e non-TD.\nIl test di interesse è l’Autism Detection in Early Childhood (ADEC). Si tratta di una checklist comportamentale composta da 16 item, appositamente sviluppata per rilevare comportamenti pre-verbali che possono essere predittivi dell’autismo nei bambini di età inferiore ai tre anni (Young, 2007).\nLeggiamo i dati.\n\ntmp_path &lt;- tempfile(fileext = \"xlsx\") # temporary file\ndownload.file(\"https://osf.io/download/tsm7x/\", destfile = tmp_path)\ndat1 &lt;- readxl::read_xlsx(tmp_path, na = \"NA\")\ndat1$asd &lt;- recode(\n    dat1$`Diagnosis(1=Non-typically developing; 2=ASD; 3=Neurotypical)`,\n    `1` = \"Non-TD\",\n    `2` = \"ASD\",\n    `3` = \"TD\"\n)\n# Filter out Neurotypical\ndat1_sub &lt;- filter(dat1, asd != \"TD\")\nglimpse(dat1_sub)\n\nRows: 192\nColumns: 21\n$ ID                                                             &lt;dbl&gt; 2, 3, 4~\n$ `Sex (1=Male; 2=Female)`                                       &lt;dbl&gt; 1, 1, 2~\n$ `Age (in Months)`                                              &lt;dbl&gt; 12, 12,~\n$ ADEC_I01                                                       &lt;dbl&gt; 0, 2, 2~\n$ ADEC_I02                                                       &lt;dbl&gt; 0, 1, 2~\n$ ADEC_I03                                                       &lt;dbl&gt; 0, 0, 0~\n$ ADEC_I04                                                       &lt;dbl&gt; 0, 1, 2~\n$ ADEC_I05                                                       &lt;dbl&gt; 0, 1, 0~\n$ ADEC_I06                                                       &lt;dbl&gt; 0, 1, 2~\n$ ADEC_I07                                                       &lt;dbl&gt; 2, 2, 2~\n$ ADEC_I08                                                       &lt;dbl&gt; 0, 1, 1~\n$ ADEC_I09                                                       &lt;dbl&gt; 0, 0, 1~\n$ ADEC_I10                                                       &lt;dbl&gt; 0, 1, 2~\n$ ADEC_I11                                                       &lt;dbl&gt; 0, 1, 1~\n$ ADEC_I12                                                       &lt;dbl&gt; 1, 0, 0~\n$ ADEC_I13                                                       &lt;dbl&gt; 1, 1, 1~\n$ ADEC_I14                                                       &lt;dbl&gt; 1, 0, 1~\n$ ADEC_I15                                                       &lt;dbl&gt; 1, 2, 1~\n$ ADEC_I16                                                       &lt;dbl&gt; 0, 1, 1~\n$ `Diagnosis(1=Non-typically developing; 2=ASD; 3=Neurotypical)` &lt;dbl&gt; 1, 1, 1~\n$ asd                                                            &lt;chr&gt; \"Non-TD~\n\n\nIniziamo trovando il punteggio totale ADEC. Si noti che la somma sarà NA se un item è NA.\n\ndat1_sub$ADEC &lt;- rowSums(select(dat1_sub, ADEC_I01:ADEC_I16)) # NA if any item is NA\n# Or treat missing as 0\ndat1_sub$ADEC_rm_na &lt;- rowSums(select(dat1_sub, ADEC_I01:ADEC_I16), na.rm = TRUE)\n\nUtilizzeremo il modello di regressione logistica per esaminare la relazione tra il punteggio totale del test ADEC e la probabilità che il bambino appartenga al gruppo diagnostico ASD.\nEsaminiamo la distribuzione del punteggio totale di ADEC in base alla diagnosi:\n\nggplot(dat1_sub, aes(x = ADEC)) +\n    geom_histogram() +\n    facet_wrap(~asd)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nWarning message:\n\"Removed 12 rows containing non-finite outside the scale range (`stat_bin()`).\"\n\n\n\n\n\n\n\n\n\nRicodifichiamo asd in modo tale che assuma valore 1 per i bambini con ASD e 0 altrimenti.\n\ndat1_sub$y &lt;- ifelse(dat1_sub$asd == \"ASD\", 1, 0)\ndat1_sub$y\n\n\n000000010000001100000000000000001100000000010011011011101100100111110101111000111001111000110111110101101111001111000010111010111111011101011111111111011001111111111101110100011111111111001000\n\n\nEseguiamo l’analisi di regressione logistica usando la funzione ´glm()´.\n\nfm &lt;- glm(y ~ ADEC, family = binomial(link=\"logit\"), data = dat1_sub)\n\nEsaminiamo i risultati.\n\nsummary(fm)\n\n\nCall:\nglm(formula = y ~ ADEC, family = binomial(link = \"logit\"), data = dat1_sub)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -3.76480    0.58071  -6.483 8.99e-11 ***\nADEC         0.35447    0.05201   6.816 9.37e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 247.73  on 179  degrees of freedom\nResidual deviance: 131.12  on 178  degrees of freedom\n  (12 osservazioni eliminate a causa di valori mancanti)\nAIC: 135.12\n\nNumber of Fisher Scoring iterations: 6\n\n\nCreiamo un grafico che mostri la relazione tra la probabilità che Y = 1 (cioè la diagnosi di ASD per il bambino), calcolata utilizzando i coefficienti del modello di regressione logistica, e il punteggio ottenuto nella scala ADEC.\n\n# Filter out rows with missing values in ADEC column\ndat1_sub &lt;- na.omit(dat1_sub)\n# Predict the probabilities of y == 1 for the filtered data\npredictions &lt;- predict(fm, type = \"response\")\n# Create a data frame with ADEC and the predicted probabilities\nplot_data &lt;- data.frame(ADEC = dat1_sub$ADEC, Prob_Y_1 = predictions)\n\n# Plot the probability of y == 1 as a function of ADEC\nlibrary(ggplot2)\nggplot(plot_data, aes(x = ADEC, y = Prob_Y_1)) +\n    geom_line() +\n    geom_point() +\n    xlab(\"ADEC\") +\n    ylab(\"Probability of y == 1\") +\n    ggtitle(\"Probability of y == 1 vs. ADEC\")\n\n\n\n\n\n\n\n\nIl grafico presenta una curva sigmoidale che illustra come, per punteggi bassi nella scala ADEC, la probabilità di una diagnosi di autismo sia bassa, mentre per punteggi alti nella scala ADEC, la probabilità di una diagnosi di autismo sia alta.\n\n\n15.1.3 Accuratezza della classificazione\nUna volta compreso come il modello di regressione logistica associa a ciascun valore di \\(x\\) la probabilità dell’evento \\(y = 1\\), esaminiamo ora come sia possibile utilizzare questo modello per valutare l’accuratezza di una classificazione binaria. Nel nostro caso, la classificazione riguarda ciascuna osservazione nelle categorie ASD e Non-TD. In questo contesto, il modello di regressione logistica stima la probabilità di appartenenza a una delle due categorie basandosi sui valori di ADEC (eq. {eq}eq-reg-logistic-prob).\nPer effettuare la classificazione, dobbiamo stabilire un punto di taglio (cut-off) che separi le due categorie. Questo punto di taglio definisce il valore della variabile dipendente al di sopra del quale l’osservazione sarà assegnata alla categoria positiva (y = 1, ovvero ASD) e al di sotto del quale sarà assegnata alla categoria negativa (y = 0). La scelta del punto di taglio è critica poiché influisce sull’accuratezza delle previsioni del modello.\nI coefficienti di regressione stimati dalla regressione logistica sono i parametri del modello che descrivono la relazione tra la variabile indipendente ADEC e la probabilità di appartenenza alla categoria positiva (y = 1). Questi coefficienti ci consentono di calcolare la probabilità stimata di y = 1 per ciascuna osservazione, utilizzando l’eq. {eq}eq-reg-logistic-prob. Una volta calcolata la probabilità stimata di y = 1 per ogni osservazione, possiamo utilizzare un determinato punto di taglio (ad esempio, 0.5) per classificare le osservazioni nelle due categorie y = 0 e y = 1. Se la probabilità stimata è maggiore o uguale al punto di taglio, l’osservazione verrà classificata come y = 1 (positiva); altrimenti, verrà classificata come y = 0 (negativa).\nTuttavia, la scelta del punto di taglio non è banale e ha un impatto diretto sulla sensibilità (la proporzione dei casi positivi effettivi che il test identifica correttamente come positivi) e specificità (la proporzione dei casi negativi effettivi che il test identifica correttamente come negativi) del modello. Un punto di taglio più alto potrebbe aumentare la specificità, ma ridurre la sensibilità, mentre un punto di taglio più basso avrebbe l’effetto opposto.\nPer ottenere una valutazione più completa delle prestazioni del modello, consideriamo tutti i possibili punti di taglio e visualizziamo la relazione tra sensibilità e specificità tramite la curva ROC (Receiver Operating Characteristic). Questa curva è un grafico che mostra come variano sensibilità e specificità al variare del punto di taglio. Un elemento chiave della curva ROC è l’Area Under the Curve (AUC), che rappresenta una misura aggregata dell’accuratezza complessiva del modello. L’AUC calcola l’area sottesa alla curva ROC e riflette la capacità del modello di discriminare tra le due categorie di risultato. L’AUC, dunque, fornisce una misura sintetica dell’accuratezza del modello su tutta la gamma di possibili punti di taglio. Un valore di AUC pari a 1 indica una perfetta capacità di discriminazione, mentre un valore pari a 0.5 indica una capacità di discriminazione casuale.\nPer mostrare come trovare l’AUC, iniziamo calcolando l’accuratezza della classificazione mediante un valore di cut-off pari a 0.5, ad esempio.\n\n# Predict the probabilities of y == 1 for the filtered data\nprob_pred &lt;- predict(fm, type = \"response\")\n\n# Use cut-off 0.5 for classification\nclassification &lt;- ifelse(prob_pred &gt;= 0.5, 1, 0)\n\n# Classification accuracy\nmean(classification == dat1_sub$y)\n\n0.855555555555556\n\n\nCon il cut-off di 0.5, l’accuratezza è 0.855, ovvero l’86% dei casi è stato correttamente classificato dal test. Vogliamo però trovare l’accuratezza della classificazione per tutti i possibili valori di cut-off. Costruiamo dunque la curva ROC in R.\n\ncompute_sens &lt;- function(\n    cut, x = dat1_sub$ADEC,\n    crit = dat1_sub$asd == \"ASD\") {\n    tp &lt;- sum(x &gt;= cut & crit, na.rm = TRUE)\n    fn &lt;- sum(x &lt; cut & crit, na.rm = TRUE)\n    tp / (tp + fn)\n}\ncompute_spec &lt;- function(\n    cut, x = dat1_sub$ADEC,\n    crit = dat1_sub$asd == \"ASD\") {\n    tn &lt;- sum(x &lt; cut & !crit, na.rm = TRUE)\n    fp &lt;- sum(x &gt;= cut & !crit, na.rm = TRUE)\n    tn / (tn + fp)\n}\nsensitivity &lt;- lapply(32:0, compute_sens) |&gt;\n    unlist()\nspecificity &lt;- lapply(32:0, compute_spec) |&gt;\n    unlist()\ndata.frame(sensitivity, fpr = 1 - specificity) |&gt;\n    ggplot(aes(x = fpr, y = sensitivity)) +\n    geom_point() +\n    geom_line()\n\n\n\n\n\n\n\n\nUna volta costruita la curva ROC possiamo calcolare l’area sottesa alla curva ROC.\n\n# AUC\ndfpr &lt;- c(diff(1 - specificity), 0)\ndsens &lt;- c(diff(sensitivity), 0)\nsum(sensitivity * dfpr) + sum(dsens * dfpr) / 2\n\n0.920626013218606\n\n\nSe consideriamo tutti i possibili valori di cut-off, l’accuratezza risulta uguale a 0.92. Questo valore indica un’ottima capacità del modello di regressione logistica nel discriminare con precisione i bambini con diagnosi di autismo da quelli senza.\nQuesti risultati suggeriscono che la scala ADEC è uno strumento utile e valido per identificare precocemente i bambini a rischio di sviluppare un disturbo dello spettro autistico. La sua elevata accuratezza predittiva consente di fornire una valutazione tempestiva e accurata per l’intervento precoce e il supporto necessario ai bambini e alle loro famiglie. In conclusione, l’analisi di regressione logistica e il calcolo dell’AUC hanno fornito evidenze solide sulla validità predittiva della scala ADEC nella diagnosi precoce dell’autismo.\n\n15.1.3.1 Pacchetto ROCit\nSi noti che si possono ottenere gli stessi risultati trovati sopra usando le funzioni del pacchetto ROCit.\n\nroc_adec &lt;- rocit(dat1_sub$ADEC, class = dat1_sub$asd == \"ASD\")\n\n\nplot(roc_adec)\n\n\n\n\n\n\n\n\n\nsummary(roc_adec)\n\n                          \n Method used: empirical   \n Number of positive(s): 99\n Number of negative(s): 81\n Area under curve: 0.9206 \n\n\n\nciAUC(roc_adec)\n\n                                                          \n   estimated AUC : 0.920626013218606                      \n   AUC estimation method : empirical                      \n                                                          \n   CI of AUC                                              \n   confidence level = 95%                                 \n   lower = 0.880257283328194     upper = 0.960994743109017",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Relazioni test-criterio</span>"
    ]
  },
  {
    "objectID": "chapters/validity/02_other_variables.html#considerazioni-conclusive",
    "href": "chapters/validity/02_other_variables.html#considerazioni-conclusive",
    "title": "15  Relazioni test-criterio",
    "section": "15.2 Considerazioni conclusive",
    "text": "15.2 Considerazioni conclusive\nIn conclusione, il capitolo ha dimostrato l’utilizzo dell’analisi di regressione logistica e del calcolo dell’Area Under the Curve (AUC) come strumenti importanti per valutare la validità di criterio di un test, specialmente quando il criterio riguarda l’appartenenza a un gruppo diagnostico specifico.\nL’analisi di regressione logistica e il calcolo dell’AUC forniscono una valutazione accurata della capacità del test di classificare correttamente i partecipanti in base al criterio diagnostico desiderato. Questa valutazione accurata offre una solida base per l’applicazione pratica del test, consentendo decisioni informate e mirate nell’identificazione di individui con particolari caratteristiche o condizioni. Inoltre, l’utilizzo di questi strumenti statistici rappresenta un passo importante verso la validazione dei test, garantendo la qualità delle diagnosi e facilitando l’implementazione di interventi mirati ed efficaci. La corretta valutazione della validità di criterio è essenziale per ottenere risultati affidabili e utili nella pratica clinica e nella ricerca.",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Relazioni test-criterio</span>"
    ]
  },
  {
    "objectID": "chapters/gtheory/01_gtheory.html",
    "href": "chapters/gtheory/01_gtheory.html",
    "title": "16  Teoria della generalizzabilità",
    "section": "",
    "text": "16.1 Introduzione\nNei precedenti capitoli è stato illustrato come la Teoria Classica dei Test (CTT) identifichi l’errore di misurazione come una fonte di varianza non spiegata e definisca l’affidabilità come la proporzione di varianza vera rispetto alla varianza totale, che include anche l’errore di misurazione. La teoria della generalizzabilità estende questo concetto nella CTT, consentendo di distinguere tra diverse fonti di errore di misurazione in casi di disegni complessi, come errori associati alle persone, alle occasioni e agli item.\nQuesto capitolo si concentra su un’applicazione specifica della teoria della generalizzabilità, che è quella di stimare l’affidabilità delle misure all’interno di un disegno longitudinale. Nel corso di questo tutorial, esploreremo come affrontare questa sfida utilizzando il framework della teoria della generalizzabilità. Nei capitoli successivi esamineremo un approccio alternativo per risolvere lo stesso problema, che è il Latent Growth Modeling.",
    "crumbs": [
      "Generalizzabilità",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Teoria della generalizzabilità</span>"
    ]
  },
  {
    "objectID": "chapters/gtheory/01_gtheory.html#molteplici-fonti-di-errore-di-misurazione",
    "href": "chapters/gtheory/01_gtheory.html#molteplici-fonti-di-errore-di-misurazione",
    "title": "16  Teoria della generalizzabilità",
    "section": "16.2 Molteplici fonti di errore di misurazione",
    "text": "16.2 Molteplici fonti di errore di misurazione\nLa Teoria della Generalizzabilità, nota anche come “Generalizability Theory,” è una teoria statistica che fornisce un quadro per studiare l’affidabilità e la validità delle misurazioni in diversi contesti. In questa teoria, i fattori che possono contribuire all’errore nelle misurazioni vengono chiamati “facets” (es. valutatori, compiti, occasioni). Ogni fattore può essere considerato fisso o casuale.\nLa terminologia utilizzata è la seguente:\n\nLe “condizioni” (condition) rappresentano i livelli dei vari fattori.\nL’oggetto di misurazione (Object of Measurement), che di solito sono le persone, non è considerato un fattore ma è sempre considerato casuale.\nL’“Universo delle Operazioni Ammissibili” (Universe of Admissible Operations, UAO) è un ampio insieme di condizioni alle quali si vogliono generalizzare i risultati osservati.\nLo “Score dell’Universo” (Universe Score) rappresenta il punteggio medio di una persona considerando tutte le possibili combinazioni di condizioni nell’UAO.\nLo studio “G” (G Study) mira a ottenere informazioni accurate sulla grandezza dei fattori di errore.\nLo studio “D” (D Study) riguarda la progettazione di uno scenario di misurazione con il livello desiderato di affidabilità utilizzando il minor numero possibile di condizioni.\n\nLa Teoria della Generalizzabilità è particolarmente utile quando si desidera valutare l’affidabilità e la validità delle misurazioni in situazioni complesse, in cui diversi fattori possono influenzare l’errore di misurazione.",
    "crumbs": [
      "Generalizzabilità",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Teoria della generalizzabilità</span>"
    ]
  },
  {
    "objectID": "chapters/gtheory/01_gtheory.html#differenze-tra-la-teoria-g-e-la-ctt",
    "href": "chapters/gtheory/01_gtheory.html#differenze-tra-la-teoria-g-e-la-ctt",
    "title": "16  Teoria della generalizzabilità",
    "section": "16.3 Differenze tra la teoria G e la CTT",
    "text": "16.3 Differenze tra la teoria G e la CTT\nLa Teoria G e la Teoria Classica dei Test (CTT) sono due approcci distinti per valutare l’affidabilità di un test psicometrico. La Teoria G fornisce una valutazione più completa delle fonti di errore di misurazione, consentendo di stimare simultaneamente molteplici fonti di errore in un’unica analisi. I coefficienti di affidabilità nella Teoria G tengono conto di tutte le fonti misurate di errore, fornendo una stima più accurata dell’affidabilità complessiva del test.\nD’altra parte, la Teoria CTT permette di stimare solo una fonte di errore di misurazione alla volta. I coefficienti di affidabilità nella Teoria CTT si concentrano su una sola fonte di errore e non forniscono una visione completa dell’affidabilità del test. Inoltre, la Teoria G offre un metodo per determinare il numero di livelli di ciascuna fonte di errore necessari per ottenere livelli di affidabilità accettabili. Ciò consente di ottimizzare il design del test e garantire che il test sia affidabile in diverse situazioni e condizioni. Infine, la Teoria G fornisce coefficienti di affidabilità sia per decisioni riferite a norme (come classificazioni rispetto a una distribuzione di riferimento) che per decisioni riferite a criteri specifici (come confronti con standard prestabiliti). D’altro canto, i coefficienti di affidabilità più comunemente utilizzati nella Teoria CTT sono più adatti per test riferiti a norme, mentre per test riferiti a criteri possono essere meno accurati.\nIn sintesi, la Teoria G offre una valutazione più completa e accurata dell’affidabilità di un test, considerando diverse fonti di errore e fornendo informazioni utili per ottimizzare il design del test. La Teoria CTT, sebbene più semplice, è limitata nella sua capacità di catturare la complessità delle fonti di errore di misurazione. Pertanto, la Teoria G è spesso preferita quando si tratta di valutare l’affidabilità di test psicometrici complessi e con molteplici fonti di variabilità.",
    "crumbs": [
      "Generalizzabilità",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Teoria della generalizzabilità</span>"
    ]
  },
  {
    "objectID": "chapters/gtheory/01_gtheory.html#fattori-incrociati-o-nidificati",
    "href": "chapters/gtheory/01_gtheory.html#fattori-incrociati-o-nidificati",
    "title": "16  Teoria della generalizzabilità",
    "section": "16.4 Fattori incrociati o nidificati",
    "text": "16.4 Fattori incrociati o nidificati\nUn concetto fondamentale della teoria della generalizzabilità è la gerarchia dei dati, che si riferisce alla struttura del disegno di ricerca. Esistono due tipi principali di disegni: annidati (nested) e incrociati (crossed). Nei disegni annidati, i livelli di un fattore sono contenuti all’interno dei livelli di un altro; nei disegni incrociati, ogni livello di un fattore si combina con tutti i livelli di un altro fattore.\nConsideriamo ad esempio uno studio in cui diversi psicologi valutano l’intelligenza di studenti in diverse scuole. Se ogni psicologo valuta gli studenti di tutte le scuole, allora i fattori “psicologo” e “scuola” sono incrociati. Ciò significa che tutte le combinazioni di psicologi e scuole sono rappresentate nel campione.\nD’altra parte, supponiamo che vi siano gruppi distinti di psicologi assegnati a diverse scuole. Ad esempio, un gruppo di psicologi potrebbe valutare gli studenti di una scuola, mentre un altro gruppo di psicologi potrebbe valutare gli studenti di un’altra scuola. In questo caso, i valutatori sono nidificati all’interno delle scuole. Ciò significa che ogni scuola ha un gruppo specifico di psicologi associati a essa per le valutazioni.\nLa distinzione tra fattori incrociati e nidificati è importante perché influisce sulla generalizzabilità delle conclusioni dello studio. Nel caso incrociato, le conclusioni possono essere generalizzate a tutte le combinazioni di valutatori e scuole presenti nel campione. Nel caso nidificato, le conclusioni possono essere generalizzate solo alle scuole specifiche associate ai rispettivi gruppi di valutatori.",
    "crumbs": [
      "Generalizzabilità",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Teoria della generalizzabilità</span>"
    ]
  },
  {
    "objectID": "chapters/gtheory/01_gtheory.html#fattori-casuali-o-fissi",
    "href": "chapters/gtheory/01_gtheory.html#fattori-casuali-o-fissi",
    "title": "16  Teoria della generalizzabilità",
    "section": "16.5 Fattori casuali o fissi",
    "text": "16.5 Fattori casuali o fissi\nUn secondo concetto fondamentale della teoria della generalizzabilità riguarda la distinzione tra fattori casuali e fattori fissi in un disegno di ricerca. Un fattore è detto casuale quando le sue condizioni specifiche nello studio sono viste come un campione di un universo più ampio di condizioni, e si presume che queste siano equivalenti e interscambiabili con qualsiasi altra condizione nello stesso universo (Universo delle Operazioni Ammissibili, UAO). Ciò consente di generalizzare i risultati dello studio a tutte le condizioni all’interno dell’UAO. Ad esempio, i valutatori in uno studio sono considerati un fattore casuale se si assume che le valutazioni di uno siano sostituibili con quelle di un altro.\nContrariamente, un fattore fisso si concentra su condizioni specifiche e predeterminate, che costituiscono il completo insieme di interesse per il ricercatore. Questo approccio è utilizzato quando lo scopo è analizzare l’effetto di queste condizioni particolari senza cercare di generalizzare i risultati oltre a queste. Ad esempio, consideriamo uno studio sull’effetto di diverse terapie sulla riduzione dell’ansia in pazienti affetti da disturbi d’ansia, dove si esaminano specificamente tre tipi di terapia: Terapia A, Terapia B e Terapia C. Se ogni paziente partecipa a una ed una sola di queste terapie, selezionata in modo casuale, il fattore “Tipo di Terapia” è considerato fisso perché l’intenzione è di valutare l’effetto specifico di queste terapie selezionate, non di altre potenziali terapie non incluse nello studio.",
    "crumbs": [
      "Generalizzabilità",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Teoria della generalizzabilità</span>"
    ]
  },
  {
    "objectID": "chapters/gtheory/01_gtheory.html#la-teoria-g",
    "href": "chapters/gtheory/01_gtheory.html#la-teoria-g",
    "title": "16  Teoria della generalizzabilità",
    "section": "16.6 La teoria G",
    "text": "16.6 La teoria G\nIn questo capitolo, esamineremo uno specifico uso della Teoria della Generalizzabilità in uno studio longitudinale. Specificamente, ci porremo il problema di stimare l’affidabilità delle misure dei partecipanti nel tempo.\nSupponiamo di condurre uno studio in cui vogliamo valutare le performance di diversi studenti nel corso del tempo. Raccogliamo i punteggi di cinque studenti (A, B, C, D, E) su un compito misurato in tre diverse occasioni temporali (T1, T2, T3). Il nostro obiettivo è comprendere come variano le performance tra gli studenti e nel corso del tempo. Il disegno è persona-per-tempo (5 persone x 3 occasioni di misurazione).\nL’equazione di decomposizione della varianza degli score osservati in questo disegno è la seguente:\n\\[\n\\sigma^2(X_{pt}) = \\sigma^2_p + \\sigma^2_t + \\sigma^2_{pt} + \\sigma^2_{pt,e},\n\\]\ndove: - \\(\\sigma^2_p\\): rappresenta l’effetto principale delle persone, ovvero quanto variano le performance tra gli studenti. - \\(\\sigma^2_t\\): indica l’effetto principale del tempo, ovvero quanto variano le performance degli studenti nel corso del tempo. - \\(\\sigma^2_{pt}\\): rappresenta l’interazione persona-per-tempo, ovvero quanto variano le performance degli studenti nel corso del tempo. - \\(\\sigma^2_{pt,e}\\): è la varianza residua o non misurata, che include l’errore casuale e altre fonti di varianza non considerate nel disegno.\nIn un secondo esempio, consideriamo un disegno a tre fattori. Supponiamo di condurre uno studio per valutare le performance di diversi studenti (A, B, C, D, E) su diversi compiti (item) nel corso del tempo (T1, T2, T3). Vogliamo comprendere come le performance degli studenti possono variare tra gli item, nel tempo e se ci sono interazioni tra questi fattori. Il disegno è persona-per-item-per-tempo (5 persone x 3 item x 3 occasioni di misura).\nL’equazione di decomposizione della varianza degli score osservati in questo disegno sarà la seguente:\n\\[\n\\sigma^2(X_{pit}) = \\sigma^2_p + \\sigma^2_i + \\sigma^2_t + \\sigma^2_{pi} + \\sigma^2_{pt} + \\sigma^2_{it} + \\sigma^2_{pit,e},\n\\]\ndove: - \\(\\sigma^2_p\\): rappresenta l’effetto principale delle persone, ovvero quanto variano le performance tra gli studenti. - \\(\\sigma^2_i\\): indica l’effetto principale degli item, ovvero quanto variano i punteggi dei compiti tra i diversi item. - \\(\\sigma^2_t\\): rappresenta l’effetto principale del tempo, ovvero quanto variano le performance degli studenti nel corso del tempo. - \\(\\sigma^2_{pi}\\): indica l’interazione persona-per-item, ovvero quanto variano le performance degli studenti tra i diversi compiti. - \\(\\sigma^2_{pt}\\): rappresenta l’interazione persona-per-tempo, ovvero quanto variano le performance degli studenti nel corso del tempo. - \\(\\sigma^2_{it}\\): indica l’interazione item-per-tempo, ovvero quanto variano i punteggi dei compiti nel corso del tempo. - \\(\\sigma^2_{pit,e}\\): è la varianza residua o non misurata, che include l’errore casuale e altre fonti di varianza non considerate nel disegno.\nQuesti modelli ci permettono di analizzare come ciascuna fonte di variabilità influenzi i punteggi osservati.",
    "crumbs": [
      "Generalizzabilità",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Teoria della generalizzabilità</span>"
    ]
  },
  {
    "objectID": "chapters/gtheory/01_gtheory.html#affidabilità",
    "href": "chapters/gtheory/01_gtheory.html#affidabilità",
    "title": "16  Teoria della generalizzabilità",
    "section": "16.7 Affidabilità",
    "text": "16.7 Affidabilità\nIl Cambiamento di Affidabilità, nota come Cambiamento di Affidabilità (RC, da Reliability Change), valuta in che misura le variazioni nei punteggi di soggetti, valutati ripetutamente nel tempo, riflettano cambiamenti reali piuttosto che essere causate da errori di misurazione. Questo indice è fondamentale in studi longitudinali, dove si osserva lo stesso gruppo di individui in più occasioni, per stabilire se i cambiamenti nei punteggi sono sistematici e affidabili, legati al tempo e alle caratteristiche uniche dei partecipanti. Un alto valore di RC indica che le misurazioni sono coerenti nel tempo, suggerendo che qualsiasi variazione rilevata rappresenti un vero cambiamento nel comportamento o nelle risposte dei soggetti. Pertanto, l’RC aiuta a distinguere tra cambiamenti autentici e fluttuazioni dovute a inesattezze nella raccolta dei dati.\nAd esempio, nel caso di un disegno a tre fattori (perone \\(\\times\\) item \\(\\times\\) tempo), la formula di RC è la seguente:\n\\[\nR_c = \\frac{\\sigma^2_{TP}}{\\sigma^2_{TP} + \\frac{1}{k}(\\sigma^2_{TPI} + \\sigma^2_{v})},\n\\]\ndove: - \\(R_c\\) rappresenta la misura di affidabilità focale (Reliability Change). - \\(\\sigma^2_{TP}\\) è la varianza tra il tempo e le persone (time by person variance). - \\(\\sigma^2_{TPI}\\) è la varianza dell’interazione tra il tempo, le persone e gli item. - \\(\\sigma^2_{v}\\) è la varianza dell’errore (error variance). - \\(k\\) è il numero di item utilizzati.\nIl numeratore contiene solo una componente, ovvero la varianza tra il tempo e le persone, \\(\\sigma^2_{TP}\\). Il denominatore invece contiene la stessa componente di varianza, sommata alla componente di varianza dell’errore, divisa per il numero di item \\(k\\). Si noti che la componente di varianza dell’errore è la somma \\(\\sigma^2_{TPI} + \\sigma^2_{v}\\).\n\n16.7.1 Un esempio concreto\nApplichiamo questi concetti ai dati di Bolger e Laurenceau (2013) riguardanti uno studio che ha coinvolto 50 persone valutate per 10 giorni su 4 item. Gli item sono “interessato,” “determinato,” “entusiasta” e “ispirato,” rispettivamente. Le valutazioni per ciascun item sono state fatte su una scala da 1 (per niente) a 5 (estremamente).\nNel contesto della Teoria della Generalizzabilità, possiamo esaminare la variabilità dei punteggi dei partecipanti e suddividerla nelle diverse fonti di errore, che in questo caso includono la variazione dovuta ai diversi partecipanti (fonte di errore “Valutatori”), a diversi giorni (fonte di errore “Giorni”) e agli item specifici utilizzati (fonte di errore “Item”). Possiamo valutare quanto della variazione totale nei punteggi è attribuibile a ciascuna di queste fonti di errore.\nPer ottenere misure più precise e generalizzabili degli stati emotivi delle persone, possiamo calcolare gli “Score dell’Universo” per ciascun individuo. Gli score dell’universo rappresentano la media dei loro punteggi su tutti gli item, in tutti i giorni e su tutti i partecipanti. Questi score ci forniranno una stima più accurata del livello medio di “interessato,” “determinato,” “entusiasta” e “ispirato” per ciascun partecipante, considerando tutte le fonti di errore specificate.\nInfine, utilizzando la Teoria della Generalizzabilità, possiamo progettare uno studio ottimale (“D Study”) per massimizzare l’affidabilità delle misurazioni con il minor numero possibile di partecipanti, giorni e item. In questo modo, otterremo misurazioni più affidabili senza la necessità di sottoporre i partecipanti a un numero eccessivo di giorni di valutazione o di utilizzare troppi item.\nIn questo esempio, ci poniamo la seguente domanda: “Le variazioni all’interno dei soggetti possono essere misurate in modo affidabile?” Per rispondere a questa domanda, dobbiamo specificare le dimensioni di generalizzabilità. In questo caso, le dimensioni sono i momenti nel tempo, le persone e gli item.\nPer condurre questa analisi, useremo un modello a effetti misti per ciascuna delle dimensioni specificate.\nIniziamo leggendo i dati di Bolger e Laurenceau (2013).\n\nfilepath &lt;- \"https://quantdev.ssri.psu.edu/sites/qdev/files/psychometrics.csv\"\nd &lt;- read.csv(filepath)\nhead(d)\n\n\nA data.frame: 6 x 4\n\n\n\nperson\ntime\nitem\ny\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n301\n1\n1\n2\n\n\n2\n301\n1\n2\n2\n\n\n3\n301\n1\n3\n3\n\n\n4\n301\n1\n4\n4\n\n\n5\n301\n2\n1\n2\n\n\n6\n301\n2\n2\n3\n\n\n\n\n\nRicodifichiamo la variabile item in modo che sia categorica utilizzando la funzione factor().\n\nd$item &lt;- factor(d$item)\n\nIl pacchetto lme4 contiene la funzione lmer(), che permette di adattare un modello lineare a effetti misti a un specifico set di dati. Utilizzando il summary() del nostro modello, possiamo osservare gli effetti di ciascuna dimensione di generalizzabilità. Questo modello è specificato solo con l’intercetta (ANOVA con effetti casuali) allo scopo di comprendere le fonti di variabilità tra le diverse dimensioni.\n\nmodel1 &lt;- lmer(\n    y ~ 1 +\n        (1 | person) +\n        (1 | time) +\n        (1 | item) +\n        (1 | person:time) +\n        (1 | person:item) +\n        (1 | time:item),\n    data = d\n)\nsummary(model1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y ~ 1 + (1 | person) + (1 | time) + (1 | item) + (1 | person:time) +  \n    (1 | person:item) + (1 | time:item)\n   Data: d\n\nREML criterion at convergence: 4046.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2721 -0.5311 -0.0105  0.5044  3.8613 \n\nRandom effects:\n Groups      Name        Variance  Std.Dev. \n person:time (Intercept) 2.553e-01 5.053e-01\n person:item (Intercept) 1.901e-01 4.360e-01\n person      (Intercept) 3.619e-01 6.016e-01\n time:item   (Intercept) 4.985e-03 7.060e-02\n time        (Intercept) 7.366e-09 8.583e-05\n item        (Intercept) 4.854e-02 2.203e-01\n Residual                2.995e-01 5.473e-01\nNumber of obs: 1802, groups:  \nperson:time, 455; person:item, 200; person, 50; time:item, 40; time, 10; item, 4\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   2.4340     0.1456   16.72\n\n\nUtilizzando la funzione VarCorr(), possiamo estrarre e salvare ciascun valore di varianza dalla tabella di riepilogo dei risultati.\n\n(personTime &lt;- VarCorr(model1)[[1]][1, 1]) # person:time\n\n0.255279541307926\n\n\n\n(personItem &lt;- VarCorr(model1)[[2]][1, 1]) # person:item\n\n0.190074153321712\n\n\n\n(person &lt;- VarCorr(model1)[[3]][1, 1]) # person\n\n0.36189689746911\n\n\n\n(timeItem &lt;- VarCorr(model1)[[4]][1,1]) #time:item \n\n0.00498471479702945\n\n\n\n(time &lt;- VarCorr(model1)[[5]][1,1]) #time \n\n7.36624370806228e-09\n\n\n\n(item &lt;- VarCorr(model1)[[6]][1, 1]) # item\n\n0.0485426397323054\n\n\n\n(residual &lt;- sigma(model1)^2) # residual\n\n0.299541741894979\n\n\nTornando alla nostra domanda iniziale: Esistono differenze affidabili all’interno di una persona nel corso del tempo?\nUtilizzeremo la seguente formula per calcolare il coefficiente di affidabilità:\n\\[\nR_c = \\frac{\\sigma^2_{\\text{persona:tempo}}}{\\sigma^2_{\\text{persona:tempo}} + \\frac{\\sigma^2_{\\text{persona:tempo:item}} + \\sigma^2_{\\text{errore}}}{k}},\n\\]\ndove \\(k\\) si riferisce al numero di elementi. Nel nostro caso, \\(k\\)=4.\nNon possiamo distinguere il termine \\(\\sigma^2_{\\text{persona:tempo:item}}\\) da \\(\\sigma^2_{\\text{errore}}\\), quindi useremo il termine di errore residuo.\n\nk &lt;- 4\n(Rc &lt;- personTime / (personTime + residual / k))\n\n0.773187828086126\n\n\nQuesto coefficiente rappresenta il grado di adeguatezza e sistematicità delle misurazioni ripetute. Utilizzando le stesse regole interpretative del coefficiente alfa di Cronbach, possiamo determinare che quattro elementi possono catturare il cambiamento all’interno di una persona in modo affidabile, con \\(R_c = 0.77\\).\nUn altro coefficiente di interesse è \\(R_{1F}\\), che viene calcolato come:\n\\[\nR_{1F} = \\frac{\\sigma^2_{\\text{persona}} + \\left(\\frac{\\sigma^2_{\\text{persona:item}}}{k}\\right)}{\\sigma^2_{\\text{persona}} + \\left(\\frac{\\sigma^2_{\\text{persona:item}}}{k}\\right) + \\left(\\frac{\\sigma^2_{\\text{errore}}}{k}\\right)}\n\\]\ndove \\(k\\) si riferisce al numero di item. Nel nostro caso, \\(k\\)=4.\nPer i dati presenti, abbiamo:\n\nk &lt;- 4\n(R1f &lt;- (person + (personItem / k)) / (person + (personItem / k) + (residual / k)))\n\n0.845374146701693\n\n\nIl coefficiente di affidabilità \\(R_{1F}\\) rappresenta la stima attesa dell’affidabilità tra le persone per un giorno fisso, una sorta di media degli alfa di Cronbach specifici per ogni giorno in diverse occasioni. Utilizzando le stesse regole interpretative del coefficiente alfa di Cronbach, possiamo concludere che quattro item possono catturare in modo affidabile le differenze tra le persone in qualsiasi giorno specifico, con \\(R_{1F}\\) = 0.85.",
    "crumbs": [
      "Generalizzabilità",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Teoria della generalizzabilità</span>"
    ]
  },
  {
    "objectID": "chapters/gtheory/01_gtheory.html#conclusioni",
    "href": "chapters/gtheory/01_gtheory.html#conclusioni",
    "title": "16  Teoria della generalizzabilità",
    "section": "16.8 Conclusioni",
    "text": "16.8 Conclusioni\nLa Teoria della Generalizzabilità (G theory) fornisce un quadro completo per stimare l’impatto di molteplici fonti di errore di misurazione simultaneamente. La G theory si basa su un modello ANOVA in cui i fattori sono chiamati “facet” (fattori) e i loro livelli sono noti come “conditions” (condizioni). Ad esempio, gli studenti potrebbero essere valutati su un insieme di compiti da un gruppo di valutatori in diverse occasioni. Compiti, valutatori e occasioni potrebbero tutti contribuire all’errore di misurazione e verrebbero considerati “facet” nel disegno della G theory. La varianza dovuta a questi “facet” e alle loro interazioni potrebbe essere stimata, e le loro relative contribuzioni alla varianza dell’errore di misurazione valutate.\nIl concetto di generalizzabilità o affidabilità nella G theory è analogo al concetto di affidabilità nella CTT. Nella G theory, l’interesse è rivolto al grado in cui i punteggi osservati ottenuti in un determinato insieme di condizioni possono essere generalizzati alla media del punteggio che potrebbe essere ottenuto in un insieme di condizioni più ampiamente definite, noto come “UAO” (Universal Attribute Object). L’UAO è definito dal ricercatore e include tutte le condizioni che produrrebbero punteggi accettabili. Il grado in cui i punteggi si generalizzano dalle condizioni osservate all’UAO è definito come affidabilità. Livelli elevati di affidabilità indicano che i punteggi ottenuti nelle condizioni osservate si generalizzeranno ai punteggi universali delle persone. Il punteggio universale è analogo al punteggio vero nella CTT e può essere concepito come il punteggio medio che una persona otterrebbe se sottoposta a ripetuti test in tutte le possibili combinazioni di condizioni presenti nell’UAO.",
    "crumbs": [
      "Generalizzabilità",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Teoria della generalizzabilità</span>"
    ]
  },
  {
    "objectID": "chapters/gtheory/01_gtheory.html#session-info",
    "href": "chapters/gtheory/01_gtheory.html#session-info",
    "title": "16  Teoria della generalizzabilità",
    "section": "16.9 Session Info",
    "text": "16.9 Session Info\n\nsessionInfo()\n\nR version 4.3.2 (2023-10-31)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.3.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] tidyr_1.3.1   lme4_1.1-35.1 Matrix_1.6-5 \n\nloaded via a namespace (and not attached):\n [1] crayon_1.5.2     vctrs_0.6.5      nlme_3.1-164     cli_3.6.2       \n [5] rlang_1.1.3      purrr_1.0.2      generics_0.1.3   jsonlite_1.8.8  \n [9] minqa_1.2.6      glue_1.7.0       htmltools_0.5.7  IRdisplay_1.1   \n[13] IRkernel_1.3.2   fansi_1.0.6      grid_4.3.2       tibble_3.2.1    \n[17] evaluate_0.23    MASS_7.3-60.0.1  fastmap_1.1.1    base64enc_0.1-3 \n[21] lifecycle_1.0.4  compiler_4.3.2   dplyr_1.1.4      pkgconfig_2.0.3 \n[25] Rcpp_1.0.12      pbdZMQ_0.3-11    lattice_0.22-5   digest_0.6.34   \n[29] R6_2.5.1         nloptr_2.0.3     tidyselect_1.2.0 repr_1.1.6      \n[33] utf8_1.2.4       pillar_1.9.0     splines_4.3.2    magrittr_2.0.3  \n[37] uuid_1.2-0       tools_4.3.2      boot_1.3-29",
    "crumbs": [
      "Generalizzabilità",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Teoria della generalizzabilità</span>"
    ]
  },
  {
    "objectID": "chapters/items/01_item_development.html",
    "href": "chapters/items/01_item_development.html",
    "title": "17  Lo sviluppo degli item",
    "section": "",
    "text": "17.1 Introduzione\nI test psicologici sono composti da item, quindi la bontà degli item determina la bontà del test. A prima vista, lo sviluppo di buoni item può sembrare un’impresa semplice e diretta, ma in realtà, la bontà degli item è determinata dalla attenta considerazione di diversi importanti fattori combinata con una valutazione quantitativa tramite specifiche procedure psicometriche. In questo capitolo, forniamo una discussione pratica su come sviluppare buoni item. Ciò include la discussione dei diversi formati di item disponibili agli autori di test e alcune linee guida di base per lo sviluppo degli item. Discutiamo lo sviluppo di item per test di massima prestazione e test di risposta tipica. Ricorderete che i test di massima prestazione sono progettati per determinare i limiti superiori delle abilità o conoscenze delle persone, mentre i test di risposta tipica valutano le loro caratteristiche quotidiane o abitudinarie. In un contesto occupazionale, un datore di lavoro potrebbe utilizzare un test di risposta tipica per determinare se un dipendente sta completando le attività quotidiane richieste per il lavoro e un test di massima prestazione per determinare se il dipendente ha la conoscenza o l’abilità necessaria per una promozione a un lavoro di livello superiore e più complesso. I test di massima prestazione e di risposta tipica hanno ruoli importanti nella valutazione psicologica, quindi consideriamo gli item utilizzati in entrambi i casi. Iniziamo questo capitolo con una breve panoramica dei formati di item più popolari prima di procedere con una discussione delle linee guida per lo sviluppo degli item.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Lo sviluppo degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/01_item_development.html#classificazione-degli-item-nei-test-psicometrici",
    "href": "chapters/items/01_item_development.html#classificazione-degli-item-nei-test-psicometrici",
    "title": "17  Lo sviluppo degli item",
    "section": "17.2 Classificazione degli Item nei Test Psicometrici",
    "text": "17.2 Classificazione degli Item nei Test Psicometrici\nNel panorama della psicometria, la classificazione degli item di test è fondamentale per determinare la loro validità e affidabilità. Tradizionalmente, gli item si distinguono in due categorie principali: oggettivi e soggettivi. Questa bipartizione, seppur utile, non esaurisce l’ampiezza e la complessità della materia. Pertanto, è essenziale esplorare in modo più approfondito i criteri di classificazione e le loro implicazioni.\n\n17.2.1 Item Oggettivi e Soggettivi: Un Continuum di Valutazione\nLa distinzione primaria tra item oggettivi e soggettivi si basa sul metodo di valutazione. Gli item oggettivi si caratterizzano per la presenza di un consenso ampio tra gli esperti circa la correttezza delle risposte, come nel caso degli item a scelta multipla, vero/falso e di abbinamento. In questi formati, la correttezza della risposta è inequivocabile e non presta il fianco a interpretazioni soggettive.\nAl contrario, gli item soggettivi implicano una maggiore discrezionalità nella valutazione. Esempi tipici sono gli item a tema o le risposte in un esame orale, dove l’apporto soggettivo del valutatore gioca un ruolo cruciale. Questa categoria di item richiede una valutazione più articolata e può portare a divergenze tra i valutatori.\n\n\n17.2.2 Classificazione Alternativa: Risposta Selezionata vs Risposta Costruita\nUna classificazione più moderna e funzionale distingue gli item in base alla natura della risposta richiesta: risposta selezionata e risposta costruita. In questo schema, gli item a risposta selezionata includono quelli a scelta multipla, vero/falso e di abbinamento, dove la risposta è già fornita e il candidato deve selezionarla. Questi item permettono una valutazione rapida, oggettiva e affidabile, rendendoli particolarmente adatti per test di ampio respiro.\nGli item a risposta costruita, invece, richiedono al candidato di generare una risposta, come nei casi di risposte brevi, saggi o valutazioni delle prestazioni. Questi item sono più idonei per valutare abilità cognitive di ordine superiore e competenze specifiche, ma sono più soggetti a valutazioni soggettive e richiedono un tempo maggiore sia per la risposta sia per la valutazione.\n\n\n17.2.3 Punti di Forza e Limitazioni\nOgni categoria di item presenta specifici punti di forza e limitazioni. Gli item a risposta selezionata sono efficienti, affidabili e permettono di includere un maggior numero di domande nel test, ma possono essere complessi da formulare e potrebbero non essere adatti per valutare tutte le tipologie di competenze. Inoltre, sono soggetti al rischio di risposte casuali o indovinate.\nGli item a risposta costruita, d’altra parte, sono più adatti per valutare competenze complesse e abilità di ordine superiore, ma richiedono più tempo per la risposta e la valutazione, e possono essere influenzati da fattori estranei non correlati al costrutto da misurare.\nNella scelta del formato di valutazione, il fattore determinante dovrebbe essere l’adeguatezza del formato nel misurare il costrutto di interesse in modo diretto e puro. La scelta dipenderà dagli obiettivi specifici del test e dalla natura del costrutto da valutare. In generale, si raccomanda di preferire gli item a risposta selezionata per la loro capacità di campionare ampiamente il dominio del contenuto e per le loro caratteristiche di valutazione più oggettive e affidabili. Tuttavia, gli item a risposta costruita sono indispensabili per valutare certe competenze e abilità cognitive di ordine superiore.\nIn conclusione, una comprensione approfondita delle diverse tipologie di item e delle loro specificità è fondamentale per lo sviluppo di strumenti psicometrici efficaci e affidabili, in grado di fornire valutazioni precise e pertinenti ai costrutti psicologici in esame.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Lo sviluppo degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/01_item_development.html#linee-guida-generali-per-la-redazione-di-item-di-test",
    "href": "chapters/items/01_item_development.html#linee-guida-generali-per-la-redazione-di-item-di-test",
    "title": "17  Lo sviluppo degli item",
    "section": "17.3 Linee Guida Generali per la Redazione di Item di Test",
    "text": "17.3 Linee Guida Generali per la Redazione di Item di Test\nVengono qui presentate alcune linee guida per lo sviluppo di vari tipi di item di test. Queste indicazioni devono però essere applicate in modo flessibile. L’obiettivo principale nella creazione di item di test è sviluppare domande che misurino in modo preciso il costrutto specificato, contribuendo alla validità psicometrica del test. I criteri usati per lo sviluppo degli item devono comunque sempre in primo luogo cercare di raggiungere quello che è l’obiettivo primario per cui il test viene costruito.\n\nFornire Istruzioni Chiare: È comune per i redattori di test inesperti assumere che i candidati sappiano come rispondere a diversi formati di item. Includere sempre istruzioni dettagliate che specificano chiaramente come il candidato debba rispondere a ciascun formato di item. Assumere che i candidati non abbiano mai visto un test simile e fornire istruzioni dettagliate per garantire che sappiano cosa ci si aspetta da loro. Tuttavia, istruzioni troppo lunghe e dettagliate possono diminuire la chiarezza.\nPresentare il Problema in Modo Chiaro: Mantenere la redazione degli item il più semplice possibile. A meno che non si stia valutando la capacità di lettura, puntare a un livello di lettura basso. Evitare termini scientifici o tecnici non necessari, così come costruzioni di frasi complesse o ambigue.\nSviluppare Item Valutabili in Modo Decisivo: Assicurarsi che gli item abbiano risposte chiare su cui quasi tutti gli esperti sarebbero d’accordo. Nel caso di saggi e valutazioni delle prestazioni, considerare se gli esperti concorderebbero sulla qualità della prestazione nel compito.\nEvitare Indizi Involontari: Fare attenzione a non includere indizi involontari che potrebbero guidare il candidato verso la risposta corretta.\nSistemare gli Item in Modo Sistematico: Organizzare gli item in modo che favoriscano la prestazione ottimale dei candidati. Se il test contiene più formati di item, raggrupparli in sezioni in base al tipo di item. Disporre gli item secondo il loro livello di difficoltà, iniziando da quelli più facili.\nMantenere gli Item su Una Pagina: Assicurarsi che ciascun item a risposta selezionata sia contenuto in una pagina, per evitare confusione e errori.\nPersonalizzare gli Item per la Popolazione di Riferimento: Considerare attentamente il tipo di clienti con cui il test sarà utilizzato e personalizzare gli item di conseguenza.\nMinimizzare l’Impatto di Fattori Irrilevanti: Cercare e minimizzare i fattori cognitivi, motori e altri che sono necessari per rispondere correttamente agli item, ma irrilevanti per il costrutto misurato.\nEvitare la Parafrasi del Materiale di Studio: Quando si preparano test di rendimento, evitare di usare la stessa formulazione presente nei libri di testo o altri materiali di studio.\nEvitare Linguaggio Pregiudizievole o Offensivo: Rivedere attentamente gli item per lingua potenzialmente pregiudizievole o offensiva.\nUsare un Formato di Stampa Chiaro e Leggibile: Utilizzare una dimensione e un interlinea del carattere chiari e appropriati per i candidati.\nDeterminare il Numero di Item da Includere: Considerare fattori come il tempo disponibile, l’età dei candidati, i tipi di item, l’ampiezza del materiale o degli argomenti valutati e il tipo di test.\n\nQueste linee guida sono fondamentali per lo sviluppo di item di test che siano non solo tecnicamente validi, ma anche accessibili e giusti per i candidati. L’applicazione flessibile e consapevole di queste indicazioni contribuirà significativamente all’efficacia e all’affidabilità degli strumenti di valutazione psicometrica.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Lo sviluppo degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/01_item_development.html#test-di-massima-prestazione",
    "href": "chapters/items/01_item_development.html#test-di-massima-prestazione",
    "title": "17  Lo sviluppo degli item",
    "section": "17.4 Test di Massima Prestazione",
    "text": "17.4 Test di Massima Prestazione\nIn questa sezione, ci concentreremo sullo sviluppo degli item per i test di massima prestazione, in particolare quelli progettati per valutare obiettivi educativi o di apprendimento. Sebbene tali linee guida siano pensate principalmente per i test di rendimento, molte di esse sono applicabili anche ai test di attitudine che utilizzano questi tipi di item. Inizieremo esaminando gli item a risposta selezionata, per poi passare agli item a risposta costruita. In sezioni successive, forniremo suggerimenti per sviluppare linee guida per i test di prestazione tipica.\n\n17.4.1 Item a Scelta Multipla\nGli item a scelta multipla sono tra i più popolari nel formato a risposta selezionata. Sono molto diffusi perché applicabili in diverse aree tematiche e capaci di valutare obiettivi semplici e complessi. Generalmente, assumono la forma di una domanda o di un’affermazione incompleta, con un insieme di possibili risposte, una delle quali è corretta. La parte dell’item che presenta la domanda o l’affermazione incompleta è chiamata “stem” o “radice”. Le possibili risposte sono denominate “alternative”. L’alternativa corretta è detta “risposta”, mentre le alternative errate sono note come “distrattori”.\n\n17.4.1.1 Suggerimenti per Sviluppare Item a Scelta Multipla\n\nUsare un Formato Chiaro: Non esiste un formato universalmente accettato, ma alcune raccomandazioni sul layout possono migliorare la chiarezza.\n\nNumerare lo stem dell’item per un’identificazione facile.\nIndentare le alternative e identificarle con lettere.\nNon capitalizzare l’inizio delle alternative, a meno che non inizino con un nome proprio.\nDisporre le alternative in un elenco verticale per facilitarne la lettura rapida.\n\nFornire Tutte le Informazioni Necessarie nello Stem dell’Item: Il problema o la domanda deve essere completamente sviluppato nello stem dell’item. Leggere lo stem dell’item senza esaminare le alternative per assicurarsi che sia sufficiente per comprendere la domanda.\nFornire da Tre a Cinque Alternative: L’uso di più alternative riduce la possibilità di indovinare la risposta corretta. Quattro è il numero più comune di alternative, ma cinque è raccomandato per ridurre ulteriormente il tasso di successo casuale.\nMantenere le Alternative Brevi e Ordinate: Le alternative devono essere il più brevi possibile e disposte in un ordine logico.\nEvitare Affermazioni Negative nello Stem dell’Item: Limitare l’uso di termini come “eccetto”, “meno”, “mai” o “non”. In casi eccezionali, evidenziare i termini negativi con maiuscole, sottolineatura o grassetto.\nAssicurare una Sola Risposta Corretta o la Migliore Risposta: Rivedere attentamente le alternative per garantire una sola risposta corretta o la migliore.\nCoerenza Grammaticale tra Stem dell’Item e Alternative: Tutte le alternative devono essere grammaticalmente corrette rispetto allo stem dell’item.\nRendere Tutti i Distrattori Plausibili: I distrattori devono sembrare ragionevoli e basarsi su errori comuni.\nPosizionare Casualmente la Risposta Corretta: Distribuire equamente la risposta corretta tra le posizioni delle alternative per evitare schemi prevedibili.\nLimitare l’Uso di ‘Nessuna delle Precedenti’ e Evitare ‘Tutte le Precedenti’: Utilizzare “Nessuna delle sopra” con parsimonia e evitare completamente “Tutte le sopra”.\nLimitare l’Uso di ‘Sempre’ e ‘Mai’ nelle Alternative: Evitare generalmente l’uso di termini assoluti come “sempre” e “mai”.\n\nGli item a scelta multipla sono un formato efficace per la valutazione, grazie alla loro versatilità, valutazione oggettiva e affidabile, e capacità di coprire ampiamente il dominio del contenuto. Tuttavia, la loro redazione non è semplice e non sono adatti per misurare tutti gli obiettivi di apprendimento.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Lo sviluppo degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/01_item_development.html#formato-di-risposta-verofalso",
    "href": "chapters/items/01_item_development.html#formato-di-risposta-verofalso",
    "title": "17  Lo sviluppo degli item",
    "section": "17.5 Formato di Risposta Vero/Falso",
    "text": "17.5 Formato di Risposta Vero/Falso\nIl formato di risposta vero/falso rappresenta una delle tipologie più popolari di item a risposta selezionata, seconda solo alla scelta multipla. Utilizzeremo il termine “vero/falso” per riferirci a una classe più ampia di item che possono includere formati binari, come accordo/disaccordo, corretto/errato, sì/no, fatto/opinione. Poiché il formato più comune è vero/falso, useremo questo termine in senso generico per indicare tutti gli item a due opzioni. Di seguito, forniremo linee guida per lo sviluppo di item vero/falso.\n\n17.5.1 Linee Guida per Sviluppare Item Vero/Falso\n\nEvitare Più di un’idea per Affermazione: Ogni item vero/falso dovrebbe affrontare una sola idea centrale. Evitare determinanti specifici e qualificatori che possano fungere da indizi per la risposta. Determinanti come “mai”, “sempre”, “nessuno” e “tutti” si trovano più frequentemente in affermazioni false e possono guidare i candidati non informati verso la risposta corretta. Al contrario, affermazioni moderate come “di solito”, “a volte” e “frequentemente” tendono a essere più veritiere e possono servire come indizi. Sebbene sia difficile evitare completamente i qualificatori, si consiglia di bilanciarli tra affermazioni vere e false per ridurne il valore come indizi.\nAvere Affermazioni Vere e False di Lunghezza Simile: Spesso gli autori tendono a formulare affermazioni vere più lunghe di quelle false. Per evitare che la lunghezza diventi un indizio involontario, è necessario assicurarsi che non ci sia una differenza evidente tra la lunghezza delle affermazioni vere e quelle false.\nIncludere un Numero Approssimativamente Uguale di Affermazioni Vere e False: Alcuni candidati tendono a scegliere “Vero” quando non sono sicuri della risposta, e altri “Falso”. Per prevenire un incremento artificiale dei punteggi dovuto a questi schemi di risposta, è consigliato includere un numero approssimativamente uguale di item veri e falsi. Alcuni autori suggerivano che nel formato vero/falso, il 60% degli item dovesse essere vero. Tuttavia, ciò è utile solo in circostanze limitate e non si applica ai test di prestazione tipica, essendo superato dal problema degli schemi di risposta e delle strategie di indovinamento. È preferibile un equilibrio.\n\nGli item vero/falso sono popolari nei test di massima prestazione. Sebbene possano essere valutati in modo oggettivo e affidabile e permettano ai candidati di rispondere a molti item in breve tempo, presentano varie debolezze. Ad esempio, sono spesso limitati alla valutazione di obiettivi di apprendimento piuttosto semplici e sono vulnerabili all’indovinamento. Prima di utilizzare gli item vero/falso, è raccomandato valutare i loro punti di forza e debolezze per assicurarsi che siano il formato più appropriato per valutare gli obiettivi specifici di apprendimento. Una checklist per lo sviluppo di item vero/falso può fornire un riferimento utile, applicabile anche ai formati sì/no spesso usati con individui più giovani.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Lo sviluppo degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/01_item_development.html#formato-di-risposta",
    "href": "chapters/items/01_item_development.html#formato-di-risposta",
    "title": "17  Lo sviluppo degli item",
    "section": "17.6 Formato di Risposta",
    "text": "17.6 Formato di Risposta\n\n17.6.1 Item di Abbinamento\nGli item di abbinamento (Matching Items) consistono in due colonne di parole o frasi: una colonna contiene i termini da abbinare (solitamente a sinistra, denominati “premesse”), e l’altra contiene le opzioni di risposta (a destra, chiamate “risposte”). Le premesse sono numerate, mentre le risposte sono identificate con lettere. Di seguito, alcune linee guida per lo sviluppo di questi item:\n\nLimitare l’Uso di Materiali Omogenei: È fondamentale che le liste siano il più omogenee possibile, basate su un tema comune. Evitare di includere materiali eterogenei.\nSpecificare nella Direzione le Basi dell’Abbinamento: Indicare chiaramente nelle istruzioni la base logica per l’abbinamento delle premesse con le risposte.\nIncludere più Risposte che Premesse: Ciò riduce la possibilità che i candidati non informati indovinino correttamente tramite eliminazione.\nIndicare che le Risposte Possono Essere Utilizzate Più Volte o Non Utilizzate: Questo riduce l’impatto dell’indovinamento.\nMantenere le Liste Brevi: Liste più brevi sono più gestibili sia per chi redige il test sia per chi lo svolge, evitando fattori confondenti come la memoria a breve termine.\nAssicurare che le Risposte Siano Brevi e Ordinate Logicamente: Ciò facilita la scansione efficiente delle opzioni da parte dei candidati.\n\nGli item di abbinamento possono essere valutati in modo oggettivo e affidabile, e sono relativamente facili da sviluppare. Tuttavia, hanno uno scopo limitato e possono promuovere la memorizzazione meccanica.\n\n\n17.6.2 Saggi\nUn item di saggio pone al candidato una domanda o un problema da rispondere in un formato scritto aperto. Essendo item a risposta costruita, richiedono una risposta elaborata dal candidato, non la selezione tra alternative. Di seguito alcune linee guida:\n\nSpecificare Chiaramente il Compito di Valutazione: È cruciale che il compito richiesto dall’item di saggio sia chiaramente definito, specificando la forma e l’ambito della risposta attesa.\nUtilizzare più Item a Risposta Ristretta Rispetto a Quelli a Risposta Estesa: Gli item a risposta ristretta sono più facili da valutare in modo affidabile e consentono una migliore campionatura del dominio di contenuto.\nSviluppare e Utilizzare una Rubrica di Valutazione: Una rubrica di valutazione fornisce indicazioni chiare per la valutazione di una risposta costruita, essenziale per una valutazione affidabile.\nLimitare l’Uso degli Item di Saggio a Obiettivi Non Misurabili con Item a Risposta Selezionata: Gli item di saggio hanno limitazioni, inclusa la difficoltà di valutazione affidabile e una minore campionatura del dominio di contenuto.\n\nIn generale, gli item di saggio sono adatti per misurare obiettivi complessi e sono relativamente facili da scrivere, ma presentano difficoltà nella valutazione affidabile e nella limitata campionatura del dominio di contenuto. Si raccomanda di limitare l’uso degli item di saggio alla misurazione di obiettivi che non sono facilmente valutabili tramite item a risposta selezionata.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Lo sviluppo degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/01_item_development.html#formato-di-risposta-breve",
    "href": "chapters/items/01_item_development.html#formato-di-risposta-breve",
    "title": "17  Lo sviluppo degli item",
    "section": "17.7 Formato di Risposta Breve",
    "text": "17.7 Formato di Risposta Breve\nGli item a risposta breve richiedono al candidato di fornire una parola, frase, numero o simbolo in risposta a una domanda diretta. Possono anche essere formulati come frasi incomplete, in un formato talvolta definito come “completamento”. Rispetto agli item di saggio, gli item a risposta breve pongono limiti più stretti sulla natura e lunghezza della risposta. Praticamente, un item a risposta breve è simile a un item di saggio a risposta ristretta, ma con ulteriori restrizioni. Di seguito alcune indicazioni specifiche per la redazione di item a risposta breve:\n\nStrutturare l’Item per una Risposta il più Breve Possibile: Gli item a risposta breve dovrebbero richiedere risposte concise, semplificando così la valutazione e rendendola più affidabile.\nGarantire una Sola Risposta Corretta: È importante che ci sia una sola risposta corretta per ogni item, evitando interpretazioni multiple.\nPreferire il Formato di Domanda Diretta alla Frase Incompleta: Generalmente, il formato di domanda diretta è meno confuso per i candidati. Utilizzare il formato di frase incompleta solo se questo comporta una maggiore brevità senza perdere in chiarezza.\nNel Formato di Frase Incompleta, Utilizzare un Solo Spazio Vuoto: Limitare ciascuna frase incompleta a uno spazio vuoto, preferibilmente vicino alla fine della frase, per maggior chiarezza.\nFornire Spazi Adeguati per le Risposte: Assicurarsi che ogni spazio vuoto fornisca spazio sufficiente per la risposta del candidato, evitando che la lunghezza dello spazio possa fornire indizi sulla risposta.\nPer Domande Quantitative, Indicare il Grado di Precisione Richiesto: Specificare, ad esempio, se la risposta deve essere espressa in pollici, o se le frazioni devono essere ridotte ai minimi termini.\nCreare una Rubrica di Valutazione e Applicarla in Modo Coerente: Come per gli item di saggio, è importante creare e utilizzare in modo coerente una rubrica di valutazione.\n\nGli item a risposta breve, simili agli item di saggio, richiedono una risposta scritta dal candidato ma con limiti più ristretti nella formulazione della risposta. Sono adatti per misurare determinati obiettivi di apprendimento (ad esempio, calcoli matematici) e sono relativamente facili da scrivere. Tuttavia, come gli item di saggio, presentano sfide nella valutazione affidabile e dovrebbero essere usati in modo oculato. Una checklist può fornire una guida utile per lo sviluppo di questi item.\n\n17.7.1 Test di Risposta Tipica\nDopo aver esaminato vari formati di item utilizzati nei test di massima prestazione, ci concentreremo sugli item comunemente usati nei test di risposta tipica, come le scale di personalità e di atteggiamento. Descriveremo diversi formati di item comuni a questi test e forniremo alcune linee guida generali per lo sviluppo di item. La valutazione di sentimenti, pensieri, dialoghi interni e altri comportamenti occulti è meglio realizzata tramite autovalutazione, che sarà il focus della nostra discussione. Tuttavia, come nei test di massima prestazione, esistono numerosi formati di item disponibili per le misure di autovalutazione.\n\n17.7.1.1 Linee Guida per la Redazione di Item in Test di Risposta Tipica\n\nConcentrarsi su Pensieri, Sentimenti e Comportamenti, non su Fatti: Nei test di risposta tipica, l’obiettivo è valutare le esperienze del candidato: i suoi pensieri, sentimenti e comportamenti tipici. Di conseguenza, si dovrebbero evitare affermazioni basate su informazioni fattuali che possono essere valutate come “corrette” o “errate”.\nLimitare le Affermazioni a un Singolo Pensiero, Sentimento o Comportamento: Ogni affermazione dovrebbe concentrarsi su un solo pensiero, sentimento, comportamento o atteggiamento.\nEvitare Affermazioni Universali: Per aumentare la varianza e migliorare l’affidabilità, si dovrebbero scrivere item che misurano le differenze individuali. Se tutti o quasi tutti rispondono a un item nello stesso modo, questo non contribuisce alla misurazione dei costrutti identificati.\nIncludere Item Formulati sia in Modo “Positivo/Favorevole” che “Negativo/Sfavorevole”: Come regola generale, usare una combinazione di item formulati in modo “positivo” e “negativo”. Ciò può incoraggiare i candidati a evitare uno stile di risposta in cui semplicemente segnano la stessa opzione di risposta su tutti gli item. Questo è più applicabile agli item Vero/Falso e alle scale Likert, e meno alle scale di valutazione dove si cerca di valutare la frequenza di pensieri, sentimenti e comportamenti problematici.\nUtilizzare un Numero Appropriato di Opzioni: Per le scale di valutazione, 4 o 5 opzioni di risposta sembrano ottimali per sviluppare affidabilità senza allungare eccessivamente il tempo richiesto per completare le valutazioni. Le scale di valutazione con più di 4 o 5 opzioni raramente migliorano l’affidabilità o la validità dell’interpretazione dei punteggi del test e richiedono più tempo ai candidati per essere completate. Per gli item Likert, il numero massimo di opzioni sembra essere di sette gradini, con un piccolo aumento dell’affidabilità oltre tale numero.\nValutare i Benefici dell’Uso di un Numero Pari o Dispari di Opzioni: Negli item Likert, generalmente si raccomanda l’uso di un numero dispari di scelte con l’opzione centrale come “Neutrale” o “Indeciso”. Questo non è universalmente accettato poiché alcuni autori sostengono l’uso di un numero pari di scelte senza opzione neutra, basandosi sul fatto che alcuni rispondenti tendono a utilizzare eccessivamente la scelta neutra se disponibile. Ciò può risultare in una ridotta varianza e affidabilità. L’eliminazione dell’opzione neutra potrebbe frustrare alcuni rispondenti che potrebbero non completare gli item quando non hanno un’opinione forte. I dati mancanti possono essere un problema significativo in questi casi. La nostra raccomandazione è di usare un numero dispari di opzioni con un’opzione neutra. Con le scale di valutazione della frequenza, questo è meno importante poiché non è necessaria un’opzione “neutrale”.\nEtichettare Chiaramente Ciascuna delle Opzioni nelle Scale di Valutazione e negli Item Likert: Per esempio, fornire etichette per ciascuna delle opzioni di risposta per risolvere eventuali incertezze.\nMinimizzare l’Uso di Determinanti Specifici: L’uso di determinanti specifici come “mai”, “sempre”, “nessuno” e “tutti” dovrebbe essere usato con cautela in quanto possono complicare il processo di risposta.\nCon i Bambini Piccoli, Considerare l’Uso di un Formato di Intervista: Per i bambini piccoli, prendere in considerazione l’utilizzo di un formato di intervista in cui gli item vengono letti al bambino. Questo può aiutare a ridurre la varianza irrilevante al costrutto introdotta dall’eliminazione dell’impatto delle abilità di lettura.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Lo sviluppo degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/01_item_development.html#sommario",
    "href": "chapters/items/01_item_development.html#sommario",
    "title": "17  Lo sviluppo degli item",
    "section": "17.8 Sommario",
    "text": "17.8 Sommario\nAll’inizio di questo capitolo, viene fatta una distinzione principale tra gli item di test in base a se sono a risposta selezionata o a risposta costruita. Successivamente, vengono considerate le loro applicazioni nei test di massima prestazione e nei test di risposta tipica. Per i test di massima prestazione, gli item a risposta selezionata includono formati a scelta multipla, vero/falso e di abbinamento, mentre gli item a risposta costruita comprendono gli item a risposta breve e i saggi. Ogni tipo di item presenta punti di forza e debolezze che sono riassunti di seguito.\nItem a Scelta Multipla: Molto popolari nei test di massima prestazione, presentano numerosi punti di forza come versatilità, valutazione oggettiva e affidabile, e campionatura efficiente del dominio di contenuto. La loro limitazione principale è che non sono efficaci per misurare tutti gli obiettivi e non sono facili da sviluppare.\nItem Vero/Falso: Possono essere valutati in modo oggettivo e affidabile e permettono di rispondere a molti item in breve tempo. Tuttavia, hanno molte debolezze, come la limitazione a obiettivi di apprendimento semplici e una forte vulnerabilità all’indovinamento.\nItem di Abbinamento: Anche questi possono essere valutati in modo oggettivo e affidabile, completati in maniera efficiente e sono relativamente facili da sviluppare. Le loro principali limitazioni includono uno scopo limitato e la possibilità di promuovere la memorizzazione meccanica.\nSaggi: Presentano una domanda o un problema a cui il candidato risponde in formato scritto. I saggi danno una notevole libertà nella formulazione delle risposte, ma sono difficili da valutare in modo affidabile e offrono una campionatura limitata del contenuto. Sono tuttavia adatti per misurare molti obiettivi complessi.\nItem a Risposta Breve: Simili ai saggi, richiedono una risposta scritta, ma con limiti più stretti. Sono adatti per misurare specifici obiettivi di apprendimento come i calcoli matematici e sono relativamente facili da scrivere.\nGli item per i test di risposta tipica si concentrano sull’autovalutazione di sentimenti, pensieri, dialoghi interni e altri comportamenti occulti. Alcuni esempi:\nItem Vero/Falso e Altri Item Dicotomici: Comuni nei test di risposta tipica, questi item si focalizzano sulle esperienze attuali del candidato.\nScale di Valutazione: Possono essere progettate sia per misure di autovalutazione sia per la valutazione di altri individui. A differenza degli item vero/falso che offrono solo due scelte, le scale di valutazione hanno tipicamente da quattro a cinque opzioni e denotano la frequenza.\nGli item Likert, simili alle scale di valutazione, si concentrano sul grado di accordo piuttosto che sulla frequenza. Sono diventati il formato più popolare per la valutazione delle attitudini. In passato, le scale cumulative come quelle di Guttman e Thurstone erano popolari, ma le scale Likert si sono rivelate più facili da sviluppare e con proprietà psicometriche equivalenti o superiori.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Lo sviluppo degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/02_item_analysis.html",
    "href": "chapters/items/02_item_analysis.html",
    "title": "18  Analisi degli item",
    "section": "",
    "text": "18.1 Introduzione\nNel processo di sviluppo di un test, esistono numerose procedure utili per valutare la qualità e le caratteristiche di misurazione degli item del test. Tuttavia, non tutte queste procedure sono adatte per tutti i tipi di test, e non tutte forniscono lo stesso livello di informazioni sulla qualità di un determinato item. Le caratteristiche della teoria classica dei test, come la difficoltà dell’item, la discriminazione dell’item e le opzioni di risposta sbagliate, sono utili, così come le caratteristiche associate alle analisi qualitative e alle tecniche della teoria della risposta agli item.\nLa sfida per tutti i sviluppatori di test è quella di valutare i risultati di queste procedure alla luce dell’uso previsto del test e prendere decisioni nella selezione degli item che supportino e massimizzino l’efficacia complessiva del test nel misurare ciò che si propone di misurare. In altre parole, è importante garantire che gli item selezionati siano coerenti con l’obiettivo del test e possano fornire una misurazione accurata di ciò che si intende misurare.\nCome evidenziato nel capitolo precedente, la bontà di un test è determinata dalla qualità dei suoi item. Fortunatamente, esistono diverse procedure quantitative di analisi degli item che sono utili per valutare la qualità e le caratteristiche di misurazione degli item individuali che compongono i test. Queste procedure sono comunemente denominate statistiche o procedure di analisi degli item. A differenza delle analisi di affidabilità e validità che valutano le caratteristiche di misurazione di un test nel suo insieme, le procedure di analisi degli item esaminano gli item individualmente, non l’intero test. Le statistiche di analisi degli item sono utili per aiutare gli sviluppatori di test a decidere quali item mantenere, quali modificare e quali eliminare.\nLa affidabilità dei punteggi di un test e la validità dell’interpretazione dei punteggi del test dipendono dalla qualità degli item presenti nel test. Migliorando la qualità degli item individuali, si migliorerà la qualità complessiva del test. Quando si discute di affidabilità, si è notato che uno dei modi più semplici per aumentare l’affidabilità dei punteggi del test è aumentare il numero di item che contribuiscono a tali punteggi. Questa affermazione è generalmente vera ed è basata sull’assunzione che allungando un test si aggiungano item della stessa qualità degli item esistenti. Se si utilizza l’analisi degli item per eliminare gli item di scarsa qualità e migliorare gli altri, è effettivamente possibile ottenere un test più breve rispetto alla versione originale, ma che produce punteggi più affidabili e risultati con interpretazioni più valide.\nInizieremo la nostra discussione descrivendo le principali procedure quantitative di analisi degli item, tra cui la Difficoltà dell’Item, la Discriminazione dell’Item e l’Analisi delle Opzioni di Risposta. Tuttavia, è importante notare che diversi tipi di item e diversi tipi di test richiedono diverse procedure di analisi degli item. Gli item che vengono valutati in modo dicotomico (cioè giusto o sbagliato) sono gestiti diversamente rispetto agli item valutati su una scala continua (ad esempio, un saggio che può ricevere punteggi da 0 a 10). I test progettati per massimizzare la variabilità dei punteggi (ad esempio, i test con riferimento alla norma) sono gestiti diversamente rispetto ai test di padronanza (cioè valutati come superato o non superato). Mentre discutiamo delle diverse procedure di analisi degli item, specificheremo quali procedure sono più appropriate per quali tipi di item e test.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Analisi degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/02_item_analysis.html#indice-di-difficoltà-dellitem",
    "href": "chapters/items/02_item_analysis.html#indice-di-difficoltà-dellitem",
    "title": "18  Analisi degli item",
    "section": "18.2 Indice di Difficoltà dell’Item",
    "text": "18.2 Indice di Difficoltà dell’Item\nL’Indice di Difficoltà dell’Item, indicato frequentemente con la sigla “p”, rappresenta un parametro fondamentale nella valutazione degli item nei test di massima performance o competenza. Esso è definito come la frazione o la percentuale di candidati che elicitano una risposta corretta all’item in questione. Matematicamente, è espresso dalla formula:\n\\[ p = \\frac{\\text{Numero di Candidati con Risposta Corretta}}{\\text{Numero Totale di Candidati}} \\]\nPer esemplificare, in un contesto di 30 studenti, qualora 20 studenti forniscano una risposta esatta, l’indice si calcola come:\n\\[ p = \\frac{20}{30} = 0.66 \\]\nQuesto indice varia nell’intervallo [0.0, 1.0], dove valori prossimi a 1.0 indicano un’alta facilità dell’item, e viceversa valori prossimi a 0.0 denotano un’alta difficoltà. Un indice pari a 1.0 o 0.0 non contribuisce significativamente alla discriminazione tra i candidati, benché talvolta possano essere impiegati per scopi motivazionali all’inizio di un test.\n\n18.2.1 Efficienza Temporale e Livello di Difficoltà Ottimale\nLa selezione degli item in base al loro livello di difficoltà deve tenere conto anche dell’efficienza temporale. Spesso, item estremamente facili o difficili non aggiungono valore significativo alla misura del test e possono risultare in una gestione subottimale del tempo disponibile.\nIdealmente, un indice di difficoltà medio di 0.50, dove la metà dei candidati risponde correttamente e l’altra metà no, massimizza la variabilità e l’affidabilità del test. Tuttavia, questa uniformità non è sempre desiderabile o fattibile, a causa delle interrelazioni tra gli item e delle specifiche esigenze di misurazione.\n\n\n18.2.2 Influenza del Guessing e Tipologie di Test\nIl livello di difficoltà ottimale varia a seconda della tipologia del test e della possibilità di indovinare le risposte. Nei test con item a risposta costruita, dove l’indovinare è meno rilevante, un indice medio di 0.50 è generalmente preferibile. Nei test a risposta selezionata, come quelli a scelta multipla, si considera un valore medio di “p” più elevato per bilanciare l’effetto dell’indovinamento.\n\n\n18.2.3 Contesti Specifici di Valutazione\nIn test con riferimento ai criteri o test di padronanza, la valutazione della difficoltà segue logiche differenti. Per esempio, in test di padronanza, è comune che la maggior parte degli item abbia un indice “p” elevato, per riflettere l’aspettativa che la maggioranza dei candidati superi il test. In contesti di selezione o per test destinati a individuare candidati altamente performanti, si potrebbero preferire item con un indice di difficoltà significativamente diverso.\n\n\n18.2.4 Variazioni in Funzione del Campione\nÈ cruciale notare che l’indice di difficoltà è intrinsecamente legato alle caratteristiche del campione considerato. Ad esempio, lo stesso item può presentare indici di difficoltà differenti se somministrato a gruppi con livelli di competenza diversi.\n\n\n18.2.5 Statistica della Percentuale di Approvazione\nPer i test di risposta tipica, si utilizza un indice analogo all’indice di difficoltà, noto come statistica della percentuale di approvazione. Questa statistica indica la percentuale di candidati che rispondono in un determinato modo a un item, e varia a seconda del campione e del contesto.\n\n\n18.2.6 Applicazioni nell’Analisi e Sviluppo di Test\nL’analisi dell’indice di difficoltà e di altre statistiche relative agli item è cruciale per gli sviluppatori di test nella selezione, modifica, o eliminazione degli item durante lo sviluppo o la revisione dei test. Tale analisi è complementare ad altre procedure di analisi degli item, come l’indice di discriminazione dell’item, che sarà discusso successivamente.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Analisi degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/02_item_analysis.html#discriminazione-dellitem",
    "href": "chapters/items/02_item_analysis.html#discriminazione-dellitem",
    "title": "18  Analisi degli item",
    "section": "18.3 Discriminazione dell’item",
    "text": "18.3 Discriminazione dell’item\nL’item discrimination, in italiano “discriminazione dell’item,” si riferisce a quanto bene un item può discriminare o differenziare tra i partecipanti al test che differiscono sulla costrutto misurato dal test. Ad esempio, se un test è progettato per misurare l’abilità di lettura, la discriminazione dell’item riflette la capacità di un item di distinguere tra individui con buone capacità di lettura e quelli con scarse capacità di lettura. A differenza del livello di difficoltà dell’item, per il quale esiste un accordo su come calcolare la statistica, nel corso degli anni sono stati sviluppati oltre 50 diversi indici di discriminazione dell’item (Anastasi & Urbina, 1997). Fortunatamente, la maggior parte di questi indici produce risultati simili (Engelhart, 1965; Oosterhof, 1976). Ci concentreremo sulla discussione di due dei più popolari indici di discriminazione dell’item: l’indice di discriminazione e le correlazioni item-totali.\n\n18.3.1 Indice di Discriminazione\nUn metodo popolare per calcolare un indice di discriminazione dell’item si basa sulla differenza nelle prestazioni tra due gruppi. Sebbene ci siano modi diversi per selezionare i due gruppi, vengono tipicamente definiti in termini di prestazioni totali al test. Un approccio comune è selezionare il 27% migliore e il 27% peggiore dei partecipanti in termini di prestazioni complessive al test, escludendo il 46% centrale (Kelley, 1939). Alcuni esperti di valutazione hanno suggerito di utilizzare il 25% migliore e il 25% peggiore, alcuni il 33% migliore e il 33% peggiore, e alcuni la metà superiore e inferiore. In pratica, tutti questi sono probabilmente accettabili, ma la nostra raccomandazione è di utilizzare il tradizionale 27% superiore e inferiore. La difficoltà dell’item è calcolata separatamente per ciascun gruppo, e questi sono denominati pT e pB (“T” per il gruppo superiore, “B” per il gruppo inferiore). La differenza tra pT e pB è l’indice di discriminazione, designato come D. Viene calcolato con la seguente formula (es. Johnson, 1951):\n\\[\nD = p_T - p_B\n\\]\ndove: - D = Indice di Discriminazione - pT = Proporzione dei partecipanti nel Gruppo Superiore che Risponde Correttamente all’Item - pB = Proporzione dei partecipanti nel Gruppo Inferiore che Risponde Correttamente all’Item\nPer illustrare la logica di questo indice, consideriamo un test di rendimento progettato per misurare il rendimento accademico in un’area specifica. Se l’item sta discriminando tra partecipanti che conoscono il materiale e quelli che non lo conoscono, allora i partecipanti più informati (cioè, quelli nel “gruppo superiore”) dovrebbero rispondere correttamente all’item più spesso dei partecipanti meno informati (cioè, quelli nel “gruppo inferiore”). Ad esempio, se pT = 0.80 (indicando che l’80% dei partecipanti nel gruppo superiore ha risposto correttamente all’item) e pB = 0.30 (indicando che il 30% dei partecipanti nel gruppo inferiore ha risposto correttamente all’item), allora:\n\\[\nD = 0.80 - 0.30 = 0.50.\n\\]\nHopkins (1998) ha fornito linee guida per valutare gli item in termini dei loro valori di D. Secondo queste linee guida, i valori di D superiori a 0.40 sono considerati eccellenti, tra 0.30 e 0.39 sono buoni, tra 0.11 e 0.29 sono accettabili e tra 0.00 e 0.10 sono scadenti. Gli item con valori di D negativi probabilmente sono stati formulati in modo errato o presentano altri problemi gravi. Altri esperti di valutazione hanno fornito linee guida diverse, alcune più rigorose e altre più indulgenti. Come regola generale, si suggerisce che gli item con valori di D superiori a 0.30 siano accettabili (quanto più alti, tanto meglio), mentre gli item con valori di D inferiori a 0.30 dovrebbero essere attentamente valutati, e eventualmente rivisti o eliminati.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Analisi degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/02_item_analysis.html#test-di-padronanza",
    "href": "chapters/items/02_item_analysis.html#test-di-padronanza",
    "title": "18  Analisi degli item",
    "section": "18.4 Test di Padronanza",
    "text": "18.4 Test di Padronanza\nNei test di padronanza, gli item tendono ad avere indici di difficoltà più elevati rispetto a quelli di test normativi, implicando che gli item sono generalmente più facili. Questa caratteristica deriva dall’assunzione che la maggior parte dei partecipanti otterrà risultati positivi nei test di padronanza. Di conseguenza, è frequente che gli item in questi test abbiano una proporzione elevata di risposte corrette (valori medi di p), talvolta raggiungendo il 90%.\nQuesta tendenza richiede un adattamento nell’interpretazione degli indici di difficoltà degli item. La necessità di adattamento si estende anche all’interpretazione degli indici di discriminazione. Nei test di padronanza, dove è comune che sia partecipanti con punteggi alti sia quelli con punteggi bassi otteniano valori elevati di p, gli indici tradizionali di discriminazione potrebbero non riflettere accuratamente le capacità di misurazione di un item.\nPer affrontare questa sfida, sono stati proposti diversi metodi per calcolare la discriminazione degli item nei test di padronanza. Aiken (2000), ad esempio, suggerisce un metodo che considera la difficoltà degli item basandosi sui partecipanti che hanno raggiunto (o non raggiunto) il punteggio di padronanza. La formula proposta è:\n\\[\nD = p_{mastery} - p_{non-mastery}\n\\]\ndove \\(p_{mastery}\\) rappresenta la proporzione di partecipanti che hanno raggiunto la padronanza e hanno risposto correttamente all’item, mentre \\(p_{non-mastery}\\) indica la proporzione di partecipanti che non hanno raggiunto la padronanza e hanno risposto correttamente.\nUn altro metodo per valutare la discriminazione degli item è l’uso della correlazione item-totale. Questo approccio correla le prestazioni su un singolo item con il punteggio totale del test, utilizzando di solito la correlazione punto-biserial. Un’alta correlazione item-totale indica che l’item misura lo stesso costrutto dell’intero test e discrimina efficacemente tra individui con alta e bassa competenza nel costrutto misurato. È preferibile calcolare questa correlazione escludendo l’item in esame dal punteggio totale del test, per evitare di “contaminare” o gonfiare la correlazione. Attualmente, la correlazione item-totale è il metodo più utilizzato per esaminare la discriminazione degli item nei test.\nQuesti approcci offrono strumenti importanti per comprendere e migliorare la qualità dei test di padronanza, garantendo che siano sia accessibili sia capaci di distinguere accuratamente tra diversi livelli di competenza.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Analisi degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/02_item_analysis.html#discriminazione-degli-item-nei-test-con-risposte-tipiche",
    "href": "chapters/items/02_item_analysis.html#discriminazione-degli-item-nei-test-con-risposte-tipiche",
    "title": "18  Analisi degli item",
    "section": "18.5 Discriminazione degli Item nei Test con Risposte Tipiche",
    "text": "18.5 Discriminazione degli Item nei Test con Risposte Tipiche\nL’analisi degli item nei test con risposte tipiche riguarda l’analisi di item quelli finalizzati alla misurazione di tendenze comportamentali. Un esempio pertinente è un item di un test progettato per valutare la propensione alla ricerca di sensazioni, basato su affermazioni a cui si risponde con “Vero” o “Falso”. In questo contesto, le risposte “Vero” (valutate con “1”) indicano una tendenza verso comportamenti di ricerca di sensazioni, mentre le risposte “Falso” (valutate con “0”) denotano una propensione ad evitarli. Pertanto, punteggi elevati in tali test suggeriscono un gusto per comportamenti ad alta sensazione, mentre punteggi bassi indicano una tendenza all’evitamento.\nLa correlazione tra gli item e il punteggio totale del test può essere usata per l’identificazione degli item più efficaci nel discriminare tra individui con diverse propensioni alla ricerca di sensazioni. Gli item con elevata correlazione risultano particolarmente utili per distinguere soggetti con alta o bassa propensione a tali comportamenti.\nL’interpretazione degli indici di difficoltà e discriminazione diventa però più complessa nei cosiddetti “test di velocità”. Questi test sono caratterizzati da item generalmente facili, ma con limiti di tempo stringenti che impediscono ai candidati di completarli tutti. La prestazione nei test di velocità è quindi influenzata principalmente dalla rapidità di risposta, contrariamente ai test di potenza, dove il tempo non è un fattore limitante e la difficoltà degli item varia significativamente.\nNei test di velocità, la difficoltà e la capacità discriminativa degli item tendono a riflettere la loro posizione all’interno del test piuttosto che la loro intrinseca difficoltà o capacità di discriminare tra candidati. Gli item situati verso la fine del test tendono ad essere completati da un numero minore di candidati a causa dei limiti temporali, non perché siano effettivamente più difficili. Allo stesso modo, l’indice di discriminazione degli item posti verso la fine potrebbe essere esagerato, in quanto solo i candidati più capaci riescono a raggiungerli e completarli.\nNonostante siano state proposte varie metodologie per controbilanciare questi fattori, ogni approccio presenta delle limitazioni e non ha ancora guadagnato un’ampia accettazione nella comunità scientifica. Pertanto, è essenziale essere consapevoli di queste complessità e tenerle in considerazione nell’interpretazione delle analisi degli item nei test di velocità.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Analisi degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/02_item_analysis.html#analisi-dei-distrattori",
    "href": "chapters/items/02_item_analysis.html#analisi-dei-distrattori",
    "title": "18  Analisi degli item",
    "section": "18.6 Analisi dei Distrattori",
    "text": "18.6 Analisi dei Distrattori\nÈ cruciale valutare l’efficacia dei distrattori nell’analisi quantitativa dei test a scelta multipla. I distrattori sono le opzioni errate fornite nelle domande, progettate per deviare l’attenzione degli esaminandi meno preparati. Questa analisi esamina la frequenza con cui esaminandi con punteggi alti e bassi scelgono ciascun distrattore. Un aspetto fondamentale è analizzare ogni distrattore ponendo due questioni centrali.\nPrima, il distrattore è effettivamente un’opzione che confonde alcuni esaminandi? Se nessuno sceglie il distrattore, questo indica che non sta adempiendo alla sua funzione. Un distrattore adeguato dovrebbe essere selezionato da alcuni esaminandi. Al contrario, se un distrattore è palesemente errato e nessuno lo sceglie, risulta inefficace e necessita di revisione o sostituzione.\nSeconda, quanto bene il distrattore discrimina tra i diversi gruppi di esaminandi? I distrattori efficaci tendono ad attrarre più candidati con punteggi bassi rispetto a quelli con punteggi alti. Analizzando la risposta corretta, ci aspettiamo che più esaminandi con punteggi alti la scelgano rispetto a quelli con punteggi bassi, indicando una discriminazione positiva. Per i distrattori, l’effetto dovrebbe essere l’opposto: più esaminandi con punteggi bassi dovrebbero scegliere i distrattori rispetto a quelli con punteggi alti, dimostrando così una discriminazione negativa.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Analisi degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/02_item_analysis.html#le-curve-caratteristiche-degli-item-nella-irt",
    "href": "chapters/items/02_item_analysis.html#le-curve-caratteristiche-degli-item-nella-irt",
    "title": "18  Analisi degli item",
    "section": "18.7 Le Curve Caratteristiche Degli Item nella IRT",
    "text": "18.7 Le Curve Caratteristiche Degli Item nella IRT\nUn argomento importante nella discussione dell’analisi degli item riguarda la nozione di curva caratteristica dell’item, così com’è stata formultata dalla IRT. Questo argomento verrà trattato in maniera approfondita in un capitolo successivo.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Analisi degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/02_item_analysis.html#considerazioni-conclusive",
    "href": "chapters/items/02_item_analysis.html#considerazioni-conclusive",
    "title": "18  Analisi degli item",
    "section": "18.8 Considerazioni Conclusive",
    "text": "18.8 Considerazioni Conclusive\nQuesto capitolo fornisce una panoramica approfondita delle procedure di analisi degli item, strumenti fondamentali per gli sviluppatori di test nel decidere quali item mantenere, modificare o eliminare. Si esplorano diverse procedure, tra cui:\n\nLivello di Difficoltà dell’Item: Definito come la percentuale di esaminandi che rispondono correttamente a un item, l’indice di difficoltà (p) varia da 0.0 a 1.0. Gli item più facili presentano valori decimali maggiori, mentre quelli difficili valori minori. Il livello ottimale di difficoltà, per massimizzare la variabilità tra gli esaminandi, è 0.50. Tuttavia, a seconda della situazione, possono essere preferiti valori diversi, generalmente tra 0.20 e 0.80.\nDiscriminazione dell’Item: Questo concetto si riferisce alla capacità di un item di distinguere tra esaminandi che variano rispetto al costrutto del test. Discutiamo l’indice di discriminazione dell’item (D), considerando accettabili valori di D pari o superiori a 0.30, mentre quelli inferiori a 0.30 potrebbero richiedere revisione o eliminazione. Esploriamo anche la correlazione item-totalità come metodo alternativo per esaminare la discriminazione.\nAnalisi dei Distrattori: Questa procedura valuta l’efficacia dei distrattori nelle domande a scelta multipla, ponendo due domande principali: un distrattore funzionale dovrebbe attirare alcuni esaminandi e, per la discriminazione, dovrebbe attrarre più esaminandi nel gruppo con punteggi bassi rispetto a quelli con punteggi alti.\nProcedure Qualitative: Oltre alle procedure quantitative, si suggerisce l’utilizzo di metodi qualitativi per migliorare i test. Tra questi, la revisione accurata del test, la valutazione da parte di colleghi fidati e il feedback degli esaminandi sulla chiarezza e problemi degli item.\nCurve Caratteristiche dell’Item e Teoria della Risposta all’Item (IRT): Le curve caratteristiche dell’item (ICC) rappresentano graficamente la relazione tra l’abilità e la probabilità di risposta corretta. L’IRT, componente centrale delle ICC, assume che le risposte agli item siano determinate da tratti latenti e ha influenzato significativamente lo sviluppo di test moderni, inclusi i test adattivi computerizzati.\n\nL’utilizzo di queste procedure durante lo sviluppo del test migliora l’affidabilità dei punteggi e la validità delle loro interpretazioni, essendo entrambe dipendenti dalla qualità degli item. La rimozione degli item scarsi e il miglioramento degli altri possono anche portare a un test più corto ed efficiente, con punteggi più affidabili e interpretazioni più valide.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Analisi degli item</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html",
    "href": "chapters/path_analysis/01_path_analysis.html",
    "title": "19  Analisi dei percorsi",
    "section": "",
    "text": "19.1 Introduzione\nLe visualizzazioni rivestono un ruolo fondamentale nel comunicare in modo chiaro e sintetico le relazioni tra variabili. Questo è particolarmente evidente quando si opera con modelli di equazioni strutturali (SEM) che delineano una rete di interconnessioni tra variabili sia osservabili che latenti. In tali contesti, i ricercatori frequentemente si avvalgono di strumenti grafici per agevolare la specificazione e l’esplicitazione del modello, oltre che per presentare in maniera comprensibile i risultati ottenuti.\nL’analisi del percorso, o path analysis, è una tecnica statistica multivariata utilizzata nell’ambito della ricerca quantitativa per esaminare e descrivere le relazioni causali tra un insieme di variabili. Questo metodo si avvale di modelli grafici, noti come diagrammi di percorso, che rappresentano le relazioni ipotizzate tra le variabili, illustrando graficamente le relazioni dirette, indirette e reciproche tra di esse.\nIl fulcro dell’analisi del percorso è la decomposizione e la quantificazione delle relazioni tra le variabili, permettendo agli analisti di distinguere tra effetti diretti, indiretti e totali. Gli effetti diretti corrispondono all’influenza immediata che una variabile esercita su un’altra, mentre gli effetti indiretti rappresentano l’impatto mediato attraverso una o più variabili intermedie. L’effetto totale è la somma degli effetti diretti e indiretti.\nSewall Wright, un genetista che operava presso il Dipartimento dell’Agricoltura degli Stati Uniti, fu il precursore nello sviluppo dei diagrammi di percorso per descrivere i modelli di equazioni strutturali già negli anni ’20 del secolo scorso. Questa sua innovazione ha permesso di ottenere una rappresentazione visiva delle connessioni tra variabili, aprendo la strada all’analisi del percorso.\nCon il trascorrere del tempo, questa metodologia è stata adottata con successo come uno strumento efficace per discriminare gli effetti diretti da quelli indiretti nelle relazioni tra variabili. Inoltre, essa si è dimostrata di grande utilità nel valutare la solidità e la validità delle relazioni causali ipotizzate all’interno dei modelli di equazioni strutturali.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#path-diagram",
    "href": "chapters/path_analysis/01_path_analysis.html#path-diagram",
    "title": "19  Analisi dei percorsi",
    "section": "19.2 Path diagram",
    "text": "19.2 Path diagram\nIl diagramma del percorso, noto anche come “path diagram,” costituisce uno strumento per la rappresentazione grafica delle relazioni tra variabili all’interno di un modello. All’interno di questo diagramma, le variabili latenti o non osservate sono rappresentate mediante cerchi o ellissi, mentre le variabili osservate sono rappresentate da quadrati o rettangoli.\nAll’interno del path diagram, è possibile individuare due categorie di variabili: quelle che subiscono influenze da parte di altre variabili nel sistema e quelle che svolgono il ruolo di generatori di effetti. Nello specifico, le variabili esogene costituiscono elementi esterni al sistema in esame, operando in qualità di variabili indipendenti che generano effetti causalmente. Al contrario, le variabili endogene possono agire sia come risultati di altre variabili che come cause per ulteriori variabili, oppure possono essere strettamente variabili dipendenti. Le origini causali delle variabili endogene trovano collocazione all’interno del path diagram, mentre quelle delle variabili esogene si trovano esternamente al diagramma. Tale distinzione presenta affinità con la distinzione tra variabili indipendenti e dipendenti all’interno dei modelli lineari.\nUn diagramma di percorso è costituito dai seguenti simboli grafici.\n\nVariabili osservate (indicatori) rappresentate con quadrati o rettangoli.\nProxy per variabili latenti, come fattori comuni con indicatori multipli, rappresentate con cerchi o ellissi.\n\nIl diagramma di percorso mette in evidenza le interazioni tra le variabili d’interesse, sottolineando i legami causali o associativi che le connettono. Le frecce unidirezionali (ad esempio,\\(\\rightarrow\\)) illustrano relazioni causali: una variabile subisce influenza da un’altra variabile collegata attraverso una freccia. Invece, le frecce curve bidirezionali denotano relazioni associative, senza implicare una causalità diretta tra le variabili, ovvero covarianze (nella soluzione non standardizzata) o correlazioni (nella soluzione standardizzata).\nL’assenza di una freccia tra due variabili nel diagramma suggerisce l’assenza di correlazione tra di esse. Nel caso della {numref}path_01-fig, si illustrano le relazioni tra nove variabili osservate e tre variabili latenti mediante il path diagram.\n\n\n\n\n\n\nFigura 19.1: Diagramma di percorso per un modello a tre fattori comuni.\n\n\n\nUn triangolo contenente il numero 1 simboleggia la media di una variabile. Una freccia curva bidirezionale che si collega a una singola variabile rappresenta la varianza residua della variabile, ovvero la quota di varianza non spiegata dalle relazioni causali illustrate nel diagramma di percorso.\n\n19.2.1 Parametri nei Modelli di Equazioni Strutturali\nI parametri nei modelli di equazioni strutturali possono essere categorizzati come segue, quando le medie non sono oggetto di analisi:\n\nVarianze e Covarianze delle Variabili Esogene:\n\nQuesti parametri rappresentano la variabilità intrinseca delle variabili esogene (quelle non influenzate da altre nel modello) e le relazioni reciproche tra di esse.\n\nEffetti Diretti sulle Variabili Endogene da Altre Variabili:\n\nQuesti parametri descrivono come le variabili endogene sono influenzate direttamente da altre variabili nel modello.\n\n\nIn termini di specificazione, un parametro nel modello può essere classificato come libero, fisso o vincolato:\n\nParametro Libero:\n\nQuesto tipo di parametro è stimato dal software statistico utilizzando i dati a disposizione.\n\nParametro Fisso:\n\nUn parametro fisso è definito per essere uguale a una costante specificata a priori. In questo caso, il software accetta il valore costante come stima, indipendentemente dai dati. Ad esempio, l’ipotesi che la variabile X non abbia effetti diretti su Y corrisponde alla specifica che il coefficiente per il percorso da X a Y sia fissato a zero.\n\nParametro Vincolato:\n\nIn questo caso, il parametro segue certe restrizioni imposte nell’analisi, che possono essere basate su teorie o ipotesi precedenti. Ad esempio, l’analista può assumere che due paraemetri siano uguali.\n\n\nLa traduzione e la presentazione sistematica del testo originale, usando un linguaggio statistico tecnico, sono le seguenti:",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#gradi-di-libertà-del-modello-in-modelli-parametrici",
    "href": "chapters/path_analysis/01_path_analysis.html#gradi-di-libertà-del-modello-in-modelli-parametrici",
    "title": "19  Analisi dei percorsi",
    "section": "19.3 Gradi di Libertà del Modello in Modelli Parametrici",
    "text": "19.3 Gradi di Libertà del Modello in Modelli Parametrici\nIn statistica, la dimensione potenziale di un modello parametrico, in termini di parametri liberi, è limitata dal numero di osservazioni. Questo numero non corrisponde alla grandezza del campione (N), ma è letteralmente il numero di entrate nella matrice di covarianza campionaria in forma triangolare inferiore. Il calcolo di tale numero segue una regola specifica:\n\nse\\(v\\)rappresenta il numero di variabili osservate nel modello, il numero di osservazioni è dato da\n\n\\[\\frac{v(v + 1)}{2}\\]\nquando le medie non sono analizzate.\nAd esempio, se \\(v = 5\\), ovvero ci sono 5 variabili osservate nel modello, il numero di osservazioni è \\(\\frac{5 \\times 6}{2} = 15\\). Questo conteggio (15) corrisponde al numero totale di varianze (5) e covarianze uniche (10) sotto la diagonale principale nella matrice dei dati. Con \\(v = 5\\), il massimo numero di parametri liberi stimabili è 15. Un modello più semplice può stimare meno parametri, ma non più di 15. Il numero di osservazioni è indipendente dalla dimensione del campione. Se cinque variabili sono misurate su 100 o 1000 casi, il numero di osservazioni rimane 15. L’aggiunta di casi non aumenta il numero di osservazioni; solo l’aggiunta di variabili osservate può farlo.\nLa differenza tra il numero di osservazioni e il numero di parametri liberi determina i gradi di libertà del modello (\\(df_M\\)), calcolati come:\n\\[df_M = p - q,\\]\ndove\\(p\\)è il numero di osservazioni e \\(q\\)è il numero di parametri liberi. Un requisito generale per l’identificazione in SEM è che\\(df_M \\geq 0\\). Questo perché un modello con più parametri liberi rispetto alle osservazioni disponibili (\\(dfM &lt; 0\\)) non è analizzabile empiricamente, dato che esistono infiniti insiemi di stime. Un modello con gradi di libertà negativi, se tentato di essere stimato, verrebbe probabilmente interrotto da un programma SEM con messaggi di errore. In alternativa, un modello parametrico con \\(dfM &lt; 0\\) deve essere ridefinito, ad esempio riducendo il numero di parametri liberi imponendo vincoli o fissando un parametro precedentemente libero a una costante.\nI modelli strutturali identificati con zero gradi di libertà (\\(df_M = 0\\)) non solo si adattano perfettamente ai dati in un campione specifico, ma si adattano anche perfettamente a qualsiasi campione arbitrario per le stesse variabili. Al contrario, i modelli con gradi di libertà positivi non hanno generalmente un adattamento perfetto, poiché\\(df_M &gt; 0\\)consente la possibilità di discrepanze tra modello e dati. Raykov e Marcoulides (2006) hanno descritto ogni grado di libertà come una dimensione lungo la quale un modello può potenzialmente essere respinto. Pertanto, i modelli ritenuti validi con maggiori gradi di libertà hanno superato una maggiore probabilità di rifiuto. Questa idea sottolinea il principio di parsimonia: dati due modelli con adattamenti simili ai dati, si preferisce il modello più semplice, assumendo che sia teoricamente plausibile.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#raffigurazione-della-varianza-residua-nelle-variabili-endogene",
    "href": "chapters/path_analysis/01_path_analysis.html#raffigurazione-della-varianza-residua-nelle-variabili-endogene",
    "title": "19  Analisi dei percorsi",
    "section": "19.4 Raffigurazione della Varianza Residua nelle Variabili Endogene",
    "text": "19.4 Raffigurazione della Varianza Residua nelle Variabili Endogene\nLa Figura 19.2 mostra la relazione tra due variabili osservabili. L’effetto totale presunto di X su Y è illustrato tramite un percorso diretto, rappresentando l’effetto causale lineare di X su Y. La varianza di X, una variabile esogena, è un parametro libero e viene rappresentata nella figura con il simbolo RAM che indica una varianza (indicata da una freccia curva bidirezionale). Al contrario, la varianza di Y, una variabile endogena, non è libera di variare; invece, è associata a una variabile latente D, il termine di disturbo o errore, che rappresenta la variazione in Y non spiegata da X.\n\n\n\n\n\n\nFigura 19.2: Diagramma per una rappresentazione contratta nel modello completo di azione reticolare McArdle-McDonald (RAM) con simbolismo grafico (a) rispetto a una versione più compatta (b). (Figura adattata da Kline (2023)`)\n\n\n\nIl numero (1) vicino al percorso nella Figura 19.2 (a) è una costante di scala che assegna una metrica al termine di disturbo. Questa specificazione è essenziale perché la varianza del termine di disturbo è latente e le variabili latenti richiedono che un fattore di scala sia fissato per la loro stima. Questa costante di scala è nota anche come il vincolo di identificazione del carico unitario (unit loading identification constraint, ULI). Il valore “1” comunica al software di suddividere la varianza totale (osservata) di Y in due componenti distinte (ortogonali): la varianza spiegata da X e la varianza non spiegata (o varianza del disturbo,\\(var_D\\)).\nLa rappresentazione nella Figura 19.2 (b) fornisce le stesse informazioni in modo più sintetico. Alternativamente, nella Figura 19.2 (a) si potrebbe rappresentare la varianza residua di Y con una freccia curva bidirezionale, al posto di utilizzare il termine di disturbo D identificato dal vincolo di identificazione del carico unitario. Il valore numerico associato a questa freccia curva bidirezionale sarebbe lo stesso di quello che si ottiene con la rappresentazione della variabile latente di disturbo:\\(1 \\times var_D \\times 1\\).\nUn’altra rappresentazione equivalente assegnerebbe il valore 1 a \\(var_D\\) e attribuirebbe alla freccia causale da D a Y il valore \\(\\sqrt{var_D}\\). Il risultato finale sarebbe identico, in quanto anche in questo caso la varianza residua di Y sarebbe rappresentata come\\(\\sqrt{var_D} \\times 1 \\times \\sqrt{var_D}\\).\nProseguendo la discussione sulla varianza del disturbo, possiamo identificare quattro fonti principali che contribuiscono a questa varianza:\n\nVariazione Sistematica da Cause Non Misure: Questa varianza origina da fattori non misurati che influenzano sistematicamente l’esito della variabile di interesse. Si tratta di influenze esterne o variabili nascoste che hanno un impatto significativo ma non sono incluse nel modello.\nVariazione Casuale Intrinseca: Questo tipo di varianza è una caratteristica fondamentale di quasi tutti i sistemi o variabili individuali. Rappresenta la variabilità naturale che esiste indipendentemente dalle misure o dagli effetti che si tenta di analizzare.\nErrore di Misurazione Casuale: Questa varianza è legata agli errori che si verificano durante il processo di misurazione. Includono gli errori casuali che possono essere stimati attraverso analisi di affidabilità, come l’accuratezza e la precisione degli strumenti di misurazione utilizzati.\nMancata Specificazione della Corretta Forma Funzionale dell’Effetto Causale: Questa varianza emerge quando la forma funzionale dell’effetto causale nel modello non corrisponde alla vera natura della relazione. Un esempio comune è modellare una relazione come lineare quando in realtà è non lineare, portando a una rappresentazione imprecisa del fenomeno sotto indagine.\n\nNel pannello (a) della Figura 19.2, il percorso da D a Y rappresenta l’effetto diretto di tutte queste cause omesse, oltre agli errori, sulla variabile endogena Y. In sostanza, questo percorso simboleggia l’insieme di tutte le influenze non incluse nel modello che possono impattare su Y. È importante notare che, mentre queste fonti di varianza del disturbo possono essere teoricamente distinte, nella pratica possono sovrapporsi e interagire tra loro.\nProseguendo il discorso sulla rappresentazione della varianza residua nelle variabili endogene, è importante notare che diversi software SEM trattano in modi differenti i termini di errore nei modelli di equazioni strutturali. Ad esempio, nella sintassi del software lavaan, il comando:\nY ~ X\ndirettamente istruisce il software a regredire la variabile Y su X e a gestire automaticamente il termine di disturbo, come rappresentato nel pannello (a) della {numref}kline_7_2_fig. Questo comando, oltre a definire l’effetto di X su Y, stabilisce anche che le varianze di X e il termine di disturbo di Y sono parametri liberi da stimare.\nDa queste considerazioni emergono due requisiti fondamentali per l’identificazione di un modello a percorsi:\n\nI gradi di libertà del modello (\\(df_M\\)) devono essere maggiori o uguali a zero.\nOgni variabile latente, inclusi i termini di errore, deve avere una scala definita (una metrica assegnata).\n\nIl conteggio dei parametri liberi è una componente cruciale nel calcolo dei\\(df_M\\). L’inclusione esplicita delle costanti di scala nei diagrammi serve come promemoria per i ricercatori sulla necessità di assegnare una scala alle variabili latenti.\nIl pannello (b) della Figura 19.2 mostra una versione più sintetica del modello, utilizzando un simbolismo grafico che omette i simboli per i parametri di varianza (per X, D), la costante di scala (1) e la rappresentazione grafica del disturbo come variabile latente. Questo diagramma fornisce una visione meno dettagliata del modello, evidenziando solamente le relazioni di base, ovvero X che causa Y, e Y influenzata da un termine di disturbo.\n\n19.4.1 Considerazioni sugli Errori di Misurazione nei Modelli a Percorsi\nRiprendendo la discussione sulla Figura 19.2, possiamo delineare le seguenti ipotesi fondamentali:\n\nAffidabilità della Variabile Esogena X: Si assume che i punteggi sulla variabile esogena X siano privi di errore, ovvero perfettamente affidabili, con un coefficiente di affidabilità (\\(r_{XX}\\)) di 1.0.\nCorrettezza della Direzione Causale: La relazione causale da X a Y è assunta come correttamente specificata e caratterizzata da una stretta linearità.\nIndipendenza delle Cause Non Misurate di Y da X: Si presume che le cause non misurate (latenti) di Y non siano correlate con X, escludendo quindi l’esistenza di cause comuni non misurate che influenzano simultaneamente entrambe le variabili – ricordiamo la discusione precedente sull’errore di specificazione.\n\nIn ambito di modellazione dei percorsi, l’assunzione che le variabili esogene siano prive di errori di misurazione riflette un presupposto simile a quello adottato nelle regressioni multiple standard, dove i predittori sono considerati esenti da errori di misurazione. Questa assunzione è necessaria poiché le variabili esogene nei modelli a percorsi non includono termini di errore, rendendo impossibile incorporare l’errore casuale in tali modelli. Al contrario, nelle variabili endogene di tali modelli, la presenza di termini di errore permette di tenere conto dell’errore di misurazione.\nNel caso di una regressione bivariata, un errore di misurazione presente solo nella variabile dipendente Y influisce sul modello aumentando l’errore standard della stima di regressione, riducendo il valore di \\(R^2\\) e diminuendo il valore assoluto del coefficiente di regressione standardizzato, a causa dell’incremento dell’errore di misurazione in Y. Invece, l’errore di misurazione presente solo nella variabile predittiva X (ma non in Y) tende a introdurre un bias negativo nei coefficienti di regressione – cioè una sistematica sottostima dei veri valori dei coefficienti di regressione.\nQuando entrambe le variabili X e Y presentano errori di misurazione, la dinamica risultante è più complessa da prevedere. Se gli errori di misurazione in X e Y sono indipendenti, il risultato più comune è un bias negativo (ossia una sottostima dei coefficienti di regressione della popolazione). Tuttavia, se gli errori di misurazione sono comuni tra X e Y, la regressione potrebbe sovrastimare i coefficienti della popolazione, portando a un bias positivo. È essenziale riconoscere che l’errore di misurazione non causa sempre un bias negativo. Di conseguenza, la presenza di errori di misurazione non modellati nelle variabili esogene può significativamente distorcere i risultati, specialmente in presenza di forti correlazioni tra multiple variabili esogene. Per ridurre questi rischi, si raccomanda di valutare l’affidabilità dei punteggi associati alle variabili esogene. Questa pratica metodologica, che consiste nel verificare la precisione e la consistenza delle misure delle variabili predittive, aiuta a identificare e quantificare eventuali errori di misurazione. Un’accurata stima dell’affidabilità contribuisce a garantire l’integrità e la validità dei risultati dei modelli a percorsi, mitigando l’impatto che gli errori di misurazione possono avere sull’analisi.\n\n\n19.4.2 Direzionalità Causale e Forma Funzionale della Relazione X-Y\nL’assunzione che la relazione tra le variabili X e Y sia lineare, come presentato nella Figura 19.2, può essere esaminata attraverso l’analisi dei dati. Se si osserva che la relazione è significativamente curvilinea, si può adeguare l’analisi per attenuare il presupposto di linearità. Ciò può essere realizzato attraverso metodi come la regressione polinomiale o la regressione non parametrica, che permettono di modellare relazioni più complesse rispetto a un semplice modello lineare.\nTuttavia, la direzionalità dell’effetto causale rappresenta una sfida differente e non è direttamente testabile attraverso metodi statistici standard. Nell’ambito dei modelli SEM, le direzioni degli effetti causali sono generalmente ipotizzate piuttosto che empiricamente verificate. Questo perché è possibile costruire modelli SEM equivalenti che utilizzano le stesse variabili e hanno lo stesso numero di gradi di libertà (\\(df_M\\)), ma con direzioni inverse di alcuni effetti causali. Inoltre, entrambi i modelli, nonostante le differenze nelle direzionalità causali, mostreranno lo stesso grado di adattamento ai dati osservati.\nUn’ulteriore ragione per cui la direzionalità causale è tipicamente assunta piuttosto che testata in SEM risiede nella natura degli studi SEM stessi. La maggior parte degli studi SEM si basa su disegni trasversali, dove tutte le variabili sono misurate contemporaneamente, senza una chiara precedenza temporale. In questi contesti, l’unica base per definire la direzionalità causale è l’argomentazione teorica del ricercatore, che deve giustificare perché si presume che X influenzi Y e non viceversa, o perché non si considera una relazione di feedback o causazione reciproca tra le due variabili.\nDi conseguenza, la metodologia SEM non è intrinsecamente una tecnica per la scoperta di relazioni causali. Se un modello è corretto, SEM può essere utilizzato per stimare le direzioni, le dimensioni e la precisione degli effetti causali. Tuttavia, questo non è il modo in cui i ricercatori tipicamente impiegano le analisi SEM. Piuttosto, un modello causale viene ipotizzato e poi adattato ai dati basandosi sulle assunzioni delineate. Se queste assunzioni risultano essere errate, anche i risultati dell’analisi saranno invalidi. Questo enfatizza il punto sollevato da Pearl (2000), che sostiene che\n\nle ipotesi causali sono un prerequisito essenziale per validare qualsiasi conclusione causale (p. 136).\n\nQuesto implica la necessità di una solida base teorica e concettuale nella formulazione di modelli causali nella modellazione SEM.\n\n\n19.4.3 Confondimento nei Modelli Parametrici\nNella teoria dei modelli statistici, l’endogenità si riferisce a una situazione in cui una variabile all’interno di un modello è correlata con i termini di errore. Questo può creare problemi nella stima dei parametri del modello e può portare a conclusioni errate riguardo le relazioni causali tra le variabili.\nNel contesto del diagramma di una catena contratta della Figura 19.3, l’endogenità è visualizzata come una covarianza tra la variabile causale misurata X e il disturbo (termine di errore) di Y, indicata con un simbolo specifico. Questo simbolo mostra che c’è una relazione non spiegata tra la causa X e il disturbo associato a Y, suggerendo che X potrebbe non essere una variabile completamente indipendente, come idealmente dovrebbe essere in un modello causale chiaro.\nIl modello nella Figura 19.3 (a) non è identificabile per due ragioni principali:\n\nGradi di libertà negativi (dfM = -1): Questo indica che ci sono più parametri da stimare nel modello rispetto al numero di informazioni (osservazioni) disponibili. In sostanza, il modello sta cercando di “apprendere” troppo da troppo pochi dati, il che lo rende statisticamente non identificabile.\nPercorso di confondimento non chiuso tra X e D: Il percorso di confondimento (o back-door) tra X e D indica che c’è una relazione non controllata o non misurata tra la variabile indipendente X e il disturbo D di Y. Poiché D è trattato come una variabile latente (cioè, una variabile non osservata direttamente), questo percorso non può essere chiuso o controllato nel modello. Ciò significa che non possiamo essere sicuri se la relazione osservata tra X e Y è effettivamente causata da X o se è influenzata da altri fattori non considerati nel modello.\n\nIn sintesi, l’endogenità in questo contesto si riferisce al problema di avere una variabile indipendente (X) che non è veramente indipendente a causa della sua relazione non spiegata con il termine di errore associato alla variabile dipendente (Y), compromettendo così la chiarezza delle relazioni causali nel modello.\n\n\n\n\n\n\nFigura 19.3: Endogenità in una catena contratta (a). Identificazione del modello controllando un proxy (P) di una causa comune non misurata (b) e attraverso metodi di variabile strumentale (Z), che affrontano anche l’errore di misurazione nella variabile X (c). Tutti i diagrammi sono mostrati in simbolismo compatto. (Figura tratta da Kline (2023))\n\n\n\nL’endogenità nei modelli parametrici può essere indotta dalle seguenti condizioni:\n\nUna causa comune non misurata di X e Y (cioè, un confonditore).\nErrore di misurazione casuale in X (cioè, \\(r_{XX} &lt; 1.0\\)).\nCausalità reciproca, o X e Y si causano a vicenda (cioè, sono entrambe variabili endogene) in un ciclo di feedback.\nErrori autoregressivi, dove X è una versione ritardata di Y e gli errori persistono tra le due variabili.\nAutoregressione spaziale, che si verifica quando i punteggi di ciascun caso sono influenzati da quelli di casi vicini o adiacenti spazialmente.\n\nNel contesto dei modelli statistici, è possibile affrontare il problema dei confonditori non misurati in due modi principali: attraverso la selezione di covariate appropriate o utilizzando i metodi delle variabili strumentali. Per illustrare, la Figura 19.3 (b) propone l’uso di un proxy (P) che funge da sostituto per un confonditore non misurato che influisce su entrambe le variabili X e Y. In questo contesto, la variabile X è considerata endogena, il che significa che è influenzata dal proxy P (che a sua volta influisce anche su Y), indicando una possibile relazione di causa-effetto tra P e X.\nPer chiarire, consideriamo il seguente esempio. Immaginiamo di essere interessati a studiare l’effetto dello stress sulle prestazioni accademiche degli studenti universitari. In questo esempio, “stress” è la variabile X e “prestazioni accademiche” è la variabile Y. Tuttavia, c’è un potenziale confonditore che potrebbe influenzare sia lo stress sia le prestazioni accademiche, ma che non è stato misurato o non può essere facilmente misurato. Questo confonditore potrebbe essere, ad esempio, il “benessere psicologico generale” degli studenti.\nIn questo caso, un proxy (P) per il benessere psicologico generale potrebbe essere “l’attività fisica regolare”, che è più facilmente misurabile. La ricerca ha mostrato che l’attività fisica regolare può influenzare sia il benessere psicologico generale sia lo stress, rendendola un buon proxy per il nostro confonditore non misurato.\nNel modello, l’attività fisica (il nostro proxy P) presumibilmente influisce sia sulla variabile causale (lo stress) sia sulla variabile di esito (le prestazioni accademiche). Analizzando i dati con questo modello, possiamo cercare di isolare meglio l’effetto dello stress sulle prestazioni accademiche, controllando per l’effetto del benessere psicologico generale tramite il proxy dell’attività fisica. In questo modo, possiamo ottenere una stima più accurata dell’effetto diretto dello stress sulle prestazioni accademiche, riducendo la distorsione potenzialmente causata dal confonditore non misurato.\nI metodi delle variabili strumentali, come mostrato nella Figura 19.3 (c), sono utilizzati per affrontare sia i confonditori non misurati sia gli errori di misurazione nella variabile esogena X. Questo viene fatto sostituendo X con una variabile strumentale XZ in un modello di regressione a due stadi (2SLS). In questo approccio, qualsiasi errore di misurazione casuale in X viene trasferito alla variabile strumentale XZ, seguendo le ipotesi standard dei metodi delle variabili strumentali. È importante notare che, nel pannello (c), la variabile X è considerata endogena, sebbene non tutti i ricercatori scelgano di includere variabili strumentali nei loro diagrammi di modelli statistici. Questo approccio consente di isolare meglio l’effetto di X su Y, controllando per le influenze esterne non misurate e gli errori di misurazione.\nPer chiarire ulteriormente questi concetti, esamineremo separatamente il modello autoregressivo e l’autoregressione spaziale.\n\n19.4.3.1 Modello Autoregressivo\nUn modello autoregressivo è un tipo di modello statistico utilizzato per analizzare dati sequenziali o temporali. In un modello autoregressivo, si prevedono i valori futuri di una variabile basandosi sui suoi valori passati. Questo è particolarmente utile in studi longitudinali o in serie temporali dove si misura la stessa variabile in diversi punti nel tempo.\nNell’esempio della Figura 19.3 (a), immaginiamo che X e Y siano le stesse variabili misurate in due momenti diversi. Ad esempio, X potrebbe essere il livello di ansia di uno studente misurato all’inizio dell’anno scolastico, mentre Y potrebbe essere il livello di ansia dello stesso studente misurato alla fine dell’anno scolastico. In questo caso, stiamo cercando di prevedere i punteggi futuri di ansia (Y) basandoci sui punteggi passati (X).\nUn aspetto importante da considerare è che gli errori nelle misure ripetute (le variazioni nei punteggi che non sono spiegati dal modello) possono essere correlati. Ad esempio, se le misurazioni sono fatte in intervalli temporali ravvicinati, le circostanze o gli stati interni che hanno influenzato la prima misurazione potrebbero ancora essere presenti durante la seconda misurazione.\n\n\n19.4.3.2 Autoregressione Spaziale\nL’autoregressione spaziale, invece, si riferisce a un modello che considera le correlazioni spaziali tra dati. Questo tipo di analisi è particolarmente rilevante quando si studiano fenomeni geografici o ambientali. Ad esempio, la diffusione di una malattia in diverse località geografiche potrebbe non essere indipendente: le aree vicine geograficamente potrebbero mostrare pattern simili di diffusione della malattia a causa della loro vicinanza.\nIn quest’ultimo caso, non stiamo più parlando di misure ripetute nel tempo sulla stessa unità, ma piuttosto di misure effettuate in diverse unità in un contesto spaziale. Le variabili misurate in diverse località fisiche possono influenzarsi a vicenda, e un modello autoregressivo spaziale cerca di catturare queste interdipendenze.\n\n\n\n19.4.4 Modelli con Cause Correlate o Effetti Indiretti\nIl modello parametrico mostrato nella Figura 19.4 (a) suggerisce che la variabile Y sia influenzata da due variabili esogene correlate, X e W. Questo significa che X e W sono due fattori esterni che hanno un impatto su Y e tra loro esiste una relazione di covarianza, ovvero tendono a variare insieme in un certo modo. Tuttavia, il diagramma non spiega il motivo della relazione tra X e W, lasciando la loro interdipendenza non esaminata in termini causali.\n\n\n\n\n\n\nFigura 19.4: Modelli con cause correlate (a) e sia effetti diretti che indiretti (b). Tutti i diagrammi sono mostrati in simbolismo compatto. (Figura tratta da Kline (2023))\n\n\n\nNell’analizzare questi dati con un software, si prenderanno in considerazione gli effetti sia di X che di W, tenendo conto della loro covarianza campionaria. Ciò significa che quando si stimano gli impatti di X e W su Y, si aggiusta per il fatto che X e W sono correlate tra loro. Alcuni software, come lavaan, presuppongono automaticamente che tutte le cause esogene misurate che influenzano lo stesso risultato (in questo caso Y) siano correlate. Utilizzando il comando\nY ~ X + W\nin lavaan, si definisce il modello rappresentato nella Figura 19.4 (a), permettendo al software di stimare gli effetti di X e W tenendo conto della loro covarianza osservata. Questo comando specifica inoltre che le varianze di X, W e il disturbo associato a Y sono tutti considerati parametri liberi da stimare.\nSe, invece, si ipotizza che le variabili esogene X e W siano indipendenti, ovvero che non ci sia una covarianza tra di loro, si può usare un comando aggiuntivo in lavaan\nX ~~ 0*W\nper impostare la covarianza tra X e W a zero. Questo comando mantiene le varianze di X e W come parametri liberi, ma specifica che non c’è una relazione di covarianza diretta tra queste due variabili. In questo modo, il modello considererà X e W come influenze separate e indipendenti su Y.\nNel modello presentato nella Figura 19.4 (a), è importante notare come vengano trattate le interazioni tra le variabili causali X e W. In questo specifico caso, si presume che non ci sia alcuna interazione tra X e W; in altre parole, l’effetto di X sulla variabile di esito Y si assume essere costante a prescindere dai diversi livelli di W, e viceversa. Questa assunzione implica che l’effetto di X su Y è indipendente da W, e l’effetto di W su Y è indipendente da X.\nIn termini di modellazione, questo significa che stiamo considerando una causalità incondizionata, dove l’effetto di una causa su un esito è costante e non influenzato da altre variabili nel modello. Il modello non prevede, quindi, che l’effetto di X su Y cambi in funzione dei diversi livelli di W. Questo è in contrasto con l’ipotesi di causalità condizionale, dove gli effetti di una variabile su un’altra possono variare in base al livello di una terza variabile. In un modello di causalità condizionale, ad esempio, si potrebbe ipotizzare che l’effetto di X su Y vari a seconda dei diversi livelli di W.\nIn sintesi, la Figura 19.4 (a) delinea un modello dove le relazioni causali tra X, W e Y sono considerate fisse e non influenzate da potenziali interazioni tra X e W. Questo tipo di modellazione fornisce una visione semplificata delle relazioni causali, che potrebbe essere appropriata in determinate circostanze, ma non tiene conto di possibili dinamiche più complesse tra le variabili.\nÈ fondamentale riconoscere che le ripercussioni degli errori di misurazione in modelli che includono cause correlate sono notevolmente intricate e imprevedibili. Questa complessità deriva principalmente dalla natura del bias che può emergere a seguito di errori di misurazione. In particolare, il bias introdotto da questi errori può manifestarsi in modi diversi, assumendo una forma sia negativa che positiva. Tale variazione dipende da diversi fattori, tra cui se l’errore di misurazione è distribuito in maniera uniforme tra molteplici variabili predittive o se è presente sia nelle variabili predittive che nella variabile di esito. Un altro elemento influente è la natura delle covarianze campionarie tra tutte le variabili coinvolte nel modello.\nData questa complessità, la capacità di modellare esplicitamente gli errori di misurazione all’interno dei modelli SEM rappresenta un vantaggio significativo. Questo approccio permette una maggiore precisione nell’analisi, consentendo di tenere conto delle varie modalità in cui gli errori di misurazione possono influenzare i risultati. La modellazione esplicita degli errori di misurazione in SEM offre quindi la possibilità di ottenere stime più accurate e affidabili, mitigando il rischio di trarre conclusioni errate a causa di bias non riconosciuti o non gestiti adeguatamente.\nNella Figura 19.4 (b), il modello mostra come la variabile X abbia sia effetti diretti che indiretti sulla variabile di esito Y. L’effetto indiretto segue il percorso X → M → Y, dove M funge da variabile intermedia o mediatrice. Questo significa che M è il canale attraverso il quale gli effetti di X sono trasmessi a Y. In questo modello, M è una variabile endogena, nel senso che è influenzata da X (indicato dal percorso X → M), e allo stesso tempo agisce come una variabile causale nei confronti di Y (come indicato da M → Y).\nLa variabile M assume un doppio ruolo in termini di affidabilità e precisione della misurazione. Da una parte, essendo un esito di X, M è soggetta a disturbi, che includono potenziali errori di misurazione. Dall’altra, nel suo ruolo di causa per Y insieme a X, si presume nelle analisi di regressione che sia X che M siano prive di errore di misurazione. Questa assunzione non presenta problemi se l’affidabilità delle misure su M è elevata, ossia se i punteggi di M sono accurati e consistenti.\nIn aggiunta, il modello descritto nella Figura 19.4 (b) include tre ipotesi importanti:\n\nX ha un effetto diretto su Y, oltre al suo effetto indiretto tramite M.\nNon ci sono interazioni negli effetti lineari di X e M su Y, il che significa che l’effetto di X su Y è lo stesso a prescindere dai livelli di M, e viceversa.\nIl modello non omette confonditori potenzialmente importanti tra le coppie di variabili X, M e Y. In altre parole, non ci sono fattori esterni non considerati nel modello che potrebbero influenzare le relazioni tra queste tre variabili.\n\nIn sintesi, la Figura 19.4 (b) presenta un modello in cui X influisce su Y sia direttamente che indirettamente attraverso M, e queste relazioni sono considerate prive di interazioni complesse o di confonditori non rilevati.\nLa gestione degli errori di misurazione e dei confonditori non considerati in modelli che includono effetti causali indiretti rappresenta una sfida notevole, poiché i loro effetti sulle stime possono essere complessi e non sempre prevedibili. Per esemplificare, consideriamo il modello della Figura 19.4 (b) dove si analizza l’effetto indiretto di X su Y attraverso la variabile intermedia M.\nSe assumiamo che non ci siano errori di misurazione nella variabile causale X, qualsiasi errore di misurazione presente nella variabile intermedia M può introdurre un bias negativo nelle stime dell’effetto indiretto di X su Y. Ciò significa che l’effetto indiretto potrebbe essere sottostimato a causa dell’errore in M. D’altro canto, se non si tiene conto dei confonditori tra M e Y, cioè se ci sono variabili o fattori non considerati che influenzano sia M che Y, ciò può portare a un bias positivo, sovrastimando l’effetto indiretto.\nQuando entrambe queste situazioni – errori di misurazione in M e confonditori tra M e Y – si verificano contemporaneamente, le conseguenze sulle stime dell’effetto indiretto possono variare ampiamente. Potrebbe verificarsi una sovrastima, una sottostima o, in rari casi, nessun bias significativo. Studi di simulazione hanno rivelato che tentare di correggere solo una fonte di bias (come l’errore di misurazione in M) in presenza di entrambi i tipi di bias può addirittura aggravare il problema, portando a stime più distorte rispetto a quelle che non tengono conto di alcun bias.\nIn sintesi, la valutazione accurata dell’effetto indiretto in un modello che comprende una variabile intermedia richiede un’attenta considerazione sia degli errori di misurazione che dei confonditori potenziali, poiché la loro interazione può influenzare in modi complessi e talvolta inaspettati la validità delle stime.\n\n\n19.4.5 Modelli Ricorsivi, Non Ricorsivi e Parzialmente Ricorsivi\nTutti i modelli di percorso parametrici più complessi possono essere “assemblati” a partire dai modelli elementari mostrati nelle figure precedenti. Ci sono due tipi fondamentali di modelli: ricorsivi e non ricorsivi. I modelli ricorsivi hanno due caratteristiche essenziali: tutti gli effetti causali sono unidirezionali e i loro disturbi sono indipendenti. La Figura 19.5 (a) è un esempio di un modello ricorsivo (Tutti i modelli considerati finora sono ricorsivi.)\nI modelli non ricorsivi, invece, hanno cicli causali (feedback) in cui ≥ 2 variabili endogene sono specificate come cause ed effetti l’una dell’altra, direttamente o indirettamente. Nella loro forma non parametrica, corrispondono a grafi ciclici diretti. La Figura 19.5 (b) è un esempio di un modello parametrico non ricorsivo con causazione reciproca rappresentata come\n\\(Y1 \\overset{\\rightarrow}{\\underset{\\leftarrow}{}} Y2,\\)\nindicando che le variabili Y1 e Y2 hanno effetti simultanei l’una sull’altra.\n\n\n\n\n\n\nFigura 19.5: Esempi di modelli ricorsivi, non ricorsivi e parzialmente ricorsivi con due diversi schemi di correlazione degli errori. Tutti i diagrammi sono mostrati in simbolismo compatto. (Figura tratta da Kline (2023))\n\n\n\nI modelli che includono cicli causali possono presentare, o meno, covarianze tra i loro termini di disturbo. La presenza di errori correlati in questi modelli implica l’esistenza di ipotesi su cause comuni non misurate che influenzano le variabili in questione.\nAd esempio, nel modello rappresentato nella Figura 19.5 (b), le variabili Y1 e Y2 sono definite come cause reciproche, ovvero ognuna influisce sull’altra. In aggiunta a ciò, se nel modello è specificata una covarianza tra i termini di disturbo \\(D_1\\) e \\(D_2\\), ciò suggerisce che Y1 e Y2 condividono almeno una causa comune non misurata. In altre parole, l’ipotesi è che esistano fattori non osservati che influenzano entrambe le variabili, \\(Y1\\) e \\(Y2\\), e questa influenza comune si manifesta attraverso la covarianza tra i loro termini di disturbo.\nEsiste anche un altro tipo di modello di percorso, quello che ha effetti unidirezionali e covarianze dei disturbi. I modelli parzialmente ricorsivi con un pattern di correlazioni dei disturbi senza “archi” possono essere trattati nell’analisi proprio come i modelli ricorsivi. Un pattern senza “archi” significa che gli errori correlati sono limitati a coppie di variabili endogene senza effetti diretti tra di loro, come Y1 e Y2 nella Figura 19.5 (c).\nI modelli parzialmente ricorsivi che presentano un pattern di correlazioni dei disturbi caratterizzato dalla presenza di “archi” richiedono un trattamento analitico simile a quello dei modelli non ricorsivi. Un pattern con “archi” si verifica quando esiste una covarianza tra i termini di disturbo di due variabili endogene che sono collegate da un effetto diretto. Ad esempio, nella Figura 19.5 (d), le variabili Y1 e Y2 sono collegate da un effetto diretto e presentano una covarianza tra i loro disturbi \\(D_1\\) e \\(D_2\\).\nLa presenza di un effetto diretto insieme a disturbi correlati tra due variabili crea un percorso di confondimento nel modello. Un percorso di confondimento è una via attraverso la quale può fluire l’influenza causale indiretta, potenzialmente distorcendo l’interpretazione dei rapporti causali diretti. In questi casi, la selezione di covariate (variabili aggiuntive che potrebbero spiegare parte della relazione osservata) non è sufficiente per “chiudere” o eliminare questo percorso di confondimento. Pertanto, questi modelli richiedono un’attenzione particolare nell’analisi per garantire che le stime degli effetti causali siano accurate e non siano influenzate in modo improprio da questi percorsi di confondimento.\nI modelli ricorsivi e quelli parzialmente ricorsivi che non includono cicli causali possono essere efficacemente rappresentati tramite grafi aciclici diretti (DAG). Questo tipo di rappresentazione grafica implica che è possibile applicare tutte le regole di identificazione grafica esposte in questo capitolo. Nei DAG, le relazioni causali sono rappresentate come flussi unidirezionali senza cicli, rendendo più chiaro e diretto l’analisi delle relazioni tra le variabili.\nD’altra parte, i modelli non ricorsivi che includono cicli causali, come quello illustrato nella Figura 19.5 (b), sono rappresentati da grafi ciclici diretti. In questi grafi, le variabili possono influenzarsi a vicenda in un ciclo continuo, creando una struttura più complessa. A causa di questa complessità, le regole di identificazione grafica per i grafi ciclici diretti non sono sviluppate quanto quelle per i DAG. Questo significa che analizzare e interpretare i modelli non ricorsivi con cicli causali è più complesso e richiede l’uso di approcci analitici più avanzati o specifici per gestire correttamente le relazioni cicliche tra le variabili.\nPossiamo stabilire una regola generale per i modelli di percorso parametrici: i modelli ricorsivi o parzialmente ricorsivi, che presentano schemi di covarianze dei disturbi privi di “archi” e che soddisfano due condizioni specifiche, sono considerati identificati. Queste condizioni sono: (1) i gradi di libertà del modello (dfM) devono essere maggiori o uguali a zero e (2) ogni variabile non misurata, inclusi i termini di errore, deve essere associata a una scala metrica.\nInoltre, i modelli di equazioni strutturali che sono identificati e hanno un numero di osservazioni uguale al numero dei parametri liberi (dfM = 0) sono classificati come “appena identificati”. Al contrario, i modelli con più osservazioni rispetto ai parametri liberi (dfM &gt; 0) sono considerati “sovraidentificati”.\nUn modello di equazioni strutturali può risultare sotto-identificato in due modi distinti: (1) se dfM è inferiore a zero, oppure (2) se, pur avendo un dfM maggiore o uguale a zero, alcuni parametri liberi rimangono sotto-identificati perché non vi sono sufficienti informazioni per la loro stima, anche se altri parametri all’interno dello stesso modello sono identificati. Nel secondo scenario, l’intero modello è considerato non identificato, anche se dfM è maggiore o uguale a zero. In generale, un modello si considera sotto-identificato quando non è possibile stimare in modo univoco tutti i suoi parametri liberi.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#analisi-dei-percorsi-e-regressione-bivariata",
    "href": "chapters/path_analysis/01_path_analysis.html#analisi-dei-percorsi-e-regressione-bivariata",
    "title": "19  Analisi dei percorsi",
    "section": "19.5 Analisi dei percorsi e regressione bivariata",
    "text": "19.5 Analisi dei percorsi e regressione bivariata\nCominciamo esaminando l’analisi dei percorsi partendo dall’esempio più semplice, ovvero il modello di regressione lineare. Il modello di regressione bivariata si esprime tramite l’equazione seguente:\n\\[y_1 = b_0 + b_1 x_1 + \\epsilon_1,\\]\ndove\\(y\\)rappresenta la variabile dipendente,\\(b_0\\)rappresenta l’intercetta,\\(b_1\\)rappresenta la pendenza della retta di regressione,\\(x\\)è la variabile indipendente e\\(\\epsilon\\)è il termine di errore.\nNell’ambito della descrizione delle relazioni tra variabili manifeste e latenti, si adotta spesso la notazione LISREL. In questa notazione, il modello presentato in precedenza può essere espresso come segue:\n\\[y_1 = \\alpha + \\gamma x_1 + \\zeta_1\\]\nDove:\n-\\(x_1\\): variabile esogena singola -\\(y_1\\): variabile endogena singola -\\(\\alpha\\): intercetta di \\(y_1\\) -\\(\\gamma_1\\): coefficiente di regressione -\\(\\zeta_1\\): termine di errore di \\(y_1\\) -\\(\\phi\\): varianza o covarianza della variabile esogena -\\(\\psi\\): varianza o covarianza residuale della variabile endogena\nIl diagramma di percorso per il modello di regressione bivariata è illustrato nella Figura 19.6.\n\n\n\n\n\n\nFigura 19.6: Diagramma di percorso per il modello di regressione bivariato.\n\n\n\nFacciamo un esempio numerico. Simuliamo tre variabili: x1, x2, y.\n\nset.seed(42)\nn &lt;- 100\nx1 &lt;- rnorm(n, 90, 20)\nx2 &lt;- x1 + rnorm(n, 0, 30)\ny &lt;- 25 + 0.5 * x1 + 1.0 * x2 + rnorm(n, 0, 30)\n\ncor(cbind(x1, x2, y))\n\ndat &lt;- data.frame(\n    y, x1, x2\n)\n\n\nA matrix: 3 x 3 of type dbl\n\n\n\nx1\nx2\ny\n\n\n\n\nx1\n1.0000000\n0.6244502\n0.5493286\n\n\nx2\n0.6244502\n1.0000000\n0.7992614\n\n\ny\n0.5493286\n0.7992614\n1.0000000\n\n\n\n\n\nConsideriamo la relazione tra x1 (variabile endogena) e y (variabile endogena). In R possiamo adattare ai dati un modello di regressione mediante la funzione lm.\n\nm1a &lt;- lm(y ~ x1, data = dat)\nsummary(m1a) |&gt;\n    print()\n\n\nCall:\nlm(formula = y ~ x1, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-82.462 -29.539  -3.437  29.200 122.234 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.5974    18.9844   1.980   0.0505 .  \nx1            1.3286     0.2042   6.508 3.25e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 42.31 on 98 degrees of freedom\nMultiple R-squared:  0.3018,    Adjusted R-squared:  0.2946 \nF-statistic: 42.35 on 1 and 98 DF,  p-value: 3.251e-09\n\n\n\nUsiamo ora lavaan per adattare lo stesso modello ai dati.\n\nm1b &lt;- \"\n    y ~ 1 + x1\n\"\nfit1b &lt;- sem(m1b, data = dat)\nparameterEstimates(fit1b) |&gt;\n    print()\n\n  lhs op rhs      est      se     z pvalue ci.lower ci.upper\n1   y ~1       37.597  18.794 2.001  0.045    0.763   74.432\n2   y  ~  x1    1.329   0.202 6.574  0.000    0.933    1.725\n3   y ~~   y 1754.100 248.067 7.071  0.000 1267.897 2240.303\n4  x1 ~~  x1  429.432   0.000    NA     NA  429.432  429.432\n5  x1 ~1       90.650   0.000    NA     NA   90.650   90.650\n\n\nL’intercetta di y ~1 (37.597) e il coefficiente di regressione di y ~ x1 (1.329) corrispondono all’output di lm() con piccoli errori di arrotondamento. L’intercetta per x1 ~1 (90.650) e la sua varianza x1 ~~ x1 (429.432) descrivono una media ed una varianza esogena e corrispondono alla media e alla varianza univariate:\n\nmean(dat$x1)\n\n90.6502963122602\n\n\n\nvar(dat$x1) * (length(dat$x1) - 1) / length(dat$x1)\n\n429.432031274383\n\n\nLa varianza residua di y, y ~~ y corrisponde alla quota della varianza osservata della variabile y che non è spiegata dalla relazione lineare su x1:\n\nvar(dat$y) * 99 / 100 - (1.3286 * 429.432 * 1.3286)\n\n1754.15693691975\n\n\nLa funzione semPaths consente di creare un diagramma di percorso a partire dall’oggetto creato da sem.\n\nsemPlot::semPaths(\n    fit1b,\n    layout = \"tree\", sizeMan = 7, sizeInt = 5, style = \"ram\", \n    residuals = TRUE, intAtSide = FALSE, edge.label.cex = 1.15,\n    whatLabels = \"est\", nCharNodes = 0, normalize = FALSE\n)",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#analisi-dei-percorsi-e-regressione-multipla",
    "href": "chapters/path_analysis/01_path_analysis.html#analisi-dei-percorsi-e-regressione-multipla",
    "title": "19  Analisi dei percorsi",
    "section": "19.6 Analisi dei percorsi e regressione multipla",
    "text": "19.6 Analisi dei percorsi e regressione multipla\nLa regressione semplice è limitata a una sola variabile esogena. Nella pratica, un ricercatore può essere interessato a studiare come un gruppo di variabili esogene possano predire una variabile di esito. Supponiamo di avere ancora una sola variabile di esito endogena ma due predittori esogeni; questo caso è noto come regressione multipla:\n\\[\ny_1 = \\alpha_1 + \\gamma_1 x_1 + \\gamma_2 x_2 + \\zeta_1.\n\\]\nIl diagramma di percorso mostra la relazione tra tutte le variabili, comprendendo anche i fattori di disturbo, e fornisce dunque la rappresentazione grafica dell’equazione precedente.\n\n\n\n\n\n\nFigura 19.7: Diagramma di percorso per il modello di regressione multipla.\n\n\n\nI coefficienti di percorso associati alle frecce orientate esprimono la portata del nesso causale e corrispondono ai pesi beta (ovvero ai coefficienti parziali di regressione standardizzati). Le frecce non orientate esprimono la portata della pura associazione tra variabili e dunque corrispondono alle correlazioni/covarianze.\nIn un diagramma di percorso, il numero di equazioni corrisponde al numero di variabili endogene del modello. Nel caso specifico, poiché vi è una sola variabile endogena (ovvero \\(y\\)), esiste un’unica equazione che descrive le relazioni causalitiche interne al path diagram. All’interno di ciascuna equazione, inoltre, il numero di termini corrisponde al numero di frecce orientate che puntano verso la variabile endogena. Nell’esempio sopra citato, pertanto, la sola equazione del modello contiene tre termini, ciascuno associato ad una freccia orientata.\nUsando lm otteniamo la seguente stima dei coefficienti:\n\nm2a &lt;- lm(y ~ 1 + x1 + x2, data = dat)\nfit2a &lt;- summary(m2a) |&gt;\n    print()\n\n\nCall:\nlm(formula = y ~ 1 + x1 + x2, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-77.299 -19.856  -2.476  19.112  75.617 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  44.4537    13.6639   3.253  0.00157 ** \nx1            0.1991     0.1879   1.060  0.29179    \nx2            1.0853     0.1127   9.628 8.54e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30.41 on 97 degrees of freedom\nMultiple R-squared:  0.643, Adjusted R-squared:  0.6356 \nF-statistic: 87.34 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\n\nGli stessi risultati si ottengono con lavaan.\n\nm2b &lt;- \"\n    y ~ 1 + x1 + x2\n    x1 ~~ x1\n    x2 ~~ x2\n    x1 ~~ x2\n\"\n\n\nfit2b &lt;- sem(m2b, data = dat)\n\n\nparameterEstimates(fit2b) |&gt;\n    print()\n\n  lhs op rhs      est      se      z pvalue ci.lower ci.upper\n1   y ~1       44.454  13.457  3.303  0.001   18.078   70.830\n2   y  ~  x1    0.199   0.185  1.076  0.282   -0.164    0.562\n3   y  ~  x2    1.085   0.111  9.775  0.000    0.868    1.303\n4  x1 ~~  x1  429.432  60.731  7.071  0.000  310.402  548.462\n5  x2 ~~  x2 1192.840 168.693  7.071  0.000  862.208 1523.472\n6  x1 ~~  x2  446.927  84.379  5.297  0.000  281.546  612.307\n7   y ~~   y  896.963 126.850  7.071  0.000  648.342 1145.584\n8  x1 ~1       90.650   2.072 43.744  0.000   86.589   94.712\n9  x2 ~1       88.026   3.454 25.487  0.000   81.257   94.795\n\n\nEsaminiamo il diagramma di percorso.\n\nsemPlot::semPaths(\n    fit2b,\n    layout = \"tree\", sizeMan = 7, sizeInt = 5, style = \"ram\",\n    residuals = TRUE, intAtSide = FALSE, edge.label.cex = 1.15,\n    whatLabels = \"est\", nCharNodes = 0, normalize = FALSE\n)",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#effetti-diretti-e-indiretti",
    "href": "chapters/path_analysis/01_path_analysis.html#effetti-diretti-e-indiretti",
    "title": "19  Analisi dei percorsi",
    "section": "19.7 Effetti diretti e indiretti",
    "text": "19.7 Effetti diretti e indiretti\nL’analisi del percorso offre un metodo essenziale per distinguere tra diverse tipologie di effetti che influenzano le variabili in esame: l’effetto diretto, l’effetto indiretto e l’effetto totale. Gli effetti diretti rappresentano l’influenza che una variabile esercita su un’altra senza mediazione di altre variabili intermedie. Gli effetti indiretti, invece, operano attraverso l’intermediazione di almeno una variabile aggiuntiva nel processo. L’effetto totale è la somma cumulativa degli effetti diretti e indiretti.\nNella Figura 19.8, la variabile \\(y_1\\) esercita un effetto diretto sulla variabile \\(y_2\\). Allo stesso tempo, \\(y_1\\) produce un effetto indiretto sulla variabile \\(y_3\\), poiché non esiste una connessione causale diretta tra \\(y_1\\) e \\(y_3\\). Nel contesto rappresentato, la variabile \\(y_1\\) agisce come variabile esogena, mentre le variabili \\(y_2\\) e \\(y_3\\) fungono da variabili endogene.\n\n\n\n\n\n\nFigura 19.8: Diagramma di percorso per un modello a catena.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#le-regole-di-wright",
    "href": "chapters/path_analysis/01_path_analysis.html#le-regole-di-wright",
    "title": "19  Analisi dei percorsi",
    "section": "19.8 Le regole di Wright",
    "text": "19.8 Le regole di Wright\nL’obiettivo primario dell’analisi del percorso consiste nella decomposizione della correlazione (o della covarianza) in base alla somma dei vari percorsi (diretti e indiretti) che collegano due variabili mediante coefficienti noti come “path coefficients.” Utilizzando il diagramma del percorso, Sewall Wright (1921, 1934) formulò le regole che, tramite le “tracing rules,” stabiliscono il collegamento tra le correlazioni (o covarianze) delle variabili e i parametri del modello. Le tracing rules si possono esprimere nei seguenti termini:\n\nÈ possibile procedere in avanti lungo una freccia e poi a ritroso, seguendo la direzione della freccia, ma non è permesso muoversi in avanti e poi tornare indietro.\nUn percorso composto non deve attraversare più di una volta la stessa variabile, cioè non possono esserci cicli.\nUn percorso non può contenere più di una linea curva.\n\nIl termine “percorso” fa riferimento al tracciato che connette due variabili e si compone di sequenze di frecce unidirezionali e curve non direzionali. A ciascun percorso valido (cioè conforme alle regole di Wright) viene assegnato un valore numerico che rappresenta il prodotto dei coefficienti presenti lungo il percorso stesso. I coefficienti di percorso possono essere coefficienti parziali di regressione standardizzati se il legame è unidirezionale, oppure coefficienti di correlazione se il legame è bidirezionale.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#scomposizione-delle-correlazionicovarianze",
    "href": "chapters/path_analysis/01_path_analysis.html#scomposizione-delle-correlazionicovarianze",
    "title": "19  Analisi dei percorsi",
    "section": "19.9 Scomposizione delle correlazioni/covarianze",
    "text": "19.9 Scomposizione delle correlazioni/covarianze\nIl principio fondamentale è stato formulato da Sewall Wright (1934) nel seguente modo:\n\nOgni correlazione tra variabili in una rete di relazioni sequenziali può essere analizzata nei contributi provenienti da tutti i percorsi (diretti o attraverso fattori comuni) con i quali le due variabili sono connesse. Ogni contributo ha un valore pari al prodotto dei coefficienti relativi ai percorsi elementari. Se sono presenti correlazioni residue (rappresentate da frecce bidirezionali), uno (ma mai più di uno) dei coefficienti moltiplicati per ottenere il contributo del percorso di connessione può essere un coefficiente di correlazione. Gli altri sono tutti coefficienti di percorso.\n\nDa questo principio possiamo derivare la regola di scomposizione della correlazione: la correlazione o covarianza tra due variabili può essere scomposta in un numero di termini uguale al numero di percorsi che le collegano. Ogni termine è ottenuto dal prodotto dei coefficienti associati alle variabili lungo il percorso. In altre parole, è possibile decomporre la correlazione o la covarianza tra due variabili in tanti contributi quanti sono i percorsi possibili che collegano le due variabili.\n\n19.9.1 Scomposizione della varianza\nLa decomposizione della varianza di una variabile endogena può essere affrontata attraverso una suddivisione in due componenti: una componente spiegata, attribuibile alle variabili che esercitano un’influenza causale su di essa, e una componente non spiegata. La componente spiegata della varianza deriva dall’aggregazione degli effetti delle diverse variabili che sono connessi alla variabile endogena, rispettando le regole di tracciamento definite da Wright. Il numero di addendi corrisponde al numero di percorsi che collegano la variabile endogena a se stessa. In tal modo, la varianza spiegata rappresenta la parte della varianza totale della variabile endogena che può essere attribuita alle influenze delle variabili correlate attraverso i percorsi definibili all’interno del modello.\n\n\n19.9.2 Relazioni tra variabili endogene e esogene\nComplessivamente, i concetti di varianza, covarianza e correlazione informano direttamente il calcolo dei coefficienti di percorso in un path diagram secondo le seguenti “8 regole dei coefficienti di percorso”.\n\nRegola 1: Le relazioni non specificate tra le variabili esogene sono semplicemente le loro correlazioni bivariate.\nRegola 2: Quando due variabili sono collegate da un singolo percorso, il coefficiente di quel percorso è il coefficiente di regressione.\nRegola 3: La forza di un percorso composto (che include più collegamenti) è il prodotto dei coefficienti individuali.\nRegola 4: Quando le variabili sono collegate da più di un percorso, ciascun percorso è il coefficiente di regressione “parziale”.\nRegola 5: Gli errori sulle variabili endogene si riferiscono alle correlazioni o varianze non spiegate che derivano dalle variabili non misurate.\nRegola 6: Le correlazioni non analizzate (residui) tra due variabili endogene sono le loro correlazioni parziali.\nRegola 7: L’effetto totale che una variabile ha su un’altra è la somma dei suoi effetti diretti e indiretti.\nRegola 8: L’effetto totale (compresi i percorsi non diretti) è equivalente alla correlazione totale.\n\nEsempio. Consideriamo nuovamente il modello di regressione multipla con due variabili esogene e una sola variabile endogena che è stato presentato sopra.\nLa la covarianza tra y e x1\n\ncov(dat$y, dat$x1) * 99 / 100\n\n570.56471345413\n\n\npuò essere ricavata usando le regole di Wright nel modo seguente:\n\n0.199 * 429.43 + 1.085 * 446.93\n\n570.37562\n\n\nLa quota di varianza non spiegata della variabile endogena è:\n\n(var(dat$y) * 99 / 100) - (\n    0.199^2 * 429.43 + (1.085)^2 * 1192.84 + 2 * (0.199 * 1.085 * 446.93)\n)\n\n897.936130308472",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#oltre-la-regressione-multipla",
    "href": "chapters/path_analysis/01_path_analysis.html#oltre-la-regressione-multipla",
    "title": "19  Analisi dei percorsi",
    "section": "19.10 Oltre la regressione multipla",
    "text": "19.10 Oltre la regressione multipla\nApprofondiamo l’utilizzo dell’analisi dei percorsi (path analysis) per studiare un modello di mediazione che supera i limiti della classica regressione multipla. L’analisi di mediazione è un metodo statistico ampiamente impiegato dai psicologi per esaminare le relazioni complesse tra le variabili di uno studio. Questa metodologia può essere applicata in studi osservazionali per affinare la comprensione della relazione tra variabili predittive e variabili di esito, introducendo mediatori. L’integrazione di mediatori permette di esplorare e chiarire i meccanismi sottostanti che influenzano la relazione tra le variabili principali, fornendo una visione più dettagliata e complessa degli effetti e delle interazioni in gioco.\nPrenderemo in esame un modello teorico specifico, ispirato alla Self Determination Theory (SDT) di Deci e Ryan (2000), una delle teorie più influenti nel campo della motivazione. In questo contesto, utilizzeremo la path analysis per valutare come la SDT possa aiutarci a comprendere i fattori psicologici e comportamentali che influenzano i sintomi bulimici in un gruppo di giovani donne adulte. Più specificamente, esamineremo come l’appagamento (soddisfazione) e l’esaurimento (frustrazione) delle risorse psicologiche essenziali, o bisogni psicologici (ad esempio, per l’autonomia, la competenza e la relazionalità), possono prevedere in modo differenziale i sintomi bulimici nelle donne attraverso due mediatori chiave, l’approvazione degli ideali culturali sulla magrezza e inflessibilità delle opinioni sul proprio corpo. Secondo la SDT, i bisogni psicologici influenzano la capacità di un individuo di autoregolarsi e far fronte alle richieste della vita quotidiana e possono rendere gli individui vulnerabili al malessere psicologico se i bisogni psicologici vengono frustrati (Vansteenkiste & Ryan, 2013). La frustrazione dei bisogni può essere psicologicamente più depauperante della mancanza di soddisfazione dei bisogni.\nGli individui i cui bisogni vengono frustrati possono impegnarsi in attività malsane e comportamenti compensatori al fine di riconquistare una soddisfazione dei bisogni a breve termine. La frustrazione dei bisogni rende gli individui più vulnerabili agli ideali culturali, in quanto le risorse personali per rifiutare questi ideali sono esaurite (Pelletier & Dion, 2007).\nIl modello che verrà testato propone che le donne i cui bisogni psicologici sono frustrati avalleranno ideali sociali più problematici sulla magrezza rispetto alle donne i cui bisogni psicologici sono soddisfatti. La frustrazione dei bisogni sarà anche predittiva dell’inflessibilità degli schemi corporei, poiché è stato dimostrato che la frustrazione dei bisogni porta a disturbi dell’immagine corporea e a comportamenti alimentari patologici (Boone, Vansteenkiste, Soenens, Van der Kaap-Deeder e Verstuyf, 2014). Il modello propone inoltre che una maggiore approvazione degli ideali culturali sulla sarà predittiva di una maggiore inflessibilità sugli schemi corporei che, di per sé, è predittiva dei sintomi bulimici.\nIl campione include 192 partecipanti, in maggioranza donne, di età media 21.2 anni (SD = 6.89). Sono stati somministrati i seguenti strumenti:\n\nBody Image-Acceptance and Action Questionnaire (Sandoz, Wilson, Merwin, & Kellum, 2013), per misurare l’inflessibilità relativa alla propria immagine corporea,\nEndorsement of Society’s Beliefs Related to Thinness and Obesity (Boyer, 1991), per valutare l’internalizzazione degli ideali di magrezza,\nBasic Psychological Needs Satisfaction and Frustration Scale (Chen et al., 2015), per misurare la soddisfazione e la frustrazione dei bisogni,\nEating Disorders Inventory-2 – Bulimic Symptomology Subscale (Garner, 1991), per misurare i sintomi bulimici.\n\nI dati sono i seguenti.\n\nupper &lt;- '\n  1 0.44 -0.41 0.55 0.63\n  1 -0.37 0.45 0.44\n  1 -0.71 -0.39\n  1 0.47\n  1\n  '\n\n\n# BFLX – Body Inflexibility,\n# END – Endorsement of Societal Beliefs about Thinness and Obesity,\n# MNS – Mean Need Satisfaction,\n# MNF – Mean Need Frustration,\n# BULS – Bulimic Symptoms\ndat_cov &lt;- lavaan::getCov(\n    upper,\n    lower = FALSE,\n    names = c(\"BFLX\", \"END\", \"MNS\", \"MNF\", \"BULS\")\n)\ndat_cov\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nBFLX\nEND\nMNS\nMNF\nBULS\n\n\n\n\nBFLX\n1.00\n0.44\n-0.41\n0.55\n0.63\n\n\nEND\n0.44\n1.00\n-0.37\n0.45\n0.44\n\n\nMNS\n-0.41\n-0.37\n1.00\n-0.71\n-0.39\n\n\nMNF\n0.55\n0.45\n-0.71\n1.00\n0.47\n\n\nBULS\n0.63\n0.44\n-0.39\n0.47\n1.00\n\n\n\n\n\n\n19.10.1 Modello di mediazione\n\ndata &lt;- c(\n    0, \"a\", 0,\n    0, 0, 0,\n    \"b\", \"c_prime\", 0\n)\nM &lt;- matrix(nrow = 3, ncol = 3, byrow = TRUE, data = data)\nplot &lt;- plotmat(M,\n    pos = c(1, 2),\n    name = c(\"X\", \"M\", \"Y\"),\n    box.type = \"rect\", box.size = 0.12, box.prop = 0.5, curve = 0\n)\n\n\n\n\n\n\n\n\nNel modello di mediazione di base abbiamo tre variabili chiave: la variabile indipendente \\(X\\), la variabile dipendente \\(Y\\) e il mediatore \\(M\\). La freccia da \\(X\\) a \\(M\\) (etichettata con ‘a’) rappresenta l’effetto di \\(X\\) su \\(M\\). La freccia da \\(M\\) a \\(Y\\) (etichettata con ‘b’) rappresenta l’effetto di \\(M\\) su \\(Y\\), mentre la freccia tratteggiata da \\(X\\) a \\(Y\\) (etichettata con “c’”) rappresenta l’effetto diretto di \\(X\\) su \\(Y\\), escludendo la mediazione di \\(M\\).\nPossiamo formalizzare il modello statistico di mediazione in equazioni di regressione lineare come segue:\n\\[M = a_0 + a \\times X + e_M\\]\n\\[Y = b_0 + b \\times M + c' \\times X + e_Y\\]\nNella prima equazione lineare, \\(M\\) è regredito su \\(X\\) con un’intercetta\\(a_0\\), pendenza\\(a\\)e termine di errore\\(e_M\\). Questa equazione rappresenta il percorso \\(X \\rightarrow M\\)con\\(a\\)come effetto di\\(X\\)su\\(M\\). Nella seconda equazione, \\(Y\\) è regredito su \\(M\\) e \\(X\\) con un’intercetta \\(b_0\\), pendenze \\(b\\) e \\(c'\\) e termine di errore \\(e_Y\\). Questa equazione rappresenta due percorsi: \\(M \\rightarrow Y\\) e \\(X \\rightarrow Y\\), con \\(b\\) come effetto di \\(M\\) su \\(Y\\) e \\(c'\\) come effetto diretto di \\(X\\) su \\(Y\\).\nNel contesto della modellazione di equazioni strutturali, il metodo più comune per calcolare gli effetti diretti, indiretti e totali si basa sul prodotto dei coefficienti, come riassunto nelle seguenti formule:\n\nEffetto diretto = \\(c'\\)\nEffetto indiretto = \\(b \\times a\\)\nEffetto totale = \\(c' + b \\times a\\)\n\n\n\n19.10.2 Stima degli Effetti Diretti, Indiretti e Totali\nLe tecniche di modellazione di equazioni strutturali permettono di stimare tutti i parametri (a, b, c’) simultaneamente, date le specifiche del modello per i dati. Di conseguenza, possiamo utilizzare le seguenti formule per stimare gli effetti diretti, indiretti e totali dai dati:\n\nEffetto diretto = \\(\\hat{c}'\\)\nEffetto indiretto = \\(\\hat{b} \\times \\hat{a}\\)\nEffetto totale = \\(\\hat{c}' + \\hat{b} \\times \\hat{a}\\)\n\ndove \\(\\hat{a}\\) è una stima di \\(a\\), e così via. Gli errori standard stimati di questi effetti possono poi essere calcolati utilizzando metodi asintotici standard, supportato dalla maggior parte dei software SEM.\n\n\n19.10.3 Analisi con lavaan\nIl caso presente prende in considerazione BFLX come variabile endogena, MNF come variabile esogena e END come variabile mediatrice. Utilizzando Mplus, Barbeau, Boileau, Sarr e Smith (2019) hanno identificato i seguenti coefficienti di percorso: \\(a = 0.37\\), \\(b = 0.29\\) e \\(c = 0.34\\).\nProcediamo con l’analisi utilizzando il pacchetto lavaan in R. Cominciamo definendo il modello di mediazione.\n\nmod &lt;- \"\n  # direct effect\n  BFLX ~ c*MNF\n  # mediator\n  BFLX ~ b*END\n  END ~ a*MNF\n\n  # indirect effect (a*b)\n  ab := a*b\n  # total effect\n  total := c + (a*b)\n\"\n\nAdattiamo il modello ai dati.\n\nfit &lt;- sem(\n    mod,\n    sample.cov = dat_cov,\n    sample.nobs = 192\n)\n\nEsaminiamo i risultati:\n\nsummary(fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) |&gt;\n    print()\n\nlavaan 0.6.16 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         5\n\n  Number of observations                           192\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nModel Test Baseline Model:\n\n  Test statistic                               125.849\n  Degrees of freedom                                 3\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.000\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)               -480.945\n  Loglikelihood unrestricted model (H1)       -480.945\n                                                      \n  Akaike (AIC)                                 971.890\n  Bayesian (BIC)                               988.178\n  Sample-size adjusted Bayesian (SABIC)        972.339\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.000\n  P-value H_0: RMSEA &lt;= 0.050                       NA\n  P-value H_0: RMSEA &gt;= 0.080                       NA\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  BFLX ~                                                                \n    MNF        (c)    0.441    0.065    6.769    0.000    0.441    0.441\n    END        (b)    0.241    0.065    3.702    0.000    0.241    0.241\n  END ~                                                                 \n    MNF        (a)    0.450    0.064    6.982    0.000    0.450    0.450\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .BFLX              0.648    0.066    9.798    0.000    0.648    0.651\n   .END               0.793    0.081    9.798    0.000    0.793    0.797\n\nR-Square:\n                   Estimate\n    BFLX              0.349\n    END               0.203\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    ab                0.109    0.033    3.271    0.001    0.109    0.109\n    total             0.550    0.060    9.125    0.000    0.550    0.550\n\n\n\nGeneriamo un diagramma di percorso.\n\nsemPlot::semPaths(\n    fit,\n    layout = \"tree\", sizeMan = 7, sizeInt = 5, style = \"ram\",\n    residuals = TRUE, intAtSide = FALSE, edge.label.cex = 1.15,\n    whatLabels = \"est\", nCharNodes = 0, normalize = FALSE\n)\n\n\n\n\n\n\n\n\nI coefficienti di percorso sono simili, ma non identici, a quelli trovati con Mplus.\nL’effetto diretto di MNF (Need Frustration) su BFLX (Body Inflexibility) è uguale a 0.44. L’effetto totale è \\(0.44 + 0.45*0.24 = 0.55\\). L’effetto di mediazione è uguale a \\(0.45*0.24 = 0.109\\). L’outout di lavaan fornisce anche gli errori standard e il test che tali effetti siano uguali a zero.\nLe correlazioni tra le variabili sono esprimibili nei termini dei coefficienti di percorso. Per esempio la correlazionetra BFLX e MNF è\n\n.44 + .45 * .24\n\n0.548\n\n\nLa correlazione tra BFLX e END è\n\n.24 + .44 * .45\n\n0.438\n\n\nL’output di lavaan fornisce anche la porzione di varianza che viene spiegata dalle variabili esogene per le due variabili endogene nel modello.\nPer esempio, la varianza spiegata di END è\n\n0.45^2\n\n0.2025\n\n\ncome riportato dall’output di lavaan.\nContinuiamo con l’analisi di questi dati e esaminiamo ora un modello di path analisi più complesso (Fig. 4 di Barbeau et al., 2019). Usando la sintassi di lavaan, il modello diventa\n\n# BFLX – Body Inflexibility,\n# END – Endorsement of Societal Beliefs about Thinness and Obesity,\n# MNS – Mean Need Satisfaction,\n# MNF – Mean Need Frustration,\n# BULS – Bulimic Symptoms\nmod &lt;- \"\n  BULS ~ MNF + BFLX\n  BFLX ~ END + MNF\n  END ~ MNS + MNF\n\"\n\nAdattiamo il modello ai dati.\n\nfit2 &lt;- sem(\n    mod,\n    sample.cov = dat_cov,\n    sample.nobs = 192\n)\n\nEsaminiamo la soluzione ottenuta.\n\nsummary(fit2, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) |&gt;\n    print()\n\nlavaan 0.6.16 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         9\n\n  Number of observations                           192\n\nModel Test User Model:\n                                                      \n  Test statistic                                 8.229\n  Degrees of freedom                                 3\n  P-value (Chi-square)                           0.042\n\nModel Test Baseline Model:\n\n  Test statistic                               239.501\n  Degrees of freedom                                 9\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.977\n  Tucker-Lewis Index (TLI)                       0.932\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)               -700.169\n  Loglikelihood unrestricted model (H1)       -696.054\n                                                      \n  Akaike (AIC)                                1418.338\n  Bayesian (BIC)                              1447.655\n  Sample-size adjusted Bayesian (SABIC)       1419.146\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.095\n  90 Percent confidence interval - lower         0.017\n  90 Percent confidence interval - upper         0.176\n  P-value H_0: RMSEA &lt;= 0.050                    0.130\n  P-value H_0: RMSEA &gt;= 0.080                    0.696\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.035\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  BULS ~                                                                \n    MNF               0.177    0.066    2.688    0.007    0.177    0.177\n    BFLX              0.533    0.066    8.085    0.000    0.533    0.533\n  BFLX ~                                                                \n    END               0.241    0.065    3.702    0.000    0.241    0.241\n    MNF               0.441    0.065    6.769    0.000    0.441    0.441\n  END ~                                                                 \n    MNS              -0.102    0.091   -1.116    0.264   -0.102   -0.102\n    MNF               0.378    0.091    4.140    0.000    0.378    0.378\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .BULS              0.578    0.059    9.798    0.000    0.578    0.581\n   .BFLX              0.648    0.066    9.798    0.000    0.648    0.651\n   .END               0.788    0.080    9.798    0.000    0.788    0.792\n\nR-Square:\n                   Estimate\n    BULS              0.419\n    BFLX              0.349\n    END               0.208\n\n\n\nGeneriamo il diagramma di percorso.\n\nsemPlot::semPaths(\n    fit2,\n    layout = \"tree\", sizeMan = 7, sizeInt = 5, style = \"ram\",\n    residuals = TRUE, intAtSide = FALSE, edge.label.cex = 1.15,\n    whatLabels = \"est\", nCharNodes = 0, normalize = FALSE\n)\n\n\n\n\n\n\n\n\nAnche in questo caso i coefficienti di percorso sono simili, ma non identici, a quelli riportati da Barbeau et al. (2019). Gli autori riportano una varianza spiegata di END pari a 0.209; con lavaan si ottiene 0.208. Per BFLX gli autori riportano 0.292; lavaan ottiene 0.349. Per BULS gli autori riportano 0.478; con lavaan si ottiene 0.419.\nCalcoliamo, ad esempio, la correlazione tra MNF e BULS prevista dal modello, combinando gli effetti diretti e indiretti. Questo processo consiste nel sommare gli effetti diretti tra queste due variabili con quelli indiretti mediati da altre variabili nel modello.\n\n-.71 * -.10 * .24 * .53 +\n.38 *.24 * .53 +\n.44 * .53 +\n.18\n\n0.4705672\n\n\nIl valore trovato corrisponde bene al valore osservato nel campione, che è pari a 0.47.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#modellare-le-medie",
    "href": "chapters/path_analysis/01_path_analysis.html#modellare-le-medie",
    "title": "19  Analisi dei percorsi",
    "section": "19.11 Modellare le Medie",
    "text": "19.11 Modellare le Medie\nIn un modello di equazioni strutturali (SEM), modellare le medie è un processo che si concentra sulla stima e sull’analisi delle medie delle variabili all’interno del modello. Questo processo è fondamentale per comprendere il comportamento medio delle variabili in studio e per integrare queste informazioni nella struttura complessiva del modello SEM.\n\nRaccolta dei Dati: Per iniziare, è necessario avere a disposizione i dati grezzi o una matrice di covarianza che riassuma le relazioni tra le variabili, oltre alle medie di tutte le variabili. Questi dati servono come input fondamentale per il modello SEM.\nSpecifica del Modello: Nel definire il modello SEM, è cruciale specificare che il modello deve considerare sia la struttura di covarianza (le relazioni tra le variabili) sia la struttura di media (le medie delle variabili). Questo assicura che il modello analizzi sia le relazioni tra le variabili sia i loro valori medi.\nInclusione delle Interfacce e delle Medie: Nel modello, le variabili endogene (quelle che sono influenzate da altre nel modello) richiedono l’inclusione delle intercette, mentre per le variabili esogene (quelle che non sono influenzate da altre nel modello) si considerano le loro medie.\nSpecificare la Costante “1”: Per modellare le medie, si include la costante “1” nell’equazione per tutte le variabili misurate, sia esogene sia endogene. Questo passaggio è fondamentale per dire al software di SEM di considerare le medie nelle sue analisi.\nUso di Software SEM (es. lavaan): In software come lavaan, si può utilizzare una sintassi specifica (ad es. meanstructure = true) per automatizzare l’inclusione delle medie. Questo comando dice al software di aggiungere la costante “1” alle equazioni, facilitando il processo di modellazione delle medie.\nCalcolo del Numero di Osservazioni e Parametri Liberi: Quando si modella sia la struttura di covarianza sia quella di media, è importante calcolare correttamente il numero di osservazioni e parametri liberi nel modello. Si utilizza una formula specifica per determinare questi numeri, considerando il numero di variabili osservate.\n\n\n19.11.1 Interpretazione delle Medie Previste\nIn un modello SEM, la struttura di covarianza genera “covarianze previste”, che possono essere confrontate con le covarianze effettivamente osservate nei dati. Analogamente, la struttura di media del modello produce “medie previste” (o “medie adattate”), che si possono confrontare con le medie osservate delle variabili.\nIl “residuo di media” è fondamentalmente la differenza tra la media osservata e la media prevista per una specifica variabile. Quando i gradi di libertà (df) sono zero per la struttura di media (come può accadere in alcune analisi), tutti i residui di media risultano essere zero. Questo significa che le medie previste dal modello corrispondono esattamente alle medie osservate. Tuttavia, se i gradi di libertà non sono zero, potrebbero esserci delle discrepanze tra le medie previste e quelle osservate.\nPer calcolare la media prevista di una variabile target nel modello, si seguono questi passaggi:\n\nSi inizia con il coefficiente del percorso diretto dalla costante “1” al target. Questo coefficiente rappresenta l’intercetto nella regressione della variabile target sulle altre variabili. In pratica, è un valore che indica la media prevista della variabile target quando tutte le altre variabili indipendenti nel modello sono a zero.\nSi aggiunge a questo il contributo di ogni “variabile genitore” (ovvero, una variabile che influisce sul target). Per ogni genitore, si moltiplica la media prevista di quel genitore per il coefficiente del percorso che collega il genitore alla variabile target. Questa operazione si ripete per tutti i genitori e i risultati si sommano insieme.\n\nIl risultato finale di questo processo è la media prevista per la variabile target nel modello SEM. Questa media prevista è un elemento cruciale per valutare l’aderenza del modello ai dati osservati, confrontando le medie previste con quelle effettivamente osservate.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#commenti-e-considerazioni-finali",
    "href": "chapters/path_analysis/01_path_analysis.html#commenti-e-considerazioni-finali",
    "title": "19  Analisi dei percorsi",
    "section": "19.12 Commenti e considerazioni finali",
    "text": "19.12 Commenti e considerazioni finali\nIl diagramma di un modello parametrico funge anche da mezzo di comunicazione, in quanto un diagramma completo rappresenta essenzialmente un insieme di istruzioni visive su come specificare il modello nella sintassi per computer. Ogni parametro del modello, sia libero sia fisso (ad esempio, le costanti di scala), è rappresentato nei diagrammi basati sul simbolismo grafico RAM di McArdle-McDonald, il quale può anche aiutare i ricercatori che stanno imparando l’analisi SEM a comprendere meglio l’analisi.\nIn pratica, questo significa che guardando il diagramma di un modello parametrico, un ricercatore può ottenere una guida visiva chiara su come impostare e strutturare il modello all’interno di un software di analisi SEM. Ogni elemento del diagramma, come le frecce e i nodi, e ogni annotazione, come le etichette dei parametri o le costanti, fornisce informazioni specifiche su come ciascuna parte del modello dovrebbe essere codificata nella sintassi del software. Questo approccio visuale facilita la comprensione dei complessi rapporti tra le variabili e dei diversi componenti del modello, rendendo più accessibile l’apprendimento e l’applicazione dell’analisi SEM.\nUtilizzando l’analisi dei percorsi per decomporre la correlazione o la covarianza, disponiamo di un metodo efficace per delineare le associazioni tra le variabili e mappare le loro potenziali connessioni causali. Questo strumento si rivela particolarmente utile per descrivere in modo chiaro e strutturato le relazioni tra diverse variabili, facilitando l’interpretazione dei loro legami e interazioni all’interno del modello considerato.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#session-info",
    "href": "chapters/path_analysis/01_path_analysis.html#session-info",
    "title": "19  Analisi dei percorsi",
    "section": "19.13 Session Info",
    "text": "19.13 Session Info\n\nsessionInfo()\n\nR version 4.3.2 (2023-10-31)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.3.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] diagram_1.6.5     shape_1.4.6       rsvg_2.6.0        DiagrammeRsvg_0.1\n [5] lavaanExtra_0.2.0 lavaanPlot_0.8.1  ggokabeito_0.1.0  viridis_0.6.5    \n [9] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    bayesplot_1.11.1 \n[13] gridExtra_2.3     patchwork_1.2.0   semTools_0.5-6    semPlot_1.1.6    \n[17] lavaan_0.6-17     psych_2.4.1       scales_1.3.0      markdown_1.12    \n[21] knitr_1.45        lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1    \n[25] dplyr_1.1.4       purrr_1.0.2       readr_2.1.5       tidyr_1.3.1      \n[29] tibble_3.2.1      ggplot2_3.4.4     tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] RColorBrewer_1.1-3 rstudioapi_0.15.0  jsonlite_1.8.8    \n  [4] magrittr_2.0.3     TH.data_1.1-2      estimability_1.5  \n  [7] nloptr_2.0.3       rmarkdown_2.25     vctrs_0.6.5       \n [10] minqa_1.2.6        base64enc_0.1-3    rstatix_0.7.2     \n [13] htmltools_0.5.7    curl_5.2.0         broom_1.0.5       \n [16] Formula_1.2-5      htmlwidgets_1.6.4  plyr_1.8.9        \n [19] sandwich_3.1-0     emmeans_1.10.0     zoo_1.8-12        \n [22] uuid_1.2-0         igraph_2.0.2       mime_0.12         \n [25] lifecycle_1.0.4    pkgconfig_2.0.3    Matrix_1.6-5      \n [28] R6_2.5.1           fastmap_1.1.1      shiny_1.8.0       \n [31] digest_0.6.34      OpenMx_2.21.11     fdrtool_1.2.17    \n [34] colorspace_2.1-0   rprojroot_2.0.4    Hmisc_5.1-1       \n [37] fansi_1.0.6        timechange_0.3.0   abind_1.4-5       \n [40] compiler_4.3.2     withr_3.0.0        glasso_1.11       \n [43] htmlTable_2.4.2    backports_1.4.1    carData_3.0-5     \n [46] ggsignif_0.6.4     MASS_7.3-60.0.1    corpcor_1.6.10    \n [49] gtools_3.9.5       tools_4.3.2        pbivnorm_0.6.0    \n [52] foreign_0.8-86     zip_2.3.1          httpuv_1.6.14     \n [55] nnet_7.3-19        glue_1.7.0         quadprog_1.5-8    \n [58] DiagrammeR_1.0.11  nlme_3.1-164       promises_1.2.1    \n [61] lisrelToR_0.3      grid_4.3.2         pbdZMQ_0.3-11     \n [64] checkmate_2.3.1    cluster_2.1.6      reshape2_1.4.4    \n [67] generics_0.1.3     gtable_0.3.4       tzdb_0.4.0        \n [70] data.table_1.15.0  hms_1.1.3          car_3.1-2         \n [73] utf8_1.2.4         sem_3.1-15         pillar_1.9.0      \n [76] IRdisplay_1.1      rockchalk_1.8.157  later_1.3.2       \n [79] splines_4.3.2      lattice_0.22-5     survival_3.5-8    \n [82] kutils_1.73        tidyselect_1.2.0   miniUI_0.1.1.1    \n [85] pbapply_1.7-2      V8_4.4.2           stats4_4.3.2      \n [88] xfun_0.42          qgraph_1.9.8       arm_1.13-1        \n [91] visNetwork_2.1.2   stringi_1.8.3      boot_1.3-29       \n [94] evaluate_0.23      codetools_0.2-19   mi_1.1            \n [97] cli_3.6.2          RcppParallel_5.1.7 IRkernel_1.3.2    \n[100] rpart_4.1.23       xtable_1.8-4       repr_1.1.6        \n[103] munsell_0.5.0      Rcpp_1.0.12        coda_0.19-4.1     \n[106] png_0.1-8          XML_3.99-0.16.1    parallel_4.3.2    \n[109] ellipsis_0.3.2     jpeg_0.1-10        lme4_1.1-35.1     \n[112] mvtnorm_1.2-4      openxlsx_4.2.5.2   crayon_1.5.2      \n[115] rlang_1.1.3        multcomp_1.4-25    mnormt_2.1.1      \n\n\n\n\n\n\nKline, Rex B. 2023. Principles and practice of structural equation modeling. Guilford publications.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/fa/01_intro_fa.html",
    "href": "chapters/fa/01_intro_fa.html",
    "title": "20  Introduzione all’analisi fattoriale",
    "section": "",
    "text": "20.1 Caratteristiche dell’analisi fattoriale\nL’analisi fattoriale è una tecnica statistica utilizzata per identificare la struttura latente sottostante a un insieme di variabili osservate, con lo scopo di ridurre la complessità del dato e individuare costrutti latenti che spiegano le relazioni tra le variabili. Le due principali forme di analisi fattoriale sono l’analisi fattoriale esplorativa (EFA) e l’analisi fattoriale confermativa (CFA), che differiscono nel loro scopo e nel grado di definizione a priori della struttura da parte del ricercatore.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduzione all'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/01_intro_fa.html#caratteristiche-dellanalisi-fattoriale",
    "href": "chapters/fa/01_intro_fa.html#caratteristiche-dellanalisi-fattoriale",
    "title": "20  Introduzione all’analisi fattoriale",
    "section": "",
    "text": "20.1.1 Analisi Fattoriale Esplorativa (EFA)\nL’EFA viene utilizzata quando il ricercatore non ha ipotesi a priori su come un gruppo di variabili si strutturi. Il suo scopo è identificare empiricamente il modello che meglio si adatta ai dati, bilanciando precisione e semplicità. Questa tecnica esplora la struttura sottostante ai dati, permettendo di individuare fattori latenti che spiegano la varianza comune tra le variabili osservate. È particolarmente utile nei primi stadi di sviluppo di test psicometrici, quando si desidera identificare le dimensioni latenti sottostanti a un nuovo insieme di item. Tuttavia, la scelta di parametri e metodi di estrazione influisce pesantemente sul risultato finale.\n\n\n20.1.2 Analisi Fattoriale Confermativa (CFA)\nL’CFA è un approccio utilizzato quando il ricercatore ha un modello teorico ben definito e desidera valutare quanto questo modello ipotizzato si adatti ai dati osservati. La CFA consente di confrontare modelli teorici alternativi e valutare quale meglio spiega i dati, tenendo conto di vari fattori come i carichi fattoriali, gli errori e le covarianze. In psicometria, viene comunemente utilizzata per verificare la validità strutturale di un test o questionario, valutando se i dati empirici supportano il modello teorico ipotizzato.\n\n\n20.1.3 Struttura e Componenti dell’Analisi Fattoriale\nIndipendentemente dal tipo di analisi, l’analisi fattoriale si basa sulla distinzione tra variabili osservate (o manifest) e variabili latenti (o fattori). Le variabili latenti rappresentano costrutti teorici non direttamente osservabili, mentre le variabili osservate sono i punteggi effettivi ottenuti da misure dirette. Un modello fattoriale può includere carichi fattoriali, errori, covarianze e percorsi di regressione.\nUn carico fattoriale rappresenta la forza della relazione tra una variabile osservata e il fattore latente, mentre il residuo o errore rappresenta la varianza non spiegata dal fattore latente. Le covarianze esprimono le relazioni tra le variabili o tra i fattori latenti. L’equazione generale di un indicatore osservato \\(X\\) in relazione a un fattore latente \\(F\\) può essere espressa come:\n\\[\nX = \\lambda \\cdot F + \\text{Intercept} + \\text{Errore}\n\\]\ndove:\n\n\\(X\\) è il valore osservato dell’indicatore;\n\\(\\lambda\\) è il carico fattoriale;\n\\(F\\) è il valore del fattore latente;\nIntercept è il valore atteso dell’indicatore quando il fattore latente è zero;\nErrore è la parte di varianza non spiegata dal fattore latente.\n\n\n\n20.1.4 Modelli Fattoriali Gerarchici e Bifattoriali\nEsistono varianti più avanzate dell’analisi fattoriale, come i modelli gerarchici e i modelli bifattoriali, che permettono di rappresentare strutture latenti più complesse. In particolare, i modelli bifattoriali sono utili quando si ritiene che un insieme di variabili possa essere spiegato sia da un fattore generale che da fattori specifici. Ad esempio, nel contesto della misurazione dell’intelligenza, un modello bifattoriale potrebbe includere un fattore generale (g) che spiega la varianza comune tra tutte le variabili, e fattori specifici che spiegano varianze più circoscritte a singoli domini cognitivi.\n\n\n20.1.5 Sviluppo Storico dell’Analisi Fattoriale\nL’analisi fattoriale è stata sviluppata all’inizio del XX secolo da Charles Spearman per studiare la struttura dell’intelligenza. Spearman introdusse il concetto di fattore generale (g), che rappresentava la dimensione comune che spiegava la covarianza tra diverse abilità cognitive. Successivamente, psicologi come Thurstone criticarono il modello unifattoriale di Spearman e proposero un modello multifattoriale, che permetteva di individuare più fattori specifici, ciascuno dei quali spiegava una dimensione distinta dell’intelligenza.\nNegli anni ’60 e ’70, l’analisi fattoriale subì una trasformazione con lo sviluppo dei modelli di equazioni strutturali (SEM), che combinavano l’analisi fattoriale con la path analysis per rappresentare relazioni più complesse tra variabili osservate e latenti. Questo sviluppo permise ai ricercatori di verificare ipotesi teoriche più articolate riguardanti la struttura di costrutti psicologici complessi.\n\n\n20.1.6 Applicazioni dell’Analisi Fattoriale\nL’analisi fattoriale è ampiamente utilizzata nello sviluppo di strumenti psicometrici, come i test di intelligenza, le scale di personalità e i questionari di auto-valutazione. Viene utilizzata per valutare la validità di costrutto, ovvero la capacità di uno strumento di misurare effettivamente il costrutto teorico che si propone di valutare. Inoltre, l’analisi fattoriale può essere impiegata per esaminare la validità discriminante, ovvero la capacità di uno strumento di distinguere tra costrutti correlati ma distinti.\nInfine, l’analisi fattoriale è uno strumento fondamentale per individuare variabili latenti sottostanti e semplificare i dati complessi, consentendo ai ricercatori di ridurre grandi set di variabili osservate a un insieme più ristretto di fattori interpretabili. La sua applicazione, tuttavia, richiede attenzione nella scelta dei parametri e delle assunzioni, poiché le decisioni prese nel processo di analisi possono influenzare significativamente i risultati finali.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduzione all'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html",
    "href": "chapters/fa/02_analisi_fattoriale_1.html",
    "title": "21  Il modello unifattoriale",
    "section": "",
    "text": "21.1 Introduzione\nQuesto capitolo presenta le nozioni fondamentali dell’analisi fattoriale, un modello statistico che consente di spiegare le correlazioni tra variabili osservate mediante la loro saturazione in uno o più fattori generali. In questo modello, le \\(p\\) variabili osservate (item) sono considerate condizionalmente indipendenti rispetto a \\(m\\) variabili latenti chiamate fattori. L’obiettivo dell’analisi fattoriale è di interpretare questi fattori come costrutti teorici inosservabili. Ad esempio, l’analisi fattoriale può essere utilizzata per spiegare le correlazioni tra le prestazioni di un gruppo di individui in una serie di compiti mediante il concetto di intelligenza. In questo modo, l’analisi fattoriale aiuta a identificare i costrutti cui gli item si riferiscono e a stabilire in che misura ciascun item rappresenta il costrutto. Il modello può essere unifattoriale (\\(m = 1\\)) o multifattoriale (\\(m &gt; 1\\)), e in questo capitolo si introdurrà il modello unifattoriale che assume l’esistenza di un unico fattore comune latente.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#errore-di-misura",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#errore-di-misura",
    "title": "21  Il modello unifattoriale",
    "section": "21.2 Errore di misura",
    "text": "21.2 Errore di misura\nL’analisi fattoriale concettualizza ogni misura osservabile \\(y\\) come risultante dalla combinazione lineare del punteggio reale associato al costrutto latente \\(\\xi\\), e di un elemento di errore di misura non osservato \\(\\delta\\). In questo contesto, il valore misurato di \\(y\\) è interpretato come il prodotto del punteggio reale latente, ponderato da un coefficiente di carico fattoriale \\(\\lambda\\), a cui si aggiunge un termine di errore specifico di misura \\(\\delta_y\\). Per illustrare con un esempio pratico, nel caso di una bilancia non completamente affidabile, ogni lettura del peso corporeo riflette sia il peso effettivo che un errore di misura intrinseco alla bilancia, che si manifesta con variazioni aleatorie da una lettura all’altra. In questa situazione, il modello fattoriale può essere formalizzato attraverso l’equazione:\n\\[\ny = \\lambda\\xi + \\delta_{y}.\n\\]\nQuando si analizzano multiple misure osservabili \\(y\\) che rappresentano lo stesso costrutto latente \\(\\xi\\), diventa fattibile la stima del punteggio reale latente \\(\\xi\\) insieme alla componente di errore di misura \\(\\delta\\), migliorando così la comprensione e l’accuratezza dell’interpretazione dei dati.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#modello-monofattoriale",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#modello-monofattoriale",
    "title": "21  Il modello unifattoriale",
    "section": "21.3 Modello monofattoriale",
    "text": "21.3 Modello monofattoriale\nCon \\(p\\) variabili manifeste \\(y_i\\), il caso più semplice è quello di un solo fattore comune:\n\\[\n\\begin{equation}\ny_i = \\mu_i + \\lambda_{i} \\xi +  1 \\cdot \\delta_i \\qquad i=1, \\dots, p,\n\\end{equation}\n\\tag{21.1}\\]\ndove \\(\\xi\\) rappresenta il fattore comune a tutte le \\(y_i\\), \\(\\delta_i\\) sono i fattori specifici o unici di ogni variabile osservata e \\(\\lambda_i\\) sono le saturazioni (o pesi) fattoriali le quali stabiliscono il peso del fattore latente su ciascuna variabile osservata.\nIl modello di analisi fattoriale e il modello di regressione possono sembrare simili, ma presentano alcune differenze importanti. In primo luogo, sia il fattore comune \\(\\xi\\) sia i fattori specifici \\(\\delta_i\\) sono inosservabili, il che rende tutto ciò che si trova a destra dell’uguaglianza incognito. In secondo luogo, l’analisi di regressione e l’analisi fattoriale hanno obiettivi diversi. L’analisi di regressione mira a individuare le variabili esplicative, osservabili direttamente, che sono in grado di spiegare la maggior parte della varianza della variabile dipendente. Al contrario, il problema dell’analisi unifattoriale consiste nell’identificare la variabile esplicativa inosservabile che è in grado di spiegare la maggior parte della covarianza tra le variabili osservate.\nSolitamente, per comodità, si assume che la media delle variabili osservate \\(y_i\\) sia zero, ovvero \\(\\mu_i=0\\). Ciò equivale a considerare gli scarti delle variabili rispetto alle rispettive medie. Il modello unifattoriale assume che le variabili osservate siano il risultato della combinazione lineare di un fattore comune \\(\\xi\\) e dei fattori specifici \\(\\delta_i\\), ovvero:\n\\[\n\\begin{equation}\ny_i -\\mu_i = \\lambda_i \\xi + 1 \\cdot \\delta_i,\n\\end{equation}\n\\tag{21.2}\\]\ndove \\(\\lambda_i\\) è la saturazione o il peso della variabile \\(i\\)-esima sul fattore comune e \\(\\delta_i\\) rappresenta il fattore specifico della variabile \\(i\\)-esima. Si assume che il fattore comune abbia media zero e varianza unitaria, mentre i fattori specifici abbiano media zero, varianza \\(\\psi_{i}\\) e siano incorrelati tra loro e con il fattore comune. Nel modello unifattoriale, l’interdipendenza tra le variabili è completamente spiegata dal fattore comune.\nLe ipotesi precedenti consentono di ricavare la covarianza tra la variabile osservata \\(y_i\\) e il fattore comune, la varianza della variabile osservata \\(y_i\\) e la covarianza tra due variabili osservate \\(y_i\\) e \\(y_k\\). L’obiettivo della discussione in questo capitolo è appunto quello di analizzare tali grandezze statistiche.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#correlazione-parziale",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#correlazione-parziale",
    "title": "21  Il modello unifattoriale",
    "section": "21.4 Correlazione parziale",
    "text": "21.4 Correlazione parziale\nPrima di entrare nel dettaglio del modello statistico dell’analisi fattoriale, è importante chiarire il concetto di correlazione parziale. Si attribuisce spesso a Charles Spearman la nascita dell’analisi fattoriale. Nel 1904, Spearman pubblicò un articolo intitolato “General Intelligence, Objectively Determined and Measured” in cui propose la Teoria dei Due Fattori. In questo articolo, dimostrò come fosse possibile identificare un fattore inosservabile a partire da una matrice di correlazioni, utilizzando il metodo dell’annullamento della tetrade (tetrad differences). L’annullamento della tetrade è un’applicazione della teoria della correlazione parziale che mira a stabilire se, controllando un insieme di variabili inosservabili chiamate fattori \\(\\xi_j\\), le correlazioni tra le variabili osservabili \\(Y_i\\), al netto degli effetti lineari delle \\(\\xi_j\\), diventino statisticamente nulle.\nPossiamo considerare un esempio con tre variabili: \\(Y_1\\), \\(Y_2\\) e \\(F\\). La correlazione tra \\(Y_1\\) e \\(Y_2\\), \\(r_{1,2}\\), può essere influenzata dalla presenza di \\(F\\). Per calcolare la correlazione parziale tra \\(Y_1\\) e \\(Y_2\\) al netto dell’effetto lineare di \\(F\\), dobbiamo trovare le componenti di \\(Y_1\\) e \\(Y_2\\) che sono linearmente indipendenti da \\(F\\).\nPer fare ciò, dobbiamo trovare la componente di \\(Y_1\\) che è ortogonale a \\(F\\). Possiamo calcolare i residui \\(E_1\\) del modello:\n\\[\nY_1 = b_{01} + b_{11}F + E_1.\n\\]\nLa componente di \\(Y_1\\) linearmente indipendente da \\(F\\) è quindi data dai residui \\(E_1\\). Possiamo eseguire un’operazione analoga per \\(Y_2\\) per trovare la sua componente ortogonale a \\(F\\). Calcolando la correlazione tra le due componenti così ottenute si ottiene la correlazione parziale tra \\(Y_1\\) e \\(Y_2\\) al netto dell’effetto lineare di \\(F\\).\nL’Equazione 21.3 consente di calcolare la correlazione parziale tra \\(Y_1\\) e \\(Y_2\\) al netto dell’effetto di \\(F\\) a partire dalle correlazioni semplici tra le tre variabili \\(Y_1\\), \\(Y_2\\) e \\(F\\).\n\\[\n\\begin{equation}\nr_{1,2 \\mid F} = \\frac{r_{12} - r_{1F}r_{2F}}{\\sqrt{(1-r_{1F}^2)(1-r_{2F}^2)}}.\n\\end{equation}\n\\tag{21.3}\\]\nIn particolare, la correlazione parziale \\(r_{1,2 \\mid F}\\) è data dalla differenza tra la correlazione \\(r_{12}\\) tra \\(Y_1\\) e \\(Y_2\\) e il prodotto tra le correlazioni \\(r_{1F}\\) e \\(r_{2F}\\) tra ciascuna delle due variabili e \\(F\\), il tutto diviso per la radice quadrata del prodotto delle differenze tra 1 e i quadrati delle correlazioni tra \\(Y_1\\) e \\(F\\) e tra \\(Y_2\\) e \\(F\\). In altre parole, la formula tiene conto dell’effetto di \\(F\\) sulle correlazioni tra \\(Y_1\\) e \\(Y_2\\) per ottenere una stima della relazione diretta tra le due variabili, eliminando l’effetto del fattore comune.\nConsideriamo un esempio numerico. Sia \\(f\\) una variabile su cui misuriamo \\(n\\) valori\n\nset.seed(123)\nn &lt;- 1000\nf &lt;- rnorm(n, 24, 12)\n\nSiano \\(y_1\\) e \\(y_2\\) funzioni lineari di \\(f\\), a cui viene aggiunta una componente d’errore gaussiano:\n\ny1 &lt;- 10 + 7 * f + rnorm(n, 0, 50)\ny2 &lt;- 3  + 2 * f + rnorm(n, 0, 50)\n\nLa correlazione tra \\(y_1\\) e \\(y_2\\) (\\(r_{12}= 0.355\\)) deriva dal fatto che \\(\\hat{y}_1\\) e \\(\\hat{y}_2\\) sono entrambe funzioni lineari di \\(f\\):\n\nY &lt;- cbind(y1, y2, f)\nR &lt;- cor(Y)\nround(R, 3)\n\n\nA matrix: 3 x 3 of type dbl\n\n\n\ny1\ny2\nf\n\n\n\n\ny1\n1.000\n0.380\n0.867\n\n\ny2\n0.380\n1.000\n0.423\n\n\nf\n0.867\n0.423\n1.000\n\n\n\n\n\nEseguiamo le regressioni di \\(y_1\\) su \\(f\\) e di \\(y_2\\) su \\(F\\):\n\nfm1 &lt;- lm(y1 ~ f)\nfm2 &lt;- lm(y2 ~ f)\n\nNella regressione, ciascuna osservazione \\(y_{i1}\\) viene scomposta in due componenti linearmente indipendenti, i valori adattati \\(\\hat{y}_{i}\\) e i residui, \\(e_{i}\\): \\(y_i = \\hat{y}_i + e_1\\). Nel caso di \\(y_1\\) abbiamo\n\nround(head(cbind(y1, y1.hat=fm1$fit, e=fm1$res, fm1$fit+fm1$res)), 3)\n\n\nA matrix: 6 x 4 of type dbl\n\n\n\ny1\ny1.hat\ne\n\n\n\n\n\n1\n81.130\n130.505\n-49.375\n81.130\n\n\n2\n106.667\n159.704\n-53.037\n106.667\n\n\n3\n308.032\n317.846\n-9.813\n308.032\n\n\n4\n177.314\n186.285\n-8.971\n177.314\n\n\n5\n61.393\n191.482\n-130.089\n61.393\n\n\n6\n374.094\n331.668\n42.426\n374.094\n\n\n\n\n\nLo stesso può dirsi di \\(y_2\\). La correlazione parziale \\(r_{12 \\mid f}\\) tra \\(y_1\\) e \\(y_2\\) dato \\(f\\) è uguale alla correlazione di Pearson tra i residui \\(e_1\\) e \\(e_2\\) calcolati mediante i due modelli di regressione descritti sopra:\n\ncor(fm1$res, fm2$res)\n\n0.0282861771586006\n\n\nLa correlazione parziale tra \\(y_1\\) e \\(y_2\\) al netto di \\(f\\) è .02829.\nPer i dati esaminati sopra, dunque, la correlazione parziale tra le variabili \\(y_1\\) e \\(y_2\\) diventa uguale a zero se la variabile \\(f\\) viene controllata (ovvero, se escludiamo da \\(y_1\\) e da \\(y_2\\) l’effetto lineare di \\(f\\)). Il fatto che la correlazione parziale sia zero significa che la correlazione che abbiamo osservato tra \\(y_1\\) e \\(y_2\\) (\\(r = 0.355\\)) non dipendeva dall’effetto che una variabile \\(y\\) esercitava sull’altra, ma bensì dal fatto che c’era una terza variabile, \\(f\\), che influenzava sia \\(y_1\\) sia \\(y_2\\). In altre parole, le variabili \\(y_1\\) e \\(y_2\\) sono condizionalmente indipendenti dato \\(f\\). Ciò significa, come abbiamo visto sopra, che la componente di \\(y_1\\) linearmente indipendente da \\(f\\) è incorrelata con la componente di \\(y_2\\) linearmente indipendente da \\(f\\).\nLa correlazione che abbiamo calcolato tra i residui di due modelli di regressione è identica alla correlazione che viene calcolata applicando l’Equazione 21.3:\n\n(R[1, 2] - R[1, 3] * R[2, 3]) / \n  sqrt((1 - R[1, 3]^2) * (1- R[2, 3]^2)) %&gt;% \n  round(3)\n\n0.0282751285315664",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#principio-base-dellanalisi-fattoriale",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#principio-base-dellanalisi-fattoriale",
    "title": "21  Il modello unifattoriale",
    "section": "21.5 Principio base dell’analisi fattoriale",
    "text": "21.5 Principio base dell’analisi fattoriale\nAttualmente, l’inferenza statistica nell’analisi fattoriale spesso si svolge mediante il calcolo di stime della massima verosimiglianza ottenute mediante procedure iterative. All’inizio dell’analisi fattoriale, tuttavia, la procedura di estrazione dei fattori faceva leva sulle relazioni invarianti che il modello fattoriale impone agli elementi della matrice di covarianza delle variabili osservate. Il più conosciuto tra tali invarianti è la tetrade che si presenta nei modelli ad un fattore.\nLa tetrade è una combinazione di quattro correlazioni. Se l’associazione osservata tra le variabili dipende effettivamente dal fatto che le variabili in questione sono state causalmente generate da un fattore comune inosservabile, allora è possibile generare una combinazione delle correlazioni tra le variabili che porta all’annullamento della tetrade. In altre parole, l’analisi fattoriale si chiede se esiste un insieme esiguo di \\(m&lt;p\\) variabili inosservabili che rendono significativamente nulle tutte le correlazioni parziali tra le \\(p\\) variabili osservate al netto dei fattori comuni. Se il metodo della correlazione parziale consente di identificare \\(m\\) variabili latenti, allora lo psicologo conclude che tali fattori corrispondono agli \\(m\\) costrutti che intende misurare.\nPer chiarire il metodo dell’annullamento della tetrade consideriamo la matrice di correlazioni riportata nella Tabella successiva. Nella tabella, la correlazione parziale tra ciascuna coppia di variabili \\(y_i\\), \\(y_j\\) (con \\(i \\neq j\\)) dato \\(\\xi\\) è sempre uguale a zero. Ad esempio, la correlazione parziale tra \\(y_3\\) e \\(y_5\\) dato \\(\\xi\\) è:\n\\[\n\\begin{align}\n  r_{35 \\mid \\xi} &= \\frac{r_{35} - r_{3\\xi}r_{5\\xi}}\n  {\\sqrt{(1-r_{3\\xi}^2)(1-r_{5\\xi}^2)}} \\notag \\\\[12pt]\n  &= \\frac{0.35 - 0.7 \\times 0.5}\n  {\\sqrt{(1-0.7^2)(1-0.5^2)}} = 0. \\notag\n\\end{align}\n\\]\nLo stesso risultato si trova per qualunque altra coppia di variabili \\(y_i\\) e \\(y_j\\), ovvero \\(r_{ij \\mid \\xi} = 0\\).\n\n\n\n\n\\(\\xi\\)\n\\(y_1\\)\n\\(y_2\\)\n\\(y_3\\)\n\\(y_4\\)\n\\(y_5\\)\n\n\n\n\n\\(\\xi\\)\n1.00\n\n\n\n\n\n\n\n\\(y_1\\)\n0.90\n1.00\n\n\n\n\n\n\n\\(y_2\\)\n0.80\n0.72\n1.00\n\n\n\n\n\n\\(y_3\\)\n0.70\n0.63\n0.56\n1.00\n\n\n\n\n\\(y_4\\)\n0.60\n0.54\n0.48\n0.42\n1.00\n\n\n\n\\(y_5\\)\n0.50\n0.45\n0.40\n0.35\n0.30\n1.00\n\n\n\nPossiamo dunque dire che, per la matrice di correlazioni della Tabella, esiste un’unica variabile \\(\\xi\\) la quale, quando viene controllata, spiega tutte le\n\\[p(p-1)/2 = 5(5-1)/2=10\\]\ncorrelazioni tra le variabili \\(y\\). Questo risultato non è sorprendente, in quanto la matrice di correlazioni della Tabella è stata costruita in modo tale da possedere tale proprietà.\nMa supponiamo di essere in una situazione diversa, ovvero di avere osservato soltanto le variabili \\(y_i\\) e di non conoscere \\(\\xi\\). In tali circostanze ci possiamo porre la seguente domanda: Esiste una variabile inosservabile \\(\\xi\\) la quale, se venisse controllata, renderebbe uguali a zero tutte le correlazioni parziali tra le variabili \\(y\\)? Se una tale variabile inosservabile esiste, ed è in grado di spiegare tutte le correlazioni tra le variabili osservate \\(y\\), allora essa viene chiamata fattore. Arriviamo dunque alla seguente definizione:\nUn fattore è una variabile inosservabile in grado di rendere significativamente nulle tutte le correlazioni parziali tra le variabili manifeste.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#vincoli-sulle-correlazioni",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#vincoli-sulle-correlazioni",
    "title": "21  Il modello unifattoriale",
    "section": "21.6 Vincoli sulle correlazioni",
    "text": "21.6 Vincoli sulle correlazioni\nCome si può stabilire se esiste una variabile inosservabile in grado di rendere nulle tutte le correlazioni parziali tra le variabili osservate? Riscriviamo l’Equazione 21.3 per specificare la correlazione parziale tra le variabili \\(y_i\\) e \\(y_j\\) dato \\(\\xi\\):\n\\[\n\\begin{align}\n  r_{ij \\mid \\xi} &= \\frac{r_{ij} - r_{i\\xi}r_{j\\xi}}\n  {\\sqrt{(1-r_{i\\xi}^2)(1-r_{j\\xi}^2)}}\n\\end{align}\n\\]\nAffinché \\(r_{ij \\mid \\xi}\\) sia uguale a zero è necessario che\n\\[\nr_{ij} - r_{i\\xi}r_{j\\xi}=0\n\\]\novvero\n\\[\n\\begin{equation}\nr_{ij} = r_{i\\xi}r_{j\\xi}.\n\\end{equation}\n\\]\nIn altri termini, se esiste un fattore non osservato \\(\\xi\\) in grado di rendere uguali a zero tutte le correlazioni parziali \\(r_{ih \\mid \\xi}\\), allora la correlazione tra ciascuna coppia di variabili \\(y\\) deve essere uguale al prodotto delle correlazioni tra ciascuna \\(y\\) e il fattore latente \\(\\xi\\). Questo è il principio base dell’analisi fattoriale.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#teoria-dei-due-fattori",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#teoria-dei-due-fattori",
    "title": "21  Il modello unifattoriale",
    "section": "21.7 Teoria dei Due Fattori",
    "text": "21.7 Teoria dei Due Fattori\nPer fare un esempio concreto relativo al metodo dell’annullamento della tetrade, esaminiamo la matrice di correlazioni originariamente analizzata da Spearman. Spearman (1904) raccolse alcune misure di capacità intellettuale su un piccolo numero di studenti di una scuola superiore. Nello specifico, esaminò i voti di tali studenti nelle seguenti materie: studio dei classici (\\(c\\)), letteratura inglese (\\(e\\)) e abilità matematiche (\\(m\\)). Considerò anche la prestazione in un compito di discriminazione dell’altezza di suoni (“pitch discrimination”) (\\(p\\)), ovvero un’abilità diversa da quelle richieste nei test scolastici.\nSecondo la Teoria dei Due Fattori, le prestazioni relative ad un determinato compito intellettuale possiedono una componente comune (detta fattore ‘g’) con le prestazioni in un qualunque altro compito intellettuale e una componente specifica a quel determinato compito. Il modello dell’intelligenza di Spearman prevede dunque due fattori, uno generale e uno specifico (detto fattore ‘s’). Il fattore ‘g’ costituisce la componente invariante dell’abilità intellettiva, mente il fattore ‘s’ è una componente che varia da condizione a condizione.\nCome è possibile stabilire se esiste una variabile latente in grado di spiegare le correlazioni tra le variabili osservate da Spearman? Lo strumento proposto da Spearman per rispondere a questa domanda è l’annullamento della tetrade. L’annullamento della tetrade utilizza i vincoli sulle correlazioni che derivano dalla definizione di correlazione parziale. In precedenza abbiamo visto che la correlazione parziale tra le variabili \\(y\\) indicizzate da \\(i\\) e \\(j\\), al netto dell’effetto di \\(\\xi\\), è nulla se\n\\[\nr_{ij} = r_{i\\xi}r_{j\\xi}.\n\\]\nNel caso dei dati di Spearman, dunque, le correlazioni parziali sono nulle se la correlazione tra ‘’studi classici’’ e ‘’letteratura inglese’’ è uguale al prodotto della correlazione tra ‘’studi classici’’ e il fattore \\(\\xi\\) e della correlazione tra ‘’letteratura inglese’’ e il fattore \\(\\xi\\). Inoltre, la correlazione tra ‘’studi classici’’ e ‘’abilità matematica’’ deve essere uguale al prodotto della correlazione tra ‘’studi classici’’ e il fattore \\(\\xi\\) e della correlazione tra ‘’abilità matematica’’ e il fattore \\(\\xi\\); e così via.\nLe correlazioni tra le variabili manifeste e il fattore latente sono dette e vengono denotate con la lettera \\(\\lambda\\). Se il modello di Spearman è corretto, avremo che\n\\[r_{ec}=\\lambda_e \\times \\lambda_{c},\\]\ndove \\(r_{ec}\\) è la correlazione tra ‘’letteratura inglese’’ (e) e ‘’studi classici’’ (c), \\(\\lambda_e\\) è la correlazione tra ‘’letteratura inglese’’ e \\(\\xi\\), e \\(\\lambda_{c}\\) è la correlazione tra ‘’studi classici’’ e \\(\\xi\\).\nAllo stesso modo, la correlazione tra ‘’studi classici’’ e ‘’matematica’’ (m) dovrà essere uguale a\n\\[\\lambda_c \\times \\lambda_m,\\]\neccetera.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#annullamento-della-tetrade",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#annullamento-della-tetrade",
    "title": "21  Il modello unifattoriale",
    "section": "21.8 Annullamento della tetrade",
    "text": "21.8 Annullamento della tetrade\nDate le correlazioni tra tre coppie di variabili manifeste, il metodo dell’annullamento della tetrade\n\nin una matrice di correlazione, si selezionino quattro coefficienti nelle posizioni che marcano gli angoli di un rettangolo. La differenza tra i prodotti dei coefficienti che giacciono sulle due diagonali di tale rettangolo costituisce la differenza delle tetradi e deve essere uguale a zero.\n\nrende possibile stimare i valori delle saturazioni fattoriali \\(\\lambda\\). Ad esempio, per le variabili \\(c\\), \\(m\\) ed \\(e\\), possiamo scrivere le seguenti tre equazioni in tre incognite:\n\\[\n\\begin{align}\n  r_{cm} &= \\lambda_c \\times \\lambda_m, \\notag \\\\\n  r_{em} &= \\lambda_e \\times \\lambda_m,  \\\\\n  r_{ce} &= \\lambda_c \\times \\lambda_e. \\notag\n\\end{align}\n\\]\nRisolvendo il precedente sistema di equazioni lineari, il coefficiente di saturazione \\(\\lambda_m\\) della variabile \\(y_m\\) nel fattore comune \\(\\xi\\), ad esempio, pu{`o} essere calcolato a partire dalle correlazioni tra le variabili manifeste \\(c\\), \\(m\\), ed \\(e\\) nel modo seguente\\footnote{ La terza delle equazioni del sistema lineare può essere riscritta come \\(\\lambda_c = \\frac{r_{ce}}{\\lambda_e}\\).\nUtilizzando tale risultato, la prima equazione diventa \\(r_{cm} = \\frac{r_{ce}}{\\lambda_e}\\lambda_m\\). Dalla seconda equazione otteniamo \\(\\lambda_e = \\frac{r_{em}}{\\lambda_m}\\). Sostituendo questo risultato nell’equazione precedente otteniamo \\(r_{cm} = \\frac{r_{ce}}{r_{em}}\\lambda_m^2\\), quindi \\(\\lambda_m^2 = \\frac{r_{cm} r_{em} }{r_{ce}}\\).\nVerifichiamo: \\(\\frac{r_{cm} r_{em}}{r_{ce}} = \\frac{\\lambda_c \\lambda_m \\lambda_e \\lambda_m}{\\lambda_c \\lambda_e} = \\lambda_m^2\\).\n\\[\n\\begin{align}\n  \\lambda_m &= \\sqrt{\n    \\frac{r_{cm} r_{em}}{r_{ce}}\n    }.\n\\end{align}\n\\tag{21.4}\\]\nLo stesso vale per le altre due saturazioni \\(\\lambda_c\\) e \\(\\lambda_e\\).\nNel suo articolo del 1904, Spearman osservò le seguenti correlazioni tra le variabili \\(Y_c\\), \\(Y_e\\), \\(Y_m\\) e \\(Y_p\\):\n\\[\n\\begin{array}{ccccc}\n  \\hline\n    & Y_C & Y_E & Y_M & Y_P \\\\\n  \\hline\n  Y_C & 1.00 & 0.78 & 0.70 & 0.66 \\\\\n  Y_E &   & 1.00 & 0.64 & 0.54 \\\\\n  Y_M &   &   & 1.00 & 0.45 \\\\\n  Y_P &   &   &   & 1.00 \\\\\n  \\hline\n\\end{array}\n\\]\nUtilizzando l’Equazione 21.4, mediante le correlazioni \\(r_{cm}\\), \\(r_{em}\\), e \\(r_{ce}\\) fornite dalla tabella precedente, la saturazione \\(\\lambda_m\\) diventa uguale a:\n\\[\n\\begin{align}\n  \\hat{\\lambda}_m &= \\sqrt{ \\frac{r_{cm} r_{em}}{r_{ce}} } = \\sqrt{\n    \\frac{0.70 \\times 0.64}{0.78} } = 0.76. \\notag\n\\end{align}\n\\]\nÈ importante notare che il metodo dell’annullamento della tetrade produce risultati falsificabili. Infatti, ci sono modi diversi per calcolare la stessa saturazione fattoriale. Se il modello fattoriale è corretto si deve ottenere lo stesso risultato in tutti i casi.\nNel caso presente, la saturazione fattoriale \\(\\lambda_m\\) può essere calcolata in altri due modi:\n\\[\n\\begin{align}\n  \\hat{\\lambda}_m &= \\sqrt{ \\frac{r_{cm} r_{mp}}{r_{cp}} } = \\sqrt{ \\frac{0.78 \\times 0.45}{0.66} } = 0.69, \\notag \\\\\n  \\hat{\\lambda}_m &= \\sqrt{ \\frac{r_{em} r_{mp}}{r_{ep}} } = \\sqrt{\n    \\frac{0.64 \\times 0.45}{0.54} } = 0.73. \\notag\n\\end{align}\n\\]\nI tre valori che sono stati ottenuti sono molto simili. Qual è allora la stima migliore di \\(\\lambda_m\\)?",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#metodo-del-centroide",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#metodo-del-centroide",
    "title": "21  Il modello unifattoriale",
    "section": "21.9 Metodo del centroide",
    "text": "21.9 Metodo del centroide\nLa soluzione più semplice è quella di fare la media di questi tre valori (\\(\\bar{\\lambda}_m = 0.73\\)). Un metodo migliore (meno vulnerabile ai valori anomali) è dato dal rapporto tra la somma dei numeratori e dei denominatori:\n\\[\n\\begin{align}\n  \\hat{\\lambda}_m &= \\sqrt{ \\frac{0.70 \\times 0.64 + 0.78 \\times 0.45 + 0.64\n      \\times 0.45}{0.78+0.66+0.54} } = 0.73 \\notag\n\\end{align}\n\\]\nIn questo caso, i due metodi danno lo stesso risultato. Le altre tre saturazioni fattoriali trovate mediante il metodo del centroide sono:\n\\[\\hat{\\lambda}_c = 0.97, \\quad \\hat{\\lambda}_e = 0.84, \\quad \\hat{\\lambda}_p = 0.65.\\]\nIn conclusione,\n\\[\n\\boldsymbol{\\hat{\\Lambda}}'=\n(\\hat{\\lambda}_c, \\hat{\\lambda}_e, \\hat{\\lambda}_m, \\hat{\\lambda}_p) = (0.97, 0.84, 0.73, 0.65).\n\\]\nQuesto risultato è la soluzione proposta da Spearman nel suo articolo del 1904 per risolvere il problema di determinare le saturazioni fattoriali di un modello con un fattore comune latente.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#introduzione-a-lavaan",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#introduzione-a-lavaan",
    "title": "21  Il modello unifattoriale",
    "section": "21.10 Introduzione a lavaan",
    "text": "21.10 Introduzione a lavaan\nAttualmente, l’analisi fattoriale viene svolta mediante software. Il pacchetto R più ampiamente utilizzato per condurre l’analisi fattoriale è lavaan.\n\n21.10.1 Sintassi del modello\nAl cuore del pacchetto lavaan si trova la “sintassi del modello”. La sintassi del modello è una descrizione del modello da stimare. In questa sezione, spieghiamo brevemente gli elementi della sintassi del modello lavaan.\nNell’ambiente R, una formula di regressione ha la seguente forma:\ny ~ x1 + x2 + x3 + x4\nIn questa formula, la tilde (“~”) è l’operatore di regressione. Sul lato sinistro dell’operatore, abbiamo la variabile dipendente (y), e sul lato destro abbiamo le variabili indipendenti, separate dall’operatore “+” . In lavaan, un modello tipico è semplicemente un insieme (o sistema) di formule di regressione, in cui alcune variabili (che iniziano con una ‘f’ qui sotto) possono essere latenti. Ad esempio:\ny ~ f1 + f2 + x1 + x2\nf1 ~ f2 + f3\nf2 ~ f3 + x1 + x2\nSe abbiamo variabili latenti in una qualsiasi delle formule di regressione, dobbiamo “definirle” elencando i loro indicatori (manifesti o latenti). Lo facciamo utilizzando l’operatore speciale “=~”, che può essere letto come “è misurato da”. Ad esempio, per definire le tre variabili latenti f1, f2 e f3, possiamo usare la sintassi seguente:\nf1 =~ y1 + y2 + y3\nf2 =~ y4 + y5 + y6\nf3 =~ y7 + y8 + y9 + y10\nInoltre, le varianze e le covarianze sono specificate utilizzando un operatore “doppia tilde”, ad esempio:\ny1 ~~ y1 # varianza\ny1 ~~ y2 # covarianza\nf1 ~~ f2 # covarianza\nE infine, le intercette per le variabili osservate e latenti sono semplici formule di regressione con solo una intercetta (esplicitamente indicato dal numero “1”) come unico predittore:\ny1 ~ 1\nf1 ~ 1\nUtilizzando questi quattro tipi di formule, è possibile descrivere una vasta gamma di modelli di variabili latenti. L’attuale insieme di tipi di formula è riassunto nella tabella sottostante.\n\n\n\ntipo di formula\noperatore\nmnemonic\n\n\n\n\ndefinizione variabile latente\n=~\nè misurato da\n\n\nregressione\n~\nviene regredito su\n\n\n(co)varianza (residuale)\n~~\nè correlato con\n\n\nintercetta\n~ 1\nintercetta\n\n\n\nUna sintassi completa del modello lavaan è semplicemente una combinazione di questi tipi di formule, racchiusi tra virgolette singole. Ad esempio:\nmy_model &lt;- ' \n  # regressions\n  y1 + y2 ~ f1 + f2 + x1 + x2\n  f1 ~ f2 + f3\n  f2 ~ f3 + x1 + x2\n\n  # latent variable definitions \n  f1 =~ y1 + y2 + y3 \n  f2 =~ y4 + y5 + y6 \n  f3 =~ y7 + y8 + y9\n  \n  # variances and covariances \n  y1 ~~ y1 \n  y1 ~~ y2 \n  f1 ~~ f2\n\n  # intercepts \n  y1 ~ 1 \n  f1 ~ 1\n'\nPer adattare il modello ai dati usiamo la seguente sintassi.\nfit &lt;- cfa(model = my_model, data = my_data)\n\n\n21.10.2 Un esempio concreto\nAnalizziamo nuovamente i dati di Spearman che abbiamo esaminato in precedenza usando lavaan. La matrice completa dei dati di Spearman è messa a disposizione da Kan, Maas, e Levine (2019).\nSpecifichiamo il nome delle variabili manifeste\n\nvarnames &lt;- c(\n  \"Classics\", \"French\", \"English\", \"Math\", \"Pitch\", \"Music\"\n)\n\ne il loro numero\n\nny &lt;- length(varnames)\n\nCreiamo la matrice di correlazione:\n\nspearman_cor_mat &lt;- matrix(\n  c(\n    1.00,  .83,  .78,  .70,  .66,  .63,\n     .83, 1.00,  .67,  .67,  .65,  .57,\n     .78,  .67, 1.00,  .64,  .54,  .51,\n     .70,  .67,  .64, 1.00,  .45,  .51,\n     .66,  .65,  .54,  .45, 1.00,  .40,\n     .63,  .57,  .51,  .51,  .40, 1.00\n  ),\n  ny, ny,\n  byrow = TRUE,\n  dimnames = list(varnames, varnames)\n)\n\nSpecifichiamo l’ampiezza campionaria:\n\nn &lt;- 33\n\nDefiniamo il modello unifattoriale in lavaan. L’operatore =~ si può leggere dicendo che la variabile latente a sinistra dell’operatore viene identificata dalle variabili manifeste elencate a destra dell’operatore e separate dal segno +. Per il caso presente, il modello dei due fattori di Spearman può essere specificato come segue.\n\nspearman_mod &lt;- \"\n  g =~ Classics + French + English + Math + Pitch + Music\n\"\n\nAdattiamo il modello ai dati con la funzione cfa():\n\nfit1 &lt;- lavaan::cfa(\n  spearman_mod,\n  sample.cov = spearman_cor_mat,\n  sample.nobs = n,\n  std.lv = TRUE\n)\n\nLa funzione cfa() è una funzione dedicata per adattare modelli di analisi fattoriale confermativa. Il primo argomento è il modello specificato dall’utente. Il secondo argomento è il dataset che contiene le variabili osservate. L’argomento std.lv = TRUE specifica che imponiamo una varianza pari a 1 a tutte le variabili latenti comuni (nel caso presente, solo una). Ciò consente di stimare le saturazioni fattoriali.\nUna volta adattato il modello, la funzione summary() ci consente di esaminare la soluzione ottenuta:\n\nout = summary(\n  fit1, \n  fit.measures = TRUE, \n  standardized = TRUE\n)\nprint(out)\n\nlavaan 0.6-18 ended normally after 23 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        12\n\n  Number of observations                            33\n\nModel Test User Model:\n                                                      \n  Test statistic                                 2.913\n  Degrees of freedom                                 9\n  P-value (Chi-square)                           0.968\n\nModel Test Baseline Model:\n\n  Test statistic                               133.625\n  Degrees of freedom                                15\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.086\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)               -212.547\n  Loglikelihood unrestricted model (H1)       -211.091\n                                                      \n  Akaike (AIC)                                 449.094\n  Bayesian (BIC)                               467.052\n  Sample-size adjusted Bayesian (SABIC)        429.622\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.000\n  P-value H_0: RMSEA &lt;= 0.050                    0.976\n  P-value H_0: RMSEA &gt;= 0.080                    0.016\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.025\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  g =~                                                                  \n    Classics          0.942    0.129    7.314    0.000    0.942    0.956\n    French            0.857    0.137    6.239    0.000    0.857    0.871\n    English           0.795    0.143    5.545    0.000    0.795    0.807\n    Math              0.732    0.149    4.923    0.000    0.732    0.743\n    Pitch             0.678    0.153    4.438    0.000    0.678    0.689\n    Music             0.643    0.155    4.142    0.000    0.643    0.653\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Classics          0.083    0.051    1.629    0.103    0.083    0.086\n   .French            0.234    0.072    3.244    0.001    0.234    0.242\n   .English           0.338    0.094    3.610    0.000    0.338    0.349\n   .Math              0.434    0.115    3.773    0.000    0.434    0.447\n   .Pitch             0.510    0.132    3.855    0.000    0.510    0.526\n   .Music             0.556    0.143    3.893    0.000    0.556    0.573\n    g                 1.000                               1.000    1.000\n\n\n\nL’output consiste in tre parti. Le prime nove righe sono chiamate intestazione. L’intestazione contiene le seguenti informazioni:\n\nil numero di versione di lavaan\nse l’ottimizzazione è terminata normalmente o meno e quante iterazioni sono state necessarie\nlo stimatore utilizzato (qui: ML, per la massima verosimiglianza)\nl’ottimizzatore utilizzato per trovare i valori dei parametri di adattamento migliori per questo stimatore (qui: NLMINB)\nil numero di parametri del modello (qui: 12)\nil numero di osservazioni che sono state effettivamente utilizzate nell’analisi (qui: 33)\nuna sezione chiamata “Model Test User Model”: che fornisce una statistica di test, i gradi di libertà e un valore p per il modello specificato dall’utente.\n\nLa sezione successiva contiene ulteriori misure di adattamento e viene mostrata solo se si utilizza l’argomento opzionale fit.measures = TRUE. Inizia con la riga Model Test Baseline Model: e termina con il valore per l’SRMR. L’ultima sezione contiene le stime dei parametri. Inizia con informazioni (tecniche) sul metodo utilizzato per calcolare gli errori standard. Quindi, vengono elencati tutti i parametri liberi (e fissati) inclusi nel modello. Di solito, prima vengono mostrate le variabili latenti, seguite dalle covarianze e dalle varianze (residui). La prima colonna (Stima) contiene il valore del parametro (stimato o fisso) per ogni parametro del modello; la seconda colonna (Std.err) contiene l’errore standard per ogni parametro stimato; la terza colonna (Z-value) contiene la statistica di Wald (che viene semplicemente ottenuta dividendo il valore del parametro per il suo errore standard), e l’ultima colonna (P(&gt;|z|)) contiene il valore p per testare l’ipotesi nulla che il valore del parametro sia uguale a zero nella popolazione.\n\nEstimate: Questo valore rappresenta lo stimatore di massima verosimiglianza per i pesi dei percorsi tra il costrutto latente (nel caso presente, il costrutto g) e le variabili osservate (le variabili manifeste: Classics, French, English, Math, Pitch, Music). In sostanza, è il peso del collegamento tra il costrutto latente e ciascuna delle variabili osservate nel modello.\nStd.lv: Questi valori rappresentano le stime dei coefficienti standardizzate rispetto alle variabili latenti. La standardizzazione avviene dividendo la stima del coefficiente non standardizzato per la deviazione standard della variabile latente. Ciò rende possibile confrontare direttamente i coefficienti all’interno del modello perché rimuove le unità di misura, facendo in modo che i coefficienti siano espressi in termini di deviazioni standard. Tuttavia, questa standardizzazione è parziale perché considera solo la varianza della variabile latente.\nStd.all: Questi valori rappresentano le stime dei coefficienti completamente standardizzate, cioè standardizzate sia rispetto alle variabili latenti che a quelle osservate. Ciò significa che sia la variabile dipendente (latente) sia le variabili indipendenti (osservate) sono state standardizzate prima di calcolare i coefficienti. Questo processo di standardizzazione completa permette un confronto diretto dei coefficienti all’interno del modello indipendentemente dalle unità di misura originali, rendendoli espressi in termini di quanti deviazioni standard la variabile dipendente cambia per ogni deviazione standard di cambiamento nella variabile indipendente.\n\nSi noti che nella sezione Varianze: c’è un punto prima dei nomi delle variabili osservate. Ciò perché sono variabili dipendenti (o endogene) (predette dalle variabili latenti) e quindi il valore della varianza stampato in output è una stima della varianza residua: la varianza rimanente che non è spiegata dal/i predittore/i. Al contrario, non c’è un punto prima dei nomi delle variabili latenti, perché in questo modello sono variabili esogene. I valori delle varianze qui sono le varianze totali stimate delle variabili latenti.\nÈ possibile semplificare l’output dalla funzione summary() in maniera tale da stampare solo la tabella completa delle stime dei parametri e degli errori standard. Qui usiamo coef(fit1).\n\nprint(round(coef(fit1), 2))\n\n       g=~Classics          g=~French         g=~English            g=~Math \n              0.94               0.86               0.79               0.73 \n          g=~Pitch           g=~Music Classics~~Classics     French~~French \n              0.68               0.64               0.08               0.23 \n  English~~English         Math~~Math       Pitch~~Pitch       Music~~Music \n              0.34               0.43               0.51               0.56 \n\n\nUsando parameterEstimates, l’output diventa il seguente.\n\nout = parameterEstimates(fit1, standardized = TRUE)\nprint(out)\n\n        lhs op      rhs   est    se     z pvalue ci.lower ci.upper std.lv\n1         g =~ Classics 0.942 0.129 7.314  0.000    0.689    1.194  0.942\n2         g =~   French 0.857 0.137 6.239  0.000    0.588    1.127  0.857\n3         g =~  English 0.795 0.143 5.545  0.000    0.514    1.076  0.795\n4         g =~     Math 0.732 0.149 4.923  0.000    0.441    1.024  0.732\n5         g =~    Pitch 0.678 0.153 4.438  0.000    0.379    0.978  0.678\n6         g =~    Music 0.643 0.155 4.142  0.000    0.339    0.948  0.643\n7  Classics ~~ Classics 0.083 0.051 1.629  0.103   -0.017    0.183  0.083\n8    French ~~   French 0.234 0.072 3.244  0.001    0.093    0.376  0.234\n9   English ~~  English 0.338 0.094 3.610  0.000    0.154    0.522  0.338\n10     Math ~~     Math 0.434 0.115 3.773  0.000    0.208    0.659  0.434\n11    Pitch ~~    Pitch 0.510 0.132 3.855  0.000    0.251    0.769  0.510\n12    Music ~~    Music 0.556 0.143 3.893  0.000    0.276    0.836  0.556\n13        g ~~        g 1.000 0.000    NA     NA    1.000    1.000  1.000\n   std.all\n1    0.956\n2    0.871\n3    0.807\n4    0.743\n5    0.689\n6    0.653\n7    0.086\n8    0.242\n9    0.349\n10   0.447\n11   0.526\n12   0.573\n13   1.000\n\n\nCon opportuni parametri possiamo semplificare l’output nel modo seguente.\n\nout = parameterEstimates(fit1, standardized = TRUE) %&gt;%\n  dplyr::filter(op == \"=~\") %&gt;%\n  dplyr::select(\n    \"Latent Factor\" = lhs,\n    Indicator = rhs,\n    B = est,\n    SE = se,\n    Z = z,\n    \"p-value\" = pvalue,\n    Beta = std.all\n  )\nprint(out)\n\n  Latent.Factor Indicator     B    SE     Z p.value  Beta\n1             g  Classics 0.942 0.129 7.314       0 0.956\n2             g    French 0.857 0.137 6.239       0 0.871\n3             g   English 0.795 0.143 5.545       0 0.807\n4             g      Math 0.732 0.149 4.923       0 0.743\n5             g     Pitch 0.678 0.153 4.438       0 0.689\n6             g     Music 0.643 0.155 4.142       0 0.653\n\n\nEsaminiamo la matrice delle correlazioni residue:\n\ncor_table &lt;- residuals(fit1, type = \"cor\")$cov\nprint(cor_table)\n\n         Clsscs French Englsh   Math  Pitch  Music\nClassics  0.000                                   \nFrench   -0.003  0.000                            \nEnglish   0.008 -0.033  0.000                     \nMath     -0.011  0.023  0.040  0.000              \nPitch     0.001  0.050 -0.016 -0.062  0.000       \nMusic     0.005  0.001 -0.017  0.024 -0.050  0.000\n\n\nCreiamo un qq-plot dei residui:\n\nres1 &lt;- residuals(fit1, type = \"cor\")$cov\nres1[upper.tri(res1, diag = TRUE)] &lt;- NA\nv1 &lt;- as.vector(res1)\nv2 &lt;- v1[!is.na(v1)]\n\ntibble(v2) %&gt;% \n  ggplot(aes(sample = v2)) + \n  stat_qq() + \n  stat_qq_line()\n\n\n\n\n\n\n\n\n\n\n21.10.3 Diagrammi di percorso\nIl pacchetto semPlot consente di disegnare diagrammi di percorso per vari modelli SEM. La funzione semPaths prende in input un oggetto creato da lavaan e disegna il diagramma, con diverse opzioni disponibili. Il diagramma prodotto controlla le dimensioni dei caratteri/etichette, la visualizzazione dei residui e il colore dei percorsi/coefficienti. Sono disponibili queste e molte altre opzioni di controllo.\n\nsemPaths(\n    fit1,\n    \"std\",\n    posCol = c(\"black\"),\n    edge.label.cex = 1.2,\n    sizeMan = 7\n)\n\n\n\n\n\n\n\n\nIl calcolo delle saturazioni fattoriali con il metodo del centroide aveva prodotto il seguente risultato:\n\nclassici (Cls): 0.97\ninglese (Eng): 0.84\nmatematica (Mth): 0.73\npitch discrimination (Ptc): 0.65\n\nSi noti la somiglianza con i valori ottenuti mediante il metodo di massima verosimiglianza riportati nella figura.\n\n\n21.10.4 Analisi fattoriale esplorativa\nQuando abbiamo un’unica variabile latente, l’analisi fattoriale confermativa si riduce al caso dell’analisi fattoriale esplorativa. Esaminiamo qui sotto la sintassi per l’analisi fattoriale esplorativa in lavaan.\nSpecifichiamo il modello.\n\nefa_model &lt;- '\n    efa(\"efa\")*g =~ Classics + French + English + Math + Pitch + Music\n'\n\nAdattiamo il modello ai dati.\n\nfit2 &lt;- lavaan::cfa(\n  efa_model,\n  sample.cov = spearman_cor_mat,\n  sample.nobs = n,\n  std.lv = TRUE\n)\n\nEsaminiamo la soluzione ottenuta.\n\nout = summary(fit2, standardized = TRUE)\nprint(out)\n\nlavaan 0.6.17 ended normally after 3 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        12\n\n  Rotation method                       GEOMIN OBLIQUE\n  Geomin epsilon                                 0.001\n  Rotation algorithm (rstarts)                GPA (30)\n  Standardized metric                             TRUE\n  Row weights                                     None\n\n  Number of observations                            33\n\nModel Test User Model:\n                                                      \n  Test statistic                                 2.913\n  Degrees of freedom                                 9\n  P-value (Chi-square)                           0.968\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  g =~ efa                                                              \n    Classics          0.942    0.129    7.314    0.000    0.942    0.956\n    French            0.857    0.137    6.239    0.000    0.857    0.871\n    English           0.795    0.143    5.545    0.000    0.795    0.807\n    Math              0.732    0.149    4.923    0.000    0.732    0.743\n    Pitch             0.678    0.153    4.438    0.000    0.678    0.689\n    Music             0.643    0.155    4.142    0.000    0.643    0.653\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Classics          0.083    0.051    1.629    0.103    0.083    0.086\n   .French            0.234    0.072    3.244    0.001    0.234    0.242\n   .English           0.338    0.094    3.610    0.000    0.338    0.349\n   .Math              0.434    0.115    3.773    0.000    0.434    0.447\n   .Pitch             0.510    0.132    3.855    0.000    0.510    0.526\n   .Music             0.556    0.143    3.893    0.000    0.556    0.573\n    g                 1.000                               1.000    1.000\n\n\n\n\nsemPaths(\n    fit2,\n    \"std\",\n    posCol = c(\"black\"),\n    edge.label.cex = 1.2,\n    sizeMan = 7\n)",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#conclusioni",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#conclusioni",
    "title": "21  Il modello unifattoriale",
    "section": "21.11 Conclusioni",
    "text": "21.11 Conclusioni\nNel presente capitolo abbiamo introdotto il metodo dell’annullamento della tetrade che consente di stimare le saturazioni di un modello monofattoriale. Abbiamo anche visto che il metodo dell’annullamento della tetrade non è altro che un’applicazione della correlazione parziale.\nPossiamo dire che un tema cruciale nella costruzione dei test psicologici è quello di stabilire il numero di fattori/tratti che sono soggiacenti all’insieme degli indicatori che vengono considerati. La teoria classica dei test richiede che il test sia monofattoriale, ovvero che gli indicatori considerati siano l’espressione di un unico tratto latente. La violazione della monodimensionalità rende problematica l’applicazione dei principi della teoria classica dei test ai punteggi di un test che non possiede tale proprietà. L’esame della dimensionalità di un gruppo di indicatori rappresenta dunque una fase cruciale nel processo di costruzione di un test e, solitamente, questo esame è affrontato mediante l’analisi fattoriale. In questo capitolo abbiamo presentato le proprietà di base del modello unifattoriale.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#session-info",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#session-info",
    "title": "21  Il modello unifattoriale",
    "section": "21.12 Session Info",
    "text": "21.12 Session Info\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] corrplot_0.94     kableExtra_1.4.0  lavaanExtra_0.2.1 lavaanPlot_0.8.1 \n [5] ggokabeito_0.1.0  viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n [9] ggExtra_0.10.1    bayesplot_1.11.1  gridExtra_2.3     patchwork_1.3.0  \n[13] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-18     psych_2.4.6.26   \n[17] scales_1.3.0      markdown_1.13     knitr_1.48        lubridate_1.9.3  \n[21] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[25] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1    \n[29] tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] RColorBrewer_1.1-3  rstudioapi_0.16.0   jsonlite_1.8.9     \n  [4] magrittr_2.0.3      TH.data_1.1-2       estimability_1.5.1 \n  [7] farver_2.1.2        nloptr_2.1.1        rmarkdown_2.28     \n [10] vctrs_0.6.5         minqa_1.2.8         base64enc_0.1-3    \n [13] rstatix_0.7.2       htmltools_0.5.8.1   broom_1.0.6        \n [16] Formula_1.2-5       htmlwidgets_1.6.4   plyr_1.8.9         \n [19] sandwich_3.1-1      emmeans_1.10.4      zoo_1.8-12         \n [22] uuid_1.2-1          igraph_2.0.3        mime_0.12          \n [25] lifecycle_1.0.4     pkgconfig_2.0.3     Matrix_1.7-0       \n [28] R6_2.5.1            fastmap_1.2.0       shiny_1.9.1        \n [31] numDeriv_2016.8-1.1 digest_0.6.37       OpenMx_2.21.12     \n [34] fdrtool_1.2.18      colorspace_2.1-1    rprojroot_2.0.4    \n [37] Hmisc_5.1-3         labeling_0.4.3      fansi_1.0.6        \n [40] timechange_0.3.0    abind_1.4-8         compiler_4.4.1     \n [43] withr_3.0.1         glasso_1.11         htmlTable_2.4.3    \n [46] backports_1.5.0     carData_3.0-5       ggsignif_0.6.4     \n [49] MASS_7.3-61         corpcor_1.6.10      gtools_3.9.5       \n [52] tools_4.4.1         pbivnorm_0.6.0      foreign_0.8-87     \n [55] zip_2.3.1           httpuv_1.6.15       nnet_7.3-19        \n [58] glue_1.7.0          quadprog_1.5-8      DiagrammeR_1.0.11  \n [61] promises_1.3.0      nlme_3.1-166        lisrelToR_0.3      \n [64] grid_4.4.1          pbdZMQ_0.3-13       checkmate_2.3.2    \n [67] cluster_2.1.6       reshape2_1.4.4      generics_0.1.3     \n [70] gtable_0.3.5        tzdb_0.4.0          data.table_1.16.0  \n [73] hms_1.1.3           xml2_1.3.6          car_3.1-2          \n [76] utf8_1.2.4          sem_3.1-16          pillar_1.9.0       \n [79] IRdisplay_1.1       rockchalk_1.8.157   later_1.3.2        \n [82] splines_4.4.1       lattice_0.22-6      survival_3.7-0     \n [85] kutils_1.73         tidyselect_1.2.1    miniUI_0.1.1.1     \n [88] pbapply_1.7-2       svglite_2.1.3       stats4_4.4.1       \n [91] xfun_0.47           qgraph_1.9.8        arm_1.14-4         \n [94] visNetwork_2.1.2    stringi_1.8.4       boot_1.3-31        \n [97] evaluate_1.0.0      codetools_0.2-20    mi_1.1             \n[100] cli_3.6.3           RcppParallel_5.1.9  IRkernel_1.3.2     \n[103] rpart_4.1.23        systemfonts_1.1.0   xtable_1.8-4       \n[106] repr_1.1.7          munsell_0.5.1       Rcpp_1.0.13        \n[109] coda_0.19-4.1       png_0.1-8           XML_3.99-0.17      \n[112] parallel_4.4.1      jpeg_0.1-10         lme4_1.1-35.5      \n[115] mvtnorm_1.3-1       openxlsx_4.2.7.1    crayon_1.5.3       \n[118] rlang_1.1.4         multcomp_1.4-26     mnormt_2.1.1       \n\n\n\n\n\n\nKan, Kees-Jan, Han LJ van der Maas, e Stephen Z Levine. 2019. «Extending psychometric network analysis: Empirical evidence against g in favor of mutualism?» Intelligence 73: 52–62.\n\n\nPetersen, Isaac T. 2024. Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html",
    "href": "chapters/fa/03_analisi_fattoriale_2.html",
    "title": "22  Il modello statistico dell’analisi fattoriale",
    "section": "",
    "text": "22.1 Modello monofattoriale\nIl punto di partenza dell’analisi fattoriale esplorativa è rappresentato da una marice di dimensioni \\(p \\times p\\) (dove \\(p\\) è il numero di variabili osservate) che contiene i coefficienti di correlazione (o di covarianza) tra le variabili. Il punto di arrivo è rappresentato da una matrice di dimensioni \\(p \\times k\\) (dove \\(k\\)) è il numero di fattori comuni che contiene i coefficienti (le saturazioni) che esprimono la relazione tra i fattori e le variabili osservate. Considereremo ora il modello matematico dell’analisi fattoriale esplorativa, con un solo fattore comune, che rappresenta il caso più semplice.\nCon \\(p\\) variabili manifeste \\(Y_i\\), il modello ad un fattore comune può essere espresso algebricamente nel modo seguente:\n\\[\nY_i = \\mu_i + \\lambda_{i} \\xi + \\delta_i \\qquad i=1, \\dots, p\n\\]\ndove \\(\\xi\\) rappresenta il fattore latente, chiamato anche fattore comune, poiché è comune a tutte le \\(Y_i\\), i \\(\\delta_i\\) sono invece specifici di ogni variabile osservata e per tale ragione vengono chiamati fattori specifici o unici, e infine i \\(\\lambda_i\\) sono detti saturazioni (o pesi) fattoriali poiché consentono di valutare il peso del fattore latente su ciascuna variabile osservata. Si suole assumere per comodità che \\(\\mu=0\\), il che corrisponde a considerare le variabili \\(Y_i\\) come ottenute dagli scarti dalle medie \\(\\mu_i\\) per \\(i = 1, \\dots, p\\):\n\\[\nY_i -\\mu_i = \\lambda_i \\xi + \\delta_i.\n\\]\nSi assume che il fattore comune abbia media zero, \\(\\mathbb{E}(\\xi)=0\\), e varianza unitaria, \\(\\mathbb{V}(\\xi)=1\\), che i fattori specifici abbiano media zero, \\(\\mathbb{E}(\\delta_j)=0\\), e varianza \\(\\mathbb{V}(\\delta_j)=\\psi_{i}\\), che i fattori specifici siano incorrelati tra loro, \\(\\mathbb{E}(\\delta_i \\delta_k)=0\\), e che i fattori specifici siano incorrelati con il fattore comune, \\(\\mathbb{E}(\\delta_i \\xi)=0\\).\nIn questo modello, poiché i fattori specifici sono tra loro incorrelati, l’interdipendenza tra le variabili manifeste è completamente spiegata dal fattore comune. Dalle ipotesi precedenti è possibile ricavare la covarianza tra \\(Y_i\\) e il fattore comune, la varianza della \\(i\\)-esima variabile manifesta \\(Y_i\\) e la covarianza tra due variabili manifeste \\(Y_i\\) e \\(Y_k\\).",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html#covarianza-tra-un-indicatore-e-il-fattore-comune",
    "href": "chapters/fa/03_analisi_fattoriale_2.html#covarianza-tra-un-indicatore-e-il-fattore-comune",
    "title": "22  Il modello statistico dell’analisi fattoriale",
    "section": "22.2 Covarianza tra un indicatore e il fattore comune",
    "text": "22.2 Covarianza tra un indicatore e il fattore comune\nDal modello monofattoriale è possibile determinare l’espressione della covarianza teorica tra una variabile manifesta \\(Y_i\\) e il fattore comune \\(\\xi\\):\n\\[\nCov(Y_i,\\xi)=\\mathbb{E}(Y_i \\xi)-\\mathbb{E}(Y_i)\\mathbb{E}(\\xi).\n\\]\nDato che \\(\\mathbb{E}(\\xi)=0\\), possiamo scrivere\n\\[\n\\begin{equation}\n\\begin{aligned}\n  Cov(Y_i,\\xi) &= \\mathbb{E}(Y_i \\xi)=\\mathbb{E}[(\\lambda_i \\xi + \\delta_i) \\xi]\\notag\\\\\n  &=\\mathbb{E}(\\lambda_i \\xi^2 + \\delta_i \\xi)\\notag\\\\\n  &=\\lambda_i\\underbrace{\\mathbb{E}(\\xi^2)}_{\\mathbb{V}(\\xi)=1} + \\underbrace{\\mathbb{E}(\\delta_i \\xi)}_{Cov(\\delta_i, \\xi)=0}\\notag\\\\\n  &= \\lambda_i.\\notag\n\\end{aligned}\n\\end{equation}\n\\]\nNel modello a un solo fattore, dunque, la saturazione \\(\\lambda_j\\) rappresenta la covarianza la variabile manifesta \\(Y_i\\) e il fattore comune \\(\\xi\\) e indica l’importanza del fattore nel determinare il punteggio osservato. Se le variabili \\(Y_i\\) sono standardizzate, la saturazione fattoriale \\(\\lambda_i\\) corrisponde alla correlazione tra \\(Y_i\\) e \\(\\xi\\).",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html#espressione-fattoriale-della-varianza",
    "href": "chapters/fa/03_analisi_fattoriale_2.html#espressione-fattoriale-della-varianza",
    "title": "22  Il modello statistico dell’analisi fattoriale",
    "section": "22.3 Espressione fattoriale della varianza",
    "text": "22.3 Espressione fattoriale della varianza\nNell’ipotesi che le variabili \\(Y_i\\) abbiano media nulla, la varianza di \\(Y_i\\)\n\\[\n\\begin{equation}\n  \\mathbb{V}(Y_i) = \\mathbb{E}(Y_i^2) -[\\mathbb{E}(Y_i)]^2 = \\mathbb{E}(Y_i^2)\\notag\n\\end{equation}\n\\]\nè data da\n\\[\n\\begin{equation}\n\\begin{aligned}\n  \\mathbb{V}(Y_i) &= \\mathbb{E}[(\\lambda_i \\xi + \\delta_i)^2 ]\\notag\\\\\n  &=\\lambda_i^2 \\underbrace{\\mathbb{E}(\\xi^2) }_{\\mathbb{V}(\\xi)=1} + \\underbrace{\\mathbb{E}(\\delta_i^2) }_{\\mathbb{V}(\\delta_i)=\\psi_{i}} + 2\\lambda_i \\underbrace{\\mathbb{E}(\\xi \\delta_i) }_{Cov(\\xi, \\delta_{i})=0}\\notag\\\\\n  &=\\lambda^2_i + \\psi_{i}.\n\\end{aligned}\n\\end{equation}\n\\]\nLa quantità \\(\\lambda^2_i\\) è denominata comunalità della \\(i\\)-esima variabile manifesta e corrisponde alla quota della varianza della \\(Y_i\\) spiegata dal fattore comune. Di conseguenza \\(\\psi_{i}\\) è la parte residua della varianza di \\(Y_i\\) non spiegata dal fattore comune ed è denominata unicità di \\(Y_i\\). Nel caso di variabili standardizzate, l’unicità diventa uguale a\n\\[\n\\psi_{i}=1-\\lambda^2_i.\n\\]\nIn definitiva, la varianza totale di una variabile osservata può essere divisa in una quota che ciascuna variabile condivide con le altre variabili ed è spiegata dal fattore comune (questa quota è chiamata comunalità ed è uguale uguale al quadrato della saturazione della variabile osservata nel fattore comune, ovvero \\(h^2_i = \\lambda_i^2\\)), e in una quota che è spiegata dal fattore specifico (questa parte è chiamata unicità ed è uguale a \\(u_i = \\psi_{i}\\)).\nEsempio. Riprendiamo l’analisi della matrice di correlazioni di Spearman. Nell’output prodotto dalla funzione factanal() viene riportata la quantità denominata SS loadings. Tale quantità indica la porzione della varianza totale delle 4 variabili manifeste che viene spiegata dal fattore comune. Ciascuna variabile standardizzata contribuisce con un’unità di varianza; nel caso presente, dunque la varianza totale è uguale a 4. Si ricordi che, nella statistica multivariata, per varianza totale si intende la somma delle varianze delle variabili manifeste (nel linguaggio dell’algebra matriciale questa quantità corrisponde alla traccia della matrice di covarianze). La quota della varianza totale spiegata dal modello, invece, è data dalla somma delle comunalità delle quattro variabili, ovvero dalla somma delle saturazioni fattoriali innalzate al quadrato.\n\nSpearman &lt;- matrix(c(\n  1.0, .78, .70, .66,\n  .78, 1.0, .64, .54,\n  .70, .64, 1.0, .45,\n  .66, .54, .45, 1.0\n),\nbyrow = TRUE, ncol = 4\n)\nrownames(Spearman) &lt;- c(\"C\", \"E\", \"M\", \"P\")\ncolnames(Spearman) &lt;- c(\"C\", \"E\", \"M\", \"P\")\nSpearman |&gt;\n  print()\n\n     C    E    M    P\nC 1.00 0.78 0.70 0.66\nE 0.78 1.00 0.64 0.54\nM 0.70 0.64 1.00 0.45\nP 0.66 0.54 0.45 1.00\n\n\nEseguiamo l’analisi fattoriale:\n\nfm &lt;- factanal(covmat = Spearman, factors = 1)\nfm |&gt;\n    print()\n\n\nCall:\nfactanal(factors = 1, covmat = Spearman)\n\nUniquenesses:\n    C     E     M     P \n0.086 0.329 0.460 0.539 \n\nLoadings:\n  Factor1\nC 0.956  \nE 0.819  \nM 0.735  \nP 0.679  \n\n               Factor1\nSS loadings      2.587\nProportion Var   0.647\n\nThe degrees of freedom for the model is 2 and the fit was 0.023 \n\n\nLe saturazioni fattoriali sono:\n\nL &lt;- c(fm$load[1], fm$load[2], fm$load[3], fm$load[4])\nprint(L)\n\n[1] 0.9562592 0.8193902 0.7350316 0.6790212\n\n\nFacendo il prodotto interno otteniamo:\n\nt(L) %*% L \n\n\nA matrix: 1 x 1 of type dbl\n\n\n2.587173\n\n\n\n\n\nIn termini proporzionali, la quota della varianza totale delle variabile manifeste che viene spiegata dal modello ad un fattore comune è dunque uguale a \\(2.587 / 4 = 0.647\\). Questa quantità è indicata nell’output con la denominazione Proportion Var.\nSi dice unicità (uniqueness) la quota della varianza della variabile considerata che non viene spiegata dalla soluzione fattoriale:\n\nround(fm$uniqueness, 3) |&gt;\n    print()\n\n    C     E     M     P \n0.086 0.329 0.460 0.539 \n\n\nLa comunalità (ovvero, la quota di varianza di ciascuna variabile manifesta che viene spiegata dal fattore comune) può essere trovata come:\n\nround(1 - fm$uniqueness, 3) |&gt;\n    print()\n\n    C     E     M     P \n0.914 0.671 0.540 0.461 \n\n\noppure con\n\nL^2 |&gt;\n    print()\n\n[1] 0.9144316 0.6714003 0.5402714 0.4610697",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html#covarianza-tra-due-variabili-manifeste",
    "href": "chapters/fa/03_analisi_fattoriale_2.html#covarianza-tra-due-variabili-manifeste",
    "title": "22  Il modello statistico dell’analisi fattoriale",
    "section": "22.4 Covarianza tra due variabili manifeste",
    "text": "22.4 Covarianza tra due variabili manifeste\nNell’ipotesi che le variabili \\(Y_i\\) abbiano media nulla, la covarianza tra \\(Y_i\\) e \\(Y_k\\)\n\\[\nCov(Y_i, Y_k)=\\mathbb{E}(Y_i Y_k) -\n\\mathbb{E}(Y_i)\\mathbb{E}(Y_k)=\\mathbb{E}(Y_i Y_k)\n\\]\nè uguale al prodotto delle corrispondenti saturazioni fattoriali:\n\\[\n\\begin{equation}\n\\begin{aligned}\nCov(Y_i, Y_k) &= \\mathbb{E}(Y_i Y_k) \\notag\\\\\n  & =\\mathbb{E}[(\\lambda_i \\xi + \\delta_i)(\\lambda_k \\xi +  \\delta_k)]\\notag\\\\\n  &=\\mathbb{E}(\\lambda_i\\lambda_k\\xi^2 + \\lambda_i  \\xi \\delta_k + \\lambda_k \\delta_i \\xi + \\delta_i \\delta_k)\\notag\\\\\n  &=\\lambda_i\\lambda_k\\underbrace{\\mathbb{E}(\\xi^2)}_{\\mathbb{V}(\\xi)=1}+\\lambda_i\\underbrace{\\mathbb{E}(\\xi \\delta_k)}_{Cov(\\xi, \\delta_k) =0}+\\notag\\\\ \\;&+\\lambda_k\\underbrace{\\mathbb{E}(\\delta_i \\xi)}_{Cov(\\delta_i, \\xi) =0} +\\underbrace{\\mathbb{E}(\\delta_i \\delta_k)}_{Cov(\\delta_i, \\delta_k)=0}\\notag\\\\\n  &=\\lambda_i\\lambda_k.\n\\end{aligned}\n\\end{equation}\n\\]",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html#correlazioni-osservate-e-correlazioni-riprodotte-dal-modello",
    "href": "chapters/fa/03_analisi_fattoriale_2.html#correlazioni-osservate-e-correlazioni-riprodotte-dal-modello",
    "title": "22  Il modello statistico dell’analisi fattoriale",
    "section": "22.5 Correlazioni osservate e correlazioni riprodotte dal modello",
    "text": "22.5 Correlazioni osservate e correlazioni riprodotte dal modello\nIn generale possiamo affermare che il modello monofattoriale è adeguato se si verifica che \\(Cov(Y_i, Y_k \\mid \\xi) = 0\\) (\\(i, k = 1, \\dots,p; \\; i\\neq k\\)), ossia se il fattore comune spiega tutta la covarianza tra le variabili osservate. La matrice di correlazioni riprodotte dal modello è chiamata \\(\\boldsymbol{\\Sigma}\\) e può essere espressa come:\n\\[\n\\boldsymbol{\\Sigma} = \\boldsymbol{\\Lambda} \\boldsymbol{\\Lambda}^\\prime + \\boldsymbol{\\Psi}\n\\]\nIn altri termini, il modello monofattoriale è adeguato se è nulla la differenza tra la matrice di correlazioni osservate e la matrice di correlazioni riprodotte dal modello. Per i dati di Spearman, le correlazioni riprodotte dal modello ad un fattore sono\n\nround( L %*% t(L) + diag(fm$uniq), 3) |&gt;\n    print()\n\n      [,1]  [,2]  [,3]  [,4]\n[1,] 1.000 0.784 0.703 0.649\n[2,] 0.784 1.000 0.602 0.556\n[3,] 0.703 0.602 1.000 0.499\n[4,] 0.649 0.556 0.499 1.000\n\n\nLa matrice delle differenze tra le correlazioni campionarie e quelle riprodotte è\n\nround(Spearman - (L %*% t(L) + diag(fm$uniq)), 3) |&gt;\n    print()\n\n       C      E      M      P\nC  0.000 -0.004 -0.003  0.011\nE -0.004  0.000  0.038 -0.016\nM -0.003  0.038  0.000 -0.049\nP  0.011 -0.016 -0.049  0.000\n\n\nLo scarto maggiore tra le correlazioni campionarie e quelle riprodotte è uguale a 0.049. Si può dunque concludere che il modello monofattoriale spiega in maniera ragionevole i dati di Spearman.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html#bontà-di-adattamento-del-modello-ai-dati",
    "href": "chapters/fa/03_analisi_fattoriale_2.html#bontà-di-adattamento-del-modello-ai-dati",
    "title": "22  Il modello statistico dell’analisi fattoriale",
    "section": "22.6 Bontà di adattamento del modello ai dati",
    "text": "22.6 Bontà di adattamento del modello ai dati\nLa verifica della bontà di adattamento del modello ai dati si determina mediante un test statistico che valuta la differenza tra la matrice di correlazioni (o di covarianze) osservata e la matrice di correlazioni (o covarianze) predetta dal modello fattoriale. L’ipotesi nulla che viene valutata è che la matrice delle correlazioni residue sia dovuta semplicemente agli errori di campionamento, ovvero che la matrice di correlazioni predetta dal modello \\(\\boldsymbol{\\Sigma}(\\theta)\\) sia uguale alla matrice di correlazioni \\(\\boldsymbol{\\Sigma}\\) nella popolazione.\nLa statistica test \\(v\\) è una funzione della differenza tra la matrice riprodotta \\(\\boldsymbol{S}(\\theta)\\) e quella osservata \\(\\boldsymbol{S}\\)\n\\[\nv = f\\left[\\boldsymbol{S}(\\theta) - \\boldsymbol{S}\\right]\n\\]\ne si distribuisce come una \\(\\chi^2\\) con \\(\\nu\\) gradi di libertà\n\\[\n\\nu = p(p+1)/ 2 - q,\n\\]\ndove \\(p\\) è il numero di variabili manifeste e \\(q\\) è il numero di parametri stimati dal modello fattoriale (ovvero, \\(\\lambda\\) e \\(\\psi\\)).\nLa statistica \\(v\\) assume valore 0 se i parametri del modello riproducono esattamente la matrice di correlazioni tra le variabili nella popolazione. Tanto maggiore è la statistica \\(v\\) tanto maggiore è la discrepanza tra le correlazioni osservate e quelle predette dal modello fattoriale.\nUn risultato statisticamente significativo (es., \\(p\\) &lt; .05) – il quale suggerisce che una tale differenza non è uguale a zero – rivela dunque una discrepanza tra il modello e i dati. Il test del modello fattoriale mediante la statistica \\(\\chi^2\\) segue dunque una logica diversa da quella utilizzata nei normali test di ipotesi statistiche: un risultato statisticamente significativo indica una mancanza di adattamento del modello ai dati.\nL’applicazione del test \\(\\chi^2\\) per valutare la bontà di adattamento del modello ai dati richiede che ciascuna variabile manifesta sia distribuita normalmente – più precisamente, richiede che le variabili manifeste siano un campione casuale che deriva da una normale multivariata. Questo requisito non è facile da rispettare in pratica.\nTuttavia, il limite principale della statistica \\(\\chi^2\\) è che essa dipende fortemente dalle dimensioni del campione: al crescere delle dimensioni campionarie è più facile ottenere un risultato statisticamente significativo (ovvero, concludere che vi è un cattivo adattamento del modello ai dati). Per questa ragione, la bontà di adattamento del modello ai dati viene valutata da molteplici indici, non soltanto dalla statistica \\(\\chi^2\\). Più comune è calcolare il rapporto \\(\\chi^2 / \\nu\\) e usare tale rapporto per valutare la bontà dell’adattamento. Valori minori di 3 o 4 suggeriscono che il modello ben si adatta ai dati.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html#lerrore-standard-della-misurazione-e-il-modello-fattoriale",
    "href": "chapters/fa/03_analisi_fattoriale_2.html#lerrore-standard-della-misurazione-e-il-modello-fattoriale",
    "title": "22  Il modello statistico dell’analisi fattoriale",
    "section": "22.7 L’errore standard della misurazione e il modello fattoriale",
    "text": "22.7 L’errore standard della misurazione e il modello fattoriale\nIn questa sezione, approfondiamo la connessione tra l’errore standard di misurazione, un concetto fondamentale della Classical Test Theory (CTT), e l’applicazione del modello fattoriale. Questa connessione ci permette di reinterpretare l’errore standard di misurazione attraverso il prisma dell’analisi fattoriale. Procediamo con un’esposizione dettagliata.\nAll’interno della CTT, si afferma che il punteggio ottenuto (\\(X\\)) in un test corrisponde alla somma del valore vero (\\(T\\)) e dell’errore di misurazione (\\(E\\)), dove \\(E\\) è considerato una variabile casuale indipendente da \\(T\\). Se focalizziamo l’attenzione sul soggetto \\(i\\)-esimo, la formula diventa \\(X_i = T_i + E_i\\), con \\(T_i\\) rappresentante il valore vero e \\(E_i\\) l’errore di misurazione, quest’ultimo avente media zero.\nTrasformiamo questa relazione nel contesto di un modello fattoriale monofattoriale che coinvolge \\(p\\) variabili osservate (o item). Per ogni item, la relazione è espressa come:\n\\[\n\\begin{equation}\n\\begin{aligned}\nY_{1i} &=  \\lambda_1 \\xi_i + \\delta_{1i} \\notag\\\\\nY_{2i} &=  \\lambda_2 \\xi_i + \\delta_{2i} \\notag\\\\\n  \\dots\\notag\\\\\nY_{pi} &=  \\lambda_p \\xi_i + \\delta_{pi}, \\notag\n\\end{aligned}\n\\end{equation}\n\\]\ndove \\(Y_{ji}\\) rappresenta il punteggio osservato per l’item \\(j\\) del soggetto \\(i\\), \\(\\lambda_j\\) è il carico fattoriale dell’item \\(j\\) sul fattore comune \\(\\xi_i\\), e \\(\\delta_{ji}\\) è l’errore unico associato all’item \\(j\\) per il soggetto \\(i\\).\nIl punteggio totale \\(X_i\\) per il soggetto \\(i\\)-esimo deriva dalla somma dei punteggi di ciascun item, il che si traduce in:\n\\[\n\\begin{equation}\n\\begin{aligned}\nX_i &= \\sum_{j=1}^p Y_{ji} = \\sum_{j=1}^p \\lambda_j \\xi_i + \\sum_{j=1}^p \\delta_{ji}\\notag\\\\[12pt]\n  &=  \\left( \\sum_{j=1}^p \\lambda_j \\right) \\xi_i  +  \\sum_{j=1}^p \\delta_{ji} \\notag\\\\[12pt]\n  &= T_i + E_i\\notag\n\\end{aligned}\n\\end{equation}\n\\]\nRispettando la struttura della CTT, la varianza del punteggio osservato \\(X_i\\) si decompone in due componenti fondamentali: la varianza del valore vero \\(\\sigma^2_{T_i}\\) e la varianza dell’errore \\(\\sigma^2_{E_i}\\). Nel contesto dell’analisi fattoriale, \\(\\sigma^2_{T_i}\\) corrisponde al quadrato della somma dei carichi fattoriali:\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\sigma^2_{T_i} &= \\mathbb{V}\\left[ \\left( \\sum_{j=1}^p \\lambda_j \\right) \\xi_i \\right]\\notag\\\\\n&= \\left( \\sum_{j=1}^p \\lambda_j \\right)^2 \\mathbb{V}(\\xi_i)\\notag\\\\\n&= \\left( \\sum_{j=1}^p \\lambda_j \\right)^2 \\notag\n\\end{aligned}\n\\end{equation}\n\\]\nInoltre, considerando la varianza dell’errore di misurazione \\(\\sigma^2_{E_i}\\) nel contesto fattoriale, questa è equivalente alla somma delle varianze degli errori unici (\\(\\delta_{ji}\\)), ovvero le unicità:\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\sigma^2_{E_i} &= \\mathbb{V}\\left( \\sum_{j=1}^p \\delta_{ji} \\right)\\notag\\\\\n&= \\sum_{j=1}^p \\mathbb{V}\\left( \\delta_{ji} \\right)\\notag\\\\\n&= \\sum_{j=1}^p \\Psi_j\\notag\n\\end{aligned}\n\\end{equation}\n\\]\nPertanto, nel contesto dell’analisi fattoriale, l’errore standard di misurazione per il punteggio totale del test è quantificabile come la radice quadrata della somma delle unicità:\n\\[\n\\begin{equation}\n\\sigma_{E} = \\sqrt{\\sum_{j=1}^p \\Psi_j}\n\\end{equation}\n\\](eq-err-stnd-meas-FA)\nQuesto collegamento tra la CTT e l’analisi fattoriale offre una prospettiva rinnovata sull’errore standard di misurazione, arricchendo la nostra comprensione della precisione dei test psicometrici.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html#un-esempio-concreto",
    "href": "chapters/fa/03_analisi_fattoriale_2.html#un-esempio-concreto",
    "title": "22  Il modello statistico dell’analisi fattoriale",
    "section": "22.8 Un esempio concreto",
    "text": "22.8 Un esempio concreto\nApplichiamo ora il risultato precedente ad un caso concreto. Consideriamo i dati utilizzati nella validazione italiana del Cognitive Style Questionnaire - Short Form (CSQ-SF, Meins et al. 2012). Il CSQ-SF viene utilizzato per misurare la vulnerabilità all’ansia e alla depressione. È costituito da cinque sottoscale: Internality, Globality, Stability, Negative consequences e Self-worth.\nLeggiamo i dati in \\(\\textsf{R}\\):\n\ncsq &lt;- rio::import(here::here(\"data\", \"csq540.csv\"))\n\nIl numero di partecipanti è\n\nn &lt;- nrow(csq)\nn\n\n540\n\n\nLe statistiche descrittive si ottengono con la seguente istruzione:\n\npsych::describe(csq, type = 2) |&gt;\n    print()\n\n  vars   n  mean    sd median trimmed   mad min max range  skew kurtosis   se\nI    1 540 47.76  5.78     48   47.87  4.45  21  64    43 -0.31     1.07 0.25\nG    2 540 45.00 11.94     42   44.55 11.86  16  78    62  0.34    -0.70 0.51\nS    3 540 44.60 12.18     42   44.24 13.34  16  77    61  0.27    -0.77 0.52\nN    4 540 22.01  6.92     21   21.86  7.41   8  39    31  0.21    -0.74 0.30\nW    5 540 44.05 13.10     43   43.66 13.34  16  79    63  0.31    -0.53 0.56\n\n\nEsaminiamo la matrice di correlazione:\n\npsych::pairs.panels(csq) |&gt;\n    print()\n\nNULL\n\n\n\n\n\n\n\n\n\nLa sottoscala di Internality è problematica, come messo anche in evidenza dall’autore del test. La consideriamo comunque in questa analisi statistica.\nSpecifichiamo il modello unifattoriale nella sintassi di lavaan:\n\nmod_csq &lt;- \"\n   F =~ NA*I + G + S + N + W\n   F ~~ 1*F\n\" \n\nAdattiamo il modello ai dati:\n\nfit &lt;- lavaan:::cfa(\n  mod_csq,\n  data = csq\n)\n\nEsaminiamo i risultati:\n\nsummary(\n  fit, \n  standardized = TRUE,\n  fit.measures = TRUE\n) |&gt;\n    print()\n\nlavaan 0.6.17 ended normally after 26 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n\n  Number of observations                           540\n\nModel Test User Model:\n                                                      \n  Test statistic                                46.716\n  Degrees of freedom                                 5\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              2361.816\n  Degrees of freedom                                10\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.982\n  Tucker-Lewis Index (TLI)                       0.965\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -8741.781\n  Loglikelihood unrestricted model (H1)      -8718.423\n                                                      \n  Akaike (AIC)                               17503.562\n  Bayesian (BIC)                             17546.478\n  Sample-size adjusted Bayesian (SABIC)      17514.734\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.124\n  90 Percent confidence interval - lower         0.093\n  90 Percent confidence interval - upper         0.158\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.989\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.033\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  F =~                                                                  \n    I                 0.725    0.253    2.867    0.004    0.725    0.126\n    G               -11.322    0.384  -29.481    0.000  -11.322   -0.949\n    S               -11.342    0.398  -28.513    0.000  -11.342   -0.932\n    N                -6.163    0.233  -26.398    0.000   -6.163   -0.891\n    W               -11.598    0.444  -26.137    0.000  -11.598   -0.886\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    F                 1.000                               1.000    1.000\n   .I                32.840    2.000   16.420    0.000   32.840    0.984\n   .G                14.038    1.473    9.532    0.000   14.038    0.099\n   .S                19.508    1.718   11.353    0.000   19.508    0.132\n   .N                 9.847    0.725   13.573    0.000    9.847    0.206\n   .W                36.892    2.685   13.737    0.000   36.892    0.215\n\n\n\nEsaminiamo solo le stime dei parametri del modello:\n\nparameterEstimates(fit) |&gt;\n    print()\n\n   lhs op rhs     est    se       z pvalue ci.lower ci.upper\n1    F =~   I   0.725 0.253   2.867  0.004    0.229    1.220\n2    F =~   G -11.322 0.384 -29.481  0.000  -12.075  -10.569\n3    F =~   S -11.342 0.398 -28.513  0.000  -12.122  -10.563\n4    F =~   N  -6.163 0.233 -26.398  0.000   -6.621   -5.705\n5    F =~   W -11.598 0.444 -26.137  0.000  -12.467  -10.728\n6    F ~~   F   1.000 0.000      NA     NA    1.000    1.000\n7    I ~~   I  32.840 2.000  16.420  0.000   28.920   36.759\n8    G ~~   G  14.038 1.473   9.532  0.000   11.151   16.924\n9    S ~~   S  19.508 1.718  11.353  0.000   16.140   22.876\n10   N ~~   N   9.847 0.725  13.573  0.000    8.425   11.269\n11   W ~~   W  36.892 2.685  13.737  0.000   31.628   42.155\n\n\nRecuperiamo le specificità:\n\npsi &lt;- parameterEstimates(fit)$est[7:11]\npsi |&gt;\n    print()\n\n[1] 32.839665 14.037578 19.508119  9.846927 36.891617\n\n\nStimiamo l’errore standard della misurazione con la @ref(eq:err-stnd-meas-FA):\n\nsqrt(sum(psi)) |&gt;\n    print()\n\n[1] 10.63597\n\n\nApplichiamo ora la formula della TCT:\n\\[\n\\sigma_E = \\sigma_X \\sqrt{1 -\\rho_{XX^\\prime}}.\n\\]\nPer trovare \\(\\sigma\\) calcoliamo prima il punteggio totale:\n\ntot_score &lt;- rowSums(csq)\n\nLa deviazione standard di tot_score ci fornisce una stima di \\(\\sigma_X\\):\n\nsigma &lt;- sd(tot_score)\nsigma |&gt;\n    print()\n\n[1] 41.26414\n\n\nPer applicare la formula della TCT abbiamo bisogno dell’attendibilità. La stimiamo usando la funzione reliability del pacchetto semTools dall’oggetto creato da lavaan:::cfa():\n\nrel &lt;- semTools::reliability(fit)\nrel |&gt;\n    print()\n\n               F\nalpha  0.8506572\nomega  0.9330313\nomega2 0.9330313\nomega3 0.9273385\navevar 0.7916575\n\n\nUtilizzando \\(\\Omega\\) otteniamo:\n\nsigma * sqrt(1- rel[2]) |&gt;\n    print()\n\n[1] 0.2587831\n\n\n10.67846074554\n\n\nSi noti come il risultato sia molto simile a quello trovato con la formula della TCT.\n\n22.8.1 Correlazioni osservate e riprodotte\nLe correlazioni riprodotte dal modello si ottengono nel modo seguente dall’oggetto fit.\n\ncor_mat &lt;- lavInspect(fit, \"cor.ov\")\ncor_mat |&gt;\n    print()\n\n       I      G      S      N      W\nI  1.000                            \nG -0.119  1.000                     \nS -0.117  0.885  1.000              \nN -0.112  0.846  0.830  1.000       \nW -0.111  0.841  0.825  0.789  1.000\n\n\nAbbiamo visto come il modello unifattoriale predice che la correlazione tra due variabili manifeste sia il prodotto delle rispettive correlazioni fattoriali. Estraiamo le saturazioni fattoriali.\n\nl &lt;- inspect(fit, what=\"std\")$lambda\nl |&gt;\n    print()\n\n       F\nI  0.126\nG -0.949\nS -0.932\nN -0.891\nW -0.886\n\n\nPer esempio, se consideriamo I e G, la correlazione predetta dal modello fattoriale tra queste due sottoscale è data dal prodotto delle rispettive saturazioni fattoriali.\n\nl[1] * l[2] |&gt;\n    print()\n\n[1] -0.9493687\n\n\n-0.11915121205349\n\n\nLa matrice di correlazioni riprodotte riportata sopra mostra il risultato di questo prodotto per ciascuna coppia di variabili manifeste.\n\nl %*% t(l) |&gt; round(3) |&gt;\n    print()\n\n       I      G      S      N      W\nI  0.016 -0.119 -0.117 -0.112 -0.111\nG -0.119  0.901  0.885  0.846  0.841\nS -0.117  0.885  0.868  0.830  0.825\nN -0.112  0.846  0.830  0.794  0.789\nW -0.111  0.841  0.825  0.789  0.785\n\n\n\n\n22.8.2 Scomposizione della varianza\nConsideriamo la variabile manifesta W. Calcoliamo la varianza.\n\nvar(csq$W) |&gt; print()\n\n[1] 171.714\n\n\nLa varianza riprodotta di questa variabile, secondo il modello fattoriale, dovrebbe esere uguale alla somma di due componenti: la varianza predetta dall’effetto causale del fattore latente e la varianza residua. La varianza predetta dall’effetto causale del fattore latente è uguale alla saturazione elevata al quadrato:\n\n(-11.598)^2 \n\n134.513604\n\n\nCalcolo ora la proporzione di varianza residua normalizzando rispetto alla varianza osservata (non a quella riprodotta dal modello):\n\n1 - (-11.598)^2 / var(csq$W) \n\n0.216641572893455\n\n\nIl valore così ottenuto è molto simile al valore della varianza residua di W.\nRipeto i calcoli per la variabile G\n\n1 - (-11.322)^2 / var(csq$G) \n\n0.1003728851332\n\n\ne per la variabile I\n\n1 - (0.725)^2 / var(csq$I) \n\n0.984275494822392\n\n\nIn tutti i casi, i valori ottenuti sono molto simili alle varianze residue ipotizzate dal modello unifattoriale.\n\n\n22.8.3 Correlazione tra variabili manifeste e fattore comune\nUn modo per verificare il fatto che, nel modello unifattoriale, la saturazione fattoriale della \\(i\\)-esima variabile manifesta è uguale alla correlazione tra i punteggi osservati sulla i$-esima variabile manifesta e il fattore latente è quella di calcolare le correlazioni tra le variabili manifeste e i punteggi fattoriali. I punteggi fattoriali rappresentano una stima del punteggio “vero”, ovvero del punteggio che ciascun rispondente otterrebbe in assenza di errori di misurazione. Vedremo in seguito come si possono stimare i punteggi fattoriali. Per ora ci limitiamo a calcolarli usando lavaan.\n\nhead(lavPredict(fit)) |&gt;\n    print()\n\n              F\n[1,]  0.2693790\n[2,] -0.9110820\n[3,]  0.1871406\n[4,] -0.3315541\n[5,]  0.8306627\n[6,]  1.1534515\n\n\nAbbiamo un punteggio diverso per ciascuno dei 540 individui che appartengono al campione di dati esaminato.\n\ndim(lavPredict(fit))\n\n\n5401\n\n\nCalcoliamo ora le correlazioni tra i valori osservati su ciascuna delle cinque scale del CSQ e le stime dei punteggi veri.\n\nc(\n  cor(csq$I, lavPredict(fit)),\n  cor(csq$G, lavPredict(fit)),\n  cor(csq$S, lavPredict(fit)),\n  cor(csq$N, lavPredict(fit)),\n  cor(csq$W, lavPredict(fit))\n) |&gt; \n  round(3) |&gt;\n    print()\n\n[1]  0.128 -0.970 -0.952 -0.910 -0.905\n\n\nSi noti che i valori ottenui sono molto simili ai valori delle saturazioni fattoriali. La piccola differenza tra le correlazioni ottenute e i valori delle saturazioni fattoriali dipende dal fatto che abbiamo stimato i punteggi fattoriali.\n\ninspect(fit, what=\"std\")$lambda |&gt;\n    print()\n\n       F\nI  0.126\nG -0.949\nS -0.932\nN -0.891\nW -0.886",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html#session-info",
    "href": "chapters/fa/03_analisi_fattoriale_2.html#session-info",
    "title": "22  Il modello statistico dell’analisi fattoriale",
    "section": "22.9 Session Info",
    "text": "22.9 Session Info\n\nsessionInfo()",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/04_analisi_fattoriale_3.html",
    "href": "chapters/fa/04_analisi_fattoriale_3.html",
    "title": "23  Il modello multifattoriale",
    "section": "",
    "text": "23.1 Modello multifattoriale: fattori ortogonali\nLa teoria dei due fattori ha orientato per diversi anni le ricerche sull’intelligenza, finché Thurstone (1945) non propose una sua modifica, conosciuta come teoria multifattoriale. Secondo Thurstone la covariazione tra le variabili manifeste non può essere spiegata da un unico fattore generale. Invece è necessario ipotizzare l’azione causale di diversi fattori, definiti comuni, i quali si riferiscono solo ad alcune delle variabili considerate.\nIl modello plurifattoriale assume che ciascuna variabile manifesta sia espressa come funzione lineare di un certo numero \\(m\\) di fattori comuni, \\(\\xi_1, \\xi_2, \\dots, \\xi_m\\), responsabili della correlazione con le altre variabili, e di un solo fattore specifico (termine d’errore), responsabile della variabilità della variabile stessa. Per \\(p\\) variabili manifeste, \\(Y_1, Y_2, \\dots, Y_p\\), il modello fattoriale diventa quello indicato dal sistema di equazioni lineari descritto di seguito. Idealmente, \\(m\\) dovrebbe essere molto più piccolo di \\(p\\) così da consentire una descrizione parsimoniosa delle variabili manifeste in funzione di pochi fattori soggiacenti.\nLe variabili manifeste \\(Y\\) sono indicizzate da \\(i = 1, \\dots, p.\\) Le variabili latenti \\(\\xi\\) (fattori) sono indicizzate da \\(j = 1, \\dots, m.\\) I fattori specifici \\(\\delta\\) sono indicizzati da \\(i = 1, \\dots, p.\\) Le saturazioni fattoriali si distinguono dunque tramite due indici, \\(i\\) e \\(j\\): il primo indice si riferisce alle variabili manifeste, il secondo si riferisce ai fattori latenti.\nIndichiamo con \\(\\mu_i\\), con \\(i=1, \\dots, p\\) le medie delle \\(p\\) variabili manifeste \\(Y_1, Y_2, \\dots, Y_p\\). Se non vi è alcun effetto delle variabili comuni latenti, allora la variabile \\(Y_{ijk}\\), dove \\(k\\) è l’indice usato per i soggetti, sarà uguale a:\n\\[\n\\begin{equation}\n\\begin{cases}\n  Y_{1k}    &= \\mu_1 + \\delta_{1k} \\\\\n&\\vdots\\\\\nY_{ik}   &= \\mu_i + \\delta_{ik}\\\\\n&\\vdots\\\\\nY_{pk}   &= \\mu_p + \\delta_{pk} \\notag\n\\end{cases}\n\\end{equation}\n\\]\nSe invece le variabili manifeste rappresentano la somma dell’effetto causale di \\(m\\) fattori comuni e di \\(p\\) fattori specifici, allora possiamo scrivere:\n\\[\n\\begin{equation}\n\\begin{cases}\n  Y_1  - \\mu_1  &= \\lambda_{11}\\xi_1 + \\dots + \\lambda_{1k}\\xi_k \\dots +\\lambda_{1m}\\xi_m + \\delta_1 \\\\\n&\\vdots\\\\\nY_i -  \\mu_i  &= \\lambda_{i1}\\xi_1 + \\dots +  \\lambda_{ik}\\xi_k \\dots +\\lambda_{im}\\xi_m + \\delta_i\\\\\n&\\vdots\\\\\nY_p - \\mu_p  &= \\lambda_{p1}\\xi_1 + \\dots +  \\lambda_{pk}\\xi_k \\dots +\\lambda_{pm}\\xi_m + \\delta_p \\notag\n\\end{cases}\n\\end{equation}\n\\]\nNel precedente sistema di equazioni lineari,\nIn conclusione, secondo il modello multifattoriale, le variabili manifeste \\(Y_i\\), con \\(i=1, \\dots, p\\), sono il risultato di una combinazione lineare di \\(m &lt; p\\) fattori inosservabili ad esse comuni \\(\\xi_j\\), con \\(j=1, \\dots, m\\), e di \\(p\\) fattori specifici \\(\\delta_i\\), con \\(i=1, \\dots, p\\), anch’essi inosservabili e di natura residua.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Il modello multifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/04_analisi_fattoriale_3.html#modello-multifattoriale-fattori-ortogonali",
    "href": "chapters/fa/04_analisi_fattoriale_3.html#modello-multifattoriale-fattori-ortogonali",
    "title": "23  Il modello multifattoriale",
    "section": "",
    "text": "\\(\\xi_j\\), con \\(j=1, \\dots, m\\), rappresenta la \\(j\\)-esima variabile inosservabile a fattore comune (ossia il \\(j\\)-esimo fattore comune a tutte le variabili \\(Y_i\\));\n\\(\\lambda_{ij}\\) rappresenta il parametro, detto saturazione o peso fattoriale, che riflette l’importanza del \\(j\\)-esimo fattore comune nella composizione della \\(i\\)-esima variabile osservabile;\n\\(\\delta_i\\) rappresenta il fattore specifico (o unico) di ogni variabile manifesta \\(Y_i\\).\n\n\n\n23.1.1 Assunzioni del modello multifattoriale\nLe variabili inosservabili a fattore comune \\(\\xi_j\\), con \\(j=1, \\dots, m\\), in quanto latenti, non possiedono unità di misura. Pertanto, per semplicità si assume che abbiano media zero, \\(\\mathbb{E}(\\xi_j)=0\\), abbiano varianza unitaria, \\(\\mathbb{V} (\\xi_j)= \\mathbb{E}(\\xi_j^2) - [\\mathbb{E}(\\xi_j)]^2=1\\), e siano incorrelate tra loro, \\(Cov(\\xi_j, \\xi_h)=0\\), con \\(j, h = 1, \\dots, m; \\;j \\neq h\\). Si assume inoltre che le variabili a fattore specifico \\(\\delta_i\\) siano tra loro incorrelate, \\(Cov(\\delta_i,\\delta_k)=0\\), con \\(i, k = 1, \\dots, p, \\; i \\neq k\\), abbiano media zero, \\(\\mathbb{E}(\\delta_i)=0\\), e varianza uguale a \\(\\mathbb{V} (\\delta_i) = \\psi_{ii}\\). La varianza \\(\\psi_{ii}\\) è detta varianza specifica o unicità della \\(i\\)-esima variabile manifesta \\(Y_i\\). Si assume infine che i fattori specifici siano linearmente incorrelati con i fattori comuni, ovvero \\(Cov(\\xi_j, \\delta_i)=0\\) per ogni \\(j=1, \\dots, m\\) e per ogni \\(i=1\\dots,p\\).\n\n\n23.1.2 Interpretazione dei parametri del modello\n\n23.1.2.1 Covarianza tra variabili e fattori\nNell’ipotesi che le variabili \\(Y_i\\) abbiano media nulla, la covarianza tra \\(Y_i\\) e \\(\\xi_j\\) è uguale alla saturazione fattoriale \\(\\lambda_{ij}\\):\n\\[\n\\begin{equation}\n\\begin{aligned}\n  Cov(Y_i, \\xi_j) &= \\mathbb{E}(Y_i \\xi_j)\\notag\\\\\n  &=\\mathbb{E}\\left[(\\lambda_{i1} \\xi_1 + \\dots + \\lambda_{im} \\xi_m + \\delta_i)\\xi_j \\right]\\notag\\\\\n  &= \\lambda_{i1}\\underbrace{\\mathbb{E}(\\xi_1\\xi_j)}_{=0} + \\dots +\n\\lambda_{ij}\\underbrace{\\mathbb{E}(\\xi_j^2)}_{=1} + \\dots \\notag\\\\\n& \\; + \\lambda_{im}\\underbrace{\\mathbb{E}(\\xi_m\\xi_j)}_{=0} +\n  \\underbrace{\\mathbb{E}(\\delta_i \\xi_j)}_{=0}\\notag\\\\\n  &= \\lambda_{ij}.\\notag\n\\end{aligned}\n\\end{equation}\n\\]\nAnche nel modello multifattoriale, dunque, le saturazioni fattoriali rappresentano le covarianze tra le variabili e i fattori:\n\\[\nCov(Y_i, \\xi_j) = \\lambda_{ij} \\qquad i=1, \\dots, p; \\quad j= 1, \\dots, m.\n\\]\nNaturalmente, se le variabili sono standardizzate, le saturazioni fattoriali diventano correlazioni:\n\\[\nr_{ij} = \\lambda_{ij}.\n\\]\n\n\n23.1.2.2 Espressione fattoriale della varianza\nCome nel modello monofattoriale, la varianza delle variabili manifeste si decompone in una componente dovuta ai fattori comuni, chiamata comunalità, e in una componente specifica alle \\(Y_i\\), chiamata unicità. Nell’ipotesi che le variabili \\(Y_i\\) abbiano media nulla, la varianza di \\(Y_i\\) è uguale a\n\\[\n\\begin{equation}\n\\begin{aligned}\n  \\mathbb{V} (Y_i)\n  &=\\mathbb{E}\\left[ (\\lambda_{i1} \\xi_1 + \\dots +\n    \\lambda_{im} \\xi_m + \\delta_i)^2 \\right].\n\\end{aligned}\n\\end{equation}\n\\](eq-eq-var-multifatt)\nCome si sviluppa il polinomio precedente? Il quadrato di un polinomio è uguale alla somma dei quadrati di tutti i termini più il doppio prodotto di ogni termine per ciascuno di quelli che lo seguono. Il valore atteso del quadrato del primo termine è uguale a \\(\\lambda_{i1}^2\\mathbb{E}(\\xi_1^2)\\) ma, essendo la varianza di \\(\\xi_1\\) uguale a \\(1\\), otteniamo semplicemente \\(\\lambda_{i1}^2\\). Lo stesso vale per i quadrati di tutti i termini seguenti tranne l’ultimo. Infatti, \\(\\mathbb{E}(\\delta_i^2)=\\psi_{ii}\\). Per quel che riguarda i doppi prodotti, sono tutti nulli. In primo luogo perché, nel caso di fattori ortogonali, la covarianza tra i fattori comuni è nulla, \\(\\mathbb{E}(\\xi_j \\xi_h)=0\\), con \\(j \\neq h\\). In secondo luogo perché il fattori comuni cono incorrelati con i fattori specifici, quindi \\(\\mathbb{E}(\\delta_i \\xi_j)=0\\).\nIn conclusione,\n\\[\n\\begin{equation}\n\\begin{aligned}\n  \\mathbb{V}(Y_i) &= \\lambda_{i1}^2 + \\lambda_{i2}^2 + \\dots + \\lambda_{im}^2 + \\psi_{ii} \\notag\\\\\n  &= \\sum_{j=1}^m \\lambda_{ij}^2 + \\psi_{ii}\\notag\\\\\n  &= h_i^2 + \\psi_{ii}\\notag\\\\\n  &=\\text{communalità} + \\text{unicità},\\notag\n\\end{aligned}\n\\end{equation}\n\\]\nla varianza della variabile manifesta \\(Y_i\\) è suddivisa in due parti: il primo addendo è definito comunalità poiché rappresenta la parte di variabilità della \\(Y_i\\) spiegata dai fattori comuni; il secondo addendo è invece definito varianza specifica (o unicità) poiché esprime la parte di variabilità della \\(Y_i\\) non spiegata dai fattori comuni.\n\n\n23.1.2.3 Espressione fattoriale della covarianza\nQuale esempio, consideriamo il caso di \\(p=5\\) variabili osservabili e \\(m=2\\) fattori ortogonali. Se le variabili manifeste sono ‘centrate’ (ovvero, se a ciascuna di esse sottraiamo la rispettiva media), allora il modello multifattoriale diventa\n\\[\n\\begin{equation}\n\\begin{aligned}\n  Y_1 &= \\lambda_{11} \\xi_1 + \\lambda_{12} \\xi_2 + \\delta_1,\\notag\\\\\n  Y_2 &= \\lambda_{21} \\xi_1 + \\lambda_{22} \\xi_2 + \\delta_2,\\notag\\\\\n  Y_3 &= \\lambda_{31} \\xi_1 + \\lambda_{32} \\xi_2 + \\delta_3,\\notag\\\\\n  Y_4 &= \\lambda_{41} \\xi_1 + \\lambda_{42} \\xi_2 + \\delta_4,\\notag\\\\\n  Y_5 &= \\lambda_{51} \\xi_1 + \\lambda_{52} \\xi_2 + \\delta_5.\\notag\n\\end{aligned}\n\\end{equation}\n\\]\nNell’ipotesi che le variabili \\(Y_i\\) abbiano media nulla, la covarianza tra \\(Y_1\\) e \\(Y_2\\), ad esempio, è uguale a:\n$$ \\[\\begin{equation}\n\\begin{aligned}\n  Cov(Y_1, Y_2) &= \\mathbb{E}\\left( Y_1 Y_2\\right) \\notag\\\\\n  &= \\mathbb{E}\\left[\n  (\\lambda_{11} \\xi_1 + \\lambda_{12} \\xi_2 + \\delta_1)\n   (\\lambda_{21} \\xi_1 + \\lambda_{22} \\xi_2 +  \\delta_2)\n  \\right]\\notag\\\\\n  &= \\lambda_{11} \\lambda_{21} \\mathbb{E}(\\xi_1^2) +\n      \\lambda_{11} \\lambda_{22} \\mathbb{E}(\\xi_1 \\xi_2) +\\notag\n      \\lambda_{11} \\mathbb{E}(\\xi_1 \\delta_2) +\\notag\\\\\n    &\\quad \\lambda_{12} \\lambda_{21}\\mathbb{E}(\\xi_1 \\xi_2)\\, +\n      \\lambda_{12} \\lambda_{22}\\mathbb{E}(\\xi^2_2)\\, +\n      \\lambda_{12} \\mathbb{E}(\\xi_2\\delta_2) +\\notag\\\\\n    &\\quad \\lambda_{21} \\mathbb{E}(\\xi_1\\delta_1) +\\notag\n     \\lambda_{22} \\mathbb{E}(\\xi_2\\delta_1) + \\mathbb{E}(\\delta_1 \\delta_2)\\notag\\\\\n   &= \\lambda_{11} \\lambda_{21} + \\lambda_{12} \\lambda_{22}.\\notag\n\\end{aligned}\n\\end{equation}\\]\n$$ In conclusione, la covarianza tra le variabili manifeste \\(Y_l\\) e \\(Y_m\\) riprodotta dal modello è data dalla somma dei prodotti delle saturazioni \\(\\lambda_l \\lambda_m\\) nei due fattori.\nEsempio. Consideriamo i dati riportati da {cite:t}brown2015confirmatory, ovvero otto misure di personalità raccolte su un campione di 250 pazienti che hanno concluso un programma di psicoterapia. Le scale sono le seguenti:\n\nanxiety (N1),\nhostility (N2),\ndepression (N3),\nself-consciousness (N4),\nwarmth (E1),\ngregariousness (E2),\nassertiveness (E3),\npositive emotions (E4).\n\n\nvarnames &lt;- c(\"N1\", \"N2\", \"N3\", \"N4\", \"E1\", \"E2\", \"E3\", \"E4\")\nsds &lt;- '5.7  5.6  6.4  5.7  6.0  6.2  5.7  5.6'\n\ncors &lt;- '\n 1.000\n 0.767  1.000 \n 0.731  0.709  1.000 \n 0.778  0.738  0.762  1.000 \n-0.351  -0.302  -0.356  -0.318  1.000 \n-0.316  -0.280  -0.300  -0.267  0.675  1.000 \n-0.296  -0.289  -0.297  -0.296  0.634  0.651  1.000 \n-0.282  -0.254  -0.292  -0.245  0.534  0.593  0.566  1.000'\n\npsychot_cor_mat &lt;- getCov(cors, names = varnames)\nn &lt;- 250\n\nEseguiamo l’analisi fattoriale esplorativa con il metodo della massima verosimiglianza ipotizzando due fattori comuni incorrelati:\n\nn_facs &lt;- 2\nfit_efa &lt;- factanal(\n  covmat = psychot_cor_mat,\n  factors = n_facs,\n  rotation = \"varimax\",\n  n.obs = n\n)\n\nEsaminiamo le saturazioni fattoriali:\n\nlambda &lt;- fit_efa$loadings\nprint(lambda)\n\n\nLoadings:\n   Factor1 Factor2\nN1  0.854  -0.228 \nN2  0.826  -0.194 \nN3  0.811  -0.233 \nN4  0.865  -0.186 \nE1 -0.202   0.773 \nE2 -0.139   0.829 \nE3 -0.158   0.771 \nE4 -0.147   0.684 \n\n               Factor1 Factor2\nSS loadings      2.923   2.526\nProportion Var   0.365   0.316\nCumulative Var   0.365   0.681\n\n\nLa soluzione fattoriale conferma la presenza di due fattori: il primo fattore satura sulle scale di neutoricismo, il secono sulle scale di estroversione.\nLa correlazione riprodotta \\(r_{12}\\) è uguale a \\(\\lambda_{11}\\lambda_{21} + \\lambda_{12}\\lambda_{22}\\)\n\nlambda[1, 1] * lambda[2, 1] + lambda[1, 2] * lambda[2, 2]\n\n0.74928439937518\n\n\ne corrisponde da vicino alla correlazione osservata 0.767.\nL’intera matrice di correlazioni riprodotte è \\(\\boldsymbol{\\Lambda} \\boldsymbol{\\Lambda}^{\\mathsf{T}} + \\boldsymbol{\\psi}\\):\n\nRr &lt;- lambda %*% t(lambda) + diag(fit_efa$uniq)\nRr %&gt;% \n  round(3)\n\n\nA matrix: 8 × 8 of type dbl\n\n\n\nN1\nN2\nN3\nN4\nE1\nE2\nE3\nE4\n\n\n\n\nN1\n1.000\n0.749\n0.745\n0.781\n-0.348\n-0.307\n-0.311\n-0.281\n\n\nN2\n0.749\n1.000\n0.715\n0.751\n-0.317\n-0.276\n-0.281\n-0.254\n\n\nN3\n0.745\n0.715\n1.000\n0.745\n-0.344\n-0.306\n-0.308\n-0.279\n\n\nN4\n0.781\n0.751\n0.745\n1.000\n-0.318\n-0.274\n-0.280\n-0.254\n\n\nE1\n-0.348\n-0.317\n-0.344\n-0.318\n1.000\n0.669\n0.628\n0.558\n\n\nE2\n-0.307\n-0.276\n-0.306\n-0.274\n0.669\n1.000\n0.661\n0.587\n\n\nE3\n-0.311\n-0.281\n-0.308\n-0.280\n0.628\n0.661\n1.000\n0.550\n\n\nE4\n-0.281\n-0.254\n-0.279\n-0.254\n0.558\n0.587\n0.550\n1.000\n\n\n\n\n\nLa differenza tra la matrice di correlazioni riprodotte e la matrice di correlazioni osservate è uguale a:\n\n(psychot_cor_mat - Rr) %&gt;% \n  round(3)\n\n\nA matrix: 8 × 8 of type dbl\n\n\n\nN1\nN2\nN3\nN4\nE1\nE2\nE3\nE4\n\n\n\n\nN1\n0.000\n0.018\n-0.014\n-0.003\n-0.003\n-0.009\n0.015\n-0.001\n\n\nN2\n0.018\n0.000\n-0.006\n-0.013\n0.015\n-0.004\n-0.008\n0.000\n\n\nN3\n-0.014\n-0.006\n0.000\n0.017\n-0.012\n0.006\n0.011\n-0.013\n\n\nN4\n-0.003\n-0.013\n0.017\n0.000\n0.000\n0.007\n-0.016\n0.009\n\n\nE1\n-0.003\n0.015\n-0.012\n0.000\n0.000\n0.006\n0.006\n-0.024\n\n\nE2\n-0.009\n-0.004\n0.006\n0.007\n0.006\n0.000\n-0.010\n0.006\n\n\nE3\n0.015\n-0.008\n0.011\n-0.016\n0.006\n-0.010\n0.000\n0.016\n\n\nE4\n-0.001\n0.000\n-0.013\n0.009\n-0.024\n0.006\n0.016\n0.000",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Il modello multifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/04_analisi_fattoriale_3.html#modello-fattoriale-fattori-obliqui",
    "href": "chapters/fa/04_analisi_fattoriale_3.html#modello-fattoriale-fattori-obliqui",
    "title": "23  Il modello multifattoriale",
    "section": "23.2 Modello fattoriale: Fattori obliqui",
    "text": "23.2 Modello fattoriale: Fattori obliqui\nAnche nel caso di fattori comuni correlati è possibile esprimere nei termini dei parametri del modello la covarianza teorica tra una variabile manifesta \\(Y_i\\) e uno dei fattori comuni, la covarianza teorica tra due variabili manifeste, e la comunalità di ciascuna variabile manifesta. Dato però che i fattori comuni risultano correlati, l’espressione fattoriale di tali quantità è più complessa che nel caso di fattori comuni ortogonali.\n\n23.2.1 Covarianza teorica tra variabili e fattori\nIn base al modello multifattoriale con \\(m\\) fattori comuni la variabile \\(Y_i\\) è\n\\[\nY_i = \\lambda_{i1} \\xi_1 + \\dots + \\lambda_{im} \\xi_m + \\delta_i.\n(\\#eq:mod-multifact)\n\\]\nPoniamoci il problema di trovare la covarianza teorica tra la variabile manifesta \\(Y_i\\) e il fattore comune \\(\\xi_j\\). Come in precedenza, il problema si riduce a quello di trovare \\(\\mathbb{E}(Y_i \\xi_j)\\). Ne segue che\n\\[\n\\begin{equation}\n\\begin{aligned}\n  Cov(Y_i, \\xi_j) &= \\mathbb{E}(Y_i \\xi_j)\\notag\\\\\n  &=\\mathbb{E}\\left[(\\lambda_{i1} \\xi_1 + \\dots + \\lambda_{ij} \\xi_j + \\dots + \\lambda_{im} \\xi_m + \\delta_i)\\xi_j \\right]\\notag\\\\\n  &= \\lambda_{i1}\\underbrace{\\mathbb{E}(\\xi_1\\xi_j)}_{\\neq 0} + \\dots + \\lambda_{ij}\\underbrace{\\mathbb{E}(\\xi_j^2)}_{=1} + \\dots \\notag\\\\\n& \\quad + \\lambda_{im}\\underbrace{\\mathbb{E}(\\xi_m\\xi_j)}_{\\neq 0} + \\underbrace{\\mathbb{E}(\\delta_i \\xi_j)}_{=0}\\notag\\\\\n  &= \\lambda_{ij} + \\lambda_{i1} Cov(\\xi_1, \\xi_j) + \\dots + \\lambda_{im} Cov(\\xi_m, \\xi_j).\n\\end{aligned}\n\\end{equation}\n\\]\nAd esempio, nel caso di tre fattori comuni \\(\\xi_1, \\xi_2, \\xi_3\\), la covarianza tra \\(Y_1\\) e \\(\\xi_{1}\\) diventa\n\\[\n\\lambda_{11} + \\lambda_{12}Cov(\\xi_1, \\xi_2) + \\lambda_{13}Cov(\\xi_1, \\xi_3).\n\\]\n\n\n23.2.2 Espressione fattoriale della varianza\nPoniamoci ora il problema di trovare la varianza teorica della variabile manifesta \\(Y_i\\). In base al modello fattoriale, la variabile \\(Y_i\\) è specificata come nella @ref(eq:mod-multifact). La varianza di \\(Y_i\\) è \\(\\mathbb{V}(Y_i) = \\mathbb{E}(Y_i^2) -[\\mathbb{E}(Y_i)]^2\\). Però, avendo espresso \\(Y_i\\) nei termini della differenza dalla sua media, l’espressione della varianza si riduce a \\(\\mathbb{V}(Y_i) = \\mathbb{E}(Y_i^2)\\). Dobbiamo dunque sviluppare l’espressione\n\\[\n\\mathbb{E}(Y_i^2) = \\mathbb{E}[(\\lambda_{i1} \\xi_1 + \\dots + \\lambda_{im} \\xi_m + \\delta_i)^2].\n\\]\nIn conclusione, la varianza teorica di \\(Y_i\\) è uguale a\n\\[\n\\begin{equation}\n\\begin{split}\n\\mathbb{V}(Y_i) &= \\lambda_{i1}^2 + \\lambda_{i2}^2 + \\dots + \\lambda_{im}^2  + \\\\\n&\\quad 2 \\lambda_{i1} \\lambda_{i2} Cov(\\xi_1, \\xi_2) + \\dots + 2 \\lambda_{i,m-1} \\lambda_{im} Cov(\\xi_{m-1}, \\xi_m) + \\\\\n&\\quad \\psi_{ii}.\\notag\n\\end{split}\n\\end{equation}\n\\]\nAd esempio, nel caso di tre fattori comuni, \\(\\xi_1, \\xi_2, \\xi_3\\), la varianza di \\(Y_1\\) è\n\\[\n\\begin{equation}\n\\begin{split}\n\\mathbb{V}(Y_1) = &\\lambda_{11}^2 + \\lambda_{12}^2 + \\lambda_{13}^2 +\\\\\n&\\quad 2 \\lambda_{11} \\lambda_{12} Cov(\\xi_1, \\xi_2) + \\\\\n&\\quad 2 \\lambda_{11} \\lambda_{13} Cov(\\xi_1, \\xi_3) + \\\\\n&\\quad 2 \\lambda_{12} \\lambda_{13} Cov(\\xi_2, \\xi_3) + \\\\\n&\\quad \\psi_{11}. \\notag\n\\end{split}\n\\end{equation}\n\\]\n\n\n23.2.3 Covarianza teorica tra due variabili\nConsideriamo ora il caso più semplice di due soli fattori comuni correlati e calcoliamo la covarianza tra \\(Y_1\\) e \\(Y_2\\):\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathbb{E}(Y_1 Y_2) =\\mathbb{E}[(&\\lambda_{11}\\xi_1 + \\lambda_{12}\\xi_2+\\delta_1) (\\lambda_{21}\\xi_1 + \\lambda_{22}\\xi_2+\\delta_2)]\\notag\\\\\n=\\mathbb{E}(\n&\\lambda_{11}\\lambda_{21}\\xi_1^2 +\n\\lambda_{11}\\lambda_{22}\\xi_1\\xi_2 +\n\\lambda_{11}\\xi_1\\delta_2 +\\notag\\\\\n+&\\lambda_{12}\\lambda_{21}\\xi_1\\xi_2 +\n\\lambda_{12}\\lambda_{22}\\xi_2^2 +\n\\lambda_{12}\\xi_2\\delta_2 +\\notag\\\\\n+&\\lambda_{21}\\xi_1\\delta_1 +\n\\lambda_{22}\\xi_2\\delta_1 +\n\\delta_1\\delta_2).\\notag\n\\end{aligned}\n\\end{equation}\n\\]\nDistribuendo l’operatore di valore atteso, dato che \\(\\mathbb{E}(\\xi^2)=1\\) e \\(\\mathbb{E}(\\xi \\delta)=0\\), otteniamo\n\\[\nCov(Y_1, Y_2) = \\lambda_{11} \\lambda_{21} + \\lambda_{12} \\lambda_{22} +\n\\lambda_{12} \\lambda_{21}Cov(\\xi_1, \\xi_2) +\\lambda_{11} \\lambda_{22}Cov(\\xi_1, \\xi_2).\n\\]\nIn termini matriciali si scrive\n\\[\n\\boldsymbol{\\Sigma} =\\boldsymbol{\\Lambda} \\boldsymbol{\\Phi} \\boldsymbol{\\Lambda}^{\\mathsf{T}} + \\boldsymbol{\\Psi},\n\\]\ndove \\(\\boldsymbol{\\Phi}\\) è la matrice di ordine \\(m \\times m\\) di varianze e covarianze tra i fattori comuni e \\(\\boldsymbol{\\Psi}\\) è una matrice diagonale di ordine \\(p\\) con le unicità delle variabili.\nEsempio. Consideriamo nuovamente i dati esaminati negli esercizi precedenti, ma questa volta il modello consente una correlazione tra i due fattori comuni:\n\nefa_result &lt;- fa(psychot_cor_mat, nfactors = 2, n.obs = n, rotate = \"oblimin\")\nprint(efa_result)\n\nLoading required namespace: GPArotation\n\n\n\nFactor Analysis using method =  minres\nCall: fa(r = psychot_cor_mat, nfactors = 2, n.obs = n, rotate = \"oblimin\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n     MR1   MR2   h2   u2 com\nN1  0.88 -0.02 0.78 0.22   1\nN2  0.85  0.01 0.72 0.28   1\nN3  0.83 -0.04 0.71 0.29   1\nN4  0.90  0.03 0.78 0.22   1\nE1 -0.05  0.77 0.63 0.37   1\nE2  0.03  0.86 0.71 0.29   1\nE3  0.00  0.79 0.63 0.37   1\nE4 -0.01  0.70 0.49 0.51   1\n\n                       MR1  MR2\nSS loadings           3.00 2.45\nProportion Var        0.37 0.31\nCumulative Var        0.37 0.68\nProportion Explained  0.55 0.45\nCumulative Proportion 0.55 1.00\n\n With factor correlations of \n      MR1   MR2\nMR1  1.00 -0.43\nMR2 -0.43  1.00\n\nMean item complexity =  1\nTest of the hypothesis that 2 factors are sufficient.\n\ndf null model =  28  with the objective function =  5.02 with Chi Square =  1231.22\ndf of  the model are 13  and the objective function was  0.04 \n\nThe root mean square of the residuals (RMSR) is  0.01 \nThe df corrected root mean square of the residuals is  0.02 \n\nThe harmonic n.obs is  250 with the empirical chi square  1.73  with prob &lt;  1 \nThe total n.obs was  250  with Likelihood Chi Square =  9.65  with prob &lt;  0.72 \n\nTucker Lewis Index of factoring reliability =  1.006\nRMSEA index =  0  and the 90 % confidence intervals are  0 0.047\nBIC =  -62.12\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   MR1  MR2\nCorrelation of (regression) scores with factors   0.96 0.94\nMultiple R square of scores with factors          0.93 0.87\nMinimum correlation of possible factor scores     0.85 0.75\n\n\n\nfa.diagram(efa_result)\n\n\n\n\n\n\n\n\nEsaminiamo la matrice delle correlazioni residue:\n\nresiduals &lt;- residuals(efa_result)\nprint(residuals)\n\n   N1    N2    N3    N4    E1    E2    E3    E4   \nN1  0.22                                          \nN2  0.02  0.28                                    \nN3 -0.01  0.00  0.29                              \nN4  0.00 -0.01  0.02  0.22                        \nE1  0.00  0.01 -0.01  0.00  0.37                  \nE2 -0.01  0.00  0.01  0.01  0.01  0.29            \nE3  0.01 -0.01  0.01 -0.02  0.01 -0.01  0.37      \nE4  0.00  0.00 -0.01  0.01 -0.02  0.01  0.01  0.51\n\n\nEsaminiamo più da vicino la matrice di correlazioni riprodotta dal modello, nel caso di fattori obliqui. Le saturazioni fattoriali sono:\n\n# Estrai i carichi fattoriali (saturazioni fattoriali)\nlambda &lt;- efa_result$loadings\n\n# Converti i carichi in una matrice 8 x 2 (assumendo 2 fattori)\n# e assegna i nomi appropriati alle righe e alle colonne\nlambda &lt;- matrix(lambda[, 1:2], nrow = 8, ncol = 2)\nrownames(lambda) &lt;- c(\"N1\", \"N2\", \"N3\", \"N4\", \"E1\", \"E2\", \"E3\", \"E4\")\ncolnames(lambda) &lt;- c(\"Factor1\", \"Factor2\")\n\n# Stampa la matrice dei carichi\nprint(lambda)\n\n        Factor1     Factor2\nN1  0.877075569 -0.01577815\nN2  0.852280802  0.01128419\nN3  0.826584447 -0.03684789\nN4  0.898762806  0.03121279\nE1 -0.048589010  0.77186847\nE2  0.034700239  0.85566012\nE3  0.002815265  0.79291602\nE4 -0.007884591  0.69545191\n\n\nLa matrice di intercorrelazoni fattoriali è\n\n# Estrai la matrice delle intercorrelazioni fattoriali\nPhi &lt;- efa_result$Phi\n\n# Stampa la matrice delle intercorrelazioni\nprint(Phi)\n\n           MR1        MR2\nMR1  1.0000000 -0.4313609\nMR2 -0.4313609  1.0000000\n\n\nLe varianze residue sono:\n\n# Estrai le varianze residue\nPsi &lt;- diag(efa_result$uniquenesses)\n\n# Stampa le varianze residue\nprint(Psi)\n\n          [,1]      [,2]      [,3]     [,4]      [,5]      [,6]      [,7]\n[1,] 0.2185506 0.0000000 0.0000000 0.000000 0.0000000 0.0000000 0.0000000\n[2,] 0.0000000 0.2817872 0.0000000 0.000000 0.0000000 0.0000000 0.0000000\n[3,] 0.0000000 0.0000000 0.2891237 0.000000 0.0000000 0.0000000 0.0000000\n[4,] 0.0000000 0.0000000 0.0000000 0.215453 0.0000000 0.0000000 0.0000000\n[5,] 0.0000000 0.0000000 0.0000000 0.000000 0.3695024 0.0000000 0.0000000\n[6,] 0.0000000 0.0000000 0.0000000 0.000000 0.0000000 0.2922572 0.0000000\n[7,] 0.0000000 0.0000000 0.0000000 0.000000 0.0000000 0.0000000 0.3732021\n[8,] 0.0000000 0.0000000 0.0000000 0.000000 0.0000000 0.0000000 0.0000000\n          [,8]\n[1,] 0.0000000\n[2,] 0.0000000\n[3,] 0.0000000\n[4,] 0.0000000\n[5,] 0.0000000\n[6,] 0.0000000\n[7,] 0.0000000\n[8,] 0.5115539\n\n\nMediante i parametri del modello la matrice di correlazione si riproduce nel modo seguente:\n\\[\n\\boldsymbol{\\Sigma} =\\boldsymbol{\\Lambda} \\boldsymbol{\\Phi} \\boldsymbol{\\Lambda}^{\\mathsf{T}} + \\boldsymbol{\\Psi}.\n\\]\nIn \\(\\textsf{R}\\) scriviamo:\n\nR_hat &lt;- lambda %*% Phi %*% t(lambda) + Psi\nR_hat %&gt;% \n  round(3)\n\n\nA matrix: 8 x 8 of type dbl\n\n\n\nN1\nN2\nN3\nN4\nE1\nE2\nE3\nE4\n\n\n\n\nN1\n1.000\n0.749\n0.745\n0.782\n-0.347\n-0.307\n-0.310\n-0.281\n\n\nN2\n0.749\n1.000\n0.714\n0.751\n-0.316\n-0.276\n-0.280\n-0.255\n\n\nN3\n0.745\n0.714\n1.000\n0.745\n-0.345\n-0.307\n-0.310\n-0.280\n\n\nN4\n0.782\n0.751\n0.745\n1.000\n-0.318\n-0.274\n-0.280\n-0.255\n\n\nE1\n-0.347\n-0.316\n-0.345\n-0.318\n1.000\n0.665\n0.628\n0.554\n\n\nE2\n-0.307\n-0.276\n-0.307\n-0.274\n0.665\n1.000\n0.666\n0.587\n\n\nE3\n-0.310\n-0.280\n-0.310\n-0.280\n0.628\n0.666\n1.000\n0.553\n\n\nE4\n-0.281\n-0.255\n-0.280\n-0.255\n0.554\n0.587\n0.553\n1.000\n\n\n\n\n\nLe correlazioni residue sono:\n\n(psychot_cor_mat - R_hat) %&gt;% \n  round(3)\n\n\nA matrix: 8 x 8 of type dbl\n\n\n\nN1\nN2\nN3\nN4\nE1\nE2\nE3\nE4\n\n\n\n\nN1\n0.000\n0.018\n-0.014\n-0.004\n-0.004\n-0.009\n0.014\n-0.001\n\n\nN2\n0.018\n0.000\n-0.005\n-0.013\n0.014\n-0.004\n-0.009\n0.001\n\n\nN3\n-0.014\n-0.005\n0.000\n0.017\n-0.011\n0.007\n0.013\n-0.012\n\n\nN4\n-0.004\n-0.013\n0.017\n0.000\n0.000\n0.007\n-0.016\n0.010\n\n\nE1\n-0.004\n0.014\n-0.011\n0.000\n0.000\n0.010\n0.006\n-0.020\n\n\nE2\n-0.009\n-0.004\n0.007\n0.007\n0.010\n0.000\n-0.015\n0.006\n\n\nE3\n0.014\n-0.009\n0.013\n-0.016\n0.006\n-0.015\n0.000\n0.013\n\n\nE4\n-0.001\n0.001\n-0.012\n0.010\n-0.020\n0.006\n0.013\n0.000\n\n\n\n\n\nPer fare un esempio relativo alla correlazione tra due indicatori, calcoliamo la correlazione predetta dal modello tra le variabili \\(Y_1\\) e \\(Y_2\\):\n\nlambda[1, 1] * lambda[2, 1] + lambda[1, 2] * lambda[2, 2] +\n  lambda[1, 1] * lambda[2, 2] * Phi[1, 2] + \n  lambda[1, 2] * lambda[2, 1] * Phi[1, 2]\n\n0.748868098259331\n\n\nQuesto valore si avvicina al valore contenuto dell’elemento (1, 2) della matrice di correlazioni osservate:\n\npsychot_cor_mat[1, 2]\n\n0.767\n\n\nUsando questa procedura possiamo riprodurre tutti gli elementi della matrice di correlazione osservata tramite i parametri stimati dal modello EFA replicando così il risultato che si trova con $ = ^{} + . $",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Il modello multifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/04_analisi_fattoriale_3.html#session-info",
    "href": "chapters/fa/04_analisi_fattoriale_3.html#session-info",
    "title": "23  Il modello multifattoriale",
    "section": "23.3 Session Info",
    "text": "23.3 Session Info\n\nsessionInfo()\n\nR version 4.3.2 (2023-10-31)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.3.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] digest_0.6.34   IRdisplay_1.1   utf8_1.2.4      base64enc_0.1-3\n [5] fastmap_1.1.1   glue_1.7.0      htmltools_0.5.7 repr_1.1.6     \n [9] lifecycle_1.0.4 cli_3.6.2       fansi_1.0.6     vctrs_0.6.5    \n[13] pbdZMQ_0.3-11   compiler_4.3.2  tools_4.3.2     evaluate_0.23  \n[17] pillar_1.9.0    crayon_1.5.2    rlang_1.1.3     jsonlite_1.8.8 \n[21] IRkernel_1.3.2  uuid_1.2-0",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Il modello multifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/04_analisi_fattoriale_3.html#session-info-1",
    "href": "chapters/fa/04_analisi_fattoriale_3.html#session-info-1",
    "title": "23  Il modello multifattoriale",
    "section": "23.4 Session Info",
    "text": "23.4 Session Info\n\nsessionInfo()\n\nR version 4.3.1 (2023-06-16)\nPlatform: x86_64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.3.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] tidySEM_0.2.4      OpenMx_2.21.8      corrplot_0.92      kableExtra_1.3.4  \n [5] ggokabeito_0.1.0   viridis_0.6.4      viridisLite_0.4.2  ggpubr_0.6.0      \n [9] ggExtra_0.10.1     bayesplot_1.10.0   gridExtra_2.3      patchwork_1.1.3   \n[13] semTools_0.5-6.920 semPlot_1.1.6      lavaan_0.6-16      psych_2.3.6       \n[17] scales_1.2.1       markdown_1.8       knitr_1.44         lubridate_1.9.2   \n[21] forcats_1.0.0      stringr_1.5.0      dplyr_1.1.3        purrr_1.0.2       \n[25] readr_2.1.4        tidyr_1.3.0        tibble_3.2.1       ggplot2_3.4.3     \n[29] tidyverse_2.0.0    here_1.0.1        \n\nloaded via a namespace (and not attached):\n  [1] splines_4.3.1         later_1.3.1           pbdZMQ_0.3-10        \n  [4] XML_3.99-0.14         rpart_4.1.19          fastDummies_1.7.3    \n  [7] lifecycle_1.0.3       rstatix_0.7.2         StanHeaders_2.26.28  \n [10] rprojroot_2.0.3       globals_0.16.2        processx_3.8.2       \n [13] lattice_0.21-8        MASS_7.3-60           rockchalk_1.8.157    \n [16] backports_1.4.1       magrittr_2.0.3        openxlsx_4.2.5.2     \n [19] Hmisc_5.1-1           rmarkdown_2.24        httpuv_1.6.11        \n [22] tmvnsim_1.0-2         qgraph_1.9.5          zip_2.3.0            \n [25] pkgbuild_1.4.2        pbapply_1.7-2         minqa_1.2.6          \n [28] multcomp_1.4-25       abind_1.4-5           rvest_1.0.3          \n [31] quadprog_1.5-8        nnet_7.3-19           TH.data_1.1-2        \n [34] sandwich_3.0-2        inline_0.3.19         listenv_0.9.0        \n [37] arm_1.13-1            proto_1.0.0           parallelly_1.36.0    \n [40] texreg_1.38.6         svglite_2.1.1         codetools_0.2-19     \n [43] xml2_1.3.5            tidyselect_1.2.0      lme4_1.1-34          \n [46] matrixStats_1.0.0     stats4_4.3.1          base64enc_0.1-3      \n [49] webshot_0.5.5         jsonlite_1.8.7        progressr_0.14.0     \n [52] ellipsis_0.3.2        Formula_1.2-5         survival_3.5-7       \n [55] emmeans_1.8.8         systemfonts_1.0.4     dbscan_1.1-11        \n [58] tools_4.3.1           Rcpp_1.0.11           glue_1.6.2           \n [61] mnormt_2.1.1          xfun_0.40             MplusAutomation_1.1.0\n [64] IRdisplay_1.1         loo_2.6.0             withr_2.5.0          \n [67] fastmap_1.1.1         boot_1.3-28.1         fansi_1.0.4          \n [70] callr_3.7.3           digest_0.6.33         mi_1.1               \n [73] timechange_0.2.0      R6_2.5.1              mime_0.12            \n [76] estimability_1.4.1    colorspace_2.1-0      gtools_3.9.4         \n [79] jpeg_0.1-10           utf8_1.2.3            generics_0.1.3       \n [82] data.table_1.14.8     corpcor_1.6.10        prettyunits_1.1.1    \n [85] httr_1.4.7            htmlwidgets_1.6.2     pkgconfig_2.0.3      \n [88] sem_3.1-15            gtable_0.3.4          bain_0.2.9           \n [91] htmltools_0.5.6       carData_3.0-5         blavaan_0.5-1        \n [94] png_0.1-8             rstudioapi_0.15.0     tzdb_0.4.0           \n [97] reshape2_1.4.4        uuid_1.1-1            coda_0.19-4          \n[100] checkmate_2.2.0       nlme_3.1-163          nloptr_2.0.3         \n[103] repr_1.1.6            zoo_1.8-12            parallel_4.3.1       \n[106] miniUI_0.1.1.1        nonnest2_0.5-6        foreign_0.8-85       \n[109] pillar_1.9.0          grid_4.3.1            vctrs_0.6.3          \n[112] RANN_2.6.1            promises_1.2.1        car_3.1-2            \n[115] xtable_1.8-4          cluster_2.1.4         GPArotation_2023.8-1 \n[118] htmlTable_2.4.1       evaluate_0.21         pbivnorm_0.6.0       \n[121] gsubfn_0.7            mvtnorm_1.2-3         cli_3.6.1            \n[124] kutils_1.72           compiler_4.3.1        rlang_1.1.1          \n[127] crayon_1.5.2          rstantools_2.3.1.1    future.apply_1.11.0  \n[130] ggsignif_0.6.4        fdrtool_1.2.17        ps_1.7.5             \n[133] plyr_1.8.8            rstan_2.26.23         stringi_1.7.12       \n[136] pander_0.6.5          QuickJSR_1.0.6        munsell_0.5.0        \n[139] CompQuadForm_1.4.3    lisrelToR_0.1.5       Matrix_1.6-1         \n[142] IRkernel_1.3.2        hms_1.1.3             glasso_1.11          \n[145] future_1.33.0         shiny_1.7.5           igraph_1.5.1         \n[148] broom_1.0.5           RcppParallel_5.1.7",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Il modello multifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/05_factor_scores.html",
    "href": "chapters/fa/05_factor_scores.html",
    "title": "24  I punteggi fattoriali",
    "section": "",
    "text": "24.0.1 Esempio di interpretazione\nIl WISC-III (Wechsler Intelligence Scale For Children - III) valuta l’abilità intellettiva di soggetti dai 6 ai 16 anni e 11 mesi. I subtest sono stati selezionati per valutare diverse abilità mentali, che tutte insieme indicano l’abilità intellettiva generale del bambino. Alcuni gli richiedono un ragionamento astratto, altri si focalizzano sulla memoria, altri ancora richiedono certe abilità percettive e così via.\nSi consideri la matrice di correlazione tra i subtest della scala WISC-III riportata dal manuale.\nlower &lt;- '\n1\n.66      1\n.57 .55      1\n.70 .69 .54       1\n.56 .59 .47 .64      1\n.34 .34 .43 .35 .29      1\n.47 .45 .39 .45 .38 .25      1\n.21 .20 .27 .26 .25 .23 .18      1\n.40 .39 .35 .40 .35 .20 .37 .28      1\n.48 .49 .52 .46 .40 .32 .52 .27 .41      1\n.41 .42 .39 .41 .34 .26 .49 .24 .37 .61      1\n.35 .35 .41 .35 .34 .28 .33 .53 .36 .45 .38      1\n.18 .18 .22 .17 .17 .14 .24 .15 .23 .31 .29 .24     1\n'\nwisc_III_cov &lt;- getCov(\n  lower,\n  names = c(\n    \"INFO\", \"SIM\", \"ARITH\", \"VOC\", \"COMP\", \"DIGIT\", \"PICTCOM\",\n    \"CODING\", \"PICTARG\", \"BLOCK\", \"OBJECT\", \"SYMBOL\", \"MAZES\"\n  )\n)\nEseguiamo l’analisi fattoriale con il metodo delle componenti principali e una rotazione Varimax:\nf_pc &lt;- psych::principal(wisc_III_cov, nfactors = 3, rotate = \"varimax\")\nprint(f_pc)\n\nPrincipal Components Analysis\nCall: psych::principal(r = wisc_III_cov, nfactors = 3, rotate = \"varimax\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n          RC1  RC3  RC2   h2   u2 com\nINFO     0.80 0.25 0.09 0.72 0.28 1.2\nSIM      0.81 0.25 0.08 0.72 0.28 1.2\nARITH    0.65 0.26 0.28 0.57 0.43 1.7\nVOC      0.83 0.19 0.13 0.75 0.25 1.2\nCOMP     0.75 0.14 0.16 0.60 0.40 1.2\nDIGIT    0.45 0.06 0.36 0.34 0.66 2.0\nPICTCOM  0.43 0.61 0.02 0.56 0.44 1.8\nCODING   0.10 0.09 0.88 0.79 0.21 1.0\nPICTARG  0.34 0.45 0.27 0.39 0.61 2.6\nBLOCK    0.41 0.66 0.22 0.66 0.34 1.9\nOBJECT   0.31 0.71 0.14 0.62 0.38 1.5\nSYMBOL   0.23 0.32 0.74 0.70 0.30 1.6\nMAZES   -0.06 0.71 0.11 0.51 0.49 1.1\n\n                       RC1  RC3  RC2\nSS loadings           3.80 2.37 1.74\nProportion Var        0.29 0.18 0.13\nCumulative Var        0.29 0.47 0.61\nProportion Explained  0.48 0.30 0.22\nCumulative Proportion 0.48 0.78 1.00\n\nMean item complexity =  1.5\nTest of the hypothesis that 3 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.07 \n\nFit based upon off diagonal values = 0.97\nSi noti che i primi cinque subtest possiedono saturazioni maggiori di \\(0.6\\) sul primo fattore. Dato che questi test sono tutti presentati verbalmente e richiedono delle risposte verbali, tale fattore può essere denominato Comprensione Verbale.\nI subtest “Cifrario” e “Ricerca di simboli” saturano sul secondo fattore. Entrambi i subtest misurano la velocità dei processi di codifica o ricerca. Questo fattore, dunque, può essere denominato Velocità di elaborazione.\nInfine, i subtest “Completamento di figure,” “Disegno con i cubi,” “Riordinamento di storie figurate” e “Labirinti” saturano sul terzo fattore. Tutti questi test condividono una componente geometrica o configurazionale: misurano infatti le abilità necessarie per la manipolazione o la disposizione di immagini, oggetti, blocchi. Questo fattore, dunque, può essere denominato Organizzazione percettiva.\nNel caso di una rotazione ortogonale, la comunalità di ciascuna sottoscala è uguale alla somma dei coefficienti di impatto al quadrato della sottoscala nei fattori. Per le 13 sottoscale del WISC-III abbiamo dunque\nh2 &lt;- rep(0,13)\nfor (i in 1:13) {\n  h2[i] &lt;- sum(f_pc$loadings[i, ]^2)\n}\nround(h2, 2)\n\n\n0.720.720.570.750.60.340.560.790.390.660.620.70.51\nQuesti risultati replicano quelli riportati nel manuale del test WISC-III.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>I punteggi fattoriali</span>"
    ]
  },
  {
    "objectID": "chapters/fa/05_factor_scores.html#punteggi-fattoriali",
    "href": "chapters/fa/05_factor_scores.html#punteggi-fattoriali",
    "title": "24  I punteggi fattoriali",
    "section": "24.1 Punteggi fattoriali",
    "text": "24.1 Punteggi fattoriali\nFino ad ora abbiamo considerato le strategie di costruzione del modello basate sulla stima e sull’interpretazione delle saturazioni fattoriali e delle comunalità. Questo è il primo passo nella costruzione del modello fattoriale. È però possibile compiere un passo ulteriore, ovvero quello della stima dei punteggi fattoriali (factor scores) i quali risultano utili sia per interpretare i risultati dell’analisi fattoriale che per fare diagnostica. I punteggi fattoriali forniscono le previsioni dei livelli dei fattori latenti per ogni rispondente. Esistono vari metodi di stima dei punteggi fattoriali. Tra questi troviamo il metodo di Thomson basato sulla regressione e il metodo di Bartlett basato sulla massima verosimiglianza. Entrambi questi metodi sono implementati nel software .\n\n24.1.1 Stima dei punteggi fattoriali\nSi definiscono punteggi fattoriali i valori assunti dai fattori comuni (inosservabili) in corrispondenza delle osservazioni campionarie. Il metodo di Thomson stima i punteggi fattoriali in base all’approccio della regressione multipla, ovvero, impiegando la matrice delle correlazioni tra le variabili e la matrice di struttura (ovvero, la matrice delle correlazioni delle variabili con i fattori). Per ottenere le stime dei punteggi fattoriali con il metodo di Thomson è necessario specificare nella funzione factanal() l’opzione scores = \"regression\".\n\n\n24.1.2 Dimostrazione di Thurstone\nPrima di descrivere il metodo della regressione, esaminiamo la dimostrazione che Thurstone (1947) ha fornito per illustrare il significato dei punteggi fattoriali (si veda Loehlin, 1987). L’idea è quella di esaminare la stima dei punteggi fattoriali in una situazione in cui i tali punteggi sono conosciuti, in maniera tale da potere controllare il risultato dell’analisi.\nSi consideri un insieme di 1000 scatole di cui conosciamo le dimensioni \\(x, y, z\\):\n\nset.seed(123)\nn &lt;- 1e3\nx &lt;- rnorm(n, 100, 1.5)\ny &lt;- rnorm(n, 200, 1.5)\nz &lt;- rnorm(n, 300, 1.5)\n\nIl problema è quello di stimare le dimensioni delle scatole disponendo soltanto di una serie di misure indirette, corrotte dal rumore di misura. Thurstone (1947) utilizzò le seguenti trasformazioni delle dimensioni delle scatole (si veda Jennrich, 2007).\n\ns &lt;- 40\ny1 &lt;- rnorm(n, mean(x), s)\ny2 &lt;- rnorm(n, mean(y), s)\ny3 &lt;- rnorm(n, mean(z), s)\ny4 &lt;- x * y + rnorm(n, 0, s)\ny5 &lt;- x * z + rnorm(n, 0, s)\ny6 &lt;- y * z + rnorm(n, 0, s)\ny7 &lt;- x^2 * y + rnorm(n, 0, s)\ny8 &lt;- x * y^2 + rnorm(n, 0, s)\ny9 &lt;- x^2 * z + rnorm(n, 0, s)\ny10 &lt;- x * z^2 + rnorm(n, 0, s)\ny11 &lt;- y^2 * z + rnorm(n, 0, s)\ny12 &lt;- y * z^2 + rnorm(n, 0, s)\ny13 &lt;- y^2 * z + rnorm(n, 0, s)\ny14 &lt;- y * z^2 + rnorm(n, 0, s)\ny15 &lt;- x / y + rnorm(n, 0, s)\ny16 &lt;- y / x + rnorm(n, 0, s)\ny17 &lt;- x / z + rnorm(n, 0, s)\ny18 &lt;- z / x + rnorm(n, 0, s)\ny19 &lt;- y / z + rnorm(n, 0, s)\ny20 &lt;- z / y + rnorm(n, 0, s)\ny21 &lt;- 2 * x + 2*y + rnorm(n, 0, s)\ny22 &lt;- 2 * x + 2*z + rnorm(n, 0, s)\ny23 &lt;- 2 * y + 2*z + rnorm(n, 0, s)\n\nEseguiamo l’analisi fattoriale con una soluzione a tre fattori sui dati così creati.\n\nY &lt;- cbind(\n  y1, y2, y3, y4, y5, y6, y7, y8, y9, \n  y10, y11, y12, y13, y14, y15, y16, \n  y17, y18, y19, y20, y21, y22, y23\n)\n\nfa &lt;- factanal(\n  Y, \n  factors = 3, \n  scores = \"regression\",\n  lower = 0.01\n)\n\nL’opzione scores = \"regression\" richiede il calcolo dei punteggi fattoriali con il metodo della regressione. Nel caso di una rotazione Varimax (default della funzione factanal()), i punteggi fattoriali risultano ovviamente incorrelati:\n\ncor(\n  cbind(fa$scores[, 1], fa$scores[, 2], fa$scores[, 3])\n  ) %&gt;% \n  round(3)\n\n\nA matrix: 3 x 3 of type dbl\n\n\n1.000\n0.002\n-0.001\n\n\n0.002\n1.000\n0.005\n\n\n-0.001\n0.005\n1.000\n\n\n\n\n\nGeneriamo ora i diagrammi di dispersione che mettono in relazione le dimensioni originarie delle scatole (\\(x, y, z\\)) con i punteggi fattoriali sui tre fattori. Se l’analisi ha successo, ci aspettiamo un’alta correlazione tra i punteggi fattoriali di ogni fattore e una sola delle dimensioni delle scatole \\(x\\), \\(y\\), \\(z\\).\n\np1 &lt;- tibble(x, fs1 = fa$scores[, 1]) %&gt;% \n  ggplot(aes(x, fs1)) +\n  geom_point(alpha = 0.2)\np2 &lt;- tibble(y, fs1 = fa$scores[, 1]) %&gt;% \n  ggplot(aes(y, fs1)) +\n  geom_point(alpha = 0.2)\np3 &lt;- tibble(z, fs1 = fa$scores[, 1]) %&gt;% \n  ggplot(aes(z, fs1)) +\n  geom_point(alpha = 0.2)\n\np4 &lt;- tibble(x, fs2 = fa$scores[, 2]) %&gt;% \n  ggplot(aes(x, fs2)) +\n  geom_point(alpha = 0.2)\np5 &lt;- tibble(y, fs2 = fa$scores[, 2]) %&gt;% \n  ggplot(aes(y, fs2)) +\n  geom_point(alpha = 0.2)\np6 &lt;- tibble(z, fs2 = fa$scores[, 2]) %&gt;% \n  ggplot(aes(z, fs2)) +\n  geom_point(alpha = 0.2)\n\np7 &lt;- tibble(x, fs3 = fa$scores[, 3]) %&gt;% \n  ggplot(aes(x, fs3)) +\n  geom_point(alpha = 0.2)\np8 &lt;- tibble(y, fs3 = fa$scores[, 3]) %&gt;% \n  ggplot(aes(y, fs3)) +\n  geom_point(alpha = 0.2)\np9 &lt;- tibble(z, fs3 = fa$scores[, 3]) %&gt;% \n  ggplot(aes(z, fs3)) +\n  geom_point(alpha = 0.2)\n\n\n(p1 | p2 | p3) /\n(p4 | p5 | p6) /\n(p7 | p8 | p9) \n\n\n\n\n\n\n\n\nI risultati riportati nella figura confermano le aspettative.\nIl metodo della regressione pone il problema della stima dei punteggi fattoriali nei termini di una ideale regressione di ogni fattore rispetto a tutte le variabili osservate. Per il fattore \\(j\\)-esimo, si può scrivere la seguente equazione:\n\\[\n\\begin{aligned}\nF_j =& \\beta_{1j}y_1 + \\dots + \\beta_{pm}y_p + \\varepsilon_j\n\\end{aligned}\n\\]\ndove \\(F_j\\) sono i punteggi fattoriali e \\(y\\) sono le variabili osservate standardizzate \\((Y-\\bar{Y})/s\\). In forma matriciale, il modello diventa\n\\[\n\\textbf{F} = \\textbf{y} \\textbf{B} +\n\\boldsymbol{\\varepsilon}\n\\]\nI coefficienti parziali di regressione B sono ignoti. Tuttavia, possono essere calcolati utilizzando i metodi della regressione lineare. Nel modello di regressione, infatti, i coefficienti dei minimi quadrati possono essere calcolati utilizzando due matrici di correlazioni: la matrice \\(\\textbf{R}_{xx}\\) (le correlazioni tra le variabili \\(X\\)) e la matrice \\(\\textbf{R}_{xy}\\) (le correlazioni tra le variabili \\(X\\) e la variabile \\(Y\\):\n\\[\n\\hat{\\textbf{B}} = \\textbf{R}_{xx}^{-1}\\textbf{R}_{xy}\n\\]\nNel caso dell’analisi fattoriale, \\(\\textbf{R}_{xx}\\) corrisponde alla matrice delle correlazioni tra le variabili osservate e \\(\\textbf{R}_{xy}\\) corrisponde alla matrice di struttura (la matrice delle correlazioni tra le variabili osservate e i fattori). Se i fattori sono ortogonali, la matrice di struttura coincide con la matrice dei pesi fattoriali \\(\\hat{\\boldsymbol{\\Lambda}}\\).\nI coefficienti B dell’equazione precedente possono dunque essere trovati nel modo seguente:\n\\[\n\\begin{equation}\n\\hat{\\textbf{B}} = \\textbf{R}_{yy}^{-1}\\textbf{R}_{xf}=\n\\textbf{R}^{-1}\\hat{\\boldsymbol{\\Lambda}}\n\\end{equation}\n\\]\nUna volta stimati i coefficienti \\(\\hat{\\textbf{B}}\\), i punteggi fattoriali si calcolano allo stesso modo dei punteggi teorici del modello di regressione:\n\\[\n\\begin{equation}\n\\hat{\\textbf{F}} = \\textbf{y} \\hat{\\textbf{B}} = \\textbf{y}\n\\textbf{R}^{-1}\\hat{\\boldsymbol{\\Lambda}},\n\\end{equation}\n\\]\ndove \\(\\textbf{y}\\) è la matrice delle variabili osservate standardizzate \\((Y-\\bar{Y})/s\\).\nEsercizio. Si utilizzino i dati dass21.txt che corrispondono alla somministrazione del test DASS-21 a 334 partecipanti. Lo schema di codifica si può trovare seguendo questo link. Ci si focalizzi sulla sottoscala Stress del DASS-21. Si trovino i punteggi fattoriali usando la funzione factanal() e si replichi il risultato seguendo la procedura delineata sopra.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>I punteggi fattoriali</span>"
    ]
  },
  {
    "objectID": "chapters/fa/05_factor_scores.html#session-info",
    "href": "chapters/fa/05_factor_scores.html#session-info",
    "title": "24  I punteggi fattoriali",
    "section": "24.2 Session Info",
    "text": "24.2 Session Info\n\nsessionInfo()",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>I punteggi fattoriali</span>"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html",
    "href": "chapters/fa/06_constraints_on_parms.html",
    "title": "25  Attendibilità e modello fattoriale",
    "section": "",
    "text": "25.1 Teoria classica dei test e analisi fattoriale\n{cite:t}mcdonald2013test illustra come la teoria classica dei test possa essere correlata al modello dell’analisi fattoriale. La figura rappresenta, attraverso i termini del modello fattoriale, la relazione che sussiste tra i punteggi \\(Y\\), derivanti dalla somministrazione di un test composto da cinque item, e i punteggi veri.\nDiagramma di percorso del modello monofattoriale.\n:::\nÈ necessario ricodificare due item.\nbfi$O2r &lt;- 7 - bfi$O2\nbfi$O5r &lt;- 7 - bfi$O5\ncor(bfi[c(\"O1\", \"O2r\", \"O3\", \"O4\", \"O5r\")], use = \"pairwise.complete.obs\") |&gt;\n    round(2)\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nO1\nO2r\nO3\nO4\nO5r\n\n\n\n\nO1\n1.00\n0.21\n0.40\n0.18\n0.24\n\n\nO2r\n0.21\n1.00\n0.26\n0.07\n0.32\n\n\nO3\n0.40\n0.26\n1.00\n0.19\n0.31\n\n\nO4\n0.18\n0.07\n0.19\n1.00\n0.18\n\n\nO5r\n0.24\n0.32\n0.31\n0.18\n1.00\nEseguiamo l’analisi fattoriale confermativa con lavaan.\nmod &lt;- \"\n    f =~ NA*O1 + O2r + O3 + O4 + O5r\n    f ~~ 1*f\n\"\n\nfit &lt;- cfa(mod, data = bfi, std.ov = TRUE, std.lv = TRUE)\nEstraiamo le saturazioni fattoriali e le specificità dall’oggetto fit.\nlambda &lt;- inspect(fit, what = \"std\")$lambda\npsy &lt;- diag(inspect(fit, what = \"est\")$theta)\nCalcoliamo il coefficiente \\(\\omega\\)\n\\[\n\\omega = \\frac{\\left( \\sum_{i=1}^p \\lambda_i \\right)^2}{\\left( \\sum_{i=1}^p \\lambda_i \\right)^2  + \\sum_{i=1}^p \\psi_{ii}}\n\\]\nusando i parametri del modello fattoriale.\nsum(lambda)^2 / (sum(lambda)^2 + sum(psy))\n\n0.618064541281708\nRipetiamo i calcoli usando la funzione compRelSEM del pacchetto semTools.\nsemTools::compRelSEM(fit, tau.eq = FALSE)\n\nf: 0.618110889806653\nIl coefficiente \\(\\omega=0.62\\) può essere interpretato dicendo che il 62% della varianza del punteggio totale \\(Y\\) della sottoscala Openness viene spiegato dal fattore comune latente.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html#teoria-classica-dei-test-e-analisi-fattoriale",
    "href": "chapters/fa/06_constraints_on_parms.html#teoria-classica-dei-test-e-analisi-fattoriale",
    "title": "25  Attendibilità e modello fattoriale",
    "section": "",
    "text": "```kxwdmvmh ../images/factmod1.png\n\n\n\n\nheight: 300px\n\n\nname: lcsm1-fig\n\n\n\n\n\nEsistono diverse strategie per stimare l'attendibilità in situazioni in cui viene somministrato un unico test. In questo contesto, analizzeremo tre metodologie che possono essere implementate attraverso l'analisi fattoriale: l'$\\alpha$ di Cronbach, l'$\\omega$ di McDonald e il metodo di Spearman-Brown.\n\nIl coefficiente $\\alpha$ rappresenta il principale indice utilizzato per quantificare l'attendibilità come misura di coerenza interna o omogeneità. Approfondiremo come questo indice rappresenti il limite inferiore dell'attendibilità di un test, a condizione che siano soddisfatte alcune ipotesi. Tuttavia, se queste assunzioni non vengono rispettate, l'$\\alpha$ si rivela un stimatore distorto dell'attendibilità.\n\nPrima di esaminare le diverse metodologie per stimare l'attendibilità in termini di coerenza interna, è essenziale distinguere tra le tre diverse forme che il modello unifattoriale può assumere. Queste tre forme corrispondono al modello con indicatori congenerici, al modello $\\tau$-equivalente e al modello parallelo.\n\n## Modello fattoriale e CTT\n\nConsiderando un insieme di item osservati $X_1, X_2, \\dots, X_p$, con $p&gt;2$, i punteggi ottenuti da questi item sono composti da due elementi distinti: una componente di punteggio vero e una componente di errore.\n\n$$\n\\begin{equation}\n\\begin{aligned}\nX_1 &=T_1+E_1,\\notag\\\\ \nX_2 &=T_2+E_2,\\notag\\\\ \n&\\dots\\notag\\\\ \nX_p &=T_p+E_p.\\notag\n\\end{aligned}\n\\end{equation}\n$$\n\nIn linea con l'approccio delineato da {cite:t}`mcdonald2013test`, questa decomposizione tra la componente vera e quella di errore può essere formalizzata mediante l'utilizzo dei parametri del modello fattoriale. L'equazione $X_i = T_i + E_i$ può quindi essere riformulata come segue:\n\n$$\nX_i = \\lambda_i \\xi + \\delta_i, \\quad{i=1, \\dots, p},\n$$ \n\nIn questa equazione, $X_i$ rappresenta il punteggio osservato per l'item $i$-esimo (espresso in termini di scarti dalla media), $\\lambda_i$ è il carico fattoriale associato all'item $i$-esimo, $\\xi$ costituisce il fattore comune e $\\delta_i$ è la componente residuale del punteggio osservato per l'item $i$-esimo. Tale formulazione si basa sulle assunzioni del modello monofattoriale. Nello specifico, si ipotizza che $\\xi$ e $\\delta_i$ siano incorrelati per ogni item $i$, e che $\\delta_i$ e $\\delta_k$ siano incorrelati per ogni coppia $i \\neq k$.\n\n## Classi di modelli\n\nNell'ambito dei modelli monofattoriali, possiamo distinguere tre scenari principali:\n\n1. **Modello con indicatori congenerici:** Questo modello rappresenta il caso più generale, in cui non vi sono restrizioni imposte sulla struttura degli indicatori. Gli indicatori sono correlati in quanto riflettono un fattore comune, ma possono avere carichi fattoriali diversi e specificità uniche.\n\n2. **Modello con indicatori $\\tau$-equivalenti:** In questo scenario, tutti gli indicatori hanno lo stesso carico fattoriale, il che implica che misurano il fattore comune con la stessa forza. Tuttavia, possono differire per quanto riguarda la loro varianza e specificità.\n\n3. **Modello con indicatori paralleli:** Qui, gli indicatori non solo condividono lo stesso carico fattoriale, ma presentano anche identica varianza degli errori. Questo indica una completa equivalenza tra gli indicatori, mostrando una struttura molto più rigida rispetto al modello $\\tau$-equivalente.\n\nIl modello con indicatori congenerici funge da base più flessibile, mentre i modelli con indicatori $\\tau$-equivalenti e paralleli introducono vincoli crescenti che specificano relazioni sempre più strette tra gli indicatori.\n\n### Indicatori congenerici\n\nGli indicatori *congenerici* rappresentano misure di uno stesso costrutto, ma non è necessario che riflettano tale costrutto con la medesima intensità. Nel contesto degli indicatori congenerici all'interno del modello monofattoriale, non vengono introdotte limitazioni né sulle saturazioni fattoriali né sulle specificità:\n\n$$\n\\lambda_1\\neq \\lambda_2 \\neq \\dots\\neq \\lambda_p,\n$$\n\n$$\n\\psi_{11}\\neq \\psi_{22} \\neq \\dots\\neq \\psi_{pp}.\n$$ \n\nIl modello mono-fattoriale con indicatori congenerici è dunque\n\n$$\n\\begin{equation}\nX_i = \\lambda_i \\xi + \\delta_i.\n\\end{equation}\n$$(eq-mod-tau-eq)\n\nDalle assunzioni precedenti possiamo derivare la matrice $\\boldsymbol{\\Sigma}$ riprodotta in base al modello congenerico la quale risulta essere uguale a\n\n$$\n\\boldsymbol{\\Sigma}=\\left[\n      \\begin{array}{ c c c c }\n        \\sigma_{11} & \\sigma_{12} & \\dots & \\sigma_{1p}, \\\\\n        \\sigma_{21} & \\sigma_{22} & \\dots & \\sigma_{2p}. \\\\\n        \\vdots & \\vdots & & \\vdots\\\\\n        \\sigma_{p1} & \\sigma_{p2} & \\dots & \\sigma_{pp} \n      \\end{array} \n    \\right].\n$$ \n    \nSi noti come tutte le varianze e tutte le covarianze siano tra loro diverse.\n\n### Indicatori tau-equivalenti\n\nNel caso di indicatori $\\tau$-equivalenti, si ha che\n\n$$\n\\lambda_1=\\lambda_2=\\dots=\\lambda_p=\\lambda,\n$$\n\n$$\n\\psi_{11}\\neq \\psi_{22} \\neq \\dots\\neq \\psi_{pp}.\n$$ \n\nIl modello monofattoriale con indicatori $\\tau$-equivalenti diventa dunque\n\n$$\n\\begin{equation}\nX_i = \\lambda \\xi + \\delta_i, \n\\end{equation}\n$$(eq-mod-tau-eq)\n\novvero \n\n$$\n\\begin{equation}\nX_i = \\tau + \\delta_i,\n\\end{equation}\n$$(eq-mod-tau-eq-b)\n\ndove $\\tau=\\lambda \\xi$ è l'attributo comune scalato nell'unità di misura dell'indicatore. Secondo il modello {eq}`eq-mod-tau-eq`, tutte le $p(p-1)$ covarianze tra gli item\ndel test devono essere uguali, ovvero\n\n$$\n\\begin{equation}\n\\sigma_{ik} = \\lambda^2=\\sigma^2_T,\n\\end{equation}\n$$(eq-cov-tau-eq)\n\nper $i\\neq k$. Gli elementi sulla diagonale principale della matrice di varianze e covarianze saranno invece\n\n$$\n\\begin{equation}\n\\sigma_{ii} = \\lambda^2 + \\psi_{ii} =\\sigma^2_T + \\psi_{ii}.\n\\end{equation}\n$$(eq-var-tau)\n\nLa matrice $\\boldsymbol{\\Sigma}$ riprodotta in base al modello $\\tau$-equivalente è dunque uguale a\n\n$$\n\\begin{equation}\n\\boldsymbol{\\Sigma}=\\left[\n      \\begin{array}{ c c c c }\n        \\sigma_{T}^2 + \\psi_{11} & \\sigma_{T}^2 & \\dots & \\sigma_{T}^2 \\\\\n        \\sigma_{T}^2 & \\sigma_{T}^2 + \\psi_{22} & \\dots & \\sigma_{T}^2 \\\\\n        \\vdots & \\vdots & & \\vdots\\\\\n        \\sigma_{T}^2 & \\sigma_{T}^2 & \\dots & \\sigma_{T}^2 + \\psi_{pp} \n      \\end{array} \n    \\right].\n\\end{equation}\n$$(eq-sigma-tau-eq)\n    \nTutte le covarianze sono uguali, mentre le varianze sono tra loro diverse.\n\n### Indicatori paralleli\n\nNel caso di indicatori paralleli si ha che\n\n$$\n\\lambda_1=\\lambda_2=\\dots=\\lambda_p=\\lambda,\n$$\n\n$$\n\\psi_{11}=\\psi_{22}=\\dots=\\psi_{pp}=\\psi.\n$$ \n\nIl modello costituito da indicatori paralleli impone dunque un'ulteriore restrizione che riguarda le varianze degli item, ovvero:\n\n$$\n\\sigma_{ii} = \\lambda^2 + \\psi =\\sigma^2_T + \\sigma^2.\n$$ \n\nLa struttura di varianze e covarianze imposta dal modello per indicatori paralleli è\ndunque tale da richiedere l'uguaglianza tra tutte le covarianze tra gli\nitem e l'uguaglianza tra tutte le varianze degli item. La matrice\n$\\boldsymbol{\\Sigma}$ riprodotta in base al modello con indicatori\nparalleli è dunque uguale a \n\n$$\n\\boldsymbol{\\Sigma}=\\left[\n      \\begin{array}{ c c c c }\n        \\sigma_{T}^2 + \\sigma^2 & \\sigma_{T}^2 & \\dots & \\sigma_{T}^2 \\\\\n        \\sigma_{T}^2 & \\sigma_{T}^2 + \\sigma^2 & \\dots & \\sigma_{T}^2 \\\\\n        \\vdots & \\vdots & & \\vdots\\\\\n        \\sigma_{T}^2 & \\sigma_{T}^2 & \\dots & \\sigma_{T}^2 +\\sigma^2 \\notag\n      \\end{array} \n    \\right].\n$$\n\n\n## Metodo dei minimi quadrati non pesati\n\nNel contesto del modello unifattoriale, la varianza di ciascun indicatore è decomposta in due componenti: la componente $\\sigma^2_T$, attribuibile all'effetto del fattore latente comune, e la componente $\\psi$, riferita all'influenza del fattore specifico. {cite:t}`mcdonald2013test` dimostra come sia possibile ottenere stime di tali componenti dai dati osservati. Queste stime vengono successivamente impiegate per calcolare l'affidabilità interna del test mediante le formule degli indici $\\alpha$ di Cronbach e $\\omega$ di McDonald.\n\nIn precedenza, abbiamo esaminato come la varianza del punteggio vero possa essere equivalente alla covarianza tra due forme parallele dello stesso test: $\\sigma^2_T = \\sigma_{XX^\\prime}$. Nel caso di indicatori $\\tau$-equivalenti, la matrice $\\boldsymbol{\\Sigma}$ prevista dal modello risulta essere:\n\n$$\n\\boldsymbol{\\Sigma}=\\left[\n      \\begin{array}{ c c c c }\n        \\sigma_{T}^2 + \\psi_{11} & \\sigma_{T}^2 & \\dots & \\sigma_{T}^2 \\\\\n        \\sigma_{T}^2 & \\sigma_{T}^2 + \\psi_{22} & \\dots & \\sigma_{T}^2 \\\\\n        \\vdots & \\vdots & & \\vdots\\\\\n        \\sigma_{T}^2 & \\sigma_{T}^2 & \\dots & \\sigma_{T}^2 + \\psi_{pp} \\notag\n      \\end{array}\n    \\right],\n$$\n\nossia, tutte le covarianze sono equivalenti tra loro. Nel caso degli indicatori $\\tau$-equivalenti, dunque, una stima $\\hat{\\sigma}^2_T$ di $\\sigma^2_T$ si ottiene calcolando la media delle covarianze della matrice **S**:\n\n$$\n\\begin{equation}\n\\hat{\\sigma}_T^2 = \\frac{1}{p(p-1)} {\\sum \\sum}_{i \\neq k} s_{ik}.\n\\end{equation}\n$$ (eq-sigma-t)\n\nQuesto metodo di stima di $\\sigma^2_T$ è noto come \"metodo dei minimi quadrati non pesati\" {cite:p}`mcdonald2013test`.\n\nInoltre, nel caso di indicatori $\\tau$-equivalenti, la stima di $\\psi_{ii}$ nell'eq. {eq}`eq-var-tau` è calcolata come:\n\n$$\n\\hat{\\psi}_{ii }= s_{ii} - \\hat{\\sigma}_T^2,\n$$\n\nper ogni item $i$.\n\nPer quanto riguarda gli *indicatori paralleli*, la stima di $\\sigma^2_T$ è ancora basata sull'eq. {eq}`eq-sigma-t`, ovvero sulla media delle covarianze della matrice $\\boldsymbol{\\Sigma}$. Tuttavia, la stima del valore costante $\\psi$ è ottenuta tramite l'equazione:\n\n$$\n\\begin{equation}\n\\hat{\\psi} = \\frac{1}{p} \\sum_i (s_{ii} - \\hat{\\sigma}_T^2)\n\\end{equation}\n$$(eq-psi-par-st)\n\n## Varianza del punteggio totale di un test\n\nConsideriamo un test omogeneo costituito da $p$ item, il cui punteggio totale $Y$ è dato dalla somma dei punteggi individuali degli item, espressi come $Y = \\sum_{i=1}^p X_i$. Analizziamo la varianza di $Y$ utilizzando un modello unifattoriale.\n\nIn un modello congenerico con un singolo fattore comune, il punteggio di ciascun item $i$, $X_i$, può essere rappresentato dalla seguente equazione:\n\n$$\nX_i = \\lambda_i \\xi + \\delta_i,\n$$\n\ndove $\\lambda_i$ rappresenta la carica fattoriale dell'item $i$ sul fattore comune $\\xi$, e $\\delta_i$ è l'errore specifico associato all'item. Questa formulazione è analoga all'equazione $X_i = T_i + E_i$ della teoria classica dei test, dove $T_i$ è il vero punteggio e $E_i$ l'errore di misurazione.\n\nIl punteggio totale, essendo la somma di tutti gli item, si esprime come $\\sum_i (\\lambda_i \\xi + \\delta_i)$. La varianza del punteggio totale può quindi essere calcolata come segue:\n\n$$\n\\begin{equation}\n\\begin{aligned}\n  \\mathbb{V}(Y) &= \\mathbb{V}\\left[ \\sum_i  (\\lambda_i \\xi + \\delta_i)  \\right] \\\\\n  &= \\mathbb{V}\\left[ \\left( \\sum_i \\lambda_i\\right) \\xi + \\sum_i \\delta_i\\right] \\\\\n  &=  \\left(\\sum_i \\lambda_i\\right)^2 \\mathbb{V}(\\xi) +  \\sum_i  \\mathbb{V}(\\delta_i) \\\\\n  &= \\left(\\sum_i \\lambda_i\\right)^2 + \\sum_i \\psi_{ii},\n\\end{aligned}\n\\end{equation}\n$$ (eq-var-y)\n\ndove $\\mathbb{V}(\\xi) = 1$ per ipotesi. La varianza di $Y$ si decompone in due parti principali: la prima parte, $(\\sum_i \\lambda_i)^2$, rappresenta la varianza attribuibile al fattore comune, riflettendo la variazione legata all'attributo misurato dagli item; la seconda parte, $\\sum_i \\psi_{ii}$, corrisponde alla somma delle varianze degli errori specifici di ciascun item, rappresentando la variazione dovuta agli errori di misurazione.\n\n## Stima dell'attendibilità\n\n### Coefficiente Omega\n\nDopo aver analizzato la varianza del punteggio totale di un test come indicato nella precedente equazione:\n\n$$\n\\mathbb{V}(Y) = \\left( \\sum_i \\lambda_i\\right)^2 + \\sum_i \\psi_{ii},\n$$\n\nsi introduce il coefficiente di affidabilità $\\omega$. {cite:t}`mcdonald2013test` definisce $\\omega$ come il rapporto tra la varianza attribuibile al fattore comune e la varianza totale del punteggio. Basandosi sui parametri del modello monofattoriale, il coefficiente $\\omega$ può essere formulato come segue:\n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\omega &= \\frac{\\left( \\sum_{i=1}^p \\lambda_i \\right)^2}{\\mathbb{V}(Y)} \\\\\n&= \\frac{\\left( \\sum_{i=1}^p \\lambda_i \\right)^2}{\\left( \\sum_{i=1}^p \\lambda_i \\right)^2  + \\sum_{i=1}^p \\psi_{ii}}\n\\end{aligned}\n\\end{equation}\n$$ (eq-omega)\n\nQuesto coefficiente $\\omega$ offre una stima quantitativa dell'affidabilità di un test, basata sui parametri del modello congenerico e utilizzando i dati raccolti da una singola somministrazione del test. La sua utilità risiede nel quantificare quanto della varianza osservata nel punteggio totale è effettivamente spiegata dal fattore comune misurato dal test.\n\n#### Un esempio concreto\n\nConsideriamo nuovamente la scala *Openness* del dataframe `bfi` discussi nel capitolo {ref}`ctt-3-notebook`. Leggiamo i dati in R.\n\n::: {#d3363486 .cell vscode='{\"languageId\":\"r\"}' execution_count=2}\n``` {.r .cell-code}\ndata(bfi, package = \"psych\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n25.1.0.1 Coefficiente \\(\\omega\\) e assunzioni della teoria classica dei test\nIl calcolo del coefficiente \\(\\omega\\) si appoggia su un’assunzione fondamentale della teoria classica dei test: che non esistano covarianze tra gli errori specifici degli item, ossia \\(\\psi_{ik}=0\\) per ogni \\(i \\neq k\\). Tuttavia, questa ipotesi potrebbe non reggere in contesti di dati empirici. Bollen (1980) sottolinea che, qualora le covarianze tra errori specifici non siano trascurabili, l’equazione per \\(\\omega\\) dovrebbe essere modificata come segue:\n\\[\n\\begin{equation}\n\\omega = \\frac{\\left( \\sum_{i=1}^p \\lambda_i \\right)^2}{\\left( \\sum_{i=1}^p \\lambda_i \\right)^2  + \\sum_{i=1}^p \\psi_{ii} + \\sum_{i, k, i\\neq k}^p \\psi_{ik}}.\n\\end{equation}\n\\]\nPer verificare la validità dell’assunzione di indipendenza tra gli errori specifici, si può ricorrere a un’analisi fattoriale confermativa. Se l’analisi rivela correlazioni significative tra molti errori specifici, potrebbe essere necessario incorporare ulteriori fattori nel modello per accomodare queste covarianze. Questo può suggerire una struttura non più unidimensionale, indicando la presenza di diverse sottoscale all’interno del test. Tuttavia, anche con l’identificazione di tali sottoscale, le covarianze tra i fattori specifici possono rimanere inesplicate. In tali casi, l’uso dell’equazione modificata per \\(\\omega\\) diventa indispensabile.\n\n\n25.1.0.2 Interpretazione del Coefficiente \\(\\omega\\)\n{cite:t}mcdonald2013test propone diverse interpretazioni del coefficiente \\(\\omega\\) che aiutano a comprenderne il significato nel contesto della teoria dei test: - \\(\\omega\\) può essere visto come il quadrato della correlazione tra il punteggio totale \\(Y\\) e il fattore comune \\(\\xi\\), che rappresenta anche la correlazione tra \\(Y\\) e il punteggio vero. Questo si allinea alla definizione classica di affidabilità, espressa come \\(\\rho_{XT}^2 = \\sigma^2_{\\tau}/\\sigma^2_X\\), dove \\(\\sigma^2_{\\tau}\\) è la varianza del punteggio vero e \\(\\sigma^2_X\\) quella del punteggio osservato. - \\(\\omega\\) descrive anche la correlazione tra due applicazioni ipotetiche del test, \\(Y\\) e \\(Y'\\), che condividono le stesse somme (o medie) delle cariche fattoriali e delle varianze specifiche nel contesto di un modello a singolo fattore. - \\(\\omega\\) rappresenta il quadrato della correlazione tra il punteggio totale di \\(p\\) item e il punteggio medio di un insieme infinito di item all’interno di un dominio omogeneo, dove i \\(p\\) item analizzati sono un sottoinsieme rappresentativo.\nIn sintesi, il coefficiente \\(\\omega\\) fornisce una misura di quanto il punteggio totale di un test sia rappresentativo del fattore latente che il test intende misurare. Attraverso la correlazione, l’omogeneità e la consistenza osservata tra diverse somministrazioni o versioni di un test, \\(\\omega\\) aiuta a interpretare la qualità e l’affidabilità del test stesso.\n\n\n25.1.1 Coefficienti \\(\\alpha\\) e \\(\\omega\\) nel modello \\(\\tau\\)-equivalente\nNel contesto dei modelli monofattoriali, i coefficienti \\(\\omega\\) e \\(\\alpha\\) offrono stime dell’attendibilità, ma in contesti distinti. Il coefficiente \\(\\omega\\) è utile per i modelli con indicatori congenerici, mentre il coefficiente \\(\\alpha\\) è specifico per i modelli con indicatori \\(\\tau\\)-equivalenti.\nIn un modello \\(\\tau\\)-equivalente, dove ciascun item ha la stessa carica fattoriale \\(\\lambda\\), la varianza di ogni item si scompone in una parte dovuta al punteggio vero e una parte d’errore, espressa come \\(\\sigma_{ii} = \\lambda^2 + \\psi_{ii} = \\sigma^2_T + \\sigma^2_i\\). In questo scenario, la formula per il coefficiente \\(\\omega\\) si semplifica nel seguente modo:\n\\[\n\\omega = \\frac{\\left( \\sum_i \\lambda_i \\right)^2}{\\left( \\sum_i \\lambda_i \\right)^2  + \\sum_i \\psi_{ii}} = \\frac{p^2 \\lambda^2}{\\sigma^2_Y} = \\frac{p^2 \\sigma_T^2}{\\sigma_Y^2},\n\\]\ndove \\(Y\\) rappresenta il punteggio totale del test.\nApplicando il metodo dei minimi quadrati non pesati, possiamo derivare la stima seguente per \\(\\omega\\):\n\\[\n\\hat{\\omega} = \\frac{p^2 \\hat{\\sigma}_T^2}{s_Y^2},\n\\]\ndove \\(\\hat{\\sigma}_T^2\\) è stimato come:\n\\[\n\\hat{\\sigma}_T^2 = \\frac{1}{p(p-1)} \\sum \\sum_{i \\neq k} s_{ik}.\n\\]\nIntegrando questa stima nella formula precedente, otteniamo:\n\\[\n\\hat{\\omega} = \\frac{p}{p-1}\\frac{\\sum \\sum_{i \\neq k} s_{ik}}{s_Y^2}.\n\\]\nPer gli indicatori \\(\\tau\\)-equivalenti, quindi, \\(\\omega\\) può essere stimato da:\n\\[\n\\hat{\\omega} = \\frac{p}{p-1}\\left(1-\\frac{\\sum_i s_{ii}}{s_Y^2}\\right).\n\\] (eq-alpha-camp)\nQuesta stima di \\(\\omega\\) ha un parallelo nei valori di popolazione definiti da \\(\\alpha\\), che si esprime come:\n\\[\n\\alpha = \\frac{p}{p-1}\\left(1-\\frac{\\sum_{i=1}^p \\sigma_{ii}}{\\sigma_Y^2}\\right) = \\frac{p}{p-1}\\frac{\\sum_{i \\neq k}^p \\text{Cov}(X_i, X_k)}{\\mathbb{V}(Y)}.\n\\] (eq-alpha-pop)\nIn condizioni ideali del modello \\(\\tau\\)-equivalente, i valori di \\(\\alpha\\) e \\(\\omega\\) convergono. Tuttavia, \\(\\alpha\\) tende a sottostimare \\(\\omega\\), posizionandosi come un limite inferiore per \\(\\omega\\). Data questa natura conservativa di \\(\\alpha\\), alcuni ricercatori lo preferiscono a \\(\\omega\\), sebbene questa proprietà valga solamente quando le assunzioni del modello \\(\\tau\\)-equivalente sono rigorosamente rispettate.\n\n25.1.1.1 Un esempio concreto\nConsideriamo la matrice di varianze e covarianze della sottoscala Openness.\n\nC &lt;- cov(bfi[c(\"O1\", \"O2r\", \"O3\", \"O4\", \"O5r\")], use = \"pairwise.complete.obs\")\nC |&gt; round(2)\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nO1\nO2r\nO3\nO4\nO5r\n\n\n\n\nO1\n1.28\n0.38\n0.54\n0.25\n0.36\n\n\nO2r\n0.38\n2.45\n0.50\n0.13\n0.67\n\n\nO3\n0.54\n0.50\n1.49\n0.29\n0.50\n\n\nO4\n0.25\n0.13\n0.29\n1.49\n0.29\n\n\nO5r\n0.36\n0.67\n0.50\n0.29\n1.76\n\n\n\n\n\nCalcoliamo il coefficiente \\(\\alpha\\) usando l’eq. {eq}eq-alpha-camp:\n\np &lt;- 5\nalpha &lt;- (p / (p - 1)) * (1 - tr(C) / sum(C))\nalpha\n\n0.600172514820215\n\n\n\n\n\n25.1.2 La formula “profetica” di Spearman-Brown\nLa formula profetica di Spearman-Brown è impiegata per calcolare l’affidabilità nei modelli di misurazione che utilizzano indicatori paralleli. Supponiamo di avere un test composto da \\(p\\) item paralleli, in cui ogni item ha la stessa carica fattoriale \\(\\lambda\\) e la stessa varianza dell’errore specifico \\(\\psi\\), ovvero \\(\\lambda_1=\\lambda_2=\\dots=\\lambda_p=\\lambda\\) e \\(\\psi_{11}=\\psi_{22}=\\dots=\\psi_{pp}=\\psi\\).\nLa proporzione di varianza nel punteggio totale del test spiegata dalla variabile latente è quindi:\n\\[\n\\left(\\sum_i \\lambda_i \\right)^2 = (p \\lambda)^2 = p^2 \\lambda^2.\n\\]\nDefinendo l’affidabilità di un singolo item, \\(\\rho_1\\), come\n\\[\n\\rho_1 = \\frac{\\lambda^2}{\\lambda^2 + \\psi},\n\\]\nper \\(p\\) item paralleli, l’affidabilità del test, \\(\\rho_p\\), diventa:\n\\[\n\\begin{equation}\n\\begin{aligned}\n  \\rho_p &= \\frac{p^2 \\lambda^2}{p^2 \\lambda^2 + p \\psi} \\\\\n         &= \\frac{p \\lambda^2}{ p \\lambda^2 + \\psi} \\\\\n         &= \\frac{p \\lambda^2}{(p-1) \\lambda^2 + (\\lambda^2 + \\psi)}.\n\\end{aligned}\n\\end{equation}\n\\]\nSfruttando l’affidabilità di un singolo item \\(\\rho_1\\), possiamo riformulare \\(\\rho_p\\) come:\n\\[\n\\begin{equation}\n\\begin{aligned}\n  \\rho_p &= \\frac{p \\rho_1}{(p-1)\\rho_1 + 1}.\n\\end{aligned}\n\\end{equation}\n\\] (eq-spearman-brown-der)\nQuesta espressione, derivata qui sopra, mostra come l’affidabilità \\(\\rho_p\\) di un test composto da \\(p\\) item paralleli possa essere calcolata a partire dall’affidabilità di un singolo item. Tale formula è nota come “formula di predizione” di Spearman-Brown (Spearman-Brown prophecy formula).\nIn contesti con item paralleli, è importante notare che le misure di affidabilità \\(\\omega\\), \\(\\alpha\\), e \\(\\rho_p\\) risultano equivalenti.\n\n25.1.2.1 Un esempio concreto\nPoniamoci il problema di calcolare l’attendibilità della sottoscala Openness utilizzando la formula di Spearman-Brown. Ipotizziamo dunque che gli item della scala Openness siano paralleli. La matrice di correlazione è:\n\nR &lt;- cor(bfi[c(\"O1\", \"O2r\", \"O3\", \"O4\", \"O5r\")], use = \"pairwise.complete.obs\")\nprint(R)\n\n           O1       O2r        O3        O4       O5r\nO1  1.0000000 0.2137348 0.3953359 0.1783758 0.2389921\nO2r 0.2137348 1.0000000 0.2615580 0.0683203 0.3248698\nO3  0.3953359 0.2615580 1.0000000 0.1945048 0.3106404\nO4  0.1783758 0.0683203 0.1945048 1.0000000 0.1790512\nO5r 0.2389921 0.3248698 0.3106404 0.1790512 1.0000000\n\n\nSeguendo {cite:t}mcdonald2013test, supponiamo di calcolare l’attendibilità di un singolo item (\\(\\rho_1\\)) come la correlazione media tra gli item:\n\nrr &lt;- NULL\np &lt;- 5\nk &lt;- 1\nfor (i in 1:p) {\n  for (j in 1:p) {\n    if (j != i) {\n      rr[k] &lt;- R[i, j]\n    }\n    k &lt;- k + 1\n  }\n}\nro_1 &lt;- mean(rr, na.rm = TRUE)\nro_1\n\n0.236538319550858\n\n\nApplicando la formula di Spearman-Brown, la stima dell’attendibilità del test diventa pari a\n\n(p * ro_1) / ((p - 1) * ro_1 + 1)\n\n0.607707322439719",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html#commenti-e-considerazioni-conclusive",
    "href": "chapters/fa/06_constraints_on_parms.html#commenti-e-considerazioni-conclusive",
    "title": "25  Attendibilità e modello fattoriale",
    "section": "25.2 Commenti e considerazioni conclusive",
    "text": "25.2 Commenti e considerazioni conclusive\nIl coefficiente \\(\\alpha\\) di Cronbach è uno degli indici di affidabilità più diffusi in psicometria. Tuttavia, la sua efficacia dipende strettamente dalla \\(\\tau\\)-equivalenza degli item, che presuppongono un tratto latente unidimensionale. Nella pratica, questa condizione è spesso violata: molti test misurano più di un fattore, e le comunalità degli item non sono uniformi, mettendo in discussione la validità dell’ipotesi di \\(\\tau\\)-equivalenza. Se gli errori sono incorrelati, il coefficiente \\(\\alpha\\) può sottostimare l’affidabilità; se invece gli errori sono correlati, può sovrastimarla.\nData questa limitazione, l’utilizzo del coefficiente \\(\\omega\\) di McDonald è generalmente più consigliabile. Il coefficiente \\(\\omega\\) fornisce una stima più robusta dell’affidabilità in vari contesti, inclusi quelli con assunzioni meno restrittive rispetto alla \\(\\tau\\)-equivalenza. Altri indici come il \\(glb\\) (Greatest Lower Bound), discusso da Ten Berge e Sočan (2004), e l’indice \\(\\beta\\) di Revelle (1979), rappresentano alternative valide al coefficiente \\(\\alpha\\), offrendo diversi vantaggi metodologici a seconda delle specifiche esigenze di misurazione e delle caratteristiche dei dati analizzati.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html#session-info",
    "href": "chapters/fa/06_constraints_on_parms.html#session-info",
    "title": "25  Attendibilità e modello fattoriale",
    "section": "25.3 Session Info",
    "text": "25.3 Session Info\n\nsessionInfo()\n\nR version 4.3.2 (2023-10-31)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.3.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] modelsummary_1.4.5 ggokabeito_0.1.0   viridis_0.6.5      viridisLite_0.4.2 \n [5] ggpubr_0.6.0       ggExtra_0.10.1     bayesplot_1.11.1   gridExtra_2.3     \n [9] patchwork_1.2.0    semTools_0.5-6     semPlot_1.1.6      lavaan_0.6-17     \n[13] psych_2.4.1        scales_1.3.0       markdown_1.12      knitr_1.45        \n[17] lubridate_1.9.3    forcats_1.0.0      stringr_1.5.1      dplyr_1.1.4       \n[21] purrr_1.0.2        readr_2.1.5        tidyr_1.3.1        tibble_3.2.1      \n[25] ggplot2_3.4.4      tidyverse_2.0.0    here_1.0.1        \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.15.0  jsonlite_1.8.8     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5   nloptr_2.0.3      \n  [7] rmarkdown_2.25     vctrs_0.6.5        minqa_1.2.6       \n [10] base64enc_0.1-3    rstatix_0.7.2      htmltools_0.5.7   \n [13] broom_1.0.5        Formula_1.2-5      htmlwidgets_1.6.4 \n [16] plyr_1.8.9         sandwich_3.1-0     emmeans_1.10.0    \n [19] zoo_1.8-12         uuid_1.2-0         igraph_2.0.2      \n [22] mime_0.12          lifecycle_1.0.4    pkgconfig_2.0.3   \n [25] Matrix_1.6-5       R6_2.5.1           fastmap_1.1.1     \n [28] shiny_1.8.0        digest_0.6.34      OpenMx_2.21.11    \n [31] fdrtool_1.2.17     colorspace_2.1-0   rprojroot_2.0.4   \n [34] Hmisc_5.1-1        fansi_1.0.6        timechange_0.3.0  \n [37] abind_1.4-5        compiler_4.3.2     withr_3.0.0       \n [40] glasso_1.11        htmlTable_2.4.2    backports_1.4.1   \n [43] carData_3.0-5      ggsignif_0.6.4     MASS_7.3-60.0.1   \n [46] corpcor_1.6.10     gtools_3.9.5       tools_4.3.2       \n [49] pbivnorm_0.6.0     foreign_0.8-86     zip_2.3.1         \n [52] httpuv_1.6.14      nnet_7.3-19        glue_1.7.0        \n [55] quadprog_1.5-8     nlme_3.1-164       promises_1.2.1    \n [58] lisrelToR_0.3      grid_4.3.2         pbdZMQ_0.3-11     \n [61] checkmate_2.3.1    cluster_2.1.6      reshape2_1.4.4    \n [64] generics_0.1.3     gtable_0.3.4       tzdb_0.4.0        \n [67] data.table_1.15.0  hms_1.1.3          car_3.1-2         \n [70] utf8_1.2.4         tables_0.9.17      sem_3.1-15        \n [73] pillar_1.9.0       IRdisplay_1.1      rockchalk_1.8.157 \n [76] later_1.3.2        splines_4.3.2      lattice_0.22-5    \n [79] survival_3.5-8     kutils_1.73        tidyselect_1.2.0  \n [82] miniUI_0.1.1.1     pbapply_1.7-2      stats4_4.3.2      \n [85] xfun_0.42          qgraph_1.9.8       arm_1.13-1        \n [88] stringi_1.8.3      boot_1.3-29        evaluate_0.23     \n [91] codetools_0.2-19   mi_1.1             cli_3.6.2         \n [94] RcppParallel_5.1.7 IRkernel_1.3.2     rpart_4.1.23      \n [97] xtable_1.8-4       repr_1.1.6         munsell_0.5.0     \n[100] Rcpp_1.0.12        coda_0.19-4.1      png_0.1-8         \n[103] XML_3.99-0.16.1    parallel_4.3.2     ellipsis_0.3.2    \n[106] jpeg_0.1-10        lme4_1.1-35.1      mvtnorm_1.2-4     \n[109] insight_0.19.8     openxlsx_4.2.5.2   crayon_1.5.2      \n[112] rlang_1.1.3        multcomp_1.4-25    mnormt_2.1.1",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/07_total_score.html",
    "href": "chapters/fa/07_total_score.html",
    "title": "26  Punteggio totale e modello fattoriale",
    "section": "",
    "text": "26.1 Punteggio totale e modello fattoriale parallelo\nMcNeish e Wolf (2020) richiamano l’attenzione sul fatto che usare il punteggio totale quale misura di un costrutto è possibile solo quando i dati soddisfano i vincoli di un modello fattoriale parallelo.\nConsideriamo l’esempio seguente, nel quale McNeish e Wolf (2020) esaminano i dati “classici” di Holzinger and Swineford (1939), i quali si riferiscono ai seguenti item:\nLeggiamo i dati in R.\nd &lt;- rio::import(\n  \"../data/1_Factor_Parallel.csv\"\n)\nMcNeish e Wolf (2020) sottolineano il fatto che il punteggio totale\n\\[\n\\text{Punteggio totale} = \\text{Item 1 + Item 2 + Item 3 + Item 4 + Item 5 + Item 6}\n\\]\nrappresenta l’idea che ciasun item fornisca la stessa quantità di informazione relativamente alla misura del costrutto. Ciò può essere specificato da un modello fattoriale nel quale le saturazioni fattoriali degli item sono tutte uguali a 1. Questo corrisponde al modello parallelo che abbiamo discusso in precedenza. In tali circostanze, i punteggi fattoriali del test risultano perfettamente associati al punteggio totale (correlazione uguale a 1). Dunque, se tale modello fattoriale è giustificato dai dati, questo giustifica l’uso del punteggio totale del test quale misura del costrutto.\nÈ facile verificare tali affermazioni. Implementiamo il modello parallelo.\nm_parallel &lt;-\n  \"\n  # all loadings are fixed to one\n  f1 =~ 1*X4 + 1*X5 + 1*X6 + 1*X7 + 1*X8 + 1*X9\n  \n  # all residual variances constrained to same value\n  X4 ~~ theta*X4\n  X5 ~~ theta*X5\n  X6 ~~ theta*X6\n  X7 ~~ theta*X7\n  X8 ~~ theta*X8\n  X9 ~~ theta*X9\n\"\nAdattiamo il modello parallelo ai dati forniti dagli autori.\nfit_parallel &lt;- sem(m_parallel, data=d)\nCalcoliamo il punteggio totale.\nd$ts &lt;- with(\n  d,\n  X4 + X5 + X6 + X7 + X8 + X9\n)\nCalcoliamo i punteggi fattoriali.\nscores &lt;- lavPredict(fit_parallel, method=\"regression\")\nd$scores &lt;- as.numeric(scores)\nUn diagramma a dispersione tra il punteggio totale e i punteggi fattoriali conferma che i due sono perfettamente associati. Quindi, usare il punteggio totale o i punteggi fattoriali è equivalente.\nd |&gt; \n  ggplot(aes(x=ts, y=scores)) + \n  geom_point()\nTuttavia, questa conclusione è valida solo se il modello parallelo è giustificato per i dati. Se esaminiamo l’output di lavaan vediamo che, nel caso presente, questo non è vero.\n# report output with fit measures and standardized estimates\nout = summary(fit_parallel, fit.measures = TRUE, standardized = TRUE)\nprint(out)\n\nlavaan 0.6.17 ended normally after 13 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         7\n  Number of equality constraints                     5\n\n  Number of observations                           301\n\nModel Test User Model:\n                                                      \n  Test statistic                               325.899\n  Degrees of freedom                                19\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               568.519\n  Degrees of freedom                                15\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.446\n  Tucker-Lewis Index (TLI)                       0.562\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -2680.931\n  Loglikelihood unrestricted model (H1)      -2517.981\n                                                      \n  Akaike (AIC)                                5365.862\n  Bayesian (BIC)                              5373.276\n  Sample-size adjusted Bayesian (SABIC)       5366.933\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.232\n  90 Percent confidence interval - lower         0.210\n  90 Percent confidence interval - upper         0.254\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.206\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  f1 =~                                                                 \n    X4                1.000                               0.633    0.551\n    X5                1.000                               0.633    0.551\n    X6                1.000                               0.633    0.551\n    X7                1.000                               0.633    0.551\n    X8                1.000                               0.633    0.551\n    X9                1.000                               0.633    0.551\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .X4      (thet)    0.920    0.034   27.432    0.000    0.920    0.697\n   .X5      (thet)    0.920    0.034   27.432    0.000    0.920    0.697\n   .X6      (thet)    0.920    0.034   27.432    0.000    0.920    0.697\n   .X7      (thet)    0.920    0.034   27.432    0.000    0.920    0.697\n   .X8      (thet)    0.920    0.034   27.432    0.000    0.920    0.697\n   .X9      (thet)    0.920    0.034   27.432    0.000    0.920    0.697\n    f1                0.400    0.045    8.803    0.000    1.000    1.000\nDunque, per questi dati, il punteggio totale può ovviamente essere calcolato. Ma non fornisce una misura adeguata del costrutto. Dunque, il punteggio totale non dovrebbe essere usato nel caso dei dati ottenuti con questo test.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Punteggio totale e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/07_total_score.html#punteggio-totale-e-modello-fattoriale-parallelo",
    "href": "chapters/fa/07_total_score.html#punteggio-totale-e-modello-fattoriale-parallelo",
    "title": "26  Punteggio totale e modello fattoriale",
    "section": "",
    "text": "Paragraph comprehension\nSentence completion\nWord definitions\nSpeeded addition\nSpeeded dot counting\nDiscrimination between curved and straight letters",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Punteggio totale e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/07_total_score.html#punteggio-totale-e-modello-fattoriale-congenerico",
    "href": "chapters/fa/07_total_score.html#punteggio-totale-e-modello-fattoriale-congenerico",
    "title": "26  Punteggio totale e modello fattoriale",
    "section": "26.2 Punteggio totale e modello fattoriale congenerico",
    "text": "26.2 Punteggio totale e modello fattoriale congenerico\nGli autori adattano ai dati un modello congenerico.\n\nm_congeneric &lt;- \n'\n  #all loadings are uniquely estimated\n  f1 =~ NA*X4 + X5 + X6 + X7 + X8 + X9\n  #constrain factor variance to 1\n  f1 ~~ 1*f1\n'\n\n\n# Fit above model\nfit_congeneric &lt;- sem(m_congeneric, data=d)\n\n\nparameterEstimates(fit_congeneric, standardized = TRUE) %&gt;%\n  dplyr::filter(op == \"=~\") %&gt;%\n  dplyr::select(\n    \"Latent Factor\" = lhs,\n    Indicator = rhs,\n    B = est,\n    SE = se,\n    Z = z,\n    \"p-value\" = pvalue,\n    Beta = std.all\n  ) %&gt;%\n  knitr::kable(\n    digits = 3, booktabs = TRUE, format = \"markdown\",\n    caption = \"Factor Loadings\"\n  )\n\n\n\nTable: Factor Loadings\n\n|Latent Factor |Indicator |     B|    SE|      Z| p-value|  Beta|\n|:-------------|:---------|-----:|-----:|------:|-------:|-----:|\n|f1            |X4        | 0.963| 0.059| 16.274|   0.000| 0.824|\n|f1            |X5        | 1.121| 0.067| 16.835|   0.000| 0.846|\n|f1            |X6        | 0.894| 0.058| 15.450|   0.000| 0.792|\n|f1            |X7        | 0.195| 0.071|  2.767|   0.006| 0.170|\n|f1            |X8        | 0.185| 0.063|  2.938|   0.003| 0.180|\n|f1            |X9        | 0.278| 0.065|  4.245|   0.000| 0.258|\n\n\nSi noti che le saturazioni fattoriali sono molto diverse tra loro, suggerendo che il punteggio del costrutto si relaziona in modo diverso con ciascun item e che sarebbe inappropriato stimare il punteggio del costrutto assegnando un peso unitario agli item.\nMcNeish e Wolf (2020) calcolano poi i punteggi fattoriali del modello congenerico.\n\nscores_cong &lt;- lavPredict(fit_congeneric, method=\"regression\")\nd$scores_cong &lt;- as.numeric(scores_cong)\n\nIl grafico seguente mostra la relazione tra i punteggi fattoriali e il punteggio totale.\n\nd |&gt; \n  ggplot(aes(x=ts, y=scores_cong)) + \n  geom_point()\n\n\n\n\n\n\n\n\nNel caso presente, il coefficiente di determinazione tra punteggio totale e punteggi fattoriali è 0.77.\n\ncor(d$ts, d$scores_cong)^2\n\n0.765992021080728\n\n\nSecondo gli autori, ciò significa che due persone con un punteggio totale identico potrebbero avere punteggi di modello congenerico potenzialmente diversi perché hanno raggiunto il loro particolare punteggio totale approvando item diversi. Poiché il modello congenerico assegna pesi diversi agli item, ciascun item contribuisce in modo diverso al punteggio fattoriale del modello congenerico, il che non è vero per il punteggio totale.\nSi noti che, per i dati di Holzinger and Swineford (1939), neppure un modello congenerico ad un fattore si dimostra adeguato.\n\nout = summary(fit_congeneric, fit.measures = TRUE, standardized = TRUE)\nprint(out)\n\nlavaan 0.6.17 ended normally after 16 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        12\n\n  Number of observations                           301\n\nModel Test User Model:\n                                                      \n  Test statistic                               115.366\n  Degrees of freedom                                 9\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               568.519\n  Degrees of freedom                                15\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.808\n  Tucker-Lewis Index (TLI)                       0.680\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -2575.664\n  Loglikelihood unrestricted model (H1)      -2517.981\n                                                      \n  Akaike (AIC)                                5175.328\n  Bayesian (BIC)                              5219.813\n  Sample-size adjusted Bayesian (SABIC)       5181.756\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.198\n  90 Percent confidence interval - lower         0.167\n  90 Percent confidence interval - upper         0.231\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.129\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  f1 =~                                                                 \n    X4                0.963    0.059   16.274    0.000    0.963    0.824\n    X5                1.121    0.067   16.835    0.000    1.121    0.846\n    X6                0.894    0.058   15.450    0.000    0.894    0.792\n    X7                0.195    0.071    2.767    0.006    0.195    0.170\n    X8                0.185    0.063    2.938    0.003    0.185    0.180\n    X9                0.278    0.065    4.245    0.000    0.278    0.258\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    f1                1.000                               1.000    1.000\n   .X4                0.437    0.056    7.775    0.000    0.437    0.320\n   .X5                0.500    0.071    6.998    0.000    0.500    0.285\n   .X6                0.474    0.054    8.777    0.000    0.474    0.372\n   .X7                1.278    0.105   12.211    0.000    1.278    0.971\n   .X8                1.023    0.084   12.204    0.000    1.023    0.967\n   .X9                1.080    0.089   12.132    0.000    1.080    0.933\n\n\n\nSe trascuriamo le considerazioni sulla struttura fattoriale e esaminiamo (per esempio) unicamente il coefficiente omega, finiamo per trovare una risposta accettabile, ma sbagliata.\n\npsych::omega(d[, 1:6])\n\nCaricamento dei namespace richiesti: GPArotation\n\n\n\nOmega \nCall: omegah(m = m, nfactors = nfactors, fm = fm, key = key, flip = flip, \n    digits = digits, title = title, sl = sl, labels = labels, \n    plot = plot, n.obs = n.obs, rotate = rotate, Phi = Phi, option = option, \n    covar = covar)\nAlpha:                 0.72 \nG.6:                   0.76 \nOmega Hierarchical:    0.55 \nOmega H asymptotic:    0.65 \nOmega Total            0.84 \n\nSchmid Leiman Factor loadings greater than  0.2 \n      g  F1*  F2*   F3*   h2   u2   p2\nX4 0.73            0.68 1.00 0.00 0.53\nX5 0.96                 0.92 0.08 1.00\nX6 0.69            0.22 0.54 0.46 0.90\nX7           0.56       0.33 0.67 0.03\nX8           0.75       0.59 0.41 0.05\nX9 0.22      0.49       0.29 0.71 0.16\n\nWith Sums of squares  of:\n   g  F1*  F2*  F3* \n2.02 0.00 1.11 0.54 \n\ngeneral/max  1.82   max/min =   257.39\nmean percent general =  0.44    with sd =  0.43 and cv of  0.97 \nExplained Common Variance of the general factor =  0.55 \n\nThe degrees of freedom are 0  and the fit is  0 \nThe number of observations was  301  with Chi Square =  0.03  with prob &lt;  NA\nThe root mean square of the residuals is  0 \nThe df corrected root mean square of the residuals is  NA\n\nCompare this with the adequacy of just a general factor and no group factors\nThe degrees of freedom for just the general factor are 9  and the fit is  0.48 \nThe number of observations was  301  with Chi Square =  142.26  with prob &lt;  3.5e-26\nThe root mean square of the residuals is  0.17 \nThe df corrected root mean square of the residuals is  0.21 \n\nRMSEA index =  0.222  and the 10 % confidence intervals are  0.191 0.255\nBIC =  90.9 \n\nMeasures of factor score adequacy             \n                                                 g   F1*  F2*  F3*\nCorrelation of scores with factors            0.96  0.08 0.83 0.96\nMultiple R square of scores with factors      0.93  0.01 0.68 0.91\nMinimum correlation of factor score estimates 0.86 -0.99 0.36 0.83\n\n Total, General and Subset omega for each subset\n                                                 g  F1*  F2*  F3*\nOmega total for total scores and subscales    0.84 0.92 0.66 0.86\nOmega general for total scores and subscales  0.55 0.92 0.04 0.61\nOmega group for total scores and subscales    0.27 0.00 0.61 0.25\n\n\n\n\n\n\n\n\n\nÈ invece necessario ipotizzare un modello congenerico a due fattori.\n\nm2f_cong &lt;- '\n  # all loadings are uniquely estimated on each factor\n  f1 =~ NA*X4 + X5 + X6\n  f2 =~ NA*X7 + X8 + X9\n  \n  # constrain factor variancse to 1\n  f1 ~~ 1*f1\n  f2 ~~ 1*f2\n  \n  # estimate factor covariance\n  f1 ~~ f2\n'\n\n\n# Fit above model\nfit_2f_congeneric &lt;- sem(m2f_cong, data=d)\n\nSolo questo modello fornisce un adattamento adeguato ai dati.\n\nout = summary(fit_2f_congeneric, fit.measures = TRUE, standardized = TRUE)\nprint(out)\n\nlavaan 0.6.17 ended normally after 18 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        13\n\n  Number of observations                           301\n\nModel Test User Model:\n                                                      \n  Test statistic                                14.736\n  Degrees of freedom                                 8\n  P-value (Chi-square)                           0.064\n\nModel Test Baseline Model:\n\n  Test statistic                               568.519\n  Degrees of freedom                                15\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.988\n  Tucker-Lewis Index (TLI)                       0.977\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -2525.349\n  Loglikelihood unrestricted model (H1)      -2517.981\n                                                      \n  Akaike (AIC)                                5076.698\n  Bayesian (BIC)                              5124.891\n  Sample-size adjusted Bayesian (SABIC)       5083.662\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.053\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.095\n  P-value H_0: RMSEA &lt;= 0.050                    0.402\n  P-value H_0: RMSEA &gt;= 0.080                    0.159\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.035\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  f1 =~                                                                 \n    X4                0.965    0.059   16.296    0.000    0.965    0.826\n    X5                1.123    0.067   16.845    0.000    1.123    0.847\n    X6                0.895    0.058   15.465    0.000    0.895    0.793\n  f2 =~                                                                 \n    X7                0.659    0.080    8.218    0.000    0.659    0.575\n    X8                0.733    0.077    9.532    0.000    0.733    0.712\n    X9                0.599    0.075    8.025    0.000    0.599    0.557\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  f1 ~~                                                                 \n    f2                0.275    0.072    3.813    0.000    0.275    0.275\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    f1                1.000                               1.000    1.000\n    f2                1.000                               1.000    1.000\n   .X4                0.433    0.056    7.679    0.000    0.433    0.318\n   .X5                0.496    0.072    6.892    0.000    0.496    0.282\n   .X6                0.472    0.054    8.732    0.000    0.472    0.371\n   .X7                0.881    0.100    8.807    0.000    0.881    0.670\n   .X8                0.521    0.094    5.534    0.000    0.521    0.492\n   .X9                0.798    0.087    9.162    0.000    0.798    0.689\n\n\n\nNel contesto di questi dati, l’utilizzo di un modello congenerico non è sufficiente a giustificare l’impiego del punteggio totale, che rappresenta la somma dei punteggi degli item. Questo perché, nel caso specifico, sommando i punteggi di tutti gli item, finiremmo per includere misurazioni di due costrutti distinti.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Punteggio totale e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/07_total_score.html#session-info",
    "href": "chapters/fa/07_total_score.html#session-info",
    "title": "26  Punteggio totale e modello fattoriale",
    "section": "26.3 Session Info",
    "text": "26.3 Session Info\n\nsessionInfo()\n\nR version 4.3.2 (2023-10-31)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.3.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] modelsummary_1.4.5 ggokabeito_0.1.0   viridis_0.6.5      viridisLite_0.4.2 \n [5] ggpubr_0.6.0       ggExtra_0.10.1     bayesplot_1.11.1   gridExtra_2.3     \n [9] patchwork_1.2.0    semTools_0.5-6     semPlot_1.1.6      lavaan_0.6-17     \n[13] psych_2.4.1        scales_1.3.0       markdown_1.12      knitr_1.45        \n[17] lubridate_1.9.3    forcats_1.0.0      stringr_1.5.1      dplyr_1.1.4       \n[21] purrr_1.0.2        readr_2.1.5        tidyr_1.3.1        tibble_3.2.1      \n[25] ggplot2_3.4.4      tidyverse_2.0.0    here_1.0.1        \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.15.0  jsonlite_1.8.8     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5   nloptr_2.0.3      \n  [7] rmarkdown_2.25     vctrs_0.6.5        minqa_1.2.6       \n [10] base64enc_0.1-3    rstatix_0.7.2      htmltools_0.5.7   \n [13] broom_1.0.5        Formula_1.2-5      htmlwidgets_1.6.4 \n [16] plyr_1.8.9         sandwich_3.1-0     emmeans_1.10.0    \n [19] zoo_1.8-12         uuid_1.2-0         igraph_2.0.2      \n [22] mime_0.12          lifecycle_1.0.4    pkgconfig_2.0.3   \n [25] Matrix_1.6-5       R6_2.5.1           fastmap_1.1.1     \n [28] shiny_1.8.0        digest_0.6.34      OpenMx_2.21.11    \n [31] fdrtool_1.2.17     colorspace_2.1-0   rprojroot_2.0.4   \n [34] Hmisc_5.1-1        fansi_1.0.6        timechange_0.3.0  \n [37] abind_1.4-5        compiler_4.3.2     withr_3.0.0       \n [40] glasso_1.11        htmlTable_2.4.2    backports_1.4.1   \n [43] carData_3.0-5      ggsignif_0.6.4     MASS_7.3-60.0.1   \n [46] corpcor_1.6.10     gtools_3.9.5       tools_4.3.2       \n [49] pbivnorm_0.6.0     foreign_0.8-86     zip_2.3.1         \n [52] httpuv_1.6.14      nnet_7.3-19        glue_1.7.0        \n [55] quadprog_1.5-8     nlme_3.1-164       promises_1.2.1    \n [58] lisrelToR_0.3      grid_4.3.2         pbdZMQ_0.3-11     \n [61] checkmate_2.3.1    cluster_2.1.6      reshape2_1.4.4    \n [64] generics_0.1.3     gtable_0.3.4       tzdb_0.4.0        \n [67] data.table_1.15.0  hms_1.1.3          car_3.1-2         \n [70] utf8_1.2.4         tables_0.9.17      sem_3.1-15        \n [73] pillar_1.9.0       IRdisplay_1.1      rockchalk_1.8.157 \n [76] later_1.3.2        splines_4.3.2      lattice_0.22-5    \n [79] survival_3.5-8     kutils_1.73        tidyselect_1.2.0  \n [82] miniUI_0.1.1.1     pbapply_1.7-2      stats4_4.3.2      \n [85] xfun_0.42          qgraph_1.9.8       arm_1.13-1        \n [88] stringi_1.8.3      boot_1.3-29        evaluate_0.23     \n [91] codetools_0.2-19   mi_1.1             cli_3.6.2         \n [94] RcppParallel_5.1.7 IRkernel_1.3.2     rpart_4.1.23      \n [97] xtable_1.8-4       repr_1.1.6         munsell_0.5.0     \n[100] Rcpp_1.0.12        coda_0.19-4.1      png_0.1-8         \n[103] XML_3.99-0.16.1    parallel_4.3.2     ellipsis_0.3.2    \n[106] jpeg_0.1-10        lme4_1.1-35.1      mvtnorm_1.2-4     \n[109] insight_0.19.8     openxlsx_4.2.5.2   crayon_1.5.2      \n[112] rlang_1.1.3        multcomp_1.4-25    mnormt_2.1.1",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Punteggio totale e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/01_val_matrici.html",
    "href": "chapters/extraction/01_val_matrici.html",
    "title": "27  Valutazione della matrice di correlazione",
    "section": "",
    "text": "27.1 Introduzione\nPrima di eseguire un’analisi fattoriale, è importante esaminare la matrice di correlazione tra le variabili. Se il determinante della matrice è nullo, l’analisi fattoriale non può essere eseguita. In caso contrario, è necessario valutare se le correlazioni campionarie sono sufficientemente grandi da giustificare l’analisi fattoriale. Se le correlazioni tra gli item sono modeste, la soluzione fornita dall’analisi fattoriale potrebbe non essere parsimoniosa. Per valutare questo, si può ispezionare visivamente la matrice di correlazione e utilizzare due test: il test della sfericità di Bartlett e il test Kaiser-Meyer-Olkin.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Valutazione della matrice di correlazione</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/01_val_matrici.html#ispezione-visiva-della-matrice-di-correlazione",
    "href": "chapters/extraction/01_val_matrici.html#ispezione-visiva-della-matrice-di-correlazione",
    "title": "27  Valutazione della matrice di correlazione",
    "section": "27.2 Ispezione visiva della matrice di correlazione",
    "text": "27.2 Ispezione visiva della matrice di correlazione\nL’ispezione visiva della matrice di correlazione viene effettuata per verificare la presenza di blocchi di correlazioni alte tra loro e basse con le altre variabili. Ciò suggerisce la presenza di più fattori comuni.\nPer fare un esempio, consideriamo il dataset HolzingerSwineford1939. Tale dataset contiene 301 osservazioni di punteggi di abilità mentale. Consideriao qui le variabili x1 – x9.\n\ndata(HolzingerSwineford1939)\nglimpse(HolzingerSwineford1939)\n\nRows: 301\nColumns: 15\n$ id     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, ~\n$ sex    &lt;int&gt; 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, ~\n$ ageyr  &lt;int&gt; 13, 13, 13, 13, 12, 14, 12, 12, 13, 12, 12, 12, 12, 12, 12, 12,~\n$ agemo  &lt;int&gt; 1, 7, 1, 2, 2, 1, 1, 2, 0, 5, 2, 11, 7, 8, 6, 1, 11, 5, 8, 3, 1~\n$ school &lt;fct&gt; Pasteur, Pasteur, Pasteur, Pasteur, Pasteur, Pasteur, Pasteur, ~\n$ grade  &lt;int&gt; 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ~\n$ x1     &lt;dbl&gt; 3.333333, 5.333333, 4.500000, 5.333333, 4.833333, 5.333333, 2.8~\n$ x2     &lt;dbl&gt; 7.75, 5.25, 5.25, 7.75, 4.75, 5.00, 6.00, 6.25, 5.75, 5.25, 5.7~\n$ x3     &lt;dbl&gt; 0.375, 2.125, 1.875, 3.000, 0.875, 2.250, 1.000, 1.875, 1.500, ~\n$ x4     &lt;dbl&gt; 2.333333, 1.666667, 1.000000, 2.666667, 2.666667, 1.000000, 3.3~\n$ x5     &lt;dbl&gt; 5.75, 3.00, 1.75, 4.50, 4.00, 3.00, 6.00, 4.25, 5.75, 5.00, 3.5~\n$ x6     &lt;dbl&gt; 1.2857143, 1.2857143, 0.4285714, 2.4285714, 2.5714286, 0.857142~\n$ x7     &lt;dbl&gt; 3.391304, 3.782609, 3.260870, 3.000000, 3.695652, 4.347826, 4.6~\n$ x8     &lt;dbl&gt; 5.75, 6.25, 3.90, 5.30, 6.30, 6.65, 6.20, 5.15, 4.65, 4.55, 5.7~\n$ x9     &lt;dbl&gt; 6.361111, 7.916667, 4.416667, 4.861111, 5.916667, 7.500000, 4.8~\n\n\n\nhz &lt;- HolzingerSwineford1939 |&gt;\n  select(x1:x9)\n\nhz |&gt;\n  slice(1:5) \n\n\nA data.frame: 5 x 9\n\n\nx1\nx2\nx3\nx4\nx5\nx6\nx7\nx8\nx9\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n3.333333\n7.75\n0.375\n2.333333\n5.75\n1.2857143\n3.391304\n5.75\n6.361111\n\n\n5.333333\n5.25\n2.125\n1.666667\n3.00\n1.2857143\n3.782609\n6.25\n7.916667\n\n\n4.500000\n5.25\n1.875\n1.000000\n1.75\n0.4285714\n3.260870\n3.90\n4.416667\n\n\n5.333333\n7.75\n3.000\n2.666667\n4.50\n2.4285714\n3.000000\n5.30\n4.861111\n\n\n4.833333\n4.75\n0.875\n2.666667\n4.00\n2.5714286\n3.695652\n6.30\n5.916667\n\n\n\n\n\nValutiamo la presenza di dati mancanti.\n\nmissings &lt;- colSums(is.na(hz)) # Count # missing in each column\nsummary(missings) \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0       0       0       0 \n\n\nIn questo set di dati non ci sono dati mancanti.\nEsaminiamo la distribuzione delle variabili.\n\ndescribe(hz)\n\n\nA psych: 9 x 13\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nx1\n1\n301\n4.935770\n1.167432\n5.000000\n4.964730\n1.235500\n0.6666667\n8.500000\n7.833333\n-0.2543455\n0.30753382\n0.06728967\n\n\nx2\n2\n301\n6.088040\n1.177451\n6.000000\n6.017635\n1.111950\n2.2500000\n9.250000\n7.000000\n0.4700766\n0.33239397\n0.06786712\n\n\nx3\n3\n301\n2.250415\n1.130979\n2.125000\n2.199170\n1.297275\n0.2500000\n4.500000\n4.250000\n0.3834294\n-0.90752645\n0.06518857\n\n\nx4\n4\n301\n3.060908\n1.164116\n3.000000\n3.024896\n0.988400\n0.0000000\n6.333333\n6.333333\n0.2674867\n0.08012676\n0.06709855\n\n\nx5\n5\n301\n4.340532\n1.290472\n4.500000\n4.395228\n1.482600\n1.0000000\n7.000000\n6.000000\n-0.3497961\n-0.55253689\n0.07438158\n\n\nx6\n6\n301\n2.185572\n1.095603\n2.000000\n2.088322\n1.059000\n0.1428571\n6.142857\n6.000000\n0.8579486\n0.81655717\n0.06314952\n\n\nx7\n7\n301\n4.185902\n1.089534\n4.086957\n4.163630\n1.095835\n1.3043478\n7.434783\n6.130435\n0.2490881\n-0.30740386\n0.06279967\n\n\nx8\n8\n301\n5.527076\n1.012615\n5.500000\n5.492946\n0.963690\n3.0500000\n10.000000\n6.950000\n0.5252580\n1.17155564\n0.05836617\n\n\nx9\n9\n301\n5.374123\n1.009152\n5.416667\n5.366067\n0.988400\n2.7777778\n9.250000\n6.472222\n0.2038709\n0.28990791\n0.05816654\n\n\n\n\n\nI valorei di asimmetria e kurosi sono adeguati.\nConsideriamo ora le correlazioni tra le variabili usando le funzioni del pacchetto corrr:\n\ncorrr::rearrange raggruppa le variabili altamente correlate\ncorrr::rplot visualizza il risultato.\n\n\ncor_tb &lt;- correlate(hz)\n\ncor_tb |&gt;\n  rearrange() |&gt;\n  rplot(colors = c(\"red\", \"white\", \"blue\"))\n\nCorrelation computed with\n* Method: 'pearson'\n* Missing treated using: 'pairwise.complete.obs'\n\n\n\n\n\n\n\n\n\nIl grafico suggerisce la presenza di tre gruppi di variabili:\n\nda x4 a x6 (primo gruppo)\nda x1 a x3 (secondo gruppo)\nda x7 a x9 (terzo gruppo).",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Valutazione della matrice di correlazione</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/01_val_matrici.html#sfericità-di-bartlett",
    "href": "chapters/extraction/01_val_matrici.html#sfericità-di-bartlett",
    "title": "27  Valutazione della matrice di correlazione",
    "section": "27.3 Sfericità di Bartlett",
    "text": "27.3 Sfericità di Bartlett\nIl test di sfericità di Bartlett verifica l’ipotesi che il campione provenga da una popolazione in cui le variabili non sono correlate. Formalmente, il test della sfericità di Bartlett verifica l’ipotesi \\(H_0 : \\boldsymbol{R} = \\boldsymbol{I}\\) tramite la formula:\n\\[\n\\chi^2 = -\\bigg[n -1 -\\frac{1}{6} (2p +5)\\bigg] \\ln |\\boldsymbol{R}|,\n\\]\nin cui \\(n\\) è il numero dei soggetti, \\(p\\) il numero delle variabili e \\(|\\boldsymbol{R}|\\) il determinante della matrice di correlazione.\nLa statistica del test di sfericità di Bartlett segue una distribuzione chi-quadro con \\(p(p - 1)/2\\) gradi di libertà. Un valore elevato della statistica indica che la matrice di correlazione R contiene valori di correlazione significativamente diversi da 0. Al contrario, un valore basso della statistica indica che le correlazioni sono basse e non si distinguono da 0.\nIl limite di questo test è che dipende dal numero delle variabili e dalla numerosità del campione, quindi tende a rigettare \\(H_0\\) all’aumentare del campione e del numero delle variabili, anche se le correlazioni sono piccole.\nApplichiamo il test di Bartlet per il dati dell’esempio in discussione.\n\ncor_mat &lt;- cor(hz)\n\nout = cortest.bartlett(R = cor_mat, n = 301)\nprint(out)\n\n$chisq\n[1] 904.0971\n\n$p.value\n[1] 1.912079e-166\n\n$df\n[1] 36\n\n\n\nIl risultato del test di Bartlett sui dati HolzingerSwineford1939 indica che esiste una correlazione tra le variabili.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Valutazione della matrice di correlazione</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/01_val_matrici.html#test-di-adeguatezza-campionaria-di-kaiser-meyer-olkin",
    "href": "chapters/extraction/01_val_matrici.html#test-di-adeguatezza-campionaria-di-kaiser-meyer-olkin",
    "title": "27  Valutazione della matrice di correlazione",
    "section": "27.4 Test di adeguatezza campionaria di Kaiser-Meyer-Olkin",
    "text": "27.4 Test di adeguatezza campionaria di Kaiser-Meyer-Olkin\nIl test di Kaiser-Meyer-Olkin (KMO) è uno strumento statistico che valuta l’adeguatezza dei dati per l’analisi fattoriale. Esso misura la proporzione di varianza tra le variabili che potrebbe essere attribuita a fattori comuni. Un valore KMO più alto indica una maggiore adattabilità dei dati all’analisi fattoriale.\nLa statistica di adeguatezza campionaria KMO è data da\n\\[\\text{KMO} = \\frac{\\sum_i\\sum_j r^2_{ij}}{\\sum_i\\sum_j r^2_{ij} +\\sum_i\\sum_jp^2_{ij}},\\]\ndove \\(r_{ij}\\) sono le correlazioni osservate e \\(p_{ij}\\) sono le correlazioni parzializzate su tutte le altre. Se le correlazioni parzializzate sono piccole, KMO tende a 1.\nSecondo Kaiser (1970), l’adeguatezza campionaria si valuta nel modo seguente:\n\nda 0.00 a 0.49: inaccettabile\nda 0.50 a 0.59: miserabile\nda 0.60 a 0.69: mediocre\nda 0.70 a 0.79: media\nda 0.80 a 0.89: meritevole\nda 0.90 a 1.00: meravigliosa.\n\nApplichiamo il test KMO ai dati HolzingerSwineford1939.\n\nout = KMO(cor_mat)\nprint(out)\n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = cor_mat)\nOverall MSA =  0.75\nMSA for each item = \n  x1   x2   x3   x4   x5   x6   x7   x8   x9 \n0.81 0.78 0.73 0.76 0.74 0.81 0.59 0.68 0.79 \n\n\nPer questi dati, il risultato del test KMO indica che l’adeguatezza campionaria è media.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Valutazione della matrice di correlazione</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html",
    "href": "chapters/extraction/02_estrazione.html",
    "title": "28  L’estrazione dei fattori",
    "section": "",
    "text": "28.1 Introduzione\nL’analisi fattoriale mira a descrivere in modo parsimonioso le relazioni tra un grande numero di item. L’obiettivo è identificare un piccolo numero di variabili latenti che, quando controllate, rendono uguali a zero le correlazioni parziali tra gli item. Una volta determinato il numero dei fattori comuni, è possibile stimare le saturazioni fattoriali, che corrispondono alle correlazioni o covarianze tra gli item e i fattori.\nIn termini matriciali, il modello multifattoriale si scrive\n\\[\n\\boldsymbol{\\Sigma} =\\boldsymbol{\\Lambda} \\boldsymbol{\\Phi} \\boldsymbol{\\Lambda}^{\\mathsf{T}} + \\boldsymbol{\\Psi}\n\\]\ndove \\(\\boldsymbol{\\Phi}\\) è la matrice di ordine \\(m \\times m\\) di varianze e covarianze tra i fattori comuni e \\(\\boldsymbol{\\Psi}\\) è una matrice diagonale di ordine \\(p\\) con le unicità delle variabili.\nIn questo capitolo descriveremo alcuni dei metodi che possono essere usati per stimare \\(\\boldsymbol{\\Lambda}\\). Esamineremo il metodo delle componenti principali, il metodo dei fattori principali, il metodo dei fattori principali iterato e il metodo di massima verosimiglianza.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html#metodo-delle-componenti-principali",
    "href": "chapters/extraction/02_estrazione.html#metodo-delle-componenti-principali",
    "title": "28  L’estrazione dei fattori",
    "section": "28.2 Metodo delle componenti principali",
    "text": "28.2 Metodo delle componenti principali\nL’analisi fattoriale eseguita mediante il metodo delle componenti principali, nonostante il nome, non è un’analisi delle componenti principali. Il metodo delle componenti principali costituisce invece un’applicazione del teorema di scomposizione spettrale di una matrice. Il teorema spettrale afferma che, data la matrice simmetrica \\(\\textbf{S}_{p \\times p}\\), è sempre possibile trovare una matrice \\(\\textbf{C}_{p \\times p}\\) ortogonale tale che $ = ^{} $ con D diagonale. Il teorema specifica inoltre che gli elementi presenti sulla diagonale di D sono gli autovalori di S, mentre le colonne di C rappresentano i rispettivi autovettori normalizzati associati agli autovalori di S.\nFacciamo un esempio numerico utilizzando i dati discussi da Rencher(2002). Brown, Williams e Barlow (1984) hanno raccolto le valutazioni di una ragazza dodicenne relativamente a sette persone di sua conoscenza. Ciascuna persona veniva valutata su una scala a nove punti rispetto a cinque variabili: kind, intelligent, happy, likeable e just. La matrice di correlazione per tali variabili è riportata di seguito:\n\nR &lt;- matrix(c(\n  1.000, .296, .881, .995, .545,\n  .296, 1.000, -.022, .326, .837,\n  .881, -.022, 1.000, .867, .130,\n  .995, .326, .867, 1.000, .544,\n  .545, .837, .130, .544, 1.000\n),\nncol = 5, byrow = T, dimnames = list(\n  c(\"K\", \"I\", \"H\", \"L\", \"J\"),\n  c(\"K\", \"I\", \"H\", \"L\", \"J\")\n)\n)\nR\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nK\nI\nH\nL\nJ\n\n\n\n\nK\n1.000\n0.296\n0.881\n0.995\n0.545\n\n\nI\n0.296\n1.000\n-0.022\n0.326\n0.837\n\n\nH\n0.881\n-0.022\n1.000\n0.867\n0.130\n\n\nL\n0.995\n0.326\n0.867\n1.000\n0.544\n\n\nJ\n0.545\n0.837\n0.130\n0.544\n1.000\n\n\n\n\n\nGli autovalori e gli autovettori si calcolano con la funzione eigen():\n\ne &lt;- eigen(R)\nprint(e)\n\neigen() decomposition\n$values\n[1] 3.2633766259 1.5383820947 0.1679692814 0.0300298228 0.0002421752\n\n$vectors\n           [,1]       [,2]        [,3]       [,4]       [,5]\n[1,] -0.5367301 -0.1859819 -0.18991841 -0.1247931  0.7910052\n[2,] -0.2874672  0.6505666  0.68488713 -0.1198141  0.1034406\n[3,] -0.4343545 -0.4736877  0.40694897  0.6136634 -0.2115794\n[4,] -0.5373909 -0.1692797 -0.09532905 -0.6293896 -0.5266275\n[5,] -0.3896544  0.5377158 -0.56583170  0.4442491 -0.2037363\n\n\n\nCome indicato in precedenza, la matrice R può essere espressa come\n\\[\n\\textbf{R} = \\textbf{C}\\textbf{D}\\textbf{C}^{\\mathsf{T}}\n\\]\n\ne$vectors %*% diag(e$values) %*% t(e$vectors)\n\n\nA matrix: 5 x 5 of type dbl\n\n\n1.000\n0.296\n0.881\n0.995\n0.545\n\n\n0.296\n1.000\n-0.022\n0.326\n0.837\n\n\n0.881\n-0.022\n1.000\n0.867\n0.130\n\n\n0.995\n0.326\n0.867\n1.000\n0.544\n\n\n0.545\n0.837\n0.130\n0.544\n1.000\n\n\n\n\n\nEsaminiamo ora gli autovalori. I primi due autovalori spiegano da soli il 96% della varianza campionaria:\n\n(e$values[1] + e$values[2]) / 5\n\n0.96035174411791\n\n\nUsando i primi due autovalori e i primi due autovettori è dunque possibile riprodurre in maniera soddisfacente la matrice R operando nel contempo una riduzione di dimensionalità dei dati.\nPer fattorizzare \\(\\textbf{R} = \\textbf{C}\\textbf{D}\\textbf{C}^{\\mathsf{T}}\\) nella forma \\(\\hat{\\boldsymbol{\\Lambda}} \\hat{\\boldsymbol{\\Lambda}}^{\\mathsf{T}}\\) iniziamo a scrivere\n\\[\n\\textbf{D}= \\textbf{D}^{1/2} \\textbf{D}^{1/2}\n\\]\ndove\n\\[\n\\textbf{D}^{1/2} =\n\\left[\n  \\begin{array}{ c c c c }\n     \\sqrt{\\theta_1} & 0 & \\dots & 0 \\\\\n     0 & \\sqrt{\\theta_2} & \\dots & 0 \\\\\n     \\dots & \\dots & & \\dots \\\\\n     0 & 0 & \\dots &  \\sqrt{\\theta_p}\n  \\end{array}\n\\right]\n\\]\nViene qui usata la notazione \\(\\theta\\) per denotare gli autovalori anziché il tradizionale \\(\\lambda\\) per evitare la confusione con la notazione \\(\\lambda_{jl}\\) usata per le saturazioni fattoriali. In questo modo, possiamo scrivere\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\textbf{R} &= \\textbf{C}\\textbf{D}\\textbf{C}^{\\mathsf{T}}\\notag\\\\\n&= \\textbf{C}\\textbf{D}^{1/2}\\textbf{D}^{1/2}\\textbf{C}^{\\mathsf{T}}\\notag\\\\\n&= (\\textbf{C}\\textbf{D}^{1/2}) (\\textbf{C}\\textbf{D}^{1/2})^{\\mathsf{T}}.\n\\end{aligned}\n\\end{equation}\n\\]\nNon possiamo però limiarci a definire \\(\\hat{\\boldsymbol{\\Lambda}}=\\textbf{C}\\textbf{D}^{1/2}\\) in quanto \\(\\textbf{C}\\textbf{D}^{1/2}\\) è di ordine \\(p \\times p\\) e non otteniamo quindi una riduzione di dimensioni. Quello che cerchiamo è una matrice \\(\\hat{\\boldsymbol{\\Lambda}}\\) di ordine \\(p \\times m\\) con \\(m &lt; p\\). Dunque, definiamo la matrice \\(\\textbf{D}_1= \\text{diag}(\\theta_1,\n\\theta_2, \\dots, \\theta_m)\\) come la la matrice contenente gli \\(m\\) autovalori più grandi di R e \\(\\textbf{C}_1=( \\textbf{c}_1,\n\\textbf{c}_2, \\dots,  \\textbf{c}_m)\\) come la matrice contenente i rispettivi autovettori.\nMediante il metodo delle componenti principali, le saturazioni fattoriali \\(\\hat{\\boldsymbol{\\Lambda}}\\) vengono quindi stimate nel modo seguente:\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\boldsymbol{\\Lambda}} &= \\textbf{C}_1 \\textbf{D}_1^{1/2}\\notag\\\\\n&= (\\sqrt{\\theta_1} \\textbf{c}_1, \\sqrt{\\theta_2} \\textbf{c}_2,\n\\dots, \\sqrt{\\theta_m} \\textbf{c}_m)\n\\end{aligned}\n\\end{equation}\n\\]\nPer l’esempio presente, con \\(m=2\\) e \\(p=5\\), avremo\n\\[\n\\left[\n  \\begin{array}{ c c }\n\\hat{\\lambda}_{11} & \\hat{\\lambda}_{12} \\\\\n\\hat{\\lambda}_{21} & \\hat{\\lambda}_{22} \\\\\n\\hat{\\lambda}_{31} & \\hat{\\lambda}_{32} \\\\\n\\hat{\\lambda}_{41} & \\hat{\\lambda}_{42} \\\\\n\\hat{\\lambda}_{51} & \\hat{\\lambda}_{52}\n  \\end{array}\n\\right] =\n\\left[\n  \\begin{array}{ c c }\nc_{11} & c_{12} \\\\\nc_{21} & c_{22} \\\\\nc_{31} & c_{32} \\\\\nc_{41} & c_{42} \\\\\nc_{51} & c_{52}\n  \\end{array}\n\\right]\n\\left[\n  \\begin{array}{ c c }\n\\sqrt{\\theta_1} & 0\\\\\n0 &\\sqrt{\\theta_2}\n  \\end{array}\n\\right]\n\\]\nLe saturazioni fattoriali stimate sono dunque uguali a\n\\[\n\\left[\n  \\begin{array}{ c c }\n\\sqrt{\\theta_1}c_{11} & \\sqrt{\\theta_2}c_{12} \\\\\n\\sqrt{\\theta_1}c_{21} & \\sqrt{\\theta_2}c_{22} \\\\\n\\sqrt{\\theta_1}c_{31} & \\sqrt{\\theta_2}c_{32} \\\\\n\\sqrt{\\theta_1}c_{41} & \\sqrt{\\theta_2}c_{42} \\\\\n\\sqrt{\\theta_1}c_{51} & \\sqrt{\\theta_2}c_{52}\n  \\end{array}\n\\right]\n\\]\nSvolgendo i calcoli con \\(\\textsf{R}\\) otteniamo:\n\nL &lt;- cbind(\n  e$vectors[, 1] * sqrt(e$values[1]),\n  e$vectors[, 2] * sqrt(e$values[2])\n)\n\nround(L, 3)\n\n\nA matrix: 5 x 2 of type dbl\n\n\n-0.970\n-0.231\n\n\n-0.519\n0.807\n\n\n-0.785\n-0.588\n\n\n-0.971\n-0.210\n\n\n-0.704\n0.667\n\n\n\n\n\nLa matrice di correlazione riprodotta (con le comunalità sulla diagonale principale) diventa\n\nR_hat &lt;- round(L %*% t(L), 3)\nR_hat\n\n\nA matrix: 5 x 5 of type dbl\n\n\n0.993\n0.317\n0.896\n0.990\n0.529\n\n\n0.317\n0.921\n-0.067\n0.335\n0.904\n\n\n0.896\n-0.067\n0.961\n0.885\n0.160\n\n\n0.990\n0.335\n0.885\n0.987\n0.543\n\n\n0.529\n0.904\n0.160\n0.543\n0.940\n\n\n\n\n\nLa matrice di correlazioni residue (con le specificità sulla diagonale principale) è la seguente.\n\nR - R_hat\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nK\nI\nH\nL\nJ\n\n\n\n\nK\n0.007\n-0.021\n-0.015\n0.005\n0.016\n\n\nI\n-0.021\n0.079\n0.045\n-0.009\n-0.067\n\n\nH\n-0.015\n0.045\n0.039\n-0.018\n-0.030\n\n\nL\n0.005\n-0.009\n-0.018\n0.013\n0.001\n\n\nJ\n0.016\n-0.067\n-0.030\n0.001\n0.060\n\n\n\n\n\nPossiamo ora capire il motivo del nome metodo delle componenti principali: le saturazioni fattoriali sono proporzionali agli autovettori di \\(\\textbf{R}\\). Tuttavia, l’interpretazione è diversa da quella che viene assegnata ai risultati dell’analisi delle componenti principali.\nÈ possibile condurre l’analisi fattoriale con il metodo delle componenti principali sia utilizzando la matrice \\(\\textbf{S}\\) di varianze-covarianze sia la matrice \\(\\textbf{R}\\) delle correlazioni. Tuttavia, le soluzioni ottenute usando \\(\\textbf{S}\\) o \\(\\textbf{R}\\) non sono legate da una relazione algebrica: il metodo delle componenti principali non è invariante rispetto ai cambiamenti di scala delle osservazioni. Un altro svantaggio del metodo delle componenti principali è che non fornisce un test di bontà di adattamento. Tale test può essere invece svolto quando la soluzione viene trovata con il metodo della massima verosimiglianza.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html#metodo-dei-fattori-principali",
    "href": "chapters/extraction/02_estrazione.html#metodo-dei-fattori-principali",
    "title": "28  L’estrazione dei fattori",
    "section": "28.3 Metodo dei fattori principali",
    "text": "28.3 Metodo dei fattori principali\nIl metodo dei fattori principali (principal factor method, anche detto principal axis method) è uno dei metodi maggiormente usati per la stima delle saturazioni fattoriali e delle comunalità. Il metodo delle componenti principali che abbiamo usato in precedenza trascura la specificità \\(\\boldsymbol{\\Psi}\\) e si limita a fattorializzare le covarianze di S o le correlazioni di R. Il metodo dei fattori principali affronta questo problema ricorrendo ad una procedura simile al metodo delle componenti principali, nella quale però viene utilizzata una matrice ridotta di varianze-covarianze \\(\\textbf{S} - \\hat{\\boldsymbol{\\Psi}}\\) in cui una stima delle comunalità viene sostituita alle varianze presenti sulla diagonale principale della matrice S o R.\nNel caso della matrice ridotta di correlazioni \\(\\textbf{R} - \\hat{\\boldsymbol{\\Psi}}\\), per la comunalità \\(i\\)-esima \\(\\sum_{j}\\lambda_{ij}^2\\) si sceglie il quadrato del coefficiente di correlazione multipla tra \\(Y_i\\) e tutte le altre \\(p-1\\) variabili. Tale valore si può trovare nel modo seguente\n\\[\n\\hat{h}^2_i=R^2_i=1-\\frac{1}{r^{ii}},\n\\]\ndove \\(r^{ii}\\) è l’elemento diagonale \\(i\\)-esimo di \\(\\textbf{R}^{-1}\\). Nel caso di una matrice ridotta di varianze-covarianze \\(\\textbf{S} - \\hat{\\boldsymbol{\\Psi}}\\), le comunalità possono essere stimate calcolando\n\\[\n\\hat{h}_i^2=s_{ii}-\\frac{1}{r^{ii}},\n\\]\ndove \\(s_{ii}\\) è l’elemento diagonale \\(i\\)-esimo di \\(\\textbf{S}\\).\nAffinché le stime comunalità possano essere calcolate come descritto sopra, la matrice \\(\\textbf{R}\\) non deve essere singolare. Se \\(\\textbf{R}\\) è singolare, per la stima della comunalità \\(i\\)-esima, \\(\\hat{h}^2_i\\), si utilizza il valore assoluto del più elevato coefficiente di correlazione lineare tra \\(Y_i\\) e le altre variabili.\nScelta la stima della comunalità, la matrice ridotta di varianze-covarianze si ottiene sostituendo le stime delle comunalità alle varianze sulla diagonale principale della matrice \\(\\textbf{S}\\):\n\\[\n\\textbf{S} - \\hat{\\boldsymbol{\\Psi}} =\n\\left[\n  \\begin{array}{ c c c c }\n    \\hat{h}^2_1 & s_{12} & \\dots & s_{1p} \\\\\n    s_{21} & \\hat{h}^2_2 & \\dots & s_{2p} \\\\\n    \\dots & \\dots &           & \\dots\\\\\n    s_{p1} &  s_{p2} & \\dots & \\hat{h}^2_p\n  \\end{array}\n\\right]\n\\]\nIn maniera equivalente, la matrice ridotta di correlazioni si ottiene nel modo seguente:\n\\[\n\\textbf{R} - \\hat{\\boldsymbol{\\Psi}} =\n\\left[\n  \\begin{array}{ c c c c }\n    \\hat{h}^2_1 & r_{12} & \\dots & r_{1p} \\\\\n    r_{21} & \\hat{h}^2_2 & \\dots & r_{2p} \\\\\n    \\dots & \\dots &           & \\dots\\\\\n    r_{p1} &  r_{p2} & \\dots & \\hat{h}^2_p\n  \\end{array}\n\\right]\n\\]\nFacciamo ora un esempio concreto usando la matrice di correlazione dell’esempio precedente. Quale stima della comunalità \\(i\\)-esima, useremo il valore assoluto più elevato nella riga \\(i\\)-esima della matrice R. Per i dati dell’esempio, le stime delle comunalità sono dunque pari a \\(0.995\\), \\(0.837\\), \\(0.881\\), \\(0.995\\) e \\(0.837\\).\nInserendo tali valori nella diagonale principale, otteniamo la matrice ridotta delle correlazioni \\(\\textbf{R} - \\hat{\\boldsymbol{\\Psi}}\\):\n\nR1 &lt;- R\nh.hat &lt;- c(.995, .837, .881, .995, .837)\nR1[cbind(1:5,1:5)] &lt;- h.hat\nR1\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nK\nI\nH\nL\nJ\n\n\n\n\nK\n0.995\n0.296\n0.881\n0.995\n0.545\n\n\nI\n0.296\n0.837\n-0.022\n0.326\n0.837\n\n\nH\n0.881\n-0.022\n0.881\n0.867\n0.130\n\n\nL\n0.995\n0.326\n0.867\n0.995\n0.544\n\n\nJ\n0.545\n0.837\n0.130\n0.544\n0.837\n\n\n\n\n\nGli autovalori della matrice ridotta di correlazioni \\(\\textbf{R} - \\hat{\\boldsymbol{\\Psi}}\\) sono:\n\nee &lt;- eigen(R1)\nprint(round(ee$values, 3))\n\n[1]  3.202  1.394  0.029  0.000 -0.080\n\n\nLa somma degli autovalori è uguale a\n\nprint(sum(ee$values))\n\n[1] 4.545\n\n\nI primi due autovalori di \\(\\textbf{R} - \\hat{\\boldsymbol{\\Psi}}\\) sono:\n\nprint(round(ee$vectors[, 1:2], 3))\n\n      [,1]   [,2]\n[1,] 0.548 -0.177\n[2,] 0.272  0.656\n[3,] 0.431 -0.461\n[4,] 0.549 -0.159\n[5,] 0.373  0.549\n\n\nMoltiplicando tali valori per la radice quadrata dei rispettivi autovalori si ottengono le stime delle saturazioni fattoriali:\n\nround(ee$vectors[,1:2] %*% sqrt(diag(ee$values[1:2])), 3)\n\n\nA matrix: 5 x 2 of type dbl\n\n\n0.981\n-0.209\n\n\n0.487\n0.774\n\n\n0.772\n-0.544\n\n\n0.982\n-0.187\n\n\n0.667\n0.648\n\n\n\n\n\nTale risultato replica quello riportato da Rencher (2002).",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html#metodo-dei-fattori-principali-iterato",
    "href": "chapters/extraction/02_estrazione.html#metodo-dei-fattori-principali-iterato",
    "title": "28  L’estrazione dei fattori",
    "section": "28.4 Metodo dei fattori principali iterato",
    "text": "28.4 Metodo dei fattori principali iterato\nSolitamente, per migliorare la stima della comunalità, la diagonale della matrice \\(\\textbf{S} - \\hat{\\boldsymbol{\\Psi}}\\) o \\(\\textbf{R} - \\hat{\\boldsymbol{\\Psi}}\\) viene ottenuta per iterazione. Dopo avere trovato \\(\\hat{\\boldsymbol{\\Lambda}}\\) a partire da \\(\\textbf{S} - \\hat{\\boldsymbol{\\Psi}}\\) o \\(\\textbf{R} - \\hat{\\boldsymbol{\\Psi}}\\) come indicato in precedenza, utilizzando le stime delle saturazioni fattoriali così ottenute possiamo stimare le comunalità nel modo seguente:\n\\[\\hat{h}^2_i = \\sum_{i=1}^m \\hat{\\lambda}_{ij}^2.\\]\nI valori di \\(\\hat{h}^2_i\\) così trovati vengono quindi sostituiti nella diagonale della matrice ridotta \\(\\textbf{S} - \\hat{\\boldsymbol{\\Psi}}\\) o \\(\\textbf{R} - \\hat{\\boldsymbol{\\Psi}}\\). A partire da questa nuova matrice, usando il metodo descritto in precedenza, possiamo ottenere una nuova stima delle saturazioni fattoriali \\(\\hat{\\boldsymbol{\\Lambda}}\\). Mediante questa nuova stima di \\(\\hat{\\boldsymbol{\\Lambda}}\\), possiamo procedere ad una nuova stima delle comunalità. Tale processo continua iterativamente sino alla convergenza. Gli autovalori e gli autovettori della versione finale di \\(\\textbf{S} - \\hat{\\boldsymbol{\\Psi}}\\) o \\(\\textbf{R} - \\hat{\\boldsymbol{\\Psi}}\\) vengono infine usati per stimare i pesi fattoriali. Il metodo dei fattori principali iterato e il metodo delle componenti principali producono risultati molto simili quando \\(m\\) assume un piccolo valore (questo si verifica quando le correlazioni sono alte) e quando \\(p\\) (il numero delle variabili) è grande.\n\n28.4.1 Casi di Heywood\nTra gli inconvenienti del metodo dei fattori principali iterato vi è il fatto che può talvolta portare a soluzioni inammissibili (quando viene fattorizzata la matrice R) caratterizzate da valori di comunalità maggiori di uno (caso di Heywood). Se \\(\\hat{h}^2_i &gt; 1\\) allora \\(\\hat{\\psi}_i &lt; 0\\) il che è chiaramente assurdo in quanto una varianza non può assumere un valore negativo. Solitamente, quando la stima di una comunalità è maggiore di uno, il processo iterativo viene interrotto e il programma riporta che non può essere trovata una soluzione.\nPer fare un esempio, usiamo la funzione fa() contenuta nel pacchetto psych. Tale funzione per trovare la soluzione dei fattori principali mediante il metodo iterativo.\n\npa &lt;- fa(R, nfactors = 2, rotate = \"none\", fm = \"pa\")\npa\n\nWarning message in fa.stats(r = r, f = f, phi = phi, n.obs = n.obs, np.obs = np.obs, :\n\"The estimated weights for the factor scores are probably incorrect.  Try a different factor score estimation method.\"\nWarning message in fac(r = r, nfactors = nfactors, n.obs = n.obs, rotate = rotate, :\n\"An ultra-Heywood case was detected.  Examine the results carefully\"\n\n\nFactor Analysis using method =  pa\nCall: fa(r = R, nfactors = 2, rotate = \"none\", fm = \"pa\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n   PA1   PA2   h2     u2 com\nK 0.98 -0.21 1.01 -0.008 1.1\nI 0.48  0.74 0.77  0.230 1.7\nH 0.78 -0.56 0.92  0.085 1.8\nL 0.98 -0.19 0.99  0.010 1.1\nJ 0.69  0.69 0.95  0.049 2.0\n\n                       PA1  PA2\nSS loadings           3.22 1.41\nProportion Var        0.64 0.28\nCumulative Var        0.64 0.93\nProportion Explained  0.70 0.30\nCumulative Proportion 0.70 1.00\n\nMean item complexity =  1.5\nTest of the hypothesis that 2 factors are sufficient.\n\ndf null model =  10  with the objective function =  12\ndf of  the model are 1  and the objective function was  5.6 \n\nThe root mean square of the residuals (RMSR) is  0.01 \nThe df corrected root mean square of the residuals is  0.04 \n\nFit based upon off diagonal values = 1\n\n\nSi noti che, in questo caso, le unicità assumono valori negativi, il che suggerisce che la soluzione è impropria.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html#metodo-di-massima-verosimiglianza",
    "href": "chapters/extraction/02_estrazione.html#metodo-di-massima-verosimiglianza",
    "title": "28  L’estrazione dei fattori",
    "section": "28.5 Metodo di massima verosimiglianza",
    "text": "28.5 Metodo di massima verosimiglianza\nIl metodo di massima verosimiglianza è indicato quando si può assumere che le variabili manifeste seguano una distribuzione normale multivariata. In queste condizioni, il metodo produce stime dei pesi fattoriali che più verosimilmente hanno prodotto le correlazioni osservate. Gli stimatori di massima verosimiglianza sono preferibili a quelli ottenuti con altri metodi, a condizione che le premesse siano pienamente realizzate.\nLa funzione \\(F\\) che viene minimizzata dal metodo di massima verosimiglianza rappresenta una misura di “distanza” tra la matrice di covarianza osservata e quella predetta dal modello. Uguagliando a zero le derivate di \\(F\\) rispetto ai parametri del modello \\(\\boldsymbol{\\Lambda}\\) e \\(\\boldsymbol{\\Psi}\\) si ottengono le equazioni per le stime di massima verosimiglianza di \\(\\hat{\\boldsymbol{\\Lambda}}\\) e \\(\\hat{\\boldsymbol{\\Psi}}\\). Risolvendo tali equazioni rispetto alle incognite \\(\\hat{\\boldsymbol{\\Lambda}}\\) e \\(\\hat{\\boldsymbol{\\Psi}}\\) si ricavano le stime di massima verosimiglianza.\nPoiché non esiste una soluzione analitica per le equazioni che stimano i parametri \\(\\boldsymbol{\\Lambda}\\) e \\(\\boldsymbol{\\Psi}\\), si utilizzano metodi numerici iterativi per minimizzare la differenza tra la matrice di covarianze (o correlazioni) osservata e quella predetta dal modello. Tuttavia, le stime di massima verosimiglianza possono presentare problemi di convergenza e casi di Heywood, in cui le stime di comunalità sono superiori a 1. Nonostante ciò, la soluzione è indipendente dall’unità di misura delle variabili manifeste e si ottiene la stessa soluzione sia analizzando la matrice delle varianze e covarianze sia quella delle correlazioni.\nLa stima di massima verosimiglianza si ottiene usando factanal. È inoltre il metodo di stima di default di lavaan.\nConsideriamo nuovamente i dati dell’esempio precedente. La stima di massima verosimiglianza dei parametri \\(\\boldsymbol{\\Lambda}\\) e \\(\\boldsymbol{\\Psi}\\) si ottiene nel modo seguente. i\n\nfactanal(covmat=R, factors=2, rotation=\"none\", n.obs=225)\n\n\nCall:\nfactanal(factors = 2, covmat = R, n.obs = 225, rotation = \"none\")\n\nUniquenesses:\n    K     I     H     L     J \n0.005 0.268 0.055 0.008 0.005 \n\nLoadings:\n  Factor1 Factor2\nK  0.955  -0.289 \nI  0.528   0.673 \nH  0.720  -0.653 \nL  0.954  -0.287 \nJ  0.764   0.642 \n\n               Factor1 Factor2\nSS loadings      3.203   1.457\nProportion Var   0.641   0.291\nCumulative Var   0.641   0.932\n\nTest of the hypothesis that 2 factors are sufficient.\nThe chi square statistic is 648.09 on 1 degree of freedom.\nThe p-value is 5.81e-143 \n\n\nSi noti che il risultato è molto simile a quello ottenuto in precedenza. Si noti inoltre che le stime di massima verosimiglianza consentono un test di bontà di adattamento del modello ai dati (test chi quadrato).",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html",
    "href": "chapters/extraction/03_numero_fattori.html",
    "title": "29  Il numero dei fattori",
    "section": "",
    "text": "29.1 Introduzione\nImmagina un scenario in cui un collega intende utilizzare un inventario della personalità che comprende sei aggettivi: loquace, assertivo, fantasioso, creativo, estroverso e intellettuale. In questo questionario, i partecipanti esprimono il grado con cui ciascun aggettivo descrive la loro personalità. Il tuo collega chiede il tuo parere su questa forma comune di valutazione della personalità basata su aggettivi. Mentre rifletti sull’inventario, emergono vari interrogativi: cosa misura esattamente questo inventario? Misura sei aspetti distinti della personalità, con ogni aspetto rappresentato da un aggettivo? O misura un singolo costrutto? In tal caso, quale sarebbe questo costrutto e cosa hanno in comune questi sei aggettivi in termini di caratteristiche o dimensioni psicologiche? O, invece, ci sono due o tre dimensioni separate rappresentate da questi sei aggettivi? Come sarà valutato e interpretato questo questionario?\nEsaminando i sei aggettivi, potresti raggrupparli in base alla loro somiglianza reciproca. Alcuni potrebbero suggerire che loquace, assertivo ed estroverso rappresentano variazioni di un attributo chiamato “estroversità”, mentre fantasioso, creativo e intellettuale rappresentano variazioni di un altro attributo chiamato “apertura all’esperienza”. Da questa prospettiva, le risposte a questi aggettivi riflettono due dimensioni di base della personalità.\nTuttavia, un’altra prospettiva potrebbe suggerire che i sei aggettivi riflettono tre dimensioni. In questo caso, loquace, assertivo ed estroverso potrebbero essere raggruppati insieme, così come fantasioso e creativo, ma intellettuale potrebbe rappresentare una dimensione separata. Da questa prospettiva, le risposte ai sei item riflettono tre dimensioni fondamentali.\nQuesto esempio mette in luce la questione della dimensionalità del test, che è una considerazione cruciale nello sviluppo, nella valutazione e nell’utilizzo di un test. Emergono almeno tre questioni psicometriche fondamentali riguardo alla dimensionalità di un test, e le risposte a queste domande hanno importanti implicazioni per valutare le proprietà psicometriche di un test comportamentale, per la valutazione appropriata in un test e per l’interpretazione corretta dei punteggi del test.\nIl dilemma presentato richiede l’impiego di metodologie statistiche per discernere la struttura latente sottostante. L’Analisi Fattoriale Esplorativa (EFA) è uno strumento chiave in questo contesto, utilizzato per identificare le dimensioni latenti e sintetizzare i dati. Tuttavia, una sfida principale nell’EFA è la selezione del numero di fattori da considerare, dato che l’aggiunta di nuovi fattori può migliorare l’adattamento ai dati, rendendo complesso stabilire un criterio statistico per determinare il numero ottimale di fattori.\nLa maggior parte dei metodi esistenti per selezionare il numero di fattori può essere diviso in due categorie.\nEsistono altri metodi oltre alle due categorie principali: il metodo Very Simple Structure (VSS) che cerca di recuperare strutture facilmente interpretabili e l’analisi grafica esplorativa (EGA) che stima una matrice di correlazione parziale regolarizzata e applica un algoritmo di rilevamento della comunità per determinare il numero di fattori.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#introduzione",
    "href": "chapters/extraction/03_numero_fattori.html#introduzione",
    "title": "29  Il numero dei fattori",
    "section": "",
    "text": "I metodi della prima categoria analizzano gli autovalori della matrice di covarianza per determinare il numero di fattori.\nLa seconda categoria affronta la stima del numero di fattori come un problema di selezione di modelli. Poiché il modello fattoriale con il maggior numero di fattori si adatta sempre meglio ai dati, la stima del numero di fattori nella popolazione richiede un compromesso tra adattamento del modello e parsimonia della soluzione.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#tre-domande-sulla-dimensionalità-del-test",
    "href": "chapters/extraction/03_numero_fattori.html#tre-domande-sulla-dimensionalità-del-test",
    "title": "29  Il numero dei fattori",
    "section": "29.2 Tre domande sulla dimensionalità del test",
    "text": "29.2 Tre domande sulla dimensionalità del test\nSono presenti almeno tre questioni rilevanti riguardo la dimensionalità di un test.\n\nNumero di Dimensioni:\n\nLa prima questione riguarda il numero di dimensioni rifletto dagli item del test. Alcuni test riflettono una sola dimensione, mentre altri ne riflettono due o più. Questa questione è importante poiché ogni dimensione del test è probabile che venga valutata separatamente, necessitando ciascuna una propria analisi psicometrica.\n\nCorrelazione tra Dimensioni:\n\nLa seconda questione indaga se, in un test con più di una dimensione, queste dimensioni siano correlate tra loro. Alcuni test presentano diverse dimensioni che sono in qualche modo correlate, mentre altri hanno dimensioni essenzialmente indipendenti e non correlate. Questa questione è rilevante, in parte, perché la natura delle associazioni tra le dimensioni di un test ha implicazioni per la significatività del “punteggio totale” del test.\n\nNatura delle Dimensioni:\n\nLa terza questione si chiede quali sono le dimensioni in un test con più di una dimensione, ovvero, quali attributi psicologici sono riflessi dalle dimensioni del test? Ad esempio, nel test della personalità con sei aggettivi descritto precedentemente, la prima dimensione riflette l’attributo psicologico dell’estroversione o qualche altro attributo? L’importanza di questa questione è evidente: per valutare ed interpretare efficacemente una dimensione di un test, è necessario comprendere il significato psicologico del punteggio.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#metodi-basati-sugli-autovalori",
    "href": "chapters/extraction/03_numero_fattori.html#metodi-basati-sugli-autovalori",
    "title": "29  Il numero dei fattori",
    "section": "29.3 Metodi basati sugli autovalori",
    "text": "29.3 Metodi basati sugli autovalori\nSono stati proposti quattro criteri basati sugli autovalori per determinare il numero \\(m\\) di fattori da estrarre (Rencher, 2002).\n\nScegliere \\(m\\) tale per cui la varianza spiegata dal modello fattoriale superi una soglia predeterminata, per esempio l’80% della varianza totale, \\(tr(\\textbf{S})\\) o \\(tr(\\textbf{R})\\).\nScegliere \\(m\\) uguale al numero di autovalori aventi un valore maggiore del valore medio degli autovalori. Per R il valore medio degli autovalori è \\(1\\); per S è \\(\\sum_{j=1}^p \\theta_j/p\\).\nUsare lo scree test.\nMediante la statistica \\(\\chi^2\\), valutare l’ipotesi che \\(m\\) sia il numero corretto di fattori, \\(H_0: \\boldsymbol{\\Sigma} =  \\boldsymbol{\\Lambda}\n  \\boldsymbol{\\Lambda}^{\\mathsf{T}} +  \\boldsymbol{\\Psi}\\), dove \\(\\boldsymbol{\\Lambda}\\) è di ordine \\(p \\times m\\).\n\n\n29.3.1 Quota di varianza spiegata\nIl primo criterio si applica soprattutto al metodo delle componenti principali. La proporzione della varianza capionaria spiegata dal fattore \\(j\\)-esimo estratto da S è uguale a\n\\[\\sum_{i=i}^p \\hat{\\lambda}_{ij}^2 / tr(\\textbf{S}).\\]\nNel caso in cui i fattori vengano estratti da R avremo\n\\[\\sum_{i=i}^p \\hat{\\lambda}_{ij}^2 / p.\\]\nNel caso di fattori incorrelati, ciascun fattore contribuisce con una quota complessiva di varianza spiegata pari alla somma dei quadrati delle saturazioni fattoriali contenute nella matrice \\(\\hat{\\boldsymbol{\\Lambda}}\\): \\(\\sum_{i=1}^p\\sum_{j=1}^m\\hat{\\lambda}_{ij}^2\\). Nel caso del metodo delle componenti principali, tale somma è anche uguale alla somma dei primi \\(m\\) autovalori, o alla somma di tutte le \\(p\\) comunalità:\n\\[\\sum_{i=1}^p\\sum_{j=1}^m\\hat{\\lambda}_{ij}^2= \\sum_{i=1}^p \\hat{h}_i^2\n= \\sum_{j=1}^m \\theta_j\\]\nSulla base di queste considerazioni, il numero \\(m\\) di fattori viene scelto in modo da spiegare una quota sufficientemente grande di S o \\(p\\).\nIl numero dei fattori può essere determinato in questo modo anche nel caso in cui l’analisi fattoriale venga eseguita con il metodo dei fattori principali (ovvero, nel caso in cui vengano usate le stime delle comunalità per generare la matrice ridotta \\(\\textbf{S} -\n\\hat{\\boldsymbol{\\Psi}}\\) o \\(\\textbf{R} - \\hat{\\boldsymbol{\\Psi}}\\)). In questo caso, però, è possibile che alcuni autovalori della matrice \\(\\textbf{S} - \\hat{\\boldsymbol{\\Psi}}\\) o \\(\\textbf{R} -\n\\hat{\\boldsymbol{\\Psi}}\\) assumano valore negativo. In tali circostanze, è possibile che la proporzione cumulativa della varianza \\(\\sum_{j=1}^m \\theta_j / \\sum_{j=1}^p \\theta_j\\) assuma un valore maggiore di \\(1.0\\) per \\(j &lt; p\\).\nLa proporzione cumulativa della varianza si riduce poi a \\(1.0\\) quando vengono considerati anche i successivi autovalori negativi. Di conseguenza, può succedere che, utilizzando la matrice \\(\\textbf{S} - \\hat{\\boldsymbol{\\Psi}}\\) o \\(\\textbf{R} - \\hat{\\boldsymbol{\\Psi}}\\), il criterio definito in base alla quota della varianza spiegata venga raggiunto per un valore \\(m\\) minore di quello che verrebbe trovato utilizzando la matrice S o R.\nNel caso del metodo dei fattori principali iterato, \\(m\\) viene specificato precedentemente a ciascuna iterazione e \\(\\sum_{i}\n\\hat{h}^2_i\\) viene ottenuto dopo ciascuna iterazione calcolando \\(\\text{tr}(\\textbf{S} - \\hat{\\boldsymbol{\\Psi}})\\). Per scegliere \\(m\\), come per il metodo delle componenti principali, possono essere usati gli autovalori di S o R.\n\n\n29.3.2 Valore medio degli autovalori\nIl calcolo del valore medio degli autovalori è una procedura euristica implementata in molti software. In una variante di tale metodo, \\(m\\) viene scelto in modo tale da uguagliare il numero degli autovalori positivi della matrice ridotta \\(\\textbf{R} - \\hat{\\boldsymbol{\\Psi}}\\) (in tale matrice vi sono solitamente degli autovalori negativi). Tale variante ha però lo svantaggio di produrre solitamente un numero di fattori troppo grande.\n\n\n29.3.3 Scree test\nLo scree test si basa su un grafico che rappresenta gli autovalori di S o R ordinati in modo decrescente in funzione del numero dei fattori. I punti che rappresentano gli autovalori vengono collegati con una spezzata. Il valore m viene determinato in corrispondenza del fattore oltre il quale il dislivello tra fattori successivi diventa esiguo e la spezzata tende a diventare orizzontale.\n\n\n29.3.4 Parallel analysis\nLa Parallel Analysis si basa sul confronto tra gli autovalori empirici della matrice di correlazione delle variabili originali e quelli generati da un campione casuale di variabili standardizzate. In questo modo si tiene conto delle variazioni dovute agli errori di campionamento. Poiché anche in presenza di variabili incorrelate la matrice di correlazione presenta sempre autovalori maggiori di uno a causa della variabilità campionaria, il confronto tra gli autovalori empirici e quelli generati dalla Parallel Analysis permette di individuare il numero di fattori significativi. Una simulazione di Monte Carlo su una matrice di correlazione di \\(p=10\\) variabili casuali mutuamente indipendenti, ciascuna con \\(n=20\\) osservazioni, può essere utilizzata per illustrare la procedura.\n\nn &lt;- 20\nnsim &lt;- 1000\ne1 &lt;- rep(0, nsim)\nfor (i in 1:nsim) {\n  Y &lt;- cbind(\n    rnorm(n), rnorm(n), rnorm(n), rnorm(n), rnorm(n),\n    rnorm(n), rnorm(n), rnorm(n), rnorm(n), rnorm(n)\n  )\n  e &lt;- eigen(cor(Y))\n  e1[i] &lt;- e$values[1]\n}\nmax(e1)\n\n3.34528401303822\n\n\nPer i dati di questa simulazione, l’autovalore maggiore ha un valore pari a \\(3.35\\), anche se i dati sono del tutto casuali. La Parallel Analysis tiene conto di questo fatto e determina \\(m\\) confrontando gli autovalori empirici con le loro “controparti casuali.” Vanno a determinare \\(m\\) solo gli autovalori empirici che hanno un valore superiore ai corrispondenti autovalori generati da una matrice di dati dello stesso ordine composta da colonne mutualmente incorrelate. Nel caso di questa simulazione di Monte Carlo, se l’autovalore maggiore derivato da una matrice di numeri casuali ha un valore di \\(3.35\\), verranno considerati solo gli autovalori empirici che superano questo valore per determinare il numero di fattori.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#metodi-basati-sul-confronto-tra-modelli",
    "href": "chapters/extraction/03_numero_fattori.html#metodi-basati-sul-confronto-tra-modelli",
    "title": "29  Il numero dei fattori",
    "section": "29.4 Metodi basati sul confronto tra modelli",
    "text": "29.4 Metodi basati sul confronto tra modelli\nIl confronto tra modelli può essere eseguito usando varie statistiche. Una scelta popolare per stimare il numero di fattori nella EFA è il Criterio d’Informazione Bayesiano (BIC; Schwarz, 1978), introdotto come un miglioramento rispetto al Criterio d’Informazione di Akaike (AIC; Akaike, 1973). Un’alternativa è l’indice RMSEA, che può essere considerato come una stima della mancanza di adattamento che tiene in considerazione i gradi di libertà del modello (Browne e Cudeck, 1992). Un altro metodo di questo tipo è il test Minimum Average Partial (MAP), che stima le correlazioni parziali residue medie per diversi numeri di fattori e seleziona quello con il valore più basso (Velicer, 1976).\n\n29.4.1 Test del rapporto di verosimiglianze\nIn questo test si confrontano due ipotesi: l’ipotesi nulla \\(H_0\\) e l’ipotesi alternativa \\(H_1\\), per valutare la bontà di adattamento di un modello fattoriale a una matrice di covarianza delle variabili oggetto di osservazione campionaria \\(Y\\). L’ipotesi nulla postula che la struttura di interdipendenza di \\(Y\\) può essere spiegata da \\(m\\) fattori comuni, mentre l’alternativa postula che i fattori comuni non sono sufficienti per spiegare la matrice di covarianza \\(\\boldsymbol{\\Sigma}\\).\nIl test si basa sulla statistica del chi-quadrato con gradi di libertà pari a \\(\\nu = \\frac{1}{2}[(p-m)^2-(p-m)]\\), dove \\(p\\) è il numero di variabili e \\(m\\) è il numero di fattori. In pratica, si inizia con \\(m^*=1\\) e si valuta l’ipotesi \\(H_0\\) per \\(m^*\\). Se \\(H_0\\) non viene rifiutata, il procedimento si arresta. In caso contrario, si considera \\(m^*+1\\) e si ripete il test finché \\(H_0\\) viene accettata o finché si raggiunge il valore minimo di gradi di libertà pari a zero.\nIl test del rapporto di verosimiglianze è particolarmente indicato quando il numero di osservazioni è grande, ma la sua applicazione è limitata dalle dimensioni del campione. In alternativa, è possibile utilizzare gli indici AIC, BIC e RMSEA per scegliere la soluzione con il valore più piccolo di tali statistiche. Tuttavia, questi indici non forniscono un test statistico per il confronto tra modelli.\nIn pratica, si può considerare il valore \\(m\\) indicato dal test come il limite superiore del numero di fattori che sono importanti dal punto di vista pratico.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#minimizzazione-dellout-of-sample-prediction-error",
    "href": "chapters/extraction/03_numero_fattori.html#minimizzazione-dellout-of-sample-prediction-error",
    "title": "29  Il numero dei fattori",
    "section": "29.5 Minimizzazione dell’out-of-sample prediction error",
    "text": "29.5 Minimizzazione dell’out-of-sample prediction error\nRecentemente è stato proposto un nuovo metodo per stimare il numero di fattori in EFA che affronta il problema come un problema di selezione del modello {cite:p}haslbeck2022estimating. L’obiettivo è confrontare i modelli con 1, 2, …, p fattori, dove \\(p\\) è il numero di variabili, e selezionare il modello con l’errore di previsione atteso più basso nella popolazione. Tuttavia, questo è un compito non banale perché il modello che minimizza l’errore di previsione nel campione non minimizza sempre l’errore di previsione nella popolazione.\nIntuitivamente, questo problema viene affrontato suddividendo il campione di dati in due insiemi: un set di training e un set di test. Il set di training viene utilizzato per stimare i parametri del modello, le cui previsioni vengono poi verificate utilizzando i dati di test (non utilizzati per la stima dei parametri). Questo calcolo dell’errore di previsione fuori campione viene ripetuto diverse volte, suddividendo ogni volta in modo casuale il campione negli insiemi di training e test. Tale metodo per stimare il numero di fattori è implementato nel pacchetto R fspe.\nEsempio. Per confrontare i metodi discussi per la scelta del numero \\(m\\) di fattori usiamo una matrice di correlazioni calcolata sulle sottoscale della WAIS. Le 11 sottoscale del test sono le seguenti:\n\nX1 = Information\nX2 = Comprehension\nX3 = Arithmetic\nX4 = Similarities\nX5 = Digit.span\nX6 = Vocabulary\nX7 = Digit.symbol\nX8 = Picture.completion\nX9 = Block.design\nX10 = Picture.arrangement\nX11 = Object.\n\nI dati sono stati ottenuti dal manuale della III edizione.\n\nvarnames &lt;- c(\n    \"IN\", \"CO\", \"AR\", \"SI\", \"DS\", \"VO\", \"SY\", \"PC\",\n    \"BD\", \"PA\", \"OA\", \"AG\", \"ED\"\n)\ntemp &lt;- matrix(c(\n    1, 0.67, 0.62, 0.66, 0.47, 0.81, 0.47, 0.60, 0.49, 0.51, 0.41,\n    -0.07, 0.66, .67, 1, 0.54, 0.60, 0.39, 0.72, 0.40, 0.54, 0.45,\n    0.49, 0.38, -0.08, 0.52, .62, .54, 1, 0.51, 0.51, 0.58, 0.41,\n    0.46, 0.48, 0.43, 0.37, -0.08, 0.49, .66, .60, .51, 1, 0.41,\n    0.68, 0.49, 0.56, 0.50, 0.50, 0.41, -0.19, 0.55, .47, .39, .51,\n    .41, 1, 0.45, 0.45, 0.42, 0.39, 0.42, 0.31, -0.19, 0.43,\n    .81, .72, .58, .68, .45, 1, 0.49, 0.57, 0.46, 0.52, 0.40, -0.02,\n    0.62, .47, .40, .41, .49, .45, .49, 1, 0.50, 0.50, 0.52, 0.46,\n    -0.46, 0.57, .60, .54, .46, .56, .42, .57, .50, 1, 0.61, 0.59,\n    0.51, -0.28, 0.48, .49, .45, .48, .50, .39, .46, .50, .61, 1,\n    0.54, 0.59, -0.32, 0.44, .51, .49, .43, .50, .42, .52, .52, .59,\n    .54, 1, 0.46, -0.37, 0.49, .41, .38, .37, .41, .31, .40, .46, .51,\n    .59, .46, 1, -0.28, 0.40, -.07, -.08, -.08, -.19, -.19, -.02,\n    -.46, -.28, -.32, -.37, -.28, 1, -0.29, .66, .52, .49, .55, .43,\n    .62, .57, .48, .44, .49, .40, -.29, 1\n), nrow = 13, ncol = 13, byrow = TRUE)\n\ncolnames(temp) &lt;- varnames\nrownames(temp) &lt;- varnames\n\nwais_cor &lt;- temp[1:11, 1:11]\nwais_cor\n\n\nA matrix: 11 x 11 of type dbl\n\n\n\nIN\nCO\nAR\nSI\nDS\nVO\nSY\nPC\nBD\nPA\nOA\n\n\n\n\nIN\n1.00\n0.67\n0.62\n0.66\n0.47\n0.81\n0.47\n0.60\n0.49\n0.51\n0.41\n\n\nCO\n0.67\n1.00\n0.54\n0.60\n0.39\n0.72\n0.40\n0.54\n0.45\n0.49\n0.38\n\n\nAR\n0.62\n0.54\n1.00\n0.51\n0.51\n0.58\n0.41\n0.46\n0.48\n0.43\n0.37\n\n\nSI\n0.66\n0.60\n0.51\n1.00\n0.41\n0.68\n0.49\n0.56\n0.50\n0.50\n0.41\n\n\nDS\n0.47\n0.39\n0.51\n0.41\n1.00\n0.45\n0.45\n0.42\n0.39\n0.42\n0.31\n\n\nVO\n0.81\n0.72\n0.58\n0.68\n0.45\n1.00\n0.49\n0.57\n0.46\n0.52\n0.40\n\n\nSY\n0.47\n0.40\n0.41\n0.49\n0.45\n0.49\n1.00\n0.50\n0.50\n0.52\n0.46\n\n\nPC\n0.60\n0.54\n0.46\n0.56\n0.42\n0.57\n0.50\n1.00\n0.61\n0.59\n0.51\n\n\nBD\n0.49\n0.45\n0.48\n0.50\n0.39\n0.46\n0.50\n0.61\n1.00\n0.54\n0.59\n\n\nPA\n0.51\n0.49\n0.43\n0.50\n0.42\n0.52\n0.52\n0.59\n0.54\n1.00\n0.46\n\n\nOA\n0.41\n0.38\n0.37\n0.41\n0.31\n0.40\n0.46\n0.51\n0.59\n0.46\n1.00\n\n\n\n\n\nIl primo metodo per la determinazione di \\(m\\) richiede di estrarre tanti fattori quanti sono necessari per spiegare una quota predeterminata della varianza totale. Supponiamo di porre il criterio pari all’80% della varianza totale.\n\nout &lt;- eigen(wais_cor)\nsum(out$val[1:4]) / sum(out$val)\nsum(out$val[1:5]) / sum(out$val)\n\n0.765678107076221\n\n\n0.811885250571924\n\n\nLa soluzione ottenuta in questo modo ci porterebbe a mantenere \\(m=5\\) fattori.\nIl secondo metodo suggerisce di mantenere tutti gli autovalori superiori al valore medio degli autovalori (che, nel caso di R è uguale a \\(1\\)).\n\nprint(round(out$values, 3))\n\n [1] 6.074 1.015 0.746 0.587 0.508 0.431 0.423 0.377 0.351 0.310 0.177\n\n\nNel caso presente, \\(m=2\\).\nLo scree test può essere eseguito creando il grafico seguente.\n\nn &lt;- dim(wais_cor)[1]\nscree_tb &lt;- tibble(\n    x = 1:n,\n    y = sort(eigen(wais_cor)$value, decreasing = TRUE)\n)\n\nscree_plot &lt;- scree_tb |&gt;\n  ggplot(aes(x, y)) +\n  geom_point() +\n  geom_line() +\n  theme_minimal() +\n  scale_x_continuous(breaks = 1:n) +\n  ggtitle(\"Scree plot\")\n\nscree_plot\n\n\n\n\n\n\n\n\nLo scree test suggerisce la presenza di un unico fattore comune.\nLa versione della Parallel Analysis può essere eseguita con la funzione paran() contenuta nel pacchetto paran.\n\nparan(wais_cor, graph = TRUE)\n\n\nUsing eigendecomposition of correlation matrix.\nComputing: 10%  20%  30%  40%  50%  60%  70%  80%  90%  100%\n\n\nResults of Horn's Parallel Analysis for component retention\n330 iterations, using the mean estimate\n\n-------------------------------------------------- \nComponent   Adjusted    Unadjusted    Estimated \n            Eigenvalue  Eigenvalue    Bias \n-------------------------------------------------- \n1           1.647667    3.765744      2.118077\n-------------------------------------------------- \n\nAdjusted eigenvalues &gt; 1 indicate dimensions to retain.\n(1 components retained)\n\n\n\n\n\n\n\n\n\n\nLa Parallel Analysis indica una soluzione a \\(m=1\\) fattore.\nIl test inferenziale relativo al numero di fattori basato sulla statistica \\(\\chi^2\\) può essere eseguito nel modo seguente.\n\nfactanal(covmat=wais_cor, factors=4, n.obs=933)\n\n\nCall:\nfactanal(factors = 4, covmat = wais_cor, n.obs = 933)\n\nUniquenesses:\n   IN    CO    AR    SI    DS    VO    SY    PC    BD    PA    OA \n0.229 0.387 0.005 0.416 0.645 0.137 0.005 0.375 0.331 0.492 0.519 \n\nLoadings:\n   Factor1 Factor2 Factor3 Factor4\nIN 0.758   0.306   0.279   0.157  \nCO 0.672   0.312   0.229   0.107  \nAR 0.368   0.247   0.886   0.120  \nSI 0.602   0.376   0.193   0.207  \nDS 0.315   0.288   0.331   0.252  \nVO 0.851   0.242   0.208   0.192  \nSY 0.238   0.359   0.144   0.888  \nPC 0.432   0.623   0.143   0.172  \nBD 0.237   0.733   0.217   0.168  \nPA 0.367   0.539   0.150   0.245  \nOA 0.207   0.620   0.133   0.190  \n\n               Factor1 Factor2 Factor3 Factor4\nSS loadings      2.826   2.264   1.233   1.137\nProportion Var   0.257   0.206   0.112   0.103\nCumulative Var   0.257   0.463   0.575   0.678\n\nTest of the hypothesis that 4 factors are sufficient.\nThe chi square statistic is 35.4 on 17 degrees of freedom.\nThe p-value is 0.00551 \n\n\n\nfactanal(covmat = wais_cor, factors = 5, n.obs = 933)\n\n\nCall:\nfactanal(factors = 5, covmat = wais_cor, n.obs = 933)\n\nUniquenesses:\n   IN    CO    AR    SI    DS    VO    SY    PC    BD    PA    OA \n0.235 0.389 0.117 0.419 0.600 0.109 0.277 0.308 0.334 0.472 0.456 \n\nLoadings:\n   Factor1 Factor2 Factor3 Factor4 Factor5\nIN  0.745   0.264   0.301   0.192   0.118 \nCO  0.667   0.278   0.244   0.129   0.111 \nAR  0.378   0.236   0.814   0.145         \nSI  0.591   0.332   0.207   0.252   0.121 \nDS  0.288   0.208   0.366   0.341   0.155 \nVO  0.865   0.216   0.207   0.229         \nSY  0.251   0.364   0.153   0.708         \nPC  0.425   0.548   0.156   0.216   0.375 \nBD  0.246   0.708   0.230   0.201   0.107 \nPA  0.355   0.457   0.163   0.325   0.245 \nOA  0.211   0.664   0.128   0.205         \n\n               Factor1 Factor2 Factor3 Factor4 Factor5\nSS loadings      2.799   1.986   1.176   1.043   0.280\nProportion Var   0.254   0.181   0.107   0.095   0.025\nCumulative Var   0.254   0.435   0.542   0.637   0.662\n\nTest of the hypothesis that 5 factors are sufficient.\nThe chi square statistic is 12.46 on 10 degrees of freedom.\nThe p-value is 0.256 \n\n\nIl test del \\(\\chi^2\\) indica una soluzione a sei fattori.\nPer concludere, si potrebbe usare il metodo basato sulla minimizzazione dell’errore di previsione. Tuttavia, non possiamo applicare tale metodo ai dati dell’esempio in quanto sarebbe necessario disporre dei dati grezzi (la matrice di correlazioni non è sufficiente). Allo scopo di illustrare la procedura relativa al metodo basato sulla minimizzazione dell’errore di previsione useremo qui un set di dati diverso, ovvero holzinger19.\n\ndata(holzinger19)\n\nsuppressWarnings(\n    fspe_out &lt;- fspe(\n        data = holzinger19,\n        maxK = 10,\n        nfold = 10,\n        rep = 10,\n        method = \"PE\"\n    )\n)\n\n  |                                                                      |   0%\n\n\nCaricamento dei namespace richiesti: GPArotation\n\n\n\n  |----------------------------------------------------------------------| 100%\n\n\n\npar(mar=c(4,4,1,1))\nplot.new()\nplot.window(xlim=c(1, 10), ylim=c(.6, .8))\naxis(1, 1:10)\naxis(2, las=2)\nabline(h=min(fspe_out$PEs), col=\"grey\")\nlines(fspe_out$PEs, lty=2)\npoints(fspe_out$PEs, pch=20, cex=1.5)\ntitle(xlab=\"Number of Factors\", ylab=\"Prediction Error\")\n\n\n\n\n\n\n\n\nPer i dati holzinger19, il metodo di {cite:t}haslbeck2022estimating produce dunque una soluzione a 4 fattori.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#considerazioni-conclusive",
    "href": "chapters/extraction/03_numero_fattori.html#considerazioni-conclusive",
    "title": "29  Il numero dei fattori",
    "section": "29.6 Considerazioni conclusive",
    "text": "29.6 Considerazioni conclusive\nIn generale, la scelta del numero di fattori \\(m\\) non è sempre ovvia e rappresenta un limite dell’analisi fattoriale. Per affrontare questo problema, tradizionalmente si utilizza uno strumento come lo scree test per valutare la proporzione di varianza spiegata di ciascun item e l’interpretabilità della soluzione ottenuta dopo una rotazione adeguata. Tuttavia, poiché la scelta di \\(m\\) è soggettiva, i limiti della soluzione ottenuta sono evidenti. In alcuni casi, la scelta di \\(m\\) può essere più certa quando tutti i metodi forniscono la stessa risposta. Un’alternativa più moderna potrebbe essere l’uso di un metodo basato sulla minimizzazione dell’errore di previsione come quello descritto da {cite:t}haslbeck2022estimating. In questo modo, si potrebbe ottenere una soluzione più affidabile e oggettiva.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/04_rotazione.html",
    "href": "chapters/extraction/04_rotazione.html",
    "title": "30  La rotazione fattoriale",
    "section": "",
    "text": "30.1 Indeterminatezza della soluzione fattoriale\nLa necessità di effettuare la rotazione deriva dal fatto che la matrice delle saturazioni non ha un’unica soluzione. Attraverso trasformazioni matematiche, è possibile ottenere infinite matrici dello stesso ordine. Questo fenomeno è noto come indeterminatezza della soluzione fattoriale.\nLa matrice delle saturazioni fattoriali \\(\\boldsymbol{\\Lambda}\\) non è univocamente definita poiché non esiste una soluzione unica per determinare le saturazioni fattoriali. Una matrice di correlazioni \\(\\boldsymbol{R}\\) può produrre diverse soluzioni fattoriali, ovvero matrici con lo stesso numero di fattori comuni ma con diverse configurazioni di saturazioni fattoriali, o matrici di saturazioni fattoriali corrispondenti a un diverso numero di fattori comuni.\nEsempio. Siano \\(\\boldsymbol{\\Lambda}_1\\) e \\(\\boldsymbol{\\Lambda}_2\\) due matrici aventi lo stesso numero di righe e colonne, ma contenenti saturazioni fattoriali diverse. \\(\\boldsymbol{\\Lambda}_1\\) è definita dai valori seguenti\nl1 &lt;- matrix(\n  c(\n    0.766,  -0.232,\n    0.670,  -0.203,\n    0.574,  -0.174,\n    0.454,   0.533,\n    0.389,   0.457,\n    0.324,   0.381\n  ),\n  byrow = TRUE, ncol = 2\n)\nmentre per \\(\\boldsymbol{\\Lambda}_2\\) abbiamo\nl2 &lt;- matrix(\n  c(\n    0.783,  0.163,\n    0.685,  0.143,\n    0.587,  0.123,\n    0.143,  0.685,\n    0.123,  0.587,\n    0.102,  0.489\n  ),\n  byrow = TRUE, ncol = 2\n)\nEsaminiamo la matrice delle correlazioni riprodotte dalle due matrici di pesi fattoriali (con le comunalità sulla diagonale di \\(\\boldsymbol{R}\\)):\nl1 %*% t(l1) |&gt; round(2)\n\n\nA matrix: 6 x 6 of type dbl\n\n\n0.64\n0.56\n0.48\n0.22\n0.19\n0.16\n\n\n0.56\n0.49\n0.42\n0.20\n0.17\n0.14\n\n\n0.48\n0.42\n0.36\n0.17\n0.14\n0.12\n\n\n0.22\n0.20\n0.17\n0.49\n0.42\n0.35\n\n\n0.19\n0.17\n0.14\n0.42\n0.36\n0.30\n\n\n0.16\n0.14\n0.12\n0.35\n0.30\n0.25\nl2 %*% t(l2) |&gt; round(2)\n\n\nA matrix: 6 x 6 of type dbl\n\n\n0.64\n0.56\n0.48\n0.22\n0.19\n0.16\n\n\n0.56\n0.49\n0.42\n0.20\n0.17\n0.14\n\n\n0.48\n0.42\n0.36\n0.17\n0.14\n0.12\n\n\n0.22\n0.20\n0.17\n0.49\n0.42\n0.35\n\n\n0.19\n0.17\n0.14\n0.42\n0.36\n0.30\n\n\n0.16\n0.14\n0.12\n0.35\n0.30\n0.25\nCome si vede, viene ottenuto lo stesso risultato utilizzando matrici \\(\\boldsymbol{\\Lambda}\\) con lo stesso numero \\(m\\) di colonne ma saturazioni fattoriali diverse.\nSi consideri ora il caso di matrici \\(\\boldsymbol{\\Lambda}\\) corrispondenti a soluzioni fattoriali con un diverso numero di fattori comuni. Siano \\(\\boldsymbol{\\Lambda}_1\\) e \\(\\boldsymbol{\\Lambda}_2\\) due matrici aventi lo stesso numero di righe ma un numero diverso di colonne:\nl1 &lt;- matrix(\n  c(\n    0.9,\n    0.7,\n    0.5,\n    0.3\n  ),\n  byrow = TRUE, ncol = 1\n)\n\nl2 &lt;- matrix(\n  c(\n    0.78, 0.45,\n    0.61, 0.35,\n    0.43, 0.25,\n    0.25, 0.15\n  ),\n  byrow = TRUE, ncol = 2\n)\nSi noti che la stessa matrice di correlazioni riprodotte (con le comunalità sulla diagonale principale) viene generata dalle saturazioni fattoriali corrispondenti ad un numero diverso di fattori comuni:\nl1 %*% t(l1) |&gt; round(2)\n\n\nA matrix: 4 x 4 of type dbl\n\n\n0.81\n0.63\n0.45\n0.27\n\n\n0.63\n0.49\n0.35\n0.21\n\n\n0.45\n0.35\n0.25\n0.15\n\n\n0.27\n0.21\n0.15\n0.09\nl2 %*% t(l2) |&gt; round(2)\n\n\nA matrix: 4 x 4 of type dbl\n\n\n0.81\n0.63\n0.45\n0.26\n\n\n0.63\n0.49\n0.35\n0.20\n\n\n0.45\n0.35\n0.25\n0.14\n\n\n0.26\n0.20\n0.14\n0.08",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>La rotazione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/04_rotazione.html#parsimonia-e-semplicità",
    "href": "chapters/extraction/04_rotazione.html#parsimonia-e-semplicità",
    "title": "30  La rotazione fattoriale",
    "section": "30.2 Parsimonia e semplicità",
    "text": "30.2 Parsimonia e semplicità\nPer ottenere risultati affidabili dall’analisi fattoriale, si affronta il problema dell’indeterminazione fattoriale scegliendo la soluzione che soddisfa due criteri fondamentali: il criterio della parsimonia e il criterio della semplicità.\nIl criterio della parsimonia richiede di scegliere il modello con il minor numero di fattori comuni che può spiegare la covarianza tra le variabili. In pratica, se ci sono due soluzioni fattoriali con un diverso numero di fattori che riproducono allo stesso modo la matrice di covarianza o di correlazione, si sceglie quella con il minor numero di fattori.\nIn caso invece ci siano diverse soluzioni fattoriali con lo stesso numero m di fattori, il criterio della semplicità guida nella scelta della trasformazione più appropriata della matrice di saturazioni fattoriali \\(\\boldsymbol{\\Lambda}\\). Questa trasformazione, nota come rotazione, cerca di rendere i fattori più interpretabili. Ci sono due tipi di rotazione: ortogonale e obliqua.\nLa rotazione ortogonale assume che i fattori siano incorrelati, mentre la rotazione obliqua consente correlazioni tra i fattori. L’obiettivo della rotazione è di trovare una soluzione che renda i fattori più facilmente interpretabili e, quindi, in grado di spiegare meglio i dati.\n\n30.2.1 Il Criterio della Struttura Semplice nell’Analisi Fattoriale\nL’analisi fattoriale impiega la rotazione degli assi fattoriali per ottenere una “struttura semplice” nella matrice delle saturazioni fattoriali. Questo criterio, proposto originariamente da Thurstone nel 1947, mira a realizzare una matrice caratterizzata da un numero limitato di saturazioni (o carichi fattoriali) significative e diverse da zero, minimizzando al contempo la presenza di variabili influenzate da più di un fattore.\nPer raggiungere una struttura semplice, Thurstone ha delineato specifiche condizioni che la matrice fattoriale ruotata deve soddisfare: 1. Ogni variabile deve presentare saturazioni nulle con la maggior parte dei fattori, escludendo uno o pochi con cui mostra saturazioni significative. 2. Per ciascun fattore, devono esistere almeno \\(m\\) saturazioni nulle, dove \\(m\\) è il numero totale di fattori comuni.\nL’obiettivo della rotazione è quindi massimizzare il numero di saturazioni nulle o quasi nulle, facilitando l’interpretazione dei fattori. Analizzando la matrice ruotata, è possibile identificare le variabili che sono fortemente associate a specifici fattori e valutare l’intensità di tali associazioni.\nUn fattore si interpreta efficacemente quando i suoi carichi sono elevati e positivi su un gruppo ristretto di variabili; ciò suggerisce che il fattore rappresenta un tratto o una caratteristica comune a tali variabili. Tuttavia, l’interpretazione diventa più complessa quando le variabili presentano saturazioni significative con più di un fattore, poiché indica la presenza di sovrapposizioni nelle influenze fattoriali.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>La rotazione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/04_rotazione.html#rotazione-nello-spazio-geometrico",
    "href": "chapters/extraction/04_rotazione.html#rotazione-nello-spazio-geometrico",
    "title": "30  La rotazione fattoriale",
    "section": "30.3 Rotazione nello Spazio Geometrico",
    "text": "30.3 Rotazione nello Spazio Geometrico\n\n30.3.1 Rotazione Ortogonale\nCome precedentemente osservato, la matrice delle saturazioni fattoriali non è unica, implicando l’esistenza di multiple soluzioni equivalenti per determinare i pesi fattoriali. La rotazione ortogonale è un tipo di trasformazione lineare applicata ai pesi fattoriali per produrre una nuova matrice di saturazioni fattoriali che rispetti criteri specifici di struttura semplice. Questo processo ha lo scopo di rendere i dati più facilmente interpretabili.\nGeometricamente parlando, la rotazione ortogonale è simile a una rotazione rigida degli assi in uno spazio cartesiano che rappresenta i pesi fattoriali. Tale rotazione conserva le distanze tra i punti (che rappresentano le saturazioni fattoriali) ma modifica la loro posizione relativa rispetto ai fattori. Di conseguenza, si ottiene una configurazione dei pesi fattoriali che è più semplice da interpretare.\nLe tecniche di rotazione ortogonale sono tipicamente implementate attraverso metodi come la massima verosimiglianza o l’analisi dei componenti principali, con l’obiettivo di massimizzare il numero di saturazioni nulle o quasi nulle nella matrice delle saturazioni risultante. Questo processo aiuta a chiarire quale variabile è influenzata maggiormente da quali fattori, facilitando l’interpretazione dei risultati dell’analisi fattoriale.\n\n\n30.3.2 Vincoli alla Rotazione dei Fattori\nIl problema della non identificabilità della matrice dei pesi fattoriali, denotata come \\(\\hat{\\boldsymbol{\\Lambda}}\\), indica l’esistenza di molteplici matrici equivalenti che possono produrre identiche correlazioni tra le variabili di un modello. Per affrontare questa questione, è essenziale imporre vincoli sulla rotazione dei fattori. Uno dei criteri fondamentali nella scelta del tipo di rotazione è l’ottenimento di una matrice \\(\\hat{\\boldsymbol{\\Lambda}}\\) semplificata, i cui elementi si avvicinano il più possibile ai valori 0 e 1. Questo facilita l’interpretazione dei fattori come combinazioni lineari delle variabili.\nLe rotazioni ortogonali, utili in presenza di fattori non correlati, mantengono inalterate le comunalità, poiché conservano le distanze geometriche tra i punti rappresentati dai pesi fattoriali. In questo caso, le comunalità sono calcolate come la somma dei quadrati dei pesi fattoriali. Al contrario, le rotazioni non ortogonali modificano la quota di varianza spiegata da ciascun fattore, calcolata dalla somma dei quadrati dei pesi fattoriali divisa per la traccia della matrice di correlazione.\nEsistono vari algoritmi per eseguire la rotazione ortogonale dei fattori, tra cui il metodo grafico, il metodo Quartimax e il metodo Varimax. Ciascuno di questi metodi ha specifiche applicazioni e impatti sulla struttura della matrice risultante, facilitando così l’interpretazione dei dati analizzati.\n\n\n30.3.3 Metodo Grafico per la Rotazione dei Fattori\nQuando si dispone di soli $ m=2 $ fattori, il sistema di coordinate bidimensionale è utilizzato per rappresentare geometricamente i fattori. La visualizzazione grafica delle saturazioni fattoriali permette di determinare visivamente la rotazione più appropriata. Ogni riga della matrice \\(\\hat{\\boldsymbol{\\Lambda}}\\) rappresenta un paio di pesi fattoriali, \\(\\hat{\\lambda}_{i1}, \\hat{\\lambda}_{i2}\\), con \\(i=1, \\dots, p\\), che corrispondono alle coordinate di \\(p\\) punti (equivalenti al numero di variabili manifeste). Per ottimizzare la rappresentazione, gli assi vengono ruotati di un angolo \\(\\phi\\) per avvicinarli il più possibile alla disposizione dei punti sul grafico. Le nuove coordinate \\((\\hat{\\lambda}_{i1}^*, \\hat{\\lambda}_{i2}^*)\\) sono calcolate mediante la trasformazione \\(\\hat{\\boldsymbol{\\Lambda}}^* = \\hat{\\boldsymbol{\\Lambda}} \\textbf{T}\\), dove\n\\[\n\\textbf{T} =\n\\begin{bmatrix}\n\\cos{\\phi} & -\\sin{\\phi}\\\\\n\\sin{\\phi} & \\cos{\\phi}\n\\end{bmatrix}\n\\]\nè una matrice ortogonale \\(2 \\times 2\\).\nEsempio: Consideriamo un caso studiato da Brown, Williams e Barlow (1984), analizzato in {cite:t}rencher10methods. A una ragazza di dodici anni è stato chiesto di valutare sette suoi conoscenti su cinque attributi: gentilezza, intelligenza, felicità, simpatia e giustizia. Per queste variabili, la matrice di correlazione \\(R\\) è stata analizzata per estrarre due fattori mediante il metodo delle componenti principali, senza rotazione iniziale:\n\nR &lt;- matrix(\n  c(\n    1.00, .296, .881, .995, .545,\n    .296, 1.000, -.022, .326, .837,\n    .881, -.022, 1.000, .867, .130,\n    .995, .326, .867, 1.000, .544,\n    .545, .837, .130, .544, 1.00\n  ),\n  ncol = 5, byrow = TRUE, dimnames = list(\n    c(\"K\", \"I\", \"H\", \"L\", \"J\"), c(\"K\", \"I\", \"H\", \"L\", \"J\")\n  )\n)\n\nprint(R)\n\n      K      I      H     L     J\nK 1.000  0.296  0.881 0.995 0.545\nI 0.296  1.000 -0.022 0.326 0.837\nH 0.881 -0.022  1.000 0.867 0.130\nL 0.995  0.326  0.867 1.000 0.544\nJ 0.545  0.837  0.130 0.544 1.000\n\n\nDalla matrice \\(R\\), estraiamo due fattori. Si osserva che i fattori risultano difficili da interpretare: il primo fattore mostra alte saturazioni positive su tutte le variabili manifeste, mentre il secondo fattore si caratterizza per alte saturazioni positive su una variabile e negative sulle altre.\n\nf.pc &lt;- principal(R, 2, rotate = FALSE) \nf.pc\n\nSpecified rotation not found, rotate='none' used\n\n\n\nPrincipal Components Analysis\nCall: principal(r = R, nfactors = 2, rotate = FALSE)\nStandardized loadings (pattern matrix) based upon correlation matrix\n   PC1   PC2   h2     u2 com\nK 0.97 -0.23 0.99 0.0067 1.1\nI 0.52  0.81 0.92 0.0792 1.7\nH 0.78 -0.59 0.96 0.0391 1.9\nL 0.97 -0.21 0.99 0.0135 1.1\nJ 0.70  0.67 0.94 0.0597 2.0\n\n                       PC1  PC2\nSS loadings           3.26 1.54\nProportion Var        0.65 0.31\nCumulative Var        0.65 0.96\nProportion Explained  0.68 0.32\nCumulative Proportion 0.68 1.00\n\nMean item complexity =  1.6\nTest of the hypothesis that 2 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.03 \n\nFit based upon off diagonal values = 1\n\n\nIn un grafico delle saturazioni fattoriali, i punti rappresentano le cinque coppie di saturazioni (una per ciascun fattore):\n\nplot(\n  f.pc$load[, 1], f.pc$load[, 2],\n  bty = \"n\", xaxt = \"n\",\n  xlab = \"Primo Fattore\", ylab = \"Secondo Fattore\",\n  ylim = c(-.6, 1), xlim = c(0, 1), pch = 19, asp = 1\n)\naxis(1, pos = c(0, 0))\nabline(0, 0)\n\n\n\n\n\n\n\n\nRencher (2002) suggerisce che una rotazione ortogonale di \\(-35^\\circ\\) avvicinerebbe efficacemente gli assi ai punti nel diagramma di dispersione. Per verificarlo, si può disegnare i nuovi assi nel grafico dopo una rotazione di \\(-35^\\circ\\).\n\nplot(\n  f.pc$load[, 1], f.pc$load[, 2],\n  bty = \"n\", xaxt = \"n\",\n  xlab = \"Primo Fattore\", ylab = \"Secondo Fattore\",\n  ylim = c(-.6, 1), xlim = c(0, 1), pch = 19, asp = 1\n)\naxis(1, pos = c(0, 0))\nabline(0, 0)\n\nar &lt;- matrix(c(\n  0, 0,\n  0, 1,\n  0, 0,\n  1, 0\n), ncol = 2, byrow = TRUE)\n\nangle &lt;- 35\nrad &lt;- angle * pi / 180\nT &lt;- matrix(c(\n  cos(rad), -sin(rad),\n  sin(rad),  cos(rad)\n), ncol = 2, byrow = TRUE)\n\nround(ar %*% T, 3)\n\narrows(0, 0, 0.574, 0.819, lwd = 2)\narrows(0, 0, 0.819, -0.574, lwd = 2)\n\n\nA matrix: 4 x 2 of type dbl\n\n\n0.000\n0.000\n\n\n0.574\n0.819\n\n\n0.000\n0.000\n\n\n0.819\n-0.574\n\n\n\n\n\n\n\n\n\n\n\n\nNella figura, le due frecce rappresentano gli assi ruotati. La rotazione di \\(-35^{\\circ}\\) ha effettivamente avvicinato gli assi ai punti del diagramma. Se usiamo dunque il valore \\(\\phi = -35^{\\circ}\\) nella matrice di rotazione, possiamo calcolare le saturazioni fattoriali della soluzione ruotata \\(\\hat{\\boldsymbol{\\Lambda}}^* = \\hat{\\boldsymbol{\\Lambda}} \\textbf{T}\\).\nLe saturazioni fattoriali ruotate corrispondono alla proiezione ortogonale dei punti sugli assi ruotati:\n\nangle &lt;- -35\nrad &lt;- angle * pi / 180\nT &lt;- matrix(c(\n  cos(rad), -sin(rad),\n  sin(rad),  cos(rad)\n), ncol = 2, byrow = TRUE)\nround(f.pc$load %*% T, 3)\n\n\nA matrix: 5 x 2 of type dbl\n\n\nK\n0.927\n0.367\n\n\nI\n-0.037\n0.959\n\n\nH\n0.980\n-0.031\n\n\nL\n0.916\n0.385\n\n\nJ\n0.194\n0.950\n\n\n\n\n\nLa soluzione ottenuta in questo modo riproduce quanto riportato da {cite:t}rencher10methods.\n\n\n30.3.4 Medodi di rotazione ortogonale\nUn tipo di rotazione ortogonale spesso utilizzata è la rotazione Varimax (Kaiser, 1958). La matrice \\(\\hat{\\boldsymbol{\\Lambda}}\\) è semplificata in modo tale che le varianze dei quadrati degli elementi \\(\\lambda_{ij}\\) appartenenti a colonne diverse di \\(\\hat{\\boldsymbol{\\Lambda}}\\) siano massime. Se le saturazioni fattoriali in una colonna di \\(\\hat{\\boldsymbol{\\Lambda}}\\) sono simili tra loro, la varianza sarà prossima a zero. Tale varianza è tanto più grande quanto più i quadrati degli elementi \\(\\lambda_{ij}\\) assumono valori prossimi a \\(0\\) e \\(1\\). Amplificando le correlazioni più alte e riducendo quelle più basse, la rotazione Varimax agevola l’interpretazione di ciascun fattore.\nUsando la funzione factanal() del modulo R base, la rotazione Varimax può essere applicata alla soluzione ottenuta mediante il metodo di massima verosimiglianza. Usando le funzioni principal() e factor.pa() disponibili nel pacchetto psych, la rotazione Varimax può essere applicata alle soluzioni ottenute mediante il metodo delle componenti principali e il metodo del fattore principale.\nAd esempio, usando il metodo delle componenti principali otteniamo:\n\nf_pc &lt;- principal(R, 2, n.obs = 7, rotate = \"varimax\")\nf_pc\n\nPrincipal Components Analysis\nCall: principal(r = R, nfactors = 2, rotate = \"varimax\", n.obs = 7)\nStandardized loadings (pattern matrix) based upon correlation matrix\n   RC1   RC2   h2     u2 com\nK 0.95  0.30 0.99 0.0067 1.2\nI 0.03  0.96 0.92 0.0792 1.0\nH 0.97 -0.10 0.96 0.0391 1.0\nL 0.94  0.32 0.99 0.0135 1.2\nJ 0.26  0.93 0.94 0.0597 1.2\n\n                       RC1  RC2\nSS loadings           2.81 1.99\nProportion Var        0.56 0.40\nCumulative Var        0.56 0.96\nProportion Explained  0.58 0.42\nCumulative Proportion 0.58 1.00\n\nMean item complexity =  1.1\nTest of the hypothesis that 2 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.03 \n with the empirical chi square  0.12  with prob &lt;  0.73 \n\nFit based upon off diagonal values = 1\n\n\nUn altro metodo di rotazione ortogonale è il metodo Quartimax (Neuhaus e Wringley, 1954), il quale opera una semplificazione della matrice \\(\\hat{\\boldsymbol{\\Lambda}}\\) massimizzando le covarianze tra i quadrati degli elementi \\(\\lambda_{ij}\\) appartenenti a righe diverse, subordinatamente alla condizione che la varianza delle righe rimanga inalterata.\n\n\n30.3.5 Metodi di Rotazione Obliqua\nIl termine “rotazione obliqua” può sembrare inappropriato, in quanto la rotazione implica generalmente una trasformazione ortogonale che preserva le distanze. Tuttavia, come evidenziato da {cite:t}rencher10methods, un’espressione più corretta potrebbe essere “trasformazione obliqua”. Nonostante ciò, l’uso comune ha consolidato il termine “rotazione obliqua”.\nNel contesto della rotazione obliqua, gli assi della soluzione ruotata non sono costretti a rimanere ortogonali tra loro, permettendo così un allineamento più diretto agli agglomerati di punti nello spazio delle saturazioni fattoriali. Questo tipo di trasformazione facilita l’interpretazione dei fattori in presenza di correlazioni tra di essi.\nEsistono diversi approcci analitici per realizzare una rotazione obliqua. Ad esempio, il metodo Direct Oblimin, sviluppato da Jennrich e Sampson nel 1966, utilizza il seguente criterio:\n\\[\n\\sum_{ij} \\left(\\sum_v \\lambda_i^2 \\lambda_j^2 - w \\frac{1}{p} \\sum_v \\lambda_i^2 \\sum_v \\lambda_j^2\\right)\n\\]\nQui, \\(\\sum_{ij}\\) rappresenta la somma su tutte le coppie di fattori \\(ij\\). Il processo prevede una minimizzazione, al contrario della massimizzazione tipica delle rotazioni ortogonali, riflettendo la ricerca di una soluzione che minimizzi la correlazione ridondante tra i fattori, mantenendo al contempo chiarezza interpretativa.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>La rotazione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/04_rotazione.html#matrice-dei-pesi-fattoriali-e-matrice-di-struttura",
    "href": "chapters/extraction/04_rotazione.html#matrice-dei-pesi-fattoriali-e-matrice-di-struttura",
    "title": "30  La rotazione fattoriale",
    "section": "30.4 Matrice dei Pesi Fattoriali e Matrice di Struttura",
    "text": "30.4 Matrice dei Pesi Fattoriali e Matrice di Struttura\n\n30.4.1 Rotazione Ortogonale\nNel contesto della rotazione ortogonale, i fattori rimangono incorrelati tra loro. Consideriamo il caso di due fattori latenti non correlati (\\(\\xi_1\\) e \\(\\xi_2\\)) e quattro variabili manifeste (\\(y_1, y_2, y_3, y_4\\)). I coefficienti \\(\\lambda_{11}, \\lambda_{12}, \\lambda_{13}, \\lambda_{14}\\) rappresentano le saturazioni fattoriali delle variabili nel primo fattore, mentre \\(\\lambda_{21}, \\lambda_{22}, \\lambda_{23}, \\lambda_{24}\\) sono quelle nel secondo fattore. In un modello di percorso, la correlazione tra due variabili è calcolata come la somma di tutti i percorsi validi che le collegano. Se i fattori comuni sono incorrelati, esiste un solo percorso valido che collega ciascuna variabile manifesta a ciascun fattore comune secondo le regole di Wright. Pertanto, le correlazioni tra variabili manifeste e fattori comuni sono direttamente uguali alle saturazioni fattoriali. Queste saturazioni possono essere interpretate come i pesi beta di un modello di regressione multipla, indicando il contributo specifico di ciascun fattore comune nella varianza spiegata degli item (Tabachnick & Fidell, 2001).\n\n\n\n\n\n\nFigura 30.1: Rotazione ortogonale.\n\n\n\n\n\n30.4.2 Rotazione Obliqua\nNel caso della rotazione obliqua, i fattori comuni risultano correlati tra loro, rendendo la soluzione fattoriale più complessa. Pertanto, la matrice delle saturazioni fattoriali non riflette più direttamente le correlazioni tra variabili e fattori. Un modello di percorso in questa configurazione include almeno due percorsi validi che collegano ciascuna variabile manifesta a ciascun fattore comune. È necessario distinguere tra tre matrici diverse:\n\nMatrice Pattern (\\(\\hat{\\boldsymbol{\\Lambda}}\\)): Conosciuta anche come matrice dei modelli, questa matrice rappresenta i coefficienti di regressione parziali delle variabili sulle dimensioni fattoriali, escludendo l’influenza degli altri fattori.\nMatrice di Struttura: Rappresenta le correlazioni complessive tra le variabili manifeste e i fattori, considerando sia gli effetti diretti che quelli indiretti dei fattori correlati.\nMatrice di Intercorrelazione Fattoriale (\\(\\hat{\\boldsymbol{\\Phi}}\\)): Indica le correlazioni tra i fattori stessi.\n\nIn un modello di percorso con rotazione obliqua, gli assi che rappresentano i fattori non sono ortogonali, il che significa che i fattori sono correlati. Le variabili manifeste sono quindi collegate ai fattori attraverso percorsi che includono effetti diretti e indiretti. Ad esempio, per la variabile \\(y_1\\) e il fattore \\(\\xi_1\\), i percorsi includono una freccia causale \\(\\lambda_{11}\\) per l’effetto diretto e un percorso indiretto rappresentato dal prodotto \\(\\lambda_{21}\\phi_{12}\\). L’analisi dei percorsi dimostra che la correlazione tra \\(\\xi_1\\) e \\(y_1\\) è la somma dei valori numerici di questi percorsi validi, ovvero \\(\\lambda_{11} + \\lambda_{21} \\phi_{12}\\).\n\n\n\n\n\n\nFigura 30.2: Rotazione obliqua.\n\n\n\nPer illustrare la rotazione obliqua, utilizziamo i dati discussi da {cite:t}rencher10methods. Si consideri la matrice di correlazione presentata qui sotto.\n\nR &lt;- matrix(\n  c(\n    1.00,  0.735, 0.711, 0.704,\n    0.735, 1.00,  0.693, 0.709,\n    0.711, 0.693, 1.00,  0.839,\n    0.704, 0.709, 0.839, 1.00\n  ),\n  ncol = 4,\n  byrow = TRUE\n)\nR\n\n\nA matrix: 4 x 4 of type dbl\n\n\n1.000\n0.735\n0.711\n0.704\n\n\n0.735\n1.000\n0.693\n0.709\n\n\n0.711\n0.693\n1.000\n0.839\n\n\n0.704\n0.709\n0.839\n1.000\n\n\n\n\n\nIniziamo calcolando la soluzione a due fattori mediante il metodo delle componenti principali e una rotazione Varimax (ovvero, ortogonale). Otteniamo le seguenti saturazioni fattoriali.\n\nf1_pc &lt;- principal(R, 2, rotate = \"varimax\") \nf1_pc\n\nPrincipal Components Analysis\nCall: principal(r = R, nfactors = 2, rotate = \"varimax\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n   RC1  RC2   h2    u2 com\n1 0.50 0.78 0.86 0.140 1.7\n2 0.47 0.81 0.88 0.124 1.6\n3 0.90 0.33 0.92 0.078 1.3\n4 0.89 0.35 0.92 0.083 1.3\n\n                       RC1  RC2\nSS loadings           2.08 1.50\nProportion Var        0.52 0.37\nCumulative Var        0.52 0.89\nProportion Explained  0.58 0.42\nCumulative Proportion 0.58 1.00\n\nMean item complexity =  1.5\nTest of the hypothesis that 2 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.06 \n\nFit based upon off diagonal values = 0.99\n\n\nSi noti che i due fattori non sono molto distinti. Consideriamo dunque la soluzione prodotta da una rotazione obliqua. Usiamo qui l’algoritmo Oblimin.\n\npr_oblimin &lt;- principal(R, 2, rotate = \"oblimin\")\n\nCaricamento dei namespace richiesti: GPArotation\n\n\n\nLa matrice \\(\\hat{\\boldsymbol{\\Lambda}}\\) delle saturazioni fattoriali si ricava come indicato di seguito.\n\ncbind(pr_oblimin$load[, 1], pr_oblimin$load[, 2])\n\n\nA matrix: 4 x 2 of type dbl\n\n\n0.03206780\n0.90186261\n\n\n-0.02543116\n0.95556536\n\n\n0.96858605\n-0.01096737\n\n\n0.94726778\n0.01327683\n\n\n\n\n\nLa matrice \\(\\hat{\\boldsymbol{\\Phi}}\\) di inter-correlazione fattoriale è la seguente.\n\npr_oblimin$Phi\n\n\nA matrix: 2 x 2 of type dbl\n\n\n\nTC1\nTC2\n\n\n\n\nTC1\n1.0000000\n0.7869776\n\n\nTC2\n0.7869776\n1.0000000\n\n\n\n\n\nLa matrice di struttura, che riporta le correlazioni tra indicatori e fattori comuni, si ottiene pre-moltiplicando la matrice \\(\\boldsymbol{\\Lambda}\\) delle saturazioni fattoriali alla matrice \\(\\boldsymbol{\\Phi}\\) di inter-correlazione fattoriale.\n\\[\n\\text{matrice di struttura} = \\boldsymbol{\\Lambda}\\boldsymbol{\\Phi}.\n\\]\nPer esempio, la correlazione tra la prima variabile manifesta e il primo fattore si ottiene nel modo seguente.\n\npr_oblimin$load[1, 1] + pr_oblimin$load[1, 2] * pr_oblimin$Phi[2, 1]\n\nTC1: 0.741813471502872\n\n\nL’intera matrice di struttura si può trovare eseguendo la moltiplicazione \\(\\boldsymbol{\\Lambda}\\boldsymbol{\\Phi}\\).\n\npr_oblimin$load %*% pr_oblimin$Phi %&gt;% \n  round(3)\n\n\nA matrix: 4 x 2 of type dbl\n\n\nTC1\nTC2\n\n\n\n\n0.742\n0.927\n\n\n0.727\n0.936\n\n\n0.960\n0.751\n\n\n0.958\n0.759",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>La rotazione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/04_rotazione.html#esempio-con-semtools",
    "href": "chapters/extraction/04_rotazione.html#esempio-con-semtools",
    "title": "30  La rotazione fattoriale",
    "section": "30.5 Esempio con semTools",
    "text": "30.5 Esempio con semTools\nPresento qui un esempio di uso di vari metodi di estrazione fattoriale. Tra tali metodi, la rotazione obliqua Geomin è molto popolare ed è il default di M-Plus.\nIniziamo a caricare il pacchetto semTools.\n\nsuppressPackageStartupMessages(library(\"semTools\")) \n\nEseguiamo l’analisi fattoriale esplorativa del classico set di dati di Holzinger e Swineford (1939) il quale è costituito dai punteggi dei test di abilità mentale di bambini di seconda e terza media di due scuole diverse (Pasteur e Grant-White). Nel set di dati originale (disponibile nel pacchetto MBESS), sono forniti i punteggi di 26 test. Tuttavia, un sottoinsieme più piccolo con 9 variabili è più ampiamente utilizzato in letteratura. Questi sono i dati qui usati.\nNel presente esempio, verrà eseguita l’analisi fattoriale esplorativa con l’estrazione di tre fattori. Il metodo di estrazione è mlr:\n\nmaximum likelihood estimation with robust (Huber-White) standard errors and a scaled test statistic that is (asymptotically) equal to the Yuan-Bentler test statistic. For both complete and incomplete data.\n\nLa soluzione iniziale non è ruotata.\n\nunrotated &lt;- efaUnrotate(HolzingerSwineford1939, nf = 3, varList = paste0(\"x\", 1:9), estimator = \"mlr\")\nout &lt;- summary(unrotated)\nprint(out)\n\nlavaan 0.6-18 ended normally after 217 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        36\n\n  Number of observations                           301\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                22.897      23.864\n  Degrees of freedom                                12          12\n  P-value (Chi-square)                           0.029       0.021\n  Scaling correction factor                                  0.959\n    Yuan-Bentler correction (Mplus variant)                       \n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  factor1 =~                                          \n    x1      (l1_1)    0.653    0.083    7.909    0.000\n    x2      (l2_1)    0.353    0.079    4.481    0.000\n    x3      (l3_1)    0.415    0.086    4.832    0.000\n    x4      (l4_1)    0.926    0.067   13.762    0.000\n    x5      (l5_1)    1.014    0.067   15.176    0.000\n    x6      (l6_1)    0.868    0.062   13.887    0.000\n    x7      (l7_1)    0.283    0.091    3.113    0.002\n    x8      (l8_1)    0.340    0.083    4.095    0.000\n    x9      (l9_1)    0.460    0.078    5.881    0.000\n  factor2 =~                                          \n    x1      (l1_2)    0.349    0.124    2.815    0.005\n    x2      (l2_2)    0.242    0.159    1.523    0.128\n    x3      (l3_2)    0.497    0.132    3.767    0.000\n    x4      (l4_2)   -0.337    0.067   -5.058    0.000\n    x5      (l5_2)   -0.461    0.077   -6.009    0.000\n    x6      (l6_2)   -0.280    0.057   -4.908    0.000\n    x7      (l7_2)    0.372    0.188    1.976    0.048\n    x8      (l8_2)    0.510    0.133    3.831    0.000\n    x9      (l9_2)    0.489    0.066    7.416    0.000\n  factor3 =~                                          \n    x1      (l1_3)   -0.338    0.103   -3.275    0.001\n    x2      (l2_3)   -0.405    0.092   -4.401    0.000\n    x3      (l3_3)   -0.404    0.120   -3.355    0.001\n    x4      (l4_3)    0.049    0.098    0.503    0.615\n    x5      (l5_3)    0.122    0.105    1.154    0.248\n    x6      (l6_3)   -0.000    0.076   -0.003    0.998\n    x7      (l7_3)    0.609    0.125    4.863    0.000\n    x8      (l8_3)    0.409    0.143    2.853    0.004\n    x9      (l9_3)    0.112    0.123    0.915    0.360\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  factor1 ~~                                          \n    factor2           0.000                           \n    factor3           0.000                           \n  factor2 ~~                                          \n    factor3           0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    factor1           1.000                           \n    factor2           1.000                           \n    factor3           1.000                           \n   .x1                0.696    0.113    6.184    0.000\n   .x2                1.035    0.106    9.803    0.000\n   .x3                0.692    0.097    7.132    0.000\n   .x4                0.377    0.053    7.170    0.000\n   .x5                0.403    0.064    6.303    0.000\n   .x6                0.365    0.046    7.984    0.000\n   .x7                0.594    0.148    4.014    0.000\n   .x8                0.479    0.099    4.842    0.000\n   .x9                0.551    0.065    8.518    0.000\n\nConstraints:\n                                               |Slack|\n    0-(1_2*1_1+2_2*2_1+3_2*3_1+4_2*4_1+5_2*5_    0.000\n    0-(1_3*1_1+2_3*2_1+3_3*3_1+4_3*4_1+5_3*5_    0.000\n    0-(1_3*1_2+2_3*2_2+3_3*3_2+4_3*4_2+5_3*5_    0.000\n\n\n\nSi noti che, in assenza di rotazione, è impossibile assegnare un significato ai fattori comuni.\n\n30.5.1 Orthogonal varimax\nUtilizziamo ora la rotazione ortogonale Varimax.\n\nout_varimax &lt;- orthRotate(unrotated, method = \"varimax\")\nout &lt;- summary(out_varimax, sort = FALSE, suppress = 0.3)\nprint(out)\n\nStandardized Rotated Factor Loadings\n\n\nWarning message in testLoadings(object):\n\"The standard error is currently invalid because it does not account for the variance of the rotation function. It is simply based on the delta method.\"\n\n\n   factor1 factor2 factor3\nx1  0.320*  0.607*        \nx2          0.481*        \nx3          0.662*        \nx4  0.838*                \nx5  0.867*                \nx6  0.815*                \nx7                  0.695*\nx8                  0.704*\nx9          0.409*  0.511*\n\nFactor Correlation\n        factor1 factor2 factor3\nfactor1       1       0       0\nfactor2       0       1       0\nfactor3       0       0       1\n\nMethod of rotation: varimax \n\nTest Statistics for Standardized Rotated Factor Loadings\n\n\nWarning message in testLoadings(object):\n\"The standard error is currently invalid because it does not account for the variance of the rotation function. It is simply based on the delta method.\"\n\n\n       lhs op rhs std.loading    se      z     p ci.lower ci.upper\n1  factor1 =~  x1       0.320 0.055  5.799 0.000    0.212    0.428\n2  factor1 =~  x2       0.135 0.063  2.151 0.031    0.012    0.259\n3  factor1 =~  x3       0.080 0.049  1.622 0.105   -0.017    0.176\n4  factor1 =~  x4       0.838 0.028 30.193 0.000    0.784    0.892\n5  factor1 =~  x5       0.867 0.024 36.189 0.000    0.820    0.914\n6  factor1 =~  x6       0.815 0.024 33.939 0.000    0.768    0.862\n7  factor1 =~  x7       0.102 0.049  2.058 0.040    0.005    0.199\n8  factor1 =~  x8       0.078 0.048  1.617 0.106   -0.016    0.172\n9  factor1 =~  x9       0.170 0.053  3.222 0.001    0.067    0.273\n10 factor2 =~  x1       0.607 0.075  8.138 0.000    0.461    0.753\n11 factor2 =~  x2       0.481 0.067  7.184 0.000    0.350    0.612\n12 factor2 =~  x3       0.662 0.058 11.499 0.000    0.549    0.775\n13 factor2 =~  x4       0.113 0.043  2.615 0.009    0.028    0.198\n14 factor2 =~  x5       0.032 0.040  0.802 0.422   -0.047    0.111\n15 factor2 =~  x6       0.162 0.042  3.855 0.000    0.079    0.244\n16 factor2 =~  x7      -0.062 0.047 -1.341 0.180   -0.154    0.029\n17 factor2 =~  x8       0.174 0.082  2.117 0.034    0.013    0.336\n18 factor2 =~  x9       0.409 0.079  5.173 0.000    0.254    0.564\n19 factor3 =~  x1       0.130 0.066  1.978 0.048    0.001    0.259\n20 factor3 =~  x2      -0.041 0.071 -0.578 0.563   -0.179    0.098\n21 factor3 =~  x3       0.113 0.049  2.324 0.020    0.018    0.209\n22 factor3 =~  x4       0.077 0.040  1.916 0.055   -0.002    0.155\n23 factor3 =~  x5       0.070 0.042  1.669 0.095   -0.012    0.153\n24 factor3 =~  x6       0.066 0.038  1.715 0.086   -0.009    0.141\n25 factor3 =~  x7       0.695 0.092  7.591 0.000    0.516    0.875\n26 factor3 =~  x8       0.704 0.083  8.520 0.000    0.542    0.865\n27 factor3 =~  x9       0.511 0.065  7.885 0.000    0.384    0.638\n       lhs op rhs std.loading    se      z     p ci.lower ci.upper\n1  factor1 =~  x1       0.320 0.055  5.799 0.000    0.212    0.428\n2  factor1 =~  x2       0.135 0.063  2.151 0.031    0.012    0.259\n3  factor1 =~  x3       0.080 0.049  1.622 0.105   -0.017    0.176\n4  factor1 =~  x4       0.838 0.028 30.193 0.000    0.784    0.892\n5  factor1 =~  x5       0.867 0.024 36.189 0.000    0.820    0.914\n6  factor1 =~  x6       0.815 0.024 33.939 0.000    0.768    0.862\n7  factor1 =~  x7       0.102 0.049  2.058 0.040    0.005    0.199\n8  factor1 =~  x8       0.078 0.048  1.617 0.106   -0.016    0.172\n9  factor1 =~  x9       0.170 0.053  3.222 0.001    0.067    0.273\n10 factor2 =~  x1       0.607 0.075  8.138 0.000    0.461    0.753\n11 factor2 =~  x2       0.481 0.067  7.184 0.000    0.350    0.612\n12 factor2 =~  x3       0.662 0.058 11.499 0.000    0.549    0.775\n13 factor2 =~  x4       0.113 0.043  2.615 0.009    0.028    0.198\n14 factor2 =~  x5       0.032 0.040  0.802 0.422   -0.047    0.111\n15 factor2 =~  x6       0.162 0.042  3.855 0.000    0.079    0.244\n16 factor2 =~  x7      -0.062 0.047 -1.341 0.180   -0.154    0.029\n17 factor2 =~  x8       0.174 0.082  2.117 0.034    0.013    0.336\n18 factor2 =~  x9       0.409 0.079  5.173 0.000    0.254    0.564\n19 factor3 =~  x1       0.130 0.066  1.978 0.048    0.001    0.259\n20 factor3 =~  x2      -0.041 0.071 -0.578 0.563   -0.179    0.098\n21 factor3 =~  x3       0.113 0.049  2.324 0.020    0.018    0.209\n22 factor3 =~  x4       0.077 0.040  1.916 0.055   -0.002    0.155\n23 factor3 =~  x5       0.070 0.042  1.669 0.095   -0.012    0.153\n24 factor3 =~  x6       0.066 0.038  1.715 0.086   -0.009    0.141\n25 factor3 =~  x7       0.695 0.092  7.591 0.000    0.516    0.875\n26 factor3 =~  x8       0.704 0.083  8.520 0.000    0.542    0.865\n27 factor3 =~  x9       0.511 0.065  7.885 0.000    0.384    0.638\n\n\n\n\n30.5.2 Orthogonal Quartimin\nUn metodo alternativo per la rotazione ortogonale è Quartimin.\n\nout_quartimin &lt;- orthRotate(unrotated, method = \"quartimin\")\nout &lt;- summary(out_quartimin, sort = FALSE, suppress = 0.3)\nprint(out)\n\nStandardized Rotated Factor Loadings\n\n\nWarning message in testLoadings(object):\n\"The standard error is currently invalid because it does not account for the variance of the rotation function. It is simply based on the delta method.\"\n\n\n   factor1 factor2 factor3\nx1  0.353*  0.590*        \nx2          0.474*        \nx3          0.657*        \nx4  0.844*                \nx5  0.869*                \nx6  0.823*                \nx7                  0.692*\nx8                  0.702*\nx9          0.397*  0.508*\n\nFactor Correlation\n        factor1 factor2 factor3\nfactor1       1       0       0\nfactor2       0       1       0\nfactor3       0       0       1\n\nMethod of rotation: Quartimin \n\nTest Statistics for Standardized Rotated Factor Loadings\n\n\nWarning message in testLoadings(object):\n\"The standard error is currently invalid because it does not account for the variance of the rotation function. It is simply based on the delta method.\"\n\n\n       lhs op rhs std.loading    se      z     p ci.lower ci.upper\n1  factor1 =~  x1       0.353 0.062  5.720 0.000    0.232    0.473\n2  factor1 =~  x2       0.158 0.066  2.375 0.018    0.028    0.288\n3  factor1 =~  x3       0.115 0.057  2.015 0.044    0.003    0.226\n4  factor1 =~  x4       0.844 0.027 30.814 0.000    0.790    0.898\n5  factor1 =~  x5       0.869 0.023 37.224 0.000    0.823    0.914\n6  factor1 =~  x6       0.823 0.024 35.026 0.000    0.777    0.869\n7  factor1 =~  x7       0.116 0.054  2.161 0.031    0.011    0.222\n8  factor1 =~  x8       0.104 0.054  1.914 0.056   -0.003    0.210\n9  factor1 =~  x9       0.202 0.059  3.403 0.001    0.086    0.319\n10 factor2 =~  x1       0.590 0.078  7.523 0.000    0.436    0.743\n11 factor2 =~  x2       0.474 0.068  6.935 0.000    0.340    0.608\n12 factor2 =~  x3       0.657 0.059 11.204 0.000    0.542    0.771\n13 factor2 =~  x4       0.072 0.041  1.748 0.080   -0.009    0.152\n14 factor2 =~  x5      -0.010 0.039 -0.263 0.793   -0.087    0.066\n15 factor2 =~  x6       0.122 0.040  3.008 0.003    0.042    0.201\n16 factor2 =~  x7      -0.071 0.048 -1.465 0.143   -0.166    0.024\n17 factor2 =~  x8       0.167 0.090  1.849 0.064   -0.010    0.343\n18 factor2 =~  x9       0.397 0.087  4.581 0.000    0.227    0.567\n19 factor3 =~  x1       0.124 0.071  1.740 0.082   -0.016    0.264\n20 factor3 =~  x2      -0.042 0.074 -0.574 0.566   -0.187    0.102\n21 factor3 =~  x3       0.114 0.054  2.093 0.036    0.007    0.221\n22 factor3 =~  x4       0.056 0.037  1.503 0.133   -0.017    0.128\n23 factor3 =~  x5       0.048 0.038  1.263 0.207   -0.027    0.123\n24 factor3 =~  x6       0.046 0.034  1.347 0.178   -0.021    0.112\n25 factor3 =~  x7       0.692 0.093  7.477 0.000    0.511    0.874\n26 factor3 =~  x8       0.702 0.084  8.331 0.000    0.537    0.867\n27 factor3 =~  x9       0.508 0.070  7.298 0.000    0.371    0.644\n       lhs op rhs std.loading    se      z     p ci.lower ci.upper\n1  factor1 =~  x1       0.353 0.062  5.720 0.000    0.232    0.473\n2  factor1 =~  x2       0.158 0.066  2.375 0.018    0.028    0.288\n3  factor1 =~  x3       0.115 0.057  2.015 0.044    0.003    0.226\n4  factor1 =~  x4       0.844 0.027 30.814 0.000    0.790    0.898\n5  factor1 =~  x5       0.869 0.023 37.224 0.000    0.823    0.914\n6  factor1 =~  x6       0.823 0.024 35.026 0.000    0.777    0.869\n7  factor1 =~  x7       0.116 0.054  2.161 0.031    0.011    0.222\n8  factor1 =~  x8       0.104 0.054  1.914 0.056   -0.003    0.210\n9  factor1 =~  x9       0.202 0.059  3.403 0.001    0.086    0.319\n10 factor2 =~  x1       0.590 0.078  7.523 0.000    0.436    0.743\n11 factor2 =~  x2       0.474 0.068  6.935 0.000    0.340    0.608\n12 factor2 =~  x3       0.657 0.059 11.204 0.000    0.542    0.771\n13 factor2 =~  x4       0.072 0.041  1.748 0.080   -0.009    0.152\n14 factor2 =~  x5      -0.010 0.039 -0.263 0.793   -0.087    0.066\n15 factor2 =~  x6       0.122 0.040  3.008 0.003    0.042    0.201\n16 factor2 =~  x7      -0.071 0.048 -1.465 0.143   -0.166    0.024\n17 factor2 =~  x8       0.167 0.090  1.849 0.064   -0.010    0.343\n18 factor2 =~  x9       0.397 0.087  4.581 0.000    0.227    0.567\n19 factor3 =~  x1       0.124 0.071  1.740 0.082   -0.016    0.264\n20 factor3 =~  x2      -0.042 0.074 -0.574 0.566   -0.187    0.102\n21 factor3 =~  x3       0.114 0.054  2.093 0.036    0.007    0.221\n22 factor3 =~  x4       0.056 0.037  1.503 0.133   -0.017    0.128\n23 factor3 =~  x5       0.048 0.038  1.263 0.207   -0.027    0.123\n24 factor3 =~  x6       0.046 0.034  1.347 0.178   -0.021    0.112\n25 factor3 =~  x7       0.692 0.093  7.477 0.000    0.511    0.874\n26 factor3 =~  x8       0.702 0.084  8.331 0.000    0.537    0.867\n27 factor3 =~  x9       0.508 0.070  7.298 0.000    0.371    0.644\n\n\n\n\n30.5.3 Oblique Quartimin\nL’algoritmo Quartimin può anche essere usato per una soluzione obliqua.\n\nout_oblq &lt;- oblqRotate(unrotated, method = \"quartimin\")\nout &lt;- summary(out_oblq, sort = FALSE, suppress = 0.3)\nprint(out)\n\nStandardized Rotated Factor Loadings\n\n\nWarning message in testLoadings(object):\n\"The standard error is currently invalid because it does not account for the variance of the rotation function. It is simply based on the delta method.\"\n\n\n   factor1 factor2 factor3\nx1          0.602*        \nx2          0.505*        \nx3          0.689*        \nx4  0.840*                \nx5  0.888*                \nx6  0.808*                \nx7                  0.723*\nx8                  0.702*\nx9          0.366*  0.463*\n\nFactor Correlation\n          factor1   factor2   factor3\nfactor1 1.0000000 0.3257795 0.2164265\nfactor2 0.3257795 1.0000000 0.2704806\nfactor3 0.2164265 0.2704806 1.0000000\n\nMethod of rotation: Quartimin \n\nTest Statistics for Standardized Rotated Factor Loadings\n\n\nWarning message in testLoadings(object):\n\"The standard error is currently invalid because it does not account for the variance of the rotation function. It is simply based on the delta method.\"\n\n\n       lhs op rhs std.loading    se      z     p ci.lower ci.upper\n1  factor1 =~  x1       0.191 0.064  2.965 0.003    0.065    0.317\n2  factor1 =~  x2       0.044 0.066  0.665 0.506   -0.085    0.172\n3  factor1 =~  x3      -0.070 0.034 -2.031 0.042   -0.137   -0.002\n4  factor1 =~  x4       0.840 0.033 25.622 0.000    0.776    0.905\n5  factor1 =~  x5       0.888 0.027 32.583 0.000    0.835    0.942\n6  factor1 =~  x6       0.808 0.028 28.441 0.000    0.752    0.863\n7  factor1 =~  x7       0.044 0.037  1.179 0.238   -0.029    0.116\n8  factor1 =~  x8      -0.033 0.036 -0.916 0.360   -0.103    0.037\n9  factor1 =~  x9       0.035 0.048  0.728 0.467   -0.059    0.129\n10 factor2 =~  x1       0.602 0.086  7.003 0.000    0.434    0.771\n11 factor2 =~  x2       0.505 0.071  7.163 0.000    0.367    0.644\n12 factor2 =~  x3       0.689 0.056 12.344 0.000    0.580    0.799\n13 factor2 =~  x4       0.022 0.045  0.483 0.629   -0.067    0.110\n14 factor2 =~  x5      -0.067 0.036 -1.890 0.059   -0.137    0.002\n15 factor2 =~  x6       0.078 0.041  1.887 0.059   -0.003    0.158\n16 factor2 =~  x7      -0.152 0.037 -4.059 0.000   -0.225   -0.078\n17 factor2 =~  x8       0.104 0.109  0.960 0.337   -0.109    0.317\n18 factor2 =~  x9       0.366 0.097  3.780 0.000    0.176    0.556\n19 factor3 =~  x1       0.031 0.062  0.500 0.617   -0.090    0.152\n20 factor3 =~  x2      -0.117 0.066 -1.776 0.076   -0.245    0.012\n21 factor3 =~  x3       0.023 0.039  0.587 0.557   -0.054    0.100\n22 factor3 =~  x4       0.005 0.042  0.128 0.898   -0.076    0.087\n23 factor3 =~  x5       0.008 0.035  0.216 0.829   -0.061    0.076\n24 factor3 =~  x6      -0.011 0.030 -0.362 0.717   -0.070    0.048\n25 factor3 =~  x7       0.723 0.087  8.328 0.000    0.553    0.893\n26 factor3 =~  x8       0.702 0.098  7.137 0.000    0.509    0.894\n27 factor3 =~  x9       0.463 0.075  6.211 0.000    0.317    0.609\n       lhs op rhs std.loading    se      z     p ci.lower ci.upper\n1  factor1 =~  x1       0.191 0.064  2.965 0.003    0.065    0.317\n2  factor1 =~  x2       0.044 0.066  0.665 0.506   -0.085    0.172\n3  factor1 =~  x3      -0.070 0.034 -2.031 0.042   -0.137   -0.002\n4  factor1 =~  x4       0.840 0.033 25.622 0.000    0.776    0.905\n5  factor1 =~  x5       0.888 0.027 32.583 0.000    0.835    0.942\n6  factor1 =~  x6       0.808 0.028 28.441 0.000    0.752    0.863\n7  factor1 =~  x7       0.044 0.037  1.179 0.238   -0.029    0.116\n8  factor1 =~  x8      -0.033 0.036 -0.916 0.360   -0.103    0.037\n9  factor1 =~  x9       0.035 0.048  0.728 0.467   -0.059    0.129\n10 factor2 =~  x1       0.602 0.086  7.003 0.000    0.434    0.771\n11 factor2 =~  x2       0.505 0.071  7.163 0.000    0.367    0.644\n12 factor2 =~  x3       0.689 0.056 12.344 0.000    0.580    0.799\n13 factor2 =~  x4       0.022 0.045  0.483 0.629   -0.067    0.110\n14 factor2 =~  x5      -0.067 0.036 -1.890 0.059   -0.137    0.002\n15 factor2 =~  x6       0.078 0.041  1.887 0.059   -0.003    0.158\n16 factor2 =~  x7      -0.152 0.037 -4.059 0.000   -0.225   -0.078\n17 factor2 =~  x8       0.104 0.109  0.960 0.337   -0.109    0.317\n18 factor2 =~  x9       0.366 0.097  3.780 0.000    0.176    0.556\n19 factor3 =~  x1       0.031 0.062  0.500 0.617   -0.090    0.152\n20 factor3 =~  x2      -0.117 0.066 -1.776 0.076   -0.245    0.012\n21 factor3 =~  x3       0.023 0.039  0.587 0.557   -0.054    0.100\n22 factor3 =~  x4       0.005 0.042  0.128 0.898   -0.076    0.087\n23 factor3 =~  x5       0.008 0.035  0.216 0.829   -0.061    0.076\n24 factor3 =~  x6      -0.011 0.030 -0.362 0.717   -0.070    0.048\n25 factor3 =~  x7       0.723 0.087  8.328 0.000    0.553    0.893\n26 factor3 =~  x8       0.702 0.098  7.137 0.000    0.509    0.894\n27 factor3 =~  x9       0.463 0.075  6.211 0.000    0.317    0.609\n\n\n\n\n30.5.4 Orthogonal Geomin\nConsideriamo ora la rotazione Geomin. L’algoritmo Geomin fornisce un metodo di rotazione che riduce al minimo la media geometrica delle saturazioni fattoriali innalzate al quadrato. Qui è usato per ottenere una soluzione ortogonale.\n\nout_geomin_orh &lt;- orthRotate(unrotated, method = \"geomin\")\nout &lt;- summary(out_geomin_orh, sort = FALSE, suppress = 0.3)\nprint(out)\n\nStandardized Rotated Factor Loadings\n\n\nWarning message in testLoadings(object):\n\"The standard error is currently invalid because it does not account for the variance of the rotation function. It is simply based on the delta method.\"\n\n\n   factor1 factor2 factor3\nx1  0.315*         -0.621*\nx2                 -0.474*\nx3                 -0.671*\nx4  0.838*                \nx5  0.867*                \nx6  0.814*                \nx7          0.696*        \nx8          0.677*        \nx9          0.456* -0.468*\n\nFactor Correlation\n        factor1 factor2 factor3\nfactor1       1       0       0\nfactor2       0       1       0\nfactor3       0       0       1\n\nMethod of rotation: Geomin \n\nTest Statistics for Standardized Rotated Factor Loadings\n\n\nWarning message in testLoadings(object):\n\"The standard error is currently invalid because it does not account for the variance of the rotation function. It is simply based on the delta method.\"\n\n\n       lhs op rhs std.loading    se       z     p ci.lower ci.upper\n1  factor1 =~  x1       0.315 0.134   2.352 0.019    0.053    0.578\n2  factor1 =~  x2       0.130 0.118   1.102 0.271   -0.101    0.360\n3  factor1 =~  x3       0.074 0.123   0.599 0.549   -0.168    0.316\n4  factor1 =~  x4       0.838 0.036  23.402 0.000    0.767    0.908\n5  factor1 =~  x5       0.867 0.025  35.190 0.000    0.819    0.915\n6  factor1 =~  x6       0.814 0.040  20.427 0.000    0.736    0.892\n7  factor1 =~  x7       0.112 0.068   1.645 0.100   -0.021    0.245\n8  factor1 =~  x8       0.085 0.063   1.341 0.180   -0.039    0.209\n9  factor1 =~  x9       0.172 0.095   1.803 0.071   -0.015    0.359\n10 factor2 =~  x1       0.053 0.061   0.868 0.386   -0.067    0.173\n11 factor2 =~  x2      -0.099 0.070  -1.409 0.159   -0.237    0.039\n12 factor2 =~  x3       0.033 0.048   0.698 0.485   -0.060    0.127\n13 factor2 =~  x4       0.051 0.046   1.115 0.265   -0.039    0.140\n14 factor2 =~  x5       0.054 0.061   0.878 0.380   -0.066    0.173\n15 factor2 =~  x6       0.035 0.037   0.924 0.355   -0.039    0.108\n16 factor2 =~  x7       0.696 0.090   7.702 0.000    0.519    0.874\n17 factor2 =~  x8       0.677 0.088   7.660 0.000    0.504    0.850\n18 factor2 =~  x9       0.456 0.073   6.261 0.000    0.314    0.599\n19 factor3 =~  x1      -0.621 0.101  -6.148 0.000   -0.818   -0.423\n20 factor3 =~  x2      -0.474 0.074  -6.414 0.000   -0.619   -0.329\n21 factor3 =~  x3      -0.671 0.058 -11.658 0.000   -0.784   -0.558\n22 factor3 =~  x4      -0.129 0.161  -0.800 0.424   -0.445    0.187\n23 factor3 =~  x5      -0.048 0.174  -0.277 0.782   -0.389    0.293\n24 factor3 =~  x6      -0.176 0.164  -1.071 0.284   -0.497    0.146\n25 factor3 =~  x7      -0.021 0.059  -0.354 0.724   -0.137    0.095\n26 factor3 =~  x8      -0.257 0.094  -2.720 0.007   -0.442   -0.072\n27 factor3 =~  x9      -0.468 0.105  -4.472 0.000   -0.673   -0.263\n       lhs op rhs std.loading    se       z     p ci.lower ci.upper\n1  factor1 =~  x1       0.315 0.134   2.352 0.019    0.053    0.578\n2  factor1 =~  x2       0.130 0.118   1.102 0.271   -0.101    0.360\n3  factor1 =~  x3       0.074 0.123   0.599 0.549   -0.168    0.316\n4  factor1 =~  x4       0.838 0.036  23.402 0.000    0.767    0.908\n5  factor1 =~  x5       0.867 0.025  35.190 0.000    0.819    0.915\n6  factor1 =~  x6       0.814 0.040  20.427 0.000    0.736    0.892\n7  factor1 =~  x7       0.112 0.068   1.645 0.100   -0.021    0.245\n8  factor1 =~  x8       0.085 0.063   1.341 0.180   -0.039    0.209\n9  factor1 =~  x9       0.172 0.095   1.803 0.071   -0.015    0.359\n10 factor2 =~  x1       0.053 0.061   0.868 0.386   -0.067    0.173\n11 factor2 =~  x2      -0.099 0.070  -1.409 0.159   -0.237    0.039\n12 factor2 =~  x3       0.033 0.048   0.698 0.485   -0.060    0.127\n13 factor2 =~  x4       0.051 0.046   1.115 0.265   -0.039    0.140\n14 factor2 =~  x5       0.054 0.061   0.878 0.380   -0.066    0.173\n15 factor2 =~  x6       0.035 0.037   0.924 0.355   -0.039    0.108\n16 factor2 =~  x7       0.696 0.090   7.702 0.000    0.519    0.874\n17 factor2 =~  x8       0.677 0.088   7.660 0.000    0.504    0.850\n18 factor2 =~  x9       0.456 0.073   6.261 0.000    0.314    0.599\n19 factor3 =~  x1      -0.621 0.101  -6.148 0.000   -0.818   -0.423\n20 factor3 =~  x2      -0.474 0.074  -6.414 0.000   -0.619   -0.329\n21 factor3 =~  x3      -0.671 0.058 -11.658 0.000   -0.784   -0.558\n22 factor3 =~  x4      -0.129 0.161  -0.800 0.424   -0.445    0.187\n23 factor3 =~  x5      -0.048 0.174  -0.277 0.782   -0.389    0.293\n24 factor3 =~  x6      -0.176 0.164  -1.071 0.284   -0.497    0.146\n25 factor3 =~  x7      -0.021 0.059  -0.354 0.724   -0.137    0.095\n26 factor3 =~  x8      -0.257 0.094  -2.720 0.007   -0.442   -0.072\n27 factor3 =~  x9      -0.468 0.105  -4.472 0.000   -0.673   -0.263\n\n\n\n\n30.5.5 Oblique Geomin\nLa rotazione Geomin può anche essere usata per ottenere una soluzione obliqua.\n\nout_geomin_obl &lt;- oblqRotate(unrotated, method = \"geomin\")\nout &lt;- summary(out_geomin_obl, sort = FALSE, suppress = 0.3)\nprint(out)\n\nStandardized Rotated Factor Loadings\n\n\nWarning message in testLoadings(object):\n\"The standard error is currently invalid because it does not account for the variance of the rotation function. It is simply based on the delta method.\"\n\n\n   factor1 factor2 factor3\nx1                 -0.604*\nx2                 -0.507*\nx3                 -0.691*\nx4  0.839*                \nx5  0.887*                \nx6  0.806*                \nx7          0.726*        \nx8          0.703*        \nx9          0.463* -0.368*\n\nFactor Correlation\n           factor1    factor2    factor3\nfactor1  1.0000000  0.2296081 -0.3271571\nfactor2  0.2296081  1.0000000 -0.2776987\nfactor3 -0.3271571 -0.2776987  1.0000000\n\nMethod of rotation: Geomin \n\nTest Statistics for Standardized Rotated Factor Loadings\n\n\nWarning message in testLoadings(object):\n\"The standard error is currently invalid because it does not account for the variance of the rotation function. It is simply based on the delta method.\"\n\n\n       lhs op rhs std.loading    se       z     p ci.lower ci.upper\n1  factor1 =~  x1       0.188 0.070   2.670 0.008    0.050    0.326\n2  factor1 =~  x2       0.044 0.054   0.806 0.420   -0.063    0.150\n3  factor1 =~  x3      -0.073 0.049  -1.467 0.142   -0.170    0.024\n4  factor1 =~  x4       0.839 0.032  26.467 0.000    0.777    0.901\n5  factor1 =~  x5       0.887 0.029  30.077 0.000    0.829    0.945\n6  factor1 =~  x6       0.806 0.030  26.717 0.000    0.747    0.865\n7  factor1 =~  x7       0.031 0.034   0.915 0.360   -0.036    0.099\n8  factor1 =~  x8      -0.045 0.048  -0.948 0.343   -0.139    0.048\n9  factor1 =~  x9       0.025 0.034   0.747 0.455   -0.041    0.091\n10 factor2 =~  x1       0.029 0.051   0.567 0.571   -0.071    0.129\n11 factor2 =~  x2      -0.119 0.072  -1.664 0.096   -0.260    0.021\n12 factor2 =~  x3       0.020 0.037   0.538 0.591   -0.053    0.092\n13 factor2 =~  x4       0.007 0.043   0.174 0.862   -0.076    0.091\n14 factor2 =~  x5       0.010 0.036   0.285 0.775   -0.060    0.080\n15 factor2 =~  x6      -0.009 0.030  -0.309 0.758   -0.068    0.049\n16 factor2 =~  x7       0.726 0.072  10.065 0.000    0.585    0.868\n17 factor2 =~  x8       0.703 0.118   5.963 0.000    0.472    0.934\n18 factor2 =~  x9       0.463 0.080   5.810 0.000    0.307    0.619\n19 factor3 =~  x1      -0.604 0.081  -7.438 0.000   -0.763   -0.445\n20 factor3 =~  x2      -0.507 0.073  -6.983 0.000   -0.649   -0.364\n21 factor3 =~  x3      -0.691 0.061 -11.373 0.000   -0.810   -0.572\n22 factor3 =~  x4      -0.024 0.034  -0.702 0.482   -0.091    0.043\n23 factor3 =~  x5       0.065 0.045   1.459 0.144   -0.022    0.153\n24 factor3 =~  x6      -0.080 0.048  -1.679 0.093   -0.173    0.013\n25 factor3 =~  x7       0.150 0.107   1.403 0.161   -0.060    0.360\n26 factor3 =~  x8      -0.106 0.164  -0.645 0.519   -0.428    0.216\n27 factor3 =~  x9      -0.368 0.133  -2.770 0.006   -0.629   -0.108\n       lhs op rhs std.loading    se       z     p ci.lower ci.upper\n1  factor1 =~  x1       0.188 0.070   2.670 0.008    0.050    0.326\n2  factor1 =~  x2       0.044 0.054   0.806 0.420   -0.063    0.150\n3  factor1 =~  x3      -0.073 0.049  -1.467 0.142   -0.170    0.024\n4  factor1 =~  x4       0.839 0.032  26.467 0.000    0.777    0.901\n5  factor1 =~  x5       0.887 0.029  30.077 0.000    0.829    0.945\n6  factor1 =~  x6       0.806 0.030  26.717 0.000    0.747    0.865\n7  factor1 =~  x7       0.031 0.034   0.915 0.360   -0.036    0.099\n8  factor1 =~  x8      -0.045 0.048  -0.948 0.343   -0.139    0.048\n9  factor1 =~  x9       0.025 0.034   0.747 0.455   -0.041    0.091\n10 factor2 =~  x1       0.029 0.051   0.567 0.571   -0.071    0.129\n11 factor2 =~  x2      -0.119 0.072  -1.664 0.096   -0.260    0.021\n12 factor2 =~  x3       0.020 0.037   0.538 0.591   -0.053    0.092\n13 factor2 =~  x4       0.007 0.043   0.174 0.862   -0.076    0.091\n14 factor2 =~  x5       0.010 0.036   0.285 0.775   -0.060    0.080\n15 factor2 =~  x6      -0.009 0.030  -0.309 0.758   -0.068    0.049\n16 factor2 =~  x7       0.726 0.072  10.065 0.000    0.585    0.868\n17 factor2 =~  x8       0.703 0.118   5.963 0.000    0.472    0.934\n18 factor2 =~  x9       0.463 0.080   5.810 0.000    0.307    0.619\n19 factor3 =~  x1      -0.604 0.081  -7.438 0.000   -0.763   -0.445\n20 factor3 =~  x2      -0.507 0.073  -6.983 0.000   -0.649   -0.364\n21 factor3 =~  x3      -0.691 0.061 -11.373 0.000   -0.810   -0.572\n22 factor3 =~  x4      -0.024 0.034  -0.702 0.482   -0.091    0.043\n23 factor3 =~  x5       0.065 0.045   1.459 0.144   -0.022    0.153\n24 factor3 =~  x6      -0.080 0.048  -1.679 0.093   -0.173    0.013\n25 factor3 =~  x7       0.150 0.107   1.403 0.161   -0.060    0.360\n26 factor3 =~  x8      -0.106 0.164  -0.645 0.519   -0.428    0.216\n27 factor3 =~  x9      -0.368 0.133  -2.770 0.006   -0.629   -0.108",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>La rotazione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/04_rotazione.html#interpretazione",
    "href": "chapters/extraction/04_rotazione.html#interpretazione",
    "title": "30  La rotazione fattoriale",
    "section": "30.6 Interpretazione",
    "text": "30.6 Interpretazione\nPer interpretare i fattori comuni latenti, dobbiamo decidere se usare la matrice pattern o la matrice struttura. Un fattore individuato dall’analisi fattoriale è una caratteristica latente univariata che rappresenta l’essenza di un fenomeno psicologico. Dovrebbe essere interpretato come il significato semplice che si trova dietro l’intersezione dei significati delle variabili che saturano nel fattore.\nNella rotazione obliqua, i fattori sono correlati ma vogliamo comunque interpretarli come dimensioni psicologiche distinte. L’etichetta che diamo al fattore \\(F_1\\) dovrebbe aiutare a separare teoricamente il fenomeno psicologico corrispondente a \\(F_1\\) dal fenomeno denotato dall’etichetta del fattore \\(F_2\\), anche se sono correlati.\nSe questa è la strategia interpretativa, allora lo strumento principale per l’interpretazione è la matrice pattern. I coefficienti della matrice pattern mostrano l’influenza causale del fattore comune sulle variabili manifeste. La matrice struttura descrive le correlazioni tra variabili e fattori e dipende sia dai percorsi diretti che indiretti. Quindi, non descrive gli effetti diretti dei fattori comuni latenti sulle variabili manifeste ma solo la covariazione tra di loro.\n\n\n\n\nRencher, AC. 2002. Methods of multivariate analysis. 2002. Wiley Publications.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>La rotazione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/05_val_soluzione.html",
    "href": "chapters/extraction/05_val_soluzione.html",
    "title": "31  Valutare e rifinire la soluzione fattoriale",
    "section": "",
    "text": "31.1 Valutazione della matrice pattern\nLa maggior parte di strumenti usati nell’assessment psicologico e neuropsicologico non valuta una singola dimensione psicologica, ma piuttosto misura molteplici aspetti di un costrutto. Di conseguenza, l’analisi fattoriale produce solitamente una soluzione a più fattori. Idealmente, dopo la rotazione, ciascun item saturerà fortemente su un singolo fattore e debolmente sugli altri. In realtà, anche dopo la rotazione degli assi fattoriali, spesso si presentano item che saturano debolmente su tutti i fattori, oppure item che saturano fortemente su più di un fattore.\nUno dei primi passi da compiere per rifinire la soluzione fattoriale è quello di valutare la matrice struttura e intervenire utilizzando il criterio della “struttura semplice”, per poi valutare gli effetti delle azioni intraprese (es., eliminazione di alcuni item) nella matrice pattern. Ricordiamo che la matrice struttura contiene le correlazioni tra item e fattori, mentre la matrice pattern contiene le saturazioni fattoriali.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Valutare e rifinire la soluzione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/05_val_soluzione.html#valutazione-della-matrice-pattern",
    "href": "chapters/extraction/05_val_soluzione.html#valutazione-della-matrice-pattern",
    "title": "31  Valutare e rifinire la soluzione fattoriale",
    "section": "",
    "text": "31.1.1 Item con basse saturazioni su tutti i fattori\nPrima di procedere con l’analisi fattoriale è auspicabile esaminare la matrice di correlazioni tra gli item ed eliminare quegli item che sono insufficientemente correlati con gli altri item della matrice. Tuttavia, anche dopo questo screening iniziale, è possibile che vi siano item caratterizzati da saturazioni basse su tutti i fattori. Dal punto di vista pratico, si considerano “basse” le saturazioni il cui valore assoluto è minore di 0.30 (Hair et al., 1995). Hair e collaboratori suggeriscono due soluzioni nel caso di item con saturazioni basse su tutti i fattori:\n\neliminare gli item con basse saturazioni,\nvalutare le comunalità degli item problematici e il contributo specifico che forniscono allo strumento.\n\nSe un item ha una bassa comunalità, o se il contributo di un item nei confronti del significato generale dello strumento è di poca importanza, allora l’item dovrebbe essere eliminato. Dopo l’eliminazione degli item critici, si procede calcolando una nuova soluzione fattoriale e si esaminano i risultati ottenuti.\nSe vi sono degli item con basse saturazioni su tutti i fattori che però contribuiscono in maniera importante a determinare il significato della scala nel suo complesso, allora questi item dovrebbero essere mantenuti. Alle volte, per tali item è possibile creare delle sottoscale separate dalle altre.\n\n\n31.1.2 Item con saturazioni evevate su più di un fattore\nÈ comune anche il caso opposto, ovvero quello nel quale ci sono item che saturano su fattori multipli (con saturazioni fattoriali \\(&gt;\\) .30), specialmente nel caso di soluzioni fattoriali ottenutie dopo una rotazione obliqua. Kline (2000) suggerisce di eliminare tali item in quanto rendono difficile da interpretare il significato della scala che così si ottiene. Hair e collaboratori (1995) ritengono invece che tali item dovrebbero essere mantenuti, dato possono chiarire il significato dei fattori che la scala identifica.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Valutare e rifinire la soluzione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/05_val_soluzione.html#valutazione-dellattendibilità",
    "href": "chapters/extraction/05_val_soluzione.html#valutazione-dellattendibilità",
    "title": "31  Valutare e rifinire la soluzione fattoriale",
    "section": "31.2 Valutazione dell’attendibilità",
    "text": "31.2 Valutazione dell’attendibilità\nAll’interno del problema della costruzione di uno strumento vengono esaminati tre aspetti dell’attendibilità: la consistenza interna, la stabilità e l’equivalenza.\n\n31.2.1 Consistenza interna\n\n31.2.1.1 La procedura split-half\nLa consistenza interna misura il grado di coerenza tra gli item che costituiscono lo strumento o le sottoscale dello strumento. Se tutti gli item che costituiscono uno strumento o una sua sottoscala misurano la stessa cosa, allora saranno fortemente associati tra loro.\nÈ possibile misurare la consistenza interna con il metodo dello split-half, ovvero mediante la correlazione di Pearson tra i punteggi ottenuti utilizzando ciascuna delle due metà degli item dello strumento. Usando un software, è meglio trovare la media delle correlazioni inter-item ricavabili a partire da tutte le possibili divisioni a metà dell’insieme di item che costituiscono lo strumento. La correlazione trovata in questo modo viene poi corretta utilizzando la formula “profetica” di Spearman-Brown per tenere in considerazione il fatto che l’attendibilità è stata calcolata utilizzando soltanto metà degli item dello strumento.\nSi noti che la formula di Spearman-Brown è basata sull’assunzione che le due metà dello strumento siano parallele, ovvero che abbiano identici punteggi veri e uguali varianze d’errore (questa assunzione comporta la conseguenza per cui le due metà degli item devono producono punteggi aventi la stessa media e la stessa varianza). Se queste assunzioni molto stringenti non vengono soddisfatte, allora la procedura descritta sopra conduce ad una sovrastima dell’attendibilità quale consisenza interna della scala.\n\n\n31.2.1.2 L’analisi della varianza\nSe tutti gli item di uno strumento o di una sottoscala sono espressione dello stesso costrutto, allora ci dobbiamo aspettare che anche le medie dei punteggi sugli item siano uguali. Come è stato detto sopra, questa è infatti una delle assunzioni delle forme strettamente parallele di un test. È dunque possibile verificare questa assunzione mediante un’ANOVA che sottopone a test l’ipotesi nulla dell’uguaglianza delle medie di gruppi. Nel caso degli item di un test, dato che ciascun soggetto completa tutti gli item che costituiscono lo strumento, è appropriato usare un’ANOVA per misure ripetute che, nella sua declinazione più moderna, corrisponde ad un modello multi-livello (mixed-effect model).\n\n\n31.2.1.3 L’indice \\(\\alpha\\) di Cronbach\nL’indice \\(\\alpha\\) di Cronbach è comunque la misura più utilizzata per valutare l’attendibilità quale consistenza interna di uno strumento. L’\\(\\alpha\\) di Cronbach è stato interpretato come la proporzione di varianza della scala che può essere attribuita al fattore comune (DeVellis, 1991). Può anche essere interpretato come la correlazione stimata tra i punteggi della scala e un’altro strumento della stessa lunghezza tratto dall’universo degli item possibili che costituiscono il dominio del costrutto (Kline, 1986). La radice quadrata del coefficiente \\(\\alpha\\) di Cronbach rappresenta la correlazione stimata tra i punteggi ottenuti tramite lo strumento e i punteggi veri (Nunnally & Bernstein, 1994).\nIn precedenza abbiamo descritto una serie di limiti del coefficiente \\(\\alpha\\) di Cronbach. In generale, molti ricercatori suggeriscono di usare al suo posto l’indice \\(\\omega\\) di McDonald.\n\n\n\n31.2.2 Stabilità temporale\nLa stabilità temporale viene valutata attraverso la procedura di test-retest. La correlazione tra le misure ottenute in due momenti negli stessi rispondenti ci fornisce l’attendibilità di test-retest.\nKline (2000) ha messo in evidenza come l’attendibilità di test-retest sia influenzata da molteplici fattori, tra cui le caratteristiche del campione, la maturità dei rispondenti, i cambiamenti nello stato emozionale, le differenze nelle condizioni di somministrazione del test, la possibilità di ricordare le risposte date in precedenza, la difficoltà degli item, la grandezza del campione e le caratteristiche del costrutto (ad esempio, stato vs. tratto).\nParticolare attenzione deve essere rivolta all’intervallo temporale usato nella procedura di test-retest. Se il periodo di tempo che intercorre tra le due somministrazioni è troppo corto, i risultati possono risultare distorti a causa del fatto che i soggetti si ricordano le risposte date in precedenza. Questo può condurre ad una sovrastima dell’attendibilità test-retest (Pedhazur & Schmelkin, 1991). Un intervallo temporale troppo lungo tra le due somministrazioni ha invece come limite il fatto che, in questo caso, vi è un’alta possibilità che intervengano dei cambiamenti nei rispondenti rispetto al costrutto in esame. Alla luce di queste considerazioni è stato suggerito di utilizzare un intervallo temporale abbastanza breve, ovvero di una o due settimane (Nunnally & Bernstein, 1994; Pedhazur & Schmelkin, 1991). Se è necessario valutare la stabilità temporale nel corso di un lungo arco temporale, Nunnally e Bernstein (1994) suggeriscono di utilizzare un intervallo di sei mesi o maggiore.\n\n\n31.2.3 Equivalenza\nPer cercare di evitare i problemi associati all’attendibilità quale stabilità temporale, alcuni autori si sono posti il problema di esaminare la correlazione tra forme parallele (o equivalenti) dello strumento. La correlazione tra forme parallele di uno strumento va sotto il nome di coefficiente di equivalenza e fornisce una misura alternativa dell’attendibilità dello strumento (Burns & Grove, 2001; Pedhazur & Schmelkin, 1991; Polit & Hungler, 1999).\nNunnally e Bernstein (1994) suggeriscono di confrontare i risultati ottenuti con la somministrazione delle forme parallele lo stesso giorno con quelli ottenuti nel caso di un intervallo temporale di due settimane. Kline (2000) ritiene che l’attendibilità tra due forme parallele debba essere di almeno 0.9 perché, per valori inferiori, sarebbe difficile sostenere che le forme sono veramente parallele.\nÈ tuttavia molto oneroso predisporre due forme parallele di uno strumento. Per questa ragione, il coefficiente di equivalenza viene raramente usato.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Valutare e rifinire la soluzione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/05_val_soluzione.html#selezione-di-un-sottoinsieme-di-item",
    "href": "chapters/extraction/05_val_soluzione.html#selezione-di-un-sottoinsieme-di-item",
    "title": "31  Valutare e rifinire la soluzione fattoriale",
    "section": "31.3 Selezione di un sottoinsieme di item",
    "text": "31.3 Selezione di un sottoinsieme di item\nTipicamente, la costruzione di un test viene realizzata somministrando un grande numero di item per poi selezionare gli item “migliori” che andranno a fare parte del test vero e proprio. Si supponga di somministrare inizialmente \\(m\\) item, quando si desidera che il test finale sia costituito da \\(p &lt; m\\) item. Un modo di affrontare questo problema potrebbe essere quello di calcolare l’attendibilità del test (coefficiente \\(\\omega\\)) per tutti i possibili sottoinsiemi di \\(p\\) item, così da individuare il sottoinsieme migliore. Questo modo di procedere, però, è problematico perché richiede la valutazione di un elevatissimo numero di possibilità. Per esempio, da un insieme iniziale neanche troppo numeroso di 100 item, il numero di sottoinsiemi di 20 item è uguale a\n\\[\n\\binom{100}{20} = 5.36 \\times 10^{20}.\n\\]\nÈ dunque necessario trovare metodi alternativi che evitino una tale esplosione combinatoria. A questo fine, ovvero per procedere alla selezione del sottoinsieme dei “migliori” item, {cite:t}mcdonald2013test suggerisce di calcolare la quantità di informazione di ciascun item. La quantità di informazione di un item è definita come rapporto tra segnale/rumore, in relazione alla scomposizione della varianza dell’item:\n\\[\n\\frac{\\lambda_i^2}{\\psi_{ii}}.\n\\]\n{cite:t}mcdonald2013test mostra come l’omissione di uno o più item produce sempre una riduzione dell’attendibilità del test (ovvero, una riduzione nel valore del coefficiente \\(\\omega\\)). Tuttavia, tale riduzione è tanto più piccola quanto più piccola è la quantità di informazione degli item omessi. Il processo di selezione degli item può dunque essere guidato da un semplice principio: si selezionano gli item aventi la quantità di informazione maggiore. Ovvero, in altre parole, si rimuovono gli item aventi la quantità di informazione più bassa.\nEsempio. Per fare un esempio, consideriamo nuovamente la matrice di varianze e di covarianze della scala SWLS.\n\nvarnames &lt;- c(\"Y1\", \"Y2\", \"Y3\", \"Y4\", \"Y5\")\nSWLS &lt;- matrix(c(\n  2.565, 1.424, 1.481, 1.328, 1.529,\n  1.424, 2.493, 1.267, 1.051, 1.308,\n  1.481, 1.267, 2.462, 1.093, 1.360,\n  1.328, 1.051, 1.093, 2.769, 1.128,\n  1.529, 1.308, 1.360, 1.128, 3.355\n),\nncol = 5, byrow = TRUE,\ndimnames = list(varnames, varnames)\n)\nSWLS\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nY1\nY2\nY3\nY4\nY5\n\n\n\n\nY1\n2.565\n1.424\n1.481\n1.328\n1.529\n\n\nY2\n1.424\n2.493\n1.267\n1.051\n1.308\n\n\nY3\n1.481\n1.267\n2.462\n1.093\n1.360\n\n\nY4\n1.328\n1.051\n1.093\n2.769\n1.128\n\n\nY5\n1.529\n1.308\n1.360\n1.128\n3.355\n\n\n\n\n\nUtilizzando la funzione cfa() contenuta nel pacchetto lavaan, il modello ad un fattore viene definito nel modo seguente.\n\nmod_1 &lt;- \"\n  F =~ Y1 + Y2 + Y3 + Y4 + Y5\n\"\n\nOtteniamo così una stima dei pesi fattoriali e delle unicità.\n\nfit &lt;- lavaan::cfa(\n  mod_1,\n  sample.cov = SWLS,\n  sample.nobs = 215,\n  std.lv = TRUE\n)\n\nCalcoliamo la quantità di informazione fornita da ciascun item. Iniziamo a estrarre dall’oggetto fit la matrice delle saturazioni fattoriali.\n\nlambda &lt;- inspect(fit, what=\"std\")$lambda\nlambda\n\n\nA lavaan.matrix: 5 x 1 of type dbl\n\n\n\nF\n\n\n\n\nY1\n0.8166956\n\n\nY2\n0.6941397\n\n\nY3\n0.7257827\n\n\nY4\n0.5905795\n\n\nY5\n0.6429117\n\n\n\n\n\nEstraiamo da fit le specificità.\n\ntheta &lt;- diag(inspect(fit, what=\"std\")$theta)\ntheta\n\nY10.333008280668232Y20.518170080229262Y30.473239478247548Y40.651215859909304Y50.586664570302436\n\n\nPossiamo ora calcolare quantità di informazione degli item facendo il rapporto tra ciascuna saturazione fattoriale innalzata al quadrato e la corrispondente specificità.\n\nfor (i in 1:5) {\n  print(lambda[i]^2 / theta[i])\n}\n\n      Y1 \n2.002928 \n       Y2 \n0.9298683 \n      Y3 \n1.113095 \n       Y4 \n0.5355891 \n       Y5 \n0.7045515 \n\n\nIl risultato ottenuto indica che il quarto item è il meno informativo e che il quinto item è il secondo meno informativo. Se un solo item deve essere eliminato, dunque, elimineremo il quarto item. Se devono essere eliminati due item, andranno eliminati il quarto e il quinto item.\n\n31.3.1 Attendibilità e numero di item\nDi quanto cambia l’attendibilità di uno strumento se viene variato il numero di item? Una risposta a questa domanda può essere fornita dalla formula profetica di Spearman-Brown. Supponiamo che nella formula di Spearman-Brown,\n\\[\n\\begin{equation}\n  \\rho_p = \\frac{p \\rho_1}{(p-1)\\rho_1 + 1},\n\\end{equation}\n\\](eq-spearman-brown)\n\\(\\rho_1\\) rappresenti l’attendibilità di un test costituito da un certo numero di item. Se poniamo \\(p=2\\), la {eq}eq-spearman-brown ci fornisce una stima dell’attendibilità che si otterrebbe raddoppiando il numero di item nel test. Valori di \\(p\\) minori di \\(1\\), invece, vengono usati per predire la diminuizione dell’attendibilità conseguente ad una diminuzione nel numero degli item del test.\nRicordiamo però che le predizioni della formula di Spearman-Brown sono accurate solo se la forma allungata o accorciata del test è parallela rispetto al test considerato. Per esempio, se ad un test con un coefficiente di attendibilità molto alto vengono aggiunti item aventi una bassa attendibilità, allora l’attendibilità del test allungato sarà minore di quella predetta dalla formula di Spearman-Brown.\nAnche se la formula di Spearman-Brown ha un ruolo centrale nella teoria classica dei test, si tenga conto che non rappresenta l’unico strumento che può essere utilizzato per valutare la relazione tra attendibilità e numero degli item del test. La quantità detta informazione dell’item (item information), formulata dai modelli IRT, consente di predire i cambiamenti nella qualità della misura a seguito dell’aggiunta o della cancellazione di un sottoinsieme di item.\nEsempio. Si consideri la scala SWLS. Chiediamoci come varia l’attendibilità della scala se il numero di item aumenta da 5 a 20. Poniamo che l’attendibilità della scala SWLS costituita da 5 item sia uguale a 0.824. Applicando la formula di Spearman-Brown otteniamo la stima seguente.\n\n(4 * 0.824) / ((4 - 1) * 0.824 + 1)\n\n0.949308755760369\n\n\nEsempio. Possiamo giungere al risultato precedente in un altro modo. Supponiamo che i 15 item aggiuntivi abbiano le stesse saturazioni fattoriali medie (\\(\\bar{\\lambda}\\)) e le stesse varianze specifiche medie (\\(\\bar{\\psi}\\)) rispetto agli item originali. Mediante gli item di cui disponiamo, stimiamo l’attendibilità di un “item medio” nel modo seguente\n\\[\n\\rho_1 = \\frac{\\bar{\\lambda}^2}{\\bar{\\lambda}^2 + \\bar{\\psi}},\n\\]\novvero otteniamo la stima di 0.48:\n\nrho_1 &lt;- mean(lambda)^2 / (mean(lambda)^2 + mean(theta)) \nrho_1\n\n0.484512352433458\n\n\nL’attendibilità predetta di un test costituito da 20 item sarà dunque uguale a\n\n(20 * rho_1) / ((20 - 1) * rho_1 + 1) \n\n0.949490393106468\n\n\nil che replica il risultato ottenuto precedentemente.\nEsempio. Un altro modo ancora per ottenere lo stesso risultato è quello di utilizzare un modello mono-fattoriale per item paralleli.\n\nmod_2 &lt;- \"\n  F =~ a*Y1 + a*Y2 + a*Y3 + a*Y4 + a*Y5\n  Y1 ~~ b*Y1\n  Y2 ~~ b*Y2\n  Y3 ~~ b*Y3\n  Y4 ~~ b*Y4\n  Y5 ~~ b*Y5\n\"\n\nAdattiamo il modello ai dati.\n\nfit2 &lt;- lavaan::cfa(\n  mod_2,\n  sample.cov = SWLS,\n  sample.nobs = 215,\n  std.lv = TRUE\n)\n\nEstraiamo dall’oggetto fit2 le saturazioni fattoriali.\n\nlambda &lt;- inspect(fit2, what=\"std\")$lambda\nlambda\n\n\nA lavaan.matrix: 5 x 1 of type dbl\n\n\n\nF\n\n\n\n\nY1\n0.6893938\n\n\nY2\n0.6893938\n\n\nY3\n0.6893938\n\n\nY4\n0.6893938\n\n\nY5\n0.6893938\n\n\n\n\n\nEstraiamo da fit2 le specificità.\n\ntheta &lt;- diag(inspect(fit2, what=\"std\")$theta)\ntheta\n\nY10.524736147775294Y20.524736147775294Y30.524736147775294Y40.524736147775294Y50.524736147775294\n\n\nCalcoliamo l’attendibilità dell’item “medio” usando \\(\\lambda\\) e \\(\\psi\\) (chiamato theta da lavaan).\n\nrho_1 &lt;- lambda[1]^2 / (lambda[1]^2 + theta[2])\nrho_1 \n\nY2: 0.475263852224706\n\n\nPosso ora applicare la formula di Spearman-Brown.\n\n(20 * rho_1) / ((20 - 1) * rho_1 + 1) \n\nY2: 0.94768340402785\n\n\nIl risultato è praticamente identico a quelli trovati in precedenza.\n\n\n31.3.2 Numero di item e affidabilità\nLa formula di Spearman-Brown può anche essere riarrangiata in maniera tale da consentirci di predire il numero degli item necessari per raggiungere un determinato livello di affidabilità:\n\\[\n\\begin{equation}\np = \\frac{\\rho_p (1-\\rho_1)}{\\rho_1(1-\\rho_p)},\n\\end{equation}\n\\](eq-s-b-inv)\ndove \\(\\rho_1\\) è l’attendibilità stimata di un “item medio,” \\(\\rho_p\\) è il livello desiderato di attendibilità del test allungato e \\(p\\) è il numero di item del test allungato.\nEsempio. L’attendibilità della scala SWLS costituita da 5 item è \\(\\omega = 0.824\\). Quanti item devono essere aggiunti se si vuole raggiungere un livello di attendibilità pari a \\(0.95\\)?\nPonendo \\(\\rho_p = 0.95\\) e \\(\\rho_1= 0.479\\), in base alla {eq}eq-s-b-inv si ottiene che\n\n(.95 * (1 - rho_1)) / (rho_1 * (1 - .95))\n\nY2: 20.9777932006845\n\n\nil test dovrà essere costituito da 21 item.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Valutare e rifinire la soluzione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/05_val_soluzione.html#analisi-degli-item",
    "href": "chapters/extraction/05_val_soluzione.html#analisi-degli-item",
    "title": "31  Valutare e rifinire la soluzione fattoriale",
    "section": "31.4 Analisi degli item",
    "text": "31.4 Analisi degli item\nL’analisi degli item svolge un ruolo importante nello sviluppo e nella revisione dei test psicometrici. L’analisi degli item esamina le risposte fornite ai singoli item del questionario allo scopo di valutare la qualità degli item e del questionario nel suo complesso. Sotto al rubrica di analisi degli item possiamo raggruppare le procedure che possono essere utilizzate per descrivere la difficoltà degli item, le relazioni tra coppie di item, il punteggio totale del test, le relazioni tra gli item e il punteggio totale del test. Tali analisi statistiche vengono usate per la selezione degli item al fine di costruire un questionario omogeneo, attendibile e dotato di validità predittiva.\nLa selezione degli item di un test, però, non può essere svolta in maniera automatica usando soltanto criteri statistici quali quelli elencati sopra. La selezione degli item, invece, deve anche tenere includere considerazioni di ordine teorico basate sulla centralità degli item rispetto alla definizione del costrutto e considerazioni relative agli scopi della misurazione e al modo in cui l’item è stato formulato e costruito. Se alcuni aspetti di un costrutto non vengono rappresentanti da item che soddisfano i criteri statistici descritti sopra, o se c’è un numero insufficiente di item per produrre uno strumento attendibile, allora alcuni item dovranno essere riscritti. Nella riformulazione degli item, risultano utili le intuizioni che si sono guadagnate dalle analisi statistiche degli item che si sono dovuti scartare.\n\n31.4.1 Difficoltà degli item\nUna statistica comune da calcolare durante l’analisi degli item è la proporzione di esaminandi che rispondono correttamente ad ogni item. Questa è nota come difficoltà dell’item, p. La proporzione \\(p_j\\) di partecipanti che rispondono correttamente all’item \\(j\\)-esimo, o proporzione di partecipanti che si dichiarano in accordo con l’affermazione espressa dall’item, se il test non è di prestazione, fornisce una stima del livello di difficoltà \\(\\pi_j\\) dell’item.\nIn realtà, \\(p_j\\) dovrebbe essere chiamato “facilità dell’item” in quanto assume il suo valore maggiore (ovvero \\(1\\)) quando tutti i rispondenti rispondono correttamente all’item e il suo valore minimo (ovvero \\(0\\)) quando le risposte sono tutte sbagliate. Questo valore non va confuso con la difficoltà dell’item nella teoria della risposta agli item o con il valore-\\(p\\) dei test di ipotesi frequentisti.\nI valori \\(p_j\\) giocano un ruolo importante nelle procedure di selezione degli item. La difficoltà degli item deve essere interpretata in riferimento alla probabilità di indovinare la risposta corretta. Si suppone, infatti, che i rispondenti tirino ad indovinare quando non conoscono la risposta alla domanda di un questionario. Nel caso di item dicotomici, per esempio, ci possiamo aspettare un valore \\(p_j\\) pari a \\(0.50\\) sulla base del caso soltanto; nel caso di item a risposta multipla con quattro opzioni di scelta, invece, \\(p_j\\) assume un valore pari a \\(0.25\\) quando i rispondenti tirano ad indovinare.\nSe il test è composto per la maggior parte da item “facili”, allora il test non sarà in grado di discriminare tra rispondenti con diversi livelli di abilità, in quanto quasi tutti i rispondenti saranno in grado di fornire una risposta corretta alla maggioranza degli item. Lo stesso si può dire per un test composto da item “difficili”. Se il test è composto unicamente da item di difficoltà media, non potrà differenziare i rispondenti che hanno un grado di abilità media da quelli con abilità superiori alla media, dato che non ci sono item “difficili”, e neppure da quelli con abilità inferiori alla media, dato che non ci sono item “facili”.\nIn generale, dunque, è buona pratica costruire test composti da item che coprano tutti i livelli di difficoltà. La scelta che viene usualmente fatta è quella di una dispersione moderata e simmetrica del livello di difficoltà attorno ad un valore leggermente superiore al valore che sta a metà tra il livello del caso (\\(1.0\\) diviso per il numero di alternative) e il punteggio pieno (\\(1.0\\)).\nPer item che presentano cinque alternative di risposta, ad esempio, il livello del caso è pari a \\(1.0 / 5 = 0.20\\). Il livello ottimale di difficoltà è uguale a\n\\[\n0.20 + (1.0 - 0.20) / 2 = 0.60.\n\\]\nPer item dicotomici, il livello del caso è \\(1.0 / 2 = 0.50\\) e il livello ottimale di difficoltà è uguale a\n\\[\n0.50 + (1.00 - 0.50) / 2 = 0.75.\n\\]\nIn generale, item con livelli di difficoltà superiore a \\(0.90\\) o inferiore a \\(0.20\\) dovrebbero essere utilizzati con cautela.\nEsempio. Riporto qui sotto le proporzioni di risposte corrette (usando la correzione per il guessing) di 192 studenti di Psicometria nel primo parziale dell’AA 2021/2022. Il test aveva 16 item con 5 alternative di risposta ciascuno. Dunque la difficoltà media ottimale è pari a 0.6.\n\nitem_par_1 &lt;- c(\n  0.54255319, 0.76063830, 0.64361702, 0.65957447, 0.67021277, 0.12234043,\n  0.14361702, 0.18085106, 0.76063830, 0.82978723, 0.81914894, 0.84042553,\n  0.07978723, 0.07978723, 0.76063830, 0.79255319\n)\n\nNel compito, la difficoltà media è risultata essere un po’ inferiore.\n\nmean(item_par_1) %&gt;% \n  round(2)\n\n0.54\n\n\nLa distribuzione dei livelli di difficoltà degli item suggerisce che forse alcuni item “difficili” si sarebbero potuti sostituire con item di difficoltà media.\n\nplot(density(item_par_1))\n\n\n\n\n\n\n\n\n:::\nUn altro esempio riguarda il data set SAPA del pacchetto hemp. Per questi dati possiamo utilizzare la funzione colMeans per calcolare la difficoltà degli item. Poiché abbiamo dei partecipanti che hanno risposte mancanti su alcuni item, dobbiamo passare l’argomento na.rm = TRUE per ignorare i dati mancanti. In caso contrario, la funzione colMeans restituirebbe NA per gli item che hanno almeno un valore mancante. Per rendere più leggibili i valori di difficoltà degli item, arrotondiamo a tre decimali utilizzando la funzione round.\n\nitem_diff &lt;- colMeans(SAPA, na.rm = TRUE)\nround(item_diff, 3)\n\nreason.40.64reason.160.698reason.170.697reason.190.615letter.70.6letter.330.571letter.340.613letter.580.444matrix.450.526matrix.460.55matrix.470.614matrix.550.374rotate.30.194rotate.40.213rotate.60.299rotate.80.185\n\n\nL’output mostra che gli item reason.16 e reason.17 ottengono i livelli di difficoltà più alti, mentre rotate.8 ha il livello di difficoltà più basso. Circa il 70% degli studenti è stato in grado di rispondere correttamente a reason.16 e reason.17, mentre solo il 19% ha risposto correttamente a rotate.8.\n\n\n31.4.2 Correzione per guessing\nAlle volte i valori \\(p_j\\) sono calcolati introducendo una correzione per le risposte fornite casualmente dai soggetti (guessing). Si consideri un test a scelta multipla composto da item aventi ciascuno \\(C\\) alternative di risposta ed una sola risposta corretta. Si supponga che un rispondente risponda correttamente a \\(R\\) item e risponda in maniera sbagliata a \\(W\\) item.\nLa correzione per guessing si ottiene applicando una formula basata sul seguente ragionamento. Se assumiamo che un rispondente si limita a tirare ad indovinare allora, ogni \\(C\\) risposte, ci aspettiamo 1 risposta giusta e \\(C-1\\) risposte sbagliate. Per calcolare il punteggio totale del test in modo da eliminare il numero di risposte corrette ottenute tirando ad indovinare è necessario sottrarre 1 punto per ogni \\(C-1\\) item a cui è stata fornita una risposta corretta. Questo ragionamento conduce alla seguente formula:\n\\[\n\\begin{equation}\nFS = R - \\frac{W}{C - 1},\n\\end{equation}\n\\](eq-guessing)\ncon \\(R\\) = # risposte corrette, \\(W\\) = # risposte sbagliate, \\(C\\) = # alternative di risposta. Per esempio, se \\(C=5\\), allora è necessario sottrarre un punto ogni 4, il che è proprio quello che fa la {eq}eq-guessing.\nLa {eq}eq-guessing produce un punteggio totale corretto per il guessing identico a quello che si otterrebbe assegnando 1 punto a ciascuna risposta corretta e assegnando \\(- \\frac{1}{C-1}\\) punti alle risposte sbagliate; le risposte non date non vengono considerate.\nLa correzione per guessing rappresenta il tentativo di scomporre il numero totale di risposte corrette in due componenti: le risposte corrette dovute alle conoscenze del soggetto, le risposte che risultano corrette come effetto del caso. La stessa formula può anche essere utilizzata per calcolare la difficoltà degli item corretta per il guessing (come è stato fatto nell’esempio del parziale di Psicometria).\n\n\n31.4.3 Discriminatività\nLa discriminatività è una misura di quanto ogni item è in grado di distinguere i soggetti con elevati livelli nel costrutto da quelli con un livello basso. L’indice di discriminatività \\(D\\) per i test di prestazione massima si trova nel modo seguente. Dopo avere calcolato il punteggio totale al test, si dividono i soggetti in due gruppi: soggetti con basso punteggio e soggetti con alto punteggio. Una volta definiti i due gruppi, l’indice di discriminatività \\(D\\) sarà dato da:\n\\[D = P(\\text{alto}) - P(\\text{basso}),\\]\ndove \\(P(\\text{alto}\\) è la proporzione di soggetti che ha risposto correttamente all’item nel gruppo con punteggi alti e \\(P(\\text{basso}\\) è la proporzione di soggetti che ha risposto correttamente all’item nel gruppo con punteggi bassi. Il valore di \\(D\\) può variare da -1 a +1. Nella tabella seguente sono fornite le linee guida per l’interpretazione di questo indice (Ebel, 1965).\n\n\n\nValore di \\(D\\)\nCommento\n\n\n\n\n\\(D \\geq 0.40\\)\nOttima, nessuna revisione\n\n\n\\(0.30 \\leq D &lt; 0.40\\)\nBuona, revisioni minime\n\n\n\\(0.20 \\leq D &lt; 0.30\\)\nSufficiente, revisioni parziali\n\n\n\\(D &lt; 0.20\\)\nInsufficiente, riformulazione o eliminazione\n\n\n\nLa discriminatività degli item di tipo Likert viene valutata con la medesima procedura degli item dei testi di prestazione massima, anche se cambiano le procedure statistiche da utilizzare. Si può dividere la distribuzione dei punteggi totali (o punteggi medi) in quartili e confrontare il punteggio medio o mediano del quartile superiore con quello del quartile inferiore, oppure, se il test è orientato al criterio e lo scopo è selezionare gli item che discriminano meglio due gruppi precostituiti di soggetto, eseguire i medesimi confronti tra il gruppo target (ad esempio, pazienti) e quello “di controllo” (per esempio, popolazione generale).\nÈ consigliabile valutare la dimensione dell’effetto, ad esempio attraverso l’indice \\(d\\) di Cohen. La dimensione dell’effetto dovrebbe essere almeno moderata (\\(d &gt; |0.50|\\)).\nEsempio. Per il primo parziale di Psicometria AA 2021/2022, l’indice \\(d\\) di Cohen calcolato sulla proporzione di risposte corrette per il gruppo di studenti con i punteggi più bassi (primo quartile) e il gruppo di studenti con i punteggi più alti (ultimo quartile) è stato di 4.76, 95% CI [4.0, 5.51]. L’indice complessivo di discriminatività sembra dunque adeguato. Sarebbe però necessario calcolare questo indice item per item.\n\n\n31.4.4 Potere discriminante dell’item e analisi fattoriale\nUn’altra statistica ampiamente utilizzata nell’analisi degli item è il potere discriminante degli item, che si riferisce alla capacità dell’item nel distinguere gli esaminandi con una alta abilità da quelli con una bassa abilità. Sebbene esistano molti modi per calcolare la discriminazione degli item, la forma più comune è la correlazione punto-biseriale tra le risposte degli esaminandi all’item e il loro punteggio totale nel test. Valori grandi e positivi indicano una forte relazione tra il rispondere correttamente all’item e avere un punteggio alto nel test, mentre valori vicini allo zero indicano nessuna relazione e valori negativi indicano che il rispondere correttamente all’item è associato a un punteggio complessivo del test più basso. Valori vicini allo zero o negativi suggeriscono che l’item potrebbe non funzionare correttamente. Alcune delle ragioni per ottenere una discriminazione degli item bassa o negativa potrebbero essere l’utilizzo di una chiave di risposta errata per l’item o l’assenza di risposte corrette. Indipendentemente dalla causa, gli item con correlazioni punto-biseriale basse o negative devono essere modificati, se il test/strumento è in fase di revisione, o rimossi dal test e dal punteggio.\nPer calcolare il potere discriminante dell’item per i dati SAPA, prima calcoliamo il punteggio totale del test utilizzando la funzione rowSums insieme all’opzione na.rm = TRUE e lo salviamo come total_score. Successivamente, correlaziamo gli item in SAPA con il punteggio totale del test utilizzando la funzione cor. Specificamente, usiamo l’argomento use = \"pairwise.complete.obs\" nella funzione cor a causa della presenza di risposte mancanti. Infine, salviamo la matrice di correlazione come item_discr e la stampiamo.\n\ntotal_score &lt;- rowSums(SAPA, na.rm = TRUE)\nitem_discr &lt;- cor(SAPA, total_score, use = \"pairwise.complete.obs\")\nround(item_discr, 2)\n\n\nA matrix: 16 x 1 of type dbl\n\n\nreason.4\n0.59\n\n\nreason.16\n0.53\n\n\nreason.17\n0.59\n\n\nreason.19\n0.56\n\n\nletter.7\n0.58\n\n\nletter.33\n0.56\n\n\nletter.34\n0.59\n\n\nletter.58\n0.58\n\n\nmatrix.45\n0.51\n\n\nmatrix.46\n0.51\n\n\nmatrix.47\n0.55\n\n\nmatrix.55\n0.45\n\n\nrotate.3\n0.51\n\n\nrotate.4\n0.56\n\n\nrotate.6\n0.55\n\n\nrotate.8\n0.48\n\n\n\n\n\nI risultati mostrano che tutti gli item del test SAPA sono moderatamente e positivamente correlati con il punteggio totale del test. Questo indica che tutti gli item funzionano correttamente e non fornisce informazioni salienti su quali item rimuovere o modificare.\nUn altro modo per calcolare il potere discriminante degli item consiste nel dividere i candidati in due gruppi (ad esempio, 1 = alto rendimento e 0 = basso rendimento) in base ai loro punteggi totali nel test e correlare questa variabile di raggruppamento con le risposte agli item. Questo è noto come indice di discriminazione degli item. Un’opzione per creare gruppi di alto e basso rendimento è selezionare il 25% più alto e il 25% più basso dei candidati in base ai loro punteggi totali nel test. Va notato che la decisione di utilizzare il 25% è arbitraria. Potremmo utilizzare un altro valore (ad esempio, il 10% o il 20%) per definire i gruppi di alto e basso rendimento. Dopo aver definito il punto di cut-off per i gruppi, calcoliamo la proporzione di candidati che hanno risposto correttamente all’elemento nei gruppi di alto e basso rendimento.\nNell’esempio seguente, calcoliamo l’indice di discriminazione dell’elemento reason.4 nel set di dati SAPA utilizzando la funzione idi del pacchetto hemp. Per specificare i gruppi di alto e basso rendimento, utilizziamo il valore perc_cut = .25 nella funzione idi.\n\nidi(SAPA, SAPA$reason.4, perc_cut = .25)\n\nUpper 25%0.805135951661631Lower 25%0.194864048338369\n\n\nAbbiamo scoperto che l’81% dei candidati nel gruppo di alto rendimento ha risposto correttamente all’item reason.4, mentre solo il 19% dei candidati nel gruppo di basso rendimento ha risposto correttamente. Questo suggerisce che l’item era più facile per i candidati di alto rendimento e più difficile per quelli di basso rendimento. Pertanto, possiamo dire che questo particolare item risulta utile per differenziare i due gruppi, ma non necessariamente all’interno di ciascun gruppo.\nSecondo McDondald (1999), la nozione di potere discriminante dell’item può essere trattata in maniera più precisa nell’ambito del modello monofattoriale. Se l’insieme di item a disposizione non è eccessivamente grande (200 o meno), infatti, è possibile procedere alla selezione degli item migliori tramite l’analisi fattoriale – ovvero, scegliendo gli item con le saturazioni maggiori.\n\n\n31.4.5 Punteggio sull’item e punteggio totale\nIl grado di associazione tra il punteggio sull’item e il punteggio totale viene considerato dalla teoria classica dei test come un indice che descrive il potere discriminante dell’item. Se il test fornisce una misura attendibile di un unico attributo, e se un item è fortemente associato al punteggio del test, allora l’item sarà in grado di distinguere tra rispondenti che ottengono un punteggio basso nel test e rispondenti che ottengono un punteggio alto nel test.\nNel caso di una forte associazione positiva tra il punteggio sull’item e il punteggio totale, la probabilità di risposta corretta sull’item è alta per rispondenti che ottengono un punteggio totale alto, e bassa per i rispondenti che ottengono un punteggio totale basso. Nel caso di una debole associazione tra il punteggio sull’item e il punteggio totale, invece, la probabilità di risposta corretta all’item non è predittiva del punteggio totale. Gli item con un basso potere discriminante dovrebbero dunque essere rimossi dal reattivo.\nÈ necessario distinguere i casi in cui gli item sono dicotomici dal caso di item continui. Nel caso di item dicotomici e di un test unidimensionale, il potere discriminante viene calcolato mediante la correlazione biseriale o punto-biseriale.\n\n\n31.4.6 Relazioni tra coppie di item\nLe relazioni tra coppie di item sono importanti sia per la costruzione sia per la validazione dei test psicometrici. La teoria classica dei test definisce l’attendibilità di un test (o di un item) come il rapporto tra la varianza del punteggio vero e la varianza del punteggio osservato. Il coefficiente di attendibilità può però essere calcolato anche trovando la correlazione tra due forme parallele di un test (o tra due item). Inoltre, è possibile interpretare la correlazione tra due forme parallele di un test (o tra due item) come il quadrato del coefficiente di correlazione tra i punteggi osservati e i punteggi veri di un test (o di un item).\nMolti indici sono disponibili per misurare il grado di associazione tra item. Per item quantitativi, possiamo usare la correlazione di Pearson o la covarianza. Per item qualitativi politomici ordinali, usiamo la correlazione policorica. Per item ordinali dicotomici, usiamo la correlazione tetracorica. Per item dicotomici usiamo, ad esempio, l’indice \\(\\phi\\).\n\n\n31.4.7 Ridondanza\nNel processo di raffinamento del test occorre anche tenere conto degli item ridondanti, ossia degli item che sono troppo associati tra loro. La ridondanza può essere valutata con indici statistici quali la correlazione: se due o più item hanno tra loro una correlazione maggiore di \\(|0.70|\\) viene mantenuto nell’item pool solo uno di essi, dato che gli altri item forniscono la stessa informazione.\n\n\n31.4.8 Massimizzazione della varianza del punteggio totale\nUno dei criteri che possono essere utilizzati per la selezionare degli item che andranno a costituire la versione finale di un test è la massimizzazione della varianza del punteggio totale. Più in particolare, si vuole massimizzare il rapporto tra la varianza del punteggio totale e la somma delle varianze dei punteggi dei \\(p\\) item. Dato che il coefficiente \\(\\alpha\\) di Cronbach ha la seguente forma:\n\\[\\alpha = \\frac{p}{p-1}\\left[1- \\frac{\\sum \\sigma^2_{Y_i}}{\\sigma^2_T} \\right],\\]\nla scelta di massimizzare il rapporto definito in precedenza avrà anche la conseguenza di massimizzare \\(\\alpha\\).\n{cite:t}mcdonald2013test fa notare che una procedura di selezione degli item basata sul principio della massimizzazione di \\(\\alpha\\) ha però dei limiti. In primo luogo, tale procedura è appropriata solo quando l’insieme di item è troppo grande per selezionare gli item in base all’esame delle saturazioni fattoriali ottenute applicando il modello mono-fattoriale. In secondo luogo, {cite:t}mcdonald2013test nota che la procedura di selezione basata sulla massimizzazione di \\(\\alpha\\) è adeguata solo nel caso di una struttura mono-fattoriale. La selezione degli item basata sulla massimizzazione di \\(\\alpha\\) deve dunque essere accompagnata da considerazione relative al contenuto e alla struttura del costrutto.\n\n\n31.4.9 Indice di affidabilità dell’item\nOltre agli indici di difficoltà e discriminazione degli item, un’altra statistica utile per l’analisi degli item è l’indice di affidabilità dell’item. L’indice di affidabilità dell’item (IRI) è definito come:\n\\[\nIRI = S_i \\cdot r_{i,tt},\n\\]\ndove \\(S_i\\) è la deviazione standard dell’item \\(i\\) e \\(r_{i,tt}\\) è la correlazione tra l’item \\(i\\) e il punteggio totale del test. L’IRI può teoricamente variare tra -0.5 e 0.5, con valori grandi e positivi indicativi di alta affidabilità.\nDi seguito calcoliamo l’IRI per tutti gli item nel set di dati SAPA. Possiamo farlo utilizzando la funzione iri in hemp.\n\niri(SAPA)\n\n\nA matrix: 16 x 1 of type dbl\n\n\nreason.4\n0.2820989\n\n\nreason.16\n0.2451971\n\n\nreason.17\n0.2692675\n\n\nreason.19\n0.2717135\n\n\nletter.7\n0.2865325\n\n\nletter.33\n0.2757209\n\n\nletter.34\n0.2897118\n\n\nletter.58\n0.2863221\n\n\nmatrix.45\n0.2544930\n\n\nmatrix.46\n0.2562540\n\n\nmatrix.47\n0.2668171\n\n\nmatrix.55\n0.2161230\n\n\nrotate.3\n0.2016459\n\n\nrotate.4\n0.2276081\n\n\nrotate.6\n0.2539219\n\n\nrotate.8\n0.1867207\n\n\n\n\n\nI risultati restituiti dalla funzione iri mostrano che l’IRI varia da circa 0.19 a 0.29 per il set di dati SAPA. Tutti questi sono valori ragionevoli per l’IRI (ovvero nessuno è negativo o vicino allo zero).\n\n\n31.4.10 Indice di validità dell’item\nQuando invece del punteggio totale del test viene utilizzato un criterio esterno, questo indice è noto come indice di validità dell’item (IVI). L’IVI può variare anche tra -0.5 e 0.5, con valori elevati (in valore assoluto) che indicano una validità maggiore. Valori negativi elevati indicano una maggiore validità quando ci si aspetta che gli elementi siano correlati in modo negativo con il criterio.\nNell’esempio seguente, utilizziamo la funzione ivi in hemp con “reason.17” come criterio esterno e “reason.4” come elemento di interesse e troviamo che l’IVI è 0.19.\n\nivi(item = SAPA$reason.4, crit = SAPA$reason.17)\n\n0.190321881267833\n\n\n\n\n31.4.11 Distrattori\nUn altro aspetto importante degli elementi che deve essere analizzato sono le opzioni di risposta. Nel contesto dei test a scelta multipla, le opzioni di risposta alternative (cioè sbagliate) vengono definite “distrattori”. I distrattori svolgono un ruolo importante in un elemento a scelta multipla. Per garantire elementi a scelta multipla di alta qualità, è cruciale includere distrattori plausibili e ben funzionanti che siano più probabili di attirare i candidati con conoscenze parziali. I distrattori non plausibili potrebbero dover essere riscritti o sostituiti con un distrattore migliore. La qualità dei distrattori viene tipicamente valutata attraverso l’analisi dei distrattori. L’analisi dei distrattori viene spesso condotta osservando la proporzione di candidati che scelgono un distrattore particolare.\nPer illustrare l’analisi dei distrattori, utilizziamo gli item del data set multiplechoice in hemp. Si tratta di un ipotetico test a scelta multipla composto da 27 item somministrati a 496 candidati. Le quattro opzioni di risposta sono codificate come 1, 2, 3 e 4 nel data set. Utilizziamo la funzione distract in hemp per calcolare la proporzione di candidati che selezionano ciascun distrattore.\n\ndistractors &lt;- distract(multiplechoice)\nhead(distractors)\n\n\nA matrix: 6 x 4 of type dbl\n\n\n\n1\n2\n3\n4\n\n\n\n\nitem1\n0.044\n0.058\n0.052\n0.845\n\n\nitem2\n0.109\n0.069\n0.792\n0.030\n\n\nitem3\n0.188\n0.562\n0.058\n0.192\n\n\nitem4\n0.034\n0.125\n0.742\n0.099\n\n\nitem5\n0.351\n0.254\n0.042\n0.353\n\n\nitem6\n0.081\n0.198\n0.558\n0.163\n\n\n\n\n\nNella tabella sopra, vediamo che molti item avevano distrattori selezionati circa il 5% delle volte o meno. Questi distrattori potrebbero essere candidati per una revisione in quanto sono stati approvati ad un livello così basso da suggerire che la maggior parte degli esaminandi non li ha considerati come opzioni plausibili. Per l’item 1, i distrattori funzionavano tutti più o meno allo stesso modo (ovvero circa il 5% delle volte ogniuno è stato approvato), suggerendo che funzionavano tutti bene rispetto l’uno all’altro, ma che l’item era troppo facile (la risposta corretta era l’opzione 4, selezionata dall’84.5% degli esaminandi). Al contrario, l’item 5 era un item più difficile, con la risposta corretta che ancora una volta era l’opzione 4. Le opzioni 1 e 2 erano molto probabilmente fraintendimenti, mentre l’opzione 3 potrebbe essere rivista o potenzialmente eliminata da questo item a causa del basso tasso di approvazione (solo il 4.2%). Dato l’approvazione molto alta dell’opzione 1 (35.1%), è molto probabile che anche questa opzione fosse corretta. Per ottenere una visione più completa del funzionamento dell’item, sarebbe consigliabile calcolare l’indice di discriminazione specifico per quell’item. Questo ci permetterebbe di ottenere ulteriori informazioni sulla capacità dell’item di distinguere tra candidati di alto e basso livello.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Valutare e rifinire la soluzione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html",
    "href": "chapters/cfa/01_cfa.html",
    "title": "32  Analisti Fattoriale Confermativa",
    "section": "",
    "text": "32.1 Introduzione\nIn questo capitolo esamineremo la CFA per l’analisi dei modelli di misurazione con fattori comuni e indicatori continui. A differenza dell’analisi fattoriale esplorativa (EFA), nella CFA vengono analizzati modelli di misurazione vincolati. Ciò significa che il ricercatore specifica (1) il numero esatto di fattori; (2) il pattern dei carichi fattoriali, ossia la corrispondenza specifica tra i fattori e gli indicatori; e (3) la presenza di errori correlati, se presenti.\nLa seconda caratteristica menzionata sopra implica che un indicatore satura solo sui fattori specificati dal ricercatore, e tutte le saturazioni incrociate di quell’indicatore su altri fattori sono fissate a zero. Sebbene sia possibile specificare un numero esatto di fattori nella EFA, la tecnica analizza modelli di misurazione non restrittivi, in cui ciascun indicatore satura su tutti i fattori (ossia tutte le saturazioni incrociate sono liberamente stimate).\nUn’altra differenza è che i modelli EFA con più fattori sono identificati solo dopo aver specificato un metodo di rotazione dei fattori, come obliqua (i fattori possono covariare) oppure ortogonale (i fattori sono non correlati). Poiché la CFA richiede un modello identificato, non c’è una fase di rotazione e di solito è permesso ai fattori di covariare.\nNell’ambito dei requisiti per l’identificazione, è possibile stimare errori correlati nella CFA, ma è più difficile ottenere questo risultato nella EFA. Pertanto, la tecnica della CFA supporta meglio l’analisi delle strutture di covarianza degli errori rispetto alla EFA.\nsource(\"../../code/_common.R\")\nsuppressPackageStartupMessages({\n    library(\"lavaan\")\n    library(\"semTools\")\n})",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#limitazioni-dellapproccio-fattoriale",
    "href": "chapters/cfa/01_cfa.html#limitazioni-dellapproccio-fattoriale",
    "title": "32  Analisti Fattoriale Confermativa",
    "section": "32.2 Limitazioni dell’approccio fattoriale",
    "text": "32.2 Limitazioni dell’approccio fattoriale\nL’approccio classico dell’analisi fattoriale (EFA più rotazione fattoriale) ha rivelato avere diversi limiti. Nella ricerca iniziale, dibattiti teorici importanti, come il numero di fattori dell’intelligenza o della personalità, erano basati sui risultati di diverse rotazioni fattoriali. Questi dibattiti si sono rivelati essere semplici speculazioni, poiché conclusioni diverse potevano essere supportate a seconda dell’interpretazione dei dati. Per esempio, il dibattito tra Eysenck e Cattell sul numero di fattori della personalità (due o sedici) dipendeva dall’uso di rotazioni ortogonali o oblique sugli stessi dati.\nNella seconda metà del XX secolo, c’era una generale insoddisfazione verso l’analisi fattoriale a causa della sua apparente capacità di adattarsi a quasi qualsiasi soluzione. Furono raccomandati criteri rigorosi per il suo uso, come la necessità di grandi campioni, che spesso rendevano l’analisi impraticabile a quei tempi. Inoltre, furono introdotti vincoli relativi alle ipotesi del modello e al requisito che le variabili nella matrice di correlazione avessero varianze equivalenti, creando problemi pratici significativi, specialmente con dati binari spesso usati nei test psicometrici.\nSolo con l’introduzione di metodi psicometrici moderni, come l’analisi fattoriale confermativa (CFA) discussa in questo capitolo e la Teoria di Risposta all’Item (discussa in una sezione successiva), questi problemi sono stati risolti.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#efa-vs.-cfa-confronto-tra-analisi-fattoriale-esplorativa-e-confermativa",
    "href": "chapters/cfa/01_cfa.html#efa-vs.-cfa-confronto-tra-analisi-fattoriale-esplorativa-e-confermativa",
    "title": "32  Analisti Fattoriale Confermativa",
    "section": "32.3 EFA vs. CFA: Confronto tra Analisi Fattoriale Esplorativa e Confermativa",
    "text": "32.3 EFA vs. CFA: Confronto tra Analisi Fattoriale Esplorativa e Confermativa\n\n32.3.1 Fondamenti Comuni e Differenze\nSia l’Analisi Fattoriale Esplorativa (EFA) che quella Confermativa (CFA) si basano sul modello dei fattori comuni. Entrambe le tecniche presuppongono che la varianza degli indicatori osservati possa essere suddivisa in varianza comune e varianza unica. La varianza comune è quella condivisa tra gli indicatori e sottende le covarianze osservate, mentre la varianza unica comprende sia la varianza specifica delle variabili che l’errore di misurazione. I fattori estratti, detti fattori comuni, rappresentano le variabili latenti costruite da questa varianza comune.\nNell’EFA, la struttura dei fattori è indeterminata e viene esplorata senza ipotesi a priori riguardo al numero o alla natura dei fattori. L’EFA è quindi particolarmente utile nelle fasi iniziali di ricerca, quando la teoria è poco sviluppata o si sospetta la presenza di fattori inaspettati.\nAl contrario, la CFA si basa su un modello di fattori predefinito, che specifica a priori quali indicatori sono associati a ciascun fattore, rendendola idonea per confermare teorie esistenti o per validare strutture fattoriali precedentemente esplorate. In CFA, i carichi incrociati (indicatori che caricano su più di un fattore) sono generalmente vincolati a zero, stabilendo una relazione diretta e specifica tra fattori e indicatori.\n\n\n32.3.2 Indeterminatezza Fattoriale\nUn problema ricorrente in entrambe le tecniche è l’indeterminatezza fattoriale, dove i fattori comuni non possono essere definiti in modo univoco dai loro indicatori a causa della natura approssimativa delle stime. Questo si manifesta sia in EFA, con l’indeterminatezza della rotazione, che in CFA, dove l’analisi potrebbe non replicarsi in nuovi campioni a causa dell’uso dei medesimi dati per verificare il modello.\n\n\n32.3.3 Indeterminatezza dei Punteggi Fattoriali\nUn’altra complicazione è l’indeterminatezza dei punteggi fattoriali, che si verifica quando esistono infinite soluzioni valide per i punteggi fattoriali a partire dagli indicatori. Questo comporta che diversi metodi possono produrre ordinamenti differenti dei casi, un problema noto come indeterminatezza dei punteggi fattoriali (Grice, 2001).\n\n\n32.3.4 Rotazione e Specificazione del Modello\nLa EFA può presentare ambiguità a causa dell’infinita quantità di configurazioni dei carichi fattoriali che potrebbero adattarsi ai dati, un fenomeno meno pronunciato nella CFA dove la specifica del modello è più rigida.\n\n\n32.3.5 Applicazioni Pratiche\nL’EFA è spesso preferita in nuovi ambiti di ricerca, dove i fattori potrebbero non essere ben definiti, mentre la CFA è utilizzata per confermare le strutture fattoriali in studi di validazione o in seguito a revisioni di test esistenti.\n\n\n32.3.6 Problemi con l’Uso Combinato di EFA e CFA\nSi noti che l’applicazione della CFA immediatamente dopo la EFA nello stesso campione può essere problematica. Talvolta, l’uso congiunto non verifica né conferma i risultati dell’EFA. La restrittività dei modelli CFA, con i carichi incrociati impostati a zero, può portare a risultati che non sono coerenti con i dati analizzati nell’EFA.\n\n\n32.3.7 Nuove Approcci Intermedi\nMetodi come l’Analisi Strutturale Esplorativa (ESEM) offrono un approccio ibrido che combina la flessibilità dell’EFA con alcuni degli aspetti confirmatori della CFA. Questo permette una maggiore precisione nel testare l’adattamento del modello, pur mantenendo la capacità di esplorare nuove strutture fattoriali.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#raccomandazioni-per-la-selezione-degli-indicatori-nellanalisi-fattoriale",
    "href": "chapters/cfa/01_cfa.html#raccomandazioni-per-la-selezione-degli-indicatori-nellanalisi-fattoriale",
    "title": "32  Analisti Fattoriale Confermativa",
    "section": "32.4 Raccomandazioni per la Selezione degli Indicatori nell’Analisi Fattoriale",
    "text": "32.4 Raccomandazioni per la Selezione degli Indicatori nell’Analisi Fattoriale\nLa selezione accurata degli indicatori è cruciale per il successo dell’analisi fattoriale, sia essa Esplorativa (EFA) o Confermativa (CFA). Le linee guida suggerite da Fabrigar e Wegener (2012) e Little et al. (1999), come riassunto in Kline (2023), enfatizzano i seguenti punti chiave:\n\nDefinizione dei Concetti Teorici: È essenziale articolare i concetti teorici in modo dettagliato per delineare chiaramente ogni dominio di interesse. Ad esempio, se lo studio riguarda le dimensioni dell’ansia, è importante riferirsi a letteratura teorica ed empirica che discute vari aspetti come ansia di stato, ansia di tratto e ansia sociale.\nScelta degli Indicatori: Gli indicatori selezionati dovrebbero coprire adeguatamente i domini d’interesse senza affidarsi esclusivamente allo stesso metodo di misurazione, come i questionari di autovalutazione, per ridurre la varianza dovuta a metodi comuni. L’impiego di modelli CFA specializzati può aiutare a stimare questi effetti del metodo.\nGuida Teorica o Empirica Forte: Se esiste una solida base teorica o empirica, gli indicatori omogenei sono preferibili poiché forniscono stime più precise e meno distorte, specialmente in analisi di tipo più confermativo.\nAnalisi di Indicatori Meno Omogenei: Se la guida teorica è debole, può essere vantaggioso esaminare un insieme di indicatori meno omogenei che coprono un’ampia gamma del dominio d’interesse. Ciò evita di basarsi su approssimazioni che potrebbero non riflettere pienamente i concetti chiave.\nUso di Indicatori di Qualità Psicometrica Inferiore: Anche gli indicatori con minore qualità psicometrica possono essere utili se coprono ampiamente il costrutto, generano punteggi che riflettono ampie differenze individuali e sono analizzati attraverso metodi più confermativi.\nProblemi Tecnici: L’analisi potrebbe incontrare problemi come i casi Heywood o la mancata convergenza se alcuni fattori hanno un numero insufficiente di indicatori, specialmente in campioni piccoli. Un numero sicuro minimo di indicatori per ogni fattore previsto è di circa 3-5. Tuttavia, in alcuni casi, potrebbe essere vantaggioso utilizzare meno indicatori per fattore se questi sono psicometricamente solidi.\n\nHayduk e Littvay (2012) hanno sottolineato che non è sempre preferibile avere più indicatori per fattore; in certi contesti, un singolo indicatore ben scelto può essere sufficiente. Se gli indicatori sono altamente ridondanti, non aggiungono informazioni significative. L’idea di una “regola d’oro” di 3-5 indicatori per fattore è una guida generale, ma la scelta dovrebbe essere basata sulle ipotesi specifiche di ricerca piuttosto che su una regola arbitraria.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#fondamenti-dei-modelli-di-base-nella-cfa",
    "href": "chapters/cfa/01_cfa.html#fondamenti-dei-modelli-di-base-nella-cfa",
    "title": "32  Analisti Fattoriale Confermativa",
    "section": "32.5 Fondamenti dei Modelli di Base nella CFA",
    "text": "32.5 Fondamenti dei Modelli di Base nella CFA\nI modelli di base nella Confermative Factor Analysis (CFA) con più fattori sono caratterizzati da specifiche fondamentali che garantiscono una misurazione precisa delle variabili latenti. Ecco una sintesi delle caratteristiche principali di tali modelli:\n\nRelazione tra Indicatori e Fattori: Ogni indicatore è una variabile continua influenzata da due principali componenti: un fattore comune, che rappresenta la variabile latente che l’indicatore è inteso a misurare, e la varianza unica. Quest’ultima include sia l’errore di misurazione casuale sia la varianza specifica non spiegata dal fattore, entrambi rappresentati dal termine di errore.\nIndipendenza dei Termini di Errore: I termini di errore sono assunti come indipendenti l’uno dall’altro e dai fattori. Ciò implica l’assenza di confondenti non misurati per qualsiasi coppia di indicatori e l’indipendenza delle cause omesse dai fattori.\nLinearità e Covarianza: Le relazioni all’interno del modello sono lineari e i fattori possono covariare, il che significa che non esistono effetti causali diretti tra i fattori.\n\nQueste caratteristiche definiscono la misurazione unidimensionale, sottolineando che ciascun indicatore è pensato per misurare una sola dimensione e non condivide varianza con altri indicatori una volta controllati i fattori comuni. Tuttavia, è anche possibile specificare modelli CFA multidimensionali, dove alcuni indicatori possono caricare su più di un fattore o dove coppie di termini di errore possono essere correlati.\nInoltre, esistono metodi specializzati per analizzare relazioni non lineari tra fattori e indicatori continui, o tra i fattori stessi, come descritto da Amemiya e Yalcin (2001). Le relazioni tra indicatori categorici e fattori sono intrinsecamente non lineari, e questi scenari sono trattati nel CFA categorico.\nUn esempio di modello CFA di base con due fattori e sei indicatori viene presentato di seguito, dove tutti i carichi incrociati sono fissati a zero (kline-14-1-fig?). Per esempio, il fattore B non ha un effetto causale diretto sull’indicatore X1, il quale è misurato da un altro fattore (A). Tuttavia, ciò non significa che X1 e il fattore B siano completamente scorrelati. La struttura del modello permette a X1 di covariare con B poiché B è correlato con A, che è una causa di X1 (l’altra causa è E1, il termine di errore di X1). In modo simile, si prevede che gli indicatori X1 e X4 covarino poiché sono influenzati dai fattori A e B, rispettivamente, i quali sono correlati.\nLe costanti di scala, indicate come (1) nel modello, definiscono le metriche per le variabili non misurate, inclusi i fattori comuni e i termini di errore degli indicatori, stabilendo così una base uniforme per la misurazione nel modello CFA.\n\n\n\n\n\n\nFigura 32.1: Modello di analisi fattoriali confermativa con due fattori comuni e sei indicatori. (Figura tratta da Kline (2023))",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#scalatura-dei-fattori-e-inclusione-delle-covariate-nei-modelli-cfa-di-base",
    "href": "chapters/cfa/01_cfa.html#scalatura-dei-fattori-e-inclusione-delle-covariate-nei-modelli-cfa-di-base",
    "title": "32  Analisti Fattoriale Confermativa",
    "section": "32.6 Scalatura dei Fattori e Inclusione delle Covariate nei Modelli CFA di Base",
    "text": "32.6 Scalatura dei Fattori e Inclusione delle Covariate nei Modelli CFA di Base\nNella rappresentazione di base dei modelli Confermative Factor Analysis (CFA), la scalatura dei fattori viene spesso eseguita utilizzando il metodo della variabile di riferimento, conosciuto anche come metodo della variabile marker o approccio di identificazione del carico di riferimento (Newsom, 2015). In questo approccio, un vincolo di Unit Loading Identification (ULI) è applicato al carico di un indicatore per ciascun fattore. Per esempio, nel modello illustrato, il carico di \\(X1\\) sul fattore \\(A\\) è fissato a 1.0, stabilendo così la varianza del fattore \\(A\\) sulla base della varianza comune dell’indicatore \\(X1\\), che funge da variabile di riferimento per \\(A\\). Analogamente, la varianza del fattore \\(B\\) è calibrata utilizzando \\(X4\\) come variabile marker.\nQuando più indicatori per lo stesso fattore presentano precisione equivalente e nessuno di essi è considerato particolarmente rappresentativo del concetto sottostante, la scelta dell’indicatore come variabile di riferimento diventa generalmente arbitraria. Questa selezione non influisce solitamente sull’adattamento globale del modello, sulla soluzione standardizzata, o sulle stime delle varianze di errore dell’indicatore nelle soluzioni non standardizzate. Le saturazioni fisse a 1.0 per le variabili di riferimento rimangono invariate nelle soluzioni non standardizzate e non sono soggette a test di significatività, poiché sono considerate costanti.\nUn potenziale svantaggio di questo metodo è l’assenza di test di significatività per le saturazioni fisse, il che può essere limitante se si desidera valutare la significatività di tutte le saturazioni. Metodi alternativi per scalare i fattori, che non richiedono la selezione di variabili di riferimento, saranno discussi nelle sezioni successive.\nNei modelli CFA di base, tutti i fattori sono considerati variabili esogene, il che significa che sono liberi di variare e covariare indipendentemente l’uno dall’altro. Tuttavia, è possibile includere variabili esterne, dette covariate, che si presume possano influenzare i fattori comuni. Ad esempio, l’età dei partecipanti potrebbe essere vista come una covariata che influisce sui fattori comuni in un modello CFA.\nL’inclusione di covariate trasforma i fattori comuni da variabili esogene a endogene, implicando che non sono più completamente liberi di variare in modo indipendente, ma possono essere direttamente influenzati dalle covariate. Questo richiede l’aggiunta di termini di disturbo nei fattori comuni per rappresentare l’effetto diretto delle covariate su di essi, integrando così l’effetto delle variabili esterne nel modello CFA.\n\n32.6.1 Parametri del Modello nella CFA\nIn un modello CFA con indicatori continui, quando le medie delle variabili non sono considerate, i parametri liberi includono varianze, covarianze di variabili esogene e gli effetti diretti (carichi) sulle variabili endogene. Ad esempio, analizzando il modello di base illustrato in figura, i parametri liberi possono essere suddivisi come segue:\n\nVarianze: Comprendono le varianze di due fattori e sei termini di errore associati agli indicatori, per un totale di otto varianze.\nCovarianza: È presente una covarianza tra i due fattori.\nEffetti Diretti (Carichi): Quattro carichi fattoriali rappresentano gli effetti diretti dei fattori sugli indicatori, specificamente per gli indicatori X2, X3, X5 e X6. Questi carichi non sono fissati, a differenza di quelli usati per scalare i fattori.\n\nSommando questi parametri, il totale dei parametri liberi nel modello è 13. Con \\(v = 6\\) variabili osservate, il numero totale di osservazioni statisticamente indipendenti, calcolato come \\(6(7)/2\\), è 21. Di conseguenza, i gradi di libertà per il modello presentato sono calcolati sottraendo i parametri liberi dalle osservazioni indipendenti, risultando in \\(21 - 13 = 8\\) gradi di libertà.\n\n32.6.1.1 Requisiti di Identificazione: Necessari ma Non Sufficienti per i Modelli di CFA\nPer assicurare che un modello di Confermative Factor Analysis (CFA) sia correttamente specificato e possa essere utilizzato per trarre conclusioni valide, è fondamentale soddisfare alcuni requisiti di identificazione essenziali. Questi requisiti sono necessari ma non sempre sufficienti; cioè, la loro soddisfazione non garantisce automaticamente l’identificazione completa del modello.\n\nGradi di Libertà (dfM) Maggiori o Uguali a Zero:\n\nCalcolo: I gradi di libertà di un modello CFA si determinano sottraendo il numero di parametri liberi (come varianze, covarianze, e carichi fattoriali) dal numero totale di osservazioni indipendenti disponibili, solitamente le varianze e covarianze degli indicatori.\nSignificato: Avere gradi di libertà positivi indica che ci sono sufficienti dati per stimare i parametri del modello e verificare il suo adattamento. Un modello con zero gradi di libertà è “saturato” e si adatterà perfettamente ai dati, ma non fornirà validazione ulteriore.\nImportanza: Mantenere dfM ≥ 0 è cruciale per evitare la sottospecificazione del modello, che potrebbe condurre a stime inaccurate e conclusioni fuorvianti.\n\nScalatura Corretta di Ogni Variabile Non Misurata:\n\nNecessità: È essenziale scalare ogni variabile latente, come i fattori, per definirne l’unità di misura. Senza una scalatura adeguata, parametri come i carichi fattoriali rimarrebbero indeterminati.\nMetodi: La scalatura può essere effettuata in vari modi, come fissando il carico di un indicatore per fattore a 1.0 (metodo della variabile di riferimento), fissando la varianza del fattore a un valore preciso, tipicamente 1.0 (metodo della standardizzazione della varianza), o applicando vincoli alle stime delle saturazioni fattoriali (metodo di codifica degli effetti).\n\n\nIn sintesi, pur essendo fondamentale soddisfare questi requisiti per stabilire una base identificabile e interpretabile per un modello CFA, l’identificazione completa del modello può richiedere considerazioni aggiuntive legate alla struttura specifica e alle ipotesi teoriche che sottendono al modello.\n\n\n\n\n\n\nFigura 32.2: Scalatura dei fattori nel metodo della variabile di riferimento con vincoli di identificazione del carico unitario (ULI) (a), metodo di standardizzazione della variabile con vincoli di identificazione della varianza unitaria (UVI) (b) e metodo di codifica degli effetti con vincoli di identificazione della codifica degli effetti (ECI) (a + b + c)/3 = (d + e + f)/3 = 1.0 (c). (Figura tratta da Kline (2023))\n\n\n\n\n\n32.6.1.2 Requisiti Sufficienti Aggiuntivi per l’Identificazione nei Modelli CFA\nOltre ai criteri base, esistono requisiti addizionali che favoriscono l’identificazione adeguata nei modelli di Confermative Factor Analysis (CFA):\n\nRegola dei Tre Indicatori per i Modelli a Singolo Fattore: Affinché un modello CFA con un solo fattore sia pienamente identificabile, è necessario che disponga di almeno tre indicatori. Questo è dovuto al fatto che con solamente due indicatori non si dispone di sufficiente informazione per separare accuratamente la varianza del fattore e i suoi carichi specifici dagli errori di misurazione. Un modello con esattamente tre indicatori ha zero gradi di libertà, il che significa che si adatterà perfettamente ai dati ma non permetterà ulteriori test o validazioni. Per garantire gradi di libertà positivi (dfM &gt; 0) e consentire un’efficace validazione del modello, è consigliabile utilizzare almeno quattro indicatori.\nRegola dei Due Indicatori per i Modelli con Più Fattori: Nei modelli CFA che coinvolgono più di un fattore, è essenziale che ogni fattore sia rappresentato da almeno due indicatori. Questa disposizione aiuta a definire chiaramente ogni fattore e a distinguerlo dagli altri fattori presenti nel modello. Tuttavia, i modelli che si limitano a due indicatori per fattore possono presentare problematiche, specialmente in campioni di dimensioni ridotte, poiché possono emergere instabilità nelle stime e complessità nell’interpretazione dei risultati.\n\nQuesti requisiti aggiuntivi sono fondamentali non solo per garantire che un modello CFA sia teoricamente valido (attraverso una corretta scalatura e definizione delle variabili latenti), ma anche per assicurare la sua utilità pratica, fornendo sufficienti gradi di libertà per consentire validazioni affidabili e interpretazioni significative del modello.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#oltre-i-requisiti-minimi-di-identificazione",
    "href": "chapters/cfa/01_cfa.html#oltre-i-requisiti-minimi-di-identificazione",
    "title": "32  Analisti Fattoriale Confermativa",
    "section": "32.7 Oltre i Requisiti Minimi di Identificazione",
    "text": "32.7 Oltre i Requisiti Minimi di Identificazione\nNel contesto della Confermative Factor Analysis (CFA), i requisiti di identificazione sono considerati necessari ma non sufficienti. Questo significa che, anche se il loro soddisfacimento è cruciale per una corretta stima dei parametri del modello, essi non garantiscono da soli che il modello sia il migliore possibile o pienamente identificato in termini di struttura e fondamenti teorici. Questa distinzione sottolinea l’importanza di andare oltre i criteri minimi per esplorare l’adeguatezza complessiva e la validità del modello all’interno del suo contesto teorico e applicativo.\n\n32.7.1 Perché sono Necessari\n\nGradi di Libertà (dfM) Maggiori o Uguali a Zero: Avere gradi di libertà non negativi è essenziale per assicurare che ci siano abbastanza dati per stimare i parametri del modello. Se i gradi di libertà sono negativi, indica che ci sono troppi parametri da stimare rispetto alle informazioni disponibili, il che rende il modello inidentificabile.\nScalatura Corretta di Ogni Variabile Non Misurata: La scalatura delle variabili latenti consente di stabilire un’unità di misura chiara, rendendo possibile l’interpretazione dei parametri come i carichi fattoriali. Senza una scalatura appropriata, i parametri del modello rimarrebbero indeterminati e potrebbero condurre a conclusioni ambigue.\n\n\n\n32.7.2 Perché Non Sono Sufficienti\nNonostante la soddisfazione di questi requisiti renda il modello tecnicamente identificabile e stima i parametri, ci sono altre considerazioni che possono influenzare l’adeguatezza del modello:\n\nAdeguamento del Modello: Anche se un modello ha gradi di libertà positivi e le variabili sono correttamente scalate, potrebbe non adattarsi bene ai dati. L’adeguatezza del modello è valutata attraverso statistiche di fit come il Chi-quadrato, RMSEA, CFI, e altri. Un modello può soddisfare i requisiti di identificazione ma avere un cattivo fit.\nValidità Teorica: Un modello può essere tecnicamente corretto ma non catturare accuratamente le relazioni teoriche tra le variabili. La costruzione del modello deve essere guidata da una solida base teorica che giustifica le relazioni tra i fattori e gli indicatori.\n\n\n\n32.7.3 Esempio Pratico\nImmaginiamo un modello CFA per misurare due concetti psicologici, come l’ansia e la depressione, con tre indicatori per ciascun fattore. Anche se il modello potrebbe avere gradi di libertà sufficienti e ogni fattore è correttamente scalato con un indicatore con carico fissato a 1.0, potrebbero sorgere problemi:\n\nCross-loadings: Gli indicatori per l’ansia potrebbero anche avere carichi significativi sulla depressione, il che non è catturato nel modello perché ogni indicatore è supposto misurare un solo fattore. Questo problema di validità del modello non è rilevato dai semplici criteri di identificazione.\nAdattamento del Modello: Il modello potrebbe mostrare un cattivo adattamento ai dati, suggerendo che la struttura ipotizzata dei fattori e degli indicatori non riflette accuratamente le relazioni tra le variabili osservate.\n\nIn conclusione, mentre i requisiti di identificazione sono critici per la fattibilità tecnica di un modello CFA, non garantiscono di per sé che il modello sia il migliore possibile o che rifletta accuratamente le dinamiche sottostanti. Ulteriori analisi e valutazioni sono necessarie per assicurare che il modello sia sia identificabile che valido.\n\n32.7.3.1 Altri Metodi per la Scalatura dei Fattori nei Modelli di CFA\nLa scalatura dei fattori è fondamentale per garantire una corretta identificazione e interpretazione dei fattori in un modello di Confermative Factor Analysis (CFA). Oltre al comune metodo della variabile di riferimento, esistono altri due approcci principali:\n\nMetodo di Standardizzazione della Varianza (Variance Standardization Method):\n\nDescrizione: Questo metodo fissa la varianza di ciascun fattore a 1.0, un approccio noto come unit variance identification (UVI).\nImplicazioni: La standardizzazione dei fattori implica che le loro varianze non sono stimate come parametri liberi. Invece, le covarianze tra i fattori sono liberamente stimate e interpretate come correlazioni di Pearson.\nCarichi degli Indicatori: Tutti i carichi degli indicatori sono considerati parametri liberi, il che permette di testarne la significatività statistica attraverso i loro errori standard.\nVantaggi e Limitazioni: Il principale vantaggio di questo metodo è la sua semplicità e l’assenza di necessità di selezionare una variabile di riferimento. Tuttavia, è generalmente più adatto per modellare fattori esogeni.\n\nMetodo di Codifica degli Effetti (Effects Coding Method):\n\nFunzionamento: A differenza dei metodi precedenti, questo non richiede la selezione di una variabile di riferimento e non implica la standardizzazione dei fattori.\nVincolo di Codifica degli Effetti (ECI): Si impone che la media dei carichi fattoriali per gli indicatori di un dato fattore sia uguale a 1.0. Questo obbliga il software SEM a trovare una combinazione ottimale di carichi che, in media, risultino in 1.0.\nStima della Varianza del Fattore: La varianza del fattore viene stimata come la varianza comune media calcolata attraverso tutti gli indicatori, considerando il loro contributo individuale alla misurazione del fattore.\nVantaggi: Questo metodo consente che tutti gli indicatori contribuiscano equamente alla scalatura del loro fattore comune. È particolarmente utile in studi longitudinali o quando si confrontano gruppi diversi, dato che le varianze dei fattori possono fornire informazioni preziose.\n\n\nOgni metodo di scalatura presenta vantaggi specifici e limitazioni, che devono essere considerati in base agli obiettivi della ricerca e alle caratteristiche del modello CFA:\n\nIl Metodo di Standardizzazione della Varianza offre una soluzione semplice e diretta, ma potrebbe non essere sempre il più appropriato, specialmente in contesti dove i fattori sono endogeni.\nIl Metodo di Codifica degli Effetti è vantaggioso per stabilire una scalatura equilibrata e stabile dei fattori, utile soprattutto in studi comparativi o longitudinali.\n\nLa scelta del metodo di scalatura dovrebbe essere guidata dalle necessità specifiche della ricerca, dalla struttura dei dati e dalle ipotesi teoriche del modello di CFA utilizzato.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#esempio-di-cfa-per-un-modello-di-abilità-cognitive-la-kaufman-assessment-battery-for-children",
    "href": "chapters/cfa/01_cfa.html#esempio-di-cfa-per-un-modello-di-abilità-cognitive-la-kaufman-assessment-battery-for-children",
    "title": "32  Analisti Fattoriale Confermativa",
    "section": "32.8 Esempio di CFA per un Modello di Abilità Cognitive: La Kaufman Assessment Battery for Children",
    "text": "32.8 Esempio di CFA per un Modello di Abilità Cognitive: La Kaufman Assessment Battery for Children\nLa Kaufman Assessment Battery for Children (KABC-I) è un test di valutazione delle abilità cognitive, somministrato individualmente a bambini dagli 2 anni e mezzo ai 12 anni e mezzo (Kaufman & Kaufman, 1983). Questo strumento è stato progettato per misurare due distinti fattori cognitivi attraverso otto indicatori.\nI primi tre compiti del test sono orientati all’elaborazione sequenziale e richiedono ai bambini di ricordare e ripetere stimoli uditivi (come nel Richiamo Numerico e nell’Ordine delle Parole) o visivi (come nei Movimenti della Mano) in un ordine specifico. Questi compiti sono pensati per riflettere la capacità di memoria a breve termine e di sequenziamento delle informazioni.\nGli altri cinque compiti, che includono la Chiusura Gestaltica e la Serie Fotografica, sono ritenuti misurare un tipo di ragionamento più olistico e meno sequenziale, associato all’elaborazione simultanea. Questi compiti valutano la capacità di integrare e sintetizzare le informazioni visuo-spaziali in un contesto più ampio, spesso indipendentemente dall’ordine in cui le informazioni sono presentate.\nKeith (1985) ha proposto delle denominazioni alternative per i fattori misurati dalla KABC-I, suggerendo i termini “memoria a breve termine” per sostituire “elaborazione sequenziale” e “ragionamento visuo-spaziale” per “elaborazione simultanea”. Queste etichette alternative riflettono una prospettiva leggermente diversa sui tipi di competenze cognitive che i due fattori intendono misurare.\nQuesto modello di CFA, utilizzando i compiti della KABC-I come indicatori, fornisce una struttura utile per comprendere come diversi tipi di elaborazione cognitiva possano essere categorizzati e valutati nei contesti educativi e diagnostici.\n\n# input the correlations in lower diagnonal form\nkabcLower.cor &lt;- \"\n 1.00\n .39 1.00\n .35  .67 1.00\n .21  .11  .16 1.00\n .32  .27  .29  .38 1.00\n .40  .29  .28  .30  .47 1.00\n .39  .32  .30  .31  .42  .41 1.00\n .39  .29  .37  .42  .58  .51  .42 1.00 \"\n\n# name the variables and convert to full correlation matrix\n# hm, hand movements; nr, number recall; wo, word order; gc, gestalt closure;\n# tr, triangles; sm, spatial memory; ma, matrix analogies; ps, photo series\nkabc.cor &lt;- lavaan::getCov(kabcLower.cor, names = c(\n    \"hm\", \"nr\", \"wo\",\n    \"gc\", \"tr\", \"sm\", \"ma\", \"ps\"\n))\n# display correlations\nkabc.cor\n\n\nA matrix: 8 x 8 of type dbl\n\n\n\nhm\nnr\nwo\ngc\ntr\nsm\nma\nps\n\n\n\n\nhm\n1.00\n0.39\n0.35\n0.21\n0.32\n0.40\n0.39\n0.39\n\n\nnr\n0.39\n1.00\n0.67\n0.11\n0.27\n0.29\n0.32\n0.29\n\n\nwo\n0.35\n0.67\n1.00\n0.16\n0.29\n0.28\n0.30\n0.37\n\n\ngc\n0.21\n0.11\n0.16\n1.00\n0.38\n0.30\n0.31\n0.42\n\n\ntr\n0.32\n0.27\n0.29\n0.38\n1.00\n0.47\n0.42\n0.58\n\n\nsm\n0.40\n0.29\n0.28\n0.30\n0.47\n1.00\n0.41\n0.51\n\n\nma\n0.39\n0.32\n0.30\n0.31\n0.42\n0.41\n1.00\n0.42\n\n\nps\n0.39\n0.29\n0.37\n0.42\n0.58\n0.51\n0.42\n1.00\n\n\n\n\n\n\n# add the standard deviations and convert to covariances\nkabc.cov &lt;- lavaan::cor2cov(kabc.cor, sds = c(3.40, 2.40, 2.90, 2.70, 2.70, 4.20, 2.80, 3.00))\n\n# display covariances\nkabc.cov\n\n\nA matrix: 8 x 8 of type dbl\n\n\n\nhm\nnr\nwo\ngc\ntr\nsm\nma\nps\n\n\n\n\nhm\n11.5600\n3.1824\n3.4510\n1.9278\n2.9376\n5.7120\n3.7128\n3.978\n\n\nnr\n3.1824\n5.7600\n4.6632\n0.7128\n1.7496\n2.9232\n2.1504\n2.088\n\n\nwo\n3.4510\n4.6632\n8.4100\n1.2528\n2.2707\n3.4104\n2.4360\n3.219\n\n\ngc\n1.9278\n0.7128\n1.2528\n7.2900\n2.7702\n3.4020\n2.3436\n3.402\n\n\ntr\n2.9376\n1.7496\n2.2707\n2.7702\n7.2900\n5.3298\n3.1752\n4.698\n\n\nsm\n5.7120\n2.9232\n3.4104\n3.4020\n5.3298\n17.6400\n4.8216\n6.426\n\n\nma\n3.7128\n2.1504\n2.4360\n2.3436\n3.1752\n4.8216\n7.8400\n3.528\n\n\nps\n3.9780\n2.0880\n3.2190\n3.4020\n4.6980\n6.4260\n3.5280\n9.000\n\n\n\n\n\n\n\n\n\n\n\nFigura 32.3: Modello CFA per la Kaufman Assessment Battery for Children. (Figura tratta da Kline (2023))\n\n\n\n\n32.8.0.1 Modello a Fattore Singolo\nNell’ambito della CFA, se il modello bersaglio ha due o più fattori, spesso il primo modello analizzato è un modello a fattore singolo. Se non si può rigettare un modello a fattore singolo, non ha molto senso valutare modelli con più fattori.\nSpecifichiamo il modello ad un fattore comune nella sintassi di lavaan.\n\n# single factor (general ability)\n# indicator hm automatically specified as reference variable\nkabc1.model &lt;- \"\n    General =~ hm + nr + wo + gc + tr + sm + ma + ps \n\"\n\nAdattiamo il modello ai dati.\n\n# variances calculated with N in the denominator, not N - 1\nkabc1 &lt;- lavaan::sem(kabc1.model, sample.cov = kabc.cov, sample.nobs = 200)\n\nGeneriamo un modello di percorso.\n\nsemPlot::semPaths(kabc1,\n    what = \"col\", whatLabels = \"par\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\n\nEsaminiamo la soluzione non standardizzata.\n\nlavaan::parameterEstimates(kabc1) |&gt; \n    print()\n\n       lhs op     rhs   est    se     z pvalue ci.lower ci.upper\n1  General =~      hm 1.000 0.000    NA     NA    1.000    1.000\n2  General =~      nr 0.636 0.111 5.708      0    0.418    0.854\n3  General =~      wo 0.805 0.136 5.910      0    0.538    1.072\n4  General =~      gc 0.659 0.123 5.361      0    0.418    0.900\n5  General =~      tr 0.963 0.138 6.984      0    0.693    1.233\n6  General =~      sm 1.433 0.211 6.796      0    1.019    1.846\n7  General =~      ma 0.883 0.137 6.459      0    0.615    1.151\n8  General =~      ps 1.166 0.159 7.324      0    0.854    1.478\n9       hm ~~      hm 7.812 0.863 9.049      0    6.120    9.504\n10      nr ~~      nr 4.240 0.456 9.294      0    3.345    5.134\n11      wo ~~      wo 5.975 0.650 9.195      0    4.702    7.249\n12      gc ~~      gc 5.652 0.599 9.432      0    4.478    6.827\n13      tr ~~      tr 3.831 0.468 8.186      0    2.914    4.748\n14      sm ~~      sm 9.979 1.179 8.463      0    7.668   12.290\n15      ma ~~      ma 4.925 0.558 8.822      0    3.831    6.020\n16      ps ~~      ps 3.936 0.531 7.410      0    2.895    4.977\n17 General ~~ General 3.690 0.921 4.008      0    1.885    5.494\n\n\nLa saturazione non standardizzata per il compito “Movimenti della Mano” è stato fissato automaticamente a 1.0 per scalare il singolo fattore comune. Le istruzioni seguenti consentono di estrarre dall’output di sem() solo le informazioni relative alle saturazioni fattoriali.\n\nparameterEstimates(kabc1, standardized = TRUE) %&gt;%\n    dplyr::filter(op == \"=~\") %&gt;%\n    dplyr::select(\n        \"Latent Factor\" = lhs,\n        Indicator = rhs,\n        B = est,\n        SE = se,\n        Z = z,\n        \"p-value\" = pvalue,\n        Beta = std.all\n    ) %&gt;%\n    knitr::kable(\n        digits = 3, booktabs = TRUE, format = \"markdown\",\n        caption = \"Factor Loadings\"\n    )\n\n\n\nTable: Factor Loadings\n\n|Latent Factor |Indicator |     B|    SE|     Z| p-value|  Beta|\n|:-------------|:---------|-----:|-----:|-----:|-------:|-----:|\n|General       |hm        | 1.000| 0.000|    NA|      NA| 0.566|\n|General       |nr        | 0.636| 0.111| 5.708|       0| 0.510|\n|General       |wo        | 0.805| 0.136| 5.910|       0| 0.535|\n|General       |gc        | 0.659| 0.123| 5.361|       0| 0.470|\n|General       |tr        | 0.963| 0.138| 6.984|       0| 0.687|\n|General       |sm        | 1.433| 0.211| 6.796|       0| 0.657|\n|General       |ma        | 0.883| 0.137| 6.459|       0| 0.607|\n|General       |ps        | 1.166| 0.159| 7.324|       0| 0.749|\n\n\nEsaminiamo le misure di bontà di adattamento.\n\nfitMeasures(kabc1, c(\"chisq\", \"df\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt;\n    print()\n\n  chisq      df     cfi     tli   rmsea    srmr \n105.427  20.000   0.818   0.746   0.146   0.084 \n\n\nTroviamo i residui grezzi, ovvero la differenza tra la matrice di covarianza osservata e quella predetta dal modello.\n\nlavaan::residuals(kabc1, type = \"raw\") |&gt;\n    print()\n\n$type\n[1] \"raw\"\n\n$cov\n       hm     nr     wo     gc     tr     sm     ma     ps\nhm  0.000                                                 \nnr  0.820  0.000                                          \nwo  0.462  2.751  0.000                                   \ngc -0.513 -0.836 -0.711  0.000                            \ntr -0.631 -0.519 -0.602  0.415  0.000                     \nsm  0.397 -0.452 -0.863 -0.097  0.212  0.000              \nma  0.437  0.069 -0.199  0.186  0.022  0.131  0.000       \nps -0.345 -0.659 -0.263  0.550  0.530  0.229 -0.289  0.000\n\n\n\nSpecificando type = \"cor.bollen\" o type = \"cor\" otteniamo la differenza tra la matrice di correlazione osservata e quella predetta dal modello.\n\nlavaan::residuals(kabc1, type = \"cor.bollen\") |&gt;\n    print()\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n       hm     nr     wo     gc     tr     sm     ma     ps\nhm  0.000                                                 \nnr  0.101  0.000                                          \nwo  0.047  0.397  0.000                                   \ngc -0.056 -0.130 -0.091  0.000                            \ntr -0.069 -0.080 -0.077  0.057  0.000                     \nsm  0.028 -0.045 -0.071 -0.009  0.019  0.000              \nma  0.046  0.010 -0.025  0.025  0.003  0.011  0.000       \nps -0.034 -0.092 -0.030  0.068  0.066  0.018 -0.035  0.000\n\n\n\nIn alternativa, possiamo ottenere i residui standardizzati alla maniera di Mplus (standardized.mplus), che vengono calcolati utilizzando la seguente formula:\n\\[\n    \\text{Residuo Standardizzato} = \\frac{\\text{Cov. Osservata} - \\text{Cov. Stimata}}{\\sqrt{\\text{Var. dell'Errore per X} \\times \\text{Var. dell'Errore per Y}}},\n\\]\ndove: - La covarianza osservata è il valore della covarianza tra due variabili nel set di dati. - La covarianza stimata è la covarianza tra le stesse due variabili, come previsto dal modello SEM. - La varianza dell’errore per la variabile X e Y sono le varianze degli errori per le due variabili in questione.\nI residui standardizzati misurano quanto la relazione osservata tra due variabili si discosta da quella prevista dal modello, in unità standardizzate. Un valore vicino a zero indica che il modello si adatta bene ai dati per quella specifica relazione. Valori più grandi in valore assoluto suggeriscono un cattivo adattamento in quella specifica parte del modello.\n\nlavaan::residuals(kabc1, type = \"standardized.mplus\") |&gt;\n    print()\n\n$type\n[1] \"standardized.mplus\"\n\n$cov\n       hm     nr     wo     gc     tr     sm     ma     ps\nhm  0.000                                                 \nnr  2.062  0.000                                          \nwo  1.026  6.218  0.000                                   \ngc -1.231 -2.727 -1.952  0.000                            \ntr -2.200 -2.364 -2.355  1.379  0.000                     \nsm  0.723 -1.188 -1.995 -0.210  0.596  0.000              \nma  1.086  0.237 -0.601  0.544  0.089  0.313  0.000       \nps -1.241 -3.422 -1.037  1.833  2.178  0.675 -1.375  0.000\n\n\n\nIl modello a fattore singolo mostra un rapporto elevato chi-quadro/df. Inoltre, i residui per questa analisi indicano che l’adattamento locale è scadente. Pertanto, il modello a fattore singolo per la KABC-I è rigettato.\n\n\n32.8.0.2 Modello a Due Fattori\nIn una seconda analisi, adattiamo ai dati il modello a due fattori rappresentato nella {numref}kline-14-3-fig.\n\nkabc2_model &lt;- \"\n    Sequent =~ hm + nr + wo\n    Simultan =~ gc + tr + sm + ma + ps \n\"\n\n\nkabc2 &lt;- lavaan::sem(kabc2_model, sample.cov = kabc.cov, sample.nobs = 200)\n\n\nsemPlot::semPaths(kabc2,\n    what = \"col\", whatLabels = \"par\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\n\n\nlavaan::parameterEstimates(kabc2) |&gt;\n    print()\n\n        lhs op      rhs   est    se     z pvalue ci.lower ci.upper\n1   Sequent =~       hm 1.000 0.000    NA     NA    1.000    1.000\n2   Sequent =~       nr 1.147 0.181 6.341  0.000    0.792    1.501\n3   Sequent =~       wo 1.388 0.219 6.340  0.000    0.959    1.817\n4  Simultan =~       gc 1.000 0.000    NA     NA    1.000    1.000\n5  Simultan =~       tr 1.445 0.227 6.352  0.000    0.999    1.890\n6  Simultan =~       sm 2.029 0.335 6.062  0.000    1.373    2.685\n7  Simultan =~       ma 1.212 0.212 5.717  0.000    0.797    1.628\n8  Simultan =~       ps 1.727 0.265 6.521  0.000    1.208    2.246\n9        hm ~~       hm 8.664 0.938 9.237  0.000    6.826   10.502\n10       nr ~~       nr 1.998 0.414 4.831  0.000    1.188    2.809\n11       wo ~~       wo 2.902 0.604 4.801  0.000    1.717    4.087\n12       gc ~~       gc 5.419 0.585 9.261  0.000    4.272    6.566\n13       tr ~~       tr 3.426 0.458 7.479  0.000    2.528    4.323\n14       sm ~~       sm 9.997 1.202 8.320  0.000    7.642   12.353\n15       ma ~~       ma 5.105 0.578 8.838  0.000    3.973    6.237\n16       ps ~~       ps 3.482 0.537 6.482  0.000    2.429    4.535\n17  Sequent ~~  Sequent 2.838 0.838 3.389  0.001    1.197    4.480\n18 Simultan ~~ Simultan 1.834 0.530 3.459  0.001    0.795    2.874\n19  Sequent ~~ Simultan 1.271 0.324 3.918  0.000    0.635    1.907\n\n\n\nstandardizedSolution(kabc2)\n\n\nA lavaan.data.frame: 19 x 9\n\n\nlhs\nop\nrhs\nest.std\nse\nz\npvalue\nci.lower\nci.upper\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nSequent\n=~\nhm\n0.4967517\n0.06185190\n8.031309\n8.881784e-16\n0.3755242\n0.6179792\n\n\nSequent\n=~\nnr\n0.8070386\n0.04626958\n17.442098\n0.000000e+00\n0.7163519\n0.8977253\n\n\nSequent\n=~\nwo\n0.8082004\n0.04624070\n17.478118\n0.000000e+00\n0.7175703\n0.8988305\n\n\nSimultan\n=~\ngc\n0.5029005\n0.06088027\n8.260485\n2.220446e-16\n0.3835774\n0.6222236\n\n\nSimultan\n=~\ntr\n0.7264627\n0.04412957\n16.462040\n0.000000e+00\n0.6399703\n0.8129550\n\n\nSimultan\n=~\nsm\n0.6560490\n0.04959951\n13.226925\n0.000000e+00\n0.5588358\n0.7532623\n\n\nSimultan\n=~\nma\n0.5878905\n0.05485948\n10.716298\n0.000000e+00\n0.4803679\n0.6954131\n\n\nSimultan\n=~\nps\n0.7817406\n0.04012341\n19.483401\n0.000000e+00\n0.7031001\n0.8603810\n\n\nhm\n~~\nhm\n0.7532377\n0.06145007\n12.257719\n0.000000e+00\n0.6327978\n0.8736777\n\n\nnr\n~~\nnr\n0.3486887\n0.07468267\n4.668937\n3.027615e-06\n0.2023134\n0.4950641\n\n\nwo\n~~\nwo\n0.3468121\n0.07474351\n4.640030\n3.483594e-06\n0.2003175\n0.4933067\n\n\ngc\n~~\ngc\n0.7470911\n0.06123343\n12.200705\n0.000000e+00\n0.6270758\n0.8671064\n\n\ntr\n~~\ntr\n0.4722520\n0.06411696\n7.365476\n1.765255e-13\n0.3465850\n0.5979189\n\n\nsm\n~~\nsm\n0.5695997\n0.06507943\n8.752377\n0.000000e+00\n0.4420463\n0.6971530\n\n\nma\n~~\nma\n0.6543848\n0.06450273\n10.145071\n0.000000e+00\n0.5279617\n0.7808078\n\n\nps\n~~\nps\n0.3888817\n0.06273220\n6.199076\n5.679559e-10\n0.2659288\n0.5118345\n\n\nSequent\n~~\nSequent\n1.0000000\n0.00000000\nNA\nNA\n1.0000000\n1.0000000\n\n\nSimultan\n~~\nSimultan\n1.0000000\n0.00000000\nNA\nNA\n1.0000000\n1.0000000\n\n\nSequent\n~~\nSimultan\n0.5569247\n0.06673231\n8.345654\n0.000000e+00\n0.4261318\n0.6877176\n\n\n\n\n\n\nfitMeasures(kabc2, c(\"chisq\", \"df\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt;\n    print()\n\n chisq     df    cfi    tli  rmsea   srmr \n38.325 19.000  0.959  0.939  0.071  0.072 \n\n\nUn modello di analisi fattoriale confermativa (CFA) che utilizza un singolo fattore può essere visto come un caso specifico o un “sottoinsieme” di modelli CFA più complessi con due o più fattori che impiegano gli stessi indicatori e lo stesso schema di covarianza degli errori, se presente. Questa struttura gerarchica tra i modelli a singolo fattore e quelli multifattoriali implica che i ricercatori possono applicare il test del chi-quadro per confrontare direttamente l’adattamento di un modello CFA a singolo fattore con quello di modelli CFA a più fattori. In pratica, ciò permette di valutare se l’introduzione di fattori aggiuntivi migliora significativamente l’adattamento del modello ai dati rispetto a un modello più semplice a singolo fattore. Questo tipo di analisi è fondamentale per determinare la complessità ottimale del modello in base alla struttura sottostante dei dati. Sebbene questo argomento verrà approfondito successivamente, è importante anticipare qui l’utilizzo del test del rapporto di verosimiglianza, che consente di confrontare i modelli in maniera quantitativa.\n\nlavTestLRT(kabc1, kabc2)\n\n\nA anova: 2 x 8\n\n\n\nDf\nAIC\nBIC\nChisq\nChisq diff\nRMSEA\nDf diff\nPr(&gt;Chisq)\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nkabc2\n19\n7592.082\n7648.153\n38.32476\nNA\nNA\nNA\nNA\n\n\nkabc1\n20\n7657.183\n7709.956\n105.42664\n67.10188\n0.5748995\n1\n2.578323e-16\n\n\n\n\n\nI risultati del test indicano che l’adattamento del modello con due fattori è statisticamente migliore rispetto a quello del modello a fattore singolo (il modello ad un fattore ha un valore \\(\\chi^2\\) superiore di 67.1 punti, con un grado di libertà).\nAnche se il test del rapporto tra verosimiglianze favorisce il modello a due fattori, possiamo notare che l’esame dei residui mostra un problema con l’indicatore hm.\n\nlavaan::residuals(kabc2, type = \"standardized.mplus\") |&gt;\n    print()\n\n$type\n[1] \"standardized.mplus\"\n\n$cov\n       hm     nr     wo     gc     tr     sm     ma     ps\nhm  0.000                                                 \nnr -0.591  0.000                                          \nwo -3.790  1.539  0.000                                   \ngc  1.126 -2.329 -1.315     NA                            \ntr  2.046 -1.558 -1.001  0.429  0.000                     \nsm  3.464 -0.112 -0.355 -0.784 -0.267  0.000              \nma  3.505  1.129  0.727  0.323 -0.245  0.664  0.008       \nps  2.991 -2.002  0.524  0.910  0.677 -0.144 -1.978  0.000\n\n\n\nPer affrontare questo problema, calcoliamo i modification indices che ci dicono quale parametro del modello ha l’effetto maggiore sulla misura di fit complessivo.\n\nmodindices(kabc2, sort = TRUE, maximum.number = 5)\n\n\nA lavaan.data.frame: 5 x 8\n\n\n\nlhs\nop\nrhs\nmi\nepc\nsepc.lv\nsepc.all\nsepc.nox\n\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n25\nSimultan\n=~\nhm\n20.097078\n1.0539461\n1.4275011\n0.4209070\n0.4209070\n\n\n35\nnr\n~~\nwo\n20.097058\n4.7406831\n4.7406831\n1.9685321\n1.9685321\n\n\n26\nSimultan\n=~\nnr\n7.013048\n-0.5104555\n-0.6913786\n-0.2887972\n-0.2887972\n\n\n29\nhm\n~~\nwo\n7.012988\n-1.7458372\n-1.7458372\n-0.3481696\n-0.3481696\n\n\n32\nhm\n~~\nsm\n4.847027\n1.6094583\n1.6094583\n0.1729329\n0.1729329\n\n\n\n\n\nI risultati degli indici di modifica (MI) indicano che il misfit del modello è principalmente attribuibile alla fissazione a zero del carico tra l’indicatore hm e il fattore comune Simulan, nonché alla fissazione a zero della covarianza tra le componenti residue di nr e wo. Per migliorare l’adattamento del modello, si propone quindi di modificare questi aspetti, iniziando con il primo, ovvero riconsiderando il carico di hm sul fattore Simulan.\n\nkabc3_model &lt;- \"\n    Sequent =~ hm + nr + wo\n    Simultan =~ hm + gc + tr + sm + ma + ps\n\"\n\n\nkabc3 &lt;- lavaan::sem(kabc3_model, sample.cov = kabc.cov, sample.nobs = 200)\n\n\nlavaan::parameterEstimates(kabc3) |&gt;\n    print()\n\n        lhs op      rhs   est    se     z pvalue ci.lower ci.upper\n1   Sequent =~       hm 1.000 0.000    NA     NA    1.000    1.000\n2   Sequent =~       nr 2.285 0.777 2.941  0.003    0.762    3.808\n3   Sequent =~       wo 2.767 0.941 2.939  0.003    0.922    4.612\n4  Simultan =~       hm 1.000 0.000    NA     NA    1.000    1.000\n5  Simultan =~       gc 1.014 0.255 3.979  0.000    0.515    1.514\n6  Simultan =~       tr 1.457 0.329 4.427  0.000    0.812    2.101\n7  Simultan =~       sm 2.103 0.483 4.354  0.000    1.157    3.050\n8  Simultan =~       ma 1.259 0.298 4.229  0.000    0.675    1.842\n9  Simultan =~       ps 1.752 0.391 4.486  0.000    0.987    2.518\n10       hm ~~       hm 7.851 0.845 9.291  0.000    6.195    9.507\n11       nr ~~       nr 1.899 0.487 3.896  0.000    0.944    2.854\n12       wo ~~       wo 2.750 0.713 3.856  0.000    1.352    4.148\n13       gc ~~       gc 5.444 0.585 9.297  0.000    4.296    6.591\n14       tr ~~       tr 3.521 0.457 7.702  0.000    2.625    4.417\n15       sm ~~       sm 9.767 1.179 8.287  0.000    7.457   12.077\n16       ma ~~       ma 5.013 0.569 8.815  0.000    3.898    6.127\n17       ps ~~       ps 3.554 0.529 6.718  0.000    2.517    4.591\n18  Sequent ~~  Sequent 0.734 0.490 1.499  0.134   -0.226    1.693\n19 Simultan ~~ Simultan 1.759 0.760 2.314  0.021    0.269    3.250\n20  Sequent ~~ Simultan 0.579 0.178 3.252  0.001    0.230    0.928\n\n\nIl modello così modificato fornisce un buon adattamento ai dati.\n\nfitMeasures(kabc3, c(\"chisq\", \"df\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt;\n    print()\n\n chisq     df    cfi    tli  rmsea   srmr \n18.108 18.000  1.000  1.000  0.005  0.035 \n\n\n\nlavaan::residuals(kabc3, type = \"standardized.mplus\") |&gt;\n    print()\n\n$type\n[1] \"standardized.mplus\"\n\n$cov\n       hm     nr     wo     gc     tr     sm     ma     ps\nhm  0.000                                                 \nnr  1.165  0.000                                          \nwo -1.637  0.000  0.000                                   \ngc -1.066 -1.919 -0.939  0.000                            \ntr -1.710 -0.763 -0.247  0.603  0.000                     \nsm  1.325  0.287  0.044 -0.867 -0.304  0.000              \nma  1.730  1.428  1.029  0.258 -0.298  0.338  0.000       \nps -0.512 -1.059  1.285  1.035  1.088 -0.361 -2.181  0.008\n\n\n\nEseguiamo il confronto tra questo terzo modello e il secondo.\n\nlavTestLRT(kabc2, kabc3)\n\n\nA anova: 2 x 8\n\n\n\nDf\nAIC\nBIC\nChisq\nChisq diff\nRMSEA\nDf diff\nPr(&gt;Chisq)\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nkabc3\n18\n7573.864\n7633.234\n18.10764\nNA\nNA\nNA\nNA\n\n\nkabc2\n19\n7592.082\n7648.153\n38.32476\n20.21711\n0.3099767\n1\n6.913179e-06\n\n\n\n\n\nIl test del rapporto tra verosimiglianze favorisce il modello nel quale hm satura su entrambi i fattori comuni.\n\nstandardizedSolution(kabc3)\n\n\nA lavaan.data.frame: 20 x 9\n\n\nlhs\nop\nrhs\nest.std\nse\nz\npvalue\nci.lower\nci.upper\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nSequent\n=~\nhm\n0.2525852\n0.08220770\n3.072524\n2.122564e-03\n0.09146104\n0.4137093\n\n\nSequent\n=~\nnr\n0.8177224\n0.05332037\n15.336023\n0.000000e+00\n0.71321642\n0.9222284\n\n\nSequent\n=~\nwo\n0.8193490\n0.05332364\n15.365586\n0.000000e+00\n0.71483657\n0.9238614\n\n\nSimultan\n=~\nhm\n0.3911086\n0.07920465\n4.937950\n7.894795e-07\n0.23587038\n0.5463469\n\n\nSimultan\n=~\ngc\n0.4995366\n0.06083478\n8.211366\n2.220446e-16\n0.38030263\n0.6187706\n\n\nSimultan\n=~\ntr\n0.7173667\n0.04430392\n16.191948\n0.000000e+00\n0.63053266\n0.8042008\n\n\nSimultan\n=~\nsm\n0.6659828\n0.04841875\n13.754648\n0.000000e+00\n0.57108381\n0.7608818\n\n\nSimultan\n=~\nma\n0.5978172\n0.05378964\n11.113985\n0.000000e+00\n0.49239147\n0.7032430\n\n\nSimultan\n=~\nps\n0.7766255\n0.03976264\n19.531536\n0.000000e+00\n0.69869211\n0.8545588\n\n\nhm\n~~\nhm\n0.6825459\n0.06079286\n11.227403\n0.000000e+00\n0.56339406\n0.8016977\n\n\nnr\n~~\nnr\n0.3313300\n0.08720253\n3.799546\n1.449613e-04\n0.16041622\n0.5022438\n\n\nwo\n~~\nwo\n0.3286672\n0.08738134\n3.761298\n1.690341e-04\n0.15740296\n0.4999315\n\n\ngc\n~~\ngc\n0.7504632\n0.06077839\n12.347532\n0.000000e+00\n0.63133972\n0.8695867\n\n\ntr\n~~\ntr\n0.4853850\n0.06356431\n7.636123\n2.242651e-14\n0.36080119\n0.6099687\n\n\nsm\n~~\nsm\n0.5564669\n0.06449211\n8.628450\n0.000000e+00\n0.43006469\n0.6828691\n\n\nma\n~~\nma\n0.6426146\n0.06431274\n9.992026\n0.000000e+00\n0.51656392\n0.7686652\n\n\nps\n~~\nps\n0.3968529\n0.06176136\n6.425586\n1.313629e-10\n0.27580286\n0.5179029\n\n\nSequent\n~~\nSequent\n1.0000000\n0.00000000\nNA\nNA\n1.00000000\n1.0000000\n\n\nSimultan\n~~\nSimultan\n1.0000000\n0.00000000\nNA\nNA\n1.00000000\n1.0000000\n\n\nSequent\n~~\nSimultan\n0.5096198\n0.07025684\n7.253668\n4.056755e-13\n0.37191892\n0.6473207\n\n\n\n\n\n\nsemPlot::semPaths(kabc3,\n    what = \"col\", whatLabels = \"std\", style = \"mx\",\n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\n\nNei modelli precedenti, abbiamo adottato un metodo di scalatura dei fattori comuni che fissava la saturazione fattoriale di uno degli indicatori per ciascun fattore comune a 1.0 come riferimento. Ora, esploreremo un diverso approccio di scalatura che prevede la standardizzazione della varianza delle variabili latenti.\nPer attuare questa procedura nel software lavaan, è necessario modificare la configurazione predefinita in cui la saturazione fattoriale del primo indicatore di ogni fattore comune è fissata a 1.0. Per fare ciò, useremo la sintassi NA* per indicare che la saturazione fattoriale del primo indicatore deve essere stimata. Questo si realizza inserendo NA* nell’istruzione che definisce la relazione tra le variabili latenti e gli indicatori (espresso tramite =~). Inoltre, è fondamentale specificare che la varianza delle variabili latenti sia fissata a 1.0, il che si attua mediante la sintassi 1* nell’istruzione che stabilisce la varianza di ciascun fattore comune (~~).\n\nkabc3alt_model &lt;- \"\n    Sequent =~ NA*hm + nr + wo\n    Simultan =~ NA*hm + gc + tr + sm + ma + ps\n\n    Sequent ~~ 1*Sequent\n    Simultan ~~ 1*Simultan\n\"\n\nAdattiamo il modello così parametrizzato ai dati.\n\nkabc3alt &lt;- lavaan::sem(\n    kabc3alt_model, sample.cov = kabc.cov, sample.nobs = 200, std.lv = TRUE\n)\n\nEsaminiamo la soluzione non standardizzata.\n\nsemPlot::semPaths(kabc3alt,\n    what = \"col\", whatLabels = \"par\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\n\nEsaminiamo la soluzione stanardizzata.\n\nloadings &lt;- standardizedSolution(kabc3alt)\nloadings\n\n\nA lavaan.data.frame: 20 x 9\n\n\nlhs\nop\nrhs\nest.std\nse\nz\npvalue\nci.lower\nci.upper\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nSequent\n=~\nhm\n0.2525843\n0.08220772\n3.072513\n2.122649e-03\n0.09146008\n0.4137084\n\n\nSequent\n=~\nnr\n0.8177223\n0.05332042\n15.336006\n0.000000e+00\n0.71321623\n0.9222284\n\n\nSequent\n=~\nwo\n0.8193488\n0.05332369\n15.365569\n0.000000e+00\n0.71483633\n0.9238614\n\n\nSimultan\n=~\nhm\n0.3911092\n0.07920463\n4.937959\n7.894445e-07\n0.23587098\n0.5463474\n\n\nSimultan\n=~\ngc\n0.4995384\n0.06083464\n8.211414\n2.220446e-16\n0.38030467\n0.6187721\n\n\nSimultan\n=~\ntr\n0.7173674\n0.04430383\n16.191995\n0.000000e+00\n0.63053351\n0.8042013\n\n\nSimultan\n=~\nsm\n0.6659822\n0.04841876\n13.754631\n0.000000e+00\n0.57108320\n0.7608813\n\n\nSimultan\n=~\nma\n0.5978175\n0.05378959\n11.114002\n0.000000e+00\n0.49239189\n0.7032432\n\n\nSimultan\n=~\nps\n0.7766261\n0.03976255\n19.531596\n0.000000e+00\n0.69869292\n0.8545593\n\n\nSequent\n~~\nSequent\n1.0000000\n0.00000000\nNA\nNA\n1.00000000\n1.0000000\n\n\nSimultan\n~~\nSimultan\n1.0000000\n0.00000000\nNA\nNA\n1.00000000\n1.0000000\n\n\nhm\n~~\nhm\n0.6825461\n0.06079284\n11.227409\n0.000000e+00\n0.56339433\n0.8016979\n\n\nnr\n~~\nnr\n0.3313302\n0.08720260\n3.799545\n1.449621e-04\n0.16041623\n0.5022441\n\n\nwo\n~~\nwo\n0.3286675\n0.08738141\n3.761297\n1.690344e-04\n0.15740304\n0.4999319\n\n\ngc\n~~\ngc\n0.7504614\n0.06077847\n12.347487\n0.000000e+00\n0.63133780\n0.8695850\n\n\ntr\n~~\ntr\n0.4853840\n0.06356425\n7.636116\n2.242651e-14\n0.36080034\n0.6099676\n\n\nsm\n~~\nsm\n0.5564677\n0.06449207\n8.628466\n0.000000e+00\n0.43006551\n0.6828698\n\n\nma\n~~\nma\n0.6426142\n0.06431272\n9.992024\n0.000000e+00\n0.51656358\n0.7686648\n\n\nps\n~~\nps\n0.3968519\n0.06176127\n6.425579\n1.313689e-10\n0.27580204\n0.5179018\n\n\nSequent\n~~\nSimultan\n0.5096198\n0.07025682\n7.253670\n4.056755e-13\n0.37191898\n0.6473207\n\n\n\n\n\n\nrelevant_loadings &lt;- loadings[loadings$op == \"=~\", c(\"lhs\", \"rhs\", \"est.std\")]\nrelevant_loadings\n\n\nA lavaan.data.frame: 9 x 3\n\n\n\nlhs\nrhs\nest.std\n\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n\n\n\n\n1\nSequent\nhm\n0.2525843\n\n\n2\nSequent\nnr\n0.8177223\n\n\n3\nSequent\nwo\n0.8193488\n\n\n4\nSimultan\nhm\n0.3911092\n\n\n5\nSimultan\ngc\n0.4995384\n\n\n6\nSimultan\ntr\n0.7173674\n\n\n7\nSimultan\nsm\n0.6659822\n\n\n8\nSimultan\nma\n0.5978175\n\n\n9\nSimultan\nps\n0.7766261\n\n\n\n\n\nIdealmente, per sostenere l’ipotesi di validità convergente, un fattore dovrebbe spiegare almeno il 50% della varianza in ciascuno dei suoi indicatori continui, come sostengono Bagozzi e Yi (2012). Ciò implica che, per essere considerato adeguatamente rappresentativo del costrutto che intende misurare, tutti gli indicatori di un fattore dovrebbero mostrare che la maggior parte della loro varianza è spiegata dal fattore stesso. Un modo meno rigoroso ma ancora informativo per valutare la validità convergente è attraverso l’uso della Varianza Media Estratta (AVE), calcolata come la media dei quadrati dei carichi fattoriali standardizzati di tutti gli indicatori associati a un particolare fattore. Un valore AVE superiore a 0.50 indica che, in media, il fattore spiega più della metà della varianza degli indicatori rispetto alla varianza residua attribuibile agli errori di misurazione, come indicato da Hair et al. (2022).\nNell’ambito di un modello a due fattori, i risultati ottenuti dall’esempio in esame evidenziano alcune criticità in relazione al criterio più stringente: il modello non riesce a spiegare una variazione significativa (R^2 &gt; 0.50) per quattro dei otto indicatori, ossia la metà di essi. Tuttavia, se consideriamo l’AVE, i risultati migliorano leggermente per il fattore sequenziale, che spiega in media circa il 52% della varianza dei suoi tre indicatori (AVE = 0.517).\nNella pratica analitica reale, valori di R^2 inferiori a 0.50 sono spesso considerati accettabili. Comrey e Lee (1992) hanno proposto una scala di valutazione gradiente in cui un R^2 superiore a 0.50 è classificato come eccellente, mentre valori approssimativamente pari a 0.40, 0.30, 0.20 e 0.10 sono considerati molto buoni, buoni, sufficienti e scarsi, rispettivamente. Secondo queste linee guida più flessibili, i risultati per gli indicatori del modello CFA a due fattori della KABC-I sono classificati come “eccellenti” (R^2 &gt; 0.50) per tre degli otto indicatori, nessuno è giudicato “scarso” (R^2 circa 0.10), e i rimanenti cinque indicatori presentano valori intermedi. È essenziale sottolineare che queste linee guida non dovrebbero essere applicate in modo indiscriminato in tutti i contesti di CFA o con tutti i tipi di indicatori. Gli indicatori continui, come i punteggi totali nell’esempio citato, tendono a mostrare carichi fattoriali più elevati rispetto agli indicatori ordinali, come quelli basati su scale di risposta tipo Likert.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#considerazioni-finali",
    "href": "chapters/cfa/01_cfa.html#considerazioni-finali",
    "title": "32  Analisti Fattoriale Confermativa",
    "section": "32.9 Considerazioni Finali",
    "text": "32.9 Considerazioni Finali\nL’analisi fattoriale confermativa (CFA) rappresenta uno strumento cruciale nell’ambito delle ricerche psicologiche e sociali, in quanto consente di esaminare modelli di misurazione riflessiva. In questi modelli, i fattori comuni agiscono come proxy per le variabili teoriche. La CFA richiede che il ricercatore definisca preventivamente aspetti critici del modello, come il numero di fattori, l’assegnazione degli indicatori ai fattori e gli schemi di covarianza degli errori.\nNei modelli CFA base, ciascun indicatore continuo è associato a un unico fattore e si presume che gli errori siano indipendenti, formando così una struttura unidimensionale. L’analisi di modelli con più fattori permette di verificare le ipotesi di validità convergente e discriminante.\nÈ anche possibile esplorare modelli CFA che includono covarianze di errore o indicatori correlati a più fattori. Tuttavia, gestire tali modelli è più complesso, specialmente in termini di identificazione del modello. Problemi tecnici come la non convergenza delle soluzioni o risultati inammissibili sono più comuni nei campioni di dimensioni ridotte o quando i fattori sono definiti da soli due indicatori. L’aggiustamento del modello può diventare una sfida, considerata l’ampia varietà di modifiche potenziali.\nUn’altra questione critica è rappresentata dai modelli CFA equivalenti, i quali possono produrre risultati simili nonostante le loro differenze strutturali. Per affrontare queste sfide efficacemente, è essenziale fondare l’analisi più su basi teoriche che su meri calcoli statistici. L’efficacia della CFA, quindi, dipende notevolmente dal contesto teorico e dalla competenza metodologica del ricercatore, essendo cruciale un’approfondita comprensione del dominio di studio per guidare l’analisi.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#session-info",
    "href": "chapters/cfa/01_cfa.html#session-info",
    "title": "32  Analisti Fattoriale Confermativa",
    "section": "32.10 Session Info",
    "text": "32.10 Session Info\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.4.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ggokabeito_0.1.0  viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n [5] ggExtra_0.10.1    bayesplot_1.11.1  gridExtra_2.3     patchwork_1.2.0  \n [9] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-17     psych_2.4.3      \n[13] scales_1.3.0      markdown_1.12     knitr_1.45        lubridate_1.9.3  \n[17] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[21] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.0    \n[25] tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.15.0  jsonlite_1.8.8     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5   nloptr_2.0.3      \n  [7] rmarkdown_2.26     vctrs_0.6.5        minqa_1.2.6       \n [10] base64enc_0.1-3    rstatix_0.7.2      htmltools_0.5.7   \n [13] broom_1.0.5        Formula_1.2-5      htmlwidgets_1.6.4 \n [16] plyr_1.8.9         sandwich_3.1-0     emmeans_1.10.0    \n [19] zoo_1.8-12         uuid_1.2-0         igraph_2.0.2      \n [22] mime_0.12          lifecycle_1.0.4    pkgconfig_2.0.3   \n [25] Matrix_1.6-5       R6_2.5.1           fastmap_1.1.1     \n [28] shiny_1.8.0        digest_0.6.35      OpenMx_2.21.11    \n [31] fdrtool_1.2.17     colorspace_2.1-0   rprojroot_2.0.4   \n [34] Hmisc_5.1-1        fansi_1.0.6        timechange_0.3.0  \n [37] abind_1.4-5        compiler_4.3.3     withr_3.0.0       \n [40] glasso_1.11        htmlTable_2.4.2    backports_1.4.1   \n [43] carData_3.0-5      ggsignif_0.6.4     MASS_7.3-60.0.1   \n [46] corpcor_1.6.10     gtools_3.9.5       tools_4.3.3       \n [49] pbivnorm_0.6.0     foreign_0.8-86     zip_2.3.1         \n [52] httpuv_1.6.14      nnet_7.3-19        glue_1.7.0        \n [55] quadprog_1.5-8     nlme_3.1-164       promises_1.2.1    \n [58] lisrelToR_0.3      grid_4.3.3         pbdZMQ_0.3-11     \n [61] checkmate_2.3.1    cluster_2.1.6      reshape2_1.4.4    \n [64] generics_0.1.3     gtable_0.3.4       tzdb_0.4.0        \n [67] data.table_1.15.2  hms_1.1.3          car_3.1-2         \n [70] utf8_1.2.4         sem_3.1-15         pillar_1.9.0      \n [73] IRdisplay_1.1      rockchalk_1.8.157  later_1.3.2       \n [76] splines_4.3.3      lattice_0.22-5     survival_3.5-8    \n [79] kutils_1.73        tidyselect_1.2.0   miniUI_0.1.1.1    \n [82] pbapply_1.7-2      stats4_4.3.3       xfun_0.42         \n [85] qgraph_1.9.8       arm_1.13-1         stringi_1.8.3     \n [88] boot_1.3-29        evaluate_0.23      codetools_0.2-19  \n [91] mi_1.1             cli_3.6.2          RcppParallel_5.1.7\n [94] IRkernel_1.3.2     rpart_4.1.23       xtable_1.8-4      \n [97] repr_1.1.6         munsell_0.5.0      Rcpp_1.0.12       \n[100] coda_0.19-4.1      png_0.1-8          XML_3.99-0.16.1   \n[103] parallel_4.3.3     ellipsis_0.3.2     jpeg_0.1-10       \n[106] lme4_1.1-35.1      mvtnorm_1.2-4      openxlsx_4.2.5.2  \n[109] crayon_1.5.2       rlang_1.1.3        multcomp_1.4-25   \n[112] mnormt_2.1.1      \n\n\n\n\n\n\nKline, Rex B. 2023. Principles and practice of structural equation modeling. Guilford publications.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/02_meanstructure.html",
    "href": "chapters/cfa/02_meanstructure.html",
    "title": "34  La struttura delle medie",
    "section": "",
    "text": "34.1 Introduzione\nNei modelli di equazioni strutturali (SEM), simili all’analisi fattoriale, esaminiamo principalmente le relazioni di covarianza tra le variabili. Una caratteristica distintiva dei modelli SEM rispetto all’analisi fattoriale tradizionale è la possibilità di includere le medie sia delle variabili osservate che di quelle latenti. Questo è particolarmente utile in modelli come quelli di analisi fattoriale confermativa (CFA) longitudinale, dove le ipotesi si concentrano sulle medie dei costrutti analizzati.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>La struttura delle medie</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/02_meanstructure.html#limpiego-delle-medie-nei-modelli-di-equazioni-strutturali-sem",
    "href": "chapters/cfa/02_meanstructure.html#limpiego-delle-medie-nei-modelli-di-equazioni-strutturali-sem",
    "title": "34  La struttura delle medie",
    "section": "",
    "text": "34.1.1 Interpretazione delle Intercette nei Modelli SEM\nIn un modello SEM, l’intercetta di una variabile indicatore (denotata con $ $) indica la media stimata di quella variabile. Il valore di $ $ rappresenta il valore atteso dell’indicatore quando il fattore latente a cui è associato è zero. La relazione generale per un indicatore $ y $ in un modello SEM è data dalla formula:\n\\[\ny = \\tau + \\lambda \\cdot \\text{fattore latente} + \\varepsilon,\n\\]\ndove: - $ y $ è il punteggio osservato dell’indicatore. - $ $ rappresenta il carico fattoriale, che indica quanto fortemente l’indicatore è influenzato dal fattore latente. - $ $ è l’intercetta, cioè la media stimata dell’indicatore. - $ $ è l’errore di misura associato all’indicatore.\n\n\n34.1.2 Struttura delle Medie nel Modello CFA\nNel contesto di un modello CFA, la struttura delle medie è descritta dalla formula:\n\\[ \\text{media(variabile latente)} = \\Lambda \\mu_{\\text{lat}} + \\tau, \\]\nqui: - $ $ è la matrice dei carichi fattoriali. - $ _{} $ è il vettore che rappresenta le medie dei costrutti latenti. - $ $ è il vettore delle intercette degli indicatori.\n\n\n34.1.3 Utilizzo delle Medie nel Software lavaan\nNel software lavaan, utilizzato per l’analisi SEM, è possibile stimare le intercette inserendo l’opzione meanstructure = TRUE nella sintassi del modello. Questo comando permette di includere automaticamente una costante “1” in tutte le equazioni del modello, facilitando così il calcolo delle intercette per le variabili endogene. È necessario fornire i dati originali o una matrice di covarianza, insieme alle medie di tutte le variabili interessate.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>La struttura delle medie</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/02_meanstructure.html#un-esempio-pratico",
    "href": "chapters/cfa/02_meanstructure.html#un-esempio-pratico",
    "title": "34  La struttura delle medie",
    "section": "34.3 Un Esempio Pratico",
    "text": "34.3 Un Esempio Pratico\nUtilizziamo il dataset HolzingerSwineford1939 per costruire un modello di misurazione con tre costrutti latenti (visual, textual, speed), ciascuno definito da tre indicatori (x1, x2, x3, ecc.).\n\ndata(HolzingerSwineford1939)\nglimpse(HolzingerSwineford1939)\n\nRows: 301\nColumns: 15\n$ id     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, ~\n$ sex    &lt;int&gt; 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, ~\n$ ageyr  &lt;int&gt; 13, 13, 13, 13, 12, 14, 12, 12, 13, 12, 12, 12, 12, 12, 12, 12,~\n$ agemo  &lt;int&gt; 1, 7, 1, 2, 2, 1, 1, 2, 0, 5, 2, 11, 7, 8, 6, 1, 11, 5, 8, 3, 1~\n$ school &lt;fct&gt; Pasteur, Pasteur, Pasteur, Pasteur, Pasteur, Pasteur, Pasteur, ~\n$ grade  &lt;int&gt; 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ~\n$ x1     &lt;dbl&gt; 3.333333, 5.333333, 4.500000, 5.333333, 4.833333, 5.333333, 2.8~\n$ x2     &lt;dbl&gt; 7.75, 5.25, 5.25, 7.75, 4.75, 5.00, 6.00, 6.25, 5.75, 5.25, 5.7~\n$ x3     &lt;dbl&gt; 0.375, 2.125, 1.875, 3.000, 0.875, 2.250, 1.000, 1.875, 1.500, ~\n$ x4     &lt;dbl&gt; 2.333333, 1.666667, 1.000000, 2.666667, 2.666667, 1.000000, 3.3~\n$ x5     &lt;dbl&gt; 5.75, 3.00, 1.75, 4.50, 4.00, 3.00, 6.00, 4.25, 5.75, 5.00, 3.5~\n$ x6     &lt;dbl&gt; 1.2857143, 1.2857143, 0.4285714, 2.4285714, 2.5714286, 0.857142~\n$ x7     &lt;dbl&gt; 3.391304, 3.782609, 3.260870, 3.000000, 3.695652, 4.347826, 4.6~\n$ x8     &lt;dbl&gt; 5.75, 6.25, 3.90, 5.30, 6.30, 6.65, 6.20, 5.15, 4.65, 4.55, 5.7~\n$ x9     &lt;dbl&gt; 6.361111, 7.916667, 4.416667, 4.861111, 5.916667, 7.500000, 4.8~\n\n\n\nhs_model &lt;- \"\n    visual =~ NA*x1 + x2 + x3\n    textual =~ NA*x4 + x5 + x6\n    speed =~ NA*x7 + x8 + x9\n\n    visual ~~ 1*visual\n    textual ~~ 1 * textual\n    speed ~~ 1 * speed\n\"\n\nUtilizziamo l’argomento meanstructure = TRUE per richiedere la stima delle intercette degli indicatori \\(\\tau\\).\n\nOgni costrutto latente è definito in relazione ai suoi indicatori, dove le intercette degli indicatori (\\(\\tau\\)) non sono fissate a priori, ma stimate dal modello.\nLe varianze dei costrutti latenti sono fissate a 1, mentre le loro medie sono fissate a 0 (come evidenziato dall’output, righe 34-36).\nNel caso presente, poiché le medie dei costrutti latenti sono fissate a zero, la media predetta per gli indicatori corrisponde alle intercette stimate.\n\n\nfit &lt;- cfa(hs_model,\n    data = HolzingerSwineford1939,\n    meanstructure = TRUE\n)\n\n\nparameterEstimates(fit) |&gt;\n    print()\n\n       lhs op     rhs   est    se      z pvalue ci.lower ci.upper\n1   visual =~      x1 0.900 0.081 11.128      0    0.741    1.058\n2   visual =~      x2 0.498 0.077  6.429      0    0.346    0.650\n3   visual =~      x3 0.656 0.074  8.817      0    0.510    0.802\n4  textual =~      x4 0.990 0.057 17.474      0    0.879    1.101\n5  textual =~      x5 1.102 0.063 17.576      0    0.979    1.224\n6  textual =~      x6 0.917 0.054 17.082      0    0.811    1.022\n7    speed =~      x7 0.619 0.070  8.903      0    0.483    0.756\n8    speed =~      x8 0.731 0.066 11.090      0    0.602    0.860\n9    speed =~      x9 0.670 0.065 10.305      0    0.543    0.797\n10  visual ~~  visual 1.000 0.000     NA     NA    1.000    1.000\n11 textual ~~ textual 1.000 0.000     NA     NA    1.000    1.000\n12   speed ~~   speed 1.000 0.000     NA     NA    1.000    1.000\n13      x1 ~~      x1 0.549 0.114  4.833      0    0.326    0.772\n14      x2 ~~      x2 1.134 0.102 11.146      0    0.934    1.333\n15      x3 ~~      x3 0.844 0.091  9.317      0    0.667    1.022\n16      x4 ~~      x4 0.371 0.048  7.779      0    0.278    0.465\n17      x5 ~~      x5 0.446 0.058  7.642      0    0.332    0.561\n18      x6 ~~      x6 0.356 0.043  8.277      0    0.272    0.441\n19      x7 ~~      x7 0.799 0.081  9.823      0    0.640    0.959\n20      x8 ~~      x8 0.488 0.074  6.573      0    0.342    0.633\n21      x9 ~~      x9 0.566 0.071  8.003      0    0.427    0.705\n22  visual ~~ textual 0.459 0.064  7.189      0    0.334    0.584\n23  visual ~~   speed 0.471 0.073  6.461      0    0.328    0.613\n24 textual ~~   speed 0.283 0.069  4.117      0    0.148    0.418\n25      x1 ~1         4.936 0.067 73.473      0    4.804    5.067\n26      x2 ~1         6.088 0.068 89.855      0    5.955    6.221\n27      x3 ~1         2.250 0.065 34.579      0    2.123    2.378\n28      x4 ~1         3.061 0.067 45.694      0    2.930    3.192\n29      x5 ~1         4.341 0.074 58.452      0    4.195    4.486\n30      x6 ~1         2.186 0.063 34.667      0    2.062    2.309\n31      x7 ~1         4.186 0.063 66.766      0    4.063    4.309\n32      x8 ~1         5.527 0.058 94.854      0    5.413    5.641\n33      x9 ~1         5.374 0.058 92.546      0    5.260    5.488\n34  visual ~1         0.000 0.000     NA     NA    0.000    0.000\n35 textual ~1         0.000 0.000     NA     NA    0.000    0.000\n36   speed ~1         0.000 0.000     NA     NA    0.000    0.000\n\n\n\n34.3.1 Interpretazione delle Medie Stimate\nLa media dei punteggi osservati per gli indicatori (x1, x2, x3, ecc.) viene calcolata attraverso le intercette stimate dal modello. È fondamentale distinguere tra la media empirica, calcolata direttamente dai dati, e la media predetta dal modello. La media predetta degli indicatori in un modello dove la media dei costrutti latenti è fissata a zero è influenzata esclusivamente dalle loro intercette.\n\n\n34.3.2 Calcolo delle Medie Osservate e Predette in R\nConsideriamo gli indicatori x1, x2, x3. Per calcolare la media osservata di questi indicatori, usiamo le loro intercette stimate.\n\nintercepts &lt;- params$est[params$op == \"~1\"][1:9] # Intercette degli indicatori (τ)\n\nQuesto ci fornisce le intercette degli indicatori:\n\nintercepts |&gt; print()\n\n[1] 4.935770 6.088040 2.250415 3.060908 4.340532 2.185572 4.185902 5.527076\n[9] 5.374123\n\n\nPer ottenere la media osservata dei punteggi di x1, x2, x3, calcoliamo la media aritmetica delle loro intercette:\n\n mean_observed_scores &lt;- mean(intercepts[1:3])\n print(mean_observed_scores)\n\n[1] 4.424742\n\n\nQuesto valore rappresenta la media osservata calcolata come la media aritmetica delle intercette di x1, x2, x3. Nel contesto del nostro modello CFA, dove la media dei costrutti latenti è fissata a zero, la media predetta degli indicatori corrisponde alla media osservata:\n\nmean((HolzingerSwineford1939$x1 + HolzingerSwineford1939$x2 + HolzingerSwineford1939$x3) / 3) \n\n4.42474160196013\n\n\n\nmean_predicted_scores &lt;- mean_observed_scores\nprint(mean_predicted_scores)\n\n[1] 4.424742\n\n\n\n\n34.3.3 Medie di Costrutti Latenti Non Zero\nIn situazioni in cui le medie dei costrutti latenti non sono fissate a zero, la media predetta degli indicatori è influenzata sia dalle intercette sia dai carichi fattoriali. Per esempio, se la media del costrutto latente fosse diversa da zero, l’equazione per calcolare la media di un indicatore (come x1) includerebbe il contributo del costrutto latente:\n\\[\n\\text{media predetta}(x1) = \\mu_{\\text{latente}} \\cdot \\lambda_{x1} + \\tau_{x1},\n\\]\ndove:\n\n\\(\\mu_{\\text{latente}}\\) è la media stimata del costrutto latente.\n\\(\\lambda_{x1}\\) è il carico dell’indicatore x1.\n\\(\\tau_{x1}\\) è l’intercetta stimata dell’indicatore x1.\n\nEsaminiamo un esempio nel quale le medie dei fattori latenti non sono fissate a zero. Per ottenere questo risultato è necessario identificare il modello introducendo due vincoli:\n\nl’intercetta degli indicatori è fissata a zero;\nuna delle intercette delle variabili latenti è fissata a zero.\n\n\nhs_model &lt;- \"\n    visual =~ NA*x1 + x2 + x3\n    textual =~ NA*x4 + x5 + x6\n    speed =~ NA*x7 + x8 + x9\n\n    visual ~~ 1*visual\n    textual ~~ 1*textual\n    speed ~~ 1*speed\n\n    x1 ~ 0*1 # Setting the intercepts of the manifest\n    x4 ~ 0*1 # variables to zero\n    x7 ~ 0*1\n\n    visual ~ 0*1 # Setting the mean of visual to zero\n    textual ~ 1 # freely estimating the mean of textual\n    speed ~ 1 # freely estimating the mean of speed\n\"\n\n\n# Fit del modello con la struttura delle medie\nfit &lt;- cfa(hs_model, data = HolzingerSwineford1939, meanstructure = TRUE)\n\n\nparams &lt;- parameterEstimates(fit)\nprint(params)\n\n       lhs op     rhs    est    se       z pvalue ci.lower ci.upper\n1   visual =~      x1  4.983 0.212  23.538  0.000    4.568    5.398\n2   visual =~      x2  1.702 0.093  18.248  0.000    1.519    1.885\n3   visual =~      x3  2.243 0.106  21.099  0.000    2.035    2.451\n4  textual =~      x4  1.783 0.081  21.945  0.000    1.624    1.942\n5  textual =~      x5  1.985 0.090  22.007  0.000    1.808    2.162\n6  textual =~      x6  1.652 0.076  21.702  0.000    1.502    1.801\n7    speed =~      x7  1.136 0.072  15.882  0.000    0.996    1.277\n8    speed =~      x8  1.341 0.070  19.127  0.000    1.204    1.478\n9    speed =~      x9  1.229 0.068  17.979  0.000    1.095    1.363\n10  visual ~~  visual  1.000 0.000      NA     NA    1.000    1.000\n11 textual ~~ textual  1.000 0.000      NA     NA    1.000    1.000\n12   speed ~~   speed  1.000 0.000      NA     NA    1.000    1.000\n13      x1 ~1          0.000 0.000      NA     NA    0.000    0.000\n14      x4 ~1          0.000 0.000      NA     NA    0.000    0.000\n15      x7 ~1          0.000 0.000      NA     NA    0.000    0.000\n16  visual ~1          0.000 0.000      NA     NA    0.000    0.000\n17 textual ~1          0.885 0.054  16.405  0.000    0.779    0.990\n18   speed ~1          2.845 0.187  15.207  0.000    2.478    3.211\n19      x1 ~~      x1  0.890 0.257   3.457  0.001    0.385    1.394\n20      x2 ~~      x2  1.134 0.100  11.368  0.000    0.938    1.329\n21      x3 ~~      x3  0.844 0.087   9.719  0.000    0.674    1.015\n22      x4 ~~      x4  0.371 0.045   8.239  0.000    0.283    0.459\n23      x5 ~~      x5  0.446 0.055   8.116  0.000    0.338    0.554\n24      x6 ~~      x6  0.356 0.041   8.679  0.000    0.276    0.437\n25      x7 ~~      x7  0.799 0.077  10.411  0.000    0.649    0.950\n26      x8 ~~      x8  0.488 0.062   7.915  0.000    0.367    0.608\n27      x9 ~~      x9  0.566 0.062   9.132  0.000    0.445    0.688\n28  visual ~~ textual  0.870 0.016  53.410  0.000    0.838    0.902\n29  visual ~~   speed  0.877 0.019  47.025  0.000    0.840    0.913\n30 textual ~~   speed  0.783 0.027  28.490  0.000    0.729    0.837\n31      x2 ~1          4.460 0.064  69.660  0.000    4.335    4.586\n32      x3 ~1          0.106 0.058   1.814  0.070   -0.008    0.220\n33      x5 ~1          0.934 0.075  12.485  0.000    0.787    1.080\n34      x6 ~1         -0.649 0.064 -10.085  0.000   -0.775   -0.523\n35      x8 ~1          0.588 0.236   2.496  0.013    0.126    1.050\n36      x9 ~1          0.847 0.226   3.742  0.000    0.403    1.291\n\n\nIn sintesi, la media predetta degli indicatori in un modello SEM può variare a seconda della configurazione delle medie dei costrutti latenti e del contributo dei carichi fattoriali.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>La struttura delle medie</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/02_meanstructure.html#session-info",
    "href": "chapters/cfa/02_meanstructure.html#session-info",
    "title": "34  La struttura delle medie",
    "section": "34.4 Session Info",
    "text": "34.4 Session Info\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.4.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ggokabeito_0.1.0  viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n [5] ggExtra_0.10.1    bayesplot_1.11.1  gridExtra_2.3     patchwork_1.2.0  \n [9] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-17     psych_2.4.3      \n[13] scales_1.3.0      markdown_1.12     knitr_1.45        lubridate_1.9.3  \n[17] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[21] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.0    \n[25] tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.15.0  jsonlite_1.8.8     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5   nloptr_2.0.3      \n  [7] rmarkdown_2.26     vctrs_0.6.5        minqa_1.2.6       \n [10] base64enc_0.1-3    rstatix_0.7.2      htmltools_0.5.7   \n [13] broom_1.0.5        Formula_1.2-5      htmlwidgets_1.6.4 \n [16] plyr_1.8.9         sandwich_3.1-0     emmeans_1.10.0    \n [19] zoo_1.8-12         uuid_1.2-0         igraph_2.0.2      \n [22] mime_0.12          lifecycle_1.0.4    pkgconfig_2.0.3   \n [25] Matrix_1.6-5       R6_2.5.1           fastmap_1.1.1     \n [28] shiny_1.8.0        digest_0.6.35      OpenMx_2.21.11    \n [31] fdrtool_1.2.17     colorspace_2.1-0   rprojroot_2.0.4   \n [34] Hmisc_5.1-1        fansi_1.0.6        timechange_0.3.0  \n [37] abind_1.4-5        compiler_4.3.3     withr_3.0.0       \n [40] glasso_1.11        htmlTable_2.4.2    backports_1.4.1   \n [43] carData_3.0-5      ggsignif_0.6.4     MASS_7.3-60.0.1   \n [46] corpcor_1.6.10     gtools_3.9.5       tools_4.3.3       \n [49] pbivnorm_0.6.0     foreign_0.8-86     zip_2.3.1         \n [52] httpuv_1.6.14      nnet_7.3-19        glue_1.7.0        \n [55] quadprog_1.5-8     nlme_3.1-164       promises_1.2.1    \n [58] lisrelToR_0.3      grid_4.3.3         pbdZMQ_0.3-11     \n [61] checkmate_2.3.1    cluster_2.1.6      reshape2_1.4.4    \n [64] generics_0.1.3     gtable_0.3.4       tzdb_0.4.0        \n [67] data.table_1.15.2  hms_1.1.3          car_3.1-2         \n [70] utf8_1.2.4         sem_3.1-15         pillar_1.9.0      \n [73] IRdisplay_1.1      rockchalk_1.8.157  later_1.3.2       \n [76] splines_4.3.3      lattice_0.22-5     survival_3.5-8    \n [79] kutils_1.73        tidyselect_1.2.0   miniUI_0.1.1.1    \n [82] pbapply_1.7-2      stats4_4.3.3       xfun_0.42         \n [85] qgraph_1.9.8       arm_1.13-1         stringi_1.8.3     \n [88] boot_1.3-29        evaluate_0.23      codetools_0.2-19  \n [91] mi_1.1             cli_3.6.2          RcppParallel_5.1.7\n [94] IRkernel_1.3.2     rpart_4.1.23       xtable_1.8-4      \n [97] repr_1.1.6         munsell_0.5.0      Rcpp_1.0.12       \n[100] coda_0.19-4.1      png_0.1-8          XML_3.99-0.16.1   \n[103] parallel_4.3.3     ellipsis_0.3.2     jpeg_0.1-10       \n[106] lme4_1.1-35.1      mvtnorm_1.2-4      openxlsx_4.2.5.2  \n[109] crayon_1.5.2       rlang_1.1.3        multcomp_1.4-25   \n[112] mnormt_2.1.1",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>La struttura delle medie</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/03_cat_data.html",
    "href": "chapters/cfa/03_cat_data.html",
    "title": "34  Dati non gaussiani e categoriali",
    "section": "",
    "text": "34.1 Dati non Gaussiani e Stimatori Alternativi\nNonostante la stima di massima verosimiglianza (ML) rimanga robusta a piccole deviazioni dalla normalità, situazioni di marcata non normalità richiedono l’adozione di stimatori alternativi per preservare l’affidabilità statistica. L’uso del ML in tali condizioni può portare a:\nQuesti problemi si accentuano in campioni di dimensioni ridotte. Per mitigare tali effetti, si raccomanda l’uso dei seguenti stimatori:\nIn sintesi, questi stimatori vengono utilizzati per valutare quanto bene un modello SEM si adatti ai dati. Differiscono nel modo in cui trattano le discrepanze tra i dati osservati e quelli stimati dal modello, e ciascuno ha specifiche situazioni in cui risulta più appropriato.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Dati non gaussiani e categoriali</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/03_cat_data.html#dati-non-gaussiani-e-stimatori-alternativi",
    "href": "chapters/cfa/03_cat_data.html#dati-non-gaussiani-e-stimatori-alternativi",
    "title": "34  Dati non gaussiani e categoriali",
    "section": "",
    "text": "Sovrastima della statistica chi-quadrato (\\(\\chi^2\\)) del modello;\nSottostima degli indici di bontà di adattamento, come il Tucker-Lewis Index (TLI) e il Comparative Fit Index (CFI);\nSottostima degli errori standard delle stime dei parametri.\n\n\n\nGLS (Generalized Least Squares):\n\nUso: Adatto per dati completi senza valori mancanti.\nFunzione di Discrepanza: La funzione di discrepanza del GLS misura quanto la matrice di covarianza stimata dal modello (\\(\\Sigma(\\theta)\\)) si differenzia dalla matrice di covarianza osservata (\\(S\\)). La formula \\(F_{\\text{GLS}}(S, \\Sigma(\\theta)) = \\frac{1}{2} \\text{traccia}(S - \\Sigma(\\theta))^2\\) utilizza la traccia (la somma degli elementi sulla diagonale principale della matrice) per quantificare questa differenza.\nInterpretazione: Un valore più basso della funzione di discrepanza indica un migliore adattamento del modello ai dati.\n\nWLS (Weighted Least Squares):\n\nUso: Conosciuto come stimatore Asintoticamente Libero da Distribuzione (ADF), utile per dati complessi.\nFunzione di Discrepanza: \\(F_{\\text{ADF}}(S, \\Sigma(\\theta)) = \\text{vecs}(S - \\Sigma(\\theta))'W\\text{vecs}(S - \\Sigma(\\theta))\\). Qui, vecs() trasforma la matrice di covarianza in un vettore (prendendo solo la parte inferiore della matrice), e W è una matrice di pesi che dà diversa importanza ai vari elementi nel calcolo della discrepanza.\nInterpretazione: Un valore più basso indica che il modello si adatta meglio ai dati, tenendo conto della ponderazione specifica di W.\n\nDWLS (Diagonally Weighted Least Squares):\n\nUso: Una versione semplificata di WLS.\nFunzione di Discrepanza: \\(F_{\\text{DWLS}}(S, \\Sigma(\\theta)) = \\text{vecs}(S - \\Sigma(\\theta))'D\\text{vecs}(S - \\Sigma(\\theta))\\), dove D è una matrice di pesi diagonale.\nInterpretazione: Simile a WLS, ma semplifica i calcoli usando solo una matrice di pesi diagonale, che considera solo gli elementi sulla diagonale della matrice di covarianza.\n\nULS (Unweighted Least Squares):\n\nUso: Considerato un caso speciale di WLS.\nFunzione di Discrepanza: \\(F_{\\text{ULS}}(S, \\Sigma(\\theta)) = \\text{vecs}(S - \\Sigma(\\theta))'\\text{vecs}(S - \\Sigma(\\theta))\\). Qui, si utilizza una matrice di identità come peso, il che significa che tutti gli elementi hanno lo stesso peso nel calcolo della discrepanza.\nInterpretazione: Un approccio più diretto rispetto a WLS, che non pondera gli elementi in modo diverso. Un valore più basso indica un migliore adattamento del modello.\n\n\n\n\n34.1.1 ML Robusto: Adattamento in Presenza di Non Normalità\nOltre ai quattro metodi di stima già menzionati (GLS, WLS, DWLS, ULS), un altro stimatore importante nel contesto del Structural Equation Modeling (SEM) è il ML Robusto (Robust Maximum Likelihood). Il ML Robusto è una variante della stima di massima verosimiglianza tradizionale, progettata per migliorare l’affidabilità statistica quando i dati deviano significativamente dalla normalità. Questo stimatore: - Corregge la Sovrastima di \\(\\chi^2\\): Offre una correzione alla sovrastima della statistica chi-quadrato tipica del ML tradizionale. - Errore Standard Affidabile: Fornisce stime più accurate degli errori standard, cruciali in presenza di non normalità. - Migliora Indici di Bontà di Adattamento: Offre valutazioni più precise di indici come TLI e CFI.\nIn conclusione, l’adozione di stimatori come il ML Robusto o il WLS si rivela essenziale per garantire l’integrità delle analisi SEM in presenza di dati non normali, specialmente quando le dimensioni del campione sono limitate o i dati presentano caratteristiche complesse.\nEsempio. Esaminiamo qui un esempio discusso da {cite:t}brown2015confirmatory (tabelle 9.5 – 9.7).\n\nd &lt;- readRDS(here::here(\"data\", \"brown_table_9_5_data.RDS\"))\nhead(d)\n\n\nA data.frame: 6 x 5\n\n\n\nx1\nx2\nx3\nx4\nx5\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n0\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n\n\n3\n0\n0\n0\n0\n0\n\n\n4\n4\n2\n2\n1\n1\n\n\n5\n1\n0\n1\n6\n0\n\n\n6\n0\n0\n0\n0\n0\n\n\n\n\n\nLe statistiche descrittive di questo campione di dati mostrano valori eccessivi di asimmetria e di curtosi.\n\npsych::describe(d)\n\n\nA psych: 5 x 13\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nx1\n1\n870\n1.4701149\n2.172832\n0\n1.0086207\n0\n0\n8\n8\n1.506406\n1.252591\n0.07366591\n\n\nx2\n2\n870\n0.8229885\n1.601474\n0\n0.4152299\n0\n0\n8\n8\n2.398394\n5.670143\n0.05429505\n\n\nx3\n3\n870\n1.2655172\n2.070024\n0\n0.7772989\n0\n0\n8\n8\n1.797942\n2.343203\n0.07018040\n\n\nx4\n4\n870\n1.0264368\n1.928047\n0\n0.5359195\n0\n0\n8\n8\n2.157445\n3.977564\n0.06536693\n\n\nx5\n5\n870\n0.6068966\n1.519175\n0\n0.1839080\n0\n0\n8\n8\n3.103965\n9.373781\n0.05150485\n\n\n\n\n\nDefiniamo un modello ad un fattore e, seguendo {cite:t}brown2015confirmatory, aggiungiamo una correlazione residua tra gli indicatori X1 e X3:\n\nmodel &lt;- '\n  f1 =~ x1 + x2 + x3 + x4 + x5\n  x1 ~~ x3 \n'\n\nProcediamo alla stima dei parametri utilizzando uno stimatore di ML robusto. La sintassi lavaan è la seguente:\n\nfit &lt;- cfa(model, data = d, mimic = \"MPLUS\", estimator = \"MLM\")\n\nPer esaminare la soluzione ottenuta ci focalizziamo sulla statistica \\(\\chi^2\\) – si consideri la soluzione robusta fornita nell’output.\n\nout &lt;- summary(fit)\nprint(out)\n\nlavaan 0.6.17 ended normally after 28 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        16\n\n  Number of observations                           870\n\nModel Test User Model:\n                                               Standard      Scaled\n  Test Statistic                                 25.913      10.356\n  Degrees of freedom                                  4           4\n  P-value (Chi-square)                            0.000       0.035\n  Scaling correction factor                                   2.502\n    Satorra-Bentler correction (Mplus variant)                     \n\nParameter Estimates:\n\n  Standard errors                           Robust.sem\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  f1 =~                                               \n    x1                1.000                           \n    x2                0.703    0.062   11.338    0.000\n    x3                1.068    0.044   24.304    0.000\n    x4                0.918    0.063   14.638    0.000\n    x5                0.748    0.055   13.582    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n .x1 ~~                                               \n   .x3                0.655    0.143    4.579    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                1.470    0.074   19.968    0.000\n   .x2                0.823    0.054   15.166    0.000\n   .x3                1.266    0.070   18.043    0.000\n   .x4                1.026    0.065   15.712    0.000\n   .x5                0.607    0.051   11.790    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                2.040    0.228    8.952    0.000\n   .x2                1.241    0.124   10.019    0.000\n   .x3                1.227    0.169    7.255    0.000\n   .x4                1.458    0.177    8.233    0.000\n   .x5                0.807    0.100    8.063    0.000\n    f1                2.675    0.289    9.273    0.000\n\n\n\nPer fare un confronto, adattiamo lo stesso modello ai dati usando lo stimatore di ML.\n\nfit2 &lt;- cfa(model, data = d)\n\nNotiamo come il valore della statistica \\(\\chi^2\\) ora ottenuto sia molto maggiore di quello trovato in precedenza.\n\nout &lt;- summary(fit2)\nprint(out)\n\nlavaan 0.6.17 ended normally after 28 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        11\n\n  Number of observations                           870\n\nModel Test User Model:\n                                                      \n  Test statistic                                25.913\n  Degrees of freedom                                 4\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  f1 =~                                               \n    x1                1.000                           \n    x2                0.703    0.035   20.133    0.000\n    x3                1.068    0.034   31.730    0.000\n    x4                0.918    0.042   21.775    0.000\n    x5                0.748    0.033   22.416    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n .x1 ~~                                               \n   .x3                0.655    0.091    7.213    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                2.040    0.128   15.897    0.000\n   .x2                1.241    0.070   17.671    0.000\n   .x3                1.227    0.095   12.942    0.000\n   .x4                1.458    0.090   16.135    0.000\n   .x5                0.807    0.053   15.119    0.000\n    f1                2.675    0.220   12.154    0.000\n\n\n\n\n\n34.1.2 Dati Categoriali\nNella discussione precedente, abbiamo esaminato il modello CFA presupponendo che i dati fossero continui e normalmente distribuiti in maniera multivariata. Tuttavia, abbiamo anche trattato la stima robusta per dati non normalmente distribuiti. Ora, è fondamentale riconoscere che molti dei dati utilizzati nelle analisi fattoriali confermative (CFA) o SEM provengono da questionari e scale di tipo Likert, che producono dati categoriali, inclusi formati binari, ordinali e nominali. Questi dati sono di natura ordinale e non sono continui.\nL’uso del metodo di massima verosimiglianza (ML) ordinario non è raccomandato quando si analizzano dati con almeno un indicatore categoriale. Trattare tali variabili come se fossero continue può portare a varie conseguenze indesiderate, tra cui:\n\nStime Attenuate delle Relazioni: Le relazioni tra gli indicatori possono risultare attenuate, specialmente se influenzate da effetti di pavimento o soffitto.\nEmergenza di “Pseudo-Fattori”: La possibilità di identificare falsi fattori, che non rappresentano veri costrutti ma sono piuttosto artefatti del metodo statistico utilizzato.\nDistorsione degli Indici di Bontà di Adattamento e delle Stime degli Errori Standard: Questi indici, che valutano la qualità dell’adattamento del modello, possono essere distorti, così come le stime degli errori standard.\nStime Errate dei Parametri: I parametri del modello potrebbero essere stimati in modo inaccurato.\n\nPer mitigare questi problemi, esistono stimatori specifici per i dati categoriali, tra cui:\n\nWLS (Weighted Least Squares): Adatto per dati categoriali, considera il peso specifico di ciascuna osservazione.\nWLSMV (Weighted Least Squares Mean and Variance Adjusted): Una versione modificata di WLS che si adatta meglio alle peculiarità dei dati categoriali.\nULS (Unweighted Least Squares): Questo stimatore non prevede ponderazioni e può essere utile per dati categoriali senza presupporre pesi specifici.\n\nNelle sezioni seguenti, approfondiremo l’approccio CFA per dati categoriali, evidenziando le specificità e le migliori pratiche per gestire questo tipo di dati nelle analisi CFA. Questo ci permetterà di effettuare inferenze più accurate, preservando l’integrità e la validità delle conclusioni derivanti dalle analisi.\n\n\n34.1.3 Un esempio concreto\nNell’esempio discusso da {cite:t}brown2015confirmatory, i ricercatori desiderano verificare un modello uni-fattoriale di dipendenza da alcol in un campione di 750 pazienti ambulatoriali. Gli indicatori di alcolismo sono item binari che riflettono la presenza/assenza di sei criteri diagnostici per l’alcolismo (0 = criterio non soddisfatto, 1 = criterio soddisfatto). I dati sono i seguenti:\n\nd1 &lt;- readRDS(here::here(\"data\", \"brown_table_9_9_data.RDS\"))\nhead(d1)\n\n\nA data.frame: 6 x 6\n\n\n\ny1\ny2\ny3\ny4\ny5\ny6\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n1\n1\n1\n1\n1\n1\n\n\n2\n1\n1\n1\n1\n1\n1\n\n\n3\n1\n1\n1\n1\n1\n0\n\n\n4\n1\n1\n1\n1\n1\n1\n\n\n5\n0\n0\n0\n0\n0\n0\n\n\n6\n1\n1\n0\n1\n1\n1\n\n\n\n\n\nÈ possibile evidenziare la natura ordinale dei dati esaminando le tabelle bivariate che mostrano la frequenza di combinazioni specifiche tra due variabili.\n\nxtabs(~ y1 + y2, d1)\n\n   y2\ny1    0   1\n  0 103  65\n  1 156 426\n\n\n\nxtabs(~ y3 + y4, d1)\n\n   y4\ny3    0   1\n  0  41  39\n  1 119 551\n\n\n\nxtabs(~ y5 + y6, d1)\n\n   y6\ny5    0   1\n  0  95 168\n  1  60 427\n\n\nNelle tabelle precedenti, si osserva una maggiore frequenza di casi in cui entrambe le variabili assumono il valore 1, rispetto ai casi in cui entrambe sono 0 o in cui una è 1 e l’altra è 0. Questo suggerisce l’esistenza di una relazione ordinale tra le coppie di variabili nel dataset.\n\n\n34.1.4 Il Modello Basato sulle Soglie per Risposte Categoriali Ordinate\nIl modello basato sulle soglie per risposte categoriali ordinate si basa sull’idea che ogni risposta di una variabile categoriale possa essere vista come il risultato di una variabile continua non osservata, che è normalmente distribuita. Questa variabile nascosta, chiamata variabile latente, rappresenta la tendenza di una persona a rispondere in un determinato modo. Le risposte che vediamo, classificate in categorie, sono in realtà approssimazioni di questa variabile latente.\nImmaginiamo di utilizzare un questionario dove le risposte sono su una scala Likert a 7 punti. Questo crea una variabile categoriale con sette categorie ordinate. Se denotiamo con I un particolare item del questionario e con I* la sua corrispondente variabile latente non osservabile, possiamo descrivere il loro legame attraverso le seguenti equazioni, che mappano la variabile latente alle risposte osservabili:\n\\[\n\\begin{align*}\nI &= 1 \\quad \\text{se} \\quad -\\infty &lt; I^* \\leq t_1 \\\\\nI &= 2 \\quad \\text{se} \\quad t_1 &lt; I^* \\leq t_2 \\\\\nI &= 3 \\quad \\text{se} \\quad t_2 &lt; I^* \\leq t_3 \\\\\nI &= 4 \\quad \\text{se} \\quad t_3 &lt; I^* \\leq t_4 \\\\\nI &= 5 \\quad \\text{se} \\quad t_4 &lt; I^* \\leq t_5 \\\\\nI &= 6 \\quad \\text{se} \\quad t_5 &lt; I^* \\leq t_6 \\\\\nI &= 7 \\quad \\text{se} \\quad t_6 &lt; I^* &lt; \\infty\n\\end{align*}\n\\]\nIn queste equazioni, $ t_i $ (con i da 1 a 6) rappresenta le soglie che dividono l’intero spettro della variabile latente in sette categorie. Le soglie sono disposte in modo che $ -&lt; t_1 &lt; t_2 &lt; t_3 &lt; t_4 &lt; t_5 &lt; t_6 &lt; $. È importante notare che il numero di soglie è sempre uno in meno rispetto al numero di categorie, un po’ come il numero di variabili dummy usate nell’analisi di regressione per codificare una variabile categoriale.\nQuesto processo di categorizzazione può essere visualizzato come segue: si immagini una curva normale che rappresenta la distribuzione della variabile latente I. Le sei linee verticali nella figura rappresentano le soglie $ t_1 $ a $ t_6 $. Le risposte possibili vanno da I = 1 a I = 7, e la categoria specifica (I) dipende dall’intervallo, definito dalle soglie, in cui il valore di I si trova.\n\n# Definire le soglie\nthresholds &lt;- c(-3, -2, -1, 0, 1, 2, 3)\n\n# Creare un dataframe per la curva normale\nx_values &lt;- seq(-4, 4, length.out = 300)\ny_values &lt;- dnorm(x_values)\ncurve_data &lt;- data.frame(x = x_values, y = y_values)\n\n# Creare il plot\nggplot(curve_data, aes(x = x, y = y)) +\n    geom_line() +\n    geom_vline(xintercept = thresholds, col = \"red\") +\n    scale_y_continuous(breaks = NULL) +\n    scale_x_continuous(breaks = thresholds, labels = c(\"t1\", \"t2\", \"t3\", \"t4\", \"t5\", \"t6\", \"t7\")) +\n    labs(\n        title = \"Categorization of Latent Continuous Variable to Categorical Variable\",\n        x = \"Latent Continuous Variable I*\",\n        y = \"\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\nLa conversione della variabile latente $ I^* $ in dati su una scala Likert comporta inevitabilmente degli errori di misurazione e campionamento. Come evidenziato da O’Brien (1985), questo processo di categorizzazione introduce due tipi principali di errore:\n\nErrore di categorizzazione: Questo errore deriva dalla segmentazione di una scala continua in una scala categoriale, dove la variabile latente viene divisa in categorie distinte.\nErrore di trasformazione: Questo errore emerge quando le categorie hanno larghezze disuguali, influenzando la fedeltà della rappresentazione delle misure originali della variabile latente.\n\nDi conseguenza, è fondamentale che le soglie siano stimate contemporaneamente agli altri parametri nel modello di equazioni strutturali per garantire che tali errori siano minimizzati e che l’analisi rifletta accuratamente la realtà sottostante.\n\n\n34.1.5 Modellazione di Variabili Categoriali nei Modelli CFA\nNell’ambito dei modelli CFA, le variabili categoriali ordinate vengono spesso modellate collegandole a una variabile latente sottostante, denominata $ I^* $. Questa variabile latente rappresenta una sorta di “propensione nascosta” che influisce sulle risposte osservate nelle variabili categoriali.\nPer esemplificare, consideriamo il seguente modello che esprime la variabile latente $ I^* $ attraverso una serie di predittori (x1, x2, …, xp), ognuno dei quali contribuisce all’esito con un effetto quantificato dai coefficienti $ _1, _2, …, _P $:\n\\[\nI^*_i = \\beta_0 + \\beta_1 x_{1i} + \\dots + \\beta_P x_{Pi} + e_i.\n\\]\nIn questa equazione: - $ I^*_i $ indica la propensione latente per l’osservatore $ i $. - $ _0 $ è un termine costante che agisce come intercetta. - $ _1, , _P $ sono i coefficienti che misurano l’impatto di ciascun predittore sulla propensione latente. - $ e_i $ è il termine di errore che rappresenta le variazioni non spiegate dai predittori.\nQuando la variabile categoriale $ I $ funge da indicatore di un fattore latente $ $ in un modello fattoriale confermativo, la formulazione dell’equazione si semplifica a:\n\\[\nI^*_i = \\beta_0 + \\beta_1 \\xi_i + e_i.\n\\]\nIn questa configurazione, $ _1 $ rappresenta il carico fattoriale, indicando quanto fortemente il fattore latente $ $ influisce sulla variabile latente $ I^* $. Questo schema è analogo a quello usato per modellare indicatori di misurazione continui nei modelli SEM.\nQuesto approccio riflette l’idea che le risposte categoriali osservabili possono essere considerate come manifestazioni esterne di una propensione interna latente. Per la stima di tali modelli, il metodo dei minimi quadrati ponderati (WLS) è generalmente appropriato. Tuttavia, è importante tenere presente che la modellazione di risposte categoriali ordinate può richiedere considerazioni aggiuntive per gestire adeguatamente la loro natura ordinale, dettagli che verranno approfonditi nelle sezioni seguenti.\n\n\n34.1.6 Adattamento del Modello con lmer\nSpecifichiamo il modello nel modo seguente:\n\nmodel1 &lt;- '\n  etoh =~ y1 + y2 + y3 + y4 + y5 + y6\n'\n\nNell’analizzare dati ottenuti da scale ordinali, il software lavaan impiega un metodo specializzato per gestire la natura particolare dei dati categoriali. Questo approccio utilizza lo stimatore WLSMV (Weighted Least Squares Mean and Variance Adjusted). La stima dei parametri avviene tramite il metodo dei minimi quadrati ponderati diagonalmente (DWLS), che si concentra sulle componenti diagonali della matrice di peso. Questa specificità rende lo stimatore WLSMV particolarmente adatto per analizzare dati non normali.\nUna caratteristica importante dello stimatore WLSMV è la sua capacità di calcolare errori standard robusti. Questi sono determinati attraverso un metodo che mantiene l’affidabilità delle stime anche quando i dati non soddisfano le tradizionali assunzioni di normalità. Inoltre, le statistiche di test prodotte da WLSMV sono adeguatamente corrette per tenere conto delle variazioni nella media e nella varianza dei dati. Questo tipo di correzione è cruciale per garantire l’accuratezza e la validità delle statistiche di test, specialmente quando la distribuzione dei dati devia dalla normalità.\nIn conclusione, lavaan offre un approccio avanzato per la modellazione di dati categoriali utilizzando lo stimatore WLSMV, che è ottimizzato per rispondere alle esigenze specifiche di questi tipi di dati. Questo si traduce in stime più precise e statistiche di test affidabili, rendendo lavaan uno strumento molto appropriato per l’analisi di dati categoriali complessi.\n\nfit1 &lt;- cfa(\n  model1, \n  data = d1, \n  ordered = names(d1), \n  estimator = \"WLSMVS\", \n  mimic = \"mplus\"\n)\n\nEsaminiamo la soluzione ottenuta:\n\nout = summary(fit1, fit.measures = TRUE)\nprint(out)\n\nlavaan 0.6.17 ended normally after 16 iterations\n\n  Estimator                                       DWLS\n  Optimization method                           NLMINB\n  Number of model parameters                        12\n\n  Number of observations                           750\n\nModel Test User Model:\n                                                  Standard      Scaled\n  Test Statistic                                     5.651       9.540\n  Degrees of freedom                                     9           9\n  P-value (Chi-square)                               0.774       0.389\n  Scaling correction factor                                      0.592\n    mean and variance adjusted correction (WLSMV)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1155.845     694.433\n  Degrees of freedom                                15           9\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.664\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000       0.999\n  Tucker-Lewis Index (TLI)                       1.005       0.999\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000       0.009\n  90 Percent confidence interval - lower         0.000       0.000\n  90 Percent confidence interval - upper         0.028       0.051\n  P-value H_0: RMSEA &lt;= 0.050                    0.999       0.944\n  P-value H_0: RMSEA &gt;= 0.080                    0.000       0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.031       0.031\n\nParameter Estimates:\n\n  Parameterization                               Delta\n  Standard errors                           Robust.sem\n  Information                                 Expected\n  Information saturated (h1) model        Unstructured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  etoh =~                                             \n    y1                1.000                           \n    y2                0.822    0.072   11.392    0.000\n    y3                0.653    0.092    7.097    0.000\n    y4                1.031    0.075   13.703    0.000\n    y5                1.002    0.072   13.861    0.000\n    y6                0.759    0.076   10.011    0.000\n\nThresholds:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    y1|t1            -0.759    0.051  -14.890    0.000\n    y2|t1            -0.398    0.047   -8.437    0.000\n    y3|t1            -1.244    0.061  -20.278    0.000\n    y4|t1            -0.795    0.051  -15.436    0.000\n    y5|t1            -0.384    0.047   -8.148    0.000\n    y6|t1            -0.818    0.052  -15.775    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .y1                0.399                           \n   .y2                0.594                           \n   .y3                0.744                           \n   .y4                0.361                           \n   .y5                0.397                           \n   .y6                0.653                           \n    etoh              0.601    0.063    9.596    0.000\n\n\n\nSi presti particolare attenzione alla seguente porzione dell’output:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    y1|t1            -0.759    0.051  -14.890    0.000\n    y2|t1            -0.398    0.047   -8.437    0.000\n    y3|t1            -1.244    0.061  -20.278    0.000\n    y4|t1            -0.795    0.051  -15.436    0.000\n    y5|t1            -0.384    0.047   -8.148    0.000\n    y6|t1            -0.818    0.052  -15.775    0.000\nIn questa porzione dell’output di lavaan sono presentati i risultati per le “soglie” (thresholds) relative alle variabili categoriali ordinate utilizzate nel modello SEM. Ecco una spiegazione dettagliata:\n\nThresholds (Soglie):\n\nOgni soglia rappresenta un punto di cutoff lungo la variabile continua latente (indicata in precedenza come I*), che determina le categorie della variabile categoriale osservata.\nNell’output, y1|t1, y2|t1, ecc., rappresentano soglie per le variabili rispettive (y1, y2, …, y6). Il termine “t1” si riferisce alla prima soglia per ciascuna di queste variabili.\n\nEstimate (Stima):\n\nQuesti valori indicano la posizione della soglia sulla scala della variabile continua latente. Per esempio, la soglia per y1 è a -0.759. Questo significa che la divisione tra le prime due categorie di y1 si verifica a -0.759 sulla scala della variabile latente.\n\nStd.Err (Errore Standard):\n\nL’errore standard della stima di ogni soglia. Ad esempio, per y1, l’errore standard è 0.051. Questo offre un’idea della variabilità o incertezza nella stima della soglia.\n\nz-value:\n\nIl valore z indica il rapporto tra la stima della soglia e il suo errore standard. Un valore z elevato suggerisce che la stima della soglia è significativamente diversa da zero (ovvero, la soglia è ben definita). Per esempio, per y1, il valore z è -14.890, che è statisticamente significativo.\n\nP(&gt;|z|):\n\nIl p-value associato al valore z. Un p-value basso (ad esempio, 0.000) indica che la stima della soglia è statisticamente significativa. Questo significa che possiamo essere abbastanza sicuri che la posizione della soglia sulla variabile latente sia accurata e non dovuta al caso.\n\n\nIn sintesi, queste soglie consentono di trasformare la variabile latente continua in una variabile categoriale osservata nel modello. La stima di queste soglie e la loro significatività statistica sono cruciali per comprendere come la variabile latente si traduce nelle categorie osservate.\nConfrontiamo ora la soluzione ottenuta con lo stimatore WLSMVS con quella ottenuta mediante lo stimatore ML.\n\nfit2 &lt;- cfa(\n  model1, \n  data = d1\n)\n\n\nout &lt;- summary(fit2, fit.measures = TRUE)\nprint(out)\n\nlavaan 0.6.17 ended normally after 35 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        12\n\n  Number of observations                           750\n\nModel Test User Model:\n                                                      \n  Test statistic                                14.182\n  Degrees of freedom                                 9\n  P-value (Chi-square)                           0.116\n\nModel Test Baseline Model:\n\n  Test statistic                               614.305\n  Degrees of freedom                                15\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.991\n  Tucker-Lewis Index (TLI)                       0.986\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -2087.600\n  Loglikelihood unrestricted model (H1)      -2080.508\n                                                      \n  Akaike (AIC)                                4199.199\n  Bayesian (BIC)                              4254.640\n  Sample-size adjusted Bayesian (SABIC)       4216.535\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.028\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.054\n  P-value H_0: RMSEA &lt;= 0.050                    0.914\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.021\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  etoh =~                                             \n    y1                1.000                           \n    y2                0.934    0.093   10.057    0.000\n    y3                0.390    0.055    7.038    0.000\n    y4                1.008    0.087   11.541    0.000\n    y5                1.158    0.101   11.468    0.000\n    y6                0.700    0.077    9.142    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .y1                0.109    0.007   14.692    0.000\n   .y2                0.169    0.010   16.781    0.000\n   .y3                0.085    0.005   18.483    0.000\n   .y4                0.102    0.007   14.285    0.000\n   .y5                0.140    0.010   14.506    0.000\n   .y6                0.132    0.008   17.514    0.000\n    etoh              0.065    0.009    7.664    0.000\n\n\n\nSi noti che la soluzione ottenuta mediante lo stimatore WLSMVS produce indici di bontà di adattamento migliori e errori standard dei parametri più piccoli.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Dati non gaussiani e categoriali</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/03_cat_data.html#considerazioni-conclusive",
    "href": "chapters/cfa/03_cat_data.html#considerazioni-conclusive",
    "title": "34  Dati non gaussiani e categoriali",
    "section": "34.2 Considerazioni Conclusive",
    "text": "34.2 Considerazioni Conclusive\nIn questo capitolo, abbiamo esplorato la modellazione CFA con dati non normalmente distribuiti. È essenziale riconoscere che, nella pratica analitica, incontrare dati non normalmente distribuiti dovrebbe essere considerato normale. Di conseguenza, si raccomanda l’utilizzo della massima verosimiglianza robusta (ML robusta) ogni volta che sorgono dubbi sulla normalità dei dati.\nCi sono alcune considerazioni importanti da tenere presente: 1. Stabilità delle stime di parametro: Anche se le versioni robuste di ML forniscono errori standard robusti e statistiche di test adattate, le stime dei parametri ottenute rimangono quelle della stima ML originale. 2. Robustezza limitata: Gli aggiustamenti robusti compensano la violazione della normalità, ma non coprono la presenza di valori anomali, che richiedono un’analisi separata. 3. Limitazioni degli aggiustamenti: Gli aggiustamenti robusti non trattano violazioni delle specifiche del modello, che è un altro argomento di discussione nella letteratura CFA e SEM.\nAbbiamo anche discusso l’uso dello stimatore WLSMV per dati categoriali, evidenziando come esso fornisca una stima dell’errore standard più precisa rispetto all’MLE standard e all’MLE robusta.\nVa notato che WLSMV è un metodo generale per dati categoriali nella CFA, ampiamente implementato in software come MPlus. In lavaan, l’uso di WLSMV può essere attivato semplicemente con lavaan(..., estimator = \"WLSMV\"), equivalente a lavaan(..., estimator = \"DWLS\", se = \"robust.sem\", test = \"scaled.shifted\").\nOltre al WLSMV, lavaan offre anche lo stimatore sperimentale di massima verosimiglianza marginale (MML), che, pur essendo preciso, può essere lento e più suscettibile a problemi di convergenza a causa della complessità dell’integrazione numerica. Un altro stimatore è l’ADF (estimator = “WLS”), che non assume specifiche distributive sui dati, ma richiede una dimensione campionaria molto grande (N &gt; 5000) per considerare affidabili le stime dei parametri, gli errori standard e le statistiche di test.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Dati non gaussiani e categoriali</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/03_cat_data.html#session-info",
    "href": "chapters/cfa/03_cat_data.html#session-info",
    "title": "34  Dati non gaussiani e categoriali",
    "section": "34.3 Session Info",
    "text": "34.3 Session Info\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.4.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ggokabeito_0.1.0  viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n [5] ggExtra_0.10.1    bayesplot_1.11.1  gridExtra_2.3     patchwork_1.2.0  \n [9] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-17     psych_2.4.3      \n[13] scales_1.3.0      markdown_1.12     knitr_1.45        lubridate_1.9.3  \n[17] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[21] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.0    \n[25] tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.15.0  jsonlite_1.8.8     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5   farver_2.1.1      \n  [7] nloptr_2.0.3       rmarkdown_2.26     vctrs_0.6.5       \n [10] minqa_1.2.6        base64enc_0.1-3    rstatix_0.7.2     \n [13] htmltools_0.5.7    broom_1.0.5        Formula_1.2-5     \n [16] htmlwidgets_1.6.4  plyr_1.8.9         sandwich_3.1-0    \n [19] emmeans_1.10.0     zoo_1.8-12         uuid_1.2-0        \n [22] igraph_2.0.2       mime_0.12          lifecycle_1.0.4   \n [25] pkgconfig_2.0.3    Matrix_1.6-5       R6_2.5.1          \n [28] fastmap_1.1.1      shiny_1.8.0        digest_0.6.35     \n [31] OpenMx_2.21.11     fdrtool_1.2.17     colorspace_2.1-0  \n [34] rprojroot_2.0.4    Hmisc_5.1-1        fansi_1.0.6       \n [37] timechange_0.3.0   abind_1.4-5        compiler_4.3.3    \n [40] withr_3.0.0        glasso_1.11        htmlTable_2.4.2   \n [43] backports_1.4.1    carData_3.0-5      ggsignif_0.6.4    \n [46] MASS_7.3-60.0.1    corpcor_1.6.10     gtools_3.9.5      \n [49] tools_4.3.3        pbivnorm_0.6.0     foreign_0.8-86    \n [52] zip_2.3.1          httpuv_1.6.14      nnet_7.3-19       \n [55] glue_1.7.0         quadprog_1.5-8     nlme_3.1-164      \n [58] promises_1.2.1     lisrelToR_0.3      grid_4.3.3        \n [61] pbdZMQ_0.3-11      checkmate_2.3.1    cluster_2.1.6     \n [64] reshape2_1.4.4     generics_0.1.3     gtable_0.3.4      \n [67] tzdb_0.4.0         data.table_1.15.2  hms_1.1.3         \n [70] car_3.1-2          utf8_1.2.4         sem_3.1-15        \n [73] pillar_1.9.0       IRdisplay_1.1      rockchalk_1.8.157 \n [76] later_1.3.2        splines_4.3.3      lattice_0.22-5    \n [79] survival_3.5-8     kutils_1.73        tidyselect_1.2.0  \n [82] miniUI_0.1.1.1     pbapply_1.7-2      stats4_4.3.3      \n [85] xfun_0.42          qgraph_1.9.8       arm_1.13-1        \n [88] stringi_1.8.3      boot_1.3-29        evaluate_0.23     \n [91] codetools_0.2-19   mi_1.1             cli_3.6.2         \n [94] RcppParallel_5.1.7 IRkernel_1.3.2     rpart_4.1.23      \n [97] xtable_1.8-4       repr_1.1.6         munsell_0.5.0     \n[100] Rcpp_1.0.12        coda_0.19-4.1      png_0.1-8         \n[103] XML_3.99-0.16.1    parallel_4.3.3     ellipsis_0.3.2    \n[106] jpeg_0.1-10        lme4_1.1-35.1      mvtnorm_1.2-4     \n[109] openxlsx_4.2.5.2   crayon_1.5.2       rlang_1.1.3       \n[112] multcomp_1.4-25    mnormt_2.1.1",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Dati non gaussiani e categoriali</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/04_mmm.html",
    "href": "chapters/cfa/04_mmm.html",
    "title": "35  CFA per matrici multi-tratto multi-metodo",
    "section": "",
    "text": "35.0.1 MTMM e CFA\nLa Matrice Multi-Tratto Multi-Metodo (MTMM) è un approccio utilizzato per valutare la validità di costrutto, esaminando la correlazione tra diversi costrutti misurati sia con gli stessi metodi sia con metodi differenti. La validità di costrutto è considerata alta quando la misura di un costrutto è indipendente dal metodo di misurazione utilizzato.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>CFA per matrici multi-tratto multi-metodo</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/04_mmm.html#session-info",
    "href": "chapters/cfa/04_mmm.html#session-info",
    "title": "35  CFA per matrici multi-tratto multi-metodo",
    "section": "35.1 Session Info",
    "text": "35.1 Session Info\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.4.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ggokabeito_0.1.0  viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n [5] ggExtra_0.10.1    bayesplot_1.11.1  gridExtra_2.3     patchwork_1.2.0  \n [9] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-17     psych_2.4.3      \n[13] scales_1.3.0      markdown_1.12     knitr_1.45        lubridate_1.9.3  \n[17] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[21] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.0    \n[25] tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.15.0  jsonlite_1.8.8     datawizard_0.9.1  \n  [4] magrittr_2.0.3     TH.data_1.1-2      estimability_1.5  \n  [7] nloptr_2.0.3       rmarkdown_2.26     vctrs_0.6.5       \n [10] minqa_1.2.6        effectsize_0.8.7   base64enc_0.1-3   \n [13] rstatix_0.7.2      htmltools_0.5.7    broom_1.0.5       \n [16] Formula_1.2-5      htmlwidgets_1.6.4  plyr_1.8.9        \n [19] sandwich_3.1-0     emmeans_1.10.0     zoo_1.8-12        \n [22] uuid_1.2-0         igraph_2.0.2       mime_0.12         \n [25] lifecycle_1.0.4    pkgconfig_2.0.3    Matrix_1.6-5      \n [28] R6_2.5.1           fastmap_1.1.1      shiny_1.8.0       \n [31] digest_0.6.35      OpenMx_2.21.11     fdrtool_1.2.17    \n [34] colorspace_2.1-0   rprojroot_2.0.4    Hmisc_5.1-1       \n [37] fansi_1.0.6        timechange_0.3.0   abind_1.4-5       \n [40] compiler_4.3.3     withr_3.0.0        glasso_1.11       \n [43] htmlTable_2.4.2    backports_1.4.1    carData_3.0-5     \n [46] performance_0.11.0 ggsignif_0.6.4     MASS_7.3-60.0.1   \n [49] corpcor_1.6.10     gtools_3.9.5       tools_4.3.3       \n [52] pbivnorm_0.6.0     foreign_0.8-86     zip_2.3.1         \n [55] httpuv_1.6.14      nnet_7.3-19        glue_1.7.0        \n [58] quadprog_1.5-8     nlme_3.1-164       promises_1.2.1    \n [61] lisrelToR_0.3      grid_4.3.3         pbdZMQ_0.3-11     \n [64] checkmate_2.3.1    cluster_2.1.6      reshape2_1.4.4    \n [67] generics_0.1.3     gtable_0.3.4       tzdb_0.4.0        \n [70] data.table_1.15.2  hms_1.1.3          car_3.1-2         \n [73] utf8_1.2.4         sem_3.1-15         pillar_1.9.0      \n [76] IRdisplay_1.1      rockchalk_1.8.157  later_1.3.2       \n [79] splines_4.3.3      lattice_0.22-5     survival_3.5-8    \n [82] kutils_1.73        tidyselect_1.2.0   miniUI_0.1.1.1    \n [85] pbapply_1.7-2      stats4_4.3.3       xfun_0.42         \n [88] qgraph_1.9.8       arm_1.13-1         stringi_1.8.3     \n [91] boot_1.3-29        evaluate_0.23      codetools_0.2-19  \n [94] mi_1.1             cli_3.6.2          RcppParallel_5.1.7\n [97] IRkernel_1.3.2     rpart_4.1.23       parameters_0.21.6 \n[100] xtable_1.8-4       repr_1.1.6         munsell_0.5.0     \n[103] Rcpp_1.0.12        coda_0.19-4.1      png_0.1-8         \n[106] XML_3.99-0.16.1    parallel_4.3.3     ellipsis_0.3.2    \n[109] bayestestR_0.13.2  jpeg_0.1-10        lme4_1.1-35.1     \n[112] mvtnorm_1.2-4      insight_0.19.10    openxlsx_4.2.5.2  \n[115] crayon_1.5.2       rlang_1.1.3        multcomp_1.4-25   \n[118] mnormt_2.1.1",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>CFA per matrici multi-tratto multi-metodo</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/05_bifactor.html",
    "href": "chapters/cfa/05_bifactor.html",
    "title": "36  Modello bifattoriale",
    "section": "",
    "text": "36.1 Struttura Fattoriale\nRecenti studi hanno evidenziato che, di fronte a misure che generano dati multidimensionali a causa di una diversificata struttura di contenuto, l’adozione di un modello di misurazione bifattoriale può essere particolarmente efficace per rappresentare la struttura sottostante. Questo modello suggerisce che le correlazioni tra gli item di un test possono essere spiegate attraverso due tipi di fattori: (a) un fattore generale che riflette la varianza condivisa tra tutti gli item, e (b) una serie di fattori di gruppo che catturano la varianza specifica non spiegata dal fattore generale e che è comune tra item simili in termini di contenuto. Generalmente, si ritiene che il fattore generale e i fattori di gruppo siano indipendenti.\nIl fattore generale rappresenta il costrutto principale che lo strumento si propone di misurare, mentre i fattori di gruppo individuano costrutti più specifici legati a sottodomini. I modelli bifattoriali sono utilizzati per diverse finalità importanti:\nQuesti approcci permettono una comprensione più profonda e una valutazione più accurata della struttura sottostante dei dati psicologici, offrendo agli specialisti gli strumenti per interpretare con maggiore precisione i risultati dei test psicologici.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Modello bifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/05_bifactor.html#struttura-fattoriale",
    "href": "chapters/cfa/05_bifactor.html#struttura-fattoriale",
    "title": "36  Modello bifattoriale",
    "section": "",
    "text": "Analizzare la distribuzione della varianza quando si presume che uno strumento misuri sia varianza generale sia specifica di gruppo.\nGestire la multidimensionalità in modo che la misura risulti “essenzialmente unidimensionale”, pur presentando dimensioni secondarie.\nVerificare la presenza di un fattore generale sufficientemente robusto da giustificare l’uso di un modello di misurazione unidimensionale.\nDeterminare l’adeguatezza di un punteggio complessivo e valutare l’utilità di analizzare le sottoscale specifiche.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Modello bifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/05_bifactor.html#un-esempio-pratico",
    "href": "chapters/cfa/05_bifactor.html#un-esempio-pratico",
    "title": "36  Modello bifattoriale",
    "section": "36.2 Un esempio pratico",
    "text": "36.2 Un esempio pratico\nConsideriamo i dati SRS_data forniti dal pacchetto BifactorIndicesCalculator. Il dataset contiene 500 risposte al test SRS-22r sulla qualità della vita legata alla scoliosi, composta da 20 item. La sottoscala “Function” è composta dagli item 5, 9, 12, 15 e 18. La sottoscala “Pain” è composta dagli item 1, 2, 8, 11 e 17. La sottoscala “SelfImage” è composta dagli item 4, 6, 10, 14 e 19. La sottoscala “MentalHealth” è composta dagli item 3, 7, 13, 16 e 20.\nIniziamo esaminando le statistiche descrittive a livello di item e le correlazioni tra gli item.\n\ndescribe(SRS_data)\n\n\nA psych: 20 x 13\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nSRS_1\n1\n500\n3.694\n1.0651917\n4\n3.7775\n1.4826\n1\n5\n4\n-0.55091527\n-0.44128777\n0.04763682\n\n\nSRS_2\n2\n500\n3.810\n1.0295727\n4\n3.9200\n1.4826\n1\n5\n4\n-0.71482596\n-0.06788341\n0.04604389\n\n\nSRS_3\n3\n500\n3.896\n1.0897519\n4\n4.0375\n1.4826\n1\n5\n4\n-0.83228535\n-0.03970322\n0.04873518\n\n\nSRS_4\n4\n500\n3.232\n1.2796292\n3\n3.2900\n1.4826\n1\n5\n4\n-0.10588960\n-1.02358475\n0.05722676\n\n\nSRS_5\n5\n500\n4.196\n0.8939700\n4\n4.2975\n1.4826\n2\n5\n3\n-0.74534622\n-0.53845331\n0.03997955\n\n\nSRS_6\n6\n500\n3.914\n0.8718510\n4\n3.9525\n1.4826\n1\n5\n4\n-0.37669898\n-0.38588958\n0.03899036\n\n\nSRS_7\n7\n500\n4.236\n1.0111126\n5\n4.4025\n0.0000\n1\n5\n4\n-1.28422859\n1.08068127\n0.04521833\n\n\nSRS_8\n8\n500\n3.796\n1.1317888\n4\n3.9075\n1.4826\n1\n5\n4\n-0.51397419\n-0.69930792\n0.05061513\n\n\nSRS_9\n9\n500\n4.408\n1.0037685\n5\n4.6275\n0.0000\n1\n5\n4\n-1.77060059\n2.45174385\n0.04488989\n\n\nSRS_10\n10\n500\n3.700\n0.8552561\n4\n3.7150\n1.4826\n1\n5\n4\n-0.23018341\n-0.20973096\n0.03824821\n\n\nSRS_11\n11\n500\n4.502\n0.8386911\n5\n4.6900\n0.0000\n1\n5\n4\n-2.03023472\n4.08617302\n0.03750741\n\n\nSRS_12\n12\n500\n4.258\n1.0283730\n5\n4.4425\n0.0000\n1\n5\n4\n-1.34621075\n1.11920378\n0.04599024\n\n\nSRS_13\n13\n500\n3.848\n0.9266138\n4\n3.9375\n1.4826\n1\n5\n4\n-0.64572868\n0.04974619\n0.04143943\n\n\nSRS_14\n14\n500\n4.722\n0.6764845\n5\n4.8950\n0.0000\n1\n5\n4\n-2.81546817\n8.20685737\n0.03025331\n\n\nSRS_15\n15\n500\n4.814\n0.6294413\n5\n4.9875\n0.0000\n1\n5\n4\n-4.11789192\n18.04005703\n0.02814947\n\n\nSRS_16\n16\n500\n4.242\n0.9598326\n5\n4.3975\n0.0000\n1\n5\n4\n-1.23003057\n0.98745880\n0.04292502\n\n\nSRS_17\n17\n500\n4.736\n0.8601067\n5\n4.9875\n0.0000\n1\n5\n4\n-3.48403036\n11.29870912\n0.03846514\n\n\nSRS_18\n18\n500\n2.920\n0.9226606\n3\n2.9175\n0.0000\n1\n5\n4\n0.05145498\n0.83635388\n0.04126264\n\n\nSRS_19\n19\n500\n3.600\n1.1074533\n4\n3.6950\n1.4826\n1\n5\n4\n-0.54423397\n-0.28182684\n0.04952682\n\n\nSRS_20\n20\n500\n3.988\n0.8997083\n4\n4.0900\n1.4826\n1\n5\n4\n-0.89924932\n0.74798504\n0.04023618\n\n\n\n\n\n\nround(cor(SRS_data, use = \"pairwise.complete.obs\"), 2)\n\n\nA matrix: 20 x 20 of type dbl\n\n\n\nSRS_1\nSRS_2\nSRS_3\nSRS_4\nSRS_5\nSRS_6\nSRS_7\nSRS_8\nSRS_9\nSRS_10\nSRS_11\nSRS_12\nSRS_13\nSRS_14\nSRS_15\nSRS_16\nSRS_17\nSRS_18\nSRS_19\nSRS_20\n\n\n\n\nSRS_1\n1.00\n0.88\n0.43\n0.36\n0.36\n0.39\n0.34\n0.70\n0.31\n0.37\n0.46\n0.59\n0.37\n0.36\n0.19\n0.40\n0.38\n0.30\n0.27\n0.33\n\n\nSRS_2\n0.88\n1.00\n0.40\n0.38\n0.35\n0.42\n0.35\n0.70\n0.34\n0.40\n0.49\n0.58\n0.35\n0.38\n0.20\n0.37\n0.39\n0.29\n0.31\n0.32\n\n\nSRS_3\n0.43\n0.40\n1.00\n0.32\n0.33\n0.39\n0.50\n0.46\n0.30\n0.32\n0.24\n0.46\n0.55\n0.34\n0.19\n0.55\n0.25\n0.25\n0.28\n0.41\n\n\nSRS_4\n0.36\n0.38\n0.32\n1.00\n0.23\n0.43\n0.32\n0.33\n0.19\n0.43\n0.20\n0.30\n0.30\n0.20\n0.23\n0.32\n0.10\n0.21\n0.54\n0.32\n\n\nSRS_5\n0.36\n0.35\n0.33\n0.23\n1.00\n0.33\n0.39\n0.33\n0.47\n0.34\n0.23\n0.52\n0.31\n0.29\n0.20\n0.42\n0.28\n0.31\n0.25\n0.40\n\n\nSRS_6\n0.39\n0.42\n0.39\n0.43\n0.33\n1.00\n0.48\n0.37\n0.25\n0.64\n0.28\n0.37\n0.41\n0.38\n0.22\n0.47\n0.22\n0.31\n0.62\n0.41\n\n\nSRS_7\n0.34\n0.35\n0.50\n0.32\n0.39\n0.48\n1.00\n0.40\n0.24\n0.37\n0.22\n0.42\n0.53\n0.44\n0.23\n0.78\n0.19\n0.39\n0.39\n0.56\n\n\nSRS_8\n0.70\n0.70\n0.46\n0.33\n0.33\n0.37\n0.40\n1.00\n0.30\n0.37\n0.36\n0.52\n0.40\n0.28\n0.14\n0.42\n0.31\n0.32\n0.28\n0.34\n\n\nSRS_9\n0.31\n0.34\n0.30\n0.19\n0.47\n0.25\n0.24\n0.30\n1.00\n0.36\n0.26\n0.49\n0.32\n0.29\n0.22\n0.32\n0.35\n0.27\n0.20\n0.31\n\n\nSRS_10\n0.37\n0.40\n0.32\n0.43\n0.34\n0.64\n0.37\n0.37\n0.36\n1.00\n0.26\n0.37\n0.30\n0.34\n0.22\n0.39\n0.19\n0.28\n0.54\n0.35\n\n\nSRS_11\n0.46\n0.49\n0.24\n0.20\n0.23\n0.28\n0.22\n0.36\n0.26\n0.26\n1.00\n0.42\n0.19\n0.33\n0.20\n0.23\n0.39\n0.18\n0.24\n0.19\n\n\nSRS_12\n0.59\n0.58\n0.46\n0.30\n0.52\n0.37\n0.42\n0.52\n0.49\n0.37\n0.42\n1.00\n0.43\n0.46\n0.23\n0.46\n0.44\n0.33\n0.32\n0.43\n\n\nSRS_13\n0.37\n0.35\n0.55\n0.30\n0.31\n0.41\n0.53\n0.40\n0.32\n0.30\n0.19\n0.43\n1.00\n0.35\n0.17\n0.57\n0.21\n0.30\n0.35\n0.60\n\n\nSRS_14\n0.36\n0.38\n0.34\n0.20\n0.29\n0.38\n0.44\n0.28\n0.29\n0.34\n0.33\n0.46\n0.35\n1.00\n0.33\n0.46\n0.38\n0.30\n0.32\n0.34\n\n\nSRS_15\n0.19\n0.20\n0.19\n0.23\n0.20\n0.22\n0.23\n0.14\n0.22\n0.22\n0.20\n0.23\n0.17\n0.33\n1.00\n0.27\n0.20\n0.17\n0.29\n0.24\n\n\nSRS_16\n0.40\n0.37\n0.55\n0.32\n0.42\n0.47\n0.78\n0.42\n0.32\n0.39\n0.23\n0.46\n0.57\n0.46\n0.27\n1.00\n0.19\n0.38\n0.36\n0.59\n\n\nSRS_17\n0.38\n0.39\n0.25\n0.10\n0.28\n0.22\n0.19\n0.31\n0.35\n0.19\n0.39\n0.44\n0.21\n0.38\n0.20\n0.19\n1.00\n0.25\n0.15\n0.16\n\n\nSRS_18\n0.30\n0.29\n0.25\n0.21\n0.31\n0.31\n0.39\n0.32\n0.27\n0.28\n0.18\n0.33\n0.30\n0.30\n0.17\n0.38\n0.25\n1.00\n0.25\n0.31\n\n\nSRS_19\n0.27\n0.31\n0.28\n0.54\n0.25\n0.62\n0.39\n0.28\n0.20\n0.54\n0.24\n0.32\n0.35\n0.32\n0.29\n0.36\n0.15\n0.25\n1.00\n0.41\n\n\nSRS_20\n0.33\n0.32\n0.41\n0.32\n0.40\n0.41\n0.56\n0.34\n0.31\n0.35\n0.19\n0.43\n0.60\n0.34\n0.24\n0.59\n0.16\n0.31\n0.41\n1.00\n\n\n\n\n\n\nSRS_UnidimensionalModel &lt;-\n\"\n    SRS =~ SRS_1 + SRS_2 + SRS_3 + SRS_4 + SRS_5 +\n    SRS_6 + SRS_7 + SRS_8 + SRS_9 + SRS_10 +\n    SRS_11 + SRS_12 + SRS_13 + SRS_14 + SRS_15 +\n    SRS_16 + SRS_17 + SRS_18 + SRS_19 + SRS_20\n\"\n\nSRS_Unidimensional &lt;- lavaan::cfa(SRS_UnidimensionalModel,\n    SRS_data,\n    ordered = paste0(\"SRS_\", 1:20),\n    orthogonal = TRUE\n)\n\n\nsemPaths(SRS_Unidimensional, intercepts = FALSE)\n\n\n\n\n\n\n\n\nEsaminiamo la bontà di adattamento.\n\nfit.subset &lt;- c(\n    \"chisq.scaled\", \"df\", \"pvalue.scaled\",\n    \"rmsea.scaled\", \"rmsea.pvalue.scale\",\n    \"rmsea.ci.lower.scaled\", \"rmsea.ci.upper.scaled\",\n    \"cfi\", \"tli\", \"srmr\"\n)\n\n\nfitmeasures(SRS_Unidimensional, fit.subset) |&gt; print()\n\n         chisq.scaled                    df         pvalue.scaled \n             2087.431               170.000                 0.000 \n         rmsea.scaled rmsea.ci.lower.scaled rmsea.ci.upper.scaled \n                0.150                 0.145                 0.156 \n                  cfi                   tli                  srmr \n                0.961                 0.956                 0.119 \n\n\n\nSRS_BifactorModel &lt;-\n\"\n    SRS =~ SRS_1 + SRS_2 + SRS_3 + SRS_4 + SRS_5 + \n           SRS_6 + SRS_7 + SRS_8 + SRS_9 + SRS_10 +\n           SRS_11 + SRS_12 + SRS_13 + SRS_14 + SRS_15 +\n           SRS_16 + SRS_17 + SRS_18 + SRS_19 + SRS_20\n    Function =~ SRS_5 + SRS_9 + SRS_12 + SRS_15 + SRS_18\n    Pain =~ SRS_1 + SRS_2 + SRS_8 + SRS_11 + SRS_17\n    SelfImage =~ SRS_4 + SRS_6 + SRS_10 + SRS_14 + SRS_19\n    MentalHealth =~ SRS_3 + SRS_7 + SRS_13 + SRS_16 + SRS_20\n\"\n\nSRS_bifactor &lt;- lavaan::cfa(SRS_BifactorModel,\n    SRS_data,\n    ordered = paste0(\"SRS_\", 1:20),\n    orthogonal = TRUE\n)\n\n\nsemPaths(SRS_bifactor, intercepts = FALSE)\n\n\n\n\n\n\n\n\nEsaminiamo la bontà di adattamento.\n\nfitmeasures(SRS_bifactor, fit.subset) |&gt; print()\n\n         chisq.scaled                    df         pvalue.scaled \n              468.648               150.000                 0.000 \n         rmsea.scaled rmsea.ci.lower.scaled rmsea.ci.upper.scaled \n                0.065                 0.059                 0.072 \n                  cfi                   tli                  srmr \n                0.997                 0.996                 0.055 \n\n\nConfrontiamo i due modelli.\n\nlavTestLRT(SRS_Unidimensional, SRS_bifactor) |&gt; print()\n\n\nScaled Chi-Squared Difference Test (method = \"satorra.2000\")\n\nlavaan NOTE:\n    The \"Chisq\" column contains standard test statistics, not the\n    robust test that should be reported per model. A robust difference\n    test is a function of two standard (not robust) statistics.\n \n                    Df AIC BIC   Chisq Chisq diff Df diff Pr(&gt;Chisq)    \nSRS_bifactor       150          308.88                                  \nSRS_Unidimensional 170         1965.48     1007.2      20  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nConsideriamo ora gli indici specifici per un modello bifattoriale.\n\nbifactorIndices(SRS_bifactor, UniLambda = SRS_Unidimensional) |&gt; print()\n\n$ModelLevelIndices\n   ECV.SRS        PUC  Omega.SRS OmegaH.SRS       ARPB \n 0.6728130  0.7894737  0.9423975  0.8338190  0.1209687 \n\n$FactorLevelIndices\n                ECV_SS     ECV_SG    ECV_GS     Omega     OmegaH         H\nSRS          0.6728130 0.67281304 0.6728130 0.9423975 0.83381903 0.9431669\nFunction     0.1972990 0.04153902 0.8027010 0.7993085 0.09117569 0.4033325\nPain         0.4123779 0.11147096 0.5876221 0.8817340 0.34266587 0.6964443\nSelfImage    0.3280132 0.08183383 0.6719868 0.8502075 0.20252347 0.5913544\nMentalHealth 0.3424358 0.09234316 0.6575642 0.8917832 0.29303417 0.6148062\n                    FD\nSRS          0.9522166\nFunction     0.7096945\nPain         0.9209793\nSelfImage    0.8297680\nMentalHealth 0.8543576\n\n$ItemLevelIndices\n            IECV RelParBias\nSRS_1  0.5104022 0.35337989\nSRS_2  0.4976737 0.36753928\nSRS_3  0.7980893 0.03658299\nSRS_4  0.6115286 0.05215982\nSRS_5  0.6353618 0.02256623\nSRS_6  0.6366985 0.08805912\nSRS_7  0.6318472 0.17893361\nSRS_8  0.6819609 0.10337235\nSRS_9  0.5875270 0.01921305\nSRS_10 0.6494275 0.06424462\nSRS_11 0.6329358 0.12171959\nSRS_12 0.9378875 0.10010704\nSRS_13 0.6242449 0.11788100\nSRS_14 0.9989633 0.09897301\nSRS_15 0.9995426 0.10107730\nSRS_16 0.6005105 0.18981092\nSRS_17 0.7498240 0.03922319\nSRS_18 0.9990197 0.09332138\nSRS_19 0.4582466 0.16640538\nSRS_20 0.6912559 0.10480345\n\n\n\nI ModelLevelIndices possono essere spiegati nel modo seguente:\n\nECV.SRS: Questo indica la proporzione di varianza spiegata dal fattore generale nel modello bifattoriale. ECV sta per “Explained Common Variance” (Varianza Comune Spiegata). Un valore più alto indica che una maggiore parte della varianza totale nei dati è spiegata dal fattore generale.\nPUC: Questo è l’acronimo di “Percentage of Uniqueness in Common” (Percentuale di Unicità nel Comune). Indica quanto della varianza unica (cioè quella non spiegata dal fattore generale) è presente nei fattori di gruppo. Un valore basso indica che i fattori di gruppo spiegano una maggiore parte della varianza unica nei dati.\nOmega.SRS: Questo indice rappresenta il coefficiente di affidabilità del fattore generale del modello bifattoriale. Indica quanto sia affidabile il fattore generale nel catturare la varianza comune tra tutti gli item del test. Un valore più alto indica maggiore affidabilità.\nOmegaH.SRS: Questo indice rappresenta il coefficiente di affidabilità dei fattori di gruppo nel modello bifattoriale. Indica quanto sia affidabile l’insieme dei fattori di gruppo nel catturare la varianza condivisa tra gli item del gruppo specifico. Anche qui, un valore più alto indica maggiore affidabilità.\nARPB: Questo sta per “Average Reproducibility of Parameter Estimates” (Riproducibilità Media delle Stime dei Parametri). Rappresenta la riproducibilità media delle stime dei parametri del modello bifattoriale. In sostanza, valuta quanto le stime dei parametri del modello sono affidabili e riproducibili.\n\nLa sezione dell’output FactorLevelIndices riguarda gli indici a livello di fattore del modello bifattoriale.\n\nECV_SS, ECV_SG, ECV_GS: Questi rappresentano rispettivamente la proporzione di varianza spiegata dal Fattore Generale (GG), dal Fattore Specifico (SS) e dall’Interazione tra Fattore Generale e Fattore Specifico (GS) per ciascun fattore. Indicano quanto ciascun tipo di varianza contribuisce alla spiegazione della varianza totale nell’insieme dei dati del fattore.\nOmega: Questo indice rappresenta il coefficiente di affidabilità dell’estratto del Fattore Generale per ciascun fattore. Indica quanto sia affidabile il Fattore Generale nel catturare la varianza comune tra gli item di quel particolare fattore. Un valore più alto indica maggiore affidabilità.\nOmegaH: Questo indice rappresenta il coefficiente di affidabilità dell’estratto del Fattore Specifico per ciascun fattore. Indica quanto sia affidabile l’insieme dei Fattori Specifici nel catturare la varianza condivisa tra gli item del gruppo specifico. Anche qui, un valore più alto indica maggiore affidabilità.\nH: Questo indice rappresenta la quota della varianza unica spiegata dal Fattore Generale per ciascun fattore. Indica quanto della varianza unica è spiegata dal Fattore Generale piuttosto che da fattori specifici.\nFD: Questo indice rappresenta la distorsione fattoriale, che è una misura di quanto i dati si adattino bene al modello bifattoriale. Valori vicini a 1 indicano un buon adattamento.\n\nInfine, l’output ItemLevelIndices riguarda gli indici a livello di item in un modello bifattoriale.\n\nIECV: Questo indica la proporzione di varianza spiegata dal Fattore Generale per ciascun item. IECV sta per “Item Explained Common Variance” (Varianza Comune Spiegata dell’Item). Indica quanto della varianza totale dell’item può essere spiegata dal Fattore Generale del modello bifattoriale. Valori più alti indicano che il Fattore Generale contribuisce maggiormente a spiegare le variazioni osservate nell’item.\nRelParBias: Questo rappresenta il bias relativo dei parametri dell’item. Indica quanto i parametri dell’item sono influenzati dalla presenza del Fattore Generale e dai fattori specifici nel modello. Valori più alti indicano una maggiore influenza dei fattori specifici rispetto al Fattore Generale nell’item.\n\nPer i dati dell’esempio considerato, di seguito è riportata un’interpretazione succinta dei risultati chiave per ciascun gruppo principale di risultati:\n\n36.2.1 Model-Level Indices\n\nECV.SRS (Explained Common Variance): Il 67.28% della varianza osservata è spiegata dal modello.\nPUC (Percentage of Uncontaminated Correlations): Il 78.95% delle correlazioni tra gli item è “puro”, cioè non contaminato da altri fattori oltre al fattore generale.\nOmega.SRS: La consistenza interna complessiva del test è molto alta (0.942), indicando una buona affidabilità.\nOmegaH.SRS: Il 83.38% della varianza totale standardizzata è attribuibile al fattore generale, confermando che è un fattore dominante nel modello.\nARPB (Average Relative Parameter Bias): Un bias relativo medio basso (0.121) suggerisce che le stime dei parametri sono relativamente poco distorte.\n\n\n\n36.2.2 Factor-Level Indices\n\nECV (Explained Common Variance) per i fattori specifici:\n\nFunzione: 19.73% della varianza è spiegata dal fattore specifico “Funzione”, con l’80.27% attribuibile al fattore generale.\nDolore (Pain): 41.24% della varianza è spiegata dal fattore specifico “Dolore”.\nAutopercezione (SelfImage): 32.80% della varianza è spiegata dal fattore specifico “Autopercezione”.\nSalute Mentale (MentalHealth): 34.24% della varianza è spiegata dal fattore specifico “Salute Mentale”.\n\nOmega e OmegaH per ogni fattore specifico: Le misure Omega indicano la consistenza interna per ciascun sottogruppo di item, mentre OmegaH indica la proporzione della varianza attribuibile ai fattori specifici rispetto al fattore generale.\n\n\n\n36.2.3 Item-Level Indices\n\nIECV (Item Explained Common Variance): Valori come 0.937 per SRS_12 e quasi 1 per SRS_14, SRS_15 e SRS_18 indicano che questi item sono molto influenzati dal fattore generale.\nRelParBias (Relative Parameter Bias): La maggior parte degli item mostra un bias relativo basso, suggerendo che gli effetti dei fattori specifici su questi item sono correttamente rappresentati senza grande distorsione.\n\nIn sintesi, il modello bifattoriale sembra adattarsi bene ai dati, con un forte fattore generale che domina la struttura del test, supportato da alcuni fattori specifici che spiegano porzioni significative della varianza in diverse aree tematiche. Gli item individuati con alti valori di IECV sono particolarmente rappresentativi del fattore generale.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Modello bifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/05_bifactor.html#commenti-e-considerazioni-conclusive",
    "href": "chapters/cfa/05_bifactor.html#commenti-e-considerazioni-conclusive",
    "title": "36  Modello bifattoriale",
    "section": "36.3 Commenti e considerazioni conclusive",
    "text": "36.3 Commenti e considerazioni conclusive\nIn questo capitolo, abbiamo esplorato diversi indici derivati dall’analisi con un modello bifattoriale, ognuno dei quali rivela aspetti specifici delle proprietà psicometriche di uno strumento di misura. Questi indici sono di grande utilità per gli sviluppatori e i valutatori di scale, oltre a essere strumenti preziosi per i ricercatori e i professionisti che le impiegano nella pratica clinica e nella ricerca. Inoltre, contribuiscono allo sviluppo e alla comprensione dei costrutti psicologici che tali strumenti intendono misurare.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Modello bifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/05_bifactor.html#session-info",
    "href": "chapters/cfa/05_bifactor.html#session-info",
    "title": "36  Modello bifattoriale",
    "section": "36.4 Session Info",
    "text": "36.4 Session Info\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.4.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] BifactorIndicesCalculator_0.2.2 ggokabeito_0.1.0               \n [3] viridis_0.6.5                   viridisLite_0.4.2              \n [5] ggpubr_0.6.0                    ggExtra_0.10.1                 \n [7] bayesplot_1.11.1                gridExtra_2.3                  \n [9] patchwork_1.2.0                 semTools_0.5-6                 \n[11] semPlot_1.1.6                   lavaan_0.6-17                  \n[13] psych_2.4.3                     scales_1.3.0                   \n[15] markdown_1.12                   knitr_1.45                     \n[17] lubridate_1.9.3                 forcats_1.0.0                  \n[19] stringr_1.5.1                   dplyr_1.1.4                    \n[21] purrr_1.0.2                     readr_2.1.5                    \n[23] tidyr_1.3.1                     tibble_3.2.1                   \n[25] ggplot2_3.5.0                   tidyverse_2.0.0                \n[27] here_1.0.1                     \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.15.0  jsonlite_1.8.8     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5   nloptr_2.0.3      \n  [7] rmarkdown_2.26     vctrs_0.6.5        minqa_1.2.6       \n [10] base64enc_0.1-3    rstatix_0.7.2      htmltools_0.5.7   \n [13] broom_1.0.5        Formula_1.2-5      htmlwidgets_1.6.4 \n [16] plyr_1.8.9         sandwich_3.1-0     emmeans_1.10.0    \n [19] zoo_1.8-12         uuid_1.2-0         igraph_2.0.2      \n [22] mime_0.12          lifecycle_1.0.4    pkgconfig_2.0.3   \n [25] Matrix_1.6-5       R6_2.5.1           fastmap_1.1.1     \n [28] shiny_1.8.0        digest_0.6.35      OpenMx_2.21.11    \n [31] fdrtool_1.2.17     colorspace_2.1-0   rprojroot_2.0.4   \n [34] Hmisc_5.1-1        fansi_1.0.6        timechange_0.3.0  \n [37] abind_1.4-5        compiler_4.3.3     withr_3.0.0       \n [40] glasso_1.11        htmlTable_2.4.2    backports_1.4.1   \n [43] carData_3.0-5      ggsignif_0.6.4     MASS_7.3-60.0.1   \n [46] corpcor_1.6.10     gtools_3.9.5       tools_4.3.3       \n [49] pbivnorm_0.6.0     foreign_0.8-86     zip_2.3.1         \n [52] httpuv_1.6.14      nnet_7.3-19        glue_1.7.0        \n [55] quadprog_1.5-8     nlme_3.1-164       promises_1.2.1    \n [58] lisrelToR_0.3      grid_4.3.3         pbdZMQ_0.3-11     \n [61] checkmate_2.3.1    cluster_2.1.6      reshape2_1.4.4    \n [64] generics_0.1.3     gtable_0.3.4       tzdb_0.4.0        \n [67] data.table_1.15.2  hms_1.1.3          car_3.1-2         \n [70] utf8_1.2.4         sem_3.1-15         pillar_1.9.0      \n [73] IRdisplay_1.1      rockchalk_1.8.157  later_1.3.2       \n [76] splines_4.3.3      lattice_0.22-5     survival_3.5-8    \n [79] kutils_1.73        tidyselect_1.2.0   miniUI_0.1.1.1    \n [82] pbapply_1.7-2      stats4_4.3.3       xfun_0.42         \n [85] qgraph_1.9.8       arm_1.13-1         stringi_1.8.3     \n [88] boot_1.3-29        evaluate_0.23      codetools_0.2-19  \n [91] mi_1.1             cli_3.6.2          RcppParallel_5.1.7\n [94] IRkernel_1.3.2     rpart_4.1.23       xtable_1.8-4      \n [97] repr_1.1.6         munsell_0.5.0      Rcpp_1.0.12       \n[100] coda_0.19-4.1      png_0.1-8          XML_3.99-0.16.1   \n[103] parallel_4.3.3     ellipsis_0.3.2     jpeg_0.1-10       \n[106] lme4_1.1-35.1      mvtnorm_1.2-4      openxlsx_4.2.5.2  \n[109] crayon_1.5.2       rlang_1.1.3        multcomp_1.4-25   \n[112] mnormt_2.1.1",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Modello bifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/06_efa_lavaan.html",
    "href": "chapters/cfa/06_efa_lavaan.html",
    "title": "37  Exploratory Structural Equation Modeling (ESEM)",
    "section": "",
    "text": "37.1 Limitazioni della CFA\nNel modello di misurazione di Confirmatory Factor Analysis (CFA), comunemente adottato nella ricerca psicologica, solitamente sappiamo quali indicatori appartengono a ciascun fattore latente, una struttura denominata “a priori”. Questo approccio è utilizzato per verificare se la struttura fattoriale presunta corrisponde effettivamente ai dati raccolti.\nTuttavia, nonostante la popolarità della CFA, essa presenta delle limitazioni significative. I modelli CFA spesso risultano eccessivamente semplici e restrittivi, presupponendo “fattori puri”, cioè assumento che ciascun item saturi solamente sui suoi fattori latenti predeterminati, con saturazioni incrociate (ovvero, contributi di un item a fattori non primari) vincolate a zero. Questa restrizione può non riflettere adeguatamente la realtà di molte misure psicologiche, dove gli item tendono a riflettere più di un costrutto. Questo approccio può portare a una rappresentazione artificiale delle relazioni tra gli item e i fattori, risultando in statistiche di adattamento del modello sovrastimate e correlazioni tra fattori positivamente distorte. Studi di simulazione hanno mostrato che anche piccole saturazioni incrociate, se ignorate, possono portare ad una distorsione nelle stime dei parametri.\nUn altro problema è rappresentato dagli indici di bontà di adattamento utilizzati nei modelli CFA, che sono spesso troppo restrittivi per strumenti psicologici multifattoriali, rendendo quasi impossibile ottenere un “buon” adattamento senza significative modifiche ai modelli. Tuttavia, quando analizzati a livello di item e per affidabilità, i modelli che non mostrano un buon adattamento possono comunque indicare saturazioni ragionevoli e alti livelli di affidabilità.\nIn risposta a queste sfide, sono stati sviluppati approcci più flessibili e robusti, come l’Exploratory Structural Equation Modeling (ESEM).",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Exploratory Structural Equation Modeling (ESEM)</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/06_efa_lavaan.html#exploratory-structural-equation-modeling",
    "href": "chapters/cfa/06_efa_lavaan.html#exploratory-structural-equation-modeling",
    "title": "37  Exploratory Structural Equation Modeling (ESEM)",
    "section": "37.2 Exploratory Structural Equation Modeling",
    "text": "37.2 Exploratory Structural Equation Modeling\nL’ESEM combina elementi delle CFA e dell’Exploratory Factor Analysis (EFA) all’interno del tradizionale framework delle Equazioni Strutturali (SEM). Questo approccio rappresenta un compromesso tra la ricerca iterativa di soluzioni fattoriali ottimali, tipica dell’EFA, e la modellazione teorica restrittiva delle CFA.\nL’ESEM è essenzialmente un metodo confermativo che permette anche un’esplorazione attraverso l’uso di rotazioni mirate, mantenendo la presenza di caricamenti incrociati, seppur minimizzati. All’interno dell’ESEM, il ricercatore può prevedere a priori una struttura fattoriale, similmente a quanto avviene nelle CFA, ma con una maggiore flessibilità permessa dalla possibilità di modellare saturazioni incrociate.\nNell’ESEM, i fattori generali e specifici devono essere specificati come totalmente indipendenti, e le rotazioni ortogonali sono comuni nei modelli bifattoriali. I metodi di rotazione più usati nell’ESEM includono le rotazioni geomin e target, con rotazioni ortogonali adatte ai modelli più complessi.\nLe analisi di simulazione indicano che le correlazioni tra i fattori latenti ottenute con l’ESEM sono generalmente meno distorte e più vicine alle vere associazioni, rendendo i modelli ESEM più coerenti con le teorie sottostanti e le intenzioni degli strumenti psicometrici misurati.\nQuando un modello ESEM include solo una parte di misurazione, viene definito come “analisi fattoriale esplorativa” o EFA. Se il modello include anche una parte strutturale, come regressioni tra variabili latenti, è classificato come “modello di equazioni strutturali esplorativo” o ESEM.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Exploratory Structural Equation Modeling (ESEM)</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/06_efa_lavaan.html#otto-misure-di-personalità",
    "href": "chapters/cfa/06_efa_lavaan.html#otto-misure-di-personalità",
    "title": "37  Exploratory Structural Equation Modeling (ESEM)",
    "section": "37.3 Otto Misure di Personalità",
    "text": "37.3 Otto Misure di Personalità\nIn questo esempio pratico analizzeremo nuovamente i dati di {cite:t}brown2015confirmatory, ovvero otto misure di personalità raccolte su un campione di 250 pazienti che hanno concluso un programma di psicoterapia. Utilizzeremo un’analisi EFA mediante la funzione efa() di lavaan.\nGli item sono i seguenti:\n\nanxiety (N1),\nhostility (N2),\ndepression (N3),\nself-consciousness (N4),\nwarmth (E1),\ngregariousness (E2),\nassertiveness (E3),\npositive emotions (E4).\n\n\nvarnames &lt;- c(\"N1\", \"N2\", \"N3\", \"N4\", \"E1\", \"E2\", \"E3\", \"E4\")\nsds &lt;- \"5.7  5.6  6.4  5.7  6.0  6.2  5.7  5.6\"\n\ncors &lt;- \"\n 1.000\n 0.767  1.000\n 0.731  0.709  1.000\n 0.778  0.738  0.762  1.000\n-0.351  -0.302  -0.356  -0.318  1.000\n-0.316  -0.280  -0.300  -0.267  0.675  1.000\n-0.296  -0.289  -0.297  -0.296  0.634  0.651  1.000\n-0.282  -0.254  -0.292  -0.245  0.534  0.593  0.566  1.000\"\n\npsychot_cor_mat &lt;- getCov(cors, names = varnames)\nn &lt;- 250\n\nDefiniamo un modello ad un solo fattore comune.\n\n# 1-factor model\nf1 &lt;- '\n    efa(\"efa\")*f1 =~ N1 + N2 + N3 + N4 + E1 + E2 + E3 + E4\n'\n\nDefiniamo un modello con due fattori comuni.\n\n# 2-factor model\nf2 &lt;- '\n    efa(\"efa\")*f1 +\n    efa(\"efa\")*f2 =~ N1 + N2 + N3 + N4 + E1 + E2 + E3 + E4\n'\n\nAdattiamo ai dati il modello ad un fattore comune.\n\nefa_f1 &lt;-cfa(\n    model = f1,\n    sample.cov = psychot_cor_mat,\n    sample.nobs = 250,\n    rotation = \"oblimin\"\n)\n\nEsaminiamo la soluzione ottenuta.\n\nsemPlot::semPaths(efa_f1,\n    what = \"col\", whatLabels = \"std\", style = \"mx\",\n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\n\n\nsummary(\n    efa_f1,\n    fit.measures = TRUE,\n    standardized = TRUE,\n    rsquare = TRUE\n) |&gt;\n    print()\n\nlavaan 0.6.17 ended normally after 2 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        16\n\n  Rotation method                      OBLIMIN OBLIQUE\n  Oblimin gamma                                      0\n  Rotation algorithm (rstarts)                GPA (30)\n  Standardized metric                             TRUE\n  Row weights                                     None\n\n  Number of observations                           250\n\nModel Test User Model:\n                                                      \n  Test statistic                               375.327\n  Degrees of freedom                                20\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              1253.791\n  Degrees of freedom                                28\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.710\n  Tucker-Lewis Index (TLI)                       0.594\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -2394.637\n  Loglikelihood unrestricted model (H1)      -2206.974\n                                                      \n  Akaike (AIC)                                4821.275\n  Bayesian (BIC)                              4877.618\n  Sample-size adjusted Bayesian (SABIC)       4826.897\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.267\n  90 Percent confidence interval - lower         0.243\n  90 Percent confidence interval - upper         0.291\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.187\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  f1 =~ efa                                                             \n    N1                0.879    0.051   17.333    0.000    0.879    0.880\n    N2                0.841    0.052   16.154    0.000    0.841    0.842\n    N3                0.841    0.052   16.175    0.000    0.841    0.843\n    N4                0.870    0.051   17.065    0.000    0.870    0.872\n    E1               -0.438    0.062   -7.041    0.000   -0.438   -0.439\n    E2               -0.398    0.063   -6.327    0.000   -0.398   -0.398\n    E3               -0.398    0.063   -6.342    0.000   -0.398   -0.399\n    E4               -0.364    0.063   -5.746    0.000   -0.364   -0.364\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .N1                0.224    0.028    7.915    0.000    0.224    0.225\n   .N2                0.289    0.033    8.880    0.000    0.289    0.290\n   .N3                0.288    0.032    8.866    0.000    0.288    0.289\n   .N4                0.239    0.029    8.174    0.000    0.239    0.240\n   .E1                0.804    0.073   10.963    0.000    0.804    0.807\n   .E2                0.838    0.076   11.008    0.000    0.838    0.841\n   .E3                0.837    0.076   11.007    0.000    0.837    0.841\n   .E4                0.864    0.078   11.041    0.000    0.864    0.867\n    f1                1.000                               1.000    1.000\n\nR-Square:\n                   Estimate\n    N1                0.775\n    N2                0.710\n    N3                0.711\n    N4                0.760\n    E1                0.193\n    E2                0.159\n    E3                0.159\n    E4                0.133\n\n\n\n\nstandardizedSolution(efa_f1)\n\n\nA lavaan.data.frame: 17 x 9\n\n\nlhs\nop\nrhs\nest.std\nse\nz\npvalue\nci.lower\nci.upper\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nf1\n=~\nN1\n0.8803717\n0.01821233\n48.339322\n0.000000e+00\n0.8446762\n0.9160673\n\n\nf1\n=~\nN2\n0.8424369\n0.02179745\n38.648409\n0.000000e+00\n0.7997147\n0.8851591\n\n\nf1\n=~\nN3\n0.8431190\n0.02173170\n38.796733\n0.000000e+00\n0.8005257\n0.8857124\n\n\nf1\n=~\nN4\n0.8719792\n0.01898826\n45.922019\n0.000000e+00\n0.8347629\n0.9091955\n\n\nf1\n=~\nE1\n-0.4389273\n0.05368053\n-8.176656\n2.220446e-16\n-0.5441392\n-0.3337153\n\n\nf1\n=~\nE2\n-0.3983268\n0.05581495\n-7.136560\n9.570122e-13\n-0.5077221\n-0.2889315\n\n\nf1\n=~\nE3\n-0.3991904\n0.05577170\n-7.157580\n8.211209e-13\n-0.5085009\n-0.2898799\n\n\nf1\n=~\nE4\n-0.3644271\n0.05743899\n-6.344595\n2.230112e-10\n-0.4770055\n-0.2518488\n\n\nN1\n~~\nN1\n0.2249456\n0.03206479\n7.015345\n2.293721e-12\n0.1620998\n0.2877914\n\n\nN2\n~~\nN2\n0.2903001\n0.03672516\n7.904666\n2.664535e-15\n0.2183201\n0.3622801\n\n\nN3\n~~\nN3\n0.2891503\n0.03664399\n7.890797\n3.108624e-15\n0.2173294\n0.3609712\n\n\nN4\n~~\nN4\n0.2396523\n0.03311262\n7.237491\n4.569678e-13\n0.1747527\n0.3045518\n\n\nE1\n~~\nE1\n0.8073429\n0.04708357\n17.147019\n0.000000e+00\n0.7150608\n0.8996250\n\n\nE2\n~~\nE2\n0.8413358\n0.04442120\n18.939962\n0.000000e+00\n0.7542718\n0.9283997\n\n\nE3\n~~\nE3\n0.8406470\n0.04448315\n18.898100\n0.000000e+00\n0.7534617\n0.9278324\n\n\nE4\n~~\nE4\n0.8671929\n0.04181755\n20.737533\n0.000000e+00\n0.7852320\n0.9491538\n\n\nf1\n~~\nf1\n1.0000000\n0.00000000\nNA\nNA\n1.0000000\n1.0000000\n\n\n\n\n\n\nlavaan::residuals(efa_f1, type = \"cor\") |&gt;\n    print()\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n       N1     N2     N3     N4     E1     E2     E3     E4\nN1  0.000                                                 \nN2  0.025  0.000                                          \nN3 -0.011 -0.001  0.000                                   \nN4  0.010  0.003  0.027  0.000                            \nE1  0.035  0.068  0.014  0.065  0.000                     \nE2  0.035  0.056  0.036  0.080  0.500  0.000              \nE3  0.055  0.047  0.040  0.052  0.459  0.492  0.000       \nE4  0.039  0.053  0.015  0.073  0.374  0.448  0.421  0.000\n\n\n\nAdattiamo ai dati il modello a due fattori comuni.\n\nefa_f2 &lt;- cfa(\n    model = f2,\n    sample.cov = psychot_cor_mat,\n    sample.nobs = 250,\n    rotation = \"oblimin\"\n)\n\nEsaminiamo la soluzione ottenuta.\n\nsemPlot::semPaths(efa_f2,\n    what = \"col\", whatLabels = \"std\", style = \"mx\",\n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\n\n\nsummary(\n    efa_f2,\n    fit.measures = TRUE,\n    standardized = TRUE,\n    rsquare = TRUE\n) |&gt;\n    print()\n\nlavaan 0.6.17 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        23\n\n  Rotation method                      OBLIMIN OBLIQUE\n  Oblimin gamma                                      0\n  Rotation algorithm (rstarts)                GPA (30)\n  Standardized metric                             TRUE\n  Row weights                                     None\n\n  Number of observations                           250\n\nModel Test User Model:\n                                                      \n  Test statistic                                 9.811\n  Degrees of freedom                                13\n  P-value (Chi-square)                           0.709\n\nModel Test Baseline Model:\n\n  Test statistic                              1253.791\n  Degrees of freedom                                28\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.006\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -2211.879\n  Loglikelihood unrestricted model (H1)      -2206.974\n                                                      \n  Akaike (AIC)                                4469.758\n  Bayesian (BIC)                              4550.752\n  Sample-size adjusted Bayesian (SABIC)       4477.840\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.048\n  P-value H_0: RMSEA &lt;= 0.050                    0.957\n  P-value H_0: RMSEA &gt;= 0.080                    0.001\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.010\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  f1 =~ efa                                                             \n    N1                0.874    0.053   16.592    0.000    0.874    0.876\n    N2                0.851    0.055   15.551    0.000    0.851    0.853\n    N3                0.826    0.054   15.179    0.000    0.826    0.828\n    N4                0.896    0.053   16.802    0.000    0.896    0.898\n    E1               -0.046    0.040   -1.138    0.255   -0.046   -0.046\n    E2                0.035    0.034    1.030    0.303    0.035    0.035\n    E3                0.000    0.040    0.010    0.992    0.000    0.000\n    E4               -0.006    0.049   -0.131    0.896   -0.006   -0.006\n  f2 =~ efa                                                             \n    N1               -0.017    0.032   -0.539    0.590   -0.017   -0.017\n    N2                0.011    0.035    0.322    0.748    0.011    0.011\n    N3               -0.035    0.036   -0.949    0.343   -0.035   -0.035\n    N4                0.031    0.031    0.994    0.320    0.031    0.031\n    E1                0.776    0.059   13.125    0.000    0.776    0.778\n    E2                0.854    0.058   14.677    0.000    0.854    0.855\n    E3                0.785    0.060   13.106    0.000    0.785    0.787\n    E4                0.695    0.063   10.955    0.000    0.695    0.697\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  f1 ~~                                                                 \n    f2               -0.432    0.059   -7.345    0.000   -0.432   -0.432\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .N1                0.218    0.028    7.790    0.000    0.218    0.219\n   .N2                0.279    0.032    8.693    0.000    0.279    0.280\n   .N3                0.287    0.032    8.907    0.000    0.287    0.289\n   .N4                0.216    0.029    7.578    0.000    0.216    0.217\n   .E1                0.361    0.044    8.226    0.000    0.361    0.362\n   .E2                0.292    0.043    6.787    0.000    0.292    0.293\n   .E3                0.379    0.046    8.315    0.000    0.379    0.381\n   .E4                0.509    0.053    9.554    0.000    0.509    0.511\n    f1                1.000                               1.000    1.000\n    f2                1.000                               1.000    1.000\n\nR-Square:\n                   Estimate\n    N1                0.781\n    N2                0.720\n    N3                0.711\n    N4                0.783\n    E1                0.638\n    E2                0.707\n    E3                0.619\n    E4                0.489\n\n\n\n\nstandardizedSolution(efa_f2)\n\n\nA lavaan.data.frame: 27 x 9\n\n\nlhs\nop\nrhs\nest.std\nse\nz\npvalue\nci.lower\nci.upper\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nf1\n=~\nN1\n0.8761395102\n0.02407417\n36.39334610\n0.000000e+00\n0.82895501\n0.92332401\n\n\nf1\n=~\nN2\n0.8531678152\n0.02702414\n31.57058251\n0.000000e+00\n0.80020148\n0.90613415\n\n\nf1\n=~\nN3\n0.8279481017\n0.02867566\n28.87284913\n0.000000e+00\n0.77174483\n0.88415137\n\n\nf1\n=~\nN4\n0.8978425227\n0.02300072\n39.03541504\n0.000000e+00\n0.85276195\n0.94292310\n\n\nf1\n=~\nE1\n-0.0459319249\n0.04188329\n-1.09666460\n2.727880e-01\n-0.12802167\n0.03615782\n\n\nf1\n=~\nE2\n0.0348803050\n0.03211406\n1.08613823\n2.774178e-01\n-0.02806209\n0.09782270\n\n\nf1\n=~\nE3\n0.0004048457\n0.04030449\n0.01004468\n9.919856e-01\n-0.07859050\n0.07940019\n\n\nf1\n=~\nE4\n-0.0064182771\n0.04931505\n-0.13014844\n8.964490e-01\n-0.10307401\n0.09023745\n\n\nf2\n=~\nN1\n-0.0170399590\n0.03201112\n-0.53231373\n5.945087e-01\n-0.07978060\n0.04570069\n\n\nf2\n=~\nN2\n0.0113498122\n0.03500532\n0.32423110\n7.457631e-01\n-0.05725935\n0.07995897\n\n\nf2\n=~\nN3\n-0.0346710957\n0.03754572\n-0.92343665\n3.557797e-01\n-0.10825936\n0.03891717\n\n\nf2\n=~\nN4\n0.0308917105\n0.02998228\n1.03033232\n3.028540e-01\n-0.02787248\n0.08965590\n\n\nf2\n=~\nE1\n0.7778646543\n0.03799546\n20.47256881\n0.000000e+00\n0.70339492\n0.85233439\n\n\nf2\n=~\nE2\n0.8553963090\n0.03237245\n26.42358914\n0.000000e+00\n0.79194747\n0.91884514\n\n\nf2\n=~\nE3\n0.7870608048\n0.03768544\n20.88501110\n0.000000e+00\n0.71319870\n0.86092291\n\n\nf2\n=~\nE4\n0.6965219353\n0.04556050\n15.28784635\n0.000000e+00\n0.60722499\n0.78581888\n\n\nN1\n~~\nN1\n0.2191956687\n0.03170768\n6.91301577\n4.744649e-12\n0.15704976\n0.28134157\n\n\nN2\n~~\nN2\n0.2803386887\n0.03624154\n7.73528689\n1.021405e-14\n0.20930658\n0.35137080\n\n\nN3\n~~\nN3\n0.2885084763\n0.03644206\n7.91690962\n2.442491e-15\n0.21708336\n0.35993360\n\n\nN4\n~~\nN4\n0.2168781811\n0.03209316\n6.75776865\n1.401324e-11\n0.15397673\n0.27977963\n\n\nE1\n~~\nE1\n0.3619601932\n0.04713705\n7.67888939\n1.598721e-14\n0.26957327\n0.45434711\n\n\nE2\n~~\nE2\n0.2928483379\n0.04628174\n6.32751356\n2.491432e-10\n0.20213779\n0.38355888\n\n\nE3\n~~\nE3\n0.3808103127\n0.04868397\n7.82208813\n5.107026e-15\n0.28539148\n0.47622914\n\n\nE4\n~~\nE4\n0.5109551463\n0.05301682\n9.63760518\n0.000000e+00\n0.40704410\n0.61486620\n\n\nf1\n~~\nf1\n1.0000000000\n0.00000000\nNA\nNA\n1.00000000\n1.00000000\n\n\nf2\n~~\nf2\n1.0000000000\n0.00000000\nNA\nNA\n1.00000000\n1.00000000\n\n\nf1\n~~\nf2\n-0.4318172824\n0.05799170\n-7.44619142\n9.614531e-14\n-0.54547892\n-0.31815564\n\n\n\n\n\nAnche se abbiamo introdotto finora soltanto la misura di bontà di adattamento del chi-quadrato, aggiungiamo qui il calcolo di altre misure di bontà di adattamento che discuteremo in seguito.\n\nfit_measures_robust &lt;- c(\n    \"chisq\", \"df\", \"pvalue\", \"cfi\", \"rmsea\", \"srmr\"\n)\n\nConfrontiamo le misure di bontà di adattamento del modello che ipotizza un solo fattore comune e il modello che ipotizza la presenza di due fattori comuni.\n\n# collect them for each model\nrbind(\n    fitmeasures(efa_f1, fit_measures_robust),\n    fitmeasures(efa_f2, fit_measures_robust)\n) %&gt;%\n    # wrangle\n    data.frame() %&gt;%\n    mutate(\n        chisq = round(chisq, digits = 0),\n        df = as.integer(df),\n        pvalue = ifelse(pvalue == 0, \"&lt; .001\", pvalue)\n    ) %&gt;%\n    mutate_at(vars(cfi:srmr), ~ round(., digits = 3))\n\n\nA data.frame: 2 x 6\n\n\nchisq\ndf\npvalue\ncfi\nrmsea\nsrmr\n\n\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n375\n20\n&lt; .001 \n0.71\n0.267\n0.187\n\n\n10\n13\n0.709310449320098\n1.00\n0.000\n0.010\n\n\n\n\n\n\nlavaan::residuals(efa_f2, type = \"cor\") |&gt;\n    print()\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n       N1     N2     N3     N4     E1     E2     E3     E4\nN1  0.000                                                 \nN2  0.018  0.000                                          \nN3 -0.014 -0.006  0.000                                   \nN4 -0.003 -0.013  0.017  0.000                            \nE1 -0.003  0.015 -0.012  0.000  0.000                     \nE2 -0.009 -0.004  0.006  0.007  0.006  0.000              \nE3  0.015 -0.008  0.011 -0.016  0.006 -0.010  0.000       \nE4 -0.001  0.000 -0.013  0.009 -0.024  0.006  0.016  0.000\n\n\n\nL’evidenza empirica supporta la superiorità del modello a due fattori rispetto a quello ad un solo fattore comune. In particolare, l’analisi fattoriale esplorativa svolta mediante la funzione efa() evidenzia la capacità del modello a due fattori di fornire una descrizione adeguata della struttura dei dati e di distinguere in modo sensato tra i due fattori ipotizzati.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Exploratory Structural Equation Modeling (ESEM)</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/06_efa_lavaan.html#considerazioni-conclusive",
    "href": "chapters/cfa/06_efa_lavaan.html#considerazioni-conclusive",
    "title": "37  Exploratory Structural Equation Modeling (ESEM)",
    "section": "37.4 Considerazioni Conclusive",
    "text": "37.4 Considerazioni Conclusive\nL’Exploratory Structural Equation Modeling (ESEM) rappresenta un ponte significativo tra i modelli di misurazione tradizionali dell’Exploratory Factor Analysis (EFA) e il più esteso quadro del Confirmatory Factor Analysis/Structural Equation Modeling (CFA/SEM). Grazie a questo, l’ESEM combina i benefici dell’EFA con quelli del CFA/SEM, fornendo un approccio più flessibile e inclusivo nell’analisi dei dati. Tale integrazione ha segnato un progresso notevole nella ricerca statistica, evidenziando l’importanza dell’EFA che precedentemente era sottovalutata.\nL’ESEM e il quadro bifattoriale-ESEM, in particolare, offrono una rappresentazione più fedele e precisa della multidimensionalità dei costrutti psicometrici, che è spesso presente nelle misurazioni. Questo approccio riconosce e gestisce meglio la natura multidimensionale dei costrutti, a differenza dell’approccio tradizionale del CFA, che tende a sovrastimare le correlazioni tra i fattori quando non considera adeguatamente la loro natura gerarchica e interconnessa (Asparouhov et al., 2015; Morin et al., 2020).\nNonostante questi vantaggi, l’ESEM presenta alcune limitazioni che devono essere considerate:\n\nComplessità Computazionale: L’ESEM può essere più complesso e richiedere maggiori risorse computazionali rispetto agli approcci tradizionali, soprattutto quando si gestiscono grandi set di dati o modelli con molti fattori.\nInterpretazione dei Risultati: A causa della sua flessibilità, l’ESEM può produrre risultati che sono più difficili da interpretare. Ad esempio, la sovrapposizione tra i fattori può complicare l’interpretazione dei costrutti.\nRischio di Overfitting: La maggiore flessibilità dell’ESEM può anche portare a un rischio maggiore di overfitting, specialmente in campioni più piccoli o con modelli eccessivamente complessi.\nNecessità di Esperienza e Conoscenza: Per utilizzare efficacemente l’ESEM, è richiesta una comprensione approfondita della teoria sottostante e delle tecniche statistiche, che può essere una barriera per alcuni ricercatori.\n\nNonostante queste limitazioni, ci si aspetta che ulteriori sviluppi e applicazioni dell’ESEM portino a soluzioni più integrate e a un consenso più ampio sulle migliori pratiche nell’uso di questo potente strumento statistico.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Exploratory Structural Equation Modeling (ESEM)</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/06_efa_lavaan.html#session-info",
    "href": "chapters/cfa/06_efa_lavaan.html#session-info",
    "title": "37  Exploratory Structural Equation Modeling (ESEM)",
    "section": "37.5 Session Info",
    "text": "37.5 Session Info\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.4.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ggokabeito_0.1.0  viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n [5] ggExtra_0.10.1    bayesplot_1.11.1  gridExtra_2.3     patchwork_1.2.0  \n [9] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-17     psych_2.4.3      \n[13] scales_1.3.0      markdown_1.12     knitr_1.45        lubridate_1.9.3  \n[17] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[21] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.0    \n[25] tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.15.0   jsonlite_1.8.8      magrittr_2.0.3     \n  [4] TH.data_1.1-2       estimability_1.5    nloptr_2.0.3       \n  [7] rmarkdown_2.26      vctrs_0.6.5         minqa_1.2.6        \n [10] base64enc_0.1-3     rstatix_0.7.2       htmltools_0.5.7    \n [13] broom_1.0.5         Formula_1.2-5       htmlwidgets_1.6.4  \n [16] plyr_1.8.9          sandwich_3.1-0      emmeans_1.10.0     \n [19] zoo_1.8-12          uuid_1.2-0          igraph_2.0.2       \n [22] mime_0.12           lifecycle_1.0.4     pkgconfig_2.0.3    \n [25] Matrix_1.6-5        R6_2.5.1            fastmap_1.1.1      \n [28] shiny_1.8.0         numDeriv_2016.8-1.1 digest_0.6.35      \n [31] OpenMx_2.21.11      fdrtool_1.2.17      colorspace_2.1-0   \n [34] rprojroot_2.0.4     Hmisc_5.1-1         fansi_1.0.6        \n [37] timechange_0.3.0    abind_1.4-5         compiler_4.3.3     \n [40] withr_3.0.0         glasso_1.11         htmlTable_2.4.2    \n [43] backports_1.4.1     carData_3.0-5       ggsignif_0.6.4     \n [46] MASS_7.3-60.0.1     corpcor_1.6.10      gtools_3.9.5       \n [49] tools_4.3.3         pbivnorm_0.6.0      foreign_0.8-86     \n [52] zip_2.3.1           httpuv_1.6.14       nnet_7.3-19        \n [55] glue_1.7.0          quadprog_1.5-8      nlme_3.1-164       \n [58] promises_1.2.1      lisrelToR_0.3       grid_4.3.3         \n [61] pbdZMQ_0.3-11       checkmate_2.3.1     cluster_2.1.6      \n [64] reshape2_1.4.4      generics_0.1.3      gtable_0.3.4       \n [67] tzdb_0.4.0          data.table_1.15.2   hms_1.1.3          \n [70] car_3.1-2           utf8_1.2.4          sem_3.1-15         \n [73] pillar_1.9.0        IRdisplay_1.1       rockchalk_1.8.157  \n [76] later_1.3.2         splines_4.3.3       lattice_0.22-5     \n [79] survival_3.5-8      kutils_1.73         tidyselect_1.2.0   \n [82] miniUI_0.1.1.1      pbapply_1.7-2       stats4_4.3.3       \n [85] xfun_0.42           qgraph_1.9.8        arm_1.13-1         \n [88] stringi_1.8.3       boot_1.3-29         evaluate_0.23      \n [91] codetools_0.2-19    mi_1.1              cli_3.6.2          \n [94] RcppParallel_5.1.7  IRkernel_1.3.2      rpart_4.1.23       \n [97] xtable_1.8-4        repr_1.1.6          munsell_0.5.0      \n[100] Rcpp_1.0.12         coda_0.19-4.1       png_0.1-8          \n[103] XML_3.99-0.16.1     parallel_4.3.3      ellipsis_0.3.2     \n[106] jpeg_0.1-10         lme4_1.1-35.1       mvtnorm_1.2-4      \n[109] openxlsx_4.2.5.2    crayon_1.5.2        rlang_1.1.3        \n[112] multcomp_1.4-25     mnormt_2.1.1",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Exploratory Structural Equation Modeling (ESEM)</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/E_01.html",
    "href": "chapters/cfa/E_01.html",
    "title": "38  ✏️ Esercizi",
    "section": "",
    "text": "source(\"../_common.R\")\nsuppressPackageStartupMessages({\n    library(\"lavaan\")\n    library(\"semTools\")\n})\nset.seed(42)\n\nE1. Si ripeta l’esercizio che abbiamo svolto in precedenza usando l’analisi fattoriale esplorativa, questa volta usando la CFA in lavaan. I dati sono forniti da {cite:t}brown2015confirmatory e riguardano a otto misure di personalità raccolte su un campione di 250 pazienti che hanno concluso un programma di psicoterapia:\n\nanxiety (N1),\nhostility (N2),\ndepression (N3),\nself-consciousness (N4),\nwarmth (E1),\ngregariousness (E2),\nassertiveness (E3),\npositive emotions (E4).\n\n\nvarnames &lt;- c(\"N1\", \"N2\", \"N3\", \"N4\", \"E1\", \"E2\", \"E3\", \"E4\")\nsds &lt;- \"5.7  5.6  6.4  5.7  6.0  6.2  5.7  5.6\"\n\ncors &lt;- \"\n 1.000\n 0.767  1.000\n 0.731  0.709  1.000\n 0.778  0.738  0.762  1.000\n-0.351  -0.302  -0.356  -0.318  1.000\n-0.316  -0.280  -0.300  -0.267  0.675  1.000\n-0.296  -0.289  -0.297  -0.296  0.634  0.651  1.000\n-0.282  -0.254  -0.292  -0.245  0.534  0.593  0.566  1.000\"\n\npsychot_cor_mat &lt;- getCov(cors, names = varnames)\nn &lt;- 250\n\nIl modello con due fattori ortogonali può essere adattato ai dati nel modo seguente.\n\ncfa_mod &lt;- \"\n  N =~ N1 + N2 + N3 + N4\n  E =~ E1 + E2 + E3 + E4\n\"\n\n\nfit_cfa &lt;- lavaan::cfa(\n    cfa_mod,\n    sample.cov = psychot_cor_mat,\n    sample.nobs = n,\n    orthogonal = TRUE,\n    std.lv = TRUE\n)\n\n\nsemPlot::semPaths(fit_cfa,\n    what = \"col\", whatLabels = \"par\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\n\nEsaminiamo le saturazioni fattoriali:\n\nparameterEstimates(fit_cfa, standardized = TRUE) %&gt;%\n    dplyr::filter(op == \"=~\") %&gt;%\n    dplyr::select(\n        \"Latent Factor\" = lhs,\n        Indicator = rhs,\n        B = est,\n        SE = se,\n        Z = z,\n        \"p-value\" = pvalue,\n        Beta = std.all\n    ) %&gt;%\n    knitr::kable(\n        digits = 3, booktabs = TRUE, format = \"markdown\",\n        caption = \"Factor Loadings\"\n    )\n\n\n\nTable: Factor Loadings\n\n|Latent Factor |Indicator |     B|    SE|      Z| p-value|  Beta|\n|:-------------|:---------|-----:|-----:|------:|-------:|-----:|\n|N             |N1        | 0.882| 0.051| 17.422|       0| 0.884|\n|N             |N2        | 0.847| 0.052| 16.340|       0| 0.849|\n|N             |N3        | 0.840| 0.052| 16.134|       0| 0.842|\n|N             |N4        | 0.882| 0.051| 17.432|       0| 0.884|\n|E             |E1        | 0.795| 0.056| 14.276|       0| 0.796|\n|E             |E2        | 0.838| 0.054| 15.369|       0| 0.839|\n|E             |E3        | 0.788| 0.056| 14.097|       0| 0.789|\n|E             |E4        | 0.697| 0.058| 11.942|       0| 0.699|\n\n\nIl risultato sembra sensato: le saturazioni su ciascun fattore sono molto alte. Tuttavia, la matrice delle correlazioni residue\n\ncor_table &lt;- residuals(fit_cfa, type = \"cor\")$cov\nknitr::kable(\n    cor_table,\n    digits = 3,\n    format = \"markdown\",\n    booktabs = TRUE\n)\n\n\n\n|   |     N1|     N2|     N3|     N4|     E1|     E2|     E3|     E4|\n|:--|------:|------:|------:|------:|------:|------:|------:|------:|\n|N1 |  0.000|  0.017| -0.013| -0.003| -0.351| -0.316| -0.296| -0.282|\n|N2 |  0.017|  0.000| -0.006| -0.012| -0.302| -0.280| -0.289| -0.254|\n|N3 | -0.013| -0.006|  0.000|  0.018| -0.356| -0.300| -0.297| -0.292|\n|N4 | -0.003| -0.012|  0.018|  0.000| -0.318| -0.267| -0.296| -0.245|\n|E1 | -0.351| -0.302| -0.356| -0.318|  0.000|  0.007|  0.006| -0.022|\n|E2 | -0.316| -0.280| -0.300| -0.267|  0.007|  0.000| -0.011|  0.007|\n|E3 | -0.296| -0.289| -0.297| -0.296|  0.006| -0.011|  0.000|  0.015|\n|E4 | -0.282| -0.254| -0.292| -0.245| -0.022|  0.007|  0.015|  0.000|\n\n\nrivela che il modello ipotizzato dall’analisi fattoriale confermativa non è adeguato.\n\nfit2_cfa &lt;- lavaan::cfa(\n    cfa_mod,\n    sample.cov = psychot_cor_mat,\n    sample.nobs = n,\n    orthogonal = FALSE,\n    std.lv = TRUE\n)\n\n\nsemPlot::semPaths(fit2_cfa,\n    what = \"col\", whatLabels = \"par\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\n\nEsaminiamo le saturazioni fattoriali.\n\nparameterEstimates(fit2_cfa, standardized = TRUE) %&gt;%\n    dplyr::filter(op == \"=~\") %&gt;%\n    dplyr::select(\n        \"Latent Factor\" = lhs,\n        Indicator = rhs,\n        B = est,\n        SE = se,\n        Z = z,\n        \"p-value\" = pvalue,\n        Beta = std.all\n    ) %&gt;%\n    knitr::kable(\n        digits = 3, booktabs = TRUE, format = \"markdown\",\n        caption = \"Factor Loadings\"\n    )\n\n\n\nTable: Factor Loadings\n\n|Latent Factor |Indicator |     B|    SE|      Z| p-value|  Beta|\n|:-------------|:---------|-----:|-----:|------:|-------:|-----:|\n|N             |N1        | 0.883| 0.051| 17.472|       0| 0.885|\n|N             |N2        | 0.847| 0.052| 16.337|       0| 0.849|\n|N             |N3        | 0.842| 0.052| 16.190|       0| 0.844|\n|N             |N4        | 0.880| 0.051| 17.381|       0| 0.882|\n|E             |E1        | 0.800| 0.055| 14.465|       0| 0.802|\n|E             |E2        | 0.832| 0.054| 15.294|       0| 0.834|\n|E             |E3        | 0.788| 0.056| 14.150|       0| 0.789|\n|E             |E4        | 0.698| 0.058| 11.974|       0| 0.699|\n\n\nEsaminiamo i residui.\n\ncor_table &lt;- residuals(fit2_cfa, type = \"cor\")$cov\nknitr::kable(\n    cor_table,\n    digits = 3,\n    format = \"markdown\",\n    booktabs = TRUE\n)\n\n\n\n|   |     N1|     N2|     N3|     N4|     E1|     E2|     E3|     E4|\n|:--|------:|------:|------:|------:|------:|------:|------:|------:|\n|N1 |  0.000|  0.016| -0.015| -0.002| -0.042|  0.005|  0.008| -0.013|\n|N2 |  0.016|  0.000| -0.007| -0.010| -0.006|  0.028|  0.002|  0.004|\n|N3 | -0.015| -0.007|  0.000|  0.018| -0.062|  0.006| -0.007| -0.035|\n|N4 | -0.002| -0.010|  0.018|  0.000| -0.010|  0.053|  0.007|  0.023|\n|E1 | -0.042| -0.006| -0.062| -0.010|  0.000|  0.006|  0.001| -0.027|\n|E2 |  0.005|  0.028|  0.006|  0.053|  0.006|  0.000| -0.007|  0.010|\n|E3 |  0.008|  0.002| -0.007|  0.007|  0.001| -0.007|  0.000|  0.014|\n|E4 | -0.013|  0.004| -0.035|  0.023| -0.027|  0.010|  0.014|  0.000|\n\n\nSistemiamo le saturazioni fattoriali in una matrice 8 \\(\\times\\) 2:\n\nlambda &lt;- inspect(fit2_cfa, what = \"std\")$lambda\nlambda\n\n\nA lavaan.matrix: 8 x 2 of type dbl\n\n\n\nN\nE\n\n\n\n\nN1\n0.8848214\n0.0000000\n\n\nN2\n0.8485128\n0.0000000\n\n\nN3\n0.8436432\n0.0000000\n\n\nN4\n0.8819736\n0.0000000\n\n\nE1\n0.0000000\n0.8018485\n\n\nE2\n0.0000000\n0.8337599\n\n\nE3\n0.0000000\n0.7894530\n\n\nE4\n0.0000000\n0.6990366\n\n\n\n\n\nOtteniamo la matrice di intercorrelazoni fattoriali.\n\nPhi &lt;- inspect(fit2_cfa, what = \"std\")$psi\nPhi\n\n\nA lavaan.matrix.symmetric: 2 x 2 of type dbl\n\n\n\nN\nE\n\n\n\n\nN\n1.000000\n-0.434962\n\n\nE\n-0.434962\n1.000000\n\n\n\n\n\nOtteniamo la matrice di varianze residue.\n\nPsi &lt;- inspect(fit2_cfa, what = \"std\")$theta\nPsi\n\n\nA lavaan.matrix.symmetric: 8 x 8 of type dbl\n\n\n\nN1\nN2\nN3\nN4\nE1\nE2\nE3\nE4\n\n\n\n\nN1\n0.217091\n0.0000000\n0.0000000\n0.0000000\n0.000000\n0.0000000\n0.000000\n0.0000000\n\n\nN2\n0.000000\n0.2800261\n0.0000000\n0.0000000\n0.000000\n0.0000000\n0.000000\n0.0000000\n\n\nN3\n0.000000\n0.0000000\n0.2882661\n0.0000000\n0.000000\n0.0000000\n0.000000\n0.0000000\n\n\nN4\n0.000000\n0.0000000\n0.0000000\n0.2221225\n0.000000\n0.0000000\n0.000000\n0.0000000\n\n\nE1\n0.000000\n0.0000000\n0.0000000\n0.0000000\n0.357039\n0.0000000\n0.000000\n0.0000000\n\n\nE2\n0.000000\n0.0000000\n0.0000000\n0.0000000\n0.000000\n0.3048445\n0.000000\n0.0000000\n\n\nE3\n0.000000\n0.0000000\n0.0000000\n0.0000000\n0.000000\n0.0000000\n0.376764\n0.0000000\n\n\nE4\n0.000000\n0.0000000\n0.0000000\n0.0000000\n0.000000\n0.0000000\n0.000000\n0.5113478\n\n\n\n\n\nMediante i parametri del modello la matrice di correlazione si riproduce nel modo seguente:\n\\[\n\\boldsymbol{\\Sigma} =\\boldsymbol{\\Lambda} \\boldsymbol{\\Phi} \\boldsymbol{\\Lambda}^{\\mathsf{T}} + \\boldsymbol{\\Psi}.\n\\]\nIn \\(\\textsf{R}\\) scriviamo:\n\nR_hat &lt;- lambda %*% Phi %*% t(lambda) + Psi\nR_hat %&gt;%\n    round(3)\n\n\nA lavaan.matrix.symmetric: 8 x 8 of type dbl\n\n\n\nN1\nN2\nN3\nN4\nE1\nE2\nE3\nE4\n\n\n\n\nN1\n1.000\n0.751\n0.746\n0.780\n-0.309\n-0.321\n-0.304\n-0.269\n\n\nN2\n0.751\n1.000\n0.716\n0.748\n-0.296\n-0.308\n-0.291\n-0.258\n\n\nN3\n0.746\n0.716\n1.000\n0.744\n-0.294\n-0.306\n-0.290\n-0.257\n\n\nN4\n0.780\n0.748\n0.744\n1.000\n-0.308\n-0.320\n-0.303\n-0.268\n\n\nE1\n-0.309\n-0.296\n-0.294\n-0.308\n1.000\n0.669\n0.633\n0.561\n\n\nE2\n-0.321\n-0.308\n-0.306\n-0.320\n0.669\n1.000\n0.658\n0.583\n\n\nE3\n-0.304\n-0.291\n-0.290\n-0.303\n0.633\n0.658\n1.000\n0.552\n\n\nE4\n-0.269\n-0.258\n-0.257\n-0.268\n0.561\n0.583\n0.552\n1.000\n\n\n\n\n\nLe correlazioni residue sono:\n\n(psychot_cor_mat - R_hat) %&gt;%\n    round(3)\n\n\nA lavaan.matrix.symmetric: 8 x 8 of type dbl\n\n\n\nN1\nN2\nN3\nN4\nE1\nE2\nE3\nE4\n\n\n\n\nN1\n0.000\n0.016\n-0.015\n-0.002\n-0.042\n0.005\n0.008\n-0.013\n\n\nN2\n0.016\n0.000\n-0.007\n-0.010\n-0.006\n0.028\n0.002\n0.004\n\n\nN3\n-0.015\n-0.007\n0.000\n0.018\n-0.062\n0.006\n-0.007\n-0.035\n\n\nN4\n-0.002\n-0.010\n0.018\n0.000\n-0.010\n0.053\n0.007\n0.023\n\n\nE1\n-0.042\n-0.006\n-0.062\n-0.010\n0.000\n0.006\n0.001\n-0.027\n\n\nE2\n0.005\n0.028\n0.006\n0.053\n0.006\n0.000\n-0.007\n0.010\n\n\nE3\n0.008\n0.002\n-0.007\n0.007\n0.001\n-0.007\n0.000\n0.014\n\n\nE4\n-0.013\n0.004\n-0.035\n0.023\n-0.027\n0.010\n0.014\n0.000\n\n\n\n\n\nCalcoliamo la correlazione predetta dal modello tra le variabili \\(Y_1\\) e \\(Y_2\\):\n\nlambda[1, 1] * lambda[2, 1] + lambda[1, 2] * lambda[2, 2] +\n    lambda[1, 1] * lambda[2, 2] * Phi[1, 2] +\n    lambda[1, 2] * lambda[2, 1] * Phi[1, 2]\n\n0.750782309575684\n\n\nQuesto risultato è molto simile al valore contenuto dell’elemento (1, 2) della matrice di correlazioni osservate:\n\npsychot_cor_mat[1, 2]\n\n0.767\n\n\nUsando le funzonalità di lavaan la matrice di correlazione predetta si ottiene con:\n\nfitted(fit2_cfa)$cov |&gt;\n    print()\n\n       N1     N2     N3     N4     E1     E2     E3     E4\nN1  0.996                                                 \nN2  0.748  0.996                                          \nN3  0.743  0.713  0.996                                   \nN4  0.777  0.745  0.741  0.996                            \nE1 -0.307 -0.295 -0.293 -0.306  0.996                     \nE2 -0.320 -0.306 -0.305 -0.319  0.666  0.996              \nE3 -0.303 -0.290 -0.289 -0.302  0.630  0.656  0.996       \nE4 -0.268 -0.257 -0.255 -0.267  0.558  0.580  0.550  0.996\n\n\nLa matrice dei residui è\n\nresid(fit2_cfa)$cov |&gt;\n    print()\n\n       N1     N2     N3     N4     E1     E2     E3     E4\nN1  0.000                                                 \nN2  0.016  0.000                                          \nN3 -0.015 -0.007  0.000                                   \nN4 -0.002 -0.010  0.018  0.000                            \nE1 -0.042 -0.006 -0.062 -0.010  0.000                     \nE2  0.005  0.028  0.006  0.053  0.006  0.000              \nE3  0.008  0.002 -0.007  0.007  0.001 -0.007  0.000       \nE4 -0.013  0.004 -0.035  0.023 -0.026  0.010  0.014  0.000\n\n\nLa matrice dei residui standardizzati è\n\nresid(fit2_cfa, type = \"standardized\")$cov |&gt;\n    print()\n\n       N1     N2     N3     N4     E1     E2     E3     E4\nN1  0.000                                                 \nN2  1.674  0.000                                          \nN3 -1.769 -0.569  0.000                                   \nN4 -0.350 -1.152  1.746  0.000                            \nE1 -1.214 -0.161 -1.646 -0.294  0.000                     \nE2  0.154  0.794  0.168  1.626  0.637  0.000              \nE3  0.219  0.062 -0.191  0.193  0.075 -0.693  0.000       \nE4 -0.314  0.092 -0.824  0.552 -1.481  0.624  0.690  0.000\n\n\nI valori precedenti possono essere considerati come punti z, dove i valori con un valore assoluto maggiore di 2 possono essere ritenuti problematici. Tuttavia, è importante considerare che in questo modo si stanno eseguendo molteplici confronti, pertanto, si dovrebbe considerare l’opportunità di applicare una qualche forma di correzione per i confronti multipli.\nE2. Si utilizzino i dati dass21.txt che corrispondono alla somministrazione del test DASS-21 a 334 partecipanti. Lo schema di codifica si può trovare seguendo questo link. Si adatti ai dati un modello a tre fattori usando l’analisi fattoriale esplorativa con la funzione lavaan::efa(). Usando le saturazioni fattoriali e la matrice di inter-correlazioni fattoriali, si trovi la matrice di correlazioni riprodotta dal modello. Senza usare l’albebra matriciale, si trovi la correlazione predetta tra gli indicatori DASS-1 e DASS-2.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html",
    "href": "chapters/sem/01_sem_intro.html",
    "title": "39  Introduzione ai Modelli SEM",
    "section": "",
    "text": "39.0.1 Modello di Regressione Lineare Multipla\nIl modello generale di regressione lineare multipla (MLR) si esprime attraverso la seguente equazione:\n\\[\ny_i = \\beta_0 + \\beta_1 x_{1i} + \\cdots + \\beta_p x_{pi} + \\epsilon_i\n\\]\ndove: - \\(i = 1, \\ldots, N\\) indica l’\\(i\\)-esima osservazione, - \\(\\beta_0\\) è l’intercetta del modello, - \\(\\beta_1, \\ldots, \\beta_p\\) sono i coefficienti di regressione associati a ciascuna variabile indipendente, - \\(\\epsilon_i\\) è il termine di errore associato all’\\(i\\)-esima osservazione, - Si presume che il termine di errore sia indipendente dalle variabili esplicative \\(X_s\\).\nIn questa formula, ogni \\(y_i\\) rappresenta il valore della variabile dipendente per l’\\(i\\)-esima osservazione, i coefficienti \\(\\beta\\) quantificano l’impatto di ogni variabile indipendente sulla variabile dipendente, e \\(\\epsilon_i\\) rappresenta l’errore o la varianza non spiegata nella previsione di \\(y_i\\). Questa struttura consente di modellare relazioni lineari tra una variabile dipendente e più variabili indipendenti.\nIl modello MLR può essere rappresentato anche in forma matriciale come segue:\n\\[\ny = X\\beta + \\epsilon\n\\]\ndove: - \\(y\\) è un vettore \\(N \\times 1\\) dei valori osservati della variabile risposta, - \\(X\\) è una matrice di progettazione \\(N \\times (p+1)\\) che incorpora tutte le \\(p\\) variabili indipendenti e una colonna di uno per l’intercetta, - \\(\\beta\\) è un vettore di dimensione \\((p+1)\\) che contiene i parametri di regressione, inclusa l’intercetta, - \\(\\epsilon\\) rappresenta il vettore dei termini di errore.\nLa matrice \\(X\\) e i vettori \\(y\\) e \\(\\epsilon\\) sono definiti nel modo seguente:\n\\[\ny =\n\\begin{pmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_N\n\\end{pmatrix}, \\quad\n\\epsilon =\n\\begin{pmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\vdots \\\\\n\\epsilon_N\n\\end{pmatrix}, \\quad\nX =\n\\begin{pmatrix}\n1 & x_{11} & \\cdots & x_{p1} \\\\\n1 & x_{12} & \\cdots & x_{p2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{1N} & \\cdots & x_{pN}\n\\end{pmatrix}\n\\]\nOgni riga di \\(X\\) corrisponde a un’osservazione e include i valori delle variabili indipendenti per quella osservazione più un uno per l’intercetta.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#errore-di-specificazione",
    "href": "chapters/sem/01_sem_intro.html#errore-di-specificazione",
    "title": "39  Introduzione ai Modelli SEM",
    "section": "39.1 Errore di Specificazione",
    "text": "39.1 Errore di Specificazione\nSpiritosamente chiamato “heartbreak of L.O.V.E.” [Left-Out Variable Error; {cite:t}mauro1990understanding], l’errore di specificazione è una caratteristica fondamentale dei modelli di regressione che deve sempre essere tenuta a mente quando interpretiamo i risultati di questa tecnica di analisi statistica.\nL’errore di specificazione si verifica quando escludiamo dal modello di regressione una variabile che ha due caratteristiche:\n\nè associata con altre variabili inserite nel modello,\nha un effetto diretto sulla \\(y\\).\n\nCome conseguenza dell’errore di specificazione, l’intensità e il segno dei coefficienti parziali di regressione risultano sistematicamente distorti.\nConsideriamo un esempio con dati simulati nei quali immaginiamo che la prestazione sia positivamente associata alla motivazione e negativamente associata all’ansia. Immaginiamo inoltre che vi sia una correlazione positiva tra ansia a motivazione. Ci chiediamo cosa succede al coefficiente parziale della variabile “motivazione” se la variabile “ansia” viene esclusa dal modello di regressione.\n\nset.seed(123)\nn &lt;- 400\nanxiety &lt;- rnorm(n, 10, 1.5)\nmotivation &lt;- 4.0 * anxiety + rnorm(n, 0, 3.5)\ncor(anxiety, motivation)\n\n0.861770572096147\n\n\nCreiamo la variabile performance come una combinazione lineare di motivazione e ansia nella quale la motivazione ha un effetto piccolo, ma positivo, sulla prestazione, e l’ansia ha un grande effetto negativo sulla prestazione:\n\nperformance &lt;-  0.5 * motivation - 5.0 * anxiety + rnorm(n, 0, 3)\n\nSalviamo i dati in un data frame:\n\nsim_dat2 &lt;- tibble(performance, motivation, anxiety)\nsim_dat2 |&gt;\n    head()\n\n\nA tibble: 6 x 3\n\n\nperformance\nmotivation\nanxiety\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n-26.53773\n36.37970\n9.159287\n\n\n-32.98337\n34.52866\n9.654734\n\n\n-35.55939\n47.13063\n12.338062\n\n\n-26.90895\n40.32210\n10.105763\n\n\n-28.57925\n43.12316\n10.193932\n\n\n-40.17393\n44.51348\n12.572597\n\n\n\n\n\nEseguiamo l’analisi di regressione specificando il modello in maniera corretta, ovvero usando come predittori l’ansia e la depressione:\n\nfm1 &lt;- lm(performance ~ motivation + anxiety, sim_dat2)\n\nLe stime dei coefficienti parziali di regressione recuperano correttamente l’intensità e il segno dei coefficienti utilizzati nel modello generatore dei dati:\n\nprint(coef(fm1))\n\n(Intercept)  motivation     anxiety \n  1.3711965   0.4953886  -5.1052176 \n\n\nEseguiamo ora l’analisi di regressione ignorando il predittore anxiety che ha le due caratteristiche di essere associato a motivation e di avere un effetto diretto sulla prestazione:\n\nfm2 &lt;- lm(performance ~ motivation, sim_dat2)\nsummary(fm2) \n\n\nCall:\nlm(formula = performance ~ motivation, data = sim_dat2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-13.501  -3.409   0.005   3.311  12.616 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -12.39720    1.44591  -8.574 2.24e-16 ***\nmotivation   -0.43717    0.03553 -12.305  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.866 on 398 degrees of freedom\nMultiple R-squared:  0.2756,    Adjusted R-squared:  0.2738 \nF-statistic: 151.4 on 1 and 398 DF,  p-value: &lt; 2.2e-16\n\n\nSi noti che il risultato prodotto dal modello di regressione è totalmente sbagliato: come conseguenza dell’errore di specificazione, il segno del coefficiente parziale di regressione della variabile “motivazione” è negativo, anche se nel modello generatore dei dati tale coefficiente aveva il segno opposto.\nQuindi, se interpretassimo il coefficiente parziale ottenuto in termini casuali, saremmo portati a concludere che la motivazione fa diminuire la prestazione. Ma in realtà è vero l’opposto.\nÈ facile vedere perché si verifica l’errore di specificazione. Supponiamo che il vero modello sia\n\\[\ny = \\alpha + \\beta_1 X_1 + \\beta_2 X_2 + \\varepsilon\n\\]\nil quale verrebbe stimato da\n\\[\ny = a + b_1 X_1 + b_2 X_2 + e.\n\\]\nSupponiamo però che il ricercatore creda invece che\n\\[\ny = \\alpha^\\prime + \\beta_1^\\prime X_1 + \\varepsilon^\\prime\n\\]\ne quindi stimi\n\\[\ny = a^\\prime + b_1^\\prime X_1 + e^\\prime\n\\]\nomettendo \\(X_2\\) dal modello.\nPer capire che relazione intercorre tra \\(b_1^\\prime\\) e \\(b_1\\), iniziamo a scrivere la formula per \\(b_1^\\prime\\):\n\\[\n\\begin{equation}\nb_1^\\prime = \\frac{Cov(X_1, Y)}{Var(X_1)}.\n\\end{equation}\n\\]\nSviluppando, otteniamo\n\\[\n\\begin{equation}\n\\begin{aligned}\nb_1^\\prime &= \\frac{Cov(X_1, a + b_1 X_1 + b_2 X_2 + e)}{Var(X_1)}\\notag\\\\\n&= \\frac{Cov(X_1, a)+b_1 Cov(X_1, X_1) + b_2 Cov(X_1, X_2) + Cov(X_1, e)}{Var(X_1)}\\notag\\\\\n&= \\frac{0 + b_1 Var(X_1) + b_2 Cov(X_1, X_2) + 0}{Var(X_1)}\\notag\\\\\n&= b_1 + b_2 \\frac{Cov(X_1, X_2)}{Var(X_1)}.\n\\end{aligned}\n\\end{equation}\n\\]\nQuindi, se erroneamente omettiamo \\(X_2\\) dal modello, abbiamo che\n\\[\n\\begin{equation}\n\\mathbb{E}(b_1^\\prime) = \\beta_1 + \\beta_2 \\frac{\\sigma_{12}}{\\sigma_1^2}.\n\\end{equation}\n\\](eq-specific-err)\nVerifichiamo tale conclusione per i dati dell’esempio che stiamo discutendo. Nel caso presente, \\(X_1\\) è motivation e \\(X_2\\) è anxiety. Applicando l’eq. {eq}eq-specific-err otteniamo lo stesso valore per il coefficiente di regressione associato a motivation che era stato ottenuto adattando ai dati il modello performance ~ motivation:\n\nfm1$coef[2] +  fm1$coef[3] * \n  cov(sim_dat2$motivation, sim_dat2$anxiety) / \n  var(sim_dat2$motivation)\n\nmotivation: -0.437167456259783\n\n\nPossiamo dunque concludere che \\(b_1^\\prime\\) è uno stimatore distorto di \\(\\beta_1\\). Si noti che questa distorsione non scompare all’aumentare della numerosità campionaria, il che (in termini statistici) significa che un tale stimatore è inconsistente. Quello che succede in pratica è che alla variabile \\(X_1\\) vengono attribuiti gli effetti delle variabili che sono state omesse dal modello. Si noti che una tale distorsione sistematica di \\(b_1^\\prime\\) può essere evitata solo se si verificano due condizioni:\n\n\\(\\beta_2 = 0\\). Questo è ovvio, dato che, se \\(\\beta_2 = 0\\), ciò significa che il modello non è specificato in modo errato, cioè \\(X_2\\) non appartiene al modello perché non ha un effetto diretto sulla \\(Y\\).\n\\(\\sigma_{12} = 0\\). Cioè, se \\(X_1\\) e \\(X_2\\) sono incorrelate, allora l’omissione di una delle due variabili non comporta stime distorte dell’effetto dell’altra.\n\n\n39.1.1 Soppressione\nLe conseguenze dell’errore di specificazione sono chiamate “soppressione” (suppression). In generale, si ha soppressione quando (1) il valore assoluto del peso beta di un predittore è maggiore di quello della sua correlazione bivariata con il criterio o (2) i due hanno segni opposti.\n\nL’esempio descritto sopra è un caso di soppressione negativa, dove il predittore ha correlazioni bivariate positive con il criterio, ma si riceve un peso beta negativo nell’analisi di regressione multipla.\nUn secondo tipo di soppressione è la soppressione classica, in cui un predittore non è correlato al criterio ma riceve un peso beta diverso da zero.\nC’è anche la soppressione reciproca che può verificarsi quando due variabili sono correlate positivamente con il criterio ma negativamente tra loro.\n\n\n\n39.1.2 Regressione Stepwise\nNel contesto della regressione, è importante comprendere che i predittori non dovrebbero essere selezionati basandosi unicamente sulle loro correlazioni bivariate con la variabile dipendente (il criterio). Queste correlazioni, note come associazioni di “ordine zero”, non tengono conto dell’influenza degli altri predittori. Di conseguenza, i valori delle correlazioni bivariate possono risultare fuorvianti quando si considerano i coefficienti di regressione parziale per le stesse variabili.\nLa significatività statistica delle correlazioni bivariate con la variabile dipendente non è un criterio affidabile per la selezione dei predittori. Questo perché tali correlazioni non considerano gli effetti complessivi degli altri predittori nel modello.\nLe procedure di selezione automatica dei predittori, come quelle impiegate nelle regressioni stepwise, possono essere seducenti per la loro facilità d’uso. Tuttavia, queste procedure sono rischiose. Anche piccole non-linearità o effetti indiretti tra i predittori, che potrebbero non essere immediatamente evidenti, possono distorcere in modo significativo i coefficienti di regressione parziale.\nInvece di affidarsi a metodi automatici, è preferibile selezionare un numero limitato di predittori basandosi su considerazioni teoriche o sui risultati di ricerche precedenti. Questo approccio più ponderato aiuta a evitare le distorsioni che possono emergere dall’uso di procedure di selezione automatica.\nUna volta che sono stati selezionati, i predittori possono essere inseriti nell’equazione di regressione in due modi diversi:\n\ntutti i predittori possono essere inseriti nel modello contemporaneamente;\ni predittori possono essere inseriti nel modello sequenzialmente, mediante una serie di passaggi.\n\nL’ordine di ingresso può essere determinato in base a standard: teorici (razionali) o empirici (statistici). Lo standard razionale corrisponde alla regressione gerarchica, in cui si comunica al computer un ordine fisso per inserire i predittori. Ad esempio, a volte le variabili demografiche vengono inserite nel primo passaggio, quindi nel secondo passaggio viene inserita una variabile psicologica di interesse. Questo ordine non solo controlla le variabili demografiche ma permette anche di valutare il potere predittivo della variabile psicologica, al di là di quello delle semplici variabili demografiche. Quest’ultimo può essere stimato come l’aumento della correlazione multipla al quadrato, o \\(\\Delta R^2\\), da quella della fase 1 con solo predittori demografici a quella della fase 2 con tutti i predittori nell’equazione di regressione.\nUn esempio di standard statistico è la regressione stepwise, in cui il computer seleziona l’inserimento dei predittori in base esclusivamente alla significatività statistica; cioè, viene chiesto: quale predittore, se inserito nell’equazione, avrebbe il valore_\\(p\\) più piccolo per il test del suo coefficiente di regressione parziale? Dopo la selezione, i predittori in una fase successiva possono essere rimossi dall’equazione di regressione in base ai loro valori-\\(p\\) (ad esempio, se \\(p \\geq\\) .05). Il processo stepwise si interrompe quando, aggiungendo più predittori, \\(\\Delta R^2\\) non migliora. Varianti della regressione stepwise includono forward inclusion, in cui i predittori selezionati non vengono successivamente rimossi dal modello, e backward elimination, che inizia con tutti i predittori nel modello per poi rimuoverne alcuni in passi successivi. Per le ragioni descritte nel paragrafo sull’errore di specificazione, i metodi basati sulle procedure di stepwise regression non dovrebbero mai essere usati. Infatti, i problemi relativi a tale procedura sono così gravi che varie riviste non accettano studi che fanno uso di una tale tecnica statistica. I risultati ottenuti con tali metodi, infatti, sono quasi certamente non replicabili in campioni diversi.\nUna considerazione finale riguarda l’idea di rimuovere i predittori “non significativi” dal modello di regressione. Questa è una cattiva idea. Il ricercatore non deve sentirsi in dovere di trascurare quei predittore che non risultano “statisticamente significativi”. In campioni piccoli, la potenza dei test di significatività è bassa e la rimozione di un predittore non significativo può alterare sostanzialmente la soluzione. Se c’è una buona ragione per includere un predittore, allora è meglio lasciarlo nel modello, fino a prova contraria. In termini generali, qualsiasi considerazione basata sulla “significatività statistia” è fuorviante.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#oltre-la-regressione-multipla-luso-dei-modelli-sem",
    "href": "chapters/sem/01_sem_intro.html#oltre-la-regressione-multipla-luso-dei-modelli-sem",
    "title": "39  Introduzione ai Modelli SEM",
    "section": "39.2 Oltre la Regressione Multipla: L’Uso dei Modelli SEM",
    "text": "39.2 Oltre la Regressione Multipla: L’Uso dei Modelli SEM\nAnalizziamo un esempio in cui il modello di Equazioni Strutturali (SEM) viene impiegato per studiare la relazione tra autocompassione e malessere psicologico, utilizzando come indicatori le sotto-scale della DASS-21 (Depressione, Ansia, Stress) e della Self-Compassion Scale. In questo contesto, definiamo due variabili latenti: “malessere psicologico” e “autocompassione”. La variabile latente “malessere psicologico” è composta dalle tre sotto-scale della DASS-21, mentre la variabile “autocompassione” è formata dalle sei sotto-scale della Self-Compassion Scale.\nIl modello strutturale esplora la relazione tra queste due variabili latenti. L’autocompassione è considerata una variabile esogena, ipotizzata come un fattore di protezione che riduce il malessere psicologico, che a sua volta è trattato come variabile endogena. L’ipotesi principale del modello è che esista una relazione di regressione negativa tra autocompassione e malessere psicologico, indicando che livelli più elevati di autocompassione sono associati a minori livelli di malessere psicologico.\nUn elemento chiave dei modelli SEM è la gestione dell’errore di misurazione. Le variabili latenti sono progettate per riflettere il nucleo vero dei costrutti teorici, in questo caso autocompassione e malessere psicologico, isolando gli effetti degli errori di misurazione che possono affliggere gli indicatori osservati. Questo approccio consente di esaminare la “vera” relazione tra i costrutti, eliminando le distorsioni introdotte dagli errori di misurazione nelle misure osservate.\nLa capacità del modello SEM di separare la variabilità attribuibile ai costrutti latenti da quella dovuta agli errori di misurazione aumenta l’accuratezza e l’affidabilità dell’analisi. Questo è particolarmente vantaggioso in campi come la psicologia, dove i costrutti teorici non sono direttamente osservabili e devono essere inferiti attraverso misure potenzialmente errate.\n\nmod_sc &lt;- \"\n  F =~ anxiety + depression + stress\n  SC =~ self_kindness   + common_humanity   + mindfulness   + self_judgment + isolation + over_identification\n  F ~ SC \n\"\n\nAdattiamo il modello ai dati.\n\nfit_sc &lt;- lavaan::sem(mod_sc, dat, std.lv = TRUE)\n\nEsaminiamo la soluzione ottenuta.\n\nstandardizedSolution(fit_sc) |&gt; print()\n\n                   lhs op                 rhs est.std    se       z pvalue\n1                    F =~             anxiety   0.847 0.014  58.505      0\n2                    F =~          depression   0.909 0.011  82.503      0\n3                    F =~              stress   0.929 0.010  91.945      0\n4                   SC =~       self_kindness   0.757 0.022  33.984      0\n5                   SC =~     common_humanity   0.621 0.030  20.695      0\n6                   SC =~         mindfulness   0.689 0.026  26.235      0\n7                   SC =~       self_judgment  -0.770 0.022 -35.799      0\n8                   SC =~           isolation  -0.770 0.022 -35.822      0\n9                   SC =~ over_identification  -0.767 0.022 -35.416      0\n10                   F  ~                  SC  -0.476 0.038 -12.385      0\n11             anxiety ~~             anxiety   0.282 0.025  11.495      0\n12          depression ~~          depression   0.173 0.020   8.633      0\n13              stress ~~              stress   0.136 0.019   7.246      0\n14       self_kindness ~~       self_kindness   0.427 0.034  12.647      0\n15     common_humanity ~~     common_humanity   0.615 0.037  16.527      0\n16         mindfulness ~~         mindfulness   0.525 0.036  14.522      0\n17       self_judgment ~~       self_judgment   0.407 0.033  12.285      0\n18           isolation ~~           isolation   0.407 0.033  12.280      0\n19 over_identification ~~ over_identification   0.411 0.033  12.360      0\n20                   F ~~                   F   0.774 0.037  21.167      0\n21                  SC ~~                  SC   1.000 0.000      NA     NA\n   ci.lower ci.upper\n1     0.819    0.876\n2     0.888    0.931\n3     0.910    0.949\n4     0.713    0.801\n5     0.562    0.679\n6     0.637    0.740\n7    -0.812   -0.728\n8    -0.812   -0.728\n9    -0.810   -0.725\n10   -0.551   -0.400\n11    0.234    0.330\n12    0.134    0.212\n13    0.099    0.173\n14    0.361    0.493\n15    0.542    0.688\n16    0.454    0.596\n17    0.342    0.472\n18    0.342    0.472\n19    0.346    0.476\n20    0.702    0.845\n21    1.000    1.000\n\n\n\nSaturazioni Fattoriali (Loadings) per le Variabili Latenti:\n\nF: Le variabili osservate “anxiety”, “depression”, e “stress” hanno elevate saturazioni fattoriali sulla variabile latente “F”. Questo suggerisce che ciascuna di queste misure è un buon indicatore della variabile latente “F”.\nSC: Le variabili “self_kindness”, “common_humanity”, “mindfulness”, “self_judgment”, “isolation”, e “over_identification” hanno anch’esse significative saturazioni sulla variabile latente “SC”. Si noti che “self_judgment”, “isolation”, e “over_identification” hanno saturazioni negative, indicando che queste variabili sono inversamente associate con “SC”.\n\nRegressione tra Variabili Latenti:\n\nLa relazione di regressione tra “F” e “SC” mostra un coefficiente negativo (-0.48), il che indica una relazione inversa tra queste due variabili latenti. Questo significa che livelli più alti di “SC” sono associati a livelli più bassi di “F”.\n\nVarianza delle Variabili Latenti:\n\nLa varianza di “F” e “SC” indica quanto della variazione nelle variabili latenti è spiegata dai loro rispettivi indicatori. La varianza di “F” (0.77) è relativamente alta, suggerendo che gli indicatori spiegano una buona parte della varianza in “F”. La varianza di “SC” è fissata a 1, un approccio comune per identificare il modello.\n\nVarianze Residue degli Indicatori:\n\nLe varianze residue (ad esempio, “anxiety ~~ anxiety”) rappresentano la varianza non spiegata in ciascun indicatore dalle variabili latenti. Valori più bassi indicano che la variabile latente spiega una maggior parte della varianza dell’indicatore. Ad esempio, “anxiety” ha una varianza residua di 0.28, suggerendo che “F” spiega una buona parte, ma non tutta, della varianza in “anxiety”.\n\n\nGeneriamo una rappresentazione grafica del modello.\n\nsemPaths(fit_sc,\n    whatLabels = \"std\",\n    sizeMan = 10,\n    edge.label.cex = 0.9,\n    style = \"mx\",\n    nCharNodes = 5, nCharEdges = 0, \n    fade=FALSE\n)\n\n\n\n\n\n\n\n\nI coefficienti stimati nel modello SEM appaiono coerenti e in linea con le aspettative, in particolare il coefficiente che descrive l’effetto “causale” del fattore dell’autocompassione sul malessere psicologico, che si attesta a -0.48. Questo valore negativo corrobora l’ipotesi secondo cui l’autocompassione svolge un ruolo di fattore protettivo contro il malessere psicologico. Tuttavia, prima di confermare definitivamente questa conclusione, è cruciale esaminare gli indici di bontà di adattamento del modello. Questi indici ci permetteranno di valutare quanto accuratamente il modello SEM si adatta ai dati osservati, fornendo un quadro più chiaro della validità delle nostre inferenze. In altre parole, sebbene il modello suggerisca una relazione negativa tra autocompassione e malessere psicologico, la conferma finale di questa associazione dipenderà dall’adeguatezza complessiva del modello rispetto ai dati. Questo argomento verrà affrontato nel prossimo capitolo.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#impiego-delle-medie-nei-modelli-sem",
    "href": "chapters/sem/01_sem_intro.html#impiego-delle-medie-nei-modelli-sem",
    "title": "39  Introduzione ai Modelli SEM",
    "section": "39.3 Impiego delle Medie nei Modelli SEM",
    "text": "39.3 Impiego delle Medie nei Modelli SEM\nNei modelli di equazioni strutturali (SEM), come nell’analisi fattoriale, l’accento è posto sull’analisi delle covarianze tra variabili. Tuttavia, a differenza dell’analisi fattoriale, i modelli SEM consentono di includere anche le medie delle variabili osservate e latenti. Questo arricchisce l’analisi, fornendo informazioni preziose in molti contesti, come nei modelli CFA longitudinali, dove le ipotesi centrali riguardano proprio le medie dei costrutti.\n\n39.3.1 Struttura delle Medie nel Modello SEM\nL’equazione generale per la struttura delle medie in un modello SEM è la seguente:\n\\[\nE(y) = \\mu_y = T + \\Lambda A\n\\]\ndove:\n\n( y ) indica i punteggi degli indicatori.\n( E(y) ) rappresenta la media attesa di ( y ).\n( _y ) è il vettore delle medie dei modelli degli indicatori, analogo a ( ) nelle strutture di covarianza.\n( ) è la matrice dei carichi fattoriali, che stima le relazioni degli indicatori con i costrutti.\n( T ) è il vettore delle medie degli indicatori.\n( A ) è un vettore delle medie dei costrutti latenti.\n\nIn un diagramma a percorsi, un triangolo contrassegnato con il numero 1 rappresenta la costante di regressione. Questo simbolo indica l’intercetta, che viene utilizzata per stimare la media quando una variabile viene regredita su di essa.\nLa media di ciascun indicatore è stimata nel vettore ( ), mentre ( ) rappresenta la matrice di saturazioni fattoriali del modello di misurazione CFA. Questo collegamento tra i carichi e le medie indica che gli indicatori con carichi più elevati hanno un impatto maggiore sulla media del costrutto.\nIl simbolo ( ) rappresenta le medie stimate dei costrutti latenti. Questa simbologia aiuta a distinguere tra le medie degli indicatori (( )) e quelle dei costrutti latenti (( )) nelle equazioni.\n\n\n39.3.2 Vincoli e Scalatura delle Medie\nSimilmente alla gestione delle strutture di covarianza nei modelli SEM, anche per le strutture delle medie è necessario impostare un vincolo per definire la scala. Per le medie, lo zero viene spesso adottato come riferimento. Stabilire questo vincolo aiuta a calcolare le distanze dai punti fissati, che possono essere sia positive sia negative.\n\n\n39.3.3 Stima delle Intercette con lavaan\nPer stimare le intercette in un modello SEM, è essenziale avere accesso ai dati originali o a una matrice di covarianza, oltre alle medie di tutte le variabili coinvolte. L’utilizzo del software lavaan facilita questo processo. Impostando meanstructure = true, si indica a lavaan di integrare automaticamente una costante “1” in tutte le equazioni del modello, facilitando il calcolo delle intercette per le variabili endogene. Questo permette di calcolare con precisione le intercette, che sono cruciali per il modello delle strutture delle medie.\nIn conclusione, la struttura delle medie in un modello SEM è essenziale per ottenere stime accurate delle medie delle variabili, permettendo di confrontare queste stime con le medie osservate nei dati raccolti, proprio come si confrontano le covarianze nel modello con quelle osservate nei dati. Questo approccio arricchisce significativamente l’analisi fornita dai modelli SEM.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#considerazioni-conclusive",
    "href": "chapters/sem/01_sem_intro.html#considerazioni-conclusive",
    "title": "39  Introduzione ai Modelli SEM",
    "section": "39.4 Considerazioni Conclusive",
    "text": "39.4 Considerazioni Conclusive\nIn questo capitolo, abbiamo esplorato i Modelli di Equazioni Strutturali (SEM), evidenziando come questi modelli non si limitino a descrivere le correlazioni tra variabili osservabili, ma permettano anche di analizzare le relazioni tra variabili latenti. La forza dei SEM risiede nella loro capacità di integrare il modello di misurazione, che definisce le relazioni tra gli indicatori e le variabili latenti, con il modello strutturale, che esamina le interazioni tra le stesse variabili latenti.\nNei prossimi capitoli, approfondiremo vari aspetti della modellazione SEM. Esamineremo la bontà di adattamento del modello, un criterio fondamentale per verificare la fedeltà con cui il modello riflette la realtà osservata. Analizzeremo anche il confronto tra modelli alternativi, un passaggio cruciale per identificare il modello che migliora l’interpretazione dei dati.\nUn altro tema importante sarà l’analisi dell’applicabilità dei modelli a gruppi diversi, vitale per valutare la loro generalizzabilità e la pertinenza in contesti specifici. Inoltre, discuteremo le sfide metodologiche legate alla gestione di dati categoriali, all’implementazione di modelli SEM multilivello e alla gestione di dati mancanti. Questi approfondimenti ci permetteranno di comprendere meglio come i modelli SEM possono essere adattati e applicati efficacemente in diversi ambiti di ricerca.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#session-info",
    "href": "chapters/sem/01_sem_intro.html#session-info",
    "title": "39  Introduzione ai Modelli SEM",
    "section": "39.5 Session Info",
    "text": "39.5 Session Info\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.4.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] effectsize_0.8.7   rsvg_2.6.0         DiagrammeRsvg_0.1  mvnormalTest_1.0.0\n [5] lavaanExtra_0.2.0  ggokabeito_0.1.0   viridis_0.6.5      viridisLite_0.4.2 \n [9] ggpubr_0.6.0       ggExtra_0.10.1     bayesplot_1.11.1   gridExtra_2.3     \n[13] patchwork_1.2.0    semTools_0.5-6     semPlot_1.1.6      lavaan_0.6-17     \n[17] psych_2.4.3        scales_1.3.0       markdown_1.12      knitr_1.45        \n[21] lubridate_1.9.3    forcats_1.0.0      stringr_1.5.1      dplyr_1.1.4       \n[25] purrr_1.0.2        readr_2.1.5        tidyr_1.3.1        tibble_3.2.1      \n[29] ggplot2_3.5.0      tidyverse_2.0.0    here_1.0.1        \n\nloaded via a namespace (and not attached):\n  [1] splines_4.3.3       later_1.3.2         pbdZMQ_0.3-11      \n  [4] datawizard_0.9.1    XML_3.99-0.16.1     rpart_4.1.23       \n  [7] lifecycle_1.0.4     rstatix_0.7.2       rprojroot_2.0.4    \n [10] lattice_0.22-5      MASS_7.3-60.0.1     insight_0.19.10    \n [13] rockchalk_1.8.157   backports_1.4.1     magrittr_2.0.3     \n [16] openxlsx_4.2.5.2    Hmisc_5.1-1         rmarkdown_2.26     \n [19] httpuv_1.6.14       qgraph_1.9.8        zip_2.3.1          \n [22] pbapply_1.7-2       minqa_1.2.6         ADGofTest_0.3      \n [25] multcomp_1.4-25     abind_1.4-5         quadprog_1.5-8     \n [28] pspline_1.0-19      nnet_7.3-19         TH.data_1.1-2      \n [31] sandwich_3.1-0      moments_0.14.1      nortest_1.0-4      \n [34] arm_1.13-1          codetools_0.2-19    tidyselect_1.2.0   \n [37] lme4_1.1-35.1       stats4_4.3.3        base64enc_0.1-3    \n [40] jsonlite_1.8.8      ellipsis_0.3.2      Formula_1.2-5      \n [43] survival_3.5-8      emmeans_1.10.0      tools_4.3.3        \n [46] Rcpp_1.0.12         glue_1.7.0          mnormt_2.1.1       \n [49] xfun_0.42           IRdisplay_1.1       withr_3.0.0        \n [52] numDeriv_2016.8-1.1 fastmap_1.1.1       boot_1.3-29        \n [55] fansi_1.0.6         digest_0.6.35       mi_1.1             \n [58] timechange_0.3.0    R6_2.5.1            mime_0.12          \n [61] estimability_1.5    colorspace_2.1-0    gtools_3.9.5       \n [64] jpeg_0.1-10         copula_1.1-3        utf8_1.2.4         \n [67] generics_0.1.3      data.table_1.15.2   corpcor_1.6.10     \n [70] htmlwidgets_1.6.4   parameters_0.21.6   pkgconfig_2.0.3    \n [73] sem_3.1-15          gtable_0.3.4        pcaPP_2.0-4        \n [76] htmltools_0.5.7     carData_3.0-5       png_0.1-8          \n [79] rstudioapi_0.15.0   tzdb_0.4.0          reshape2_1.4.4     \n [82] uuid_1.2-0          coda_0.19-4.1       checkmate_2.3.1    \n [85] nlme_3.1-164        curl_5.2.1          nloptr_2.0.3       \n [88] repr_1.1.6          zoo_1.8-12          parallel_4.3.3     \n [91] miniUI_0.1.1.1      foreign_0.8-86      pillar_1.9.0       \n [94] grid_4.3.3          vctrs_0.6.5         promises_1.2.1     \n [97] car_3.1-2           OpenMx_2.21.11      xtable_1.8-4       \n[100] cluster_2.1.6       htmlTable_2.4.2     evaluate_0.23      \n[103] pbivnorm_0.6.0      mvtnorm_1.2-4       cli_3.6.2          \n[106] kutils_1.73         compiler_4.3.3      rlang_1.1.3        \n[109] crayon_1.5.2        ggsignif_0.6.4      fdrtool_1.2.17     \n[112] plyr_1.8.9          stringi_1.8.3       munsell_0.5.0      \n[115] gsl_2.1-8           lisrelToR_0.3       bayestestR_0.13.2  \n[118] V8_4.4.2            Matrix_1.6-5        IRkernel_1.3.2     \n[121] hms_1.1.3           stabledist_0.7-1    glasso_1.11        \n[124] shiny_1.8.0         igraph_2.0.2        broom_1.0.5        \n[127] RcppParallel_5.1.7",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/02_data_preparation.html",
    "href": "chapters/sem/02_data_preparation.html",
    "title": "40  Preparazione dei Dati",
    "section": "",
    "text": "40.1 Formati dei Dati di Input\nI ricercatori spesso analizzano file di dati grezzi. Tuttavia, alcune analisi SEM possono essere eseguite anche con matrici di covarianze e medie. Se si utilizzano dati grezzi, il software SEM crea una propria matrice di covarianza per l’analisi. Talvolta, è necessario usare dati grezzi, come in casi di distribuzioni non normali, dati mancanti o variabili categoriali.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Preparazione dei Dati</span>"
    ]
  },
  {
    "objectID": "chapters/sem/02_data_preparation.html#definitezza-positiva",
    "href": "chapters/sem/02_data_preparation.html#definitezza-positiva",
    "title": "40  Preparazione dei Dati",
    "section": "40.2 Definitezza Positiva",
    "text": "40.2 Definitezza Positiva\nÈ fondamentale che la matrice di dati, sia quella inizialmente fornita come input che quella calcolata dal computer durante l’analisi, soddisfi i criteri di essere positiva definita. Questo concetto implica diverse proprietà chiave: innanzitutto, la matrice deve avere un inverso, il che significa che non è singolare e può essere invertita matematicamente. Inoltre, è necessario che tutti gli autovalori della matrice siano positivi, indicando che non esistono autovalori negativi che potrebbero causare problemi durante l’analisi. Inoltre, la matrice deve essere priva di correlazioni o covarianze al di fuori limite.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Preparazione dei Dati</span>"
    ]
  },
  {
    "objectID": "chapters/sem/02_data_preparation.html#dati-mancanti",
    "href": "chapters/sem/02_data_preparation.html#dati-mancanti",
    "title": "40  Preparazione dei Dati",
    "section": "40.3 Dati Mancanti",
    "text": "40.3 Dati Mancanti\nQuesto è un argomento complesso che richiede l’uso di metodi statistici moderni e sarà approfondito in un capitolo successivo.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Preparazione dei Dati</span>"
    ]
  },
  {
    "objectID": "chapters/sem/02_data_preparation.html#screening-dei-dati",
    "href": "chapters/sem/02_data_preparation.html#screening-dei-dati",
    "title": "40  Preparazione dei Dati",
    "section": "40.4 Screening dei Dati",
    "text": "40.4 Screening dei Dati\n\nCollinearità Estrema, Valori Anomali e Violazioni delle Assunzioni Distribuzionali: È importante gestire questi problemi per assicurare l’affidabilità dei risultati SEM. La collinearità estrema può essere rilevata tramite il fattore di inflazione della varianza (VIF), mentre i valori anomali e le violazioni delle ipotesi distribuzionali richiedono metodi specifici per essere identificati e gestiti.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Preparazione dei Dati</span>"
    ]
  },
  {
    "objectID": "chapters/sem/02_data_preparation.html#varianze-relative",
    "href": "chapters/sem/02_data_preparation.html#varianze-relative",
    "title": "40  Preparazione dei Dati",
    "section": "40.5 Varianze Relative",
    "text": "40.5 Varianze Relative\n\nGestione delle Varianze: La differenza eccessiva tra le varianze può complicare l’iterazione dei metodi di stima in SEM. Per mitigare questo aspetto, i dati con varianze molto basse o alte possono essere riscalati.\n\nIn sintesi, prima di procedere a qualunque analisi statistica è necessario affrontare diversi problemi relativi alla corretta preparazione e gestione dei dati. Questi aspetti sono fondamentali per assicurare l’accuratezza e l’affidabilità dei risultati delle analisi SEM.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Preparazione dei Dati</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html",
    "href": "chapters/sem/03_gof.html",
    "title": "41  Test del Modello e Indicizzazione",
    "section": "",
    "text": "41.1 Introduzione\nIn questo capitolo, focalizziamo l’attenzione sulle due principali categorie di statistiche per la valutazione dell’adattamento globale nei modelli SEM: le statistiche di test del modello e gli indici di adattamento approssimativo. Queste due categorie corrispondono, rispettivamente, al test del modello e all’indicizzazione dell’adattamento del modello, come delineato da Hayduk nel 2014.\nUn aspetto critico da considerare è che, nonostante le statistiche di adattamento globale in entrambe le categorie misurino la corrispondenza media o generale tra modello e dati, esse possono non rilevare un cattivo adattamento locale. Questo si riferisce a specifiche coppie di variabili osservate per le quali il modello non spiega adeguatamente le loro associazioni osservate nel campione. È importante riconoscere che i modelli con un cattivo adattamento locale non dovrebbero essere mantenuti, indipendentemente dal loro adattamento globale.\nLa valutazione complessiva del modello SEM segue una sequenza metodica che include la specificazione del modello, la stima dei parametri, l’esame dell’adattamento del modello e dei parametri, e la possibile modifica del modello. Questo processo iterativo prosegue fino all’identificazione di un modello considerato accettabile.\nIn aggiunta, questo capitolo si propone di esplorare due metodi fondamentali per la pianificazione della dimensione del campione in SEM: l’analisi della potenza e l’accuratezza nella stima dei parametri, anche nota come precisione nella pianificazione. Questi approcci sono essenziali per garantire che la ricerca sia adeguatamente dimensionata e che i parametri del modello siano stimati con la massima precisione possibile. La valutazione degli indici di bontà dell’adattamento, largamente utilizzati nella letteratura, sarà un focus chiave in questo contesto, fornendo una panoramica completa degli strumenti a disposizione dei ricercatori per valutare l’efficacia dei loro modelli SEM.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#introduzione",
    "href": "chapters/sem/03_gof.html#introduzione",
    "title": "41  Test del Modello e Indicizzazione",
    "section": "",
    "text": "Statistiche di Test del Modello: Queste statistiche si basano su una decisione binaria, ovvero se respingere o mantenere le ipotesi nulle relative al modello. Tale decisione è determinata dai valori p derivati dai test di significatività. In questo contesto, l’obiettivo è stabilire se il modello, nella sua interezza, si adatta ai dati osservati.\nIndici di Adattamento Approssimativo: A differenza delle statistiche di test del modello, gli indici di adattamento approssimativo si basano su misure continue che valutano quanto il modello corrisponda ai dati. Questo approccio è simile alla stima della dimensione dell’effetto quantitativo piuttosto che al test di significatività dicotomico. Questi indici forniscono una valutazione più sfumata dell’adattamento del modello, andando oltre il semplice rifiuto o accettazione dell’ipotesi nulla.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#valutazione-della-bontà-di-adattamento-nel-modello-sem",
    "href": "chapters/sem/03_gof.html#valutazione-della-bontà-di-adattamento-nel-modello-sem",
    "title": "41  Test del Modello e Indicizzazione",
    "section": "41.2 Valutazione della Bontà di Adattamento nel Modello SEM",
    "text": "41.2 Valutazione della Bontà di Adattamento nel Modello SEM\nNel contesto dei modelli SEM (Structural Equation Modeling), la valutazione dell’adattamento del modello si basa sul confronto tra la matrice di varianze e covarianze stimata dal modello, \\(\\Sigma(\\hat{\\theta})\\), e la matrice di covarianza campionaria, \\(S\\). Il nostro obiettivo è verificare se la discrepanza tra queste due matrici indica possibili inadeguatezze nel modello proposto. Ecco alcuni aspetti rilevanti da considerare:\n\nModelli Saturi vs Modelli Ristretti: Un modello saturo include un numero di parametri in \\(\\theta\\) pari al numero di elementi distinti nella matrice di covarianza. In contrasto, un modello ristretto ha meno parametri rispetto al numero degli elementi distinti nella matrice di covarianza. La differenza tra questi due numeri corrisponde ai gradi di libertà del modello. Per esempio, in un modello saturo, se il numero dei parametri in \\(\\theta\\) e il numero degli elementi distinti nella matrice di covarianza sono entrambi 3, allora il modello ha zero gradi di libertà.\nPerfetto Adattamento dei Modelli Saturi: In un modello saturo, \\(\\Sigma(\\hat{\\theta})\\) coincide sempre con \\(S\\), poiché il modello ha abbastanza parametri per adattarsi perfettamente ai dati del campione. Tuttavia, ciò non implica necessariamente che il modello rappresenti fedelmente la popolazione più ampia. Le stime dei parametri in un modello saturo possono fornire informazioni sui pattern di relazione tra le variabili nel campione specifico, ma è cruciale interpretarle con cautela.\nStima e Identificabilità del Modello: Generalmente, la stima dei parametri non si basa sul semplice risolvere un sistema di equazioni matematiche. Invece, si utilizza una funzione di adattamento o discrepanza tra \\(\\Sigma(\\theta)\\) e \\(S\\), cercando il valore ottimale di \\(\\hat{\\theta}\\) attraverso tecniche di ottimizzazione numerica. Un modello SEM deve essere identificabile, il che significa che deve essere possibile stimare univocamente i parametri del modello. L’identificabilità implica che il numero di unità di informazione, come elementi nella matrice di covarianza, sia maggiore o uguale al numero di parametri da stimare.\n\n\n41.2.1 Gradi di Libertà e Identificabilità del Modello\nI gradi di libertà (dof) in un modello SEM sono calcolati come:\n\\[\ndof = \\# (\\text{unità di informazione}) - \\# (\\text{parametri da stimare})\n\\]\nPer una matrice di covarianza di ordine $ p $, il numero di unità di informazione è $ $. Per garantire l’identificabilità, è necessario soddisfare alcune condizioni:\n\nIn tutti i modelli, l’unità di misura delle variabili latenti deve essere specificata.\nIl numero di unità di informazione deve essere uguale o superiore al numero di parametri da stimare.\nIn modelli ad un fattore, è richiesto un minimo di tre indicatori per una soluzione “appena identificata”.\nIn modelli a più fattori, si raccomanda un minimo di tre indicatori per ogni variabile latente.\n\nUn modello è: - Non identificato se $ dof &lt; 0 $. - Appena identificato o “saturo” se $ dof = 0 $. - Sovra-identificato se $ dof &gt; 0 $.\nÈ importante notare che un’analisi fattoriale con solo due indicatori per un fattore non è possibile, poiché ci sono meno unità di informazione rispetto ai parametri da stimare. Un modello con tre indicatori e un fattore è “appena identificato”, senza gradi di libertà per valutare la bontà dell’adattamento. Per modelli ad un solo fattore comune latente, è quindi necessario disporre di almeno quattro indicatori.\n\n\n41.2.2 Funzione di Discrepanza e Valutazione della Bontà di Adattamento\nLa funzione di discrepanza tra \\(|S|\\) e \\(|\\Sigma(\\theta)|\\) basata sulla massima verosimiglianza (ML) deriva dalla verosimiglianza normale multivariata dei dati ed è formulata nel seguente modo:\n\\[\nFML(S, \\Sigma(\\theta)) = \\log|\\Sigma(\\theta)| - \\log|S| + \\text{traccia}(S\\Sigma(\\theta)^{-1}) - p\n\\]\ndove \\(|S|\\) e \\(|\\Sigma(\\theta)|\\) sono i determinanti di \\(S\\) e \\(\\Sigma(\\theta)\\) rispettivamente, e \\(p\\) è la dimensione di \\(S\\) o \\(\\Sigma(\\theta)\\). In questo contesto, \\(\\log|\\Sigma(\\theta)|\\) rappresenta il logaritmo del determinante della matrice di covarianza stimata dal modello, mentre \\(\\log|S|\\) è il logaritmo del determinante della matrice di covarianza campionaria. La “traccia” (\\(S\\Sigma(\\theta)^{-1}\\)) è la somma degli elementi della diagonale principale del prodotto di \\(S\\) per l’inverso di \\(\\Sigma(\\theta)\\). Questa funzione di discrepanza misura la differenza tra le covarianze osservate e quelle stimate dal modello, prendendo in considerazione sia le dimensioni delle matrici sia la loro forma.\nLa funzione di discrepanza basata sulla massima verosimiglianza (ML), come quella descritta dalla formula precedente, si distribuisce asintoticamente come una variabile casuale seguente una distribuzione chi quadrato (χ²) sotto l’ipotesi nulla che il modello specificato si adatti perfettamente ai dati.\nLa distribuzione chi quadrato è utilizzata per testare l’adattamento del modello nei contesti di modellazione di equazioni strutturali (SEM) e di analisi fattoriale. Il valore calcolato dalla funzione di discrepanza viene confrontato con un valore critico dalla distribuzione chi quadrato. Il numero di gradi di libertà per la distribuzione chi quadrato è tipicamente calcolato come la differenza tra il numero di stime indipendenti nella matrice di covarianza e il numero di parametri stimati nel modello.\nSe il valore calcolato dalla funzione di discrepanza è inferiore al valore critico della distribuzione chi quadrato per i dati gradi di libertà, allora non si rifiuta l’ipotesi nulla e si conclude che il modello ha un buon adattamento. Se invece è maggiore, si rifiuta l’ipotesi nulla, indicando che il modello non si adatta bene ai dati.\nIn pratica, questo test è utilizzato per valutare la bontà di adattamento del modello ai dati osservati e per decidere se il modello specificato è adeguato o se sono necessarie ulteriori modifiche.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#test-chi2",
    "href": "chapters/sem/03_gof.html#test-chi2",
    "title": "41  Test del Modello e Indicizzazione",
    "section": "41.3 Test \\(\\chi^2\\)",
    "text": "41.3 Test \\(\\chi^2\\)\nIl test del chi quadrato (\\(\\chi^2\\)) è utilizzato per determinare quanto bene un modello teorico si adatta ai dati osservati. La formula per calcolare la statistica \\(\\chi^2\\) è:\n\\[\n\\chi^2 = N \\times F_{\\text{min}},\n\\]\ndove: - $ N $ rappresenta la dimensione del campione. - $ F_{} $ è il valore minimo della funzione di discrepanza.\nLa funzione di discrepanza, $ F $, è una misura di quanto le covarianze (o le varianze) osservate nei dati differiscano da quelle previste dal modello. Durante il processo di stima dei parametri del modello, questa funzione viene minimizzata. Il valore di $ F $ al suo minimo, $ F_{} $, rappresenta la discrepanza minima tra i dati osservati e quelli previsti dal modello.\nNell’ambito dell’analisi strutturale di covarianza, il valore di $ F_{} $ è tipicamente ottenuto attraverso la stima di massima verosimiglianza (Maximum Likelihood, ML). Tuttavia, ci sono due modi comuni per calcolare $ ^2 $, che possono variare a seconda del software utilizzato: 1. $ ^2 = (N - 1) F_{} $ 2. $ ^2 = N F_{} $\nLa scelta tra $ N $ e $ N-1 $ dipende da come il software gestisce la normalizzazione e l’adattamento delle strutture di covarianza.\n\n41.3.1 Interpretazione del Test del \\(\\chi^2\\)\n\nIpotesi Nulla $ H_0 $: Il modello si adatta bene ai dati. Ciò significa che non c’è una differenza significativa tra le covarianze osservate e quelle previste dal modello.\nValore p: Un valore p basso (ad esempio, minore di 0,05) suggerisce che dovremmo rifiutare l’ipotesi nulla, indicando che il modello non si adatta bene ai dati.\n\n\n\n41.3.2 Limitazioni\nLa statistica \\(\\chi^2\\) è influenzata dalla dimensione del campione. Con campioni più grandi, anche piccole discrepanze tra il modello e i dati possono risultare in un valore di \\(\\chi^2\\) elevato, portando a rifiutare erroneamente un buon modello. Inoltre, il test del \\(\\chi^2\\) - Non indica la direzione o la natura della discrepanza tra modello e dati. - Non sempre adatto per modelli complessi o in situazioni in cui le ipotesi di base (come la normalità multivariata) non sono soddisfatte.\nPer queste ragioni, il test del \\(\\chi^2\\) è spesso accompagnato da altri indici di adattamento del modello, come l’indice di adattamento comparativo (CFI) o l’indice di adattamento radice quadrata media dell’errore approssimativo (RMSEA), per ottenere una valutazione più completa della bontà di adattamento del modello ai dati.\nIn conclusione, il test del \\(\\chi^2\\) è utile ma ha limitazioni e deve essere interpretato con cautela, considerando la dimensione del campione e altri fattori influenti. Nonostante i suoi limiti, la statistica \\(\\chi^2\\) viene comunque utilizzata per altri scopi, come il confronto di modelli nidificati, il calcolo di altri indici di adattamento (ad es. l’indice di Tucker–Lewis) e il calcolo del rapporto tra \\(\\chi^2\\) e gradi di libertà.\n\n\n41.3.3 Test di rapporto di verosimiglianza\nIl test del \\(\\chi^2\\) può essere impiegato come un test di rapporto di verosimiglianza per confrontare due modelli nidificati. In questo contesto, “nidificati” significa che uno dei modelli (considerato il modello più semplice o ristretto) è un caso speciale dell’altro (il modello più complesso), con meno parametri liberi da stimare. Questo tipo di test è particolarmente utile per valutare se l’aggiunta di parametri supplementari (rendendo il modello più complesso) migliora significativamente l’adattamento del modello ai dati.\nIl processo di confronto tra i due modelli avviene nel seguente modo: 1. Si stima il modello più semplice e si calcola il suo valore di \\(\\chi^2\\). 2. Si stima il modello più complesso e si calcola il suo valore di \\(\\chi^2\\). 3. Si confrontano i due valori di \\(\\chi^2\\) per determinare se l’aggiunta di parametri aggiuntivi giustifica un miglioramento dell’adattamento del modello ai dati, dati i gradi di libertà aggiuntivi.\nSe il valore p associato al \\(\\chi^2\\) del modello più complesso è significativamente più basso rispetto a quello del modello più semplice, questo suggerisce che l’aggiunta dei parametri fornisce un miglioramento significativo nell’adattamento del modello. Al contrario, se non vi è un miglioramento significativo, si può concludere che il modello più semplice è preferibile in termini di parsimonia e adattamento.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#chi-quadrato-normalizzato-nc",
    "href": "chapters/sem/03_gof.html#chi-quadrato-normalizzato-nc",
    "title": "41  Test del Modello e Indicizzazione",
    "section": "41.4 Chi Quadrato Normalizzato (NC)",
    "text": "41.4 Chi Quadrato Normalizzato (NC)\nIl Chi Quadrato Normalizzato (NC) emerge come un tentativo di attenuare l’effetto della dimensione del campione sulla statistica del chi quadrato del modello (\\(\\chi^2\\)). Questa pratica, adottata da alcuni ricercatori, consiste nel dividere \\(\\chi^2\\) per il numero dei gradi di libertà del modello (dfM), risultando nella formula \\(\\frac{\\chi_{ML}}{dfM}\\). Nonostante l’intento di mitigare l’impatto della dimensione del campione (N), l’impiego di NC presenta limitazioni sostanziali:\n\nInfluenza di N sui Modelli Erronei: La statistica \\(\\chi_{ML}\\) è sensibile a N esclusivamente per i modelli non corretti. Questo implica che l’uso di NC per modelli veritieri potrebbe essere fuorviante.\nIndipendenza di dfM da N: I gradi di libertà del modello (dfM) non sono correlati con la dimensione del campione, rendendo la divisione di \\(\\chi_{ML}\\) per dfM arbitraria e priva di fondamento statistico.\nMancanza di Linee Guida: Non esistono criteri consolidati che definiscano i limiti “accettabili” per il valore di NC. Per esempio, non è chiaro se un valore massimo di NC debba essere inferiore a 2.0, 3.0, o altro.\n\nIn conclusione, data la mancanza di una solida giustificazione statistica o logica, {cite:r}kline2023principles sconsiglia l’utilizzo del Chi Quadrato Normalizzato come strumento di valutazione della bontà di adattamento del modello.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#chi-quadrato-scalato-e-errori-standard-robusti-per-distribuzioni-non-normali",
    "href": "chapters/sem/03_gof.html#chi-quadrato-scalato-e-errori-standard-robusti-per-distribuzioni-non-normali",
    "title": "41  Test del Modello e Indicizzazione",
    "section": "41.5 Chi Quadrato Scalato e Errori Standard Robusti per Distribuzioni Non Normali",
    "text": "41.5 Chi Quadrato Scalato e Errori Standard Robusti per Distribuzioni Non Normali\nNell’ambito dell’analisi di massima verosimiglianza (ML), sia l’approccio ML standard che quello robusto forniscono le stesse stime dei parametri. Tuttavia, il ML robusto differisce nell’introduzione di chi quadrati scalati (\\(\\chi^2\\) scalati) e di errori standard robusti, i quali sono adattati per controbilanciare gli effetti della non normalità dei dati.\nUn metodo pionieristico sviluppato da Satorra e Bentler, che si basa sull’utilizzo di dati completi, calcola il chi quadrato scalato (\\(\\chi_{SB}\\)) applicando un fattore di correzione di scala, indicato con $ c \\(, al valore del chi quadrato non scalato del modello (\\)_{ML}$). Questo fattore di scala $ c $ è determinato dalla curtosi multivariata media osservata nei dati grezzi. La formula specifica per il calcolo di \\(\\chi_{SB}\\) è:\n\\[\n\\chi_{SB} = \\frac{\\chi_{ML}}{c}.\n\\]\nQuesta formula evidenzia come il chi quadrato scalato di Satorra-Bentler modifica il chi quadrato tradizionale per tenere conto della curtosi nei dati, fornendo così una misura più affidabile della bontà di adattamento del modello in presenza di distribuzioni non normali.\nQuando si utilizza il Chi Quadrato Scalato di Satorra-Bentler (\\(\\chi_{SB}\\)), è importante comprendere come esso si comporti in campioni casuali. Le distribuzioni di \\(\\chi_{SB}\\) tendono ad avvicinarsi alle distribuzioni chi quadrato centrali, ma con una caratteristica fondamentale: le loro medie sono asintoticamente corrette. Questo significa che, su larga scala, \\(\\chi_{SB}\\) fornisce una stima media accurata della discrepanza tra i dati osservati e quelli previsti dal modello, correggendo per eventuali distorsioni causate dalla non normalità dei dati.\nUn altro tipo di chi quadrato, sviluppato da Asparouhov e Muthén, non si basa sul \\(\\chi_{ML}\\) standard. Invece, nei campioni di grandi dimensioni, il loro chi quadrato scalato corrisponde alla statistica T2* di Yuan e Bentler. Questa versione del chi quadrato è particolarmente adatta per gestire dati non normali o con valori mancanti. I gradi di libertà, sia per \\(\\chi_{SB}\\) che per T2*, sono rappresentati da dfM, indicando la flessibilità del modello in termini di numero di parametri stimabili.\nAl di là di questi, esistono chi quadrati che sono corretti sia per la media che per la varianza. Questi chi quadrati utilizzano fattori di scala diversi e, in genere, seguono distribuzioni chi quadrato centrali con medie e varianze che sono corrette in modo asintotico. Sebbene questi metodi richiedano maggiori risorse computazionali rispetto ai metodi che correggono solo per la media, tendono ad essere più precisi, specialmente in campioni di grandi dimensioni. Questa precisione aggiuntiva è particolarmente utile quando si affrontano set di dati complessi o di ampie dimensioni, permettendo una stima più accurata della bontà di adattamento del modello.\nPer quanto riguarda il software lavaan, ci sono diverse opzioni per il metodo ML robusto:\n\n“MLM” per dati completi, genera un chi quadrato scalato per la media di Satorra-Bentler.\n“MLR” per dati completi o incompleti, produce un chi quadrato corretto per la media basato sulla statistica T2* di Yuan-Bentler.\n“MLMV” per dati completi, calcola un chi quadrato scalato corretto per media e varianza.\n“MLMVS” genera un chi quadrato corretto per media e varianza con una correzione per l’eteroschedasticità di Satterthwaite.\n\nNel contesto dei metodi ML robusti utilizzati nel software lavaan, la “matrice di informazione” è un concetto fondamentale che incide sul calcolo degli errori standard. La matrice di informazione, in statistica, è una matrice che contiene informazioni sulla varianza e covarianza dei parametri stimati di un modello. Viene utilizzata per calcolare gli errori standard delle stime dei parametri, che sono essenziali per testare l’ipotesi statistica e per la costruzione di intervalli di confidenza.\nNel software lavaan, la matrice di informazione può essere di due tipi:\n\nMatrice di Informazione Attesa: Questa è la matrice predefinita utilizzata da lavaan per calcolare gli errori standard nei metodi ML robusti. La matrice di informazione attesa si basa sulle aspettative teoriche della varianza e covarianza dei parametri stimati, derivanti dal modello e dall’insieme dei dati.\nMatrice di Informazione Osservata: Quando si hanno dati mancanti, lavaan passa all’utilizzo della matrice di informazione osservata. Questa matrice utilizza i dati effettivamente osservati per calcolare la varianza e la covarianza dei parametri stimati. Essa può fornire stime più accurate degli errori standard in presenza di dati mancanti.\n\nGli utenti di lavaan hanno la possibilità di specificare se utilizzare la matrice di informazione attesa anche in presenza di dati incompleti, in base alle esigenze specifiche della loro analisi.\nÈ importante per i ricercatori utilizzare questi strumenti in modo etico e metodologicamente corretto. Evitare di selezionare in modo opportunistico le combinazioni di chi quadrati scalati e errori standard robusti che sembrano meglio supportare le ipotesi, e dichiarare chiaramente qualsiasi variabilità nei risultati dovuta alla scelta del metodo di calcolo, è cruciale per mantenere l’integrità e l’affidabilità della ricerca.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#indicizzazione-delladattamento-del-modello",
    "href": "chapters/sem/03_gof.html#indicizzazione-delladattamento-del-modello",
    "title": "41  Test del Modello e Indicizzazione",
    "section": "41.6 Indicizzazione dell’Adattamento del Modello",
    "text": "41.6 Indicizzazione dell’Adattamento del Modello\nL’indicizzazione dell’adattamento del modello si basa sull’uso di indici di adattamento approssimati, i quali si differenziano dai test di significatività tradizionali. Invece di fornire una decisione dicotomica, come il rifiuto o l’accettazione di un’ipotesi nulla, questi indici offrono una misura continua di quanto bene un modello si adatta ai dati osservati. Non essendoci una separazione netta tra i limiti dell’errore di campionamento, gli indici di adattamento forniscono una valutazione più sfumata e graduale della bontà di adattamento.\nQuesti indici possono essere classificati in due categorie principali: 1. Statistiche di Cattivo Adattamento: In questa categoria, valori più elevati indicano un peggior adattamento del modello ai dati. Un esempio tipico di questa categoria è il chi quadrato del modello, dove valori più alti suggeriscono una maggiore discrepanza tra il modello e i dati.\n\nStatistiche di Buon Adattamento: Al contrario, per gli indici in questa categoria, valori più alti segnalano un migliore adattamento del modello ai dati. Molti di questi indici sono normalizzati in modo che il loro intervallo varii da 0 a 1.0, dove 1.0 rappresenta l’adattamento ottimale del modello.\n\nA differenza del test del chi quadrato, che si basa su un framework teorico ben definito, l’interpretazione e l’applicazione degli indici di adattamento approssimati non sono guidate da un unico insieme di principi teorici consolidati. Questa situazione fa sì che la valutazione dell’adattamento del modello si allinei maggiormente a ciò che Little (2013) ha descritto come “scuola di modellazione”. Questo approccio contempla l’analisi di modelli statistici complessi in un contesto in cui le regole decisionali sono meno rigide e più soggette a interpretazione.\nLa natura flessibile di questo approccio rispecchia la varietà e la complessità dei modelli statistici, che devono essere personalizzati per rispondere a specifiche domande di ricerca. Questa flessibilità, tuttavia, porta con sé una certa ambiguità nelle regole di valutazione dei modelli statistici. Pur offrendo la possibilità di adattare l’analisi alle particolarità di ogni studio, questa mancanza di rigore teorico uniforme può talvolta non tradursi in pratiche ottimali di modellazione.\nLa questione filosofica relativa all’adattamento esatto dei modelli statistici solleva dubbi sull’idea di perfezione come standard per questi modelli. In effetti, è ampiamente riconosciuto che tutti i modelli statistici sono in qualche misura imperfetti; sono piuttosto strumenti di approssimazione che aiutano i ricercatori a organizzare e interpretare le loro osservazioni sui fenomeni di interesse. Un modello troppo semplificato, che non cattura la complessità del fenomeno, può essere inadeguato e quindi rifiutato. Allo stesso tempo, un modello eccessivamente complesso, che cerca di replicare fedelmente il fenomeno, può risultare di scarsa utilità scientifica a causa della sua complessità eccessiva.\nGeorge Box, nel suo influente lavoro del 1976, avanzò l’idea che nessun modello statistico potesse essere considerato perfettamente “corretto”. Questa visione nasce dalla consapevolezza che tutti i modelli hanno una certa dose di imperfezione intrinseca. Box suggeriva che lo scopo principale di uno scienziato dovrebbe essere la ricerca di una “descrizione economica” dei fenomeni naturali, cercando cioè di formulare modelli che siano semplici, ma al contempo efficaci, nella rappresentazione della realtà. Egli criticava la tendenza a sovraelaborare o sovraparametrizzare i modelli, considerandola un segno di mediocrità nella pratica scientifica. Box enfatizzava l’importanza di concentrarsi sugli errori significativi, o “tigri”, piuttosto che su piccole imperfezioni, o “topi”, affermando:\n\n“Since all models are wrong the scientist must be alert to what is importantly wrong. It is inappropriate to be concerned about mice when there are tigers abroad.”\n\nCiò implica che l’obiettivo nella modellazione statistica non dovrebbe essere una perfezione irraggiungibile, ma piuttosto lo sviluppo di modelli che, pur nella loro semplicità, riescano a cogliere gli aspetti fondamentali dei fenomeni analizzati. Questo richiede un equilibrio tra la complessità necessaria per una descrizione accurata e la semplicità che rende un modello pratico e interpretabile.\nHayduk (2014), nel commentare l’affermazione di Box, si focalizza specificatamente sul contesto della modellizzazione SEM (Structural Equation Modeling). Egli identifica le “tigri”, ovvero gli errori gravi nei modelli, come indicatori di una specificazione errata del modello. Hayduk sottolinea l’importanza critica di riconoscere e correggere gli errori significativi piuttosto che disperdere energie su dettagli minori. In sostanza, Hayduk rafforza l’idea che è essenziale distinguere tra errori minori e maggiori, questi ultimi potendo compromettere seriamente la validità e l’utilità di un modello statistico.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#tipologie-di-indici-di-adattamento-approssimati",
    "href": "chapters/sem/03_gof.html#tipologie-di-indici-di-adattamento-approssimati",
    "title": "41  Test del Modello e Indicizzazione",
    "section": "41.7 Tipologie di Indici di Adattamento Approssimati",
    "text": "41.7 Tipologie di Indici di Adattamento Approssimati\nGli indici di adattamento approssimati possono essere classificati in diverse categorie, che riflettono diversi aspetti della bontà di adattamento di un modello statistico ai dati. Sebbene questa classificazione non sia esaustiva né le categorie siano mutualmente esclusive, i tipi principali di indici di adattamento sono i seguenti:\n\nIndici di Adattamento Assoluto: Questi indici, come il GFI (Goodness of Fit Index), misurano quanto bene un modello spiega i dati senza riferimento ad altri modelli. Indicano l’abilità del modello di riprodurre i dati osservati.\nIndici di Adattamento Parsimonioso: Questi indici confrontano i gradi di libertà del modello (dfM) con il numero massimo possibile di gradi di libertà disponibili nei dati. Un esempio è l’AGFI (Adjusted Goodness of Fit Index), che incorpora una penalità per la complessità del modello, benché non sia un indice di adattamento parsimonioso come definito in questa categoria.\nIndici di Adattamento Incrementale (Relativo o Comparativo): Questi indici confrontano l’adattamento del modello del ricercatore con quello di un modello di base, tipicamente un modello di indipendenza che assume covarianze nulle tra le variabili osservate. È possibile scegliere un modello di base diverso, sebbene il calcolo manuale dell’indice possa essere necessario se il modello di base desiderato differisce da quello predefinito nel software.\nIndici di Adattamento Non Centrale: Stimano il grado in cui l’ipotesi di adattamento esatto è falsa, dati il modello e i dati. Questi indici approssimano parametri nelle distribuzioni chi quadrato non centrali, che descrivono anche le distribuzioni campionarie per gli indici di adattamento di questo tipo.\nIndici di Adattamento Predittivo (o basati sulla Teoria dell’Informazione): Derivati dalla teoria dell’informazione, stimano l’adattamento del modello in campioni di replica ipotetici della stessa dimensione, estratti casualmente dalla stessa popolazione del campione originale. Sono utilizzati principalmente per confrontare modelli alternativi basati sulle stesse variabili e adattati agli stessi dati, ma dove i modelli non sono gerarchicamente correlati.\n\nNon tutti gli indici di adattamento approssimati hanno resistito alla prova del tempo. Ad esempio, gli indici di adattamento parsimonioso non hanno mai raggiunto una popolarità significativa tra i ricercatori applicati, restando relativamente oscuri. Altri indici, come il GFI e l’AGFI, sono stati criticati per la loro sensibilità alla dimensione del campione e al numero di indicatori nei modelli di analisi fattoriale.\nI software moderni per la Structural Equation Modeling (SEM) presentano una notevole varietà nel numero di indici di adattamento approssimati forniti nei loro output. Programmi come Amos e LISREL elencano un numero elevato di indici (oltre 12), mentre altri come lavaan e Mplus ne includono un numero più limitato (circa 4-5). Questa abbondanza di indici può portare al rischio di “cherry-picking”, cioè la tendenza a selezionare e riportare solo quegli indici che mostrano risultati favorevoli al modello proposto dal ricercatore. Per mitigare questo rischio, è consigliabile limitarsi a un insieme essenziale di indici e prestare attenzione all’analisi dei residui.\n\n41.7.1 Set di Indici di Adattamento Consigliati\nKline (2023) suggerisce un insieme essenziale di soli tre indici di adattamento approssimati, che sono ampiamente utilizzati nei software SEM e frequentemente presenti negli studi pubblicati. Questi indici sono stati selezionati per le seguenti ragioni:\n\nAmpia Presenza nella Letteratura: Sono ampiamente riportati in numerosi studi SEM, rendendoli familiari sia ai ricercatori che ai revisori.\nStandardizzazione: Le scale di questi indici non dipendono dalle variabili osservate o latenti, fornendo così una misura standardizzata di adattamento.\nValidità Statistica Estesa: Almeno uno di questi indici, l’RMSEA, possiede un solido fondamento statistico e un quadro interpretativo più ampio per la stima degli intervalli, i test delle ipotesi e la pianificazione della dimensione del campione.\n\nNonostante la loro utilità, è fondamentale usare questi indici con attenzione. I ricercatori dovrebbero evitare l’uso acritico di soglie o punti di taglio, sia fissi sia variabili, che si suppone differenzino tra modelli con un buon o cattivo adattamento. L’applicazione di queste soglie può essere problematica, poiché non sono valide universalmente per tutti i tipi di modelli e set di dati. L’uso improprio di tali soglie può portare a decisioni errate, in particolare se si trascura l’analisi dei residui.\nIl gruppo principale di tre indici di adattamento approssimati raccomandato comprende:\n\nRoot Mean Square Error of Approximation (RMSEA) di Steiger-Lind (Steiger, 1990), accompagnato dal suo intervallo di confidenza al 90%. L’RMSEA valuta l’adattamento assoluto del modello, penalizzando la complessità del modello, ma non è un indice di adattamento parsimonioso. È un indice di cattivo adattamento dove il valore zero rappresenta l’adattamento ideale, senza un limite massimo teorico.\nComparative Fit Index (CFI) di Bentler (Bentler, 1990). Il CFI è un indice di adattamento incrementale e valuta la bontà di adattamento relativa del modello rispetto a un modello di base. Si estende su una scala da 0 a 1.0, dove 1.0 indica l’adattamento ottimale, e non impone penalità per la complessità del modello.\nStandardized Root Mean Square Residual (SRMR) (Jöreskog & Sörbom; 1981). L’SRMR è un indice di adattamento assoluto che misura la discrepanza tra le correlazioni osservate e quelle previste dal modello. Un valore di zero indica un adattamento perfetto.\n\nSia l’RMSEA sia il CFI incorporano il chi quadrato del modello e i suoi gradi di libertà nelle loro formule. Questo implica che condividono le stesse assunzioni distributive della corrispondente statistica di test. Se tali assunzioni non sono valide, i valori degli indici e della statistica di test (incluso il valore p) potrebbero non essere accurati. Entrambi gli indici sono stati inizialmente definiti per dati continui con distribuzioni normali analizzati tramite ML standard. Tuttavia, in presenza di dati significativamente non normali, i valori di chiML, RMSEA e CFI possono risultare distorti. Alcuni software SEM implementano correzioni ad hoc per la non normalità.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#misure-di-adeguamento-per-il-confronto",
    "href": "chapters/sem/03_gof.html#misure-di-adeguamento-per-il-confronto",
    "title": "41  Test del Modello e Indicizzazione",
    "section": "41.8 Misure di adeguamento per il confronto",
    "text": "41.8 Misure di adeguamento per il confronto\n\n41.8.1 CFI\nGli indici di adattamento comparativo [detti anche indici di adattamento incrementale; ad es. Hu e Bentler (1998)] valutano l’adattamento di una soluzione specificata dall’utente in relazione a un modello di base nidificato più ristretto. Tipicamente, il modello base è un modello “nullo” o “di indipendenza” in cui le covarianze tra tutti gli indicatori di input sono fissate a zero, ma nessun vincolo viene posto sulle varianze degli indicatori.\nUno di questi indici, l’indice di adattamento comparativo (comparative fit index, CFI; Bentler, 1990), è calcolato come segue. Sia \\(\\delta = \\chi^2 - dof\\), dove \\(dof\\) sono i gradi di libertà di un particolare modello. Tanto più \\(\\delta\\) è prossimo allo zero tanto maggiore è la bontà dell’adattamento. La formula di CFI è\n\\[\n\\begin{equation}\nCFI = \\frac{\\delta_B - \\delta_T}{\\delta_B},\n\\end{equation}\n\\]\ndove il pedice \\(T\\) denota il modello target (cioè il modello in valutazione) e il pedice \\(B\\) denota il modello baseline (cioè il modello “nullo”).\nHu e Bentler (1995) suggerirono che un valore del Comparative Fit Index (CFI) maggiore o uguale a 0.95 rappresenta un risultato favorevole. Questa soglia è in linea con i risultati di alcuni studi successivi, come quello di Hu e Bentler del 1999, che utilizzarono metodi di simulazione di Monte Carlo. Tuttavia, studi di simulazione più recenti, come quelli di Fan e Sivo nel 2005 e di Yuan nel 2005, hanno messo in dubbio l’universalità di un valore soglia specifico per il CFI, evidenziando che l’adeguatezza di tale valore può variare a seconda delle caratteristiche dei modelli e del grado di non normalità nei dati.\nInoltre, Brosseau-Liard e Savalei (2014) hanno descritto delle versioni robuste del CFI adatte per dati non normali. Queste versioni del CFI sono calcolate e fornite dal software lavaan quando si utilizzano metodi di stima Maximum Likelihood (ML) robusti. Questo implica che, quando si lavora con dati che presentano deviazioni dalla normalità, queste versioni robuste del CFI possono offrire una misura più affidabile dell’adattamento del modello.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#misure-di-adeguamento-parsimonioso",
    "href": "chapters/sem/03_gof.html#misure-di-adeguamento-parsimonioso",
    "title": "41  Test del Modello e Indicizzazione",
    "section": "41.9 Misure di adeguamento parsimonioso",
    "text": "41.9 Misure di adeguamento parsimonioso\n\n41.9.1 TLI\nUn indice che rientra nella degli indici di adeguamento parsimonioso è l’indice Tucker-Lewis (Tucker–Lewis index, TLI, anche chiamato indice di adattamento non normato). Il TLI si pone il problema di penalizzare la complessità del modello, ovvero include una funzione di penalizzazione per l’addizione di parametri che non migliorano in maniera sostanziale l’adattamento del modello. Il TLI è calcolato con la seguente formula:\n\\[\n\\begin{equation}\nTLI = \\frac{(\\chi^2_B / dof_B)–(\\chi^2_T / dof_T)}{(\\chi^2_B / dof_B) – 1},\n\\end{equation}\n\\]\ndove \\(\\chi^2_T\\) è il valore \\(\\chi^2\\) del modello target, \\(dof_T\\) sono i gradi di libertà del modello target, \\(\\chi^2_B\\) è il valore \\(\\chi^2\\) del modello baseline e \\(dof_B\\) sono i gradi di libertà del modello base.\nL’Indice di Tucker-Lewis (TLI) può, in teoria, assumere valori inferiori a zero se il modello di base, ovvero un modello diverso da quello studiato dal ricercatore, mostra un ottimo adattamento ai dati. Tuttavia, questa eventualità è rara nella pratica. Al contrario, il TLI può superare il valore di 1.0 se il modello analizzato dal ricercatore si adatta in modo particolarmente stretto ai dati. Marsh e Balla (1994) hanno evidenziato che la dimensione del campione influenza poco i valori del TLI.\nSecondo quanto osservato da Kenny (2020), si possono trarre due conclusioni importanti:\n\nIl Comparative Fit Index (CFI) e il TLI sono entrambi influenzati dall’entità delle correlazioni tra le variabili misurate. Ciò significa che valori medi di correlazione più elevati risultano in valori più alti sia per il CFI che per il TLI, e il contrario è vero per correlazioni medie più basse.\nI valori del CFI e del TLI mostrano una forte correlazione tra loro. Di conseguenza, è consigliabile riportare solo uno dei due indici per evitare ripetizioni e per mantenere la chiarezza del report. La scelta tra CFI e TLI dovrebbe basarsi su criteri specifici relativi al contesto e agli obiettivi dello studio in questione.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#misure-di-adeguamento-assoluto",
    "href": "chapters/sem/03_gof.html#misure-di-adeguamento-assoluto",
    "title": "41  Test del Modello e Indicizzazione",
    "section": "41.10 Misure di adeguamento assoluto",
    "text": "41.10 Misure di adeguamento assoluto\n\n41.10.1 Root Mean Square Error of Approximation (RMSEA)\nL’Errore Quadratico Medio di Approssimazione (RMSEA) è un indicatore che misura quanto bene un modello statistico si adatta ai dati. A differenza di altri indici come il CFI o il TLI, che confrontano il modello con un modello di base, l’RMSEA valuta l’adattamento del modello in maniera assoluta.\nIl calcolo dell’RMSEA si basa sull’utilizzo del chi quadrato (\\(\\chi^2\\)), che quantifica la discrepanza tra le covarianze osservate nei dati e quelle stimate dal modello. Questa discrepanza, che indichiamo con \\(\\delta\\), è la differenza tra il valore del chi quadrato e i gradi di libertà (df) del modello: \\(\\delta = \\chi^2 - df\\). Un valore di \\(\\delta\\) più alto indica una maggiore discrepanza e quindi un peggior adattamento del modello ai dati.\nLa formula dell’RMSEA è la seguente:\n\\[\nRMSEA = \\sqrt{\\frac{\\max(0, \\delta)}{df \\cdot (n-1)}},\n\\]\ndove \\(n\\) è il numero di osservazioni nel campione. L’RMSEA stima l’errore nell’approssimare la matrice di correlazione (o covarianza) osservata con quella derivante dal modello. Fornisce un’indicazione sulla qualità dell’adattamento del modello alla popolazione, considerando i gradi di libertà e la parsimonia del modello.\nAssumendo che i dati seguano una distribuzione normale multivariata, che il modello sia corretto, e che il campione sia ampio e casuale, il chi quadrato del modello (\\(\\chi^2_{ML}\\)) segue una distribuzione chi quadrato con gradi di libertà \\(dfM\\). Se il modello non è corretto, il chi quadrato segue una distribuzione non centrale \\(\\chi^2(dfM, \\delta)\\), dove \\(\\delta\\) rappresenta il grado di discrepanza tra modello e dati. Se \\(\\delta = 0\\), il modello è perfetto; se \\(\\delta &gt; 0\\), il modello non è adeguato.\nIl parametro di non centralità normalizzato (\\(\\delta_{nor}\\)), meno sensibile alla dimensione del campione, si calcola così:\n\\[\n\\delta_{nor} = \\max(0, \\chi^2_{ML} - dfM).\n\\]\nQuesto parametro fa parte di una funzione che stima gli errori di approssimazione, ossia la discrepanza tra le matrici di covarianza del campione e della popolazione quando il modello è applicato alla matrice di popolazione.\nIl valore finale dell’RMSEA, indicato come \\(eˆ\\) (epsilon minuscolo), si ottiene dalla formula:\n\\[\neˆ = \\sqrt{\\frac{\\delta_{nor}}{dfM (N - 1)}}\n\\]\nSebbene la stima di \\(eˆ\\) non sia imparziale a causa della restrizione \\(eˆ ≥ 0\\), è considerata una buona approssimazione. Browne e Cudeck (1993) suggerirono che un valore di \\(eˆ ≤ 0.05\\) indichi un buon adattamento del modello, ma ricerche successive hanno evidenziato che non esiste una soglia universale precisa. È quindi raccomandato valutare anche il limite superiore dell’intervallo di confidenza di \\(eˆ\\) (indicato come \\(eˆU\\)) per una valutazione più accurata dell’adattamento del modello.\nAltre caratteristiche del RMSEA sono riassunte di seguito:\n\nL’interpretazione di \\(eˆ\\), \\(eˆL\\) e \\(eˆU\\) in relazione a soglie fisse è generalmente appropriata in grandi campioni con modelli ben specificati; in modelli più piccoli o con errori di specificazione significativi, questa interpretazione può richiedere maggiore cautela.\nStudi di simulazione hanno rilevato che l’RMSEA tende a imporre una penalità più severa per la complessità su modelli più piccoli con poche variabili. Ciò è dovuto al fatto che i modelli più piccoli tendono ad avere meno gradi di libertà, mentre i modelli più grandi hanno più “spazio” per valori di dfM più alti.\nStudi di simulazione hanno valutato le prestazioni di forme robuste dell’RMSEA corrette per la non normalità, una delle quali è basata sul chi quadrato scalato di Satorra-Bentler. Questa versione robusta corretta per la popolazione dell’RMSEA generalmente supera la versione non corretta, che tende ad essere sovrastimata in condizioni di non normalità.\nSono stati sviluppati metodi per stimare la potenza statistica delle varie ipotesi nulle basate sull’RMSEA, così come metodi per generare le dimensioni minime del campione richieste per ottenere livelli target di potenza statistica.\n\n\n\n41.10.2 Root Mean Square Residual (RMRS)\nA differenza del chi quadrato del modello e dei gradi di libertà, che valutano la bontà di adattamento di un modello in base a criteri di adattamento globale, l’indice RMRS (Root Mean Square Residual) si concentra esclusivamente sui residui del modello, ovvero le discrepanze tra le correlazioni osservate e quelle previste dal modello.\nLa formula per calcolare l’RMRS è la seguente:\n\\[ RMRS = \\sqrt{ \\frac{2 \\sum_i\\sum_j(r_{ij} - \\hat{r}_{ij})^2}{p(p+1)}}, \\]\ndove: - $ p $ rappresenta il numero di item (variabili) nel modello, - $ r_{ij} $ è la correlazione osservata tra le variabili $ i $ e $ j $, - $ _{ij} $ è la correlazione prevista dal modello tra le variabili $ i $ e $ j $.\nUn valore di RMRS pari a 0 indica un adattamento perfetto del modello, mentre valori crescenti indicano un adattamento meno preciso. In generale, un valore di SRMR inferiore a 0.08 è considerato favorevole (Hu e Bentler, 1999).\nTuttavia, è importante notare che il SRMR è una misura media e può nascondere variazioni significative tra i residui di correlazione individuali. Ad esempio, se il SRMR è 0.03, potrebbe sembrare un buon adattamento. Ma se i residui di correlazione variano da -0.12 a 0.18, con alcuni residui superiori a 0.10, potrebbe indicare problemi di adattamento locali più gravi.\nPertanto, quando si riportano i risultati in un report, per ottenere una comprensione più completa dell’adattamento del modello è consigliabile descrivere i residui di correlazione o, meglio ancora, presentare l’intera matrice dei residui, anziché basarsi esclusivamente su un valore medio come il SRMR.\n\n\n41.10.3 Interpretazione con lavaan\nL’interpretazione degli indici di bontà di adattamento trovati nella CFA o nella modellazione di equazioni strutturali può essere ottenuta usando le funzioni del pacchetto effectsize.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#adattamento-locale",
    "href": "chapters/sem/03_gof.html#adattamento-locale",
    "title": "41  Test del Modello e Indicizzazione",
    "section": "41.11 Adattamento Locale",
    "text": "41.11 Adattamento Locale\nI modelli SEM possono teoricamente superare i test di adattamento globale ma fallire nei test di adattamento locale. Questi dettagli, relativi all’adattamento del modello, sono esaminati direttamente nei test di adattamento locale. L’analisi dei residui (sia standardizzati che di correlazione) è quindi cruciale per una valutazione completa del modello (Maydeu-Olivares e Shi, 2017). Le recenti norme di reportistica per il SEM richiedono agli autori di descrivere sia l’adattamento globale che quello locale (Appelbaum et al., 2018); Greiff e Heene, 2017; Vernon e Eysenck, 2007).\n\n41.11.1 Residui di Covarianza, Residui Standardizzati, Residui Normalizzati\n\nResidui di Covarianza: Sono le differenze tra le covarianze osservate e quelle previste dal modello. Questi residui possono essere difficili da interpretare perché non sono standardizzati, ovvero la loro metrica dipende dalle scale delle variabili coinvolte. Pertanto, residui di covarianza per coppie di variabili diverse non sono direttamente confrontabili a meno che tutte le variabili non siano sulla stessa metrica.\nResidui Standardizzati: Sono versioni standardizzate dei residui di covarianza, interpretati come un test z in campioni grandi. Un residuo standardizzato significativamente diverso da zero indica una discrepanza tra modello e dati. Tuttavia, la significatività di questi residui può dipendere dalla dimensione del campione, con residui vicini allo zero che possono essere significativi in campioni grandi, mentre residui relativamente grandi potrebbero non essere significativi in campioni piccoli.\nResidui Normalizzati: Sono i rapporti tra i residui di covarianza e l’errore standard della covarianza campionaria. Sono generalmente più conservativi dei residui standardizzati in termini di test di significatività. In modelli complessi, quando non è possibile calcolare il denominatore di un residuo standardizzato, il residuo normalizzato fornisce un’alternativa più conservativa.\n\nNel software lavaan ci sono due opzioni principali per calcolare i residui di correlazione:\n\nOpzione cor.bollen: Questa specifica indica al computer di convertire separatamente le matrici di covarianza del campione e quelle implicite dal modello in matrici di correlazione prima di calcolare i residui. Questo processo comporta la standardizzazione di ciascuna matrice in base alle varianze (deviazioni standard quadrate) presenti nella diagonale principale di ciascuna matrice. Le varianze nella matrice di covarianza del campione sono osservate direttamente, mentre le varianze per le variabili endogene nella matrice di covarianza implicata dal modello sono previste dal modello e possono differire dalle varianze osservate corrispondenti.\nOpzione cor.bentler: Questa opzione standardizza sia la matrice di covarianza del campione che quella implicata dal modello basandosi sulle varianze presenti solo nella matrice di covarianza del campione. Poiché non tutti gli elementi della diagonale principale nella matrice di covarianza implicata dal modello sono varianze osservate, alcuni valori dei residui di correlazione del tipo Bentler potrebbero non essere pari a zero. Tuttavia, i valori dei residui fuori diagonale per entrambi i metodi sono generalmente simili.\n\nPer impostazione predefinita, lavaan utilizza il metodo cor.bollen per calcolare i residui di correlazione nelle sue analisi.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#esempio-1",
    "href": "chapters/sem/03_gof.html#esempio-1",
    "title": "41  Test del Modello e Indicizzazione",
    "section": "41.12 Esempio 1",
    "text": "41.12 Esempio 1\nNel capitolo precedente abbiamo formulato un modello SEM nel quale abbiamo definito una variabile latente con le sei sottoscale della Self-Compassion Scale e una seconda variabile latente con le tre sottoscale della DASS-21. Abbiamo ipotizzato che il fattore dell’autocompassione eserciti un effetto (protettivo) nei confronti del disagio psicologico misurato dal fattore definito dalle sottoscale della DASS-21.\n\nd_sc &lt;- read.csv(\"../../data/dass_rosenberg_scs.csv\", header = TRUE)\n\n\nmod_sc &lt;- \"\n  F =~ anxiety + depression + stress\n  SC =~ self_kindness   + common_humanity   + mindfulness   + self_judgment + isolation + over_identification\n  F ~ SC\n\"\n\n\nfit_sc &lt;- lavaan::sem(mod_sc, d_sc)\n\n\nsemPlot::semPaths(fit_sc,\n    what = \"col\", whatLabels = \"std\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\n\nI coefficienti stimati nel modello SEM appaiono coerenti e in linea con le aspettative, in particolare il coefficiente che descrive l’effetto “causale” del fattore dell’autocompassione sul malessere psicologico, che si attesta a -0.48. Questo valore negativo corrobora l’ipotesi secondo cui l’autocompassione svolge un ruolo di fattore protettivo contro il malessere psicologico. Tuttavia, prima di confermare definitivamente questa conclusione, è cruciale esaminare gli indici di bontà di adattamento del modello. Questi indici ci permetteranno di valutare quanto accuratamente il modello SEM si adatta ai dati osservati, fornendo un quadro più chiaro della validità delle nostre inferenze. In altre parole, sebbene il modello suggerisca una relazione negativa tra autocompassione e malessere psicologico, la conferma finale di questa associazione dipenderà dall’adeguatezza complessiva del modello rispetto ai dati.\n\nfitMeasures(fit_sc) |&gt;\n    print()\n\n                 npar                  fmin                 chisq \n               19.000                 0.427               449.141 \n                   df                pvalue        baseline.chisq \n               26.000                 0.000              3129.133 \n          baseline.df       baseline.pvalue                   cfi \n               36.000                 0.000                 0.863 \n                  tli                  nnfi                   rfi \n                0.811                 0.811                 0.801 \n                  nfi                  pnfi                   ifi \n                0.856                 0.619                 0.864 \n                  rni                  logl     unrestricted.logl \n                0.863            -12308.490            -12083.920 \n                  aic                   bic                ntotal \n            24654.980             24736.021               526.000 \n                 bic2                 rmsea        rmsea.ci.lower \n            24675.710                 0.176                 0.162 \n       rmsea.ci.upper        rmsea.ci.level          rmsea.pvalue \n                0.190                 0.900                 0.000 \n       rmsea.close.h0 rmsea.notclose.pvalue     rmsea.notclose.h0 \n                0.050                 1.000                 0.080 \n                  rmr            rmr_nomean                  srmr \n                1.203                 1.203                 0.071 \n         srmr_bentler   srmr_bentler_nomean                  crmr \n                0.071                 0.071                 0.079 \n          crmr_nomean            srmr_mplus     srmr_mplus_nomean \n                0.079                 0.071                 0.071 \n                cn_05                 cn_01                   gfi \n               46.539                54.452                 0.846 \n                 agfi                  pgfi                   mfi \n                0.733                 0.489                 0.669 \n                 ecvi \n                0.926 \n\n\nL’analisi degli indici di bontà di adattamento rivela alcune preoccupazioni significative riguardo alla validità del nostro modello SEM. Il rapporto \\(\\chi^2 / df\\) emerge come eccessivamente elevato, segnalando una possibile mancanza di adattamento:\n\n449.141 / 26\n\n17.2746538461538\n\n\nAnalogamente, i valori di CFI e TLI sono inferiori al livello desiderato, suggerendo che il modello non rappresenta adeguatamente la struttura dei dati. In aggiunta, gli indici RMSEA e SRMR superano le soglie accettabili, indicando ulteriormente un’inadeguata aderenza del modello ai dati.\nDi fronte a questi risultati, è imprudente accettare la conclusione precedentemente formulata secondo cui l’autocompassione agisce come un fattore protettivo contro il malessere psicologico. Questa interpretazione, benché teoricamente fondata, non trova un solido supporto empirico nel contesto del modello attuale.\nIn questa situazione, un percorso costruttivo potrebbe essere quello di rivedere e potenzialmente modificare il modello. L’obiettivo sarebbe quello di esplorare alternative che potrebbero risultare in un migliore adattamento ai dati, mantenendo al contempo l’adeguatezza teorica. Ciò potrebbe includere la revisione delle assunzioni del modello, la riconsiderazione delle variabili incluse o la ristrutturazione delle relazioni ipotizzate tra di esse. Solo attraverso un modello che dimostra una bontà di adattamento adeguata possiamo affermare con maggiore sicurezza che i dati empirici sostengono l’ipotesi dell’effetto protettivo dell’autocompassione sul malessere psicologico.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#potere-statistico-e-precisione",
    "href": "chapters/sem/03_gof.html#potere-statistico-e-precisione",
    "title": "41  Test del Modello e Indicizzazione",
    "section": "41.13 Potere Statistico e Precisione",
    "text": "41.13 Potere Statistico e Precisione\nNell’ambito dei modelli di Structural Equation Modeling (SEM), l’analisi della potenza statistica è fondamentale per garantire l’affidabilità e la validità dei risultati. Esistono due approcci principali per quest’analisi: la potenza a priori (prospettica) e la potenza retrospettiva (post hoc, osservata).\n\nPotenza a priori (Prospettica): Questa analisi viene effettuata prima della raccolta dei dati e mira a stimare la probabilità che uno studio identifichi un effetto significativo, se presente nella popolazione. È cruciale nella pianificazione della ricerca per determinare la dimensione del campione necessaria, aumentando così l’efficienza dello studio e prevenendo l’uso di campioni eccessivamente grandi o inadeguati. In SEM, la potenza a priori si stima specificando nel software le caratteristiche del modello di popolazione, ipotesi nulle e alternative, il livello di significatività statistica e la dimensione campionaria prevista.\nPotenza Retrospettiva (Post Hoc, Osservata): A differenza dell’analisi a priori, questa viene condotta dopo la raccolta dei dati. Le statistiche campionarie vengono trattate come parametri reali della popolazione, ma questa pratica presenta limitazioni significative. Le stime possono essere distorte, e una maggiore potenza osservata non implica necessariamente una forte evidenza a favore delle ipotesi nulle non rifiutate. Inoltre, essendo una misura post hoc, non aiuta nella progettazione proattiva della ricerca.\n\nPer l’analisi della potenza in SEM, sono stati sviluppati diversi metodi, tra cui:\n\nIl metodo Satorra–Saris stima la potenza del test del rapporto di verosimiglianza per un singolo parametro.\nIl metodo MacCallum–RMSEA si basa sulla RMSEA di popolazione e sulle distribuzioni chi-quadrato non centrali.\nIl metodo di simulazione Monte Carlo è un’alternativa moderna e flessibile che non presuppone né risultati continui né stima ML predefinita.\n\nCon l’avanzamento degli strumenti informatici, l’analisi della potenza statistica in SEM è diventata più accessibile:\nSoftware SEM con Simulazione Monte Carlo: Software come Mplus e LISREL includono capacità di simulazione Monte Carlo, permettendo di generare dati campionari basati su ipotesi del modello e di valutare la frequenza con cui i risultati significativi vengono ottenuti.\nMetodo Kelley–Lai Precision: Calcola la dimensione campionaria minima necessaria per stimare parametri come l’indice RMSEA entro un margine di errore specificato.\nNel contesto di R, le funzioni semTools::findRMSEApower e semTools::findRMSEAsamplesize del pacchetto semTools facilitano queste analisi:\n\nsemTools::findRMSEApower: Determina la potenza di un test SEM data una dimensione specifica del campione, basandosi sull’RMSEA e altri parametri del test.\nsemTools::findRMSEAsamplesize: Calcola la dimensione del campione necessaria per raggiungere una specifica potenza statistica in un test SEM, considerando l’RMSEA e altri criteri come il livello di significatività e la potenza desiderata.\n\nQuesti strumenti sono importanti per ottimizzare la progettazione della ricerca SEM, garantendo campioni adeguati e potenza statistica sufficiente per rilevare gli effetti di interesse.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#commenti-e-considerazioni-finali",
    "href": "chapters/sem/03_gof.html#commenti-e-considerazioni-finali",
    "title": "41  Test del Modello e Indicizzazione",
    "section": "41.14 Commenti e considerazioni finali",
    "text": "41.14 Commenti e considerazioni finali\nNella letteratura SEM sono state sollevate forti argomentazioni contro l’applicazione di RMSEA, CFI e TLI e i loro valori di cutoff convenzionali [si veda, ad esempio, {cite:t}barrett2007structural]. Tuttavia, prima che i ricercatori propongano e accettino alternative migliori, questi indici di bontà dell’adattamento continueranno ad essere applicati nella maggior parte degli studi SEM. {cite:t}xia2019rmsea fanno notare come, in base alla consuetudine corrente, valori RMSEA più grandi e valori CFI e TLI più piccoli indicano un adattamento peggiore. Ciò spinge i ricercatori a modificare i loro modelli per cercare di ottenere indici migliori. Tuttavia, la pratica attuale si è evoluta a tal punto da raggiungere la fase per cui gli indici di adattamento servono come gli unici criteri (in molte situazioni) per determinare se accettare o rifiutare un modello ipotizzato: se i valori degli indici di adattamento raggiungono la soglia “di pubblicabilità” (ad es. RMSEA &lt; .06), allora non si ritiene più necessario migliorare il modello. In realtà, un’affermazione come la seguente non è sufficiente: “poiché i valori RMSEA, CFI e TLI suggeriscono un buon adattamento, questo modello è stato scelto come modello finale”. Il raggiungimento di una serie di soglie desiderate di RMSEA, CFI e TLI è solo uno dei possibili indicatori che devono essere considerati nel processo di selezione di modelli. I ricercatori dovrebbero anche spiegare se esistono altre opzioni per migliorare il modello, perché tali opzioni sono o non sono adottate, e quali sono le conseguenze scientifiche e cliniche che derivano dalla scelta del modello in questione come quello finale.\n\n\n\n\nHu, Li-tze, e Peter M Bentler. 1998. «Fit indices in covariance structure modeling: Sensitivity to underparameterized model misspecification.» Psychological Methods 3 (4): 424--453.\n\n\nKline, Rex B. 2023. Principles and practice of structural equation modeling. Guilford publications.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html",
    "href": "chapters/sem/04_mod_comp.html",
    "title": "42  Confronto tra modelli",
    "section": "",
    "text": "42.1 Confrontare Modelli nel SEM\nNel contesto dei Modelli di Equazioni Strutturali (SEM), un aspetto critico è il confronto tra diversi modelli per determinare quale sia il più adeguato. Questo confronto si presenta frequentemente nella forma di analisi di modelli nidificati. In tale contesto, si confronta un modello considerato “pieno” o “meno restrittivo” con un altro modello che è “ridotto” o “più restrittivo”.\nIl modello pieno include un insieme più ampio di parametri e ipotesi, offrendo una rappresentazione più complessa delle relazioni tra le variabili. Al contrario, il modello ridotto è una versione più semplificata, con meno parametri e ipotesi, risultando in una struttura più contenuta e potenzialmente più parsimoniosa.\nQuesto tipo di confronto è cruciale per valutare l’adeguatezza dei modelli SEM, permettendo ai ricercatori di decidere se la complessità aggiuntiva del modello pieno sia giustificata rispetto al modello ridotto in termini di adattamento ai dati e coerenza con la teoria sottostante.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html#confrontare-modelli-nel-sem",
    "href": "chapters/sem/04_mod_comp.html#confrontare-modelli-nel-sem",
    "title": "42  Confronto tra modelli",
    "section": "",
    "text": "42.1.1 Analisi dei Modelli Nidificati\nNell’ambito dei Modelli di Equazioni Strutturali (SEM), i modelli nidificati occupano un ruolo centrale. Due modelli sono definiti come nidificati quando soddisfano specifici criteri gerarchici, delineati come segue:\n\nFormazione del Modello Vincolato:\n\nSi crea un modello vincolato applicando una o più restrizioni a un modello non vincolato esistente. Questo processo aumenta i gradi di libertà del modello vincolato (C) rispetto a quelli del modello non vincolato (U), risultando in $ _C &gt; _U $.\n\nDifferenze nei Gradi di Libertà:\n\nLa differenza $ _C - _U $ rappresenta il numero di restrizioni imposte al modello non vincolato per creare il modello vincolato, che equivale alla variazione nel numero di parametri liberi tra i due modelli.\n\nParametri Liberi e Vincolati:\n\nI parametri liberi nel modello vincolato costituiscono un sottoinsieme di quelli presenti nel modello non vincolato. Allo stesso modo, i parametri fissi nel modello non vincolato formano un sottoinsieme di quelli nel modello vincolato.\n\nConfronto dei Valori di Chi-Quadro:\n\nTra i due modelli, il valore di $ ^2 $ è minore o uguale nel modello non vincolato rispetto a quello nel modello vincolato, ovvero $ ^2_U ^2_C $. Questo implica che le distribuzioni di probabilità possibili nel modello vincolato sono comprese anche nel modello non vincolato, che può tuttavia suggerire ulteriori distribuzioni non coerenti con il modello vincolato.\n\n\nQuesto tipo di relazione gerarchica, conosciuta come annidamento dei parametri, permette di valutare l’impatto di specifiche restrizioni o aggiunte di parametri. Ad esempio, un parametro libero in un modello non vincolato può essere fissato a zero, eliminando di conseguenza l’effetto corrispondente nel modello vincolato, oppure può essere sottoposto a un vincolo specificato dal ricercatore, riducendo così il numero di parametri liberi ma mantenendo l’effetto nel modello vincolato.\nConsideriamo, per esempio, un modello di percorso non vincolato U con effetti diretti: $ X Y_1 Y_2 $ e $ X Y_2 $. Ridefinendo il percorso $ X Y_2 = 0 $, eliminiamo questa connessione dal modello U, generando così il modello vincolato C1, nidificato sotto U. Un’alternativa potrebbe essere imporre un vincolo di uguaglianza tra $ X Y_2 $ e $ Y_1 Y_2 $, indicando che gli effetti diretti non standardizzati di X e Y1 su Y2 sono identici. Questo produce un modello vincolato, C2, con un parametro libero in meno rispetto a U ma che include tutti i percorsi di U.\nNelle prossime sezioni, esamineremo come testare le ipotesi inerenti a questi modelli nidificati e come valutare la loro adeguatezza nel contesto SEM.\n\n\n42.1.2 Strategie di Costruzione e Potatura nei Modelli SEM\n\n42.1.2.1 Costruzione Progressiva del Modello\nLa costruzione di modelli SEM inizia tipicamente con un modello iniziale semplice e vincolato, che riflette le ipotesi fondamentali basate su teorie sostanziali. Questo approccio, detto anche ricerca in avanti, implica l’aggiunta progressiva di parametri liberi che rappresentano ipotesi precedentemente escluse, in base alla loro importanza. Sebbene questo processo possa generalmente migliorare l’adattamento del modello (riduzione di chiML), un adattamento migliore non è necessariamente indicativo di una maggiore correttezza del modello. La costruzione del modello può teoricamente proseguire fino a quando non si raggiunge un modello perfettamente adatto ai dati (dfM = 0), ma è essenziale valutare la validità teorica e la parsimonia del modello in ogni passaggio.\n\n\n42.1.2.2 Potatura Retrograda del Modello\nIn contrasto, la potatura del modello, o ricerca all’indietro, inizia con un modello più complesso e non vincolato. In questa fase, il ricercatore semplifica il modello eliminando parametri liberi (fissandoli a zero) o imponendo vincoli di stima. Questo processo richiede di dare priorità alle ipotesi in ordine inverso di importanza. Il modello iniziale dovrebbe essere congruente con i dati, altrimenti non ha senso restringerlo ulteriormente. Tipicamente, come si procede con la potatura, l’adattamento complessivo del modello ai dati tende a peggiorare (aumento di chiML). Il criterio per arrestare la potatura si basa sull’adattamento del modello: si ferma quando ulteriori restrizioni peggiorerebbero significativamente l’adattamento ai dati.\n\n\n42.1.2.3 Obiettivi e Considerazioni\nL’obiettivo sia nella costruzione che nella potatura di modelli è identificare un modello con una struttura di covarianza (e, se presente, anche di media) correttamente specificata e teoricamente giustificata. Idealmente, entrambi gli approcci dovrebbero convergere verso lo stesso modello ottimale, benché ciò non sia garantito. È importante evitare il rischio di formulare ipotesi post hoc (HARKing), presentando modelli scoperti in modo esplorativo come se fossero stati ipotizzati a priori. Una soluzione a questo problema è la preregistrazione del piano di analisi.\n\n\n42.1.2.4 Punti di Forza Relativi\n\nCostruzione del Modello: Partire da un modello più semplice può essere vantaggioso, soprattutto per i neofiti del SEM, poiché facilita l’identificazione statistica e riduce il rischio di errori nella specificazione del modello.\nPotatura del Modello: Questo approccio può essere particolarmente efficace per i modelli di misurazione, dove le variabili osservate sono usate come indicatori di un numero limitato di fattori comuni. Un modello di misurazione correttamente specificato inizialmente può rendere la potatura più efficace rispetto alla costruzione.\n\nIn entrambi i casi, è cruciale basare le decisioni su solide basi teoriche oltre che su considerazioni statistiche, per assicurare che il modello finale sia non solo adatto ai dati ma anche coerente con il quadro teorico sottostante.\n\n\n\n42.1.3 Strategie di Ridefinizione dei Modelli SEM: Approcci Teorici ed Empirici\n\n42.1.3.1 Approccio Teorico nella Ridefinizione dei Modelli\nNel processo di costruzione o potatura dei Modelli di Equazioni Strutturali (SEM), l’approccio teorico gioca un ruolo fondamentale. Qui, le modifiche al modello sono guidate da ipotesi teoriche predefinite e specifiche. Ad esempio, considerando un modello di percorso non vincolato U con le relazioni \\(X \\rightarrow Y_1 \\rightarrow Y_2\\) e \\(X \\rightarrow Y_2\\), un ricercatore potrebbe ipotizzare che l’effetto di X su Y2 sia esclusivamente indiretto attraverso Y1. Questa ipotesi può essere testata vincolando il coefficiente di \\(X \\rightarrow Y_2\\) a zero. Se l’adattamento del modello così modificato non è significativamente inferiore rispetto al modello non vincolato, l’ipotesi di un effetto indiretto viene supportata, a patto che la direzionalità delle relazioni sia corretta.\nQuesto approccio enfatizza che le modifiche al modello dovrebbero essere effettuate sulla base di solide basi teoriche e concettuali, piuttosto che su criteri puramente statistici, come evidenziato da Jöreskog (1969): “La decisione di smettere di aggiungere parametri non può basarsi solo su una base statistica; ciò dipende in gran parte dall’interpretazione dei dati da parte del ricercatore, basata su considerazioni teoriche e concettuali sostanziali.”\n\n\n42.1.3.2 Approccio Empirico nella Ridefinizione dei Modelli\nAl contrario, l’approccio empirico nella costruzione o potatura dei modelli SEM si basa su criteri statistici. In questo scenario, i parametri liberi vengono aggiunti o eliminati a seconda della loro significatività statistica o di altri indicatori empirici. Per esempio, se i percorsi sono eliminati solo perché i loro coefficienti non sono statisticamente significativi, la ridefinizione del modello è guidata da considerazioni puramente empiriche. Questo approccio è analogo alla tecnica di eliminazione all’indietro nella regressione multipla, dove il software sceglie quali predittori rimuovere in base a criteri di significatività statistica.\n\n\n42.1.3.3 Implicazioni per l’Interpretazione dei Modelli\nLa scelta tra un approccio teorico o empirico nella ridefinizione dei modelli SEM ha implicazioni significative per come interpretiamo i risultati. Un modello modificato in base a criteri teorici forti offre una maggiore fiducia nella validità delle sue conclusioni, mentre un modello costruito o potato basandosi principalmente su criteri empirici può essere soggetto a errori di Tipo I o II e può non essere replicabile in campioni diversi.\nÈ fondamentale che i ricercatori si avvicinino alla costruzione e alla potatura dei modelli SEM con un equilibrio tra intuizioni teoriche e risultati empirici, per assicurare che i modelli finali siano non solo statisticamente validi ma anche teoricamente giustificati e interpretativamente significativi.\n\n\n\n42.1.4 Test della Differenza Chi-Quadro nel SEM\n\n42.1.4.1 Principi Fondamentali\nIl test della differenza Chi-Quadro (chiD) è una tecnica statistica essenziale nel contesto dei Modelli di Equazioni Strutturali (SEM) per valutare l’effetto delle modifiche ai parametri sui modelli. Questo test viene utilizzato sia nella potatura (restrizione dei parametri) che nella costruzione (aggiunta di parametri) dei modelli. Il valore chiD rappresenta la differenza tra i valori di chi-quadro (chi-quadro massima verosimiglianza, chiML) di due modelli nidificati. I gradi di libertà associati, dfD, sono determinati dalla differenza nei gradi di libertà dei due modelli ($ _C - _U $).\n\n\n42.1.4.2 Applicazione del Test\nPer applicare il test della differenza chi-quadro, si seguono questi passaggi: 1. Definizione dei Modelli: Identificare il modello pieno (con tutti i parametri ritenuti rilevanti) e il modello ridotto (una versione semplificata del modello pieno con alcune restrizioni). 2. Stima dei Modelli: Utilizzare metodi di massima verosimiglianza per stimare entrambi i modelli. 3. Calcolo del Rapporto di Verosimiglianze: Calcolare la differenza tra i logaritmi delle funzioni di verosimiglianza dei due modelli ($ D = -2((L_r) - (L_f)) $). 4. Test Statistico: Utilizzare la distribuzione chi-quadrato per determinare il p-value. Un p-value basso indica che il modello ridotto non si adatta ai dati così come il modello pieno.\n\n\n42.1.4.3 Interpretazione dei Risultati\nUn valore piccolo di chiD suggerisce che non c’è una differenza significativa nell’adattamento tra i due modelli, mentre un valore grande indica una differenza significativa. In termini di potatura, un grande chiD implica che il modello è stato eccessivamente vincolato. Nella costruzione, un grande chiD supporta la conservazione del parametro libero aggiunto. Tuttavia, prima di trarre conclusioni definitive, è cruciale considerare l’adattamento complessivo del modello, sia a livello globale che locale.\n\n\n42.1.4.4 Considerazioni nella Stima ML Robusta\nNel caso di stima ML robusta, la differenza tra i chi-quadri scalati non può essere interpretata come un test dell’ipotesi di adattamento uguale a causa delle distribuzioni non centrali in condizioni di non normalità. Sono disponibili metodi specifici per calcolare una statistica di differenza chi-quadro scalata che segue approssimativamente le distribuzioni chi-quadro.\n\n\n42.1.4.5 Implicazioni Teoriche ed Empiriche\nLa decisione di modificare un modello basandosi su un approccio teorico o empirico ha implicazioni significative. Ad esempio, l’eliminazione di percorsi non significativi su base puramente statistica può portare a conclusioni errate se non supportate da una solida base teorica. Inoltre, è importante essere consapevoli dei rischi associati alla capitalizzazione sul caso, come errori di Tipo I e II, e del pericolo di seguire “sentieri che si biforcano” nelle decisioni analitiche, che possono rendere i risultati specifici del campione e difficili da replicare. Per mitigare questi rischi, è consigliabile basare le modifiche del modello più su orientamenti teorici che sui risultati dei test di significatività.\n\n\n\n42.1.5 Test della Differenza Chi-Quadro Scalato\nIl metodo di Satorra e Bentler (2001) permette di calcolare manualmente una statistica di differenza chi-quadro scalata quando si confrontano due modelli gerarchici nella stima ML robusta. Si presume che il modello 1 sia più vincolato rispetto al modello 2 (cioè, dfM1 &gt; dfM2), i chi-quadri non scalati siano chiML e i chi-quadri scalati siano chiSB. La statistica di test Satorra-Bentler è definita come segue: 1. Calcolare la Statistica di Differenza Chi-Quadro non Scalata e i suoi Gradi di Libertà: - chiD = chiML1 - chiML2 e dfD = dfM1 - dfM2. 2. Recuperare il Fattore di Correzione di Scala, c, per Ogni Modello: - c1 = chiML1 / chiSB1 e c2 = chiML2 / chiSB2. 3. Calcolare la Statistica di Differenza Chi-Quadro Scalata, chiSD: - chiSD = chiD / ((c1 / dfM1 - c2 / dfM2) / dfD). - La probabilità per chiSD (dfD) in una distribuzione chi-quadro centrale rappresenta il p-value per il test di differenza chi-quadro scalato.\nIn campioni piccoli o quando il modello più vincolato è molto errato, il denominatore di chiSD può essere &lt; 0, invalidando il test. Questo test è implementato nella funzione lavTestLRT() in lavaan (Rosseel et al., 2023).",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html#esempio",
    "href": "chapters/sem/04_mod_comp.html#esempio",
    "title": "42  Confronto tra modelli",
    "section": "42.2 Esempio",
    "text": "42.2 Esempio\nConsideriamo nuovamente i dati discussi da {cite:t}brown2015confirmatory relativi al modello di misurazione per la depressione maggiore così come è definita nel DSM-IV. Ignoriamo qui le differenze di genere – si veda il Capitolo {ref}factorial-invariance-notebook. Leggiamo i dati in \\(\\mathsf{R}\\):\n\nd_mdd &lt;- readRDS(\n    here::here(\"data\", \"mdd_sex.RDS\")\n)\n\nConsideriamo il seguente modello:\n\nmodel_mdd &lt;- \"\n  MDD =~ mdd1 + mdd2 + mdd3 + mdd4 + mdd5 + mdd6 + mdd7 + mdd8 + mdd9\n\"\n\nAdattiamo il modello ai dati.\n\nfit_mdd &lt;- cfa(\n    model_mdd,\n    data = d_mdd\n)\n\n\nsemPaths(fit_mdd,\n    whatLabels = \"std\",\n    sizeMan = 8,\n    edge.label.cex = 0.7,\n    style = \"mx\",\n    nCharNodes = 0, nCharEdges = 0\n)\n\n\n\n\n\n\n\n\n\nfitMeasures(fit_mdd) |&gt;\n    print()\n\n                 npar                  fmin                 chisq \n               18.000                 0.074               110.272 \n                   df                pvalue        baseline.chisq \n               27.000                 0.000              1297.618 \n          baseline.df       baseline.pvalue                   cfi \n               36.000                 0.000                 0.934 \n                  tli                  nnfi                   rfi \n                0.912                 0.912                 0.887 \n                  nfi                  pnfi                   ifi \n                0.915                 0.686                 0.934 \n                  rni                  logl     unrestricted.logl \n                0.934            -13747.192            -13692.056 \n                  aic                   bic                ntotal \n            27530.385             27613.546               750.000 \n                 bic2                 rmsea        rmsea.ci.lower \n            27556.389                 0.064                 0.052 \n       rmsea.ci.upper        rmsea.ci.level          rmsea.pvalue \n                0.077                 0.900                 0.029 \n       rmsea.close.h0 rmsea.notclose.pvalue     rmsea.notclose.h0 \n                0.050                 0.019                 0.080 \n                  rmr            rmr_nomean                  srmr \n                0.191                 0.191                 0.044 \n         srmr_bentler   srmr_bentler_nomean                  crmr \n                0.044                 0.044                 0.050 \n          crmr_nomean            srmr_mplus     srmr_mplus_nomean \n                0.050                 0.044                 0.044 \n                cn_05                 cn_01                   gfi \n              273.824               320.411                 0.964 \n                 agfi                  pgfi                   mfi \n                0.940                 0.578                 0.946 \n                 ecvi \n                0.195 \n\n\nGli indici Comparative Fit Index (CFI) = 0.934 e Tucker-Lewis Index (TLI) = 0.912 sono superiori a 0.9, dunque sono almeno sufficienti per gli standard correnti. L’indice RMSEA = 0.064 è appena superiore alla soglia di 0.06. L’indice SRMR = 0.044 è inferiore alla soglia 0.05. Dunque, complessivamente, il modello sembra adeguato.\nAdattiamo ora il modello con la modifica proposta da {cite:t}brown2015confirmatory, ovvero\n\nmodel2_mdd &lt;- \"\n  MDD =~ mdd1 + mdd2 + mdd3 + mdd4 + mdd5 + mdd6 + mdd7 + mdd8 +  mdd9\n  mdd1 ~~ mdd2\n\"\n\nfit2_mdd &lt;- cfa(\n    model2_mdd,\n    data = d_mdd\n)\n\nEseguiamo il test del rapporto di verosimiglianze:\n\nlavTestLRT(fit_mdd, fit2_mdd)\n\n\nA anova: 2 x 8\n\n\n\nDf\nAIC\nBIC\nChisq\nChisq diff\nRMSEA\nDf diff\nPr(&gt;Chisq)\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nfit2_mdd\n26\n27489.67\n27577.45\n67.55856\nNA\nNA\nNA\nNA\n\n\nfit_mdd\n27\n27530.38\n27613.55\n110.27242\n42.71385\n0.2358357\n1\n6.336194e-11\n\n\n\n\n\nIl test indica che il modello alternativo si adatta meglio ai dati del modello originale.\nEsaminiamo gli indici di bontà di adattamento.\n\neffectsize::interpret(fit2_mdd)\n\n\nA data.frame: 10 x 4\n\n\nName\nValue\nThreshold\nInterpretation\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;effctsz_&gt;\n\n\n\n\nGFI\n0.97807123\n0.95\nsatisfactory\n\n\nAGFI\n0.96204635\n0.90\nsatisfactory\n\n\nNFI\n0.94793648\n0.90\nsatisfactory\n\n\nNNFI\n0.95438982\n0.90\nsatisfactory\n\n\nCFI\n0.96705932\n0.90\nsatisfactory\n\n\nRMSEA\n0.04616501\n0.05\nsatisfactory\n\n\nSRMR\n0.03675390\n0.08\nsatisfactory\n\n\nRFI\n0.92791205\n0.90\nsatisfactory\n\n\nPNFI\n0.68462079\n0.50\nsatisfactory\n\n\nIFI\n0.96731836\n0.90\nsatisfactory\n\n\n\n\n\nGli indici Comparative Fit Index (CFI) = 0.967 e Tucker-Lewis Index (TLI) = 0.954 sono superiori a 0.95. L’indice RMSEA = 0.046. L’indice SRMR = 0.037.\nIl “costo” che si paga per questo miglioramento dell’adattamento è che indici di adattamento così buoni, probabilmente, non si replicheranno in un altro campione di dati, a meno che venga introdotto un qualche altro aggiustamento che, sicuramente, sarà diverso da quello usato nel campione corrente. Personalmente, non avrei introdotto il “miglioramento” proposto da {cite:t}brown2015confirmatory in quanto, anche senza un tale aggiustamento post-hoc, il modello produce un adattamento accettabile.\n\n42.2.1 Confronto di Modelli Non Annidati\nProseguendo, analizzeremo l’adattamento di due modelli distinti, entrambi costituiti dalle stesse variabili e applicati agli stessi dati. Tuttavia, a differenza di quanto precedentemente delineato, questi modelli non sono collegati gerarchicamente, ma si configurano come modelli non annidati. Questa situazione si presenta comunemente quando i ricercatori mettono a confronto modelli basati su teorie divergenti. È possibile effettuare un confronto informale dei valori del chi-quadrato derivanti da modelli non annidati, ma la loro differenza non va interpretata come una statistica di test valida. In altre parole, i test di differenza del chi-quadrato, sia in forma scalata che non, non sono appropriati in questo contesto. La ragione risiede nel fatto che la differenza tra le statistiche di test di modelli non annidati non segue una distribuzione chi-quadrato centrale. Sebbene siano stati compiuti sforzi per elaborare test di significatività adatti al confronto di modelli non annidati, questi metodi non hanno trovato un ampio utilizzo e spesso portano a complicazioni interpretative (Levy & Hancock, 2007).\nUna soluzione più pragmatica è rappresentata dalla famiglia degli indici di adattamento predittivo, conosciuti anche come criteri teorico-informativi. Questi indici non sono test di significatività, poiché le loro distribuzioni di probabilità variano ampiamente a seconda del tipo di modello e dei dati considerati e, pertanto, rimangono generalmente ignote. Piuttosto, essi riflettono sia la qualità dell’adattamento del modello sia la sua complessità, bilanciando questi due aspetti. Ciò implica l’applicazione di una penalità per la complessità del modello, che consente di regolare l’adattamento in funzione del numero di parametri liberi. Per esempio, nel caso di due modelli non annidati che mostrano un adattamento simile agli stessi dati, verrà privilegiato il modello meno complesso, in quanto considerato più probabile nella generalizzazione su campioni replicati. In questo scenario, il valore del criterio informativo sarà inferiore per il modello più semplice, dato che una penalità maggiore per la complessità viene applicata all’adattamento del modello più complesso. Di conseguenza, il modello con il criterio informativo più basso è da preferire. In questo capitolo, dopo aver introdotto un problema di ricerca, esploreremo due indici di adattamento predittivo ampiamente usati (AIC e BIC).\n\n\n\n```mfloqxmu ../images/romney.png\n\n\n\n\nheight: 550px\n\n\nname: romney-fig\n\n\n\nModelli alternativi non annidati di percorso ricorsivo per l’adattamento dopo un intervento chirurgico cardiaco. (Figura tratta da {cite:t}kline2023principles.)\n\nNella figura sono presentati due modelli di percorso che descrivono il recupero dei pazienti dopo un intervento chirurgico cardiaco (Romney et al., 1992) -- si veda {cite:t}`kline2023principles`, cap. 11. Il modello psicosomatico rappresenta le ipotesi che il morale del paziente trasmetta gli effetti della disfunzione neurologica e dello stato socioeconomico ridotto (SES) su sintomi della malattia e scarse relazioni sociali. Il modello medico rappresenta un diverso schema di relazioni causali tra le stesse variabili. In particolare, sia i sintomi della malattia sia la disfunzione neurologica sono specificati come variabili esogene con effetti diretti sul SES ridotto, basso morale e scarse relazioni. Tra queste tre variabili endogene, si ipotizza che il SES ridotto influenzi indirettamente le scarse relazioni attraverso il suo impatto precedente sul basso morale. Ci sono ulteriori effetti indiretti nel modello medico convenzionale dalle variabili esogene a quelle endogene. I due modelli nella figura non sono annidati, quindi il test della differenza del chi-quadro non può essere utilizzato per confrontarli direttamente. È dunque necessario seguire un altro approccio.\n\n## AIC e BIC\n\nUno degli indici di adattamento predittivo più noti basato sulla stima di massima verosimiglianza (ML) è il Criterio di Informazione di Akaike (AIC), che prende il nome dallo statistico Hirotugu Akaike. La formula per l'AIC di Akaike (1974, p. 719) è:\n\n$$ \\text{AIC} = -2 \\ln L_0 + 2q $$\n\ndove $ L_0 $ è la funzione di verosimiglianza massimizzata nella stima ML per il modello del ricercatore e $ q $ è il numero di parametri liberi del modello. Si noti che la penalità per la complessità nell'equazione precedente, $ 2q $, diventa relativamente più piccola all'aumentare della dimensione del campione (Mulaik, 2009b). \n\nUn diverso indice teorico-informativo che tiene direttamente conto della dimensione del campione è il Criterio di Informazione Bayesiano (BIC) (Raftery, 1993; Schwarz, 1978). La formula è\n\n$$ \\text{BIC} = -2 \\ln L_0 + q \\ln N $$\n\nConfrontato con l'AIC, il BIC impone una penalità relativa maggiore per la complessità del modello. \n\nSupponiamo che il numero di parametri stimati liberamente sia $ q = 10 $ e che $ N = 300 $. La penalità AIC equivale a $ 2(10) $, ovvero 20.000 (Equazione 11.4), ma la penalità BIC per lo stesso modello è $ 10 (\\ln 300) $, ovvero 50.038, più del doppio rispetto all'AIC. I valori relativi delle penalità BIC aumentano più lentamente all'aumentare della dimensione del campione; in altre parole, la sua penalità è asintotica su campioni sempre più grandi (Mulaik, 2009b).\n\nIn sostanza, sia l'AIC che il BIC sono strumenti per bilanciare l'adattamento del modello con la sua complessità, ma differiscono nel modo in cui valutano e penalizzano questa complessità, soprattutto in relazione alla dimensione del campione.\n\nUtilizzando lo script fornito da {cite:t}`kline2023principles`, iniziamo a importare i dati in `R`:\n\n::: {#cell-22 .cell vscode='{\"languageId\":\"r\"}' execution_count=11}\n``` {.r .cell-code}\n# input the correlations in lower diagnonal form\nromneyLower.cor &lt;- \"\n 1.00\n  .53 1.00\n  .15  .18 1.00\n  .52  .29 -.05 1.00\n  .30  .34  .23  .09 1.00 \"\n\n# name the variables and convert to full correlation matrix\nromney.cor &lt;- lavaan::getCov(romneyLower.cor, names = c(\n    \"morale\", \"symptoms\",\n    \"dysfunction\", \"relations\", \"ses\"\n))\n:::\nEsaminiamo le matrici di correlazioni e covarianze:\n\n# display the correlations\nromney.cor\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nmorale\nsymptoms\ndysfunction\nrelations\nses\n\n\n\n\nmorale\n1.00\n0.53\n0.15\n0.52\n0.30\n\n\nsymptoms\n0.53\n1.00\n0.18\n0.29\n0.34\n\n\ndysfunction\n0.15\n0.18\n1.00\n-0.05\n0.23\n\n\nrelations\n0.52\n0.29\n-0.05\n1.00\n0.09\n\n\nses\n0.30\n0.34\n0.23\n0.09\n1.00\n\n\n\n\n\n\n# add the standard deviations and convert to covariances\nromney.cov &lt;- lavaan::cor2cov(romney.cor, sds = c(\n    3.75, 17.00, 19.50,\n    3.50, 24.70\n))\n\n# display the covariances\nromney.cov\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nmorale\nsymptoms\ndysfunction\nrelations\nses\n\n\n\n\nmorale\n14.06250\n33.7875\n10.96875\n6.8250\n27.7875\n\n\nsymptoms\n33.78750\n289.0000\n59.67000\n17.2550\n142.7660\n\n\ndysfunction\n10.96875\n59.6700\n380.25000\n-3.4125\n110.7795\n\n\nrelations\n6.82500\n17.2550\n-3.41250\n12.2500\n7.7805\n\n\nses\n27.78750\n142.7660\n110.77950\n7.7805\n610.0900\n\n\n\n\n\nSpecifichiamo il modello psicosomatico.\n\nsomatic.model &lt;- \"\n    # regressions\n    morale ~ ses + dysfunction\n    relations ~ morale\n    symptoms ~ morale\n    # without the zero constraint listed next,\n    # lavaan automatically specifies correlated\n    # disturbances for symptoms and relations,\n    # but their disturbances are independent in\n    # figure 11.1\n    symptoms ~~ 0*relations\n    # unanalyzed association between ses and dysfunction\n    # automatically specified \n\"\n\nSpecifichiamo il modello medico convenzionale.\n\nmedical.model &lt;- \"\n    # regressions\n    ses ~ symptoms + dysfunction\n    morale ~ symptoms + ses\n    relations ~ dysfunction + morale\n    # unanalyzed association between symptoms and dysfunction\n    # automatically specified \n\"\n\nAdattiamo ai dati il modello psicosomatico.\n\nsomatic &lt;- lavaan::sem(somatic.model,\n    sample.cov = romney.cov,\n    sample.nobs = 469, fixed.x = FALSE, sample.cov.rescale = FALSE\n)\n\n\nsemPaths(somatic,\n    whatLabels = \"std\",\n    sizeMan = 10,\n    edge.label.cex = 1.0,\n    style = \"mx\",\n    nCharNodes = 0, nCharEdges = 0\n)\n\n\n\n\n\n\n\n\n\nlavaan::summary(somatic, fit.measures = TRUE, rsquare = TRUE) |&gt; \n    print()\n\nlavaan 0.6.17 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n\n  Number of observations                           469\n\nModel Test User Model:\n                                                      \n  Test statistic                                40.488\n  Degrees of freedom                                 5\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               390.816\n  Degrees of freedom                                 9\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.907\n  Tucker-Lewis Index (TLI)                       0.833\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -8572.844\n  Loglikelihood unrestricted model (H1)      -8552.599\n                                                      \n  Akaike (AIC)                               17165.687\n  Bayesian (BIC)                             17207.193\n  Sample-size adjusted Bayesian (SABIC)      17175.455\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.123\n  90 Percent confidence interval - lower         0.090\n  90 Percent confidence interval - upper         0.159\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.982\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.065\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  morale ~                                            \n    ses               0.043    0.007    6.217    0.000\n    dysfunction       0.016    0.009    1.897    0.058\n  relations ~                                         \n    morale            0.485    0.037   13.184    0.000\n  symptoms ~                                          \n    morale            2.403    0.178   13.535    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n .relations ~~                                        \n   .symptoms          0.000                           \n  ses ~~                                              \n    dysfunction     110.779   22.821    4.854    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .morale           12.699    0.829   15.313    0.000\n   .relations         8.938    0.584   15.313    0.000\n   .symptoms        207.820   13.571   15.313    0.000\n    ses             610.090   39.840   15.313    0.000\n    dysfunction     380.250   24.831   15.313    0.000\n\nR-Square:\n                   Estimate\n    morale            0.097\n    relations         0.270\n    symptoms          0.281\n\n\n\nOra approfondiamo l’analisi dei residui. Nel contesto di un modello SEM, i residui sono derivati dalle differenze tra la matrice di correlazioni (o covarianze) osservata e quella prevista dal modello. Queste differenze sono elaborate attraverso specifiche funzioni per generare i residui. Utilizzando il pacchetto lavaan in R, possiamo accedere a tre principali tipi di residui: standardized.mplus, normalized, e cor.bollen.\n\nResidui Standardizzati (Standardized.mplus): Questo tipo di residuo è una versione standardizzata dei residui. I residui standardizzati sono ottenuti calcolando la differenza tra i valori osservati e quelli previsti dal modello, e dividendo questa differenza per uno stimatore della deviazione standard del residuo. Questo processo trasforma i residui in una scala in cui hanno una varianza approssimativamente uguale. I residui standardizzati sono utili per identificare punti dati che il modello non riesce a spiegare bene. In lavaan, type = \"standardized.mplus\" si riferisce a una particolare forma di standardizzazione dei residui, simile a quella utilizzata nel software Mplus.\nResidui Normalizzati (Normalized): I residui normalizzati sono un altro tipo di residui standardizzati. Sono calcolati come i residui standardizzati ma poi vengono normalizzati. La normalizzazione qui significa che i residui vengono ulteriormente trasformati in modo che la loro distribuzione si avvicini a una distribuzione normale. Questo è utile per verificare se i residui seguono una distribuzione normale, il che è un’assunzione comune in molti modelli statistici, inclusi quelli SEM.\nCorrelazione dei Residui secondo Bollen (Cor.bollen): Questo tipo di residuo si riferisce alla correlazione tra i residui di due variabili diverse nel modello. Il metodo cor.bollen calcola la correlazione tra i residui dopo che il modello è stato adattato ai dati. Questo tipo di analisi è utile per rilevare se ci sono correlazioni non modellate tra le variabili che potrebbero influenzare la validità del modello.\n\n\nlavaan::residuals(somatic, type = \"standardized.mplus\") |&gt; print()\n\n$type\n[1] \"standardized.mplus\"\n\n$cov\n            morale reltns symptm    ses dysfnc\nmorale       0.000                            \nrelations    0.000  0.000                     \nsymptoms     0.000  0.427  0.000              \nses          0.000 -1.776  4.542  0.000       \ndysfunction  0.000 -3.291  2.549  0.000  0.000\n\n\n\n\nlavaan::residuals(somatic, type = \"normalized\") |&gt; print()\n\n$type\n[1] \"normalized\"\n\n$cov\n            morale reltns symptm    ses dysfnc\nmorale       0.000                            \nrelations    0.000  0.000                     \nsymptoms     0.000  0.300  0.000              \nses          0.000 -1.424  3.711  0.000       \ndysfunction  0.000 -2.769  2.142  0.000  0.000\n\n\n\n\nlavaan::residuals(somatic, type = \"cor.bollen\")\n\n\n    $type\n        'cor.bollen'\n    $cov\n        \n\nA lavaan.matrix.symmetric: 5 x 5 of type dbl\n\n\n\nmorale\nrelations\nsymptoms\nses\ndysfunction\n\n\n\n\nmorale\n0.000000e+00\n-1.110223e-16\n0.0000\n0.000\n0.0000\n\n\nrelations\n-1.110223e-16\n-3.330669e-16\n0.0144\n-0.066\n-0.1280\n\n\nsymptoms\n0.000000e+00\n1.440000e-02\n0.0000\n0.181\n0.1005\n\n\nses\n0.000000e+00\n-6.600000e-02\n0.1810\n0.000\n0.0000\n\n\ndysfunction\n0.000000e+00\n-1.280000e-01\n0.1005\n0.000\n0.0000\n\n\n\n\n\n\n\n\nAdattiamo ai dati il modello medico convenzionale.\n\nmedical &lt;- lavaan::sem(medical.model,\n    sample.cov = romney.cov,\n    sample.nobs = 469, fixed.x = FALSE, sample.cov.rescale = FALSE\n)\n\n\nsemPaths(medical,\n    whatLabels = \"std\",\n    sizeMan = 10,\n    edge.label.cex = 1.15,\n    style = \"mx\",\n    nCharNodes = 0, nCharEdges = 0\n)\n\n\n\n\n\n\n\n\n\nlavaan::summary(medical, fit.measures = TRUE, rsquare = TRUE) |&gt; \n    print()\n\nlavaan 0.6.17 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        12\n\n  Number of observations                           469\n\nModel Test User Model:\n                                                      \n  Test statistic                                 3.245\n  Degrees of freedom                                 3\n  P-value (Chi-square)                           0.355\n\nModel Test Baseline Model:\n\n  Test statistic                               400.859\n  Degrees of freedom                                 9\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.999\n  Tucker-Lewis Index (TLI)                       0.998\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -8554.222\n  Loglikelihood unrestricted model (H1)      -8552.599\n                                                      \n  Akaike (AIC)                               17132.444\n  Bayesian (BIC)                             17182.251\n  Sample-size adjusted Bayesian (SABIC)      17144.166\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.013\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.080\n  P-value H_0: RMSEA &lt;= 0.050                    0.742\n  P-value H_0: RMSEA &gt;= 0.080                    0.050\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.016\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  ses ~                                               \n    symptoms          0.448    0.063    7.110    0.000\n    dysfunction       0.221    0.055    4.019    0.000\n  morale ~                                            \n    symptoms          0.107    0.009   11.756    0.000\n    ses               0.021    0.006    3.291    0.001\n  relations ~                                         \n    dysfunction      -0.024    0.007   -3.335    0.001\n    morale            0.504    0.037   13.745    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  symptoms ~~                                         \n    dysfunction      59.670   15.553    3.836    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .ses             521.598   34.062   15.313    0.000\n   .morale            9.884    0.645   15.313    0.000\n   .relations         8.732    0.570   15.313    0.000\n    symptoms        289.000   18.872   15.313    0.000\n    dysfunction     380.250   24.831   15.313    0.000\n\nR-Square:\n                   Estimate\n    ses               0.145\n    morale            0.297\n    relations         0.290\n\n\n\n\nlavaan::residuals(medical, type = \"standardized.mplus\") |&gt; print()\n\n$type\n[1] \"standardized.mplus\"\n\n$cov\n               ses morale reltns symptm dysfnc\nses          0.000                            \nmorale       0.000  0.000                     \nrelations   -1.161 -1.818     NA              \nsymptoms     0.000  0.000  0.833  0.000       \ndysfunction  0.000  0.842  0.862  0.000  0.000\n\n\n\n\nlavaan::residuals(medical, type = \"normalized\") |&gt; print()\n\n$type\n[1] \"normalized\"\n\n$cov\n               ses morale reltns symptm dysfnc\nses          0.000                            \nmorale       0.000  0.000                     \nrelations   -0.901 -0.080 -0.069              \nsymptoms     0.000  0.000  0.573  0.000       \ndysfunction  0.000  0.680  0.370  0.000  0.000\n\n\n\n\nlavaan::residuals(medical, type = \"cor.bollen\") |&gt; print()\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n               ses morale reltns symptm dysfnc\nses          0.000                            \nmorale       0.000  0.000                     \nrelations   -0.041 -0.003  0.000              \nsymptoms     0.000  0.000  0.028  0.000       \ndysfunction  0.000  0.032  0.017  0.000  0.000\n\n\n\nIn conclusione, i valori i valori degli indici di adattamento predittivo per i due modelli alternativi di Romney et al. (1992) sono stati generati seguendo le istruzioni precedentemente descritte. Non sorprende che l’adattamento globale del modello medico convenzionale, più complesso (con $ dfM = 3 $), sia migliore rispetto a quello del modello psicosomatico, più semplice (con $ dfM = 5 $). Nonostante il modello medico convenzionale sia più complesso e quindi soggetto a una penalità maggiore per il minor numero di gradi di libertà, i valori ottenuti sia nell’AIC che nel BIC sono inferiori rispetto a quelli del modello psicosomatico. Questo indica che il vantaggio in termini di adattamento del modello medico convenzionale è sufficiente a superare la penalità per la sua maggiore complessità. In base a queste analisi, il modello medico convenzionale è considerato più adatto rispetto al modello psicosomatico, come evidenziato dai valori più bassi nei criteri AIC e BIC.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html#indici-di-modifica-e-statistiche-correlate-nel-sem",
    "href": "chapters/sem/04_mod_comp.html#indici-di-modifica-e-statistiche-correlate-nel-sem",
    "title": "42  Confronto tra modelli",
    "section": "42.3 Indici di Modifica e Statistiche Correlate nel SEM",
    "text": "42.3 Indici di Modifica e Statistiche Correlate nel SEM\nNell’elaborazione di modelli SEM (Structural Equation Modeling), esiste la possibilità di implementare modifiche attraverso processi automatizzati. Questi interventi, di carattere esplorativo, si basano sull’aggiunta o sulla rimozione di parametri seguendo criteri empirici, come la significatività statistica di un indice di modifica (MI) o di un test di punteggio. Gli indici di modifica, in particolare, sono calcolati per quei parametri che nel modello sono stati inizialmente vincolati e servono a stimare quanto il chi-quadrato del modello di massima verosimiglianza (chiML) si ridurrebbe se un dato parametro vincolato venisse liberato.\nIl meccanismo di modifica automatica opera liberando, ad ogni iterazione, il parametro vincolato che presenta il valore di MI più alto. Questo processo continua finché non si raggiunge un MI che è statisticamente significativo secondo i criteri stabiliti dal ricercatore. È cruciale, tuttavia, riconoscere che l’uso di questa metodologia, specialmente in campioni di piccole dimensioni, può portare alla formulazione di modelli che si basano eccessivamente sul caso. Di conseguenza, questi modelli potrebbero risultare poco robusti e difficilmente replicabili in studi successivi. L’automazione nel processo di ottimizzazione dei modelli SEM richiede dunque un’attenta valutazione del contesto e della dimensione del campione per garantire l’affidabilità e la validità dei risultati ottenuti.\nUn altro strumento di uso frequente nei SEM è il test di Wald, basato sulla statistica W. Questo test è progettato per valutare l’impatto che avrebbe la fissazione a zero di un parametro precedentemente stimato liberamente nel modello. In termini più tecnici, il test di Wald stima l’incremento che si verificherebbe nel chi-quadrato del modello di massima verosimiglianza (chiML) se tale parametro fosse “potato”, ovvero escluso dal modello.\nSimilmente ai processi di modifica automatica, l’efficacia del test di Wald può essere influenzata dalla casualità. Un aspetto cruciale da considerare è la sensibilità di questi test alla dimensione del campione. Infatti, anche piccole modifiche nella bontà di adattamento del modello possono assumere una significatività statistica notevole in campioni di ampie dimensioni.\nDi conseguenza, quando si valutano gli indici di modifica come il MI (Modifica Index), è essenziale che il ricercatore non si limiti a considerarne la sola significatività statistica. È importante anche valutare l’entità del cambiamento che si verificherebbe nel coefficiente del parametro se fosse liberato. Se il cambiamento previsto è minimo, la significatività statistica dell’indice di modifica potrebbe essere più indicativa della dimensione del campione che non della sostanziale rilevanza dell’effetto analizzato. Questa considerazione sottolinea l’importanza di un approccio olistico e critico nell’interpretazione dei risultati dei test diagnostici in SEM, specialmente in contesti dove la dimensione del campione può influenzare significativamente i risultati.\n\nfit_mdd &lt;- cfa(\n    model_mdd,\n    data = d_mdd\n)\n\nCalcoliamo gli indici di modifica.\n\nmodification_indices &lt;- lavInspect(fit_mdd, \"mi\")\nmodification_indices |&gt; print()\n\n    lhs op  rhs     mi    epc sepc.lv sepc.all sepc.nox\n20 mdd1 ~~ mdd2 46.877  0.732   0.732    0.560    0.560\n21 mdd1 ~~ mdd3  4.996 -0.211  -0.211   -0.108   -0.108\n22 mdd1 ~~ mdd4  4.625 -0.215  -0.215   -0.108   -0.108\n23 mdd1 ~~ mdd5  7.797 -0.266  -0.266   -0.137   -0.137\n24 mdd1 ~~ mdd6  6.434 -0.236  -0.236   -0.132   -0.132\n25 mdd1 ~~ mdd7  0.293  0.053   0.053    0.026    0.026\n26 mdd1 ~~ mdd8  5.046 -0.219  -0.219   -0.116   -0.116\n27 mdd1 ~~ mdd9  5.385  0.187   0.187    0.111    0.111\n28 mdd2 ~~ mdd3  1.435 -0.138  -0.138   -0.055   -0.055\n29 mdd2 ~~ mdd4 10.931 -0.402  -0.402   -0.158   -0.158\n30 mdd2 ~~ mdd5  5.835 -0.281  -0.281   -0.113   -0.113\n31 mdd2 ~~ mdd6  2.235 -0.169  -0.169   -0.074   -0.074\n32 mdd2 ~~ mdd7  0.394 -0.076  -0.076   -0.029   -0.029\n33 mdd2 ~~ mdd8  0.027 -0.019  -0.019   -0.008   -0.008\n34 mdd2 ~~ mdd9  2.070 -0.142  -0.142   -0.066   -0.066\n35 mdd3 ~~ mdd4 16.199  0.591   0.591    0.155    0.155\n36 mdd3 ~~ mdd5  3.298  0.258   0.258    0.069    0.069\n37 mdd3 ~~ mdd6  6.361  0.337   0.337    0.098    0.098\n38 mdd3 ~~ mdd7  0.499 -0.106  -0.106   -0.027   -0.027\n39 mdd3 ~~ mdd8  0.014 -0.017  -0.017   -0.005   -0.005\n40 mdd3 ~~ mdd9  2.860 -0.208  -0.208   -0.064   -0.064\n41 mdd4 ~~ mdd5 12.198  0.509   0.509    0.136    0.136\n42 mdd4 ~~ mdd6 17.435  0.573   0.573    0.165    0.165\n43 mdd4 ~~ mdd7  1.272 -0.174  -0.174   -0.043   -0.043\n44 mdd4 ~~ mdd8  0.975  0.143   0.143    0.039    0.039\n45 mdd4 ~~ mdd9  1.879 -0.173  -0.173   -0.053   -0.053\n46 mdd5 ~~ mdd6  7.502  0.364   0.364    0.107    0.107\n47 mdd5 ~~ mdd7  0.096  0.046   0.046    0.012    0.012\n48 mdd5 ~~ mdd8  4.217  0.288   0.288    0.080    0.080\n49 mdd5 ~~ mdd9  0.544 -0.090  -0.090   -0.028   -0.028\n50 mdd6 ~~ mdd7  2.046 -0.201  -0.201   -0.055   -0.055\n51 mdd6 ~~ mdd8  0.877  0.124   0.124    0.037    0.037\n52 mdd6 ~~ mdd9  2.479 -0.180  -0.180   -0.061   -0.061\n53 mdd7 ~~ mdd8  0.188  0.064   0.064    0.017    0.017\n54 mdd7 ~~ mdd9 13.527  0.474   0.474    0.139    0.139\n55 mdd8 ~~ mdd9  0.322  0.069   0.069    0.022    0.022\n\n\nNel modello che stiamo analizzando, l’indice di modifica (MI) più elevato si riferisce alla possibile modifica del parametro che governa la correlazione tra i residui degli indicatori mm1 e mm2. Nel modello model_mdd questa correlazione residua è impostata a zero, indicando l’assenza di una correlazione diretta tra questi residui. Tuttavia, l’indice di modifica suggerisce che se permettessimo a questa correlazione di essere stimata liberamente dal modello (anziché tenerla fissa a zero), si verificherebbe un miglioramento dell’adattamento del modello ai dati osservati. Questa osservazione è in linea con il modello alternativo che è stato proposto per questi dati proposto da {cite:t}brown2015confirmatory.\nIncorporare una correlazione residua tra mm1 e mm2 significa riconoscere che, oltre alla variazione spiegata dalle variabili latenti comuni, esiste una relazione unica tra questi due indicatori che non è catturata dal modello. Tale relazione potrebbe essere dovuta a fattori specifici relativi a questi indicatori o a una misurazione comune non prevista dal modello originale.\nÈ importante sottolineare che ogni modifica al modello basata sugli indici di modifica dovrebbe essere attentamente valutata per assicurarsi che sia supportata sia da giustificazioni teoriche che empiriche. Aggiungere correlazioni residue può migliorare l’adattamento del modello, ma dovrebbe essere fatto con cautela per evitare di creare un modello eccessivamente complesso che potrebbe non essere generalizzabile al di fuori del campione di dati specifico utilizzato.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html#considerazioni-conclusive",
    "href": "chapters/sem/04_mod_comp.html#considerazioni-conclusive",
    "title": "42  Confronto tra modelli",
    "section": "42.4 Considerazioni Conclusive",
    "text": "42.4 Considerazioni Conclusive\nNel campo dei modelli SEM, è pratica comune selezionare il modello più appropriato da un insieme di alternative, tutte calibrate sugli stessi dati. È frequente il confronto tra modelli gerarchicamente collegati, in cui il modello più restrittivo è incluso, o annidato, in quello meno restrittivo. In queste situazioni, il test di differenza del chi-quadrato viene impiegato per valutare se i modelli hanno un adattamento equivalente. Utilizzare questo test e le statistiche diagnostiche correlate, come gli indici di modifica, richiede un approccio guidato dalla teoria, non solo da criteri empirici. Un eccessivo affidamento su criteri puramente empirici, come la significatività statistica, può portare a una dipendenza eccessiva dal caso.\nPer confrontare modelli non annidati, il test di differenza del chi-quadrato non è idoneo, ma si possono adottare gli indici di adattamento predittivo, basati sulla teoria dell’informazione, per la loro valutazione. Quando si decide di mantenere un modello, è cruciale prendere in considerazione anche altri modelli alternativi potenzialmente equivalenti. È importante fornire motivazioni solide su perché il modello selezionato dal ricercatore sia da preferire rispetto a queste alternative equivalenti.\nQuesto approccio consente una comprensione più profonda e una scelta più informata del modello, assicurando che la selezione sia fondata su basi teoriche solide e non solamente su risultati statistici.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/05_cfa_mod_comp.html",
    "href": "chapters/sem/05_cfa_mod_comp.html",
    "title": "43  CFA: confronto tra modelli",
    "section": "",
    "text": "43.1 Modello congenerico\nmodel.congeneric &lt;- '\n  auditorymemory =~ x1 + x2 + x3\n  visualmemory   =~ x4 + x5 + x6\n'\nfit.congeneric &lt;- cfa(\n  model.congeneric, \n  sample.cov = covs, \n  sample.nobs = 200, \n  std.lv = TRUE\n)\nL’output si ottiene con:\nout = summary(\n  fit.congeneric, \n  fit.measures = TRUE, \n  standardized = TRUE, \n  rsquare = TRUE\n)\nprint(out)\n\nlavaan 0.6.17 ended normally after 21 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        13\n\n  Number of observations                           200\n\nModel Test User Model:\n                                                      \n  Test statistic                                 4.877\n  Degrees of freedom                                 8\n  P-value (Chi-square)                           0.771\n\nModel Test Baseline Model:\n\n  Test statistic                               719.515\n  Degrees of freedom                                15\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.008\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -2337.980\n  Loglikelihood unrestricted model (H1)      -2335.541\n                                                      \n  Akaike (AIC)                                4701.959\n  Bayesian (BIC)                              4744.837\n  Sample-size adjusted Bayesian (SABIC)       4703.652\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.057\n  P-value H_0: RMSEA &lt;= 0.050                    0.929\n  P-value H_0: RMSEA &gt;= 0.080                    0.010\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.012\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  auditorymemory =~                                                      \n    x1                 2.101    0.166   12.663    0.000    2.101    0.807\n    x2                 2.182    0.168   12.976    0.000    2.182    0.823\n    x3                 2.013    0.166   12.124    0.000    2.013    0.779\n  visualmemory =~                                                        \n    x4                 1.756    0.108   16.183    0.000    1.756    0.907\n    x5                 1.795    0.115   15.608    0.000    1.795    0.887\n    x6                 1.796    0.117   15.378    0.000    1.796    0.878\n\nCovariances:\n                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  auditorymemory ~~                                                      \n    visualmemory       0.382    0.070    5.463    0.000    0.382    0.382\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .x1                2.366    0.372    6.365    0.000    2.366    0.349\n   .x2                2.277    0.383    5.940    0.000    2.277    0.323\n   .x3                2.621    0.373    7.027    0.000    2.621    0.393\n   .x4                0.662    0.117    5.668    0.000    0.662    0.177\n   .x5                0.877    0.134    6.554    0.000    0.877    0.214\n   .x6                0.956    0.139    6.866    0.000    0.956    0.229\n    auditorymemory    1.000                               1.000    1.000\n    visualmemory      1.000                               1.000    1.000\n\nR-Square:\n                   Estimate\n    x1                0.651\n    x2                0.677\n    x3                0.607\n    x4                0.823\n    x5                0.786\n    x6                0.771\nIl diagramma di percorso del modello è il seguente.\nsemPaths(\n  fit.congeneric,\n  \"std\",\n  posCol = c(\"black\"),\n  edge.label.cex = 1.2,\n  sizeMan = 7\n)",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>CFA: confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/05_cfa_mod_comp.html#modello-tau-equivalente",
    "href": "chapters/sem/05_cfa_mod_comp.html#modello-tau-equivalente",
    "title": "43  CFA: confronto tra modelli",
    "section": "43.2 Modello tau-equivalente",
    "text": "43.2 Modello tau-equivalente\nSolo memoria auditiva:\n\nmodel.tau.a &lt;- '\n  auditorymemory =~ x1 + v1*x1 + v1*x2 + v1*x3\n  visualmemory   =~ x4 + x5 + x6\n'\n\n\nfit.tau.a &lt;- cfa(\n  model.tau.a, \n  sample.cov = covs, \n  sample.nobs = 200, \n  std.lv = TRUE\n)\n\nMemoria auditiva e visiva:\n\nmodel.tau.av &lt;- '\n  auditorymemory =~ NA*x1 + v1*x1 + v1*x2 + v1*x3\n  visualmemory   =~ NA*x4 + v2*x4 + v2*x5 + v2*x6\n'\n\n\nfit.tau.av &lt;- cfa(\n  model.tau.av, \n  sample.cov = covs, \n  sample.nobs = 200, \n  std.lv = TRUE\n)\n\n\nsemPaths(\n  fit.tau.av,\n  \"std\",\n  posCol = c(\"black\"),\n  edge.label.cex = 1.2,\n  sizeMan = 7\n)",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>CFA: confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/05_cfa_mod_comp.html#modello-parallelo",
    "href": "chapters/sem/05_cfa_mod_comp.html#modello-parallelo",
    "title": "43  CFA: confronto tra modelli",
    "section": "43.3 Modello parallelo",
    "text": "43.3 Modello parallelo\nSolo memoria auditiva:\n\nmodel.parallel.a &lt;- '\n  auditorymemory =~ x1 + v1*x1 + v1*x2 + v1*x3\n  visualmemory   =~ x4 + v2*x4 + v2*x5 + v2*x6\n  x1 ~~ v3 * x1\n  x2 ~~ v3 * x2\n  x3 ~~ v3 * x3\n'\n\n\nfit.parallel.a &lt;- cfa(\n  model.parallel.a, \n  sample.cov = covs, \n  sample.nobs = 200, \n  std.lv = TRUE\n)\n\nMemoria auditiva e visiva:\n\nmodel.parallel.av &lt;- '\n  auditorymemory =~ x1 + v1*x1 + v1*x2 + v1*x3\n  visualmemory   =~ x4 + v2*x4 + v2*x5 + v2*x6\n  x1 ~~ v3 * x1\n  x2 ~~ v3 * x2\n  x3 ~~ v3 * x3\n \n  x4 ~~ v4 * x4\n  x5 ~~ v4 * x5\n  x6 ~~ v4 * x6\n'\n\n\nfit.parallel.av &lt;- cfa(\n  model.parallel.av, \n  sample.cov = covs, \n  sample.nobs = 200, \n  std.lv = TRUE\n)\n\n\nsemPaths(\n  fit.parallel.av,\n  \"std\",\n  posCol = c(\"black\"),\n  edge.label.cex = 1.2,\n  sizeMan = 7\n)",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>CFA: confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/05_cfa_mod_comp.html#il-test-del-chi2",
    "href": "chapters/sem/05_cfa_mod_comp.html#il-test-del-chi2",
    "title": "43  CFA: confronto tra modelli",
    "section": "43.4 Il test del \\(\\chi^2\\)",
    "text": "43.4 Il test del \\(\\chi^2\\)\nIl confronto tra modelli nidificati procede attraverso il test \\(\\chi^2\\). Tale test si basa su una proprietà delle variabili casuali distribuite come \\(\\chi^2\\): la differenza tra due v.c. \\(X_1\\) e \\(X_2\\) che seguono la distribuzione \\(\\chi^2\\), rispettivamente con \\(\\nu_1\\) e \\(\\nu_2\\), con \\(\\nu_1 &gt; \\nu_2\\), è una variabile causale che segue la distribuzione \\(\\chi^2\\) con gradi di libertà pari a \\(\\nu_1 - \\nu_2\\).\nUn modello nidificato è un modello che impone dei vincoli sui parametri del modello di partenza. L’imposizione di vincoli sui parametri ha la conseguenza che vi sarà un numero minore di parametri da stimare. Il confronto tra i modelli si esegue valutando in maniera relativa la bontà di adattamento di ciascun modello per mezzo della statistica chi-quadrato. La statistica così calcolata avrà un numero di gradi di libertà uguale alla differenza tra i gradi di libertà dei due modelli.\nNel caso dell’esempio in dicussione, abbiamo\n\nout = anova(\n  fit.congeneric, \n  fit.tau.a, \n  fit.tau.av, \n  fit.parallel.a, \n  fit.parallel.av, \n  test = \"chisq\"\n)\nprint(out)\n\n\nChi-Squared Difference Test\n\n                Df    AIC    BIC  Chisq Chisq diff    RMSEA Df diff Pr(&gt;Chisq)\nfit.congeneric   8 4702.0 4744.8 4.8773                                       \nfit.tau.a       10 4698.7 4735.0 5.6597     0.7823 0.000000       2     0.6763\nfit.tau.av      12 4695.0 4724.6 5.8810     0.2213 0.000000       2     0.8952\nfit.parallel.a  14 4691.1 4714.1 5.9769     0.0959 0.000000       2     0.9532\nfit.parallel.av 16 4690.4 4706.9 9.2772     3.3003 0.057016       2     0.1920\n\n\nI test precedenti indicano come non vi sia una perdita di adattamento passando dal modello congenerico al modello più restrittivo (ovvero, il modello parallelo per entrambi i fattori). Per questi dati, dunque, può essere adottato il modello più semplice, cioè il modello parallelo.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>CFA: confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html",
    "href": "chapters/sem/06_refine_solution.html",
    "title": "44  La revisione del modello",
    "section": "",
    "text": "44.1 Stima del modello\nConsideriamo qui un modello SEM con una sola variabile latente identificata da un insieme di indicatori, ovvero un modello CFA. L’obiettivo della CFA è ottenere stime per i parametro del modello (vale a dire, saturazioni fattoriali, varianze e covarianze fattoriali, varianze residue ed eventualmente covarianze degli errori) che sono in grado di produrre una matrice di covarianza prevista (denotata da \\(\\boldsymbol{\\Sigma}\\)) la quale è il più possibile simile alla matrice di covarianze campionarie (denotata da \\(\\boldsymbol{S}\\)). Questo processo di stima è basato sulla minimizzazione di una funzione che descrive la differenza tra \\(\\boldsymbol{\\Sigma}\\) e \\(\\boldsymbol{S}\\). Il metodo di stima più utilizzato nella CFA (e, in generale, nei modelli SEM) è la massima verosimiglianza (ML).",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html#massima-verosimiglianza",
    "href": "chapters/sem/06_refine_solution.html#massima-verosimiglianza",
    "title": "44  La revisione del modello",
    "section": "44.2 Massima verosimiglianza",
    "text": "44.2 Massima verosimiglianza\nL’equazione fondamentale dell’analisi fattoriale è\n\\[\n\\boldsymbol y = \\boldsymbol \\Lambda  \\boldsymbol x  + \\boldsymbol z,\n\\]\ndove \\(\\boldsymbol{y}\\) è un vettore di \\(p\\) componenti (i punteggi osservati nel del test), \\(\\boldsymbol{x}\\) è un vettore di \\(k &lt; p\\) componenti (i punteggi fattoriali), \\(\\boldsymbol{\\Lambda}\\) è una \\(p \\cdot k\\) matrice (di saturazioni fattoriali), e \\(\\boldsymbol{z}\\) è un vettore di \\(p\\) componenti (la componenti dei punteggi del test non dovute all’effetto causale delle variabili comuni latenti). Per l’item \\(i\\)-esimo, in precedenza abbiamo scritto l’equazione precedente come\n\\[\ny_i = \\lambda_{i1} \\xi_1 + \\dots + \\lambda_{ik} \\xi_k + \\delta_i.\n\\]\nDalle assunzioni del modello fattoriale deriva che\n\\[\n\\boldsymbol{\\Sigma} = \\boldsymbol{\\Lambda}\\boldsymbol{\\Phi}\\boldsymbol{\\Lambda}^\\prime + \\Psi,\n\\]\ndove \\(\\boldsymbol{\\Phi}\\) è la matrice delle inter-correlazioni fattoriali.\nSi assume che il vettore casuale \\(\\boldsymbol{y}\\) abbia una distribuzione normale multivariata con matrice di covarianza \\(\\boldsymbol{\\Sigma}\\) e che da tale distribuzione sia stato estratto un campione casuale di \\(n\\) osservazioni \\(y_l, y_2, \\dots, y_n\\). Il logaritmo della funzione di verosimiglianza per il campione è dato da\n\\[\n\\log L = \\frac{1}{2}n [\\log | \\boldsymbol{\\Sigma}| + tr(\\boldsymbol{\\boldsymbol{S} \\Sigma}^{-1})].\n\\]\nL’equazione precedente viene vista come funzione di \\(\\Lambda\\) e \\(\\Psi\\). Anziché massimizzare \\(\\log L\\), è equivalente e più conveniente minimizzare\n\\[\nF_{k}(\\Lambda, \\Psi) = \\log |\\boldsymbol{\\Sigma}| + tr[\\boldsymbol{S}\\boldsymbol{\\Sigma}^{-1}]  - \\log|\\boldsymbol{S}| – p,\n\\]\ndove \\(|\\boldsymbol{S}|\\) è il determinante della matrice di covarianza tra le variabili osservate, \\(|\\boldsymbol{\\Sigma}|\\) è il determinante della matrice di covarianza prevista e \\(p\\) è il numero di indicatori.\nL’obiettivo della stima di massima verosimiglianza della CFA è trovare le stime dei parametri che rendono più verosimili i dati osservati (o, al contrario, massimizzano la verosimiglianza dei parametri dati i dati). Le stime dei parametri in un modello CFA si ottengono con una procedura iterativa. Cioè, l’algoritmo inizia con una serie iniziale di stime dei parametri (denominate valori iniziali o stime iniziali, che possono essere generate automaticamente dal software o specificate dall’utente) e raffina ripetutamente queste stime nel tentativo di minimizzare la differenza tra \\(\\boldsymbol{\\Sigma}\\) e \\(\\boldsymbol{S}\\). Il programma effettua controlli interni per valutare i suoi progressi nell’ottenere stime dei parametri che al meglio riproducono \\(\\boldsymbol{S}\\). Si raggiunge la convergenza quando l’algoritmo produce una serie di stime dei parametri che non possono essere ulteriormente migliorate per ridurre la differenza tra \\(\\boldsymbol{\\Sigma}\\) e \\(\\boldsymbol{S}\\).",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html#identificabilità-del-modello",
    "href": "chapters/sem/06_refine_solution.html#identificabilità-del-modello",
    "title": "44  La revisione del modello",
    "section": "44.3 Identificabilità del modello",
    "text": "44.3 Identificabilità del modello\nUn modello CFA deve essere formulato in modo tale da garantire la risolvibilità matematica dello stesso, ovvero deve essere tale da consentire una stima univoca dei parametri del modello. Detto in altre parole, la specificazione del modello ne deve garantire l’dentificabilità.\nIl problema dell’identificazione richiede, innanzitutto, di chiarire il concetto di gradi di libertà (degrees of freedom). Nel presente contesto, per gradi di libertà (\\(dof\\)) intendiamo\n\\[\ndof = \\# (\\text{unità di informazione}) - \\# (\\text{parametri da stimare}).\n\\]\nI dati che vengono analizzati da un modello CFA sono contenuti in una matrice di covarianza. Per una matrice di covarianza di ordine \\(p\\), il numero di unità di informazione è\n\\[\n\\frac{p (p+1)}{2}.\n\\]\nAffinché il modello sia identificabile, devono essere soddisfatte le seguenti condizioni.\n\nIndipendentemente dalla complessità del modello (ad es. modelli ad un fattore rispetto a più fattori), l’unità di misura delle variabili latenti deve essere specificata (di solito fissandola a un valore di 1);\nIndipendentemente dalla complessità del modello, il numero di unità di informazione (es. la matrice di covarianza degli indicatori) deve essere uguale o superiore al numero di parametri da stimare (es. saturazioni fattoriali, specificità, covarianze degli errori dell’indicatore, covarianze tra i fattori);\nNel caso di modelli ad un fattore è richiesto un minimo di tre indicatori. Quando vengono utilizzati tre indicatori, la soluzione a un fattore si dice “appena identificata” (just-identified); in tali condizioni non è possibile valutare la bontà dell’adattamento.\nNel caso di modelli a due o più fattori e due indicatori per costrutto latente, la soluzione è sovraidentificata, a condizione che ogni variabile latente sia correlata con almeno un’altra variabile latente e gli errori tra gli indicatori siano tra loro incorrelati. Tuttavia, poiché tali soluzioni sono suscettibili di scarsa identificazione empirica, viene raccomandato un minimo di tre indicatori per variabile latente.\n\nIn conclusione, una semplice e necessaria condizione per l’identificazione di un modello CFA è che vi siano più unità di informazione che parametri da stimare. Dunque, abbiamo che:\n\nse \\(dof &lt; 0\\), il modello non è identificato e, in questo caso, non è possibile stimare i parametri;\nse \\(dof = 0\\), il modello è appena identificato o “saturo”; in questo caso, la matrice di covarianza riprodotta coincide con la matrice di covarianza delle variabili osservate e, di conseguenza, non esiste un residuo attraverso cui valutare la bontà dell’adattamento del modello;\nse \\(dof &gt; 0\\), il modello è sovra-identificato ed esistono le condizioni per valutare la bontà dell’adattamento.\n\nLe considerazioni precedenti ci fanno capire perché non si può fare un’analisi fattoriale con solo due indicatori e un fattore; in tali circostanze, infatti, ci sono \\((2 \\cdot 3)/2 = 3\\) gradi di libertà, ma 4 parametri da stimare (due saturazioni fattoriali e due specificità). Il caso di tre item e un fattore definisce un modello “appena identificato”, ovvero, il caso in cui ci sono zero gradi di libertà. In tali circostanze è possibile stimare i parametri (ricordiamo il metodo dell’annullamento della tetrade), ma non è possibile un test di bontà dell’adattamento. Questo vuol dire, in pratica, che per un modello SEM ad un solo fattore comune latente è necessario disporre di almeno quattro indicatori.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html#un-esempio-concreto",
    "href": "chapters/sem/06_refine_solution.html#un-esempio-concreto",
    "title": "44  La revisione del modello",
    "section": "44.4 Un Esempio Concreto",
    "text": "44.4 Un Esempio Concreto\nNell’approfondire la tematica dei Modelli di Equazioni Strutturali, è utile considerare alcune problematiche comuni che possono emergere nella fase di adattamento del modello ai dati. Facendo riferimento agli esempi discussi da Brown (2015) nel contesto dell’analisi fattoriale confermativa (CFA), possiamo identificare diverse potenziali cause di inadeguato adattamento. Queste cause possono essere di natura sia teorica che tecnica e spesso richiedono un’attenta riflessione e analisi per essere risolte. Esaminiamo alcune delle questioni più rilevanti:\n\nNumero Errato di Fattori Comuni Latenti:\n\nUno degli errori più comuni è ipotizzare un numero di fattori latenti che non riflette adeguatamente la struttura sottostante dei dati. Un numero insufficiente di fattori può portare a un modello semplificato eccessivamente, mentre un numero eccessivo può causare sovra-aggiustamento e complessità non necessaria.\n\nItem che Saturano su Fattori Multipli:\n\nIn alcuni casi, un item può essere erroneamente ipotizzato per saturare su un singolo fattore comune, mentre in realtà ha relazioni significative con più fattori. Questo errore nella specificazione del modello può portare a stime imprecise e a un adattamento inadeguato.\n\nAssegnazione Errata degli Item ai Fattori:\n\nUn’altra possibile causa di inadeguato adattamento riguarda l’errata assegnazione di un item al fattore comune sbagliato. Tale errore può derivare da una comprensione insufficiente delle dimensioni teoriche che si stanno misurando o da una cattiva interpretazione dei dati empirici.\n\nCorrelazioni Residue Non Considerate:\n\nInfine, le correlazioni residue non incorporate nel modello possono giocare un ruolo significativo nell’adattamento del modello. Queste correlazioni possono indicare relazioni non catturate dai fattori comuni, suggerendo la necessità di rivedere l’ipotesi di base del modello o di aggiungere percorsi specifici per accomodare queste correlazioni.\n\n\nIn sintesi, l’adattamento del modello SEM ai dati è un processo complesso che richiede una profonda comprensione sia della teoria sottostante che della natura dei dati. Ogni volta che un modello non si adatta adeguatamente, è essenziale esaminare criticamente questi e altri potenziali fattori per identificare e correggere le cause alla base di tale inadeguatezza. Questo processo non solo migliora l’adattamento del modello, ma può anche fornire intuizioni preziose sulla struttura dei dati e sulla validità delle teorie sottostanti.\nBrown (2015) mostra come il ricercatore possa usare i Modification Indices per valutare le cause del mancato adattamento del modello ai dati. I Modification Indices sono una misura utilizzata per identificare le covariate tra le variabili del modello che potrebbero migliorare l’aderenza del modello ai dati. I modification indices indicano quale sarebbe il miglioramento nell’aderenza del modello, ad esempio, se venisse permessa la correlazione tra due variabili che attualmente non sono considerate correlate. Ciò consente di identificare le relazioni nascoste tra le variabili e può aiutare a migliorare la precisione e l’accuratezza del modello.\nTuttavia, è importante tenere presente che i modification indices da soli non dovrebbero essere usati per prendere decisioni definitive sulle modifiche del modello. Invece, dovrebbero essere considerati insieme ad altre informazioni, come la conoscenza teorica, l’esperienza e altre tecniche di analisi dei dati per determinare se una modifica del modello è giustificata e in che modo.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html#un-numero-di-fattori-troppo-piccolo",
    "href": "chapters/sem/06_refine_solution.html#un-numero-di-fattori-troppo-piccolo",
    "title": "44  La revisione del modello",
    "section": "44.5 Un numero di fattori troppo piccolo",
    "text": "44.5 Un numero di fattori troppo piccolo\nUna delle possibili fonti di mancanza di adattamento del modello può dipendere dal fatto che è stato ipotizzato un numero insufficiente di fattori latenti comuni. Brown (2015) discute il caso nel quale si confrontano gli indici di bontà di adattamento di un modello ad un solo fattore comune e un modello a due fattori comuni. L’esempio riguarda i dati già in precedenza discussi e relativi relativi a otto misure di personalità raccolte su un campione di 250 pazienti che hanno concluso un programma di psicoterapia. Le scale sono le seguenti:\n\nanxiety (N1),\nhostility (N2),\ndepression (N3),\nself-consciousness (N4),\nwarmth (E1),\ngregariousness (E2),\nassertiveness (E3),\npositive emotions (E4).\n\nLeggiamo i dati in \\(\\mathsf{R}\\).\n\nvarnames &lt;- c(\"N1\", \"N2\", \"N3\", \"N4\", \"E1\", \"E2\", \"E3\", \"E4\")\n\nsds &lt;- c(5.7,  5.6,  6.4,  5.7,  6.0,  6.2,  5.7,  5.6)\n\ncors &lt;- '\n 1.000\n 0.767  1.000 \n 0.731  0.709  1.000 \n 0.778  0.738  0.762  1.000 \n-0.351  -0.302  -0.356  -0.318  1.000 \n-0.316  -0.280  -0.300  -0.267  0.675  1.000 \n-0.296  -0.289  -0.297  -0.296  0.634  0.651  1.000 \n-0.282  -0.254  -0.292  -0.245  0.534  0.593  0.566  1.000'\n\npsychot_cor_mat &lt;- getCov(cors, names = varnames)\n\nn &lt;- 250\n\nSupponiamo di adattare ai dati il modello “sbagliato” che include un unico fattore comune. Svolgiamo qui l’analisi fattoriale esplorativa usando la funzione sperimentale efa() di lavaan.\n\n# 1-factor model\nf1 &lt;- '\n  efa(\"efa\")*f1 =~ N1 + N2 + N3 + N4 + E1 + E2 + E3 + E4\n'\n\nAdattiamo il modello ai dati.\n\nefa_f1 &lt;-\n  cfa(\n    model = f1,\n    sample.cov = psychot_cor_mat,\n    sample.nobs = 250,\n    rotation = \"oblimin\"\n  )\n\nConsideriamo ora un modello a due fattori.\n\nf2 &lt;- '\n  efa(\"efa\")*f1 +\n  efa(\"efa\")*f2 =~ N1 + N2 + N3 + N4 + E1 + E2 + E3 + E4\n'\n\nAdattiamo il modello ai dati.\n\nefa_f2 &lt;-\n  cfa(\n    model = f2,\n    sample.cov = psychot_cor_mat,\n    sample.nobs = 250,\n    rotation = \"oblimin\"\n  )\n\nEsaminiamo gli indici di bontà di adattamento.\n\n# define the fit measures\nfit_measures_robust &lt;- c(\"chisq\", \"df\", \"pvalue\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")\n\n# collect them for each model\nrbind(\n  fitmeasures(efa_f1, fit_measures_robust),\n  fitmeasures(efa_f2, fit_measures_robust)\n) %&gt;%\n  # wrangle\n  data.frame() %&gt;%\n  mutate(\n    chisq = round(chisq, digits = 0),\n    df = as.integer(df),\n    pvalue = ifelse(pvalue == 0, \"&lt; .001\", pvalue)\n  ) %&gt;%\n  mutate_at(vars(cfi:srmr), ~ round(., digits = 3))\n\n\nA data.frame: 2 x 7\n\n\nchisq\ndf\npvalue\ncfi\ntli\nrmsea\nsrmr\n\n\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n375\n20\n&lt; .001 \n0.71\n0.594\n0.267\n0.187\n\n\n10\n13\n0.709310449320098\n1.00\n1.006\n0.000\n0.010\n\n\n\n\n\n\nprint(effectsize::interpret(efa_f1))\n\n    Name     Value Threshold Interpretation\n1    GFI 0.6713421      0.95           poor\n2   AGFI 0.4084158      0.90           poor\n3    NFI 0.7006460      0.90           poor\n4   NNFI 0.5941736      0.90           poor\n5    CFI 0.7101240      0.90           poor\n6  RMSEA 0.2665811      0.05           poor\n7   SRMR 0.1873289      0.08           poor\n8    RFI 0.5809044      0.90           poor\n9   PNFI 0.5004614      0.50   satisfactory\n10   IFI 0.7120036      0.90           poor\n\n\n\nprint(effectsize::interpret(efa_f2))\n\n    Name       Value Threshold Interpretation\n1    GFI 0.990554109      0.95   satisfactory\n2   AGFI 0.973842148      0.90   satisfactory\n3    NFI 0.992174918      0.90   satisfactory\n4   NNFI 1.005603388      0.90   satisfactory\n5    CFI 1.000000000      0.90   satisfactory\n6  RMSEA 0.000000000      0.05   satisfactory\n7   SRMR 0.009907613      0.08   satisfactory\n8    RFI 0.983145977      0.90   satisfactory\n9   PNFI 0.460652640      0.50           poor\n10   IFI 1.002570123      0.90   satisfactory\n\n\nI risultati mostrano come, in un modello EFA, una soluzione a due fattori produca un adattamento adeguato, mentre ciò non si verifica con un modello ad un solo fattore.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html#specificazione-errata-delle-relazioni-tra-indicatori-e-fattori-latenti",
    "href": "chapters/sem/06_refine_solution.html#specificazione-errata-delle-relazioni-tra-indicatori-e-fattori-latenti",
    "title": "44  La revisione del modello",
    "section": "44.6 Specificazione errata delle relazioni tra indicatori e fattori latenti",
    "text": "44.6 Specificazione errata delle relazioni tra indicatori e fattori latenti\nUn’altra potenziale fonte di errata specificazione del modello CFA è una designazione errata delle relazioni tra indicatori e fattori latenti.\nIn questo esempio, un ricercatore ha sviluppato un questionario di 12 item (gli item sono valutati su scale da 0 a 8) progettato per valutare le motivazioni dei giovani adulti a consumare bevande alcoliche (Cooper, 1994). La misura aveva lo scopo di valutare tre aspetti di questo costrutto (4 item ciascuno): (1) motivazioni di coping (item 1–4), (2) motivazioni sociali (item 5–8) e (3) motivazioni di miglioramento (item 9 –12). I dati sono i seguenti.\n\nsds &lt;- c(2.06, 1.52, 1.92, 1.41, 1.73, 1.77, 2.49, 2.27, 2.68, 1.75, 2.57, 2.66)\n\ncors &lt;- '\n  1.000 \n  0.300  1.000 \n  0.229  0.261  1.000 \n  0.411  0.406  0.429  1.000 \n  0.172  0.252  0.218  0.481  1.000 \n  0.214  0.268  0.267  0.579  0.484  1.000 \n  0.200  0.214  0.241  0.543  0.426  0.492  1.000 \n  0.185  0.230  0.185  0.545  0.463  0.548  0.522  1.000 \n  0.134  0.146  0.108  0.186  0.122  0.131  0.108  0.151  1.000 \n  0.134  0.099  0.061  0.223  0.133  0.188  0.105  0.170  0.448  1.000 \n  0.160  0.131  0.158  0.161  0.044  0.124  0.066  0.061  0.370  0.350  1.000 \n  0.087  0.088  0.101  0.198  0.077  0.177  0.128  0.112  0.356  0.359  0.507  1.000'\n\ncovs &lt;- getCov(cors, sds = sds, names = paste(\"x\", 1:12, sep = \"\"))\n\nIniziamo con un modello che ipotizza tre fattori comuni latenti correlati, coerentemente con la motivazione che stava alla base della costruzione dello strumento.\n\nmodel1 &lt;- '\n  copingm  =~ x1 + x2 + x3 + x4\n  socialm  =~ x5 + x6 + x7 + x8\n  enhancem =~ x9 + x10 + x11 + x12\n'\n\nAdattiamo il modello ai dati.\n\nfit1 &lt;- cfa(\n  model1, \n  sample.cov = covs, \n  sample.nobs = 500, \n  mimic = \"mplus\"\n)\n\nWarning message:\n\"lavaan-&gt;lav_lavaan_step05_samplestats():  \n   sample.mean= argument is missing, but model contains mean/intercept \n   parameters.\"\n\n\nEsaminando le misure di adattamento potremmo concludere che il modello è adeguato.\n\nprint(effectsize::interpret(fit1))\n\n    Name      Value Threshold Interpretation\n1    GFI 0.97009178      0.95   satisfactory\n2   AGFI 0.94722078      0.90   satisfactory\n3    NFI 0.94785001      0.90   satisfactory\n4   NNFI 0.97102541      0.90   satisfactory\n5    CFI 0.97761054      0.90   satisfactory\n6  RMSEA 0.03745791      0.05   satisfactory\n7   SRMR 0.03438699      0.08   satisfactory\n8    RFI 0.93251177      0.90   satisfactory\n9   PNFI 0.73242955      0.50   satisfactory\n10   IFI 0.97781875      0.90   satisfactory\n\n\nTuttavia, un esame più attento mette in evidenza un comportamento anomalo dell’item x4 e alcune caratteristiche anomale del modello in generale.\n\nprint(standardizedSolution(fit1))\n\n        lhs op      rhs est.std    se      z pvalue ci.lower ci.upper\n1   copingm =~       x1   0.432 0.039 11.030   0.00    0.355    0.508\n2   copingm =~       x2   0.436 0.039 11.174   0.00    0.359    0.512\n3   copingm =~       x3   0.451 0.038 11.730   0.00    0.376    0.527\n4   copingm =~       x4   0.953 0.024 38.967   0.00    0.905    1.001\n5   socialm =~       x5   0.633 0.032 20.064   0.00    0.571    0.695\n6   socialm =~       x6   0.748 0.025 29.363   0.00    0.698    0.798\n7   socialm =~       x7   0.690 0.029 24.154   0.00    0.634    0.746\n8   socialm =~       x8   0.729 0.026 27.519   0.00    0.677    0.781\n9  enhancem =~       x9   0.602 0.039 15.581   0.00    0.526    0.678\n10 enhancem =~      x10   0.597 0.039 15.397   0.00    0.521    0.673\n11 enhancem =~      x11   0.661 0.037 17.982   0.00    0.589    0.733\n12 enhancem =~      x12   0.665 0.037 18.167   0.00    0.593    0.737\n13       x1 ~~       x1   0.814 0.034 24.085   0.00    0.747    0.880\n14       x2 ~~       x2   0.810 0.034 23.837   0.00    0.744    0.877\n15       x3 ~~       x3   0.796 0.035 22.938   0.00    0.728    0.864\n16       x4 ~~       x4   0.091 0.047  1.959   0.05    0.000    0.183\n17       x5 ~~       x5   0.599 0.040 14.985   0.00    0.521    0.677\n18       x6 ~~       x6   0.441 0.038 11.573   0.00    0.366    0.515\n19       x7 ~~       x7   0.524 0.039 13.293   0.00    0.447    0.601\n20       x8 ~~       x8   0.469 0.039 12.151   0.00    0.393    0.545\n21       x9 ~~       x9   0.638 0.047 13.707   0.00    0.546    0.729\n22      x10 ~~      x10   0.643 0.046 13.875   0.00    0.552    0.734\n23      x11 ~~      x11   0.563 0.049 11.605   0.00    0.468    0.659\n24      x12 ~~      x12   0.558 0.049 11.447   0.00    0.462    0.653\n25  copingm ~~  copingm   1.000 0.000     NA     NA    1.000    1.000\n26  socialm ~~  socialm   1.000 0.000     NA     NA    1.000    1.000\n27 enhancem ~~ enhancem   1.000 0.000     NA     NA    1.000    1.000\n28  copingm ~~  socialm   0.799 0.031 26.150   0.00    0.739    0.859\n29  copingm ~~ enhancem   0.322 0.051  6.336   0.00    0.222    0.422\n30  socialm ~~ enhancem   0.268 0.056  4.817   0.00    0.159    0.377\n31       x1 ~1            0.000 0.045  0.000   1.00   -0.088    0.088\n32       x2 ~1            0.000 0.045  0.000   1.00   -0.088    0.088\n33       x3 ~1            0.000 0.045  0.000   1.00   -0.088    0.088\n34       x4 ~1            0.000 0.045  0.000   1.00   -0.088    0.088\n35       x5 ~1            0.000 0.045  0.000   1.00   -0.088    0.088\n36       x6 ~1            0.000 0.045  0.000   1.00   -0.088    0.088\n37       x7 ~1            0.000 0.045  0.000   1.00   -0.088    0.088\n38       x8 ~1            0.000 0.045  0.000   1.00   -0.088    0.088\n39       x9 ~1            0.000 0.045  0.000   1.00   -0.088    0.088\n40      x10 ~1            0.000 0.045  0.000   1.00   -0.088    0.088\n41      x11 ~1            0.000 0.045  0.000   1.00   -0.088    0.088\n42      x12 ~1            0.000 0.045  0.000   1.00   -0.088    0.088\n43  copingm ~1            0.000 0.000     NA     NA    0.000    0.000\n44  socialm ~1            0.000 0.000     NA     NA    0.000    0.000\n45 enhancem ~1            0.000 0.000     NA     NA    0.000    0.000\n\n\nIn particolare, l’item x4 mostra una saturazione molto forte sul fattore Motivi di coping (.955) ed emerge una correlazione molto alta tra i fattori Motivi di coping e Motivi sociali (.798).\n{cite:t}brown2015confirmatory suggerisce di esaminare i Modification Indices. Tale esame mostra che il MI associato a x4 è molto alto, 18.916.\n\nprint(modindices(fit1))\n\n         lhs op rhs     mi    epc sepc.lv sepc.all sepc.nox\n46   copingm =~  x5  0.030 -0.030  -0.027   -0.015   -0.015\n47   copingm =~  x6  0.484  0.127   0.113    0.064    0.064\n48   copingm =~  x7  0.780  0.220   0.196    0.079    0.079\n49   copingm =~  x8  1.962 -0.323  -0.287   -0.127   -0.127\n50   copingm =~  x9  0.101  0.044   0.039    0.015    0.015\n51   copingm =~ x10  2.016  0.129   0.114    0.065    0.065\n52   copingm =~ x11  1.870 -0.181  -0.161   -0.063   -0.063\n53   copingm =~ x12  0.040 -0.027  -0.024   -0.009   -0.009\n54   socialm =~  x1  6.927 -0.520  -0.569   -0.277   -0.277\n55   socialm =~  x2  0.052 -0.033  -0.036   -0.024   -0.024\n56   socialm =~  x3  2.058 -0.267  -0.292   -0.152   -0.152\n57   socialm =~  x4 18.916  1.300   1.423    1.010    1.010\n58   socialm =~  x9  0.338  0.067   0.073    0.027    0.027\n59   socialm =~ x10  2.884  0.128   0.140    0.080    0.080\n60   socialm =~ x11  4.357 -0.229  -0.251   -0.098   -0.098\n61   socialm =~ x12  0.001  0.004   0.004    0.002    0.002\n62  enhancem =~  x1  1.954  0.093   0.149    0.072    0.072\n63  enhancem =~  x2  0.863  0.045   0.073    0.048    0.048\n64  enhancem =~  x3  0.380  0.038   0.061    0.032    0.032\n65  enhancem =~  x4  3.102 -0.104  -0.168   -0.119   -0.119\n66  enhancem =~  x5  0.596 -0.039  -0.063   -0.036   -0.036\n67  enhancem =~  x6  2.495  0.078   0.125    0.071    0.071\n68  enhancem =~  x7  0.539 -0.052  -0.084   -0.034   -0.034\n69  enhancem =~  x8  0.093 -0.019  -0.031   -0.014   -0.014\n70        x1 ~~  x2 10.299  0.379   0.379    0.149    0.149\n71        x1 ~~  x3  0.986  0.147   0.147    0.046    0.046\n72        x1 ~~  x4  0.016 -0.015  -0.015   -0.019   -0.019\n73        x1 ~~  x5  0.452 -0.080  -0.080   -0.032   -0.032\n74        x1 ~~  x6  0.484 -0.078  -0.078   -0.036   -0.036\n75        x1 ~~  x7  0.290 -0.089  -0.089   -0.027   -0.027\n76        x1 ~~  x8  1.535 -0.181  -0.181   -0.063   -0.063\n77        x1 ~~  x9  0.468  0.133   0.133    0.034    0.034\n78        x1 ~~ x10  0.067  0.033   0.033    0.013    0.013\n79        x1 ~~ x11  4.030  0.364   0.364    0.102    0.102\n80        x1 ~~ x12  1.504 -0.229  -0.229   -0.062   -0.062\n81        x2 ~~  x3  3.508  0.205   0.205    0.088    0.088\n82        x2 ~~  x4  6.780 -0.229  -0.229   -0.393   -0.393\n83        x2 ~~  x5  1.449  0.106   0.106    0.058    0.058\n84        x2 ~~  x6  0.102  0.026   0.026    0.016    0.016\n85        x2 ~~  x7  1.144 -0.130  -0.130   -0.053   -0.053\n86        x2 ~~  x8  0.366 -0.065  -0.065   -0.031   -0.031\n87        x2 ~~  x9  1.877  0.196   0.196    0.067    0.067\n88        x2 ~~ x10  0.434 -0.062  -0.062   -0.032   -0.032\n89        x2 ~~ x11  1.599  0.169   0.169    0.064    0.064\n90        x2 ~~ x12  0.726 -0.117  -0.117   -0.043   -0.043\n91        x3 ~~  x4  0.107 -0.037  -0.037   -0.051   -0.051\n92        x3 ~~  x5  0.024  0.017   0.017    0.008    0.008\n93        x3 ~~  x6  0.211  0.048   0.048    0.024    0.024\n94        x3 ~~  x7  0.009  0.015   0.015    0.005    0.005\n95        x3 ~~  x8  5.281 -0.310  -0.310   -0.117   -0.117\n96        x3 ~~  x9  0.031  0.031   0.031    0.009    0.009\n97        x3 ~~ x10  3.545 -0.221  -0.221   -0.092   -0.092\n98        x3 ~~ x11  5.967  0.408   0.408    0.124    0.124\n99        x3 ~~ x12  0.055 -0.040  -0.040   -0.012   -0.012\n100       x4 ~~  x5  0.063 -0.016  -0.016   -0.028   -0.028\n101       x4 ~~  x6  0.052  0.015   0.015    0.029    0.029\n102       x4 ~~  x7  2.114  0.131   0.131    0.170    0.170\n103       x4 ~~  x8  0.208  0.037   0.037    0.057    0.057\n104       x4 ~~  x9  0.887 -0.091  -0.091   -0.100   -0.100\n105       x4 ~~ x10  1.063  0.065   0.065    0.109    0.109\n106       x4 ~~ x11  2.637 -0.149  -0.149   -0.181   -0.181\n107       x4 ~~ x12  0.169  0.039   0.039    0.046    0.046\n108       x5 ~~  x6  0.370  0.057   0.057    0.036    0.036\n109       x5 ~~  x7  0.292 -0.072  -0.072   -0.030   -0.030\n110       x5 ~~  x8  0.007  0.010   0.010    0.005    0.005\n111       x5 ~~  x9  0.822  0.133   0.133    0.047    0.047\n112       x5 ~~ x10  0.339  0.056   0.056    0.030    0.030\n113       x5 ~~ x11  1.126 -0.145  -0.145   -0.056   -0.056\n114       x5 ~~ x12  1.143 -0.151  -0.151   -0.057   -0.057\n115       x6 ~~  x7  2.528 -0.215  -0.215   -0.101   -0.101\n116       x6 ~~  x8  0.053  0.029   0.029    0.016    0.016\n117       x6 ~~  x9  1.056 -0.141  -0.141   -0.056   -0.056\n118       x6 ~~ x10  0.598  0.069   0.069    0.042    0.042\n119       x6 ~~ x11  0.248  0.064   0.064    0.028    0.028\n120       x6 ~~ x12  1.667  0.170   0.170    0.073    0.073\n121       x7 ~~  x8  1.431  0.206   0.206    0.074    0.074\n122       x7 ~~  x9  0.032 -0.036  -0.036   -0.009   -0.009\n123       x7 ~~ x10  1.521 -0.163  -0.163   -0.065   -0.065\n124       x7 ~~ x11  0.263 -0.097  -0.097   -0.028   -0.028\n125       x7 ~~ x12  0.637  0.156   0.156    0.044    0.044\n126       x8 ~~  x9  1.621  0.227   0.227    0.068    0.068\n127       x8 ~~ x10  1.311  0.134   0.134    0.061    0.061\n128       x8 ~~ x11  2.144 -0.244  -0.244   -0.081   -0.081\n129       x8 ~~ x12  0.591 -0.132  -0.132   -0.043   -0.043\n130       x9 ~~ x10 19.846  0.862   0.862    0.288    0.288\n131       x9 ~~ x11  2.908 -0.518  -0.518   -0.126   -0.126\n132       x9 ~~ x12  7.696 -0.876  -0.876   -0.207   -0.207\n133      x10 ~~ x11  7.331 -0.534  -0.534   -0.197   -0.197\n134      x10 ~~ x12  5.572 -0.484  -0.484   -0.174   -0.174\n135      x11 ~~ x12 26.947  1.711   1.711    0.447    0.447\n\n\nLe considerazioni precedenti, dunque, suggeriscono che il modello potrebbe non avere descritto in maniera adeguata le relazioni tra x4 e i fattori comuni latenti. In base a considerazioni teoriche, supponiamo che abbia senso pensare che x4 saturi non solo sul fattore Motivi di coping ma anche sul fattore di Motivi Sociali. Specifichiamo dunque un nuovo modello nel modo seguente.\n\nmodel2 &lt;- '\n  copingm  =~ x1 + x2 + x3 + x4\n  socialm  =~ x4 + x5 + x6 + x7 + x8\n  enhancem =~ x9 + x10 + x11 + x12\n'\n\nAdattiamo il modello.\n\nfit2 &lt;- cfa(\n  model2, \n  sample.cov = covs, \n  sample.nobs = 500, \n  mimic = \"mplus\"\n)\n\nWarning message:\n\"lavaan-&gt;lav_lavaan_step05_samplestats():  \n   sample.mean= argument is missing, but model contains mean/intercept \n   parameters.\"\n\n\nEsaminiamo gli indici di bontà di adattamento.\n\nprint(effectsize::interpret(fit2))\n\n    Name      Value Threshold Interpretation\n1    GFI 0.97684139      0.95   satisfactory\n2   AGFI 0.95831451      0.90   satisfactory\n3    NFI 0.95826773      0.90   satisfactory\n4   NNFI 0.98393923      0.90   satisfactory\n5    CFI 0.98783275      0.90   satisfactory\n6  RMSEA 0.02788804      0.05   satisfactory\n7   SRMR 0.02887855      0.08   satisfactory\n8    RFI 0.94491340      0.90   satisfactory\n9   PNFI 0.72596040      0.50   satisfactory\n10   IFI 0.98795337      0.90   satisfactory\n\n\nLa bontà di adattamento è migliorata.\nEsaminiamo la soluzione standardizzata. Vediamo ora che sono scomparse le due anomalie trovate in precedenza.\n\nprint(standardizedSolution(fit2))\n\n        lhs op      rhs est.std    se      z pvalue ci.lower ci.upper\n1   copingm =~       x1   0.514 0.043 12.034      0    0.430    0.597\n2   copingm =~       x2   0.515 0.043 12.072      0    0.431    0.599\n3   copingm =~       x3   0.516 0.043 12.106      0    0.432    0.600\n4   copingm =~       x4   0.538 0.062  8.660      0    0.416    0.660\n5   socialm =~       x4   0.439 0.061  7.204      0    0.320    0.558\n6   socialm =~       x5   0.632 0.032 19.995      0    0.570    0.694\n7   socialm =~       x6   0.746 0.025 29.279      0    0.696    0.796\n8   socialm =~       x7   0.691 0.028 24.235      0    0.635    0.746\n9   socialm =~       x8   0.731 0.026 27.762      0    0.679    0.782\n10 enhancem =~       x9   0.603 0.039 15.625      0    0.527    0.678\n11 enhancem =~      x10   0.595 0.039 15.308      0    0.519    0.671\n12 enhancem =~      x11   0.665 0.037 18.188      0    0.593    0.737\n13 enhancem =~      x12   0.663 0.037 18.103      0    0.591    0.735\n14       x1 ~~       x1   0.736 0.044 16.786      0    0.650    0.822\n15       x2 ~~       x2   0.735 0.044 16.729      0    0.649    0.821\n16       x3 ~~       x3   0.734 0.044 16.678      0    0.647    0.820\n17       x4 ~~       x4   0.230 0.037  6.292      0    0.158    0.301\n18       x5 ~~       x5   0.601 0.040 15.043      0    0.522    0.679\n19       x6 ~~       x6   0.443 0.038 11.634      0    0.368    0.517\n20       x7 ~~       x7   0.523 0.039 13.292      0    0.446    0.600\n21       x8 ~~       x8   0.466 0.038 12.106      0    0.390    0.541\n22       x9 ~~       x9   0.637 0.046 13.701      0    0.546    0.728\n23      x10 ~~      x10   0.646 0.046 13.990      0    0.556    0.737\n24      x11 ~~      x11   0.558 0.049 11.474      0    0.463    0.653\n25      x12 ~~      x12   0.561 0.049 11.546      0    0.465    0.656\n26  copingm ~~  copingm   1.000 0.000     NA     NA    1.000    1.000\n27  socialm ~~  socialm   1.000 0.000     NA     NA    1.000    1.000\n28 enhancem ~~ enhancem   1.000 0.000     NA     NA    1.000    1.000\n29  copingm ~~  socialm   0.610 0.057 10.744      0    0.498    0.721\n30  copingm ~~ enhancem   0.350 0.059  5.964      0    0.235    0.465\n31  socialm ~~ enhancem   0.265 0.055  4.794      0    0.156    0.373\n32       x1 ~1            0.000 0.045  0.000      1   -0.088    0.088\n33       x2 ~1            0.000 0.045  0.000      1   -0.088    0.088\n34       x3 ~1            0.000 0.045  0.000      1   -0.088    0.088\n35       x4 ~1            0.000 0.045  0.000      1   -0.088    0.088\n36       x5 ~1            0.000 0.045  0.000      1   -0.088    0.088\n37       x6 ~1            0.000 0.045  0.000      1   -0.088    0.088\n38       x7 ~1            0.000 0.045  0.000      1   -0.088    0.088\n39       x8 ~1            0.000 0.045  0.000      1   -0.088    0.088\n40       x9 ~1            0.000 0.045  0.000      1   -0.088    0.088\n41      x10 ~1            0.000 0.045  0.000      1   -0.088    0.088\n42      x11 ~1            0.000 0.045  0.000      1   -0.088    0.088\n43      x12 ~1            0.000 0.045  0.000      1   -0.088    0.088\n44  copingm ~1            0.000 0.000     NA     NA    0.000    0.000\n45  socialm ~1            0.000 0.000     NA     NA    0.000    0.000\n46 enhancem ~1            0.000 0.000     NA     NA    0.000    0.000\n\n\nEsaminando i MI, notiamo che il modello potrebbe migliorare se introduciamo una correlazione tra le specificità x11 e x12.\n\nprint(modindices(fit2))\n\n         lhs op rhs     mi    epc sepc.lv sepc.all sepc.nox\n47   copingm =~  x5  0.076  0.032   0.034    0.020    0.020\n48   copingm =~  x6  1.413  0.143   0.151    0.086    0.086\n49   copingm =~  x7  0.245  0.083   0.088    0.035    0.035\n50   copingm =~  x8  3.668 -0.295  -0.311   -0.137   -0.137\n51   copingm =~  x9  0.243  0.066   0.069    0.026    0.026\n52   copingm =~ x10  0.566  0.065   0.069    0.040    0.040\n53   copingm =~ x11  0.119 -0.044  -0.046   -0.018   -0.018\n54   copingm =~ x12  0.598 -0.102  -0.108   -0.041   -0.041\n55   socialm =~  x1  1.948 -0.396  -0.245   -0.119   -0.119\n56   socialm =~  x2  0.718  0.177   0.110    0.072    0.072\n57   socialm =~  x3  0.298  0.144   0.089    0.047    0.047\n58   socialm =~  x9  0.316  0.114   0.071    0.026    0.026\n59   socialm =~ x10  3.169  0.236   0.146    0.084    0.084\n60   socialm =~ x11  4.927 -0.430  -0.266   -0.104   -0.104\n61   socialm =~ x12  0.017  0.026   0.016    0.006    0.006\n62  enhancem =~  x1  0.314  0.040   0.064    0.031    0.031\n63  enhancem =~  x2  0.003  0.003   0.004    0.003    0.003\n64  enhancem =~  x3  0.037 -0.013  -0.020   -0.011   -0.011\n65  enhancem =~  x4  0.106 -0.013  -0.021   -0.015   -0.015\n66  enhancem =~  x5  0.464 -0.034  -0.055   -0.032   -0.032\n67  enhancem =~  x6  2.703  0.079   0.128    0.072    0.072\n68  enhancem =~  x7  0.467 -0.048  -0.077   -0.031   -0.031\n69  enhancem =~  x8  0.095 -0.019  -0.031   -0.014   -0.014\n70        x1 ~~  x2  1.966  0.187   0.187    0.081    0.081\n71        x1 ~~  x3  2.042 -0.241  -0.241   -0.083   -0.083\n72        x1 ~~  x4  0.775  0.098   0.098    0.082    0.082\n73        x1 ~~  x5  0.238 -0.058  -0.058   -0.024   -0.024\n74        x1 ~~  x6  0.187 -0.048  -0.048   -0.023   -0.023\n75        x1 ~~  x7  0.019 -0.022  -0.022   -0.007   -0.007\n76        x1 ~~  x8  0.366 -0.087  -0.087   -0.032   -0.032\n77        x1 ~~  x9  0.155  0.076   0.076    0.020    0.020\n78        x1 ~~ x10  0.104  0.041   0.041    0.016    0.016\n79        x1 ~~ x11  2.019  0.255   0.255    0.075    0.075\n80        x1 ~~ x12  1.911 -0.257  -0.257   -0.073   -0.073\n81        x2 ~~  x3  0.035 -0.023  -0.023   -0.011   -0.011\n82        x2 ~~  x4  3.029 -0.144  -0.144   -0.163   -0.163\n83        x2 ~~  x5  2.503  0.138   0.138    0.079    0.079\n84        x2 ~~  x6  0.509  0.058   0.058    0.038    0.038\n85        x2 ~~  x7  0.471 -0.082  -0.082   -0.035   -0.035\n86        x2 ~~  x8  0.015  0.013   0.013    0.006    0.006\n87        x2 ~~  x9  1.289  0.161   0.161    0.058    0.058\n88        x2 ~~ x10  0.467 -0.064  -0.064   -0.035   -0.035\n89        x2 ~~ x11  0.338  0.077   0.077    0.031    0.031\n90        x2 ~~ x12  0.970 -0.135  -0.135   -0.052   -0.052\n91        x3 ~~  x4  1.095  0.109   0.109    0.098    0.098\n92        x3 ~~  x5  0.169  0.045   0.045    0.021    0.021\n93        x3 ~~  x6  0.681  0.085   0.085    0.044    0.044\n94        x3 ~~  x7  0.315  0.085   0.085    0.029    0.029\n95        x3 ~~  x8  3.075 -0.235  -0.235   -0.092   -0.092\n96        x3 ~~  x9  0.022 -0.026  -0.026   -0.008   -0.008\n97        x3 ~~ x10  3.825 -0.230  -0.230   -0.100   -0.100\n98        x3 ~~ x11  3.498  0.313   0.313    0.099    0.099\n99        x3 ~~ x12  0.079 -0.049  -0.049   -0.015   -0.015\n100       x4 ~~  x5  0.337 -0.037  -0.037   -0.041   -0.041\n101       x4 ~~  x6  0.033 -0.012  -0.012   -0.015   -0.015\n102       x4 ~~  x7  1.053  0.094   0.094    0.077    0.077\n103       x4 ~~  x8  0.071 -0.022  -0.022   -0.021   -0.021\n104       x4 ~~  x9  0.541 -0.070  -0.070   -0.048   -0.048\n105       x4 ~~ x10  1.128  0.066   0.066    0.070    0.070\n106       x4 ~~ x11  1.313 -0.102  -0.102   -0.079   -0.079\n107       x4 ~~ x12  0.322  0.052   0.052    0.039    0.039\n108       x5 ~~  x6  0.504  0.066   0.066    0.042    0.042\n109       x5 ~~  x7  0.262 -0.068  -0.068   -0.028   -0.028\n110       x5 ~~  x8  0.004  0.008   0.008    0.004    0.004\n111       x5 ~~  x9  0.850  0.135   0.135    0.047    0.047\n112       x5 ~~ x10  0.288  0.052   0.052    0.027    0.027\n113       x5 ~~ x11  1.019 -0.138  -0.138   -0.054   -0.054\n114       x5 ~~ x12  1.224 -0.157  -0.157   -0.059   -0.059\n115       x6 ~~  x7  2.404 -0.209  -0.209   -0.099   -0.099\n116       x6 ~~  x8  0.034  0.023   0.023    0.012    0.012\n117       x6 ~~  x9  0.978 -0.135  -0.135   -0.054   -0.054\n118       x6 ~~ x10  0.524  0.065   0.065    0.039    0.039\n119       x6 ~~ x11  0.341  0.074   0.074    0.033    0.033\n120       x6 ~~ x12  1.520  0.163   0.163    0.069    0.069\n121       x7 ~~  x8  1.171  0.186   0.186    0.067    0.067\n122       x7 ~~  x9  0.020 -0.028  -0.028   -0.007   -0.007\n123       x7 ~~ x10  1.593 -0.167  -0.167   -0.066   -0.066\n124       x7 ~~ x11  0.175 -0.079  -0.079   -0.023   -0.023\n125       x7 ~~ x12  0.586  0.149   0.149    0.042    0.042\n126       x8 ~~  x9  1.808  0.239   0.239    0.072    0.072\n127       x8 ~~ x10  1.267  0.131   0.131    0.060    0.060\n128       x8 ~~ x11  1.791 -0.222  -0.222   -0.075   -0.075\n129       x8 ~~ x12  0.595 -0.132  -0.132   -0.043   -0.043\n130       x9 ~~ x10 20.103  0.864   0.864    0.288    0.288\n131       x9 ~~ x11  3.658 -0.582  -0.582   -0.142   -0.142\n132       x9 ~~ x12  7.229 -0.845  -0.845   -0.199   -0.199\n133      x10 ~~ x11  7.617 -0.543  -0.543   -0.201   -0.201\n134      x10 ~~ x12  4.512 -0.431  -0.431   -0.154   -0.154\n135      x11 ~~ x12 26.071  1.680   1.680    0.440    0.440\n\n\nIl nuovo modello diventa dunque il seguente.\n\nmodel3 &lt;- '\n  copingm  =~ x1 + x2 + x3 + x4\n  socialm  =~ x4 + x5 + x6 + x7 + x8\n  enhancem =~ x9 + x10 + x11 + x12\n  x11 ~~ x12\n'\n\nAdattiamo il modello.\n\nfit3 &lt;- cfa(\n  model3, \n  sample.cov = covs, \n  sample.nobs = 500, \n  mimic = \"mplus\"\n)\n\nWarning message:\n\"lavaan-&gt;lav_lavaan_step05_samplestats():  \n   sample.mean= argument is missing, but model contains mean/intercept \n   parameters.\"\n\n\nUn test basato sul rapporto di verosimiglianze conferma che il miglioramento di adattamento è sostanziale.\n\nprint(lavTestLRT(fit2, fit3))\n\n\nChi-Squared Difference Test\n\n     Df   AIC   BIC  Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)    \nfit3 49 23934 24107 44.955                                          \nfit2 50 23957 24125 69.444     24.488 0.21674       1  7.477e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nEsaminiamo gli indici di bontà di adattamento.\n\nprint(summary(fit3, fit.measures = TRUE))\n\nlavaan 0.6-18 ended normally after 61 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        41\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                                44.955\n  Degrees of freedom                                49\n  P-value (Chi-square)                           0.638\n\nModel Test Baseline Model:\n\n  Test statistic                              1664.026\n  Degrees of freedom                                66\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.003\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -11926.170\n  Loglikelihood unrestricted model (H1)     -11903.692\n                                                      \n  Akaike (AIC)                               23934.339\n  Bayesian (BIC)                             24107.138\n  Sample-size adjusted Bayesian (SABIC)      23977.002\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.025\n  P-value H_0: RMSEA &lt;= 0.050                    1.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.023\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  copingm =~                                          \n    x1                1.000                           \n    x2                0.740    0.094    7.909    0.000\n    x3                0.933    0.118    7.903    0.000\n    x4                0.719    0.118    6.070    0.000\n  socialm =~                                          \n    x4                1.000                           \n    x5                1.771    0.273    6.485    0.000\n    x6                2.141    0.319    6.703    0.000\n    x7                2.784    0.421    6.611    0.000\n    x8                2.689    0.402    6.681    0.000\n  enhancem =~                                         \n    x9                1.000                           \n    x10               0.648    0.070    9.293    0.000\n    x11               0.776    0.093    8.340    0.000\n    x12               0.802    0.096    8.327    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n .x11 ~~                                              \n   .x12               1.460    0.300    4.873    0.000\n  copingm ~~                                          \n    socialm           0.398    0.071    5.603    0.000\n    enhancem          0.669    0.145    4.613    0.000\n  socialm ~~                                          \n    enhancem          0.320    0.084    3.783    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                0.000    0.092    0.000    1.000\n   .x2                0.000    0.068    0.000    1.000\n   .x3                0.000    0.086    0.000    1.000\n   .x4                0.000    0.063    0.000    1.000\n   .x5                0.000    0.077    0.000    1.000\n   .x6                0.000    0.079    0.000    1.000\n   .x7                0.000    0.111    0.000    1.000\n   .x8                0.000    0.101    0.000    1.000\n   .x9                0.000    0.120    0.000    1.000\n   .x10               0.000    0.078    0.000    1.000\n   .x11               0.000    0.115    0.000    1.000\n   .x12               0.000    0.119    0.000    1.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                3.117    0.230   13.546    0.000\n   .x2                1.694    0.125   13.527    0.000\n   .x3                2.705    0.200   13.536    0.000\n   .x4                0.454    0.070    6.502    0.000\n   .x5                1.794    0.130   13.835    0.000\n   .x6                1.384    0.115   12.015    0.000\n   .x7                3.240    0.248   13.089    0.000\n   .x8                2.393    0.194   12.352    0.000\n   .x9                3.958    0.400    9.895    0.000\n   .x10               1.710    0.170   10.063    0.000\n   .x11               4.657    0.371   12.545    0.000\n   .x12               4.997    0.398   12.561    0.000\n    copingm           1.118    0.217    5.158    0.000\n    socialm           0.380    0.110    3.469    0.001\n    enhancem          3.210    0.490    6.550    0.000\n\n\n\nGli indici di fit sono migliorati.\nEsaminiamo la soluzione standardizzata.\n\nprint(standardizedSolution(fit3))\n\n        lhs op      rhs est.std    se      z pvalue ci.lower ci.upper\n1   copingm =~       x1   0.514 0.043 12.016      0    0.430    0.598\n2   copingm =~       x2   0.515 0.043 12.055      0    0.431    0.599\n3   copingm =~       x3   0.514 0.043 12.037      0    0.431    0.598\n4   copingm =~       x4   0.540 0.063  8.609      0    0.417    0.663\n5   socialm =~       x4   0.438 0.061  7.129      0    0.317    0.558\n6   socialm =~       x5   0.632 0.032 20.004      0    0.570    0.694\n7   socialm =~       x6   0.746 0.025 29.291      0    0.697    0.796\n8   socialm =~       x7   0.690 0.029 24.206      0    0.634    0.746\n9   socialm =~       x8   0.731 0.026 27.800      0    0.680    0.783\n10 enhancem =~       x9   0.669 0.041 16.388      0    0.589    0.749\n11 enhancem =~      x10   0.664 0.041 16.243      0    0.584    0.744\n12 enhancem =~      x11   0.542 0.045 12.120      0    0.454    0.629\n13 enhancem =~      x12   0.541 0.045 12.083      0    0.453    0.628\n14      x11 ~~      x12   0.303 0.050  6.097      0    0.205    0.400\n15       x1 ~~       x1   0.736 0.044 16.757      0    0.650    0.822\n16       x2 ~~       x2   0.735 0.044 16.697      0    0.649    0.821\n17       x3 ~~       x3   0.735 0.044 16.726      0    0.649    0.822\n18       x4 ~~       x4   0.229 0.037  6.223      0    0.157    0.301\n19       x5 ~~       x5   0.601 0.040 15.043      0    0.522    0.679\n20       x6 ~~       x6   0.443 0.038 11.636      0    0.368    0.517\n21       x7 ~~       x7   0.524 0.039 13.307      0    0.447    0.601\n22       x8 ~~       x8   0.465 0.038 12.099      0    0.390    0.541\n23       x9 ~~       x9   0.552 0.055 10.104      0    0.445    0.659\n24      x10 ~~      x10   0.559 0.054 10.314      0    0.453    0.666\n25      x11 ~~      x11   0.706 0.048 14.582      0    0.611    0.801\n26      x12 ~~      x12   0.708 0.048 14.622      0    0.613    0.802\n27  copingm ~~  copingm   1.000 0.000     NA     NA    1.000    1.000\n28  socialm ~~  socialm   1.000 0.000     NA     NA    1.000    1.000\n29 enhancem ~~ enhancem   1.000 0.000     NA     NA    1.000    1.000\n30  copingm ~~  socialm   0.610 0.057 10.735      0    0.499    0.721\n31  copingm ~~ enhancem   0.353 0.060  5.844      0    0.235    0.472\n32  socialm ~~ enhancem   0.289 0.056  5.141      0    0.179    0.399\n33       x1 ~1            0.000 0.045  0.000      1   -0.088    0.088\n34       x2 ~1            0.000 0.045  0.000      1   -0.088    0.088\n35       x3 ~1            0.000 0.045  0.000      1   -0.088    0.088\n36       x4 ~1            0.000 0.045  0.000      1   -0.088    0.088\n37       x5 ~1            0.000 0.045  0.000      1   -0.088    0.088\n38       x6 ~1            0.000 0.045  0.000      1   -0.088    0.088\n39       x7 ~1            0.000 0.045  0.000      1   -0.088    0.088\n40       x8 ~1            0.000 0.045  0.000      1   -0.088    0.088\n41       x9 ~1            0.000 0.045  0.000      1   -0.088    0.088\n42      x10 ~1            0.000 0.045  0.000      1   -0.088    0.088\n43      x11 ~1            0.000 0.045  0.000      1   -0.088    0.088\n44      x12 ~1            0.000 0.045  0.000      1   -0.088    0.088\n45  copingm ~1            0.000 0.000     NA     NA    0.000    0.000\n46  socialm ~1            0.000 0.000     NA     NA    0.000    0.000\n47 enhancem ~1            0.000 0.000     NA     NA    0.000    0.000\n\n\nNon ci sono ulteriori motivi di preoccupazione. Brown (2015) conclude che il modello più adeguato sia model3.\nNel caso presente, a mio parare, l’introduzione della correlazione residua tra x11 e x12 si sarebbe anche potuta evitare, dato che il modello model3 (con meno idiosincrasie legate al campione) si era già dimostrato adeguato.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html#saturazione-sul-fattore-sbagliato",
    "href": "chapters/sem/06_refine_solution.html#saturazione-sul-fattore-sbagliato",
    "title": "44  La revisione del modello",
    "section": "44.7 Saturazione sul fattore sbagliato",
    "text": "44.7 Saturazione sul fattore sbagliato\nBrown (2015) considera anche il caso opposto, ovvero quello nel quale il ricercatore ipotizza una saturazione spuria. Per i dati in discussione, si può avere la situazione presente.\n\nmodel4 &lt;- '\n  copingm  =~ x1 + x2 + x3 + x4\n  socialm  =~ x4 +x5 + x6 + x7 + x8 + x12\n  enhancem =~ x9 + x10 + x11\n'\n\nAdattiamo il modello ai dati.\n\nfit4 &lt;- cfa(\n  model4, \n  sample.cov = covs, \n  sample.nobs = 500, \n  mimic = \"mplus\"\n)\n\nWarning message:\n\"lavaan-&gt;lav_lavaan_step05_samplestats():  \n   sample.mean= argument is missing, but model contains mean/intercept \n   parameters.\"\n\n\nEsaminiamo la soluzione ottenuta.\n\nprint(summary(fit4, fit.measures = TRUE))\n\nlavaan 0.6-18 ended normally after 59 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        40\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                               212.717\n  Degrees of freedom                                50\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              1664.026\n  Degrees of freedom                                66\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.898\n  Tucker-Lewis Index (TLI)                       0.866\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -12010.051\n  Loglikelihood unrestricted model (H1)     -11903.692\n                                                      \n  Akaike (AIC)                               24100.101\n  Bayesian (BIC)                             24268.685\n  Sample-size adjusted Bayesian (SABIC)      24141.723\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.081\n  90 Percent confidence interval - lower         0.070\n  90 Percent confidence interval - upper         0.092\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.554\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.073\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  copingm =~                                          \n    x1                1.000                           \n    x2                0.741    0.093    7.925    0.000\n    x3                0.932    0.118    7.906    0.000\n    x4                0.699    0.117    5.995    0.000\n  socialm =~                                          \n    x4                1.000                           \n    x5                1.725    0.260    6.634    0.000\n    x6                2.098    0.305    6.879    0.000\n    x7                2.717    0.401    6.775    0.000\n    x8                2.619    0.382    6.848    0.000\n    x12               0.900    0.236    3.818    0.000\n  enhancem =~                                         \n    x9                1.000                           \n    x10               0.638    0.076    8.408    0.000\n    x11               0.767    0.094    8.153    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  copingm ~~                                          \n    socialm           0.410    0.072    5.663    0.000\n    enhancem          0.661    0.148    4.456    0.000\n  socialm ~~                                          \n    enhancem          0.347    0.089    3.902    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                0.000    0.092    0.000    1.000\n   .x2                0.000    0.068    0.000    1.000\n   .x3                0.000    0.086    0.000    1.000\n   .x4                0.000    0.063    0.000    1.000\n   .x5                0.000    0.077    0.000    1.000\n   .x6                0.000    0.079    0.000    1.000\n   .x7                0.000    0.111    0.000    1.000\n   .x8                0.000    0.101    0.000    1.000\n   .x12               0.000    0.119    0.000    1.000\n   .x9                0.000    0.120    0.000    1.000\n   .x10               0.000    0.078    0.000    1.000\n   .x11               0.000    0.115    0.000    1.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                3.106    0.230   13.478    0.000\n   .x2                1.686    0.125   13.449    0.000\n   .x3                2.698    0.200   13.477    0.000\n   .x4                0.463    0.069    6.719    0.000\n   .x5                1.805    0.130   13.886    0.000\n   .x6                1.378    0.115   12.022    0.000\n   .x7                3.255    0.248   13.143    0.000\n   .x8                2.418    0.194   12.455    0.000\n   .x12               6.740    0.430   15.673    0.000\n   .x9                3.891    0.436    8.933    0.000\n   .x10               1.724    0.183    9.435    0.000\n   .x11               4.662    0.371   12.579    0.000\n    copingm           1.129    0.218    5.170    0.000\n    socialm           0.397    0.111    3.566    0.000\n    enhancem          3.277    0.524    6.258    0.000\n\n\n\nÈ chiaro che il modello model4 è inadeguato. Il problema emerge chiaramente anche esaminando i MI.\n\nprint(modindices(fit4))\n\n         lhs op rhs      mi    epc sepc.lv sepc.all sepc.nox\n47   copingm =~  x5   0.090  0.036   0.038    0.022    0.022\n48   copingm =~  x6   0.554  0.090   0.096    0.054    0.054\n49   copingm =~  x7   0.107  0.055   0.059    0.024    0.024\n50   copingm =~  x8   3.919 -0.306  -0.325   -0.143   -0.143\n51   copingm =~ x12   6.109  0.499   0.530    0.199    0.199\n52   copingm =~  x9   0.390 -0.096  -0.102   -0.038   -0.038\n53   copingm =~ x10   0.027 -0.016  -0.017   -0.010   -0.010\n54   copingm =~ x11   0.823  0.123   0.131    0.051    0.051\n55   socialm =~  x1   1.990 -0.398  -0.251   -0.122   -0.122\n56   socialm =~  x2   0.638  0.166   0.105    0.069    0.069\n57   socialm =~  x3   0.372  0.160   0.101    0.053    0.053\n58   socialm =~  x9   0.315 -0.130  -0.082   -0.031   -0.031\n59   socialm =~ x10   1.423  0.179   0.113    0.064    0.064\n60   socialm =~ x11   0.520 -0.150  -0.094   -0.037   -0.037\n61  enhancem =~  x1   1.029  0.067   0.121    0.059    0.059\n62  enhancem =~  x2   0.232  0.023   0.042    0.028    0.028\n63  enhancem =~  x3   0.153 -0.024  -0.043   -0.023   -0.023\n64  enhancem =~  x4   0.745 -0.031  -0.056   -0.040   -0.040\n65  enhancem =~  x5   0.343 -0.028  -0.050   -0.029   -0.029\n66  enhancem =~  x6   0.103  0.015   0.027    0.015    0.015\n67  enhancem =~  x7   2.752 -0.110  -0.198   -0.080   -0.080\n68  enhancem =~  x8   0.129 -0.021  -0.038   -0.017   -0.017\n69  enhancem =~ x12 116.781  0.916   1.658    0.624    0.624\n70        x1 ~~  x2   1.709  0.177   0.177    0.077    0.077\n71        x1 ~~  x3   2.273 -0.257  -0.257   -0.089   -0.089\n72        x1 ~~  x4   0.850  0.103   0.103    0.086    0.086\n73        x1 ~~  x5   0.292 -0.064  -0.064   -0.027   -0.027\n74        x1 ~~  x6   0.188 -0.048  -0.048   -0.023   -0.023\n75        x1 ~~  x7   0.023 -0.025  -0.025   -0.008   -0.008\n76        x1 ~~  x8   0.419 -0.093  -0.093   -0.034   -0.034\n77        x1 ~~ x12   0.025 -0.034  -0.034   -0.007   -0.007\n78        x1 ~~  x9   0.011  0.020   0.020    0.006    0.006\n79        x1 ~~ x10   0.004  0.008   0.008    0.003    0.003\n80        x1 ~~ x11   1.804  0.259   0.259    0.068    0.068\n81        x2 ~~  x3   0.071 -0.034  -0.034   -0.016   -0.016\n82        x2 ~~  x4   2.979 -0.143  -0.143   -0.162   -0.162\n83        x2 ~~  x5   2.403  0.135   0.135    0.077    0.077\n84        x2 ~~  x6   0.551  0.060   0.060    0.040    0.040\n85        x2 ~~  x7   0.457 -0.081  -0.081   -0.035   -0.035\n86        x2 ~~  x8   0.012  0.011   0.011    0.006    0.006\n87        x2 ~~ x12   0.134 -0.058  -0.058   -0.017   -0.017\n88        x2 ~~  x9   1.033  0.145   0.145    0.056    0.056\n89        x2 ~~ x10   1.140 -0.100  -0.100   -0.058   -0.058\n90        x2 ~~ x11   0.323  0.081   0.081    0.029    0.029\n91        x3 ~~  x4   1.472  0.127   0.127    0.113    0.113\n92        x3 ~~  x5   0.140  0.041   0.041    0.019    0.019\n93        x3 ~~  x6   0.717  0.087   0.087    0.045    0.045\n94        x3 ~~  x7   0.317  0.086   0.086    0.029    0.029\n95        x3 ~~  x8   3.121 -0.237  -0.237   -0.093   -0.093\n96        x3 ~~ x12   0.001  0.006   0.006    0.001    0.001\n97        x3 ~~  x9   0.000  0.003   0.003    0.001    0.001\n98        x3 ~~ x10   4.165 -0.241  -0.241   -0.111   -0.111\n99        x3 ~~ x11   3.806  0.350   0.350    0.099    0.099\n100       x4 ~~  x5   0.316 -0.036  -0.036   -0.039   -0.039\n101       x4 ~~  x6   0.052 -0.015  -0.015   -0.019   -0.019\n102       x4 ~~  x7   1.182  0.099   0.099    0.081    0.081\n103       x4 ~~  x8   0.062 -0.021  -0.021   -0.020   -0.020\n104       x4 ~~ x12   0.033  0.020   0.020    0.011    0.011\n105       x4 ~~  x9   1.418 -0.115  -0.115   -0.086   -0.086\n106       x4 ~~ x10   0.914  0.061   0.061    0.068    0.068\n107       x4 ~~ x11   0.517 -0.068  -0.068   -0.047   -0.047\n108       x5 ~~  x6   0.611  0.073   0.073    0.046    0.046\n109       x5 ~~  x7   0.115 -0.045  -0.045   -0.019   -0.019\n110       x5 ~~  x8   0.079  0.034   0.034    0.016    0.016\n111       x5 ~~ x12   3.265 -0.302  -0.302   -0.087   -0.087\n112       x5 ~~  x9   0.203  0.066   0.066    0.025    0.025\n113       x5 ~~ x10   0.000  0.002   0.002    0.001    0.001\n114       x5 ~~ x11   2.312 -0.224  -0.224   -0.077   -0.077\n115       x6 ~~  x7   2.239 -0.200  -0.200   -0.094   -0.094\n116       x6 ~~  x8   0.073  0.033   0.033    0.018    0.018\n117       x6 ~~ x12   0.478  0.109   0.109    0.036    0.036\n118       x6 ~~  x9   1.251 -0.153  -0.153   -0.066   -0.066\n119       x6 ~~ x10   0.784  0.079   0.079    0.051    0.051\n120       x6 ~~ x11   0.370  0.083   0.083    0.033    0.033\n121       x7 ~~  x8   1.644  0.219   0.219    0.078    0.078\n122       x7 ~~ x12   0.433 -0.152  -0.152   -0.032   -0.032\n123       x7 ~~  x9   0.005 -0.015  -0.015   -0.004   -0.004\n124       x7 ~~ x10   1.836 -0.179  -0.179   -0.076   -0.076\n125       x7 ~~ x11   0.348 -0.119  -0.119   -0.031   -0.031\n126       x8 ~~ x12   2.680 -0.335  -0.335   -0.083   -0.083\n127       x8 ~~  x9   0.676  0.147   0.147    0.048    0.048\n128       x8 ~~ x10   0.337  0.068   0.068    0.033    0.033\n129       x8 ~~ x11   3.437 -0.330  -0.330   -0.098   -0.098\n130      x12 ~~  x9   7.051  0.713   0.713    0.139    0.139\n131      x12 ~~ x10   6.960  0.465   0.465    0.136    0.136\n132      x12 ~~ x11  68.717  2.238   2.238    0.399    0.399\n133       x9 ~~ x10   0.081  0.138   0.138    0.053    0.053\n134       x9 ~~ x11   0.166  0.209   0.209    0.049    0.049\n135      x10 ~~ x11   0.423 -0.211  -0.211   -0.075   -0.075\n\n\nIl MI relativo alla saturazione di x12 su enhancem è uguale a 116.781. Chiaramente, in una revisione del modello, questo problema dovrebbe essere affrontato.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html#commenti-e-considerazioni-finali",
    "href": "chapters/sem/06_refine_solution.html#commenti-e-considerazioni-finali",
    "title": "44  La revisione del modello",
    "section": "44.8 Commenti e considerazioni finali",
    "text": "44.8 Commenti e considerazioni finali\nGli esempi presentati da Brown (2015) mostrano come l’applicazione dei MI, combinata con l’esame delle soluzioni fattoriali, rappresenti un approccio fondamentale per ottimizzare e perfezionare il modello proposto.\n\n\n\n\nBrown, Timothy A. 2015. Confirmatory factor analysis for applied research. Guilford publications.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/07_group_invariance.html",
    "href": "chapters/sem/07_group_invariance.html",
    "title": "45  Invarianza di misura",
    "section": "",
    "text": "45.1 Indicatori continui",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Invarianza di misura</span>"
    ]
  },
  {
    "objectID": "chapters/sem/07_group_invariance.html#indicatori-continui",
    "href": "chapters/sem/07_group_invariance.html#indicatori-continui",
    "title": "45  Invarianza di misura",
    "section": "",
    "text": "45.1.1 Intercette degli item\nIn generale, i modelli di equazioni strutturali vengono utilizzati per modellare unicamente la matrice di covarianza delle variabili osservate in un set di dati. Ricordiamo che, quando abbiamo introdotto il modello dell’analisi fattoriale,\n\\[\ny_i = \\mu + \\lambda_j \\xi_k + \\delta_i,\n\\]\nper semplicità abbiamo ignorato la media \\(\\mu\\) degli indicatori esprimendo i dati osservati nei termini degli scarti dalla media, \\(y_i -\\mu\\), in quanto ciò lascia immutate le covarianze. Tuttavia, in alcune applicazioni (quali, appunto, l’invarianza di misura), è utile considerare anche le medie delle variabili osservate. Per includere nel modello fattoriale le informazioni sulle medie facciamo esplicito riferimento all’intercetta della precedente equazione. Usando la sintassi lavaan, la media di una variabile manifesta viene inserita nel modello specificando l’intercetta dell’equazione precedente come segue\n\nmy_item ~ 1\n\nmy_item ~ 1\n\n\nLa parte sinistra dell’espressione precedente contiene il nome della variabile manifesta a cui si fa riferimento; la parte destra dell’espressione precedente specifica la presenza dell’intercetta.\nPer esempio, nella specificazione di un modello a due fattori comuni, è possibile aggiungere al modello le medie delle variabili manifeste nel modo seguente:\n\nmod1 &lt;- \"\n  # two-factor model\n  f1 =~ x1 + x2 + x3\n  f2 =~ x4 + x5 + x6\n  # intercepts\n  x1 ~ 1\n  x2 ~ 1\n  x3 ~ 1\n  x4 ~ 1\n  x5 ~ 1\n  x6 ~ 1\n\"\n\nTuttavia, è più conveniente omettere le intercette nella specificazione del modello e aggiungere l’argomento meanstructure = TRUE nella funzione cfa().\n\nmod2 &lt;- \"\n  f1 =~ x1 + x2 + x3\n  f2 =~ x4 + x5 + x6\n\"\n\nfit &lt;- cfa(\n  mod2,\n  data = d,\n  meanstructure = TRUE\n)\nSi noti che modelli con o senza meanstructure avranno la stessa statistica chi-quadrato e lo stesso numero di gradi di libertà. Il motivo è che, nel caso di un modello con meanstructure, vengono introdotti \\(p\\) nuovi dati (ovvero, il valore della media per ciascuno dei \\(p\\) indicatori) ma vengono anche stimati ulteriori \\(p\\) parametri (ovvero, un’intercetta per ciascuno dei \\(p\\) indicatori). Il risultato finale è che la bontà dell’adattamento resta immutata. In pratica, l’unico motivo per aggiungere le intercette nella sintassi del modello è quello di introdurre dei vincoli nella stima di tali parametri.\n\n\n45.1.2 Tipologie di Invarianza\nÈ possibile definire diversi livelli di invarianza tra gruppi nel contesto di un’analisi fattoriale confermativa (CFA) multigruppo:\n\nInvarianza Configurale (fit_ef): Questo è il livello più basilare di invarianza. Si verifica quando la stessa struttura fattoriale si adatta a ogni gruppo. Questo significa che gli stessi fattori sono presenti in ogni gruppo e che gli stessi item misurano questi fattori in tutti i gruppi. Non si assumono ancora uguaglianze nelle saturazioni fattoriali o in altri parametri del modello.\nInvarianza Metrica (fit_efl): Oltre all’invarianza configurale, si testa l’uguaglianza delle saturazioni fattoriali (“loadings”) attraverso i gruppi. Questo passo verifica se gli item hanno lo stesso rapporto con il fattore latente in tutti i gruppi. L’invarianza metrica è fondamentale per affermare che la scala del fattore latente è la stessa tra i gruppi.\nInvarianza delle Intercette degli Indicatori (fit_eii): A questo livello, si testa se le intercette degli item sono uguali tra i gruppi, oltre all’uguaglianza delle saturazioni fattoriali. Questo passo è cruciale per sostenere che i gruppi interpretano il significato dei punteggi degli item allo stesso modo.\nInvarianza delle Varianze degli Errori degli Indicatori (fit_eir): Qui si aggiunge il test sull’uguaglianza delle varianze degli errori degli indicatori tra i gruppi. Questo implica che l’affidabilità di ciascun item (in termini di errore di misurazione) è la stessa in tutti i gruppi.\nInvarianza delle Varianze dei Fattori Latenti (fit_fv): In questo modello, si testa l’uguaglianza delle varianze dei fattori latenti tra i gruppi, oltre ai livelli precedenti di invarianza. Questo indica che la variabilità dei fattori latenti è simile tra i gruppi.\nInvarianza delle Medie dei Fattori Latenti (fit_fm): Questo è il livello più stringente di invarianza, dove si testa anche l’uguaglianza delle medie dei fattori latenti tra i gruppi. Questo significa che non solo la struttura e il funzionamento del modello sono simili tra i gruppi, ma anche che i gruppi hanno medie latenti equivalenti.\n\nOgni livello successivo di invarianza include i vincoli del livello precedente. Il passaggio da un livello all’altro richiede un confronto degli indici di adattamento per determinare se l’aggiunta di nuovi vincoli peggiora significativamente l’adattamento del modello. Questa sequenza di modelli aiuta a comprendere in modo rigoroso se e come un costrutto misurato dal modello CFA si manifesti in modo simile o diverso tra i gruppi.\n\n\n45.1.3 Un esempio concreto\nConsideriamo qui un esempio discusso da {cite:t}brown2015confirmatory. Il modello CFA riguarda un modello di misurazione per la depressione maggiore così come è definita nel DSM-IV. Il campione include 9 indicatori:\n\nMDD1, depressed mood;\nMDD2, loss of interest in usual activities;\nMDD3, weight/appetite change;\nMDD4, sleep disturbance;\nMDD5, psychomotor agitation/retardation;\nMDD6, fatigue/loss of energy;\nMDD7, feelings of worthlessness/guilt;\nMDD8, concentration difficulties;\nMDD9, thoughts of death/suicidality.\n\nLeggiamo i dati in \\(\\mathsf{R}\\):\n\nd &lt;- readRDS(\n  here::here(\"data\", \"mdd_sex.RDS\")\n)\n\n\nhead(d)\n\n\nA data.frame: 6 x 10\n\n\n\nsex\nmdd1\nmdd2\nmdd3\nmdd4\nmdd5\nmdd6\nmdd7\nmdd8\nmdd9\n\n\n\n&lt;fct&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\nfemale\n5\n4\n1\n6\n5\n6\n5\n4\n2\n\n\n2\nfemale\n5\n5\n5\n5\n4\n5\n4\n5\n4\n\n\n3\nfemale\n4\n5\n4\n2\n6\n6\n0\n0\n0\n\n\n4\nfemale\n5\n5\n3\n3\n5\n5\n6\n4\n0\n\n\n5\nfemale\n5\n5\n0\n5\n0\n4\n6\n0\n0\n\n\n6\nfemale\n6\n6\n4\n6\n4\n6\n5\n6\n2\n\n\n\n\n\nNel caso presente, i gruppi corrispondono al genere. Confrontiamo le distribuzioni di densità empirica degli item tra i due gruppi.\n\nd_long &lt;- d |&gt;\n    pivot_longer(!sex, names_to = \"item\", values_to = \"value\")\n\nd_long |&gt;\n    ggplot(aes(value, col=sex)) +\n    geom_density(linewidth=1.5) +\n    facet_wrap(~item, nrow=3, scales=\"free\") +\n    labs(x=\" \", y=\"Density\")\n\n\n\n\n\n\n\n\nCi poniamo dunque il problema di stabilire l’invarianza fattoriale in funzione del genere. Consideriamo il seguente modello:\n\nmodel_mdd &lt;- \"\n  MDD =~ mdd1 + mdd2 + mdd3 + mdd4 + mdd5 + mdd6 + mdd7 + mdd8 +\n         mdd9\n  mdd1 ~~ mdd2\n\"\n\nSi noti la presenza di una correlazione residua tra gli indicatori mdd1 e mdd2.\nIn precedenza, abbiamo discusso le strutture di media e covarianza dei modelli CFA nel contesto di un’analisi a singolo gruppo. La stima dei parametri del modello, data un campione, può essere effettuata selezionando o individuando il miglior insieme di stime che minimizza una funzione di discrepanza.\nL’estensione di questo metodo di stima per un singolo campione all’analisi CFA multi-gruppo è diretta. Prima di tutto, si definisce una funzione di discrepanza individuale per ogni gruppo o campione. Per stimare tutti i parametri in gruppi indipendenti simultaneamente, si definisce una funzione di discrepanza complessiva come somma ponderata delle funzioni di discrepanza specifiche per gruppo.\nProcediamo quindi con la definizione di modelli che facilitano il confronto tra diversi tipi di invarianza fattoriale. È importante prestare attenzione ai vincoli che vengono progressivamente introdotti man mano che si specificano modelli sempre più restrittivi. Nella sintassi del software lavaan, utilizzato per l’analisi SEM, questi vincoli vengono impostati tramite l’argomento group.equal.\n\n# configural invariance\nfit_ef &lt;- cfa(\n  model_mdd,\n  data = d,\n  group = \"sex\",\n  meanstructure = TRUE\n)\n\n# plus equal factor loadings- metric invariance\nfit_efl &lt;- update(\n  fit_ef,\n  group.equal = c(\"loadings\")\n)\n\n# plus equal indicator intercepts\nfit_eii &lt;- update(\n  fit_efl,\n  group.equal = c(\"loadings\", \"intercepts\")\n)\n\n# plus equal indicator error variances\nfit_eir &lt;- update(\n  fit_eii,\n  group.equal = c(\"loadings\", \"intercepts\", \"residuals\")\n)\n\n# plus equal factor variances\nfit_fv &lt;- update(\n  fit_eir,\n  group.equal = c(\n    \"loadings\", \"intercepts\", \"residuals\",\n    \"lv.variances\"\n  )\n)\n\n# plus equal latent means\nfit_fm &lt;- update(\n  fit_fv,\n  group.equal = c(\n    \"loadings\", \"intercepts\", \"residuals\",\n    \"lv.variances\", \"means\"\n  )\n)\n\nConfrontiamo i modelli mediate il test del rapporto di verosimiglianze:\n\nout &lt;- lavTestLRT(fit_ef, fit_efl, fit_eii, fit_eir, fit_fv, fit_fm)\nprint(out)\n\n\nChi-Squared Difference Test\n\n        Df   AIC   BIC   Chisq Chisq diff    RMSEA Df diff Pr(&gt;Chisq)\nfit_ef  52 27526 27784  98.911                                       \nfit_efl 60 27514 27736 102.839     3.9286 0.000000       8     0.8635\nfit_eii 68 27510 27695 115.309    12.4699 0.038600       8     0.1314\nfit_eir 77 27502 27645 125.021     9.7115 0.014520       9     0.3743\nfit_fv  78 27501 27639 125.814     0.7931 0.000000       1     0.3732\nfit_fm  79 27501 27635 127.734     1.9201 0.049533       1     0.1659\n\n\nIl confronto tra i precedenti modelli nidificati che introducono vincoli sempre più stringenti sui parametri indica che non vi è una “significativa” perdita di bontà dell’adattamento passando dal modello congenerico al modello che assume l’uguaglianza delle saturazioni fattoriali, delle intercette, delle varianze residue, delle varianze delle variabili latenti e delle medie dei due gruppi. Per i dati discussi da {cite:t}brown2015confirmatory, dunque, possiamo concludere che vi sono forti evidenze di invarianza fattoriale tra maschi e femmine in relazione al costrutto di depressione maggiore. L’invarianza fattoriali giustifica, per questi dati, un confronto tra le medie dei punteggi totali del test calcolate nei due gruppi.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Invarianza di misura</span>"
    ]
  },
  {
    "objectID": "chapters/sem/07_group_invariance.html#indicatori-a-livello-di-scala-ordinale",
    "href": "chapters/sem/07_group_invariance.html#indicatori-a-livello-di-scala-ordinale",
    "title": "45  Invarianza di misura",
    "section": "45.2 Indicatori a livello di scala ordinale",
    "text": "45.2 Indicatori a livello di scala ordinale\nNell’analisi multi-gruppo per dati ordinali devono essere affrontati problemi specifici per questo tipo di dati. I dati ordinali si distinguono dalle variabili continue in due modi principali: per il metodo utilizzato nella stima delle saturazioni fattoriali e per l’approccio nell’analisi statistica.\nLe variabili ordinali sono formate da categorie con un ordine logico. Ad esempio, risposte come “fortemente in disaccordo” fino a “fortemente d’accordo”, o “mai”, “a volte”, “spesso”, “sempre” sono sequenze logiche. Sebbene queste categorie siano ordinate, la distanza tra di loro non è necessariamente uniforme o misurabile in termini quantitativi. Pertanto, assegnare valori numerici a queste risposte è un processo arbitrario e non riflette differenze di intensità o grandezza.\nUno dei problemi principali nell’analizzare dati ordinali è come calcolare le correlazioni. Le correlazioni policoriche offrono una soluzione, presupponendo l’esistenza di una variabile latente continua e normalmente distribuita che influisce sulla distribuzione delle risposte. In questo modello, ogni categoria di risposta ordinale corrisponde a un intervallo specifico sulla variabile latente sottostante. I punti di taglio o soglie (\\(\\tau_1, \\tau_2, \\dots, \\tau_k\\)) dividono la distribuzione normale in sezioni che rappresentano le frequenze osservate per ogni categoria.\nNell’ambito dell’invarianza fattoriale per dati ordinali, si pone inizialmente la questione dell’invarianza delle soglie (treshold invariance). Questo concetto presuppone che le soglie che definiscono le correlazioni policoriche rimangano costanti tra i gruppi. La stabilità di queste soglie è fondamentale per assicurare che le relazioni tra le categorie di risposta siano comparabili tra diversi gruppi.\nUn secondo aspetto cruciale è la scelta dello stimatore per la stima delle saturazioni fattoriali. Per i dati ordinali, lo stimatore raccomandato è quello dei minimi quadrati ponderati (Weighted Least Squares, WLS). Questo metodo è preferito perché tiene conto della natura specifica dei dati ordinali, fornendo stime più accurate e affidabili delle saturazioni fattoriali rispetto a metodi pensati per dati continui.\nIn sintesi, l’analisi di invarianza fattoriale con dati ordinali richiede considerazioni e tecniche specifiche, che includono l’utilizzo di correlazioni policoriche e l’impiego di stimatori adeguati, per garantire risultati validi e interpretabili.\n\n45.2.1 Un esempio concreto\n{cite:t}wu2016identification sostengono che la metodologia convenzionale per valutare l’invarianza fattoriale nei dati continui richiede adattamenti significativi quando applicata a indicatori categoriali. Il processo standard inizia con la definizione di un modello di base, seguito dall’applicazione di restrizioni incrementali ai parametri. Tuttavia, secondo {cite:t}wu2016identification, questo approccio risulta subottimale per i dati categoriali, a causa della sua dipendenza critica dalla definizione delle soglie utilizzate per stabilire le correlazioni policoriche con le variabili latenti continue nel modello configurale.\n{cite:t}wu2016identification enfatizzano che, in contesti con dati categoriali, è primario valutare prima l’equivalenza delle soglie tra i gruppi, definendo ciò che viene chiamato un “modello di soglia” (threshold model). Solo dopo questa valutazione preliminare è consigliabile procedere con l’analisi dell’equivalenza delle saturazioni fattoriali tra i gruppi. Questo cambiamento nell’ordine di valutazione è cruciale per garantire che le soglie, che sono fondamentali per la definizione delle correlazioni policoriche nei dati categoriali, siano coerenti e comparabili tra i gruppi prima di procedere con ulteriori analisi sull’invarianza fattoriale. In sostanza, ciò che {cite:t}wu2016identification propongono è una riconsiderazione della sequenza di passaggi nell’analisi dell’invarianza per dati categoriali, ponendo una maggiore enfasi sull’allineamento delle soglie prima di esplorare altre forme di equivalenza nei modelli multigruppo.\nPer illustrare tale procedura, replichiamo qui il tutorial messo a punto da {cite:t}svetina2020multiple. Questi autori utilizzano quattro item di una scala del bullismo ed esaminano i dati raccolti in tre paesi (31 = Azerbaigian; 40 = Austria; 246 = Finlandia). Tutti gli item sono misurati su una scala di tipo Likert a 4 punti, che va da 0 (mai) a 3 (almeno una volta alla settimana). Gli item chiedono al partecipante di valutare delle affermazioni relative ad episodi di bullismo. Per esempio, “mi prendevano in giro o mi insultavano”. Per l’Azerbaigian, l’Austria e la Finlandia, le dimensioni del campione sono rispettivamente pari a 3,808, 4,457 e 4,520.\nLeggiamo in dati in \\(\\textsf{R}\\):\n\ndat &lt;- read.table(\"../data/BULLY.dat\", header = FALSE)\nnames(dat) &lt;- c(\"IDCNTRY\", \"R09A\", \"R09B\", \"R09C\", \"R09D\")\nhead(dat)\n\n\nA data.frame: 6 x 5\n\n\n\nIDCNTRY\nR09A\nR09B\nR09C\nR09D\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n31\n3\n3\n0\n0\n\n\n2\n31\n0\n0\n0\n0\n\n\n3\n31\n3\n2\n1\n3\n\n\n4\n31\n0\n0\n3\n0\n\n\n5\n31\n0\n0\n0\n0\n\n\n6\n31\n0\n0\n0\n0\n\n\n\n\n\nViene creata la matrice all.results per immagazzinare i risultati dei diversi modelli che verranno confrontati, chiamati baseline (nessun vincolo tra i gruppi), proposition 4 (equivalenza delle soglie tra i gruppi), e proposition 7 (equivalenza delle soglie e delle saturazioni fattoriali tra i gruppi). Gli indici di bontà dell’adattamento che verranno considerati sono: chi-square, df, p, RMSEA, CFI, e TLI.\n\nall.results &lt;- matrix(NA, ncol = 6, nrow = 3)\n\n\n\n45.2.2 Baseline model\nNel baseline model non viene posto alcun vincolo tra i gruppi. È quello dell’invarianza configurale.\n\nmod.cat &lt;- \"F1 =~ R09A + R09B + R09C + R09D\"\n\n\nbaseline &lt;- measEq.syntax(\n  configural.model = mod.cat,\n  data = dat,\n  ordered = c(\"R09A\", \"R09B\", \"R09C\", \"R09D\"),\n  parameterization = \"delta\",\n  ID.fac = \"std.lv\",\n  ID.cat = \"Wu.Estabrook.2016\",\n  group = \"IDCNTRY\",\n  group.equal = \"configural\"\n)\n\nInformazioni sul modello baseline si ottengono nel modo seguente:\n\nout &lt;- summary(baseline)\nprint(out)\n\nThis lavaan model syntax specifies a CFA with 4 manifest indicators (4 of which are ordinal) of 1 common factor(s).\n\nTo identify the location and scale of each common factor, the factor means and variances were fixed to 0 and 1, respectively, unless equality constraints on measurement parameters allow them to be freed.\n\nThe location and scale of each latent item-response underlying 4 ordinal indicators were identified using the \"delta\" parameterization, and the identification constraints recommended by Wu & Estabrook (2016). For details, read:\n\n    https://doi.org/10.1007/s11336-016-9506-0 \n\nPattern matrix indicating num(eric), ord(ered), and lat(ent) indicators per factor:\n\n     F1 \nR09A ord\nR09B ord\nR09C ord\nR09D ord\n\nThe following types of parameter were constrained to equality across groups:\n\n    configural\n\n     F1   \nR09A \"ord\"\nR09B \"ord\"\nR09C \"ord\"\nR09D \"ord\"\n\n\nLe proprietà del modello possono essere esplicitate con la seguente istruzione:\n\ncat(as.character(baseline))\n\n## LOADINGS:\n\nF1 =~ c(NA, NA, NA)*R09A + c(lambda.1_1.g1, lambda.1_1.g2, lambda.1_1.g3)*R09A\nF1 =~ c(NA, NA, NA)*R09B + c(lambda.2_1.g1, lambda.2_1.g2, lambda.2_1.g3)*R09B\nF1 =~ c(NA, NA, NA)*R09C + c(lambda.3_1.g1, lambda.3_1.g2, lambda.3_1.g3)*R09C\nF1 =~ c(NA, NA, NA)*R09D + c(lambda.4_1.g1, lambda.4_1.g2, lambda.4_1.g3)*R09D\n\n## THRESHOLDS:\n\nR09A | c(NA, NA, NA)*t1 + c(R09A.thr1.g1, R09A.thr1.g2, R09A.thr1.g3)*t1\nR09A | c(NA, NA, NA)*t2 + c(R09A.thr2.g1, R09A.thr2.g2, R09A.thr2.g3)*t2\nR09A | c(NA, NA, NA)*t3 + c(R09A.thr3.g1, R09A.thr3.g2, R09A.thr3.g3)*t3\nR09B | c(NA, NA, NA)*t1 + c(R09B.thr1.g1, R09B.thr1.g2, R09B.thr1.g3)*t1\nR09B | c(NA, NA, NA)*t2 + c(R09B.thr2.g1, R09B.thr2.g2, R09B.thr2.g3)*t2\nR09B | c(NA, NA, NA)*t3 + c(R09B.thr3.g1, R09B.thr3.g2, R09B.thr3.g3)*t3\nR09C | c(NA, NA, NA)*t1 + c(R09C.thr1.g1, R09C.thr1.g2, R09C.thr1.g3)*t1\nR09C | c(NA, NA, NA)*t2 + c(R09C.thr2.g1, R09C.thr2.g2, R09C.thr2.g3)*t2\nR09C | c(NA, NA, NA)*t3 + c(R09C.thr3.g1, R09C.thr3.g2, R09C.thr3.g3)*t3\nR09D | c(NA, NA, NA)*t1 + c(R09D.thr1.g1, R09D.thr1.g2, R09D.thr1.g3)*t1\nR09D | c(NA, NA, NA)*t2 + c(R09D.thr2.g1, R09D.thr2.g2, R09D.thr2.g3)*t2\nR09D | c(NA, NA, NA)*t3 + c(R09D.thr3.g1, R09D.thr3.g2, R09D.thr3.g3)*t3\n\n## INTERCEPTS:\n\nR09A ~ c(0, 0, 0)*1 + c(nu.1.g1, nu.1.g2, nu.1.g3)*1\nR09B ~ c(0, 0, 0)*1 + c(nu.2.g1, nu.2.g2, nu.2.g3)*1\nR09C ~ c(0, 0, 0)*1 + c(nu.3.g1, nu.3.g2, nu.3.g3)*1\nR09D ~ c(0, 0, 0)*1 + c(nu.4.g1, nu.4.g2, nu.4.g3)*1\n\n## SCALING FACTORS:\n\nR09A ~*~ c(1, 1, 1)*R09A\nR09B ~*~ c(1, 1, 1)*R09B\nR09C ~*~ c(1, 1, 1)*R09C\nR09D ~*~ c(1, 1, 1)*R09D\n\n\n## LATENT MEANS/INTERCEPTS:\n\nF1 ~ c(0, 0, 0)*1 + c(alpha.1.g1, alpha.1.g2, alpha.1.g3)*1\n\n## COMMON-FACTOR VARIANCES:\n\nF1 ~~ c(1, 1, 1)*F1 + c(psi.1_1.g1, psi.1_1.g2, psi.1_1.g3)*F1\n\n\nPer potere essere passato a lavaan, l’oggetto baseline deve essere in formato char:\n\nmodel.baseline &lt;- as.character(baseline)\n\nAdattiamo il modello ai dati:\n\nfit.baseline &lt;- cfa(\n  model.baseline, \n  data = dat, \n  group = \"IDCNTRY\", \n  ordered = c(\"R09A\", \"R09B\", \"R09C\", \"R09D\")\n)\n\nSalviamo i risultati:\n\nall.results[1, ] &lt;-\n  round(\n    data.matrix(\n      fitmeasures(fit.baseline, fit.measures = c(\n        \"chisq.scaled\", \"df.scaled\", \"pvalue.scaled\",\n        \"rmsea.scaled\", \"cfi.scaled\", \"tli.scaled\"\n      ))\n    ),\n    digits = 3\n  )\n\n\n\n45.2.3 Invarianza delle soglie\nConsideriamo ora il modello threshold invariance (chiamato Proposition 4 da Wu e Estabrook 2016).\n\nprop4 &lt;- measEq.syntax(\n  configural.model = mod.cat,\n  data = dat,\n  ordered = c(\"R09A\", \"R09B\", \"R09C\", \"R09D\"),\n  parameterization = \"delta\",\n  ID.fac = \"std.lv\",\n  ID.cat = \"Wu.Estabrook.2016\",\n  group = \"IDCNTRY\",\n  group.equal = c(\"thresholds\")\n)\n\nAdattiamo il modello ai dati:\n\nmodel.prop4 &lt;- as.character(prop4)\nfit.prop4 &lt;- cfa(\n  model.prop4,\n  data = dat,\n  group = \"IDCNTRY\",\n  ordered = c(\"R09A\", \"R09B\", \"R09C\", \"R09D\")\n)\n\nSalviamo i risulati\n\n#store model fit information for proposition 4\nall.results[2, ] &lt;-\n  round(data.matrix(\n  fitmeasures(fit.prop4,fit.measures = c(\n    \"chisq.scaled\",\"df.scaled\",\"pvalue.scaled\", \n    \"rmsea.scaled\", \"cfi.scaled\", \"tli.scaled\"))), \n  digits=3\n  )\n\nEseguiamo il confronto tra il modello threshold invariance e il modello baseline:\n\nlavTestLRT(fit.baseline, fit.prop4)\n\n\nA anova: 2 x 7\n\n\n\nDf\nAIC\nBIC\nChisq\nChisq diff\nDf diff\nPr(&gt;Chisq)\n\n\n\n&lt;int&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nfit.baseline\n6\nNA\nNA\n26.94149\nNA\nNA\nNA\n\n\nfit.prop4\n14\nNA\nNA\n42.16983\n61.01121\n8\n2.950764e-10\n\n\n\n\n\n\n\n45.2.4 Invarianza delle soglie e delle saturazioni fattoriali\nConsideriamo ora il modello threshold and loading invariance (chiamato Proposition 7 da Wu e Estabrook 2016).\n\nprop7 &lt;- measEq.syntax(\n  configural.model = mod.cat,\n  data = dat,\n  ordered = c(\"R09A\", \"R09B\", \"R09C\", \"R09D\"),\n  parameterization = \"delta\",\n  ID.fac = \"std.lv\",\n  ID.cat = \"Wu.Estabrook.2016\",\n  group = \"IDCNTRY\",\n  group.equal = c(\"thresholds\", \"loadings\")\n)\n\nAdattiamo il modello ai dati:\n\nmodel.prop7 &lt;- as.character(prop7)\nfit.prop7 &lt;- cfa(\n  model.prop7, \n  data = dat, group = \"IDCNTRY\", \n  ordered = c(\"R09A\", \"R09B\", \"R09C\", \"R09D\")\n  )\n\nSalviamo i risultati:\n\nall.results[3, ] &lt;-\n  round(data.matrix(\n    fitmeasures(fit.prop7, fit.measures = c(\n      \"chisq.scaled\", \"df.scaled\", \"pvalue.scaled\",\n      \"rmsea.scaled\", \"cfi.scaled\", \"tli.scaled\"\n    ))\n  ), digits = 3)\n\ncolumn.names &lt;-\n  c(\n    \"chisq.scaled\", \"df.scaled\", \"pvalue.scaled\", \"rmsea.scaled\",\n    \"cfi.scaled\", \"tli.scaled\"\n  )\n\nrow.names &lt;- c(\"baseline\", \"prop4\", \"prop7\")\n\ncolnames(all.results) &lt;- column.names\nrownames(all.results) &lt;- row.names\n\nEseguiamo i confronti tra modelli:\n\nlavTestLRT(fit.prop4, fit.prop7)\n\n\nA anova: 2 x 7\n\n\n\nDf\nAIC\nBIC\nChisq\nChisq diff\nDf diff\nPr(&gt;Chisq)\n\n\n\n&lt;int&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nfit.prop4\n14\nNA\nNA\n42.16983\nNA\nNA\nNA\n\n\nfit.prop7\n20\nNA\nNA\n93.11534\n73.70792\n6\n7.079865e-14\n\n\n\n\n\n\nlavTestLRT(fit.prop7, fit.baseline)\n\n\nA anova: 2 x 7\n\n\n\nDf\nAIC\nBIC\nChisq\nChisq diff\nDf diff\nPr(&gt;Chisq)\n\n\n\n&lt;int&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nfit.baseline\n6\nNA\nNA\n26.94149\nNA\nNA\nNA\n\n\nfit.prop7\n20\nNA\nNA\n93.11534\n136.1424\n14\n4.139318e-22\n\n\n\n\n\nUn confronto tra gli indici di bontà di adattamento dei tre modelli è fornito di seguito:\n\nprint(all.results)\n\n         chisq.scaled df.scaled pvalue.scaled rmsea.scaled cfi.scaled\nbaseline       50.944         6             0        0.042      0.997\nprop4         107.839        14             0        0.040      0.994\nprop7         186.542        20             0        0.044      0.989\n         tli.scaled\nbaseline      0.991\nprop4         0.992\nprop7         0.990\n\n\nIn conclusione, nel caso presente, il test del rapporto di verosimiglianza indica che non viene rispettata neppure l’invarianza delle soglie tra gruppi. Gli altri confronti, dunque, sono superflui e sono stati qui presentati solo allo scopo di illustrare la procedura.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Invarianza di misura</span>"
    ]
  },
  {
    "objectID": "chapters/sem/07_group_invariance.html#vincoli-inter-gruppi",
    "href": "chapters/sem/07_group_invariance.html#vincoli-inter-gruppi",
    "title": "45  Invarianza di misura",
    "section": "45.3 Vincoli Inter-gruppi",
    "text": "45.3 Vincoli Inter-gruppi\nQuando si adatta un modello di equazioni strutturali ai dati provenienti da più gruppi, è comune imporre vincoli di uguaglianza inter-gruppi su certe stime di parametri non standardizzati. Questi parametri possono riguardare effetti causali, inclusi effetti diretti, indiretti o totali, o per varianze, covarianze, medie o intercette. La scelta tra questi dovrebbe essere correlata a specifiche ipotesi riguardo le differenze tra i gruppi. Se l’adattamento del modello vincolato è molto peggiore rispetto a quello del modello non vincolato—e il modello non vincolato si adatta bene ai dati—possiamo concludere che le popolazioni da cui sono stati selezionati i gruppi possono differire rispetto ai parametri soggetti a vincoli di uguaglianza. Un’alternativa all’analisi multi-gruppo consiste nel rappresentare l’appartenenza al gruppo in un modello a singolo gruppo, adattato ai dati di tutti i gruppi combinati. Alcune variabili endogene nel modello sono regresse sul gruppo e su termini prodotto che coinvolgono il gruppo e altre variabili moderatrici presunte. I coefficienti per i termini prodotto stimano gli effetti interattivi del gruppo con altre variabili causali, e i loro valori possono essere convertiti in stime di effetti indiretti condizionali. Un inconveniente è che i modelli a singolo gruppo generalmente assumono l’omoscedasticità tra i gruppi, e le stime possono essere inesatte se tali assunzioni sono insostenibili. In contrasto, è semplice testare le ipotesi di omogeneità in un approccio multi-gruppo.\nEsaminiamo un esempio discuso da {cite:t}kline2023principles. Lynam e colleghi (1993) hanno misurato lo status socioeconomico familiare (SES), il quoziente intellettivo verbale (QI verbale), la motivazione dell’esaminato durante il test del QI, il rendimento scolastico e i comportamenti delinquenzali in campioni di adolescenti maschi bianchi (n = 181) e neri (n = 214) di età compresa tra i 12 e i 13 anni. Erano studenti di quarta elementare in scuole pubbliche urbane negli Stati Uniti ed erano parte di uno studio longitudinale su individui ad alto rischio di forme precoci di delinquenza.\nLa figura mostra un modello di percorso con una struttura di media (rappresentata con linee tratteggiate) basato sulle variabili dello studio. La figura rappresenta l’ipotesi che SES, motivazione e QI verbale siano cause correlate che influenzano la delinquenza sia direttamente che indirettamente attraverso il rendimento scolastico. Ad esempio, i ragazzi adolescenti con scarsa abilità verbale potrebbero essere più propensi ad abbandonare la scuola, contribuendo così alla delinquenza a causa di prospettive di impiego ridotte o del maggiore tempo non supervisionato per strada.\n\n\n\n```txgaoinm ../images/lynam.png\n\n\n\n\nheight: 450px\n\n\nname: path_01-fig\n\n\n\nDiagramma di percorso proposto da Lynam et al. (1993).\n\nLo studio di Lynam et al. (1993) era trasversale, senza precedenza temporale nelle misurazioni, quindi l'unica base per la specificazione della direzionalità è l'argomentazione. La logica appena riassunta è soggetta a critiche. Ad esempio, Block (1995) ha sostenuto che Lynam et al. (1993) hanno trascurato il ruolo potenziale dell'impulsività come mediatore degli effetti dell'abilità verbale sulla delinquenza. Ovvero, Block (1995) ha sostenuto che non è l'abilità verbale compromessa di per sé che è causalmente correlata alla delinquenza; piuttosto, una capacità ridotta di ritardare la gratificazione, pensare prima di agire o rimanere concentrati su un obiettivo sono fattori più importanti nella delinquenza. \n\n1. Senza vincoli, il modello di percorso nella figura è \"appena identificato\" (*just identified*) e si adatterebbe perfettamente ai dati in entrambi i gruppi. Vincoli di uguaglianza inter-gruppi sono stati imposti su parametri chiave in questo esempio per testare l'ipotesi che rimanere a scuola sia un fattore protettivo relativamente più forte contro la delinquenza tra i giovani maschi neri rispetto ai giovani maschi bianchi, specialmente in aree urbane a basso SES con relativamente più famiglie monogenitoriali dove i giovani neri sono rappresentati sproporzionatamente (Lynam et al., 1993). La strategia di analisi è delineata di seguito: per il modello 1, tutti i 7 effetti diretti non standardizzati sono vincolati all'uguaglianza nei due gruppi. Ci si aspettava che il modello 1 fosse incoerente con i dati, se l'effetto del rendimento scolastico sull'illegalità è diverso tra i gruppi.\n2. Per il modello 2, il vincolo di uguaglianza per l'effetto diretto del rendimento sulla delinquenza è rilasciato, ciò che ci si aspettava migliorasse notevolmente l'adattamento rispetto a quello del modello 1.\n3. Assumendo un adattamento globale e locale soddisfacente del modello 2 in entrambi i gruppi, vengono testati due modelli aggiuntivi. Per il modello 3, l'intercetta per la regressione del rendimento su SES, sforzo nel test e QI verbale è vincolata all'uguaglianza\n4. Modello 4: l'intercetta per la regressione della delinquenza su SES, sforzo nel test, QI verbale e rendimento è vincolata all'uguaglianza. Il confronto degli adattamenti relativi dei modelli 3 e 4 con il modello 2 testa l'uguaglianza delle intercette di regressione per le variabili endogene tra i gruppi.\n\nSpecifichiamo i dati.\n\n::: {#823dc6b1 .cell vscode='{\"languageId\":\"r\"}' execution_count=51}\n``` {.r .cell-code}\n# black, n = 214\n# white, n = 181\n# input the correlations in lower diagnonal form\n\nblackLower.cor &lt;- \"\n 1.00\n  .08 1.00\n  .28  .30 1.00\n  .05  .21  .50 1.00\n -.11 -.17 -.26 -.33 1.00 \"\n\nwhiteLower.cor &lt;- \"\n 1.00\n  .25 1.00\n  .37  .40 1.00\n  .27  .28  .61 1.00\n -.11 -.20 -.31 -.21 1.00 \"\n\n# name the variables and convert to full correlation matrix\nblack.cor &lt;- lavaan::getCov(blackLower.cor, names = c(\n    \"ses\", \"effort\", \"viq\",\n    \"achieve\", \"delinq\"\n))\nwhite.cor &lt;- lavaan::getCov(whiteLower.cor, names = c(\n    \"ses\", \"effort\", \"viq\",\n    \"achieve\", \"delinq\"\n))\n\n# display the correlations\nblack.cor\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nses\neffort\nviq\nachieve\ndelinq\n\n\n\n\nses\n1.00\n0.08\n0.28\n0.05\n-0.11\n\n\neffort\n0.08\n1.00\n0.30\n0.21\n-0.17\n\n\nviq\n0.28\n0.30\n1.00\n0.50\n-0.26\n\n\nachieve\n0.05\n0.21\n0.50\n1.00\n-0.33\n\n\ndelinq\n-0.11\n-0.17\n-0.26\n-0.33\n1.00\n\n\n\n\n:::\n\nwhite.cor\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nses\neffort\nviq\nachieve\ndelinq\n\n\n\n\nses\n1.00\n0.25\n0.37\n0.27\n-0.11\n\n\neffort\n0.25\n1.00\n0.40\n0.28\n-0.20\n\n\nviq\n0.37\n0.40\n1.00\n0.61\n-0.31\n\n\nachieve\n0.27\n0.28\n0.61\n1.00\n-0.21\n\n\ndelinq\n-0.11\n-0.20\n-0.31\n-0.21\n1.00\n\n\n\n\n\n\n# add the standard deviations and convert to covariances\nblack.cov &lt;- lavaan::cor2cov(black.cor, sds = c(10.58, 1.35, 13.62, .79, 1.63))\nwhite.cov &lt;- lavaan::cor2cov(white.cor, sds = c(11.53, 1.32, 16.32, .96, 1.45))\n\n# input group means\nblack.mean &lt;- c(31.96, -.01, 93.76, 2.51, 1.40)\nwhite.mean &lt;- c(34.64, .05, 104.18, 2.88, 1.22)\n\n# display the covariances and means\nblack.cov\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nses\neffort\nviq\nachieve\ndelinq\n\n\n\n\nses\n111.936400\n1.142640\n40.347888\n0.417910\n-1.896994\n\n\neffort\n1.142640\n1.822500\n5.516100\n0.223965\n-0.374085\n\n\nviq\n40.347888\n5.516100\n185.504400\n5.379900\n-5.772156\n\n\nachieve\n0.417910\n0.223965\n5.379900\n0.624100\n-0.424941\n\n\ndelinq\n-1.896994\n-0.374085\n-5.772156\n-0.424941\n2.656900\n\n\n\n\n\n\nblack.mean\n\n\n31.96-0.0193.762.511.4\n\n\n\nwhite.cov\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nses\neffort\nviq\nachieve\ndelinq\n\n\n\n\nses\n132.940900\n3.804900\n69.622752\n2.988576\n-1.839035\n\n\neffort\n3.804900\n1.742400\n8.616960\n0.354816\n-0.382800\n\n\nviq\n69.622752\n8.616960\n266.342400\n9.556992\n-7.335840\n\n\nachieve\n2.988576\n0.354816\n9.556992\n0.921600\n-0.292320\n\n\ndelinq\n-1.839035\n-0.382800\n-7.335840\n-0.292320\n2.102500\n\n\n\n\n\n\nwhite.mean\n\n\n34.640.05104.182.881.22\n\n\n\n# specify fit statistics for abbreviated output\nfit.stats &lt;- c(\n    \"chisq\", \"df\", \"pvalue\",\n    \"rmsea\", \"rmsea.ci.lower\", \"rmsea.ci.upper\", \"cfi\", \"srmr\"\n)\n\n# combine covariances matrices, mean vectors, and group sizes\n# into single list objects\ncombined.cov &lt;- list(black = black.cov, white = white.cov)\ncombined.mean &lt;- list(black = black.mean, white = white.mean)\ncombined.n &lt;- list(black = 214, white = 181)\n\nAdattiamo i quattro modelli ai dati:\n\n# specify basic covariance model\nlynam.model &lt;- \"\n achieve ~ ses + effort + viq\n delinq ~ achieve + ses + effort + viq \"\n\n\n# model 1\n# all 7 direct effect constrained to equality\n# over groups\nlynam1 &lt;- lavaan::sem(\n    lynam.model,\n    sample.cov = combined.cov,\n    sample.mean = combined.mean, sample.nobs = combined.n,\n    group.equal = c(\"regressions\"), fixed.x = FALSE, meanstructure = TRUE\n)\n\n\n# model 2\n# direct effects constrained to equality over groups\n# but the constraint on achievement -&gt; delinguency is\n# released in syntax for \"group.partial\"\n# so 6 direct effects are constrained over groups\n\nlynam2 &lt;- lavaan::sem(\n    lynam.model,\n    sample.cov = combined.cov,\n    sample.mean = combined.mean, sample.nobs = combined.n,\n    group.equal = c(\"regressions\"), group.partial = c(\"delinq ~ achieve\"),\n    fixed.x = FALSE, meanstructure = TRUE\n)\n\n\n# model 3\n# retained model\n# 6 direct effects constrained to equality over groups\n# achievement -&gt; delinguency is a free parameter in both groups\n# intercept for achievement constrained to equality\n\nlynam3 &lt;- lavaan::sem(lynam.model,\n    sample.cov = combined.cov,\n    sample.mean = combined.mean, sample.nobs = combined.n,\n    group.equal = c(\"regressions\", \"intercepts\"),\n    group.partial = c(\"delinq ~ achieve\", \"delinq ~ 1\"),\n    fixed.x = FALSE, meanstructure = TRUE\n)\n\n\n# model 4\n# 6 direct effects constrained to equality over groups\n# achievement -&gt; delinguency is a free parameter in both groups\n# intercept for deleinquency constrained to equality\n\nlynam4 &lt;- lavaan::sem(lynam.model,\n    sample.cov = combined.cov,\n    sample.mean = combined.mean, sample.nobs = combined.n,\n    group.equal = c(\"regressions\", \"intercepts\"),\n    group.partial = c(\"delinq ~ achieve\", \"achieve ~ 1\"),\n    fixed.x = FALSE, meanstructure = TRUE\n)\n\nEsaminiamo la bontà di adattamento.\n\n# global fit statistics\nlavaan::fitMeasures(lynam1, fit.stats) |&gt; print()\n\n         chisq             df         pvalue          rmsea rmsea.ci.lower \n        11.736          7.000          0.110          0.059          0.000 \nrmsea.ci.upper            cfi           srmr \n         0.115          0.975          0.036 \n\n\n\nlavaan::fitMeasures(lynam2, fit.stats) |&gt; print()\n\n         chisq             df         pvalue          rmsea rmsea.ci.lower \n         6.107          6.000          0.411          0.010          0.000 \nrmsea.ci.upper            cfi           srmr \n         0.093          0.999          0.029 \n\n\n\nlavaan::fitMeasures(lynam3, fit.stats) |&gt; print()\n\n         chisq             df         pvalue          rmsea rmsea.ci.lower \n         6.409          7.000          0.493          0.000          0.000 \nrmsea.ci.upper            cfi           srmr \n         0.083          1.000          0.030 \n\n\n\nlavaan::fitMeasures(lynam4, fit.stats) |&gt; print()\n\n         chisq             df         pvalue          rmsea rmsea.ci.lower \n        10.237          7.000          0.176          0.048          0.000 \nrmsea.ci.upper            cfi           srmr \n         0.107          0.983          0.033 \n\n\nEseguiamo il test del rapporto tra verosimiglianze.\n\n# chi square difference tests\nlavaan::anova(lynam1, lynam2)\n\n\nA anova: 2 x 8\n\n\n\nDf\nAIC\nBIC\nChisq\nChisq diff\nRMSEA\nDf diff\nPr(&gt;Chisq)\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nlynam2\n6\n9850.196\n9985.478\n6.107287\nNA\nNA\nNA\nNA\n\n\nlynam1\n7\n9853.825\n9985.128\n11.735711\n5.628424\n0.1530851\n1\n0.01767152\n\n\n\n\n\n\nlavaan::anova(lynam2, lynam3)\n\n\nA anova: 2 x 8\n\n\n\nDf\nAIC\nBIC\nChisq\nChisq diff\nRMSEA\nDf diff\nPr(&gt;Chisq)\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nlynam2\n6\n9850.196\n9985.478\n6.107287\nNA\nNA\nNA\nNA\n\n\nlynam3\n7\n9848.498\n9979.801\n6.409026\n0.3017387\n0\n1\n0.5827945\n\n\n\n\n\n\nlavaan::anova(lynam2, lynam4)\n\n\nA anova: 2 x 8\n\n\n\nDf\nAIC\nBIC\nChisq\nChisq diff\nRMSEA\nDf diff\nPr(&gt;Chisq)\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nlynam2\n6\n9850.196\n9985.478\n6.107287\nNA\nNA\nNA\nNA\n\n\nlynam4\n7\n9852.326\n9983.629\n10.236768\n4.129481\n0.1258788\n1\n0.04214226\n\n\n\n\n\nI risultati finora descritti sono coerenti con l’ipotesi che l’effetto diretto non standardizzato del rendimento sulla delinquenza vari in base all’appartenenza al gruppo. Tuttavia, prima di stabilire una scelta definitiva per questo esempio, consideriamo due modelli aggiuntivi. Queste analisi sono più esplorative poiché i modelli di percorso di Lynam et al. (1993) non avevano strutture di media. Il modello 3, rispetto al modello 2, presenta intercette vincolate all’uguaglianza per il rendimento. Si prevedeva che vincolare questo parametro avrebbe leggermente degradato l’adattamento globale rispetto al modello 2, in cui la stessa intercetta è liberamente stimata in entrambi i gruppi. Questo perché il modello 2 con effetti diretti uguali per le cause del rendimento - SES, sforzo nel test e QI verbale - nei due gruppi è generalmente coerente con i dati. I risultati del modello 3 supportano questa previsione.\nPoiché l’intercetta per il rendimento è ugualmente vincolata nel modello 3, la sua intera struttura di media non è più solo identificata; specificamente, ha gradi di libertà (df) = 1, quindi non tutte le medie previste per le variabili endogene, rendimento e delinquenza, corrisponderanno esattamente alle loro controparti osservate. I residui delle medie per le variabili esogene - SES, sforzo nel test e QI verbale - saranno tutti uguali a zero poiché nessun vincolo ha influenzato le loro medie previste. Il modello 3 prevede quindi accuratamente covarianze e medie in entrambi i gruppi, quindi viene mantenuto.\nNel modello 4, l’intercetta per la regressione della delinquenza su SES, sforzo nel test, QI verbale e rendimento è vincolata all’uguaglianza nei gruppi più tutti gli effetti diretti eccetto quello del rendimento sulla delinquenza, che è liberamente stimato in entrambi i campioni. Poiché l’effetto diretto appena menzionato contribuisce all’intercetta per la delinquenza, si prevedeva che vincolare l’intercetta appena menzionata all’uguaglianza avrebbe peggiorato notevolmente l’adattamento rispetto al modello 2. Questa previsione è coerente con i risultati; in particolare, sebbene il modello 4 superi il test chi-quadrato, il suo adattamento è relativamente peggiore rispetto a quello del modello 2. Anche i residui per il modello 4 sono simili a quelli del modello 1; cioè, insoddisfacenti. Pertanto, il modello 4 viene rifiutato.\n\n\n\n\nWu, Hao, e Ryne Estabrook. 2016. «Identification of confirmatory factor analysis models of different levels of invariance for ordered categorical outcomes». Psychometrika 81 (4): 1014–45.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Invarianza di misura</span>"
    ]
  },
  {
    "objectID": "chapters/sem/08_multilevel_sem.html",
    "href": "chapters/sem/08_multilevel_sem.html",
    "title": "46  Modelli di Equazioni Strutturali Multilivello",
    "section": "",
    "text": "46.0.1 Coefficiente di Correlazione Intraclass\nLo scopo dell’analisi SEM Multilivello è quello di calcolare il Coefficiente di Correlazione Intraclasse (ICC). L’ICC è una misura statistica usata in studi dove i dati sono strutturati in gruppi o cluster. L’ICC valuta la somiglianza o l’omogeneità delle misurazioni all’interno dei gruppi. In termini più tecnici, l’ICC quantifica la proporzione di varianza totale nei dati che può essere attribuita alle differenze tra i gruppi. Più alto è l’ICC, maggiore è l’influenza del raggruppamento sulle misurazioni, indicando che una parte sostanziale della varianza osservata nei dati deriva dalle differenze tra i gruppi piuttosto che dalle variazioni individuali all’interno di essi.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Modelli di Equazioni Strutturali Multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/08_multilevel_sem.html#considerazioni-conclusive",
    "href": "chapters/sem/08_multilevel_sem.html#considerazioni-conclusive",
    "title": "46  Modelli di Equazioni Strutturali Multilivello",
    "section": "46.1 Considerazioni Conclusive",
    "text": "46.1 Considerazioni Conclusive\nIn questo capitolo, abbiamo illustrato il modello di equazioni strutturali multilivello utilizzando lavaan. Come evidenziato dall’esempio, l’implementazione in lavaan è molto diretta, richiedendo solo l’inclusione dell’opzione cluster nella funzione sem. È importante sottolineare che, al momento, lavaan supporta solo modelli SEM a due livelli.\nNell’ambito dei modelli SEM multilivello, abbiamo visto come l’interpretazione dei coefficienti di correlazione intra-classe (ICC) possa fornire intuizioni significative sulla variazione dei dati all’interno di gruppi o cluster. Un ICC basso, come quello osservato nell’esempio (0.129), indica che una porzione minore della variazione totale è attribuibile alle differenze tra i cluster. Nel contesto specifico dei nostri dati, dove ogni studente è considerato un cluster individuale, ciò suggerisce che fattori esterni agli studenti stessi potrebbero giocare un ruolo più significativo nella variazione osservata rispetto alle caratteristiche individuali degli studenti.\nIn conclusione, la modellazione di equazioni strutturali multilivello è uno strumento potente e flessibile nell’analisi di dati strutturati gerarchicamente. lavaan, sebbene limitato ai modelli a due livelli, fornisce un approccio accessibile e diretto per questi tipi di analisi. Per modelli più complessi e a più livelli, Mplus e Stata offrono soluzioni alternative che possono gestire una gamma più ampia di esigenze analitiche.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Modelli di Equazioni Strutturali Multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/09_structural_regr.html",
    "href": "chapters/sem/09_structural_regr.html",
    "title": "47  Modelli di Regressione Strutturale",
    "section": "",
    "text": "47.1 Una Applicazione Concreta\nLa {numref}kline-15-3-fig illustra un modello SR (Regressione Strutturale) iniziale che esplora il rendimento scolastico e l’adattamento in aula di studenti di età media corrispondente ai gradi 7-8. Il modello considera l’influenza dell’abilità cognitiva generale e del livello di rischio di disturbi psicopatologici. Uno degli indicatori di rischio deriva dalla diagnosi di disturbi psichiatrici maggiori nei genitori, mentre il secondo è basato sul livello socio-economico (SES) della famiglia, con punteggi più alti che indicano un SES inferiore. Le abilità cognitive sono valutate tramite i punteggi in ragionamento verbale, analisi visivo-spaziale e memoria, ottenuti da un test di QI somministrato individualmente.\nModello iniziale completo di regressione strutturale del rendimento scolastico e dell’adattamento in classe come funzione dell’abilità cognitiva e del rischio di psicopatologia. (Figura tratta da {cite:t}kline2023principles)\n:::\nPrimo Passaggio nella modellazione in due fasi per un modello SR completo di rendimento scolastico e adattamento in classe\n# 4-factor CFA\nworlandCFA.model &lt;- \"\n    Cognitive =~ verbal + visual + memory\n    Achieve =~ read + math + spell\n    Adjust =~ motive + harmony + stable\n    Risk =~ parent + ses \n \"\nworlandCFA &lt;- lavaan::cfa(worlandCFA.model,\n    sample.cov = worland.cov,\n    sample.nobs = 158\n)\nsemPlot::semPaths(worlandCFA,\n    what = \"col\", whatLabels = \"par\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\nfitMeasures(worlandCFA, c(\"chisq\", \"df\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt; print()\n\n chisq     df    cfi    tli  rmsea   srmr \n16.212 38.000  1.000  1.049  0.000  0.023\n# predicted correlation matrix for factors\nlavaan::lavInspect(worlandCFA, \"cor.lv\") |&gt; print()\n\n          Cogntv Achiev Adjust   Risk\nCognitive  1.000                     \nAchieve    0.703  1.000              \nAdjust     0.500  0.751  1.000       \nRisk      -0.459 -0.401 -0.349  1.000\nlavaan::residuals(worlandCFA, type = \"standardized.mplus\") |&gt; print()\n\n$type\n[1] \"standardized.mplus\"\n\n$cov\n        verbal visual memory   read   math  spell motive harmny stable parent\nverbal   0.000                                                               \nvisual   0.000  0.000                                                        \nmemory   0.123 -0.130  0.000                                                 \nread     0.598  0.113 -0.285  0.000                                          \nmath     0.505  0.038 -0.377  0.597  0.000                                   \nspell   -0.952 -0.255 -0.704 -0.667 -0.206  0.000                            \nmotive  -0.821 -0.157  0.310 -0.042 -0.078  1.484  0.000                     \nharmony  0.117 -0.453  0.171  0.942 -1.179  1.196 -1.010  0.000              \nstable   0.285  0.112  0.566 -1.893 -0.467  0.942  0.422  0.436  0.000       \nparent  -0.092 -0.260 -0.157  0.234  0.379  0.157  0.301  0.020 -0.022  0.000\nses      1.890 -0.247 -1.417 -0.370 -0.114 -0.112  0.213 -0.125 -0.362  0.000\n           ses\nverbal        \nvisual        \nmemory        \nread          \nmath          \nspell         \nmotive        \nharmony       \nstable        \nparent        \nses      0.000\nlavaan::residuals(worlandCFA, type = \"cor.bollen\") |&gt; print()\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n        verbal visual memory   read   math  spell motive harmny stable parent\nverbal   0.000                                                               \nvisual   0.000  0.000                                                        \nmemory   0.001 -0.002  0.000                                                 \nread     0.015  0.003 -0.010  0.000                                          \nmath     0.017  0.001 -0.016  0.007  0.000                                   \nspell   -0.040 -0.012 -0.035 -0.010 -0.006  0.000                            \nmotive  -0.031 -0.007  0.016 -0.001 -0.003  0.075  0.000                     \nharmony  0.006 -0.026  0.010  0.041 -0.053  0.071 -0.027  0.000              \nstable   0.011  0.005  0.028 -0.034 -0.015  0.045  0.005  0.012  0.000       \nparent  -0.003 -0.010 -0.007  0.008  0.017  0.009  0.015  0.001 -0.001  0.000\nses      0.050 -0.010 -0.068 -0.012 -0.005 -0.006  0.010 -0.008 -0.016  0.000\n           ses\nverbal        \nvisual        \nmemory        \nread          \nmath          \nspell         \nmotive        \nharmony       \nstable        \nparent        \nses      0.000\n# calculate factor reliability coefficients (semTools)\nsemTools::reliability(worlandCFA) |&gt; print()\n\n       Cognitive   Achieve    Adjust      Risk\nalpha  0.8463118 0.8087788 0.7249416 0.5675488\nomega  0.8513712 0.8207835 0.7296141 0.5770344\nomega2 0.8513712 0.8207835 0.7296141 0.5770344\nomega3 0.8515872 0.8227890 0.7327431 0.5770344\navevar 0.6588592 0.6101369 0.4744677 0.4094519\nSecondo Passaggio nella modellazione in due fasi per un modello SR completo di rendimento scolastico e adattamento in classe\nI risultati del Passaggio 1 del metodo in due fasi, che si concentrava sul modello di misurazione, consentono di procedere all’analisi del modello SR originale, che prevede cinque percorsi nella {numref}kline-15-3-fig, nel Passaggio 2 del metodo. Anche questa seconda analisi ha portato a una soluzione ammissibile.\n# step 2a\n# 4-factor SR model with 5 paths among factors\n\n# by default, lavaan frees the disturbance covariance\n# between a pair of outcomes in a structural model\n# when there is no direct effect between them\n# thus, this parameter is explicitly fixed to zero\n# in this analysis\n\nworlandSRa_model &lt;- \"\n    # measurement part\n    Cognitive =~ verbal + visual + memory\n    Achieve =~ read + math + spell\n    Adjust =~ motive + harmony + stable\n    Risk =~ parent + ses\n    # structural part (5 paths)\n    Achieve ~ Cognitive + Risk\n    Adjust ~ Cognitive + Risk\n    # constrain disturbance covariance to zero\n    Adjust ~~ 0*Achieve \n\"\nworlandSRa &lt;- lavaan::sem(worlandSRa_model,\n    sample.cov = worland.cov,\n    sample.nobs = 158\n)\nsemPlot::semPaths(worlandSRa,\n    what = \"col\", whatLabels = \"par\", style = \"mx\",\n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\nfitMeasures(worlandSRa, c(\"chisq\", \"df\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt; print()\n\n chisq     df    cfi    tli  rmsea   srmr \n49.747 39.000  0.983  0.976  0.042  0.074\nlavaan::residuals(worlandSRa, type = \"standardized.mplus\") |&gt; print()\n\n$type\n[1] \"standardized.mplus\"\n\n$cov\n        verbal visual memory   read   math  spell motive harmny stable parent\nverbal   0.135                                                               \nvisual   0.892  0.122                                                        \nmemory   0.670  0.193  0.098                                                 \nread    -0.270 -0.710 -1.047  0.009                                          \nmath    -0.004 -0.498 -0.910 -0.340     NA                                   \nspell   -1.008 -0.332 -0.808  0.034  0.202  0.030                            \nmotive  -1.540 -0.721 -0.155  3.466  3.022  3.348  0.034                     \nharmony -0.289 -0.858 -0.173  2.903  1.340  2.579 -0.244  0.057              \nstable  -1.412 -1.235 -0.441  2.917  2.763  2.910  0.169  0.092  0.050       \nparent  -0.309 -0.394 -0.260  0.960  0.849  0.340  0.625  0.244  0.702     NA\nses      1.413 -0.337 -1.439  0.567  0.478  0.135  0.653  0.157  0.546  1.467\n           ses\nverbal        \nvisual        \nmemory        \nread          \nmath          \nspell         \nmotive        \nharmony       \nstable        \nparent        \nses         NA\nlavaan::residuals(worlandSRa, type = \"cor.bollen\") |&gt; print()\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n        verbal visual memory   read   math  spell motive harmny stable parent\nverbal   0.000                                                               \nvisual   0.013  0.000                                                        \nmemory   0.011  0.004  0.000                                                 \nread    -0.006 -0.019 -0.032  0.000                                          \nmath     0.000 -0.017 -0.034 -0.002  0.000                                   \nspell   -0.042 -0.016 -0.040  0.000  0.005  0.000                            \nmotive  -0.054 -0.031 -0.008  0.219  0.197  0.242  0.000                     \nharmony -0.015 -0.048 -0.010  0.199  0.090  0.192 -0.007  0.000              \nstable  -0.041 -0.045 -0.019  0.170  0.171  0.201  0.001  0.002  0.000       \nparent  -0.011 -0.017 -0.013  0.031  0.037  0.020  0.031  0.015  0.029  0.000\nses      0.044 -0.014 -0.071  0.016  0.020  0.008  0.030  0.010  0.020  0.012\n           ses\nverbal        \nvisual        \nmemory        \nread          \nmath          \nspell         \nmotive        \nharmony       \nstable        \nparent        \nses      0.000\nSebbene gli indici di fit siano buoni, l’adattamento locale del modello con cinque percorsi tra i fattori è scarso. Ad esempio, i residui standardizzati per diverse coppie di indicatori dei fattori di rendimento e adattamento hanno spesso un valore maggiore di 2:\nBasandosi su tutti questi risultati relativi all’adattamento globale e locale, il modello SR iniziale nella {numref}kline-15-3-fig con cinque percorsi tra i fattori è rifiutato.\nEsaminiamo i modification indices.\nmodificationIndices(worlandSRa, sort = TRUE, minimum.value = 5)\n\n\nA lavaan.data.frame: 6 x 8\n\n\n\nlhs\nop\nrhs\nmi\nepc\nsepc.lv\nsepc.all\nsepc.nox\n\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n126\nAdjust\n~\nAchieve\n31.938548\n0.4553494\n0.8558291\n0.8558291\n0.8558291\n\n\n16\nAchieve\n~~\nAdjust\n31.938547\n34.8789498\n0.7039273\n0.7039273\n0.7039273\n\n\n125\nAchieve\n~\nAdjust\n31.938539\n1.0882050\n0.5789865\n0.5789865\n0.5789865\n\n\n44\nAchieve\n=~\nmotive\n5.438535\n0.1491767\n1.9561758\n0.2065710\n0.2065710\n\n\n54\nAdjust\n=~\nspell\n5.240735\n0.3894844\n2.7174077\n0.1968289\n0.1968289\n\n\n52\nAdjust\n=~\nread\n5.038370\n0.3467951\n2.4195674\n0.1629036\n0.1629036\nI risultati dei modification indices mostrano che l’assenza di un percorso tra i fattori di rendimento e adattamento nella {numref}kline-15-3-fig è chiaramente incoerente con i dati. Per aggiungere una covariazione tra i fattori di rendimento e adattamento abbiamo due opzioni: o aggiungere un effetto diretto tra i fattori o permettere alle loro perturbazioni di covariare. Ma sarebbe difficile giustificare un effetto diretto rispetto all’altro: scarse abilità scolastiche potrebbero peggiorare l’adattamento in classe tanto quanto i problemi comportamentali a scuola potrebbero influire negativamente sul rendimento. La specificazione di una causalità reciproca tra Rendimento e Adattamento renderebbe il modello strutturale non ricorsivo, ma il modello non sarebbe identificato senza imporre vincoli irrealistici. Riformuliamo dunque il modello della {numref}kline-15-3-fig permettendo alle perturbazioni tra i fattori di rendimento e adattamento di covariare.\n# step 2b\n# 4-factor SR model with 6 paths among factors\n# this model is equivalent to the basic 4-factor\n# CFA measurement model analyzed in step 1\n\nworlandSRb_model &lt;- \"\n    # measurement part\n    Cognitive =~ verbal + visual + memory\n    Achieve =~ read + math + spell\n    Adjust =~ motive + harmony + stable\n    Risk =~ parent + ses\n    # structural part (6 paths)\n    Achieve ~ Cognitive + Risk\n    Adjust ~ Cognitive + Risk\n    Adjust ~~ Achieve \n\"\nworlandSRb &lt;- lavaan::sem(worlandSRb_model,\n    sample.cov = worland.cov,\n    sample.nobs = 158\n)\nsemPlot::semPaths(worlandSRb,\n    what = \"col\", whatLabels = \"par\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\nparameterEstimates(worlandSRb) |&gt; print()\n\n         lhs op       rhs     est     se      z pvalue ci.lower ci.upper\n1  Cognitive =~    verbal   1.000  0.000     NA     NA    1.000    1.000\n2  Cognitive =~    visual   1.000  0.090 11.144  0.000    0.824    1.175\n3  Cognitive =~    memory   0.788  0.077 10.217  0.000    0.637    0.940\n4    Achieve =~      read   1.000  0.000     NA     NA    1.000    1.000\n5    Achieve =~      math   0.925  0.083 11.100  0.000    0.761    1.088\n6    Achieve =~     spell   0.678  0.080  8.480  0.000    0.521    0.835\n7     Adjust =~    motive   1.000  0.000     NA     NA    1.000    1.000\n8     Adjust =~   harmony   0.861  0.136  6.311  0.000    0.593    1.128\n9     Adjust =~    stable   0.940  0.114  8.231  0.000    0.716    1.164\n10      Risk =~    parent   1.000  0.000     NA     NA    1.000    1.000\n11      Risk =~       ses   0.773  0.224  3.445  0.001    0.333    1.212\n12   Achieve  ~ Cognitive   0.719  0.109  6.574  0.000    0.504    0.933\n13   Achieve  ~      Risk  -0.175  0.190 -0.922  0.357   -0.548    0.198\n14    Adjust  ~ Cognitive   0.261  0.070  3.743  0.000    0.124    0.398\n15    Adjust  ~      Risk  -0.146  0.127 -1.153  0.249   -0.395    0.102\n16   Achieve ~~    Adjust  36.374  7.967  4.565  0.000   20.758   51.989\n17    verbal ~~    verbal  46.253  9.782  4.728  0.000   27.080   65.426\n18    visual ~~    visual  76.171 12.153  6.268  0.000   52.351   99.991\n19    memory ~~    memory  69.732  9.717  7.177  0.000   50.688   88.776\n20      read ~~      read  51.273 11.238  4.562  0.000   29.246   73.299\n21      math ~~      math  86.341 13.049  6.616  0.000   60.764  111.917\n22     spell ~~     spell 112.797 14.078  8.012  0.000   85.205  140.389\n23    motive ~~    motive  37.737  6.463  5.839  0.000   25.071   50.404\n24   harmony ~~   harmony  83.953 10.590  7.927  0.000   63.197  104.709\n25    stable ~~    stable  29.306  5.386  5.441  0.000   18.749   39.863\n26    parent ~~    parent  88.005 18.343  4.798  0.000   52.053  123.958\n27       ses ~~       ses  38.896 10.208  3.810  0.000   18.889   58.904\n28 Cognitive ~~ Cognitive 141.617 22.098  6.409  0.000   98.307  184.928\n29   Achieve ~~   Achieve  84.317 16.324  5.165  0.000   52.322  116.312\n30    Adjust ~~    Adjust  38.008  8.188  4.642  0.000   21.960   54.055\n31      Risk ~~      Risk  55.079 19.989  2.755  0.006   15.902   94.257\n32 Cognitive ~~      Risk -40.517 12.448 -3.255  0.001  -64.914  -16.119\nfitMeasures(worlandSRb, c(\"chisq\", \"df\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt; print()\n\n chisq     df    cfi    tli  rmsea   srmr \n16.212 38.000  1.000  1.049  0.000  0.023\n# predicted correlation matrix for factors\nlavaan::lavInspect(worlandSRb, \"cor.lv\") |&gt; print()\n\n          Cogntv Achiev Adjust   Risk\nCognitive  1.000                     \nAchieve    0.703  1.000              \nAdjust     0.500  0.751  1.000       \nRisk      -0.459 -0.401 -0.349  1.000\nlavaan::residuals(worlandSRb, type = \"standardized.mplus\") |&gt; print()\n\n$type\n[1] \"standardized.mplus\"\n\n$cov\n        verbal visual memory   read   math  spell motive harmny stable parent\nverbal      NA                                                               \nvisual  -0.002     NA                                                        \nmemory   0.122 -0.131     NA                                                 \nread     0.597  0.113 -0.285     NA                                          \nmath     0.504  0.038 -0.377  0.597     NA                                   \nspell   -0.952 -0.255 -0.704 -0.667 -0.206     NA                            \nmotive  -0.821 -0.157  0.309 -0.042 -0.078  1.484     NA                     \nharmony  0.117 -0.453  0.171  0.942 -1.179  1.196 -1.010     NA              \nstable   0.284  0.112  0.566 -1.893 -0.467  0.942  0.422  0.436     NA       \nparent  -0.093 -0.261 -0.157  0.234  0.378  0.157  0.300  0.019 -0.022  0.034\nses      1.891 -0.247 -1.417 -0.370 -0.114 -0.112  0.213 -0.125 -0.362  0.043\n           ses\nverbal        \nvisual        \nmemory        \nread          \nmath          \nspell         \nmotive        \nharmony       \nstable        \nparent        \nses      0.017\nlavaan::residuals(worlandSRb, type = \"cor.bollen\") |&gt; print()\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n        verbal visual memory   read   math  spell motive harmny stable parent\nverbal   0.000                                                               \nvisual   0.000  0.000                                                        \nmemory   0.001 -0.002  0.000                                                 \nread     0.015  0.003 -0.010  0.000                                          \nmath     0.017  0.001 -0.016  0.007  0.000                                   \nspell   -0.040 -0.012 -0.036 -0.010 -0.006  0.000                            \nmotive  -0.031 -0.007  0.016 -0.001 -0.003  0.075  0.000                     \nharmony  0.006 -0.026  0.010  0.041 -0.053  0.071 -0.027  0.000              \nstable   0.011  0.005  0.028 -0.034 -0.015  0.045  0.005  0.012  0.000       \nparent  -0.003 -0.011 -0.007  0.008  0.017  0.009  0.015  0.001 -0.001  0.000\nses      0.050 -0.010 -0.068 -0.012 -0.005 -0.006  0.010 -0.008 -0.016  0.000\n           ses\nverbal        \nvisual        \nmemory        \nread          \nmath          \nspell         \nmotive        \nharmony       \nstable        \nparent        \nses      0.000\nConfrontiamo i due modelli con il test del rapporto di verosimiglianza.\nlavTestLRT(worlandSRa, worlandSRb)\n\n\nA anova: 2 x 8\n\n\n\nDf\nAIC\nBIC\nChisq\nChisq diff\nRMSEA\nDf diff\nPr(&gt;Chisq)\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nworlandSRb\n38\n12938.16\n13023.92\n16.21231\nNA\nNA\nNA\nNA\n\n\nworlandSRa\n39\n12969.70\n13052.39\n49.74654\n33.53423\n0.4537762\n1\n7.002091e-09\nL’adattamento del modello SR con 5 percorsi tra i fattori è significativamente peggiore rispetto a quello del modello CFA con 6 percorsi. Gli indici di fit del modello con 6 percorsi sono buoni così come il suo adattamento locale.\n# standardized estimates with standard errors\nlavaan::standardizedSolution(worlandSRb) |&gt; print()\n\n         lhs op       rhs est.std    se      z pvalue ci.lower ci.upper\n1  Cognitive =~    verbal   0.868 0.032 27.075  0.000    0.805    0.931\n2  Cognitive =~    visual   0.806 0.037 21.720  0.000    0.733    0.879\n3  Cognitive =~    memory   0.747 0.043 17.470  0.000    0.663    0.831\n4    Achieve =~      read   0.876 0.031 28.212  0.000    0.815    0.937\n5    Achieve =~      math   0.791 0.038 20.777  0.000    0.717    0.866\n6    Achieve =~     spell   0.639 0.053 11.984  0.000    0.534    0.743\n7     Adjust =~    motive   0.761 0.049 15.552  0.000    0.665    0.857\n8     Adjust =~   harmony   0.561 0.065  8.657  0.000    0.434    0.688\n9     Adjust =~    stable   0.781 0.048 16.381  0.000    0.688    0.875\n10      Risk =~    parent   0.620 0.100  6.217  0.000    0.425    0.816\n11      Risk =~       ses   0.677 0.104  6.495  0.000    0.473    0.881\n12   Achieve  ~ Cognitive   0.657 0.079  8.357  0.000    0.503    0.811\n13   Achieve  ~      Risk  -0.100 0.107 -0.935  0.350   -0.310    0.110\n14    Adjust  ~ Cognitive   0.431 0.103  4.179  0.000    0.229    0.633\n15    Adjust  ~      Risk  -0.151 0.127 -1.186  0.236   -0.400    0.098\n16   Achieve ~~    Adjust   0.643 0.085  7.592  0.000    0.477    0.808\n17    verbal ~~    verbal   0.246 0.056  4.421  0.000    0.137    0.355\n18    visual ~~    visual   0.350 0.060  5.847  0.000    0.233    0.467\n19    memory ~~    memory   0.442 0.064  6.920  0.000    0.317    0.567\n20      read ~~      read   0.232 0.054  4.271  0.000    0.126    0.339\n21      math ~~      math   0.374 0.060  6.197  0.000    0.255    0.492\n22     spell ~~     spell   0.592 0.068  8.686  0.000    0.458    0.725\n23    motive ~~    motive   0.421 0.074  5.649  0.000    0.275    0.567\n24   harmony ~~   harmony   0.686 0.073  9.445  0.000    0.543    0.828\n25    stable ~~    stable   0.390 0.075  5.229  0.000    0.244    0.536\n26    parent ~~    parent   0.615 0.124  4.967  0.000    0.372    0.858\n27       ses ~~       ses   0.542 0.141  3.840  0.000    0.265    0.818\n28 Cognitive ~~ Cognitive   1.000 0.000     NA     NA    1.000    1.000\n29   Achieve ~~   Achieve   0.498 0.077  6.470  0.000    0.347    0.649\n30    Adjust ~~    Adjust   0.732 0.080  9.108  0.000    0.574    0.889\n31      Risk ~~      Risk   1.000 0.000     NA     NA    1.000    1.000\n32 Cognitive ~~      Risk  -0.459 0.098 -4.686  0.000   -0.651   -0.267\nLa correlazione stimata tra i fattori esogeni abilità cognitiva e rischio, –.459, è sensata: è negativa (più alto il rischio, minore l’abilità cognitiva). Questa correlazione non è prossima a -1.0, il che suggerisce che i due fattori sono distinti e non quasi identici, confermando così l’ipotesi di validità discriminante.\nAnalizzando gli impatti specifici, un incremento di un punto nel fattore cognitivo (misurato come varianza comune del ragionamento verbale) prevede un aumento di .719 punti nel rendimento scolastico (misurato come varianza comune dell’abilità di lettura), tenendo conto del fattore di rischio. In termini standardizzati, un aumento di una deviazione standard nell’abilità cognitiva si traduce in un aumento di .657 deviazioni standard nel rendimento scolastico, sempre controllando per il rischio.\nL’influenza del rischio sul rendimento scolastico è meno marcata: un incremento di un punto nel rischio (misurato come varianza comune del disturbo genitoriale) prevede una diminuzione di .175 punti nel rendimento scolastico. Standardizzando, un aumento di una deviazione standard nel rischio si associa a una diminuzione di .100 deviazioni standard nel rendimento, controllando per l’abilità cognitiva.\nLa correlazione di perturbazione di .643 misura la relazione tra il rendimento scolastico e l’adattamento in classe, dopo aver escluso l’influenza di altri fattori noti, in questo caso l’abilità cognitiva e il rischio di psicopatologia. In termini più semplici, la correlazione di perturbazione ci dice quanto sono correlati il rendimento scolastico e l’adattamento in classe quando si tiene conto (o si “controlla”) dell’effetto dell’abilità cognitiva e del rischio. Un valore di .643 indica una correlazione moderatamente forte, suggerendo che quando il rendimento scolastico di uno studente migliora (o peggiora), anche il suo adattamento in classe tende a migliorare (o peggiorare) in modo simile, indipendentemente dal suo livello di abilità cognitiva o dal grado di rischio di psicopatologia.\nLa presenza di questa correlazione parziale sostanziale implica che ci sono fattori non misurati nel modello che influenzano sia il rendimento scolastico sia l’adattamento in classe. Questi fattori non misurati potrebbero includere variabili come il sostegno familiare, la qualità dell’insegnamento, fattori ambientali o personalità dello studente. Importante è che questi fattori non misurati sono distinti sia dall’abilità cognitiva dello studente sia dal suo rischio di psicopatologia. In conclusione, il valore di .643 non solo mette in luce l’interdipendenza tra rendimento scolastico e adattamento in classe, ma suggerisce anche l’esistenza di altre variabili influenti che non sono state direttamente misurate o incluse nel modello. Questa informazione può essere preziosa per indirizzare ulteriori ricerche o interventi educativi.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Modelli di Regressione Strutturale</span>"
    ]
  },
  {
    "objectID": "chapters/sem/09_structural_regr.html#una-applicazione-concreta",
    "href": "chapters/sem/09_structural_regr.html#una-applicazione-concreta",
    "title": "47  Modelli di Regressione Strutturale",
    "section": "",
    "text": "```iafxquid ../images/kline_15_3.png\n\n\n\n\nheight: 600px\n\n\nname: kline-15-3-fig\n\n\n\n\n\nIl modello comprende due fattori endogeni: il rendimento scolastico, valutato attraverso test standardizzati di lettura, aritmetica e ortografia, e l'adattamento in classe, misurato con tre indicatori forniti dagli insegnanti riguardo alla motivazione, stabilità emotiva e qualità delle relazioni sociali degli studenti. In questo modello strutturale, sia il rendimento scolastico sia l'adattamento in classe sono influenzati dall'abilità cognitiva e dal rischio, ma non vi è un effetto diretto o una covarianza delle perturbazioni tra questi due fattori endogeni, indicando che eventuali associazioni tra di essi sono attribuibili alle loro cause comuni, i fattori esogeni.\n\n::: {#cell-6 .cell vscode='{\"languageId\":\"r\"}' execution_count=2}\n``` {.r .cell-code}\n# input the correlations in lower diagnonal form\nworlandLower.cor &lt;- \"\n1.00\n .70 1.00\n .65  .60 1.00\n .55  .50  .45 1.00\n .50  .45  .40  .70 1.00\n .35  .35  .30  .55  .50 1.00\n .30  .30  .30  .50  .45  .44 1.00\n .25  .20  .22  .41  .28  .34  .40 1.00\n .35  .32  .32  .48  .45  .42  .60  .45 1.00\n-.25 -.24 -.22 -.21 -.18 -.15 -.15 -.12 -.17 1.00\n-.22 -.26 -.30 -.25 -.22 -.18 -.17 -.14 -.20  .42 1.00 \"\n\n# name the variables and convert to full correlation matrix\nworland.cor &lt;- lavaan::getCov(worlandLower.cor, names = c(\n    \"verbal\", \"visual\",\n    \"memory\", \"read\", \"math\", \"spell\", \"motive\", \"harmony\", \"stable\", \"parent\", \"ses\"\n))\n\n# add the standard deviations and convert to covariances\nworland.cov &lt;- lavaan::cor2cov(worland.cor,\n    sds = c(\n        13.75, 14.80, 12.60, 14.90, 15.25, 13.85, 9.50, 11.10, 8.70,\n        12.00, 8.50\n    )\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLettura, Motivazione, 3.466\nOrtografia, Motivazione, 3.348\nLettura, Armonia, 2.903",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Modelli di Regressione Strutturale</span>"
    ]
  },
  {
    "objectID": "chapters/sem/09_structural_regr.html#strategie-avanzate-di-modellazione-per-i-modelli-di-regressione-strutturale",
    "href": "chapters/sem/09_structural_regr.html#strategie-avanzate-di-modellazione-per-i-modelli-di-regressione-strutturale",
    "title": "47  Modelli di Regressione Strutturale",
    "section": "47.2 Strategie Avanzate di Modellazione per i Modelli di Regressione Strutturale",
    "text": "47.2 Strategie Avanzate di Modellazione per i Modelli di Regressione Strutturale\nOltre all’approccio tradizionale di modellazione in due fasi, esiste un metodo più complesso a quattro fasi per analizzare i modelli SR completi. Questa strategia, introdotta da Mulaik e Millsap nel 2000, amplia la modellazione bifase aggiungendo ulteriori analisi esplorative che possono portare a conclusioni più definitive in una serie più estesa di studi. Questo metodo prevede che ogni fattore comune abbia almeno quattro indicatori, numero ritenuto sufficiente per testare l’unidimensionalità con il test dell’annullamento della tetrade. I quattro indicatori rappresentano anche il numero minimo perché un modello CFA a singolo fattore sia considerato sovraidentificato. Il ricercatore testa quindi una serie di almeno quattro modelli gerarchicamente correlati, seguendo questi passaggi:\n\nPrimo Passaggio: Il modello iniziale meno restrittivo è un modello EFA, dove ogni indicatore satura su tutti i fattori. Il numero di fattori è lo stesso dei modelli analizzati nei passaggi successivi. Questo modello viene analizzato con lo stesso metodo di stima utilizzato nei passaggi successivi, ad esempio il metodo ML per indicatori continui e normalmente distribuiti. Alternativamente, si possono usare tecniche come ESEM o E/CFA al posto dell’EFA. Questo passaggio serve a testare la correttezza provvisoria delle ipotesi riguardo al numero di fattori.\nSecondo Passaggio: Corrisponde al Primo Passaggio della modellazione bifase. Qui, si specifica un modello CFA con alcuni carichi incrociati fissati a zero, identificando gli indicatori che non dipendono da certi fattori comuni. Se l’adattamento del modello CFA è ragionevole, si può procedere al test del modello SR; in caso contrario, il modello di misurazione va rivisto.\nTerzo Passaggio: Si specifica il modello SR target con lo stesso schema di carichi incrociati fissati a zero del modello CFA del Secondo Passaggio. Tipicamente, la parte strutturale del modello SR include meno effetti diretti rispetto al totale delle covarianze tra fattori nel modello CFA. Se la parte strutturale del modello SR ha tanti percorsi quanti il modello CFA, i due modelli saranno equivalenti e questo passaggio può essere omesso.\nQuarto Passaggio: Coinvolge test su ipotesi specifiche sui parametri definiti dall’inizio del processo. Questi test possono comportare l’applicazione di vincoli zero o altri, aumentando di uno dfM. I Passaggi 3 e 4 della modellazione a quattro fasi rappresentano una precisazione delle attività generali del Secondo Passaggio della modellazione bifase.\n\nUna delle critiche alla modellazione a quattro fasi riguarda la necessità di avere almeno quattro indicatori per fattore, condizione non sempre pratica o desiderabile, specialmente quando pochi indicatori, o anche un singolo indicatore ottimale, presentano migliori caratteristiche psicometriche rispetto a quattro. Tuttavia, Mulaik e Millsap hanno osservato che avere almeno quattro indicatori può compensare, in parte, le limitazioni di un campione più piccolo incrementando dfM.\nEntrambi gli approcci, bifase e quattro fasi, sfruttano la variazione casuale quando i modelli vengono testati e riformulati utilizzando gli stessi dati, e sono considerati migliori della modellazione monofase, dove non esiste una distinzione tra questioni di misurazione e struttura.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Modelli di Regressione Strutturale</span>"
    ]
  },
  {
    "objectID": "chapters/sem/10_missing_data.html",
    "href": "chapters/sem/10_missing_data.html",
    "title": "48  Dati mancanti",
    "section": "",
    "text": "48.1 Tipologie di dati mancanti\nCi sono molti motivi che possono stare alla base dei dati mancanti. Ad esempio, i dati possono mancare per disegno dello studio (“mancanza pianificata”), come ad esempio nei progetti di ricerca in cui i partecipanti al campione vengono selezionati casualmente per completare sottoinsiemi diversi della batteria di valutazione (una scelta di questo tipo viene motivata, ad esempio, a causa di considerazioni pratiche come i vincoli di tempo). In tali condizioni, si presume che i dati mancanti si distribuiscano in un modo completamente casuale rispetto a tutte le altre variabili nello studio.\nIn generale, i meccanismi che determinano la presenza di dati mancanti possono essere classificati in tre categorie:",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Dati mancanti</span>"
    ]
  },
  {
    "objectID": "chapters/sem/10_missing_data.html#tipologie-di-dati-mancanti",
    "href": "chapters/sem/10_missing_data.html#tipologie-di-dati-mancanti",
    "title": "48  Dati mancanti",
    "section": "",
    "text": "valori mancanti completamente casuali (Missing Completely At Random, MCAR). La probabilità di dati mancanti su una variabile non è collegata né al valore mancante sulla variabile, né al valore di ogni altra variabile presente nella matrice dati che si sta analizzando;\nvalori mancanti casuali (Missing At Random, MAR). I valori mancanti sono indipendenti dal valore che viene a mancare, ma dipendono da altre variabili, cioè i dati sulla variabile sono mancanti per categorie di partecipanti che potrebbero essere identificati dai valori assunti dalle altre variabili presenti nello studio;\nvalori mancanti non ignorabili (Missing Not At Random, MNAR). La mancanza di un dato può dipendere sia dal valore del dato stesso che dalle altre variabili. Per esempio, se si studia la salute mentale e le persone depresse riferiscono meno volentieri informazioni riguardanti il loro stato di salute, allora i dati non sono mancanti per caso.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Dati mancanti</span>"
    ]
  },
  {
    "objectID": "chapters/sem/10_missing_data.html#la-gestione-dei-dati-mancanti",
    "href": "chapters/sem/10_missing_data.html#la-gestione-dei-dati-mancanti",
    "title": "48  Dati mancanti",
    "section": "48.2 La gestione dei dati mancanti",
    "text": "48.2 La gestione dei dati mancanti\nIl passo successivo dopo la definizione dei meccanismi è quello della gestione dei dati mancanti. Sostanzialmente le scelte possibili sono due: l’eliminazione dei casi o la sostituzione dei dati mancanti. Un metodo semplice, indicato solo nel caso in cui l’ammontare dei dati mancanti è limitato e questi sono mancanti completamente a caso (MCAR), è quello di rimuovere i casi con dati mancanti (case deletion).\nCi sono due metodi per eliminare le osservazioni con valori mancanti: listwise deletion e pairwise deletion. Nel primo caso si elimina dal campione ogni osservazione che contiene dati mancanti. Le analisi avverranno quindi solo sui casi che hanno valori validi su tutte le variabili in esame. In questo modo si ottiene una maggiore semplicità di trattazione nell’analisi statistica, tuttavia non si utilizza tutta l’informazione osservata (si riduce la numerosità campionaria e, quindi, l’informazione). Il secondo metodo è la pairwise deletion, che utilizza tutti i casi che hanno i dati validi su due variabili volta per volta. In questo modo si riesce a massimizzare la numerosità del campione da utilizzare, ma si tratta comunque di un metodo che presenta dei problemi, per esempio il fatto che con questo approccio i parametri del modello saranno basati su differenti insiemi di dati, con differenti numerosità campionarie e differenti errori standard.\nQuando i dati non sono MNAR è opportuno sostituirli con appropriate funzioni dei dati effettivamente osservati. Questa procedura è chiamata imputazione (imputation). Di seguito sono indicati alcuni metodi.\n\nMean Imputation. Il dato mancante viene sostituito con la media della variabile. Questo metodo, utilizzato troppo spesso per la sua semplicità, riducendo la variabilità dei dati, ha effetti importanti su molte analisi dei dati e, in generale, dovrebbe essere evitato.\nRegression Imputation. Si tratta di un approccio basato sulle informazioni disponibili sulle altre variabili. Si stima una equazione di regressione lineare per ogni variabile utilizzando le altre variabili come predittori. Questo metodo offre il vantaggio di poter utilizzare i rapporti esistenti tra le variabili per effettuare le valutazioni dei dati mancanti; tuttavia esso è usato raramente, in quanto amplifica le correlazioni tra le variabili; quindi, se le analisi si basano su regressioni o modelli SEM, questo metodo è sconsigliato.\nMultiple Imputation. La tecnica di multiple imputation, applicabile in caso di MAR, prevede che un dato mancante su una variabile sia sostituito, sulla base dei dati esistenti sulle altre variabili, con un valore che però comprende anche una componente di errore ricavata dalla distribuzione dei residui della variabile.\nExpectation-Maximization. Un altro approccio moderno del trattamento dei dati mancanti è l’applicazione dell’algoritmo Expectation Maximization (EM). La tecnica è quella di stimare i parametri sulla base dei dati osservati, e di stimare poi i dati mancanti sulla base di questi parametri (fase E). Poi i parametri vengono nuovamente stimati sulla base della nuova matrice di dati (fase M), e così via. Questo processo viene iterato fino a quando i valori stimati convergono. Tuttavia, una limitazione fondamentale dell’utilizzo dell’algoritmo EM per calcolare le matrici di input per le analisi CFA/SEM è che gli errori standard risultanti delle stime dei parametri non sono consistenti. Pertanto, gli intervalli di confidenza e i test di significatività possono risultare compromessi.\n\n\n48.2.1 Metodo Direct ML\nBenché i metodi precedenti vengano spesso usati, nella pratica concreta è preferibile usare il metodo Direct ML, conosciuto anche come “raw ML” o “full information ML” (FIML), in quanto è generalmente considerano come il metodo migliore per gestire i dati mancanti nella maggior parte delle applicazioni CFA e SEM. Il metodo full information ML è esente dai problemi associati all’utilizzo dell’algoritmo EM e produce stime consistenti sotto l’ipotesi di normalità multivariata per dati mancanti MAR.\nIntuitivamente, l’approccio utilizza la relazione tra le variabili per dedurre quali siano i valori mancanti con maggiore probabilità. Ad esempio, se due variabili, \\(X\\) e \\(Y\\), sono correlate positivamente, allora se, per alcune osservazioni \\(i\\), \\(X_i\\) è il valore più alto nella variabile, è probabile che anche il valore mancante \\(Y_i\\) sia un valore alto. FIML utilizza queste informazioni senza procedere all’imputazione dei valori mancanti, ma invece basandosi sulle stime più verosimili dei parametri della popolazione, ovvero massimizzando direttamente la verosimiglianza del modello specificato. Sotto l’assunzione di normalità multivariata, la funzione di verosimiglianza diventa\n\\[\nL(\\mu, \\Sigma) = \\prod_i f(y_i \\mid \\mu_i, \\Sigma_i),\n\\]\ndove \\(y_i\\) sono i dati, \\(\\mu_i\\) e \\(\\Sigma_i\\) sono i parametri della popolazione se gli elementi mancanti in \\(y_i\\) vengono rimossi. Si cercano i valori \\(\\mu\\) e \\(\\Sigma\\) che massimizzano la verosimiglianza.\nIn lavaan l’applicazione di tale metodo si ottiene specificando l’argomento missing = \"ml\".\n\n\n48.2.2 Un esempio concreto\nPer applicare il metodo direct ML, {cite:t}brown2015confirmatory prende in esame i dati reali di un questionario (un singolo fattore, quattro item, una covarianza di errore) caratterizzato dalla presenza di dati mancanti. Importiamo i dati in R:\n\nd &lt;- rio::import(here::here(\"data\", \"brown_table_9_1.csv\"))\nhead(d)\n\n\nA data.frame: 6 x 5\n\n\n\nsubject\ns1\ns2\ns3\ns4\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n5760\n2\n0\n1\nNA\n\n\n2\n5761\n3\n3\n3\nNA\n\n\n3\n5763\n2\n4\n4\nNA\n\n\n4\n5761\n2\n0\n0\nNA\n\n\n5\n5769\n2\n1\n1\nNA\n\n\n6\n5771\n4\n3\n3\nNA\n\n\n\n\n\nAbbiamo 650 osservazioni:\n\ndim(d)\n\n\n6505\n\n\nLe frequenze di dati mancanti vengono ottentute mediante la funzione summary()\n\nsummary(d)\n\n    subject           s1              s2              s3              s4       \n Min.   :5756   Min.   :0.000   Min.   :0.000   Min.   :0.000   Min.   :0.000  \n 1st Qu.:5934   1st Qu.:2.000   1st Qu.:2.000   1st Qu.:1.000   1st Qu.:2.000  \n Median :6102   Median :3.000   Median :3.000   Median :2.000   Median :3.000  \n Mean   :6104   Mean   :2.926   Mean   :2.563   Mean   :2.208   Mean   :2.404  \n 3rd Qu.:6275   3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:3.000  \n Max.   :6451   Max.   :4.000   Max.   :4.000   Max.   :4.000   Max.   :4.000  \n                NA's   :25      NA's   :25      NA's   :25      NA's   :190    \n\n\nIl modello viene specificato come segue {cite:p}brown2015confirmatory:\n\nmodel &lt;- '\n  esteem =~ s1 + s2 + s3 + s4\n  s2 ~~ s4\n'\n\nAdattiamo il modello ai dati specificanto l’utilizzo del metodo full information ML per la gestione dei dati mancanti:\n\nfit &lt;- cfa(model, data = d, missing = \"fiml\")\n\nÈ possibile identificare le configurazioni di risposte agli item che contengono dati mancanti:\n\nfit@Data@Mp[[1]]$npatterns\n\n5\n\n\n\npats &lt;- fit@Data@Mp[[1]]$pat * 1L\ncolnames(pats) &lt;- fit@Data@ov.names[[1]]\nprint(pats)\n\n     s1 s2 s3 s4\n[1,]  1  1  1  1\n[2,]  1  1  1  0\n[3,]  0  1  1  1\n[4,]  1  0  1  1\n[5,]  1  1  0  1\n\n\nPossiamo esaminare la proporzione di dati disponibili per ciascun indicatore e per ciascuna coppia di indicatori:\n\ncoverage &lt;- fit@Data@Mp[[1]]$coverage\ncolnames(coverage) &lt;- rownames(coverage) &lt;- fit@Data@ov.names[[1]]\nprint(coverage)\n\n          s1        s2        s3        s4\ns1 0.9615385 0.9230769 0.9230769 0.6692308\ns2 0.9230769 0.9615385 0.9230769 0.6692308\ns3 0.9230769 0.9230769 0.9615385 0.6692308\ns4 0.6692308 0.6692308 0.6692308 0.7076923\n\n\nAd esempio, consideriamo l’item s1; se moltiplichiamo la copertura di questo elemento per la numerosità campionaria possiamo concludere che questa variabile contiene 25 osservazioni mancanti; e così via.\n\n650 * 0.9615385\n\n625.000025\n\n\nProcediamo poi come sempre per esaminare la soluzione ottenuta.\n\neffectsize::interpret(fit)\n\n\nA data.frame: 10 x 4\n\n\nName\nValue\nThreshold\nInterpretation\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;effctsz_&gt;\n\n\n\n\nGFI\n0.999449432\n0.95\nsatisfactory\n\n\nAGFI\n0.992292047\n0.90\nsatisfactory\n\n\nNFI\n0.999192581\n0.90\nsatisfactory\n\n\nNNFI\n0.998977535\n0.90\nsatisfactory\n\n\nCFI\n0.999829589\n0.90\nsatisfactory\n\n\nRMSEA\n0.020237880\n0.05\nsatisfactory\n\n\nSRMR\n0.004853126\n0.08\nsatisfactory\n\n\nRFI\n0.995155487\n0.90\nsatisfactory\n\n\nPNFI\n0.166532097\n0.50\npoor\n\n\nIFI\n0.999830133\n0.90\nsatisfactory\n\n\n\n\n\n\nstandardizedSolution(fit)\n\n\nA lavaan.data.frame: 15 x 9\n\n\nlhs\nop\nrhs\nest.std\nse\nz\npvalue\nci.lower\nci.upper\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nesteem\n=~\ns1\n0.7374475\n0.01988465\n37.086269\n0.000000e+00\n0.6984743\n0.7764207\n\n\nesteem\n=~\ns2\n0.9204499\n0.01340775\n68.650574\n0.000000e+00\n0.8941712\n0.9467286\n\n\nesteem\n=~\ns3\n0.8804116\n0.01325286\n66.431820\n0.000000e+00\n0.8544365\n0.9063867\n\n\nesteem\n=~\ns4\n0.9045975\n0.01632859\n55.399610\n0.000000e+00\n0.8725941\n0.9366010\n\n\ns2\n~~\ns4\n-0.8859917\n0.21560986\n-4.109236\n3.969709e-05\n-1.3085793\n-0.4634042\n\n\ns1\n~~\ns1\n0.4561711\n0.02932777\n15.554237\n0.000000e+00\n0.3986898\n0.5136525\n\n\ns2\n~~\ns2\n0.1527720\n0.02468233\n6.189531\n6.034349e-10\n0.1043956\n0.2011485\n\n\ns3\n~~\ns3\n0.2248754\n0.02333594\n9.636441\n0.000000e+00\n0.1791378\n0.2706130\n\n\ns4\n~~\ns4\n0.1817033\n0.02954160\n6.150760\n7.711243e-10\n0.1238028\n0.2396038\n\n\nesteem\n~~\nesteem\n1.0000000\n0.00000000\nNA\nNA\n1.0000000\n1.0000000\n\n\ns1\n~1\n\n2.3753651\n0.07760125\n30.609882\n0.000000e+00\n2.2232695\n2.5274608\n\n\ns2\n~1\n\n1.8809244\n0.06578470\n28.592125\n0.000000e+00\n1.7519887\n2.0098600\n\n\ns3\n~1\n\n1.5838240\n0.05914055\n26.780676\n0.000000e+00\n1.4679106\n1.6997373\n\n\ns4\n~1\n\n1.8496616\n0.07101084\n26.047595\n0.000000e+00\n1.7104829\n1.9888403\n\n\nesteem\n~1\n\n0.0000000\n0.00000000\nNA\nNA\n0.0000000\n0.0000000",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Dati mancanti</span>"
    ]
  },
  {
    "objectID": "chapters/sem/10_missing_data.html#dati-mancanti-in-r",
    "href": "chapters/sem/10_missing_data.html#dati-mancanti-in-r",
    "title": "48  Dati mancanti",
    "section": "48.3 Dati mancanti in R",
    "text": "48.3 Dati mancanti in R\nConcludiamo il capitolo con qualche breve accenno alla gestione dei dati mancanti in R.\nIn R, i valori mancanti vengono indicati dal codice NA, che significa not available — non disponibile.\nSe una variabile contiene valori mancanti, R non è in grado di applicare ad essa alcune funzioni, come ad esempio la media. Per questa ragione, la gran parte delle funzioni di R prevedono modi specifici per trattare i valori mancanti.\nCi sono diversi tipi di dati “mancanti” in R;\n\nNA - generico dato mancante;\nNaN - il codice NaN (Not a Number) indica i valori numerici impossibili, quali ad esempio un valore 0/0;\nInf e -Inf - Infinity, si verifca, ad esempio, quando si divide un numero per 0.\n\nLa funzione is.na() ritorna un output che indica con TRUE le celle che contengono NA o NaN.\nSi noti che\n\nse is.na(x) è TRUE, allora !is.na(x) è FALSE;\nall(!is.na(x)) ritorna TRUE se tutti i valori x sono NOT NA;\nany(is.na(x)) risponde alla domanda: c’è qualche valore NA (almeno uno) in x?;\ncomplete.cases(x) ritorna TRUE se ciascun elemento di x è is NOT NA; ritorna FALSE se almeno un elemento di x è NA;\n\nLe funzioni R is.nan() e is.infinite() si applicano ai tipi di dati NaN e Inf.\nPer esempio, consideriamo il seguente data.frame:\n\nd &lt;- tibble(\n  w = c(1, 2, NA, 3, NA), \n  x = 1:5, \n  y = 1, \n  z = x ^ 2 + y,\n  q = c(3, NA, 5, 1, 4)\n)\nd\n\n\nA tibble: 5 x 5\n\n\nw\nx\ny\nz\nq\n\n\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1\n1\n2\n3\n\n\n2\n2\n1\n5\nNA\n\n\nNA\n3\n1\n10\n5\n\n\n3\n4\n1\n17\n1\n\n\nNA\n5\n1\n26\n4\n\n\n\n\n\n\nis.na(d$w)\nis.na(d$x)\n\n\nFALSEFALSETRUEFALSETRUE\n\n\n\nFALSEFALSEFALSEFALSEFALSE\n\n\nPer creare un nuovo Dataframe senza valori mancanti:\n\nd_clean &lt;- d[complete.cases(d), ]\nd_clean\n\n\nA tibble: 2 x 5\n\n\nw\nx\ny\nz\nq\n\n\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1\n1\n2\n3\n\n\n3\n4\n1\n17\n1\n\n\n\n\n\nOppure, se vogliamo eliminare le righe con NA solo in una variabile:\n\nd1 &lt;- d[!is.na(d$q), ]\nd1\n\n\nA tibble: 4 x 5\n\n\nw\nx\ny\nz\nq\n\n\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1\n1\n2\n3\n\n\nNA\n3\n1\n10\n5\n\n\n3\n4\n1\n17\n1\n\n\nNA\n5\n1\n26\n4\n\n\n\n\n\nSe vogliamo esaminare le righe con i dati mancanti in qualunque colonna:\n\nd_na &lt;- d[!complete.cases(d), ]\nd_na\n\n\nA tibble: 3 x 5\n\n\nw\nx\ny\nz\nq\n\n\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n2\n2\n1\n5\nNA\n\n\nNA\n3\n1\n10\n5\n\n\nNA\n5\n1\n26\n4",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Dati mancanti</span>"
    ]
  },
  {
    "objectID": "chapters/sem/11_small_samples.html",
    "href": "chapters/sem/11_small_samples.html",
    "title": "49  Modellizzazione SEM in Piccoli Campioni",
    "section": "",
    "text": "49.1 Introduzione\nQuesto capitolo si concentra sull’applicazione dei modelli SEM in contesti caratterizzati da campioni di piccole dimensioni e sintetizza l’esposizione di Rosseel (2020) su questo argomento, mentre si avvale di esempi numerici tratti dai principi di Kline (2023).\nÈ risaputo che la maggior parte dei metodi di stima e inferenza in SEM si basa su presupposti asintotici, presupponendo la presenza di campioni casuali di grandi dimensioni. Tuttavia, nei casi di campioni più limitati, come quelli con N &lt; 200, emergono specifiche problematiche: i metodi iterativi possono non convergere, si possono verificare soluzioni non valide a causa dei casi di Heywood o di altri risultati anomali difficili da interpretare, e le stime dei parametri possono risultare fortemente distorte.\nInnanzitutto, i modelli strutturali possono diventare molto complessi, coinvolgendo numerose variabili (sia osservate che latenti), rendendo necessaria l’analisi di numerosi parametri e richiedendo un adeguato volume di dati per ottenere stime accurate. Inoltre, il framework statistico alla base della SEM tradizionale si fonda sulla teoria dei grandi campioni, suggerendo che una buona precisione nelle stime dei parametri e nell’inferenza sia garantita solo con campioni di dimensioni considerevoli. Alcuni studi di simulazione hanno addirittura suggerito che dimensioni del campione enormi siano necessarie per risultati affidabili, sebbene tali conclusioni siano rilevanti solo in specifici contesti e abbiano contribuito alla convinzione generalizzata che la SEM sia applicabile solo con campioni di dimensioni considerevoli (ad es. n &gt; 500) o addirittura molto grandi (n &gt; 2000).\nTuttavia, la realtà delle dimensioni ridotte dei campioni è una situazione comune per molte ragioni. In tali casi, molti ricercatori esitano ad utilizzare la SEM e si affidano a metodologie subottimali, come l’analisi di regressione o l’analisi di percorso basate su punteggi sommati. Tuttavia, è importante notare che il bias associato alle dimensioni ridotte del campione può essere ancora più accentuato in tecniche come la regressione multipla o l’analisi di percorso con variabili manifeste, soprattutto in assenza di considerazioni sull’errore di misurazione. Una strategia più efficace potrebbe essere quella di adottare l’approccio della SEM, pur cercando soluzioni per affrontare le sfide poste dalle dimensioni ridotte del campione.\nQuesto capitolo si propone di esplorare diverse strategie per superare tali sfide nell’utilizzo della SEM con campioni di piccole dimensioni. Sarà organizzato in tre sezioni: innanzitutto, verranno esaminate le problematiche comuni associate alle dimensioni ridotte del campione nella SEM. Successivamente, saranno presentati quattro approcci alternativi di stima che possono essere impiegati quando le dimensioni del campione sono limitate, anziché ricorrere alla SEM tradizionale. Infine, saranno discussi alcuni possibili correttivi per le statistiche di test e gli errori standard nelle situazioni di piccoli campioni. L’efficacia di alcune di queste tecniche sarà illustrata tramite l’analisi di un modello di fattore comune applicato a un campione di dimensioni ridotte.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Modellizzazione SEM in Piccoli Campioni</span>"
    ]
  },
  {
    "objectID": "chapters/sem/11_small_samples.html#problemi-con-le-piccole-dimensioni-del-campione-nella-sem",
    "href": "chapters/sem/11_small_samples.html#problemi-con-le-piccole-dimensioni-del-campione-nella-sem",
    "title": "49  Modellizzazione SEM in Piccoli Campioni",
    "section": "49.2 Problemi con le Piccole Dimensioni del Campione nella SEM",
    "text": "49.2 Problemi con le Piccole Dimensioni del Campione nella SEM\nConsideriamo un modello SEM con almeno 4 indicatori continui per ciascuna variabile latente. Se tutte le variabili osservate sono continue, lo stimatore di default nella maggior parte (se non in tutti) dei pacchetti software SEM è il metodo della massima verosimiglianza. Generalmente, lo stimatore della massima verosimiglianza è una buona scelta perché presenta molte proprietà statistiche desiderabili. Inoltre, l’approccio della massima verosimiglianza può essere adattato per gestire dati mancanti (sotto l’assunzione che i dati siano mancanti casualmente) e sono stati sviluppati errori standard e statistiche di test “robusti” per trattare dati non normali e modelli mal specificati.\nTuttavia, se la dimensione del campione è relativamente piccola (ad esempio, n &lt; 200), possono sorgere diversi problemi. Innanzitutto, il modello potrebbe non convergere, il che significa che l’ottimizzatore (l’algoritmo che cerca di trovare i valori dei parametri del modello che massimizzano la verosimiglianza dei dati) non è riuscito a trovare una soluzione che soddisfi uno o più criteri di convergenza. In rare occasioni, l’ottimizzatore potrebbe semplicemente sbagliare. In questo caso, modificare i criteri di convergenza, passare a un altro algoritmo di ottimizzazione o fornire valori iniziali migliori potrebbe risolvere il problema. Ma se la dimensione del campione è piccola, potrebbe benissimo essere che il set di dati non contenga informazioni sufficienti per trovare una soluzione unica per il modello.\nUn secondo problema potrebbe essere che il modello ottenga la convergenza ma produca una soluzione non ammissibile. Ciò significa che alcuni parametri assumono valori inamissibili. L’esempio più comune è una varianza negativa. Un altro esempio è un valore di correlazione che supera 1 (in valore assoluto). È importante rendersi conto che alcuni approcci di stima (sia frequentisti che bayesiani) potrebbero, per progettazione, non produrre mai soluzioni fuori gamma. Sebbene ciò possa sembrare una caratteristica desiderabile, maschera potenziali problemi con il modello o i dati. È importante che gli utenti notino varianze negative (o altri parametri fuori gamma). Le varianze negative sono spesso innocue, ma possono essere un sintomo di una cattiva specificazione strutturale.\nUn terzo problema riguarda il fatto che la massima verosimiglianza è una tecnica per grandi campioni. Questo implica che lavorare con piccole dimensioni del campione può portare a stime puntuali distorte, errori standard troppo piccoli, intervalli di confidenza non sufficientemente ampi e p-valori per test di ipotesi non affidabili.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Modellizzazione SEM in Piccoli Campioni</span>"
    ]
  },
  {
    "objectID": "chapters/sem/11_small_samples.html#soluzioni-possibili-per-la-stima-dei-parametri",
    "href": "chapters/sem/11_small_samples.html#soluzioni-possibili-per-la-stima-dei-parametri",
    "title": "49  Modellizzazione SEM in Piccoli Campioni",
    "section": "49.3 Soluzioni Possibili per la Stima dei Parametri",
    "text": "49.3 Soluzioni Possibili per la Stima dei Parametri\nIn questa sezione, consideriamo brevemente quattro approcci alternativi per stimare i parametri in un contesto SEM con dimensioni di campione piccole. Ci limitiamo ai metodi frequentisti e alle soluzioni disponibili in software gratuiti e open-source.\n\n49.3.1 Stima di Verosimiglianza Penalizzata\nI metodi di stima di verosimiglianza penalizzata (o metodi di regolarizzazione) sono stati sviluppati nella letteratura del machine learning statistico e sono particolarmente utili quando la dimensione del campione è piccola rispetto al numero di variabili nel modello. Questi metodi sono simili ai metodi di verosimiglianza ordinari (come la massima verosimiglianza) ma includono un termine di penalità aggiuntivo per controllare la complessità del modello. Il termine di penalità può essere formulato per incorporare conoscenze pregresse sui parametri o per scoraggiare valori dei parametri meno realistici (ad esempio, lontani da zero). Due termini di penalità popolari sono la penalità ridge e la penalità lasso (least absolute shrinkage and selection operator).\nPer illustrare come funziona questa penalizzazione, immaginate un modello di regressione univariata con un gran numero di predittori. Senza penalizzazione, tutti i coefficienti di regressione sono calcolati nel modo usuale. Tuttavia, il termine di penalità ridge ridurrà tutti i coefficienti verso zero, mentre la penalità lasso ridurrà ulteriormente i piccoli coefficienti fino a zero. In quest’ultimo approccio, sopravvivono solo i predittori “forti” (per i quali c’è un forte supporto nei dati), mentre i predittori “deboli” che possono essere difficilmente distinti dal rumore vengono eliminati. In generale, l’aggiunta di termini di penalità porta a modelli meno complessi, il che è particolarmente vantaggioso se la dimensione del campione è piccola.\nSebbene queste approcci di penalizzazione siano esistiti da alcuni decenni, sono stati applicati solo di recente alla SEM. Due esempi nel software R sono il pacchetto regsem (Jacobucci, Grimm, Brandmaier, Serang e Kievit, 2018) e il pacchetto lslx (Huang e Hu, 2018).\nUno svantaggio di questi metodi di penalizzazione è che l’utente deve indicare quali parametri richiedono la penalizzazione e in che misura. In un’analisi esplorativa, può essere utile e persino vantaggioso penalizzare i parametri verso zero se nel dati non si trova un forte supporto per essi. Tuttavia, la SEM è di solito un approccio confermativo, e l’utente deve assicurarsi che tutti i parametri inizialmente postulati nel modello non vengano rimossi dalla penalizzazione.\n\n\n49.3.2 Model-implied instrumental variables\nBollen (1996) ha proposto un approccio alternativo di stima per i modelli SEM basato sull’utilizzo di variabili strumentali implicite nel modello in combinazione con il metodo dei minimi quadrati a due stadi (MIIV-2SLS). In questo approccio, il modello viene tradotto in un insieme di equazioni (di regressione). Successivamente, ogni variabile latente in queste equazioni viene sostituita con il suo indicatore principale (solitamente il primo indicatore, dove il carico fattoriale è fissato a uno e l’intercetta a zero) meno il suo termine di errore residuo. Le equazioni risultanti non contengono più variabili latenti ma hanno una struttura dell’errore più complessa. È importante notare che la stima dei minimi quadrati ordinari non è più adatta per risolvere queste equazioni poiché alcuni predittori sono ora correlati con il termine di errore nell’equazione. Qui entrano in gioco le variabili strumentali (anche chiamate strumenti). Per ogni equazione, è necessario trovare un insieme di variabili strumentali. Una variabile strumentale deve essere non correlata con il termine di errore dell’equazione ma fortemente correlata con il predittore problematico. Di solito, le variabili strumentali sono ricercate al di fuori del modello, ma nell’approccio di Bollen, le variabili strumentali sono selezionate tra le variabili osservate che fanno parte del modello. Sono stati sviluppati diversi procedimenti automatizzati per trovare queste variabili strumentali all’interno del modello. Una volta selezionati gli strumenti, è necessaria una procedura di stima per stimare tutti i coefficienti delle equazioni, come il metodo dei minimi quadrati a due stadi (2SLS).\nUna motivazione principale per MIIV-2SLS è che è robusto: non si basa sulla normalità ed è meno probabile che diffonda il bias (che può derivare da errate specificazioni strutturali) in una parte del modello ad altre parti del modello. Un’altra caratteristica attraente di MIIV-2SLS è che non è iterativo. Ciò significa che non possono esserci problemi di convergenza e MIIV-2SLS può fornire una soluzione ragionevole per modelli in cui il massimo verosimigliante fallisce nella convergenza.\nSono necessarie ulteriori ricerche per valutare le prestazioni di questo stimatore in contesti in cui la dimensione del campione è (molto) piccola. L’approccio MIIV-2SLS è disponibile nel pacchetto R MIIVsem (Fisher, Bollen, Gates, & Rönkkö, 2017).\n\n\n49.3.3 Stima a Due Fasi\nNel metodo di stima a due fasi, si effettua una distinzione tra la parte di misurazione e la parte strutturale (di regressione) del modello, e la stima avviene in due passaggi distinti. Nel primo passo, vengono adattati uno per uno tutti i modelli di misurazione. Nel secondo passo, viene adattato il modello completo, inclusa la parte strutturale, ma i parametri dei modelli di misurazione vengono mantenuti fissi ai valori trovati nel primo passo. La principale motivazione per l’approccio a due fasi è quella di separare il modello (o i modelli) di misurazione dalla parte strutturale durante la stima in modo che non possano influenzarsi reciprocamente. Nel tradizionale framework della massima verosimiglianza, invece, tutti i parametri vengono adattati simultaneamente. Di conseguenza, errori nella specificazione del modello strutturale possono influenzare i pesi fattoriali stimati di uno o più modelli di misurazione, e ciò può causare problemi di interpretazione per le variabili latenti.\nL’approccio a due fasi è stato recentemente implementato nel pacchetto R lavaan (Rosseel, 2012).\n\n\n49.3.4 Regressione del Punteggio dei Fattori\nL’idea fondamentale della regressione del punteggio dei fattori è quella di sostituire tutte le variabili latenti con i loro punteggi. Questo processo è simile al metodo in due fasi, dove ciascun modello di misurazione viene adattato individualmente. Successivamente, si calcolano i punteggi dei fattori per tutte le variabili latenti nel modo consueto. Una volta che le variabili latenti sono sostituite dai loro punteggi, tutte le variabili diventano osservabili. In un passaggio finale, si stima la parte strutturale del modello. Questa stima può consistere in un’analisi di regressione o in un’analisi dei percorsi. Il termine “regressione del punteggio dei fattori” si riferisce a entrambi gli scenari.\nSe usata in modo ingenuo, questa regressione potrebbe portare a un notevole bias nelle stime dei parametri della parte strutturale, anche con campioni di grandi dimensioni. Questo si verifica perché i punteggi dei fattori vengono trattati come se fossero osservati senza errore di misurazione. Esistono però diversi metodi per correggere questo bias. Ad esempio, il metodo di Croon (2002) procede come segue: prima, si calcola la matrice di varianza-covarianza dei punteggi dei fattori. Poi, sulla base delle informazioni dei modelli di misurazione, gli elementi di questa matrice vengono corretti per approssimare le varianze e covarianze implicite dal modello delle variabili latenti. Questa matrice di varianza-covarianza corretta diventa poi l’input per un’analisi di regressione o dei percorsi regolare.\nSimile al metodo in due fasi, la regressione del punteggio dei fattori (combinata con la correzione di Croon) può essere un’alternativa utile per modelli piuttosto grandi in combinazione con una dimensione campionaria relativamente piccola. Inoltre, è possibile adattare i modelli di misurazione utilizzando un stimatore non iterativo, evitando problemi di convergenza. Tuttavia, la correzione di Croon può produrre una matrice di varianza-covarianza (per le variabili appartenenti alla parte strutturale) che non è definita positiva, specialmente se l’errore di misurazione è sostanziale. Pertanto, la correzione di Croon non è esente da problemi di stima. In questo caso, l’unica soluzione potrebbe essere quella di creare un punteggio somma per ogni variabile latente e stimare un modello in cui ogni variabile latente ha un unico indicatore (il punteggio somma) con la sua affidabilità fissata a un valore realistico fornito dall’utente.\n\n\n49.3.5 Discussione\nTutti i metodi descritti in questa sezione hanno vantaggi e svantaggi. L’approccio della verosimiglianza penalizzata è forse l’unico metodo specificamente progettato per gestire campioni (molto) piccoli. Gli altri tre metodi utilizzano un approccio di “divide et impera”; scompongono il modello completo in parti più piccole e stimano i parametri di ciascuna parte in successione. Oltre a ridurre la complessità e a essere meno vulnerabili a problemi di convergenza, gli ultimi tre metodi hanno il vantaggio di essere efficaci nel localizzare le parti problematiche all’interno di un modello ampio.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Modellizzazione SEM in Piccoli Campioni</span>"
    ]
  },
  {
    "objectID": "chapters/sem/11_small_samples.html#inferenze-per-modelli-sem-in-piccoli-campioni",
    "href": "chapters/sem/11_small_samples.html#inferenze-per-modelli-sem-in-piccoli-campioni",
    "title": "49  Modellizzazione SEM in Piccoli Campioni",
    "section": "49.4 Inferenze per Modelli SEM in Piccoli Campioni",
    "text": "49.4 Inferenze per Modelli SEM in Piccoli Campioni\nMolti autori hanno documentato che quando la dimensione del campione è piccola, il test del chi-quadrato porta ad un inflezione degli errori di Tipo I anche nelle circostanze ideali (cioè, modello correttamente specificato, dati normali). Allo stesso modo, gli errori standard sono spesso attenuati (troppo piccoli) e gli intervalli di confidenza non sono sufficientemente ampi.\nNelle due sottosezioni successive, {cite:t}rosseel2020small discute brevemente alcuni tentativi per affrontare questi problemi di inferenza su campioni piccoli nella modellizzazione SEM.\n\n49.4.1 Migliorare la statistica test del chi-quadrato\nSono state suggerite diverse correzioni per migliorare le prestazioni della statistica del test del chi-quadrato, come la correzione di Bartlett. I risultati di studi di simulazione su questo tema, però, non sono coerenti e, secondo {cite:t}rosseel2020small, per valutare i modelli quando la dimensione del campione è piccola, potrebbe essere opportuno abbandonare del tutto il test del chi-quadrato e esplorare approcci alternativi.\nUn approccio è quello di considerare gli intervalli di confidenza e i test di aderenza basati sull’indice SRMR (standardized root mean square residuals; Maydeu-Olivares, Shi, & Rosseel, 2018). Questi test sembrano funzionare bene anche quando n = 100 (la dimensione del campione più piccola considerata in Maydeu-Olivares et al., 2018) e il modello non è troppo grande. Questi test sono stati implementati come parte della funzione lavResiduals() del pacchetto lavaan.\n\n\n49.4.2 Una migliore stima degli Errori Standard e degli Intervalli di Confidenza\nIn generale, è ben noto che se si utilizza la teoria dei grandi campioni per costruire espressioni analitiche al fine di calcolare gli errori standard, questi possono avere prestazioni scadenti in campioni di piccole dimensioni.\nQuando le assunzioni alla base degli errori standard analitici non sono soddisfatte, spesso si suggerisce di utilizzare un approccio di resampling. Un metodo popolare è il bootstrap (Efron & Tibshirani, 1993): viene generato un campione bootstrap (o campione di replica) e si stima un nuovo set di parametri per questo campione bootstrap. Questo processo viene ripetuto un gran numero di volte (ad esempio, 1.000), e la deviazione standard di un parametro su tutti i campioni bootstrap replicati viene utilizzata come stima dell’errore standard per quel parametro. Purtroppo, nonostante molti altri vantaggi, sembra che il bootstrap non sia una soluzione affidabile quando la dimensione del campione è (molto) piccola (Yung & Bentler, 1996).\nIn alternativa, sono state sviluppate correzioni per dimensioni di campione piccole sugli errori standard (robusti) e sono state recentemente adattate al contesto SEM. Tuttavia, questa tecnologia non è ancora disponibile nei software SEM.\n\n\n49.4.3 Strategie Alternative\nOltre ai metodi precedenti suggeriti da {cite:t}rosseel2020small, altre strategie possibili sono stati indicate da {cite:t}kline2023principles.\n\nSelezione di Indicatori con Elevate Caratteristiche Psicometriche: {cite:t}kline2023principles consiglia di utilizzare indicatori che mostrino eccellenti proprietà psicometriche, idealmente con carichi standardizzati superiori a .70 per gli indicatori continui. Questa pratica riduce il rischio di incorrere in casi di Heywood.\nApplicazione di Restrizioni di Uguaglianza sui Carichi Non Standardizzati: Imponendo vincoli di uguaglianza sulle saturazioni degli indicatori relativi allo stesso fattore si possono evitare soluzioni inammissibili. Questo approccio è particolarmente valido quando gli indicatori sono sulla stessa scala. Un’alternativa valida consiste nel fissare le saturazioni degli indicatori dello stesso fattore a valori costanti non nulli, che riflettano le variazioni nelle loro deviazioni standard.\nSEM Basato su Compositi per Modelli Complessi in Campioni Piccoli: Questo approccio implica l’utilizzo di metodi basati su compositi, che sono combinazioni lineari di variabili osservate. In pratica, le variabili osservate vengono combinate in modi specifici per formare indicatori composti che rappresentano i costrutti latenti nel modello. Questi indicatori composti vengono quindi utilizzati per stimare i parametri del modello SEM. L’obiettivo è ottenere risultati per modelli complessi che richiederebbero campioni molto più ampi con il tradizionale approccio SEM. Tuttavia, è importante notare che i risultati del SEM basato su compositi possono essere soggetti a distorsioni anche nei campioni di piccole dimensioni.\nParceling: Questa strategia coinvolge la creazione di “parcel” o aggregati di due o più indicatori a livello di item attraverso la media dei singoli item. In altre parole, gli indicatori originali sono raggruppati in insiemi più piccoli e i loro valori sono combinati per creare nuovi indicatori aggregati. Questi nuovi indicatori vengono quindi utilizzati per rappresentare i costrutti latenti nel modello SEM. Sebbene il parceling possa ridurre la varianza dell’errore e migliorare la stabilità dei modelli, è importante considerare le sue limitazioni, tra cui la possibile perdita di informazioni e la sensibilità alle scelte fatte durante il processo di parceling. I risultati ottenuti con il parceling possono variare notevolmente in base alle decisioni prese dal ricercatore durante l’analisi.\n\n\n49.4.3.1 Parceling\nApprofondiamo brevemente la strategia del parceling. Il parceling è una strategia che comporta la suddivisione di un set di item in gruppi più piccoli, o “parcel”, per semplificare i modelli e migliorare la loro stima e adattamento.\nUn esempio, citato in {cite:t}kline2023principles, illustra come il parceling possa essere utilizzato in una CFA per ridurre il numero di indicatori e semplificare il modello. {cite:t}kline2023principles considera un questionario di 120 item diviso in tre gruppi distinti di 40 item ciascuno, ognuno mirato a misurare un dominio specifico di un costrutto. In un campione di 150 partecipanti, un’analisi fattoriale confermativa (CFA) con tre fattori e 40 indicatori per fattore, con 120 indicatori totali, può presentare sfide notevoli nella stima del modello a causa della ridotta dimensione del campione. Per affrontare questi problemi, il ricercatore può suddividere ogni gruppo di 40 item in 4 gruppi minori (o “parcel”) di 10 item ciascuno, sommando i punteggi all’interno di ogni “parcel”. Questi punteggi aggregati sostituiscono poi gli item singoli come indicatori in un modello CFA a 3 fattori che avrà quindi solo 12 indicatori in totale (4 indicatori parcellizzati per fattore). Se gli indicatori parcellizzati hanno una distribuzione normale, per la stima si può ricorrere al metodo dei minimi quadrati (ML); altrimenti, si può utilizzare un estimatore ML robusto.\nQuesto metodo è particolarmente utile in situazioni dove si hanno molti item e campioni di dimensioni ridotte, e offre diversi benefici, tra cui:\n\nMaggiore Affidabilità: Il parceling può aumentare l’affidabilità di una scala psicometrica, poiché gli item aggregati tendono ad avere maggior coerenza interna rispetto agli item singoli.\nRapporto Varianza Comune/Varianza Unica: Utilizzando il parceling, si può ottenere un rapporto più favorevole tra la varianza comune (quella spiegata dai fattori comuni) e la varianza unica (quella non spiegata).\nMinore Probabilità di Violazioni Distribuzionali: La pratica del parceling riduce la probabilità che le assunzioni distribuzionali siano violate, il che è importante per l’applicazione di certe tecniche statistiche.\n\nNonostante i suoi benefici, il parceling ha anche limitazioni. La metodologia utilizzata per formare i parcel può influenzare i risultati. Inoltre, il parceling non è consigliabile quando gli item all’interno di un parcel non sono unidimensionali, poiché ciò può distorcere i risultati. È fondamentale verificare l’unidimensionalità prima di procedere con il parceling.\nIn studi con campioni di piccole dimensioni, il parceling ha dimostrato diversi vantaggi, come mostrato nello studio di simulazione di Orçan e Yanyun (2016). Questi includono una riduzione della complessità del modello, tassi di errore di Tipo I più ragionevoli e tassi di errore di Tipo I più bassi quando si utilizza il metodo di stima della massima verosimiglianza con errori standard robusti (MLR) a livello di parcel.\nIn conclusione, il parceling è una tecnica utile che può migliorare l’affidabilità e la validità dei modelli psicometrici, specialmente in presenza di grandi set di item e campioni di piccole dimensioni. Tuttavia, è essenziale valutare attentamente la sua applicabilità e procedere con cautela, specialmente per quanto riguarda l’unidimensionalità dei parcel.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Modellizzazione SEM in Piccoli Campioni</span>"
    ]
  },
  {
    "objectID": "chapters/sem/11_small_samples.html#sem-in-un-piccolo-campione",
    "href": "chapters/sem/11_small_samples.html#sem-in-un-piccolo-campione",
    "title": "49  Modellizzazione SEM in Piccoli Campioni",
    "section": "49.5 SEM in un Piccolo Campione",
    "text": "49.5 SEM in un Piccolo Campione\n{cite:t}kline2023principles discute uno studio in cui è stato applicato un modello CFA a due fattori ad un campione di 103 donne, le quali hanno compilato questionari su esperienze di origine familiare e adattamento coniugale {cite:p}sabatelli2003family.\n\n# input the correlations in lower diagnonal form\nsabatelliLower.cor &lt;- \"\n 1.000\n  .740 1.000\n  .265  .422 1.000\n  .305  .401  .791 1.000\n  .315  .351  .662  .587 1.000 \"\n\n# name the variables and convert to full correlation matrix\nsabatelli.cor &lt;- lavaan::getCov(sabatelliLower.cor, names = c(\n    \"problems\", \"intimacy\", \"father\", \"mother\", \"both\"\n    )\n)\n\n# add the standard deviations and convert to covariances\nsabatelli.cov &lt;- lavaan::cor2cov(sabatelli.cor, sds = c(\n    32.936, 22.749, 13.390, 13.679, 14.382\n    )\n)\n\nIl modello proposto dagli autori è specificato di seguito:\n\nsabatelli_model &lt;- \"\n    # common factors\n    # loading for intimacy constrained to equal .691\n    Marital =~ problems + intimacy\n    FOE =~ father + mother + both\n\"\n\nIn riferimento al modello specificato sopra, la soluzione fornita da lavaan risulta inammissibile a causa di un caso di Heywood, evidenziato da una varianza d’errore negativa per la variabile “intimità”.\n\noriginal &lt;- lavaan::sem(sabatelli_model,\n    sample.cov = sabatelli.cov,\n    sample.nobs = 103\n)\n\nWarning message in lav_object_post_check(object):\n\"lavaan WARNING: some estimated ov variances are negative\"\n\n\n\nlavaan::summary(original,\n    fit.measures = TRUE, standardized = TRUE,\n    rsquare = TRUE\n) |&gt; print()\n\nlavaan 0.6.17 ended normally after 141 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        11\n\n  Number of observations                           103\n\nModel Test User Model:\n                                                      \n  Test statistic                                 4.688\n  Degrees of freedom                                 4\n  P-value (Chi-square)                           0.321\n\nModel Test Baseline Model:\n\n  Test statistic                               271.302\n  Degrees of freedom                                10\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.997\n  Tucker-Lewis Index (TLI)                       0.993\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -2087.964\n  Loglikelihood unrestricted model (H1)      -2085.620\n                                                      \n  Akaike (AIC)                                4197.928\n  Bayesian (BIC)                              4226.910\n  Sample-size adjusted Bayesian (SABIC)       4192.163\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.041\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.159\n  P-value H_0: RMSEA &lt;= 0.050                    0.448\n  P-value H_0: RMSEA &gt;= 0.080                    0.387\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.028\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Marital =~                                                            \n    problems          1.000                              23.362    0.713\n    intimacy          1.006    0.221    4.547    0.000   23.503    1.038\n  FOE =~                                                                \n    father            1.000                              12.488    0.937\n    mother            0.919    0.089   10.320    0.000   11.480    0.843\n    both              0.808    0.098    8.206    0.000   10.088    0.705\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Marital ~~                                                            \n    FOE             129.409   44.216    2.927    0.003    0.444    0.444\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .problems        528.472  130.514    4.049    0.000  528.472    0.492\n   .intimacy        -39.892  109.200   -0.365    0.715  -39.892   -0.078\n   .father           21.613   10.983    1.968    0.049   21.613    0.122\n   .mother           53.509   11.710    4.570    0.000   53.509    0.289\n   .both            103.075   16.168    6.375    0.000  103.075    0.503\n    Marital         545.776  169.103    3.227    0.001    1.000    1.000\n    FOE             155.939   26.732    5.833    0.000    1.000    1.000\n\nR-Square:\n                   Estimate\n    problems          0.508\n    intimacy             NA\n    father            0.878\n    mother            0.711\n    both              0.497\n\n\n\nPer ovviare ad un tale problema, in una nuova analisi del modello di adattamento coniugale è stato applicato un vincolo specifico ai carichi non standardizzati degli indicatori. A causa delle differenze sostanziali nelle metriche tra le due variabili, ovvero “intimacy” (con una deviazione standard di 22.749) e “problems” (con una deviazione standard di 32.936), sono state fissate le seguenti saturazioni fattoriali: il carico per la variabile “problemi” è stato fissato a 1, mentre il carico per la variabile “intimità” è stato fissato a 0.691. Questi valori sono stati calcolati in modo da riflettere proporzionalmente la differenza nelle deviazioni standard tra le due variabili.\n\n# analysis with constrained loadings for indicators of marital adjustment\n# model df = 5\n\n# standard deviations for both indicators\n# of the marital factor are listed next\n# intimacy, sd = 22.749\n# problems, sd = 32.936\n# ratio = 22.749/32.936 = .691\n\n# specify model with constrained loadings for problems, intimacy\n\nproportional.model &lt;- \"\n    # common factors\n    # loading for intimacy constrained to equal .691\n    Marital =~ 1*problems + .691*intimacy\n    FOE =~ father + mother + both \n\"\n\n\nproportional &lt;- lavaan::sem(proportional.model,\n    sample.cov = sabatelli.cov,\n    sample.nobs = 103\n)\n\n\nsemPlot::semPaths(proportional,\n    what = \"col\", whatLabels = \"par\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\n\n\nlavaan::summary(proportional,\n    fit.measures = TRUE, standardized = TRUE,\n    rsquare = TRUE\n) |&gt; print()\n\nlavaan 0.6.17 ended normally after 110 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n\n  Number of observations                           103\n\nModel Test User Model:\n                                                      \n  Test statistic                                 8.449\n  Degrees of freedom                                 5\n  P-value (Chi-square)                           0.133\n\nModel Test Baseline Model:\n\n  Test statistic                               271.302\n  Degrees of freedom                                10\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.987\n  Tucker-Lewis Index (TLI)                       0.974\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -2089.845\n  Loglikelihood unrestricted model (H1)      -2085.620\n                                                      \n  Akaike (AIC)                                4199.690\n  Bayesian (BIC)                              4226.037\n  Sample-size adjusted Bayesian (SABIC)       4194.449\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.082\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.174\n  P-value H_0: RMSEA &lt;= 0.050                    0.242\n  P-value H_0: RMSEA &gt;= 0.080                    0.584\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.045\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Marital =~                                                            \n    problems          1.000                              28.391    0.840\n    intimacy          0.691                              19.618    0.885\n  FOE =~                                                                \n    father            1.000                              12.373    0.929\n    mother            0.935    0.091   10.279    0.000   11.568    0.850\n    both              0.821    0.100    8.235    0.000   10.155    0.710\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Marital ~~                                                            \n    FOE             164.822   42.788    3.852    0.000    0.469    0.469\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .problems        335.923   80.080    4.195    0.000  335.923    0.294\n   .intimacy        106.214   34.373    3.090    0.002  106.214    0.216\n   .father           24.457   11.060    2.211    0.027   24.457    0.138\n   .mother           51.489   11.730    4.389    0.000   51.489    0.278\n   .both            101.712   16.070    6.329    0.000  101.712    0.497\n    Marital         806.040  132.946    6.063    0.000    1.000    1.000\n    FOE             153.095   26.669    5.741    0.000    1.000    1.000\n\nR-Square:\n                   Estimate\n    problems          0.706\n    intimacy          0.784\n    father            0.862\n    mother            0.722\n    both              0.503\n\n\n\n\nfitMeasures(proportional, c(\"chisq\", \"df\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt;\n    print()\n\nchisq    df   cfi   tli rmsea  srmr \n8.449 5.000 0.987 0.974 0.082 0.045 \n\n\n\nlavaan::parameterEstimates(proportional) |&gt; print()\n\n        lhs op      rhs     est      se      z pvalue ci.lower ci.upper\n1   Marital =~ problems   1.000   0.000     NA     NA    1.000    1.000\n2   Marital =~ intimacy   0.691   0.000     NA     NA    0.691    0.691\n3       FOE =~   father   1.000   0.000     NA     NA    1.000    1.000\n4       FOE =~   mother   0.935   0.091 10.279  0.000    0.757    1.113\n5       FOE =~     both   0.821   0.100  8.235  0.000    0.625    1.016\n6  problems ~~ problems 335.923  80.080  4.195  0.000  178.970  492.877\n7  intimacy ~~ intimacy 106.214  34.373  3.090  0.002   38.843  173.584\n8    father ~~   father  24.457  11.060  2.211  0.027    2.779   46.134\n9    mother ~~   mother  51.489  11.730  4.389  0.000   28.498   74.481\n10     both ~~     both 101.712  16.070  6.329  0.000   70.216  133.208\n11  Marital ~~  Marital 806.040 132.946  6.063  0.000  545.471 1066.608\n12      FOE ~~      FOE 153.095  26.669  5.741  0.000  100.825  205.365\n13  Marital ~~      FOE 164.822  42.788  3.852  0.000   80.959  248.684\n\n\n\nlavaan::standardizedSolution(proportional) |&gt; print()\n\n        lhs op      rhs est.std    se      z pvalue ci.lower ci.upper\n1   Marital =~ problems   0.840 0.036 23.439  0.000    0.770    0.910\n2   Marital =~ intimacy   0.885 0.037 24.040  0.000    0.813    0.957\n3       FOE =~   father   0.929 0.035 26.779  0.000    0.861    0.997\n4       FOE =~   mother   0.850 0.040 21.126  0.000    0.771    0.929\n5       FOE =~     both   0.710 0.055 12.800  0.000    0.601    0.818\n6  problems ~~ problems   0.294 0.060  4.884  0.000    0.176    0.412\n7  intimacy ~~ intimacy   0.216 0.065  3.317  0.001    0.088    0.344\n8    father ~~   father   0.138 0.064  2.139  0.032    0.012    0.264\n9    mother ~~   mother   0.278 0.068  4.065  0.000    0.144    0.412\n10     both ~~     both   0.497 0.079  6.313  0.000    0.342    0.651\n11  Marital ~~  Marital   1.000 0.000     NA     NA    1.000    1.000\n12      FOE ~~      FOE   1.000 0.000     NA     NA    1.000    1.000\n13  Marital ~~      FOE   0.469 0.091  5.177  0.000    0.292    0.647\n\n\nNonostante gli indici di bontà di adattamento siano eccellenti, la potenza di questa analisi statistica risulta estremamente limitata. Per valutare questa limitazione, è possibile utilizzare la funzione semTools::findRMSEAsamplesize(). Questa funzione calcola la dimensione del campione necessaria per rilevare una differenza significativa tra RMSEA_0 e RMSEA_A, considerando un modello con df gradi di libertà.\nPer esempio, se desideriamo distinguere tra RMSEA_0=0.05 e RMSEA_A=0.10 utilizzando il modello attuale con 5 gradi di libertà, la funzione ci indica che sono necessarie 561 osservazioni per ottenere una potenza statistica di 0.8:\n\nsemTools::findRMSEAsamplesize(0.05, .10, 5, .80, .05, 1)\n\n561\n\n\nPer creare un grafico che rappresenti la potenza statistica per rilevare la differenza tra RMSEA_0=0.05 e RMSEA_A=0.10 (utilizzati qui come esempio) al variare della dimensione del campione, è possibile seguire la seguente procedura:\n\nsemTools::plotRMSEApower(rmsea0 = .05, rmseaA = .10, df = 5, 50, 1000)\n\n\n\n\n\n\n\n\nQuesta analisi di potenza indica che la dimensione del campione utilizzato (\\(n\\) = 103) è del tutto inadeguata.\nPer migliorare il nostro giudizio sull’adattamento del modello consideriamo l’analisi dei residui.\n\nlavaan::residuals(proportional, type = \"standardized.mplus\") |&gt; print()\n\n$type\n[1] \"standardized.mplus\"\n\n$cov\n         prblms intmcy father mother   both\nproblems     NA                            \nintimacy     NA  0.918                     \nfather   -3.994  1.039  0.002              \nmother   -0.871  1.049  0.328  0.000       \nboth      0.407  0.930  0.352 -2.776     NA\n\n\n\n\nlavaan::lavResiduals(proportional, type = \"cor.bollen\", summary = TRUE) |&gt; print()\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n         prblms intmcy father mother   both\nproblems  0.000                            \nintimacy -0.004  0.000                     \nfather   -0.101  0.036  0.000              \nmother   -0.030  0.048  0.002  0.000       \nboth      0.035  0.056  0.003 -0.016  0.000\n\n$cov.z\n         prblms intmcy father mother   both\nproblems  0.000                            \nintimacy -2.524  0.000                     \nfather   -2.386  1.245  0.000              \nmother   -0.586  1.134  0.881  0.000       \nboth      0.523  0.936  0.507 -1.143  0.000\n\n$summary\n                           cov\ncrmr                     0.044\ncrmr.se                  0.015\ncrmr.exactfit.z          0.504\ncrmr.exactfit.pvalue     0.307\nucrmr                    0.023\nucrmr.se                 0.029\nucrmr.ci.lower          -0.024\nucrmr.cilupper           0.071\nucrmr.closefit.h0.value  0.050\nucrmr.closefit.z        -0.928\nucrmr.closefit.pvalue    0.823\n\n\n\nQuesti sono risultati relativamente scarsi per un modello così piccolo. Il computer non è stato in grado di calcolare tutti i residui standardizzati possibili, il che non è sorprendente in un campione così ridotto.\n\n49.5.1 Stimatore MIIV-2SLS\nUna seconda analisi viene condotta utilizzando lo stimatore MIIV-2SLS. Il pacchetto MIIVsem non calcola statistiche globali di bontà di adattamento. Al contrario, calcola il test di Sargan per ciascun indicatore previsto dal modello. Le statistiche del test di Sargan approssimano distribuzioni chi-quadro centrali con gradi di libertà equivalenti al numero di item meno uno, quindi df = 2. L’ipotesi nulla è che ogni insieme di strumenti multipli sia incorrelato con il termine di errore per l’equazione. Il mancato rifiuto dell’ipotesi nulla per il test di Sargan suggerisce una buona corrispondenza del modello con i dati.\n\nMIIVsem::miivs(sabatelli_model)\n\nModel Equation Information \n\n LHS        RHS        MIIVs                     \n intimacy   problems   father, mother, both      \n mother     father     problems, intimacy, both  \n both       father     problems, intimacy, mother\n\n\n\n\nsabatelli &lt;- MIIVsem::miive(sabatelli_model,\n    sample.cov = sabatelli.cov,\n    sample.nobs = 103, var.cov = TRUE\n)\n\nlavaan::summary(sabatelli, rsquare = TRUE) |&gt; print()\n\nMIIVsem (0.5.8) results \n\nNumber of observations                                                    103\nNumber of equations                                                         3\nEstimator                                                           MIIV-2SLS\nStandard Errors                                                      standard\nMissing                                                              listwise\n\n\nParameter Estimates:\n\n\nSTRUCTURAL COEFFICIENTS:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Sargan   df   P(Chi)\n  FOE =~                                                                     \n    father            1.000                                                  \n    mother            0.899    0.089   10.149    0.000    1.763    2    0.414\n    both              0.787    0.099    7.935    0.000    3.590    2    0.166\n  Marital =~                                                                 \n    problems          1.000                                                  \n    intimacy          0.805    0.155    5.195    0.000    4.980    2    0.083\n\nINTERCEPTS:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   \n    both              0.000                              \n    father            0.000                              \n    intimacy          0.000                              \n    mother            0.000                              \n    problems          0.000                              \n\nVARIANCES:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   \n    FOE             158.501                              \n    Marital         702.393                              \n    both            103.301                              \n    father           20.856                              \n    intimacy         50.871                              \n    mother           54.195                              \n    problems        422.427                              \n\nCOVARIANCES:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   \n  Marital ~~                                             \n    FOE             157.495                              \n\nR-SQUARE:\n                   Estimate\n    problems          0.624\n    intimacy          0.900\n    father            0.884\n    mother            0.703\n    both              0.487\nNULL\n\n\nSi noti che le stime standardizzate non sono calcolate nella versione del pacchetto MIIVsem utilizzata in questa analisi. I valori non standardizzati delle saturazioni fattoriali sono simili a quelli ottenuti in precedenza.\nIl pacchetto MIIVsem non fornisce né le correlazioni previste dal modello per gli indicatori né i residui di correlazione. Per ottenere i residui di correlazione per l’estimatore 2SLS, è possibile utilizzare il pacchetto lavaan per specificare nuovamente il modello precedentemente adattato, ma con l’importante modifica di fissare tutti i parametri non standardizzati in modo che siano identici alle loro controparti 2SLS. Successivamente, è possibile adattare nuovamente il modello con questi parametri fissati alla matrice di covarianza. La matrice di correlazione prevista in questa analisi si basa sulle stime dei parametri 2SLS, consentendo così di ottenere i residui di correlazione desiderati.\n\nsabatelliFixed.model &lt;- \"\n    # common factors\n    Marital =~ 1.0*problems + .805*intimacy\n    FOE =~ 1.0*father + .899*mother + .787*both\n    # factor variances, covariances\n    FOE ~~ 158.501*FOE\n    Marital ~~ 157.495*FOE\n    Marital ~~ 702.393*Marital\n    # indicator error variances\n    father ~~ 20.856*father\n    mother ~~ 54.195*mother\n    both ~~ 103.301*both\n    problems ~~ 422.427*problems\n    intimacy ~~ 50.781*intimacy \n \"\n\n\nsabatelliFixed &lt;- lavaan::sem(sabatelliFixed.model,\n    sample.cov = sabatelli.cov,\n    sample.nobs = 103\n)\n\n\n# standardized parameter \"estimates\" listed\n# next are fixed to nonzero constants, and\n# standard errors are undefined\nlavaan::parameterEstimates(sabatelliFixed)\n\n\nA lavaan.data.frame: 13 x 9\n\n\nlhs\nop\nrhs\nest\nse\nz\npvalue\nci.lower\nci.upper\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nMarital\n=~\nproblems\n1.000\n0\nNA\nNA\n1.000\n1.000\n\n\nMarital\n=~\nintimacy\n0.805\n0\nNA\nNA\n0.805\n0.805\n\n\nFOE\n=~\nfather\n1.000\n0\nNA\nNA\n1.000\n1.000\n\n\nFOE\n=~\nmother\n0.899\n0\nNA\nNA\n0.899\n0.899\n\n\nFOE\n=~\nboth\n0.787\n0\nNA\nNA\n0.787\n0.787\n\n\nFOE\n~~\nFOE\n158.501\n0\nNA\nNA\n158.501\n158.501\n\n\nMarital\n~~\nFOE\n157.495\n0\nNA\nNA\n157.495\n157.495\n\n\nMarital\n~~\nMarital\n702.393\n0\nNA\nNA\n702.393\n702.393\n\n\nfather\n~~\nfather\n20.856\n0\nNA\nNA\n20.856\n20.856\n\n\nmother\n~~\nmother\n54.195\n0\nNA\nNA\n54.195\n54.195\n\n\nboth\n~~\nboth\n103.301\n0\nNA\nNA\n103.301\n103.301\n\n\nproblems\n~~\nproblems\n422.427\n0\nNA\nNA\n422.427\n422.427\n\n\nintimacy\n~~\nintimacy\n50.781\n0\nNA\nNA\n50.781\n50.781\n\n\n\n\n\n\nlavaan::residuals(sabatelliFixed, type = \"standardized.mplus\") |&gt; print()\n\n$type\n[1] \"standardized.mplus\"\n\n$cov\n         prblms intmcy father mother   both\nproblems -0.338                            \nintimacy -0.180  0.092                     \nfather   -0.938  0.016 -0.073              \nmother   -0.120  0.293  0.043  0.116       \nboth      0.491  0.412  0.067  0.100  0.118\n\n\n\n\nlavaan::residuals(sabatelliFixed, type = \"cor.bollen\") |&gt; print()\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n         prblms intmcy father mother   both\nproblems  0.000                            \nintimacy -0.010  0.000                     \nfather   -0.086  0.001  0.000              \nmother   -0.008  0.026  0.003  0.000       \nboth      0.055  0.038  0.006  0.002  0.000\n\n\n\nSi noti che nessuno dei residui di correlazione assoluti basati sui risultati 2SLS supera lo 0.10, compreso il residuo per la coppia di indicatori “problems” e “father”. In termini di adattamento locale, dunque, in questo esempio i risultati dello stimatore 2SLS sono da preferire rispetto a quelli dello stimatore ML.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Modellizzazione SEM in Piccoli Campioni</span>"
    ]
  },
  {
    "objectID": "chapters/sem/11_small_samples.html#considerazioni-conclusive",
    "href": "chapters/sem/11_small_samples.html#considerazioni-conclusive",
    "title": "49  Modellizzazione SEM in Piccoli Campioni",
    "section": "49.6 Considerazioni Conclusive",
    "text": "49.6 Considerazioni Conclusive\nIn questo capitolo abbiamo discusso diversi problemi nel contesto della SEM quando le dimensioni del campione sono ridotte e vengono utilizzati metodi di stima standard (massima verosimiglianza), come la mancata convergenza, le soluzioni non ammissibili, il bias, le statistiche di test poco performanti e gli intervalli di confidenza e gli errori standard inaccurati. Come possibili soluzioni per ottenere stime puntuali migliori, {cite:t}rosseel2020small presenta quattro approcci alternativi alla stima: la stima della verosimiglianza penalizzata, le variabili strumentali derivanti dal modello, la stima a due fasi e la regressione dei punteggi fattoriali. Solo il primo metodo è stato specificamente progettato per gestire campioni ridotti. Gli altri approcci sono stati sviluppati con altre preoccupazioni in mente, ma potrebbero essere alternative valide per la stima quando le dimensioni del campione sono ridotte.\nPer quanto riguarda l’inferenza, {cite:t}rosseel2020small discute vari tentativi per migliorare le prestazioni della statistica del chi-quadro per valutare l’adattamento globale in presenza di campioni ridotti. Per quanto riguarda gli errori standard, sottolinea che il bootstrapping potrebbe non essere la soluzione che stiamo cercando. Per ottenere errori standard (e intervalli di confidenza) migliori nel contesto di campioni ridotti, {cite:t}rosseel2020small ritiene che sia necessario aspettare fino a quando nuove tecnologie saranno disponibili. Altri suggerimenti sono stati forniti da {cite:t}kline2023principles. La tecnica del “parceling” è stata presentata in relazione alla discussione fornita da {cite:t}rioux2020item.\n\n\n\n\nKline, Rex B. 2023. Principles and practice of structural equation modeling. Guilford publications.\n\n\nRosseel, Yves. 2020. «Small sample solutions for structural equation modeling». In Small sample size solutions: A guide for applied researchers and practitioners, 226–38. Routledge.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Modellizzazione SEM in Piccoli Campioni</span>"
    ]
  },
  {
    "objectID": "chapters/sem/12_temp_reliability.html",
    "href": "chapters/sem/12_temp_reliability.html",
    "title": "50  Affidabilità longitudinale",
    "section": "",
    "text": "50.1 Introduzione\nNel (interrater-reliability?) abbiamo illustrato come calcolare l’affidabilità delle misure di un disegno longitudinale usando il framework della teoria della generalizzabilità. Questo capitolo affronta lo stesso problema usano i modelli di equazioni strutturali.\nGrazie ai progressi tecnologici, i metodi di raccolta dati longitudinali intensivi si sono sviluppati notevolmente. Tali dati possono ora essere raccolti in maniera meno invasiva, riducendo gli ostacoli per i partecipanti. Tradizionalmente, i dati longitudinali si caratterizzavano per un numero limitato di misurazioni ripetute con ampi intervalli temporali. Le nuove tecniche di raccolta dati, ad esempio tramite applicazioni per smartphone o tablet, hanno portato a dati con un maggior numero di occasioni di misurazione, vicine tra loro temporalmente. Questi dati longitudinali intensivi permettono di investigare la dinamica di processi variabili, come i cambiamenti quotidiani di vari stati psicologici.\nI dati raccolti con misure quotidiane presentano una struttura annidata, in quanto più occasioni di misurazione sono raggruppate all’interno della stessa persona. Attualmente, due tecniche sono comunemente impiegate per analizzare l’affidabilità con dati annidati: la teoria della generalizzabilità e l’approccio fattoriale. La teoria della generalizzabilità scompone la varianza totale in elementi di tempo, item e persona, valutando così l’affidabilità del cambiamento nel tempo a livello individuale. Nonostante i suoi vantaggi, questo approccio si basa su assunzioni che potrebbero non essere sempre verificate dai dati.\nL’approccio fattoriale è più flessibile e permette di modellare le associazioni degli item con il punteggio vero e le varianze degli errori. In particolare, l’analisi fattoriale confermativa multilivello (MCFA) viene utilizzata per ottenere elementi di varianza al fine di determinare l’affidabilità specifica per il tempo (a livello intra-individuale) e per la persona (a livello inter-individuale).\nL’articolo di {cite:t}van2022determining descrive come sia possibile valutare l’affidabilità con dati longitudinali intensivi quotidiani. Nel loro tutorial, gli autori utilizzano dati empirici raccolti tramite una misura dello stress lavorativo quotidiano per insegnanti di scuola secondaria. Inoltre, {cite:t}van2022determining confrontano gli indici di affidabilità derivati dal metodo MCFA con quelli ottenuti tramite la teoria della generalizzabilità.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Affidabilità longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/sem/12_temp_reliability.html#affidabilità-nei-modelli-fattoriali-a-livello-singolo",
    "href": "chapters/sem/12_temp_reliability.html#affidabilità-nei-modelli-fattoriali-a-livello-singolo",
    "title": "50  Affidabilità longitudinale",
    "section": "50.2 Affidabilità nei Modelli Fattoriali a Livello Singolo",
    "text": "50.2 Affidabilità nei Modelli Fattoriali a Livello Singolo\nL’analisi fattoriale confermativa (CFA) è diventata lo standard per determinare la dimensionalità e l’affidabilità dei punteggi in psicologia. L’affidabilità, in dati con un singolo livello, può essere valutata mediante diversi indici. A differenza del coefficiente di consistenza interna \\(\\alpha\\), l’indice \\(\\omega\\) non assume che i diversi carichi fattoriali degli item contribuiscano equamente al costrutto latente.\nI valori di \\(\\omega\\) variano da zero a uno, dove valori vicini a uno indicano una migliore affidabilità della scala. Il valore effettivo di \\(\\omega\\) può essere interpretato come la proporzione di varianza nei punteggi della scala spiegata dalla variabile latente comune a tutti gli indicatori.\nL’affidabilità composita \\(\\omega\\), per un costrutto misurato con \\(p\\) item, è definita come:\n\\[\n\\omega = \\frac{\\sum_{i=1}^{p} \\lambda_i^2 \\Phi}{\\sum_{i=1}^{p} \\lambda_i^2 \\Phi + \\sum_{i=1}^{p} \\theta_i},\n\\]\ndove \\(i\\) indica l’item, \\(\\lambda\\) rappresenta un carico fattoriale, \\(\\Phi\\) rappresenta la varianza del fattore, e \\(\\theta\\) rappresenta la varianza residua dell’item.\n{cite:t}van2022determining propongono il seguente esempio. Consideriamo un modello unifattoriale in cui la varianza del fattore sia fissata a 1 per l’identificazione del modello, con saturazioni su tre indicatori pari a 0.7, 0.8 e 0.9. Le specificità saranno dunque 0.51, 0.36 e 0.19. Possiamo determinare l’affidabilità \\(\\omega\\) della scala inserendo questi valori nell’equazione precedente, il che produce un’affidabilità di .84.\n\\[\n\\begin{equation}\n\\omega = \\frac{\\left(0.70 + 0.80 + 0.90\\right)^{2} 1}{\\left(0.70 + 0.80 + 0.90\\right)^{2} 1 + \\left(0.51 + 0.36 + 0.19\\right)}.\n\\end{equation}\n\\]\nQuesto significa che l’84% della varianza totale nei punteggi della scala è spiegata dal fattore comune.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Affidabilità longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/sem/12_temp_reliability.html#affidabilità-nei-modelli-fattoriali-multilivello",
    "href": "chapters/sem/12_temp_reliability.html#affidabilità-nei-modelli-fattoriali-multilivello",
    "title": "50  Affidabilità longitudinale",
    "section": "50.3 Affidabilità nei Modelli Fattoriali Multilivello",
    "text": "50.3 Affidabilità nei Modelli Fattoriali Multilivello\nNella ricerca psicologica, i dati spesso presentano una struttura annidata, dove le unità di livello inferiore sono considerate annidate in unità di livello superiore. Ad esempio, gli studenti possono essere annidati nelle classi, o i pazienti negli ospedali. Con misure giornaliere degli individui su diversi giorni, le occasioni di misurazione sono annidate negli individui. Allo stesso modo, i dati empirici utilizzati nell’esempio presentato qui, sono raccolti dagli stessi insegnanti durante 15 occasioni di misurazione. Queste occasioni sono annidate all’interno di ciascun insegnante. L’analisi fattoriale multilivello consente modelli diversi per varianze e covarianze delle differenze intra-individuali e inter-individuali (Muthén, 1994). Nell’esempio discusso, {cite:t}van2022determining si concentrano su strutture con due livelli, costituiti da occasioni (Livello 1, o il livello interno) all’interno di individui (Livello 2, o il livello esterno).\nLa Fig.{ref}vanalphen-fig fornisce una rappresentazione grafica di un modello fattoriale multilivello. In tale analisi fattoriale confermativa (CFA) a due livelli, i punteggi degli item sono decomposti in componenti (latenti) a livello interno ed esterno. La parte a livello esterno modella la struttura di covarianza a livello inter-individuale, spiegando le differenze tra gli individui. L’interpretazione di questa parte del modello è paragonabile a una CFA ad un singolo livello. La parte a livello interno modella la struttura di covarianza a livello delle occasioni di misurazione, spiegando le differenze all’interno degli individui tra i diversi punti temporali. In questo esempio, il livello dell’occasione è rappresentativo delle caratteristiche di stato degli individui, perché mostra i cambiamenti giornalieri delle condizioni degli individui. Il livello inter-individuale, quindi, si riferisce alle caratteristiche di tratto degli individui, poiché tali misure sono un aggregato delle misure giornaliere e rappresentano una misura più stabile (cioè, a lungo termine), simile a una misura della personalità.\n\n\n\n\n\n\nFigura 50.1: Un modello configurale multilivello con i carichi fattoriali, varianze residuali e varianza del fattoriali dell’esempio discusso da Alphen et al. (2022). (Figura tratta da Alphen et al. (2022))\n\n\n\nGeldhof et al. (2014) hanno esteso il metodo esistente per determinare ω a modelli a due livelli, risultando in indici di affidabilità a livello interno (ωw) e a livello esterno (ωb). Questo approccio è stato è stato poi sviluppato da Lai (2021).",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Affidabilità longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/sem/12_temp_reliability.html#calcolo-dellaffidabilità-della-scala-con-dati-giornalieri",
    "href": "chapters/sem/12_temp_reliability.html#calcolo-dellaffidabilità-della-scala-con-dati-giornalieri",
    "title": "50  Affidabilità longitudinale",
    "section": "50.4 Calcolo dell’Affidabilità della Scala con Dati Giornalieri",
    "text": "50.4 Calcolo dell’Affidabilità della Scala con Dati Giornalieri\nAlphen et al. (2022) considerano un dataset che include misure longitudinali intensive giornaliere sullo stress degli insegnanti. In questo contesto, il fattore comune a livello esterno può essere interpretato come la componente stabile del fattore stress, mentre il fattore comune a livello interno rappresenta la parte variabile nel tempo del fattore stress. Quando ci si concentra sulle componenti a livello interno ed esterno dello stesso fattore, il modello fattoriale multilivello è conosciuto come modello configurale (Stapleton et al., 2016).\nNel modello fattoriale configurale multilivello, i fattori a livello interno ed esterno riflettono le componenti interne ed esterne della stessa variabile latente. Di conseguenza, la struttura fattoriale è la stessa per entrambi i livelli e i carichi fattoriali sono uguali tra i livelli (Asparouhov & Muthen, 2012). Lai (2021) ha fornito equazioni per calcolare stime di affidabilità a livello interno (ωw) ed esterno (ωb), specificamente per questi modelli configurali. Alphen et al. (2022) forniscono un esempio di calcolo per entrambi questi indici di affidabilità.\nLa seguente equazione viene utilizzata per determinare l’affidabilità a livello interno in un modello configurale:\n\\[\n\\omega_w = \\frac{\\sum (\\lambda_i^2 \\Phi_w)}{\\sum (\\lambda_i^2 \\Phi_w) + \\sum (\\theta_w)},\n\\]\ndove il pedice \\(w\\) si riferisce al livello interno. Si noti che i carichi fattoriali (\\(\\lambda\\)) non hanno un pedice specifico di livello perché sono vincolati ad essere uguali tra i livelli.\nConfrontando questa equazione omega al livello interno (2) con l’equazione utilizzata per determinare l’affidabilità utilizzando una CFA ad un livello singolo, si vede che la varianza fattoriale a livello interno (\\(\\Phi_w\\)) viene utilizzata al posto della varianza totale (\\(\\Phi\\)). In questo contesto multilivello, \\(\\theta_w\\) rappresenta la varianza residua solo al livello interno.\nInserendo i nostri valori esemplificativi della (vanalphen-fig?) nell’Equazione precedente si ottiene un’affidabilità a livello interno di 0.84. Ciò significa che il fattore comune a livello interno spiega l’84% della varianza totale nei punteggi di deviazione della scala a livello interno:\n\\[\n\\omega_w = \\frac{(0.70 + 0.80 + 0.90)^2}{(0.70 + 0.80 + 0.90)^2 + (0.51 + 0.36 + 0.19)} = 0.84.\n\\]\nCon il pedice \\(b\\) che si riferisce al livello esterno, l’equazione per l’affidabilità a livello esterno in un modello configurale diventa quindi:\n\\[\n\\omega_b = \\frac{\\sum (\\lambda_i^2 \\Phi_b)}{\\sum (\\lambda_i^2 (\\Phi_b + \\Phi_w/n)) + \\sum (\\theta_b + \\theta_w/n)},\n\\]\ndove \\(n\\) è il numero di misurazioni. In questa equazione, la varianza dell’errore di campionamento delle medie osservate a livello di persona viene aggiunta al denominatore aggiungendo \\(\\Phi_w/n\\) e \\(\\Sigma \\theta_w/n\\). Nel nostro esempio, useremo \\(n = 15\\). Nel contesto di misure longitudinali, ciò significa che i dati sono stati raccolti in 15 occasioni. Inserendo i valori esemplificativi di Alphen et al. (2022), l’affidabilità a livello esterno è 0.90, indicando che il fattore comune a livello esterno spiega il 90% della varianza totale nelle medie osservate a livello di persona dei punteggi della scala:\n\\[\n\\omega_b = \\frac{(0.70 + 0.80 + 0.90)^2}{(0.70 + 0.80 + 0.90)^2(0.90 + 1/15) + (0.05 + 0.05 + 0.05) + ((0.51 + 0.36 + 0.19)/15)} = 0.90.\n\\]\nQueste equazioni forniscono un metodo per valutare l’affidabilità delle componenti a livello interno ed esterno di una scala in studi longitudinali intensivi, consentendo ai ricercatori di distinguere tra variazioni stabili e temporanee all’interno dei dati.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Affidabilità longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/sem/12_temp_reliability.html#confronto-con-la-teoria-della-generalizzabilità",
    "href": "chapters/sem/12_temp_reliability.html#confronto-con-la-teoria-della-generalizzabilità",
    "title": "50  Affidabilità longitudinale",
    "section": "50.5 Confronto con la Teoria della Generalizzabilità",
    "text": "50.5 Confronto con la Teoria della Generalizzabilità\nAlphen et al. (2022) hanno anche derivato le componenti di varianza per il calcolo del punteggio di affidabilità a livello interno e a livello esterno utilizzando la teoria della generalizzabilità. Per questi dati, Alphen et al. (2022) trovano che la stima dell’affidabilità a livello interno è .87, molto simile alla stima ottenuta con l’approccio CFA multilivello. Tuttavia, la stima a livello esterno ottenuta con il metodo della generalizzabilità è 0.99, che è .11 più alta rispetto all’approccio analitico fattoriale. Questa differenza potrebbe essere causata dalle assunzioni più rigide fatte dal metodo della teoria della generalizzabilità. Tuttavia, Alphen et al. (2022) notano che questi risultati sono specifici al dataset utilizzato e sarebbe necessario uno studio di simulazione per valutare, in generale, quali sono le differenze sistematiche tra le stime di affidabilità ottenute con i due diversi metodi.\nQui sotto viene presentato il metodo SEM per il calcolo dell’affidabilità inter- e intra-persona usando gli script R forniti da Alphen et al. (2022).\n\nvan_alphen &lt;- read.table(\"../../data/data_van_alphen.dat\", na.strings = \"9999\")\ncolnames(van_alphen) &lt;- c(\"day\", \"school\", \"ID\", \"str1\", \"str2\", \"str3\", \"str4\")\nvan_alphen |&gt; head()\n\n\nA data.frame: 6 x 7\n\n\n\nday\nschool\nID\nstr1\nstr2\nstr3\nstr4\n\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n1\n1\n1\n26\nNA\n24\n78\n\n\n2\n1\n1\n30\nNA\n24\n24\n50\n\n\n3\n1\n1\n55\nNA\n36\n70\n72\n\n\n4\n1\n1\n92\nNA\n37\n34\n41\n\n\n5\n1\n2\n20\n24\nNA\n24\n36\n\n\n6\n1\n2\n22\n12\nNA\n18\n39\n\n\n\n\n\n\nvan_alphen |&gt; tail()\n\n\nA data.frame: 6 x 7\n\n\n\nday\nschool\nID\nstr1\nstr2\nstr3\nstr4\n\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1264\n15\n6\n62\nNA\n66\n57\n61\n\n\n1265\n15\n6\n87\nNA\n42\n59\n83\n\n\n1266\n15\n6\n115\n53\nNA\n53\n45\n\n\n1267\n15\n6\n118\nNA\n16\n32\n16\n\n\n1268\n15\n6\n123\n23\n22\n23\nNA\n\n\n1269\n15\n6\n143\n64\n64\n65\nNA\n\n\n\n\n\n\n## Step 5: calculating reliability indices\n\nmodel5 &lt;- \"\n        level: 1\n            stress =~ L1*str1 + L2*str2 + L3*str3 + L4*str4\n            stress ~~ 1*stress\n\n            str1 ~~ tw1*str1\n            str2 ~~ tw2*str2\n            str3 ~~ tw3*str3\n            str4 ~~ tw4*str4\n\n        level: 2\n            stress =~ L1*str1 + L2*str2 + L3*str3 + L4*str4\n            stress ~~ fb*stress\n\n            str1 ~~ tb1*str1\n            str2 ~~ tb2*str2\n            str3 ~~ tb3*str3\n            str4 ~~ tb4*str4\n\n          # means\n            str1 + str2 + str3 + str4  ~ 1\n\n          # reliability calculations\n          lambda := L1 + L2 + L3 + L4\n          thetaw := tw1 + tw2 + tw3 + tw4\n          thetab := tb1 + tb2 + tb3 + tb4\n          omega_w := lambda^2 / (lambda^2 + thetaw)\n          omega_b := (lambda^2 * fb) / (lambda^2 * (1 / 15 + fb) + thetab + thetaw / 15)\n    \"\n\n\nfit.step5 &lt;- lavaan(\n    model = model5, \n    data = van_alphen, \n    cluster = \"ID\",\n    auto.var = TRUE,\n    missing = \"fiml\"\n)\n\n\nsummary(fit.step5) |&gt; print()\n\nlavaan 0.6-18 did not run (perhaps do.fit = FALSE)?\n** WARNING ** Estimates below are simply the starting values\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        21\n  Number of equality constraints                     4\n\n                                                  Used       Total\n  Number of observations                          1255        1269\n  Number of clusters [ID]                          151            \n  Number of missing patterns -- level 1              3            \n\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\n\nLevel 1 [within]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  stress =~                                           \n    str1      (L1)    1.000       NA                  \n    str2      (L2)    0.814       NA                  \n    str3      (L3)    0.928       NA                  \n    str4      (L4)    0.889       NA                  \n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    stress            1.000                           \n   .str1     (tw1)  434.798       NA                  \n   .str2     (tw2)  361.343       NA                  \n   .str3     (tw3)  415.584       NA                  \n   .str4     (tw4)  431.975       NA                  \n\n\nLevel 2 [ID]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  stress =~                                           \n    str1      (L1)    1.000       NA                  \n    str2      (L2)    0.814       NA                  \n    str3      (L3)    0.928       NA                  \n    str4      (L4)    0.889       NA                  \n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .str1             33.658       NA                  \n   .str2             26.361       NA                  \n   .str3             34.638       NA                  \n   .str4             34.939       NA                  \n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    stress    (fb)    0.050       NA                  \n   .str1     (tb1)  434.798       NA                  \n   .str2     (tb2)  361.343       NA                  \n   .str3     (tb3)  415.584       NA                  \n   .str4     (tb4)  431.975       NA                  \n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    lambda            3.631                           \n    thetaw         1643.701                           \n    thetab         1643.701                           \n    omega_w           0.008                           \n    omega_b           0.000                           \n\n\n\n\n\n\n\nAlphen, Thijmen van, Suzanne Jak, Joost Jansen in de Wal, Jaap Schuitema, e Thea Peetsma. 2022. «Determining reliability of daily measures: An illustration with data on teacher stress». Applied Measurement in Education 35 (1): 63–79.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Affidabilità longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/sem/E_01.html",
    "href": "chapters/sem/E_01.html",
    "title": "51  ✏️ Esercizi",
    "section": "",
    "text": "Nello studio di {cite:t}weiss2018difficulties viene esaminata la relazione tra la difficiltà di regolare le emozioni positive e l’abuso di alcol e di sostanze. Gli autori propongono due modelli SEM. Si riproduca l’analisi svolta da {cite:t}weiss2018difficulties usando lavaan.\n\nsource(\"../_common.R\")\n\nsuppressPackageStartupMessages({\n    library(\"lavaan\")\n    library(\"lavaanExtra\")\n    library(\"lavaanPlot\")\n    library(\"psych\")\n    library(\"dplyr\") \n    library(\"tidyr\")\n    library(\"knitr\")\n    library(\"mvnormalTest\")\n    library(\"semPlot\")\n    library(\"DiagrammeRsvg\")\n    library(\"rsvg\")\n    library(\"effectsize\")\n})\nset.seed(42)\n\nNello studio di {cite:t}weiss2018difficulties\n\nLa difficoltà di regolare le emozioni positive viene misurata con la Difficulties in Emotion Regulation Scale – Positive (DERS-P; Weiss, Gratz, & Lavender, 2015), che comprende le sottoscale di Acceptance, Impulse, e Goals.\nL’abuso di sostanze viene misurato con la Drug Abuse Screening Test (DAST; Skinner, 1982).\nL’abuso di alcol viene misurato con la Alcohol Use Disorder Identification Test (AUDIT; Saunders, Aasland, Babor, De la Fuente, & Grant, 1993), con le sottoscale di Hazardous Consumption, Dependence, e Consequences.\n\nI dati di un campione di 284 partecipanti sono riportati nella forma di una matrice di correlazione.\n\nlower &lt;- \"\n   1\n   .38 1\n   .41 .64 1\n   .34 .44 .30 1\n   .29 .12 .27 .06 1\n   .29 .22 .20 .17 .54 1\n   .30 .15 .23 .09 .73 .69 1\n\"\n\n\ndat_cov &lt;- lavaan::getCov(\n    lower,\n    names = c(\"dmis\", \"con\", \"dep\", \"consu\", \"acc\", \"goal\", \"imp\")\n)\nprint(dat_cov)\n\n      dmis  con  dep consu  acc goal  imp\ndmis  1.00 0.38 0.41  0.34 0.29 0.29 0.30\ncon   0.38 1.00 0.64  0.44 0.12 0.22 0.15\ndep   0.41 0.64 1.00  0.30 0.27 0.20 0.23\nconsu 0.34 0.44 0.30  1.00 0.06 0.17 0.09\nacc   0.29 0.12 0.27  0.06 1.00 0.54 0.73\ngoal  0.29 0.22 0.20  0.17 0.54 1.00 0.69\nimp   0.30 0.15 0.23  0.09 0.73 0.69 1.00\n\n\nIn questo studio, gli autori adottano due modelli SEM distinti per analizzare i dati. Nel primo modello, si postula che la difficoltà nella regolazione delle emozioni positive funzioni come variabile esogena, influenzando sia l’abuso di sostanze sia l’abuso di alcol. Inoltre, si ipotizza una correlazione tra abuso di sostanze e abuso di alcol, suggerendo una possibile interdipendenza tra questi due comportamenti problematici.\nPer quanto riguarda le variabili latenti specifiche, la difficoltà di regolare le emozioni positive, indicata come drpe, è rappresentata da una variabile latente che si basa su tre indicatori.Parallelamente, l’abuso di alcol, etichettato come amis, è concepito come una seconda variabile latente, anch’essa identificata tramite tre indicatori distinti.\n\nmod &lt;- \"\n  drpe =~ NA*acc + goal + imp\n  amis =~ NA*con + dep + consu\n  amis ~ drpe\n  dmis ~ drpe\n  dmis ~~ amis\n  drpe ~~ 1*drpe\n  amis ~~ 1*amis\n\"\n\nAdattiamo il modello ai dati con sem().\n\nfit &lt;- lavaan::sem(mod, sample.cov = dat_cov, sample.nobs = 284)\n\nEsaminiamo i risultati.\n\nstandardizedSolution(fit) |&gt; print()\n\n     lhs op   rhs est.std    se      z pvalue ci.lower ci.upper\n1   drpe =~   acc   0.770 0.031 24.982  0.000    0.710    0.830\n2   drpe =~  goal   0.728 0.033 21.849  0.000    0.663    0.794\n3   drpe =~   imp   0.945 0.024 39.322  0.000    0.898    0.992\n4   amis =~   con   0.837 0.039 21.217  0.000    0.759    0.914\n5   amis =~   dep   0.756 0.041 18.420  0.000    0.676    0.837\n6   amis =~ consu   0.494 0.052  9.439  0.000    0.392    0.597\n7   amis  ~  drpe   0.254 0.066  3.863  0.000    0.125    0.383\n8   dmis  ~  drpe   0.334 0.056  6.001  0.000    0.225    0.443\n9   amis ~~  dmis   0.458 0.055  8.303  0.000    0.350    0.567\n10  drpe ~~  drpe   1.000 0.000     NA     NA    1.000    1.000\n11  amis ~~  amis   0.936 0.033 28.023  0.000    0.870    1.001\n12   acc ~~   acc   0.407 0.047  8.575  0.000    0.314    0.500\n13  goal ~~  goal   0.470 0.049  9.677  0.000    0.375    0.565\n14   imp ~~   imp   0.107 0.045  2.349  0.019    0.018    0.196\n15   con ~~   con   0.300 0.066  4.551  0.000    0.171    0.430\n16   dep ~~   dep   0.428 0.062  6.900  0.000    0.307    0.550\n17 consu ~~ consu   0.756 0.052 14.595  0.000    0.654    0.857\n18  dmis ~~  dmis   0.889 0.037 23.960  0.000    0.816    0.961\n\n\nCreiamo un path diagram.\n\nsemPaths(fit,\n    whatLabels = \"std\",\n    sizeMan = 10,\n    edge.label.cex = 0.9,\n    style = \"ram\",\n    nCharNodes = 0, nCharEdges = 0\n)\n\n\n\n\n\n\n\n\nGli autori esplorano un modello alternativo nel quale le relazioni causali vengono rovesciate: in questo caso è la difficoltà di regolazione delle emozioni positive ad essere la variabile esogena, e l’abuso di sostanze e l’abuso di alcol sono le variabili esogene.\n\nmod_alt &lt;- \"\n  drpe =~ NA*acc + goal + imp\n  amis =~ NA*con + dep + consu\n  drpe ~ amis + dmis\n  dmis ~~ amis\n  drpe ~~ 1*drpe\n  amis ~~ 1*amis\n\"\n\n\nfit_alt &lt;- sem(mod_alt, sample.cov = dat_cov, sample.nobs = 311)\n\n\nstandardizedSolution(fit_alt) |&gt; print()\n\n     lhs op   rhs est.std    se      z pvalue ci.lower ci.upper\n1   drpe =~   acc   0.770 0.029 26.143  0.000    0.712    0.828\n2   drpe =~  goal   0.728 0.032 22.864  0.000    0.666    0.791\n3   drpe =~   imp   0.945 0.023 41.149  0.000    0.900    0.990\n4   amis =~   con   0.837 0.038 22.203  0.000    0.763    0.910\n5   amis =~   dep   0.756 0.039 19.276  0.000    0.679    0.833\n6   amis =~ consu   0.494 0.050  9.877  0.000    0.396    0.592\n7   drpe  ~  amis   0.115 0.075  1.549  0.121   -0.031    0.261\n8   drpe  ~  dmis   0.276 0.066  4.189  0.000    0.147    0.405\n9   amis ~~  dmis   0.503 0.050 10.122  0.000    0.405    0.600\n10  drpe ~~  drpe   0.879 0.037 23.633  0.000    0.806    0.952\n11  amis ~~  amis   1.000 0.000     NA     NA    1.000    1.000\n12   acc ~~   acc   0.407 0.045  8.973  0.000    0.318    0.496\n13  goal ~~  goal   0.470 0.046 10.126  0.000    0.379    0.561\n14   imp ~~   imp   0.107 0.043  2.458  0.014    0.022    0.192\n15   con ~~   con   0.300 0.063  4.763  0.000    0.177    0.424\n16   dep ~~   dep   0.428 0.059  7.221  0.000    0.312    0.545\n17 consu ~~ consu   0.756 0.049 15.273  0.000    0.659    0.853\n18  dmis ~~  dmis   1.000 0.000     NA     NA    1.000    1.000\n\n\n\nsemPaths(fit_alt,\n    whatLabels = \"std\",\n    sizeMan = 10,\n    edge.label.cex = 0.9,\n    style = \"ram\",\n    nCharNodes = 0, nCharEdges = 0\n)\n\n\n\n\n\n\n\n\nVedremo in seguito come sia possibile eseguire un test statistico per stabilire quale di due modelli sia più appropriato. Anticipando qui tale discussione, applichiamo il test del rapporto di verosimiglianze.\n\nlavTestLRT(fit, fit_alt) |&gt; print()\n\nWarning message in lavTestLRT(fit, fit_alt):\n\"lavaan WARNING: some models have the same degrees of freedom\"\n\n\n\nChi-Squared Difference Test\n\n        Df    AIC    BIC  Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\nfit     12 4963.6 5022.0 38.211                                    \nfit_alt 12 5433.1 5492.9 41.844     3.6327     0       0           \n\n\nI risultati di questo test suggeriscono che il primo modello è maggiormente appropriato per descrivere i dati raccolti da {cite:t}weiss2018difficulties.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html",
    "href": "chapters/mokken/01_core_issues.html",
    "title": "52  Analisi della Scala di Mokken",
    "section": "",
    "text": "52.1 Introduzione\nL’Analisi delle Scale Mokken (MSA), così denominata in onore del matematico e scienziato politico olandese Robert J. Mokken, è un insieme di metodi utilizzati nell’ambito della Teoria Non Parametrica della Risposta agli Item (NIRT) per valutare l’adeguatezza dei dati ai suoi modelli. Secondo i principi fondamentali dei modelli della Teoria della Risposta agli Item (IRT), i costrutti psicologici sono considerati latenti, ovvero non direttamente osservabili, e si manifestano attraverso le risposte ai test. Le reazioni dei partecipanti ai test (cioè, le risposte agli item) indicano la loro posizione su un continuum latente e riflettono il grado in cui essi possiedono il costrutto in esame.\nNonostante l’apparente correlazione diretta, gli item di un test e le corrispondenti risposte dei partecipanti non sempre rappresentano fedelmente il costrutto in esame. I modelli della IRT forniscono strumenti analitici per esaminare la congruenza e la pertinenza degli item di un test con la variabile latente sottostante. In particolare, i modelli MSA, che sono modelli probabilistici basati su tratti latenti, rivestono un ruolo cruciale nella validazione di strumenti di misura psicometrici e nell’ordinare rispondenti e item lungo una scala ordinale. I modelli MSA sono applicabili sia a item dicotomici che politomici, hanno una natura non parametrica sono caratterizzati da presupposti meno restrittivi rispetto ai modelli IRT parametrici, sebbene ciò possa comportare alcune restrizioni interpretative e di applicazione.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html#scaling-di-guttman",
    "href": "chapters/mokken/01_core_issues.html#scaling-di-guttman",
    "title": "52  Analisi della Scala di Mokken",
    "section": "52.2 Scaling di Guttman",
    "text": "52.2 Scaling di Guttman\nLa MSA trova le sue radici nello scaling di Guttman, sviluppato nel 1950 da Louis Guttman. Questo metodo, originariamente concepito per item dicotomici, mira a estrarre una singola dimensione dai dati, posizionando sia le persone che gli item su questa dimensione tramite valori numerici. Ad esempio, consideriamo cinque item di un test psicologico e cinque rispondenti ipotetici, A, B, C, D ed E, che rispondono a questi item. Le risposte vengono rappresentate in una tabella dove 1 indica una risposta corretta e 0 una errata.\n\n\n\nItems\nEsaminati\n1\n2\n3\n4\n5\n\n\n\n\n\nA\n1\n1\n1\n1\n1\n\n\n\nB\n1\n1\n1\n1\n0\n\n\n\nC\n1\n1\n1\n0\n0\n\n\n\nD\n1\n1\n0\n0\n0\n\n\n\nE\n1\n0\n0\n0\n0\n\n\n\nIn questa tabella, che rappresenta una scala di Guttman, gli item sono ordinati dal più facile al più difficile, e i rispondenti dall’abilità maggiore a quella minore. Si presume che un rispondente che ha risposto correttamente ad un item di difficoltà superiore abbia risposto correttamente anche a tutti gli item di difficoltà inferiore. Le deviazioni da questo modello sono considerate “errori di Guttman”.\nLa scala di Guttman, basandosi su un principio deterministico, non riesce sempre a catturare pienamente la complessità dei dati reali. Tuttavia, offre una rappresentazione chiara della relazione cumulativa o gerarchica tra gli item di un test e le abilità dei rispondenti. Questo modello suggerisce che, se una persona dimostra una competenza specifica, si presume che possieda anche tutte le competenze di base correlate.\nTuttavia, quando emergono eccezioni a questa regola, ossia risposte corrette a item difficili ma errate a quelli più semplici, si potrebbe dedurre che il test richieda competenze multiple e non unicamente riconducibili a una dimensione unica. Questo fenomeno mette in luce la complessità e la multidimensionalità delle competenze umane.\nIn conclusione, la MSA si adatta particolarmente a contesti dove i processi di risposta non sono completamente chiari (come quando la prestazione dipende da una struttura multidimensionale di abilità), consentendo di verificare se le risposte dei partecipanti rispettino i requisiti del modello e di ordinare persone e item su una scala ordinale, basandosi sui punteggi totali grezzi.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html#analisi-della-scala-di-mokken",
    "href": "chapters/mokken/01_core_issues.html#analisi-della-scala-di-mokken",
    "title": "52  Analisi della Scala di Mokken",
    "section": "52.3 Analisi della Scala di Mokken",
    "text": "52.3 Analisi della Scala di Mokken\nLa MSA si colloca all’interno del paradigma dello scaling di Guttman, ma transita dal modello deterministico classico di Guttman a un approccio probabilistico. Il modello di Guttman si basa sul principio di perfetta transitività, secondo cui, in presenza di una sequenza di item ordinati per difficoltà, un rispondente che riesce a rispondere correttamente a un item difficoltoso dovrebbe anche rispondere correttamente agli item più semplici. Tuttavia, questo modello deterministico cumulativo spesso non riflette la realtà dei dati empirici, influenzati dalla complessità delle abilità umane e da altre variabili. Di conseguenza, sono stati adottati modelli probabilistici, come la MSA e il modello di Rasch, che permettono una certa varianza nelle risposte.\nLa MSA, evoluzione probabilistica dello scaling di Guttman, consente un certo grado di violazioni di questo modello, risultando meno restrittiva e più aderente ai dati empirici. Questo modello facilita l’analisi dell’unidimensionalità e la misurazione di variabili latenti su una scala unidimensionale, specialmente utile con un numero limitato di item.\nA differenza di metodi come l’analisi fattoriale e l’analisi di affidabilità, la scala di Guttman e, per estensione, la MSA, sono ottimizzate per l’analisi di item con significative differenze di difficoltà. La MSA si articola in due modelli IRT non parametrici principali: il Modello di Omogeneità Monotona (MHM) e il Modello di Monotonicità Doppia (DMM).\nModello di Omogeneità Monotona (MHM) Il MHM, il primo modello della MSA, si fonda su tre presupposti fondamentali: unidimensionalità, monotonicità e indipendenza locale. Queste assunzioni sono vitali per il suo funzionamento. Se valide, consentono di posizionare gli individui su una scala unidimensionale ordinale, con un ordinamento costante attraverso tutti gli item. In presenza di un insieme omogeneo di item, l’ordinamento degli individui rimane invariato per ogni sottoinsieme di item. Il MHM fornisce una base robusta per l’analisi delle risposte agli item, soprattutto in contesti dove la monotonicità è cruciale.\nModello di Monotonicità Doppia (DMM) Il DMM estende il MHM includendo un ulteriore vincolo: le Funzioni di Risposta agli Item (IRF) non devono intersecarsi. Questo modello esamina la relazione tra la difficoltà degli item e il livello di abilità dei rispondenti. La validità dell’Invariance of Item Ordering (IIO) indica che l’ordinamento degli item per difficoltà è uniforme a tutti i livelli di abilità. La violazione dell’IIO può suggerire un fenomeno di Differential Item Functioning (DIF), mettendo in dubbio l’invarianza della misurazione e suggerendo che l’ordine degli item potrebbe variare tra sottogruppi di rispondenti con abilità simili.\nIn conclusione, la MSA rappresenta un’evoluzione metodologica significativa rispetto allo scaling di Guttman, offrendo un approccio meno restrittivo e più adatto all’analisi dei dati empirici. Fornisce un quadro analitico efficace per l’indagine dettagliata delle risposte agli item e delle abilità dei rispondenti in contesti psicologici e educativi.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html#confronto-tra-il-modello-di-rasch-e-lanalisi-della-scala-di-mokken",
    "href": "chapters/mokken/01_core_issues.html#confronto-tra-il-modello-di-rasch-e-lanalisi-della-scala-di-mokken",
    "title": "52  Analisi della Scala di Mokken",
    "section": "52.4 Confronto tra il Modello di Rasch e l’Analisi della Scala di Mokken",
    "text": "52.4 Confronto tra il Modello di Rasch e l’Analisi della Scala di Mokken\nIl Modello di Rasch (RM) assume che la probabilità di risposta corretta a un item sia determinata dall’abilità della persona (θ) e dalla difficoltà dell’item (δ). La relazione tra la probabilità di una risposta corretta e θ è descritta da una funzione logistica, con IRFs parallele e dalla stessa pendenza. Questo modello è definito parametrico poiché utilizza una funzione parametrica specifica, la funzione logistica, per stabilire tale relazione.\nIl Modello di Rasch prevede che i punteggi grezzi totali siano sufficienti per stimare i parametri delle persone e degli item. È considerato il modello più restrittivo rispetto al Modello di Monotonicità Omogenea (MHM) e al Modello di Monotonicità Doppia (DMM) per le sue assunzioni aggiuntive.\nD’altra parte, l’Analisi della Scala di Mokken (MSA) appartiene alla categoria dei modelli non parametrici della IRT (NIRT), che non prevedono una funzione specifica per le IRFs. In questi modelli, il punteggio grezzo totale fornisce un ordine basato sulla variabile latente θ, poiché θ non è direttamente stimato. Questo approccio è meno restrittivo rispetto ai modelli parametrici e consente una maggiore flessibilità nell’analisi dei dati.\nIl MHM, il modello meno restrittivo tra i tre, si basa su tre assunzioni fondamentali: unidimensionalità, monotonicità e indipendenza locale. Questo modello permette di ordinare gli individui su una scala unidimensionale ordinale, mantenendo costante questo ordinamento attraverso tutti gli item. Il DMM, invece, aggiunge l’assunzione di IRF non intersecanti al MHM, producendo scale ordinali separate per persone e item.\nUn punto critico è che, mentre il Modello di Rasch crea una scala metrica comune per persone e item, consentendo misurazioni indipendenti da persone e item, il DMM genera scale ordinali distinte. Inoltre, i modelli NIRT, come la MSA, non richiedono la conformità degli item a una funzione logistica specifica, evitando così la necessità di scartare gli item che non seguono tale funzione, una limitazione dei modelli PIRT (Parametric Item Response Theory).\nIn sintesi, mentre il Modello di Rasch offre un approccio più restrittivo ma preciso, basato su assunzioni specifiche e una scala metrica comune, la MSA offre una maggiore flessibilità e applicabilità a un’ampia gamma di dati, pur producendo scale ordinali distinte per persone e item. Questa differenza chiave tra i due approcci sottolinea l’importanza di scegliere il modello più adatto in base agli obiettivi specifici e alla natura dei dati in esame.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html#confronto-tra-la-teoria-classica-dei-test-e-lanalisi-della-scala-di-mokken",
    "href": "chapters/mokken/01_core_issues.html#confronto-tra-la-teoria-classica-dei-test-e-lanalisi-della-scala-di-mokken",
    "title": "52  Analisi della Scala di Mokken",
    "section": "52.5 Confronto tra la Teoria Classica dei Test e l’Analisi della Scala di Mokken",
    "text": "52.5 Confronto tra la Teoria Classica dei Test e l’Analisi della Scala di Mokken\nEsaminiamo ora le somiglianze e le differenze tra la Teoria Classica dei Test (CTT) e l’Analisi della Scala di Mokken (MSA).\nLa CTT si basa su diverse assunzioni fondamentali: 1. I punteggi osservati sono la somma dei punteggi veri e dei punteggi di errore, con l’aspettativa che i punteggi di errore abbiano una media di zero su prove ripetute. 2. Non c’è correlazione tra i punteggi di errore e i punteggi veri. 3. I punteggi veri in un test non sono correlati ai punteggi di errore in un altro test. 4. I punteggi di errore in due test somministrati agli stessi soggetti sono non correlati.\nNella CTT, i punteggi grezzi totali sono considerati indicatori delle posizioni delle persone sul continuum del tratto latente. La proporzione di item corretti (valore p) indica la facilità degli item, mentre la correlazione corretta tra item e punteggio totale misura la discriminazione degli item. La CTT enfatizza anche l’importanza dell’affidabilità, definita come la correlazione tra i punteggi osservati su due forme parallele del test.\nConfrontando la CTT con la MSA, troviamo alcune somiglianze nelle metodologie di calcolo degli indici di abilità delle persone e di difficoltà degli item. Nella MSA, il coefficiente di scalabilità dell’item Hi è analogo alle correlazioni corrette tra item e punteggio totale nella CTT. Analogamente, il coefficiente di scalabilità tra coppie di item Hij nella MSA corrisponde alle correlazioni tra coppie di item nella CTT, e il coefficiente di scalabilità complessivo H nella MSA è paragonabile agli indici di discriminazione media degli item nella CTT.\nTuttavia, una differenza fondamentale tra la MSA e la CTT risiede nella testabilità dei modelli. I modelli MSA permettono di verificare empiricamente le loro assunzioni, come l’indipendenza locale, l’unidimensionalità e la monotonicità. Ad esempio, un coefficiente di scalabilità negativo nella MSA smentirebbe gli assiomi del Modello di Omogeneità Monotona (MHM). Questa capacità di testare empiricamente le sue assunzioni rende la MSA un modello particolarmente robusto e trasparente.\nIn conclusione, mentre la CTT fornisce un quadro teorico solido per la comprensione e l’interpretazione dei punteggi dei test, la MSA offre un approccio più flessibile e testabile, particolarmente utile nell’analizzare la struttura dei dati dei test e nella valutazione della validità delle scale di misurazione. Queste caratteristiche rendono la MSA un complemento prezioso alla CTT nella pratica della misurazione psicologica ed educativa.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html#estensione-dellanalisi-delle-scale-di-mokken-agli-item-politomici",
    "href": "chapters/mokken/01_core_issues.html#estensione-dellanalisi-delle-scale-di-mokken-agli-item-politomici",
    "title": "52  Analisi della Scala di Mokken",
    "section": "52.6 Estensione dell’Analisi delle Scale di Mokken agli Item Politomici",
    "text": "52.6 Estensione dell’Analisi delle Scale di Mokken agli Item Politomici\nL’Analisi delle Scale di Mokken (MSA), inizialmente sviluppata per item dicotomici, è stata estesa da Molenaar (1982a, 1997) per includere anche gli item politomici. Questa estensione mantiene i principi fondamentali della MSA applicati agli item dicotomici, ma aggiunge alcune specificità legate alla natura degli item politomici.\nNel caso degli item politomici, come quelli usati nelle scale Likert, le assunzioni del modello MSA vengono esaminate non solo a livello dell’intero item, ma anche per ciascun “passaggio” o categoria di risposta. Prendendo come esempio un item Likert a cinque punti, che va da “fortemente d’accordo” a “fortemente in disaccordo”, ci sono quattro passaggi distinti, ognuno rappresentante una transizione tra due categorie consecutive.\nPer ogni passaggio di un item politomico, si definisce una Funzione di Risposta del Passaggio dell’Item (ISRF), che descrive la probabilità di scegliere una specifica categoria di risposta in funzione del tratto latente θ. Le ISRF sono cruciali per comprendere come le diverse categorie di risposta si relazionino al tratto latente misurato dall’item.\nAffinché il modello di omogeneità monotona sia valido per gli item politomici, è necessario che le probabilità di scegliere una categoria di risposta k o superiore aumentino monotonamente con l’aumento di θ. Questo implica che le categorie di risposta debbano essere ordinate in modo significativo, rappresentando livelli progressivamente più alti del tratto latente.\nUn aspetto fondamentale nell’analisi di item politomici nella MSA è l’assunzione di monotonicità, che richiede che le ISRF siano funzioni crescenti in θ. In altre parole, man mano che il tratto latente aumenta, aumenta anche la probabilità che un individuo scelga categorie di risposta superiori.\nIn sintesi, l’estensione della MSA agli item politomici fornisce uno strumento potente per analizzare item con più categorie di risposta, consentendo una misurazione più dettagliata e sfumata del tratto latente. Questa estensione rende la MSA particolarmente adatta per applicazioni in cui si utilizzano scale di risposta con gradazioni multiple, come nei questionari di valutazione del benessere psicologico, nei sondaggi di opinione o nelle valutazioni educative. Attraverso l’analisi dei passaggi degli item, la MSA per item politomici permette una comprensione più approfondita di come gli individui interagiscano con le diverse opzioni di risposta e di come queste risposte riflettano i loro livelli sul tratto latente.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html#laffidabilità-nei-test",
    "href": "chapters/mokken/01_core_issues.html#laffidabilità-nei-test",
    "title": "52  Analisi della Scala di Mokken",
    "section": "52.7 L’Affidabilità nei Test",
    "text": "52.7 L’Affidabilità nei Test\nL’affidabilità in ambito di test psicometrici si riferisce al grado in cui un test è esente da errori di misurazione. Si valuta tipicamente esaminando la stabilità dei punteggi ottenuti dagli esaminandi in diverse somministrazioni del test, sia nel tempo che attraverso forme parallele del test. L’idea di base è che, in assenza di cambiamenti nei punteggi veri degli esaminandi, ci si aspetterebbe una correlazione perfetta tra le diverse amministrazioni. Ogni deviazione da questa correlazione perfetta è attribuita all’influenza dell’errore di misurazione.\nTuttavia, ottenere misure di affidabilità attraverso forme parallele o ripetute somministrazioni nel tempo può essere impraticabile, a causa di problemi logistici e degli effetti di memoria o pratica. Pertanto, l’affidabilità è spesso stimata attraverso metodi che richiedono una singola somministrazione del test.\nL’alfa di Cronbach è uno degli estimatori di affidabilità più utilizzati, sebbene presenti diverse limitazioni. In risposta a queste limitazioni, Mokken (1971) ha sviluppato un coefficiente di affidabilità non distorto, noto come ρ (rho) o statistica MS. Questo coefficiente presuppone la validità della doppia monotonicità, una supposizione piuttosto forte. Per affrontare alcune delle sfide associate alla statistica ρ, Van der Ark, Van der Palm e Sijtsma (2011) hanno proposto un altro indicatore di affidabilità chiamato Coefficiente di Affidabilità delle Classi Latenti (LCRC). Questo è uno stimatore non distorto dell’affidabilità dei punteggi dei test, le cui assunzioni sono meno stringenti rispetto alla statistica ρ, richiedendo solamente l’indipendenza locale. Questo rappresenta un vantaggio significativo del coefficiente LCRC rispetto al coefficiente ρ, poiché rende l’LCRC più applicabile e flessibile in una varietà di contesti di test.\nIn conclusione, la scelta del metodo più appropriato per stimare l’affidabilità dipende dalle caratteristiche specifiche del test e dalle esigenze di misurazione. Mentre l’alfa di Cronbach rimane uno standard ampiamente utilizzato, le alternative come il coefficiente ρ di Mokken e il LCRC offrono strumenti aggiuntivi e talvolta più adatti per valutare l’affidabilità, specialmente in situazioni dove le assunzioni dell’alfa di Cronbach non sono soddisfatte o quando si utilizzano modelli non parametrici come quelli proposti nell’Analisi delle Scale di Mokken.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html#coefficienti-di-scalabilità-nelle-scale-mokken",
    "href": "chapters/mokken/01_core_issues.html#coefficienti-di-scalabilità-nelle-scale-mokken",
    "title": "52  Analisi della Scala di Mokken",
    "section": "52.8 Coefficienti di Scalabilità nelle Scale Mokken",
    "text": "52.8 Coefficienti di Scalabilità nelle Scale Mokken\nI coefficienti di scalabilità nelle Scale Mokken, ovvero \\(H\\), \\(H_i\\) e \\(H_{ij}\\), sono indici chiave utilizzati per valutare la qualità di una misurazione nell’Analisi delle Scale Mokken (MSA). Questi coefficienti misurano la coerenza e l’ordinamento degli item e dei punteggi complessivi su un continuum latente, indicando in che misura gli item formano una gerarchia e se i punteggi degli item sono ordinati consistentemente.\n\nCoefficienti di Scalabilità Singoli (\\(H_i\\)): Indicano la qualità di ogni singolo item. Un valore elevato di \\(H_i\\) significa che l’item ha una buona discriminazione e contribuisce efficacemente all’ordinamento degli esaminandi. Valori superiori a 0.30 sono generalmente considerati accettabili.\nCoefficienti di Scalabilità per Coppie di Item (\\(H_{ij}\\)): Misurano la coerenza tra coppie di item. Valori positivi indicano che la coppia di item è coerente con il modello di omogeneità monotona. Valori negativi possono suggerire multidimensionalità o non monotonicità.\nCoefficienti di Scalabilità per l’Intero Test (\\(H\\)): Questo indice valuta la qualità dell’intero test, indicando in che misura la struttura complessiva dei dati si avvicina a un modello di Guttman perfetto. Valori tra 0.30 e 0.40 indicano una scala debole, tra 0.40 e 0.50 una scala media e superiori a 0.50 una scala forte.\n\nQuesti coefficienti vengono calcolati basandosi sul rapporto tra gli errori di Guttman osservati e quelli attesi. Un coefficiente di \\(H\\) vicino a uno implica una perfetta conformità al modello di Guttman, mentre valori vicini a zero indicano la presenza di numerosi errori di Guttman.\nLa MSA consente di testare empiricamente se i dati si adattano al modello di omogeneità monotona, fornendo un quadro robusto per l’analisi degli item e dei punteggi dei test. I coefficienti di scalabilità offrono una guida per determinare la qualità e la coerenza degli item nel contesto di una scala unidimensionale. Sono particolarmente utili per identificare item che potrebbero essere ridondanti o non allineati con il tratto latente misurato.\nInoltre, i coefficienti di scalabilità forniscono informazioni preziose sulla validità di costrutto di una scala. Anche se una scala ha una forte discriminazione (indicata da valori elevati di \\(H_i\\) e \\(H\\)), potrebbe mancare di validità di costrutto se i suoi item misurano solo una porzione ristretta del costrutto. Allo stesso modo, valori elevati di \\(H_{ij}\\) tra specifiche coppie di item possono suggerire che uno degli item nella coppia sia ridondante.\nIn sintesi, i coefficienti di scalabilità nelle Scale Mokken non solo valutano la precisione nell’ordinamento degli esaminandi e la qualità degli item, ma aiutano anche a comprendere meglio la struttura e la validità di una scala. Questi coefficienti, quindi, giocano un ruolo cruciale nella selezione e nell’analisi degli item in contesti di misurazione psicometrica, educativa e di ricerca.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html#gli-errori-standard-nei-coefficienti-di-scalabilità-delle-scale-mokken",
    "href": "chapters/mokken/01_core_issues.html#gli-errori-standard-nei-coefficienti-di-scalabilità-delle-scale-mokken",
    "title": "52  Analisi della Scala di Mokken",
    "section": "52.9 Gli Errori Standard nei Coefficienti di Scalabilità delle Scale Mokken",
    "text": "52.9 Gli Errori Standard nei Coefficienti di Scalabilità delle Scale Mokken\nGli errori standard (SE) sono fondamentali per interpretare correttamente i coefficienti di scalabilità nelle scale Mokken. Questi errori standard tengono conto dell’incertezza delle stime. Se l’errore standard è grande rispetto al coefficiente stesso, ad esempio un SE di .08 per un coefficiente Hi di .30, è probabile che il valore reale del coefficiente nella popolazione sia inferiore a .30, suggerendo che gli item potrebbero non essere scalabili.\nLa dimensione dell’errore standard dipende da due fattori: la dimensione del campione e l’asimmetria delle distribuzioni dei punteggi degli item. Con un campione più grande, gli errori standard sono generalmente più piccoli, mentre distribuzioni dei punteggi più asimmetriche portano a errori standard più grandi. Tuttavia, un ampio campione non garantisce stime precise dei coefficienti di scalabilità.\nPer i coefficienti di scalabilità, possiamo calcolare gli intervalli di confidenza al 95% (CI) usando la formula:\n\\[\n\\text{95% CI} = H_i \\pm (1.96 \\times \\text{SE})\n\\]\nPer esempio, se \\(H_i\\) è .30 con un SE di .10, il CI sarà tra .10 e .50. Questo intervallo ampio implica che il valore reale di \\(H_i\\), con il 95% di confidenza, si trova in questo range, indicando una bassa affidabilità del coefficiente. Se \\(H_i\\) è .15 con un SE di .10, il CI sarà tra -.05 e .35, suggerendo che il vero coefficiente potrebbe essere zero o anche negativo nella popolazione, e quindi l’item dovrebbe essere scartato.\nMokken (1971) ha indicato che la monotonicità delle funzioni di risposta all’item (IRF) per tutti gli item utilizzati nel calcolo del punteggio totale X+ è una condizione sufficiente per la loro utilità nella classificazione degli esaminandi. Di conseguenza, gli item con bassi coefficienti di scalabilità vengono generalmente scartati.\nTuttavia, Crișan e colleghi (2020) consigliano di non rimuovere item inadatti dalle scale se non vi sono altri argomenti (ad esempio, di contenuto) per farlo. I guadagni in affidabilità, selezione delle persone e validità predittiva potrebbero non compensare la perdita di copertura del costrutto e validità dei criteri. Pertanto, la decisione di mantenere o rimuovere item da una scala dovrebbe basarsi principalmente su considerazioni teoriche, e i ricercatori applicati dovrebbero essere cauti nel non utilizzare regole empiriche per eliminare item in modo acritico.\nIn sintesi, l’analisi degli errori standard nei coefficienti di scalabilità delle scale Mokken fornisce informazioni cruciali sulla affidabilità e la validità degli item della scala. Tuttavia, le decisioni su quali item mantenere o scartare dovrebbero essere prese considerando non solo gli aspetti psicometrici ma anche il contesto teorico e il contenuto della scala stessa.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html#procedura-di-selezione-automatica-degli-item",
    "href": "chapters/mokken/01_core_issues.html#procedura-di-selezione-automatica-degli-item",
    "title": "52  Analisi della Scala di Mokken",
    "section": "52.10 Procedura di Selezione Automatica degli Item",
    "text": "52.10 Procedura di Selezione Automatica degli Item\nLa Procedura di Selezione Automatica degli Item (AISP) è una metodologia impiegata nella MSA per selezionare un insieme di item da un pool più ampio che aderiscano alle assunzioni del Modello di Mokken (MHM). L’AISP aiuta a esaminare l’unidimensionalità e identifica item non scalabili.\nUna scala di Mokken si compone di una serie di item selezionati in base a due criteri specifici. Prima di tutto, ogni item deve avere una covarianza (\\(H_{i}\\)) che superi un valore soglia (c), scelto dall’utente. Solitamente, si raccomanda di impostare questo valore soglia a c=.30. Il secondo criterio richiede che la covarianza tra ogni coppia di item (\\(H_{ij}\\)) sia maggiore di zero. In sintesi, per essere inclusi in una scala di Mokken, gli item devono avere sia una covarianza individuale (\\(H_{i}\\)) sia una covarianza reciproca (\\(H_{ij}\\)) positive e superiori a un valore minimo predefinito.\nQuesto processo inizia selezionando due item con la più alta \\(H_{ij}\\) e continua aggiungendo nuovi item che soddisfano i criteri. Se alcuni item non rispettano questi criteri, l’AISP tenta di costruire una seconda scala o li identifica come non scalabili.\nLe scale costruite con l’AISP misurano un tratto latente comune, ordina in modo affidabile le persone e discriminano bene. Tuttavia, talvolta, un item può essere selezionato con un valore \\(H_{i}\\) inferiore a c, contraddicendo la definizione di scala di Mokken. Questi item inadatti dovrebbero essere esclusi successivamente.\nL’AISP può essere vista come un’alternativa più efficiente all’analisi fattoriale, in quanto non è influenzata dalle difficoltà degli item e può essere applicata sia a item dicotomici che politomici. Tuttavia, i ricercatori dovrebbero considerare la teoria sostanziale e non affidarsi solo alle soluzioni statistiche prodotte dal software.\nÈ importante notare che la scelta del valore limite inferiore c influisce sulla struttura della scala identificata. Valori più alti di c possono portare al rifiuto di molti item e alla formazione di scale sostanzialmente prive di significato con pochi item. D’altra parte, valori bassi di c possono nascondere la vera dimensionalità dei dati includendo tutti gli item in una singola scala. Il valore scelto dovrebbe dipendere dall’obiettivo specifico della ricerca.\nInoltre, l’AISP è paragonabile all’analisi fattoriale esplorativa, ma a differenza dell’analisi fattoriale, l’AISP può concludere senza trovare una scala valida se tutti i valori di \\(H_{ij}\\) sono inferiori a .30. Invece, l’analisi fattoriale trova sempre una soluzione, anche se non necessariamente significativa.\nIn conclusione, l’AISP è uno strumento utile per la costruzione di scale di Mokken, ma presenta limitazioni nella valutazione della dimensionalità. Gli studi di simulazione mostrano che questo metodo può essere meno efficiente rispetto ad altri metodi non parametrici nel rilevare la vera dimensionalità dei dati, soprattutto quando le dimensioni sono correlate o gli item saturano su più di una dimensione. Pertanto, i ricercatori dovrebbero utilizzare questo strumento con cautela e considerare un’ampia gamma di valori limite inferiori c per rivelare la vera struttura dei dati.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html#monotonicità",
    "href": "chapters/mokken/01_core_issues.html#monotonicità",
    "title": "52  Analisi della Scala di Mokken",
    "section": "52.11 Monotonicità",
    "text": "52.11 Monotonicità\nLa monotonicità, un concetto chiave nelle scale Mokken, si riferisce alla relazione tra la posizione di una persona su una variabile latente (una caratteristica o tratto non direttamente osservabile) e la sua probabilità di rispondere correttamente a un item (domanda o affermazione). In sostanza, man mano che una persona si sposta verso livelli più elevati sulla variabile latente, la sua probabilità di dare una risposta corretta dovrebbe aumentare o rimanere la stessa, ma non diminuire. Questo principio si applica sia agli item con due possibili risposte (dicotomici) sia a quelli con più risposte (politomici).\nPer valutare la monotonicità, si utilizzano diversi metodi, tra cui l’analisi dei gruppi di restscore. Il “restscore” è il punteggio totale ottenuto da un individuo in un test, escludendo il punteggio dell’item specifico che si sta analizzando. Ad esempio, in un test di 10 item, se si vuole esaminare l’item numero 10, il restscore per ogni persona sarà il suo punteggio totale escludendo il punteggio ottenuto all’item 10. Di conseguenza, si creano diversi gruppi di restscore, che vanno da 0 a 9 in questo caso.\nLa relazione tra restscore e monotonicità è la seguente: nei grafici, i gruppi di restscore sono confrontati con la percentuale di persone che hanno risposto correttamente all’item in questione all’interno di ogni gruppo. Idealmente, al crescere del restscore, la percentuale di risposte corrette dovrebbe aumentare o rimanere costante. Se i gruppi di restscore sono piccoli e quindi non forniscono stime affidabili, possono essere combinati con gruppi adiacenti per ottenere dimensioni maggiori e stime più precise.\nIl restscore funge da sostituto per θ, la posizione sulla variabile latente. Se la monotonicità è rispettata, ci si aspetta che la percentuale di risposte corrette aumenti (o almeno rimanga costante) man mano che aumenta il restscore. In altre parole, persone con un restscore più alto dovrebbero avere una probabilità maggiore di rispondere correttamente rispetto a quelle con un restscore più basso. Questa aspettativa dovrebbe essere valida per tutte le coppie di gruppi di restscore.\nL’analisi delle Funzioni di Risposta all’Item (IRF) è particolarmente utile perché permette di osservare come la performance degli item varia lungo il continuum del tratto latente. A differenza dell’IRT parametrico, dove l’attenzione è sulla stima dei parametri, l’IRT non parametrico (NIRT) si concentra sui metodi grafici, che sono fondamentali per comprendere come gli item funzionino a diversi livelli del tratto latente.\nPer gli item politomici, la monotonicità è valutata sia complessivamente sia all’interno delle singole categorie di risposta. Inoltre, si utilizzano i coefficienti di scalabilità per valutare la monotonicità. Se il Modello di Omoegeneità Monotona (MHM) è valido, le covarianze tra tutte le coppie di item (Hij) devono essere non negative. Tuttavia, Hij non negativi non sono una condizione sufficiente per garantire IRFs non decrescenti e non assicurano l’adattamento al MHM. Nella pratica, item con valori di Hi superiori a .30 sono generalmente considerati accettabili.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html#ordinamento-invariante-degli-item",
    "href": "chapters/mokken/01_core_issues.html#ordinamento-invariante-degli-item",
    "title": "52  Analisi della Scala di Mokken",
    "section": "52.12 Ordinamento Invariante degli Item",
    "text": "52.12 Ordinamento Invariante degli Item\nIn contesti psicologici ed educativi, la definizione della difficoltà degli item di un test è cruciale. Generalmente, questa difficoltà viene determinata attraverso le medie degli item nella popolazione target. Tuttavia, è importante considerare che l’ordine di difficoltà derivato dalle risposte medie della popolazione potrebbe non essere universale per ogni individuo.\nIl concetto di Ordinamento Invariante degli Item (IIO) si riferisce alla necessità che l’ordine di difficoltà degli item rimanga consistente tra diversi sottogruppi di persone. Questo aspetto è fondamentale per garantire che i confronti tra i gruppi basati sui punteggi totali siano validi e che le differenze nei punteggi totali abbiano un significato reale. In ambito psicologico, ad esempio nei questionari sulla depressione o sull’ansia, l’IIO implica che un individuo con un punteggio totale più alto manifesti tutti i sintomi di una persona con un punteggio inferiore, oltre a sintomi aggiuntivi.\nL’IIO è altresì desiderabile quando si ordinano gli item di un test da quelli più facili a quelli più difficili, per garantire che questa progressione sia valida per tutti gli esaminandi. In altre parole, un item considerato facile dovrebbe essere tale per tutti i partecipanti, così come un item difficile dovrebbe rappresentare una sfida per tutti.\nUn ordinamento degli item che non rispetta l’IIO indica una variazione nella difficoltà degli item tra diversi gruppi. Questo può suggerire una funzione differenziale dell’item (DIF) o un bias, rendendo problematico ordinare gli item in base alla loro difficoltà.\nPer valutare l’IIO, si utilizzano diverse tecniche come il metodo dei gruppi di restscore, il metodo di divisione degli item, le matrici delle proporzioni P(++)/P(–), e il metodo di divisione dei restscore. Queste procedure aiutano a determinare se l’ordine di difficoltà degli item è coerente attraverso diversi gruppi.\nIn conclusione, l’IIO è essenziale sia per la teoria della misurazione sia per l’interpretazione accurata dei punteggi dei test. Pur essendo un presupposto fondamentale nell’uso degli strumenti di misurazione, l’IIO spesso non viene verificato empiricamente. La sua conferma è particolarmente importante nei test che mirano a riflettere una struttura gerarchica o cumulativa dei tratti misurati. Per trarre conclusioni affidabili sui processi cognitivi evolutivi basati sull’ordine di difficoltà degli item, è cruciale dimostrare la validità dell’IIO.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html#dimensione-del-campione",
    "href": "chapters/mokken/01_core_issues.html#dimensione-del-campione",
    "title": "52  Analisi della Scala di Mokken",
    "section": "52.13 Dimensione del Campione",
    "text": "52.13 Dimensione del Campione\nNel campo della psicometria, determinare la dimensione minima di un campione per i test statistici è un’area ben stabilita. Tuttavia, per quanto riguarda l’Analisi delle Scale Mokken (MSA), questo è un ambito ancora poco esplorato e ci sono pochi studi a riguardo. La ricerca in questo settore è necessaria per evitare sia la capitalizzazione sul caso sia l’utilizzo di campioni eccessivamente grandi, specialmente quando le risorse e il tempo a disposizione dei ricercatori sono limitati.\nLa “capitalizzazione sul caso” si riferisce a una condizione in cui una scala di Mokken viene identificata casualmente quando, in realtà, tale scala non esiste e ciò è dovuto alla ridotta dimensione del campione. Al contrario, può anche accadere che una scala esistente non venga identificata.\nUno studio condotto da Straat et al. (2014) ha esaminato le dimensioni minime del campione necessarie per l’Automated Item Selection Procedure (AISP) e per l’algoritmo genetico (GA). Lo studio ha valutato l’impatto di diversi fattori, inclusa la lunghezza del test, i valori approssimativi dei coefficienti di scalabilità (Hi) degli item e la correlazione tra le dimensioni nella scala. I risultati hanno evidenziato che la dimensione del campione necessaria dipende da tutti questi fattori. Tuttavia, il fattore più influente è risultato essere il valore di Hi. Con l’aumento di Hi, erano necessarie dimensioni di campione più piccole per assegnare correttamente gli item alle scale appropriate. La lunghezza del test non ha avuto un grande impatto sulla precisione della classificazione degli item nelle scale corrette. Tuttavia, le correlazioni tra le dimensioni hanno avuto qualche effetto in combinazione con vari livelli di Hi. Per valori di Hi intorno a .22, sono necessarie dimensioni di campione di 750-1000 persone per ottenere una precisione mediocre o adeguata, e di 1250-2500 persone per una precisione buona o eccellente. Con valori di Hi di .42, per una precisione mediocre o adeguata sono necessarie dimensioni di campione di 50 persone, mentre per una precisione buona o eccellente, la dimensione del campione dovrebbe essere di almeno 250. Quando le correlazioni tra le due dimensioni erano alte (ad esempio, .60) e i valori di Hi erano .42, erano necessarie dimensioni di campione maggiori per assegnare correttamente gli item alle scale rispetto alla condizione in cui le correlazioni erano .30 o 1.\nUn altro studio condotto da Watson et al. (2018) ha indagato l’impatto della dimensione del campione sui coefficienti di scalabilità utilizzando dati reali. Hanno estratto campioni di 50, 250, 500, 600, 750 e 1000 persone da un campione più ampio di 7510 persone che hanno risposto a un questionario di 14 item con item a 5 punti Likert. Utilizzando il bootstrapping, hanno estratto 1000 campioni per ogni dimensione del campione. I risultati hanno mostrato che i valori medi di H e Hi non cambiavano notevolmente tra le diverse dimensioni del campione. Tuttavia, considerando gli intervalli di confidenza al 95%, dimensioni di campione più piccole hanno portato a un maggior numero di occasioni in cui il limite inferiore degli intervalli di confidenza al 95% per Hi era inferiore a .30. Ad esempio, per N=50, il numero di volte in cui il limite inferiore degli intervalli di confidenza per Hi era inferiore a .30 era 592, mentre per N=1000, questo numero era zero. Ciò significa che, basandosi sugli errori standard di Hi, quando N=50, in 592 casi su 1000 si dovrebbe rifiutare l’item, concludendo che il suo Hi potrebbe essere inferiore a .30. Tuttavia, i valori medi di Hi per N=50 e N=1000 erano esattamente gli stessi. Questo suggerisce che la dimensione del campione non influisce sulle stime puntuali dei coefficienti di scalabilità, ma gioca un ruolo cruciale quando si considerano gli errori standard dei coefficienti di scalabilità e gli intervalli di confidenza per decidere sulla qualità degli item.\nIn conclusione, questi studi evidenziano l’importanza di considerare la dimensione del campione nell’analisi delle Scale Mokken. Mentre i valori medi di scalabilità possono non variare significativamente con la dimensione del campione, la precisione e l’affidabilità delle stime, così come la capacità di trarre conclusioni affidabili sulla qualità degli item, sono influenzate dalla grandezza del campione. Pertanto, è fondamentale scegliere una dimensione di campione adeguata per garantire risultati validi e affidabili nelle scale di Mokken. Questo è particolarmente critico in situazioni dove risorse e tempo sono limitati, e una scelta accurata della dimensione del campione può contribuire a un utilizzo più efficiente di tali risorse.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html#il-contributo-della-msa-alla-validazione-dei-test",
    "href": "chapters/mokken/01_core_issues.html#il-contributo-della-msa-alla-validazione-dei-test",
    "title": "52  Analisi della Scala di Mokken",
    "section": "52.14 Il Contributo della MSA alla Validazione dei Test",
    "text": "52.14 Il Contributo della MSA alla Validazione dei Test\nNell’ambito dell’Analisi delle Scale Mokken (MSA), la validità del modello di omogeneità monotona è cruciale. Questo modello è confermato quando le assunzioni fondamentali di unidimensionalità, monotonicità e indipendenza locale sono rispettate. In particolare, se i coefficienti di scalabilità H, Hi e Hij risultano positivi e significativamente superiori a zero (o meglio ancora, superiori a .30), ciò indica che i dati si conformano efficacemente a una struttura di Guttman. Tale conformità fornisce una forte indicazione dell’esistenza di un costrutto unidimensionale.\nL’adeguamento al modello di omogeneità monotona implica inoltre la presenza di monotonicità. Ciò significa che deve esistere una relazione non decrescente tra la variabile latente θ e la probabilità di ottenere una risposta corretta. Questo concetto è perfettamente in linea con il secondo criterio di validità nell’approccio basato sugli strumenti, secondo il quale variazioni nel tratto latente dovrebbero produrre variazioni corrispondenti nelle risposte agli item.\nUn’ulteriore dimensione della MSA è il modello di doppia monotonicità, che introduce l’assunzione dell’Ordinamento Invariante degli Item (IIO). Secondo questa assunzione, le Funzioni di Risposta all’Item (IRF) degli item di un test non dovrebbero intersecarsi. Anche se una sua violazione non rende di per sé un test invalido secondo l’approccio basato sugli strumenti, la conformità all’IIO migliora notevolmente l’interpretabilità dei punteggi del test. Inoltre, la violazione dell’IIO è analoga alla presenza di funzione differenziale dell’item (DIF) nei modelli IRT parametrici.\nIn conclusione, la MSA si rivela uno strumento estremamente utile e potente per la validazione di test in ambiti psicologici ed educativi. La capacità della MSA di confermare il modello di omogeneità monotona attraverso i coefficienti di scalabilità offre una valida evidenza che i test misurano effettivamente il costrutto unidimensionale che intendono valutare. Questo aspetto è fondamentale per garantire che i punteggi dei test riflettano veramente le capacità o le caratteristiche misurate.\nL’incorporazione dell’Ordinamento Invariante degli Item (IIO) nel modello di doppia monotonicità aggiunge un ulteriore livello di rigorosità. Assicurandosi che le IRF degli item non si intersechino, si aumenta la precisione con cui il test misura il costrutto e si migliora l’interpretazione dei punteggi. Questo approccio riduce il rischio di bias e garantisce che il test sia equamente rappresentativo per tutti i partecipanti, indipendentemente dalle loro caratteristiche individuali.\nInoltre, la MSA fornisce una base solida per affermare che le variazioni nei punteggi dei test sono effettivamente causate da variazioni nel costrutto misurato. Questa caratteristica rende la MSA particolarmente preziosa in contesti dove è essenziale dimostrare una relazione causale tra il costrutto e i punteggi del test.\nIn sintesi, l’impiego della MSA nella validazione dei test non solo rafforza la fiducia nella precisione e nell’affidabilità dei test stessi, ma contribuisce anche a una maggiore chiarezza e trasparenza nell’interpretazione dei risultati.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html#critiche-alla-msa",
    "href": "chapters/mokken/01_core_issues.html#critiche-alla-msa",
    "title": "52  Analisi della Scala di Mokken",
    "section": "52.15 Critiche alla MSA",
    "text": "52.15 Critiche alla MSA\nNegli anni ’80, l’Analisi delle Scale Mokken (MSA) è stata criticata per la limitata applicabilità del coefficiente di scalabilità H, ritenuto dipendente dalle caratteristiche del campione e degli item, e non adeguato come misura di adattamento del modello. Ulteriori critiche hanno riguardato il coefficiente di scalabilità degli item Hi, accusato di selezionare solo item con IRF ripide e distanti, escludendo item validi e riducendo la varianza e l’affidabilità del test. I critici hanno anche messo in dubbio l’adeguatezza della MSA per l’ordinamento libero degli item secondo il rango latente, suggerendo una possibile necessità del modello di Rasch.\nIn risposta, i difensori della MSA hanno sottolineato che le critiche si basano su una lettura selettiva e una mancata comprensione dei modelli non parametrici. Hanno ribadito che H e Hi sono intesi come misure dell’omogeneità monotona, e non come indici di doppia monotonia, e che la dipendenza di H dalla varianza della popolazione è in linea con le assunzioni del modello. Questo dibattito evidenzia l’importanza di valutare attentamente i metodi statistici come la MSA nel loro contesto di applicazione.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html#considerazioni-conclusive",
    "href": "chapters/mokken/01_core_issues.html#considerazioni-conclusive",
    "title": "52  Analisi della Scala di Mokken",
    "section": "52.16 Considerazioni Conclusive",
    "text": "52.16 Considerazioni Conclusive\nIn questo capitolo, abbiamo esplorato una questione fondamentale nella misurazione psicologica: l’efficacia dei punteggi totali grezzi nell’ordinare gli esaminandi. Tradizionalmente, tali punteggi vengono utilizzati per classificare i soggetti, da quelli più competenti a quelli meno competenti, da quelli più ansiosi a quelli meno ansiosi, o da quelli più depressi a quelli meno depressi. Sebbene sia comunemente accettato che i punteggi grezzi siano dati su scala ordinale, molti ricercatori li trattano come se fossero su scala intervallo. Ciò significa che, utilizzando i punteggi grezzi, si può solamente stabilire l’ordine dei rispondenti, ma non discernere le differenze tra di loro.\nIl tema principale affrontato in questo capitolo è che i punteggi grezzi potrebbero non essere nemmeno dati ordinali. In altre parole, i punteggi totali grezzi potrebbero non essere sufficientemente affidabili per ordinare gli esaminandi. Affinché i punteggi grezzi siano considerati ordinali, i pattern di risposta devono adattarsi al Modello di Omogeneità Monotona (MHM). Nella teoria classica dei test, si assume che i punteggi totali siano ordinali senza verificarlo. Il MHM, come modello IRT non parametrico, ci permette di testare se i punteggi totali rispettano l’assioma di una scala ordinale. Lo stesso vale per gli item: l’adattamento al Modello di Doppia Monotonicità (DMM) ci permette di ordinare gli item in base alle loro proporzioni di risposte corrette (valore p).\nIn questo capitolo, abbiamo discusso le procedure conosciute collettivamente come Analisi delle Scale Mokken, per testare l’adattamento dei dati al MHM e al DMM. Queste analisi offrono strumenti preziosi per verificare l’affidabilità e la validità dei punteggi totali grezzi utilizzati in una vasta gamma di contesti psicologici.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_applications.html",
    "href": "chapters/mokken/02_applications.html",
    "title": "53  Applicazione Pratica",
    "section": "",
    "text": "53.1 Introduzione\nIn questo capitolo, esamineremo l’applicazione pratica dei concetti e delle metodologie esplorate nel precedente capitolo, affrontando un’analisi dettagliata di un set di dati concreti. Il nostro focus è un caso di studio di grande rilevanza psicologica: l’indagine condotta dai ricercatori dell’ospedale Meyer, mirata a comprendere la capacità di coping delle famiglie di fronte alla diagnosi di una grave malattia in un figlio.\nQuesto lavoro non solo rappresenta un’opportunità per mettere in pratica le teorie e i metodi discussi, ma offre anche una finestra su questioni di vitale importanza nel campo della psicologia. Affrontare tematiche così delicate ci permette di esplorare le dinamiche familiari in situazioni di stress estremo, fornendo intuizioni preziose che possono guidare interventi psicosociali efficaci.\nPer garantire la massima accuratezza e rilevanza dei nostri risultati, iniziamo con un’attenta preparazione e pulizia dei dati. Questo passo ci assicura che l’analisi sia condotta su informazioni ben distribuite e rappresentative, eliminando gli item con eccessiva asimmetria e curtosi.\nAttraverso questa esplorazione approfondita, miriamo a dimostrare come le competenze metodologiche e analitiche possano essere efficacemente applicate a questioni di profondo impatto psicologico, evidenziando il potere dell’analisi statistica nel trasformare set di dati complessi in comprensioni approfondite e applicabili.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_applications.html#importazione-dei-dati",
    "href": "chapters/mokken/02_applications.html#importazione-dei-dati",
    "title": "53  Applicazione Pratica",
    "section": "53.2 Importazione dei dati",
    "text": "53.2 Importazione dei dati\n\ndf_tot &lt;- readRDS(\"../../data/fai_2022_11_20.rds\")\n\ntemp &lt;- df_tot |&gt; \n  dplyr::filter(FLAG == \"keep\")\ntemp$FLAG &lt;- NULL\n\nPer migliorare la qualità del nostro set di dati, rimuoveremo gli item che presentano livelli eccessivi di asimmetria (skewness) e curtosi. Questo passaggio è fondamentale per garantire che i nostri dati siano ben distribuiti e rappresentativi, migliorando così l’affidabilità e la validità delle nostre analisi. Gli item con asimmetria e curtosi estreme possono infatti distorcere i risultati degli analisi statistiche e influenzare negativamente le conclusioni tratte dallo studio.\n\nitems_stats &lt;- psych::describe(temp)\n\nitems_skew_kurt_bad &lt;- items_stats |&gt;\n    dplyr::filter(skew &gt; 2.5 | kurtosis &gt; 7.5) |&gt;\n    row.names()\nprint(items_skew_kurt_bad)\n\n [1] \"other\"                  \"child_birth_place\"      \"has_chronic_disease\"   \n [4] \"hospitalization_number\" \"emergency_care_number\"  \"divorce\"               \n [7] \"low_income\"             \"change_address\"         \"change_city\"           \n[10] \"is_mother_italian\"      \"is_father_italian\"      \"is_father_working\"     \n[13] \"is_child_italian\"       \"FAI_24\"                 \"FAI_32\"                \n[16] \"FAI_52\"                 \"FAI_53\"                 \"FAI_61\"                \n[19] \"FAI_74\"                 \"FAI_76\"                 \"FAI_77\"                \n[22] \"FAI_138\"                \"FAI_152\"                \"FAI_174\"               \n[25] \"FAI_175\"                \"FAI_182\"                \"FAI_193\"               \n\n\n\n# Select the strings starting with \"FAI_\"\nbad_fai_items &lt;- grep(\"^FAI_\", items_skew_kurt_bad, value = TRUE)\n\ndf &lt;- temp |&gt;\n    dplyr::select(!any_of(bad_fai_items))\n\nCi concentreremo qui su un sottoinsieme di item, ovvero quelli che riguardano l’area delle caratteristiche del bambino.\n\n# First subscale: items names.\nitem_subscale &lt;- c(\n    \"FAI_49\", \"FAI_106\", \"FAI_60\", \"FAI_124\", \"FAI_86\",\n    \"FAI_47\", \"FAI_121\", \"FAI_167\", \"FAI_99\",\n    \"FAI_63\", \"FAI_168\", \"FAI_5\", \"FAI_132\", \"FAI_85\", \"FAI_81\",\n    \"FAI_83\",\n    # \"FAI_152\",  \"FAI_175\",\n    \"FAI_57\", \"FAI_91\", \"FAI_135\", \"FAI_1\"\n)\n\n# Select only the items of this subscale.\nsubscale_data &lt;- df %&gt;%\n    dplyr::select(all_of(item_subscale))\ndim(subscale_data)\n\n\n45320",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_applications.html#statistiche-descrittive",
    "href": "chapters/mokken/02_applications.html#statistiche-descrittive",
    "title": "53  Applicazione Pratica",
    "section": "53.3 Statistiche descrittive",
    "text": "53.3 Statistiche descrittive\nEsaminiamo le statistiche descrittive degli item.\n\npsych::describe(subscale_data)\n\n\nA psych: 20 x 13\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nFAI_49\n1\n453\n0.5629139\n1.0320709\n0\n0.3085399\n0.0000\n0\n4\n4\n2.0665609\n3.6052375\n0.04849092\n\n\nFAI_106\n2\n453\n0.6710817\n0.9911011\n0\n0.4710744\n0.0000\n0\n4\n4\n1.6036473\n2.0835785\n0.04656599\n\n\nFAI_60\n3\n453\n0.9403974\n1.2169441\n0\n0.7272727\n0.0000\n0\n4\n4\n1.1792903\n0.2955897\n0.05717702\n\n\nFAI_124\n4\n453\n1.0463576\n1.1955175\n1\n0.8429752\n1.4826\n0\n4\n4\n1.1667416\n0.4658086\n0.05617031\n\n\nFAI_86\n5\n453\n1.1898455\n1.0171900\n1\n1.0606061\n1.4826\n0\n4\n4\n0.9491960\n0.6428214\n0.04779175\n\n\nFAI_47\n6\n453\n1.6247241\n1.1132778\n2\n1.5950413\n1.4826\n0\n4\n4\n0.1994007\n-0.7340689\n0.05230635\n\n\nFAI_121\n7\n453\n0.4900662\n0.7121035\n0\n0.3746556\n0.0000\n0\n4\n4\n1.8321794\n4.5725145\n0.03345754\n\n\nFAI_167\n8\n453\n1.9072848\n1.0306218\n2\n1.8815427\n1.4826\n0\n4\n4\n0.1974188\n-0.2695266\n0.04842284\n\n\nFAI_99\n9\n453\n2.0618102\n1.1543198\n2\n2.0220386\n1.4826\n0\n4\n4\n0.3447637\n-0.7883135\n0.05423468\n\n\nFAI_63\n10\n453\n1.1501104\n1.1615070\n1\n1.0192837\n1.4826\n0\n4\n4\n0.7295382\n-0.4985964\n0.05457236\n\n\nFAI_168\n11\n453\n0.7748344\n1.1298770\n0\n0.5399449\n0.0000\n0\n4\n4\n1.4492317\n1.1504289\n0.05308625\n\n\nFAI_5\n12\n453\n1.0132450\n1.1805209\n1\n0.8264463\n1.4826\n0\n4\n4\n1.0371531\n0.1238107\n0.05546571\n\n\nFAI_132\n13\n453\n1.4238411\n1.2360930\n1\n1.3002755\n1.4826\n0\n4\n4\n0.5953586\n-0.6157883\n0.05807672\n\n\nFAI_85\n14\n453\n1.3311258\n0.9027136\n1\n1.3002755\n1.4826\n0\n4\n4\n0.2930726\n-0.1115428\n0.04241319\n\n\nFAI_81\n15\n453\n1.7969095\n1.2685052\n2\n1.7465565\n1.4826\n0\n4\n4\n0.2930069\n-1.0004309\n0.05959957\n\n\nFAI_83\n16\n453\n1.1986755\n1.1825793\n1\n1.0468320\n1.4826\n0\n4\n4\n0.8458362\n-0.1601818\n0.05556242\n\n\nFAI_57\n17\n453\n0.3863135\n0.7123983\n0\n0.2341598\n0.0000\n0\n4\n4\n2.2165283\n5.6437160\n0.03347139\n\n\nFAI_91\n18\n453\n0.5320088\n0.9298228\n0\n0.3057851\n0.0000\n0\n4\n4\n2.1393688\n4.4338138\n0.04368689\n\n\nFAI_135\n19\n453\n0.4039735\n0.7686801\n0\n0.2231405\n0.0000\n0\n4\n4\n2.2914674\n5.8874641\n0.03611574\n\n\nFAI_1\n20\n453\n0.4547461\n0.7530503\n0\n0.2892562\n0.0000\n0\n4\n4\n1.8594704\n3.7153232\n0.03538139\n\n\n\n\n\nEsaminiamo la distribuzione del punteggio totale.\n\nscores &lt;- apply(subscale_data, 1, sum)\nhist(scores)",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_applications.html#automated-item-selection-procedure-aisp",
    "href": "chapters/mokken/02_applications.html#automated-item-selection-procedure-aisp",
    "title": "53  Applicazione Pratica",
    "section": "53.4 Automated Item Selection Procedure (AISP)",
    "text": "53.4 Automated Item Selection Procedure (AISP)\nIl primo passo nell’Analisi delle Scale Mokken (MSA) consiste nell’esecuzione dell’Automated Item Selection Procedure (AISP). Come precedentemente discusso, questa analisi ricerca insiemi di item (scale) che si conformano al modello di omogeneità monotona. Similmente all’analisi fattoriale esplorativa, l’AISP è un metodo per suddividere i dati in diverse sottoscale che soddisfano i criteri della MSA, includendo possibilmente anche l’individuazione di eventuali item non scalabili. Per eseguire la AISP, è necessario eseguire il codice seguente.\n\nsubscale_data &lt;- as.data.frame(subscale_data)\nscale &lt;- aisp(subscale_data, verbose = FALSE)\nprint(scale)\n\n        0.3\nFAI_49    1\nFAI_106   1\nFAI_60    1\nFAI_124   1\nFAI_86    0\nFAI_47    2\nFAI_121   0\nFAI_167   0\nFAI_99    0\nFAI_63    2\nFAI_168   0\nFAI_5     1\nFAI_132   2\nFAI_85    2\nFAI_81    1\nFAI_83    1\nFAI_57    2\nFAI_91    0\nFAI_135   0\nFAI_1     0\n\n\nNell’output precedente, FAI_* sono le etichette degli item considerati. Il valore .30 in alto indica il limite inferiore del coefficiente di scalabilità per la costruzione delle scale. ‘1’ indica che l’item appartiene alla scala 1 e 2 significa che l’item appartiene alla scala 2. ‘0’ indica che l’item non è scalabile. Dei 20 item di questa scala, sette item formano la scala 1 mentre altri cinque item formano la scala 2. Sette item risultano non scalabili.\nÈ possibile modificare sia il limite inferiore c (il limite inferiore predefinito è .30) sia il livello di alpha, che di default è .05. Per esempio:\n\nscale &lt;- aisp(subscale_data, lowerbound = 0.4, alpha = 0.1)\nprint(scale)\n\n        0.4\nFAI_49    1\nFAI_106   1\nFAI_60    1\nFAI_124   1\nFAI_86    0\nFAI_47    0\nFAI_121   0\nFAI_167   0\nFAI_99    0\nFAI_63    3\nFAI_168   0\nFAI_5     2\nFAI_132   0\nFAI_85    0\nFAI_81    2\nFAI_83    2\nFAI_57    3\nFAI_91    0\nFAI_135   0\nFAI_1     0\n\n\nSi noti che modificare il valore predefinito di c cambia la struttura della scala. Sijtsma e van der Ark (2017) hanno mostrato che il valore predefinito di .30 è quello che si dimostra più utile nella maggior parte delle applicazioni pratiche. Tuttavia, raccomandano di eseguire l’AISP 12 volte con c=0, .05, .10, .15, .20, .25, .30, .35, .40, .45, .50 e .55, così da potere esaminare le seguenti condizioni:\n\nNei dati unidimensionali, all’aumentare di c, si trova successivamente (a) la maggior parte o tutti gli item in una scala; (b) una scala più piccola; e (c) una o poche scale piccole e diversi item non scalabili. Viene raccomandato di prendere il risultato della fase (a) come finale.\nNei dati multidimensionali, all’aumentare di c, si trova successivamente (a) la maggior parte o tutti gli item in una scala; (b) due o più scale; e (c) due o più scale più piccole e diversi item non scalabili. Prendere il risultato della fase (b) come finale.\n\nLa decisione finale sulla struttura dei dati dovrebbe essere presa dal ricercatore sulla base di considerazioni teoriche e non sono statistiche.\nPer eseguire l’AISP con molteplici limiti inferiori, possiamo usare l’istruzione seguente:\n\naisp(\n    subscale_data, \n    lowerbound = c(.05, .10, .15, .20, .25, .30, .35, .40, .45, .50, .55), \n    verbose = FALSE\n)\n\n\nA matrix: 20 x 11 of type dbl\n\n\n\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n0.55\n\n\n\n\nFAI_49\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nFAI_106\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nFAI_60\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nFAI_124\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nFAI_86\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nFAI_47\n1\n1\n1\n1\n2\n2\n2\n0\n0\n0\n0\n\n\nFAI_121\n1\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n\n\nFAI_167\n1\n1\n1\n2\n0\n0\n0\n0\n0\n0\n0\n\n\nFAI_99\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nFAI_63\n1\n1\n1\n1\n2\n2\n2\n3\n3\n3\n0\n\n\nFAI_168\n1\n1\n1\n1\n3\n0\n0\n0\n0\n0\n0\n\n\nFAI_5\n1\n1\n1\n1\n1\n1\n0\n2\n0\n0\n0\n\n\nFAI_132\n1\n1\n1\n1\n2\n2\n0\n0\n0\n0\n0\n\n\nFAI_85\n2\n2\n0\n2\n2\n2\n2\n0\n0\n0\n0\n\n\nFAI_81\n1\n1\n1\n1\n1\n1\n1\n2\n2\n2\n2\n\n\nFAI_83\n1\n1\n1\n1\n1\n1\n1\n2\n2\n2\n2\n\n\nFAI_57\n1\n1\n1\n1\n2\n2\n2\n3\n3\n3\n0\n\n\nFAI_91\n1\n1\n1\n1\n3\n0\n0\n0\n0\n0\n0\n\n\nFAI_135\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nFAI_1\n2\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\nPer eseguire un’analisi della dimensionalità utilizzando un metodo diverso, ovvero l’algoritmo genetico (Straat, van der Ark & Sijtsma, 2013), si può utilizzare il seguente codice:\n\nscale &lt;- aisp(subscale_data, search = \"ga\")\nprint(scale)\n\n        0.3\nFAI_49    1\nFAI_106   1\nFAI_60    1\nFAI_124   1\nFAI_86    0\nFAI_47    2\nFAI_121   0\nFAI_167   0\nFAI_99    0\nFAI_63    2\nFAI_168   0\nFAI_5     1\nFAI_132   2\nFAI_85    2\nFAI_81    1\nFAI_83    1\nFAI_57    2\nFAI_91    0\nFAI_135   0\nFAI_1     0\n\n\nI risultati dell’algoritmo genetico sono equivalenti a quelli ottenuti utilizzando il limite inferiore raccomandato di 0.3.\nI risultati dell’analisi della dimensionalità ottenuti tramite l’AISP e l’algoritmo genetico (GA) dovrebbero essere replicabili in altri campioni. Pertanto, nelle procedure AISP e GA, la dimensione del campione è un fattore critico. Sijtsma e Molenaar (2002) affermano che l’AISP richiede almeno 100 partecipanti. Tuttavia, in studi basati su simulazioni di Monte Carlo, Straat, van der Ark e Sijtsma (2014) hanno dimostrato che sia l’AISP sia il GA necessitano di un campione compreso tra 250 e 500 partecipanti quando la qualità degli item (ovvero la loro capacità discriminante) è elevata, e tra 1250 e 1750 partecipanti quando la qualità degli item è scarsa.\nPossiamo selezionare gli item della scala 1 individuata dalla AISP nel modo seguente.\n\naisp.lb &lt;- aisp(subscale_data, lowerbound = .3)\ngood_items &lt;- subscale_data[, aisp.lb == 1]\nnames(good_items) |&gt; print()\n\n[1] \"FAI_49\"  \"FAI_106\" \"FAI_60\"  \"FAI_124\" \"FAI_5\"   \"FAI_81\"  \"FAI_83\"",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_applications.html#scalability-coefficients",
    "href": "chapters/mokken/02_applications.html#scalability-coefficients",
    "title": "53  Applicazione Pratica",
    "section": "53.5 Scalability Coefficients",
    "text": "53.5 Scalability Coefficients\nIl codice seguente ritorna i valori \\(H_{ij}\\), \\(H_j\\), e \\(H\\). Nelle parentesi tonde sono riportati gli errori standard.\n\nscal_coef &lt;- coefH(good_items, se = TRUE)\nscal_coef |&gt; print()\n\n$Hij\n        FAI_49  se      FAI_106 se      FAI_60  se      FAI_124 se      FAI_5  \nFAI_49                   0.561  (0.057)  0.608  (0.054)  0.601  (0.048)  0.212 \nFAI_106  0.561  (0.057)                  0.617  (0.042)  0.722  (0.039)  0.317 \nFAI_60   0.608  (0.054)  0.617  (0.042)                  0.688  (0.037)  0.280 \nFAI_124  0.601  (0.048)  0.722  (0.039)  0.688  (0.037)                  0.288 \nFAI_5    0.212  (0.056)  0.317  (0.055)  0.280  (0.052)  0.288  (0.052)        \nFAI_81   0.254  (0.060)  0.354  (0.056)  0.324  (0.050)  0.381  (0.053)  0.413 \nFAI_83   0.223  (0.057)  0.381  (0.056)  0.430  (0.047)  0.387  (0.048)  0.409 \n        se      FAI_81  se      FAI_83  se     \nFAI_49  (0.056)  0.254  (0.060)  0.223  (0.057)\nFAI_106 (0.055)  0.354  (0.056)  0.381  (0.056)\nFAI_60  (0.052)  0.324  (0.050)  0.430  (0.047)\nFAI_124 (0.052)  0.381  (0.053)  0.387  (0.048)\nFAI_5            0.413  (0.049)  0.409  (0.048)\nFAI_81  (0.049)                  0.551  (0.041)\nFAI_83  (0.048)  0.551  (0.041)                \n\n$Hi\n        Item H  se     \nFAI_49    0.410 (0.039)\nFAI_106   0.493 (0.032)\nFAI_60    0.484 (0.029)\nFAI_124   0.502 (0.029)\nFAI_5     0.323 (0.037)\nFAI_81    0.386 (0.034)\nFAI_83    0.404 (0.032)\n\n$H\nScale H      se \n  0.428 (0.026) \n\n$Hij\n        FAI_49  se      FAI_106 se      FAI_60  se      FAI_124 se      FAI_5  \nFAI_49                   0.561  (0.057)  0.608  (0.054)  0.601  (0.048)  0.212 \nFAI_106  0.561  (0.057)                  0.617  (0.042)  0.722  (0.039)  0.317 \nFAI_60   0.608  (0.054)  0.617  (0.042)                  0.688  (0.037)  0.280 \nFAI_124  0.601  (0.048)  0.722  (0.039)  0.688  (0.037)                  0.288 \nFAI_5    0.212  (0.056)  0.317  (0.055)  0.280  (0.052)  0.288  (0.052)        \nFAI_81   0.254  (0.060)  0.354  (0.056)  0.324  (0.050)  0.381  (0.053)  0.413 \nFAI_83   0.223  (0.057)  0.381  (0.056)  0.430  (0.047)  0.387  (0.048)  0.409 \n        se      FAI_81  se      FAI_83  se     \nFAI_49  (0.056)  0.254  (0.060)  0.223  (0.057)\nFAI_106 (0.055)  0.354  (0.056)  0.381  (0.056)\nFAI_60  (0.052)  0.324  (0.050)  0.430  (0.047)\nFAI_124 (0.052)  0.381  (0.053)  0.387  (0.048)\nFAI_5            0.413  (0.049)  0.409  (0.048)\nFAI_81  (0.049)                  0.551  (0.041)\nFAI_83  (0.048)  0.551  (0.041)                \n\n$Hi\n        Item H  se     \nFAI_49    0.410 (0.039)\nFAI_106   0.493 (0.032)\nFAI_60    0.484 (0.029)\nFAI_124   0.502 (0.029)\nFAI_5     0.323 (0.037)\nFAI_81    0.386 (0.034)\nFAI_83    0.404 (0.032)\n\n$H\nScale H      se \n  0.428 (0.026) \n\n$covHij\n               [,1]          [,2]          [,3]          [,4]          [,5]\n [1,]  3.256522e-03  1.972567e-03  1.673569e-03  6.850269e-04  0.0009303758\n [2,]  1.972567e-03  2.872632e-03  1.379662e-03  5.619181e-04  0.0007468355\n [3,]  1.673569e-03  1.379662e-03  2.324218e-03  7.130034e-04  0.0009755748\n [4,]  6.850269e-04  5.619181e-04  7.130034e-04  3.132843e-03  0.0014577910\n [5,]  9.303758e-04  7.468355e-04  9.755748e-04  1.457791e-03  0.0035999264\n [6,]  1.176059e-03  1.372448e-03  1.121228e-03  1.001618e-03  0.0007258314\n [7,]  1.143505e-03  6.796210e-04  6.075535e-04  3.611057e-04  0.0005325189\n [8,]  9.205140e-04  4.625925e-04  3.783489e-04  2.484545e-04  0.0003248843\n [9,]  1.281966e-04  2.342448e-04  1.519768e-04  1.410060e-03  0.0006882034\n[10,]  4.801610e-05  3.000776e-04  1.972389e-04  6.442391e-04  0.0016398978\n[11,]  6.330714e-05  1.877564e-04  2.994296e-04  4.332023e-04  0.0002887708\n[12,]  7.230668e-04  6.943860e-04  7.140702e-04  2.950943e-04  0.0003590450\n[13,]  3.194478e-04  4.391644e-04  3.080059e-04  1.808926e-03  0.0006665414\n[14,]  3.310593e-04  2.500548e-04  2.591961e-04  5.841647e-04  0.0016242307\n[15,] -2.626538e-05  1.143234e-04  3.461109e-04  5.949934e-04  0.0002896028\n[16,]  1.885976e-04  2.599218e-04  3.321093e-04  1.573800e-03  0.0006905796\n[17,]  1.132431e-04  1.511628e-04  5.240330e-05  5.614512e-04  0.0016082483\n[18,]  2.425335e-04  4.564482e-04  2.708591e-04  4.202745e-04  0.0002271944\n[19,]  6.016676e-05 -5.677038e-08  6.439974e-05  4.080167e-04  0.0003000318\n[20,] -6.323510e-05  2.770648e-04  9.115472e-05  4.564269e-04 -0.0000318649\n[21,] -1.452389e-04  4.045822e-05 -1.410212e-04 -8.502939e-05  0.0001437967\n               [,6]          [,7]          [,8]         [,9]        [,10]\n [1,]  1.176059e-03  1.143505e-03  9.205140e-04 1.281966e-04 0.0000480161\n [2,]  1.372448e-03  6.796210e-04  4.625925e-04 2.342448e-04 0.0003000776\n [3,]  1.121228e-03  6.075535e-04  3.783489e-04 1.519768e-04 0.0001972389\n [4,]  1.001618e-03  3.611057e-04  2.484545e-04 1.410060e-03 0.0006442391\n [5,]  7.258314e-04  5.325189e-04  3.248843e-04 6.882034e-04 0.0016398978\n [6,]  3.235792e-03  3.934074e-04  4.943276e-04 4.792224e-04 0.0002711309\n [7,]  3.934074e-04  1.765623e-03  7.886910e-04 3.994963e-04 0.0003576934\n [8,]  4.943276e-04  7.886910e-04  1.518517e-03 3.437684e-04 0.0004289051\n [9,]  4.792224e-04  3.994963e-04  3.437684e-04 3.075515e-03 0.0011512604\n[10,]  2.711309e-04  3.576934e-04  4.289051e-04 1.151260e-03 0.0030881212\n[11,]  1.580928e-03  7.872979e-04  6.366403e-04 1.439675e-03 0.0011535909\n[12,]  5.881401e-04  7.514867e-04  4.972229e-04 5.983421e-05 0.0002224072\n[13,]  7.426056e-04  4.490709e-04  9.903078e-05 1.401373e-03 0.0005654204\n[14,]  3.670699e-04  4.022907e-04  2.546547e-04 5.617317e-04 0.0015767924\n[15,]  1.394333e-03  3.075009e-04  2.545034e-04 4.954782e-04 0.0003710759\n[16,]  5.291520e-04  1.750076e-04  4.106679e-04 1.618941e-03 0.0007001615\n[17,]  2.446268e-04  2.201295e-04  2.059815e-04 6.070280e-04 0.0017308946\n[18,]  1.440363e-03  3.706182e-04  4.282926e-04 4.721104e-04 0.0005205689\n[19,] -2.408574e-05  7.725707e-05  8.859144e-05 7.086189e-04 0.0004992864\n[20,]  6.059513e-04  6.986203e-05 -5.566603e-05 8.917753e-04 0.0004276745\n[21,]  2.480873e-04 -1.299639e-05 -3.392847e-05 3.442378e-04 0.0005256114\n             [,11]         [,12]        [,13]        [,14]         [,15]\n [1,] 6.330714e-05  7.230668e-04 3.194478e-04 0.0003310593 -2.626538e-05\n [2,] 1.877564e-04  6.943860e-04 4.391644e-04 0.0002500548  1.143234e-04\n [3,] 2.994296e-04  7.140702e-04 3.080059e-04 0.0002591961  3.461109e-04\n [4,] 4.332023e-04  2.950943e-04 1.808926e-03 0.0005841647  5.949934e-04\n [5,] 2.887708e-04  3.590450e-04 6.665414e-04 0.0016242307  2.896028e-04\n [6,] 1.580928e-03  5.881401e-04 7.426056e-04 0.0003670699  1.394333e-03\n [7,] 7.872979e-04  7.514867e-04 4.490709e-04 0.0004022907  3.075009e-04\n [8,] 6.366403e-04  4.972229e-04 9.903078e-05 0.0002546547  2.545034e-04\n [9,] 1.439675e-03  5.983421e-05 1.401373e-03 0.0005617317  4.954782e-04\n[10,] 1.153591e-03  2.224072e-04 5.654204e-04 0.0015767924  3.710759e-04\n[11,] 3.100653e-03  3.797460e-04 5.558660e-04 0.0004499206  1.388343e-03\n[12,] 3.797460e-04  1.353425e-03 3.361125e-04 0.0003599263  3.711701e-04\n[13,] 5.558660e-04  3.361125e-04 2.679508e-03 0.0010149171  9.133291e-04\n[14,] 4.499206e-04  3.599263e-04 1.014917e-03 0.0025074061  7.814797e-04\n[15,] 1.388343e-03  3.711701e-04 9.133291e-04 0.0007814797  2.239613e-03\n[16,] 5.671980e-04  3.549025e-04 1.809069e-03 0.0006726168  7.152364e-04\n[17,] 5.230923e-04  8.662901e-05 5.994210e-04 0.0016280518  4.484829e-04\n[18,] 1.453794e-03  3.694086e-04 6.818643e-04 0.0005001348  1.390561e-03\n[19,] 3.721344e-04  8.121422e-05 6.065452e-04 0.0004767595  4.240890e-05\n[20,] 6.949490e-04  2.048871e-04 8.608315e-04 0.0001955560  4.525888e-04\n[21,] 3.341346e-04 -7.180751e-05 6.989798e-05 0.0004952616  1.117222e-04\n             [,16]        [,17]        [,18]         [,19]         [,20]\n [1,] 1.885976e-04 1.132431e-04 0.0002425335  6.016676e-05 -6.323510e-05\n [2,] 2.599218e-04 1.511628e-04 0.0004564482 -5.677038e-08  2.770648e-04\n [3,] 3.321093e-04 5.240330e-05 0.0002708591  6.439974e-05  9.115472e-05\n [4,] 1.573800e-03 5.614512e-04 0.0004202745  4.080167e-04  4.564269e-04\n [5,] 6.905796e-04 1.608248e-03 0.0002271944  3.000318e-04 -3.186490e-05\n [6,] 5.291520e-04 2.446268e-04 0.0014403628 -2.408574e-05  6.059513e-04\n [7,] 1.750076e-04 2.201295e-04 0.0003706182  7.725707e-05  6.986203e-05\n [8,] 4.106679e-04 2.059815e-04 0.0004282926  8.859144e-05 -5.566603e-05\n [9,] 1.618941e-03 6.070280e-04 0.0004721104  7.086189e-04  8.917753e-04\n[10,] 7.001615e-04 1.730895e-03 0.0005205689  4.992864e-04  4.276745e-04\n[11,] 5.671980e-04 5.230923e-04 0.0014537943  3.721344e-04  6.949490e-04\n[12,] 3.549025e-04 8.662901e-05 0.0003694086  8.121422e-05  2.048871e-04\n[13,] 1.809069e-03 5.994210e-04 0.0006818643  6.065452e-04  8.608315e-04\n[14,] 6.726168e-04 1.628052e-03 0.0005001348  4.767595e-04  1.955560e-04\n[15,] 7.152364e-04 4.484829e-04 0.0013905614  4.240890e-05  4.525888e-04\n[16,] 2.714269e-03 9.497205e-04 0.0009504702  8.252786e-04  9.374942e-04\n[17,] 9.497205e-04 2.781060e-03 0.0011345975  3.571314e-04  1.293712e-04\n[18,] 9.504702e-04 1.134597e-03 0.0023072463  5.416910e-05  4.234757e-04\n[19,] 8.252786e-04 3.571314e-04 0.0000541691  2.416128e-03  1.043338e-03\n[20,] 9.374942e-04 1.293712e-04 0.0004234757  1.043338e-03  2.288321e-03\n[21,] 8.680063e-05 4.749021e-04 0.0002621942  4.096861e-04  5.441687e-04\n              [,21]\n [1,] -1.452389e-04\n [2,]  4.045822e-05\n [3,] -1.410212e-04\n [4,] -8.502939e-05\n [5,]  1.437967e-04\n [6,]  2.480873e-04\n [7,] -1.299639e-05\n [8,] -3.392847e-05\n [9,]  3.442378e-04\n[10,]  5.256114e-04\n[11,]  3.341346e-04\n[12,] -7.180751e-05\n[13,]  6.989798e-05\n[14,]  4.952616e-04\n[15,]  1.117222e-04\n[16,]  8.680063e-05\n[17,]  4.749021e-04\n[18,]  2.621942e-04\n[19,]  4.096861e-04\n[20,]  5.441687e-04\n[21,]  1.663587e-03\n\n$covHi\n             [,1]         [,2]         [,3]         [,4]         [,5]\n[1,] 0.0014980127 0.0007465455 0.0007793789 0.0007134071 0.0005453263\n[2,] 0.0007465455 0.0010331381 0.0006387837 0.0006398164 0.0005589013\n[3,] 0.0007793789 0.0006387837 0.0008679826 0.0006400363 0.0005827344\n[4,] 0.0007134071 0.0006398164 0.0006400363 0.0008379947 0.0005613310\n[5,] 0.0005453263 0.0005589013 0.0005827344 0.0005613310 0.0013482139\n[6,] 0.0004596982 0.0005289703 0.0005019538 0.0005248655 0.0006078101\n[7,] 0.0004856933 0.0005415021 0.0005393324 0.0005410202 0.0005874255\n             [,6]         [,7]\n[1,] 0.0004596982 0.0004856933\n[2,] 0.0005289703 0.0005415021\n[3,] 0.0005019538 0.0005393324\n[4,] 0.0005248655 0.0005410202\n[5,] 0.0006078101 0.0005874255\n[6,] 0.0011488908 0.0004461349\n[7,] 0.0004461349 0.0010515910\n\n$covH\n             [,1]\n[1,] 0.0006624019\n\n\n\nPossiamo interpretare i coefficienti di scalabilità nel modo seguente.\n\nCoefficiente di Scalabilità tra Coppie di Item (Hij): Per ogni coppia di item (i, j), il coefficiente \\(H_{ij}\\) valuta l’efficacia con cui questi due item riflettono la variabile latente. Un coefficiente \\(H_{ij}\\) positivo per coppie di item appartenenti alla stessa scala di Mokken indica che questi item sono coerenti e misurano efficacemente la stessa variabile latente. Matematicamente, \\(H_{ij}\\) è definito per ogni coppia di item i e j, dove i, j = 1, …, J.\nCoeffiente di Scalabilità dell’Item (Hj): Il coefficiente \\(H_{j}\\) di un singolo item è analogo ai parametri di discriminazione nei modelli IRT parametrici. Esprime l’efficacia con cui un item distingue tra individui a diversi livelli della variabile latente. Per essere considerato efficace, \\(H_{j}\\) dovrebbe superare un certo limite inferiore, generalmente stabilito a c &gt; 0.3.\nCoefficiente di Scalabilità del Test (H): H rappresenta la scalabilità complessiva dell’intero insieme di item. L’interpretazione di H segue le seguenti soglie euristiche:\n\nDebole: se 0.3 ≤ H &lt; 0.4.\nModerato: se 0.4 ≤ H &lt; 0.5.\nForte: se H &gt; 0.5.\n\nQuesti valori indicano la forza con cui l’insieme di item misura la variabile latente. I coefficienti di scalabilità degli item forniscono indicazioni sulla discriminazione degli item e sulla loro aderenza al modello di omogeneità monotona. Item con bassa discriminazione non contribuiscono a un ordinamento affidabile degli esaminandi e dovrebbero essere scartati.\n\nSecondo Sijtsma e Molenaar (2002), le assunzioni di unidimensionalità, indipendenza locale e monotonicità implicano le seguenti restrizioni sui coefficienti di scalabilità: - 0 ≤ \\(H_{ij}\\) ≤ 1, per tutte le coppie di item i ≠ j. - 0 ≤ \\(H_{j}\\) ≤ 1, per tutti gli item j. - 0 ≤ \\(H\\) ≤ 1, per l’intero insieme di item.\nI coefficienti di scalabilità sono fondamentali per valutare quanto efficacemente un insieme di item lavori insieme per misurare una variabile latente. Valori alti di \\(H\\) suggeriscono che l’insieme di item è fortemente correlato e misura in modo affidabile la variabile latente, garantendo che l’analisi con la MSA sia valida e affidabile.\nÈ possibile ottenere il numero di valori negativi \\(H_{ij}\\) per ciascun item usando il codice seguente.\n\nscal_coef &lt;- coefH(good_items, se = FALSE)$Hij\napply(scal_coef, 1, function(x) sum(x &lt; 0)) |&gt; print()\n\n$Hij\n           FAI_49   FAI_106    FAI_60   FAI_124     FAI_5    FAI_81    FAI_83\nFAI_49  1.0000000 0.5613569 0.6075009 0.6005396 0.2118286 0.2539023 0.2229846\nFAI_106 0.5613569 1.0000000 0.6172766 0.7223965 0.3168352 0.3539449 0.3812326\nFAI_60  0.6075009 0.6172766 1.0000000 0.6879216 0.2795178 0.3243635 0.4303254\nFAI_124 0.6005396 0.7223965 0.6879216 1.0000000 0.2879511 0.3813744 0.3866023\nFAI_5   0.2118286 0.3168352 0.2795178 0.2879511 1.0000000 0.4132827 0.4085199\nFAI_81  0.2539023 0.3539449 0.3243635 0.3813744 0.4132827 1.0000000 0.5506000\nFAI_83  0.2229846 0.3812326 0.4303254 0.3866023 0.4085199 0.5506000 1.0000000\n\n$Hi\n   FAI_49   FAI_106    FAI_60   FAI_124     FAI_5    FAI_81    FAI_83 \n0.4096841 0.4929127 0.4838063 0.5019022 0.3231299 0.3862103 0.4040312 \n\n$H\n[1] 0.4279251\n\n FAI_49 FAI_106  FAI_60 FAI_124   FAI_5  FAI_81  FAI_83 \n      0       0       0       0       0       0       0 \n\n\nL’istruzione seguente esegue la procedura frequentista del test di ipotesi, con l’ipotesi alternativa che i coefficienti di scalabilità sono maggiori di zero. Il test è unidirezionale, dunque il valore soglia della statistica \\(z\\) è 1.65.\n\ncoefZ(good_items) |&gt; print()\n\n$Zij\n           FAI_49   FAI_106    FAI_60   FAI_124    FAI_5    FAI_81    FAI_83\nFAI_49   0.000000 11.122254 11.766708 11.519386 4.077595  4.303706  4.213676\nFAI_106 11.122254  0.000000 12.328013 14.049032 6.189922  6.313023  7.368202\nFAI_60  11.766708 12.328013  0.000000 14.039308 5.741312  6.231554  8.542324\nFAI_124 11.519386 14.049032 14.039308  0.000000 5.889676  7.265907  7.779304\nFAI_5    4.077595  6.189922  5.741312  5.889676 0.000000  8.141997  8.214378\nFAI_81   4.303706  6.313023  6.231554  7.265907 8.141997  0.000000 10.794877\nFAI_83   4.213676  7.368202  8.542324  7.779304 8.214378 10.794877  0.000000\n\n$Zi\n  FAI_49  FAI_106   FAI_60  FAI_124    FAI_5   FAI_81   FAI_83 \n18.87934 23.25525 23.49005 24.24297 15.73678 17.72590 19.40083 \n\n$Z\n[1] 37.97973\n\n\n\nNel caso presente, il test indica che i coefficienti di scalabilità di tutti gli item, sia considerati singolarmente sia considerati a coppie, sono maggiori di zero. Lo stesso si può dire per il coefficiente di scalabilità della scala nel suo complesso.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_applications.html#monotonicità",
    "href": "chapters/mokken/02_applications.html#monotonicità",
    "title": "53  Applicazione Pratica",
    "section": "53.6 Monotonicità",
    "text": "53.6 Monotonicità\nCome spiegato in precedenza, la monotonicità è un’importante assunzione della MSA. La probabilità di una risposta corretta dovrebbe aumentare con θ. La monotonicità può essere esaminata con il seguente codice:\n\nmonoton &lt;- check.monotonicity(good_items)\nsummary(monoton) |&gt; print()\n\n        ItemH #ac #vi #vi/#ac maxvi sum sum/#ac zmax #zsig crit\nFAI_49   0.41  20   0       0     0   0       0    0     0    0\nFAI_106  0.49  12   0       0     0   0       0    0     0    0\nFAI_60   0.48  18   0       0     0   0       0    0     0    0\nFAI_124  0.50  21   0       0     0   0       0    0     0    0\nFAI_5    0.32  24   0       0     0   0       0    0     0    0\nFAI_81   0.39  24   0       0     0   0       0    0     0    0\nFAI_83   0.40  21   0       0     0   0       0    0     0    0\n\n\nOgni riga dell’output rappresenta un item (ad esempio, FAI_49, FAI_106, ecc.). La spiegazione delle colonne è la seguente:\n\nItemH: Il coefficiente H per ogni item, che misura l’omogeneità dell’item. Un valore più alto indica una maggiore omogeneità. Nelle scale di Mokken, si cercano generalmente valori superiori a 0.3.\n#ac (Active pairs): Il numero di coppie attive, ovvero coppie di risposte che contribuiscono alla stima dell’H.\n#vi (Violations): Il numero di violazioni della monotonicità. La monotonicità implica che, man mano che aumenta il punteggio totale del test, la probabilità di una risposta positiva all’item non diminuisce.\n#vi/#ac: Il rapporto tra il numero di violazioni e il numero di coppie attive.\nmaxvi (Maximum violation): La massima violazione osservata.\nsum (Sum of violation size): La somma delle dimensioni delle violazioni.\nsum/#ac: Il rapporto tra la somma delle dimensioni delle violazioni e il numero di coppie attive.\nzmax: Il valore massimo della statistica Z per le violazioni.\n#zsig (Number of significant Z): Numero di statistiche Z significative.\ncrit: Un criterio per giudicare se le violazioni sono problematiche.\n\nNel caso presente, sembra che non ci siano violazioni della monotonicità per nessuno degli item elencati. Questo significa che per questi item, all’aumentare del punteggio totale, non si osserva una diminuzione della probabilità di una risposta positiva, mantenendo quindi una buona coerenza interna e validità per la scala.\nUn grafico della monotonicità per una coppia di item si ottiene nel modo seguente.\n\nplot(check.monotonicity(good_items), item = c(1, 2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa figura di questo esempio illustra i grafici di monotonicità per gli Item 49 e 106. Il grafico è diviso in due pannelli. Il pannello sul lato sinistro mostra le Funzioni di Risposta del Passaggio dell’Item (ISRFs), mentre il pannello sul lato destro mostra la Funzione di Risposta all’Item (IRF) complessiva per ciascun item. Il grafico evidenzia che sia l’IRF sia le ISRFs sono sempre non decrescenti per l’item 49; per l’item 106, invece, si osserva una piccola violazione della monotonicità.\nSe esaminiamo tutti gli item dell’area relativa alle caratteristiche del bambino (non solo quelli selezionati dalla procedura AISP) notiamo come, per alcuni item, si osserva un numero di violazioni maggiore di zero.\n\nmonoton2 &lt;- check.monotonicity(subscale_data)\nsummary(monoton2) |&gt; print()\n\n        ItemH #ac #vi #vi/#ac maxvi  sum sum/#ac zmax #zsig crit\nFAI_49   0.25  24   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_106  0.30  15   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_60   0.30  18   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_124  0.32  24   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_86   0.16  21   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_47   0.23  24   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_121  0.18  23   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_167  0.18  24   2    0.08  0.04 0.07  0.0031 1.07     0   31\nFAI_99   0.12  24   1    0.04  0.05 0.05  0.0023 1.58     0   31\nFAI_63   0.23  24   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_168  0.23  18   1    0.06  0.06 0.06  0.0032 0.73     0   25\nFAI_5    0.23  24   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_132  0.21  24   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_85   0.17  19   1    0.05  0.07 0.07  0.0035 1.08     0   30\nFAI_81   0.24  24   1    0.04  0.04 0.04  0.0015 0.52     0   17\nFAI_83   0.24  24   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_57   0.21  15   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_91   0.18  21   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_135  0.15  21   1    0.05  0.03 0.03  0.0016 0.39     0   22\nFAI_1    0.10  21   0    0.00  0.00 0.00  0.0000 0.00     0    0",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_applications.html#ordinamento-invariante-degli-item",
    "href": "chapters/mokken/02_applications.html#ordinamento-invariante-degli-item",
    "title": "53  Applicazione Pratica",
    "section": "53.7 Ordinamento Invariante degli Item",
    "text": "53.7 Ordinamento Invariante degli Item\nIl passo successivo nella MSA è indagare l’ordinamento invariante degli item (IIO) o la non intersezione delle Funzioni di Risposta all’Item (IRFs). È fondamentale determinare se l’ordine degli item sia lo stesso per tutti i rispondenti con diversi livelli del tratto. Esistono diversi metodi per esaminare l’IIO. Il metodo predefinito nel pacchetto R mokken è l’IIO manifesto o MIIO (Manifest IIO) (Ligtvoet, Van der Ark, Te Marvelde & Sijtsma, 2010). Per esaminare l’ordinamento invariante degli item, eseguiamo il seguente codice.\n\niio &lt;- check.iio(good_items)\nsummary(iio) |&gt; print()\n\n$method\n[1] \"MIIO\"\n\n$item.summary\n        ItemH #ac #vi #vi/#ac maxvi  sum sum/#ac tmax #tsig crit\nFAI_81   0.39  18   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_83   0.40  18   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_124  0.50  18   1    0.06  0.31 0.31  0.0171 3.50     1   83\nFAI_5    0.32  18   2    0.11  0.42 0.73  0.0407 3.50     2  145\nFAI_60   0.48  18   1    0.06  0.42 0.42  0.0236 2.49     1   97\nFAI_106  0.49  18   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_49   0.41  18   0    0.00  0.00 0.00  0.0000 0.00     0    0\n\n$backward.selection\n        step 1 step 2\nFAI_81       0      0\nFAI_83       0      0\nFAI_124      1      0\nFAI_5        2     NA\nFAI_60       1      0\nFAI_106      0      0\nFAI_49       0      0\n\n$HT\n[1] 0.3358101\n\n\n\nL’output include due tabelle principali. La prima tabella contiene le seguenti colonne. La prima colonna, “ItemH”, mostra il coefficiente di scalabilità Hi per ciascun item, ‘#ac’ indica il numero totale di coppie attive, ‘#vi’ segnala il numero totale di violazioni, ‘#vi/#ac’ mostra il numero medio di violazioni per coppia attiva, ‘maxvi’ indica la massima violazione, ‘sum’ rappresenta la somma di tutte le violazioni, ‘sum/#ac’ mostra la media delle violazioni per coppia attiva, ‘tmax’ indica la statistica di test massima, ‘#tsig’ il numero di violazioni significative, e il valore ‘crit’ è una somma ponderata di altri elementi come ‘itemH’, ‘#ac’, ecc. Valori elevati di ‘crit’ indicano item di scarsa qualità (il valore 0 è perfetto, valori più alti sono peggiori).\nL’output precedente mostra che l’Item FAI_5 ha 2 violazioni. In altre parole, la IRF per questo item si interseca con la IRF di altri due item, ed entrambe queste violazioni sono significative (#tsig per questo item è 2). Poiché ha il numero più alto di violazioni, è un buon candidato per essere rimosso dal test. Rimuovere questo item risolve l’intersezione degli altri item con questo item.\nPer esaminare graficamente queste intersezione, usiamo il codice seguente.\n\nplot(iio)",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_applications.html#non-intersezione-delle-funzioni-di-risposta-degli-step-degli-item",
    "href": "chapters/mokken/02_applications.html#non-intersezione-delle-funzioni-di-risposta-degli-step-degli-item",
    "title": "53  Applicazione Pratica",
    "section": "53.8 Non intersezione delle funzioni di risposta degli step degli item",
    "text": "53.8 Non intersezione delle funzioni di risposta degli step degli item\nPer indagare sulla non intersezione delle funzioni di risposta degli step degli item si possono impiegare le matrici P++ e P– (Molenaar & Sijtsma, 2000).\n\npmatrix &lt;- check.pmatrix(good_items)\nsummary(pmatrix) |&gt; print()\n\n        ItemH  #ac #vi #vi/#ac maxvi  sum sum/#ac zmax #zsig crit\nFAI_49   0.41 1920  17    0.01  0.05 0.65   3e-04 2.93    15   67\nFAI_106  0.49 1920  10    0.01  0.06 0.39   2e-04 4.21    10   59\nFAI_60   0.48 1920  23    0.01  0.06 0.94   5e-04 3.88    22   81\nFAI_124  0.50 1920  20    0.01  0.06 0.84   4e-04 4.60    20   80\nFAI_5    0.32 1920  25    0.01  0.06 1.07   6e-04 3.88    24   92\nFAI_81   0.39 1920  28    0.01  0.05 1.09   6e-04 3.60    27   89\nFAI_83   0.40 1920  17    0.01  0.06 0.68   4e-04 4.60    16   78\n\n\nCome spiegato nelle sezioni precedenti, la colonna ‘#vi’ indica il numero di violazioni delle ISRF. La colonna ‘#zsig’ mostra il numero di violazioni statisticamente significative. Le violazioni possono anche essere controllate graficamente utilizzando il seguente codice:\n\nplot(check.pmatrix(good_items), item = 5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa non intersezione delle ISRF può essere esaminata anche con il metodo del punteggio residuo.\n\nrestscore &lt;- check.restscore(good_items) \nsummary(restscore) |&gt; print()\n\n        ItemH #ac #vi #vi/#ac maxvi  sum sum/#ac zmax #zsig crit\nFAI_49   0.41 288   5    0.02  0.08 0.23  0.0008 1.19     0   18\nFAI_106  0.49 288   7    0.02  0.07 0.33  0.0011 2.16     2   34\nFAI_60   0.48 288   6    0.02  0.14 0.47  0.0016 2.19     4   50\nFAI_124  0.50 288   9    0.03  0.15 0.56  0.0019 2.78     1   44\nFAI_5    0.32 288  14    0.05  0.15 1.01  0.0035 2.78     5   74\nFAI_81   0.39 288   8    0.03  0.10 0.44  0.0015 1.70     1   37\nFAI_83   0.40 288   9    0.03  0.07 0.40  0.0014 2.07     1   36\n\n\nI risultati del metodo del punteggio residuo sono diversi da quelli delle matrici P++ e P–. L’output viene interpretato come abbiamo fatto in precedenza. La colonna ‘#vi’ indica il numero di violazioni delle ISRF. La colonna ‘#zsig’ mostra il numero di violazioni statisticamente significative. Questa colonna mostra che il numero di violazioni statisticamente significative è molto inferiore rispetto a quelle riportate dalle matrici P++ e P–. La violazione della non intersezione può essere esaminata graficamente nel modo seguente. Per esempio, consideriamo gli item FAI_49 e FAI_5.\n\nplot(restscore, item.pairs = c(4, 5))",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_applications.html#affidabilità",
    "href": "chapters/mokken/02_applications.html#affidabilità",
    "title": "53  Applicazione Pratica",
    "section": "53.9 Affidabilità",
    "text": "53.9 Affidabilità\nIl pacchetto Mokken calcola quattro diversi coefficienti di affidabilità: l’affidabilità della scala Mokken (MS) ρ, (Mokken, 1971), Lambda-2 (Guttman, 1945), l’alpha di Cronbach (Cronbach, 1951), e il coefficiente di affidabilità della classe latente (LCRC, van der Ark, van der Palm, & Sijtsma, 2011).\n\nMS (Molenaar-Sijtsma Method): Questo indice è basato sul metodo Molenaar-Sijtsma di stima dell’affidabilità per scale non parametriche, come quelle analizzate con l’analisi Mokken. Questo metodo considera la varianza tra gli item e la varianza totale per stimare l’affidabilità.\nAlpha (Cronbach’s Alpha): L’alpha di Cronbach è forse il più noto indice di affidabilità, utilizzato per valutare la consistenza interna degli item di un test. Misura fino a che punto gli item di un test sono correlati tra loro.\nLambda-2: Un altro indice di affidabilità, simile all’alpha di Cronbach, ma talvolta considerato più robusto poiché tiene conto delle correlazioni medie tra gli item.\nLCRC (Latent Class Reliability Coefficient): Questo è un indice di affidabilità che tiene conto dell’approccio delle classi latenti. È particolarmente utile quando gli item possono essere raggruppati in sottoscale che riflettono diversi costrutti o dimensioni.\n\nPer ottenere queste stime dell’affidabilità è possibile eseguire il seguente codice.\n\ncheck.reliability(good_items, LCRC = TRUE)\n\n\n    $MS\n        0.819110476790128\n    $alpha\n        0.81675073603741\n    $lambda.2\n        0.824148383038108\n    $LCRC\n        0.843559865692261\n\n\n\nIn generale, tutti i valori ottenuti indicano che la scala in considerazione ha un’alta affidabilità. Questo significa che è probabile che produca risultati coerenti nel tempo e che gli item che la compongono siano correlati tra loro in modo significativo, contribuendo tutti a misurare lo stesso costrutto o costrutti correlati.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_applications.html#identificazione-degli-outlier",
    "href": "chapters/mokken/02_applications.html#identificazione-degli-outlier",
    "title": "53  Applicazione Pratica",
    "section": "53.10 Identificazione degli Outlier",
    "text": "53.10 Identificazione degli Outlier\nGli outlier sono persone con modelli di risposta aberranti o molti errori di Guttman. Sijtsma e van der Ark (2017) raccomandano di rimuovere gli outlier e di rieseguire l’analisi senza di essi. Se c’è una differenza notevole nei risultati, allora la rimozione degli outlier è giustificata. Per ottenere il numero di errori di Guttman per ogni rispondente eseguiamo il seguente codice:\n\ngutt &lt;- check.errors(good_items)\nprint(gutt)\n\n$Gplus\n  [1]  2  6  0  7  2  4 23 30  2  7 15  0  1  9  2  0  5 24  3  3  0  0 11 51  0\n [26]  4  2 19  9  2 60  2  9 35  2 17 39  2  4  3  3 12  3  0 10  2  0  0 37  4\n [51] 68  0  2 31  2  0 12 36  1 70 12  1 28 27 25  3  4 10  9 16 28 22 81 26  1\n [76]  0 65 28  0 25  3 59 60 40  0  5  0  0  1  0  8  3  8  6 27 12 22 16 12  4\n[101] 12  9  0  0 20 25 25  6  4  6  0 47 20 18  4  7  6 33 14 25 12  4  0  1 65\n[126] 17 83 26 65  5  5 30  2  9 42  7 13  8 16  3 41  0 12 21  4  8 26 22  3 46\n[151]  7 14 38 12 12  1  6  4 32 11  2  6 17  1  8  8 27  8  3  5 58  2 62  3  0\n[176] 12 10 45 21  0  2 10 31 19 46 11 21  2 25  9 17  2  4 20 21 49 18 19 14 16\n[201] 44  5  7 14 41 24 44  0 18  9 31 12  9 46  8  6 11 14 23 41 24  1  7 39 23\n[226]  0 23  3  6  2  2  4  0  0  8 24 38  0 16 62  2  2 21  7 25  9  0  0  8 14\n[251] 10  4  1  0  2 18  0 21 19  0 12  0 41  8  7 16 25 20  3 41  3  0  6 18  0\n[276]  4  2  4  4 12  5 13  6  9  7  6  0  6  7  2  0  0 13 26  8 41 30  2  0  6\n[301]  6  0  5  2  0  0 39 48  0 43  8 15 10  0  4 25 16  9  0  5  2  2  8  2  8\n[326]  7  1  0 53 32  9  3  0  4 13  3  6 35 30  2 34  8 10 10  5 23 22 24 12  3\n[351]  0  9 16 21 39 27 47 19 28 32  7  2 16  0  3 26 34  6  4 11  7 25  1 15  6\n[376] 10 38 17  0  6  3 38  4  0 52  5  2  3  0  0 14  0 27 22 32  8 32 14  2 31\n[401] 14 29 52 12 11 32  2  3 17  2  0  5 62 86 34  5 24 39  7 10 31  0  0 25 10\n[426] 19 31 13  7 31 36  8  0  0 28  6 23 15 52  3  1  7  4 33 24 11 24  0  2  3\n[451]  0  1  0\n\n$UGplus\n$UGplus$U1Gplus\n[1] 54.5\n\n$UGplus$U2Gplus\n[1] 354.5259\n\n\n\n\nIl vettore $Gplus mostra il numero di errori di Guttman G+ per ogni persona. I valori 54.5 e 354.5259 sono le Tukey fences per il rilevamento degli outlier. Essi segnalano i casi in cui il numero di errori di Guttman G+ è oltre il Tukey fence della distribuzione di G+.\nIl primo valore, 54.5, è il limite per il numero di errori di Guttman se la distribuzione è approssimativamente normale. I rispondenti con un numero di errori di Guttman superiore al limite possono essere considerati sospetti. Il valore 354.5259 è un limite di soglia per il numero di errori di Guttman se la distribuzione è asimmetrica.\n\nhist(gutt$Gplus)\n\n\n\n\n\n\n\n\nPer il caso presente, l’istogramma del numero di errori di Gutmann mostra che la loro distribuzione è asimmetrica positiva. Pertanto, possiamo scegliere il secondo limite soglia. Per trovare le persone con valori G+ sopra il Tucky fence superiore, eseguiamo il seguente codice.\n\nerr &lt;- gutt$Gplus \nwhich(err &gt; 354.5259)\n\n\n\n\nI risultati indicano che, in base a questo secondo criterio, non ci sono modelli di risposta sospetti nei dati.\nSe questa procedura producesse invece come risultato l’individuazione di alcuni outlier, potremmo rimuoverli dai dati nel modo seguente:\ngood_items_clean &lt;- good_items[-which(err &gt; 354.5259), ]",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_applications.html#considerazioni-conclusive",
    "href": "chapters/mokken/02_applications.html#considerazioni-conclusive",
    "title": "53  Applicazione Pratica",
    "section": "53.11 Considerazioni conclusive",
    "text": "53.11 Considerazioni conclusive\nIn questo capitolo, abbiamo esplorato un approccio pratico all’analisi di un set di dati reali, utilizzando il caso di studio sull’indagine della capacità di coping delle famiglie di fronte alla diagnosi di una grave malattia in un figlio, condotta dai ricercatori del Meyer. Attraverso una serie di passaggi metodici e analitici rigorosi, siamo riusciti a trasformare un complesso insieme di dati in informazioni comprensibili e significative.\nAbbiamo iniziato importando i dati e conducendo un’accurata pulizia per rimuovere gli item con eccessiva asimmetria e curtosi. Successivamente, abbiamo implementato la Procedura di Selezione Automatica degli Item (AISP) nell’ambito dell’Analisi delle Scale Mokken (MSA) per identificare e selezionare scale omogenee e coerenti.\nUna volta stabilite le scale, abbiamo approfondito le caratteristiche degli item, esaminando le loro statistiche descrittive e la loro distribuzione. Abbiamo anche valutato la loro affidabilità attraverso vari coefficienti, tra cui l’Alpha di Cronbach e il Coefficiente di Affidabilità della Classe Latente (LCRC), rilevando un’alta coerenza interna.\nUlteriori analisi hanno incluso la verifica della monotonicità degli item e dell’ordinamento invariante degli item (IIO), fondamentali per garantire che la scala rispettasse i principi teorici sottostanti la MSA. Infine, abbiamo esaminato la presenza di outlier, utilizzando i Tukey fences per identificare e gestire i modelli di risposta aberranti.\nIn conclusione, questo capitolo dimostra come l’applicazione metodica e sistematica delle tecniche di analisi statistica possa fornire intuizioni preziose e comprensibili da un set di dati complesso. Ciò non solo rafforza la validità e l’affidabilità della ricerca, ma fornisce anche una base solida per ulteriori indagini e interpretazioni.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_applications.html#session-info",
    "href": "chapters/mokken/02_applications.html#session-info",
    "title": "53  Applicazione Pratica",
    "section": "53.12 Session Info",
    "text": "53.12 Session Info\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats4    stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] mokken_3.1.2         poLCA_1.6.0.1        MASS_7.3-61         \n [4] scatterplot3d_0.3-44 mirt_1.42            lattice_0.22-6      \n [7] TAM_4.2-21           CDM_8.2-6            mvtnorm_1.3-1       \n[10] ggokabeito_0.1.0     viridis_0.6.5        viridisLite_0.4.2   \n[13] ggpubr_0.6.0         ggExtra_0.10.1       bayesplot_1.11.1    \n[16] gridExtra_2.3        patchwork_1.3.0      semTools_0.5-6      \n[19] semPlot_1.1.6        lavaan_0.6-18        psych_2.4.6.26      \n[22] scales_1.3.0         markdown_1.13        knitr_1.48          \n[25] lubridate_1.9.3      forcats_1.0.0        stringr_1.5.1       \n[28] dplyr_1.1.4          purrr_1.0.2          readr_2.1.5         \n[31] tidyr_1.3.1          tibble_3.2.1         ggplot2_3.5.1       \n[34] tidyverse_2.0.0      here_1.0.1          \n\nloaded via a namespace (and not attached):\n  [1] splines_4.4.1        later_1.3.2          pbdZMQ_0.3-13       \n  [4] R.oo_1.26.0          XML_3.99-0.17        rpart_4.1.23        \n  [7] lifecycle_1.0.4      rstatix_0.7.2        rprojroot_2.0.4     \n [10] globals_0.16.3       rockchalk_1.8.157    backports_1.5.0     \n [13] magrittr_2.0.3       openxlsx_4.2.7.1     Hmisc_5.1-3         \n [16] rmarkdown_2.28       httpuv_1.6.15        qgraph_1.9.8        \n [19] zip_2.3.1            sessioninfo_1.2.2    pbapply_1.7-2       \n [22] minqa_1.2.8          audio_0.1-11         multcomp_1.4-26     \n [25] abind_1.4-8          quadprog_1.5-8       R.utils_2.12.3      \n [28] nnet_7.3-19          TH.data_1.1-2        sandwich_3.1-1      \n [31] listenv_0.9.1        testthat_3.2.1.1     RPushbullet_0.3.4   \n [34] vegan_2.6-8          arm_1.14-4           parallelly_1.38.0   \n [37] permute_0.9-7        codetools_0.2-20     tidyselect_1.2.1    \n [40] farver_2.1.2         lme4_1.1-35.5        base64enc_0.1-3     \n [43] jsonlite_1.8.9       polycor_0.8-1        progressr_0.14.0    \n [46] Formula_1.2-5        survival_3.7-0       emmeans_1.10.4      \n [49] tools_4.4.1          snow_0.4-4           Rcpp_1.0.13         \n [52] glue_1.7.0           mnormt_2.1.1         xfun_0.47           \n [55] mgcv_1.9-1           admisc_0.36          IRdisplay_1.1       \n [58] withr_3.0.1          beepr_2.0            fastmap_1.2.0       \n [61] boot_1.3-31          fansi_1.0.6          digest_0.6.37       \n [64] mi_1.1               timechange_0.3.0     R6_2.5.1            \n [67] mime_0.12            estimability_1.5.1   colorspace_2.1-1    \n [70] gtools_3.9.5         jpeg_0.1-10          R.methodsS3_1.8.2   \n [73] utf8_1.2.4           generics_0.1.3       data.table_1.16.0   \n [76] corpcor_1.6.10       SimDesign_2.17.1     htmlwidgets_1.6.4   \n [79] pkgconfig_2.0.3      sem_3.1-16           gtable_0.3.5        \n [82] brio_1.1.5           htmltools_0.5.8.1    carData_3.0-5       \n [85] png_0.1-8            rstudioapi_0.16.0    tzdb_0.4.0          \n [88] reshape2_1.4.4       uuid_1.2-1           curl_5.2.3          \n [91] coda_0.19-4.1        checkmate_2.3.2      nlme_3.1-166        \n [94] nloptr_2.1.1         repr_1.1.7           zoo_1.8-12          \n [97] parallel_4.4.1       miniUI_0.1.1.1       foreign_0.8-87      \n[100] pillar_1.9.0         grid_4.4.1           vctrs_0.6.5         \n[103] promises_1.3.0       car_3.1-2            OpenMx_2.21.12      \n[106] xtable_1.8-4         Deriv_4.1.6          cluster_2.1.6       \n[109] dcurver_0.9.2        GPArotation_2024.3-1 htmlTable_2.4.3     \n[112] evaluate_1.0.0       pbivnorm_0.6.0       cli_3.6.3           \n[115] kutils_1.73          compiler_4.4.1       rlang_1.1.4         \n[118] crayon_1.5.3         future.apply_1.11.2  ggsignif_0.6.4      \n[121] fdrtool_1.2.18       plyr_1.8.9           stringi_1.8.4       \n[124] munsell_0.5.1        lisrelToR_0.3        pacman_0.5.1        \n[127] Matrix_1.7-0         IRkernel_1.3.2       hms_1.1.3           \n[130] glasso_1.11          future_1.34.0        shiny_1.9.1         \n[133] igraph_2.0.3         broom_1.0.6          RcppParallel_5.1.9",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_logistic_regr.html",
    "href": "chapters/irt/01_logistic_regr.html",
    "title": "54  Modello di Regressione Logistica",
    "section": "",
    "text": "54.1 Introduzione\nPrima di immergerci nell’esplorazione dei modelli di Teoria della Risposta all’Item (IRT), è fondamentale acquisire una solida comprensione del modello di regressione logistica. Questo modello, ampiamente utilizzato per analizzare dati categorici, ci serve come solido punto di partenza per comprendere i più complessi modelli IRT. La regressione logistica, infatti, si occupa di stimare la probabilità che un evento avvenga, date certe variabili predittive. Analogamente, i modelli IRT si focalizzano sulla probabilità che un esaminando risponda correttamente a un item, basandosi su parametri che descrivono sia l’abilità dell’esaminando sia le caratteristiche dell’item.\nNonostante le somiglianze, i modelli IRT si distaccano in maniera sostanziale dalla regressione logistica per la loro capacità di modellare simultaneamente le proprietà degli item e le abilità degli individui. Mentre la regressione logistica assume che ogni osservazione sia indipendente, i modelli IRT considerano le interdipendenze tra le risposte agli item e tra gli item stessi. Nel presente capitolo, ci dedicheremo ad esplorare il modello di regressione logistica.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_logistic_regr.html#modello-di-regressione-logistica-per-variabili-binarie",
    "href": "chapters/irt/01_logistic_regr.html#modello-di-regressione-logistica-per-variabili-binarie",
    "title": "54  Modello di Regressione Logistica",
    "section": "54.2 Modello di Regressione Logistica per Variabili Binarie",
    "text": "54.2 Modello di Regressione Logistica per Variabili Binarie\nIl modello di regressione logistica si utilizza per analizzare la relazione tra una variabile dipendente dicotomica, che assume i valori di “successo” e “fallimento”, e una o più variabili indipendenti, che possono essere sia quantitative che qualitative. Consideriamo \\(n\\) osservazioni indipendenti, dove \\(Y_i\\) indica l’osservazione \\(i\\)-esima della variabile risposta, per \\(i=1, \\dots, n\\). Ogni osservazione è associata a un vettore di variabili esplicative \\((x_1, \\dots, x_p)\\). La relazione che vogliamo esaminare è tra la probabilità di successo \\(\\pi_i\\) e le variabili esplicative, espressa dalla formula:\n\\[\nP(Y=1 \\mid X=x_i) = \\pi_i.\n\\]\nIn questo contesto, la variabile dipendente \\(Y\\) segue una distribuzione di Bernoulli, con i seguenti possibili valori:\n\\[\ny_i =\n\\begin{cases}\n    1 & \\text{per un successo (per l'osservazione $i$-esima)},\\\\\n    0 & \\text{per un fallimento}.\n\\end{cases}\n\\]\nLe probabilità associate a questi valori sono rispettivamente \\(\\pi\\) per il successo e \\(1-\\pi\\) per il fallimento:\n\\[\n\\begin{aligned}\n    P(Y_i = 1) &= \\pi,\\\\\n    P(Y_i = 0) &= 1-\\pi.\n\\end{aligned}\n\\]\nQuesto modello permette quindi di studiare come le variabili esplicative influenzino la probabilità di un evento binario, quali il successo o il fallimento.\nPer illustrare, consideriamo un dataset di 100 volontari, dove age è la variabile esplicativa e chd è la variabile risposta che indica la presenza (chd = 1) o assenza (chd = 0) di disturbi cardiaci. La media condizionata \\(\\mathbb{E}(Y \\mid X=x)\\) in una popolazione può essere vista come la proporzione di valori 1 per un dato punteggio \\(x\\) sulla variabile esplicativa (ad esempio, l’età), ovvero la probabilità condizionata \\(\\pi_i\\) di osservare la presenza di sintomi cardiaci in un certo gruppo d’età:\n\\[\n\\pi_i \\equiv P(Y = 1 \\mid X = x).\n\\]\nIl valore atteso diventa:\n\\[\n\\mathbb{E}(Y \\mid x) = \\pi_i.\n\\]\nSe \\(X\\) è una variabile discreta, possiamo calcolare la proporzione di \\(Y=1\\) per ogni valore di \\(X=x\\) nel campione. Queste proporzioni rappresentano una stima non parametrica della funzione di regressione di \\(Y\\) su \\(X\\), e possono essere stimate tramite tecniche di smoothing, come indicato nella figura seguente.\n\nchdage &lt;- read.table(\"../../data/logistic_reg/chdage_dat.txt\", header = TRUE)\nchdage |&gt;\n    head()\n\n\nA data.frame: 6 x 3\n\n\n\nid\nage\nchd\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n1\n20\n0\n\n\n2\n2\n23\n0\n\n\n3\n3\n24\n0\n\n\n4\n5\n25\n1\n\n\n5\n4\n25\n0\n\n\n6\n7\n26\n0\n\n\n\n\n\n\n# Calcolo delle proporzioni di Y = 1 per ogni valore di X\nprop_data &lt;- chdage %&gt;%\n    group_by(age) %&gt;%\n    summarise(prop_chd = mean(chd))\n\n# Grafico con smoothing\nggplot(prop_data, aes(x = age, y = prop_chd)) +\n    geom_point() + # Mostra i punti di proporzione\n    geom_smooth(method = \"loess\", span = 0.7) + # Regressione LOESS\n    labs(\n        title = \"Stima Non Parametrica della Regressione di CHD su Age\",\n        x = \"Età\",\n        y = \"Proporzione di CHD = 1\"\n    ) \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nPer valori bassi della variabile age la proporzione condizionata di valori \\(Y=1\\) è prossima allo 0. Per valori alti dell’età la proporzione di valori \\(Y=1\\) è prossima a 1.0. A livelli di età intermedi, la curva di regressione non parametrica gradualmente approssima i valori 0 e 1 seguendo un andamento sigmoidale.\nAnche se nel caso presente la regressione non parametrica produce un risultato sensato, è utile rappresentare la dipendenza di \\(Y\\) da \\(X\\) con una semplice funzione, in particolare quando ci sono molteplici variabili esplicative.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_logistic_regr.html#modello-lineare-nelle-probabilità",
    "href": "chapters/irt/01_logistic_regr.html#modello-lineare-nelle-probabilità",
    "title": "54  Modello di Regressione Logistica",
    "section": "54.3 Modello Lineare nelle Probabilità",
    "text": "54.3 Modello Lineare nelle Probabilità\nIntroduciamo un modello lineare con le seguenti assunzioni standard:\n\\[\nY_i = \\alpha + \\beta X_i + \\varepsilon_i,\n\\]\ndove \\(\\varepsilon_i\\) segue una distribuzione normale con media 0 e varianza 1 (\\(\\varepsilon_i \\sim \\mathcal{N}(0, 1)\\)) e gli errori \\(\\varepsilon_i\\) e \\(\\varepsilon_j\\) sono indipendenti per ogni \\(i \\neq j\\). Il valore atteso di \\(Y_i\\) è quindi \\(\\mathbb{E}(Y_i) = \\alpha + \\beta X_i\\), portando a:\n\\[\n\\pi_i = \\alpha + \\beta X_i.\n\\]\nQuesto è noto come modello lineare nelle probabilità (linear probability model). Tuttavia, questo approccio presenta una limitazione significativa: non garantisce che i valori predetti di \\(\\pi_i\\) siano confinati nell’intervallo [0,1], come richiesto per le probabilità.\n\n54.3.1 Problemi di Normalità\nConsiderando che \\(Y_i\\) può assumere solo i valori 0 o 1, i residui \\(\\varepsilon_i\\) risultano anch’essi dicotomici e quindi non possono seguire una distribuzione normale. Ad esempio, se \\(Y_i=1\\) con probabilità \\(\\pi_i\\), il residuo sarà:\n\\[\n\\varepsilon_i = 1 - \\mathbb{E}(Y_i) = 1 - (\\alpha + \\beta X_i) = 1 - \\pi_i.\n\\]\nSe, invece, \\(Y_i=0\\) con probabilità \\(1-\\pi_i\\), il residuo sarà:\n\\[\n\\varepsilon_i = 0 - \\mathbb{E}(Y_i) = 0 - (\\alpha + \\beta X_i) = - \\pi_i.\n\\]\nTuttavia, se la dimensione del campione è grande, il teorema del limite centrale può mitigare l’importanza dell’assunzione di normalità per le stime dei minimi quadrati.\n\n\n54.3.2 Problematiche di Omoschedasticità\nUtilizzare il metodo dei minimi quadrati può essere inappropriato in questo contesto poiché la varianza dei residui non è costante ma dipende dalla media, e quindi dalla variabile \\(X\\). Assumendo che il modello sia lineare, abbiamo che \\(\\mathbb{E}(\\varepsilon_i)=0\\). Sfruttando le relazioni discusse in precedenza, la varianza dei residui si calcola come:\n\\[\n\\mathbb{V}(\\varepsilon_i) = (1-\\pi_i)\\pi_i.\n\\]\nConsideriamo che la varianza dei residui \\(\\varepsilon_i\\) può essere espressa come:\n\\[\n\\text{Var}(\\varepsilon_i) = \\mathbb{E}(\\varepsilon_i^2) - \\mathbb{E}(\\varepsilon_i)^2,\n\\]\ndove \\(\\mathbb{E}(\\varepsilon_i^2)\\) è il valore atteso del quadrato dei residui e \\(\\mathbb{E}(\\varepsilon_i)^2\\) è il quadrato del valore atteso dei residui.\nOra calcoliamo \\(\\mathbb{E}(\\varepsilon_i^2)\\):\n\\[\n\\begin{align*}\n\\mathbb{E}(\\varepsilon_i^2) &= \\mathbb{E}[(Y_i - \\mathbb{E}(Y_i))^2] \\\\\n&= \\mathbb{E}[(Y_i - \\pi_i)^2] \\\\\n&= \\mathbb{E}[(Y_i^2 - 2Y_i\\pi_i + \\pi_i^2)] \\\\\n&= \\mathbb{E}(Y_i^2) - 2\\mathbb{E}(Y_i\\pi_i) + \\mathbb{E}(\\pi_i^2) \\\\\n&= \\mathbb{E}(Y_i) - 2\\mathbb{E}(Y_i\\pi_i) + \\pi_i^2 \\\\\n&= \\pi_i - 2\\pi_i^2 + \\pi_i^2 \\\\\n&= \\pi_i - \\pi_i^2 \\\\\n&= \\pi_i(1 - \\pi_i)\n\\end{align*}\n\\]\nOra calcoliamo \\(\\mathbb{E}(\\varepsilon_i)^2\\):\n\\[\n\\begin{align*}\n\\mathbb{E}(\\varepsilon_i)^2 &= (\\mathbb{E}(Y_i - \\mathbb{E}(Y_i)))^2 \\\\\n&= (\\mathbb{E}(Y_i - \\pi_i))^2 \\\\\n&= (0)^2 \\\\\n&= 0\n\\end{align*}\n\\]\nQuindi, sostituendo questi risultati nella formula della varianza dei residui, otteniamo:\n\\[\n\\text{Var}(\\varepsilon_i) = \\mathbb{E}(\\varepsilon_i^2) - \\mathbb{E}(\\varepsilon_i)^2 = \\pi_i(1 - \\pi_i)\n\\]\nQuindi, abbiamo dimostrato che la varianza dei residui nella regressione logistica può essere espressa come \\((1-\\pi_i)\\pi_i\\).\nDato che \\(\\pi_i\\) dipende da \\(x\\), ciò significa che la varianza non è costante in funzione di \\(x\\). Questa eteroschedasticità dei residui rappresenta un problema per le stime dei minimi quadrati nel modello lineare, specialmente quando le probabilità \\(\\pi_i\\) sono vicine a 0 o 1.\n\n\n54.3.3 Linearità\nIl maggiore inconveniente connesso all’adozione del modello lineare nelle probabilità deriva dal fatto che la stima della probabilità di successo, \\(P(\\hat{Y}_i=1)=\\hat{\\pi}_i\\), non è necessariamente compresa nell’intervallo (\\(0,1\\)), ma può essere sia negativa sia maggiore di 1. Nel caso dell’esempio in discussione, ciò significa che la retta dei minimi quadrati produce valori attesi \\(\\hat{\\pi}\\) inferiori a 0 per bassi valori della variabile età e valori \\(\\hat{\\pi}\\) superiori a 1 per valori di età alti.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_logistic_regr.html#modello-lineare-nelle-probabilità-vincolato",
    "href": "chapters/irt/01_logistic_regr.html#modello-lineare-nelle-probabilità-vincolato",
    "title": "54  Modello di Regressione Logistica",
    "section": "54.4 Modello Lineare nelle Probabilità Vincolato",
    "text": "54.4 Modello Lineare nelle Probabilità Vincolato\nUna soluzione per mantenere \\(\\pi\\) all’interno dell’intervallo (0, 1) è la seguente specificazione del modello:\n\\[\n\\pi=\n\\begin{cases}\n  0                           &\\text{se $\\alpha + \\beta X &lt; 0$},\\\\\n  \\alpha + \\beta X           &\\text{se $0 \\leq \\alpha + \\beta X \\leq 1$},\\\\\n  1 &\\text{se $\\alpha + \\beta X &gt; 1$}.\n\\end{cases}\n\\]\nQuesto modello lineare nelle probabilità vincolato mostra alcune instabilità, soprattutto a causa della sua dipendenza critica dai valori estremi di \\(\\pi\\), dove assume i valori 0 o 1. La linearità di \\(\\pi = \\alpha + \\beta X\\) si basa fortemente sui punti in cui si verificano questi estremi. In particolare, la stima di \\(\\pi = 0\\) può essere influenzata dal valore minimo di \\(X\\) associato a \\(Y=1\\), mentre la stima di \\(\\pi = 1\\) può dipendere dal valore massimo di \\(X\\) per cui \\(Y=0\\). Questi valori estremi tendono a variare significativamente tra diversi campioni e possono diventare più estremi all’aumentare della dimensione del campione.\nLa presenza di più variabili esplicative (\\(k \\geq 2\\)) complica ulteriormente la stima dei parametri del modello. Inoltre, il modello mostra un cambiamento brusco nella pendenza della curva di regressione ai punti estremi (0 e 1 di \\(\\pi\\)), risultando poco realistico in molte situazioni pratiche. Questo rende il modello meno adatto a descrivere relazioni complesse e gradualmente variabili tra \\(\\pi\\) e \\(X\\).\nUna funzione che modella una relazione più fluida e continua tra \\(\\pi\\) e \\(X\\) sarebbe più realistica e rappresentativa delle dinamiche osservate. Questo motiva la preferenza per modelli alternativi, come il modello di regressione logistica, che tende a fornire una rappresentazione più accurata e realistica delle interazioni tra variabili dicotomiche e esplicative.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_logistic_regr.html#regressione-logistica",
    "href": "chapters/irt/01_logistic_regr.html#regressione-logistica",
    "title": "54  Modello di Regressione Logistica",
    "section": "54.5 Regressione Logistica",
    "text": "54.5 Regressione Logistica\nUn metodo efficace per gestire il problema del vincolo sulle probabilità è specificare modelli non direttamente per le probabilità stesse, ma per una loro trasformazione che elimina tale vincolo. Invece di definire un modello lineare per la probabilità condizionata \\(\\pi_i\\), si può specificare un modello lineare per il logaritmo degli odds (logit):\n\\[\n\\eta_i = \\log_e \\frac{\\pi_i}{1-\\pi_i} = \\alpha + \\beta x_i,\n\\]\nQuesto approccio non presenta problemi poiché il logit \\(\\eta\\) è sempre un numero reale, permettendo di modellare una trasformazione lineare di \\(\\pi\\). L’immagine che segue è stata generata calcolando i logit empirici, ovvero il logaritmo degli odds \\(\\log_e(\\hat{\\pi}_i/(1-\\hat{\\pi}_i))\\) per 8 intervalli in cui è stata suddivisa la variabile “età”. Questa analisi mostra chiaramente una relazione lineare tra i logit empirici e l’età, evidenziando come la regressione logistica possa efficacemente modellare e interpretare relazioni tra variabili trasformate e fattori esplicativi.\n\nxc &lt;- c(24.5, 32, 37, 42, 47, 52, 57, 64.5)\nyc &lt;- c(0.1, 0.13, 0.25, 0.33, 0.46, 0.63, 0.76, 0.8)\nlogit_y &lt;- log(yc / (1 - yc))\nfit2 &lt;- lm(logit_y ~ xc)\nplot(\n    xc, logit_y,\n    xlab = \"Eta'\", ylab = \"Logit(Y)\",\n    main = \"\", type = \"n\"\n)\npoints(xc, logit_y, cex = 2)\nabline(fit2)",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_logistic_regr.html#probabilità-odds-e-logit",
    "href": "chapters/irt/01_logistic_regr.html#probabilità-odds-e-logit",
    "title": "54  Modello di Regressione Logistica",
    "section": "54.6 Probabilità, Odds e Logit",
    "text": "54.6 Probabilità, Odds e Logit\nLa relazione tra probabilità, odds e logit viene illustrata nella tabella qui sotto. È importante notare che gli odds e il logit trasformano l’intervallo di probabilità (0, 1) in uno spettro più ampio. Gli odds rappresentano il rapporto tra la probabilità di un evento e la probabilità del suo complemento. Il logit, invece, è il logaritmo naturale degli odds, trasformando così l’intervallo di probabilità in tutta la linea dei numeri reali. Quando la probabilità è 0.5, gli odds sono 1 e il logit è 0. Logit negativi indicano probabilità inferiori a 0.5, mentre logit positivi indicano probabilità superiori a 0.5.\n\n\n\n\n\n\n\n\nProbabilità (P)\nOdds (O)\nlogit (L)\n\n\n\n\n0.01\n0.01 / 0.99 = 0.0101\n\\(\\ln(\\frac{0.01}{0.99}) = -4.60\\)\n\n\n0.05\n0.05 / 0.95 = 0.0526\n\\(\\ln(\\frac{0.05}{0.95}) = -2.94\\)\n\n\n0.10\n0.10 / 0.90 = 0.1111\n\\(\\ln(\\frac{0.10}{0.90}) = -2.20\\)\n\n\n0.30\n0.30 / 0.70 = 0.4286\n\\(\\ln(\\frac{0.30}{0.70}) = -0.85\\)\n\n\n0.50\n0.50 / 0.50 = 1\n\\(\\ln(\\frac{0.50}{0.50}) = 0.00\\)\n\n\n0.70\n0.70 / 0.30 = 2.3333\n\\(\\ln(\\frac{0.70}{0.30}) = 0.85\\)\n\n\n0.90\n0.90 / 0.10 = 9\n\\(\\ln(\\frac{0.90}{0.10}) = 2.20\\)\n\n\n0.95\n0.95 / 0.05 = 19\n\\(\\ln(\\frac{0.95}{0.05}) = 2.94\\)\n\n\n0.99\n0.99 / 0.01 = 99\n\\(\\ln(\\frac{0.99}{0.01}) = 4.60\\)\n\n\n\n\n54.6.1 Trasformazione Inversa del Logit\nLa trasformazione inversa del logit, detta antilogit, consente di trasformare i logit in probabilità:\n\\[\n  \\pi_i =\\frac{e^{\\eta_i}}{1+e^{\\eta_i}}.\n\\]\nLogit e probabilità possono dunque essere trasformati gli uni nelle altre. La trasformazione inversa del logit consente di specificare un modello non lineare per le probabilità \\(\\pi_i\\). Tale modello non lineare è detto logit, o modello di regressione logistica:\n\\[\n  \\pi_i = \\frac{e^{\\eta_i}}{1+e^{\\eta_i}} =  \\frac{e^{\\alpha + \\beta x_i}}{1+e^{\\alpha + \\beta x_i}}.\n\\]\nLa funzione logistica ben rappresenta l’andamento sigmoidale delle proporzioni di casi \\(Y=1\\), ovvero \\(\\hat{\\pi}_i = E(Y \\mid x_i)\\) (le proporzioni di presenza di disturbi coronarici), in funzione di livelli crescenti della variabile age:\n\nfm &lt;- glm(chd ~ age, family = binomial(link = \"logit\"), data = chdage)\nlogit_hat &lt;- fm$coef[1] + fm$coef[2] * chdage$age\npi_hat &lt;- exp(logit_hat) / (1 + exp(logit_hat))\n\nplot(chdage$age, pi_hat,\n    xlab = \"Eta'\",\n    ylab = \"P(CHD)\",\n    main = \"\", type = \"n\"\n)\nlines(chdage$age, pi_hat)\nxc &lt;- c(24.5, 32, 37, 42, 47, 52, 57, 64.5)\nyc &lt;- c(0.1, 0.13, 0.25, 0.33, 0.46, 0.63, 0.76, 0.8)\npoints(xc, yc, cex = 2)",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_logistic_regr.html#modelli-lineari-generalizzati",
    "href": "chapters/irt/01_logistic_regr.html#modelli-lineari-generalizzati",
    "title": "54  Modello di Regressione Logistica",
    "section": "54.7 Modelli Lineari Generalizzati",
    "text": "54.7 Modelli Lineari Generalizzati\nNel caso di una variabile risposta binaria, il modello classico di regressione lineare si scontra con sfide specifiche:\n\nDistribuzione Binomiale: \\(Y_i\\) segue una distribuzione binomiale (con indice \\(n_i\\), potenzialmente uguale a uno nel caso individuale), rendendo non applicabile l’ipotesi di normalità.\nLimiti delle Probabilità: Utilizzando una specificazione lineare come \\(\\pi_i= \\beta_0 + \\beta_1 x_i\\), si possono ottenere stime di probabilità esterne all’intervallo 0-1.\nVarianze Non Costanti: La varianza di \\(\\varepsilon\\) varia in base alla specificazione del modello di probabilità, seguendo la formula \\(V(\\varepsilon_i)=\\pi_i(1-\\pi_i)\\).\n\nPer superare queste sfide, si utilizzano i Modelli Lineari Generalizzati (GLM). Questi modelli consentono l’uso di variabili risposta di diversa natura e includono:\n\nRegressione Lineare: Per variabili dipendenti continue e variabili esplicative continue o qualitative.\nRegressione Logistica: Per variabili risposta binarie.\nModello Loglineare di Poisson: Per modellare frequenze in tabelle di contingenza.\n\nI GLM allentano alcune ipotesi fondamentali del modello lineare classico, come linearità, normalità della componente erratica, e omoschedasticità delle osservazioni. Sono strutturati in tre componenti principali:\n\nComponente Aleatoria: Definisce la distribuzione di probabilità della variabile risposta \\(Y\\).\nComponente Sistematica: Specifica la relazione lineare tra le variabili esplicative e una trasformazione della variabile risposta.\nFunzione Legame: Trasforma la media attesa \\(\\mathbb{E}(Y)\\) in un formato che possa essere modellato linearmente rispetto alle variabili esplicative. Non è la variabile risposta stessa ad essere modellizzata direttamente, ma una sua trasformazione, come il logit nel caso della regressione logistica.\n\nEsempi di combinazioni di componenti aleatorie, funzioni di legame e sistematiche nei GLM includono:\n\n\n\n\n\n\n\n\n\nComponente Aleatoria\nFunzione Legame\nComponente Sistematica\nModello\n\n\n\n\nGaussiana\nIdentità\nContinua\nRegressione\n\n\nGaussiana\nIdentità\nCategoriale\nAnalisi della varianza\n\n\nGaussiana\nIdentità\nMista\nAnalisi della covarianza\n\n\nBinomiale\nLogit\nMista\nRegressione logistica\n\n\nPoisson\nLogaritmo\nMista\nModello Loglineare\n\n\n\nQuesta struttura rende i GLM particolarmente flessibili e adatti a una vasta gamma di situazioni statistiche, superando i limiti del modello lineare classico.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_logistic_regr.html#componente-sistematica",
    "href": "chapters/irt/01_logistic_regr.html#componente-sistematica",
    "title": "54  Modello di Regressione Logistica",
    "section": "54.8 Componente Sistematica",
    "text": "54.8 Componente Sistematica\nLa componente sistematica mette in relazione un vettore (\\(\\eta_1, \\eta_2, \\dots, \\eta_k\\)) con le variabili esplicative mediante un modello lineare. Sia \\(X_{ij}\\) il valore della \\(j\\)-esima variabile esplicativa (\\(j=1, 2, \\dots, p\\)) per l’\\(i\\)-esima osservazione (\\(i=1, \\dots, k\\)). Allora\n\\[\n\\eta_i = \\sum_j \\beta_j X_{ij}.\n\\]\nQuesta combinazione lineare di variabili esplicative è chiamata il predittore lineare. Un \\(X_{ij}=1, \\forall i\\) viene utilizzato per il coefficiente dell’intercetta del modello (talvolta denotata da \\(\\alpha\\)).",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_logistic_regr.html#componente-aleatoria",
    "href": "chapters/irt/01_logistic_regr.html#componente-aleatoria",
    "title": "54  Modello di Regressione Logistica",
    "section": "54.9 Componente Aleatoria",
    "text": "54.9 Componente Aleatoria\nLa componente aleatoria del modello suppone l’esistenza di \\(k\\) osservazioni indipendenti \\(y_1, y_2, \\dots, y_k\\), ciascuna delle quali viene trattata come la realizzazione di una variabile casuale \\(Y_i\\). Si assume che \\(Y_i\\) abbia una distribuzione binomiale:\n\\[\nY_i \\sim Bin(n_i, \\pi_i)\n\\]\ncon parametri \\(n_i\\) e \\(\\pi_i\\). Per dati individuali (uno per ciascun valore \\(x_i\\)), \\(n_i=1,\n    \\forall i\\).",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_logistic_regr.html#funzione-legame",
    "href": "chapters/irt/01_logistic_regr.html#funzione-legame",
    "title": "54  Modello di Regressione Logistica",
    "section": "54.10 Funzione Legame",
    "text": "54.10 Funzione Legame\nLa funzione legame \\(g(\\cdot)\\) mette in relazione il valore atteso della variabile risposta \\(Y_i\\) con la componente sistematica \\(\\eta_i\\) del modello. Abbiamo visto che \\(\\mathbb{E}(Y_i)=\\pi_i\\). Che relazione c’è tra \\(\\pi_i\\) e il predittore lineare \\(\\eta_i= \\alpha + \\sum_j  \\beta_j X_{ij}\\)? La risposta a questa domanda è data dalla funzione legame:\n\\[\n\\eta_i = g(\\pi_i) = \\ln{\\frac{\\pi_i}{1-\\pi_i}}\n\\]\nSi noti che la funzione legame non trasforma la variabile risposta \\(Y_i\\) ma bensì il suo valore atteso \\(\\pi_i\\).\nLa funzione legame è invertibile: anziché trasformare il valore atteso nel predittore lineare si può trasformare il predittore lineare nel valore atteso \\(\\pi_i\\):\n\\[\n\\pi_i = \\frac{e^{\\eta_i}}{1+e^{\\eta_i}} =  \\frac{e^{\\alpha + \\sum_j  \\beta_j X_{ij}}}{1+e^{\\alpha + \\sum_j  \\beta_j X_{ij}}}.\n\\]\nSi ottiene così un modello non lineare per le probabilità \\(\\pi_i\\).\n\nx &lt;- -5:5\nprob &lt;- invlogit(x)\nplot(x, prob, type = \"l\", main = \"Funzione logistica\", ylab = \"Probabilita'\", xlab = \"Valori X\")",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_logistic_regr.html#regressione-logistica-con-r",
    "href": "chapters/irt/01_logistic_regr.html#regressione-logistica-con-r",
    "title": "54  Modello di Regressione Logistica",
    "section": "54.11 Regressione Logistica con R",
    "text": "54.11 Regressione Logistica con R\nLa stima dei parametri del modello di regressione logistica per i dati in esame si ottiene in R utilizzando la funzione glm():\n\nfm &lt;- glm(chd ~ age,\n    family = binomial(link = \"logit\"),\n    data = chdage\n)\n\nSi noti che è necessario specificare sia la funzione teorica (family = binomial) della componente erratica del modello, sia la funzione legame (link = \"logit\"). L’output della funzione glm() può essere visualizzato utilizzando summary():\n\nsummary(fm) |&gt;\n    print()\n\n\nCall:\nglm(formula = chd ~ age, family = binomial(link = \"logit\"), data = chdage)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.30945    1.13365  -4.683 2.82e-06 ***\nage          0.11092    0.02406   4.610 4.02e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 136.66  on 99  degrees of freedom\nResidual deviance: 107.35  on 98  degrees of freedom\nAIC: 111.35\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nPer i dati dell’esempio, le probabilità predette sono uguali a\n\\[\n\\hat{\\pi}(x)=\\frac{e^{-5.309 + 0.111 \\times {\\tt age}}}{1+e^{-5.309 + 0.111 \\times {\\tt age}}}.\n\\]\nI logit stimati sono dati dall’equazione\n\\[\n\\hat{\\eta}(x)=-5.309 + 0.111 \\times {\\tt age}.\n\\]",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_logistic_regr.html#interpretazione-dei-coefficienti",
    "href": "chapters/irt/01_logistic_regr.html#interpretazione-dei-coefficienti",
    "title": "54  Modello di Regressione Logistica",
    "section": "54.12 Interpretazione dei Coefficienti",
    "text": "54.12 Interpretazione dei Coefficienti\nIl modello di regressione logistica può essere interpretato focalizzandosi sui coefficienti di regressione.\n\n54.12.1 Interpretazione Basata sui Log-Odds\n\nIntercetta (-5.30945):\n\nRappresenta il log-odds di sviluppare CHD quando l’età è 0. Benché questo non sia praticamente realistico, offre un punto di riferimento teorico.\nIl valore negativo suggerisce una bassa probabilità di CHD a 0 anni.\n\nCoefficiente di Età (0.11092):\n\nIndica la variazione dei log-odds di CHD per ogni anno aggiuntivo di età.\nUn valore positivo implica che, all’aumentare dell’età, aumenta anche la probabilità di CHD.\n\n\n\n\n54.12.2 Interpretazione Attraverso l’Odds Ratio\nPer calcolare l’odds ratio associato all’età, si esponenzia il coefficiente:\n\\[\n\\text{Odds Ratio per Età} = e^{0.11092} \\approx 1.12.\n\\]\nQuesto valore mostra la variazione degli odds di CHD per ogni incremento annuale di età. Un odds ratio maggiore di 1 indica un aumento della probabilità con l’età. Ad esempio, un odds ratio di 1.12 significa che per ogni anno in più, gli odds di avere CHD aumentano del 12%. L’odds ratio fornisce una visione relativa e intuitiva del cambiamento del rischio.\n\n\n54.12.3 Interpretazione Basata sulle Probabilità Predette\nL’interpretazione più diretta e intuitiva dei coefficienti nel modello di regressione logistica è tramite il calcolo delle probabilità predette. A differenza dei coefficienti grezzi o degli odds ratio, le probabilità predette offrono una comprensione immediata dell’impatto delle variabili.\nNel caso dello studio della probabilità di CHD in base all’età, si possono calcolare le probabilità predette di CHD per individui di diverse età. Questo metodo fornisce una rappresentazione chiara di come il rischio di CHD vari in funzione dell’età, rendendo l’interpretazione accessibile anche a chi non è esperto in statistica.\n\neffect_plot(fm,\n    pred = age, interval = TRUE, plot.points = TRUE,\n    jitter = 0.05\n)",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_logistic_regr.html#considerazioni-conclusive",
    "href": "chapters/irt/01_logistic_regr.html#considerazioni-conclusive",
    "title": "54  Modello di Regressione Logistica",
    "section": "54.13 Considerazioni Conclusive",
    "text": "54.13 Considerazioni Conclusive\nNel caso di una variabile dipendente binaria \\(Y_i\\), il tradizionale modello di regressione lineare non può essere applicato. Tale problema può essere risolto applicando il modello lineare non direttamente al valore atteso della variabile risposta, ma ad una sua trasformazione, il logit. La componente sistematica del modello di regressione lineare esprime il valore atteso della variabile dipendente come una funzione lineare dei predittori: \\(\\mu_i = \\boldsymbol{\\beta x}_i\\).\nPer il modello di regressione lineare, il valore atteso di \\(Y\\) è la media delle distribuzioni condizionate \\(Y \\mid x_i\\).\nPer il modello di regressione logistica, il valore atteso della variabile risposta binaria, condizionato ad un determinato valore della variabile esplicativa (o ad un insieme di valori del vettore di variabili esplicative) è uguale alla probabilità che \\(Y\\) assuma il valore 1:\n\\[\n\\mathbb{E}(Y \\mid x_i) \\equiv Pr(Y=1 \\mid X=x_i) \\equiv \\pi_i.\n\\]\nTale valore atteso può essere interpretato come la proporzione di individui nella popolazione per i quali \\(Y=1\\) in corrispondenza di \\(X=x_i\\).\nLa componente sistematica del modello di regressione logistica rappresenta una trasformazione di \\(\\pi_i\\) come funzione lineare dei predittori:\n\\[\n\\ln \\frac{\\pi_i}{1-\\pi_i} = \\alpha + \\beta X_i.\n\\]\nIl modello è dunque lineare nei logit. Equivalentemente, esponenziando\n\\[\n\\frac{\\pi_i}{1-\\pi_i} = \\exp(\\alpha + \\beta X_i)\n\\]\nl’odds stimato di \\(Y_i=1\\) è uguale a \\(\\exp(\\alpha + \\beta X_i)\\).\nLa funzione antilogit trasforma il predittore lineare \\(\\eta_i = \\alpha + \\beta X_i\\) nelle probabilità:\n\\[\n\\pi_i = \\frac{\\exp(\\alpha+\\beta X_i)}{1 + \\exp(\\alpha+\\beta X_i)}.\n\\]\nIl modello di regressione logistica è un modello non lineare nelle probabilità (ovvero, rispetto al valore atteso della variabile risposta).\nLa funzione logistica\n\\[\n\\Lambda(\\eta) = \\frac{\\exp(\\eta)}{1 + \\exp(\\eta)}\n\\]\nviene scelta quale funzione legame per trasformare la componente lineare del modello, \\(\\eta_i =\\alpha+\\beta X_i\\), nel valore atteso della variabile dipendente, \\(\\pi_i\\). Qualunque funzione cumulativa di probabilità potrebbe fungere da funzione legame, tuttavia, è conveniente scegliere la funzione logistica per facilità di interpretazione.\nLa componente aleatoria del modello di regressione logistica, infine, ci porta a considerare la variabile dipendente come una variabile aleatoria binomiale, sia nel caso di dati raggruppati (con denomiatore binomiale uguale alla frequenza delle osservazioni in ciascun gruppo \\(n_i\\) corrispondente a modalità omogenee della/e variabile/i esplicativa/e) che nel caso di dati individuali (dove \\(n_i\\) = 1, \\(\\forall i\\)).",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_logistic_regr.html#session-info",
    "href": "chapters/irt/01_logistic_regr.html#session-info",
    "title": "54  Modello di Regressione Logistica",
    "section": "54.14 Session Info",
    "text": "54.14 Session Info\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] jtools_2.3.0         effects_4.2-2        gmodels_2.19.1      \n [4] LaplacesDemon_16.1.6 car_3.1-2            carData_3.0-5       \n [7] ggokabeito_0.1.0     viridis_0.6.5        viridisLite_0.4.2   \n[10] ggpubr_0.6.0         ggExtra_0.10.1       bayesplot_1.11.1    \n[13] gridExtra_2.3        patchwork_1.3.0      semTools_0.5-6      \n[16] semPlot_1.1.6        lavaan_0.6-18        psych_2.4.6.26      \n[19] scales_1.3.0         markdown_1.13        knitr_1.48          \n[22] lubridate_1.9.3      forcats_1.0.0        stringr_1.5.1       \n[25] dplyr_1.1.4          purrr_1.0.2          readr_2.1.5         \n[28] tidyr_1.3.1          tibble_3.2.1         ggplot2_3.5.1       \n[31] tidyverse_2.0.0      here_1.0.1          \n\nloaded via a namespace (and not attached):\n  [1] splines_4.4.1       later_1.3.2         pbdZMQ_0.3-13      \n  [4] XML_3.99-0.17       rpart_4.1.23        lifecycle_1.0.4    \n  [7] rstatix_0.7.2       rprojroot_2.0.4     globals_0.16.3     \n [10] lattice_0.22-6      MASS_7.3-61         insight_0.20.4     \n [13] rockchalk_1.8.157   backports_1.5.0     survey_4.4-2       \n [16] magrittr_2.0.3      openxlsx_4.2.7.1    Hmisc_5.1-3        \n [19] rmarkdown_2.28      httpuv_1.6.15       qgraph_1.9.8       \n [22] zip_2.3.1           pbapply_1.7-2       DBI_1.2.3          \n [25] minqa_1.2.8         multcomp_1.4-26     abind_1.4-8        \n [28] quadprog_1.5-8      nnet_7.3-19         TH.data_1.1-2      \n [31] sandwich_3.1-1      listenv_0.9.1       gdata_3.0.0        \n [34] arm_1.14-4          parallelly_1.38.0   codetools_0.2-20   \n [37] tidyselect_1.2.1    farver_2.1.2        lme4_1.1-35.5      \n [40] broom.mixed_0.2.9.5 stats4_4.4.1        base64enc_0.1-3    \n [43] jsonlite_1.8.9      Formula_1.2-5       survival_3.7-0     \n [46] emmeans_1.10.4      tools_4.4.1         Rcpp_1.0.13        \n [49] glue_1.7.0          mnormt_2.1.1        mgcv_1.9-1         \n [52] xfun_0.47           IRdisplay_1.1       withr_3.0.1        \n [55] fastmap_1.2.0       mitools_2.4         boot_1.3-31        \n [58] fansi_1.0.6         digest_0.6.37       mi_1.1             \n [61] timechange_0.3.0    R6_2.5.1            mime_0.12          \n [64] estimability_1.5.1  colorspace_2.1-1    gtools_3.9.5       \n [67] jpeg_0.1-10         utf8_1.2.4          generics_0.1.3     \n [70] data.table_1.16.0   corpcor_1.6.10      htmlwidgets_1.6.4  \n [73] pkgconfig_2.0.3     sem_3.1-16          gtable_0.3.5       \n [76] furrr_0.3.1         htmltools_0.5.8.1   png_0.1-8          \n [79] rstudioapi_0.16.0   tzdb_0.4.0          reshape2_1.4.4     \n [82] uuid_1.2-1          coda_0.19-4.1       checkmate_2.3.2    \n [85] nlme_3.1-166        nloptr_2.1.1        repr_1.1.7         \n [88] zoo_1.8-12          parallel_4.4.1      miniUI_0.1.1.1     \n [91] foreign_0.8-87      pillar_1.9.0        grid_4.4.1         \n [94] vctrs_0.6.5         promises_1.3.0      OpenMx_2.21.12     \n [97] xtable_1.8-4        cluster_2.1.6       htmlTable_2.4.3    \n[100] evaluate_1.0.0      pbivnorm_0.6.0      mvtnorm_1.3-1      \n[103] cli_3.6.3           kutils_1.73         compiler_4.4.1     \n[106] rlang_1.1.4         crayon_1.5.3        ggsignif_0.6.4     \n[109] labeling_0.4.3      fdrtool_1.2.18      plyr_1.8.9         \n[112] stringi_1.8.4       pander_0.6.5        munsell_0.5.1      \n[115] lisrelToR_0.3       pacman_0.5.1        Matrix_1.7-0       \n[118] IRkernel_1.3.2      hms_1.1.3           glasso_1.11        \n[121] future_1.34.0       shiny_1.9.1         igraph_2.0.3       \n[124] broom_1.0.6         RcppParallel_5.1.9",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/irt/E_01.html",
    "href": "chapters/irt/E_01.html",
    "title": "55  ✏️ Esercizi",
    "section": "",
    "text": "(esercizi-logistic-reg)=\n\nsuppressPackageStartupMessages({\n    library(\"tidyverse\")\n    library(\"car\")\n    library(\"LaplacesDemon\")\n    library(\"gmodels\")\n    library(\"effects\")\n    library(\"psych\")\n    library(\"epitools\")\n    })\n\nE1. Per chiarire il significato del coefficiente \\(\\beta\\) nel caso della regressione logistica, si utilizzino i dati chdage e si ripeta l’analisi precedente dicotomizzando l’età in funzione della media.\n\nchdage &lt;- read.table(\"../../data/logistic_reg/chdage_dat.txt\", header = TRUE)\nchdage |&gt;\n    head()\n\n\nA data.frame: 6 x 3\n\n\n\nid\nage\nchd\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n1\n20\n0\n\n\n2\n2\n23\n0\n\n\n3\n3\n24\n0\n\n\n4\n5\n25\n1\n\n\n5\n4\n25\n0\n\n\n6\n7\n26\n0\n\n\n\n\n\n\n# Calcolo della media dell'età\nmedia_eta &lt;- mean(chdage$age)\n\n# Creazione di una variabile dicotomica per l'età\nchdage$eta_categoria &lt;- ifelse(chdage$age &gt; media_eta, \"SopraMedia\", \"SottoMedia\")\n\n# Visualizzazione della tavola 2x2\ntable(chdage$eta_categoria, chdage$chd)\n\n            \n              0  1\n  SopraMedia 16 32\n  SottoMedia 41 11\n\n\nAnalizziamo la relazione tra le due variabili categoriche (età categorizzata e CHD) utilizzando una tavola di contingenza 2x2 e il test del chi-quadrato.\n\ngmodels::CrossTable(chdage$eta_categoria, chdage$chd, chisq = TRUE, missing.include = TRUE)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  100 \n\n \n                     | chdage$chd \nchdage$eta_categoria |         0 |         1 | Row Total | \n---------------------|-----------|-----------|-----------|\n          SopraMedia |        16 |        32 |        48 | \n                     |     4.717 |     6.252 |           | \n                     |     0.333 |     0.667 |     0.480 | \n                     |     0.281 |     0.744 |           | \n                     |     0.160 |     0.320 |           | \n---------------------|-----------|-----------|-----------|\n          SottoMedia |        41 |        11 |        52 | \n                     |     4.354 |     5.771 |           | \n                     |     0.788 |     0.212 |     0.520 | \n                     |     0.719 |     0.256 |           | \n                     |     0.410 |     0.110 |           | \n---------------------|-----------|-----------|-----------|\n        Column Total |        57 |        43 |       100 | \n                     |     0.570 |     0.430 |           | \n---------------------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  21.09448     d.f. =  1     p =  4.371863e-06 \n\nPearson's Chi-squared test with Yates' continuity correction \n------------------------------------------------------------\nChi^2 =  19.27843     d.f. =  1     p =  1.129752e-05 \n\n \n\n\nCalcoliamo l’odds ratio per valutare le probabilità relative di CHD tra le due categorie di età.\n\nepitools::oddsratio(chdage$eta_categoria, chdage$chd, conf.level = 0.95, method = \"wald\") |&gt; print()\n\n$data\n            Outcome\nPredictor     0  1 Total\n  SopraMedia 16 32    48\n  SottoMedia 41 11    52\n  Total      57 43   100\n\n$measure\n            odds ratio with 95% C.I.\nPredictor     estimate      lower     upper\n  SopraMedia 1.0000000         NA        NA\n  SottoMedia 0.1341463 0.05475114 0.3286733\n\n$p.value\n            two-sided\nPredictor      midp.exact fisher.exact   chi.square\n  SopraMedia           NA           NA           NA\n  SottoMedia 4.582028e-06 6.194004e-06 4.371863e-06\n\n$correction\n[1] FALSE\n\nattr(,\"method\")\n[1] \"Unconditional MLE & normal approximation (Wald) CI\"\n\n\nCreiamo un modello di regressione logistica per stimare l’odds ratio, ponendo CHD come variabile dipendente e l’età categorizzata come variabile indipendente. Il coefficiente del modello sarà interpretato come log odds.\n\nlogit_model &lt;- glm(formula = chd ~ eta_categoria, data = chdage, family = binomial(link = \"logit\"))\nsummary(logit_model)\n\n\nCall:\nglm(formula = chd ~ eta_categoria, family = binomial(link = \"logit\"), \n    data = chdage)\n\nCoefficients:\n                        Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)               0.6931     0.3062   2.264   0.0236 *  \neta_categoriaSottoMedia  -2.0088     0.4572  -4.394 1.12e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 136.66  on 99  degrees of freedom\nResidual deviance: 114.77  on 98  degrees of freedom\nAIC: 118.77\n\nNumber of Fisher Scoring iterations: 4\n\n\nGeneriamo l’intervallo di confidenza al 95%\n\n\nconfint(logit_model)\n\nWaiting for profiling to be done...\n\n\n\n\nA matrix: 2 x 2 of type dbl\n\n\n\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n0.108547\n1.318141\n\n\neta_categoriaSottoMedia\n-2.942187\n-1.140921\n\n\n\n\n\nEsponenziazione dei coefficienti per ottenere l’Odds Ratio\n\nexp(coef(logit_model))     # Odds Ratio\nexp(confint(logit_model))  # 95% CI (Odds Ratio)\n\n(Intercept)1.99999999999959eta_categoriaSottoMedia0.134146341463534\n\n\nWaiting for profiling to be done...\n\n\n\n\nA matrix: 2 x 2 of type dbl\n\n\n\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n1.11465733\n3.7364701\n\n\neta_categoriaSottoMedia\n0.05275022\n0.3195247\n\n\n\n\n\nNella regressione logistica, il coefficiente \\(\\beta\\) per la variabile esplicativa dicotomica (in questo caso, l’età categorizzata) indica la variazione dei log odds di CHD per la categoria “SopraMedia” rispetto alla categoria “SottoMedia”. Esponenziando questo coefficiente, si ottiene l’odds ratio, che rappresenta la variazione relativa del rischio (o delle probabilità) di CHD tra le due categorie di età.\nRicordiamo che l’interpretazione deve tenere conto del significato statistico e della grandezza dell’odds ratio, oltre che della posizione dell’intervallo di confidenza rispetto al valore neutro 1. Se l’intervallo di confidenza attraversa 1, la differenza non è statisticamente significativa.\nUn odds ratio di 1.13 può essere interpretato nel modo seguente: passando dalla categoria ‘SottoMedia’ a quella ‘SopraMedia’, gli odds di malattie coronariche aumentano del 13%. L’intervallo di confidenza al 95% per l’odds ratio si estende da 1.05 a 1.32. Questo significa che siamo il 95% confidenti che il vero incremento degli odds di malattie coronariche, passando dalla categoria ‘SottoMedia’ a ‘SopraMedia’, sia compreso tra il 5% e il 32%.\nE2. Prendiamo in considerazione lo studio condotto da Cowles e Davis (1987). In questo studio, gli autori hanno intervistato un campione di 1421 studenti universitari per determinare se fossero disposti, in linea di principio, a partecipare a ulteriori ricerche. La variabile di risposta, chiamata volunteer, è una variabile dicotomica che indica se gli studenti erano disposti o meno a partecipare. Inoltre, hanno misurato i livelli di neuroticismo ed estroversione utilizzando l’Inventario della personalità di Eysenck. I dati relativi a questo studio sono disponibili nel data frame chiamato Cowles all’interno del pacchetto R chiamato effects.\nIl problema chiede di costruire un modello GLM (Generalized Linear Model) che possa prevedere la probabilità di essere disposti a partecipare (volunteer) in base al genere degli studenti, tenendo conto dei livelli di neuroticismo ed estroversione, nonché della loro interazione. Questo modello ci permetterà di comprendere come queste variabili influenzino la probabilità di partecipazione degli studenti a ulteriori ricerche.\nDopo aver costruito il modello, il problema chiede interpretare i risultati ottenuti per comprendere come il genere, il neuroticismo e l’estroversione influenzino la volontà degli studenti di partecipare a ulteriori ricerche.\n\ndata(Cowles)\ndim(Cowles)\n\n\n14214\n\n\n\nhead(Cowles)\n\n\nA data.frame: 6 x 4\n\n\n\nneuroticism\nextraversion\nsex\nvolunteer\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;fct&gt;\n&lt;fct&gt;\n\n\n\n\n1\n16\n13\nfemale\nno\n\n\n2\n8\n14\nmale\nno\n\n\n3\n5\n16\nmale\nno\n\n\n4\n8\n20\nfemale\nno\n\n\n5\n9\n19\nmale\nno\n\n\n6\n6\n15\nmale\nno\n\n\n\n\n\n\nCowles$sex &lt;- relevel(Cowles$sex, ref = \"male\")\ncontrasts(Cowles$sex)\n\n\nA matrix: 2 x 1 of type dbl\n\n\n\nfemale\n\n\n\n\nmale\n0\n\n\nfemale\n1\n\n\n\n\n\n\nfit_cowles &lt;- glm(\n    volunteer ~ sex + neuroticism*extraversion,\n    family = binomial,\n    data = Cowles\n)\nsummary(fit_cowles)\n\n\nCall:\nglm(formula = volunteer ~ sex + neuroticism * extraversion, family = binomial, \n    data = Cowles)\n\nCoefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)              -2.605359   0.500719  -5.203 1.96e-07 ***\nsexfemale                 0.247152   0.111631   2.214  0.02683 *  \nneuroticism               0.110777   0.037648   2.942  0.00326 ** \nextraversion              0.166816   0.037719   4.423 9.75e-06 ***\nneuroticism:extraversion -0.008552   0.002934  -2.915  0.00355 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1933.5  on 1420  degrees of freedom\nResidual deviance: 1897.4  on 1416  degrees of freedom\nAIC: 1907.4\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nplot(Effect(focal.predictors = c(\"neuroticism\",\"extraversion\",\"sex\"), \n            mod = fit_cowles,\n            xlevels=list(extraversion=seq(10, 20, 2), neuroticism=10:20)),\n     multiline = TRUE)\n\n\n\n\n\n\n\n\nCalcoliamo l’Odds Ratio:\n\nexp(coef(fit_cowles)) |&gt; print()\n\n             (Intercept)                sexfemale              neuroticism \n              0.07387658               1.28037375               1.11714535 \n            extraversion neuroticism:extraversion \n              1.18153740               0.99148400 \n\n\nCalcoliamo il 95% CI dell’Odds Ratio:\n\nexp(confint(fit_cowles)) |&gt; print()\n\nWaiting for profiling to be done...\n\n\n\n                              2.5 %    97.5 %\n(Intercept)              0.02723627 0.1943229\nsexfemale                1.02911058 1.5942774\nneuroticism              1.03815341 1.2034925\nextraversion             1.09828160 1.2735034\nneuroticism:extraversion 0.98575501 0.9971703\n\n\nGli odds ratio ottenuti dal modello di regressione logistica possono essere interpretati come segue: in questo campione di studio, gli odds di partecipare a ulteriori ricerche sono risultati essere 1.28 volte più alti per le femmine rispetto ai maschi, quando vengono controllati gli effetti del neuroticismo e dell’estroversione, nonché della loro interazione.\nL’intervallo di confidenza al 95% per l’odds ratio è compreso tra 1.03 e 1.59. Questo significa che siamo sicuri al 95% che il reale aumento degli odds di partecipare a ulteriori ricerche per le femmine rispetto ai maschi, una volta che si tengono in considerazione gli effetti del neuroticismo, dell’estroversione e della loro interazione, varia tra il 3% e il 59%.\nIn altre parole, ci sono evidenze statistiche significative che suggeriscono che il genere ha un impatto sulla volontà di partecipare a ulteriori ricerche, e tale impatto è positivo per le femmine rispetto ai maschi, anche quando si considerano le influenze dei livelli di neuroticismo ed estroversione e della loro interazione.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/irt/02_rasch_model.html",
    "href": "chapters/irt/02_rasch_model.html",
    "title": "56  Modello di Rasch",
    "section": "",
    "text": "56.1 Introduzione\nLa psicometria, una disciplina focalizzata sulla misurazione quantitativa di abilità, atteggiamenti e tratti psicologici, ha subìto un’importante evoluzione con l’introduzione della Teoria della Risposta all’Item (IRT). Questo modello rappresenta un passo avanti rispetto alla teoria classica dei test (CTT) per quanto riguarda la gestione e la concettualizzazione degli errori di misurazione. Nel presente capitolo, verranno esaminate le basi dell’IRT.\nL’IRT segna un vero e proprio cambiamento paradigmatico nella misurazione psicometrica. A differenza della CTT, che si basa sull’analisi del punteggio totale di un test, l’IRT si concentra sulle risposte ai singoli item, consentendo un’analisi più dettagliata e accurata delle capacità di un individuo. Il cuore dell’IRT risiede nella sua capacità di modellare la probabilità che un individuo risponda correttamente a un item specifico, in base alle caratteristiche dell’item stesso e al livello di abilità dell’individuo.\nUno degli aspetti peculiari dell’IRT è la sua capacità di trattare le risposte categoriali, che sono molto comuni nei test psicometrici, dove le persone scelgono tra un insieme di categorie predefinite.\nL’IRT ha un’ampia gamma di applicazioni, tra cui lo sviluppo di test standardizzati, la valutazione della qualità degli item, e la costruzione di punteggi compositi basati sulle risposte a più item. Questa teoria fornisce strumenti per creare misurazioni più valide e accurate, migliorando la comprensione delle abilità o tratti misurati.\nUn aspetto chiave dell’IRT è la sua capacità di analizzare l’interazione tra le caratteristiche degli item e le abilità degli individui. Ciò permette di ottenere una comprensione più approfondita di come diversi tipi di item influenzino le risposte, consentendo una misurazione più precisa e personalizzata.\nTuttavia, l’implementazione dell’IRT presenta delle sfide, tra cui la necessità di campioni di dati di grandi dimensioni e una maggiore complessità nella modellizzazione statistica. In questo capitolo, seguirò l’approccio di Debelak, Strobl, e Zeigenfuse (2022) per approfondire i modelli IRT e le curve caratteristiche degli item (ICC).",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/02_rasch_model.html#la-scala-di-guttman",
    "href": "chapters/irt/02_rasch_model.html#la-scala-di-guttman",
    "title": "56  Modello di Rasch",
    "section": "56.3 La Scala di Guttman",
    "text": "56.3 La Scala di Guttman\nPrima di esplorare più in dettaglio i modelli IRT, è importante comprendere il concetto di Scala di Guttman, che stabilisce una relazione gerarchica tra la difficoltà degli item e le abilità degli individui. Nella Scala di Guttman, si presuppone che una persona con un certo livello di abilità sia in grado di rispondere correttamente a tutti gli item meno difficili, ma non a quelli più difficili.\nGraficamente, la Scala di Guttman è rappresentata attraverso le curve caratteristiche degli item (ICC), dove:\n\nL’asse verticale rappresenta la probabilità di rispondere correttamente a un item, che secondo Guttman è binaria: o si risponde correttamente (1.0) o no (0.0).\nL’asse orizzontale rappresenta il livello di abilità dell’individuo.\n\nIn un test ideale basato su questa scala, ogni individuo dovrebbe rispondere correttamente a tutti gli item a sinistra della propria posizione (item meno difficili) e non rispondere correttamente a quelli a destra (item più difficili).\nLe cinque frecce nel grafico in basso indicano cinque persone con diversi livelli di abilità. Ogni freccia mostra il punto in cui l’abilità di una persona interseca le varie curve caratteristiche degli item. In un test ideale basato sulla Scala di Guttman, ogni persona dovrebbe essere in grado di rispondere correttamente a tutti gli item a sinistra della sua posizione sul grafico (ovvero, quelli meno difficili) e non riuscire a rispondere a quelli a destra (più difficili).\n\n# Data creation\nabilita &lt;- seq(0, 5, by = 0.1)\ndifficolta_item_1 &lt;- 1\ndifficolta_item_2 &lt;- 2\ndifficolta_item_3 &lt;- 3\nprobabilita_item_1 &lt;- as.numeric(abilita &gt;= difficolta_item_1)\nprobabilita_item_2 &lt;- as.numeric(abilita &gt;= difficolta_item_2)\nprobabilita_item_3 &lt;- as.numeric(abilita &gt;= difficolta_item_3)\ndata &lt;- data.frame(abilita, probabilita_item_1, probabilita_item_2, probabilita_item_3)\n\n# Plot creation\np &lt;- ggplot(data, aes(x = abilita)) +\n    geom_line(aes(y = probabilita_item_1), color = \"blue\", linewidth = 1.5) +\n    geom_line(aes(y = probabilita_item_2), color = \"red\", linewidth = 1.5) +\n    geom_line(aes(y = probabilita_item_3), color = \"green\", linewidth = 1.5) +\n    labs(\n        x = \"Abilita' dei partecipanti\", y = \"Probabilita' di risposta corretta\",\n        title = \"Modello di Guttman per la Scala degli Item\"\n    )\n\ny_posizione_frecce &lt;- -0.15\n\n# Add arrows using annotate to avoid repetition warnings\np &lt;- p + annotate(\"segment\",\n    x = 0.5, xend = 0.5, y = y_posizione_frecce, yend = 0,\n    arrow = arrow(), color = \"black\"\n) +\n    annotate(\"segment\",\n        x = 1.5, xend = 1.5, y = y_posizione_frecce, yend = 0,\n        arrow = arrow(), color = \"black\"\n    ) +\n    annotate(\"segment\",\n        x = 2.5, xend = 2.5, y = y_posizione_frecce, yend = 0,\n        arrow = arrow(), color = \"black\"\n    ) +\n    annotate(\"segment\",\n        x = 3.5, xend = 3.5, y = y_posizione_frecce, yend = 0,\n        arrow = arrow(), color = \"black\"\n    ) +\n    annotate(\"segment\",\n        x = 4.5, xend = 4.5, y = y_posizione_frecce, yend = 0,\n        arrow = arrow(), color = \"black\"\n    )\n\n# Print the plot\nprint(p)\n\n\n\n\n\n\n\n\nIl modello di Guttman si basa su un’ipotesi chiave riguardante la somma dei punteggi ottenuti dai partecipanti a un test. Secondo questo modello, se un individuo risponde correttamente a un item di difficile risoluzione, è logicamente presunto che abbia anche risposto correttamente a tutti gli item che sono considerati di difficoltà inferiore. Questo concetto implica che la capacità di rispondere a domande più difficili include implicitamente l’abilità di rispondere a quelle più semplici, rendendo il punteggio totale ottenuto in un test un indicatore affidabile dell’abilità complessiva della persona.\nTuttavia, il modello di Guttman non è esente da limitazioni. Una delle critiche principali è che raramente si osservano scale di Guttman pure nella pratica. In altre parole, non è sempre vero che i test seguano un andamento così rigido e prevedibile come quello proposto da Guttman. Nella realtà, la probabilità di rispondere correttamente a un item non sempre aumenta bruscamente da 0 a 1 al raggiungimento di una certa soglia di abilità. Piuttosto, questa probabilità può variare in modo più graduale, spesso descritta da una funzione sigmoidale.\nInoltre, è importante considerare che le curve caratteristiche degli item (ICC) possono variare nella loro ripidità per i diversi item di un test. Ciò significa che alcuni item potrebbero mostrare un cambiamento più marcato nella probabilità di risposta corretta al variare dell’abilità, mentre altri potrebbero mostrare un cambiamento più graduale. Questa variabilità delle ICC riflette la complessità e la diversità delle abilità umane e delle modalità con cui queste si manifestano in contesti dei test.\nIn sintesi, il modello di Guttman fornisce un quadro utile per comprendere la relazione tra abilità e performance in un test, ma è importante riconoscerne le limitazioni e comprendere che nella realtà psicometrica, le relazioni tra abilità, difficoltà dell’item e probabilità di risposta corretta possono essere più complesse e meno prevedibili di quanto il modello di Guttman suggerisca.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/02_rasch_model.html#il-modello-di-rasch",
    "href": "chapters/irt/02_rasch_model.html#il-modello-di-rasch",
    "title": "56  Modello di Rasch",
    "section": "56.4 Il Modello di Rasch",
    "text": "56.4 Il Modello di Rasch\nIl modello di Rasch può essere visto come un’evoluzione o un superamento dei limiti dello scaling di Guttman. Consideriamo, ad esempio, la competenza matematica. Possiamo considerare la competenza matematica come una variabile latente, cioè un tratto non direttamente osservabile che può essere dedotto tramite comportamenti misurabili. Supponiamo di utilizzare un test di matematica con cinque domande per valutare la competenza matematica degli individui; le risposte a queste domande sono le nostre osservazioni misurabili.\nSecondo il modello di Rasch, sia la difficoltà delle domande del test che le abilità dei partecipanti sono rappresentate su un unico continuum. Le competenze più elevate si trovano verso l’estremità superiore di questo continuum, mentre le inferiori si collocano verso l’estremità opposta. Le domande del test sono disposte lungo questo continuum in base alla loro difficoltà. Ad esempio, i valori come -2, -1 ecc. indicano la difficoltà specifica di ciascuna domanda, dove \\(\\beta_i\\) rappresenta la posizione della i-esima domanda.\nAnalogamente, la competenza matematica di un individuo è indicata da \\(\\theta\\), che rappresenta la sua posizione sul continuum. Questo permette di effettuare comparazioni dirette tra come un individuo potrebbe rispondere a una determinata domanda. Un individuo collocato al valore 0 del continuum ha una probabilità maggiore di rispondere correttamente a domande meno difficili (ad esempio, una con \\(\\beta = -2\\)), mentre la sua probabilità di rispondere correttamente diminuisce per domande poste vicino al suo valore di \\(\\theta\\).\nQuando si somministra una domanda più difficile (come una con \\(\\beta = 1\\)) a una persona con \\(\\theta = 0\\), è probabile che la risposta sia errata, ma esiste comunque una possibilità che risponda correttamente a causa della vicinanza tra la competenza richiesta dalla domanda e quella posseduta dall’individuo. La distanza tra le posizioni di una persona e di una domanda sul continuum influisce sulla certezza della risposta prevista: maggiore è la distanza, più certa è la risposta attesa. Quando questa distanza si riduce, la probabilità di una risposta corretta o errata diventa quasi equiprobabile, trasformando le previsioni sulle risposte in stime probabilistiche e offrendo una visione più matizzata delle competenze dell’individuo.\nPer analizzare il modello di Rasch, consideriamo i dati reali da un test di matematica per esaminare come i punteggi totali dei partecipanti (un indicatore delle loro abilità latenti) influenzino la probabilità di rispondere correttamente a domande di varia difficoltà. L’analisi di questi dati ci aiuta a verificare in pratica la teoria alla base del modello di Rasch, permettendo di osservare che gli individui con punteggi più alti tendono a rispondere correttamente a domande più difficili, mentre quelli con punteggi più bassi mostrano una maggiore probabilità di successo con domande più facili.\n\nmath_dat &lt;- rio::import(\"../../data/deAyala/Math.txt\")\nhead(math_dat)\n\n\nA data.frame: 6 x 5\n\n\n\nV1\nV2\nV3\nV4\nV5\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n1\n1\n0\n0\n0\n\n\n2\n1\n1\n1\n0\n0\n\n\n3\n1\n0\n0\n0\n0\n\n\n4\n1\n1\n1\n0\n0\n\n\n5\n1\n0\n1\n1\n0\n\n\n6\n1\n1\n1\n0\n0\n\n\n\n\n\nCalcoliamo la media delle risposte corrette per ciascun item:\n\ncolMeans(math_dat)\n\nV10.887454721697873V20.644048773021785V30.565991531044335V40.426968011836131V50.387327177184838\n\n\nGli item del test sono ordinati per difficoltà crescente: V1 è il più facile, seguito da V2, V3, V4 e V5, che sono progressivamente più difficili.\nOra calcoliamo la proporzione di risposte corrette per ciascun item, tenendo conto del livello di abilità dei partecipanti, che deduciamo dal loro punteggio complessivo nel test.\n\n# Calculate the total score for each subject\nmath_dat2 &lt;- math_dat\nmath_dat2$total_score &lt;- rowSums(math_dat2[, -1])\n\n# Prepare data for plotting\nplot_data &lt;- lapply(names(math_dat2)[1:5], function(item) {\n    math_dat2 %&gt;%\n        group_by(total_score) %&gt;%\n        summarise(\n            proportion = mean(get(item) == 1)\n        ) %&gt;%\n        mutate(item = item)\n})\n\nplot_data &lt;- do.call(rbind, plot_data)\n\n# Plotting\nggplot(plot_data, aes(x = total_score, y = proportion, group = item, color = item)) +\n    geom_line(linewidth = 1.5) +\n    labs(\n        x = \"Total Score\", \n        y = \"Proportion of Correct Responses\",\n        title = \"Proportion of Correct Responses by Total Score\"\n    ) \n\n\n\n\n\n\n\n\nSi nota che la probabilità di rispondere correttamente a un item aumenta con il crescere della competenza matematica generale, rappresentata dal punteggio totale. Inoltre, gli item più facili tendono ad avere una probabilità più alta di risposta corretta rispetto agli item più difficili. Questo pattern conferma che quanto maggiore è l’abilità generale di un individuo, tanto maggiore è la sua capacità di rispondere correttamente agli item del test.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/02_rasch_model.html#curva-caratteristica-dellitem",
    "href": "chapters/irt/02_rasch_model.html#curva-caratteristica-dellitem",
    "title": "56  Modello di Rasch",
    "section": "56.5 Curva Caratteristica dell’Item",
    "text": "56.5 Curva Caratteristica dell’Item\nDopo aver analizzato le tendenze nelle risposte ai vari item del test, ci possiamo porre il problema di sviluppare un modello matematico che descriva come la posizione di un individuo su un continuum latente influisce sulle sue risposte. La non linearità osservata nelle curve caratteristiche degli item (ICC) empiriche della figura precedente suggerisce che un modello di regressione lineare non sarebbe appropriato in questo contesto. Infatti, l’andamento curvilineo e ogivale delle curve, simile alle distribuzioni cumulative logistiche o normali, indica la necessità di utilizzare una funzione logistica per la nostra analisi, scelta per la sua capacità di modellare variazioni graduali e continuative.\nLa curva caratteristica dell’item, definita dalla funzione logistica, rappresenta un’evoluzione del modello di Guttman. A differenza di quest’ultimo, che suggerisce un cambiamento netto e improvviso nella probabilità di risposta corretta, il modello rappresentato dall’ICC descrive un cambiamento graduale, delineato da una curva sigmoidale. Questo rende la rappresentazione più fedele alla realtà del comportamento di risposta.\nLa teoria della risposta agli item (IRT) si distacca dallo scaling di Guttman fornendo un modello statistico per descrivere le ICC. In questo contesto, il modello di Rasch si distingue per la sua elegante semplicità, postulando che le curve caratteristiche di tutti gli item siano parallele e abbiano la stessa pendenza. Secondo il modello di Rasch, quindi, la difficoltà di un item è l’unico fattore che modifica la forma della curva caratteristica, illustrando chiaramente come la probabilità di risposta corretta varia in funzione dell’abilità latente del rispondente, \\(\\theta\\).\nNella figura seguente, vediamo le ICC rappresentate con delle funzioni matematiche per i cinque item analizzati secondo il modello di Rasch. L’asse orizzontale mostra l’abilità latente dei rispondenti, mentre l’asse verticale rappresenta la probabilità di una risposta corretta. Ogni curva sigmoidale, distinta per colore, corrisponde a uno degli item del test. La parallelità di queste curve conferma che la probabilità di rispondere correttamente varia esclusivamente in base all’abilità dei rispondenti, senza influenze esterne sulla forma della curva.\n\n# Adatta il modello di Rasch\nrasch_model &lt;- rasch(math_dat)\n\n# Crea le curve caratteristiche degli item\nplot(rasch_model, type = \"ICC\")",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/02_rasch_model.html#il-modello-di-rasch-per-risposte-binarie",
    "href": "chapters/irt/02_rasch_model.html#il-modello-di-rasch-per-risposte-binarie",
    "title": "56  Modello di Rasch",
    "section": "56.6 Il Modello di Rasch per Risposte Binarie",
    "text": "56.6 Il Modello di Rasch per Risposte Binarie\nDopo aver introdotto il modello di Rasch in termini intuitivi, esploriamo in modo più formale il legame tra l’abilità latente di un individuo, indicata con \\(\\theta\\), e le risposte osservate. Il modello di Rasch postula che un incremento nell’abilità latente \\(\\theta_p\\) di una persona \\(p\\) elevi la probabilità di rispondere correttamente all’item \\(i\\). Questo rapporto è descritto attraverso la funzione logistica, impiegata anche nella regressione logistica:\n\\[\n\\text{Pr}(U_{i} = 1 \\mid \\theta) = \\frac{e^z}{1 + e^z},\n\\]\ndove \\(\\text{Pr}(U_{i} = 1 \\mid \\theta)\\) denota la probabilità che la risposta all’item \\(i\\) sia corretta, data l’abilità latente \\(\\theta\\). La variabile \\(z\\) in questa equazione è definita come:\n\\[z = \\theta_p - \\beta_i\\]\nQuesto valore rappresenta la distanza tra la posizione latente della persona \\(p\\), espressa come \\(\\theta_p\\), e quella dell’item \\(i\\), \\(\\beta_i\\). La definizione di \\(z\\) in questi termini permette al modello di Rasch di calcolare la probabilità di una risposta corretta all’item \\(i\\) basandosi sulla posizione relativa sul continuum di abilità tra la persona \\(p\\) e l’item \\(i\\).\nInserendo \\(\\theta_p - \\beta_i\\) al posto di \\(z\\) nella funzione logistica, otteniamo l’espressione seguente:\n\\[\n\\text{Pr}(U_{pi} = 1 \\mid \\theta_p, \\beta_i) = \\frac{\\exp(\\theta_p - \\beta_i)}{1 + \\exp(\\theta_p - \\beta_i)} = \\frac{1}{1 + \\exp(-(\\theta_p - \\beta_i))}.\n\\] (eq-rasch-model)\nQuesta formula mostra come la probabilità che una persona con un certo livello di abilità \\(\\theta_p\\) risponda correttamente all’item \\(i\\) sia influenzata dalla differenza tra l’abilità del rispondente \\(\\theta_p\\) e la difficoltà dell’item \\(\\beta_i\\). Maggiore è questa differenza a favore dell’abilità, più alta sarà la probabilità di successo. In termini semplici, l’equazione afferma che la probabilità di una risposta corretta è funzione della distanza tra la posizione dell’individuo \\(p\\) (\\(\\theta_p\\)) e quella dell’item \\(i\\) (\\(\\beta_i\\)). L’equazione trasforma questa distanza, che potrebbe teoricamente variare da \\(-\\infty\\) a \\(+\\infty\\), in una probabilità confinata nell’intervallo [0, 1].\nÈ importante sottolineare che, sebbene teoricamente le posizioni degli item (\\(\\beta_i\\)) e delle persone (\\(\\theta_p\\)) possano estendersi da \\(-\\infty\\) a \\(+\\infty\\), nella pratica esse si collocano generalmente tra -3 e +3. Nel contesto di test di competenza, queste posizioni rappresentano i vari livelli di difficoltà: item con valori inferiori a 0.0 sono considerati “facili” (ad esempio, -2.0), mentre quelli con valori superiori a 0.0 sono “difficili” (esempio, +2.0). Gli item intorno allo 0.0 sono di difficoltà intermedia e, mentre gli item “facili” sono spesso risolti correttamente anche da persone meno abili, quelli “difficili” tendono a essere superati solo da individui più competenti.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/02_rasch_model.html#rappresentazione-alternativa-della-funzione-logistica",
    "href": "chapters/irt/02_rasch_model.html#rappresentazione-alternativa-della-funzione-logistica",
    "title": "56  Modello di Rasch",
    "section": "56.7 Rappresentazione Alternativa della Funzione Logistica",
    "text": "56.7 Rappresentazione Alternativa della Funzione Logistica\nLa funzione logistica utilizzata nel modello di Rasch può essere scritta in due modi: con la funzione esponenziale sia al numeratore sia al denominatore (a sinistra), oppure equivalentemente con la funzione esponenziale solo al denominatore, seguita dal suo argomento negativo (a destra):\n\\[\n\\frac{\\exp(\\theta_p - \\beta_i)}{1 + \\exp(\\theta_p - \\beta_i)} = \\frac{1}{1 + \\exp(-(\\theta_p - \\beta_i))}\n\\]\nPer dimostrare l’equivalenza delle due espressioni della funzione logistica nel modello di Rasch, seguiamo i seguenti passaggi algebrici. Per semplificare il lato destro, utilizziamo la proprietà dell’esponenziale che afferma $ e^{-x} = $. Quindi, riscriviamo $ (-(_p - _i)) $ come $ $:\n\\[ \\frac{1}{1 + \\frac{1}{\\exp(\\theta_p - \\beta_i)}} \\]\nIl denominatore del lato destro diventa $ 1 + $. Per combinare i termini nel denominatore, otteniamo un denominatore comune:\n\\[ \\frac{1}{\\frac{\\exp(\\theta_p - \\beta_i) + 1}{\\exp(\\theta_p - \\beta_i)}} \\]\nSimplificando ulteriormente, il denominatore diventa $ (_p - _i) + 1 $, quindi l’intera espressione diventa:\n\\[ \\frac{1}{\\exp(\\theta_p - \\beta_i) + 1} \\]\nPossiamo ora invertire la frazione per ottenere il lato sinistro dell’equazione originale:\n\\[ \\frac{\\exp(\\theta_p - \\beta_i)}{\\exp(\\theta_p - \\beta_i) + 1} \\]\nQuindi, abbiamo dimostrato che il lato sinistro e il lato destro dell’equazione originale sono effettivamente equivalenti.\n\n56.7.1 Esempio Computazionale\nPer illustrare come il modello di Rasch venga utilizzato per calcolare i punti su una curva caratteristica dell’item, consideriamo il seguente problema esemplificativo. I valori dei parametri dell’item sono:\n\na = 1 è il parametro di discriminazione dell’item,\nb = -0.5 è il parametro di difficoltà dell’item.\n\nTroviamo la probabilità di rispondere correttamente a questo item al livello di abilità theta = 1.5.\n\nicc &lt;- function(a, b, theta) {\n    1 / (1 + exp(-a * (theta - b)))\n}\n\na = 1\nb = -0.5\ntheta = 1.5\nicc(a, b, theta)\n\n0.880797077977882\n\n\n\ntheta_range &lt;- seq(-3, 3, .1)\nplot(theta_range, icc(a, b, theta_range),\n    type = \"l\", xlim = c(-3, 3), ylim = c(0, 1),\n    xlab = \"Ability\", ylab = \"Probability of Correct Response\"\n)\npoints(theta, icc(a, b, theta), cex=2)\nsegments(-3, icc(a, b, theta), theta, icc(a, b, theta), lty = \"dashed\")\nsegments(theta, icc(a, b, theta), theta, 0, lty = \"dashed\")",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/02_rasch_model.html#la-funzione-logistica-e-la-proprietà-di-oggettività-specifica-nel-modello-di-rasch",
    "href": "chapters/irt/02_rasch_model.html#la-funzione-logistica-e-la-proprietà-di-oggettività-specifica-nel-modello-di-rasch",
    "title": "56  Modello di Rasch",
    "section": "56.8 La Funzione Logistica e la Proprietà di Oggettività Specifica nel Modello di Rasch",
    "text": "56.8 La Funzione Logistica e la Proprietà di Oggettività Specifica nel Modello di Rasch\nUna delle caratteristiche fondamentali del modello di Rasch è la proprietà di oggettività specifica, che afferma che la differenza tra il logit della probabilità di rispondere correttamente a due diversi item, \\(i\\) e \\(j\\), è costante per ogni livello di abilità \\(\\theta\\). Questo implica che, indipendentemente dall’abilità di un rispondente, il confronto tra due item rimane stabile, riflettendo un principio fondamentale di misurazione oggettiva nel modello di Rasch.\n\n56.8.1 La Funzione Logistica nel Modello di Rasch\nNel modello di Rasch, la funzione logistica trasforma i logit — i logaritmi delle quote di probabilità di una risposta corretta rispetto a una errata — in probabilità effettive. I logit sono definiti come \\(\\theta_p - \\beta_i\\), dove \\(\\theta_p\\) rappresenta il livello di abilità di una persona e \\(\\beta_i\\) la difficoltà dell’item. Questa trasformazione è cruciale per interpretare i dati raccolti nei test.\nMatematicamente, il logit della probabilità di una risposta corretta a un item è espresso dalla seguente formula:\n\\[\n\\log \\left( \\frac{\\text{Pr}(U_{pi} = 1 | \\theta_p, \\beta_i)}{\\text{Pr}(U_{pi} = 0 | \\theta_p, \\beta_i)} \\right) = \\theta_p - \\beta_i.\n\\]\nLa probabilità \\(\\pi\\) di risposta corretta è calcolata così:\n\\[\n\\pi = \\frac{\\exp(\\theta_p - \\beta_i)}{1 + \\exp(\\theta_p - \\beta_i)},\n\\]\ne il suo complemento, \\(1 - \\pi\\), è dato da:\n\\[\n1 - \\pi = \\frac{1}{1 + \\exp(\\theta_p - \\beta_i)}.\n\\]\nDefinendo le quote \\(O\\) come \\(\\exp(\\theta_p - \\beta_i)\\), otteniamo una forma semplice per il logaritmo delle quote:\n\\[\n\\log(O) = \\theta_p - \\beta_i.\n\\]\nQuesta formula dimostra che i logit sono direttamente proporzionali alla differenza tra l’abilità del rispondente e la difficoltà dell’item. Un incremento in questa differenza aumenta la probabilità di una risposta corretta, riflettendo che valori più alti sui logit indicano un vantaggio dell’abilità rispetto alla difficoltà dell’item.\n\n\n56.8.2 Implicazioni della Proprietà di Oggettività Specifica\nL’oggettività specifica nel modello di Rasch significa che il confronto tra due item è indipendente dall’abilità dei rispondenti. Nella pratica, ciò si traduce nel fatto che le curve caratteristiche degli item (ICC) per diversi item sono parallele lungo la scala dei logit, poiché la differenza \\(\\theta_p - \\beta_i\\) è costante tra gli item. Le curve per item con diverse difficoltà si intersecano a livelli diversi sull’asse verticale ma mantengono una pendenza costante, illustrando che ogni differenza nelle probabilità di risposta tra due item è esclusivamente attribuibile alle loro difficoltà intrinseche e non varia con il cambiamento dell’abilità dei partecipanti.\nIn sintesi, la rappresentazione logit fornisce non solo un modo per calcolare la probabilità di una risposta corretta, ma anche un metodo robusto per mantenere la comparabilità e la coerenza delle misure tra diversi item, garantendo così una valutazione equa e precisa delle abilità del rispondente.\n\n# Creazione di un dataframe con i valori di abilità (theta_p) e le difficoltà degli item (beta)\ntheta_p &lt;- seq(-3, 3, length.out = 100)\nbeta_i &lt;- -1\nbeta_j &lt;- 1\n\n# Calcolo dei logit per gli item i e j\nlogit_i &lt;- theta_p - beta_i\nlogit_j &lt;- theta_p - beta_j\n\ndata &lt;- data.frame(\n    Ability = c(theta_p, theta_p),\n    Logit = c(logit_i, logit_j),\n    Item = factor(c(rep(\"Item i (beta_i = -1)\", length(theta_p)), rep(\"Item j (beta_j = 1)\", length(theta_p))))\n)\n\nggplot(data, aes(x = Ability, y = Logit, color = Item)) +\n    geom_line() +\n    scale_color_manual(values = c(\"blue\", \"red\")) +\n    ggtitle(TeX(\"Parallel Lines for Item i and j in the Rasch Model\")) +\n    xlab(TeX(\"Ability ($\\\\theta_p$)\")) +\n    ylab(TeX(\"Logit Probability\"))\n\n\n\n\n\n\n\n\n(sec-rasch-prob-risp-errata)= ## Probabilità di una Risposta Errata\nIl modello di Rasch non ammette il credito parziale. Pertanto, $ U_{pi} $ è o zero o uno per ogni persona e item del test, e le probabilità di questi due possibili esiti devono sommarsi a uno, il che significa\n\\[ \\text{Pr}(U_{pi} = 1 | \\theta_p, \\beta_i) + \\text{Pr}(U_{pi} = 0 | \\theta_p, \\beta_i) = 1. \\]\nRisolvendo per $ (U_{pi} = 0 | _p, _i) $, otteniamo\n\\[ \\text{Pr}(U_{pi} = 0 | \\theta_p, \\beta_i) = 1 - \\text{Pr}(U_{pi} = 1 | \\theta_p, \\beta_i) = 1 - \\frac{\\exp(\\theta_p - \\beta_i)}{1 + \\exp(\\theta_p - \\beta_i)} \\]\ndopo aver sostituito l’eq. {eq}eq-rasch-logit per $ (U_{pi} = 1 | _p, _i) $. Possiamo semplificare l’ultima espressione espandendo il 1 per ottenere\n\\[ \\text{Pr}(U_{pi} = 0 | \\theta_p, \\beta_i) = \\frac{1 + \\exp(\\theta_p - \\beta_i) - \\exp(\\theta_p - \\beta_i)}{1 + \\exp(\\theta_p - \\beta_i)} = \\frac{1}{1 + \\exp(\\theta_p - \\beta_i)}. \\]",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/02_rasch_model.html#il-modello-di-rasch-e-lanalisi-fattoriale",
    "href": "chapters/irt/02_rasch_model.html#il-modello-di-rasch-e-lanalisi-fattoriale",
    "title": "56  Modello di Rasch",
    "section": "56.9 Il Modello di Rasch e l’Analisi Fattoriale",
    "text": "56.9 Il Modello di Rasch e l’Analisi Fattoriale\nPer una migliore comprensione del Modello di Rasch, è utile confrontarlo con l’Analisi Fattoriale. Nonostante le loro differenze come metodologie statistiche, entrambi cercano un obiettivo comune: comprendere le dimensioni latenti sottostanti alle risposte osservate nei dati. Questo parallelo aiuta a mettere in luce le somiglianze e le differenze tra i due approcci.\nNell’Analisi Fattoriale, il modello tipico è espresso come \\(Y_i = \\lambda_i \\xi + \\delta_i\\), dove \\(Y_i\\) è il punteggio osservato per l’item i-esimo, \\(\\lambda_i\\) rappresenta la saturazione fattoriale che indica quanto l’item è influenzato dal fattore latente \\(\\xi\\), e \\(\\delta_i\\) è il termine di errore specifico per quell’item. L’idea centrale è che, controllando per \\(\\xi\\), le correlazioni tra gli item \\(Y_i\\) diventano nulle, poiché qualsiasi associazione comune è spiegata dal fattore latente.\nD’altra parte, il Modello di Rasch adotta un approccio leggermente diverso, ma con lo stesso obiettivo fondamentale: identificare e gestire l’influenza di una dimensione latente (spesso chiamata abilità) sulle risposte agli item. In questo contesto, si considerano risposte dicotomiche (0 o 1), e si presume che la probabilità di una risposta corretta a un item sia una funzione logistica dell’abilità del rispondente \\(\\theta\\) e della difficoltà dell’item \\(\\delta_i\\).\nLa principale differenza tra il Modello di Rasch e l’Analisi Fattoriale risiede nella formulazione dei parametri. Mentre l’Analisi Fattoriale stima le saturazioni fattoriali per ciascun item, il Modello di Rasch assume che tutti gli item abbiano lo stesso potere discriminante, cioè sono ugualmente efficaci nel distinguere tra rispondenti con diversi livelli di abilità. Invece di concentrarsi sulle saturazioni fattoriali, il Modello di Rasch si concentra sulla stima dell’abilità dei rispondenti \\(\\theta\\) e sulla difficoltà degli item \\(\\delta_i\\), presupponendo che gli item siano equivalenti in termini di discriminazione.\nIn conclusione, sia il Modello di Rasch che l’Analisi Fattoriale mirano a isolare e controllare l’effetto di una dimensione latente sull’associazione tra gli item, cercando di spiegare le risposte osservate attraverso questa dimensione. Mentre l’Analisi Fattoriale si concentra sulla stima delle saturazioni fattoriali e sull’identificazione di fattori latenti comuni tra gli item, il Modello di Rasch si focalizza sulla stima dell’abilità dei rispondenti e sulla difficoltà degli item, fornendo un quadro dettagliato delle dinamiche che influenzano le risposte agli item.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/02_rasch_model.html#considerazioni-conclusive",
    "href": "chapters/irt/02_rasch_model.html#considerazioni-conclusive",
    "title": "56  Modello di Rasch",
    "section": "56.10 Considerazioni Conclusive",
    "text": "56.10 Considerazioni Conclusive\nIl modello di Rasch si distingue notevolmente dalla Teoria Classica dei Test (CTT) per diversi aspetti fondamentali. In primo luogo, il modello di Rasch consente un’analisi dettagliata sia a livello di singolo item sia per l’intero strumento di misurazione. Questo contrasta con la CTT, che si concentra principalmente sull’analisi aggregata dell’intero test, esprimendo il punteggio totale con la formula \\(X = T + E\\), dove \\(T\\) indica l’abilità vera e \\(E\\) l’errore. Il modello di Rasch, invece, modella la probabilità di una risposta corretta per ogni item specifico, seguendo l’approccio focalizzato sugli item proposto da Guttman.\nL’approccio del modello di Rasch rappresenta una deviazione significativa dalla CTT poiché analizza le risposte osservate piuttosto che semplicemente sommarle. Questo cambio di paradigma offre numerosi vantaggi rispetto alla CTT:\nPrecisione e Dettaglio: L’analisi item per item nel modello di Rasch fornisce una comprensione molto più fine di come ciascun item funziona individualmente, permettendo di identificare specifiche aree di forza o debolezza sia nei test che nei rispondenti. Questo livello di dettaglio contribuisce a una misurazione più accurata e a una riduzione degli errori di misurazione.\nSeparazione tra Attributi della Persona e Caratteristiche dell’Item: Un vantaggio chiave del modello di Rasch rispetto alla CTT è la netta distinzione tra l’abilità della persona (\\(\\theta_p\\)) e la difficoltà dell’item (\\(b_i\\)). Nel modello di Rasch, la difficoltà degli item è considerata una proprietà costante, indipendente dal gruppo di rispondenti, il che aumenta notevolmente la precisione e la flessibilità nella misurazione.\nFlessibilità nei Modelli di Risposta: Il modello di Rasch può adattarsi a diversi tipi di formati di domanda, inclusi quelli a scelta multipla, le scale Likert, e le domande aperte, permettendo un’analisi complessiva che cattura meglio la cognizione e il comportamento umano.\nValutazione Adattiva: L’IRT, e in particolare il modello di Rasch, permette valutazioni personalizzate basate sul livello di abilità dei rispondenti. Questo approccio riduce gli errori di misurazione, fornendo informazioni più precise e utili.\nAnalisi Approfondita degli Item: Il modello di Rasch consente un’analisi dettagliata degli item, valutando aspetti come la discriminazione, la difficoltà e i parametri di indovinamento. Queste informazioni sono essenziali per il miglioramento continuo degli item e dei test.\nSebbene il modello di Rasch sia ampiamente apprezzato per la sua eleganza matematica e la sua applicazione pratica, è anche oggetto di critiche per le sue assunzioni relativamente restrittive, che alcuni studiosi ritengono limitino la sua capacità di catturare la complessità delle risposte agli item in scenari reali. Tuttavia, le sue assunzioni di base fornicono un quadro robusto per l’analisi dei dati, sostenendo l’invarianza delle proprietà degli item e delle abilità delle persone attraverso diversi contesti e gruppi di rispondenti.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/02_rasch_model.html#session-info",
    "href": "chapters/irt/02_rasch_model.html#session-info",
    "title": "56  Modello di Rasch",
    "section": "56.11 Session Info",
    "text": "56.11 Session Info\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] grid      stats4    stats     graphics  grDevices utils     datasets \n[8] methods   base     \n\nother attached packages:\n [1] ggmirt_0.1.0      latex2exp_0.9.6   TAM_4.2-21        CDM_8.2-6        \n [5] mvtnorm_1.3-1     ltm_1.2-0         polycor_0.8-1     msm_1.8          \n [9] MASS_7.3-61       mirt_1.42         lattice_0.22-6    ggokabeito_0.1.0 \n[13] viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1   \n[17] bayesplot_1.11.1  gridExtra_2.3     patchwork_1.3.0   semTools_0.5-6   \n[21] semPlot_1.1.6     lavaan_0.6-18     psych_2.4.6.26    scales_1.3.0     \n[25] markdown_1.13     knitr_1.48        lubridate_1.9.3   forcats_1.0.0    \n[29] stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2       readr_2.1.5      \n[33] tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0  \n[37] here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] splines_4.4.1        later_1.3.2          pbdZMQ_0.3-13       \n  [4] R.oo_1.26.0          XML_3.99-0.17        rpart_4.1.23        \n  [7] lifecycle_1.0.4      rstatix_0.7.2        rprojroot_2.0.4     \n [10] globals_0.16.3       rockchalk_1.8.157    backports_1.5.0     \n [13] magrittr_2.0.3       openxlsx_4.2.7.1     Hmisc_5.1-3         \n [16] rmarkdown_2.28       httpuv_1.6.15        qgraph_1.9.8        \n [19] zip_2.3.1            sessioninfo_1.2.2    pbapply_1.7-2       \n [22] minqa_1.2.8          multcomp_1.4-26      abind_1.4-8         \n [25] audio_0.1-11         expm_1.0-0           quadprog_1.5-8      \n [28] R.utils_2.12.3       nnet_7.3-19          TH.data_1.1-2       \n [31] sandwich_3.1-1       listenv_0.9.1        testthat_3.2.1.1    \n [34] RPushbullet_0.3.4    vegan_2.6-8          arm_1.14-4          \n [37] parallelly_1.38.0    permute_0.9-7        codetools_0.2-20    \n [40] tidyselect_1.2.1     farver_2.1.2         lme4_1.1-35.5       \n [43] base64enc_0.1-3      jsonlite_1.8.9       progressr_0.14.0    \n [46] Formula_1.2-5        survival_3.7-0       emmeans_1.10.4      \n [49] tools_4.4.1          rio_1.2.2            snow_0.4-4          \n [52] Rcpp_1.0.13          glue_1.7.0           mnormt_2.1.1        \n [55] admisc_0.36          xfun_0.47            mgcv_1.9-1          \n [58] IRdisplay_1.1        withr_3.0.1          beepr_2.0           \n [61] fastmap_1.2.0        boot_1.3-31          fansi_1.0.6         \n [64] digest_0.6.37        mi_1.1               timechange_0.3.0    \n [67] R6_2.5.1             mime_0.12            estimability_1.5.1  \n [70] colorspace_2.1-1     gtools_3.9.5         jpeg_0.1-10         \n [73] R.methodsS3_1.8.2    utf8_1.2.4           generics_0.1.3      \n [76] data.table_1.16.0    corpcor_1.6.10       SimDesign_2.17.1    \n [79] htmlwidgets_1.6.4    pkgconfig_2.0.3      sem_3.1-16          \n [82] gtable_0.3.5         brio_1.1.5           htmltools_0.5.8.1   \n [85] carData_3.0-5        png_0.1-8            rstudioapi_0.16.0   \n [88] tzdb_0.4.0           reshape2_1.4.4       uuid_1.2-1          \n [91] coda_0.19-4.1        checkmate_2.3.2      nlme_3.1-166        \n [94] curl_5.2.3           nloptr_2.1.1         repr_1.1.7          \n [97] zoo_1.8-12           parallel_4.4.1       miniUI_0.1.1.1      \n[100] foreign_0.8-87       pillar_1.9.0         vctrs_0.6.5         \n[103] promises_1.3.0       car_3.1-2            OpenMx_2.21.12      \n[106] xtable_1.8-4         Deriv_4.1.6          cluster_2.1.6       \n[109] dcurver_0.9.2        GPArotation_2024.3-1 htmlTable_2.4.3     \n[112] evaluate_1.0.0       pbivnorm_0.6.0       cli_3.6.3           \n[115] kutils_1.73          compiler_4.4.1       rlang_1.1.4         \n[118] crayon_1.5.3         future.apply_1.11.2  ggsignif_0.6.4      \n[121] labeling_0.4.3       fdrtool_1.2.18       plyr_1.8.9          \n[124] stringi_1.8.4        munsell_0.5.1        lisrelToR_0.3       \n[127] pacman_0.5.1         Matrix_1.7-0         IRkernel_1.3.2      \n[130] hms_1.1.3            glasso_1.11          future_1.34.0       \n[133] shiny_1.9.1          igraph_2.0.3         broom_1.0.6         \n[136] RcppParallel_5.1.9  \n\n\n\n\n\n\nDebelak, Rudolf, Carolin Strobl, e Matthew D Zeigenfuse. 2022. An introduction to the rasch model with examples in r. Crc Press.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/03_assumptions.html",
    "href": "chapters/irt/03_assumptions.html",
    "title": "57  Assunzioni e Proprietà del Modello di Rasch",
    "section": "",
    "text": "57.1 Introduzione\nIn questo capitolo, esaminiamo le proprietà importanti del modello di Rasch. Queste proprietà sono il motivo per cui il modello di Rasch è così teoricamente attraente e hanno portato al suo ampio utilizzo. Forse la proprietà più importante è il fatto che permette una misurazione oggettiva dei tratti latenti.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Assunzioni e Proprietà del Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/03_assumptions.html#statistiche-sufficienti-definizione-e-applicazioni",
    "href": "chapters/irt/03_assumptions.html#statistiche-sufficienti-definizione-e-applicazioni",
    "title": "57  Assunzioni e Proprietà del Modello di Rasch",
    "section": "57.2 Statistiche Sufficienti: Definizione e Applicazioni",
    "text": "57.2 Statistiche Sufficienti: Definizione e Applicazioni\nUna statistica è definita come una funzione dei dati osservati e serve comunemente a riassumere caratteristiche salienti di un insieme di dati. Un esempio chiaro è la media campionaria, calcolata come:\n\\[ \\bar{x} = \\frac{1}{P} \\sum_{p=1}^{P} x_p, \\]\ndove \\(\\bar{x}\\) rappresenta la media dei valori \\(x_p\\) per individui da 1 a \\(P\\). Questa statistica è frequentemente impiegata per stimare il valore atteso di una popolazione, dato che sintetizza le informazioni riguardanti la media del campione.\nEsistono altre statistiche oltre a \\(\\bar{x}\\), ad esempio, si potrebbe utilizzare la media di soli alcuni valori selezionati, come \\(x^* = \\frac{1}{3} (x_1 + x_3 + x_5)\\). Tuttavia, questa alternativa, nonostante sia un valido stimatore, risulta generalmente meno accurata di \\(\\bar{x}\\) perché non considera tutti i valori del campione, come \\(x_2, x_4\\), ecc.\nIl concetto di statistica sufficiente emerge quando una statistica, come \\(\\bar{x}\\), cattura completamente tutte le informazioni necessarie sul parametro di interesse (in questo caso, la media) contenute nel campione. Le statistiche sufficienti sono particolarmente potenti perché, una volta conosciute, rendono superflue le informazioni aggiuntive fornite dai dati individuali.\n\n57.2.1 Applicazioni nel Modello di Rasch\nNel contesto del modello di Rasch, utilizzato in teoria della risposta agli item (IRT), si identificano statistiche sufficienti specifiche per diversi parametri:\n\nStatistica Sufficiente per \\(\\theta_p\\): Per il parametro di abilità di una persona \\(\\theta_p\\), la statistica sufficiente è il punteggio totale \\(r_p\\), ottenuto sommando tutte le risposte corrette della persona \\(p\\) ai vari item. Questo punteggio riflette direttamente l’abilità della persona senza necessità di analizzare le risposte a ogni singolo item.\nStatistica Sufficiente per \\(\\beta_i\\): Analogamente, per il parametro di difficoltà dell’item \\(\\beta_i\\), la statistica sufficiente è data dal totale delle risposte corrette \\(c_i\\) a quell’item da parte di tutte le persone. Questa somma rappresenta la difficoltà intrinseca dell’item.\n\nSecondo il modello di Rasch, la probabilità che una persona \\(p\\) risponda correttamente all’item \\(i\\) è modellata tramite una funzione logistica della differenza tra abilità e difficoltà:\n\\[\nP(Y_{pi} = 1 | \\theta_p, \\beta_i) = \\frac{e^{(\\theta_p - \\beta_i)}}{1 + e^{(\\theta_p - \\beta_i)}}.\n\\]\nIn questo modello, il punteggio totale \\(r_p\\) e il numero totale di risposte corrette \\(c_i\\) diventano essenziali, poiché la distribuzione congiunta delle risposte corrette può essere completamente descritta da questi totali. Inoltre, l’indipendenza condizionale delle risposte, data l’abilità di una persona o la difficoltà di un item, supporta l’utilizzo dei punteggi totali come statistiche sufficienti.\nQueste proprietà del modello di Rasch evidenziano come le statistiche sufficienti, quali \\(r_p\\) e \\(c_i\\), incorporino efficacemente tutte le informazioni necessarie per stimare i parametri di interesse, semplificando così il processo di analisi statistica.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Assunzioni e Proprietà del Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/03_assumptions.html#considerazioni-conclusive-sul-modello-di-rasch",
    "href": "chapters/irt/03_assumptions.html#considerazioni-conclusive-sul-modello-di-rasch",
    "title": "57  Assunzioni e Proprietà del Modello di Rasch",
    "section": "57.3 Considerazioni Conclusive sul Modello di Rasch",
    "text": "57.3 Considerazioni Conclusive sul Modello di Rasch\nIl modello di Rasch si basa su tre fondamentali assunzioni che ne garantiscono la validità e l’applicabilità: unidimensionalità, monotonicità e indipendenza locale.\n\nUnidimensionalità: Quest’assunzione presuppone che le risposte agli item di un test siano influenzate principalmente da un unico tratto latente o dimensione attributiva. Questo significa che il modello assume la predominanza di una sola caratteristica o abilità nel determinare le risposte, rendendo problematica la presenza di dimensioni multiple che richiederebbero un’analisi in uno spazio multidimensionale.\nMonotonicità: L’assunzione di monotonicità stabilisce che con l’incremento del tratto latente (\\(\\theta\\)), aumenta anche la probabilità di una risposta corretta. Ciò si allinea con l’intuizione generale nella misurazione: individui con un livello più elevato del tratto latente tendono a ottenere punteggi migliori nei test.\nIndipendenza Locale: Questa assunzione afferma che, controllato il tratto latente, non dovrebbero esistere correlazioni tra le risposte a due item distinti. Eventuali associazioni osservate tra risposte a item diversi sono attribuibili unicamente al tratto latente. In altre parole, la risposta a un item non deve essere influenzata da o influenzare la risposta a un altro item, una volta controllato per il tratto latente.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Assunzioni e Proprietà del Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/03_assumptions.html#indipendenza-stocastica-locale-e-applicazioni-nel-modello-di-rasch",
    "href": "chapters/irt/03_assumptions.html#indipendenza-stocastica-locale-e-applicazioni-nel-modello-di-rasch",
    "title": "57  Assunzioni e Proprietà del Modello di Rasch",
    "section": "57.4 Indipendenza Stocastica Locale e Applicazioni nel Modello di Rasch",
    "text": "57.4 Indipendenza Stocastica Locale e Applicazioni nel Modello di Rasch\n\n57.4.1 Concetto di Indipendenza Stocastica Locale\nIn molti modelli statistici si presuppone che gli eventi siano stocasticamente indipendenti, ovvero che la realizzazione di un evento non fornisca informazioni sulla realizzazione degli altri. Questa assunzione di indipendenza stocastica è fondamentale perché semplifica notevolmente il calcolo delle probabilità congiunte. Ad esempio, consideriamo il lancio ripetuto di una moneta equilibrata: la probabilità di ottenere testa in ciascun lancio è del 50%, e la probabilità congiunta di ottenere testa due volte consecutive è data dal prodotto delle probabilità individuali:\n\\[ \\text{Pr}(testa, testa) = 0.5 \\times 0.5 = 0.25. \\]\nQuesta formula utilizza l’indipendenza stocastica per calcolare la probabilità congiunta, ovvero sapere l’esito del primo lancio non cambia la probabilità del secondo.\n\n\n57.4.2 Indipendenza nel Modello di Rasch\nIl modello di Rasch, utilizzato nella teoria della risposta agli item (IRT), si basa sull’assunzione di indipendenza stocastica locale per calcolare la probabilità congiunta delle risposte a un test. Assumendo che le risposte di una persona agli item di un test siano indipendenti data la sua abilità, possiamo calcolare la probabilità congiunta moltiplicando le probabilità individuali di risposta corretta ai singoli item:\n\\[\n\\text{Pr}(U_{p1} = u_{p1}, U_{p2} = u_{p2} | \\theta_p, \\beta_1, \\beta_2) = \\text{Pr}(U_{p1} = u_{p1} | \\theta_p, \\beta_1) \\times \\text{Pr}(U_{p2} = u_{p2} | \\theta_p, \\beta_2).\n\\]\n\n\n57.4.3 Uso dei Vettori per Semplificare il Modello\nNel modello di Rasch, l’uso dei vettori di risposta e dei parametri di difficoltà degli item facilita il calcolo delle probabilità congiunte per un test di più item. Ad esempio, definiamo \\(\\beta = (\\beta_1, ..., \\beta_I)\\) come il vettore dei parametri di difficoltà e \\(U_{p\\cdot} = (U_{p1}, ..., U_{pI})\\) come il vettore casuale delle risposte della persona \\(p\\). La probabilità congiunta delle risposte si esprime come:\n\\[\n\\text{Pr}(U_{p\\cdot} = u_{p\\cdot} | \\theta_p, \\beta) = \\prod_{i=1}^{I} \\text{Pr}(U_{pi} = u_{pi} | \\theta_p, \\beta_i).\n\\]\nQuesta formulazione permette di calcolare efficientemente la probabilità congiunta di un pattern di risposte utilizzando le proprietà dei prodotti e delle esponenziali:\n\\[\n\\text{Pr}(U_{p\\cdot} = u_{p\\cdot} \\mid \\theta_p, \\beta) = \\frac{\\exp\\{\\theta_p r_p - \\sum_{i=1}^{I} u_{pi} \\beta_i\\}}{\\prod_{i=1}^{I} [1 + \\exp(\\theta_p - \\beta_i)]},\n\\]\ndove \\(r_p = \\sum_{i=1}^{I} u_{pi}\\) rappresenta il numero totale di item risolti correttamente dalla persona \\(p\\).\n\n\n57.4.4 Limitazioni e Considerazioni\nNonostante l’utilità dell’indipendenza stocastica locale, ci sono situazioni in cui questa assunzione può non essere valida, come nei test di matematica dove la soluzione di un problema dipende da quella di problemi precedenti, o nei testlet, dove gli item condividono un tema comune e le risposte possono essere correlate. In questi casi, la probabilità di risposta corretta agli item successivi può dipendere dalle risposte date agli item precedenti, violando l’assunzione di indipendenza stocastica locale.\nIn sintesi, l’indipendenza stocastica locale è un principio potente nel modello di Rasch che facilita il calcolo delle probabilità congiunte nelle valutazioni standardizzate, ma è essenziale riconoscere i suoi limiti e applicarla in modo appropriato a seconda del contesto specifico del test.\n\n\n57.4.5 Estensione della Probabilità Congiunta: Dalle Singole Risposte alla Collettività\n\n57.4.5.1 Probabilità Congiunta delle Risposte di Tutti i Partecipanti\nPossiamo estendere il concetto di probabilità congiunta da una singola persona a tutti i partecipanti di un test. Questo si realizza assumendo che le risposte di tutti i partecipanti siano stocasticamente indipendenti l’una dall’altra. A livello notazionale, raggruppiamo i parametri di abilità dei partecipanti, \\(\\theta_p\\), in un vettore \\(\\theta\\), dove ogni elemento corrisponde al parametro di abilità di un individuo. Analogamente, formiamo una matrice casuale \\(U\\), dove ogni riga rappresenta le risposte di un singolo partecipante e ogni colonna si riferisce alle risposte a un singolo item.\n\n\n57.4.5.2 Matrice delle Risposte e Probabilità Congiunta\nLe risposte effettive di ciascun partecipante sono raccolte in una matrice \\(u\\). La cella nella riga \\(p\\) e colonna \\(i\\) contiene la risposta \\(u_{pi}\\), ovvero la risposta del partecipante \\(p\\) all’item \\(i\\). Utilizzando queste matrici, la probabilità congiunta delle risposte di tutti i partecipanti ai vari item è data da:\n\\[\n\\text{Pr}(U = u | \\theta, \\beta) = \\prod_{p=1}^{P} \\frac{\\exp\\{r_p \\cdot \\theta_p - \\sum_{i=1}^{I} u_{pi} \\cdot \\beta_i\\}}{\\prod_{i=1}^{I} [1 + \\exp(\\theta_p - \\beta_i)]},\n\\]\ndove \\(r_p\\) indica il punteggio totale del partecipante \\(p\\).\n\n\n57.4.5.3 Semplificazione del Calcolo\nIl calcolo può essere ulteriormente semplificato considerando la somma di tutti i termini nel numeratore e distribuendo la somma sui termini \\(\\theta_p\\) e \\(\\beta_i\\):\n\\[\n\\text{Pr}(U = u | \\theta, \\beta) = \\frac{\\exp\\{\\sum_{p=1}^{P}r_p \\cdot \\theta_p - \\sum_{p=1}^{P}\\sum_{i=1}^{I} u_{pi} \\cdot \\beta_i\\}}{\\prod_{p=1}^{P} \\prod_{i=1}^{I} [1 + \\exp(\\theta_p - \\beta_i)]}.\n\\]\nInvertendo l’ordine delle sommatorie nel doppio sommatorio, e riconoscendo che i termini \\(\\beta_i\\) sono costanti rispetto a \\(p\\), possiamo riscrivere la somma come:\n\\[\n\\sum_{i=1}^{I}\\sum_{p=1}^{P} u_{pi} \\cdot \\beta_i = \\sum_{i=1}^{I} c_i \\cdot \\beta_i,\n\\]\ndove \\(c_i\\) rappresenta la somma totale delle risposte corrette all’item \\(i\\) da parte di tutti i partecipanti. Sostituendo nella formula finale, otteniamo:\n\\[\n\\text{Pr}(U = u | \\theta, \\beta) = \\frac{\\exp\\{\\sum_{p=1}^{P} r_p \\cdot \\theta_p - \\sum_{i=1}^{I} c_i \\cdot \\beta_i\\}}{\\prod_{p=1}^{P} \\prod_{i=1}^{I} [1 + \\exp(\\theta_p - \\beta_i)]},\n\\]\nche rappresenta la probabilità congiunta desiderata.\n\n\n57.4.5.4 Considerazioni sulla Validità dell’Assunzione di Indipendenza\nBisogna valutare criticamente l’assunzione che le risposte di diversi partecipanti siano indipendenti. Per esempio, questa assunzione non è valida quando un partecipante copia le risposte da un altro. L’indipendenza locale è cruciale per applicare correttamente il modello di Rasch e per semplificare i calcoli delle probabilità congiunte.\n\n\n57.4.5.5 Implicazioni del Teorema di Andersen\nIl modello di Rasch e la sua formulazione si basano su questa indipendenza e su altre piccole assunzioni, come dimostrato dal teorema di Andersen. Verificare l’applicabilità di queste assunzioni è essenziale prima di implementare il modello in situazioni pratiche.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Assunzioni e Proprietà del Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/03_assumptions.html#unidimensionalità-nel-modello-di-rasch",
    "href": "chapters/irt/03_assumptions.html#unidimensionalità-nel-modello-di-rasch",
    "title": "57  Assunzioni e Proprietà del Modello di Rasch",
    "section": "57.5 Unidimensionalità nel Modello di Rasch",
    "text": "57.5 Unidimensionalità nel Modello di Rasch\n\n57.5.1 Concetto di Unidimensionalità\nIl modello di Rasch presuppone unidimensionalità, ovvero assume che esista una singola dimensione latente che ordina le persone secondo le loro abilità. Nel contesto dei test psicometrici, un test è considerato unidimensionale quando misura esclusivamente un’unica abilità specifica. Ad esempio, un test di matematica puro misurerebbe solo l’abilità matematica. Al contrario, test come il SAT, che valutano sia competenze matematiche che verbali, non sono unidimensionali perché ogni sezione misura una diversa dimensione di abilità.\n\n\n57.5.2 Implicazioni dell’Unidimensionalità\nUn test ideale unidimensionale assegna a ciascun partecipante un singolo valore di abilità, riflettendo precisamente la competenza nella dimensione target del test. Tuttavia, la presenza di più dimensioni latenti può causare problemi come il Funzionamento Differenziale degli Item (DIF). Il DIF si verifica quando la difficoltà di specifici item varia tra diversi gruppi di candidati, indipendentemente dalle loro abilità nella dimensione misurata dal test.\n\n\n57.5.3 Esempio di DIF e Multidimensionalità\nConsideriamo un test di matematica dove non solo la competenza matematica (la dimensione primaria) è rilevante, ma anche le abilità linguistiche (una dimensione secondaria) influenzano i risultati. Se due gruppi di candidati differiscono significativamente nelle loro abilità linguistiche, questa dimensione secondaria potrebbe distorcere i risultati del test, rendendo alcuni item più facili o difficili per un gruppo rispetto all’altro. Questo fenomeno evidenzia il DIF, dove gli item sono influenzati da fattori non direttamente pertinenti all’abilità che il test intende misurare.\n\n\n57.5.4 Rilevazione e Impatto del DIF\nI test statistici per rilevare il DIF devono considerare la possibile presenza di multidimensionalità. Un test che non isoli adeguatamente la dimensione d’interesse può portare a valutazioni errate o ingiuste dei candidati. La presenza di DIF suggerisce che il test potrebbe non essere calibrato equamente per tutti i partecipanti o che alcuni item siano parziali verso determinati gruppi.\n\n\n57.5.5 Considerazioni su Costrutti Psicologici Complessi\nMolti costrutti psicologici, come l’intelligenza, sono intrinsecamente multidimensionali. Ad esempio, il modello di intelligenza di Carroll (1993) propone una struttura gerarchica con categorie come l’intelligenza fluida e quella cristallizzata, dimostrando la complessità e la multidimensionalità del costrutto.\n\n\n57.5.6 Estensioni Multidimensionali del Modello di Rasch\nSebbene il modello di Rasch classico sia limitato alla misurazione di una singola dimensione, sono state sviluppate estensioni che possono gestire e incorporare dimensioni multiple. Queste versioni multidimensionali del modello di Rasch permettono una valutazione simultanea di più dimensioni, fornendo un’analisi più completa di costrutti psicologici complessi.\n\n\n57.5.7 Conclusione\nL’unidimensionalità è una presunzione centrale nel modello di Rasch, essenziale per la validità dei test che mirano a misurare specifiche abilità. Tuttavia, la realtà dei costrutti psicologici spesso richiede un approccio più sfumato che tenga conto delle loro nature multidimensionali. Le estensioni multidimensionali del modello di Rasch rappresentano un adattamento cruciale per l’analisi di tali costrutti complessi, migliorando la precisione e l’equità delle valutazioni psicometriche.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Assunzioni e Proprietà del Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/03_assumptions.html#scala-di-misurazione-nel-modello-di-rasch",
    "href": "chapters/irt/03_assumptions.html#scala-di-misurazione-nel-modello-di-rasch",
    "title": "57  Assunzioni e Proprietà del Modello di Rasch",
    "section": "57.6 Scala di Misurazione nel Modello di Rasch",
    "text": "57.6 Scala di Misurazione nel Modello di Rasch\n\n57.6.1 Definizione di Misurazione secondo Stevens\nSecondo Stevens (1946), la misurazione è definita come “l’assegnazione di numeri a oggetti o eventi secondo regole”. Questa definizione, ampiamente riconosciuta in psicologia, abbraccia tanto la misurazione fisica (es. l’uso di un righello per misurare una distanza) quanto quella psicologica (es. l’uso di paradigmi per valutare la percezione del volume sonoro).\n\n\n57.6.2 Classificazione delle Scale di Misurazione\nStevens identifica quattro categorie principali di scale di misurazione: 1. Scale Nominali: Numeri usati esclusivamente come etichette (es. numeri sulle maglie dei giocatori di calcio). 2. Scale Ordinali: Numeri utilizzati per stabilire un ordine (es. classifiche olimpiche). 3. Scale di Intervallo: Misurano distanze relative senza un punto zero naturale (es. gradi Celsius e Fahrenheit). 4. Scale di Rapporto: Simili alle scale di intervallo ma con un punto zero assoluto (es. lunghezza in metri o peso in chilogrammi).\n\n\n57.6.3 Applicazione nel Modello di Rasch\nNel modello di Rasch, la conversione delle misure di abilità da una unità a un’altra (da “Unità A” a “Unità B”) è analoga al cambio di unità di temperatura. Per esempio, ogni abilità \\(\\theta_p\\) può essere trasformata in \\(\\theta_p' = \\theta_p - b\\) e ogni difficoltà dell’item \\(\\beta_i\\) in \\(\\beta_i' = \\beta_i - b\\), mantenendo inalterate le probabilità di una risposta corretta.\n\n\n57.6.4 Scalabilità e Ricalibrazione\nConsideriamo un’ulteriore riscalatura da “Unità A” a “Unità C”, dove \\(\\theta\\) diventa \\(\\theta'' = (1/a) \\cdot \\theta\\) e \\(\\beta\\) diventa \\(\\beta'' = (1/a) \\cdot \\beta\\). Questa trasformazione riduce le logit di un fattore \\(1/a\\). Adattando la funzione logistica \\(f(x)\\) in modo che l’argomento sia scalato da \\(a\\), \\(f(a \\cdot x)\\), manteniamo invariate le probabilità di una risposta corretta tra le Unità A e C.\n\n\n57.6.5 Implicazioni della Scala di Misurazione\nQueste trasformazioni dimostrano che nel modello di Rasch gli item e le persone possono essere misurati su una scala di intervallo, ma il punto zero e la scala non sono intrinsecamente definiti. In pratica, questo implica la necessità di stabilire un punto zero e una scala per la funzione di risposta degli item, ad esempio fissando la difficoltà del primo item a zero, normalizzando la somma delle difficoltà a zero, o bilanciando la somma delle abilità a zero. La scala è comunemente impostata a 1 nel modello di Rasch, corrispondente alla standardizzazione della pendenza degli item.\n\n\n57.6.6 Discussione Futura\nNel prossimo capitolo dedicato alla stima dei parametri, esploreremo ulteriormente come selezionare e applicare queste convenzioni per calibrare efficacemente il modello di Rasch, garantendo che le misurazioni riflettano accuratamente le abilità e le difficoltà all’interno del contesto testato.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Assunzioni e Proprietà del Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/03_assumptions.html#oggettività-specifica-nei-test-psicometrici",
    "href": "chapters/irt/03_assumptions.html#oggettività-specifica-nei-test-psicometrici",
    "title": "57  Assunzioni e Proprietà del Modello di Rasch",
    "section": "57.7 Oggettività Specifica nei Test Psicometrici",
    "text": "57.7 Oggettività Specifica nei Test Psicometrici\n\n57.7.1 Definizione e Importanza dell’Oggettività Specifica\nL’oggettività specifica è un concetto fondamentale nei test psicometrici, volto a garantire l’equità nei confronti tra individui. Essa assicura che i confronti tra le persone siano determinati unicamente dalle loro abilità e non dagli specifici item utilizzati nel test. Questo principio implica che se un individuo ha una probabilità maggiore di rispondere correttamente a un item rispetto a un altro, allora questa superiorità dovrebbe manifestarsi uniformemente su tutti gli item del test. Analogamente, se un item risulta più facile per una persona, dovrebbe risultare più facile per chiunque altro.\n\n\n57.7.2 Verifica dell’Oggettività Specifica\nUn metodo per controllare se un modello rispetta l’oggettività specifica è osservare le Curve di Caratteristica dell’Item (ICC). Nel modello di Rasch, l’oggettività specifica è soddisfatta se le ICC per diversi item non si incrociano, indicando che le probabilità di risposta corretta sono funzioni consistenti dell’abilità, indipendentemente dagli item specifici.\n\n\n57.7.3 Formalizzazione Algebrica dell’Oggettività Specifica\nSecondo Irtel (1996), l’oggettività specifica può essere definita più rigorosamente attraverso il concetto di rapporto di probabilità. Per due persone $ p $ e $ q $ e un item $ i $, definiamo $ O_{pi} $ come la probabilità che la persona $ p $ risponda correttamente all’item $ i $. L’oggettività specifica richiede che il rapporto di probabilità tra le persone sia costante attraverso tutti gli item del test:\n\\[\n\\frac{O_{p1}}{O_{q1}} = \\dots = \\frac{O_{pI}}{O_{qI}}\n\\]\nQuesto implica che per ogni coppia di item $ i $ e $ j $, il rapporto di probabilità tra due persone per l’item $ i $ sia uguale a quello per l’item $ j $, ossia:\n\\[\n\\frac{O_{pi}}{O_{qi}} = \\frac{O_{pj}}{O_{qj}}\n\\]\nCiò equivale matematicamente a che $ O_{pi} O_{qj} = O_{pj} O_{qi} $.\n\n\n57.7.4 Applicazione Pratica: Esempio di Oggettività Specifica\nPer illustrare, consideriamo Marco e Cora che completano un test di venti item. Se le probabilità (e di conseguenza le quote) di Marco risolvere il primo item sono 1:4 e quelle di Cora sono 1:1, l’oggettività specifica impone che il rapporto di 4:1 tra le quote di Cora e Marco debba rimanere costante per tutti i 20 item del test.\n\n\n57.7.5 Limitazioni e Implicazioni\nNonostante l’oggettività specifica fornisca un criterio rigoroso per la coerenza dei test, il suo uso può essere limitato da interpretazioni errate. Ad esempio, definire l’oggettività specifica come “indipendenza del campione” può portare a conclusioni erronee riguardo alla trasferibilità di un test tra gruppi diversi, come banchieri d’investimento e ingegneri del software, che potrebbero interpretare gli item in modo differente.\n\n\n57.7.6 Considerazioni Finali\nL’oggettività specifica non è una proprietà che può essere universalmente garantita in tutti i contesti. È un’ipotesi di lavoro che deve essere continuamente verificata attraverso dati empirici. Rasch stesso ha sottolineato l’importanza di testare ripetutamente questa proprietà ogni volta che si raccolgono nuovi dati. Questa attenzione costante ai dettagli e alle variabili del contesto è cruciale per mantenere l’integrità e l’affidabilità dei test psicometrici.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Assunzioni e Proprietà del Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/03_assumptions.html#considerazioni-conclusive",
    "href": "chapters/irt/03_assumptions.html#considerazioni-conclusive",
    "title": "57  Assunzioni e Proprietà del Modello di Rasch",
    "section": "57.8 Considerazioni conclusive",
    "text": "57.8 Considerazioni conclusive\nIl modello di Rasch si basa su tre fondamentali assunzioni che ne garantiscono la validità e l’applicabilità: unidimensionalità, monotonicità e indipendenza locale. La violazione di queste assunzioni può indicare la necessità di ricorrere a metodologie più complesse o a modelli alternativi per un’analisi accurata dei dati. Questo può richiedere un esame più approfondito dei dati o l’adozione di modelli statistici avanzati capaci di gestire la complessità delle informazioni raccolte.\nUn aspetto distintivo del modello di Rasch è la sua capacità di quantificare la difficoltà degli item indipendentemente dalle abilità dei partecipanti, grazie all’uso della stima di massima verosimiglianza condizionale. Questo approccio garantisce che la difficoltà di ciascun item venga determinata basandosi unicamente sulle risposte specifiche a quell’item, in maniera indipendente dal livello complessivo di abilità dei rispondenti.\nQuesto principio di oggettività specifica è simile al concetto di invarianza in analisi di regressione, dove le caratteristiche di una linea di regressione (come pendenza e intercetta) non variano a seconda del campione analizzato. Analogamente, nel modello di Rasch, i parametri di difficoltà degli item restano costanti e non sono influenzati dalle competenze generali dei partecipanti, rendendo le valutazioni di difficoltà degli item stabili e affidabili, indipendentemente dalla varietà o dal livello di abilità del campione di rispondenti.\nL’oggettività specifica è particolarmente preziosa poiché elimina la necessità di selezionare campioni normati o rappresentativi della popolazione generale per la calibrazione degli item. Quasi qualsiasi gruppo di persone, purché con una varietà sufficiente nelle risposte, può essere utilizzato per stabilire la difficoltà degli item. Questo approccio si contrappone a quello dei test tradizionali, dove spesso è necessario un campione rappresentativo per sviluppare tabelle normative basate sulle percentuali di risposte corrette.\nIn conclusione, il modello di Rasch offre un framework robusto per la misurazione psicometrica, pur richiedendo una verifica attenta delle sue assunzioni fondamentali per assicurare risultati di misurazione accurati e equi.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Assunzioni e Proprietà del Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/04_estimation.html",
    "href": "chapters/irt/04_estimation.html",
    "title": "58  Stima",
    "section": "",
    "text": "58.1 Introduzione\nPer valutare e applicare il modello di Rasch nella ricerca pratica, è necessario essere in grado di stimare i parametri del modello basandosi su dati empirici. Presenteremo diversi approcci per stimare i parametri del modello di Rasch a partire dai dati dei test osservati. Tutti questi approcci possono essere utilizzati per stimare sia i parametri degli item sia quelli delle persone, ma lo fanno in modi diversi. Due degli approcci – la massima verosimiglianza congiunta e l’inferenza bayesiana – stimano i parametri delle persone e degli item simultaneamente. Gli altri due approcci – la massima verosimiglianza condizionale e la massima verosimiglianza marginale – li stimano separatamente, con i parametri delle persone che seguono in un secondo passaggio.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Stima</span>"
    ]
  },
  {
    "objectID": "chapters/irt/04_estimation.html#la-funzione-di-verosimiglianza",
    "href": "chapters/irt/04_estimation.html#la-funzione-di-verosimiglianza",
    "title": "58  Stima",
    "section": "58.2 La Funzione di Verosimiglianza",
    "text": "58.2 La Funzione di Verosimiglianza\nTutti i metodi per la stima dei parametri nel modello di Rasch si basano sulla funzione di verosimiglianza, che rappresenta la probabilità dei dati osservati data una serie di parametri del modello ancora sconosciuti. In particolare, nel contesto del modello di Rasch, $ U_{pi} $ indica la risposta (corretta o errata) fornita dalla persona $ p $ all’item $ i $, dove una risposta corretta è codificata come 1 e una risposta errata come 0. La verosimiglianza della risposta della persona $ p $ all’item $ i $ è espressa dalla seguente formula:\n\\[\nL_{upi}(\\theta_p, \\beta_i) = \\text{Pr}(U_{pi} = u_{pi} | \\theta_p, \\beta_i) = \\frac{\\exp\\{u_{pi} \\cdot (\\theta_p - \\beta_i)\\}}{1 + \\exp(\\theta_p - \\beta_i)}.\n\\]\nQuesta equazione calcola la probabilità che la persona $ p $ dia una risposta specifica $ u_{pi} $ all’item $ i $, in funzione del proprio livello di abilità $ _p $ e della difficoltà dell’item $ _i $.\nPer calcolare la verosimiglianza complessiva delle risposte di una persona $ p $ a tutti gli item del test (da $ i = 1 $ a $ I $), moltiplichiamo le verosimiglianze di tutte le sue risposte. La formula diventa:\n\\[\n\\begin{align}\nL_{up}(\\theta_p, \\beta) &= \\prod_{i=1}^{I} \\frac{\\exp\\{u_{pi} \\cdot (\\theta_p - \\beta_i)\\}}{1 + \\exp(\\theta_p - \\beta_i)} \\notag\\\\\n&= \\frac{\\exp(rp \\cdot \\theta_p - \\sum_{i=1}^{I} u_{pi} \\cdot \\beta_i)}{\\prod_{i=1}^{I} [1 + \\exp(\\theta_p - \\beta_i)]}.  \n\\end{align}\n\\] (eq-rasch-likelihood)\nQuesta equazione rappresenta il punto di partenza per tutti gli approcci di stima dei parametri nel modello di Rasch. Tuttavia, il metodo specifico scelto per l’analisi dei dati influenzerà come i parametri relativi alle abilità delle persone ($ _p \\() e le difficoltà degli item (\\) _i $) vengono stimati e interpretati. Ogni approccio di stima ha le proprie peculiarità e assunzioni che determinano come queste stime vengono ottenute.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Stima</span>"
    ]
  },
  {
    "objectID": "chapters/irt/04_estimation.html#stima-della-massima-verosimiglianza-congiunta",
    "href": "chapters/irt/04_estimation.html#stima-della-massima-verosimiglianza-congiunta",
    "title": "58  Stima",
    "section": "58.3 Stima della Massima Verosimiglianza Congiunta",
    "text": "58.3 Stima della Massima Verosimiglianza Congiunta\nLa stima della massima verosimiglianza congiunta, comunemente nota come JML, è un metodo che mira a identificare i parametri delle persone e degli item che massimizzano la probabilità complessiva dei dati osservati, come descritto nell’equazione di verosimiglianza del modello di Rasch. In sostanza, questo approccio seleziona i parametri ritenuti più probabili nell’avere generato il set di dati in esame.\nNonostante il suo approccio diretto e intuitivo, l’uso del metodo JML è relativamente limitato in pratica. Questo perché, in molti casi, non fornisce stime consistenti dei parametri degli item, anche quando il numero di persone nel campione è molto grande. Tale limitazione rende JML meno affidabile rispetto ad altri metodi, specialmente in contesti dove la precisione e la consistenza delle stime sono cruciali.\nNel contesto del software statistico R, è possibile applicare l’approccio JML utilizzando la funzione tam.jml() disponibile nel pacchetto TAM. Allo stesso modo, è possibile sfruttare funzioni di uso generale per la stima di modelli lineari generalizzati. Tuttavia, data la tendenza del metodo JML a non fornire stime affidabili e consistenti dei parametri degli item, il suo impiego è generalmente sconsigliato in analisi avanzate o in situazioni dove la precisione delle stime è di fondamentale importanza.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Stima</span>"
    ]
  },
  {
    "objectID": "chapters/irt/04_estimation.html#stima-della-massima-verosimiglianza-condizionale",
    "href": "chapters/irt/04_estimation.html#stima-della-massima-verosimiglianza-condizionale",
    "title": "58  Stima",
    "section": "58.4 Stima della Massima Verosimiglianza Condizionale",
    "text": "58.4 Stima della Massima Verosimiglianza Condizionale\nPer superare i limiti associati alla stima congiunta, si ricorre spesso alla stima della massima verosimiglianza condizionale, che segue un approccio in due fasi. In questo processo, i parametri degli item vengono stimati inizialmente, indipendentemente dai parametri delle persone, per poi procedere con la stima dei parametri individuali.\nIn una fase iniziale, l’analisi si concentra esclusivamente sui parametri degli item, utilizzando le cosiddette statistiche sufficienti delle persone. Questo permette di isolare e stimare i parametri degli item senza la necessità di conoscere preliminarmente i parametri delle persone. L’obiettivo è ottenere stime precise dei parametri degli item, che saranno poi utilizzate nella fase successiva.\nDopo aver stimato i parametri degli item, questi vengono utilizzati per stimare i parametri relativi alle persone. A questo punto, si presume che i parametri degli item siano stati calcolati con una precisione sufficiente da fornire basi affidabili per la stima dei parametri individuali.\nLa validità e l’accuratezza delle stime dei parametri delle persone dipendono fortemente dalla precisione raggiunta nella stima dei parametri degli item nella prima fase. Se i parametri degli item non sono stati stimati con un alto grado di accuratezza, le stime dei parametri delle persone potrebbero risultare meno attendibili.\nLa stima della massima verosimiglianza condizionale si presenta come un approccio sequenziale e metodico, affrontando le sfide della stima congiunta attraverso un processo che massimizza inizialmente la verosimiglianza degli item e, successivamente, quella delle persone. Tale metodo offre un equilibrio tra precisione e praticità, risultando particolarmente utile in contesti di stima complessi.\nNell’ambiente di programmazione R, la stima della massima verosimiglianza condizionale può essere realizzata attraverso la funzione RM() fornita dal pacchetto eRm (Mair, Hatzinger e Maier, 2021).",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Stima</span>"
    ]
  },
  {
    "objectID": "chapters/irt/04_estimation.html#stima-della-massima-verosimiglianza-marginale",
    "href": "chapters/irt/04_estimation.html#stima-della-massima-verosimiglianza-marginale",
    "title": "58  Stima",
    "section": "58.5 Stima della Massima Verosimiglianza Marginale",
    "text": "58.5 Stima della Massima Verosimiglianza Marginale\nLa stima della massima verosimiglianza marginale rappresenta un approccio avanzato nell’analisi dei modelli di Rasch e si differenzia dalla stima della massima verosimiglianza condizionale. Mentre la stima condizionale fissa i parametri delle persone ai loro punteggi totali ottenuti nel test, la stima marginale “media” questi parametri dalla funzione di verosimiglianza complessiva.\nIn pratica, la stima della massima verosimiglianza marginale integra una distribuzione marginale, o di popolazione, per i parametri delle persone. Questa distribuzione rappresenta la probabilità relativa di ogni possibile livello di abilità all’interno della popolazione. Secondo De Ayala (2009), questo metodo considera le abilità come effetti casuali, simili ai modelli misti o multilivello, a differenza della stima congiunta che le tratta come effetti fissi.\nUna scelta comune per la distribuzione marginale dei parametri delle persone è quella normale. Questo approccio si basa sull’ipotesi che le abilità siano distribuite simmetricamente nella popolazione, con la maggior parte delle persone che si raggruppa attorno alla media e relativamente poche che presentano valori estremamente alti o bassi. Ad esempio, questo si applica bene in contesti come la misurazione dell’intelligenza, dove si presume che la maggior parte delle persone abbia un’intelligenza media e solo una minoranza presenti livelli estremamente alti o bassi.\nLa stima della massima verosimiglianza marginale consente quindi di considerare la distribuzione delle abilità nella popolazione, portando a stime dei parametri degli item che sono più realistiche e precise, specialmente in situazioni dove le abilità variano in modo continuo e seguono una distribuzione nota o ipotizzata.\nNell’ambito di R, la stima della massima verosimiglianza marginale è implementata sia nel pacchetto mirt (Chalmers, 2021) che in TAM (Robitzsch et al., 2021). Un altro pacchetto che utilizzava questo metodo è ltm (Rizopoulos, 2006), sebbene non sia più attivamente mantenuto.\nAnalogamente alla stima condizionale, anche nell’approccio marginale, i parametri delle persone sono tipicamente stimati dopo una prima stima dei parametri degli item, in un processo noto come “scoring” o calcolo del punteggio.\n\n58.5.1 Implementazione\nConsideriamo ora la procedura di stima del livello di abilità \\(\\theta\\) di un individuo nel modello di Rasch attraverso l’uso della massima verosimiglianza marginale. La procedura per stimare la posizione di un individuo, dato un particolare pattern di risposte, può essere formulata con i seguenti passaggi.\n\nConsideriamo un determinato pattern di risposta. Per esempio, il pattern “11000” indica che un particolare individuo ha fornito due risposte corrette seguite da tre errate a cinque item, con un totale di \\(X = 2\\) risposte corrette.\nCalcoliamo le probabilità per ogni risposta. Utilizziamo l’Eq. {eq}eq-rasch-model per calcolare la probabilità di ciascuna risposta nel pattern, in base a un dato livello di abilità \\(\\theta\\).\nDeterminiamo la probabilità del pattern di risposta. Questo passaggio si basa sull’assunzione di indipendenza condizionale (ovvero, per un dato \\(\\theta\\), le risposte sono indipendenti l’una dall’altra). Questa assunzione ci permette di applicare la regola di moltiplicazione per eventi indipendenti alle probabilità degli item per ottenere la probabilità complessiva del pattern di risposta per un dato \\(\\theta\\).\nRipetiamo i calcoli per diversi valori di \\(\\theta\\). Ripetiamo i passaggi 1 e 2 per una serie di valori di \\(\\theta\\). Nel nostro esempio, il range di \\(\\theta\\) va da \\(-3\\) a \\(3\\).\nDeterminiamo il valore di \\(\\theta\\) con la massima verosimiglianza. L’ultimo passaggio consiste nel determinare quale valore di \\(\\theta\\) tra quelli calcolati nel passaggio 3 abbia la più alta verosimiglianza di produrre il pattern “11000”. Per fare questo scegliamo il valore \\(\\theta\\) per cui la verosimiglianza è massima.\n\nDi seguito, esaminiamo uno script in R che implementa questa procedura.\n\n# Definiamo il pattern di risposta\nresponse_pattern &lt;- c(1, 1, 0, 0, 0)\n\n# Range di valori di theta da esplorare\ntheta_values &lt;- seq(-3, 3, by = 0.01)\n\n# Funzione per calcolare la probabilità di un singolo pattern di risposta\ncalculate_probability &lt;- function(theta, pattern) {\n    correct_probs &lt;- exp(theta) / (1 + exp(theta))\n    item_probs &lt;- ifelse(pattern == 1, correct_probs, 1 - correct_probs)\n    prod(item_probs)\n}\n# Per semplicità, assumiamo che il parametro di difficoltà (beta) sia zero per tutti gli item.\n\n# Calcoliamo le probabilità per ogni valore di theta. Usiamo sapply per applicare \n# la funzione calculate_probability a ciascun valore di theta nel range specificato.\nprobabilities &lt;- sapply(theta_values, calculate_probability, pattern = response_pattern)\n\n# Identifichiamo il valore di theta con la massima verosimiglianza\nbest_theta &lt;- theta_values[which.max(probabilities)]\n\nprint(paste(\"Valore di theta calcolato con la massima verosimiglianza:\", best_theta))\n\n[1] \"Valore di theta calcolato con la massima verosimiglianza: -0.41\"\n\n\nQuesto script calcola la probabilità di ottenere il pattern di risposta “11000” per cinque item per un dato intervallo di valori di \\(\\theta\\) e identifica il valore di \\(\\theta\\) che massimizza questa probabilità. Si noti che il modello di Rasch prevede che tutti gli item abbiano la stessa discriminazione, quindi non è necessario specificare un parametro di discriminazione per ogni item. Abbiamo assunto inoltre che la difficoltà di tutti gli item sia uguale a zero.\nLa verosimiglianza di un pattern di risposta di un singolo rispondente a diversi item può essere rappresentata simbolicamente nel modo seguente. Se consideriamo \\(x\\) come il pattern di risposta di un rispondente (ad esempio, \\(x = 11000\\) indica che il rispondente ha risposto correttamente ai primi due item e ha dato risposte sbagliate agli ultimi tre), la verosimiglianza del vettore di risposta \\(x_i\\) della persona \\(i\\) è espressa come:\n\\[\n\\begin{equation}\nL(x_i) = \\prod_{j=1}^{L} p_{ij},\n\\end{equation}\n\\]\ndove \\(p_{ij} = p(x_{ij} = 1 \\mid \\theta_i, \\alpha_j, \\delta_j)\\) rappresenta la probabilità che la persona \\(i\\), con un livello di abilità \\(\\theta_i\\), risponda correttamente all’item \\(j\\). In questa formula, \\(\\alpha_j\\) è il parametro di discriminazione dell’item \\(j\\) e \\(\\delta_j\\) è il suo parametro di difficoltà. Il parametro \\(\\alpha_j\\) indica quanto bene l’item \\(j\\) è in grado di discriminare tra rispondenti di diversi livelli di abilità, mentre \\(\\delta_j\\) rappresenta il livello di abilità per cui la probabilità di una risposta corretta è del 50%. Il prodotto è calcolato su tutti gli \\(L\\) item a cui il rispondente ha risposto, e il simbolo \\(\\prod\\) rappresenta il prodotto di tutte queste probabilità individuali.\nIl calcolo diretto della verosimiglianza può diventare problematico all’aumentare del numero di item, poiché il prodotto di molteplici probabilità può risultare in valori molto piccoli, difficili da gestire con precisione in calcoli numerici. Pertanto, è spesso più pratico lavorare con la trasformazione logaritmica naturale della verosimiglianza, ovvero \\(\\log_e(L(x_i))\\) o \\(\\ln(L(x_i))\\). Questa trasformazione converte il prodotto in una somma, come segue:\n\\[\n\\begin{equation}\n\\ln L(x_i) = \\sum_{j=1}^{L} \\ln(p_{ij}).\n\\end{equation}\n\\]\nL’uso del logaritmo naturale trasforma quindi la verosimiglianza in una somma di logaritmi, semplificando il calcolo e riducendo i problemi di rappresentazione numerica nei calcoli complessi.\n\n# Definizione del pattern di risposta\nresponse_pattern &lt;- c(1, 1, 0, 0, 0)\n\n# Range di valori di theta da esplorare\ntheta_values &lt;- seq(-3, 3, by = 0.1)\n\n# Calcolo della log-verosimiglianza per ogni valore di theta\nlog_likelihoods &lt;- numeric(length(theta_values))\nfor (i in seq_along(theta_values)) {\n    theta &lt;- theta_values[i]\n    log_item_probs &lt;- numeric(length(response_pattern))\n\n    # Calcolo delle probabilità logaritmiche individuali per ogni item nel pattern\n    for (j in seq_along(response_pattern)) {\n        prob_correct &lt;- exp(theta) / (1 + exp(theta))\n        prob &lt;- ifelse(response_pattern[j] == 1, prob_correct, 1 - prob_correct)\n        log_item_probs[j] &lt;- log(prob)\n    }\n\n    # Calcolo della log-verosimiglianza\n    log_likelihoods[i] &lt;- sum(log_item_probs)\n}\n\n# Creazione di un dataframe per il plotting\nplot_data &lt;- data.frame(theta = theta_values, log_likelihood = log_likelihoods)\n\n# Rappresentazione grafica della log-verosimiglianza\nggplot(plot_data, aes(x = theta, y = log_likelihood)) +\n    geom_line() +\n    labs(\n        x = expression(theta), y = \"Log-likelihood\",\n        title = \"Log-likelihood Function for Response Pattern 11000\"\n    )",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Stima</span>"
    ]
  },
  {
    "objectID": "chapters/irt/04_estimation.html#errore-standard-della-stima-e-informazione-dellitem",
    "href": "chapters/irt/04_estimation.html#errore-standard-della-stima-e-informazione-dellitem",
    "title": "58  Stima",
    "section": "58.6 Errore Standard della Stima e Informazione dell’Item",
    "text": "58.6 Errore Standard della Stima e Informazione dell’Item\nNel contesto del modello di Rasch, l’Errore Standard della Stima (EES) è un indice cruciale che misura l’incertezza associata alla stima del livello di abilità di un individuo, \\(\\theta\\). L’EES è fondamentale perché offre una misura di quanto fiduciosamente possiamo affermare che la stima di \\(\\theta\\) rifletta l’abilità reale del rispondente.\nNel modello di Rasch, l’EES per un dato livello di abilità \\(\\theta\\) è calcolato come l’inverso della radice quadrata dell’informazione totale dell’item a quel livello di abilità. Matematicamente, l’EES è espresso come segue:\n\\[\n\\begin{equation}\n\\text{EES}(\\theta) = \\frac{1}{\\sqrt{I(\\theta)}},\n\\end{equation}\n\\]\ndove \\(I(\\theta)\\) rappresenta l’informazione totale dell’item a un dato livello di abilità \\(\\theta\\). L’informazione dell’item, a sua volta, è una funzione della probabilità di una risposta corretta e della probabilità di una risposta errata per ciascun item, e viene calcolata come:\n\\[\n\\begin{equation}\nI(\\theta) = \\sum_{j=1}^{L} p_{ij}(1 - p_{ij}),\n\\end{equation}\n\\]\ndove \\(L\\) è il numero totale di item, e \\(p_{ij}\\) è la probabilità di una risposta corretta all’item \\(j\\) da parte di una persona con abilità \\(\\theta\\).\nQuesto concetto di informazione dell’item è fondamentale nel modello di Rasch. L’informazione fornita da un item varia in base al livello di abilità del rispondente e raggiunge il suo massimo quando la difficoltà dell’item (\\(\\delta_j\\)) è uguale al livello di abilità del rispondente (\\(\\theta\\)). Pertanto, gli item che sono più informativi per un dato livello di abilità contribuiscono maggiormente alla precisione della stima di \\(\\theta\\) per quel livello.\nPer rappresentare graficamente l’informazione dell’item, possiamo tracciare l’informazione fornita da ciascun item su un grafico in funzione del livello di abilità \\(\\theta\\). Questo grafico è noto come “curva di informazione dell’item” e illustra come ogni item contribuisca diversamente all’informazione totale in base al livello di abilità del rispondente.\n\n58.6.1 Stima dell’Abilità\nNel contesto della IRT, la stima dell’abilità di un esaminando può essere effettuata attraverso metodi di massima verosimiglianza. Il processo inizia con un’ipotesi iniziale o un valore a priori dell’abilità dell’esaminando. Questo valore può essere basato su considerazioni teoriche o su una media presunta dell’abilità.\nI valori noti dei parametri degli item del test (ad esempio, difficoltà e discriminazione degli item) vengono utilizzati per calcolare la probabilità che l’esaminando risponda correttamente a ciascun item, in base al livello di abilità iniziale ipotizzato.\nIl valore dell’abilità viene poi aggiustato iterativamente. L’obiettivo di questo aggiustamento è migliorare la corrispondenza tra le probabilità calcolate di risposta corretta e il pattern effettivo di risposte dell’esaminando (vettore di risposta). Questo si basa sull’idea che una migliore stima dell’abilità porterà a una maggiore coerenza tra le risposte osservate e quelle previste dai parametri degli item. L’iterazione continua fino a quando le modifiche alla stima dell’abilità diventano trascurabili, indicando che la stima ha raggiunto un punto di convergenza. Il risultato finale è una stima affidabile del parametro di abilità per quell’esaminando.\nQuesto processo viene applicato separatamente a ciascun esaminando che ha completato il test, assicurando che ogni stima di abilità sia personalizzata e basata sulle risposte specifiche fornite.\nEsistono anche metodi che permettono la stima simultanea dei livelli di abilità di tutti gli esaminandi. Tali metodi sono particolarmente utili per ottimizzare il processo di stima in contesti con un grande numero di rispondenti.\nLa stima dell’abilità è fondamentale in IRT per due ragioni principali:\n\nConsente di valutare l’abilità di ciascun esaminando in maniera individualizzata, tenendo conto delle specifiche interazioni tra l’esaminando e gli item del test.\nFornisce una misura accurata dell’abilità che è direttamente legata ai parametri degli item, permettendo analisi dettagliate e mirate sull’efficacia del test e sulle caratteristiche degli esaminandi.\n\nIn conclusione, la stima dell’abilità in IRT è un processo iterativo che sfrutta i parametri degli item per produrre stime precise dell’abilità degli esaminandi.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Stima</span>"
    ]
  },
  {
    "objectID": "chapters/irt/04_estimation.html#stima-bayesiana",
    "href": "chapters/irt/04_estimation.html#stima-bayesiana",
    "title": "58  Stima",
    "section": "58.7 Stima Bayesiana",
    "text": "58.7 Stima Bayesiana\nLa stima bayesiana sta diventando un metodo sempre più popolare per stimare i parametri del modello di Rasch. Come la stima della massima verosimiglianza congiunta, la stima bayesiana stima simultaneamente sia i parametri delle persone che quelli degli item. Tuttavia, mentre la stima della massima verosimiglianza congiunta trova i valori di $ $ e $ $ massimizzando la verosimiglianza congiunta, la stima bayesiana utilizza la regola di Bayes per trovare la densità a posteriori, $ f(,u) $. Per un’introduzione all’inferenza bayesiana, si veda l’Appendice B.1.4.\nNel modello di Rasch, la regola di Bayes afferma che:\n\\[\nf(\\theta,\\beta \\mid u) = \\frac{\\text{Pr}(u \\mid \\theta,\\beta)f(\\theta,\\beta)}{\\text{Pr}(u)}.\n\\]\nIl primo termine nel numeratore, $ (u | , ) $, è la verosimiglianza congiunta. Il secondo è la distribuzione a priori congiunta per $ $ e $ $. Il denominatore è la probabilità media dei dati osservati rispetto alla distribuzione a priori congiunta.\nA differenza della stima della massima verosimiglianza, che si concentra sulla massimizzazione della verosimiglianza, la stima bayesiana integra le informazioni a priori con i dati osservati. La regola di Bayes combina la verosimiglianza dei dati osservati (la probabilità di osservare i dati dati i parametri) con la distribuzione a priori (le nostre credenze sui parametri prima di osservare i dati) per produrre una distribuzione a posteriori (le nostre credenze aggiornate sui parametri dopo aver osservato i dati). La densità a posteriori $ f(,u) $ ci fornisce una stima completa dei parametri, considerando sia i dati osservati sia le informazioni a priori.\nIn pratica, la stima bayesiana fornisce un approccio flessibile e informativo alla stima dei parametri nel modello di Rasch, consentendo l’integrazione di conoscenze pregresse e osservazioni attuali.\n\n58.7.1 Implementazione\nEsaminiamo un’applicazione della stima Bayesiana usando il linguaggio probabilistico Stan. Il modello di Rasch è implementato nel file rasch_model.stan utilizzando le distribuzioni a priori specificate da {cite:t}debelak2022introduction.\n\nstan_file &lt;- \"../../code/rasch_model.stan\"\nmod &lt;- cmdstan_model(stan_file)\nmod$print()\n\ndata {\n  int&lt;lower=1&gt; num_person;\n  int&lt;lower=1&gt; num_item;\n  array[num_person, num_item] int&lt;lower=0, upper=1&gt; U;\n}\nparameters {\n  vector[num_person] theta;\n  vector[num_item] beta;\n  real mu_beta;\n  real&lt;lower=0&gt; sigma2_theta;\n  real&lt;lower=0&gt; sigma2_beta;\n}\ntransformed parameters {\n  array[num_person, num_item] real&lt;lower=0, upper=1&gt; prob_solve;\n  for (p in 1:num_person) \n    for (i in 1:num_item) \n      prob_solve[p, i] = inv_logit(theta[p] - beta[i]);\n}\nmodel {\n  for (p in 1:num_person) \n    for (i in 1:num_item) \n      U[p, i] ~ bernoulli(prob_solve[p, i]);\n  theta ~ normal(0, sqrt(sigma2_theta));\n  beta ~ normal(mu_beta, sqrt(sigma2_beta));\n  sigma2_theta ~ inv_chi_square(0.5);\n  sigma2_beta ~ inv_chi_square(0.5);\n}\n\n\n\n\nNella presente implementazione bayesiana del modello di Rasch, le sezioni “transformed parameters” e “model” hanno un ruolo centrale nel definire come i dati vengono processati e come il modello viene applicato. Vediamo dettagliatamente ciascuna sezione:\n\n\n58.7.2 Sezione Transformed Parameters\nNella sezione transformed parameters, viene definita la trasformazione dei parametri di base (i parametri theta per le abilità delle persone e beta per la difficoltà degli item) in una probabilità di risposta corretta per ogni coppia persona-item. Qui viene usata la funzione logistica inversa per convertire la differenza tra l’abilità della persona e la difficoltà dell’item in una probabilità:\ntransformed parameters {\n  array[num_person, num_item] real&lt;lower=0, upper=1&gt; prob_solve;\n  for (p in 1:num_person) \n    for (i in 1:num_item) \n      prob_solve[p, i] = inv_logit(theta[p] - beta[i]);\n}\nQuesta trasformazione serve a mappare la differenza tra l’abilità della persona (theta[p]) e la difficoltà dell’item (beta[i]) in un intervallo di probabilità tra 0 e 1. La funzione inv_logit è comunemente usata per questo scopo, essendo la funzione logistica inversa.\n\n\n58.7.3 Sezione Model\nNella sezione model, vengono definite le distribuzioni di probabilità per i dati osservati e i parametri del modello, che sono essenziali per la stima bayesiana. Questa parte del codice descrive come i dati sono generati, supponendo il modello di Rasch:\nmodel {\n  for (p in 1:num_person) \n    for (i in 1:num_item) \n      U[p, i] ~ bernoulli(prob_solve[p, i]);\n  theta ~ normal(0, sqrt(sigma2_theta));\n  beta ~ normal(mu_beta, sqrt(sigma2_beta));\n  sigma2_theta ~ inv_chi_square(0.5);\n  sigma2_beta ~ inv_chi_square(0.5);\n}\n\nU[p, i] ~ bernoulli(prob_solve[p, i]): ogni risposta U[p, i], che indica se la persona p ha risposto correttamente all’item i, segue una distribuzione di Bernoulli dove la probabilità di successo è data da prob_solve[p, i]. Questa è la vera verosimiglianza del modello, che collega i dati osservati alle probabilità calcolate tramite il modello logistico.\ntheta ~ normal(0, sqrt(sigma2_theta)) e beta ~ normal(mu_beta, sqrt(sigma2_beta)): le distribuzioni a priori per i parametri theta e beta sono normali. Questo significa che, in assenza di dati, si assume che queste variabili si distribuiscano normalmente con una media di 0 per theta e mu_beta per beta, e una deviazione standard derivata dai parametri di varianza sigma2_theta e sigma2_beta.\nsigma2_theta ~ inv_chi_square(0.5) e sigma2_beta ~ inv_chi_square(0.5): le varianze sigma2_theta e sigma2_beta hanno distribuzioni a priori che seguono una distribuzione chi quadrato inversa con parametro di forma 0.5. Questa è una scelta comune per imporre una distribuzione non informativa (vaga) sui parametri di scala.\n\nIn conclusione, la sezione transformed parameters calcola le probabilità di risposta corretta basate sui parametri di abilità e difficoltà, mentre la sezione model specifica come questi parametri e le risposte osservate interagiscono secondo il modello di Rasch, definendo così la struttura della verosimiglianza e delle priorità nel contesto bayesiano.\nCompiliamo il modello usando CmdStan:\n\nmod$compile()\n\nDefiniamo i dati nel formato appropriato per Stan:\n\ndata(data.fims.Aus.Jpn.scored, package = \"TAM\")\npeople &lt;- 1:400\nresponses &lt;- data.fims.Aus.Jpn.scored[people, 2:15]\nresponses &lt;- as.matrix(sapply(responses, as.integer))\ncolnames(responses) &lt;- gsub(\"M1PTI\", \"I\", colnames(responses))\n\nstan_data &lt;- list(\n    num_person = nrow(responses),\n    num_item = ncol(responses),\n    U = responses\n)\n\nstan_data\n\n\n    $num_person\n        400\n    $num_item\n        14\n    $U\n        \n\nA matrix: 400 x 14 of type int\n\n\nI1\nI2\nI3\nI6\nI7\nI11\nI12\nI14\nI17\nI18\nI19\nI21\nI22\nI23\n\n\n\n\n1\n0\n1\n1\n0\n1\n0\n0\n1\n0\n0\n0\n0\n1\n\n\n0\n1\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n\n\n1\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n\n\n1\n1\n1\n1\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n1\n1\n1\n0\n1\n0\n1\n0\n1\n0\n0\n0\n1\n\n\n1\n1\n1\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n\n\n0\n0\n1\n0\n0\n1\n1\n1\n1\n0\n0\n0\n0\n0\n\n\n1\n1\n0\n1\n0\n1\n0\n1\n0\n1\n0\n0\n0\n1\n\n\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n\n\n0\n1\n1\n0\n0\n1\n0\n0\n0\n1\n0\n0\n1\n1\n\n\n1\n1\n1\n1\n0\n1\n1\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n1\n1\n1\n0\n1\n0\n1\n1\n1\n0\n0\n0\n0\n\n\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n0\n1\n1\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n1\n\n\n0\n0\n1\n1\n0\n1\n0\n1\n0\n1\n0\n0\n0\n1\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n\n\n1\n1\n1\n1\n0\n1\n0\n1\n1\n0\n0\n0\n0\n0\n\n\n1\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n1\n1\n0\n0\n1\n0\n1\n0\n1\n0\n0\n0\n1\n\n\n1\n1\n1\n0\n0\n1\n0\n1\n0\n1\n0\n0\n0\n1\n\n\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n1\n1\n1\n1\n0\n1\n0\n1\n0\n0\n0\n0\n0\n1\n\n\n0\n1\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n1\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n0\n1\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n1\n\n\n1\n0\n1\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n1\n\n\n0\n0\n0\n1\n0\n1\n0\n1\n0\n1\n0\n1\n0\n0\n\n\n1\n1\n1\n1\n0\n1\n0\n1\n1\n0\n0\n0\n0\n1\n\n\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n\n\n1\n1\n1\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n0\n1\n1\n0\n1\n1\n1\n1\n0\n1\n0\n1\n0\n1\n\n\n0\n0\n1\n0\n0\n1\n0\n1\n0\n0\n0\n0\n1\n1\n\n\n1\n1\n1\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n1\n\n\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n\n\n0\n0\n1\n0\n1\n1\n0\n1\n0\n1\n0\n0\n0\n1\n\n\n0\n0\n1\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n1\n\n\n0\n1\n1\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n1\n\n\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n1\n\n\n1\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n1\n\n\n0\n1\n1\n0\n0\n1\n0\n1\n0\n1\n0\n0\n0\n1\n\n\n1\n1\n1\n1\n0\n1\n0\n0\n0\n1\n0\n0\n0\n1\n\n\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n\n\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n1\n\n\n1\n1\n1\n0\n0\n0\n1\n1\n0\n1\n0\n0\n0\n1\n\n\n1\n1\n1\n1\n0\n1\n0\n0\n0\n1\n0\n1\n0\n1\n\n\n1\n1\n1\n0\n0\n0\n1\n0\n0\n1\n0\n0\n1\n1\n\n\n1\n1\n1\n1\n0\n1\n0\n1\n0\n1\n0\n0\n1\n1\n\n\n1\n1\n1\n1\n0\n1\n0\n0\n0\n1\n0\n0\n0\n1\n\n\n1\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n1\n1\n1\n0\n1\n0\n1\n0\n1\n0\n0\n0\n1\n\n\n1\n1\n1\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n1\n\n\n1\n1\n1\n1\n0\n1\n0\n0\n1\n1\n0\n0\n0\n1\n\n\n0\n1\n1\n1\n0\n1\n0\n0\n1\n0\n1\n0\n0\n1\n\n\n1\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n1\n1\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n1\n\n\n1\n1\n1\n0\n0\n1\n0\n1\n0\n1\n0\n0\n0\n1\n\n\n1\n1\n1\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n\n\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n\n\n\n\n\nEseguiamo il campionamento MCMC per ottenere la distribuzione a posteriori dei parametri.\n\nfit &lt;- mod$sample(\n    data = stan_data,\n    chains = 4, # Number of MCMC chains\n    parallel_chains = 2, # Number of chains to run in parallel \n    iter_warmup = 2000, # Number of warmup iterations per chain\n    iter_sampling = 2000, # Number of sampling iterations per chain\n    seed = 1234 # Set a seed for reproducibility\n)\n\nEsaminiamo le tracce per due parametri.\n\nfit_draws &lt;- fit$draws() # extract the posterior draws\nmcmc_trace(fit_draws, pars = c(\"beta[1]\"))\n\n\n\n\n\n\n\n\n\nmcmc_trace(fit_draws, pars = c(\"theta[1]\"))\n\n\n\n\n\n\n\n\nFocalizziamoci sulla stima dei parametri degli item.\n\nparameters &lt;- c(\n    \"beta[1]\", \"beta[2]\", \"beta[3]\", \"beta[4]\", \"beta[5]\", \n    \"beta[6]\", \"beta[7]\", \"beta[8]\", \"beta[9]\",\"beta[10]\",\n    \"beta[11]\", \"beta[12]\", \"beta[13]\", \"beta[14]\"\n)\n\nEsaminiamo la statistica rhat.\n\nrhats &lt;- rhat(fit_draws, pars = parameters)\nmcmc_rhat(rhats)\n\n\n\n\n\n\n\n\nEsaminiamo l’effect ratio:\n\neff_ratio &lt;- neff_ratio(fit, pars = parameters)\neff_ratio\nmcmc_neff(eff_ratio) \n\nbeta[1]1.13228653874956beta[2]1.18893586285432beta[3]1.16206445337895beta[4]1.09833035160828beta[5]1.35717661914907beta[6]1.15952811419412beta[7]1.17595353070278beta[8]1.08208811843576beta[9]1.30070203631086beta[10]1.20775869658246beta[11]1.26276161662907beta[12]1.2345210973256beta[13]1.39244422596835beta[14]1.30555561451952\n\n\n\n\n\n\n\n\n\nEsaminiamo l’autocorrelazione.\n\nmcmc_acf(fit_draws, pars = parameters)\n\n\n\n\n\n\n\n\nOtteniamo le statistiche riassuntive delle distribuzioni a posteriori dei parametri degli item.\n\nfit$summary(\n    variables = parameters,\n    posterior::default_summary_measures(),\n    extra_quantiles = ~ posterior::quantile2(., probs = c(.0275, .975))\n)\n\n\nA draws_summary: 14 x 9\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nq2.75\nq97.5\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nbeta[1]\n-1.09785464\n-1.09808500\n0.1287407\n0.1263694\n-1.3108765\n-0.8920219\n-1.3506538\n-0.8494646\n\n\nbeta[2]\n-1.24465346\n-1.24430500\n0.1297084\n0.1296608\n-1.4583135\n-1.0334105\n-1.4964014\n-0.9966666\n\n\nbeta[3]\n-2.02867408\n-2.02456000\n0.1582019\n0.1564069\n-2.2950065\n-1.7685855\n-2.3374176\n-1.7285708\n\n\nbeta[4]\n-0.04775289\n-0.04890095\n0.1199480\n0.1210026\n-0.2409295\n0.1488078\n-0.2739345\n0.1910600\n\n\nbeta[5]\n2.51167467\n2.50869500\n0.1827246\n0.1845170\n2.2185885\n2.8191735\n2.1762056\n2.8849028\n\n\nbeta[6]\n-1.24474000\n-1.24396500\n0.1319090\n0.1308765\n-1.4629795\n-1.0287640\n-1.5049036\n-0.9885849\n\n\nbeta[7]\n0.81119366\n0.80993100\n0.1236541\n0.1236192\n0.6101101\n1.0130450\n0.5763395\n1.0545912\n\n\nbeta[8]\n-0.49325414\n-0.49277150\n0.1198008\n0.1194027\n-0.6876882\n-0.2941435\n-0.7229137\n-0.2628265\n\n\nbeta[9]\n1.32280348\n1.32135000\n0.1345428\n0.1384971\n1.1070695\n1.5460100\n1.0707870\n1.5832405\n\n\nbeta[10]\n-0.39484418\n-0.39514000\n0.1202975\n0.1214264\n-0.5921317\n-0.1955259\n-0.6216329\n-0.1579470\n\n\nbeta[11]\n2.05292263\n2.05293500\n0.1582483\n0.1587197\n1.7956570\n2.3130265\n1.7541263\n2.3664770\n\n\nbeta[12]\n1.74686288\n1.74451500\n0.1481399\n0.1458656\n1.5048420\n1.9932850\n1.4650029\n2.0408020\n\n\nbeta[13]\n2.39625463\n2.39383000\n0.1722816\n0.1722336\n2.1167730\n2.6859200\n2.0690167\n2.7361960\n\n\nbeta[14]\n-1.92004023\n-1.91840000\n0.1518714\n0.1477559\n-2.1754130\n-1.6739390\n-2.2225508\n-1.6289105\n\n\n\n\n\nI risultati ottenuti replicano quelli riportati da {cite:t}debelak2022introduction.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Stima</span>"
    ]
  },
  {
    "objectID": "chapters/irt/04_estimation.html#informazione",
    "href": "chapters/irt/04_estimation.html#informazione",
    "title": "58  Stima",
    "section": "58.8 Informazione",
    "text": "58.8 Informazione\nNel contesto della psicometria e della statistica, il concetto di “informazione” assume un significato tecnico specifico, collegato alla precisione con cui è possibile stimare un parametro. Questa nozione, attribuita a Sir R.A. Fisher, definisce l’informazione come l’inverso della varianza con la quale un parametro può essere stimato. Dunque, un’alta precisione nella stima (ovvero una bassa variabilità) implica una maggiore quantità di informazione riguardo al valore del parametro.\n\n58.8.1 Funzione Informativa dell’Item\nIn termini della IRT, l’interesse si concentra sulla stima del parametro di abilità di un esaminando, denotato da $ $. L’informazione relativa a un dato livello di abilità è data dall’inverso della varianza della stima di quell’abilità. Una grande quantità di informazione indica che il livello di abilità corrispondente può essere stimato con precisione, mentre una piccola quantità di informazione suggerisce una stima meno precisa e più dispersa attorno al valore vero dell’abilità.\nL’informazione nella IRT è additiva rispetto agli item. L’informazione fornita da un singolo item $ i $ a un dato livello di abilità $ _p $ è espressa dalla formula:\n\\[\nI_i(\\theta_p) = \\text{Pr}(U_{pi} = 1 \\mid \\theta_p, \\beta_i) \\cdot (1 - \\text{Pr}(U_{pi} = 1 \\mid \\theta_p, \\beta_i)).\n\\]\nPer il modello di Rasch, l’informazione è equivalente alla pendenza della Curva Caratteristica dell’Item (ICC) in un punto specifico. Maggiore è la pendenza dell’ICC, maggiore è la differenza nella probabilità per una data differenza di abilità, permettendo di stimare con maggiore precisione l’abilità di un esaminando quando questa è vicina alla difficoltà dell’item.\nLa stima dell’abilità e della difficoltà degli item può essere dimostrata con un esempio in R. Utilizzando il modello di Rasch, possiamo calcolare le probabilità di risposta corretta per diversi valori di abilità e, di conseguenza, la Funzione Informativa dell’Item (Item Information Function, IIF):\n\n# Definizione di un range di abilità\ntheta &lt;- seq(-4, 4, by = 0.1)\n\n# Definizione di un parametro di difficoltà dell'item\nbeta &lt;- 0\n\n# Calcolo delle probabilità di risposta corretta per ciascun valore di abilità usando la funzione logistica\nprob_correct &lt;- exp(theta - beta) / (1 + exp(theta - beta))\n\n# Calcolo dell'informazione dell'item\nitem_info &lt;- prob_correct * (1 - prob_correct)\n\n# Creazione della prima grafica (ICC)\nplot(theta, prob_correct,\n    type = \"l\", col = \"blue\", lwd = 2,\n    xlab = \"Abilita' theta\", ylab = \"Probabilita' di Risposta Corretta\",\n    main = \"Curva Caratteristica dell'Item (ICC) e Informazione dell'Item\"\n)\n\n# Aggiunta di un secondo asse y per l'informazione\npar(new = TRUE)\nplot(theta, item_info,\n    type = \"l\", col = \"red\", lwd = 2,\n    xlab = \"\", ylab = \"\", axes = FALSE, ann = FALSE\n)\n\n# Aggiungere l'asse y di destra per l'informazione\naxis(side = 4, at = pretty(range(item_info)))\nmtext(\"Informazione\", side = 4, line = 3)\n\n# Aggiunta della legenda\nlegend(\"topright\",\n    legend = c(\"ICC\", \"Informazione\"),\n    col = c(\"blue\", \"red\"), lty = 1, cex = 0.8\n)\n\n\n\n\n\n\n\n\nQuesta rappresentazione grafica in R mostra come l’informazione vari in funzione del livello di abilità. In generale, l’informazione è massima quando l’abilità dell’esaminando è vicina alla difficoltà dell’item e diminuisce man mano che ci si allontana da questo punto.\nIl concetto di informazione in IRT è fondamentale sia per la costruzione del test sia per la sua interpretazione. Indica quanto efficacemente ciascun item misura l’abilità a vari livelli e aiuta a determinare quali item sono più informativi per la stima dell’abilità degli esaminandi. Inoltre, fornisce indicazioni sulla precisione con cui l’abilità degli esaminandi può essere stimata a vari punti lungo la scala di abilità.\n\n\n58.8.2 Funzione di Informazione del Test (TIF)\nL’analisi dell’informazione fornita da un test nella Teoria delle Risposte agli Item (IRT) si amplia ulteriormente considerando la Funzione di Informazione del Test (TIF). Questa funzione è essenziale per comprendere quanto efficacemente un test nel suo insieme stima l’abilità degli esaminandi a vari livelli.\n\nDefinizione della TIF: La TIF è definita come la somma delle informazioni fornite da ciascun item a un determinato livello di abilità. Matematicamente, se $ I() $ rappresenta l’ammontare di informazione del test a un livello di abilità $ $, e $ I_j() $ è l’informazione fornita dall’item $ j $ a quel livello, allora la TIF è data da:\n\\[ I(\\theta) = \\sum_{j=1}^{J} I_j(\\theta), \\]\ndove $ J $ è il numero di item nel test.\nLivello Generale della TIF: La TIF di un test è generalmente molto più alta rispetto all’informazione fornita da un singolo item. Questo implica che un test, come insieme di item, stima l’abilità in modo più preciso rispetto a un singolo item.\nRelazione tra Lunghezza del Test e Informazione: Un principio importante evidenziato dalla TIF è che, in generale, i test più lunghi forniscono una stima dell’abilità con maggiore precisione rispetto ai test più brevi. Questo è dovuto al fatto che l’aggiunta di item aumenta la quantità totale di informazione disponibile.\nGrafico della TIF: Tracciando l’ammontare di informazione del test contro i livelli di abilità, si ottiene un grafico della TIF. Per esempio, per un test di dieci item, il valore massimo della TIF potrebbe essere modesto e la quantità di informazione potrebbe diminuire progressivamente man mano che il livello di abilità si discosta da quello corrispondente al massimo. Questo indica che l’abilità viene stimata con una certa precisione vicino al centro della scala di abilità, ma la precisione diminuisce significativamente ai suoi estremi.\nUtilità della TIF: La TIF è estremamente utile in IRT perché indica quanto bene il test stima l’abilità su tutta la gamma dei punteggi di abilità. Sebbene una TIF ideale possa essere una linea orizzontale, ciò potrebbe non essere ottimale per scopi specifici. Ad esempio, in un test per assegnare borse di studio, una TIF con un picco al punteggio di taglio potrebbe essere più appropriata.\nDipendenza dal Modello di ICC: La definizione matematica dell’informazione di un item dipende dal modello di curva caratteristica dell’item impiegato. Ogni modello fornisce una formula specifica per calcolare l’informazione fornita da un item.\n\nPer dimostrare come calcolare la TIF in R, possiamo estendere l’esempio precedente includendo più item e sommando le loro informazioni:\n\n# Definizione di parametri di difficoltà per diversi item\nbeta_items &lt;- c(-1, 0, 1) # Esempio di tre item con difficoltà diverse\n\n# Calcolo dell'informazione per ogni item e somma per ottenere la TIF\ntest_info &lt;- rep(0, length(theta))\nfor (beta in beta_items) {\n    prob_correct &lt;- exp(theta - beta) / (1 + exp(theta - beta))\n    item_info &lt;- prob_correct * (1 - prob_correct)\n    test_info &lt;- test_info + item_info\n}\n\n# Creazione del grafico della TIF\nplot(theta, test_info,\n    type = \"l\", col = \"blue\", lwd = 2,\n    xlab = \"Abilità theta\", ylab = \"Informazione del Test\",\n    main = \"Funzione di Informazione del Test (TIF)\"\n)\n\n\n\n\n\n\n\n\nIn questo esempio, calcoliamo e sommiamo le informazioni di tre item con diverse difficoltà per visualizzare la TIF di un test ipotetico. La TIF mostra in modo chiaro come il test nel suo insieme stima l’abilità degli esaminandi a vari livelli, fornendo così indicazioni preziose sulla costruzione e sull’utilizzo ottimale del test in diversi contesti.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Stima</span>"
    ]
  },
  {
    "objectID": "chapters/irt/04_estimation.html#grandezza-del-campione",
    "href": "chapters/irt/04_estimation.html#grandezza-del-campione",
    "title": "58  Stima",
    "section": "58.9 Grandezza del Campione",
    "text": "58.9 Grandezza del Campione\nLa stima dei parametri degli item basata su un campione osservato di risposte è spesso definita come la calibrazione degli item. Generalmente, un campione di calibrazione più ampio consente una stima più accurata dei parametri degli item, sebbene altri fattori influenzino anch’essi l’accuratezza della stima. Ad esempio, la difficoltà di un item può essere stimata con maggiore precisione se l’item non è né troppo facile né troppo difficile per il campione di partecipanti al test. Pertanto, i fattori che influenzano l’accuratezza della stima includono l’allineamento e la forma delle distribuzioni dei parametri degli item e delle persone, il numero di item e la tecnica di stima utilizzata.\nDiverse pubblicazioni hanno affrontato la questione della dimensione del campione tipicamente necessaria per lavorare con il modello di Rasch e come questa sia influenzata da questi e altri fattori. Ad esempio, De Ayala (2009) fornisce la linea guida generale che un campione di calibrazione dovrebbe contenere almeno diverse centinaia di rispondenti e cita, tra le altre referenze, un articolo precedente di Wright (1977) che afferma che un campione di calibrazione di 500 sarebbe più che adeguato. De Ayala (2009) suggerisce anche che 250 o più rispondenti sono necessari per adattare un modello di Partial Credit. Poiché il modello di Partial Credit è una generalizzazione del modello di Rasch con più parametri degli item, ciò implica che la dimensione del campione suggerita di 250 dovrebbe essere sufficiente anche per adattare un modello di Rasch. Studi più recenti hanno indagato l’applicazione del modello di Rasch con dimensioni del campione di soli 100 rispondenti (ad esempio, Steinfeld & Robitzsch, 2021; Suárez-Falcón & Glas, 2003). Tali linee guida non devono essere interpretate come regole fisse, ma solo come indicazioni generali in quanto una dimensione del campione adeguata dipende dalle condizioni e dagli obiettivi dell’analisi.\nUn metodo più elaborato per determinare la dimensione del campione necessaria è l’analisi della potenza statistica. Qui, l’accuratezza della stima desiderata o il rischio di falsi positivi e falsi negativi devono essere formalizzati prima dell’analisi. La dimensione del campione necessaria viene quindi determinata in base a queste considerazioni.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Stima</span>"
    ]
  },
  {
    "objectID": "chapters/irt/04_estimation.html#session-info",
    "href": "chapters/irt/04_estimation.html#session-info",
    "title": "58  Stima",
    "section": "58.10 Session Info",
    "text": "58.10 Session Info\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats4    grid      stats     graphics  grDevices utils     datasets \n[8] methods   base     \n\nother attached packages:\n [1] effectsize_0.8.9    rsvg_2.6.1          rstan_2.32.6       \n [4] StanHeaders_2.32.10 posterior_1.6.0     cmdstanr_0.8.1.9000\n [7] ggmirt_0.1.0        TAM_4.2-21          CDM_8.2-6          \n[10] mvtnorm_1.3-1       mirt_1.42           lattice_0.22-6     \n[13] latex2exp_0.9.6     ggokabeito_0.1.0    viridis_0.6.5      \n[16] viridisLite_0.4.2   ggpubr_0.6.0        ggExtra_0.10.1     \n[19] bayesplot_1.11.1    gridExtra_2.3       patchwork_1.3.0    \n[22] semTools_0.5-6      semPlot_1.1.6       lavaan_0.6-18      \n[25] psych_2.4.6.26      scales_1.3.0        markdown_1.13      \n[28] knitr_1.48          lubridate_1.9.3     forcats_1.0.0      \n[31] stringr_1.5.1       dplyr_1.1.4         purrr_1.0.2        \n[34] readr_2.1.5         tidyr_1.3.1         tibble_3.2.1       \n[37] ggplot2_3.5.1       tidyverse_2.0.0     here_1.0.1         \n\nloaded via a namespace (and not attached):\n  [1] splines_4.4.1        later_1.3.2          pbdZMQ_0.3-13       \n  [4] R.oo_1.26.0          datawizard_0.12.3    XML_3.99-0.17       \n  [7] rpart_4.1.23         lifecycle_1.0.4      rstatix_0.7.2       \n [10] rprojroot_2.0.4      processx_3.8.4       globals_0.16.3      \n [13] MASS_7.3-61          insight_0.20.4       rockchalk_1.8.157   \n [16] backports_1.5.0      magrittr_2.0.3       openxlsx_4.2.7.1    \n [19] Hmisc_5.1-3          rmarkdown_2.28       httpuv_1.6.15       \n [22] qgraph_1.9.8         zip_2.3.1            pkgbuild_1.4.4      \n [25] sessioninfo_1.2.2    pbapply_1.7-2        minqa_1.2.8         \n [28] multcomp_1.4-26      abind_1.4-8          audio_0.1-11        \n [31] quadprog_1.5-8       R.utils_2.12.3       tensorA_0.36.2.1    \n [34] nnet_7.3-19          TH.data_1.1-2        sandwich_3.1-1      \n [37] inline_0.3.19        listenv_0.9.1        testthat_3.2.1.1    \n [40] RPushbullet_0.3.4    vegan_2.6-8          arm_1.14-4          \n [43] parallelly_1.38.0    permute_0.9-7        codetools_0.2-20    \n [46] tidyselect_1.2.1     farver_2.1.2         lme4_1.1-35.5       \n [49] matrixStats_1.4.1    base64enc_0.1-3      jsonlite_1.8.9      \n [52] polycor_0.8-1        progressr_0.14.0     Formula_1.2-5       \n [55] survival_3.7-0       emmeans_1.10.4       tools_4.4.1         \n [58] snow_0.4-4           Rcpp_1.0.13          glue_1.7.0          \n [61] mnormt_2.1.1         admisc_0.36          xfun_0.47           \n [64] mgcv_1.9-1           distributional_0.5.0 IRdisplay_1.1       \n [67] loo_2.8.0            withr_3.0.1          beepr_2.0           \n [70] fastmap_1.2.0        boot_1.3-31          fansi_1.0.6         \n [73] digest_0.6.37        mi_1.1               timechange_0.3.0    \n [76] R6_2.5.1             mime_0.12            estimability_1.5.1  \n [79] colorspace_2.1-1     gtools_3.9.5         jpeg_0.1-10         \n [82] R.methodsS3_1.8.2    utf8_1.2.4           generics_0.1.3      \n [85] data.table_1.16.0    corpcor_1.6.10       SimDesign_2.17.1    \n [88] htmlwidgets_1.6.4    parameters_0.22.2    pkgconfig_2.0.3     \n [91] sem_3.1-16           gtable_0.3.5         brio_1.1.5          \n [94] htmltools_0.5.8.1    carData_3.0-5        png_0.1-8           \n [97] rstudioapi_0.16.0    tzdb_0.4.0           reshape2_1.4.4      \n[100] uuid_1.2-1           coda_0.19-4.1        checkmate_2.3.2     \n[103] nlme_3.1-166         curl_5.2.3           nloptr_2.1.1        \n[106] repr_1.1.7           zoo_1.8-12           parallel_4.4.1      \n[109] miniUI_0.1.1.1       foreign_0.8-87       pillar_1.9.0        \n[112] vctrs_0.6.5          promises_1.3.0       car_3.1-2           \n[115] OpenMx_2.21.12       xtable_1.8-4         Deriv_4.1.6         \n[118] cluster_2.1.6        dcurver_0.9.2        GPArotation_2024.3-1\n[121] htmlTable_2.4.3      evaluate_1.0.0       pbivnorm_0.6.0      \n[124] cli_3.6.3            kutils_1.73          compiler_4.4.1      \n[127] rlang_1.1.4          crayon_1.5.3         future.apply_1.11.2 \n[130] ggsignif_0.6.4       labeling_0.4.3       fdrtool_1.2.18      \n[133] ps_1.8.0             plyr_1.8.9           stringi_1.8.4       \n[136] QuickJSR_1.3.1       munsell_0.5.1        lisrelToR_0.3       \n[139] bayestestR_0.14.0    V8_5.0.1             pacman_0.5.1        \n[142] Matrix_1.7-0         IRkernel_1.3.2       hms_1.1.3           \n[145] glasso_1.11          future_1.34.0        shiny_1.9.1         \n[148] igraph_2.0.3         broom_1.0.6          RcppParallel_5.1.9",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Stima</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_1pl_2pl_3pl.html",
    "href": "chapters/irt/05_1pl_2pl_3pl.html",
    "title": "59  Oltre il Modello di Rasch",
    "section": "",
    "text": "59.1 Introduzione\nNell’ambito dei modelli IRT, il modello di Rasch impone i vincoli più restrittivi. Tali vincoli possono essere rilassati progressivamente definendo quelli che vengono chiamati modelli 1PL, 2PL e 3Pl.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>Oltre il Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_1pl_2pl_3pl.html#un-esempio-pratico",
    "href": "chapters/irt/05_1pl_2pl_3pl.html#un-esempio-pratico",
    "title": "59  Oltre il Modello di Rasch",
    "section": "59.2 Un Esempio Pratico",
    "text": "59.2 Un Esempio Pratico\nIn questo capitolo, utilizzeremo nuovamente i dati che abbiamo esaminato in precedenza.\n\ndata(data.fims.Aus.Jpn.scored, package = \"TAM\")\nfims &lt;- data.fims.Aus.Jpn.scored\n\nEsaminiamo le risposte dei primi 400 partecipanti. Per facilitare la manipolazione dei dati, cambiamo il nome delle colonne.\n\nresponses &lt;- fims[1:400, 2:15]\ncolnames(responses) &lt;- gsub(\"M1PTI\", \"I\", colnames(responses))\nglimpse(responses)\n\nRows: 400\nColumns: 14\n$ I1  &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, ~\n$ I2  &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, ~\n$ I3  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, ~\n$ I6  &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, ~\n$ I7  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\n$ I11 &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, ~\n$ I12 &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\n$ I14 &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, ~\n$ I17 &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, ~\n$ I18 &lt;dbl&gt; 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, ~\n$ I19 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\n$ I21 &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ~\n$ I22 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ~\n$ I23 &lt;dbl&gt; 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, ~\n\n\n\ngender &lt;- as.factor(fims$SEX[1:400])\nlevels(gender) &lt;- c(\"male\", \"female\")",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>Oltre il Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_1pl_2pl_3pl.html#modello-ad-un-parametro",
    "href": "chapters/irt/05_1pl_2pl_3pl.html#modello-ad-un-parametro",
    "title": "59  Oltre il Modello di Rasch",
    "section": "59.3 Modello ad Un Parametro",
    "text": "59.3 Modello ad Un Parametro\nIl modello ad un parametro è formulato nel modo seguente:\n\\[\n\\begin{equation}\nP(X_i = 1 \\mid \\theta_v, \\alpha, \\delta_i) = \\frac{\\exp(\\alpha(\\theta_v - \\delta_i))}{1 + \\exp(\\alpha(\\theta_v - \\delta_i))} = \\frac{1}{1 + \\exp(-\\alpha(\\theta_v - \\delta_i))},\n\\end{equation}\n\\] (eq-1pl-model)\ndove \\(\\alpha\\) rappresenta la pendenza delle ICC. L’assenza di un pedice significa che \\(\\alpha\\) non varia tra gli item.\nNel contesto del modello IRT (Teoria della Risposta all’Item) a un parametro, il parametro \\(\\alpha\\) è collegato alla pendenza della Funzione di Risposta all’Item (IRF). Esso riflette quanto bene un item è in grado di discriminare tra individui situati in punti diversi lungo il continuum. Di conseguenza, \\(\\alpha\\) è noto come il parametro di discriminazione dell’item.\nPer facilitare la comprensione, supponiamo di avere tre item con diversi valori di \\(\\alpha\\), tutti posizionati a \\(0.0\\) (cioè, \\(\\delta_1 = \\delta_2 = \\delta_3 = 0\\)). I nostri tre parametri di discriminazione sono \\(0\\), \\(1\\) e \\(2\\). Inoltre, abbiamo un rispondente A situato a \\(-1\\) (\\(\\theta_A = -1\\)) e un altro rispondente B situato a \\(1\\) (cioè, \\(\\theta_B = 1\\)). Per l’item con \\(\\alpha = 0.0\\), la nostra IRF e la linea di regressione logit sono orizzontali. Di conseguenza, la probabilità prevista di una risposta di tipo 1 per entrambi i rispondenti è \\(0.5\\):\n\\[\nP(X_i = 1 \\mid \\theta, \\delta_i) = 0.5.\n\\]\nIn questo caso, l’item non fornisce alcuna informazione utile per differenziare tra i due rispondenti. Questa mancanza di potere discriminatorio è una funzione diretta di \\(\\alpha = 0.0\\).\nAl contrario, con il secondo item (\\(\\alpha = 1\\)) abbiamo previsioni diverse per i nostri rispondenti:\n\\[\nP(X_i = 1 \\mid \\theta, \\delta_i) = \\frac{1}{1 + e^{-(\\theta - \\delta_i)}}.\n\\]\nPer il rispondente A la probabilità \\(p_2 = 0.2689\\) e per il rispondente B \\(p_2 = 0.7311\\). Pertanto, il parametro \\(\\alpha\\) di questo item ci permette di distinguere tra i due rispondenti.\nSviluppando ulteriormente questa idea, troviamo che il terzo item (\\(\\alpha = 2.0\\)) avrebbe l’ICC (e la linea di regressione logit) più ripida dei tre item. Questa ripidità si riflette in una maggiore differenza nelle probabilità previste per i nostri rispondenti rispetto ai due item precedenti:\n\\[\nP(X_i = 1 \\mid \\theta, \\delta_i) = \\frac{1}{1 + e^{-2(\\theta - \\delta_i)}}.\n\\]\nOvvero, per questo item il rispondente A ha una probabilità \\(p_3 = 0.1192\\) e per il rispondente B \\(p_3 = 0.8808\\). In breve, l’entità della differenza in queste probabilità previste è una funzione diretta del parametro \\(\\alpha\\) dell’item. Pertanto, gli item con \\(\\alpha\\) maggiori (cioè con linee di regressione logit e IRF più ripide) discriminano meglio tra i rispondenti situati in punti diversi del continuum rispetto agli item con \\(\\alpha\\) minori.\nIn sintesi, maggiore è il valore di \\(\\alpha\\), più ripida sarà la curva ICC e maggiore sarà la discriminazione tra individui con diversi livelli di abilità \\(\\theta\\).\n\n59.3.1 Modello di Rasch e Modello 1PL\nPer riassumere, sia il modello 1PL che il modello di Rasch richiedono che gli item abbiano un parametro \\(\\alpha\\) costante, ma permettono che gli item differiscano nelle loro posizioni. Nel modello di Rasch, questo parametro costante è fissato a \\(1.0\\), mentre nel modello 1PL, \\(\\alpha\\) non deve necessariamente essere uguale a \\(1.0\\). Matematicamente, i modelli 1PL e Rasch sono equivalenti e i valori di un modello possono essere trasformati nell’altro attraverso una riscalatura appropriata.\nTuttavia, per alcuni, il modello di Rasch rappresenta una prospettiva filosofica diversa da quella incarnata nel modello 1PL. Il modello 1PL si concentra sull’adattare i dati nel miglior modo possibile, data la struttura del modello. Al contrario, il modello di Rasch viene utilizzato per costruire la variabile di interesse (cfr. Andrich, 1988; Wilson, 2005; Wright, 1984; Wright & Masters, 1982; Wright & Stone, 1979). In breve, questa prospettiva sostiene che il modello di Rasch sia lo standard secondo il quale si può creare uno strumento per misurare una variabile. Questa visione è analoga a ciò che avviene nelle scienze fisiche. Ad esempio, consideriamo la misurazione del tempo. La misurazione del tempo implica un processo ripetitivo che segna incrementi uguali (cioè, unità) della variabile latente del tempo. Per misurare il tempo, dobbiamo definire la nostra unità (ad esempio, un periodo standard di oscillazione). Con il modello di Rasch, l’unità è definita come il logit, ossia la distanza sul nostro continuum che porta a un aumento del rapporto di successo di un fattore uguale alla costante trascendentale \\(e\\). Pertanto, analogamente alla misurazione del tempo, le nostre misurazioni con un modello a un parametro si basano sull’uso (ripetitivo) di un’unità che rimane costante nella nostra metrica.\nPer semplicità, nel seguito utilizzeremo il termine generale modello 1PL per riferirci sia alla situazione in cui \\(\\alpha = 1.0\\) (cioè, il modello di Rasch) sia alla situazione in cui \\(\\alpha\\) è uguale a qualche altra costante. Tuttavia, quando usiamo il termine modello di Rasch, ci riferiamo alla situazione in cui \\(\\alpha = 1.0\\) e a una filosofia di misurazione che afferma che il modello di Rasch sia la base per costruire la variabile di interesse.\n\nmirt_rm &lt;- mirt(responses, 1, \"Rasch\")\n\nIteration: 16, Log-Lik: -2816.487, Max-Change: 0.00008\n\n\n\n\n59.3.2 Discriminazione\nLa formula generale per una curva caratteristica dell’item (ICC) in un modello 1PL è:\n\\[\nP(\\theta) = \\frac{1}{1 + \\exp(-a \\cdot (\\theta - b))},\n\\]\ndove: - $ P() $ è la probabilità di una risposta corretta. - $ $ è l’abilità del rispondente. - $ a $ è il parametro di discriminazione dell’item, fisso per tutti gli item. - $ b $ è il parametro di difficoltà dell’item.\nNel modello di Rasch, il parametro di discriminazione \\(a\\) viene fissato a 1 per tutti gli item, per cui otteniamo\n\\[\nP(\\theta) = \\frac{1}{1 + \\exp(-(\\theta - b))}.\n\\]\nIl concetto di “discriminazione” nel contesto dei modelli di risposta all’item (IRT) si riferisce alla capacità di un item di distinguere tra individui con abilità leggermente superiore e quelli con abilità leggermente inferiore rispetto alla difficoltà dell’item stesso. Questa capacità discriminante è caratterizzata dalla pendenza della curva caratteristica dell’item (ICC) “nel mezzo”, ovvero per livelli di abilità vicini alla difficoltà dell’item, con discriminazioni più elevate che corrispondono a pendenze più ripide.\n\n# Definizione della funzione sigmoide per rappresentare le ICC\nsigmoid &lt;- function(x, beta) {\n    1 / (1 + exp(-beta * (x - 0)))\n}\n\n# Creazione di una sequenza di livelli di abilità\nabilities &lt;- seq(-2, 2, length.out = 100)\n\n# Calcolo delle probabilità per due item con diversi valori di discriminazione\n# Per l'item di sinistra (discriminazione inferiore, beta=1)\nprobabilities_left &lt;- sigmoid(abilities, beta = 1)\n\n# Per l'item di destra (discriminazione superiore, beta=2)\nprobabilities_right &lt;- sigmoid(abilities, beta = 2)\n\n# Creazione del dataframe per ggplot\ndata &lt;- data.frame(\n    Ability = rep(abilities, 2),\n    Probability = c(probabilities_left, probabilities_right),\n    Item = rep(c(\"Lower Discrimination (beta=1)\", \"Higher Discrimination (beta=2)\"), each = 100)\n)\n\n# Creazione del grafico\nggplot(data, aes(x = Ability, y = Probability, color = Item)) +\n    geom_line() +\n    scale_color_manual(values = c(\"blue\", \"orange\")) +\n    geom_point(\n        data = data.frame(\n            Ability = c(-0.5, 0.5, -0.5, 0.5),\n            Probability = c(sigmoid(-0.5, 1), sigmoid(0.5, 1), sigmoid(-0.5, 2), sigmoid(0.5, 2)),\n            Item = c(\"Lower Discrimination (beta=1)\", \"Lower Discrimination (beta=1)\", \"Higher Discrimination (beta=2)\", \"Higher Discrimination (beta=2)\")\n        ),\n        aes(x = Ability, y = Probability, color = Item)\n    ) +\n    ggtitle(\"Item Characteristic Curves (ICCs) for Different Discriminations\") +\n    xlab(\"Ability (theta)\") +\n    ylab(\"Probability of Correct Response\") \n\n\n\n\n\n\n\n\nConsideriamo due diverse Curve Caratteristiche dell’Item (ICC) per item con difficoltà zero. In questa funzione, sigmoid &lt;- function(x, beta) { 1 / (1 + exp(-beta * (x - 0))) }``, il parametrobeta` rappresenta la discriminazione dell’item, mentre la difficoltà dell’item è implicitamente fissata a zero (b = 0). Questa funzione, quindi, non modella direttamente variazioni nella difficoltà dell’item, ma solo nella discriminazione.\nLe ICC sono calcolate per due item con diversi livelli di discriminazione, rappresentati come “Lower Discrimination (beta=1)” e “Higher Discrimination (beta=2)” nel grafico. Le probabilità che persone con abilità di −0.5 e +0.5 risolvano correttamente l’item sono rispettivamente approssimativamente 0.38 e 0.62 per l’item con discriminazione inferiore (beta=1, rappresentato con la curva blu)\n\nsigmoid &lt;- function(x, beta) {\n    1 / (1 + exp(-beta * (x - 0)))\n}\nsigmoid(-0.5, 1)\nsigmoid(0.5, 1)\n\n0.377540668798145\n\n\n0.622459331201855\n\n\ne 0.27 e 0.73 per l’item con discriminazione superiore (beta=2, rappresentato con la curva arancione)\n\nsigmoid(-0.5, 2)\nsigmoid(0.5, 2)\n\n0.268941421369995\n\n\n0.731058578630005\n\n\nCiò indica che due persone con una differenza di 1 nella loro abilità mostrano una differenza di 0.24 nella loro probabilità di risolvere correttamente l’item con discriminazione inferiore (curva blu) e una differenza di 0.46 per l’item con discriminazione superiore (curva arancione).\nLa pendenza al centro dell’ICC, dove la curva è quasi lineare, può essere approssimata da questa differenza sull’asse y divisa per la differenza sull’asse x, risultando in circa 0.24 per l’item con discriminazione inferiore e 0.46 per quello con discriminazione superiore. In termini più semplici, la pendenza è una misura di quanto rapidamente la probabilità di una risposta corretta aumenta (o diminuisce) in relazione a un cambiamento nell’abilità dell’individuo. Una pendenza più ripida, come quella dell’item con discriminazione superiore (curva arancione), indica una maggiore discriminazione, cioè l’item è più efficace nel distinguere tra persone con livelli di abilità leggermente differenti.\nNel Modello di Rasch, si assume che tutti gli item abbiano la stessa discriminazione, con una pendenza di 1. Questo è evidente dalla formula del modello, dove l’unico parametro per ciascun item \\(i\\) è la sua difficoltà \\(\\beta_i\\), senza parametri aggiuntivi per modellare la discriminazione. Tuttavia, questo approccio è spesso considerato troppo rigido dal punto di vista della modellazione, poiché molti test reali contengono item con discriminazioni disuguali. In contrasto, il modello logistico a due parametri (2PL) di Birnbaum incorpora un parametro di pendenza, permettendo a ciascun item di avere una discriminazione diversa.\nIn sintesi, la discriminazione in un test psicometrico è un concetto chiave che descrive quanto efficacemente un item può differenziare tra rispondenti con abilità simili ma non identiche. Maggiore è la discriminazione, più efficacemente l’item può identificare le differenze sottili nelle abilità dei rispondenti.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>Oltre il Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_1pl_2pl_3pl.html#modello-irt-a-due-parametri",
    "href": "chapters/irt/05_1pl_2pl_3pl.html#modello-irt-a-due-parametri",
    "title": "59  Oltre il Modello di Rasch",
    "section": "59.4 Modello IRT a due parametri",
    "text": "59.4 Modello IRT a due parametri\nIl modello 2PL, noto come “Modello IRT a due parametri”, prevede che le curve caratteristiche degli item non siano tra loro parallele. Questo modello utilizza due parametri per descrivere le curve caratteristiche di ciascun item: il parametro di difficoltà \\(\\delta_i\\) e il parametro di discriminazione \\(\\alpha_i\\). Il parametro \\(\\alpha_i\\) consente alle curve caratteristiche di avere pendenze diverse e riflette la capacità discriminante dell’item rispetto alla variabile latente. Le curve caratteristiche nel modello 2PL hanno la seguente forma:\n\\[\n\\begin{equation}\n  Pr(X_{vi} = 1 \\mid \\theta_v, \\delta_i, \\alpha_i) = \\frac{\\exp(\\alpha_i(\\theta_v-\\delta_i))}{1+ \\exp(\\alpha_i(\\theta_v-\\delta_i))}.\n\\end{equation}\n\\] (eq-2pl)\nAdattiamo il modello 2PL ai dati.\n\nmirt_2pl &lt;- mirt(responses, 1, \"2PL\")\n\nIteration: 14, Log-Lik: -2759.601, Max-Change: 0.00009\n\n\nEsaminiamo le curve caratteristiche degli item.\n\nplot(mirt_2pl, type = \"trace\")\n\n\n\n\n\n\n\n\n\nplot(mirt_2pl, type = \"trace\", facet_items = FALSE)\n\n\n\n\n\n\n\n\nPossiamo visualizzare le stime dei parametri degli item per il 2PL utilizzando la funzione coef().\n\ncoef(mirt_2pl, IRTpars = TRUE, simplify = TRUE)\n\n\n    $items\n        \n\nA matrix: 14 x 4 of type dbl\n\n\n\na\nb\ng\nu\n\n\n\n\nI1\n1.1474247\n-1.02151843\n0\n1\n\n\nI2\n1.7687105\n-0.91065318\n0\n1\n\n\nI3\n1.3724569\n-1.67979991\n0\n1\n\n\nI6\n1.4786119\n-0.04254241\n0\n1\n\n\nI7\n1.0714345\n2.44143950\n0\n1\n\n\nI11\n1.5944811\n-0.95699944\n0\n1\n\n\nI12\n0.7033227\n1.07888714\n0\n1\n\n\nI14\n0.7707027\n-0.61243178\n0\n1\n\n\nI17\n0.7066769\n1.75774430\n0\n1\n\n\nI18\n0.7498268\n-0.50187180\n0\n1\n\n\nI19\n1.8311083\n1.45897744\n0\n1\n\n\nI21\n-0.2139602\n-7.07607206\n0\n1\n\n\nI22\n0.2769275\n7.65692247\n0\n1\n\n\nI23\n1.5212898\n-1.50340795\n0\n1\n\n\n\n\n\n    $means\n        F1: 0\n    $cov\n        \n\nA matrix: 1 x 1 of type dbl\n\n\n\nF1\n\n\n\n\nF1\n1\n\n\n\n\n\n\n\n\nEffettuiamo un confronto tra il modello di Rash e il modello 2PL.\n\nanova(mirt_rm, mirt_2pl)\n\n\nA mirt_df: 2 x 8\n\n\n\nAIC\nSABIC\nHQ\nBIC\nlogLik\nX2\ndf\np\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nmirt_rm\n5662.973\n5675.249\n5686.683\n5722.845\n-2816.487\nNA\nNA\nNA\n\n\nmirt_2pl\n5575.203\n5598.118\n5619.462\n5686.964\n-2759.601\n113.7703\n13\n0\n\n\n\n\n\nQuesto indica che il 2PL fornisce un adattamento significativamente migliore rispetto al modello di Rasch, suggerendo che gli item non hanno tutti la stessa pendenza.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>Oltre il Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_1pl_2pl_3pl.html#modello-irt-a-tre-parametri",
    "href": "chapters/irt/05_1pl_2pl_3pl.html#modello-irt-a-tre-parametri",
    "title": "59  Oltre il Modello di Rasch",
    "section": "59.5 Modello IRT a tre parametri",
    "text": "59.5 Modello IRT a tre parametri\nPer considerare l’eventuale tendenza dei rispondenti a indovinare, i modelli IRT introducono un ulteriore parametro, denotato con \\(\\gamma_i\\). Il modello a tre parametri (3PL) assume la seguente forma:\n\\[\n\\begin{equation}\nPr(X_{vi} = 1 \\mid \\theta_v, \\delta_i, \\alpha_i, \\gamma_i) = \\gamma_i + (1-\\gamma_i) \\frac{\\exp(\\alpha_i(\\theta_v-\\delta_i))}{1 + \\exp(\\alpha_i(\\theta_v-\\delta_i))}.\n\\end{equation}\n\\] (eq-3pl)\nIl parametro \\(\\gamma_i\\) ha l’effetto di introdurre un asintoto orizzontale maggiore di zero per valori di \\(\\theta_v\\) tendenti a \\(-\\infty\\). In altre parole, per item con \\(\\gamma_i = 0.25\\), la probabilità di risposta corretta dovuta al caso è almeno pari a 0.25, anche per i livelli di abilità latente più bassi.\n\nmirt_3pl &lt;- mirt(responses, 1, \"3PL\")\n\nIteration: 41, Log-Lik: -2740.210, Max-Change: 0.00009\n\n\nEsaminiamo le curve caratteristiche degli item.\n\nplot(mirt_3pl, type = \"trace\", facet_items = FALSE)\n\n\n\n\n\n\n\n\nUn elemento chiave del modello 3PL è che l’asintoto inferiore, rappresentato dal parametro \\(\\gamma_i\\), assume un valore maggiore di zero. Questo implica che la probabilità di una risposta corretta, rappresentata da \\(\\gamma_i\\), risulta essere superiore a 0.5 per valori relativamente bassi di abilità latente. In altre parole, gli item con \\(\\gamma_i &gt; 0\\) forniscono una probabilità di risposta corretta più alta anche per rispondenti con livelli di abilità latente relativamente bassi.\nCiò comporta che tali item risultano essere più facili per i rispondenti con abilità latente inferiore, in confronto agli item con \\(\\gamma_i = 0\\). Questa caratteristica dei modelli 3PL permette di includere il tasso di guessing nella probabilità di risposta corretta e offre una maggiore flessibilità nella modellazione del comportamento degli item rispetto al modello a due parametri (2PL), nel quale l’asintoto inferiore è pari a zero.\nA negative slope indicates that, all else being equal, a person with a higher value on the latent trait is less likely to give a positive response than a person with a lower value. What a negative slope estimate means for test construction depends on context. For achievement tests, a negative slope indicates that a test item is not behaving like it should. For other psychological tests, such as personality or attitude scales, nega- tive slopes may be intended for some of the items. In this setting, test designers routinely use negatively worded items. A negatively worded version of “I like long walks on the beach”, for example, might be “I do not like long walks on the beach”. A person agreeing with the first would ideally disagree with the second3, so we could legitimately get a negative slope when the version we use is in the opposite direction to the rest of the items.\nPossiamo visualizzare le stime dei parametri degli item per il 3PL utilizzando la funzione coef().\n\ncoef(mirt_3pl, IRTpars = TRUE, simplify = TRUE)\n\n\n    $items\n        \n\nA matrix: 14 x 4 of type dbl\n\n\n\na\nb\ng\nu\n\n\n\n\nI1\n1.4097712\n-0.493210687\n2.441664e-01\n1\n\n\nI2\n2.6665125\n-0.403577318\n2.909004e-01\n1\n\n\nI3\n1.4449658\n-1.632526461\n1.224781e-04\n1\n\n\nI6\n1.5135421\n0.008607129\n1.949812e-02\n1\n\n\nI7\n1.1634394\n2.297031163\n1.177019e-05\n1\n\n\nI11\n1.4873719\n-0.983622151\n1.450244e-05\n1\n\n\nI12\n2.1309230\n1.275060454\n2.074352e-01\n1\n\n\nI14\n0.8047002\n-0.585743232\n6.144279e-05\n1\n\n\nI17\n3.0758348\n1.319978270\n1.380818e-01\n1\n\n\nI18\n0.7239776\n-0.510317344\n4.102312e-05\n1\n\n\nI19\n1.8437010\n1.448606523\n3.537302e-06\n1\n\n\nI21\n-5.2635776\n-2.270615508\n1.671759e-01\n1\n\n\nI22\n9.1791590\n1.989994379\n8.967797e-02\n1\n\n\nI23\n1.6252401\n-1.452268207\n4.396566e-05\n1\n\n\n\n\n\n    $means\n        F1: 0\n    $cov\n        \n\nA matrix: 1 x 1 of type dbl\n\n\n\nF1\n\n\n\n\nF1\n1\n\n\n\n\n\n\n\n\nEffettuiamo il confronto tra i modelli 2PL e 3PL.\n\nanova(mirt_2pl, mirt_3pl)\n\n\nA mirt_df: 2 x 8\n\n\n\nAIC\nSABIC\nHQ\nBIC\nlogLik\nX2\ndf\np\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nmirt_2pl\n5575.203\n5598.118\n5619.462\n5686.964\n-2759.601\nNA\nNA\nNA\n\n\nmirt_3pl\n5564.419\n5598.792\n5630.808\n5732.061\n-2740.210\n38.78358\n14\n0.0003938891\n\n\n\n\n\nIl modello 2PL deriva dal modello 3PL impostando un tipo specifico di parametro, ovvero il parametro di guessing, a un valore di 0. Il parametro di guessing nel modello 3PL rappresenta la probabilità che un candidato indovini correttamente un item, anche se non possiede la competenza richiesta per rispondere. Poiché il parametro di guessing può assumere valori tra 0 e 1, ciò significa che il modello ristretto nella nostra analisi, che è il modello 2PL, deriva dal modello più generale impostando un parametro a un valore ai limiti del suo spazio parametrico. In questo scenario, il classico test del rapporto di verosimiglianza può portare a risultati inaccurati.\nPertanto, per interpretare il confronto tra i due modelli utilizziamo solo gli indici di informazione per confrontare il modello 2PL e il modello 3PL. Mentre l’AIC indica una preferenza per il modello 3PL (forse a causa del guessing presente in alcuni item), il BIC mostra una lieve preferenza per il modello 2PL più semplice, poiché penalizza maggiormente l’uso di un maggior numero di parametri nel modello.\n\n59.5.1 Bontà dell’Adattamento\nLe precedenti comparazioni mostrano che il 3PL fornisce una rappresentazione migliore dei dati rispetto ai modelli Rasch o 2PL. Tuttavia, ciò non ci dice se la rappresentazione fornita dal modello 3PL sia ragionevole, ma semplicemente che è la più ragionevole tra i modelli che abbiamo confrontato. Possiamo ulteriormente testare l’adattamento di questo modello utilizzando la statistica M2, fornita dalla funzione M2() in mirt. Le prime tre colonne dell’output forniscono la statistica del test, i gradi di libertà e il valore p per la statistica M2. In questo caso, il valore p testa l’ipotesi nulla secondo cui le somme osservate per righe e colonne sono coerenti con il modello 3PL. In questo caso, il valore p è superiore al 5%, il che significa che il test non fornisce evidenza che il 3PL non possa rappresentare i dati osservati.\n\nM2(mirt_3pl)\n\n\nA data.frame: 1 x 9\n\n\n\nM2\ndf\np\nRMSEA\nRMSEA_5\nRMSEA_95\nSRMSR\nTLI\nCFI\n\n\n\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nstats\n76.06463\n63\n0.1249838\n0.02279774\n0\n0.03943958\n0.04533455\n0.975068\n0.9827394\n\n\n\n\n\nOltre alla statistica M2, la funzione M2() stampa anche l’errore quadratico medio di approssimazione (RMSEA, insieme a un intervallo di confidenza del 90%) e il residuo quadratico medio standardizzato (SRMR), così come altri due indici di adattamento dalla statistica multivariata basati sulla statistica del test χ2. La funzione M2() calcola ciascuna di queste statistiche, sostituendo χ2 con il suo corrispondente a informazione limitata, M2, come spiegato nella Sezione 4.3.1 e Sezione 4.3.2.\nLa statistica più comunemente riportata tra queste è il RMSEA. Un modello è considerato di fornire una buona rappresentazione dei dati del test quando il limite superiore dell’intervallo di confidenza è inferiore a 0.05. In questo caso, il limite superiore è circa 0.039, suggerendo che il modello fornisce una buona rappresentazione dei dati.\nÈ anche possibile ottenere un numero di altre statistiche di adattamento degli item tramite l’argomento fit_stats della funzione itemfit():\n\nset.seed(1234)\nitemfit(mirt_3pl, fit_stats = \"PV_Q1\")\n\n\nA mirt_df: 14 x 5\n\n\nitem\nPV_Q1\ndf.PV_Q1\nRMSEA.PV_Q1\np.PV_Q1\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nI1\n8.206189\n7\n0.02078128\n0.31476412\n\n\nI2\n13.072024\n7\n0.04662634\n0.07037285\n\n\nI3\n9.077944\n7\n0.02727605\n0.24710836\n\n\nI6\n8.956994\n7\n0.02647032\n0.25575849\n\n\nI7\n7.995062\n7\n0.01887512\n0.33302949\n\n\nI11\n16.205815\n7\n0.05741107\n0.02330110\n\n\nI12\n8.924171\n7\n0.02624740\n0.25814623\n\n\nI14\n9.732213\n7\n0.03127676\n0.20426338\n\n\nI17\n13.384634\n7\n0.04781152\n0.06327324\n\n\nI18\n8.254458\n7\n0.02119301\n0.31069184\n\n\nI19\n9.125313\n7\n0.02758519\n0.24378391\n\n\nI21\n9.636107\n7\n0.03072176\n0.21014965\n\n\nI22\n15.522847\n7\n0.05524041\n0.02985173\n\n\nI23\n9.169654\n7\n0.02787146\n0.24070406\n\n\n\n\n\nIn questo modo viene calcolata la statistica PV_Q1, una statistica avanzata che fornisce una misura accurata di quanto bene un item specifico si adatta a un modello IRT, tenendo conto dell’incertezza nelle stime dei parametri delle persone e offrendo l’opzione di un calcolo più accurato attraverso il bootstrap.\nGli indici infit e outfit si ottengono nel modo seguente:\n\nitemfit(mirt_3pl, fit_stats = \"infit\", method = \"ML\")\n\nWarning message:\n\"The following factor score estimates failed to converge successfully:\n    127,128,169,180,189,233,263,298\"\n\n\n\nA mirt_df: 14 x 5\n\n\nitem\noutfit\nz.outfit\ninfit\nz.infit\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nI1\n1.0053224\n0.08416827\n0.9828264\n-0.2734579\n\n\nI2\n0.8650919\n-0.36666653\n0.8733259\n-2.0161598\n\n\nI3\n0.8763686\n-0.40387987\n0.9089194\n-0.9403723\n\n\nI6\n0.9807241\n-0.14508359\n0.9155032\n-1.5032687\n\n\nI7\n0.7829731\n-0.79735536\n0.8957264\n-0.8992810\n\n\nI11\n1.4621993\n2.38315999\n0.8855767\n-1.5671665\n\n\nI12\n0.9450892\n-0.92931105\n0.9415297\n-1.1574013\n\n\nI14\n0.9674548\n-0.56489989\n1.0089689\n0.2225748\n\n\nI17\n0.7836005\n-2.45664322\n0.8073054\n-2.7086428\n\n\nI18\n1.0252274\n0.53910770\n1.0130304\n0.3486484\n\n\nI19\n0.5897564\n-1.39294237\n0.8156508\n-2.1647249\n\n\nI21\n0.9151478\n-0.98342248\n0.9170739\n-0.9629679\n\n\nI22\n0.8604586\n-0.97831569\n0.8436097\n-1.1595145\n\n\nI23\n0.7780428\n-0.73611602\n0.8723068\n-1.3697710",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>Oltre il Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_1pl_2pl_3pl.html#invarianza-di-gruppo-dei-parametri-degli-item-nella-irt",
    "href": "chapters/irt/05_1pl_2pl_3pl.html#invarianza-di-gruppo-dei-parametri-degli-item-nella-irt",
    "title": "59  Oltre il Modello di Rasch",
    "section": "59.6 Invarianza di Gruppo dei Parametri degli Item nella IRT",
    "text": "59.6 Invarianza di Gruppo dei Parametri degli Item nella IRT\nUna caratteristica rilevante della teoria della IRT è l’invarianza dei parametri degli item rispetto al livello di abilità degli esaminandi che rispondono agli item. Ciò significa che i parametri degli item sono ciò che si definisce “invarianti di gruppo”. Questo aspetto della teoria può essere spiegato come segue.\nImmaginiamo due gruppi di esaminandi, entrambi estratti dalla stessa popolazione. Il primo gruppo presenta punteggi di abilità variabili da -3 a -1, con una media di -2, mentre il secondo gruppo ha punteggi che vanno da +1 a +3, con una media di +2. Si calcola la proporzione osservata di risposte corrette a un determinato item, per ogni livello di abilità, in entrambi i gruppi. Utilizzando la procedura di stima a massima verosimiglianza, si adatta una curva caratteristica dell’item ai dati, ottenendo stime dei parametri dell’item, ad esempio, b = 0.39 e a = 1.27. La curva caratteristica dell’item definita da queste stime viene poi tracciata per l’intervallo di abilità del primo gruppo.\n\ngroupinv &lt;- function(mdl, t1l, t1u, t2l, t2u) {\n    if (missing(t1l)) t1l &lt;- -3\n    if (missing(t1u)) t1u &lt;- -1\n    if (missing(t2l)) t2l &lt;- 1\n    if (missing(t2u)) t2u &lt;- 3\n    theta &lt;- seq(-3, 3, .1875)\n    f &lt;- rep(21, length(theta))\n    wb &lt;- round(runif(1, -3, 3), 2)\n    wa &lt;- round(runif(1, 0.2, 2.8), 2)\n    wc &lt;- round(runif(1, 0, .35), 2)\n    if (mdl == 1 | mdl == 2) {\n        wc &lt;- 0\n    }\n    if (mdl == 1) {\n        wa &lt;- 1\n    }\n    for (g in 1:length(theta)) {\n        P &lt;- wc + (1 - wc) / (1 + exp(-wa * (theta - wb)))\n    }\n    p &lt;- rbinom(length(theta), f, P) / f\n    lowerg1 &lt;- 0\n    for (g in 1:length(theta)) {\n        if (theta[g] &lt;= t1l) {\n            lowerg1 &lt;- lowerg1 + 1\n        }\n    }\n    upperg1 &lt;- 0\n    for (g in 1:length(theta)) {\n        if (theta[g] &lt;= t1u) {\n            upperg1 &lt;- upperg1 + 1\n        }\n    }\n    theta1 &lt;- theta[lowerg1:upperg1]\n    p1 &lt;- p[lowerg1:upperg1]\n    lowerg2 &lt;- 0\n    for (g in 1:length(theta)) {\n        if (theta[g] &lt;= t2l) {\n            lowerg2 &lt;- lowerg2 + 1\n        }\n    }\n    upperg2 &lt;- 0\n    for (g in 1:length(theta)) {\n        if (theta[g] &lt;= t2u) {\n            upperg2 &lt;- upperg2 + 1\n        }\n    }\n    theta2 &lt;- theta[lowerg2:upperg2]\n    p2 &lt;- p[lowerg2:upperg2]\n    theta12 &lt;- c(theta1, theta2)\n    p12 &lt;- c(p1, p2)\n    par(lab = c(7, 5, 3))\n    plot(theta12, p12,\n        xlim = c(-3, 3), ylim = c(0, 1),\n        xlab = \"Ability\", ylab = \"Probability of Correct Response\"\n    )\n    if (mdl == 1) {\n        maintext &lt;- paste(\"Pooled Groups\", \"\\n\", \"b=\", wb)\n    }\n    if (mdl == 2) {\n        maintext &lt;- paste(\"Pooled Groups\", \"\\n\", \"a=\", wa, \"b=\", wb)\n    }\n    if (mdl == 3) {\n        maintext &lt;- paste(\n            \"Pooled Groups\", \"\\n\",\n            \"a=\", wa, \"b=\", wb, \"c=\", wc\n        )\n    }\n    par(new = \"T\")\n    plot(theta, P,\n        xlim = c(-3, 3), ylim = c(0, 1), type = \"l\",\n        xlab = \"\", ylab = \"\", main = maintext\n    )\n}\n\n\nset.seed(1)\ngroupinv(1, -3, -1, 1, 3)\n\n\n\n\n\n\n\n\nIl punto fondamentale è che gli stessi valori dei parametri a e b si otterrebbero applicando il modello IRT esclusivamente ai dati del primo o del secondo gruppo. Di conseguenza, i parametri dell’item sono invarianti di gruppo. Questa invarianza è una caratteristica potente della teoria della risposta all’item, indicando che i valori dei parametri dell’item sono una proprietà dell’item stesso, non del gruppo che ha risposto all’item. Questo si contrappone alla teoria classica dei test, dove la difficoltà di un item è determinata dalla proporzione complessiva di risposte corrette da parte di un gruppo di esaminandi.\nSebbene i parametri degli item - definiti come attributi intrinseci degli item indipendentemente dalle popolazioni di esaminandi - siano teoricamente invarianti tra gruppi diversi estratti dalla medesima popolazione, le stime ottenute di tali parametri attraverso procedure di massima verosimiglianza possono differire a causa della variabilità campionaria.\nIn termini statistici, la stima dei parametri degli item per un dato campione è uno stimatore dei parametri reali della popolazione, ma è soggetta a errore campionario. L’errore campionario è direttamente proporzionale alla dimensione del campione e alla varianza dei dati all’interno del campione. Pertanto, benché i valori reali dei parametri degli item siano costanti tra i gruppi estratti dalla stessa popolazione, le stime campionarie di questi parametri mostreranno una distribuzione attorno al vero valore popolazionale, con una variabilità che riflette l’errore campionario.\nInoltre, è fondamentale che l’item venga utilizzato per misurare lo stesso tratto latente in entrambi i gruppi. I parametri di un item non mantengono l’invarianza di gruppo se utilizzati fuori contesto, ovvero per misurare un tratto latente diverso, con esaminandi da una popolazione non adeguata, o se i due gruppi provengono da popolazioni diverse.\nL’invarianza di gruppo dei parametri degli item illustra anche una caratteristica fondamentale della curva caratteristica dell’item. Come affermato nei capitoli precedenti, questa curva rappresenta la relazione tra la probabilità di risposta corretta all’item e la scala di abilità. Il principio di invarianza riflette questo, poiché i parametri dell’item sono indipendenti dalla distribuzione degli esaminandi sulla scala di abilità. Da un punto di vista pratico, ciò significa che i parametri dell’intera curva caratteristica dell’item possono essere stimati da qualsiasi segmento della curva.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>Oltre il Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_1pl_2pl_3pl.html#la-curva-caratteristica-del-test-nella-irt",
    "href": "chapters/irt/05_1pl_2pl_3pl.html#la-curva-caratteristica-del-test-nella-irt",
    "title": "59  Oltre il Modello di Rasch",
    "section": "59.7 La Curva Caratteristica del Test nella IRT",
    "text": "59.7 La Curva Caratteristica del Test nella IRT\nLa Curva Caratteristica del Test (CCT) è un concetto fondamentale nella IRT che rappresenta la relazione tra il punteggio vero medio previsto (TS, True Score) di un campione di rispondenti e i vari livelli di abilità latente (\\(\\theta\\)) nel campione.\n\n59.7.0.1 Definizione della CCT\nPer un campione di rispondenti, la CCT è definita come la somma delle probabilità medie di risposta corretta a ciascun item del test, a vari livelli di abilità. Matematicamente, è espressa come:\n\\[ TS(\\theta) = \\sum_{j=1}^{J} P_j(\\theta), \\]\ndove: - $ TS() $ è il punteggio vero medio per i rispondenti con un livello di abilità \\(\\theta\\), - $ J $ è il numero totale di item nel test, - $ P_j() $ è la probabilità media di risposta corretta all’item $ j $ per un livello di abilità \\(\\theta\\), che dipende dal modello specifico di curva caratteristica dell’item utilizzato.\n\n\n59.7.0.2 Esempio in R\nPer illustrare la CCT, consideriamo un test di quattro item. Calcoliamo la probabilità media di risposta corretta per ciascun item a vari livelli di abilità utilizzando il modello a due parametri. I parametri di discriminazione (\\(a_j\\)) e difficoltà (\\(b_j\\)) per questi item sono:\n\nItem 1: \\(a_1 = 0.5\\), \\(b_1 = -1.0\\)\nItem 2: \\(a_2 = 1.2\\), \\(b_2 = 0.75\\)\nItem 3: \\(a_3 = 0.8\\), \\(b_3 = 0.0\\)\nItem 4: \\(a_4 = 1.0\\), \\(b_4 = 0.5\\)\n\nLa formula per la probabilità media di risposta corretta è:\n\\[ P_j(\\theta) = \\frac{1}{1 + \\exp[-a_j(\\theta - b_j)]}. \\]\nUtilizzando R, calcoliamo le probabilità medie per ogni item a vari livelli di abilità e poi sommiamo queste probabilità per ottenere il punteggio vero medio.\n\n# Definizione dei parametri per ciascun item\na &lt;- c(0.5, 1.2, 0.8, 1.0) # Parametri di discriminazione\nb &lt;- c(-1.0, 0.75, 0.0, 0.5) # Parametri di difficoltà\n\n# Funzione per calcolare la probabilità media di risposta corretta\nprob_correct &lt;- function(theta, a, b) {\n    1 / (1 + exp(-a * (theta - b)))\n}\n\n# Vettore di livelli di abilità (theta) da esplorare\ntheta_values &lt;- seq(-3, 3, by = 0.1)\n\n# Calcolo dei punteggi veri medi per ogni livello di abilità\ntrue_scores &lt;- sapply(theta_values, function(theta) {\n    sum(prob_correct(theta, a, b))\n})\n\n# Creazione di un data frame per il plotting\ndf &lt;- data.frame(Theta = theta_values, TrueScore = true_scores)\n\n# Creazione del grafico della CCT\nggplot(df, aes(x = Theta, y = TrueScore)) +\n    geom_line(color = \"blue\") +\n    labs(\n        title = \"Curva Caratteristica del Test (CCT)\",\n        x = \"Livello di Abilita' (theta)\",\n        y = \"Punteggio Vero Medio\"\n    ) \n\n\n\n\n\n\n\n\nLa CCT risultante mostra come il punteggio vero medio varia al variare del livello di abilità in un campione di rispondenti. Essa fornisce informazioni preziose sull’efficacia e sulla difficoltà del test, rivelando come il test nel suo complesso discrimina tra individui a diversi livelli di abilità. Questo rende la CCT uno strumento cruciale nella valutazione e nell’interpretazione dei risultati dei test basati sulla IRT.\n\n\n59.7.1 Caratterizzazione di un Test Secondo la IRT\nNella pratica attuale, molti test sono progettati secondo i principi della teoria classica dei test, ma analizzati con metodi della IRT. Per ottimizzare l’uso dell’IRT, è essenziale che la progettazione, costruzione, analisi e interpretazione dei test avvengano interamente all’interno del suo quadro teorico. Questo richiede un’armonizzazione tra la creazione del test e le procedure analitiche dell’IRT, oltrepassando il tradizionale approccio basato sulla teoria classica dei test.\n\n59.7.1.1 Processo di Precalibrazione degli Item\n\nDefinizione del Tratto Latente: È cruciale identificare con precisione il tratto latente (come la competenza matematica) che i test intendono misurare.\nCreazione e Selezione degli Item: Gli item vengono sviluppati per misurare il tratto latente e selezionati per l’inclusione in un pool di item. La selezione si basa sia sul contenuto che sulle caratteristiche tecniche degli item.\nTest Pilota e Analisi dei Dati: Gli item vengono somministrati a un campione pilota e i dati raccolti vengono analizzati per determinare i parametri degli item, come difficoltà e discriminazione.\nCalibrazione degli Item: Utilizzando un modello di IRT, gli item vengono calibrati in base ai dati raccolti, con la stima dei parametri di difficoltà e discriminazione.\nStabilimento della Scala di Abilità: Qui si entra nel cuore della precalibrazione: i parametri degli item vengono espressi in termini di una metrica di scala di abilità stabilita e conosciuta. Questo passaggio permette di avere un riferimento comune per la selezione degli item nei test futuri. In pratica, ciò significa che gli item sono valutati e classificati secondo una scala uniforme, facilitando la selezione degli item più adatti per specifici obiettivi di test.\nCostruzione del Test da un Item Pool Precalibrato: Gli item selezionati dall’item pool precalibrato permettono di assemblare un test in cui le caratteristiche tecniche (come la TCC e la TIF) possono essere predette prima della somministrazione effettiva del test.\n\n\n\n59.7.1.2 Vantaggi della Precalibrazione\n\nEfficienza: Risparmio di tempo e risorse, riducendo la necessità di pilotare ogni nuova versione del test.\nPrecisione: Capacità di scegliere gli item in base ai requisiti specifici del test.\nFlessibilità: Facilità nella creazione di versioni multiple del test o test adattivi.\n\n\n\n59.7.1.3 Obiettivi Specifici dei Test\n\nTest di Screening: Progettati per distinguere tra esaminandi sotto e sopra un livello di abilità specifico. La TCC e la TIF sono ottimizzate per questo scopo.\nTest a Gamma Ampia: Mirati a misurare l’abilità su un’ampia gamma della scala di abilità, con la TCC che riflette questa ampiezza.\nTest con Picco: Focalizzati su una specifica regione della scala di abilità, misurando l’abilità efficacemente in quella regione e meno accuratamente al di fuori di essa.\n\n\n\n59.7.1.4 Sperimentazione e Personalizzazione\nI costruttori di test possono sperimentare con diverse combinazioni di obiettivi di test, modelli di curva caratteristica degli item e numero di item per conseguire TCC e TIF ottimali. L’obiettivo è la creazione di test che misurino in modo preciso e coerente il tratto latente di interesse, adattandosi agli scopi specifici per cui il test è stato progettato.\nIn conclusione, la costruzione di test nell’ambito della IRT richiede un approccio integrato, che consideri la fase di precalibrazione degli item e la fase di selezione e combinazione di questi per creare test che soddisfino obiettivi specifici, garantendo così misurazioni accurate e utili.\n\nmathdata &lt;- rio::import(\"../../data/mathdata.txt\")\nmathdata |&gt; head()\n\n\nA data.frame: 6 x 5\n\n\n\nV1\nV2\nV3\nV4\nV5\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n1\n1\n0\n0\n0\n\n\n2\n1\n1\n1\n0\n0\n\n\n3\n1\n0\n0\n0\n0\n\n\n4\n1\n1\n1\n0\n0\n\n\n5\n1\n0\n1\n1\n0\n\n\n6\n1\n1\n1\n0\n0\n\n\n\n\n\n\ndim(mathdata)\n\n\n196015\n\n\n\nrasch &lt;- mirt(mathdata, 1, \"Rasch\", SE = TRUE, SE.type = \"Fisher\")\nTwoPL &lt;- mirt(mathdata, 1, \"2PL\", SE = TRUE, SE.type = \"Fisher\")\n\nIteration: 38, Log-Lik: -55387.029, Max-Change: 0.00009\n\nCalculating information matrix...\nIteration: 20, Log-Lik: -55198.496, Max-Change: 0.00007\n\nCalculating information matrix...\n\n\n\nM2(rasch, CI = 0.95) # Maydeu-Olivares & Joe statistic\n\n\nA data.frame: 1 x 9\n\n\n\nM2\ndf\np\nRMSEA\nRMSEA_2.5\nRMSEA_97.5\nSRMSR\nTLI\nCFI\n\n\n\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nstats\n428.241\n9\n0\n0.04875089\n0.04411739\n0.05349489\n0.04211215\n0.9597483\n0.9637735\n\n\n\n\n\n\nM2(TwoPL, CI = 0.95) # Maydeu-Olivares & Joe statistic\n\n\nA data.frame: 1 x 9\n\n\n\nM2\ndf\np\nRMSEA\nRMSEA_2.5\nRMSEA_97.5\nSRMSR\nTLI\nCFI\n\n\n\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nstats\n57.02138\n5\n5.005907e-11\n0.02303976\n0.01685003\n0.02961753\n0.01336253\n0.9910097\n0.9955048\n\n\n\n\n\n\nprint(itemfit(TwoPL, fit_stats = \"S_X2\"), digits = 5)\n\n  item     S_X2 df.S_X2 RMSEA.S_X2  p.S_X2\n1   V1 80.50442       2    0.04475 0.00000\n2   V2 19.40097       2    0.02107 0.00006\n3   V3 13.66416       2    0.01725 0.00108\n4   V4 65.60787       2    0.04028 0.00000\n5   V5 34.67477       2    0.02887 0.00000\n\n\n\nitemfit(TwoPL, S_X2.tables = T, empirical.table = 1)\n\n\nA mirt_df: 5 x 5\n\n\nitem\nS_X2\ndf.S_X2\nRMSEA.S_X2\np.S_X2\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nV1\n80.50442\n2\n0.04475113\n3.301323e-18\n\n\nV2\n19.40097\n2\n0.02106899\n6.125392e-05\n\n\nV3\n13.66416\n2\n0.01724979\n1.078610e-03\n\n\nV4\n65.60787\n2\n0.04028213\n5.668037e-15\n\n\nV5\n34.67477\n2\n0.02887110\n2.954394e-08\n\n\n\n\n\n\nitemfit(TwoPL, group.bins = 10, empirical.plot = 1, empirical.CI = 0)\n\n\n\n\n\n\n\n\n\nitemfit(TwoPL,\n    group.bins = 6, empirical.plot = 1,\n    empirical.CI = 0\n)",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>Oltre il Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_1pl_2pl_3pl.html#considerazioni-conclusive",
    "href": "chapters/irt/05_1pl_2pl_3pl.html#considerazioni-conclusive",
    "title": "59  Oltre il Modello di Rasch",
    "section": "59.8 Considerazioni Conclusive",
    "text": "59.8 Considerazioni Conclusive\nIl pacchetto mirt (Chalmers, 2021) offre funzionalità per l’adattamento di una varietà di modelli IRT, inclusi i modelli Rasch, 2PL e 3PL, nonché diversi modelli per risposte politomiche utilizzando la massima verosimiglianza marginale. In questo capitolo, abbiamo esplorato come sia possibile confrontare i modelli 1PL, 2PL e 3PL, che sono stati adattati ai dati usando mirt, attraverso il test del rapporto di verosimiglianza e gli indici di informazione.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>Oltre il Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_1pl_2pl_3pl.html#session-info",
    "href": "chapters/irt/05_1pl_2pl_3pl.html#session-info",
    "title": "59  Oltre il Modello di Rasch",
    "section": "59.9 Session Info",
    "text": "59.9 Session Info\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] grid      stats4    stats     graphics  grDevices utils     datasets \n[8] methods   base     \n\nother attached packages:\n [1] latex2exp_0.9.6   psychotools_0.7-4 ggmirt_0.1.0      TAM_4.2-21       \n [5] CDM_8.2-6         mvtnorm_1.3-1     mirt_1.42         lattice_0.22-6   \n [9] eRm_1.0-6         ggokabeito_0.1.0  viridis_0.6.5     viridisLite_0.4.2\n[13] ggpubr_0.6.0      ggExtra_0.10.1    bayesplot_1.11.1  gridExtra_2.3    \n[17] patchwork_1.3.0   semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-18    \n[21] psych_2.4.6.26    scales_1.3.0      markdown_1.13     knitr_1.48       \n[25] lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n[29] purrr_1.0.2       readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n[33] ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] splines_4.4.1        later_1.3.2          pbdZMQ_0.3-13       \n  [4] R.oo_1.26.0          XML_3.99-0.17        rpart_4.1.23        \n  [7] lifecycle_1.0.4      rstatix_0.7.2        rprojroot_2.0.4     \n [10] globals_0.16.3       MASS_7.3-61          rockchalk_1.8.157   \n [13] backports_1.5.0      magrittr_2.0.3       openxlsx_4.2.7.1    \n [16] Hmisc_5.1-3          rmarkdown_2.28       httpuv_1.6.15       \n [19] qgraph_1.9.8         zip_2.3.1            sessioninfo_1.2.2   \n [22] pbapply_1.7-2        minqa_1.2.8          multcomp_1.4-26     \n [25] abind_1.4-8          audio_0.1-11         quadprog_1.5-8      \n [28] R.utils_2.12.3       nnet_7.3-19          TH.data_1.1-2       \n [31] sandwich_3.1-1       listenv_0.9.1        testthat_3.2.1.1    \n [34] RPushbullet_0.3.4    vegan_2.6-8          arm_1.14-4          \n [37] parallelly_1.38.0    permute_0.9-7        codetools_0.2-20    \n [40] tidyselect_1.2.1     farver_2.1.2         lme4_1.1-35.5       \n [43] base64enc_0.1-3      jsonlite_1.8.9       polycor_0.8-1       \n [46] progressr_0.14.0     Formula_1.2-5        survival_3.7-0      \n [49] emmeans_1.10.4       tools_4.4.1          rio_1.2.2           \n [52] snow_0.4-4           Rcpp_1.0.13          glue_1.7.0          \n [55] mnormt_2.1.1         admisc_0.36          xfun_0.47           \n [58] mgcv_1.9-1           IRdisplay_1.1        withr_3.0.1         \n [61] beepr_2.0            fastmap_1.2.0        boot_1.3-31         \n [64] fansi_1.0.6          digest_0.6.37        mi_1.1              \n [67] timechange_0.3.0     R6_2.5.1             mime_0.12           \n [70] estimability_1.5.1   colorspace_2.1-1     gtools_3.9.5        \n [73] jpeg_0.1-10          R.methodsS3_1.8.2    utf8_1.2.4          \n [76] generics_0.1.3       data.table_1.16.0    corpcor_1.6.10      \n [79] SimDesign_2.17.1     htmlwidgets_1.6.4    pkgconfig_2.0.3     \n [82] sem_3.1-16           gtable_0.3.5         brio_1.1.5          \n [85] htmltools_0.5.8.1    carData_3.0-5        png_0.1-8           \n [88] rstudioapi_0.16.0    tzdb_0.4.0           reshape2_1.4.4      \n [91] uuid_1.2-1           coda_0.19-4.1        checkmate_2.3.2     \n [94] nlme_3.1-166         curl_5.2.3           nloptr_2.1.1        \n [97] repr_1.1.7           zoo_1.8-12           parallel_4.4.1      \n[100] miniUI_0.1.1.1       foreign_0.8-87       pillar_1.9.0        \n[103] vctrs_0.6.5          promises_1.3.0       car_3.1-2           \n[106] OpenMx_2.21.12       xtable_1.8-4         Deriv_4.1.6         \n[109] cluster_2.1.6        dcurver_0.9.2        GPArotation_2024.3-1\n[112] htmlTable_2.4.3      evaluate_1.0.0       pbivnorm_0.6.0      \n[115] cli_3.6.3            kutils_1.73          compiler_4.4.1      \n[118] rlang_1.1.4          crayon_1.5.3         future.apply_1.11.2 \n[121] ggsignif_0.6.4       labeling_0.4.3       fdrtool_1.2.18      \n[124] plyr_1.8.9           stringi_1.8.4        munsell_0.5.1       \n[127] lisrelToR_0.3        pacman_0.5.1         Matrix_1.7-0        \n[130] IRkernel_1.3.2       hms_1.1.3            glasso_1.11         \n[133] future_1.34.0        shiny_1.9.1          igraph_2.0.3        \n[136] broom_1.0.6          RcppParallel_5.1.9",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>Oltre il Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/06_implementation.html",
    "href": "chapters/irt/06_implementation.html",
    "title": "61  Implementazione",
    "section": "",
    "text": "61.1 Introduzione\nIn questo capitolo esamineremo il tutorial di Debelak, Strobl, e Zeigenfuse (2022).",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/06_implementation.html#un-esempio-pratico",
    "href": "chapters/irt/06_implementation.html#un-esempio-pratico",
    "title": "61  Implementazione",
    "section": "61.2 Un esempio pratico",
    "text": "61.2 Un esempio pratico\nIl set di dati data.fims.Aus.Jpn.scored contiene le risposte valutate per un sottoinsieme di item da parte di studenti australiani e giapponesi nello studio “First International Mathematics Study” (FIMS, Husén, 1967).\n\ndata(data.fims.Aus.Jpn.scored, package = \"TAM\")\nfims &lt;- data.fims.Aus.Jpn.scored\n\n\nglimpse(fims)\n\nRows: 6,371\nColumns: 16\n$ SEX     &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~\n$ M1PTI1  &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,~\n$ M1PTI2  &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,~\n$ M1PTI3  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,~\n$ M1PTI6  &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,~\n$ M1PTI7  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ M1PTI11 &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,~\n$ M1PTI12 &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ M1PTI14 &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,~\n$ M1PTI17 &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,~\n$ M1PTI18 &lt;dbl&gt; 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,~\n$ M1PTI19 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ M1PTI21 &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,~\n$ M1PTI22 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,~\n$ M1PTI23 &lt;dbl&gt; 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,~\n$ country &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~\n\n\nOltre alle risposte sui 14 item di matematica, il data set contiene anche informazioni sul genere del partecipate e sul paese d’origine.\n\nfims$SEX &lt;- as.factor(fims$SEX)\nlevels(fims$SEX) &lt;- c(\"male\", \"female\")\nfims$country &lt;- as.factor(fims$country)\nlevels(fims$country) &lt;- c(\"Australia\", \"Japan\")\n\n\nsummary(fims[, c(\"SEX\", \"country\")])\n\n     SEX            country    \n male  :3319   Australia:4320  \n female:3052   Japan    :2051  \n\n\nEsaminiamo le risposte dei primi 400 partecipanti. Con le seguenti istruzioni, per facilitare la manipolazione dei dati, cambiamo il nome delle colonne.\n\nresponses &lt;- fims[1:400, 2:15]\ncolnames(responses) &lt;- gsub(\"M1PTI\", \"I\", colnames(responses))\nglimpse(responses)\n\nRows: 400\nColumns: 14\n$ I1  &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, ~\n$ I2  &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, ~\n$ I3  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, ~\n$ I6  &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, ~\n$ I7  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\n$ I11 &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, ~\n$ I12 &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\n$ I14 &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, ~\n$ I17 &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, ~\n$ I18 &lt;dbl&gt; 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, ~\n$ I19 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\n$ I21 &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ~\n$ I22 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ~\n$ I23 &lt;dbl&gt; 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, ~\n\n\n\ngender &lt;- as.factor(fims$SEX[1:400])\nlevels(gender) &lt;- c(\"male\", \"female\")\n\nUn’analisi IRT può essere paragonata a un’analisi fattoriale. Dopo avere adattato il modello di Rasch ai dati usando mirt(), possiamo usare la funzione summary() per ottenere quella che viene definita “soluzione fattoriale”, che include i carichi fattoriali (F1) e le comunalità (h2). Le comunalità, essendo carichi fattoriali al quadrato, sono interpretate come la varianza spiegata in un item dal tratto latente. Nel caso presente, tutti gli item hanno una relazione sostanziale (saturazioni \\(\\approx\\) .50) con il tratto latente, indicando che il tratto latente è un buon indicatore della varianza osservata in quegli item. Questo suggerisce che il tratto latente è in grado di spiegare una porzione almento moderata della varianza nei punteggi degli item.\n\nmirt_rm &lt;- mirt(responses, 1, itemtype = \"Rasch\", verbose = FALSE)\nsummary(mirt_rm)\n\n       F1    h2\nI1  0.488 0.238\nI2  0.488 0.238\nI3  0.488 0.238\nI6  0.488 0.238\nI7  0.488 0.238\nI11 0.488 0.238\nI12 0.488 0.238\nI14 0.488 0.238\nI17 0.488 0.238\nI18 0.488 0.238\nI19 0.488 0.238\nI21 0.488 0.238\nI22 0.488 0.238\nI23 0.488 0.238\n\nSS loadings:  3.33 \nProportion Var:  0.238 \n\nFactor correlations: \n\n   F1\nF1  1\n\n\nNell’IRT, tuttavia, siamo generalmente più interessati ai parametri specifici IRT (discriminazione e difficoltà). Questi parametri possono essere estratti dall’oggetto creato da mirt() nel seguente modo:\n\nparams_rm &lt;- coef(mirt_rm, IRTpars = TRUE, simplify = TRUE)\nround(params_rm$items, 2) # g = c = guessing parameter\n\n\nA matrix: 14 x 4 of type dbl\n\n\n\na\nb\ng\nu\n\n\n\n\nI1\n1\n-1.10\n0\n1\n\n\nI2\n1\n-1.25\n0\n1\n\n\nI3\n1\n-2.04\n0\n1\n\n\nI6\n1\n-0.05\n0\n1\n\n\nI7\n1\n2.53\n0\n1\n\n\nI11\n1\n-1.25\n0\n1\n\n\nI12\n1\n0.81\n0\n1\n\n\nI14\n1\n-0.50\n0\n1\n\n\nI17\n1\n1.33\n0\n1\n\n\nI18\n1\n-0.40\n0\n1\n\n\nI19\n1\n2.06\n0\n1\n\n\nI21\n1\n1.75\n0\n1\n\n\nI22\n1\n2.41\n0\n1\n\n\nI23\n1\n-1.93\n0\n1\n\n\n\n\n\n\n\\(a\\) (Discriminazione): Il parametro \\(a\\) (discriminazione) rappresenta la pendenza delle curve caratteristiche degli item (ICC - Item Characteristic Curves). Una pendenza elevata (valore alto di \\(a\\)) indica che l’item è molto efficace nel distinguere tra individui con livelli diversi del tratto latente (ad esempio, abilità). Questo significa che piccole variazioni nel tratto latente portano a grandi cambiamenti nella probabilità di rispondere correttamente all’item. Una pendenza bassa (valore basso di \\(a\\)) suggerisce che l’item non è altrettanto efficace nel discriminare tra livelli diversi del tratto latente. In questo caso, anche ampie variazioni nel tratto latente comportano solo piccoli cambiamenti nella probabilità di risposta corretta. Nel modello di Rasch si assume che tutti gli item abbiano la stessa pendenza (o potere discriminante), e quindi tutti i valori di \\(a\\) sono fissati allo stesso valore (ovvero 1).\n\\(b\\) (Difficoltà): Rappresenta il livello di abilità a cui un rispondente ha il 50% di probabilità di rispondere correttamente all’item. Un valore positivo indica un item più difficile (richiede un livello di abilità superiore per rispondere correttamente), mentre un valore negativo indica un item più facile. Ad esempio, I7 ha un valore di difficoltà di 2.53, il che significa che è relativamente difficile, mentre I3, con un valore di -2.04, è relativamente facile.\n\\(g\\) (Probabilità di Indovinare): In questo modello, la probabilità di indovinare è impostata a zero per tutti gli item, il che è coerente con il modello di Rasch, dove non si considera la possibilità di indovinare correttamente un item per caso.\n\nAdattiamo ora ai dati il modello di Rasch con la funzione eRm::RM()\n\nrm_sum0 &lt;- eRm::RM(responses)\n\n\nsummary(rm_sum0)\n\n\nResults of RM estimation: \n\nCall:  eRm::RM(X = responses) \n\nConditional log-likelihood: -1886.529 \nNumber of iterations: 23 \nNumber of parameters: 13 \n\nItem (Category) Difficulty Parameters (eta): with 0.95 CI:\n    Estimate Std. Error lower CI upper CI\nI2    -1.420      0.121   -1.658   -1.183\nI3    -2.210      0.145   -2.494   -1.926\nI6    -0.215      0.108   -0.426   -0.004\nI7     2.364      0.170    2.031    2.697\nI11   -1.420      0.121   -1.658   -1.183\nI12    0.642      0.113    0.422    0.863\nI14   -0.663      0.110   -0.879   -0.448\nI17    1.152      0.122    0.913    1.391\nI18   -0.565      0.109   -0.778   -0.351\nI19    1.889      0.146    1.602    2.175\nI21    1.578      0.134    1.315    1.841\nI22    2.244      0.163    1.925    2.564\nI23   -2.103      0.141   -2.379   -1.827\n\nItem Easiness Parameters (beta) with 0.95 CI:\n         Estimate Std. Error lower CI upper CI\nbeta I1     1.273      0.118    1.041    1.504\nbeta I2     1.420      0.121    1.183    1.658\nbeta I3     2.210      0.145    1.926    2.494\nbeta I6     0.215      0.108    0.004    0.426\nbeta I7    -2.364      0.170   -2.697   -2.031\nbeta I11    1.420      0.121    1.183    1.658\nbeta I12   -0.642      0.113   -0.863   -0.422\nbeta I14    0.663      0.110    0.448    0.879\nbeta I17   -1.152      0.122   -1.391   -0.913\nbeta I18    0.565      0.109    0.351    0.778\nbeta I19   -1.889      0.146   -2.175   -1.602\nbeta I21   -1.578      0.134   -1.841   -1.315\nbeta I22   -2.244      0.163   -2.564   -1.925\nbeta I23    2.103      0.141    1.827    2.379\n\n\n\nLa funzione RM() impone un vincolo sulle stime dei parametri di difficoltà degli item. Questo vincolo è che la media di questi parametri (beta) sia zero. Questo approccio è noto come “parametrizzazione ancorata” o “centrata”. Il vantaggio di questa parametrizzazione è che posiziona la scala di difficoltà degli item in un punto di riferimento fisso, facilitando il confronto tra diversi set di item o tra diverse applicazioni dello stesso test.\nVerifichiamo.\n\ncoef(rm_sum0) |&gt; print()\n\n   beta I1    beta I2    beta I3    beta I6    beta I7   beta I11   beta I12 \n 1.2725717  1.4203230  2.2098398  0.2153106 -2.3639411  1.4203230 -0.6423859 \n  beta I14   beta I17   beta I18   beta I19   beta I21   beta I22   beta I23 \n 0.6632561 -1.1517115  0.5646248 -1.8886152 -1.5781132 -2.2444186  2.1029365 \n\n\n\nsum(rm_sum0$betapar)\n\n8.88178419700125e-16\n\n\nNella parametrizzazione utilizzata da mirt(), i parametri di difficoltà vengono invece stimati senza un vincolo sulla loro media. Questo può portare a stime dei parametri di difficoltà che differiscono da quelle ottenute tramite RM(). Questa libertà nella stima dei parametri permette una flessibilità maggiore, specialmente in modelli IRT complessi o multidimensionali, ma può rendere più complesso il confronto diretto tra set di item o test differenti.\nDalla soluzione prodotta da eRm::RM() possiamo estrarre le stime sia nei termini della facilità che della difficoltà degli item.\n\ntab &lt;- data.frame(\n    item_score = colSums(responses),\n    easiness = coef(rm_sum0),\n    difficulty = -coef(rm_sum0)\n)\ntab[order(tab$item_score), ]\n\n\nA data.frame: 14 x 3\n\n\n\nitem_score\neasiness\ndifficulty\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nI7\n40\n-2.3639411\n2.3639411\n\n\nI22\n44\n-2.2444186\n2.2444186\n\n\nI19\n58\n-1.8886152\n1.8886152\n\n\nI21\n73\n-1.5781132\n1.5781132\n\n\nI17\n98\n-1.1517115\n1.1517115\n\n\nI12\n134\n-0.6423859\n0.6423859\n\n\nI6\n204\n0.2153106\n-0.2153106\n\n\nI18\n233\n0.5646248\n-0.5646248\n\n\nI14\n241\n0.6632561\n-0.6632561\n\n\nI1\n287\n1.2725717\n-1.2725717\n\n\nI2\n297\n1.4203230\n-1.4203230\n\n\nI11\n297\n1.4203230\n-1.4203230\n\n\nI23\n336\n2.1029365\n-2.1029365\n\n\nI3\n341\n2.2098398\n-2.2098398\n\n\n\n\n\nIn alterativa, possiamo usare il pacchetto TAM. Come nel caso di mirt, anche in questo caso viene usata una procedura di stima di massima verosimiglianza marginale.\n\ntam_rm &lt;- tam.mml(responses)\n\n....................................................\nProcessing Data      2024-09-29 08:43:13.152552 \n    * Response Data: 400 Persons and  14 Items \n    * Numerical integration with 21 nodes\n    * Created Design Matrices   ( 2024-09-29 08:43:13.160422 )\n    * Calculated Sufficient Statistics   ( 2024-09-29 08:43:13.167343 )\n....................................................\nIteration 1     2024-09-29 08:43:13.173216\nE Step\nM Step Intercepts   |----\n  Deviance = 5667.9246\n  Maximum item intercept parameter change: 0.314748\n  Maximum item slope parameter change: 0\n  Maximum regression parameter change: 0\n  Maximum variance parameter change: 0.123048\n....................................................\nIteration 2     2024-09-29 08:43:13.18617\nE Step\nM Step Intercepts   |--\n  Deviance = 5633.0417 | Absolute change: 34.8829 | Relative change: 0.00619256\n  Maximum item intercept parameter change: 0.003284\n  Maximum item slope parameter change: 0\n  Maximum regression parameter change: 0\n  Maximum variance parameter change: 0.007956\n....................................................\nIteration 3     2024-09-29 08:43:13.192589\nE Step\nM Step Intercepts   |--\n  Deviance = 5633.0057 | Absolute change: 0.036 | Relative change: 6.39e-06\n  Maximum item intercept parameter change: 0.002198\n  Maximum item slope parameter change: 0\n  Maximum regression parameter change: 0\n  Maximum variance parameter change: 0.005837\n....................................................\nIteration 4     2024-09-29 08:43:13.199112\nE Step\nM Step Intercepts   |--\n  Deviance = 5632.9887 | Absolute change: 0.017 | Relative change: 3.02e-06\n  Maximum item intercept parameter change: 0.00151\n  Maximum item slope parameter change: 0\n  Maximum regression parameter change: 0\n  Maximum variance parameter change: 0.004059\n....................................................\nIteration 5     2024-09-29 08:43:13.205461\nE Step\nM Step Intercepts   |--\n  Deviance = 5632.9805 | Absolute change: 0.0081 | Relative change: 1.44e-06\n  Maximum item intercept parameter change: 0.001036\n  Maximum item slope parameter change: 0\n  Maximum regression parameter change: 0\n  Maximum variance parameter change: 0.002812\n....................................................\nIteration 6     2024-09-29 08:43:13.211215\nE Step\nM Step Intercepts   |--\n  Deviance = 5632.9767 | Absolute change: 0.0039 | Relative change: 6.8e-07\n  Maximum item intercept parameter change: 0.00071\n  Maximum item slope parameter change: 0\n  Maximum regression parameter change: 0\n  Maximum variance parameter change: 0.001944\n....................................................\nIteration 7     2024-09-29 08:43:13.218962\nE Step\nM Step Intercepts   |--\n  Deviance = 5632.9749 | Absolute change: 0.0018 | Relative change: 3.2e-07\n  Maximum item intercept parameter change: 0.000486\n  Maximum item slope parameter change: 0\n  Maximum regression parameter change: 0\n  Maximum variance parameter change: 0.001342\n....................................................\nIteration 8     2024-09-29 08:43:13.223978\nE Step\nM Step Intercepts   |--\n  Deviance = 5632.974 | Absolute change: 9e-04 | Relative change: 1.5e-07\n  Maximum item intercept parameter change: 0.000333\n  Maximum item slope parameter change: 0\n  Maximum regression parameter change: 0\n  Maximum variance parameter change: 0.000925\n....................................................\nIteration 9     2024-09-29 08:43:13.230362\nE Step\nM Step Intercepts   |--\n  Deviance = 5632.9736 | Absolute change: 4e-04 | Relative change: 7e-08\n  Maximum item intercept parameter change: 0.000228\n  Maximum item slope parameter change: 0\n  Maximum regression parameter change: 0\n  Maximum variance parameter change: 0.000637\n....................................................\nIteration 10     2024-09-29 08:43:13.237538\nE Step\nM Step Intercepts   |--\n  Deviance = 5632.9734 | Absolute change: 2e-04 | Relative change: 3e-08\n  Maximum item intercept parameter change: 0.000156\n  Maximum item slope parameter change: 0\n  Maximum regression parameter change: 0\n  Maximum variance parameter change: 0.000439\n....................................................\nIteration 11     2024-09-29 08:43:13.246915\nE Step\nM Step Intercepts   |--\n  Deviance = 5632.9733 | Absolute change: 1e-04 | Relative change: 2e-08\n  Maximum item intercept parameter change: 0.000107\n  Maximum item slope parameter change: 0\n  Maximum regression parameter change: 0\n  Maximum variance parameter change: 0.000302\n....................................................\nIteration 12     2024-09-29 08:43:13.257183\nE Step\nM Step Intercepts   |-\n  Deviance = 5632.9733 | Absolute change: 0 | Relative change: 1e-08\n  Maximum item intercept parameter change: 7.3e-05\n  Maximum item slope parameter change: 0\n  Maximum regression parameter change: 0\n  Maximum variance parameter change: 0.000208\n....................................................\nIteration 13     2024-09-29 08:43:13.261703\nE Step\nM Step Intercepts   |-\n  Deviance = 5632.9732 | Absolute change: 0 | Relative change: 0\n  Maximum item intercept parameter change: 5e-05\n  Maximum item slope parameter change: 0\n  Maximum regression parameter change: 0\n  Maximum variance parameter change: 0.000143\n....................................................\nIteration 14     2024-09-29 08:43:13.265634\nE Step\nM Step Intercepts   |-\n  Deviance = 5632.9732 | Absolute change: 0 | Relative change: 0\n  Maximum item intercept parameter change: 3.4e-05\n  Maximum item slope parameter change: 0\n  Maximum regression parameter change: 0\n  Maximum variance parameter change: 9.8e-05\n....................................................\nItem Parameters\n   xsi.index xsi.label     est\n1          1        I1 -1.1029\n2          2        I2 -1.2505\n3          3        I3 -2.0422\n4          4        I6 -0.0480\n5          5        I7  2.5312\n6          6       I11 -1.2505\n7          7       I12  0.8132\n8          8       I14 -0.4952\n9          9       I17  1.3263\n10        10       I18 -0.3969\n11        11       I19  2.0639\n12        12       I21  1.7545\n13        13       I22  2.4145\n14        14       I23 -1.9345\n...................................\nRegression Coefficients\n     [,1]\n[1,]    0\n\nVariance:\n       [,1]\n[1,] 0.9037\n\n\nEAP Reliability:\n[1] 0.656\n\n-----------------------------\nStart:  2024-09-29 08:43:13.150704\nEnd:  2024-09-29 08:43:13.27764 \nTime difference of 0.1269362 secs\n\n\n\nPossiamo ispezionare le stime dei parametri con\n\ntam_rm$item\n\n\nA data.frame: 14 x 6\n\n\n\nitem\nN\nM\nxsi.item\nAXsi_.Cat1\nB.Cat1.Dim1\n\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nI1\nI1\n400\n0.7175\n-1.10292502\n-1.10292502\n1\n\n\nI2\nI2\n400\n0.7425\n-1.25047869\n-1.25047869\n1\n\n\nI3\nI3\n400\n0.8525\n-2.04219530\n-2.04219530\n1\n\n\nI6\nI6\n400\n0.5100\n-0.04803217\n-0.04803217\n1\n\n\nI7\nI7\n400\n0.1000\n2.53122857\n2.53122857\n1\n\n\nI11\nI11\n400\n0.7425\n-1.25047869\n-1.25047869\n1\n\n\nI12\nI12\n400\n0.3350\n0.81318838\n0.81318838\n1\n\n\nI14\nI14\n400\n0.6025\n-0.49523230\n-0.49523230\n1\n\n\nI17\nI17\n400\n0.2450\n1.32631928\n1.32631928\n1\n\n\nI18\nI18\n400\n0.5825\n-0.39686510\n-0.39686510\n1\n\n\nI19\nI19\n400\n0.1450\n2.06394307\n2.06394307\n1\n\n\nI21\nI21\n400\n0.1825\n1.75454300\n1.75454300\n1\n\n\nI22\nI22\n400\n0.1100\n2.41452138\n2.41452138\n1\n\n\nI23\nI23\n400\n0.8400\n-1.93453524\n-1.93453524\n1\n\n\n\n\n\nLe colonne di questo output possono essere interpretate come segue: - item indica il nome dell’item. - N indica il numero di candidati che hanno risposto a ciascun item. In questo caso, tutti i 400 candidati hanno risposto a ogni item. - M è la media delle risposte a ciascun item. Nel caso di un item con una media alta ciò significa che a tale item è stata fornita uba risposta corretta da una alta percentuale di candidati. - xsi.item per il modello di Rasch è il parametro di difficoltà dell’item. Gli item con valori alti tendono ad essere più difficili. - AXsi.Cat1 ripete la difficoltà dell’item per il modello di Rasch, ma permetterebbe l’inclusione di una matrice di design A, che non abbiamo usato qui. Per i modelli politomici, l’output includerà parametri dell’item per più di una categoria. - B.Cat1.Dim1 è il parametro di discriminazione o pendenza dell’item. Per il modello di Rasch, la pendenza è 1 per ogni item.\nPossiamo mostrare solo la difficoltà e l’errore standard con:\n\ntam_rm$xsi\n\n\nA data.frame: 14 x 2\n\n\n\nxsi\nse.xsi\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nI1\n-1.10292502\n0.1199213\n\n\nI2\n-1.25047869\n0.1231185\n\n\nI3\n-2.04219530\n0.1490275\n\n\nI6\n-0.04803217\n0.1091832\n\n\nI7\n2.53122857\n0.1742477\n\n\nI11\n-1.25047869\n0.1231185\n\n\nI12\n0.81318838\n0.1149197\n\n\nI14\n-0.49523230\n0.1112816\n\n\nI17\n1.32631928\n0.1249697\n\n\nI18\n-0.39686510\n0.1105217\n\n\nI19\n2.06394307\n0.1501087\n\n\nI21\n1.75454300\n0.1378399\n\n\nI22\n2.41452138\n0.1675220\n\n\nI23\n-1.93453524\n0.1445361\n\n\n\n\n\nLa parametrizzazione classica IRT si ottiene con:\n\ntam_rm$item_irt\n\n\nA data.frame: 14 x 3\n\n\nitem\nalpha\nbeta\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nI1\n1\n-1.10292502\n\n\nI2\n1\n-1.25047869\n\n\nI3\n1\n-2.04219530\n\n\nI6\n1\n-0.04803217\n\n\nI7\n1\n2.53122857\n\n\nI11\n1\n-1.25047869\n\n\nI12\n1\n0.81318838\n\n\nI14\n1\n-0.49523230\n\n\nI17\n1\n1.32631928\n\n\nI18\n1\n-0.39686510\n\n\nI19\n1\n2.06394307\n\n\nI21\n1\n1.75454300\n\n\nI22\n1\n2.41452138\n\n\nI23\n1\n-1.93453524",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/06_implementation.html#valutazione-del-test",
    "href": "chapters/irt/06_implementation.html#valutazione-del-test",
    "title": "61  Implementazione",
    "section": "61.3 Valutazione del Test",
    "text": "61.3 Valutazione del Test\nIl primo strumento per la valutazione dei test che vorremmo presentare sono i metodi grafici. Il primo di questi, la mappa persona-item, mostra se il campione di persone copre l’intera gamma degli item e viceversa. Il secondo approccio, che consiste nel confrontare le ICC (Curve Caratteristiche dell’Item) teoriche ed empiriche, può aiutare a rilevare gli item che non si adattano bene. Il terzo, il test grafico, è un test visivo per il Funzionamento Differenziale degli Item (DIF).\n\n61.3.1 Mappa Persona-Item\nAbbiamo in precedenza che il modello di Rasch posiziona persone e item sulla stessa scala latente e che l’accuratezza delle stime dei parametri dipende dalla posizione delle persone rispetto agli item. La mappa persona-item è una rappresentazione visiva delle posizioni degli item e delle persone sul continuum latente. Per poter stimare con precisione i parametri degli item a partire dal campione delle persone e viceversa, le difficoltà degli item dovrebbero coprire l’intera gamma delle abilità delle persone e viceversa.\nLa mappa persona-item (person−item map) si ottiene nel modo seguente con eRm.\n\nplotPImap(rm_sum0)\n\n\n\n\n\n\n\n\nLa parte superiore della mappa persona-item mostra un istogramma delle stime dei parametri di abilità, mentre la parte inferiore mostra le stime delle difficoltà per ciascun item del test. Per ogni item, la stima della difficoltà è indicata dalla posizione del punto sulla linea tratteggiata corrispondente a quell’item. Ad esempio, la difficoltà stimata per l’item 1 corrisponde alla posizione del punto sulla linea tratteggiata più in alto. La mappa persona-item offre un controllo visivo di coerenza per le stime del nostro modello IRT (Teoria della Risposta all’Item). Le stime delle abilità sono più accurate quando cadono nel mezzo della distribuzione dei parametri degli item e viceversa. Pertanto, idealmente, l’istogramma delle abilità e le stime delle difficoltà dovrebbero essere centrate sullo stesso punto e mostrare un’ampia sovrapposizione. Nel nostro test, sembra essere questo il caso.\n\n\n61.3.2 ICC Empiriche\nLe Curve Caratteristiche degli Item (ICC) descrivono la relazione teorica tra l’abilità dei partecipanti al test e la probabilità di una risposta corretta che ci aspettiamo sotto il modello di Rasch per una data difficoltà. La ICC attesa per un item può essere tracciata dopo che la sua difficoltà è stata stimata. Oltre alle probabilità attese di una risposta corretta illustrate dall’ICC, possiamo anche tracciare le frequenze relative empiriche di una risposta corretta. Queste frequenze relative empiriche sono indicate nella figura come punti e vengono chiamate ICC empiriche.\nUsando eRm possimo generare le ECC empiriche nel modo seguente.\n\nplotICC(\n    rm_sum0, \n    item.subset = \"all\",\n    empICC = list(\"raw\"), \n    empCI = list()\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLe ICC empiriche sono rappresentate dai singoli punti, mentre la ICC attesa sotto il modello di Rasch è indicata dalla linea liscia. Dalle figure precedenti, per gli item 12 e 14 notiamo che in generale la forma dell’ICC empirica è molto ben allineata con l’ICC attesa, ma per l’item 12 l’ICC empirica mostra valori sopra zero anche per le abilità più basse a sinistra della dimensione latente. Questo potrebbe indicare una tendenza al tentativo di indovinare (guessing). Per l’item 19, l’ICC empirica appare più ripida dell’ICC attesa sotto il modello di Rasch. Mostra un salto molto più pronunciato tra la prima metà approssimativa dei punti e i punti rimanenti. Per l’item 21, al contrario, l’ICC empirica è molto più piatta rispetto a quella attesa. Confronteremo la nostra impressione visiva con le statistiche di adattamento degli item per questi item di seguito.\nÈ possibile visualizzare le ICC attese di tutti gli item del test in un unico grafico utilizzando la funzione plotjointICC(). Questo grafico ci permette di esaminare come la difficoltà influenzi la probabilità che un candidato risponda correttamente a un item. Ricordiamo che la difficoltà di un item è definita come il livello di abilità in cui una persona ha una probabilità del 50% di rispondere correttamente all’item. Abbiamo aggiunto una linea tratteggiata orizzontale alla probabilità di 0.5 usando il comando segments. Il punto in cui un’ICC interseca questa linea rappresenta la sua difficoltà. Questo ci permette di leggere facilmente le difficoltà relative degli item dal grafico. Spostandosi da sinistra a destra, il primo ICC intersecato dalla linea orizzontale corrisponde all’item meno difficile (in questo caso l’item 3, seguito da vicino dall’item 23, come indicato nell’ordine degli item nella legenda), e l’ultimo ICC intersecato dalla linea orizzontale è per l’item più difficile (in questo caso l’item 7).\nDa notare che le ICC attese nella figura sono parallele per definizione. Il modello di Rasch assume che le ICC siano parallele, quindi produrrà sempre ICC teoriche o attese parallele, anche quando gli item hanno in realtà pendenze o tassi di guessing diversi, come abbiamo visto in precedenza per le ICC empiriche.\n\neRm::plotjointICC(rm_sum0, cex = 0.7)\nsegments(-2.5, 0.5, 4, 0.5, lty = 2)\n\n\n\n\n\n\n\n\nIn alternativa, possiamo generare le ICC usando il pacchetto mirt:\n\nmirt_rm &lt;- mirt(responses, 1, \"Rasch\")\nplot(mirt_rm, type = \"trace\")\n\nIteration: 16, Log-Lik: -2816.487, Max-Change: 0.00008\n\n\n\n\n\n\n\n\n\nLe curve caratteristiche degli item offrono un quadro dettagliato e visuale di come ciascun item del test si comporta attraverso diversi livelli dell’abilità latente. Per esempio:\n\nVisualizzazione della Difficoltà e della Discriminazione:\n\nSupponiamo di avere un item che mostra una curva con una ripida salita in un punto specifico della scala di abilità. Questo indica che l’item ha una difficoltà concentrata attorno a quel punto e che discrimina efficacemente tra rispondenti con abilità appena al di sotto e al di sopra di quel livello.\nAl contrario, una curva più graduale suggerisce che l’item è meno discriminante, con una variazione più ampia nella probabilità di risposta corretta a seconda del livello di abilità.\n\nIdentificazione di Lacune nella Valutazione:\n\nVisualizzando le curve di più item, possiamo identificare se ci sono lacune nella copertura dell’abilità latente. Ad esempio, se tutti gli item hanno curve che si concentrano su livelli di abilità bassi, potrebbe esserci una mancanza di item difficili per misurare l’abilità ad alti livelli.\nInoltre, se le curve degli item si sovrappongono eccessivamente, potrebbe indicare ridondanza tra gli item, suggerendo che alcuni di essi non aggiungono informazioni uniche alla valutazione.\n\nConfronto tra Diversi Tipi di Item:\n\nPer esempio, gli item progettati per misurare concetti di base potrebbero avere curve che mostrano alta probabilità di risposta corretta anche a livelli di abilità bassi.\nAl contrario, item progettati per essere più impegnativi potrebbero mostrare probabilità elevate di risposta corretta solo a livelli di abilità più alti.\n\n\n\n\n61.3.3 Test Grafico\nIl test grafico del modello, basato sui principi di Rasch (1960), è un metodo intuitivo per valutare l’invarianza degli item in un test, confrontando i parametri degli item stimati per due gruppi di persone. Affinché il modello di Rasch sia considerato valido, è necessario che le stime dei parametri degli item per i diversi gruppi concordino, fino a una trasformazione lineare. In termini pratici, ciò si traduce nel fatto che, quando visualizzate in un grafico, le stime dei parametri degli item dei due gruppi dovrebbero allinearsi lungo una linea retta.\nPer complementare questa analisi, possiamo ricorrere al test del rapporto di verosimiglianza di Andersen (1973), un approccio ben consolidato per verificare l’adeguatezza del modello di Rasch nel rappresentare il comportamento dei partecipanti ai test. Il test di Andersen valuta se le stime dei parametri degli item rimangono consistenti tra diversi gruppi di partecipanti. Se i parametri degli item stimati individualmente per ciascun gruppo differiscono significativamente, ciò indica che il modello di Rasch potrebbe non essere un’adeguata rappresentazione del comportamento osservato nei test.\nA differenza del test grafico, il test del rapporto di verosimiglianza confronta il massimo della verosimiglianza condizionata sotto il modello di Rasch con il massimo della verosimiglianza condizionata quando i parametri degli item possono variare tra i gruppi. Questa metodologia offre un’indicazione di quanto efficacemente ciascun modello rappresenti il comportamento dei partecipanti.\nIl test del rapporto di verosimiglianza utilizza la statistica di test \\(T = −2 \\cdot log(LR)\\), che ha una distribuzione campionaria approssimativamente \\(\\chi^2\\) per campioni grandi. Valori del rapporto di verosimiglianza inferiori a 1, o valori elevati di T, suggeriscono una violazione del modello di Rasch.\nIl test di Andersen è implementato nel pacchetto eRm in R, offrendo uno strumento utile per l’analisi. Tuttavia, è importante notare che un risultato non significativo in questo test non può essere interpretato automaticamente come supporto per il modello di Rasch, specialmente se il modello più generale non descrive adeguatamente i dati. Inoltre, la capacità di rilevare differenze tra i gruppi specificati dipende dall’effettiva diversità dei parametri del modello tra questi gruppi. Sono stati messi a punto approcci più flessibili per rilevare le differenze nei parametri.\n\nlrt_mean_split &lt;- LRtest(rm_sum0, splitcr = \"mean\")\nlrt_mean_split\n\n\nAndersen LR-test: \nLR-value: 79.71 \nChi-square df: 13 \np-value:  0 \n\n\nL’output di questo test mostra una violazione significativa del modello di Rasch al livello \\(\\alpha\\) = 0.05.\n\nplotGOF(\n    lrt_mean_split,\n    tlab = \"item\", pos = 1,\n    main = \"Difficulty by Score (with Item Names)\",\n    conf = list(gamma = 0.95, col = 1)\n)\n\n\n\n\n\n\n\n\nOra possiamo tracciare le stime delle difficoltà di ciascun gruppo utilizzando la funzione plotGOF() per creare il test grafico. La funzione plotGOF() prende il risultato di LRtest() e traccia le stime dei parametri degli item per i due gruppi. Per facilitare la valutazione visiva, plotGOF() può opzionalmente etichettare gli item e aggiungere ellissi di confidenza.\nPer creare il grafico per il test grafico basato sulla divisione media, possiamo procedere in questo modo: ogni piccolo cerchio nella Figura mostra le stime delle difficoltà per un singolo item. La coordinata x di un cerchio indica la sua stima di difficoltà per i partecipanti al test con punteggi sotto la media e la sua coordinata y indica la stima di difficoltà per i partecipanti al test con punteggi sopra la media. La linea y = x è fornita come riferimento, poiché i punti che cadono su questa linea avrebbero la stessa stima in entrambi i gruppi. La distanza tra qualsiasi punto e la linea di riferimento y = x indica quanto le stime differiscono tra i due gruppi. Indica anche la direzione di questa differenza. Gli item sotto la linea sono più difficili per i partecipanti al test con punteggi sotto la media, mentre gli item sopra la linea sono più difficili per i partecipanti al test con punteggi sopra la media.\nGli assi orizzontali e verticali mostrano intervalli di confidenza per le stime per ciascun gruppo di partecipanti al test. La larghezza di ciascun intervallo di confidenza è determinata dall’elemento gamma della lista fornita a conf. L’impostazione predefinita gamma = .95 produce intervalli di confidenza al 95% per ciascun asse dell’ellisse. Quando un’ellisse di confidenza non incrocia la linea di riferimento, l’item rispettivo è diagnosticato come mostrante un significativo DIF.\nLa figura indica che gli item 2, 6, 21 e 22 differiscono significativamente tra le persone con punteggi sopra e sotto la media, poiché le loro ellissi di confidenza non incrociano la linea di riferimento. Gli item 21 e 22 sono più difficili per le persone con punteggi pari o superiori alla media, mentre gli item 2 e 6 sono più difficili per le persone con punteggi sotto la media. Tali violazioni del modello possono verificarsi quando le ICC osservate differiscono dalle ICC attese sotto il modello di Rasch per i partecipanti al test con abilità basse e alte. Questo può accadere, ad esempio, se è presente il tentativo di indovinare (guessing), o se la pendenza è più ripida o meno ripida di quanto previsto dal modello di Rasch.\nPossiamo anche fornire all’argomento splitcr una variabile che divide i partecipanti al test in gruppi. Ad esempio, possiamo testare se i parametri degli item differiscono in base al genere passando un vettore contenente le appartenenze di gruppo come argomento splitcr.\n\nlrt_gender &lt;- LRtest(rm_sum0, splitcr = gender)\nlrt_gender\n\nERROR: Error in UseMethod(\"LRtest\"): su un oggetto di classe \"LRtest\" `e stato usato un metodo non applicabile per \"c('SingleGroupClass', 'AllModelClass')\"\n\nError in UseMethod(\"LRtest\"): su un oggetto di classe \"LRtest\" `e stato usato un metodo non applicabile per \"c('SingleGroupClass', 'AllModelClass')\"\nTraceback:\n\n1. .handleSimpleError(function (cnd) \n . {\n .     watcher$capture_plot_and_output()\n .     cnd &lt;- sanitize_call(cnd)\n .     watcher$push(cnd)\n .     switch(on_error, continue = invokeRestart(\"eval_continue\"), \n .         stop = invokeRestart(\"eval_stop\"), error = invokeRestart(\"eval_error\", \n .             cnd))\n . }, \"su un oggetto di classe \\\"LRtest\\\" `e stato usato un metodo non applicabile per \\\"c('SingleGroupClass', 'AllModelClass')\\\"\", \n .     base::quote(UseMethod(\"LRtest\")))\n\n\nCome nel test precedente, anche il Test del Rapporto di Verosimiglianza (LRT) per il genere indica una violazione significativa del modello di Rasch al livello α = 0.05.\n\nplotGOF(\n    lrt_gender,\n    tlab = \"item\", pos = 1,\n    main = \"Difficulty by Score (with Item Names)\",\n    conf = list(gamma = 0.95, col = 1)\n)\n\nERROR: Error: oggetto 'lrt_gender' non trovato\n\nError: oggetto 'lrt_gender' non trovato\nTraceback:\n\n1. .handleSimpleError(function (cnd) \n . {\n .     watcher$capture_plot_and_output()\n .     cnd &lt;- sanitize_call(cnd)\n .     watcher$push(cnd)\n .     switch(on_error, continue = invokeRestart(\"eval_continue\"), \n .         stop = invokeRestart(\"eval_stop\"), error = invokeRestart(\"eval_error\", \n .             cnd))\n . }, \"oggetto 'lrt_gender' non trovato\", base::quote(eval(expr, \n .     envir)))\n\n\nLa figura indica che gli item 2, 7 e 21 differiscono tra partecipanti al test femminili e maschili. Gli item 2 e 7 sono più difficili per i partecipanti femminili, mentre l’item 21 è più difficile per i partecipanti maschili.\n\n\n61.3.4 Test di Wald\nLe impostazioni del test del rapporto di verosimiglianza di Andersen (1973) e del test di Wald sono molto simili. Entrambi i test si basano sull’idea che il modello di Rasch sia un modello ragionevole per i dati dei test solo se i parametri degli item stimati non variano sistematicamente tra gruppi di persone. In entrambi i test, consideriamo le stime dei parametri degli item per ciascun gruppo di persone. A differenza del test del rapporto di verosimiglianza, tuttavia, il test di Wald confronta direttamente le stime dei parametri degli item dei gruppi. In sostanza, il test di Wald calcola la differenza tra la stima del primo gruppo della difficoltà dell’item i, β̂(1)i, e quella del secondo gruppo, β̂(2)i. Questa differenza viene divisa per il suo errore standard per tenere conto del fatto che tutte le stime sono soggette a rumore. Questo porta alla statistica di test per l’item i:\n\\[\nT_i = \\frac{\\hat{\\beta}^{(1)}_i - \\hat{\\beta}^{(2)}_i}{\\sqrt{se(\\hat{\\beta}^{(1)}_i)^2 + se(\\hat{\\beta}^{(2)}_i)^2}},\n\\]\ndove $ se(^{(1)}_i) $ e $ se(^{(2)}_i) $ indicano rispettivamente gli errori standard di $ ^{(1)}_i $ e $ ^{(2)}_i $.\nPer campioni di grandi dimensioni, $ T_i $ approssimativamente segue una distribuzione normale standard sotto l’ipotesi nulla che il vero parametro dell’item sia lo stesso per entrambi i gruppi. Valori estremi di $ T_i $ sono improbabili sotto la distribuzione normale. Quindi, un valore estremo di $ T_i $, con un piccolo valore p, indica che l’item i viola il modello di Rasch.\nEseguiamo il test con R:\n\nWaldtest(rm_sum0, splitcr = \"mean\")\n\n\nWald test on item level (z-values):\n\n         z-statistic p-value\nbeta I1       -0.514   0.607\nbeta I2       -3.328   0.001\nbeta I3       -0.838   0.402\nbeta I6       -2.555   0.011\nbeta I7        0.210   0.834\nbeta I11      -1.773   0.076\nbeta I12       1.562   0.118\nbeta I14       1.821   0.069\nbeta I17       1.550   0.121\nbeta I18       0.333   0.739\nbeta I19      -1.827   0.068\nbeta I21       5.768   0.000\nbeta I22       4.106   0.000\nbeta I23      -1.560   0.119\n\n\nQuesti test indicano nuovamente che gli item 2, 6, 21 e 22 differiscono significativamente tra i partecipanti al test con punteggi sopra e sotto la media.\nPossiamo anche eseguire il test per la differenza tra maschi e femmine:\n\nWaldtest(rm_sum0, splitcr = gender)\n\nERROR: Error: oggetto 'gender' non trovato\n\nError: oggetto 'gender' non trovato\nTraceback:\n\n1. Waldtest.Rm(rm_sum0, splitcr = gender)\n2. .handleSimpleError(function (cnd) \n . {\n .     watcher$capture_plot_and_output()\n .     cnd &lt;- sanitize_call(cnd)\n .     watcher$push(cnd)\n .     switch(on_error, continue = invokeRestart(\"eval_continue\"), \n .         stop = invokeRestart(\"eval_stop\"), error = invokeRestart(\"eval_error\", \n .             cnd))\n . }, \"oggetto 'gender' non trovato\", base::quote(eval(expr, envir)))\n\n\nI risultati qui concordano in gran parte anche con la figura precedente. In linea con il test grafico, il test di Wald indica che gli item 2, 7 e 21 differiscono tra i gruppi.\n\n\n61.3.5 Ancoraggio\nL’ancoraggio è una procedura cruciale quando si confrontano le stime dei parametri degli item tra diversi gruppi, un passo fondamentale in test come il Wald e in metodi grafici. Tale processo necessita di particolare attenzione perché implica la restrizione di alcuni parametri degli item per allineare le scale latenti tra i gruppi. Ad esempio, fissare il parametro del primo item a zero in entrambi i gruppi crea un punto di riferimento comune, ma anche limitazioni.\nLa scelta degli item di ancoraggio è delicata: fissare un parametro in entrambi i gruppi significa non poter più valutare la differenza per quell’item specifico. La selezione dovrebbe essere guidata da un’attenta analisi dei dati e da considerazioni teoriche. Approcci guidati dai dati sono stati proposti per identificare item invarianti o escludere quelli con DIF, processo noto come purificazione. Tuttavia, occorre cautela: anche metodi ben progettati possono portare a conclusioni errate se gli item di ancoraggio scelti sono inappropriati.\nIn pratica, spesso si adotta una restrizione in cui la somma dei parametri degli item è zero per tutti i gruppi. Questo approccio, adottato da pacchetti software come eRm e difR in R, si basa sull’assunzione che eventuali DIF si annullino su tutti gli item. Ma se questa assunzione non è valida, o se l’ancoraggio include item con DIF, potremmo incorrere in errori interpretativi.\nIn sintesi, l’ancoraggio è una strategia potente ma che richiede un’attenta considerazione e un’analisi critica. È fondamentale non solo selezionare gli item di ancoraggio adeguati ma anche interpretare i risultati con una comprensione chiara delle ipotesi e delle potenziali limitazioni del metodo scelto.\n\nresp &lt;- as.matrix(responses)\nanchortest(\n    resp ~ gender,\n    class = \"constant\",\n    select = \"MPT\"\n)\n\nERROR: Error in eval(predvars, data, env): oggetto 'gender' non trovato\n\nError in eval(predvars, data, env): oggetto 'gender' non trovato\nTraceback:\n\n1. anchortest.formula(resp ~ gender, class = \"constant\", select = \"MPT\")\n2. eval(mf, parent.frame())\n3. eval(mf, parent.frame())\n4. model.frame(formula = resp ~ gender, drop.unused.levels = TRUE)\n5. model.frame.Formula(formula = resp ~ gender, drop.unused.levels = TRUE)\n6. model.frame(terms(formula, lhs = lhs, rhs = rhs, data = data, \n .     dot = dot), data = data, ...)\n7. model.frame.default(terms(formula, lhs = lhs, rhs = rhs, data = data, \n .     dot = dot), data = data, ...)\n8. eval(predvars, data, env)\n9. eval(predvars, data, env)\n10. .handleSimpleError(function (cnd) \n  . {\n  .     watcher$capture_plot_and_output()\n  .     cnd &lt;- sanitize_call(cnd)\n  .     watcher$push(cnd)\n  .     switch(on_error, continue = invokeRestart(\"eval_continue\"), \n  .         stop = invokeRestart(\"eval_stop\"), error = invokeRestart(\"eval_error\", \n  .             cnd))\n  . }, \"oggetto 'gender' non trovato\", base::quote(eval(predvars, \n  .     data, env)))\n\n\n\nanchortest(\n    resp ~ gender,\n    class = \"forward\",\n    select = \"MTT\"\n)\n\nAnchor items:\nrespI3, respI14, respI12, respI18, respI6, respI1, respI11, respI19,\nrespI17\n\nFinal DIF tests:\n\n     Simultaneous Tests for General Linear Hypotheses\n\nLinear Hypotheses:\n              Estimate Std. Error z value Pr(&gt;|z|)   \nrespI1 == 0   0.280742   0.236295   1.188  0.23480   \nrespI2 == 0  -0.772419   0.266065  -2.903  0.00369 **\nrespI6 == 0  -0.157844   0.215852  -0.731  0.46462   \nrespI7 == 0  -1.735924   0.557443  -3.114  0.00185 **\nrespI11 == 0  0.357017   0.243337   1.467  0.14233   \nrespI12 == 0  0.058030   0.226352   0.256  0.79767   \nrespI14 == 0  0.009681   0.219006   0.044  0.96474   \nrespI17 == 0 -0.354019   0.251559  -1.407  0.15934   \nrespI18 == 0 -0.031915   0.217673  -0.147  0.88343   \nrespI19 == 0 -0.325546   0.303128  -1.074  0.28284   \nrespI21 == 0  0.486221   0.295389   1.646  0.09976 . \nrespI22 == 0 -0.132434   0.371866  -0.356  0.72174   \nrespI23 == 0  0.154642   0.313401   0.493  0.62171   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Univariate p values reported)\n\n\n\nanchortest(\n    resp ~ gender,\n    select = \"Gini\"\n)\n\nAnchor items:\nrespI23\n\nFinal DIF tests:\n\n     Simultaneous Tests for General Linear Hypotheses\n\nLinear Hypotheses:\n              Estimate Std. Error z value Pr(&gt;|z|)   \nrespI1 == 0   0.126100   0.387369   0.326  0.74478   \nrespI2 == 0  -0.927061   0.389044  -2.383  0.01718 * \nrespI3 == 0   0.009212   0.427183   0.022  0.98279   \nrespI6 == 0  -0.312486   0.376562  -0.830  0.40663   \nrespI7 == 0  -1.890565   0.632101  -2.991  0.00278 **\nrespI11 == 0  0.202375   0.392278   0.516  0.60593   \nrespI12 == 0 -0.096612   0.386789  -0.250  0.80276   \nrespI14 == 0 -0.144961   0.376949  -0.385  0.70056   \nrespI17 == 0 -0.508660   0.407380  -1.249  0.21181   \nrespI18 == 0 -0.186557   0.376409  -0.496  0.62016   \nrespI19 == 0 -0.480188   0.450796  -1.065  0.28679   \nrespI21 == 0  0.331579   0.418092   0.793  0.42773   \nrespI22 == 0 -0.287076   0.476210  -0.603  0.54662   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Univariate p values reported)\n\n\nGli output di R della funzione anchortest() elencano gli item di ancoraggio selezionati dai rispettivi approcci di selezione dell’ancoraggio, oltre ai risultati del test di Wald basati su questi item di ancoraggio. Tutti e tre gli approcci portano a risultati in cui solo gli item 2 e 7 mostrano DIF per genere, mentre il test grafico e il test di Wald in eRm hanno identificato anche l’item 21 e l’item 11 (al limite) come aventi DIF.\nRiesaminando il test grafico nella figura precedente, notiamo che gli item 2 e 7 mostrano DIF nella stessa direzione (sopra la diagonale), mentre gli item 21 e 11 sono orientati nella direzione opposta (sotto la diagonale) e in misura minore.\nConsiderando questi risultati nel loro insieme, si può concludere che potrebbe essere presente un DIF non bilanciato e che la diagonale usata nella Figura 6.4 non è ideale per valutare gli item. Per illustrare ciò, tracciamo manualmente una linea di riferimento alternativa attraverso la posizione dell’item 23, che è stato selezionato come item di ancoraggio (primario) dai tre approcci presentati in psychotools, utilizzando il comando abline.\n\n plotGOF(\n    lrt_gender,\n    tlab = \"item\", pos = 1,\n    main = \"Difficulty by Gender (with Item Names)\",\n    conf = list(gamma = 0.95, col = 1)\n)\nabline(-0.3, 1, lty=2)\n\n\n\n\n\n\n\n\nCome si può vedere nella figura risultante, basandoci sulla linea di riferimento alternativa, non troviamo più DIF negli item 11 e 21, ma gli item 2 e 7 mostrano ancora più chiaramente un DIF.\nPer questo set di dati, la stessa conclusione viene raggiunta in eRm quando si utilizza la funzione stepwiseIt(), che esegue diversi test di Wald e ad ogni passo esclude l’item singolo con la statistica di test più grande.\n\nstepwiseIt(rm_sum0, criterion = list(\"Waldtest\", gender))\n\nEliminated item - Step 1: I7\nEliminated item - Step 2: I2\n\n\n\nResults for stepwise item elimination:\nNumber of steps: 2 \nCriterion: Waldtest\n\n           z-statistic p-value\nStep 1: I7       3.089   0.002\nStep 2: I2       3.059   0.002\n\n\nUtilizzando questo metodo, dopo l’esclusione degli item 7 e 2, che presentavano il DIF più marcato, non si rilevano più differenze significative nei test degli item rimanenti. Per visualizzare meglio questo processo, immaginiamo la figura precedente: inizialmente, la linea di riferimento corrisponde alla diagonale solida. Tuttavia, dopo aver eliminato l’item 7, questa linea si sposta verso quella tratteggiata nel secondo passaggio e, rimuovendo poi l’item 2, si allinea o si avvicina molto alla linea tratteggiata nel terzo passaggio. Di conseguenza, gli item restanti non mostrano più un DIF significativo.\nIn sintesi, mentre i test grafici e di Wald basati sulla restrizione della somma zero possono risultare ingannevoli in presenza di un DIF non bilanciato, l’impiego di metodi di ancoraggio avanzati e l’approccio di eliminazione graduale degli item possono offrire una visione più accurata e dettagliata della situazione.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/06_implementation.html#rimozione-di-item",
    "href": "chapters/irt/06_implementation.html#rimozione-di-item",
    "title": "61  Implementazione",
    "section": "61.4 Rimozione di item",
    "text": "61.4 Rimozione di item\nSe questa analisi facesse parte della costruzione di un test reale, gli item che mostrano DIF (o altre anomalie nelle analisi successive) dovrebbero essere attentamente esaminati da esperti di contenuto per decidere se modificarli o rimuoverli dal test. Nella discussione seguente, tuttavia, non rimuoveremo gli item perché desideriamo mantenere il set di dati completo. Tuttavia, se si desiderasse rimuovere alcuni item (ovvero colonne) dal set di dati, ciò potrebbe essere fatto con i seguenti comandi.\n\nresponses_removeDIFitems &lt;- responses[, -which(colnames(responses) %in% c(\"I2\", \"I7\"))]\ncolnames(responses_removeDIFitems)\n\n\n'I1''I3''I6''I11''I12''I14''I17''I18''I19''I21''I22''I23'\n\n\nDopo aver rimosso degli item, l’intero processo dovrebbe ricominciare da capo, rifacendo il modello di Rasch e indagando sugli item rimanenti.\n\n61.4.1 Test di Martin-Löf\nNella sezione precedente, abbiamo visto che il test del rapporto di verosimiglianza di Andersen (1973) verifica l’ipotesi che i parametri degli item siano invarianti per vari gruppi di persone. Una ipotesi correlata riguarda l’invarianza dei parametri delle persone per diversi gruppi di item.\nQui, la domanda fondamentale è se diversi gruppi di item misurino tratti latenti differenti. Ciò rappresenterebbe una violazione del modello di Rasch, il quale implica un singolo tratto latente alla base di tutti gli item. Se questo tipo di violazione del modello viene rilevato, un modello IRT multidimensionale potrebbe essere più appropriato.\nUn metodo comune per valutare la dimensionalità in generale è l’analisi fattoriale esplorativa. Qui invece descriveremo il test di Martin-Löf che affronta l’ipotesi alternativa secondo cui gruppi di item misurano tratti latenti differenti ed è disponibile nel pacchetto eRm. Come il test del rapporto di verosimiglianza di Andersen, questo test si basa sul confronto di due verosimiglianze condizionate. La prima verosimiglianza condizionata Lu(r,β) è quella del modello di Rasch. La seconda verosimiglianza condizionata Lu(r1, r2, β) è nuovamente quella di un modello più generale che ora permette diversi parametri di persona per specifici gruppi di item. I gruppi di item devono essere definiti prima dell’analisi, il che può essere fatto in base alle loro difficoltà (cioè, testiamo item facili contro difficili) o in base a diverse dimensioni latenti che si sospetta siano misurate dai gruppi di item (cioè, il gruppo di item 1 è sospettato di misurare una dimensione latente diversa rispetto al gruppo di item 2). Se la seconda verosimiglianza è maggiore, ciò indica una violazione del modello di Rasch (analogamente al test del rapporto di verosimiglianza di Andersen).\nIl test di Martin-Löf è spesso descritto come un test per la unidimensionalità. Certi tipi di multidimensionalità possono anche manifestarsi come DIF. Per questa ragione, i test che mirano a rilevare il DIF, possono anche essere sensibili a certe violazioni della unidimensionalità.\n\nmloef_median &lt;- MLoef(rm_sum0, splitcr = \"median\")\nmloef_median\n\n\nMartin-Loef-Test (split criterion: median)\nLR-value: 67.083 \nChi-square df: 48 \np-value: 0.036 \n\n\nOtteniamo un valore p inferiore a 0.05. Ciò indica che le stime dei parametri delle persone ottenute dagli item facili e difficili differiscono in modo significativo, ovvero, una violazione del modello di Rasch.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/06_implementation.html#item-e-person-fit",
    "href": "chapters/irt/06_implementation.html#item-e-person-fit",
    "title": "61  Implementazione",
    "section": "61.5 Item e Person Fit",
    "text": "61.5 Item e Person Fit\n\n61.5.1 Tests e Statistiche di Bontà di Adattamento\nIn questa sezione esaminiamo una varietà di metodi per valutare l’adattamento dei dati di risposta agli item e al modello di Rasch. Alcuni di questi metodi sono test statistici formali, mentre altri sono statistiche descrittive per le quali sono stati suggeriti nella letteratura dei limiti critici empirici. Vedremo anche che esistono approcci per valutare l’adattamento a livello dell’intero test psicologico, così come approcci focalizzati sulla valutazione dell’adattamento di singoli item o individui.\n\n61.5.1.1 Test di Bontà di Adattamento χ2 e G2\nNella valutazione del modello di Rasch, esaminiamo due classi principali di test di bontà di adattamento: il test χ2 e il test G2, entrambi noti nell’analisi delle tabelle di contingenza. A differenza dei test del rapporto di verosimiglianza o dei test di Martin-Löf, il test χ2 non confronta l’adattamento relativo di due modelli. Piuttosto, esso valuta quanto accuratamente i modelli di risposta previsti dal modello di Rasch corrispondano ai modelli di risposta osservati. Questo avviene attraverso il confronto tra il numero di partecipanti che mostrano ciascun modello di risposta osservato e il numero previsto dal modello di Rasch.\nIl principio dei test di bontà di adattamento χ2 per il modello di Rasch è basato sull’analisi di tutti i possibili modelli di risposta (combinazioni di 0 e 1 per risposte errate e corrette). Definiamo Ou come il numero osservato di partecipanti con il modello di risposta u e Eu come il numero previsto sotto il modello di Rasch. La statistica del test χ2 è data da:\n\\[ T = \\sum_{u} \\frac{(O_u - E_u)^2}{E_u} \\]\nIn questa formula, le differenze tra osservazioni e previsioni sono elevate al quadrato e poi ponderate inversamente rispetto alla frequenza attesa. In campioni di grandi dimensioni, T segue approssimativamente una distribuzione χ2, se il modello di Rasch è appropriato. Valori alti di T indicano una cattiva adattazione del modello.\nTuttavia, il test χ2 richiede che ogni modello di risposta abbia una frequenza attesa sufficientemente alta, una condizione spesso non soddisfatta in test con molti item. In questi casi, il test χ2 non segue una distribuzione χ2 sotto l’ipotesi nulla, rendendolo poco pratico. Una soluzione potrebbe essere quella di raggruppare i modelli di risposta per aumentare le frequenze attese.\nParallelamente, la statistica del rapporto di verosimiglianza G2, anch’essa derivante dall’analisi dei dati categoriali, è calcolata come:\n\\[ G^2 = 2 \\sum_{u} O_u \\log \\left( \\frac{O_u}{E_u} \\right) \\]\nG2 confronta le frequenze osservate con quelle attese, anziché le verosimiglianze di due modelli. Se le frequenze attese sono vicine a quelle osservate, il rapporto \\(\\frac{O_u}{E_u}\\) si avvicina a 1, rendendo il logaritmo naturale \\(\\log\\left(\\frac{O_u}{E_u}\\right)\\) vicino a 0 e la statistica G2 tende a 0, indicando un buon adattamento. Anche G2 segue una distribuzione χ2 se il modello di Rasch è appropriato. Tuttavia, proprio come per il test χ2, G2 è praticabile solo con grandi frequenze attese, limitandone l’uso effettivo. Nonostante ciò, G2 è importante da comprendere poiché molte altre statistiche di test si basano su di esso.\n\n\n\n61.5.2 Statistica M2\nLa statistica M2, sviluppata da Maydeu-Olivares e Joe (2006), affronta il problema dei modelli di risposta rari che possono complicare i test χ2. Invece di confrontare le frequenze di interi modelli di risposta, la statistica M2 utilizza le informazioni provenienti dagli item individuali e dalle coppie di item. Specificatamente, confronta: 1. Le frequenze attese e osservate delle risposte corrette agli item individuali. 2. Le frequenze attese e osservate delle risposte corrette a entrambi gli item in una coppia di item.\nPer esempio, con due item, confronterebbe le frequenze osservate e attese per una risposta corretta al primo item, al secondo item e ad entrambi gli item insieme. Questo approccio è simile all’analisi delle tabelle di frequenza per le coppie di item. La statistica M2, come il test di bontà di adattamento χ2, implica un cattivo adattamento tra i dati e il modello di Rasch se produce un valore elevato o, equivalentemente, un valore p piccolo. Senza violazione del modello, la statistica M2 segue approssimativamente una distribuzione χ2 con gradi di libertà calcolati come $ k - d $, dove $ k $ è il numero di frequenze confrontate e $ d $ è il numero di parametri liberi del modello.\n\n\n61.5.3 Errore Quadratico Medio di Approssimazione (RMSEA)\nIl RMSEA deriva dalla statistica M2. Utilizza i gradi di libertà (nuovamente $ k - d $) e la dimensione del campione $ P $ per calcolare il valore RMSEA. La formula per il RMSEA è:\n\\[ \\text{RMSEA} = \\sqrt{\\frac{M2 - df}{P \\cdot df}} \\]\nValori di RMSEA vicini a 0 generalmente indicano un buon adattamento del modello ai dati. Sebbene non esistano linee guida universalmente accettate per interpretare il RMSEA, un valore intorno a 0,05 è spesso considerato indicativo di un buon adattamento del modello.\n\n\n61.5.4 Residuo Quadratico Medio Standardizzato (SRMSR)\nSRMSR è un’altra statistica di adattamento complessivo che confronta le correlazioni o le covarianze osservate tra tutte le coppie di item con quelle previste sotto il modello di Rasch (o un altro modello della teoria della risposta agli item). Valori vicini a 0 suggeriscono un buon adattamento del modello. Maydeu-Olivares (2013) raccomanda l’uso di un valore di soglia di 0.05 per SRMSR, simile al RMSEA.\nNel complesso, queste statistiche (M2, RMSEA e SRMSR) sono utili per valutare l’adattamento di un modello, come il modello di Rasch, a un dato insieme di dati di risposta agli item. Forniscono diverse prospettive attraverso le quali la congruenza tra i dati e il modello teorico può essere valutata, ognuna con il suo focus unico e metodo di calcolo.\n\nfit_rasch &lt;- mirt(responses, 1, itemtype = \"Rasch\", verbose = FALSE)\nfit_rasch\n\n\nCall:\nmirt(data = responses, model = 1, itemtype = \"Rasch\", verbose = FALSE)\n\nFull-information item factor analysis with 1 factor(s).\nConverged within 1e-04 tolerance after 16 EM iterations.\nmirt version: 1.42 \nM-step optimizer: nlminb \nEM acceleration: Ramsay \nNumber of rectangular quadrature: 61\nLatent density type: Gaussian \n\nLog-likelihood = -2816.487\nEstimated parameters: 15 \nAIC = 5662.973\nBIC = 5722.845; SABIC = 5675.249\nG2 (16368) = 1318.65, p = 1\nRMSEA = 0, CFI = NaN, TLI = NaN\n\n\n\nM2(fit_rasch)\n\n\nA data.frame: 1 x 9\n\n\n\nM2\ndf\np\nRMSEA\nRMSEA_5\nRMSEA_95\nSRMSR\nTLI\nCFI\n\n\n\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nstats\n277.5505\n90\n0\n0.0722689\n0.06261658\n0.08191791\n0.09413757\n0.7494602\n0.7522134\n\n\n\n\n\nLa statistica M2 è alta e significativa, indicando che ci sono differenze preoccupanti tra il modello e i dati. Questo è ulteriormente supportato da un RMSEA troppo alto e da un CFA e TLI lontani da 1.\nRicordiamo il significato degli indici RMSEA, CFA e TLI.\nRMSEA (Root Mean Square Error of Approximation): - Il RMSEA è una misura di adattamento che valuta quanto bene un modello si adatta ai dati a livello di popolazione. - Un valore basso di RMSEA indica un buon adattamento, suggerendo che il modello approssima bene la realtà. - Generalmente, un RMSEA inferiore a 0.05 o 0.06 è considerato indicativo di un ottimo adattamento del modello.\nCFA (Comparative Fit Index): - Il CFA è un indice relativo di bontà di adattamento che confronta il modello specificato con un modello nullo o di base. - Valori più vicini a 1 indicano un adattamento migliore. Un CFA superiore a 0.90 o 0.95 è spesso considerato indicativo di un buon adattamento.\nTLI (Tucker-Lewis Index): - Simile al CFA, il TLI è un altro indice relativo di adattamento che tiene conto della complessità del modello. - Anche per il TLI, valori più vicini a 1 indicano un adattamento migliore. Valori superiori a 0.90 o 0.95 sono generalmente considerati buoni.\n\n\n61.5.5 Valutare l’Adattamento degli Item\nTuttavia, nell’IRT, ci interessiamo maggiormente agli indici di adattamento degli item e delle persone. L’IRT ci consente di valutare quanto bene ogni item si adatti al modello e se i pattern di risposta individuali sono allineati con il modello.\nIniziamo con l’addattamento agli item. Sono stati proposti diversi indici per valutare l’adattamento degli item e possiamo utilizzare la funzione itemfit() per ottenere una varietà di questi indici. Di default, riceviamo l’S_X2 di Orlando e Thissen (2000) con i corrispondenti gradi di libertà (dfs), RMSEA e valori p. Questo test dovrebbe risultare non significativo per indicare un buon adattamento dell’item. Come vediamo qui sotto, diversi item mostra un cattivo adattamento.\n\nitemfit(fit_rasch)\n\n\nA mirt_df: 14 x 5\n\n\nitem\nS_X2\ndf.S_X2\nRMSEA.S_X2\np.S_X2\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nI1\n4.149893\n7\n0.00000000\n7.623599e-01\n\n\nI2\n19.333566\n7\n0.06645212\n7.204188e-03\n\n\nI3\n5.527890\n7\n0.00000000\n5.958220e-01\n\n\nI6\n14.085952\n8\n0.04366493\n7.955244e-02\n\n\nI7\n8.976726\n7\n0.02660344\n2.543313e-01\n\n\nI11\n23.364314\n7\n0.07654440\n1.472551e-03\n\n\nI12\n17.673493\n7\n0.06181841\n1.353327e-02\n\n\nI14\n9.789665\n7\n0.03160389\n2.008096e-01\n\n\nI17\n35.134304\n7\n0.10036508\n1.055277e-05\n\n\nI18\n3.696905\n7\n0.00000000\n8.139508e-01\n\n\nI19\n20.932189\n7\n0.07062756\n3.871820e-03\n\n\nI21\n86.542412\n7\n0.16875782\n6.334315e-16\n\n\nI22\n73.458312\n7\n0.15425498\n2.947293e-13\n\n\nI23\n7.611913\n7\n0.01480162\n3.680546e-01\n\n\n\n\n\n\n\n61.5.6 Statistiche di Infit e Outfit\nNella sezione precedente abbiamo discusso i test χ2 e M2, basati sul confronto tra le frequenze osservate e quelle attese secondo il modello di Rasch. Le statistiche di adattamento presentate di seguito si basano su un approccio simile, utilizzando i residui di Rasch. Questi sono le differenze tra le risposte osservate (vale a dire, le risposte 0 o 1 per gli item dicotomici) e i loro valori attesi (cioè, le probabilità predette di una risposta corretta secondo il modello di Rasch). Tipicamente, questi valori attesi vengono calcolati in base alle stime dei parametri degli item e delle persone.\nGeneralmente, quando c’è un buon adattamento tra i dati e il modello, si può prevedere che i residui siano piccoli. Pertanto, è naturale che i residui di Rasch possano essere utilizzati per valutare l’adattamento del modello di Rasch. Vedremo che nell’analisi di Rasch non solo i casi in cui i residui sono più grandi del previsto possono essere motivo di preoccupazione, ma anche quelli in cui i residui sono più piccoli del previsto.\nUn approccio comune per verificare l’adattamento di singoli item usando i residui di Rasch consiste nel calcolare le statistiche di infit e outfit. Descriveremo i passaggi per calcolare queste statistiche prima di affrontarne l’interpretazione. Ci concentreremo sul caso in cui queste statistiche vengono calcolate per singoli item.\n\n61.5.6.1 Outfit\nLa funzione principale della statistica di outfit è quella di quantificare in che misura le risposte dei partecipanti si allontanano dalle previsioni del modello. Questo indice si calcola attraverso diversi passaggi, che mirano a stabilire la misura in cui le risposte individuali si discostano dalle aspettative teoriche.\n1. Definizione dei Residui di Rasch: Inizialmente, per ogni partecipante e per ciascun item del test, si calcola il residuo di Rasch. Un residuo è essenzialmente la differenza tra la risposta osservata di un individuo a un determinato item e la risposta prevista da quel partecipante per lo stesso item. La risposta prevista è calcolata sulla base della probabilità, fornita dal modello di Rasch, che il partecipante risponda correttamente all’item. Ad esempio, se il modello prevede che un partecipante abbia il 40% di probabilità di rispondere correttamente a un item e il partecipante risponde effettivamente correttamente, il residuo corrispondente sarà $ 1 - 0.40 = 0.60 $.\n2. Standardizzazione dei Residui di Rasch: Successivamente, questi residui vengono standardizzati. La standardizzazione implica l’adeguamento dei residui in modo che abbiano una media di zero e una varianza di uno. Ciò permette di confrontare i residui in maniera uniforme, indipendentemente dalle caratteristiche specifiche degli item o dei partecipanti.\n3. Calcolo dello Z-Score: Per ciascun residuo, si calcola lo z-score standardizzato, $ Z_{si} $, utilizzando la formula:\n\\[\n   Z_{si} = \\frac{X_{si} - E(X_{si})}{\\sqrt{Var(X_{si})}},\n   \\]\ndove $ Z_{si} $ rappresenta lo z-score del residuo per il partecipante $ s $ all’item $ i $, $ X_{si} $ è la risposta osservata, $ E(X_{si}) $ è la risposta attesa (basata sulla probabilità di una risposta corretta secondo il modello di Rasch), e $ Var(X_{si}) $ è la varianza della risposta attesa.\n4. Calcolo della Statistica di Outfit: Per calcolare la statistica di outfit mean square (MSQ) per un specifico item, si seguono questi passaggi: - Si elevano al quadrato gli z-score standardizzati di ogni partecipante per l’item in questione. - Si sommano tutti questi valori quadrati. - Si divide la somma ottenuta per il numero totale dei partecipanti.\nLa formula risultante per la statistica di outfit MSQ per l’item $ i $ è la seguente:\n\\[\n   \\text{Outfit MSQ}_i = \\frac{\\sum_{p=1}^{P} Z_{pi}^2}{P}.\n   \\]\nQuesta procedura fornisce una misura dell’adattamento delle risposte degli individui all’item specifico, rispetto alle previsioni del modello di Rasch. Un valore di MSQ significativamente alto o basso può indicare potenziali discrepanze tra le risposte osservate e quelle previste, suggerendo la necessità di ulteriori analisi o revisioni del modello o degli item del test.\nSecondo Wright e Masters (1990), questa statistica ha un valore atteso di 1 sotto il modello di Rasch. Valori superiori a 1 indicano residui di Rasch più grandi del previsto secondo il modello di Rasch, e quindi una possibile violazione del modello. Tali item vengono anche detti mostrare un underfit. Valori inferiori a 1 indicano che i residui sono inferiori al previsto. Ciò è considerato indicare un overfit delle risposte al modello di Rasch. In questo contesto, overfit significa che la deviazione tra i valori attesi e i dati empirici è minore del previsto.\nPossiamo inoltre ottenere una statistica di mean square pesata e standardizzata per ciascun item, tipicamente denotata da ti. Siano \\(\\sqrt[3]{\\text{MSQ}_i}\\) e sd(MSQ_i) il cubo radice e la deviazione standard attesa di Outfit MSQ_i, rispettivamente. Allora la statistica standardizzata ti è\n\\[\n\\text{Outfit ti} = \\left( \\sqrt[3]{\\text{MSQ}_i} - 1 \\right) \\left( \\frac{3}{\\text{sd(MSQ}_i)} \\right) + \\left( \\frac{\\text{sd(MSQ}_i)}{3} \\right).\n\\]\nQuesta statistica standardizzata ti è spesso presentata nei risultati del software in aggiunta alla statistica MSQ.\nItem che mostrano underfit e overfit possono anche essere identificati approssimativamente usando le loro ICC empiriche, come abbiamo già visto in precedenza. Gli item che mostrano underfit hanno ICC empiriche più piatte di quelle previste sotto il modello di Rasch. Gli item che mostrano overfit hanno ICC empiriche più ripide del previsto.\n\n\n61.5.6.2 Infit\nL’indice di infit è un altro indice critico nel modello di Rasch. A differenza dell’outfit, che è più influenzato da risposte casuali o outlier, l’infit è più sensibile alle risposte che sono incoerenti con il pattern generale del modello. L’infit è calcolato come una media ponderata dei residui standardizzati, dove i pesi sono inversamente proporzionali alla varianza degli item. Questo rende l’infit particolarmente utile per identificare problemi di adattamento del modello legati alla consistenza interna delle risposte.\nLa statistica di infit MSQ, come quella di outfit, serve a valutare l’adattamento delle risposte individuali rispetto alle aspettative teoriche del modello. Tuttavia, la statistica di infit differisce dall’outfit per il modo in cui tratta i residui.\n1. Ponderazione dei Residui di Rasch: Nella statistica di infit, i residui di Rasch delle risposte individuali vengono ponderati in base alla loro varianza attesa sotto il modello di Rasch. Ciò significa che i residui con varianze minori (che tendono a verificarsi quando c’è una grande distanza tra le abilità dei rispondenti e la difficoltà degli item) hanno un impatto relativamente minore sulla statistica di infit rispetto a quelli con varianze maggiori.\n2. Riduzione dell’Impatto degli Outlier: Questo approccio di ponderazione rende la statistica di infit meno sensibile agli outlier rispetto all’outfit. In altre parole, mentre la statistica di outfit è influenzata in maniera più uniforme da tutte le deviazioni dalle aspettative del modello, l’infit dà maggiore peso alle deviazioni che sono meno estreme o più prevedibili data la struttura del modello.\n3. Formula per la Statistica di Infit MSQ: La formula per calcolare l’Infit MSQ per un dato item $ i $ è la seguente:\n\\[\n   \\text{Infit MSQ}_i = \\frac{\\sum_{p=1}^{P} W_{pi} Z_{pi}^2}{\\sum_{p=1}^{P} W_{pi}},\n   \\]\ndove: - $ Z_{pi} $ rappresenta il residuo di Rasch standardizzato per il rispondente $ p $ all’item $ i $. - $ W_{pi} $ è la varianza attesa del residuo $ Z_{pi} $ sotto il modello di Rasch. - $ P $ è il numero totale dei rispondenti.\n4. Standardizzazione della Statistica Infit: Come per l’outfit, è anche possibile calcolare una versione standardizzata dell’Infit MSQ per ogni item. Questa versione standardizzata, nota come statistica Infit t, consente di confrontare più facilmente l’adattamento degli item in diverse situazioni o in diversi test, normalizzando i valori su una scala comune.\nIn sintesi, la statistica di infit MSQ offre un modo ponderato per valutare l’adattamento delle risposte ai singoli item in un test basato sul modello di Rasch, tenendo conto della varianza attesa delle risposte. Questo la rende particolarmente utile per identificare i casi in cui le risposte si discostano dalle previsioni del modello in modi meno estremi o più in linea con la struttura del modello stesso.\n\n\n61.5.6.3 Soglie\nPer entrambi i valori MSQ e t delle statistiche di infit e outfit, sono stati proposti vari valori di soglia. Bond e Fox (2007) e Engelhard (2013) menzionano valori di soglia di -2 e 2 per le statistiche t, mentre Paek e Cole (2020) suggeriscono -3 e 3. Analogamente, Bond e Fox (2007) danno 0.75 e 1.3 come valori di soglia per le statistiche MSQ, mentre DeMars (2010) menziona 0.6 e 1.5 come possibili alternative. Desjardins e Bulut (2018), d’altra parte, si oppongono all’uso di valori di soglia specifici per queste statistiche.\nPossiamo calcolare le statistiche infit e oputfit degli item usando il pacchetto eRm:\n\nrm_sum0 &lt;- RM(responses)\neRm::itemfit(person.parameter(rm_sum0))\n\n\nItemfit Statistics: \n      Chisq  df p-value Outfit MSQ Infit MSQ Outfit t Infit t Discrim\nI1  325.410 397   0.996      0.818     0.904   -1.750  -1.666   0.418\nI2  273.788 397   1.000      0.688     0.809   -2.928  -3.271   0.520\nI3  289.188 397   1.000      0.727     0.869   -1.574  -1.505   0.371\nI6  333.574 397   0.991      0.838     0.860   -2.317  -3.257   0.505\nI7  272.900 397   1.000      0.686     0.838   -1.328  -1.423   0.279\nI11 332.540 397   0.992      0.836     0.816   -1.432  -3.143   0.473\nI12 458.651 397   0.018      1.152     0.972    1.538  -0.538   0.321\nI14 395.777 397   0.508      0.994     1.023   -0.043   0.492   0.320\nI17 524.132 397   0.000      1.317     0.936    2.290  -1.031   0.280\nI18 432.879 397   0.104      1.088     1.019    1.121   0.424   0.314\nI19 226.655 397   1.000      0.569     0.750   -2.579  -2.999   0.453\nI21 905.981 397   0.000      2.276     1.246    5.846   2.986  -0.108\nI22 727.958 397   0.000      1.829     0.985    2.918  -0.105   0.059\nI23 275.909 397   1.000      0.693     0.852   -1.918  -1.812   0.412\n\n\nIn alternativa, è possibile usare la funzione mirt del pacchetto mirt:\n\nmirt_rm &lt;- mirt(responses, 1, \"Rasch\")\nmirt::itemfit(mirt_rm, fit_stats = \"infit\", method = \"ML\")\n\nIteration: 16, Log-Lik: -2816.487, Max-Change: 0.00008\n\n\n\nA mirt_df: 14 x 5\n\n\nitem\noutfit\nz.outfit\ninfit\nz.infit\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nI1\n0.8138091\n-1.68334189\n0.9044759\n-1.6647753\n\n\nI2\n0.6845674\n-2.79308715\n0.8088811\n-3.2706129\n\n\nI3\n0.7240040\n-1.50990166\n0.8700715\n-1.4928072\n\n\nI6\n0.8343214\n-2.27167027\n0.8601240\n-3.2582397\n\n\nI7\n0.6824297\n-1.35748185\n0.8313778\n-1.5036967\n\n\nI11\n0.8317490\n-1.38101079\n0.8158106\n-3.1426850\n\n\nI12\n1.1478287\n1.48400995\n0.9712695\n-0.5543699\n\n\nI14\n0.9902636\n-0.08597728\n1.0231872\n0.4939914\n\n\nI17\n1.3153478\n2.27622298\n0.9351173\n-1.0397905\n\n\nI18\n1.0831826\n1.01125192\n1.0194375\n0.4257799\n\n\nI19\n0.5690339\n-2.58505641\n0.7482922\n-3.0143544\n\n\nI21\n2.2786259\n5.85170614\n1.2442387\n2.9601154\n\n\nI22\n1.8226830\n2.91124500\n0.9781526\n-0.1650957\n\n\nI23\n0.6903966\n-1.83114254\n0.8525321\n-1.8039446\n\n\n\n\n\nLa tabella risultante inizia con le statistiche del test di adattamento χ2 approssimativo, i suoi gradi di libertà e i valori di p risultanti. Se il modello di Rasch è valido, la statistica di test risultante può essere approssimativamente descritta da una distribuzione χ2, il che porta ai valori di p presentati.\nLe colonne seguenti presentano le statistiche MSQ e t di infit e outfit. Per le statistiche MSQ di infit e outfit, valori vicini a 1 indicano un buon adattamento del modello, mentre per le statistiche t di infit e outfit, valori vicini a 0 indicano un buon adattamento. Valori più alti indicano che le risposte sono più casuali di quanto previsto dal modello di Rasch, segnalando un sottoadattamento (underfit); valori più bassi indicano che le risposte sono meno casuali del previsto, segnalando un sovradattamento (overfit).\nSeguendo una delle linee guida proposte, esamineremo ulteriormente quegli item i cui valori t di infit o outfit sono inferiori a -2 o superiori a 2 (ma esistono linee guida alternative). Troviamo che per gli item 2, 6, 11 e 19, almeno un valore t è inferiore a -2, indicando un sovradattamento. Per l’item 19 ciò è supportato dal fatto che la ICC empirica ha una pendenza più ripida rispetto alla ICC attesa.\nPer gli item 17, 21 e 22, invece, almeno un valore t per le statistiche di infit e outfit è superiore a 2, indicando un sottoadattamento. Questo è nuovamente in linea con l’esame delle ICC, dove abbiamo riscontrato che la ICC empirica per l’item 21 ha una pendenza inferiore rispetto alla ICC attesa.\n\nitemfitPlot(mirt_rm)\n\n`geom_line()`: Each group consists of only one observation.\ni Do you need to adjust the group aesthetic?\n`geom_line()`: Each group consists of only one observation.\ni Do you need to adjust the group aesthetic?\n\n\n\n\n\n\n\n\n\n\n\n\n61.5.7 Valutare l’Adattamento delle Persone\nPossiamo generare le stesse misure di adattamento per ogni persona per valutare quanto bene i pattern di risposta di ciascuno si allineano con il modello. Ragioniamo in questo modo: se una persona con un alto valore di \\(\\theta\\) (cioè alta abilità latente) non risponde correttamente a un item facile, questa persona non si adatta bene al modello. Al contrario, se una persona con bassa abilità risponde correttamente a una domanda molto difficile, anche questo non è conforme al modello. Nella pratica, è probabile che ci saranno alcune persone che non si adattano bene al modello. Tuttavia, finché il numero di rispondenti non conformi è basso, la situazione è accettabile. Di solito, ci concentriamo nuovamente sulle statistiche di infit e outfit. Se meno del 5% dei rispondenti presenta valori di infit e outfit superiori o inferiori a 1.96 e -1.96, possiamo considerare il modello adeguato.\nStimiamo gli indici infit e outfit delle persone usando eRm:\n\neRm::personfit(person.parameter(rm_sum0))\n\n\nPersonfit Statistics: \n      Chisq df p-value Outfit MSQ Infit MSQ Outfit t Infit t\n1    13.191 13   0.433      0.942     1.080     0.07    0.35\n2     7.004 13   0.902      0.500     0.744    -0.44   -0.85\n3     7.564 13   0.871      0.540     0.799    -0.37   -0.62\n4    13.651 13   0.399      0.975     1.112     0.14    0.44\n5     4.024 13   0.991      0.287     0.354    -1.81   -2.20\n6    22.049 13   0.055      1.575     1.304     0.96    1.01\n7    22.823 13   0.044      1.630     1.623     1.02    1.83\n8    12.937 13   0.453      0.924     0.746     0.00   -0.66\n9    71.924 13   0.000      5.137     1.376     2.32    1.11\n10   22.507 13   0.048      1.608     1.065     1.12    0.30\n11    8.244 13   0.827      0.589     0.786    -0.83   -0.52\n12   17.925 13   0.160      1.280     0.916     0.67   -0.10\n13    5.629 13   0.959      0.402     0.667    -0.33   -1.01\n14   18.081 13   0.154      1.291     0.950     0.62   -0.07\n15    9.427 13   0.740      0.673     0.890    -0.50   -0.24\n16   49.152 13   0.000      3.511     1.562     1.53    1.20\n17   16.140 13   0.242      1.153     1.094     0.46    0.37\n18    7.400 13   0.880      0.529     0.785    -0.39   -0.68\n19    4.698 13   0.981      0.336     0.424    -1.70   -1.96\n20    4.698 13   0.981      0.336     0.424    -1.70   -1.96\n21   45.736 13   0.000      3.267     1.221     1.99    0.79\n22    5.441 13   0.964      0.389     0.501    -1.49   -1.60\n23    6.693 13   0.917      0.478     0.812    -0.21   -0.49\n24    9.043 13   0.770      0.646     0.957    -0.19   -0.05\n25   11.936 13   0.533      0.853     1.032    -0.01    0.21\n26    8.349 13   0.820      0.596     0.851    -0.49   -0.42\n27   28.139 13   0.009      2.010     1.853     1.40    2.35\n28    7.921 13   0.849      0.566     0.731    -0.84   -0.66\n29   65.841 13   0.000      4.703     1.677     1.83    1.38\n30    8.297 13   0.824      0.593     0.752    -0.70   -0.71\n31    6.582 13   0.922      0.470     0.660    -0.78   -1.18\n32   11.044 13   0.607      0.789     1.055    -0.12    0.28\n33   10.084 13   0.687      0.720     0.908    -0.39   -0.18\n34   13.555 13   0.406      0.968     1.375     0.25    1.23\n35   23.622 13   0.035      1.687     1.170     1.23    0.60\n36   10.893 13   0.620      0.778     0.863    -0.33   -0.28\n37    6.064 13   0.944      0.433     0.585    -1.15   -1.38\n38   28.190 13   0.009      2.014     1.700     1.77    1.73\n39   13.747 13   0.392      0.982     1.271     0.36    0.85\n40   34.164 13   0.001      2.440     1.211     1.50    0.76\n41   71.924 13   0.000      5.137     1.376     2.32    1.11\n42   10.846 13   0.624      0.775     0.840    -0.31   -0.32\n43    4.024 13   0.991      0.287     0.354    -1.81   -2.20\n44    6.981 13   0.903      0.499     0.661    -1.04   -0.90\n45   68.333 13   0.000      4.881     1.024     2.24    0.18\n46   15.795 13   0.260      1.128     1.141     0.41    0.55\n47   21.711 13   0.060      1.551     1.756     0.86    1.83\n48   13.778 13   0.390      0.984     1.094     0.25    0.37\n49   15.991 13   0.250      1.142     1.334     0.60    0.81\n50   10.989 13   0.612      0.785     1.008    -0.24    0.13\n51   12.656 13   0.475      0.904     1.053    -0.04    0.27\n52    4.166 13   0.989      0.298     0.804     0.40    0.00\n53    7.533 13   0.873      0.538     0.972    -0.17    0.04\n54    5.025 13   0.975      0.359     0.491    -1.08   -1.99\n55   11.323 13   0.584      0.809     1.061    -0.08    0.30\n56   42.848 13   0.000      3.061     1.148     1.39    0.46\n57   30.248 13   0.004      2.161     1.909     1.95    2.13\n58   17.432 13   0.180      1.245     1.137     0.57    0.47\n59    5.671 13   0.957      0.405     0.521    -1.43   -1.51\n60    4.093 13   0.990      0.292     0.390    -1.43   -2.02\n61   16.035 13   0.247      1.145     1.125     0.43    0.44\n62   18.197 13   0.150      1.300     1.095     0.65    0.37\n63   19.944 13   0.097      1.425     1.438     0.90    1.15\n64   37.689 13   0.000      2.692     2.222     1.74    2.65\n65   24.532 13   0.027      1.752     1.618     1.21    1.51\n66   12.174 13   0.513      0.870     0.973     0.09    0.04\n67   19.625 13   0.105      1.402     1.199     0.84    0.68\n68   12.566 13   0.482      0.898     0.877    -0.05   -0.24\n69   18.670 13   0.134      1.334     1.365     0.74    1.10\n70    7.604 13   0.868      0.543     0.755    -0.60   -0.78\n71    9.725 13   0.716      0.695     0.926    -0.50   -0.08\n72    5.025 13   0.975      0.359     0.491    -1.08   -1.99\n73    7.079 13   0.898      0.506     0.637    -1.02   -0.98\n74    7.604 13   0.868      0.543     0.755    -0.60   -0.78\n75    4.219 13   0.989      0.301     0.456    -0.98   -1.79\n76    5.671 13   0.957      0.405     0.521    -1.43   -1.51\n77    9.064 13   0.768      0.647     0.829    -0.38   -0.50\n78    8.268 13   0.826      0.591     0.796    -0.58   -0.46\n79   11.044 13   0.607      0.789     1.055    -0.12    0.28\n80    5.828 13   0.952      0.416     0.549    -1.31   -1.31\n81   10.485 13   0.654      0.749     0.917    -0.24   -0.10\n82   10.894 13   0.620      0.778     1.038    -0.04    0.22\n83   10.090 13   0.687      0.721     0.868    -0.30   -0.24\n84    6.768 13   0.914      0.483     0.619    -1.09   -1.05\n85    4.024 13   0.991      0.287     0.354    -1.81   -2.20\n86    6.471 13   0.927      0.462     0.649    -0.80   -1.22\n87    8.129 13   0.835      0.581     0.875    -0.36   -0.24\n88   12.393 13   0.496      0.885     0.903    -0.06   -0.14\n89    5.671 13   0.957      0.405     0.521    -1.43   -1.51\n90    5.123 13   0.972      0.366     0.513    -1.18   -1.46\n91    5.358 13   0.966      0.383     0.507    -1.32   -1.73\n92    4.093 13   0.990      0.292     0.390    -1.43   -2.02\n93    6.582 13   0.922      0.470     0.660    -0.78   -1.18\n94   12.089 13   0.520      0.864     0.995    -0.08    0.09\n95    5.629 13   0.959      0.402     0.667    -0.33   -1.01\n96   24.693 13   0.025      1.764     1.076     1.33    0.34\n97   15.117 13   0.300      1.080     1.025     0.32    0.19\n98    9.836 13   0.707      0.703     0.918    -0.43   -0.15\n99   10.893 13   0.620      0.778     0.863    -0.33   -0.28\n100   7.268 13   0.888      0.519     0.764    -0.41   -0.76\n101   8.268 13   0.826      0.591     0.796    -0.58   -0.46\n102  13.275 13   0.427      0.948     0.998     0.06    0.11\n103  20.669 13   0.080      1.476     1.181     0.98    0.58\n104  24.874 13   0.024      1.777     1.370     1.40    1.00\n105   6.582 13   0.922      0.470     0.660    -0.78   -1.18\n106   5.025 13   0.975      0.359     0.491    -1.08   -1.99\n107   5.159 13   0.972      0.368     0.485    -1.37   -1.84\n108  22.383 13   0.050      1.599     1.205     1.16    0.64\n109   4.024 13   0.991      0.287     0.354    -1.81   -2.20\n110   4.698 13   0.981      0.336     0.424    -1.70   -1.96\n111   5.025 13   0.975      0.359     0.491    -1.08   -1.99\n112   6.942 13   0.905      0.496     0.675    -0.81   -0.85\n113   5.358 13   0.966      0.383     0.507    -1.32   -1.73\n114   9.883 13   0.703      0.706     0.844    -0.51   -0.34\n115  18.178 13   0.151      1.298     1.534     0.63    1.62\n116   4.347 13   0.987      0.310     0.634    -0.11   -0.77\n117  10.403 13   0.661      0.743     1.095    -0.05    0.41\n118  26.269 13   0.016      1.876     1.226     1.46    0.75\n119   5.828 13   0.952      0.416     0.549    -1.31   -1.31\n120  40.540 13   0.000      2.896     1.718     1.51    1.87\n121   5.441 13   0.964      0.389     0.501    -1.49   -1.60\n122  28.230 13   0.008      2.016     1.666     1.41    1.93\n123   5.638 13   0.958      0.403     0.735    -0.38   -0.69\n124   5.671 13   0.957      0.405     0.521    -1.43   -1.51\n125  38.771 13   0.000      2.769     1.219     1.46    0.72\n126  45.617 13   0.000      3.258     1.202     1.98    0.73\n127 102.531 13   0.000      7.324     1.711     2.35    1.44\n128  40.524 13   0.000      2.895     1.439     1.51    1.26\n129   7.301 13   0.886      0.521     0.779    -0.40   -0.70\n130  26.975 13   0.013      1.927     1.304     1.52    0.95\n131   8.842 13   0.785      0.632     0.892    -0.42   -0.27\n132   9.427 13   0.740      0.673     0.890    -0.50   -0.24\n133  27.405 13   0.011      1.957     1.442     1.35    1.38\n134  19.833 13   0.099      1.417     1.647     0.72    1.92\n135  25.521 13   0.020      1.823     1.546     1.22    1.64\n136  11.116 13   0.601      0.794     1.081    -0.15    0.33\n137  17.812 13   0.165      1.272     1.014     0.67    0.16\n138  16.064 13   0.246      1.147     1.220     0.45    0.67\n139   5.739 13   0.955      0.410     0.644    -0.70   -1.01\n140   5.123 13   0.972      0.366     0.513    -1.18   -1.46\n141   7.079 13   0.898      0.506     0.637    -1.02   -0.98\n142  11.503 13   0.569      0.822     1.160     0.06    0.61\n143  10.950 13   0.615      0.782     0.932    -0.18   -0.06\n144   6.064 13   0.944      0.433     0.585    -1.15   -1.38\n145   4.917 13   0.977      0.351     0.633    -0.47   -1.06\n146   5.284 13   0.968      0.377     0.630    -0.37   -1.16\n147  24.053 13   0.031      1.718     1.470     1.37    1.26\n148  15.117 13   0.300      1.080     1.025     0.32    0.19\n149   5.441 13   0.964      0.389     0.501    -1.49   -1.60\n150   5.159 13   0.972      0.368     0.485    -1.37   -1.84\n151  11.637 13   0.558      0.831     0.940    -0.20   -0.05\n152   8.673 13   0.797      0.619     0.812    -0.44   -0.56\n153   6.263 13   0.936      0.447     0.628    -0.83   -1.32\n154  15.001 13   0.307      1.071     1.263     0.31    0.77\n155  17.014 13   0.199      1.215     1.418     0.55    1.23\n156   7.268 13   0.888      0.519     0.764    -0.41   -0.76\n157  12.366 13   0.498      0.883     1.136    -0.04    0.50\n158   8.924 13   0.779      0.637     0.938    -0.21   -0.11\n159   9.201 13   0.758      0.657     0.810    -0.59   -0.41\n160   5.441 13   0.964      0.389     0.501    -1.49   -1.60\n161  27.205 13   0.012      1.943     1.221     1.16    0.79\n162   5.123 13   0.972      0.366     0.513    -1.18   -1.46\n163  58.610 13   0.000      4.186     1.621     2.44    1.86\n164  10.699 13   0.636      0.764     0.896    -0.16   -0.26\n165  41.122 13   0.000      2.937     1.378     1.53    1.12\n166   5.159 13   0.972      0.368     0.485    -1.37   -1.84\n167  58.082 13   0.000      4.149     1.313     1.56    0.62\n168  17.858 13   0.163      1.276     1.502     0.60    1.54\n169 146.257 13   0.000     10.447     1.338     2.27    0.65\n170  21.252 13   0.068      1.518     1.420     1.00    1.24\n171  26.862 13   0.013      1.919     1.962     1.31    2.59\n172   4.219 13   0.989      0.301     0.456    -0.98   -1.79\n173   9.220 13   0.756      0.659     1.019     0.02    0.17\n174   7.856 13   0.853      0.561     0.817    -0.10   -0.47\n175  11.453 13   0.573      0.818     0.834    -0.21   -0.34\n176   7.466 13   0.877      0.533     0.799    -0.13   -0.53\n177   4.219 13   0.989      0.301     0.456    -0.98   -1.79\n178  10.699 13   0.636      0.764     0.896    -0.16   -0.26\n179  20.347 13   0.087      1.453     1.320     0.91    0.99\n180 123.641 13   0.000      8.832     2.158     3.23    2.72\n181   8.282 13   0.825      0.592     1.068     0.61    0.35\n182   8.856 13   0.784      0.633     0.808    -0.41   -0.58\n183  48.852 13   0.000      3.489     1.539     2.10    1.65\n184   4.093 13   0.990      0.292     0.390    -1.43   -2.02\n185  10.976 13   0.613      0.784     1.153     0.01    0.59\n186   4.219 13   0.989      0.301     0.456    -0.98   -1.79\n187   6.768 13   0.914      0.483     0.619    -1.09   -1.05\n188  46.013 13   0.000      3.287     1.243     2.00    0.86\n189  36.359 13   0.001      2.597     1.751     1.60    2.16\n190  20.042 13   0.094      1.432     1.448     0.88    1.30\n191   5.159 13   0.972      0.368     0.485    -1.37   -1.84\n192  58.333 13   0.000      4.167     1.599     2.43    1.80\n193   7.144 13   0.895      0.510     0.663    -0.92   -1.06\n194   5.671 13   0.957      0.405     0.521    -1.43   -1.51\n195  11.730 13   0.550      0.838     0.857    -0.16   -0.27\n196  13.872 13   0.383      0.991     0.924     0.14   -0.10\n197   9.064 13   0.768      0.647     0.829    -0.38   -0.50\n198   4.281 13   0.988      0.306     0.544    -0.56   -1.41\n199   5.739 13   0.955      0.410     0.644    -0.70   -1.01\n200  10.577 13   0.646      0.755     0.746    -0.35   -0.61\n201  17.316 13   0.185      1.237     1.268     0.60    0.78\n202   6.263 13   0.936      0.447     0.628    -0.83   -1.32\n203   9.807 13   0.710      0.700     0.767    -0.43   -0.66\n204   6.263 13   0.936      0.447     0.628    -0.83   -1.32\n205  10.485 13   0.654      0.749     0.917    -0.24   -0.10\n206   9.169 13   0.760      0.655     1.086    -0.01    0.35\n207  16.656 13   0.215      1.190     1.409     0.52    1.13\n208  11.758 13   0.548      0.840     0.980    -0.07    0.07\n209   5.828 13   0.952      0.416     0.549    -1.31   -1.31\n210  14.935 13   0.311      1.067     1.217     0.35    0.68\n211   5.348 13   0.967      0.382     0.570    -0.68   -1.62\n212   5.441 13   0.964      0.389     0.501    -1.49   -1.60\n213   5.671 13   0.957      0.405     0.521    -1.43   -1.51\n214   5.123 13   0.972      0.366     0.513    -1.18   -1.46\n215   5.638 13   0.958      0.403     0.735    -0.38   -0.69\n216   6.768 13   0.914      0.483     0.619    -1.09   -1.05\n217   5.775 13   0.954      0.413     0.860    -0.01   -0.18\n218  12.704 13   0.471      0.907     0.921    -0.01   -0.09\n219   5.920 13   0.949      0.423     0.747    -0.34   -0.65\n220   4.698 13   0.981      0.336     0.424    -1.70   -1.96\n221  27.073 13   0.012      1.934     1.200     1.15    0.73\n222   7.441 13   0.878      0.532     0.713    -0.86   -0.86\n223   5.629 13   0.959      0.402     0.667    -0.33   -1.01\n224   4.219 13   0.989      0.301     0.456    -0.98   -1.79\n225   9.128 13   0.763      0.652     0.859    -0.44   -0.27\n226  14.019 13   0.372      1.001     1.074     0.17    0.32\n227  10.950 13   0.615      0.782     0.932    -0.18   -0.06\n228  13.778 13   0.390      0.984     1.094     0.25    0.37\n229   5.671 13   0.957      0.405     0.521    -1.43   -1.51\n230   8.261 13   0.826      0.590     0.741    -0.77   -0.63\n231   6.768 13   0.914      0.483     0.619    -1.09   -1.05\n232  10.663 13   0.639      0.762     0.843    -0.37   -0.34\n233  62.458 13   0.000      4.461     2.648     3.13    3.89\n234   8.736 13   0.793      0.624     0.755    -0.50   -0.59\n235   7.856 13   0.853      0.561     0.817    -0.10   -0.47\n236   7.921 13   0.849      0.566     0.731    -0.84   -0.66\n237   7.921 13   0.849      0.566     0.731    -0.84   -0.66\n238   4.374 13   0.987      0.312     0.865     0.40    0.07\n239  18.149 13   0.152      1.296     1.223     0.92    0.53\n240  33.118 13   0.002      2.366     1.889     1.72    2.43\n241   4.024 13   0.991      0.287     0.354    -1.81   -2.20\n242   8.795 13   0.788      0.628     0.782    -0.71   -0.54\n243   7.441 13   0.878      0.532     0.713    -0.86   -0.86\n244   8.232 13   0.828      0.588     0.749    -0.78   -0.60\n245   5.629 13   0.959      0.402     0.667    -0.33   -1.01\n246   4.219 13   0.989      0.301     0.456    -0.98   -1.79\n247  22.642 13   0.046      1.617     0.967     1.00   -0.01\n248  16.010 13   0.249      1.144     1.334     0.44    0.96\n249   4.024 13   0.991      0.287     0.354    -1.81   -2.20\n250  18.843 13   0.128      1.346     1.182     0.65    0.62\n251   4.347 13   0.987      0.310     0.634    -0.11   -0.77\n252   9.201 13   0.758      0.657     0.810    -0.59   -0.41\n253   6.471 13   0.927      0.462     0.649    -0.80   -1.22\n254   9.300 13   0.750      0.664     0.822    -0.62   -0.41\n255   5.671 13   0.957      0.405     0.521    -1.43   -1.51\n256  18.159 13   0.152      1.297     1.362     0.70    0.99\n257  12.841 13   0.460      0.917     1.162     0.07    0.53\n258   5.358 13   0.966      0.383     0.507    -1.32   -1.73\n259   6.914 13   0.907      0.494     0.910     0.11   -0.05\n260   7.500 13   0.875      0.536     0.710    -0.98   -0.78\n261   6.981 13   0.903      0.499     0.661    -1.04   -0.90\n262   4.024 13   0.991      0.287     0.354    -1.81   -2.20\n263   7.784 13   0.857      0.556     1.094     0.58    0.37\n264   8.736 13   0.793      0.624     0.755    -0.50   -0.59\n265   9.302 13   0.750      0.664     1.038    -0.21    0.23\n266   8.484 13   0.811      0.606     0.901    -0.26   -0.24\n267   4.219 13   0.989      0.301     0.456    -0.98   -1.79\n268  20.660 13   0.080      1.476     1.214     0.84    0.76\n269   6.471 13   0.927      0.462     0.649    -0.80   -1.22\n270   8.367 13   0.819      0.598     0.829    -0.27   -0.51\n271  23.034 13   0.041      1.645     1.150     0.95    0.52\n272   8.666 13   0.798      0.619     0.756    -0.51   -0.58\n273   7.851 13   0.853      0.561     0.740    -0.78   -0.76\n274  19.063 13   0.121      1.362     1.210     0.66    0.70\n275   8.232 13   0.828      0.588     0.749    -0.78   -0.60\n276  14.978 13   0.309      1.070     1.236     0.31    0.78\n277  12.381 13   0.497      0.884     1.034    -0.08    0.21\n278   4.917 13   0.977      0.351     0.633    -0.47   -1.06\n279   5.123 13   0.972      0.366     0.513    -1.18   -1.46\n280  11.266 13   0.589      0.805     1.035     0.00    0.22\n281  30.642 13   0.004      2.189     1.642     1.39    1.61\n282  16.130 13   0.242      1.152     1.025     0.45    0.19\n283  10.799 13   0.628      0.771     1.136    -0.01    0.54\n284   5.441 13   0.964      0.389     0.501    -1.49   -1.60\n285  17.337 13   0.184      1.238     1.623     0.57    1.67\n286   5.894 13   0.950      0.421     0.769    -0.35   -0.58\n287  32.260 13   0.002      2.304     1.363     1.77    0.99\n288  23.818 13   0.033      1.701     1.277     1.00    0.82\n289   4.024 13   0.991      0.287     0.354    -1.81   -2.20\n290   9.693 13   0.719      0.692     0.728    -0.50   -0.67\n291  11.568 13   0.563      0.826     0.991    -0.21    0.09\n292  15.005 13   0.307      1.072     1.001     0.32    0.13\n293  12.043 13   0.524      0.860     0.736    -0.14   -0.69\n294   8.465 13   0.812      0.605     0.790    -0.47   -0.64\n296   5.739 13   0.955      0.410     0.644    -0.70   -1.01\n297  15.671 13   0.267      1.119     1.279     0.41    0.83\n298   7.784 13   0.857      0.556     1.094     0.58    0.37\n299  15.524 13   0.276      1.109     1.298     0.38    0.85\n300  32.032 13   0.002      2.288     1.795     2.03    1.84\n301   5.025 13   0.975      0.359     0.491    -1.08   -1.99\n302   5.358 13   0.966      0.383     0.507    -1.32   -1.73\n303  10.577 13   0.646      0.755     0.746    -0.35   -0.61\n304  12.076 13   0.521      0.863     0.975    -0.08    0.03\n305  14.767 13   0.322      1.055     0.934     0.27   -0.07\n306  18.873 13   0.127      1.348     1.290     0.78    0.83\n308  14.151 13   0.363      1.011     1.091     0.28    0.37\n309   5.739 13   0.955      0.410     0.644    -0.70   -1.01\n310  18.159 13   0.152      1.297     1.087     0.72    0.36\n311   5.348 13   0.967      0.382     0.570    -0.68   -1.62\n312  16.035 13   0.247      1.145     1.125     0.43    0.44\n313  23.034 13   0.041      1.645     1.150     0.95    0.52\n314   6.942 13   0.905      0.496     0.675    -0.81   -0.85\n315  11.315 13   0.584      0.808     1.024    -0.23    0.19\n316   7.471 13   0.876      0.534     0.633    -0.99   -1.06\n317  24.891 13   0.024      1.778     1.098     1.34    0.40\n318  18.175 13   0.151      1.298     0.811     0.70   -0.41\n319   6.024 13   0.945      0.430     0.903     0.01   -0.08\n320  13.667 13   0.398      0.976     1.072     0.11    0.32\n321   7.500 13   0.875      0.536     0.710    -0.98   -0.78\n322   8.736 13   0.793      0.624     0.755    -0.50   -0.59\n323  13.747 13   0.392      0.982     1.201     0.18    0.63\n324  28.435 13   0.008      2.031     1.448     1.03    1.04\n325   6.896 13   0.907      0.493     0.768    -0.53   -0.58\n326  15.660 13   0.268      1.119     1.228     0.39    0.69\n327   8.736 13   0.793      0.624     0.755    -0.50   -0.59\n328   4.024 13   0.991      0.287     0.354    -1.81   -2.20\n329   9.693 13   0.719      0.692     0.728    -0.50   -0.67\n330   5.441 13   0.964      0.389     0.501    -1.49   -1.60\n331   4.093 13   0.990      0.292     0.390    -1.43   -2.02\n332   7.441 13   0.878      0.532     0.713    -0.86   -0.86\n333   5.123 13   0.972      0.366     0.513    -1.18   -1.46\n334   8.049 13   0.840      0.575     0.762    -0.74   -0.68\n335  31.383 13   0.003      2.242     1.672     1.97    1.61\n336   4.698 13   0.981      0.336     0.424    -1.70   -1.96\n337  10.034 13   0.691      0.717     0.928    -0.13   -0.08\n338  11.779 13   0.546      0.841     1.183     0.09    0.68\n339  11.266 13   0.589      0.805     1.035     0.00    0.22\n340  30.471 13   0.004      2.176     1.461     1.65    1.20\n341   4.093 13   0.990      0.292     0.390    -1.43   -2.02\n342  17.006 13   0.199      1.215     1.125     0.54    0.45\n343   5.123 13   0.972      0.366     0.513    -1.18   -1.46\n344  12.126 13   0.517      0.866     0.920    -0.10   -0.09\n345  12.257 13   0.507      0.876     0.905     0.10   -0.15\n346  12.937 13   0.453      0.924     0.746     0.00   -0.66\n347  21.895 13   0.057      1.564     1.668     0.81    1.67\n348  13.515 13   0.409      0.965     0.934     0.10   -0.05\n349   5.739 13   0.955      0.410     0.644    -0.70   -1.01\n350  12.704 13   0.471      0.907     0.921    -0.01   -0.09\n351  23.700 13   0.034      1.693     1.714     1.14    1.69\n352   7.471 13   0.876      0.534     0.633    -0.99   -1.06\n353  26.390 13   0.015      1.885     1.588     1.60    1.51\n354  15.315 13   0.288      1.094     1.203     0.35    0.69\n355   4.024 13   0.991      0.287     0.354    -1.81   -2.20\n356   7.144 13   0.895      0.510     0.663    -0.92   -1.06\n357  13.627 13   0.401      0.973     1.200     0.11    0.64\n358   5.441 13   0.964      0.389     0.501    -1.49   -1.60\n359  18.068 13   0.155      1.291     1.289     0.62    0.97\n360  11.936 13   0.533      0.853     1.032    -0.01    0.21\n361   8.297 13   0.824      0.593     0.752    -0.70   -0.71\n362  21.689 13   0.060      1.549     1.504     1.09    1.28\n363   8.673 13   0.797      0.619     0.812    -0.44   -0.56\n364   8.349 13   0.820      0.596     0.851    -0.49   -0.42\n365  17.601 13   0.173      1.257     1.041     0.65    0.23\n366  20.608 13   0.081      1.472     1.003     0.97    0.13\n367   8.282 13   0.825      0.592     1.068     0.61    0.35\n368   4.567 13   0.984      0.326     0.848     0.42    0.06\n369  63.802 13   0.000      4.557     1.160     1.79    0.49\n370  38.911 13   0.000      2.779     1.507     1.46    1.42\n371  20.974 13   0.073      1.498     1.488     0.91    1.25\n372  31.487 13   0.003      2.249     1.150     1.62    0.57\n373   5.159 13   0.972      0.368     0.485    -1.37   -1.84\n374   7.466 13   0.877      0.533     0.799    -0.13   -0.53\n375  26.269 13   0.016      1.876     1.226     1.46    0.75\n376   7.268 13   0.888      0.519     0.764    -0.41   -0.76\n377   8.747 13   0.792      0.625     0.921    -0.23   -0.17\n378   6.389 13   0.931      0.456     0.873     0.07   -0.14\n379  19.958 13   0.096      1.426     1.151     0.79    0.58\n380  11.666 13   0.555      0.833     1.198     0.21    0.66\n381   9.056 13   0.769      0.647     0.959    -0.19   -0.04\n382   6.735 13   0.915      0.481     0.635    -1.01   -1.17\n383   5.671 13   0.957      0.405     0.521    -1.43   -1.51\n384  46.623 13   0.000      3.330     1.814     1.70    2.07\n385  27.073 13   0.012      1.934     1.200     1.15    0.73\n386   9.883 13   0.703      0.706     0.844    -0.51   -0.34\n387   9.512 13   0.733      0.679     0.828    -0.53   -0.36\n388  20.986 13   0.073      1.499     1.349     1.05    1.00\n389   8.214 13   0.829      0.587     0.737    -0.59   -0.64\n390   5.671 13   0.957      0.405     0.521    -1.43   -1.51\n391   7.400 13   0.880      0.529     0.785    -0.39   -0.68\n392   4.024 13   0.991      0.287     0.354    -1.81   -2.20\n393   9.024 13   0.771      0.645     0.802    -0.67   -0.47\n394   8.232 13   0.828      0.588     0.749    -0.78   -0.60\n395  19.666 13   0.104      1.405     1.465     0.90    1.25\n396   8.664 13   0.798      0.619     0.865    -0.24   -0.38\n397   5.358 13   0.966      0.383     0.507    -1.32   -1.73\n398   4.698 13   0.981      0.336     0.424    -1.70   -1.96\n399  14.347 13   0.350      1.025     1.217     0.26    0.77\n400   5.284 13   0.968      0.377     0.630    -0.37   -1.16\n\n\nCome per le statistiche di infit e outfit per i singoli item, individui con valori di t superiori a 2 mostrano un comportamento di risposta più casuale rispetto a quanto previsto dal modello di Rasch. Questo può indicare, ad esempio, comportamenti di risposta basati su supposizioni o scarsa attenzione. I modelli di risposta che portano a valori di t inferiori a -2 indicano un comportamento di risposta più deterministico rispetto a quello atteso. In questo esempio, le persone identificate con il numero 5 e 43 mostrano questo comportamento.\nOtteniamo le stime infit e outfit per le persone con mirt:\n\nhead(personfit(mirt_rm))\n\n\nA data.frame: 6 x 5\n\n\n\noutfit\nz.outfit\ninfit\nz.infit\nZh\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n0.9312232\n0.04185066\n1.0902553\n0.3722087\n-0.1421791\n\n\n2\n0.5102523\n-0.64959154\n0.7279243\n-0.8983287\n0.8888458\n\n\n3\n0.5477460\n-0.56603982\n0.7820722\n-0.6817931\n0.7512104\n\n\n4\n0.9963324\n0.16858181\n1.1111285\n0.4302049\n-0.2315919\n\n\n5\n0.2892823\n-1.88738642\n0.3522188\n-2.2386237\n1.6537608\n\n\n6\n1.4647442\n0.88480217\n1.3150003\n1.0095971\n-1.0020311\n\n\n\n\n\n\npersonfit(mirt_rm) %&gt;%\n    summarize(\n        infit.outside = prop.table(table(z.infit &gt; 1.96 | z.infit &lt; -1.96)),\n        outfit.outside = prop.table(table(z.outfit &gt; 1.96 | z.outfit &lt; -1.96))\n    ) # lower row = non-fitting people\n\nWarning message:\n\"Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\ni Please use `reframe()` instead.\ni When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\"\n\n\n\nA data.frame: 2 x 2\n\n\ninfit.outside\noutfit.outside\n\n\n&lt;table[1d]&gt;\n&lt;table[1d]&gt;\n\n\n\n\n0.9175\n0.98\n\n\n0.0825\n0.02\n\n\n\n\n\n\npersonfitPlot(mirt_rm)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nIn conclusione, nel caso dei dati in esame, meno del 5% dei rispondenti mostra valori di outfit che eccedono la soglia di 1.96 o che sono inferiori a -1.96. Invece, l’8% dei rispondenti mostra valori di infit che eccedono la soglia di 1.96 o che sono inferiori a -1.96. Questi risultati suggeriscono che il modello di Rasch non è del tutto coerente con i dati esaminati.\n\n\n61.5.8 Grafici Specifici IRT\nOltre alla valutazione complessiva dell’adattamento del modello, ci possiamo chiedere quanto bene gli item coprono l’intervallo dell’abilità latente. A tale domanda si può rispondere creando una “Mappa Item-Persona” (noto anche come Wright Map). Questa visualizzazione inizia tracciando la distribuzione dell’abilità latente nel campione studiato. Successivamente, tracciamo anche la difficoltà di ciascun item sulla stessa scala di theta. Allineando entrambi i grafici, possiamo vedere quanto bene gli item coprono l’abilità latente.\n\nitempersonMap(mirt_rm)\n\nWarning message:\n\"Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\"\n\n\n\n\n\n\n\n\n\nNel caso presente vediamo che c’è un buon allineamento tra le abilità dei partecipanti e la difficoltà degli item.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/06_implementation.html#curva-di-informazione-dellitem",
    "href": "chapters/irt/06_implementation.html#curva-di-informazione-dellitem",
    "title": "61  Implementazione",
    "section": "61.6 Curva di Informazione dell’Item",
    "text": "61.6 Curva di Informazione dell’Item\nUn altro modo per valutare la qualità di ciascun item è tramite la creazione delle cosiddette curve di informazione degli item. L’informazione è un concetto statistico che si riferisce alla capacità di un item di stimare con precisione i punteggi su theta. L’informazione a livello di item chiarisce quanto bene ogni item contribuisca alla precisione nella stima dei punteggi, con livelli più elevati di informazione che portano a stime dei punteggi più accurate.\nPer esempio: - Un item con un’elevata informazione sarà molto utile per discriminare tra rispondenti con diversi livelli di abilità latente attorno a un certo punto della scala di theta. Questo significa che l’item fornisce dati affidabili e significativi sulla capacità o conoscenza che si sta misurando. - Al contrario, un item con bassa informazione non aggiunge molto alla precisione della stima del punteggio. Questo potrebbe accadere se l’item è troppo facile o troppo difficile per la maggior parte dei rispondenti, o se non è strettamente correlato al tratto latente che si sta cercando di misurare.\nLa posizione delle Curve Caratteristiche degli Item (ICC) determina le regioni sul tratto latente dove ciascun item fornisce il massimo di informazione. Questo viene illustrato tramite il grafico dell’informazione dell’item.\n\nplotINFO(rm_sum0, type = \"item\", legpos = FALSE)\n\n\n\n\n\n\n\n\nQui vediamo che alcuni item forniscono maggiori informazioni sui livelli più bassi di \\(\\theta\\), altri a livelli medi di \\(\\theta\\) e altri ancora ai livelli alti di \\(\\theta\\).\n\n61.6.1 Informazione del Test\nIl concetto di “informazione” può essere applicato anche all’intera scala del test. In questo caso, osserviamo che la scala è molto efficace nel stimare i punteggi di theta tra -2 e 3, ma presenta una minore precisione nella stima dei punteggi di theta agli estremi. In altre parole, il test fornisce stime accurate per una vasta gamma di abilità medie e leggermente superiori alla media, ma diventa meno affidabile per valutare abilità molto basse o molto elevate.\nQuesta osservazione ha importanti implicazioni pratiche:\n\nValutazione Ottimale per la Maggior Parte dei Rispondenti: La scala è particolarmente adatta per valutare rispondenti il cui livello di abilità si trova all’interno dell’intervallo in cui il test è più informativo (-2 a 4).\nLimiti nella Valutazione degli Estremi: Per rispondenti con abilità molto al di sotto di -2 o molto al di sopra di 4, il test potrebbe non fornire stime di abilità così precise. Questo significa che per questi individui, il test potrebbe non essere in grado di discriminare efficacemente tra diversi livelli di abilità.\n\nLe curve di informazione del test aiutano a identificare dove il test è più efficace e dove potrebbe aver bisogno di miglioramenti o aggiustamenti, come l’aggiunta di item più difficili o più facili per estendere la sua precisione ai livelli estremi di abilità. Questa analisi consente di ottimizzare il test per una valutazione più accurata su tutta la gamma di abilità latente che si intende misurare.\nIl grafico dell’informazione del test può essere generato utilizzando eRm::plotINFO:\n\neRm::plotINFO(rm_sum0, type = \"test\")\n\n\n\n\n\n\n\n\nOppure possiamo usare testInfoPlot():\n\ntestInfoPlot(mirt_rm, adj_factor = 2)\n\n\n\n\n\n\n\n\nCome abbiamo già osservato nella mappa persona-item, la maggior parte degli item si trova intorno allo zero. Di conseguenza, l’informazione del test nella figura è maggiore attorno allo zero. Si noti che, di conseguenza, gli errori standard aumentano allontanandosi dallo zero.\n\n\n61.6.2 Stima dei Parametri delle Persone\nLa stima dei parametri delle persone ottenuta con il metodo di massima verosimiglianza si ottiene nel modo seguente:\n\ntheta &lt;- eRm::person.parameter(rm_sum0)\ntheta\n\n\nPerson Parameters:\n\n Raw Score    Estimate Std.Error\n         1 -3.39916338 1.0848195\n         2 -2.52177653 0.8317915\n         3 -1.91597998 0.7365453\n         4 -1.40925895 0.6923920\n         5 -0.94517977 0.6731151\n         6 -0.49649946 0.6684716\n         7 -0.04741155 0.6729635\n         8  0.41178325 0.6830616\n         9  0.88780188 0.6976852\n        10  1.38917320 0.7203115\n        11  1.93510328 0.7620315\n        12  2.57656206 0.8512968\n        13  3.48323588 1.0967898\n        14  4.45486143        NA\n\n\nDa notare che questa tabella non mostra una stima per ogni persona. La stima dell’abilità di una persona dipende unicamente dal numero di item a cui ha risposto correttamente. Questo significa che dobbiamo calcolare una stima dell’abilità per ogni possibile punteggio totale (indicato come “punteggi grezzi” nella tabella) e possiamo assegnare tale stima a ciascuna persona che ottiene quel punteggio. Ad esempio, stimiamo che l’abilità di una persona che risponde correttamente a dieci item sia circa 1.39.\nVediamo che le stime dell’abilità aumentano con il punteggio grezzo. Questo ha senso, poiché un candidato ha maggiori probabilità di rispondere correttamente a un item se la sua abilità supera la difficoltà di quell’item. Più item vengono risposti correttamente, più è probabile che l’abilità del candidato sia elevata. Inoltre, vediamo che l’errore standard aumenta con la distanza da zero, come era prevedibile dalla mappa persona-item o dalle curve di informazione degli item e del test, dove abbiamo visto che la maggior parte degli item si trova intorno allo zero.\nLa mancanza di un errore standard per i candidati che rispondono correttamente a tutti i 14 item potrebbe lasciarci perplessi. La ragione di tale mancanza è che non esiste una stima di massima verosimiglianza per questo punteggio perfetto. Per gestire questo, la funzione person.parameter() utilizza un metodo chiamato interpolazione spline per produrre una stima dell’abilità, ma la procedura non fornisce stime dell’errore. Lo stesso sarebbe vero per i candidati che risolvono correttamente 0 item, ma in questo campione non si è verificato un punteggio di zero.\nPossiamo ottenere informazioni sulle stime dell’abilità dei singoli candidati utilizzando la funzione summary(), cioè,\n\nsummary(theta)\n\n\nEstimation of Ability Parameters\n\nCollapsed log-likelihood: -76.27866 \nNumber of iterations: 10 \nNumber of parameters: 13 \n\nML estimated ability parameters (without spline interpolated values): \n             Estimate Std. Err.       2.5 %      97.5 %\ntheta 1   -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 2   -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 3   -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 4   -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 5    0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 6   -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 7   -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 8   -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 9   -1.91597998 0.7365453 -3.35958233 -0.47237764\ntheta 10  -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 11  -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 12   0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 13  -1.91597998 0.7365453 -3.35958233 -0.47237764\ntheta 14  -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 15  -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 16  -2.52177653 0.8317915 -4.15205791 -0.89149514\ntheta 17  -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 18  -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 19  -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 20  -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 21  -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 22  -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 23  -1.91597998 0.7365453 -3.35958233 -0.47237764\ntheta 24  -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 25  -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 26  -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 27  -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 28   0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 29  -2.52177653 0.8317915 -4.15205791 -0.89149514\ntheta 30  -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 31  -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 32  -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 33  -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 34  -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 35  -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 36  -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 37  -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 38  -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 39  -1.91597998 0.7365453 -3.35958233 -0.47237764\ntheta 40  -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 41  -1.91597998 0.7365453 -3.35958233 -0.47237764\ntheta 42   0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 43   0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 44   0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 45  -1.91597998 0.7365453 -3.35958233 -0.47237764\ntheta 46  -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 47   1.38917320 0.7203115 -0.02261148  2.80095788\ntheta 48   1.38917320 0.7203115 -0.02261148  2.80095788\ntheta 49  -2.52177653 0.8317915 -4.15205791 -0.89149514\ntheta 50  -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 51  -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 52  -3.39916338 1.0848195 -5.52537044 -1.27295631\ntheta 53   1.93510328 0.7620315  0.44154905  3.42865751\ntheta 54  -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 55  -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 56  -2.52177653 0.8317915 -4.15205791 -0.89149514\ntheta 57  -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 58   0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 59  -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 60   0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 61   0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 62   0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 63   0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 64   1.38917320 0.7203115 -0.02261148  2.80095788\ntheta 65   0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 66   1.38917320 0.7203115 -0.02261148  2.80095788\ntheta 67  -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 68  -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 69  -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 70  -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 71   0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 72  -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 73   0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 74  -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 75   1.38917320 0.7203115 -0.02261148  2.80095788\ntheta 76  -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 77  -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 78   0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 79  -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 80   0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 81   0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 82   1.38917320 0.7203115 -0.02261148  2.80095788\ntheta 83   0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 84   0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 85   0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 86  -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 87   1.38917320 0.7203115 -0.02261148  2.80095788\ntheta 88   0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 89  -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 90   0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 91  -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 92   0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 93  -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 94  -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 95  -1.91597998 0.7365453 -3.35958233 -0.47237764\ntheta 96  -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 97  -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 98  -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 99  -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 100 -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 101  0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 102 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 103  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 104  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 105 -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 106 -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 107 -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 108  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 109  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 110 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 111 -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 112  0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 113 -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 114 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 115 -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 116 -2.52177653 0.8317915 -4.15205791 -0.89149514\ntheta 117 -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 118 -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 119  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 120 -1.91597998 0.7365453 -3.35958233 -0.47237764\ntheta 121 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 122 -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 123  1.93510328 0.7620315  0.44154905  3.42865751\ntheta 124 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 125 -1.91597998 0.7365453 -3.35958233 -0.47237764\ntheta 126 -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 127 -2.52177653 0.8317915 -4.15205791 -0.89149514\ntheta 128 -1.91597998 0.7365453 -3.35958233 -0.47237764\ntheta 129 -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 130 -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 131 -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 132 -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 133 -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 134 -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 135 -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 136  0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 137 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 138  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 139  1.38917320 0.7203115 -0.02261148  2.80095788\ntheta 140  0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 141  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 142 -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 143  0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 144 -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 145  1.93510328 0.7620315  0.44154905  3.42865751\ntheta 146 -1.91597998 0.7365453 -3.35958233 -0.47237764\ntheta 147 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 148 -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 149 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 150 -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 151 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 152 -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 153 -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 154  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 155 -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 156 -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 157 -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 158 -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 159  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 160 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 161 -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 162  0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 163 -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 164 -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 165 -1.91597998 0.7365453 -3.35958233 -0.47237764\ntheta 166 -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 167 -3.39916338 1.0848195 -5.52537044 -1.27295631\ntheta 168 -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 169 -3.39916338 1.0848195 -5.52537044 -1.27295631\ntheta 170 -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 171 -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 172  1.38917320 0.7203115 -0.02261148  2.80095788\ntheta 173 -1.91597998 0.7365453 -3.35958233 -0.47237764\ntheta 174 -1.91597998 0.7365453 -3.35958233 -0.47237764\ntheta 175  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 176 -1.91597998 0.7365453 -3.35958233 -0.47237764\ntheta 177  1.38917320 0.7203115 -0.02261148  2.80095788\ntheta 178 -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 179 -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 180 -1.91597998 0.7365453 -3.35958233 -0.47237764\ntheta 181 -3.39916338 1.0848195 -5.52537044 -1.27295631\ntheta 182 -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 183 -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 184  0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 185 -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 186  1.38917320 0.7203115 -0.02261148  2.80095788\ntheta 187  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 188 -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 189 -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 190 -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 191 -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 192 -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 193 -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 194 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 195  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 196 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 197 -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 198  1.93510328 0.7620315  0.44154905  3.42865751\ntheta 199  1.38917320 0.7203115 -0.02261148  2.80095788\ntheta 200  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 201  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 202 -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 203 -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 204 -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 205  0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 206  1.93510328 0.7620315  0.44154905  3.42865751\ntheta 207  1.93510328 0.7620315  0.44154905  3.42865751\ntheta 208  0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 209  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 210  1.38917320 0.7203115 -0.02261148  2.80095788\ntheta 211 -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 212 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 213 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 214  0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 215  1.93510328 0.7620315  0.44154905  3.42865751\ntheta 216  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 217  2.57656206 0.8512968  0.90805108  4.24507304\ntheta 218  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 219  1.93510328 0.7620315  0.44154905  3.42865751\ntheta 220 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 221 -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 222 -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 223 -1.91597998 0.7365453 -3.35958233 -0.47237764\ntheta 224  1.38917320 0.7203115 -0.02261148  2.80095788\ntheta 225  0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 226 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 227  0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 228  1.38917320 0.7203115 -0.02261148  2.80095788\ntheta 229 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 230  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 231  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 232 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 233 -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 234  0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 235 -1.91597998 0.7365453 -3.35958233 -0.47237764\ntheta 236  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 237  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 238  3.48323588 1.0967898  1.33356744  5.63290432\ntheta 239 -3.39916338 1.0848195 -5.52537044 -1.27295631\ntheta 240 -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 241  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 242 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 243 -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 244  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 245 -1.91597998 0.7365453 -3.35958233 -0.47237764\ntheta 246  1.38917320 0.7203115 -0.02261148  2.80095788\ntheta 247 -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 248 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 249  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 250 -1.91597998 0.7365453 -3.35958233 -0.47237764\ntheta 251 -2.52177653 0.8317915 -4.15205791 -0.89149514\ntheta 252  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 253 -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 254 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 255 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 256  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 257  0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 258 -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 259 -2.52177653 0.8317915 -4.15205791 -0.89149514\ntheta 260 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 261  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 262  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 263  3.48323588 1.0967898  1.33356744  5.63290432\ntheta 264  0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 265  1.38917320 0.7203115 -0.02261148  2.80095788\ntheta 266 -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 267  1.38917320 0.7203115 -0.02261148  2.80095788\ntheta 268 -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 269 -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 270 -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 271  1.38917320 0.7203115 -0.02261148  2.80095788\ntheta 272  0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 273 -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 274 -1.91597998 0.7365453 -3.35958233 -0.47237764\ntheta 275  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 276 -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 277 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 278  1.93510328 0.7620315  0.44154905  3.42865751\ntheta 279  0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 280  1.38917320 0.7203115 -0.02261148  2.80095788\ntheta 281  1.38917320 0.7203115 -0.02261148  2.80095788\ntheta 282  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 283 -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 284 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 285 -1.91597998 0.7365453 -3.35958233 -0.47237764\ntheta 286  1.93510328 0.7620315  0.44154905  3.42865751\ntheta 287  0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 288  1.38917320 0.7203115 -0.02261148  2.80095788\ntheta 289  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 290  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 291 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 292  0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 293 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 294 -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 296  1.38917320 0.7203115 -0.02261148  2.80095788\ntheta 297  1.38917320 0.7203115 -0.02261148  2.80095788\ntheta 298  3.48323588 1.0967898  1.33356744  5.63290432\ntheta 299  0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 300  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 301 -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 302 -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 303  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 304 -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 305 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 306  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 308  1.38917320 0.7203115 -0.02261148  2.80095788\ntheta 309  1.38917320 0.7203115 -0.02261148  2.80095788\ntheta 310 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 311 -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 312  0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 313  1.38917320 0.7203115 -0.02261148  2.80095788\ntheta 314  0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 315  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 316 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 317 -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 318  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 319  2.57656206 0.8512968  0.90805108  4.24507304\ntheta 320 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 321 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 322  0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 323  0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 324  2.57656206 0.8512968  0.90805108  4.24507304\ntheta 325  1.38917320 0.7203115 -0.02261148  2.80095788\ntheta 326  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 327  0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 328  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 329  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 330 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 331  0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 332 -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 333  0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 334 -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 335  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 336 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 337  1.38917320 0.7203115 -0.02261148  2.80095788\ntheta 338 -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 339  1.38917320 0.7203115 -0.02261148  2.80095788\ntheta 340  0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 341  0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 342  1.93510328 0.7620315  0.44154905  3.42865751\ntheta 343  0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 344  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 345  1.38917320 0.7203115 -0.02261148  2.80095788\ntheta 346 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 347  1.93510328 0.7620315  0.44154905  3.42865751\ntheta 348  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 349  1.38917320 0.7203115 -0.02261148  2.80095788\ntheta 350  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 351  0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 352 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 353 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 354 -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 355  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 356 -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 357 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 358 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 359 -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 360 -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 361 -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 362  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 363 -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 364 -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 365 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 366  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 367 -3.39916338 1.0848195 -5.52537044 -1.27295631\ntheta 368 -3.39916338 1.0848195 -5.52537044 -1.27295631\ntheta 369 -2.52177653 0.8317915 -4.15205791 -0.89149514\ntheta 370 -1.91597998 0.7365453 -3.35958233 -0.47237764\ntheta 371  0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 372 -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 373 -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 374 -1.91597998 0.7365453 -3.35958233 -0.47237764\ntheta 375 -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 376 -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 377 -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 378 -2.52177653 0.8317915 -4.15205791 -0.89149514\ntheta 379 -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 380 -1.91597998 0.7365453 -3.35958233 -0.47237764\ntheta 381 -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 382 -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 383 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 384 -1.91597998 0.7365453 -3.35958233 -0.47237764\ntheta 385 -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 386 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 387  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 388 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 389  0.88780188 0.6976852 -0.47963602  2.25523978\ntheta 390 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 391 -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 392  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 393 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 394  0.41178325 0.6830616 -0.92699284  1.75055934\ntheta 395 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 396 -1.40925895 0.6923920 -2.76632227 -0.05219563\ntheta 397 -0.49649946 0.6684716 -1.80667980  0.81368089\ntheta 398 -0.04741155 0.6729635 -1.36639576  1.27157265\ntheta 399 -0.94517977 0.6731151 -2.26446118  0.37410164\ntheta 400 -1.91597998 0.7365453 -3.35958233 -0.47237764\n\n\nL’output – che qui abbiamo di nuovo troncato per risparmiare spazio – contiene stime ed errori standard, insieme ai limiti inferiori (2.5%) e superiori (97.5%) degli intervalli di confidenza al 95%, per tutti i candidati che hanno risposto correttamente fino a 13 item. I candidati che hanno risposto correttamente a tutti i 14 item (o a nessun item) sono omessi da questo output. Possiamo anche notare che alcuni candidati, ad esempio il secondo e il terzo, ricevono esattamente la stessa stima di abilità ed errore standard. Questo è dovuto al fatto che hanno lavorato sullo stesso set di item e hanno ottenuto lo stesso punteggio totale. In alternativa, possiamo utilizzare il comando coef(theta) per ottenere solo la stima dell’abilità per ciascun candidato.\nPossiamo anche stimare l’abilità dei candidati utilizzando la funzione mirt::fscores(). Per i modelli unidimensionali, gli argomenti più importanti di fscores() sono object e method. L’argomento object accetta il risultato della funzione mirt(). L’argomento method indica quale metodo utilizzare per stimare i parametri della persona. Per impostazione predefinita, method=\"EAP\", il che indica che il parametro della persona dovrebbe essere stimato utilizzando il metodo expected a posteriori (EAP). Possiamo calcolare le stime EAP per il modello di Rasch e stampare le sue prime sei voci inserendo:\n\ntheta_eap &lt;- fscores(mirt_rm)\nhead(theta_eap)\n\n\nA matrix: 6 x 1 of type dbl\n\n\nF1\n\n\n\n\n-0.2177416\n\n\n-0.8276435\n\n\n-0.8276435\n\n\n-0.2177416\n\n\n0.3913603\n\n\n-0.5213117\n\n\n\n\n\nPer impostazione predefinita mirt mostra solo le stime puntuali, ma è possibile aggiungere gli errori standard tramite l’opzione full.scores.SE = TRUE alla funzione fscores(). Gli errori standard dovrebbero essere esaminati prima di interpretare o riportare le stime dei parametri della persona.\nLa funzione fscores() fornisce anche stimatori di massima verosimiglianza (ML), massimo a posteriori (MAP) e likelihood ponderata (WLE). Ora confrontiamo i quattro tipi di stime dei parametri della persona fornite da mirt. Gli stimatori ML, MAP e WLE possono essere calcolati inserendo\n\ntheta_ml &lt;- fscores(mirt_rm, method = \"ML\", max_theta = 30)\ntheta_map &lt;- fscores(mirt_rm, method = \"MAP\")\ntheta_wle &lt;- fscores(mirt_rm, method = \"WLE\")\n\n\nests &lt;- cbind(theta_eap, theta_ml, theta_map, theta_wle)\ncolnames(ests) &lt;- c(\"EAP\", \"ML\", \"MAP\", \"WLE\")\npairs(ests, xlim = c(-3, 3), ylim = c(-3, 3))\n\n\n\n\n\n\n\n\n\n\n61.6.3 Affidabilità Condizionale\nIl concetto di affidabilità varia tra la CTT e la IRT. Nell’IRT, possiamo calcolare l’affidabilità condizionale, ossia l’affidabilità della scala a diversi livelli di theta.\n\nNella CTT, l’affidabilità è solitamente considerata come una proprietà fissa del test, indipendentemente dal livello di abilità dei rispondenti. Si misura spesso attraverso il coefficiente alfa di Cronbach o metodi simili.\nNell’IRT, invece, l’affidabilità è vista come una proprietà variabile che dipende dal livello di theta del rispondente. A diversi livelli di theta, la precisione con cui il test misura l’abilità può variare significativamente.\n\nL’affidabilità condizionale fornisce una misura più specifica e dettagliata di quanto affidabilmente un test misura l’abilità a diversi livelli di \\(\\theta\\).\n\nconRelPlot(mirt_rm)\n\n\n\n\n\n\n\n\nNel caso presente, - a livelli medi di \\(\\theta\\): Il test mostra una buona affidabilità, indicando che è in grado di distinguere con precisione tra rispondenti con abilità medie. - agli estremi di \\(\\theta\\): Il test mostra un’affidabilità più bassa, suggerendo che non è altrettanto efficace nel distinguere tra livelli di abilità molto alti o molto bassi.\nIn sostanza, l’affidabilità condizionale nell’IRT ci fornisce una comprensione più dettagliata di dove il test funziona bene e dove potrebbe richiedere miglioramenti per valutare con precisione l’abilità su tutta la gamma di theta.\nÈ comunque possibile calcolare un singolo valore di attendibilità:\n\nmarginal_rxx(mirt_rm)\n\n0.67652957591305\n\n\n\n\n61.6.4 Curve Caratteristiche del Test\nUna proprietà aggiuntiva di un modello IRT è che il punteggio complessivo delle risposte corrette (la somma dei punteggi per le risposte corrette) risulta essere una stima efficace del tratto latente sottostante. Un grafico delle cosiddette curve caratteristiche della scala permette di valutare visivamente questo aspetto tracciando la relazione tra theta e il punteggio di risposte corrette.\n\nQuesto tipo di grafico mostra come il punteggio totale delle risposte corrette si correla con il livello di abilità latente (theta) stimato dal modello IRT.\nAd esempio, se la curva mostra che punteggi più alti di risposte corrette corrispondono sistematicamente a livelli più alti di theta e viceversa, ciò indica che il punteggio totale è un buon indicatore del tratto latente.\nAl contrario, se la curva non mostra una relazione chiara o lineare tra punteggio totale e theta, ciò potrebbe suggerire che il punteggio totale non cattura completamente la complessità o le sfumature del tratto latente.\n\nIn sintesi, le curve caratteristiche della scala forniscono una rappresentazione visiva di come il punteggio totale di risposte corrette rifletta l’abilità latente misurata dal test, offrendo una visione utile per valutare l’efficacia del punteggio totale come indicatore del tratto latente in questione.\n\nscaleCharPlot(mirt_rm)\n\n\n\n\n\n\n\n\nQuesta curva di solito assume la forma di una S, poiché la relazione è più forte nel range medio di theta e meno precisa agli estremi (come già visto nella curva di informazione del test).\nPossiamo ovviamente testare anche questo con una semplice correlazione. Per prima cosa, estraiamo il punteggio latente IRT utilizzando la funzione fscores(). Quindi lo correliamo con il punteggio di risposte corrette.\n\nscore &lt;- fscores(mirt_rm)\nsumscore &lt;- rowSums(responses)\ncor.test(score, sumscore)\n\n\n    Pearson's product-moment correlation\n\ndata:  score and sumscore\nt = 1097.1, df = 398, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9997988 0.9998642\nsample estimates:\n      cor \n0.9998347 \n\n\nNel caso presente, la correlazione è quasi perfetta.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/06_implementation.html#considerazioni-conclusive",
    "href": "chapters/irt/06_implementation.html#considerazioni-conclusive",
    "title": "61  Implementazione",
    "section": "61.7 Considerazioni Conclusive",
    "text": "61.7 Considerazioni Conclusive\nTradizionalmente, il punteggio totale ottenuto in un test psicologico è stato considerato come la misura più efficace dell’abilità o della predisposizione di una persona rispetto a un certo tratto di personalità. Tuttavia, la dipendenza del punteggio totale dalla difficoltà degli item presenta limitazioni significative. Ad esempio, due persone possono ottenere lo stesso punteggio totale rispondendo in modo diverso a item di varia difficoltà, il che non riflette accuratamente le loro abilità reali.\nNella Teoria Classica dei Test (CTT), l’enfasi è posta sul punteggio totale, ma questa prospettiva ignora le variazioni nella difficoltà degli item e assume che gli errori di misurazione si annullino reciprocamente attraverso la procedura di sommazione. Tuttavia, la CTT è limitata dalla sua assunzione di varianze di errore uniformi per tutti i rispondenti, dall’aspettativa di errori di misurazione nulli e dalla focalizzazione esclusiva sui punteggi totali, senza considerare l’adattamento di item e persone.\nAl contrario, la Teoria della Risposta all’Item (IRT) cambia il focus dai punteggi totali alle risposte a ciascun item, sfruttando le caratteristiche degli item. L’IRT descrive come attributi come abilità, atteggiamento o personalità, insieme alle caratteristiche degli item, influenzino la probabilità di fornire una risposta. Il Modello di Rasch, una forma semplice di IRT per risposte binarie, stabilisce una relazione diretta tra la probabilità di una risposta corretta e il livello di abilità del rispondente.\nLa stima dell’abilità in IRT non dipende dagli specifici item somministrati, permettendo di confrontare i risultati tra gruppi diversi con lo stesso set di item. Inoltre, la qualità degli item è valutata indipendentemente dal campione di rispondenti, rendendo le proprietà degli item costanti tra diversi gruppi con varie abilità.\nL’IRT supera i limiti della CTT stimando congiuntamente le proprietà degli item e il livello di abilità dei rispondenti. Le caratteristiche degli item diventano indipendenti dal campione di individui utilizzato per costruire il test, permettendo la creazione di insiemi di item equivalenti per misurare abilità latenti. Questo approccio offre maggiore precisione e affidabilità nelle misurazioni, assicurando la comparabilità tra diversi gruppi di individui. In conclusione, l’IRT rappresenta un metodo statistico avanzato e versatile per una valutazione più accurata e affidabile di tratti e abilità in contesti psicometrici.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/06_implementation.html#session-info",
    "href": "chapters/irt/06_implementation.html#session-info",
    "title": "61  Implementazione",
    "section": "61.8 Session Info",
    "text": "61.8 Session Info\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] grid      stats4    stats     graphics  grDevices utils     datasets \n[8] methods   base     \n\nother attached packages:\n [1] psychotools_0.7-4 ggmirt_0.1.0      TAM_4.2-21        CDM_8.2-6        \n [5] mvtnorm_1.3-1     mirt_1.42         lattice_0.22-6    eRm_1.0-6        \n [9] ggokabeito_0.1.0  viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n[13] ggExtra_0.10.1    bayesplot_1.11.1  gridExtra_2.3     patchwork_1.3.0  \n[17] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-18     psych_2.4.6.26   \n[21] scales_1.3.0      markdown_1.13     knitr_1.48        lubridate_1.9.3  \n[25] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[29] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1    \n[33] tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] splines_4.4.1        later_1.3.2          pbdZMQ_0.3-13       \n  [4] R.oo_1.26.0          XML_3.99-0.17        rpart_4.1.23        \n  [7] lifecycle_1.0.4      rstatix_0.7.2        rprojroot_2.0.4     \n [10] globals_0.16.3       MASS_7.3-61          rockchalk_1.8.157   \n [13] backports_1.5.0      magrittr_2.0.3       openxlsx_4.2.7.1    \n [16] Hmisc_5.1-3          rmarkdown_2.28       httpuv_1.6.15       \n [19] qgraph_1.9.8         zip_2.3.1            sessioninfo_1.2.2   \n [22] cowplot_1.1.3        pbapply_1.7-2        minqa_1.2.8         \n [25] multcomp_1.4-26      abind_1.4-8          audio_0.1-11        \n [28] quadprog_1.5-8       R.utils_2.12.3       nnet_7.3-19         \n [31] TH.data_1.1-2        sandwich_3.1-1       listenv_0.9.1       \n [34] testthat_3.2.1.1     RPushbullet_0.3.4    vegan_2.6-8         \n [37] arm_1.14-4           parallelly_1.38.0    permute_0.9-7       \n [40] codetools_0.2-20     tidyselect_1.2.1     farver_2.1.2        \n [43] lme4_1.1-35.5        base64enc_0.1-3      jsonlite_1.8.9      \n [46] polycor_0.8-1        progressr_0.14.0     Formula_1.2-5       \n [49] survival_3.7-0       emmeans_1.10.4       tools_4.4.1         \n [52] snow_0.4-4           Rcpp_1.0.13          glue_1.7.0          \n [55] mnormt_2.1.1         admisc_0.36          xfun_0.47           \n [58] mgcv_1.9-1           IRdisplay_1.1        withr_3.0.1         \n [61] beepr_2.0            fastmap_1.2.0        boot_1.3-31         \n [64] fansi_1.0.6          digest_0.6.37        mi_1.1              \n [67] timechange_0.3.0     R6_2.5.1             mime_0.12           \n [70] estimability_1.5.1   colorspace_2.1-1     gtools_3.9.5        \n [73] jpeg_0.1-10          R.methodsS3_1.8.2    utf8_1.2.4          \n [76] generics_0.1.3       data.table_1.16.0    corpcor_1.6.10      \n [79] SimDesign_2.17.1     htmlwidgets_1.6.4    pkgconfig_2.0.3     \n [82] sem_3.1-16           gtable_0.3.5         brio_1.1.5          \n [85] htmltools_0.5.8.1    carData_3.0-5        png_0.1-8           \n [88] rstudioapi_0.16.0    tzdb_0.4.0           reshape2_1.4.4      \n [91] uuid_1.2-1           coda_0.19-4.1        checkmate_2.3.2     \n [94] nlme_3.1-166         curl_5.2.3           nloptr_2.1.1        \n [97] repr_1.1.7           zoo_1.8-12           parallel_4.4.1      \n[100] miniUI_0.1.1.1       foreign_0.8-87       pillar_1.9.0        \n[103] vctrs_0.6.5          promises_1.3.0       car_3.1-2           \n[106] OpenMx_2.21.12       xtable_1.8-4         Deriv_4.1.6         \n[109] cluster_2.1.6        dcurver_0.9.2        GPArotation_2024.3-1\n[112] htmlTable_2.4.3      evaluate_1.0.0       pbivnorm_0.6.0      \n[115] cli_3.6.3            kutils_1.73          compiler_4.4.1      \n[118] rlang_1.1.4          crayon_1.5.3         future.apply_1.11.2 \n[121] ggsignif_0.6.4       labeling_0.4.3       fdrtool_1.2.18      \n[124] plyr_1.8.9           stringi_1.8.4        munsell_0.5.1       \n[127] lisrelToR_0.3        pacman_0.5.1         Matrix_1.7-0        \n[130] IRkernel_1.3.2       hms_1.1.3            glasso_1.11         \n[133] future_1.34.0        shiny_1.9.1          igraph_2.0.3        \n[136] broom_1.0.6          RcppParallel_5.1.9  \n\n\n\n\n\n\nDebelak, Rudolf, Carolin Strobl, e Matthew D Zeigenfuse. 2022. An introduction to the rasch model with examples in r. Crc Press.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/07_irt_poly.html",
    "href": "chapters/irt/07_irt_poly.html",
    "title": "61  Modelli per Risposte Politomiche",
    "section": "",
    "text": "61.1 Introduzione\nIn questo capitolo, esamineremo i modelli per risposte politomiche. La nostra esplorazione inizia con un passo indietro, riconoscendo l’origine e l’evoluzione di questi modelli dal loro antenato più semplice: il modello dicotomico. Finora ci siamo affidati a quest’ultimo per interpretare risposte che cadevano in una dicotomia di ‘corretto’ o ‘errato’, ‘accordo’ o ‘disaccordo’. Il passaggio ai modelli politomici ci permette di considerare risposte che vanno oltre il semplice sì o no. In un test di matematica, ad esempio, possiamo ora riconoscere e attribuire valore a risposte parzialmente corrette. In un questionario sulla personalità, siamo in grado di misurare diversi gradi di accordo o intensità di un tratto, piuttosto che limitarci a una risposta binaria.\nQuesti modelli politomici si presentano come estensioni naturali del modello Rasch. Nel corso di questo capitolo, esploreremo in dettaglio tre modelli politomici chiave: il modello a Crediti Parziali, il modello a Scala di Valutazione e il modello di Risposta Graduata.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Modelli per Risposte Politomiche</span>"
    ]
  },
  {
    "objectID": "chapters/irt/07_irt_poly.html#modello-a-crediti-parziali",
    "href": "chapters/irt/07_irt_poly.html#modello-a-crediti-parziali",
    "title": "61  Modelli per Risposte Politomiche",
    "section": "61.2 Modello a Crediti Parziali",
    "text": "61.2 Modello a Crediti Parziali\nI modelli discussi fino ad ora sono adatti per dati di risposta dicotomica. Questo tipo di dati può essere ottenuto sia direttamente utilizzando un elemento con due opzioni (ad esempio, il formato dell’elemento vero/falso usato con il MMPI) sia indirettamente codificando la risposta di un individuo come 0 o 1 attraverso un paradigma di valutazione della risposta dell’individuo. La risposta 0 riflette l’assenza di una caratteristica o, nel caso di test di competenza, una risposta errata. Al contrario, la risposta 1 rappresenta la presenza della caratteristica, l’approvazione del rispondente o, nel caso di test di competenza, una risposta corretta. Tuttavia, in alcune situazioni, i nostri dati possono avere più di due categorie di risposta. Per esempio, i giudici possono valutare la performance di una persona su una scala di valutazione, mentre una scala Likert permette gradi variabili di consenso a un’affermazione. In questi casi, i nostri dati sono politomici (ossia, con più di due categorie di risposta) e intrinsecamente ordinati.\nIl Modello a Crediti Parziali (PCM) è un’estensione del modello di Rasch per gli item che presentano più di due possibili risposte. Nel Modello a Crediti Parziali, consideriamo che ogni risposta su un item con più categorie possa essere vista come una serie di scelte tra categorie adiacenti. Questo modello non calcola direttamente la probabilità di scegliere una specifica categoria di risposta in un unico passaggio, ma piuttosto considera le transizioni tra le categorie.\nPer ogni transizione da una categoria a quella immediatamente superiore, il modello stima una probabilità. Per esempio, se un item ha tre categorie di risposta (“in disaccordo”=0, “neutrale”=1, “d’accordo”=2), il modello valuterà separatamente la probabilità di passare da 0 a 1 e da 1 a 2.\nPer un item specifico con risposte multiple ordinate, la probabilità che un individuo con un certo livello di abilità $ _p $ scelga la categoria $ x $ piuttosto che la categoria $ x-1 $ è data dalla seguente formula:\n\\[\n\\frac{P_{pix}}{P_{pi(x-1)}} = \\frac{\\exp(\\theta_p - \\delta_{ix})}{1 + \\exp(\\theta_p - \\delta_{ix})},\n\\]\ndove: - $ P_{pix} $ è la probabilità condizionale di scegliere la categoria $ x $ avendo già superato la categoria $ x-1 $, - $ p $ rappresenta il livello di abilità del rispondente, - $ {ix} $ è il parametro di difficoltà associato al passaggio dalla categoria $ x-1 $ alla categoria $ x $.\nQuesta formula, che assomiglia alla funzione logistica usata in molti modelli statistici, modella la probabilità di ‘successo’ (ossia di passaggio alla categoria superiore) in funzione della differenza tra l’abilità del rispondente e la difficoltà di quel passaggio specifico.\nUna volta che abbiamo le probabilità condizionali per ogni transizione tra categorie, possiamo calcolare la probabilità totale che un rispondente scelga una specifica categoria. Questo si fa moltiplicando le probabilità condizionali di tutte le transizioni necessarie per arrivare a quella categoria partendo dalla categoria più bassa:\n\\[\nP_{pij} = \\prod_{k=1}^{j} \\frac{P_{pik}}{P_{pi(k-1)}},\n\\]\ndove $ j $ è il numero della categoria desiderata. È importante notare che questa catena di moltiplicazioni implica l’uso delle probabilità condizionali calcolate precedentemente.\nNella pratica, il PCM offre un modo più flessibile e informativo per analizzare le risposte dei partecipanti. Il Modello a Crediti Parziali ci permette quindi non solo di vedere se una persona sa fare qualcosa, ma quanto bene sa farlo, permettendo un’analisi più dettagliata e una migliore comprensione delle capacità e delle opinioni.\n\n61.2.1 Curva Caratteristica della Categoria\nIl concetto delle Curve Caratteristiche della Categoria (CCC) nel Modello a Crediti Parziali (PCM) è fondamentale per visualizzare e comprendere come le probabilità di selezione delle diverse categorie di risposta cambiano al variare dell’abilità del rispondente. Ogni categoria di risposta ha associata una curva specifica che descrive la probabilità di essere scelta in funzione dell’abilità del rispondente. Ad esempio, per un item con quattro categorie di risposta, si disegnano quattro CCC.\n\n\n\n\n\n\nFigura 61.1: Curve Caratteristiche delle Categorie per un item con parametri di soglia $ {i1}, {i2}, _{i3} $. I numeri in grassetto indicano la categoria con la più alta probabilità di risposta. (Figura tratta da Debelak, Strobl, e Zeigenfuse (2022)).\n\n\n\nIn un modello di risposta a item con più categorie, come il PCM, ogni categoria ha una propria curva caratteristica che mostra la probabilità di essere selezionata all’aumentare o diminuire dell’abilità del rispondente. Per esempio, in un item con quattro categorie di risposta, si avranno quattro curve diverse, ognuna rappresentante una delle categorie dal livello più basso al più alto.\nQueste curve sono essenziali per comprendere in modo grafico e intuitivo come le probabilità di scegliere specifiche categorie si modificano al variare dell’abilità del rispondente. Le curve per le categorie intermedie tendono ad avere una forma a campana, indicando che c’è un certo range di abilità dove queste categorie sono più probabili, mentre le curve per le categorie estreme (la più bassa e la più alta) mostrano un andamento crescente o decrescente rispettivamente.\nLe soglie di transizione, indicate nei modelli come \\(\\delta_{i1}, \\delta_{i2}, \\delta_{i3}\\) e così via, sono punti critici lungo l’asse dell’abilità dove la probabilità di scegliere due categorie adiacenti è la stessa. Questi punti di soglia rappresentano graficamente il momento in cui un rispondente con un certo livello di abilità passa dall’essere più probabile a scegliere una categoria inferiore a quella superiore, o viceversa.\nLa rappresentazione grafica di queste soglie nelle CCC è cruciale perché permette ai ricercatori di identificare le abilità specifiche necessarie per passare da una risposta all’altra. In questo modello, i parametri di soglia, come \\(\\delta_{i1}\\) e \\(\\delta_{i2}\\), giocano un ruolo simile alla difficoltà dell’item nel modello di Rasch, determinando l’abilità alla quale la probabilità di scegliere una categoria è la stessa di quella di sceglierne un’altra. Una caratteristica importante del PCM e delle sue CCC è che la somma delle probabilità di tutte le categorie di risposta per un dato livello di abilità è sempre pari a uno. Questo assicura che il modello sia probabilisticamente valido e che tutte le possibilità di risposta siano adeguatamente rappresentate. In pratica, questo significa che per qualsiasi valore di abilità, la distribuzione delle probabilità tra le diverse categorie è completa e esaustiva.\nLe CCC sono strumenti visivi potenti per analizzare e interpretare come vari gruppi di individui rispondono agli item di un test. Per esempio, in contesti educativi, le CCC possono aiutare a identificare se alcuni item sono troppo facili o troppo difficili per certi gruppi di studenti, o se ci sono punti di ambiguità nelle domande che portano a una selezione inaspettata delle categorie di risposta.\nIn conclusione, le Curve Caratteristiche della Categoria nel PCM non solo arricchiscono la comprensione dell’interazione tra abilità del rispondente e la struttura delle risposte, ma offrono anche una base per migliorare la qualità e la precisione degli strumenti di valutazione, garantendo che ogni categoria di risposta sia adeguatamente calibrata in relazione alle abilità misurate.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Modelli per Risposte Politomiche</span>"
    ]
  },
  {
    "objectID": "chapters/irt/07_irt_poly.html#una-applicazione-concreta",
    "href": "chapters/irt/07_irt_poly.html#una-applicazione-concreta",
    "title": "61  Modelli per Risposte Politomiche",
    "section": "61.3 Una Applicazione Concreta",
    "text": "61.3 Una Applicazione Concreta\nConsideriamo i dati forniti da Braun (1988). Il dataset originale include valutazioni di 32 studenti fatte da 12 valutatori su tre diverse composizioni scritte. Per questa analisi, ci concentreremo sui dati relativi alla prima composizione. Per semplificare l’interpretazione, le valutazioni delle composizioni nel dataset originale sono state ricodificate, passando da nove a tre categorie (0 = basso rendimento, 1 = rendimento medio; 2 = alto rendimento).\nNella nostra analisi, tratteremo i 12 valutatori come se fossero 12 “item” politomici che condividono la struttura di una scala di valutazione a tre categorie.\nI valutatori con una calibrazione di “difficoltà” elevata possono essere interpretati come severi: questi valutatori tendono ad assegnare punteggi bassi più frequentemente. Al contrario, valutatori con una calibrazione di “difficoltà” bassa possono essere interpretati come indulgenti: questi valutatori tendono ad assegnare punteggi alti più frequentemente.\n\nbraun_data &lt;- read_csv(\"../../data/braun_data.csv\")\nhead(braun_data)\n\nRows: 32 Columns: 13\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\ndbl (13): Student, rater_1, rater_2, rater_3, rater_4, rater_5, rater_6, rat...\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nA tibble: 6 x 13\n\n\nStudent\nrater_1\nrater_2\nrater_3\nrater_4\nrater_5\nrater_6\nrater_7\nrater_8\nrater_9\nrater_10\nrater_11\nrater_12\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n2\n2\n1\n2\n3\n2\n3\n3\n3\n2\n3\n3\n\n\n2\n2\n2\n2\n2\n2\n1\n2\n2\n2\n2\n1\n2\n\n\n3\n2\n1\n2\n2\n2\n2\n1\n2\n1\n2\n2\n2\n\n\n4\n1\n2\n2\n2\n1\n2\n1\n1\n1\n2\n2\n2\n\n\n5\n2\n2\n1\n2\n2\n3\n2\n2\n2\n2\n2\n2\n\n\n6\n2\n2\n2\n3\n2\n2\n3\n2\n1\n2\n2\n2\n\n\n\n\n\nEliminiamo la colonna con l’identificativo dei partecipanti:\n\nPC_data &lt;- braun_data[, -1]\n\nSottraiamo 1 dalle osservazioni così che la cagegoria inferiore abbia valore 0, come richiesto da eRm.\n\nPC_data_balanced &lt;- PC_data - 1\n\nAdattiamo ai dati il Partial Credit Model:\n\nPC_model &lt;- PCM(PC_data_balanced)\n\n\nfor (i in 1:12) {\n    rater_col_name &lt;- paste(\"rater\", i, sep = \"_\")\n    unique_sorted_values &lt;- sort(unique(PC_data_balanced[[rater_col_name]]))\n    print(unique_sorted_values)\n}\n\n[1] 0 1 2\n[1] 0 1\n[1] 0 1 2\n[1] 0 1 2\n[1] 0 1 2\n[1] 0 1 2\n[1] 0 1 2\n[1] 0 1 2\n[1] 0 1 2\n[1] 0 1 2\n[1] 0 1 2\n[1] 0 1 2\n\n\n\nsummary(PC_model)\n\n\nResults of PCM estimation: \n\nCall:  PCM(X = PC_data_balanced) \n\nConditional log-likelihood: -176.2042 \nNumber of iterations: 85 \nNumber of parameters: 22 \n\nItem (Category) Difficulty Parameters (eta): with 0.95 CI:\n            Estimate Std. Error lower CI upper CI\nrater_1.c2     3.431      1.089    1.296    5.565\nrater_2.c1    -1.037      0.479   -1.976   -0.099\nrater_3.c1    -0.193      0.436   -1.048    0.662\nrater_3.c2     3.394      0.854    1.720    5.069\nrater_4.c1    -3.621      1.007   -5.594   -1.647\nrater_4.c2    -1.933      1.032   -3.957    0.090\nrater_5.c1    -1.481      0.508   -2.476   -0.485\nrater_5.c2     2.235      0.878    0.514    3.957\nrater_6.c1    -1.002      0.471   -1.925   -0.079\nrater_6.c2     2.679      0.863    0.987    4.371\nrater_7.c1    -0.332      0.483   -1.279    0.616\nrater_7.c2     1.077      0.620   -0.138    2.292\nrater_8.c1    -0.193      0.436   -1.048    0.662\nrater_8.c2     3.394      0.854    1.720    5.069\nrater_9.c1    -0.463      0.465   -1.375    0.448\nrater_9.c2     1.689      0.660    0.395    2.984\nrater_10.c1   -2.422      0.641   -3.679   -1.165\nrater_10.c2    0.484      0.822   -1.126    2.095\nrater_11.c1   -1.741      0.538   -2.796   -0.686\nrater_11.c2    1.505      0.805   -0.073    3.084\nrater_12.c1   -3.691      1.001   -5.653   -1.730\nrater_12.c2   -0.761      1.088   -2.894    1.372\n\nItem Easiness Parameters (beta) with 0.95 CI:\n                 Estimate Std. Error lower CI upper CI\nbeta rater_1.c1     1.020      0.469    0.102    1.939\nbeta rater_1.c2    -3.431      1.089   -5.565   -1.296\nbeta rater_2.c1     1.037      0.479    0.099    1.976\nbeta rater_3.c1     0.193      0.436   -0.662    1.048\nbeta rater_3.c2    -3.394      0.854   -5.069   -1.720\nbeta rater_4.c1     3.621      1.007    1.647    5.594\nbeta rater_4.c2     1.933      1.032   -0.090    3.957\nbeta rater_5.c1     1.481      0.508    0.485    2.476\nbeta rater_5.c2    -2.235      0.878   -3.957   -0.514\nbeta rater_6.c1     1.002      0.471    0.079    1.925\nbeta rater_6.c2    -2.679      0.863   -4.371   -0.987\nbeta rater_7.c1     0.332      0.483   -0.616    1.279\nbeta rater_7.c2    -1.077      0.620   -2.292    0.138\nbeta rater_8.c1     0.193      0.436   -0.662    1.048\nbeta rater_8.c2    -3.394      0.854   -5.069   -1.720\nbeta rater_9.c1     0.463      0.465   -0.448    1.375\nbeta rater_9.c2    -1.689      0.660   -2.984   -0.395\nbeta rater_10.c1    2.422      0.641    1.165    3.679\nbeta rater_10.c2   -0.484      0.822   -2.095    1.126\nbeta rater_11.c1    1.741      0.538    0.686    2.796\nbeta rater_11.c2   -1.505      0.805   -3.084    0.073\nbeta rater_12.c1    3.691      1.001    1.730    5.653\nbeta rater_12.c2    0.761      1.088   -1.372    2.894\n\n\n\nEsaminiamo la Wright Map, ovvero un grafico bidimensionale che mostra sia le abilità dei rispondenti sia le difficoltà degli item su una scala comune. Questa mappa aiuta a comprendere come si rapportano le abilità dei rispondenti alle difficoltà degli item.\n\nplotPImap(PC_model)\n\n\n\n\n\n\n\n\nEsaminiamo le CCC per ciascun item.\n\nplotICC(PC_model, ask = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConsideriamo ora le stime delle posizioni e delle soglie degli item (o “rater”, valutatori, nel contesto di una valutazione soggettiva) all’interno del modello.\nNell’output, la colonna “Location” indica la posizione media o la difficoltà generale dell’item. Un valore più alto indica un item più difficile (o un valutatore più severo), mentre un valore più basso indica un item più facile (o un valutatore più indulgente).\n\nitem.estimates &lt;- thresholds(PC_model)\nitem.estimates\n\n\nDesign Matrix Block 1:\n         Location Threshold 1 Threshold 2\nrater_1   1.71533    -1.02044     4.45110\nrater_2  -1.03738    -1.03738          NA\nrater_3   1.69705    -0.19282     3.58692\nrater_4  -0.96654    -3.62052     1.68743\nrater_5   1.11751    -1.48079     3.71582\nrater_6   1.33961    -1.00214     3.68137\nrater_7   0.53860    -0.33161     1.40880\nrater_8   1.69705    -0.19282     3.58692\nrater_9   0.84473    -0.46316     2.15262\nrater_10  0.24219    -2.42170     2.90607\nrater_11  0.75273    -1.74141     3.24687\nrater_12 -0.38032    -3.69109     2.93045\n\n\nNell’output, le colonne “Threshold 1” e “Threshold 2” rappresentano le soglie tra le categorie di risposta per ciascun item. Queste soglie indicano i punti lungo la scala di abilità in cui un rispondente ha probabilità uguali di essere classificato nella categoria successiva rispetto alla categoria corrente. Ad esempio, la soglia tra le categorie 0 e 1, e tra le categorie 1 e 2, ecc.\n\nitem_difficulty &lt;- item.estimates[[\"threshtable\"]][[\"1\"]]\nitem_difficulty\n\n\nA matrix: 12 x 3 of type dbl\n\n\n\nLocation\nThreshold 1\nThreshold 2\n\n\n\n\nrater_1\n1.7153306\n-1.0204400\n4.451101\n\n\nrater_2\n-1.0373789\n-1.0373789\nNA\n\n\nrater_3\n1.6970521\n-0.1928160\n3.586920\n\n\nrater_4\n-0.9665425\n-3.6205191\n1.687434\n\n\nrater_5\n1.1175131\n-1.4807928\n3.715819\n\n\nrater_6\n1.3396144\n-1.0021416\n3.681370\n\n\nrater_7\n0.5385965\n-0.3316062\n1.408799\n\n\nrater_8\n1.6970521\n-0.1928160\n3.586920\n\n\nrater_9\n0.8447300\n-0.4631578\n2.152618\n\n\nrater_10\n0.2421855\n-2.4216996\n2.906071\n\n\nrater_11\n0.7527270\n-1.7414142\n3.246868\n\n\nrater_12\n-0.3803219\n-3.6910915\n2.930448\n\n\n\n\n\nConsideriamo ora gli errori standard associati alle stime delle soglie. Questi valori indicano la variabilità o l’incertezza nelle stime delle soglie per ogni item e categoria di risposta. Un errore standard più piccolo indica una stima più precisa, mentre un errore standard più grande suggerisce una maggiore incertezza nella stima.\n\nitem.se &lt;- item.estimates$se.thresh\nprint(item.se)\n\n thresh beta rater_1.c1  thresh beta rater_1.c2  thresh beta rater_2.c1 \n              0.4685144               1.0382662               0.4788183 \n thresh beta rater_3.c1  thresh beta rater_3.c2  thresh beta rater_4.c1 \n              0.4362711               0.7916286               1.0069324 \n thresh beta rater_4.c2  thresh beta rater_5.c1  thresh beta rater_5.c2 \n              0.4715851               0.5078346               0.7766767 \n thresh beta rater_6.c1  thresh beta rater_6.c2  thresh beta rater_7.c1 \n              0.4710432               0.7807332               0.4833196 \n thresh beta rater_7.c2  thresh beta rater_8.c1  thresh beta rater_8.c2 \n              0.5116545               0.4362703               0.7916276 \n thresh beta rater_9.c1  thresh beta rater_9.c2 thresh beta rater_10.c1 \n              0.4651360               0.5557134               0.6412767 \nthresh beta rater_10.c2 thresh beta rater_11.c1 thresh beta rater_11.c2 \n              0.6006855               0.5382443               0.6666473 \nthresh beta rater_12.c1 thresh beta rater_12.c2 \n              1.0007824               0.5970149 \n\n\nLe stime delle abilità dei partecipanti si ottengono nel modo seguente:\n\nperson.locations.estimate &lt;- person.parameter(PC_model)\nsummary(person.locations.estimate)\n\n\nEstimation of Ability Parameters\n\nCollapsed log-likelihood: -90.03962 \nNumber of iterations: 6 \nNumber of parameters: 13 \n\nML estimated ability parameters (without spline interpolated values): \n            Estimate Std. Err.       2.5 %     97.5 %\ntheta P1   2.9215618 0.6285530  1.68962065  4.1535030\ntheta P2   0.1159553 0.6183277 -1.09594469  1.3278553\ntheta P3  -0.2617525 0.6115679 -1.46040365  0.9368986\ntheta P4  -1.0101623 0.6169475 -2.21935720  0.1990326\ntheta P5   0.9042291 0.6366182 -0.34351956  2.1519777\ntheta P6   1.3127857 0.6407991  0.05684262  2.5687289\ntheta P7   1.3127857 0.6407991  0.05684262  2.5687289\ntheta P8   0.9042291 0.6366182 -0.34351956  2.1519777\ntheta P9  -0.6343993 0.6104489 -1.83085719  0.5620587\ntheta P10  2.9215618 0.6285530  1.68962065  4.1535030\ntheta P11  0.1159553 0.6183277 -1.09594469  1.3278553\ntheta P12  0.5041406 0.6279370 -0.72659321  1.7348745\ntheta P13  1.7229829 0.6392558  0.47006454  2.9759012\ntheta P14 -1.3996406 0.6329282 -2.64015704 -0.1591241\ntheta P15  2.1284749 0.6339771  0.88590270  3.3710471\ntheta P16  0.9042291 0.6366182 -0.34351956  2.1519777\ntheta P17  0.5041406 0.6279370 -0.72659321  1.7348745\ntheta P18 -0.2617525 0.6115679 -1.46040365  0.9368986\ntheta P19  2.9215618 0.6285530  1.68962065  4.1535030\ntheta P20  0.5041406 0.6279370 -0.72659321  1.7348745\ntheta P21 -1.0101623 0.6169475 -2.21935720  0.1990326\ntheta P22  2.1284749 0.6339771  0.88590270  3.3710471\ntheta P23  0.9042291 0.6366182 -0.34351956  2.1519777\ntheta P24 -2.8215012 0.7705575 -4.33176617 -1.3112361\ntheta P25  0.9042291 0.6366182 -0.34351956  2.1519777\ntheta P26  2.1284749 0.6339771  0.88590270  3.3710471\ntheta P27 -1.3996406 0.6329282 -2.64015704 -0.1591241\ntheta P28 -1.8166969 0.6608129 -3.11186639 -0.5215274\ntheta P29 -1.0101623 0.6169475 -2.21935720  0.1990326\ntheta P30  0.1159553 0.6183277 -1.09594469  1.3278553\ntheta P31 -1.8166969 0.6608129 -3.11186639 -0.5215274\ntheta P32  1.3127857 0.6407991  0.05684262  2.5687289\n\n\nI valori “Estimate” indicano dove si posiziona il rispondente sulla scala di abilità rispetto agli altri. Un valore più alto indica un’abilità maggiore. L’errore standard (“Std. Err.”) associato a ciascuna stima di abilità indica l’incertezza o la variabilità di quella stima. Un errore standard più piccolo suggerisce una maggiore precisione nella stima dell’abilità, mentre un errore standard più grande indica una maggiore incertezza. L’intervallo di confidenza dà un’idea di dove potrebbe effettivamente cadere il vero valore dell’abilità del rispondente, con un livello di confidenza del 95%.\nEsaminiamo gli indici outfit e infit per valutare quanto bene ciascun item si adatta al modello utilizzato.\n\nitem.fit &lt;- eRm::itemfit(person.locations.estimate)\nitem.fit\n\n\nItemfit Statistics: \n          Chisq df p-value Outfit MSQ Infit MSQ Outfit t Infit t Discrim\nrater_1  16.463 31   0.985      0.514     0.618   -1.726  -1.582   0.726\nrater_2  28.274 31   0.607      0.884     1.193   -0.042   0.822   0.366\nrater_3  39.505 31   0.141      1.235     1.229    0.920   0.974   0.357\nrater_4  25.803 31   0.731      0.806     0.821   -0.605  -0.702   0.582\nrater_5  27.703 31   0.636      0.866     0.878   -0.326  -0.379   0.549\nrater_6  38.967 31   0.154      1.218     1.176    0.810   0.738   0.333\nrater_7  14.923 31   0.993      0.466     0.508   -2.432  -2.570   0.845\nrater_8  23.143 31   0.844      0.723     0.799   -1.084  -0.826   0.671\nrater_9  49.382 31   0.019      1.543     1.324    2.009   1.358   0.336\nrater_10 18.362 31   0.965      0.574     0.725   -1.406  -0.997   0.635\nrater_11 28.200 31   0.611      0.881     0.827   -0.293  -0.605   0.546\nrater_12 29.993 31   0.518      0.937     0.987    0.006   0.069   0.353\n\n\nIl valore Chi-Quadrato è un test statistico che misura quanto le osservazioni si discostano dalle aspettative sotto il modello. Un valore più alto indica una maggiore discrepanza. Il p-value è associato al test Chi-Quadrato. Un p-value alto (tipicamente &gt; 0.05) suggerisce che l’item si adatta bene al modello, mentre un p-value basso indica un cattivo adattamento. Outfit MSQ (Mean Square Error) misura la sensibilità alle osservazioni anomale o ai valutatori che si comportano in modo imprevedibile. Valori vicini a 1 indicano un buon adattamento; valori molto più alti o più bassi di 1 indicano un adattamento povero. L’indice Infit MSQ (Mean Square Error) è simile all’Outfit MSQ, ma meno sensibile alle osservazioni anomale. Anche qui, valori vicini a 1 sono desiderabili. I valori Outfit t e Infit t sono i punteggi t standardizzati per Outfit e Infit MSQ. Un punteggio t che si discosta molto da 0 (sia positivamente che negativamente) indica un adattamento povero. La discriminazione misura quanto bene l’item distingue tra rispondenti con diversi livelli di abilità. Valori più alti indicano una migliore discriminazione.\nIn generale, possiamo dire che Rater come “rater_1”, “rater_7”, e “rater_10” mostrano un buon adattamento al modello. I loro valori Outfit e Infit MSQ sono vicini a 1 e hanno p-value alti nel test Chi-Quadrato. Rater come “rater_3”, “rater_6”, e “rater_9” mostrano segni di un cattivo adattamento. Per esempio, “rater_9” ha un p-value basso (0.019) e valori Outfit e Infit MSQ relativamente alti. La discriminazione varia tra i rater. Alcuni (come “rater_7”) hanno valori più alti, indicando una buona capacità di distinguere tra rispondenti di diversa abilità.\nEsaminiamo ora le statistiche di adattamento dei rispondenti:\n\nitem.fit.table &lt;- cbind(item.fit[[\"i.outfitMSQ\"]], item.fit[[\"i.infitMSQ\"]], item.fit[[\"i.infitMSQ\"]], item.fit[[\"i.infitZ\"]])\npfit &lt;- eRm::personfit(person.locations.estimate)\npfit\n\n\nPersonfit Statistics: \n     Chisq df p-value Outfit MSQ Infit MSQ Outfit t Infit t\nP1  18.317 11   0.075      1.526     1.733     0.89    1.92\nP2  10.340 11   0.500      0.862     0.766    -0.10   -0.46\nP3   7.507 11   0.757      0.626     0.766    -0.70   -0.55\nP4   8.787 11   0.642      0.732     0.813    -0.52   -0.50\nP5  10.399 11   0.495      0.867     0.813    -0.05   -0.24\nP6   6.927 11   0.805      0.577     0.915    -0.67   -0.01\nP7   6.250 11   0.856      0.521     0.438    -0.82   -1.29\nP8  17.151 11   0.103      1.429     1.244     0.85    0.62\nP9   7.851 11   0.727      0.654     0.763    -0.68   -0.64\nP10  8.773 11   0.643      0.731     0.853    -0.21   -0.35\nP11 14.839 11   0.190      1.237     1.368     0.59    0.92\nP12  5.979 11   0.875      0.498     0.383    -0.94   -1.60\nP13  9.738 11   0.554      0.811     0.800    -0.16   -0.30\nP14 12.314 11   0.341      1.026     1.074     0.20    0.33\nP15 14.142 11   0.225      1.179     1.549     0.48    1.24\nP16  8.965 11   0.625      0.747     0.834    -0.30   -0.19\nP17 39.175 11   0.000      3.265     2.312     2.81    2.22\nP18  8.442 11   0.673      0.704     0.796    -0.49   -0.45\nP19 16.823 11   0.113      1.402     1.555     0.75    1.54\nP20  6.893 11   0.808      0.574     0.861    -0.73   -0.16\nP21  6.922 11   0.805      0.577     0.662    -0.99   -1.07\nP22  3.572 11   0.981      0.298     0.366    -1.52   -1.83\nP23  5.636 11   0.896      0.470     0.792    -0.98   -0.29\nP24 15.824 11   0.148      1.319     1.166     0.66    0.49\nP25 10.606 11   0.477      0.884     0.669    -0.02   -0.59\nP26  4.911 11   0.935      0.409     0.474    -1.14   -1.39\nP27 17.844 11   0.085      1.487     1.363     1.15    1.09\nP28 11.542 11   0.399      0.962     1.051     0.05    0.26\nP29  6.086 11   0.868      0.507     0.577    -1.23   -1.43\nP30  4.711 11   0.944      0.393     0.555    -1.34   -1.12\nP31 11.117 11   0.434      0.926     0.891    -0.03   -0.19\nP32  2.336 11   0.997      0.195     0.239    -1.98   -2.12\n\n\n\nChisq: Misura quanto le risposte del rispondente si discostano dalle aspettative del modello. Un valore più alto suggerisce una maggiore discrepanza.\ndf: Indica i gradi di libertà per il test del Chi-Quadrato.\np-value: Associa un valore di probabilità al risultato del Chi-Quadrato. Un p-value alto (tipicamente &gt; 0.05) indica un buon adattamento del rispondente al modello. Un p-value basso suggerisce un cattivo adattamento.\nOutfit MSQ (Mean Square Error): Media quadratica ponderata basata su tutte le risposte del rispondente. Valori vicini a 1 indicano un buon adattamento; valori molto alti o molto bassi indicano un cattivo adattamento.\nInfit MSQ (Mean Square Error): Simile all’Outfit, ma dà più peso alle risposte che sono più informative per la stima dell’abilità del rispondente.\nOutfit t e Infit t: Questi sono i punteggi t standardizzati per Outfit e Infit MSQ. Un punteggio t che si discosta molto da 0 (in senso positivo o negativo) indica un adattamento povero.\n\nPer questi dati, rispondenti come P2, P3, P4, ecc., mostrano un buon adattamento, con p-value elevati e valori Outfit e Infit MSQ vicini a 1. Rispondenti come P1, P17, e P27 mostrano segni di cattivo adattamento. P1 e P17 hanno valori Outfit e Infit MSQ più alti e p-value al limite o al di sotto della soglia di significatività. Rispondenti come P22 e P32 mostrano un adattamento eccellente, con valori molto bassi sia per Outfit che per Infit MSQ e p-value molto alti.\n\n61.3.1 Il Modello a Scala di Valutazione\nIl Modello a Scala di Valutazione (Rating Scale Model, RSM), introdotto da Andrich nel 1978, rappresenta una versione specifica e più restrittiva del Modello a Crediti Parziali (Partial Credit Model, PCM). La caratteristica distintiva del RSM è l’uniformità nella scala di risposta: questo modello presume che tutti gli item di un test utilizzino esattamente la stessa scala di valutazione, con un numero identico di categorie di risposta e con interpretazioni equivalenti per queste categorie in tutti gli item.\nAd esempio, in un questionario dove ogni domanda presenta quattro opzioni di risposta - “mai”, “qualche volta”, “spesso”, “sempre” - il RSM assumerà che il passaggio da “mai” a “qualche volta” abbia lo stesso significato e la stessa “distanza” interpretativa in ogni domanda del questionario. La stessa logica si applica alle transizioni tra le altre categorie di risposta.\nD’altro canto, il PCM offre una flessibilità maggiore. In questo modello, ogni item nel test può avere un numero diverso di categorie di risposta, e la significanza di queste categorie può variare da un item all’altro. Ad esempio, una domanda potrebbe avere tre opzioni di risposta, mentre un’altra nello stesso test potrebbe presentarne cinque. Le soglie, ovvero i punti di transizione tra le categorie di risposta, possono essere diversi per ogni item.\nIn sostanza, il RSM è un sottocaso del PCM in cui si applica una struttura di risposta standardizzata per tutti gli item: stesse categorie di risposta, stesso numero di queste categorie, e stesse soglie tra le categorie. Mentre il PCM si adatta alle peculiarità di ciascun item, offrendo una maggiore varietà e adattabilità, il RSM adotta un approccio più uniforme e strutturato, imponendo la stessa scala di valutazione a tutti gli item del test.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Modelli per Risposte Politomiche</span>"
    ]
  },
  {
    "objectID": "chapters/irt/07_irt_poly.html#modello-a-crediti-parziali-generalizzato",
    "href": "chapters/irt/07_irt_poly.html#modello-a-crediti-parziali-generalizzato",
    "title": "61  Modelli per Risposte Politomiche",
    "section": "61.4 Modello a Crediti Parziali Generalizzato",
    "text": "61.4 Modello a Crediti Parziali Generalizzato\nIl Modello a Crediti Parziali Generalizzato (Generalized Partial Credit Model, GPCM) è una versione estesa e più versatile del Modello a Crediti Parziali (Partial Credit Model, PCM). Sviluppato per affrontare alcune limitazioni del PCM e del RSM (Rating Scale Model), il GPCM offre un quadro analitico ancora più flessibile per gestire una varietà di formati di risposta nei test.\nSimile al PCM, nel GPCM, ogni item in un test può avere un numero diverso di categorie di risposta. Questa caratteristica lo rende particolarmente adatto per test che includono item con formati di risposta variabili.\nA differenza del RSM, che impone la stessa struttura di risposta su tutti gli item, il GPCM permette che ogni item abbia le proprie soglie specifiche. Questo significa che la transizione da una categoria di risposta all’altra può avere significati diversi per item differenti.\nUna caratteristica distintiva del GPCM è che permette a ciascun item di avere il proprio parametro di discriminazione. Questo parametro misura quanto efficacemente un item distingue tra rispondenti con livelli diversi di abilità. In altre parole, mentre il PCM assume che tutti gli item abbiano la stessa capacità di discriminazione, il GPCM riconosce che alcuni item possono essere più informativi di altri nel distinguere tra rispondenti.\nIl GPCM è particolarmente utile in contesti di test dove gli item variano significativamente in termini di formato, difficoltà e capacità di discriminazione. Ad esempio, in un test educativo, alcune domande potrebbero essere più efficaci nell’identificare studenti con abilità elevate, mentre altre potrebbero essere migliori nel differenziare tra studenti con abilità più basse. Il GPCM permette di modellare questa varietà in modo più accurato e preciso rispetto al PCM o al RSM.\nIn sintesi, il Generalized Partial Credit Model rappresenta un avanzamento significativo nella teoria della risposta all’item, fornendo una struttura flessibile e adattabile che può essere calibrata per adattarsi alle specifiche caratteristiche di ciascun item in un test. Questa flessibilità lo rende uno strumento prezioso per analisi psicometriche complesse e per la creazione di test che riflettano accuratamente le diverse sfumature delle abilità dei rispondenti.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Modelli per Risposte Politomiche</span>"
    ]
  },
  {
    "objectID": "chapters/irt/07_irt_poly.html#modello-di-risposta-nominale",
    "href": "chapters/irt/07_irt_poly.html#modello-di-risposta-nominale",
    "title": "61  Modelli per Risposte Politomiche",
    "section": "61.5 Modello di Risposta Nominale",
    "text": "61.5 Modello di Risposta Nominale\nIl Modello di Risposta Nominale (Nominal Response Model, NRM) è un’altra variante nella famiglia dei modelli della teoria della risposta all’item (Item Response Theory, IRT). A differenza dei modelli precedentemente menzionati, come il Generalized Partial Credit Model (GPCM) o il Partial Credit Model (PCM), il NRM è specificamente progettato per analizzare dati di risposta dove le categorie non hanno un ordinamento o una gerarchia naturale. Questo lo rende particolarmente adatto per item con risposte nominali.\nNel NRM, le categorie di risposta per ogni item sono trattate come nominali, il che significa che non esiste un ordine intrinseco o una scala di valutazione. Ad esempio, in un questionario che chiede il colore preferito (rosso, verde, blu, ecc.), non c’è un ordine naturale tra le scelte.\nOgni categoria di risposta all’interno di un item ha il suo parametro unico. Questo approccio differisce dai modelli come il PCM, dove le categorie sono ordinate e hanno soglie specifiche per l’item. Nel NRM, ogni scelta risposta è indipendente e ha il proprio parametro che descrive la probabilità che un rispondente con una certa abilità scelga quella categoria.\nMentre modelli come il PCM o il GPCM presuppongono una struttura ordinale nelle risposte, il NRM non fa tale assunzione, rendendolo ideale per item dove le risposte sono categoriche e non ordinabili.\nIl Modello di Risposta Nominale trova applicazione in situazioni di test dove le risposte degli item sono categoriche e non esiste un ordine logico o una scala di preferenza tra le categorie. È utilizzato in ambiti come la ricerca di mercato, i sondaggi di opinione, e in contesti psicometrici dove le domande sono di natura qualitativa piuttosto che quantitativa.\nIn conclusione, il Nominal Response Model si distingue all’interno della teoria della risposta all’item per la sua capacità di gestire efficacemente dati di risposta nominale. Offre un metodo robusto e flessibile per analizzare risposte a scelta multipla dove le opzioni non implicano un gradimento o una preferenza ordinata, consentendo un’analisi dettagliata e accurata di item con risposte non ordinali.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Modelli per Risposte Politomiche</span>"
    ]
  },
  {
    "objectID": "chapters/irt/07_irt_poly.html#modello-di-risposta-graduata",
    "href": "chapters/irt/07_irt_poly.html#modello-di-risposta-graduata",
    "title": "61  Modelli per Risposte Politomiche",
    "section": "61.6 Modello di Risposta Graduata",
    "text": "61.6 Modello di Risposta Graduata\nIl Modello di Risposta Graduata (Graded Response Model, GRM), proposto da Samejima, è un approccio fondamentale nella teoria della risposta all’item (IRT) per l’analisi di dati di test con risposte ordinali, come quelle trovate nei questionari psicologici o educativi che utilizzano scale Likert. Questo modello si distingue per la sua capacità di gestire risposte che esprimono gradi o livelli, ad esempio, da “per niente d’accordo” a “completamente d’accordo”.\nA differenza del Modello a Crediti Parziali (Partial Credit Model, PCM), il GRM utilizza soglie cumulative per modellare le risposte. In questo contesto, ogni soglia indica il punto lungo la scala di abilità in cui un rispondente ha il 50% di probabilità di rispondere a un certo livello o a un livello superiore. Il GRM, quindi, si concentra sul calcolo delle probabilità cumulative di rispondere a un livello specifico o superiore su una scala ordinale fissa.\nIl PCM, sviluppato da Masters, differisce significativamente dal GRM. Mentre il GRM è ideale per questionari con scale di risposta fisse e ordinate, il PCM offre una maggiore flessibilità nella gestione degli item. Ogni item nel PCM può avere un numero variabile di categorie di risposta, e ogni transizione tra queste categorie è modellata separatamente, permettendo di calcolare la probabilità di rispondere in una specifica categoria.\nQuesta differenza rende il PCM particolarmente adatto per valutare risposte che possono variare in termini di correttezza o completezza, come in un test educativo dove le risposte possono essere completamente corrette, parzialmente corrette, o completamente errate. In confronto, il GRM è più comunemente impiegato in ricerche psicologiche e sondaggi dove le risposte riflettono gradi di accordo, soddisfazione, o intensità di un sentimento, su una scala ordinale fissa.\nIn sintesi, mentre il PCM si adatta bene ai test che richiedono una valutazione dettagliata e specifica di risposte con diverse gradazioni di correttezza, il GRM è più efficace in situazioni dove le risposte sono espressivamente ordinali e scalari, fornendo una misura accurata di gradi o livelli di un tratto o di un’abilità.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Modelli per Risposte Politomiche</span>"
    ]
  },
  {
    "objectID": "chapters/irt/07_irt_poly.html#modello-sequenziale",
    "href": "chapters/irt/07_irt_poly.html#modello-sequenziale",
    "title": "61  Modelli per Risposte Politomiche",
    "section": "61.7 Modello Sequenziale",
    "text": "61.7 Modello Sequenziale\nIl Modello Sequenziale (Sequential Model), sviluppato da Tutz nel 1990 e ulteriormente elaborato nel 1997, è un approccio innovativo nella teoria della risposta all’item (IRT) per analizzare dati di test con risposte ordinate, in particolare quando le risposte possono essere considerate come il risultato di un processo decisionale sequenziale.\nIl Modello Sequenziale si basa sull’idea che le risposte a un item siano il risultato di una serie di decisioni. Invece di scegliere direttamente tra diverse categorie di risposta, si presume che i rispondenti passino attraverso una sequenza di passaggi decisionali, con ogni passaggio che porta a una scelta tra due alternative.\nIl modello considera la risposta a un item come una serie di decisioni binarie (sì/no). Ad esempio, in un test a scelta multipla, un rispondente potrebbe prima decidere se una risposta è corretta o errata, e poi, se errata, scegliere tra le opzioni rimanenti.\nOgni passaggio decisionale nella sequenza è modellato separatamente. Questo permette una comprensione più dettagliata di come i rispondenti arrivino alla loro scelta finale.\nIl modello è particolarmente utile per item complessi dove la risposta finale è il risultato di una serie di considerazioni o giudizi.\nImmaginiamo un questionario per la valutazione di un corso. I partecipanti potrebbero prima decidere se il corso è stato generale positivo o negativo (primo stadio decisionale). Se positivo, potrebbero poi decidere se è stato “buono” o “eccellente” (secondo stadio). Se negativo, la scelta potrebbe essere tra “insufficiente” e “mediocre”.\nIl Modello Sequenziale trova applicazione in situazioni di test complesse, dove le risposte non sono semplicemente scelte tra categorie, ma il risultato di una serie di giudizi o valutazioni. Questo può includere valutazioni educative, sondaggi di opinione, e ricerche psicologiche.\nIn sintesi, il Modello Sequenziale di Tutz offre una prospettiva unica sull’analisi di risposte ordinate, enfatizzando il processo decisionale a più stadi che sta dietro la selezione di una risposta.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Modelli per Risposte Politomiche</span>"
    ]
  },
  {
    "objectID": "chapters/irt/07_irt_poly.html#dimensione-del-campione",
    "href": "chapters/irt/07_irt_poly.html#dimensione-del-campione",
    "title": "61  Modelli per Risposte Politomiche",
    "section": "61.8 Dimensione del Campione",
    "text": "61.8 Dimensione del Campione\nNel contesto della teoria della risposta all’item (IRT) per dati politomici, la questione della grandezza del campione è fondamentale per garantire l’accuratezza e l’affidabilità delle stime dei parametri del modello. De Ayala (2009) ha fornito una panoramica delle ricerche riguardanti i requisiti di dimensione del campione per i modelli IRT, sia dicotomici che politomici, e ha offerto alcune linee guida utili.\n\nNumero di Categorie di Risposta: Nei modelli politomici, il numero di categorie di risposta per item influisce significativamente sulla precisione della stima. Maggiore è il numero di categorie, maggiore sarà il numero complessivo di parametri dell’item da stimare.\nDistribuzione dei Parametri di Item e Persone: Come nei modelli dicotomici, la forma e l’allineamento delle distribuzioni dei parametri di item e persone sono importanti. Una distribuzione ben bilanciata può contribuire a una stima più accurata.\n\nDe Ayala propone alcune regole empiriche per le dimensioni minime del campione nei modelli politomici:\n\nModello a Crediti Parziali e Modello a Scala di Valutazione: Per questi modelli, è suggerita una dimensione minima del campione di circa 250 rispondenti. Questo è particolarmente pertinente per test che comprendono, ad esempio, 25 item con 5 categorie di risposta ciascuno.\nModello di Risposta Graduata: Per il GRM, è raccomandata una dimensione del campione di circa 500 rispondenti, soprattutto quando si lavora con un numero elevato di item e categorie di risposta.\n\nProblemi nella convergenza dell’algoritmo di stima e la presenza di errori standard elevati possono essere segnali che la dimensione del campione utilizzato non è sufficiente per una stima affidabile dei parametri del modello.\nIn sintesi, la scelta della grandezza del campione nei modelli IRT politomici deve considerare diversi fattori, inclusi il numero di item, il numero di categorie di risposta per item e il rapporto tra il numero di rispondenti e il numero di parametri dell’item da stimare. Seguendo le linee guida suggerite, i ricercatori possono assicurarsi di avere un campione di dimensioni adeguate per ottenere stime affidabili dei parametri del modello.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Modelli per Risposte Politomiche</span>"
    ]
  },
  {
    "objectID": "chapters/irt/07_irt_poly.html#session-info",
    "href": "chapters/irt/07_irt_poly.html#session-info",
    "title": "61  Modelli per Risposte Politomiche",
    "section": "61.9 Session Info",
    "text": "61.9 Session Info\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] grid      stats4    stats     graphics  grDevices utils     datasets \n[8] methods   base     \n\nother attached packages:\n [1] kableExtra_1.4.0  latex2exp_0.9.6   WrightMap_1.4     psychotools_0.7-4\n [5] ggmirt_0.1.0      TAM_4.2-21        CDM_8.2-6         mvtnorm_1.3-1    \n [9] mirt_1.42         lattice_0.22-6    eRm_1.0-6         ggokabeito_0.1.0 \n[13] viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1   \n[17] bayesplot_1.11.1  gridExtra_2.3     patchwork_1.3.0   semTools_0.5-6   \n[21] semPlot_1.1.6     lavaan_0.6-18     psych_2.4.6.26    scales_1.3.0     \n[25] markdown_1.13     knitr_1.48        lubridate_1.9.3   forcats_1.0.0    \n[29] stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2       readr_2.1.5      \n[33] tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0  \n[37] here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] splines_4.4.1        later_1.3.2          pbdZMQ_0.3-13       \n  [4] R.oo_1.26.0          XML_3.99-0.17        rpart_4.1.23        \n  [7] lifecycle_1.0.4      rstatix_0.7.2        rprojroot_2.0.4     \n [10] vroom_1.6.5          globals_0.16.3       MASS_7.3-61         \n [13] rockchalk_1.8.157    backports_1.5.0      magrittr_2.0.3      \n [16] openxlsx_4.2.7.1     Hmisc_5.1-3          rmarkdown_2.28      \n [19] httpuv_1.6.15        qgraph_1.9.8         zip_2.3.1           \n [22] sessioninfo_1.2.2    RColorBrewer_1.1-3   pbapply_1.7-2       \n [25] minqa_1.2.8          multcomp_1.4-26      abind_1.4-8         \n [28] audio_0.1-11         quadprog_1.5-8       R.utils_2.12.3      \n [31] nnet_7.3-19          TH.data_1.1-2        sandwich_3.1-1      \n [34] listenv_0.9.1        testthat_3.2.1.1     RPushbullet_0.3.4   \n [37] vegan_2.6-8          arm_1.14-4           parallelly_1.38.0   \n [40] svglite_2.1.3        permute_0.9-7        codetools_0.2-20    \n [43] xml2_1.3.6           tidyselect_1.2.1     farver_2.1.2        \n [46] lme4_1.1-35.5        base64enc_0.1-3      jsonlite_1.8.9      \n [49] polycor_0.8-1        progressr_0.14.0     Formula_1.2-5       \n [52] survival_3.7-0       emmeans_1.10.4       systemfonts_1.1.0   \n [55] tools_4.4.1          snow_0.4-4           Rcpp_1.0.13         \n [58] glue_1.7.0           mnormt_2.1.1         admisc_0.36         \n [61] xfun_0.47            mgcv_1.9-1           IRdisplay_1.1       \n [64] withr_3.0.1          beepr_2.0            fastmap_1.2.0       \n [67] boot_1.3-31          fansi_1.0.6          digest_0.6.37       \n [70] mi_1.1               timechange_0.3.0     R6_2.5.1            \n [73] mime_0.12            estimability_1.5.1   colorspace_2.1-1    \n [76] gtools_3.9.5         jpeg_0.1-10          R.methodsS3_1.8.2   \n [79] utf8_1.2.4           generics_0.1.3       data.table_1.16.0   \n [82] corpcor_1.6.10       SimDesign_2.17.1     htmlwidgets_1.6.4   \n [85] pkgconfig_2.0.3      sem_3.1-16           gtable_0.3.5        \n [88] brio_1.1.5           htmltools_0.5.8.1    carData_3.0-5       \n [91] png_0.1-8            rstudioapi_0.16.0    tzdb_0.4.0          \n [94] reshape2_1.4.4       uuid_1.2-1           coda_0.19-4.1       \n [97] checkmate_2.3.2      nlme_3.1-166         curl_5.2.3          \n[100] nloptr_2.1.1         repr_1.1.7           zoo_1.8-12          \n[103] parallel_4.4.1       miniUI_0.1.1.1       foreign_0.8-87      \n[106] pillar_1.9.0         vctrs_0.6.5          promises_1.3.0      \n[109] car_3.1-2            OpenMx_2.21.12       xtable_1.8-4        \n[112] Deriv_4.1.6          cluster_2.1.6        dcurver_0.9.2       \n[115] GPArotation_2024.3-1 htmlTable_2.4.3      evaluate_1.0.0      \n[118] pbivnorm_0.6.0       cli_3.6.3            kutils_1.73         \n[121] compiler_4.4.1       rlang_1.1.4          crayon_1.5.3        \n[124] future.apply_1.11.2  ggsignif_0.6.4       fdrtool_1.2.18      \n[127] plyr_1.8.9           stringi_1.8.4        munsell_0.5.1       \n[130] lisrelToR_0.3        pacman_0.5.1         Matrix_1.7-0        \n[133] IRkernel_1.3.2       hms_1.1.3            glasso_1.11         \n[136] bit64_4.5.2          future_1.34.0        shiny_1.9.1         \n[139] igraph_2.0.3         broom_1.0.6          RcppParallel_5.1.9  \n[142] bit_4.5.0           \n\n\n\n\n\n\nBraun, Henry I. 1988. «Understanding scoring reliability: Experiments in calibrating essay readers». Journal of Educational Statistics 13 (1): 1–18.\n\n\nDebelak, Rudolf, Carolin Strobl, e Matthew D Zeigenfuse. 2022. An introduction to the rasch model with examples in r. Crc Press.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Modelli per Risposte Politomiche</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a1_intro_r.html",
    "href": "chapters/appendix/a1_intro_r.html",
    "title": "Appendice A — Linguaggio R",
    "section": "",
    "text": "\\(\\mathsf{R}\\) è un linguaggio di programmazione per l’analisi dei dati, il calcolo e la visualizzazione grafica. È open source ed estensibile, il che significa che il codice sorgente è disponibile per essere esaminato e riutilizzato. Può essere scaricato gratuitamente dal sito web del Comprehensive R Archive Network (CRAN) ed è disponibile per PC, MacOS e sistemi operativi Linux/Unix. Gran parte del core-R è scritto in Fortran o C++, ma molti pacchetti per \\(\\mathsf{R}\\) sono scritti in \\(\\mathsf{R}\\) stesso. Chiunque può aggiungere pacchetti al CRAN o ad altri repository come GitHub o BioConductor. CRAN ha test di garanzia della qualità per garantire che i programmi contribuiti abbiano una documentazione coerente e non falliscano durante l’esecuzione degli esempi forniti. Al momento ci sono migliaia di pacchetti disponibili per \\(\\mathsf{R}\\) e questo numero aumenta quotidianamente.\nPer programmare in \\(\\mathsf{R}\\), è importante seguire le regole sintattiche del linguaggio. Se una riga di codice non è scritta correttamente, l’interprete di \\(\\mathsf{R}\\) segnalerà un errore. Questo può essere difficile per i principianti, ma ci sono due vantaggi nell’atto di scrivere codice.\n\nPrima di poter risolvere un problema con il codice, è necessario comprenderlo e analizzarlo in modo preciso. Questo aiuta a sviluppare una comprensione più profonda del problema e a identificare soluzioni efficaci.\nInoltre, se un programma non funziona correttamente, il programmatore che lo ha scritto è l’unico responsabile. Questo aiuta i programmatori a sviluppare una maggiore autoconsapevolezza e responsabilità nei confronti del proprio lavoro.\n\nInoltre, su Internet è disponibile una vasta gamma di materiali utili per avvicinarsi all’ambiente \\(\\mathsf{R}\\) e aiutare l’utente nell’apprendimento di questo software statistico. Tra le tante introduzioni al linguaggio \\(\\mathsf{R}\\), si veda ad esempio, Introduction to R di Venables, Smith, and the R development core team (2023).",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Linguaggio R</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a2_sums.html",
    "href": "chapters/appendix/a2_sums.html",
    "title": "Appendice B — Simbolo di somma",
    "section": "",
    "text": "B.1 Manipolazione di somme\nÈ conveniente utilizzare le seguenti regole per semplificare i calcoli che coinvolgono l’operatore della sommatoria.",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Simbolo di somma</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a2_sums.html#manipolazione-di-somme",
    "href": "chapters/appendix/a2_sums.html#manipolazione-di-somme",
    "title": "Appendice B — Simbolo di somma",
    "section": "",
    "text": "B.1.1 Proprietà 1\nLa sommatoria di \\(n\\) valori tutti pari alla stessa costante \\(a\\) è pari a \\(n\\) volte la costante stessa:\n\\[\n\\sum_{i=1}^{n} a = \\underbrace{a + a + \\dots + a} = {n\\text{ volte } a} = n a.\n\\]\n\n\nB.1.2 Proprietà 2 (proprietà distributiva)\nNel caso in cui l’argomento contenga una costante, è possibile riscrivere la sommatoria. Ad esempio con\n\\[\n\\sum_{i=1}^{n} a x_i = a x_1 + a x_2 + \\dots + a x_n\n\\]\nè possibile raccogliere la costante \\(a\\) e fare \\(a(x_1 +x_2 + \\dots + x_n)\\). Quindi possiamo scrivere\n\\[\n\\sum_{i=1}^{n} a x_i = a \\sum_{i=1}^{n} x_i.\n\\]\n\n\nB.1.3 Proprietà 3 (proprietà associativa)\nNel caso in cui\n\\[\n\\sum_{i=1}^{n} (a + x_i) = (a + x_1) + (a + x_1) + \\dots  (a + x_n)\n\\]\nsi ha che\n\\[\n\\sum_{i=1}^{n} (a + x_i) = n a + \\sum_{i=1}^{n} x_i.\n\\]\nÈ dunque chiaro che in generale possiamo scrivere\n\\[\n\\sum_{i=1}^{n} (x_i + y_i) = \\sum_{i=1}^{n} x_i + \\sum_{i=1}^{n} y_i.\n\\]\n\n\nB.1.4 Proprietà 4\nSe deve essere eseguita un’operazione algebrica (innalzamento a potenza, logaritmo, ecc.) sull’argomento della sommatoria, allora tale operazione algebrica deve essere eseguita prima della somma. Per esempio,\n\\[\n\\sum_{i=1}^{n} x_i^2 = x_1^2 + x_2^2 + \\dots + x_n^2 \\neq \\left(\\sum_{i=1}^{n} x_i \\right)^2.\n\\]\n\n\nB.1.5 Proprietà 5\nNel caso si voglia calcolare \\(\\sum_{i=1}^{n} x_i y_i\\), il prodotto tra i punteggi appaiati deve essere eseguito prima e la somma dopo:\n\\[\n\\sum_{i=1}^{n} x_i y_i = x_1 y_1 + x_2 y_2 + \\dots + x_n y_n,\n\\]\ninfatti, \\(a_1 b_1 + a_2 b_2 \\neq (a_1 + a_2)(b_1 + b_2)\\).",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Simbolo di somma</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a2_sums.html#doppia-sommatoria",
    "href": "chapters/appendix/a2_sums.html#doppia-sommatoria",
    "title": "Appendice B — Simbolo di somma",
    "section": "B.2 Doppia sommatoria",
    "text": "B.2 Doppia sommatoria\nÈ possibile incontrare la seguente espressione in cui figurano una doppia sommatoria e un doppio indice:\n\\[\n\\sum_{i=1}^{n}\\sum_{j=1}^{m} x_{ij}.\n\\]\nLa doppia sommatoria comporta che per ogni valore dell’indice esterno, \\(i\\) da \\(1\\) ad \\(n\\), occorre sviluppare la seconda sommatoria per \\(j\\) da \\(1\\) ad \\(m\\). Quindi,\n\\[\n\\sum_{i=1}^{3}\\sum_{j=4}^{6} x_{ij} = (x_{1, 4} + x_{1, 5} + x_{1, 6}) + (x_{2, 4} + x_{2, 5} + x_{2, 6}) + (x_{3, 4} + x_{3, 5} + x_{3, 6}).\n\\]\nUn caso particolare interessante di doppia sommatoria è il seguente:\n\\[\n\\sum_{i=1}^{n}\\sum_{j=1}^{n} x_i y_j\n\\]\nSi può osservare che nella sommatoria interna (quella che dipende dall’indice \\(j\\)), la quantità \\(x_i\\) è costante, ovvero non dipende dall’indice (che è \\(j\\)). Allora possiamo estrarre \\(x_i\\) dall’operatore di sommatoria interna e scrivere\n\\[\n\\sum_{i=1}^{n} \\left( x_i \\sum_{j=1}^{n} y_j \\right).\n\\]\nAllo stesso modo si può osservare che nell’argomento della sommatoria esterna la quantità costituita dalla sommatoria in \\(j\\) non dipende dall’indice \\(i\\) e quindi questa quantità può essere estratta dalla sommatoria esterna. Si ottiene quindi\n\\[\n\\sum_{i=1}^{n}\\sum_{j=1}^{n} x_i y_j = \\sum_{i=1}^{n} \\left( x_i \\sum_{j=1}^{n} y_j \\right) = \\sum_{i=1}^{n} x_i \\sum_{j=1}^{n} y_j.\n\\]\nFacciamo un esercizio. Verifichiamo quanto detto sopra nel caso particolare di \\(x = \\{2, 3, 1\\}\\) e \\(y = \\{1, 4, 9\\}\\), svolgendo prima la doppia sommatoria per poi verificare che quanto così ottenuto sia uguale al prodotto delle due sommatorie.\n\\[\n\\begin{align}\n\\sum_{i=1}^3 \\sum_{j=1}^3 x_i y_j &= x_1y_1 + x_1y_2 + x_1y_3 +\nx_2y_1 + x_2y_2 + x_2y_3 +\nx_3y_1 + x_3y_2 + x_3y_3 \\notag\\\\\n&= 2 \\times (1+4+9) + 3 \\times (1+4+9) + 2 \\times (1+4+9) = 84,\\notag\n\\end{align}\n\\]\novvero\n\\[\n(2 + 3 + 1) \\times (1+4+9) = 84.\n\\]",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Simbolo di somma</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a3_calculus.html",
    "href": "chapters/appendix/a3_calculus.html",
    "title": "Appendice C — Per liberarvi dai terrori preliminari",
    "section": "",
    "text": "C.1 Introduzione ai logaritmi\nIl logaritmo è una funzione matematica che risponde alla domanda: “quante volte devo moltiplicare un certo numero (chiamato”base”) per ottenere un altro numero?” Matematicamente, questo è espresso come:\n\\[\n\\log_b(a) = x \\iff b^x = a\n\\]\nAd esempio, \\(\\log_2(8) = 3\\) perché \\(2^3 = 8\\).\nNel contesto dei logaritmi, i valori molto piccoli (compresi tra 0 e 1) diventano più grandi (in termini assoluti) e negativi quando applichiamo una funzione logaritmica. Questo è utile per stabilizzare i calcoli, specialmente quando lavoriamo con prodotti di numeri molto piccoli che potrebbero portare a problemi di underflow.\nPer esempio: - \\(\\log(1) = 0\\) - \\(\\log(0.1) = -1\\) - \\(\\log(0.01) = -2\\) - \\(\\log(0.001) = -3\\)\nCome si può vedere, i valori assoluti dei logaritmi crescono man mano che il numero originale si avvicina a zero.\nUna delle proprietà più utili dei logaritmi è che consentono di trasformare un prodotto in una somma:\n\\[\n\\log_b(a \\times c) = \\log_b(a) + \\log_b(c)\n\\]\nQuesta proprietà è estremamente utile in calcoli complessi, come nella statistica bayesiana, dove il prodotto di molte probabilità potrebbe diventare un numero molto piccolo e causare problemi numerici.\nUn’altra proprietà utile dei logaritmi è che un rapporto tra due numeri diventa la differenza dei loro logaritmi:\n\\[\n\\log_b\\left(\\frac{a}{c}\\right) = \\log_b(a) - \\log_b(c)\n\\]\nAnche questa proprietà è molto utilizzata in matematica, specialmente in situazioni in cui è necessario normalizzare i dati.\nIn sintesi, i logaritmi sono strumenti potenti per semplificare e stabilizzare i calcoli matematici. Essi consentono di lavorare più agevolmente con numeri molto grandi o molto piccoli e di trasformare operazioni complesse come prodotti e divisioni in somme e differenze, rendendo i calcoli più gestibili e meno inclini a errori numerici.",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Per liberarvi dai terrori preliminari</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a4_linear_alg.html",
    "href": "chapters/appendix/a4_linear_alg.html",
    "title": "Appendice D — Elementi di algebra lineare",
    "section": "",
    "text": "D.1 Vettori",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a4_linear_alg.html#vettori",
    "href": "chapters/appendix/a4_linear_alg.html#vettori",
    "title": "Appendice D — Elementi di algebra lineare",
    "section": "",
    "text": "D.1.1 Vettori nello spazio euclideo\nUn vettore geometrico è un segmento orientato dotato di una lunghezza, una direzione e un verso. Spesso viene rappresentato con una freccia. Dato che i vettori non hanno posizione (ma solo direzione, verso e intensità), sono possibili rappresentazioni multiple dello stesso vettore. Nella discussione seguente, considereremo soltanto vettori che hanno origine nel punto (0, 0). Questo verrà chiarito dall’esempio seguente. La posizione di un punto nel piano può essere espressa nei termini di una coppia ordinata di numeri (\\(x, y\\)), le coordinate di quel punto. Tale coppia di valori rappresenta la distanza verticale dal punto a ciascuno degli assi coordinati.\nPossiamo anche definire il punto \\(P\\) specificando la distanza e la direzione di \\(P\\) dall’origine, ovvero nei termini del vettore \\(\\overrightarrow{OP}\\). A sua volta, questo vettore può essere espresso nei termini delle sue componenti nelle direzioni orizzontali e verticali:\n\\[\n\\overrightarrow{OP} = \\left[ \\begin{array}{c}\n2\\\\\n3\n\\end{array}\n\\right]\n\\]\nSe volessimo specificare un punto in uno spazio a 3 dimensioni, avremmo:\n\\[\n\\overrightarrow{OP} = \\left[ \\begin{array}{c}\nx\\\\\ny\\\\\nz\n\\end{array}\n\\right]\n\\]\nIn generale, un punto \\(P\\) in uno spazio a \\(n\\)-dimensioni sarà specificato da:\n\\[\n\\overrightarrow{OP} = \\left[ \\begin{array}{c}\nv_1\\\\\nv_2\\\\\n\\dots\\\\\nv_n\n\\end{array}\n\\right]\n\\]\nDal punto di vista geometrico, dunque, un vettore rappresenta un punto in uno spazio \\(n\\)-dimensionale.\nIn \\(\\mathsf{R}\\), un vettore è definito come\n\na &lt;- c(1, 3, 2) \na |&gt; print()\n\n[1] 1 3 2\n\n\n\ndf &lt;- data.frame(\n    x = c(0, 3), # Start points for arrows\n    y = c(0, 1), # Start points for arrows\n    xend = c(2, 5), # End points for arrows\n    yend = c(3, 4) # End points for arrows\n)\n\n# Plot arrows using geom_segment with the updated 'linewidth' aesthetic\nggplot(df, aes(x = x, y = y)) +\n    geom_segment(aes(xend = xend, yend = yend),\n        arrow = arrow(length = unit(0.3, \"cm\")), linewidth = 1\n    ) +\n    geom_hline(yintercept = 0, linetype = \"dashed\") +\n    geom_vline(xintercept = 0, linetype = \"dashed\")\n\n\n\n\n\n\n\n\n\n\nD.1.2 Somma e differenza di vettori\nLa somma di due vettori è definita come\n\\[\n(a_1, a_2) + (b_1, b_2) = (a_1 + b_1, a_2 + b_2).\n\\]\nIn \\(\\mathsf{R}\\) abbiamo\n\na &lt;- c(1, 3, 2) \nb &lt;- c(2, 8, 9) \na + b |&gt; print()\n\n[1] 2 8 9\n\n\n\n31111\n\n\nLa differenza di due vettori è\n\\[\n(a_1, a_2) - (b_1, b_2) = (a_1 - b_1, b_2 - b_2).\n\\]\nIn \\(\\mathsf{R}\\) abbiamo\n\na &lt;- c(1, 3, 2) \nb &lt;- c(2, 8, 9) \na - b |&gt; print()\n\n[1] 2 8 9\n\n\n\n-1-5-7\n\n\n\n\nD.1.3 Moltiplicazione scalare\nLa moltiplicazione scalare di un vettore per un numero reale (o scalare) è data da\n\\[\n\\rho (a_1, a_2) = (\\rho a_1, \\rho a_2)\n\\]\nDal punto di vista geometrico, la moltiplicazione scalare effettua una estensione o contrazione del vettore \\(\\boldsymbol{a}\\), preservandone la direzione.\nIn \\(\\mathsf{R}\\) abbiamo\n\na &lt;- 2\nx &lt;- c(2, 8, 9) \na * x |&gt; print()\n\n[1] 2 8 9\n\n\n\n41618\n\n\n\n\nD.1.4 Combinazione lineare\nSe \\(\\mathbf{v}_{1}, \\dots, \\mathbf{v}_{n}\\) sono vettori e \\(a_1, \\dots, a_n\\) sono scalari, allora la combinazione lineare di questi vettori con questi coefficienti scalari è data da\n\\[\n{\\displaystyle a_{1}\\mathbf {v} _{1}+a_{2}\\mathbf {v} _{2}+a_{3}\\mathbf {v} _{3}+\\cdots +a_{n}\\mathbf {v} _{n}.}\n\\] Per esempio, in \\(\\mathsf{R}\\) possiamo aver\n\na &lt;- c(2, 3, 4)\nv1 &lt;- c(2, 8, 3) \nv2 &lt;- c(4, 5, 1) \nv3 &lt;- c(1, 3, 2) \ny &lt;- a[1] * v1 + a[2] * v2 + a[3] * v3\ny |&gt; print()\n\n[1] 20 43 17\n\n\n\n\nD.1.5 Vettore 0 e vettore 1\nIl vettore 0 è costituito da \\(n\\) elementi, tutti uguali a 0. Il vettore 1 è costituito da \\(n\\) elementi, tutti uguali a 1.\nIn \\(\\mathsf{R}\\) abbiamo\n\nx &lt;- rep(0, 5) \nx |&gt; print()\ny &lt;- rep(1, 5)\ny |&gt; print()\n\n[1] 0 0 0 0 0\n[1] 1 1 1 1 1\n\n\n\n\nD.1.6 Ortogonalità tra vettori\nDue vettori si dicono ortogonali, e si scrive \\(\\boldsymbol{a} \\bot \\boldsymbol{b}\\), se e solo se il loro prodotto scalare è nullo:\n\\[\n\\boldsymbol{a}'\\boldsymbol{b} = 0.\n\\]\nIn \\(\\mathsf{R}\\) abbiamo\n\nv1 &lt;- c(1, 1)\nv2 &lt;- c(-1, 1)\nsum(v1 * v2) |&gt; print()\n\n[1] 0\n\n\n\n\nD.1.7 Trasposta di un vettore\nIn un vettore trasposto gli indici delle righe prendono il posto degli indici delle colonne, e viceversa.\nIn \\(\\mathsf{R}\\) abbiamo\n\nv1 &lt;- c(1, 3, 7) %&gt;% \n  as.matrix()\nv1 |&gt; print()\n\n     [,1]\n[1,]    1\n[2,]    3\n[3,]    7\n\n\nLe dimensioni di v1 sono\n\ndim(v1)\n\n\n31\n\n\nLa trasposta di v1 è\n\nv2 &lt;- t(v1)\nv2 |&gt; print()\n\n     [,1] [,2] [,3]\n[1,]    1    3    7\n\n\ndi dimensioni\n\ndim(v2) |&gt; print()\n\n[1] 1 3\n\n\n\n\nD.1.8 Norma o lunghezza di un vettore\nPer il teorema di Pitagora, la norma di un vettore \\((a_1,\na_2)\\) è \\(\\sqrt{a_1^2 + a_2^2}\\) ed è denotata da \\(\\| (a_1,\na_2) \\|\\). Infatti, se un vettore \\(\\boldsymbol{a}\\) (l’ipotenusa) è la somma di due vettori ortogonali \\(\\boldsymbol{a}_1\\) e \\(\\boldsymbol{a}_2\\) (i cateti), allora la lunghezza al quadrato di \\(\\boldsymbol{a}\\) è uguale alla somma dei quadrati delle lunghezze di \\(\\boldsymbol{a}_1\\) e \\(\\boldsymbol{a}_2\\).\nViene detta norma di \\(\\boldsymbol{a}\\) la radice del prodotto scalare di un vettore per se stesso:\n\\[\n\\| \\boldsymbol{a} \\| = \\sqrt{\\boldsymbol{a}'\\boldsymbol{a}}.\n\\]\nIn \\(\\mathsf{R}\\) abbiamo\n\nsqrt(t(v1) %*% v1) |&gt; print()\n\n         [,1]\n[1,] 7.681146",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a4_linear_alg.html#matrici",
    "href": "chapters/appendix/a4_linear_alg.html#matrici",
    "title": "Appendice D — Elementi di algebra lineare",
    "section": "D.2 Matrici",
    "text": "D.2 Matrici\nUna matrice costituisce un insieme rettangolare di scalari ordinati per riga e colonna. Può anche essere vista come la raccolta di \\(m\\) vettori colonna di dimensione \\(n\\) o come la raccolta di \\(n\\) vettori riga di dimensione \\(m\\). Per esempio:\n\\[\n\\boldsymbol{A} =  \\left[ \\begin{array}{c c c}\na_{11} & a_{12} & a_{13}\\\\\na_{21} & a_{22} & a_{23} \\end{array} \\right]\n\\]\nIn \\(\\mathsf{R}\\) abbiamo\n\nM &lt;- matrix(\n  c(1, 2, 3, 4, 5, 6), \n  ncol = 3, \n  byrow = FALSE)\nM |&gt; print()\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\n\nD.2.1 Dimensioni della matrice\nI numeri interi \\(m\\) ed \\(n\\) si dicono dimensioni della matrice, ovvero \\(\\boldsymbol{A}\\) si dice matrice di dimensioni \\(m \\times n\\) o di ordine \\(m \\times n\\). Nel caso presente, la matrice \\(\\boldsymbol{A}\\) ha dimensioni \\(2 \\times 3\\).\n\ndim(M) |&gt; print()\n\n[1] 2 3\n\n\n\n\nD.2.2 Matrice trasposta\nSi definisce matrice trasposta di \\(\\boldsymbol{A}\\), e si denota con \\(\\boldsymbol{A}'\\) oppure \\(\\boldsymbol{A}'\\), la matrice \\(\\boldsymbol{B} = \\boldsymbol{A}'\\) di ordine \\(n \\times m\\) cui elementi sono:\n\\[\nb_{ij} = a_{ji},  \\quad        i = 1 \\dots m, j = 1 \\dots n\n\\]\nPer esempio,\n\\[\n\\left[ \\begin{array}{c c}\n-2 & 5\\\\\n3 & 1\\\\\n7 & -6\n\\end{array}\n\\right]'=\n\\left[ \\begin{array}{c c c}\n-2 & 3 & 7\\\\\n5 & 1 & -6\n\\end{array}\n\\right]\n\\]\n\n\nD.2.3 Matrice simmetrica\nSe accade che \\(\\boldsymbol{A} = \\boldsymbol{A}'\\) allora la matrice è detta simmetrica.\n\\[\n\\boldsymbol{A} =  \\left[ \\begin{array}{c c c}\n7 & 1 & 2\\\\\n1 & 8 & 3\\\\\n2 & 3 & 9 \\end{array} \\right]\n\\]\n\n\\((\\boldsymbol{A} + \\boldsymbol{B})' = (\\boldsymbol{A})' +\n(\\boldsymbol{B})'\\)\n\\((\\boldsymbol{A} - \\boldsymbol{B})' = (\\boldsymbol{A})' -\n(\\boldsymbol{B})'\\)\n\\((\\boldsymbol{a} + \\boldsymbol{b})' = (\\boldsymbol{a})' +\n(\\boldsymbol{b})'\\)\n\\((\\boldsymbol{a} - \\boldsymbol{b})' = (\\boldsymbol{a})' -\n(\\boldsymbol{b})'\\)\n\n\n\nD.2.4 Matrice quadrata o rettangolare\nSe \\(m = n\\) allora la matrice \\(\\boldsymbol{A}\\) si dice quadrata di dimensione \\(n\\) o di ordine \\(n\\) altrimenti si dice rettangolare. Le righe di \\(\\boldsymbol{A}\\) sono \\([a_{11}\\ a_{12}\\ a_{13}]\\) e \\([a_{21}\\ a_{22}\\ a_{23}]\\). Le colonne di \\(\\boldsymbol{A}\\) sono \\(\\left[\\begin{array}{c} a_{11} \\\\ a_{21} \\end{array} \\right]\\), \\(\\left[\n\\begin{array}{c} a_{12} \\\\ a_{22} \\end{array} \\right]\\) e \\(\\left[\n\\begin{array}{c} a_{13} \\\\ a_{23} \\end{array} \\right]\\).\n\n\nD.2.5 Diagonale principale\nSe \\(i\\) e \\(j\\) sono numeri interi con \\(1 \\leq i \\leq m\\) e \\(1 \\leq j\n\\leq n\\) allora l’elemento della matrice \\(\\boldsymbol{A}\\) di dimensione \\(m \\times n\\) che si trova in posizione (\\(i, j\\)) viene indicato con \\(a_{ij}\\). Gli elementi \\(a_{ij}\\) di una matrice quadrata \\(\\boldsymbol{A}\\) di ordine \\(n\\) tali che \\(i = j\\) sono detti elementi principali o diagonali e formano la cosiddetta diagonale principale di \\(\\boldsymbol{A}\\).\n\\[\n\\boldsymbol{A} =  \\left[ \\begin{array}{c c c}\na_{11} & a_{12} & a_{13}\\\\\na_{21} &  a_{22} & a_{23}\\\\\na_{31} & a_{32} & a_{33} \\end{array} \\right]\n\\]\n\n\nD.2.6 Matrice diagonale\nSe gli elementi \\(a_{ij}\\) di una matrice quadrata \\(\\boldsymbol{A}\\) sono tali che \\(a_{ij} =0\\) e \\(a_{ii} \\neq 0\\), allora la matrice \\(\\boldsymbol{A}\\) viene detta matrice diagonale.\n\\[\n\\boldsymbol{A} =  \\left[ \\begin{array}{c c c}\na_{11} & 0 & 0\\\\\n0 & a_{22} & 0\\\\\n0 & 0 & a_{33} \\end{array} \\right]\n\\]\n\n\nD.2.7 Matrice identità\nSi definisce matrice identità di ordine \\(n\\) la matrice quadrata diagonale \\(\\boldsymbol{I}_n\\) avente tutti gli elementi principali uguali a \\(1\\):\n\\[\n\\boldsymbol{I}_3 =  \\left[ \\begin{array}{c c c}\n1 & 0 & 0\\\\\n0 & 1 & 0\\\\\n0 & 0 & 1 \\end{array} \\right]\n\\]\nLa matrice identità ha la stessa funzione del numero “1” nel sistema dei numeri reali.\n\n\nD.2.8 Matrici diagonali e triangolari\nGli elementi di una matrice che si trovano al di sopra della diagonale principale sono detti sopradiagonali, mentre quelli che si trovano al di sotto della stessa diagonale principale sono detti sottodiagonali. Se una matrice ha tutti gli elementi sopradiagonali e sottodiagonali uguali a zero viene detta matrice diagonale. Se invece ha solo gli elementi sopradiagonali nulli allora viene detta triangolare inferiore. Se ha gli elementi sottodiagonali nulli allora è detta triangolare superiore.\n\n\nD.2.9 Somma e sottrazione\nLa somma e la sottrazione di due matrici sono operazioni definite elemento per elemento. Per sommare due matrici sommiamo gli elementi corrispondenti. Per sottrarre due matrici sottraiamo gli elementi corrispondenti. Si noti che queste operazioni hanno senso solo se le due matrici hanno le stesse dimensioni (altrimenti queste operazioni non sono definite). Per esempio,\n\\[\n\\left[ \\begin{array}{c c}\n-2 & 5\\\\\n3 & 1\\\\\n7 & -6\n\\end{array}\n\\right]+\n\\left[ \\begin{array}{c c}\n3 & -2\\\\\n4 & 5\\\\\n10 & -3\n\\end{array}\n\\right]=\n\\left[ \\begin{array}{c c}\n1 & 3\\\\\n7 & 6\\\\\n17 & -9\n\\end{array}\n\\right]\n\\]\n\nA &lt;- matrix(\n  c(-2, 5, 3, 1, 7, -6), nrow = 3, byrow = TRUE\n)\nA |&gt; print()\n\n     [,1] [,2]\n[1,]   -2    5\n[2,]    3    1\n[3,]    7   -6\n\n\n\nB &lt;- matrix(\n  c(3, -2, 4, 5, 10, -3), nrow = 3, byrow = TRUE\n)\nB |&gt; print()\n\n     [,1] [,2]\n[1,]    3   -2\n[2,]    4    5\n[3,]   10   -3\n\n\n\n(A + B) |&gt; print()\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    7    6\n[3,]   17   -9\n\n\n\\[\n\\left[ \\begin{array}{c c}\n-2 & 5\\\\\n3 & 1\\\\\n7 & -6\n\\end{array}\n\\right]-\n\\left[ \\begin{array}{c c}\n3 & -2\\\\\n4 & 5\\\\\n10 & -3\n\\end{array}\n\\right]=\n\\left[ \\begin{array}{c c}\n-5 & 7\\\\\n-1 & -4\\\\\n-3 & -3\n\\end{array}\n\\right]\n\\]\n\n(A - B) |&gt; print()\n\n     [,1] [,2]\n[1,]   -5    7\n[2,]   -1   -4\n[3,]   -3   -3\n\n\n\n\nD.2.10 Moltiplicazione di scalari e matrici\nL’effetto della moltiplicazione di una matrice \\(\\boldsymbol{A}\\) di qualsiasi dimensione per un numero reale b (scalare) è quello di moltiplicare ciascun elemento in \\(\\boldsymbol{A}\\) per b. Questo è equivalente a sommare \\(\\boldsymbol{A}\\) a se stessa b volte. Per esempio,\n\\[\n3 \\left[ \\begin{array}{c c}\n-2 & 5\\\\\n3 & 1\\\\\n7 & -6\n\\end{array}\n\\right]=\n\\left[ \\begin{array}{c c}\n-6 & 15\\\\\n9 & 3\\\\\n21 & -18\n\\end{array}\n\\right]\n\\]\n\n(3 * A) |&gt; print()\n\n     [,1] [,2]\n[1,]   -6   15\n[2,]    9    3\n[3,]   21  -18\n\n\n\n\nD.2.11 Proprietà della somma e differenza\nSiano \\(A\\), \\(B\\) e \\(C\\) matrici di dimensioni \\(m \\times n\\), e siano \\(k\\) e \\(p\\) scalari appartenenti a un campo \\(F\\). Allora, valgono le seguenti proprietà:\n\n\\(A + B = B + A\\) (commutatività)\n\\(A + (B + C) = (A + B) + C\\) (associatività)\n\\(0 + A = A\\) (0 è l’identità additiva)\n\\(A + (-A) = 0\\) ( (-A) è l’inverso additivo di \\(A\\))\n\\(k(A + B) = kA + kB\\) (la moltiplicazione scalare è distributiva rispetto all’addizione di matrici)\n\\((k + p)A = kA + pA\\) (la moltiplicazione scalare è distributiva rispetto all’addizione di scalari)\n\\((kp)A = k(pA)\\) (associatività della moltiplicazione scalare)\n\\(1A = A\\) (la moltiplicazione per 1 preserva la matrice)\n\n\n\nD.2.12 Moltiplicazione matrice-vettore\nSia \\(A\\) una matrice \\(m \\times n\\) con colonne \\(a_1, a_2, \\dots, a_n\\):\n\\[\nA = \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n\\end{bmatrix}\n\\]\ne sia \\(x\\) un vettore colonna di dimensione \\(n \\times 1\\), cioè \\(x \\in \\mathbb{R}^n\\):\n\\[\nx = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}.\n\\]\nIl prodotto matrice-vettore \\(Ax\\) è definito come la combinazione lineare delle colonne di \\(A\\) con i coefficienti dati dalle componenti di \\(x\\):\n\\[\nAx = x_1 a_1 + x_2 a_2 + \\dots + x_n a_n = \\sum_{j=1}^n a_{ij}x_j.\n\\]\nInterpretazione geometrica: La moltiplicazione di un vettore per una matrice può essere vista come una trasformazione lineare dello spazio.\nEsempio: Consideriamo le matrici:\n\\[\nA =\n\\begin{bmatrix}\n2 & -1 & 0 \\\\\n3 & 1/2 & \\pi \\\\\n-2 & 1 & 1 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n\\quad \\text{e} \\quad x =\n\\begin{bmatrix}\n2 \\\\\n1 \\\\\n2\n\\end{bmatrix}.\n\\]\nAllora:\n\\[\nAx = 2 \\begin{bmatrix} 2 \\\\ 3 \\\\ -2 \\\\ 0 \\end{bmatrix} + 1 \\begin{bmatrix} -1 \\\\ 1/2 \\\\ 1 \\\\ 0 \\end{bmatrix} + 2 \\begin{bmatrix} 0 \\\\ \\pi \\\\ 1 \\\\ 0 \\end{bmatrix}\n=\n\\begin{bmatrix} -3 \\\\ -5/2 + 2\\pi \\\\ 5 \\\\ 0 \\end{bmatrix}.\n\\]\n\n\nD.2.13 Prodotto di matrici\nLa moltiplicazione di matrici è un’operazione fondamentale in algebra lineare, ma richiede una certa attenzione. A differenza della somma, il prodotto di matrici non è commutativo e la sua definizione è un po’ più complessa.\nCondizione di conformabilità: Due matrici \\(\\boldsymbol{A}\\) e \\(\\boldsymbol{B}\\) si dicono conformabili per il prodotto \\(\\boldsymbol{AB}\\) se il numero di colonne di \\(\\boldsymbol{A}\\) è uguale al numero di righe di \\(\\boldsymbol{B}\\).\nDefinizione: Siano \\(\\boldsymbol{A}\\) una matrice \\(m \\times p\\) e \\(\\boldsymbol{B}\\) una matrice \\(p \\times n\\). Il prodotto \\(\\boldsymbol{C} = \\boldsymbol{AB}\\) è una matrice \\(m \\times n\\) tale che:\n\\[\nc_{ij} = \\sum_{k=1}^{p} a_{ik}b_{kj}.\n\\]\nIn altre parole, l’elemento \\(c_{ij}\\) di \\(\\boldsymbol{C}\\) si ottiene moltiplicando gli elementi corrispondenti della i-esima riga di \\(\\boldsymbol{A}\\) per gli elementi della j-esima colonna di \\(\\boldsymbol{B}\\) e sommando i prodotti ottenuti.\nProprietà:\n\nAssociatività: \\((AB)C = A(BC)\\).\nNon commutatività: In generale, \\(AB \\neq BA\\).\nDistributività rispetto alla somma: \\(A(B+C) = AB + AC\\) e \\((A+B)C = AC + BC\\).\nMoltiplicazione per uno scalare: \\(k(AB) = (kA)B = A(kB)\\).\nMoltiplicazione per la matrice identità: Se \\(I\\) è la matrice identità, allora \\(AI = A\\) e \\(IA = A\\).\n\nAd esempio, siano \\(\\boldsymbol{A}\\) e \\(\\boldsymbol{B}\\) le seguenti matrici\n\\[\n\\left[ \\begin{array}{c c c}\n-2 & 1 & 1\\\\\n1 & 1 & 4\\\\\n2 & -3 & 2\n\\end{array}\n\\right] \\quad \\text{e} \\quad\n\\left[ \\begin{array}{c c c}\n3 & -2 &1\\\\\n4 & 5 & 0\\\\\n1 & -3 & 1\n\\end{array}\n\\right]\n\\]\nCalcoliamo la matrice \\(\\boldsymbol{C} = \\boldsymbol{AB}\\). L’elemento \\(c_{ij}\\) è uguale alla somma dei prodotti degli elementi della i-esima riga di \\(\\boldsymbol{A}\\) per la j-esima colonna di \\(\\boldsymbol{B}\\).\n\\(c_{11} = (-2) \\cdot 3 + 1 \\cdot 4 + 1 \\cdot 1 = -1\\)\n\\(c_{12} = (-2) \\cdot (-2) + 1 \\cdot 5 + 1 \\cdot (-3) = 6\\)\n\\(c_{13} = (-2) \\cdot 3 + 1 \\cdot 0 + 1 \\cdot 1 = -1\\)\n\\(c_{21} = 1 \\cdot 3 + 1 \\cdot 4 + 4 \\cdot 1 = 11\\)\n\\(c_{22} = 1 \\cdot (-2) + 1 \\cdot 5 + 4 \\cdot (-3) = -9\\)\n\\(c_{23} = 1 \\cdot 3 + 1 \\cdot 0 + 4 \\cdot 1 = 5\\)\n\\(c_{31} = 2 \\cdot 3 +(-3) \\cdot 4 + 2 \\cdot 1 = -4\\)\n\\(c_{32} = 2 \\cdot (-2) +(-3) \\cdot 5 + 2 \\cdot (-3) = -25\\)\n\\(c_{33} = 2 \\cdot 1 + (-3) \\cdot 0 + 2 \\cdot 1 = 4\\)\nIn definitiva\n\\[\n\\boldsymbol{C} =  \\left[ \\begin{array}{c c c}\n-1 & 6 & -1\\\\\n11 & -9 & 5\\\\\n-4 & -25 & 4\n\\end{array}\n\\right].\n\\]\nPossiamo facilmente svolgere i calcoli precedenti in R nel modo seguente:\n\nA = matrix(\n  c(-2, 1, 1, 1, 1, 4, 2, -3, 2), \n  nrow = 3,\n  byrow = TRUE\n)\nA |&gt; print()\n\n     [,1] [,2] [,3]\n[1,]   -2    1    1\n[2,]    1    1    4\n[3,]    2   -3    2\n\n\n\nB = matrix(\n  c(3, -2, 1, 4, 5, 0, 1, -3, 1), \n  nrow = 3,\n  byrow = TRUE\n)\nB |&gt; print()\n\n     [,1] [,2] [,3]\n[1,]    3   -2    1\n[2,]    4    5    0\n[3,]    1   -3    1\n\n\n\n(A %*% B) |&gt; print()\n\n     [,1] [,2] [,3]\n[1,]   -1    6   -1\n[2,]   11   -9    5\n[3,]   -4  -25    4\n\n\nCalcolando il prodotto \\(\\boldsymbol{D} = \\boldsymbol{BA}\\) si trova invece:\n\\[\n\\boldsymbol{D} =  \\left[ \\begin{array}{c c c}\n-6 & -2 & -3\\\\\n-3 & 9 & 24\\\\\n-3 & -5 & -9\n\\end{array}\n\\right]\n\\]\nda cui risulta evidente che \\(\\boldsymbol{AB} \\neq \\boldsymbol{BA}\\).\n\n(B %*% A) |&gt; print()\n\n     [,1] [,2] [,3]\n[1,]   -6   -2   -3\n[2,]   -3    9   24\n[3,]   -3   -5   -9\n\n\n\n\nD.2.14 Casi particolari\nLa matrice identità è l’elemento neutro per il prodotto, cioè se \\(\\boldsymbol{I}\\) è una matrice \\(n \\times n\\) si ha\n\\[\n\\boldsymbol{A} \\boldsymbol{I}_n = \\boldsymbol{I}_n \\boldsymbol{A}\n= \\boldsymbol{A}.\n\\]\nPer esempio,\n\\[\n\\boldsymbol{IA} = \\left(%\n\\begin{array}{cc}\n  1 & 0 \\\\\n  0 & 1 \\\\\n\\end{array}%\n\\right)\n\\left(%\n\\begin{array}{ccc}\n  2 & 3 & -1 \\\\\n  1 & 4 & 7 \\\\\n\\end{array}%\n\\right)=\n\\left(%\n\\begin{array}{ccc}\n  2 & 3 & -1 \\\\\n  1 & 4 & 7 \\\\\n\\end{array}%\n\\right).\n\\]\nIn R la matrice identità si crea nel modo seguente.\n\nprint(diag(2)) \n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\n\nDunque\n\nA &lt;- matrix(\n  c(2, 3, -1, 1, 4, 7),\n  nrow = 2, byrow = TRUE\n)\nA |&gt; print()\n\n     [,1] [,2] [,3]\n[1,]    2    3   -1\n[2,]    1    4    7\n\n\n\n(diag(2) %*% A) |&gt; print()\n\n     [,1] [,2] [,3]\n[1,]    2    3   -1\n[2,]    1    4    7\n\n\n\n(A %*% diag(3)) |&gt; print()\n\n     [,1] [,2] [,3]\n[1,]    2    3   -1\n[2,]    1    4    7\n\n\nUn secondo caso particolare si verifica quando una matrice è costituita da un’unica colonna o un’unica riga. Se la matrice \\(\\boldsymbol{A}\\) si riduce ad una sola colonna (o una sola riga) e viene detta vettore colonna (o riga) ad \\(m\\) elementi o componenti. Un vettore colonna è una matrice \\(n \\times 1\\); un vettore riga è una matrice \\(1 \\times m\\). Se \\(\\boldsymbol{a}\\) è un vettore colonna di \\(m\\) elementi allora \\(\\boldsymbol{a}'\\) è un vettore riga sempre di \\(m\\) elementi.\nPer le operazioni tra vettori valgono le stesse regole viste per le matrici, cioè la somma e la differenza sono possibili tra vettori dello stesso tipo e con lo stesso numero di componenti. La moltiplicazione è possibile tra una matrice e un vettore di dimensioni appropriate, e tra due vettori di dimensioni appropriate. In questo secondo caso, distinguiamo tra prodotto interno e prodotto esterno.\n\n\nD.2.15 Operazioni tra vettori\nIl prodotto interno (o scalare) di un vettore \\(\\boldsymbol{a}'\\) \\(1 \\times n\\) che premoltiplica un vettore \\(\\boldsymbol{b}\\) \\(n \\times 1\\) produce uno scalare:\n\\[\n\\boldsymbol{a}'\\boldsymbol{b} = \\sum_{i=1}^{n}a_i b_i.\n\\]\nDati due vettori \\(\\boldsymbol{a}\\), \\(\\boldsymbol{b}\\) di ordini \\(n\n\\times 1\\) e \\(m \\times 1\\), il prodotto esterno \\(\\boldsymbol{C} = \\boldsymbol{ab}'\\) è una matrice \\(n \\times m\\) di elementi \\(c_{ij} = a_i b_j\\).\n\n\nD.2.16 Prodotto interno\nSiano \\(\\boldsymbol{a}\\) e \\(\\boldsymbol{b}\\) i seguenti vettori:\n\\[\n\\left[ \\begin{array}{c}\n1 \\\\\n2 \\\\\n3\n\\end{array}\n\\right] \\quad e \\quad\n\\left[ \\begin{array}{c}\n-1 \\\\\n-2 \\\\\n4\n\\end{array}\n\\right].\n\\]\nIl prodotto interno è:\n\\[\n\\boldsymbol{a}'\\boldsymbol{b}= 1 \\cdot (-1) + 2 \\cdot (-2) + 3\n\\cdot 4 = 7.\n\\]\nOsserviamo che tale operazione gode della proprietà commutativa, poichè \\(\\boldsymbol{b}'\\boldsymbol{a}=7\\).\n\na &lt;- matrix(\n  c(1, 2, 3), \n  nrow = 3, \n  byrow = TRUE\n)\na |&gt; print()\n\n     [,1]\n[1,]    1\n[2,]    2\n[3,]    3\n\n\n\nb &lt;- matrix(\n  c(-1, -2, 4), \n  nrow = 3, \n  byrow = TRUE\n)\nb |&gt; print()\n\n     [,1]\n[1,]   -1\n[2,]   -2\n[3,]    4\n\n\n\n(t(a) %*% b) |&gt; print()\n\n     [,1]\n[1,]    7\n\n\n\n(t(b) %*% a) |&gt; print()\n\n     [,1]\n[1,]    7\n\n\n\n\nD.2.17 Prodotto esterno\nIl prodotto esterno è la matrice\n\\[\n\\boldsymbol{C} = \\boldsymbol{a}\\boldsymbol{b}'= \\left[\n\\begin{array}{c c c}\n-1 & -2 & 4\\\\\n-2 & -4 & 8\\\\\n-3 & -6 & 12\n\\end{array}\n\\right].\n\\]\n\na %*% t(b) |&gt; print()\n\n     [,1] [,2] [,3]\n[1,]   -1   -2    4\n[2,]   -2   -4    8\n[3,]   -3   -6   12\n\n\nTale prodotto non gode della proprietà commutativa, infatti:\n\\[\n\\boldsymbol{D} = \\boldsymbol{b}\\boldsymbol{a}'= \\left[\n\\begin{array}{c c c}\n-1 & -2 & -3\\\\\n-2 & -4 & -6\\\\\n4 & 8 & 12\n\\end{array}\n\\right]\n\\]\n\nb %*% t(a) |&gt; print()\n\n     [,1] [,2] [,3]\n[1,]   -1   -2   -3\n[2,]   -2   -4   -6\n[3,]    4    8   12\n\n\n\n\nD.2.18 Traccia di una matrice\nSi definisce traccia di una matrice quadrata \\(\\boldsymbol{A}\\) \\(n \\times n\\), e si denota con \\(tr(\\boldsymbol{A})\\) la somma degli elementi sulla diagonale principale di \\(\\boldsymbol{A}\\):\n\\[\ntr(\\boldsymbol{A}) = \\sum_{i=1}^{n} a_{ii}.\n\\]\nLa traccia gode delle seguenti proprietà:\n\\[\n\\begin{aligned}\n&tr(\\rho \\boldsymbol{A}) = \\rho tr( \\boldsymbol{A}) \\notag \\\\\n&tr(\\boldsymbol{A} + \\boldsymbol{B}) =  tr( \\boldsymbol{A})+tr( \\boldsymbol{B}) \\notag \\\\\n&tr(\\boldsymbol{A}') =  tr( \\boldsymbol{A}) \\notag \\\\\n&tr(\\boldsymbol{AB}) =  tr( \\boldsymbol{BA}) \\notag\\end{aligned}\n\\]\nPer esempio, sia\n\\[\n\\boldsymbol{A} =  \\left[ \\begin{array}{c c c}\n7 & 1 & 2\\\\\n1 & 8 & 3\\\\\n2 & 3 & 9 \\end{array} \\right]\n\\]\nallora\n\\[\ntr(\\boldsymbol{A}) = 7 + 8 + 9 = 24.\n\\]\n\nA &lt;- matrix(\n  c(7,1, 2, 1, 8, 3, 2, 3, 9),\n  nrow = 3,\n  byrow = TRUE\n)\nA |&gt; print()\n\n     [,1] [,2] [,3]\n[1,]    7    1    2\n[2,]    1    8    3\n[3,]    2    3    9\n\n\n\nsum(diag(A)) |&gt; print()\n\n[1] 24\n\n\n\n\nD.2.19 Dipendenza lineare\nSi consideri la matrice\n\\[\n\\boldsymbol{A}=\n\\left(%\n\\begin{array}{ccc}\n  1 & 1 & 1 \\\\\n  3 & 1 & 5 \\\\\n  2 & 3 & 1 \\\\\n\\end{array}%\n\\right).\n\\]\nSiano \\(\\boldsymbol{c}_1\\), \\(\\boldsymbol{c}_2\\), \\(\\boldsymbol{c}_3\\) le colonne di \\(\\boldsymbol{A}\\). Si noti che\n\\[\n2\\boldsymbol{c}_1 + -\\boldsymbol{c}_2 + - \\boldsymbol{c}_3 =\n\\boldsymbol{0}\n\\]\ndove \\(\\boldsymbol{0}\\) è un vettore (\\(3 \\times 1\\)) di zeri.\nDato che le 3 colonne di \\(\\boldsymbol{A}\\) possono essere combinate linearmente in modo da produrre un vettore \\(\\boldsymbol{0}\\) vi è chiaramente una qualche forma di relazione, o dipendenza, tra le informazioni nelle colonne. Detto in un altro modo, sembra esserci una qualche duplicazione delle informazione nelle colonne. In generale, si dice che \\(k\\) colonne \\(\\boldsymbol{c}_1, \\boldsymbol{c}_2,\n\\dots \\boldsymbol{c}_k\\) di una matrice sono linearmente dipendenti se esiste un insieme di valori scalari \\(\\lambda_1,\n\\dots, \\lambda_k\\) tale per cui\n\\[\n\\lambda_1 \\boldsymbol{c}_1 + \\dots + \\lambda_k \\boldsymbol{c}_k=\\boldsymbol{0}\n\\]\ne almeno uno dei valori \\(\\lambda_i\\) non è uguale a 0.\nLa dipendenza lineare implica che ciascun vettore colonna è una combinazione degli altri. Per esempio\n\\[\n\\boldsymbol{c}_k= -(\\lambda_1 \\boldsymbol{c}_1 + \\dots + \\lambda_{k-1}\n   \\boldsymbol{c}_{k-1})/\\lambda_k.\n\\]\nQuesto implica che tutta “l’informazione” della matrice è contenuta in un sottoinsieme delle colonne – se \\(k-1\\) colonne sono conosciute, l’ultima resta determinata. È in questo senso che abbiamo detto che l’informazione della matrice veniva “duplicata”.\nSe l’unico insieme di valori scalari \\(\\lambda_i\\) che soddisfa l’equazione\n\\[\n\\lambda_1 \\boldsymbol{c}_1 + \\dots + \\lambda_k \\boldsymbol{c}_k=\\boldsymbol{0}\n\\]\nè un vettore di zeri, allora questo significa che non vi è alcuna relazione tra le colonne della matrice. Le colonne si dicono linearmente indipendenti, nel senso che non contengono alcuna “duplicazione” di informazione.\n\n\nD.2.20 Rango di una matrice\nIl rango della matrice è il massimo numero di vettori colonna linearmente indipendenti che possono essere selezionati dalla matrice. In maniera equivalente, il rango di una matrice può essere definito come il massimo numero di vettori riga linermente indipendenti. Il rango minimo di una matrice è 1, il che significa che vi è una colonna tale per cui le altre colonne sono dei multipli di questa. Per l’esempio precedente, il rango della matrice \\(\\boldsymbol{A}\\) è 2.\nSe la matrice è quadrata, \\(\\boldsymbol{A}_{n \\times n}\\), ed è costituita da vettori tutti indipendenti tra di loro, allora il suo rango è \\(n\\). Se, invece, la matrice è rettangolare, \\(\\boldsymbol{A}_{m \\times n}\\), allora il suo rango può essere al massimo il più piccolo tra i due valori m ed n, cioè:\n\\[\nr(\\boldsymbol{A}_{m \\times n}) \\leq min(m,n).\n\\]",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a4_linear_alg.html#analisi-delle-componenti-principali",
    "href": "chapters/appendix/a4_linear_alg.html#analisi-delle-componenti-principali",
    "title": "Appendice D — Elementi di algebra lineare",
    "section": "D.6 Analisi delle componenti principali",
    "text": "D.6 Analisi delle componenti principali\nL’analisi precedente, mirata a offrire una lettura geometrica degli autovettori e autovalori, pone le premesse per un approccio avanzato nell’analisi multivariata, noto come Analisi delle Componenti Principali (PCA). Questo metodo sfrutta la proiezione ortogonale dei dati su assi definiti dagli autovettori per calcolare i cosiddetti punteggi delle componenti principali.\nLa PCA si propone di semplificare la complessità dei dati riducendone le dimensioni. Ad esempio, se due variabili mostrano un’elevata correlazione, la prima componente principale può racchiudere l’essenza della loro varianza. In pratica, potrebbe risultare più efficace basarsi sui punteggi della prima componente principale piuttosto che sui punteggi derivanti dalle due variabili separate.\nLa varianza totale dei dati è rappresentata dalla somma degli autovalori, equivalenti alla traccia della matrice di covarianza. La prima componente principale incapsula una porzione di varianza pari al suo corrispondente autovalore. Se il rapporto tra questo primo autovalore e la varianza totale è significativo (per esempio, 0.8), optare per i punteggi della prima componente principale offre un vantaggio nell’interpretazione e nella riduzione dimensionale dei dati.\nL’implementazione della PCA si basa su proiezioni ortogonali: i punteggi della prima componente sono determinati proiettando i dati lungo l’asse del primo autovettore, mentre i punteggi della seconda componente derivano dalla proiezione lungo il secondo autovettore, e così via per le componenti successive.\nNell’esempio trattato, è possibile allineare il sistema di coordinate in modo che l’asse X coincida con il primo autovettore e l’asse Y con il secondo. Questo reorientamento si realizza attraverso i seguenti passaggi.\nInizio standardizzando i dati, in modo che il centro del sistema di assi coordinati corrisponda con la media delle due variabili.\n\nd &lt;- data.frame(tibble(zx = scale(x), zy= scale(y)))  %&gt;% as.matrix()\ndim(d)\n\n\n202\n\n\nRuoto gli assi coordinati.\n\ndata_set_3 &lt;- d %*% solve(ee$vectors) # Inverse of eigenvectors matrix\n\n\n# Scatter showing the rotation \nggplot(data.frame(data_set_3), aes(X1, X2)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_vline(xintercept = 0, linewidth = .5) +\n  geom_hline(yintercept = 0, linewidth = .5) +\n  xlab(\"PC1 (78.8%)\") +\n  ylab(\"PC2 (21.2%)\")\n\n\n\n\n\n\n\n\nAvendo ruotato gli assi coordinati, la proiezione ortogonale dei punti su ciascun asse non è altro che la coordinata cartesiana di quel punto all’interno del nuovo sistema di assi coordinati.\n\nhead(data_set_3)\n\n\nA matrix: 6 x 2 of type dbl\n\n\n0.2843804\n0.1433279\n\n\n-0.6256511\n-0.4783489\n\n\n-0.1818790\n-1.0310925\n\n\n-0.1749611\n-0.4272913\n\n\n0.0329488\n2.3525898\n\n\n0.1624602\n0.2662514\n\n\n\n\n\nControlliamo il risultato ottenuto svolgendo la PCA usando la funzione princomp di R. Calcoliamo anche i punteggi delle due “componenti principali”.\n\npca_mod &lt;- princomp(d)\nloadings_mod &lt;- pca_mod$loadings\nscr_mod &lt;- pca_mod$scores\nscr_mod |&gt; head()\n\n\nA matrix: 6 x 2 of type dbl\n\n\nComp.1\nComp.2\n\n\n\n\n0.1433279\n0.2843804\n\n\n-0.4783489\n-0.6256511\n\n\n-1.0310925\n-0.1818790\n\n\n-0.4272913\n-0.1749611\n\n\n2.3525898\n0.0329488\n\n\n0.2662514\n0.1624602\n\n\n\n\n\nSi noti che i punteggi che abbiamo ottenuto sono identici a quelli trovati in precedenza.\n\ncor(data_set_3[, 1], scr_mod[, 2])\n\n1\n\n\nLa varianza di questi punteggi corrisponde agli autovalori:\n\nvar(scr_mod[, 1])\n\n1.82910333474767\n\n\n\nee$values[1]\n\n1.82910333474767\n\n\n\nvar(scr_mod[, 2])\n\n0.170896665252332\n\n\n\nee$values[2]\n\n0.170896665252332",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/solutions_probability.html",
    "href": "chapters/appendix/solutions_probability.html",
    "title": "Appendice E — Probabilità",
    "section": "",
    "text": "# Standard library imports\nimport os\n\n# Third-party imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport arviz as az\nimport scipy.stats as stats\nfrom scipy.special import expit  # Funzione logistica\nimport math\nfrom cmdstanpy import cmdstan_path, CmdStanModel\n\n# Configuration\nseed = sum(map(ord, \"stan_poisson_regression\"))\nrng = np.random.default_rng(seed=seed)\naz.style.use(\"arviz-darkgrid\")\n%config InlineBackend.figure_format = \"retina\"\n\n# Define directories\nhome_directory = os.path.expanduser(\"~\")\nproject_directory = f\"{home_directory}/_repositories/psicometria\"\n\n# Print project directory to verify\nprint(f\"Project directory: {project_directory}\")\n\nProject directory: /Users/corradocaudek/_repositories/psicometria\n\n\n\n?sec-prob-on-general-spaces\n?exr-prob-on-general-spaces-1\nPer calcolare questa probabilità in maniera analitica, utilizziamo la seguente uguaglianza:\n\\[\nP(\\text{almeno 2 psicologi clinici}) = 1 - P(\\text{nessun psicologo clinico}) - P(\\text{1 psicologo clinico}).\n\\]\nIl numero totale di modi per selezionare 5 persone dal gruppo di 20 è dato da:\n\\[\n\\binom{20}{5} = \\frac{20!}{5!(15!)} = 15,504.\n\\]\nIl numero di modi per avere nessun psicologo clinico nella commissione (ovvero, selezionare solo psicologi del lavoro) è:\n\\[\n\\binom{10}{0} \\times \\binom{10}{5} = 1 \\times 252 = 252.\n\\]\nQuindi, la probabilità di avere nessun psicologo clinico è:\n\\[\nP(\\text{nessun psicologo clinico}) = \\frac{252}{15,504} \\approx 0.016.\n\\]\nIl numero di modi per avere esattamente 1 psicologo clinico nella commissione è:\n\\[\n\\binom{10}{1} \\times \\binom{10}{4} = 10 \\times 210 = 2,100.\n\\]\nQuindi, la probabilità di avere esattamente 1 psicologo clinico è:\n\\[\nP(\\text{1 psicologo clinico}) = \\frac{2,100}{15,504} \\approx 0.135.\n\\]\nLa probabilità di avere almeno 2 psicologi clinici nella commissione è quindi:\n\\[\n\\begin{align}\nP(\\text{almeno 2 psicologi clinici}) &= 1 - P(\\text{nessun psicologo clinico}) - P(\\text{1 psicologo clinico}) \\notag\\\\\n&= 1 - 0.016 - 0.135 \\notag\\\\\n&= 0.848.\\notag\n\\end{align}\n\\]\nQuindi, la probabilità che almeno 2 psicologi clinici siano nella commissione è circa 0.848.\n\n# Funzione per calcolare le combinazioni\ndef nCk(n, k):\n    return math.factorial(n) // (math.factorial(k) * math.factorial(n - k))\n\n\n# Calcolo delle probabilità per il problema della commissione\ntotal_ways = nCk(20, 5)\nno_clinical = nCk(10, 0) * nCk(10, 5)\none_clinical = nCk(10, 1) * nCk(10, 4)\n\np_no_clinical = no_clinical / total_ways\np_one_clinical = one_clinical / total_ways\n\np_at_least_two_clinical = 1 - p_no_clinical - p_one_clinical\n\nprint(f\"Probabilità di almeno 2 psicologi clinici: {p_at_least_two_clinical:.3f}\")\n\nProbabilità di almeno 2 psicologi clinici: 0.848\n\n\nIn maniera più intuitiva, possiamo risolvere il problema con una simulazione Monte Carlo.\n\nimport random\n\n# Numero di simulazioni\nsimulations = 1000000\n\n# Numero di successi (almeno 2 psicologi clinici nella commissione)\nsuccess_count = 0\n\n# Creiamo una lista che rappresenta il gruppo di 20 persone\n# 1 rappresenta un psicologo clinico, 0 rappresenta un psicologo del lavoro\ngroup = [1] * 10 + [0] * 10\n\n# Simulazione Monte Carlo\nfor _ in range(simulations):\n    # Estrai casualmente 5 persone dal gruppo\n    committee = random.sample(group, 5)\n\n    # Conta quanti psicologi clinici ci sono nella commissione\n    num_clinical_psychologists = sum(committee)\n\n    # Verifica se ci sono almeno 2 psicologi clinici\n    if num_clinical_psychologists &gt;= 2:\n        success_count += 1\n\n# Calcola la probabilità\nprobability = success_count / simulations\n\n# Mostra il risultato\nprint(\n    f\"La probabilità che almeno 2 psicologi clinici siano nella commissione è: {probability:.4f}\"\n)\n\nLa probabilità che almeno 2 psicologi clinici siano nella commissione è: 0.8482\n\n\n\n\n?sec-simulations\n?exr-prob-simulation-1\nPer calcolare le deviazioni standard delle distribuzioni gaussiane date le percentuali di studenti che ottengono meno di 18, possiamo utilizzare le proprietà della distribuzione normale e i quantili della distribuzione normale standard (distribuzione normale con media 0 e deviazione standard 1).\nLe distribuzioni normali hanno la proprietà che possiamo trasformare qualsiasi valore \\(X\\) della distribuzione \\(N(\\mu, \\sigma)\\) nella distribuzione normale standard \\(N(0, 1)\\) tramite la formula:\n\\[ Z = \\frac{X - \\mu}{\\sigma}, \\]\ndove \\(Z\\) è il quantile standardizzato.\nPer trovare il valore di \\(\\sigma\\) dato un certo percentile, utilizziamo l’inverso della funzione di distribuzione cumulativa (CDF) della distribuzione normale standard. Per un dato percentile \\(p\\), \\(z_p\\) è tale che:\n\\[ p = P(Z \\leq z_p) \\]\nQuindi possiamo trovare \\(\\sigma\\) risolvendo per \\(\\sigma\\) nella formula:\n\\[ z_p = \\frac{X - \\mu}{\\sigma}, \\]\n\\[ \\sigma = \\frac{X - \\mu}{z_p}, \\]\ndove:\n\n\\(X\\) è il punteggio di soglia (18 in questo caso).\n\\(\\mu\\) è la media della distribuzione.\n\\(z_p\\) è il quantile della distribuzione normale standard per il percentile \\(p\\).\n\nI quantili della distribuzione normale standard per i percentili desiderati sono:\n\nPer il 15%, il quantile è \\(z_{0.15} \\approx -1.036\\).\nPer il 10%, il quantile è \\(z_{0.10} \\approx -1.281\\).\nPer il 5%, il quantile è \\(z_{0.05} \\approx -1.645\\).\n\nPrima Prova\n\nMedia: \\(\\mu = 24\\)\nPercentuale che ottiene meno di 18: 15%\nQuantile: \\(z_{0.15} = -1.036\\)\nSoglia: \\(X = 18\\)\n\n\\[ \\sigma_1 = \\frac{24 - 18}{1.036} \\approx 5.79 \\]\nSeconda Prova\n\nMedia: \\(\\mu = 25\\)\nPercentuale che ottiene meno di 18: 10%\nQuantile: \\(z_{0.10} = -1.281\\)\nSoglia: \\(X = 18\\)\n\n\\[ \\sigma_2 = \\frac{25 - 18}{1.281} \\approx 5.46 \\]\nTerza Prova\n\nMedia: \\(\\mu = 26\\)\nPercentuale che ottiene meno di 18: 5%\nQuantile: \\(z_{0.05} = -1.645\\)\nSoglia: \\(X = 18\\)\n\n\\[ \\sigma_3 = \\frac{26 - 18}{1.645} \\approx 4.86 \\]\n\n# Funzione per calcolare la deviazione standard data la media, la soglia e il quantile\ndef calculate_std(mean, threshold, quantile):\n    return abs((mean - threshold) / quantile)\n\n\n# Parametri delle distribuzioni gaussiane per le tre prove\nmean_test1 = 24\nstd_test1 = calculate_std(mean_test1, 18, -1.036)\nmean_test2 = 25\nstd_test2 = calculate_std(mean_test2, 18, -1.281)\nmean_test3 = 26\nstd_test3 = calculate_std(mean_test3, 18, -1.645)\n\n# Numero di studenti\nn_students = 220\n\n# Percentuale di studenti che non fa le prove\ndrop_test1 = 0.10\ndrop_test2 = 0.05\n\n# Seed per il generatore di numeri casuali basato sulla stringa \"simulation\"\nseed = sum(map(ord, \"simulation\"))\nrng = np.random.default_rng(seed=seed)\n\n# Generazione dei voti per le tre prove\n# Genera i voti solo per gli studenti che partecipano alla prova\ntest1_scores = np.where(\n    rng.random(n_students) &gt; drop_test1,\n    rng.normal(mean_test1, std_test1, n_students),\n    np.nan,\n)\ntest2_scores = np.where(\n    rng.random(n_students) &gt; drop_test2,\n    rng.normal(mean_test2, std_test2, n_students),\n    np.nan,\n)\ntest3_scores = rng.normal(mean_test3, std_test3, n_students)\n\n# Calcola il voto finale solo per gli studenti che hanno partecipato a tutte e tre le prove\nfinal_scores = np.nanmean(\n    np.column_stack((test1_scores, test2_scores, test3_scores)), axis=1\n)\n\n# Filtra gli studenti che non hanno partecipato a tutte e tre le prove\nvalid_final_scores = final_scores[~np.isnan(final_scores)]\n\n# Visualizzazione della distribuzione finale dei voti\nplt.hist(valid_final_scores, bins=30, edgecolor=\"black\")\nplt.title(\"Distribuzione dei voti finali\")\nplt.xlabel(\"Voto finale\")\nplt.ylabel(\"Frequenza\")\nplt.show()\n\n# Statistiche descrittive dei voti finali\nmean_final_score = np.mean(valid_final_scores)\nmedian_final_score = np.median(valid_final_scores)\nstd_final_score = np.std(valid_final_scores)\n\nprint(f\"Media dei voti finali: {mean_final_score:.2f}\")\nprint(f\"Mediana dei voti finali: {median_final_score:.2f}\")\nprint(f\"Deviazione standard dei voti finali: {std_final_score:.2f}\")",
    "crumbs": [
      "Appendici",
      "Soluzioni degli esercizi",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Probabilità</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Testing Psicologico",
    "section": "",
    "text": "Benvenuti\nQuesto sito web è dedicato al materiale didattico dell’insegnamento di Testing Psicologico (A.A. 2024/2025), rivolto agli studenti del primo anno del Corso di Laurea Magistrale PSICOLOGIA CLINICA E DELLA SALUTE E NEUROPSICOLOGIA dell’Università degli Studi di Firenze.\nL’insegnamento si propone quale stimolo e guida per l’apprendimento delle basi dell’assessment psicologico.",
    "crumbs": [
      "Benvenuti"
    ]
  },
  {
    "objectID": "index.html#informazioni-sullinsegnamento",
    "href": "index.html#informazioni-sullinsegnamento",
    "title": "Testing Psicologico",
    "section": "Informazioni sull’insegnamento",
    "text": "Informazioni sull’insegnamento\n\n\nCodice: B033288 - TESTING PSICOLOGICO \n\nModulo: B033288 - TESTING PSICOLOGICO (Cognomi L-Z) \n\nCorso di laurea: Laurea Magistrale: PSICOLOGIA CLINICA E DELLA SALUTE E NEUROPSICOLOGIA \n\nAnno Accademico: 2024-2025 \n\nCalendario: Il corso si terrà dal 3 marzo al 31 maggio 2025.\n\nOrario delle lezioni: Le lezioni si svolgeranno il lunedì e il martedì dalle 8:30 alle 10:30 e il giovedì dalle 11:30 alle 13:30.\n\nLuogo: Le lezioni si terranno presso il Plesso didattico La Torretta.\n\nModalità di svolgimento della didattica: Le lezioni ed esercitazioni saranno svolte in modalità frontale.\n\n\n\n\n\n\n\nIl presente sito web costituisce l’unica fonte ufficiale da consultare per ottenere informazioni sul programma dell’insegnamento B033288 - TESTING PSICOLOGICO (Cognomi A-K) A.A. 2024-2025 e sulle modalità d’esame.",
    "crumbs": [
      "Benvenuti"
    ]
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "Testing Psicologico",
    "section": "Syllabus",
    "text": "Syllabus\nIl Syllabus può essere scaricato utilizzando questo link.",
    "crumbs": [
      "Benvenuti"
    ]
  },
  {
    "objectID": "prefazione.html",
    "href": "prefazione.html",
    "title": "Prefazione",
    "section": "",
    "text": "Definizione di misurazione\nLa misurazione psicologica è un pilastro fondamentale nella comprensione e nell’analisi del comportamento umano, fornendo un mezzo quantitativo per esplorare le dinamiche della mente e della personalità. La definizione di misurazione proposta da Stevens (1951), uno dei pionieri della teoria della misurazione, stabilisce che essa consiste nell’assegnare numeri a oggetti o eventi secondo regole definite. Tuttavia, è ormai ampiamente accettato che questa visione sia troppo semplicistica e che la misurazione richieda un approccio più sofisticato. Si concorda comunemente sul fatto che la misurazione debba essere considerata come un processo di creazione di modelli che rappresentano i fenomeni di interesse, principalmente in forma quantitativa.\nDi conseguenza, la misurazione si basa su regole che attribuiscono scale o valori alle entità che rappresentano i costrutti di interesse. Come avviene per tutti i modelli, quelli di misurazione, come i test, le scale o le variabili, devono semplificare la realtà per risultare utili. Pertanto, è fondamentale specificare chiaramente i modelli di misurazione per poterli valutare, confutare e migliorare.\nInoltre, anziché chiedersi se un modello sia vero o corretto, è più utile sviluppare diversi modelli alternativi plausibili e porre domande del tipo: quale modello è meno inaccurato? Questo approccio al confronto dei modelli rappresenta la strategia migliore per valutare e perfezionare le procedure di misurazione, consentendo un’analisi più approfondita e accurata delle variabili coinvolte.\nPer illustrare l’approccio alla misurazione come descritto, prendiamo in considerazione un esempio concreto: la valutazione dell’intelligenza attraverso il test del quoziente intellettivo (QI).\nIniziamo definendo il concetto di interesse, ovvero l’intelligenza, che può essere concepita come la capacità di apprendere, comprendere e applicare conoscenze, risolvere problemi e adattarsi a nuove situazioni. Tuttavia, trattandosi di un concetto astratto, è necessario operazionalizzarlo in modo misurabile.\nPer misurare l’intelligenza, si crea un test di QI che comprende una serie di compiti e domande progettati per valutare diverse dimensioni della capacità cognitiva, quali la memoria, il ragionamento logico e la comprensione verbale.\nCiascun compito nel test di QI è associato a un punteggio. I risultati individuali vengono quindi calcolati e confrontati con una norma statistica per attribuire un punteggio di QI.\nSuccessivamente, il test di QI viene sottoposto a diverse analisi per verificare la sua validità (ovvero se misura effettivamente l’intelligenza) e affidabilità (se fornisce risultati consistenti nel tempo).\nTuttavia, esistono diverse teorie dell’intelligenza, come ad esempio quella delle intelligenze multiple di Gardner, che suggeriscono modelli alternativi di misurazione. Confrontando il modello del QI con questi approcci alternativi, gli psicologi possono valutare quale modello è meno distorto o più adatto per specifici scopi.\nIn risposta alle critiche, alle nuove scoperte e ai cambiamenti culturali e sociali, il modello del QI viene regolarmente rivisto e adattato per assicurare che continui a essere uno strumento utile di misurazione.\nQuesto esempio mostra come la misurazione in psicologia non sia semplicemente un atto di assegnare numeri a un costrutto, ma piuttosto un processo complesso che implica la creazione, la valutazione e il continuo perfezionamento di modelli teorici.",
    "crumbs": [
      "Prefazione"
    ]
  },
  {
    "objectID": "prefazione.html#temi-centrali-nellapproccio-psicometrico",
    "href": "prefazione.html#temi-centrali-nellapproccio-psicometrico",
    "title": "Prefazione",
    "section": "Temi Centrali nell’Approccio Psicometrico",
    "text": "Temi Centrali nell’Approccio Psicometrico\n\nAffidabilità: Questo concetto si riferisce alla capacità di un test di produrre risultati consistenti nel tempo e in contesti diversi, costituendo una base fondamentale per la misurazione psicologica.\nValidazione del Costrutto e Test dei Modelli: L’evoluzione della psicometria ha portato a una sempre maggiore enfasi sulla validazione dei costrutti e sull’importanza dei test di modelli, utilizzando tecniche come i modelli a equazioni strutturali (SEM) per verificare la coerenza e la validità dei costrutti psicologici.\nDimensionalità e Validità Strutturale: La dimensionalità viene considerata un elemento fondamentale nella valutazione della validità strutturale, poiché permette di esplorare come i diversi aspetti di un costrutto si manifestano e interagiscono all’interno del modello di misurazione.\nCostruzione dei Questionari: La progettazione e la formulazione degli item dei questionari rivestono un ruolo cruciale, in quanto influenzano direttamente l’affidabilità e la validità dei risultati ottenuti. La scelta degli item, il loro ordine e la chiarezza della formulazione sono tutti aspetti che contribuiscono alla qualità e all’efficacia della misurazione psicologica.\n\nAttraverso questi approcci, la misurazione psicologica si adatta alle sfide uniche poste dalla natura astratta e complessa dei costrutti psicologici, cercando di fornire strumenti validi e affidabili per la loro esplorazione e comprensione.",
    "crumbs": [
      "Prefazione"
    ]
  },
  {
    "objectID": "prefazione.html#affidabilità-e-generalizzabilità-nelle-misure-psicologiche",
    "href": "prefazione.html#affidabilità-e-generalizzabilità-nelle-misure-psicologiche",
    "title": "Prefazione",
    "section": "Affidabilità e Generalizzabilità nelle Misure Psicologiche",
    "text": "Affidabilità e Generalizzabilità nelle Misure Psicologiche\nNel contesto della misurazione psicologica, così come in altre discipline, è cruciale considerare le variabili che possono influenzare la precisione delle misure. L’affidabilità di uno strumento di misurazione psicologica si riferisce alla sua consistenza nel produrre risultati replicabili nel tempo e in contesti diversi. Gli indici di affidabilità sono utilizzati per quantificare il grado di riproducibilità e l’assenza di errori casuali nelle misurazioni.\n\nTeoria Classica dei Test\nL’approccio più ampiamente utilizzato nello studio dell’affidabilità delle misure psicologiche è rappresentato dalla teoria classica dei test, come descritto da Lord e Novick (1968). Secondo questa teoria, ogni misurazione (\\(X\\)) è composta da due componenti distintive: un punteggio “vero” (\\(T\\)) e un errore di misurazione (\\(e\\)). Il concetto di misurazione accurata, o “vera”, può essere rappresentato come \\(X - e\\), evidenziando il fatto che ogni misurazione può essere decomposta in tali elementi distinti.\nLa teoria classica dei test enfatizza l’importanza di condurre misurazioni ripetute per valutare l’affidabilità. Un concetto fondamentale è quello dei test paralleli, che consistono in due test con medie, varianze e distribuzioni identiche, e che mostrano una correlazione simile con variabili esterne. In questa prospettiva, il punteggio vero e l’errore di misurazione sono considerati indipendenti. Di conseguenza, la varianza dei punteggi osservati (Varianza \\(X\\)) è la somma della varianza dei punteggi veri (Varianza \\(T\\)) e della varianza dell’errore di misurazione (Varianza \\(e\\)).\nL’affidabilità è quindi definita come il rapporto tra la varianza del punteggio vero e la varianza del punteggio osservato:\n\\[\n\\text{Affidabilità} = \\frac{\\text{Varianza}(T)}{\\text{Varianza}(X)}.\n\\]\nIn termini pratici, un’affidabilità di 1 indicherebbe l’assenza di errori, mentre un’affidabilità di 0 implicherebbe che i punteggi derivano esclusivamente dall’errore. La correlazione tra il punteggio osservato e il punteggio vero è la radice quadrata dell’affidabilità, fornendo una stima della precisione della misurazione.\nQuesto framework fornisce una solida base per comprendere e quantificare l’affidabilità nelle misure psicologiche, sottolineando l’importanza di considerare sia i punteggi veri sia gli errori di misurazione per ottenere misurazioni precise e affidabili.\n\n\nEvidenze Multiple di Affidabilità\nNonostante la teoria classica dei test fornisca una definizione matematica dei test paralleli, non fornisce dettagliate linee guida sulle procedure specifiche per costruirli. Tuttavia, a partire dagli anni ’50, sono stati sviluppati diversi metodi che consentono di valutare empiricamente l’affidabilità delle misurazioni:\n\nTest-Retest: Questo approccio implica la somministrazione dello stesso test ai partecipanti in due momenti diversi. L’obiettivo è valutare la stabilità dei punteggi nel tempo. Una correlazione elevata tra i punteggi ottenuti nei due momenti indica una buona affidabilità del test-retest.\nEquivalenza di Forme Parallele: Questo metodo prevede l’utilizzo di due versioni diverse del test, ma che coprono lo stesso contenuto, somministrate simultaneamente ai partecipanti. Una forte correlazione tra i punteggi ottenuti dalle due versioni suggerisce che entrambe misurano il medesimo costrutto in modo affidabile.\nSplit-Half e Coerenza Interna:\n\nSplit-Half: I partecipanti completano una sola versione del test, la quale è divisa in due parti equivalenti. Si calcola poi la correlazione tra i punteggi delle due metà. Questo metodo valuta la coerenza interna del test.\nCoerenza Interna (ad esempio, Omega di McDonals): Valuta la correlazione tra tutti gli elementi del test. Un alto valore di coerenza interna indica che tutti gli elementi del test misurano aspetti simili del costrutto.\n\nValutazione da Giudici Multipli: In questo caso, i partecipanti sono valutati da più giudici in un’unica occasione. Un alto grado di accordo tra i giudici fornisce un’indicazione dell’affidabilità delle valutazioni.\n\nCiascuno di questi approcci fornisce indicazioni sull’affidabilità di un test, ma è fondamentale considerare che alcuni potrebbero essere più appropriati di altri in base alla natura del test e del costrutto misurato. L’affidabilità è pertanto un concetto multidimensionale che richiede l’impiego di diversi approcci per una valutazione completa delle misurazioni psicologiche.\n\n\nIl Ruolo del Coefficiente Alpha nella Misurazione Psicologica\nIl coefficiente alpha, introdotto da Cronbach nel 1951, è diventato un importante indicatore di coerenza interna nella letteratura psicologica, principalmente grazie alla sua facilità di calcolo. A differenza dell’affidabilità test-retest, che richiede dati raccolti in due momenti diversi, o dell’affidabilità delle forme parallele, che richiede la costruzione di due versioni alternative di un test, il coefficiente alpha può essere calcolato utilizzando un unico set di dati, rendendolo estremamente pratico come indice di affidabilità.\nTuttavia, è importante correggere un comune malinteso riguardo al coefficiente alpha: esso non misura direttamente l’omogeneità delle intercorrelazioni tra gli elementi o conferma la unidimensionalità di una scala. In realtà, il coefficiente alpha non fornisce informazioni dirette su questi aspetti strutturali della scala.\nPer affrontare la questione della unidimensionalità, è necessario ricorrere a approcci più sofisticati come l’analisi fattoriale confermativa e i modelli di equazioni strutturali (SEM). Questi metodi consentono di testare quanto bene la struttura di correlazione degli elementi si adatti a un modello con un singolo fattore rispetto a modelli multifattoriali, valutando se le correlazioni tra gli elementi possono essere meglio spiegate da un singolo costrutto sottostante.\nNel contesto delle analisi SEM, le saturazioni degli item indicano quanto della varianza di un item sia condivisa con gli altri (e quindi generalizzabile), mentre la varianza residua dell’item cattura l’errore unico associato a quell’item. La presenza di multidimensionalità emerge dalla capacità di un modello multifattoriale di adattarsi meglio ai dati rispetto a un modello a singolo fattore.\nQuando un test è considerato multidimensionale, è ancora appropriato utilizzare il coefficiente alpha come indice di affidabilità? La risposta è negativa. In presenza di multidimensionalità, il coefficiente alpha tende a sottostimare l’affidabilità. Pertanto, è consigliabile, in tali casi, utilizzare altri metodi per valutare l’affidabilità, anziché basarsi esclusivamente sul coefficiente alpha.\n\n\nIl Fenomeno dell’Attenuazione in Relazione all’Affidabilità\nAll’interno del contesto della teoria classica dei test, come delineato da Lord e Novick (1968), l’affidabilità svolge un ruolo cruciale poiché influisce sulla forza della correlazione che una misura può mostrare con altre variabili, come un criterio esterno. Secondo questa teoria, se l’errore nelle misurazioni è genuinamente casuale, il massimo teorico della correlazione tra una misura e un’altra variabile non è 1.0, ma piuttosto la radice quadrata dell’affidabilità di quella misura.\nCiò implica che, in presenza di un’affidabilità meno che ottimale, la correlazione effettiva tra una misura e qualsiasi altra variabile viene sistematicamente sottostimata, fenomeno noto come attenuazione. Questa attenuazione è direttamente proporzionale all’inadeguatezza dell’affidabilità: più bassa è l’affidabilità di una misura, maggiore sarà la sottostima della sua correlazione con altre variabili. Pertanto, per ottenere stime accurate delle correlazioni e comprendere veramente le relazioni tra diverse variabili, è fondamentale garantire che le misure utilizzate siano il più affidabili possibile. Questa considerazione enfatizza l’importanza dell’accuratezza e della precisione nelle procedure di misurazione psicologica.\n\n\nLa Teoria della Generalizzabilità\nLa Teoria della Generalizzabilità propone un approccio più completo e flessibile per comprendere l’affidabilità delle misure psicologiche rispetto alla classificazione tradizionale delle tipologie di affidabilità. Invece di limitarsi a categorizzare le misure in base a criteri specifici come test-retest, affidabilità interna o inter-valutatori, la Teoria della Generalizzabilità considera una serie di dimensioni che possono influenzare l’affidabilità in contesti diversi.\nUna delle principali criticità della teoria classica dei test è la sua presunzione di uniformità e parallelismo delle misurazioni e degli errori casuali. La Teoria della Generalizzabilità, al contrario, riconosce che l’affidabilità dipende dalla specifica dimensione di generalizzazione considerata. Ad esempio, un test potrebbe essere affidabile per misurare una certa caratteristica in un contesto, ma non altrettanto affidabile in un contesto diverso o per una caratteristica correlata ma non identica.\nPer superare le limitazioni della teoria classica dei test, l’American Psychological Association ha proposto l’adozione della Teoria della Generalizzabilità. Tuttavia, nonostante questa proposta, la pratica nei campi di ricerca non si è adeguatamente evoluta e la teoria della generalizzabilità non ha ancora completamente sostituito le nozioni più semplicistiche popolari in psicologia.\nLa Teoria della Generalizzabilità esamina diverse dimensioni che influenzano l’affidabilità, tra cui la dimensione temporale, delle forme, degli item e dei giudici o osservatori. Questa teoria enfatizza l’importanza di estendere le osservazioni a un’ampia varietà di situazioni e identificare l’impatto specifico delle fonti di varianza nei punteggi dei test in contesti particolari.\nInvece dei tradizionali coefficienti di affidabilità come il coefficiente di stabilità o il coefficiente alfa, la Teoria della Generalizzabilità suggerisce l’uso di misure più ampie di affidabilità, come il coefficiente di correlazione intraclasse, per esaminare specifici aspetti dell’affidabilità. Questo approccio è particolarmente utile in ricerche con dati strutturati in maniera nidificata e dove diverse dimensioni possono influenzare l’affidabilità, come nei metodi di valutazione ecologica momentanea.### La Teoria della Risposta agli Item\nLa Teoria della Risposta agli Item (IRT) rappresenta un avanzamento rispetto alla teoria classica dei test, offrendo un approccio più sofisticato per analizzare le risposte degli individui agli item e la loro relazione con un costrutto latente. Questa teoria stabilisce un collegamento tra le risposte degli individui a un particolare item e il costrutto latente utilizzando una funzione chiamata “curva caratteristica dell’item”.\nLa curva caratteristica dell’item mostra la probabilità che individui con differenti livelli del costrutto latente rispondano correttamente all’item, fornendo inoltre informazioni sulla capacità dell’item di distinguere tra individui con livelli elevati e bassi del tratto latente, oltre a misurare la sua difficoltà. Queste informazioni sono cruciali per identificare eventuali distorsioni negli item, noto come bias. Secondo la IRT, un item è privo di bias nel misurare un costrutto se individui con lo stesso livello del tratto ottengono punteggi attesi simili sull’item, indipendentemente da caratteristiche non rilevanti come genere, etnia o background culturale.\nLa Teoria della Risposta agli Item offre diversi vantaggi nel processo di creazione e valutazione di scale psicometriche:\n\nSelezione degli Item: Permette di selezionare gli item in base alla loro difficoltà e alla capacità di discriminazione, superando così la limitazione della teoria classica che si basa esclusivamente sulle correlazioni tra gli item e il punteggio totale.\nTesting Adattivo Computerizzato: La IRT facilita la valutazione della posizione di un individuo su un costrutto latente senza la necessità di somministrare l’intero test, grazie a tecniche come il testing adattivo computerizzato.\n\nIn conclusione, la Teoria della Risposta agli Item fornisce strumenti quantitativi per esaminare approfonditamente la relazione tra un item specifico e il costrutto latente, attraverso parametri di difficoltà e discriminazione.",
    "crumbs": [
      "Prefazione"
    ]
  },
  {
    "objectID": "prefazione.html#evoluzione-e-comprensione-della-validità-nelle-misure-psicologiche",
    "href": "prefazione.html#evoluzione-e-comprensione-della-validità-nelle-misure-psicologiche",
    "title": "Prefazione",
    "section": "Evoluzione e Comprensione della Validità nelle Misure Psicologiche",
    "text": "Evoluzione e Comprensione della Validità nelle Misure Psicologiche\nLa nostra comprensione della validità nelle misure psicologiche ha subito un notevole sviluppo nel corso del tempo, passando da una visione iniziale più frammentata a un approccio più olistico e dinamico. Inizialmente, la validità veniva suddivisa in diversi tipi, tra cui la validità di contenuto, di facciata, orientata al criterio e di costrutto.\nLa validità di contenuto si riferisce alla rappresentatività degli item di un test rispetto al costrutto che si intende misurare, mentre la validità di facciata valuta se superficialmente gli item sembrano idonei a misurare il costrutto, sebbene questa non sia considerata un indice rigoroso di validità. La validità orientata al criterio si divide ulteriormente in predittiva e concorrente, che valutano la capacità del test di prevedere comportamenti futuri o di correlare con criteri esterni contemporaneamente misurati. Infine, la validità di costrutto indaga se il test misura effettivamente il costrutto in questione, richiedendo una comprensione approfondita sia del costrutto sia della metodologia del test.\nTuttavia, queste distinzioni sono state gradualmente considerate limitate e frammentarie. Un punto di svolta è stato rappresentato dall’approccio olistico di Samuel Messick, che ha enfatizzato che la validità va oltre la misura stessa, coinvolgendo l’interpretazione e l’uso dei punteggi del test. Messick ha sottolineato l’importanza di considerare le evidenze di validità da molteplici fonti e di assicurare la coerenza delle interpretazioni dei punteggi del test con le teorie psicologiche sottostanti.\nUn’importante correzione concettuale è stata l’idea che la validità non sia un attributo statico dei test, ma piuttosto un processo continuo di accumulo di evidenze e giustificazioni teoriche. Questo processo di validazione riflette l’evoluzione delle teorie psicologiche e delle metodologie di misurazione, sottolineando che la validità è dinamica e contestuale.\nIn sintesi, l’evoluzione della concezione di validità nelle misure psicologiche sottolinea l’importanza di un approccio comprensivo, teoricamente informato e basato sull’evidenza per valutare, interpretare e utilizzare i punteggi dei test. Questo approccio moderno incoraggia i ricercatori e i praticanti a considerare la validità come un concetto ampio che incorpora molteplici aspetti della progettazione, dell’implementazione e dell’interpretazione dei test psicologici.",
    "crumbs": [
      "Prefazione"
    ]
  },
  {
    "objectID": "prefazione.html#approfondimento-su-tecniche-di-validazione-di-costrutto-e-costruzione-di-scale",
    "href": "prefazione.html#approfondimento-su-tecniche-di-validazione-di-costrutto-e-costruzione-di-scale",
    "title": "Prefazione",
    "section": "Approfondimento su Tecniche di Validazione di Costrutto e Costruzione di Scale",
    "text": "Approfondimento su Tecniche di Validazione di Costrutto e Costruzione di Scale\nLa discussione sulla evoluzione della validità nelle misure psicologiche può proseguire con l’esame delle tecniche che vengono usate per la validazione di costrutto e per la costruzione di scale. In particolare, gli strumenti maggiormente usati dagli psicometristi sono l’Analisi Fattoriale Confermativa (CFA) e i Modelli di Equazioni Strutturali (SEM).\nL’Analisi Fattoriale Confermativa (CFA) rappresenta un approccio metodologico rigoroso, basato sull’ipotesi che un insieme di osservazioni possa essere spiegato da pochi costrutti latenti. A differenza dell’Analisi Fattoriale Esplorativa, che non prevede ipotesi a priori sui fattori, la CFA richiede che i ricercatori definiscano anticipatamente un modello teorico. Questo specifica le relazioni tra le variabili osservabili e i costrutti latenti, permettendo di testare l’adeguatezza del modello ai dati. La capacità della CFA di confrontare diversi modelli offre un mezzo potente per identificare la struttura che meglio rappresenta i dati.\nNel contesto della valutazione della coerenza interna di una scala, l’utilizzo della CFA supera i limiti dei metodi basati sulla teoria classica dei test, fornendo una valutazione più dettagliata e strutturata delle relazioni tra item e costrutti latenti.\nI Modelli di Equazioni Strutturali (SEM) estendono le possibilità offerte dalla CFA, abilitando l’analisi delle relazioni di regressione non solo tra variabili manifeste e latenti, ma anche tra i costrutti latenti stessi. Questa caratteristica rende i SEM strumenti eccezionalmente potenti per esplorare le interazioni complesse tra variabili in uno studio psicometrico.\nL’esame della dimensionalità di un costrutto attraverso la CFA e i SEM consente di testare con precisione le ipotesi sulla struttura dimensionale dei costrutti, verificando se l’organizzazione teorizzata degli item in fattori latenti corrisponde ai dati. Questi strumenti sono quindi fondamentali per confermare la struttura di un costrutto come ipotizzato dalla teoria sottostante.\nIn aggiunta, l’approccio Multitrait-Multimethod (MTMM) per esaminare la validità esterna, incorporando la validità convergente e discriminante, arricchisce ulteriormente la comprensione della misura. L’uso del disegno MTMM permette di distinguere efficacemente tra costrutti correlati ma distinti, assicurando che le misure non solo riflettano accuratamente il costrutto target, ma siano anche discriminanti rispetto ad altri costrutti.\nIn sintesi, l’integrazione di CFA e SEM nel processo di validazione di costrutti e nella costruzione di scale psicometriche rappresenta un avanzamento metodologico significativo. Questi approcci non solo migliorano la precisione e la comprensione delle relazioni tra variabili osservabili e latenti, ma contribuiscono anche a elevare la qualità e l’affidabilità delle misure psicologiche. Attraverso un uso attento e informato di queste tecniche, i ricercatori possono arricchire la validità e l’utilità delle scale psicometriche. Chi volesse approfondire ulteriormente questi argomenti, può fare riferimento al testo di John e Benet-Martinez (2014).\n\n\n\n\nJohn, Oliver P., e Veronica Benet-Martinez. 2014. «Measurement: Reliability, construct validation, and scale construction». In Handbook of research methods in social and personality psychology, a cura di Harry T. Reis e Charles M. Judd, 2nd ed., 473–503. Cambridge University Press.",
    "crumbs": [
      "Prefazione"
    ]
  },
  {
    "objectID": "99-references.html",
    "href": "99-references.html",
    "title": "Bibliografia",
    "section": "",
    "text": "Allen, Mary J, and Wendy M Yen. 2001. Introduction to Measurement\nTheory. Waveland Press.\n\n\nAlphen, Thijmen van, Suzanne Jak, Joost Jansen in de Wal, Jaap\nSchuitema, and Thea Peetsma. 2022. “Determining Reliability of\nDaily Measures: An Illustration with Data on Teacher Stress.”\nApplied Measurement in Education 35 (1): 63–79.\n\n\nBollen, Kenneth, and Richard Lennox. 1991. “Conventional Wisdom on\nMeasurement: A Structural Equation Perspective.”\nPsychological Bulletin 110 (2): 305–14.\n\n\nBraun, Henry I. 1988. “Understanding Scoring Reliability:\nExperiments in Calibrating Essay Readers.” Journal of\nEducational Statistics 13 (1): 1–18.\n\n\nBrown, Timothy A. 2015. Confirmatory Factor Analysis for Applied\nResearch. Guilford publications.\n\n\nDebelak, Rudolf, Carolin Strobl, and Matthew D Zeigenfuse. 2022. An\nIntroduction to the Rasch Model with Examples in r. Crc Press.\n\n\nHirsch, Micah E, Austin Thompson, Yunjung Kim, and Kaitlin L Lansford.\n2022. “The Reliability and Validity of Speech-Language\nPathologists’ Estimations of Intelligibility in Dysarthria.”\nBrain Sciences 12 (8): 1011.\n\n\nHu, Li-tze, and Peter M Bentler. 1998. “Fit Indices in Covariance\nStructure Modeling: Sensitivity to Underparameterized Model\nMisspecification.” Psychological Methods 3 (4):\n424--453.\n\n\nJohn, Oliver P., and Veronica Benet-Martinez. 2014. “Measurement:\nReliability, Construct Validation, and Scale Construction.” In\nHandbook of Research Methods in Social and Personality\nPsychology, edited by Harry T. Reis and Charles M. Judd, 2nd ed.,\n473–503. Cambridge University Press.\n\n\nKan, Kees-Jan, Han LJ van der Maas, and Stephen Z Levine. 2019.\n“Extending Psychometric Network Analysis: Empirical Evidence\nAgainst g in Favor of Mutualism?” Intelligence 73:\n52–62.\n\n\nKline, Paul. 2013. Handbook of Psychological Testing.\nRoutledge.\n\n\nKline, Rex B. 2023. Principles and Practice of Structural Equation\nModeling. Guilford publications.\n\n\nLord, Frederic M, and Melvin R Novick. 1968. Statistical Theories of\nMental Test Scores. Addison-Wesley.\n\n\nMcDonald, Roderick P. 2013. Test Theory: A Unified Treatment.\nPsychology Press.\n\n\nNunnally, Jum C. 1994. Psychometric Theory. McGraw-Hill.\n\n\nPetersen, Isaac T. 2024. Principles of Psychological Assessment:\nWith Applied Examples in r. CRC Press.\n\n\nRencher, AC. 2002. Methods of Multivariate Analysis. 2002.\nWiley Publications.\n\n\nRosseel, Yves. 2020. “Small Sample Solutions for Structural\nEquation Modeling.” In Small Sample Size Solutions: A Guide\nfor Applied Researchers and Practitioners, 226–38. Routledge.\n\n\nSpearman, C. 1904. “General Intelligence Objectively Determined\nand Measured.” American Journal of Psychology 15:\n201–93.\n\n\nWu, Hao, and Ryne Estabrook. 2016. “Identification of Confirmatory\nFactor Analysis Models of Different Levels of Invariance for Ordered\nCategorical Outcomes.” Psychometrika 81 (4): 1014–45.",
    "crumbs": [
      "Bibliografia"
    ]
  },
  {
    "objectID": "chapters/irt/02_rasch_model.html#modelli-irt-e-curve-caratteristiche-degli-item-icc",
    "href": "chapters/irt/02_rasch_model.html#modelli-irt-e-curve-caratteristiche-degli-item-icc",
    "title": "56  Modello di Rasch",
    "section": "56.2 Modelli IRT e Curve Caratteristiche degli Item (ICC)",
    "text": "56.2 Modelli IRT e Curve Caratteristiche degli Item (ICC)\nLa Teoria Classica dei Test (CTT) fornisce una metodologia per stimare la relazione tra le misure (o item) e il costrutto psicologico sottostante. Ad esempio, con un approccio CTT, è possibile calcolare la correlazione tra il punteggio di un item e il punteggio totale del test (es. punteggio sommatorio). Questo approccio, pur utile, fornisce solo una stima approssimativa della relazione tra un item e il costrutto. Esistono però altri modi per caratterizzare questa relazione, come la Teoria della Risposta all’Item (IRT).\nA differenza della CTT, che si focalizza sulla relazione tra il punteggio totale del test e il costrutto, l’IRT si concentra su come un singolo item si relaziona con il costrutto. Per esempio, dato il livello di un individuo su un costrutto, qual è la probabilità che risponda “VERO” a un particolare item?\nL’IRT è un approccio alla modellizzazione delle variabili latenti. In questo modello, il livello di una persona sul costrutto (chiamato theta (θ)) viene stimato in base alle sue risposte agli item. Theta rappresenta la capacità latente o il livello di abilità su un continuum latente.\nUna delle rappresentazioni principali nell’IRT è la Curva Caratteristica dell’Item (ICC). L’ICC è un grafico che mostra la probabilità stimata di una risposta corretta (o della presenza di un sintomo) in funzione del livello di abilità di una persona su un continuum latente. Questo tipo di curva è generalmente modellato con una funzione logistica, che produce una curva sigmoide.\nLe ICC possono essere utilizzate per valutare l’utilità diagnostica di un item. Ad esempio, un item che presenta un effetto soffitto (tutti rispondono correttamente, indipendentemente dal livello di abilità) o un effetto pavimento (nessuno risponde correttamente se non a livelli molto alti di abilità) fornisce poche informazioni diagnostiche.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/02_development.html",
    "href": "chapters/measurement/02_development.html",
    "title": "5  Sviluppo dello strumento",
    "section": "",
    "text": "5.1 Introduzione\nLo sviluppo di un buon test psicologico non è semplice come potrebbe sembrare a prima vista. Si tratta di un processo articolato in più fasi, che richiede generalmente un notevole investimento di tempo, ricerca e, aspetto fondamentale, la disponibilità di partecipanti disposti a sottoporsi al test. Questo capitolo offre una panoramica del processo di sviluppo del test distinguendo quattro fasi principali: la concettualizzazione del test, la definizione della sua struttura e formato, la pianificazione delle standardizzazioni e degli studi psicometrici, e l’implementazione del piano (Reynolds e Livingston 2021).",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sviluppo dello strumento</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/02_development.html#fasi-di-sviluppo",
    "href": "chapters/measurement/02_development.html#fasi-di-sviluppo",
    "title": "5  Sviluppo dello strumento",
    "section": "5.2 Fasi di sviluppo",
    "text": "5.2 Fasi di sviluppo\n\n5.2.1 Identificazione del costrutto\nLo sviluppo di un test psicologico o educativo è un processo che inizia con la chiara identificazione di una necessità specifica all’interno del campo. È imperativo che lo sviluppatore del test determini il costrutto che desidera misurare e dimostri la necessità di un nuovo metodo di misurazione. Con l’ampia varietà di test psicologici disponibili, diventa cruciale identificare una lacuna specifica o un bisogno non ancora soddisfatto.\nPrima di procedere con la creazione di un nuovo strumento di valutazione, è vitale condurre un’indagine approfondita per stabilire se esistano già misure valide e affidabili per il costrutto di interesse. Questo implica un’analisi meticolosa della letteratura scientifica e una revisione dei test psicometrici esistenti. Un tale approccio permette di scoprire se strumenti adeguati siano già disponibili, evitando così di duplicare inutilmente il lavoro già fatto. Inoltre, l’esistenza di misure preesistenti può servire da prezioso punto di riferimento per confrontare e validare il nuovo test in fase di sviluppo, assicurando che il nuovo strumento apporti un contributo significativo e unico al campo della psicologia e dell’educazione.\nCon il progredire della psicologia, nuovi costrutti vengono definiti e quelli esistenti vengono modificati. Ad esempio, la concezione dell’intelligenza è cambiata nel tempo, passando dalla misurazione del tempo di reazione e dell’acume sensoriale, all’enfasi sulla conoscenza, fino all’attuale enfasi sul problem solving in termini di intelligenza cristallizzata e fluida.\nIn alcuni casi, un clinico o ricercatore potrebbe avere la necessità di misurare una variabile ben definita, ma i test esistenti potrebbero essere di qualità dubbia o con qualità psicometriche obsolete. In tali situazioni, potrebbe essere necessario sviluppare un metodo di misurazione migliore o più esatto. Per esempio, le misurazioni del tempo di reazione, un tempo effettuate con l’osservazione umana e il cronometraggio, sono state sostituite da metodi elettronici molto più precisi.\nTuttavia, alcuni test possono valutare costrutti clinicamente utili ma essere impraticabili per l’applicazione clinica reale. Per esempio, la ricerca di sensazioni è stata misurata in modo lungo e complicato fino agli anni ’90, quando sono state sviluppate misurazioni rapide e affidabili che hanno reso il costrutto pratico per l’applicazione clinica.\nInoltre, i campioni standardizzati usati nei test possono diventare obsoleti e inapplicabili ai soggetti attuali, creando opportunità per lo sviluppo di nuovi test se persiste la necessità di misurare i costrutti in questione.\nNuovi costrutti vengono talvolta definiti anche nei campi della psicologia e dell’educazione. Questi costrutti, di solito derivati da osservazioni teoriche, devono essere studiati e manipolati per essere compresi, e ciò richiede regole per assegnare valori numerici alle osservazioni, ovvero la definizione stessa di un test. Di conseguenza, c’è sempre bisogno di nuovi strumenti di test, sia per modernizzare quelli esistenti sia per misurare nuovi costrutti. Gli sviluppatori e i ricercatori di test possono condurre ricerche letterarie dettagliate per determinare quali strumenti sono disponibili per valutare un costrutto e valutarne la qualità e l’applicabilità prima di stabilire un reale bisogno.\nIn conclusione, la decisione di sviluppare un nuovo test dovrebbe basarsi sulla domanda se questo migliorerebbe la pratica o la ricerca in un determinato campo, o se potrebbe migliorare la condizione umana o la nostra comprensione di essa.\nEsempio di Studio: Nello studio di Watson et al. (2007), una notevole parte dell’introduzione è dedicata all’analisi critica della letteratura esistente. Questa analisi mira a evidenziare i limiti degli strumenti di misurazione disponibili, esaminando le caratteristiche degli item utilizzati, la correlazione tra gli indicatori impiegati dagli strumenti esistenti e gli approcci teorici relativi alla depressione e all’ansia, nonché le soluzioni fattoriali risultanti dai dati raccolti attraverso tali strumenti. Viene inoltre considerata l’insufficiente esplorazione di alcune aree del costrutto da parte degli strumenti esistenti.\nWatson et al. (2007) si sono proposti di sviluppare uno strumento per la misurazione della depressione che superi i limiti di strumenti preesistenti come il Beck Depression Inventory-II (BDI-II; Beck, Steer, & Brown, 1996) e il Center for Epidemiological Studies Depression Scale (CES-D; Radloff, 1977). La scala sviluppata prende il nome di Inventory of Depression and Anxiety Symptoms (IDAS).\nPer rispondere alla prima questione, gli autori sottolineano che gli strumenti esistenti includono contenuti non specifici o non direttamente legati alla depressione. Sia il BDI-II sia il CES-D, per esempio, contengono item relativi a vari tipi di ansia, compromettendo così la loro validità discriminante. Gli strumenti esistenti non coprono inoltre l’intero dominio del costrutto della depressione maggiore come definito dal Diagnostic and Statistical Manual of Mental Disorders (4ª edizione). Presentano inoltre il limite di produrre un unico punteggio di severità dei sintomi, ignorando così l’eterogeneità e la multidimensionalità del fenomeno depressivo. Ciò si riflette nella struttura fattoriale poco chiara di tali strumenti, con diverse soluzioni fattoriali trovate da vari autori. L’IDAS, sviluppato da Watson et al. (2007), mira a superare queste difficoltà, creando sottoscale che riflettano direttamente gli aspetti distintivi della depressione.\nPer rispondere alla seconda questione, gli autori evidenziano come la depressione sia collocata all’interno di una rete nomologica di costrutti che include primariamente l’ansia. A differenza di strumenti preesistenti come BDI-II e CES-D, l’IDAS è stato progettato esplicitamente per creare scale che riflettano aspetti specifici della depressione, distinti dall’ansia. Questo è stato realizzato considerando un’ampia gamma di item che rappresentano sintomi associati all’ansia, al fine di esplorare la relazione tra i sintomi di ansia e quelli della depressione e creare scale distinte per queste dimensioni, aumentando così la validità discriminante dello strumento.\nInfine, per rispondere alla terza questione, Watson et al. (2007) dichiarano l’intento di sviluppare uno strumento che, nel suo punteggio complessivo, rifletta le caratteristiche generali della depressione e che, nelle sue sottoscale, misuri con precisione le varie dimensioni del costrutto esaminato.\n\n\n5.2.2 Obiettivo\nDopo aver identificato la necessità di un nuovo test per misurare un costrutto, è importante descrivere l’obiettivo primario nello sviluppo di una scala psicologica, gli usi previsti e le possibili interpretazioni dei risultati. Ci si dovrebbe chiedere: in quali contesti e per quali scopi verrà impiegato questo strumento? Quali interpretazioni dei risultati sono previste una volta che il test è stato somministrato e i risultati sono stati raccolti?\nLa risposta a queste domande dovrebbe scaturire logicamente dal passo precedente. Se risulta difficile rispondere, ciò potrebbe indicare che la concezione del test e dei costrutti da misurare è ancora troppo vaga. In tal caso, sarebbe opportuno ritornare al primo passo e sviluppare ulteriormente le idee e il concetto del test.\nComprendere come e in quale contesto un test può essere utilizzato è fondamentale per molti aspetti del suo sviluppo. Ad esempio, esistono numerosi test di personalità, ma ciascuno tende a enfatizzare aspetti diversi della personalità, delle emozioni e dell’affettività. Alcuni si concentrano sulla personalità normale, altri sugli stati psicopatologici. Il contesto (ad esempio, un ospedale psichiatrico rispetto alla selezione del personale) e l’uso previsto dei risultati determineranno contenuti e schemi interpretativi differenti.\nConoscere lo scopo del test e il contesto in cui verrà utilizzato influenza quindi la risposta a tutti gli altri fattori nel processo di sviluppo, dalla definizione dell’utente del test, al campione normativo appropriato, fino ai tipi di studi di validità necessari per convalidare le interpretazioni proposte dei punteggi.\n\n\n5.2.3 Utenti\nNello sviluppo di un test, è cruciale determinare chi lo utilizzerà e per quale motivo. I test dovrebbero essere progettati tenendo conto degli utenti specifici, ovvero individui che svolgono funzioni particolari facilitati dall’uso di test psicologici ed educativi. È importante considerare il tipo di formazione accademica formale e le esperienze supervisionate che potrebbero essere richieste agli utenti per applicare correttamente i risultati dei test. Ad esempio, per la maggior parte dei test psicometrici, esistono requisiti di formazione imposti dalle leggi che limitano l’uso dei test agli psicologi. Tuttavia, è anche possibile pensare di sviluppare un test che non sarà somministrato da psicologi.\nÈ anche utile, in questa fase, determinare quali individui in quali contesti troveranno il test proposto utile nel loro ruolo. Questo dovrebbe derivare direttamente dallo scopo del test e dalle interpretazioni proposte dei risultati. Ad esempio, se lo scopo del test è diagnosticare una condizione clinica come il disturbo bipolare pediatrico, l’utente target sarà probabilmente uno psicologo clinico, o forse un psichiatra. Tuttavia, un test progettato per esaminare un gran numero di bambini per verificare se presentano livelli di rischio elevati per disturbi emotivi e comportamentali potrebbe essere concepito in modo da poter essere somministrato e valutato da insegnanti o infermieri.\nLa conoscenza dell’utente previsto influenzerà le caratteristiche del test, in particolare riguardo alla complessità della sua somministrazione, valutazione e interpretazione, poiché diverse categorie di utenti possiedono diversi livelli di competenza.\n\n\n5.2.4 4. Definizione concettuale e operativa\nLa creazione di un test psicologico efficace inizia con l’identificazione e la definizione precisa del costrutto o della caratteristica psicologica che si intende misurare. Questo processo non è banale: spesso crediamo di comprendere pienamente costrutti come depressione, ansia, intelligenza cristallizzata, intelligenza fluida, aggressività, amabilità, fino a quando non tentiamo di esprimerli a parole. È in questo momento che possiamo renderci conto che la nostra comprensione del costrutto potrebbe non essere così chiara come inizialmente pensavamo.\nSi raccomanda di scrivere due tipi di definizioni: una concettuale e una operativa. Una definizione concettuale spiega il costrutto a livello teorico. Per esempio, una possibile definizione concettuale della depressione potrebbe essere: “La depressione è uno stato di malinconia, tristezza e bassi livelli di energia che porta all’anedonia, sentimenti di inutilità e stanchezza cronica.” Una definizione operativa, invece, dice esattamente come il nostro test definirà o misurerà il costrutto. Ad esempio: “Nella Scala di Valutazione della Depressione degli Studenti, la depressione sarà valutata sommando le valutazioni in direzione punteggiata su osservazioni di comportamento come espressioni di sentimenti di tristezza, sentimenti di solitudine, di sentirsi incompresi e non apprezzati, mancanza di coinvolgimento in attività piacevoli, troppo o troppo poco sonno, pianto in momenti inappropriati e lamentele di stanchezza.” Quindi, mentre la definizione concettuale ci dice ciò che vogliamo misurare in astratto, la definizione operativa ci informa in modo più diretto e specifico su come sarà derivato il punteggio del costrutto. Anche se questi sforzi possono sembrare tediosi per scale che hanno molti costrutti, più costrutti sono presenti in un test, più tali definizioni si rivelano utili.\nEsempio: Nel loro studio sulla depressione, Watson et al. (2007) si riferiscono al DSM-IV, che elenca nove criteri sintomatici per diagnosticare un episodio depressivo maggiore. Questi includono: (1) umore depresso per la maggior parte del giorno; (2) notevole riduzione di interesse o piacere per quasi tutte le attività; (3) variazione significativa del peso o dell’appetito; (4) insonnia o ipersonnia; (5) agitazione o rallentamento psicomotorio; (6) affaticamento o perdita di energia; (7) sentimenti di inutilità o eccessiva colpa; (8) difficoltà di concentrazione o indecisione; (9) pensieri di morte o ideazione suicidaria.\nPer ottimizzare l’utilità dell’Inventory of Depression and Anxiety Symptoms (IDAS), Watson et al. (2007) hanno incluso diversi item per ciascuno di questi nove criteri. Con l’intento di garantire una rappresentazione adeguata di ciascuna potenziale dimensione del costrutto, gli autori hanno inizialmente organizzato gli item in gruppi denominati homogeneous item composites (HIC). Tuttavia, hanno precisato che la formazione di questi HIC non implica necessariamente l’emergere di un fattore corrispondente, ma serve piuttosto a coprire l’intero dominio potenziale del costrutto di depressione.\n\n\n5.2.5 Dissimulazione\nDeterminare la necessità di misure per rilevare la dissimulazione. La dissimulazione è il comportamento di presentare se stessi in modo diverso dalla realtà. Per esempio, una persona che si sente triste ma non vuole ammetterlo potrebbe rispondere falsamente a una domanda di un test di personalità riguardo la tristezza per nascondere i propri sentimenti. La dissimulazione può verificarsi anche quando si valutano altre persone, ad esempio compilando una scala di valutazione per un bambino, coniuge o genitore anziano.\nLe persone possono impegnarsi nella dissimulazione per vari motivi, come negare sintomi in una valutazione di personalità o psicopatologia per non apparire con tratti indesiderati o comportamenti inaccettabili. Questo fenomeno si verifica anche tra coloro che cercano trattamento. Nell’ambito lavorativo, non è insolito che i candidati rispondano alle domande in modo da aumentare le possibilità di ottenere il lavoro.\nIn altri casi, le persone possono esagerare i sintomi per apparire più compromessi di quanto siano in realtà, un comportamento noto come simulazione, spesso motivato dal guadagno personale. Ad esempio, possono fingere problemi di personalità, comportamentali o cognitivi per ottenere benefici di invalidità, aumentare i risarcimenti in cause legali o evitare punizioni.\nNelle valutazioni cognitive, la dissimulazione può essere rilevata attraverso test di sforzo, ossia test che chiunque può completare correttamente se ci prova. Questi test sono raccomandati in quasi tutte le situazioni forensi e quando un paziente ha qualcosa da guadagnare fingendo deficit cognitivi.\nLa dissimulazione può manifestarsi in molti modi e talvolta si vedono definizioni più specifiche. Ad esempio, la validità dei sintomi si riferisce all’accuratezza o veridicità della presentazione comportamentale del soggetto, mentre il bias di risposta è un tentativo di ingannare l’esaminatore con risposte inesatte o incomplete. L’effort riguarda l’impegno nell’eseguire al meglio delle proprie capacità.\nCapire lo scopo del test, chi lo utilizzerà e in quali circostanze aiuta a determinare se includere scale per rilevare la dissimulazione. La psicologia ha sviluppato nel tempo metodi sofisticati per rilevare la dissimulazione, e le tecniche comuni verranno ora esaminate.\nLe scale più comuni per rilevare la dissimulazione (spesso note come scale di validità) nelle misure di personalità e comportamento includono le scale F, le scale L e gli indici di inconsistenza. Queste sono brevemente descritte qui sotto:\n\nScale F (Infrequency Scales): Queste scale sono progettate per rilevare la presentazione esagerata dei sintomi. Gli item delle scale F riflettono sintomi raramente segnalati anche da persone con livelli significativi di psicopatologia, o rappresentano approvazioni estreme di sintomi comuni che sono anch’essi rari. Poiché questi item rappresentano risposte infrequenti e scarsamente correlate tra loro, i soggetti che indicano di sperimentare un gran numero di questi sintomi forniscono probabilmente una presentazione esagerata della psicopatologia.\nScale L (Social Desirability Scales): Sono progettate per rilevare la negazione inaccurata di sintomi realmente presenti, rilevando il bias opposto della scala F. Le scale L includono item speciali che riflettono piccoli difetti comuni che quasi tutti sperimentano in qualche momento. Ad esempio, una risposta negativa a un item come “Talvolta mi sento triste” può indicare dissimulazione.\nScale di Inconsistenza: Sono progettate per rilevare incongruenze nelle risposte a item simili su scale di personalità e comportamento. Quando un rispondente non è coerente nel rispondere a item simili, i risultati non sono considerati affidabili. Per esempio, un rispondente che afferma sia “Sono pieno di energia” sia “Mi sento affaticato” potrebbe essere incoerente.\n\nPer alcuni tipi di test, possono essere utili altre scale di dissimulazione per scopi speciali. Ad esempio, nello sviluppo di misure per la selezione di personale, potrebbe essere utile includere indicatori più avanzati e sottili di bias di risposta.\nNonostante non siano misure di dissimulazione, gli autori dei test a volte includono anche scale che esaminano la comprensione degli item e il livello di cooperazione del soggetto nei test auto-somministrati e nelle scale di valutazione. Queste scale, a volte chiamate V-scale o scale di validità, contengono tipicamente item privi di senso dove la risposta è la stessa per tutti coloro che prendono il test seriamente e cooperano con il processo di esame.\nNelle valutazioni attitudinali e nei test di profitto, alcune persone potrebbero non impegnarsi adeguatamente nei test per ragioni di guadagno, un comportamento comunemente visto come simulazione. Tuttavia, ci sono altre ragioni plausibili, come la sindrome amotivazionale a seguito di infortuni cerebrali o la avolizione in disturbi psichiatrici come la schizofrenia.\nPochi test cognitivi hanno misure incorporate di dissimulazione o mancanza di sforzo. Tuttavia, alcuni costruiscono scale o item per rilevare la mancanza di sforzo. Spesso, la simulazione su misure cognitive è valutata attraverso modelli di prestazione indicativi di risposte non valide, incongruenze tra risultati del test e comportamenti osservati, e discrepanze tra i risultati dei test e le informazioni di background documentate.\nIn conclusione, le scale di dissimulazione sono strumenti essenziali nel processo di sviluppo dei test, aiutando a garantire che i risultati riflettano accuratamente le caratteristiche e le abilità del soggetto.\n\n\n5.2.6 Formato degli Item\nLa determinazione del formato degli item è un passo cruciale nello sviluppo di un test psicologico o educativo. Questa fase comporta la scelta della struttura e del formato degli item, i quali devono essere in linea con il costrutto da misurare e gli obiettivi specifici del test. I formati possono variare, includendo domande a scelta multipla, scale Likert o altri tipi di risposta. È importante sottolineare che la scelta del formato influisce sull’accuratezza e sull’affidabilità del test, richiedendo quindi un’attenta valutazione.\nIn questa fase, si devono considerare anche aspetti pratici come la modalità di somministrazione del test, che può essere individuale o di gruppo, e il formato del test, che può essere cartaceo o computerizzato. Un’ulteriore considerazione riguarda chi completerà effettivamente il test o il foglio delle risposte, che potrebbe essere l’esaminatore, l’esaminando o un informatore terzo, come nel caso delle scale di valutazione comportamentale.\nDifferenti tipi di item sono utili in diverse circostanze e possono misurare caratteristiche in modi diversi. Per esempio, la valutazione di sentimenti, pensieri e altri comportamenti non osservabili è solitamente meglio realizzata tramite autovalutazione. Una volta selezionato il formato di autovalutazione, è importante decidere il tipo di item da utilizzare, poiché esistono diverse opzioni di item per l’autovalutazione. Ad esempio, l’item “Mi sento triste” può variare significativamente a seconda del formato di risposta scelto, influenzando l’intento e l’interpretazione delle risposte.\nDopo aver scelto i formati degli item appropriati, è fondamentale scrivere esempi di item per ciascun formato che si prevede di utilizzare nel test. Questo include anche la redazione delle istruzioni per la somministrazione e la valutazione del test, che dovrebbero essere chiare e comprensibili anche per chi non è familiare con il test.\nInfine, è utile stimare il numero di item necessari per valutare in modo affidabile i costrutti. Come regola generale, si dovrebbe inizialmente scrivere almeno il doppio del numero di item che si prevede di utilizzare nel test finale. Questo perché molti item potrebbero essere scartati a causa di statistiche insufficienti, pregiudizi o ambiguità. La lunghezza del test dovrebbe essere adeguata alla popolazione bersaglio, tenendo conto della sensibilità di alcuni gruppi, come i giovani o gli anziani, al tempo richiesto per completare il test.\nUn altro aspetto fondamentale da considerare è chi completerà effettivamente il test o il foglio delle risposte. Questo può variare a seconda delle circostanze: potrebbe essere l’esaminatore, l’esaminato, o un terzo informatore, come nel caso delle scale di valutazione comportamentale, dove i rispondenti possono essere genitori, insegnanti o altre figure rilevanti. La scelta di chi completerà il test può influenzare non solo la logistica della somministrazione del test, ma anche la natura delle informazioni raccolte, e quindi la validità e l’utilità dei risultati.\nScala Likert. Una scala Likert è un tipo di scala ordinale che viene utilizzata per misurare gli atteggiamenti di una persona. Viene chiesto al rispondente di valutare il grado di accordo o disaccordo con un’affermazione utilizzando un’alternativa di risposta che di solito varia da cinque a sette punti. Tuttavia, poiché è una scala ordinale, le distanze tra i livelli della scala non sono quantificabili e non possiamo assumere che le differenze tra i livelli di risposta siano equidistanti. Pertanto, c’è una lunga controversia sulla possibilità di trattare i valori numerici di una scala ordinale come se provenissero da una scala ad intervalli. Alcuni autori ritengono problematico non potere trattare i dati provenienti da scale di tipo Likert come se fossero a livello di scala ad intervalli, mentre altri autori lo considerano giustificato in presenza di un’ampia numerosità campionaria e di una distribuzione approssimativamente normale dei dati. In ogni caso, la procedura che sta alla base delle scale Likert consiste nella somma dei punti attribuiti ad ogni singola domanda. I vantaggi della scala Likert sono la sua semplicità e applicabilità, mentre i suoi svantaggi sono il fatto che i suoi elementi vengono trattati come scale cardinali pur essendo ordinali e il fatto che il punteggio finale non rappresenta una variabile cardinale.\nItem a codifica inversa. In parole più semplici, ci sono alcune domande in un test che sono strettamente correlate in modo negativo con le altre domande e con il punteggio totale del test. Queste domande richiedono una risposta diversa rispetto alle altre domande. Ad esempio, in un questionario sull’ansia, una domanda potrebbe chiedere “Sono preoccupata” e la scala di risposta potrebbe essere “Per nulla”, “Un po’”, “Abbastanza” e “Moltissimo” con valori 1, 2, 3 e 4 rispettivamente. Tuttavia, un’altra domanda potrebbe chiedere “Mi sento bene” e la scala di risposta potrebbe essere la stessa, ma con valori 4, 3, 2 e 1 rispettivamente. Questo perché le proprietà contrarie si trovano sullo stesso continuum latente. Questo è importante nella costruzione di un test psicologico, dove è consigliato utilizzare sia domande orientate nella direzione del costrutto (chiamate “straight item”) sia nella direzione opposta (chiamate “reverse item”) per contrastare l’acquiescenza e ottenere risposte più accurate.\n\n\n5.2.7 Sviluppare una struttura del test\nLo sviluppo di una struttura per un test psicologico o educativo richiede passaggi ben definiti prima di procedere alla creazione dei singoli item della scala. Prima di tutto, è essenziale stabilire un’organizzazione strutturale del test, delineando gli obiettivi specifici che si intendono raggiungere attraverso la sua somministrazione. Questo passaggio comporta la definizione dei sottodomini o delle dimensioni che si vogliono misurare e di come questi contribuiranno alla valutazione complessiva del costrutto.\nUna questione fondamentale da risolvere è se il test produrrà un unico punteggio, come somma di tutte le risposte agli item, o se sarà necessario suddividerlo in sottoscale. La struttura del test è spesso determinata dai costrutti che si intendono misurare, quindi le definizioni precedentemente scritte sono di grande importanza in questa fase decisionale. Se il test include sottoscale o sotto-test, è importante considerare anche se ci saranno punteggi compositi che forniscono indici riassuntivi dei raggruppamenti dei sotto-test.\nNella fase di sviluppo del test, è cruciale spiegare come gli item e le sottoscale (se presenti) saranno organizzati. Se la logica alla base del test e la discussione sui costrutti sottostanti non rendono evidente la specifica organizzazione degli item e dei sotto-test, è importante affrontare il motivo per cui questa particolare organizzazione è la più appropriata. Naturalmente, è da tenere in considerazione che la ricerca condotta durante il processo di sviluppo del test potrebbe portare a modifiche della struttura intesa, in base ai dati effettivamente raccolti.\nEsempio: Nello studio di Watson et al. (2007), per garantire un campionamento esaustivo dell’intero ambito del costrutto, sono stati definiti 20 homogeneous item composites (HIC), che includono: Umore Depresso, Perdita di Interesse o Piacere, Disturbi dell’Appetito, Disturbi del Sonno, Problemi Psicomotori, Fatica/Anergia, Sentimenti di Inutilità o Colpa, Problemi Cognitivi, Ideazione Suicidaria, Senso di Speranza, Depressione Melanconica, Umore Arrabbiato/Irritabile, Alta Energia/Affetto Positivo, Umore Ansioso, Preoccupazione, Panico, Agorafobia, Ansia Sociale, Intrusioni Traumatiche, Sintomi Ossessivo-Compulsivi.\nDi questi, 13 HIC (comprendenti in totale 117 item) sono stati dedicati agli indicatori specifici della depressione. Nove di questi HIC (79 item in totale) coprono i sintomi fondamentali della depressione maggiore secondo il DSM-IV, tra cui umore depresso, perdita di interesse o piacere, disturbi dell’appetito, del sonno, problemi psicomotori, fatica/anergia, senso di inutilità e colpa, problemi cognitivi e ideazione suicidaria. I restanti quattro HIC trattano temi quali la disperazione (Hopelessness, secondo Abramson, Metalsky, & Alloy, 1989), sintomi specifici della depressione melanconica (Joiner et al., 2005), umore arrabbiato/irritabile (una forma alternativa di depressione negli adolescenti secondo il DSM-IV, American Psychiatric Association, 1994, p. 327), e indicatori di energia e affetto positivo (associati alla depressione secondo Mineka et al., 1998).\nI rimanenti sette HIC (63 item totali) sono stati introdotti per valutare sintomi associati all’ansia. Questi HIC includono categorie come umore ansioso, preoccupazione, panico, agorafobia, ansia sociale e intrusioni traumatiche associate al PTSD. Questa approfondita categorizzazione mira a una valutazione comprensiva e differenziata dei sintomi legati sia alla depressione sia all’ansia, fornendo così una visione più completa del panorama psicopatologico.\n\n\n5.2.8 Tabella delle Specifiche\nLa creazione di un test psicologico o educativo implica lo sviluppo di una Tabella delle Specifiche (TOS), che funge da “schema” per assicurare l’allineamento tra le definizioni dei costrutti, le definizioni operative e il contenuto del test stesso. La TOS è particolarmente utile nelle misure di rendimento, dove serve a garantire che il test sia in congruenza con un determinato curriculum o campo di studio. Questo strumento definisce in modo chiaro le aree di contenuto principali che il test intende esplorare, selezionate attraverso un’analisi accurata degli obiettivi educativi. Questo processo è fondamentale per assicurare che il contenuto del test sia direttamente collegato agli obiettivi di apprendimento e di valutazione, rendendo il test pertinente e focalizzato sui temi chiave.\nÈ importante che gli autori di test evitino di affidarsi eccessivamente a processi cognitivi di livello inferiore, come la memorizzazione meccanica, e che includano anche processi di livello superiore per garantire una valutazione equilibrata e completa delle capacità cognitive. Inoltre, la distribuzione degli item tra le varie aree di contenuto dovrebbe essere bilanciata per riflettere adeguatamente l’importanza di ciascuna area nel quadro complessivo del curriculum o dell’area di studio.\nLe TOS non sono utili solo per test di rendimento e attitudine, ma anche per quelli di personalità e comportamento. Ad esempio, nello sviluppo di un test sulla depressione, la TOS può aiutare a garantire che il test esamini accuratamente il vasto dominio dei sintomi depressivi, coprendo le diverse sfaccettature della depressione in proporzioni adeguate. La TOS può elencare le diverse aree o aspetti dei sintomi depressivi da valutare, includendo sia comportamenti osservabili sia pensieri e sentimenti interni, rilevanti per la diagnosi. In questo modo, la TOS funge da guida nella redazione degli item, assicurando che tutte le dimensioni rilevanti della depressione siano adeguatamente rappresentate nel test.\n\n\n5.2.9 Pool Iniziale degli Item\nCreare il pool iniziale di item. A questo punto, si procede a sviluppare una vasta gamma di item che coprano i diversi aspetti del costrutto di interesse. Questo pool iniziale di item dovrebbe essere variegato e ben bilanciato, rappresentando in modo adeguato la complessità del costrutto e i diversi livelli di abilità o atteggiamenti che si vogliono misurare. È importante anche scrivere le risposte corrette secondo ciò che si intende misurare e la direzione del punteggio del test.\n\n\n5.2.10 Revisione\nCondurre la revisione iniziale degli item (e apportare modifiche). Gli item raccolti nel pool iniziale vengono sottoposti a un esame attento da parte di esperti nel campo. Si valuta la pertinenza, la chiarezza, la coerenza e la validità dei singoli item. Sulla base dei feedback ricevuti dagli esperti, possono essere apportate modifiche o eliminati item problematici.\n\n\n5.2.11 Validazione\nNella fase di validazione di un test, l’implementazione di un ampio trial empirico degli item è un passaggio metodologico fondamentale. Tale processo prevede la somministrazione degli item a un campione rappresentativo della popolazione di riferimento. L’obiettivo è di valutare la discriminazione item-partecipante e di identificare eventuali anomalie o limitazioni strutturali della scala.\nLa definizione del campione target, ovvero la popolazione di riferimento per la quale il test è stato progettato, costituisce il primo passo nella pianificazione del campionamento. La selezione del campione di standardizzazione è cruciale in quanto determina il gruppo di riferimento per il confronto dei punteggi nei test normativi, oltre a stabilire le norme o i punteggi normativi del test. Questo è rilevante sia nei test normativi sia nei test basati su criteri, dove la performance delle popolazioni target è fondamentale nella definizione dei punteggi di taglio e nelle decisioni correlate alle prestazioni.\nUna volta identificata la popolazione target, si procede alla formulazione di un piano di campionamento. Idealmente, si ricercano campioni casuali veri della popolazione target, tuttavia, tale approccio è spesso impraticabile data l’impossibilità di conoscere e coinvolgere tutti i membri della popolazione. È quindi essenziale garantire la rappresentatività del campione attraverso un piano di campionamento stratificato proporzionale. Questo implica la definizione delle caratteristiche salienti (strati) della popolazione e la determinazione delle percentuali necessarie di soggetti con tali caratteristiche per assicurare la rappresentatività del campione.\nIn conclusione, il processo di campionamento deve essere accuratamente progettato per assicurare che il campione sia rappresentativo della popolazione target, superando le limitazioni pratiche e metodologiche incontrate nella selezione del campione stesso.\nNumero di soggetti. In ambito psicometrico non c’è un accordo univoco sulla dimensione del campione necessaria per condurre un’analisi fattoriale. Tuttavia, gli autori hanno fornito alcune indicazioni che possono essere utili come riferimento. Nunnally (1978) ha suggerito che il campione debba essere composto da almeno 10 soggetti per ogni item. Comrey e Lee (1992) hanno fornito una scala che valuta la qualità del campione in base alla dimensione: “molto scarsa” per 50, “scarso” per 100, “sufficiente” per 200, “buona” per 300, “molto buona” per 500 e “eccellente” per 1.000 o più. Altri autori hanno suggerito come regola generale di avere almeno 300 casi per l’analisi fattoriale (Tabachnick e Fidell, 2001). In ogni caso, è importante tenere presente che la scelta della dimensione del campione dipende anche dalla complessità del costrutto che si intende analizzare e dalla qualità degli item utilizzati nel test.\n\n\n5.2.12 Analisi degli item\nI dati raccolti dal test di campo vengono analizzati utilizzando metodi statistici adeguati. Questo processo mira a identificare item che non funzionano correttamente, che mostrano una bassa discriminazione o che potrebbero causare distorsioni nelle risposte. Gli item che superano questa fase sono considerati per la versione finale della scala.\n\n\n5.2.13 Revisione degli item\nSulla base dei risultati dell’analisi, gli item della scala possono essere rivisti o sostituiti, al fine di migliorarne l’accuratezza, la coerenza e l’affidabilità.\nNumero di item. La lunghezza del test dovrebbe essere adatta al suo scopo. Ad esempio, un test per valutare le abilità degli studenti delle scuole primarie non dovrebbe richiedere più di 30 minuti per essere completato, perché l’affaticamento e la noia possono influire sui risultati. Lo stesso vale per un test di personalità per adulti. In generale, un test dovrebbe essere il più breve possibile, ma deve raggiungere un livello accettabile di validità. Come regola generale, Kline (1986) suggerisce di avere almeno 50 domande nella versione finale del test.\n\n\n5.2.14 Calcolo dell’Affidabilità\nLa consistenza interna della scala viene valutata tramite il calcolo dell’affidabilità, ad esempio utilizzando il coefficiente alpha di Cronbach. Questo passaggio assicura che gli item scelti per la scala si correlino tra loro in modo coerente, riflettendo così la coerenza delle misure.\n\n\n5.2.15 Seconda Somministrazione\nUna volta apportate le revisioni agli item, viene eseguita una seconda somministrazione per confermare l’efficacia delle modifiche e per valutare l’affidabilità della versione rivista della scala su un nuovo campione.\n\n\n5.2.16 Ripetere i Passaggi Precedenti\nSe durante la seconda somministrazione emergono ancora problemi o se l’affidabilità della scala non raggiunge i livelli desiderati, è necessario ripetere i passaggi precedenti fino a raggiungere una versione della scala che soddisfi gli standard di qualità e affidabilità.\n\n\n5.2.17 Studi di validazione\nPer garantire la validità di una scala, è fondamentale condurre studi di validazione che dimostrino la capacità della scala di misurare con precisione il costrutto di interesse. Questi studi possono includere l’analisi delle relazioni tra i punteggi della scala e altre misure correlate, nonché il confronto tra gruppi con differenze note nel costrutto.\nCome per gli studi di affidabilità, è cruciale pianificare in anticipo gli studi di validità, assicurandosi che siano concettualizzati e progettati in modo da consentire la valutazione dell’adeguatezza delle interpretazioni proposte dei punteggi del test una volta completato. In linea con quanto discusso nel Capitolo 5, ciascuna delle cinque categorie fondamentali di evidenza di validità deve essere affrontata, pur variando l’enfasi e il livello di dettaglio a seconda dei costrutti valutati, delle interpretazioni proposte dei punteggi e degli scopi per i quali i punteggi saranno applicati.\nAd esempio, nei test di intelligenza progettati per predire il successo accademico, dovrebbero essere sottolineati e resi prioritari gli studi predittivi basati su criteri specifici che rappresentino il successo accademico, quali il tasso di laurea o i punteggi ACT. In maniera simile, i test destinati alla selezione del personale richiedono studi accurati sulla capacità predittiva dei punteggi in relazione al successo lavorativo, il quale dovrebbe essere definito in anticipo dall’ente o dall’individuo che utilizza il test. È importante riconoscere che la definizione di successo lavorativo può variare ampiamente; pertanto, è necessario chiarire e specificare le definizioni dei criteri utilizzati.\nPer una misura della psicopatologia dello sviluppo, ad esempio quella finalizzata a perfezionare l’accuratezza diagnostica nei Disturbi dello Spettro Autistico (ASD), l’accento dovrebbe essere posto sulla capacità dei punteggi del test di discriminare tra gruppi di soggetti diagnosticati (indipendentemente dal test) con ciascuno dei disturbi rilevanti. Gli studi di validità frequentemente si concentrano soltanto sulla distinzione tra gruppi diagnosticati e soggetti non diagnosticati o normodotati; tuttavia, questo approccio di ricerca risulta spesso di limitata utilità, in quanto la maggior parte dei test è in grado di differenziare tra normalità e patologia grazie ad un’alta affidabilità dei punteggi e a campioni di ampia dimensione. La sfida effettiva consiste nel distinguere tra le diverse diagnosi all’interno di un campione specifico, come ad esempio differenziare bambini con ASD da quelli con ADHD o depressione.\nIn definitiva, è fondamentale che gli studi di validità si concentrino sulle interpretazioni proposte dei punteggi e sulle loro applicazioni intenzionali. Non esistono limiti a ciò che può essere progettato come studi di validità; sono limitati solo dalla creatività del ricercatore e dalla loro conoscenza del costrutto e delle teorie pertinenti che incarnano il costrutto misurato.\n\n\n5.2.18 Linee Guida\nNel processo di sviluppo di un test, la fase finale si concentra sull’elaborazione di linee guida dettagliate, fondamentali per garantire una somministrazione appropriata e un’accurata valutazione dei punteggi. Questo passaggio si rivela cruciale per assicurare che il test sia utilizzato nel modo previsto e che i risultati siano interpretati correttamente.\nLe istruzioni per partecipare allo studio devono essere chiare e concise, fornendo un’idea generale degli obiettivi della ricerca e dei trattamenti previsti. I partecipanti devono essere informati dei benefici prevedibili e dei rischi, e della libertà di scegliere di non partecipare. Inoltre, la privacy dei partecipanti è protetta dalla legge sulla protezione dei dati personali e i loro dati verranno raccolti e conservati in forma anonima, tranne che per il nominativo. I partecipanti possono esercitare i propri diritti di protezione dei dati personali e interrompere la partecipazione in qualsiasi momento. Alla fine dello studio, i partecipanti possono ricevere i risultati della ricerca e possono rivolgersi al Comitato Etico dell’Università degli Studi di Firenze per segnalare qualsiasi problema. Prima di partecipare, i partecipanti devono firmare una dichiarazione di consenso informato per accettare di partecipare alla ricerca e di autorizzare il trattamento dei loro dati personali.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sviluppo dello strumento</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/02_development.html#considerazioni-finali",
    "href": "chapters/measurement/02_development.html#considerazioni-finali",
    "title": "5  Sviluppo dello strumento",
    "section": "5.3 Considerazioni Finali",
    "text": "5.3 Considerazioni Finali\nQuesto capitolo ha offerto un quadro pratico, così come discuso da Reynolds e Livingston (2021), riguardante le tappe cruciali e gli elementi fondamentali implicati nel processo di creazione di un test. Una particolare attenzione è stata rivolta alle fasi iniziali di concettualizzazione, vitali per la riuscita del progetto. Tali fasi includono l’identificazione della necessità di sviluppare un nuovo test e la formulazione di definizioni sia concettuali che operative dei costrutti da valutare.\nAbbiamo anche messo in evidenza l’importanza di delineare anticipatamente gli scopi specifici per cui il test è stato progettato, le modalità di interpretazione dei risultati e il pubblico target che ne farà uso. Questo approccio preliminare aiuta a definire chiaramente il contesto e le aspettative relative all’uso del test.\nUn altro aspetto trattato riguarda la stesura di una descrizione esauriente del test, che comprende la creazione di una tabella delle specifiche o di uno schema del test. Questo strumento si rivela essenziale per guidare sistematicamente lo sviluppatore attraverso le varie fasi di creazione del test, assicurando che tutti gli aspetti cruciali siano presi in considerazione.\n\n\n\n\nReynolds, Cecil R, e RA Livingston. 2021. Mastering modern psychological testing. Springer.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sviluppo dello strumento</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/02_development.html#calcolo-dellaffidabilità",
    "href": "chapters/measurement/02_development.html#calcolo-dellaffidabilità",
    "title": "5  Sviluppo dello strumento",
    "section": "5.3 Calcolo dell’Affidabilità",
    "text": "5.3 Calcolo dell’Affidabilità\nLa consistenza interna della scala viene valutata tramite il calcolo dell’affidabilità, ad esempio utilizzando il coefficiente alpha di Cronbach. Questo passaggio assicura che gli item scelti per la scala si correlino tra loro in modo coerente, riflettendo così la coerenza delle misure.\n\n5.3.1 Seconda Somministrazione\nUna volta apportate le revisioni agli item, viene eseguita una seconda somministrazione per confermare l’efficacia delle modifiche e per valutare l’affidabilità della versione rivista della scala su un nuovo campione.\n\n\n5.3.2 Ripetere i Passaggi Precedenti\nSe durante la seconda somministrazione emergono ancora problemi o se l’affidabilità della scala non raggiunge i livelli desiderati, è necessario ripetere i passaggi precedenti fino a raggiungere una versione della scala che soddisfi gli standard di qualità e affidabilità.\n\n\n5.3.3 Studi di validazione\nPer garantire la validità di una scala, è fondamentale condurre studi di validazione che dimostrino la capacità della scala di misurare con precisione il costrutto di interesse. Questi studi possono includere l’analisi delle relazioni tra i punteggi della scala e altre misure correlate, nonché il confronto tra gruppi con differenze note nel costrutto.\nCome per gli studi di affidabilità, è cruciale pianificare in anticipo gli studi di validità, assicurandosi che siano concettualizzati e progettati in modo da consentire la valutazione dell’adeguatezza delle interpretazioni proposte dei punteggi del test una volta completato. In linea con quanto discusso nel Capitolo 5, ciascuna delle cinque categorie fondamentali di evidenza di validità deve essere affrontata, pur variando l’enfasi e il livello di dettaglio a seconda dei costrutti valutati, delle interpretazioni proposte dei punteggi e degli scopi per i quali i punteggi saranno applicati.\nAd esempio, nei test di intelligenza progettati per predire il successo accademico, dovrebbero essere sottolineati e resi prioritari gli studi predittivi basati su criteri specifici che rappresentino il successo accademico, quali il tasso di laurea o i punteggi ACT. In maniera simile, i test destinati alla selezione del personale richiedono studi accurati sulla capacità predittiva dei punteggi in relazione al successo lavorativo, il quale dovrebbe essere definito in anticipo dall’ente o dall’individuo che utilizza il test. È importante riconoscere che la definizione di successo lavorativo può variare ampiamente; pertanto, è necessario chiarire e specificare le definizioni dei criteri utilizzati.\nPer una misura della psicopatologia dello sviluppo, ad esempio quella finalizzata a perfezionare l’accuratezza diagnostica nei Disturbi dello Spettro Autistico (ASD), l’accento dovrebbe essere posto sulla capacità dei punteggi del test di discriminare tra gruppi di soggetti diagnosticati (indipendentemente dal test) con ciascuno dei disturbi rilevanti. Gli studi di validità frequentemente si concentrano soltanto sulla distinzione tra gruppi diagnosticati e soggetti non diagnosticati o normodotati; tuttavia, questo approccio di ricerca risulta spesso di limitata utilità, in quanto la maggior parte dei test è in grado di differenziare tra normalità e patologia grazie ad un’alta affidabilità dei punteggi e a campioni di ampia dimensione. La sfida effettiva consiste nel distinguere tra le diverse diagnosi all’interno di un campione specifico, come ad esempio differenziare bambini con ASD da quelli con ADHD o depressione.\nIn definitiva, è fondamentale che gli studi di validità si concentrino sulle interpretazioni proposte dei punteggi e sulle loro applicazioni intenzionali. Non esistono limiti a ciò che può essere progettato come studi di validità; sono limitati solo dalla creatività del ricercatore e dalla loro conoscenza del costrutto e delle teorie pertinenti che incarnano il costrutto misurato.\n\n\n5.3.4 Linee Guida\nNel processo di sviluppo di un test, la fase finale si concentra sull’elaborazione di linee guida dettagliate, fondamentali per garantire una somministrazione appropriata e un’accurata valutazione dei punteggi. Questo passaggio si rivela cruciale per assicurare che il test sia utilizzato nel modo previsto e che i risultati siano interpretati correttamente.\nLe istruzioni per partecipare allo studio devono essere chiare e concise, fornendo un’idea generale degli obiettivi della ricerca e dei trattamenti previsti. I partecipanti devono essere informati dei benefici prevedibili e dei rischi, e della libertà di scegliere di non partecipare. Inoltre, la privacy dei partecipanti è protetta dalla legge sulla protezione dei dati personali e i loro dati verranno raccolti e conservati in forma anonima, tranne che per il nominativo. I partecipanti possono esercitare i propri diritti di protezione dei dati personali e interrompere la partecipazione in qualsiasi momento. Alla fine dello studio, i partecipanti possono ricevere i risultati della ricerca e possono rivolgersi al Comitato Etico dell’Università degli Studi di Firenze per segnalare qualsiasi problema. Prima di partecipare, i partecipanti devono firmare una dichiarazione di consenso informato per accettare di partecipare alla ricerca e di autorizzare il trattamento dei loro dati personali.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sviluppo dello strumento</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a4_linear_alg.html#matrice-inversa",
    "href": "chapters/appendix/a4_linear_alg.html#matrice-inversa",
    "title": "Appendice D — Elementi di algebra lineare",
    "section": "D.3 Matrice inversa",
    "text": "D.3 Matrice inversa\nL’inversa di una matrice quadrata è un concetto analogo al reciproco di un numero. Tuttavia, a differenza dei numeri, non tutte le matrici hanno un’inversa. Una matrice che non ha inversa è detta singolare.\nSia \\(\\boldsymbol{A}\\) una matrice quadrata di dimensione \\(n\\). La matrice inversa di \\(\\boldsymbol{A}\\), indicata con \\(\\boldsymbol{A}^{-1}\\), è una matrice tale che:\n\\[\n\\boldsymbol{A}\\boldsymbol{A}^{-1}=\\boldsymbol{A}^{-1}\\boldsymbol{A}=\\boldsymbol{I}\n\\]\ndove \\(\\boldsymbol{I}\\) è la matrice identità di dimensione \\(n\\).\nCondizione di esistenza: Una matrice quadrata \\(\\boldsymbol{A}\\) ha un’inversa se e solo se il suo determinante è diverso da zero. Il determinante è una funzione che associa a ogni matrice quadrata un numero.\nInterpretazione geometrica: Geometricamente, l’inversa di una matrice rappresenta la trasformazione inversa della trasformazione rappresentata dalla matrice stessa. Ad esempio, se una matrice rappresenta una rotazione, la sua inversa rappresenta la rotazione opposta.\nCalcolo dell’inversa:\n\nMatrici diagonali: Per le matrici diagonali, l’inversa si ottiene invertendo gli elementi sulla diagonale principale.\nMetodo di Gauss-Jordan: Questo è un algoritmo sistematico per calcolare l’inversa di una matrice. Si basa su operazioni elementari sulle righe della matrice aumentata \\([\\boldsymbol{A} | \\boldsymbol{I}]\\), dove \\(\\boldsymbol{I}\\) è la matrice identità.\nAltri metodi: Esistono altri metodi, come l’uso della matrice aggiunta, ma sono generalmente meno efficienti.\n\nProprietà:\n\nUnicità: Se una matrice ha un’inversa, questa è unica.\nInversa del prodotto: \\((AB)^{-1} = B^{-1}A^{-1}\\) (a condizione che le inverse esistano).\nInversa della trasposta: \\((A^T)^{-1} = (A^{-1})^T\\).\n\nPer esempio, sia\n\\[\n\\boldsymbol{A} =  \\left[ \\begin{array}{c c}\n3 & 4 \\\\\n2 & 6\n\\end{array}\n\\right]\n\\]\nallora\n\\[\n\\boldsymbol{A}^{-1} =  \\left[ \\begin{array}{c c}\n.6 & -.4 \\\\\n-.2 & .3\n\\end{array}\n\\right]\n\\]\ne\n\\[\n\\boldsymbol{A}\\boldsymbol{A}^{-1} =\\left[ \\begin{array}{c c}\n3 & 4 \\\\\n2 & 6\n\\end{array}\n\\right]\n\\left[ \\begin{array}{c c}\n.6 & -.4 \\\\\n-.2 & .3\n\\end{array}\n\\right] =\n\\left[ \\begin{array}{c c}\n1 & 0 \\\\\n0 & 1\n\\end{array}\n\\right]\n\\]\n\nA &lt;- matrix(\n  c(3, 4, 2, 6),\n  nrow = 2,\n  byrow = TRUE\n)\nA |&gt; print()\n\n     [,1] [,2]\n[1,]    3    4\n[2,]    2    6\n\n\n\nsolve(A) |&gt; print()\n\n     [,1] [,2]\n[1,]  0.6 -0.4\n[2,] -0.2  0.3\n\n\n\nA %*% solve(A) |&gt; print()\n\n             [,1] [,2]\n[1,] 1.000000e+00    0\n[2,] 5.551115e-17    1\n\n\n\nsolve(A) %*% A |&gt; print()\n\n     [,1]         [,2]\n[1,]    1 1.110223e-16\n[2,]    0 1.000000e+00\n\n\nAbbiamo detto sopra che, se \\(\\boldsymbol{A}\\) e \\(\\boldsymbol{B}\\) sono due matrici non singolari aventi le stesse dimensioni, allora l’inversa del loro prodotto è uguale al prodotto delle loro inverse nella sequenza opposta:\n\\[\n(\\boldsymbol{AB})^{-1}=\\boldsymbol{B}^{-1}\\boldsymbol{A}^{-1}.\n\\]\nEsaminiamo un esempio numerico.\n\nB &lt;- matrix(\n  c(1, 2, 9, 7),\n  nrow = 2,\n  byrow = TRUE\n)\nB |&gt; print()\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    9    7\n\n\n\nB %*% solve(B) \n\n\nA matrix: 2 × 2 of type dbl\n\n\n1\n0\n\n\n0\n1\n\n\n\n\n\n\nsolve(A %*% B) |&gt; print()\n\n           [,1]       [,2]\n[1,] -0.4181818  0.3090909\n[2,]  0.5090909 -0.3545455\n\n\n\nsolve(B) %*% solve(A) |&gt; print()\n\n           [,1]       [,2]\n[1,] -0.4181818  0.3090909\n[2,]  0.5090909 -0.3545455\n\n\nL’inversa della trasposta di una matrice non singolare è uguale alla trasposta dell’inversa:\n\\[\n(\\boldsymbol{A}')^{-1}=(\\boldsymbol{A}^{-1})'.\n\\]\nConsideriamo un esempio numerico.\n\nsolve(t(A)) |&gt; print()\n\n     [,1] [,2]\n[1,]  0.6 -0.2\n[2,] -0.4  0.3\n\n\n\nt(solve(A)) |&gt; print()\n\n     [,1] [,2]\n[1,]  0.6 -0.2\n[2,] -0.4  0.3",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a4_linear_alg.html#determinante-di-una-matrice",
    "href": "chapters/appendix/a4_linear_alg.html#determinante-di-una-matrice",
    "title": "Appendice D — Elementi di algebra lineare",
    "section": "D.4 Determinante di una matrice",
    "text": "D.4 Determinante di una matrice\nIl determinante di una matrice quadrata \\(\\boldsymbol{A}\\), indicato con \\(|\\boldsymbol{A}|\\), è uno scalare che fornisce importanti informazioni sulla matrice stessa. Geometricamente, il valore assoluto del determinante rappresenta il volume del parallelepipedo generato dalle colonne della matrice (nel caso di una matrice 3x3). Per matrici di dimensioni inferiori, il determinante ha analoghe interpretazioni geometriche: ad esempio, per una matrice 2x2 rappresenta l’area del parallelogramma determinato dalle sue colonne.\n\nD.4.1 Calcolo del determinante\n\nMatrice diagonale: Per una matrice diagonale \\(\\boldsymbol{D} = diag(d_1, \\dots, d_n)\\), il determinante è semplicemente il prodotto degli elementi sulla diagonale: \\(|\\boldsymbol{D}| = d_1 \\cdot d_2 \\cdots d_n\\).\nMatrice 2x2: Per una matrice\n\\[\n  \\boldsymbol{A} =  \\left[ \\begin{array}{c c}\n  a_{11}& a_{12} \\\\\n  a_{21} & a_{22} \\end{array} \\right]\n  \\]\nil determinante è dato da:\n\\[\n  |\\boldsymbol{A}| =  a_{11}a_{22}-a_{12}a_{21}\n  \\]\nMatrici di dimensioni superiori: Per matrici di dimensioni superiori, esistono diversi metodi per calcolare il determinante, tra cui lo sviluppo di Laplace e la regola di Sarrus (per matrici 3x3).\n\nInterpretazioni e applicazioni:\n\nInvertibilità: Una matrice quadrata è invertibile se e solo se il suo determinante è diverso da zero.\nSistemi lineari: Il teorema di Cramer lega il determinante al numero di soluzioni di un sistema lineare.\n\nConsideriamo un esempio numerico nel caso più semplice di una matrice 2$$2.\n\nA &lt;- matrix(\n  c(1, -2, 3, 9),\n  nrow = 2, \n  byrow = TRUE\n)\nA |&gt; print()\n\n     [,1] [,2]\n[1,]    1   -2\n[2,]    3    9\n\n\n\ndet(A) |&gt; print()\n\n[1] 15\n\n\n\n\nD.4.2 Determinante e inversa\nVi è una relazione tra il determinante e l’inversa di una matrice. Se la matrice \\(\\boldsymbol{A}\\) ha dimensioni \\(2 \\times 2\\) l’inversa di \\(\\boldsymbol{A}\\) si trova nel modo seguente\n\\[\n\\boldsymbol{A}^{-1} = \\frac{1}{|\\boldsymbol{A}|} \\left[\n\\begin{array}{c c}\na_{22} & -a_{12} \\\\\n-a_{21} & a_{11}\n\\end{array}\n\\right].\n\\]\nAnche per le matrici di dimensioni maggiori la matrice inversa è definita nei termini del determinante, ma le formule di calcolo sono molto più complesse.\nPer esempio, sia\n\\[\n\\boldsymbol{A} = \\left[ \\begin{array}{c c}\n3 & 4 \\\\\n2 & 6\n\\end{array}\n\\right]\n\\]\nallora\n\\[\n\\boldsymbol{A}^{-1} = \\frac{1}{10} \\left[\n\\begin{array}{c c}\n6 & -4 \\\\\n-2 & 3\n\\end{array}\n\\right]= \\left[ \\begin{array}{c c}\n.6 & -.4 \\\\\n-.2 & .3\n\\end{array}\n\\right].\n\\]\nIn precedenza abbiamo detto che, in alcuni casi, una matrice “si comporta come lo 0.” Il determinante di una matrice è ci dice quando una matrice “si comporta come lo 0.” \\(|\\boldsymbol{A}| = 0\\), infatti, se una riga (o una colonna) è una combinazione lineare di due (o più) righe (o colonne) di \\(\\boldsymbol{A}\\).\nPer esempio, nel caso di una matrice (\\(2 \\times 2\\))\n\\[\n\\boldsymbol{A} =  \\left( \\begin{array}{c c}\na_{11}& a_{12} \\\\\na_{21} & a_{22} \\end{array} \\right)\n\\]\nsupponiamo che\n\\[\n\\left(%\n\\begin{array}{c}\n  a_{11} \\\\\n  a_{21} \\\\\n\\end{array}%\n\\right)=2\n\\left(%\n\\begin{array}{c}\n  a_{12} \\\\\n  a_{22} \\\\\n\\end{array}%\n\\right).\n\\]\nAllora\n\\[\n\\boldsymbol{A} =  \\left( \\begin{array}{c c}\n2a_{12}& a_{12} \\\\\n2a_{22} & a_{22} \\end{array} \\right)\n\\]\ne\n\\[\n|\\boldsymbol{A}| = 2a_{12}a_{22}-2a_{12}a_{22}=0.\n\\]\nIn conclusione, se il determinante è uguale a zero, allora la matrice inversa non esiste. Nel caso di una matrice (\\(2 \\times 2\\)), infatti, la formula dell’inversa richiede la divisione per \\(a_{11}a_{22}-a_{12}a_{21}\\) che, nel caso di una matrice singolare, è uguale a zero.\n\nA &lt;- matrix(\n  c(2, 4, 3, 6),\n  nrow = 2,\n  byrow = TRUE\n)\nA |&gt; print()\n\n     [,1] [,2]\n[1,]    2    4\n[2,]    3    6\n\n\n\ndet(A) |&gt; print()\n\n[1] -6.661338e-16\n\n\n\n\nD.4.3 Proprietà del determinante\n\n\\(|\\boldsymbol{A}'| = |\\boldsymbol{A}|\\).\nSe \\(\\boldsymbol{A}\\) contiene una colonna o una riga i cui elementi sono tutti 0, allora \\(|\\boldsymbol{A}|=0\\).\nSe \\(\\boldsymbol{A}\\) contiene due colonne (o righe) identiche, allora \\(|\\boldsymbol{A}|=0\\).\n\\(|\\boldsymbol{A}| = 0\\) se una riga (o una colonna) è combinazione lineare di due (o più) righe (o colonne) di \\(\\boldsymbol{A}\\).\n\\(|\\boldsymbol{A}| = 1/|\\boldsymbol{A}^{-1}|\\).\n\\(|\\boldsymbol{I}| = 1\\).\n\\(|\\boldsymbol{A} \\boldsymbol{B}| = |\\boldsymbol{A}| |\\boldsymbol{B}|\\).\n\nPer una matrice quadrata \\(\\boldsymbol{A}\\), le seguenti affermazioni sono equivalenti: \\(\\boldsymbol{A}\\) è non singolare, \\(|\\boldsymbol{A}|\\neq 0\\), \\(\\boldsymbol{A}^{-1}\\) esiste.",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a4_linear_alg.html#radici-e-vettori-latenti",
    "href": "chapters/appendix/a4_linear_alg.html#radici-e-vettori-latenti",
    "title": "Appendice D — Elementi di algebra lineare",
    "section": "D.5 Radici e vettori latenti",
    "text": "D.5 Radici e vettori latenti\nDal determinante di una matrice si possono ricavare le radici latenti o autovalori (denotati da \\(\\lambda_i\\)) e i vettori latenti o autovettori della matrice. Alle nozioni di autovalore e autovettore verrà qui fornita un’interpretazione geometrica.\nSimuliamo di dati di due variabili associate tra loro:\n\nset.seed(123456)\noptions(repr.plot.width = 8, repr.plot.height = 8)\n\nnpoints &lt;- 20\nx &lt;- as.numeric(scale(rnorm(npoints, 0, 1)))\ny &lt;- as.numeric(scale(3 * x + rnorm(npoints, 0, 2)))\nmean(x) |&gt; print()\nmean(y) |&gt; print()\ncor(x, y) |&gt; print()\n\n[1] -2.775558e-17\n[1] -7.771561e-17\n[1] 0.8291033\n\n\nDisegnamo il diagramma di dispersione con un ellisse che contiene la nube di punti:\n\nY &lt;- cbind(x, y)\ncar::dataEllipse(\n  Y[, 1], Y[, 2],\n  levels = 0.95,\n  lty = 2,\n  ylim = c(-3, 3),\n  xlim = c(-3, 3)\n)\n\n\n\n\n\n\n\n\nSe racchiudiamo le osservazioni (\\(v_1, v_2\\)) con un’ellisse, allora la lunghezza dei semiassi maggiori e minori dell’ellisse sarà proporzionale a \\(\\sqrt{\\lambda_1}\\) e \\(\\sqrt{\\lambda_2}\\). L’asse maggiore è la linea passante per il punto (\\(\\bar{v_1}, \\bar{v_2}\\)) nella direzione determinata dal primo autovettore \\(\\boldsymbol{a}_1'\\) con pendenza uguale a \\(a_{12}/a_{11}\\). L’asse minore è la linea passante per il punto (\\(\\bar{v_1}, \\bar{v_2}\\)) nella direzione determinata dal secondo autovettore \\(\\boldsymbol{a}_2\\).\nCalcoliamo ora gli autovettori e gli autovalori:\n\ns &lt;- cov(Y)\nee &lt;- eigen(s)\nee |&gt; print()\n\neigen() decomposition\n$values\n[1] 1.8291033 0.1708967\n\n$vectors\n          [,1]       [,2]\n[1,] 0.7071068 -0.7071068\n[2,] 0.7071068  0.7071068\n\n\n\n\n# First eigenvector \nev_1 &lt;- ee$vectors[, 1]\n\n# Slope of the first eigenvector\nev1_m &lt;- ev_1[2] / ev_1[1]\n\n# Second eigenvector \nev_2 &lt;- ee$vectors[, 2]\n\n# Slope of the second eigenvector\nev2_m &lt;- ev_2[2] / ev_2[1]\n\n\n# Scatter plot showing the span of both eigenvectors \ndata.frame(zx=scale(x), zy=scale(y))  |&gt;\nggplot(aes(x = zx, y = zy)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_vline(xintercept = 0, linewidth = .5) +\n  geom_hline(yintercept = 0, linewidth = .5) +\n  geom_abline(slope = ev1_m, color = \"blue\", linewidth = 0.7) +\n  geom_abline(slope = ev2_m, color = \"red\", linewidth = 0.7) \n\n\n\n\n\n\n\n\nGli autovettori sono ortogonali:\n\n# Multiply both eigenvectors \nprint(ev_1 %*% ev_2)\n\n             [,1]\n[1,] 2.237114e-17\n\n\nGeneriamo uno Scree Plot.\n\n# Calculate the estimated variance for each eigenvalue\ne_var &lt;- ee$values / (length(x) - 1)\n\n# Data frame with variance percentages\nvar_per &lt;- tibble(\n  PC  = c(\"PC1\", \"PC2\"),\n  PER = c(e_var) * 100 / sum(e_var) # Calculate the percentage\n    )\n\n# Scree plot \nggplot(var_per, aes(x = PC, y = PER)) +\n  geom_col(width = 0.5, color = \"black\") +\n  xlab(\"Principal component\") +\n  ylab(\"Percentage of variation (%)\") \n\n\n\n\n\n\n\n\nVerifichiamo che la somma degli autovalori sia uguale alla varianza totale.\n\nvar(x) + var(y)\n\n2\n\n\n\nee$values |&gt; sum()\n\n2\n\n\nGli autovettori ottenuti utilizzando la funzione eigen() sono normalizzati. Ciò significa che la loro lunghezza è uguale a 1:\n\nt(as.matrix(ee$vectors[, 1])) %*% as.matrix(ee$vectors[, 1]) |&gt; print()\n\n     [,1]\n[1,]    1\n\n\nUtilizziamo le informazioni degli autovettori e degli autovalori per disegnare gli assi dell’ellisse:\n\ngli autovettori determinano la direzione degli assi;\nla radice quadrata degli autovalori è proporzionale alla lunghezza degli assi dell’ellisse.\n\n\ncar::dataEllipse(\n  Y[, 1], Y[, 2],\n  levels = 0.95,\n  lty = 2,\n  ylim = c(-3, 3),\n  xlim = c(-3, 3)\n)\nk &lt;- 2.65\narrows(\n  0, 0, \n  k * sqrt(ee$values[1]) * ee$vectors[1],\n  k * sqrt(ee$values[1]) * ee$vectors[2],\n  code = 2, \n  col = \"red\", \n  lwd = 2\n)\narrows(\n  0, 0, \n  k * sqrt(ee$values[2]) * ee$vectors[1],\n  k * sqrt(ee$values[2]) * -ee$vectors[2],\n  code = 2, \n  col = \"red\", \n  lwd = 2\n)\n\n\n\n\n\n\n\n\nTale analisi si può estendere a qualunque numero di variabili. Per esempio, nel caso di tre variabili, possiamo pensare di disegnare un ellisoide attorno ad una nube di punti nello spazio tridimensionale. Anche in questo caso, gli autovalori e gli associati autovettori corrisponderanno agli assi dell’elissoide.",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a4_linear_alg.html#scomposizione-spettrale-di-una-matrice",
    "href": "chapters/appendix/a4_linear_alg.html#scomposizione-spettrale-di-una-matrice",
    "title": "Appendice D — Elementi di algebra lineare",
    "section": "D.7 Scomposizione spettrale di una matrice",
    "text": "D.7 Scomposizione spettrale di una matrice\nData una matrice quadrata e simmetrica di dimensione \\(n\\), \\(\\boldsymbol{A}\\), esistono una matrice diagonale \\(\\boldsymbol{\\Lambda}\\) e una matrice ortogonale \\(\\boldsymbol{V}\\) tali che\n\\[\\boldsymbol{A} =\\boldsymbol{V} \\boldsymbol{\\Lambda} \\boldsymbol{V}',\\] dove\n\n\\(\\boldsymbol{\\Lambda}\\) è una matrice diagonale i cui elementi sono gli autovalori di \\(\\boldsymbol{A}\\): \\(\\boldsymbol{\\Lambda} = diag(\\lambda_1, \\lambda_2,\n    \\dots, \\lambda_n)\\);\n\\(\\boldsymbol{V}\\) è una matrice ortogonale le cui colonne \\((v_1, v_2, \\dots, v_p)\\) sono gli autovettori di \\(\\boldsymbol{A}\\) associati ai rispettivi autovalori.\n\nIn maniera equivalente\n\\[\\boldsymbol{A} \\boldsymbol{V} =  \\boldsymbol{\\Lambda} \\boldsymbol{V}'.\\]\nPremoltiplicando entrambi i membri per \\(\\boldsymbol{V}'\\) si ottiene\n\\[\\boldsymbol{V}'\\boldsymbol{A} \\boldsymbol{V} =\n\\boldsymbol{\\Lambda},\\]\nda cui l’affermazione che la matrice degli autovettori diagonalizza \\(\\boldsymbol{A}\\).\nPer esempio,\n\nsigma &lt;- matrix(\n  data = c(1, 0.5, 0.5, 1.25), \n  nrow = 2, \n  ncol = 2\n)\nsigma |&gt; print()\n\n     [,1] [,2]\n[1,]  1.0 0.50\n[2,]  0.5 1.25\n\n\n\nout &lt;- eigen(sigma)\nout |&gt; print()\n\neigen() decomposition\n$values\n[1] 1.6403882 0.6096118\n\n$vectors\n          [,1]       [,2]\n[1,] 0.6154122 -0.7882054\n[2,] 0.7882054  0.6154122\n\n\n\n\nLambda &lt;- diag(out$values)\nLambda |&gt; print()\n\n         [,1]      [,2]\n[1,] 1.640388 0.0000000\n[2,] 0.000000 0.6096118\n\n\n\nU &lt;- out$vectors\nU |&gt; print()\n\n          [,1]       [,2]\n[1,] 0.6154122 -0.7882054\n[2,] 0.7882054  0.6154122\n\n\n\nU %*% Lambda %*% t(U) |&gt; print()\n\n     [,1] [,2]\n[1,]  1.0 0.50\n[2,]  0.5 1.25\n\n\n\nD.7.1 Autovalori e determinante\nIl determinante di una matrice è il prodotto degli autovalori:\n\\[\\begin{aligned}\n    |\\boldsymbol{A}| &= \\prod_{i=1}^{p} \\lambda_i. \\notag\n    \\end{aligned}\\]\nLa traccia di una matrice è uguale alla somma degli autovalori:\n\\[\\begin{aligned}\n    tr(\\boldsymbol{A}) &= \\sum_{i=1}^{p} \\lambda_i. \\notag\n    \\end{aligned}\\]\n\nsigma &lt;- matrix(data = c(1, 0.5, 0.5, 2), nrow = 2, ncol = 2)\nsigma |&gt; print()\n\nout &lt;- eigen(sigma)\nout |&gt; print()\n\n     [,1] [,2]\n[1,]  1.0  0.5\n[2,]  0.5  2.0\neigen() decomposition\n$values\n[1] 2.2071068 0.7928932\n\n$vectors\n          [,1]       [,2]\n[1,] 0.3826834 -0.9238795\n[2,] 0.9238795  0.3826834\n\n\n\nLa traccia di una matrice è uguale alla somma degli autovalori:\n\nsum(out$values) |&gt; print()\n\n[1] 3\n\n\nIl determinante di una matrice è il prodotto degli autovalori:\n\ndet(sigma) |&gt; print()\n(out$values[1] * out$values[2]) |&gt; print()\n\n[1] 1.75\n[1] 1.75\n\n\nGli autovalori di \\(\\boldsymbol{A}^{-1}\\) sono i reciproci degli autovalori di \\(\\boldsymbol{A}\\); gli autovettori sono coincidenti.",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/02_meanstructure.html#introduzione",
    "href": "chapters/cfa/02_meanstructure.html#introduzione",
    "title": "34  La struttura delle medie",
    "section": "",
    "text": "34.1.1 Interpretazione delle Intercette nei Modelli SEM\nIn un modello SEM, l’intercetta di una variabile indicatore (denotata con $ $) indica la media stimata di quella variabile. Il valore di $ $ rappresenta il valore atteso dell’indicatore quando il fattore latente a cui è associato è zero. La relazione generale per un indicatore $ y $ in un modello SEM è data dalla formula:\n\\[\ny = \\tau + \\lambda \\cdot \\text{fattore latente} + \\varepsilon,\n\\]\ndove: - $ y $ è il punteggio osservato dell’indicatore. - $ $ rappresenta il carico fattoriale, che indica quanto fortemente l’indicatore è influenzato dal fattore latente. - $ $ è l’intercetta, cioè la media stimata dell’indicatore. - $ $ è l’errore di misura associato all’indicatore.\n\n\n34.1.2 Struttura delle Medie nel Modello CFA\nNel contesto di un modello CFA, la struttura delle medie è descritta dalla formula:\n\\[ \\text{media(variabile latente)} = \\Lambda \\mu_{\\text{lat}} + \\tau, \\]\nqui: - $ $ è la matrice dei carichi fattoriali. - $ _{} $ è il vettore che rappresenta le medie dei costrutti latenti. - $ $ è il vettore delle intercette degli indicatori.\n\n\n34.1.3 Utilizzo delle Medie nel Software lavaan\nNel software lavaan, utilizzato per l’analisi SEM, è possibile stimare le intercette inserendo l’opzione meanstructure = TRUE nella sintassi del modello. Questo comando permette di includere automaticamente una costante “1” in tutte le equazioni del modello, facilitando così il calcolo delle intercette per le variabili endogene. È necessario fornire i dati originali o una matrice di covarianza, insieme alle medie di tutte le variabili interessate.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>La struttura delle medie</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/02_meanstructure.html#interpretazione-delle-intercette-nei-modelli-sem",
    "href": "chapters/cfa/02_meanstructure.html#interpretazione-delle-intercette-nei-modelli-sem",
    "title": "34  La struttura delle medie",
    "section": "34.2 Interpretazione delle Intercette nei Modelli SEM",
    "text": "34.2 Interpretazione delle Intercette nei Modelli SEM\nIn un modello SEM, l’intercetta di una variabile indicatore (denotata con \\(\\tau\\)) indica la media stimata di quella variabile. Il valore di \\(\\tau\\) rappresenta il valore atteso dell’indicatore quando il fattore latente a cui è associato è zero. La relazione generale per un indicatore \\(y\\) in un modello SEM è data dalla formula:\n\\[\ny = \\tau + \\lambda \\cdot \\text{fattore latente} + \\varepsilon,\n\\]\ndove:\n\n\\(y\\) è il punteggio osservato dell’indicatore.\n\\(\\lambda\\) rappresenta il carico fattoriale, che indica quanto fortemente l’indicatore è influenzato dal fattore latente.\n\\(\\tau\\) è l’intercetta, cioè la media stimata dell’indicatore.\n\\(\\varepsilon\\) è l’errore di misura associato all’indicatore.\n\n\n34.2.1 Struttura delle Medie nel Modello CFA\nNel contesto di un modello CFA, la struttura delle medie è descritta dalla formula:\n\\[ \\text{media(variabile latente)} = \\Lambda \\mu_{\\text{lat}} + \\tau, \\]\nqui:\n\n\\(\\Lambda\\) è la matrice dei carichi fattoriali.\n\\(\\mu_{\\text{lat}}\\) è il vettore che rappresenta le medie dei costrutti latenti.\n\\(\\tau\\) è il vettore delle intercette degli indicatori.\n\n\n\n34.2.2 Utilizzo delle Medie nel Software lavaan\nNel software lavaan, utilizzato per l’analisi SEM, è possibile stimare le intercette inserendo l’opzione meanstructure = TRUE nella sintassi del modello. Questo comando permette di includere automaticamente una costante “1” in tutte le equazioni del modello, facilitando così il calcolo delle intercette per le variabili endogene. È necessario fornire i dati originali o una matrice di covarianza, insieme alle medie di tutte le variabili interessate.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>La struttura delle medie</span>"
    ]
  }
]