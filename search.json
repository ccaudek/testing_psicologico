[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Testing Psicologico",
    "section": "",
    "text": "Benvenuti\nQuesto sito web è dedicato al materiale didattico dell’insegnamento di Testing Psicologico (A.A. 2024/2025), rivolto agli studenti del primo anno del Corso di Laurea Magistrale Psicologia Clinica e della Salute e Neuropsicologia dell’Università degli Studi di Firenze.\nL’insegnamento di Testing Psicologico si propone quale stimolo e guida per l’apprendimento delle basi dell’assessment psicologico.",
    "crumbs": [
      "Benvenuti"
    ]
  },
  {
    "objectID": "index.html#informazioni-sullinsegnamento",
    "href": "index.html#informazioni-sullinsegnamento",
    "title": "Testing Psicologico",
    "section": "Informazioni sull’insegnamento",
    "text": "Informazioni sull’insegnamento\n\nCodice: B033288 - Testing Psicologico\nModulo: B033288 - Testing Psicologico (Cognomi L-Z)\nCorso di laurea: Laurea Magistrale: Psicologia Clinica e della Salute e Neuropsicologia\nAnno Accademico: 2024-2025\nCalendario: Il corso si terrà dal 4 marzo al 31 maggio 2025.\nOrario delle lezioni: Le lezioni si svolgeranno il martedì dalle 10:30 alle 13:30 e il giovedì dalle 8:30 alle 11:30.\nLuogo: Le lezioni si terranno presso il Plesso didattico La Torretta.\nModalità di svolgimento della didattica: Le lezioni ed esercitazioni saranno svolte in modalità frontale.\n\n\n\n\n\n\n\nQuesto sito web è la fonte ufficiale per tutte le informazioni relative al programma dell’insegnamento B033288 - Testing Psicologico (Cognomi A-K) per l’A.A. 2024-2025 e le modalità d’esame.",
    "crumbs": [
      "Benvenuti"
    ]
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "Testing Psicologico",
    "section": "Syllabus",
    "text": "Syllabus\nIl Syllabus può essere scaricato utilizzando questo link.",
    "crumbs": [
      "Benvenuti"
    ]
  },
  {
    "objectID": "prefazione.html",
    "href": "prefazione.html",
    "title": "Prefazione",
    "section": "",
    "text": "Definizione di misurazione\nGli obiettivi di questo insegnamento sono:\nViene presentata qui una panoramica degli argomenti che verranno trattati.\nLa misurazione psicologica è un pilastro fondamentale nella comprensione e nell’analisi del comportamento umano, fornendo un mezzo quantitativo per esplorare le dinamiche della mente e della personalità. La definizione di misurazione proposta da Stevens (1951), uno dei pionieri della teoria della misurazione, stabilisce che essa consiste nell’assegnare numeri a oggetti o eventi secondo regole definite. Tuttavia, è ormai ampiamente accettato che questa visione sia troppo semplicistica e che la misurazione richieda un approccio più sofisticato. Si concorda comunemente sul fatto che la misurazione debba essere considerata come un processo di creazione di modelli che rappresentano i fenomeni di interesse, principalmente in forma quantitativa.\nDi conseguenza, la misurazione si basa su regole che attribuiscono scale o valori alle entità che rappresentano i costrutti di interesse. Come avviene per tutti i modelli, quelli di misurazione, come i test, le scale o le variabili, devono semplificare la realtà per risultare utili. Pertanto, è fondamentale specificare chiaramente i modelli di misurazione per poterli valutare, confutare e migliorare.\nInoltre, anziché chiedersi se un modello sia vero o corretto, è più utile sviluppare diversi modelli alternativi plausibili e porre domande del tipo: quale modello è meno inaccurato? Questo approccio al confronto dei modelli rappresenta la strategia migliore per valutare e perfezionare le procedure di misurazione, consentendo un’analisi più approfondita e accurata delle variabili coinvolte.\nPer illustrare l’approccio alla misurazione come descritto, prendiamo in considerazione un esempio concreto: la valutazione dell’intelligenza attraverso il test del quoziente intellettivo (QI).\nIniziamo definendo il concetto di interesse, ovvero l’intelligenza, che può essere concepita come la capacità di apprendere, comprendere e applicare conoscenze, risolvere problemi e adattarsi a nuove situazioni. Tuttavia, trattandosi di un concetto astratto, è necessario operazionalizzarlo in modo misurabile.\nPer misurare l’intelligenza, si crea un test di QI che comprende una serie di compiti e domande progettati per valutare diverse dimensioni della capacità cognitiva, quali la memoria, il ragionamento logico e la comprensione verbale.\nCiascun compito nel test di QI è associato a un punteggio. I risultati individuali vengono quindi calcolati e confrontati con una norma statistica per attribuire un punteggio di QI.\nSuccessivamente, il test di QI viene sottoposto a diverse analisi per verificare la sua validità (ovvero se misura effettivamente l’intelligenza) e affidabilità (se fornisce risultati consistenti nel tempo).\nTuttavia, esistono diverse teorie dell’intelligenza, come ad esempio quella delle intelligenze multiple di Gardner, che suggeriscono modelli alternativi di misurazione. Confrontando il modello del QI con questi approcci alternativi, gli psicologi possono valutare quale modello è meno distorto o più adatto per specifici scopi.\nIn risposta alle critiche, alle nuove scoperte e ai cambiamenti culturali e sociali, il modello del QI viene regolarmente rivisto e adattato per assicurare che continui a essere uno strumento utile di misurazione.\nQuesto esempio mostra come la misurazione in psicologia non sia semplicemente un atto di assegnare numeri a un costrutto, ma piuttosto un processo complesso che implica la creazione, la valutazione e il continuo perfezionamento di modelli teorici.",
    "crumbs": [
      "Prefazione"
    ]
  },
  {
    "objectID": "prefazione.html#temi-centrali-nellapproccio-psicometrico",
    "href": "prefazione.html#temi-centrali-nellapproccio-psicometrico",
    "title": "Prefazione",
    "section": "Temi Centrali nell’Approccio Psicometrico",
    "text": "Temi Centrali nell’Approccio Psicometrico\n\nAffidabilità: Questo concetto si riferisce alla capacità di un test di produrre risultati consistenti nel tempo e in contesti diversi, costituendo una base fondamentale per la misurazione psicologica.\nValidazione del Costrutto e Test dei Modelli: L’evoluzione della psicometria ha portato a una sempre maggiore enfasi sulla validazione dei costrutti e sull’importanza dei test di modelli, utilizzando tecniche come i modelli a equazioni strutturali (SEM) per verificare la coerenza e la validità dei costrutti psicologici.\nDimensionalità e Validità Strutturale: La dimensionalità viene considerata un elemento fondamentale nella valutazione della validità strutturale, poiché permette di esplorare come i diversi aspetti di un costrutto si manifestano e interagiscono all’interno del modello di misurazione.\nCostruzione dei Questionari: La progettazione e la formulazione degli item dei questionari rivestono un ruolo cruciale, in quanto influenzano direttamente l’affidabilità e la validità dei risultati ottenuti. La scelta degli item, il loro ordine e la chiarezza della formulazione sono tutti aspetti che contribuiscono alla qualità e all’efficacia della misurazione psicologica.\n\nAttraverso questi approcci, la misurazione psicologica si adatta alle sfide uniche poste dalla natura astratta e complessa dei costrutti psicologici, cercando di fornire strumenti validi e affidabili per la loro esplorazione e comprensione.",
    "crumbs": [
      "Prefazione"
    ]
  },
  {
    "objectID": "prefazione.html#affidabilità-e-generalizzabilità-nelle-misure-psicologiche",
    "href": "prefazione.html#affidabilità-e-generalizzabilità-nelle-misure-psicologiche",
    "title": "Prefazione",
    "section": "Affidabilità e Generalizzabilità nelle Misure Psicologiche",
    "text": "Affidabilità e Generalizzabilità nelle Misure Psicologiche\nNel contesto della misurazione psicologica, così come in altre discipline, è cruciale considerare le variabili che possono influenzare la precisione delle misure. L’affidabilità di uno strumento di misurazione psicologica si riferisce alla sua consistenza nel produrre risultati replicabili nel tempo e in contesti diversi. Gli indici di affidabilità sono utilizzati per quantificare il grado di riproducibilità e l’assenza di errori casuali nelle misurazioni.\n\nTeoria Classica dei Test\nL’approccio più ampiamente utilizzato nello studio dell’affidabilità delle misure psicologiche è rappresentato dalla teoria classica dei test, come descritto da Lord e Novick (1968). Secondo questa teoria, ogni misurazione (\\(X\\)) è composta da due componenti distintive: un punteggio “vero” (\\(T\\)) e un errore di misurazione (\\(e\\)). Il concetto di misurazione accurata, o “vera”, può essere rappresentato come \\(X - e\\), evidenziando il fatto che ogni misurazione può essere decomposta in tali elementi distinti.\nLa teoria classica dei test enfatizza l’importanza di condurre misurazioni ripetute per valutare l’affidabilità. Un concetto fondamentale è quello dei test paralleli, che consistono in due test con medie, varianze e distribuzioni identiche, e che mostrano una correlazione simile con variabili esterne. In questa prospettiva, il punteggio vero e l’errore di misurazione sono considerati indipendenti. Di conseguenza, la varianza dei punteggi osservati (Varianza \\(X\\)) è la somma della varianza dei punteggi veri (Varianza \\(T\\)) e della varianza dell’errore di misurazione (Varianza \\(e\\)).\nL’affidabilità è quindi definita come il rapporto tra la varianza del punteggio vero e la varianza del punteggio osservato:\n\\[\n\\text{Affidabilità} = \\frac{\\text{Varianza}(T)}{\\text{Varianza}(X)}.\n\\]\nIn termini pratici, un’affidabilità di 1 indicherebbe l’assenza di errori, mentre un’affidabilità di 0 implicherebbe che i punteggi derivano esclusivamente dall’errore. La correlazione tra il punteggio osservato e il punteggio vero è la radice quadrata dell’affidabilità, fornendo una stima della precisione della misurazione.\nQuesto framework fornisce una solida base per comprendere e quantificare l’affidabilità nelle misure psicologiche, sottolineando l’importanza di considerare sia i punteggi veri sia gli errori di misurazione per ottenere misurazioni precise e affidabili.\n\n\nEvidenze Multiple di Affidabilità\nNonostante la teoria classica dei test fornisca una definizione matematica dei test paralleli, non fornisce dettagliate linee guida sulle procedure specifiche per costruirli. Tuttavia, a partire dagli anni ’50, sono stati sviluppati diversi metodi che consentono di valutare empiricamente l’affidabilità delle misurazioni:\n\nTest-Retest: Questo approccio implica la somministrazione dello stesso test ai partecipanti in due momenti diversi. L’obiettivo è valutare la stabilità dei punteggi nel tempo. Una correlazione elevata tra i punteggi ottenuti nei due momenti indica una buona affidabilità del test-retest.\nEquivalenza di Forme Parallele: Questo metodo prevede l’utilizzo di due versioni diverse del test, ma che coprono lo stesso contenuto, somministrate simultaneamente ai partecipanti. Una forte correlazione tra i punteggi ottenuti dalle due versioni suggerisce che entrambe misurano il medesimo costrutto in modo affidabile.\nSplit-Half e Coerenza Interna:\n\nSplit-Half: I partecipanti completano una sola versione del test, la quale è divisa in due parti equivalenti. Si calcola poi la correlazione tra i punteggi delle due metà. Questo metodo valuta la coerenza interna del test.\nCoerenza Interna (ad esempio, Omega di McDonals): Valuta la correlazione tra tutti gli elementi del test. Un alto valore di coerenza interna indica che tutti gli elementi del test misurano aspetti simili del costrutto.\n\nValutazione da Giudici Multipli: In questo caso, i partecipanti sono valutati da più giudici in un’unica occasione. Un alto grado di accordo tra i giudici fornisce un’indicazione dell’affidabilità delle valutazioni.\n\nCiascuno di questi approcci fornisce indicazioni sull’affidabilità di un test, ma è fondamentale considerare che alcuni potrebbero essere più appropriati di altri in base alla natura del test e del costrutto misurato. L’affidabilità è pertanto un concetto multidimensionale che richiede l’impiego di diversi approcci per una valutazione completa delle misurazioni psicologiche.\n\n\nIl Ruolo del Coefficiente Alpha nella Misurazione Psicologica\nIl coefficiente alpha, introdotto da Cronbach nel 1951, è diventato un importante indicatore di coerenza interna nella letteratura psicologica, principalmente grazie alla sua facilità di calcolo. A differenza dell’affidabilità test-retest, che richiede dati raccolti in due momenti diversi, o dell’affidabilità delle forme parallele, che richiede la costruzione di due versioni alternative di un test, il coefficiente alpha può essere calcolato utilizzando un unico set di dati, rendendolo estremamente pratico come indice di affidabilità.\nTuttavia, è importante correggere un comune malinteso riguardo al coefficiente alpha: esso non misura direttamente l’omogeneità delle intercorrelazioni tra gli elementi o conferma la unidimensionalità di una scala. In realtà, il coefficiente alpha non fornisce informazioni dirette su questi aspetti strutturali della scala.\nPer affrontare la questione della unidimensionalità, è necessario ricorrere a approcci più sofisticati come l’analisi fattoriale confermativa e i modelli di equazioni strutturali (SEM). Questi metodi consentono di testare quanto bene la struttura di correlazione degli elementi si adatti a un modello con un singolo fattore rispetto a modelli multifattoriali, valutando se le correlazioni tra gli elementi possono essere meglio spiegate da un singolo costrutto sottostante.\nNel contesto delle analisi SEM, le saturazioni degli item indicano quanto della varianza di un item sia condivisa con gli altri (e quindi generalizzabile), mentre la varianza residua dell’item cattura l’errore unico associato a quell’item. La presenza di multidimensionalità emerge dalla capacità di un modello multifattoriale di adattarsi meglio ai dati rispetto a un modello a singolo fattore.\nQuando un test è considerato multidimensionale, è ancora appropriato utilizzare il coefficiente alpha come indice di affidabilità? La risposta è negativa. In presenza di multidimensionalità, il coefficiente alpha tende a sottostimare l’affidabilità. Pertanto, è consigliabile, in tali casi, utilizzare altri metodi per valutare l’affidabilità, anziché basarsi esclusivamente sul coefficiente alpha.\n\n\nIl Fenomeno dell’Attenuazione in Relazione all’Affidabilità\nAll’interno del contesto della teoria classica dei test, come delineato da Lord e Novick (1968), l’affidabilità svolge un ruolo cruciale poiché influisce sulla forza della correlazione che una misura può mostrare con altre variabili, come un criterio esterno. Secondo questa teoria, se l’errore nelle misurazioni è genuinamente casuale, il massimo teorico della correlazione tra una misura e un’altra variabile non è 1.0, ma piuttosto la radice quadrata dell’affidabilità di quella misura.\nCiò implica che, in presenza di un’affidabilità meno che ottimale, la correlazione effettiva tra una misura e qualsiasi altra variabile viene sistematicamente sottostimata, fenomeno noto come attenuazione. Questa attenuazione è direttamente proporzionale all’inadeguatezza dell’affidabilità: più bassa è l’affidabilità di una misura, maggiore sarà la sottostima della sua correlazione con altre variabili. Pertanto, per ottenere stime accurate delle correlazioni e comprendere veramente le relazioni tra diverse variabili, è fondamentale garantire che le misure utilizzate siano il più affidabili possibile. Questa considerazione enfatizza l’importanza dell’accuratezza e della precisione nelle procedure di misurazione psicologica.\n\n\nLa Teoria della Generalizzabilità\nLa Teoria della Generalizzabilità propone un approccio più completo e flessibile per comprendere l’affidabilità delle misure psicologiche rispetto alla classificazione tradizionale delle tipologie di affidabilità. Invece di limitarsi a categorizzare le misure in base a criteri specifici come test-retest, affidabilità interna o inter-valutatori, la Teoria della Generalizzabilità considera una serie di dimensioni che possono influenzare l’affidabilità in contesti diversi.\nUna delle principali criticità della teoria classica dei test è la sua presunzione di uniformità e parallelismo delle misurazioni e degli errori casuali. La Teoria della Generalizzabilità, al contrario, riconosce che l’affidabilità dipende dalla specifica dimensione di generalizzazione considerata. Ad esempio, un test potrebbe essere affidabile per misurare una certa caratteristica in un contesto, ma non altrettanto affidabile in un contesto diverso o per una caratteristica correlata ma non identica.\nPer superare le limitazioni della teoria classica dei test, l’American Psychological Association ha proposto l’adozione della Teoria della Generalizzabilità. Tuttavia, nonostante questa proposta, la pratica nei campi di ricerca non si è adeguatamente evoluta e la teoria della generalizzabilità non ha ancora completamente sostituito le nozioni più semplicistiche popolari in psicologia.\nLa Teoria della Generalizzabilità esamina diverse dimensioni che influenzano l’affidabilità, tra cui la dimensione temporale, delle forme, degli item e dei giudici o osservatori. Questa teoria enfatizza l’importanza di estendere le osservazioni a un’ampia varietà di situazioni e identificare l’impatto specifico delle fonti di varianza nei punteggi dei test in contesti particolari.\nInvece dei tradizionali coefficienti di affidabilità come il coefficiente di stabilità o il coefficiente alfa, la Teoria della Generalizzabilità suggerisce l’uso di misure più ampie di affidabilità, come il coefficiente di correlazione intraclasse, per esaminare specifici aspetti dell’affidabilità. Questo approccio è particolarmente utile in ricerche con dati strutturati in maniera nidificata e dove diverse dimensioni possono influenzare l’affidabilità, come nei metodi di valutazione ecologica momentanea.### La Teoria della Risposta agli Item\nLa Teoria della Risposta agli Item (IRT) rappresenta un avanzamento rispetto alla teoria classica dei test, offrendo un approccio più sofisticato per analizzare le risposte degli individui agli item e la loro relazione con un costrutto latente. Questa teoria stabilisce un collegamento tra le risposte degli individui a un particolare item e il costrutto latente utilizzando una funzione chiamata “curva caratteristica dell’item”.\nLa curva caratteristica dell’item mostra la probabilità che individui con differenti livelli del costrutto latente rispondano correttamente all’item, fornendo inoltre informazioni sulla capacità dell’item di distinguere tra individui con livelli elevati e bassi del tratto latente, oltre a misurare la sua difficoltà. Queste informazioni sono cruciali per identificare eventuali distorsioni negli item, noto come bias. Secondo la IRT, un item è privo di bias nel misurare un costrutto se individui con lo stesso livello del tratto ottengono punteggi attesi simili sull’item, indipendentemente da caratteristiche non rilevanti come genere, etnia o background culturale.\nLa Teoria della Risposta agli Item offre diversi vantaggi nel processo di creazione e valutazione di scale psicometriche:\n\nSelezione degli Item: Permette di selezionare gli item in base alla loro difficoltà e alla capacità di discriminazione, superando così la limitazione della teoria classica che si basa esclusivamente sulle correlazioni tra gli item e il punteggio totale.\nTesting Adattivo Computerizzato: La IRT facilita la valutazione della posizione di un individuo su un costrutto latente senza la necessità di somministrare l’intero test, grazie a tecniche come il testing adattivo computerizzato.\n\nIn conclusione, la Teoria della Risposta agli Item fornisce strumenti quantitativi per esaminare approfonditamente la relazione tra un item specifico e il costrutto latente, attraverso parametri di difficoltà e discriminazione.",
    "crumbs": [
      "Prefazione"
    ]
  },
  {
    "objectID": "prefazione.html#evoluzione-e-comprensione-della-validità-nelle-misure-psicologiche",
    "href": "prefazione.html#evoluzione-e-comprensione-della-validità-nelle-misure-psicologiche",
    "title": "Prefazione",
    "section": "Evoluzione e Comprensione della Validità nelle Misure Psicologiche",
    "text": "Evoluzione e Comprensione della Validità nelle Misure Psicologiche\nLa nostra comprensione della validità nelle misure psicologiche ha subito un notevole sviluppo nel corso del tempo, passando da una visione iniziale più frammentata a un approccio più olistico e dinamico. Inizialmente, la validità veniva suddivisa in diversi tipi, tra cui la validità di contenuto, di facciata, orientata al criterio e di costrutto.\nLa validità di contenuto si riferisce alla rappresentatività degli item di un test rispetto al costrutto che si intende misurare, mentre la validità di facciata valuta se superficialmente gli item sembrano idonei a misurare il costrutto, sebbene questa non sia considerata un indice rigoroso di validità. La validità orientata al criterio si divide ulteriormente in predittiva e concorrente, che valutano la capacità del test di prevedere comportamenti futuri o di correlare con criteri esterni contemporaneamente misurati. Infine, la validità di costrutto indaga se il test misura effettivamente il costrutto in questione, richiedendo una comprensione approfondita sia del costrutto sia della metodologia del test.\nTuttavia, queste distinzioni sono state gradualmente considerate limitate e frammentarie. Un punto di svolta è stato rappresentato dall’approccio olistico di Samuel Messick, che ha enfatizzato che la validità va oltre la misura stessa, coinvolgendo l’interpretazione e l’uso dei punteggi del test. Messick ha sottolineato l’importanza di considerare le evidenze di validità da molteplici fonti e di assicurare la coerenza delle interpretazioni dei punteggi del test con le teorie psicologiche sottostanti.\nUn’importante correzione concettuale è stata l’idea che la validità non sia un attributo statico dei test, ma piuttosto un processo continuo di accumulo di evidenze e giustificazioni teoriche. Questo processo di validazione riflette l’evoluzione delle teorie psicologiche e delle metodologie di misurazione, sottolineando che la validità è dinamica e contestuale.\nIn sintesi, l’evoluzione della concezione di validità nelle misure psicologiche sottolinea l’importanza di un approccio comprensivo, teoricamente informato e basato sull’evidenza per valutare, interpretare e utilizzare i punteggi dei test. Questo approccio moderno incoraggia i ricercatori e i praticanti a considerare la validità come un concetto ampio che incorpora molteplici aspetti della progettazione, dell’implementazione e dell’interpretazione dei test psicologici.",
    "crumbs": [
      "Prefazione"
    ]
  },
  {
    "objectID": "prefazione.html#approfondimento-su-tecniche-di-validazione-di-costrutto-e-costruzione-di-scale",
    "href": "prefazione.html#approfondimento-su-tecniche-di-validazione-di-costrutto-e-costruzione-di-scale",
    "title": "Prefazione",
    "section": "Approfondimento su Tecniche di Validazione di Costrutto e Costruzione di Scale",
    "text": "Approfondimento su Tecniche di Validazione di Costrutto e Costruzione di Scale\nLa discussione sulla evoluzione della validità nelle misure psicologiche può proseguire con l’esame delle tecniche che vengono usate per la validazione di costrutto e per la costruzione di scale. In particolare, gli strumenti maggiormente usati dagli psicometristi sono l’Analisi Fattoriale Confermativa (CFA) e i Modelli di Equazioni Strutturali (SEM).\nL’Analisi Fattoriale Confermativa (CFA) rappresenta un approccio metodologico rigoroso, basato sull’ipotesi che un insieme di osservazioni possa essere spiegato da pochi costrutti latenti. A differenza dell’Analisi Fattoriale Esplorativa, che non prevede ipotesi a priori sui fattori, la CFA richiede che i ricercatori definiscano anticipatamente un modello teorico. Questo specifica le relazioni tra le variabili osservabili e i costrutti latenti, permettendo di testare l’adeguatezza del modello ai dati. La capacità della CFA di confrontare diversi modelli offre un mezzo potente per identificare la struttura che meglio rappresenta i dati.\nNel contesto della valutazione della coerenza interna di una scala, l’utilizzo della CFA supera i limiti dei metodi basati sulla teoria classica dei test, fornendo una valutazione più dettagliata e strutturata delle relazioni tra item e costrutti latenti.\nI Modelli di Equazioni Strutturali (SEM) estendono le possibilità offerte dalla CFA, abilitando l’analisi delle relazioni di regressione non solo tra variabili manifeste e latenti, ma anche tra i costrutti latenti stessi. Questa caratteristica rende i SEM strumenti eccezionalmente potenti per esplorare le interazioni complesse tra variabili in uno studio psicometrico.\nL’esame della dimensionalità di un costrutto attraverso la CFA e i SEM consente di testare con precisione le ipotesi sulla struttura dimensionale dei costrutti, verificando se l’organizzazione teorizzata degli item in fattori latenti corrisponde ai dati. Questi strumenti sono quindi fondamentali per confermare la struttura di un costrutto come ipotizzato dalla teoria sottostante.\nIn aggiunta, l’approccio Multitrait-Multimethod (MTMM) per esaminare la validità esterna, incorporando la validità convergente e discriminante, arricchisce ulteriormente la comprensione della misura. L’uso del disegno MTMM permette di distinguere efficacemente tra costrutti correlati ma distinti, assicurando che le misure non solo riflettano accuratamente il costrutto target, ma siano anche discriminanti rispetto ad altri costrutti.\nIn sintesi, l’integrazione di CFA e SEM nel processo di validazione di costrutti e nella costruzione di scale psicometriche rappresenta un avanzamento metodologico significativo. Questi approcci non solo migliorano la precisione e la comprensione delle relazioni tra variabili osservabili e latenti, ma contribuiscono anche a elevare la qualità e l’affidabilità delle misure psicologiche. Attraverso un uso attento e informato di queste tecniche, i ricercatori possono arricchire la validità e l’utilità delle scale psicometriche. Chi volesse approfondire ulteriormente questi argomenti, può fare riferimento al testo di John & Benet-Martinez (2014).\n\n\n\n\nJohn, O. P., & Benet-Martinez, V. (2014). Measurement: Reliability, construct validation, and scale construction. In H. T. Reis & C. M. Judd (A c. Di), Handbook of research methods in social and personality psychology (2nd ed., pp. 473–503). Cambridge University Press.",
    "crumbs": [
      "Prefazione"
    ]
  },
  {
    "objectID": "cal_testing_psic_2025.html",
    "href": "cal_testing_psic_2025.html",
    "title": "Calendario Didattico e Programma del Corso",
    "section": "",
    "text": "Struttura degli Incontri\nIl calendario didattico prevede 14 incontri di 3 ore ciascuno. Nell’ultimo tratto del corso, ci sarà una verifica tramite Quiz Moodle (1 ora) e le presentazioni finali degli studenti negli ultimi due incontri.",
    "crumbs": [
      "Calendario",
      "Calendario Didattico e Programma del Corso"
    ]
  },
  {
    "objectID": "cal_testing_psic_2025.html#struttura-degli-incontri",
    "href": "cal_testing_psic_2025.html#struttura-degli-incontri",
    "title": "Calendario Didattico e Programma del Corso",
    "section": "",
    "text": "Incontro 1\n\nData: 4 marzo 2025\n\nOrario: 10:30-13:30\n\nArgomenti:\n\nPresentazione del corso.\n\nIntroduzione a \\(\\mathsf{R}\\).\n\n\nCommenti: Incontro introduttivo con panoramica sul corso e introduzione ai fondamenti di R.\n\n\n\n\n\nIncontro 2\n\nData: 6 marzo 2025\n\nOrario: 8:30-11:30\n\nArgomenti:\n\nPunteggi e scale: punteggi derivati, scaling Likert, codifica inversa.\n\nImputazione e calcolo del punteggio totale.\n\nOttimizzazione dello scoring dei dati ordinali.\n\nScaling di Thurstone e sviluppo dello strumento.\n\nEquating nei test psicologici.\n\nIntroduzione al modello lineare.\n\n\nCommenti: Approfondimento su metodi di scoring psicometrico e scaling.\n\n\n\n\n\nIncontro 3\n\nData: 11 marzo 2025\n\nOrario: 10:30-13:30\n\nArgomenti:\n\nTeoria Classica dei Test.\n\nRelazione con il modello lineare.\n\nErrore standard della misurazione.\n\nMisure congeneriche, tau-equivalenti e parallele.\n\nAffidabilità.\n\nStima del punteggio vero ed errore standard della stima.\n\nApplicazioni pratiche.\n\n\nCommenti: Panoramica completa sulla Teoria Classica dei Test.\n\n\n\n\n\nIncontro 4\n\nData: 13 marzo 2025\n\nOrario: 8:30-11:30\n\nArgomenti:\n\nModello di regressione logistica.\n\nMokken Scale Analysis.\n\nPredizione.\n\n\nCommenti: Introduzione ai modelli di regressione logistica e analisi di Mokken.\n\n\n\n\n\nIncontro 5\n\nData: 18 marzo 2025\n\nOrario: 10:30-13:30\n\nArgomenti:\n\nItem Response Theory (IRT).\n\nConcetti di validità.\n\n\nCommenti: Introduzione alla IRT e alla validità psicometrica.\n\n\n\n\n\nIncontro 6\n\nData: 20 marzo 2025\n\nOrario: 8:30-11:30\n\nArgomenti:\n\nPath Analysis.\n\nTutorial di (Clement & Bradley-Garcia, 2022).\n\nNetwork Analysis.\n\n\nCommenti: Approfondimento su Path Analysis e Network Analysis.\n\n\n\n\n\nIncontro 7\n\nData: 25 marzo 2025\n\nOrario: 10:30-13:30\n\nArgomenti:\n\nElementi di algebra lineare.\n\nAnalisi delle componenti principali (PCA).\n\n\nCommenti: Fondamenti di algebra lineare e analisi delle componenti principali.\n\n\n\n\n\nIncontro 8\n\nData: 27 marzo 2025\n\nOrario: 8:30-11:30\n\nArgomenti:\n\nAnalisi fattoriale esplorativa (EFA).\n\nIl modello statistico dell’analisi fattoriale.\n\n\nCommenti: Introduzione all’analisi fattoriale esplorativa e al modello statistico.\n\n\n\n\n\nIncontro 9\n\nData: 1 aprile 2025\n\nOrario: 10:30-13:30\n\nArgomenti:\n\nEstrazione dei fattori.\n\nRotazione dei fattori.\n\n\nCommenti: Tecniche di estrazione e rotazione dei fattori.\n\n\n\n\n\nIncontro 10\n\nData: 3 aprile 2025\n\nOrario: 8:30-11:30\n\nArgomenti:\n\nAnalisi fattoriale confermativa (CFA).\n\n\nCommenti: Introduzione all’analisi fattoriale confermativa e alle sue applicazioni.\n\n\n\n\n\nIncontro 11\n\nData: 8 aprile 2025\n\nOrario: 10:30-13:30\n\nArgomenti:\n\nIntroduzione ai modelli di equazioni strutturali (SEM).\n\n\nCommenti: Panoramica sui modelli di equazioni strutturali.\n\n\n\n\n\nIncontro 12\n\nData: 10 aprile 2025\n\nOrario: 8:30-11:30\n\nArgomenti:\n\nModelli multilivello.\n\nAttendibilità dei giudici.\n\nModelli di crescita latente (LGM).\n\n\nCommenti: Discussione sui modelli multilivello e sulla crescita latente.\n\n\n\n\nIncontro 13\n\nData: 15 aprile 2025\n\nOrario: 10:30-13:30\n\nArgomenti:\n\nVerifica tramite Quiz Moodle (1 ora).\n\nPresentazioni finali degli studenti.\n\n\nCommenti: Quiz di valutazione e presentazioni finali.\n\n\n\n\nIncontro 14\n\nData: 17 aprile 2025\n\nOrario: 8:30-11:30\n\nArgomenti:\n\nPresentazioni finali degli studenti.\n\n\nCommenti: Conclusione del corso con le presentazioni finali degli studenti.\n\n\n\n\n\nClement, L. M., & Bradley-Garcia, M. (2022). A step-by-step tutorial for performing a moderated mediation analysis using PROCESS. The Quantitative Methods for Psychology, 18(3), 258–271.",
    "crumbs": [
      "Calendario",
      "Calendario Didattico e Programma del Corso"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html",
    "href": "chapters/measurement/01_scores_scales.html",
    "title": "1  Punteggi e scale",
    "section": "",
    "text": "1.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nQuesto capitolo si propone di introdurre l’utilizzo del software “R”, ponendo l’attenzione sulla differenza tra valutazioni normative e criteriali.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#tipologie-di-dati",
    "href": "chapters/measurement/01_scores_scales.html#tipologie-di-dati",
    "title": "1  Punteggi e scale",
    "section": "\n1.2 Tipologie di Dati",
    "text": "1.2 Tipologie di Dati\nSi possono identificare quattro principali categorie di dati: nominali, ordinali, di intervallo e di rapporto. È opportuno notare che, in funzione dell’utilizzo della variabile, i dati possono rientrare in più di una categoria. La tipologia del dato influisce significativamente sulle modalità di analisi applicabili. A titolo esemplificativo, l’analisi statistica parametrica (come la regressione lineare) presuppone che i dati siano di intervallo o di rapporto.\n\n1.2.1 Dati Nominali\nI dati nominali si configurano come categorie distinte, caratterizzate da natura categorica e prive di ordinamento. Tali dati non esprimono affermazioni di natura quantitativa, bensì rappresentano entità nominabili (ad esempio, “felino” e “canino”). Sebbene possano essere rappresentati numericamente, come nel caso dei codici postali o dei codici identificativi di genere, etnia o razza dei partecipanti, è fondamentale sottolineare che valori numerici più elevati non riflettono livelli superiori (o inferiori) del costrutto, in quanto i numeri rappresentano meramente categorie prive di ordine intrinseco.\n\n1.2.2 Dati Ordinali\nI dati ordinali si distinguono per essere categorie ordinate: possiedono una denominazione e un ordine. Non forniscono informazioni sulla distanza concettuale tra i ranghi, ma indicano esclusivamente che valori più elevati rappresentano livelli superiori (o inferiori) del costrutto. Un esempio paradigmatico è costituito dalle posizioni in classifica successive a una competizione: il concorrente classificato al primo posto ha concluso la gara prima del secondo classificato, il quale a sua volta ha preceduto il terzo (1 &gt; 2 &gt; 3 &gt; 4). È cruciale evidenziare che la distanza concettuale tra numeri adiacenti non è necessariamente equivalente.\n\n1.2.3 Dati di Intervallo\nI dati di intervallo sono caratterizzati da un ordine e da distanze significative (ovvero, intervalli equidistanti). Questi dati consentono operazioni di somma (ad esempio, 2 dista 2 unità da 4), ma non di moltiplicazione (\\(2 \\times 2 \\ne 4\\)). Esempi emblematici sono le temperature espresse in gradi Fahrenheit o Celsius: 100 gradi Fahrenheit non equivalgono al doppio di 50 gradi Fahrenheit. È importante sottolineare che, sebbene in psicologia molti dati presentino la medesima distanza matematica tra gli intervalli, è probabile che tali intervalli non rappresentino la medesima distanza concettuale.\n\n1.2.4 Dati di Rapporto\nI dati di rapporto si distinguono per essere ordinati, caratterizzati da distanze significative e da uno zero assoluto che rappresenta l’assenza del costrutto. In questa tipologia di dati, le relazioni moltiplicative risultano valide. Un esempio paradigmatico è la temperatura espressa in gradi Kelvin: 100 gradi Kelvin corrispondono effettivamente al doppio di 50 gradi Kelvin. Nel campo della psicologia, l’aspirazione a disporre di scale di rapporto persiste, nonostante la difficoltà di definire uno zero assoluto per i costrutti psicologici: come si potrebbe, infatti, concettualizzare l’assenza totale di depressione?",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#punteggi-grezzi-e-trasformati",
    "href": "chapters/measurement/01_scores_scales.html#punteggi-grezzi-e-trasformati",
    "title": "1  Punteggi e scale",
    "section": "\n1.3 Punteggi Grezzi e Trasformati",
    "text": "1.3 Punteggi Grezzi e Trasformati\nNell’ambito dei test psicometrici, il punteggio grezzo costituisce la valutazione più immediata e si basa sulla somma delle risposte categorizzate, come quelle corrette o errate, o vero o falso. Nonostante la sua immediatezza, il punteggio grezzo presenta limitazioni interpretative, poiché non considera fattori contestuali quali il numero totale di domande o il livello di difficoltà di queste.\nPer mitigare queste limitazioni, i punteggi grezzi vengono spesso convertiti in formati che permettono un’interpretazione più contestualizzata, quali i punteggi standardizzati o scalati. Queste trasformazioni facilitano l’interpretazione dei risultati ottenuti.\nL’interpretazione dei risultati dei test necessita di un riferimento comparativo. A seconda del contesto, può essere utile confrontare le prestazioni con una norma di riferimento o con criteri specifici.\nUn altro approccio interpretativo è offerto dalla Teoria della Risposta agli Item (IRT), che fornisce un’analisi avanzata delle prestazioni nei test, permettendo un’esplorazione dettagliata delle risposte individuali.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#interpretazioni-basate-sulla-norma",
    "href": "chapters/measurement/01_scores_scales.html#interpretazioni-basate-sulla-norma",
    "title": "1  Punteggi e scale",
    "section": "\n1.4 Interpretazioni Basate sulla Norma",
    "text": "1.4 Interpretazioni Basate sulla Norma\nLa maggior parte dei test psicologici—come quelli di intelligenza, dello sviluppo, neuropsicologici e clinici—utilizza punteggi basati su norme di riferimento. In questi test, i punteggi grezzi vengono trasformati in punteggi normativi che esprimono la performance di un individuo in relazione a una popolazione di riferimento. Questo tipo di interpretazione permette di collocare il risultato del soggetto all’interno di una distribuzione normativa, fornendo una misura relativa rispetto a ciò che è considerato tipico o normale per quel gruppo. Mentre i test normativi offrono informazioni sulla posizione relativa di un soggetto rispetto a un gruppo, i test basati su criteri determinano se il soggetto ha raggiunto un livello specifico di competenza o conoscenza.\nUn aspetto cruciale in queste interpretazioni è la pertinenza del gruppo di riferimento. È fondamentale che questo gruppo sia rappresentativo degli individui a cui il test è destinato o con cui il partecipante viene confrontato. La selezione del campione normativo, o campione di standardizzazione, segue il principio del campionamento casuale stratificato proporzionale, assicurando che il campione rifletta proporzionalmente le caratteristiche demografiche nazionali. Questa rappresentatività è essenziale per un’interpretazione basata sulla norma valida, rendendo necessaria l’accurata selezione e descrizione del campione da parte degli sviluppatori del test.\nQuando si utilizzano questi test, è fondamentale valutare se il campione di standardizzazione è rappresentativo per l’uso previsto e se le caratteristiche demografiche del campione corrispondono a quelle dei soggetti testati. La pertinenza e l’attualità del campione, insieme alla sua dimensione, sono fattori chiave per garantire interpretazioni valide e affidabili.\nUn’altra considerazione importante riguarda l’importanza della standardizzazione nella somministrazione del test. È essenziale che il campione di riferimento sia stato sottoposto al test nelle stesse condizioni e secondo le stesse procedure amministrative che saranno utilizzate nella pratica effettiva. Pertanto, quando il test viene somministrato in contesti clinici, è cruciale che l’esaminatore segua attentamente le procedure amministrative prescritte. Ad esempio, nel caso di test standardizzati, è fondamentale leggere le istruzioni esattamente come fornite e rispettare rigorosamente i limiti di tempo. Sarebbe inappropriato confrontare la performance di un esaminando in un test a tempo con quella di un campione di standardizzazione che ha avuto più o meno tempo per completare gli item. Questa necessità di seguire procedure standardizzate si applica a tutti i test standardizzati, sia quelli con interpretazioni basate sulla norma che quelli basati sul criterio.\n\n1.4.1 Approccio Tradizionale vs Approccio Basato sulla Regressione per la Normazione\nUna caratteristica distintiva dei test psicologici con punteggi normati è che le norme spesso dipendono da caratteristiche specifiche come l’età, il sesso e/o il livello educativo. Ciò significa che esistono diverse popolazioni di riferimento, ciascuna con le proprie norme, che insieme costituiscono la popolazione normativa del test. Le norme vengono definite durante la fase di costruzione del test, basandosi sui punteggi raccolti in un campione rappresentativo.\nLa normazione dei punteggi grezzi è essenziale per interpretare le prestazioni di un individuo rispetto a un gruppo di riferimento. Quando i punteggi dipendono da variabili continue come l’età, è possibile adottare due approcci principali: il metodo tradizionale e quello basato sulla regressione. La figura seguente illustra le differenze tra questi due metodi (Timmerman et al., 2021).\n\n\n\n\n\nFigura 1.1: Punteggi di un campione normativo illustrativo (N = 1.660) del Test 14 dell’IDS-2 in funzione dell’età, con la mediana stimata per intervalli di età di 4 anni (pannello A) e di 1 anno (pannello B), e i limiti degli intervalli di confidenza al 95% (IC 95%) della mediana del punteggio del test, come calcolati con il metodo tradizionale di normazione, e come funzione non lineare continua dell’età utilizzata nella normazione basata sulla regressione (pannello C) (figura tratta da Timmerman et al., 2021).\n\n\n\n1.4.1.1 Normazione Tradizionale\nNell’approccio tradizionale, la variabile continua (ad esempio, l’età) viene suddivisa in intervalli discreti (ad esempio, fasce di età), e i punteggi normativi vengono calcolati per ciascun intervallo. Ad esempio:\n\nNel Pannello A, i punteggi grezzi del test sono suddivisi in intervalli di 4 anni. Per ogni fascia, viene calcolata la mediana e i relativi intervalli di confidenza al 95% (IC 95%).\nNel Pannello B, si utilizzano intervalli più stretti di 1 anno, ottenendo stime più precise.\n\nQuesto metodo implica che i punteggi grezzi all’interno di ogni intervallo siano trattati come equivalenti, assumendo implicitamente che la distribuzione dei punteggi sia costante per tutte le età nell’intervallo.\nProblemi dell’approccio tradizionale:\n\nDiscontinuità tra intervalli: Quando un soggetto si trova vicino al limite tra due intervalli, lo stesso punteggio grezzo può essere interpretato diversamente. Ad esempio, un bambino di 12 anni e 364 giorni potrebbe ricevere un’interpretazione diversa rispetto a uno di 13 anni e 1 giorno, nonostante la minima differenza di età.\nIrrealismo: La suddivisione in fasce può produrre salti “artificiali” nei punteggi normativi, non allineati con l’andamento reale secondo cui le prestazioni cambiano gradualmente con l’età.\nDifficoltà con intervalli ampi: Intervalli larghi, come quelli di 4 anni, rischiano di non catturare le variazioni sottili nella distribuzione dei punteggi legate all’età (Pannello A). Intervalli più stretti (Pannello B) migliorano la precisione, ma richiedono campioni molto più grandi per ciascun intervallo.\n\n1.4.1.2 Normazione Basata sulla Regressione\nL’approccio basato sulla regressione supera i limiti del metodo tradizionale modellando i punteggi grezzi come una funzione continua della variabile di riferimento (ad esempio, l’età). Nel Pannello C, la relazione tra i punteggi grezzi e l’età è rappresentata come una curva continua e non lineare, che:\n\nPermette di stimare i punteggi normativi per qualsiasi età.\nRiflette un cambiamento graduale e realistico dei punteggi rispetto all’età, evitando salti artificiali.\n\nVantaggi della regressione:\n\nContinuità: Non ci sono salti tra intervalli di età. I punteggi grezzi vengono interpretati in modo fluido e realistico, anche per età non incluse esplicitamente nei dati del campione.\nFlessibilità: Il modello può adattarsi a distribuzioni non lineari, catturando andamenti complessi nei dati senza richiedere un’eccessiva stratificazione del campione.\nEfficienza: Si richiedono meno dati, poiché non è necessario suddividere la popolazione in molti piccoli intervalli.\n\n1.4.1.3 Confronto tra i Metodi\nLa figura mostra chiaramente che:\n\nNel Pannello A (approccio tradizionale con intervalli ampi), le stime mediane sono grossolane e non catturano variazioni sottili.\nNel Pannello B (approccio tradizionale con intervalli più stretti), la precisione aumenta, ma a costo di richiedere campioni molto grandi.\nNel Pannello C (approccio basato sulla regressione), la curva continua offre una stima più accurata e senza discontinuità.\n\nIn conclusione, l’approccio basato sulla regressione rappresenta un metodo moderno e flessibile per la normazione dei punteggi nei test psicologici. Esso è in grado di catturare andamenti graduali nei punteggi grezzi e di evitare le discontinuità tipiche dell’approccio tradizionale. Questo metodo è particolarmente utile quando la variabile esplicativa, come l’età, influenza i punteggi in modo non lineare e continuo.\n\n1.4.2 Punteggi Derivati\nIn ambito psicometrico, i punteggi derivati da test possono assumere diverse forme, ciascuna con implicazioni specifiche per l’interpretazione dei dati. Esploreremo le tipologie più comuni:\n\n\nPunteggi Standardizzati:\n\nQuesti punteggi trasformano i punteggi grezzi (ad esempio, il numero di risposte corrette) in misure standardizzate. Ciò permette di ottenere valori invarianti rispetto a variabili come l’età dell’individuo.\nSi calcolano stabilendo una media e una deviazione standard specifiche a priori.\nEsempi:\n\n\nz-scores: Misurano la distanza di un punteggio dalla media, espressa in deviazioni standard. Hanno una media di 0 e una deviazione standard di 1.\n\nT-scores: Trasformano i punteggi in valori positivi, con una media di 50 e una deviazione standard di 10.\n\nPunteggi di QI: Tipici delle scale di intelligenza, hanno una media di 100 e una deviazione standard di 15.\n\n\n\n\n\nPunteggi Standardizzati Normalizzati:\n\nQuando i punteggi originali non seguono una distribuzione normale, si utilizzano trasformazioni non lineari per normalizzarli.\nEsempi:\n\n\nStanine: Suddividono i punteggi in 9 categorie (da 1 a 9).\n\nPunteggi scalati di Wechsler: Utilizzati nei test di intelligenza di Wechsler.\n\nEquivalenti della Curva Normale (NCE): Esprimono la posizione di un punteggio rispetto alla distribuzione normale.\n\n\n\n\n\nRanghi Percentili:\n\nVanno da 1 a 99 e indicano la posizione relativa di un soggetto rispetto alla popolazione.\nAd esempio, un punteggio al 75° percentile significa che il soggetto ha ottenuto un risultato migliore del 75% della popolazione.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#interpretazioni-basate-su-criteri",
    "href": "chapters/measurement/01_scores_scales.html#interpretazioni-basate-su-criteri",
    "title": "1  Punteggi e scale",
    "section": "\n1.5 Interpretazioni Basate su Criteri",
    "text": "1.5 Interpretazioni Basate su Criteri\nL’approccio delle valutazioni basate su criteri specifici è diventato sempre più rilevante nel mondo dell’educazione e della psicometria a partire dagli anni Sessanta. Questo approccio, noto anche come valutazione basata su contenuti, dominio o obiettivi, si concentra sulla misurazione delle competenze individuali rispetto a standard definiti, piuttosto che sul confronto con le prestazioni di un gruppo di riferimento.\nEcco alcune metodologie e applicazioni comuni:\n\n\nPercentuale di Risposte Corrette:\n\nQuesto metodo fornisce un’indicazione diretta delle competenze di uno studente.\nAd esempio, se uno studente risponde correttamente all’85% delle domande di matematica, l’insegnante può valutare le sue abilità in modo specifico.\n\n\n\nTest di Padronanza:\n\nQuesti test determinano se uno studente ha acquisito una competenza specifica.\nAd esempio, gli esami per la patente di guida valutano se lo studente ha raggiunto il livello di padronanza richiesto.\n\n\n\nValutazioni Basate su Standard:\n\nQueste valutazioni classificano i risultati in categorie di prestazione (ad esempio, base, competente, avanzato).\nSpesso, i punteggi vengono correlati a voti letterali basati su una percentuale di correttezza.\n\n\n\nI punti di forza delle valutazioni basate su criteri includono:\n\n\nComparazione con Standard Predefiniti:\n\nValutano il raggiungimento di competenze o obiettivi specifici, indipendentemente dalle prestazioni altrui.\nQuesto approccio evita il bias derivante dal confronto con altri studenti.\n\n\n\nFocalizzazione su Competenze Specifiche:\n\nQuesti test richiedono una definizione precisa dell’area di conoscenza o abilità valutata.\nSono ideali per valutare aree di contenuto specifiche.\n\n\n\n\n1.5.0.1 Benefici\n\n\nValutazione Mirata delle Competenze: Fornisce una verifica concreta del conseguimento delle conoscenze e abilità delineate dal programma di studi.\n\nPersonalizzazione dell’Insegnamento: Identifica le aree di debolezza, consentendo un approccio didattico più focalizzato e personalizzato.\n\nIn conclusione, le valutazioni basate su criteri rappresentano un’alternativa preziosa ai metodi di valutazione tradizionali, specialmente in contesti in cui è fondamentale misurare le competenze individuali. Questo approccio è in crescente adozione in ambiti educativi e formativi, enfatizzando l’importanza dell’acquisizione di conoscenze e abilità mirate.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#analisi-comparativa-tra-valutazioni-normative-e-basate-su-criteri",
    "href": "chapters/measurement/01_scores_scales.html#analisi-comparativa-tra-valutazioni-normative-e-basate-su-criteri",
    "title": "1  Punteggi e scale",
    "section": "\n1.6 Analisi Comparativa tra Valutazioni Normative e Basate su Criteri",
    "text": "1.6 Analisi Comparativa tra Valutazioni Normative e Basate su Criteri\nLa distinzione tra valutazioni normative (norm-referenced) e basate su criteri (criterion-referenced) è fondamentale per interpretare le prestazioni individuali nei test. Sebbene un test possa teoricamente adottare entrambi gli approcci interpretativi, di solito si orienta verso uno dei due, a seconda dell’obiettivo specifico.\nEcco una panoramica delle differenze:\n\n\nValutazioni Normative:\n\n\nVersatilità: Si applicano a test che valutano una vasta gamma di dimensioni, come attitudini, risultati scolastici, interessi, atteggiamenti e comportamenti.\n\nAmpio Quadro: Ideali per esplorare costrutti generali come l’attitudine generale o l’intelligenza.\n\nSelezione delle Domande: Preferiscono domande di difficoltà intermedia, evitando quelle troppo semplici o complesse.\n\n\n\nValutazioni Basate su Criteri:\n\n\nSpecificità: Associate principalmente a test che mirano a valutare conoscenze o competenze specifiche.\n\nFocalizzazione: Concentrate su abilità e competenze ben definite.\n\nCalibrazione delle Domande: La difficoltà delle domande è tarata in base alle conoscenze o abilità specifiche da valutare.\n\n\n\nÈ importante notare che queste interpretazioni non sono mutuamente esclusive. Alcuni test offrono sia valutazioni normative che basate su criteri, fornendo una visione completa delle prestazioni relative rispetto a un gruppo di riferimento e del livello di competenza in un ambito specifico. Questa dualità interpretativa è preziosa in vari contesti.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#analisi-dei-punteggi-secondo-la-teoria-della-risposta-agli-item",
    "href": "chapters/measurement/01_scores_scales.html#analisi-dei-punteggi-secondo-la-teoria-della-risposta-agli-item",
    "title": "1  Punteggi e scale",
    "section": "\n1.7 Analisi dei Punteggi secondo la Teoria della Risposta agli Item",
    "text": "1.7 Analisi dei Punteggi secondo la Teoria della Risposta agli Item\nLa Teoria della Risposta agli Item (IRT) rappresenta un notevole avanzamento nel campo della psicometria, fornendo strumenti essenziali per valutare con precisione le capacità e i tratti latenti degli individui.\nFondamenti e Principi dell’IRT: L’IRT si basa sull’assunto che ogni persona possieda un livello di un tratto latente, come l’intelligenza, che è indipendente dalle specifiche domande del test o dal metodo di valutazione utilizzato. Attraverso l’applicazione di modelli matematici complessi, l’IRT consente di posizionare ogni individuo su un continuum di tratto latente, offrendo una misurazione delle capacità più precisa rispetto ai tradizionali punteggi grezzi.\nVantaggi dei Punteggi basati sull’IRT: I punteggi derivati dall’IRT presentano significativi vantaggi. Essi sono trattati come punteggi a intervalli costanti, consentendo comparazioni valide tra le performance di soggetti o gruppi diversi. Inoltre, questi punteggi mantengono una deviazione standard uniforme attraverso diverse fasce d’età, rendendoli particolarmente adatti per monitorare l’evoluzione o il progresso delle abilità nel tempo.\nApplicazioni Pratiche e Prospettive Future dell’IRT: Una delle applicazioni più innovative dell’IRT è lo sviluppo dei test adattivi computerizzati (CAT), in cui le domande vengono selezionate dinamicamente in base alle risposte precedenti del candidato. Questo metodo consente valutazioni precise ed efficienti delle abilità in tempo reale. Ad esempio, i punteggi IRT, come i W-scores nel Woodcock-Johnson IV, vengono utilizzati per analizzare variazioni nelle capacità cognitive legate ai processi di apprendimento o ai declini cognitivi.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#la-selezione-del-punteggio-appropriato-per-la-valutazione",
    "href": "chapters/measurement/01_scores_scales.html#la-selezione-del-punteggio-appropriato-per-la-valutazione",
    "title": "1  Punteggi e scale",
    "section": "\n1.8 La Selezione del Punteggio Appropriato per la Valutazione",
    "text": "1.8 La Selezione del Punteggio Appropriato per la Valutazione\nDeterminare il tipo di punteggio più adeguato per un test è essenziale per ottenere informazioni specifiche e pertinenti dalla valutazione. Le diverse categorie di punteggi forniscono risposte a domande distinte riguardo alle prestazioni degli esaminandi:\n\n\nPunteggi Grezzi:\n\nRappresentano la quantità totale di risposte corrette accumulate da un individuo.\nOffrono una visione immediata del livello di prestazione e permettono di stabilire un ordine tra i partecipanti.\nSono utili per identificare rapidamente il posizionamento relativo di un individuo all’interno di un gruppo.\n\n\n\nPunteggi Norm-Referenced Standard:\n\nForniscono un confronto diretto tra le prestazioni di un individuo e quelle di un gruppo normativo.\nConsentono di interpretare la prestazione su una scala relativa, facilitando la comprensione del rendimento in termini di posizione all’interno di una popolazione di riferimento.\n\n\n\nPunteggi Criterion-Referenced:\n\nIndicano se un individuo ha raggiunto un determinato standard di competenza.\nSono particolarmente indicati per valutare il conseguimento di obiettivi specifici o competenze chiave.\n\n\n\nPunteggi Basati sull’IRT (Inclusi i Punteggi Rasch):\n\nOffrono una misurazione su scala a intervalli costanti, riflettendo la posizione di un individuo su un continuum di un tratto latente.\nSono ideali per tracciare il progresso nel tempo o confrontare le prestazioni attraverso diverse valutazioni di un medesimo tratto.\n\n\n\nAd esempio, nel caso di Giovanni, che ha beneficiato di un programma di supporto alla lettura:\n\n\nPunteggi Norm-Referenced: Fornirebbero insight su come le capacità di lettura di Giovanni si confrontano con quelle dei suoi coetanei dopo l’intervento.\n\nPunteggi Rasch o IRT: Consentirebbero di valutare l’evoluzione precisa delle competenze di lettura di Giovanni, misurando il progresso a partire dal suo livello iniziale.\n\nPunteggi Grezzi: Darebbero indicazioni sul miglioramento assoluto, sebbene privi della capacità di riflettere le variazioni in termini di difficoltà degli item o di altri fattori.\n\nPunteggi Criterion-Referenced: Stabilirebbero se Giovanni ha raggiunto specifici obiettivi di competenza in lettura definiti a priori.\n\nIn contesti educativi, l’uso di punteggi norm-referenced standardizzati per età può essere preferibile per determinare se uno studente sta progredendo adeguatamente rispetto ai suoi pari. In contesti clinici, come nella gestione della depressione, i punteggi criterion-referenced possono offrire una valutazione mirata del raggiungimento di soglie di miglioramento clinico significativo.\nIn conclusione, la scelta del tipo di punteggio da utilizzare è guidata dal contesto di valutazione e dall’obiettivo specifico della misurazione. Diverse tipologie di punteggi illuminano aspetti distinti delle prestazioni, rendendoli più o meno adatti a seconda delle esigenze informative della valutazione.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#significato-e-applicazione-delle-norme-e-dei-punteggi-standardizzati",
    "href": "chapters/measurement/01_scores_scales.html#significato-e-applicazione-delle-norme-e-dei-punteggi-standardizzati",
    "title": "1  Punteggi e scale",
    "section": "\n1.9 Significato e Applicazione delle Norme e dei Punteggi Standardizzati",
    "text": "1.9 Significato e Applicazione delle Norme e dei Punteggi Standardizzati\nPer chiarire questi concetti, esaminiamo i dati della Tabella 2.1 di Bandalos (2018). Con degli esempi numerici, analizzeremo vari tipi di punteggi normativi, tra cui:\n\n\nPunteggi Percentili: Che indicano la posizione relativa di un individuo all’interno del gruppo normativo.\n\nPunteggi Standardizzati e Normalizzati: Che trasformano i punteggi grezzi in una scala standard per facilitare il confronto tra diversi individui o gruppi.\n\nStanini: Un metodo di punteggio che divide i punteggi in intervalli standardizzati.\n\nEquivalenti alla Curva Normale: Che adattano i punteggi a una distribuzione normale.\n\nNei capitoli successivi esamineremo come calcolare i punteggi basati sulla teoria IRT.\nIniziamo a leggere i dati.\n\nraw_score &lt;- c(\n    26, 25, 33, 31, 26, 34, 29, 36, 25, 29, 28, 32, 25,\n    30, 27, 31, 30, 30, 35, 30, 27, 26, 34, 32, 26, 34,\n    30, 28, 28, 31, 30, 27, 26, 29, 29, 33, 27, 35, 26,\n    27, 28, 29, 28, 27, 34, 36, 26, 26, 34, 30, 34, 27\n)\n\n\n1.9.1 Distribuzione di frequenze\n\nfreq &lt;- table(raw_score) # frequency\ncumfreq &lt;- cumsum(freq) # cumulative frequency\nperc &lt;- prop.table(freq) * 100 # percentage\ncumperc &lt;- cumsum(perc) # cumulative percentage\npr &lt;- (cumperc - 0.5 * perc) # percentile rank\ncbind(freq, cumfreq, perc, cumperc, pr)\n#&gt;    freq cumfreq  perc cumperc    pr\n#&gt; 25    3       3  5.77    5.77  2.88\n#&gt; 26    8      11 15.38   21.15 13.46\n#&gt; 27    7      18 13.46   34.62 27.88\n#&gt; 28    5      23  9.62   44.23 39.42\n#&gt; 29    5      28  9.62   53.85 49.04\n#&gt; 30    7      35 13.46   67.31 60.58\n#&gt; 31    3      38  5.77   73.08 70.19\n#&gt; 32    2      40  3.85   76.92 75.00\n#&gt; 33    2      42  3.85   80.77 78.85\n#&gt; 34    6      48 11.54   92.31 86.54\n#&gt; 35    2      50  3.85   96.15 94.23\n#&gt; 36    2      52  3.85  100.00 98.08\n\n\n1.9.2 Punteggi Percentili\nI punteggi percentili sono un modo efficace per interpretare e confrontare i punteggi di un individuo con quelli di un campione normativo. Un punteggio percentile indica la posizione relativa di un individuo all’interno di un gruppo normativo. Più specificamente, un punteggio percentile mostra la percentuale di persone nel campione normativo che ha ottenuto un punteggio uguale o inferiore a quello dell’individuo in questione.\nPer esemplificare il concetto, consideriamo il calcolo di un quantile di ordine 0.74. Questo significa che stiamo cercando il valore al di sotto del quale si trova il 74% dei punteggi nel campione normativo. In altre parole, un individuo con un punteggio corrispondente a questo quantile ha superato il 74% delle persone nel gruppo normativo.\nIl calcolo dei punteggi percentili può essere effettuato attraverso l’analisi statistica dei dati di un campione rappresentativo. Questi dati vengono ordinati in modo crescente, e si identifica il punteggio che corrisponde al percentile desiderato. Nel caso del quantile 0.74, si cerca il punteggio che si trova alla posizione che corrisponde al 74% della lunghezza totale dell’elenco ordinato dei punteggi.\n\nquantile(raw_score, .74) \n#&gt;  74% \n#&gt; 31.7\n\n\n# Use a different type (see https://en.wikipedia.org/wiki/Quantile#Estimating_quantiles_from_a_sample)\nquantile(raw_score, .74, type = 6)\n#&gt; 74% \n#&gt;  32\n\nI punteggi percentili sono particolarmente utili perché offrono una comprensione intuitiva della posizione di un individuo rispetto agli altri. Tuttavia, è importante notare che essi rappresentano una scala ordinale e, pertanto, le differenze tra i punteggi percentili non sono necessariamente uniformi o proporzionali attraverso l’intera gamma di punteggi.\nIn conclusione, i punteggi percentili sono uno strumento fondamentale nella valutazione psicologica e educativa, poiché forniscono un modo diretto e facilmente interpretabile per valutare le prestazioni di un individuo in confronto a un campione normativo.\n\n1.9.3 Punteggi Standardizzati\nI punteggi standardizzati rappresentano una trasformazione essenziale nel campo della psicometria, che consente di convertire i punteggi grezzi ottenuti in un test in una scala unificata. Questa trasformazione permette di confrontare i risultati di individui o gruppi in maniera equa e coerente, superando le variazioni di scala o di difficoltà tra diversi test.\n\n1.9.3.1 Principi Fondamentali dei Punteggi Standardizzati\n\n\nMedia e Deviazione Standard Predefinite: I punteggi standardizzati sono calcolati in modo tale da avere una media e una deviazione standard specifiche, stabilite in anticipo. Per esempio, spesso si utilizza una media di 100 e una deviazione standard di 15 (come nei test di intelligenza) o una media di 0 e una deviazione standard di 1 (come negli z-score).\n\nRisultati Confrontabili: Attraverso questa standardizzazione, i punteggi diventano direttamente confrontabili. Un punteggio standardizzato rispetto a una media di 100 e una deviazione standard di 15, ad esempio, permette di valutare rapidamente se un punteggio è al di sopra, al di sotto o vicino alla media del campione normativo.\n\n1.9.3.2 Come Funziona la Trasformazione\nIl processo di standardizzazione implica la sottrazione della media del campione normativo dal punteggio grezzo di un individuo, seguita dalla divisione del risultato per la deviazione standard del campione normativo. In termini matematici, se $ X $ è un punteggio grezzo, $ $ è la media del campione normativo e $ $ è la deviazione standard del campione normativo, allora il punteggio standardizzato $ Z $ è calcolato come:\n\\[\nZ = \\frac{X - \\mu}{\\sigma}.\n\\]\n\n1.9.3.3 Utilità dei Punteggi Standardizzati\n\n\nComparabilità: Rendono i punteggi ottenuti da test diversi o da campioni diversi direttamente comparabili.\n\nInterpretazione Facilitata: Forniscono un modo semplice per interpretare i punteggi individuali in termini di posizione relativa rispetto alla media del campione normativo.\n\nAdattabilità: Sono utili in una varietà di contesti, da test educativi a valutazioni cliniche.\n\nIn conclusione, i punteggi standardizzati sono uno strumento cruciale nella psicometria e nella valutazione educativa. Trasformando i punteggi grezzi in una scala comune con media e deviazione standard specifiche, facilitano il confronto e l’interpretazione dei risultati dei test, rendendo più accessibile l’analisi e la valutazione delle prestazioni individuali e di gruppo.\nNel caso dell’esempio, i calcoli si svolgono in R nel modo seguente:\n\nz_score &lt;- (raw_score - mean(raw_score)) / sd(raw_score)\nc(mean = mean(z_score), sd = sd(z_score))\n#&gt;      mean        sd \n#&gt; -5.62e-16  1.00e+00\n\n\n1.9.3.4 Punteggi T\nI punteggi T sono una forma specifica di punteggi standardizzati, utilizzati frequentemente nella psicometria per rendere più accessibili e interpretabili i risultati dei test. A differenza dei punteggi z, che tipicamente hanno una media di 0 e una deviazione standard di 1, i punteggi T sono trasformati in modo da avere una media fissata a 50 e una deviazione standard di 10.\n\n1.9.3.5 Caratteristiche Principali dei Punteggi T\n\nMedia e Deviazione Standard: La media fissata a 50 e la deviazione standard di 10 sono scelte per offrire una scala più intuitiva e di facile lettura rispetto agli z-score. Questa trasformazione sposta la scala degli z-score in una gamma numericamente più familiare e più semplice da interpretare per la maggior parte delle persone.\n\nCalcolo dei Punteggi T: Il calcolo dei punteggi T avviene trasformando prima i punteggi grezzi in z-score e poi convertendo questi z-score nella scala dei punteggi T. Matematicamente, se $ Z $ è lo z-score, il punteggio T corrispondente $ T $ è calcolato come:\n\\[\nT = 50 + 10 \\times Z.\n\\]\nQuesta formula adatta lo z-score in una scala che inizia da 50 e si allarga in entrambe le direzioni con incrementi standard di 10 per ogni deviazione standard.\n\n\n1.9.3.6 Utilizzo dei Punteggi T\n\n\nFacilità di Interpretazione: I punteggi T sono particolarmente utili quando si desidera presentare i risultati dei test in un formato che sia immediatamente comprensibile, senza la necessità di ulteriori calcoli o trasformazioni.\n\nComparabilità: Consentono di confrontare i risultati di test diversi in modo più diretto, grazie alla loro scala standardizzata.\n\nAmpio Utilizzo: Sono ampiamente usati in vari ambiti della valutazione psicologica, inclusi l’educazione, la ricerca e la pratica clinica.\n\nIn sintesi, i punteggi T offrono un modo efficace e standardizzato per interpretare i risultati dei test, rendendo i dati più accessibili e immediatamente comprensibili. La loro trasformazione da z-score a una scala con media 50 e deviazione standard 10 facilita la comprensione e la comparazione dei punteggi tra diversi test e diversi individui.\nSvolgendo i calcoli in R otteniamo\n\nT_score &lt;- z_score * 10 + 50\nc(mean = mean(T_score), sd = sd(T_score))\n#&gt; mean   sd \n#&gt;   50   10\n\n\n1.9.4 Punteggi Stanini\nI punteggi Stanini (dall’inglese “standard nine”) rappresentano un metodo standardizzato per categorizzare i risultati dei test in psicometria, dividendoli in nove intervalli. Questa scala, progettata per semplificare l’interpretazione dei dati, permette di valutare la posizione relativa di un individuo all’interno di un gruppo di riferimento.\nCome funzionano? Ogni intervallo Stanine corrisponde a un range di punteggi grezzi, con un’ampiezza che può variare leggermente a seconda della distribuzione dei dati. Un punteggio Stanine di 5 indica una prestazione media, mentre valori più alti o più bassi indicano prestazioni rispettivamente superiori o inferiori alla media. È importante notare che i punteggi Stanini sono principalmente utilizzati per confronti relativi all’interno di un gruppo, piuttosto che per misurazioni assolute.\nCalcolo dei Punteggi Stanini. Per calcolare i punteggi Stanini, è necessario seguire alcuni passaggi:\n\nDeterminare Media e Deviazione Standard: Inizialmente, si calcolano la media e la deviazione standard dei dati del campione normativo.\n\nApplicare la Formula dei Punteggi Stanini: Per ogni punteggio grezzo, si applica la seguente formula per calcolare il punteggio Stanine corrispondente:\n\\[\n\\text{Stanine} = \\left( \\frac{\\text{Punteggio Grezzo} - \\text{Media}}{\\text{Deviazione Standard}} \\right) \\times 2 + 5.\n\\]\nQuesta formula trasforma il punteggio grezzo in un valore sulla scala dei punteggi Stanini.\n\nArrotondare al Numero Intero Più Vicino: Infine, si arrotonda il risultato al numero intero più vicino per ottenere il punteggio Stanini finale.\n\nI punteggi Stanini offrono diversi vantaggi:\n\nSemplicità: La scala a nove punti è facile da comprendere e memorizzare.\nRapidità: Permettono una valutazione rapida della performance.\nStandardizzazione: Consentono di confrontare i risultati ottenuti in test diversi o da gruppi diversi.\n\nLimitazioni:\nSebbene i punteggi Stanini siano uno strumento utile, è importante considerarne anche i limiti: essi assumono una distribuzione normale dei dati e quindi non sono adatti a tutti i tipi di test.\nPer l’esempio presente abbiamo:\n\nmean_score &lt;- mean(raw_score)\nsd_score &lt;- sd(raw_score)\n\nstanine_scores &lt;- round((raw_score - mean_score) / sd_score * 2 + 5)\nprint(stanine_scores)\n#&gt;  [1] 3 2 7 6 3 8 5 9 2 5 4 7 2 5 3 6 5 5 8 5 3 3 8 7 3 8 5 4 4 6 5 3 3 5 5 7\n#&gt; [37] 3 8 3 3 4 5 4 3 8 9 3 3 8 5 8 3\n\nÈ importante ricordare che la trasformazione in punti z non cambia la forma della distribuzione.\n\nplot(density(raw_score))\n\n\n\n\n\n\n\n\nplot(density(z_score))\n\n\n\n\n\n\n\n\nplot(density(T_score))\n\n\n\n\n\n\n\n\nplot(density(stanine_scores))\n\n\n\n\n\n\n\nLa seguente figura proposta da Petersen (2024) illustra le relazioni tra i punteggi stanini e altre tipologie di punteggi derivati.\n\n\n\n\n\nFigura 1.2: Varie scale normate (figura tratta da Petersen, 2024).\n\n\n\n1.9.5 Equivalenti alla Curva Normale (NCE)\nGli Equivalenti alla Curva Normale, noti come NCE (dall’inglese “Normal Curve Equivalents”), sono un tipo di punteggio standardizzato utilizzato in ambito psicometrico. Questi punteggi vengono calcolati per trasformare i punteggi grezzi ottenuti in un test in una scala che rifletta una distribuzione approssimativamente normale. L’obiettivo principale dei punteggi NCE è quello di rendere i punteggi di diverse misure o test direttamente confrontabili, mantenendo una distribuzione che si allinea strettamente con una curva normale standard.\n\n1.9.5.1 Caratteristiche dei Punteggi NCE\n\n\nDistribuzione Normalizzata: I punteggi NCE sono progettati per aderire a una distribuzione normale. Ciò significa che, a differenza di altri tipi di punteggi, i NCE si allineano più da vicino con le caratteristiche di una curva di distribuzione gaussiana, con la maggior parte dei punteggi concentrati intorno alla media e una distribuzione simmetrica verso gli estremi.\n\nFacilità di Comparazione: Grazie alla loro standardizzazione, i punteggi NCE consentono un confronto diretto e significativo tra le prestazioni in diversi test o misure. Questo è particolarmente utile in contesti educativi e clinici dove è necessario interpretare e confrontare i risultati di diversi test.\n\n1.9.5.2 Calcolo e Utilizzo dei Punteggi NCE\nIl calcolo dei punteggi NCE si basa sulla trasformazione dei punteggi grezzi in modo che si adattino a una distribuzione normalizzata. Questo processo implica l’uso di formule matematiche che riallineano i dati grezzi su una scala standard, considerando la media e la deviazione standard del campione normativo.\nUna volta calcolati, i punteggi NCE offrono una visione chiara e immediata delle prestazioni relative di un individuo o di un gruppo, rispetto a un campione normativo. Questo tipo di punteggio è particolarmente utile quando i punteggi grezzi provengono da distribuzioni che non seguono una curva normale, consentendo così un’interpretazione più accurata e standardizzata dei risultati.\n\n1.9.5.3 Applicazioni Pratiche dei Punteggi NCE\nI punteggi NCE trovano impiego in una varietà di contesti, tra cui:\n\n\nValutazioni Educative: In ambito scolastico, per confrontare le prestazioni degli studenti in test diversi.\n\nRicerca Psicologica: Per analizzare e confrontare i risultati di diversi studi o misure psicometriche.\n\nPratica Clinica: Nella valutazione di clienti o pazienti utilizzando diversi strumenti diagnostici.\n\nIn conclusione, i punteggi Equivalenti alla Curva Normale rappresentano uno strumento psicometrico potente per standardizzare e confrontare efficacemente i risultati di diversi test o misure, assicurando che questi siano interpretati all’interno di un quadro coerente e comparabile.\nPer i dati dell’esempio abbiamo:\n\n# Using normal quantile\nqnorm_pr &lt;- qnorm(pr / 100)\n# Convert raw scores\nnormalized_zscore &lt;- as.vector(qnorm_pr[as.character(raw_score)])\n\nIn alternativa, è possibile usare la trasformazione di Box-Cox, che è una tecnica parametrica che cerca di correggere le asimmetrie e trasformare i dati in una forma che approssima una distribuzione normale. È efficace per i dati positivi. La trasformazione è definita come segue:\n\\[\ny(\\lambda) = \\begin{cases} \\frac{x^\\lambda - 1}{\\lambda} & \\text{se } \\lambda \\neq 0 \\\\ \\log(x) & \\text{se } \\lambda = 0 \\end{cases},\n\\]\ndove \\(x\\) è il valore originale e \\(\\lambda\\) è il parametro di trasformazione che viene spesso trovato attraverso la massimizzazione della verosimiglianza.\nSupponiamo di voler utilizzare la trasformazione di Box-Cox sul nostro set di dati. La procedura è la seguente. Questo codice utilizza la funzione boxcox dal pacchetto MASS per trovare il valore di \\(\\lambda\\) che massimizza la log-verosimiglianza della trasformazione di Box-Cox applicata ai dati. Poi, utilizza questo \\(\\lambda\\) per trasformare i dati.\n\n# Dati di esempio\nset.seed(123) # Per rendere l'esempio riproducibile\ndata &lt;- raw_score\n\n# Trova il miglior lambda per la trasformazione di Box-Cox\nbc &lt;- boxcox(data ~ 1, lambda = seq(-2, 2, by = 0.1))\n\n# Calcola la trasformazione di Box-Cox con il lambda ottimale\nlambda_opt &lt;- bc$x[which.max(bc$y)]\ndata_transformed &lt;- (data^lambda_opt - 1) / lambda_opt\n\n\n\n\n\n\n\nAvendo trovato i dati trasformati con la procedura Box-Cox, li confrontiamo con gli Equivalenti alla Curva Normale (NCE) calcolati con la procedura usuale.\n\nplot(normalized_zscore, data_transformed)\n\n\n\n\n\n\n\n\nplot(density(normalized_zscore)) # the shape will be closer to normal\n\n\n\n\n\n\n\n\nplot(density((data_transformed - mean(data_transformed)) / sd(data_transformed)))\n\n\n\n\n\n\n\nSi osservi che i dati NCE presentano una distribuzione più simile a una curva a forma campanulare rispetto ai dati grezzi.\n\nlillie.test(normalized_zscore)\n#&gt; \n#&gt;  Lilliefors (Kolmogorov-Smirnov) normality test\n#&gt; \n#&gt; data:  normalized_zscore\n#&gt; D = 0.09, p-value = 0.4\n\n\nlillie.test(data_transformed)\n#&gt; \n#&gt;  Lilliefors (Kolmogorov-Smirnov) normality test\n#&gt; \n#&gt; data:  data_transformed\n#&gt; D = 0.1, p-value = 0.04",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#riflessioni-finali-sui-metodi-di-trasformazione-dei-punteggi",
    "href": "chapters/measurement/01_scores_scales.html#riflessioni-finali-sui-metodi-di-trasformazione-dei-punteggi",
    "title": "1  Punteggi e scale",
    "section": "\n1.10 Riflessioni Finali sui Metodi di Trasformazione dei Punteggi",
    "text": "1.10 Riflessioni Finali sui Metodi di Trasformazione dei Punteggi\nLa trasformazione dei punteggi grezzi in formati più interpretabili è una pratica cruciale in psicometria. Due sono i principali approcci utilizzati per attribuire significato ai punteggi di un test: il riferimento normativo e il riferimento criteriale.\n\n1.10.1 Riferimento Normativo\nNel riferimento normativo, si confronta il punteggio di un individuo con quello medio del gruppo normativo, ovvero gli altri soggetti che hanno svolto lo stesso test. Ci sono diversi tipi di punteggi normati, ciascuno con i suoi specifici vantaggi e limitazioni:\n\nPunteggi Percentili: Questi punteggi sono intuitivi e offrono un’indicazione immediata della posizione relativa di un individuo all’interno di un gruppo. Tuttavia, sono una scala ordinale e non si prestano bene a calcoli matematici più complessi.\nPunteggi Standardizzati: Gli z-score e i T-scores rientrano in questa categoria. Sono scalari a intervallo, quindi adatti a operazioni matematiche. Mantengono la forma distributiva originale dei punteggi grezzi, rendendo più agevole la loro elaborazione statistica.\n\n1.10.2 Riferimento Criteriale\nAl contrario del riferimento normativo, il riferimento criteriale confronta i punteggi di un individuo con uno standard prestabilito o un criterio specifico, piuttosto che con i punteggi di altri individui.\n\n1.10.3 Trasformazioni per la Normalizzazione\nPer ottenere una distribuzione dei punteggi più vicina alla curva normale, si possono utilizzare trasformazioni come gli stanini o gli NCE (Normal Curve Equivalents). Questi metodi di normalizzazione aiutano a standardizzare la distribuzione dei punteggi, facilitando così l’interpretazione e l’analisi dei dati psicometrici.\nIn conclusione, la scelta del metodo di trasformazione dei punteggi dipende dagli obiettivi specifici della valutazione e dall’interpretazione desiderata. La comprensione di queste diverse tecniche è essenziale per una corretta interpretazione dei risultati dei test e per l’analisi psicometrica più generale.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#riflessioni-conclusive",
    "href": "chapters/measurement/01_scores_scales.html#riflessioni-conclusive",
    "title": "1  Punteggi e scale",
    "section": "\n1.11 Riflessioni Conclusive",
    "text": "1.11 Riflessioni Conclusive\nQuesto capitolo fornisce una panoramica sui diversi tipi di punteggi dei test e il loro significato. Iniziamo notando che i punteggi grezzi, sebbene facili da calcolare, di solito forniscono poche informazioni utili sul rendimento di un esaminando in un test. Di conseguenza, di solito trasformiamo i punteggi grezzi in punteggi derivati, che possono essere di riferimento normativo o al criterio. I punteggi di riferimento normativo confrontano il rendimento di un esaminando con quello di altre persone nel campione di standardizzazione, mentre quelli al criterio confrontano il rendimento con un livello di competenza specificato. È importante valutare l’adeguatezza del campione di standardizzazione quando si utilizzano punteggi di riferimento normativo.\nPer interpretazioni basate sui punteggi di riferimento normativo, è utile conoscere la distribuzione normale e i punteggi standard di riferimento. Questi ultimi hanno una media predefinita e una deviazione standard. Esistono anche punteggi normalizzati quando i punteggi non seguono una distribuzione normale. Altri tipi di punteggi di riferimento normativo includono il rango percentile e i punteggi basati su età o livello di scolarità. Tuttavia, questi ultimi sono da evitare, se possibile, a favore di punteggi standard e ranghi percentile.\nI punteggi al criterio confrontano il rendimento con un livello specifico di competenza. Sono utili per valutare abilità in domini specifici, ma richiedono una chiara definizione del dominio. A volte, un test può produrre entrambi i tipi di punteggi. Forniamo anche una panoramica dei punteggi basati sulla teoria della risposta agli item (IRT), che sono utili per misurare i cambiamenti nel tempo.\nIn conclusione, i diversi tipi di punteggi dei test forniscono informazioni per rispondere a diverse domande e devono essere scelti in base alle esigenze specifiche dell’analisi dei dati del test.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#esercizi",
    "href": "chapters/measurement/01_scores_scales.html#esercizi",
    "title": "1  Punteggi e scale",
    "section": "Esercizi",
    "text": "Esercizi\n\n\n\n\n\n\nProblemi\n\n\n\n\n\nEsercizio 1: Classificazione delle variabili\n\nCreare un data frame in R contenente cinque variabili con dati simulati:\n\n\nID soggetto (codice numerico univoco, dati nominali).\n\nSesso (M/F, dati nominali).\n\nLivello di ansia (scala Likert da 1 a 5, dati ordinali).\n\nPunteggio QI (media = 100, deviazione standard = 15, dati di intervallo).\n\nTempo di reazione in secondi (valore continuo, dati di rapporto).\n\n\nVerificare la classe di ciascuna variabile con class() e discuterne la tipologia.\n\nEsercizio 2: Calcolo di Punteggi Standardizzati\n\nUtilizzando il data frame dell’Esercizio 1, standardizzare i punteggi QI e Tempo di Reazione con la formula dello z-score: \\[\nZ = \\frac{X - \\text{media}}{\\text{deviazione standard}}\n\\]\n\nVisualizzare la distribuzione dei punteggi grezzi e dei punteggi standardizzati con istogrammi.\n\nEsercizio 3: Calcolo di Punteggi Percentili\n\nUtilizzare i punteggi QI generati in precedenza.\nCalcolare il percentile rank per ciascun partecipante.\nIdentificare il punteggio QI corrispondente al percentile 90.\n\nEsercizio 4: Confronto tra Normazione Tradizionale e Basata sulla Regressione\n\nCreare una variabile di età compresa tra 18 e 65 anni.\nGenerare punteggi grezzi simulati di un test cognitivo.\nApplicare due metodi di normazione:\n\n\nMetodo tradizionale: Dividere il campione in fasce d’età (18-30, 31-45, 46-65) e calcolare la media e la deviazione standard per ciascuna fascia.\n\nMetodo della regressione: Applicare una regressione lineare per predire i punteggi in funzione dell’età.\n\n\n\nEsercizio 5: Simulazione della Distribuzione dei Punteggi Stanini\n\nGenerare una distribuzione di punteggi di un test (N = 1000) con media = 50 e deviazione standard = 10.\nTrasformare i punteggi in stanini utilizzando la formula: \\[\n\\text{Stanine} = \\left(\\frac{\\text{Punteggio} - \\text{Media}}{\\text{Deviazione Standard}}\\right) \\times 2 + 5\n\\]\n\nCreare un istogramma con la distribuzione dei punteggi stanini.\n\n\n\n\n\n\n\n\n\n\nSoluzioni\n\n\n\n\n\n\n# Esercizio 1: Classificazione delle variabili\n\nset.seed(123)  # Per riproducibilità\ndata &lt;- data.frame(\n  ID = 1:20,\n  Sesso = sample(c(\"M\", \"F\"), 20, replace = TRUE),\n  Ansia = factor(sample(1:5, 20, replace = TRUE), ordered = TRUE),  # Dati ordinali\n  QI = rnorm(20, mean = 100, sd = 15),  # Dati di intervallo\n  Tempo_Reazione = runif(20, min = 0.3, max = 2.5)  # Dati di rapporto\n)\n\n# Verifica della classe delle variabili\nsapply(data, class)\n#&gt; $ID\n#&gt; [1] \"integer\"\n#&gt; \n#&gt; $Sesso\n#&gt; [1] \"character\"\n#&gt; \n#&gt; $Ansia\n#&gt; [1] \"ordered\" \"factor\" \n#&gt; \n#&gt; $QI\n#&gt; [1] \"numeric\"\n#&gt; \n#&gt; $Tempo_Reazione\n#&gt; [1] \"numeric\"\n\n# Esercizio 2: Calcolo di Punteggi Standardizzati\ndata$QI_z &lt;- scale(data$QI)\ndata$Tempo_Reazione_z &lt;- scale(data$Tempo_Reazione)\n\n# Visualizzazione della distribuzione\npar(mfrow = c(2, 2))\nhist(data$QI, main = \"Distribuzione QI Grezzo\", col = \"lightblue\")\nhist(data$QI_z, main = \"Distribuzione QI Standardizzato\", col = \"lightgreen\")\nhist(data$Tempo_Reazione, main = \"Distribuzione Tempo di Reazione Grezzo\", col = \"lightblue\")\nhist(data$Tempo_Reazione_z, main = \"Distribuzione Tempo di Reazione Standardizzato\", col = \"lightgreen\")\npar(mfrow = c(1, 1))\n\n# Esercizio 3: Calcolo di Punteggi Percentili\ndata$Percentile_QI &lt;- ecdf(data$QI)(data$QI) * 100\n\n# Mostra i primi 10 percentili calcolati\nhead(data[, c(\"QI\", \"Percentile_QI\")], 10)\n#&gt;       QI Percentile_QI\n#&gt; 1   90.6            25\n#&gt; 2   74.7             5\n#&gt; 3  112.6            80\n#&gt; 4  102.3            55\n#&gt; 5   82.9            15\n#&gt; 6  118.8            95\n#&gt; 7  106.4            60\n#&gt; 8   95.6            40\n#&gt; 9  113.4            90\n#&gt; 10 113.2            85\n\n# Calcola il valore di QI corrispondente al 90° percentile\nquantile(data$QI, 0.90)\n#&gt; 90% \n#&gt; 114\n\n# Esercizio 4: Confronto tra Normazione Tradizionale e Basata sulla Regressione\nset.seed(123)\ndata$Eta &lt;- sample(18:65, 20, replace = TRUE)\ndata$Test_Cognitivo &lt;- 50 + 0.3 * data$Eta + rnorm(20, sd = 5)\n\n# Normazione Tradizionale: suddivisione in gruppi di età\ndata$Gruppo_Età &lt;- cut(data$Eta, breaks = c(18, 30, 45, 65), labels = c(\"18-30\", \"31-45\", \"46-65\"))\n\n# Calcolo della media e deviazione standard per ciascun gruppo\naggregate(Test_Cognitivo ~ Gruppo_Età, data = data, FUN = function(x) c(media = mean(x), sd = sd(x)))\n#&gt;   Gruppo_Età Test_Cognitivo.media Test_Cognitivo.sd\n#&gt; 1      18-30                57.79              3.37\n#&gt; 2      31-45                61.60              4.05\n#&gt; 3      46-65                62.27              2.64\n\n# Normazione basata sulla regressione\nmod &lt;- lm(Test_Cognitivo ~ Eta, data = data)\ndata$Test_Cognitivo_Pred &lt;- predict(mod)\n\n# Confronto tra metodi\nplot(data$Eta, data$Test_Cognitivo, main = \"Normazione Tradizionale vs Regressione\", pch = 19, col = \"blue\")\nlines(data$Eta, data$Test_Cognitivo_Pred, col = \"red\", lwd = 2)\nlegend(\"topright\", legend = c(\"Dati Osservati\", \"Modello Regressione\"), col = c(\"blue\", \"red\"), pch = c(19, NA), lwd = c(NA, 2))\n\n# Esercizio 5: Simulazione della Distribuzione dei Punteggi Stanini\nset.seed(123)\npunteggi &lt;- rnorm(1000, mean = 50, sd = 10)\nstanine &lt;- round((punteggi - mean(punteggi)) / sd(punteggi) * 2 + 5)\nstanine &lt;- pmax(pmin(stanine, 9), 1)\n\n# Visualizzazione della distribuzione\nhist(stanine, breaks = seq(0.5, 9.5, 1), main = \"Distribuzione dei Punteggi Stanini\", col = \"lightblue\", xlab = \"Stanine\", xaxt = \"n\")\naxis(1, at = 1:9, labels = 1:9)",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#session-info",
    "href": "chapters/measurement/01_scores_scales.html#session-info",
    "title": "1  Punteggi e scale",
    "section": "Session Info",
    "text": "Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] aspect_1.0-6      missForest_1.5    nortest_1.0-4     ggokabeito_0.1.0 \n#&gt;  [5] see_0.10.0        MASS_7.3-65       viridis_0.6.5     viridisLite_0.4.2\n#&gt;  [9] ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0  \n#&gt; [13] bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-19    \n#&gt; [17] psych_2.4.12      scales_1.3.0      markdown_1.13     knitr_1.49       \n#&gt; [21] lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n#&gt; [25] purrr_1.0.4       readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n#&gt; [29] ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1    jsonlite_1.9.1       magrittr_2.0.3      \n#&gt;   [4] TH.data_1.1-3        estimability_1.5.1   farver_2.1.2        \n#&gt;   [7] nloptr_2.1.1         rmarkdown_2.29       vctrs_0.6.5         \n#&gt;  [10] minqa_1.2.8          base64enc_0.1-3      rstatix_0.7.2       \n#&gt;  [13] itertools_0.1-3      htmltools_0.5.8.1    broom_1.0.7         \n#&gt;  [16] Formula_1.2-5        htmlwidgets_1.6.4    plyr_1.8.9          \n#&gt;  [19] sandwich_3.1-1       emmeans_1.10.7       zoo_1.8-13          \n#&gt;  [22] igraph_2.1.4         iterators_1.0.14     mime_0.12           \n#&gt;  [25] lifecycle_1.0.4      pkgconfig_2.0.3      Matrix_1.7-2        \n#&gt;  [28] R6_2.6.1             fastmap_1.2.0        rbibutils_2.3       \n#&gt;  [31] shiny_1.10.0         digest_0.6.37        OpenMx_2.21.13      \n#&gt;  [34] fdrtool_1.2.18       colorspace_2.1-1     rprojroot_2.0.4     \n#&gt;  [37] Hmisc_5.2-2          randomForest_4.7-1.2 timechange_0.3.0    \n#&gt;  [40] abind_1.4-8          compiler_4.4.2       rngtools_1.5.2      \n#&gt;  [43] withr_3.0.2          glasso_1.11          htmlTable_2.4.3     \n#&gt;  [46] backports_1.5.0      carData_3.0-5        ggsignif_0.6.4      \n#&gt;  [49] corpcor_1.6.10       gtools_3.9.5         tools_4.4.2         \n#&gt;  [52] pbivnorm_0.6.0       foreign_0.8-88       zip_2.3.2           \n#&gt;  [55] httpuv_1.6.15        nnet_7.3-20          glue_1.8.0          \n#&gt;  [58] quadprog_1.5-8       nlme_3.1-167         promises_1.3.2      \n#&gt;  [61] lisrelToR_0.3        grid_4.4.2           checkmate_2.3.2     \n#&gt;  [64] cluster_2.1.8        reshape2_1.4.4       generics_0.1.3      \n#&gt;  [67] gtable_0.3.6         tzdb_0.4.0           data.table_1.17.0   \n#&gt;  [70] hms_1.1.3            car_3.1-3            sem_3.1-16          \n#&gt;  [73] foreach_1.5.2        pillar_1.10.1        rockchalk_1.8.157   \n#&gt;  [76] later_1.4.1          splines_4.4.2        lattice_0.22-6      \n#&gt;  [79] survival_3.8-3       kutils_1.73          tidyselect_1.2.1    \n#&gt;  [82] miniUI_0.1.1.1       pbapply_1.7-2        reformulas_0.4.0    \n#&gt;  [85] stats4_4.4.2         xfun_0.51            qgraph_1.9.8        \n#&gt;  [88] arm_1.14-4           stringi_1.8.4        yaml_2.3.10         \n#&gt;  [91] pacman_0.5.1         boot_1.3-31          evaluate_1.0.3      \n#&gt;  [94] codetools_0.2-20     mi_1.1               cli_3.6.4           \n#&gt;  [97] RcppParallel_5.1.10  rpart_4.1.24         xtable_1.8-4        \n#&gt; [100] Rdpack_2.6.2         munsell_0.5.1        Rcpp_1.0.14         \n#&gt; [103] coda_0.19-4.1        png_0.1-8            XML_3.99-0.18       \n#&gt; [106] parallel_4.4.2       doRNG_1.8.6.1        jpeg_0.1-10         \n#&gt; [109] lme4_1.1-36          mvtnorm_1.3-3        openxlsx_4.2.8      \n#&gt; [112] rlang_1.1.5          multcomp_1.4-28      mnormt_2.1.1\n\n\n\n\n\nBandalos, D. L. (2018). Measurement theory and applications for the social sciences. Guilford Publications.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.\n\n\nTimmerman, M. E., Voncken, L., & Albers, C. J. (2021). A tutorial on regression-based norming of psychological tests with GAMLSS. Psychological Methods, 26(3), 357–373.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E1_likert.html",
    "href": "chapters/measurement/E1_likert.html",
    "title": "\n2  ✏ Esercizi\n",
    "section": "",
    "text": "2.1 Manipolazione di dati a livello di scala Likert\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nIn questo tutorial, ripreso da Brown (2023), esamineremo i dati di un questionario ordinale. In particolare, esamineremo il Strengths and Difficulties Questionnaire (SDQ), ovvero un breve questionario di screening comportamentale progettato per valutare i comportamenti di bambini e adolescenti tra i 3 e i 16 anni. Il SDQ è disponibile in diverse versioni per soddisfare le esigenze di ricercatori, clinici ed educatori. Per maggiori informazioni, è possibile consultare il sito ufficiale http://www.sdqinfo.org/, dove è possibile scaricare il questionario, insieme alle chiavi di scoring e alle norme pubblicate dal distributore del test.\nIl questionario include 25 item suddivisi in 5 scale (o dimensioni) che misurano specifici aspetti comportamentali. Ogni scala comprende 5 item:\nOgni item viene valutato dai partecipanti utilizzando le seguenti opzioni di risposta:\nAlcuni item nel SDQ rappresentano comportamenti che devono essere invertiti rispetto alla scala di appartenenza, ossia item a punteggio invertito. Questo significa che punteggi alti sulla scala corrispondono a punteggi bassi per questi specifici item. Ad esempio, l’item “Di solito faccio ciò che mi viene detto” (variabile “obeys”) è un item a punteggio invertito per la scala “Problemi di Condotta”.\nNel SDQ sono presenti 5 item di questo tipo, contrassegnati con un asterisco (*) nella tabella. Questi item devono essere codificati invertendo i punteggi (ad esempio, da 0 a 2 e viceversa) prima di calcolare il punteggio complessivo della scala.\nIn questo studio, i partecipanti sono studenti di prima media (Year 7) provenienti dalla stessa scuola, per un totale di 228 ragazzi. Si tratta di un campione della comunità scolastica, quindi non ci si aspetta che molti dei partecipanti superino le soglie cliniche indicate dal test.\nIl questionario SDQ è stato somministrato due volte:\nQuesta progettazione longitudinale consente di analizzare eventuali cambiamenti nei punteggi SDQ durante il passaggio tra il primo e il secondo anno di scuola secondaria.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>✏ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E1_likert.html#manipolazione-di-dati-a-livello-di-scala-likert",
    "href": "chapters/measurement/E1_likert.html#manipolazione-di-dati-a-livello-di-scala-likert",
    "title": "\n2  ✏ Esercizi\n",
    "section": "",
    "text": "Sintomi Emotivi: somatizzazione, preoccupazioni, infelicità, attaccamento, paura\n\n\nProblemi di Condotta: capricci, ubbidienza*, litigi, bugie, furti\n\n\nIperattività: irrequietezza, agitazione, distrazione, riflessione, attenzione\n\n\nProblemi con i Pari: solitudine, amicizia, popolarità, vittimismo, miglior amico più grande\n\n\nComportamento Prosociale: considerazione, condivisione, empatia, gentilezza, aiuto agli altri\n\n\n\n\n0 = “Non vero”\n\n\n1 = “Parzialmente vero”\n\n2 = “Assolutamente vero”\n\n\n\n\n\n\nLa prima somministrazione è avvenuta all’inizio della scuola secondaria, quando i ragazzi erano nel Year 7.\n\nLa seconda somministrazione è avvenuta un anno dopo, quando i partecipanti erano nel Year 8.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>✏ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E1_likert.html#emotional-symptoms-scale",
    "href": "chapters/measurement/E1_likert.html#emotional-symptoms-scale",
    "title": "\n2  ✏ Esercizi\n",
    "section": "\n2.2 Emotional Symptoms scale",
    "text": "2.2 Emotional Symptoms scale\nIniziamo ad esaminare la scala Emotional Symptoms. Questa scala non contiene item reverse. Importiamo i dati in R.\n\nload(\"../../data/data_sdq/SDQ.RData\")\nglimpse(SDQ)\n#&gt; Rows: 228\n#&gt; Columns: 51\n#&gt; $ Gender   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n#&gt; $ consid   &lt;dbl&gt; 1, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, …\n#&gt; $ restles  &lt;dbl&gt; 2, 0, 0, 0, 1, 0, 2, 1, 2, 0, 1, 1, 0, 1, 0, 2, 0, 1, 1, …\n#&gt; $ somatic  &lt;dbl&gt; 2, 2, 0, 0, 2, 1, 0, 0, 1, 0, 0, 2, 0, 0, 1, 2, 1, 1, 1, …\n#&gt; $ shares   &lt;dbl&gt; 1, 1, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, …\n#&gt; $ tantrum  &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 2, 0, 2, 0, 0, 1, 0, 1, 1, 2, 0, 1, 1, …\n#&gt; $ loner    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 1, 0, 0, 0, 1, 0, 0, 2, …\n#&gt; $ obeys    &lt;dbl&gt; 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, …\n#&gt; $ worries  &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 1, 2, 0, 1, 1, 2, …\n#&gt; $ caring   &lt;dbl&gt; 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, …\n#&gt; $ fidgety  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, …\n#&gt; $ friend   &lt;dbl&gt; 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, …\n#&gt; $ fights   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ unhappy  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 2, …\n#&gt; $ popular  &lt;dbl&gt; 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, …\n#&gt; $ distrac  &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 2, 0, 0, 1, …\n#&gt; $ clingy   &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 2, 0, 0, 0, 0, 1, 0, 2, 2, 1, 2, 0, 2, …\n#&gt; $ kind     &lt;dbl&gt; 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2, …\n#&gt; $ lies     &lt;dbl&gt; 0, 0, 0, 0, 2, 0, 1, 0, 2, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, …\n#&gt; $ bullied  &lt;dbl&gt; 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, …\n#&gt; $ helpout  &lt;dbl&gt; 2, 1, 2, 2, 0, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, …\n#&gt; $ reflect  &lt;dbl&gt; 1, 1, 2, 2, 0, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, …\n#&gt; $ steals   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, …\n#&gt; $ oldbest  &lt;dbl&gt; 1, 0, 2, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, …\n#&gt; $ afraid   &lt;dbl&gt; 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 2, 2, 0, 1, 1, 1, 0, 1, …\n#&gt; $ attends  &lt;dbl&gt; 2, 2, 1, 2, 0, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1, 1, …\n#&gt; $ consid2  &lt;dbl&gt; 1, 2, 2, 2, NA, 2, 2, 2, 2, 2, NA, 1, NA, 2, 2, NA, 1, 2,…\n#&gt; $ restles2 &lt;dbl&gt; 0, 1, 2, 1, NA, 0, 1, 1, 0, 0, NA, 2, NA, 0, 1, NA, 1, 1,…\n#&gt; $ somatic2 &lt;dbl&gt; 0, 1, 1, 0, NA, 0, 0, 0, 0, 0, NA, 2, NA, 0, 1, NA, 0, 1,…\n#&gt; $ shares2  &lt;dbl&gt; 1, 2, 2, 1, NA, 2, 1, 2, 2, 2, NA, 2, NA, 2, 2, NA, 1, 2,…\n#&gt; $ tantrum2 &lt;dbl&gt; 0, 1, 2, 0, NA, 0, 2, 0, 0, 0, NA, 2, NA, 0, 1, NA, 1, 0,…\n#&gt; $ loner2   &lt;dbl&gt; 0, 0, 1, 0, NA, 0, 0, 0, 0, 0, NA, 1, NA, 1, 0, NA, 0, 0,…\n#&gt; $ obeys2   &lt;dbl&gt; 2, 1, 2, 1, NA, 2, 2, 2, 2, 1, NA, 1, NA, 2, 1, NA, 1, 2,…\n#&gt; $ worries2 &lt;dbl&gt; 0, 0, 1, 0, NA, NA, 1, 0, 0, 0, NA, 1, NA, 1, 2, NA, 0, 0…\n#&gt; $ caring2  &lt;dbl&gt; 2, 2, 1, 2, NA, 2, 2, 2, 2, 2, NA, 2, NA, 2, 2, NA, 1, 2,…\n#&gt; $ fidgety2 &lt;dbl&gt; 0, 1, 0, 0, NA, 0, 1, 0, 0, 0, NA, 2, NA, 0, 0, NA, 1, 0,…\n#&gt; $ friend2  &lt;dbl&gt; 2, 2, 1, 2, NA, 2, 2, 2, 2, 2, NA, 2, NA, 1, 2, NA, 2, 2,…\n#&gt; $ fights2  &lt;dbl&gt; 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA, 2, NA, 0, 0, NA, 0, 0,…\n#&gt; $ unhappy2 &lt;dbl&gt; 0, 0, 1, 0, NA, 0, 0, 0, 0, 0, NA, 1, NA, 0, 0, NA, 0, 0,…\n#&gt; $ popular2 &lt;dbl&gt; 2, 1, 1, 2, NA, 2, 1, 2, 2, 2, NA, 2, NA, 2, 2, NA, 1, 2,…\n#&gt; $ distrac2 &lt;dbl&gt; 0, 0, 0, 2, NA, 0, 2, 1, 0, 0, NA, 1, NA, 0, 1, NA, 1, 0,…\n#&gt; $ clingy2  &lt;dbl&gt; 1, 1, 1, 0, NA, 1, 1, 1, 0, 0, NA, 1, NA, 0, 0, NA, 2, 0,…\n#&gt; $ kind2    &lt;dbl&gt; 2, 2, 2, 2, NA, 2, 2, 2, 2, 2, NA, 2, NA, 2, 2, NA, 1, 2,…\n#&gt; $ lies2    &lt;dbl&gt; 1, 0, 0, 0, NA, 0, 1, 0, 1, 0, NA, 1, NA, 0, 0, NA, 1, 0,…\n#&gt; $ bullied2 &lt;dbl&gt; 0, 0, 0, 0, NA, 0, 2, 0, 0, 0, NA, 0, NA, 0, 0, NA, 0, 0,…\n#&gt; $ helpout2 &lt;dbl&gt; 1, 1, 1, 2, NA, 2, 2, 1, 2, 1, NA, 2, NA, 2, 1, NA, 0, 2,…\n#&gt; $ reflect2 &lt;dbl&gt; 1, 1, 2, 1, NA, 2, 1, 2, 1, 2, NA, 1, NA, 2, 1, NA, 1, 2,…\n#&gt; $ steals2  &lt;dbl&gt; 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA, 2, NA, 0, 0, NA, 0, 0,…\n#&gt; $ oldbest2 &lt;dbl&gt; 0, 0, 1, 0, NA, 1, 0, 1, 1, 0, NA, 1, NA, 0, 0, NA, 0, 0,…\n#&gt; $ afraid2  &lt;dbl&gt; 0, 1, 0, 0, NA, 0, 0, 0, 0, 0, NA, 2, NA, 0, 0, NA, 0, 0,…\n#&gt; $ attends2 &lt;dbl&gt; 1, 1, 2, 0, NA, 2, 2, 2, 2, 1, NA, 1, NA, 2, 2, NA, 1, 1,…\n\nSelezioniamo solo gli item della Emotional Symptoms scale al tempo 1.\n\nitems_emotion &lt;- c(\"somatic\", \"worries\", \"unhappy\", \"clingy\", \"afraid\")\nsdq_emo &lt;- SDQ[, items_emotion]  \nsdq_emo |&gt;\n    head()\n#&gt; # A tibble: 6 × 5\n#&gt;   somatic worries unhappy clingy afraid\n#&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1       2       1       0      1      0\n#&gt; 2       2       0       0      1      0\n#&gt; 3       0       0       0      0      1\n#&gt; 4       0       0       0      1      1\n#&gt; 5       2       1       0      1      0\n#&gt; 6       1       0       0      1      0\n\nCalcoliamo il punteggio della scala.\n\nrowSums(sdq_emo) \n#&gt;   [1]  4  3  1  2  4  2  4  0  1  1  0  8  2  3  7  4  5  2  8  6  1  4  9\n#&gt;  [24]  4  5  9  0  3  3  1  0  2  6  3  9  4  4  0  7  1  3  6  4  5  4  1\n#&gt;  [47]  4  1  0  5  1  2  2  4  4  4  6  1  8  3  2  2  4  1  1  0  2  2  7\n#&gt;  [70]  5  0 NA NA  1  1  7  4  1  8  3  5  0  5  4  0  1  1  5  3  6  1  3\n#&gt;  [93]  2  6  6  0  2  4  5  3  3  1  1  7  2  3  5  5 NA  0  4  0  4  1  1\n#&gt; [116]  1  1  0  2  7  0  3  8  4  6 NA  2  4  7  1  0  0  1  0  4  3  0 10\n#&gt; [139]  5  2  1  6  1  2  1  0  1 NA  4  4  2  4  7  5  6  1  0  5  3  1  3\n#&gt; [162]  3  6  4  2  3  1  0  3  3  0  3  0  0  0  2  2  2  0  1  5  3  3  1\n#&gt; [185]  4  3  1  6  2  4  2 NA  0  2  5  5  0  2  2  3  4  0  2  4  2  2  1\n#&gt; [208]  3  2  0  1  0  0  8  1  1  2  1  2  2  4  0  0  1  2  2  1  6\n\nNotiamo che ci sono diversi punteggi mancanti, denotati da NA. Un primo metodo per affrontare i dati mancanti è semplicemente quello di ignorarli:\n\nrowSums(sdq_emo, na.rm = TRUE) \n#&gt;   [1]  4  3  1  2  4  2  4  0  1  1  0  8  2  3  7  4  5  2  8  6  1  4  9\n#&gt;  [24]  4  5  9  0  3  3  1  0  2  6  3  9  4  4  0  7  1  3  6  4  5  4  1\n#&gt;  [47]  4  1  0  5  1  2  2  4  4  4  6  1  8  3  2  2  4  1  1  0  2  2  7\n#&gt;  [70]  5  0  2  7  1  1  7  4  1  8  3  5  0  5  4  0  1  1  5  3  6  1  3\n#&gt;  [93]  2  6  6  0  2  4  5  3  3  1  1  7  2  3  5  5  4  0  4  0  4  1  1\n#&gt; [116]  1  1  0  2  7  0  3  8  4  6  0  2  4  7  1  0  0  1  0  4  3  0 10\n#&gt; [139]  5  2  1  6  1  2  1  0  1  4  4  4  2  4  7  5  6  1  0  5  3  1  3\n#&gt; [162]  3  6  4  2  3  1  0  3  3  0  3  0  0  0  2  2  2  0  1  5  3  3  1\n#&gt; [185]  4  3  1  6  2  4  2  4  0  2  5  5  0  2  2  3  4  0  2  4  2  2  1\n#&gt; [208]  3  2  0  1  0  0  8  1  1  2  1  2  2  4  0  0  1  2  2  1  6\n\nTuttavia, questa non è una buona idea. Anche per il fatto che, in questo modo non verrà calcolato il punteggio totale di 7 partecipanti. Possiamo identificare le colonne in cui ci sono dei valori mancanti usando summary().\n\nsummary(sdq_emo)\n#&gt;     somatic         worries         unhappy          clingy     \n#&gt;  Min.   :0.000   Min.   :0.000   Min.   :0.000   Min.   :0.000  \n#&gt;  1st Qu.:0.000   1st Qu.:0.000   1st Qu.:0.000   1st Qu.:0.000  \n#&gt;  Median :0.000   Median :0.000   Median :0.000   Median :1.000  \n#&gt;  Mean   :0.611   Mean   :0.621   Mean   :0.317   Mean   :0.842  \n#&gt;  3rd Qu.:1.000   3rd Qu.:1.000   3rd Qu.:1.000   3rd Qu.:1.000  \n#&gt;  Max.   :2.000   Max.   :2.000   Max.   :2.000   Max.   :2.000  \n#&gt;  NA's   :2       NA's   :1       NA's   :1                      \n#&gt;      afraid    \n#&gt;  Min.   :0.00  \n#&gt;  1st Qu.:0.00  \n#&gt;  Median :0.00  \n#&gt;  Mean   :0.48  \n#&gt;  3rd Qu.:1.00  \n#&gt;  Max.   :2.00  \n#&gt;  NA's   :3\n\nUn approccio semplice per gestire il problema dei dati mancanti è l’imputazione, che consiste nel sostituire i valori mancanti con stime plausibili basate sulle informazioni disponibili nel dataset. Il metodo più elementare di imputazione prevede la sostituzione del valore mancante con la media della colonna corrispondente. Questo approccio è facile da implementare e può essere utile come soluzione preliminare, ma potrebbe non catturare correttamente la variabilità e le relazioni tra le variabili.\n\nsdq_emo &lt;- sdq_emo %&gt;%\n    mutate_at(vars(somatic:afraid), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .))\n\nQuesta istruzione utilizza la funzione mutate_at del pacchetto dplyr per applicare una trasformazione a colonne specifiche (da somatic a afraid). All’interno della funzione di trasformazione, essa controlla se ogni valore è mancante (NA). Se lo è, lo sostituisce con la media della colonna usando mean(., na.rm = TRUE), che calcola la media escludendo eventuali valori mancanti.\nPossiamo ora calcolare il punteggio della scala per ciascun partecipante.\n\nSDQ$s_emotion &lt;- rowSums(sdq_emo) |&gt; round()\nSDQ$s_emotion \n#&gt;   [1]  4  3  1  2  4  2  4  0  1  1  0  8  2  3  7  4  5  2  8  6  1  4  9\n#&gt;  [24]  4  5  9  0  3  3  1  0  2  6  3  9  4  4  0  7  1  3  6  4  5  4  1\n#&gt;  [47]  4  1  0  5  1  2  2  4  4  4  6  1  8  3  2  2  4  1  1  0  2  2  7\n#&gt;  [70]  5  0  2  8  1  1  7  4  1  8  3  5  0  5  4  0  1  1  5  3  6  1  3\n#&gt;  [93]  2  6  6  0  2  4  5  3  3  1  1  7  2  3  5  5  5  0  4  0  4  1  1\n#&gt; [116]  1  1  0  2  7  0  3  8  4  6  0  2  4  7  1  0  0  1  0  4  3  0 10\n#&gt; [139]  5  2  1  6  1  2  1  0  1  4  4  4  2  4  7  5  6  1  0  5  3  1  3\n#&gt; [162]  3  6  4  2  3  1  0  3  3  0  3  0  0  0  2  2  2  0  1  5  3  3  1\n#&gt; [185]  4  3  1  6  2  4  2  5  0  2  5  5  0  2  2  3  4  0  2  4  2  2  1\n#&gt; [208]  3  2  0  1  0  0  8  1  1  2  1  2  2  4  0  0  1  2  2  1  6\n\nUn istogramma si ottiene nel modo seguente.\n\nSDQ |&gt;\n    ggplot(aes(x = s_emotion)) +\n    geom_histogram(bins = 10)\n\n\n\n\n\n\n\n\nhist(SDQ$s_emotion)\n\n\n\n\n\n\n\nPiù utile è un KDE plot.\n\nSDQ |&gt;\n    ggplot(aes(x = s_emotion)) +\n    geom_density()\n\n\n\n\n\n\n\nPossiamo ottenere le statistiche descrittive della scala usando la funzione describe del pacchetto psych.\n\ndescribe(SDQ$s_emotion)\n#&gt;    vars   n mean   sd median trimmed  mad min max range skew kurtosis   se\n#&gt; X1    1 228 2.87 2.31      2    2.65 2.97   0  10    10 0.72    -0.14 0.15\n\nCome si può vedere, la mediana (il punteggio al di sotto del quale si trova la metà del campione) di s_emotion è 2, mentre la media è più alta e pari a 2.87. Questo perché la distribuione dei punteggi è asimmetrica positiva; in questo caso, la mediana è più rappresentativa della tendenza centrale. Queste statistiche sono coerenti con la nostra osservazione dell’istogramma, che mostra un forte floor effect.\nDi seguito sono riportati i valori di soglia per i casi “Normali”, “Borderline” e “Anormali” per i Sintomi Emotivi forniti dal publisher del test (vedi https://sdqinfo.org/). Questi sono i punteggi che distinguono i casi probabilmente borderline e anormali dai casi “normali”.\nNormale: 0-5\nBorderline: 6\nAnormale: 7-10\n\ntable(SDQ$s_emotion &lt;= 5)\n#&gt; \n#&gt; FALSE  TRUE \n#&gt;    32   196\n\nIn questo campione, dunque, l’85% dei partecipanti è classificato nell’intervallo Normale.\n\ntable(SDQ$s_emotion &lt;= 5)[2] / length(SDQ$s_emotion)\n#&gt; TRUE \n#&gt; 0.86\n\nIn maniera equivalente otteniamo i valori dei partecipanti “borderline”:\n\ntable(SDQ$s_emotion == 6)[2] / length(SDQ$s_emotion)\n#&gt;  TRUE \n#&gt; 0.057\n\ne dei partecipanti “non-normali”:\n\ntable(SDQ$s_emotion &gt;= 7)[2] / length(SDQ$s_emotion)\n#&gt;   TRUE \n#&gt; 0.0833",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>✏ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E1_likert.html#item-reverse",
    "href": "chapters/measurement/E1_likert.html#item-reverse",
    "title": "\n2  ✏ Esercizi\n",
    "section": "\n2.3 Item reverse",
    "text": "2.3 Item reverse\nIn un secondo esempio consideriamo la codifica delle risposte degli item SDQ che misurano i Problemi di Condotta. Alcuni item sono stati codificati usando una codifica inversa. Prima di calcolare il punteggio totale è dunque necessario invertire il punteggio degli item a codifica inversa.\n\nitems_conduct &lt;- c(\"tantrum\", \"obeys\", \"fights\", \"lies\", \"steals\")\n\nPer i Problemi di Condotta, abbiamo solo un item reverse, obeys.\ntantrum    obeys*      fights       lies       steals\nPer invertire il codice di questo item, useremo una funzione dedicata del pacchetto psych, reverse.code(). Questa funzione ha la forma generale reverse.code(keys, items,…). L’argomento keys è un vettore di valori 1 o -1, dove -1 implica l’inversione dell’item. L’argomento items sono i nomi delle variabili che vogliamo valutare.\n\nR_conduct &lt;- reverse.code(keys = c(1, -1, 1, 1, 1), SDQ[, items_conduct]) |&gt; \n  as_tibble()\nR_conduct |&gt; \n  head()\n#&gt; # A tibble: 6 × 5\n#&gt;   tantrum `obeys-` fights  lies steals\n#&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1       0        0      0     0      0\n#&gt; 2       0        0      0     0      0\n#&gt; 3       0        0      0     0      0\n#&gt; 4       0        0      0     0      0\n#&gt; 5       1        2      0     2      0\n#&gt; 6       0        0      0     0      0\n\n\nSDQ[, items_conduct] |&gt; head()\n#&gt; # A tibble: 6 × 5\n#&gt;   tantrum obeys fights  lies steals\n#&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1       0     2      0     0      0\n#&gt; 2       0     2      0     0      0\n#&gt; 3       0     2      0     0      0\n#&gt; 4       0     2      0     0      0\n#&gt; 5       1     0      0     2      0\n#&gt; 6       0     2      0     0      0\n\nAnche in questo caso ci sono dei dati mancanti.\n\nsummary(R_conduct)\n#&gt;     tantrum          obeys-          fights           lies      \n#&gt;  Min.   :0.000   Min.   :0.000   Min.   :0.000   Min.   :0.000  \n#&gt;  1st Qu.:0.000   1st Qu.:0.000   1st Qu.:0.000   1st Qu.:0.000  \n#&gt;  Median :0.000   Median :1.000   Median :0.000   Median :0.000  \n#&gt;  Mean   :0.571   Mean   :0.579   Mean   :0.193   Mean   :0.544  \n#&gt;  3rd Qu.:1.000   3rd Qu.:1.000   3rd Qu.:0.000   3rd Qu.:1.000  \n#&gt;  Max.   :2.000   Max.   :2.000   Max.   :2.000   Max.   :2.000  \n#&gt;  NA's   :2                                       NA's   :2      \n#&gt;      steals     \n#&gt;  Min.   :0.000  \n#&gt;  1st Qu.:0.000  \n#&gt;  Median :0.000  \n#&gt;  Mean   :0.185  \n#&gt;  3rd Qu.:0.000  \n#&gt;  Max.   :2.000  \n#&gt;  NA's   :1\n\nUsiamo la stessa procedura descritta in precedenza:\n\nR_conduct &lt;- R_conduct %&gt;%\n    mutate_at(\n      vars(tantrum:steals), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)\n    )\n\nCalcoliamo ora il punteggio totale.\n\nSDQ$s_conduct &lt;- rowMeans(R_conduct)\n\n\nSDQ |&gt;\n    ggplot(aes(x = s_conduct)) +\n    geom_histogram(bins = 10)",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>✏ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E1_likert.html#session-info",
    "href": "chapters/measurement/E1_likert.html#session-info",
    "title": "\n2  ✏ Esercizi\n",
    "section": "\n2.4 Session Info",
    "text": "2.4 Session Info\n\nsessionInfo() \n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] ggokabeito_0.1.0  see_0.10.0        MASS_7.3-65       viridis_0.6.5    \n#&gt;  [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n#&gt;  [9] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n#&gt; [13] lavaan_0.6-19     psych_2.4.12      scales_1.3.0      markdown_1.13    \n#&gt; [17] knitr_1.49        lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n#&gt; [21] dplyr_1.1.4       purrr_1.0.4       readr_2.1.5       tidyr_1.3.1      \n#&gt; [25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.0      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] mime_0.12           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [25] Matrix_1.7-2        R6_2.6.1            fastmap_1.2.0      \n#&gt;  [28] rbibutils_2.3       shiny_1.10.0        digest_0.6.37      \n#&gt;  [31] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n#&gt;  [34] rprojroot_2.0.4     Hmisc_5.2-2         labeling_0.4.3     \n#&gt;  [37] timechange_0.3.0    abind_1.4-8         compiler_4.4.2     \n#&gt;  [40] withr_3.0.2         glasso_1.11         htmlTable_2.4.3    \n#&gt;  [43] backports_1.5.0     carData_3.0-5       ggsignif_0.6.4     \n#&gt;  [46] corpcor_1.6.10      gtools_3.9.5        tools_4.4.2        \n#&gt;  [49] pbivnorm_0.6.0      foreign_0.8-88      zip_2.3.2          \n#&gt;  [52] httpuv_1.6.15       nnet_7.3-20         glue_1.8.0         \n#&gt;  [55] quadprog_1.5-8      nlme_3.1-167        promises_1.3.2     \n#&gt;  [58] lisrelToR_0.3       grid_4.4.2          checkmate_2.3.2    \n#&gt;  [61] cluster_2.1.8       reshape2_1.4.4      generics_0.1.3     \n#&gt;  [64] gtable_0.3.6        tzdb_0.4.0          data.table_1.17.0  \n#&gt;  [67] hms_1.1.3           car_3.1-3           sem_3.1-16         \n#&gt;  [70] pillar_1.10.1       rockchalk_1.8.157   later_1.4.1        \n#&gt;  [73] splines_4.4.2       lattice_0.22-6      survival_3.8-3     \n#&gt;  [76] kutils_1.73         tidyselect_1.2.1    miniUI_0.1.1.1     \n#&gt;  [79] pbapply_1.7-2       reformulas_0.4.0    stats4_4.4.2       \n#&gt;  [82] xfun_0.51           qgraph_1.9.8        arm_1.14-4         \n#&gt;  [85] stringi_1.8.4       yaml_2.3.10         pacman_0.5.1       \n#&gt;  [88] boot_1.3-31         evaluate_1.0.3      codetools_0.2-20   \n#&gt;  [91] mi_1.1              cli_3.6.4           RcppParallel_5.1.10\n#&gt;  [94] rpart_4.1.24        xtable_1.8-4        Rdpack_2.6.2       \n#&gt;  [97] munsell_0.5.1       Rcpp_1.0.14         coda_0.19-4.1      \n#&gt; [100] png_0.1-8           XML_3.99-0.18       parallel_4.4.2     \n#&gt; [103] jpeg_0.1-10         lme4_1.1-36         mvtnorm_1.3-3      \n#&gt; [106] openxlsx_4.2.8      rlang_1.1.5         multcomp_1.4-28    \n#&gt; [109] mnormt_2.1.1\n\n\n\n\n\nBrown, A. (2023). Psychometrics in Exercises using R and RStudio: Textbook and Data Resource. https://bookdown.org/annabrown/psychometricsR",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>✏ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E2_optimal_scoring.html",
    "href": "chapters/measurement/E2_optimal_scoring.html",
    "title": "\n3  ✏️ Esercizi\n",
    "section": "",
    "text": "3.1 Ottimizzazione dello scoring dei dati di questionari ordinali\nNell’Esercizio precedente, abbiamo calcolato i punteggi del Strength and Difficulties Questionnaire (SDQ) utilizzando il cosiddetto approccio della “scalatura Likert”. In questo metodo, alle categorie di risposta “Non vero”, “Parzialmente vero” e “Assolutamente vero” sono stati assegnati interi consecutivi, rispettivamente 0-1-2. Sebbene questo assegnamento rifletta apparentemente un grado crescente di accordo nelle opzioni di risposta, la scelta degli interi è stata arbitraria: non vi era un motivo particolare per assegnare 0-1-2 anziché, ad esempio, 1-2-3. Questo tipo di assegnazione arbitraria dei punteggi agli item è comunemente chiamato “misurazione per decreto” (measurement by fiat).\nIn questo secondo esercizio, cercheremo di individuare punteggi “ottimali” per le risposte ordinali al SDQ. Per “ottimali” intendiamo che i punteggi assegnati non siano semplicemente arbitrari, ma rappresentino la “migliore” scelta possibile in base a un determinato criterio statistico.\nEsistono diversi modi per “ottimizzare” i punteggi degli item. In questo caso, ci concentreremo sulla massimizzazione del rapporto tra la varianza del punteggio totale e la somma delle varianze dei punteggi degli item. In psicometria, soddisfare questo criterio significa massimizzare la somma delle correlazioni tra gli item e, di conseguenza, migliorare la consistenza interna del test, misurata, ad esempio, dall’alfa di Cronbach.\nQuesto approccio consente di definire punteggi più informativi, che riflettono meglio la coerenza tra le risposte degli item e il punteggio totale del test, migliorando la qualità psicometrica della scala.\nPer fare un esempio, useremo di nuovo gli item della scala Sintomi Emotivi. Utilizzeremo il pacchetto aspect, che semplifica l’ottimizzazione della scalatura grazie a una gamma di opzioni utili e a funzioni grafiche integrate.\nsource(\"../../code/_common.R\")\nlibrary(\"aspect\")\nImportiamo i dati del Strengths and Difficulties Questionnaire (SDQ).\nload(\"../../data/data_sdq/SDQ.RData\")\nglimpse(SDQ)\n#&gt; Rows: 228\n#&gt; Columns: 51\n#&gt; $ Gender   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n#&gt; $ consid   &lt;dbl&gt; 1, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, …\n#&gt; $ restles  &lt;dbl&gt; 2, 0, 0, 0, 1, 0, 2, 1, 2, 0, 1, 1, 0, 1, 0, 2, 0, 1, 1, …\n#&gt; $ somatic  &lt;dbl&gt; 2, 2, 0, 0, 2, 1, 0, 0, 1, 0, 0, 2, 0, 0, 1, 2, 1, 1, 1, …\n#&gt; $ shares   &lt;dbl&gt; 1, 1, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, …\n#&gt; $ tantrum  &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 2, 0, 2, 0, 0, 1, 0, 1, 1, 2, 0, 1, 1, …\n#&gt; $ loner    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 1, 0, 0, 0, 1, 0, 0, 2, …\n#&gt; $ obeys    &lt;dbl&gt; 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, …\n#&gt; $ worries  &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 1, 2, 0, 1, 1, 2, …\n#&gt; $ caring   &lt;dbl&gt; 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, …\n#&gt; $ fidgety  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, …\n#&gt; $ friend   &lt;dbl&gt; 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, …\n#&gt; $ fights   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ unhappy  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 2, …\n#&gt; $ popular  &lt;dbl&gt; 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, …\n#&gt; $ distrac  &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 2, 0, 0, 1, …\n#&gt; $ clingy   &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 2, 0, 0, 0, 0, 1, 0, 2, 2, 1, 2, 0, 2, …\n#&gt; $ kind     &lt;dbl&gt; 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2, …\n#&gt; $ lies     &lt;dbl&gt; 0, 0, 0, 0, 2, 0, 1, 0, 2, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, …\n#&gt; $ bullied  &lt;dbl&gt; 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, …\n#&gt; $ helpout  &lt;dbl&gt; 2, 1, 2, 2, 0, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, …\n#&gt; $ reflect  &lt;dbl&gt; 1, 1, 2, 2, 0, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, …\n#&gt; $ steals   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, …\n#&gt; $ oldbest  &lt;dbl&gt; 1, 0, 2, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, …\n#&gt; $ afraid   &lt;dbl&gt; 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 2, 2, 0, 1, 1, 1, 0, 1, …\n#&gt; $ attends  &lt;dbl&gt; 2, 2, 1, 2, 0, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1, 1, …\n#&gt; $ consid2  &lt;dbl&gt; 1, 2, 2, 2, NA, 2, 2, 2, 2, 2, NA, 1, NA, 2, 2, NA, 1, 2,…\n#&gt; $ restles2 &lt;dbl&gt; 0, 1, 2, 1, NA, 0, 1, 1, 0, 0, NA, 2, NA, 0, 1, NA, 1, 1,…\n#&gt; $ somatic2 &lt;dbl&gt; 0, 1, 1, 0, NA, 0, 0, 0, 0, 0, NA, 2, NA, 0, 1, NA, 0, 1,…\n#&gt; $ shares2  &lt;dbl&gt; 1, 2, 2, 1, NA, 2, 1, 2, 2, 2, NA, 2, NA, 2, 2, NA, 1, 2,…\n#&gt; $ tantrum2 &lt;dbl&gt; 0, 1, 2, 0, NA, 0, 2, 0, 0, 0, NA, 2, NA, 0, 1, NA, 1, 0,…\n#&gt; $ loner2   &lt;dbl&gt; 0, 0, 1, 0, NA, 0, 0, 0, 0, 0, NA, 1, NA, 1, 0, NA, 0, 0,…\n#&gt; $ obeys2   &lt;dbl&gt; 2, 1, 2, 1, NA, 2, 2, 2, 2, 1, NA, 1, NA, 2, 1, NA, 1, 2,…\n#&gt; $ worries2 &lt;dbl&gt; 0, 0, 1, 0, NA, NA, 1, 0, 0, 0, NA, 1, NA, 1, 2, NA, 0, 0…\n#&gt; $ caring2  &lt;dbl&gt; 2, 2, 1, 2, NA, 2, 2, 2, 2, 2, NA, 2, NA, 2, 2, NA, 1, 2,…\n#&gt; $ fidgety2 &lt;dbl&gt; 0, 1, 0, 0, NA, 0, 1, 0, 0, 0, NA, 2, NA, 0, 0, NA, 1, 0,…\n#&gt; $ friend2  &lt;dbl&gt; 2, 2, 1, 2, NA, 2, 2, 2, 2, 2, NA, 2, NA, 1, 2, NA, 2, 2,…\n#&gt; $ fights2  &lt;dbl&gt; 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA, 2, NA, 0, 0, NA, 0, 0,…\n#&gt; $ unhappy2 &lt;dbl&gt; 0, 0, 1, 0, NA, 0, 0, 0, 0, 0, NA, 1, NA, 0, 0, NA, 0, 0,…\n#&gt; $ popular2 &lt;dbl&gt; 2, 1, 1, 2, NA, 2, 1, 2, 2, 2, NA, 2, NA, 2, 2, NA, 1, 2,…\n#&gt; $ distrac2 &lt;dbl&gt; 0, 0, 0, 2, NA, 0, 2, 1, 0, 0, NA, 1, NA, 0, 1, NA, 1, 0,…\n#&gt; $ clingy2  &lt;dbl&gt; 1, 1, 1, 0, NA, 1, 1, 1, 0, 0, NA, 1, NA, 0, 0, NA, 2, 0,…\n#&gt; $ kind2    &lt;dbl&gt; 2, 2, 2, 2, NA, 2, 2, 2, 2, 2, NA, 2, NA, 2, 2, NA, 1, 2,…\n#&gt; $ lies2    &lt;dbl&gt; 1, 0, 0, 0, NA, 0, 1, 0, 1, 0, NA, 1, NA, 0, 0, NA, 1, 0,…\n#&gt; $ bullied2 &lt;dbl&gt; 0, 0, 0, 0, NA, 0, 2, 0, 0, 0, NA, 0, NA, 0, 0, NA, 0, 0,…\n#&gt; $ helpout2 &lt;dbl&gt; 1, 1, 1, 2, NA, 2, 2, 1, 2, 1, NA, 2, NA, 2, 1, NA, 0, 2,…\n#&gt; $ reflect2 &lt;dbl&gt; 1, 1, 2, 1, NA, 2, 1, 2, 1, 2, NA, 1, NA, 2, 1, NA, 1, 2,…\n#&gt; $ steals2  &lt;dbl&gt; 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA, 2, NA, 0, 0, NA, 0, 0,…\n#&gt; $ oldbest2 &lt;dbl&gt; 0, 0, 1, 0, NA, 1, 0, 1, 1, 0, NA, 1, NA, 0, 0, NA, 0, 0,…\n#&gt; $ afraid2  &lt;dbl&gt; 0, 1, 0, 0, NA, 0, 0, 0, 0, 0, NA, 2, NA, 0, 0, NA, 0, 0,…\n#&gt; $ attends2 &lt;dbl&gt; 1, 1, 2, 0, NA, 2, 2, 2, 2, 1, NA, 1, NA, 2, 2, NA, 1, 1,…\nPer analizzare solo gli item che misurano i Sintomi Emotivi, è conveniente creare un nuovo data frame.\nitems_emotion &lt;- c(\"somatic\", \"worries\", \"unhappy\", \"clingy\", \"afraid\")\nsdq_emo &lt;- SDQ[, items_emotion]\nsdq_emo |&gt;\n    head()\n#&gt; # A tibble: 6 × 5\n#&gt;   somatic worries unhappy clingy afraid\n#&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1       2       1       0      1      0\n#&gt; 2       2       0       0      1      0\n#&gt; 3       0       0       0      0      1\n#&gt; 4       0       0       0      1      1\n#&gt; 5       2       1       0      1      0\n#&gt; 6       1       0       0      1      0\nAffrontiamo il problema dei dati mancanti come discusso in precedenza.\nsdq_emo &lt;- sdq_emo %&gt;%\n    mutate_at(\n      vars(somatic:afraid), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)\n      ) |&gt; \n  round()\nEsaminiamo le modalità di ciascun item:\nemotional_symptoms &lt;- c(\"somatic\", \"worries\", \"unhappy\", \"clingy\", \"afraid\")\nresult &lt;- lapply(emotional_symptoms, function(x) sort(unique(sdq_emo[[x]])))\nresult |&gt; \n  print()\n#&gt; [[1]]\n#&gt; [1] 0 1 2\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 0 1 2\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 0 1 2\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 0 1 2\n#&gt; \n#&gt; [[5]]\n#&gt; [1] 0 1 2\nTrasformiamo il data frame in una matrice.\nM &lt;- sdq_emo |&gt; \n  as.matrix()\nImplementiamo lo scaling ottimale con la funzione corAspect().\nopt &lt;- corAspect(M, aspect = \"aspectSum\", level = \"ordinal\")\nParametri principali della funzione:\nEsaminiamo il risultato ottenuto.\nattributes(opt) \n#&gt; $names\n#&gt;  [1] \"loss\"      \"catscores\" \"cormat\"    \"eigencor\"  \"indmat\"    \"scoremat\" \n#&gt;  [7] \"data\"      \"burtmat\"   \"niter\"     \"call\"     \n#&gt; \n#&gt; $class\n#&gt; [1] \"aspect\"\nsummary(opt)\n#&gt; \n#&gt; Correlation matrix of the scaled data:\n#&gt;         somatic worries unhappy clingy afraid\n#&gt; somatic   1.000   0.339   0.369  0.254  0.313\n#&gt; worries   0.339   1.000   0.468  0.397  0.378\n#&gt; unhappy   0.369   0.468   1.000  0.367  0.454\n#&gt; clingy    0.254   0.397   0.367  1.000  0.378\n#&gt; afraid    0.313   0.378   0.454  0.378  1.000\n#&gt; \n#&gt; \n#&gt; Eigenvalues of the correlation matrix:\n#&gt; [1] 2.497 0.757 0.634 0.610 0.502\n#&gt; \n#&gt; Category scores:\n#&gt; somatic:\n#&gt;     score\n#&gt; 0 -0.901\n#&gt; 1  0.586\n#&gt; 2  1.972\n#&gt; \n#&gt; worries:\n#&gt;     score\n#&gt; 0 -0.854\n#&gt; 1  0.445\n#&gt; 2  2.096\n#&gt; \n#&gt; unhappy:\n#&gt;     score\n#&gt; 0 -0.601\n#&gt; 1  1.393\n#&gt; 2  2.659\n#&gt; \n#&gt; clingy:\n#&gt;     score\n#&gt; 0 -1.199\n#&gt; 1  0.237\n#&gt; 2  1.617\n#&gt; \n#&gt; afraid:\n#&gt;     score\n#&gt; 0 -0.769\n#&gt; 1  1.009\n#&gt; 2  1.951\nQuesto approccio offre un metodo rigoroso per ottimizzare la misurazione degli item, migliorando la qualità psicometrica della scala e assicurando che l’interpretazione delle risposte rifletta al meglio la coerenza interna del test.\nI punteggi ottenuti si ottengono nel modo seguente:\nopt$scoremat\n#&gt;     somatic worries unhappy clingy afraid\n#&gt; 1     1.972   0.445  -0.601  0.237 -0.769\n#&gt; 2     1.972  -0.854  -0.601  0.237 -0.769\n#&gt; 3    -0.901  -0.854  -0.601 -1.199  1.009\n#&gt; 4    -0.901  -0.854  -0.601  0.237  1.009\n#&gt; 5     1.972   0.445  -0.601  0.237 -0.769\n#&gt; 6     0.586  -0.854  -0.601  0.237 -0.769\n#&gt; 7    -0.901   0.445   1.393  1.617 -0.769\n#&gt; 8    -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 9     0.586  -0.854  -0.601 -1.199 -0.769\n#&gt; 10   -0.901  -0.854  -0.601 -1.199  1.009\n#&gt; 11   -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 12    1.972   2.096   1.393  0.237  1.951\n#&gt; 13   -0.901  -0.854  -0.601 -1.199  1.951\n#&gt; 14   -0.901   0.445  -0.601  1.617 -0.769\n#&gt; 15    0.586   2.096   1.393  1.617  1.009\n#&gt; 16    1.972  -0.854  -0.601  0.237  1.009\n#&gt; 17    0.586   0.445  -0.601  1.617  1.009\n#&gt; 18    0.586   0.445  -0.601 -1.199 -0.769\n#&gt; 19    0.586   2.096   2.659  1.617  1.009\n#&gt; 20    0.586   0.445   1.393  1.617  1.009\n#&gt; 21    0.586  -0.854  -0.601 -1.199 -0.769\n#&gt; 22    1.972   0.445  -0.601 -1.199  1.009\n#&gt; 23    1.972   2.096   2.659  0.237  1.951\n#&gt; 24   -0.901   0.445   1.393  0.237  1.009\n#&gt; 25    0.586   2.096  -0.601 -1.199  1.951\n#&gt; 26    1.972   2.096   2.659  1.617  1.009\n#&gt; 27   -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 28    0.586  -0.854  -0.601  1.617 -0.769\n#&gt; 29    0.586  -0.854   1.393  0.237 -0.769\n#&gt; 30   -0.901  -0.854  -0.601  0.237 -0.769\n#&gt; 31   -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 32   -0.901   0.445  -0.601  0.237 -0.769\n#&gt; 33    0.586   0.445   1.393  1.617  1.009\n#&gt; 34    0.586   0.445  -0.601  0.237 -0.769\n#&gt; 35    1.972   2.096   1.393  1.617  1.951\n#&gt; 36    0.586   0.445   1.393 -1.199  1.009\n#&gt; 37   -0.901   0.445   1.393  0.237  1.009\n#&gt; 38   -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 39    0.586   2.096   1.393  1.617  1.009\n#&gt; 40    0.586  -0.854  -0.601 -1.199 -0.769\n#&gt; 41    0.586   0.445  -0.601 -1.199  1.009\n#&gt; 42    0.586  -0.854   1.393  1.617  1.951\n#&gt; 43    0.586   0.445  -0.601  0.237  1.009\n#&gt; 44    1.972   0.445  -0.601  0.237  1.009\n#&gt; 45   -0.901   0.445   1.393  0.237  1.009\n#&gt; 46    0.586  -0.854  -0.601 -1.199 -0.769\n#&gt; 47   -0.901   0.445   1.393  0.237  1.009\n#&gt; 48    0.586  -0.854  -0.601 -1.199 -0.769\n#&gt; 49   -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 50   -0.901   0.445   1.393  1.617  1.009\n#&gt; 51   -0.901  -0.854  -0.601  0.237 -0.769\n#&gt; 52    0.586   0.445  -0.601 -1.199 -0.769\n#&gt; 53    1.972  -0.854  -0.601 -1.199 -0.769\n#&gt; 54   -0.901   0.445   1.393  0.237  1.009\n#&gt; 55   -0.901   0.445  -0.601  1.617  1.009\n#&gt; 56    0.586   0.445  -0.601  0.237  1.009\n#&gt; 57    0.586   0.445   2.659  0.237  1.009\n#&gt; 58   -0.901  -0.854  -0.601  0.237 -0.769\n#&gt; 59    1.972   2.096   2.659  0.237  1.009\n#&gt; 60   -0.901  -0.854  -0.601  0.237  1.951\n#&gt; 61   -0.901   0.445  -0.601  0.237 -0.769\n#&gt; 62   -0.901  -0.854  -0.601  0.237  1.009\n#&gt; 63    0.586   0.445   1.393  0.237 -0.769\n#&gt; 64   -0.901  -0.854  -0.601  0.237 -0.769\n#&gt; 65   -0.901  -0.854  -0.601  0.237 -0.769\n#&gt; 66   -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 67   -0.901   0.445  -0.601  0.237 -0.769\n#&gt; 68    0.586  -0.854  -0.601  0.237 -0.769\n#&gt; 69    1.972   0.445   1.393  1.617  1.009\n#&gt; 70   -0.901   0.445   1.393  1.617  1.009\n#&gt; 71   -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 72   -0.901   0.445  -0.601  0.237 -0.769\n#&gt; 73    1.972   0.445   1.393  1.617  1.951\n#&gt; 74   -0.901  -0.854  -0.601 -1.199  1.009\n#&gt; 75   -0.901  -0.854  -0.601  0.237 -0.769\n#&gt; 76    1.972   0.445  -0.601  1.617  1.951\n#&gt; 77    0.586  -0.854  -0.601  0.237  1.951\n#&gt; 78   -0.901  -0.854  -0.601  0.237 -0.769\n#&gt; 79    0.586   2.096   1.393  1.617  1.951\n#&gt; 80   -0.901   0.445  -0.601  0.237  1.009\n#&gt; 81    0.586   2.096  -0.601  1.617 -0.769\n#&gt; 82   -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 83    0.586   0.445   1.393  0.237  1.009\n#&gt; 84    0.586   0.445  -0.601  0.237  1.009\n#&gt; 85   -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 86    0.586  -0.854  -0.601 -1.199 -0.769\n#&gt; 87    0.586  -0.854  -0.601 -1.199 -0.769\n#&gt; 88   -0.901   2.096   1.393  0.237  1.009\n#&gt; 89   -0.901   0.445  -0.601  0.237  1.009\n#&gt; 90    0.586   2.096  -0.601  0.237  1.951\n#&gt; 91    0.586  -0.854  -0.601 -1.199 -0.769\n#&gt; 92    0.586  -0.854  -0.601  0.237  1.009\n#&gt; 93   -0.901   0.445  -0.601  0.237 -0.769\n#&gt; 94    0.586   2.096   1.393  0.237  1.009\n#&gt; 95   -0.901   2.096   1.393  1.617  1.009\n#&gt; 96   -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 97   -0.901  -0.854  -0.601  0.237  1.009\n#&gt; 98   -0.901   0.445  -0.601  1.617  1.009\n#&gt; 99    0.586   2.096  -0.601  0.237  1.009\n#&gt; 100  -0.901   0.445  -0.601  0.237  1.009\n#&gt; 101   0.586  -0.854  -0.601  0.237  1.009\n#&gt; 102  -0.901  -0.854  -0.601  0.237 -0.769\n#&gt; 103  -0.901  -0.854  -0.601  0.237 -0.769\n#&gt; 104   1.972   0.445   1.393  1.617  1.009\n#&gt; 105  -0.901  -0.854   1.393  0.237 -0.769\n#&gt; 106   0.586   0.445  -0.601  0.237 -0.769\n#&gt; 107   0.586   2.096  -0.601  1.617 -0.769\n#&gt; 108   0.586  -0.854   1.393  0.237  1.951\n#&gt; 109   0.586   2.096   1.393  0.237 -0.769\n#&gt; 110  -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 111  -0.901   0.445  -0.601  1.617  1.009\n#&gt; 112  -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 113   0.586   0.445   1.393  0.237 -0.769\n#&gt; 114  -0.901  -0.854  -0.601  0.237 -0.769\n#&gt; 115  -0.901  -0.854  -0.601  0.237 -0.769\n#&gt; 116  -0.901   0.445  -0.601 -1.199 -0.769\n#&gt; 117  -0.901   0.445  -0.601 -1.199 -0.769\n#&gt; 118  -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 119   0.586  -0.854  -0.601  0.237 -0.769\n#&gt; 120   0.586   2.096   1.393  1.617  1.009\n#&gt; 121  -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 122   0.586   0.445  -0.601  0.237 -0.769\n#&gt; 123   0.586   0.445   2.659  1.617  1.951\n#&gt; 124   0.586  -0.854  -0.601  0.237  1.951\n#&gt; 125   1.972   0.445   1.393  0.237  1.009\n#&gt; 126  -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 127  -0.901  -0.854  -0.601  0.237  1.009\n#&gt; 128   1.972   0.445  -0.601  0.237 -0.769\n#&gt; 129   1.972   0.445   1.393  0.237  1.951\n#&gt; 130  -0.901   0.445  -0.601 -1.199 -0.769\n#&gt; 131  -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 132  -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 133   0.586  -0.854  -0.601 -1.199 -0.769\n#&gt; 134  -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 135   0.586   2.096  -0.601  0.237 -0.769\n#&gt; 136   0.586   0.445  -0.601  0.237 -0.769\n#&gt; 137  -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 138   1.972   2.096   2.659  1.617  1.951\n#&gt; 139   0.586   0.445   1.393  0.237  1.009\n#&gt; 140  -0.901   0.445  -0.601  0.237 -0.769\n#&gt; 141  -0.901   0.445  -0.601 -1.199 -0.769\n#&gt; 142   1.972   2.096   2.659 -1.199 -0.769\n#&gt; 143   0.586  -0.854  -0.601 -1.199 -0.769\n#&gt; 144  -0.901   0.445  -0.601  0.237 -0.769\n#&gt; 145  -0.901  -0.854   1.393 -1.199 -0.769\n#&gt; 146  -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 147  -0.901   0.445  -0.601 -1.199 -0.769\n#&gt; 148   1.972  -0.854  -0.601  1.617 -0.769\n#&gt; 149  -0.901  -0.854   2.659  0.237  1.009\n#&gt; 150   0.586  -0.854   1.393  0.237  1.009\n#&gt; 151   0.586  -0.854   1.393 -1.199 -0.769\n#&gt; 152   0.586   0.445  -0.601  1.617 -0.769\n#&gt; 153   1.972   0.445   1.393  1.617  1.009\n#&gt; 154   0.586   2.096   1.393 -1.199  1.009\n#&gt; 155   1.972   2.096   1.393  0.237 -0.769\n#&gt; 156  -0.901  -0.854  -0.601  0.237 -0.769\n#&gt; 157  -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 158   0.586   0.445   1.393  0.237  1.009\n#&gt; 159  -0.901   0.445   1.393  0.237 -0.769\n#&gt; 160   0.586  -0.854  -0.601 -1.199 -0.769\n#&gt; 161   0.586  -0.854  -0.601  0.237  1.009\n#&gt; 162  -0.901  -0.854  -0.601  1.617  1.009\n#&gt; 163  -0.901   2.096  -0.601  1.617  1.951\n#&gt; 164  -0.901   0.445  -0.601  0.237  1.951\n#&gt; 165  -0.901  -0.854  -0.601  1.617 -0.769\n#&gt; 166  -0.901  -0.854   1.393  1.617 -0.769\n#&gt; 167  -0.901  -0.854  -0.601  0.237 -0.769\n#&gt; 168  -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 169   0.586  -0.854  -0.601  0.237  1.009\n#&gt; 170   0.586   0.445  -0.601  0.237 -0.769\n#&gt; 171  -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 172   0.586  -0.854   1.393  0.237 -0.769\n#&gt; 173  -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 174  -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 175  -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 176   0.586  -0.854  -0.601  0.237 -0.769\n#&gt; 177  -0.901  -0.854  -0.601  1.617 -0.769\n#&gt; 178  -0.901   0.445  -0.601  0.237 -0.769\n#&gt; 179  -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 180  -0.901   0.445  -0.601 -1.199 -0.769\n#&gt; 181   0.586   0.445   1.393  0.237  1.009\n#&gt; 182   0.586  -0.854   1.393  0.237 -0.769\n#&gt; 183  -0.901   0.445  -0.601  1.617 -0.769\n#&gt; 184  -0.901  -0.854  -0.601  0.237 -0.769\n#&gt; 185   1.972   0.445  -0.601  0.237 -0.769\n#&gt; 186   0.586   0.445  -0.601  0.237 -0.769\n#&gt; 187  -0.901  -0.854  -0.601  0.237 -0.769\n#&gt; 188   0.586   2.096   1.393  0.237  1.009\n#&gt; 189   1.972  -0.854  -0.601 -1.199 -0.769\n#&gt; 190   0.586   0.445   1.393  0.237 -0.769\n#&gt; 191  -0.901   0.445  -0.601  0.237 -0.769\n#&gt; 192   0.586   0.445   1.393  1.617 -0.769\n#&gt; 193  -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 194  -0.901   0.445  -0.601  0.237 -0.769\n#&gt; 195   0.586   2.096  -0.601  1.617 -0.769\n#&gt; 196   0.586   0.445   1.393  0.237  1.009\n#&gt; 197  -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 198   0.586  -0.854  -0.601  0.237 -0.769\n#&gt; 199   0.586  -0.854  -0.601  0.237 -0.769\n#&gt; 200   0.586  -0.854  -0.601  0.237  1.009\n#&gt; 201   0.586   0.445  -0.601  0.237  1.009\n#&gt; 202  -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 203  -0.901   2.096  -0.601 -1.199 -0.769\n#&gt; 204   0.586   0.445   1.393 -1.199  1.009\n#&gt; 205   0.586  -0.854  -0.601  0.237 -0.769\n#&gt; 206   0.586  -0.854  -0.601  0.237 -0.769\n#&gt; 207  -0.901  -0.854  -0.601  0.237 -0.769\n#&gt; 208   0.586  -0.854  -0.601  1.617 -0.769\n#&gt; 209  -0.901   0.445  -0.601  0.237 -0.769\n#&gt; 210  -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 211  -0.901   0.445  -0.601 -1.199 -0.769\n#&gt; 212  -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 213  -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 214   0.586   2.096   1.393  1.617  1.951\n#&gt; 215  -0.901   0.445  -0.601 -1.199 -0.769\n#&gt; 216  -0.901  -0.854  -0.601  0.237 -0.769\n#&gt; 217  -0.901  -0.854  -0.601  0.237  1.009\n#&gt; 218   0.586  -0.854  -0.601 -1.199 -0.769\n#&gt; 219  -0.901  -0.854  -0.601  1.617 -0.769\n#&gt; 220   0.586  -0.854  -0.601 -1.199  1.009\n#&gt; 221   0.586  -0.854   1.393  0.237  1.009\n#&gt; 222  -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 223  -0.901  -0.854  -0.601 -1.199 -0.769\n#&gt; 224  -0.901   0.445  -0.601 -1.199 -0.769\n#&gt; 225   0.586  -0.854  -0.601 -1.199  1.009\n#&gt; 226  -0.901   0.445  -0.601  0.237 -0.769\n#&gt; 227  -0.901   0.445  -0.601 -1.199 -0.769\n#&gt; 228   1.972   0.445   1.393  0.237  1.009\nEsaminiamo la relazione tra lo scoring basato sul metodo Likert con lo scoring ottimale.\nplot(opt$scoremat[, 1], sdq_emo$somatic)\nplot(opt$scoremat[, 4], sdq_emo$clingy)\nplot(opt$scoremat[, 3], sdq_emo$unhappy)\nplot(opt$scoremat[, 2], sdq_emo$worries)\nplot(opt$scoremat[, 5], sdq_emo$afraid)\nGuardando ai grafici ottenuti, si può notare che 1) i punteggi per le categorie successive aumentano quasi linearmente; 2) le categorie sono approssimativamente equidistanti. Concludiamo che per la valutazione degli item ordinali nella scala dei Sintomi Emotivi del SDQ, la scala Likert è appropriata, e l’ottimizzazione della scala rispetto alla semplice scala Likert di base produce cambiamenti minimi. Per altri dati, comunque, la situazione potrebbe essere molto diversa.\nIn conclusione, l’ottimizzazione dello scoring dei dati di questionari ordinali offre un metodo rigoroso per ottimizzare la misurazione degli item, migliorando la qualità psicometrica della scala e assicurando che l’interpretazione delle risposte rifletta al meglio la coerenza interna del test.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E2_optimal_scoring.html#ottimizzazione-dello-scoring-dei-dati-di-questionari-ordinali",
    "href": "chapters/measurement/E2_optimal_scoring.html#ottimizzazione-dello-scoring-dei-dati-di-questionari-ordinali",
    "title": "\n3  ✏️ Esercizi\n",
    "section": "",
    "text": "data\nQuesto argomento rappresenta il data frame che contiene i dati da analizzare. Nel nostro caso, si tratta degli item relativi alla scala che stiamo studiando (ad esempio, quelli che misurano i Sintomi Emotivi).\naspect\nQuesto parametro specifica il criterio da ottimizzare. Per impostazione predefinita, aspect=\"aspectSum\" massimizza la somma delle correlazioni tra gli item. Questo criterio è utile per migliorare la consistenza interna della scala, ad esempio incrementando l’alfa di Cronbach. Nel nostro caso, utilizziamo questa impostazione predefinita.\n\nlevel\nQuesto argomento definisce il livello di misura delle variabili analizzate:\n\n\nnominal (impostazione predefinita): suppone che le variabili rappresentino categorie nominali. In questo caso, non vi sono restrizioni sui punteggi risultanti.\n\n\nordinal: richiede che l’ordine dei punteggi venga preservato.\n\n\nnumerical: oltre a preservare l’ordine, richiede che le distanze tra i punteggi siano uguali.\nNel caso delle categorie di risposta del SDQ (“non vero”, “parzialmente vero”, “assolutamente vero”), queste riflettono chiaramente un ordine crescente di accordo. Vogliamo preservare questo ordine durante l’ottimizzazione, quindi impostiamo level=\"ordinal\".\n\n\n\n\n\n\n\nPunteggi ottimali per ogni item:\nLa funzione calcola i punteggi “ottimali” per ogni item, ovvero valori che massimizzano la somma delle correlazioni tra gli item. Questo migliora la coerenza interna della scala.\n\nPreservazione dell’ordine delle risposte:\nUtilizzando level=\"ordinal\", i punteggi ottimizzati mantengono l’ordine crescente delle categorie di risposta, come ad esempio:\n\n“non vero” &lt; “parzialmente vero” &lt; “assolutamente vero”.\n\nCiò assicura che la struttura ordinata delle risposte venga rispettata.\n\n\nCorrelazioni e punteggi trasformati:\nL’output include:\n\nLa matrice di correlazione dei punteggi trasformati, ovvero le correlazioni tra gli item dopo la scalatura ottimale.\n\nLe correlazioni possono essere confrontate con quelle calcolate sulle variabili originali utilizzando la funzione cor(items).\n\n\n\nAutovalori della matrice di correlazione:\nL’output mostra anche gli autovalori della matrice di correlazione, che rappresentano le varianze delle componenti principali (da un’Analisi delle Componenti Principali, PCA).\n\nGli autovalori sono utili per determinare il numero di dimensioni misurate dal set di item.\n\nAd esempio, se il primo autovalore è notevolmente più grande degli altri, e ciò suggerisce che gli item misurano una sola dimensione, come ci si aspettava.\n\n\n\nPunteggi delle categorie:\nLa funzione mostra i punteggi assegnati a ciascuna categoria di risposta dopo la scalatura ottimale. Ad esempio, per l’item somatic, i risultati potrebbero indicare:\n\n“non vero” → -0.886\n\n“parzialmente vero” → 0.584\n\n“assolutamente vero” → 2.045\n\nQuesti punteggi sono scelti in modo da:\n\nAvere una media pari a 0 nel campione analizzato.\n\nMassimizzare le correlazioni tra gli item, migliorando la coerenza interna della scala.\n\n\nGrafici delle trasformazioni:\nIl pacchetto aspect offre grafici utili che mostrano visivamente l’assegnazione dei punteggi alle categorie. Questi grafici aiutano a interpretare il risultato della scalatura ottimale in modo intuitivo.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E2_optimal_scoring.html#session-info",
    "href": "chapters/measurement/E2_optimal_scoring.html#session-info",
    "title": "\n3  ✏️ Esercizi\n",
    "section": "\n3.2 Session Info",
    "text": "3.2 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] aspect_1.0-6      ggokabeito_0.1.0  see_0.10.0        MASS_7.3-65      \n#&gt;  [5] viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1   \n#&gt;  [9] gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6   \n#&gt; [13] semPlot_1.1.6     lavaan_0.6-19     psych_2.4.12      scales_1.3.0     \n#&gt; [17] markdown_1.13     knitr_1.49        lubridate_1.9.4   forcats_1.0.0    \n#&gt; [21] stringr_1.5.1     dplyr_1.1.4       purrr_1.0.4       readr_2.1.5      \n#&gt; [25] tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0  \n#&gt; [29] here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.0      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] mime_0.12           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [25] Matrix_1.7-2        R6_2.6.1            fastmap_1.2.0      \n#&gt;  [28] rbibutils_2.3       shiny_1.10.0        digest_0.6.37      \n#&gt;  [31] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n#&gt;  [34] rprojroot_2.0.4     Hmisc_5.2-2         timechange_0.3.0   \n#&gt;  [37] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n#&gt;  [40] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n#&gt;  [43] carData_3.0-5       ggsignif_0.6.4      corpcor_1.6.10     \n#&gt;  [46] gtools_3.9.5        tools_4.4.2         pbivnorm_0.6.0     \n#&gt;  [49] foreign_0.8-88      zip_2.3.2           httpuv_1.6.15      \n#&gt;  [52] nnet_7.3-20         glue_1.8.0          quadprog_1.5-8     \n#&gt;  [55] nlme_3.1-167        promises_1.3.2      lisrelToR_0.3      \n#&gt;  [58] grid_4.4.2          checkmate_2.3.2     cluster_2.1.8      \n#&gt;  [61] reshape2_1.4.4      generics_0.1.3      gtable_0.3.6       \n#&gt;  [64] tzdb_0.4.0          data.table_1.17.0   hms_1.1.3          \n#&gt;  [67] car_3.1-3           sem_3.1-16          pillar_1.10.1      \n#&gt;  [70] rockchalk_1.8.157   later_1.4.1         splines_4.4.2      \n#&gt;  [73] lattice_0.22-6      survival_3.8-3      kutils_1.73        \n#&gt;  [76] tidyselect_1.2.1    miniUI_0.1.1.1      pbapply_1.7-2      \n#&gt;  [79] reformulas_0.4.0    stats4_4.4.2        xfun_0.51          \n#&gt;  [82] qgraph_1.9.8        arm_1.14-4          stringi_1.8.4      \n#&gt;  [85] yaml_2.3.10         pacman_0.5.1        boot_1.3-31        \n#&gt;  [88] evaluate_1.0.3      codetools_0.2-20    mi_1.1             \n#&gt;  [91] cli_3.6.4           RcppParallel_5.1.10 rpart_4.1.24       \n#&gt;  [94] xtable_1.8-4        Rdpack_2.6.2        munsell_0.5.1      \n#&gt;  [97] Rcpp_1.0.14         coda_0.19-4.1       png_0.1-8          \n#&gt; [100] XML_3.99-0.18       parallel_4.4.2      jpeg_0.1-10        \n#&gt; [103] lme4_1.1-36         mvtnorm_1.3-3       openxlsx_4.2.8     \n#&gt; [106] rlang_1.1.5         multcomp_1.4-28     mnormt_2.1.1",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E3_thurstone.html",
    "href": "chapters/measurement/E3_thurstone.html",
    "title": "\n4  ✏ Esercizi\n",
    "section": "",
    "text": "4.1 Il Modello Thurstoniano\nIl modello Thurstoniano fornisce un quadro statistico per analizzare e interpretare preferenze o ranking di oggetti (o stimoli) da parte di singoli individui. L’idea di base è che, dietro i ranking osservati, esista una scala latente continua, non direttamente misurabile, dove ogni individuo assegna un punteggio personale a ciascun oggetto. Tali punteggi individuali, sebbene soggettivi, possono essere descritti complessivamente su una dimensione unica, grazie a ipotesi specifiche sulle distribuzioni di questi punteggi.\nPiù in dettaglio, il modello ipotizza che:\nSulla base di questi presupposti, si procede così:\nQuesta procedura consente di trasformare i dati di ranking discreti in stime numeriche di utilità psicologica, evidenziando le differenze relative di importanza fra gli oggetti analizzati.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>✏ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E3_thurstone.html#introduzione-allo-scaling-di-thurstone",
    "href": "chapters/measurement/E3_thurstone.html#introduzione-allo-scaling-di-thurstone",
    "title": "\n4  ✏ Esercizi\n",
    "section": "",
    "text": "4.1.1 Il Modello Thurstoniano\nIl modello Thurstoniano rappresenta un approccio statistico per analizzare e interpretare le preferenze o i ranking individuali rispetto a vari oggetti o stimoli. Questo modello si basa sull’idea che esista una scala latente, ovvero una dimensione non direttamente osservabile, attorno alla quale si distribuiscono i ranking individuali. In altre parole, ogni individuo assegna un punteggio ad ogni oggetto basandosi su criteri personali, ma queste valutazioni individuali sono influenzate da una percezione collettiva o aggregata che può essere descritta su una scala continua latente.\nIl principale obiettivo del modello Thurstoniano è di trasformare queste medie di ranking latenti aggregati, che esistono su una scala continua, in un ranking discreto che possiamo interpretare più facilmente. Per farlo, il modello si avvale di alcune ipotesi chiave:\n\nDistribuzione Gaussiana: Si assume che il ranking latente per ciascun oggetto possa essere descritto da una distribuzione gaussiana.\nMedia Differenziata, Varianza Costante: Il modello presuppone che le distribuzioni gaussiane dei ranking per ciascun oggetto differiscano tra loro solo per la media, mantenendo costante la varianza (scaling di Thurstone caso V). Questo implica che, sebbene gli oggetti possano avere livelli di preferenza medi diversi (alcuni potrebbero essere generalmente preferiti ad altri), la variabilità delle valutazioni (quanto le opinioni dei rispondenti differiscono tra loro) è la stessa per tutti gli oggetti.\n\nPer posizionare gli oggetti sulla scala di Thurstone, si procede nel seguente modo:\n\nSi calcola la proporzione di rispondenti che preferiscono un oggetto rispetto a ciascuno degli altri.\nSi determinano i corrispondenti percentile (z-scores) della distribuzione cumulativa normale, che ci dicono quante deviazioni standard un valore è distante dalla media.\nSi calcola la media di questi z-scores per ciascun oggetto.\n\nDi seguito propongo un esempio più lineare e coerente dello scaling di Thurstone per tre oggetti (A, B e C), assieme a una spiegazione puntuale dei vari passaggi.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>✏ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E3_thurstone.html#studio-sulle-preferenze-riguardanti-le-caratteristiche-delloccupazione-ideale",
    "href": "chapters/measurement/E3_thurstone.html#studio-sulle-preferenze-riguardanti-le-caratteristiche-delloccupazione-ideale",
    "title": "\n4  ✏ Esercizi\n",
    "section": "\n4.4 Studio sulle preferenze riguardanti le caratteristiche dell’occupazione ideale",
    "text": "4.4 Studio sulle preferenze riguardanti le caratteristiche dell’occupazione ideale\nI dati utilizzati in questo studio sono stati raccolti nell’ambito di una ricerca sulla motivazione lavorativa condotta da Ilke Inceoglu. Nel corso di questa indagine, 1079 partecipanti sono stati invitati a classificare nove aspetti lavorativi in base all’importanza che desideravano che fossero presenti nella loro occupazione ideale:\n\nAmbiente di Supporto (Supporto)\nLavoro Stimolante (Sfida)\nProgressione di Carriera (Carriera)\nLavoro Etico (Etica)\nControllo sul Lavoro, Impatto Personale (Autonomia)\nSviluppo (Sviluppo)\nInterazione Sociale (Interazione)\nAmbiente Competitivo (Competizione)\nAmbiente Piacevole e Sicuro (Sicurezza)\n\nL’obiettivo è identificare la struttura latente delle preferenze attraverso il modello di scaling di Thurstone (Caso V), tecnica psicometrica classica per l’analisi delle preferenze comparative.\nUn punteggio di 1 attribuito a qualsiasi aspetto lavorativo indica che tale aspetto era il più importante per quel partecipante, mentre un punteggio di 9 indica che era il meno importante.\n\nJobFeatures &lt;- rio::import(\"../../data/JobFeatures.txt\")\nglimpse(JobFeatures)\n#&gt; Rows: 1,079\n#&gt; Columns: 9\n#&gt; $ Support     &lt;int&gt; 8, 7, 5, 7, 1, 6, 5, 1, 1, 7, 6, 8, 5, 9, 8, 1, 6, 7, …\n#&gt; $ Challenge   &lt;int&gt; 3, 5, 8, 6, 4, 1, 4, 9, 3, 4, 2, 1, 4, 8, 6, 7, 4, 4, …\n#&gt; $ Career      &lt;int&gt; 4, 1, 1, 8, 8, 3, 7, 2, 7, 6, 3, 4, 6, 1, 3, 5, 8, 3, …\n#&gt; $ Ethics      &lt;int&gt; 5, 6, 9, 9, 3, 7, 2, 8, 4, 1, 9, 3, 7, 5, 9, 6, 7, 5, …\n#&gt; $ Autonomy    &lt;int&gt; 2, 2, 6, 3, 9, 8, 3, 7, 9, 8, 4, 6, 3, 7, 5, 2, 3, 8, …\n#&gt; $ Development &lt;int&gt; 6, 8, 2, 4, 2, 5, 6, 5, 2, 5, 1, 2, 2, 6, 1, 3, 1, 2, …\n#&gt; $ Interaction &lt;int&gt; 1, 3, 3, 2, 6, 2, 1, 4, 6, 9, 5, 5, 1, 4, 2, 8, 2, 6, …\n#&gt; $ Competition &lt;int&gt; 7, 9, 4, 5, 7, 4, 9, 6, 8, 3, 7, 7, 9, 2, 7, 9, 9, 1, …\n#&gt; $ Safety      &lt;int&gt; 9, 4, 7, 1, 5, 9, 8, 3, 5, 2, 8, 9, 8, 3, 4, 4, 5, 9, …\n\nConsideriamo i dati del primo rispondente:\n\nJobFeatures[1, ]\n#&gt;   Support Challenge Career Ethics Autonomy Development Interaction\n#&gt; 1       8         3      4      5        2           6           1\n#&gt;   Competition Safety\n#&gt; 1           7      9\n\nQuesto rispondente ha risposto assegnando la caratteristica più importante dell’impego a “Interaction”, seguita da “Autonomy”. L’ultima preferenza è “Safety”.\nEseguiamo lo scaling di Thurstone usando la funzione thurstone del pacchetto psych:\n\nscaling &lt;- psych::thurstone(JobFeatures, ranks = TRUE)\n\nGli attributi dell’oggetto scaling prodotto da thurstone() possono essere elencati nel modo seguente.\n\nattributes(scaling)\n#&gt; $names\n#&gt; [1] \"scale\"    \"GF\"       \"choice\"   \"residual\" \"Call\"    \n#&gt; \n#&gt; $class\n#&gt; [1] \"psych\"     \"thurstone\"\n\nI risultati dello scaling si ottengono nel modo seguente. Sono elencati nell’ordine fornito sopra, ovvero Support, Challenge, Career, Ethics, Autonomy, Development, Interaction, Competition e Safety.\nUna media alta indica che i partecipanti attribuiscono un alto valore a questo aspetto lavorativo rispetto agli altri. Tuttavia, poiché le preferenze sono sempre relative, è impossibile identificare in maniera univoca tutte le medie. Pertanto, una delle medie deve essere fissata a un valore arbitrario. È consuetudine fissare la media dell’aspetto meno preferito a 0. Quindi, tutte le altre medie sono positive.\n\nscaling$scale |&gt; print()\n#&gt; [1] 0.97 0.93 0.91 0.92 0.60 1.04 0.63 0.00 0.23\n\nLa media più bassa (0.0) corrisponde all’8° aspetto, Competizione, mentre la media più alta (1.04) corrisponde al 6° aspetto, Sviluppo. Ciò significa che l’ambiente competitivo era il meno desiderato, mentre le opportunità di sviluppo personale erano le più desiderate dalle persone nel loro lavoro ideale. Gli altri aspetti sono stati valutati come aventi un’importanza relativa intermedia a queste due, con Sicurezza che ha una media bassa (0.23) - appena superiore a 0 per la Competizione, mentre Supporto, Sfida, Carriera ed Etica hanno medie simili (intorno a 0.9). Autonomia e Interazione hanno medie moderate simili intorno a 0.6.\nL’istruzione seguente produce una matrice 9x9 contenente le proporzioni dei partecipanti nel campione che hanno preferito l’aspetto nella colonna rispetto all’aspetto nella riga. Nella matrice risultante, le righe e le colonne seguono l’ordine delle variabili nel file originale.\n\nscaling$choice |&gt; \n  round(2)\n#&gt;       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\n#&gt;  [1,] 0.50 0.47 0.47 0.47 0.36 0.51 0.36 0.20 0.23\n#&gt;  [2,] 0.53 0.50 0.47 0.49 0.38 0.53 0.36 0.17 0.28\n#&gt;  [3,] 0.53 0.53 0.50 0.50 0.38 0.52 0.39 0.19 0.26\n#&gt;  [4,] 0.53 0.51 0.50 0.50 0.36 0.52 0.38 0.19 0.26\n#&gt;  [5,] 0.64 0.62 0.62 0.64 0.50 0.67 0.51 0.29 0.34\n#&gt;  [6,] 0.49 0.47 0.48 0.48 0.33 0.50 0.29 0.15 0.20\n#&gt;  [7,] 0.64 0.64 0.61 0.62 0.49 0.71 0.50 0.22 0.30\n#&gt;  [8,] 0.80 0.83 0.81 0.81 0.71 0.85 0.78 0.50 0.61\n#&gt;  [9,] 0.77 0.72 0.74 0.74 0.66 0.80 0.70 0.39 0.50\n\nIl valore maggiore è\n\nmax(scaling$choice)\n#&gt; [1] 0.853\n\nQuesto valore, 0.8526, rappresenta la proporzione di partecipanti che hanno preferito l’8° aspetto, Competizione, al 6° aspetto, Sviluppo, ed è il valore più grande nella matrice precedente: questa coppia di caratteristiche ha la preferenza più decisa per un aspetto rispetto all’altro.\nLa preferenza più decisa in termini di proporzioni di persone che scelgono un aspetto rispetto all’altro deve avere la maggiore distanza/differenza sulla scala delle preferenze soggettive (il 6° aspetto, Sviluppo, deve avere una preferenza percepita media molto più alta dell’8° aspetto, Competizione). Questo risultato è effettivamente in linea con i risultati per le medie di utilità, dove la media dello Sviluppo è la più alta con un valore di 1.04 e la Competizione è la più bassa con un valore di 0.\nConsideriamo i residui del modello:\n\nscaling$residual |&gt; \n  round(2)\n#&gt;        [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]\n#&gt;  [1,]  0.00  0.01  0.00  0.01 -0.01  0.02  0.01 -0.04  0.01\n#&gt;  [2,] -0.01  0.00  0.02  0.00 -0.01  0.01  0.02  0.01 -0.04\n#&gt;  [3,]  0.00 -0.02  0.00  0.01  0.00  0.03  0.00  0.00 -0.01\n#&gt;  [4,] -0.01  0.00 -0.01  0.00  0.01  0.03  0.01 -0.01 -0.01\n#&gt;  [5,]  0.01  0.01  0.00 -0.01  0.00 -0.01  0.01 -0.01  0.02\n#&gt;  [6,] -0.02 -0.01 -0.03 -0.03  0.01  0.00  0.05  0.00  0.02\n#&gt;  [7,] -0.01 -0.02  0.00 -0.01 -0.01 -0.05  0.00  0.04  0.04\n#&gt;  [8,]  0.04 -0.01  0.00  0.01  0.01  0.00 -0.04  0.00 -0.02\n#&gt;  [9,] -0.01  0.04  0.01  0.01 -0.02 -0.02 -0.04  0.02  0.00\n\nL’istruzione precedente produce una matrice 9x9 contenente le differenze tra le proporzioni osservate (la matrice delle scelte) e le proporzioni attese (proporzioni che preferiscono l’aspetto nella riga rispetto all’aspetto nella colonna, che sarebbe atteso in base alle distribuzioni normali standard delle preferenze soggettive intorno alle medie scalate come sopra). Gli scarti tra i valori attesi e quelli osservati sono il modo più diretto di misurare se un modello (in questo caso, il modello proposto da Thurstone) “si adatta” ai dati osservati. Gli scarti piccoli (vicini allo zero) indicano che ci sono piccole discrepanze tra le scelte osservate e le scelte previste dal modello; il che significa che il modello che abbiamo adottato è piuttosto buono.\nInfine, esaminiamo un indice di bontà di adattamento:\n\nscaling$GF\n#&gt; [1] 0.999\n\nIl valore GF (Goodness of Fit) viene calcolato come 1 meno la somma dei residui al quadrato divisi per i valori osservati al quadrato. Quando i residui sono quasi zero, i loro rapporti al quadrato rispetto alle proporzioni osservate dovrebbero anch’essi avvicinarsi a zero. Di conseguenza, l’indice di bontà di adattamento di un modello ben adattato dovrebbe essere vicino a 1.\nNella nostra analisi, tutti i residui sono notevolmente piccoli, indicando una stretta corrispondenza tra le scelte osservate (proporzioni di preferenze per una caratteristica rispetto a un’altra). Questo allineamento preciso si riflette nell’indice GF, che è quasi 1, suggerendo che il modello di Thurstone cattura adeguatamente le proprietà dei dati relativi alle caratteristiche dell’occupazione ideale.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>✏ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E3_thurstone.html#considerazioni-conclusive",
    "href": "chapters/measurement/E3_thurstone.html#considerazioni-conclusive",
    "title": "\n4  ✏ Esercizi\n",
    "section": "\n4.5 Considerazioni conclusive",
    "text": "4.5 Considerazioni conclusive\nQuesta metodologia, introdotta da Louis Leon Thurstone negli anni ’20, rappresenta una delle forme più semplici e intuitive di scaling, dove per “scaling” si intende il processo di costruzione di un ordinamento di valori lungo un continuum psicologico. Lo scaling thurstoniano si basa sulla premessa che sia possibile ordinare stimoli o concetti secondo il grado in cui incarnano una certa proprietà psicologica, creando così una scala di misura che riflette le percezioni, le attitudini o i giudizi degli individui.\nUno degli aspetti centrali dello scaling di Thurstone, in particolare il caso V della sua legge del giudizio comparativo, è l’assunzione che le distribuzioni di ranking degli stimoli abbiano varianze uguali. Questa ipotesi, pur facilitando la modellizzazione matematica e l’interpretazione dei dati, è stata oggetto di critiche poiché difficilmente riscontrabile nella pratica. Le varianze possono differire significativamente tra gli stimoli a seconda della coerenza dei giudizi degli individui e della natura degli stimoli stessi. Questa limitazione ha stimolato lo sviluppo e l’adozione di metodi alternativi più flessibili per affrontare la complessità dello scaling psicologico.\nUn’ulteriore criticità metodologica, spesso poco discussa, riguarda la falsificazione della coerenza interna del modello. Ad esempio, nello scaling di Mokken si assume la monotonicità, ovvero che la probabilità di una risposta positiva a un item aumenti monotonamente con l’aumentare del livello del tratto psicologico misurato. Tale assunzione può essere esplicitamente verificata sui dati empirici, e quando viene violata indica un problema di validità interna della scala. Lo scaling di Thurstone, tuttavia, non prevede procedure sistematiche o strumenti consolidati per testare esplicitamente e falsificare la validità interna del modello. In altre parole, se gli individui forniscono risposte non coerenti con l’assunzione di uguale varianza o di un unico continuum psicologico comune, il modello thurstoniano stesso non offre criteri chiari per identificare e gestire tali violazioni. Questa assenza di procedure di falsificazione interna rappresenta uno dei limiti più significativi del modello di Thurstone.\nNel panorama contemporaneo, l’approccio più diffuso e metodologicamente avanzato per lo scaling psicologico deriva dalla Teoria della Risposta all’Item (IRT). L’IRT supera alcune delle limitazioni intrinseche allo scaling thurstoniano offrendo un quadro teorico e metodologico che considera la probabilità di una certa risposta a un item in funzione delle caratteristiche dell’item stesso e del livello dell’attributo psicologico del rispondente. Questo approccio permette di gestire in modo più efficace la varianza tra gli stimoli, di testare esplicitamente assunzioni chiave (come la monotonicità e l’indipendenza locale) e di fornire stime più accurate delle proprietà psicometriche degli item e delle caratteristiche degli individui.\nIn conclusione, mentre lo scaling thurstoniano ha rappresentato un passo fondamentale nello sviluppo degli strumenti di misurazione in psicologia, l’evoluzione metodologica e teorica ha portato a preferire approcci basati sull’IRT. Questo non diminuisce il valore storico e didattico dello scaling di Thurstone, che continua a essere un esempio introduttivo prezioso per comprendere i concetti fondamentali dello scaling psicologico. Tuttavia, è nell’ambito della IRT che attualmente si trovano le soluzioni più robuste e sofisticate per affrontare le sfide della misurazione psicologica, guidando la ricerca e l’applicazione pratica nel campo della psicometria contemporanea.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>✏ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E3_thurstone.html#sesssion-info",
    "href": "chapters/measurement/E3_thurstone.html#sesssion-info",
    "title": "\n4  ✏ Esercizi\n",
    "section": "\n4.5 Sesssion Info",
    "text": "4.5 Sesssion Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] rio_1.2.3         ggokabeito_0.1.0  see_0.10.0        MASS_7.3-65      \n#&gt;  [5] viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1   \n#&gt;  [9] gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6   \n#&gt; [13] semPlot_1.1.6     lavaan_0.6-19     psych_2.4.12      scales_1.3.0     \n#&gt; [17] markdown_1.13     knitr_1.49        lubridate_1.9.4   forcats_1.0.0    \n#&gt; [21] stringr_1.5.1     dplyr_1.1.4       purrr_1.0.4       readr_2.1.5      \n#&gt; [25] tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0  \n#&gt; [29] here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.1      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] mime_0.12           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [25] Matrix_1.7-2        R6_2.6.1            fastmap_1.2.0      \n#&gt;  [28] rbibutils_2.3       shiny_1.10.0        digest_0.6.37      \n#&gt;  [31] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n#&gt;  [34] rprojroot_2.0.4     Hmisc_5.2-2         timechange_0.3.0   \n#&gt;  [37] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n#&gt;  [40] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n#&gt;  [43] carData_3.0-5       R.utils_2.13.0      ggsignif_0.6.4     \n#&gt;  [46] corpcor_1.6.10      gtools_3.9.5        tools_4.4.2        \n#&gt;  [49] pbivnorm_0.6.0      foreign_0.8-88      zip_2.3.2          \n#&gt;  [52] httpuv_1.6.15       nnet_7.3-20         R.oo_1.27.0        \n#&gt;  [55] glue_1.8.0          quadprog_1.5-8      nlme_3.1-167       \n#&gt;  [58] promises_1.3.2      lisrelToR_0.3       grid_4.4.2         \n#&gt;  [61] checkmate_2.3.2     cluster_2.1.8       reshape2_1.4.4     \n#&gt;  [64] generics_0.1.3      gtable_0.3.6        tzdb_0.4.0         \n#&gt;  [67] R.methodsS3_1.8.2   data.table_1.17.0   hms_1.1.3          \n#&gt;  [70] car_3.1-3           sem_3.1-16          pillar_1.10.1      \n#&gt;  [73] rockchalk_1.8.157   later_1.4.1         splines_4.4.2      \n#&gt;  [76] lattice_0.22-6      survival_3.8-3      kutils_1.73        \n#&gt;  [79] tidyselect_1.2.1    miniUI_0.1.1.1      pbapply_1.7-2      \n#&gt;  [82] reformulas_0.4.0    stats4_4.4.2        xfun_0.51          \n#&gt;  [85] qgraph_1.9.8        arm_1.14-4          stringi_1.8.4      \n#&gt;  [88] pacman_0.5.1        boot_1.3-31         evaluate_1.0.3     \n#&gt;  [91] codetools_0.2-20    mi_1.1              cli_3.6.4          \n#&gt;  [94] RcppParallel_5.1.10 rpart_4.1.24        xtable_1.8-4       \n#&gt;  [97] Rdpack_2.6.2        munsell_0.5.1       Rcpp_1.0.14        \n#&gt; [100] coda_0.19-4.1       png_0.1-8           XML_3.99-0.18      \n#&gt; [103] parallel_4.4.2      jpeg_0.1-10         lme4_1.1-36        \n#&gt; [106] mvtnorm_1.3-3       openxlsx_4.2.8      rlang_1.1.5        \n#&gt; [109] multcomp_1.4-28     mnormt_2.1.1\n\n\n\n\n\nDomini, F., & Caudek, C. (2009). The intrinsic constraint model and Fechnerian sensory scaling. Journal of Vision, 9(2), 25–25.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>✏ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/02_development.html",
    "href": "chapters/measurement/02_development.html",
    "title": "5  Sviluppo dello strumento",
    "section": "",
    "text": "5.1 Introduzione\nLo sviluppo di un buon test psicologico non è semplice come potrebbe sembrare a prima vista. Si tratta di un processo articolato in più fasi, che richiede generalmente un notevole investimento di tempo, ricerca e, aspetto fondamentale, la disponibilità di partecipanti disposti a sottoporsi al test. Questo capitolo offre una panoramica del processo di sviluppo del test distinguendo quattro fasi principali: la concettualizzazione del test, la definizione della sua struttura e formato, la pianificazione delle standardizzazioni e degli studi psicometrici, e l’implementazione del piano (Reynolds & Livingston, 2021).",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sviluppo dello strumento</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/02_development.html#fasi-di-sviluppo",
    "href": "chapters/measurement/02_development.html#fasi-di-sviluppo",
    "title": "5  Sviluppo dello strumento",
    "section": "5.2 Fasi di sviluppo",
    "text": "5.2 Fasi di sviluppo\n\n5.2.1 Identificazione del costrutto\nLo sviluppo di un test psicologico o educativo è un processo che inizia con la chiara identificazione di una necessità specifica all’interno del campo. È imperativo che lo sviluppatore del test determini il costrutto che desidera misurare e dimostri la necessità di un nuovo metodo di misurazione. Con l’ampia varietà di test psicologici disponibili, diventa cruciale identificare una lacuna specifica o un bisogno non ancora soddisfatto.\nPrima di procedere con la creazione di un nuovo strumento di valutazione, è vitale condurre un’indagine approfondita per stabilire se esistano già misure valide e affidabili per il costrutto di interesse. Questo implica un’analisi meticolosa della letteratura scientifica e una revisione dei test psicometrici esistenti. Un tale approccio permette di scoprire se strumenti adeguati siano già disponibili, evitando così di duplicare inutilmente il lavoro già fatto. Inoltre, l’esistenza di misure preesistenti può servire da prezioso punto di riferimento per confrontare e validare il nuovo test in fase di sviluppo, assicurando che il nuovo strumento apporti un contributo significativo e unico al campo della psicologia e dell’educazione.\nCon il progredire della psicologia, nuovi costrutti vengono definiti e quelli esistenti vengono modificati. Ad esempio, la concezione dell’intelligenza è cambiata nel tempo, passando dalla misurazione del tempo di reazione e dell’acume sensoriale, all’enfasi sulla conoscenza, fino all’attuale enfasi sul problem solving in termini di intelligenza cristallizzata e fluida.\nIn alcuni casi, un clinico o ricercatore potrebbe avere la necessità di misurare una variabile ben definita, ma i test esistenti potrebbero essere di qualità dubbia o con qualità psicometriche obsolete. In tali situazioni, potrebbe essere necessario sviluppare un metodo di misurazione migliore o più esatto. Per esempio, le misurazioni del tempo di reazione, un tempo effettuate con l’osservazione umana e il cronometraggio, sono state sostituite da metodi elettronici molto più precisi.\nTuttavia, alcuni test possono valutare costrutti clinicamente utili ma essere impraticabili per l’applicazione clinica reale. Per esempio, la ricerca di sensazioni è stata misurata in modo lungo e complicato fino agli anni ’90, quando sono state sviluppate misurazioni rapide e affidabili che hanno reso il costrutto pratico per l’applicazione clinica.\nInoltre, i campioni standardizzati usati nei test possono diventare obsoleti e inapplicabili ai soggetti attuali, creando opportunità per lo sviluppo di nuovi test se persiste la necessità di misurare i costrutti in questione.\nNuovi costrutti vengono talvolta definiti anche nei campi della psicologia e dell’educazione. Questi costrutti, di solito derivati da osservazioni teoriche, devono essere studiati e manipolati per essere compresi, e ciò richiede regole per assegnare valori numerici alle osservazioni, ovvero la definizione stessa di un test. Di conseguenza, c’è sempre bisogno di nuovi strumenti di test, sia per modernizzare quelli esistenti sia per misurare nuovi costrutti. Gli sviluppatori e i ricercatori di test possono condurre ricerche letterarie dettagliate per determinare quali strumenti sono disponibili per valutare un costrutto e valutarne la qualità e l’applicabilità prima di stabilire un reale bisogno.\nIn conclusione, la decisione di sviluppare un nuovo test dovrebbe basarsi sulla domanda se questo migliorerebbe la pratica o la ricerca in un determinato campo, o se potrebbe migliorare la condizione umana o la nostra comprensione di essa.\nEsempio di Studio: Nello studio di Watson et al. (2007), una notevole parte dell’introduzione è dedicata all’analisi critica della letteratura esistente. Questa analisi mira a evidenziare i limiti degli strumenti di misurazione disponibili, esaminando le caratteristiche degli item utilizzati, la correlazione tra gli indicatori impiegati dagli strumenti esistenti e gli approcci teorici relativi alla depressione e all’ansia, nonché le soluzioni fattoriali risultanti dai dati raccolti attraverso tali strumenti. Viene inoltre considerata l’insufficiente esplorazione di alcune aree del costrutto da parte degli strumenti esistenti.\nWatson et al. (2007) si sono proposti di sviluppare uno strumento per la misurazione della depressione che superi i limiti di strumenti preesistenti come il Beck Depression Inventory-II (BDI-II; Beck, Steer, & Brown, 1996) e il Center for Epidemiological Studies Depression Scale (CES-D; Radloff, 1977). La scala sviluppata prende il nome di Inventory of Depression and Anxiety Symptoms (IDAS).\nPer rispondere alla prima questione, gli autori sottolineano che gli strumenti esistenti includono contenuti non specifici o non direttamente legati alla depressione. Sia il BDI-II sia il CES-D, per esempio, contengono item relativi a vari tipi di ansia, compromettendo così la loro validità discriminante. Gli strumenti esistenti non coprono inoltre l’intero dominio del costrutto della depressione maggiore come definito dal Diagnostic and Statistical Manual of Mental Disorders (4ª edizione). Presentano inoltre il limite di produrre un unico punteggio di severità dei sintomi, ignorando così l’eterogeneità e la multidimensionalità del fenomeno depressivo. Ciò si riflette nella struttura fattoriale poco chiara di tali strumenti, con diverse soluzioni fattoriali trovate da vari autori. L’IDAS, sviluppato da Watson et al. (2007), mira a superare queste difficoltà, creando sottoscale che riflettano direttamente gli aspetti distintivi della depressione.\nPer rispondere alla seconda questione, gli autori evidenziano come la depressione sia collocata all’interno di una rete nomologica di costrutti che include primariamente l’ansia. A differenza di strumenti preesistenti come BDI-II e CES-D, l’IDAS è stato progettato esplicitamente per creare scale che riflettano aspetti specifici della depressione, distinti dall’ansia. Questo è stato realizzato considerando un’ampia gamma di item che rappresentano sintomi associati all’ansia, al fine di esplorare la relazione tra i sintomi di ansia e quelli della depressione e creare scale distinte per queste dimensioni, aumentando così la validità discriminante dello strumento.\nInfine, per rispondere alla terza questione, Watson et al. (2007) dichiarano l’intento di sviluppare uno strumento che, nel suo punteggio complessivo, rifletta le caratteristiche generali della depressione e che, nelle sue sottoscale, misuri con precisione le varie dimensioni del costrutto esaminato.\n\n\n5.2.2 Obiettivo\nDopo aver identificato la necessità di un nuovo test per misurare un costrutto, è importante descrivere l’obiettivo primario nello sviluppo di una scala psicologica, gli usi previsti e le possibili interpretazioni dei risultati. Ci si dovrebbe chiedere: in quali contesti e per quali scopi verrà impiegato questo strumento? Quali interpretazioni dei risultati sono previste una volta che il test è stato somministrato e i risultati sono stati raccolti?\nLa risposta a queste domande dovrebbe scaturire logicamente dal passo precedente. Se risulta difficile rispondere, ciò potrebbe indicare che la concezione del test e dei costrutti da misurare è ancora troppo vaga. In tal caso, sarebbe opportuno ritornare al primo passo e sviluppare ulteriormente le idee e il concetto del test.\nComprendere come e in quale contesto un test può essere utilizzato è fondamentale per molti aspetti del suo sviluppo. Ad esempio, esistono numerosi test di personalità, ma ciascuno tende a enfatizzare aspetti diversi della personalità, delle emozioni e dell’affettività. Alcuni si concentrano sulla personalità normale, altri sugli stati psicopatologici. Il contesto (ad esempio, un ospedale psichiatrico rispetto alla selezione del personale) e l’uso previsto dei risultati determineranno contenuti e schemi interpretativi differenti.\nConoscere lo scopo del test e il contesto in cui verrà utilizzato influenza quindi la risposta a tutti gli altri fattori nel processo di sviluppo, dalla definizione dell’utente del test, al campione normativo appropriato, fino ai tipi di studi di validità necessari per convalidare le interpretazioni proposte dei punteggi.\n\n\n5.2.3 Utenti\nNello sviluppo di un test, è cruciale determinare chi lo utilizzerà e per quale motivo. I test dovrebbero essere progettati tenendo conto degli utenti specifici, ovvero individui che svolgono funzioni particolari facilitati dall’uso di test psicologici ed educativi. È importante considerare il tipo di formazione accademica formale e le esperienze supervisionate che potrebbero essere richieste agli utenti per applicare correttamente i risultati dei test. Ad esempio, per la maggior parte dei test psicometrici, esistono requisiti di formazione imposti dalle leggi che limitano l’uso dei test agli psicologi. Tuttavia, è anche possibile pensare di sviluppare un test che non sarà somministrato da psicologi.\nÈ anche utile, in questa fase, determinare quali individui in quali contesti troveranno il test proposto utile nel loro ruolo. Questo dovrebbe derivare direttamente dallo scopo del test e dalle interpretazioni proposte dei risultati. Ad esempio, se lo scopo del test è diagnosticare una condizione clinica come il disturbo bipolare pediatrico, l’utente target sarà probabilmente uno psicologo clinico, o forse un psichiatra. Tuttavia, un test progettato per esaminare un gran numero di bambini per verificare se presentano livelli di rischio elevati per disturbi emotivi e comportamentali potrebbe essere concepito in modo da poter essere somministrato e valutato da insegnanti o infermieri.\nLa conoscenza dell’utente previsto influenzerà le caratteristiche del test, in particolare riguardo alla complessità della sua somministrazione, valutazione e interpretazione, poiché diverse categorie di utenti possiedono diversi livelli di competenza.\n\n\n5.2.4 4. Definizione concettuale e operativa\nLa creazione di un test psicologico efficace inizia con l’identificazione e la definizione precisa del costrutto o della caratteristica psicologica che si intende misurare. Questo processo non è banale: spesso crediamo di comprendere pienamente costrutti come depressione, ansia, intelligenza cristallizzata, intelligenza fluida, aggressività, amabilità, fino a quando non tentiamo di esprimerli a parole. È in questo momento che possiamo renderci conto che la nostra comprensione del costrutto potrebbe non essere così chiara come inizialmente pensavamo.\nSi raccomanda di scrivere due tipi di definizioni: una concettuale e una operativa. Una definizione concettuale spiega il costrutto a livello teorico. Per esempio, una possibile definizione concettuale della depressione potrebbe essere: “La depressione è uno stato di malinconia, tristezza e bassi livelli di energia che porta all’anedonia, sentimenti di inutilità e stanchezza cronica.” Una definizione operativa, invece, dice esattamente come il nostro test definirà o misurerà il costrutto. Ad esempio: “Nella Scala di Valutazione della Depressione degli Studenti, la depressione sarà valutata sommando le valutazioni in direzione punteggiata su osservazioni di comportamento come espressioni di sentimenti di tristezza, sentimenti di solitudine, di sentirsi incompresi e non apprezzati, mancanza di coinvolgimento in attività piacevoli, troppo o troppo poco sonno, pianto in momenti inappropriati e lamentele di stanchezza.” Quindi, mentre la definizione concettuale ci dice ciò che vogliamo misurare in astratto, la definizione operativa ci informa in modo più diretto e specifico su come sarà derivato il punteggio del costrutto. Anche se questi sforzi possono sembrare tediosi per scale che hanno molti costrutti, più costrutti sono presenti in un test, più tali definizioni si rivelano utili.\nEsempio: Nel loro studio sulla depressione, Watson et al. (2007) si riferiscono al DSM-IV, che elenca nove criteri sintomatici per diagnosticare un episodio depressivo maggiore. Questi includono: (1) umore depresso per la maggior parte del giorno; (2) notevole riduzione di interesse o piacere per quasi tutte le attività; (3) variazione significativa del peso o dell’appetito; (4) insonnia o ipersonnia; (5) agitazione o rallentamento psicomotorio; (6) affaticamento o perdita di energia; (7) sentimenti di inutilità o eccessiva colpa; (8) difficoltà di concentrazione o indecisione; (9) pensieri di morte o ideazione suicidaria.\nPer ottimizzare l’utilità dell’Inventory of Depression and Anxiety Symptoms (IDAS), Watson et al. (2007) hanno incluso diversi item per ciascuno di questi nove criteri. Con l’intento di garantire una rappresentazione adeguata di ciascuna potenziale dimensione del costrutto, gli autori hanno inizialmente organizzato gli item in gruppi denominati homogeneous item composites (HIC). Tuttavia, hanno precisato che la formazione di questi HIC non implica necessariamente l’emergere di un fattore corrispondente, ma serve piuttosto a coprire l’intero dominio potenziale del costrutto di depressione.\n\n\n5.2.5 Dissimulazione\nDeterminare la necessità di misure per rilevare la dissimulazione. La dissimulazione è il comportamento di presentare se stessi in modo diverso dalla realtà. Per esempio, una persona che si sente triste ma non vuole ammetterlo potrebbe rispondere falsamente a una domanda di un test di personalità riguardo la tristezza per nascondere i propri sentimenti. La dissimulazione può verificarsi anche quando si valutano altre persone, ad esempio compilando una scala di valutazione per un bambino, coniuge o genitore anziano.\nLe persone possono impegnarsi nella dissimulazione per vari motivi, come negare sintomi in una valutazione di personalità o psicopatologia per non apparire con tratti indesiderati o comportamenti inaccettabili. Questo fenomeno si verifica anche tra coloro che cercano trattamento. Nell’ambito lavorativo, non è insolito che i candidati rispondano alle domande in modo da aumentare le possibilità di ottenere il lavoro.\nIn altri casi, le persone possono esagerare i sintomi per apparire più compromessi di quanto siano in realtà, un comportamento noto come simulazione, spesso motivato dal guadagno personale. Ad esempio, possono fingere problemi di personalità, comportamentali o cognitivi per ottenere benefici di invalidità, aumentare i risarcimenti in cause legali o evitare punizioni.\nNelle valutazioni cognitive, la dissimulazione può essere rilevata attraverso test di sforzo, ossia test che chiunque può completare correttamente se ci prova. Questi test sono raccomandati in quasi tutte le situazioni forensi e quando un paziente ha qualcosa da guadagnare fingendo deficit cognitivi.\nLa dissimulazione può manifestarsi in molti modi e talvolta si vedono definizioni più specifiche. Ad esempio, la validità dei sintomi si riferisce all’accuratezza o veridicità della presentazione comportamentale del soggetto, mentre il bias di risposta è un tentativo di ingannare l’esaminatore con risposte inesatte o incomplete. L’effort riguarda l’impegno nell’eseguire al meglio delle proprie capacità.\nCapire lo scopo del test, chi lo utilizzerà e in quali circostanze aiuta a determinare se includere scale per rilevare la dissimulazione. La psicologia ha sviluppato nel tempo metodi sofisticati per rilevare la dissimulazione, e le tecniche comuni verranno ora esaminate.\nLe scale più comuni per rilevare la dissimulazione (spesso note come scale di validità) nelle misure di personalità e comportamento includono le scale F, le scale L e gli indici di inconsistenza. Queste sono brevemente descritte qui sotto:\n\nScale F (Infrequency Scales): Queste scale sono progettate per rilevare la presentazione esagerata dei sintomi. Gli item delle scale F riflettono sintomi raramente segnalati anche da persone con livelli significativi di psicopatologia, o rappresentano approvazioni estreme di sintomi comuni che sono anch’essi rari. Poiché questi item rappresentano risposte infrequenti e scarsamente correlate tra loro, i soggetti che indicano di sperimentare un gran numero di questi sintomi forniscono probabilmente una presentazione esagerata della psicopatologia.\nScale L (Social Desirability Scales): Sono progettate per rilevare la negazione inaccurata di sintomi realmente presenti, rilevando il bias opposto della scala F. Le scale L includono item speciali che riflettono piccoli difetti comuni che quasi tutti sperimentano in qualche momento. Ad esempio, una risposta negativa a un item come “Talvolta mi sento triste” può indicare dissimulazione.\nScale di Inconsistenza: Sono progettate per rilevare incongruenze nelle risposte a item simili su scale di personalità e comportamento. Quando un rispondente non è coerente nel rispondere a item simili, i risultati non sono considerati affidabili. Per esempio, un rispondente che afferma sia “Sono pieno di energia” sia “Mi sento affaticato” potrebbe essere incoerente.\n\nPer alcuni tipi di test, possono essere utili altre scale di dissimulazione per scopi speciali. Ad esempio, nello sviluppo di misure per la selezione di personale, potrebbe essere utile includere indicatori più avanzati e sottili di bias di risposta.\nNonostante non siano misure di dissimulazione, gli autori dei test a volte includono anche scale che esaminano la comprensione degli item e il livello di cooperazione del soggetto nei test auto-somministrati e nelle scale di valutazione. Queste scale, a volte chiamate V-scale o scale di validità, contengono tipicamente item privi di senso dove la risposta è la stessa per tutti coloro che prendono il test seriamente e cooperano con il processo di esame.\nNelle valutazioni attitudinali e nei test di profitto, alcune persone potrebbero non impegnarsi adeguatamente nei test per ragioni di guadagno, un comportamento comunemente visto come simulazione. Tuttavia, ci sono altre ragioni plausibili, come la sindrome amotivazionale a seguito di infortuni cerebrali o la avolizione in disturbi psichiatrici come la schizofrenia.\nPochi test cognitivi hanno misure incorporate di dissimulazione o mancanza di sforzo. Tuttavia, alcuni costruiscono scale o item per rilevare la mancanza di sforzo. Spesso, la simulazione su misure cognitive è valutata attraverso modelli di prestazione indicativi di risposte non valide, incongruenze tra risultati del test e comportamenti osservati, e discrepanze tra i risultati dei test e le informazioni di background documentate.\nIn conclusione, le scale di dissimulazione sono strumenti essenziali nel processo di sviluppo dei test, aiutando a garantire che i risultati riflettano accuratamente le caratteristiche e le abilità del soggetto.\n\n\n5.2.6 Formato degli Item\nLa determinazione del formato degli item è un passo cruciale nello sviluppo di un test psicologico o educativo. Questa fase comporta la scelta della struttura e del formato degli item, i quali devono essere in linea con il costrutto da misurare e gli obiettivi specifici del test. I formati possono variare, includendo domande a scelta multipla, scale Likert o altri tipi di risposta. È importante sottolineare che la scelta del formato influisce sull’accuratezza e sull’affidabilità del test, richiedendo quindi un’attenta valutazione.\nIn questa fase, si devono considerare anche aspetti pratici come la modalità di somministrazione del test, che può essere individuale o di gruppo, e il formato del test, che può essere cartaceo o computerizzato. Un’ulteriore considerazione riguarda chi completerà effettivamente il test o il foglio delle risposte, che potrebbe essere l’esaminatore, l’esaminando o un informatore terzo, come nel caso delle scale di valutazione comportamentale.\nDifferenti tipi di item sono utili in diverse circostanze e possono misurare caratteristiche in modi diversi. Per esempio, la valutazione di sentimenti, pensieri e altri comportamenti non osservabili è solitamente meglio realizzata tramite autovalutazione. Una volta selezionato il formato di autovalutazione, è importante decidere il tipo di item da utilizzare, poiché esistono diverse opzioni di item per l’autovalutazione. Ad esempio, l’item “Mi sento triste” può variare significativamente a seconda del formato di risposta scelto, influenzando l’intento e l’interpretazione delle risposte.\nDopo aver scelto i formati degli item appropriati, è fondamentale scrivere esempi di item per ciascun formato che si prevede di utilizzare nel test. Questo include anche la redazione delle istruzioni per la somministrazione e la valutazione del test, che dovrebbero essere chiare e comprensibili anche per chi non è familiare con il test.\nInfine, è utile stimare il numero di item necessari per valutare in modo affidabile i costrutti. Come regola generale, si dovrebbe inizialmente scrivere almeno il doppio del numero di item che si prevede di utilizzare nel test finale. Questo perché molti item potrebbero essere scartati a causa di statistiche insufficienti, pregiudizi o ambiguità. La lunghezza del test dovrebbe essere adeguata alla popolazione bersaglio, tenendo conto della sensibilità di alcuni gruppi, come i giovani o gli anziani, al tempo richiesto per completare il test.\nUn altro aspetto fondamentale da considerare è chi completerà effettivamente il test o il foglio delle risposte. Questo può variare a seconda delle circostanze: potrebbe essere l’esaminatore, l’esaminato, o un terzo informatore, come nel caso delle scale di valutazione comportamentale, dove i rispondenti possono essere genitori, insegnanti o altre figure rilevanti. La scelta di chi completerà il test può influenzare non solo la logistica della somministrazione del test, ma anche la natura delle informazioni raccolte, e quindi la validità e l’utilità dei risultati.\nScala Likert. Una scala Likert è un tipo di scala ordinale che viene utilizzata per misurare gli atteggiamenti di una persona. Viene chiesto al rispondente di valutare il grado di accordo o disaccordo con un’affermazione utilizzando un’alternativa di risposta che di solito varia da cinque a sette punti. Tuttavia, poiché è una scala ordinale, le distanze tra i livelli della scala non sono quantificabili e non possiamo assumere che le differenze tra i livelli di risposta siano equidistanti. Pertanto, c’è una lunga controversia sulla possibilità di trattare i valori numerici di una scala ordinale come se provenissero da una scala ad intervalli. Alcuni autori ritengono problematico non potere trattare i dati provenienti da scale di tipo Likert come se fossero a livello di scala ad intervalli, mentre altri autori lo considerano giustificato in presenza di un’ampia numerosità campionaria e di una distribuzione approssimativamente normale dei dati. In ogni caso, la procedura che sta alla base delle scale Likert consiste nella somma dei punti attribuiti ad ogni singola domanda. I vantaggi della scala Likert sono la sua semplicità e applicabilità, mentre i suoi svantaggi sono il fatto che i suoi elementi vengono trattati come scale cardinali pur essendo ordinali e il fatto che il punteggio finale non rappresenta una variabile cardinale.\nItem a codifica inversa. In parole più semplici, ci sono alcune domande in un test che sono strettamente correlate in modo negativo con le altre domande e con il punteggio totale del test. Queste domande richiedono una risposta diversa rispetto alle altre domande. Ad esempio, in un questionario sull’ansia, una domanda potrebbe chiedere “Sono preoccupata” e la scala di risposta potrebbe essere “Per nulla”, “Un po’”, “Abbastanza” e “Moltissimo” con valori 1, 2, 3 e 4 rispettivamente. Tuttavia, un’altra domanda potrebbe chiedere “Mi sento bene” e la scala di risposta potrebbe essere la stessa, ma con valori 4, 3, 2 e 1 rispettivamente. Questo perché le proprietà contrarie si trovano sullo stesso continuum latente. Questo è importante nella costruzione di un test psicologico, dove è consigliato utilizzare sia domande orientate nella direzione del costrutto (chiamate “straight item”) sia nella direzione opposta (chiamate “reverse item”) per contrastare l’acquiescenza e ottenere risposte più accurate.\n\n\n5.2.7 Sviluppare una struttura del test\nLo sviluppo di una struttura per un test psicologico o educativo richiede passaggi ben definiti prima di procedere alla creazione dei singoli item della scala. Prima di tutto, è essenziale stabilire un’organizzazione strutturale del test, delineando gli obiettivi specifici che si intendono raggiungere attraverso la sua somministrazione. Questo passaggio comporta la definizione dei sottodomini o delle dimensioni che si vogliono misurare e di come questi contribuiranno alla valutazione complessiva del costrutto.\nUna questione fondamentale da risolvere è se il test produrrà un unico punteggio, come somma di tutte le risposte agli item, o se sarà necessario suddividerlo in sottoscale. La struttura del test è spesso determinata dai costrutti che si intendono misurare, quindi le definizioni precedentemente scritte sono di grande importanza in questa fase decisionale. Se il test include sottoscale o sotto-test, è importante considerare anche se ci saranno punteggi compositi che forniscono indici riassuntivi dei raggruppamenti dei sotto-test.\nNella fase di sviluppo del test, è cruciale spiegare come gli item e le sottoscale (se presenti) saranno organizzati. Se la logica alla base del test e la discussione sui costrutti sottostanti non rendono evidente la specifica organizzazione degli item e dei sotto-test, è importante affrontare il motivo per cui questa particolare organizzazione è la più appropriata. Naturalmente, è da tenere in considerazione che la ricerca condotta durante il processo di sviluppo del test potrebbe portare a modifiche della struttura intesa, in base ai dati effettivamente raccolti.\nEsempio: Nello studio di Watson et al. (2007), per garantire un campionamento esaustivo dell’intero ambito del costrutto, sono stati definiti 20 homogeneous item composites (HIC), che includono: Umore Depresso, Perdita di Interesse o Piacere, Disturbi dell’Appetito, Disturbi del Sonno, Problemi Psicomotori, Fatica/Anergia, Sentimenti di Inutilità o Colpa, Problemi Cognitivi, Ideazione Suicidaria, Senso di Speranza, Depressione Melanconica, Umore Arrabbiato/Irritabile, Alta Energia/Affetto Positivo, Umore Ansioso, Preoccupazione, Panico, Agorafobia, Ansia Sociale, Intrusioni Traumatiche, Sintomi Ossessivo-Compulsivi.\nDi questi, 13 HIC (comprendenti in totale 117 item) sono stati dedicati agli indicatori specifici della depressione. Nove di questi HIC (79 item in totale) coprono i sintomi fondamentali della depressione maggiore secondo il DSM-IV, tra cui umore depresso, perdita di interesse o piacere, disturbi dell’appetito, del sonno, problemi psicomotori, fatica/anergia, senso di inutilità e colpa, problemi cognitivi e ideazione suicidaria. I restanti quattro HIC trattano temi quali la disperazione (Hopelessness, secondo Abramson, Metalsky, & Alloy, 1989), sintomi specifici della depressione melanconica (Joiner et al., 2005), umore arrabbiato/irritabile (una forma alternativa di depressione negli adolescenti secondo il DSM-IV, American Psychiatric Association, 1994, p. 327), e indicatori di energia e affetto positivo (associati alla depressione secondo Mineka et al., 1998).\nI rimanenti sette HIC (63 item totali) sono stati introdotti per valutare sintomi associati all’ansia. Questi HIC includono categorie come umore ansioso, preoccupazione, panico, agorafobia, ansia sociale e intrusioni traumatiche associate al PTSD. Questa approfondita categorizzazione mira a una valutazione comprensiva e differenziata dei sintomi legati sia alla depressione sia all’ansia, fornendo così una visione più completa del panorama psicopatologico.\n\n\n5.2.8 Tabella delle Specifiche\nLa creazione di un test psicologico o educativo implica lo sviluppo di una Tabella delle Specifiche (TOS), che funge da “schema” per assicurare l’allineamento tra le definizioni dei costrutti, le definizioni operative e il contenuto del test stesso. La TOS è particolarmente utile nelle misure di rendimento, dove serve a garantire che il test sia in congruenza con un determinato curriculum o campo di studio. Questo strumento definisce in modo chiaro le aree di contenuto principali che il test intende esplorare, selezionate attraverso un’analisi accurata degli obiettivi educativi. Questo processo è fondamentale per assicurare che il contenuto del test sia direttamente collegato agli obiettivi di apprendimento e di valutazione, rendendo il test pertinente e focalizzato sui temi chiave.\nÈ importante che gli autori di test evitino di affidarsi eccessivamente a processi cognitivi di livello inferiore, come la memorizzazione meccanica, e che includano anche processi di livello superiore per garantire una valutazione equilibrata e completa delle capacità cognitive. Inoltre, la distribuzione degli item tra le varie aree di contenuto dovrebbe essere bilanciata per riflettere adeguatamente l’importanza di ciascuna area nel quadro complessivo del curriculum o dell’area di studio.\nLe TOS non sono utili solo per test di rendimento e attitudine, ma anche per quelli di personalità e comportamento. Ad esempio, nello sviluppo di un test sulla depressione, la TOS può aiutare a garantire che il test esamini accuratamente il vasto dominio dei sintomi depressivi, coprendo le diverse sfaccettature della depressione in proporzioni adeguate. La TOS può elencare le diverse aree o aspetti dei sintomi depressivi da valutare, includendo sia comportamenti osservabili sia pensieri e sentimenti interni, rilevanti per la diagnosi. In questo modo, la TOS funge da guida nella redazione degli item, assicurando che tutte le dimensioni rilevanti della depressione siano adeguatamente rappresentate nel test.\n\n\n5.2.9 Pool Iniziale degli Item\nCreare il pool iniziale di item. A questo punto, si procede a sviluppare una vasta gamma di item che coprano i diversi aspetti del costrutto di interesse. Questo pool iniziale di item dovrebbe essere variegato e ben bilanciato, rappresentando in modo adeguato la complessità del costrutto e i diversi livelli di abilità o atteggiamenti che si vogliono misurare. È importante anche scrivere le risposte corrette secondo ciò che si intende misurare e la direzione del punteggio del test.\n\n\n5.2.10 Revisione\nCondurre la revisione iniziale degli item (e apportare modifiche). Gli item raccolti nel pool iniziale vengono sottoposti a un esame attento da parte di esperti nel campo. Si valuta la pertinenza, la chiarezza, la coerenza e la validità dei singoli item. Sulla base dei feedback ricevuti dagli esperti, possono essere apportate modifiche o eliminati item problematici.\n\n\n5.2.11 Validazione\nNella fase di validazione di un test, l’implementazione di un ampio trial empirico degli item è un passaggio metodologico fondamentale. Tale processo prevede la somministrazione degli item a un campione rappresentativo della popolazione di riferimento. L’obiettivo è di valutare la discriminazione item-partecipante e di identificare eventuali anomalie o limitazioni strutturali della scala.\nLa definizione del campione target, ovvero la popolazione di riferimento per la quale il test è stato progettato, costituisce il primo passo nella pianificazione del campionamento. La selezione del campione di standardizzazione è cruciale in quanto determina il gruppo di riferimento per il confronto dei punteggi nei test normativi, oltre a stabilire le norme o i punteggi normativi del test. Questo è rilevante sia nei test normativi sia nei test basati su criteri, dove la performance delle popolazioni target è fondamentale nella definizione dei punteggi di taglio e nelle decisioni correlate alle prestazioni.\nUna volta identificata la popolazione target, si procede alla formulazione di un piano di campionamento. Idealmente, si ricercano campioni casuali veri della popolazione target, tuttavia, tale approccio è spesso impraticabile data l’impossibilità di conoscere e coinvolgere tutti i membri della popolazione. È quindi essenziale garantire la rappresentatività del campione attraverso un piano di campionamento stratificato proporzionale. Questo implica la definizione delle caratteristiche salienti (strati) della popolazione e la determinazione delle percentuali necessarie di soggetti con tali caratteristiche per assicurare la rappresentatività del campione.\nIn conclusione, il processo di campionamento deve essere accuratamente progettato per assicurare che il campione sia rappresentativo della popolazione target, superando le limitazioni pratiche e metodologiche incontrate nella selezione del campione stesso.\nNumero di soggetti. In ambito psicometrico non c’è un accordo univoco sulla dimensione del campione necessaria per condurre un’analisi fattoriale. Tuttavia, gli autori hanno fornito alcune indicazioni che possono essere utili come riferimento. Nunnally (1978) ha suggerito che il campione debba essere composto da almeno 10 soggetti per ogni item. Comrey e Lee (1992) hanno fornito una scala che valuta la qualità del campione in base alla dimensione: “molto scarsa” per 50, “scarso” per 100, “sufficiente” per 200, “buona” per 300, “molto buona” per 500 e “eccellente” per 1.000 o più. Altri autori hanno suggerito come regola generale di avere almeno 300 casi per l’analisi fattoriale (Tabachnick e Fidell, 2001). In ogni caso, è importante tenere presente che la scelta della dimensione del campione dipende anche dalla complessità del costrutto che si intende analizzare e dalla qualità degli item utilizzati nel test.\n\n\n5.2.12 Analisi degli item\nI dati raccolti dal test di campo vengono analizzati utilizzando metodi statistici adeguati. Questo processo mira a identificare item che non funzionano correttamente, che mostrano una bassa discriminazione o che potrebbero causare distorsioni nelle risposte. Gli item che superano questa fase sono considerati per la versione finale della scala.\n\n\n5.2.13 Revisione degli item\nSulla base dei risultati dell’analisi, gli item della scala possono essere rivisti o sostituiti, al fine di migliorarne l’accuratezza, la coerenza e l’affidabilità.\nNumero di item. La lunghezza del test dovrebbe essere adatta al suo scopo. Ad esempio, un test per valutare le abilità degli studenti delle scuole primarie non dovrebbe richiedere più di 30 minuti per essere completato, perché l’affaticamento e la noia possono influire sui risultati. Lo stesso vale per un test di personalità per adulti. In generale, un test dovrebbe essere il più breve possibile, ma deve raggiungere un livello accettabile di validità. Come regola generale, Kline (1986) suggerisce di avere almeno 50 domande nella versione finale del test.\n\n\n5.2.14 Calcolo dell’Affidabilità\nLa consistenza interna della scala viene valutata tramite il calcolo dell’affidabilità, ad esempio utilizzando il coefficiente alpha di Cronbach. Questo passaggio assicura che gli item scelti per la scala si correlino tra loro in modo coerente, riflettendo così la coerenza delle misure.\n\n\n5.2.15 Seconda Somministrazione\nUna volta apportate le revisioni agli item, viene eseguita una seconda somministrazione per confermare l’efficacia delle modifiche e per valutare l’affidabilità della versione rivista della scala su un nuovo campione.\n\n\n5.2.16 Ripetere i Passaggi Precedenti\nSe durante la seconda somministrazione emergono ancora problemi o se l’affidabilità della scala non raggiunge i livelli desiderati, è necessario ripetere i passaggi precedenti fino a raggiungere una versione della scala che soddisfi gli standard di qualità e affidabilità.\n\n\n5.2.17 Studi di validazione\nPer garantire la validità di una scala, è fondamentale condurre studi di validazione che dimostrino la capacità della scala di misurare con precisione il costrutto di interesse. Questi studi possono includere l’analisi delle relazioni tra i punteggi della scala e altre misure correlate, nonché il confronto tra gruppi con differenze note nel costrutto.\nCome per gli studi di affidabilità, è cruciale pianificare in anticipo gli studi di validità, assicurandosi che siano concettualizzati e progettati in modo da consentire la valutazione dell’adeguatezza delle interpretazioni proposte dei punteggi del test una volta completato. In linea con quanto discusso nel Capitolo 5, ciascuna delle cinque categorie fondamentali di evidenza di validità deve essere affrontata, pur variando l’enfasi e il livello di dettaglio a seconda dei costrutti valutati, delle interpretazioni proposte dei punteggi e degli scopi per i quali i punteggi saranno applicati.\nAd esempio, nei test di intelligenza progettati per predire il successo accademico, dovrebbero essere sottolineati e resi prioritari gli studi predittivi basati su criteri specifici che rappresentino il successo accademico, quali il tasso di laurea o i punteggi ACT. In maniera simile, i test destinati alla selezione del personale richiedono studi accurati sulla capacità predittiva dei punteggi in relazione al successo lavorativo, il quale dovrebbe essere definito in anticipo dall’ente o dall’individuo che utilizza il test. È importante riconoscere che la definizione di successo lavorativo può variare ampiamente; pertanto, è necessario chiarire e specificare le definizioni dei criteri utilizzati.\nPer una misura della psicopatologia dello sviluppo, ad esempio quella finalizzata a perfezionare l’accuratezza diagnostica nei Disturbi dello Spettro Autistico (ASD), l’accento dovrebbe essere posto sulla capacità dei punteggi del test di discriminare tra gruppi di soggetti diagnosticati (indipendentemente dal test) con ciascuno dei disturbi rilevanti. Gli studi di validità frequentemente si concentrano soltanto sulla distinzione tra gruppi diagnosticati e soggetti non diagnosticati o normodotati; tuttavia, questo approccio di ricerca risulta spesso di limitata utilità, in quanto la maggior parte dei test è in grado di differenziare tra normalità e patologia grazie ad un’alta affidabilità dei punteggi e a campioni di ampia dimensione. La sfida effettiva consiste nel distinguere tra le diverse diagnosi all’interno di un campione specifico, come ad esempio differenziare bambini con ASD da quelli con ADHD o depressione.\nIn definitiva, è fondamentale che gli studi di validità si concentrino sulle interpretazioni proposte dei punteggi e sulle loro applicazioni intenzionali. Non esistono limiti a ciò che può essere progettato come studi di validità; sono limitati solo dalla creatività del ricercatore e dalla loro conoscenza del costrutto e delle teorie pertinenti che incarnano il costrutto misurato.\n\n\n5.2.18 Linee Guida\nNel processo di sviluppo di un test, la fase finale si concentra sull’elaborazione di linee guida dettagliate, fondamentali per garantire una somministrazione appropriata e un’accurata valutazione dei punteggi. Questo passaggio si rivela cruciale per assicurare che il test sia utilizzato nel modo previsto e che i risultati siano interpretati correttamente.\nLe istruzioni per partecipare allo studio devono essere chiare e concise, fornendo un’idea generale degli obiettivi della ricerca e dei trattamenti previsti. I partecipanti devono essere informati dei benefici prevedibili e dei rischi, e della libertà di scegliere di non partecipare. Inoltre, la privacy dei partecipanti è protetta dalla legge sulla protezione dei dati personali e i loro dati verranno raccolti e conservati in forma anonima, tranne che per il nominativo. I partecipanti possono esercitare i propri diritti di protezione dei dati personali e interrompere la partecipazione in qualsiasi momento. Alla fine dello studio, i partecipanti possono ricevere i risultati della ricerca e possono rivolgersi al Comitato Etico dell’Università degli Studi di Firenze per segnalare qualsiasi problema. Prima di partecipare, i partecipanti devono firmare una dichiarazione di consenso informato per accettare di partecipare alla ricerca e di autorizzare il trattamento dei loro dati personali.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sviluppo dello strumento</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/02_development.html#considerazioni-finali",
    "href": "chapters/measurement/02_development.html#considerazioni-finali",
    "title": "5  Sviluppo dello strumento",
    "section": "5.3 Considerazioni Finali",
    "text": "5.3 Considerazioni Finali\nQuesto capitolo ha offerto un quadro pratico, così come discuso da Reynolds & Livingston (2021), riguardante le tappe cruciali e gli elementi fondamentali implicati nel processo di creazione di un test. Una particolare attenzione è stata rivolta alle fasi iniziali di concettualizzazione, vitali per la riuscita del progetto. Tali fasi includono l’identificazione della necessità di sviluppare un nuovo test e la formulazione di definizioni sia concettuali che operative dei costrutti da valutare.\nAbbiamo anche messo in evidenza l’importanza di delineare anticipatamente gli scopi specifici per cui il test è stato progettato, le modalità di interpretazione dei risultati e il pubblico target che ne farà uso. Questo approccio preliminare aiuta a definire chiaramente il contesto e le aspettative relative all’uso del test.\nUn altro aspetto trattato riguarda la stesura di una descrizione esauriente del test, che comprende la creazione di una tabella delle specifiche o di uno schema del test. Questo strumento si rivela essenziale per guidare sistematicamente lo sviluppatore attraverso le varie fasi di creazione del test, assicurando che tutti gli aspetti cruciali siano presi in considerazione.\n\n\n\n\nReynolds, C. R., & Livingston, R. (2021). Mastering modern psychological testing. Springer.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sviluppo dello strumento</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/03_equating.html",
    "href": "chapters/measurement/03_equating.html",
    "title": "6  Equating nei Test Psicologici",
    "section": "",
    "text": "6.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nL’equating è una procedura statistica utilizzata quando somministrazioni su più occasioni e a diversi gruppi di esaminandi possono portare a una sovraesposizione degli item, compromettendo la sicurezza del test. Per limitare l’esposizione degli item, si possono utilizzare forme alternative del test. Tuttavia, l’uso di forme multiple genera scale di punteggio diverse, che misurano il costrutto di interesse a livelli di difficoltà differenti. L’obiettivo dell’equating è quindi correggere queste differenze di difficoltà tra le forme del test, rendendo le scale di punteggio comparabili.\nL’equating è il processo che stabilisce una relazione statistica tra le distribuzioni dei punteggi di diverse forme di un test e, di conseguenza, tra le scale di punteggio associate. Questo permette di rendere confrontabili i punteggi, anche se derivano da forme diverse dello stesso test. Quando le forme del test sono costruite seguendo le stesse specifiche (ad esempio, misurano lo stesso costrutto con uguale affidabilità) e hanno caratteristiche statistiche simili (difficoltà comparabile, distribuzione dei punteggi analoga), questa relazione è definita come funzione di equating. La funzione di equating consente di tradurre i punteggi da una scala all’altra, trattandole come scale parallele secondo la teoria classica dei test.\nSe, invece, le forme del test differiscono in aspetti significativi, come la lunghezza o il contenuto, la relazione tra le scale viene definita linking, e non equating. In questo caso, le scale risultano correlate ma non parallele né interscambiabili, poiché non garantiscono piena equivalenza nei punteggi. La relazione tra queste scale è descritta da una funzione di linking.\nPer ottenere una funzione di equating valida, devono essere soddisfatti alcuni requisiti fondamentali:\nIn sintesi, si può dire che l’equating si applica a scale parallele nel senso della teoria classica dei test (stessa media della distribuzione, identica varianza, medesima affidabilità, misurazione dello stesso costrutto, struttura e difficoltà degli item pressoché identiche), mentre il linking si utilizza per scale non parallele, che possono essere comparabili solo in modo parziale (lunghezza differente, contenuti parzialmente diversi, variabilità non perfettamente sovrapponibile, difficoltà degli item non completamente omogenee).",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Equating nei Test Psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/03_equating.html#introduzione",
    "href": "chapters/measurement/03_equating.html#introduzione",
    "title": "6  Equating nei Test Psicologici",
    "section": "",
    "text": "Le forme del test devono misurare lo stesso costrutto con la stessa affidabilità.\nI risultati dell’equating devono garantire equità, ovvero ogni esaminando dovrebbe ottenere lo stesso risultato indipendentemente dal fatto che gli venga somministrata la forma \\(X\\) o la forma \\(Y\\) (Lord, 1980).\nLa funzione di equating deve essere simmetrica: se un punteggio è tradotto da una scala all’altra, il processo inverso deve restituire il punteggio originale.\nLa funzione deve essere invariabile rispetto alla popolazione degli esaminandi: deve funzionare nello stesso modo per diversi gruppi di persone.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Equating nei Test Psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/03_equating.html#progettazione-dellequating",
    "href": "chapters/measurement/03_equating.html#progettazione-dellequating",
    "title": "6  Equating nei Test Psicologici",
    "section": "\n6.2 Progettazione dell’Equating",
    "text": "6.2 Progettazione dell’Equating\nLe procedure di equating e linking dei punteggi osservati richiedono dati raccolti in diverse somministrazioni del test. Il disegno di equating specifica come le forme del test e i gruppi di esaminandi differiscono tra le somministrazioni. Per semplicità, i disegni di equating possono essere classificati in tre tipi principali: gruppo unico, gruppi equivalenti e gruppi non equivalenti. Le forme del test vengono costruite in base al tipo di gruppo considerato.\n\n6.2.1 Disegno a gruppo unico\nIn questo disegno, un unico gruppo, campionato dalla popolazione target \\(T\\), sostiene due forme del test \\(X\\) e \\(Y\\). È possibile bilanciare l’ordine di somministrazione (metà del gruppo sostiene \\(X\\) prima di \\(Y\\) e l’altra metà viceversa). Eventuali differenze nelle distribuzioni dei punteggi tra \\(X\\) e \\(Y\\) vengono attribuite esclusivamente alle forme del test, poiché si assume che l’abilità del gruppo resti costante.\n\n6.2.2 Disegno a gruppi equivalenti\nQuesto disegno prevede che due campioni casuali della popolazione target \\(T\\) sostengano rispettivamente \\(X\\) e \\(Y\\). Anche in questo caso, si assume che l’abilità dei due gruppi sia costante, e le differenze nei punteggi riflettono le differenze di difficoltà tra le forme del test.\n\n6.2.3 Disegno a gruppi non equivalenti\nQuando i gruppi non sono equivalenti, sorgono due problemi:\n\nLa popolazione target deve essere definita indirettamente, utilizzando campioni da due popolazioni di esaminandi, \\(P\\) e \\(Q\\).\nLe differenze di abilità tra i gruppi devono essere considerate, poiché possono confondere la stima delle differenze di difficoltà tra le forme del test.\n\nPer affrontare questi problemi, si utilizza un anchor test, \\(V\\), un test comune somministrato a entrambi i gruppi per controllare o eliminare le differenze di abilità. In alternativa, si possono usare covariate esterne, come i punteggi ottenuti in altri test.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Equating nei Test Psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/03_equating.html#funzioni-di-equating",
    "href": "chapters/measurement/03_equating.html#funzioni-di-equating",
    "title": "6  Equating nei Test Psicologici",
    "section": "\n6.3 Funzioni di Equating",
    "text": "6.3 Funzioni di Equating\nLe procedure di equating utilizzano funzioni matematiche per mappare i punteggi di una forma del test su una scala comune. Queste funzioni possono essere classificate come lineari (ad esempio, identità, media e regressione lineare) o non lineari (ad esempio, equipercentile). Ogni tipo di funzione si basa su assunzioni specifiche riguardo le differenze di difficoltà tra le forme del test.\n\n6.3.1 Funzione di identità\nLa funzione di identità si applica quando le forme del test hanno la stessa scala. In questo caso, i punteggi non vengono modificati, e la relazione tra \\(X\\) e \\(Y\\) è semplicemente:\n\\[\nid_Y(x) = x.\n\\]\n\n6.3.2 Funzione della media\nNell’equating basato sulla media, si assume che la forma \\(X\\) differisca dalla forma \\(Y\\) per un valore costante lungo l’intera scala dei punteggi. Ad esempio, se la forma \\(X\\) è più facile di 2 punti rispetto alla forma \\(Y\\) per gli esaminandi con punteggi alti, si assume che sia ugualmente più facile di 2 punti anche per quelli con punteggi bassi. Sebbene questa ipotesi di differenza costante possa risultare troppo restrittiva in molte situazioni di testing, l’equating basato sulla media è utile per illustrare alcuni concetti fondamentali dell’equating.\nLa funzione basata sulla media calcola le differenze di difficoltà tra le due forme utilizzando la differenza tra le medie dei punteggi, \\(\\mu_Y - \\mu_X\\). La funzione è espressa come:\n\\[\nmean_Y(x) = x + (\\mu_Y - \\mu_X).\n\\]\n\n6.3.3 Funzione Lineare\nA differenza della funzione basata sulla media, che assume una differenza costante tra due forme del test, l’equating lineare consente che le differenze di difficoltà tra le forme \\(X\\) e \\(Y\\) varino lungo la scala dei punteggi. Ad esempio, l’equating lineare permette che la forma \\(X\\) sia più difficile della forma \\(Y\\) per esaminandi con punteggi bassi, ma meno difficile per esaminandi con punteggi alti.\nNell’equating lineare, i punteggi che si trovano a una distanza standardizzata equivalente (in unità di deviazione standard) dalle rispettive medie vengono resi uguali. In questo modo, l’equating lineare tiene conto sia delle differenze nei valori medi, sia delle differenze nelle unità di scala (deviazioni standard) delle due forme. Definendo \\(\\sigma_X\\) e \\(\\sigma_Y\\) come le deviazioni standard dei punteggi nelle forme \\(X\\) e \\(Y\\), l’equazione di conversione lineare si basa sull’uguaglianza degli z-score tra le due forme, espressa come:\n\\[\n\\frac{x - \\mu_X}{\\sigma_X} = \\frac{y - \\mu_Y}{\\sigma_Y}.\n\\]\nRisolvendo questa equazione per \\(y\\), si ottiene la funzione di equating lineare:\n\\[\nlin_Y(x) = \\frac{\\sigma_Y}{\\sigma_X}x + \\left(\\mu_Y - \\frac{\\sigma_Y}{\\sigma_X}\\mu_X\\right),\n\\]\ndove il coefficiente angolare (\\(\\text{slope}\\)) è dato da \\(\\frac{\\sigma_Y}{\\sigma_X}\\) e l’intercetta (\\(\\text{intercept}\\)) è \\(\\mu_Y - \\frac{\\sigma_Y}{\\sigma_X}\\mu_X\\).\nSe le deviazioni standard delle due forme (\\(\\sigma_X\\) e \\(\\sigma_Y\\)) fossero uguali, la funzione lineare si ridurrebbe alla funzione basata sulla media. Questo dimostra che la funzione lineare generalizza quella basata sulla media, includendo la possibilità di differenze nella dispersione dei punteggi tra le forme.\n\n6.3.4 Equipercentile Equating\nL’equating equipercentile è una tecnica avanzata utilizzata nei casi in cui le differenze tra le forme di un test non possono essere descritte da una relazione lineare. A differenza dell’equating lineare, che assume una variazione proporzionale lungo tutta la scala dei punteggi, l’equipercentile permette di modellare differenze complesse utilizzando una curva che descrive le variazioni di difficoltà tra le forme.\nAd esempio, con l’equipercentile, la forma \\(X\\) potrebbe risultare più difficile della forma \\(Y\\) per i punteggi estremamente alti e bassi, ma meno difficile per i punteggi intermedi. Questa flessibilità rende il metodo equipercentile più generale rispetto a quello lineare.\nLa funzione di equating è definita come equipepercentile quando la distribuzione dei punteggi della forma \\(X\\), convertita sulla scala della forma \\(Y\\), risulta identica alla distribuzione dei punteggi della forma \\(Y\\) nella popolazione. In altre parole, i punteggi di \\(X\\) vengono mappati sui punteggi di \\(Y\\) in modo che i ranghi percentili corrispondano tra le due forme.\nIl processo di sviluppo della funzione equipercentile consiste nell’identificare, per ciascun punteggio di \\(X\\), il punteggio di \\(Y\\) che ha lo stesso rango percentile. Questo metodo garantisce che la relazione tra le due scale tenga conto delle differenze lungo l’intera distribuzione, offrendo una maggiore precisione nei casi in cui i punteggi non si distribuiscono in modo simile tra le due forme del test.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Equating nei Test Psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/03_equating.html#equating-dei-test-basato-sulla-ctt",
    "href": "chapters/measurement/03_equating.html#equating-dei-test-basato-sulla-ctt",
    "title": "6  Equating nei Test Psicologici",
    "section": "\n6.4 Equating dei Test basato sulla CTT",
    "text": "6.4 Equating dei Test basato sulla CTT\nNelle sezioni successive, applicheremo i metodi di equating basati sulla Classical Test Theory (CTT) descritti in precedenza. Come esempio, analizzeremo il caso di un disegno a gruppi equivalenti utilizzando i dati forniti dal set ACTmath, disponibile nel pacchetto equate. Questi dati derivano da due somministrazioni del test di matematica ACT (Kolen e Brennan, 2014). Il dataset è organizzato in un data frame con tre colonne: la prima rappresenta la scala dei punteggi (da 0 a 40 punti), mentre la seconda e la terza colonna riportano il numero di esaminandi delle forme \\(X\\) e \\(Y\\) che hanno ottenuto ciascun punteggio.\nNel disegno a gruppi equivalenti, un campione casuale di esaminandi sostiene la forma \\(X\\), mentre un secondo campione casuale sostiene la forma \\(Y\\). Poiché entrambi i campioni sono estratti in modo casuale dalla stessa popolazione, eventuali differenze nella distribuzione dei punteggi sono attribuite esclusivamente alle differenze di difficoltà tra le due forme del test. La randomizzazione garantisce, infatti, che i due gruppi siano equivalenti in termini di abilità valutata.\nProcediamo ora all’estrazione dei dati per le forme \\(X\\) e \\(Y\\), ottenendo le rispettive distribuzioni di frequenza.\n\nform_x &lt;- as.freqtab(ACTmath[, 1:2])\nform_y &lt;- as.freqtab(ACTmath[, c(1, 3)])\n\n\nhead(form_x)\n#&gt;   total count\n#&gt; 1     0     0\n#&gt; 2     1     1\n#&gt; 3     2     1\n#&gt; 4     3     3\n#&gt; 5     4     9\n#&gt; 6     5    18\n\n\nhead(form_y)\n#&gt;   total count\n#&gt; 1     0     0\n#&gt; 2     1     1\n#&gt; 3     2     3\n#&gt; 4     3    13\n#&gt; 5     4    42\n#&gt; 6     5    59\n\nEsaminiamo le statistiche descrittive.\n\nrbind(x = summary(form_x), y = summary(form_y))\n#&gt;   mean   sd  skew kurt min max    n\n#&gt; x 19.9 8.21 0.375 2.30   1  40 4329\n#&gt; y 19.0 8.94 0.353 2.15   1  40 4152\n\nEsaminiamo le distribuzioni dei punteggi. Il metodo di equating basato sulla CTT richiede che le due forme del test abbiano la stessa distribuzione.\n\nplot(x = form_x, main = \"Bar plot of the test scores on form X\")\n\n\n\n\n\n\n\n\nplot(x = form_y, main = \"Bar plot of the test scores on form Y\")",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Equating nei Test Psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/03_equating.html#metodi-di-equating-lineare",
    "href": "chapters/measurement/03_equating.html#metodi-di-equating-lineare",
    "title": "6  Equating nei Test Psicologici",
    "section": "\n6.5 Metodi di Equating Lineare",
    "text": "6.5 Metodi di Equating Lineare\nI metodi di equating lineare comprendono diverse funzioni: identità (ossia, nessun equating), media, lineare semplice e nonlineare.\n\n6.5.1 Equating Basato sulla Media\nQuesto metodo corregge i punteggi della nuova forma (X) utilizzando la differenza tra le medie delle due forme (X e Y) come costante fissa. In altre parole, il punteggio sulla scala di X viene traslato di un valore pari alla differenza delle medie, per compensare eventuali discrepanze di difficoltà tra le due forme.\nTrasformiamo i punteggi X con il metodo di equating basato sulla media usando la funzione equate().\n\nmean_yx &lt;- equate(form_x, form_y, type = \"mean\")\nmean_yx\n#&gt; \n#&gt; Mean Equating: form_x to form_y \n#&gt; \n#&gt; Design: equivalent groups \n#&gt; \n#&gt; Summary Statistics:\n#&gt;    mean   sd skew kurt  min  max    n\n#&gt; x  19.9 8.21 0.38 2.30 1.00 40.0 4329\n#&gt; y  19.0 8.94 0.35 2.15 1.00 40.0 4152\n#&gt; yx 19.0 8.21 0.38 2.30 0.13 39.1 4329\n#&gt; \n#&gt; Coefficients:\n#&gt; intercept     slope        cx        cy        sx        sy \n#&gt;    -0.873     1.000    20.000    20.000    40.000    40.000\n\nPossiamo anche consultare la tabella di concordanza per verificare come i punteggi della forma \\(X\\) siano cambiati dopo essere stati equiparati alla forma \\(Y\\). La tabella di concordanza indica che un esaminando che ha ottenuto un punteggio di 1 sulla forma \\(X\\) si aspetterebbe di ottenere un punteggio di 1-0.873 sulla forma \\(Y\\). In pratica, si tratta di una semplice trasformazione: il punteggio della forma \\(X\\) viene incrementato di -0.873.\n\nhead(mean_yx$concordance)\n#&gt;   scale     yx\n#&gt; 1     0 -0.873\n#&gt; 2     1  0.127\n#&gt; 3     2  1.127\n#&gt; 4     3  2.127\n#&gt; 5     4  3.127\n#&gt; 6     5  4.127\n\nPossiamo unire la tabella di concordanza alla forma \\(X\\) in modo da visualizzare insieme sia i punteggi originali che quelli equiparati per ogni esaminando.\n\n# Save the concordance table\nform_yx &lt;- mean_yx$concordance\n\n# Rename the first column to total\ncolnames(form_yx)[1] &lt;- \"total\"\n\n# Merge the concordance table to form x\ndata_xy &lt;- merge(form_x, form_yx)\nhead(data_xy)\n#&gt;   total count     yx\n#&gt; 1     0     0 -0.873\n#&gt; 2     1     1  0.127\n#&gt; 3     2     1  1.127\n#&gt; 4     3     3  2.127\n#&gt; 5     4     9  3.127\n#&gt; 6     5    18  4.127\n\nSia form_x che form_xy sono tabelle di contingenza.\n\nform_yx |&gt; head()\n#&gt;   total     yx\n#&gt; 1     0 -0.873\n#&gt; 2     1  0.127\n#&gt; 3     2  1.127\n#&gt; 4     3  2.127\n#&gt; 5     4  3.127\n#&gt; 6     5  4.127\n\n\nform_x |&gt; head()\n#&gt;   total count\n#&gt; 1     0     0\n#&gt; 2     1     1\n#&gt; 3     2     1\n#&gt; 4     3     3\n#&gt; 5     4     9\n#&gt; 6     5    18\n\nPer ottenere i dati grezzi, prima le trasformiamo in un oggetto data.frame.\n\nform_x_df &lt;- form_x |&gt; as.data.frame()\nform_yx_df &lt;- form_yx |&gt; as.data.frame()\n\nPoi ricostruiamo i dati grezzi.\n\n# Ricostruire i dati grezzi per form_x\nraw_data_x &lt;- rep(form_x_df$total, form_x_df$count)\n\n# Ricostruire i dati grezzi per form_yx (usando i punteggi trasformati yx)\nraw_data_yx &lt;- rep(form_yx_df$yx, form_x_df$count) # Assumendo che i conteggi siano identici\n\nI dati trasformati X sono semplicemente una tralazione lineare dei dati originari.\n\nplot(raw_data_x, raw_data_yx, type = 'l')\n\n\n\n\n\n\n\n\n6.5.2 Equating Lineare\nQuesto metodo corregge i punteggi della nuova forma (\\(X\\)) utilizzando sia la differenza delle medie che quella delle deviazioni standard tra le due forme (\\(X\\) e \\(Y\\)), applicando un’equazione lineare.\n\nlinear_yx &lt;- equate(form_x, form_y, type = \"linear\")\nlinear_yx\n#&gt; \n#&gt; Linear Equating: form_x to form_y \n#&gt; \n#&gt; Design: equivalent groups \n#&gt; \n#&gt; Summary Statistics:\n#&gt;    mean   sd skew kurt   min  max    n\n#&gt; x  19.9 8.21 0.38 2.30  1.00 40.0 4329\n#&gt; y  19.0 8.94 0.35 2.15  1.00 40.0 4152\n#&gt; yx 19.0 8.94 0.38 2.30 -1.54 40.9 4329\n#&gt; \n#&gt; Coefficients:\n#&gt; intercept     slope        cx        cy        sx        sy \n#&gt;     -2.63      1.09     20.00     20.00     40.00     40.00\n\n\nplot(linear_yx)\n\n\n\n\n\n\n\nPossiamo anche eseguire un bootstrap per il processo di equating e creare un grafico degli errori standard:\n\nlinear_yx_boot &lt;- equate(form_x, form_y, type = \"linear\", boot = TRUE, reps = 5)\nplot(linear_yx_boot, out = \"se\")\n\n\n\n\n\n\n\n\n6.5.3 Equating Non Lineare\nI metodi di equating non lineare includono l’equipercentile, l’arco circolare e le funzioni composte.\n\n6.5.3.1 Equating Equipercentile\nL’equipercentile funziona suddividendo i punteggi delle forme \\(X\\) e \\(Y\\) in percentili. Successivamente, i percentili della forma \\(X\\) vengono abbinati ai percentili della forma \\(Y\\).\nAd esempio, possiamo applicare l’equipercentile con il seguente comando:\n\nequi_yx &lt;- equate(form_x, form_y, type = \"equipercentile\")\n\nOra possiamo verificare come i punteggi di \\(X\\) siano stati corretti in base ai ranghi percentili delle forme \\(X\\) e \\(Y\\). Il grafico seguente mostra che le maggiori discrepanze tra queste due funzioni si trovano alle estremità della distribuzione:\n\nplot(equi_yx$concordance$yx ~ equi_yx$concordance$scale,\n    type = \"p\",\n    xlab = \"Punteggi Forma X\",\n    ylab = \"Punteggi corretti su Forma Y\"\n)\npoints(linear_yx$concordance$yx ~ linear_yx$concordance$scale, pch = 4)\n\n\n\n\n\n\n\n\n6.5.3.2 Smoothing Loglineare\nPossiamo anche aggiungere uno smoothing loglineare al processo e confrontarlo con il metodo equipercentile standard. La linea curva indica l’equipercentile con smussamento loglineare, mentre la linea dritta (con un punto di rottura) rappresenta l’equipercentile standard.\nQuesto confronto consente di osservare come il processo di smussamento renda la relazione tra le due forme più fluida e meno sensibile alle variazioni estreme.\n\nequismooth_yx &lt;- equate(form_x, form_y, type = \"equipercentile\", smooth = \"loglin\", degree = 3)\n\n# Compare equating functions\nplot(equi_yx, equismooth_yx, addident = FALSE)\n\n\n\n\n\n\n\nIn questo caso, lo smoothing non ha praticamente effetto.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Equating nei Test Psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/03_equating.html#gruppi-non-equivalenti",
    "href": "chapters/measurement/03_equating.html#gruppi-non-equivalenti",
    "title": "6  Equating nei Test Psicologici",
    "section": "\n6.6 Gruppi Non Equivalenti",
    "text": "6.6 Gruppi Non Equivalenti\nIn un disegno a gruppi non equivalenti, non si può assumere che la popolazione di esaminandi che sostiene le diverse forme del test provenga dalla stessa popolazione. Pertanto, è necessario tenere conto delle potenziali differenze nelle loro abilità relative al costrutto misurato. Quando si utilizza un disegno a gruppi non equivalenti, è indispensabile specificare un metodo di equating adeguato.\nGeneralmente, questi metodi stabiliscono una relazione tra i punteggi totali della forma \\(X\\) e della forma \\(Y\\) attraverso i punteggi ottenuti su un insieme comune di item presenti in entrambe le forme (noti come anchor items) e la creazione di una popolazione sintetica ponderata. Per maggiori informazioni su questi metodi, si rimanda ad Albano (2016).\nI metodi di equating disponibili nel pacchetto equate per i gruppi non equivalenti includono:\n\nTucker\nNominal\nLevine true score\nBraun/Holland\nFrequency\nChained\n\nPer un approfondimento, si rimanda al manuale del pacchetto equate.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Equating nei Test Psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/03_equating.html#equating-basato-sulla-teoria-della-risposta-allitem",
    "href": "chapters/measurement/03_equating.html#equating-basato-sulla-teoria-della-risposta-allitem",
    "title": "6  Equating nei Test Psicologici",
    "section": "\n6.7 Equating Basato sulla Teoria della Risposta all’Item",
    "text": "6.7 Equating Basato sulla Teoria della Risposta all’Item\nL’equating basato sulla Item Response Theory (IRT) rappresenta un approccio avanzato per confrontare e rendere comparabili punteggi di diverse forme di un test. Questo metodo sfrutta la capacità della IRT di modellare la relazione tra le risposte degli esaminandi e le caratteristiche degli item. I metodi IRT di equating si dividono in due categorie principali:\n\nEquating attraverso item comuni\nEquating attraverso persone comuni\n\nIn entrambi i casi, gli item comuni o le persone comuni fungono da ponte tra le due forme del test, consentendo di stabilire una scala comune per il confronto dei punteggi.\n\n6.7.1 Equating Attraverso Item Comuni\nL’equating basato su item comuni utilizza item identici inclusi in entrambe le forme del test. Questi item condivisi, detti item ancorati (anchor items), permettono di calibrare le due forme su una scala comune. Le applicazioni tipiche includono:\n\nEquating tramite costanti di equating\nIn questo approccio, si utilizzano costanti per collegare i parametri degli item delle due forme. Queste costanti sono derivate dagli item comuni, che aiutano a compensare le differenze tra le due forme del test.\nEquating tramite curve caratteristiche del test\nLe curve caratteristiche del test (TCC, Test Characteristic Curves) rappresentano la relazione tra i punteggi latenti (\\(\\theta\\)) e i punteggi osservati. Con questo metodo, si utilizzano le TCC delle due forme per stabilire una corrispondenza tra i punteggi.\nEquating tramite calibrazione simultanea\nLa calibrazione simultanea prevede di stimare i parametri degli item di entrambe le forme del test in un’unica analisi. Gli item comuni agiscono come vincoli per collegare le due forme sulla stessa scala.\n\n6.7.2 Vantaggi dell’Equating IRT\n\nConsente di correggere le differenze tra le forme del test basandosi su parametri specifici degli item (difficoltà, discriminazione, probabilità di indovinare).\nNon richiede che le distribuzioni delle abilità degli esaminandi siano identiche per le due forme, rendendolo particolarmente utile per disegni a gruppi non equivalenti.\nPuò gestire forme di test con numero di item diverso o contenuti solo parzialmente sovrapposti.\n\nL’utilizzo dell’IRT garantisce una maggiore precisione rispetto ai metodi tradizionali, soprattutto in presenza di forme di test complesse. Le funzionalità necessarie per implementare questi metodi sono disponibili nel pacchetto R equateIRT, che offre strumenti avanzati per l’equating basato sulla teoria della risposta all’item. Per ulteriori approfondimenti, si consiglia di consultare la documentazione del pacchetto.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Equating nei Test Psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/03_equating.html#riflessioni-conclusive",
    "href": "chapters/measurement/03_equating.html#riflessioni-conclusive",
    "title": "6  Equating nei Test Psicologici",
    "section": "\n6.8 Riflessioni Conclusive",
    "text": "6.8 Riflessioni Conclusive\nIn questo capitolo abbiamo esplorato i metodi di equating basati sulla Classical Test Theory (CTT) e abbiamo fornito alcuni accenni sui metodi basati sulla Item Response Theory (IRT). Abbiamo analizzato come l’equating consenta di confrontare i punteggi di diverse forme di un test, garantendo equità e comparabilità anche in presenza di differenze di difficoltà tra le forme. In conclusione, la capacità di confrontare e convertire punteggi tra diverse forme di test non è solo un esercizio statistico, ma un elemento fondamentale per assicurare l’equità e la validità dei processi valutativi.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Equating nei Test Psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/03_equating.html#session-info",
    "href": "chapters/measurement/03_equating.html#session-info",
    "title": "6  Equating nei Test Psicologici",
    "section": "\n6.9 Session Info",
    "text": "6.9 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] equate_2.0.8      ggokabeito_0.1.0  see_0.10.0        MASS_7.3-65      \n#&gt;  [5] viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1   \n#&gt;  [9] gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6   \n#&gt; [13] semPlot_1.1.6     lavaan_0.6-19     psych_2.4.12      scales_1.3.0     \n#&gt; [17] markdown_1.13     knitr_1.49        lubridate_1.9.4   forcats_1.0.0    \n#&gt; [21] stringr_1.5.1     dplyr_1.1.4       purrr_1.0.4       readr_2.1.5      \n#&gt; [25] tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0  \n#&gt; [29] here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.0      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] mime_0.12           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [25] Matrix_1.7-2        R6_2.6.1            fastmap_1.2.0      \n#&gt;  [28] rbibutils_2.3       shiny_1.10.0        digest_0.6.37      \n#&gt;  [31] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n#&gt;  [34] rprojroot_2.0.4     Hmisc_5.2-2         timechange_0.3.0   \n#&gt;  [37] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n#&gt;  [40] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n#&gt;  [43] carData_3.0-5       ggsignif_0.6.4      corpcor_1.6.10     \n#&gt;  [46] gtools_3.9.5        tools_4.4.2         pbivnorm_0.6.0     \n#&gt;  [49] foreign_0.8-88      zip_2.3.2           httpuv_1.6.15      \n#&gt;  [52] nnet_7.3-20         glue_1.8.0          quadprog_1.5-8     \n#&gt;  [55] nlme_3.1-167        promises_1.3.2      lisrelToR_0.3      \n#&gt;  [58] grid_4.4.2          checkmate_2.3.2     cluster_2.1.8      \n#&gt;  [61] reshape2_1.4.4      generics_0.1.3      gtable_0.3.6       \n#&gt;  [64] tzdb_0.4.0          data.table_1.17.0   hms_1.1.3          \n#&gt;  [67] car_3.1-3           sem_3.1-16          pillar_1.10.1      \n#&gt;  [70] rockchalk_1.8.157   later_1.4.1         splines_4.4.2      \n#&gt;  [73] lattice_0.22-6      survival_3.8-3      kutils_1.73        \n#&gt;  [76] tidyselect_1.2.1    miniUI_0.1.1.1      pbapply_1.7-2      \n#&gt;  [79] reformulas_0.4.0    stats4_4.4.2        xfun_0.51          \n#&gt;  [82] qgraph_1.9.8        arm_1.14-4          stringi_1.8.4      \n#&gt;  [85] yaml_2.3.10         pacman_0.5.1        boot_1.3-31        \n#&gt;  [88] evaluate_1.0.3      codetools_0.2-20    mi_1.1             \n#&gt;  [91] cli_3.6.4           RcppParallel_5.1.10 rpart_4.1.24       \n#&gt;  [94] xtable_1.8-4        Rdpack_2.6.2        munsell_0.5.1      \n#&gt;  [97] Rcpp_1.0.14         coda_0.19-4.1       png_0.1-8          \n#&gt; [100] XML_3.99-0.18       parallel_4.4.2      jpeg_0.1-10        \n#&gt; [103] lme4_1.1-36         mvtnorm_1.3-3       openxlsx_4.2.8     \n#&gt; [106] rlang_1.1.5         multcomp_1.4-28     mnormt_2.1.1",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Equating nei Test Psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E4_norming.html",
    "href": "chapters/measurement/E4_norming.html",
    "title": "\n7  ✏️ Esercizi\n",
    "section": "",
    "text": "7.1 Normazione Basata sulla Regressione: Metodi Avanzati per l’Interpretazione dei Test Psicometrici\nLa normazione basata sulla regressione rappresenta un metodo statistico avanzato utilizzato per trasformare i punteggi grezzi di un test in punteggi standardizzati, consentendo un confronto equo tra individui di diverse età o caratteristiche demografiche. Questo approccio supera i limiti delle metodologie tradizionali, che tipicamente si basano su fasce di età discrete e tabelle di conversione statiche, introducendo modelli di regressione che descrivono in modo continuo la relazione tra i punteggi grezzi e le variabili predittive, come l’età.\nTale metodologia risulta particolarmente preziosa nell’ambito dei test psicometrici e delle valutazioni cognitive, dove le performance possono variare significativamente in funzione dello sviluppo cognitivo o di altri fattori demografici rilevanti.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E4_norming.html#principio-fondamentale-e-vantaggi",
    "href": "chapters/measurement/E4_norming.html#principio-fondamentale-e-vantaggi",
    "title": "\n7  ✏️ Esercizi\n",
    "section": "\n7.2 Principio Fondamentale e Vantaggi",
    "text": "7.2 Principio Fondamentale e Vantaggi\nNei test di valutazione cognitiva, come quelli che misurano il ragionamento verbale, l’intelligenza o la memoria, si osserva naturalmente una variazione dei punteggi grezzi in relazione all’età. Ad esempio, un bambino di 7 anni tipicamente ottiene un punteggio inferiore rispetto a un adolescente di 15 anni in molti test cognitivi, senza che questo indichi necessariamente una performance deficitaria, ma piuttosto una normale differenza nello sviluppo cognitivo atteso per quelle fasce d’età.\nLa normazione basata sulla regressione presenta diversi vantaggi rispetto ai metodi tradizionali:\n\n\nContinuità: Evita i “salti” artificiali nei punteggi normativi che possono verificarsi al passaggio tra fasce d’età discrete.\n\nPrecisione: Produce stime più accurate dei punteggi normativi, specialmente per età ai confini delle fasce tradizionali.\n\nEfficienza statistica: Utilizza in modo più efficiente tutti i dati disponibili nel campione normativo.\n\nFlessibilità: Può incorporare facilmente più variabili predittive oltre all’età (es. genere, livello di istruzione).\n\nAdattabilità: Può modellare relazioni non lineari complesse tra età e performance.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E4_norming.html#applicazione-della-normazione-con-il-test-ids-2",
    "href": "chapters/measurement/E4_norming.html#applicazione-della-normazione-con-il-test-ids-2",
    "title": "\n7  ✏️ Esercizi\n",
    "section": "\n7.3 Applicazione della Normazione con il Test IDS-2",
    "text": "7.3 Applicazione della Normazione con il Test IDS-2\n\n7.3.1 Descrizione dello Strumento\nIl test IDS-2 (Intelligence and Development Scales, 2a Edizione) è una batteria completa che valuta lo sviluppo cognitivo e le competenze evolutive in soggetti dai 5 ai 20 anni. Questo strumento, ampiamente utilizzato in ambito clinico ed educativo, permette di ottenere un profilo dettagliato delle abilità del soggetto in diverse aree funzionali.\nNell’esempio che segue, analizziamo i dati relativi al Sottotest 14 (Denominare Contrari), che fa parte della scala di Ragionamento Verbale e misura specificamente:\n\n\nCompetenza lessicale: L’ampiezza e la profondità del vocabolario posseduto\n\nFlessibilità cognitiva: La capacità di identificare relazioni semantiche opposte\n\nVelocità di elaborazione semantica: L’efficienza nell’accesso al lessico mentale\n\n7.3.2 Dataset e Preparazione dei Dati\nIl dataset IDS2_sample contiene dati di un campione normativo con le seguenti variabili principali:\n\n\nage: Età dei partecipanti (espressa in mesi)\n\ny14: Punteggio grezzo ottenuto nel sottotest “Denominare Contrari”\n\n\n# Caricamento dati\nIDS2_sample &lt;- read.table(\n    here::here(\"data\", \"IDS2_sample.txt\"),\n    header = TRUE\n    )\n\n# Anteprima dati\nhead(IDS2_sample)\n#&gt;     age y7 y14\n#&gt; 1 10.97 17  17\n#&gt; 2 11.67 20  19\n#&gt; 3 11.44 22  23\n#&gt; 4 10.05 14  14\n#&gt; 5 14.18 20  19\n#&gt; 6  6.55 11  11\n\nPer migliorare la stabilità numerica nei calcoli statistici successivi, aggiungiamo una costante infinitesimale ai punteggi per eliminare eventuali valori zero che potrebbero creare problemi in alcune trasformazioni matematiche:\n\nIDS2_sample$y14_a &lt;- IDS2_sample$y14 + 0.0001",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E4_norming.html#modellazione-statistica-avanzata-con-il-bcpe",
    "href": "chapters/measurement/E4_norming.html#modellazione-statistica-avanzata-con-il-bcpe",
    "title": "\n7  ✏️ Esercizi\n",
    "section": "\n7.4 Modellazione Statistica Avanzata con il BCPE",
    "text": "7.4 Modellazione Statistica Avanzata con il BCPE\nIl modello Box-Cox Power Exponential (BCPE) rappresenta una scelta particolarmente avanzata per la normazione basata sulla regressione, poiché permette di modellare simultaneamente quattro parametri fondamentali della distribuzione dei punteggi:\n\n\nTendenza centrale (μ): Il valore medio o mediano dei punteggi grezzi in funzione dell’età\n\nDispersione (σ): La variabilità dei punteggi attorno alla tendenza centrale\n\nAsimmetria (ν): Il grado di simmetria/asimmetria nella distribuzione dei punteggi\n\nCurtosi (τ): La forma delle code della distribuzione (più o meno pesanti rispetto alla distribuzione normale)\n\nL’implementazione del modello utilizza P-splines (spline penalizzate), che garantiscono che la relazione tra punteggio grezzo ed età sia modellata in modo flessibile e adattativo, senza imporre forme funzionali rigide. Questo è particolarmente importante quando la relazione tra età e performance non segue un semplice andamento lineare, come spesso accade nei test cognitivi.\nIl modello viene implementato utilizzando la libreria gamlss in R:\n\nBCPE_mod_sp &lt;- gamlss(\n    y14_a ~ pbm(age, method = \"GAIC\", k = log(nrow(IDS2_sample)), inter = 5, mono = \"up\"),\n    sigma.formula = ~ pb(age, method = \"GAIC\", k = log(nrow(IDS2_sample)), inter = 5),\n    nu.formula = ~1,\n    tau.formula = ~1,\n    family = BCPE,\n    data = IDS2_sample,\n    method = RS(1000)\n)\n#&gt; GAMLSS-RS iteration 1: Global Deviance = 8348 \n#&gt; GAMLSS-RS iteration 2: Global Deviance = 8346 \n#&gt; GAMLSS-RS iteration 3: Global Deviance = 8346 \n#&gt; GAMLSS-RS iteration 4: Global Deviance = 8346 \n#&gt; GAMLSS-RS iteration 5: Global Deviance = 8346 \n#&gt; GAMLSS-RS iteration 6: Global Deviance = 8346\n\n\n7.4.1 Elementi Chiave del Modello\n\n\npbm(age, ..., mono = \"up\"): Utilizza P-splines monotone crescenti per modellare la tendenza centrale, riflettendo l’aspettativa che le prestazioni cognitive mediamente aumentino con l’età.\n\npb(age, ...): Utilizza P-splines standard (non monotone) per modellare la dispersione, permettendo variazioni nella variabilità dei punteggi durante lo sviluppo.\n\nmethod = \"GAIC\", k = log(nrow(IDS2_sample)): Utilizza il criterio GAIC (Generalized Akaike Information Criterion) per selezionare automaticamente il grado ottimale di smoothing nelle splines.\n\ninter = 5: Specifica il numero di intervalli per le splines, bilanciando flessibilità e parsimonia.\n\nnu.formula = ~1, tau.formula = ~1: Modella asimmetria e curtosi come costanti indipendenti dall’età, semplificando il modello.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E4_norming.html#visualizzazione-e-interpretazione-delle-curve-centili",
    "href": "chapters/measurement/E4_norming.html#visualizzazione-e-interpretazione-delle-curve-centili",
    "title": "\n7  ✏️ Esercizi\n",
    "section": "\n7.5 Visualizzazione e Interpretazione delle Curve Centili",
    "text": "7.5 Visualizzazione e Interpretazione delle Curve Centili\nLa funzione centiles.fan di gamlss consente di visualizzare graficamente la distribuzione dei punteggi normati in funzione dell’età, fornendo un potente strumento per l’interpretazione dei risultati:\n\ncentiles.fan(\n    BCPE_mod_sp,\n    xvar = IDS2_sample$age,\n    points = TRUE,\n    colors = \"gray\",\n    col = \"black\",\n    pch = 16,\n    ylab = \"Punteggio grezzo\",\n    xlab = \"Età in mesi\"\n)\n\n\n\n\n\n\n\n\n7.5.1 Interpretazione delle Curve Centili\nLe curve centili rappresentano visivamente la distribuzione condizionata dei punteggi a ogni età:\n\n\n50° percentile (curva mediana): Rappresenta il valore mediano previsto per una data età. Un bambino che ottiene un punteggio su questa curva ha una performance esattamente nella media rispetto ai suoi coetanei.\n\n10°-90° percentile: Delimita l’intervallo in cui ricade l’80% centrale dei punteggi. Punteggi all’interno di questo intervallo sono considerati nella norma.\n\n2°-98° percentile: Delimita una fascia di valori più ampia che include il 96% dei soggetti. Punteggi al di fuori di questo intervallo meritano particolare attenzione.\n\n&lt;2° o &gt;98° percentile: Valori estremamente bassi o alti che possono indicare significative deviazioni dalla norma e giustificare approfondimenti diagnostici.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E4_norming.html#calcolo-dei-punteggi-normati-individuali",
    "href": "chapters/measurement/E4_norming.html#calcolo-dei-punteggi-normati-individuali",
    "title": "\n7  ✏️ Esercizi\n",
    "section": "\n7.6 Calcolo dei Punteggi Normati Individuali",
    "text": "7.6 Calcolo dei Punteggi Normati Individuali\nPer determinare il punteggio normato di un individuo, dobbiamo stabilire la sua posizione relativa rispetto alla distribuzione condizionata dei punteggi per la sua specifica età. Questo processo avviene in due fasi principali:\n\n7.6.1 1. Calcolo del Percentile Individuale\nPrima calcoliamo il percentile corrispondente al punteggio grezzo dell’individuo, data la sua età:\n\nCDF_sample &lt;- pBCPE(\n    IDS2_sample$y14_a,\n    mu = BCPE_mod_sp$mu.fv,\n    sigma = BCPE_mod_sp$sigma.fv,\n    nu = BCPE_mod_sp$nu.fv,\n    tau = BCPE_mod_sp$tau.fv,\n    lower.tail = TRUE\n)\n\nIn questo passaggio, pBCPE calcola la funzione di distribuzione cumulativa del modello BCPE, restituendo la probabilità che un soggetto con una data età ottenga un punteggio inferiore o uguale al punteggio osservato. Questo valore rappresenta il percentile del soggetto nella distribuzione.\n\n7.6.2 2. Conversione in Punteggi Z\nSuccessivamente, convertiamo i percentili in punteggi Z utilizzando l’inversa della distribuzione normale standard:\n\nIDS2_sample$z_score &lt;- qnorm(CDF_sample)\n\nIl punteggio Z (o punteggio standard) rappresenta la distanza del punteggio individuale dalla media della popolazione di riferimento (per quella specifica età), espressa in unità di deviazione standard. L’interpretazione dei punteggi Z è la seguente:\n\n\nZ = 0: Performance esattamente nella media\n\nZ = +1: Performance superiore di una deviazione standard rispetto alla media\n\nZ = -1: Performance inferiore di una deviazione standard rispetto alla media\n\nZ ≥ +2: Performance significativamente superiore alla media (circa top 2.5%)\n\nZ ≤ -2: Performance significativamente inferiore alla media (circa bottom 2.5%)\n\n7.6.3 Esempio Concreto\nPer illustrare il processo con un esempio concreto:\n\nSupponiamo che un bambino di 96 mesi (8 anni) ottenga un punteggio grezzo di 12 nel sottotest “Denominare Contrari”\nDalle curve centili, determiniamo che per questa età il punteggio mediano atteso è circa 11\nApplicando il modello BCPE, calcoliamo che questo punteggio corrisponde al 65° percentile\nConvertendo in punteggio Z, otteniamo Z = 0.39\nInterpretiamo: il bambino ha una performance leggermente sopra la media rispetto ai coetanei, ma ben all’interno del range normale",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E4_norming.html#rappresentazione-grafica-della-relazione-tra-punteggi-grezzi-e-normati",
    "href": "chapters/measurement/E4_norming.html#rappresentazione-grafica-della-relazione-tra-punteggi-grezzi-e-normati",
    "title": "\n7  ✏️ Esercizi\n",
    "section": "\n7.7 Rappresentazione Grafica della Relazione tra Punteggi Grezzi e Normati",
    "text": "7.7 Rappresentazione Grafica della Relazione tra Punteggi Grezzi e Normati\nPer comprendere meglio la relazione tra i punteggi grezzi originali e i punteggi Z derivati, possiamo visualizzare questa relazione graficamente:\n\nplot(IDS2_sample$y14, IDS2_sample$z_score,\n     xlab = \"Punteggio Grezzo\",\n     ylab = \"Punteggio Normato (Z-score)\",\n     main = \"Relazione tra Punteggi Grezzi e Punteggi Normati\",\n     pch = 16, col = \"blue\")\n\n# Aggiungere una linea di tendenza\nabline(lm(z_score ~ y14, data = IDS2_sample), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\nQuesta visualizzazione mostra chiaramente che:\n\nLa relazione tra punteggi grezzi e Z-score non è perfettamente lineare\nLo stesso punteggio grezzo può corrispondere a diversi Z-score a seconda dell’età del soggetto\nLa dispersione attorno alla linea di tendenza illustra l’effetto della correzione per età",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E4_norming.html#estensioni-e-considerazioni-avanzate",
    "href": "chapters/measurement/E4_norming.html#estensioni-e-considerazioni-avanzate",
    "title": "\n7  ✏️ Esercizi\n",
    "section": "\n7.8 Estensioni e Considerazioni Avanzate",
    "text": "7.8 Estensioni e Considerazioni Avanzate\nIl modello BCPE presentato può essere ulteriormente esteso per incorporare altre variabili predittive rilevanti oltre all’età, come:\n\n\nGenere: Se esistono differenze sistematiche tra maschi e femmine\n\nLivello di istruzione: Per tenere conto del background educativo\n\nBackground socioeconomico: Per controlli demografici più completi\n\nBilinguismo: Per considerare l’influenza di specifiche caratteristiche linguistiche\n\nInoltre, è possibile implementare modelli ancora più flessibili utilizzando:\n\n\nReti neurali: Per catturare relazioni non parametriche complesse\n\nModelli bayesiani: Per incorporare conoscenze a priori e quantificare l’incertezza nelle stime\n\nAnalisi longitudinali: Per tracciare lo sviluppo individuale nel tempo",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E4_norming.html#conclusione",
    "href": "chapters/measurement/E4_norming.html#conclusione",
    "title": "\n7  ✏️ Esercizi\n",
    "section": "\n7.9 Conclusione",
    "text": "7.9 Conclusione\nLa normazione basata sulla regressione, specialmente quando implementata attraverso modelli avanzati come il BCPE con P-splines, offre un approccio statisticamente robusto e clinicamente rilevante per l’interpretazione dei test psicometrici. Rispetto ai metodi tradizionali basati su tabelle normative discrete, questo approccio garantisce:\n\n\nMaggiore precisione: Stime più accurate dei valori normativi a tutte le età\n\nContinuità: Eliminazione degli artefatti ai confini delle fasce d’età\n\nFlessibilità: Capacità di modellare relazioni complesse e non lineari tra età e performance\n\nEfficienza statistica: Utilizzo ottimale dei dati disponibili nel campione normativo\n\nL’implementazione di questi metodi avanzati di normazione permette ai clinici e ai ricercatori di ottenere valutazioni più precise e affidabili, migliorando sia l’accuratezza diagnostica che l’utilità dei test psicometrici nella pratica professionale e nella ricerca.\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] parallel  splines   stats     graphics  grDevices utils     datasets \n#&gt; [8] methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] gamlss.add_5.1-13 rpart_4.1.24      nnet_7.3-20       mgcv_1.9-1       \n#&gt;  [5] gamlss_5.4-22     nlme_3.1-167      gamlss.dist_6.1-1 gamlss.data_6.0-6\n#&gt;  [9] rio_1.2.3         ggokabeito_0.1.0  see_0.10.0        MASS_7.3-65      \n#&gt; [13] viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1   \n#&gt; [17] gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6   \n#&gt; [21] semPlot_1.1.6     lavaan_0.6-19     psych_2.4.12      scales_1.3.0     \n#&gt; [25] markdown_1.13     knitr_1.49        lubridate_1.9.4   forcats_1.0.0    \n#&gt; [29] stringr_1.5.1     dplyr_1.1.4       purrr_1.0.4       readr_2.1.5      \n#&gt; [33] tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0  \n#&gt; [37] here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.0      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] mime_0.12           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [25] Matrix_1.7-2        R6_2.6.1            fastmap_1.2.0      \n#&gt;  [28] rbibutils_2.3       shiny_1.10.0        digest_0.6.37      \n#&gt;  [31] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n#&gt;  [34] rprojroot_2.0.4     Hmisc_5.2-2         timechange_0.3.0   \n#&gt;  [37] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n#&gt;  [40] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n#&gt;  [43] carData_3.0-5       ggsignif_0.6.4      corpcor_1.6.10     \n#&gt;  [46] gtools_3.9.5        tools_4.4.2         pbivnorm_0.6.0     \n#&gt;  [49] foreign_0.8-88      zip_2.3.2           httpuv_1.6.15      \n#&gt;  [52] glue_1.8.0          quadprog_1.5-8      promises_1.3.2     \n#&gt;  [55] lisrelToR_0.3       grid_4.4.2          checkmate_2.3.2    \n#&gt;  [58] cluster_2.1.8       reshape2_1.4.4      generics_0.1.3     \n#&gt;  [61] gtable_0.3.6        tzdb_0.4.0          data.table_1.17.0  \n#&gt;  [64] hms_1.1.3           car_3.1-3           sem_3.1-16         \n#&gt;  [67] pillar_1.10.1       rockchalk_1.8.157   later_1.4.1        \n#&gt;  [70] lattice_0.22-6      survival_3.8-3      kutils_1.73        \n#&gt;  [73] tidyselect_1.2.1    miniUI_0.1.1.1      pbapply_1.7-2      \n#&gt;  [76] reformulas_0.4.0    stats4_4.4.2        xfun_0.51          \n#&gt;  [79] qgraph_1.9.8        arm_1.14-4          stringi_1.8.4      \n#&gt;  [82] pacman_0.5.1        boot_1.3-31         evaluate_1.0.3     \n#&gt;  [85] codetools_0.2-20    mi_1.1              cli_3.6.4          \n#&gt;  [88] RcppParallel_5.1.10 xtable_1.8-4        Rdpack_2.6.2       \n#&gt;  [91] munsell_0.5.1       Rcpp_1.0.14         coda_0.19-4.1      \n#&gt;  [94] png_0.1-8           XML_3.99-0.18       jpeg_0.1-10        \n#&gt;  [97] lme4_1.1-36         mvtnorm_1.3-3       openxlsx_4.2.8     \n#&gt; [100] rlang_1.1.5         multcomp_1.4-28     mnormt_2.1.1",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html",
    "href": "chapters/ctt/01_ctt_1.html",
    "title": "8  Fondamenti teorici",
    "section": "",
    "text": "8.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nLa CTT, originariamente sviluppata da Spearman (1904) e successivamente formalizzata da Lord & Novick (1968), è una teoria utilizzata in psicometria per valutare e misurare le caratteristiche psicologiche degli individui attraverso l’uso di test e questionari. Alla base della CTT c’è l’idea che il punteggio di un individuo in un test rifletta tanto il suo vero livello nella caratteristica misurata quanto un certo grado di errore casuale di misurazione. Questa teoria fornisce un quadro concettuale per comprendere come il punteggio osservato possa differire dal punteggio vero e per analizzare la precisione e l’affidabilità del test stesso. Sebbene siano stati sviluppati approcci più recenti, come la teoria della risposta all’item e la teoria della generalizzabilità, la CTT rimane una componente fondamentale nella psicometria e continua a guidare la costruzione, l’interpretazione e la valutazione dei test psicologici.\nSecondo questa teoria, il punteggio ottenuto da un individuo in un test è influenzato da due componenti: il punteggio vero dell’individuo sulla caratteristica misurata e l’errore casuale di misurazione.\nIl punteggio vero rappresenta la misura effettiva della caratteristica che si intende valutare nel soggetto. Tuttavia, a causa di vari fattori come l’errore di misurazione, le distrazioni o l’incertezza dell’individuo durante il test, il punteggio osservato può deviare dal punteggio vero. Questa discrepanza tra il punteggio vero e il punteggio osservato viene definita errore di misurazione.\nLa teoria classica dei test si focalizza sulla quantificazione della relazione tra il punteggio vero, il punteggio osservato e l’errore di misurazione. Attraverso l’uso di statistiche come la media, la deviazione standard e il coefficiente di affidabilità, questa teoria fornisce una base concettuale per la costruzione dei test, l’interpretazione dei risultati e l’analisi dell’affidabilità del test stesso.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html#costrutti",
    "href": "chapters/ctt/01_ctt_1.html#costrutti",
    "title": "8  Fondamenti teorici",
    "section": "\n8.2 Costrutti",
    "text": "8.2 Costrutti\nUn costrutto è un concetto, spesso considerato una idea latente o fenomeno non direttamente osservabile (Petersen, 2024). Ad esempio, la depressione può essere un costrutto perché non possiamo misurare direttamente il livello di depressione di una persona, ma lo inferiamo attraverso indicatori indiretti come umore basso, perdita di interesse, difficoltà nel sonno, ecc. Gli indicatori sono misure che riflettono il costrutto.\nEsistono due tipi principali di costrutti: costrutti riflessivi e costrutti formativi.\n\nIn un costrutto riflessivo, il costrutto causa le misure e gli indicatori riflettono il costrutto (Bollen & Lennox, 1991). Ad esempio, l’estroversione è un costrutto riflessivo perché la risposta agli indicatori come “piacere di parlare con sconosciuti” o “andare a feste” riflette il livello di estroversione della persona. Gli indicatori sono correlati perché riflettono tutti un unico costrutto latente. La consistenza interna tra gli indicatori è quindi attesa.\nIn un costrutto formativo, invece, le misure causano il costrutto (Bollen & Lennox, 1991). Ad esempio, lo stato socioeconomico (SES) può essere formato dall’educazione, dal reddito e dal prestigio occupazionale di una persona. Gli indicatori formano il costrutto e potrebbero non essere correlati tra loro, in contrasto con i costrutti riflessivi.\n\nDifferenze tra costrutti riflessivi e formativi:\n\nCorrelazioni tra indicatori: Gli indicatori riflessivi sono correlati, mentre quelli formativi non lo devono necessariamente essere.\nCampionamento degli indicatori: Nei costrutti formativi è essenziale campionare tutti gli aspetti del costrutto, mentre nei riflessivi gli indicatori possono essere intercambiabili.\nCorrelazioni ottimali: Nei costrutti riflessivi, alte correlazioni tra indicatori sono desiderabili, mentre nei formativi correlazioni troppo alte possono creare multicollinearità.\n\nI costrutti riflessivi devono essere stimati con modelli latenti come SEM, analisi fattoriale o teoria della risposta dell’item (IRT). I costrutti formativi possono essere stimati con medie pesate o tramite SEM.\nIn sintesi, prima di stimare un costrutto, è importante comprendere la sua natura teorica. I costrutti riflessivi richiedono modelli che riflettano la varianza comune tra gli indicatori, mentre i costrutti formativi possono essere stimati con medie o punteggi sommativi. La teoria è essenziale per guidare la scelta del metodo di stima.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html#lequazione-fondamentale-della-ctt",
    "href": "chapters/ctt/01_ctt_1.html#lequazione-fondamentale-della-ctt",
    "title": "8  Fondamenti teorici",
    "section": "\n8.3 L’Equazione Fondamentale della CTT",
    "text": "8.3 L’Equazione Fondamentale della CTT\nLa CTT descrive come i punteggi ottenuti da un test psicometrico siano legati a un costrutto latente. Uno dei concetti fondamentali all’interno della CTT riguarda l’affidabilità dei punteggi ottenuti dai test. L’affidabilità, in questo contesto, indica la capacità del test di produrre risultati coerenti e stabili in diverse occasioni. Questo concetto può essere compreso attraverso l’equazione fondamentale della CTT:\n\\[\nX = T + E,\n\\tag{8.1}\\]\ndove \\(X\\) rappresenta il punteggio osservato nel test, \\(T\\) è il punteggio vero (ovvero la rappresentazione della variabile latente di interesse), e \\(E\\) rappresenta l’errore di misurazione.\nUn aspetto di particolare rilevanza all’interno della CTT riguarda la varianza dell’errore. Maggiore è questa varianza, minore sarà la precisione con cui il punteggio vero si riflette nei punteggi osservati. In un contesto ideale, gli errori di misurazione sarebbero tutti nulli, garantendo punteggi esatti per ogni partecipante. Tuttavia, a causa delle inevitabili imperfezioni, si verifica una certa variazione negli errori. La deviazione standard associata a questi errori è chiamata errore standard di misurazione e viene indicata con \\(\\sigma_E\\). Uno degli obiettivi principali della CTT è stimare \\(\\sigma_E\\) al fine di valutare la qualità di una scala psicometrica.\n\n8.3.1 Le due componenti del punteggio osservato\nL’Equazione 8.1 rappresenta il cuore del modello, sottolineando che il punteggio osservato è il risultato dell’addizione del punteggio vero e dell’errore di misurazione.\nL’obiettivo principale della CTT è quantificare l’errore di misurazione (rappresentato da \\(\\sigma_E\\)) per valutare l’affidabilità del test e ottenere una stima dell’errore standard di misurazione. L’affidabilità del test riflette la precisione con cui il test può misurare il punteggio vero (Coaley, 2014). Si calcola come il rapporto tra la varianza dei punteggi veri e la varianza dei punteggi osservati. Un’alta affidabilità indica una ridotta incertezza dovuta all’errore di misurazione (\\(\\sigma_E\\)), indicando che il punteggio osservato (\\(X\\)) fornisce una misura accurata del punteggio vero (\\(T\\)). Al contrario, una bassa affidabilità indica un elevato errore di misurazione (\\(\\sigma_E\\)) e una significativa discrepanza tra il punteggio osservato e il punteggio vero.\nLa stima dell’errore standard di misurazione comporta il calcolo della deviazione standard della variabile casuale \\(E\\) (ossia \\(\\sigma_E\\)), che rappresenta l’errore di misurazione influente sui punteggi veri. Questa stima offre un’indicazione della dispersione dei punteggi osservati attorno ai punteggi veri, causata dall’errore di misurazione.\nNelle prossime sezioni, esploreremo come il concetto chiave di attendibilità nella CTT possa essere collegato al coefficiente di determinazione nel contesto del modello statistico di regressione lineare. Inoltre, vedremo come l’errore standard di misurazione della CTT possa essere associato all’errore standard nella regressione.\n\n8.3.2 Il punteggio vero\nL’Equazione 8.1 ci spiega che il punteggio osservato è il risultato della combinazione di due componenti: una componente sistematica (il punteggio vero) e una componente aleatoria (l’errore di misurazione). Ma cosa rappresenta esattamente il punteggio vero? La Teoria Classica dei Test (CTT) attribuisce diverse interpretazioni al concetto di punteggio vero.\n\nDa un punto di vista psicologico, la CTT considera il test come una selezione casuale di domande da un insieme più ampio di domande che riflettono il costrutto da misurare (Kline, 2013; Nunnally, 1994). In questo contesto, il punteggio vero rappresenta il punteggio che un partecipante otterrebbe se rispondesse a tutte le domande dell’insieme completo. L’errore di misurazione riflette quindi quanto le domande selezionate rappresentano l’intero insieme di domande relative al costrutto.\nIn modo equivalente, il punteggio vero può essere considerato come il punteggio non influenzato da fattori esterni al costrutto, come effetti di apprendimento, fatica, memoria, motivazione, e così via. Poiché è concepito come un processo completamente casuale, la componente aleatoria non introduce alcun bias nella tendenza centrale della misurazione (la media di \\(E\\) è assunta essere uguale a 0).\nDal punto di vista statistico, il punteggio vero è un punteggio inosservabile che rappresenta il valore atteso di infinite misurazioni del punteggio ottenute:\n\n\\[\nT = \\mathbb{E}(X) \\equiv \\mu_X \\equiv \\mu_{T}.\n\\]\nCombinando le definizioni presentate sopra, Lord & Novick (1968) concepiscono il punteggio vero come la media dei punteggi che un soggetto otterrebbe se il test venisse somministrato ripetutamente nelle stesse condizioni, senza effetti di apprendimento o fatica.\n\n8.3.3 Somministrazioni ripetute\nNella CTT, possiamo distinguere due tipi di esperimenti casuali: uno in cui l’unità di osservazione (l’individuo) è considerata una variabile campionaria, e un altro in cui il punteggio di un singolo individuo è trattato come una variabile casuale. La combinazione di questi due esperimenti consente di estendere i risultati della CTT, originariamente sviluppata assumendo somministrazioni ripetute immaginarie del test allo stesso individuo in condizioni identiche, al caso di una singola somministrazione su un campione di individui (Allen & Yen, 2001).\nQuesta estensione si fonda sull’assunzione ergodica, secondo cui è possibile interpretare la variabilità nelle misurazioni ripetute su un singolo individuo come rappresentativa della variabilità in un campione di individui, a condizione che siano soddisfatte le seguenti condizioni:\n\nOmogeneità: le proprietà fondamentali del costrutto misurato sono uguali per tutti gli individui nel campione. In altre parole, ogni individuo risponde in modo simile rispetto alla dimensione latente misurata dal test, e le differenze individuali sono esclusivamente dovute alla variabilità casuale o all’errore di misura, non a variazioni sostanziali nel costrutto.\nStabilità: le caratteristiche individuali misurate restano costanti nel tempo durante le somministrazioni ripetute. Ciò significa che, per uno stesso individuo, il costrutto misurato non cambia tra una somministrazione e l’altra, e le variazioni osservate riflettono soltanto la componente di errore o la variabilità casuale.\n\nQuando queste condizioni sono soddisfatte, le quantità fondamentali della CTT assumono un significato empirico valido anche per la somministrazione del test a una popolazione di individui. In questo contesto:\n\n\\(\\sigma^2_X\\) rappresenta la varianza del punteggio osservato nella popolazione,\n\\(\\sigma^2_T\\) rappresenta la varianza del punteggio vero nella popolazione,\n\\(\\sigma^2_E\\) rappresenta la varianza della componente d’errore nella popolazione.\n\nL’assunzione ergodica permette quindi di inferire proprietà della popolazione a partire da misurazioni su individui singoli e viceversa, in quanto la variabilità all’interno di un individuo e quella tra individui sono considerate comparabili nelle stesse condizioni.\n\n8.3.4 Le assunzioni sul punteggio ottenuto\nLa CTT assume che la media del punteggio osservato \\(X\\) sia uguale alla media del punteggio vero,\n\\[\n\\mu_X \\equiv \\mu_{T},\n\\tag{8.2}\\]\nin altri termini, assume che il punteggio osservato fornisca una stima statisticamente corretta dell’abilità latente (punteggio vero).\nIn pratica, il punteggio osservato non sarà mai uguale all’abilità latente, ma corrisponde solo ad uno dei possibili punteggi che il soggetto può ottenere, subordinatamente alla sua abilità latente. L’errore della misura è la differenza tra il punteggio osservato e il punteggio vero:\n\\[\nE \\equiv X - T.\n\\]\nIn base all’assunzione secondo cui il valore atteso dei punteggi è uguale alla media del valore vero, segue che\n\\[\n\\mathbb{E}(E) = \\mathbb{E}(X - T) = \\mathbb{E}(X) - \\mathbb{E}(T) = \\mu_{T} - \\mu_{T} = 0,\n\\]\novvero, il valore atteso degli errori è uguale a zero.\nPer dare un contenuto concreto alle affermazioni precedenti, consideriamo la seguente simulazione svolta in \\(\\textsf{R}\\). In tale simulazione il punteggio vero \\(T\\) e l’errore \\(E\\) sono creati in modo tale da soddisfare i vincoli della CTT: \\(T\\) e \\(E\\) sono variabili casuali gaussiane tra loro incorrelate. Nella simulazione generiamo 100 coppie di valori \\(X\\) e \\(T\\) con i seguenti parametri: \\(T \\sim \\mathcal{N}(\\mu_T = 12, \\sigma^2_T = 6)\\), \\(E \\sim \\mathcal{N}(\\mu_E = 0, \\sigma^2_T = 3)\\):\n\nset.seed(8394)\n\nn &lt;- 100\nSigma &lt;- matrix(c(6, 0, 0, 3), byrow = TRUE, ncol = 2)\nmu &lt;- c(12, 0)\ndat &lt;- mvrnorm(n, mu, Sigma, empirical = TRUE)\nT &lt;- dat[, 1]\nE &lt;- dat[, 2]\n\nLe istruzioni precedenti (empirical = TRUE) creano un campione di valori nei quali le medie e la matrice di covarianze assumono esattamente i valori richiesti. Possiamo dunque immaginare tale insieme di dati come la “popolazione”.\nSecondo la CTT, il punteggio osservato è \\(X = T + E\\). Simuliamo dunque il punteggio osservato \\(X\\) come:\n\nX &lt;- T + E\n\nLe prime 6 osservazioni così ottenute sono:\n\ntibble(X, T, E) |&gt; head()\n#&gt; # A tibble: 6 × 3\n#&gt;       X     T      E\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 15.7  16.8  -1.07 \n#&gt; 2 13.7  12.2   1.41 \n#&gt; 3  6.73  7.85 -1.12 \n#&gt; 4 14.6  14.2   0.388\n#&gt; 5 10.6  10.2   0.420\n#&gt; 6 12.4  13.3  -0.960\n\n\nUn diagramma di dispersione è fornito nella figura seguente:\n\ntibble(X, T) |&gt;\nggplot(aes(T, X)) +\n    geom_point(position = position_jitter(w = .3, h = .3)) +\n    geom_abline(col = \"blue\")\n\n\n\n\n\n\n\nSecondo la CTT, il valore atteso di \\(T\\) è uguale al valore atteso di \\(X\\). Verifichiamo questa assunzione nei nostri dati\n\nmean(T) == mean(X)\n#&gt; [1] TRUE\n\n\nL’errore deve avere media zero:\n\nmean(E)\n#&gt; [1] -8.88e-17\n\n\nLe varianze dei punteggi veri, dei punteggi osservati e degli errori sono rispettivamente uguali a:\n\nc(var(T), var(X), var(E))  \n#&gt; [1] 6 9 3",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html#lerrore-standard-della-misurazione-sigma_e",
    "href": "chapters/ctt/01_ctt_1.html#lerrore-standard-della-misurazione-sigma_e",
    "title": "8  Fondamenti teorici",
    "section": "\n8.4 L’errore standard della misurazione \\(\\sigma_E\\)\n",
    "text": "8.4 L’errore standard della misurazione \\(\\sigma_E\\)\n\nLa radice quadrata della varianza degli errori di misurazione, ovvero la deviazione standard degli errori, \\(\\sigma_E\\), è la quantità fondamentale della CTT ed è chiamata errore standard della misurazione. La stima dell’errore standard della misurazione costituisce uno degli obiettivi più importanti della CTT.\nNel caso presente, abbiamo:\n\nsqrt(var(E))\n#&gt; [1] 1.73\n\n\nRicordiamo che la deviazione standard indica quanto i dati di una distribuzione si discostano dalla media di quella distribuzione. È simile allo scarto tipico, ovvero la distanza media tra i valori della distribuzione e la loro media. Possiamo dunque utilizzare questa proprietà per descrivere il modo in cui la CTT interpreta la quantità \\(\\sigma_E\\): l’errore standard della misurazione \\(\\sigma_E\\) ci dice qual è, approssimativamente, la quantità attesa di variazione del punteggio osservato, se il test venisse somministrato ripetute volte al medesimo rispondente sotto le stesse condizioni (ovvero, in assenza di effetti di apprendimento o di fatica).",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html#assiomi-della-teoria-classica",
    "href": "chapters/ctt/01_ctt_1.html#assiomi-della-teoria-classica",
    "title": "8  Fondamenti teorici",
    "section": "\n8.5 Assiomi della Teoria Classica",
    "text": "8.5 Assiomi della Teoria Classica\nLa CTT assume che gli errori siano delle variabili casuali incorrelate tra loro\n\\[\n\\rho(E_i, E_k \\mid T) = 0, \\qquad\\text{con}\\; i \\neq k,\n\\]\ne incorrelate con il punteggio vero,\n\\[\n\\rho(E, T) = 0,\n\\]\nle quali seguono una distribuzione gaussiana con media zero e deviazione standard pari a \\(\\sigma_E\\):\n\\[\nE \\sim \\mathcal{N}(0, \\sigma_E).\n\\]\nLa quantità \\(\\sigma_E\\) è appunto l’errore standard della misurazione. Sulla base di tali assunzioni la CTT deriva la formula dell’attendibilità di un test. Si noti che le assunzioni della CTT hanno una corrispondenza puntuale con le assunzioni su cui si basa il modello di regressione lineare.\nVerifichiamo le assunzioni per i dati dell’esempio.\n\ncor(E, T)\n#&gt; [1] -4.23e-17\n\n\nplot(density(E))\ncurve(dnorm(x, mean(E), sd(E)), add = TRUE, col = \"red\")",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html#quattro-livelli-di-misurazione-nella-ctt",
    "href": "chapters/ctt/01_ctt_1.html#quattro-livelli-di-misurazione-nella-ctt",
    "title": "8  Fondamenti teorici",
    "section": "\n8.6 Quattro Livelli di Misurazione nella CTT",
    "text": "8.6 Quattro Livelli di Misurazione nella CTT\nNell’ambito della CTT, le misure della stessa entità (che possono essere item, sottoscale o test) possono essere classificate in base al loro livello di similarità. In questa sezione, verranno definiti quattro livelli di similarità: misure parallele, \\(\\tau\\)-equivalenti, essenzialmente \\(\\tau\\)-equivalenti e congeneriche.\nÈ importante notare che questi livelli sono gerarchici nel senso che il livello più alto (misure parallele) richiede la maggiore similarità, mentre i livelli inferiori nella gerarchia consentono una minore similarità nelle proprietà del test. Ad esempio, le misure parallele devono avere varianze di vero punteggio uguali, mentre le misure congenetiche non richiedono questa condizione.\nUn modo utile per comprendere questi livelli è riflettere sulle relazioni tra i punteggi veri di coppie di misure (Komaroff, 1997). Nella CTT, la relazione tra i punteggi veri su due misure (\\(t_i\\) e \\(t_j\\)) è espressa come:\n\\[\nt_i = a_{ij }+ b_{ij} t_{j}.\n\\]\n\n\\(a_{ij}\\): Rappresenta lo scarto medio tra i punteggi delle due misure. Se è diverso da zero, una misura tende a dare punteggi sistematicamente più alti o più bassi dell’altra.\n\\(b_{ij}\\): Rappresenta la scala con cui una misura misura il tratto latente rispetto all’altra. Se è diverso da uno, le misure non misurano lo stesso tratto con la stessa intensità.\n\nCosa significano questi parametri per i livelli di similarità?\n\nMisure parallele: Entrambe le misure sono identiche, sia nella scala che nello scarto medio.\nMisure τ-equivalenti: Le misure hanno la stessa scala, ma potrebbero avere uno scarto medio diverso.\nMisure essenzialmente τ-equivalenti: Le misure possono differire sia nella scala che nello scarto medio, ma entro certi limiti.\nMisure congeneriche: Le misure possono differire in modo sostanziale sia nella scala che nello scarto medio.\n\nIn sostanza, più i parametri \\(a_{ij}\\) e \\(b_{ij}\\) sono vicini a zero e uno, rispettivamente, più le misure sono simili e misurano lo stesso costrutto in modo più coerente.\n\n8.6.1 Misure parallele\nLe misure parallele rappresentano il livello più alto di similarità tra le misure. Ciò significa che due misure sono considerate parallele quando soddisfano le seguenti condizioni:\n\nUguaglianza delle medie dei punteggi veri: Il termine \\(a_{ij}\\) nell’equazione è uguale a zero per tutte le coppie di misure, indicando che non esiste alcuna differenza sistematica tra le medie dei punteggi veri delle due misure.\nUguaglianza delle varianze dei punteggi veri: Il termine \\(b_{ij}\\) è uguale a uno per tutte le coppie di misure, indicando che le varianze dei punteggi veri delle due misure sono identiche.\nUguaglianza delle varianze di errore: Le misure parallele presentano la stessa quantità di errore di misura.\n\nQueste condizioni implicano che:\n\nI punteggi osservati (ovvero, i punteggi effettivamente ottenuti dai soggetti) avranno medie, varianze e correlazioni uguali.\nGli item che costituiscono misure parallele hanno lo stesso potere discriminativo (carico fattoriale) rispetto al costrutto che misurano.\n\nIn sostanza, le misure parallele misurano esattamente lo stesso costrutto, con la stessa precisione e senza alcuna distorsione sistematica.\nSimuliamo i punteggi di due test paralleli in R.\n\nset.seed(2237) # setting the seed ensure reproducibility\nnum_person &lt;- 1000 # number of respondents\n# True scores for Test 1\nt1 &lt;- rnorm(num_person, mean = 20, sd = 5)\n# Error scores for Test 1\ne1 &lt;- rnorm(num_person, mean = 0, sd = 2)\n# Observed scores for Test 1\nx1 &lt;- t1 + e1\n# True scores for Test 2\nt2 &lt;- t1 # parallel tests have equal true scores\n# Error scores for Test 2\ne2 &lt;- rnorm(num_person, mean = 0, sd = 2)\n# Observed scores for Test 2\nx2 &lt;- t2 + e2\n\n\n# Merge into a data frame\ntest_df &lt;- data.frame(x1, x2)\n# Get means and variances\nmv &lt;- datasummary(x1 + x2 ~ Mean + Var,\n    data = test_df,\n    output = \"data.frame\"\n)\nmv\n#&gt;       Mean   Var\n#&gt; 1 x1 20.41 29.20\n#&gt; 2 x2 20.31 30.27\n\n\ncor(test_df$x1, test_df$x2)\n#&gt; [1] 0.865\n\n\nNel caso di due test paralleli, le medie e le varianze dei punteggi osservati sono (teoricamente) uguali; la correlazione descrive l’affidabilità del test.\n\n8.6.2 Misure \\(\\tau\\)-equivalenti\nLe misure τ-equivalenti rappresentano un livello di similarità leggermente inferiore rispetto alle misure parallele.\nCaratteristiche delle misure τ-equivalenti:\n\nUguaglianza delle varianze dei punteggi veri: Come le misure parallele, anche le misure τ-equivalenti hanno lo stesso valore del punteggio vero per ogni individuo, indipendentemente dal test utilizzato. Ciò implica che il parametro \\(b_{ij}\\) è sempre uguale a 1.\nPossibile differenza nelle varianze di errore: A differenza delle misure parallele, le misure τ-equivalenti possono presentare diversi livelli di errore di misura. Questo significa che, pur misurando lo stesso costrutto, un test potrebbe essere più preciso di un altro.\n\nIn sintesi, le misure τ-equivalenti misurano lo stesso costrutto sullo stesso scala, ma possono differire nella precisione con cui lo misurano. In altre parole, i punteggi veri sono uguali per tutti gli item, ma gli errori di misura possono variare.\nConseguenze:\n\nCovarianze uguali: Le misure τ-equivalenti presentano le stesse covarianze tra i punteggi veri e tra i punteggi osservati.\nVariazioni nelle varianze osservate: A causa delle possibili differenze nelle varianze di errore, le varianze dei punteggi osservati possono differire tra le misure τ-equivalenti.\n\nSimuliamo due misure \\(\\tau\\)-equivalenti.\n\nset.seed(2237) # setting the seed ensure reproducibility\nnum_person &lt;- 1000 # number of respondents\n# True scores for Test 1\nt1 &lt;- rnorm(num_person, mean = 20, sd = 5)\n# Error scores for Test 1\ne1 &lt;- rnorm(num_person, mean = 0, sd = 2)\n# Observed scores for Test 1\nx1 &lt;- t1 + e1\n# True scores for Test 2\nt2 &lt;- t1 # parallel tests have equal true scores\n# Error scores for Test 2\ne2 &lt;- rnorm(num_person, mean = 0, sd = 2)\n# Observed scores for Test 2\nx2 &lt;- t2 + e2\n\nSe conosciamo i punteggi veri, le stime dell’affidabilità di x1 e x2 sono:\n\n# Reliability for x1\nvar(t1) / var(x1)\n#&gt; [1] 0.878\n\n\n# Reliability for x2\nvar(t2) / var(x2)\n#&gt; [1] 0.847\n\n\n# Merge into a data frame\ntest_df &lt;- data.frame(x1, x2)\n# Get means and variances\nmv &lt;- datasummary(x1 + x2 ~ Mean + Var,\n    data = test_df,\n    output = \"data.frame\"\n)\nmv\n#&gt;       Mean   Var\n#&gt; 1 x1 20.41 29.20\n#&gt; 2 x2 20.31 30.27\n\n\ncor(test_df$x1, test_df$x2)\n#&gt; [1] 0.865\n\n\nIn conclusione, nel caso di due test \\(\\tau\\)-equivalenti, le medie e le varianze dei punteggi osservati sono (teoricamente) uguali. Anche in questo caso, la correlazione descrive l’affidabilità del test.\n\n8.6.3 Misure essenzialmente \\(\\tau\\)-equivalenti\nLe misure essenzialmente \\(\\tau\\)-equivalenti rappresentano una forma di misurazione in cui i punteggi veri possono differire di una costante additiva. Ciò significa che, pur misurando lo stesso costrutto, le medie dei punteggi veri di diversi item possono variare leggermente.\nCaratteristiche delle misure essenzialmente \\(\\tau\\)-equivalenti:\n\nUguaglianza delle varianze dei punteggi veri: Come nelle misure \\(\\tau\\)-equivalenti, la varianza del punteggio vero è la stessa per tutti gli item.\nPossibili differenze nelle medie dei punteggi veri: Il parametro \\(a_{ij}\\) può essere diverso da zero, indicando che le medie dei punteggi veri possono variare.\nPossibili differenze nelle varianze di errore: Come nelle misure \\(\\tau\\)-equivalenti, gli item possono avere diversi livelli di precisione, ovvero diverse varianze di errore.\n\nImplicazioni:\n\nCorrelazione perfetta tra i punteggi veri: Nonostante le differenze nelle medie, i punteggi veri sono perfettamente correlati linearmente.\nCovarianze uguali tra i punteggi veri: Le covarianze tra i punteggi veri sono uguali per tutte le coppie di item.\nPossibili differenze nelle varianze e nelle covarianze dei punteggi osservati: A causa delle differenze nelle varianze di errore, le varianze e le covarianze dei punteggi osservati possono variare.\n\nIn sintesi, le misure essenzialmente \\(\\tau\\)-equivalenti sono utili quando si desidera confrontare diversi item che misurano lo stesso costrutto, ma si ammette la possibilità di piccole differenze sistematiche nelle medie dei punteggi.\n\n# True scores for Test 3\nt3 &lt;- 5 + t1 # essentially tau-equivalent tests\n# Error scores for Test 3 (larger error SDs)\ne3 &lt;- rnorm(num_person, mean = 0, sd = 4)\n# Observed scores for Test 2\nx3 &lt;- t3 + e3\n\n\n# Merge into a data frame\ntest_df2 &lt;- data.frame(x1, x3)\n# Get means and variances\nmv &lt;- datasummary(x1 + x3 ~ Mean + Var,\n    data = test_df2,\n    output = \"data.frame\"\n)\nmv\n#&gt;       Mean   Var\n#&gt; 1 x1 20.41 29.20\n#&gt; 2 x3 25.41 41.50\n\nSe conosciamo i punteggi veri, la stima dell’affidabilità di x3 è:\n\n# Reliability for x3\nvar(t3) / var(x3)\n#&gt; [1] 0.618\n\n\nIn conclusione, nel caso di test essenzialmente \\(\\tau\\)-equivalenti, le medie e le varianze dei punteggi osservati sono diverse; la correlazione non è uguale all’affidabilità.\n\n8.6.4 Misure Congeneriche\nLe misure congeneriche rappresentano il livello di similarità più basso tra le diverse tipologie di misure.\nCaratteristiche delle misure congeneriche:\n\n\nNessuna restrizione: A differenza delle misure parallele, τ-equivalenti ed essenzialmente τ-equivalenti, le misure congeneriche non sono soggette a restrizioni specifiche sui parametri \\(a_{ij}\\) e \\(b_{ij}\\). Ciò significa che:\n\nLe medie dei punteggi veri possono differire significativamente.\nLe varianze dei punteggi veri possono essere diverse.\nLe varianze di errore possono variare notevolmente.\n\n\nUnidimensionalità: Nonostante queste differenze, si assume che tutte le misure congeneriche misurino lo stesso costrutto latente sottostante.\n\nImplicazioni:\n\nFlessibilità: Le misure congeneriche offrono la massima flessibilità in termini di differenze tra gli item.\nMinor comparabilità: A causa delle numerose differenze, confrontare direttamente i punteggi ottenuti con misure congeneriche può essere più complesso.\n\nIn sintesi, le misure congeneriche rappresentano un modello molto generale, che consente di includere una vasta gamma di situazioni. Tuttavia, la loro flessibilità comporta una minore comparabilità tra gli item.\n\n# True scores for Test 4\nt4 &lt;- 2 + 0.8 * t1\n# Error scores for Test 4 (larger error SDs)\ne4 &lt;- rnorm(num_person, mean = 0, sd = 3)\n# Observed scores for Test 2\nx4 &lt;- t4 + e4\n\n\n# Merge into a data frame\ntest_df3 &lt;- data.frame(x1, x4)\n# Get means and variances\nmv &lt;- datasummary(x1 + x4 ~ Mean + Var,\n    data = test_df3,\n    output = \"data.frame\"\n)\nmv\n#&gt;       Mean   Var\n#&gt; 1 x1 20.41 29.20\n#&gt; 2 x4 18.27 24.23\n\nSe conosciamo i punteggi veri, la stima dell’affidabilità di x4 è:\n\n# Reliability for x4\nvar(t4) / var(x4)\n#&gt; [1] 0.677\n\n\nNel caso di test congenerici, le medie e le varianze dei punteggi osservati sono diverse; la correlazione non è uguale all’affidabilità. Per distinguere test congenerici dai test essenzialmente \\(\\tau\\)-equivalenti sono necessari più di due test.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html#riflessioni-conclusive",
    "href": "chapters/ctt/01_ctt_1.html#riflessioni-conclusive",
    "title": "8  Fondamenti teorici",
    "section": "\n8.7 Riflessioni Conclusive",
    "text": "8.7 Riflessioni Conclusive\nQuesto capitolo ha offerto una panoramica dei concetti chiave della teoria classica dei test (CTT) e ha introdotto quattro tipi di misure psicometriche. Le misure parallele si distinguono per l’elevata somiglianza nei punteggi veri, garantendo che le varianze siano uguali per tutte le misure. Le misure τ-equivalenti condividono questa equivalenza nelle varianze dei punteggi veri, ma non richiedono una somiglianza così stretta come le misure parallele. Le misure essenzialmente τ-equivalenti tollerano una maggiore variabilità nei punteggi veri, pur mantenendo la coerenza dei risultati. Infine, le misure congeneriche presentano le minori restrizioni tra le quattro tipologie, consentendo differenze sia nelle medie sia nelle varianze dei punteggi veri.\nComprendere le differenze tra queste tipologie di misure è fondamentale per valutare l’affidabilità e la validità di un test e per interpretare in modo accurato i risultati. Nelle prossime sezioni della dispensa, approfondiremo l’applicazione pratica della CTT nello sviluppo e nella valutazione dei test psicometrici. Per un’esplorazione più dettagliata, si rimanda alle letture di riferimento: McDonald (2013) e Lord & Novick (1968).",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html#session-info",
    "href": "chapters/ctt/01_ctt_1.html#session-info",
    "title": "8  Fondamenti teorici",
    "section": "\n8.8 Session Info",
    "text": "8.8 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] modelsummary_2.3.0 ggokabeito_0.1.0   see_0.10.0        \n#&gt;  [4] MASS_7.3-65        viridis_0.6.5      viridisLite_0.4.2 \n#&gt;  [7] ggpubr_0.6.0       ggExtra_0.10.1     gridExtra_2.3     \n#&gt; [10] patchwork_1.3.0    bayesplot_1.11.1   semTools_0.5-6    \n#&gt; [13] semPlot_1.1.6      lavaan_0.6-19      psych_2.4.12      \n#&gt; [16] scales_1.3.0       markdown_1.13      knitr_1.49        \n#&gt; [19] lubridate_1.9.4    forcats_1.0.0      stringr_1.5.1     \n#&gt; [22] dplyr_1.1.4        purrr_1.0.4        readr_2.1.5       \n#&gt; [25] tidyr_1.3.1        tibble_3.2.1       ggplot2_3.5.1     \n#&gt; [28] tidyverse_2.0.0    here_1.0.1        \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.0      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] mime_0.12           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [25] Matrix_1.7-2        R6_2.6.1            fastmap_1.2.0      \n#&gt;  [28] rbibutils_2.3       shiny_1.10.0        digest_0.6.37      \n#&gt;  [31] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n#&gt;  [34] rprojroot_2.0.4     Hmisc_5.2-2         labeling_0.4.3     \n#&gt;  [37] timechange_0.3.0    abind_1.4-8         compiler_4.4.2     \n#&gt;  [40] withr_3.0.2         glasso_1.11         htmlTable_2.4.3    \n#&gt;  [43] backports_1.5.0     carData_3.0-5       ggsignif_0.6.4     \n#&gt;  [46] corpcor_1.6.10      gtools_3.9.5        tools_4.4.2        \n#&gt;  [49] pbivnorm_0.6.0      foreign_0.8-88      zip_2.3.2          \n#&gt;  [52] httpuv_1.6.15       nnet_7.3-20         glue_1.8.0         \n#&gt;  [55] quadprog_1.5-8      nlme_3.1-167        promises_1.3.2     \n#&gt;  [58] lisrelToR_0.3       grid_4.4.2          checkmate_2.3.2    \n#&gt;  [61] cluster_2.1.8       reshape2_1.4.4      generics_0.1.3     \n#&gt;  [64] gtable_0.3.6        tzdb_0.4.0          data.table_1.17.0  \n#&gt;  [67] hms_1.1.3           car_3.1-3           tables_0.9.31      \n#&gt;  [70] sem_3.1-16          pillar_1.10.1       rockchalk_1.8.157  \n#&gt;  [73] later_1.4.1         splines_4.4.2       lattice_0.22-6     \n#&gt;  [76] survival_3.8-3      kutils_1.73         tidyselect_1.2.1   \n#&gt;  [79] miniUI_0.1.1.1      pbapply_1.7-2       reformulas_0.4.0   \n#&gt;  [82] stats4_4.4.2        xfun_0.51           qgraph_1.9.8       \n#&gt;  [85] arm_1.14-4          stringi_1.8.4       yaml_2.3.10        \n#&gt;  [88] pacman_0.5.1        boot_1.3-31         evaluate_1.0.3     \n#&gt;  [91] codetools_0.2-20    mi_1.1              cli_3.6.4          \n#&gt;  [94] RcppParallel_5.1.10 rpart_4.1.24        xtable_1.8-4       \n#&gt;  [97] Rdpack_2.6.2        munsell_0.5.1       Rcpp_1.0.14        \n#&gt; [100] coda_0.19-4.1       png_0.1-8           XML_3.99-0.18      \n#&gt; [103] parallel_4.4.2      jpeg_0.1-10         lme4_1.1-36        \n#&gt; [106] mvtnorm_1.3-3       openxlsx_4.2.8      rlang_1.1.5        \n#&gt; [109] multcomp_1.4-28     mnormt_2.1.1\n\n\n\n\n\nAllen, M. J., & Yen, W. M. (2001). Introduction to measurement theory. Waveland Press.\n\n\nBollen, K., & Lennox, R. (1991). Conventional wisdom on measurement: A structural equation perspective. Psychological bulletin, 110(2), 305–314.\n\n\nKline, P. (2013). Handbook of psychological testing. Routledge.\n\n\nKomaroff, E. (1997). Effect of simultaneous violations of essential/g= t/-equivalence and uncorrelated error on coefficient/g= a. Applied Psychological Measurement, 21(4), 337–348.\n\n\nLord, F. M., & Novick, M. R. (1968). Statistical theories of mental test scores. Addison-Wesley.\n\n\nMcDonald, R. P. (2013). Test theory: A unified treatment. Psychology Press.\n\n\nNunnally, J. C. (1994). Psychometric theory. McGraw-Hill.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.\n\n\nSpearman, C. (1904). General intelligence objectively determined and measured. American Journal of Psychology, 15, 201–293.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/02_ctt_2.html",
    "href": "chapters/ctt/02_ctt_2.html",
    "title": "9  L’affidabilità del test",
    "section": "",
    "text": "9.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nUno degli obiettivi principali della CTT è quello di suddividere la varianza di un insieme di punteggi osservati in varianza del punteggio vero e varianza dell’errore. Per definire l’attendibilità, la CTT si basa su due informazioni chiave:\nVedremo come ottenere queste informazioni utilizzando le assunzioni del modello statistico alla base della CTT. Queste assunzioni includono:",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>L'affidabilità del test</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/02_ctt_2.html#introduzione",
    "href": "chapters/ctt/02_ctt_2.html#introduzione",
    "title": "9  L’affidabilità del test",
    "section": "",
    "text": "La varianza dei punteggi osservati.\nLa correlazione tra il punteggio osservato e il punteggio vero.\n\n\n\nErrore medio nullo: Si assume che l’errore di misurazione abbia una media pari a zero, cioè \\(E(e) = 0\\). Questo implica che l’errore è casuale e distribuito uniformemente attorno al punteggio vero.\nIndipendenza tra punteggio vero e errore: La CTT assume che non ci sia correlazione tra il punteggio vero e l’errore di misurazione (\\(r_{T,e} = 0\\)).\nIndipendenza dell’errore nel tempo: Si assume che l’errore di misurazione in un determinato momento non sia correlato con l’errore in un altro momento (\\(r_{e1,e2} = 0\\)).",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>L'affidabilità del test</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/02_ctt_2.html#la-varianza-del-punteggio-osservato",
    "href": "chapters/ctt/02_ctt_2.html#la-varianza-del-punteggio-osservato",
    "title": "9  L’affidabilità del test",
    "section": "9.2 La varianza del punteggio osservato",
    "text": "9.2 La varianza del punteggio osservato\nLa varianza del punteggio osservato \\(X\\) è uguale alla somma della varianza del punteggio vero e della varianza dell’errore di misurazione:\n\\[\n\\sigma^2_X =   \\sigma_T^2 + \\sigma_E^2.\n\\tag{9.1}\\]\nLa dimostrazione è la seguente. La varianza del punteggio osservato è uguale a\n\\[\n\\sigma^2_X =  \\mathbb{V}(T+E) =  \\sigma_T^2 + \\sigma_E^2 + 2 \\sigma_{TE}.\n\\tag{9.2}\\]\nDato che \\(\\sigma_{TE}=\\rho_{TE}\\sigma_T \\sigma_E=0\\), in quanto \\(\\rho_{TE}=0\\), ne segue che\n\\[\n\\sigma^2_X =   \\sigma_T^2 + \\sigma_E^2.\n\\]\nPer fare un esempio concreto, riprendiamo la simulazione del capitolo precedente.\n\nset.seed(8394)\n\nn &lt;- 100\nSigma &lt;- matrix(c(6, 0, 0, 3), byrow = TRUE, ncol = 2)\nmu &lt;- c(12, 0)\ndat &lt;- mvrnorm(n, mu, Sigma, empirical = TRUE)\nT &lt;- dat[, 1]\nE &lt;- dat[, 2]\nX &lt;- T + E\n\ntibble(X, T, E) |&gt; head()\n\n\nA tibble: 6 × 3\n\n\nX\nT\nE\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n15.698623\n16.765359\n-1.0667358\n\n\n13.657503\n12.248096\n1.4094073\n\n\n6.731979\n7.852136\n-1.1201563\n\n\n14.621813\n14.233699\n0.3881133\n\n\n10.606647\n10.187035\n0.4196115\n\n\n12.370288\n13.329971\n-0.9596831\n\n\n\n\n\n\nvar(X) == var(T) + var(E)\n\nTRUE",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>L'affidabilità del test</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/02_ctt_2.html#la-covarianza-tra-punteggio-osservato-e-punteggio-vero",
    "href": "chapters/ctt/02_ctt_2.html#la-covarianza-tra-punteggio-osservato-e-punteggio-vero",
    "title": "9  L’affidabilità del test",
    "section": "9.3 La covarianza tra punteggio osservato e punteggio vero",
    "text": "9.3 La covarianza tra punteggio osservato e punteggio vero\nLa covarianza tra punteggio osservato \\(X\\) e punteggio vero \\(T\\) è uguale alla varianza del punteggio vero:\n\\[\n\\sigma_{X T} = \\sigma_T^2.\n\\tag{9.3}\\]\nLa dimostrazione è la seguente. La covarianza tra punteggio osservato e punteggio vero è uguale a\n\\[\n\\begin{aligned}\n\\sigma_{X T} &= \\mathbb{E}(XT) - \\mathbb{E}(X)\\mathbb{E}(T)\\notag\\\\\n&=  \\mathbb{E}[(T+E)T] - \\mathbb{E}(T+E)\\mathbb{E}(T)\\notag\\\\\n&=  \\mathbb{E}(T^2) + \\underbrace{\\mathbb{E}(ET)}_{=0} - [\\mathbb{E}(T)]^2 -  \\underbrace{\\mathbb{E}(E)}_{=0} \\mathbb{E}(T)\\notag\\\\\n&=\\mathbb{E}(T^2) - [\\mathbb{E}(T)]^2\\notag \\\\\n&= \\sigma_T^2.\n\\end{aligned}\n\\]\nVerifichiamo per i dati dell’esempio.\n\ncov(X, T) == var(T)\n\nTRUE",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>L'affidabilità del test</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/02_ctt_2.html#correlazione-tra-punteggio-osservato-e-punteggio-vero",
    "href": "chapters/ctt/02_ctt_2.html#correlazione-tra-punteggio-osservato-e-punteggio-vero",
    "title": "9  L’affidabilità del test",
    "section": "9.4 Correlazione tra punteggio osservato e punteggio vero",
    "text": "9.4 Correlazione tra punteggio osservato e punteggio vero\nLa correlazione tra punteggio osservato \\(X\\) e punteggio vero \\(T\\) è uguale al rapporto tra la covarianza tra \\(X\\) e \\(T\\) divisa per il prodotto delle due deviazioni standard:\n\\[\n\\rho_{XT} = \\frac{\\sigma_{XT}}{\\sigma_X \\sigma_T} = \\frac{\\sigma^2_{T}}{\\sigma_X \\sigma_T} = \\frac{\\sigma_{T}}{\\sigma_X}.\n\\tag{9.4}\\]\nDunque, la correlazione tra il punteggio osservato e il punteggio vero è uguale al rapporto tra la deviazione standard dei punteggi veri e la deviazione standard dei punteggi osservati.\nVerifichiamo per i dati dell’esempio.\n\ncor(X, T) \n\n0.816496580927726\n\n\n\nsd(T) / sd(X)\n\n0.816496580927726",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>L'affidabilità del test</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/02_ctt_2.html#definizione-e-significato-dellattendibilità",
    "href": "chapters/ctt/02_ctt_2.html#definizione-e-significato-dellattendibilità",
    "title": "9  L’affidabilità del test",
    "section": "9.5 Definizione e significato dell’attendibilità",
    "text": "9.5 Definizione e significato dell’attendibilità\nSulla base dell’Equazione 9.4, possiamo giungere alla definizione di attendibilità. La Teoria della Misurazione Classica (CTT) definisce l’attendibilità di un test (o di un singolo elemento) come il rapporto tra la varianza del punteggio vero e la varianza del punteggio osservato. In altre parole, l’attendibilità rappresenta il quadrato della correlazione tra il punteggio osservato \\(X\\) e il punteggio vero \\(T\\):\n\\[\n\\begin{equation}\n\\rho_{XT}^2 = \\frac{\\sigma_{T}^2}{\\sigma_{X}^2}.\n\\end{equation}\n\\]\nQuesta formula è il concetto fondamentale della CTT e misura il livello di variazione del punteggio vero rispetto alla variazione del punteggio osservato.\nAdesso possiamo procedere a verificare questa relazione utilizzando i dati forniti nell’esempio.\n\ncor(X, T)^2\n\n0.666666666666667\n\n\n\nvar(T) / var(X)\n\n0.666666666666667\n\n\nDato che \\(\\sigma^2_X = \\sigma_T^2 + \\sigma_E^2\\), in base alla {ref}eq-reliability-1 possiamo scrivere\n\\[\n\\begin{equation}\n\\rho_{XT}^2 = \\frac{\\sigma_{T}^2}{\\sigma_X^2} =\\frac{\\sigma_{X}^2 - \\sigma^2_E}{\\sigma_X^2} = 1-\\frac{\\sigma_{E}^2}{\\sigma_{X}^2}.\n\\end{equation}\n\\tag{9.5}\\]\n\n1 - (var(E) / var(X))\n\n0.666666666666667\n\n\nDall’Equazione 9.5, possiamo dedurre che il coefficiente di affidabilità assume il valore di \\(1\\) quando la varianza degli errori \\(\\sigma_{E}^2\\) è nulla, e assume il valore di \\(0\\) quando la varianza degli errori è uguale alla varianza del punteggio osservato. Quindi, il coefficiente di affidabilità è un valore assoluto situato nell’intervallo tra \\(0\\) e \\(1\\).",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>L'affidabilità del test</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/02_ctt_2.html#attendibilità-e-modello-di-regressione-lineare",
    "href": "chapters/ctt/02_ctt_2.html#attendibilità-e-modello-di-regressione-lineare",
    "title": "9  L’affidabilità del test",
    "section": "9.6 Attendibilità e modello di regressione lineare",
    "text": "9.6 Attendibilità e modello di regressione lineare\nIn parole semplici, la CTT si basa sul modello di regressione lineare, dove i punteggi osservati sono considerati come variabile dipendente e i punteggi veri come variabile indipendente. Il coefficiente di attendibilità \\(\\rho_{XT}^2\\) rappresenta la proporzione di varianza nella variabile dipendente spiegata dalla variabile indipendente in un modello di regressione lineare con una pendenza unitaria e un’intercetta di zero. In altre parole, il coefficiente di attendibilità è equivalente al coefficiente di determinazione del modello di regressione.\nPer rendere questo concetto più chiaro, possiamo tornare a considerare i dati simulati come esempio.\nLa motivazione di questa simulazione è quella di mettere in relazione il coefficiente di attendibilità, calcolato con la formula della CTT (come abbiamo fatto sopra), con il modello di regressione lineare. Analizziamo dunque i dati della simulazione mediante il seguente modello di regressione lineare:\n\\[\nX = a + b T + E.\n\\]\nUsando \\(\\textsf{R}\\) otteniamo:\n\nfm &lt;- lm(X ~ T)\nsummary(fm)\n\n\nCall:\nlm(formula = X ~ T)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.4343 -0.9720 -0.0865  1.0803  3.7347 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 9.948e-15  8.746e-01       0        1    \nT           1.000e+00  7.143e-02      14   &lt;2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.741 on 98 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6633 \nF-statistic:   196 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nSi noti che la retta di regressione ha intercetta 0 e pendenza 1. Questo è coerente con l’assunzione \\(\\mathbb{E}(X) = \\mathbb{E}(T)\\). Ma il risultato più importante di questa simulazione è che il coefficiente di determinazione (\\(R^2\\) = 0.67) del modello di regressione \\(X = 0 + 1 \\times T + E\\) è identico al coefficiente di attendibilità calcolato con la formula \\(\\rho_{XT}^2 = \\frac{\\sigma_{T}^2}{\\sigma_X^2}\\):\n\nvar(T) / var(X)\n\n0.666666666666667\n\n\nQuesti risultati ci permettono di interpretare il coefficiente di affidabilità nel seguente modo: l’affidabilità di un test rappresenta la porzione di varianza presente nel punteggio osservato \\(X\\) che viene spiegata dalla regressione di \\(X\\) rispetto al punteggio vero \\(T\\). Questo risultato è stato ottenuto mediante una regressione lineare, dove il coefficiente angolare \\(\\beta\\) è uguale a 1 e l’intercetta \\(\\alpha\\) è uguale a 0.\nInoltre, ricordiamo che la radice quadrata della varianza degli errori è l’errore standard della misurazione, \\(\\sigma_E\\). La quantità \\(\\sqrt{\\sigma_E^2}\\) fornisce una misura della dispersione del punteggio osservato attorno al valore vero, nella condizione ipotetica di ripetute somministrazioni del test:\n\nsqrt(var(E) * 99 / 98)\n\n1.74086537242199\n\n\nL’output della funzione lm() rende chiaro che l’errore standard della misurazione della CTT è identico all’errore standard della regressione nel caso di un modello di regressione definito come abbiamo fatto sopra.\nNel codice precedente è stato incluso il termine correttivo 99/98. Questa correzione è necessaria poiché, mentre R calcola la deviazione standard con \\(n-1\\) al denominatore, l’errore standard della regressione richiede \\(n-2\\) al denominatore.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>L'affidabilità del test</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/02_ctt_2.html#misurazioni-parallele-e-affidabilità",
    "href": "chapters/ctt/02_ctt_2.html#misurazioni-parallele-e-affidabilità",
    "title": "9  L’affidabilità del test",
    "section": "9.7 Misurazioni parallele e affidabilità",
    "text": "9.7 Misurazioni parallele e affidabilità\nL’equazione \\(\\rho_{XT}^2 = \\frac{\\sigma_{T}^2}{\\sigma_X^2}\\) definisce il coefficiente di affidabilità, ma non ci fornisce gli strumenti pratici per calcolarlo direttamente. Questo perché la varianza del punteggio reale \\(\\sigma_{T}^2\\) rappresenta un valore sconosciuto. Il metodo utilizzato dalla CTT per ottenere una stima empirica dell’attendibilità è quello delle forme parallele del test. In pratica, se è possibile creare versioni alternative del test che siano equivalenti in termini di contenuto, modalità di risposta e caratteristiche statistiche, allora diventa possibile ottenere una stima empirica del coefficiente di affidabilità.\nSecondo la CTT, due test \\(X=T+E\\) e \\(X^\\prime=T^\\prime+E^\\prime\\) sono considerati misurazioni parallele della stessa abilità latente quando:\n\n\\(T = T^\\prime\\),\n\\(\\mathbb{V}(E) = \\mathbb{V}(E^\\prime)\\).\n\nQueste premesse implicano che \\(\\mathbb{E}(X) = \\mathbb{E}(X^\\prime)\\).\nLa dimostrazione procede come segue. Considerando che \\(\\mathbb{E}(X) = T\\) e \\(\\mathbb{E}(X^\\prime) = T\\), è evidente che \\(\\mathbb{E}(X) =\\mathbb{E}(X^\\prime)\\) poiché \\(\\mathbb{E}(E) = \\mathbb{E}(E^\\prime) = 0\\).\nIn modo analogo, l’uguaglianza delle varianze nei punteggi osservati delle due misurazioni parallele deve essere verificata, cioè \\(\\mathbb{V}(X) = \\mathbb{V}(X^\\prime)\\).\nQuesta dimostrazione si sviluppa come segue. Per \\(X\\), possiamo scrivere\n\\[\\mathbb{V}(X) = \\mathbb{V}(T + E) = \\mathbb{V}(T) + \\mathbb{V}(E);\\]\nmentre per \\(X^\\prime\\) possiamo scrivere\n\\[\\mathbb{V}(X^\\prime) = \\mathbb{V}(T^\\prime + E^\\prime) = \\mathbb{V}(T^\\prime) + \\mathbb{V}(E^\\prime).\\]\nPoiché sappiamo che \\(\\mathbb{V}(E) = \\mathbb{V}(E^\\prime)\\) e che \\(T = T^\\prime\\), possiamo dedurre che \\(\\mathbb{V}(X) = \\mathbb{V}(X^\\prime)\\).\nIn aggiunta, è importante notare che per costruzione gli errori \\(E\\) e \\(E^\\prime\\) sono incorrelati sia con \\(T\\) che tra di loro.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>L'affidabilità del test</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/02_ctt_2.html#la-correlazione-tra-due-forme-parallele-del-test",
    "href": "chapters/ctt/02_ctt_2.html#la-correlazione-tra-due-forme-parallele-del-test",
    "title": "9  L’affidabilità del test",
    "section": "9.8 La correlazione tra due forme parallele del test",
    "text": "9.8 La correlazione tra due forme parallele del test\nOra procediamo a dimostrare che, secondo le ipotesi della Teoria della CTT, la correlazione tra due versioni parallele di un test è effettivamente equivalente al rapporto tra la varianza del punteggio reale e la varianza del punteggio osservato. Come discusso nel capitolo precedente, le misurazioni parallele rappresentano il grado più elevato di somiglianza tra due diverse versioni di un test.\nLa dimostrazione è la seguente. Consideriamo, senza perdita di generalità, che \\(\\mathbb{E}(X) = \\mathbb{E}(X') = \\mathbb{E}(T) = 0\\). Questa scelta ci consente di scrivere:\n\\[\n\\begin{aligned}\n\\rho_{X X^\\prime} &= \\frac{\\sigma(X, X^\\prime)}{\\sigma(X) \\sigma(X^\\prime)} \\\\\n&= \\frac{\\mathbb{E}(XX^\\prime)}{\\sigma(X) \\sigma(X^\\prime)} \\\\\n&= \\frac{\\mathbb{E}[(T+E)(T+E^\\prime)]}{\\sigma(X) \\sigma(X^\\prime)} \\\\\n&= \\frac{\\mathbb{E}(T^2) + \\mathbb{E}(TE^\\prime) + \\mathbb{E}(TE) + \\mathbb{E}(EE^\\prime)}{\\sigma(X) \\sigma(X^\\prime)}.\n\\end{aligned}\n\\]\nTuttavia, sappiamo che \\(\\mathbb{E}(TE) = \\mathbb{E}(TE^\\prime) = \\mathbb{E}(EE^\\prime) = 0\\). Inoltre, \\(\\sigma(X) = \\sigma(X^\\prime) = \\sigma_X\\). Pertanto, giungiamo a:\n\\[\n\\rho_{X X^\\prime} = \\frac{\\mathbb{E}(T^2)}{\\sigma_X \\sigma_X} = \\frac{\\sigma^2_T}{\\sigma^2_X}.\n\\] {#eq:3-3-5}\nNotiamo che il risultato ottenuto, insieme all’equazione che definisce il coefficiente di affidabilità \\(\\rho_{XT}^2 = \\frac{\\sigma_{T}^2}{\\sigma_X^2}\\), presentano entrambi la stessa espressione a destra del segno di uguale. Questo conduce a un risultato cruciale: il coefficiente di affidabilità, ossia il quadrato della correlazione tra il punteggio osservato e il punteggio reale, è identico alla correlazione tra i punteggi osservati di due versioni parallele del test:\n\\[\n\\rho^2_{XT} =  \\rho_{XX^\\prime}.\n\\tag{9.6}\\]\nQuesta conclusione è di notevole importanza in quanto consente di esprimere la variabile inosservabile \\(\\rho^2_{XT}\\) in termini della variabile osservabile \\(\\rho_{XX^\\prime}\\), la quale può essere calcolata in base ai punteggi osservati delle due forme parallele del test. Fondamentalmente, la stima di \\(\\rho^2_{XT}\\) si semplifica nella stima di \\(\\rho^2_{XX^\\prime}\\). Questo spiega l’importanza dell’equazione {eq}eq-rho2xt-rhoxx nella CTT. Inoltre, è da sottolineare che l’equazione {ref}eq:rho2xt-rhoxx fornisce una giustificazione per l’utilizzo della correlazione split-half come misura di affidabilità.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>L'affidabilità del test</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/02_ctt_2.html#la-correlazione-tra-punteggio-osservato-e-punteggio-vero",
    "href": "chapters/ctt/02_ctt_2.html#la-correlazione-tra-punteggio-osservato-e-punteggio-vero",
    "title": "9  L’affidabilità del test",
    "section": "9.9 La correlazione tra punteggio osservato e punteggio vero",
    "text": "9.9 La correlazione tra punteggio osservato e punteggio vero\nEsaminiamo adesso la correlazione tra il punteggio osservato e il punteggio reale. L’Equazione 9.6 può essere riformulata come segue:\n\\[\n\\rho_{XT} = \\sqrt{\\rho_{XX^\\prime}}.\n\\]\nIn altre parole, la radice quadrata del coefficiente di affidabilità equivale alla correlazione tra il punteggio osservato e il punteggio reale.\nProcediamo ora a verificare questa relazione utilizzando i dati dell’esempio.\n\nsqrt(var(T) / var(X))\n\n0.816496580927726\n\n\n\ncor(X, T)\n\n0.816496580927726",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>L'affidabilità del test</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/02_ctt_2.html#i-fattori-che-influenzano-lattendibilità",
    "href": "chapters/ctt/02_ctt_2.html#i-fattori-che-influenzano-lattendibilità",
    "title": "9  L’affidabilità del test",
    "section": "9.10 I fattori che influenzano l’attendibilità",
    "text": "9.10 I fattori che influenzano l’attendibilità\nConsiderando le tre equazioni:\n\\[\n\\rho^2_{XT} = \\rho_{XX'},\\quad\n\\rho_{XT}^2 = \\frac{\\sigma_{T}^2}{\\sigma_X^2}, \\quad\n\\rho_{XT}^2 = 1-\\frac{\\sigma_{E}^2}{\\sigma_X^2},\n\\]\npossiamo affermare che esistono tre modi equivalenti per giungere alla conclusione che l’attendibilità di un test è elevata. L’attendibilità di un test è considerata alta quando si verificano le seguenti condizioni:\n\nLa correlazione tra le forme parallele del test è elevata.\nLa varianza del punteggio vero è ampia rispetto alla varianza del punteggio osservato.\nLa varianza dell’errore di misurazione è ridotta rispetto alla varianza del punteggio osservato.\n\nQueste considerazioni rivestono un’importanza fondamentale nella progettazione di un test. In particolare, l’equazione \\(\\rho^2_{XT} = \\rho_{XX'}\\) fornisce un criterio per la selezione degli item da includere nel test. Se interpretiamo \\(\\rho_{XX'}\\) come la correlazione tra due item, allora gli item che presentano la correlazione più elevata tra di loro dovrebbero essere inclusi nel test. In questo modo, l’attendibilità del test aumenta, poiché gli item selezionati risultano fortemente correlati con il punteggio vero.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>L'affidabilità del test</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/02_ctt_2.html#riflessioni-conclusive",
    "href": "chapters/ctt/02_ctt_2.html#riflessioni-conclusive",
    "title": "9  L’affidabilità del test",
    "section": "9.11 Riflessioni conclusive",
    "text": "9.11 Riflessioni conclusive\nL’affidabilità costituisce un concetto fondamentale all’interno della teoria della misurazione, poiché si riferisce alla coerenza dei punteggi in varie situazioni, come diverse configurazioni di item, versioni del test o momenti di somministrazione. Nel corso di questo capitolo, abbiamo esplorato le basi teoriche dell’affidabilità. All’interno della CTT, l’affidabilità è definita come la correlazione tra il punteggio vero e il punteggio osservato, oppure, equivalentemente, come uno meno la correlazione tra il punteggio di errore e il punteggio osservato. Dal momento che il punteggio vero non è direttamente osservabile, è necessario ricorrere a metodi alternativi per stimare l’affidabilità. Il metodo proposto dalla CTT per ottenere tale stima è quello della correlazione dei punteggi ottenuti da due test paralleli.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>L'affidabilità del test</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/02_ctt_2.html#session-info",
    "href": "chapters/ctt/02_ctt_2.html#session-info",
    "title": "9  L’affidabilità del test",
    "section": "9.12 Session Info",
    "text": "9.12 Session Info\n\nsessionInfo()\n\nR version 4.3.2 (2023-10-31)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.3.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] MASS_7.3-60.0.1    modelsummary_1.4.5 ggokabeito_0.1.0   viridis_0.6.5     \n [5] viridisLite_0.4.2  ggpubr_0.6.0       ggExtra_0.10.1     bayesplot_1.11.1  \n [9] gridExtra_2.3      patchwork_1.2.0    semTools_0.5-6     semPlot_1.1.6     \n[13] lavaan_0.6-17      psych_2.4.1        scales_1.3.0       markdown_1.12     \n[17] knitr_1.45         lubridate_1.9.3    forcats_1.0.0      stringr_1.5.1     \n[21] dplyr_1.1.4        purrr_1.0.2        readr_2.1.5        tidyr_1.3.1       \n[25] tibble_3.2.1       ggplot2_3.4.4      tidyverse_2.0.0    here_1.0.1        \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.15.0  jsonlite_1.8.8     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5   nloptr_2.0.3      \n  [7] rmarkdown_2.25     vctrs_0.6.5        minqa_1.2.6       \n [10] base64enc_0.1-3    rstatix_0.7.2      htmltools_0.5.7   \n [13] broom_1.0.5        Formula_1.2-5      htmlwidgets_1.6.4 \n [16] plyr_1.8.9         sandwich_3.1-0     emmeans_1.10.0    \n [19] zoo_1.8-12         uuid_1.2-0         igraph_2.0.2      \n [22] mime_0.12          lifecycle_1.0.4    pkgconfig_2.0.3   \n [25] Matrix_1.6-5       R6_2.5.1           fastmap_1.1.1     \n [28] shiny_1.8.0        digest_0.6.34      OpenMx_2.21.11    \n [31] fdrtool_1.2.17     colorspace_2.1-0   rprojroot_2.0.4   \n [34] Hmisc_5.1-1        fansi_1.0.6        timechange_0.3.0  \n [37] abind_1.4-5        compiler_4.3.2     withr_3.0.0       \n [40] glasso_1.11        htmlTable_2.4.2    backports_1.4.1   \n [43] carData_3.0-5      ggsignif_0.6.4     corpcor_1.6.10    \n [46] gtools_3.9.5       tools_4.3.2        pbivnorm_0.6.0    \n [49] foreign_0.8-86     zip_2.3.1          httpuv_1.6.14     \n [52] nnet_7.3-19        glue_1.7.0         quadprog_1.5-8    \n [55] nlme_3.1-164       promises_1.2.1     lisrelToR_0.3     \n [58] grid_4.3.2         pbdZMQ_0.3-11      checkmate_2.3.1   \n [61] cluster_2.1.6      reshape2_1.4.4     generics_0.1.3    \n [64] gtable_0.3.4       tzdb_0.4.0         data.table_1.15.0 \n [67] hms_1.1.3          car_3.1-2          utf8_1.2.4        \n [70] tables_0.9.17      sem_3.1-15         pillar_1.9.0      \n [73] IRdisplay_1.1      rockchalk_1.8.157  later_1.3.2       \n [76] splines_4.3.2      lattice_0.22-5     survival_3.5-8    \n [79] kutils_1.73        tidyselect_1.2.0   miniUI_0.1.1.1    \n [82] pbapply_1.7-2      stats4_4.3.2       xfun_0.42         \n [85] qgraph_1.9.8       arm_1.13-1         stringi_1.8.3     \n [88] boot_1.3-29        evaluate_0.23      codetools_0.2-19  \n [91] mi_1.1             cli_3.6.2          RcppParallel_5.1.7\n [94] IRkernel_1.3.2     rpart_4.1.23       xtable_1.8-4      \n [97] repr_1.1.6         munsell_0.5.0      Rcpp_1.0.12       \n[100] coda_0.19-4.1      png_0.1-8          XML_3.99-0.16.1   \n[103] parallel_4.3.2     ellipsis_0.3.2     jpeg_0.1-10       \n[106] lme4_1.1-35.1      mvtnorm_1.2-4      insight_0.19.8    \n[109] openxlsx_4.2.5.2   crayon_1.5.2       rlang_1.1.3       \n[112] multcomp_1.4-25    mnormt_2.1.1",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>L'affidabilità del test</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/03_ctt_3.html",
    "href": "chapters/ctt/03_ctt_3.html",
    "title": "10  Metodi di stima dell’affidabilità",
    "section": "",
    "text": "10.1 Approcci per Stimare l’Affidabilità\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nPer stimare l’affidabilità (\\(\\rho_{TT'}\\)), ci troviamo di fronte alla sfida di dover stimare una delle due componenti non direttamente osservabili: il punteggio vero o la varianza dell’errore. Ma come possiamo affrontare questa sfida? La risposta è complessa e dipende da come intendiamo concettualizzare la varianza dell’errore (\\(\\sigma^2_E\\)).\nIn sostanza, le equazioni dell’affidabilità presentate in precedenza possono essere applicate a ciascuno dei tre tipi di affidabilità descritti sopra. La differenza fondamentale risiede nella nostra concezione e nel calcolo di \\(\\sigma^2_E\\), che varia a seconda del contesto e degli obiettivi specifici dell’analisi.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Metodi di stima dell'affidabilità</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/03_ctt_3.html#approcci-per-stimare-laffidabilità",
    "href": "chapters/ctt/03_ctt_3.html#approcci-per-stimare-laffidabilità",
    "title": "10  Metodi di stima dell’affidabilità",
    "section": "",
    "text": "Affidabilità delle Forme Parallele: Se il nostro interesse principale è misurare quanto accuratamente possiamo stimare il punteggio vero dai dati osservati, potrebbe essere più appropriato considerare \\(\\sigma^2_E\\) come l’incertezza nella nostra stima attraverso ripetute somministrazioni di una misura equivalente. Questo approccio ci porta alla definizione di affidabilità delle forme parallele.\nConsistenza Interna: Se invece vogliamo valutare se più elementi su una scala riflettono lo stesso costrutto sottostante, possiamo utilizzare un concetto simile all’Alpha di Cronbach (\\(\\alpha\\)). Questo ci porta alla definizione di affidabilità come consistenza interna.\nCoerenza Temporale (Affidabilità Test-Retest): Se ci interessa la coerenza di una misura nel tempo, allora \\(\\sigma^2_E\\) potrebbe essere meglio interpretato come la varianza non comune attraverso diverse somministrazioni della stessa misura su un periodo di tempo arbitrario. Questo concetto ci conduce alla definizione di coerenza temporale o affidabilità test-retest.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Metodi di stima dell'affidabilità</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/03_ctt_3.html#affidabilità-come-consistenza-interna",
    "href": "chapters/ctt/03_ctt_3.html#affidabilità-come-consistenza-interna",
    "title": "10  Metodi di stima dell’affidabilità",
    "section": "10.2 Affidabilità come Consistenza Interna",
    "text": "10.2 Affidabilità come Consistenza Interna\nIniziamo esaminando tre scenari distinti che illustrano le possibili relazioni tra gli item di un test: quelli con indicatori congenerici, tau-equivalenti e paralleli. Nell’ambito della CTT, sono disponibili due indicatori principali per valutare l’affidabilità in termini di coerenza interna, a seconda del tipo di relazione tra gli item presunta: l’indice alpha di Cronbach per gli item tau-equivalenti e l’indice di Spearman-Brown per gli item paralleli.\nOltre alla consistenza interna, esistono altre misure di affidabilità, tra cui la affidabilità test-retest, la affidabilità tra forme alternative, la affidabilità tra valutatori, la affidabilità dei punteggi compositi e la affidabilità dei punteggi delle differenze.\nAl centro della misurazione dell’affidabilità c’è l’errore di misurazione, e in precedenza abbiamo esaminato come lo standard error of measurement sia uno dei metodi per valutare l’errore di misurazione.\nVa notato che ci riferiamo all’affidabilità come una stima, poiché l’affidabilità assoluta o precisa dei risultati della valutazione non può essere conosciuta con certezza. Proprio come ci sono sempre degli errori nei punteggi dei test, ci sono anche degli errori nei nostri tentativi di misurare l’affidabilità. Tuttavia, i metodi di stima dell’affidabilità che discuteremo sono considerati stime conservative e rappresentano il limite inferiore della vera affidabilità dei punteggi dei test. In altre parole, l’affidabilità effettiva dei punteggi dei test è almeno altrettanto alta, se non superiore, rispetto all’affidabilità stimata (Reynolds, 1999).\n\n10.2.0.1 Coefficienti di consistenza interna\nLa CTT presenta il metodo delle forme parallele come un approccio parziale per stimare l’attendibilità dei test. Questo metodo prevede la somministrazione di due test distinti, indicati come \\(X\\) e \\(X^\\prime\\), che valutano lo stesso costrutto, a un campione di individui nello stesso momento. In questo contesto, la correlazione tra i punteggi totali dei due test, \\(\\rho^2_{XT} = \\rho_{XX^\\prime}\\), rappresenta l’indicatore principale dell’attendibilità. Tuttavia, è cruciale che le due versioni del test siano effettivamente parallele, secondo la definizione fornita dalla teoria classica dei test, affinché questa relazione sia valida.\nNella pratica, risulta impraticabile somministrare lo stesso test due volte agli stessi partecipanti “nelle stesse condizioni”, come richiesto dal metodo delle forme parallele. Di conseguenza, la stima dell’attendibilità deve basarsi sui dati raccolti attraverso una singola somministrazione del test. La CTT risponde a questa sfida introducendo specifici indicatori di coerenza interna, mirati a valutare l’affidabilità.\nQuesti indicatori di coerenza interna costituiscono la soluzione proposta dalla CTT per affrontare tale problematica. La loro logica si basa sull’idea che una correlazione tra i punteggi di diversi item che misurano lo stesso costrutto rifletta la varianza condivisa del punteggio reale, anziché la varianza condivisa dell’errore. Considerando che gli errori casuali dovrebbero mancare di una varianza condivisa, i coefficienti di coerenza interna riflettono la correlazione tra gli item all’interno del test, offrendo così un’indicazione dell’affidabilità generale della scala di misurazione.\nOltre a questo, gli item stessi possono rappresentare una fonte di errore nei punteggi dei test. Problemi come formulazioni confuse, item non coerenti con il costrutto, linguaggio poco comprensibile o item con risposte ambigue possono emergere quando gli item non sono formulati in modo adeguato. Tali problemi possono portare a risposte inconsistenti per due ragioni: innanzitutto, i partecipanti potrebbero reagire in modi diversi agli item problematici; in secondo luogo, tali item interferiscono con la capacità dei partecipanti di esprimere il loro reale livello del costrutto.\nPer valutare la coerenza delle risposte tra gli item all’interno di una scala, vengono impiegati i coefficienti di consistenza interna. Questi coefficienti si basano sull’assunto che una correlazione tra due punteggi osservati, che misurano lo stesso costrutto, rifletta la varianza condivisa del punteggio reale, non la varianza condivisa dell’errore. Dal momento che gli errori casuali dovrebbero mancare di varianza condivisa, i coefficienti di consistenza interna riflettono la correlazione tra gli item del test e forniscono un’indicazione dell’affidabilità complessiva della scala.\nQuando si valuta l’attendibilità con una singola somministrazione del test, sono disponibili vari approcci. In questo capitolo, esamineremo due metodi proposti dalla CTT: l’indice \\(\\alpha\\) di Cronbach e il metodo di Spearman-Brown. L’indice \\(\\alpha\\) è l’indicatore più comunemente usato per valutare l’attendibilità in termini di coerenza interna o omogeneità. Analizzeremo come questo indice rappresenta il valore minimo possibile dell’attendibilità di un test, sotto determinate ipotesi soddisfatte, e come, allo stesso tempo, può fornire una valutazione distorta dell’attendibilità se le assunzioni che delineeremo non sono rispettate.\nTuttavia, prima di esplorare dettagliatamente questi due diversi metodi di stima dell’attendibilità come coerenza interna, è essenziale distinguere tra tre diverse tipologie di relazioni tra gli item: item congenerici, item \\(\\tau\\)-equivalenti e item paralleli.\n\n\n10.2.0.2 Test paralleli\nSimuliamo i punteggi di due test paralleli.\n\nset.seed(2237) # setting the seed ensure reproducibility\nnum_person &lt;- 1000 # number of respondents\n# True scores for Test 1\nt1 &lt;- rnorm(num_person, mean = 20, sd = 5)\n# Error scores for Test 1\ne1 &lt;- rnorm(num_person, mean = 0, sd = 2)\n# Observed scores for Test 1\nx1 &lt;- t1 + e1\n# True scores for Test 2\nt2 &lt;- t1 # parallel tests have equal true scores\n# Error scores for Test 2\ne2 &lt;- rnorm(num_person, mean = 0, sd = 2)\n# Observed scores for Test 2\nx2 &lt;- t2 + e2\n\n# Merge into a data frame\ntest_df &lt;- data.frame(x1, x2)\n\nmv &lt;- datasummary(x1 + x2 ~ Mean + Var,\n    data = test_df,\n    output = \"data.frame\"\n)\nmv\n\n\nA data.frame: 2 x 3\n\n\n\nMean\nVar\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\nx1\n20.41\n29.20\n\n\nx2\n20.31\n30.27\n\n\n\n\n\n\n# Correlation\ncor(test_df) |&gt;\n    round(2)\n\n\nA matrix: 2 x 2 of type dbl\n\n\n\nx1\nx2\n\n\n\n\nx1\n1.00\n0.87\n\n\nx2\n0.87\n1.00\n\n\n\n\n\n\nvar(t1) / var(x1)\n\n0.878424313030747\n\n\n\nvar(t2) / var(x2)\n\n0.847351804948915\n\n\nIn conclusione, per test paralleli: - le medie e le varianze dei punteggi osservati sono statisticamente uguali; - la correlazione è uguale all’attendibilità.\n\n\n10.2.0.3 Test \\(\\tau\\)-equivalenti\n\n# True scores for Test 3\nt3 &lt;- 5 + t1 # essentially tau-equivalent tests\n# Error scores for Test 3 (larger error SDs)\ne3 &lt;- rnorm(num_person, mean = 0, sd = 4)\n# Observed scores for Test 2\nx3 &lt;- t3 + e3\n\n# Merge into a data frame\ntest_df2 &lt;- data.frame(x1, x3)\n# Get means and variances\nmv &lt;- datasummary(x1 + x3 ~ Mean + Var,\n    data = test_df2,\n    output = \"data.frame\"\n)\nmv\n\n\nA data.frame: 2 x 3\n\n\n\nMean\nVar\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\nx1\n20.41\n29.20\n\n\nx3\n25.41\n41.50\n\n\n\n\n\n\n# Correlation\ncor(test_df2) |&gt;\n    round(2)\n\n\nA matrix: 2 x 2 of type dbl\n\n\n\nx1\nx3\n\n\n\n\nx1\n1.00\n0.72\n\n\nx3\n0.72\n1.00\n\n\n\n\n\nSe conosciamo i punteggi veri, l’attendibilità di X3 si trova come\n\n# Reliability for x3\nvar(t3) / var(x3)\n\n0.618012243898734\n\n\nIn conclusione, per test tau-equivalenti: - le medie e le varianze dei punteggi osservati sono diverse; - correlazione \\(\\neq\\) attendibilità.\n\n\n10.2.0.4 Test congenerici\n\n# True scores for Test 4\nt4 &lt;- 2 + 0.8 * t1\n# Error scores for Test 4 (larger error SDs)\ne4 &lt;- rnorm(num_person, mean = 0, sd = 3)\n# Observed scores for Test 2\nx4 &lt;- t4 + e4\n\n# Merge into a data frame\ntest_df3 &lt;- data.frame(x1, x4)\n# Get means and variances\nmv &lt;- datasummary(x1 + x4 ~ Mean + Var,\n    data = test_df3,\n    output = \"data.frame\"\n)\nmv\n\n\nA data.frame: 2 x 3\n\n\n\nMean\nVar\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\nx1\n20.41\n29.20\n\n\nx4\n18.27\n24.23\n\n\n\n\n\n\n# Correlation\ncor(test_df3) |&gt;\n    round(2)\n\n\nA matrix: 2 x 2 of type dbl\n\n\n\nx1\nx4\n\n\n\n\nx1\n1.00\n0.73\n\n\nx4\n0.73\n1.00\n\n\n\n\n\nSe conosciamo i punteggi veri, l’attendibilità di X4 si trova come\n\n# Reliability for x4\nvar(t4) / var(x4)\n\n0.677398252481377\n\n\nIn conclusione, per test congenerici: - le medie e le varianze dei punteggi osservati sono diverse; - correlazione \\(\\neq\\) attendibilità; - sono necessari più di due test per distinguere test congenerici e test \\(\\tau\\)-equivalenti.\n\n\n10.2.0.5 Coefficiente \\(\\alpha\\) di Cronbach\nIl coefficiente \\(\\alpha\\) consente la stima dell’affidabilità nel contesto di indicatori \\(\\tau\\)-equivalenti. In queste circostanze, l’attendibilità viene valutata utilizzando l’equazione:\n\\[\n\\alpha = \\frac{{k}}{{k-1}} \\left( 1 - \\frac{{\\sum_{i=1}^{k} \\sigma_{X_i}^{2}}}{{\\sigma_{X}^{2}}} \\right)\n\\]\ndove: - \\(k\\) è il numero di item nel test, - \\(\\sigma_{i}^{2}\\) rappresenta la varianza del punteggio dell’item \\(i\\), - \\(\\sigma_{X}^{2}\\) è la varianza totale dei punteggi del test.\nUna derivazione della formula del coefficiente alpha di Cronbach è fornita nel capitolo {ref}reliability-fa-notebook.\nFu Guttman nel 1945 a scoprire questo coefficiente, anche se erroneamente attribuito a Cronbach. È spesso noto come coefficiente \\(\\alpha\\) di Guttman-Cronbach o G-C \\(\\alpha\\).\nQuando il modello di \\(\\tau\\)-equivalenza è applicabile, il coefficiente \\(\\alpha\\) costituisce un limite inferiore dell’affidabilità, in altri termini, il coefficiente \\(\\alpha\\) offre una stima prudente dell’affidabilità. Questa caratteristica è considerata uno dei principali vantaggi di questo indice. Tuttavia, è fondamentale notare che questa natura conservativa del coefficiente \\(\\alpha\\) vale solo se le ipotesi del modello \\(\\tau\\)-equivalente sono rispettate.\nIl coefficiente di attendibilità \\(\\alpha\\) è ampiamente utilizzato nell’ambito della psicometria. Tuttavia, come menzionato in precedenza, quando l’assunzione di \\(\\tau\\)-equivalenza non è valida, \\(\\alpha\\) può perdere la sua proprietà conservativa e sovrastimare l’attendibilità del test (Sijtsma, 2009). In tal caso, è necessario valutare attentamente l’adeguatezza dell’utilizzo del coefficiente \\(\\alpha\\) come indicatore di affidabilità.\nEsempio. Per illustrare la procedura di calcolo del coefficiente \\(\\alpha\\), useremo i dati bfi contenuti nel pacchetto psych. Il dataframe bfi comprende 25 item di autovalutazione della personalità. Sono riportati i dati di 2800 soggetti. Ci concentreremo qui sulla sottoscala Openness. - O1: Am full of ideas; - O2: Avoid difficult reading material; - O3: Carry the conversation to a higher level; - O4: Spend time reflecting on things; - O5: Will not probe deeply into a subject.\nLeggiamo i dati in R.\n\ndata(bfi, package = \"psych\")\nhead(bfi[c(\"O1\", \"O2\", \"O3\", \"O4\", \"O5\")])\n\n\nA data.frame: 6 x 5\n\n\n\nO1\nO2\nO3\nO4\nO5\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n61617\n3\n6\n3\n4\n3\n\n\n61618\n4\n2\n4\n3\n3\n\n\n61620\n4\n2\n5\n5\n2\n\n\n61621\n3\n3\n4\n3\n5\n\n\n61622\n3\n3\n4\n3\n3\n\n\n61623\n4\n3\n5\n6\n1\n\n\n\n\n\nEsaminiamo la correlazione tra gli item della sottoscale Openness.\n\ncor(bfi[c(\"O1\", \"O2\", \"O3\", \"O4\", \"O5\")], use = \"pairwise.complete.obs\") |&gt;\n    round(2)\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nO1\nO2\nO3\nO4\nO5\n\n\n\n\nO1\n1.00\n-0.21\n0.40\n0.18\n-0.24\n\n\nO2\n-0.21\n1.00\n-0.26\n-0.07\n0.32\n\n\nO3\n0.40\n-0.26\n1.00\n0.19\n-0.31\n\n\nO4\n0.18\n-0.07\n0.19\n1.00\n-0.18\n\n\nO5\n-0.24\n0.32\n-0.31\n-0.18\n1.00\n\n\n\n\n\nÈ necessario ricodificare due item.\n\nbfi$O2r &lt;- 7 - bfi$O2\nbfi$O5r &lt;- 7 - bfi$O5\n\n\ncor(bfi[c(\"O1\", \"O2r\", \"O3\", \"O4\", \"O5r\")], use = \"pairwise.complete.obs\") |&gt;\n    round(2)\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nO1\nO2r\nO3\nO4\nO5r\n\n\n\n\nO1\n1.00\n0.21\n0.40\n0.18\n0.24\n\n\nO2r\n0.21\n1.00\n0.26\n0.07\n0.32\n\n\nO3\n0.40\n0.26\n1.00\n0.19\n0.31\n\n\nO4\n0.18\n0.07\n0.19\n1.00\n0.18\n\n\nO5r\n0.24\n0.32\n0.31\n0.18\n1.00\n\n\n\n\n\nConsideriamo la matrice di varianze e covarianze della sottoscala Openness.\n\nC &lt;- cov(bfi[c(\"O1\", \"O2r\", \"O3\", \"O4\", \"O5r\")], use = \"pairwise.complete.obs\")\nC |&gt; round(2)\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nO1\nO2r\nO3\nO4\nO5r\n\n\n\n\nO1\n1.28\n0.38\n0.54\n0.25\n0.36\n\n\nO2r\n0.38\n2.45\n0.50\n0.13\n0.67\n\n\nO3\n0.54\n0.50\n1.49\n0.29\n0.50\n\n\nO4\n0.25\n0.13\n0.29\n1.49\n0.29\n\n\nO5r\n0.36\n0.67\n0.50\n0.29\n1.76\n\n\n\n\n\nCalcoliamo alpha:\n\np &lt;- 5\nalpha &lt;- (p / (p - 1)) * (1 - tr(C) / sum(C))\nalpha\n\n0.600172514820215\n\n\nLo stesso risultato si ottiene utilizzando la funzione alpha() contenuta nel pacchetto psych:\n\npsych::alpha(C)\n\n\nReliability analysis   \nCall: psych::alpha(x = C)\n\n  raw_alpha std.alpha G6(smc) average_r S/N median_r\n       0.6      0.61    0.57      0.24 1.5     0.23\n\n    95% confidence boundaries \n      lower alpha upper\nFeldt -0.49   0.6  0.95\n\n Reliability if an item is dropped:\n    raw_alpha std.alpha G6(smc) average_r S/N  var.r med.r\nO1       0.53      0.53    0.48      0.22 1.1 0.0092  0.23\nO2r      0.57      0.57    0.51      0.25 1.3 0.0076  0.22\nO3       0.50      0.50    0.44      0.20 1.0 0.0071  0.20\nO4       0.61      0.62    0.56      0.29 1.6 0.0044  0.29\nO5r      0.51      0.53    0.47      0.22 1.1 0.0115  0.20\n\n Item statistics \n       r r.cor r.drop\nO1  0.65  0.52   0.39\nO2r 0.60  0.43   0.33\nO3  0.69  0.59   0.45\nO4  0.52  0.29   0.22\nO5r 0.66  0.52   0.42\n\n\n\n\n10.2.0.6 Metodi alternativi per la stima del coefficiente di attendibilità\nCi sono altri coefficienti di consistenza interna oltre al coefficiente alpha di Cronbach. Alcuni esempi includono il coefficiente KR-20 e il coefficiente KR-21, che vengono utilizzati con item dicotomici (ossia con risposte a due alternative, come vero/falso).\n\n\n10.2.0.7 Coefficiente KR-20\nLa formula di Kuder-Richardson-20 (KR-20) è un caso particolare del coefficiente α. Se ogni item è dicotomico, il coefficiente α diventa il KR-20. Il coefficiente Coefficiente KR-20 si calcola con la formula:\n\\[\nKR\\_20 = \\frac{{k}}{{k-1}} \\left( 1 - \\frac{{p(1-p)}}{{\\sigma_{X}^{2}}} \\right)\n\\]\ndove: - \\(k\\) è il numero di item nel test, - \\(p\\) è la proporzione di individui che rispondono correttamente all’item, - \\(\\sigma_{X}^{2}\\) è la varianza totale dei punteggi del test.\nEsempio. Per fare un esempio, consideriamo il data-set LSAT contenuto nel pacchetto ltm.\n\nKR20 &lt;- function(responses) {\n    # Get number of items (N) and individuals\n    n.items &lt;- ncol(responses)\n    n.persons &lt;- nrow(responses)\n    # get p_j for each item\n    p &lt;- colMeans(responses)\n    # Get total scores (X)\n    x &lt;- rowSums(responses)\n    # observed score variance\n    var.x &lt;- var(x) * (n.persons - 1) / n.persons\n    # Apply KR-20 formula\n    rel &lt;- (n.items / (n.items - 1)) * (1 - sum(p * (1 - p)) / var.x)\n    return(rel)\n}\n\n\ndata(LSAT)\nhead(LSAT)\n\n\nA data.frame: 6 x 5\n\n\n\nItem 1\nItem 2\nItem 3\nItem 4\nItem 5\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n0\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n\n\n3\n0\n0\n0\n0\n0\n\n\n4\n0\n0\n0\n0\n1\n\n\n5\n0\n0\n0\n0\n1\n\n\n6\n0\n0\n0\n0\n1\n\n\n\n\n\n\nKR20(LSAT)\n\n0.294997192215955\n\n\n\n\n10.2.0.8 Coefficiente KR-21\nIl coefficiente Coefficiente KR-21 si calcola con la formula:\n\\[\nKR\\_21 = \\frac{{k}}{{k-1}} \\left( 1 - \\frac{{\\frac{{\\sum_{i=1}^{k} p_{i}(1-p_{i})}}{{\\sigma_{X}^{2}}}}}{{1 - \\frac{{\\sum_{i=1}^{k} p_{i}}}{k}}} \\right)\n\\]\ndove: - \\(k\\) è il numero di item nel test, - \\(p_{i}\\) è la proporzione di individui che rispondono correttamente all’item \\(i\\), - \\(\\sigma_{X}^{2}\\) è la varianza totale dei punteggi del test.\n\n\n10.2.0.9 La formula “profetica” di Spearman-Brown\nL’indice di Spearman-Brown stima l’attendibilità nel caso di \\(p\\) indicatori paralleli:\n\\[\n\\begin{equation}\n  \\rho_p = \\frac{p \\rho_1}{(p-1)\\rho_1 + 1},\n\\end{equation}\n\\] (eq-spearman-brown-der)\ndove \\(\\rho_1\\) rappresenta l’attendibilità di un singolo elemento.\nUna derivazione della formula Spearman-Brown è fornita nel capitolo {ref}reliability-fa-notebook.\nL’equazione {eq}eq-spearman-brown-der esprime l’attendibilità \\(\\rho_p\\) di un test composto da \\(p\\) elementi paralleli in termini dell’attendibilità di un singolo elemento. Questa equazione è universalmente riconosciuta come la formula “profetica” di Spearman-Brown (Spearman-Brown prophecy formula).\nPer fare un esempio concreto, poniamoci il problema di calcolare l’attendibilità della sottoscala Openness utilizzando la formula di Spearman-Brown. Ipotizziamo dunque che gli item della scala Openness siano paralleli. La matrice di correlazione è:\n\nR &lt;- cor(bfi[c(\"O1\", \"O2r\", \"O3\", \"O4\", \"O5r\")], use = \"pairwise.complete.obs\")\nR |&gt; round(2)\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nO1\nO2r\nO3\nO4\nO5r\n\n\n\n\nO1\n1.00\n0.21\n0.40\n0.18\n0.24\n\n\nO2r\n0.21\n1.00\n0.26\n0.07\n0.32\n\n\nO3\n0.40\n0.26\n1.00\n0.19\n0.31\n\n\nO4\n0.18\n0.07\n0.19\n1.00\n0.18\n\n\nO5r\n0.24\n0.32\n0.31\n0.18\n1.00\n\n\n\n\n\nSupponiamo di calcolare l’attendibilità di un singolo item (\\(\\rho_1\\)) come la correlazione media tra gli item:\n\nrr &lt;- NULL\np &lt;- 5\nk &lt;- 1\nfor (i in 1:p) {\n    for (j in 1:p) {\n        if (j != i) {\n            rr[k] &lt;- R[i, j]\n        }\n        k &lt;- k + 1\n    }\n}\nro_1 &lt;- mean(rr, na.rm = TRUE)\nro_1\n\n0.236538319550858\n\n\nApplicando la formula di Spearman-Brown, la stima dell’attendibilità del test diventa pari a\n\n(p * ro_1) / ((p - 1) * ro_1 + 1)\n\n0.607707322439719",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Metodi di stima dell'affidabilità</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/03_ctt_3.html#forme-parallele-del-test",
    "href": "chapters/ctt/03_ctt_3.html#forme-parallele-del-test",
    "title": "10  Metodi di stima dell’affidabilità",
    "section": "10.3 Forme parallele del test",
    "text": "10.3 Forme parallele del test\nIn alcune situazioni, è possibile avere a disposizione diverse versioni di un test che sono progettate per essere interscambiabili, in modo tale che la specifica versione del test non influenzi i punteggi ottenuti dai partecipanti. Queste forme alternative del test sono comuni soprattutto nel campo dell’educazione, dove spesso vengono preparate diverse versioni al fine di prevenire frodi o imbrogli. Inoltre, anche i ricercatori possono adottare forme alternative in studi che coinvolgono pre-test e post-test, al fine di evitare che i partecipanti beneficiino degli effetti di pratica o memoria. Tuttavia, è di fondamentale importanza determinare se i punteggi ottenuti da queste diverse versioni sono coerenti, poiché la mancanza di equivalenza tra le forme potrebbe condurre a conclusioni errate riguardo alle variazioni dei punteggi.\nLe principali fonti di errore di misurazione per le forme alternative di test cognitivi derivano dalle differenze nei contenuti, nella difficoltà e nella complessità cognitiva degli item. Per quanto riguarda i test non-cognitivi, le differenze nei contenuti e nell’intensità degli item sono motivo di attenzione. Gli sviluppatori di forme alternative adottano diverse procedure al fine di garantire l’equivalenza tra le varie versioni, basandosi sulla stessa tabella di specifiche che stabilisce la proporzione di item per i diversi domini di contenuto e i livelli cognitivi o non-cognitivi. Inoltre, vengono appaiati gli item in base alla loro difficoltà e alla loro capacità discriminante.\nI coefficienti di equivalenza, noti anche come affidabilità delle forme alternative, valutano la similitudine tra due o più versioni di un test. Per calcolare questi coefficienti, le diverse forme vengono somministrate agli stessi partecipanti e i punteggi ottenuti vengono correlati. Tuttavia, vi sono alcune considerazioni legate alla somministrazione dei test e alla possibile fatica dei partecipanti. Al fine di affrontare tali problematiche, possono essere adottate strategie come il bilanciamento dell’ordine di somministrazione e l’introduzione di un breve intervallo di tempo tra le diverse versioni. Inoltre, è importante considerare gli effetti della pratica o della memoria, i quali potrebbero influenzare i punteggi ottenuti nel secondo test somministrato. L’impiego del bilanciamento tra gruppi può contribuire a controllare tali effetti.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Metodi di stima dell'affidabilità</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/03_ctt_3.html#attendibilità-test-retest",
    "href": "chapters/ctt/03_ctt_3.html#attendibilità-test-retest",
    "title": "10  Metodi di stima dell’affidabilità",
    "section": "10.4 Attendibilità test-retest",
    "text": "10.4 Attendibilità test-retest\nInfine, esaminiamo il concetto di “affidabilità test-retest”, che si riferisce alla coerenza o stabilità dei punteggi di un test in diverse occasioni nel corso del tempo. Questo tipo di affidabilità riveste una particolare importanza nelle situazioni in cui i punteggi vengono ottenuti in momenti diversi e confrontati, come nel caso di test effettuati prima e dopo un intervento. Inoltre, è di rilievo quando i punteggi del test vengono utilizzati per prendere decisioni diagnostiche, di selezione o di collocazione. Tuttavia, è importante sottolineare che l’affidabilità test-retest non è adatta per valutare costrutti che non sono noti per la loro stabilità nel tempo. Ciò deriva dal fatto che l’analisi della stabilità di un test potrebbe essere influenzata da effettivi cambiamenti nei livelli veri del costrutto tra i partecipanti. Di conseguenza, è essenziale che i ricercatori siano consapevoli in anticipo della stabilità del costrutto che intendono misurare. È importante notare che molti costrutti di interesse nelle scienze sociali sono generalmente considerati stabili nel tempo, come ad esempio la creatività, l’abilità cognitiva e alcune caratteristiche della personalità.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Metodi di stima dell'affidabilità</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/03_ctt_3.html#affidabilità-dei-punteggi-compositi",
    "href": "chapters/ctt/03_ctt_3.html#affidabilità-dei-punteggi-compositi",
    "title": "10  Metodi di stima dell’affidabilità",
    "section": "10.5 Affidabilità dei punteggi compositi",
    "text": "10.5 Affidabilità dei punteggi compositi\nL’affidabilità dei punteggi compositi si riferisce alla misura in cui più punteggi ottenuti da diverse fonti possono essere combinati per creare un punteggio complessivo. Ad esempio, nella valutazione educativa, la determinazione delle votazioni spesso si basa su un punteggio complessivo ottenuto da diverse prove e altre valutazioni somministrate durante un periodo di valutazione o un semestre. Molti test psicologici standardizzati includono diverse sottoscale che vengono combinate per formare un punteggio complessivo.\nIl vantaggio dei punteggi compositi è che la loro affidabilità è generalmente maggiore rispetto a quella dei punteggi individuali delle sottoscale (o item) che contribuiscono al punteggio composto. Più precisamente, l’affidabilità di un punteggio composto è il risultato del numero di punteggi inclusi nel composto, dell’affidabilità dei punteggi individuali e della correlazione tra questi punteggi. Più punteggi sono inclusi nel composto, più alta è la correlazione tra di essi e maggiore è l’affidabilità individuale, maggiore è l’affidabilità del composto. Come abbiamo notato in precedenza, i test rappresentano semplicemente dei campioni del dominio che si intende misurare, e la combinazione di misurazioni multiple è analoga all’aumento del numero di osservazioni o della dimensione del campione.\nPer fare un esempio, supponiamo di avere due variabili aleatorie, $ X $ e $ Y $, che rappresentano i punteggi di due subtest diversi. L’affidabilità (indicata come $ $) di un test è legata alla varianza del test stesso. Un modo per esprimere l’affidabilità è attraverso il rapporto tra la varianza del vero punteggio (quello che il test intende misurare) e la varianza totale del test. Supponendo che il vero punteggio e l’errore di misura siano indipendenti, la varianza totale del test è la somma della varianza del vero punteggio e della varianza dell’errore.\nQuando combiniamo più subtest in un punteggio composito, stiamo in effetti aumentando la varianza del vero punteggio (poiché stiamo combinando più misurazioni del costrutto che vogliamo misurare) mentre l’errore di misura, supposto indipendente tra i subtest, si somma meno che proporzionalmente.\nPer rendere queste affermazioni più concrete, consideriamo un esempio numerico nel quale supponiamo che i subtest siano correlati (il che è spesso il caso in psicometria, dove diversi subtest possono misurare aspetti correlati di un costrutto più ampio).\n\n10.5.1 Calcolo per il Puniteggio Composito\nPer esempio, dati due subtest con una varianza del vero punteggio di 25 ciascuno e una covarianza di 15 (dovuta al vero punteggio), la varianza del vero punteggio nel composito è data da:\n\\[ \\text{Var}(Z_{vero}) = 25 + 25 + 2 \\cdot 15 = 80 \\]\nLa varianza totale nel composito, tenendo conto anche della varianza dell’errore di misura, sarà:\n\\[ \\text{Var}(Z_{totale}) = 35 + 35 + 2 \\cdot 15 = 100 \\]\nIl rapporto tra la varianza del vero punteggio e la varianza totale nel composito è:\n\\[ \\text{Rapporto} = \\frac{\\text{Var}(Z_{vero})}{\\text{Var}(Z_{totale})} = \\frac{80}{100} = 0.8 \\]\n\n\n10.5.2 Confronto con un Singolo Subtest\nLa varianza del vero punteggio in un singolo subtest è data (come da ipotesi) da 25.\nLa varianza totale in un singolo subtest è la somma della varianza del vero punteggio e quella dell’errore di misura, quindi 35 (25 di vero punteggio + 10 di errore).\nIl rapporto tra la varianza del vero punteggio e la varianza totale in un singolo subtest è:\n\\[ \\text{Rapporto} = \\frac{\\text{Var}(X_{vero})}{\\text{Var}(X_{totale})} = \\frac{25}{35} \\approx 0.714 \\]\nIl confronto mostra che l’affidabilità del punteggio composito (0.8) è maggiore di quella di un singolo subtest (circa 0.714). Questo esemplifica come la correlazione positiva tra i subtest possa effettivamente aumentare l’affidabilità del punteggio composito rispetto ai subtest individuali.\nQuindi, il vantaggio di combinare i punteggi dai subtest in un punteggio composito emerge principalmente quando i subtest sono in qualche modo correlati e/o quando la varianza dell’errore di misura è ridotta rispetto alla varianza del vero punteggio. In pratica, l’uso di punteggi compositi è spesso giustificato dall’idea che essi forniscono una misura più completa e rappresentativa del costrutto di interesse, riducendo l’impatto dell’errore di misura specifico di ciascun subtest.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Metodi di stima dell'affidabilità</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/03_ctt_3.html#laffidabilità-dei-punteggi-differenza",
    "href": "chapters/ctt/03_ctt_3.html#laffidabilità-dei-punteggi-differenza",
    "title": "10  Metodi di stima dell’affidabilità",
    "section": "10.6 L’affidabilità dei Punteggi Differenza",
    "text": "10.6 L’affidabilità dei Punteggi Differenza\nCi sono numerose situazioni in cui ricercatori e clinici vogliono considerare la differenza tra due punteggi. Qui, la variabile di interesse è un punteggio differenza che viene calcolato come:\n\\[ D = X - Y, \\]\ndove X è il punteggio su un test e Y su un altro. Ad esempio, un approccio alla diagnosi delle difficoltà di apprendimento prevede il calcolo dei punteggi differenza sottraendo il punteggio di un esaminando in un test di rendimento (ad esempio, comprensione della lettura) dal suo QI. Si presume che se la discrepanza è negativa e sufficientemente ampia (ad esempio, due o più deviazioni standard), l’esaminando non sta dimostrando un rendimento accademico commisurato all’attitudine. Se ulteriori valutazioni escludono una serie di spiegazioni come opportunità educative inadeguate o problemi sensoriali (ad esempio, problemi visivi o uditivi), la discrepanza potrebbe riflettere una difficoltà di apprendimento intrinseca.\nUn altro esempio comune dell’utilizzo dei punteggi differenza si ha quando uno psicologo vuole considerare i guadagni (o le perdite) nella performance di un test nel tempo. Ad esempio, un ricercatore potrebbe voler determinare se un trattamento specifico ha portato a un miglioramento nelle prestazioni su un determinato compito. Ciò è spesso realizzato somministrando test prima e dopo l’intervento.\nIn queste situazioni, la variabile di interesse è un punteggio differenza. Quando si trattano punteggi differenza, è però importante ricordare che l’affidabilità dei punteggi differenza è tipicamente considerevolmente inferiore rispetto alle affidabilità dei punteggi individuali. Come regola generale, l’affidabilità dei punteggi differenza diminuisce all’aumentare della correlazione tra le misure individuali.\nLa formula per l’affidabilità dei punteggi differenza è data da:\n\\[\nr_{dd} = \\frac{0.5 (r_{xx} + r_{yy}) - r_{xy}}{1 - r_{xy}}\n\\],\ndove \\(r_{xx}\\) e \\(r_{yy}\\) sono le affidabilità delle due componenti della differenza e \\(r_{xy}\\) è la loro correlazione. Facciamo un esempio numerico varianza la correlazione tra le due componenti.\n\nrdd &lt;- function(rxx, ryy, rxy) {\n    (0.5 * (rxx + ryy) - rxy) / (1 - rxy)\n}\n\nseq(0.01, 0.81, by = 0.1)\n\n\n0.010.110.210.310.410.510.610.710.81\n\n\n\nrxx &lt;- 0.9\nryy &lt;- 0.8\n\nrdd(rxx, ryy, seq(0.01, 0.81, by = 0.1))\n\n\n0.8484848484848490.8314606741573040.8101265822784810.7826086956521740.7457627118644070.6938775510204080.6153846153846160.4827586206896550.210526315789474\n\n\nSi vede che, all’aumentare di \\(r_{xy}\\), l’affidabilità del punteggio differenza diminuisce.\nIn sintesi, si dovrebbe essere cauti nell’interpretare i punteggi differenza. L’affidabilità dei punteggi differenza è tipicamente considerevolmente inferiore rispetto alle affidabilità dei punteggi individuali. Per aggravare il problema, i punteggi differenza sono spesso calcolati utilizzando punteggi che hanno correlazioni piuttosto forti tra loro (ad esempio, punteggi di QI e di rendimento; punteggi pre e post test).",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Metodi di stima dell'affidabilità</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/03_ctt_3.html#scelta-del-coefficiente-di-affidabilità-in-funzione-del-contesto",
    "href": "chapters/ctt/03_ctt_3.html#scelta-del-coefficiente-di-affidabilità-in-funzione-del-contesto",
    "title": "10  Metodi di stima dell’affidabilità",
    "section": "10.7 Scelta del Coefficiente di Affidabilità in Funzione del Contesto",
    "text": "10.7 Scelta del Coefficiente di Affidabilità in Funzione del Contesto\nLa selezione di un coefficiente di affidabilità adeguato dipende da diversi fattori, tra cui la natura del costrutto psicologico misurato e l’uso che si intende fare dei risultati del test. È fondamentale considerare il contesto specifico in cui verrà applicato il test per identificare l’indice di affidabilità più appropriato.\n\n10.7.1 Affidabilità Test-Retest\nL’affidabilità test-retest è utile per test che vengono somministrati più volte agli stessi individui e misura la stabilità dei punteggi nel tempo. Questa misura è particolarmente importante per i test che potrebbero essere influenzati da errori di misurazione temporali. Ad esempio, in un test utilizzato per prevedere il comportamento futuro, l’affidabilità test-retest può fornire una stima affidabile della variabilità legata al tempo.\n\n\n10.7.2 Affidabilità della Coerenza Interna\nPer test somministrati una sola volta, è più rilevante considerare la coerenza interna. Si distinguono principalmente due metodi:\n\nAffidabilità Split-Half: Questa stima dell’affidabilità valuta l’errore dovuto alla varianza del campionamento del contenuto, risultando utile in test con contenuti eterogenei. Ad esempio, in un test che misura costrutti multipli (depressione, ansia, rabbia, impulsività), l’approccio split-half può essere preferito, poiché divide idealmente il test in due parti equilibrate per ciascun costrutto.\nCoefficienti Alfa e KR-20: Questi coefficienti stimano l’errore associato sia al campionamento del contenuto sia all’eterogeneità del costrutto misurato, risultando appropriati quando il test copre un singolo ambito di conoscenza o un unico tratto psicologico. Ad esempio, per un test sull’umore depressivo, l’alfa o il KR-20 sono indicati in quanto mirano a un dominio specifico e omogeneo.\n\n\n\n10.7.3 Affidabilità delle Forme Alternate\nPer test con diverse versioni, è necessario stimare l’affidabilità delle forme alternate per garantire la coerenza dei punteggi tra le varie versioni, assicurandosi che esse siano equivalenti e affidabili.\n\n\n10.7.4 Affidabilità Inter-Valutatori\nQuando il test richiede giudizi soggettivi da parte dei valutatori, diventa essenziale considerare l’affidabilità inter-valutatori. Questo tipo di affidabilità valuta la consistenza tra giudizi di diversi valutatori, assicurando che le valutazioni siano oggettive e riducendo la dipendenza dalle interpretazioni individuali.\nIn sintesi, la scelta del coefficiente di affidabilità dipende dal contesto del test, dalla natura del costrutto e dallo scopo del test. Una selezione accurata è cruciale per garantire la validità e l’accuratezza delle misurazioni psicologiche.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Metodi di stima dell'affidabilità</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/03_ctt_3.html#linee-guida-sulla-scelta-e-valutazione-dei-coefficienti-di-affidabilità",
    "href": "chapters/ctt/03_ctt_3.html#linee-guida-sulla-scelta-e-valutazione-dei-coefficienti-di-affidabilità",
    "title": "10  Metodi di stima dell’affidabilità",
    "section": "10.8 Linee Guida sulla Scelta e Valutazione dei Coefficienti di Affidabilità",
    "text": "10.8 Linee Guida sulla Scelta e Valutazione dei Coefficienti di Affidabilità\nLa valutazione dei coefficienti di affidabilità in ambito psicometrico è influenzata da molteplici fattori.\n\n10.8.1 Significato e Importanza dei Coefficienti di Affidabilità\nI coefficienti di affidabilità rappresentano la proporzione della varianza dei punteggi attribuibile a differenze reali tra gli individui nel costrutto misurato. Ideale sarebbe raggiungere un valore di 1.0, suggerendo che tutta la varianza dei punteggi è legata a differenze effettive tra gli individui. Tuttavia, a causa dell’inevitabile errore di misurazione, una misura perfettamente affidabile è irrealizzabile. Un livello “accettabile” di affidabilità varia in base a costrutto, tempo disponibile, uso dei punteggi e metodo di stima.\n\n\n10.8.2 Fattori da Considerare nella Valutazione dell’Affidabilità\n\nCostrutto: Costrutti complessi come quelli legati alla personalità possono essere più difficili da misurare rispetto alle abilità cognitive. Un livello di affidabilità accettabile per una scala di “dipendenza” potrebbe non essere adeguato per una misura di intelligenza.\nTempo per il Test: Il tempo limitato influisce sull’affidabilità, poiché meno item aumentano l’errore di campionamento. Test brevi, come quelli per lo screening, richiedono standard di affidabilità diversi rispetto a quelli più lunghi.\nUso dei Punteggi del Test: Test diagnostici che influenzano decisioni cruciali richiedono standard di affidabilità elevati. Ad esempio, test sull’intelligenza utilizzati per diagnosi necessitano di alta affidabilità rispetto ai test usati per ricerche di gruppo o screening.\nMetodo di Stima dell’Affidabilità: I metodi di stima influenzano la grandezza dei coefficienti. Ad esempio, KR-20 e alfa tendono a stimare affidabilità più bassa rispetto al metodo split-half.\n\n\n\n10.8.3 Linee Guida Generali per i Coefficienti di Affidabilità\nEcco alcune linee guida generali:\n\nDecisioni importanti: Coefficienti ≥ 0.90, o persino 0.95, sono consigliabili.\nTest di rendimento e personalità: Coefficienti ≥ 0.80 sono generalmente accettabili.\nTest didattici o di screening: Coefficienti ≥ 0.70.\nRicerca di gruppo: Coefficienti ≥ 0.60 possono essere accettabili, ma con cautela se sotto 0.70.\n\nIn conclusione, la valutazione dell’affidabilità di un test psicometrico richiede considerazioni dettagliate dei vari fattori chiave, con standard di accettabilità che variano in base al contesto del test e al suo scopo specifico.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Metodi di stima dell'affidabilità</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/03_ctt_3.html#riflessioni-conclusive",
    "href": "chapters/ctt/03_ctt_3.html#riflessioni-conclusive",
    "title": "10  Metodi di stima dell’affidabilità",
    "section": "10.9 Riflessioni Conclusive",
    "text": "10.9 Riflessioni Conclusive\nIn conclusione, la valutazione dell’affidabilità di un test richiede l’impiego di diversi coefficienti che tengono conto delle varie fonti di errore. I coefficienti di consistenza interna si concentrano sull’errore derivante dalle fluttuazioni delle risposte tra gli item, mentre quelli di equivalenza esaminano la coerenza dei punteggi tra diverse versioni del test. I coefficienti di stabilità misurano la coerenza dei punteggi nel corso del tempo. È di fondamentale importanza selezionare il tipo di affidabilità appropriato in base allo scopo del test, al fine di ottenere informazioni affidabili e utili per le decisioni basate sui punteggi ottenuti dal test.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Metodi di stima dell'affidabilità</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/03_ctt_3.html#session-info",
    "href": "chapters/ctt/03_ctt_3.html#session-info",
    "title": "10  Metodi di stima dell’affidabilità",
    "section": "10.10 Session Info",
    "text": "10.10 Session Info\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ltm_1.2-0          polycor_0.8-1      msm_1.7.1          MASS_7.3-60.0.1   \n [5] modelsummary_1.4.5 ggokabeito_0.1.0   viridis_0.6.5      viridisLite_0.4.2 \n [9] ggpubr_0.6.0       ggExtra_0.10.1     bayesplot_1.11.1   gridExtra_2.3     \n[13] patchwork_1.2.0    semTools_0.5-6     semPlot_1.1.6      lavaan_0.6-17     \n[17] psych_2.4.1        scales_1.3.0       markdown_1.12      knitr_1.45        \n[21] lubridate_1.9.3    forcats_1.0.0      stringr_1.5.1      dplyr_1.1.4       \n[25] purrr_1.0.2        readr_2.1.5        tidyr_1.3.1        tibble_3.2.1      \n[29] ggplot2_3.5.0      tidyverse_2.0.0    here_1.0.1        \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.15.0  jsonlite_1.8.8     magrittr_2.0.3    \n  [4] nloptr_2.0.3       rmarkdown_2.26     vctrs_0.6.5       \n  [7] minqa_1.2.6        base64enc_0.1-3    rstatix_0.7.2     \n [10] htmltools_0.5.7    broom_1.0.5        Formula_1.2-5     \n [13] htmlwidgets_1.6.4  plyr_1.8.9         uuid_1.2-0        \n [16] admisc_0.35        igraph_2.0.2       mime_0.12         \n [19] lifecycle_1.0.4    pkgconfig_2.0.3    Matrix_1.6-5      \n [22] R6_2.5.1           fastmap_1.1.1      shiny_1.8.0       \n [25] digest_0.6.34      OpenMx_2.21.11     fdrtool_1.2.17    \n [28] colorspace_2.1-0   rprojroot_2.0.4    Hmisc_5.1-1       \n [31] fansi_1.0.6        timechange_0.3.0   abind_1.4-5       \n [34] compiler_4.3.3     withr_3.0.0        glasso_1.11       \n [37] htmlTable_2.4.2    backports_1.4.1    carData_3.0-5     \n [40] ggsignif_0.6.4     corpcor_1.6.10     gtools_3.9.5      \n [43] tools_4.3.3        pbivnorm_0.6.0     foreign_0.8-86    \n [46] zip_2.3.1          httpuv_1.6.14      nnet_7.3-19       \n [49] glue_1.7.0         quadprog_1.5-8     nlme_3.1-164      \n [52] promises_1.2.1     lisrelToR_0.3      grid_4.3.3        \n [55] pbdZMQ_0.3-11      checkmate_2.3.1    cluster_2.1.6     \n [58] reshape2_1.4.4     generics_0.1.3     gtable_0.3.4      \n [61] tzdb_0.4.0         data.table_1.15.2  hms_1.1.3         \n [64] car_3.1-2          utf8_1.2.4         tables_0.9.17     \n [67] sem_3.1-15         pillar_1.9.0       IRdisplay_1.1     \n [70] rockchalk_1.8.157  later_1.3.2        splines_4.3.3     \n [73] lattice_0.22-5     survival_3.5-8     kutils_1.73       \n [76] tidyselect_1.2.0   miniUI_0.1.1.1     pbapply_1.7-2     \n [79] stats4_4.3.3       xfun_0.42          expm_0.999-9      \n [82] qgraph_1.9.8       arm_1.13-1         stringi_1.8.3     \n [85] boot_1.3-29        evaluate_0.23      mi_1.1            \n [88] cli_3.6.2          RcppParallel_5.1.7 IRkernel_1.3.2    \n [91] rpart_4.1.23       xtable_1.8-4       repr_1.1.6        \n [94] munsell_0.5.0      Rcpp_1.0.12        coda_0.19-4.1     \n [97] png_0.1-8          XML_3.99-0.16.1    parallel_4.3.3    \n[100] ellipsis_0.3.2     jpeg_0.1-10        lme4_1.1-35.1     \n[103] mvtnorm_1.2-4      insight_0.19.8     openxlsx_4.2.5.2  \n[106] crayon_1.5.2       rlang_1.1.3        mnormt_2.1.1",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Metodi di stima dell'affidabilità</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/04_err_std_mis.html",
    "href": "chapters/ctt/04_err_std_mis.html",
    "title": "11  L’errore standard della misurazione",
    "section": "",
    "text": "11.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nI coefficienti di affidabilità che abbiamo discusso nel capitolo precedente rappresentano una misura proporzionale della varianza osservata di un test che è attribuibile alla varianza reale. Questi coefficienti sono fondamentali per confrontare l’affidabilità dei punteggi ottenuti da diverse procedure di valutazione. In generale, preferiremo selezionare il test che produce i punteggi con la migliore affidabilità. Tuttavia, una volta scelto il test, il nostro focus si sposta sull’interpretazione dei punteggi.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>L'errore standard della misurazione</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/04_err_std_mis.html#errore-standard-della-misurazione",
    "href": "chapters/ctt/04_err_std_mis.html#errore-standard-della-misurazione",
    "title": "11  L’errore standard della misurazione",
    "section": "11.2 Errore Standard della Misurazione",
    "text": "11.2 Errore Standard della Misurazione\nL’Errore Standard della Misurazione (SEM) diventa una statistica più pratica quando l’attenzione è rivolta all’interpretazione dei punteggi di un test. Il SEM è definito come la deviazione standard della distribuzione dei punteggi che un individuo otterrebbe se fosse sottoposto a un numero infinito di forme parallele del test, costituite da item campionati casualmente dallo stesso dominio di contenuto.\nPer comprendere meglio, immaginiamo di creare un numero infinito di forme parallele di un test e di far svolgere queste forme alla stessa persona, senza che vi siano effetti di trasferimento. La presenza dell’errore di misurazione impedirebbe alla persona di ottenere sempre lo stesso punteggio. Anche se ogni test rappresenta ugualmente bene il dominio di contenuto, il candidato potrebbe ottenere risultati migliori in alcuni test e peggiori in altri, semplicemente a causa di errori casuali (ad esempio, la fortuna nel conoscere le risposte agli item selezionati per una versione del test ma non per un’altra). Prendendo i punteggi ottenuti in tutti questi test, si otterrebbe una distribuzione di punteggi. La media di questa distribuzione rappresenta il punteggio vero (T) dell’individuo, mentre il SEM è la deviazione standard di questa distribuzione di punteggi di errore.\nOvviamente, non è possibile attuare questi procedimenti nella realtà, quindi dobbiamo stimare il SEM utilizzando le informazioni disponibili. Esamineremo qui l’approccio utilizzato dalla Teoria Classica dei Test (CTT) per raggiungere questo obiettivo.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>L'errore standard della misurazione</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/04_err_std_mis.html#stima-di-sem",
    "href": "chapters/ctt/04_err_std_mis.html#stima-di-sem",
    "title": "11  L’errore standard della misurazione",
    "section": "11.3 Stima di SEM",
    "text": "11.3 Stima di SEM\nSecondo Lord (1968), l’errore \\(E = X - T\\) rappresenta la variabile aleatoria di interesse primario nella CTT. L’obiettivo della CTT è stimare il punteggio vero di ogni rispondente e confrontare le stime ottenute per rispondenti diversi. La grandezza dell’errore \\(E\\) fornisce informazioni essenziali in questo contesto. La discrepanza tra il punteggio osservato e il punteggio vero può essere misurata utilizzando la deviazione standard degli errori \\(E\\), conosciuta appunto come “Errore Standard della Misurazione” o SEM. Il SEM è quindi lo strumento impiegato dalla CTT per stimare in che misura un punteggio osservato differisce dal punteggio vero.\nNel presente capitolo esploreremo come sia possibile stimare la deviazione standard dell’errore (\\(\\sigma_E\\)) in un campione di osservazioni. Questo consente di comprendere meglio la precisione dei punteggi ottenuti attraverso un test psicometrico e di interpretare in modo più accurato i risultati.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>L'errore standard della misurazione</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/04_err_std_mis.html#lincertezza-della-misura",
    "href": "chapters/ctt/04_err_std_mis.html#lincertezza-della-misura",
    "title": "11  L’errore standard della misurazione",
    "section": "11.4 L’incertezza della misura",
    "text": "11.4 L’incertezza della misura\nIn base alla CTT, è possibile stimare l’errore standard della misurazione utilizzando una formula che dipende dalla deviazione standard della distribuzione dei punteggi del test e dall’attendibilità del test. Mediante questa formula, è possibile ottenere una stima dell’errore standard associato a un singolo punteggio, il quale indica quanto il punteggio osservato può variare rispetto al vero punteggio di un individuo:\n\\[\n\\sigma_E = \\sigma_X \\sqrt{1 -\\rho_{XX^\\prime}},\n\\tag{11.1}\\]\ndove \\(\\sigma_X\\) rappresenta la deviazione standard dei punteggi ottenuti da un campione di soggetti e \\(\\rho_{XX^\\prime}\\) è il coefficiente di attendibilità. Attraverso questo calcolo, si ottiene l’errore standard della misurazione sottraendo l’attendibilità del test da 1, quindi calcolando la radice quadrata del risultato e moltiplicandolo per la deviazione standard dei punteggi del test.\nLa logica alla base dell’errore standard della misurazione si fonda sull’assunzione che se una persona dovesse sostenere numerosi test equivalenti, i punteggi ottenuti seguirebbero una distribuzione normale con il vero punteggio dell’individuo come media. In altre parole, possiamo immaginare che l’individuo affronti ripetutamente versioni identiche del test, in circostanze simili e senza ricordare le risposte precedenti. In tale contesto ipotetico, l’errore standard della misurazione rappresenterebbe la deviazione standard tra queste misurazioni ripetute.\nLa formula sopra indicata evidenzia come l’errore standard della misurazione (\\(\\sigma_E\\)) sia strettamente correlato all’attendibilità del test: all’aumentare dell’attendibilità del test, l’errore standard della misurazione diminuisce. Se l’attendibilità del test si avvicina a 0, l’errore standard della misurazione tende a diventare uguale alla deviazione standard dei punteggi osservati del test. In contrasto, se l’attendibilità del test raggiunge 1, l’errore standard della misurazione si riduce a zero: in una situazione di perfetta affidabilità, in cui non vi è alcun errore di misurazione, \\(\\sigma_E\\) assume valore zero.\n\n11.4.1 Interpretazione\nLa Teoria Classica dei Test (CTT) postula che, se un individuo dovesse ripetere un test un numero infinito di volte, mantenendo inalterate le condizioni di somministrazione, i punteggi ottenuti si distribuirebbero in maniera normale attorno al suo vero punteggio. L’errore standard di misura (SEM) viene quindi definito come la stima della deviazione standard di questa distribuzione ipotetica di punteggi. Di conseguenza, un SEM elevato indica una maggiore incertezza nell’utilizzo del test per valutare l’abilità latente dell’individuo.\nSecondo McDonald, invece, il termine di errore (E) segue una distribuzione di propensione, che riflette le variazioni casuali nelle prestazioni di un individuo nel tempo a causa di test. Queste variazioni possono essere influenzate da fattori quali lo stato d’animo, la motivazione e altre variabili contestuali. L’errore standard di misura, in questo contesto, fornisce una quantificazione della deviazione standard dei punteggi attesi per un individuo, se fosse possibile testarlo un numero infinito di volte (o attraverso test equivalenti) in condizioni identiche, assumendo che il suo vero punteggio rimanga invariato.\nIl coefficiente di attendibilità, la varianza dell’errore e l’errore standard di misura rappresentano metriche che riflettono la precisione di un test psicometrico, ciascuna fornendo un tipo di insight specifico sulla precisione:\n\nL’errore standard di misura (SEM) offre una stima della precisione di un punteggio osservato per un individuo, offrendo una base per inferenze riguardo l’affidabilità di quel punteggio specifico. Al contrario, il coefficiente di attendibilità non si presta a una interpretazione così diretta in relazione ai punteggi individuali.\nIl SEM è calcolato nell’unità di misura dei punteggi del test, facilitando la comprensione e l’interpretazione della variabilità attorno al punteggio osservato di un individuo. Diversamente, la varianza dell’errore è espressa come il quadrato delle unità di misura del punteggio, rendendola meno intuitiva per interpretazioni dirette riguardanti la precisione del punteggio.\nIl coefficiente di attendibilità quantifica il rapporto tra la varianza dei punteggi veri e la varianza totale dei punteggi osservati, risultando in un indice senza unità di misura (adimensionale). Questo lo distingue dal SEM e dalla varianza dell’errore, in quanto l’attendibilità valuta la consistenza relativa dei punteggi all’interno dell’intero test piuttosto che la precisione di un singolo punteggio osservato.\n\nEsempio 1. Consideriamo un esempio in cui un test di intelligenza fornisce un punteggio medio di 100 con una deviazione standard di 15. Supponiamo inoltre che l’attendibilità di questo test sia pari a 0.73. Vogliamo calcolare l’errore standard della misurazione.\nUtilizzando la formula dell’errore standard della misurazione, otteniamo:\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\sigma_E &= \\sigma_X \\sqrt{1 -\\rho_{XX^\\prime}} \\notag\\\\\n&= 15 \\sqrt{1 - 0.73} \\notag\\\\\n&= 7.79.\\notag\n\\end{aligned}\n\\end{equation}\n\\]\nIl valore 7.79 rappresenta l’errore standard atteso nei punteggi ottenuti da un singolo individuo se il test fosse somministrato più volte sotto identiche condizioni. In altre parole, ci aspettiamo che i punteggi variino in media di circa 8 punti tra diverse somministrazioni del test.\nInoltre, possiamo utilizzare l’errore standard della misurazione per calcolare un intervallo di confidenza intorno al vero punteggio del rispondente. Utilizzando la proprietà della distribuzione gaussiana, possiamo stimare che il 95% dei punteggi ottenuti da ripetute somministrazioni del test si troveranno nell’intervallo:\n\\[\n\\text{punteggio vero del rispondente} \\pm 1.96 \\cdot \\text{errore standard della misurazione}.\n\\]\nNel nostro caso, questo intervallo sarebbe pari a \\(2 \\cdot 1.96 \\cdot 7.79 = 30.54\\) punti. Quindi, ci aspettiamo che i punteggi del QI di un singolo rispondente varino all’interno di un intervallo di 30 punti se il test fosse somministrato molte volte sotto le stesse condizioni.\nQuesto esempio dimostra che se un test ha un’attendibilità di 0.73 e una deviazione standard dei punteggi di 15, la misurazione del test su un singolo individuo risulterebbe poco affidabile a causa dell’ampio errore di misurazione. A titolo di confronto, la Full Scale IQ (FSIQ) della WAIS-IV Wechsler (2008) ha un’attendibilità split-half di 0.98 e un errore standard di misurazione di 2.16.\nL’errore standard della misurazione può anche essere calcolato utilizzando la funzione SE.Means() del pacchetto psychometric.\n\nSE.Meas(15, .73)\n\n7.79422863405995\n\n\nEsempio 2. Continuando con l’esempio precedente, per gli ipotetici dati riportati sopra, poniamoci ora la seguente domanda: qual è la probabilità che un rispondente ottenga un punteggio minore o uguale a 116 nel test, se il suo punteggio vero fosse uguale a 120?\nIl problema si risolve rendendosi conto che i punteggi del rispondente si distribuiscono normalmente attorno al punteggio vero di 120, con una deviazione standard uguale a 7.79. Dobbiamo dunque trovare l’area sottesa alla normale \\(\\mathcal{N}(120, 7.79)\\) nell’intervallo \\([-\\infty, 116]\\). Utilizzando R, la soluzione si trova nel modo seguente:\n\npnorm(116, 120, 7.79)\n\n0.303808211691303\n\n\nSe la variabile aleatoria che corrisponde al punteggio osservato segue una distribuzione \\(\\mathcal{N}(120, 7.79)\\), la probabilità che il rispondente ottenga un punteggio minore o uguale a 116 è dunque uguale a 0.30.\nEsempio 3. Sempre per l’esempio discusso, poniamoci ora la seguente domanda: quale intervallo di valori centrato sul punteggio vero contiene, con una probabilità di 0.95, i punteggi che il rispondente otterrebbe in ipotetiche somministrazioni ripetute del test sotto le stesse identiche condizioni?\nDobbiamo trovare i quantili della distribuzione \\(\\mathcal{N}(120, 7.79)\\) a cui sono associate le probabilità di 0.025 e 0.975. La soluzione è data da:\n\nqnorm(c(.025, .975), 120, 7.79)\n\n\n104.731880560433135.268119439567\n\n\nL’intervallo cercato è dunque \\([104.7, 135.3]\\).\nEsempio 4. Calcoliamo ora l’errore standard di misurazione utilizzando un campione di dati grezzi. Esamineremo un set di dati discusso da Brown (2015). Il set di dati grezzi contiene 9 indicatori utilizzati per misurare la depressione maggiore così come è definita nel DSM-IV:\n\nMDD1: depressed mood;\nMDD2: loss of interest in usual activities;\nMDD3: weight/appetite change;\nMDD4: sleep disturbance;\nMDD5: psychomotor agitation/retardation;\nMDD6: fatigue/loss of energy;\nMDD7: feelings of worthlessness/guilt;\nMDD8: concentration difficulties;\nMDD9: thoughts of death/suicidality.\n\nImportiamo i dati:\n\ndf &lt;- readRDS(\n    here::here(\"data\", \"mdd_sex.RDS\")\n) |&gt;\n    dplyr::select(-sex)\n\nCi sono 750 osservazioni:\n\ndim(df) |&gt; print()\n\n[1] 750   9\n\n\n\nhead(df)\n\n\nA data.frame: 6 x 9\n\n\n\nmdd1\nmdd2\nmdd3\nmdd4\nmdd5\nmdd6\nmdd7\nmdd8\nmdd9\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n5\n4\n1\n6\n5\n6\n5\n4\n2\n\n\n2\n5\n5\n5\n5\n4\n5\n4\n5\n4\n\n\n3\n4\n5\n4\n2\n6\n6\n0\n0\n0\n\n\n4\n5\n5\n3\n3\n5\n5\n6\n4\n0\n\n\n5\n5\n5\n0\n5\n0\n4\n6\n0\n0\n\n\n6\n6\n6\n4\n6\n4\n6\n5\n6\n2\n\n\n\n\n\nCalcoliamo il coefficiente di attendibilità \\(\\alpha\\) di Cronbach con la funzione alpha() del pacchetto psych.\n\nres &lt;- psych::alpha(df)\nalpha &lt;- res$total$raw_alpha\nalpha\n\n0.753150463775787\n\n\nCalcoliamo un vettore che contiene il punteggio totale del test per ciascun individuo:\n\ntotal_score &lt;- rowSums(df)\n\nTroviamo l’errore standard di misurazione:\n\nsd(total_score) * sqrt(1 - alpha)\n\n5.29643177867088\n\n\nConfrontiamo il risultato con quello ottenuto con la funzione SE.Meas():\n\nSE.Meas(sd(total_score), alpha)\n\n5.29643177867088",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>L'errore standard della misurazione</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/04_err_std_mis.html#dimostrazione",
    "href": "chapters/ctt/04_err_std_mis.html#dimostrazione",
    "title": "11  L’errore standard della misurazione",
    "section": "11.5 Dimostrazione",
    "text": "11.5 Dimostrazione\nEsaminiamo ora la derivazione della formula per l’errore standard di misurazione, \\(\\sigma_E = \\sigma_X \\sqrt{1 - \\rho_{XX^\\prime}}\\). Per arrivare a questa formula, seguiremo due passaggi chiave: innanzitutto, calcoleremo la varianza del punteggio vero e successivamente rappresenteremo il punteggio osservato come la somma della varianza del punteggio vero e la varianza dell’errore.\nIniziamo definendo il coefficiente di attendibilità come \\(\\rho_{XX^\\prime} = \\frac{\\sigma^2_T}{\\sigma^2_X}\\), in cui \\(\\sigma^2_T\\) è la varianza del punteggio vero e \\(\\sigma^2_X\\) è la varianza del punteggio osservato. Utilizzando questa definizione, possiamo riscrivere \\(\\sigma^2_T\\) come \\(\\sigma^2_T = \\rho_{XX^\\prime} \\sigma^2_X\\), considerando che \\(X\\) e \\(X^\\prime\\) sono forme parallele di un test.\nDato che \\(\\sigma_X = \\sigma_{X^\\prime}\\), possiamo scrivere l’equazione precedente come \\(\\sigma^2_T = \\rho_{XX^\\prime} \\sigma_X \\sigma_{X^\\prime}\\). Inoltre, la covarianza tra \\(X\\) e \\(X^\\prime\\) è definita come \\(\\sigma_{XX^\\prime} = \\rho_{XX^\\prime} \\sigma_X \\sigma_{X^\\prime}\\). Da qui, possiamo affermare che \\(\\sigma^2_T = \\sigma_{XX^\\prime}\\), ovvero che la varianza del punteggio vero equivale alla covarianza tra due misurazioni parallele.\nOra, passiamo a calcolare la varianza dell’errore, \\(\\sigma^2_E\\). La varianza del punteggio osservato è espressa come \\(\\sigma^2_X = \\sigma^2_T + \\sigma^2_E\\). Utilizzando la definizione di attendibilità, possiamo riscrivere questa equazione come \\(\\sigma^2_X = \\rho_{XX^\\prime} \\sigma^2_X + \\sigma^2_E\\), da cui otteniamo:\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\sigma^2_E &= \\sigma^2_X - \\sigma^2_X \\rho_{XX^\\prime} \\\\\n&= \\sigma^2_X (1 - \\rho_{XX^\\prime}).\n\\end{aligned}\n\\end{equation}\n\\]\nDi conseguenza, la varianza dell’errore di misurazione, \\(\\sigma^2_E\\), può essere espressa come il prodotto di due fattori: il primo rappresenta la varianza del punteggio osservato, mentre il secondo equivale a uno meno la correlazione tra le due forme parallele del test (\\(\\rho_{XX^\\prime}\\)). In conclusione, abbiamo calcolato l’incognita \\(\\sigma^2_E\\) in termini di due quantità osservabili, \\(\\sigma^2_X\\) e \\(\\rho_{XX^\\prime}\\).",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>L'errore standard della misurazione</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/04_err_std_mis.html#relazione-tra-affidabilità-e-sem",
    "href": "chapters/ctt/04_err_std_mis.html#relazione-tra-affidabilità-e-sem",
    "title": "11  L’errore standard della misurazione",
    "section": "11.6 Relazione tra Affidabilità e SEM",
    "text": "11.6 Relazione tra Affidabilità e SEM\nSi osserva che, all’aumentare dell’affidabilità di un test, l’Errore Standard di Misurazione (SEM) diminuisce. Questa relazione inversa è coerente con il fatto che il coefficiente di affidabilità riflette la proporzione della varianza dei punteggi osservati dovuta alla varianza dei punteggi veri, e il SEM è una stima dell’errore presente nei punteggi del test. Quindi, maggiore è l’affidabilità dei punteggi di un test, minore è il SEM, e maggior fiducia possiamo avere nella precisione dei punteggi del test. Viceversa, minore è l’affidabilità di un test, maggiore è il SEM, e minore è la nostra fiducia nella precisione dei punteggi del test.\nPer esempio, con un coefficiente di affidabilità perfetto pari a 1.0, il SEM sarebbe uguale a 0, indicando l’assenza di errore nella misurazione e che il punteggio ottenuto rappresenta il punteggio vero. Un coefficiente di affidabilità pari a 0, invece, produrrebbe un SEM uguale alla deviazione standard (SD) dei punteggi ottenuti, indicando che tutta la varianza dei punteggi del test è dovuta a errori.\nIl SEM è tradizionalmente utilizzato nel calcolo di intervalli o bande intorno ai punteggi osservati, all’interno dei quali ci si aspetta che cada il punteggio vero. Ora passeremo a questa applicazione del SEM.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>L'errore standard della misurazione</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/04_err_std_mis.html#intervallo-di-confidenza-e-errore-standard-di-misurazione-sem",
    "href": "chapters/ctt/04_err_std_mis.html#intervallo-di-confidenza-e-errore-standard-di-misurazione-sem",
    "title": "11  L’errore standard della misurazione",
    "section": "11.7 Intervallo di Confidenza e Errore Standard di Misurazione (SEM)",
    "text": "11.7 Intervallo di Confidenza e Errore Standard di Misurazione (SEM)\nL’intervallo di confidenza rappresenta un range di punteggi che include il vero punteggio di un individuo con una probabilità prescritta. Generalmente, utilizziamo il SEM per calcolare gli intervalli di confidenza. Il SEM fornisce informazioni sulla distribuzione dei punteggi osservati intorno ai punteggi veri.\nAd esempio, se un individuo ha un punteggio vero di 70 in un test con un SEM di 3, ci aspetteremmo che ottenga punteggi tra 67 e 73 due terzi delle volte, a patto che non ci siano cambiamenti nelle prestazioni a causa della ripetizione del test.\n\n# Definiamo il punteggio vero e il SEM\npunteggio_vero &lt;- 70\nSEM &lt;- 3\n\npnorm(73, 70, 3) - pnorm(67, 70, 3)\n\n0.682689492137086\n\n\nPer ottenere un intervallo di confidenza del 95%, determiniamo il numero di deviazioni standard che comprendono il 95% dei punteggi in una distribuzione. Con un punteggio vero di 70 e un SEM di 3, l’intervallo di confidenza del 95% sarebbe 70 ± 3(1.96), ovvero 70 ± 5.88. Quindi, in questa situazione, ci aspetteremmo che il punteggio osservato dell’individuo sia tra 64.12 e 75.88, il 95% delle volte.\n\n# Calcoliamo il valore critico Z per il livello di confidenza del 95%\nlivello_confidenza &lt;- 0.95\nz_critico &lt;- qnorm((1 + livello_confidenza) / 2)\n\n# Calcoliamo l'errore standard dell'intervallo\nerrore_standard_intervallo &lt;- SEM * z_critico\n\n# Calcoliamo l'intervallo di confidenza\nintervallo_confidenza_inf &lt;- punteggio_vero - errore_standard_intervallo\nintervallo_confidenza_sup &lt;- punteggio_vero + errore_standard_intervallo\n\n# Stampiamo l'intervallo di confidenza\ncat(\"L'intervallo di confidenza al 95% e' [\", intervallo_confidenza_inf, \", \", intervallo_confidenza_sup, \"]\\n\")\n\nL'intervallo di confidenza al 95% e' [ 64.12011 ,  75.87989 ]\n\n\n\n11.7.1 Relazione tra Affidabilità, SEM e Intervalli di Confidenza\nÈ utile notare la relazione tra l’affidabilità di un punteggio di test, il SEM e gli intervalli di confidenza. Ricordiamo che all’aumentare dell’affidabilità dei punteggi, il SEM diminuisce. La stessa relazione esiste tra l’affidabilità dei punteggi di test e gli intervalli di confidenza. Man mano che l’affidabilità dei punteggi di test aumenta (denotando meno errore di misurazione), gli intervalli di confidenza diventano più piccoli (denotando maggiore precisione nella misurazione).\n\n\n11.7.2 Vantaggio del SEM e dell’Uso degli Intervalli di Confidenza\nIl SEM e l’utilizzo degli intervalli di confidenza forniscono ci ricordano che l’errore di misurazione è un elemento intrinseco a tutti i punteggi e che dovremmo interpretare tali punteggi con cautela. Troppo spesso, si tende a interpretare un singolo punteggio numerico come se fosse assolutamente preciso, trascurando la presenza di errori associati.\nPer esempio, se si riporta che Alice ha un QI totale di 113, i suoi genitori potrebbero essere inclini a interpretare questo dato come un’indicazione precisa del QI di Alice, assumendo che sia esattamente 113. Tuttavia, anche quando si utilizzano test di alta qualità per misurare il QI, i punteggi ottenuti non sono privi di errore. Il SEM e gli intervalli di confidenza sono strumenti utili che ci consentono di quantificare e illustrare questa inevitabile incertezza associata ai punteggi di misurazione. Essi ci avvertono che ogni punteggio contiene una certa dose di errore e ci invitano a considerare i risultati con una visione più prudente e completa.\n\n\n11.7.3 Problema nel Calcolare l’Intervallo di Confidenza\nUn problema potenziale con l’approccio descritto sopra è che non conosciamo il vero punteggio dell’esaminato, ma solo il punteggio osservato. È comune usare il SEM per stabilire intervalli di confidenza intorno ai punteggi ottenuti. Tuttavia, è importante sottolineare che questa pratica non è corretta Charter (1996).\n\nIn spite of Dudek (1979)’s reminder that the SEM should not be used to construct confidence intervals, many test manuals, computer-scoring programs, and texts in psychology and education continue to do so. Because authors of many textbooks and manuals make these errors, it is understandable that those who learned from and look to these sources for guidance also make these errors. In summary, the SEM should not be used to construct confidence intervals for test scores (p. 1141).\n\nÈ invece possibile costruire gli intervalli di confidenza basati su punteggi veri stimati e sull’errore standard della stima (SEE). Questo approccio verrà descritto nel prossimo capitolo.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>L'errore standard della misurazione</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/04_err_std_mis.html#riflessioni-conclusive",
    "href": "chapters/ctt/04_err_std_mis.html#riflessioni-conclusive",
    "title": "11  L’errore standard della misurazione",
    "section": "11.8 Riflessioni Conclusive",
    "text": "11.8 Riflessioni Conclusive\nNel contesto della CTT, le stime di affidabilità si rivelano uno strumento fondamentale per valutare la coerenza dei test. Tuttavia, quando si affrontano decisioni relative al singolo individuo, come ad esempio determinare se un candidato supera un esame, diventa più vantaggioso fare riferimento all’errore standard di misurazione (SEM). Il SEM rende evidente quanto i punteggi di un test siano suscettibili di fluttuazioni casuali se lo stesso test venisse ripetuto più volte dallo stesso esaminando. In generale, un SEM più ridotto corrisponde a un intervallo di fluttuazioni casuali più stretto. Ciò implica che, grazie a un SEM più basso, i punteggi rifletteranno in modo più coerente le vere capacità dell’esaminando.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>L'errore standard della misurazione</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/04_err_std_mis.html#session-info",
    "href": "chapters/ctt/04_err_std_mis.html#session-info",
    "title": "11  L’errore standard della misurazione",
    "section": "11.9 Session Info",
    "text": "11.9 Session Info\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] psychometric_2.4  multilevel_2.7    nlme_3.1-166      MASS_7.3-61      \n [5] viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1   \n [9] gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6   \n[13] semPlot_1.1.6     lavaan_0.6-19     psych_2.4.6.26    scales_1.3.0     \n[17] markdown_1.13     knitr_1.49        lubridate_1.9.3   forcats_1.0.0    \n[21] stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2       readr_2.1.5      \n[25] tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0  \n[29] here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.17.1   jsonlite_1.8.9      magrittr_2.0.3     \n  [4] TH.data_1.1-2       estimability_1.5.1  farver_2.1.2       \n  [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n [19] emmeans_1.10.5      zoo_1.8-12          uuid_1.2-1         \n [22] igraph_2.1.1        mime_0.12           lifecycle_1.0.4    \n [25] pkgconfig_2.0.3     Matrix_1.7-1        R6_2.5.1           \n [28] fastmap_1.2.0       shiny_1.9.1         digest_0.6.37      \n [31] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n [34] rprojroot_2.0.4     Hmisc_5.2-0         fansi_1.0.6        \n [37] timechange_0.3.0    abind_1.4-8         compiler_4.4.2     \n [40] withr_3.0.2         glasso_1.11         htmlTable_2.4.3    \n [43] backports_1.5.0     carData_3.0-5       ggsignif_0.6.4     \n [46] corpcor_1.6.10      gtools_3.9.5        tools_4.4.2        \n [49] pbivnorm_0.6.0      foreign_0.8-87      zip_2.3.1          \n [52] httpuv_1.6.15       nnet_7.3-19         glue_1.8.0         \n [55] quadprog_1.5-8      promises_1.3.0      lisrelToR_0.3      \n [58] grid_4.4.2          pbdZMQ_0.3-13       checkmate_2.3.2    \n [61] cluster_2.1.6       reshape2_1.4.4      generics_0.1.3     \n [64] gtable_0.3.6        tzdb_0.4.0          data.table_1.16.2  \n [67] hms_1.1.3           car_3.1-3           utf8_1.2.4         \n [70] sem_3.1-16          pillar_1.9.0        IRdisplay_1.1      \n [73] rockchalk_1.8.157   later_1.3.2         splines_4.4.2      \n [76] cherryblossom_0.1.0 lattice_0.22-6      survival_3.7-0     \n [79] kutils_1.73         tidyselect_1.2.1    miniUI_0.1.1.1     \n [82] pbapply_1.7-2       airports_0.1.0      stats4_4.4.2       \n [85] xfun_0.49           qgraph_1.9.8        arm_1.14-4         \n [88] stringi_1.8.4       pacman_0.5.1        boot_1.3-31        \n [91] evaluate_1.0.1      codetools_0.2-20    mi_1.1             \n [94] cli_3.6.3           RcppParallel_5.1.9  IRkernel_1.3.2     \n [97] rpart_4.1.23        xtable_1.8-4        repr_1.1.7         \n[100] munsell_0.5.1       Rcpp_1.0.13-1       coda_0.19-4.1      \n[103] png_0.1-8           XML_3.99-0.17       parallel_4.4.2     \n[106] usdata_0.3.1        jpeg_0.1-10         lme4_1.1-35.5      \n[109] mvtnorm_1.3-2       openxlsx_4.2.7.1    crayon_1.5.3       \n[112] openintro_2.5.0     rlang_1.1.4         multcomp_1.4-26    \n[115] mnormt_2.1.1       \n\n\n\n\n\n\nBrown, T. A. (2015). Confirmatory factor analysis for applied research. Guilford publications.\n\n\nCharter, R. A. (1996). Revisiting the standard errors of measurement, estimate, and prediction and their application to test scores. Perceptual and Motor Skills, 82(3), 1139–1144.\n\n\nDudek, F. J. (1979). The continuing misinterpretation of the standard error of measurement. Psychological Bulletin, 86(2), 335--337.\n\n\nWechsler, D. (2008). Wechsler adult intelligence scale–Fourth Edition (WAIS–IV). San Antonio, TX: NCS Pearson, 22(498), 816–827.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>L'errore standard della misurazione</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/05_err_std_stima.html",
    "href": "chapters/ctt/05_err_std_stima.html",
    "title": "12  La stima del punteggio vero",
    "section": "",
    "text": "12.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nUno dei principali scopi della valutazione psicologica è stimare il punteggio vero del soggetto. Il punteggio osservato \\(X\\) differisce dal punteggio vero \\(T\\) a causa dell’errore di misurazione: \\(X = T + E\\). Ora poniamoci l’obiettivo di utilizzare i concetti della Teoria Classica per stimare il punteggio vero di un soggetto, utilizzando il suo punteggio osservato e l’affidabilità del test. Questa stima risulta particolarmente utile quando è necessario costruire un intervallo di confidenza per il punteggio vero del soggetto.\nPer costruire l’intervallo di confidenza del vero punteggio, sono necessarie due misurazioni:\nCominciamo affrontando il problema della stima del vero punteggio.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>La stima del punteggio vero</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/05_err_std_stima.html#introduzione",
    "href": "chapters/ctt/05_err_std_stima.html#introduzione",
    "title": "12  La stima del punteggio vero",
    "section": "",
    "text": "Una stima del vero punteggio.\nL’errore standard della stima (ossia, una stima della deviazione standard della distribuzione delle stime del punteggio vero che si otterrebbero se il test venisse somministrato infinite volte nelle stesse condizioni).",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>La stima del punteggio vero</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/05_err_std_stima.html#il-paradosso-di-kelley",
    "href": "chapters/ctt/05_err_std_stima.html#il-paradosso-di-kelley",
    "title": "12  La stima del punteggio vero",
    "section": "12.2 Il paradosso di Kelley",
    "text": "12.2 Il paradosso di Kelley\nNegli anni ’20, Kelly ha dimostrato come sia possibile stimare il punteggio vero del rispondente utilizzando un modello di regressione. La formula di Kelley si fonda sull’equivalenza algebrica che lega l’attendibilità al quadrato del coefficiente di correlazione tra i punteggi osservati e quelli veri. Pertanto, la stima del punteggio vero di un rispondente può essere calcolata nel seguente modo:\n\\[\n\\begin{equation}\n\\hat{T} = \\mu_x + \\rho  (X - \\mu_x),\n\\end{equation}\n\\tag{12.1}\\]\ndove \\(X\\) rappresenta il punteggio osservato, \\(\\mu_x\\) è la media dei punteggi ottenuti da tutti i partecipanti nel campione, e \\(\\rho\\) è l’attendibilità del test.\nQuando l’attendibilità è perfetta (\\(\\rho = 1\\)), il punteggio vero coincide con il punteggio osservato. Nel caso di attendibilità nulla (dove tutta la varianza è attribuibile all’errore di misurazione), la stima più accurata del punteggio vero è semplicemente la media del campione. Per valori di \\(\\rho\\) compresi tra 0 e 1, la stima del punteggio vero si discosta dal punteggio osservato in direzione della media campionaria. In questo modo, la stima del punteggio vero illustra il concetto di regressione verso la media dei punteggi osservati, considerando l’attendibilità del test.\nLa formula del punteggio vero può essere interpretata nel modo seguente: per stimare il vero punteggio di un individuo, si parte dalla media della distribuzione di tutti i partecipanti e si procede in direzione del punteggio osservato. Tuttavia, il punteggio osservato non viene raggiunto completamente; l’entità dello spostamento è proporzionale all’attendibilità del test. Ciò implica che la stima del punteggio vero di un individuo, a seconda del valore di \\(\\rho\\), tiene conto anche della sua posizione rispetto alla media del gruppo. Se il soggetto si trova al di sotto della media, la stima del punteggio vero sarà maggiorata e viceversa. Questo fenomeno è noto come il “paradosso di Kelley”.\nÈ cruciale mettere in evidenza una discrepanza tra la formula di Kelley e l’intuizione comune che suggerisce il punteggio osservato come una stima accurata del vero punteggio (cioè \\(\\hat{T} = X\\)). Tuttavia, questo punto di vista è valido solo quando la misura è perfettamente attendibile (\\(\\rho = 1\\)). Al contrario, quando \\(\\rho = 0\\), la formula di Kelley suggerisce di utilizzare la media dei punteggi osservati come stima del vero punteggio, implicando che il punteggio osservato non rifletta necessariamente il vero punteggio, ma solo l’errore di misurazione.\nIn pratica, è estremamente improbabile che \\(\\rho\\) sia esattamente uguale a zero. Invece, con valori di \\(\\rho\\) compresi tra 0 e 1, la stima del punteggio vero si troverà in una posizione intermedia tra il punteggio osservato e la media della popolazione. Per una comprensione più dettagliata di questo concetto, possiamo fare riferimento a Kelley (1947), il quale ha osservato che:\n\nThis is an interesting equation in that it expresses the estimate of true ability as the weighted sum of two separate estimates, – one based upon the individual’s observed score, \\(X_1\\) (\\(X\\) nella notazione corrente) and the other based upon the mean of the group to which he belongs, \\(M_1\\) (\\(\\mu_x\\) nella notazione corrente). If the test is highly reliable, much weight is given to the test score and little to the group mean, and vice versa.\n\nPer chiarire l’Equazione 12.1 e la sua derivazione in termini di predizione del punteggio vero a partire dal punteggio osservato, iniziamo esaminando il modello di base di regressione lineare semplice. Questo modello stabilisce una relazione diretta tra il punteggio osservato \\(X\\) e il punteggio vero \\(T\\), una relazione che inizialmente abbiamo descritto con la formula \\(X = 0 + 1 \\cdot T + E\\). Il nostro interesse specifico qui, però, si sposta verso la predizione del punteggio vero \\(T\\) utilizzando il punteggio osservato \\(X\\), attraverso un modello di regressione. La formula per questa predizione assume la forma:\n\\[\nT = \\alpha + \\beta X + \\varepsilon.\n\\]\nRiorganizzando le variabili in termini di deviazioni dalla loro media (\\(x = X - \\bar{X}\\) e \\(\\tau = T - \\mathbb{E}(T)\\)), e considerando l’intercetta \\(\\alpha\\) come 0, il modello si semplifica in \\(\\hat{\\tau} = \\beta x\\), dove \\(\\hat{\\tau}\\) rappresenta la nostra stima del punteggio vero come deviazione dalla media. La questione centrale diventa quindi il calcolo di \\(\\beta\\), che è la pendenza della retta di regressione nel nostro modello semplificato.\nIl valore di \\(\\beta\\) viene definito come \\(\\beta = \\frac{\\sigma_{\\tau x}}{\\sigma^2_x}\\), che ci permette di esprimere il modello come:\n\\[\n\\hat{\\tau} = \\frac{\\sigma_{\\tau x}}{\\sigma^2_x} x.\n\\]\nIntroducendo la correlazione tra \\(x\\) (o \\(X\\)) e \\(\\tau\\) (o \\(T\\)), denotata \\(\\rho_{\\tau x}\\), e sostituendo la covarianza con il prodotto della correlazione per le deviazioni standard dei punteggi, riscriviamo l’equazione come:\n\\[\n\\hat{\\tau} = \\rho_{\\tau x}\\frac{\\sigma_{\\tau}}{\\sigma_x} x.\n\\]\nQuesto ci porta a considerare la definizione di attendibilità, secondo cui la varianza del punteggio vero può essere espressa come \\(\\sigma^2_{\\tau} = \\sigma^2_x \\rho_{xx^\\prime}\\). La stima del punteggio vero, in termini di deviazioni dalla media, diventa quindi una funzione del coefficiente di attendibilità e della deviazione del punteggio osservato dalla sua media:\n\\[\n\\hat{\\tau} = \\rho_{\\tau x} \\sqrt{\\rho_{xx^\\prime}} x.\n\\]\nAvendo dimostrato che \\(\\rho^2_{\\tau x} = \\rho_{xx^\\prime}\\), possiamo ulteriormente semplificare la nostra stima del punteggio vero come:\n\\[\n\\hat{\\tau} = \\rho_{xx^\\prime} x.\n\\]\nQuesto ci indica che la stima del punteggio vero (in termini di deviazioni dalla media) si ottiene moltiplicando il punteggio osservato, anch’esso espresso come deviazione dalla media, per il coefficiente di attendibilità.\nPer ritornare alla formula in termini di punteggi grezzi, aggiungiamo la media dei punteggi osservati \\(\\bar{X}\\) alla nostra equazione, ottenendo così la stima del punteggio vero grezzo \\(\\hat{T}\\):\n\\[\n\\hat{T} = \\rho_{XX^\\prime} (X - \\bar{X}) + \\bar{X}.\n\\]\nEspandendo e riorganizzando l’equazione, arriviamo a una forma che chiarisce la relazione tra la media dei punteggi osservati, il coefficiente di attendibilità, e il punteggio osservato grezzo:\n\\[\n\\hat{T} = \\bar{X} + \\rho_{XX^\\prime} (X - \\bar{X}).\n\\]\nQuesta equazione finale dimostra come la stima del punteggio vero grezzo possa essere calcolata regolando il punteggio osservato per la media dei punteggi e il coefficiente di attendibilità.\nNel contesto di dati campionari, dove il coefficiente di attendibilità popolazionale \\(\\rho_{XX^\\prime}\\) viene sostituito con il suo corrispettivo campionario \\(r_{XX^\\prime}\\), la formula diventa:\n\\[\n\\hat{T} = \\bar{X} + r_{XX^\\prime} (X - \\bar{X}),\n\\]\noffrendo un metodo pratico per stimare il punteggio vero di un individuo a partire dal suo punteggio osservato, con l’aggiunta della media dei punteggi osservati e il coefficiente di attendibilità campionario.\nEsercizio. Posto un coefficiente di attendibilità pari a 0.80 e una media del test pari a \\(\\bar{X} = 100\\), si trovi una stima del punteggio vero per un rispondente con un punteggio osservato uguale a \\(X\\) = 115.\nLa stima del punteggio vero \\(\\hat{T}\\) è uguale a\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{T} &= \\bar{X} + r_{XX^\\prime}  (X - \\bar{X})\\notag\\\\\n&= 100 + 0.80 \\cdot (115 - 100) = 112.\n\\end{aligned}\n\\end{equation}\n\\]\nIn alternativa, possiamo usare la funzione Est.true del pacchetto psychometric.\n\nEst.true(115, 100, .8)\n\n112",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>La stima del punteggio vero</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/05_err_std_stima.html#lerrore-standard-della-stima",
    "href": "chapters/ctt/05_err_std_stima.html#lerrore-standard-della-stima",
    "title": "12  La stima del punteggio vero",
    "section": "12.3 L’Errore Standard della Stima",
    "text": "12.3 L’Errore Standard della Stima\nNell’ambito del modello di regressione di Kelley, uno strumento fondamentale per valutare la precisione delle stime del punteggio vero ottenute dai punteggi osservati è l’errore standard della stima. Questo indice quantifica quanto le nostre stime del punteggio vero possano variare se ripetessimo il test più volte sotto le stesse condizioni. Denotiamo l’errore standard della stima con \\(\\sigma_{\\hat{T}}\\), dove \\(\\hat{T}\\) rappresenta la stima del valore vero.\nL’errore standard della stima è cruciale per comprendere quanto sia affidabile la stima del punteggio vero. Un errore standard più piccolo indica una stima più precisa. Matematicamente, l’errore standard della stima si calcola come:\n\\[\n\\sigma_{\\hat{T}} = \\sigma_X \\sqrt{\\rho_{XX^\\prime} (1 -\\rho_{XX^\\prime})},\n\\tag{12.2}\\]\ndove \\(\\sigma_X\\) è la deviazione standard dei punteggi osservati, e \\(\\rho_{XX^\\prime}\\) rappresenta il coefficiente di correlazione tra i punteggi osservati e i punteggi veri. Questa formula assume una distribuzione normale dei punteggi e una relazione lineare tra i punteggi osservati e i punteggi veri.\nPer dati campionari, utilizziamo una formula leggermente modificata per calcolare l’errore standard della stima:\n\\[\ns_{\\hat{T}} = s_X \\sqrt{r_{XX^\\prime} (1-r_{XX^\\prime})},\n\\]\nqui \\(s_X\\) indica la deviazione standard campionaria, e \\(r_{XX^\\prime}\\) è il coefficiente di affidabilità campionario.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>La stima del punteggio vero</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/05_err_std_stima.html#derivazione-dellerrore-standard-della-stima",
    "href": "chapters/ctt/05_err_std_stima.html#derivazione-dellerrore-standard-della-stima",
    "title": "12  La stima del punteggio vero",
    "section": "12.4 Derivazione dell’Errore Standard della Stima",
    "text": "12.4 Derivazione dell’Errore Standard della Stima\nPer derivare l’Equazione 12.2, iniziamo con la definizione dell’errore \\(\\varepsilon\\), che rappresenta la discrepanza tra il punteggio reale \\(T\\) e il punteggio stimato \\(\\hat{T}\\), come illustrato nella formula:\n\\[\n\\varepsilon = T - \\hat{T}.\n\\]\nSi sottolinea la distinzione tra l’errore di misurazione, indicato con \\(E = X - T\\) (dove \\(E\\) quantifica l’errore di misurazione, ovvero la differenza tra il punteggio osservato \\(X\\) e il punteggio reale \\(T\\)), e l’errore \\(\\varepsilon = T - \\hat{T}\\) (che esprime la discrepanza tra il punteggio reale \\(T\\) e la sua stima \\(\\hat{T}\\)).\nAdottando la formula \\(\\hat{T} = \\bar{X} + \\rho_{XX^\\prime} (X - \\bar{X})\\) per esprimere \\(\\hat{T}\\), si può calcolare la varianza di \\(\\varepsilon\\) come segue:\n\\[\n\\begin{aligned}\n\\mathbb{V}(\\varepsilon) &=  \\mathbb{V}(T - \\hat{T}) \\\\\n&= \\mathbb{V}(T - \\bar{X} - \\rho_{XX^\\prime} X + \\rho_{XX^\\prime}\\bar{X}).\n\\end{aligned}\n\\]\nDato che l’aggiunta di una costante non altera la varianza di una variabile aleatoria, possiamo semplificare l’espressione a:\n\\[\n\\mathbb{V}(\\varepsilon) = \\mathbb{V}(T - \\rho_{XX^\\prime}X).\n\\]\nSfruttando la regola della varianza per la somma di variabili aleatorie, incluso il caso in cui una variabile è moltiplicata per una costante, arriviamo a:\n\\[\n\\begin{aligned}\n\\mathbb{V}(\\varepsilon) &= \\mathbb{V}(T) + \\rho_{XX^\\prime}^2 \\mathbb{V}(X) - 2 \\rho_{XX^\\prime} \\mbox{Cov}(X,T) \\\\\n&= \\sigma^2_T + \\rho_{XX^\\prime}^2 \\sigma^2_X - 2 \\rho_{XX^\\prime} \\sigma_{XT}.\n\\end{aligned}\n\\]\nCon \\(\\sigma_{XT} = \\sigma^2_T\\), possiamo ridurre ulteriormente l’espressione a:\n\\[\n\\sigma^2_{\\varepsilon} = \\sigma^2_T + \\left(\\frac{\\sigma_T^2}{\\sigma_X^2}\\right)^2 \\sigma^2_X - 2 \\frac{\\sigma_T^2}{\\sigma_X^2} \\sigma_{XT}.\n\\]\nSemplificando, otteniamo:\n\\[\n\\begin{aligned}\n\\sigma^2_{\\varepsilon} &= \\sigma^2_T + \\frac{\\sigma_T^4}{\\sigma_X^4} \\sigma^2_X - 2 \\frac{\\sigma_T^2}{\\sigma_X^2} \\sigma_{XT} \\\\\n&= \\sigma^2_T \\left(1 + \\frac{\\sigma_T^2}{\\sigma_X^2} - 2 \\frac{\\sigma_{XT}}{\\sigma_X^2}\\right).\n\\end{aligned}\n\\]\nInfine, con \\(\\sigma_{XT} = \\sigma^2_T\\), semplifichiamo a:\n\\[\n\\begin{aligned}\n\\sigma^2_{\\varepsilon} &= \\sigma^2_T \\left(1 - \\frac{\\sigma_{T}^2}{\\sigma_X^2}\\right).\n\\end{aligned}\n\\]\nQuesto ci porta alla formula dell’errore standard della stima \\(\\sigma_{\\varepsilon}\\):\n\\[\n\\begin{aligned}\n\\sigma_{\\varepsilon} &= \\sigma_T \\sqrt{1 - \\frac{\\sigma^2_T}{\\sigma^2_X}} \\\\\n&= \\sigma_T \\sqrt{\\frac{\\sigma^2_X - \\sigma^2_T}{\\sigma^2_X}} \\\\\n&= \\frac{\\sigma_T}{\\sigma_X} \\sqrt{\\sigma^2_X - \\sigma^2_T}.\n\\end{aligned}\n\\]\nConsiderando che \\(\\sigma^2_X = \\sigma^2_T + \\sigma^2_E\\), l’errore standard di stima diventa:\n\\[\n\\begin{aligned}\n\\sigma_{\\varepsilon} &= \\frac{\\sigma_T}{\\sigma_X} \\sqrt{\\sigma^2_E } \\\\\n&= \\frac{\\sigma_T}{\\sigma_X} \\sigma_E \\\\\n&= \\sqrt{\\rho_{XX^\\prime}} \\sigma_E.\n\\end{aligned}\n\\]\nDato che l’errore standard di misurazione è definito come \\(\\sigma_E = \\sigma_X \\sqrt{1 - \\rho_{XX^\\prime}}\\), possiamo concludere che:\n\\[\n\\begin{aligned}\n\\sigma_{\\varepsilon} &= \\sqrt{\\rho_{XX^\\prime}} \\sigma_E \\\\\n&= \\sqrt{\\rho_{XX^\\prime}} \\sigma_X \\sqrt{1-\\rho_{XX^\\prime}} \\\\\n&= \\sigma_X \\sqrt{\\rho_{XX^\\prime} (1 - \\rho_{XX^\\prime})}.\n\\end{aligned}\n\\]\nQuest’ultima espressione dimostra come l’errore standard della stima sia determinato dalla deviazione standard dei punteggi osservati, modulata dal coefficiente di correlazione tra i punteggi osservati e i punteggi veri.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>La stima del punteggio vero</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/05_err_std_stima.html#intervallo-di-confidenza-per-il-punteggio-vero",
    "href": "chapters/ctt/05_err_std_stima.html#intervallo-di-confidenza-per-il-punteggio-vero",
    "title": "12  La stima del punteggio vero",
    "section": "12.5 Intervallo di confidenza per il punteggio vero",
    "text": "12.5 Intervallo di confidenza per il punteggio vero\nSiamo ora finalmente nelle condizioni di potere calcolare l’intervallo di confidenza per il punteggio vero. Conoscendo l’errore standard della stima \\(\\sigma_{\\hat{T}}\\), l’intervallo di confidenza per il punteggio vero è dato da:\n\\[\n\\hat{T} \\pm z  \\sigma_{\\hat{T}},\n\\]\nladdove \\(\\hat{T}\\) è la stima del punteggio vero e \\(z\\) è il quantile della normale standardizzata al livello di probabilità desiderato. Se il campione è piccolo (minore di 30) è opportuno usare \\(t\\) anziché \\(z\\).\n\n12.5.1 Interpretazione\nNotiamo che l’intervallo \\(\\hat{T} \\pm z \\sigma_{\\hat{T}}\\) è centrato sulla stima puntuale del valore vero e la sua ampiezza dipende sia dal livello di confidenza desiderato (rappresentato dal quantile \\(z_{\\frac{\\alpha}{2}}\\)), sia dal grado di precisione dello stimatore, misurato dall’errore standard della stima, \\(\\sigma_{\\hat{T}} = \\sigma_X \\sqrt{\\rho_{XX^\\prime} (1 -\\rho_{XX^\\prime})}\\). È importante notare che l’errore standard della stima diventa sempre più grande man mano che diminuisce l’attendibilità \\(\\rho_{XX^\\prime}\\) del test.\nL’intervallo di confidenza indica quanto l’imprecisione della misura influisce sull’interpretazione dei dati. Più l’intervallo di confidenza è ampio, maggiore è l’incertezza nella valutazione dei risultati.\nEsercizio. Charter (1996) ha esaminato l’effetto della variazione dell’attendibilità del test sull’ampiezza dell’intervallo di confidenza per il punteggio vero. Utilizzando come esempio i punteggi di QI (\\(\\mu\\) = 100, \\(\\sigma\\) = 15), Charter ha immaginato di variare il coefficiente di attendibilità del test utilizzato per la misurazione del QI. I valori presi in considerazione sono 0.55, 0.65, 0.75, 0.85 e 0.95. Ad esempio, supponiamo di avere un punteggio osservato pari a QI = 120 e un coefficiente di attendibilità del test \\(\\rho_{xx^\\prime}\\) pari a 0.65. In tali circostanze, la stima del punteggio vero è pari a\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{T} &= \\bar{X} + r_{XX^\\prime}  (X - \\bar{X}) \\notag\\\\\n&= 100 + 0.65 (120 - 100)\\notag\\\\\n&= 113.\\notag\n\\end{aligned}\n\\end{equation}\n\\]\nL’errore standard della stima è uguale a\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\sigma_{\\hat{T}} &= \\sigma_{X} \\sqrt{r_{XX^\\prime} (1 - r_{XX^\\prime})} \\notag\\\\\n&= 15 \\sqrt{0.65 (1 - 0.65)}\\notag\\\\\n&= 7.15.\\notag\n\\end{aligned}\n\\end{equation}\n\\]\nL’intervallo di confidenza al 95% per la stima del punteggio vero diventa pertanto uguale a\n\\[\n113 \\pm 1.96 \\cdot 7.15 = [98.98, 127.02].\n\\]\nSi noti che si può calcolare l’errore standard della stima con la funzione SE.Est() del pacchetto psychometric.\n\nSE.Est(15, .65)\n\n7.15454401062709\n\n\n\nInoltre, la funzione CI.tscore() restituisce sia la stima del punteggio vero sia l’intervallo di fiducia al livello desiderato di significatività.\n\nCI.tscore(120, 100, 15, 0.65, level = 0.95)\n\n\nA data.frame: 1 x 4\n\n\nSE.Est\nLCL\nT.Score\nUCL\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n7.154544\n98.97735\n113\n127.0226",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>La stima del punteggio vero</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/05_err_std_stima.html#cut-off",
    "href": "chapters/ctt/05_err_std_stima.html#cut-off",
    "title": "12  La stima del punteggio vero",
    "section": "12.6 Cut-off",
    "text": "12.6 Cut-off\nGli intervalli di confidenza per il punteggio vero possono essere utilizzati per confrontare i limiti dell’intervallo con un cut-off. Ci sono tre possibili esiti: il limite inferiore dell’intervallo di confidenza è maggiore del cut-off, il limite superiore dell’intervallo è minore del cut-off, o il valore del cut-off è compreso all’interno dell’intervallo. Nel primo caso, lo psicologo può affermare, con un grado di certezza \\(1-\\alpha\\), che il valore vero del rispondente è superiore al cut-off. Nel secondo caso, lo psicologo può affermare, con un grado di certezza \\(1-\\alpha\\), che il valore vero del rispondente è inferiore al cut-off. Nel terzo caso, lo psicologo non può concludere né che il valore vero sia inferiore né che sia superiore al cut-off, con un certo grado di incertezza.\n\nSi considerino i punteggi del QI, per cui \\(\\bar{X}\\) = 100 e \\(s_X\\) = 15. Sia l’attendibilità del test \\(\\rho_{XX^\\prime}\\) = 0.95. Supponiamo che il rispondente abbia un QI = 130. Poniamo che il cut-off per ammettere il rispondente ad un corso avanzato sia 120. Ci sono tre alternative: il valore vero del rispondente è sicuramente maggiore di 120; il valore vero del rispondente è sicuramente inferiore di 120; le evidenze disponibili ci lasciano in dubbio se il punteggio vero sia maggiore o minore di 120. Svolgiamo i calcoli per trovare l’intervallo di confidenza al livello di certezza del 95%:\n\nxm &lt;- 100\nsx &lt;- 15\nrho &lt;- .95\nx &lt;- 130\nt.hat &lt;- xm + rho * (x - xm)\nt.hat\nse.t &lt;- sx * sqrt(rho * (1 - rho))\nse.t\nt.hat + c(1, -1) * qnorm(.025, 0, 1) * se.t\n\n128.5\n\n\n3.26917420765551\n\n\n\n122.092536293808134.907463706192\n\n\nDato che il limite inferiore dell’intervallo di confidenza è maggiore del cut-off, lo psicologo conclude che il punteggio vero del rispondente è maggiore di 120. Quindi, raccomanda che il rispondente sia ammesso al corso avanzato.\nContinuando con l’esempio precedente, supponiamo che l’attendibilità del test abbia un valore simile a quello che solitamente si ottiene empiricamente, ovvero 0.80.\n\nxm &lt;- 100\nsx &lt;- 15\nrho &lt;- .8\nx &lt;- 130\nt.hat &lt;- xm + rho * (x - xm)\nt.hat\nse.t &lt;- sx * sqrt(rho * (1 - rho))\nse.t\nt.hat + c(1, -1) * qnorm(.025, 0, 1) * se.t\n\n124\n\n\n6\n\n\n\n112.24021609276135.75978390724\n\n\nIn questo secondo esempio, l’intervallo di confidenza al 95% è \\([112.24,\n135.76]\\) e contiene il valore del cut-off. Dunque, la decisione dello psicologo è che non vi sono evidenze sufficienti che il vero valore del rispondente sia superiore al cut-off. Si noti come la diminuzione dell’attendibilità del test porta all’aumento delle dimensioni dell’intervallo di confidenza.\n\n12.7 Riflessioni Conclusive\nLa teoria classica del punteggio vero si basa su un modello additivo, in cui il punteggio osservato \\(X\\) è considerato come la somma di due componenti: il punteggio vero stabile \\(T\\) e il punteggio di errore casuale \\(E\\). Si suppone che i punteggi di errore all’interno di un test non siano correlati né con i punteggi veri di quel test, né con i punteggi veri o di errore di altri test. I test paralleli hanno gli stessi punteggi veri e le stesse varianze di errore. I test che sono considerati “sostanzialmente equivalenti” o \\(\\tau\\)-equivalenti, differiscono solo per una costante additiva nei punteggi veri. Tuttavia, queste assunzioni possono essere violate in presenza di diverse condizioni che influenzano i punteggi dei test. Tuttavia, poiché non possiamo osservare direttamente \\(T\\) ed \\(E\\), non possiamo verificare direttamente l’adeguatezza di queste assunzioni, e possiamo solo fare delle supposizioni su quando sarebbero appropriate.\nÈ importante tenere a mente che i punteggi veri e quelli di errore sono concetti teorici e non osservabili. Ciò che possiamo osservare sono solamente i punteggi \\(X\\). Quando parliamo di punteggi veri, è essenziale considerare che il “punteggio vero”, cioè la media dei punteggi su ripetuti test indipendenti con lo stesso test, è un’astrazione teorica. Questo punteggio potrebbe non riflettere completamente l’attributo “vero” di interesse, a meno che il test non abbia una precisione perfetta, cioè che misuri esattamente ciò che afferma di misurare.\nL’approccio della teoria classica dei test (CTT) nel processo di sviluppo dei test presenta diversi vantaggi. In primo luogo, i concetti della CTT sono ampiamente diffusi e comprensibili. Inoltre, sono relativamente accessibili sia per l’apprendimento che per l’applicazione. Le statistiche descrittive dei test (come la media, la deviazione standard, l’intervallo, ecc.) e le analisi degli item (in particolare la facilità e la discriminazione degli item) possono essere calcolate facilmente. Inoltre, il modello CTT risponde a varie esigenze di misurazione, specialmente nello sviluppo di valutazioni di competenze e collocazione, utili per decisioni di ammissione, confronti tra programmi e valutazioni in vari contesti lavorativi. Infine, il modello CTT permette l’interpretazione dei punteggi degli esaminati sia al 0% che al 100% e delle stime di facilità degli item da 0.0 a 1.0, riflettendo risultati realistici. Tuttavia, queste interpretazioni non sono comuni nei modelli di teoria della risposta agli item (IRT).\nTuttavia, l’adozione della CTT presenta anche alcune limitazioni. In primo luogo, i test basati sulla CTT tendono a essere lunghi e composti da elementi omogenei. In secondo luogo, gli individui che svolgono test sviluppati con il metodo CTT potrebbero essere confrontati con item troppo facili o troppo difficili per le loro abilità. In terzo luogo, i risultati dei test CTT si applicano solo al campione considerato o a campioni molto simili. In quarto luogo, tali risultati si applicano solo alla selezione corrente di item. In quinto luogo, a causa della dipendenza dalla distribuzione normale, la CTT è adatta solo per lo sviluppo di test normativi. In sesto luogo, a causa della correlazione tra discriminazione degli item, affidabilità e alcune stime di validità, gli item e i test basati sulla CTT possono risultare sensibili alle differenze agli estremi della scala. Infine, sebbene gli errori di misurazione nei test CTT varino lungo tutto il range dei possibili punteggi (ossia, l’errore standard di misurazione è minore vicino alla media e aumenta man mano che i punteggi si discostano dalla media in entrambe le direzioni), l’errore standard di misurazione stimato nei CTT rappresenta una media su tutto questo intervallo.\n\n\n12.8 Session Info\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] psychometric_2.4  multilevel_2.7    MASS_7.3-60.0.1   nlme_3.1-164     \n [5] ggokabeito_0.1.0  viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n [9] ggExtra_0.10.1    bayesplot_1.11.1  gridExtra_2.3     patchwork_1.2.0  \n[13] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-17     psych_2.4.1      \n[17] scales_1.3.0      markdown_1.12     knitr_1.45        lubridate_1.9.3  \n[21] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[25] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.0    \n[29] tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.15.0  jsonlite_1.8.8     magrittr_2.0.3    \n  [4] nloptr_2.0.3       rmarkdown_2.26     vctrs_0.6.5       \n  [7] minqa_1.2.6        base64enc_0.1-3    rstatix_0.7.2     \n [10] htmltools_0.5.7    broom_1.0.5        Formula_1.2-5     \n [13] htmlwidgets_1.6.4  plyr_1.8.9         uuid_1.2-0        \n [16] igraph_2.0.2       mime_0.12          lifecycle_1.0.4   \n [19] pkgconfig_2.0.3    Matrix_1.6-5       R6_2.5.1          \n [22] fastmap_1.1.1      shiny_1.8.0        digest_0.6.34     \n [25] OpenMx_2.21.11     fdrtool_1.2.17     colorspace_2.1-0  \n [28] rprojroot_2.0.4    Hmisc_5.1-1        fansi_1.0.6       \n [31] timechange_0.3.0   abind_1.4-5        compiler_4.3.3    \n [34] withr_3.0.0        glasso_1.11        htmlTable_2.4.2   \n [37] backports_1.4.1    carData_3.0-5      ggsignif_0.6.4    \n [40] corpcor_1.6.10     gtools_3.9.5       tools_4.3.3       \n [43] pbivnorm_0.6.0     foreign_0.8-86     zip_2.3.1         \n [46] httpuv_1.6.14      nnet_7.3-19        glue_1.7.0        \n [49] quadprog_1.5-8     promises_1.2.1     lisrelToR_0.3     \n [52] grid_4.3.3         pbdZMQ_0.3-11      checkmate_2.3.1   \n [55] cluster_2.1.6      reshape2_1.4.4     generics_0.1.3    \n [58] gtable_0.3.4       tzdb_0.4.0         data.table_1.15.2 \n [61] hms_1.1.3          car_3.1-2          utf8_1.2.4        \n [64] sem_3.1-15         pillar_1.9.0       IRdisplay_1.1     \n [67] rockchalk_1.8.157  later_1.3.2        splines_4.3.3     \n [70] lattice_0.22-5     kutils_1.73        tidyselect_1.2.0  \n [73] miniUI_0.1.1.1     pbapply_1.7-2      stats4_4.3.3      \n [76] xfun_0.42          qgraph_1.9.8       arm_1.13-1        \n [79] stringi_1.8.3      boot_1.3-29        evaluate_0.23     \n [82] mi_1.1             cli_3.6.2          RcppParallel_5.1.7\n [85] IRkernel_1.3.2     rpart_4.1.23       xtable_1.8-4      \n [88] repr_1.1.6         munsell_0.5.0      Rcpp_1.0.12       \n [91] coda_0.19-4.1      png_0.1-8          XML_3.99-0.16.1   \n [94] parallel_4.3.3     ellipsis_0.3.2     jpeg_0.1-10       \n [97] lme4_1.1-35.1      openxlsx_4.2.5.2   crayon_1.5.2      \n[100] rlang_1.1.3        mnormt_2.1.1      \n\n\n\n\n\n\n\n\nCharter, R. A. (1996). Revisiting the standard errors of measurement, estimate, and prediction and their application to test scores. Perceptual and Motor Skills, 82(3), 1139–1144.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>La stima del punteggio vero</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/05_err_std_stima.html#riflessioni-conclusive",
    "href": "chapters/ctt/05_err_std_stima.html#riflessioni-conclusive",
    "title": "12  La stima del punteggio vero",
    "section": "12.7 Riflessioni Conclusive",
    "text": "12.7 Riflessioni Conclusive\nLa teoria classica del punteggio vero si basa su un modello additivo, in cui il punteggio osservato \\(X\\) è considerato come la somma di due componenti: il punteggio vero stabile \\(T\\) e il punteggio di errore casuale \\(E\\). Si suppone che i punteggi di errore all’interno di un test non siano correlati né con i punteggi veri di quel test, né con i punteggi veri o di errore di altri test. I test paralleli hanno gli stessi punteggi veri e le stesse varianze di errore. I test che sono considerati “sostanzialmente equivalenti” o \\(\\tau\\)-equivalenti, differiscono solo per una costante additiva nei punteggi veri. Tuttavia, queste assunzioni possono essere violate in presenza di diverse condizioni che influenzano i punteggi dei test. Tuttavia, poiché non possiamo osservare direttamente \\(T\\) ed \\(E\\), non possiamo verificare direttamente l’adeguatezza di queste assunzioni, e possiamo solo fare delle supposizioni su quando sarebbero appropriate.\nÈ importante tenere a mente che i punteggi veri e quelli di errore sono concetti teorici e non osservabili. Ciò che possiamo osservare sono solamente i punteggi \\(X\\). Quando parliamo di punteggi veri, è essenziale considerare che il “punteggio vero”, cioè la media dei punteggi su ripetuti test indipendenti con lo stesso test, è un’astrazione teorica. Questo punteggio potrebbe non riflettere completamente l’attributo “vero” di interesse, a meno che il test non abbia una precisione perfetta, cioè che misuri esattamente ciò che afferma di misurare.\nL’approccio della teoria classica dei test (CTT) nel processo di sviluppo dei test presenta diversi vantaggi. In primo luogo, i concetti della CTT sono ampiamente diffusi e comprensibili. Inoltre, sono relativamente accessibili sia per l’apprendimento che per l’applicazione. Le statistiche descrittive dei test (come la media, la deviazione standard, l’intervallo, ecc.) e le analisi degli item (in particolare la facilità e la discriminazione degli item) possono essere calcolate facilmente. Inoltre, il modello CTT risponde a varie esigenze di misurazione, specialmente nello sviluppo di valutazioni di competenze e collocazione, utili per decisioni di ammissione, confronti tra programmi e valutazioni in vari contesti lavorativi. Infine, il modello CTT permette l’interpretazione dei punteggi degli esaminati sia al 0% che al 100% e delle stime di facilità degli item da 0.0 a 1.0, riflettendo risultati realistici. Tuttavia, queste interpretazioni non sono comuni nei modelli di teoria della risposta agli item (IRT).\nTuttavia, l’adozione della CTT presenta anche alcune limitazioni. In primo luogo, i test basati sulla CTT tendono a essere lunghi e composti da elementi omogenei. In secondo luogo, gli individui che svolgono test sviluppati con il metodo CTT potrebbero essere confrontati con item troppo facili o troppo difficili per le loro abilità. In terzo luogo, i risultati dei test CTT si applicano solo al campione considerato o a campioni molto simili. In quarto luogo, tali risultati si applicano solo alla selezione corrente di item. In quinto luogo, a causa della dipendenza dalla distribuzione normale, la CTT è adatta solo per lo sviluppo di test normativi. In sesto luogo, a causa della correlazione tra discriminazione degli item, affidabilità e alcune stime di validità, gli item e i test basati sulla CTT possono risultare sensibili alle differenze agli estremi della scala. Infine, sebbene gli errori di misurazione nei test CTT varino lungo tutto il range dei possibili punteggi (ossia, l’errore standard di misurazione è minore vicino alla media e aumenta man mano che i punteggi si discostano dalla media in entrambe le direzioni), l’errore standard di misurazione stimato nei CTT rappresenta una media su tutto questo intervallo.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>La stima del punteggio vero</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/05_err_std_stima.html#session-info",
    "href": "chapters/ctt/05_err_std_stima.html#session-info",
    "title": "12  La stima del punteggio vero",
    "section": "12.8 Session Info",
    "text": "12.8 Session Info\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] psychometric_2.4  multilevel_2.7    MASS_7.3-60.0.1   nlme_3.1-164     \n [5] ggokabeito_0.1.0  viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n [9] ggExtra_0.10.1    bayesplot_1.11.1  gridExtra_2.3     patchwork_1.2.0  \n[13] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-17     psych_2.4.1      \n[17] scales_1.3.0      markdown_1.12     knitr_1.45        lubridate_1.9.3  \n[21] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[25] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.0    \n[29] tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.15.0  jsonlite_1.8.8     magrittr_2.0.3    \n  [4] nloptr_2.0.3       rmarkdown_2.26     vctrs_0.6.5       \n  [7] minqa_1.2.6        base64enc_0.1-3    rstatix_0.7.2     \n [10] htmltools_0.5.7    broom_1.0.5        Formula_1.2-5     \n [13] htmlwidgets_1.6.4  plyr_1.8.9         uuid_1.2-0        \n [16] igraph_2.0.2       mime_0.12          lifecycle_1.0.4   \n [19] pkgconfig_2.0.3    Matrix_1.6-5       R6_2.5.1          \n [22] fastmap_1.1.1      shiny_1.8.0        digest_0.6.34     \n [25] OpenMx_2.21.11     fdrtool_1.2.17     colorspace_2.1-0  \n [28] rprojroot_2.0.4    Hmisc_5.1-1        fansi_1.0.6       \n [31] timechange_0.3.0   abind_1.4-5        compiler_4.3.3    \n [34] withr_3.0.0        glasso_1.11        htmlTable_2.4.2   \n [37] backports_1.4.1    carData_3.0-5      ggsignif_0.6.4    \n [40] corpcor_1.6.10     gtools_3.9.5       tools_4.3.3       \n [43] pbivnorm_0.6.0     foreign_0.8-86     zip_2.3.1         \n [46] httpuv_1.6.14      nnet_7.3-19        glue_1.7.0        \n [49] quadprog_1.5-8     promises_1.2.1     lisrelToR_0.3     \n [52] grid_4.3.3         pbdZMQ_0.3-11      checkmate_2.3.1   \n [55] cluster_2.1.6      reshape2_1.4.4     generics_0.1.3    \n [58] gtable_0.3.4       tzdb_0.4.0         data.table_1.15.2 \n [61] hms_1.1.3          car_3.1-2          utf8_1.2.4        \n [64] sem_3.1-15         pillar_1.9.0       IRdisplay_1.1     \n [67] rockchalk_1.8.157  later_1.3.2        splines_4.3.3     \n [70] lattice_0.22-5     kutils_1.73        tidyselect_1.2.0  \n [73] miniUI_0.1.1.1     pbapply_1.7-2      stats4_4.3.3      \n [76] xfun_0.42          qgraph_1.9.8       arm_1.13-1        \n [79] stringi_1.8.3      boot_1.3-29        evaluate_0.23     \n [82] mi_1.1             cli_3.6.2          RcppParallel_5.1.7\n [85] IRkernel_1.3.2     rpart_4.1.23       xtable_1.8-4      \n [88] repr_1.1.6         munsell_0.5.0      Rcpp_1.0.12       \n [91] coda_0.19-4.1      png_0.1-8          XML_3.99-0.16.1   \n [94] parallel_4.3.3     ellipsis_0.3.2     jpeg_0.1-10       \n [97] lme4_1.1-35.1      openxlsx_4.2.5.2   crayon_1.5.2      \n[100] rlang_1.1.3        mnormt_2.1.1",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>La stima del punteggio vero</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/06_ctt_applications.html",
    "href": "chapters/ctt/06_ctt_applications.html",
    "title": "13  Applicazioni della CTT",
    "section": "",
    "text": "13.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nQuesto capitolo si focalizza sull’esplorazione di diverse applicazioni della Teoria Classica dei Test (CTT). Innanzitutto, verrà analizzato il metodo per determinare il numero di item necessari al fine di ottenere un livello specifico di affidabilità. Successivamente, si approfondirà il concetto di correlazione disattenuata e si esaminerà il metodo proposto per mitigare tale disattenuazione. Infine, verrà presentato l’utilizzo del metodo di Kelly per migliorare la stima dei punteggi reali a livello individuale, e sarà esaminato come i modelli bayesiani gerarchici rappresentino un’alternativa più moderna a tale approccio.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Applicazioni della CTT</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/06_ctt_applications.html#stimare-un-cambiamento-clinicamente-significativo",
    "href": "chapters/ctt/06_ctt_applications.html#stimare-un-cambiamento-clinicamente-significativo",
    "title": "13  Applicazioni della CTT",
    "section": "13.2 Stimare un Cambiamento Clinicamente Significativo",
    "text": "13.2 Stimare un Cambiamento Clinicamente Significativo\nIn psicologia clinica, uno dei principali problemi è determinare se si sia verificato un cambiamento clinicamente significativo in un individuo. Le metodologie utilizzate per questa valutazione sono generalmente suddivise in due categorie: i metodi “basati su ancoraggi” e quelli “basati sulla distribuzione” (Blampied, 2022).\nI metodi basati su ancoraggi definiscono un cambiamento clinicamente significativo come una variazione nei punteggi che corrisponde a un evento clinico rilevante. In altre parole, si considera che un cambiamento sia significativo se riflette un miglioramento o peggioramento percepito in seguito a un evento clinico importante, come ad esempio il miglioramento dopo un intervento terapeutico. I metodi basati sulla distribuzione, invece, utilizzano gli errori di misurazione psicometrica come parametro per valutare la significatività clinica del cambiamento. Uno dei primi metodi di questo tipo è stato l’indice di cambiamento affidabile, o Reliable Change Index (RCI), proposto da Jacobson e Truax. Questo indice si basa sull’errore standard della differenza (SED), che a sua volta è derivato dall’errore standard di misura (SEM). Quest’ultimo è calcolato come:\n\\[\nSEM = s_x \\sqrt{1 - r_{xx'}},\n\\]\ndove \\(s_x\\) è la deviazione standard dei punteggi al pre-test e \\(r_{xx'}\\) è la affidabilità dello strumento di misura.\nUn cambiamento è considerato clinicamente significativo se supera l’errore di misura intrinseco, ossia la variabilità casuale che può essere attribuita all’errore di misurazione e non a un cambiamento reale.\n\n13.2.1 Calcolo del Reliable Change Index (RCI)\nPer comprendere come si calcola il Reliable Change Index, è utile ricordare che qualsiasi misurazione può essere scomposta in due componenti: il punteggio reale (o vero) e un componente di errore. Possiamo rappresentare qualsiasi misurazione come:\n\\[\nX = T \\pm E,\n\\]\ndove \\(X\\) è il punteggio osservato, \\(T\\) il punteggio vero e \\(E\\) l’errore di misurazione. Se un cambiamento effettivo si è verificato nel tempo per un individuo nella dimensione catturata dalla variabile dipendente e all’interno della sensibilità dello strumento di misurazione, allora il punteggio vero al tempo \\(t1\\) sarà diverso dal punteggio vero al tempo \\(t2\\). Tuttavia, data la variabilità e l’errore intrinseci alle misurazioni psicologiche, è possibile che si osservi una differenza tra i punteggi \\(t1\\) e \\(t2\\) anche in assenza di un reale cambiamento. Pertanto, come possiamo determinare quanto debba essere grande un cambiamento osservato affinché si possa concludere che un cambiamento reale sia effettivamente avvenuto, piuttosto che semplicemente una variazione insignificante?\nLa derivazione dell’RCI si basa sulla distribuzione di frequenza degli errori di misurazione. La deviazione standard della distribuzione degli errori di misurazione è data dal SEM, e Jacobson e colleghi (1984) hanno utilizzato questa conoscenza di base per definire l’RCI. Tuttavia, per comprendere questa definizione, è necessario prima comprendere la distribuzione degli errori dei punteggi di differenza.\n\n\n13.2.2 Calcolo della Differenza tra i Punteggi e la sua Distribuzione di Errore\nIl modo più semplice per identificare se un cambiamento si è verificato è calcolare un punteggio di differenza non nullo, rappresentato come un punteggio di cambiamento grezzo (\\(C\\)) dato dalla differenza tra il punteggio al tempo \\(t2\\) e quello al tempo \\(t1\\) per un individuo:\n\\[\nC_i = X_{t1} - X_{t2},\n\\]\ndove \\(C_i\\) è il punteggio di cambiamento per l’individuo \\(i\\), e \\(X\\) rappresenta il punteggio misurato in due momenti temporali distinti \\(t1\\) e \\(t2\\). Poiché ogni misurazione contiene sia un punteggio vero che un errore di misurazione, il punteggio di cambiamento grezzo comprenderà il vero cambiamento più o meno un errore di misurazione. Poiché l’errore è il risultato della combinazione degli errori presenti in ciascuna misurazione, esso sarà più grande rispetto agli errori associati ai singoli punteggi.\nLa distribuzione degli errori dei punteggi di differenza segue anch’essa una distribuzione normale con media pari a zero ma con una deviazione standard più ampia, chiamata deviazione standard della differenza (\\(SD_{Diff}\\)), calcolata come:\n\\[\nSD_{Diff} = \\sqrt{2 \\times SEM^2}.\n\\]\nQuesto perché, secondo la proprietà della varianza di una differenza tra due variabili indipendenti (o scorrelate), la varianza della differenza è data dalla somma delle varianze individuali, ossia:\n\\[\n\\sigma_{X1 - X2} = \\sigma_1^2 + \\sigma_2^2 - 2 \\cdot \\text{cov}(X1, X2).\n\\]\nNel caso in cui le misurazioni siano indipendenti o non correlate, \\(\\text{cov}(X1, X2) = 0\\), e quindi la deviazione standard della differenza diventa \\(\\sqrt{2 \\cdot SEM^2}\\).\nAnche se i punteggi osservati prima e dopo l’intervento riflettono lo stesso individuo e sono quindi correlati nei loro “punteggi veri” (cioè il cambiamento reale), si considera che gli errori di misurazione associati a ciascun punteggio siano indipendenti. Questo è un presupposto comune nelle analisi psicometriche, poiché ogni volta che si ripete una misurazione su uno stesso individuo, le fonti di errore possono variare (per esempio, variazioni casuali nel modo in cui risponde al test in un determinato giorno, piccole differenze nel contesto della misurazione, ecc.).\n\n\n13.2.3 Definizione dell’RCI\nJacobson e Truax (1991) hanno definito l’RCI come il punteggio di cambiamento standardizzato, ottenuto dividendo il punteggio di differenza per \\(SD_{Diff}\\):\n\\[\nC_i (\\text{Standardized}) = \\frac{C_i}{SD_{Diff}}.\n\\]\nQuesta trasformazione converte il punteggio di cambiamento grezzo in unità di deviazione standard, analogamente a come uno z-score rappresenta la differenza tra un punteggio individuale e la media, standardizzata tramite la deviazione standard. Un valore di RCI clinicamente significativo suggerisce che il cambiamento osservato è sufficientemente grande da non poter essere attribuito al solo errore di misurazione.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Applicazioni della CTT</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/06_ctt_applications.html#affidabilità-e-lunghezza-del-test",
    "href": "chapters/ctt/06_ctt_applications.html#affidabilità-e-lunghezza-del-test",
    "title": "13  Applicazioni della CTT",
    "section": "13.3 Affidabilità e lunghezza del test",
    "text": "13.3 Affidabilità e lunghezza del test\nL’affidabilità può essere utilizzata per determinare la lunghezza di un test. La formula di Spearman-Brown può essere adattata per calcolare il numero di item necessari al fine di raggiungere una specifica affidabilità:\n\\[\n\\begin{equation}\np = \\frac{\\rho_p(1 - \\rho_1)}{\\rho_1(1 - \\rho_p)},\n\\end{equation}\n\\tag{13.1}\\]\ndove \\(\\rho_1\\) rappresenta l’affidabilità stimata di un “item medio”, \\(\\rho_p\\) è il livello desiderato di affidabilità complessiva del test, e \\(p\\) è il numero di item nel test esteso.\nPer esempio, supponiamo che l’attendibilità di un test composto da 5 item sia 0.824, e che \\(\\rho_1\\) sia 0.479. Possiamo chiederci quanti item debbano essere aggiunti per raggiungere un livello di affidabilità pari a 0.95.\nPonendo \\(\\rho_p\\) a 0.95 e \\(\\rho_1\\) a 0.479, in base all’equazione Equazione 13.1, otteniamo che:\n\nrho_1 &lt;- 0.479\n(.95 * (1 - rho_1)) / (rho_1 * (1 - .95))\n\n20.6659707724426\n\n\nPertanto, per ottenere un livello di affidabilità pari a 0.95 sono necessari almeno 21 item.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Applicazioni della CTT</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/06_ctt_applications.html#attenuazione",
    "href": "chapters/ctt/06_ctt_applications.html#attenuazione",
    "title": "13  Applicazioni della CTT",
    "section": "13.4 Attenuazione",
    "text": "13.4 Attenuazione\n\n13.4.1 Attenuazione e Correlazioni Disattenuate\nUn aspetto cruciale nell’analisi statistica riguarda il fenomeno dell’attenuazione, che si verifica quando l’incremento dell’errore di misurazione porta a una riduzione della correlazione osservata tra due variabili. Questo errore di misurazione tende a “nascondere” la vera associazione esistente tra le variabili, generando quello che è noto come effetto di attenuazione.\nLord e Novick (1967) hanno sottolineato che, nel tentativo di esplorare la relazione tra due costrutti, gli psicologi spesso ricorrono allo sviluppo di scale di misura. Se esiste una relazione lineare tra queste scale, è possibile calcolare il grado di correlazione attraverso il coefficiente di correlazione. Tuttavia, dato che le scale includono inevitabilmente un certo livello di errore, la correlazione empiricamente osservata tra di esse risulta inferiore rispetto alla correlazione “vera” tra i costrutti. In queste circostanze, è possibile ricorrere a formule specifiche per stimare la correlazione corretta tra i tratti latenti.\nSi può dimostrare che la correlazione tra i punteggi veri di due costrutti, \\(T_X\\) e \\(T_Y\\), può essere calcolata utilizzando la correlazione \\(\\rho_{XY}\\) tra i punteggi osservati \\(X\\) e \\(Y\\), e i coefficienti di affidabilità \\(\\rho_{XX'}\\) e \\(\\rho_{YY'}\\) dei due test, come segue:\n\\[\n\\begin{equation}\n\\rho(T_X, T_Y)  = \\frac{\\rho_{XY}}{\\sqrt{\\rho_{XX^\\prime} \\rho_{YY^\\prime}}}.\n\\end{equation}\n\\](eq-3-9-6)\nAnalogamente, la correlazione tra i punteggi osservati di un test e i punteggi veri di un secondo test può essere espressa attraverso la correlazione tra i punteggi osservati dei due test e il coefficiente di affidabilità del secondo test:\n\\[\n\\begin{equation}\n\\rho(X, T_Y)  = \\frac{\\rho_{XY}}{\\sqrt{\\rho_{YY^\\prime}}}.\n\\end{equation}\n\\](eq-3-9-7)\nQueste equazioni forniscono gli strumenti per calcolare le correlazioni disattenuate secondo la Teoria Classica dei Test (CTT).\nIl calcolo degli intervalli di confidenza per la correlazione corretta richiede un approccio che tenga conto dell’attenuazione dell’affidabilità. Applicando la formula di disattenuazione agli estremi dell’intervallo di confidenza osservato, possiamo ottenere stime più precise degli intervalli di confidenza per la correlazione tra i punteggi veri.\nPer fare un esempio, supponiamo di avere una correlazione osservata di 0.5 tra due misure, con affidabilità di 0.7 per la prima misura e 0.8 per la seconda misura. Vogliamo calcolare la correlazione disattenuata e il relativo intervallo di confidenza.\n\n# Parametri\nr_osservata &lt;- 0.5\nrho_X &lt;- 0.7\nrho_Y &lt;- 0.8\n\n# Calcolo della correlazione disattenuata\nr_corretta &lt;- r_osservata / sqrt(rho_X * rho_Y)\n\n# Stampa della correlazione disattenuata\nprint(paste(\"Correlazione disattenuata:\", r_corretta))\n\n# Calcolo approssimativo dell'intervallo di confidenza (per semplificazione)\n# NOTA: Questo è un esempio semplificato e non riflette il calcolo preciso degli intervalli di confidenza.\nCI_lower_observed &lt;- 0.4 # Limite inferiore osservato\nCI_upper_observed &lt;- 0.6 # Limite superiore osservato\n\nCI_lower_corrected &lt;- CI_lower_observed / sqrt(rho_X * rho_Y)\nCI_upper_corrected &lt;- CI_upper_observed / sqrt(rho_X * rho_Y)\n\n# Stampa dell'intervallo di confidenza corretto\nprint(paste(\"Intervallo di confidenza corretto: da\", CI_lower_corrected, \"a\", CI_upper_corrected))\n\n[1] \"Correlazione disattenuata: 0.668153104781061\"\n[1] \"Intervallo di confidenza corretto: da 0.534522483824849 a 0.801783725737273\"\n\n\n\n\n13.4.2 L’impiego delle Correlazioni Disattenuate\nL’uso delle correlazioni disattenuate risale al 1904 con Spearman, che le applicò in uno studio in cui \\(X\\) misurava la discriminazione dell’altezza del suono e \\(Y\\) l’intelligenza valutata da un insegnante. La correlazione tra queste due misure era \\(\\hat{\\rho}_{XY} = 0.38\\), con affidabilità di \\(\\hat{\\rho}_{XX'} = 0.25\\) e \\(\\hat{\\rho}_{YY'} = 0.55\\). Utilizzando le formule sopra citate, la correlazione predetta tra i punteggi veri di discriminazione del suono e l’intelligenza risultava essere \\(\\hat{\\rho}(X, T_Y) = 0.76\\), mentre tra i punteggi veri dei due costrutti era \\(\\hat{\\rho}(T_X, T_Y) = 1.025\\).\nQuesto esempio evidenzia come l’uso delle correlazioni disattenuate possa portare a stime eccessive, una problematica già rilevata nell’interazione tra Spearman e Karl Pearson. Spearman, attraverso l’applicazione della sua formula, sottolineò come le correlazioni empiriche basse proposte da Pearson potessero essere sottostimate a causa dell’errore di misurazione. Tuttavia, Pearson non accolse queste osservazioni, rimanendo scettico riguardo alla possibilità che la formula di Spearman generasse correlazioni superiori a 1 e rigettando l’idea di quantità non osservabili.\nNonostante queste controversie, Spearman proseguì nello studio delle variabili psicologiche, trovando in numerosi casi che le correlazioni disattenuate si avvicinavano all’unità, suggerendo un’associazione stretta tra variabili indicative dello stesso fenomeno. Queste osservazioni lo portarono a sviluppare ulteriormente l’analisi fattoriale.\nMcDonald (1999) avverte sull’utilizzo delle correlazioni disattenuate, evidenziando la necessità di cautela. Propone come alternativa più affidabile l’uso di modelli di equazioni strutturali per calcolare le correlazioni tra variabili latenti, ovvero quelle non influenzate da errori di misurazione, consentendo un’esplorazione diretta e più accurata delle ipotesi, inclusa la correlazione tra variabili latenti.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Applicazioni della CTT</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/06_ctt_applications.html#usare-laffidabilità-per-migliorare-linferenza-a-livello-individuale",
    "href": "chapters/ctt/06_ctt_applications.html#usare-laffidabilità-per-migliorare-linferenza-a-livello-individuale",
    "title": "13  Applicazioni della CTT",
    "section": "13.5 Usare l’Affidabilità per Migliorare l’Inferenza a Livello Individuale",
    "text": "13.5 Usare l’Affidabilità per Migliorare l’Inferenza a Livello Individuale\nUn altro uso importante dell’affidabilità è quello che ci consente di migliorare la nostra inferenza sui punteggi veri a livello individuale.\nKelley ha dimostrato – già nel 1920 (vedi Kelley, 1947) – che possiamo stimare i punteggi veri per ciascun individuo, regredendo i punteggi osservati sulla stima dell’affidabilità:\n\\[ \\hat{T} = \\bar{X} + r_{xx'}(X - \\bar{X}). \\]\nQui, \\(\\bar{X}\\) è la media dei punteggi osservati su tutti i soggetti, data da:\n\\[ \\bar{X} = \\frac{1}{N} \\sum_{i=1}^{N} X_i. \\]\nIntuitivamente, il punteggio vero per ciascun soggetto è stimato avvicinando il loro punteggio osservato verso la media dei punteggi a livello di gruppo in proporzione all’affidabilità della stima a livello individuale.\nIn aggiunta alla sua teoria che i punteggi osservati tendono ad essere regolati verso la media del gruppo quando si stima il vero punteggio, Kelley ha evidenziato come l’errore standard della stima del vero punteggio sia ridotto secondo la formula:\n\\[\n\\sigma_{\\hat{T}} = \\sigma_X \\sqrt{\\rho_{XX^\\prime} (1 -\\rho_{XX^\\prime})}.\n\\]\nQui, \\(\\sigma_{\\hat{T}}\\) rappresenta l’errore standard della stima del vero punteggio, \\(\\sigma_X\\) è la deviazione standard dei punteggi osservati, e \\(\\rho_{XX^\\prime}\\) indica il coefficiente di affidabilità tra i punteggi osservati e quelli veri. Questo errore standard per le stime dei punteggi veri è inferiore rispetto all’errore standard dei punteggi osservati, espresso come:\n\\[\nSE_{X} = \\sigma_{X} \\sqrt{1 - \\rho_{XX'}}.\n\\]\nIl confronto tra le due formule rivela che l’errore standard della stima del vero punteggio include un fattore aggiuntivo, \\(\\rho_{XX'}\\), che rappresenta il coefficiente di affidabilità. Questo evidenzia l’importanza del coefficiente di affidabilità nell’influenzare la precisione della stima del vero punteggio: un alto coefficiente di affidabilità contribuisce a ridurre l’errore standard della stima, migliorando così la precisione della stima del vero punteggio.\nLe equazioni di Kelley, scoperte nel 1920, anticipano di molti anni i principi alla base degli stimatori di James-Stein, che analogamente aggiustano le stime individuali avvicinandole alla media del gruppo. Questa affinità storica evidenzia un precedente significativo alla comprensione moderna di come le stime possano essere migliorate mediante l’incorporazione di informazioni aggiuntive.\nLa relazione tra le equazioni di Kelley e i concetti bayesiani offre una prospettiva ancora più profonda. Assumendo che i punteggi veri seguano una distribuzione a priori normale e che esista una distribuzione normale dei punteggi veri intorno ai punteggi osservati, l’approccio bayesiano empirico genera medie posteriori che corrispondono alle stime di Kelley dei punteggi veri. Questa equivalenza, come discussa da de Gruijter e van der Kamp nel 2008, stabilisce un ponte concettuale tra la psicometria classica e l’inferenza bayesiana, sottolineando come l’incorporazione di presupposti a priori possa affinare le nostre stime.\nQuesta connessione è ulteriormente rafforzata dall’uso di tecniche simili alla stima bayesiana empirica nei software di modellazione multilivello, come ad esempio il pacchetto lmer in R. Questi software si avvalgono della potenza dell’inferenza bayesiana per integrare informazioni di gruppo, migliorando così la precisione delle inferenze a livello individuale. La pratica di utilizzare informazioni a livello di gruppo per affinare le stime individuali non solo ha radici storiche profonde ma continua a essere una componente essenziale nell’evoluzione delle tecniche statistiche, dimostrando il suo valore nell’arricchire l’accuratezza e l’affidabilità delle inferenze statistiche.\nPer illustrare in modo pratico come avviene la stima dei punteggi veri, ossia il processo di pooling, eseguiremo una simulazione basata sul codice R di Nathaniel Haines. Questa simulazione genera dati seguendo una distribuzione binomiale per 20 soggetti, con una probabilità media di successo di 0.7. La simulazione considera tre diversi set di item: 10, 30 e 100, al fine di esaminare come le variazioni nel numero di item influenzino l’affidabilità ottenuta e, di conseguenza, gli effetti del pooling.\nIl codice inizia definendo il numero di soggetti e la varietà delle dimensioni degli item. Successivamente, genera un campione casuale di “punteggi veri” intorno a 0.7 per ogni soggetto. Viene poi definita una funzione per stimare l’errore standard della misurazione (al quadrato), basata sulla probabilità di successo per ogni item.\nPer ogni set di item, il codice simula i dati osservati per ogni soggetto utilizzando il suo “punteggio vero”. Calcola quindi la media del gruppo per i punteggi osservati, la affidabilità, e l’errore standard di misurazione, utilizzando l’approccio basato sulla varianza. Infine, stima i punteggi veri e gli errori standard associati sia per i punteggi osservati sia per quelli stimati.\nI risultati della simulazione vengono visualizzati in un grafico, che confronta i punteggi veri, osservati e stimati per ogni soggetto, evidenziando come la precisione della stima vari in funzione del numero di item. Il grafico include anche intervalli di confidenza al 95% per i punteggi osservati e stimati, e una linea orizzontale che rappresenta la media del gruppo per i punteggi osservati, offrendo una rappresentazione visiva dell’efficacia del processo di pooling nel recuperare i punteggi veri a partire da dati osservati affetti da errore di misurazione.\n\nset.seed(43202)\n\n# Number of subjects and items\nn_subj &lt;- 20\nn_items &lt;- c(10, 30, 100)\n\n# Random sample of \"true\" scores around .7\ntheta &lt;- rnorm(n_subj, .7, .1)\n\n# Estimate standard error of measurement (squared)\nest_se2 &lt;- function(x) {\n    # Success and failure probability\n    n &lt;- length(x)\n    p &lt;- mean(x)\n    q &lt;- 1 - p\n\n    sig2_ep_i &lt;- (p * q) / (n - 1)\n\n    return(sig2_ep_i)\n}\n\n# Estimate observed and true score\ndis_dat &lt;- foreach(i = seq_along(n_items), .combine = \"rbind\") %do% {\n    # Generate observed data for each subject using \"true\" score\n    X_all &lt;- foreach(t = seq_along(theta), .combine = \"rbind\") %do% {\n        rbinom(n_items[i], 1, prob = theta[t])\n    }\n\n    # group average observed score\n    X_bar &lt;- mean(rowMeans(X_all))\n\n    # Reliability\n    X &lt;- rowMeans(X_all)\n\n    # Standard arror of measurement approach\n    sig2_ep &lt;- mean(apply(X_all, 1, est_se2))\n    sig2_X &lt;- var(X)\n    rho &lt;- 1 - (sig2_ep / sig2_X)\n\n    foreach(t = seq_along(theta), .combine = \"rbind\") %do% {\n        # Using observed scores from parallel form 1\n        X_obs &lt;- X_all[t, ]\n        X_i &lt;- mean(X_obs)\n\n        data.frame(\n            subj_num = t,\n            n_items = n_items[i],\n            theta = theta[t],\n            rho = rho,\n            X = X_i,\n            se_obs = sd(X) * sqrt(1 - rho),\n            se_hat = sd(X) * sqrt(1 - rho) * sqrt(rho),\n            theta_hat = (1 - rho) * X_bar + rho * X_i\n        )\n    }\n}\n\n# Plot true, observed, and estimated true scores\ndis_dat %&gt;%\n    mutate(subj_num = reorder(subj_num, theta)) %&gt;%\n    ggplot(aes(x = subj_num, y = theta)) +\n    geom_point(color = I(\"black\")) +\n    geom_point(aes(x = subj_num, y = X),\n        color = I(\"#DCBCBC\"),\n        position = position_jitter(width = .2, height = 0, seed = 1)\n    ) +\n    geom_linerange(\n        aes(\n            x = subj_num,\n            ymin = X - 1.96 * se_obs,\n            ymax = X + 1.96 * se_obs\n        ),\n        color = I(\"#DCBCBC\"),\n        position = position_jitter(width = .2, height = 0, seed = 1)\n    ) +\n    geom_point(aes(x = subj_num, y = theta_hat),\n        color = I(\"#8F2727\"),\n        position = position_jitter(width = .2, height = 0, seed = 2)\n    ) +\n    geom_linerange(\n        aes(\n            x = subj_num,\n            ymin = theta_hat - 1.96 * se_hat,\n            ymax = theta_hat + 1.96 * se_hat\n        ),\n        color = I(\"#8F2727\"),\n        position = position_jitter(width = .2, height = 0, seed = 2)\n    ) +\n    geom_hline(yintercept = X_bar, linetype = 2, color = I(\"gray\")) +\n    annotate(\"text\",\n        x = 15, y = .4, label = expression(\"True\" ~ theta[i]),\n        color = \"black\", size = 5\n    ) +\n    annotate(\"text\",\n        x = 15, y = .3, label = expression(\"Obs\" ~ X[i]),\n        color = \"#DCBCBC\", size = 5\n    ) +\n    annotate(\"text\",\n        x = 15, y = .2, label = expression(\"Est\" ~ hat(theta)[i]),\n        color = \"#8F2727\", size = 5\n    ) +\n    facet_wrap(c(\"n_items\"), nrow = 1) +\n    ggtitle(\"Regression-Based True Score Estimates\") +\n    xlab(\"Subject\") +\n    ylab(\"Value\") +\n    theme_minimal(base_size = 15) +\n    theme(\n        panel.grid = element_blank(),\n        axis.text.x.bottom = element_blank()\n    )\n\n\n\n\n\n\n\n\nSi notino tre risultati di questa simulazione:\n\nle stime puntuali basate sulla regressione di Kelley (i punti neri nel grafico) risultano più vicine alla media a livello di gruppo (rappresentata dalla linea tratteggiata grigia orizzontale) di quanto lo siano le stime individuali “non corrette” (punti grigi);\nquesto effetto di “pooling” è tanto maggiore quanto minore è l’attendibilità (in questa simulazione l’attendibilità è stata manipolata variando il numero di item);\ngli intervalli di confidenza per i punteggi veri stimati sono più stretti rispetto a quelli dei punteggi osservati.\n\n\n13.5.1 Approccio Bayesiano\nNella seguente simulazione mostreremo come i risultati raggiunti con la regressione di Kelley possano essere replicati se i dati vengono analizzati con un modello gerarchico bayesiano.\nQuando si analizzano dati provenienti da questionari con risposte dicotomiche (ad esempio, vero/falso o corretto/errato), è possibile applicare la distribuzione di Bernoulli. In questo contesto, ogni risposta data a un item del questionario può essere vista come il risultato di un esperimento di Bernoulli. Se indichiamo con \\(X\\) una variabile casuale che segue tale distribuzione, la probabilità di ottenere un successo (ad esempio, una risposta corretta) è espressa come:\n\\[\n\\Pr(X=1) = p, \\quad \\text{e quindi} \\quad \\Pr(X=0) = 1 - p = q,\n\\]\ndove \\(p\\) indica la probabilità di successo e \\(q\\) quella di insuccesso.\nIntroduciamo il modello di Bernoulli tramite l’equazione logistica:\n\\[\np = \\frac{1}{1 + e^{-\\theta}}.\n\\]\nQuesta formula ci permette di modellare \\(p\\) in termini di \\(\\theta\\), un parametro che riflette una caratteristica o “abilità” dell’individuo. Il modello logistico assicura che \\(p\\), la probabilità di successo, sia sempre compresa nell’intervallo \\([0, 1]\\). Il parametro \\(\\theta\\) viene definito come:\n\\[\n\\theta = \\log\\left(\\frac{p}{1-p}\\right),\n\\]\ne può variare tra \\(-\\infty\\) e \\(+\\infty\\). Attraverso la trasformazione logistica, \\(\\theta\\) viene mappato in un valore di \\(p\\) che rispetta i limiti di una probabilità. Questa funzione di collegamento permette di interpretare il legame tra \\(\\theta\\) e \\(p\\).\nIl modello descritto sopra può essere considerato una forma estremamente semplificata della Teoria della Risposta all’Item (IRT), dove ogni persona è caratterizzata da un unico parametro di abilità (\\(\\theta\\)), e tutti gli item del test sono assunti avere uguale difficoltà e capacità di discriminazione, fissate convenzionalmente a 1.\nIl nostro obiettivo principale nell’analisi dei dati è quindi stimare il parametro \\(\\theta\\) per ogni individuo. La relazione tra \\(\\theta\\) e \\(p\\) è fondamentale: \\(\\theta\\) determina il valore di \\(p\\) attraverso la funzione logistica, che trasforma i valori di \\(\\theta\\) in probabilità \\(p\\) comprese tra 0 e 1. La stima di \\(\\theta\\) ci fornisce, di conseguenza, una misura della probabilità di successo di un individuo in risposta agli item del questionario.\nPer approfondire la nostra comprensione su come emergono le risposte osservate, è fondamentale definire la modalità con cui i parametri \\(\\theta\\) vengono generati per ogni individuo. Similmente a quanto avviene nella teoria classica dei test, dove si presume l’esistenza di una distribuzione di campionamento a livello di popolazione, nell’ambito della modellazione generativa bayesiana si postula una distribuzione generativa per il gruppo. In termini pratici, possiamo ipotizzare che i parametri \\(\\theta\\) individuali derivino da una distribuzione normale standardizzata:\n\\[\n\\theta \\sim \\mathcal{N}(0, 1)\n\\]\nNel contesto bayesiano, questa distribuzione di gruppo viene comunemente identificata come una distribuzione a priori per \\(\\theta\\). In alternativa, possiamo dedurre questi parametri direttamente dai dati:\n\\[\n\\theta \\sim \\mathcal{N}(\\mu, \\sigma)\n\\]\nDi conseguenza, si introduce un’ipotesi generativa riguardante i parametri di media \\(\\mu\\) e deviazione standard \\(\\sigma\\) del gruppo, che potrebbero essere descritti, in termini bayesiani tradizionali, come a priori del gruppo. Nel nostro esempio, supponiamo \\(\\mu = 0\\) e \\(\\sigma \\sim \\text{HalfNormal}(1)\\), dove \\(\\text{HalfNormal}(1)\\) rappresenta una distribuzione normale limitata ai valori positivi, coerente con il principio che le deviazioni standard debbano essere positive.\nQuesto approccio introduce un modello gerarchico: durante l’adattamento del modello, i parametri individuali influenzano quelli di gruppo, che a loro volta modellano nuovamente quelli individuali. Analogamente alle stime dei punteggi “veri” ottenuti tramite regressione nella teoria classica dei test, i nostri parametri individuali verranno regolati (“pooled”) verso la media di gruppo, portando a una riduzione degli intervalli di incertezza per le stime individuali.\nPer facilitare la comprensione di come queste assunzioni generative si traducano in pratica, eseguiamo la seguente simulazione.\n\nfile &lt;- file.path(\"hbern.stan\")\n\n\nmod &lt;- cmdstan_model(file)\n\n\nmod$print()\n\ndata {\n  int&lt;lower=0&gt; N;      // Number of subjects\n  int&lt;lower=0&gt; N_items; // Number of timepoints\n  array[N, N_items] int Y; // Binary responses for each subject and item\n}\n\nparameters {\n  real&lt;lower=0&gt; sigma_theta; // SD of individual effects\n  real mu_theta; // Mean of individual effects\n  \n  vector[N] theta_pr; // Non-centered individual-level parameters\n}\n\ntransformed parameters {\n  vector[N] theta = mu_theta + sigma_theta * theta_pr; // Individual-level effects\n}\n\nmodel {\n  // Priors\n  mu_theta ~ normal(0, 1);\n  sigma_theta ~ normal(0, 1);\n  theta_pr ~ normal(0, 1);\n  \n  // Likelihood\n  for (i in 1:N) {\n    for (j in 1:N_items) {\n      Y[i, j] ~ bernoulli_logit(theta[i]);\n    }\n  }\n}\n\ngenerated quantities {\n  array[N] real p; // Success probability estimate for each individual\n  \n  for (i in 1:N) {\n    p[i] = inv_logit(theta[i]);\n  }\n}\n\n\nIl codice Stan presentato adotta una parametrizzazione non centrata (non-centered parameterization) per la parte di modello a livello di gruppo, una scelta motivata per migliorare l’efficienza computazionale e facilitare la convergenza degli algoritmi di stima, come il campionamento Hamiltoniano Monte Carlo (HMC) usato da Stan. Questa scelta di design è matematicamente equivalente al modello generativo descritto dalle equazioni precedenti, pur offrendo vantaggi pratici significativi in fase di implementazione.\nLa parametrizzazione non centrata è una strategia avanzata nella modellazione bayesiana, specialmente utile nei modelli gerarchici o multilivello. Essa differisce dalla parametrizzazione centrata, nella quale i parametri di gruppo sono direttamente definiti dai parametri individuali. Invece, con la parametrizzazione non centrata, i parametri individuali sono inizialmente espressi come variazioni indipendenti rispetto alla media e deviazione standard di gruppo, per poi essere trasformati.\nImplementazione nel codice Stan:\n\nDefinizione dei Parametri:\n\nsigma_theta denota la deviazione standard degli effetti individuali, indicando la variabilità dei parametri \\(\\theta\\) a livello personale.\nmu_theta rappresenta la media degli effetti individuali.\ntheta_pr corrisponde ai parametri individuali nella forma non centrata, esprimendo le deviazioni rispetto alla media di gruppo in unità standardizzate.\n\nTrasformazione dei Parametri:\n\nGli effetti individuali effettivi (theta) sono ottenuti trasformando theta_pr per allinearli attorno a mu_theta e adattarli alla scala definita da sigma_theta. Questo processo è sintetizzato dall’equazione theta = mu_theta + sigma_theta * theta_pr, che trasla e scala theta_pr per ottenere valori centrati e proporzionati correttamente.\n\nApplicazione nel Modello:\n\nAll’interno del modello, sia mu_theta che sigma_theta sono sottoposti a priori normali (normal(0, 1)), presupponendo una distribuzione iniziale per questi parametri a livello di gruppo. Anche theta_pr è assoggettato a una distribuzione normale standard come priori, rispecchiando l’approccio di considerare le variazioni in termini standardizzati.\nLa verosimiglianza del modello è calcolata usando una distribuzione di Bernoulli con una funzione di collegamento logit, basata sui valori di theta trasformati, per analizzare le risposte binarie Y fornite da ogni soggetto per ogni item.\n\n\nAttraverso questa struttura, il modello mira a una stima più stabile e accurata dei parametri, beneficiando della maggiore efficienza computazionale e della riduzione dei problemi di convergenza che spesso accompagnano la modellazione bayesiana gerarchica.\nSimuliamo i dati di un singolo soggetto.\n\n# Initialize parameters for a single subject\nn_subj &lt;- 1\nn_items &lt;- 30 # Example with 30 items for simplicity\n\n# Generate \"true\" theta for the subject\ntheta &lt;- rnorm(n_subj, .7, .1)\n\n# Generate observed data for the subject using \"true\" theta\nY &lt;- rbinom(n_items, 1, prob = theta)\n\nAdattiamo il modello gerarchico bayesiano ai dati.\n\nfit_bernoulli &lt;- mod$sample(\n    data = list(\n        N = n_subj,\n        N_items = n_items,\n        Y = matrix(Y, nrow = 1) # Ensure Y is a matrix even for a single subject\n    ),\n    iter_sampling = 2500,\n    iter_warmup = 500,\n    chains = 4,\n    parallel_chains = 4,\n    seed = 43202\n)\n\nCalcoliamo la media a posteriori di \\(\\theta\\) e l’intervallo di confidenza al 95%:\n\n# Extract posterior samples for parameter 'p'\nbayes_est &lt;- fit_bernoulli$draws(variables = \"p\")\n\nbayes_est_p &lt;- as.vector(bayes_est)\n\n# Calculate the mean of the Bayesian estimates for 'p'\nbayes_theta_est &lt;- mean(bayes_est_p)\n\n# Calculate the 95% HDI using quantiles for the flattened vector\nhdi_bounds &lt;- quantile(bayes_est_p, probs = c(0.025, 0.975))\n\n# Prepare the results with a single HDI for 'p'\nresults &lt;- data.frame(\n    subj_num = 1,\n    n_items = n_items,\n    theta = theta,\n    bayes_theta = bayes_theta_est,\n    bayes_lo = hdi_bounds[1], # Lower bound of HDI\n    bayes_hi = hdi_bounds[2] # Upper bound of HDI\n)\n\n# Print the corrected results\nprint(results)\n\n     subj_num n_items theta bayes_theta bayes_lo bayes_hi\n2.5%        1      30 0.699       0.712    0.547    0.849\n\n\nAdesso svolgiamo la stessa simulazione considerando però 20 soggetti e facendo variare il numero di item del questionario (10, 30, 100).\n\nset.seed(43202)\n\nn_subj &lt;- 20\nn_items_vec &lt;- c(10, 30, 100)\n\n# Placeholder for results\nresults &lt;- list()\n\nfor (n_items in n_items_vec) {\n    for (subj in 1:n_subj) {\n        # Generate \"true\" theta for the subject\n        theta &lt;- rnorm(1, .7, .1)\n\n        # Generate observed data for the subject using \"true\" theta\n        Y &lt;- rbinom(n_items, 1, prob = theta)\n\n        # Fit the model\n        fit_bernoulli &lt;- mod$sample(\n            data = list(\n                N = 1,\n                N_items = n_items,\n                Y = matrix(Y, nrow = 1) # Ensure Y is a matrix\n            ),\n            iter_sampling = 2500,\n            iter_warmup = 500,\n            chains = 4,\n            parallel_chains = 4,\n            seed = 43202\n        )\n\n        # Extract and process posterior samples for 'p'\n        bayes_est_p &lt;- as.vector(fit_bernoulli$draws(variables = \"p\"))\n        bayes_theta_est &lt;- mean(bayes_est_p)\n        hdi_bounds &lt;- quantile(bayes_est_p, probs = c(0.025, 0.975))\n\n        # Collect results\n        results[[paste(subj, n_items)]] &lt;- data.frame(\n            subj_num = subj,\n            n_items = n_items,\n            theta = theta,\n            bayes_theta = bayes_theta_est,\n            bayes_lo = hdi_bounds[1],\n            bayes_hi = hdi_bounds[2]\n        )\n    }\n}\n\nCombiniamo tutti i risultati in un singolo data frame.\n\nall_results &lt;- bind_rows(results)\nall_results |&gt; head()\n\n\nA data.frame: 6 x 6\n\n\n\nsubj_num\nn_items\ntheta\nbayes_theta\nbayes_lo\nbayes_hi\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n2.5%...1\n1\n10\n0.637\n0.576\n0.311\n0.819\n\n\n2.5%...2\n2\n10\n0.717\n0.500\n0.241\n0.754\n\n\n2.5%...3\n3\n10\n0.741\n0.731\n0.472\n0.926\n\n\n2.5%...4\n4\n10\n0.637\n0.577\n0.315\n0.821\n\n\n2.5%...5\n5\n10\n0.690\n0.499\n0.237\n0.760\n\n\n2.5%...6\n6\n10\n0.693\n0.654\n0.385\n0.878\n\n\n\n\n\nCreiamo un grafico con i risultati ottenuti.\n\nggplot(all_results, aes(x = theta, y = bayes_theta)) +\n    geom_point() +\n    geom_errorbar(aes(ymin = bayes_lo, ymax = bayes_hi), width = 0.02) +\n    geom_hline(yintercept = 0.7, linetype = \"dashed\", color = \"gray\") + # Add dashed line at y = 0.7\n    # facet_wrap(~n_items, scales = \"free_x\", ncol = 1) + # Separate panels for each n_items, with a common y-axis\n    facet_wrap(c(\"n_items\"), nrow = 1) +\n    labs(x = \"True Theta\", y = \"Estimated p\") +\n    ggtitle(\"Estimated p vs. True Theta \\nfor Different Numbers of Items\") \n\n\n\n\n\n\n\n\nLo scopo di questa simulazione è quello di confrontare i risultati del modello gerarchico bayesiano con i risultati ottenuti mediante la tecnica di Kelly. Per gli stessi dati utilizzati nel modello gerarchico bayesiamo, calcoliamo dunque la stima dei punteggi veri e gli intervalli di confidenza al 95% secondo il metodo di Kelley.\nLa formula di Kelley per stimare i punteggi veri dai punteggi osservati coinvolge l’affidabilità del test e la media e la deviazione standard dei punteggi osservati:\n\\[\n\\text{Punteggio Vero} = \\text{Media} + (\\text{Affidabilità}) \\times (\\text{Punteggio Osservato} - \\text{Media}).\n\\]\nPer calcolare il CI al 95% per i punteggi veri, dobbiamo tener conto dell’errore standard di misurazione, che deriva dall’affidabilità del test:\n\\[\n\\text{SEM} = \\sigma \\times \\sqrt{1 - \\text{Affidabilità}},\n\\]\ndove $ $ è la deviazione standard dei punteggi osservati.\nDate la stima di SEM, l’intervallo di confidenza al 95% per il punteggio vero di un individuo può essere calcolato come segue:\n\\[\n\\text{CI} = \\text{Punteggio Vero} \\pm (1.96 \\times \\text{SEM}).\n\\]\nSvolgiamo ora i calcoli in R.\n\n# Assuming a reliability coefficient\nr_xx &lt;- 0.8\nZ_alpha &lt;- qnorm(0.975) # For a 95% CI\n\n# Calculate estimated true scores and CIs\nall_results$kelley_true_score &lt;- all_results$bayes_theta\nall_results$kelley_lo &lt;- all_results$bayes_theta - (Z_alpha * sqrt(1 - r_xx) * sd(all_results$bayes_theta))\nall_results$kelley_hi &lt;- all_results$bayes_theta + (Z_alpha * sqrt(1 - r_xx) * sd(all_results$bayes_theta))\n\nA questo punto possiamo generare un grarico che contiene sia la stima del punteggio vero basata sul metodo di Kelley, insieme all’intervallo di confidenza al 95% (colore grigio), sia le stime bayesiane trovate in precedenza (colore blue).\nPer semplicità, ho solo considerato il caso in cui la stima di Kelley si riferisce al caso di 100 items.\n\nggplot() +\n    geom_point(data = all_results, aes(x = theta - 0.02, y = bayes_theta, color = \"Bayesian Estimate\")) +\n    geom_errorbar(data = all_results, aes(x = theta - 0.02, ymin = bayes_lo, ymax = bayes_hi, color = \"Bayesian Estimate\"), width = 0.02) +\n    geom_point(data = all_results, aes(x = theta, y = kelley_true_score, color = \"Kelley's Estimate\")) +\n    geom_errorbar(data = all_results, aes(x = theta, ymin = kelley_lo, ymax = kelley_hi, color = \"Kelley's Estimate\"), width = 0.02) +\n    geom_hline(yintercept = 0.7, linetype = \"dashed\", color = \"gray\") +\n    facet_wrap(c(\"n_items\"), nrow = 1) +\n    labs(x = \"True Theta\", y = \"Estimated Score\") +\n    ggtitle(\"Estimated Scores vs. True Theta\\nfor Different Numbers of Items\") +\n    scale_color_manual(values = c(\"Bayesian Estimate\" = \"blue\", \"Kelley's Estimate\" = \"darkgray\"))\n\n\n\n\n\n\n\n\nI risultati della simulazione completa sono riportati nella figura seguente.\n\n\n\n\n\n\nFigura 13.1\n\n\n\nI risultati della simulazione indicano che le stime medie a posteriori del modello bayesiano, così come gli intervalli di credibilità al 95% (definiti come intervalli di densità di probabilità più elevata), mostrano una notevole congruenza con le stime corrispondenti dei punteggi veri ottenute mediante la regressione di Kelley, insieme ai relativi intervalli di confidenza al 95%. Le stime puntuali prodotte da entrambi i metodi risultano quasi sovrapponibili. Considerando che i punteggi veri derivanti dalla regressione di Kelley posseggono un’interpretazione bayesiana, la similitudine tra i risultati non dovrebbe sorprendere eccessivamente. Tuttavia, una conferma empirica di questa corrispondenza fornisce una validazione più robusta.\nQuesto esempio illustra come i modelli bayesiani gerarchici siano capaci di generare stime dei “punteggi veri” comparabili a quelle prodotte dalla teoria classica dei test, offrendo l’ulteriore vantaggio di non richiedere il calcolo dell’affidabilità per giungere a tali stime. Al contrario, l’approccio bayesiano si basa sull’adozione di assunzioni generative e distribuzionali riguardo le relazioni sia tra i parametri del modello a diversi livelli (ad esempio, la struttura gerarchica delinea le connessioni tra i parametri individuali e quelli di gruppo) sia con i dati osservati. In questo modo, adottando la media posteriore come stima dell’aspettativa dei parametri a livello individuale, siamo in grado di ottenere le stime più accurate dei parametri reali che sottendono la generazione dei dati osservati.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Applicazioni della CTT</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/06_ctt_applications.html#riflessioni-conclusive",
    "href": "chapters/ctt/06_ctt_applications.html#riflessioni-conclusive",
    "title": "13  Applicazioni della CTT",
    "section": "13.6 Riflessioni Conclusive",
    "text": "13.6 Riflessioni Conclusive\nIn questo capitolo, abbiamo analizzato diverse applicazioni pratiche della CTT. Ci siamo concentrati sulla comprensione dei concetti di attenuazione e sul metodo per determinare il numero di item necessari per ottenere un livello desiderato di affidabilità. Inoltre, abbiamo esaminato come stimare i punteggi veri individuali utilizzando due approcci differenti: la regressione di Kelley basata sulla CTT e la regressione gerarchica bayesiana. Approfondire questi argomenti ci ha permesso di ottenere una visione più completa e concreta sull’utilizzo e sull’applicazione della CTT, migliorando la nostra comprensione dei concetti chiave e delle implicazioni pratiche della teoria.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Applicazioni della CTT</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/06_ctt_applications.html#session-info",
    "href": "chapters/ctt/06_ctt_applications.html#session-info",
    "title": "13  Applicazioni della CTT",
    "section": "13.7 Session Info",
    "text": "13.7 Session Info\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] parallel  stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] doParallel_1.0.17   iterators_1.0.14    cmdstanr_0.8.1.9000\n [4] truncnorm_1.0-9     ggridges_0.5.6      foreach_1.5.2      \n [7] modelsummary_2.2.0  MASS_7.3-61         viridis_0.6.5      \n[10] viridisLite_0.4.2   ggpubr_0.6.0        ggExtra_0.10.1     \n[13] gridExtra_2.3       patchwork_1.3.0     bayesplot_1.11.1   \n[16] semTools_0.5-6      semPlot_1.1.6       lavaan_0.6-19      \n[19] psych_2.4.6.26      scales_1.3.0        markdown_1.13      \n[22] knitr_1.49          lubridate_1.9.3     forcats_1.0.0      \n[25] stringr_1.5.1       dplyr_1.1.4         purrr_1.0.2        \n[28] readr_2.1.5         tidyr_1.3.1         tibble_3.2.1       \n[31] ggplot2_3.5.1       tidyverse_2.0.0     here_1.0.1         \n\nloaded via a namespace (and not attached):\n  [1] tensorA_0.36.2.1     rstudioapi_0.17.1    jsonlite_1.8.9      \n  [4] magrittr_2.0.3       TH.data_1.1-2        estimability_1.5.1  \n  [7] farver_2.1.2         nloptr_2.1.1         rmarkdown_2.29      \n [10] vctrs_0.6.5          Cairo_1.6-2          minqa_1.2.8         \n [13] base64enc_0.1-3      rstatix_0.7.2        htmltools_0.5.8.1   \n [16] distributional_0.5.0 broom_1.0.7          Formula_1.2-5       \n [19] htmlwidgets_1.6.4    plyr_1.8.9           sandwich_3.1-1      \n [22] emmeans_1.10.5       zoo_1.8-12           uuid_1.2-1          \n [25] igraph_2.1.1         mime_0.12            lifecycle_1.0.4     \n [28] pkgconfig_2.0.3      Matrix_1.7-1         R6_2.5.1            \n [31] fastmap_1.2.0        shiny_1.9.1          digest_0.6.37       \n [34] OpenMx_2.21.13       fdrtool_1.2.18       colorspace_2.1-1    \n [37] ps_1.8.1             rprojroot_2.0.4      Hmisc_5.2-0         \n [40] labeling_0.4.3       fansi_1.0.6          timechange_0.3.0    \n [43] abind_1.4-8          compiler_4.4.2       withr_3.0.2         \n [46] glasso_1.11          htmlTable_2.4.3      backports_1.5.0     \n [49] carData_3.0-5        ggsignif_0.6.4       corpcor_1.6.10      \n [52] gtools_3.9.5         tools_4.4.2          pbivnorm_0.6.0      \n [55] foreign_0.8-87       zip_2.3.1            httpuv_1.6.15       \n [58] nnet_7.3-19          glue_1.8.0           quadprog_1.5-8      \n [61] promises_1.3.0       nlme_3.1-166         lisrelToR_0.3       \n [64] grid_4.4.2           pbdZMQ_0.3-13        checkmate_2.3.2     \n [67] cluster_2.1.6        reshape2_1.4.4       generics_0.1.3      \n [70] gtable_0.3.6         tzdb_0.4.0           data.table_1.16.2   \n [73] hms_1.1.3            car_3.1-3            utf8_1.2.4          \n [76] tables_0.9.31        sem_3.1-16           pillar_1.9.0        \n [79] IRdisplay_1.1        rockchalk_1.8.157    posterior_1.6.0     \n [82] later_1.3.2          splines_4.4.2        lattice_0.22-6      \n [85] survival_3.7-0       kutils_1.73          tidyselect_1.2.1    \n [88] miniUI_0.1.1.1       pbapply_1.7-2        stats4_4.4.2        \n [91] xfun_0.49            qgraph_1.9.8         arm_1.14-4          \n [94] stringi_1.8.4        pacman_0.5.1         boot_1.3-31         \n [97] evaluate_1.0.1       codetools_0.2-20     mi_1.1              \n[100] cli_3.6.3            RcppParallel_5.1.9   IRkernel_1.3.2      \n[103] rpart_4.1.23         xtable_1.8-4         processx_3.8.4      \n[106] repr_1.1.7           munsell_0.5.1        Rcpp_1.0.13-1       \n[109] coda_0.19-4.1        png_0.1-8            XML_3.99-0.17       \n[112] jpeg_0.1-10          lme4_1.1-35.5        mvtnorm_1.3-2       \n[115] insight_0.20.5       openxlsx_4.2.7.1     crayon_1.5.3        \n[118] rlang_1.1.4          multcomp_1.4-26      mnormt_2.1.1        \n\n\n\n\n\n\nBlampied, N. M. (2022). Reliable change and the reliable change index: still useful after all these years? The Cognitive Behaviour Therapist, 15, e50.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Applicazioni della CTT</span>"
    ]
  },
  {
    "objectID": "chapters/raters/01_multilevel.html",
    "href": "chapters/raters/01_multilevel.html",
    "title": "14  Modelli multilivello",
    "section": "",
    "text": "14.1 Introduzione\nI modelli multilivello, anche denominati modelli gerarchici o a effetti misti, costituiscono un approccio statistico avanzato progettato per analizzare dati organizzati in strutture gerarchiche o nidificate. Tali modelli consentono di modellare simultaneamente variazioni a diversi livelli di aggregazione, come il livello individuale e quello di gruppo, migliorando la precisione dell’inferenza e la comprensione delle dinamiche sottostanti ai dati.\nIn psicologia, i modelli multilivello rivestono un ruolo fondamentale, poiché i dati spesso derivano da contesti complessi in cui fattori individuali e ambientali interagiscono. Per esempio, nell’analisi delle prestazioni cognitive o delle risposte emotive, questi modelli permettono di distinguere le variazioni attribuibili a caratteristiche individuali (ad es., abilità cognitiva, tratti di personalità) da quelle dovute a fattori ambientali (ad es., clima scolastico, dinamiche familiari).\nLe principali applicazioni dei modelli multilivello includono:",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Modelli multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/raters/01_multilevel.html#introduzione",
    "href": "chapters/raters/01_multilevel.html#introduzione",
    "title": "14  Modelli multilivello",
    "section": "",
    "text": "Analisi di dati longitudinali: Gestiscono la dipendenza seriale introdotta da misurazioni ripetute sugli stessi soggetti, migliorando l’accuratezza nell’estimare gli effetti temporali.\n\nStudio dell’impatto di fattori contestuali: Permettono di quantificare l’influenza dell’ambiente su variabili psicologiche, fornendo stime che isolano gli effetti specifici dei contesti.\n\nRappresentazione della variabilità intra- e inter-individuale: Offrono una modellazione flessibile che cattura sia le differenze tra individui sia le fluttuazioni all’interno dello stesso individuo, riflettendo fedelmente la complessità dei processi psicologici.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Modelli multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/raters/01_multilevel.html#un-esempio-concreto-analisi-dei-dati-di-deprivazione-del-sonno",
    "href": "chapters/raters/01_multilevel.html#un-esempio-concreto-analisi-dei-dati-di-deprivazione-del-sonno",
    "title": "14  Modelli multilivello",
    "section": "\n14.2 Un Esempio Concreto: Analisi dei Dati di Deprivazione del Sonno",
    "text": "14.2 Un Esempio Concreto: Analisi dei Dati di Deprivazione del Sonno\nQuesto capitolo illustra l’applicazione pratica dei modelli multilivello nell’analisi dei dati di uno studio sperimentale che ha investigato l’effetto della deprivazione del sonno sulle prestazioni psicomotorie. I dati analizzati provengono dalla ricerca di Belenky et al. (2003), che ha esaminato gli effetti cumulativi della restrizione del sonno.\n\n14.2.1 Accesso ai Dati\nI dati utilizzati sono inclusi nel dataset sleepstudy, disponibile nel pacchetto lme4 di R (Bates et al., 2014):\n\ndata(sleepstudy)\n\nIl dataset è costituito da 180 osservazioni e contiene tre variabili principali:\n\n\nReaction: Tempo medio di reazione (in millisecondi).\n\nDays: Numero di giorni consecutivi di deprivazione del sonno.\n\nSubject: Identificativo del partecipante.\n\n14.2.2 Struttura del Dataset\nIl dataset presenta una tipica struttura multilivello: dati longitudinali con misurazioni ripetute del tempo medio di reazione (variabile dipendente) raccolte dai medesimi partecipanti nell’arco di dieci giorni. Tale configurazione è comune in psicologia sperimentale, dove è frequente analizzare variazioni intra-individuali e inter-individuali.\nI 18 partecipanti dello studio sono stati sottoposti a una restrizione del sonno di tre ore per notte. Ogni giorno, per dieci giorni consecutivi, hanno eseguito un “test di vigilanza psicomotoria” di dieci minuti. Durante il test, i partecipanti monitoravano uno schermo e premevano un pulsante il più rapidamente possibile alla comparsa di uno stimolo visivo. La variabile di interesse è il tempo medio di reazione (RT), utilizzato come indicatore delle prestazioni psicomotorie.\n\n14.2.3 Analisi Esplorativa\n\n14.2.3.1 Visualizzazione per un Singolo Partecipante\nPer iniziare, si può esaminare l’andamento del tempo medio di reazione per un singolo partecipante. Ad esempio, consideriamo il soggetto identificato con 308:\n\njust_308 &lt;- sleepstudy |&gt;\n    filter(Subject == \"308\")\n\nggplot(just_308, aes(x = Days, y = Reaction)) +\n    geom_point(size = 2.5) +\n    scale_x_continuous(breaks = 0:9) +\n    labs(title = \"Andamento del tempo di reazione (Soggetto 308)\", \n         x = \"Giorni di deprivazione del sonno\", \n         y = \"Tempo medio di reazione (ms)\"\n    )\n\n\n\n\n\n\n\n\n14.2.3.2 Visualizzazione per Tutti i Partecipanti\nPer un’analisi più completa, è utile rappresentare i dati di tutti i 18 partecipanti in un’unica visualizzazione, evidenziando le differenze individuali nelle prestazioni psicomotorie durante il periodo di studio:\n\nggplot(sleepstudy, aes(x = Days, y = Reaction)) +\n    geom_point(alpha = 0.6) +\n    scale_x_continuous(breaks = 0:9) +\n    facet_wrap(~Subject) +\n    labs(title = \"Andamento del tempo di reazione per tutti i partecipanti\", \n         x = \"Giorni di deprivazione del sonno\", \n         y = \"Tempo medio di reazione (ms)\"\n    )\n\n\n\n\n\n\n\nQueste visualizzazioni preliminari evidenziano sia le tendenze generali che le differenze individuali nel tempo di reazione in risposta alla deprivazione del sonno. Tali pattern saranno ulteriormente analizzati utilizzando modelli multilivello, che consentono di quantificare le variazioni intra-individuali e inter-individuali, isolando l’effetto cumulativo della deprivazione del sonno.\n\n14.2.4 Descrizione del Disegno Sperimentale\n\nFase di Adattamento e Baseline: Lo studio è iniziato con tre giorni preliminari dedicati all’adattamento e all’addestramento (T1 e T2) seguiti dalla misurazione baseline (B). Durante questi giorni, i partecipanti hanno mantenuto un regime di sonno controllato, trascorrendo 8 ore a letto ogni notte (dalle 23:00 alle 07:00). Lo scopo di questa fase era garantire una condizione standardizzata e ridurre la variabilità individuale dovuta a precedenti abitudini di sonno.\nCondizioni di Restrizione e Prolungamento del Sonno: A partire dal quarto giorno, i partecipanti sono stati assegnati a diverse condizioni sperimentali per un periodo di sette giorni consecutivi (E1-E7). La durata del tempo trascorso a letto (Time in Bed, TIB) è stata manipolata e variava da un minimo di 3 ore a un massimo di 9 ore, al fine di indurre stati progressivi di deprivazione o recupero del sonno.\n\nNella codifica temporale dello studio: - I giorni 0 e 1 rappresentano la fase di adattamento e addestramento. - Il giorno 2 corrisponde alla misurazione baseline, che riflette le prestazioni psicomotorie in condizioni di sonno ottimale. - I giorni 3-9 rappresentano il periodo sperimentale, durante il quale è stata applicata la manipolazione del sonno.\nPer l’analisi, è fondamentale considerare il giorno di baseline (2) come punto di riferimento per valutare gli effetti della manipolazione del sonno sulle prestazioni. I giorni di adattamento e addestramento (0 e 1) devono essere esclusi dall’analisi, poiché le variazioni osservate in questa fase sono attribuibili all’acclimatazione ai protocolli dello studio e non alle condizioni sperimentali di sonno. L’esclusione di questi giorni garantisce che le stime dell’effetto siano focalizzate esclusivamente sull’impatto della restrizione o del prolungamento del sonno.\n\n14.2.5 Preparazione dei Dati\nLa preparazione dei dati è un passaggio essenziale per garantire che l’analisi rifletta accuratamente gli effetti della manipolazione sperimentale. I seguenti passaggi sono stati eseguiti:\n\nRimozione delle Osservazioni Iniziali\nLe osservazioni relative ai giorni 0 e 1, corrispondenti alla fase di adattamento e addestramento, sono state escluse dal dataset. Questo assicura che l’analisi si concentri esclusivamente sul periodo di interesse, ovvero il baseline (giorno 2) e i successivi giorni di manipolazione del sonno.\nCreazione di una Variabile Ricodificata\nÈ stata generata una nuova variabile, days_deprived, per rappresentare il numero di giorni di privazione del sonno a partire dal giorno baseline (giorno 2). La ricodifica inizia con 0 per il giorno baseline, 1 per il primo giorno di privazione (giorno 3), e così via. Questa variabile facilita la lettura e l’interpretazione dei risultati nel contesto del protocollo sperimentale.\nSalvataggio del Dataset Modificato\nIl dataset modificato è stato denominato sleep2 e ora include solo le osservazioni rilevanti per l’analisi, con una chiara distinzione tra il baseline e i giorni di privazione del sonno.\n\nEcco il codice per implementare questi passaggi:\n\n# Filtraggio e creazione della nuova variabile\nsleep2 &lt;- sleepstudy |&gt;\n    filter(Days &gt;= 2L) |&gt;  # Escludiamo i giorni 0 e 1\n    mutate(days_deprived = Days - 2L)  # Ricodifichiamo la variabile Days\n\n\n\nVerifica della Ricodifica\nLa correttezza della ricodifica è stata verificata calcolando il numero di osservazioni per ciascun valore di days_deprived e confrontandolo con i valori originali della variabile Days:\n\n\nsleep2 |&gt;\n    count(days_deprived, Days)\n#&gt;   days_deprived Days  n\n#&gt; 1             0    2 18\n#&gt; 2             1    3 18\n#&gt; 3             2    4 18\n#&gt; 4             3    5 18\n#&gt; 5             4    6 18\n#&gt; 6             5    7 18\n#&gt; 7             6    8 18\n#&gt; 8             7    9 18\n\n\n\nVisualizzazione dei Dati\nPer esplorare le tendenze individuali nel tempo di reazione in funzione dei giorni di privazione del sonno, è stato creato un grafico a dispersione con le seguenti caratteristiche:\n\nL’asse x rappresenta i giorni di privazione del sonno, con 0 corrispondente al baseline.\nL’asse y rappresenta il tempo medio di reazione (Reaction).\nOgni partecipante è visualizzato separatamente tramite il wrapping dei facet.\n\n\n\n\nggplot(sleep2, aes(x = days_deprived, y = Reaction)) +\n    geom_point() +\n    scale_x_continuous(breaks = 0:7) +\n    facet_wrap(~Subject) +\n    labs(\n        y = \"Tempo medio di reazione (ms)\",\n        x = \"Giorni di privazione del sonno (0 = baseline)\",\n        title = \"Andamento del tempo di reazione durante la privazione del sonno\"\n    )\n\n\n\n\n\n\n\nQuesta preparazione consente un’analisi più mirata, isolando gli effetti della privazione del sonno e semplificando l’interpretazione delle dinamiche temporali.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Modelli multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/raters/01_multilevel.html#relazione-tra-tempo-di-reazione-e-privazione-del-sonno",
    "href": "chapters/raters/01_multilevel.html#relazione-tra-tempo-di-reazione-e-privazione-del-sonno",
    "title": "14  Modelli multilivello",
    "section": "\n14.3 Relazione tra Tempo di Reazione e Privazione del Sonno",
    "text": "14.3 Relazione tra Tempo di Reazione e Privazione del Sonno\nL’analisi dei dati relativi alla privazione del sonno rivela che, con una singola eccezione (il soggetto 335), il tempo medio di reazione tende ad aumentare progressivamente con ogni giorno di privazione aggiuntivo. Questo trend suggerisce che un modello di regressione lineare potrebbe essere utile per descrivere le prestazioni di ciascun partecipante.\nLa regressione lineare è definita dalla seguente equazione:\n\\[\nE(Y) = \\beta_0 + \\beta_1 X,\n\\]\ndove:\n\n\n\\(Y\\) rappresenta la variabile dipendente (il tempo di reazione medio),\n\n\\(\\beta_0\\) è l’intercetta, ovvero il tempo di reazione stimato al giorno iniziale (baseline, senza privazione del sonno),\n\n\\(\\beta_1\\) è il coefficiente di pendenza, che rappresenta la variazione stimata nel tempo di reazione per ogni giorno di privazione aggiuntivo,\n\n\\(X\\) indica il numero di giorni di privazione del sonno.\n\nI parametri \\(\\beta_0\\) e \\(\\beta_1\\) vengono stimati dai dati, e la loro interpretazione fornisce informazioni chiave sulle dinamiche delle prestazioni psicomotorie.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Modelli multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/raters/01_multilevel.html#scelta-del-modello-di-regressione",
    "href": "chapters/raters/01_multilevel.html#scelta-del-modello-di-regressione",
    "title": "14  Modelli multilivello",
    "section": "\n14.4 Scelta del Modello di Regressione",
    "text": "14.4 Scelta del Modello di Regressione\nUn aspetto cruciale nell’analisi è decidere quale struttura modellistica adottare per descrivere i dati. Si devono considerare le seguenti opzioni:\n\nComplete Pooling\nQuesto approccio utilizza un unico modello di regressione lineare per tutti i partecipanti, assumendo che la relazione tra privazione del sonno e tempo di reazione sia identica per tutti. In pratica, si stima una singola intercetta (\\(\\beta_0\\)) e una singola pendenza (\\(\\beta_1\\)) comuni a tutti i soggetti. Sebbene semplice, questo metodo ignora completamente le differenze individuali.\nNo Pooling\nQui, si stima un modello di regressione lineare separato per ciascun partecipante, consentendo a ogni soggetto di avere una propria intercetta e una propria pendenza. Questo approccio riconosce pienamente le variazioni individuali, ma potrebbe risultare eccessivamente complesso e sensibile al rumore nei dati, specialmente con pochi punti di osservazione per soggetto.\nPartial Pooling\nQuesto approccio intermedio, spesso implementato attraverso modelli multilivello, bilancia i due estremi sopra descritti. Si assume una relazione media condivisa tra i soggetti (ad esempio, una pendenza media), ma si consente una variazione individuale attorno a questa media. Il partial pooling sfrutta le informazioni condivise tra i partecipanti, migliorando la robustezza delle stime, specialmente in presenza di pochi dati per soggetto.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Modelli multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/raters/01_multilevel.html#complete-pooling",
    "href": "chapters/raters/01_multilevel.html#complete-pooling",
    "title": "14  Modelli multilivello",
    "section": "\n14.5 Complete pooling",
    "text": "14.5 Complete pooling\nL’approccio di “complete pooling” in analisi statistica implica l’utilizzo di un modello che calcola un’unica intercetta e una sola pendenza per l’intero dataset. Questo metodo si basa sull’ipotesi che tutti i soggetti nel dataset condividano le stesse caratteristiche di base riguardo alla relazione tra la variabile dipendente e indipendente.\nQuesto approccio non tiene conto delle possibili differenze individuali nelle intercette o nelle pendenze tra i diversi soggetti. Ad esempio, ignorara come ciascun soggetto reagisce in modo diverso alla privazione del sonno.\nDall’analisi preliminare dei dati, abbiamo notato che l’approccio di complete pooling potrebbe non essere adatto per il nostro studio. La visualizzazione dei dati suggerisce che ogni partecipante potrebbe avere una propria relazione unica tra il tempo di reazione e i giorni di privazione del sonno, indicando la necessità di valori individuali per le intercette e le pendenze.\n\n14.5.1 Modello di Regressione Lineare in Complete Pooling\nIl modello generale lineare (GLM) per l’approccio di complete pooling è formulato come segue:\n\\[\nY_{sd} = \\beta_0 + \\beta_1 X_{sd} + e_{sd},\n\\]\ndove:\n\n\n\\(Y_{sd}\\) rappresenta il tempo di reazione medio del soggetto \\(s\\) nel giorno \\(d\\).\n\n\\(X_{sd}\\) è il numero di giorni di privazione del sonno (variabile days_deprived), che varia da 0 a 7.\n\n\\(e_{sd}\\) è il termine di errore, che rappresenta le fluttuazioni casuali non spiegate dal modello.\n\nPer adattare questo modello in R, si utilizza la funzione lm():\n\ncp_model &lt;- lm(Reaction ~ days_deprived, sleep2)\nsummary(cp_model)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Reaction ~ days_deprived, data = sleep2)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -112.28  -26.73    2.14   27.73  140.45 \n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error t value Pr(&gt;|t|)\n#&gt; (Intercept)     267.97       7.74   34.63  &lt; 2e-16\n#&gt; days_deprived    11.44       1.85    6.18  6.3e-09\n#&gt; \n#&gt; Residual standard error: 50.9 on 142 degrees of freedom\n#&gt; Multiple R-squared:  0.212,  Adjusted R-squared:  0.207 \n#&gt; F-statistic: 38.2 on 1 and 142 DF,  p-value: 6.32e-09\n\nIl modello di regressione che abbiamo considerato offre una stima del tempo di risposta medio per i soggetti allo studio al Giorno 0 (prima della privazione del sonno) e la variazione media del tempo di risposta per ogni giorno aggiuntivo di privazione. Secondo questo modello, il tempo di risposta medio iniziale è stimato essere di circa 268 millisecondi, con un incremento medio di circa 11 millisecondi per ogni giorno successivo di privazione del sonno.\nÈ importante notare, tuttavia, che questo modello potrebbe avere delle limitazioni nella sua applicabilità:\n\n\nAssunzione di Indipendenza: Il modello assume che tutte le osservazioni siano indipendenti. Questa assunzione potrebbe non essere valida nel nostro studio, dato che le osservazioni provengono da misurazioni ripetute sugli stessi soggetti.\n\nErrori Standard dei Coefficienti: La presunta indipendenza delle osservazioni implica che gli errori standard dei coefficienti di regressione potrebbero non essere completamente affidabili.\n\n14.5.2 Visualizzazione\nPer visualizzare meglio questi risultati, possiamo aggiungere le previsioni del modello al grafico che abbiamo già creato. Utilizziamo la funzione geom_abline() di R per tracciare la linea di regressione stimata direttamente sul grafico esistente:\n\n\nUtilizzo di geom_abline(): Questa funzione ci permette di aggiungere una linea di regressione al grafico, specificando l’intercetta e la pendenza.\n\nCoefficienti del Modello: Utilizziamo coef(cp_model) per ottenere i coefficienti di regressione (intercetta e pendenza) dal nostro modello. Questa funzione restituisce un vettore con due elementi corrispondenti all’intercetta e alla pendenza, che possono essere poi utilizzati per definire la linea nel grafico.\n\n\nggplot(sleep2, aes(x = days_deprived, y = Reaction)) +\n    geom_abline(\n        intercept = coef(cp_model)[1],\n        slope = coef(cp_model)[2],\n        color = \"blue\"\n    ) +\n    geom_point() +\n    scale_x_continuous(breaks = 0:7) +\n    facet_wrap(~Subject) +\n    labs(y = \"Reaction Time\", x = \"Days deprived of sleep (0 = baseline)\")\n\n\n\n\n\n\n\nDall’analisi effettuata, emerge che il modello attuale non si adatta in modo ottimale ai dati raccolti. Questa situazione indica la necessità di esplorare un approccio diverso per modellare in modo più accurato le relazioni presenti nei dati.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Modelli multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/raters/01_multilevel.html#approccio-di-no-pooling",
    "href": "chapters/raters/01_multilevel.html#approccio-di-no-pooling",
    "title": "14  Modelli multilivello",
    "section": "\n14.6 Approccio di No Pooling",
    "text": "14.6 Approccio di No Pooling\nIn alternativa al modello di “complete pooling”, consideriamo l’approccio di “no pooling”. Questo approccio si basa sull’idea di adattare modelli di regressione separati per ogni partecipante, trattando ogni individuo come un’entità distinta.\n\n14.6.1 Caratteristiche del No Pooling\n\n\nIndipendenza delle Stime: In questo approccio, ogni partecipante ha il proprio set di stime per l’intercetta e la pendenza. Le stime relative a un partecipante non sono influenzate dalle stime degli altri.\n\nStime Individualizzate: Si stima separatamente una coppia di intercetta/pendenza per ciascuno dei 18 partecipanti, riconoscendo la possibilità di variazioni significative nelle risposte individuali.\n\n14.6.2 Implementazione del Modello di No Pooling\nEsistono due modi principali per implementare questo approccio:\n\n\nRegressioni Separate per Ogni Partecipante: Eseguire una serie di regressioni lineari individuali, una per ogni soggetto.\n\nModello di Regressione Unificato con Effetti Principali e Interazione: Utilizzare un unico modello di regressione che includa sia gli effetti principali sia l’interazione tra le variabili Subject (soggetto) e Day (giorno). Questo metodo permette di includere tutte le stime in un unico modello.\n\nPer il secondo approccio, è necessario considerare le seguenti fasi:\n\n\nCreazione di Variabili Dummy per il Fattore Subject: Poiché Subject ha 18 livelli, saranno necessarie 17 variabili dummy per rappresentare questi livelli. In R, questo può essere fatto automaticamente definendo Subject come un fattore.\n\nIncludere Subject come Fattore nel Modello: Aggiungere Subject, definito come un fattore, come predittore nel modello. L’inclusione dell’interazione tra Subject e days_deprived permette variazioni nelle intercette e nelle pendenze tra i soggetti.\n\nPrima di procedere, è importante assicurarsi che Subject sia definito correttamente come un fattore. Questo può essere verificato utilizzando la funzione summary() in R, che fornisce una sintesi delle caratteristiche della variabile, compreso se è trattata come un fattore.\n\nsleep2 |&gt; \n    summary()\n#&gt;     Reaction        Days         Subject   days_deprived \n#&gt;  Min.   :203   Min.   :2.00   308    : 8   Min.   :0.00  \n#&gt;  1st Qu.:265   1st Qu.:3.75   309    : 8   1st Qu.:1.75  \n#&gt;  Median :303   Median :5.50   310    : 8   Median :3.50  \n#&gt;  Mean   :308   Mean   :5.50   330    : 8   Mean   :3.50  \n#&gt;  3rd Qu.:348   3rd Qu.:7.25   331    : 8   3rd Qu.:5.25  \n#&gt;  Max.   :466   Max.   :9.00   332    : 8   Max.   :7.00  \n#&gt;                               (Other):96\n\nLa funzione pull() viene utilizzata per estrarre una specifica colonna da un data frame. Con le seguenti istruzioni verifichiamo se la colonna Subject è codificata come factor.\n\nsleep2 |&gt;\n    pull(Subject) |&gt;\n    is.factor()\n#&gt; [1] TRUE\n\nAdattiamo il modello di regressione ai dati. Si noti che la sintassi seguente può essere semplificata utilizzando Reaction ~ days_deprived * Subject.\n\nnp_model &lt;- lm(Reaction ~ days_deprived + Subject + days_deprived:Subject,\n    data = sleep2\n)\n\nsummary(np_model)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Reaction ~ days_deprived + Subject + days_deprived:Subject, \n#&gt;     data = sleep2)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -106.52   -8.54    1.14    8.89  128.55 \n#&gt; \n#&gt; Coefficients:\n#&gt;                          Estimate Std. Error t value Pr(&gt;|t|)\n#&gt; (Intercept)               288.217     16.477   17.49  &lt; 2e-16\n#&gt; days_deprived              21.690      3.939    5.51  2.5e-07\n#&gt; Subject309                -87.926     23.302   -3.77  0.00026\n#&gt; Subject310                -62.286     23.302   -2.67  0.00869\n#&gt; Subject330                -14.953     23.302   -0.64  0.52242\n#&gt; Subject331                  9.966     23.302    0.43  0.66974\n#&gt; Subject332                 27.816     23.302    1.19  0.23522\n#&gt; Subject333                 -2.758     23.302   -0.12  0.90600\n#&gt; Subject334                -50.205     23.302   -2.15  0.03342\n#&gt; Subject335                -25.343     23.302   -1.09  0.27921\n#&gt; Subject337                 24.614     23.302    1.06  0.29319\n#&gt; Subject349                -59.218     23.302   -2.54  0.01246\n#&gt; Subject350                -40.202     23.302   -1.73  0.08734\n#&gt; Subject351                -24.247     23.302   -1.04  0.30042\n#&gt; Subject352                 43.065     23.302    1.85  0.06732\n#&gt; Subject369                -21.504     23.302   -0.92  0.35815\n#&gt; Subject370                -53.307     23.302   -2.29  0.02411\n#&gt; Subject371                -30.490     23.302   -1.31  0.19350\n#&gt; Subject372                  2.477     23.302    0.11  0.91554\n#&gt; days_deprived:Subject309  -17.333      5.570   -3.11  0.00238\n#&gt; days_deprived:Subject310  -17.792      5.570   -3.19  0.00184\n#&gt; days_deprived:Subject330  -13.685      5.570   -2.46  0.01561\n#&gt; days_deprived:Subject331  -16.823      5.570   -3.02  0.00315\n#&gt; days_deprived:Subject332  -19.295      5.570   -3.46  0.00076\n#&gt; days_deprived:Subject333  -10.815      5.570   -1.94  0.05480\n#&gt; days_deprived:Subject334   -3.575      5.570   -0.64  0.52242\n#&gt; days_deprived:Subject335  -25.899      5.570   -4.65  9.5e-06\n#&gt; days_deprived:Subject337    0.752      5.570    0.13  0.89289\n#&gt; days_deprived:Subject349   -5.264      5.570   -0.95  0.34673\n#&gt; days_deprived:Subject350    1.601      5.570    0.29  0.77438\n#&gt; days_deprived:Subject351  -13.168      5.570   -2.36  0.01987\n#&gt; days_deprived:Subject352  -14.402      5.570   -2.59  0.01106\n#&gt; days_deprived:Subject369   -7.895      5.570   -1.42  0.15927\n#&gt; days_deprived:Subject370   -1.049      5.570   -0.19  0.85091\n#&gt; days_deprived:Subject371   -9.344      5.570   -1.68  0.09633\n#&gt; days_deprived:Subject372  -10.604      5.570   -1.90  0.05961\n#&gt; \n#&gt; Residual standard error: 25.5 on 108 degrees of freedom\n#&gt; Multiple R-squared:  0.849,  Adjusted R-squared:   0.8 \n#&gt; F-statistic: 17.4 on 35 and 108 DF,  p-value: &lt;2e-16\n\nPer chiarire, il soggetto di riferimento è il 308; in R, la modalità predefinita è quella di ordinare i livelli del fattore in ordine alfabetico e di scegliere il primo come soggetto di riferimento. Questo significa che l’intercetta e la pendenza per il soggetto 308 sono rappresentate rispettivamente da (Intercept) e days_deprived, poiché tutte le altre 17 variabili dummy saranno nulle per il soggetto 308.\nTutti i coefficienti di regressione degli altri soggetti sono rappresentati come scostamenti da questo soggetto di riferimento. Se desideriamo calcolare l’intercetta e la pendenza per un dato soggetto, dobbiamo semplicemente sommare gli scostamenti corrispondenti. Pertanto, abbiamo:\nIntercetta per 308: 288.217\nPendenza per 308: 21.69\nIntercetta per 335: (Intercept) + Subject335 = 288.217 + -25.343 = 262.874\nPendenza per 335: days_deprived + days_deprived:Subject335 = 21.69 + -25.899 = -4.209\nE così via.\nNel modello “no pooling”, non viene stimata un’intercetta e una pendenza complessive per l’intera popolazione; in questo caso, (Intercept) e days_deprived sono stime dell’intercetta e della pendenza per il soggetto 308, che è stato scelto (arbitrariamente) come soggetto di riferimento. Per ottenere stime per l’intera popolazione, è possibile procedere con una seconda fase dell’analisi statistica in cui calcoliamo le medie delle intercette e delle pendenze individuali.\n\ncoef(np_model) |&gt; as.data.frame()\n#&gt;                          coef(np_model)\n#&gt; (Intercept)                     288.217\n#&gt; days_deprived                    21.690\n#&gt; Subject309                      -87.926\n#&gt; Subject310                      -62.286\n#&gt; Subject330                      -14.953\n#&gt; Subject331                        9.966\n#&gt; Subject332                       27.816\n#&gt; Subject333                       -2.758\n#&gt; Subject334                      -50.205\n#&gt; Subject335                      -25.343\n#&gt; Subject337                       24.614\n#&gt; Subject349                      -59.218\n#&gt; Subject350                      -40.202\n#&gt; Subject351                      -24.247\n#&gt; Subject352                       43.065\n#&gt; Subject369                      -21.504\n#&gt; Subject370                      -53.307\n#&gt; Subject371                      -30.490\n#&gt; Subject372                        2.477\n#&gt; days_deprived:Subject309        -17.333\n#&gt; days_deprived:Subject310        -17.792\n#&gt; days_deprived:Subject330        -13.685\n#&gt; days_deprived:Subject331        -16.823\n#&gt; days_deprived:Subject332        -19.295\n#&gt; days_deprived:Subject333        -10.815\n#&gt; days_deprived:Subject334         -3.575\n#&gt; days_deprived:Subject335        -25.899\n#&gt; days_deprived:Subject337          0.752\n#&gt; days_deprived:Subject349         -5.264\n#&gt; days_deprived:Subject350          1.601\n#&gt; days_deprived:Subject351        -13.168\n#&gt; days_deprived:Subject352        -14.402\n#&gt; days_deprived:Subject369         -7.895\n#&gt; days_deprived:Subject370         -1.049\n#&gt; days_deprived:Subject371         -9.344\n#&gt; days_deprived:Subject372        -10.604\n\nCalcoliamo le intercette individuali:\n\nall_intercepts &lt;- c(\n    coef(np_model)[\"(Intercept)\"],\n    coef(np_model)[3:19] + coef(np_model)[\"(Intercept)\"]\n)\n\nCalcliamo le pendenze individuali:\n\nall_slopes &lt;- c(\n    coef(np_model)[\"days_deprived\"],\n    coef(np_model)[20:36] + coef(np_model)[\"days_deprived\"]\n)\n\nCreiamo un DataFrame con le colonne Subject, intercept e slope:\n\nids &lt;- sleep2 |&gt;\n    pull(Subject) |&gt;\n    levels() |&gt;\n    factor()\nprint(ids)\n#&gt;  [1] 308 309 310 330 331 332 333 334 335 337 349 350 351 352 369 370 371 372\n#&gt; 18 Levels: 308 309 310 330 331 332 333 334 335 337 349 350 351 352 ... 372\n\n\n# make a tibble with the data extracted above\nnp_coef &lt;- tibble(\n    Subject = ids,\n    intercept = all_intercepts,\n    slope = all_slopes\n)\n\nprint(np_coef)\n#&gt; # A tibble: 18 × 3\n#&gt;   Subject intercept slope\n#&gt;   &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 308          288. 21.7 \n#&gt; 2 309          200.  4.36\n#&gt; 3 310          226.  3.90\n#&gt; 4 330          273.  8.01\n#&gt; 5 331          298.  4.87\n#&gt; 6 332          316.  2.40\n#&gt; # ℹ 12 more rows\n\nEsaminiamo l’adattamento di questo modello ai dati.\n\nggplot(sleep2, aes(x = days_deprived, y = Reaction)) +\n    geom_abline(\n        data = np_coef,\n        mapping = aes(\n            intercept = intercept,\n            slope = slope\n        ),\n        color = \"blue\"\n    ) +\n    geom_point() +\n    scale_x_continuous(breaks = 0:7) +\n    facet_wrap(~Subject) +\n    labs(y = \"Reaction Time\", x = \"Days deprived of sleep (0 = baseline)\")\n\n\n\n\n\n\n\nQuesta situazione è notevolmente migliorata rispetto al modello di pooling completo. Se desideriamo testare l’ipotesi nulla secondo cui la pendenza della retta di regressione è uguale a zero, possiamo farlo eseguendo un test \\(t\\) di Student sul campione di pendenze individuali.\n\nnp_coef |&gt;\n    pull(slope) |&gt;\n    t.test()\n#&gt; \n#&gt;  One Sample t-test\n#&gt; \n#&gt; data:  pull(np_coef, slope)\n#&gt; t = 6, df = 17, p-value = 1e-05\n#&gt; alternative hypothesis: true mean is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;   7.54 15.33\n#&gt; sample estimates:\n#&gt; mean of x \n#&gt;      11.4\n\nQuesto test suggerisce che la pendenza media di 11.435 è diversa da zero, t(17) = 6.20 p &lt; .001.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Modelli multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/raters/01_multilevel.html#partial-pooling",
    "href": "chapters/raters/01_multilevel.html#partial-pooling",
    "title": "14  Modelli multilivello",
    "section": "\n14.7 Partial Pooling",
    "text": "14.7 Partial Pooling\nNell’analisi dei dati psicologici, la scelta del metodo di pooling rappresenta una decisione cruciale per bilanciare la variabilità individuale con la necessità di sfruttare le informazioni aggregate. Da un lato, l’approccio di complete pooling tratta tutti i dati come se appartenessero a un unico gruppo omogeneo, ignorando completamente le differenze tra individui. Dall’altro lato, l’approccio di no pooling analizza i dati di ciascun partecipante separatamente, senza beneficiare delle informazioni condivise tra i soggetti. Entrambi questi estremi presentano limiti significativi: il complete pooling può occultare variazioni individuali rilevanti, mentre il no pooling rischia di produrre stime instabili e meno robuste, soprattutto in presenza di pochi dati per soggetto.\n\n14.7.1 Il Vantaggio del Partial Pooling\nIl partial pooling, implementato tramite modelli lineari a effetti misti, rappresenta un approccio intermedio che supera queste limitazioni. Questo metodo sfrutta le informazioni condivise tra i partecipanti per migliorare le stime a livello individuale, mantenendo al contempo la capacità di catturare variazioni specifiche. In pratica, il modello:\n\n\nCondivide informazioni tra soggetti: Le stime individuali sono influenzate dalle tendenze generali osservate nel gruppo, evitando così di basarsi esclusivamente sui dati limitati di ciascun partecipante.\n\nPermette variazioni individuali: Ogni soggetto può avere una propria intercetta e pendenza che si discostano dalla media di gruppo, riflettendo le differenze individuali.\n\nQuesto equilibrio consente di ottenere stime che sono sia robuste sia rappresentative, riducendo il rischio di sovradattamento (overfitting) e migliorando la generalizzabilità dei risultati.\n\n14.7.2 Implementazione dei Modelli a Effetti Misti\n\n\nTrattare i Soggetti come Fattori Casuali: Nel partial pooling, i soggetti vengono considerati come un fattore casuale anziché fisso. Ciò implica che i livelli del fattore (i soggetti nel nostro caso) sono visti come un campione casuale da una popolazione più ampia.\n\nModello Lineare a Effetti Misti: Questo tipo di modello statistico consente di includere i fattori casuali nell’analisi. In un modello misto, le stime per ogni soggetto sono “informate” o influenzate dalle informazioni aggregate degli altri soggetti.\n\nShrinkage: Il fenomeno dello shrinkage (restringimento) indica che le stime per ciascun soggetto vengono regolate o “spostate” verso le stime medie della popolazione, permettendo una valutazione più equilibrata e meno influenzata da variazioni estreme o casuali.\n\n14.7.3 Articolazione e Applicazione del Modello Multilivello\nIl modello multilivello descritto cattura le relazioni tra variabili misurate a più livelli, integrando dinamicamente la variabilità individuale e di gruppo. La struttura a due livelli consente di modellare sia le differenze tra soggetti che le relazioni all’interno di ciascun soggetto.\n\n14.7.4 Livello 1: Relazione Individuale\nAl livello individuale, il modello esprime una relazione lineare tra la variabile di risposta \\(Y_{sd}\\) (tempo di reazione) e il predittore \\(X_{sd}\\) (giorni di privazione del sonno):\n\\[\nY_{sd} = \\beta_{0s} + \\beta_{1s} X_{sd} + e_{sd},\n\\]\ndove:\n\n\n\\(Y_{sd}\\): Tempo di reazione del soggetto \\(s\\) al giorno \\(d\\),\n\n\\(\\beta_{0s}\\): Intercetta specifica del soggetto \\(s\\),\n\n\\(\\beta_{1s}\\): Pendenza specifica del soggetto \\(s\\),\n\n\\(e_{sd} \\sim N(0, \\sigma^2)\\): Errore residuo per il soggetto \\(s\\) al giorno \\(d\\), distribuito normalmente con varianza \\(\\sigma^2\\).\n\nLe intercette (\\(\\beta_{0s}\\)) e le pendenze (\\(\\beta_{1s}\\)) variano tra i soggetti e sono determinate al Livello 2.\n\n14.7.5 Livello 2: Variabilità Tra Soggetti\nAl livello superiore, il modello specifica come le intercette e le pendenze variano tra i soggetti:\n\\[\n\\beta_{0s} = \\gamma_{0} + S_{0s},\n\\] \\[\n\\beta_{1s} = \\gamma_{1} + S_{1s},\n\\]\ndove:\n\n\n\\(\\gamma_0\\): Intercetta media della popolazione,\n\n\\(\\gamma_1\\): Pendenza media della popolazione,\n\n\\(S_{0s} \\sim N(0, \\tau_{00}^2)\\): Effetto casuale sull’intercetta per il soggetto \\(s\\),\n\n\\(S_{1s} \\sim N(0, \\tau_{11}^2)\\): Effetto casuale sulla pendenza per il soggetto \\(s\\),\n\n\\(\\langle S_{0s}, S_{1s} \\rangle\\): Effetti casuali correlati con covarianza \\(\\rho \\tau_{00} \\tau_{11}\\).\n\nLa matrice di varianza-covarianza degli effetti casuali è data da:\n\\[\n\\Sigma =\n\\begin{pmatrix}\n\\tau_{00}^2 & \\rho \\tau_{00} \\tau_{11} \\\\\n\\rho \\tau_{00} \\tau_{11} & \\tau_{11}^2\n\\end{pmatrix},\n\\]\ndove \\(\\tau_{00}^2\\) e \\(\\tau_{11}^2\\) rappresentano rispettivamente la varianza delle intercette e delle pendenze, e \\(\\rho\\) descrive la correlazione tra questi effetti.\n\n14.7.6 Modello Completo\nCombinando i due livelli, il modello multilivello può essere espresso come:\n\\[\nY_{sd} = \\gamma_0 + S_{0s} + (\\gamma_1 + S_{1s}) X_{sd} + e_{sd},\n\\]\ndove:\n\n\n\\(\\gamma_0\\) e \\(\\gamma_1\\): Effetti fissi (intercetta e pendenza medie della popolazione),\n\n\\(S_{0s}\\) e \\(S_{1s}\\): Effetti casuali che catturano variazioni individuali,\n\n\\(e_{sd}\\): Errore residuo specifico per ciascuna osservazione.\n\n14.7.7 Interpretazione degli Effetti Fissi e Casuali\n\n\nEffetti Fissi (\\(\\gamma_0\\), \\(\\gamma_1\\)): Rappresentano la tendenza generale della popolazione, fornendo stime medie valide per tutti i soggetti.\n\nEffetti Casuali (\\(S_{0s}\\), \\(S_{1s}\\)): Catturano la variabilità tra i soggetti, modellando le deviazioni rispetto agli effetti fissi.\n\n14.7.8 Importanza della Matrice di Varianza-Covarianza\nLa matrice di varianza-covarianza consente di:\n\n\nQuantificare la variabilità tra i soggetti: Le varianze \\(\\tau_{00}^2\\) e \\(\\tau_{11}^2\\) indicano l’eterogeneità delle intercette e delle pendenze rispettivamente.\n\nEsaminare le relazioni tra intercette e pendenze: La covarianza \\(\\rho \\tau_{00} \\tau_{11}\\) fornisce informazioni sulle correlazioni tra i due parametri. Ad esempio, una correlazione positiva potrebbe indicare che soggetti con tempi di reazione iniziali più elevati (intercetta maggiore) tendono a mostrare aumenti più rapidi del tempo di reazione con la privazione del sonno.\n\n14.7.9 Vantaggi del Modello Multilivello\n\n\nBilancia complete pooling e no pooling: Integra tendenze generali con variazioni specifiche.\n\nStime robuste: Migliora l’accuratezza delle stime per i soggetti individuali sfruttando le informazioni condivise tra tutti i soggetti.\n\nGeneralizzabilità: Fornisce una base solida per inferenze valide sull’intera popolazione.\n\nIn conclusione, il modello multilivello offre un approccio flessibile e potente per analizzare dati gerarchici, come quelli longitudinali o nidificati. Permette di distinguere tra effetti generali e variazioni individuali, fornendo una rappresentazione dettagliata dei fenomeni psicologici e una base solida per interpretazioni e inferenze.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Modelli multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/raters/01_multilevel.html#stima-dei-parametri-del-modello",
    "href": "chapters/raters/01_multilevel.html#stima-dei-parametri-del-modello",
    "title": "14  Modelli multilivello",
    "section": "\n14.8 Stima dei Parametri del Modello",
    "text": "14.8 Stima dei Parametri del Modello\nPer stimare i parametri del modello in R, utilizzeremo la funzione lmer() del pacchetto lme4 (Bates et al., 2014). Questa funzione consente di specificare sia gli effetti fissi (ad esempio, giorni di deprivazione del sonno) sia gli effetti casuali (ad esempio, variazioni tra soggetti), costruendo un modello che bilancia informazioni aggregate e individuali.\n\n14.8.1 Sintassi Generale di lmer()\n\nLa sintassi base di lmer() è:\n\\[\n\\text{lmer(formula, data, ...)},\n\\]\ndove:\n\n\nformula definisce la struttura del modello, specificando la relazione tra variabili dipendenti, effetti fissi e casuali.\n\ndata indica il dataset contenente le variabili.\n\nLa struttura generale della formula è:\nDV ~ fix1 + fix2 + ... + fixN + (ran1 + ran2 + ... + ranK | random_factor)\ndove:\n\n\nDV: variabile dipendente.\n\nfix1, fix2, ..., fixN: effetti fissi (fattori indipendenti a livello globale).\n\nran1, ran2, ..., ranK: effetti casuali che variano tra i livelli del fattore casuale specificato da random_factor.\n\nLe interazioni tra i fattori possono essere definite utilizzando:\n\n\nA * B (effetti principali + interazione),\n\nA:B (solo interazione).\n\n14.8.2 Effetti Casuali nella Formula\nGli effetti casuali sono specificati all’interno di parentesi, ad esempio:\n(1 + days_deprived | Subject)\nQuesta sintassi indica che sia l’intercetta (\\(1\\)) sia il coefficiente associato a days_deprived variano tra i livelli del fattore casuale Subject. È anche possibile avere più termini di effetti casuali nella stessa formula, ad esempio, per fattori casuali incrociati.\nSul lato sinistro della barra | si elencano gli effetti che vogliamo far variare, mentre sul lato destro si specifica la variabile che identifica i livelli del fattore casuale (ad esempio, Subject).\n\n14.8.3 Esempi di Modelli con sleep2\n\nConsideriamo diverse specifiche di modello per i dati sleep2, con le rispettive formule e strutture della matrice di varianza-covarianza (\\(\\mathbf{\\Sigma}\\)).\n\n14.8.3.1 Solo Intercette Casuali\nReaction ~ days_deprived + (1 | Subject)\nIn questo modello, solo l’intercetta varia tra i soggetti (\\(\\tau_{00}^2\\)):\n\\[\n\\mathbf{\\Sigma} =\n\\begin{pmatrix}\n\\tau_{00}^2 & 0 \\\\\n0 & 0\n\\end{pmatrix}\n\\]\n\n14.8.3.2 Intercette e Pendenze Casuali\nReaction ~ days_deprived + (1 + days_deprived | Subject)\nIn questo modello, sia l’intercetta (\\(\\tau_{00}^2\\)) sia la pendenza (\\(\\tau_{11}^2\\)) variano tra i soggetti, con una correlazione (\\(\\rho\\)) tra i due:\n\\[\n\\mathbf{\\Sigma} =\n\\begin{pmatrix}\n\\tau_{00}^2 & \\rho \\tau_{00} \\tau_{11} \\\\\n\\rho \\tau_{00} \\tau_{11} & \\tau_{11}^2\n\\end{pmatrix}\n\\]\n\n14.8.3.3 Pendenze Casuali Senza Correlazione\nReaction ~ days_deprived + (days_deprived || Subject)\nQuesta sintassi alternativa esclude la covarianza tra intercette e pendenze, risultando in una matrice diagonale:\n\\[\n\\mathbf{\\Sigma} =\n\\begin{pmatrix}\n\\tau_{00}^2 & 0 \\\\\n0 & \\tau_{11}^2\n\\end{pmatrix}\n\\]\n\n14.8.3.4 Solo Pendenze Casuali\nReaction ~ days_deprived + (0 + days_deprived | Subject)\nIn questo caso, solo le pendenze variano tra i soggetti (\\(\\tau_{11}^2\\)):\n\\[\n\\mathbf{\\Sigma} =\n\\begin{pmatrix}\n0 & 0 \\\\\n0 & \\tau_{11}^2\n\\end{pmatrix}\n\\]\nTabella Riassuntiva dei Modelli\n\n\n\n\n\n\n\nModello\nFormula\nStruttura di \\(\\mathbf{\\Sigma}\\)\n\n\n\n1. Solo intercette\nReaction ~ days_deprived + (1 | Subject)\n\\(\\begin{pmatrix} \\tau_{00}^2 & 0 \\\\ 0 & 0 \\end{pmatrix}\\)\n\n\n2. Intercette e pendenze\nReaction ~ days_deprived + (1 + days_deprived | Subject)\n\\(\\begin{pmatrix} \\tau_{00}^2 & \\rho \\tau_{00} \\tau_{11} \\\\ \\rho \\tau_{00} \\tau_{11} & \\tau_{11}^2 \\end{pmatrix}\\)\n\n\n3. No covarianza\nReaction ~ days_deprived + (days_deprived || Subject)\n\\(\\begin{pmatrix} \\tau_{00}^2 & 0 \\\\ 0 & \\tau_{11}^2 \\end{pmatrix}\\)\n\n\n4. Solo pendenze\nReaction ~ days_deprived + (0 + days_deprived | Subject)\n\\(\\begin{pmatrix} 0 & 0 \\\\ 0 & \\tau_{11}^2 \\end{pmatrix}\\)\n\n\n\n14.8.4 Scelta del Modello\nLa scelta del modello dipende dalla complessità dei dati e dalle ipotesi sull’eterogeneità tra i soggetti:\n\n\nModello 1: È il più semplice, adatto quando si ritiene che le differenze tra soggetti influenzino solo l’intercetta.\n\nModello 2: Modello più flessibile, consente variazioni sia nelle intercette che nelle pendenze.\n\nModello 3: Variante di Modello 2, esclude la correlazione tra intercette e pendenze, utile per semplificare il modello.\n\nModello 4: Adatto quando si ritiene che la variabilità tra soggetti influenzi solo le pendenze.\n\nIn conclusione, la funzione lmer() offre una potente flessibilità per modellare dati multilivello, adattandosi a diverse ipotesi e configurazioni. La comprensione delle strutture della matrice di varianza-covarianza è fondamentale per interpretare correttamente i risultati e scegliere il modello più appropriato per il contesto analitico.\n\n14.8.5 Applicazione\nPer i dati dell’esempio, il modello più appropriato è il Modello 2, che include intercette e pendenze casuali per ciascun soggetto. Procediamo quindi a stimarlo:\n\npp_mod &lt;- lmer(\n    Reaction ~ 1 + days_deprived + (1 + days_deprived | Subject), \n    data = sleep2\n)\n\nPer verificare la stima del modello, utilizziamo la funzione summary():\n\nsummary(pp_mod)\n#&gt; Linear mixed model fit by REML ['lmerMod']\n#&gt; Formula: Reaction ~ 1 + days_deprived + (1 + days_deprived | Subject)\n#&gt;    Data: sleep2\n#&gt; \n#&gt; REML criterion at convergence: 1404\n#&gt; \n#&gt; Scaled residuals: \n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -4.016 -0.354  0.007  0.468  5.073 \n#&gt; \n#&gt; Random effects:\n#&gt;  Groups   Name          Variance Std.Dev. Corr\n#&gt;  Subject  (Intercept)   958.4    30.96        \n#&gt;           days_deprived  45.8     6.77    0.18\n#&gt;  Residual               651.6    25.53        \n#&gt; Number of obs: 144, groups:  Subject, 18\n#&gt; \n#&gt; Fixed effects:\n#&gt;               Estimate Std. Error t value\n#&gt; (Intercept)     267.97       8.27    32.4\n#&gt; days_deprived    11.44       1.85     6.2\n#&gt; \n#&gt; Correlation of Fixed Effects:\n#&gt;             (Intr)\n#&gt; days_deprvd -0.062\n\n\n14.8.6 Predizioni del Modello\nPrima di interpretare i risultati, rappresentiamo graficamente i dati osservati e le previsioni del modello. La funzione predict() permette di calcolare le stime previste per ciascun livello del predittore, considerando gli effetti casuali e fissi. Di seguito, i passaggi per creare un dataset per le predizioni.\n\n14.8.6.1 Creazione di un Nuovo Dataset\nPer ottenere le previsioni, creiamo un nuovo dataset (newdata) con tutte le combinazioni dei livelli di Subject e days_deprived. Utilizziamo la funzione crossing() del pacchetto dplyr.\n\nnewdata &lt;- crossing(\n    Subject = sleep2 |&gt; \n        pull(Subject) |&gt; \n        levels() |&gt; \n        factor(),\n    days_deprived = 0:7\n)\n\nEcco un’anteprima del dataset generato:\n\nhead(newdata, 17)\n#&gt; # A tibble: 17 × 2\n#&gt;   Subject days_deprived\n#&gt;   &lt;fct&gt;           &lt;int&gt;\n#&gt; 1 308                 0\n#&gt; 2 308                 1\n#&gt; 3 308                 2\n#&gt; 4 308                 3\n#&gt; 5 308                 4\n#&gt; 6 308                 5\n#&gt; # ℹ 11 more rows\n\n\n14.8.6.2 Dettagli del Codice\n\n\nSubject = sleep2 |&gt; pull(Subject) |&gt; levels() |&gt; factor():\n\n\npull(Subject): Estrae la colonna Subject dal dataset sleep2.\n\nlevels(): Recupera i livelli unici della variabile categorica Subject.\n\nfactor(): Converte i livelli in un fattore, garantendo la compatibilità con la funzione crossing().\n\n\ndays_deprived = 0:7: Genera una sequenza di valori da 0 a 7, rappresentando i giorni di privazione del sonno.\ncrossing(): Combina tutte le possibili coppie di livelli di Subject e valori di days_deprived, creando un dataset completo per calcolare le predizioni.\n\n14.8.6.3 Calcolo delle Predizioni\nUtilizziamo la funzione predict() per stimare i valori di Reaction per ciascuna combinazione di soggetto e giorno:\n\nnewdata2 &lt;- newdata |&gt;\n    mutate(Reaction = predict(pp_mod, newdata))\n\nVisualizziamo i primi valori del nuovo dataset:\n\nhead(newdata2)\n#&gt; # A tibble: 6 × 3\n#&gt;   Subject days_deprived Reaction\n#&gt;   &lt;fct&gt;           &lt;int&gt;    &lt;dbl&gt;\n#&gt; 1 308                 0     292.\n#&gt; 2 308                 1     313.\n#&gt; 3 308                 2     333.\n#&gt; 4 308                 3     353.\n#&gt; 5 308                 4     373.\n#&gt; 6 308                 5     393.\n\n\n14.8.7 Visualizzazione Grafica\nPer rappresentare graficamente le rette di regressione previste dal modello per ciascun soggetto, utilizziamo ggplot2:\n\nggplot(sleep2, aes(x = days_deprived, y = Reaction)) +\n    geom_line(\n        data = newdata2,\n        aes(group = Subject),\n        color = \"blue\"\n    ) +\n    geom_point() +\n    scale_x_continuous(breaks = 0:7) +\n    facet_wrap(~Subject) +\n    labs(\n        y = \"Reaction Time (ms)\", \n        x = \"Days Deprived of Sleep (0 = baseline)\",\n        title = \"Reaction Time by Days of Sleep Deprivation\"\n    )\n\n\n\n\n\n\n\n\n\ngeom_line(): Disegna le rette di regressione previste per ciascun soggetto utilizzando i dati di newdata2.\n\n\naes(group = Subject): Specifica che ogni linea è associata a un soggetto distinto.\n\ncolor = \"blue\": Imposta il colore delle linee.\n\n\ngeom_point(): Aggiunge i punti corrispondenti ai dati osservati.\nfacet_wrap(~Subject): Crea un pannello separato per ciascun soggetto, facilitando il confronto tra dati osservati e predizioni.\nlabs(): Personalizza le etichette degli assi e il titolo del grafico.\n\nIl grafico risultante mostra le predizioni del modello (linee blu) sovrapposte ai dati osservati (punti) per ciascun soggetto. Questo consente di valutare visivamente l’adeguatezza del modello nel catturare sia le tendenze generali sia le variazioni individuali.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Modelli multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/raters/01_multilevel.html#interpretare-loutput-di-lmer-ed-estrarre-le-stime",
    "href": "chapters/raters/01_multilevel.html#interpretare-loutput-di-lmer-ed-estrarre-le-stime",
    "title": "14  Modelli multilivello",
    "section": "\n14.9 Interpretare l’output di lmer() ed estrarre le stime",
    "text": "14.9 Interpretare l’output di lmer() ed estrarre le stime\nLa chiamata a lmer() restituisce un oggetto della classe “lmerMod”.\n\n14.9.1 Effetti fissi\nLa sezione dell’output chiamata “Effetti fissi:” è simile a ciò che si vede nell’output per un modello lineare semplice adattato con lm().\nFixed effects:\n              Estimate Std. Error t value\n(Intercept)    267.967      8.266  32.418\ndays_deprived   11.435      1.845   6.197\nL’output precedente indica che il tempo di reazione medio stimato per i partecipanti al Giorno 0 era di circa 268 millisecondi, con ogni giorno di privazione del sonno che aggiungeva mediamente ulteriori 11 millisecondi al tempo di risposta.\nSe dobbiamo ottenere gli effetti fissi dal modello, possiamo estrarli utilizzando la funzione fixef().\n\nfixef(pp_mod) |&gt; \n    print()\n#&gt;   (Intercept) days_deprived \n#&gt;         268.0          11.4\n\nGli errori standard ci forniscono stime della variabilità di questi parametri dovuta all’errore di campionamento. Possiamo utilizzarli per calcolare i valori \\(t\\) o derivare gli intervalli di confidenza. Per estrarli, utilizziamo vcov(pp_mod), che restituirà una matrice di varianza-covarianza (non quella associata agli effetti casuali), quindi estraiamo la diagonale utilizzando diag() e calcoliamo infine la radice quadrata utilizzando sqrt().\n\nvcov(pp_mod)\n#&gt; 2 x 2 Matrix of class \"dpoMatrix\"\n#&gt;               (Intercept) days_deprived\n#&gt; (Intercept)         68.33         -0.95\n#&gt; days_deprived       -0.95          3.41\n\n\nvcov(pp_mod) |&gt; \n    diag() |&gt; \n    sqrt() |&gt; \n    print()\n#&gt;   (Intercept) days_deprived \n#&gt;          8.27          1.85\n\nSi noti che, nell’output di lmer, i valori \\(t\\) non sono accompagnati dai valori \\(p\\), come avviene di solito nei contesti di modellazione più semplici. Esistono molteplici approcci per ottenere i valori \\(p\\) da modelli a effetti misti, ciascuno con vantaggi e svantaggi (si veda, ad esempio, Luke (2017) per un’analisi delle opzioni disponibili). I valori \\(t\\) non vengono accompagnati dai gradi di libertà, poiché i gradi di libertà in un modello a effetti misti non sono ben definiti. Spesso i ricercatori trattano i valori \\(t\\) come valori \\(z\\) di Wald, ossia come osservazioni provenienti da una distribuzione normale standard. Poiché la distribuzione \\(t\\) si avvicina alla distribuzione normale standard all’aumentare del numero di osservazioni, questa pratica “t-as-z” è legittima se il numero di osservazioni campionarie è sufficientemente grande.\nPer calcolare i valori \\(z\\) di Wald, basta dividere la stima dell’effetto fisso per il suo errore standard:\n\ntvals &lt;- fixef(pp_mod) / sqrt(diag(vcov(pp_mod)))\n\ntvals |&gt; \n    print()\n#&gt;   (Intercept) days_deprived \n#&gt;          32.4           6.2\n\nI valori-\\(p\\) si ottengono nel modo seguente:\n\nprint(2 * (1 - pnorm(abs(tvals))))\n#&gt;   (Intercept) days_deprived \n#&gt;      0.00e+00      5.75e-10\n\nQuesto fornisce una forte evidenza contro l’ipotesi nulla \\(H_0: \\gamma_1 = 0\\). Sembra che la privazione del sonno aumenti effettivamente il tempo di risposta.\nÈ possibile ottenere gli intervalli di confidenza per le stime utilizzando la funzione confint() (questa tecnica utilizza il bootstrap parametrico).\n\nconfint(pp_mod) |&gt; \n    print()\n#&gt;                 2.5 %  97.5 %\n#&gt; .sig01         19.098  46.337\n#&gt; .sig02         -0.405   0.806\n#&gt; .sig03          4.008  10.249\n#&gt; .sigma         22.467  29.349\n#&gt; (Intercept)   251.344 284.590\n#&gt; days_deprived   7.725  15.146\n\n\n14.9.2 Effetti random\nRandom effects:\n Groups   Name          Variance Std.Dev. Corr\n Subject  (Intercept)   958.35   30.957       \n          days_deprived  45.78    6.766   0.18\n Residual               651.60   25.526       \nNumber of obs: 144, groups:  Subject, 18\nLa parte relativa agli effetti casuali dell’output di summary() ci fornisce una tabella con informazioni sulle diverse componenti della varianza: la matrice di varianza-covarianza (o matrici, se ci sono più fattori casuali) e la varianza residua.\nCominciamo con la riga Residual. Questo ci indica che la varianza residua, \\(\\sigma^2\\), è stata stimata a circa 651.6. Il valore nella colonna successiva, 25.526, è la deviazione standard, \\(\\sigma\\), che è la radice quadrata della varianza.\nEstraiamo la deviazione standard dei residui utilizzando la funzione sigma().\n\nsigma(pp_mod) # residual\n#&gt; [1] 25.5\n\nLe due righe sopra la riga Residual ci forniscono informazioni sulla matrice di varianza-covarianza per il fattore casuale “Subject”.\nRandom effects:\n Groups   Name          Variance Std.Dev. Corr\n Subject  (Intercept)   958.35   30.957       \n          days_deprived  45.78    6.766   0.18\nI valori nella colonna “Variance” ci forniscono la diagonale principale della matrice, mentre i valori nella colonna “Std.Dev.” rappresentano semplicemente le radici quadrate di questi valori. La colonna “Corr” indica la correlazione tra l’intercetta e la pendenza.\nPossiamo estrarre questi valori dall’oggetto adattato pp_mod utilizzando la funzione VarCorr(). Questa funzione restituisce una lista nominata, con un elemento per ciascun fattore casuale. Nel nostro caso, “Subject” è l’unico fattore casuale, quindi la lista avrà lunghezza 1.\n\n# variance-covariance matrix for random factor Subject\nVarCorr(pp_mod)[[\"Subject\"]] |&gt; \n    print() # oppure: VarCorr(pp_mod)[[1]]\n#&gt;               (Intercept) days_deprived\n#&gt; (Intercept)         958.4          37.2\n#&gt; days_deprived        37.2          45.8\n#&gt; attr(,\"stddev\")\n#&gt;   (Intercept) days_deprived \n#&gt;         30.96          6.77 \n#&gt; attr(,\"correlation\")\n#&gt;               (Intercept) days_deprived\n#&gt; (Intercept)         1.000         0.178\n#&gt; days_deprived       0.178         1.000\n\nLe prime righe rappresentano la matrice di varianza-covarianza. Le varianze sono riportate sulla diagonale principale. correlation indica la correlazione tra la stima della pendenza e la stima dell’intercetta.\nPossiamo estrarre gli effetti casuali stimati (BLUPS) utilizzando la funzione ranef().\n\nranef(pp_mod)[[\"Subject\"]] |&gt; \n    print()\n#&gt;     (Intercept) days_deprived\n#&gt; 308      24.499         8.602\n#&gt; 309     -59.372        -8.128\n#&gt; 310     -39.476        -7.429\n#&gt; 330       1.350        -2.385\n#&gt; 331      18.458        -3.748\n#&gt; 332      30.527        -4.894\n#&gt; 333      13.368         0.289\n#&gt; 334     -18.158         3.844\n#&gt; 335     -16.974       -12.070\n#&gt; 337      44.585        10.176\n#&gt; 349     -26.684         2.195\n#&gt; 350      -5.966         8.176\n#&gt; 351      -5.571        -2.372\n#&gt; 352      46.635        -0.562\n#&gt; 369       0.962         1.739\n#&gt; 370     -18.522         5.632\n#&gt; 371      -7.343         0.273\n#&gt; 372      17.683         0.662\n\nPossiamo ottenere i valori stimati dal modello utilizzando fitted() e i residui utilizzando residuals().\n\nmutate(sleep2,\n    fit = fitted(pp_mod),\n    resid = residuals(pp_mod)\n) |&gt;\n    group_by(Subject) %&gt;%\n    slice(c(1, 10)) %&gt;%\n    print(n = +Inf)\n#&gt; # A tibble: 18 × 6\n#&gt; # Groups:   Subject [18]\n#&gt;    Reaction  Days Subject days_deprived   fit  resid\n#&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;           &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1     251.     2 308                 0  292. -41.7 \n#&gt;  2     203.     2 309                 0  209.  -5.62\n#&gt;  3     234.     2 310                 0  228.   5.83\n#&gt;  4     284.     2 330                 0  269.  14.5 \n#&gt;  5     302.     2 331                 0  286.  15.4 \n#&gt;  6     273.     2 332                 0  298. -25.5 \n#&gt;  7     277.     2 333                 0  281.  -4.57\n#&gt;  8     243.     2 334                 0  250.  -6.44\n#&gt;  9     254.     2 335                 0  251.   3.50\n#&gt; 10     292.     2 337                 0  313. -20.9 \n#&gt; 11     239.     2 349                 0  241.  -2.36\n#&gt; 12     256.     2 350                 0  262.  -5.80\n#&gt; 13     270.     2 351                 0  262.   7.50\n#&gt; 14     327.     2 352                 0  315.  12.3 \n#&gt; 15     257.     2 369                 0  269. -11.7 \n#&gt; 16     239.     2 370                 0  249. -10.5 \n#&gt; 17     278.     2 371                 0  261.  17.3 \n#&gt; 18     298.     2 372                 0  286.  11.9\n\nInfine, possiamo ottenere previsioni per nuovi dati utilizzando predict(), come abbiamo fatto in precedenza.\n\n## create the table with new predictor values\nndat &lt;- crossing(\n    Subject = sleep2 %&gt;% pull(Subject) %&gt;% levels() %&gt;% factor(),\n    days_deprived = 8:10\n) %&gt;%\n    mutate(Reaction = predict(pp_mod, newdata = .))\n\nndat |&gt; \n    head()\n#&gt; # A tibble: 6 × 3\n#&gt;   Subject days_deprived Reaction\n#&gt;   &lt;fct&gt;           &lt;int&gt;    &lt;dbl&gt;\n#&gt; 1 308                 8     453.\n#&gt; 2 308                 9     473.\n#&gt; 3 308                10     493.\n#&gt; 4 309                 8     235.\n#&gt; 5 309                 9     238.\n#&gt; 6 309                10     242.\n\n\nggplot(sleep2, aes(x = days_deprived, y = Reaction)) +\n    geom_line(\n        data = bind_rows(newdata2, ndat),\n        color = \"blue\"\n    ) +\n    geom_point() +\n    scale_x_continuous(breaks = 0:10) +\n    facet_wrap(~Subject) +\n    labs(y = \"Reaction Time\", x = \"Days deprived of sleep (0 = baseline)\")",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Modelli multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/raters/01_multilevel.html#riflessioni-conclusive",
    "href": "chapters/raters/01_multilevel.html#riflessioni-conclusive",
    "title": "14  Modelli multilivello",
    "section": "\n14.10 Riflessioni Conclusive",
    "text": "14.10 Riflessioni Conclusive\nQuesto capitolo ha presentato una panoramica sui modelli statistici multilivello, con un focus sull’effetto della deprivazione del sonno sulle prestazioni psicomotorie utilizzando il dataset sleepstudy. In tale contesto, sono stati introdotti e discussi concetti fondamentali come il “complete pooling”, il “no pooling” e il “partial pooling”, esplorandone le implicazioni nella modellazione dei dati con misure ripetute.\nL’analisi ha posto l’accento sull’applicazione pratica dei modelli multilivello, con una particolare attenzione alla distinzione tra variabili fisse e casuali e alla loro rilevanza per la struttura del modello. Un aspetto centrale della trattazione è stata la matrice di varianza-covarianza, essenziale per comprendere le relazioni interne ai modelli multilivello.\nQuesti modelli rivestono un ruolo cruciale nel campo dell’assessment psicologico e della psicometria, poiché permettono di analizzare dati complessi tenendo conto delle variazioni individuali e di gruppo. Offrono strumenti flessibili per esaminare come fattori contestuali e individuali influenzino il comportamento e le prestazioni psicologiche, un elemento chiave per una valutazione psicologica accurata.\nInfine, il capitolo prepara il terreno per il successivo approfondimento sui modelli multilivello nell’ambito del calcolo dell’affidabilità tra giudici, argomento che sarà sviluppato nel capitolo seguente. Inoltre, viene tracciato un collegamento con i modelli di crescita latente, i quali saranno trattati nei capitoli successivi come naturale prosecuzione o alternativa ai modelli multilivello. Questo sottolinea la continuità e la rilevanza di tali approcci nell’ambito della ricerca psicologica e psicometrica.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Modelli multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/raters/01_multilevel.html#session-info",
    "href": "chapters/raters/01_multilevel.html#session-info",
    "title": "14  Modelli multilivello",
    "section": "\n14.11 Session Info",
    "text": "14.11 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] kableExtra_1.4.0  repr_1.1.7        lme4_1.1-36       Matrix_1.7-2     \n#&gt;  [5] car_3.1-3         carData_3.0-5     ggokabeito_0.1.0  see_0.10.0       \n#&gt;  [9] MASS_7.3-65       viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n#&gt; [13] ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1 \n#&gt; [17] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-19     psych_2.4.12     \n#&gt; [21] scales_1.3.0      markdown_1.13     knitr_1.49        lubridate_1.9.4  \n#&gt; [25] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.4      \n#&gt; [29] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1    \n#&gt; [33] tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.0      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] mime_0.12           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [25] R6_2.6.1            fastmap_1.2.0       rbibutils_2.3      \n#&gt;  [28] shiny_1.10.0        digest_0.6.37       OpenMx_2.21.13     \n#&gt;  [31] fdrtool_1.2.18      colorspace_2.1-1    rprojroot_2.0.4    \n#&gt;  [34] Hmisc_5.2-2         labeling_0.4.3      timechange_0.3.0   \n#&gt;  [37] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n#&gt;  [40] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n#&gt;  [43] ggsignif_0.6.4      corpcor_1.6.10      gtools_3.9.5       \n#&gt;  [46] tools_4.4.2         pbivnorm_0.6.0      foreign_0.8-88     \n#&gt;  [49] zip_2.3.2           httpuv_1.6.15       nnet_7.3-20        \n#&gt;  [52] glue_1.8.0          quadprog_1.5-8      nlme_3.1-167       \n#&gt;  [55] promises_1.3.2      lisrelToR_0.3       grid_4.4.2         \n#&gt;  [58] checkmate_2.3.2     cluster_2.1.8       reshape2_1.4.4     \n#&gt;  [61] generics_0.1.3      gtable_0.3.6        tzdb_0.4.0         \n#&gt;  [64] data.table_1.17.0   hms_1.1.3           utf8_1.2.4         \n#&gt;  [67] xml2_1.3.7          sem_3.1-16          pillar_1.10.1      \n#&gt;  [70] rockchalk_1.8.157   later_1.4.1         splines_4.4.2      \n#&gt;  [73] lattice_0.22-6      survival_3.8-3      kutils_1.73        \n#&gt;  [76] tidyselect_1.2.1    miniUI_0.1.1.1      pbapply_1.7-2      \n#&gt;  [79] reformulas_0.4.0    svglite_2.1.3       stats4_4.4.2       \n#&gt;  [82] xfun_0.51           qgraph_1.9.8        arm_1.14-4         \n#&gt;  [85] stringi_1.8.4       yaml_2.3.10         pacman_0.5.1       \n#&gt;  [88] boot_1.3-31         evaluate_1.0.3      codetools_0.2-20   \n#&gt;  [91] mi_1.1              cli_3.6.4           RcppParallel_5.1.10\n#&gt;  [94] rpart_4.1.24        systemfonts_1.2.1   xtable_1.8-4       \n#&gt;  [97] Rdpack_2.6.2        munsell_0.5.1       Rcpp_1.0.14        \n#&gt; [100] coda_0.19-4.1       png_0.1-8           XML_3.99-0.18      \n#&gt; [103] parallel_4.4.2      jpeg_0.1-10         mvtnorm_1.3-3      \n#&gt; [106] openxlsx_4.2.8      rlang_1.1.5         multcomp_1.4-28    \n#&gt; [109] mnormt_2.1.1\n\n\n\n\n\nBates, D., Mächler, M., Bolker, B., & Walker, S. (2014). Fitting linear mixed-effects models using lme4. arXiv preprint arXiv:1406.5823.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Modelli multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/raters/02_interrater_reliability.html",
    "href": "chapters/raters/02_interrater_reliability.html",
    "title": "15  L’affidabilità tra giudici",
    "section": "",
    "text": "15.1 Introduzione\nL’affidabilità inter-valutatore, nota anche come inter-rater reliability, è un elemento cruciale in molti ambiti della psicologia, soprattutto quando i test si basano su giudizi soggettivi. Questo concetto si riferisce al grado di concordanza tra valutatori diversi nel misurare lo stesso fenomeno o campione. Un’elevata affidabilità inter-valutatore indica che i punteggi o i giudizi assegnati da persone diverse sono altamente coerenti, a parità di condizioni, e quindi replicabili.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>L'affidabilità tra giudici</span>"
    ]
  },
  {
    "objectID": "chapters/raters/02_interrater_reliability.html#introduzione",
    "href": "chapters/raters/02_interrater_reliability.html#introduzione",
    "title": "15  L’affidabilità tra giudici",
    "section": "",
    "text": "15.1.1 Importanza dell’Affidabilità Inter-Valutatore\nL’affidabilità inter-valutatore garantisce che i risultati di un test siano consistenti e riproducibili indipendentemente dal valutatore. Questo aspetto è essenziale per assicurare la validità e la solidità delle conclusioni derivanti dai dati, riducendo l’influenza di bias soggettivi.\nAd esempio:\n\n\nDiagnosi psicologica: Nella valutazione di disturbi dell’umore, diversi psicologi devono giungere a conclusioni simili basandosi sui sintomi osservati e sulle risposte dei pazienti a questionari standardizzati. Un disaccordo significativo tra valutatori comprometterebbe l’accuratezza diagnostica.\n\nValutazione terapeutica: Durante uno studio sull’efficacia di una terapia contro l’ansia, terapeuti diversi possono valutare i livelli di ansia prima e dopo il trattamento. La coerenza tra le loro valutazioni è fondamentale per confermare l’efficacia dell’intervento.\n\nQuesto capitolo approfondisce il concetto di affidabilità inter-valutatore in due contesti principali:\n\n\nDue giudici: Verranno analizzati l’accordo nominale e l’indice di concordanza Kappa di Cohen, che corregge per l’accordo atteso casuale.\n\nGiudici multipli: Esploreremo tecniche avanzate come il Coefficiente di Correlazione Intraclasse (ICC) e l’ANOVA, sia a una via che a due vie. In questo contesto, i modelli a effetti misti saranno utilizzati per:\n\nDistinguere la variabilità tra giudici e partecipanti.\nFornire una stima globale dell’affidabilità delle valutazioni.\n\n\n\nL’obiettivo è offrire una panoramica completa delle metodologie per misurare e interpretare l’affidabilità inter-valutatore, fornendo strumenti pratici per applicazioni in contesti psicologici e di ricerca.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>L'affidabilità tra giudici</span>"
    ]
  },
  {
    "objectID": "chapters/raters/02_interrater_reliability.html#un-esempio-concreto",
    "href": "chapters/raters/02_interrater_reliability.html#un-esempio-concreto",
    "title": "15  L’affidabilità tra giudici",
    "section": "\n15.2 Un Esempio Concreto",
    "text": "15.2 Un Esempio Concreto\nIn questo capitolo, analizzeremo i dati dello studio The Reliability and Validity of Speech-Language Pathologists’ Estimations of Intelligibility in Dysarthria condotto da Hirsch et al. (2022). Lo studio si concentra sulla valutazione della “Percentuale di Intelligibilità del Discorso”, un indicatore soggettivo che misura la percentuale di parole o frasi comprese durante una conversazione con persone affette da disartria.\nLa disartria è una condizione patologica che compromette la chiarezza e l’intelligibilità del linguaggio parlato, influenzando negativamente la comunicazione quotidiana. Per stimare questa percentuale, i giudici ascoltano registrazioni vocali dei partecipanti disartrici e forniscono una valutazione del grado di comprensibilità del loro discorso. Questa misura rappresenta un parametro chiave per quantificare l’impatto della disartria sulla capacità di comunicazione verbale e per progettare interventi mirati a migliorare la qualità della vita delle persone colpite.\nImportiamo i dati in R:\n\nslp_dat &lt;- read.csv(\"https://osf.io/download/p9gqk/\")\nslp_vas_wide &lt;- slp_dat |&gt;\n    dplyr::select(slpID, Speaker, slp_VAS) |&gt;\n    pivot_wider(names_from = slpID, values_from = slp_VAS)\n\nVisualizziamo il data frame:\n\nglimpse(slp_vas_wide)\n#&gt; Rows: 20\n#&gt; Columns: 22\n#&gt; $ Speaker &lt;chr&gt; \"AF1\", \"AF9\", \"ALSF6\", \"ALSF7\", \"ALSF9\", \"ALSM1\", \"ALSM4\",…\n#&gt; $ slp10   &lt;dbl&gt; 96.3, 12.3, NA, 94.0, 29.5, 96.3, 68.5, 69.3, 100.0, 95.4,…\n#&gt; $ slp11   &lt;dbl&gt; 100.00, 34.68, 30.84, 96.31, 52.20, NA, 100.00, 20.35, 68.…\n#&gt; $ slp13   &lt;dbl&gt; 96.56, 0.86, 5.73, 61.32, 43.55, 29.23, 93.98, 14.90, NA, …\n#&gt; $ slp14   &lt;dbl&gt; 99.30, 3.52, 73.94, 64.37, 46.76, 69.44, 84.51, 81.69, 88.…\n#&gt; $ slp15   &lt;dbl&gt; 100.0, 12.6, 73.9, 100.0, 60.6, 99.3, 88.4, 86.5, 82.6, 95…\n#&gt; $ slp16   &lt;dbl&gt; 12.58, 0.00, 4.52, 7.74, 23.87, 82.26, 74.52, 12.26, 17.74…\n#&gt; $ slp17   &lt;dbl&gt; 100.00, 6.69, 45.13, 69.92, 25.21, 70.19, 91.64, 25.63, 57…\n#&gt; $ slp18   &lt;dbl&gt; 88.16, 0.00, 41.92, 88.16, 91.64, 79.72, 76.74, 24.23, 47.…\n#&gt; $ slp19   &lt;dbl&gt; 52.58, 7.42, 26.13, 37.42, 37.10, 78.39, 60.97, 31.29, 60.…\n#&gt; $ slp2    &lt;dbl&gt; 93.41, 19.20, 39.83, 43.27, 58.74, 87.39, 98.85, 25.21, 57…\n#&gt; $ slp20   &lt;dbl&gt; 67.42, 9.03, 17.42, 59.35, 19.35, 36.45, 54.19, 23.55, 37.…\n#&gt; $ slp21   &lt;dbl&gt; 77.4, 13.6, 25.8, 59.4, 57.7, 83.9, 76.1, 41.0, 37.7, 48.7…\n#&gt; $ slp22   &lt;dbl&gt; 73.64, 0.00, 8.31, 34.10, 29.51, 84.24, 34.67, 58.74, 49.0…\n#&gt; $ slp23   &lt;dbl&gt; 100.00, 0.00, 18.46, 90.63, 50.14, 92.29, 97.80, 56.75, 69…\n#&gt; $ slp25   &lt;dbl&gt; 100.0, 2.9, 86.8, 53.2, 64.8, 100.0, 74.2, 88.1, 83.9, 94.…\n#&gt; $ slp3    &lt;dbl&gt; 37.82, 0.00, 0.00, 99.71, 0.00, 100.00, 8.31, 30.09, 57.31…\n#&gt; $ slp4    &lt;dbl&gt; 73.24, 14.51, 26.20, 63.10, 31.13, 18.03, 80.85, 26.34, 29…\n#&gt; $ slp5    &lt;dbl&gt; 55.01, 1.72, 3.72, 56.73, 40.11, 58.74, 46.99, 34.38, 34.6…\n#&gt; $ slp6    &lt;dbl&gt; 100.0, NA, NA, 88.8, 16.9, 66.5, NA, NA, NA, NA, 76.2, 61.…\n#&gt; $ slp8    &lt;dbl&gt; 53.94, 6.76, 9.86, 64.93, 5.92, 51.83, 65.63, 25.21, 41.55…\n#&gt; $ slp9    &lt;dbl&gt; 85.1, 10.3, 37.8, 53.8, 50.0, 60.3, 91.7, 34.1, 61.8, 59.3…",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>L'affidabilità tra giudici</span>"
    ]
  },
  {
    "objectID": "chapters/raters/02_interrater_reliability.html#due-giudici",
    "href": "chapters/raters/02_interrater_reliability.html#due-giudici",
    "title": "15  L’affidabilità tra giudici",
    "section": "\n15.3 Due giudici",
    "text": "15.3 Due giudici\nIn questa sezione, esamineremo come calcolare l’accordo nominale e l’indice di concordanza Kappa di Cohen nel caso di due giudici.\nSelezioniamo due giudici.\n\nslp_2rater &lt;- slp_vas_wide |&gt;\n    dplyr::select(slp14, slp15)\nhead(slp_2rater)\n#&gt; # A tibble: 6 × 2\n#&gt;   slp14 slp15\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 99.3  100  \n#&gt; 2  3.52  12.6\n#&gt; 3 73.9   73.9\n#&gt; 4 64.4  100  \n#&gt; 5 46.8   60.6\n#&gt; 6 69.4   99.4\n\n\n15.3.1 Accordo nominale\nL’accordo nominale misura la percentuale di perfetta corrispondenza tra le valutazioni fornite dai due giudici. In altre parole, questo metodo rileva quanto spesso i giudici assegnano esattamente lo stesso punteggio per ciascun caso osservato. Ad esempio, se i giudici forniscono valutazioni identiche per il 40% dei casi, l’accordo nominale sarà pari a 0.4.\nUtilizzando questa definizione, si ottiene:\n\nwith(slp_2rater, mean(slp14 == slp15))\n#&gt; [1] 0.1\n\nPer rendere il criterio di concordanza più flessibile tra i valutatori, viene applicato un arrotondamento ai punteggi dividendo ciascun punteggio per 10 e arrotondando al numero intero più vicino. Questo permette di considerare concordanti anche valutazioni leggermente diverse tra loro. Ad esempio, valutazioni come 75 e 78, dopo essere state divise per 10 e arrotondate, diventano entrambe 8.\n\n# Arrotondamento delle valutazioni\nslp14_round &lt;- round(slp_2rater$slp14 / 10)\nslp15_round &lt;- round(slp_2rater$slp15 / 10)\n\nIl codice opera in tre passaggi principali:\n\nPrima effettua l’arrotondamento delle valutazioni dei due valutatori (slp14_round e slp15_round), dividendo per 10 e arrotondando all’intero più vicino.\nSuccessivamente crea una tabella di contingenza usando la funzione table(), che mostra la distribuzione delle valutazioni arrotondate tra i due valutatori. Questa tabella permette di visualizzare facilmente dove c’è maggiore accordo.\n\n\n# Creazione della tabella di contingenza\nagreement_table &lt;- table(slp14_round, slp15_round)\nagreement_table\n#&gt;            slp15_round\n#&gt; slp14_round 1 5 6 7 8 9 10\n#&gt;          0  1 0 0 0 0 0  0\n#&gt;          5  0 0 1 0 0 0  0\n#&gt;          6  0 1 0 0 0 0  2\n#&gt;          7  0 0 0 1 0 0  1\n#&gt;          8  0 0 0 0 0 2  0\n#&gt;          9  0 0 0 0 1 0  2\n#&gt;          10 0 0 0 1 0 0  7\n\n\nInfine calcola la proporzione di accordo utilizzando la funzione mean() che confronta le valutazioni arrotondate dei due valutatori.\n\n\n# Calcolo della proporzione di accordo\nagreement_proportion &lt;- mean(slp14_round == slp15_round)\n\n# Visualizzazione dei risultati\nprint(agreement_table)\n#&gt;            slp15_round\n#&gt; slp14_round 1 5 6 7 8 9 10\n#&gt;          0  1 0 0 0 0 0  0\n#&gt;          5  0 0 1 0 0 0  0\n#&gt;          6  0 1 0 0 0 0  2\n#&gt;          7  0 0 0 1 0 0  1\n#&gt;          8  0 0 0 0 0 2  0\n#&gt;          9  0 0 0 0 1 0  2\n#&gt;          10 0 0 0 1 0 0  7\n\n\nprint(paste(\"Proporzione di accordo:\", agreement_proportion))\n#&gt; [1] \"Proporzione di accordo: 0.4\"\n\nQuesto approccio riflette una visione più tollerante dell’accordo tra valutatori, riconoscendo che piccole differenze nelle valutazioni (come tra 75 e 78) potrebbero non essere rilevanti in un contesto pratico. L’arrotondamento permette infatti di catturare l’essenza del giudizio piuttosto che concentrarsi su piccole discrepanze numeriche.\nLa proporzione finale di accordo che otteniamo rappresenta quindi la percentuale di casi in cui i due valutatori hanno fornito valutazioni che, una volta arrotondate, risultano equivalenti. Questo metodo fornisce una misura più realistica dell’accordo effettivo tra i valutatori, specialmente in contesti dove non è necessaria una precisione estrema nelle valutazioni.\n\n15.3.2 Kappa di Cohen\nL’indice di concordanza Kappa di Cohen è una misura più robusta dell’accordo tra i giudici, poiché tiene conto della concordanza casuale. A differenza dell’accordo nominale, che considera solo la frequenza delle concordanze perfette, Kappa di Cohen corregge per l’accordo che potrebbe verificarsi per puro caso.\nKappa è definito dalla formula:\n\\[\n\\kappa = \\frac{p_o - p_c}{1 - p_c},\n\\tag{15.1}\\]\ndove:\n\n\n\\(p_o\\) è la proporzione di accordo osservato, ossia la frazione di volte in cui i giudici concordano effettivamente nelle loro valutazioni,\n\n\\(p_c\\) è la proporzione di accordo attesa per caso, calcolata assumendo che le valutazioni siano indipendenti.\n\nIl valore \\(p_o\\) si calcola nel modo seguente:\n\\[\np_o = \\frac{{\\text{Numero di accordi osservati}}}{{\\text{Numero totale di confronti}}}.\n\\]\nIn altre parole, \\(p_o\\) è il rapporto tra il numero di volte in cui gli osservatori sono concordi (hanno dato la stessa valutazione) e il numero totale di confronti effettuati.\nIl valore \\(p_c\\) rappresenta l’indice di concordanza attesa per caso:\n\\[\np_c = \\frac{{\\sum (\\text{Riga i-esima del totale}) \\times (\\text{Colonna i-esima del totale})}}{{\\text{Numero totale di confronti}}^2}.\n\\]\nIn altre parole, \\(p_c\\) è il rapporto atteso di concordanza tra gli osservatori nel caso in cui le loro valutazioni siano indipendenti.\nUn valore di Kappa vicino a 1 indica un’alta concordanza corretta per l’accordo casuale, mentre un valore vicino a 0 suggerisce che l’accordo non è superiore a quello casuale. Un valore negativo indica una concordanza peggiore di quella casuale.\n\n# Creazione della tabella di contingenza con le valutazioni arrotondate\nagreement_table &lt;- table(slp14_round, slp15_round)\n\n# Calcolo di p0 (proporzione di accordo osservato)\np0 &lt;- mean(slp14_round == slp15_round)\n\n# Calcolo di pc (proporzione di accordo atteso per caso)\npc &lt;- sum(colSums(agreement_table) * rowSums(agreement_table)) / sum(agreement_table)^2\n\n# Calcolo del Kappa di Cohen\nkappa &lt;- (p0 - pc) / (1 - pc)\n\n# Visualizzazione dei risultati\nprint(paste(\"Kappa di Cohen:\", round(kappa, 3)))\n#&gt; [1] \"Kappa di Cohen: 0.164\"\n\nTuttavia, è importante notare che Kappa può risultare basso quando i giudizi si concentrano su una o poche categorie, creando una distribuzione non uniforme, come nell’esempio fornito. Questo può accadere perché, nonostante ci possa essere un buon accordo effettivo tra i valutatori, la correzione per l’accordo casuale risulta particolarmente severa quando le valutazioni non sono distribuite uniformemente tra tutte le categorie possibili.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>L'affidabilità tra giudici</span>"
    ]
  },
  {
    "objectID": "chapters/raters/02_interrater_reliability.html#giudici-multipli",
    "href": "chapters/raters/02_interrater_reliability.html#giudici-multipli",
    "title": "15  L’affidabilità tra giudici",
    "section": "\n15.4 Giudici multipli",
    "text": "15.4 Giudici multipli\n\n15.4.1 Coefficiente \\(\\alpha\\)\n\nIl coefficiente \\(\\alpha\\) di Cronbach è comunemente utilizzato per misurare l’affidabilità interna di un test, cioè la coerenza con cui gli item all’interno del test misurano un costrutto comune. Nel nostro caso, lo utilizziamo per valutare l’affidabilità tra giudici, misurando quanto consistentemente i diversi valutatori assegnano punteggi agli stessi item.\nNel contesto dell’affidabilità tra giudici, il coefficiente \\(\\alpha\\) di Cronbach misura quanto consistentemente i diversi giudici valutano gli stessi item. Un valore di \\(\\alpha\\) vicino a 1 indica un alto livello di consistenza (o affidabilità) tra i giudici, suggerendo che stanno valutando gli item in modo simile. Un valore più basso indica una minore coerenza nelle valutazioni.\nPer calcolare il coefficiente \\(\\alpha\\) di Cronbach, consideriamo i punteggi assegnati da ciascun giudice come se fossero item individuali di un test. Quindi, applichiamo la formula standard del coefficiente \\(\\alpha\\) per valutare la coerenza interna di queste valutazioni.\n\npsych::alpha(slp_vas_wide[-1])\n#&gt; \n#&gt; Reliability analysis   \n#&gt; Call: psych::alpha(x = slp_vas_wide[-1])\n#&gt; \n#&gt;   raw_alpha std.alpha G6(smc) average_r S/N    ase mean sd median_r\n#&gt;       0.98      0.98       1      0.69  46 0.0076   62 25      0.7\n#&gt; \n#&gt;     95% confidence boundaries \n#&gt;          lower alpha upper\n#&gt; Feldt     0.96  0.98  0.99\n#&gt; Duhachek  0.96  0.98  0.99\n#&gt; \n#&gt;  Reliability if an item is dropped:\n#&gt;       raw_alpha std.alpha G6(smc) average_r S/N var.r med.r\n#&gt; slp10      0.97      0.98    0.99      0.68  43 0.014  0.70\n#&gt; slp11      0.97      0.98    0.99      0.68  43 0.014  0.70\n#&gt; slp13      0.97      0.98    0.99      0.69  44 0.015  0.70\n#&gt; slp14      0.97      0.98    0.99      0.69  44 0.015  0.70\n#&gt; slp15      0.97      0.98    0.99      0.69  44 0.014  0.70\n#&gt; slp16      0.98      0.98    0.99      0.70  46 0.014  0.71\n#&gt; slp17      0.97      0.98    0.99      0.68  43 0.015  0.69\n#&gt; slp18      0.97      0.98    0.99      0.69  45 0.013  0.70\n#&gt; slp19      0.97      0.98    0.99      0.69  45 0.014  0.71\n#&gt; slp2       0.97      0.98    1.00      0.69  45 0.014  0.71\n#&gt; slp20      0.97      0.98    0.99      0.68  43 0.015  0.69\n#&gt; slp21      0.97      0.98    0.99      0.68  43 0.014  0.69\n#&gt; slp22      0.97      0.98    0.99      0.69  44 0.015  0.70\n#&gt; slp23      0.97      0.98    0.99      0.68  43 0.015  0.69\n#&gt; slp25      0.97      0.98    0.99      0.69  45 0.014  0.71\n#&gt; slp3       0.98      0.98    0.99      0.70  46 0.014  0.71\n#&gt; slp4       0.97      0.98    0.99      0.69  45 0.014  0.71\n#&gt; slp5       0.97      0.98    0.99      0.69  45 0.015  0.70\n#&gt; slp6       0.98      0.98    0.99      0.70  47 0.010  0.71\n#&gt; slp8       0.97      0.98    0.99      0.69  44 0.015  0.70\n#&gt; slp9       0.97      0.98    1.00      0.68  43 0.014  0.69\n#&gt; \n#&gt;  Item statistics \n#&gt;        n raw.r std.r r.cor r.drop mean sd\n#&gt; slp10 18  0.89  0.89  0.90   0.86   81 28\n#&gt; slp11 18  0.89  0.90  0.90   0.90   69 33\n#&gt; slp13 19  0.86  0.86  0.85   0.86   54 33\n#&gt; slp14 20  0.82  0.84  0.84   0.81   80 25\n#&gt; slp15 20  0.81  0.83  0.83   0.80   85 22\n#&gt; slp16 20  0.77  0.76  0.76   0.73   50 42\n#&gt; slp17 20  0.90  0.91  0.91   0.90   63 34\n#&gt; slp18 20  0.82  0.80  0.77   0.79   70 30\n#&gt; slp19 19  0.83  0.82  0.82   0.81   56 25\n#&gt; slp2  20  0.83  0.82  0.82   0.80   64 32\n#&gt; slp20 20  0.90  0.90  0.90   0.89   49 25\n#&gt; slp21 19  0.91  0.91  0.90   0.91   59 25\n#&gt; slp22 20  0.86  0.86  0.86   0.84   50 27\n#&gt; slp23 20  0.89  0.89  0.89   0.86   71 32\n#&gt; slp25 20  0.79  0.79  0.79   0.77   79 28\n#&gt; slp3  19  0.76  0.75  0.76   0.74   52 40\n#&gt; slp4  20  0.79  0.79  0.78   0.77   47 27\n#&gt; slp5  19  0.82  0.82  0.82   0.81   43 23\n#&gt; slp6  13  0.80  0.71  0.71   0.54   76 23\n#&gt; slp8  20  0.86  0.87  0.87   0.86   53 31\n#&gt; slp9  19  0.89  0.90  0.90   0.90   58 23\n\nI risultati mostrano un coefficiente \\(\\alpha\\) di Cronbach molto alto. Questo indica un’eccellente affidabilità tra i valutatori, suggerendo che hanno fornito valutazioni molto consistenti tra loro. Un valore così elevato di \\(\\alpha\\) è notevole e indica che:\n\ni valutatori hanno applicato criteri di valutazione molto simili;\nla “Percentuale di Intelligibilità del Discorso” è stata valutata in modo coerente e affidabile;\nle valutazioni dei diversi giudici possono essere considerate sostanzialmente intercambiabili.\n\n15.4.2 Coefficiente di correlazione intraclasse\nIl Coefficiente di Correlazione Intraclasse (ICC) quantifica il grado di somiglianza tra le unità all’interno dello stesso gruppo. A differenza della maggior parte delle altre misure di correlazione, l’ICC viene impiegato con dati organizzati in gruppi anziché con coppie di osservazioni. Questo lo rende particolarmente idoneo per valutare la concordanza tra giudici che stanno valutando lo stesso insieme di individui o item.\nL’ICC si basa sul framework del modello a effetti misti. Questi modelli statistici consentono di analizzare le variazioni dei punteggi sia all’interno dei gruppi (ovvero le differenze tra le osservazioni all’interno dello stesso gruppo) che tra i gruppi (ovvero le differenze tra i gruppi stessi). Pertanto, l’ICC tiene conto di due fonti di varianza: la varianza all’interno dei gruppi (varianza delle valutazioni dei giudici all’interno dello stesso gruppo) e la varianza tra i gruppi (varianza delle valutazioni dei giudici tra gruppi diversi). Inoltre, viene considerato l’errore casuale presente nelle valutazioni.\nIn sintesi, l’ICC valuta la consistenza delle misurazioni quando queste vengono effettuate da diversi osservatori su un insieme di soggetti o item. Un valore elevato di ICC segnala una forte concordanza tra le valutazioni degli osservatori, indicando che le misurazioni sono affidabili e riproducibili. Al contrario, un ICC basso riflette una discrepanza rilevante tra gli osservatori, suggerendo che le valutazioni sono meno consistenti.\n\n15.4.2.1 Calcolo dell’ICC\nIl calcolo dell’ICC si basa su un modello ad effetti misti, in cui i giudici (o osservatori) sono considerati come variabili indipendenti e i punteggi assegnati costituiscono la variabile dipendente. Questo modello permette di scomporre e quantificare le diverse fonti di variazione nei dati, tra cui:\n\nLa variazione attribuibile ai soggetti (o item) valutati, che rappresenta la varianza principale di interesse.\nLa variazione imputabile ai giudici (differenze tra valutatori).\nLa variazione dovuta all’interazione tra soggetti (o item) e giudici.\nLa variazione derivante dall’errore casuale.\n\nL’ICC si ottiene dividendo la varianza attribuibile ai soggetti valutati per la somma di questa varianza con la varianza dovuta all’errore. Questo rapporto rappresenta la quota della varianza totale dei punteggi che può essere attribuita alle differenze tra i soggetti stessi, fornendo un indicatore dell’affidabilità delle misurazioni. Un ICC elevato indica che la maggior parte della varianza dei punteggi è dovuta alle differenze reali tra i soggetti, suggerendo che le valutazioni sono affidabili.\nTuttavia, cosa viene considerato come “errore” dipende da alcune considerazioni chiave:\n\n\nTipo di disegno: crossed o nested\n\nIn un disegno crossed, tutti i giudici valutano ogni soggetto o item, quindi l’errore riflette le differenze tra giudici che valutano gli stessi soggetti.\nIn un disegno nested, diversi gruppi di giudici valutano gruppi diversi di soggetti o item; in questo caso, l’errore riflette le differenze tra i gruppi di giudici.\n\n\n\nGiudici come fixed o random\n\nSe i giudici sono considerati fixed (fissi), si presume che i giudici inclusi siano gli unici rilevanti e che l’interesse sia nella loro specifica concordanza.\nSe i giudici sono random (casuali), si assume che siano un campione rappresentativo di una popolazione più ampia di giudici, e l’ICC misura la concordanza generalizzabile a tutta questa popolazione.\n\n\n\nConsistenza vs. accordo assoluto\n\n\nConsistenza: misura il grado di concordanza nell’ordinamento dei punteggi assegnati dai giudici, ignorando le differenze nei valori numerici assoluti. È utile quando interessa solo l’ordine relativo dei punteggi.\n\nAccordo assoluto: valuta la concordanza nei valori effettivi dei punteggi assegnati dai giudici. È rilevante quando i giudici devono assegnare esattamente lo stesso punteggio.\n\n\n\nQueste considerazioni sono essenziali per interpretare correttamente l’ICC o altri indici di affidabilità tra giudici. La scelta del modello statistico e la definizione dell’errore dipendono da queste variabili e da come si desidera interpretare la concordanza. Una comprensione approfondita di questi aspetti è necessaria per una valutazione accurata della coerenza e dell’affidabilità delle misurazioni effettuate da giudici o osservatori multipli.\n\n15.4.3 ANOVA ad una via per disegni nidificati\nConsideriamo il caso in cui i dati rappresentano da due valutazioni assegnate a ciascuna persona da giudici diversi.\n\nslp_vas_nested &lt;- slp_dat |&gt;\n    mutate(SpeakerID = as.numeric(as.factor(Speaker))) |&gt;\n    # Select only 10 speakers\n    dplyr::filter(SpeakerID &lt;= 10) |&gt;\n    group_by(Speaker) |&gt;\n    # Filter specific raters\n    dplyr::filter(row_number() %in% (SpeakerID[1] * 2 - (1:0)))\n\nhead(slp_vas_nested)\n#&gt; # A tibble: 6 × 7\n#&gt; # Groups:   Speaker [3]\n#&gt;   slpID Speaker slp_VAS slp_VAS.Rel slp_EST slp_EST.Rel SpeakerID\n#&gt;   &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;       &lt;dbl&gt;   &lt;int&gt;       &lt;int&gt;     &lt;dbl&gt;\n#&gt; 1 slp10 AF1       96.3        NA         90          NA         1\n#&gt; 2 slp11 AF1      100          NA         80          NA         1\n#&gt; 3 slp13 AF9        0.86       NA         10          NA         2\n#&gt; 4 slp14 AF9        3.52       NA         10          NA         2\n#&gt; 5 slp15 ALSF6     73.9        NA         10          NA         3\n#&gt; 6 slp16 ALSF6      4.52        9.03       5          NA         3\n\nVerifichiamo che ogni giudice abbia fornito due giudizi per soggetto:\n\nslp_vas_nested %&gt;%\n  group_by(Speaker) %&gt;%\n  summarise(Count = n())\n#&gt; # A tibble: 10 × 2\n#&gt;   Speaker Count\n#&gt;   &lt;chr&gt;   &lt;int&gt;\n#&gt; 1 AF1         2\n#&gt; 2 AF9         2\n#&gt; 3 ALSF6       2\n#&gt; 4 ALSF7       2\n#&gt; 5 ALSF9       2\n#&gt; 6 ALSM1       2\n#&gt; # ℹ 4 more rows\n\nCi sono 20 giudici:\n\nlength(slp_vas_nested$Speaker)\n#&gt; [1] 20\n\nIn questo studio, analizziamo dati costituiti da due valutazioni fornite a ciascun individuo da giudici diversi. Il nostro obiettivo è identificare e separare due principali fonti di variazione: le differenze tra i giudici e le variazioni casuali all’interno delle valutazioni di ciascun giudice.\nLa sfida risiede nel distinguere queste fonti di variazione, essenziale per determinare la proporzione di variabilità nei punteggi attribuibile a reali differenze tra i giudici, rispetto a quella derivante da errori casuali. Per affrontare questo problema, applichiamo un’analisi della varianza a effetti casuali (Random-Effects ANOVA), utilizzando il modello lineare misto implementato attraverso la funzione lmer() di R. Questa metodologia ci permette di stimare separatamente la varianza dovuta alle differenze tra i giudici e quella associata all’errore casuale.\nImplementiamo il modello lmer(slp_VAS ~ 1 + (1 | Speaker), data = slp_vas_nested), dove il termine Speaker rappresenta i giudici. Attraverso questo modello, trattiamo le intercette associate a ciascun giudice come effetti casuali, consentendoci di catturare le specifiche variazioni tra i giudici al di là delle fluttuazioni casuali.\n\nm1 &lt;- lmer(slp_VAS ~ 1 + (1 | Speaker), data = slp_vas_nested)\nsummary(m1)\n#&gt; Linear mixed model fit by REML ['lmerMod']\n#&gt; Formula: slp_VAS ~ 1 + (1 | Speaker)\n#&gt;    Data: slp_vas_nested\n#&gt; \n#&gt; REML criterion at convergence: 180\n#&gt; \n#&gt; Scaled residuals: \n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -1.5005 -0.7761 -0.0595  0.8758  1.3559 \n#&gt; \n#&gt; Random effects:\n#&gt;  Groups   Name        Variance Std.Dev.\n#&gt;  Speaker  (Intercept) 309      17.6    \n#&gt;  Residual             817      28.6    \n#&gt; Number of obs: 19, groups:  Speaker, 10\n#&gt; \n#&gt; Fixed effects:\n#&gt;             Estimate Std. Error t value\n#&gt; (Intercept)    53.61       8.63    6.21\n\nDall’analisi emergono due componenti principali di varianza:\n\nLa varianza attribuibile a differenze intrinseche tra i soggetti valutati (slpID), che riflette la variabilità naturale tra gli individui.\nLa varianza attribuibile ai giudici (Speaker), che misura l’estensione del bias sistematico, ossia la tendenza di alcuni giudici a fornire valutazioni sistematicamente più alte o più basse rispetto ad altri.\n\nIn aggiunta, abbiamo la varianza residua, che include l’errore di misurazione e altre fonti di variabilità non spiegate dal modello.\n\nvc_m1 &lt;- as.data.frame(VarCorr(m1))\nvc_m1\n#&gt;        grp        var1 var2 vcov sdcor\n#&gt; 1  Speaker (Intercept) &lt;NA&gt;  309  17.6\n#&gt; 2 Residual        &lt;NA&gt; &lt;NA&gt;  817  28.6\n\nPer valutare l’affidabilità delle valutazioni, calcoliamo l’Intraclass Correlation Coefficient (ICC) utilizzando i dati estratti dal modello. Questo coefficiente quantifica la proporzione della varianza totale attribuibile alle differenze tra i soggetti valutati, offrendoci un indice dell’affidabilità delle valutazioni in contesti dove sono coinvolti giudizi o misurazioni ripetute.\nUn ICC prossimo a 1 indica che la maggior parte della variabilità nei dati è dovuta a differenze reali tra i soggetti valutati, mentre un valore più basso suggerisce un’influenza significativa di variazioni casuali o di bias dei giudici sulla variabilità totale osservata.\nCalcoliamo l’ICC per una singola valutazione:\n\\[\nICC = \\frac{\\sigma^2_{\\text{giudici}}} {\\sigma^2_{\\text{giudici}} + \\sigma^2_E}.\n\\]\nNella formula, l’ICC misura la proporzione della varianza totale del punteggio slp_VAS che è attribuibile alla variazione tra i gruppi dei giudici (Speaker) rispetto alla variazione residua o errore.\n\nvc_m1$vcov[1] / (vc_m1$vcov[1] + vc_m1$vcov[2])\n#&gt; [1] 0.274\n\nUtilizziamo la funzione icc() del pacchetto performance:\n\nperformance::icc(m1)\n#&gt; # Intraclass Correlation Coefficient\n#&gt; \n#&gt;     Adjusted ICC: 0.274\n#&gt;   Unadjusted ICC: 0.274\n\nUtilizzando il modello specificato m1 &lt;- lmer(slp_VAS ~ 1 + (1 | Speaker), data = slp_vas_nested), con l’ICC calcolato tramite la formula vc_m1$vcov[1] / (vc_m1$vcov[1] + vc_m1$vcov[2]), è possibile fare alcune osservazioni sulle considerazioni precedenti:\n\nCrossed vs. Nested:\nIl disegno è “nested”, il che implica che diversi gruppi di giudici valutano gruppi diversi di soggetti o item. In un disegno nested, non tutti i giudici valutano tutti i soggetti, quindi l’errore riflette le differenze tra i gruppi di giudici assegnati a specifici soggetti o item. Questo approccio è adatto in situazioni dove non si può o non si vuole che tutti i giudici valutino ogni soggetto.\nFixed vs. Random:\nNella formulazione (1 | Speaker), l’effetto per “Speaker” (che identifica i giudici) è specificato come random. Ciò significa che stiamo trattando i giudici come un campione casuale di una popolazione più ampia, piuttosto che come un insieme fisso di valutatori. Di conseguenza, l’ICC calcolato qui è interpretato come una misura di affidabilità che potrebbe essere generalizzata a una popolazione di giudici simili, non limitata solo ai giudici specifici coinvolti nel campione.\nConsistenza vs. Accordo Assoluto:\nIl modello calcolato è basato su un’intercetta casuale (1 | Speaker), il che indica che l’ICC stima la consistenza tra i giudici, piuttosto che l’accordo assoluto. Questo significa che l’ICC si concentra sul grado in cui i giudici mantengono lo stesso ordine o classificazione tra i soggetti, senza richiedere che assegnino esattamente gli stessi valori numerici.\n\n\n15.4.3.1 ICC per la valutazione media\nCalcoliamo ora l’ICC per la valutazione media:\n\\[\nICC = \\frac{\\sigma^2_{\\text{giudici}}} {\\sigma^2_{\\text{giudici}} + \\sigma^2_E / k},\n\\]\ndove \\(k\\) rappresenta il numero di giudizi assegnati a ciascun soggetto da ciascun giudice.\nLa formula calcola l’ICC basandosi sulla varianza tra i gruppi (in questo caso, la varianza attribuibile ai giudici, \\(\\sigma^2_{\\text{giudici}}\\)) e la varianza entro i gruppi (l’errore casuale, \\(\\sigma^2_E\\)), aggiustando per il numero di giudizi (\\(k\\)) dati per ogni soggetto. L’inserimento di \\(k\\) nel denominatore serve a normalizzare l’effetto dell’errore casuale in base al numero di giudizi, assumendo che più giudizi per soggetto possano ridurre l’impatto dell’errore casuale sulla stima dell’affidabilità.\nQuindi, se l’ICC è vicino a 1, ciò indica che la maggior parte della varianza nei dati è dovuta a differenze reali tra i soggetti valutati, piuttosto che a variazioni casuali o a differenze tra i giudici. In questo modo, l’ICC fornisce un indice utile per valutare l’affidabilità delle valutazioni in studi in cui sono coinvolti giudizi o misurazioni ripetute.\nNel caso presente, ogni giudice fornisce due valutazioni per ogni soggetto:\n\nvc_m1$vcov[1] / (vc_m1$vcov[1] + vc_m1$vcov[2] / 2)\n#&gt; [1] 0.431\n\nQuesta seconda formula calcola l’affidabilità per la media delle valutazioni di più giudici. È pertinente quando si ha a che fare con più valutazioni per ciascun soggetto e si vuole sapere l’affidabilità della media di queste valutazioni. Questa formula differisce dalla prima perché considera la riduzione dell’errore di misurazione che si verifica quando si calcola la media di più valutazioni.\n\n15.4.4 ANOVA a due vie e l’Impatto del Bias dei Giudici\nL’ANOVA a due vie è uno strumento statistico cruciale per esaminare gli effetti di più fattori, inclusi i giudici, sulla variabile dipendente in contesti di valutazione. Per interpretare accuratamente i risultati, è fondamentale distinguere tra il bias sistematico introdotto dai giudici e la variabilità intrinseca nelle valutazioni.\n\nBias dei Giudici: Questo si verifica quando i giudici hanno una tendenza costante a fornire valutazioni più alte o più basse rispetto agli altri, influenzando potenzialmente l’equità delle valutazioni.\nVariabilità Intrinseca: Si riferisce alle differenze naturali nelle valutazioni che emergono dalle percezioni individuali o dalle interpretazioni soggettive dei giudici.\n\nIn termini di Contesto di Valutazione:\n\nNelle analisi di Consistenza, dove l’attenzione è sull’ordine relativo delle valutazioni, il bias dei giudici non è considerato problematico poiché non altera questo ordine.\nNelle analisi di Accordo, che si concentrano sulla concordanza dei valori assoluti delle misurazioni, il bias dei giudici è trattato come una fonte di errore significativa.\n\nL’applicazione dell’ANOVA a due vie consente di distinguere l’effetto del bias dei giudici rispetto ad altre fonti di varianza, offrendo così un’importante metodologia per valutare l’affidabilità delle valutazioni e l’impatto del bias in diversi contesti.\nUtilizzando lmer(slp_VAS ~ 1 + (1 | Speaker) + (1 | slpID), data = slp_dat), analizziamo i dati considerando due principali fonti di variazione:\n\n\nEffetti Casuali dei Giudici ((1 | Speaker)): Valutiamo il bias dei giudici come fonte di variazione, distinguendo tra analisi di consistenza e di accordo a seconda di come questo bias influisce sui risultati.\n\nVariabilità Intrinseca tra i Soggetti ((1 | slpID)): Esploriamo le differenze naturali tra i soggetti valutati, fondamentali per comprendere la diversità delle risposte individuali.\n\nQuesto approccio ci permette di facilitare la distinzione tra l’effetto del bias dei giudici e altre fonti di varianza, fornendo una metodologia preziosa per valutare l’affidabilità delle valutazioni e l’impatto del bias in diversi contesti.\nEseguiamo l’analisi con lmer():\n\nm2 &lt;- lmer(slp_VAS ~ 1 + (1 | Speaker) + (1 | slpID), data = slp_dat)\n\nQuesta formula considera due principali fonti di variazione:\n\nEffetti Casuali dei Giudici ((1 | Speaker)): Valutiamo il bias dei giudici come una fonte di variazione, differenziando tra le analisi di consistenza e di accordo a seconda dell’impatto di questo bias sui risultati.\nVariabilità Intrinseca tra i Soggetti ((1 | slpID)): Esploriamo le differenze naturali tra i soggetti valutati, cruciali per comprendere la diversità delle risposte individuali.\n\n\nsummary(m2)\n#&gt; Linear mixed model fit by REML ['lmerMod']\n#&gt; Formula: slp_VAS ~ 1 + (1 | Speaker) + (1 | slpID)\n#&gt;    Data: slp_dat\n#&gt; \n#&gt; REML criterion at convergence: 3544\n#&gt; \n#&gt; Scaled residuals: \n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -3.328 -0.581  0.086  0.661  2.575 \n#&gt; \n#&gt; Random effects:\n#&gt;  Groups   Name        Variance Std.Dev.\n#&gt;  slpID    (Intercept) 133      11.5    \n#&gt;  Speaker  (Intercept) 586      24.2    \n#&gt;  Residual             291      17.1    \n#&gt; Number of obs: 403, groups:  slpID, 21; Speaker, 20\n#&gt; \n#&gt; Fixed effects:\n#&gt;             Estimate Std. Error t value\n#&gt; (Intercept)    61.88       6.03    10.3\n\nEstraiamo le fonti di varianza:\n\nvc_m2 &lt;- as.data.frame(VarCorr(m2))\nvc_m2\n#&gt;        grp        var1 var2 vcov sdcor\n#&gt; 1    slpID (Intercept) &lt;NA&gt;  133  11.5\n#&gt; 2  Speaker (Intercept) &lt;NA&gt;  586  24.2\n#&gt; 3 Residual        &lt;NA&gt; &lt;NA&gt;  291  17.1\n\n\n15.4.5 Componenti di Varianza\n\n\nVariazione dovuta a slpID (Intercept): Indica la variabilità attribuibile alle differenze tra i soggetti.\n\nVariazione dovuta a Speaker (Intercept): Riflette la variabilità nelle valutazioni dovuta al bias dei giudici.\n\nResidui (Residual): Comprende l’errore di misurazione e altre fonti di variabilità non spiegate.\n\n15.4.6 Calcolo dell’ICC\nPer calcolare l’ICC, dobbiamo distinguere tra le diverse versioni dell’ICC basate sulle definizioni generali. Tuttavia, il calcolo specifico dell’ICC può variare a seconda della struttura del modello e dell’interpretazione desiderata. Nella versione più semplice e comune, si considera la formula:\n\\[\n\\text{ICC} = \\frac{\\sigma^2_{\\text{tra gruppo}}}{\\sigma^2_{\\text{tra gruppo}} + \\sigma^2_{\\text{errore}}},\n\\]\ndove \\(\\sigma^2_{\\text{tra gruppo}}\\) è la varianza attribuita agli effetti casuali tra gruppi, e \\(\\sigma^2_{\\text{errore}}\\) è la varianza residua.\nNel modello precedente, ci sono due componenti di varianza tra gruppi (slpID e Speaker), quindi è possibile considerare la somma di queste due come la varianza totale tra gruppo. Così, l’ICC adjusted può essere calcolato come:\n\\[\n\\text{ICC}_{\\text{adjusted}} = \\frac{\\sigma^2_{slpID} + \\sigma^2_{Speaker}}{\\sigma^2_{slpID} + \\sigma^2_{Speaker} + \\sigma^2_{Residual}}.\n\\]\nSostituendo i valori ottenuti:\n\\[\n\\text{ICC}_{\\text{adjusted}} = \\frac{132.8257 + 585.6568}{132.8257 + 585.6568 + 291.2401}.\n\\]\nQuesto calcolo assume che tutte le componenti di varianza contribuiscano al calcolo dell’ICC in modo equivalente e che l’ICC adjusted consideri la somma delle varianze tra gruppi (in questo caso, slpID e Speaker) rispetto alla varianza totale (inclusa la varianza residua).\nCalcoliamo ora questo valore usando R per ottenere il risultato esatto.\n\n(vc_m2$vcov[1] + vc_m2$vcov[2]) / (vc_m2$vcov[1] + vc_m2$vcov[2] + vc_m2$vcov[3])\n#&gt; [1] 0.712\n\nQuesto valore riproduce quello trovato da performance::icc():\n\nperformance::icc(m2)\n#&gt; # Intraclass Correlation Coefficient\n#&gt; \n#&gt;     Adjusted ICC: 0.712\n#&gt;   Unadjusted ICC: 0.712\n\nL’Intraclass Correlation Coefficient (ICC) di 0.7115643 offre una misura dell’affidabilità o della coerenza delle valutazioni all’interno dei gruppi definiti nel tuo modello (in questo caso, slpID e Speaker).\n\nValore dell’ICC: Il valore di 0.71 indica un livello relativamente alto di coerenza o omogeneità tra le misurazioni all’interno dei gruppi rispetto alla variabilità totale. In altre parole, una proporzione sostanziale della varianza totale nei dati è attribuibile alle differenze tra i gruppi piuttosto che alle variazioni casuali all’interno dei gruppi o agli errori di misurazione.\nAffidabilità delle Misure: Un ICC più vicino a 1 suggerisce che le misure sono molto affidabili, poiché indica che la maggior parte della varianza nei dati può essere attribuita alle differenze sistematiche tra i gruppi piuttosto che al rumore casuale o agli errori. Nel tuo caso, un ICC di circa 0.71 può essere interpretato come indicativo di un’alta affidabilità, suggerendo che le differenze tra i gruppi (ad esempio, tra diversi Speaker o differenti slpID) sono significative e consistenti.\nImplicazioni per la Ricerca: Per la ricerca, un ICC alto come questo implica che il disegno dello studio e la misurazione utilizzata sono adeguatamente sensibili per distinguere tra gli individui o le unità all’interno dei gruppi definiti. Questo è particolarmente rilevante quando si studiano gli effetti di interventi o trattamenti specifici su gruppi distinti o si valuta la consistenza delle risposte tra i membri di un gruppo.\nContesto e Benchmark: L’interpretazione dell’ICC dipende dal contesto specifico e dal campo di studio. Alcuni campi potrebbero considerare un ICC di 0.71 come eccellente, mentre altri potrebbero considerarlo solo moderatamente buono. È importante confrontare questo valore con benchmark o standard specifici del campo di interesse.\nStruttura del Modello e Dati: L’interpretazione dovrebbe anche tenere conto della struttura specifica del modello e della natura dei dati. Diversi tipi di ICC possono essere calcolati a seconda degli obiettivi dello studio e della configurazione del modello misto, quindi è cruciale assicurarsi che l’ICC calcolato sia il più appropriato per il tuo specifico contesto di ricerca.\n\nIn sintesi, un ICC di 0.7115643 indica un’alta affidabilità nelle misure all’interno dei gruppi definiti nel tuo modello, suggerendo che le variazioni osservate sono significativamente influenzate dalle differenze tra i gruppi piuttosto che dalla varianza casuale o dall’errore di misurazione.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>L'affidabilità tra giudici</span>"
    ]
  },
  {
    "objectID": "chapters/raters/02_interrater_reliability.html#riflessioni-conclusive",
    "href": "chapters/raters/02_interrater_reliability.html#riflessioni-conclusive",
    "title": "15  L’affidabilità tra giudici",
    "section": "\n15.5 Riflessioni Conclusive",
    "text": "15.5 Riflessioni Conclusive\nL’Intraclass Correlation Coefficient (ICC) misura la proporzione di varianza spiegata dalla struttura di raggruppamento in una popolazione. In altre parole, rappresenta il grado di somiglianza tra le misurazioni appartenenti allo stesso gruppo. Ad esempio, in un contesto scolastico, i punteggi di un test somministrato in diverse classi possono essere raggruppati per classe, e l’ICC indicherà quanto i punteggi all’interno della stessa classe siano simili. Questo coefficiente varia tra 0 e 1:\n\n\nICC pari a 0: il raggruppamento non fornisce alcuna informazione aggiuntiva, e le misurazioni nei gruppi sono indipendenti.\n\nICC pari a 1: tutte le osservazioni all’interno di un gruppo sono identiche.\n\nSecondo Gelman e Hill (2007), l’ICC può essere interpretato come la correlazione attesa tra due unità scelte casualmente dallo stesso gruppo. Tuttavia, questa definizione può non essere applicabile in modelli misti con strutture di effetti casuali più complesse (Hox, 2010).\nL’ICC è particolarmente utile per valutare se un modello misto è necessario:\n\n\nAlto ICC: indica un forte raggruppamento nei dati; le osservazioni all’interno dei gruppi sono fortemente correlate.\n\nBasso ICC: suggerisce un basso livello di raggruppamento; le osservazioni all’interno e tra i gruppi sono simili.\n\nUn ICC vicino a zero implica che l’aggiunta di effetti casuali al modello potrebbe essere superflua, poiché i cluster non contribuiscono a spiegare la variabilità dei dati.\n\n15.5.1 Relazione tra ICC e R²\nIl coefficiente di determinazione \\(R^2\\) rappresenta la proporzione di varianza spiegata da un modello statistico complessivo. Nel caso di modelli misti, la relazione tra \\(R^2\\) e ICC diventa evidente: entrambi derivano dal rapporto tra componenti di varianza.\n\n\n\\(R^2\\): proporzione di varianza totale spiegata dal modello completo (effetti fissi e casuali).\n\nICC: proporzione di varianza attribuibile esclusivamente agli effetti casuali.\n\nLa formula dell’ICC è:\n\\[\nICC = \\frac{\\sigma^2_i}{\\sigma^2_i + \\sigma^2_\\varepsilon}\n\\]\ndove:\n\n\n\\(\\sigma^2_i\\) è la varianza degli effetti casuali.\n\n\n\\(\\sigma^2_\\varepsilon\\) è la varianza residua.\n\n15.5.2 ICC Aggiustato e Non Aggiustato\nLa funzione icc() in R calcola sia l’ICC aggiustato che non aggiustato:\n\n\nICC aggiustato: considera solo gli effetti casuali ed è utile quando l’interesse è focalizzato su questi ultimi.\n\nICC non aggiustato: include anche la varianza degli effetti fissi nel denominatore, fornendo una misura più globale.\n\nSecondo Nakagawa et al. (2017), l’ICC aggiustato è spesso più rilevante in analisi che si concentrano sugli effetti casuali. La funzione icc() è versatile, calcolando l’ICC anche in modelli complessi, come quelli con pendenze casuali o design nidificati, e può essere applicata a modelli con distribuzioni non gaussiane.\n\n15.5.3 Conclusione\nL’ICC è uno strumento potente per quantificare il raggruppamento nei dati e decidere se un modello misto è appropriato. Oltre alla sua interpretazione intuitiva, la sua stretta relazione con \\(R^2\\) ne sottolinea l’importanza nell’analisi di varianza nei modelli misti. Funzioni come icc() offrono calcoli adattabili a una vasta gamma di contesti, rendendolo un indicatore flessibile e indispensabile per analisi avanzate.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>L'affidabilità tra giudici</span>"
    ]
  },
  {
    "objectID": "chapters/raters/02_interrater_reliability.html#session-info",
    "href": "chapters/raters/02_interrater_reliability.html#session-info",
    "title": "15  L’affidabilità tra giudici",
    "section": "\n15.6 Session Info",
    "text": "15.6 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] lme4_1.1-36       Matrix_1.7-2      car_3.1-3         carData_3.0-5    \n#&gt;  [5] ggokabeito_0.1.0  see_0.10.0        MASS_7.3-65       viridis_0.6.5    \n#&gt;  [9] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n#&gt; [13] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n#&gt; [17] lavaan_0.6-19     psych_2.4.12      scales_1.3.0      markdown_1.13    \n#&gt; [21] knitr_1.49        lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n#&gt; [25] dplyr_1.1.4       purrr_1.0.4       readr_2.1.5       tidyr_1.3.1      \n#&gt; [29] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.0      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] mime_0.12           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [25] R6_2.6.1            fastmap_1.2.0       rbibutils_2.3      \n#&gt;  [28] shiny_1.10.0        digest_0.6.37       OpenMx_2.21.13     \n#&gt;  [31] fdrtool_1.2.18      colorspace_2.1-1    rprojroot_2.0.4    \n#&gt;  [34] Hmisc_5.2-2         timechange_0.3.0    abind_1.4-8        \n#&gt;  [37] compiler_4.4.2      withr_3.0.2         glasso_1.11        \n#&gt;  [40] htmlTable_2.4.3     backports_1.5.0     performance_0.13.0 \n#&gt;  [43] ggsignif_0.6.4      corpcor_1.6.10      gtools_3.9.5       \n#&gt;  [46] tools_4.4.2         pbivnorm_0.6.0      foreign_0.8-88     \n#&gt;  [49] zip_2.3.2           httpuv_1.6.15       nnet_7.3-20        \n#&gt;  [52] glue_1.8.0          quadprog_1.5-8      nlme_3.1-167       \n#&gt;  [55] promises_1.3.2      lisrelToR_0.3       grid_4.4.2         \n#&gt;  [58] checkmate_2.3.2     cluster_2.1.8       reshape2_1.4.4     \n#&gt;  [61] generics_0.1.3      gtable_0.3.6        tzdb_0.4.0         \n#&gt;  [64] data.table_1.17.0   hms_1.1.3           utf8_1.2.4         \n#&gt;  [67] sem_3.1-16          pillar_1.10.1       rockchalk_1.8.157  \n#&gt;  [70] later_1.4.1         splines_4.4.2       lattice_0.22-6     \n#&gt;  [73] survival_3.8-3      kutils_1.73         tidyselect_1.2.1   \n#&gt;  [76] miniUI_0.1.1.1      pbapply_1.7-2       reformulas_0.4.0   \n#&gt;  [79] stats4_4.4.2        xfun_0.51           qgraph_1.9.8       \n#&gt;  [82] arm_1.14-4          stringi_1.8.4       yaml_2.3.10        \n#&gt;  [85] pacman_0.5.1        boot_1.3-31         evaluate_1.0.3     \n#&gt;  [88] codetools_0.2-20    mi_1.1              cli_3.6.4          \n#&gt;  [91] RcppParallel_5.1.10 rpart_4.1.24        xtable_1.8-4       \n#&gt;  [94] Rdpack_2.6.2        munsell_0.5.1       Rcpp_1.0.14        \n#&gt;  [97] coda_0.19-4.1       png_0.1-8           XML_3.99-0.18      \n#&gt; [100] parallel_4.4.2      jpeg_0.1-10         mvtnorm_1.3-3      \n#&gt; [103] insight_1.0.2       openxlsx_4.2.8      rlang_1.1.5        \n#&gt; [106] multcomp_1.4-28     mnormt_2.1.1\n\n\n\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>L'affidabilità tra giudici</span>"
    ]
  },
  {
    "objectID": "chapters/raters/E1_irr.html",
    "href": "chapters/raters/E1_irr.html",
    "title": "16  ✏️ Esercizi",
    "section": "",
    "text": "16.1 Dati categoriali\n(esercizi-irr)=\nIn questo tutorial vedremo come calcolare l’affidabilità tra giudici. Il metodo per calcolare l’affidabilità tra giudici dipende dal tipo di dati (categoriali, ordinali, continui) e dal numero di giudici.\nIniziamo con dati categoriali e utilizziamo un sottoinsieme dei dati forniti dal data frame diagnoses del pacchetto irr.\ndata(diagnoses)\nglimpse(diagnoses)\n\nRows: 30\nColumns: 6\n$ rater1 &lt;fct&gt; 4. Neurosis, 2. Personality Disorder, 2. Personality Disorder, ~\n$ rater2 &lt;fct&gt; 4. Neurosis, 2. Personality Disorder, 3. Schizophrenia, 5. Othe~\n$ rater3 &lt;fct&gt; 4. Neurosis, 2. Personality Disorder, 3. Schizophrenia, 5. Othe~\n$ rater4 &lt;fct&gt; 4. Neurosis, 5. Other, 3. Schizophrenia, 5. Other, 4. Neurosis,~\n$ rater5 &lt;fct&gt; 4. Neurosis, 5. Other, 3. Schizophrenia, 5. Other, 4. Neurosis,~\n$ rater6 &lt;fct&gt; 4. Neurosis, 5. Other, 5. Other, 5. Other, 4. Neurosis, 3. Schi~\ndat &lt;- diagnoses[, 1:3]\nhead(dat)\n\n\nA data.frame: 6 x 3\n\n\n\nrater1\nrater2\nrater3\n\n\n\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;fct&gt;\n\n\n\n\n1\n4. Neurosis\n4. Neurosis\n4. Neurosis\n\n\n2\n2. Personality Disorder\n2. Personality Disorder\n2. Personality Disorder\n\n\n3\n2. Personality Disorder\n3. Schizophrenia\n3. Schizophrenia\n\n\n4\n5. Other\n5. Other\n5. Other\n\n\n5\n2. Personality Disorder\n2. Personality Disorder\n2. Personality Disorder\n\n\n6\n1. Depression\n1. Depression\n3. Schizophrenia\nIniziamo considerando il caso di due giudici. Per questi dati calcoleremo il Kappa di Cohen.\nkappa2(dat[, c(1, 2)], \"unweighted\")\n\n Cohen's Kappa for 2 Raters (Weights: unweighted)\n\n Subjects = 30 \n   Raters = 2 \n    Kappa = 0.651 \n\n        z = 7 \n  p-value = 2.63e-12\nIl Kappa di Cohen valuta la concordanza tra due valutatori su una scala categoriale. Può essere usato quando i valutatori assegnano categorie, non punteggi numerici, agli oggetti di interesse. Un valore di Kappa maggiore di 0 indica una maggiore concordanza tra i valutatori rispetto a quella che ci si aspetterebbe per caso, mentre un valore di 0 indica che la concordanza non è maggiore di quella casuale.\nNel caso presente, l’output indica che:\nL’output fornisce anche un test statistico per valutare se la concordanza osservata è significativamente maggiore di quella che ci si aspetterebbe per caso:\nIn sintesi, l’output indica una concordanza sostanziale tra i due valutatori sulla scala categorica utilizzata, e questa concordanza è statisticamente significativa. Questo suggerisce che i giudizi dei due valutatori sono affidabili e consistenti tra loro oltre quello che ci si aspetterebbe semplicemente per caso.\nSe ci sono più di due giudici, usiamo il Kappa di Fleiss.\nkappam.fleiss(dat)\n\n Fleiss' Kappa for m Raters\n\n Subjects = 30 \n   Raters = 3 \n    Kappa = 0.534 \n\n        z = 9.89 \n  p-value = 0\nÈ anche possibile utilizzare il Kappa esatto di Conger (1980).\nkappam.fleiss(dat, exact = TRUE)\n\n Fleiss' Kappa for m Raters (exact value)\n\n Subjects = 30 \n   Raters = 3 \n    Kappa = 0.55\nNel caso di 3 giudici, otteniamo un valore di Kappa di Fleiss pari a 0.53, il che indica un’accordo moderato tra gli osservatori. Questo significa che c’è una certa concordanza tra di loro, ma non c’è né una forte né una perfetta corrispondenza. In generale, un Kappa di Fleiss compreso tra 0.41 e 0.60 può essere considerato come un’accordo moderato.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/raters/E1_irr.html#dati-categoriali",
    "href": "chapters/raters/E1_irr.html#dati-categoriali",
    "title": "16  ✏️ Esercizi",
    "section": "",
    "text": "Subjects = 30: Ci sono 30 oggetti o soggetti che sono stati valutati dai giudici.\nRaters = 2: Due valutatori hanno partecipato alla valutazione.\nKappa = 0.651: Il valore di Kappa, 0.651, suggerisce una concordanza sostanziale tra i due valutatori. Secondo le linee guida generalmente accettate per l’interpretazione del coefficiente di Kappa, un valore tra 0.61 e 0.80 indica una concordanza sostanziale.\n\n\n\nz = 7: Il valore z del test statistico è 7. Questo indica la distanza della statistica di test (Kappa osservato) dal valore di Kappa atteso sotto l’ipotesi nulla (nessuna concordanza oltre il caso), misurata in unità di deviazione standard. Un valore elevato indica una forte evidenza contro l’ipotesi nulla.\np-value = 2.63e-12: Il p-value è estremamente basso, molto inferiore a qualsiasi soglia di significatività comune (ad esempio, 0.05 o 0.01). Questo suggerisce che la probabilità di osservare un valore di Kappa come quello ottenuto (o più estremo) se in realtà non ci fosse concordanza tra i valutatori (oltre quella casuale) è estremamente bassa. In altre parole, c’è una forte evidenza statistica che la concordanza tra i due valutatori è significativamente maggiore di quella che ci si aspetterebbe per caso.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/raters/E1_irr.html#dati-ordinali",
    "href": "chapters/raters/E1_irr.html#dati-ordinali",
    "title": "16  ✏️ Esercizi",
    "section": "16.2 Dati ordinali",
    "text": "16.2 Dati ordinali\nSe i dati sono ordinali, è necessario utilizzare un Kappa ponderato. Ad esempio, se i valori possibili sono basso, medio e alto, allora se un caso viene valutato come medio da un codificatore e alto dall’altro, essi sarebbero in un accordo maggiore rispetto a una situazione in cui le valutazioni fossero basso e alto.\nPer chiarire ulteriormente, il concetto di Kappa ponderato si basa sull’idea che non tutte le discrepanze tra i codificatori siano ugualmente gravi. Nell’esempio dato, la differenza tra le valutazioni medio e alto è considerata meno significativa rispetto alla differenza tra basso e alto, perché le categorie sono vicine l’una all’altra nell’ordine. Il Kappa ponderato introduce quindi dei pesi per riflettere questa differenza di gravità nelle discrepanze, valutando le discrepanze minori (come tra medio e alto) meno severamente delle discrepanze maggiori (come tra basso e alto). Questo approccio è particolarmente utile in contesti dove l’ordine e la distanza tra le categorie sono informativi e importanti per l’analisi.\nUsiamo i dati forniti dal data frame anxiety del pacchetto irr.\n\ndata(anxiety)\n\ndfa &lt;- anxiety[, c(1, 2)]\ndfa\n\n\nA data.frame: 20 x 2\n\n\n\nrater1\nrater2\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n3\n3\n\n\n2\n3\n6\n\n\n3\n3\n4\n\n\n4\n4\n6\n\n\n5\n5\n2\n\n\n6\n5\n4\n\n\n7\n2\n2\n\n\n8\n3\n4\n\n\n9\n5\n3\n\n\n10\n2\n3\n\n\n11\n2\n2\n\n\n12\n6\n3\n\n\n13\n1\n3\n\n\n14\n5\n3\n\n\n15\n2\n2\n\n\n16\n2\n2\n\n\n17\n1\n1\n\n\n18\n2\n3\n\n\n19\n4\n3\n\n\n20\n3\n4\n\n\n\n\n\nPossiamo calcolare il Kappa ponderato sui giudizi di due valutatori. È possibile usare pesi lineari o quadrati delle differenze.\n\nkappa2(dfa, \"squared\")\n\n Cohen's Kappa for 2 Raters (Weights: squared)\n\n Subjects = 20 \n   Raters = 2 \n    Kappa = 0.297 \n\n        z = 1.34 \n  p-value = 0.18 \n\n\nQuesto valore di Kappa suggerisce che c’è un livello di accordo “equo” tra i due valutatori, considerando che i pesi delle discrepanze tra le valutazioni sono calcolati al quadrato. La scala convenzionale per interpretare Kappa è la seguente: valori ≤ 0 indicano nessun accordo, 0.01–0.20 leggero, 0.21–0.40 equo, 0.41–0.60 moderato, 0.61–0.80 sostanziale, e 0.81–1.00 quasi perfetto.\nIl valore di z è una misura della distanza statistica del valore di Kappa dal valore nullo (nessun accordo oltre il caso), espresso in termini di deviazioni standard. Un p-value di 0.18 indica che non c’è una significatività statistica per rifiutare l’ipotesi nulla di nessun accordo oltre il caso, al livello di significatività convenzionale di 0.05.\n\nkappa2(dfa, \"equal\")\n\n Cohen's Kappa for 2 Raters (Weights: equal)\n\n Subjects = 20 \n   Raters = 2 \n    Kappa = 0.189 \n\n        z = 1.42 \n  p-value = 0.157 \n\n\nCon pesi uguali per le discrepanze tra le valutazioni, il valore di Kappa scende a 0.189, indicando sempre un livello di accordo “leggero” verso “equo” tra i valutatori. Questo suggerisce che l’accordo non è molto forte e che le valutazioni differiscono più di quanto non facciano con i pesi al quadrato.\nCon un valore di z leggermente più alto rispetto al primo caso e un p-value di 0.157, anche qui non c’è evidenza statistica sufficiente per rifiutare l’ipotesi nulla. Il risultato implica che l’accordo osservato potrebbe ancora essere dovuto al caso, anche se il p-value è leggermente più basso qui, suggerendo una tendenza (anche se non significativa) verso un accordo maggiore rispetto al caso.\nIn conclusione, entrambi gli output indicano un livello di accordo che va da leggero a equo tra i due valutatori, con nessuna delle due misure che raggiunge la significatività statistica. Ciò suggerisce che, mentre c’è qualche grado di accordo oltre la coincidenza casuale, non è forte. La scelta dei pesi influisce leggermente sui risultati, con i pesi al quadrato che mostrano un livello di accordo leggermente superiore rispetto ai pesi uguali. Questo potrebbe riflettere la natura delle discrepanze nelle valutazioni: l’utilizzo di pesi al quadrato penalizza di più le grandi discrepanze rispetto ai pesi uguali.\n\nkappa2(dfa, \"unweighted\")\n\n Cohen's Kappa for 2 Raters (Weights: unweighted)\n\n Subjects = 20 \n   Raters = 2 \n    Kappa = 0.119 \n\n        z = 1.16 \n  p-value = 0.245 \n\n\n\nKappa = 0.119: Questo valore rappresenta il livello più basso di accordo tra i tre casi analizzati, indicando un accordo molto debole tra i valutatori. La mancanza di pesi implica che tutti i disaccordi sono stati trattati uniformemente, indipendentemente dalla loro gravità o distanza.\nz = 1.16, p-value = 0.245: Questo risultato conferma ulteriormente che l’accordo tra i valutatori non è statisticamente significativo, con un p-value ancora più alto rispetto ai casi precedenti, suggerendo una forte possibilità che qualsiasi accordo osservato sia casuale.\n\nI diversi valori di Kappa nei tre scenari riflettono l’effetto della ponderazione (o mancanza di essa) sulla valutazione dell’accordo. L’accordo sembra diminuire man mano che si passa da pesi quadrati a pesi uguali e infine a nessuna ponderazione, indicando che la severità dei disaccordi ha un impatto notevole sull’accordo percepito.\nNessuno dei tre scenari ha mostrato un accordo statisticamente significativo tra i valutatori, come indicato dai p-value superiori a 0.05. Ciò suggerisce che, indipendentemente dal metodo di ponderazione utilizzato, l’accordo osservato tra i valutatori potrebbe non essere distintamente migliore di quello che ci si aspetterebbe per caso.\nLa scelta del metodo di ponderazione può influenzare fortemente la stima dell’accordo tra i valutatori. In contesti dove la gravità dei disaccordi è importante, i pesi possono fornire una misura più accurata dell’accordo effettivo. Tuttavia, la mancanza di significatività statistica in tutti e tre i casi solleva questioni sulla coerenza delle valutazioni o sulla possibile necessità di formazione aggiuntiva per i valutatori per migliorare l’affidabilità delle loro valutazioni.\nI dati usati in precedenza erano numerici, ma il Kappa ponderato può anche essere calcolato nel caso di dati categoriali. In R i dati categoriali sono codificati in termini di fattori. Si noti che i livelli dei fattori devono essere nell’ordine corretto, altrimenti i risultati saranno errati.\n\ndfa2 &lt;- dfa\ndfa2$rater1 &lt;- factor(dfa2$rater1, levels = 1:6, labels = LETTERS[1:6])\ndfa2$rater2 &lt;- factor(dfa2$rater2, levels = 1:6, labels = LETTERS[1:6])\ndfa2 |&gt; head()\n\n\nA data.frame: 6 x 2\n\n\n\nrater1\nrater2\n\n\n\n&lt;fct&gt;\n&lt;fct&gt;\n\n\n\n\n1\nC\nC\n\n\n2\nC\nF\n\n\n3\nC\nD\n\n\n4\nD\nF\n\n\n5\nE\nB\n\n\n6\nE\nD\n\n\n\n\n\n\nlevels(dfa2$rater1) |&gt; print()\n\n[1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\"\n\n\n\nlevels(dfa2$rater2) |&gt; print()\n\n[1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\"\n\n\nI risultati sono uguali a quelli ottenuti con i dati numerici.\n\nkappa2(dfa2, \"squared\")\n\n Cohen's Kappa for 2 Raters (Weights: squared)\n\n Subjects = 20 \n   Raters = 2 \n    Kappa = 0.297 \n\n        z = 1.34 \n  p-value = 0.18 \n\n\n\nkappa2(dfa2, \"equal\")\n\n Cohen's Kappa for 2 Raters (Weights: equal)\n\n Subjects = 20 \n   Raters = 2 \n    Kappa = 0.189 \n\n        z = 1.42 \n  p-value = 0.157",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/raters/E1_irr.html#dati-continui",
    "href": "chapters/raters/E1_irr.html#dati-continui",
    "title": "16  ✏️ Esercizi",
    "section": "16.3 Dati continui",
    "text": "16.3 Dati continui\nQuando ci si confronta con variabili continue, è necessario calcolare il Coefficiente di Correlazione Intraclasse (ICC) per valutare l’accordo tra le misurazioni. La selezione dell’ICC appropriato richiede l’attenzione su diversi aspetti (Shrout e Fleiss, 1979). Questi includono:\n\nSelezione del Modello: È cruciale determinare se trattare esclusivamente i soggetti come effetti casuali, seguendo il modello “oneway” (impostazione predefinita), oppure se sia soggetti che valutatori sono stati scelti casualmente da un pool più ampio, adottando il modello “twoway”. Tale decisione dipende dalla struttura del disegno di studio e dall’obiettivo dell’analisi.\nInteresse per le Differenze: Nel caso in cui si desideri esaminare le differenze nei punteggi medi tra i valutatori, è preferibile calcolare l’“accordo” anziché la “coerenza” (quest’ultima è l’opzione predefinita). Questa scelta è cruciale quando le differenze sistematiche tra i valutatori sono rilevanti per l’analisi.\nUnità di Analisi: È necessario decidere se l’unità di analisi dovrebbe essere la media di più punteggi o se mantenere l’analisi su valori singoli (impostazione predefinita, unità=“singola”). La trasformazione dell’unità di analisi in “media” può essere appropriata quando l’interesse è concentrato sulla stima aggregata delle misurazioni, mentre l’opzione “singola” è generalmente preferita per esaminare la variabilità tra misurazioni individuali.\n\nLa scelta tra queste opzioni dovrebbe essere guidata dagli obiettivi specifici della ricerca, dalla natura dei dati e dal contesto in cui le misurazioni sono state raccolte. Tali decisioni influenzano notevolmente l’interpretazione del Coefficiente di Correlazione Intraclasse e, di conseguenza, le conclusioni che possono essere tratte riguardo all’accordo o alla variabilità delle misurazioni all’interno del campione studiato.\nPer illustrare questi concetti, useremo il set di dati anxiety dal pacchetto “irr”.\n\ndata(anxiety)\nanxiety |&gt; head()\n\n\nA data.frame: 6 x 3\n\n\n\nrater1\nrater2\nrater3\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n3\n3\n2\n\n\n2\n3\n6\n1\n\n\n3\n3\n4\n4\n\n\n4\n4\n6\n4\n\n\n5\n5\n2\n3\n\n\n6\n5\n4\n2\n\n\n\n\n\n\nicc(anxiety, model = \"twoway\", type = \"agreement\")\n\n Single Score Intraclass Correlation\n\n   Model: twoway \n   Type : agreement \n\n   Subjects = 20 \n     Raters = 3 \n   ICC(A,1) = 0.198\n\n F-Test, H0: r0 = 0 ; H1: r0 &gt; 0 \n F(19,39.7) = 1.83 , p = 0.0543 \n\n 95%-Confidence Interval for ICC Population Values:\n  -0.039 &lt; ICC &lt; 0.494\n\n\nPossiamo replicare questo risultato usando un modello ad effetti misti. Per fare questo dobbiamo prima trasformare i dati in formato long.\n\nanxiety_long &lt;- anxiety %&gt;%\n    mutate(subject = row_number()) %&gt;%\n    pivot_longer(\n        cols = starts_with(\"rater\"),\n        names_to = \"rater\",\n        values_to = \"score\",\n        names_prefix = \"rater\"\n    )\nglimpse(anxiety_long)\n\nRows: 60\nColumns: 3\n$ subject &lt;int&gt; 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7,~\n$ rater   &lt;chr&gt; \"1\", \"2\", \"3\", \"1\", \"2\", \"3\", \"1\", \"2\", \"3\", \"1\", \"2\", \"3\", \"1~\n$ score   &lt;int&gt; 3, 3, 2, 3, 6, 1, 3, 4, 4, 4, 6, 4, 5, 2, 3, 5, 4, 2, 2, 2, 1,~\n\n\nPossiamo ora adattare il modello misto ai dati.\n\nmodel &lt;- lmer(score ~ 1 + (1 | subject) + (1 | rater), data = anxiety_long)\n\nIl procedimento utilizzato da lmer per modellare i dati e calcolare componenti di varianza, che poi possono essere utilizzati per stimare l’Intraclass Correlation Coefficient (ICC), si basa sull’analisi degli effetti misti. Ecco una spiegazione dettagliata del processo:\n\nDefinizione del Modello: lmer(score ~ 1 + (1 | subject) + (1 | rater), data = anxiety_long) specifica un modello lineare misto, dove score è la variabile dipendente. La notazione (1 | subject) indica che si vuole considerare un effetto casuale per ogni soggetto (subject) che corrisponde unicamente all’intercetta. Analogamente, (1 | rater) indica un effetto casuale, corrispondente all’intercetta, per ogni valutatore (rater), permettendo che ogni valutatore possa avere una sua propria tendenza generale nella valutazione. Il termine 1 rappresenta l’intercetta fissa comune a tutti i dati.\nEffetti Fissi e Casuali: In questo modello, l’unico effetto fisso è l’intercetta globale (il termine 1 nel modello), che rappresenta la media generale dei punteggi di ansia. Gli effetti casuali sono rappresentati dalle varianze associate a ciascun soggetto e valutatore, indicando che si riconosce la presenza di variazioni individuali nei punteggi di ansia attribuibili a questi due fattori.\nInterpretazione delle Componenti di Varianza: Le componenti di varianza estratte dal modello rappresentano la quantità di variazione nei punteggi di ansia attribuibili a variazioni tra soggetti (var_subject), a variazioni tra valutatori (var_rater), e alla variazione residua non spiegata dal modello (var_residual).\n\nDopo aver stimato le componenti di varianza, l’ICC può essere calcolato come una proporzione della varianza tra soggetti e tra giudici rispetto alla varianza totale.\nEsaminiamo le componenti di varianza:\n\nvc &lt;- as.data.frame(VarCorr(model))\nvc\n\n\nA data.frame: 3 x 5\n\n\ngrp\nvar1\nvar2\nvcov\nsdcor\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nsubject\n(Intercept)\nNA\n0.3991227\n0.6317616\n\n\nrater\n(Intercept)\nNA\n0.1684205\n0.4103906\n\n\nResidual\nNA\nNA\n1.4482458\n1.2034308\n\n\n\n\n\nCalcolo dell’ICC:\n\n(vc$vcov[1] + vc$vcov[2]) / (vc$vcov[1] + vc$vcov[2] + vc$vcov[3])\n\n0.281548906954477\n\n\nQuesto risultato riproduce quello trovato da icc():\n\nperformance::icc(model)\n\n\nA icc: 1 x 3\n\n\nICC_adjusted\nICC_conditional\nICC_unadjusted\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n0.2815489\n0.2815489\n0.2815489",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/validity/01_validity.html",
    "href": "chapters/validity/01_validity.html",
    "title": "17  La Validità del test",
    "section": "",
    "text": "17.1 Introduzione\nOltre all’affidabilità, la validità rappresenta la seconda caratteristica essenziale che uno strumento psicometrico deve avere. La validità è una proprietà psicometrica fondamentale dei test psicologici. La definizione degli Standards for educational and psychological testing (American Educational Research Association et al., 2014) è la seguente:\nIn altre parole, la validità riguarda sia il significato dei punteggi del test che il modo in cui li utilizziamo. Pertanto, la validità è giustamente “la considerazione più fondamentale nello sviluppo e nella valutazione dei test”, come indicato negli Standards (p. 11).\nIl concetto di validità, un tempo circoscritto alla triade contenuto-criterio-costrutto, si è evoluto in un quadro concettuale più ampio e dinamico. Gli Standards affermano:\nDi conseguenza, la maggior parte delle concezioni moderne di validità enfatizzano un’integrazione di tutte le forme di evidenza utili a chiarire il significato(i) che possono essere attribuiti ai punteggi del test. Spetta all’utente del test valutare le prove disponibili per giudicare in che misura la sua interpretazione o utilizzo previsto sia appropriato.\nNel campo della psicometria, esiste un consenso sul fatto che i concetti tradizionali di validità, legati direttamente a un test, siano stati superati. Oggi si riconosce che la validità non riguarda il test in sé, ma l’adeguatezza e l’accuratezza delle interpretazioni dei punteggi ottenuti. In altre parole, non è corretto parlare di “validità di un test”. La validità si riferisce alle interpretazioni che vengono fatte dei punteggi del test.\nPertanto, non è corretto chiedere: “Il Wechsler Intelligence Scale for Children—Quinta Edizione (WISC-V) è un test valido?”. La domanda più appropriata è: “È valida l’interpretazione delle prestazioni sul WISC-V come misura dell’intelligenza?”. La validità dipende sempre dal contesto dell’interpretazione: cosa significa ottenere un certo punteggio su questo test? La validità si applica all’interpretazione di questo risultato, non al test stesso.",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>La Validità del test</span>"
    ]
  },
  {
    "objectID": "chapters/validity/01_validity.html#introduzione",
    "href": "chapters/validity/01_validity.html#introduzione",
    "title": "17  La Validità del test",
    "section": "",
    "text": "Validity refers to the degree to which evidence and theory support the interpretations of test scores for proposed uses of tests.\n\n\n\n\nValidity is a unitary concept. It is the degree to which all the accumulated evidence supports the intended interpretation of test scores for the proposed use. Like the 1999 Standards, this edition refers to types of validity evidence, rather than distinct types of validity. To emphasize this distinction, the treatment that follows does not follow historical nomenclature (i.e., the use of the terms content validity or predictive validity). (2014, p. 14)",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>La Validità del test</span>"
    ]
  },
  {
    "objectID": "chapters/validity/01_validity.html#minacce-alla-validità",
    "href": "chapters/validity/01_validity.html#minacce-alla-validità",
    "title": "17  La Validità del test",
    "section": "17.2 Minacce alla Validità",
    "text": "17.2 Minacce alla Validità\nLa validità di un test può essere compromessa quando esso non misura in modo completo il costrutto di interesse, oppure quando include aspetti non pertinenti al costrutto stesso. Anche test con alta affidabilità possono risultare vulnerabili a queste problematiche, con il rischio di interpretazioni distorte dei risultati.\nIn questa sezione, analizzeremo i principali tipi di validità e le evidenze associate, evidenziando come le diverse fonti di prova possano essere integrate per costruire un argomento solido a sostegno della validità di un test. L’obiettivo è fornire una panoramica chiara e pratica su come riconoscere e affrontare le potenziali minacce alla validità nella valutazione psicometrica.\n\n17.2.1 Sotto-Rappresentazione del Costrutto\nLa sotto-rappresentazione del costrutto si verifica quando il test non riesce a misurare aspetti cruciali del costrutto target. Ad esempio, un test di matematica per la terza elementare che valuta solo la divisione non rappresenta adeguatamente tutte le competenze matematiche previste per quel livello scolastico. Per affrontare questa lacuna, è necessario ampliare il contenuto del test per includere tutte le abilità matematiche rilevanti nel curriculum della terza elementare.\n\n\n17.2.2 Varianza Estranea al Costrutto\nLa varianza estranea al costrutto si verifica quando un test, oltre a valutare il costrutto di interesse, misura involontariamente altre caratteristiche o competenze non pertinenti. Ad esempio, un test di matematica che richiede un elevato livello di comprensione del testo potrebbe finire per valutare anche le capacità di lettura, anziché concentrarsi esclusivamente sulle competenze matematiche.\nPer minimizzare questa varianza estranea, è fondamentale progettare il test con attenzione, utilizzando istruzioni chiare e un linguaggio accessibile, adeguato al livello di comprensione della popolazione di riferimento. Questo garantisce che il test misuri il costrutto target in modo più preciso e riduca il rischio di interferenze da fattori non rilevanti.\n\n\n17.2.3 Altri Fattori Che Influenzano la Validità\nOltre alle caratteristiche intrinseche del test, ci sono fattori esterni che possono influenzare la validità delle interpretazioni dei risultati. Questi includono:\n\nCaratteristiche dell’Esaminando:\n\nFattori personali, come ansia, bassa motivazione o distrazioni, possono influenzare le prestazioni e ridurre la validità delle interpretazioni dei punteggi.\n\nProcedure di Amministrazione e Valutazione:\n\nQualsiasi deviazione dalle procedure standard di somministrazione può compromettere la validità. Anche gli adattamenti per esigenze speciali devono essere gestiti con cura per garantire che le interpretazioni dei risultati rimangano valide.\n\nIstruzione e Coaching:\n\nIstruzioni o coaching specifici prima del test possono alterare la validità, soprattutto se gli esaminandi vengono addestrati a rispondere a particolari tipologie di domande, distorcendo così l’interpretazione delle loro competenze reali.\n\n\nInfine, la validità delle interpretazioni basate su punteggi norm-referenced (cioè confronti rispetto a un gruppo di riferimento) dipende dall’adeguatezza e rappresentatività del campione di riferimento utilizzato per il confronto.\nIn sintesi, le minacce alla validità richiedono un’attenta valutazione. Solo affrontando questi fattori sarà possibile garantire che le interpretazioni dei risultati del test siano appropriate. Il processo di validazione deve quindi considerare sia il contenuto e la struttura del test sia le influenze esterne che possono distorcere le conclusioni tratte dai punteggi.",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>La Validità del test</span>"
    ]
  },
  {
    "objectID": "chapters/validity/01_validity.html#tipologie-di-validità",
    "href": "chapters/validity/01_validity.html#tipologie-di-validità",
    "title": "17  La Validità del test",
    "section": "17.3 Tipologie di Validità",
    "text": "17.3 Tipologie di Validità\n\n17.3.1 Evoluzione Storica e Cambiamenti Concettuali\nLa definizione e l’applicazione della validità nei test psicologici hanno subito un’evoluzione sostanziale nel tempo, come documentato negli Standards for Educational and Psychological Testing del 2014 (American Educational Research Association et al., 2014). In passato, la validità era suddivisa in tre categorie principali: validità di contenuto, validità criteriale e validità di costrutto (Messick, 1989). Questa distinzione serviva a fornire una struttura per interpretare i punteggi dei test. Tuttavia, si è successivamente adottato un approccio più integrato, in cui ogni evidenza di validità contribuisce collettivamente a sostenere le interpretazioni dei punteggi.\n\n\n17.3.2 Le Tre Categorie Tradizionali di Validità\n\nValidità di Contenuto\nValuta quanto il contenuto del test rappresenti adeguatamente il dominio teorico del costrutto.\n\nEsempio: Un test di geometria deve coprire i principali concetti previsti dal curriculum.\n\nMetodo: Giudizi esperti analizzano l’aderenza del contenuto al costrutto teorico.\n\nValidità Criteriale\nEsamina la relazione tra i punteggi del test e un criterio esterno rilevante.\n\nEsempio: Un test di selezione deve predire con precisione le prestazioni lavorative.\n\nAnalisi: Correlazioni e regressioni tra il test e i criteri esterni.\n\nValidità di Costrutto\nVerifica quanto il test misuri il costrutto teorico di riferimento.\n\nEvidenze chiave:\n\nCoerenza con modelli teorici.\n\nCorrelazioni con misure simili (validità convergente).\n\nCorrelazioni ridotte con misure di costrutti diversi (validità divergente).\n\nAnalisi fattoriale per confermare la struttura teorica.\n\nCapacità predittiva in relazione a comportamenti o fenomeni legati al costrutto.\n\n\n\n\n\n17.3.3 Il Passaggio a un Modello Unitario\nCon l’introduzione degli Standards del 1985 (APA et al., 1985), le tre categorie di validità sono state superate in favore di un approccio unitario. La validità è ora intesa come il grado in cui tutte le evidenze disponibili supportano l’interpretazione prevista dei punteggi di un test per uno scopo specifico.\nGli Standards del 2014 (American Educational Research Association et al., 2014) hanno formalizzato questa prospettiva, organizzando le evidenze di validità in cinque categorie principali:\n\nProve Basate sul Contenuto del Test\nValutano se il contenuto riflette accuratamente il costrutto teorico.\nProve Basate sui Processi di Risposta\nAnalizzano i processi cognitivi e comportamentali adottati dagli esaminandi per rispondere agli item.\nProve Basate sulla Struttura Interna\nEsaminano la coerenza tra la struttura empirica del test (es. fattori o dimensioni) e il modello teorico.\nProve Basate sulle Relazioni con Altre Variabili\nEsplorano le connessioni tra i punteggi del test e altre variabili esterne pertinenti.\nProve Basate sulle Conseguenze del Test\nConsiderano gli effetti derivanti dall’utilizzo del test, inclusi impatti sociali o educativi.\n\nIn conclusione, l’evoluzione concettuale della validità testimonia il passaggio da un approccio frammentato a una prospettiva integrata. L’obiettivo è garantire che l’interpretazione dei punteggi sia supportata da un insieme di evidenze complementari, fornendo così una base solida e scientificamente fondata per l’utilizzo dei test psicologici.",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>La Validità del test</span>"
    ]
  },
  {
    "objectID": "chapters/validity/01_validity.html#ottenere-evidenze-di-validità",
    "href": "chapters/validity/01_validity.html#ottenere-evidenze-di-validità",
    "title": "17  La Validità del test",
    "section": "17.4 Ottenere Evidenze di Validità",
    "text": "17.4 Ottenere Evidenze di Validità\nLa validità si definisce come il grado in cui evidenze empiriche e teoria sostengono le interpretazioni dei punteggi di un test per gli scopi previsti. Questa definizione implica che non è possibile ottenere prove per tutte le interpretazioni o applicazioni possibili di un test. Pertanto, il primo passo cruciale nel processo di validazione consiste nel definire chiaramente le interpretazioni e gli utilizzi specifici per cui il test è stato progettato.\nGli Standards for Educational and Psychological Testing del 2014 (American Educational Research Association et al., 2014) suddividono le evidenze di validità in cinque categorie principali, che offrono una struttura sistematica per raccogliere e organizzare prove a supporto delle interpretazioni e degli utilizzi previsti. Di seguito, ciascuna categoria viene analizzata nel dettaglio, evidenziando il loro ruolo nel processo di validazione.\n1. Prove Basate sul Contenuto del Test\nLe prove basate sul contenuto si concentrano sulla misura in cui il contenuto di un test riflette accuratamente il dominio teorico che si propone di valutare. Tuttavia, nella pratica, è raro che un test riesca a coprire integralmente il dominio di un costrutto. Questo limite può generare due problemi principali: la sotto-rappresentazione del costrutto, che si verifica quando il test non include aspetti fondamentali del dominio, e la presenza di varianza estranea, causata da elementi non pertinenti che interferiscono con la misurazione. Entrambi i problemi rappresentano potenziali minacce alla validità.\nPer ridurre tali rischi, il contenuto del test deve essere pianificato con cura, garantendo una selezione bilanciata e rappresentativa degli item. Il processo di validazione coinvolge spesso giudizi esperti, i quali valutano due criteri essenziali:\n\nRilevanza degli item: Ogni item deve essere pertinente al costrutto che si intende misurare.\nAmpiezza della copertura: L’insieme degli item deve rappresentare in modo sufficientemente completo le varie dimensioni del costrutto.\n\nUn aspetto correlato è la validità di facciata, che si riferisce alla percezione, da parte di non esperti o degli esaminandi, della plausibilità del test come misura del costrutto. Sebbene non costituisca una prova tecnica di validità, può influenzare la motivazione e la collaborazione degli esaminandi durante la somministrazione.\n2. Prove Basate sui Processi di Risposta\nQuesto tipo di evidenza valuta se i processi cognitivi che gli esaminandi utilizzano per rispondere al test riflettono il costrutto che il test intende misurare. Per esempio, in un test di ragionamento matematico, è essenziale che i partecipanti applichino strategie di risoluzione dei problemi piuttosto che semplici procedure meccaniche.\nI processi di risposta possono essere valutati tramite interviste, analisi dei tempi di risposta, o monitoraggio dei movimenti oculari. Anche i criteri utilizzati dai valutatori nel punteggio possono essere inclusi in questo tipo di evidenza. L’obiettivo è garantire che il processo di risoluzione degli item sia coerente con il costrutto che si desidera misurare.\n3. Prove Basate sulla Struttura Interna\nL’evidenza basata sulla struttura interna si concentra sulle relazioni tra gli elementi del test e la loro coerenza con le dimensioni teoriche del costrutto. L’analisi fattoriale è uno strumento chiave per esaminare se la struttura interna del test riflette le dimensioni ipotizzate. Questo tipo di evidenza è particolarmente importante per i test multidimensionali, come quelli di personalità, in cui ci si aspetta che diverse dimensioni siano rappresentate coerentemente dagli item.\n4. Prove Basate sulle Relazioni con Altre Variabili.\nLe evidenze ottenute da relazioni con altre variabili permettono di valutare se un test misura effettivamente il costrutto target e come i suoi punteggi si collegano a misure esterne rilevanti. Queste relazioni includono correlazioni con strumenti simili, criteri di riferimento o variabili teoricamente associate. Tra le principali tipologie di evidenze troviamo la validità convergente, discriminante e criteriale.\nValidità Convergente e Discriminante. La validità convergente indica quanto i punteggi di un test correlano con quelli di altre misure che valutano lo stesso costrutto o costrutti correlati. Ad esempio, un nuovo test di intelligenza dovrebbe mostrare una forte correlazione con strumenti consolidati che misurano lo stesso costrutto, dimostrando coerenza e supportando l’ipotesi che stia misurando ciò che dichiara.\nAl contrario, la validità discriminante verifica che i punteggi del test non siano correlati con misure di costrutti teoricamente distinti. Per esempio, un test di abilità verbali non dovrebbe mostrare un’alta correlazione con un test di abilità motorie. Questo conferma che il test è specifico per il dominio che intende misurare, evitando influenze estranee.\nValidità Criteriale. La validità criteriale esamina la relazione tra i punteggi del test e un criterio esterno rilevante, articolandosi in:\n\nValidità predittiva, che valuta la capacità del test di prevedere risultati futuri legati al costrutto. Ad esempio, un test di selezione dovrebbe prevedere con precisione la performance lavorativa.\nValidità concorrente, che analizza se i punteggi del test concordano con una misura esistente raccolta nello stesso momento. Per esempio, un test di abilità sociali dovrebbe produrre risultati coerenti con quelli di strumenti già consolidati.\n\nEntrambe le tipologie si basano su analisi statistiche come correlazioni e regressioni per quantificare la relazione tra test e criterio.\nConfronto tra Gruppi. Un’altra fonte di evidenze deriva dall’analisi delle differenze tra gruppi. Se un test misura correttamente il costrutto, ci si aspetta che i suoi punteggi varino tra gruppi con caratteristiche rilevanti. Ad esempio, un test di intelligenza dovrebbe produrre punteggi più elevati in individui con alti livelli di istruzione rispetto a quelli con livelli più bassi. Questa sensibilità alle differenze teoricamente attese rafforza l’affidabilità del test.\nSensibilità e Specificità. Nel contesto della classificazione o diagnosi, è essenziale valutare:\n\nSensibilità, ossia la capacità del test di identificare correttamente i casi positivi (ad esempio, individui con una determinata condizione).\n\nSpecificità, che misura la capacità di escludere correttamente i casi negativi.\n\nUn test efficace bilancia sensibilità e specificità, riducendo al minimo errori di classificazione.\nGeneralizzazione della Validità. La validità di un test deve essere confermata anche in nuovi contesti, popolazioni o condizioni, un processo noto come generalizzazione della validità. Studi come le meta-analisi aiutano a identificare come differenze nei risultati possano derivare da variazioni metodologiche o artefatti statistici, offrendo una base per estendere l’uso del test.\nIn conclusione, le relazioni con altre variabili forniscono evidenze fondamentali per dimostrare che un test misura accuratamente il costrutto target e si comporta come previsto in relazione a criteri esterni, misure correlate e differenze tra gruppi. Attraverso analisi di validità convergente, discriminante e criteriale, nonché grazie a metriche come sensibilità e specificità, queste evidenze sostengono l’utilità e l’affidabilità del test in molteplici applicazioni.\n5. Prove Basate sulle Conseguenze del Test\nLe conseguenze derivanti dall’uso di un test, sia previste che non previste, rappresentano un aspetto critico della validità. Queste conseguenze includono l’impatto diretto sui singoli individui e le implicazioni più ampie per la società, come nel caso di test di ammissione scolastica o di strumenti psicologici impiegati in ambito forense. Anche gli effetti indesiderati, come l’uso improprio del test o l’amplificazione di disuguaglianze, devono essere attentamente valutati per garantire l’integrità e l’etica del processo di misurazione.\nGli Standards distinguono tra conseguenze intenzionali, legate agli obiettivi dichiarati del test, e conseguenze non intenzionali, che emergono come effetti collaterali non previsti. Entrambe le categorie sono essenziali per una valutazione completa della validità e per promuovere un uso responsabile dei test.\nConseguenze Intenzionali. Le conseguenze intenzionali si riferiscono ai risultati attesi e desiderati per cui il test è stato progettato.\n\nEsempio: Un test di selezione del personale è progettato per individuare i candidati più qualificati per un ruolo specifico. Analogamente, un test di rendimento scolastico mira a valutare le competenze degli studenti rispetto a uno standard educativo.\n\nValutazione della validità consequenziale: Questo tipo di analisi esamina se il test raggiunge gli obiettivi dichiarati, come migliorare i processi decisionali o ridurre costi organizzativi, e garantisce che il suo utilizzo sia giustificato dai benefici ottenuti.\n\nConseguenze Non Intenzionali. Le conseguenze non intenzionali si manifestano quando l’uso di un test genera effetti non previsti, che possono minacciare l’equità o l’utilità del test.\n\nEsempio: Un test valido per la selezione del personale potrebbe penalizzare sistematicamente candidati appartenenti a gruppi socioeconomici svantaggiati, evidenziando una potenziale distorsione culturale o metodologica.\n\nRilevanza: Questi effetti collaterali devono essere monitorati per garantire che il test non introduca disparità ingiustificate, induca ansia nei partecipanti o sia utilizzato in modi non appropriati al contesto.\n\nValutazione delle Conseguenze. Gli Standards sottolineano che il monitoraggio delle conseguenze, sia intenzionali che non intenzionali, deve essere parte integrante del processo di validazione. Ciò include:\n\nMonitoraggio continuo: Verifica dell’impatto del test nel tempo, sia sui singoli individui che sul sistema sociale.\n\nAnalisi etica: Considerazione degli effetti potenziali sull’equità, sull’accesso alle opportunità e sulla riduzione dei pregiudizi sistemici.\n\nConseguenze Sociali e Politiche. L’utilizzo dei test può avere implicazioni più ampie, che vanno oltre la validità tecnica e riguardano questioni sociali e politiche. Ad esempio, i test standardizzati possono influenzare le politiche di accesso all’istruzione o al lavoro.\n\nDistinzione chiara: Sebbene queste conseguenze siano importanti, gli Standards suggeriscono di trattarle separatamente dalle evidenze strettamente legate alla validità del test. Ciò evita di sovrapporre questioni tecniche con considerazioni di valore più ampie, pur riconoscendone l’importanza etica.\n\nConsiderazione delle Alternative. Valutare le conseguenze di un test implica anche considerare gli effetti dell’assenza di un test o dell’uso di approcci alternativi.\n\nEsempio: L’eliminazione di test strutturati potrebbe portare a decisioni basate su criteri meno oggettivi, come valutazioni soggettive o pregiudizi personali.\n\nEquilibrio: Pur con i loro limiti, i test standardizzati spesso offrono una maggiore equità rispetto a processi meno formalizzati, che possono amplificare distorsioni culturali o di genere.\n\nResponsabilità Etica e Validità Consequenziale. Gli sviluppatori e gli utilizzatori di test hanno la responsabilità di garantire che l’uso del test generi benefici giustificati e minimizzi le conseguenze negative. Questo richiede:\n\nRevisione continua: Aggiornamento dei test per adattarli a nuove esigenze e contesti.\nPromozione dell’equità: Minimizzazione degli effetti collaterali negativi e attenzione ai principi di giustizia ed etica.\n\nLa validità consequenziale non si limita alla misurazione accurata di un costrutto, ma si estende all’analisi degli effetti pratici e morali dell’uso del test.\nIn sintesi, le prove basate sulle conseguenze del test rappresentano un aspetto fondamentale per valutare la validità di uno strumento psicometrico. Considerare sia le conseguenze intenzionali che quelle non intenzionali consente di garantire che il test non solo misuri accuratamente il costrutto di interesse, ma contribuisca anche in modo etico e positivo agli scopi dichiarati. Monitorare costantemente gli effetti di un test è essenziale per preservarne l’efficacia, l’integrità e l’equità nell’applicazione.",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>La Validità del test</span>"
    ]
  },
  {
    "objectID": "chapters/validity/01_validity.html#integrazione-delle-prove-di-validità",
    "href": "chapters/validity/01_validity.html#integrazione-delle-prove-di-validità",
    "title": "17  La Validità del test",
    "section": "17.5 Integrazione delle Prove di Validità",
    "text": "17.5 Integrazione delle Prove di Validità\nGli Standards descrivono la validità come un processo continuo di costruzione di un argomento coerente e supportato da evidenze a favore dell’interpretazione e dell’uso dei punteggi di un test. L’integrazione delle diverse prove di validità è cruciale per garantire che le interpretazioni dei risultati siano appropriate e sostenute da evidenze robuste.\nL’integrazione delle prove di validità comporta la combinazione di diverse fonti di evidenza per costruire un argomento completo e coerente che giustifichi l’uso del test per uno scopo specifico. Questo processo si realizza raccogliendo diverse linee di prova, che possono includere prove basate sul contenuto, sulla struttura interna, sui processi di risposta, sulle relazioni con altre variabili e sulle conseguenze del test.\nL’integrazione non avviene in modo meccanico, ma richiede una riflessione critica su come ogni prova contribuisca all’argomento complessivo. Ogni tipo di prova fornisce un’informazione parziale, e il loro insieme contribuisce a creare un quadro completo della validità del test. Ad esempio, se un test mostra coerenza interna ma non è in grado di predire accuratamente i criteri per cui è stato progettato, la validità potrebbe essere compromessa. Viceversa, l’integrazione di evidenze positive da più fonti rafforza la giustificazione per l’uso del test.\nL’integrazione delle prove di validità serve a supportare l’argomento secondo cui i punteggi del test sono appropriati per l’interpretazione e l’uso previsto. Questo approccio consente di ottenere una visione olistica della validità del test e di garantire che le diverse dimensioni della validità siano state considerate in modo approfondito. L’obiettivo finale è dimostrare che il test è non solo tecnicamente affidabile, ma anche giustificato eticamente e utilizzabile per prendere decisioni informate.\nLa validazione non è mai un processo singolare o statico. Un argomento di validità ben costruito considera come le diverse evidenze interagiscono per confermare o smentire l’uso del test in contesti specifici. Gli Standards sottolineano che la validità non è una proprietà del test in sé, ma riguarda le interpretazioni e gli utilizzi dei punteggi. Pertanto, ogni volta che il test viene applicato in un nuovo contesto o con un obiettivo diverso, è necessario rivalutare la validità delle interpretazioni, raccogliendo nuove prove se necessario.\nL’integrazione delle prove di validità non si conclude con lo sviluppo iniziale del test. Al contrario, è un processo continuo che si evolve con il tempo, man mano che vengono condotte nuove ricerche o che cambiano i contesti d’uso del test. Gli Standards evidenziano che, oltre alle prove fornite dai creatori del test, la ricerca indipendente svolge un ruolo fondamentale nel mantenere e aggiornare l’argomento di validità. Studi successivi all’adozione del test, condotti da ricercatori indipendenti, possono rafforzare, modificare o persino contraddire le evidenze iniziali, contribuendo così a una comprensione più completa della validità.\nIn sintesi, l’integrazione delle prove di validità è un processo critico per costruire un argomento solido a sostegno dell’uso e dell’interpretazione dei punteggi di un test. Essa comporta la raccolta e la sintesi di diverse linee di evidenza, ciascuna delle quali contribuisce a illuminare un aspetto particolare della validità. L’obiettivo finale è garantire che i test siano non solo tecnicamente adeguati, ma anche utili e giustificabili per gli scopi previsti, e che continuino a esserlo nel tempo grazie alla continua ricerca e revisione.",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>La Validità del test</span>"
    ]
  },
  {
    "objectID": "chapters/validity/01_validity.html#riflessioni-conclusive",
    "href": "chapters/validity/01_validity.html#riflessioni-conclusive",
    "title": "17  La Validità del test",
    "section": "17.6 Riflessioni Conclusive",
    "text": "17.6 Riflessioni Conclusive\nNel campo della psicometria, la validità rappresenta un concetto dinamico, sfaccettato e complesso, che richiede un’integrazione critica di molteplici forme di evidenza. Questo capitolo ha esplorato le diverse dimensioni della validità, mettendo in luce l’importanza di un approccio olistico per garantire che l’interpretazione dei punteggi dei test sia appropriata e significativa. L’analisi della validità va ben oltre la semplice coerenza tra il contenuto del test e il costrutto target; essa include un esame rigoroso della struttura interna del test, dei processi cognitivi e comportamentali attivati nei rispondenti e delle conseguenze – sia attese che inattese – del suo utilizzo.\nGli Standards sottolineano come la validità non sia un attributo fisso del test stesso, ma una proprietà delle interpretazioni e degli utilizzi dei punteggi del test. Questo implica che il processo di validazione deve essere continuo, assimilando nuove ricerche e aggiornamenti man mano che emergono nuove evidenze. La validità si costruisce attraverso l’integrazione di diverse linee di prova – dalle relazioni con altre variabili, alla struttura interna, ai processi di risposta, fino alle conseguenze del test – ognuna delle quali contribuisce a consolidare l’argomento di validità.\nL’integrazione delle prove non si esaurisce con lo sviluppo iniziale del test, ma continua nel tempo, con una costante attenzione critica che deve accompagnare ogni nuovo contesto d’uso. Gli psicologi, pertanto, hanno la responsabilità di valutare e rivalutare l’uso dei test nel loro specifico contesto professionale, garantendo che le decisioni prese siano informate, etiche e giustificate da prove solide.\nIn conclusione, la validazione di un test psicometrico deve essere intesa come un processo evolutivo e dinamico. Non si tratta di un’analisi statica, ma di una valutazione continua della capacità del test di produrre interpretazioni affidabili e pertinenti nei diversi contesti applicativi. Questo richiede un costante impegno da parte dei professionisti nel garantire che i test siano non solo strumenti tecnicamente validi, ma anche adeguati e responsabili dal punto di vista etico e pratico. La validità, dunque, è il risultato di un’interazione tra prove empiriche, teoria e pratica, che richiede una continua revisione e miglioramento per mantenere l’efficacia e l’integrità del test nel tempo.",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>La Validità del test</span>"
    ]
  },
  {
    "objectID": "chapters/validity/01_validity.html#esercizi",
    "href": "chapters/validity/01_validity.html#esercizi",
    "title": "17  La Validità del test",
    "section": "17.7 Esercizi",
    "text": "17.7 Esercizi\nPresentazione in classe dei lavori di Randall et al. (2023) e Randall (2021).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAmerican Educational Research Association, American Psychological Association, & National Council on Measurement in Education. (2014). Standards for Educational and Psychological Testing. American Educational Research Association.\n\n\nArias, A. (2024). A Short Tutorial on Validation in Educational and Psychological Assessment. Teaching Quantitative Methods Vignettes, 20(3).\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.\n\n\nRandall, J. (2021). «Color-neutral» is not a thing: Redefining construct definition and representation through a justice-oriented critical antiracist lens. Educational Measurement: Issues and Practice, 40(4), 82–90.\n\n\nRandall, J., Slomp, D., Poe, M., & Oliveri, E. (2023). Disrupting white supremacy in assessment: Toward a justice-oriented, antiracist validity framework. In Twin Pandemics (pp. 78–86). Routledge.",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>La Validità del test</span>"
    ]
  },
  {
    "objectID": "chapters/validity/02_other_variables.html",
    "href": "chapters/validity/02_other_variables.html",
    "title": "18  Relazioni test-criterio",
    "section": "",
    "text": "18.1 Introduzione\nIn questo capitolo, approfondiamo un aspetto specifico della validità già discusso: la relazione tra test e criterio. Per analizzare questo concetto in maniera sistematica, utilizzeremo un approccio quantitativo basato sulla regressione logistica. Questo strumento permette di valutare in modo dettagliato e rigoroso come i punteggi di un test siano correlati o predittivi rispetto a un criterio esterno specifico. Tale analisi offre una comprensione più completa della validità di uno strumento psicometrico, evidenziandone l’utilità pratica e teorica.\nIn particolare, ci concentreremo sulla capacità dei test di predire o differenziare fenomeni specifici attraverso la Relazione Test-Criterio. L’uso della regressione logistica consente di identificare le relazioni chiave tra le variabili e di quantificare il grado di validità di un test in contesti applicativi, fornendo così uno strumento fondamentale per la valutazione della qualità psicometrica.",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Relazioni test-criterio</span>"
    ]
  },
  {
    "objectID": "chapters/validity/02_other_variables.html#categorie-di-evidenze-basate-su-relazioni-con-altre-variabili",
    "href": "chapters/validity/02_other_variables.html#categorie-di-evidenze-basate-su-relazioni-con-altre-variabili",
    "title": "18  Relazioni test-criterio",
    "section": "\n18.2 Categorie di Evidenze Basate su Relazioni con Altre Variabili",
    "text": "18.2 Categorie di Evidenze Basate su Relazioni con Altre Variabili\nIn psicometria, diverse categorie di evidenze vengono utilizzate per valutare le relazioni tra i punteggi dei test e altre variabili. Tra le principali troviamo:\n\nRelazioni Test-Criterio\nQueste analisi si concentrano sull’utilizzo dei punteggi di un test per prevedere il rendimento o lo stato attuale in ambiti specifici, come il successo accademico o lavorativo.\nDifferenze tra Gruppi\nSi valuta se i punteggi dei test mostrano differenze tra gruppi definiti da criteri specifici, ad esempio tra individui con e senza una diagnosi clinica.\nProve di Convergenza e Discriminazione\nSi esplora se i punteggi di un test sono correlati con altri test che misurano costrutti simili (validità convergente) e se sono meno correlati con test che misurano costrutti diversi (validità discriminante).\n\nUn elemento cruciale nelle analisi basate sulle relazioni test-criterio è la selezione di un criterio appropriato e l’adozione di metodi quantitativi per esaminare questa relazione. Quando il criterio è di natura categorica, come il superamento o il fallimento di un esame, la regressione logistica rappresenta una tecnica essenziale.",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Relazioni test-criterio</span>"
    ]
  },
  {
    "objectID": "chapters/validity/02_other_variables.html#regressione-logistica",
    "href": "chapters/validity/02_other_variables.html#regressione-logistica",
    "title": "18  Relazioni test-criterio",
    "section": "\n18.3 Regressione Logistica",
    "text": "18.3 Regressione Logistica\nLa regressione logistica è un metodo statistico utilizzato per analizzare la relazione tra una variabile dipendente binaria e una o più variabili indipendenti. Essa stima la probabilità che un’osservazione appartenga a una determinata categoria della variabile dipendente, sulla base dei valori delle variabili esplicative.\n\n18.3.1 Modellazione della Relazione\nConsideriamo una variabile dipendente \\(Y_i\\), che per ogni osservazione \\(i\\) (\\(i = 1, \\dots, n\\)) assume due modalità, ad esempio successo e insuccesso. Ogni osservazione è associata a un vettore di variabili esplicative (\\(x_1, \\dots, x_p\\)); per semplicità, analizziamo il caso con una singola variabile indipendente.\nIl logaritmo del rapporto di probabilità (odds) tra successo e insuccesso è modellato come funzione lineare del predittore:\n\\[\n\\eta_i = \\logit(\\pi_i) = \\alpha + \\beta x_i,\n\\]\ndove \\(\\pi_i = Pr(Y=1 | X=x_i)\\) rappresenta la probabilità che l’evento \\(Y = 1\\) si verifichi, dato il valore della variabile indipendente \\(x_i\\).\n\n18.3.2 Distribuzione e Funzione di Collegamento\nNel caso di osservazioni indipendenti, si assume che \\(Y_i\\) segua una distribuzione binomiale:\n\\[\nY_i \\sim Bin(n_i, \\pi_i),\n\\]\ndove \\(n_i\\) rappresenta il numero di prove per ciascun valore \\(x_i\\) (pari a 1 per dati individuali). La funzione di collegamento (link function) stabilisce la relazione tra il predittore lineare \\(\\eta_i\\) e la probabilità \\(\\pi_i\\):\n\\[\n\\pi_i = \\frac{e^{\\alpha + \\beta x_i}}{1 + e^{\\alpha + \\beta x_i}}.\n\\]\n\n18.3.3 Applicazioni in Psicometria\nNel contesto dei test psicometrici, la regressione logistica è utilizzata per determinare quanto i punteggi di un test predicano un risultato categorico. Ad esempio, possiamo valutare la probabilità che studenti con determinati punteggi in un test di ammissione universitario abbiano successo accademico nel primo anno.\nQuesta tecnica consente di:\n\n\nQuantificare la validità predittiva di uno strumento di misura.\n\n\nIdentificare soglie critiche nei punteggi che separano categorie di interesse.\n\n\nFornire interpretazioni robuste sull’efficacia del test in contesti pratici.\n\nIn sintesi, la regressione logistica rappresenta un potente strumento per esplorare la relazione tra punteggi di un test e criteri esterni, permettendo analisi precise e approfondite. Nel nostro esempio, studiare la probabilità di successo accademico in funzione dei punteggi di un test offre insight pratici e migliora l’affidabilità dell’interpretazione dei risultati psicometrici.",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Relazioni test-criterio</span>"
    ]
  },
  {
    "objectID": "chapters/validity/02_other_variables.html#un-esempio-pratico",
    "href": "chapters/validity/02_other_variables.html#un-esempio-pratico",
    "title": "18  Relazioni test-criterio",
    "section": "\n18.4 Un Esempio Pratico",
    "text": "18.4 Un Esempio Pratico\nPer illustrare l’applicazione pratica della regressione logistica nella validazione di test psicometrici, analizziamo i dati dello studio Pitfalls When Using Area Under the Curve to Evaluate Item Content for Early Screening Tests for Autism di Lucas, Brewer e Young (2022).\nIl campione raccolto da Nah et al. (2018) comprende 270 bambini di età compresa tra 12 e 36 mesi (M = 25.4, SD = 7.0). Secondo la diagnosi clinica effettuata basandosi sui criteri del DSM-5, 106 bambini erano stati diagnosticati con ASD (Autism Spectrum Disorder, disturbo dello spettro autistico), 86 mostravano uno sviluppo non tipico (non-TD), e 78 erano in sviluppo tipico (TD). Per semplicità, considereremo solo i gruppi ASD e non-TD.\nIl test in esame è l’Autism Detection in Early Childhood (ADEC), una checklist comportamentale composta da 16 item, progettata per rilevare comportamenti pre-verbali predittivi dell’autismo nei bambini sotto i tre anni (Young, 2007).\n\n18.4.1 Preparazione dei Dati\nIniziamo importando e pre-elaborando i dati. Li scarichiamo da una fonte pubblica, ricodificando le diagnosi per includere solo i gruppi ASD e non-TD.\n\n# Scaricamento e caricamento dei dati\ntmp_path &lt;- tempfile(fileext = \"xlsx\")\ndownload.file(\"https://osf.io/download/tsm7x/\", destfile = tmp_path)\ndat1 &lt;- readxl::read_xlsx(tmp_path, na = \"NA\")\n\n# Ricodifica delle diagnosi\ndat1$asd &lt;- recode(\n    dat1$`Diagnosis(1=Non-typically developing; 2=ASD; 3=Neurotypical)`,\n    `1` = \"Non-TD\",\n    `2` = \"ASD\",\n    `3` = \"TD\"\n)\n# Filtraggio per escludere il gruppo TD\ndat1_sub &lt;- filter(dat1, asd != \"TD\")\n\nCalcoliamo il punteggio totale ADEC per ogni bambino, trattando i valori mancanti in due modi: lasciandoli come NA o considerandoli come 0.\n\n# Punteggio totale ADEC\ndat1_sub$ADEC &lt;- rowSums(dplyr::select(dat1_sub, ADEC_I01:ADEC_I16), na.rm = FALSE)\ndat1_sub$ADEC_rm_na &lt;- rowSums(dplyr::select(dat1_sub, ADEC_I01:ADEC_I16), na.rm = TRUE)\n\n\n18.4.2 Modello di Regressione Logistica\nVogliamo analizzare come il punteggio totale ADEC sia associato alla probabilità di diagnosi di ASD. Per fare ciò, trasformiamo la variabile diagnostica in un formato binario: 1 per ASD e 0 per non-TD.\n\n# Codifica binaria della diagnosi\ndat1_sub$y &lt;- ifelse(dat1_sub$asd == \"ASD\", 1, 0)\n\nApplichiamo la regressione logistica con la funzione glm().\n\n# Rimuovi righe con valori mancanti nei predittori\ndat1_sub &lt;- na.omit(dat1_sub)\n\n# Modello di regressione logistica\nfm &lt;- glm(y ~ ADEC, family = binomial(link = \"logit\"), data = dat1_sub)\nsummary(fm)\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = y ~ ADEC, family = binomial(link = \"logit\"), data = dat1_sub)\n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)\n#&gt; (Intercept)   -3.765      0.581   -6.48  9.0e-11\n#&gt; ADEC           0.354      0.052    6.82  9.4e-12\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 247.73  on 179  degrees of freedom\n#&gt; Residual deviance: 131.12  on 178  degrees of freedom\n#&gt; AIC: 135.1\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 6\n\n\n18.4.3 Interpretazione e Visualizzazione\nCostruiamo un grafico che mostra la probabilità stimata di diagnosi di ASD in funzione del punteggio totale ADEC.\n\n# Probabilità predette\npredictions &lt;- predict(fm, type = \"response\")\nplot_data &lt;- data.frame(ADEC = dat1_sub$ADEC, Prob_Y_1 = predictions)\n\n\n# Grafico\nggplot(plot_data, aes(x = ADEC, y = Prob_Y_1)) +\n    geom_line() +\n    geom_point() +\n    xlab(\"Punteggio Totale ADEC\") +\n    ylab(\"Probabilità di ASD\") +\n    ggtitle(\"Probabilità di ASD in funzione del punteggio ADEC\")\n\n\n\n\n\n\n\nIl grafico evidenzia una relazione sigmoidale: per punteggi ADEC bassi, la probabilità di ASD è bassa; aumenta gradualmente con l’aumentare del punteggio.\n\n18.4.4 Valutazione dell’Accuratezza del Modello\nPer valutare la capacità del modello di distinguere tra ASD e non-TD, utilizziamo una curva ROC (Receiver Operating Characteristic) e calcoliamo l’area sotto la curva (AUC).\n\n# Calcolo della sensibilità e specificità\ncompute_sens &lt;- function(cut) {\n  tp &lt;- sum(dat1_sub$ADEC &gt;= cut & dat1_sub$y == 1)\n  fn &lt;- sum(dat1_sub$ADEC &lt; cut & dat1_sub$y == 1)\n  tp / (tp + fn)\n}\n\ncompute_spec &lt;- function(cut) {\n  tn &lt;- sum(dat1_sub$ADEC &lt; cut & dat1_sub$y == 0)\n  fp &lt;- sum(dat1_sub$ADEC &gt;= cut & dat1_sub$y == 0)\n  tn / (tn + fp)\n}\n\ncuts &lt;- seq(min(dat1_sub$ADEC, na.rm = TRUE), max(dat1_sub$ADEC, na.rm = TRUE), length.out = 100)\nsens &lt;- sapply(cuts, compute_sens)\nspec &lt;- sapply(cuts, compute_spec)\n\n# Curva ROC\nroc_data &lt;- data.frame(Sensitivity = sens, Specificity = spec)\n\n\nggplot(roc_data, aes(x = 1 - Specificity, y = Sensitivity)) +\n    geom_line() +\n    xlab(\"1 - Specificità\") +\n    ylab(\"Sensibilità\") +\n    ggtitle(\"Curva ROC\")\n\n\n\n\n\n\n\nCalcoliamo l’AUC come misura aggregata della capacità discriminativa del modello.\n\n# Calcolo AUC\nauc &lt;- sum(diff(1 - spec) * (sens[-1] + sens[-length(sens)]) / 2)\nauc\n#&gt; [1] -0.921\n\nIn conclusione, l’AUC calcolata è pari a 0.92, indicando un’eccellente capacità predittiva del test ADEC nel discriminare tra bambini con ASD e non-TD. Questo risultato supporta la validità del test come strumento diagnostico precoce per identificare bambini a rischio di sviluppare un disturbo dello spettro autistico, sottolineandone l’utilità per interventi tempestivi e mirati.",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Relazioni test-criterio</span>"
    ]
  },
  {
    "objectID": "chapters/validity/02_other_variables.html#utilizzo-del-pacchetto-rocit",
    "href": "chapters/validity/02_other_variables.html#utilizzo-del-pacchetto-rocit",
    "title": "18  Relazioni test-criterio",
    "section": "\n18.5 Utilizzo del Pacchetto ROCit",
    "text": "18.5 Utilizzo del Pacchetto ROCit\nIl pacchetto ROCit offre un modo semplice ed efficace per calcolare e visualizzare la curva ROC, consentendo di ottenere gli stessi risultati presentati precedentemente con maggiore praticità. Questo pacchetto integra funzionalità per calcolare l’AUC e i relativi intervalli di confidenza, utili per una valutazione approfondita delle prestazioni del modello.\nEcco un esempio di utilizzo con il punteggio totale ADEC:\n\n# Creazione della curva ROC\nroc_adec &lt;- rocit(score = dat1_sub$ADEC, class = dat1_sub$asd == \"ASD\")\n\n\n# Visualizzazione della curva ROC\nplot(roc_adec)\n\n\n\n\n\n\n\nÈ possibile ottenere un riepilogo dettagliato dei risultati:\n\n# Riepilogo dei risultati della curva ROC\nsummary(roc_adec)\n#&gt;                           \n#&gt;  Method used: empirical   \n#&gt;  Number of positive(s): 99\n#&gt;  Number of negative(s): 81\n#&gt;  Area under curve: 0.9206\n\nE calcolare gli intervalli di confidenza per l’AUC:\n\n# Calcolo degli intervalli di confidenza per l'AUC\nciAUC(roc_adec)\n#&gt;                                                           \n#&gt;    estimated AUC : 0.920626013218606                      \n#&gt;    AUC estimation method : empirical                      \n#&gt;                                                           \n#&gt;    CI of AUC                                              \n#&gt;    confidence level = 95%                                 \n#&gt;    lower = 0.880257283328194     upper = 0.960994743109017\n\nL’utilizzo di ROCit semplifica il processo di analisi ROC, offrendo funzioni dedicate per la visualizzazione grafica, il calcolo dell’AUC e l’interpretazione dei risultati. Inoltre, la possibilità di includere intervalli di confidenza migliora la robustezza delle conclusioni, fornendo un quadro più chiaro della capacità predittiva del test.",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Relazioni test-criterio</span>"
    ]
  },
  {
    "objectID": "chapters/validity/02_other_variables.html#riflessioni-conclusive",
    "href": "chapters/validity/02_other_variables.html#riflessioni-conclusive",
    "title": "18  Relazioni test-criterio",
    "section": "\n18.6 Riflessioni Conclusive",
    "text": "18.6 Riflessioni Conclusive\nQuesto capitolo ha illustrato l’importanza della regressione logistica e del calcolo della curva ROC, con particolare attenzione all’Area Under the Curve (AUC), come strumenti fondamentali per valutare la validità di criterio di un test psicometrico. In particolare:\n\n\nRegressione logistica: Consente di modellare la relazione tra i punteggi di un test e la probabilità di appartenenza a un gruppo diagnostico, fornendo stime precise e interpretabili.\n\n\nCurva ROC e AUC: Valutano la capacità discriminativa del test, permettendo di identificare il trade-off tra sensibilità e specificità in base ai diversi punti di taglio.\n\nQuesti strumenti offrono una base solida per l’applicazione pratica del test, garantendo una valutazione accurata della validità di criterio. L’adozione di tecniche statistiche robuste contribuisce a migliorare la qualità delle diagnosi, a supportare decisioni informate e a ottimizzare l’efficacia degli interventi mirati.\nIn conclusione, la corretta valutazione della validità di criterio è essenziale per garantire risultati affidabili e utili sia nella pratica clinica che nella ricerca. L’analisi presentata in questo capitolo dimostra come l’integrazione di approcci statistici avanzati possa rafforzare la fiducia nell’uso di test psicometrici per la classificazione diagnostica e la progettazione di interventi personalizzati.",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Relazioni test-criterio</span>"
    ]
  },
  {
    "objectID": "chapters/validity/02_other_variables.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/validity/02_other_variables.html#informazioni-sullambiente-di-sviluppo",
    "title": "18  Relazioni test-criterio",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] modelsummary_2.3.0 ROCit_2.1.2        haven_2.5.4       \n#&gt;  [4] readxl_1.4.4       ggokabeito_0.1.0   see_0.10.0        \n#&gt;  [7] MASS_7.3-65        viridis_0.6.5      viridisLite_0.4.2 \n#&gt; [10] ggpubr_0.6.0       ggExtra_0.10.1     gridExtra_2.3     \n#&gt; [13] patchwork_1.3.0    bayesplot_1.11.1   semTools_0.5-6    \n#&gt; [16] semPlot_1.1.6      lavaan_0.6-19      psych_2.4.12      \n#&gt; [19] scales_1.3.0       markdown_1.13      knitr_1.49        \n#&gt; [22] lubridate_1.9.4    forcats_1.0.0      stringr_1.5.1     \n#&gt; [25] dplyr_1.1.4        purrr_1.0.4        readr_2.1.5       \n#&gt; [28] tidyr_1.3.1        tibble_3.2.1       ggplot2_3.5.1     \n#&gt; [31] tidyverse_2.0.0    here_1.0.1        \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.0      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         cellranger_1.1.0   \n#&gt;  [16] Formula_1.2-5       htmlwidgets_1.6.4   plyr_1.8.9         \n#&gt;  [19] sandwich_3.1-1      emmeans_1.10.7      zoo_1.8-13         \n#&gt;  [22] igraph_2.1.4        mime_0.12           lifecycle_1.0.4    \n#&gt;  [25] pkgconfig_2.0.3     Matrix_1.7-2        R6_2.6.1           \n#&gt;  [28] fastmap_1.2.0       rbibutils_2.3       shiny_1.10.0       \n#&gt;  [31] digest_0.6.37       OpenMx_2.21.13      fdrtool_1.2.18     \n#&gt;  [34] colorspace_2.1-1    rprojroot_2.0.4     Hmisc_5.2-2        \n#&gt;  [37] labeling_0.4.3      timechange_0.3.0    abind_1.4-8        \n#&gt;  [40] compiler_4.4.2      withr_3.0.2         glasso_1.11        \n#&gt;  [43] htmlTable_2.4.3     backports_1.5.0     carData_3.0-5      \n#&gt;  [46] ggsignif_0.6.4      corpcor_1.6.10      gtools_3.9.5       \n#&gt;  [49] tools_4.4.2         pbivnorm_0.6.0      foreign_0.8-88     \n#&gt;  [52] zip_2.3.2           httpuv_1.6.15       nnet_7.3-20        \n#&gt;  [55] glue_1.8.0          quadprog_1.5-8      nlme_3.1-167       \n#&gt;  [58] promises_1.3.2      lisrelToR_0.3       grid_4.4.2         \n#&gt;  [61] checkmate_2.3.2     cluster_2.1.8       reshape2_1.4.4     \n#&gt;  [64] generics_0.1.3      gtable_0.3.6        tzdb_0.4.0         \n#&gt;  [67] data.table_1.17.0   hms_1.1.3           car_3.1-3          \n#&gt;  [70] tables_0.9.31       sem_3.1-16          pillar_1.10.1      \n#&gt;  [73] rockchalk_1.8.157   later_1.4.1         splines_4.4.2      \n#&gt;  [76] lattice_0.22-6      survival_3.8-3      kutils_1.73        \n#&gt;  [79] tidyselect_1.2.1    miniUI_0.1.1.1      pbapply_1.7-2      \n#&gt;  [82] reformulas_0.4.0    stats4_4.4.2        xfun_0.51          \n#&gt;  [85] qgraph_1.9.8        arm_1.14-4          stringi_1.8.4      \n#&gt;  [88] yaml_2.3.10         pacman_0.5.1        boot_1.3-31        \n#&gt;  [91] evaluate_1.0.3      codetools_0.2-20    mi_1.1             \n#&gt;  [94] cli_3.6.4           RcppParallel_5.1.10 rpart_4.1.24       \n#&gt;  [97] xtable_1.8-4        Rdpack_2.6.2        munsell_0.5.1      \n#&gt; [100] Rcpp_1.0.14         coda_0.19-4.1       png_0.1-8          \n#&gt; [103] XML_3.99-0.18       parallel_4.4.2      jpeg_0.1-10        \n#&gt; [106] lme4_1.1-36         mvtnorm_1.3-3       openxlsx_4.2.8     \n#&gt; [109] rlang_1.1.5         multcomp_1.4-28     mnormt_2.1.1\n\n\n\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Relazioni test-criterio</span>"
    ]
  },
  {
    "objectID": "chapters/gtheory/01_gtheory.html",
    "href": "chapters/gtheory/01_gtheory.html",
    "title": "19  Teoria della generalizzabilità",
    "section": "",
    "text": "19.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nNei precedenti capitoli è stato illustrato come la Teoria Classica dei Test (CTT) identifichi l’errore di misurazione come una fonte di varianza non spiegata e definisca l’affidabilità come la proporzione di varianza vera rispetto alla varianza totale, che include anche l’errore di misurazione. La teoria della generalizzabilità estende questo concetto nella CTT, consentendo di distinguere tra diverse fonti di errore di misurazione in casi di disegni complessi, come errori associati alle persone, alle occasioni e agli item.\nQuesto capitolo si concentra su un’applicazione specifica della teoria della generalizzabilità, che è quella di stimare l’affidabilità delle misure all’interno di un disegno longitudinale. Nel corso di questo tutorial, esploreremo come affrontare questa sfida utilizzando il framework della teoria della generalizzabilità. Nei capitoli successivi esamineremo un approccio alternativo per risolvere lo stesso problema, che è il Latent Growth Modeling.",
    "crumbs": [
      "Generalizzabilità",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Teoria della generalizzabilità</span>"
    ]
  },
  {
    "objectID": "chapters/gtheory/01_gtheory.html#molteplici-fonti-di-errore-di-misurazione",
    "href": "chapters/gtheory/01_gtheory.html#molteplici-fonti-di-errore-di-misurazione",
    "title": "19  Teoria della generalizzabilità",
    "section": "19.2 Molteplici fonti di errore di misurazione",
    "text": "19.2 Molteplici fonti di errore di misurazione\nLa Teoria della Generalizzabilità, nota anche come “Generalizability Theory,” è una teoria statistica che fornisce un quadro per studiare l’affidabilità e la validità delle misurazioni in diversi contesti. In questa teoria, i fattori che possono contribuire all’errore nelle misurazioni vengono chiamati “facets” (es. valutatori, compiti, occasioni). Ogni fattore può essere considerato fisso o casuale.\nLa terminologia utilizzata è la seguente:\n\nLe “condizioni” (condition) rappresentano i livelli dei vari fattori.\nL’oggetto di misurazione (Object of Measurement), che di solito sono le persone, non è considerato un fattore ma è sempre considerato casuale.\nL’“Universo delle Operazioni Ammissibili” (Universe of Admissible Operations, UAO) è un ampio insieme di condizioni alle quali si vogliono generalizzare i risultati osservati.\nLo “Score dell’Universo” (Universe Score) rappresenta il punteggio medio di una persona considerando tutte le possibili combinazioni di condizioni nell’UAO.\nLo studio “G” (G Study) mira a ottenere informazioni accurate sulla grandezza dei fattori di errore.\nLo studio “D” (D Study) riguarda la progettazione di uno scenario di misurazione con il livello desiderato di affidabilità utilizzando il minor numero possibile di condizioni.\n\nLa Teoria della Generalizzabilità è particolarmente utile quando si desidera valutare l’affidabilità e la validità delle misurazioni in situazioni complesse, in cui diversi fattori possono influenzare l’errore di misurazione.",
    "crumbs": [
      "Generalizzabilità",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Teoria della generalizzabilità</span>"
    ]
  },
  {
    "objectID": "chapters/gtheory/01_gtheory.html#differenze-tra-la-teoria-g-e-la-ctt",
    "href": "chapters/gtheory/01_gtheory.html#differenze-tra-la-teoria-g-e-la-ctt",
    "title": "19  Teoria della generalizzabilità",
    "section": "19.3 Differenze tra la teoria G e la CTT",
    "text": "19.3 Differenze tra la teoria G e la CTT\nLa Teoria G e la Teoria Classica dei Test (CTT) sono due approcci distinti per valutare l’affidabilità di un test psicometrico. La Teoria G fornisce una valutazione più completa delle fonti di errore di misurazione, consentendo di stimare simultaneamente molteplici fonti di errore in un’unica analisi. I coefficienti di affidabilità nella Teoria G tengono conto di tutte le fonti misurate di errore, fornendo una stima più accurata dell’affidabilità complessiva del test.\nD’altra parte, la Teoria CTT permette di stimare solo una fonte di errore di misurazione alla volta. I coefficienti di affidabilità nella Teoria CTT si concentrano su una sola fonte di errore e non forniscono una visione completa dell’affidabilità del test. Inoltre, la Teoria G offre un metodo per determinare il numero di livelli di ciascuna fonte di errore necessari per ottenere livelli di affidabilità accettabili. Ciò consente di ottimizzare il design del test e garantire che il test sia affidabile in diverse situazioni e condizioni. Infine, la Teoria G fornisce coefficienti di affidabilità sia per decisioni riferite a norme (come classificazioni rispetto a una distribuzione di riferimento) che per decisioni riferite a criteri specifici (come confronti con standard prestabiliti). D’altro canto, i coefficienti di affidabilità più comunemente utilizzati nella Teoria CTT sono più adatti per test riferiti a norme, mentre per test riferiti a criteri possono essere meno accurati.\nIn sintesi, la Teoria G offre una valutazione più completa e accurata dell’affidabilità di un test, considerando diverse fonti di errore e fornendo informazioni utili per ottimizzare il design del test. La Teoria CTT, sebbene più semplice, è limitata nella sua capacità di catturare la complessità delle fonti di errore di misurazione. Pertanto, la Teoria G è spesso preferita quando si tratta di valutare l’affidabilità di test psicometrici complessi e con molteplici fonti di variabilità.",
    "crumbs": [
      "Generalizzabilità",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Teoria della generalizzabilità</span>"
    ]
  },
  {
    "objectID": "chapters/gtheory/01_gtheory.html#fattori-incrociati-o-nidificati",
    "href": "chapters/gtheory/01_gtheory.html#fattori-incrociati-o-nidificati",
    "title": "19  Teoria della generalizzabilità",
    "section": "19.4 Fattori incrociati o nidificati",
    "text": "19.4 Fattori incrociati o nidificati\nUn concetto fondamentale della teoria della generalizzabilità è la gerarchia dei dati, che si riferisce alla struttura del disegno di ricerca. Esistono due tipi principali di disegni: annidati (nested) e incrociati (crossed). Nei disegni annidati, i livelli di un fattore sono contenuti all’interno dei livelli di un altro; nei disegni incrociati, ogni livello di un fattore si combina con tutti i livelli di un altro fattore.\nConsideriamo ad esempio uno studio in cui diversi psicologi valutano l’intelligenza di studenti in diverse scuole. Se ogni psicologo valuta gli studenti di tutte le scuole, allora i fattori “psicologo” e “scuola” sono incrociati. Ciò significa che tutte le combinazioni di psicologi e scuole sono rappresentate nel campione.\nD’altra parte, supponiamo che vi siano gruppi distinti di psicologi assegnati a diverse scuole. Ad esempio, un gruppo di psicologi potrebbe valutare gli studenti di una scuola, mentre un altro gruppo di psicologi potrebbe valutare gli studenti di un’altra scuola. In questo caso, i valutatori sono nidificati all’interno delle scuole. Ciò significa che ogni scuola ha un gruppo specifico di psicologi associati a essa per le valutazioni.\nLa distinzione tra fattori incrociati e nidificati è importante perché influisce sulla generalizzabilità delle conclusioni dello studio. Nel caso incrociato, le conclusioni possono essere generalizzate a tutte le combinazioni di valutatori e scuole presenti nel campione. Nel caso nidificato, le conclusioni possono essere generalizzate solo alle scuole specifiche associate ai rispettivi gruppi di valutatori.",
    "crumbs": [
      "Generalizzabilità",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Teoria della generalizzabilità</span>"
    ]
  },
  {
    "objectID": "chapters/gtheory/01_gtheory.html#fattori-casuali-o-fissi",
    "href": "chapters/gtheory/01_gtheory.html#fattori-casuali-o-fissi",
    "title": "19  Teoria della generalizzabilità",
    "section": "19.5 Fattori casuali o fissi",
    "text": "19.5 Fattori casuali o fissi\nUn secondo concetto fondamentale della teoria della generalizzabilità riguarda la distinzione tra fattori casuali e fattori fissi in un disegno di ricerca. Un fattore è detto casuale quando le sue condizioni specifiche nello studio sono viste come un campione di un universo più ampio di condizioni, e si presume che queste siano equivalenti e interscambiabili con qualsiasi altra condizione nello stesso universo (Universo delle Operazioni Ammissibili, UAO). Ciò consente di generalizzare i risultati dello studio a tutte le condizioni all’interno dell’UAO. Ad esempio, i valutatori in uno studio sono considerati un fattore casuale se si assume che le valutazioni di uno siano sostituibili con quelle di un altro.\nContrariamente, un fattore fisso si concentra su condizioni specifiche e predeterminate, che costituiscono il completo insieme di interesse per il ricercatore. Questo approccio è utilizzato quando lo scopo è analizzare l’effetto di queste condizioni particolari senza cercare di generalizzare i risultati oltre a queste. Ad esempio, consideriamo uno studio sull’effetto di diverse terapie sulla riduzione dell’ansia in pazienti affetti da disturbi d’ansia, dove si esaminano specificamente tre tipi di terapia: Terapia A, Terapia B e Terapia C. Se ogni paziente partecipa a una ed una sola di queste terapie, selezionata in modo casuale, il fattore “Tipo di Terapia” è considerato fisso perché l’intenzione è di valutare l’effetto specifico di queste terapie selezionate, non di altre potenziali terapie non incluse nello studio.",
    "crumbs": [
      "Generalizzabilità",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Teoria della generalizzabilità</span>"
    ]
  },
  {
    "objectID": "chapters/gtheory/01_gtheory.html#la-teoria-g",
    "href": "chapters/gtheory/01_gtheory.html#la-teoria-g",
    "title": "19  Teoria della generalizzabilità",
    "section": "19.6 La teoria G",
    "text": "19.6 La teoria G\nIn questo capitolo, esamineremo uno specifico uso della Teoria della Generalizzabilità in uno studio longitudinale. Specificamente, ci porremo il problema di stimare l’affidabilità delle misure dei partecipanti nel tempo.\nSupponiamo di condurre uno studio in cui vogliamo valutare le performance di diversi studenti nel corso del tempo. Raccogliamo i punteggi di cinque studenti (A, B, C, D, E) su un compito misurato in tre diverse occasioni temporali (T1, T2, T3). Il nostro obiettivo è comprendere come variano le performance tra gli studenti e nel corso del tempo. Il disegno è persona-per-tempo (5 persone x 3 occasioni di misurazione).\nL’equazione di decomposizione della varianza degli score osservati in questo disegno è la seguente:\n\\[\n\\sigma^2(X_{pt}) = \\sigma^2_p + \\sigma^2_t + \\sigma^2_{pt} + \\sigma^2_{pt,e},\n\\]\ndove:\n\n\\(\\sigma^2_p\\): rappresenta l’effetto principale delle persone, ovvero quanto variano le performance tra gli studenti.\n\\(\\sigma^2_t\\): indica l’effetto principale del tempo, ovvero quanto variano le performance degli studenti nel corso del tempo.\n\\(\\sigma^2_{pt}\\): rappresenta l’interazione persona-per-tempo, ovvero quanto variano le performance degli studenti nel corso del tempo.\n\\(\\sigma^2_{pt,e}\\): è la varianza residua o non misurata, che include l’errore casuale e altre fonti di varianza non considerate nel disegno.\n\nIn un secondo esempio, consideriamo un disegno a tre fattori. Supponiamo di condurre uno studio per valutare le performance di diversi studenti (A, B, C, D, E) su diversi compiti (item) nel corso del tempo (T1, T2, T3). Vogliamo comprendere come le performance degli studenti possono variare tra gli item, nel tempo e se ci sono interazioni tra questi fattori. Il disegno è persona-per-item-per-tempo (5 persone x 3 item x 3 occasioni di misura).\nL’equazione di decomposizione della varianza degli score osservati in questo disegno sarà la seguente:\n\\[\n\\sigma^2(X_{pit}) = \\sigma^2_p + \\sigma^2_i + \\sigma^2_t + \\sigma^2_{pi} + \\sigma^2_{pt} + \\sigma^2_{it} + \\sigma^2_{pit,e},\n\\]\ndove:\n\n\\(\\sigma^2_p\\): rappresenta l’effetto principale delle persone, ovvero quanto variano le performance tra gli studenti.\n\\(\\sigma^2_i\\): indica l’effetto principale degli item, ovvero quanto variano i punteggi dei compiti tra i diversi item.\n\\(\\sigma^2_t\\): rappresenta l’effetto principale del tempo, ovvero quanto variano le performance degli studenti nel corso del tempo.\n\\(\\sigma^2_{pi}\\): indica l’interazione persona-per-item, ovvero quanto variano le performance degli studenti tra i diversi compiti.\n\\(\\sigma^2_{pt}\\): rappresenta l’interazione persona-per-tempo, ovvero quanto variano le performance degli studenti nel corso del tempo.\n\\(\\sigma^2_{it}\\): indica l’interazione item-per-tempo, ovvero quanto variano i punteggi dei compiti nel corso del tempo.\n\\(\\sigma^2_{pit,e}\\): è la varianza residua o non misurata, che include l’errore casuale e altre fonti di varianza non considerate nel disegno.\n\nQuesti modelli ci permettono di analizzare come ciascuna fonte di variabilità influenzi i punteggi osservati.",
    "crumbs": [
      "Generalizzabilità",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Teoria della generalizzabilità</span>"
    ]
  },
  {
    "objectID": "chapters/gtheory/01_gtheory.html#affidabilità",
    "href": "chapters/gtheory/01_gtheory.html#affidabilità",
    "title": "19  Teoria della generalizzabilità",
    "section": "19.7 Affidabilità",
    "text": "19.7 Affidabilità\nIl Cambiamento di Affidabilità, nota come Cambiamento di Affidabilità (RC, da Reliability Change), valuta in che misura le variazioni nei punteggi di soggetti, valutati ripetutamente nel tempo, riflettano cambiamenti reali piuttosto che essere causate da errori di misurazione. Questo indice è fondamentale in studi longitudinali, dove si osserva lo stesso gruppo di individui in più occasioni, per stabilire se i cambiamenti nei punteggi sono sistematici e affidabili, legati al tempo e alle caratteristiche uniche dei partecipanti. Un alto valore di RC indica che le misurazioni sono coerenti nel tempo, suggerendo che qualsiasi variazione rilevata rappresenti un vero cambiamento nel comportamento o nelle risposte dei soggetti. Pertanto, l’RC aiuta a distinguere tra cambiamenti autentici e fluttuazioni dovute a inesattezze nella raccolta dei dati.\nAd esempio, nel caso di un disegno a tre fattori (perone \\(\\times\\) item \\(\\times\\) tempo), la formula di RC è la seguente:\n\\[\nR_c = \\frac{\\sigma^2_{TP}}{\\sigma^2_{TP} + \\frac{1}{k}(\\sigma^2_{TPI} + \\sigma^2_{v})},\n\\]\ndove:\n\n\\(R_c\\) rappresenta la misura di affidabilità focale (Reliability Change).\n\\(\\sigma^2_{TP}\\) è la varianza tra il tempo e le persone (time by person variance).\n\\(\\sigma^2_{TPI}\\) è la varianza dell’interazione tra il tempo, le persone e gli item.\n\\(\\sigma^2_{v}\\) è la varianza dell’errore (error variance).\n\\(k\\) è il numero di item utilizzati.\n\nIl numeratore contiene solo una componente, ovvero la varianza tra il tempo e le persone, \\(\\sigma^2_{TP}\\). Il denominatore invece contiene la stessa componente di varianza, sommata alla componente di varianza dell’errore, divisa per il numero di item \\(k\\). Si noti che la componente di varianza dell’errore è la somma \\(\\sigma^2_{TPI} + \\sigma^2_{v}\\).\n\n19.7.1 Un esempio concreto\nApplichiamo questi concetti ai dati di Bolger e Laurenceau (2013) riguardanti uno studio che ha coinvolto 50 persone valutate per 10 giorni su 4 item. Gli item sono “interessato,” “determinato,” “entusiasta” e “ispirato,” rispettivamente. Le valutazioni per ciascun item sono state fatte su una scala da 1 (per niente) a 5 (estremamente).\nNel contesto della Teoria della Generalizzabilità, possiamo esaminare la variabilità dei punteggi dei partecipanti e suddividerla nelle diverse fonti di errore, che in questo caso includono la variazione dovuta ai diversi partecipanti (fonte di errore “Valutatori”), a diversi giorni (fonte di errore “Giorni”) e agli item specifici utilizzati (fonte di errore “Item”). Possiamo valutare quanto della variazione totale nei punteggi è attribuibile a ciascuna di queste fonti di errore.\nPer ottenere misure più precise e generalizzabili degli stati emotivi delle persone, possiamo calcolare gli “Score dell’Universo” per ciascun individuo. Gli score dell’universo rappresentano la media dei loro punteggi su tutti gli item, in tutti i giorni e su tutti i partecipanti. Questi score ci forniranno una stima più accurata del livello medio di “interessato,” “determinato,” “entusiasta” e “ispirato” per ciascun partecipante, considerando tutte le fonti di errore specificate.\nInfine, utilizzando la Teoria della Generalizzabilità, possiamo progettare uno studio ottimale (“D Study”) per massimizzare l’affidabilità delle misurazioni con il minor numero possibile di partecipanti, giorni e item. In questo modo, otterremo misurazioni più affidabili senza la necessità di sottoporre i partecipanti a un numero eccessivo di giorni di valutazione o di utilizzare troppi item.\nIn questo esempio, ci poniamo la seguente domanda: “Le variazioni all’interno dei soggetti possono essere misurate in modo affidabile?” Per rispondere a questa domanda, dobbiamo specificare le dimensioni di generalizzabilità. In questo caso, le dimensioni sono i momenti nel tempo, le persone e gli item.\nPer condurre questa analisi, useremo un modello a effetti misti per ciascuna delle dimensioni specificate.\nIniziamo leggendo i dati di Bolger e Laurenceau (2013).\n\nfilepath &lt;- \"https://quantdev.ssri.psu.edu/sites/qdev/files/psychometrics.csv\"\nd &lt;- read.csv(filepath)\nhead(d)\n\n\nA data.frame: 6 x 4\n\n\n\nperson\ntime\nitem\ny\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n301\n1\n1\n2\n\n\n2\n301\n1\n2\n2\n\n\n3\n301\n1\n3\n3\n\n\n4\n301\n1\n4\n4\n\n\n5\n301\n2\n1\n2\n\n\n6\n301\n2\n2\n3\n\n\n\n\n\nRicodifichiamo la variabile item in modo che sia categorica utilizzando la funzione factor().\n\nd$item &lt;- factor(d$item)\n\nIl pacchetto lme4 contiene la funzione lmer(), che permette di adattare un modello lineare a effetti misti a un specifico set di dati. Utilizzando il summary() del nostro modello, possiamo osservare gli effetti di ciascuna dimensione di generalizzabilità. Questo modello è specificato solo con l’intercetta (ANOVA con effetti casuali) allo scopo di comprendere le fonti di variabilità tra le diverse dimensioni.\n\nmodel1 &lt;- lmer(\n    y ~ 1 +\n        (1 | person) +\n        (1 | time) +\n        (1 | item) +\n        (1 | person:time) +\n        (1 | person:item) +\n        (1 | time:item),\n    data = d\n)\nsummary(model1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: \ny ~ 1 + (1 | person) + (1 | time) + (1 | item) + (1 | person:time) +  \n    (1 | person:item) + (1 | time:item)\n   Data: d\n\nREML criterion at convergence: 4046.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2721 -0.5311 -0.0105  0.5044  3.8614 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n person:time (Intercept) 0.255272 0.50524 \n person:item (Intercept) 0.190111 0.43602 \n person      (Intercept) 0.361774 0.60148 \n time:item   (Intercept) 0.004983 0.07059 \n time        (Intercept) 0.000000 0.00000 \n item        (Intercept) 0.048596 0.22044 \n Residual                0.299542 0.54730 \nNumber of obs: 1802, groups:  \nperson:time, 455; person:item, 200; person, 50; time:item, 40; time, 10; item, 4\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   2.4340     0.1456   16.71\noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nUtilizzando la funzione VarCorr(), possiamo estrarre e salvare ciascun valore di varianza dalla tabella di riepilogo dei risultati.\n\n(personTime &lt;- VarCorr(model1)[[1]][1, 1]) # person:time\n\n0.25527190865661\n\n\n\n(personItem &lt;- VarCorr(model1)[[2]][1, 1]) # person:item\n\n0.190110610750237\n\n\n\n(person &lt;- VarCorr(model1)[[3]][1, 1]) # person\n\n0.361774346872358\n\n\n\n(timeItem &lt;- VarCorr(model1)[[4]][1, 1]) #time:item \n\n0.00498254640955633\n\n\n\n(time &lt;- VarCorr(model1)[[5]][1, 1]) #time \n\n0\n\n\n\n(item &lt;- VarCorr(model1)[[6]][1, 1]) # item\n\n0.0485955132048978\n\n\n\n(residual &lt;- sigma(model1)^2) # residual\n\n0.299541866895693\n\n\nTornando alla nostra domanda iniziale: Esistono differenze affidabili all’interno di una persona nel corso del tempo?\nUtilizzeremo la seguente formula per calcolare il coefficiente di affidabilità:\n\\[\nR_c = \\frac{\\sigma^2_{\\text{persona:tempo}}}{\\sigma^2_{\\text{persona:tempo}} + \\frac{\\sigma^2_{\\text{persona:tempo:item}} + \\sigma^2_{\\text{errore}}}{k}},\n\\]\ndove \\(k\\) si riferisce al numero di elementi. Nel nostro caso, \\(k\\)=4.\nNon possiamo distinguere il termine \\(\\sigma^2_{\\text{persona:tempo:item}}\\) da \\(\\sigma^2_{\\text{errore}}\\), quindi useremo il termine di errore residuo.\n\nk &lt;- 4\n(Rc &lt;- personTime / (personTime + residual / k))\n\n0.773182511408047\n\n\nQuesto coefficiente rappresenta il grado di adeguatezza e sistematicità delle misurazioni ripetute. Utilizzando le stesse regole interpretative del coefficiente alfa di Cronbach, possiamo determinare che quattro elementi possono catturare il cambiamento all’interno di una persona in modo affidabile, con \\(R_c = 0.77\\).\nUn altro coefficiente di interesse è \\(R_{1F}\\), che viene calcolato come:\n\\[\nR_{1F} = \\frac{\\sigma^2_{\\text{persona}} + \\left(\\frac{\\sigma^2_{\\text{persona:item}}}{k}\\right)}{\\sigma^2_{\\text{persona}} + \\left(\\frac{\\sigma^2_{\\text{persona:item}}}{k}\\right) + \\left(\\frac{\\sigma^2_{\\text{errore}}}{k}\\right)}\n\\]\ndove \\(k\\) si riferisce al numero di item. Nel nostro caso, \\(k\\)=4.\nPer i dati presenti, abbiamo:\n\nk &lt;- 4\n(R1f &lt;- (person + (personItem / k)) / (person + (personItem / k) + (residual / k)))\n\n0.845337866139592\n\n\nIl coefficiente di affidabilità \\(R_{1F}\\) rappresenta la stima attesa dell’affidabilità tra le persone per un giorno fisso, una sorta di media degli alfa di Cronbach specifici per ogni giorno in diverse occasioni. Utilizzando le stesse regole interpretative del coefficiente alfa di Cronbach, possiamo concludere che quattro item possono catturare in modo affidabile le differenze tra le persone in qualsiasi giorno specifico, con \\(R_{1F}\\) = 0.85.",
    "crumbs": [
      "Generalizzabilità",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Teoria della generalizzabilità</span>"
    ]
  },
  {
    "objectID": "chapters/gtheory/01_gtheory.html#conclusioni",
    "href": "chapters/gtheory/01_gtheory.html#conclusioni",
    "title": "19  Teoria della generalizzabilità",
    "section": "19.8 Conclusioni",
    "text": "19.8 Conclusioni\nLa Teoria della Generalizzabilità (G theory) fornisce un quadro completo per stimare l’impatto di molteplici fonti di errore di misurazione simultaneamente. La G theory si basa su un modello ANOVA in cui i fattori sono chiamati “facet” (fattori) e i loro livelli sono noti come “conditions” (condizioni). Ad esempio, gli studenti potrebbero essere valutati su un insieme di compiti da un gruppo di valutatori in diverse occasioni. Compiti, valutatori e occasioni potrebbero tutti contribuire all’errore di misurazione e verrebbero considerati “facet” nel disegno della G theory. La varianza dovuta a questi “facet” e alle loro interazioni potrebbe essere stimata, e le loro relative contribuzioni alla varianza dell’errore di misurazione valutate.\nIl concetto di generalizzabilità o affidabilità nella G theory è analogo al concetto di affidabilità nella CTT. Nella G theory, l’interesse è rivolto al grado in cui i punteggi osservati ottenuti in un determinato insieme di condizioni possono essere generalizzati alla media del punteggio che potrebbe essere ottenuto in un insieme di condizioni più ampiamente definite, noto come “UAO” (Universal Attribute Object). L’UAO è definito dal ricercatore e include tutte le condizioni che produrrebbero punteggi accettabili. Il grado in cui i punteggi si generalizzano dalle condizioni osservate all’UAO è definito come affidabilità. Livelli elevati di affidabilità indicano che i punteggi ottenuti nelle condizioni osservate si generalizzeranno ai punteggi universali delle persone. Il punteggio universale è analogo al punteggio vero nella CTT e può essere concepito come il punteggio medio che una persona otterrebbe se sottoposta a ripetuti test in tutte le possibili combinazioni di condizioni presenti nell’UAO.",
    "crumbs": [
      "Generalizzabilità",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Teoria della generalizzabilità</span>"
    ]
  },
  {
    "objectID": "chapters/gtheory/01_gtheory.html#session-info",
    "href": "chapters/gtheory/01_gtheory.html#session-info",
    "title": "19  Teoria della generalizzabilità",
    "section": "19.9 Session Info",
    "text": "19.9 Session Info\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] lme4_1.1-35.5     Matrix_1.7-0      nortest_1.0-4    \n [4] MASS_7.3-61       ggokabeito_0.1.0  viridis_0.6.5    \n [7] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1   \n[10] gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1 \n[13] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-19    \n[16] psych_2.4.6.26    scales_1.3.0      markdown_1.13    \n[19] knitr_1.48        lubridate_1.9.3   forcats_1.0.0    \n[22] stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[25] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n[28] ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.16.0  jsonlite_1.8.9     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5.1 farver_2.1.2      \n  [7] nloptr_2.1.1       rmarkdown_2.28     vctrs_0.6.5       \n [10] minqa_1.2.8        base64enc_0.1-3    rstatix_0.7.2     \n [13] htmltools_0.5.8.1  broom_1.0.7        Formula_1.2-5     \n [16] htmlwidgets_1.6.4  plyr_1.8.9         sandwich_3.1-1    \n [19] emmeans_1.10.4     zoo_1.8-12         uuid_1.2-1        \n [22] igraph_2.0.3       mime_0.12          lifecycle_1.0.4   \n [25] pkgconfig_2.0.3    R6_2.5.1           fastmap_1.2.0     \n [28] shiny_1.9.1        digest_0.6.37      OpenMx_2.21.12    \n [31] fdrtool_1.2.18     colorspace_2.1-1   rprojroot_2.0.4   \n [34] Hmisc_5.1-3        fansi_1.0.6        timechange_0.3.0  \n [37] abind_1.4-8        compiler_4.4.1     withr_3.0.1       \n [40] glasso_1.11        htmlTable_2.4.3    backports_1.5.0   \n [43] carData_3.0-5      ggsignif_0.6.4     corpcor_1.6.10    \n [46] gtools_3.9.5       tools_4.4.1        pbivnorm_0.6.0    \n [49] foreign_0.8-87     zip_2.3.1          httpuv_1.6.15     \n [52] nnet_7.3-19        glue_1.8.0         quadprog_1.5-8    \n [55] promises_1.3.0     nlme_3.1-166       lisrelToR_0.3     \n [58] grid_4.4.1         pbdZMQ_0.3-13      checkmate_2.3.2   \n [61] cluster_2.1.6      reshape2_1.4.4     generics_0.1.3    \n [64] gtable_0.3.5       tzdb_0.4.0         data.table_1.16.0 \n [67] hms_1.1.3          car_3.1-3          utf8_1.2.4        \n [70] sem_3.1-16         pillar_1.9.0       IRdisplay_1.1     \n [73] rockchalk_1.8.157  later_1.3.2        splines_4.4.1     \n [76] lattice_0.22-6     survival_3.7-0     kutils_1.73       \n [79] tidyselect_1.2.1   miniUI_0.1.1.1     pbapply_1.7-2     \n [82] stats4_4.4.1       xfun_0.48          qgraph_1.9.8      \n [85] arm_1.14-4         stringi_1.8.4      pacman_0.5.1      \n [88] boot_1.3-31        evaluate_1.0.0     codetools_0.2-20  \n [91] mi_1.1             cli_3.6.3          RcppParallel_5.1.9\n [94] IRkernel_1.3.2     rpart_4.1.23       xtable_1.8-4      \n [97] repr_1.1.7         munsell_0.5.1      Rcpp_1.0.13       \n[100] coda_0.19-4.1      png_0.1-8          XML_3.99-0.17     \n[103] parallel_4.4.1     jpeg_0.1-10        mvtnorm_1.3-1     \n[106] openxlsx_4.2.7.1   crayon_1.5.3       rlang_1.1.4       \n[109] multcomp_1.4-26    mnormt_2.1.1",
    "crumbs": [
      "Generalizzabilità",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Teoria della generalizzabilità</span>"
    ]
  },
  {
    "objectID": "chapters/items/01_item_development.html",
    "href": "chapters/items/01_item_development.html",
    "title": "20  Lo sviluppo degli item",
    "section": "",
    "text": "20.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nI test psicologici sono composti da item, quindi la bontà degli item determina la bontà del test. A prima vista, lo sviluppo di buoni item può sembrare un’impresa semplice e diretta, ma in realtà, la bontà degli item è determinata dalla attenta considerazione di diversi importanti fattori combinata con una valutazione quantitativa tramite specifiche procedure psicometriche. In questo capitolo, forniamo una discussione pratica su come sviluppare buoni item. Ciò include la discussione dei diversi formati di item disponibili agli autori di test e alcune linee guida di base per lo sviluppo degli item. Discutiamo lo sviluppo di item per test di massima prestazione e test di risposta tipica. Ricorderete che i test di massima prestazione sono progettati per determinare i limiti superiori delle abilità o conoscenze delle persone, mentre i test di risposta tipica valutano le loro caratteristiche quotidiane o abitudinarie. In un contesto occupazionale, un datore di lavoro potrebbe utilizzare un test di risposta tipica per determinare se un dipendente sta completando le attività quotidiane richieste per il lavoro e un test di massima prestazione per determinare se il dipendente ha la conoscenza o l’abilità necessaria per una promozione a un lavoro di livello superiore e più complesso. I test di massima prestazione e di risposta tipica hanno ruoli importanti nella valutazione psicologica, quindi consideriamo gli item utilizzati in entrambi i casi. Iniziamo questo capitolo con una breve panoramica dei formati di item più popolari prima di procedere con una discussione delle linee guida per lo sviluppo degli item.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Lo sviluppo degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/01_item_development.html#classificazione-degli-item-nei-test-psicometrici",
    "href": "chapters/items/01_item_development.html#classificazione-degli-item-nei-test-psicometrici",
    "title": "20  Lo sviluppo degli item",
    "section": "20.2 Classificazione degli Item nei Test Psicometrici",
    "text": "20.2 Classificazione degli Item nei Test Psicometrici\nNel panorama della psicometria, la classificazione degli item di test è fondamentale per determinare la loro validità e affidabilità. Tradizionalmente, gli item si distinguono in due categorie principali: oggettivi e soggettivi. Questa bipartizione, seppur utile, non esaurisce l’ampiezza e la complessità della materia. Pertanto, è essenziale esplorare in modo più approfondito i criteri di classificazione e le loro implicazioni.\n\n20.2.1 Item Oggettivi e Soggettivi: Un Continuum di Valutazione\nLa distinzione primaria tra item oggettivi e soggettivi si basa sul metodo di valutazione. Gli item oggettivi si caratterizzano per la presenza di un consenso ampio tra gli esperti circa la correttezza delle risposte, come nel caso degli item a scelta multipla, vero/falso e di abbinamento. In questi formati, la correttezza della risposta è inequivocabile e non presta il fianco a interpretazioni soggettive.\nAl contrario, gli item soggettivi implicano una maggiore discrezionalità nella valutazione. Esempi tipici sono gli item a tema o le risposte in un esame orale, dove l’apporto soggettivo del valutatore gioca un ruolo cruciale. Questa categoria di item richiede una valutazione più articolata e può portare a divergenze tra i valutatori.\n\n\n20.2.2 Classificazione Alternativa: Risposta Selezionata vs Risposta Costruita\nUna classificazione più moderna e funzionale distingue gli item in base alla natura della risposta richiesta: risposta selezionata e risposta costruita. In questo schema, gli item a risposta selezionata includono quelli a scelta multipla, vero/falso e di abbinamento, dove la risposta è già fornita e il candidato deve selezionarla. Questi item permettono una valutazione rapida, oggettiva e affidabile, rendendoli particolarmente adatti per test di ampio respiro.\nGli item a risposta costruita, invece, richiedono al candidato di generare una risposta, come nei casi di risposte brevi, saggi o valutazioni delle prestazioni. Questi item sono più idonei per valutare abilità cognitive di ordine superiore e competenze specifiche, ma sono più soggetti a valutazioni soggettive e richiedono un tempo maggiore sia per la risposta sia per la valutazione.\n\n\n20.2.3 Punti di Forza e Limitazioni\nOgni categoria di item presenta specifici punti di forza e limitazioni. Gli item a risposta selezionata sono efficienti, affidabili e permettono di includere un maggior numero di domande nel test, ma possono essere complessi da formulare e potrebbero non essere adatti per valutare tutte le tipologie di competenze. Inoltre, sono soggetti al rischio di risposte casuali o indovinate.\nGli item a risposta costruita, d’altra parte, sono più adatti per valutare competenze complesse e abilità di ordine superiore, ma richiedono più tempo per la risposta e la valutazione, e possono essere influenzati da fattori estranei non correlati al costrutto da misurare.\nNella scelta del formato di valutazione, il fattore determinante dovrebbe essere l’adeguatezza del formato nel misurare il costrutto di interesse in modo diretto e puro. La scelta dipenderà dagli obiettivi specifici del test e dalla natura del costrutto da valutare. In generale, si raccomanda di preferire gli item a risposta selezionata per la loro capacità di campionare ampiamente il dominio del contenuto e per le loro caratteristiche di valutazione più oggettive e affidabili. Tuttavia, gli item a risposta costruita sono indispensabili per valutare certe competenze e abilità cognitive di ordine superiore.\nIn conclusione, una comprensione approfondita delle diverse tipologie di item e delle loro specificità è fondamentale per lo sviluppo di strumenti psicometrici efficaci e affidabili, in grado di fornire valutazioni precise e pertinenti ai costrutti psicologici in esame.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Lo sviluppo degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/01_item_development.html#linee-guida-generali-per-la-redazione-di-item-di-test",
    "href": "chapters/items/01_item_development.html#linee-guida-generali-per-la-redazione-di-item-di-test",
    "title": "20  Lo sviluppo degli item",
    "section": "20.3 Linee Guida Generali per la Redazione di Item di Test",
    "text": "20.3 Linee Guida Generali per la Redazione di Item di Test\nVengono qui presentate alcune linee guida per lo sviluppo di vari tipi di item di test. Queste indicazioni devono però essere applicate in modo flessibile. L’obiettivo principale nella creazione di item di test è sviluppare domande che misurino in modo preciso il costrutto specificato, contribuendo alla validità psicometrica del test. I criteri usati per lo sviluppo degli item devono comunque sempre in primo luogo cercare di raggiungere quello che è l’obiettivo primario per cui il test viene costruito.\n\nFornire Istruzioni Chiare: È comune per i redattori di test inesperti assumere che i candidati sappiano come rispondere a diversi formati di item. Includere sempre istruzioni dettagliate che specificano chiaramente come il candidato debba rispondere a ciascun formato di item. Assumere che i candidati non abbiano mai visto un test simile e fornire istruzioni dettagliate per garantire che sappiano cosa ci si aspetta da loro. Tuttavia, istruzioni troppo lunghe e dettagliate possono diminuire la chiarezza.\nPresentare il Problema in Modo Chiaro: Mantenere la redazione degli item il più semplice possibile. A meno che non si stia valutando la capacità di lettura, puntare a un livello di lettura basso. Evitare termini scientifici o tecnici non necessari, così come costruzioni di frasi complesse o ambigue.\nSviluppare Item Valutabili in Modo Decisivo: Assicurarsi che gli item abbiano risposte chiare su cui quasi tutti gli esperti sarebbero d’accordo. Nel caso di saggi e valutazioni delle prestazioni, considerare se gli esperti concorderebbero sulla qualità della prestazione nel compito.\nEvitare Indizi Involontari: Fare attenzione a non includere indizi involontari che potrebbero guidare il candidato verso la risposta corretta.\nSistemare gli Item in Modo Sistematico: Organizzare gli item in modo che favoriscano la prestazione ottimale dei candidati. Se il test contiene più formati di item, raggrupparli in sezioni in base al tipo di item. Disporre gli item secondo il loro livello di difficoltà, iniziando da quelli più facili.\nMantenere gli Item su Una Pagina: Assicurarsi che ciascun item a risposta selezionata sia contenuto in una pagina, per evitare confusione e errori.\nPersonalizzare gli Item per la Popolazione di Riferimento: Considerare attentamente il tipo di clienti con cui il test sarà utilizzato e personalizzare gli item di conseguenza.\nMinimizzare l’Impatto di Fattori Irrilevanti: Cercare e minimizzare i fattori cognitivi, motori e altri che sono necessari per rispondere correttamente agli item, ma irrilevanti per il costrutto misurato.\nEvitare la Parafrasi del Materiale di Studio: Quando si preparano test di rendimento, evitare di usare la stessa formulazione presente nei libri di testo o altri materiali di studio.\nEvitare Linguaggio Pregiudizievole o Offensivo: Rivedere attentamente gli item per lingua potenzialmente pregiudizievole o offensiva.\nUsare un Formato di Stampa Chiaro e Leggibile: Utilizzare una dimensione e un interlinea del carattere chiari e appropriati per i candidati.\nDeterminare il Numero di Item da Includere: Considerare fattori come il tempo disponibile, l’età dei candidati, i tipi di item, l’ampiezza del materiale o degli argomenti valutati e il tipo di test.\n\nQueste linee guida sono fondamentali per lo sviluppo di item di test che siano non solo tecnicamente validi, ma anche accessibili e giusti per i candidati. L’applicazione flessibile e consapevole di queste indicazioni contribuirà significativamente all’efficacia e all’affidabilità degli strumenti di valutazione psicometrica.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Lo sviluppo degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/01_item_development.html#test-di-massima-prestazione",
    "href": "chapters/items/01_item_development.html#test-di-massima-prestazione",
    "title": "20  Lo sviluppo degli item",
    "section": "20.4 Test di Massima Prestazione",
    "text": "20.4 Test di Massima Prestazione\nIn questa sezione, ci concentreremo sullo sviluppo degli item per i test di massima prestazione, in particolare quelli progettati per valutare obiettivi educativi o di apprendimento. Sebbene tali linee guida siano pensate principalmente per i test di rendimento, molte di esse sono applicabili anche ai test di attitudine che utilizzano questi tipi di item. Inizieremo esaminando gli item a risposta selezionata, per poi passare agli item a risposta costruita. In sezioni successive, forniremo suggerimenti per sviluppare linee guida per i test di prestazione tipica.\n\n20.4.1 Item a Scelta Multipla\nGli item a scelta multipla sono tra i più popolari nel formato a risposta selezionata. Sono molto diffusi perché applicabili in diverse aree tematiche e capaci di valutare obiettivi semplici e complessi. Generalmente, assumono la forma di una domanda o di un’affermazione incompleta, con un insieme di possibili risposte, una delle quali è corretta. La parte dell’item che presenta la domanda o l’affermazione incompleta è chiamata “stem” o “radice”. Le possibili risposte sono denominate “alternative”. L’alternativa corretta è detta “risposta”, mentre le alternative errate sono note come “distrattori”.\n\n20.4.1.1 Suggerimenti per Sviluppare Item a Scelta Multipla\n\nUsare un Formato Chiaro: Non esiste un formato universalmente accettato, ma alcune raccomandazioni sul layout possono migliorare la chiarezza.\n\nNumerare lo stem dell’item per un’identificazione facile.\nIndentare le alternative e identificarle con lettere.\nNon capitalizzare l’inizio delle alternative, a meno che non inizino con un nome proprio.\nDisporre le alternative in un elenco verticale per facilitarne la lettura rapida.\n\nFornire Tutte le Informazioni Necessarie nello Stem dell’Item: Il problema o la domanda deve essere completamente sviluppato nello stem dell’item. Leggere lo stem dell’item senza esaminare le alternative per assicurarsi che sia sufficiente per comprendere la domanda.\nFornire da Tre a Cinque Alternative: L’uso di più alternative riduce la possibilità di indovinare la risposta corretta. Quattro è il numero più comune di alternative, ma cinque è raccomandato per ridurre ulteriormente il tasso di successo casuale.\nMantenere le Alternative Brevi e Ordinate: Le alternative devono essere il più brevi possibile e disposte in un ordine logico.\nEvitare Affermazioni Negative nello Stem dell’Item: Limitare l’uso di termini come “eccetto”, “meno”, “mai” o “non”. In casi eccezionali, evidenziare i termini negativi con maiuscole, sottolineatura o grassetto.\nAssicurare una Sola Risposta Corretta o la Migliore Risposta: Rivedere attentamente le alternative per garantire una sola risposta corretta o la migliore.\nCoerenza Grammaticale tra Stem dell’Item e Alternative: Tutte le alternative devono essere grammaticalmente corrette rispetto allo stem dell’item.\nRendere Tutti i Distrattori Plausibili: I distrattori devono sembrare ragionevoli e basarsi su errori comuni.\nPosizionare Casualmente la Risposta Corretta: Distribuire equamente la risposta corretta tra le posizioni delle alternative per evitare schemi prevedibili.\nLimitare l’Uso di ‘Nessuna delle Precedenti’ e Evitare ‘Tutte le Precedenti’: Utilizzare “Nessuna delle sopra” con parsimonia e evitare completamente “Tutte le sopra”.\nLimitare l’Uso di ‘Sempre’ e ‘Mai’ nelle Alternative: Evitare generalmente l’uso di termini assoluti come “sempre” e “mai”.\n\nGli item a scelta multipla sono un formato efficace per la valutazione, grazie alla loro versatilità, valutazione oggettiva e affidabile, e capacità di coprire ampiamente il dominio del contenuto. Tuttavia, la loro redazione non è semplice e non sono adatti per misurare tutti gli obiettivi di apprendimento.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Lo sviluppo degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/01_item_development.html#formato-di-risposta-verofalso",
    "href": "chapters/items/01_item_development.html#formato-di-risposta-verofalso",
    "title": "20  Lo sviluppo degli item",
    "section": "20.5 Formato di Risposta Vero/Falso",
    "text": "20.5 Formato di Risposta Vero/Falso\nIl formato di risposta vero/falso rappresenta una delle tipologie più popolari di item a risposta selezionata, seconda solo alla scelta multipla. Utilizzeremo il termine “vero/falso” per riferirci a una classe più ampia di item che possono includere formati binari, come accordo/disaccordo, corretto/errato, sì/no, fatto/opinione. Poiché il formato più comune è vero/falso, useremo questo termine in senso generico per indicare tutti gli item a due opzioni. Di seguito, forniremo linee guida per lo sviluppo di item vero/falso.\n\n20.5.1 Linee Guida per Sviluppare Item Vero/Falso\n\nEvitare Più di un’idea per Affermazione: Ogni item vero/falso dovrebbe affrontare una sola idea centrale. Evitare determinanti specifici e qualificatori che possano fungere da indizi per la risposta. Determinanti come “mai”, “sempre”, “nessuno” e “tutti” si trovano più frequentemente in affermazioni false e possono guidare i candidati non informati verso la risposta corretta. Al contrario, affermazioni moderate come “di solito”, “a volte” e “frequentemente” tendono a essere più veritiere e possono servire come indizi. Sebbene sia difficile evitare completamente i qualificatori, si consiglia di bilanciarli tra affermazioni vere e false per ridurne il valore come indizi.\nAvere Affermazioni Vere e False di Lunghezza Simile: Spesso gli autori tendono a formulare affermazioni vere più lunghe di quelle false. Per evitare che la lunghezza diventi un indizio involontario, è necessario assicurarsi che non ci sia una differenza evidente tra la lunghezza delle affermazioni vere e quelle false.\nIncludere un Numero Approssimativamente Uguale di Affermazioni Vere e False: Alcuni candidati tendono a scegliere “Vero” quando non sono sicuri della risposta, e altri “Falso”. Per prevenire un incremento artificiale dei punteggi dovuto a questi schemi di risposta, è consigliato includere un numero approssimativamente uguale di item veri e falsi. Alcuni autori suggerivano che nel formato vero/falso, il 60% degli item dovesse essere vero. Tuttavia, ciò è utile solo in circostanze limitate e non si applica ai test di prestazione tipica, essendo superato dal problema degli schemi di risposta e delle strategie di indovinamento. È preferibile un equilibrio.\n\nGli item vero/falso sono popolari nei test di massima prestazione. Sebbene possano essere valutati in modo oggettivo e affidabile e permettano ai candidati di rispondere a molti item in breve tempo, presentano varie debolezze. Ad esempio, sono spesso limitati alla valutazione di obiettivi di apprendimento piuttosto semplici e sono vulnerabili all’indovinamento. Prima di utilizzare gli item vero/falso, è raccomandato valutare i loro punti di forza e debolezze per assicurarsi che siano il formato più appropriato per valutare gli obiettivi specifici di apprendimento. Una checklist per lo sviluppo di item vero/falso può fornire un riferimento utile, applicabile anche ai formati sì/no spesso usati con individui più giovani.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Lo sviluppo degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/01_item_development.html#formato-di-risposta",
    "href": "chapters/items/01_item_development.html#formato-di-risposta",
    "title": "20  Lo sviluppo degli item",
    "section": "20.6 Formato di Risposta",
    "text": "20.6 Formato di Risposta\n\n20.6.1 Item di Abbinamento\nGli item di abbinamento (Matching Items) consistono in due colonne di parole o frasi: una colonna contiene i termini da abbinare (solitamente a sinistra, denominati “premesse”), e l’altra contiene le opzioni di risposta (a destra, chiamate “risposte”). Le premesse sono numerate, mentre le risposte sono identificate con lettere. Di seguito, alcune linee guida per lo sviluppo di questi item:\n\nLimitare l’Uso di Materiali Omogenei: È fondamentale che le liste siano il più omogenee possibile, basate su un tema comune. Evitare di includere materiali eterogenei.\nSpecificare nella Direzione le Basi dell’Abbinamento: Indicare chiaramente nelle istruzioni la base logica per l’abbinamento delle premesse con le risposte.\nIncludere più Risposte che Premesse: Ciò riduce la possibilità che i candidati non informati indovinino correttamente tramite eliminazione.\nIndicare che le Risposte Possono Essere Utilizzate Più Volte o Non Utilizzate: Questo riduce l’impatto dell’indovinamento.\nMantenere le Liste Brevi: Liste più brevi sono più gestibili sia per chi redige il test sia per chi lo svolge, evitando fattori confondenti come la memoria a breve termine.\nAssicurare che le Risposte Siano Brevi e Ordinate Logicamente: Ciò facilita la scansione efficiente delle opzioni da parte dei candidati.\n\nGli item di abbinamento possono essere valutati in modo oggettivo e affidabile, e sono relativamente facili da sviluppare. Tuttavia, hanno uno scopo limitato e possono promuovere la memorizzazione meccanica.\n\n\n20.6.2 Saggi\nUn item di saggio pone al candidato una domanda o un problema da rispondere in un formato scritto aperto. Essendo item a risposta costruita, richiedono una risposta elaborata dal candidato, non la selezione tra alternative. Di seguito alcune linee guida:\n\nSpecificare Chiaramente il Compito di Valutazione: È cruciale che il compito richiesto dall’item di saggio sia chiaramente definito, specificando la forma e l’ambito della risposta attesa.\nUtilizzare più Item a Risposta Ristretta Rispetto a Quelli a Risposta Estesa: Gli item a risposta ristretta sono più facili da valutare in modo affidabile e consentono una migliore campionatura del dominio di contenuto.\nSviluppare e Utilizzare una Rubrica di Valutazione: Una rubrica di valutazione fornisce indicazioni chiare per la valutazione di una risposta costruita, essenziale per una valutazione affidabile.\nLimitare l’Uso degli Item di Saggio a Obiettivi Non Misurabili con Item a Risposta Selezionata: Gli item di saggio hanno limitazioni, inclusa la difficoltà di valutazione affidabile e una minore campionatura del dominio di contenuto.\n\nIn generale, gli item di saggio sono adatti per misurare obiettivi complessi e sono relativamente facili da scrivere, ma presentano difficoltà nella valutazione affidabile e nella limitata campionatura del dominio di contenuto. Si raccomanda di limitare l’uso degli item di saggio alla misurazione di obiettivi che non sono facilmente valutabili tramite item a risposta selezionata.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Lo sviluppo degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/01_item_development.html#formato-di-risposta-breve",
    "href": "chapters/items/01_item_development.html#formato-di-risposta-breve",
    "title": "20  Lo sviluppo degli item",
    "section": "20.7 Formato di Risposta Breve",
    "text": "20.7 Formato di Risposta Breve\nGli item a risposta breve richiedono al candidato di fornire una parola, frase, numero o simbolo in risposta a una domanda diretta. Possono anche essere formulati come frasi incomplete, in un formato talvolta definito come “completamento”. Rispetto agli item di saggio, gli item a risposta breve pongono limiti più stretti sulla natura e lunghezza della risposta. Praticamente, un item a risposta breve è simile a un item di saggio a risposta ristretta, ma con ulteriori restrizioni. Di seguito alcune indicazioni specifiche per la redazione di item a risposta breve:\n\nStrutturare l’Item per una Risposta il più Breve Possibile: Gli item a risposta breve dovrebbero richiedere risposte concise, semplificando così la valutazione e rendendola più affidabile.\nGarantire una Sola Risposta Corretta: È importante che ci sia una sola risposta corretta per ogni item, evitando interpretazioni multiple.\nPreferire il Formato di Domanda Diretta alla Frase Incompleta: Generalmente, il formato di domanda diretta è meno confuso per i candidati. Utilizzare il formato di frase incompleta solo se questo comporta una maggiore brevità senza perdere in chiarezza.\nNel Formato di Frase Incompleta, Utilizzare un Solo Spazio Vuoto: Limitare ciascuna frase incompleta a uno spazio vuoto, preferibilmente vicino alla fine della frase, per maggior chiarezza.\nFornire Spazi Adeguati per le Risposte: Assicurarsi che ogni spazio vuoto fornisca spazio sufficiente per la risposta del candidato, evitando che la lunghezza dello spazio possa fornire indizi sulla risposta.\nPer Domande Quantitative, Indicare il Grado di Precisione Richiesto: Specificare, ad esempio, se la risposta deve essere espressa in pollici, o se le frazioni devono essere ridotte ai minimi termini.\nCreare una Rubrica di Valutazione e Applicarla in Modo Coerente: Come per gli item di saggio, è importante creare e utilizzare in modo coerente una rubrica di valutazione.\n\nGli item a risposta breve, simili agli item di saggio, richiedono una risposta scritta dal candidato ma con limiti più ristretti nella formulazione della risposta. Sono adatti per misurare determinati obiettivi di apprendimento (ad esempio, calcoli matematici) e sono relativamente facili da scrivere. Tuttavia, come gli item di saggio, presentano sfide nella valutazione affidabile e dovrebbero essere usati in modo oculato. Una checklist può fornire una guida utile per lo sviluppo di questi item.\n\n20.7.1 Test di Risposta Tipica\nDopo aver esaminato vari formati di item utilizzati nei test di massima prestazione, ci concentreremo sugli item comunemente usati nei test di risposta tipica, come le scale di personalità e di atteggiamento. Descriveremo diversi formati di item comuni a questi test e forniremo alcune linee guida generali per lo sviluppo di item. La valutazione di sentimenti, pensieri, dialoghi interni e altri comportamenti occulti è meglio realizzata tramite autovalutazione, che sarà il focus della nostra discussione. Tuttavia, come nei test di massima prestazione, esistono numerosi formati di item disponibili per le misure di autovalutazione.\n\n20.7.1.1 Linee Guida per la Redazione di Item in Test di Risposta Tipica\n\nConcentrarsi su Pensieri, Sentimenti e Comportamenti, non su Fatti: Nei test di risposta tipica, l’obiettivo è valutare le esperienze del candidato: i suoi pensieri, sentimenti e comportamenti tipici. Di conseguenza, si dovrebbero evitare affermazioni basate su informazioni fattuali che possono essere valutate come “corrette” o “errate”.\nLimitare le Affermazioni a un Singolo Pensiero, Sentimento o Comportamento: Ogni affermazione dovrebbe concentrarsi su un solo pensiero, sentimento, comportamento o atteggiamento.\nEvitare Affermazioni Universali: Per aumentare la varianza e migliorare l’affidabilità, si dovrebbero scrivere item che misurano le differenze individuali. Se tutti o quasi tutti rispondono a un item nello stesso modo, questo non contribuisce alla misurazione dei costrutti identificati.\nIncludere Item Formulati sia in Modo “Positivo/Favorevole” che “Negativo/Sfavorevole”: Come regola generale, usare una combinazione di item formulati in modo “positivo” e “negativo”. Ciò può incoraggiare i candidati a evitare uno stile di risposta in cui semplicemente segnano la stessa opzione di risposta su tutti gli item. Questo è più applicabile agli item Vero/Falso e alle scale Likert, e meno alle scale di valutazione dove si cerca di valutare la frequenza di pensieri, sentimenti e comportamenti problematici.\nUtilizzare un Numero Appropriato di Opzioni: Per le scale di valutazione, 4 o 5 opzioni di risposta sembrano ottimali per sviluppare affidabilità senza allungare eccessivamente il tempo richiesto per completare le valutazioni. Le scale di valutazione con più di 4 o 5 opzioni raramente migliorano l’affidabilità o la validità dell’interpretazione dei punteggi del test e richiedono più tempo ai candidati per essere completate. Per gli item Likert, il numero massimo di opzioni sembra essere di sette gradini, con un piccolo aumento dell’affidabilità oltre tale numero.\nValutare i Benefici dell’Uso di un Numero Pari o Dispari di Opzioni: Negli item Likert, generalmente si raccomanda l’uso di un numero dispari di scelte con l’opzione centrale come “Neutrale” o “Indeciso”. Questo non è universalmente accettato poiché alcuni autori sostengono l’uso di un numero pari di scelte senza opzione neutra, basandosi sul fatto che alcuni rispondenti tendono a utilizzare eccessivamente la scelta neutra se disponibile. Ciò può risultare in una ridotta varianza e affidabilità. L’eliminazione dell’opzione neutra potrebbe frustrare alcuni rispondenti che potrebbero non completare gli item quando non hanno un’opinione forte. I dati mancanti possono essere un problema significativo in questi casi. La nostra raccomandazione è di usare un numero dispari di opzioni con un’opzione neutra. Con le scale di valutazione della frequenza, questo è meno importante poiché non è necessaria un’opzione “neutrale”.\nEtichettare Chiaramente Ciascuna delle Opzioni nelle Scale di Valutazione e negli Item Likert: Per esempio, fornire etichette per ciascuna delle opzioni di risposta per risolvere eventuali incertezze.\nMinimizzare l’Uso di Determinanti Specifici: L’uso di determinanti specifici come “mai”, “sempre”, “nessuno” e “tutti” dovrebbe essere usato con cautela in quanto possono complicare il processo di risposta.\nCon i Bambini Piccoli, Considerare l’Uso di un Formato di Intervista: Per i bambini piccoli, prendere in considerazione l’utilizzo di un formato di intervista in cui gli item vengono letti al bambino. Questo può aiutare a ridurre la varianza irrilevante al costrutto introdotta dall’eliminazione dell’impatto delle abilità di lettura.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Lo sviluppo degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/01_item_development.html#sommario",
    "href": "chapters/items/01_item_development.html#sommario",
    "title": "20  Lo sviluppo degli item",
    "section": "20.8 Sommario",
    "text": "20.8 Sommario\nAll’inizio di questo capitolo, viene fatta una distinzione principale tra gli item di test in base a se sono a risposta selezionata o a risposta costruita. Successivamente, vengono considerate le loro applicazioni nei test di massima prestazione e nei test di risposta tipica. Per i test di massima prestazione, gli item a risposta selezionata includono formati a scelta multipla, vero/falso e di abbinamento, mentre gli item a risposta costruita comprendono gli item a risposta breve e i saggi. Ogni tipo di item presenta punti di forza e debolezze che sono riassunti di seguito.\nItem a Scelta Multipla: Molto popolari nei test di massima prestazione, presentano numerosi punti di forza come versatilità, valutazione oggettiva e affidabile, e campionatura efficiente del dominio di contenuto. La loro limitazione principale è che non sono efficaci per misurare tutti gli obiettivi e non sono facili da sviluppare.\nItem Vero/Falso: Possono essere valutati in modo oggettivo e affidabile e permettono di rispondere a molti item in breve tempo. Tuttavia, hanno molte debolezze, come la limitazione a obiettivi di apprendimento semplici e una forte vulnerabilità all’indovinamento.\nItem di Abbinamento: Anche questi possono essere valutati in modo oggettivo e affidabile, completati in maniera efficiente e sono relativamente facili da sviluppare. Le loro principali limitazioni includono uno scopo limitato e la possibilità di promuovere la memorizzazione meccanica.\nSaggi: Presentano una domanda o un problema a cui il candidato risponde in formato scritto. I saggi danno una notevole libertà nella formulazione delle risposte, ma sono difficili da valutare in modo affidabile e offrono una campionatura limitata del contenuto. Sono tuttavia adatti per misurare molti obiettivi complessi.\nItem a Risposta Breve: Simili ai saggi, richiedono una risposta scritta, ma con limiti più stretti. Sono adatti per misurare specifici obiettivi di apprendimento come i calcoli matematici e sono relativamente facili da scrivere.\nGli item per i test di risposta tipica si concentrano sull’autovalutazione di sentimenti, pensieri, dialoghi interni e altri comportamenti occulti. Alcuni esempi:\nItem Vero/Falso e Altri Item Dicotomici: Comuni nei test di risposta tipica, questi item si focalizzano sulle esperienze attuali del candidato.\nScale di Valutazione: Possono essere progettate sia per misure di autovalutazione sia per la valutazione di altri individui. A differenza degli item vero/falso che offrono solo due scelte, le scale di valutazione hanno tipicamente da quattro a cinque opzioni e denotano la frequenza.\nGli item Likert, simili alle scale di valutazione, si concentrano sul grado di accordo piuttosto che sulla frequenza. Sono diventati il formato più popolare per la valutazione delle attitudini. In passato, le scale cumulative come quelle di Guttman e Thurstone erano popolari, ma le scale Likert si sono rivelate più facili da sviluppare e con proprietà psicometriche equivalenti o superiori.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Lo sviluppo degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/02_item_analysis.html",
    "href": "chapters/items/02_item_analysis.html",
    "title": "21  Analisi degli item",
    "section": "",
    "text": "21.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nNel processo di sviluppo di un test, esistono numerose procedure utili per valutare la qualità e le caratteristiche di misurazione degli item del test. Tuttavia, non tutte queste procedure sono adatte per tutti i tipi di test, e non tutte forniscono lo stesso livello di informazioni sulla qualità di un determinato item. Le caratteristiche della teoria classica dei test, come la difficoltà dell’item, la discriminazione dell’item e le opzioni di risposta sbagliate, sono utili, così come le caratteristiche associate alle analisi qualitative e alle tecniche della teoria della risposta agli item.\nLa sfida per tutti i sviluppatori di test è quella di valutare i risultati di queste procedure alla luce dell’uso previsto del test e prendere decisioni nella selezione degli item che supportino e massimizzino l’efficacia complessiva del test nel misurare ciò che si propone di misurare. In altre parole, è importante garantire che gli item selezionati siano coerenti con l’obiettivo del test e possano fornire una misurazione accurata di ciò che si intende misurare.\nCome evidenziato nel capitolo precedente, la bontà di un test è determinata dalla qualità dei suoi item. Fortunatamente, esistono diverse procedure quantitative di analisi degli item che sono utili per valutare la qualità e le caratteristiche di misurazione degli item individuali che compongono i test. Queste procedure sono comunemente denominate statistiche o procedure di analisi degli item. A differenza delle analisi di affidabilità e validità che valutano le caratteristiche di misurazione di un test nel suo insieme, le procedure di analisi degli item esaminano gli item individualmente, non l’intero test. Le statistiche di analisi degli item sono utili per aiutare gli sviluppatori di test a decidere quali item mantenere, quali modificare e quali eliminare.\nLa affidabilità dei punteggi di un test e la validità dell’interpretazione dei punteggi del test dipendono dalla qualità degli item presenti nel test. Migliorando la qualità degli item individuali, si migliorerà la qualità complessiva del test. Quando si discute di affidabilità, si è notato che uno dei modi più semplici per aumentare l’affidabilità dei punteggi del test è aumentare il numero di item che contribuiscono a tali punteggi. Questa affermazione è generalmente vera ed è basata sull’assunzione che allungando un test si aggiungano item della stessa qualità degli item esistenti. Se si utilizza l’analisi degli item per eliminare gli item di scarsa qualità e migliorare gli altri, è effettivamente possibile ottenere un test più breve rispetto alla versione originale, ma che produce punteggi più affidabili e risultati con interpretazioni più valide.\nInizieremo la nostra discussione descrivendo le principali procedure quantitative di analisi degli item, tra cui la Difficoltà dell’Item, la Discriminazione dell’Item e l’Analisi delle Opzioni di Risposta. Tuttavia, è importante notare che diversi tipi di item e diversi tipi di test richiedono diverse procedure di analisi degli item. Gli item che vengono valutati in modo dicotomico (cioè giusto o sbagliato) sono gestiti diversamente rispetto agli item valutati su una scala continua (ad esempio, un saggio che può ricevere punteggi da 0 a 10). I test progettati per massimizzare la variabilità dei punteggi (ad esempio, i test con riferimento alla norma) sono gestiti diversamente rispetto ai test di padronanza (cioè valutati come superato o non superato). Mentre discutiamo delle diverse procedure di analisi degli item, specificheremo quali procedure sono più appropriate per quali tipi di item e test.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Analisi degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/02_item_analysis.html#indice-di-difficoltà-dellitem",
    "href": "chapters/items/02_item_analysis.html#indice-di-difficoltà-dellitem",
    "title": "21  Analisi degli item",
    "section": "21.2 Indice di Difficoltà dell’Item",
    "text": "21.2 Indice di Difficoltà dell’Item\nL’Indice di Difficoltà dell’Item, indicato frequentemente con la sigla “p”, rappresenta un parametro fondamentale nella valutazione degli item nei test di massima performance o competenza. Esso è definito come la frazione o la percentuale di candidati che elicitano una risposta corretta all’item in questione. Matematicamente, è espresso dalla formula:\n\\[ p = \\frac{\\text{Numero di Candidati con Risposta Corretta}}{\\text{Numero Totale di Candidati}} \\]\nPer esemplificare, in un contesto di 30 studenti, qualora 20 studenti forniscano una risposta esatta, l’indice si calcola come:\n\\[ p = \\frac{20}{30} = 0.66 \\]\nQuesto indice varia nell’intervallo [0.0, 1.0], dove valori prossimi a 1.0 indicano un’alta facilità dell’item, e viceversa valori prossimi a 0.0 denotano un’alta difficoltà. Un indice pari a 1.0 o 0.0 non contribuisce significativamente alla discriminazione tra i candidati, benché talvolta possano essere impiegati per scopi motivazionali all’inizio di un test.\n\n21.2.1 Efficienza Temporale e Livello di Difficoltà Ottimale\nLa selezione degli item in base al loro livello di difficoltà deve tenere conto anche dell’efficienza temporale. Spesso, item estremamente facili o difficili non aggiungono valore significativo alla misura del test e possono risultare in una gestione subottimale del tempo disponibile.\nIdealmente, un indice di difficoltà medio di 0.50, dove la metà dei candidati risponde correttamente e l’altra metà no, massimizza la variabilità e l’affidabilità del test. Tuttavia, questa uniformità non è sempre desiderabile o fattibile, a causa delle interrelazioni tra gli item e delle specifiche esigenze di misurazione.\n\n\n21.2.2 Influenza del Guessing e Tipologie di Test\nIl livello di difficoltà ottimale varia a seconda della tipologia del test e della possibilità di indovinare le risposte. Nei test con item a risposta costruita, dove l’indovinare è meno rilevante, un indice medio di 0.50 è generalmente preferibile. Nei test a risposta selezionata, come quelli a scelta multipla, si considera un valore medio di “p” più elevato per bilanciare l’effetto dell’indovinamento.\n\n\n21.2.3 Contesti Specifici di Valutazione\nIn test con riferimento ai criteri o test di padronanza, la valutazione della difficoltà segue logiche differenti. Per esempio, in test di padronanza, è comune che la maggior parte degli item abbia un indice “p” elevato, per riflettere l’aspettativa che la maggioranza dei candidati superi il test. In contesti di selezione o per test destinati a individuare candidati altamente performanti, si potrebbero preferire item con un indice di difficoltà significativamente diverso.\n\n\n21.2.4 Variazioni in Funzione del Campione\nÈ cruciale notare che l’indice di difficoltà è intrinsecamente legato alle caratteristiche del campione considerato. Ad esempio, lo stesso item può presentare indici di difficoltà differenti se somministrato a gruppi con livelli di competenza diversi.\n\n\n21.2.5 Statistica della Percentuale di Approvazione\nPer i test di risposta tipica, si utilizza un indice analogo all’indice di difficoltà, noto come statistica della percentuale di approvazione. Questa statistica indica la percentuale di candidati che rispondono in un determinato modo a un item, e varia a seconda del campione e del contesto.\n\n\n21.2.6 Applicazioni nell’Analisi e Sviluppo di Test\nL’analisi dell’indice di difficoltà e di altre statistiche relative agli item è cruciale per gli sviluppatori di test nella selezione, modifica, o eliminazione degli item durante lo sviluppo o la revisione dei test. Tale analisi è complementare ad altre procedure di analisi degli item, come l’indice di discriminazione dell’item, che sarà discusso successivamente.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Analisi degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/02_item_analysis.html#discriminazione-dellitem",
    "href": "chapters/items/02_item_analysis.html#discriminazione-dellitem",
    "title": "21  Analisi degli item",
    "section": "21.3 Discriminazione dell’item",
    "text": "21.3 Discriminazione dell’item\nL’item discrimination, in italiano “discriminazione dell’item,” si riferisce a quanto bene un item può discriminare o differenziare tra i partecipanti al test che differiscono sulla costrutto misurato dal test. Ad esempio, se un test è progettato per misurare l’abilità di lettura, la discriminazione dell’item riflette la capacità di un item di distinguere tra individui con buone capacità di lettura e quelli con scarse capacità di lettura. A differenza del livello di difficoltà dell’item, per il quale esiste un accordo su come calcolare la statistica, nel corso degli anni sono stati sviluppati oltre 50 diversi indici di discriminazione dell’item (Anastasi & Urbina, 1997). Fortunatamente, la maggior parte di questi indici produce risultati simili (Engelhart, 1965; Oosterhof, 1976). Ci concentreremo sulla discussione di due dei più popolari indici di discriminazione dell’item: l’indice di discriminazione e le correlazioni item-totali.\n\n21.3.1 Indice di Discriminazione\nUn metodo popolare per calcolare un indice di discriminazione dell’item si basa sulla differenza nelle prestazioni tra due gruppi. Sebbene ci siano modi diversi per selezionare i due gruppi, vengono tipicamente definiti in termini di prestazioni totali al test. Un approccio comune è selezionare il 27% migliore e il 27% peggiore dei partecipanti in termini di prestazioni complessive al test, escludendo il 46% centrale (Kelley, 1939). Alcuni esperti di valutazione hanno suggerito di utilizzare il 25% migliore e il 25% peggiore, alcuni il 33% migliore e il 33% peggiore, e alcuni la metà superiore e inferiore. In pratica, tutti questi sono probabilmente accettabili, ma la nostra raccomandazione è di utilizzare il tradizionale 27% superiore e inferiore. La difficoltà dell’item è calcolata separatamente per ciascun gruppo, e questi sono denominati pT e pB (“T” per il gruppo superiore, “B” per il gruppo inferiore). La differenza tra pT e pB è l’indice di discriminazione, designato come D. Viene calcolato con la seguente formula (es. Johnson, 1951):\n\\[\nD = p_T - p_B\n\\]\ndove: - D = Indice di Discriminazione - pT = Proporzione dei partecipanti nel Gruppo Superiore che Risponde Correttamente all’Item - pB = Proporzione dei partecipanti nel Gruppo Inferiore che Risponde Correttamente all’Item\nPer illustrare la logica di questo indice, consideriamo un test di rendimento progettato per misurare il rendimento accademico in un’area specifica. Se l’item sta discriminando tra partecipanti che conoscono il materiale e quelli che non lo conoscono, allora i partecipanti più informati (cioè, quelli nel “gruppo superiore”) dovrebbero rispondere correttamente all’item più spesso dei partecipanti meno informati (cioè, quelli nel “gruppo inferiore”). Ad esempio, se pT = 0.80 (indicando che l’80% dei partecipanti nel gruppo superiore ha risposto correttamente all’item) e pB = 0.30 (indicando che il 30% dei partecipanti nel gruppo inferiore ha risposto correttamente all’item), allora:\n\\[\nD = 0.80 - 0.30 = 0.50.\n\\]\nHopkins (1998) ha fornito linee guida per valutare gli item in termini dei loro valori di D. Secondo queste linee guida, i valori di D superiori a 0.40 sono considerati eccellenti, tra 0.30 e 0.39 sono buoni, tra 0.11 e 0.29 sono accettabili e tra 0.00 e 0.10 sono scadenti. Gli item con valori di D negativi probabilmente sono stati formulati in modo errato o presentano altri problemi gravi. Altri esperti di valutazione hanno fornito linee guida diverse, alcune più rigorose e altre più indulgenti. Come regola generale, si suggerisce che gli item con valori di D superiori a 0.30 siano accettabili (quanto più alti, tanto meglio), mentre gli item con valori di D inferiori a 0.30 dovrebbero essere attentamente valutati, e eventualmente rivisti o eliminati.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Analisi degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/02_item_analysis.html#test-di-padronanza",
    "href": "chapters/items/02_item_analysis.html#test-di-padronanza",
    "title": "21  Analisi degli item",
    "section": "21.4 Test di Padronanza",
    "text": "21.4 Test di Padronanza\nNei test di padronanza, gli item tendono ad avere indici di difficoltà più elevati rispetto a quelli di test normativi, implicando che gli item sono generalmente più facili. Questa caratteristica deriva dall’assunzione che la maggior parte dei partecipanti otterrà risultati positivi nei test di padronanza. Di conseguenza, è frequente che gli item in questi test abbiano una proporzione elevata di risposte corrette (valori medi di p), talvolta raggiungendo il 90%.\nQuesta tendenza richiede un adattamento nell’interpretazione degli indici di difficoltà degli item. La necessità di adattamento si estende anche all’interpretazione degli indici di discriminazione. Nei test di padronanza, dove è comune che sia partecipanti con punteggi alti sia quelli con punteggi bassi otteniano valori elevati di p, gli indici tradizionali di discriminazione potrebbero non riflettere accuratamente le capacità di misurazione di un item.\nPer affrontare questa sfida, sono stati proposti diversi metodi per calcolare la discriminazione degli item nei test di padronanza. Aiken (2000), ad esempio, suggerisce un metodo che considera la difficoltà degli item basandosi sui partecipanti che hanno raggiunto (o non raggiunto) il punteggio di padronanza. La formula proposta è:\n\\[\nD = p_{mastery} - p_{non-mastery}\n\\]\ndove \\(p_{mastery}\\) rappresenta la proporzione di partecipanti che hanno raggiunto la padronanza e hanno risposto correttamente all’item, mentre \\(p_{non-mastery}\\) indica la proporzione di partecipanti che non hanno raggiunto la padronanza e hanno risposto correttamente.\nUn altro metodo per valutare la discriminazione degli item è l’uso della correlazione item-totale. Questo approccio correla le prestazioni su un singolo item con il punteggio totale del test, utilizzando di solito la correlazione punto-biserial. Un’alta correlazione item-totale indica che l’item misura lo stesso costrutto dell’intero test e discrimina efficacemente tra individui con alta e bassa competenza nel costrutto misurato. È preferibile calcolare questa correlazione escludendo l’item in esame dal punteggio totale del test, per evitare di “contaminare” o gonfiare la correlazione. Attualmente, la correlazione item-totale è il metodo più utilizzato per esaminare la discriminazione degli item nei test.\nQuesti approcci offrono strumenti importanti per comprendere e migliorare la qualità dei test di padronanza, garantendo che siano sia accessibili sia capaci di distinguere accuratamente tra diversi livelli di competenza.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Analisi degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/02_item_analysis.html#discriminazione-degli-item-nei-test-con-risposte-tipiche",
    "href": "chapters/items/02_item_analysis.html#discriminazione-degli-item-nei-test-con-risposte-tipiche",
    "title": "21  Analisi degli item",
    "section": "21.5 Discriminazione degli Item nei Test con Risposte Tipiche",
    "text": "21.5 Discriminazione degli Item nei Test con Risposte Tipiche\nL’analisi degli item nei test con risposte tipiche riguarda l’analisi di item quelli finalizzati alla misurazione di tendenze comportamentali. Un esempio pertinente è un item di un test progettato per valutare la propensione alla ricerca di sensazioni, basato su affermazioni a cui si risponde con “Vero” o “Falso”. In questo contesto, le risposte “Vero” (valutate con “1”) indicano una tendenza verso comportamenti di ricerca di sensazioni, mentre le risposte “Falso” (valutate con “0”) denotano una propensione ad evitarli. Pertanto, punteggi elevati in tali test suggeriscono un gusto per comportamenti ad alta sensazione, mentre punteggi bassi indicano una tendenza all’evitamento.\nLa correlazione tra gli item e il punteggio totale del test può essere usata per l’identificazione degli item più efficaci nel discriminare tra individui con diverse propensioni alla ricerca di sensazioni. Gli item con elevata correlazione risultano particolarmente utili per distinguere soggetti con alta o bassa propensione a tali comportamenti.\nL’interpretazione degli indici di difficoltà e discriminazione diventa però più complessa nei cosiddetti “test di velocità”. Questi test sono caratterizzati da item generalmente facili, ma con limiti di tempo stringenti che impediscono ai candidati di completarli tutti. La prestazione nei test di velocità è quindi influenzata principalmente dalla rapidità di risposta, contrariamente ai test di potenza, dove il tempo non è un fattore limitante e la difficoltà degli item varia significativamente.\nNei test di velocità, la difficoltà e la capacità discriminativa degli item tendono a riflettere la loro posizione all’interno del test piuttosto che la loro intrinseca difficoltà o capacità di discriminare tra candidati. Gli item situati verso la fine del test tendono ad essere completati da un numero minore di candidati a causa dei limiti temporali, non perché siano effettivamente più difficili. Allo stesso modo, l’indice di discriminazione degli item posti verso la fine potrebbe essere esagerato, in quanto solo i candidati più capaci riescono a raggiungerli e completarli.\nNonostante siano state proposte varie metodologie per controbilanciare questi fattori, ogni approccio presenta delle limitazioni e non ha ancora guadagnato un’ampia accettazione nella comunità scientifica. Pertanto, è essenziale essere consapevoli di queste complessità e tenerle in considerazione nell’interpretazione delle analisi degli item nei test di velocità.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Analisi degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/02_item_analysis.html#analisi-dei-distrattori",
    "href": "chapters/items/02_item_analysis.html#analisi-dei-distrattori",
    "title": "21  Analisi degli item",
    "section": "21.6 Analisi dei Distrattori",
    "text": "21.6 Analisi dei Distrattori\nÈ cruciale valutare l’efficacia dei distrattori nell’analisi quantitativa dei test a scelta multipla. I distrattori sono le opzioni errate fornite nelle domande, progettate per deviare l’attenzione degli esaminandi meno preparati. Questa analisi esamina la frequenza con cui esaminandi con punteggi alti e bassi scelgono ciascun distrattore. Un aspetto fondamentale è analizzare ogni distrattore ponendo due questioni centrali.\nPrima, il distrattore è effettivamente un’opzione che confonde alcuni esaminandi? Se nessuno sceglie il distrattore, questo indica che non sta adempiendo alla sua funzione. Un distrattore adeguato dovrebbe essere selezionato da alcuni esaminandi. Al contrario, se un distrattore è palesemente errato e nessuno lo sceglie, risulta inefficace e necessita di revisione o sostituzione.\nSeconda, quanto bene il distrattore discrimina tra i diversi gruppi di esaminandi? I distrattori efficaci tendono ad attrarre più candidati con punteggi bassi rispetto a quelli con punteggi alti. Analizzando la risposta corretta, ci aspettiamo che più esaminandi con punteggi alti la scelgano rispetto a quelli con punteggi bassi, indicando una discriminazione positiva. Per i distrattori, l’effetto dovrebbe essere l’opposto: più esaminandi con punteggi bassi dovrebbero scegliere i distrattori rispetto a quelli con punteggi alti, dimostrando così una discriminazione negativa.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Analisi degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/02_item_analysis.html#le-curve-caratteristiche-degli-item-nella-irt",
    "href": "chapters/items/02_item_analysis.html#le-curve-caratteristiche-degli-item-nella-irt",
    "title": "21  Analisi degli item",
    "section": "21.7 Le Curve Caratteristiche Degli Item nella IRT",
    "text": "21.7 Le Curve Caratteristiche Degli Item nella IRT\nUn argomento importante nella discussione dell’analisi degli item riguarda la nozione di curva caratteristica dell’item, così com’è stata formultata dalla IRT. Questo argomento verrà trattato in maniera approfondita in un capitolo successivo.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Analisi degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/02_item_analysis.html#considerazioni-conclusive",
    "href": "chapters/items/02_item_analysis.html#considerazioni-conclusive",
    "title": "21  Analisi degli item",
    "section": "21.8 Considerazioni Conclusive",
    "text": "21.8 Considerazioni Conclusive\nQuesto capitolo fornisce una panoramica approfondita delle procedure di analisi degli item, strumenti fondamentali per gli sviluppatori di test nel decidere quali item mantenere, modificare o eliminare. Si esplorano diverse procedure, tra cui:\n\nLivello di Difficoltà dell’Item: Definito come la percentuale di esaminandi che rispondono correttamente a un item, l’indice di difficoltà (p) varia da 0.0 a 1.0. Gli item più facili presentano valori decimali maggiori, mentre quelli difficili valori minori. Il livello ottimale di difficoltà, per massimizzare la variabilità tra gli esaminandi, è 0.50. Tuttavia, a seconda della situazione, possono essere preferiti valori diversi, generalmente tra 0.20 e 0.80.\nDiscriminazione dell’Item: Questo concetto si riferisce alla capacità di un item di distinguere tra esaminandi che variano rispetto al costrutto del test. Discutiamo l’indice di discriminazione dell’item (D), considerando accettabili valori di D pari o superiori a 0.30, mentre quelli inferiori a 0.30 potrebbero richiedere revisione o eliminazione. Esploriamo anche la correlazione item-totalità come metodo alternativo per esaminare la discriminazione.\nAnalisi dei Distrattori: Questa procedura valuta l’efficacia dei distrattori nelle domande a scelta multipla, ponendo due domande principali: un distrattore funzionale dovrebbe attirare alcuni esaminandi e, per la discriminazione, dovrebbe attrarre più esaminandi nel gruppo con punteggi bassi rispetto a quelli con punteggi alti.\nProcedure Qualitative: Oltre alle procedure quantitative, si suggerisce l’utilizzo di metodi qualitativi per migliorare i test. Tra questi, la revisione accurata del test, la valutazione da parte di colleghi fidati e il feedback degli esaminandi sulla chiarezza e problemi degli item.\nCurve Caratteristiche dell’Item e Teoria della Risposta all’Item (IRT): Le curve caratteristiche dell’item (ICC) rappresentano graficamente la relazione tra l’abilità e la probabilità di risposta corretta. L’IRT, componente centrale delle ICC, assume che le risposte agli item siano determinate da tratti latenti e ha influenzato significativamente lo sviluppo di test moderni, inclusi i test adattivi computerizzati.\n\nL’utilizzo di queste procedure durante lo sviluppo del test migliora l’affidabilità dei punteggi e la validità delle loro interpretazioni, essendo entrambe dipendenti dalla qualità degli item. La rimozione degli item scarsi e il miglioramento degli altri possono anche portare a un test più corto ed efficiente, con punteggi più affidabili e interpretazioni più valide.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Analisi degli item</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html",
    "href": "chapters/path_analysis/01_path_analysis.html",
    "title": "22  Analisi dei percorsi",
    "section": "",
    "text": "22.1 Introduzione\nLe visualizzazioni rivestono un ruolo fondamentale nel comunicare in modo chiaro e sintetico le relazioni tra variabili. Questo è particolarmente evidente quando si opera con modelli di equazioni strutturali (SEM) che delineano una rete di interconnessioni tra variabili sia osservabili che latenti. In tali contesti, i ricercatori frequentemente si avvalgono di strumenti grafici per agevolare la specificazione e l’esplicitazione del modello, oltre che per presentare in maniera comprensibile i risultati ottenuti.\nL’analisi del percorso, o path analysis, è una tecnica statistica multivariata utilizzata nell’ambito della ricerca quantitativa per esaminare e descrivere le relazioni causali tra un insieme di variabili. Questo metodo si avvale di modelli grafici, noti come diagrammi di percorso, che rappresentano le relazioni ipotizzate tra le variabili, illustrando graficamente le relazioni dirette, indirette e reciproche tra di esse.\nIl fulcro dell’analisi del percorso è la decomposizione e la quantificazione delle relazioni tra le variabili, permettendo agli analisti di distinguere tra effetti diretti, indiretti e totali. Gli effetti diretti corrispondono all’influenza immediata che una variabile esercita su un’altra, mentre gli effetti indiretti rappresentano l’impatto mediato attraverso una o più variabili intermedie. L’effetto totale è la somma degli effetti diretti e indiretti.\nSewall Wright, un genetista che operava presso il Dipartimento dell’Agricoltura degli Stati Uniti, fu il precursore nello sviluppo dei diagrammi di percorso per descrivere i modelli di equazioni strutturali già negli anni ’20 del secolo scorso. Questa sua innovazione ha permesso di ottenere una rappresentazione visiva delle connessioni tra variabili, aprendo la strada all’analisi dei percorsi.\nCon il trascorrere del tempo, questa metodologia è stata adottata con successo come uno strumento efficace per discriminare gli effetti diretti da quelli indiretti nelle relazioni tra variabili. Inoltre, essa si è dimostrata di grande utilità nel valutare la solidità e la validità delle relazioni causali ipotizzate all’interno dei modelli di equazioni strutturali.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#path-diagram",
    "href": "chapters/path_analysis/01_path_analysis.html#path-diagram",
    "title": "22  Analisi dei percorsi",
    "section": "\n22.2 Path diagram",
    "text": "22.2 Path diagram\nNel path diagram è possibile distinguere due tipi di variabili: quelle che sono influenzate da altre variabili nel sistema e quelle che fungono da sorgenti di effetti.\n\nVariabili esogene: rappresentano elementi esterni al sistema in esame. Esse agiscono come variabili indipendenti, generando effetti in modo causale senza essere influenzate da altre variabili presenti nel modello. Nel diagramma, le loro cause si trovano al di fuori del sistema rappresentato.\nVariabili endogene: sono quelle che possono assumere il doppio ruolo di risultati (essendo influenzate da altre variabili) e di cause (influenzando ulteriori variabili). In alcuni casi, svolgono un ruolo esclusivamente dipendente. Le cause delle variabili endogene sono sempre incluse all’interno del path diagram.\n\nQuesta distinzione riflette quella tra variabili indipendenti e dipendenti nei modelli lineari, ma con una maggiore enfasi sulla natura causale e sulla posizione delle variabili nel sistema rappresentato.\nUn path diagram (diagramma di percorso) utilizza specifici simboli grafici per rappresentare le variabili e le loro relazioni:\n\n\nVariabili osservate (o indicatori): rappresentate con quadrati o rettangoli.\n\nVariabili latenti (come fattori comuni con più indicatori): rappresentate con cerchi o ellissi.\n\nIl path diagram evidenzia le interazioni tra le variabili di interesse, distinguendo i legami causali da quelli associativi:\n\n\nFrecce unidirezionali (\\(\\rightarrow\\)): indicano relazioni causali. La variabile alla punta della freccia è influenzata da quella alla base.\n\nFrecce curve bidirezionali (\\(\\leftrightarrow\\)): rappresentano relazioni associative, indicando covarianze (nella soluzione non standardizzata) o correlazioni (nella soluzione standardizzata), senza implicare una relazione causale diretta.\n\nL’assenza di una freccia tra due variabili implica che non vi è correlazione o relazione causale diretta tra esse nel modello. Il diagramma, quindi, sintetizza visivamente le ipotesi teoriche sulle relazioni tra le variabili.\nNella Figura 22.1, si illustrano le relazioni tra nove variabili osservate e tre variabili latenti mediante il path diagram. Una freccia curva bidirezionale che si collega a una singola variabile rappresenta la varianza residua della variabile, ovvero la quota di varianza non spiegata dalle relazioni causali illustrate nel diagramma di percorso.\n\n\n\n\n\nFigura 22.1: Diagramma di percorso per un modello a tre fattori comuni.\n\n\nUn triangolo contenente il numero 1 simboleggia la media di una variabile (qui non presente).\n\n22.2.1 Parametri nei Modelli di Equazioni Strutturali\nI parametri nei modelli di equazioni strutturali possono essere categorizzati come segue, quando le medie non sono oggetto di analisi:\n\n\nVarianze e Covarianze delle Variabili Esogene:\n\nQuesti parametri rappresentano la variabilità intrinseca delle variabili esogene (quelle non influenzate da altre nel modello) e le relazioni reciproche tra di esse.\n\n\n\nEffetti Diretti sulle Variabili Endogene da Altre Variabili:\n\nQuesti parametri descrivono come le variabili endogene sono influenzate direttamente da altre variabili nel modello.\n\n\n\nIn termini di specificazione, un parametro nel modello può essere classificato come libero, fisso o vincolato:\n\n\nParametro Libero:\n\nQuesto tipo di parametro è stimato dal software statistico utilizzando i dati a disposizione.\n\n\n\nParametro Fisso:\n\nUn parametro fisso è definito per essere uguale a una costante specificata a priori. In questo caso, il software accetta il valore costante come stima, indipendentemente dai dati. Ad esempio, l’ipotesi che la variabile X non abbia effetti diretti su Y corrisponde alla specifica che il coefficiente per il percorso da X a Y sia fissato a zero.\n\n\n\nParametro Vincolato:\n\nIn questo caso, il parametro segue certe restrizioni imposte nell’analisi, che possono essere basate su teorie o ipotesi precedenti. Ad esempio, l’analista può assumere che due parametri siano uguali.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#gradi-di-libertà-nei-modelli-parametrici",
    "href": "chapters/path_analysis/01_path_analysis.html#gradi-di-libertà-nei-modelli-parametrici",
    "title": "22  Analisi dei percorsi",
    "section": "\n22.3 Gradi di Libertà nei Modelli Parametrici",
    "text": "22.3 Gradi di Libertà nei Modelli Parametrici\nIn statistica, la complessità di un modello parametrico è limitata dalla quantità di informazioni statistiche disponibili nei dati, ovvero il numero di varianze e covarianze uniche che possono essere derivate dalla matrice di covarianza campionaria. Questo numero non dipende dalla dimensione del campione (\\(N\\)), ma esclusivamente dal numero di variabili osservate (\\(v\\)).\n\n22.3.1 Calcolo della Quantità di Informazioni\nLa quantità di informazioni statistiche in un modello è data dalla formula:\n\\[\np = \\frac{v(v + 1)}{2},\n\\]\ndove \\(v\\) è il numero di variabili osservate. Questo conteggio include:\n\n\n\\(v\\): le varianze delle variabili osservate (i termini sulla diagonale della matrice di covarianza),\n\n\\(\\frac{v(v - 1)}{2}\\): le covarianze uniche tra coppie di variabili (i termini sotto la diagonale principale).\n\nAd esempio, se \\(v = 5\\), la quantità di informazioni sarà:\n\\[\np = \\frac{5 \\times 6}{2} = 15.\n\\]\nIn questo caso, le 15 informazioni statistiche comprendono 5 varianze e 10 covarianze uniche. Quindi, un modello parametrico con 5 variabili osservate può stimare al massimo 15 parametri. Aggiungere più casi al campione non aumenta la quantità di informazioni, ma incrementare il numero di variabili osservate sì.\n\n22.3.2 Gradi di Libertà del Modello\nI gradi di libertà del modello (\\(df_M\\)) rappresentano la differenza tra la quantità di informazioni disponibili (\\(p\\)) e il numero di parametri liberi (\\(q\\)) che il modello cerca di stimare:\n\\[\ndf_M = p - q.\n\\]\n\n\nModello identificabile: \\(df_M \\geq 0\\). Il modello ha abbastanza informazioni per stimare i parametri in modo univoco.\n\nModello non identificabile: \\(df_M &lt; 0\\). Non ci sono sufficienti informazioni per stimare tutti i parametri, portando a infinite soluzioni possibili.\n\nSe \\(df_M &lt; 0\\), il modello deve essere modificato riducendo il numero di parametri liberi, ad esempio imponendo vincoli o fissando alcuni parametri a valori specifici. In caso contrario, i software di modellazione produrranno errori.\n\n22.3.3 Interpretazione dei Gradi di Libertà\n\nModello con \\(df_M = 0\\): Si adatta perfettamente ai dati, ma questa perfetta aderenza è inevitabile e non garantisce che il modello sia generalizzabile o valido per altri campioni.\nModello con \\(df_M &gt; 0\\): Consente un margine di discrepanza tra i dati osservati e le stime del modello. Modelli con più gradi di libertà sono più esposti al rischio di essere rifiutati, ma una loro validazione aumenta la fiducia nella loro generalizzabilità.\n\nRaykov e Marcoulides (2006) descrivono i gradi di libertà come “dimensioni lungo cui un modello può essere rifiutato”. Un modello con più gradi di libertà che si adatta bene ai dati dimostra maggiore robustezza rispetto a un modello con pochi gradi di libertà.\n\n22.3.4 Principio di Parsimonia\nNella scelta tra modelli, è preferibile optare per quello più semplice (con meno parametri liberi), a parità di adattamento ai dati, purché sia teoricamente plausibile. Questo principio di parsimonia è cruciale per evitare sovradattamento e garantire la generalizzabilità del modello.\n\nIn sintesi, i gradi di libertà rappresentano un equilibrio tra la complessità del modello e le informazioni disponibili nei dati. La loro corretta interpretazione è essenziale per valutare l’identificabilità, la validità e la parsimonia di un modello parametrico.\n\n22.3.5 Varianza Residua nelle Variabili Endogene\nLa Figura 22.2 illustra la relazione tra due variabili osservabili e il modo in cui la varianza residua viene trattata nei modelli a percorsi. L’effetto totale di \\(X\\) su \\(Y\\) è rappresentato tramite un percorso diretto, che evidenzia l’effetto causale lineare di \\(X\\) su \\(Y\\). Nel diagramma:\n\nLa varianza di \\(X\\), una variabile esogena, è un parametro libero e viene rappresentata da una freccia curva bidirezionale (secondo il simbolismo RAM), che indica una varianza.\nLa varianza di \\(Y\\), una variabile endogena, non è libera, poiché include un termine di disturbo o errore (\\(D\\)), una variabile latente che rappresenta la porzione di varianza in \\(Y\\) non spiegata da \\(X\\).\n\n\n\n\n\n\nFigura 22.2: Diagramma di percorso: (a) rappresentazione esplicita secondo il modello RAM; (b) versione semplificata. (Adattata da Kline (2023)).\n\n\nNel pannello (a), il valore numerico “1” accanto al percorso tra il termine di disturbo (\\(D\\)) e \\(Y\\) è una costante di scala. Questo valore fissa una metrica per il termine di disturbo, necessaria per stimare la varianza latente. Questo approccio è noto come vincolo di identificazione del carico unitario (unit loading identification constraint, ULI). Tale costante informa il software di suddividere la varianza totale di \\(Y\\) in due componenti ortogonali:\n\nLa varianza spiegata da \\(X\\).\nLa varianza residua, rappresentata dal termine di disturbo (\\(var_D\\)).\n\nNel pannello (b), la stessa relazione è presentata in modo più sintetico. Qui, il termine di disturbo non è rappresentato esplicitamente, ma la varianza residua può essere descritta in modo equivalente con una freccia curva bidirezionale che denota \\(1 \\times var_D \\times 1\\).\n\n22.3.6 Rappresentazioni Alternative della Varianza Residua\nUn altro modo per rappresentare la varianza residua di \\(Y\\) consiste nell’attribuire \\(1\\) a \\(var_D\\) e utilizzare il valore \\(\\sqrt{var_D}\\) per la freccia causale da \\(D\\) a \\(Y\\). Il risultato finale resta invariato, poiché la varianza residua di \\(Y\\) sarebbe comunque espressa come:\n\\[\n\\sqrt{var_D} \\times 1 \\times \\sqrt{var_D}.\n\\]\n\n22.3.7 Fonti della Varianza Residua\nLa varianza residua (\\(var_D\\)) rappresenta la porzione di varianza in \\(Y\\) non spiegata da \\(X\\). Essa può derivare da diverse fonti, tra cui:\n\n\nVariazione sistematica da cause non misurate: Fattori non inclusi nel modello che influenzano sistematicamente \\(Y\\).\n\nVariazione casuale intrinseca: Variabilità naturale che esiste indipendentemente dalle relazioni modellate.\n\nErrore di misurazione casuale: Errori nel processo di misurazione, stimabili tramite analisi di affidabilità degli strumenti.\n\nMancata specificazione della forma funzionale corretta: Varianza dovuta a un’errata rappresentazione della relazione causale (ad esempio, una relazione modellata come lineare quando è in realtà non lineare).\n\nNel pannello (a), il percorso da \\(D\\) a \\(Y\\) rappresenta l’effetto diretto cumulativo di queste fonti sulla variabile endogena \\(Y\\). Sebbene teoricamente distinguibili, queste fonti spesso si sovrappongono o interagiscono nella pratica.\n\n22.3.8 Gestione della Varianza Residua nei Software SEM\nNei software per l’analisi SEM, i termini di disturbo vengono gestiti automaticamente. Ad esempio, in lavaan, il comando:\nY ~ X\nistruisce il software a regredire \\(Y\\) su \\(X\\) e a trattare il termine di disturbo come parametro libero. Questo comando:\n\nDefinisce l’effetto causale di \\(X\\) su \\(Y\\).\nStabilisce che la varianza di \\(X\\) e quella del termine di disturbo di \\(Y\\) siano parametri da stimare.\n\n22.3.9 Requisiti per l’Identificazione del Modello\nPer garantire l’identificazione del modello, sono necessari due requisiti fondamentali:\n\nI gradi di libertà (\\(df_M\\)) devono essere maggiori o uguali a zero: \\[\ndf_M = p - q,\n\\] dove \\(p\\) è il numero di informazioni statistiche disponibili e \\(q\\) il numero di parametri liberi.\nOgni variabile latente, inclusi i termini di disturbo, deve avere una scala definita.\n\n22.3.10 Confronto tra le Rappresentazioni\n\n\nRappresentazione dettagliata (pannello a): Include tutti i termini espliciti, come il termine di disturbo e i vincoli di scala. È utile per comprendere la struttura completa del modello.\n\nRappresentazione sintetica (pannello b): Ommette i simboli per i parametri di varianza e il termine di disturbo, fornendo una visione semplificata delle relazioni principali.\n\nIn sintesi, entrambe le rappresentazioni descrivono lo stesso modello, ma con diversi livelli di dettaglio. La scelta dipende dall’obiettivo: chiarezza concettuale o sintesi grafica.\n\n22.3.11 Considerazioni sugli Errori di Misurazione nei Modelli a Percorsi\nRiprendendo la discussione sulla Figura 22.2, possiamo delineare le seguenti ipotesi fondamentali:\n\nAffidabilità della Variabile Esogena X: Si assume che i punteggi sulla variabile esogena X siano privi di errore, ovvero perfettamente affidabili, con un coefficiente di affidabilità (\\(r_{XX}\\)) di 1.0.\nCorrettezza della Direzione Causale: La relazione causale da X a Y è assunta come correttamente specificata e caratterizzata da una stretta linearità.\nIndipendenza delle Cause Non Misurate di Y da X: Si presume che le cause non misurate (latenti) di Y non siano correlate con X, escludendo quindi l’esistenza di cause comuni non misurate che influenzano simultaneamente entrambe le variabili – ricordiamo la discusione precedente sull’errore di specificazione.\n\nIn ambito di modellazione dei percorsi, l’assunzione che le variabili esogene siano prive di errori di misurazione riflette un presupposto simile a quello adottato nelle regressioni multiple standard, dove i predittori sono considerati esenti da errori di misurazione. Questa assunzione è necessaria poiché le variabili esogene nei modelli a percorsi non includono termini di errore, rendendo impossibile incorporare l’errore casuale in tali modelli. Al contrario, nelle variabili endogene di tali modelli, la presenza di termini di errore permette di tenere conto dell’errore di misurazione.\nNel caso di una regressione bivariata, un errore di misurazione presente solo nella variabile dipendente Y influisce sul modello aumentando l’errore standard della stima di regressione, riducendo il valore di \\(R^2\\) e diminuendo il valore assoluto del coefficiente di regressione standardizzato, a causa dell’incremento dell’errore di misurazione in Y. Invece, l’errore di misurazione presente solo nella variabile predittiva X (ma non in Y) tende a introdurre un bias negativo nei coefficienti di regressione – cioè una sistematica sottostima dei veri valori dei coefficienti di regressione.\nQuando entrambe le variabili X e Y presentano errori di misurazione, la dinamica risultante è più complessa da prevedere. Se gli errori di misurazione in X e Y sono indipendenti, il risultato più comune è un bias negativo (ossia una sottostima dei coefficienti di regressione della popolazione). Tuttavia, se gli errori di misurazione sono comuni tra X e Y, la regressione potrebbe sovrastimare i coefficienti della popolazione, portando a un bias positivo. È essenziale riconoscere che l’errore di misurazione non causa sempre un bias negativo. Di conseguenza, la presenza di errori di misurazione non modellati nelle variabili esogene può significativamente distorcere i risultati, specialmente in presenza di forti correlazioni tra multiple variabili esogene. Per ridurre questi rischi, si raccomanda di valutare l’affidabilità dei punteggi associati alle variabili esogene. Questa pratica metodologica, che consiste nel verificare la precisione e la consistenza delle misure delle variabili predittive, aiuta a identificare e quantificare eventuali errori di misurazione. Un’accurata stima dell’affidabilità contribuisce a garantire l’integrità e la validità dei risultati dei modelli a percorsi, mitigando l’impatto che gli errori di misurazione possono avere sull’analisi.\n\n22.3.12 Direzionalità Causale e Forma Funzionale della Relazione X-Y\nL’assunzione che la relazione tra le variabili X e Y sia lineare, come presentato nella Figura 22.2, può essere esaminata attraverso l’analisi dei dati. Se si osserva che la relazione è significativamente curvilinea, si può adeguare l’analisi per attenuare il presupposto di linearità. Ciò può essere realizzato attraverso metodi come la regressione polinomiale o la regressione non parametrica, che permettono di modellare relazioni più complesse rispetto a un semplice modello lineare.\nTuttavia, la direzionalità dell’effetto causale rappresenta una sfida differente e non è direttamente testabile attraverso metodi statistici standard. Nell’ambito dei modelli SEM, le direzioni degli effetti causali sono generalmente ipotizzate piuttosto che empiricamente verificate. Questo perché è possibile costruire modelli SEM equivalenti che utilizzano le stesse variabili e hanno lo stesso numero di gradi di libertà (\\(df_M\\)), ma con direzioni inverse di alcuni effetti causali. Inoltre, entrambi i modelli, nonostante le differenze nelle direzionalità causali, mostreranno lo stesso grado di adattamento ai dati osservati.\nUn’ulteriore ragione per cui la direzionalità causale è tipicamente assunta piuttosto che testata in SEM risiede nella natura degli studi SEM stessi. La maggior parte degli studi SEM si basa su disegni trasversali, dove tutte le variabili sono misurate contemporaneamente, senza una chiara precedenza temporale. In questi contesti, l’unica base per definire la direzionalità causale è l’argomentazione teorica del ricercatore, che deve giustificare perché si presume che X influenzi Y e non viceversa, o perché non si considera una relazione di feedback o causazione reciproca tra le due variabili.\nDi conseguenza, la metodologia SEM non è intrinsecamente una tecnica per la scoperta di relazioni causali. Se un modello è corretto, SEM può essere utilizzato per stimare le direzioni, le dimensioni e la precisione degli effetti causali. Tuttavia, questo non è il modo in cui i ricercatori tipicamente impiegano le analisi SEM. Piuttosto, un modello causale viene ipotizzato e poi adattato ai dati basandosi sulle assunzioni delineate. Se queste assunzioni risultano essere errate, anche i risultati dell’analisi saranno invalidi. Questo enfatizza il punto sollevato da Pearl (2000), che sostiene che\n\nle ipotesi causali sono un prerequisito essenziale per validare qualsiasi conclusione causale (p. 136).\n\nQuesto implica la necessità di una solida base teorica e concettuale nella formulazione di modelli causali nella modellazione SEM.\n\n22.3.13 Confondimento nei Modelli Parametrici\nNella teoria dei modelli statistici, l’endogenità si riferisce a una situazione in cui una variabile all’interno di un modello è correlata con i termini di errore. Questo può creare problemi nella stima dei parametri del modello e può portare a conclusioni errate riguardo le relazioni causali tra le variabili.\nNel contesto del diagramma di una catena contratta della Figura 22.3, l’endogenità è visualizzata come una covarianza tra la variabile causale misurata X e il disturbo (termine di errore) di Y, indicata con un simbolo specifico. Questo simbolo mostra che c’è una relazione non spiegata tra la causa X e il disturbo associato a Y, suggerendo che X potrebbe non essere una variabile completamente indipendente, come idealmente dovrebbe essere in un modello causale chiaro.\nIl modello nella Figura 22.3 (a) non è identificabile per due ragioni principali:\n\nGradi di libertà negativi (dfM = -1): Questo indica che ci sono più parametri da stimare nel modello rispetto al numero di informazioni (osservazioni) disponibili. In sostanza, il modello sta cercando di “apprendere” troppo da troppo pochi dati, il che lo rende statisticamente non identificabile.\nPercorso di confondimento non chiuso tra X e D: Il percorso di confondimento (o back-door) tra X e D indica che c’è una relazione non controllata o non misurata tra la variabile indipendente X e il disturbo D di Y. Poiché D è trattato come una variabile latente (cioè, una variabile non osservata direttamente), questo percorso non può essere chiuso o controllato nel modello. Ciò significa che non possiamo essere sicuri se la relazione osservata tra X e Y è effettivamente causata da X o se è influenzata da altri fattori non considerati nel modello.\n\nIn sintesi, l’endogenità in questo contesto si riferisce al problema di avere una variabile indipendente (X) che non è veramente indipendente a causa della sua relazione non spiegata con il termine di errore associato alla variabile dipendente (Y), compromettendo così la chiarezza delle relazioni causali nel modello.\n\n\n\n\n\nFigura 22.3: Endogenità in una catena contratta (a). Identificazione del modello controllando un proxy (P) di una causa comune non misurata (b) e attraverso metodi di variabile strumentale (Z), che affrontano anche l’errore di misurazione nella variabile X (c). Tutti i diagrammi sono mostrati in simbolismo compatto. (Figura tratta da Kline (2023))\n\n\nL’endogenità nei modelli parametrici può essere indotta dalle seguenti condizioni:\n\nUna causa comune non misurata di X e Y (cioè, un confonditore).\nErrore di misurazione casuale in X (cioè, \\(r_{XX} &lt; 1.0\\)).\nCausalità reciproca, o X e Y si causano a vicenda (cioè, sono entrambe variabili endogene) in un ciclo di feedback.\nErrori autoregressivi, dove X è una versione ritardata di Y e gli errori persistono tra le due variabili.\nAutoregressione spaziale, che si verifica quando i punteggi di ciascun caso sono influenzati da quelli di casi vicini o adiacenti spazialmente.\n\nNel contesto dei modelli statistici, è possibile affrontare il problema dei confonditori non misurati in due modi principali: attraverso la selezione di covariate appropriate o utilizzando i metodi delle variabili strumentali. Per illustrare, la Figura 22.3 (b) propone l’uso di un proxy (P) che funge da sostituto per un confonditore non misurato che influisce su entrambe le variabili X e Y. In questo contesto, la variabile X è considerata endogena, il che significa che è influenzata dal proxy P (che a sua volta influisce anche su Y), indicando una possibile relazione di causa-effetto tra P e X.\nPer chiarire, consideriamo il seguente esempio. Immaginiamo di essere interessati a studiare l’effetto dello stress sulle prestazioni accademiche degli studenti universitari. In questo esempio, “stress” è la variabile X e “prestazioni accademiche” è la variabile Y. Tuttavia, c’è un potenziale confonditore che potrebbe influenzare sia lo stress sia le prestazioni accademiche, ma che non è stato misurato o non può essere facilmente misurato. Questo confonditore potrebbe essere, ad esempio, il “benessere psicologico generale” degli studenti.\nIn questo caso, un proxy (P) per il benessere psicologico generale potrebbe essere “l’attività fisica regolare”, che è più facilmente misurabile. La ricerca ha mostrato che l’attività fisica regolare può influenzare sia il benessere psicologico generale sia lo stress, rendendola un buon proxy per il nostro confonditore non misurato.\nNel modello, l’attività fisica (il nostro proxy P) presumibilmente influisce sia sulla variabile causale (lo stress) sia sulla variabile di esito (le prestazioni accademiche). Analizzando i dati con questo modello, possiamo cercare di isolare meglio l’effetto dello stress sulle prestazioni accademiche, controllando per l’effetto del benessere psicologico generale tramite il proxy dell’attività fisica. In questo modo, possiamo ottenere una stima più accurata dell’effetto diretto dello stress sulle prestazioni accademiche, riducendo la distorsione potenzialmente causata dal confonditore non misurato.\nI metodi delle variabili strumentali, come mostrato nella Figura 22.3 (c), sono utilizzati per affrontare sia i confonditori non misurati sia gli errori di misurazione nella variabile esogena X. Questo viene fatto sostituendo X con una variabile strumentale XZ in un modello di regressione a due stadi (2SLS). In questo approccio, qualsiasi errore di misurazione casuale in X viene trasferito alla variabile strumentale XZ, seguendo le ipotesi standard dei metodi delle variabili strumentali. È importante notare che, nel pannello (c), la variabile X è considerata endogena, sebbene non tutti i ricercatori scelgano di includere variabili strumentali nei loro diagrammi di modelli statistici. Questo approccio consente di isolare meglio l’effetto di X su Y, controllando per le influenze esterne non misurate e gli errori di misurazione.\nPer chiarire ulteriormente questi concetti, esamineremo separatamente il modello autoregressivo e l’autoregressione spaziale.\n\n22.3.13.1 Modello Autoregressivo\nUn modello autoregressivo è un tipo di modello statistico utilizzato per analizzare dati sequenziali o temporali. In un modello autoregressivo, si prevedono i valori futuri di una variabile basandosi sui suoi valori passati. Questo è particolarmente utile in studi longitudinali o in serie temporali dove si misura la stessa variabile in diversi punti nel tempo.\nNell’esempio della Figura 22.3 (a), immaginiamo che X e Y siano le stesse variabili misurate in due momenti diversi. Ad esempio, X potrebbe essere il livello di ansia di uno studente misurato all’inizio dell’anno scolastico, mentre Y potrebbe essere il livello di ansia dello stesso studente misurato alla fine dell’anno scolastico. In questo caso, stiamo cercando di prevedere i punteggi futuri di ansia (Y) basandoci sui punteggi passati (X).\nUn aspetto importante da considerare è che gli errori nelle misure ripetute (le variazioni nei punteggi che non sono spiegati dal modello) possono essere correlati. Ad esempio, se le misurazioni sono fatte in intervalli temporali ravvicinati, le circostanze o gli stati interni che hanno influenzato la prima misurazione potrebbero ancora essere presenti durante la seconda misurazione.\n\n22.3.13.2 Autoregressione Spaziale\nL’autoregressione spaziale, invece, si riferisce a un modello che considera le correlazioni spaziali tra dati. Questo tipo di analisi è particolarmente rilevante quando si studiano fenomeni geografici o ambientali. Ad esempio, la diffusione di una malattia in diverse località geografiche potrebbe non essere indipendente: le aree vicine geograficamente potrebbero mostrare pattern simili di diffusione della malattia a causa della loro vicinanza.\nIn quest’ultimo caso, non stiamo più parlando di misure ripetute nel tempo sulla stessa unità, ma piuttosto di misure effettuate in diverse unità in un contesto spaziale. Le variabili misurate in diverse località fisiche possono influenzarsi a vicenda, e un modello autoregressivo spaziale cerca di catturare queste interdipendenze.\n\n22.3.14 Modelli con Cause Correlate o Effetti Indiretti\nIl modello parametrico mostrato nella Figura 22.4 (a) suggerisce che la variabile Y sia influenzata da due variabili esogene correlate, X e W. Questo significa che X e W sono due fattori esterni che hanno un impatto su Y e tra loro esiste una relazione di covarianza, ovvero tendono a variare insieme in un certo modo. Tuttavia, il diagramma non spiega il motivo della relazione tra X e W, lasciando la loro interdipendenza non esaminata in termini causali.\n\n\n\n\n\nFigura 22.4: Modelli con cause correlate (a) e sia effetti diretti che indiretti (b). Tutti i diagrammi sono mostrati in simbolismo compatto. (Figura tratta da Kline (2023))\n\n\nNell’analizzare questi dati con un software, si prenderanno in considerazione gli effetti sia di X che di W, tenendo conto della loro covarianza campionaria. Ciò significa che quando si stimano gli impatti di X e W su Y, si aggiusta per il fatto che X e W sono correlate tra loro. Alcuni software, come lavaan, presuppongono automaticamente che tutte le cause esogene misurate che influenzano lo stesso risultato (in questo caso Y) siano correlate. Utilizzando il comando\nY ~ X + W\nin lavaan, si definisce il modello rappresentato nella Figura 22.4 (a), permettendo al software di stimare gli effetti di X e W tenendo conto della loro covarianza osservata. Questo comando specifica inoltre che le varianze di X, W e il disturbo associato a Y sono tutti considerati parametri liberi da stimare.\nSe, invece, si ipotizza che le variabili esogene X e W siano indipendenti, ovvero che non ci sia una covarianza tra di loro, si può usare un comando aggiuntivo in lavaan\nX ~~ 0*W\nper impostare la covarianza tra X e W a zero. Questo comando mantiene le varianze di X e W come parametri liberi, ma specifica che non c’è una relazione di covarianza diretta tra queste due variabili. In questo modo, il modello considererà X e W come influenze separate e indipendenti su Y.\nNel modello presentato nella Figura 22.4 (a), è importante notare come vengano trattate le interazioni tra le variabili causali X e W. In questo specifico caso, si presume che non ci sia alcuna interazione tra X e W; in altre parole, l’effetto di X sulla variabile di esito Y si assume essere costante a prescindere dai diversi livelli di W, e viceversa. Questa assunzione implica che l’effetto di X su Y è indipendente da W, e l’effetto di W su Y è indipendente da X.\nIn termini di modellazione, questo significa che stiamo considerando una causalità incondizionata, dove l’effetto di una causa su un esito è costante e non influenzato da altre variabili nel modello. Il modello non prevede, quindi, che l’effetto di X su Y cambi in funzione dei diversi livelli di W. Questo è in contrasto con l’ipotesi di causalità condizionale, dove gli effetti di una variabile su un’altra possono variare in base al livello di una terza variabile. In un modello di causalità condizionale, ad esempio, si potrebbe ipotizzare che l’effetto di X su Y vari a seconda dei diversi livelli di W.\nIn sintesi, la Figura 22.4 (a) delinea un modello dove le relazioni causali tra X, W e Y sono considerate fisse e non influenzate da potenziali interazioni tra X e W. Questo tipo di modellazione fornisce una visione semplificata delle relazioni causali, che potrebbe essere appropriata in determinate circostanze, ma non tiene conto di possibili dinamiche più complesse tra le variabili.\nÈ fondamentale riconoscere che le ripercussioni degli errori di misurazione in modelli che includono cause correlate sono notevolmente intricate e imprevedibili. Questa complessità deriva principalmente dalla natura del bias che può emergere a seguito di errori di misurazione. In particolare, il bias introdotto da questi errori può manifestarsi in modi diversi, assumendo una forma sia negativa che positiva. Tale variazione dipende da diversi fattori, tra cui se l’errore di misurazione è distribuito in maniera uniforme tra molteplici variabili predittive o se è presente sia nelle variabili predittive che nella variabile di esito. Un altro elemento influente è la natura delle covarianze campionarie tra tutte le variabili coinvolte nel modello.\nData questa complessità, la capacità di modellare esplicitamente gli errori di misurazione all’interno dei modelli SEM rappresenta un vantaggio significativo. Questo approccio permette una maggiore precisione nell’analisi, consentendo di tenere conto delle varie modalità in cui gli errori di misurazione possono influenzare i risultati. La modellazione esplicita degli errori di misurazione in SEM offre quindi la possibilità di ottenere stime più accurate e affidabili, mitigando il rischio di trarre conclusioni errate a causa di bias non riconosciuti o non gestiti adeguatamente.\nNella Figura 22.4 (b), il modello mostra come la variabile X abbia sia effetti diretti che indiretti sulla variabile di esito Y. L’effetto indiretto segue il percorso X → M → Y, dove M funge da variabile intermedia o mediatrice. Questo significa che M è il canale attraverso il quale gli effetti di X sono trasmessi a Y. In questo modello, M è una variabile endogena, nel senso che è influenzata da X (indicato dal percorso X → M), e allo stesso tempo agisce come una variabile causale nei confronti di Y (come indicato da M → Y).\nLa variabile M assume un doppio ruolo in termini di affidabilità e precisione della misurazione. Da una parte, essendo un esito di X, M è soggetta a disturbi, che includono potenziali errori di misurazione. Dall’altra, nel suo ruolo di causa per Y insieme a X, si presume nelle analisi di regressione che sia X che M siano prive di errore di misurazione. Questa assunzione non presenta problemi se l’affidabilità delle misure su M è elevata, ossia se i punteggi di M sono accurati e consistenti.\nIn aggiunta, il modello descritto nella Figura 22.4 (b) include tre ipotesi importanti:\n\nX ha un effetto diretto su Y, oltre al suo effetto indiretto tramite M.\nNon ci sono interazioni negli effetti lineari di X e M su Y, il che significa che l’effetto di X su Y è lo stesso a prescindere dai livelli di M, e viceversa.\nIl modello non omette confonditori potenzialmente importanti tra le coppie di variabili X, M e Y. In altre parole, non ci sono fattori esterni non considerati nel modello che potrebbero influenzare le relazioni tra queste tre variabili.\n\nIn sintesi, la Figura 22.4 (b) presenta un modello in cui X influisce su Y sia direttamente che indirettamente attraverso M, e queste relazioni sono considerate prive di interazioni complesse o di confonditori non rilevati.\nLa gestione degli errori di misurazione e dei confonditori non considerati in modelli che includono effetti causali indiretti rappresenta una sfida notevole, poiché i loro effetti sulle stime possono essere complessi e non sempre prevedibili. Per esemplificare, consideriamo il modello della Figura 22.4 (b) dove si analizza l’effetto indiretto di X su Y attraverso la variabile intermedia M.\nSe assumiamo che non ci siano errori di misurazione nella variabile causale X, qualsiasi errore di misurazione presente nella variabile intermedia M può introdurre un bias negativo nelle stime dell’effetto indiretto di X su Y. Ciò significa che l’effetto indiretto potrebbe essere sottostimato a causa dell’errore in M. D’altro canto, se non si tiene conto dei confonditori tra M e Y, cioè se ci sono variabili o fattori non considerati che influenzano sia M che Y, ciò può portare a un bias positivo, sovrastimando l’effetto indiretto.\nQuando entrambe queste situazioni – errori di misurazione in M e confonditori tra M e Y – si verificano contemporaneamente, le conseguenze sulle stime dell’effetto indiretto possono variare ampiamente. Potrebbe verificarsi una sovrastima, una sottostima o, in rari casi, nessun bias significativo. Studi di simulazione hanno rivelato che tentare di correggere solo una fonte di bias (come l’errore di misurazione in M) in presenza di entrambi i tipi di bias può addirittura aggravare il problema, portando a stime più distorte rispetto a quelle che non tengono conto di alcun bias.\nIn sintesi, la valutazione accurata dell’effetto indiretto in un modello che comprende una variabile intermedia richiede un’attenta considerazione sia degli errori di misurazione che dei confonditori potenziali, poiché la loro interazione può influenzare in modi complessi e talvolta inaspettati la validità delle stime.\n\n22.3.15 Modelli Ricorsivi, Non Ricorsivi e Parzialmente Ricorsivi\nTutti i modelli di percorso parametrici più complessi possono essere “assemblati” a partire dai modelli elementari mostrati nelle figure precedenti. Ci sono due tipi fondamentali di modelli: ricorsivi e non ricorsivi. I modelli ricorsivi hanno due caratteristiche essenziali: tutti gli effetti causali sono unidirezionali e i loro disturbi sono indipendenti. La Figura 22.5 (a) è un esempio di un modello ricorsivo (Tutti i modelli considerati finora sono ricorsivi.)\nI modelli non ricorsivi, invece, hanno cicli causali (feedback) in cui ≥ 2 variabili endogene sono specificate come cause ed effetti l’una dell’altra, direttamente o indirettamente. Nella loro forma non parametrica, corrispondono a grafi ciclici diretti. La Figura 22.5 (b) è un esempio di un modello parametrico non ricorsivo con causazione reciproca rappresentata come\n\\(Y1 \\overset{\\rightarrow}{\\underset{\\leftarrow}{}} Y2,\\)\nindicando che le variabili Y1 e Y2 hanno effetti simultanei l’una sull’altra.\n\n\n\n\n\nFigura 22.5: Esempi di modelli ricorsivi, non ricorsivi e parzialmente ricorsivi con due diversi schemi di correlazione degli errori. Tutti i diagrammi sono mostrati in simbolismo compatto. (Figura tratta da Kline (2023))\n\n\nI modelli che includono cicli causali possono presentare, o meno, covarianze tra i loro termini di disturbo. La presenza di errori correlati in questi modelli implica l’esistenza di ipotesi su cause comuni non misurate che influenzano le variabili in questione.\nAd esempio, nel modello rappresentato nella Figura 22.5 (b), le variabili Y1 e Y2 sono definite come cause reciproche, ovvero ognuna influisce sull’altra. In aggiunta a ciò, se nel modello è specificata una covarianza tra i termini di disturbo \\(D_1\\) e \\(D_2\\), ciò suggerisce che Y1 e Y2 condividono almeno una causa comune non misurata. In altre parole, l’ipotesi è che esistano fattori non osservati che influenzano entrambe le variabili, \\(Y1\\) e \\(Y2\\), e questa influenza comune si manifesta attraverso la covarianza tra i loro termini di disturbo.\nEsiste anche un altro tipo di modello di percorso, quello che ha effetti unidirezionali e covarianze dei disturbi. I modelli parzialmente ricorsivi con un pattern di correlazioni dei disturbi senza “archi” possono essere trattati nell’analisi proprio come i modelli ricorsivi. Un pattern senza “archi” significa che gli errori correlati sono limitati a coppie di variabili endogene senza effetti diretti tra di loro, come Y1 e Y2 nella Figura 22.5 (c).\nI modelli parzialmente ricorsivi che presentano un pattern di correlazioni dei disturbi caratterizzato dalla presenza di “archi” richiedono un trattamento analitico simile a quello dei modelli non ricorsivi. Un pattern con “archi” si verifica quando esiste una covarianza tra i termini di disturbo di due variabili endogene che sono collegate da un effetto diretto. Ad esempio, nella Figura 22.5 (d), le variabili Y1 e Y2 sono collegate da un effetto diretto e presentano una covarianza tra i loro disturbi \\(D_1\\) e \\(D_2\\).\nLa presenza di un effetto diretto insieme a disturbi correlati tra due variabili crea un percorso di confondimento nel modello. Un percorso di confondimento è una via attraverso la quale può fluire l’influenza causale indiretta, potenzialmente distorcendo l’interpretazione dei rapporti causali diretti. In questi casi, la selezione di covariate (variabili aggiuntive che potrebbero spiegare parte della relazione osservata) non è sufficiente per “chiudere” o eliminare questo percorso di confondimento. Pertanto, questi modelli richiedono un’attenzione particolare nell’analisi per garantire che le stime degli effetti causali siano accurate e non siano influenzate in modo improprio da questi percorsi di confondimento.\nI modelli ricorsivi e quelli parzialmente ricorsivi che non includono cicli causali possono essere efficacemente rappresentati tramite grafi aciclici diretti (DAG). Questo tipo di rappresentazione grafica implica che è possibile applicare tutte le regole di identificazione grafica esposte in questo capitolo. Nei DAG, le relazioni causali sono rappresentate come flussi unidirezionali senza cicli, rendendo più chiaro e diretto l’analisi delle relazioni tra le variabili.\nD’altra parte, i modelli non ricorsivi che includono cicli causali, come quello illustrato nella Figura 22.5 (b), sono rappresentati da grafi ciclici diretti. In questi grafi, le variabili possono influenzarsi a vicenda in un ciclo continuo, creando una struttura più complessa. A causa di questa complessità, le regole di identificazione grafica per i grafi ciclici diretti non sono sviluppate quanto quelle per i DAG. Questo significa che analizzare e interpretare i modelli non ricorsivi con cicli causali è più complesso e richiede l’uso di approcci analitici più avanzati o specifici per gestire correttamente le relazioni cicliche tra le variabili.\nPossiamo stabilire una regola generale per i modelli di percorso parametrici: i modelli ricorsivi o parzialmente ricorsivi, che presentano schemi di covarianze dei disturbi privi di “archi” e che soddisfano due condizioni specifiche, sono considerati identificati. Queste condizioni sono: (1) i gradi di libertà del modello (dfM) devono essere maggiori o uguali a zero e (2) ogni variabile non misurata, inclusi i termini di errore, deve essere associata a una scala metrica.\nInoltre, i modelli di equazioni strutturali che sono identificati e hanno un numero di osservazioni uguale al numero dei parametri liberi (dfM = 0) sono classificati come “appena identificati”. Al contrario, i modelli con più osservazioni rispetto ai parametri liberi (dfM &gt; 0) sono considerati “sovraidentificati”.\nUn modello di equazioni strutturali può risultare sotto-identificato in due modi distinti: (1) se dfM è inferiore a zero, oppure (2) se, pur avendo un dfM maggiore o uguale a zero, alcuni parametri liberi rimangono sotto-identificati perché non vi sono sufficienti informazioni per la loro stima, anche se altri parametri all’interno dello stesso modello sono identificati. Nel secondo scenario, l’intero modello è considerato non identificato, anche se dfM è maggiore o uguale a zero. In generale, un modello si considera sotto-identificato quando non è possibile stimare in modo univoco tutti i suoi parametri liberi.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#analisi-dei-percorsi-e-regressione-bivariata",
    "href": "chapters/path_analysis/01_path_analysis.html#analisi-dei-percorsi-e-regressione-bivariata",
    "title": "22  Analisi dei percorsi",
    "section": "\n22.4 Analisi dei percorsi e regressione bivariata",
    "text": "22.4 Analisi dei percorsi e regressione bivariata\nCominciamo esaminando l’analisi dei percorsi partendo dall’esempio più semplice, ovvero il modello di regressione lineare. Il modello di regressione bivariata si esprime tramite l’equazione seguente:\n\\[y_1 = b_0 + b_1 x_1 + \\epsilon_1,\\]\ndove \\(y\\) rappresenta la variabile dipendente, \\(b_0\\) rappresenta l’intercetta, \\(b_1\\) rappresenta la pendenza della retta di regressione, \\(x\\) è la variabile indipendente e \\(\\epsilon\\) è il termine di errore.\nNell’ambito della descrizione delle relazioni tra variabili manifeste e latenti, si adotta spesso la notazione LISREL. In questa notazione, il modello presentato in precedenza può essere espresso come segue:\n\\[y_1 = \\alpha + \\gamma x_1 + \\zeta_1,\\]\ndove:\n\n\n\\(x_1\\): variabile esogena singola,\n\n\\(y_1\\): variabile endogena singola,\n\n\\(\\alpha\\): intercetta di \\(y_1\\),\n\n\\(\\gamma_1\\): coefficiente di regressione,\n\n\\(\\zeta_1\\): termine di errore di \\(y_1\\),\n\n\\(\\phi\\): varianza o covarianza della variabile esogena,\n\n\\(\\psi\\): varianza o covarianza residuale della variabile endogena.\n\nIl diagramma di percorso per il modello di regressione bivariata è illustrato nella Figura 22.6.\n\n\n\n\n\nFigura 22.6: Diagramma di percorso per il modello di regressione bivariato.\n\n\nFacciamo un esempio numerico. Simuliamo tre variabili: x1, x2, y.\n\nset.seed(42)\nn &lt;- 100\nx1 &lt;- rnorm(n, 90, 20)\nx2 &lt;- x1 + rnorm(n, 0, 30)\ny &lt;- 25 + 0.5 * x1 + 1.0 * x2 + rnorm(n, 0, 30)\n\ndat &lt;- data.frame(\n    y, x1, x2\n)\n\ncor(dat) |&gt;\n    round(2)\n#&gt;       y   x1   x2\n#&gt; y  1.00 0.55 0.80\n#&gt; x1 0.55 1.00 0.62\n#&gt; x2 0.80 0.62 1.00\n\nConsideriamo la relazione tra x1 (variabile endogena) e y (variabile endogena). In R possiamo adattare ai dati un modello di regressione mediante la funzione lm.\n\nm1a &lt;- lm(y ~ x1, data = dat)\nsummary(m1a)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y ~ x1, data = dat)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -82.46 -29.54  -3.44  29.20 122.23 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)\n#&gt; (Intercept)   37.597     18.984    1.98     0.05\n#&gt; x1             1.329      0.204    6.51  3.3e-09\n#&gt; \n#&gt; Residual standard error: 42.3 on 98 degrees of freedom\n#&gt; Multiple R-squared:  0.302,  Adjusted R-squared:  0.295 \n#&gt; F-statistic: 42.4 on 1 and 98 DF,  p-value: 3.25e-09\n\nUsiamo ora lavaan per adattare lo stesso modello ai dati.\n\nm1b &lt;- \"\n    y ~ 1 + x1\n\"\nfit1b &lt;- sem(m1b, data = dat)\nparameterEstimates(fit1b) \n#&gt;   lhs op rhs     est      se    z pvalue ci.lower ci.upper\n#&gt; 1   y ~1       37.60  18.794 2.00  0.045    0.763    74.43\n#&gt; 2   y  ~  x1    1.33   0.202 6.57  0.000    0.933     1.73\n#&gt; 3   y ~~   y 1754.10 248.067 7.07  0.000 1267.897  2240.30\n#&gt; 4  x1 ~~  x1  429.43   0.000   NA     NA  429.432   429.43\n#&gt; 5  x1 ~1       90.65   0.000   NA     NA   90.650    90.65\n\nL’intercetta di y ~1 (37.597) e il coefficiente di regressione di y ~ x1 (1.329) corrispondono all’output di lm() con piccoli errori di arrotondamento. L’intercetta per x1 ~1 (90.650) e la sua varianza x1 ~~ x1 (429.432) descrivono una media ed una varianza esogena e corrispondono alla media e alla varianza univariate:\n\nmean(dat$x1)\n#&gt; [1] 90.7\n\n\nvar(dat$x1) * (length(dat$x1) - 1) / length(dat$x1)\n#&gt; [1] 429\n\nLa varianza residua di y, y ~~ y corrisponde alla quota della varianza osservata della variabile y che non è spiegata dalla relazione lineare su x1:\n\nvar(dat$y) * 99 / 100 - (1.3286 * 429.432 * 1.3286)\n#&gt; [1] 1754\n\nLa funzione semPaths consente di creare un diagramma di percorso a partire dall’oggetto creato da sem.\n\nsemPlot::semPaths(\n    fit1b,\n    layout = \"tree\", sizeMan = 7, sizeInt = 5, style = \"ram\", \n    residuals = TRUE, intAtSide = FALSE, edge.label.cex = 1.15,\n    whatLabels = \"est\", nCharNodes = 0, normalize = FALSE\n)",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#analisi-dei-percorsi-e-regressione-multipla",
    "href": "chapters/path_analysis/01_path_analysis.html#analisi-dei-percorsi-e-regressione-multipla",
    "title": "22  Analisi dei percorsi",
    "section": "\n22.5 Analisi dei percorsi e regressione multipla",
    "text": "22.5 Analisi dei percorsi e regressione multipla\nLa regressione semplice è limitata a una sola variabile esogena. Nella pratica, un ricercatore può essere interessato a studiare come un gruppo di variabili esogene possano predire una variabile di esito. Supponiamo di avere ancora una sola variabile di esito endogena ma due predittori esogeni; questo caso è noto come regressione multipla:\n\\[\ny_1 = \\alpha_1 + \\gamma_1 x_1 + \\gamma_2 x_2 + \\zeta_1.\n\\]\nIl diagramma di percorso mostra la relazione tra tutte le variabili, comprendendo anche i fattori di disturbo, e fornisce dunque la rappresentazione grafica dell’equazione precedente.\n\n\n\n\n\nFigura 22.7: Diagramma di percorso per il modello di regressione multipla.\n\n\nI coefficienti di percorso associati alle frecce orientate esprimono la portata del nesso causale e corrispondono ai pesi beta (ovvero ai coefficienti parziali di regressione standardizzati). Le frecce non orientate esprimono la portata della pura associazione tra variabili e dunque corrispondono alle correlazioni/covarianze.\nIn un diagramma di percorso, il numero di equazioni corrisponde al numero di variabili endogene del modello. Nel caso specifico, poiché vi è una sola variabile endogena (ovvero \\(y\\)), esiste un’unica equazione che descrive le relazioni causalitiche interne al path diagram. All’interno di ciascuna equazione, inoltre, il numero di termini corrisponde al numero di frecce orientate che puntano verso la variabile endogena. Nell’esempio sopra citato, pertanto, la sola equazione del modello contiene tre termini, ciascuno associato ad una freccia orientata.\nUsando lm otteniamo la seguente stima dei coefficienti:\n\nm2a &lt;- lm(y ~ 1 + x1 + x2, data = dat)\nfit2a &lt;- summary(m2a) \n\nGli stessi risultati si ottengono con lavaan.\n\nm2b &lt;- \"\n    y ~ 1 + x1 + x2\n    x1 ~~ x1\n    x2 ~~ x2\n    x1 ~~ x2\n\"\n\n\nfit2b &lt;- sem(m2b, data = dat)\n\n\nparameterEstimates(fit2b)\n#&gt;   lhs op rhs      est      se     z pvalue ci.lower ci.upper\n#&gt; 1   y ~1       44.454  13.457  3.30  0.001   18.078   70.830\n#&gt; 2   y  ~  x1    0.199   0.185  1.08  0.282   -0.164    0.562\n#&gt; 3   y  ~  x2    1.085   0.111  9.78  0.000    0.868    1.303\n#&gt; 4  x1 ~~  x1  429.432  60.731  7.07  0.000  310.402  548.462\n#&gt; 5  x2 ~~  x2 1192.840 168.693  7.07  0.000  862.208 1523.472\n#&gt; 6  x1 ~~  x2  446.927  84.379  5.30  0.000  281.546  612.307\n#&gt; 7   y ~~   y  896.963 126.850  7.07  0.000  648.342 1145.584\n#&gt; 8  x1 ~1       90.650   2.072 43.74  0.000   86.589   94.712\n#&gt; 9  x2 ~1       88.026   3.454 25.49  0.000   81.257   94.795\n\nEsaminiamo il diagramma di percorso.\n\nsemPlot::semPaths(\n    fit2b,\n    layout = \"tree\", sizeMan = 7, sizeInt = 5, style = \"ram\",\n    residuals = TRUE, intAtSide = FALSE, edge.label.cex = 1.15,\n    whatLabels = \"est\", nCharNodes = 0, normalize = FALSE\n)",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#effetti-diretti-e-indiretti",
    "href": "chapters/path_analysis/01_path_analysis.html#effetti-diretti-e-indiretti",
    "title": "22  Analisi dei percorsi",
    "section": "\n22.6 Effetti diretti e indiretti",
    "text": "22.6 Effetti diretti e indiretti\nL’analisi del percorso offre un metodo essenziale per distinguere tra diverse tipologie di effetti che influenzano le variabili in esame: l’effetto diretto, l’effetto indiretto e l’effetto totale. Gli effetti diretti rappresentano l’influenza che una variabile esercita su un’altra senza mediazione di altre variabili intermedie. Gli effetti indiretti, invece, operano attraverso l’intermediazione di almeno una variabile aggiuntiva nel processo. L’effetto totale è la somma cumulativa degli effetti diretti e indiretti.\nNella Figura 22.8, la variabile \\(y_1\\) esercita un effetto diretto sulla variabile \\(y_2\\). Allo stesso tempo, \\(y_1\\) produce un effetto indiretto sulla variabile \\(y_3\\), poiché non esiste una connessione causale diretta tra \\(y_1\\) e \\(y_3\\). Nel contesto rappresentato, la variabile \\(y_1\\) agisce come variabile esogena, mentre le variabili \\(y_2\\) e \\(y_3\\) fungono da variabili endogene.\n\n\n\n\n\nFigura 22.8: Diagramma di percorso per un modello a catena.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#le-regole-di-wright",
    "href": "chapters/path_analysis/01_path_analysis.html#le-regole-di-wright",
    "title": "22  Analisi dei percorsi",
    "section": "\n22.7 Le regole di Wright",
    "text": "22.7 Le regole di Wright\nL’obiettivo primario dell’analisi del percorso consiste nella decomposizione della correlazione (o della covarianza) in base alla somma dei vari percorsi (diretti e indiretti) che collegano due variabili mediante coefficienti noti come “path coefficients.” Utilizzando il diagramma del percorso, Sewall Wright (1921, 1934) formulò le regole che, tramite le “tracing rules,” stabiliscono il collegamento tra le correlazioni (o covarianze) delle variabili e i parametri del modello. Le tracing rules si possono esprimere nei seguenti termini:\n\nÈ possibile procedere in avanti lungo una freccia e poi a ritroso, seguendo la direzione della freccia, ma non è permesso muoversi in avanti e poi tornare indietro.\nUn percorso composto non deve attraversare più di una volta la stessa variabile, cioè non possono esserci cicli.\nUn percorso non può contenere più di una linea curva.\n\nIl termine “percorso” fa riferimento al tracciato che connette due variabili e si compone di sequenze di frecce unidirezionali e curve non direzionali. A ciascun percorso valido (cioè conforme alle regole di Wright) viene assegnato un valore numerico che rappresenta il prodotto dei coefficienti presenti lungo il percorso stesso. I coefficienti di percorso possono essere coefficienti parziali di regressione standardizzati se il legame è unidirezionale, oppure coefficienti di correlazione se il legame è bidirezionale.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#scomposizione-delle-correlazionicovarianze",
    "href": "chapters/path_analysis/01_path_analysis.html#scomposizione-delle-correlazionicovarianze",
    "title": "22  Analisi dei percorsi",
    "section": "\n22.8 Scomposizione delle correlazioni/covarianze",
    "text": "22.8 Scomposizione delle correlazioni/covarianze\nIl principio fondamentale è stato formulato da Sewall Wright (1934) nel seguente modo:\n\nOgni correlazione tra variabili in una rete di relazioni sequenziali può essere analizzata nei contributi provenienti da tutti i percorsi (diretti o attraverso fattori comuni) con i quali le due variabili sono connesse. Ogni contributo ha un valore pari al prodotto dei coefficienti relativi ai percorsi elementari. Se sono presenti correlazioni residue (rappresentate da frecce bidirezionali), uno (ma mai più di uno) dei coefficienti moltiplicati per ottenere il contributo del percorso di connessione può essere un coefficiente di correlazione. Gli altri sono tutti coefficienti di percorso.\n\nDa questo principio possiamo derivare la regola di scomposizione della correlazione: la correlazione o covarianza tra due variabili può essere scomposta in un numero di termini uguale al numero di percorsi che le collegano. Ogni termine è ottenuto dal prodotto dei coefficienti associati alle variabili lungo il percorso. In altre parole, è possibile decomporre la correlazione o la covarianza tra due variabili in tanti contributi quanti sono i percorsi possibili che collegano le due variabili.\n\n22.8.1 Scomposizione della varianza\nLa decomposizione della varianza di una variabile endogena può essere affrontata attraverso una suddivisione in due componenti: una componente spiegata, attribuibile alle variabili che esercitano un’influenza causale su di essa, e una componente non spiegata. La componente spiegata della varianza deriva dall’aggregazione degli effetti delle diverse variabili che sono connessi alla variabile endogena, rispettando le regole di tracciamento definite da Wright. Il numero di addendi corrisponde al numero di percorsi che collegano la variabile endogena a se stessa. In tal modo, la varianza spiegata rappresenta la parte della varianza totale della variabile endogena che può essere attribuita alle influenze delle variabili correlate attraverso i percorsi definibili all’interno del modello.\n\n22.8.2 Relazioni tra variabili endogene e esogene\nComplessivamente, i concetti di varianza, covarianza e correlazione informano direttamente il calcolo dei coefficienti di percorso in un path diagram secondo le seguenti “8 regole dei coefficienti di percorso”.\n\nRegola 1: Le relazioni non specificate tra le variabili esogene sono semplicemente le loro correlazioni bivariate.\nRegola 2: Quando due variabili sono collegate da un singolo percorso, il coefficiente di quel percorso è il coefficiente di regressione.\nRegola 3: La forza di un percorso composto (che include più collegamenti) è il prodotto dei coefficienti individuali.\nRegola 4: Quando le variabili sono collegate da più di un percorso, ciascun percorso è il coefficiente di regressione “parziale”.\nRegola 5: Gli errori sulle variabili endogene si riferiscono alle correlazioni o varianze non spiegate che derivano dalle variabili non misurate.\nRegola 6: Le correlazioni non analizzate (residui) tra due variabili endogene sono le loro correlazioni parziali.\nRegola 7: L’effetto totale che una variabile ha su un’altra è la somma dei suoi effetti diretti e indiretti.\nRegola 8: L’effetto totale (compresi i percorsi non diretti) è equivalente alla correlazione totale.\n\nEsempio. Consideriamo nuovamente il modello di regressione multipla con due variabili esogene e una sola variabile endogena che è stato presentato sopra.\nLa la covarianza tra y e x1\n\ncov(dat$y, dat$x1) * 99 / 100\n#&gt; [1] 571\n\npuò essere ricavata usando le regole di Wright nel modo seguente:\n\n0.199 * 429.43 + 1.085 * 446.93\n#&gt; [1] 570\n\nLa quota di varianza non spiegata della variabile endogena è:\n\n(var(dat$y) * 99 / 100) - (\n    0.199^2 * 429.43 + (1.085)^2 * 1192.84 + 2 * (0.199 * 1.085 * 446.93)\n)\n#&gt; [1] 898",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#oltre-la-regressione-multipla",
    "href": "chapters/path_analysis/01_path_analysis.html#oltre-la-regressione-multipla",
    "title": "22  Analisi dei percorsi",
    "section": "\n22.9 Oltre la Regressione Multipla",
    "text": "22.9 Oltre la Regressione Multipla\nPer comprendere come applicare la path analysis a strutture più complesse rispetto alla regressione multipla, esaminiamo uno studio esemplare di Barbeau et al. (2019). Questo studio illustra l’utilizzo della path analysis attraverso due modelli: uno di mediazione e un altro più articolato.\nL’analisi di mediazione consente di studiare come una variabile indipendente (\\(X\\)) influenzi una variabile dipendente (\\(Y\\)) tramite una variabile intermedia, chiamata mediatore (\\(M\\)). Questo approccio offre una visione più approfondita rispetto alla regressione multipla, esplorando i meccanismi sottostanti le relazioni tra le variabili.\nUn esempio applicativo deriva dalla Self-Determination Theory (SDT) di Deci e Ryan (2000), che esamina come la soddisfazione e la frustrazione dei bisogni psicologici fondamentali influenzino i sintomi bulimici in giovani donne adulte. Secondo la SDT, i bisogni di autonomia, competenza e relazionalità sono cruciali per il benessere psicologico. La loro frustrazione può promuovere comportamenti disfunzionali, mentre la loro soddisfazione agisce da fattore protettivo (Vansteenkiste & Ryan, 2013).\nIl modello proposto è costituito dalle seguenti componenti.\n\n\nFrustrazione dei bisogni psicologici:\n\nFavorisce l’approvazione degli ideali culturali di magrezza.\nÈ associata a una maggiore inflessibilità negli schemi corporei.\n\n\n\nApprovazione degli ideali di magrezza:\n\nPredice l’inflessibilità negli schemi corporei.\n\n\n\nInflessibilità degli schemi corporei:\n\nMedia l’effetto della frustrazione dei bisogni sui sintomi bulimici.\n\n\n\nQuesto modello concettuale suggerisce che la frustrazione dei bisogni riduca le risorse psicologiche necessarie per resistere agli ideali culturali disfunzionali, aumentando la vulnerabilità psicologica e il rischio di sintomi patologici (Pelletier & Dion, 2007).\nIl modello teorico può essere formalizzato attraverso tre percorsi principali:\n\n\nEffetto di \\(X\\) su \\(M\\) (coefficiente \\(a\\)): Misura l’influenza della frustrazione dei bisogni (\\(X\\)) sull’approvazione degli ideali culturali (\\(M\\)).\n\nEffetto di \\(M\\) su \\(Y\\) (coefficiente \\(b\\)): Valuta come \\(M\\) influenzi l’inflessibilità degli schemi corporei e i sintomi bulimici (\\(Y\\)).\n\nEffetto diretto di \\(X\\) su \\(Y\\) (coefficiente \\(c'\\)): Analizza l’effetto residuo di \\(X\\) su \\(Y\\), indipendentemente da \\(M\\).\n\nMatematicamente, il modello è descritto dalle seguenti equazioni di regressione:\n\n\nEquazione per il mediatore (\\(M\\)):\n\\[\nM = a_0 + a \\cdot X + e_M ,\n\\]\ndove \\(a\\) è l’effetto di \\(X\\) su \\(M\\); \\(e_M\\) è il termine di errore.\n\n\nEquazione per la variabile dipendente (\\(Y\\)):\n\\[\nY = b_0 + b \\cdot M + c' \\cdot X + e_Y ,\n\\]\ndove \\(b\\) è l’effetto di \\(M\\) su \\(Y\\); \\(c'\\) è l’effetto diretto di \\(X\\) su \\(Y\\).\n\n\nNel modello di mediazione, è possibile distinguere tre tipi di effetti, ciascuno quantificabile in modo specifico.\n\n\nEffetto diretto (\\(c'\\)): rappresenta l’influenza della variabile indipendente (\\(X\\)) sulla variabile dipendente (\\(Y\\)) che non passa attraverso il mediatore (\\(M\\)).\n\nEffetto indiretto (\\(a \\cdot b\\)): misura il contributo della variabile indipendente (\\(X\\)) alla variabile dipendente (\\(Y\\)) mediato dal percorso attraverso il mediatore (\\(M\\)).\n\n\nEffetto totale (\\(c' + a \\cdot b\\)): corrisponde alla somma degli effetti diretto e indiretto, rappresentando l’influenza complessiva di \\(X\\) su \\(Y\\).\n\nQui, \\(a\\), \\(b\\) e \\(c'\\) sono i coefficienti stimati nel modello di path analysis.\nLo studio di Barbeau et al. (2019) utilizza un campione di 192 partecipanti. Sono stati utilizzati i seguenti strumenti:\n\n\nFrustrazione e soddisfazione dei bisogni: Basic Psychological Needs Satisfaction and Frustration Scale (Chen et al., 2015).\n\nApprovazione degli ideali di magrezza: Endorsement of Societal Beliefs Related to Thinness and Obesity (Boyer, 1991).\n\nInflessibilità degli schemi corporei: Body Image-Acceptance and Action Questionnaire (Sandoz et al., 2013).\n\nSintomi bulimici: Eating Disorders Inventory-2 (Garner, 1991).\n\nLa matrice di covarianza delle variabili è rappresentata come segue:\n\nupper &lt;- '\n  1 0.44 -0.41 0.55 0.63\n  1 -0.37 0.45 0.44\n  1 -0.71 -0.39\n  1 0.47\n  1\n'\ndat_cov &lt;- lavaan::getCov(\n    upper,\n    lower = FALSE,\n    names = c(\"BFLX\", \"END\", \"MNS\", \"MNF\", \"BULS\")\n)\ndat_cov\n#&gt;       BFLX   END   MNS   MNF  BULS\n#&gt; BFLX  1.00  0.44 -0.41  0.55  0.63\n#&gt; END   0.44  1.00 -0.37  0.45  0.44\n#&gt; MNS  -0.41 -0.37  1.00 -0.71 -0.39\n#&gt; MNF   0.55  0.45 -0.71  1.00  0.47\n#&gt; BULS  0.63  0.44 -0.39  0.47  1.00\n\nNell’esempio, BFLX (Body Inflexibility) è la variabile endogena, MNF (Mean Need Frustration) è la variabile esogena, ed END (Endorsement of Societal Beliefs about Thinness and Obesity) è la variabile mediatrice.\nBarbeau et al. (2019) hanno utilizzato Mplus per stimare i coefficienti di percorso, ottenendo \\(a = 0.37\\), \\(b = 0.29\\) e \\(c = 0.34\\). Esamineremo il medesimo modello utilizzando il pacchetto lavaan in R.\nIl modello di mediazione viene definito come segue:\n\nmod &lt;- \"\n  # Effetto diretto\n  BFLX ~ c*MNF\n  # Mediatore\n  BFLX ~ b*END\n  END ~ a*MNF\n\n  # Effetto indiretto\n  ab := a*b\n  # Effetto totale\n  total := c + (a*b)\n\"\n\nAdattiamo il modello ai dati osservati, utilizzando una matrice di covarianza e un campione di 192 partecipanti:\n\nfit &lt;- sem(\n    mod,\n    sample.cov = dat_cov,\n    sample.nobs = 192\n)\n\nI risultati vengono analizzati con il seguente comando:\n\nsummary(\n  fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE\n) \n#&gt; lavaan 0.6-19 ended normally after 1 iteration\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                         5\n#&gt; \n#&gt;   Number of observations                           192\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                 0.000\n#&gt;   Degrees of freedom                                 0\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               125.849\n#&gt;   Degrees of freedom                                 3\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    1.000\n#&gt;   Tucker-Lewis Index (TLI)                       1.000\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)               -480.945\n#&gt;   Loglikelihood unrestricted model (H1)       -480.945\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                 971.890\n#&gt;   Bayesian (BIC)                               988.178\n#&gt;   Sample-size adjusted Bayesian (SABIC)        972.339\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.000\n#&gt;   90 Percent confidence interval - lower         0.000\n#&gt;   90 Percent confidence interval - upper         0.000\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                       NA\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                       NA\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.000\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Regressions:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   BFLX ~                                                                \n#&gt;     MNF        (c)    0.441    0.065    6.769    0.000    0.441    0.441\n#&gt;     END        (b)    0.241    0.065    3.702    0.000    0.241    0.241\n#&gt;   END ~                                                                 \n#&gt;     MNF        (a)    0.450    0.064    6.982    0.000    0.450    0.450\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .BFLX              0.648    0.066    9.798    0.000    0.648    0.651\n#&gt;    .END               0.793    0.081    9.798    0.000    0.793    0.797\n#&gt; \n#&gt; R-Square:\n#&gt;                    Estimate\n#&gt;     BFLX              0.349\n#&gt;     END               0.203\n#&gt; \n#&gt; Defined Parameters:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;     ab                0.109    0.033    3.271    0.001    0.109    0.109\n#&gt;     total             0.550    0.060    9.125    0.000    0.550    0.550\n\nGeneriamo un diagramma per visualizzare il modello stimato:\n\nsemPlot::semPaths(\n    fit,\n    layout = \"tree\", sizeMan = 7, sizeInt = 5, style = \"ram\",\n    residuals = TRUE, intAtSide = FALSE, edge.label.cex = 1.15,\n    whatLabels = \"est\", nCharNodes = 0, normalize = FALSE\n)\n\n\n\n\n\n\n\nI coefficienti stimati con lavaan sono simili ma non identici a quelli ottenuti con Mplus. Ad esempio:\n\n\nEffetto diretto di MNF su BFLX: \\(c' = 0.44\\)\n\n\nEffetto indiretto: \\(a \\cdot b = 0.45 \\cdot 0.24 = 0.109\\)\n\n\nEffetto totale: \\(c' + a \\cdot b = 0.44 + 0.109 = 0.55\\)\n\n\nI valori riportati nell’output di lavaan includono anche gli errori standard e i test per verificare se gli effetti siano significativamente diversi da zero.\nLe correlazioni tra le variabili possono essere calcolate combinando i coefficienti di percorso. Ad esempio:\n\n\nCorrelazione tra BFLX e MNF:\n\n.44 + .45 * .24\n#&gt; [1] 0.548\n\n\n\nCorrelazione tra BFLX e END:\n\n.24 + .44 * .45\n#&gt; [1] 0.438\n\n\n\nLa varianza spiegata dalle variabili esogene per le due variabili endogene è riportata nell’output di lavaan. Ad esempio, la varianza spiegata di END è calcolata come:\n\n0.45^2\n#&gt; [1] 0.203\n\nConsideriamo ora un modello più complesso (Fig. 4 in Barbeau et al., 2019), che include ulteriori relazioni tra variabili:\n\nmod &lt;- \"\n  BULS ~ MNF + BFLX\n  BFLX ~ END + MNF\n  END ~ MNS + MNF\n\"\n\nIl modello esteso viene adattato ai dati utilizzando il seguente codice:\n\nfit2 &lt;- sem(\n    mod,\n    sample.cov = dat_cov,\n    sample.nobs = 192\n)\n\nI risultati vengono analizzati e rappresentati graficamente:\n\nsummary(\n  fit2, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE\n)\n#&gt; lavaan 0.6-19 ended normally after 1 iteration\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                         9\n#&gt; \n#&gt;   Number of observations                           192\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                 8.229\n#&gt;   Degrees of freedom                                 3\n#&gt;   P-value (Chi-square)                           0.042\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               239.501\n#&gt;   Degrees of freedom                                 9\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.977\n#&gt;   Tucker-Lewis Index (TLI)                       0.932\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)               -700.169\n#&gt;   Loglikelihood unrestricted model (H1)       -696.054\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                1418.338\n#&gt;   Bayesian (BIC)                              1447.655\n#&gt;   Sample-size adjusted Bayesian (SABIC)       1419.146\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.095\n#&gt;   90 Percent confidence interval - lower         0.017\n#&gt;   90 Percent confidence interval - upper         0.176\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.130\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.696\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.035\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Regressions:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   BULS ~                                                                \n#&gt;     MNF               0.177    0.066    2.688    0.007    0.177    0.177\n#&gt;     BFLX              0.533    0.066    8.085    0.000    0.533    0.533\n#&gt;   BFLX ~                                                                \n#&gt;     END               0.241    0.065    3.702    0.000    0.241    0.241\n#&gt;     MNF               0.441    0.065    6.769    0.000    0.441    0.441\n#&gt;   END ~                                                                 \n#&gt;     MNS              -0.102    0.091   -1.116    0.264   -0.102   -0.102\n#&gt;     MNF               0.378    0.091    4.140    0.000    0.378    0.378\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .BULS              0.578    0.059    9.798    0.000    0.578    0.581\n#&gt;    .BFLX              0.648    0.066    9.798    0.000    0.648    0.651\n#&gt;    .END               0.788    0.080    9.798    0.000    0.788    0.792\n#&gt; \n#&gt; R-Square:\n#&gt;                    Estimate\n#&gt;     BULS              0.419\n#&gt;     BFLX              0.349\n#&gt;     END               0.208\n\nsemPlot::semPaths(\n    fit2,\n    layout = \"tree\", sizeMan = 7, sizeInt = 1, style = \"ram\",\n    residuals = TRUE, intAtSide = FALSE, edge.label.cex = 1.15,\n    whatLabels = \"est\", nCharNodes = 0, normalize = FALSE\n)\n\n\n\n\n\n\n\nI coefficienti ottenuti con lavaan sono confrontati con quelli riportati da Barbeau et al. (2019):\n\n\nVarianza spiegata di END: lavaan = 0.208, Barbeau et al. (2019) = 0.209\n\nVarianza spiegata di BFLX: lavaan = 0.349, Barbeau et al. (2019) = 0.292\n\nVarianza spiegata di BULS: lavaan = 0.419, Barbeau et al. (2019) = 0.478\n\nAd esempio, la correlazione tra MNF e BULS può essere calcolata come somma degli effetti diretti e indiretti:\n\n-.71 * -.10 * .24 * .53 +\n.38 * .24 * .53 +\n.44 * .53 +\n.18\n#&gt; [1] 0.471\n\nIl valore risultante corrisponde al valore osservato nel campione (0.47), confermando la coerenza del modello stimato.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#modellare-le-medie-con-la-path-analysis",
    "href": "chapters/path_analysis/01_path_analysis.html#modellare-le-medie-con-la-path-analysis",
    "title": "22  Analisi dei percorsi",
    "section": "\n22.10 Modellare le Medie con la Path Analysis",
    "text": "22.10 Modellare le Medie con la Path Analysis\nLa modellazione delle medie in un’analisi dei percorsi (path analysis) si concentra sulla stima e sull’interpretazione delle medie delle variabili manifeste, oltre che sulle relazioni tra di esse. Questo approccio consente di integrare le informazioni sulle medie direttamente nella struttura complessiva del modello, offrendo una visione più completa dei dati.\n\n22.10.1 Passaggi per Modellare le Medie\n\nRaccolta dei Dati:\nÈ necessario disporre di una matrice di covarianza o di correlazione che riassuma le relazioni tra le variabili, oltre alle medie delle variabili stesse. Le medie sono fondamentali per modellare la struttura media del sistema.\nSpecifica del Modello:\nIl modello deve includere sia la struttura di covarianza (che descrive le relazioni tra le variabili) sia la struttura delle medie (che descrive i valori medi delle variabili). Questo garantisce che il modello consideri non solo le relazioni, ma anche il comportamento medio delle variabili in studio.\n\nIntegrazione delle Medie:\n\nPer le variabili esogene (quelle che non dipendono da altre variabili nel modello), vengono considerate direttamente le medie osservate.\n\nPer le variabili endogene (quelle influenzate da altre variabili nel modello), è necessario includere le intercette, che rappresentano i valori medi previsti nel modello.\n\n\nCostante “1” per le Medie:\nPer modellare le medie, si aggiunge una costante “1” a tutte le equazioni che descrivono le variabili manifeste in lavaan. Questo passaggio permette di includere esplicitamente le medie nella struttura del modello.\nStima del Modello:\nDurante la stima, il modello calcola le medie previste per ciascuna variabile endogena, basandosi sui valori medi delle variabili esogene e sui coefficienti di percorso. Questi calcoli consentono di confrontare le medie previste con quelle osservate, valutando l’aderenza del modello ai dati.\n\n22.10.2 Calcolo delle Medie Previste\nLa path analysis permette di modellare non solo le relazioni tra variabili, ma anche le loro medie. Per una variabile dipendente \\(Y\\), il modello base è:\n\\[\nY = c_0 + \\sum_{i=1}^p b_i X_i + \\varepsilon ,\n\\]\ndove:\n\n\n\\(c_0\\) è l’intercetta (il valore atteso di Y quando tutte le X sono zero),\n\n\\(b_i\\) sono i coefficiente di percorso che collegano \\(X_i\\) a \\(Y\\),\n\n\\(X_i\\) sono le variabili predittive,\n\n\\(\\varepsilon\\) è il termine di errore (con media zero).\n\nLa media prevista di \\(Y\\), indicata con \\(\\mu_Y\\), è calcolata come:\n\\[\n\\mu_Y = c_0 + \\sum_{i=1}^p b_i \\mu_{X_i} ,\n\\]\ndove \\(\\mu_{X_i}\\) rappresenta la media osservata della variabile \\(X_i\\).\nÈ importante notare che:\n\nL’intercetta \\(c_0\\) contribuisce direttamente alla media.\nOgni predittore contribuisce attraverso il prodotto \\(b_i \\mu_{X_i}\\).\nL’errore \\(\\varepsilon\\) non influenza la media prevista poiché ha media zero.\n\nPer valutare l’accuratezza del modello rispetto alle medie, definiamo il residuo di media:\n\\[\n\\text{Residuo di Media} = \\bar{Y} - \\mu_Y ,\n\\]\ndove:\n\n\n\\(\\bar{Y}\\) è la media osservata nella variabile dipendente;\n\n\\(\\mu_Y\\) è la media prevista dal modello.\n\nEsaminiamo un esempio pratico. Consideriamo un modello modello con due predittori:\n\\[Y = 2.5 + 0.6X_1 - 0.3X_2 + \\varepsilon\\]\ncon:\n\n\n\\(\\mu_{X_1} = 4\\) (media di \\(X_1\\));\n\n\\(\\mu_{X_2} = 3\\) (media di \\(X_2\\)).\n\nLa media prevista si calcola come:\n\\[\\mu_Y = 2.5 + (0.6 \\cdot 4) + (-0.3 \\cdot 3) = 4.0 .\\] Interpretazione dei componenti:\n\nIntercetta: 2.5.\nContributo di \\(X_1\\): \\(0.6 \\cdot 4 = 2.4\\).\nContributo di \\(X_2\\): \\(-0.3 \\cdot 3 = -0.9\\).\nTotale: \\(2.5 + 2.4 - 0.9 = 4.0\\).\n\nSe la media osservata di \\(Y\\) fosse \\(\\bar{Y} = 4.2\\), il residuo di media sarebbe:\n\\[\n\\text{Residuo di Media} = \\bar{Y} - \\mu_Y = 4.2 - 4.0 = 0.2 .\n\\]\nI residui delle medie forniscono un’indicazione della bontà del modello in termini di previsione delle medie osservate. Se i residui sono grandi o sistematicamente devianti, ciò potrebbe indicare la necessità di rivedere la specificazione del modello, ad esempio aggiungendo variabili o ricalibrando i coefficienti.\nIn conclusione, il calcolo delle medie previste in un’analisi dei percorsi consente di integrare la struttura media nel modello e di confrontare queste previsioni con i dati osservati. Questo processo rappresenta uno strumento utile per valutare l’adeguatezza del modello e identificare eventuali aree di miglioramento.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#riflessioni-conclusive",
    "href": "chapters/path_analysis/01_path_analysis.html#riflessioni-conclusive",
    "title": "22  Analisi dei percorsi",
    "section": "\n22.11 Riflessioni Conclusive",
    "text": "22.11 Riflessioni Conclusive\nIl diagramma di un modello di path analysis rappresenta non solo uno strumento analitico, ma anche un mezzo di comunicazione. Un diagramma completo fornisce una rappresentazione visiva chiara delle relazioni tra le variabili e una guida diretta su come specificare il modello in termini computazionali. Ogni elemento del diagramma — come frecce, nodi e annotazioni — traduce i parametri del modello, sia liberi sia fissi, in istruzioni visive che possono essere facilmente codificate.\nQuesta rappresentazione grafica, spesso basata sul simbolismo RAM di McArdle e McDonald, permette ai ricercatori di comprendere rapidamente la struttura del modello. Ad esempio:\n\nLe frecce unidirezionali rappresentano i coefficienti di percorso, indicando relazioni causali o di influenza diretta.\n\nLe frecce bidirezionali raffigurano le covarianze o correlazioni tra variabili.\n\nI nodi corrispondono alle variabili osservate o latenti.\n\nOgni annotazione sul diagramma — come etichette per i parametri o costanti di scala — comunica informazioni essenziali su come ciascun componente del modello deve essere interpretato e implementato. Questo approccio facilita non solo la comprensione delle relazioni complesse tra variabili, ma rende anche più immediato il processo di traduzione del modello visivo in sintassi utilizzabile nei software statistici.\nLa path analysis offre uno strumento potente per decomporre e interpretare le relazioni tra variabili. Attraverso la decomposizione di correlazioni o covarianze, è possibile delineare in modo chiaro:\n\nLe relazioni dirette tra variabili, espresse dai coefficienti di percorso.\n\nLe relazioni indirette, che emergono attraverso l’interazione di più percorsi nel modello.\n\nLe associazioni complessive, ottenute dalla combinazione degli effetti diretti e indiretti.\n\nQuesta capacità di mappare le potenziali connessioni causali e le interazioni tra le variabili rende la path analysis uno strumento prezioso non solo per l’analisi statistica, ma anche per l’interpretazione teorica. Grazie alla sua natura visiva e strutturata, essa facilita il processo di comprensione, comunicazione e applicazione delle relazioni modellate, consentendo ai ricercatori di descrivere in modo chiaro e accessibile le dinamiche complesse tra variabili in studio.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#session-info",
    "href": "chapters/path_analysis/01_path_analysis.html#session-info",
    "title": "22  Analisi dei percorsi",
    "section": "\n22.12 Session Info",
    "text": "22.12 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] ggokabeito_0.1.0  see_0.10.0        MASS_7.3-65       viridis_0.6.5    \n#&gt;  [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n#&gt;  [9] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n#&gt; [13] lavaan_0.6-19     psych_2.4.12      scales_1.3.0      markdown_1.13    \n#&gt; [17] knitr_1.49        lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n#&gt; [21] dplyr_1.1.4       purrr_1.0.4       readr_2.1.5       tidyr_1.3.1      \n#&gt; [25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.0      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] mime_0.12           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [25] Matrix_1.7-2        R6_2.6.1            fastmap_1.2.0      \n#&gt;  [28] rbibutils_2.3       shiny_1.10.0        digest_0.6.37      \n#&gt;  [31] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n#&gt;  [34] rprojroot_2.0.4     Hmisc_5.2-2         timechange_0.3.0   \n#&gt;  [37] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n#&gt;  [40] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n#&gt;  [43] carData_3.0-5       ggsignif_0.6.4      corpcor_1.6.10     \n#&gt;  [46] gtools_3.9.5        tools_4.4.2         pbivnorm_0.6.0     \n#&gt;  [49] foreign_0.8-88      zip_2.3.2           httpuv_1.6.15      \n#&gt;  [52] nnet_7.3-20         glue_1.8.0          quadprog_1.5-8     \n#&gt;  [55] nlme_3.1-167        promises_1.3.2      lisrelToR_0.3      \n#&gt;  [58] grid_4.4.2          checkmate_2.3.2     cluster_2.1.8      \n#&gt;  [61] reshape2_1.4.4      generics_0.1.3      gtable_0.3.6       \n#&gt;  [64] tzdb_0.4.0          data.table_1.17.0   hms_1.1.3          \n#&gt;  [67] car_3.1-3           sem_3.1-16          pillar_1.10.1      \n#&gt;  [70] rockchalk_1.8.157   later_1.4.1         splines_4.4.2      \n#&gt;  [73] lattice_0.22-6      survival_3.8-3      kutils_1.73        \n#&gt;  [76] tidyselect_1.2.1    miniUI_0.1.1.1      pbapply_1.7-2      \n#&gt;  [79] reformulas_0.4.0    stats4_4.4.2        xfun_0.51          \n#&gt;  [82] qgraph_1.9.8        arm_1.14-4          stringi_1.8.4      \n#&gt;  [85] pacman_0.5.1        boot_1.3-31         evaluate_1.0.3     \n#&gt;  [88] codetools_0.2-20    mi_1.1              cli_3.6.4          \n#&gt;  [91] RcppParallel_5.1.10 rpart_4.1.24        xtable_1.8-4       \n#&gt;  [94] Rdpack_2.6.2        munsell_0.5.1       Rcpp_1.0.14        \n#&gt;  [97] coda_0.19-4.1       png_0.1-8           XML_3.99-0.18      \n#&gt; [100] parallel_4.4.2      jpeg_0.1-10         lme4_1.1-36        \n#&gt; [103] mvtnorm_1.3-3       openxlsx_4.2.8      rlang_1.1.5        \n#&gt; [106] multcomp_1.4-28     mnormt_2.1.1\n\n\n\n\n\nBarbeau, K., Boileau, K., Sarr, F., & Smith, K. (2019). Path analysis in Mplus: A tutorial using a conceptual model of psychological and behavioral antecedents of bulimic symptoms in young adults. The Quantitative Methods for Psychology, 15(1), 38–53.\n\n\nKline, R. B. (2023). Principles and practice of structural equation modeling. Guilford publications.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/02_clement_2022.html",
    "href": "chapters/path_analysis/02_clement_2022.html",
    "title": "23  Riparazione affettiva post-infedeltà",
    "section": "",
    "text": "23.1 Introduzione\nLo scopo di questo capitolo è quello di discutere il tutorial di Clement & Bradley-Garcia (2022) in cui la path anaysis viene impiegata per tre analisi statistiche: il modello di moderazione, il modello di mediazione semplice, e il modello di mediazione moderata.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Riparazione affettiva post-infedeltà</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/02_clement_2022.html#modello-di-mediazione-semplice",
    "href": "chapters/path_analysis/02_clement_2022.html#modello-di-mediazione-semplice",
    "title": "23  Riparazione affettiva post-infedeltà",
    "section": "\n23.2 Modello di Mediazione Semplice",
    "text": "23.2 Modello di Mediazione Semplice\nIl modello di mediazione esplora come una variabile indipendente (IV o X) influenzi una variabile dipendente (DV o Y) attraverso una variabile intermediaria, detta variabile mediatrice (M). In questo approccio, l’effetto dell’IV sulla DV si suddivide in due componenti: un effetto diretto, che rappresenta l’influenza dell’IV sulla DV al netto di M, e un effetto indiretto, che riflette il contributo di M nel collegare IV e DV.\nAd esempio, nel contesto delle relazioni di coppia, si potrebbe ipotizzare che la fiducia nel partner trasgressore (IV) favorisca il perdono (DV) attraverso una maggiore comprensione o empatia (M) (Clement & Bradley-Garcia, 2022; Hargrave & Hammer, 2016). Analizzare questa dinamica con un modello di mediazione semplice può fornire informazioni utili per interventi mirati, come focalizzarsi su strategie per migliorare la fiducia e l’empatia, promuovendo così il perdono.\nUn modello di mediazione semplice si basa quindi su questa struttura sequenziale:\n\n\nIV → M: L’IV influenza direttamente la variabile mediatrice.\n\nM → DV: La variabile mediatrice influisce sulla DV.\n\nIV → DV (effetto diretto): L’IV mantiene un’influenza residua sulla DV anche al netto dell’effetto di M.\n\nL’obiettivo principale è quantificare l’effetto indiretto, che rappresenta la relazione mediata. Questo modello consente di identificare percorsi specifici di influenza e di valutare se e come una variabile intermediaria contribuisca alla relazione tra IV e DV.\n\n\n\n\n\nFigura 23.1: Pannello A: Mediazione semplice; Pannello B: Moderazione; e Pannello C: Modelli di Mediazione Moderata. Nel Pannello A, le linee tratteggiate indicano il percorso indiretto (ab), mentre la linea continua indica il percorso diretto (c’) (Figura tratta da Clement & Bradley-Garcia, 2022).\n\n\nLa letteratura mostra che la fiducia in un partner romantico è associata alla compassione verso tale partner (Salazar, 2015). A sua volta, la compassione risulta essere uno dei principali predittori di perdono (Davis, 2017). Sulla base di queste evidenze, si può ipotizzare che la fiducia contribuisca a promuovere il perdono attraverso la compassione per il partner romantico.\nPer testare questa ipotesi, si può impiegare un modello di mediazione in cui la fiducia nel partner romantico influenza la compassione (percorso “a”), e la compassione, a sua volta, influenza il perdono (percorso “b”). L’effetto indiretto (ab) rappresenta dunque l’influenza che la fiducia esercita sul perdono passando attraverso la compassione. Se l’analisi statistica mostra che questo effetto indiretto è solido, si può concludere che la compassione media l’associazione tra fiducia e perdono. Se invece l’effetto indiretto non risulta significativo, non è possibile affermare che la compassione ricopra un ruolo di mediazione.\nOltre all’effetto indiretto, il modello di mediazione consente di esaminare anche l’effetto diretto della fiducia sul perdono, controllando per la compassione. In altre parole, si valuta se la fiducia nel partner romantico rimanga un predittore significativo del perdono quando la compassione viene mantenuta costante. In questa prospettiva, l’effetto totale (c) del modello di mediazione rappresenta la somma dell’effetto diretto (c′) e dell’effetto indiretto (a × b). Il valore di (a × b) rappresenta il prodotto dei percorsi a e b, mentre c = c′ + a × b.\nSecondo la letteratura (Meule, 2019), si parla di “mediazione completa” quando l’effetto indiretto è significativo e l’effetto diretto non lo è; si parla invece di “mediazione parziale” quando entrambi gli effetti (diretto e indiretto) risultano significativi.\nIn conclusione, un modello di mediazione semplice può far luce su come la compassione per il partner romantico sia associata alla fiducia in quel partner e al perdono. Al contempo, per chiarire in quali condizioni la fiducia e la compassione siano più o meno correlate, occorre prendere in considerazione un modello di moderazione.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Riparazione affettiva post-infedeltà</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/02_clement_2022.html#modello-di-moderazione",
    "href": "chapters/path_analysis/02_clement_2022.html#modello-di-moderazione",
    "title": "23  Riparazione affettiva post-infedeltà",
    "section": "\n23.3 Modello di Moderazione",
    "text": "23.3 Modello di Moderazione\nIn un modello di moderazione, l’interesse è comprendere se una terza variabile, detta variabile moderatrice (W), influenzi la forza o la direzione della relazione tra la variabile indipendente (IV) e la variabile dipendente (DV). Come sottolinea Hayes (2018, p. 220):\n\n“Identificare un moderatore di un effetto aiuta a stabilire le condizioni limite di quell’effetto o le circostanze, gli stimoli o il tipo di persone per cui l’effetto è maggiore o minore, presente o assente, positivo o negativo, e così via.”\n\nDa un punto di vista statistico, verificare un modello di moderazione equivale a testare l’interazione tra fattori, come avviene nell’analisi della varianza (ANOVA; Frazier, Tix, & Barron, 2004). Si parla di effetto di interazione quando l’effetto dell’IV sulla DV varia in funzione dei livelli di W (Hayes, 2018). In altri termini, il ricercatore si chiede se l’associazione tra IV e DV sia più o meno forte (o addirittura diversa in segno) a seconda del valore assunto dal moderatore.\nCome illustrato in Figura 23.1 B, un modello di moderazione può spiegare condizioni specifiche in cui una determinata relazione si presenta più o meno intensa. Tornando all’esempio di fiducia, compassione e perdono, ci si potrebbe domandare in quali circostanze la fiducia favorisca maggiormente la compassione. Nella letteratura sui comportamenti prosociali, l’umiltà è stata identificata come elemento cruciale per la coltivazione della compassione (Worthington & Allison, 2018). Diversi studi evidenziano che percepire il proprio partner romantico come umile è associato al vederlo anche come compassionevole (McDonald, Olson, Goddard, & Marshall, 2018), e che la fiducia è positivamente correlata a livelli di umiltà (Wang, Edwards, & Hill, 2017). Di conseguenza, è plausibile ipotizzare che l’effetto della fiducia sulla compassione vari a seconda di quanto il partner venga considerato umile, configurando così un tipico esempio di moderazione.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Riparazione affettiva post-infedeltà</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/02_clement_2022.html#modello-di-mediazione-moderata",
    "href": "chapters/path_analysis/02_clement_2022.html#modello-di-mediazione-moderata",
    "title": "23  Riparazione affettiva post-infedeltà",
    "section": "\n23.4 Modello di Mediazione Moderata",
    "text": "23.4 Modello di Mediazione Moderata\nIl concetto di mediazione moderata, introdotto da James e Brett (1984), si riferisce all’analisi di come una variabile moderatrice (W) possa influenzare l’intensità o la presenza di un effetto indiretto (Preacher, Rucker, & Hayes, 2007). In altre parole, si parla di mediazione moderata quando l’ampiezza dell’effetto di mediazione varia in funzione dei livelli del moderatore (Preacher et al., 2007).\nCome illustrato in Figura 23.1 C, in un modello di mediazione moderata la relazione tra una variabile indipendente (X) e una variabile dipendente (Y) mediata da un mediatore (M) cambia a seconda del valore assunto da W. Tornando all’esempio di fiducia, umiltà, compassione e perdono, è possibile ipotizzare che la fiducia nel partner romantico (X) eserciti il proprio effetto sul perdono (Y) per il tramite della compassione (M), ma che la forza di questo processo di mediazione sia potenziata (o attenuata) dalla percezione del partner come umile (W). In pratica, quando il partner è percepito come altamente umile, l’effetto indiretto di X su Y attraverso M potrebbe risultare particolarmente marcato; viceversa, a livelli bassi di umiltà, la mediazione potrebbe attenuarsi o persino annullarsi.\nL’analisi di mediazione moderata, pertanto, consente di cogliere in che modo e in quali condizioni un processo di mediazione si inneschi e, di conseguenza, di formulare ipotesi e interventi mirati, basati sulle circostanze che rafforzano o indeboliscono l’effetto di X su Y attraverso M.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Riparazione affettiva post-infedeltà</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/02_clement_2022.html#esempio-di-mediazione-moderata",
    "href": "chapters/path_analysis/02_clement_2022.html#esempio-di-mediazione-moderata",
    "title": "23  Riparazione affettiva post-infedeltà",
    "section": "\n23.5 Esempio di Mediazione Moderata",
    "text": "23.5 Esempio di Mediazione Moderata\nNel suo tutorial, Clement & Bradley-Garcia (2022) presenta i dati di uno studio che ha indagato l’associazione tra la fiducia di coppia, la compassione verso il partner responsabile di un’offesa, la percezione di quel partner come umile e il perdono in persone che avevano sperimentato un “attaccamento ferito”. Secondo Johnson, Makinen e Millikin (2001), le ferite da attaccamento consistono in una violazione percepita della fiducia o in un abbandono verificatosi in un momento critico, in cui la persona ferita necessitava del sostegno e della cura del partner.\nNell’ambito di questo studio, la fiducia di coppia è stata definita come il grado di onestà e di buona volontà che il partner ferito attribuisce a chi ha commesso l’offesa (Larzelere & Huston, 1980). Un aspetto fondamentale per la risoluzione delle ferite da attaccamento, nonché per il ripristino della fiducia nella relazione romantica, è il perdono. Quest’ultimo è stato qui concettualizzato come la presenza di livelli elevati di motivazioni benevolenti (ad es. buona volontà) e livelli bassi di motivazioni di evitamento o vendetta nei confronti del partner offensore.\nPartecipanti\nIl campione utilizzato per questo tutorial era composto da 138 persone che hanno riferito di aver vissuto una ferita da attaccamento nella loro attuale relazione romantica e di aver perdonato il proprio partner.\nI partecipanti hanno completato i questionari auto-compilati online.\nI partecipanti hanno descritto brevemente la ferita da attaccamento subita nella relazione e indicato se avevano perdonato o meno il partner romantico per tale ferita (sì/no).\nLa fiducia di coppia è stata misurata utilizzando la Dyadic Trust Scale (DTS; Larzelere & Huston, 1980), composta da 8 item valutati su una scala da 1 (fortemente d’accordo) a 7 (fortemente in disaccordo).\nLa compassione verso il partner è stata misurata con la Compassion Scale (CS; Pommier, Neff, & Tóth-Király, 2020), modificata per riflettere la compassione verso il partner che ha commesso l’offesa. La scala includeva 16 item valutati da 1 (quasi mai) a 5 (quasi sempre).\nLa percezione del partner come umile è stata misurata con la Relational Humility Scale (RHS; Davis et al., 2011), modificata per riflettere i sentimenti attuali del partner ferito verso il partner che ha commesso l’offesa. Gli item sono stati valutati su una scala da 1 (fortemente in disaccordo) a 5 (fortemente d’accordo).\nIl perdono del partner che ha commesso l’offesa è stato misurato con il Transgressions-related Interpersonal Motivations Inventory (TRIM; McCullough et al., 1998; McCullough, Fincham, & Tsang, 2003). Il TRIM misura:\n\nMotivazione ad evitare il partner.\nMotivazione a cercare vendetta.\nMotivazione a dimostrare benevolenza verso il partner.\n\nPer questo studio, il perdono è stato definito come bassi livelli di motivazione ad evitare o vendicarsi del partner, e alti livelli di motivazione benevolente verso di lui/lei. L’esempio discusso nel tutorial include solo la sottoscala della benevolenza del TRIM-18.\n\n# Load the data from SPSS file\ndata &lt;- read_sav(here::here(\"data\", \"clement_2022.sav\"))\n\n\nglimpse(data)\n#&gt; Rows: 138\n#&gt; Columns: 4\n#&gt; $ CS_TOT   &lt;dbl&gt; 58, 68, 66, 70, 70, 44, 66, 76, 67, 43, 66, 68, 68, 69, 7…\n#&gt; $ RHSTOT   &lt;dbl&gt; 59, 53, 52, 35, 55, 36, 53, 54, 49, 45, 27, 52, 34, 29, 4…\n#&gt; $ TRIM_Ben &lt;dbl&gt; 19, 22, 26, 24, 25, 12, 21, 29, 30, 6, 25, 17, 29, 22, 9,…\n#&gt; $ DTST     &lt;dbl&gt; 54, 51, 48, 34, 48, 32, 52, 52, 49, 47, 36, 34, 47, 56, 5…\n\n\n# Statistiche descrittive\ndescribe(data)\n#&gt;          vars   n mean    sd median trimmed   mad min max range  skew\n#&gt; CS_TOT      1 138 64.2  8.53   66.0    65.0  8.15  35  76    41 -0.88\n#&gt; RHSTOT      2 138 42.9 10.19   44.5    43.3 11.12  17  60    43 -0.33\n#&gt; TRIM_Ben    3 138 21.9  5.68   22.0    22.4  5.93   6  30    24 -0.74\n#&gt; DTST        4 138 38.6 10.25   39.0    39.0 11.86  11  56    45 -0.26\n#&gt;          kurtosis   se\n#&gt; CS_TOT       0.50 0.73\n#&gt; RHSTOT      -0.57 0.87\n#&gt; TRIM_Ben     0.12 0.48\n#&gt; DTST        -0.64 0.87\n\n\n# Identificazione degli outliers per ogni variabile\noutliers_results &lt;- check_outliers(data)\n\n# Visualizzazione dei risultati\nprint(outliers_results)\n#&gt; OK: No outliers detected.\n#&gt; - Based on the following method and threshold: mahalanobis (20).\n#&gt; - For variables: CS_TOT, RHSTOT, TRIM_Ben, DTST\n\n\n# Visualizzare un boxplot con outliers\nboxplot(data, main = \"Boxplot delle variabili\", col = \"lightblue\", las = 2)\n\n\n\n\n\n\n\n\n23.5.1 Indipendenza\nNella regressione lineare, l’assunzione di indipendenza richiede che i residui del modello siano indipendenti l’uno dall’altro. In altre parole, l’errore commesso nel predire il valore di un partecipante non deve influenzare l’errore commesso nel predire il valore di un altro partecipante. Nel nostro caso, ciò significa che l’errore nella stima del punteggio di benevolenza di una persona non dovrebbe influenzare l’errore nella stima del punteggio di un’altra.\nPer verificare l’indipendenza dei residui, si utilizza la statistica di Durbin-Watson, che rileva la presenza di autocorrelazione nei termini di errore (Uyanto, 2020). Questa statistica varia da 0 a 4, e valori compresi tra 1.5 e 2.5 indicano che l’assunzione di indipendenza è plausibilmente soddisfatta (Glen, 2022).\nIn R, la funzione durbinWatsonTest() del pacchetto car permette di calcolare la statistica di Durbin-Watson. Dopo aver definito un modello di regressione (nell’esempio riportato, con TRIM_Ben come variabile dipendente e DTST, CS_TOT, RHSTOT come predittori):\n\n# Creazione del modello di regressione\nmodel &lt;- lm(TRIM_Ben ~ DTST + CS_TOT + RHSTOT, data = data)\n\nsi esegue il test di Durbin-Watson nel seguente modo:\n\n# Esecuzione del test di Durbin-Watson per verificare l'indipendenza dei residui\ndw_test &lt;- durbinWatsonTest(model)\n\n# Visualizzazione del risultato\nprint(dw_test)\n#&gt;  lag Autocorrelation D-W Statistic p-value\n#&gt;    1          -0.032          2.04   0.846\n#&gt;  Alternative hypothesis: rho != 0\n\nSe il valore della statistica rientra tra 1.5 e 2.5, possiamo concludere che l’assunzione di indipendenza dei residui è rispettata. Una volta verificata questa condizione, è possibile procedere con il controllo della successiva assunzione di linearità.\n\n23.5.2 Linearità\nUn’ipotesi fondamentale nella regressione lineare è che esista una relazione lineare tra le variabili indipendenti (IV) e la variabile dipendente (DV). Questo significa che un cambiamento nelle IV comporta un cambiamento proporzionale nella DV. La verifica di questa assunzione è cruciale per garantire che il modello di regressione catturi correttamente la relazione tra le variabili.\nPer valutare la linearità, si possono utilizzare grafici di dispersione che mettono in relazione le IV con la DV oppure grafici parziali per esaminare la relazione lineare tra ogni IV e la DV. Nel suo tutorial, Clement & Bradley-Garcia (2022) verifica l’assunzione di linearità tracciando un grafico tra i residui studentizzati (SRE_1) e i valori previsti non standardizzati (PRE_1).\n\n# Calcolare i residui studentizzati e i valori previsti\nSRE_1 &lt;- rstudent(model)  # Residui studentizzati\nPRE_1 &lt;- predict(model)   # Valori previsti non standardizzati\n\n# Creare un grafico di dispersione per verificare la linearità\nplot(PRE_1, SRE_1,\n    main = \"Verifica della linearità: residui vs valori previsti\",\n    xlab = \"Valori previsti (PRE_1)\",\n    ylab = \"Residui studentizzati (SRE_1)\",\n    pch = 19, col = \"blue\"\n)\nabline(h = 0, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\nInterpretazione:\n\nse i punti nel grafico sono distribuiti casualmente attorno alla linea orizzontale a residuo zero (linea rossa), l’assunzione di linearità è soddisfatta;\nla presenza di pattern evidenti (come curve o trend sistematici) indica una possibile violazione della linearità, suggerendo che il modello di regressione potrebbe non essere adeguato.\n\n23.5.3 Omoschedasticità\nL’assunzione di omoschedasticità richiede che la variabilità degli errori (residui) rimanga costante per tutti i valori delle variabili indipendenti (IV). In altre parole, l’errore nella relazione tra le IV e la variabile dipendente (DV) non dovrebbe aumentare o diminuire sistematicamente in funzione dei valori previsti della DV. Quando questa assunzione viene violata, si parla di eteroschedasticità, che si verifica quando la variabilità degli errori cambia al variare dei valori previsti (Osborne & Waters, 2002).\n\n23.5.3.1 Verifica visiva dell’omoschedasticità\nPer valutare l’omoschedasticità, possiamo utilizzare un grafico di dispersione che rappresenta i residui rispetto ai valori previsti:\n\n# Calcolo dei residui e dei valori previsti\nresiduals &lt;- resid(model)\npredicted_values &lt;- predict(model)\n\n# Grafico di dispersione per verificare l'omoschedasticità\nplot(predicted_values, residuals,\n     main = \"Verifica dell'omoschedasticità: Residui vs Valori previsti\",\n     xlab = \"Valori previsti\",\n     ylab = \"Residui\",\n     pch = 19, col = \"blue\"\n)\nabline(h = 0, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\nSe i punti nel grafico sono distribuiti casualmente attorno alla linea orizzontale (residui pari a 0) e formano una distribuzione rettangolare uniforme, l’assunzione di omoschedasticità è rispettata.\nSe i punti mostrano una dispersione non uniforme (ad esempio, un cono che si allarga o si restringe, o una forma curva), è probabile che ci sia eteroschedasticità e che l’assunzione sia violata.\n\n23.5.3.2 Test statistico per l’omoschedasticità\nOltre al controllo visivo, è possibile eseguire un test formale, come il test di Breusch-Pagan, disponibile nel pacchetto lmtest:\n\n# Esecuzione del test di Breusch-Pagan\nbptest(model)\n#&gt; \n#&gt;  studentized Breusch-Pagan test\n#&gt; \n#&gt; data:  model\n#&gt; BP = 3, df = 3, p-value = 0.3\n\n\n\nInterpretazione del p-value:\n\nSe il p-value è elevato (di solito &gt; 0.05), non ci sono evidenze di eteroschedasticità e l’assunzione di omoschedasticità è plausibilmente soddisfatta.\nSe il p-value è basso (di solito &lt; 0.05), si rileva eteroschedasticità, indicando una violazione dell’assunzione.\n\n\n\n23.5.3.3 Cosa fare in caso di eteroschedasticità?\nSe l’assunzione di omoschedasticità è violata, ci sono diversi approcci per affrontare il problema:\n\n\nTrasformazione delle variabili: Applicare trasformazioni logaritmiche o di potenza alle variabili può ridurre l’eteroschedasticità.\n\nStime robuste: Utilizzare errori standard robusti per ridurre l’impatto dell’eteroschedasticità sui risultati del modello.\n\nModelli alternativi: Considerare modelli che tengano conto dell’eteroschedasticità, come la regressione ponderata.\n\nQuesto tipo di analisi aiuta a garantire che le inferenze statistiche basate sul modello di regressione siano valide e affidabili.\n\n23.5.4 Multicollinearità\nLa multicollinearità si verifica quando due o più variabili indipendenti (IV) sono fortemente correlate tra loro, rendendo difficile isolare gli effetti unici di ciascuna variabile sulla variabile dipendente (DV). Questo problema si manifesta quando i punteggi di una o più IV sono altamente prevedibili in base alle altre IV incluse nel modello (Kim, 2019). La presenza di multicollinearità può compromettere l’interpretazione dei coefficienti di regressione e ridurre l’affidabilità dei risultati.\n\n23.5.4.1 Come verificare la multicollinearità\nPer valutare la multicollinearità, possiamo utilizzare due metriche principali:\n\n\nFattore di inflazione della varianza (VIF): Misura quanto la varianza stimata di un coefficiente di regressione è aumentata a causa della multicollinearità.\n\nStatistica di tolleranza: Rappresenta il reciproco del VIF ed esprime la proporzione della varianza di una variabile indipendente che non è spiegata dalle altre IV.\n\nCriteri di riferimento:\n\nLa tolleranza dovrebbe essere superiore a 0.1 (valori più bassi indicano multicollinearità).\nIl VIF dovrebbe essere inferiore a 10 (Miles, 2005).\n\n23.5.4.2 Calcolo del VIF in R\n\n# Calcolo del VIF per le variabili indipendenti\nvif_values &lt;- vif(model)\n\n# Visualizzazione dei valori VIF\nprint(vif_values)\n#&gt;   DTST CS_TOT RHSTOT \n#&gt;   1.56   1.09   1.59\n\n\n23.5.4.3 Interpretazione dei risultati\n\n\nValori di VIF inferiori a 10: Non ci sono problemi significativi di multicollinearità.\n\nValori di VIF superiori a 10: È presente multicollinearità nel modello, e potrebbe essere necessario intervenire.\n\n23.5.4.4 Come affrontare la multicollinearità\nSe si riscontra multicollinearità, è possibile considerare le seguenti strategie:\n\n\nRimuovere variabili ridondanti: Identificare le IV che contribuiscono meno al modello o sono fortemente correlate tra loro, ed eliminarle.\n\nCombinare variabili correlate: Creare indici compositi o utilizzare tecniche di riduzione della dimensionalità come l’analisi delle componenti principali (PCA).\n\nStandardizzare le variabili: In alcuni casi, la standardizzazione può ridurre gli effetti della multicollinearità.\n\nUtilizzare modelli robusti: Considerare approcci come la regressione ridge o la regressione con penalizzazione (es. LASSO) che possono gestire la multicollinearità.\n\nAffrontare la multicollinearità è essenziale per garantire un’interpretazione affidabile dei coefficienti di regressione e la validità complessiva del modello.\n\n23.5.5 Normalità\nL’assunzione di normalità richiede che i residui, ossia gli errori nella stima del modello di regressione, siano distribuiti normalmente (Hayes, 2018). Questa assunzione è importante principalmente per garantire la validità dei test statistici utilizzati per valutare i coefficienti di regressione, poiché si basa sull’ipotesi di normalità degli errori.\n\n23.5.5.1 Verifica della normalità\nPer verificare questa assunzione, possiamo utilizzare strumenti grafici per un’ispezione visiva e, se necessario, test statistici.\n\n23.5.5.1.1 Verifiche grafiche\n\n\nIstogramma dei residui: Visualizzare la distribuzione dei residui sovrapponendo una curva di densità normale per verificare se i residui seguono una forma a campana.\n\nGrafico P-P (Probability-Probability Plot): Confrontare la distribuzione cumulativa dei residui con una distribuzione normale teorica. Se i punti si allineano lungo la diagonale, la normalità è rispettata.\n\nAnche se ci sono piccole deviazioni dalla normalità, la regressione è robusta a violazioni non gravi di questa assunzione, quindi l’analisi può essere comunque considerata valida.\n\n23.5.5.1.2 Verifica con l’istogramma\nIl seguente codice crea un istogramma dei residui con una curva di densità normale sovrapposta:\n\n# Calcolo dei residui\nresiduals &lt;- resid(model)\n\n# Creare l'istogramma con curva di normalità sovrapposta\nhist(\n  residuals,\n  prob = TRUE, main = \"Istogramma dei residui\",\n  xlab = \"Residui\", col = \"lightblue\", border = \"black\"\n)\n\n# Sovrapporre la curva di densità normale\nlines(density(residuals), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\nInterpretazione: Se la curva di densità (linea rossa) segue la forma dell’istogramma, possiamo concludere che i residui sono distribuiti normalmente.\n\n23.5.5.1.3 Verifica con il grafico P-P\nIl grafico P-P è un ulteriore strumento per verificare la normalità:\n\n# Creazione del P-P plot\nqqnorm(residuals, main = \"Grafico P-P dei residui\")\nqqline(residuals, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\nInterpretazione: Se i punti nel grafico si distribuiscono lungo la linea diagonale (linea rossa), i residui sono distribuiti normalmente. Deviazioni sistematiche dalla linea diagonale indicano violazioni della normalità.\n\n23.5.5.2 Test statistici per la normalità\nSe necessario, è possibile utilizzare test statistici come il test di Shapiro-Wilk o il test di Kolmogorov-Smirnov per valutare formalmente la normalità dei residui:\n\n# Test di Shapiro-Wilk\nshapiro.test(residuals)\n#&gt; \n#&gt;  Shapiro-Wilk normality test\n#&gt; \n#&gt; data:  residuals\n#&gt; W = 0.9, p-value = 5e-06\n\nInterpretazione del p-value:\n\n\np &gt; 0.05: Non ci sono evidenze di una deviazione significativa dalla normalità.\n\np ≤ 0.05: I residui non seguono una distribuzione normale.\n\nIn conclusione, se i residui mostrano piccole deviazioni dalla normalità, ciò potrebbe non compromettere significativamente l’analisi, poiché la regressione lineare è relativamente robusta a violazioni lievi di questa assunzione. Tuttavia, in caso di deviazioni gravi, si potrebbe considerare:\n\n\nTrasformazioni delle variabili (ad esempio, logaritmica o radice quadrata).\n\nModelli alternativi come modelli di regressione robusti o non parametrici.\n\nLa verifica della normalità è una parte essenziale del controllo delle assunzioni, ma va interpretata nel contesto generale della robustezza del modello.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Riparazione affettiva post-infedeltà</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/02_clement_2022.html#analisi-di-moderazione",
    "href": "chapters/path_analysis/02_clement_2022.html#analisi-di-moderazione",
    "title": "23  Riparazione affettiva post-infedeltà",
    "section": "\n23.6 Analisi di Moderazione",
    "text": "23.6 Analisi di Moderazione\nPer valutare se esiste una relazione di moderazione tra DTST (fiducia di coppia) e RHSTOT (umiltà percepita del partner) sull’esito CS_TOT (compassione verso il partner che ha ferito), possiamo analizzare i risultati di un modello di regressione lineare con interazione:\n\n# Creare il modello di moderazione\nmodel &lt;- lm(CS_TOT ~ DTST * RHSTOT, data = data)\nsummary(model)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = CS_TOT ~ DTST * RHSTOT, data = data)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -25.859  -4.131   0.851   5.954  15.033 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)\n#&gt; (Intercept) 74.85822    9.90301    7.56  5.7e-12\n#&gt; DTST        -0.51699    0.27916   -1.85    0.066\n#&gt; RHSTOT      -0.35486    0.24140   -1.47    0.144\n#&gt; DTST:RHSTOT  0.01427    0.00618    2.31    0.022\n#&gt; \n#&gt; Residual standard error: 8.1 on 134 degrees of freedom\n#&gt; Multiple R-squared:  0.117,  Adjusted R-squared:  0.0976 \n#&gt; F-statistic: 5.94 on 3 and 134 DF,  p-value: 0.000781\n\nDall’output della regressione lineare, possiamo osservare quanto segue:\n\n\nIntercetta: Il valore stimato è 74.86 (errore standard = 9.90), risultando altamente significativo (p &lt; 0.001).\n\nEffetto principale di DTST (fiducia di coppia): L’effetto è negativo (b = -0.517) ma non significativo al livello del 5% (p = 0.0662).\n\nEffetto principale di RHSTOT (umiltà percepita): L’effetto è anch’esso negativo (b = -0.355) e non significativo (p = 0.1439).\n\nInterazione tra DTST e RHSTOT: L’interazione è significativa (b = 0.0143, p = 0.0224), suggerendo che l’effetto di DTST su CS_TOT varia in base ai livelli di RHSTOT.\n\nLa significatività dell’interazione indica che la relazione tra DTST (fiducia di coppia) e CS_TOT (compassione) dipende dai livelli di RHSTOT (umiltà percepita del partner). In particolare:\n\nQuando l’umiltà percepita (RHSTOT) è alta, la fiducia di coppia (DTST) ha un effetto maggiore sulla compassione (CS_TOT).\nViceversa, a livelli bassi di umiltà percepita, l’effetto della fiducia sulla compassione potrebbe attenuarsi.\n\n\n23.6.1 Visualizzazione delle Pendenze Semplici\nPer comprendere meglio l’interazione, possiamo visualizzare l’effetto di DTST (fiducia di coppia) su CS_TOT (compassione) a tre livelli di RHSTOT:\n\nAlla media di RHSTOT.\nA ±1 deviazione standard dalla media.\n\n\n# Calcolo della media e delle deviazioni standard di RHSTOT\nmean_RHSTOT &lt;- mean(data$RHSTOT, na.rm = TRUE)\nsd_RHSTOT &lt;- sd(data$RHSTOT, na.rm = TRUE)\n\n# Visualizzare le pendenze semplici dell'interazione\ninteract_plot(model,\n    pred = DTST, modx = RHSTOT,\n    modx.values = c(mean_RHSTOT - sd_RHSTOT, mean_RHSTOT, mean_RHSTOT + sd_RHSTOT),\n    plot.points = TRUE,\n    main.title = \"Interazione tra DTST e RHSTOT su CS_TOT\",\n    x.label = \"Fiducia di coppia (DTST)\",\n    y.label = \"Compassione (CS_TOT)\",\n    legend.main = \"Livelli di umiltà percepita (RHSTOT)\"\n)\n\n\n\n\n\n\n\n\n23.6.2 Interpretazione del Grafico\nDal grafico risultante, è possibile osservare:\n\n\nA livelli elevati di RHSTOT: L’effetto della fiducia di coppia (DTST) sulla compassione (CS_TOT) è più forte e positivo.\n\nA livelli medi di RHSTOT: L’effetto è moderato.\n\nA livelli bassi di RHSTOT: L’effetto si riduce o può persino diventare trascurabile.\n\nQuesta visualizzazione conferma che la percezione dell’umiltà del partner modula l’effetto della fiducia di coppia sulla compassione, fornendo un’interpretazione più chiara del ruolo della moderazione.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Riparazione affettiva post-infedeltà</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/02_clement_2022.html#analisi-della-mediazione",
    "href": "chapters/path_analysis/02_clement_2022.html#analisi-della-mediazione",
    "title": "23  Riparazione affettiva post-infedeltà",
    "section": "\n23.7 Analisi della Mediazione",
    "text": "23.7 Analisi della Mediazione\nPer comprendere la relazione tra la variabile indipendente (X, la fiducia di coppia) e la variabile dipendente (Y, il perdono), Clement & Bradley-Garcia (2022) valutano sia l’effetto diretto che l’effetto mediato da una variabile intermediaria (M, la compassione). A tal fine, viene utilizzato un modello di mediazione.\nIl modello considera tre componenti principali:\n\n\nEffetto diretto (\\(c'\\)): l’influenza diretta di X (fiducia di coppia) su Y (perdono).\n\nEffetto mediato (\\(ab\\)): l’effetto indiretto di X su Y attraverso M (compassione).\n\nEffetto totale (\\(c\\)): la combinazione degli effetti diretto e mediato.\n\n\n# Definizione del modello di mediazione\nmodel &lt;- \"\n  # Effetto diretto di DTST su TRIM_Ben\n  TRIM_Ben ~ c_prime*DTST\n\n  # Effetto di DTST su CS_TOT (mediazione)\n  CS_TOT ~ a*DTST\n\n  # Effetto di CS_TOT su TRIM_Ben (mediazione)\n  TRIM_Ben ~ b*CS_TOT\n\n  # Effetto indiretto (a * b)\n  ab := a*b\n\n  # Effetto totale di DTST su TRIM_Ben\n  total := c_prime + ab\n\"\n\n# Esegui il modello con i dati\nfit &lt;- sem(model, data = data)\n\n# Riassunto dei risultati\nsummary(fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) |&gt;\n  print()\n#&gt; lavaan 0.6-19 ended normally after 1 iteration\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                         5\n#&gt; \n#&gt;   Number of observations                           138\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                 0.000\n#&gt;   Degrees of freedom                                 0\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                                60.916\n#&gt;   Degrees of freedom                                 3\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    1.000\n#&gt;   Tucker-Lewis Index (TLI)                       1.000\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)               -895.673\n#&gt;   Loglikelihood unrestricted model (H1)       -895.673\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                1801.345\n#&gt;   Bayesian (BIC)                              1815.981\n#&gt;   Sample-size adjusted Bayesian (SABIC)       1800.163\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.000\n#&gt;   90 Percent confidence interval - lower         0.000\n#&gt;   90 Percent confidence interval - upper         0.000\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                       NA\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                       NA\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.000\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Regressions:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   TRIM_Ben ~                                                            \n#&gt;     DTST    (c_pr)    0.148    0.040    3.691    0.000    0.148    0.267\n#&gt;   CS_TOT ~                                                              \n#&gt;     DTST       (a)    0.197    0.069    2.864    0.004    0.197    0.237\n#&gt;   TRIM_Ben ~                                                            \n#&gt;     CS_TOT     (b)    0.292    0.048    6.058    0.000    0.292    0.438\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .TRIM_Ben         21.819    2.627    8.307    0.000   21.819    0.681\n#&gt;    .CS_TOT           68.186    8.209    8.307    0.000   68.186    0.944\n#&gt; \n#&gt; R-Square:\n#&gt;                    Estimate\n#&gt;     TRIM_Ben          0.319\n#&gt;     CS_TOT            0.056\n#&gt; \n#&gt; Defined Parameters:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;     ab                0.058    0.022    2.589    0.010    0.058    0.104\n#&gt;     total             0.205    0.044    4.690    0.000    0.205    0.371\n\nL’output di lavaan fornisce informazioni dettagliate su tutti i percorsi del modello, inclusi gli effetti diretto, indiretto e totale, insieme ai coefficienti standardizzati e non standardizzati. Poiché il modello ha zero gradi di libertà, gli indici di adattamento globali (CFI, TLI, RMSEA) non sono interpretabili, quindi ci concentriamo sui coefficienti di percorso.\n\n23.7.0.1 Effetto Diretto\n\n\nPercorso diretto (\\(c'\\)): Il coefficiente standardizzato dell’effetto diretto di DTST (fiducia di coppia) su TRIM_Ben (perdono) è pari a 0.267, mentre il coefficiente non standardizzato è 0.148 (SE = 0.040, \\(z = 3.691\\), \\(p &lt; 0.001\\)).\n\nQuesto risultato indica che la fiducia di coppia ha un effetto positivo diretto sul perdono, indipendentemente dalla compassione verso il partner.\n\n\n\n23.7.0.2 Effetto Indiretto\n\n\nPercorso indiretto (\\(ab\\)): L’effetto mediato da CS_TOT (compassione) ha un coefficiente standardizzato pari a 0.104 e un coefficiente non standardizzato di 0.058 (SE = 0.022, \\(z = 2.589\\), \\(p = 0.010\\)).\n\nQuesto suggerisce che la fiducia di coppia influisce sul perdono anche attraverso la compassione. In particolare, maggiore fiducia nel partner favorisce un incremento della compassione, che a sua volta aumenta la propensione al perdono.\n\n\n\n23.7.0.3 Effetto Totale\n\n\nPercorso totale (\\(c = c' + ab\\)): L’effetto totale di DTST su TRIM_Ben è pari a 0.371 (coefficiente standardizzato) e 0.205 (coefficiente non standardizzato, SE = 0.044, \\(z = 4.690\\), \\(p &lt; 0.001\\)).\n\nQuesto risultato evidenzia che la fiducia di coppia ha un’influenza complessiva positiva sul perdono, combinando l’effetto diretto e quello mediato.\n\n\n\n23.7.1 Varianza Spiegata (R²)\n\n\nPerdono (\\(TRIM_Ben\\)): La fiducia di coppia (DTST) e la compassione (CS_TOT) spiegano il 31.9% della varianza in TRIM_Ben. Questo suggerisce che il modello rappresenta un’importante porzione della variabilità nel perdono.\n\nCompassione (\\(CS_TOT\\)): La fiducia di coppia spiega il 5.6% della varianza in CS_TOT, indicando che la compassione è parzialmente influenzata dalla fiducia di coppia.\n\nIn sintesi, questi risultati suggeriscono che la fiducia nel partner è un elemento centrale per favorire il perdono. Inoltre, la compassione rappresenta un meccanismo importante che media questa relazione. Interventi mirati a rafforzare sia la fiducia che la compassione potrebbero migliorare la capacità di perdonare nelle relazioni di coppia, contribuendo così al benessere e alla resilienza dei partner.\n\n23.7.2 Mediazione Moderata\nLa mediazione moderata combina mediazione e moderazione per analizzare come una variabile indipendente (X, ad esempio la fiducia di coppia) influenzi una variabile dipendente (Y, come il perdono) attraverso una variabile mediatrice (M, ad esempio la compassione), mentre una variabile moderatrice (W, come l’umiltà percepita del partner) condiziona i percorsi di relazione.\nIn questo approccio si distinguono tre componenti: - Effetto diretto: l’influenza di X su Y, modulata da W. - Percorso mediato: il contributo di M nel collegare X a Y, con possibilità di moderazione da parte di W. - Effetto complessivo: somma degli effetti diretto e mediato.\n\n23.7.3 Analisi del Modello\nIl codice seguente stima un modello di mediazione moderata, includendo effetti principali e di interazione:\nmodel &lt;- \"\n  # Effetto diretto di X su Y\n  TRIM_Ben ~ c*DTST + b*CS_TOT + d*RHSTOT + e*DTST:RHSTOT\n\n  # Percorso mediato (X -&gt; M -&gt; Y)\n  CS_TOT ~ a*DTST + f*RHSTOT + g*DTST:RHSTOT\n\n  # Effetti indiretti, diretti e totali\n  indirect := a*b\n  direct := c\n  total := indirect + direct\n\"\n\n# Stima del modello\nfit &lt;- sem(model, data = data, se = \"bootstrap\", bootstrap = 100)\n\n# Resoconto del modello\nsummary(fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) |&gt; print()\nQuesto modello esplora come l’umiltà percepita (RHSTOT) moderi sia la relazione diretta tra fiducia di coppia (DTST) e perdono (TRIM_Ben), sia il percorso mediato attraverso la compassione (CS_TOT).\n\n23.7.4 Risultati Principali\n\n\nEffetto Diretto:\n\nCoefficiente: 0.203 (z = 1.378, p = 0.168). L’effetto diretto non è significativo ma suggerisce una relazione positiva tra fiducia di coppia e perdono.\n\n\n\nEffetto Indiretto:\n\nCoefficiente: -0.149 (z = -1.435, p = 0.151). L’effetto mediato non contribuisce in modo rilevante.\n\n\n\nModerazione:\n\nL’interazione tra fiducia di coppia (DTST) e umiltà percepita (RHSTOT) su compassione (CS_TOT) è significativa (coefficiente: 0.014, z = 2.162, p = 0.031). La fiducia di coppia ha un effetto maggiore sulla compassione quando l’umiltà percepita è elevata.\n\n\n\nEffetto Totale:\n\nCoefficiente: 0.054 (z = 0.380, p = 0.704). L’influenza complessiva della fiducia di coppia sul perdono è limitata.\n\n\n\nVarianza Spiegata:\n\nIl modello spiega il 32.5% della varianza in TRIM_Ben e l’11.7% in CS_TOT. Questo indica un’influenza moderata sulle variabili chiave.\n\n\n\n23.7.5 Interpretazione\nI risultati suggeriscono che: - L’umiltà percepita del partner (RHSTOT) è cruciale per modulare il percorso tra fiducia di coppia (DTST) e compassione (CS_TOT). - Non emergono evidenze chiare di un effetto mediato significativo di DTST su TRIM_Ben tramite CS_TOT. - Sebbene la fiducia abbia un impatto complessivo limitato sul perdono, la moderazione da parte dell’umiltà percepita evidenzia l’importanza delle percezioni interpersonali nel rafforzare i legami emotivi e promuovere il perdono.\n\n23.7.6 Conclusioni\nIl modello analizzato evidenzia che l’umiltà percepita del partner (RHSTOT) gioca un ruolo centrale nel modulare la relazione tra fiducia di coppia (DTST), compassione (CS_TOT), e perdono (TRIM_Ben).\nSebbene l’effetto diretto della fiducia sul perdono non sia significativo, RHSTOT modera in modo significativo l’effetto di DTST su CS_TOT, suggerendo che percezioni positive del partner favoriscono una maggiore compassione. Tuttavia, non emergono evidenze di un effetto indiretto significativo di DTST su TRIM_Ben tramite CS_TOT.\nIn sintesi, il modello sottolinea l’importanza delle percezioni interpersonali nel promuovere la compassione, ma il loro impatto sul perdono rimane limitato.\n\n23.7.7 Riflessioni Finali\nLe tre analisi condotte delineano un quadro articolato delle relazioni tra fiducia di coppia (DTST), compassione verso il partner che ha ferito (CS_TOT) e perdono (TRIM_Ben).\nI risultati indicano che la fiducia di coppia ha un’influenza diretta positiva sul perdono, sebbene in alcuni modelli questo effetto risulti meno robusto. Tuttavia, l’effetto mediato attraverso la compassione appare più consistente, evidenziando il ruolo centrale di quest’ultima nel favorire il perdono.\nUn elemento fondamentale emerso è il ruolo della percezione dell’umiltà del partner (RHSTOT). Questa variabile non solo modera l’effetto della fiducia sulla compassione, ma amplifica l’influenza complessiva della fiducia sul perdono. In contesti in cui il partner è percepito come umile, l’effetto della fiducia sulla compassione è più forte, contribuendo così a rafforzare la propensione al perdono.\nIn sintesi, i risultati evidenziano che:\n\n\nLa fiducia di coppia è rilevante per il perdono, ma il suo effetto è potenziato dalla compassione come mediatore.\n\nL’umiltà percepita del partner gioca un ruolo cruciale nel modellare il legame tra fiducia e compassione, sottolineando l’importanza del contesto relazionale.\n\nIl perdono dipende non solo dalla fiducia, ma anche dalla capacità di sviluppare compassione, particolarmente influenzata dalla percezione che il partner sia umile e disposto a riconoscere i propri errori.\n\nQuesti risultati suggeriscono che interventi per favorire il perdono nelle relazioni di coppia dovrebbero considerare sia il rafforzamento della fiducia sia il miglioramento delle dinamiche empatiche e della percezione di umiltà reciproca, promuovendo così una maggiore capacità di compassione e comprensione.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Riparazione affettiva post-infedeltà</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/02_clement_2022.html#session-info",
    "href": "chapters/path_analysis/02_clement_2022.html#session-info",
    "title": "23  Riparazione affettiva post-infedeltà",
    "section": "\n23.8 Session Info",
    "text": "23.8 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] sjPlot_2.8.17      interactions_1.2.0 lmtest_0.9-40     \n#&gt;  [4] zoo_1.8-13         car_3.1-3          carData_3.0-5     \n#&gt;  [7] performance_0.13.0 mediation_4.5.0    sandwich_3.1-1    \n#&gt; [10] mvtnorm_1.3-3      Matrix_1.7-2       haven_2.5.4       \n#&gt; [13] lavaanPlot_0.8.1   lavaanExtra_0.2.1  ggokabeito_0.1.0  \n#&gt; [16] see_0.10.0         MASS_7.3-65        viridis_0.6.5     \n#&gt; [19] viridisLite_0.4.2  ggpubr_0.6.0       ggExtra_0.10.1    \n#&gt; [22] gridExtra_2.3      patchwork_1.3.0    bayesplot_1.11.1  \n#&gt; [25] semTools_0.5-6     semPlot_1.1.6      lavaan_0.6-19     \n#&gt; [28] psych_2.4.12       scales_1.3.0       markdown_1.13     \n#&gt; [31] knitr_1.49         lubridate_1.9.4    forcats_1.0.0     \n#&gt; [34] stringr_1.5.1      dplyr_1.1.4        purrr_1.0.4       \n#&gt; [37] readr_2.1.5        tidyr_1.3.1        tibble_3.2.1      \n#&gt; [40] ggplot2_3.5.1      tidyverse_2.0.0    here_1.0.1        \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] splines_4.4.2       later_1.4.1         datawizard_1.0.0   \n#&gt;   [4] XML_3.99-0.18       rpart_4.1.24        lifecycle_1.0.4    \n#&gt;   [7] Rdpack_2.6.2        rstatix_0.7.2       rprojroot_2.0.4    \n#&gt;  [10] globals_0.16.3      lattice_0.22-6      insight_1.0.2      \n#&gt;  [13] rockchalk_1.8.157   backports_1.5.0     magrittr_2.0.3     \n#&gt;  [16] openxlsx_4.2.8      Hmisc_5.2-2         rmarkdown_2.29     \n#&gt;  [19] yaml_2.3.10         httpuv_1.6.15       qgraph_1.9.8       \n#&gt;  [22] zip_2.3.2           pbapply_1.7-2       minqa_1.2.8        \n#&gt;  [25] RColorBrewer_1.1-3  multcomp_1.4-28     abind_1.4-8        \n#&gt;  [28] quadprog_1.5-8      nnet_7.3-20         TH.data_1.1-3      \n#&gt;  [31] jtools_2.3.0        listenv_0.9.1       arm_1.14-4         \n#&gt;  [34] parallelly_1.42.0   codetools_0.2-20    tidyselect_1.2.1   \n#&gt;  [37] ggeffects_2.2.0     farver_2.1.2        lme4_1.1-36        \n#&gt;  [40] broom.mixed_0.2.9.6 stats4_4.4.2        base64enc_0.1-3    \n#&gt;  [43] jsonlite_1.9.0      Formula_1.2-5       survival_3.8-3     \n#&gt;  [46] emmeans_1.10.7      tools_4.4.2         Rcpp_1.0.14        \n#&gt;  [49] glue_1.8.0          mnormt_2.1.1        xfun_0.51          \n#&gt;  [52] withr_3.0.2         fastmap_1.2.0       boot_1.3-31        \n#&gt;  [55] digest_0.6.37       mi_1.1              timechange_0.3.0   \n#&gt;  [58] R6_2.6.1            mime_0.12           estimability_1.5.1 \n#&gt;  [61] colorspace_2.1-1    lpSolve_5.6.23      gtools_3.9.5       \n#&gt;  [64] jpeg_0.1-10         DiagrammeR_1.0.11   generics_0.1.3     \n#&gt;  [67] data.table_1.17.0   corpcor_1.6.10      htmlwidgets_1.6.4  \n#&gt;  [70] pkgconfig_2.0.3     sem_3.1-16          gtable_0.3.6       \n#&gt;  [73] furrr_0.3.1         htmltools_0.5.8.1   png_0.1-8          \n#&gt;  [76] reformulas_0.4.0    rstudioapi_0.17.1   tzdb_0.4.0         \n#&gt;  [79] reshape2_1.4.4      coda_0.19-4.1       visNetwork_2.1.2   \n#&gt;  [82] checkmate_2.3.2     nlme_3.1-167        nloptr_2.1.1       \n#&gt;  [85] sjlabelled_1.2.0    parallel_4.4.2      miniUI_0.1.1.1     \n#&gt;  [88] foreign_0.8-88      pillar_1.10.1       grid_4.4.2         \n#&gt;  [91] vctrs_0.6.5         promises_1.3.2      OpenMx_2.21.13     \n#&gt;  [94] xtable_1.8-4        cluster_2.1.8       htmlTable_2.4.3    \n#&gt;  [97] evaluate_1.0.3      pbivnorm_0.6.0      cli_3.6.4          \n#&gt; [100] kutils_1.73         compiler_4.4.2      rlang_1.1.5        \n#&gt; [103] ggsignif_0.6.4      labeling_0.4.3      fdrtool_1.2.18     \n#&gt; [106] sjmisc_2.8.10       plyr_1.8.9          stringi_1.8.4      \n#&gt; [109] pander_0.6.6        munsell_0.5.1       lisrelToR_0.3      \n#&gt; [112] pacman_0.5.1        sjstats_0.19.0      hms_1.1.3          \n#&gt; [115] glasso_1.11         future_1.34.0       shiny_1.10.0       \n#&gt; [118] rbibutils_2.3       igraph_2.1.4        broom_1.0.7        \n#&gt; [121] RcppParallel_5.1.10\n\n\n\n\n\nClement, L. M., & Bradley-Garcia, M. (2022). A step-by-step tutorial for performing a moderated mediation analysis using PROCESS. The Quantitative Methods for Psychology, 18(3), 258–271.\n\n\nHargrave, T. D., & Hammer, M. Y. (2016). Restoration of Relationships After Affairs. In Techniques for the Couple Therapist (pp. 190–193). Routledge.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Riparazione affettiva post-infedeltà</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html",
    "href": "chapters/networks/01_networks.html",
    "title": "24  Network psicologici",
    "section": "",
    "text": "24.1 Introduzione\nQuesto capitolo costituisce un riassunto semplificato del capitolo Psychological Networks: A Modern Approach to Analysis of Learning and Complex Learning Processes del testo di Saqr & López-Pernas (2024).",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#la-network-analysis",
    "href": "chapters/networks/01_networks.html#la-network-analysis",
    "title": "24  Network psicologici",
    "section": "\n24.2 La Network Analysis",
    "text": "24.2 La Network Analysis\nL’analisi delle reti è un potente strumento per i ricercatori, utile per mappare relazioni, individuare connessioni e identificare cluster o comunità tra elementi interagenti. Questa metodologia si è affermata come una delle più importanti per comprendere sistemi complessi. In psicologia, l’analisi delle reti non è più limitata allo studio delle interazioni sociali, ma viene sempre più utilizzata per esplorare processi astratti come quelli cognitivi, emotivi e comportamentali. In questo ambito, le reti probabilistiche svolgono un ruolo centrale, rappresentando i nodi come variabili psicologiche (ad esempio, punteggi di scale o indicatori di costrutti) e gli archi come associazioni probabilistiche tra di esse.\nUn esempio di spicco è il Gaussian Graphical Model (GGM), in cui i nodi rappresentano costrutti psicologici come emozioni, comportamenti o tratti, mentre gli archi riflettono correlazioni parziali. La correlazione parziale rappresenta la relazione tra due variabili controllando l’effetto di tutte le altre variabili nella rete, secondo il principio del ceteris paribus.\nAd esempio, in una rete in cui i nodi rappresentano motivazione, successo accademico, coinvolgimento, autoregolazione e benessere, un arco tra benessere e successo accademico indica che il benessere è associato al successo accademico, indipendentemente dagli effetti delle altre variabili. L’assenza di un arco, invece, suggerisce che due nodi sono condizionalmente indipendenti, una volta considerati gli effetti delle altre variabili.\n\n24.2.1 Vantaggi delle Reti Psicologiche\nL’analisi delle reti offre diversi strumenti per valutare la robustezza e la precisione delle stime:\n\n\nAccuratezza delle stime: Tecniche come il bootstrapping permettono di valutare la stabilità dei pesi degli archi.\n\nCentralità: Misure come la forza, vicinanza e intermediazione dei nodi permettono di identificare elementi centrali o influenti nella rete.\n\nSimulazioni: Analisi basate su campioni simulati consentono di stimare la replicabilità dei risultati.\n\nConsideriamo, ad esempio, una rete che rappresenta le relazioni tra emozioni negative (ansia, tristezza, rabbia), pensieri disfunzionali e strategie di coping. Un arco tra ansia e pensieri disfunzionali potrebbe indicare che l’ansia è strettamente associata ai pensieri disfunzionali, controllando per l’effetto di tristezza, rabbia e coping. Questo tipo di analisi non solo chiarisce le interazioni tra variabili, ma suggerisce interventi mirati: ad esempio, rafforzare le strategie di coping per ridurre l’impatto delle emozioni negative sui pensieri disfunzionali.\nIn sintesi, la Network Analysis rappresenta un approccio innovativo e rigoroso per indagare i sistemi psicologici complessi, offrendo una visione dettagliata delle interazioni tra variabili e potenziali punti di intervento.",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#tutorial-con-r",
    "href": "chapters/networks/01_networks.html#tutorial-con-r",
    "title": "24  Network psicologici",
    "section": "\n24.3 Tutorial con R",
    "text": "24.3 Tutorial con R\nQuesto capitolo presenta un tutorial passo-passo su come utilizzare le reti psicologiche applicandole a dati raccolti in indagini trasversali, così come discusso da Saqr & López-Pernas (2024). Il dataset utilizzato contiene le risposte di 6071 studenti a un questionario che indaga le caratteristiche psicologiche legate al loro benessere durante la pandemia di COVID-19, condotto in Finlandia e Austria.\nLe domande del questionario riguardano i bisogni psicologici fondamentali degli studenti (relazionalità, autonomia e competenza percepita), l’apprendimento autoregolato, le emozioni positive e la motivazione intrinseca verso l’apprendimento. Inoltre, il dataset include variabili demografiche come paese di residenza, genere ed età.\nNel tutorial, mostreremo come costruire e visualizzare una rete che rappresenti le relazioni tra le diverse caratteristiche psicologiche. Successivamente, interpreteremo e valuteremo queste relazioni e confronteremo le differenze nelle reti tra gruppi demografici. Questo approccio consente di esplorare in modo visivo e quantitativo i collegamenti tra i costrutti psicologici, identificando eventuali differenze tra le categorie demografiche.",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#importazione-e-preparazione-dei-dati",
    "href": "chapters/networks/01_networks.html#importazione-e-preparazione-dei-dati",
    "title": "24  Network psicologici",
    "section": "\n24.4 Importazione e Preparazione dei Dati",
    "text": "24.4 Importazione e Preparazione dei Dati\nLa prima fase prevede l’importazione dei dati e la loro preparazione, eliminando risposte mancanti o incomplete per garantire un dataset coerente e utilizzabile nelle analisi successive.\nI dati vengono importati direttamente da una fonte online. Utilizziamo la funzione import() per caricare il file e drop_na() per rimuovere eventuali righe contenenti valori mancanti.\n\nURL &lt;- (\"https://raw.githubusercontent.com/lamethods/data/main/11_universityCovid/data.sav\") \ndf &lt;- import(URL) |&gt;\n    drop_na()\n\nPer rappresentare ciascun costrutto misurato dal questionario, combiniamo le colonne relative agli item dello stesso costrutto calcolando la media delle risposte. Questo approccio permette di ottenere una sintesi per ogni costrutto psicologico (ad esempio Competence, Autonomy, ecc.).\n\naggregated &lt;- df |&gt; rowwise() |&gt; mutate(\n    Competence = rowMeans(cbind\n    (comp1.rec , comp2.rec, comp3.rec),\n    na.rm = T),\n    Autonomy = rowMeans(cbind\n    (auto1.rec , auto2.rec, auto3.rec),\n    na.rm = T),\n    Motivation = rowMeans(cbind\n    (lm1.rec , lm2.rec, lm3.rec),\n    na.rm = T),\n    Emotion = rowMeans(cbind\n    (pa1.rec , pa2.rec, pa3.rec),\n    na.rm = T),\n    Relatedness = rowMeans(cbind\n    (sr1.rec , sr2.rec, sr3.rec),\n    na.rm = T),\n    SRL = rowMeans(cbind\n    (gp1.rec , gp2.rec, gp3.rec),\n    na.rm = T)\n)\n\nDopo aver calcolato i valori medi per ciascun costrutto, manteniamo solo le colonne appena generate. Questo rende il dataset più pulito e specifico per l’analisi.\nInoltre, creiamo dei sottoinsiemi di dati in base al genere (un dataset per i maschi e uno per le femmine) e al paese (un dataset per l’Austria e un altro per la Finlandia). Utilizzeremo questi dataset successivamente per confronti tra generi e paesi.\n\ncols &lt;- c(\n    \"Relatedness\", \"Competence\", \"Autonomy\",\n    \"Emotion\", \"Motivation\", \"SRL\"\n)\ndplyr::filter(aggregated, country == 1) |&gt;\n    dplyr::select(all_of(cols)) -&gt; finlandData\ndplyr::filter(aggregated, country == 0) |&gt;\n    dplyr::select(all_of(cols)) -&gt; austriaData\ndplyr::filter(aggregated, gender == 1) |&gt;\n    dplyr::select(all_of(cols)) -&gt; femaleData\ndplyr::filter(aggregated, gender == 2) |&gt;\n    dplyr::select(all_of(cols)) -&gt; maleData\ndplyr::select(aggregated, all_of(cols)) -&gt; allData\n\nInfine, utilizziamo glimpse() per visualizzare una panoramica del dataset finale, verificando che i dati siano stati preparati correttamente.\n\nallData |&gt; glimpse()\n#&gt; Rows: 7,160\n#&gt; Columns: 6\n#&gt; Rowwise: \n#&gt; $ Relatedness &lt;dbl&gt; 3.00, 5.00, 4.67, 4.33, 5.00, 4.00, 4.67, 2.33, 3.00, …\n#&gt; $ Competence  &lt;dbl&gt; 2.33, 3.00, 4.00, 3.33, 4.00, 2.67, 4.33, 3.67, 3.33, …\n#&gt; $ Autonomy    &lt;dbl&gt; 2.33, 1.67, 2.67, 3.00, 3.00, 2.67, 3.33, 2.67, 3.00, …\n#&gt; $ Emotion     &lt;dbl&gt; 2.00, 2.33, 4.00, 3.33, 4.00, 3.33, 4.67, 5.00, 3.67, …\n#&gt; $ Motivation  &lt;dbl&gt; 1.33, 2.00, 3.00, 2.67, 3.00, 2.67, 4.33, 1.33, 2.00, …\n#&gt; $ SRL         &lt;dbl&gt; 2.67, 4.33, 4.33, 2.67, 4.33, 5.00, 4.67, 2.67, 4.00, …",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#controllo-delle-assunzioni",
    "href": "chapters/networks/01_networks.html#controllo-delle-assunzioni",
    "title": "24  Network psicologici",
    "section": "\n24.5 Controllo delle Assunzioni",
    "text": "24.5 Controllo delle Assunzioni\nPrima di procedere con l’analisi, è essenziale verificare alcune assunzioni per assicurarsi che il dataset e la rete stimata siano appropriati e robusti.\n\n24.5.1 Matrice di Correlazione Definita Positiva\nLa matrice di correlazione deve essere definita positiva, il che implica che le variabili incluse non devono essere combinazioni lineari tra loro. In termini pratici, ogni variabile deve aggiungere informazioni uniche al modello. Per controllare questa proprietà, utilizziamo la funzione is.positive.definite() del pacchetto matrixcalc.\nSe la matrice di correlazione non fosse definita positiva, è possibile utilizzare metodi alternativi, come l’opzione cor_auto, per ottenere una matrice che soddisfi questa condizione. Nel nostro caso, la matrice risulta già definita positiva. Inoltre, specifichiamo l’argomento use = \"pairwise.complete.obs\" per includere tutte le osservazioni disponibili per ogni coppia di variabili.\n\ncorrelationMatrix &lt;- cor(\n  x = allData, use = c(\"pairwise.complete.obs\")\n)\nis.positive.definite(correlationMatrix)\n#&gt; [1] TRUE\n\nSe la funzione restituisce TRUE, possiamo procedere con l’analisi; in caso contrario, sarà necessario intervenire per correggere il problema.\n\n24.5.2 Assenza di ridondanza tra le variabili\nLa seconda verifica consiste nell’assicurarci che non ci siano variabili altamente correlate al punto da risultare ridondanti. Questo è fondamentale per garantire che ogni variabile rappresenti un costrutto unico e non una semplice sovrapposizione di altri costrutti già inclusi.\nUtilizziamo l’algoritmo goldbricker, che identifica pattern di correlazione fortemente simili tra coppie di variabili. I criteri utilizzati per individuare la ridondanza sono:\n\nCorrelazione alta: \\(r &gt; 0.50\\)\n\nFrazione significativa: almeno il 25% delle variabili altamente correlate.\np-value: 0.05 (significatività statistica).\n\nEseguiamo il controllo con il codice seguente:\n\ngoldbricker(allData,\n    p = 0.05, method = \"hittner2003\",\n    threshold = 0.25, corMin = 0.5, \n    progressbar = FALSE\n)\n#&gt; Suggested reductions: Less than 25 % of correlations are significantly different for the following pairs: \n#&gt; [1] \"No suggested reductions\"\n\nSe vengono identificate variabili ridondanti, sarà necessario rivedere il dataset, eliminando o modificando alcune variabili per ridurre la sovrapposizione.",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#stima-della-rete",
    "href": "chapters/networks/01_networks.html#stima-della-rete",
    "title": "24  Network psicologici",
    "section": "\n24.6 Stima della Rete",
    "text": "24.6 Stima della Rete\nUna volta verificato che i dati soddisfano le assunzioni necessarie, possiamo procedere alla stima della rete. Questo processo consiste nel quantificare le associazioni tra le variabili per analizzare come i loro valori si influenzano reciprocamente. Nelle reti psicologiche, le associazioni più comuni sono le correlazioni, che permettono di individuare relazioni dirette o condizionate tra le variabili (ad esempio, se alti livelli di motivazione sono associati ad alti livelli di coinvolgimento).\nLe associazioni possono essere stimate attraverso diverse misure, tra cui covarianze, correlazioni semplici, correlazioni parziali e modelli basati su regressioni. In questo contesto, ci concentriamo sulla stima delle correlazioni parziali regolarizzate, una tecnica molto utilizzata nelle analisi di rete psicologica.\n\n24.6.1 Correlazioni Parziali Regolarizzate\nLe correlazioni parziali regolarizzate offrono numerosi vantaggi e rappresentano uno standard nell’analisi delle reti psicologiche. Queste consentono di ottenere una struttura della rete interpretabile, evidenziando associazioni condizionate tra le variabili.\nVantaggi Principali:\n\n\nRecupero della struttura reale: Le correlazioni parziali regolarizzate aiutano a identificare le relazioni condizionate effettive, eliminando interferenze di altre variabili.\n\nSparsità: Forniscono una rete più chiara e leggibile, mantenendo solo gli archi più rilevanti.\n\nLa correlazione parziale misura l’associazione tra due variabili controllando l’effetto di tutte le altre variabili nella rete (ceteris paribus). Ad esempio, possiamo stimare l’associazione tra motivazione e coinvolgimento, escludendo l’influenza di variabili come successo accademico, ansia o benessere. Questo approccio permette di concentrarsi su relazioni specifiche e significative.\n\n24.6.2 Regolarizzazione\nLa regolarizzazione introduce una penalità per semplificare la complessità del modello di rete, contribuendo a eliminare associazioni spurie e a migliorare l’interpretabilità del risultato.\nVantaggi della Regolarizzazione:\n\n\nRiduzione degli archi spuri: Elimina associazioni false o deboli causate da rumore statistico o sovrapposizione.\n\nChiarezza: Imposta a zero i pesi degli archi trascurabili, producendo una rete meno densa e più interpretabile.\n\nAffidabilità: Riduce il rischio di errori di Tipo 1 (falsi positivi), mantenendo solo le associazioni più forti e significative.\n\nLa tecnica più comune per applicare la regolarizzazione è il LASSO (Least Absolute Shrinkage and Selection Operator), che penalizza la complessità della rete eliminando gli archi di importanza marginale. Questo approccio garantisce che la rete rappresenti fedelmente le relazioni chiave tra le variabili, riducendo il rumore e semplificando l’interpretazione.\nIn sintesi, la stima delle correlazioni parziali regolarizzate consente di costruire una rete psicologica robusta, chiara e focalizzata sulle relazioni essenziali tra variabili. Questo metodo combina l’efficacia delle correlazioni parziali con l’efficienza della regolarizzazione, fornendo una rappresentazione affidabile e interpretabile delle interazioni tra costrutti psicologici.",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#procedura-di-stima-della-rete",
    "href": "chapters/networks/01_networks.html#procedura-di-stima-della-rete",
    "title": "24  Network psicologici",
    "section": "\n24.7 Procedura di Stima della Rete",
    "text": "24.7 Procedura di Stima della Rete\nIl processo di stima utilizza la funzione estimateNetwork() del pacchetto bootnet, che richiede tre elementi fondamentali:\n\nIl dataset di input contenente le variabili da analizzare.\nIl metodo “EBICglasso” per la regolarizzazione della rete.\nIl calcolo automatico delle correlazioni tra variabili.\n\nLa funzione opera attraverso questi passaggi:\n\nGenera 100 modelli di rete differenti.\nValuta ogni modello utilizzando l’Extended Bayesian Information Criterion (EBIC).\nSeleziona il modello ottimale che bilancia precisione e parsimonia.\n\nIl parametro gamma (γ) controlla il livello di parsimonia della rete:\n\nγ = 0: produce una rete più densa con molte connessioni.\nγ = 0.5 (valore consigliato): mantiene solo le connessioni più rilevanti.\n\n\nallNetwork &lt;- estimateNetwork(\n    allData,\n    default = \"EBICglasso\",\n    corMethod = \"cor_auto\",\n    tuning = 0.5\n)\n\nQuesto codice produrrà una rete statistica ottimizzata che evidenzia le relazioni più importanti tra le variabili, eliminando le connessioni spurie o meno rilevanti.\n\n# Visualizzazione del risultato\nsummary(allNetwork)\n#&gt; \n#&gt; === Estimated network ===\n#&gt; Number of nodes: 6 \n#&gt; Number of non-zero edges: 15 / 15 \n#&gt; Mean weight: 0.14 \n#&gt; Network stored in object$graph \n#&gt;  \n#&gt; Default set used: EBICglasso \n#&gt;  \n#&gt; Use plot(object) to plot estimated network \n#&gt; Use bootnet(object) to bootstrap edge weights and centrality indices \n#&gt; \n#&gt; Relevant references:\n#&gt; \n#&gt;      Friedman, J. H., Hastie, T., & Tibshirani, R. (2008). Sparse inverse covariance estimation with the graphical lasso. Biostatistics, 9 (3), 432-441.\n#&gt;  Foygel, R., & Drton, M. (2010). Extended Bayesian information criteria for Gaussian graphical models. \n#&gt;  Friedman, J. H., Hastie, T., & Tibshirani, R. (2014). glasso: Graphical lasso estimation of gaussian graphical models. Retrieved from https://CRAN.R-project.org/package=glasso\n#&gt;  Epskamp, S., Cramer, A., Waldorp, L., Schmittmann, V. D., & Borsboom, D. (2012). qgraph: Network visualizations of relationships in psychometric data. Journal of Statistical Software, 48 (1), 1-18.\n#&gt;  Epskamp, S., Borsboom, D., & Fried, E. I. (2016). Estimating psychological networks and their accuracy: a tutorial paper. arXiv preprint, arXiv:1604.08462.\n\nIn sintesi, la funzione estimateNetwork() consente di costruire una rete psicologica affidabile e parsimoniosa, sfruttando il metodo delle correlazioni parziali regolarizzate con penalizzazione LASSO. Questo approccio garantisce un equilibrio ottimale tra complessità e accuratezza, rendendo la rete uno strumento efficace per analizzare le relazioni tra variabili.",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#creazione-del-grafico-della-rete",
    "href": "chapters/networks/01_networks.html#creazione-del-grafico-della-rete",
    "title": "24  Network psicologici",
    "section": "\n24.8 Creazione del Grafico della Rete",
    "text": "24.8 Creazione del Grafico della Rete\nLa visualizzazione della rete è un passaggio fondamentale per interpretare le relazioni tra variabili. Con la funzione plot(), possiamo creare un grafico chiaro e informativo. Per impostazione predefinita, il grafico utilizza un tema inclusivo per daltonici e rappresenta le associazioni condizionate tra i nodi con le seguenti caratteristiche:\n\n\nArchi blu: rappresentano correlazioni positive.\n\n\nArchi rossi: rappresentano correlazioni negative.\n\n\nSpessore degli archi: proporzionale alla magnitudine delle correlazioni parziali regolarizzate.\n\nAd esempio, il grafico può mostrare una forte associazione tra motivazione, autonomia e competenza, mentre le emozioni risultano strettamente legate alla competenza. Tutte le relazioni visualizzate nel grafico sono condizionate, ovvero tengono conto degli effetti di tutte le altre variabili nella rete, analogamente a quanto avviene in un’analisi di regressione.\n\nallDataPlot &lt;- plot(allNetwork)\nLX &lt;- allDataPlot$layout\n\n\n\n\n\n\n\nÈ possibile salvare il grafico in un oggetto R, come allDataPlot. Questo oggetto non solo memorizza il grafico, ma contiene anche informazioni utili, tra cui:\n\nLa matrice di correlazione.\nI parametri di configurazione del grafico.\nIl layout della rete (disposizione dei nodi), salvato in allDataPlot$layout. Questo layout può essere riutilizzato per mantenere una disposizione visiva coerente tra grafici di reti diverse, agevolando i confronti.\n\nPer migliorare la leggibilità e l’interpretazione, è utile personalizzare il grafico. Ecco alcuni esempi di opzioni disponibili:\n\n\nTitolo: Utilizzare l’argomento title per aggiungere un titolo descrittivo.\n\nDimensione dei nodi: Regolare con l’opzione vsize per migliorare la visibilità.\n\n\nPesi degli archi: Impostare edge.labels = TRUE per visualizzare i valori numerici delle correlazioni sul grafico.\n\n\nSoglia di visibilità degli archi:\n\n\ncut = 0.10: evidenzia gli archi con correlazioni superiori a 0.10, mentre gli altri saranno rappresentati con colori più tenui.\n\n\nminimum = 0.05: nasconde archi con valori inferiori a 0.05, riducendo il rumore visivo senza eliminarli dalla rete.\n\n\n\n\nLayout: Specificare un layout, ad esempio \"spring\", per posizionare automaticamente i nodi in modo intuitivo.\n\nEsempio di grafico personalizzato:\n\nallDataPlot &lt;- plot(\n    allNetwork,\n    title = \"Both countries combined\",\n    vsize = 9,\n    edge.labels = TRUE,\n    cut = 0.10,\n    minimum = 0.05,\n    layout = \"spring\"\n)\n\n\n\n\n\n\n\nPersonalizzare il grafico permette di:\n\nMigliorare la leggibilità, evidenziando le connessioni più rilevanti.\n\nRidurre il rumore visivo, concentrandosi su associazioni più robuste.\n\nGarantire coerenza visiva tra grafici di reti diverse, facilitando analisi comparative.\n\nLa possibilità di salvare e riutilizzare il layout o altre configurazioni consente di ottimizzare la comunicazione visiva, rendendo i risultati più chiari e facilmente interpretabili.",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#predicibilità-dei-nodi",
    "href": "chapters/networks/01_networks.html#predicibilità-dei-nodi",
    "title": "24  Network psicologici",
    "section": "\n24.9 Predicibilità dei Nodi",
    "text": "24.9 Predicibilità dei Nodi\nLa predicibilità di un nodo rappresenta la proporzione di varianza che può essere spiegata dalle connessioni di quel nodo con gli altri nodi della rete. È una misura chiave per comprendere l’influenza reciproca delle variabili all’interno della rete e per identificare nodi centrali o meno integrati.\nLa predicibilità si basa su una regressione lineare in cui:\n\nOgni nodo viene considerato come variabile dipendente.\nGli altri nodi della rete agiscono come predittori.\nSi calcola il coefficiente di determinazione (\\(R^2\\)), che indica la percentuale di varianza spiegata.\n\nLa predicibilità viene calcolata per ciascun nodo. Valori di \\(R^2\\) vicini a 0 indicano che il nodo è poco influenzato dalle sue connessioni, mentre valori vicini a 1 suggeriscono che il nodo è fortemente spiegato dalle sue relazioni.\n\n24.9.0.1 Interpretazione di \\(R^2\\)\n\n\n\n\\(R^2 = 0\\): Il nodo non è spiegato dalle sue connessioni. Questo potrebbe indicare che la variabile è marginale nella rete o mal misurata.\n\n\\(R^2 &gt; 0\\): Indica una connessione significativa con altri nodi, proporzionata al valore di \\(R^2\\).\n\n\\(R^2\\) molto alto (vicino a 1): Potrebbe suggerire una ridondanza del nodo rispetto ad altri oppure un modello sovrastimato.\n\n24.9.1 Predicibilità e Controllabilità\nLa predicibilità è strettamente collegata al concetto di controllabilità, cioè la capacità di influenzare un nodo tramite le sue connessioni con altri nodi. Un nodo con alta predicibilità è particolarmente sensibile ai cambiamenti delle variabili ad esso connesse, rendendolo un potenziale target per interventi mirati in ambiti come la psicologia clinica o l’educazione.\n\n24.9.2 Calcolo con il Pacchetto mgm\n\nPer stimare la predicibilità utilizziamo il pacchetto mgm, che permette di specificare il tipo di variabili incluse (es. gaussiane per variabili continue).\n\nfitAllData &lt;- mgm(\n  as.matrix(allData), \n  type = rep('g', 6) # Variabili gaussiane\n)\n#&gt; \n  |                                                                        \n  |                                                                  |   0%\n  |                                                                        \n  |-----------                                                       |  17%\n  |                                                                        \n  |----------------------                                            |  33%\n  |                                                                        \n  |---------------------------------                                 |  50%\n  |                                                                        \n  |--------------------------------------------                      |  67%\n  |                                                                        \n  |-------------------------------------------------------           |  83%\n  |                                                                        \n  |------------------------------------------------------------------| 100%\n#&gt; Note that the sign of parameter estimates is stored separately; see ?mgm\n\nSuccessivamente, possiamo calcolare la predicibilità per ciascun nodo:\n\npredictAll &lt;- predict(fitAllData, na.omit(allData))\npredictAll$errors$R2\n#&gt; [1] 0.139 0.518 0.442 0.315 0.458 0.126\n\nPer valutare la qualità esplicativa complessiva della rete, calcoliamo la predicibilità media:\n\nmean(predictAll$errors$R2)\n#&gt; [1] 0.333\n\nOltre a \\(R^2\\), possiamo esaminare l’Errore Quadratico Medio (RMSE), che misura la discrepanza tra i valori osservati e quelli previsti:\n\nmean(predictAll$errors$RMSE)\n#&gt; [1] 0.811\n\n\n24.9.3 Interpretazione dei Risultati\nL’analisi della predicibilità evidenzia quanto ciascun nodo sia integrato nella rete e fornisce indicazioni utili sul ruolo di ogni variabile:\n\nNodi con Alta Predicibilità: Variabili come competenza, motivazione e autonomia presentano valori di \\(R^2\\) elevati. Questi nodi sono fortemente influenzati dalle loro connessioni, indicando che sono centrali nella rete e ben integrati nel modello.\n\nNodi con Bassa Predicibilità: Variabili come apprendimento autoregolato (SRL) e relazionalità hanno \\(R^2\\) più bassi. Questi nodi potrebbero essere meno influenti o scarsamente connessi, suggerendo la necessità di ulteriori verifiche, ad esempio:\n\nLa variabile è meno centrale nel sistema studiato.\nMancano connessioni significative con altri nodi.\nPossono esserci problemi nella misurazione o nella definizione della variabile.\n\n\n\nLa predicibilità può anche essere rappresentata graficamente, con il valore di \\(R^2\\) visualizzato come grafici a torta all’interno dei nodi:\n\nallDataPlot &lt;- plot(\n    allNetwork,\n    title = \"Both countries combined\",\n    vsize = 9,\n    edge.labels = TRUE,\n    cut = 0.10,\n    minimum = 0.05,\n    pie = predictAll$errors$R2\n)\n\n\n\n\n\n\n\nL’analisi della predicibilità offre informazioni utili per:\n\n\nIdentificare target di intervento: Nodi con alta predicibilità (es. competenza e motivazione) sono particolarmente sensibili alle connessioni, rendendoli strategici per interventi mirati.\n\nRivedere nodi marginali: Nodi con bassa predicibilità (es. SRL e relazionalità) richiedono un’ulteriore esplorazione per comprendere meglio il loro ruolo nel sistema.\n\nLa combinazione di \\(R^2\\), RMSE e la rappresentazione grafica permette di ottenere una visione completa della rete, aiutando a individuare punti di forza e aree da approfondire.",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#inferenza-sulla-rete-misure-di-centralità",
    "href": "chapters/networks/01_networks.html#inferenza-sulla-rete-misure-di-centralità",
    "title": "24  Network psicologici",
    "section": "\n24.10 Inferenza sulla Rete: Misure di Centralità",
    "text": "24.10 Inferenza sulla Rete: Misure di Centralità\nLe misure di centralità consentono di identificare i nodi più influenti e importanti all’interno di una rete psicologica. Queste misure forniscono informazioni sul ruolo di ciascun nodo nella rete, aiutando a individuare variabili chiave che potrebbero rappresentare bersagli strategici per interventi o analisi approfondite.\n\n24.10.1 Principali Misure di Centralità\nTra le numerose misure disponibili, le seguenti sono le più comunemente utilizzate per le reti psicologiche grazie alla loro interpretazione chiara e al valore pratico:\n\n\nGrado di centralità (Degree Centrality): Indica il numero di connessioni dirette di un nodo, ignorando il peso degli archi.\n\nForza di centralità (Strength Centrality): Somma i pesi assoluti di tutte le connessioni di un nodo, evidenziando l’intensità complessiva delle sue relazioni.\n\nInfluenza attesa (Expected Influence): Somma i pesi grezzi (positivi e negativi) delle connessioni di un nodo, fornendo un’indicazione dell’effetto complessivo delle sue relazioni.\n\nAd esempio, per un nodo con connessioni 0.3, -0.1 e 0.5:\n\n\nDegree Centrality: 3 (numero di connessioni).\n\n\nStrength Centrality: \\(|0.3| + |-0.1| + |0.5| = 0.9\\) (somma dei pesi assoluti).\n\n\nExpected Influence: \\(0.3 + (-0.1) + 0.5 = 0.7\\) (somma dei pesi grezzi).\n\nSe nella rete non sono presenti archi negativi, la Strength Centrality e l’Expected Influence coincidono.\nAltre misure, come closeness, betweenness ed eigenvector centrality, possono essere calcolate, ma spesso hanno un’interpretazione meno diretta nel contesto delle reti psicologiche e non sono generalmente raccomandate per un’analisi standard.\n\n24.10.2 Calcolo e Visualizzazione delle Misure di Centralità\nPer stimare e visualizzare le misure di centralità, utilizziamo il pacchetto bootnet. La funzione centralityPlot() consente di creare un grafico con le misure selezionate, standardizzate come z-score per una migliore interpretazione visiva.\n\n24.10.2.1 Esempio di Visualizzazione\n\ncentralityPlot(\n    allNetwork,\n    include = c(\"ExpectedInfluence\", \"Strength\"),\n    scale = \"z-scores\"\n)\n\n\n\n\n\n\n\nSe si desiderano i valori numerici delle centralità, è possibile utilizzare la funzione centralityTable():\n\ncentralityTable(allNetwork)\n#&gt;      graph type        node           measure  value\n#&gt; 1  graph 1   NA Relatedness       Betweenness -0.617\n#&gt; 2  graph 1   NA  Competence       Betweenness  0.772\n#&gt; 3  graph 1   NA    Autonomy       Betweenness  1.697\n#&gt; 4  graph 1   NA     Emotion       Betweenness -0.617\n#&gt; 5  graph 1   NA  Motivation       Betweenness -0.617\n#&gt; 6  graph 1   NA         SRL       Betweenness -0.617\n#&gt; 7  graph 1   NA Relatedness         Closeness -1.144\n#&gt; 8  graph 1   NA  Competence         Closeness  0.721\n#&gt; 9  graph 1   NA    Autonomy         Closeness  1.206\n#&gt; 10 graph 1   NA     Emotion         Closeness -0.202\n#&gt; 11 graph 1   NA  Motivation         Closeness  0.578\n#&gt; 12 graph 1   NA         SRL         Closeness -1.158\n#&gt; 13 graph 1   NA Relatedness          Strength -1.074\n#&gt; 14 graph 1   NA  Competence          Strength  1.224\n#&gt; 15 graph 1   NA    Autonomy          Strength  0.634\n#&gt; 16 graph 1   NA     Emotion          Strength -0.310\n#&gt; 17 graph 1   NA  Motivation          Strength  0.695\n#&gt; 18 graph 1   NA         SRL          Strength -1.170\n#&gt; 19 graph 1   NA Relatedness ExpectedInfluence -1.003\n#&gt; 20 graph 1   NA  Competence ExpectedInfluence  1.220\n#&gt; 21 graph 1   NA    Autonomy ExpectedInfluence  0.649\n#&gt; 22 graph 1   NA     Emotion ExpectedInfluence -0.371\n#&gt; 23 graph 1   NA  Motivation ExpectedInfluence  0.708\n#&gt; 24 graph 1   NA         SRL ExpectedInfluence -1.203\n\n\n24.10.3 Misure Avanzate con NetworkToolbox\n\nIl pacchetto NetworkToolbox offre un’ampia gamma di misure di centralità, tra cui:\n\n\nDegree Centrality: Numero di connessioni dirette di un nodo.\n\nStrength Centrality: Intensità complessiva delle connessioni.\n\nCloseness Centrality: Misura della vicinanza di un nodo a tutti gli altri.\n\nEigenvector Centrality: Valuta l’importanza di un nodo considerando anche l’importanza dei suoi vicini.\n\nLeverage Centrality: Peso relativo delle connessioni di un nodo rispetto ai suoi vicini.\n\n\nDegree &lt;- degree(allNetwork$graph)\nStrength &lt;- strength(allNetwork$graph)\nBetweenness &lt;- betweenness(allNetwork$graph)\nCloseness &lt;- closeness(allNetwork$graph)\nEigenvector &lt;- eigenvector(allNetwork$graph)\nLeverage &lt;- leverage(allNetwork$graph)\n\ndata.frame(\n    Variable = names(Degree),\n    Degree,\n    Strength,\n    Betweenness,\n    Closeness,\n    Eigenvector,\n    Leverage\n)\n#&gt;                Variable Degree Strength Betweenness Closeness Eigenvector\n#&gt; Relatedness Relatedness      5     0.40           0      1.89       0.217\n#&gt; Competence   Competence      5     1.07           6      3.27       0.553\n#&gt; Autonomy       Autonomy      5     0.90          10      3.63       0.475\n#&gt; Emotion         Emotion      5     0.62           0      2.58       0.363\n#&gt; Motivation   Motivation      5     0.91           0      3.16       0.495\n#&gt; SRL                 SRL      5     0.37           0      1.88       0.211\n#&gt;             Leverage\n#&gt; Relatedness   -3.429\n#&gt; Competence     1.349\n#&gt; Autonomy       1.066\n#&gt; Emotion       -0.425\n#&gt; Motivation     1.105\n#&gt; SRL           -5.379\n\nInterpretazione delle misure:\n\n\nGrado e forza di centralità: Indicazioni semplici e dirette del numero e della forza delle connessioni di un nodo.\n\nInfluenza attesa: Misura più raffinata che considera il bilancio complessivo delle connessioni, includendo sia pesi positivi sia negativi.\n\nMisure avanzate: Strumenti utili in analisi specifiche, ma da usare con cautela in contesti psicologici standard, poiché la loro interpretazione può risultare meno intuitiva.\n\n24.10.4 Implicazioni Pratiche\nLe misure di centralità sono strumenti cruciali per:\n\n\nIndividuare target di intervento: Nodi con elevata forza o influenza attesa possono rappresentare variabili chiave su cui focalizzarsi.\n\nValutare l’integrazione dei nodi nella rete: Nodi con basse misure di centralità possono essere marginali o meno influenti, suggerendo potenziali problemi nella definizione o nella misurazione della variabile.\n\nLe misure calcolate forniscono una visione approfondita della struttura della rete, guidando analisi teoriche e applicative in contesti come la psicologia clinica, l’educazione e la ricerca sui sistemi complessi.",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#altre-opzioni-per-la-stima-delle-reti",
    "href": "chapters/networks/01_networks.html#altre-opzioni-per-la-stima-delle-reti",
    "title": "24  Network psicologici",
    "section": "\n24.11 Altre Opzioni per la Stima delle Reti",
    "text": "24.11 Altre Opzioni per la Stima delle Reti\nCome accennato in precedenza, oltre alle reti basate su correlazioni parziali regolarizzate, esistono diverse altre opzioni di stima delle reti. Di seguito ne presentiamo alcune, ma per ulteriori dettagli si consiglia di consultare le pagine del manuale della funzione estimateNetwork().\n\n24.11.1 Rete di Associazione\nLa rete di associazione (correlation network) si basa sulle correlazioni semplici tra le variabili. Questo tipo di rete è utile principalmente per esplorazioni preliminari dei dati, ma non è generalmente raccomandata per analisi definitive, poiché tende a produrre reti molto dense con molteplici connessioni non significative.\n\nallNetwork_cor &lt;- estimateNetwork(allData,\n    default = \"cor\", verbose = FALSE\n)\n\n\n24.11.2 Metodo ggmModSelect()\n\nIl metodo ggmModSelect() è particolarmente indicato per dataset di grandi dimensioni con un numero ridotto di nodi. Funziona in questo modo:\n\nParte da una rete regolarizzata come punto di riferimento iniziale.\nStima tutte le possibili reti non regolarizzate.\nSeleziona il modello migliore in base al criterio EBIC (Extended Bayesian Information Criterion), scegliendo quello con il valore più basso.\n\nQuesto approccio combina i vantaggi della regolarizzazione con la flessibilità di reti non regolarizzate, risultando in un modello più accurato per dataset specifici.\n\nallNetwork_mgm &lt;- estimateNetwork(allData,\n    default = \"ggmModSelect\", verbose = FALSE\n)\n\n\n24.11.3 Rete di Importanza Relativa\nLa rete di importanza relativa (relimp; Relative Importance Network) stima una rete direzionale basata sull’importanza relativa dei predittori in un modello di regressione lineare. In questa rete:\n\nGli archi rappresentano la magnitudine dell’importanza relativa di ciascun predittore.\nLa direzione degli archi indica come ciascuna variabile influenza le altre, secondo i risultati della regressione.\n\nQuesta rete è utile per identificare relazioni causali teoriche o per evidenziare come alcune variabili predicono altre all’interno del dataset.\n\nallNetwork_relimp &lt;- estimateNetwork(allData,\n    default = \"relimp\", verbose = FALSE\n)\n\n\n24.11.4 Confronto tra i Modelli\n\nLa rete ggmModSelect() produce risultati molto simili alla rete regolarizzata standard, con differenze minime nelle connessioni più deboli.\nLa rete di associazione è invece molto densa, poiché include tutte le correlazioni, senza applicare alcuna penalizzazione.\nLa rete di importanza relativa è direzionale e fornisce informazioni aggiuntive su come una variabile influenza le altre.\n\n\nplot(\n    allNetwork_cor, title = \"Correlation\", vsize = 18, edge.labels = TRUE, \n    cut = 0.10, minimum = 0.05, layout = LX\n)\n\n\n\n\n\n\n\n\nplot(\n    allNetwork, title = \"EBICglasso\", vsize = 18, edge.labels = TRUE,\n    cut = 0.10, minimum = 0.05, layout = LX\n)\n\n\n\n\n\n\n\n\nplot(\n    allNetwork_mgm, title = \"ggmModSelect\", vsize = 18, edge.labels = TRUE,\n    cut = 0.10, minimum = 0.05, layout = LX\n)\n\n\n\n\n\n\n\n\nplot(\n    allNetwork_relimp, title = \"Relative importance\", vsize = 18, \n    edge.labels = TRUE, cut = 0.10, minimum = 0.05, layout = LX\n)",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#confronto-tra-reti",
    "href": "chapters/networks/01_networks.html#confronto-tra-reti",
    "title": "24  Network psicologici",
    "section": "\n24.12 Confronto tra Reti",
    "text": "24.12 Confronto tra Reti\nDopo aver illustrato i passaggi fondamentali per stimare una singola rete, procediamo ora al confronto tra diverse reti. Le reti psicologiche offrono metodi per confrontare le reti nel loro complesso, i pesi degli archi (edge weights) e le misure di centralità.\nNel nostro caso, disponendo di dati provenienti da due paesi (Finlandia e Austria), possiamo stimare due reti separate, una per ciascun paese, e confrontarle per osservare come differiscono.\nPer confrontare le reti, dobbiamo innanzitutto stimarle separatamente per ciascun paese. I passaggi di stima sono gli stessi descritti in precedenza. Per ciascun paese, iniziamo con i passi seguenti:\n\nControllo delle assunzioni per ciascun dataset (ad esempio, verifica che la matrice di correlazione sia positiva definita).\nVerifica con l’algoritmo goldbricker per identificare nodi altamente simili che potrebbero necessitare di essere combinati o ridotti. In questo caso, i risultati mostrano che:\nLe matrici di correlazione di entrambe le reti sono già positive definite.\nNon ci sono nodi altamente simili che richiedono riduzioni.\n\n\n### Check the assumptions\n## Finland\n# check for positive definitiveness\ncorrelationMatrix &lt;- cor(\n    x = finlandData, \n    use = c(\"pairwise.complete.obs\")\n)\n\nis.positive.definite(correlationMatrix)\n#&gt; [1] TRUE\n# check for redundancy\ngoldbricker(finlandData,\n    p = 0.05, method = \"hittner2003\",\n    threshold = 0.25, corMin = 0.5, progressbar = FALSE\n)\n#&gt; Suggested reductions: Less than 25 % of correlations are significantly different for the following pairs: \n#&gt; [1] \"No suggested reductions\"\n\n\n## Austria\n# check for positive definitiveness\ncorrelationMatrix &lt;- cor(\n    x = austriaData, \n    use = c(\"pairwise.complete.obs\")\n)\n\nis.positive.definite(correlationMatrix)\n#&gt; [1] TRUE\n# check for redundancy\ngoldbricker(austriaData,\n    p = 0.05, method = \"hittner2003\",\n    threshold = 0.25, corMin = 0.5, progressbar = FALSE\n)\n#&gt; Suggested reductions: Less than 25 % of correlations are significantly different for the following pairs: \n#&gt; [1] \"No suggested reductions\"\n\nStima dei networks:\n\n# Estimate the networks\nfinlandNetwork &lt;- estimateNetwork(\n    finlandData,\n    default = \"EBICglasso\", corMethod = \"cor_auto\", tuning = 0.5\n)\n\naustriaNetwork &lt;- estimateNetwork(\n    austriaData,\n    default = \"EBICglasso\", corMethod = \"cor_auto\", tuning = 0.5\n)\n\nCalcoliamo la predicibilità per ciascun paese.\n\n# Compute the predictability\nfitFinland &lt;- mgm(\n    as.matrix(finlandData), # data\n    c(\"g\", \"g\", \"g\", \"g\", \"g\", \"g\"),\n    # distribution for each var\n    verbatim = TRUE, # hide warnings and progress bar\n    signInfo = FALSE # hide message about signs\n)\n\n\npredictFinland &lt;- predict(fitFinland, na.omit(finlandData))\nmean(predictFinland$errors$R2)\n#&gt; [1] 0.308\nmean(predictFinland$errors$RMSE)\n#&gt; [1] 0.828\n\n\nfitAustria &lt;- mgm(\n    as.matrix(austriaData), # data\n    c(\"g\", \"g\", \"g\", \"g\", \"g\", \"g\"),\n    # distribution for each var\n    verbatim = TRUE, # hide warnings and progress bar\n    signInfo = FALSE # hide message about signs\n)\n\n\npredictAustria &lt;- predict(fitAustria, na.omit(austriaData))\nmean(predictAustria$errors$R2)\n#&gt; [1] 0.344\nmean(predictAustria$errors$RMSE)\n#&gt; [1] 0.804\n\nDopo aver stimato le reti, possiamo visualizzarle per facilitare il confronto. Questo approccio consente di identificare rapidamente le differenze e le somiglianze tra le reti dei due paesi.\n\nAverageLayout &lt;- averageLayout(finlandNetwork, austriaNetwork)\n\n\nplot(finlandNetwork, # input network\n    title = \"Finland\", # plot title\n    vsize = 19, # size of the nodes\n    edge.labels = TRUE, # label the edge weights\n    cut = 0.10, # saturate edges &gt; .10\n    minimum = 0.05, # remove edges &lt; .05\n    pie = predictFinland$errors$R2, # put R2 as pie\n    layout = LX\n) # set the layout\n\n\n\n\n\n\n\n\nplot(austriaNetwork, # input network\n    title = \"Austria\", # plot title\n    vsize = 19, # size of the nodes\n    edge.labels = TRUE, # label the edge weights\n    cut = 0.10, # saturate edges &gt; .10\n    minimum = 0.05, # remove edges &lt; .05\n    pie = predictAustria$errors$R2, # put R2 as pie\n    layout = LX\n) # set the layout\n\n\n\n\n\n\n\nPossiamo rappresentare graficamente la differenza tra le due reti usando qgraph(). La funzione qgraph() richiede come input una rete stimata o una matrice. Per creare una rete di differenza, è necessario sottrarre le due matrici delle connessioni delle reti (ad esempio, finlandNetwork$graph - austriaNetwork$graph).\nIl seguente codice mostra come visualizzare la rete di differenza che evidenzia le variazioni nei pesi delle connessioni tra le due reti.\n\nqgraph(finlandNetwork$graph - abs(austriaNetwork$graph),\n    title = \"Difference\", # plot title\n    theme = allDataPlot$Arguments$theme,\n    vsize = 19, # size of the nodes\n    edge.labels = TRUE, # label the edge weights\n    labels = allDataPlot$Arguments$labels, # node labels\n    cut = 0.10, # saturate edges &gt; .10\n    layout = LX\n) # set the layout\n\n\n\n\n\n\n\nIl confronto tra le reti evidenzia differenze tra i due paesi:\n\n\nFinlandia:\n\nConnessione più forte tra competenza ed emozione.\nConnessione più forte tra motivazione e relazionalità.\n\n\n\nAustria:\n\nConnessioni più forti tra:\n\n\nMotivazione e competenza.\n\nMotivazione ed emozione.\n\nCompetenza e autonomia.\n\nAutonomia e relazionalità.\n\n\n\n\n\nQueste differenze suggeriscono che le relazioni psicologiche tra le variabili possono essere influenzate da fattori specifici di ciascun contesto culturale o sociale. La rete di differenza rappresenta un utile strumento visivo per evidenziare queste variazioni e guidare l’interpretazione.\nUn confronto visivo delle centralità può essere effettuato nello stesso modo descritto in precedenza. Per farlo, forniamo le reti che vogliamo confrontare come una lista e specifichiamo le misure di centralità da calcolare.\nI risultati mostrano che:\n\nNella rete dell’Austria, la variabile con il valore di centralità più alto è la motivazione, indicando che la motivazione è il fattore principale che guida la connettività nella rete.\nNella rete della Finlandia, la variabile più centrale è la competenza, che risulta essere il driver principale della connettività della rete.\n\nQuesto confronto mette in evidenza come le variabili centrali differiscano tra i due contesti, suggerendo che fattori culturali o ambientali possono influenzare il ruolo delle variabili psicologiche nella struttura della rete.\n\ncentralityPlot(\n    list(\n        Finland = finlandNetwork,\n        Austria = austriaNetwork\n    ),\n    include = c(\"ExpectedInfluence\", \"Strength\")\n)",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#confronto-statistico-tra-reti",
    "href": "chapters/networks/01_networks.html#confronto-statistico-tra-reti",
    "title": "24  Network psicologici",
    "section": "\n24.13 Confronto Statistico tra Reti",
    "text": "24.13 Confronto Statistico tra Reti\nPer confrontare in modo rigoroso le reti, è necessario utilizzare un test statistico che permetta di stabilire quali differenze nei pesi degli archi o nelle misure di centralità siano significative e non dovute al caso. Il Network Comparison Test (NCT) è uno strumento per effettuare un confronto dettagliato della struttura delle reti, dei pesi degli archi e delle centralità.\nL’NCT utilizza un approccio basato su permutazioni:\n\nGenera un grande numero di reti permutate a partire dalle reti originali, creando una distribuzione di riferimento.\nConfronta le reti originali con quelle permutate per determinare se le differenze osservate sono statisticamente significative.\n\nPer eseguire il test, dobbiamo:\n\nFornire le due reti da confrontare.\nSpecificare il numero di iterazioni (almeno 1000 è raccomandato per risultati affidabili).\nTestare gli archi con test.edges = TRUE e edges = 'all' per verificare tutte le connessioni.\nTestare le centralità con test.centrality = TRUE (poiché non vengono testate di default).\n\n\nset.seed(1337)\n\nCompared &lt;- NCT(\n    finlandNetwork, # network 1\n    austriaNetwork, # network 2\n    verbose = FALSE, # hide warnings and progress bar\n    it = 1000, # number of iterations\n    abs = TRUE,\n    binary.data = FALSE, # set data distribution\n    test.edges = TRUE, # test edge differences\n    edges = 'all', # which edges to test\n    test.centrality = TRUE, # test centrality\n    progressbar = FALSE # progress bar\n)\n\n\nCompared$glstrinv.sep # Separate global strength values of the individual networks\n#&gt; [1] 2.15 2.17\n\n\nCompared$einv.pvals # Holm-Bonferroni adjusted p-values for each edge\n#&gt;           Var1       Var2  p-value Test statistic E\n#&gt; 7  Relatedness Competence 0.017982           0.0681\n#&gt; 13 Relatedness   Autonomy 0.001998           0.0952\n#&gt; 14  Competence   Autonomy 0.000999           0.1348\n#&gt; 19 Relatedness    Emotion 0.165834           0.0414\n#&gt; 20  Competence    Emotion 0.000999           0.1713\n#&gt; 21    Autonomy    Emotion 0.009990           0.0767\n#&gt; 25 Relatedness Motivation 0.000999           0.1371\n#&gt; 26  Competence Motivation 0.000999           0.1605\n#&gt; 27    Autonomy Motivation 0.003996           0.0876\n#&gt; 28     Emotion Motivation 0.001998           0.1287\n#&gt; 31 Relatedness        SRL 0.077922           0.0467\n#&gt; 32  Competence        SRL 0.000999           0.1258\n#&gt; 33    Autonomy        SRL 0.688312           0.0123\n#&gt; 34     Emotion        SRL 0.073926           0.0530\n#&gt; 35  Motivation        SRL 0.301698           0.0306\n\n\nCompared$diffcen.real # Difference in centralities\n#&gt;             strength expectedInfluence\n#&gt; Relatedness    0.105             0.105\n#&gt; Competence     0.070             0.070\n#&gt; Autonomy      -0.229            -0.229\n#&gt; Emotion        0.127             0.214\n#&gt; Motivation    -0.209            -0.209\n#&gt; SRL            0.088             0.175\n\n\nCompared$diffcen.pval # Holm-Bonferroni adjusted p-values for each centrality\n#&gt;             strength expectedInfluence\n#&gt; Relatedness 0.006993          0.006993\n#&gt; Competence  0.127872          0.127872\n#&gt; Autonomy    0.000999          0.000999\n#&gt; Emotion     0.009990          0.000999\n#&gt; Motivation  0.000999          0.000999\n#&gt; SRL         0.132867          0.000999\n\n\n24.13.1 Interpretazione dei Risultati\nForza Globale delle Reti. Finlandia: 2.15, Austria: 2.17. La differenza tra le due reti (\\(\\Delta = 0.024\\)) non è statisticamente significativa, indicando che le reti hanno una connettività complessiva simile.\nDifferenze nei Pesi degli Archi. Gli archi Competence-Autonomy, Competence-Emotion, Competence-Motivation, Relatedness-Competence e Relatedness-Motivation mostrano differenze statisticamente significative (\\(p &lt; 0.05\\)). Archi come Relatedness-Emotion e Autonomy-SRL non mostrano differenze significative.\nDifferenze nelle Centralità. Autonomy e Motivation hanno differenze significative sia in strength che in expected influence (\\(p &lt; 0.001\\)), suggerendo che il loro ruolo nella rete varia notevolmente tra i due paesi. Relatedness e Emotion mostrano differenze significative (\\(p &lt; 0.01\\)). Competence e SRL non presentano differenze significative nelle centralità.\nIn conclusione, le reti di Finlandia e Austria hanno una forza globale simile, ma mostrano differenze significative in alcune connessioni chiave e centralità. Competence, Autonomy, e Motivation giocano ruoli diversi nei due contesti culturali. Le differenze nei pesi degli archi e nelle centralità suggeriscono influenze specifiche di fattori contestuali o culturali.",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#la-rete-della-variabilità",
    "href": "chapters/networks/01_networks.html#la-rete-della-variabilità",
    "title": "24  Network psicologici",
    "section": "\n24.14 La Rete della Variabilità",
    "text": "24.14 La Rete della Variabilità\nLa rete della variabilità fornisce un’indicazione di come i pesi degli archi (le connessioni tra nodi) variano tra le reti. In altre parole, questa rete riflette il grado di variabilità o le differenze individuali presenti nella popolazione analizzata.\n\n\nArchi con bassa variabilità: Indicano che le connessioni sono simili tra le reti, ovvero stabili e consistenti.\n\nArchi con alta variabilità: Indicano che le connessioni differiscono significativamente tra le reti, suggerendo la presenza di differenze individuali o contestuali.\n\nPer costruire la rete della variabilità, calcoliamo la deviazione standard dei pesi degli archi tra le due reti. Il processo include:\n\nLa creazione di due matrici, una per ciascuna rete.\nUn ciclo che calcola la deviazione standard per ogni arco tra le due reti.\n\n\n# Construct a network where edges are standard deviations across edge weights of networks\nedgeMeanJoint &lt;- matrix(0, 6, 6)\nedgeSDJoint &lt;- matrix(0, 6, 6)\nfor (i in 1:6) {\n    for (j in 1:6) {\n        vector &lt;- c(getWmat(finlandNetwork)[i, j], getWmat\n        (austriaNetwork)[i, j])\n        edgeMeanJoint[i, j] &lt;- mean(vector)\n        edgeSDJoint[i, j] &lt;- sd(vector)\n    }\n}\n\nSuccessivamente, tracciamo le reti in cui i pesi degli archi rappresentano le deviazioni standard di tutti gli archi.\n\nqgraph(edgeSDJoint,\n    layout = LX, edge.labels = TRUE,\n    labels = allDataPlot$Arguments$labels, vsize = 9,\n    cut = 0.09, minimum = 0.01, theme = \"colorblind\"\n)\n\n\n\n\n\n\n\nAllo stesso modo in cui abbiamo confrontato i paesi, possiamo confrontare i generi. Come mostrato nel seguente blocco di codice, stimiamo la rete per il gruppo maschile, quella per il gruppo femminile e la rete di differenza. Le differenze risultano molto piccole o addirittura trascurabili.\n\nmaleNetwork &lt;- estimateNetwork(maleData, default = \"EBICglasso\")\nfemaleNetwork &lt;- estimateNetwork(femaleData, default = \"EBICglasso\")\n\nplot(maleNetwork,\n    title = \"Male\", vsize = 9, edge.labels = TRUE,\n    cut = 0.10, minimum = 0.05, layout = LX\n)\n\nplot(femaleNetwork,\n    title = \"Female\", vsize = 9,\n    edge.labels = TRUE,\n    cut = 0.10, minimum = 0.05, layout = LX\n)\n\nqgraph(femaleNetwork$graph - maleNetwork$graph,\n    title =\n        \"Difference\", cut = 0.1,\n    labels = allDataPlot$Arguments$labels, vsize = 9,\n    minimum = 0.01,\n    edge.labels = TRUE, layout = LX, theme =\n        \"colorblind\"\n)\n\nDi seguito eseguiamo il Network Comparison Test (NCT) e osserviamo che i valori \\(p\\) relativi alle differenze tra tutti gli archi non sono statisticamente significativi.\n\nComparedGender &lt;- NCT(\nmaleNetwork, # network 1\nfemaleNetwork, # network 2\nverbose = FALSE, # hide warnings and progress bar\nit = 1000, # number of iterations\nabs = T, # test strength or expected influence?\nbinary.data = FALSE, # set data distribution\ntest.edges = TRUE, # test edge differences\nedges = 'all', # which edges to test\nprogressbar = FALSE) # progress bar\nComparedGender$einv.pvals # Holm-Bonferroni adjusted p-values for each edge",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#valutazione-della-robustezza-e-accuratezza",
    "href": "chapters/networks/01_networks.html#valutazione-della-robustezza-e-accuratezza",
    "title": "24  Network psicologici",
    "section": "\n24.15 Valutazione della Robustezza e Accuratezza",
    "text": "24.15 Valutazione della Robustezza e Accuratezza\nIl metodo più comune per valutare la stabilità e l’accuratezza delle reti stimate è il bootstrapping. Questo procedimento prevede la creazione di un grande numero di reti bootstrappate (almeno 1000) a partire dai dati originali.\nPassaggi del Bootstrapping:\n\nSi generano un numero elevato di reti bootstrappate basate sui dati originali.\nSi calcolano i pesi degli archi per ciascuna di queste reti.\nSi utilizzano i pesi degli archi delle reti bootstrappate per costruire intervalli di confidenza che rappresentano l’accuratezza degli archi.\n\nInterpretazione:\n\nPer ogni arco nella rete stimata, i pesi vengono confrontati con gli intervalli di confidenza generati dalle reti bootstrappate.\nUn arco è considerato statisticamente significativo se i limiti superiore e inferiore dell’intervallo di confidenza non includono lo zero.\nAl contrario, un arco è non significativo se uno dei limiti dell’intervallo di confidenza attraversa la linea dello zero.\n\nQuesto approccio consente di identificare quali archi sono robusti e quali potrebbero essere il risultato di variabilità casuale nei dati.\n\nnCores &lt;- parallel::detectCores() - 1\n# Non-parametric bootstrap for stability of edges and of edge differences\n\nallBoot &lt;- bootnet(\n    allNetwork, # network input\n    default = \"EBICglasso\", # method\n    nCores = nCores, # number of cores for parallelization\n    computeCentrality = FALSE, # estimate centrality?\n    statistics = \"edge\" # what statistics do we want?\n)\n\n\nplot(allBoot,\n    plot = \"area\", order = \"sample\", legend = FALSE\n)\n\n\n\n\n\n\n\nCome mostrato nella Figura precedente, solo gli archi autonomy-emotion ed emotion-SRL attraversano la linea dello zero e, pertanto, non sono significativi.\nPossiamo inoltre tracciare il grafico delle differenze tra gli archi, che verifica se i pesi degli archi differiscono significativamente tra loro.\n\nplot(allBoot,\n    plot = \"difference\", order = \"sample\",\n    onlyNonZero = FALSE, labels = TRUE\n)\n\nInterpretazione del Grafico delle Differenze tra gli Archi\n\n\nQuadrati grigi: Indicano che l’intervallo di confidenza al 95% ottenuto dal bootstrapping per la differenza tra due archi attraversa la linea dello zero, suggerendo che la differenza non è statisticamente significativa.\n\nQuadrati neri: Indicano che l’intervallo di confidenza non attraversa lo zero, quindi la differenza tra i due archi è significativa.\n\nAd esempio:\n\nGli archi autonomy-emotion ed emotion-SRL presentano un quadrato grigio, indicando una differenza non significativa.\nGli archi emotion-SRL e relatedness-SRL presentano un quadrato nero, indicando che i due archi differiscono significativamente.\n\nL’accuratezza delle misure di centralità viene valutata tramite il case dropping test. In questo test, vengono eliminate diverse proporzioni di casi dai dati, e si calcola la correlazione tra la misura di centralità osservata e quella ottenuta dai dati ridotti. Se la correlazione diminuisce significativamente dopo l’eliminazione di un piccolo sottoinsieme di casi, la misura di centralità è considerata non affidabile.\n\nset.seed(1)\ncentBoot &lt;- bootnet(\n    allNetwork, # network input\n    default = \"EBICglasso\", # method\n    type = \"case\", # method for testing centrality stability\n    nCores = nCores, # number of cores\n    computeCentrality = TRUE, # compute centrality\n    statistics = c(\"strength\", \"expectedInfluence\"),\n    nBoots = 19000, # number of bootstraps\n    caseMin = .05, # min cases to drop\n    caseMax = .95 # max cases to drop\n)\n\nIl coefficiente di stabilità della correlazione è una metrica utilizzata per valutare la stabilità delle misure di centralità attraverso il case dropping test. Esso viene stimato come la massima proporzione di casi che può essere eliminata mantenendo una correlazione di almeno 0.7 con il campione originale.\n\ncorStability(centBoot)\n#&gt; === Correlation Stability Analysis === \n#&gt; \n#&gt; Sampling levels tested:\n#&gt;    nPerson Drop%    n\n#&gt; 1      358    95 1860\n#&gt; 2     1074    85 1932\n#&gt; 3     1790    75 1884\n#&gt; 4     2506    65 1893\n#&gt; 5     3222    55 1950\n#&gt; 6     3938    45 1904\n#&gt; 7     4654    35 1887\n#&gt; 8     5370    25 1860\n#&gt; 9     6086    15 1834\n#&gt; 10    6802     5 1996\n#&gt; \n#&gt; Maximum drop proportions to retain correlation of 0.7 in at least 95% of the samples:\n#&gt; \n#&gt; expectedInfluence: 0.95 (CS-coefficient is highest level tested)\n#&gt;   - For more accuracy, run bootnet(..., caseMin = 0.85, caseMax = 1) \n#&gt; \n#&gt; strength: 0.95 (CS-coefficient is highest level tested)\n#&gt;   - For more accuracy, run bootnet(..., caseMin = 0.85, caseMax = 1) \n#&gt; \n#&gt; Accuracy can also be increased by increasing both 'nBoots' and 'caseN'.\n\nSe tracciamo i risultati, possiamo osservare che il coefficiente di stabilità della correlazione è pari a 0.95, un valore molto elevato che indica un’alta stabilità degli archi.\n\nplot(centBoot)",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#riflessioni-conclusive",
    "href": "chapters/networks/01_networks.html#riflessioni-conclusive",
    "title": "24  Network psicologici",
    "section": "\n24.16 Riflessioni Conclusive",
    "text": "24.16 Riflessioni Conclusive\nIl campo delle reti psicologiche è in continua evoluzione, con metodi sempre più raffinati e applicazioni in costante crescita. In questo capitolo abbiamo esplorato i passaggi fondamentali per analizzare una rete psicologica, visualizzarne i risultati e confrontare diverse reti, oltre a introdurre tecniche statistiche robuste come il bootstrapping per valutare l’accuratezza delle reti stimate.\nUn tema rilevante è il confronto tra le reti psicologiche e altri approcci analitici. Ad esempio, l’Epistemic Network Analysis (ENA) presenta alcune limitazioni rispetto all’analisi delle reti psicologiche:\n\nNon offre strumenti per verificare se i pesi degli archi si discostano dal caso.\nNon include metodi rigorosi per confrontare reti o valutare i pesi degli archi.\nNon prevede misure di centralità o altre metriche tipiche delle reti.\n\nAllo stesso modo, il process mining, che genera reti di transizione, è limitato nella sua capacità di fornire test statistici per confermare la validità dei modelli o confrontarli con altri. L’analisi delle reti sociali (SNA), pur essendo più affine alle reti psicologiche, si concentra prevalentemente su archi non direzionati e positivi, utilizzati per analizzare interazioni sociali o semantiche, risultando meno flessibile per studiare le dipendenze complesse tra variabili psicologiche.\nLe reti psicologiche si distinguono per la loro capacità di rappresentare in modo dettagliato le interazioni e le dipendenze tra variabili. Grazie a una vasta gamma di metodi di stima e tecniche di ottimizzazione, offrono una prospettiva unica e ricca di possibilità analitiche. La comunità scientifica che sostiene questo approccio è particolarmente dinamica, contribuendo con continue innovazioni.\nUn ulteriore punto di forza è la flessibilità: le reti psicologiche non richiedono una teoria predefinita o assunzioni rigide sulle variabili. Questo le rende strumenti estremamente versatili, adatti sia per esplorazioni teoriche sia per analisi applicate. Come sottolineato da Borsboom et al., le reti psicologiche costituiscono un “ponte naturale tra l’analisi dei dati e la formulazione di teorie basate sui principi della scienza delle reti,” aprendo la strada alla generazione di ipotesi causali.\nIn conclusione, le reti psicologiche rappresentano uno strumento potente e innovativo per l’analisi dei sistemi complessi. Offrono nuove modalità per esplorare le interazioni tra variabili psicologiche, superando molte delle limitazioni di altri approcci. La loro capacità di integrare analisi empiriche e teorie emergenti le rende un contributo fondamentale per il progresso della ricerca psicologica, favorendo la scoperta di nuovi schemi interpretativi e la formulazione di ipotesi teoriche di grande impatto.\nRisorse Raccomandate\n\nEpskamp, S., Borsboom, D., & Fried, E. I. (2018). Estimating psychological networks and their accuracy: A tutorial paper. Behavior Research Methods, 50(1), 195–212. https://doi.org/10.3758/s13428-017-0862-1\nEpskamp, S., & Fried, E. I. (2018). A tutorial on regularized partial correlation networks. Psychological Methods, 23(4), 617–634. https://doi.org/10.1037/met0000167\nVan Borkulo, C. D., Van Bork, R., Boschloo, L., Kossakowski, J. J., Tio, P., Schoevers, R. A., & Waldorp, L. J. (2022). Comparing network structures on three aspects: A permutation test. Psychological Methods. https://doi.org/10.1037/met0000427\nBorsboom, D., Deserno, M. K., Rhemtulla, M., Epskamp, S., Fried, E. I., McNally, R. J., & Waldorp, L. J. (2021). Network analysis of multivariate data in psychological science. Nature Reviews Methods Primers, 1(1), 58. https://doi.org/10.1038/s43586-021-00055-w\nBringmann, L. F., Elmer, T., Epskamp, S., Krause, R. W., Schoch, D., Wichers, M., & Snippe, E. (2019). What do centrality measures measure in psychological networks? Journal of Abnormal Psychology, 128(8), 892–903. https://doi.org/10.1037/abn0000446",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/networks/01_networks.html#informazioni-sullambiente-di-sviluppo",
    "title": "24  Network psicologici",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] matrixcalc_1.0-6            mgm_1.2-14                 \n#&gt;  [3] qgraph_1.9.8                NetworkComparisonTest_2.2.2\n#&gt;  [5] NetworkToolbox_1.4.2        networktools_1.5.2         \n#&gt;  [7] bootnet_1.6                 rio_1.2.3                  \n#&gt;  [9] ggokabeito_0.1.0            see_0.10.0                 \n#&gt; [11] MASS_7.3-65                 viridis_0.6.5              \n#&gt; [13] viridisLite_0.4.2           ggpubr_0.6.0               \n#&gt; [15] ggExtra_0.10.1              gridExtra_2.3              \n#&gt; [17] patchwork_1.3.0             bayesplot_1.11.1           \n#&gt; [19] semTools_0.5-6              semPlot_1.1.6              \n#&gt; [21] lavaan_0.6-19               psych_2.4.12               \n#&gt; [23] scales_1.3.0                markdown_1.13              \n#&gt; [25] knitr_1.49                  lubridate_1.9.4            \n#&gt; [27] forcats_1.0.0               stringr_1.5.1              \n#&gt; [29] dplyr_1.1.4                 purrr_1.0.4                \n#&gt; [31] readr_2.1.5                 tidyr_1.3.1                \n#&gt; [33] tibble_3.2.1                ggplot2_3.5.1              \n#&gt; [35] tidyverse_2.0.0             here_1.0.1                 \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] splines_4.4.2       later_1.4.1         R.oo_1.27.0        \n#&gt;   [4] XML_3.99-0.18       rpart_4.1.24        lifecycle_1.0.4    \n#&gt;   [7] Rdpack_2.6.2        rstatix_0.7.2       doParallel_1.0.17  \n#&gt;  [10] rprojroot_2.0.4     lattice_0.22-6      survey_4.4-2       \n#&gt;  [13] rockchalk_1.8.157   backports_1.5.0     magrittr_2.0.3     \n#&gt;  [16] openxlsx_4.2.8      Hmisc_5.2-2         rmarkdown_2.29     \n#&gt;  [19] plotrix_3.8-4       yaml_2.3.10         IsingFit_0.4       \n#&gt;  [22] httpuv_1.6.15       zip_2.3.2           DBI_1.2.3          \n#&gt;  [25] pbapply_1.7-2       minqa_1.2.8         RColorBrewer_1.1-3 \n#&gt;  [28] multcomp_1.4-28     abind_1.4-8         quadprog_1.5-8     \n#&gt;  [31] R.utils_2.13.0      nnet_7.3-20         TH.data_1.1-3      \n#&gt;  [34] sandwich_3.1-1      relaimpo_2.2-7      gdata_3.0.1        \n#&gt;  [37] ellipse_0.5.0       arm_1.14-4          codetools_0.2-20   \n#&gt;  [40] tidyselect_1.2.1    shape_1.4.6.1       farver_2.1.2       \n#&gt;  [43] IsingSampler_0.2.3  lme4_1.1-36         stats4_4.4.2       \n#&gt;  [46] base64enc_0.1-3     eigenmodel_1.11     jsonlite_1.9.0     \n#&gt;  [49] e1071_1.7-16        mitml_0.4-5         Formula_1.2-5      \n#&gt;  [52] survival_3.8-3      iterators_1.0.14    emmeans_1.10.7     \n#&gt;  [55] foreach_1.5.2       tools_4.4.2         snow_0.4-4         \n#&gt;  [58] Rcpp_1.0.14         glue_1.8.0          mnormt_2.1.1       \n#&gt;  [61] pan_1.9             xfun_0.51           withr_3.0.2        \n#&gt;  [64] fastmap_1.2.0       mitools_2.4         boot_1.3-31        \n#&gt;  [67] digest_0.6.37       mi_1.1              timechange_0.3.0   \n#&gt;  [70] R6_2.6.1            mime_0.12           estimability_1.5.1 \n#&gt;  [73] mice_3.17.0         colorspace_2.1-1    gtools_3.9.5       \n#&gt;  [76] jpeg_0.1-10         weights_1.0.4       R.methodsS3_1.8.2  \n#&gt;  [79] generics_0.1.3      data.table_1.17.0   corpcor_1.6.10     \n#&gt;  [82] class_7.3-23        htmlwidgets_1.6.4   pkgconfig_2.0.3    \n#&gt;  [85] sem_3.1-16          gtable_0.3.6        htmltools_0.5.8.1  \n#&gt;  [88] carData_3.0-5       cocor_1.1-4         png_0.1-8          \n#&gt;  [91] wordcloud_2.6       reformulas_0.4.0    rstudioapi_0.17.1  \n#&gt;  [94] tzdb_0.4.0          reshape2_1.4.4      curl_6.2.1         \n#&gt;  [97] coda_0.19-4.1       checkmate_2.3.2     nlme_3.1-167       \n#&gt; [100] nloptr_2.1.1        proxy_0.4-27        zoo_1.8-13         \n#&gt; [103] parallel_4.4.2      miniUI_0.1.1.1      foreign_0.8-88     \n#&gt; [106] pillar_1.10.1       grid_4.4.2          vctrs_0.6.5        \n#&gt; [109] promises_1.3.2      car_3.1-3           OpenMx_2.21.13     \n#&gt; [112] jomo_2.7-6          xtable_1.8-4        cluster_2.1.8      \n#&gt; [115] htmlTable_2.4.3     evaluate_1.0.3      pbivnorm_0.6.0     \n#&gt; [118] mvtnorm_1.3-3       cli_3.6.4           kutils_1.73        \n#&gt; [121] compiler_4.4.2      rlang_1.1.5         smacof_2.1-7       \n#&gt; [124] ggsignif_0.6.4      labeling_0.4.3      fdrtool_1.2.18     \n#&gt; [127] plyr_1.8.9          stringi_1.8.4       nnls_1.6           \n#&gt; [130] munsell_0.5.1       lisrelToR_0.3       glmnet_4.1-8       \n#&gt; [133] pacman_0.5.1        Matrix_1.7-2        hms_1.1.3          \n#&gt; [136] glasso_1.11         shiny_1.10.0        haven_2.5.4        \n#&gt; [139] rbibutils_2.3       igraph_2.1.4        broom_1.0.7        \n#&gt; [142] RcppParallel_5.1.10 polynom_1.4-1\n\n\n\n\n\nSaqr, M., & López-Pernas, S. (2024). Learning analytics methods and tutorials: A practical guide using R. Springer Nature.",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/pca/01_linear_algebra.html",
    "href": "chapters/pca/01_linear_algebra.html",
    "title": "\n25  Elementi di algebra lineare\n",
    "section": "",
    "text": "25.1 Introduzione\nQuesto capitolo presenta alcune nozioni di base dell’algebra lineare, una branca della matematica essenziale per la comprensione e l’analisi dei modelli di regressione lineare.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/pca/01_linear_algebra.html#rappresentazione-dei-vettori",
    "href": "chapters/pca/01_linear_algebra.html#rappresentazione-dei-vettori",
    "title": "\n25  Elementi di algebra lineare\n",
    "section": "\n25.2 Rappresentazione dei Vettori",
    "text": "25.2 Rappresentazione dei Vettori\nNell’algebra lineare, un vettore, che rappresenta una lista ordinata di scalari, è solitamente indicato con una lettera minuscola in grassetto, come \\(\\mathbf{v}\\). Gli elementi di un vettore sono generalmente indicati con un indice, ad esempio \\(\\mathbf{v}_1\\) si riferisce al primo elemento del vettore \\(\\mathbf{v}\\).\nUn vettore \\(\\mathbf{v}\\) di \\(n\\) elementi può essere rappresentato sia come una colonna che come una riga, a seconda della convenzione scelta. Ad esempio, un vettore colonna di \\(n\\) elementi è scritto come:\n\\[\n\\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix},\n\\]\nmentre un vettore riga appare come:\n\\[\n\\mathbf{v} = \\begin{bmatrix} v_1 & v_2 & \\cdots & v_n \\end{bmatrix}.\n\\]\nQuesta notazione consente di visualizzare chiaramente i singoli elementi del vettore e di riferirsi a ciascuno di essi in modo specifico.\nUna lista di \\(n\\) scalari organizzata in un vettore \\(\\mathbf{v}\\) è chiamata “dimensione” del vettore. Formalmente, si esprime come \\(\\mathbf{v} \\in \\mathbb{R}^n\\), indicando che il vettore \\(\\mathbf{v}\\) appartiene all’insieme di tutti i vettori reali di dimensione \\(n\\).",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/pca/01_linear_algebra.html#visualizzazione-geometrica-dei-vettori",
    "href": "chapters/pca/01_linear_algebra.html#visualizzazione-geometrica-dei-vettori",
    "title": "\n25  Elementi di algebra lineare\n",
    "section": "\n25.3 Visualizzazione Geometrica dei Vettori",
    "text": "25.3 Visualizzazione Geometrica dei Vettori\nI vettori possono essere rappresentati come frecce in uno spazio \\(n\\)-dimensionale, con l’origine come punto di partenza e la punta della freccia che corrisponde alle coordinate specificate dal vettore. La norma \\(L_2\\) (o lunghezza) di un vettore, denotata come \\(\\|\\mathbf{v}\\|\\), rappresenta la distanza euclidea dall’origine alla punta del vettore.\nPer un vettore \\(\\mathbf{v} = [v_1, v_2, \\ldots, v_n]\\), la norma è definita come:\n\\[\n\\|\\mathbf{v}\\| = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2}.\n\\]\n\n25.3.1 Esempio Numerico\nConsideriamo un vettore in uno spazio bidimensionale, ad esempio \\(\\mathbf{v} = [3, 4]\\). Geometricamente, questo vettore parte dall’origine \\((0, 0)\\) e termina nel punto \\((3, 4)\\) del piano cartesiano.\nPer calcolare la norma \\(L_2\\) di questo vettore, applichiamo la formula:\n\\[\n\\|\\mathbf{v}\\| = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5.\n\\]\nQuindi, la norma del vettore \\(\\mathbf{v} = [3, 4]\\) è 5, che rappresenta la lunghezza della freccia dal punto di origine \\((0, 0)\\) al punto \\((3, 4)\\) nello spazio bidimensionale.\n\n25.3.2 Rappresentazione Geometrica\ny\n^\n|       * (3, 4)\n|      /\n|     /\n|    /\n|   /\n|  /\n| / \n|/____________&gt; x\n(0, 0)\nIn questo diagramma, il punto * rappresenta la fine del vettore \\(\\mathbf{v}\\) e la linea inclinata mostra il vettore stesso che parte dall’origine. L’altezza della linea fino al punto (3, 4) rappresenta visivamente la norma del vettore, che è la distanza di 5 unità dall’origine.\nQuesto esempio illustra chiaramente la relazione tra la rappresentazione numerica di un vettore e la sua interpretazione geometrica, facilitando la comprensione della lunghezza del vettore e della sua direzione nello spazio bidimensionale.\nSebbene noi siamo principalmente limitati a ragionare su spazi bidimensionali (2D) e tridimensionali (3D), i dati che raccogliamo spesso risiedono in spazi di dimensioni superiori. L’algebra lineare permette di ragionare e sviluppare intuizioni su vettori e spazi di dimensioni molto più elevate, superando i limiti della visualizzazione diretta.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/pca/01_linear_algebra.html#operazioni-di-base-sui-vettori",
    "href": "chapters/pca/01_linear_algebra.html#operazioni-di-base-sui-vettori",
    "title": "\n25  Elementi di algebra lineare\n",
    "section": "\n25.4 Operazioni di Base sui Vettori",
    "text": "25.4 Operazioni di Base sui Vettori\n\n25.4.1 1. Moltiplicazione di un Vettore per uno Scalare\nLa moltiplicazione di un vettore per uno scalare produce un nuovo vettore. Questa operazione può essere interpretata come una “scalatura” del vettore nello spazio: il vettore risultante mantiene la stessa direzione dell’originale, ma la sua lunghezza viene modificata in base allo scalare.\nSe \\(\\mathbf{v} = [v_1, v_2, \\ldots, v_n]\\) è un vettore e \\(c\\) è uno scalare, la moltiplicazione del vettore per lo scalare è data da:\n\\[\nc\\mathbf{v} = [cv_1, cv_2, \\ldots, cv_n]\n\\]\n\n25.4.2 2. Addizione di Vettori\nÈ possibile sommare due vettori della stessa dimensione. La somma vettoriale si ottiene sommando gli elementi corrispondenti di ciascun vettore.\nSe \\(\\mathbf{u} = [u_1, u_2, \\ldots, u_n]\\) e \\(\\mathbf{v} = [v_1, v_2, \\ldots, v_n]\\) sono due vettori di dimensione \\(n\\), la loro somma è:\n\\[\n\\mathbf{u} + \\mathbf{v} = [u_1 + v_1, u_2 + v_2, \\ldots, u_n + v_n]\n\\]\n\n25.4.3 3. Prodotto Scalare (o Prodotto Interno)\nIl prodotto scalare tra due vettori della stessa dimensione è uno scalare che fornisce informazioni sull’angolo tra i vettori nello spazio. Formalmente, il prodotto scalare di \\(\\mathbf{u} = [u_1, u_2, \\ldots, u_n]\\) e \\(\\mathbf{v} = [v_1, v_2, \\ldots, v_n]\\) è definito come:\n\\[\n\\mathbf{u} \\cdot \\mathbf{v} = u_1v_1 + u_2v_2 + \\cdots + u_nv_n\n\\]\nQuesto prodotto scalare può anche essere espresso in termini dell’angolo \\(\\theta\\) tra i vettori:\n\\[\n\\mathbf{u} \\cdot \\mathbf{v} = \\|\\mathbf{u}\\| \\|\\mathbf{v}\\| \\cos(\\theta)\n\\]\nSe due vettori sono ortogonali, ovvero formano un angolo di \\(90^\\circ\\) tra loro, il loro prodotto scalare è zero: \\(\\mathbf{u} \\cdot \\mathbf{v} = 0\\).\n\n25.4.4 4. Prodotto Scalare di un Vettore con Se Stesso\nIl prodotto scalare di un vettore con se stesso fornisce il quadrato della sua lunghezza. Se \\(\\mathbf{v} = [v_1, v_2, \\ldots, v_n]\\), allora:\n\\[\n\\mathbf{v} \\cdot \\mathbf{v} = v_1^2 + v_2^2 + \\cdots + v_n^2 = \\|\\mathbf{v}\\|^2\n\\]\nQueste operazioni di base sui vettori sono fondamentali per molte applicazioni in matematica, fisica, informatica e altre scienze, fornendo una struttura potente per analizzare e risolvere problemi in spazi multidimensionali.\n\n25.4.5 Vettori in R\nIn R, possiamo creare un vettore con tre elementi usando la funzione c():\n\n# Creazione di un vettore\nv &lt;- c(1, 2, 3)\nv\n#&gt; [1] 1 2 3\n\nIn questo esempio, v è un vettore con tre elementi: 1, 2 e 3.\nPer eseguire il prodotto tra un vettore e uno scalare, possiamo semplicemente moltiplicare il vettore per lo scalare. Questo moltiplica ogni elemento del vettore per lo scalare:\n\n# Scalari e vettori\na &lt;- 5\n\n# Prodotto vettore-scalare\nva &lt;- v * a\nva\n#&gt; [1]  5 10 15\n\nIl risultato sarà [5, 10, 15].\nIl prodotto interno (o prodotto scalare) tra due vettori si può calcolare con la funzione sum() per ottenere la somma dei prodotti degli elementi corrispondenti:\n\n# Un altro vettore\nv2 &lt;- c(4, 5, 6)\n\n# Prodotto interno\nprodotto_interno &lt;- sum(v * v2)\nprodotto_interno\n#&gt; [1] 32\n\nIl risultato sarà 32, dato che il prodotto interno è calcolato come \\(1*4 + 2*5 + 3*6 = 32\\).\nIn alternativa, si può utilizzare la funzione crossprod() che calcola il prodotto interno in modo efficiente:\n\n# Prodotto interno con crossprod\nprodotto_interno2 &lt;- crossprod(v, v2)\nprodotto_interno2\n#&gt;      [,1]\n#&gt; [1,]   32\n\nLa funzione crossprod() restituisce una matrice \\(1 \\times 1\\), quindi il risultato sarà simile.\nIl prodotto esterno tra due vettori produce una matrice dove ogni elemento è il prodotto degli elementi corrispondenti dei due vettori. In R, possiamo usare la funzione outer():\n\n# Prodotto esterno\nprodotto_esterno &lt;- outer(v, v2)\nprodotto_esterno\n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]    4    5    6\n#&gt; [2,]    8   10   12\n#&gt; [3,]   12   15   18\n\nIl risultato sarà una matrice in cui ogni elemento è il prodotto dei corrispondenti elementi dei vettori v e v2.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/pca/01_linear_algebra.html#matrici",
    "href": "chapters/pca/01_linear_algebra.html#matrici",
    "title": "\n25  Elementi di algebra lineare\n",
    "section": "\n25.5 Matrici",
    "text": "25.5 Matrici\nUna matrice è una struttura matematica bidimensionale costituita da elementi disposti in righe e colonne. Formalmente, una matrice \\(\\mathbf{A}\\) di dimensioni \\(m \\times n\\) (si legge “m per n”) è un array rettangolare di numeri reali o complessi, denotato come:\n\\[ \\mathbf{A} = (a_{ij})_{m \\times n} = \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix} \\]\ndove \\(a_{ij}\\) rappresenta l’elemento nella \\(i\\)-esima riga e \\(j\\)-esima colonna della matrice.\nLe matrici sono comunemente indicate con lettere maiuscole in grassetto, come \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), \\(\\mathbf{C}\\), etc. Una matrice con \\(m\\) righe e \\(n\\) colonne si dice di ordine \\(m \\times n\\).\nIn molte matrici di dati, ogni elemento \\(a_{ij}\\) è uno scalare che rappresenta il valore della \\(j\\)-esima variabile del \\(i\\)-esimo campione. Formalmente, possiamo indicare \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\), il che significa che la matrice \\(\\mathbf{A}\\) ha \\(m\\) righe e \\(n\\) colonne. Si dice comunemente che la “dimensione” di \\(\\mathbf{A}\\) è \\(m \\times n\\).\n\n25.5.1 Matrici come Collezioni di Vettori Colonna\nLe matrici possono essere interpretate come collezioni di vettori colonna. Ad esempio, una matrice di dati può essere rappresentata come:\n\\[\n\\mathbf{A} = \\begin{bmatrix} \\mathbf{a}_1 & \\mathbf{a}_2 & \\cdots & \\mathbf{a}_n \\end{bmatrix}\n\\]\nIn questo caso, \\(\\mathbf{A}\\) è composta da una sequenza di \\(n\\) vettori colonna \\(\\mathbf{a}_1, \\mathbf{a}_2, \\ldots, \\mathbf{a}_n\\), ciascuno dei quali è un vettore di dimensione \\(m\\). Più precisamente, ogni vettore colonna \\(\\mathbf{a}_j\\) rappresenta i dati di tutti i campioni per la \\(j\\)-esima variabile o feature.\n\n25.5.2 Matrici come Collezioni di Vettori Riga\nIn alternativa, una matrice può essere vista come una collezione di vettori riga. In questo contesto, ogni riga di \\(\\mathbf{A}\\) rappresenta tutte le variabili misurate per un dato campione:\n\\[\n\\mathbf{A} = \\begin{bmatrix}\n\\mathbf{a}_1^T \\\\\n\\mathbf{a}_2^T \\\\\n\\vdots \\\\\n\\mathbf{a}_m^T\n\\end{bmatrix}\n\\]\nQui, la matrice \\(\\mathbf{A}\\) è composta da \\(m\\) vettori riga, denotati come \\(\\mathbf{a}_i^T\\). Ognuno di questi vettori riga \\(\\mathbf{a}_i^T\\) è di dimensione \\(n\\), indicando che ciascun campione ha \\(n\\) variabili o feature associate.\n\n25.5.3 Trasposta di una Matrice\nIl simbolo \\(T\\) rappresenta la trasposta di una matrice. La trasposta di una matrice, denotata con un apice \\(T\\) (es. \\(\\mathbf{A}^T\\)), è un’operazione che trasforma ciascuna delle righe di \\(\\mathbf{A}\\) in colonne di \\(\\mathbf{A}^T\\). In altre parole, se \\(\\mathbf{A}\\) ha dimensione \\(m \\times n\\), allora \\(\\mathbf{A}^T\\) avrà dimensione \\(n \\times m\\):\n\\[\n\\mathbf{A}^T = \\begin{bmatrix}\na_{11} & a_{21} & \\cdots & a_{m1} \\\\\na_{12} & a_{22} & \\cdots & a_{m2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{1n} & a_{2n} & \\cdots & a_{mn}\n\\end{bmatrix}\n\\]\nCon la trasposta, le variabili misurate diventano colonne e i campioni diventano righe. Essenzialmente, i vettori riga sono le trasposte dei vettori colonna. Questo concetto è molto utile in algebra lineare, poiché permette di passare facilmente da una rappresentazione dei dati a un’altra.\n\n25.5.4 Matrici in R\nIn R, una matrice può essere creata utilizzando la funzione matrix(). Per esempio, possiamo creare una matrice 3x4 fornendo un vettore di elementi e specificando il numero di righe e colonne.\nEcco come definire una matrice 3x4:\n\n# Definizione della matrice 3x4\nM &lt;- matrix(c(\n    1, 2, 3, 4, \n    5, 6, 7, 8, \n    9, 10, 11, 12\n    ), \n    nrow = 3, ncol = 4, byrow = TRUE)\n\nprint(\"Matrice originale:\")\n#&gt; [1] \"Matrice originale:\"\nprint(M)\n#&gt;      [,1] [,2] [,3] [,4]\n#&gt; [1,]    1    2    3    4\n#&gt; [2,]    5    6    7    8\n#&gt; [3,]    9   10   11   12\n\nQui, byrow = TRUE indica che i dati vengono inseriti riga per riga. Se si utilizza byrow = FALSE, i dati vengono inseriti colonna per colonna.\nIn R, puoi calcolare la trasposta di una matrice utilizzando la funzione t():\n\n# Calcolo della trasposta\ntrasposta &lt;- t(M)\ntrasposta\n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]    1    5    9\n#&gt; [2,]    2    6   10\n#&gt; [3,]    3    7   11\n#&gt; [4,]    4    8   12",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/pca/01_linear_algebra.html#moltiplicazione-tra-matrici",
    "href": "chapters/pca/01_linear_algebra.html#moltiplicazione-tra-matrici",
    "title": "\n25  Elementi di algebra lineare\n",
    "section": "\n25.6 Moltiplicazione tra Matrici",
    "text": "25.6 Moltiplicazione tra Matrici\nLa moltiplicazione tra matrici è un’operazione fondamentale nell’algebra lineare. Per poter moltiplicare due matrici, è necessario che siano conformabili, il che significa che il numero di colonne della prima matrice deve essere uguale al numero di righe della seconda matrice.\nSe abbiamo una matrice \\(\\mathbf{A}\\) di dimensioni \\(m \\times n\\) (cioè, \\(m\\) righe e \\(n\\) colonne) e una matrice \\(\\mathbf{B}\\) di dimensioni \\(n \\times p\\) (cioè, \\(n\\) righe e \\(p\\) colonne), allora il prodotto delle due matrici \\(\\mathbf{A} \\mathbf{B}\\) sarà una matrice \\(\\mathbf{C}\\) di dimensioni \\(m \\times p\\).\nIl prodotto tra due matrici \\(\\mathbf{A}\\) e \\(\\mathbf{B}\\) si ottiene calcolando il prodotto interno tra le righe della prima matrice e le colonne della seconda matrice.\nPer ciascun elemento \\(c_{ij}\\) della matrice risultante \\(\\mathbf{C}\\), si esegue il seguente calcolo:\n\\[\nc_{ij} = \\sum_{k=1}^{n} a_{ik} b_{kj}.\n\\]\nQuesto significa che l’elemento \\(c_{ij}\\) è il risultato del prodotto interno tra la \\(i\\)-esima riga della matrice \\(\\mathbf{A}\\) e la \\(j\\)-esima colonna della matrice \\(\\mathbf{B}\\).\nLa moltiplicazione di una matrice per un vettore è un caso particolare della moltiplicazione tra matrici, dove il vettore può essere visto come una matrice con una delle dimensioni uguale a 1.\nSe \\(\\mathbf{A}\\) è una matrice \\(m \\times n\\) e \\(\\mathbf{x}\\) è un vettore di dimensione \\(n\\) (cioè una matrice di dimensione \\(n \\times 1\\)), allora il prodotto \\(\\mathbf{A} \\mathbf{x}\\) è un vettore di dimensione \\(m\\). Ogni elemento del vettore risultante è il prodotto interno tra una riga della matrice \\(\\mathbf{A}\\) e il vettore \\(\\mathbf{x}\\).\nConsideriamo le seguenti matrici:\n\\[\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix}, \\quad \\mathbf{B} = \\begin{bmatrix}\n7 & 8 \\\\\n9 & 10 \\\\\n11 & 12\n\\end{bmatrix}\n\\]\n\n\n\\(\\mathbf{A}\\) è una matrice \\(2 \\times 3\\).\n\n\\(\\mathbf{B}\\) è una matrice \\(3 \\times 2\\).\n\nIl prodotto \\(\\mathbf{A} \\mathbf{B}\\) è una matrice \\(2 \\times 2\\) calcolata come segue:\n\\[\n\\mathbf{C} = \\mathbf{A} \\mathbf{B} = \\begin{bmatrix}\n(1 \\cdot 7 + 2 \\cdot 9 + 3 \\cdot 11) & (1 \\cdot 8 + 2 \\cdot 10 + 3 \\cdot 12) \\\\\n(4 \\cdot 7 + 5 \\cdot 9 + 6 \\cdot 11) & (4 \\cdot 8 + 5 \\cdot 10 + 6 \\cdot 12)\n\\end{bmatrix}\n\\]\nCalcolando ogni elemento:\n\\[\n\\mathbf{C} = \\begin{bmatrix}\n58 & 64 \\\\\n139 & 154\n\\end{bmatrix}\n\\]\nIn questo esempio, ogni elemento della matrice risultante \\(\\mathbf{C}\\) è stato ottenuto calcolando il prodotto interno tra le righe di \\(\\mathbf{A}\\) e le colonne di \\(\\mathbf{B}\\).\n\n25.6.1 Calcoli con Matrici in R\nIn R, il prodotto tra matrici può essere calcolato utilizzando l’operatore %*%.\n\n# Definizione della matrice A (2x3)\nA &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2, ncol = 3, byrow = TRUE)\n\n# Definizione della matrice B (3x2)\nB &lt;- matrix(c(7, 8, 9, 10, 11, 12), nrow = 3, ncol = 2, byrow = TRUE)\n\n# Calcolo del prodotto A * B\nprodotto_AB &lt;- A %*% B\nprint(\"Prodotto A * B usando l'operatore %*%:\")\n#&gt; [1] \"Prodotto A * B usando l'operatore %*%:\"\nprodotto_AB\n#&gt;      [,1] [,2]\n#&gt; [1,]   58   64\n#&gt; [2,]  139  154\n\nIn R, %*% è l’operatore per il prodotto matriciale.\n\n25.6.2 Matrice Identità e Matrice Inversa\n\n25.6.3 Matrice Identità\nLa matrice identità, denotata come \\(\\mathbf{I}_n\\), è una matrice quadrata di dimensione \\(n \\times n\\) con tutti gli elementi sulla diagonale principale uguali a 1 e tutti gli altri elementi uguali a 0. Ad esempio, una matrice identità 3x3 è:\n\\[\n\\mathbf{I}_3 = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\]\nIn generale, una matrice identità di dimensione \\(n \\times n\\) è:\n\\[\n\\mathbf{I}_n = \\begin{bmatrix}\n1 & 0 & \\cdots & 0 \\\\\n0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 1\n\\end{bmatrix}\n\\]\nLa matrice identità ha la proprietà fondamentale di essere l’elemento neutro per la moltiplicazione matriciale. Per qualsiasi matrice \\(\\mathbf{A}\\) di dimensioni \\(n \\times n\\):\n\\[\n\\mathbf{A} \\mathbf{I}_n = \\mathbf{A} \\quad \\text{e} \\quad \\mathbf{I}_n \\mathbf{A} = \\mathbf{A}.\n\\]\nIn R, puoi creare una matrice identità utilizzando la funzione diag():\n\n# Creazione della matrice identità 3x3\nI &lt;- diag(3)\n\nprint(\"Matrice identità 3x3:\")\n#&gt; [1] \"Matrice identità 3x3:\"\nprint(I)\n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]    1    0    0\n#&gt; [2,]    0    1    0\n#&gt; [3,]    0    0    1\n\nLa funzione diag(3) crea una matrice identità 3x3 con 1 lungo la diagonale principale e 0 altrove.\n\n25.6.4 Determinante di una Matrice\nIl determinante è un numero associato a una matrice quadrata che fornisce informazioni essenziali sulle proprietà della matrice stessa. È uno scalare che può indicare se una matrice è invertibile, se un sistema di equazioni lineari ha una soluzione unica, e molto altro.\nIl determinante di una matrice può essere interpretato in diversi modi:\n\nIn termini geometrici, il determinante di una matrice \\(2 \\times 2\\) o \\(3 \\times 3\\) rappresenta rispettivamente l’area o il volume del parallelogramma o del parallelepipedo definito dai vettori delle righe (o colonne) della matrice. Un determinante pari a zero indica che i vettori sono linearmente dipendenti e che l’area o il volume è nullo, suggerendo che la matrice non ha un’inversa.\nAlgebraicamente, il determinante di una matrice quadrata può dirci se la matrice è invertibile. Se il determinante è diverso da zero, la matrice è invertibile, cioè esiste una matrice inversa tale che il prodotto delle due sia la matrice identità. Se il determinante è zero, la matrice non è invertibile.\nNel contesto dei sistemi di equazioni lineari, se il determinante del coefficiente della matrice associata a un sistema è zero, il sistema può non avere soluzioni o avere un numero infinito di soluzioni. Se è diverso da zero, il sistema ha una soluzione unica.\n\n\n25.6.4.1 Calcolo del Determinante per una Matrice 2x2\nPer una matrice \\(2 \\times 2\\):\n\\[\n\\mathbf{A} = \\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\n\\]\nil determinante è calcolato come:\n\\[\n\\det(\\mathbf{A}) = ad - bc\n\\]\nQuesto semplice calcolo deriva dalla differenza tra il prodotto degli elementi della diagonale principale (dall’angolo superiore sinistro all’angolo inferiore destro) e il prodotto degli elementi della diagonale secondaria (dall’angolo superiore destro all’angolo inferiore sinistro).\n\n25.6.4.2 Calcolo del Determinante per Matrici di Dimensioni Superiori\nPer matrici di dimensioni superiori a \\(2 \\times 2\\), il calcolo del determinante diventa più complesso. Un metodo comune per calcolare il determinante di matrici più grandi è l’espansione di Laplace o espansione per cofattori. Questo metodo si basa sulla ricorsione, calcolando il determinante attraverso una somma pesata di determinanti di matrici più piccole (minori) che si ottengono eliminando una riga e una colonna dalla matrice originale.\n\n25.6.5 Calcolo del Determinante in R\nPer calcolare il determinante di una matrice quadrata in R, puoi usare la funzione det(). Questa funzione funziona per matrici quadrate di qualsiasi dimensione.\n\n25.6.5.1 Esempio con una matrice \\(2 \\times 2\\)\n\n\n# Definizione di una matrice 2x2\nA &lt;- matrix(c(1, 2, 3, 4), nrow = 2, ncol = 2, byrow = TRUE)\n\n# Calcolo del determinante\ndeterminante_A &lt;- det(A)\ndeterminante_A\n#&gt; [1] -2\n\n\n25.6.5.2 Esempio con una matrice \\(3 \\times 3\\)\n\n\n# Definizione di una matrice 3x3\nB &lt;- matrix(c(6, 1, 1, 4, -2, 5, 2, 8, 7), nrow = 3, ncol = 3, byrow = TRUE)\n\n# Calcolo del determinante\ndeterminante_B &lt;- det(B)\ndeterminante_B\n#&gt; [1] -306\n\nIn R, come in Python, il determinante è uno strumento fondamentale per comprendere le proprietà di una matrice. Può essere utilizzato per determinare:\n\n\nInvertibilità: Se il determinante è \\(0\\), la matrice non è invertibile.\n\nTrasformazioni geometriche: Il valore del determinante descrive il fattore di scala della trasformazione rappresentata dalla matrice.\n\nSistemi lineari: Il determinante aiuta a identificare la singolarità dei sistemi di equazioni.\n\n25.6.6 Inversa di una Matrice\nL’inversa di una matrice quadrata \\(\\mathbf{A}\\), denotata come \\(\\mathbf{A}^{-1}\\), è una matrice che, moltiplicata per \\(\\mathbf{A}\\), restituisce la matrice identità \\(\\mathbf{I}_n\\). L’inversa di una matrice esiste solo per matrici quadrate non singolari, ovvero matrici il cui determinante è diverso da zero.\nLa proprietà fondamentale dell’inversa è:\n\\[\n\\mathbf{A} \\mathbf{A}^{-1} = \\mathbf{I}_n \\quad \\text{e} \\quad \\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_n.\n\\]\ndove \\(\\mathbf{I}_n\\) è la matrice identità di dimensione \\(n \\times n\\).\n\n25.6.6.1 Esempio: Calcolo dell’Inversa di una Matrice \\(2 \\times 2\\)\n\nPer una matrice \\(2 \\times 2\\):\n\\[\n\\mathbf{A} = \\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\n\\]\nl’inversa, se esiste, è data dalla formula:\n\\[\n\\mathbf{A}^{-1} = \\frac{1}{ad-bc} \\begin{bmatrix}\nd & -b \\\\\n-c & a\n\\end{bmatrix}\n\\]\ndove \\(ad-bc\\) è il determinante della matrice \\(\\mathbf{A}\\). L’inversa esiste solo se questo determinante è diverso da zero (cioè, se \\(\\mathbf{A}\\) è non singolare).\n\n25.6.7 Utilizzo dell’Inversa di una Matrice\nL’inversa di una matrice è particolarmente utile per risolvere sistemi di equazioni lineari. Ad esempio, consideriamo un sistema rappresentato in forma matriciale come \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\), dove \\(\\mathbf{A}\\) è la matrice dei coefficienti, \\(\\mathbf{x}\\) è il vettore delle variabili incognite e \\(\\mathbf{b}\\) è il vettore dei termini noti.\nSe \\(\\mathbf{A}\\) è una matrice invertibile, possiamo risolvere per \\(\\mathbf{x}\\) moltiplicando entrambi i lati dell’equazione per \\(\\mathbf{A}^{-1}\\):\n\\[\n\\mathbf{A}^{-1} \\mathbf{A} \\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}.\n\\]\nPoiché \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}\\), otteniamo:\n\\[\n\\mathbf{I} \\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b},\n\\]\n\\[\n\\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}.\n\\]\nQuesta proprietà è utile anche per altre applicazioni, come nella derivazione della formula per i coefficienti della regressione lineare.\nEcco come calcolare l’inversa di una matrice in R utilizzando la funzione solve():\n\n25.6.8 Calcolo dell’Inversa in R\nIn R, possiamo calcolare l’inversa di una matrice quadrata (se invertibile) utilizzando la funzione solve(). È importante verificare che la matrice abbia un determinante diverso da zero, altrimenti non è invertibile.\n\n# Definizione di una matrice 2x2\nA &lt;- matrix(c(1, 2, 3, 4), nrow = 2, ncol = 2, byrow = TRUE)\n\n# Calcolo dell'inversa\nA_inv &lt;- solve(A)\n\ncat(\"Inversa di A:\\n\")\n#&gt; Inversa di A:\nprint(A_inv)\n#&gt;      [,1] [,2]\n#&gt; [1,] -2.0  1.0\n#&gt; [2,]  1.5 -0.5\n\nPer verificare che l’inversa sia stata calcolata correttamente, possiamo moltiplicare la matrice originale \\(\\mathbf{A}\\) per la sua inversa \\(\\mathbf{A}^{-1}\\) e verificare che il risultato sia la matrice identità:\n\n# Prodotto di A e A_inv\nidentita &lt;- A %*% A_inv\n\ncat(\"Prodotto di A e A_inv (matrice identità):\\n\")\n#&gt; Prodotto di A e A_inv (matrice identità):\nprint(identita)\n#&gt;      [,1]     [,2]\n#&gt; [1,]    1 1.11e-16\n#&gt; [2,]    0 1.00e+00\n\nIn conclusione, l’inversa di una matrice è uno strumento potente e utile per diverse applicazioni, come:\n\nRisoluzione di sistemi di equazioni lineari\nTrasformazioni geometriche\nAnalisi di modelli lineari\n\nIn R, solve() rende semplice e veloce il calcolo dell’inversa, a patto che la matrice sia:\n\n\nQuadrata: Deve avere lo stesso numero di righe e colonne.\n\nInvertibile: Il determinante della matrice deve essere diverso da zero, altrimenti solve() restituirà un errore.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/pca/01_linear_algebra.html#regressione-lineare-e-stima-dei-coefficienti",
    "href": "chapters/pca/01_linear_algebra.html#regressione-lineare-e-stima-dei-coefficienti",
    "title": "\n25  Elementi di algebra lineare\n",
    "section": "\n25.7 Regressione Lineare e Stima dei Coefficienti",
    "text": "25.7 Regressione Lineare e Stima dei Coefficienti\nLa regressione lineare è una tecnica statistica utilizzata per modellare la relazione tra una variabile dipendente (o risposta) e una o più variabili indipendenti (o predittori). È possibile rappresentare questo modello in termini di algebra matriciale per semplificare il calcolo dei coefficienti.\n\n25.7.1 Regressione Lineare Semplice\nLa regressione lineare semplice descrive una relazione lineare tra una variabile indipendente \\(x\\) e una variabile dipendente \\(y\\). Quando abbiamo un campione di \\(n\\) osservazioni, il modello assume la seguente forma:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + e_i, \\quad \\text{per} \\; i = 1, 2, \\ldots, n,\n\\]\ndove:\n\n\n\\(y_i\\) è il valore osservato della variabile dipendente per l’osservazione \\(i\\),\n\n\\(\\beta_0\\) è l’intercetta, che rappresenta il valore di \\(y\\) quando \\(x = 0\\),\n\n\\(\\beta_1\\) è il coefficiente di regressione, che indica quanto varia \\(y\\) per una variazione unitaria di \\(x\\),\n\n\\(x_i\\) è il valore della variabile indipendente per l’osservazione \\(i\\),\n\n\\(e_i\\) è l’errore o residuo per l’osservazione \\(i\\), rappresenta la differenza tra il valore osservato \\(y_i\\) e il valore previsto \\(\\hat{y}_i = \\beta_0 + \\beta_1 x_i\\).\n\nPer un campione di \\(n\\) osservazioni, possiamo rappresentare la regressione lineare in forma matriciale, che rende il modello più compatto e facilita i calcoli statistici. La rappresentazione matriciale del modello di regressione lineare è:\n\\[\n\\mathbf{y} = \\mathbf{Xb} + \\mathbf{e},\n\\]\ndove:\n\n\n\\(\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\\) è il vettore delle osservazioni della variabile dipendente,\n\n\\(\\mathbf{X} = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix}\\) è la matrice di design, in cui la prima colonna è costituita da 1 per includere l’intercetta \\(\\beta_0\\),\n\n\\(\\mathbf{b} = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}\\) è il vettore dei coefficienti del modello,\n\n\\(\\mathbf{e} = \\begin{bmatrix} e_1 \\\\ e_2 \\\\ \\vdots \\\\ e_n \\end{bmatrix}\\) è il vettore degli errori o residui.\n\nQuesta forma matriciale sintetizza tutte le \\(n\\) equazioni del modello di regressione lineare semplice in un’unica espressione compatta, che rappresenta la relazione tra le osservazioni della variabile dipendente \\(y\\) e le corrispondenti osservazioni della variabile indipendente \\(x\\), tenendo conto degli errori di previsione.\n\n25.7.2 Regressione Lineare Multipla\nLa regressione lineare multipla estende la regressione lineare semplice includendo più variabili indipendenti, consentendo di modellare la relazione tra una variabile dipendente e diverse variabili indipendenti. Il modello di regressione lineare multipla per un campione di \\(n\\) osservazioni con \\(p\\) variabili indipendenti può essere scritto come:\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip} + e_i, \\quad \\text{per} \\; i = 1, 2, \\ldots, n,\n\\]\ndove:\n\n\n\\(y_i\\) è il valore osservato della variabile dipendente per l’osservazione \\(i\\),\n\n\\(\\beta_0\\) è l’intercetta del modello,\n\n\\(\\beta_1, \\beta_2, \\ldots, \\beta_p\\) sono i coefficienti di regressione associati alle variabili indipendenti,\n\n\\(x_{i1}, x_{i2}, \\ldots, x_{ip}\\) sono i valori delle variabili indipendenti per l’osservazione \\(i\\),\n\n\\(e_i\\) è l’errore o residuo per l’osservazione \\(i\\), che rappresenta la differenza tra il valore osservato \\(y_i\\) e il valore previsto \\(\\hat{y}_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip}\\).\n\nIn termini matriciali, il modello di regressione lineare multipla può essere scritto come:\n\\[\n\\mathbf{y} = \\mathbf{Xb} + \\mathbf{e},\n\\]\ndove:\n\n\n\\(\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\\) è il vettore delle osservazioni della variabile dipendente, di dimensione \\(n \\times 1\\),\n\n\\(\\mathbf{X} = \\begin{bmatrix} 1 & x_{11} & x_{12} & \\cdots & x_{1p} \\\\ 1 & x_{21} & x_{22} & \\cdots & x_{2p} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n1} & x_{n2} & \\cdots & x_{np} \\end{bmatrix}\\) è la matrice di design, di dimensione \\(n \\times (p+1)\\), dove la prima colonna è composta da 1 per includere l’intercetta \\(\\beta_0\\),\n\n\\(\\mathbf{b} = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}\\) è il vettore dei coefficienti, di dimensione \\((p+1) \\times 1\\),\n\n\\(\\mathbf{e} = \\begin{bmatrix} e_1 \\\\ e_2 \\\\ \\vdots \\\\ e_n \\end{bmatrix}\\) è il vettore degli errori o residui, di dimensione \\(n \\times 1\\).\n\nL’equazione in forma matriciale esplicita per il campione di \\(n\\) osservazioni con \\(p\\) variabili indipendenti è:\n\\[\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & x_{11} & x_{12} & \\cdots & x_{1p} \\\\\n1 & x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n1} & x_{n2} & \\cdots & x_{np}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\beta_2 \\\\\n\\vdots \\\\\n\\beta_p\n\\end{bmatrix} +\n\\begin{bmatrix}\ne_1 \\\\\ne_2 \\\\\n\\vdots \\\\\ne_n\n\\end{bmatrix}.\n\\]\nIn questa rappresentazione:\n\nIl prodotto \\(\\mathbf{Xb}\\) rappresenta i valori previsti (o stimati) del modello come combinazione lineare delle colonne della matrice di design \\(\\mathbf{X}\\), ponderata dai coefficienti \\(\\mathbf{b}\\).\nIl vettore \\(\\mathbf{e}\\) rappresenta gli errori o residui, che sono le differenze tra i valori osservati \\(\\mathbf{y}\\) e i valori previsti \\(\\mathbf{Xb}\\).\n\nQuesta forma compatta e ordinata consente un’efficiente analisi statistica e facilita i calcoli necessari per stimare i coefficienti del modello di regressione (Caudek & Luccio, 2001).",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/pca/01_linear_algebra.html#stima-dei-coefficienti-con-il-metodo-dei-minimi-quadrati",
    "href": "chapters/pca/01_linear_algebra.html#stima-dei-coefficienti-con-il-metodo-dei-minimi-quadrati",
    "title": "\n25  Elementi di algebra lineare\n",
    "section": "\n25.8 Stima dei Coefficienti con il Metodo dei Minimi Quadrati",
    "text": "25.8 Stima dei Coefficienti con il Metodo dei Minimi Quadrati\nPer ogni osservazione \\(i\\), l’errore (o residuo) è definito come la differenza tra il valore osservato \\(y_i\\) e il valore predetto \\(\\hat{y}_i\\) dal modello:\n\\[\ne_i = y_i - \\hat{y}_i,\n\\]\ndove:\n\n\n\\(y_i\\) è il valore osservato dell’output per l’osservazione \\(i\\),\n\n\\(\\hat{y}_i\\) è il valore predetto dal modello per l’osservazione \\(i\\).\n\nIn forma matriciale, possiamo rappresentare l’errore per tutte le \\(n\\) osservazioni come segue:\n\\[\n\\mathbf{e} = \\mathbf{y} - \\hat{\\mathbf{y}},\n\\]\ndove:\n\n\n\\(\\mathbf{e} = \\begin{bmatrix} e_1 \\\\ e_2 \\\\ \\vdots \\\\ e_n \\end{bmatrix}\\) è il vettore degli errori o residui,\n\n\\(\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\\) è il vettore delle osservazioni della variabile dipendente,\n\n\\(\\hat{\\mathbf{y}} = \\begin{bmatrix} \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\vdots \\\\ \\hat{y}_n \\end{bmatrix} = \\mathbf{Xb}\\) è il vettore dei valori predetti dal modello.\n\nL’equazione matriciale esplicita per il vettore degli errori \\(\\mathbf{e}\\) è quindi:\n\\[\n\\mathbf{e} = \\mathbf{y} - \\mathbf{Xb}.\n\\]\nQuesta equazione mostra che il vettore degli errori \\(\\mathbf{e}\\) è la differenza tra il vettore delle osservazioni \\(\\mathbf{y}\\) e il vettore dei valori predetti \\(\\hat{\\mathbf{y}} = \\mathbf{Xb}\\). In altre parole, ogni elemento \\(e_i\\) del vettore degli errori rappresenta la differenza tra il valore osservato \\(y_i\\) e il valore predetto \\(\\hat{y}_i\\) per l’osservazione \\(i\\).\nL’obiettivo della regressione lineare è minimizzare la somma degli errori quadrati (\\(SSE\\), Sum of Squared Errors) per tutte le osservazioni. Questa somma è data da:\n\\[\n\\text{SSE} = \\sum_{i=1}^m e_i^2 = \\sum_{i=1}^m (y_i - \\hat{y}_i)^2.\n\\]\nUtilizzando la notazione matriciale, possiamo esprimere la somma degli errori quadrati come:\n\\[\n\\text{SSE} = \\mathbf{e}^T \\mathbf{e} = (\\mathbf{y} - \\mathbf{X} \\mathbf{b})^T (\\mathbf{y} - \\mathbf{X} \\mathbf{b}).\n\\]\nIl problema di ottimizzazione per minimizzare la somma degli errori quadrati si traduce in:\n\\[\n\\min_{\\mathbf{b}} (\\mathbf{y} - \\mathbf{X} \\mathbf{b})^T (\\mathbf{y} - \\mathbf{X} \\mathbf{b}),\n\\]\ndove:\n\n\n\\(\\mathbf{b}\\) è il vettore dei coefficienti da stimare.\n\n\\(\\mathbf{X}\\) è la matrice di design che include tutte le osservazioni delle variabili indipendenti.\n\n\\(\\mathbf{y}\\) è il vettore delle osservazioni della variabile dipendente.\n\nPer trovare i coefficienti ottimali \\(\\mathbf{b}\\), calcoliamo la derivata parziale dell’errore quadratico totale rispetto a \\(\\mathbf{b}\\) e la impostiamo a zero:\n\\[\n\\frac{\\partial}{\\partial \\mathbf{b}} (\\mathbf{y} - \\mathbf{X} \\mathbf{b})^T (\\mathbf{y} - \\mathbf{X} \\mathbf{b}) = -2 \\mathbf{X}^T (\\mathbf{y} - \\mathbf{X} \\mathbf{b}).\n\\]\nImpostando questa derivata uguale a zero, otteniamo:\n\\[\n-2 \\mathbf{X}^T (\\mathbf{y} - \\mathbf{X} \\mathbf{b}) = 0.\n\\]\nSemplificando, possiamo riscrivere l’equazione come:\n\\[\n\\mathbf{X}^T \\mathbf{y} = \\mathbf{X}^T \\mathbf{X} \\mathbf{b}.\n\\]\nAssumendo che la matrice \\(\\mathbf{X}^T \\mathbf{X}\\) sia invertibile, risolviamo per \\(\\mathbf{b}\\):\n\\[\n\\mathbf{b} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}.\n\\]\nPer gli scopi presenti, non è necessario comprendere la derivazione formale in dettaglio. Tuttavia, possiamo fare un parallelo con il metodo dei minimi quadrati per il caso univariato per ottenere un’intuizione geometrica su cosa stiamo facendo.\nNel caso della regressione lineare semplice (univariata), minimizzare la somma degli errori quadrati significa trovare la retta che meglio si adatta ai dati in uno spazio bidimensionale (2D). Dal punto di vista geometrico, questo processo equivale a calcolare la derivata della funzione di errore rispetto ai coefficienti della retta, quindi impostando la derivata a zero per trovare il punto in cui la pendenza della tangente è piatta. In altre parole, cerchiamo il punto in cui la pendenza della funzione di errore è zero, che corrisponde a un minimo della funzione.\nNel caso della regressione lineare multipla, invece di lavorare in uno spazio bidimensionale, stiamo operando in uno spazio multidimensionale. Ogni dimensione aggiuntiva rappresenta una variabile indipendente (regressore) nel nostro modello. Quando prendiamo la derivata dell’errore quadratico totale rispetto ai coefficienti \\(\\mathbf{b}\\) e la impostiamo a zero, stiamo essenzialmente cercando il punto in questo spazio multidimensionale in cui tutte le “pendenze” (derivate parziali) sono zero. Questo punto rappresenta il minimo dell’errore quadratico totale e corrisponde alla migliore stima dei coefficienti del nostro modello di regressione lineare, minimizzando l’errore di previsione su tutti i dati.\nQuindi, mentre nel caso univariato minimizzare l’errore quadratico trova la migliore linea retta che si adatta ai dati in 2D, nel caso multivariato troviamo il miglior piano o iperpiano che si adatta ai dati in uno spazio di dimensioni superiori.\n\n25.8.1 Stima dei Coefficienti OLS\nQuesta formula:\n\\[\n\\mathbf{b} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n\\]\nè conosciuta come stima dei minimi quadrati ordinari (Ordinary Least Squares, OLS) per i coefficienti della regressione lineare multivariata. Essa fornisce i valori dei coefficienti \\(\\mathbf{b}\\) che minimizzano la somma degli errori quadrati e, quindi, rappresenta la migliore approssimazione lineare dei dati osservati.\n\n25.8.2 Simulazione di una Regressione Lineare Semplice in R\n\n25.8.2.1 Definizione dei dati e calcolo dei coefficienti dei minimi quadrati\n\n# Scegli valori per i coefficienti veri\nb &lt;- c(3.4, 12.35)  # Intercetta e pendenza\n\n# Simula n punti dati\nset.seed(123)  # Per riproducibilità\nn &lt;- 30\ndata_mean &lt;- 0\ndata_sd &lt;- 1\ndata &lt;- rnorm(n, mean = data_mean, sd = data_sd)  # Variabile indipendente\n\n# Aggiungi una colonna di 1s per la matrice di design\nx &lt;- cbind(1, data)  # Matrice di design\n\n# Aggiungi rumore gaussiano\nnoise_mean &lt;- 0\nnoise_sd &lt;- 5\ne &lt;- rnorm(n, mean = noise_mean, sd = noise_sd)\n\n# Simula i valori di y\ny &lt;- x %*% b + e\n\n# Calcola i coefficienti stimati (minimi quadrati)\nb_hat &lt;- solve(t(x) %*% x) %*% t(x) %*% y\n\ncat(\"Valori veri di b:\\n\")\n#&gt; Valori veri di b:\nprint(b)\n#&gt; [1]  3.4 12.3\ncat(\"Stima di b:\\n\")\n#&gt; Stima di b:\nprint(b_hat)\n#&gt;       [,1]\n#&gt;       4.26\n#&gt; data 11.68\n\n\n25.8.2.2 Calcolo delle previsioni e del coefficiente di determinazione (\\(R^2\\))\n\n# Previsioni\ny_hat &lt;- x %*% b_hat\n\n# Calcola R^2\nSS_res &lt;- sum((y - y_hat)^2)  # Somma dei residui al quadrato\nSS_tot &lt;- sum((y - mean(y))^2)  # Somma totale dei quadrati\nr2 &lt;- 1 - (SS_res / SS_tot)\n\ncat(\"Coefficiente di determinazione (R^2):\", r2, \"\\n\")\n#&gt; Coefficiente di determinazione (R^2): 0.885\n\n\n25.8.2.3 Rappresentazione Grafica dei Dati e della Regressione\n\n# Grafico dei dati\nplot(\n  data, \n  y, \n  main = \"Regressione Lineare Semplice\", \n  xlab = \"x\", \n  ylab = \"y\", \n  pch = 16, \n  col = \"blue\"\n)\nabline(\n  a = b[1], b = b[2], \n  col = \"black\", lwd = 2, lty = 2\n)  # Linea con valori veri\nabline(\n  a = b_hat[1], b = b_hat[2], \n  col = \"red\", lwd = 2\n)  # Linea stimata\nlegend(\n  \"topleft\", \n  legend = c(\"Valori veri\", \"Stima\"), \n  col = c(\"black\", \"red\"), \n   lty = c(2, 1), lwd = 2\n)\n\n\n\n\n\n\n\n\n25.8.2.4 Analisi con il Pacchetto lm\n\n\n# Modello di regressione con lm()\nlm_model &lt;- lm(y ~ data)\nsummary(lm_model)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y ~ data)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt;  -8.04  -2.53  -1.08   3.47  10.06 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)\n#&gt; (Intercept)    4.260      0.767    5.55  6.1e-06\n#&gt; data          11.680      0.794   14.70  1.1e-14\n#&gt; \n#&gt; Residual standard error: 4.2 on 28 degrees of freedom\n#&gt; Multiple R-squared:  0.885,  Adjusted R-squared:  0.881 \n#&gt; F-statistic:  216 on 1 and 28 DF,  p-value: 1.07e-14\n\n\nLa matrice x è la matrice di design, che include una colonna di 1 per l’intercetta.\nLa funzione solve() calcola i coefficienti dei minimi quadrati usando l’equazione \\((X'X)^{-1}X'Y\\).\nLa somma dei quadrati residui (\\(SS_{res}\\)) e la somma totale dei quadrati (\\(SS_{tot}\\)) sono calcolate manualmente per derivare \\(R^2\\).\nLa funzione lm() offre un modo alternativo e diretto per ottenere il modello di regressione e i relativi output statistici.\n\n25.8.3 Traccia di una matrice\nSi definisce traccia di una matrice quadrata \\(\\boldsymbol{A}\\) \\(n \\times n\\), e si denota con \\(tr(\\boldsymbol{A})\\) la somma degli elementi sulla diagonale principale di \\(\\boldsymbol{A}\\):\n\\[\ntr(\\boldsymbol{A}) = \\sum_{i=1}^{n} a_{ii}.\n\\]\nLa traccia gode delle seguenti proprietà:\n\\[\n\\begin{aligned}\n&tr(\\rho \\boldsymbol{A}) = \\rho tr( \\boldsymbol{A}) \\notag \\\\\n&tr(\\boldsymbol{A} + \\boldsymbol{B}) =  tr( \\boldsymbol{A})+tr( \\boldsymbol{B}) \\notag \\\\\n&tr(\\boldsymbol{A}') =  tr( \\boldsymbol{A}) \\notag \\\\\n&tr(\\boldsymbol{AB}) =  tr( \\boldsymbol{BA}) \\notag\\end{aligned}\n\\]\nPer esempio, sia\n\\[\n\\boldsymbol{A} =  \\left[ \\begin{array}{c c c}\n7 & 1 & 2\\\\\n1 & 8 & 3\\\\\n2 & 3 & 9 \\end{array} \\right]\n\\]\nallora\n\\[\ntr(\\boldsymbol{A}) = 7 + 8 + 9 = 24.\n\\]\n\nA &lt;- matrix(\n  c(7,1, 2, 1, 8, 3, 2, 3, 9),\n  nrow = 3,\n  byrow = TRUE\n)\nA |&gt; print()\n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]    7    1    2\n#&gt; [2,]    1    8    3\n#&gt; [3,]    2    3    9\n\n\nsum(diag(A)) |&gt; print()\n#&gt; [1] 24\n\n\n25.8.4 Dipendenza lineare\nSi consideri la matrice\n\\[\n\\boldsymbol{A}=\n\\left(%\n\\begin{array}{ccc}\n  1 & 1 & 1 \\\\\n  3 & 1 & 5 \\\\\n  2 & 3 & 1 \\\\\n\\end{array}%\n\\right).\n\\]\nSiano \\(\\boldsymbol{c}_1\\), \\(\\boldsymbol{c}_2\\), \\(\\boldsymbol{c}_3\\) le colonne di \\(\\boldsymbol{A}\\). Si noti che\n\\[\n2\\boldsymbol{c}_1 + -\\boldsymbol{c}_2 + - \\boldsymbol{c}_3 =\n\\boldsymbol{0}\n\\]\ndove \\(\\boldsymbol{0}\\) è un vettore (\\(3 \\times 1\\)) di zeri.\nDato che le 3 colonne di \\(\\boldsymbol{A}\\) possono essere combinate linearmente in modo da produrre un vettore \\(\\boldsymbol{0}\\) vi è chiaramente una qualche forma di relazione, o dipendenza, tra le informazioni nelle colonne. Detto in un altro modo, sembra esserci una qualche duplicazione delle informazione nelle colonne. In generale, si dice che \\(k\\) colonne \\(\\boldsymbol{c}_1, \\boldsymbol{c}_2,\n\\dots \\boldsymbol{c}_k\\) di una matrice sono linearmente dipendenti se esiste un insieme di valori scalari \\(\\lambda_1,\n\\dots, \\lambda_k\\) tale per cui\n\\[\n\\lambda_1 \\boldsymbol{c}_1 + \\dots + \\lambda_k \\boldsymbol{c}_k=\\boldsymbol{0}\n\\]\ne almeno uno dei valori \\(\\lambda_i\\) non è uguale a 0.\nLa dipendenza lineare implica che ciascun vettore colonna è una combinazione degli altri. Per esempio\n\\[\n\\boldsymbol{c}_k= -(\\lambda_1 \\boldsymbol{c}_1 + \\dots + \\lambda_{k-1}\n   \\boldsymbol{c}_{k-1})/\\lambda_k.\n\\]\nQuesto implica che tutta “l’informazione” della matrice è contenuta in un sottoinsieme delle colonne – se \\(k-1\\) colonne sono conosciute, l’ultima resta determinata. È in questo senso che abbiamo detto che l’informazione della matrice veniva “duplicata”.\nSe l’unico insieme di valori scalari \\(\\lambda_i\\) che soddisfa l’equazione\n\\[\n\\lambda_1 \\boldsymbol{c}_1 + \\dots + \\lambda_k \\boldsymbol{c}_k=\\boldsymbol{0}\n\\]\nè un vettore di zeri, allora questo significa che non vi è alcuna relazione tra le colonne della matrice. Le colonne si dicono linearmente indipendenti, nel senso che non contengono alcuna “duplicazione” di informazione.\n\n25.8.5 Rango di una matrice\nIl rango della matrice è il massimo numero di vettori colonna linearmente indipendenti che possono essere selezionati dalla matrice. In maniera equivalente, il rango di una matrice può essere definito come il massimo numero di vettori riga linermente indipendenti. Il rango minimo di una matrice è 1, il che significa che vi è una colonna tale per cui le altre colonne sono dei multipli di questa. Per l’esempio precedente, il rango della matrice \\(\\boldsymbol{A}\\) è 2.\nSe la matrice è quadrata, \\(\\boldsymbol{A}_{n \\times n}\\), ed è costituita da vettori tutti indipendenti tra di loro, allora il suo rango è \\(n\\). Se, invece, la matrice è rettangolare, \\(\\boldsymbol{A}_{m \\times n}\\), allora il suo rango può essere al massimo il più piccolo tra i due valori m ed n, cioè:\n\\[\nr(\\boldsymbol{A}_{m \\times n}) \\leq min(m,n).\n\\]",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/pca/01_linear_algebra.html#radici-e-vettori-latenti",
    "href": "chapters/pca/01_linear_algebra.html#radici-e-vettori-latenti",
    "title": "\n25  Elementi di algebra lineare\n",
    "section": "\n25.9 Radici e vettori latenti",
    "text": "25.9 Radici e vettori latenti\nDal determinante di una matrice si possono ricavare le radici latenti o autovalori (denotati da \\(\\lambda_i\\)) e i vettori latenti o autovettori della matrice. Alle nozioni di autovalore e autovettore verrà qui fornita un’interpretazione geometrica.\nSimuliamo di dati di due variabili associate tra loro:\n\nset.seed(123456)\noptions(repr.plot.width = 8, repr.plot.height = 8)\n\nnpoints &lt;- 20\nx &lt;- as.numeric(scale(rnorm(npoints, 0, 1)))\ny &lt;- as.numeric(scale(3 * x + rnorm(npoints, 0, 2)))\nmean(x) |&gt; print()\n#&gt; [1] -2.78e-17\nmean(y) |&gt; print()\n#&gt; [1] -7.77e-17\ncor(x, y) |&gt; print()\n#&gt; [1] 0.829\n\nDisegnamo il diagramma di dispersione con un ellisse che contiene la nube di punti:\n\nY &lt;- cbind(x, y)\ncar::dataEllipse(\n  Y[, 1], Y[, 2],\n  levels = 0.95,\n  lty = 2,\n  ylim = c(-3, 3),\n  xlim = c(-3, 3)\n)\n\n\n\n\n\n\n\nSe racchiudiamo le osservazioni (\\(v_1, v_2\\)) con un’ellisse, allora la lunghezza dei semiassi maggiori e minori dell’ellisse sarà proporzionale a \\(\\sqrt{\\lambda_1}\\) e \\(\\sqrt{\\lambda_2}\\). L’asse maggiore è la linea passante per il punto (\\(\\bar{v_1}, \\bar{v_2}\\)) nella direzione determinata dal primo autovettore \\(\\boldsymbol{a}_1'\\) con pendenza uguale a \\(a_{12}/a_{11}\\). L’asse minore è la linea passante per il punto (\\(\\bar{v_1}, \\bar{v_2}\\)) nella direzione determinata dal secondo autovettore \\(\\boldsymbol{a}_2\\).\nCalcoliamo ora gli autovettori e gli autovalori:\n\ns &lt;- cov(Y)\nee &lt;- eigen(s)\nee |&gt; print()\n#&gt; eigen() decomposition\n#&gt; $values\n#&gt; [1] 1.829 0.171\n#&gt; \n#&gt; $vectors\n#&gt;       [,1]   [,2]\n#&gt; [1,] 0.707 -0.707\n#&gt; [2,] 0.707  0.707\n\n\n# First eigenvector \nev_1 &lt;- ee$vectors[, 1]\n\n# Slope of the first eigenvector\nev1_m &lt;- ev_1[2] / ev_1[1]\n\n# Second eigenvector \nev_2 &lt;- ee$vectors[, 2]\n\n# Slope of the second eigenvector\nev2_m &lt;- ev_2[2] / ev_2[1]\n\n\n# Scatter plot showing the span of both eigenvectors \ndata.frame(zx=scale(x), zy=scale(y))  |&gt;\nggplot(aes(x = zx, y = zy)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_vline(xintercept = 0, linewidth = .5) +\n  geom_hline(yintercept = 0, linewidth = .5) +\n  geom_abline(slope = ev1_m, color = \"blue\", linewidth = 0.7) +\n  geom_abline(slope = ev2_m, color = \"red\", linewidth = 0.7) \n\n\n\n\n\n\n\nGli autovettori sono ortogonali:\n\n# Multiply both eigenvectors \nprint(ev_1 %*% ev_2)\n#&gt;          [,1]\n#&gt; [1,] 2.24e-17\n\nGeneriamo uno Scree Plot.\n\n# Calculate the estimated variance for each eigenvalue\ne_var &lt;- ee$values / (length(x) - 1)\n\n# Data frame with variance percentages\nvar_per &lt;- tibble(\n  PC  = c(\"PC1\", \"PC2\"),\n  PER = c(e_var) * 100 / sum(e_var) # Calculate the percentage\n    )\n\n# Scree plot \nggplot(var_per, aes(x = PC, y = PER)) +\n  geom_col(width = 0.5, color = \"black\") +\n  xlab(\"Principal component\") +\n  ylab(\"Percentage of variation (%)\") \n\n\n\n\n\n\n\nVerifichiamo che la somma degli autovalori sia uguale alla varianza totale.\n\nvar(x) + var(y)\n#&gt; [1] 2\n\n\nee$values |&gt; sum()\n#&gt; [1] 2\n\nGli autovettori ottenuti utilizzando la funzione eigen() sono normalizzati. Ciò significa che la loro lunghezza è uguale a 1:\n\nt(as.matrix(ee$vectors[, 1])) %*% as.matrix(ee$vectors[, 1]) |&gt; print()\n#&gt;      [,1]\n#&gt; [1,]    1\n\nUtilizziamo le informazioni degli autovettori e degli autovalori per disegnare gli assi dell’ellisse:\n\ngli autovettori determinano la direzione degli assi;\nla radice quadrata degli autovalori è proporzionale alla lunghezza degli assi dell’ellisse.\n\n\ncar::dataEllipse(\n  Y[, 1], Y[, 2],\n  levels = 0.95,\n  lty = 2,\n  ylim = c(-3, 3),\n  xlim = c(-3, 3)\n)\nk &lt;- 2.65\narrows(\n  0, 0, \n  k * sqrt(ee$values[1]) * ee$vectors[1],\n  k * sqrt(ee$values[1]) * ee$vectors[2],\n  code = 2, \n  col = \"red\", \n  lwd = 2\n)\narrows(\n  0, 0, \n  k * sqrt(ee$values[2]) * ee$vectors[1],\n  k * sqrt(ee$values[2]) * -ee$vectors[2],\n  code = 2, \n  col = \"red\", \n  lwd = 2\n)\n\n\n\n\n\n\n\nTale analisi si può estendere a qualunque numero di variabili. Per esempio, nel caso di tre variabili, possiamo pensare di disegnare un ellisoide attorno ad una nube di punti nello spazio tridimensionale. Anche in questo caso, gli autovalori e gli associati autovettori corrisponderanno agli assi dell’elissoide.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/pca/01_linear_algebra.html#scomposizione-spettrale-di-una-matrice",
    "href": "chapters/pca/01_linear_algebra.html#scomposizione-spettrale-di-una-matrice",
    "title": "\n25  Elementi di algebra lineare\n",
    "section": "\n25.10 Scomposizione spettrale di una matrice",
    "text": "25.10 Scomposizione spettrale di una matrice\nData una matrice quadrata e simmetrica di dimensione \\(n\\), \\(\\boldsymbol{A}\\), esistono una matrice diagonale \\(\\boldsymbol{\\Lambda}\\) e una matrice ortogonale \\(\\boldsymbol{V}\\) tali che\n\\[\\boldsymbol{A} =\\boldsymbol{V} \\boldsymbol{\\Lambda} \\boldsymbol{V}',\\] dove\n\n\n\\(\\boldsymbol{\\Lambda}\\) è una matrice diagonale i cui elementi sono gli autovalori di \\(\\boldsymbol{A}\\): \\(\\boldsymbol{\\Lambda} = diag(\\lambda_1, \\lambda_2,\n    \\dots, \\lambda_n)\\);\n\n\\(\\boldsymbol{V}\\) è una matrice ortogonale le cui colonne \\((v_1, v_2, \\dots, v_p)\\) sono gli autovettori di \\(\\boldsymbol{A}\\) associati ai rispettivi autovalori.\n\nIn maniera equivalente\n\\[\\boldsymbol{A} \\boldsymbol{V} =  \\boldsymbol{\\Lambda} \\boldsymbol{V}'.\\]\nPremoltiplicando entrambi i membri per \\(\\boldsymbol{V}'\\) si ottiene\n\\[\\boldsymbol{V}'\\boldsymbol{A} \\boldsymbol{V} =\n\\boldsymbol{\\Lambda},\\]\nda cui l’affermazione che la matrice degli autovettori diagonalizza \\(\\boldsymbol{A}\\).\nPer esempio,\n\nsigma &lt;- matrix(\n  data = c(1, 0.5, 0.5, 1.25), \n  nrow = 2, \n  ncol = 2\n)\nsigma |&gt; print()\n#&gt;      [,1] [,2]\n#&gt; [1,]  1.0 0.50\n#&gt; [2,]  0.5 1.25\n\n\nout &lt;- eigen(sigma)\nout |&gt; print()\n#&gt; eigen() decomposition\n#&gt; $values\n#&gt; [1] 1.64 0.61\n#&gt; \n#&gt; $vectors\n#&gt;       [,1]   [,2]\n#&gt; [1,] 0.615 -0.788\n#&gt; [2,] 0.788  0.615\n\n\nLambda &lt;- diag(out$values)\nLambda |&gt; print()\n#&gt;      [,1] [,2]\n#&gt; [1,] 1.64 0.00\n#&gt; [2,] 0.00 0.61\n\n\nU &lt;- out$vectors\nU |&gt; print()\n#&gt;       [,1]   [,2]\n#&gt; [1,] 0.615 -0.788\n#&gt; [2,] 0.788  0.615\n\n\nU %*% Lambda %*% t(U) |&gt; print()\n#&gt;      [,1] [,2]\n#&gt; [1,]  1.0 0.50\n#&gt; [2,]  0.5 1.25\n\n\n25.10.1 Autovalori e determinante\nIl determinante di una matrice è il prodotto degli autovalori:\n\\[\\begin{aligned}\n    |\\boldsymbol{A}| &= \\prod_{i=1}^{p} \\lambda_i. \\notag\n    \\end{aligned}\\]\nLa traccia di una matrice è uguale alla somma degli autovalori:\n\\[\\begin{aligned}\n    tr(\\boldsymbol{A}) &= \\sum_{i=1}^{p} \\lambda_i. \\notag\n    \\end{aligned}\\]\n\nsigma &lt;- matrix(data = c(1, 0.5, 0.5, 2), nrow = 2, ncol = 2)\nsigma |&gt; print()\n#&gt;      [,1] [,2]\n#&gt; [1,]  1.0  0.5\n#&gt; [2,]  0.5  2.0\n\nout &lt;- eigen(sigma)\nout |&gt; print()\n#&gt; eigen() decomposition\n#&gt; $values\n#&gt; [1] 2.207 0.793\n#&gt; \n#&gt; $vectors\n#&gt;       [,1]   [,2]\n#&gt; [1,] 0.383 -0.924\n#&gt; [2,] 0.924  0.383\n\nLa traccia di una matrice è uguale alla somma degli autovalori:\n\nsum(out$values) |&gt; print()\n#&gt; [1] 3\n\nIl determinante di una matrice è il prodotto degli autovalori:\n\ndet(sigma) |&gt; print()\n#&gt; [1] 1.75\n(out$values[1] * out$values[2]) |&gt; print()\n#&gt; [1] 1.75\n\nGli autovalori di \\(\\boldsymbol{A}^{-1}\\) sono i reciproci degli autovalori di \\(\\boldsymbol{A}\\); gli autovettori sono coincidenti.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/pca/01_linear_algebra.html#la-distanza-euclidea",
    "href": "chapters/pca/01_linear_algebra.html#la-distanza-euclidea",
    "title": "\n25  Elementi di algebra lineare\n",
    "section": "\n25.11 La distanza euclidea",
    "text": "25.11 La distanza euclidea\nPer calcolare la distanza euclidea tra due punti utilizzando l’algebra matriciale, consideriamo i punti come vettori in uno spazio euclideo.\n\n25.11.1 In due dimensioni:\nSiano dati i vettori:\n\n\nx = \\(\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\\),\n\ny = \\(\\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix}\\).\n\nLa distanza euclidea tra x e y è la norma del vettore differenza (\\(\\mathbf{x} - \\mathbf{y}\\)):\n\\[ d(\\mathbf{x}, \\mathbf{y}) = \\| \\mathbf{x} - \\mathbf{y} \\| \\].\nIn termini di algebra matriciale, questa norma è calcolata come:\n\\[ d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{ (\\mathbf{x} - \\mathbf{y})^\\top (\\mathbf{x} - \\mathbf{y}) } \\]\nPassaggi dettagliati:\n\n\nCalcolo del vettore differenza:\n\\[\n\\mathbf{x} - \\mathbf{y} = \\begin{bmatrix} x_1 - y_1 \\\\ x_2 - y_2 \\end{bmatrix}\n\\]\n\n\nCalcolo del prodotto scalare:\n\\[\n(\\mathbf{x} - \\mathbf{y})^\\top (\\mathbf{x} - \\mathbf{y}) = (x_1 - y_1)^2 + (x_2 - y_2)^2\n\\]\n\n\nCalcolo della radice quadrata:\n\\[\nd(\\mathbf{x}, \\mathbf{y}) = \\sqrt{ (x_1 - y_1)^2 + (x_2 - y_2)^2 }\n\\]\n\n\nQuesto risultato corrisponde alla formula classica per la distanza tra due punti nel piano cartesiano.\n\n25.11.2 Estensione a più dimensioni\nPer vettori in uno spazio $ n $-dimensionale:\n\n\nx = \\(\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}\\)\n\n\ny = \\(\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\\)\n\n\nLa distanza euclidea diventa:\n\\[ d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{ (\\mathbf{x} - \\mathbf{y})^\\top (\\mathbf{x} - \\mathbf{y}) } \\]\nChe si espande in:\n\\[ d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{ \\sum_{i=1}^{n} (x_i - y_i)^2 } \\]\nIn conclusione:\nl’utilizzo dell’algebra matriciale permette di esprimere in modo compatto e generalizzato il calcolo della distanza euclidea tra due punti in qualsiasi dimensione, sfruttando operazioni matriciali come la trasposizione e il prodotto scalare.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/pca/01_linear_algebra.html#la-distanza-di-mahalanobis",
    "href": "chapters/pca/01_linear_algebra.html#la-distanza-di-mahalanobis",
    "title": "\n25  Elementi di algebra lineare\n",
    "section": "\n25.12 La distanza di Mahalanobis",
    "text": "25.12 La distanza di Mahalanobis\nLa distanza euclidea presuppone che le variabili siano non correlate e su scale comparabili. Tuttavia, in molti casi, le variabili possono avere scale diverse e possono essere correlate tra loro. La distanza di Mahalanobis tiene conto di queste differenze utilizzando la matrice di covarianza, permettendo una misurazione della distanza che considera sia la scala che la correlazione tra le variabili.\n\n25.12.1 Definizione\nLa distanza di Mahalanobis tra due vettori \\(\\mathbf{x}\\) e \\(\\mathbf{y}\\) è definita come:\n\\[\nd_M(\\mathbf{x}, \\mathbf{y}) = \\sqrt{ (\\mathbf{x} - \\mathbf{y})^\\top \\mathbf{S}^{-1} (\\mathbf{x} - \\mathbf{y}) }\n\\]\ndove:\n\n\n\\(\\mathbf{S}\\) è la matrice di covarianza delle variabili.\n\n\\(\\mathbf{S}^{-1}\\) è l’inversa della matrice di covarianza.\n\n25.12.2 Perché è necessaria la matrice di covarianza?\n\n\nScala delle variabili: Se le variabili hanno varianze diverse, la matrice di covarianza normalizza queste differenze, evitando che variabili con varianze maggiori dominino la misura della distanza.\n\nCorrelazione tra variabili: La matrice di covarianza tiene conto delle correlazioni tra le variabili, riducendo l’influenza delle variabili altamente correlate sulla distanza totale.\n\n25.12.3 Esempio numerico\nSupponiamo di avere due punti in uno spazio bidimensionale:\n\n\nPunto A: \\(\\mathbf{x} = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}\\)\n\n\nPunto B: \\(\\mathbf{y} = \\begin{bmatrix} 5 \\\\ 7 \\end{bmatrix}\\)\n\n\nE una matrice di covarianza stimata:\n\\[\n\\mathbf{S} = \\begin{bmatrix}\n4 & 2 \\\\\n2 & 3\n\\end{bmatrix}\n\\]\n\n25.12.3.1 Passaggi per il calcolo\n\n\nCalcolo del vettore differenza \\(\\mathbf{d}\\):\n\\[\n\\mathbf{d} = \\mathbf{x} - \\mathbf{y} = \\begin{bmatrix} 2 - 5 \\\\ 3 - 7 \\end{bmatrix} = \\begin{bmatrix} -3 \\\\ -4 \\end{bmatrix}\n\\]\n\n\nCalcolo dell’inversa della matrice di covarianza \\(\\mathbf{S}^{-1}\\):\n\n\nDeterminante di \\(\\mathbf{S}\\):\n\\[\n\\det(\\mathbf{S}) = (4)(3) - (2)(2) = 12 - 4 = 8\n\\]\n\n\nMatrice aggiunta (comatrice trasposta) di \\(\\mathbf{S}\\):\n\\[\n\\text{adj}(\\mathbf{S}) = \\begin{bmatrix}\n3 & -2 \\\\\n-2 & 4\n\\end{bmatrix}\n\\]\n\n\nInversa di \\(\\mathbf{S}\\):\n\\[\n\\mathbf{S}^{-1} = \\frac{1}{\\det(\\mathbf{S})} \\text{adj}(\\mathbf{S}) = \\frac{1}{8} \\begin{bmatrix}\n3 & -2 \\\\\n-2 & 4\n\\end{bmatrix} = \\begin{bmatrix}\n0.375 & -0.25 \\\\\n-0.25 & 0.5\n\\end{bmatrix}\n\\]\n\n\n\n\nCalcolo della distanza di Mahalanobis:\n\\[\nd_M(\\mathbf{x}, \\mathbf{y}) = \\sqrt{ \\mathbf{d}^\\top \\mathbf{S}^{-1} \\mathbf{d} }\n\\]\n\n\nCalcolo del prodotto \\(\\mathbf{S}^{-1} \\mathbf{d}\\):\n\\[\n\\mathbf{S}^{-1} \\mathbf{d} = \\begin{bmatrix}\n0.375 & -0.25 \\\\\n-0.25 & 0.5\n\\end{bmatrix} \\begin{bmatrix} -3 \\\\ -4 \\end{bmatrix} = \\begin{bmatrix}\n(-0.375 \\times 3) + (0.25 \\times 4) \\\\\n(0.25 \\times 3) + (-0.5 \\times 4)\n\\end{bmatrix} = \\begin{bmatrix}\n-0.125 \\\\\n-1.25\n\\end{bmatrix}\n\\]\n\n\nCalcolo del prodotto scalare:\n\\[\n\\mathbf{d}^\\top (\\mathbf{S}^{-1} \\mathbf{d}) = \\begin{bmatrix} -3 & -4 \\end{bmatrix} \\begin{bmatrix} -0.125 \\\\ -1.25 \\end{bmatrix} = (-3)(-0.125) + (-4)(-1.25) = 0.375 + 5 = 5.375\n\\]\n\n\nCalcolo della distanza:\n\\[\nd_M(\\mathbf{x}, \\mathbf{y}) = \\sqrt{5.375} \\approx 2.318\n\\]\n\n\n\n\n25.12.4 Confronto con la distanza euclidea\nLa distanza euclidea tra gli stessi punti è:\n\\[\nd_E(\\mathbf{x}, \\mathbf{y}) = \\sqrt{ (-3)^2 + (-4)^2 } = \\sqrt{9 + 16} = \\sqrt{25} = 5\n\\]\nCome si può notare, la distanza di Mahalanobis (\\(\\approx 2.318\\)) è diversa dalla distanza euclidea (5) a causa della considerazione delle varianze e delle correlazioni tra le variabili.\n\n25.12.5 Interpretazione\n\n\nVarianze diverse: Se una variabile ha una varianza elevata, le differenze lungo quella direzione avranno meno peso nella distanza totale.\n\nCorrelazioni: Se due variabili sono altamente correlate, la distanza di Mahalanobis riduce l’importanza delle differenze lungo la direzione in cui le variabili sono correlate.\n\nIn conclusione, la distanza di Mahalanobis è particolarmente utile in contesti multivariati dove le variabili hanno scale diverse e possono essere correlate. Essa fornisce una misura di distanza che è invariante rispetto alle trasformazioni lineari dei dati, rendendola ideale per l’analisi di dati statistici e il rilevamento di outlier.\nNota: È importante assicurarsi che la matrice di covarianza \\(\\mathbf{S}\\) sia non singolare (invertibile). In pratica, quando si lavora con campioni di dati, \\(\\mathbf{S}\\) viene stimata dai dati stessi.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/pca/01_linear_algebra.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/pca/01_linear_algebra.html#informazioni-sullambiente-di-sviluppo",
    "title": "\n25  Elementi di algebra lineare\n",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] ggokabeito_0.1.0  see_0.10.0        MASS_7.3-65       viridis_0.6.5    \n#&gt;  [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n#&gt;  [9] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n#&gt; [13] lavaan_0.6-19     psych_2.4.12      scales_1.3.0      markdown_1.13    \n#&gt; [17] knitr_1.49        lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n#&gt; [21] dplyr_1.1.4       purrr_1.0.4       readr_2.1.5       tidyr_1.3.1      \n#&gt; [25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.0      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] mime_0.12           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [25] Matrix_1.7-2        R6_2.6.1            fastmap_1.2.0      \n#&gt;  [28] rbibutils_2.3       shiny_1.10.0        digest_0.6.37      \n#&gt;  [31] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n#&gt;  [34] rprojroot_2.0.4     Hmisc_5.2-2         labeling_0.4.3     \n#&gt;  [37] timechange_0.3.0    abind_1.4-8         compiler_4.4.2     \n#&gt;  [40] withr_3.0.2         glasso_1.11         htmlTable_2.4.3    \n#&gt;  [43] backports_1.5.0     carData_3.0-5       ggsignif_0.6.4     \n#&gt;  [46] corpcor_1.6.10      gtools_3.9.5        tools_4.4.2        \n#&gt;  [49] pbivnorm_0.6.0      foreign_0.8-88      zip_2.3.2          \n#&gt;  [52] httpuv_1.6.15       nnet_7.3-20         glue_1.8.0         \n#&gt;  [55] quadprog_1.5-8      nlme_3.1-167        promises_1.3.2     \n#&gt;  [58] lisrelToR_0.3       grid_4.4.2          checkmate_2.3.2    \n#&gt;  [61] cluster_2.1.8       reshape2_1.4.4      generics_0.1.3     \n#&gt;  [64] gtable_0.3.6        tzdb_0.4.0          data.table_1.17.0  \n#&gt;  [67] hms_1.1.3           car_3.1-3           sem_3.1-16         \n#&gt;  [70] pillar_1.10.1       rockchalk_1.8.157   later_1.4.1        \n#&gt;  [73] splines_4.4.2       lattice_0.22-6      survival_3.8-3     \n#&gt;  [76] kutils_1.73         tidyselect_1.2.1    miniUI_0.1.1.1     \n#&gt;  [79] pbapply_1.7-2       reformulas_0.4.0    stats4_4.4.2       \n#&gt;  [82] xfun_0.51           qgraph_1.9.8        arm_1.14-4         \n#&gt;  [85] stringi_1.8.4       yaml_2.3.10         pacman_0.5.1       \n#&gt;  [88] boot_1.3-31         evaluate_1.0.3      codetools_0.2-20   \n#&gt;  [91] mi_1.1              cli_3.6.4           RcppParallel_5.1.10\n#&gt;  [94] rpart_4.1.24        xtable_1.8-4        Rdpack_2.6.2       \n#&gt;  [97] munsell_0.5.1       Rcpp_1.0.14         coda_0.19-4.1      \n#&gt; [100] png_0.1-8           XML_3.99-0.18       parallel_4.4.2     \n#&gt; [103] jpeg_0.1-10         lme4_1.1-36         mvtnorm_1.3-3      \n#&gt; [106] openxlsx_4.2.8      rlang_1.1.5         multcomp_1.4-28    \n#&gt; [109] mnormt_2.1.1\n\n\n\n\n\nCaudek, C., & Luccio, R. (2001). Statistica per psicologi (III rist. 2023, Vol. 11, p. 320). Laterza.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/pca/02_pca.html",
    "href": "chapters/pca/02_pca.html",
    "title": "\n26  Analisi delle componenti principali\n",
    "section": "",
    "text": "26.1 Introduzione\nL’Analisi delle Componenti Principali (PCA) è una tecnica di riduzione dei dati che permette di semplificare analisi complesse riducendo un grande numero di variabili correlate a un insieme più piccolo di componenti principali. Queste componenti sono nuove variabili calcolate come combinazioni lineari delle variabili originali, progettate per spiegare la massima varianza possibile nei dati.\nIn psicologia, la PCA è ampiamente utilizzata per:",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Analisi delle componenti principali</span>"
    ]
  },
  {
    "objectID": "chapters/pca/02_pca.html#introduzione",
    "href": "chapters/pca/02_pca.html#introduzione",
    "title": "\n26  Analisi delle componenti principali\n",
    "section": "",
    "text": "Ridurre il numero di variabili in studi con molti questionari o test psicometrici.\nIdentificare le dimensioni sottostanti a un set di item (ad esempio, esplorare le dimensioni latenti di una scala).\nPreparare i dati per analisi successive (ad esempio, in regressioni o modelli strutturali).\n\n\n26.1.1 Perché Usare la PCA in Psicologia?\nQuando un grande numero di variabili è fortemente correlato, può essere difficile interpretare i dati. In questi casi, la PCA permette di semplificare l’analisi mantenendo gran parte dell’informazione originale:\n\nLe componenti principali catturano la varianza condivisa tra le variabili, fornendo un riepilogo efficace dei dati.\nSe le prime componenti principali spiegano una quota sostanziale della varianza totale, possiamo ridurre il numero di variabili senza perdere significative informazioni.\n\n26.1.2 Cos’è la Varianza Totale?\nLa varianza totale rappresenta la quantità complessiva di variabilità nei dati. Nella PCA, è definita come la somma delle varianze delle variabili originali. Ad esempio, se abbiamo un dataset con tre variabili, la varianza totale è:\n\\[\n\\text{Varianza Totale} = \\sigma_1^2 + \\sigma_2^2 + \\sigma_3^2 ,\n\\]\ndove \\(\\sigma_i^2\\) è la varianza della variabile \\(i\\)-esima.\nNella PCA:\n\nGli autovalori (eigenvalues) rappresentano la varianza spiegata da ciascuna componente principale.\nLa somma degli autovalori corrisponde alla varianza totale dei dati:\n\n\\[\n\\text{Somma degli autovalori} = \\text{Varianza Totale}\n\\]\n\n26.1.3 Un Nuovo Sistema di Coordinate\nLa PCA può essere interpretata come una ridescrizione dei dati in un nuovo sistema di assi coordinati. Questi nuovi assi (le componenti principali) sono calcolati come segue:\n\nLe componenti principali sono orientate lungo le direzioni di massima varianza nei dati.\nLa prima componente principale (PC1) è la direzione che spiega la massima quantità di varianza.\nLa seconda componente principale (PC2) è ortogonale alla prima e spiega la successiva maggiore quantità di varianza, e così via.\n\nQuesto significa che la PCA non elimina le variabili, ma le ricombina in modo tale da rappresentare i dati in un sistema più semplice e interpretabile.\nEsempio Pratico. Supponiamo di avere 10 variabili in un questionario psicologico, molte delle quali sono fortemente correlate. Con la PCA, potremmo scoprire che le prime due componenti principali spiegano l’80% della varianza totale. In questo caso, potremmo ridurre l’analisi a queste due componenti, semplificando notevolmente l’interpretazione.\n\n26.1.4 Riduzione della Dimensionalità\nL’obiettivo principale della PCA è dunque quello di identificare il minor numero di componenti che spiegano la maggior parte della varianza nei dati. In psicologia, questo è particolarmente utile quando:\n\nSi vuole ridurre il numero di variabili per facilitare l’interpretazione.\nSi cerca di individuare dimensioni sottostanti (ad esempio, in uno studio sui tratti di personalità).\n\nAd esempio, in uno studio sui Big Five, la PCA potrebbe ridurre centinaia di item iniziali alle cinque dimensioni principali.\n\n26.1.5 Interpretazione dei Risultati\nLa PCA produce due risultati principali:\n\nPunteggi delle Componenti Principali: Ogni osservazione ottiene un punteggio per ciascuna componente principale, che rappresenta la sua posizione nel nuovo spazio.\nVarianza Spiegata: La proporzione di varianza spiegata da ciascuna componente principale è un indicatore della sua importanza: \\(\\text{Varianza Spiegata per PC} = \\text{Autovalore della PC} / \\text{Somma degli autovalori}\\).\n\nSe, ad esempio, la PC1 spiega il 60% della varianza e la PC2 il 20%, possiamo concludere che le prime due componenti rappresentano l’80% della variabilità nei dati.\nIn sintesi, la PCA è uno strumento potente per semplificare e interpretare dataset complessi in psicologia, soprattutto quando ci troviamo di fronte a molte variabili correlate. Questo metodo non solo facilita l’analisi, ma può anche fornire una nuova prospettiva sulle relazioni tra le variabili, evidenziando dimensioni latenti che altrimenti potrebbero non essere immediatamente evidenti.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Analisi delle componenti principali</span>"
    ]
  },
  {
    "objectID": "chapters/pca/02_pca.html#tutorial",
    "href": "chapters/pca/02_pca.html#tutorial",
    "title": "\n26  Analisi delle componenti principali\n",
    "section": "\n26.2 Tutorial",
    "text": "26.2 Tutorial\nEsaminiamo qui di seguito l’analisi delle componenti principali passo passo.\n\n26.2.1 Passo 1: Creare un dataset\nPer cominciare, generiamo un dataset di esempio per applicare la PCA.\n\n# Generare un dataset con due variabili correlate\nset.seed(123)\nX &lt;- data.frame(\n  x1 = rnorm(100, mean = 5, sd = 2),\n  x2 = rnorm(100, mean = 10, sd = 3)\n)\nX$x2 &lt;- 0.8 * X$x1 + rnorm(100, mean = 0, sd = 1)  # Introduciamo correlazione\n\n\n26.2.2 Passo 2: Standardizzare i dati\nPrima di calcolare la PCA, è importante standardizzare le variabili (sottrarre la media e dividere per la deviazione standard) per garantire che abbiano lo stesso peso.\n\n# Centrare e scalare le variabili\nX_scaled &lt;- scale(X)\n\n# Plot con aspect ratio 1\nplot(X_scaled[, 1], X_scaled[, 2], asp = 1, \n     col = \"blue\", pch = 19, \n     main = \"Dati standardizzati con aspect ratio = 1\",\n     xlab = \"Variabile x1 standardizzata\",\n     ylab = \"Variabile x2 standardizzata\")\n\n\n\n\n\n\n\n\ncar::dataEllipse(\n  X_scaled[, 1], X_scaled[, 2],\n  levels = 0.95,\n  lty = 2,\n  ylim = c(-3, 3),\n  xlim = c(-3, 3),\n  asp = 1, \n)\n\n\n\n\n\n\n\n\n26.2.3 Passo 3: Calcolare la matrice di covarianza\nLa PCA utilizza la matrice di covarianza per calcolare le componenti principali.\n\ncov_matrix &lt;- cov(X_scaled)\nprint(cov_matrix)\n#&gt;       x1    x2\n#&gt; x1 1.000 0.818\n#&gt; x2 0.818 1.000\n\n\n26.2.4 Passo 4: Calcolare autovalori e autovettori\nUtilizziamo l’algebra lineare per ottenere gli autovalori e gli autovettori della matrice di covarianza.\n\neigen_decomp &lt;- eigen(cov_matrix)\neigenvalues &lt;- eigen_decomp$values       # Autovalori\neigenvectors &lt;- eigen_decomp$vectors     # Autovettori\nprint(eigenvalues)\n#&gt; [1] 1.818 0.182\n\n\nprint(eigenvectors)\n#&gt;       [,1]   [,2]\n#&gt; [1,] 0.707 -0.707\n#&gt; [2,] 0.707  0.707\n\nGli autovalori rappresentano la varianza spiegata dalle componenti principali, mentre gli autovettori indicano le direzioni delle componenti principali.\n\n# First eigenvector \nev_1 &lt;- eigen_decomp$vectors[, 1]\n\n# Slope of the first eigenvector\nev1_m &lt;- ev_1[2] / ev_1[1]\n\n# Second eigenvector \nev_2 &lt;- eigen_decomp$vectors[, 2]\n\n# Slope of the second eigenvector\nev2_m &lt;- ev_2[2] / ev_2[1]\n\n\n# Scatter plot showing the span of both eigenvectors \ndata.frame(zx=X_scaled[, 1], zy= X_scaled[, 2])  |&gt;\nggplot(aes(x = zx, y = zy)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_vline(xintercept = 0, linewidth = .5) +\n  geom_hline(yintercept = 0, linewidth = .5) +\n  geom_abline(slope = ev1_m, color = \"blue\", linewidth = 0.7) +\n  geom_abline(slope = ev2_m, color = \"red\", linewidth = 0.7) \n\n\n\n\n\n\n\nGli autovettori sono ortogonali:\n\nprint(ev_1 %*% ev_2)\n#&gt;          [,1]\n#&gt; [1,] 2.24e-17\n\nGeneriamo uno Scree Plot.\n\n# Calculate the estimated variance for each eigenvalue\ne_var &lt;- eigen_decomp$values / (length(X_scaled[, 1]) - 1)\n\n# Data frame with variance percentages\nvar_per &lt;- tibble(\n  PC  = c(\"PC1\", \"PC2\"),\n  PER = c(e_var) * 100 / sum(e_var) # Calculate the percentage\n    )\n\n# Scree plot \nggplot(var_per, aes(x = PC, y = PER)) +\n  geom_col(width = 0.5, color = \"black\") +\n  xlab(\"Principal component\") +\n  ylab(\"Percentage of variation (%)\") \n\n\n\n\n\n\n\nVerifichiamo che la somma degli autovalori sia uguale alla varianza totale.\n\nvar(X_scaled[, 1]) + var(X_scaled[, 2])\n#&gt; [1] 2\n\n\neigen_decomp$values |&gt; sum()\n#&gt; [1] 2\n\nGli autovettori ottenuti utilizzando la funzione eigen() sono normalizzati. Ciò significa che la loro lunghezza è uguale a 1:\n\nt(as.matrix(eigen_decomp$vectors[, 1])) %*% \n  as.matrix(eigen_decomp$vectors[, 1]) \n#&gt;      [,1]\n#&gt; [1,]    1\n\nUtilizziamo le informazioni degli autovettori e degli autovalori per disegnare gli assi dell’ellisse:\n\ngli autovettori determinano la direzione degli assi;\nla radice quadrata degli autovalori è proporzionale alla lunghezza degli assi dell’ellisse.\n\n\ncar::dataEllipse(\n  X_scaled[, 1], X_scaled[, 2],\n  levels = 0.95,\n  lty = 2,\n  ylim = c(-3, 3),\n  xlim = c(-3, 3), \n  asp = 1\n)\nk &lt;- 2.5\narrows(\n  0, 0, \n  k * sqrt(eigen_decomp$values[1]) * eigen_decomp$vectors[1],\n  k * sqrt(eigen_decomp$values[1]) * eigen_decomp$vectors[2],\n  code = 2, \n  col = \"red\", \n  lwd = 2\n)\narrows(\n  0, 0, \n  k * sqrt(eigen_decomp$values[2]) * eigen_decomp$vectors[1],\n  k * sqrt(eigen_decomp$values[2]) * -eigen_decomp$vectors[2],\n  code = 2, \n  col = \"red\", \n  lwd = 2\n)\n\n\n\n\n\n\n\nTale analisi si può estendere a qualunque numero di variabili. Per esempio, nel caso di tre variabili, possiamo pensare di disegnare un ellisoide attorno ad una nube di punti nello spazio tridimensionale. Anche in questo caso, gli autovalori e gli associati autovettori corrisponderanno agli assi dell’elissoide.\n\n26.2.5 Passo 5: Proiettare i dati sulle componenti principali\nPer calcolare i punteggi delle Componenti Principali, dobbiamo proiettare ortogonalmente i punti originali del dataset sulle nuove coordinate, definite dalle direzioni principali (autovettori). Questo processo ci permette di rappresentare ogni osservazione nello spazio delle componenti principali.\nNell’algebra lineare, la proiezione ortogonale consiste nel trovare la posizione di un punto su una retta o un piano, in modo che il vettore risultante sia perpendicolare alla direzione di proiezione.\nNel contesto della PCA:\n\nGli autovettori rappresentano le direzioni principali (componenti principali) lungo cui la varianza dei dati è massimizzata.\nProiettare un punto sui componenti principali significa calcolare la sua posizione lungo queste nuove direzioni.\n\n\n26.2.5.1 Formulazione Matematica\nConsideriamo le seguenti matrici:\n\n\n\\(\\mathbf{X}_{\\text{scaled}}\\): la matrice dei dati standardizzati, in cui ogni riga rappresenta un’osservazione e ogni colonna una variabile.\n\n\\(\\mathbf{V}\\): la matrice degli autovettori, le cui colonne rappresentano le nuove direzioni principali.\n\nLa proiezione dei dati nello spazio delle componenti principali si calcola come:\n\\[\n\\mathbf{Z} = \\mathbf{X}_{\\text{scaled}} \\cdot \\mathbf{V}\n\\]\ndove:\n\n\n\\(\\mathbf{Z}\\) è la matrice dei punteggi delle componenti principali.\nOgni riga di \\(\\mathbf{Z}\\) rappresenta un’osservazione trasformata nello spazio delle componenti principali.\nOgni colonna di \\(\\mathbf{Z}\\) corrisponde a una componente principale (ad esempio, PC1, PC2).\n\n26.2.5.2 Implementazione in R\nIn R, questo calcolo può essere realizzato attraverso il prodotto matrice-matrice. Ecco il codice per calcolare i punteggi delle componenti principali:\n\n# Calcolo dei punteggi delle componenti principali\npc_scores &lt;- as.matrix(X_scaled) %*% eigenvectors\ncolnames(pc_scores) &lt;- c(\"PC1\", \"PC2\")  # Etichettare le componenti principali\n\nPer verificare i risultati, possiamo visualizzare i primi punteggi calcolati:\n\n# Stampare i primi punteggi delle componenti principali\nprint(head(pc_scores))\n#&gt;          PC1    PC2\n#&gt; [1,] -0.0561  0.952\n#&gt; [2,]  0.0451  0.542\n#&gt; [3,]  1.9861 -0.289\n#&gt; [4,]  0.1535  0.184\n#&gt; [5,] -0.1741 -0.234\n#&gt; [6,]  2.1241 -0.393\n\n\n26.2.5.3 Interpretazione dei Punteggi\nOgni valore in pc_scores rappresenta la posizione dell’osservazione nello spazio trasformato delle componenti principali:\n\nLa PC1 è la direzione lungo cui si osserva la massima varianza dei dati.\nLa PC2 è la direzione ortogonale successiva con la seconda massima varianza, e così via.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Analisi delle componenti principali</span>"
    ]
  },
  {
    "objectID": "chapters/pca/02_pca.html#passo-6-confrontare-con-loutput-di-prcomp",
    "href": "chapters/pca/02_pca.html#passo-6-confrontare-con-loutput-di-prcomp",
    "title": "\n26  Analisi delle componenti principali\n",
    "section": "\n26.3 Passo 6: Confrontare con l’output di prcomp\n",
    "text": "26.3 Passo 6: Confrontare con l’output di prcomp\n\nUtilizziamo la funzione prcomp di R per confermare i risultati.\n\npca &lt;- prcomp(X, scale. = TRUE)\nprint(pca)\n#&gt; Standard deviations (1, .., p=2):\n#&gt; [1] 1.348 0.427\n#&gt; \n#&gt; Rotation (n x k) = (2 x 2):\n#&gt;      PC1    PC2\n#&gt; x1 0.707  0.707\n#&gt; x2 0.707 -0.707\n\n\n# Confronto tra i punteggi calcolati manualmente e quelli di prcomp\nprint(head(pca$x))\n#&gt;          PC1    PC2\n#&gt; [1,] -0.0561 -0.952\n#&gt; [2,]  0.0451 -0.542\n#&gt; [3,]  1.9861  0.289\n#&gt; [4,]  0.1535 -0.184\n#&gt; [5,] -0.1741  0.234\n#&gt; [6,]  2.1241  0.393\n\n\n26.3.1 Passo 7: Visualizzare la proiezione dei dati\nPossiamo visualizzare i punti originali proiettati sulle componenti principali.\n\n# Grafico del dataset originale\nplot(\n  X_scaled, \n  col = \"blue\", pch = 19, \n  main = \"Dati originali e componenti principali\",\n  asp = 1\n)\nabline(0, eigenvectors[2,1] / eigenvectors[1,1], col = \"red\", lwd = 2)  \n# Prima componente\nabline(0, eigenvectors[2,2] / eigenvectors[1,2], col = \"green\", lwd = 2)  \n# Seconda componente\n\n\n\n\n\n\n\n\n# Grafico delle componenti principali\nplot(\n  pc_scores, \n  col = \"blue\", pch = 19, \n  main = \"Punteggi delle componenti principali\",\n  asp = 1)",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Analisi delle componenti principali</span>"
    ]
  },
  {
    "objectID": "chapters/pca/02_pca.html#biplot",
    "href": "chapters/pca/02_pca.html#biplot",
    "title": "\n26  Analisi delle componenti principali\n",
    "section": "\n26.4 Biplot",
    "text": "26.4 Biplot\nIl biplot è uno strumento grafico che combina la visualizzazione dei punteggi delle componenti principali e delle variabili originali in un unico grafico. Questo permette di:\n\nInterpretare la relazione tra le variabili originali.\nVisualizzare come le osservazioni (campioni) si distribuiscono nello spazio delle componenti principali.\nIdentificare cluster, outlier, o pattern nei dati.\n\nUn biplot combina due tipi di informazioni:\n\n\nI punteggi delle componenti principali (proiezioni delle osservazioni sulle componenti principali), rappresentati come punti.\n\nI carichi delle variabili originali sulle componenti principali (autovettori), rappresentati come frecce.\n\nLe frecce indicano:\n\nLa direzione della variabilità spiegata da ciascuna variabile.\nLa correlazione tra le variabili e le componenti principali.\n\n\n26.4.1 Come Creare un Biplot in R\nPer creare un biplot in R possiamo utilizzare prcomp. Supponiamo di avere già calcolato la PCA con la funzione prcomp:\n\n# PCA con prcomp\npca &lt;- prcomp(X, scale. = TRUE)\n\nIl biplot si visualizza nel modo seguente.\n\n# Creare un biplot\nbiplot(\n  pca, scale = 0, \n  main = \"Biplot delle Componenti Principali\", \n  xlab = \"PC1\", ylab = \"PC2\"\n)\n\n\n\n\n\n\n\n\n\nscale = 0: Evita di ridimensionare le frecce e i punteggi per semplificare l’interpretazione.\n\nNel grafico:\n\n\nI punti rappresentano le osservazioni, proiettate sulle componenti principali.\n\nLe frecce rappresentano le variabili originali, con:\n\nLa lunghezza della freccia che indica la forza della correlazione con le componenti principali.\nL’angolo tra due frecce che rappresenta la correlazione tra le due variabili:\n\nUn angolo piccolo indica una correlazione positiva.\nUn angolo di 90° indica una correlazione nulla.\nUn angolo ampio (vicino a 180°) indica una correlazione negativa.\n\n\n\n\n\n26.4.2 Interpretazione\nIn psicologia, il biplot è particolarmente utile per:\n\n\nIdentificare pattern nei dati: Ad esempio, come i partecipanti si distribuiscono lungo dimensioni psicologiche latenti (es. tratti di personalità).\n\nEsaminare le relazioni tra variabili: Le frecce possono evidenziare cluster di variabili correlate che rappresentano dimensioni psicologiche (es. ansia, stress, depressione).\n\nValutare l’adeguatezza della PCA: Se le frecce delle variabili sono lunghe e ben distribuite lungo le componenti principali, ciò suggerisce che la PCA sta spiegando bene la varianza delle variabili.\n\nIn sostanza, il biplot è uno strumento grafico che semplifica l’interpretazione della PCA. Combina in un unico diagramma sia le informazioni sulle variabili originali che sulla loro proiezione nello spazio delle componenti principali, offrendo una visione d’insieme chiara e immediata dei dati.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Analisi delle componenti principali</span>"
    ]
  },
  {
    "objectID": "chapters/pca/02_pca.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/pca/02_pca.html#informazioni-sullambiente-di-sviluppo",
    "title": "\n26  Analisi delle componenti principali\n",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] ggokabeito_0.1.0  see_0.10.0        MASS_7.3-65       viridis_0.6.5    \n#&gt;  [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n#&gt;  [9] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n#&gt; [13] lavaan_0.6-19     psych_2.4.12      scales_1.3.0      markdown_1.13    \n#&gt; [17] knitr_1.49        lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n#&gt; [21] dplyr_1.1.4       purrr_1.0.4       readr_2.1.5       tidyr_1.3.1      \n#&gt; [25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.0      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] mime_0.12           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [25] Matrix_1.7-2        R6_2.6.1            fastmap_1.2.0      \n#&gt;  [28] rbibutils_2.3       shiny_1.10.0        digest_0.6.37      \n#&gt;  [31] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n#&gt;  [34] rprojroot_2.0.4     Hmisc_5.2-2         labeling_0.4.3     \n#&gt;  [37] timechange_0.3.0    abind_1.4-8         compiler_4.4.2     \n#&gt;  [40] withr_3.0.2         glasso_1.11         htmlTable_2.4.3    \n#&gt;  [43] backports_1.5.0     carData_3.0-5       ggsignif_0.6.4     \n#&gt;  [46] corpcor_1.6.10      gtools_3.9.5        tools_4.4.2        \n#&gt;  [49] pbivnorm_0.6.0      foreign_0.8-88      zip_2.3.2          \n#&gt;  [52] httpuv_1.6.15       nnet_7.3-20         glue_1.8.0         \n#&gt;  [55] quadprog_1.5-8      nlme_3.1-167        promises_1.3.2     \n#&gt;  [58] lisrelToR_0.3       grid_4.4.2          checkmate_2.3.2    \n#&gt;  [61] cluster_2.1.8       reshape2_1.4.4      generics_0.1.3     \n#&gt;  [64] gtable_0.3.6        tzdb_0.4.0          data.table_1.17.0  \n#&gt;  [67] hms_1.1.3           car_3.1-3           sem_3.1-16         \n#&gt;  [70] pillar_1.10.1       rockchalk_1.8.157   later_1.4.1        \n#&gt;  [73] splines_4.4.2       lattice_0.22-6      survival_3.8-3     \n#&gt;  [76] kutils_1.73         tidyselect_1.2.1    miniUI_0.1.1.1     \n#&gt;  [79] pbapply_1.7-2       reformulas_0.4.0    stats4_4.4.2       \n#&gt;  [82] xfun_0.51           qgraph_1.9.8        arm_1.14-4         \n#&gt;  [85] stringi_1.8.4       yaml_2.3.10         pacman_0.5.1       \n#&gt;  [88] boot_1.3-31         evaluate_1.0.3      codetools_0.2-20   \n#&gt;  [91] mi_1.1              cli_3.6.4           RcppParallel_5.1.10\n#&gt;  [94] rpart_4.1.24        xtable_1.8-4        Rdpack_2.6.2       \n#&gt;  [97] munsell_0.5.1       Rcpp_1.0.14         coda_0.19-4.1      \n#&gt; [100] png_0.1-8           XML_3.99-0.18       parallel_4.4.2     \n#&gt; [103] jpeg_0.1-10         lme4_1.1-36         mvtnorm_1.3-3      \n#&gt; [106] openxlsx_4.2.8      rlang_1.1.5         multcomp_1.4-28    \n#&gt; [109] mnormt_2.1.1",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Analisi delle componenti principali</span>"
    ]
  },
  {
    "objectID": "chapters/pca/02_pca.html#bibliografia",
    "href": "chapters/pca/02_pca.html#bibliografia",
    "title": "\n26  Analisi delle componenti principali\n",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "PCA",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Analisi delle componenti principali</span>"
    ]
  },
  {
    "objectID": "chapters/fa/01_intro_fa.html",
    "href": "chapters/fa/01_intro_fa.html",
    "title": "27  Introduzione all’analisi fattoriale",
    "section": "",
    "text": "27.1 Analisi Fattoriale Esplorativa (EFA)\nL’EFA viene utilizzata quando il ricercatore non ha ipotesi a priori su come un gruppo di variabili si strutturi. Il suo scopo è identificare empiricamente il modello che meglio si adatta ai dati, bilanciando precisione e semplicità. Questa tecnica esplora la struttura sottostante ai dati, permettendo di individuare fattori latenti che spiegano la varianza comune tra le variabili osservate. È particolarmente utile nei primi stadi di sviluppo di test psicometrici, quando si desidera identificare le dimensioni latenti sottostanti a un nuovo insieme di item. Tuttavia, la scelta di parametri e metodi di estrazione influisce pesantemente sul risultato finale.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Introduzione all'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/01_intro_fa.html#analisi-fattoriale-confermativa-cfa",
    "href": "chapters/fa/01_intro_fa.html#analisi-fattoriale-confermativa-cfa",
    "title": "27  Introduzione all’analisi fattoriale",
    "section": "27.2 Analisi Fattoriale Confermativa (CFA)",
    "text": "27.2 Analisi Fattoriale Confermativa (CFA)\nL’CFA è un approccio utilizzato quando il ricercatore ha un modello teorico ben definito e desidera valutare quanto questo modello ipotizzato si adatti ai dati osservati. La CFA consente di confrontare modelli teorici alternativi e valutare quale meglio spiega i dati, tenendo conto di vari fattori come i carichi fattoriali, gli errori e le covarianze. In psicometria, viene comunemente utilizzata per verificare la validità strutturale di un test o questionario, valutando se i dati empirici supportano il modello teorico ipotizzato.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Introduzione all'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/01_intro_fa.html#struttura-e-componenti-dellanalisi-fattoriale",
    "href": "chapters/fa/01_intro_fa.html#struttura-e-componenti-dellanalisi-fattoriale",
    "title": "27  Introduzione all’analisi fattoriale",
    "section": "27.3 Struttura e Componenti dell’Analisi Fattoriale",
    "text": "27.3 Struttura e Componenti dell’Analisi Fattoriale\nIndipendentemente dal tipo di analisi, l’analisi fattoriale si basa sulla distinzione tra variabili osservate (o manifest) e variabili latenti (o fattori). Le variabili latenti rappresentano costrutti teorici non direttamente osservabili, mentre le variabili osservate sono i punteggi effettivi ottenuti da misure dirette. Un modello fattoriale può includere carichi fattoriali, errori, covarianze e percorsi di regressione.\nUn carico fattoriale rappresenta la forza della relazione tra una variabile osservata e il fattore latente, mentre il residuo o errore rappresenta la varianza non spiegata dal fattore latente. Le covarianze esprimono le relazioni tra le variabili o tra i fattori latenti. L’equazione generale di un indicatore osservato \\(X\\) in relazione a un fattore latente \\(F\\) può essere espressa come:\n\\[\nX = \\lambda \\cdot F + \\text{Intercept} + \\text{Errore}\n\\]\ndove:\n\n\\(X\\) è il valore osservato dell’indicatore;\n\\(\\lambda\\) è il carico fattoriale;\n\\(F\\) è il valore del fattore latente;\nIntercept è il valore atteso dell’indicatore quando il fattore latente è zero;\nErrore è la parte di varianza non spiegata dal fattore latente.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Introduzione all'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/01_intro_fa.html#modelli-fattoriali-gerarchici-e-bifattoriali",
    "href": "chapters/fa/01_intro_fa.html#modelli-fattoriali-gerarchici-e-bifattoriali",
    "title": "27  Introduzione all’analisi fattoriale",
    "section": "27.4 Modelli Fattoriali Gerarchici e Bifattoriali",
    "text": "27.4 Modelli Fattoriali Gerarchici e Bifattoriali\nEsistono varianti più avanzate dell’analisi fattoriale, come i modelli gerarchici e i modelli bifattoriali, che permettono di rappresentare strutture latenti più complesse. In particolare, i modelli bifattoriali sono utili quando si ritiene che un insieme di variabili possa essere spiegato sia da un fattore generale che da fattori specifici. Ad esempio, nel contesto della misurazione dell’intelligenza, un modello bifattoriale potrebbe includere un fattore generale (g) che spiega la varianza comune tra tutte le variabili, e fattori specifici che spiegano varianze più circoscritte a singoli domini cognitivi.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Introduzione all'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/01_intro_fa.html#sviluppo-storico-dellanalisi-fattoriale",
    "href": "chapters/fa/01_intro_fa.html#sviluppo-storico-dellanalisi-fattoriale",
    "title": "27  Introduzione all’analisi fattoriale",
    "section": "27.5 Sviluppo Storico dell’Analisi Fattoriale",
    "text": "27.5 Sviluppo Storico dell’Analisi Fattoriale\nL’analisi fattoriale è stata sviluppata all’inizio del XX secolo da Charles Spearman per studiare la struttura dell’intelligenza. Spearman introdusse il concetto di fattore generale (g), che rappresentava la dimensione comune che spiegava la covarianza tra diverse abilità cognitive. Successivamente, psicologi come Thurstone criticarono il modello unifattoriale di Spearman e proposero un modello multifattoriale, che permetteva di individuare più fattori specifici, ciascuno dei quali spiegava una dimensione distinta dell’intelligenza.\nNegli anni ’60 e ’70, l’analisi fattoriale subì una trasformazione con lo sviluppo dei modelli di equazioni strutturali (SEM), che combinavano l’analisi fattoriale con la path analysis per rappresentare relazioni più complesse tra variabili osservate e latenti. Questo sviluppo permise ai ricercatori di verificare ipotesi teoriche più articolate riguardanti la struttura di costrutti psicologici complessi.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Introduzione all'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/01_intro_fa.html#applicazioni-dellanalisi-fattoriale",
    "href": "chapters/fa/01_intro_fa.html#applicazioni-dellanalisi-fattoriale",
    "title": "27  Introduzione all’analisi fattoriale",
    "section": "27.6 Applicazioni dell’Analisi Fattoriale",
    "text": "27.6 Applicazioni dell’Analisi Fattoriale\nL’analisi fattoriale è ampiamente utilizzata nello sviluppo di strumenti psicometrici, come i test di intelligenza, le scale di personalità e i questionari di auto-valutazione. Viene utilizzata per valutare la validità di costrutto, ovvero la capacità di uno strumento di misurare effettivamente il costrutto teorico che si propone di valutare. Inoltre, l’analisi fattoriale può essere impiegata per esaminare la validità discriminante, ovvero la capacità di uno strumento di distinguere tra costrutti correlati ma distinti.\nInfine, l’analisi fattoriale è uno strumento fondamentale per individuare variabili latenti sottostanti e semplificare i dati complessi, consentendo ai ricercatori di ridurre grandi set di variabili osservate a un insieme più ristretto di fattori interpretabili. La sua applicazione, tuttavia, richiede attenzione nella scelta dei parametri e delle assunzioni, poiché le decisioni prese nel processo di analisi possono influenzare significativamente i risultati finali.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Introduzione all'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html",
    "href": "chapters/fa/02_analisi_fattoriale_1.html",
    "title": "28  Il modello unifattoriale",
    "section": "",
    "text": "28.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nNel contesto dell’analisi fattoriale, i costrutti teorici di interesse vengono rappresentati da variabili latenti, che riflettono le comunanze sottostanti tra un insieme di variabili manifeste (ovvero misurabili). Le variabili manifeste, come gli item di un questionario o gli indici derivati da un compito comportamentale, sono direttamente osservabili e vengono solitamente rappresentate graficamente come quadrati. Al contrario, le variabili latenti descrivono costrutti ipotetici non direttamente osservabili, come un fattore latente di intelligenza o memoria, e sono rappresentate come cerchi nei diagrammi.\nIl legame tra variabili latenti e manifeste si esprime tramite i carichi fattoriali, cioè percorsi che collegano una variabile latente a una variabile osservabile. L’intensità di un carico fattoriale indica la quota di varianza osservata spiegata dal fattore latente. In altre parole, il carico fattoriale riflette la capacità di una data variabile manifesta di rappresentare il costrutto latente.\nDal punto di vista matematico, l’analisi fattoriale descrive ogni misura osservabile \\(y\\) come la combinazione lineare del punteggio latente \\(\\xi\\) relativo al costrutto di interesse e di un elemento di errore non osservato \\(\\delta\\). In questo modello, il valore di \\(y\\) è interpretato come il prodotto del punteggio latente, ponderato da un coefficiente di carico \\(\\lambda\\), a cui si somma un termine di errore specifico \\(\\delta_y\\):\n\\[\ny = \\lambda \\xi + \\delta_y.\n\\]\nUn esempio concreto può aiutare a chiarire questo concetto: supponiamo di usare una bilancia non perfettamente affidabile per misurare il peso corporeo. Ogni lettura non rifletterà solo il peso reale della persona, ma includerà anche una componente di errore dovuta alla bilancia stessa, manifestando variazioni casuali tra una misurazione e l’altra.\nQuando si dispone di più misure osservabili \\(y\\) che rappresentano il medesimo costrutto latente \\(\\xi\\), diventa possibile stimare con maggiore accuratezza sia il punteggio reale latente \\(\\xi\\) sia la componente di errore di misura \\(\\delta\\). Questo permette di migliorare la precisione interpretativa dei dati e di ottenere una rappresentazione più affidabile del costrutto teorico di interesse.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#modello-monofattoriale",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#modello-monofattoriale",
    "title": "28  Il modello unifattoriale",
    "section": "\n28.2 Modello monofattoriale",
    "text": "28.2 Modello monofattoriale\nCon \\(p\\) variabili manifeste \\(y_i\\), il caso più semplice è quello di un solo fattore comune:\n\\[\n\\begin{equation}\ny_i = \\mu_i + \\lambda_{i} \\xi +  1 \\cdot \\delta_i \\qquad i=1, \\dots, p,\n\\end{equation}\n\\tag{28.1}\\]\ndove \\(\\xi\\) rappresenta il fattore comune a tutte le \\(y_i\\), \\(\\delta_i\\) sono i fattori specifici o unici di ogni variabile osservata e \\(\\lambda_i\\) sono le saturazioni (o pesi) fattoriali le quali stabiliscono il peso del fattore latente su ciascuna variabile osservata.\nIl modello di analisi fattoriale e il modello di regressione possono sembrare simili, ma presentano alcune differenze importanti. In primo luogo, sia il fattore comune \\(\\xi\\) sia i fattori specifici \\(\\delta_i\\) sono inosservabili, il che rende tutto ciò che si trova a destra dell’uguaglianza incognito. In secondo luogo, l’analisi di regressione e l’analisi fattoriale hanno obiettivi diversi. L’analisi di regressione mira a individuare le variabili esplicative, osservabili direttamente, che sono in grado di spiegare la maggior parte della varianza della variabile dipendente. Al contrario, il problema dell’analisi unifattoriale consiste nell’identificare la variabile esplicativa inosservabile che è in grado di spiegare la maggior parte della covarianza tra le variabili osservate.\nSolitamente, per comodità, si assume che la media delle variabili osservate \\(y_i\\) sia zero, ovvero \\(\\mu_i=0\\). Ciò equivale a considerare gli scarti delle variabili rispetto alle rispettive medie. Il modello unifattoriale assume che le variabili osservate siano il risultato della combinazione lineare di un fattore comune \\(\\xi\\) e dei fattori specifici \\(\\delta_i\\), ovvero:\n\\[\n\\begin{equation}\ny_i -\\mu_i = \\lambda_i \\xi + 1 \\cdot \\delta_i,\n\\end{equation}\n\\tag{28.2}\\]\ndove \\(\\lambda_i\\) è la saturazione o il peso della variabile \\(i\\)-esima sul fattore comune e \\(\\delta_i\\) rappresenta il fattore specifico della variabile \\(i\\)-esima. Si assume che il fattore comune abbia media zero e varianza unitaria, mentre i fattori specifici abbiano media zero, varianza \\(\\psi_{i}\\) e siano incorrelati tra loro e con il fattore comune. Nel modello unifattoriale, l’interdipendenza tra le variabili è completamente spiegata dal fattore comune.\nLe ipotesi precedenti consentono di ricavare la covarianza tra la variabile osservata \\(y_i\\) e il fattore comune, la varianza della variabile osservata \\(y_i\\) e la covarianza tra due variabili osservate \\(y_i\\) e \\(y_k\\). L’obiettivo della discussione in questo capitolo è appunto quello di analizzare tali grandezze statistiche.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#correlazione-parziale",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#correlazione-parziale",
    "title": "28  Il modello unifattoriale",
    "section": "\n28.3 Correlazione parziale",
    "text": "28.3 Correlazione parziale\nPrima di entrare nel dettaglio del modello statistico dell’analisi fattoriale, è importante chiarire il concetto di correlazione parziale. Si attribuisce spesso a Charles Spearman la nascita dell’analisi fattoriale. Nel 1904, Spearman pubblicò un articolo intitolato “General Intelligence, Objectively Determined and Measured” in cui propose la Teoria dei Due Fattori. In questo articolo, dimostrò come fosse possibile identificare un fattore inosservabile a partire da una matrice di correlazioni, utilizzando il metodo dell’annullamento della tetrade (tetrad differences). L’annullamento della tetrade è un’applicazione della teoria della correlazione parziale che mira a stabilire se, controllando un insieme di variabili inosservabili chiamate fattori \\(\\xi_j\\), le correlazioni tra le variabili osservabili \\(Y_i\\), al netto degli effetti lineari delle \\(\\xi_j\\), diventino statisticamente nulle.\nPossiamo considerare un esempio con tre variabili: \\(Y_1\\), \\(Y_2\\) e \\(F\\). La correlazione tra \\(Y_1\\) e \\(Y_2\\), \\(r_{1,2}\\), può essere influenzata dalla presenza di \\(F\\). Per calcolare la correlazione parziale tra \\(Y_1\\) e \\(Y_2\\) al netto dell’effetto lineare di \\(F\\), dobbiamo trovare le componenti di \\(Y_1\\) e \\(Y_2\\) che sono linearmente indipendenti da \\(F\\).\nPer fare ciò, dobbiamo trovare la componente di \\(Y_1\\) che è ortogonale a \\(F\\). Possiamo calcolare i residui \\(E_1\\) del modello:\n\\[\nY_1 = b_{01} + b_{11}F + E_1.\n\\tag{28.3}\\]\nLa componente di \\(Y_1\\) linearmente indipendente da \\(F\\) è quindi data dai residui \\(E_1\\). Possiamo eseguire un’operazione analoga per \\(Y_2\\) per trovare la sua componente ortogonale a \\(F\\). Calcolando la correlazione tra le due componenti così ottenute si ottiene la correlazione parziale tra \\(Y_1\\) e \\(Y_2\\) al netto dell’effetto lineare di \\(F\\).\nL’Equazione 28.4 consente di calcolare la correlazione parziale tra \\(Y_1\\) e \\(Y_2\\) al netto dell’effetto di \\(F\\) a partire dalle correlazioni semplici tra le tre variabili \\(Y_1\\), \\(Y_2\\) e \\(F\\).\n\\[\n\\begin{equation}\nr_{1,2 \\mid F} = \\frac{r_{12} - r_{1F}r_{2F}}{\\sqrt{(1-r_{1F}^2)(1-r_{2F}^2)}}.\n\\end{equation}\n\\tag{28.4}\\]\nIn particolare, la correlazione parziale \\(r_{1,2 \\mid F}\\) è data dalla differenza tra la correlazione \\(r_{12}\\) tra \\(Y_1\\) e \\(Y_2\\) e il prodotto tra le correlazioni \\(r_{1F}\\) e \\(r_{2F}\\) tra ciascuna delle due variabili e \\(F\\), il tutto diviso per la radice quadrata del prodotto delle differenze tra 1 e i quadrati delle correlazioni tra \\(Y_1\\) e \\(F\\) e tra \\(Y_2\\) e \\(F\\). In altre parole, la formula tiene conto dell’effetto di \\(F\\) sulle correlazioni tra \\(Y_1\\) e \\(Y_2\\) per ottenere una stima della relazione diretta tra le due variabili, eliminando l’effetto del fattore comune.\nConsideriamo un esempio numerico. Sia \\(f\\) una variabile su cui misuriamo \\(n\\) valori\n\nset.seed(123)\nn &lt;- 1000\nf &lt;- rnorm(n, 24, 12)\n\nSiano \\(y_1\\) e \\(y_2\\) funzioni lineari di \\(f\\), a cui viene aggiunta una componente d’errore gaussiano:\n\ny1 &lt;- 10 + 7 * f + rnorm(n, 0, 50)\ny2 &lt;- 3  + 2 * f + rnorm(n, 0, 50)\n\nLa correlazione tra \\(y_1\\) e \\(y_2\\) (\\(r_{12}= 0.355\\)) deriva dal fatto che \\(\\hat{y}_1\\) e \\(\\hat{y}_2\\) sono entrambe funzioni lineari di \\(f\\):\n\nY &lt;- cbind(y1, y2, f)\ncor(Y) |&gt;\n    round(3)\n#&gt;       y1    y2     f\n#&gt; y1 1.000 0.380 0.867\n#&gt; y2 0.380 1.000 0.423\n#&gt; f  0.867 0.423 1.000\n\nEseguiamo le regressioni di \\(y_1\\) su \\(f\\) e di \\(y_2\\) su \\(F\\):\n\nfm1 &lt;- lm(y1 ~ f)\nfm2 &lt;- lm(y2 ~ f)\n\nNella regressione, ciascuna osservazione \\(y_{i1}\\) viene scomposta in due componenti linearmente indipendenti, i valori adattati \\(\\hat{y}_{i}\\) e i residui, \\(e_{i}\\): \\(y_i = \\hat{y}_i + e_1\\). Nel caso di \\(y_1\\) abbiamo\n\ncbind(y1, y1.hat=fm1$fit, e=fm1$res, fm1$fit+fm1$res) |&gt;\n    head() |&gt;\n    round(3)\n#&gt;      y1 y1.hat       e      \n#&gt; 1  81.1    131  -49.38  81.1\n#&gt; 2 106.7    160  -53.04 106.7\n#&gt; 3 308.0    318   -9.81 308.0\n#&gt; 4 177.3    186   -8.97 177.3\n#&gt; 5  61.4    191 -130.09  61.4\n#&gt; 6 374.1    332   42.43 374.1\n\nLo stesso può dirsi di \\(y_2\\). La correlazione parziale \\(r_{12 \\mid f}\\) tra \\(y_1\\) e \\(y_2\\) dato \\(f\\) è uguale alla correlazione di Pearson tra i residui \\(e_1\\) e \\(e_2\\) calcolati mediante i due modelli di regressione descritti sopra:\n\ncor(fm1$res, fm2$res)\n#&gt; [1] 0.0283\n\n\nLa correlazione parziale tra \\(y_1\\) e \\(y_2\\) al netto di \\(f\\) è .02829. Per i dati esaminati sopra, dunque, la correlazione parziale tra le variabili \\(y_1\\) e \\(y_2\\) diventa uguale a zero se la variabile \\(f\\) viene controllata (ovvero, se escludiamo da \\(y_1\\) e da \\(y_2\\) l’effetto lineare di \\(f\\)).\nIl fatto che la correlazione parziale sia zero significa che la correlazione che abbiamo osservato tra \\(y_1\\) e \\(y_2\\) (\\(r = 0.355\\)) non dipendeva dall’effetto che una variabile \\(y\\) esercitava sull’altra, ma bensì dal fatto che c’era una terza variabile, \\(f\\), che influenzava sia \\(y_1\\) sia \\(y_2\\). In altre parole, le variabili \\(y_1\\) e \\(y_2\\) sono condizionalmente indipendenti dato \\(f\\). Ciò significa, come abbiamo visto sopra, che la componente di \\(y_1\\) linearmente indipendente da \\(f\\) è incorrelata con la componente di \\(y_2\\) linearmente indipendente da \\(f\\).\nLa correlazione che abbiamo calcolato tra i residui di due modelli di regressione è identica alla correlazione che viene calcolata applicando l’Equazione 28.4:\n\nR &lt;- cor(Y)\n\n(R[1, 2] - R[1, 3] * R[2, 3]) / \n  sqrt((1 - R[1, 3]^2) * (1- R[2, 3]^2)) |&gt;\n  round(3)\n#&gt; [1] 0.0283",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#principio-base-dellanalisi-fattoriale",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#principio-base-dellanalisi-fattoriale",
    "title": "28  Il modello unifattoriale",
    "section": "\n28.4 Principio base dell’analisi fattoriale",
    "text": "28.4 Principio base dell’analisi fattoriale\nAttualmente, l’inferenza statistica nell’analisi fattoriale spesso si svolge mediante il calcolo di stime della massima verosimiglianza ottenute mediante procedure iterative. All’inizio dell’analisi fattoriale, tuttavia, la procedura di estrazione dei fattori faceva leva sulle relazioni invarianti che il modello fattoriale impone agli elementi della matrice di covarianza delle variabili osservate. Il più conosciuto tra tali invarianti è la tetrade che si presenta nei modelli ad un fattore.\nLa tetrade è una combinazione di quattro correlazioni. Se l’associazione osservata tra le variabili dipende effettivamente dal fatto che le variabili in questione sono state causalmente generate da un fattore comune inosservabile, allora è possibile generare una combinazione delle correlazioni tra le variabili che porta all’annullamento della tetrade. In altre parole, l’analisi fattoriale si chiede se esiste un insieme esiguo di \\(m&lt;p\\) variabili inosservabili che rendono significativamente nulle tutte le correlazioni parziali tra le \\(p\\) variabili osservate al netto dei fattori comuni. Se il metodo della correlazione parziale consente di identificare \\(m\\) variabili latenti, allora lo psicologo conclude che tali fattori corrispondono agli \\(m\\) costrutti che intende misurare.\nPer chiarire il metodo dell’annullamento della tetrade consideriamo la matrice di correlazioni riportata nella Tabella successiva. Nella tabella, la correlazione parziale tra ciascuna coppia di variabili \\(y_i\\), \\(y_j\\) (con \\(i \\neq j\\)) dato \\(\\xi\\) è sempre uguale a zero. Ad esempio, la correlazione parziale tra \\(y_3\\) e \\(y_5\\) dato \\(\\xi\\) è:\n\\[\n\\begin{align}\n  r_{35 \\mid \\xi} &= \\frac{r_{35} - r_{3\\xi}r_{5\\xi}}\n  {\\sqrt{(1-r_{3\\xi}^2)(1-r_{5\\xi}^2)}} \\notag \\\\[12pt]\n  &= \\frac{0.35 - 0.7 \\times 0.5}\n  {\\sqrt{(1-0.7^2)(1-0.5^2)}} = 0. \\notag\n\\end{align}\n\\]\nLo stesso risultato si trova per qualunque altra coppia di variabili \\(y_i\\) e \\(y_j\\), ovvero \\(r_{ij \\mid \\xi} = 0\\).\n\n\n\n\\(\\xi\\)\n\\(y_1\\)\n\\(y_2\\)\n\\(y_3\\)\n\\(y_4\\)\n\\(y_5\\)\n\n\n\n\\(\\xi\\)\n1.00\n\n\n\n\n\n\n\n\\(y_1\\)\n0.90\n1.00\n\n\n\n\n\n\n\\(y_2\\)\n0.80\n0.72\n1.00\n\n\n\n\n\n\\(y_3\\)\n0.70\n0.63\n0.56\n1.00\n\n\n\n\n\\(y_4\\)\n0.60\n0.54\n0.48\n0.42\n1.00\n\n\n\n\\(y_5\\)\n0.50\n0.45\n0.40\n0.35\n0.30\n1.00\n\n\n\nPossiamo dunque dire che, per la matrice di correlazioni della Tabella, esiste un’unica variabile \\(\\xi\\) la quale, quando viene controllata, spiega tutte le\n\\[p(p-1)/2 = 5(5-1)/2=10\\]\ncorrelazioni tra le variabili \\(y\\). Questo risultato non è sorprendente, in quanto la matrice di correlazioni della Tabella è stata costruita in modo tale da possedere tale proprietà.\nMa supponiamo di essere in una situazione diversa, ovvero di avere osservato soltanto le variabili \\(y_i\\) e di non conoscere \\(\\xi\\). In tali circostanze ci possiamo porre la seguente domanda: Esiste una variabile inosservabile \\(\\xi\\) la quale, se venisse controllata, renderebbe uguali a zero tutte le correlazioni parziali tra le variabili \\(y\\)? Se una tale variabile inosservabile esiste, ed è in grado di spiegare tutte le correlazioni tra le variabili osservate \\(y\\), allora essa viene chiamata fattore. Arriviamo dunque alla seguente definizione:\nUn fattore è una variabile inosservabile in grado di rendere significativamente nulle tutte le correlazioni parziali tra le variabili manifeste.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#vincoli-sulle-correlazioni",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#vincoli-sulle-correlazioni",
    "title": "28  Il modello unifattoriale",
    "section": "\n28.5 Vincoli sulle correlazioni",
    "text": "28.5 Vincoli sulle correlazioni\nCome si può stabilire se esiste una variabile inosservabile in grado di rendere nulle tutte le correlazioni parziali tra le variabili osservate? Riscriviamo l’Equazione 28.4 per specificare la correlazione parziale tra le variabili \\(y_i\\) e \\(y_j\\) dato \\(\\xi\\):\n\\[\n\\begin{align}\n  r_{ij \\mid \\xi} &= \\frac{r_{ij} - r_{i\\xi}r_{j\\xi}}\n  {\\sqrt{(1-r_{i\\xi}^2)(1-r_{j\\xi}^2)}}\n\\end{align}\n\\]\nAffinché \\(r_{ij \\mid \\xi}\\) sia uguale a zero è necessario che\n\\[\nr_{ij} - r_{i\\xi}r_{j\\xi}=0\n\\]\novvero\n\\[\n\\begin{equation}\nr_{ij} = r_{i\\xi}r_{j\\xi}.\n\\end{equation}\n\\]\nIn altri termini, se esiste un fattore non osservato \\(\\xi\\) in grado di rendere uguali a zero tutte le correlazioni parziali \\(r_{ih \\mid \\xi}\\), allora la correlazione tra ciascuna coppia di variabili \\(y\\) deve essere uguale al prodotto delle correlazioni tra ciascuna \\(y\\) e il fattore latente \\(\\xi\\). Questo è il principio base dell’analisi fattoriale.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#teoria-dei-due-fattori",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#teoria-dei-due-fattori",
    "title": "28  Il modello unifattoriale",
    "section": "\n28.6 Teoria dei Due Fattori",
    "text": "28.6 Teoria dei Due Fattori\nPer fare un esempio concreto relativo al metodo dell’annullamento della tetrade, esaminiamo la matrice di correlazioni originariamente analizzata da Spearman. Spearman (1904) raccolse alcune misure di capacità intellettuale su un piccolo numero di studenti di una scuola superiore. Nello specifico, esaminò i voti di tali studenti nelle seguenti materie: studio dei classici (\\(c\\)), letteratura inglese (\\(e\\)) e abilità matematiche (\\(m\\)). Considerò anche la prestazione in un compito di discriminazione dell’altezza di suoni (“pitch discrimination”) (\\(p\\)), ovvero un’abilità diversa da quelle richieste nei test scolastici.\nSecondo la Teoria dei Due Fattori, le prestazioni relative ad un determinato compito intellettuale possiedono una componente comune (detta fattore ‘g’) con le prestazioni in un qualunque altro compito intellettuale e una componente specifica a quel determinato compito. Il modello dell’intelligenza di Spearman prevede dunque due fattori, uno generale e uno specifico (detto fattore ‘s’). Il fattore ‘g’ costituisce la componente invariante dell’abilità intellettiva, mente il fattore ‘s’ è una componente che varia da condizione a condizione.\nCome è possibile stabilire se esiste una variabile latente in grado di spiegare le correlazioni tra le variabili osservate da Spearman? Lo strumento proposto da Spearman per rispondere a questa domanda è l’annullamento della tetrade. L’annullamento della tetrade utilizza i vincoli sulle correlazioni che derivano dalla definizione di correlazione parziale. In precedenza abbiamo visto che la correlazione parziale tra le variabili \\(y\\) indicizzate da \\(i\\) e \\(j\\), al netto dell’effetto di \\(\\xi\\), è nulla se\n\\[\nr_{ij} = r_{i\\xi}r_{j\\xi}.\n\\]\nNel caso dei dati di Spearman, dunque, le correlazioni parziali sono nulle se la correlazione tra ‘’studi classici’’ e ‘’letteratura inglese’’ è uguale al prodotto della correlazione tra ‘’studi classici’’ e il fattore \\(\\xi\\) e della correlazione tra ‘’letteratura inglese’’ e il fattore \\(\\xi\\). Inoltre, la correlazione tra ‘’studi classici’’ e ‘’abilità matematica’’ deve essere uguale al prodotto della correlazione tra ‘’studi classici’’ e il fattore \\(\\xi\\) e della correlazione tra ‘’abilità matematica’’ e il fattore \\(\\xi\\); e così via.\nLe correlazioni tra le variabili manifeste e il fattore latente sono dette e vengono denotate con la lettera \\(\\lambda\\). Se il modello di Spearman è corretto, avremo che\n\\[r_{ec}=\\lambda_e \\times \\lambda_{c},\\]\ndove \\(r_{ec}\\) è la correlazione tra ‘’letteratura inglese’’ (e) e ‘’studi classici’’ (c), \\(\\lambda_e\\) è la correlazione tra ‘’letteratura inglese’’ e \\(\\xi\\), e \\(\\lambda_{c}\\) è la correlazione tra ‘’studi classici’’ e \\(\\xi\\).\nAllo stesso modo, la correlazione tra ‘’studi classici’’ e ‘’matematica’’ (m) dovrà essere uguale a\n\\[\\lambda_c \\times \\lambda_m,\\]\neccetera.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#annullamento-della-tetrade",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#annullamento-della-tetrade",
    "title": "28  Il modello unifattoriale",
    "section": "\n28.7 Annullamento della tetrade",
    "text": "28.7 Annullamento della tetrade\nDate le correlazioni tra tre coppie di variabili manifeste, il metodo dell’annullamento della tetrade\n\nin una matrice di correlazione, si selezionino quattro coefficienti nelle posizioni che marcano gli angoli di un rettangolo. La differenza tra i prodotti dei coefficienti che giacciono sulle due diagonali di tale rettangolo costituisce la differenza delle tetradi e deve essere uguale a zero.\n\nrende possibile stimare i valori delle saturazioni fattoriali \\(\\lambda\\). Ad esempio, per le variabili \\(c\\), \\(m\\) ed \\(e\\), possiamo scrivere le seguenti tre equazioni in tre incognite:\n\\[\n\\begin{align}\n  r_{cm} &= \\lambda_c \\times \\lambda_m, \\notag \\\\\n  r_{em} &= \\lambda_e \\times \\lambda_m,  \\\\\n  r_{ce} &= \\lambda_c \\times \\lambda_e. \\notag\n\\end{align}\n\\]\nRisolvendo il precedente sistema di equazioni lineari, il coefficiente di saturazione \\(\\lambda_m\\) della variabile \\(y_m\\) nel fattore comune \\(\\xi\\), ad esempio, pu{`o} essere calcolato a partire dalle correlazioni tra le variabili manifeste \\(c\\), \\(m\\), ed \\(e\\) nel modo seguente\\footnote{ La terza delle equazioni del sistema lineare può essere riscritta come \\(\\lambda_c = \\frac{r_{ce}}{\\lambda_e}\\).\nUtilizzando tale risultato, la prima equazione diventa \\(r_{cm} = \\frac{r_{ce}}{\\lambda_e}\\lambda_m\\). Dalla seconda equazione otteniamo \\(\\lambda_e = \\frac{r_{em}}{\\lambda_m}\\). Sostituendo questo risultato nell’equazione precedente otteniamo \\(r_{cm} = \\frac{r_{ce}}{r_{em}}\\lambda_m^2\\), quindi \\(\\lambda_m^2 = \\frac{r_{cm} r_{em} }{r_{ce}}\\).\nVerifichiamo: \\(\\frac{r_{cm} r_{em}}{r_{ce}} = \\frac{\\lambda_c \\lambda_m \\lambda_e \\lambda_m}{\\lambda_c \\lambda_e} = \\lambda_m^2\\).\n\\[\n\\begin{align}\n  \\lambda_m &= \\sqrt{\n    \\frac{r_{cm} r_{em}}{r_{ce}}\n    }.\n\\end{align}\n\\tag{28.5}\\]\nLo stesso vale per le altre due saturazioni \\(\\lambda_c\\) e \\(\\lambda_e\\).\nNel suo articolo del 1904, Spearman osservò le seguenti correlazioni tra le variabili \\(Y_c\\), \\(Y_e\\), \\(Y_m\\) e \\(Y_p\\):\n\\[\n\\begin{array}{ccccc}\n  \\hline\n    & Y_C & Y_E & Y_M & Y_P \\\\\n  \\hline\n  Y_C & 1.00 & 0.78 & 0.70 & 0.66 \\\\\n  Y_E &   & 1.00 & 0.64 & 0.54 \\\\\n  Y_M &   &   & 1.00 & 0.45 \\\\\n  Y_P &   &   &   & 1.00 \\\\\n  \\hline\n\\end{array}\n\\]\nUtilizzando l’Equazione 28.5, mediante le correlazioni \\(r_{cm}\\), \\(r_{em}\\), e \\(r_{ce}\\) fornite dalla tabella precedente, la saturazione \\(\\lambda_m\\) diventa uguale a:\n\\[\n\\begin{align}\n  \\hat{\\lambda}_m &= \\sqrt{ \\frac{r_{cm} r_{em}}{r_{ce}} } = \\sqrt{\n    \\frac{0.70 \\times 0.64}{0.78} } = 0.76. \\notag\n\\end{align}\n\\]\nÈ importante notare che il metodo dell’annullamento della tetrade produce risultati falsificabili. Infatti, ci sono modi diversi per calcolare la stessa saturazione fattoriale. Se il modello fattoriale è corretto si deve ottenere lo stesso risultato in tutti i casi.\nNel caso presente, la saturazione fattoriale \\(\\lambda_m\\) può essere calcolata in altri due modi:\n\\[\n\\begin{align}\n  \\hat{\\lambda}_m &= \\sqrt{ \\frac{r_{cm} r_{mp}}{r_{cp}} } = \\sqrt{ \\frac{0.78 \\times 0.45}{0.66} } = 0.69, \\notag \\\\\n  \\hat{\\lambda}_m &= \\sqrt{ \\frac{r_{em} r_{mp}}{r_{ep}} } = \\sqrt{\n    \\frac{0.64 \\times 0.45}{0.54} } = 0.73. \\notag\n\\end{align}\n\\]\nI tre valori che sono stati ottenuti sono molto simili. Qual è allora la stima migliore di \\(\\lambda_m\\)?",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#metodo-del-centroide",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#metodo-del-centroide",
    "title": "28  Il modello unifattoriale",
    "section": "\n28.8 Metodo del centroide",
    "text": "28.8 Metodo del centroide\nLa soluzione più semplice è quella di fare la media di questi tre valori (\\(\\bar{\\lambda}_m = 0.73\\)). Un metodo migliore (meno vulnerabile ai valori anomali) è dato dal rapporto tra la somma dei numeratori e dei denominatori:\n\\[\n\\begin{align}\n  \\hat{\\lambda}_m &= \\sqrt{ \\frac{0.70 \\times 0.64 + 0.78 \\times 0.45 + 0.64\n      \\times 0.45}{0.78+0.66+0.54} } = 0.73 \\notag\n\\end{align}\n\\]\nIn questo caso, i due metodi danno lo stesso risultato. Le altre tre saturazioni fattoriali trovate mediante il metodo del centroide sono:\n\\[\\hat{\\lambda}_c = 0.97, \\quad \\hat{\\lambda}_e = 0.84, \\quad \\hat{\\lambda}_p = 0.65.\\]\nIn conclusione,\n\\[\n\\boldsymbol{\\hat{\\Lambda}}'=\n(\\hat{\\lambda}_c, \\hat{\\lambda}_e, \\hat{\\lambda}_m, \\hat{\\lambda}_p) = (0.97, 0.84, 0.73, 0.65).\n\\]\nQuesto risultato è la soluzione proposta da Spearman nel suo articolo del 1904 per risolvere il problema di determinare le saturazioni fattoriali di un modello con un fattore comune latente.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#introduzione-a-lavaan",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#introduzione-a-lavaan",
    "title": "28  Il modello unifattoriale",
    "section": "\n28.9 Introduzione a lavaan\n",
    "text": "28.9 Introduzione a lavaan\n\nAttualmente, l’analisi fattoriale viene svolta mediante software. Il pacchetto R più ampiamente utilizzato per condurre l’analisi fattoriale è lavaan.\n\n28.9.1 Sintassi del modello\nAl cuore del pacchetto lavaan si trova la “sintassi del modello”. La sintassi del modello è una descrizione del modello da stimare. In questa sezione, spieghiamo brevemente gli elementi della sintassi del modello lavaan.\nNell’ambiente R, una formula di regressione ha la seguente forma:\ny ~ x1 + x2 + x3 + x4\nIn questa formula, la tilde (“~”) è l’operatore di regressione. Sul lato sinistro dell’operatore, abbiamo la variabile dipendente (y), e sul lato destro abbiamo le variabili indipendenti, separate dall’operatore “+” . In lavaan, un modello tipico è semplicemente un insieme (o sistema) di formule di regressione, in cui alcune variabili (che iniziano con una ‘f’ qui sotto) possono essere latenti. Ad esempio:\ny ~ f1 + f2 + x1 + x2\nf1 ~ f2 + f3\nf2 ~ f3 + x1 + x2\nSe abbiamo variabili latenti in una qualsiasi delle formule di regressione, dobbiamo “definirle” elencando i loro indicatori (manifesti o latenti). Lo facciamo utilizzando l’operatore speciale “=~”, che può essere letto come “è misurato da”. Ad esempio, per definire le tre variabili latenti f1, f2 e f3, possiamo usare la sintassi seguente:\nf1 =~ y1 + y2 + y3\nf2 =~ y4 + y5 + y6\nf3 =~ y7 + y8 + y9 + y10\nInoltre, le varianze e le covarianze sono specificate utilizzando un operatore “doppia tilde”, ad esempio:\ny1 ~~ y1 # varianza\ny1 ~~ y2 # covarianza\nf1 ~~ f2 # covarianza\nE infine, le intercette per le variabili osservate e latenti sono semplici formule di regressione con solo una intercetta (esplicitamente indicato dal numero “1”) come unico predittore:\ny1 ~ 1\nf1 ~ 1\nUtilizzando questi quattro tipi di formule, è possibile descrivere una vasta gamma di modelli di variabili latenti. L’attuale insieme di tipi di formula è riassunto nella tabella sottostante.\n\n\ntipo di formula\noperatore\nmnemonic\n\n\n\ndefinizione variabile latente\n=~\nè misurato da\n\n\nregressione\n~\nviene regredito su\n\n\n(co)varianza (residuale)\n~~\nè correlato con\n\n\nintercetta\n~ 1\nintercetta\n\n\n\nUna sintassi completa del modello lavaan è semplicemente una combinazione di questi tipi di formule, racchiusi tra virgolette singole. Ad esempio:\nmy_model &lt;- ' \n  # regressions\n  y1 + y2 ~ f1 + f2 + x1 + x2\n  f1 ~ f2 + f3\n  f2 ~ f3 + x1 + x2\n\n  # latent variable definitions \n  f1 =~ y1 + y2 + y3 \n  f2 =~ y4 + y5 + y6 \n  f3 =~ y7 + y8 + y9\n  \n  # variances and covariances \n  y1 ~~ y1 \n  y1 ~~ y2 \n  f1 ~~ f2\n\n  # intercepts \n  y1 ~ 1 \n  f1 ~ 1\n'\nPer adattare il modello ai dati usiamo la seguente sintassi.\nfit &lt;- cfa(model = my_model, data = my_data)\n\n28.9.2 Un esempio concreto\nAnalizziamo nuovamente i dati di Spearman che abbiamo esaminato in precedenza usando lavaan. La matrice completa dei dati di Spearman è messa a disposizione da Kan et al. (2019).\nSpecifichiamo il nome delle variabili manifeste\n\nvarnames &lt;- c(\n  \"Classics\", \"French\", \"English\", \"Math\", \"Pitch\", \"Music\"\n)\n\ne il loro numero\n\nny &lt;- length(varnames)\n\nCreiamo la matrice di correlazione:\n\nspearman_cor_mat &lt;- matrix(\n  c(\n    1.00,  .83,  .78,  .70,  .66,  .63,\n     .83, 1.00,  .67,  .67,  .65,  .57,\n     .78,  .67, 1.00,  .64,  .54,  .51,\n     .70,  .67,  .64, 1.00,  .45,  .51,\n     .66,  .65,  .54,  .45, 1.00,  .40,\n     .63,  .57,  .51,  .51,  .40, 1.00\n  ),\n  ny, ny,\n  byrow = TRUE,\n  dimnames = list(varnames, varnames)\n)\nspearman_cor_mat\n#&gt;          Classics French English Math Pitch Music\n#&gt; Classics     1.00   0.83    0.78 0.70  0.66  0.63\n#&gt; French       0.83   1.00    0.67 0.67  0.65  0.57\n#&gt; English      0.78   0.67    1.00 0.64  0.54  0.51\n#&gt; Math         0.70   0.67    0.64 1.00  0.45  0.51\n#&gt; Pitch        0.66   0.65    0.54 0.45  1.00  0.40\n#&gt; Music        0.63   0.57    0.51 0.51  0.40  1.00\n\nSpecifichiamo l’ampiezza campionaria:\n\nn &lt;- 33\n\nDefiniamo il modello unifattoriale in lavaan. L’operatore =~ si può leggere dicendo che la variabile latente a sinistra dell’operatore viene identificata dalle variabili manifeste elencate a destra dell’operatore e separate dal segno +. Per il caso presente, il modello dei due fattori di Spearman può essere specificato come segue.\n\nspearman_mod &lt;- \"\n  g =~ Classics + French + English + Math + Pitch + Music\n\"\n\nAdattiamo il modello ai dati con la funzione cfa():\n\nfit1 &lt;- lavaan::cfa(\n  spearman_mod,\n  sample.cov = spearman_cor_mat,\n  sample.nobs = n,\n  std.lv = TRUE\n)\n\nLa funzione cfa() è una funzione dedicata per adattare modelli di analisi fattoriale confermativa. Il primo argomento è il modello specificato dall’utente. Il secondo argomento è il dataset che contiene le variabili osservate. L’argomento std.lv = TRUE specifica che imponiamo una varianza pari a 1 a tutte le variabili latenti comuni (nel caso presente, solo una). Ciò consente di stimare le saturazioni fattoriali.\nUna volta adattato il modello, la funzione summary() ci consente di esaminare la soluzione ottenuta:\n\nout = summary(\n  fit1, \n  fit.measures = TRUE, \n  standardized = TRUE\n)\nout\n#&gt; lavaan 0.6-19 ended normally after 23 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        12\n#&gt; \n#&gt;   Number of observations                            33\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                 2.913\n#&gt;   Degrees of freedom                                 9\n#&gt;   P-value (Chi-square)                           0.968\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               133.625\n#&gt;   Degrees of freedom                                15\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    1.000\n#&gt;   Tucker-Lewis Index (TLI)                       1.086\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)               -212.547\n#&gt;   Loglikelihood unrestricted model (H1)       -211.091\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                 449.094\n#&gt;   Bayesian (BIC)                               467.052\n#&gt;   Sample-size adjusted Bayesian (SABIC)        429.622\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.000\n#&gt;   90 Percent confidence interval - lower         0.000\n#&gt;   90 Percent confidence interval - upper         0.000\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.976\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.016\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.025\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   g =~                                                                  \n#&gt;     Classics          0.942    0.129    7.314    0.000    0.942    0.956\n#&gt;     French            0.857    0.137    6.239    0.000    0.857    0.871\n#&gt;     English           0.795    0.143    5.545    0.000    0.795    0.807\n#&gt;     Math              0.732    0.149    4.923    0.000    0.732    0.743\n#&gt;     Pitch             0.678    0.153    4.438    0.000    0.678    0.689\n#&gt;     Music             0.643    0.155    4.142    0.000    0.643    0.653\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .Classics          0.083    0.051    1.629    0.103    0.083    0.086\n#&gt;    .French            0.234    0.072    3.244    0.001    0.234    0.242\n#&gt;    .English           0.338    0.094    3.610    0.000    0.338    0.349\n#&gt;    .Math              0.434    0.115    3.773    0.000    0.434    0.447\n#&gt;    .Pitch             0.510    0.132    3.855    0.000    0.510    0.526\n#&gt;    .Music             0.556    0.143    3.893    0.000    0.556    0.573\n#&gt;     g                 1.000                               1.000    1.000\n\nL’output di lavaan si divide in tre sezioni principali:\n\n\nIntestazione: Le prime nove righe dell’output costituiscono l’intestazione, che fornisce informazioni chiave sul modello e sull’analisi, tra cui:\n\n\nVersione di lavaan: specifica la versione del pacchetto utilizzata.\n\nEsito dell’ottimizzazione: indica se l’algoritmo di ottimizzazione è terminato correttamente e il numero di iterazioni necessarie.\n\nStimatore: mostra il metodo di stima utilizzato, come ML (massima verosimiglianza).\n\nOttimizzatore: specifica l’algoritmo di ottimizzazione (es., NLMINB) utilizzato per trovare i parametri migliori per lo stimatore selezionato.\n\nNumero di parametri del modello: fornisce il totale dei parametri stimati nel modello (es., 12).\n\nNumero di osservazioni: indica il numero di dati effettivamente utilizzati nell’analisi (es., 33).\n\n“Model Test User Model”: contiene la statistica di test, i gradi di libertà e il valore p per il modello specificato dall’utente.\n\n\nMisure di adattamento: Questa sezione, visibile solo con fit.measures = TRUE, presenta una serie di indicatori di adattamento del modello, iniziando dalla riga “Model Test Baseline Model” e terminando con il valore per l’SRMR. Questi indicatori forniscono informazioni sulla bontà del modello rispetto ai dati.\n\nStime dei parametri: Questa sezione contiene le stime dei parametri e inizia con dettagli tecnici sul metodo utilizzato per calcolare gli errori standard. Di seguito, sono elencati i parametri liberi e fissi del modello, in genere con ordine che parte dalle variabili latenti, seguito dalle covarianze e dalle varianze residue. Le colonne includono:\n\n\nEstimate: indica il valore stimato (non standardizzato) per ciascun parametro, rappresentando il peso del collegamento tra il costrutto latente (es., g) e le variabili osservate.\n\nStd.err: l’errore standard per ogni stima, utile per valutare l’accuratezza della stima.\n\nZ-value: la statistica di Wald, calcolata dividendo la stima per il suo errore standard.\n\nP(&gt;|z|): il valore p, utilizzato per testare l’ipotesi nulla che la stima sia zero nella popolazione.\n\n\n\nUlteriori dettagli sulle colonne:\n\n\nEstimate: Fornisce lo stimatore di massima verosimiglianza per i pesi dei percorsi, rappresentando l’effetto diretto di ogni costrutto latente sulle variabili osservate.\n\nStd.lv: Questi valori sono standardizzati solo rispetto alle variabili latenti, permettendo un confronto all’interno del modello indipendentemente dalle unità di misura originali.\n\nStd.all: Fornisce le stime completamente standardizzate, considerando sia le variabili latenti sia quelle osservate, facilitando un confronto dei coefficienti in termini di deviazioni standard.\n\nNella sezione delle Varianze, si osserva un punto prima dei nomi delle variabili osservate. Questo formato indica che sono variabili endogene, cioè predette dalle variabili latenti, e che il valore riportato rappresenta la varianza residua non spiegata dai predittori. Invece, le variabili latenti, che non hanno un punto prima del loro nome, sono considerate esogene; i valori riportati sono le loro varianze totali stimate.\nÈ possibile semplificare l’output dalla funzione summary() in maniera tale da stampare solo la tabella completa delle stime dei parametri e degli errori standard. Qui usiamo coef(fit1).\n\ncoef(fit1) \n#&gt;        g=~Classics          g=~French         g=~English            g=~Math \n#&gt;              0.942              0.857              0.795              0.732 \n#&gt;           g=~Pitch           g=~Music Classics~~Classics     French~~French \n#&gt;              0.678              0.643              0.083              0.234 \n#&gt;   English~~English         Math~~Math       Pitch~~Pitch       Music~~Music \n#&gt;              0.338              0.434              0.510              0.556\n\nUsando parameterEstimates, l’output diventa il seguente.\n\nparameterEstimates(fit1, standardized = TRUE)\n#&gt;         lhs op      rhs   est    se    z pvalue ci.lower ci.upper std.lv\n#&gt; 1         g =~ Classics 0.942 0.129 7.31  0.000    0.689    1.194  0.942\n#&gt; 2         g =~   French 0.857 0.137 6.24  0.000    0.588    1.127  0.857\n#&gt; 3         g =~  English 0.795 0.143 5.54  0.000    0.514    1.076  0.795\n#&gt; 4         g =~     Math 0.732 0.149 4.92  0.000    0.441    1.024  0.732\n#&gt; 5         g =~    Pitch 0.678 0.153 4.44  0.000    0.379    0.978  0.678\n#&gt; 6         g =~    Music 0.643 0.155 4.14  0.000    0.339    0.948  0.643\n#&gt; 7  Classics ~~ Classics 0.083 0.051 1.63  0.103   -0.017    0.183  0.083\n#&gt; 8    French ~~   French 0.234 0.072 3.24  0.001    0.093    0.376  0.234\n#&gt; 9   English ~~  English 0.338 0.094 3.61  0.000    0.154    0.522  0.338\n#&gt; 10     Math ~~     Math 0.434 0.115 3.77  0.000    0.208    0.659  0.434\n#&gt; 11    Pitch ~~    Pitch 0.510 0.132 3.85  0.000    0.251    0.769  0.510\n#&gt; 12    Music ~~    Music 0.556 0.143 3.89  0.000    0.276    0.836  0.556\n#&gt; 13        g ~~        g 1.000 0.000   NA     NA    1.000    1.000  1.000\n#&gt;    std.all\n#&gt; 1    0.956\n#&gt; 2    0.871\n#&gt; 3    0.807\n#&gt; 4    0.743\n#&gt; 5    0.689\n#&gt; 6    0.653\n#&gt; 7    0.086\n#&gt; 8    0.242\n#&gt; 9    0.349\n#&gt; 10   0.447\n#&gt; 11   0.526\n#&gt; 12   0.573\n#&gt; 13   1.000\n\nCon opportuni parametri possiamo semplificare l’output nel modo seguente.\n\nparameterEstimates(fit1, standardized = TRUE) |&gt;\n  dplyr::filter(op == \"=~\") |&gt;\n  dplyr::select(\n    \"Latent Factor\" = lhs,\n    Indicator = rhs,\n    B = est,\n    SE = se,\n    Z = z,\n    \"p-value\" = pvalue,\n    Beta = std.all\n  ) \n#&gt;   Latent.Factor Indicator     B    SE    Z p.value  Beta\n#&gt; 1             g  Classics 0.942 0.129 7.31       0 0.956\n#&gt; 2             g    French 0.857 0.137 6.24       0 0.871\n#&gt; 3             g   English 0.795 0.143 5.54       0 0.807\n#&gt; 4             g      Math 0.732 0.149 4.92       0 0.743\n#&gt; 5             g     Pitch 0.678 0.153 4.44       0 0.689\n#&gt; 6             g     Music 0.643 0.155 4.14       0 0.653\n\nEsaminiamo la matrice delle correlazioni residue:\n\nresiduals(fit1, type = \"cor\")$cov \n#&gt;          Clsscs French Englsh   Math  Pitch  Music\n#&gt; Classics  0.000                                   \n#&gt; French   -0.003  0.000                            \n#&gt; English   0.008 -0.033  0.000                     \n#&gt; Math     -0.011  0.023  0.040  0.000              \n#&gt; Pitch     0.001  0.050 -0.016 -0.062  0.000       \n#&gt; Music     0.005  0.001 -0.017  0.024 -0.050  0.000\n\nCreiamo un qq-plot dei residui:\n\nres1 &lt;- residuals(fit1, type = \"cor\")$cov\nres1[upper.tri(res1, diag = TRUE)] &lt;- NA\nv1 &lt;- as.vector(res1)\nv2 &lt;- v1[!is.na(v1)]\n\ntibble(v2) %&gt;% \n  ggplot(aes(sample = v2)) + \n  stat_qq() + \n  stat_qq_line()",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#diagrammi-di-percorso",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#diagrammi-di-percorso",
    "title": "28  Il modello unifattoriale",
    "section": "\n28.10 Diagrammi di percorso",
    "text": "28.10 Diagrammi di percorso\nIl pacchetto semPlot consente di disegnare diagrammi di percorso per vari modelli SEM. La funzione semPaths prende in input un oggetto creato da lavaan e disegna il diagramma, con diverse opzioni disponibili. Il diagramma prodotto controlla le dimensioni dei caratteri/etichette, la visualizzazione dei residui e il colore dei percorsi/coefficienti. Sono disponibili queste e molte altre opzioni di controllo.\n\nsemPaths(\n    fit1,\n    \"std\",\n    posCol = c(\"black\"),\n    edge.label.cex = 1.2,\n    whatLabels = \"std\", \n    edge.width = 0.3, # Imposta lo spessore delle linee \n    fade = FALSE # Disabilita il fading\n)\n\n\n\n\n\n\n\nIl calcolo delle saturazioni fattoriali con il metodo del centroide aveva prodotto il seguente risultato:\n\nclassici (Cls): 0.97\ninglese (Eng): 0.84\nmatematica (Mth): 0.73\npitch discrimination (Ptc): 0.65\n\nSi noti la somiglianza con i valori ottenuti mediante il metodo di massima verosimiglianza riportati nella figura.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#analisi-fattoriale-esplorativa",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#analisi-fattoriale-esplorativa",
    "title": "28  Il modello unifattoriale",
    "section": "\n28.11 Analisi fattoriale esplorativa",
    "text": "28.11 Analisi fattoriale esplorativa\nQuando abbiamo un’unica variabile latente, l’analisi fattoriale confermativa si riduce al caso dell’analisi fattoriale esplorativa. Esaminiamo qui sotto la sintassi per l’analisi fattoriale esplorativa in lavaan.\nSpecifichiamo il modello.\n\nefa_model &lt;- '\n    efa(\"efa\")*g =~ Classics + French + English + Math + Pitch + Music\n'\n\nAdattiamo il modello ai dati.\n\nfit2 &lt;- lavaan::cfa(\n  efa_model,\n  sample.cov = spearman_cor_mat,\n  sample.nobs = n,\n  std.lv = TRUE\n)\n\nEsaminiamo la soluzione ottenuta.\n\nsummary(fit2, standardized = TRUE) \n#&gt; lavaan 0.6-19 ended normally after 3 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        12\n#&gt; \n#&gt;   Rotation method                       GEOMIN OBLIQUE\n#&gt;   Geomin epsilon                                 0.001\n#&gt;   Rotation algorithm (rstarts)                GPA (30)\n#&gt;   Standardized metric                             TRUE\n#&gt;   Row weights                                     None\n#&gt; \n#&gt;   Number of observations                            33\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                 2.913\n#&gt;   Degrees of freedom                                 9\n#&gt;   P-value (Chi-square)                           0.968\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   g =~ efa                                                              \n#&gt;     Classics          0.942    0.129    7.314    0.000    0.942    0.956\n#&gt;     French            0.857    0.137    6.239    0.000    0.857    0.871\n#&gt;     English           0.795    0.143    5.545    0.000    0.795    0.807\n#&gt;     Math              0.732    0.149    4.923    0.000    0.732    0.743\n#&gt;     Pitch             0.678    0.153    4.438    0.000    0.678    0.689\n#&gt;     Music             0.643    0.155    4.142    0.000    0.643    0.653\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .Classics          0.083    0.051    1.629    0.103    0.083    0.086\n#&gt;    .French            0.234    0.072    3.244    0.001    0.234    0.242\n#&gt;    .English           0.338    0.094    3.610    0.000    0.338    0.349\n#&gt;    .Math              0.434    0.115    3.773    0.000    0.434    0.447\n#&gt;    .Pitch             0.510    0.132    3.855    0.000    0.510    0.526\n#&gt;    .Music             0.556    0.143    3.893    0.000    0.556    0.573\n#&gt;     g                 1.000                               1.000    1.000\n\n\nsemPaths(\n    fit2,\n    \"std\",\n    posCol = c(\"black\"),\n    edge.label.cex = 1.2,\n    whatLabels = \"std\",\n    edge.width = 0.3, # Imposta lo spessore delle linee\n    fade = FALSE # Disabilita il fading\n)",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#riflessioni-conclusive",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#riflessioni-conclusive",
    "title": "28  Il modello unifattoriale",
    "section": "\n28.12 Riflessioni Conclusive",
    "text": "28.12 Riflessioni Conclusive\nIn questo capitolo, abbiamo introdotto il metodo dell’annullamento della tetrade, che permette di stimare le saturazioni in un modello monofattoriale. Abbiamo anche illustrato come questo metodo sia, in effetti, un’applicazione del concetto di correlazione parziale.\nUn aspetto fondamentale nella costruzione dei test psicologici riguarda la determinazione del numero di fattori o tratti sottostanti al set di indicatori in esame. La teoria classica dei test presuppone che un test sia monofattoriale, cioè che gli indicatori riflettano un unico tratto latente. La mancata monodimensionalità introduce difficoltà nell’applicare i principi della teoria classica ai punteggi di un test che non soddisfa tale proprietà.\nL’analisi della dimensionalità di un insieme di indicatori rappresenta, quindi, una fase cruciale nel processo di costruzione di un test. Solitamente, questa valutazione viene effettuata attraverso l’analisi fattoriale. In questo capitolo, abbiamo descritto le proprietà di base del modello unifattoriale, gettando le fondamenta per una comprensione più approfondita della dimensionalità e dell’influenza di un singolo tratto latente sugli indicatori.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#session-info",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#session-info",
    "title": "28  Il modello unifattoriale",
    "section": "\n28.13 Session Info",
    "text": "28.13 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] lavaanExtra_0.2.1 lavaanPlot_0.8.1  kableExtra_1.4.0  corrplot_0.95    \n#&gt;  [5] ggokabeito_0.1.0  see_0.10.0        MASS_7.3-65       viridis_0.6.5    \n#&gt;  [9] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n#&gt; [13] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n#&gt; [17] lavaan_0.6-19     psych_2.4.12      scales_1.3.0      markdown_1.13    \n#&gt; [21] knitr_1.49        lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n#&gt; [25] dplyr_1.1.4       purrr_1.0.4       readr_2.1.5       tidyr_1.3.1      \n#&gt; [29] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] RColorBrewer_1.1-3  rstudioapi_0.17.1   jsonlite_1.9.0     \n#&gt;   [4] magrittr_2.0.3      TH.data_1.1-3       estimability_1.5.1 \n#&gt;   [7] farver_2.1.2        nloptr_2.1.1        rmarkdown_2.29     \n#&gt;  [10] vctrs_0.6.5         minqa_1.2.8         base64enc_0.1-3    \n#&gt;  [13] rstatix_0.7.2       htmltools_0.5.8.1   broom_1.0.7        \n#&gt;  [16] Formula_1.2-5       htmlwidgets_1.6.4   plyr_1.8.9         \n#&gt;  [19] sandwich_3.1-1      emmeans_1.10.7      zoo_1.8-13         \n#&gt;  [22] igraph_2.1.4        mime_0.12           lifecycle_1.0.4    \n#&gt;  [25] pkgconfig_2.0.3     Matrix_1.7-2        R6_2.6.1           \n#&gt;  [28] fastmap_1.2.0       rbibutils_2.3       shiny_1.10.0       \n#&gt;  [31] numDeriv_2016.8-1.1 digest_0.6.37       OpenMx_2.21.13     \n#&gt;  [34] fdrtool_1.2.18      colorspace_2.1-1    rprojroot_2.0.4    \n#&gt;  [37] Hmisc_5.2-2         labeling_0.4.3      timechange_0.3.0   \n#&gt;  [40] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n#&gt;  [43] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n#&gt;  [46] carData_3.0-5       ggsignif_0.6.4      corpcor_1.6.10     \n#&gt;  [49] gtools_3.9.5        tools_4.4.2         pbivnorm_0.6.0     \n#&gt;  [52] foreign_0.8-88      zip_2.3.2           httpuv_1.6.15      \n#&gt;  [55] nnet_7.3-20         glue_1.8.0          quadprog_1.5-8     \n#&gt;  [58] DiagrammeR_1.0.11   nlme_3.1-167        promises_1.3.2     \n#&gt;  [61] lisrelToR_0.3       grid_4.4.2          checkmate_2.3.2    \n#&gt;  [64] cluster_2.1.8       reshape2_1.4.4      generics_0.1.3     \n#&gt;  [67] gtable_0.3.6        tzdb_0.4.0          data.table_1.17.0  \n#&gt;  [70] hms_1.1.3           xml2_1.3.7          car_3.1-3          \n#&gt;  [73] sem_3.1-16          pillar_1.10.1       rockchalk_1.8.157  \n#&gt;  [76] later_1.4.1         splines_4.4.2       lattice_0.22-6     \n#&gt;  [79] survival_3.8-3      kutils_1.73         tidyselect_1.2.1   \n#&gt;  [82] miniUI_0.1.1.1      pbapply_1.7-2       reformulas_0.4.0   \n#&gt;  [85] svglite_2.1.3       stats4_4.4.2        xfun_0.51          \n#&gt;  [88] qgraph_1.9.8        arm_1.14-4          visNetwork_2.1.2   \n#&gt;  [91] stringi_1.8.4       yaml_2.3.10         pacman_0.5.1       \n#&gt;  [94] boot_1.3-31         evaluate_1.0.3      codetools_0.2-20   \n#&gt;  [97] mi_1.1              cli_3.6.4           RcppParallel_5.1.10\n#&gt; [100] rpart_4.1.24        systemfonts_1.2.1   xtable_1.8-4       \n#&gt; [103] Rdpack_2.6.2        munsell_0.5.1       Rcpp_1.0.14        \n#&gt; [106] coda_0.19-4.1       png_0.1-8           XML_3.99-0.18      \n#&gt; [109] parallel_4.4.2      jpeg_0.1-10         lme4_1.1-36        \n#&gt; [112] mvtnorm_1.3-3       openxlsx_4.2.8      rlang_1.1.5        \n#&gt; [115] multcomp_1.4-28     mnormt_2.1.1\n\n\n\n\n\nKan, K.-J., Maas, H. L. van der, & Levine, S. Z. (2019). Extending psychometric network analysis: Empirical evidence against g in favor of mutualism? Intelligence, 73, 52–62.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html",
    "href": "chapters/fa/03_analisi_fattoriale_2.html",
    "title": "29  Il modello statistico dell’analisi fattoriale",
    "section": "",
    "text": "29.1 Modello monofattoriale\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nIl punto di partenza dell’analisi fattoriale esplorativa è rappresentato da una marice di dimensioni \\(p \\times p\\) (dove \\(p\\) è il numero di variabili osservate) che contiene i coefficienti di correlazione (o di covarianza) tra le variabili. Il punto di arrivo è rappresentato da una matrice di dimensioni \\(p \\times k\\) (dove \\(k\\)) è il numero di fattori comuni che contiene i coefficienti (le saturazioni) che esprimono la relazione tra i fattori e le variabili osservate. Considereremo ora il modello matematico dell’analisi fattoriale esplorativa, con un solo fattore comune, che rappresenta il caso più semplice.\nCon \\(p\\) variabili manifeste \\(Y_i\\), il modello ad un fattore comune può essere espresso algebricamente nel modo seguente:\n\\[\nY_i = \\mu_i + \\lambda_{i} \\xi + \\delta_i \\qquad i=1, \\dots, p\n\\]\ndove \\(\\xi\\) rappresenta il fattore latente, chiamato anche fattore comune, poiché è comune a tutte le \\(Y_i\\), i \\(\\delta_i\\) sono invece specifici di ogni variabile osservata e per tale ragione vengono chiamati fattori specifici o unici, e infine i \\(\\lambda_i\\) sono detti saturazioni (o pesi) fattoriali poiché consentono di valutare il peso del fattore latente su ciascuna variabile osservata. Si suole assumere per comodità che \\(\\mu=0\\), il che corrisponde a considerare le variabili \\(Y_i\\) come ottenute dagli scarti dalle medie \\(\\mu_i\\) per \\(i = 1, \\dots, p\\):\n\\[\nY_i -\\mu_i = \\lambda_i \\xi + \\delta_i.\n\\]\nSi assume che il fattore comune abbia media zero, \\(\\mathbb{E}(\\xi)=0\\), e varianza unitaria, \\(\\mathbb{V}(\\xi)=1\\), che i fattori specifici abbiano media zero, \\(\\mathbb{E}(\\delta_j)=0\\), e varianza \\(\\mathbb{V}(\\delta_j)=\\psi_{i}\\), che i fattori specifici siano incorrelati tra loro, \\(\\mathbb{E}(\\delta_i \\delta_k)=0\\), e che i fattori specifici siano incorrelati con il fattore comune, \\(\\mathbb{E}(\\delta_i \\xi)=0\\).\nIn questo modello, poiché i fattori specifici sono tra loro incorrelati, l’interdipendenza tra le variabili manifeste è completamente spiegata dal fattore comune. Dalle ipotesi precedenti è possibile ricavare la covarianza tra \\(Y_i\\) e il fattore comune, la varianza della \\(i\\)-esima variabile manifesta \\(Y_i\\) e la covarianza tra due variabili manifeste \\(Y_i\\) e \\(Y_k\\).",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html#covarianza-tra-un-indicatore-e-il-fattore-comune",
    "href": "chapters/fa/03_analisi_fattoriale_2.html#covarianza-tra-un-indicatore-e-il-fattore-comune",
    "title": "29  Il modello statistico dell’analisi fattoriale",
    "section": "\n29.2 Covarianza tra un indicatore e il fattore comune",
    "text": "29.2 Covarianza tra un indicatore e il fattore comune\nDal modello monofattoriale è possibile determinare l’espressione della covarianza teorica tra una variabile manifesta \\(Y_i\\) e il fattore comune \\(\\xi\\):\n\\[\nCov(Y_i,\\xi)=\\mathbb{E}(Y_i \\xi)-\\mathbb{E}(Y_i)\\mathbb{E}(\\xi).\n\\]\nDato che \\(\\mathbb{E}(\\xi)=0\\), possiamo scrivere\n\\[\n\\begin{equation}\n\\begin{aligned}\n  Cov(Y_i,\\xi) &= \\mathbb{E}(Y_i \\xi)=\\mathbb{E}[(\\lambda_i \\xi + \\delta_i) \\xi]\\notag\\\\\n  &=\\mathbb{E}(\\lambda_i \\xi^2 + \\delta_i \\xi)\\notag\\\\\n  &=\\lambda_i\\underbrace{\\mathbb{E}(\\xi^2)}_{\\mathbb{V}(\\xi)=1} + \\underbrace{\\mathbb{E}(\\delta_i \\xi)}_{Cov(\\delta_i, \\xi)=0}\\notag\\\\\n  &= \\lambda_i.\\notag\n\\end{aligned}\n\\end{equation}\n\\]\nNel modello a un solo fattore, dunque, la saturazione \\(\\lambda_j\\) rappresenta la covarianza la variabile manifesta \\(Y_i\\) e il fattore comune \\(\\xi\\) e indica l’importanza del fattore nel determinare il punteggio osservato. Se le variabili \\(Y_i\\) sono standardizzate, la saturazione fattoriale \\(\\lambda_i\\) corrisponde alla correlazione tra \\(Y_i\\) e \\(\\xi\\).",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html#espressione-fattoriale-della-varianza",
    "href": "chapters/fa/03_analisi_fattoriale_2.html#espressione-fattoriale-della-varianza",
    "title": "29  Il modello statistico dell’analisi fattoriale",
    "section": "\n29.3 Espressione fattoriale della varianza",
    "text": "29.3 Espressione fattoriale della varianza\nNell’ipotesi che le variabili \\(Y_i\\) abbiano media nulla, la varianza di \\(Y_i\\)\n\\[\n\\begin{equation}\n  \\mathbb{V}(Y_i) = \\mathbb{E}(Y_i^2) -[\\mathbb{E}(Y_i)]^2 = \\mathbb{E}(Y_i^2)\\notag\n\\end{equation}\n\\]\nè data da\n\\[\n\\begin{equation}\n\\begin{aligned}\n  \\mathbb{V}(Y_i) &= \\mathbb{E}[(\\lambda_i \\xi + \\delta_i)^2 ]\\notag\\\\\n  &=\\lambda_i^2 \\underbrace{\\mathbb{E}(\\xi^2) }_{\\mathbb{V}(\\xi)=1} + \\underbrace{\\mathbb{E}(\\delta_i^2) }_{\\mathbb{V}(\\delta_i)=\\psi_{i}} + 2\\lambda_i \\underbrace{\\mathbb{E}(\\xi \\delta_i) }_{Cov(\\xi, \\delta_{i})=0}\\notag\\\\\n  &=\\lambda^2_i + \\psi_{i}.\n\\end{aligned}\n\\end{equation}\n\\]\nLa quantità \\(\\lambda^2_i\\) è denominata comunalità della \\(i\\)-esima variabile manifesta e corrisponde alla quota della varianza della \\(Y_i\\) spiegata dal fattore comune. Di conseguenza \\(\\psi_{i}\\) è la parte residua della varianza di \\(Y_i\\) non spiegata dal fattore comune ed è denominata unicità di \\(Y_i\\). Nel caso di variabili standardizzate, l’unicità diventa uguale a\n\\[\n\\psi_{i}=1-\\lambda^2_i.\n\\]\nIn definitiva, la varianza totale di una variabile osservata può essere divisa in una quota che ciascuna variabile condivide con le altre variabili ed è spiegata dal fattore comune (questa quota è chiamata comunalità ed è uguale uguale al quadrato della saturazione della variabile osservata nel fattore comune, ovvero \\(h^2_i = \\lambda_i^2\\)), e in una quota che è spiegata dal fattore specifico (questa parte è chiamata unicità ed è uguale a \\(u_i = \\psi_{i}\\)).\nEsempio. Riprendiamo l’analisi della matrice di correlazioni di Spearman. Nell’output prodotto dalla funzione factanal() viene riportata la quantità denominata SS loadings. Tale quantità indica la porzione della varianza totale delle 4 variabili manifeste che viene spiegata dal fattore comune. Ciascuna variabile standardizzata contribuisce con un’unità di varianza; nel caso presente, dunque la varianza totale è uguale a 4. Si ricordi che, nella statistica multivariata, per varianza totale si intende la somma delle varianze delle variabili manifeste (nel linguaggio dell’algebra matriciale questa quantità corrisponde alla traccia della matrice di covarianze). La quota della varianza totale spiegata dal modello, invece, è data dalla somma delle comunalità delle quattro variabili, ovvero dalla somma delle saturazioni fattoriali innalzate al quadrato.\n\nSpearman &lt;- matrix(c(\n  1.0, .78, .70, .66,\n  .78, 1.0, .64, .54,\n  .70, .64, 1.0, .45,\n  .66, .54, .45, 1.0\n),\nbyrow = TRUE, ncol = 4\n)\nrownames(Spearman) &lt;- c(\"C\", \"E\", \"M\", \"P\")\ncolnames(Spearman) &lt;- c(\"C\", \"E\", \"M\", \"P\")\nSpearman |&gt;\n  print()\n#&gt;      C    E    M    P\n#&gt; C 1.00 0.78 0.70 0.66\n#&gt; E 0.78 1.00 0.64 0.54\n#&gt; M 0.70 0.64 1.00 0.45\n#&gt; P 0.66 0.54 0.45 1.00\n\nEseguiamo l’analisi fattoriale:\n\nfm &lt;- factanal(covmat = Spearman, factors = 1)\nfm |&gt;\n    print()\n#&gt; \n#&gt; Call:\n#&gt; factanal(factors = 1, covmat = Spearman)\n#&gt; \n#&gt; Uniquenesses:\n#&gt;     C     E     M     P \n#&gt; 0.086 0.329 0.460 0.539 \n#&gt; \n#&gt; Loadings:\n#&gt;   Factor1\n#&gt; C 0.956  \n#&gt; E 0.819  \n#&gt; M 0.735  \n#&gt; P 0.679  \n#&gt; \n#&gt;                Factor1\n#&gt; SS loadings      2.587\n#&gt; Proportion Var   0.647\n#&gt; \n#&gt; The degrees of freedom for the model is 2 and the fit was 0.023\n\nLe saturazioni fattoriali sono:\n\nL &lt;- c(fm$load[1], fm$load[2], fm$load[3], fm$load[4])\nprint(L)\n#&gt; [1] 0.956 0.819 0.735 0.679\n\nFacendo il prodotto interno otteniamo:\n\nt(L) %*% L \n#&gt;      [,1]\n#&gt; [1,] 2.59\n\nIn termini proporzionali, la quota della varianza totale delle variabile manifeste che viene spiegata dal modello ad un fattore comune è dunque uguale a \\(2.587 / 4 = 0.647\\). Questa quantità è indicata nell’output con la denominazione Proportion Var.\nSi dice unicità (uniqueness) la quota della varianza della variabile considerata che non viene spiegata dalla soluzione fattoriale:\n\nround(fm$uniqueness, 3) |&gt;\n    print()\n#&gt;     C     E     M     P \n#&gt; 0.086 0.329 0.460 0.539\n\nLa comunalità (ovvero, la quota di varianza di ciascuna variabile manifesta che viene spiegata dal fattore comune) può essere trovata come:\n\nround(1 - fm$uniqueness, 3) |&gt;\n    print()\n#&gt;     C     E     M     P \n#&gt; 0.914 0.671 0.540 0.461\n\noppure con\n\nL^2 |&gt;\n    round(3) |&gt;\n    print()\n#&gt; [1] 0.914 0.671 0.540 0.461",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html#covarianza-tra-due-variabili-manifeste",
    "href": "chapters/fa/03_analisi_fattoriale_2.html#covarianza-tra-due-variabili-manifeste",
    "title": "29  Il modello statistico dell’analisi fattoriale",
    "section": "\n29.4 Covarianza tra due variabili manifeste",
    "text": "29.4 Covarianza tra due variabili manifeste\nNell’ipotesi che le variabili \\(Y_i\\) abbiano media nulla, la covarianza tra \\(Y_i\\) e \\(Y_k\\)\n\\[\nCov(Y_i, Y_k)=\\mathbb{E}(Y_i Y_k) -\n\\mathbb{E}(Y_i)\\mathbb{E}(Y_k)=\\mathbb{E}(Y_i Y_k)\n\\]\nè uguale al prodotto delle corrispondenti saturazioni fattoriali:\n\\[\n\\begin{equation}\n\\begin{aligned}\nCov(Y_i, Y_k) &= \\mathbb{E}(Y_i Y_k) \\notag\\\\\n  & =\\mathbb{E}[(\\lambda_i \\xi + \\delta_i)(\\lambda_k \\xi +  \\delta_k)]\\notag\\\\\n  &=\\mathbb{E}(\\lambda_i\\lambda_k\\xi^2 + \\lambda_i  \\xi \\delta_k + \\lambda_k \\delta_i \\xi + \\delta_i \\delta_k)\\notag\\\\\n  &=\\lambda_i\\lambda_k\\underbrace{\\mathbb{E}(\\xi^2)}_{\\mathbb{V}(\\xi)=1}+\\lambda_i\\underbrace{\\mathbb{E}(\\xi \\delta_k)}_{Cov(\\xi, \\delta_k) =0}+\\notag\\\\ \\;&+\\lambda_k\\underbrace{\\mathbb{E}(\\delta_i \\xi)}_{Cov(\\delta_i, \\xi) =0} +\\underbrace{\\mathbb{E}(\\delta_i \\delta_k)}_{Cov(\\delta_i, \\delta_k)=0}\\notag\\\\\n  &=\\lambda_i\\lambda_k.\n\\end{aligned}\n\\end{equation}\n\\]",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html#correlazioni-osservate-e-correlazioni-riprodotte-dal-modello",
    "href": "chapters/fa/03_analisi_fattoriale_2.html#correlazioni-osservate-e-correlazioni-riprodotte-dal-modello",
    "title": "29  Il modello statistico dell’analisi fattoriale",
    "section": "\n29.5 Correlazioni osservate e correlazioni riprodotte dal modello",
    "text": "29.5 Correlazioni osservate e correlazioni riprodotte dal modello\nIn generale possiamo affermare che il modello monofattoriale è adeguato se si verifica che \\(Cov(Y_i, Y_k \\mid \\xi) = 0\\) (\\(i, k = 1, \\dots,p; \\; i\\neq k\\)), ossia se il fattore comune spiega tutta la covarianza tra le variabili osservate. La matrice di correlazioni riprodotte dal modello è chiamata \\(\\boldsymbol{\\Sigma}\\) e può essere espressa come:\n\\[\n\\boldsymbol{\\Sigma} = \\boldsymbol{\\Lambda} \\boldsymbol{\\Lambda}^\\prime + \\boldsymbol{\\Psi}\n\\]\nIn altri termini, il modello monofattoriale è adeguato se è nulla la differenza tra la matrice di correlazioni osservate e la matrice di correlazioni riprodotte dal modello. Per i dati di Spearman, le correlazioni riprodotte dal modello ad un fattore sono\n\nround(L %*% t(L) + diag(fm$uniq), 3)\n#&gt;       [,1]  [,2]  [,3]  [,4]\n#&gt; [1,] 1.000 0.784 0.703 0.649\n#&gt; [2,] 0.784 1.000 0.602 0.556\n#&gt; [3,] 0.703 0.602 1.000 0.499\n#&gt; [4,] 0.649 0.556 0.499 1.000\n\nLa matrice delle differenze tra le correlazioni campionarie e quelle riprodotte è\n\nround(Spearman - (L %*% t(L) + diag(fm$uniq)), 3) \n#&gt;        C      E      M      P\n#&gt; C  0.000 -0.004 -0.003  0.011\n#&gt; E -0.004  0.000  0.038 -0.016\n#&gt; M -0.003  0.038  0.000 -0.049\n#&gt; P  0.011 -0.016 -0.049  0.000\n\nLo scarto maggiore tra le correlazioni campionarie e quelle riprodotte è uguale a 0.049. Si può dunque concludere che il modello monofattoriale spiega in maniera ragionevole i dati di Spearman.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html#bontà-di-adattamento-del-modello-ai-dati",
    "href": "chapters/fa/03_analisi_fattoriale_2.html#bontà-di-adattamento-del-modello-ai-dati",
    "title": "29  Il modello statistico dell’analisi fattoriale",
    "section": "\n29.6 Bontà di adattamento del modello ai dati",
    "text": "29.6 Bontà di adattamento del modello ai dati\nLa verifica della bontà di adattamento del modello ai dati si determina mediante un test statistico che valuta la differenza tra la matrice di correlazioni (o di covarianze) osservata e la matrice di correlazioni (o covarianze) predetta dal modello fattoriale. L’ipotesi nulla che viene valutata è che la matrice delle correlazioni residue sia dovuta semplicemente agli errori di campionamento, ovvero che la matrice di correlazioni predetta dal modello \\(\\boldsymbol{\\Sigma}(\\theta)\\) sia uguale alla matrice di correlazioni \\(\\boldsymbol{\\Sigma}\\) nella popolazione.\nLa statistica test \\(v\\) è una funzione della differenza tra la matrice riprodotta \\(\\boldsymbol{S}(\\theta)\\) e quella osservata \\(\\boldsymbol{S}\\)\n\\[\nv = f\\left[\\boldsymbol{S}(\\theta) - \\boldsymbol{S}\\right]\n\\]\ne si distribuisce come una \\(\\chi^2\\) con \\(\\nu\\) gradi di libertà\n\\[\n\\nu = p(p+1)/ 2 - q,\n\\]\ndove \\(p\\) è il numero di variabili manifeste e \\(q\\) è il numero di parametri stimati dal modello fattoriale (ovvero, \\(\\lambda\\) e \\(\\psi\\)).\nLa statistica \\(v\\) assume valore 0 se i parametri del modello riproducono esattamente la matrice di correlazioni tra le variabili nella popolazione. Tanto maggiore è la statistica \\(v\\) tanto maggiore è la discrepanza tra le correlazioni osservate e quelle predette dal modello fattoriale.\nUn risultato statisticamente significativo (es., \\(p\\) &lt; .05) – il quale suggerisce che una tale differenza non è uguale a zero – rivela dunque una discrepanza tra il modello e i dati. Il test del modello fattoriale mediante la statistica \\(\\chi^2\\) segue dunque una logica diversa da quella utilizzata nei normali test di ipotesi statistiche: un risultato statisticamente significativo indica una mancanza di adattamento del modello ai dati.\nL’applicazione del test \\(\\chi^2\\) per valutare la bontà di adattamento del modello ai dati richiede che ciascuna variabile manifesta sia distribuita normalmente – più precisamente, richiede che le variabili manifeste siano un campione casuale che deriva da una normale multivariata. Questo requisito non è facile da rispettare in pratica.\nTuttavia, il limite principale della statistica \\(\\chi^2\\) è che essa dipende fortemente dalle dimensioni del campione: al crescere delle dimensioni campionarie è più facile ottenere un risultato statisticamente significativo (ovvero, concludere che vi è un cattivo adattamento del modello ai dati). Per questa ragione, la bontà di adattamento del modello ai dati viene valutata da molteplici indici, non soltanto dalla statistica \\(\\chi^2\\). Più comune è calcolare il rapporto \\(\\chi^2 / \\nu\\) e usare tale rapporto per valutare la bontà dell’adattamento. Valori minori di 3 o 4 suggeriscono che il modello ben si adatta ai dati.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html#lerrore-standard-della-misurazione-e-il-modello-fattoriale",
    "href": "chapters/fa/03_analisi_fattoriale_2.html#lerrore-standard-della-misurazione-e-il-modello-fattoriale",
    "title": "29  Il modello statistico dell’analisi fattoriale",
    "section": "\n29.7 L’errore standard della misurazione e il modello fattoriale",
    "text": "29.7 L’errore standard della misurazione e il modello fattoriale\nIn questa sezione, approfondiamo la connessione tra l’errore standard di misurazione, un concetto fondamentale della Classical Test Theory (CTT), e l’applicazione del modello fattoriale. Questa connessione ci permette di reinterpretare l’errore standard di misurazione attraverso il prisma dell’analisi fattoriale. Procediamo con un’esposizione dettagliata.\nAll’interno della CTT, si afferma che il punteggio ottenuto (\\(X\\)) in un test corrisponde alla somma del valore vero (\\(T\\)) e dell’errore di misurazione (\\(E\\)), dove \\(E\\) è considerato una variabile casuale indipendente da \\(T\\). Se focalizziamo l’attenzione sul soggetto \\(i\\)-esimo, la formula diventa \\(X_i = T_i + E_i\\), con \\(T_i\\) rappresentante il valore vero e \\(E_i\\) l’errore di misurazione, quest’ultimo avente media zero.\nTrasformiamo questa relazione nel contesto di un modello fattoriale monofattoriale che coinvolge \\(p\\) variabili osservate (o item). Per ogni item, la relazione è espressa come:\n\\[\n\\begin{equation}\n\\begin{aligned}\nY_{1i} &=  \\lambda_1 \\xi_i + \\delta_{1i} \\notag\\\\\nY_{2i} &=  \\lambda_2 \\xi_i + \\delta_{2i} \\notag\\\\\n  \\dots\\notag\\\\\nY_{pi} &=  \\lambda_p \\xi_i + \\delta_{pi}, \\notag\n\\end{aligned}\n\\end{equation}\n\\]\ndove \\(Y_{ji}\\) rappresenta il punteggio osservato per l’item \\(j\\) del soggetto \\(i\\), \\(\\lambda_j\\) è il carico fattoriale dell’item \\(j\\) sul fattore comune \\(\\xi_i\\), e \\(\\delta_{ji}\\) è l’errore unico associato all’item \\(j\\) per il soggetto \\(i\\).\nIl punteggio totale \\(X_i\\) per il soggetto \\(i\\)-esimo deriva dalla somma dei punteggi di ciascun item, il che si traduce in:\n\\[\n\\begin{equation}\n\\begin{aligned}\nX_i &= \\sum_{j=1}^p Y_{ji} = \\sum_{j=1}^p \\lambda_j \\xi_i + \\sum_{j=1}^p \\delta_{ji}\\notag\\\\[12pt]\n  &=  \\left( \\sum_{j=1}^p \\lambda_j \\right) \\xi_i  +  \\sum_{j=1}^p \\delta_{ji} \\notag\\\\[12pt]\n  &= T_i + E_i\\notag\n\\end{aligned}\n\\end{equation}\n\\]\nRispettando la struttura della CTT, la varianza del punteggio osservato \\(X_i\\) si decompone in due componenti fondamentali: la varianza del valore vero \\(\\sigma^2_{T_i}\\) e la varianza dell’errore \\(\\sigma^2_{E_i}\\). Nel contesto dell’analisi fattoriale, \\(\\sigma^2_{T_i}\\) corrisponde al quadrato della somma dei carichi fattoriali:\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\sigma^2_{T_i} &= \\mathbb{V}\\left[ \\left( \\sum_{j=1}^p \\lambda_j \\right) \\xi_i \\right]\\notag\\\\\n&= \\left( \\sum_{j=1}^p \\lambda_j \\right)^2 \\mathbb{V}(\\xi_i)\\notag\\\\\n&= \\left( \\sum_{j=1}^p \\lambda_j \\right)^2 \\notag\n\\end{aligned}\n\\end{equation}\n\\]\nInoltre, considerando la varianza dell’errore di misurazione \\(\\sigma^2_{E_i}\\) nel contesto fattoriale, questa è equivalente alla somma delle varianze degli errori unici (\\(\\delta_{ji}\\)), ovvero le unicità:\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\sigma^2_{E_i} &= \\mathbb{V}\\left( \\sum_{j=1}^p \\delta_{ji} \\right)\\notag\\\\\n&= \\sum_{j=1}^p \\mathbb{V}\\left( \\delta_{ji} \\right)\\notag\\\\\n&= \\sum_{j=1}^p \\Psi_j\\notag\n\\end{aligned}\n\\end{equation}\n\\]\nPertanto, nel contesto dell’analisi fattoriale, l’errore standard di misurazione per il punteggio totale del test è quantificabile come la radice quadrata della somma delle unicità:\n\\[\n\\begin{equation}\n\\sigma_{E} = \\sqrt{\\sum_{j=1}^p \\Psi_j}\n\\end{equation}\n\\](eq-err-stnd-meas-FA)\nQuesto collegamento tra la CTT e l’analisi fattoriale offre una prospettiva rinnovata sull’errore standard di misurazione, arricchendo la nostra comprensione della precisione dei test psicometrici.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html#un-esempio-concreto",
    "href": "chapters/fa/03_analisi_fattoriale_2.html#un-esempio-concreto",
    "title": "29  Il modello statistico dell’analisi fattoriale",
    "section": "\n29.8 Un esempio concreto",
    "text": "29.8 Un esempio concreto\nApplichiamo ora il risultato precedente ad un caso concreto. Consideriamo i dati utilizzati nella validazione italiana del Cognitive Style Questionnaire - Short Form (CSQ-SF, Meins et al. 2012). Il CSQ-SF viene utilizzato per misurare la vulnerabilità all’ansia e alla depressione. È costituito da cinque sottoscale: Internality, Globality, Stability, Negative consequences e Self-worth.\nLeggiamo i dati in \\(\\textsf{R}\\):\n\ncsq &lt;- rio::import(here::here(\"data\", \"csq540.csv\"))\n\nIl numero di partecipanti è\n\nn &lt;- nrow(csq)\nn\n#&gt; [1] 540\n\nLe statistiche descrittive si ottengono con la seguente istruzione:\n\npsych::describe(csq, type = 2) \n#&gt;   vars   n mean    sd median trimmed   mad min max range  skew kurtosis\n#&gt; I    1 540 47.8  5.78     48    47.9  4.45  21  64    43 -0.31     1.07\n#&gt; G    2 540 45.0 11.94     42    44.5 11.86  16  78    62  0.34    -0.70\n#&gt; S    3 540 44.6 12.18     42    44.2 13.34  16  77    61  0.27    -0.77\n#&gt; N    4 540 22.0  6.92     21    21.9  7.41   8  39    31  0.21    -0.74\n#&gt; W    5 540 44.0 13.10     43    43.7 13.34  16  79    63  0.31    -0.53\n#&gt;     se\n#&gt; I 0.25\n#&gt; G 0.51\n#&gt; S 0.52\n#&gt; N 0.30\n#&gt; W 0.56\n\nEsaminiamo la matrice di correlazione:\n\npsych::pairs.panels(csq) \n\n\n\n\n\n\n\nLa sottoscala di Internality è problematica, come messo anche in evidenza dall’autore del test. La consideriamo comunque in questa analisi statistica.\nSpecifichiamo il modello unifattoriale nella sintassi di lavaan:\n\nmod_csq &lt;- \"\n   F =~ NA*I + G + S + N + W\n   F ~~ 1*F\n\" \n\nAdattiamo il modello ai dati:\n\nfit &lt;- lavaan:::cfa(\n  mod_csq,\n  data = csq\n)\n\nEsaminiamo i risultati:\n\nsummary(\n  fit, \n  standardized = TRUE,\n  fit.measures = TRUE\n) |&gt;\n  print()\n#&gt; lavaan 0.6-19 ended normally after 26 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        10\n#&gt; \n#&gt;   Number of observations                           540\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                46.716\n#&gt;   Degrees of freedom                                 5\n#&gt;   P-value (Chi-square)                           0.000\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                              2361.816\n#&gt;   Degrees of freedom                                10\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.982\n#&gt;   Tucker-Lewis Index (TLI)                       0.965\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -8741.781\n#&gt;   Loglikelihood unrestricted model (H1)      -8718.423\n#&gt;                                                       \n#&gt;   Akaike (AIC)                               17503.562\n#&gt;   Bayesian (BIC)                             17546.478\n#&gt;   Sample-size adjusted Bayesian (SABIC)      17514.734\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.124\n#&gt;   90 Percent confidence interval - lower         0.093\n#&gt;   90 Percent confidence interval - upper         0.158\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.000\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.989\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.033\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   F =~                                                                  \n#&gt;     I                 0.725    0.253    2.867    0.004    0.725    0.126\n#&gt;     G               -11.322    0.384  -29.481    0.000  -11.322   -0.949\n#&gt;     S               -11.342    0.398  -28.513    0.000  -11.342   -0.932\n#&gt;     N                -6.163    0.233  -26.398    0.000   -6.163   -0.891\n#&gt;     W               -11.598    0.444  -26.137    0.000  -11.598   -0.886\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;     F                 1.000                               1.000    1.000\n#&gt;    .I                32.840    2.000   16.420    0.000   32.840    0.984\n#&gt;    .G                14.038    1.473    9.532    0.000   14.038    0.099\n#&gt;    .S                19.508    1.718   11.353    0.000   19.508    0.132\n#&gt;    .N                 9.847    0.725   13.573    0.000    9.847    0.206\n#&gt;    .W                36.892    2.685   13.737    0.000   36.892    0.215\n\nEsaminiamo solo le stime dei parametri del modello:\n\nparameterEstimates(fit) |&gt;\n    print()\n#&gt;    lhs op rhs     est    se      z pvalue ci.lower ci.upper\n#&gt; 1    F =~   I   0.725 0.253   2.87  0.004    0.229     1.22\n#&gt; 2    F =~   G -11.322 0.384 -29.48  0.000  -12.075   -10.57\n#&gt; 3    F =~   S -11.342 0.398 -28.51  0.000  -12.122   -10.56\n#&gt; 4    F =~   N  -6.163 0.233 -26.40  0.000   -6.621    -5.71\n#&gt; 5    F =~   W -11.598 0.444 -26.14  0.000  -12.467   -10.73\n#&gt; 6    F ~~   F   1.000 0.000     NA     NA    1.000     1.00\n#&gt; 7    I ~~   I  32.840 2.000  16.42  0.000   28.920    36.76\n#&gt; 8    G ~~   G  14.038 1.473   9.53  0.000   11.151    16.92\n#&gt; 9    S ~~   S  19.508 1.718  11.35  0.000   16.140    22.88\n#&gt; 10   N ~~   N   9.847 0.725  13.57  0.000    8.425    11.27\n#&gt; 11   W ~~   W  36.892 2.685  13.74  0.000   31.628    42.16\n\nRecuperiamo le specificità:\n\npsi &lt;- parameterEstimates(fit)$est[7:11]\npsi |&gt;\n    print()\n#&gt; [1] 32.84 14.04 19.51  9.85 36.89\n\nStimiamo l’errore standard della misurazione con la @ref(eq:err-stnd-meas-FA):\n\nsqrt(sum(psi)) |&gt;\n    print()\n#&gt; [1] 10.6\n\nApplichiamo ora la formula della TCT:\n\\[\n\\sigma_E = \\sigma_X \\sqrt{1 -\\rho_{XX^\\prime}}.\n\\]\nPer trovare \\(\\sigma\\) calcoliamo prima il punteggio totale:\n\ntot_score &lt;- rowSums(csq)\n\nLa deviazione standard di tot_score ci fornisce una stima di \\(\\sigma_X\\):\n\nsigma &lt;- sd(tot_score)\nsigma |&gt;\n    print()\n#&gt; [1] 41.3\n\nPer applicare la formula della TCT abbiamo bisogno dell’attendibilità. La stimiamo usando la funzione reliability del pacchetto semTools dall’oggetto creato da lavaan:::cfa():\n\nrel &lt;- semTools::reliability(fit)\nrel |&gt;\n    print()\n#&gt;            F\n#&gt; alpha  0.851\n#&gt; omega  0.933\n#&gt; omega2 0.933\n#&gt; omega3 0.927\n#&gt; avevar 0.792\n\nUtilizzando \\(\\Omega\\) otteniamo:\n\nsigma * sqrt(1- rel[2]) |&gt;\n    print()\n#&gt; [1] 0.259\n#&gt; [1] 10.7\n\nSi noti come il risultato sia molto simile a quello trovato con la formula della TCT.\n\n29.8.1 Correlazioni osservate e riprodotte\nLe correlazioni riprodotte dal modello si ottengono nel modo seguente dall’oggetto fit.\n\ncor_mat &lt;- lavInspect(fit, \"cor.ov\")\ncor_mat |&gt;\n    print()\n#&gt;        I      G      S      N      W\n#&gt; I  1.000                            \n#&gt; G -0.119  1.000                     \n#&gt; S -0.117  0.885  1.000              \n#&gt; N -0.112  0.846  0.830  1.000       \n#&gt; W -0.111  0.841  0.825  0.789  1.000\n\nAbbiamo visto come il modello unifattoriale predice che la correlazione tra due variabili manifeste sia il prodotto delle rispettive correlazioni fattoriali. Estraiamo le saturazioni fattoriali.\n\nl &lt;- inspect(fit, what=\"std\")$lambda\nl |&gt;\n    print()\n#&gt;        F\n#&gt; I  0.126\n#&gt; G -0.949\n#&gt; S -0.932\n#&gt; N -0.891\n#&gt; W -0.886\n\nPer esempio, se consideriamo I e G, la correlazione predetta dal modello fattoriale tra queste due sottoscale è data dal prodotto delle rispettive saturazioni fattoriali.\n\nl[1] * l[2] \n#&gt; [1] -0.119\n\nLa matrice di correlazioni riprodotte riportata sopra mostra il risultato di questo prodotto per ciascuna coppia di variabili manifeste.\n\nl %*% t(l) |&gt; round(3) |&gt;\n    print()\n#&gt;        I      G      S      N      W\n#&gt; I  0.016 -0.119 -0.117 -0.112 -0.111\n#&gt; G -0.119  0.901  0.885  0.846  0.841\n#&gt; S -0.117  0.885  0.868  0.830  0.825\n#&gt; N -0.112  0.846  0.830  0.794  0.789\n#&gt; W -0.111  0.841  0.825  0.789  0.785\n\n\n29.8.2 Scomposizione della varianza\nConsideriamo la variabile manifesta W. Calcoliamo la varianza.\n\nvar(csq$W) |&gt; print()\n#&gt; [1] 172\n\nLa varianza riprodotta di questa variabile, secondo il modello fattoriale, dovrebbe esere uguale alla somma di due componenti: la varianza predetta dall’effetto causale del fattore latente e la varianza residua. La varianza predetta dall’effetto causale del fattore latente è uguale alla saturazione elevata al quadrato:\n\n(-11.598)^2 \n#&gt; [1] 135\n\nCalcolo ora la proporzione di varianza residua normalizzando rispetto alla varianza osservata (non a quella riprodotta dal modello):\n\n1 - (-11.598)^2 / var(csq$W) \n#&gt; [1] 0.217\n\nIl valore così ottenuto è molto simile al valore della varianza residua di W.\nRipeto i calcoli per la variabile G\n\n1 - (-11.322)^2 / var(csq$G) \n#&gt; [1] 0.1\n\ne per la variabile I\n\n1 - (0.725)^2 / var(csq$I) \n#&gt; [1] 0.984\n\nIn tutti i casi, i valori ottenuti sono molto simili alle varianze residue ipotizzate dal modello unifattoriale.\n\n29.8.3 Correlazione tra variabili manifeste e fattore comune\nUn modo per verificare il fatto che, nel modello unifattoriale, la saturazione fattoriale della \\(i\\)-esima variabile manifesta è uguale alla correlazione tra i punteggi osservati sulla i$-esima variabile manifesta e il fattore latente è quella di calcolare le correlazioni tra le variabili manifeste e i punteggi fattoriali. I punteggi fattoriali rappresentano una stima del punteggio “vero”, ovvero del punteggio che ciascun rispondente otterrebbe in assenza di errori di misurazione. Vedremo in seguito come si possono stimare i punteggi fattoriali. Per ora ci limitiamo a calcolarli usando lavaan.\n\nhead(lavPredict(fit)) |&gt;\n    print()\n#&gt;           F\n#&gt; [1,]  0.269\n#&gt; [2,] -0.911\n#&gt; [3,]  0.187\n#&gt; [4,] -0.332\n#&gt; [5,]  0.831\n#&gt; [6,]  1.153\n\nAbbiamo un punteggio diverso per ciascuno dei 540 individui che appartengono al campione di dati esaminato.\n\ndim(lavPredict(fit))\n#&gt; [1] 540   1\n\nCalcoliamo ora le correlazioni tra i valori osservati su ciascuna delle cinque scale del CSQ e le stime dei punteggi veri.\n\nc(\n  cor(csq$I, lavPredict(fit)),\n  cor(csq$G, lavPredict(fit)),\n  cor(csq$S, lavPredict(fit)),\n  cor(csq$N, lavPredict(fit)),\n  cor(csq$W, lavPredict(fit))\n) |&gt; \n  round(3) |&gt;\n    print()\n#&gt; [1]  0.128 -0.970 -0.952 -0.910 -0.905\n\nSi noti che i valori ottenui sono molto simili ai valori delle saturazioni fattoriali. La piccola differenza tra le correlazioni ottenute e i valori delle saturazioni fattoriali dipende dal fatto che abbiamo stimato i punteggi fattoriali.\n\ninspect(fit, what=\"std\")$lambda |&gt;\n    print()\n#&gt;        F\n#&gt; I  0.126\n#&gt; G -0.949\n#&gt; S -0.932\n#&gt; N -0.891\n#&gt; W -0.886",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html#session-info",
    "href": "chapters/fa/03_analisi_fattoriale_2.html#session-info",
    "title": "29  Il modello statistico dell’analisi fattoriale",
    "section": "\n29.9 Session Info",
    "text": "29.9 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] ggokabeito_0.1.0  see_0.10.0        MASS_7.3-65       viridis_0.6.5    \n#&gt;  [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n#&gt;  [9] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n#&gt; [13] lavaan_0.6-19     psych_2.4.12      scales_1.3.0      markdown_1.13    \n#&gt; [17] knitr_1.49        lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n#&gt; [21] dplyr_1.1.4       purrr_1.0.4       readr_2.1.5       tidyr_1.3.1      \n#&gt; [25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.0      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] rio_1.2.3           emmeans_1.10.7      zoo_1.8-13         \n#&gt;  [22] igraph_2.1.4        mime_0.12           lifecycle_1.0.4    \n#&gt;  [25] pkgconfig_2.0.3     Matrix_1.7-2        R6_2.6.1           \n#&gt;  [28] fastmap_1.2.0       rbibutils_2.3       shiny_1.10.0       \n#&gt;  [31] digest_0.6.37       OpenMx_2.21.13      fdrtool_1.2.18     \n#&gt;  [34] colorspace_2.1-1    rprojroot_2.0.4     Hmisc_5.2-2        \n#&gt;  [37] timechange_0.3.0    abind_1.4-8         compiler_4.4.2     \n#&gt;  [40] withr_3.0.2         glasso_1.11         htmlTable_2.4.3    \n#&gt;  [43] backports_1.5.0     carData_3.0-5       R.utils_2.13.0     \n#&gt;  [46] ggsignif_0.6.4      corpcor_1.6.10      gtools_3.9.5       \n#&gt;  [49] tools_4.4.2         pbivnorm_0.6.0      foreign_0.8-88     \n#&gt;  [52] zip_2.3.2           httpuv_1.6.15       nnet_7.3-20        \n#&gt;  [55] R.oo_1.27.0         glue_1.8.0          quadprog_1.5-8     \n#&gt;  [58] nlme_3.1-167        promises_1.3.2      lisrelToR_0.3      \n#&gt;  [61] grid_4.4.2          checkmate_2.3.2     cluster_2.1.8      \n#&gt;  [64] reshape2_1.4.4      generics_0.1.3      gtable_0.3.6       \n#&gt;  [67] tzdb_0.4.0          R.methodsS3_1.8.2   data.table_1.17.0  \n#&gt;  [70] hms_1.1.3           car_3.1-3           sem_3.1-16         \n#&gt;  [73] pillar_1.10.1       rockchalk_1.8.157   later_1.4.1        \n#&gt;  [76] splines_4.4.2       lattice_0.22-6      survival_3.8-3     \n#&gt;  [79] kutils_1.73         tidyselect_1.2.1    miniUI_0.1.1.1     \n#&gt;  [82] pbapply_1.7-2       reformulas_0.4.0    stats4_4.4.2       \n#&gt;  [85] xfun_0.51           qgraph_1.9.8        arm_1.14-4         \n#&gt;  [88] stringi_1.8.4       yaml_2.3.10         pacman_0.5.1       \n#&gt;  [91] boot_1.3-31         evaluate_1.0.3      codetools_0.2-20   \n#&gt;  [94] mi_1.1              cli_3.6.4           RcppParallel_5.1.10\n#&gt;  [97] rpart_4.1.24        xtable_1.8-4        Rdpack_2.6.2       \n#&gt; [100] munsell_0.5.1       Rcpp_1.0.14         coda_0.19-4.1      \n#&gt; [103] png_0.1-8           XML_3.99-0.18       parallel_4.4.2     \n#&gt; [106] jpeg_0.1-10         lme4_1.1-36         mvtnorm_1.3-3      \n#&gt; [109] openxlsx_4.2.8      rlang_1.1.5         multcomp_1.4-28    \n#&gt; [112] mnormt_2.1.1\n\n\n\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/04_analisi_fattoriale_3.html",
    "href": "chapters/fa/04_analisi_fattoriale_3.html",
    "title": "30  Il modello multifattoriale",
    "section": "",
    "text": "30.1 Fattori ortogonali\nLa teoria dei due fattori ha orientato per diversi anni le ricerche sull’intelligenza, finché Thurstone (1945) non propose una sua modifica, conosciuta come teoria multifattoriale. Secondo Thurstone la covariazione tra le variabili manifeste non può essere spiegata da un unico fattore generale. Invece è necessario ipotizzare l’azione causale di diversi fattori, definiti comuni, i quali si riferiscono solo ad alcune delle variabili considerate.\nIl modello plurifattoriale assume che ciascuna variabile manifesta sia espressa come funzione lineare di un certo numero \\(m\\) di fattori comuni, \\(\\xi_1, \\xi_2, \\dots, \\xi_m\\), responsabili della correlazione con le altre variabili, e di un solo fattore specifico (termine d’errore), responsabile della variabilità della variabile stessa. Per \\(p\\) variabili manifeste, \\(Y_1, Y_2, \\dots, Y_p\\), il modello fattoriale diventa quello indicato dal sistema di equazioni lineari descritto di seguito. Idealmente, \\(m\\) dovrebbe essere molto più piccolo di \\(p\\) così da consentire una descrizione parsimoniosa delle variabili manifeste in funzione di pochi fattori soggiacenti.\nLe variabili manifeste \\(Y\\) sono indicizzate da \\(i = 1, \\dots, p.\\) Le variabili latenti \\(\\xi\\) (fattori) sono indicizzate da \\(j = 1, \\dots, m.\\) I fattori specifici \\(\\delta\\) sono indicizzati da \\(i = 1, \\dots, p.\\) Le saturazioni fattoriali si distinguono dunque tramite due indici, \\(i\\) e \\(j\\): il primo indice si riferisce alle variabili manifeste, il secondo si riferisce ai fattori latenti.\nIndichiamo con \\(\\mu_i\\), con \\(i=1, \\dots, p\\) le medie delle \\(p\\) variabili manifeste \\(Y_1, Y_2, \\dots, Y_p\\). Se non vi è alcun effetto delle variabili comuni latenti, allora la variabile \\(Y_{ijk}\\), dove \\(k\\) è l’indice usato per i soggetti, sarà uguale a:\n\\[\n\\begin{equation}\n\\begin{cases}\n  Y_{1k}    &= \\mu_1 + \\delta_{1k} \\\\\n&\\vdots\\\\\nY_{ik}   &= \\mu_i + \\delta_{ik}\\\\\n&\\vdots\\\\\nY_{pk}   &= \\mu_p + \\delta_{pk} \\notag\n\\end{cases}\n\\end{equation}\n\\]\nSe invece le variabili manifeste rappresentano la somma dell’effetto causale di \\(m\\) fattori comuni e di \\(p\\) fattori specifici, allora possiamo scrivere:\n\\[\n\\begin{equation}\n\\begin{cases}\n  Y_1  - \\mu_1  &= \\lambda_{11}\\xi_1 + \\dots + \\lambda_{1k}\\xi_k \\dots +\\lambda_{1m}\\xi_m + \\delta_1 \\\\\n&\\vdots\\\\\nY_i -  \\mu_i  &= \\lambda_{i1}\\xi_1 + \\dots +  \\lambda_{ik}\\xi_k \\dots +\\lambda_{im}\\xi_m + \\delta_i\\\\\n&\\vdots\\\\\nY_p - \\mu_p  &= \\lambda_{p1}\\xi_1 + \\dots +  \\lambda_{pk}\\xi_k \\dots +\\lambda_{pm}\\xi_m + \\delta_p \\notag\n\\end{cases}\n\\end{equation}\n\\]\nNel precedente sistema di equazioni lineari,\nIn conclusione, secondo il modello multifattoriale, le variabili manifeste \\(Y_i\\), con \\(i=1, \\dots, p\\), sono il risultato di una combinazione lineare di \\(m &lt; p\\) fattori inosservabili ad esse comuni \\(\\xi_j\\), con \\(j=1, \\dots, m\\), e di \\(p\\) fattori specifici \\(\\delta_i\\), con \\(i=1, \\dots, p\\), anch’essi inosservabili e di natura residua.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Il modello multifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/04_analisi_fattoriale_3.html#fattori-ortogonali",
    "href": "chapters/fa/04_analisi_fattoriale_3.html#fattori-ortogonali",
    "title": "30  Il modello multifattoriale",
    "section": "",
    "text": "\\(\\xi_j\\), con \\(j=1, \\dots, m\\), rappresenta la \\(j\\)-esima variabile inosservabile a fattore comune (ossia il \\(j\\)-esimo fattore comune a tutte le variabili \\(Y_i\\));\n\n\\(\\lambda_{ij}\\) rappresenta il parametro, detto saturazione o peso fattoriale, che riflette l’importanza del \\(j\\)-esimo fattore comune nella composizione della \\(i\\)-esima variabile osservabile;\n\n\\(\\delta_i\\) rappresenta il fattore specifico (o unico) di ogni variabile manifesta \\(Y_i\\).\n\n\n\n30.1.1 Assunzioni del modello multifattoriale\nLe variabili inosservabili a fattore comune \\(\\xi_j\\), con \\(j=1, \\dots, m\\), in quanto latenti, non possiedono unità di misura. Pertanto, per semplicità si assume che abbiano media zero, \\(\\mathbb{E}(\\xi_j)=0\\), abbiano varianza unitaria, \\(\\mathbb{V} (\\xi_j)= \\mathbb{E}(\\xi_j^2) - [\\mathbb{E}(\\xi_j)]^2=1\\), e siano incorrelate tra loro, \\(Cov(\\xi_j, \\xi_h)=0\\), con \\(j, h = 1, \\dots, m; \\;j \\neq h\\). Si assume inoltre che le variabili a fattore specifico \\(\\delta_i\\) siano tra loro incorrelate, \\(Cov(\\delta_i,\\delta_k)=0\\), con \\(i, k = 1, \\dots, p, \\; i \\neq k\\), abbiano media zero, \\(\\mathbb{E}(\\delta_i)=0\\), e varianza uguale a \\(\\mathbb{V} (\\delta_i) = \\psi_{ii}\\). La varianza \\(\\psi_{ii}\\) è detta varianza specifica o unicità della \\(i\\)-esima variabile manifesta \\(Y_i\\). Si assume infine che i fattori specifici siano linearmente incorrelati con i fattori comuni, ovvero \\(Cov(\\xi_j, \\delta_i)=0\\) per ogni \\(j=1, \\dots, m\\) e per ogni \\(i=1\\dots,p\\).\n\n30.1.2 Interpretazione dei parametri del modello\n\n30.1.2.1 Covarianza tra variabili e fattori\nNell’ipotesi che le variabili \\(Y_i\\) abbiano media nulla, la covarianza tra \\(Y_i\\) e \\(\\xi_j\\) è uguale alla saturazione fattoriale \\(\\lambda_{ij}\\):\n\\[\n\\begin{equation}\n\\begin{aligned}\n  Cov(Y_i, \\xi_j) &= \\mathbb{E}(Y_i \\xi_j)\\notag\\\\\n  &=\\mathbb{E}\\left[(\\lambda_{i1} \\xi_1 + \\dots + \\lambda_{im} \\xi_m + \\delta_i)\\xi_j \\right]\\notag\\\\\n  &= \\lambda_{i1}\\underbrace{\\mathbb{E}(\\xi_1\\xi_j)}_{=0} + \\dots +\n\\lambda_{ij}\\underbrace{\\mathbb{E}(\\xi_j^2)}_{=1} + \\dots \\notag\\\\\n& \\; + \\lambda_{im}\\underbrace{\\mathbb{E}(\\xi_m\\xi_j)}_{=0} +\n  \\underbrace{\\mathbb{E}(\\delta_i \\xi_j)}_{=0}\\notag\\\\\n  &= \\lambda_{ij}.\\notag\n\\end{aligned}\n\\end{equation}\n\\]\nAnche nel modello multifattoriale, dunque, le saturazioni fattoriali rappresentano le covarianze tra le variabili e i fattori:\n\\[\nCov(Y_i, \\xi_j) = \\lambda_{ij} \\qquad i=1, \\dots, p; \\quad j= 1, \\dots, m.\n\\]\nNaturalmente, se le variabili sono standardizzate, le saturazioni fattoriali diventano correlazioni:\n\\[\nr_{ij} = \\lambda_{ij}.\n\\]\n\n30.1.2.2 Espressione fattoriale della varianza\nCome nel modello monofattoriale, la varianza delle variabili manifeste si decompone in una componente dovuta ai fattori comuni, chiamata comunalità, e in una componente specifica alle \\(Y_i\\), chiamata unicità. Nell’ipotesi che le variabili \\(Y_i\\) abbiano media nulla, la varianza di \\(Y_i\\) è uguale a\n\\[\n\\begin{aligned}\n  \\mathbb{V} (Y_i)\n  &=\\mathbb{E}\\left[ (\\lambda_{i1} \\xi_1 + \\dots +\n    \\lambda_{im} \\xi_m + \\delta_i)^2 \\right].\n\\end{aligned}\n\\tag{30.1}\\]\nCome si sviluppa il polinomio precedente? Il quadrato di un polinomio è uguale alla somma dei quadrati di tutti i termini più il doppio prodotto di ogni termine per ciascuno di quelli che lo seguono. Il valore atteso del quadrato del primo termine è uguale a \\(\\lambda_{i1}^2\\mathbb{E}(\\xi_1^2)\\) ma, essendo la varianza di \\(\\xi_1\\) uguale a \\(1\\), otteniamo semplicemente \\(\\lambda_{i1}^2\\). Lo stesso vale per i quadrati di tutti i termini seguenti tranne l’ultimo. Infatti, \\(\\mathbb{E}(\\delta_i^2)=\\psi_{ii}\\). Per quel che riguarda i doppi prodotti, sono tutti nulli. In primo luogo perché, nel caso di fattori ortogonali, la covarianza tra i fattori comuni è nulla, \\(\\mathbb{E}(\\xi_j \\xi_h)=0\\), con \\(j \\neq h\\). In secondo luogo perché il fattori comuni cono incorrelati con i fattori specifici, quindi \\(\\mathbb{E}(\\delta_i \\xi_j)=0\\).\nIn conclusione,\n\\[\n\\begin{aligned}\n  \\mathbb{V}(Y_i) &= \\lambda_{i1}^2 + \\lambda_{i2}^2 + \\dots + \\lambda_{im}^2 + \\psi_{ii} \\notag\\\\\n  &= \\sum_{j=1}^m \\lambda_{ij}^2 + \\psi_{ii}\\notag\\\\\n  &= h_i^2 + \\psi_{ii}\\notag\\\\\n  &=\\text{communalità} + \\text{unicità},\\notag\n\\end{aligned}\n\\]\nla varianza della variabile manifesta \\(Y_i\\) è suddivisa in due parti: il primo addendo è definito comunalità poiché rappresenta la parte di variabilità della \\(Y_i\\) spiegata dai fattori comuni; il secondo addendo è invece definito varianza specifica (o unicità) poiché esprime la parte di variabilità della \\(Y_i\\) non spiegata dai fattori comuni.\n\n30.1.2.3 Espressione fattoriale della covarianza\nQuale esempio, consideriamo il caso di \\(p=5\\) variabili osservabili e \\(m=2\\) fattori ortogonali. Se le variabili manifeste sono ‘centrate’ (ovvero, se a ciascuna di esse sottraiamo la rispettiva media), allora il modello multifattoriale diventa\n\\[\n\\begin{aligned}\n  Y_1 &= \\lambda_{11} \\xi_1 + \\lambda_{12} \\xi_2 + \\delta_1,\\notag\\\\\n  Y_2 &= \\lambda_{21} \\xi_1 + \\lambda_{22} \\xi_2 + \\delta_2,\\notag\\\\\n  Y_3 &= \\lambda_{31} \\xi_1 + \\lambda_{32} \\xi_2 + \\delta_3,\\notag\\\\\n  Y_4 &= \\lambda_{41} \\xi_1 + \\lambda_{42} \\xi_2 + \\delta_4,\\notag\\\\\n  Y_5 &= \\lambda_{51} \\xi_1 + \\lambda_{52} \\xi_2 + \\delta_5.\\notag\n\\end{aligned}\n\\]\nNell’ipotesi che le variabili \\(Y_i\\) abbiano media nulla, la covarianza tra \\(Y_1\\) e \\(Y_2\\), ad esempio, è uguale a:\n\\[\n\\begin{aligned}\n  Cov(Y_1, Y_2) &= \\mathbb{E}\\left( Y_1 Y_2\\right) \\notag\\\\\n  &= \\mathbb{E}\\left[\n  (\\lambda_{11} \\xi_1 + \\lambda_{12} \\xi_2 + \\delta_1)\n   (\\lambda_{21} \\xi_1 + \\lambda_{22} \\xi_2 +  \\delta_2)\n  \\right]\\notag\\\\\n  &= \\lambda_{11} \\lambda_{21} \\mathbb{E}(\\xi_1^2) +\n      \\lambda_{11} \\lambda_{22} \\mathbb{E}(\\xi_1 \\xi_2) +\\notag\n      \\lambda_{11} \\mathbb{E}(\\xi_1 \\delta_2) +\\notag\\\\\n    &\\quad \\lambda_{12} \\lambda_{21}\\mathbb{E}(\\xi_1 \\xi_2)\\, +\n      \\lambda_{12} \\lambda_{22}\\mathbb{E}(\\xi^2_2)\\, +\n      \\lambda_{12} \\mathbb{E}(\\xi_2\\delta_2) +\\notag\\\\\n    &\\quad \\lambda_{21} \\mathbb{E}(\\xi_1\\delta_1) +\\notag\n     \\lambda_{22} \\mathbb{E}(\\xi_2\\delta_1) + \\mathbb{E}(\\delta_1 \\delta_2)\\notag\\\\\n   &= \\lambda_{11} \\lambda_{21} + \\lambda_{12} \\lambda_{22}.\\notag\n\\end{aligned}\n\\]\nIn conclusione, la covarianza tra le variabili manifeste \\(Y_l\\) e \\(Y_m\\) riprodotta dal modello è data dalla somma dei prodotti delle saturazioni \\(\\lambda_l \\lambda_m\\) nei due fattori.\nEsempio. Consideriamo i dati riportati da Brown (2015), ovvero otto misure di personalità raccolte su un campione di 250 pazienti che hanno concluso un programma di psicoterapia. Le scale sono le seguenti:\n\nanxiety (N1),\nhostility (N2),\ndepression (N3),\nself-consciousness (N4),\nwarmth (E1),\ngregariousness (E2),\nassertiveness (E3),\npositive emotions (E4).\n\n\nvarnames &lt;- c(\"N1\", \"N2\", \"N3\", \"N4\", \"E1\", \"E2\", \"E3\", \"E4\")\nsds &lt;- '5.7  5.6  6.4  5.7  6.0  6.2  5.7  5.6'\n\ncors &lt;- '\n    1.000\n    0.767  1.000 \n    0.731  0.709  1.000 \n    0.778  0.738  0.762  1.000 \n    -0.351  -0.302  -0.356  -0.318  1.000 \n    -0.316  -0.280  -0.300  -0.267  0.675  1.000 \n    -0.296  -0.289  -0.297  -0.296  0.634  0.651  1.000 \n    -0.282  -0.254  -0.292  -0.245  0.534  0.593  0.566  1.000\n'\n\npsychot_cor_mat &lt;- getCov(cors, names = varnames)\nn &lt;- 250\n\nEseguiamo l’analisi fattoriale esplorativa con il metodo della massima verosimiglianza ipotizzando due fattori comuni incorrelati:\n\nn_facs &lt;- 2\nfit_efa &lt;- factanal(\n  covmat = psychot_cor_mat,\n  factors = n_facs,\n  rotation = \"varimax\",\n  n.obs = n\n)\n\nEsaminiamo le saturazioni fattoriali:\n\nlambda &lt;- fit_efa$loadings\nlambda\n#&gt; \n#&gt; Loadings:\n#&gt;    Factor1 Factor2\n#&gt; N1  0.854  -0.228 \n#&gt; N2  0.826  -0.194 \n#&gt; N3  0.811  -0.233 \n#&gt; N4  0.865  -0.186 \n#&gt; E1 -0.202   0.773 \n#&gt; E2 -0.139   0.829 \n#&gt; E3 -0.158   0.771 \n#&gt; E4 -0.147   0.684 \n#&gt; \n#&gt;                Factor1 Factor2\n#&gt; SS loadings      2.923   2.526\n#&gt; Proportion Var   0.365   0.316\n#&gt; Cumulative Var   0.365   0.681\n\nLa soluzione fattoriale conferma la presenza di due fattori: il primo fattore satura sulle scale di neutoricismo, il secono sulle scale di estroversione.\nLa correlazione riprodotta \\(r_{12}\\) è uguale a \\(\\lambda_{11}\\lambda_{21} + \\lambda_{12}\\lambda_{22}\\)\n\nlambda[1, 1] * lambda[2, 1] + lambda[1, 2] * lambda[2, 2]\n#&gt; [1] 0.749\n\ne corrisponde da vicino alla correlazione osservata 0.767.\nL’intera matrice di correlazioni riprodotte è \\(\\boldsymbol{\\Lambda} \\boldsymbol{\\Lambda}^{\\mathsf{T}} + \\boldsymbol{\\psi}\\):\n\nRr &lt;- lambda %*% t(lambda) + diag(fit_efa$uniq)\nRr |&gt; \n  round(3)\n#&gt;        N1     N2     N3     N4     E1     E2     E3     E4\n#&gt; N1  1.000  0.749  0.745  0.781 -0.348 -0.307 -0.311 -0.281\n#&gt; N2  0.749  1.000  0.715  0.751 -0.317 -0.276 -0.281 -0.254\n#&gt; N3  0.745  0.715  1.000  0.745 -0.344 -0.306 -0.308 -0.279\n#&gt; N4  0.781  0.751  0.745  1.000 -0.318 -0.274 -0.280 -0.254\n#&gt; E1 -0.348 -0.317 -0.344 -0.318  1.000  0.669  0.628  0.558\n#&gt; E2 -0.307 -0.276 -0.306 -0.274  0.669  1.000  0.661  0.587\n#&gt; E3 -0.311 -0.281 -0.308 -0.280  0.628  0.661  1.000  0.550\n#&gt; E4 -0.281 -0.254 -0.279 -0.254  0.558  0.587  0.550  1.000\n\nLa differenza tra la matrice di correlazioni riprodotte e la matrice di correlazioni osservate è uguale a:\n\n(psychot_cor_mat - Rr) |&gt; \n  round(3)\n#&gt;        N1     N2     N3     N4     E1     E2     E3     E4\n#&gt; N1  0.000  0.018 -0.014 -0.003 -0.003 -0.009  0.015 -0.001\n#&gt; N2  0.018  0.000 -0.006 -0.013  0.015 -0.004 -0.008  0.000\n#&gt; N3 -0.014 -0.006  0.000  0.017 -0.012  0.006  0.011 -0.013\n#&gt; N4 -0.003 -0.013  0.017  0.000  0.000  0.007 -0.016  0.009\n#&gt; E1 -0.003  0.015 -0.012  0.000  0.000  0.006  0.006 -0.024\n#&gt; E2 -0.009 -0.004  0.006  0.007  0.006  0.000 -0.010  0.006\n#&gt; E3  0.015 -0.008  0.011 -0.016  0.006 -0.010  0.000  0.016\n#&gt; E4 -0.001  0.000 -0.013  0.009 -0.024  0.006  0.016  0.000",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Il modello multifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/04_analisi_fattoriale_3.html#fattori-obliqui",
    "href": "chapters/fa/04_analisi_fattoriale_3.html#fattori-obliqui",
    "title": "30  Il modello multifattoriale",
    "section": "\n30.2 Fattori obliqui",
    "text": "30.2 Fattori obliqui\nAnche nel caso di fattori comuni correlati è possibile esprimere nei termini dei parametri del modello la covarianza teorica tra una variabile manifesta \\(Y_i\\) e uno dei fattori comuni, la covarianza teorica tra due variabili manifeste, e la comunalità di ciascuna variabile manifesta. Dato però che i fattori comuni risultano correlati, l’espressione fattoriale di tali quantità è più complessa che nel caso di fattori comuni ortogonali.\n\n30.2.1 Covarianza teorica tra variabili e fattori\nIn base al modello multifattoriale con \\(m\\) fattori comuni la variabile \\(Y_i\\) è\n\\[\nY_i = \\lambda_{i1} \\xi_1 + \\dots + \\lambda_{im} \\xi_m + \\delta_i.\n(\\#eq:mod-multifact)\n\\]\nPoniamoci il problema di trovare la covarianza teorica tra la variabile manifesta \\(Y_i\\) e il fattore comune \\(\\xi_j\\). Come in precedenza, il problema si riduce a quello di trovare \\(\\mathbb{E}(Y_i \\xi_j)\\). Ne segue che\n\\[\n\\begin{equation}\n\\begin{aligned}\n  Cov(Y_i, \\xi_j) &= \\mathbb{E}(Y_i \\xi_j)\\notag\\\\\n  &=\\mathbb{E}\\left[(\\lambda_{i1} \\xi_1 + \\dots + \\lambda_{ij} \\xi_j + \\dots + \\lambda_{im} \\xi_m + \\delta_i)\\xi_j \\right]\\notag\\\\\n  &= \\lambda_{i1}\\underbrace{\\mathbb{E}(\\xi_1\\xi_j)}_{\\neq 0} + \\dots + \\lambda_{ij}\\underbrace{\\mathbb{E}(\\xi_j^2)}_{=1} + \\dots \\notag\\\\\n& \\quad + \\lambda_{im}\\underbrace{\\mathbb{E}(\\xi_m\\xi_j)}_{\\neq 0} + \\underbrace{\\mathbb{E}(\\delta_i \\xi_j)}_{=0}\\notag\\\\\n  &= \\lambda_{ij} + \\lambda_{i1} Cov(\\xi_1, \\xi_j) + \\dots + \\lambda_{im} Cov(\\xi_m, \\xi_j).\n\\end{aligned}\n\\end{equation}\n\\]\nAd esempio, nel caso di tre fattori comuni \\(\\xi_1, \\xi_2, \\xi_3\\), la covarianza tra \\(Y_1\\) e \\(\\xi_{1}\\) diventa\n\\[\n\\lambda_{11} + \\lambda_{12}Cov(\\xi_1, \\xi_2) + \\lambda_{13}Cov(\\xi_1, \\xi_3).\n\\]\n\n30.2.2 Espressione fattoriale della varianza\nPoniamoci ora il problema di trovare la varianza teorica della variabile manifesta \\(Y_i\\). In base al modello fattoriale, la variabile \\(Y_i\\) è specificata come nella @ref(eq:mod-multifact). La varianza di \\(Y_i\\) è \\(\\mathbb{V}(Y_i) = \\mathbb{E}(Y_i^2) -[\\mathbb{E}(Y_i)]^2\\). Però, avendo espresso \\(Y_i\\) nei termini della differenza dalla sua media, l’espressione della varianza si riduce a \\(\\mathbb{V}(Y_i) = \\mathbb{E}(Y_i^2)\\). Dobbiamo dunque sviluppare l’espressione\n\\[\n\\mathbb{E}(Y_i^2) = \\mathbb{E}[(\\lambda_{i1} \\xi_1 + \\dots + \\lambda_{im} \\xi_m + \\delta_i)^2].\n\\]\nIn conclusione, la varianza teorica di \\(Y_i\\) è uguale a\n\\[\n\\begin{equation}\n\\begin{split}\n\\mathbb{V}(Y_i) &= \\lambda_{i1}^2 + \\lambda_{i2}^2 + \\dots + \\lambda_{im}^2  + \\\\\n&\\quad 2 \\lambda_{i1} \\lambda_{i2} Cov(\\xi_1, \\xi_2) + \\dots + 2 \\lambda_{i,m-1} \\lambda_{im} Cov(\\xi_{m-1}, \\xi_m) + \\\\\n&\\quad \\psi_{ii}.\\notag\n\\end{split}\n\\end{equation}\n\\]\nAd esempio, nel caso di tre fattori comuni, \\(\\xi_1, \\xi_2, \\xi_3\\), la varianza di \\(Y_1\\) è\n\\[\n\\begin{equation}\n\\begin{split}\n\\mathbb{V}(Y_1) = &\\lambda_{11}^2 + \\lambda_{12}^2 + \\lambda_{13}^2 +\\\\\n&\\quad 2 \\lambda_{11} \\lambda_{12} Cov(\\xi_1, \\xi_2) + \\\\\n&\\quad 2 \\lambda_{11} \\lambda_{13} Cov(\\xi_1, \\xi_3) + \\\\\n&\\quad 2 \\lambda_{12} \\lambda_{13} Cov(\\xi_2, \\xi_3) + \\\\\n&\\quad \\psi_{11}. \\notag\n\\end{split}\n\\end{equation}\n\\]\n\n30.2.3 Covarianza teorica tra due variabili\nConsideriamo ora il caso più semplice di due soli fattori comuni correlati e calcoliamo la covarianza tra \\(Y_1\\) e \\(Y_2\\):\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathbb{E}(Y_1 Y_2) =\\mathbb{E}[(&\\lambda_{11}\\xi_1 + \\lambda_{12}\\xi_2+\\delta_1) (\\lambda_{21}\\xi_1 + \\lambda_{22}\\xi_2+\\delta_2)]\\notag\\\\\n=\\mathbb{E}(\n&\\lambda_{11}\\lambda_{21}\\xi_1^2 +\n\\lambda_{11}\\lambda_{22}\\xi_1\\xi_2 +\n\\lambda_{11}\\xi_1\\delta_2 +\\notag\\\\\n+&\\lambda_{12}\\lambda_{21}\\xi_1\\xi_2 +\n\\lambda_{12}\\lambda_{22}\\xi_2^2 +\n\\lambda_{12}\\xi_2\\delta_2 +\\notag\\\\\n+&\\lambda_{21}\\xi_1\\delta_1 +\n\\lambda_{22}\\xi_2\\delta_1 +\n\\delta_1\\delta_2).\\notag\n\\end{aligned}\n\\end{equation}\n\\]\nDistribuendo l’operatore di valore atteso, dato che \\(\\mathbb{E}(\\xi^2)=1\\) e \\(\\mathbb{E}(\\xi \\delta)=0\\), otteniamo\n\\[\nCov(Y_1, Y_2) = \\lambda_{11} \\lambda_{21} + \\lambda_{12} \\lambda_{22} +\n\\lambda_{12} \\lambda_{21}Cov(\\xi_1, \\xi_2) +\\lambda_{11} \\lambda_{22}Cov(\\xi_1, \\xi_2).\n\\]\nIn termini matriciali si scrive\n\\[\n\\boldsymbol{\\Sigma} =\\boldsymbol{\\Lambda} \\boldsymbol{\\Phi} \\boldsymbol{\\Lambda}^{\\mathsf{T}} + \\boldsymbol{\\Psi},\n\\]\ndove \\(\\boldsymbol{\\Phi}\\) è la matrice di ordine \\(m \\times m\\) di varianze e covarianze tra i fattori comuni e \\(\\boldsymbol{\\Psi}\\) è una matrice diagonale di ordine \\(p\\) con le unicità delle variabili.\nEsempio. Consideriamo nuovamente i dati esaminati negli esercizi precedenti, ma questa volta il modello consente una correlazione tra i due fattori comuni:\n\nefa_result &lt;- fa(\n    psychot_cor_mat, \n    nfactors = 2, \n    n.obs = n, \n    rotate = \"oblimin\"\n)\nefa_result\n#&gt; Factor Analysis using method =  minres\n#&gt; Call: fa(r = psychot_cor_mat, nfactors = 2, n.obs = n, rotate = \"oblimin\")\n#&gt; Standardized loadings (pattern matrix) based upon correlation matrix\n#&gt;      MR1   MR2   h2   u2 com\n#&gt; N1  0.88 -0.02 0.78 0.22   1\n#&gt; N2  0.85  0.01 0.72 0.28   1\n#&gt; N3  0.83 -0.04 0.71 0.29   1\n#&gt; N4  0.90  0.03 0.78 0.22   1\n#&gt; E1 -0.05  0.77 0.63 0.37   1\n#&gt; E2  0.03  0.86 0.71 0.29   1\n#&gt; E3  0.00  0.79 0.63 0.37   1\n#&gt; E4 -0.01  0.70 0.49 0.51   1\n#&gt; \n#&gt;                        MR1  MR2\n#&gt; SS loadings           3.00 2.45\n#&gt; Proportion Var        0.37 0.31\n#&gt; Cumulative Var        0.37 0.68\n#&gt; Proportion Explained  0.55 0.45\n#&gt; Cumulative Proportion 0.55 1.00\n#&gt; \n#&gt;  With factor correlations of \n#&gt;       MR1   MR2\n#&gt; MR1  1.00 -0.43\n#&gt; MR2 -0.43  1.00\n#&gt; \n#&gt; Mean item complexity =  1\n#&gt; Test of the hypothesis that 2 factors are sufficient.\n#&gt; \n#&gt; df null model =  28  with the objective function =  5.02 with Chi Square =  1231\n#&gt; df of  the model are 13  and the objective function was  0.04 \n#&gt; \n#&gt; The root mean square of the residuals (RMSR) is  0.01 \n#&gt; The df corrected root mean square of the residuals is  0.02 \n#&gt; \n#&gt; The harmonic n.obs is  250 with the empirical chi square  1.73  with prob &lt;  1 \n#&gt; The total n.obs was  250  with Likelihood Chi Square =  9.65  with prob &lt;  0.72 \n#&gt; \n#&gt; Tucker Lewis Index of factoring reliability =  1.01\n#&gt; RMSEA index =  0  and the 90 % confidence intervals are  0 0.047\n#&gt; BIC =  -62.1\n#&gt; Fit based upon off diagonal values = 1\n#&gt; Measures of factor score adequacy             \n#&gt;                                                    MR1  MR2\n#&gt; Correlation of (regression) scores with factors   0.96 0.94\n#&gt; Multiple R square of scores with factors          0.93 0.87\n#&gt; Minimum correlation of possible factor scores     0.85 0.75\n\n\nfa.diagram(efa_result)\n\n\n\n\n\n\n\nEsaminiamo la matrice delle correlazioni residue:\n\nresiduals &lt;- residuals(efa_result)\nresiduals\n#&gt;    N1    N2    N3    N4    E1    E2    E3    E4   \n#&gt; N1  0.22                                          \n#&gt; N2  0.02  0.28                                    \n#&gt; N3 -0.01  0.00  0.29                              \n#&gt; N4  0.00 -0.01  0.02  0.22                        \n#&gt; E1  0.00  0.01 -0.01  0.00  0.37                  \n#&gt; E2 -0.01  0.00  0.01  0.01  0.01  0.29            \n#&gt; E3  0.01 -0.01  0.01 -0.02  0.01 -0.01  0.37      \n#&gt; E4  0.00  0.00 -0.01  0.01 -0.02  0.01  0.01  0.51\n\nEsaminiamo più da vicino la matrice di correlazioni riprodotta dal modello, nel caso di fattori obliqui. Le saturazioni fattoriali sono:\n\n# Estrai i carichi fattoriali (saturazioni fattoriali)\nlambda &lt;- efa_result$loadings\n\n# Converti i carichi in una matrice 8 x 2 (assumendo 2 fattori)\n# e assegna i nomi appropriati alle righe e alle colonne\nlambda &lt;- matrix(lambda[, 1:2], nrow = 8, ncol = 2)\nrownames(lambda) &lt;- c(\"N1\", \"N2\", \"N3\", \"N4\", \"E1\", \"E2\", \"E3\", \"E4\")\ncolnames(lambda) &lt;- c(\"Factor1\", \"Factor2\")\n\n# Stampa la matrice dei carichi\nlambda\n#&gt;     Factor1 Factor2\n#&gt; N1  0.87708 -0.0158\n#&gt; N2  0.85228  0.0113\n#&gt; N3  0.82658 -0.0368\n#&gt; N4  0.89876  0.0312\n#&gt; E1 -0.04859  0.7719\n#&gt; E2  0.03470  0.8557\n#&gt; E3  0.00282  0.7929\n#&gt; E4 -0.00788  0.6955\n\nLa matrice di intercorrelazoni fattoriali è\n\n# Estrai la matrice delle intercorrelazioni fattoriali\nPhi &lt;- efa_result$Phi\n\n# Stampa la matrice delle intercorrelazioni\nPhi\n#&gt;        MR1    MR2\n#&gt; MR1  1.000 -0.431\n#&gt; MR2 -0.431  1.000\n\nLe varianze residue sono:\n\n# Estrai le varianze residue\nPsi &lt;- diag(efa_result$uniquenesses)\n\nPsi |&gt;\n    round(2)\n#&gt;      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n#&gt; [1,] 0.22 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n#&gt; [2,] 0.00 0.28 0.00 0.00 0.00 0.00 0.00 0.00\n#&gt; [3,] 0.00 0.00 0.29 0.00 0.00 0.00 0.00 0.00\n#&gt; [4,] 0.00 0.00 0.00 0.22 0.00 0.00 0.00 0.00\n#&gt; [5,] 0.00 0.00 0.00 0.00 0.37 0.00 0.00 0.00\n#&gt; [6,] 0.00 0.00 0.00 0.00 0.00 0.29 0.00 0.00\n#&gt; [7,] 0.00 0.00 0.00 0.00 0.00 0.00 0.37 0.00\n#&gt; [8,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.51\n\nMediante i parametri del modello la matrice di correlazione si riproduce nel modo seguente:\n\\[\n\\boldsymbol{\\Sigma} =\\boldsymbol{\\Lambda} \\boldsymbol{\\Phi} \\boldsymbol{\\Lambda}^{\\mathsf{T}} + \\boldsymbol{\\Psi}.\n\\]\nIn \\(\\textsf{R}\\) scriviamo:\n\nR_hat &lt;- lambda %*% Phi %*% t(lambda) + Psi\nR_hat |&gt; \n  round(2)\n#&gt;       N1    N2    N3    N4    E1    E2    E3    E4\n#&gt; N1  1.00  0.75  0.75  0.78 -0.35 -0.31 -0.31 -0.28\n#&gt; N2  0.75  1.00  0.71  0.75 -0.32 -0.28 -0.28 -0.25\n#&gt; N3  0.75  0.71  1.00  0.74 -0.34 -0.31 -0.31 -0.28\n#&gt; N4  0.78  0.75  0.74  1.00 -0.32 -0.27 -0.28 -0.25\n#&gt; E1 -0.35 -0.32 -0.34 -0.32  1.00  0.67  0.63  0.55\n#&gt; E2 -0.31 -0.28 -0.31 -0.27  0.67  1.00  0.67  0.59\n#&gt; E3 -0.31 -0.28 -0.31 -0.28  0.63  0.67  1.00  0.55\n#&gt; E4 -0.28 -0.25 -0.28 -0.25  0.55  0.59  0.55  1.00\n\nLe correlazioni residue sono:\n\npsychot_cor_mat - R_hat |&gt;\n  round(2)\n#&gt;        N1     N2     N3     N4     E1     E2     E3     E4\n#&gt; N1  0.000  0.017 -0.019 -0.002 -0.001 -0.006  0.014 -0.002\n#&gt; N2  0.017  0.000 -0.001 -0.012  0.018  0.000 -0.009 -0.004\n#&gt; N3 -0.019 -0.001  0.000  0.022 -0.016  0.010  0.013 -0.012\n#&gt; N4 -0.002 -0.012  0.022  0.000  0.002  0.003 -0.016  0.005\n#&gt; E1 -0.001  0.018 -0.016  0.002  0.000  0.005  0.004 -0.016\n#&gt; E2 -0.006  0.000  0.010  0.003  0.005  0.000 -0.019  0.003\n#&gt; E3  0.014 -0.009  0.013 -0.016  0.004 -0.019  0.000  0.016\n#&gt; E4 -0.002 -0.004 -0.012  0.005 -0.016  0.003  0.016  0.000\n\nPer fare un esempio relativo alla correlazione tra due indicatori, calcoliamo la correlazione predetta dal modello tra le variabili \\(Y_1\\) e \\(Y_2\\):\n\nlambda[1, 1] * lambda[2, 1] + lambda[1, 2] * lambda[2, 2] +\n  lambda[1, 1] * lambda[2, 2] * Phi[1, 2] + \n  lambda[1, 2] * lambda[2, 1] * Phi[1, 2] \n#&gt; [1] 0.749\n\nQuesto valore si avvicina al valore contenuto dell’elemento (1, 2) della matrice di correlazioni osservate:\n\npsychot_cor_mat[1, 2]\n#&gt; [1] 0.767\n\nUsando questa procedura possiamo riprodurre tutti gli elementi della matrice di correlazione osservata tramite i parametri stimati dal modello EFA replicando così il risultato che si trova con \\(\\boldsymbol{\\Sigma} =\\boldsymbol{\\Lambda} \\boldsymbol{\\Phi} \\boldsymbol{\\Lambda}^{\\mathsf{T}} + \\boldsymbol{\\Psi}\\).",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Il modello multifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/04_analisi_fattoriale_3.html#session-info",
    "href": "chapters/fa/04_analisi_fattoriale_3.html#session-info",
    "title": "30  Il modello multifattoriale",
    "section": "\n30.3 Session Info",
    "text": "30.3 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] kableExtra_1.4.0  tidySEM_0.2.7     OpenMx_2.21.13    corrplot_0.95    \n#&gt;  [5] ggokabeito_0.1.0  see_0.10.0        MASS_7.3-65       viridis_0.6.5    \n#&gt;  [9] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n#&gt; [13] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n#&gt; [17] lavaan_0.6-19     psych_2.4.12      scales_1.3.0      markdown_1.13    \n#&gt; [21] knitr_1.49        lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n#&gt; [25] dplyr_1.1.4       purrr_1.0.4       readr_2.1.5       tidyr_1.3.1      \n#&gt; [29] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] splines_4.4.2         later_1.4.1           XML_3.99-0.18        \n#&gt;   [4] rpart_4.1.24          fastDummies_1.7.5     lifecycle_1.0.4      \n#&gt;   [7] Rdpack_2.6.2          rstatix_0.7.2         rprojroot_2.0.4      \n#&gt;  [10] StanHeaders_2.32.10   globals_0.16.3        lattice_0.22-6       \n#&gt;  [13] rockchalk_1.8.157     backports_1.5.0       magrittr_2.0.3       \n#&gt;  [16] openxlsx_4.2.8        Hmisc_5.2-2           rmarkdown_2.29       \n#&gt;  [19] httpuv_1.6.15         tmvnsim_1.0-2         qgraph_1.9.8         \n#&gt;  [22] zip_2.3.2             pkgbuild_1.4.6        pbapply_1.7-2        \n#&gt;  [25] minqa_1.2.8           multcomp_1.4-28       abind_1.4-8          \n#&gt;  [28] quadprog_1.5-8        nnet_7.3-20           TH.data_1.1-3        \n#&gt;  [31] sandwich_3.1-1        inline_0.3.21         listenv_0.9.1        \n#&gt;  [34] arm_1.14-4            proto_1.0.0           parallelly_1.42.0    \n#&gt;  [37] texreg_1.39.4         svglite_2.1.3         codetools_0.2-20     \n#&gt;  [40] xml2_1.3.7            tidyselect_1.2.1      farver_2.1.2         \n#&gt;  [43] lme4_1.1-36           matrixStats_1.5.0     stats4_4.4.2         \n#&gt;  [46] base64enc_0.1-3       jsonlite_1.9.0        progressr_0.15.1     \n#&gt;  [49] Formula_1.2-5         survival_3.8-3        emmeans_1.10.7       \n#&gt;  [52] systemfonts_1.2.1     dbscan_1.2.2          tools_4.4.2          \n#&gt;  [55] Rcpp_1.0.14           glue_1.8.0            mnormt_2.1.1         \n#&gt;  [58] xfun_0.51             MplusAutomation_1.1.1 loo_2.8.0            \n#&gt;  [61] withr_3.0.2           fastmap_1.2.0         boot_1.3-31          \n#&gt;  [64] digest_0.6.37         mi_1.1                timechange_0.3.0     \n#&gt;  [67] R6_2.6.1              mime_0.12             estimability_1.5.1   \n#&gt;  [70] colorspace_2.1-1      gtools_3.9.5          jpeg_0.1-10          \n#&gt;  [73] generics_0.1.3        data.table_1.17.0     corpcor_1.6.10       \n#&gt;  [76] httr_1.4.7            htmlwidgets_1.6.4     pkgconfig_2.0.3      \n#&gt;  [79] sem_3.1-16            gtable_0.3.6          bain_0.2.11          \n#&gt;  [82] htmltools_0.5.8.1     carData_3.0-5         blavaan_0.5-8        \n#&gt;  [85] png_0.1-8             reformulas_0.4.0      rstudioapi_0.17.1    \n#&gt;  [88] tzdb_0.4.0            reshape2_1.4.4        curl_6.2.1           \n#&gt;  [91] coda_0.19-4.1         checkmate_2.3.2       nlme_3.1-167         \n#&gt;  [94] nloptr_2.1.1          zoo_1.8-13            parallel_4.4.2       \n#&gt;  [97] miniUI_0.1.1.1        nonnest2_0.5-8        foreign_0.8-88       \n#&gt; [100] pillar_1.10.1         grid_4.4.2            vctrs_0.6.5          \n#&gt; [103] RANN_2.6.2            promises_1.3.2        car_3.1-3            \n#&gt; [106] xtable_1.8-4          cluster_2.1.8         GPArotation_2024.3-1 \n#&gt; [109] htmlTable_2.4.3       evaluate_1.0.3        pbivnorm_0.6.0       \n#&gt; [112] gsubfn_0.7            mvtnorm_1.3-3         cli_3.6.4            \n#&gt; [115] kutils_1.73           compiler_4.4.2        rlang_1.1.5          \n#&gt; [118] rstantools_2.4.0      future.apply_1.11.3   ggsignif_0.6.4       \n#&gt; [121] fdrtool_1.2.18        plyr_1.8.9            stringi_1.8.4        \n#&gt; [124] rstan_2.32.6          pander_0.6.6          QuickJSR_1.6.0       \n#&gt; [127] munsell_0.5.1         lisrelToR_0.3         CompQuadForm_1.4.3   \n#&gt; [130] V8_6.0.1              pacman_0.5.1          Matrix_1.7-2         \n#&gt; [133] hms_1.1.3             glasso_1.11           future_1.34.0        \n#&gt; [136] shiny_1.10.0          rbibutils_2.3         igraph_2.1.4         \n#&gt; [139] broom_1.0.7           RcppParallel_5.1.10\n\n\n\n\n\nBrown, T. A. (2015). Confirmatory factor analysis for applied research. Guilford publications.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Il modello multifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/05_factor_scores.html",
    "href": "chapters/fa/05_factor_scores.html",
    "title": "31  I punteggi fattoriali",
    "section": "",
    "text": "31.0.1 Esempio di interpretazione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nUno dei momenti più difficili nel processo di sviluppo di un test psicometrico è quello dell’interpretazione dei fattori. La verifica del livello di affidabilità rivela il grado di precisione delle misure ottenute ma non fornisce alcuna informazione sulla natura di ciò che si sta misurando. Non esistono specifiche indicazioni che guidino il lavoro interpretativo. Dipende, perciò, dalla capacità e dall’esperienza del ricercatore cogliere il significato comune delle variabili confluite in un fattore, attenendosi alla realtà delle singole variabili senza fornire interpretazioni fantasiose. È importante rendersi conto che sia la scelta del metodo di estrazione dei fattori, sia il problema del numero dei fattori da estrarre, sia la scelta del metodo con cui effettuare la rotazione, rendono molto arbitraria l’interpretazione della soluzione fattoriale.\nI passaggi teorici necessari per interpretare una matrice fattoriale ruotata possono essere descritti nel modo seguente.\nIl WISC-III (Wechsler Intelligence Scale For Children - III) valuta l’abilità intellettiva di soggetti dai 6 ai 16 anni e 11 mesi. I subtest sono stati selezionati per valutare diverse abilità mentali, che tutte insieme indicano l’abilità intellettiva generale del bambino. Alcuni gli richiedono un ragionamento astratto, altri si focalizzano sulla memoria, altri ancora richiedono certe abilità percettive e così via.\nSi consideri la matrice di correlazione tra i subtest della scala WISC-III riportata dal manuale.\nlower &lt;- '\n1\n.66      1\n.57 .55      1\n.70 .69 .54       1\n.56 .59 .47 .64      1\n.34 .34 .43 .35 .29      1\n.47 .45 .39 .45 .38 .25      1\n.21 .20 .27 .26 .25 .23 .18      1\n.40 .39 .35 .40 .35 .20 .37 .28      1\n.48 .49 .52 .46 .40 .32 .52 .27 .41      1\n.41 .42 .39 .41 .34 .26 .49 .24 .37 .61      1\n.35 .35 .41 .35 .34 .28 .33 .53 .36 .45 .38      1\n.18 .18 .22 .17 .17 .14 .24 .15 .23 .31 .29 .24     1\n'\nwisc_III_cov &lt;- getCov(\n  lower,\n  names = c(\n    \"INFO\", \"SIM\", \"ARITH\", \"VOC\", \"COMP\", \"DIGIT\", \"PICTCOM\",\n    \"CODING\", \"PICTARG\", \"BLOCK\", \"OBJECT\", \"SYMBOL\", \"MAZES\"\n  )\n)\nEseguiamo l’analisi fattoriale con il metodo delle componenti principali e una rotazione Varimax:\nf_pc &lt;- psych::principal(wisc_III_cov, nfactors = 3, rotate = \"varimax\")\nprint(f_pc)\n#&gt; Principal Components Analysis\n#&gt; Call: psych::principal(r = wisc_III_cov, nfactors = 3, rotate = \"varimax\")\n#&gt; Standardized loadings (pattern matrix) based upon correlation matrix\n#&gt;           RC1  RC3  RC2   h2   u2 com\n#&gt; INFO     0.80 0.25 0.09 0.72 0.28 1.2\n#&gt; SIM      0.81 0.25 0.08 0.72 0.28 1.2\n#&gt; ARITH    0.65 0.26 0.28 0.57 0.43 1.7\n#&gt; VOC      0.83 0.19 0.13 0.75 0.25 1.2\n#&gt; COMP     0.75 0.14 0.16 0.60 0.40 1.2\n#&gt; DIGIT    0.45 0.06 0.36 0.34 0.66 2.0\n#&gt; PICTCOM  0.43 0.61 0.02 0.56 0.44 1.8\n#&gt; CODING   0.10 0.09 0.88 0.79 0.21 1.0\n#&gt; PICTARG  0.34 0.45 0.27 0.39 0.61 2.6\n#&gt; BLOCK    0.41 0.66 0.22 0.66 0.34 1.9\n#&gt; OBJECT   0.31 0.71 0.14 0.62 0.38 1.5\n#&gt; SYMBOL   0.23 0.32 0.74 0.70 0.30 1.6\n#&gt; MAZES   -0.06 0.71 0.11 0.51 0.49 1.1\n#&gt; \n#&gt;                        RC1  RC3  RC2\n#&gt; SS loadings           3.80 2.37 1.74\n#&gt; Proportion Var        0.29 0.18 0.13\n#&gt; Cumulative Var        0.29 0.47 0.61\n#&gt; Proportion Explained  0.48 0.30 0.22\n#&gt; Cumulative Proportion 0.48 0.78 1.00\n#&gt; \n#&gt; Mean item complexity =  1.5\n#&gt; Test of the hypothesis that 3 components are sufficient.\n#&gt; \n#&gt; The root mean square of the residuals (RMSR) is  0.07 \n#&gt; \n#&gt; Fit based upon off diagonal values = 0.97\nSi noti che i primi cinque subtest possiedono saturazioni maggiori di \\(0.6\\) sul primo fattore. Dato che questi test sono tutti presentati verbalmente e richiedono delle risposte verbali, tale fattore può essere denominato Comprensione Verbale.\nI subtest “Cifrario” e “Ricerca di simboli” saturano sul secondo fattore. Entrambi i subtest misurano la velocità dei processi di codifica o ricerca. Questo fattore, dunque, può essere denominato Velocità di elaborazione.\nInfine, i subtest “Completamento di figure,” “Disegno con i cubi,” “Riordinamento di storie figurate” e “Labirinti” saturano sul terzo fattore. Tutti questi test condividono una componente geometrica o configurazionale: misurano infatti le abilità necessarie per la manipolazione o la disposizione di immagini, oggetti, blocchi. Questo fattore, dunque, può essere denominato Organizzazione percettiva.\nNel caso di una rotazione ortogonale, la comunalità di ciascuna sottoscala è uguale alla somma delle saturazioni fattoriali al quadrato della sottoscala nei fattori.\nPer le 13 sottoscale del WISC-III abbiamo:\nh2 &lt;- rep(0,13)\nfor (i in 1:13) {\n  h2[i] &lt;- sum(f_pc$loadings[i, ]^2)\n}\nround(h2, 2)\n#&gt;  [1] 0.72 0.72 0.57 0.75 0.60 0.34 0.56 0.79 0.39 0.66 0.62 0.70 0.51\nQuesti risultati replicano quelli riportati nel manuale del test WISC-III.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>I punteggi fattoriali</span>"
    ]
  },
  {
    "objectID": "chapters/fa/05_factor_scores.html#punteggi-fattoriali",
    "href": "chapters/fa/05_factor_scores.html#punteggi-fattoriali",
    "title": "31  I punteggi fattoriali",
    "section": "\n31.1 Punteggi fattoriali",
    "text": "31.1 Punteggi fattoriali\nFino ad ora abbiamo considerato le strategie di costruzione del modello basate sulla stima e sull’interpretazione delle saturazioni fattoriali e delle comunalità. Questo è il primo passo nella costruzione del modello fattoriale. È però possibile compiere un passo ulteriore, ovvero quello della stima dei punteggi fattoriali (factor scores) i quali risultano utili sia per interpretare i risultati dell’analisi fattoriale che per fare diagnostica. I punteggi fattoriali forniscono le previsioni dei livelli dei fattori latenti per ogni rispondente. Esistono vari metodi di stima dei punteggi fattoriali. Tra questi troviamo il metodo di Thomson basato sulla regressione e il metodo di Bartlett basato sulla massima verosimiglianza. Entrambi questi metodi sono implementati nel software .\n\n31.1.1 Stima dei punteggi fattoriali\nSi definiscono punteggi fattoriali i valori assunti dai fattori comuni (inosservabili) in corrispondenza delle osservazioni campionarie. Il metodo di Thomson stima i punteggi fattoriali in base all’approccio della regressione multipla, ovvero, impiegando la matrice delle correlazioni tra le variabili e la matrice di struttura (ovvero, la matrice delle correlazioni delle variabili con i fattori). Per ottenere le stime dei punteggi fattoriali con il metodo di Thomson è necessario specificare nella funzione factanal() l’opzione scores = \"regression\".\n\n31.1.2 Dimostrazione di Thurstone\nPrima di descrivere il metodo della regressione, esaminiamo la dimostrazione che Thurstone (1947) ha fornito per illustrare il significato dei punteggi fattoriali (si veda Loehlin, 1987). L’idea è quella di esaminare la stima dei punteggi fattoriali in una situazione in cui i tali punteggi sono conosciuti, in maniera tale da potere controllare il risultato dell’analisi.\nSi consideri un insieme di 1000 scatole di cui conosciamo le dimensioni \\(x, y, z\\):\n\nset.seed(123)\nn &lt;- 1e3\nx &lt;- rnorm(n, 100, 1.5)\ny &lt;- rnorm(n, 200, 1.5)\nz &lt;- rnorm(n, 300, 1.5)\n\nIl problema è quello di stimare le dimensioni delle scatole disponendo soltanto di una serie di misure indirette, corrotte dal rumore di misura. Thurstone (1947) utilizzò le seguenti trasformazioni delle dimensioni delle scatole (si veda Jennrich, 2007).\n\ns &lt;- 40\ny1 &lt;- rnorm(n, mean(x), s)\ny2 &lt;- rnorm(n, mean(y), s)\ny3 &lt;- rnorm(n, mean(z), s)\ny4 &lt;- x * y + rnorm(n, 0, s)\ny5 &lt;- x * z + rnorm(n, 0, s)\ny6 &lt;- y * z + rnorm(n, 0, s)\ny7 &lt;- x^2 * y + rnorm(n, 0, s)\ny8 &lt;- x * y^2 + rnorm(n, 0, s)\ny9 &lt;- x^2 * z + rnorm(n, 0, s)\ny10 &lt;- x * z^2 + rnorm(n, 0, s)\ny11 &lt;- y^2 * z + rnorm(n, 0, s)\ny12 &lt;- y * z^2 + rnorm(n, 0, s)\ny13 &lt;- y^2 * z + rnorm(n, 0, s)\ny14 &lt;- y * z^2 + rnorm(n, 0, s)\ny15 &lt;- x / y + rnorm(n, 0, s)\ny16 &lt;- y / x + rnorm(n, 0, s)\ny17 &lt;- x / z + rnorm(n, 0, s)\ny18 &lt;- z / x + rnorm(n, 0, s)\ny19 &lt;- y / z + rnorm(n, 0, s)\ny20 &lt;- z / y + rnorm(n, 0, s)\ny21 &lt;- 2 * x + 2*y + rnorm(n, 0, s)\ny22 &lt;- 2 * x + 2*z + rnorm(n, 0, s)\ny23 &lt;- 2 * y + 2*z + rnorm(n, 0, s)\n\nEseguiamo l’analisi fattoriale con una soluzione a tre fattori sui dati così creati.\n\nY &lt;- cbind(\n  y1, y2, y3, y4, y5, y6, y7, y8, y9, \n  y10, y11, y12, y13, y14, y15, y16, \n  y17, y18, y19, y20, y21, y22, y23\n)\n\nfa &lt;- factanal(\n  Y, \n  factors = 3, \n  scores = \"regression\",\n  lower = 0.01\n)\n\nL’opzione scores = \"regression\" richiede il calcolo dei punteggi fattoriali con il metodo della regressione. Nel caso di una rotazione Varimax (default della funzione factanal()), i punteggi fattoriali risultano ovviamente incorrelati:\n\ncor(\n  cbind(fa$scores[, 1], fa$scores[, 2], fa$scores[, 3])\n  ) %&gt;% \n  round(3)\n#&gt;        [,1]  [,2]   [,3]\n#&gt; [1,]  1.000 0.002 -0.001\n#&gt; [2,]  0.002 1.000  0.005\n#&gt; [3,] -0.001 0.005  1.000\n\nGeneriamo ora i diagrammi di dispersione che mettono in relazione le dimensioni originarie delle scatole (\\(x, y, z\\)) con i punteggi fattoriali sui tre fattori. Se l’analisi ha successo, ci aspettiamo un’alta correlazione tra i punteggi fattoriali di ogni fattore e una sola delle dimensioni delle scatole \\(x\\), \\(y\\), \\(z\\).\n\np1 &lt;- tibble(x, fs1 = fa$scores[, 1]) %&gt;% \n  ggplot(aes(x, fs1)) +\n  geom_point(alpha = 0.2)\np2 &lt;- tibble(y, fs1 = fa$scores[, 1]) %&gt;% \n  ggplot(aes(y, fs1)) +\n  geom_point(alpha = 0.2)\np3 &lt;- tibble(z, fs1 = fa$scores[, 1]) %&gt;% \n  ggplot(aes(z, fs1)) +\n  geom_point(alpha = 0.2)\n\np4 &lt;- tibble(x, fs2 = fa$scores[, 2]) %&gt;% \n  ggplot(aes(x, fs2)) +\n  geom_point(alpha = 0.2)\np5 &lt;- tibble(y, fs2 = fa$scores[, 2]) %&gt;% \n  ggplot(aes(y, fs2)) +\n  geom_point(alpha = 0.2)\np6 &lt;- tibble(z, fs2 = fa$scores[, 2]) %&gt;% \n  ggplot(aes(z, fs2)) +\n  geom_point(alpha = 0.2)\n\np7 &lt;- tibble(x, fs3 = fa$scores[, 3]) %&gt;% \n  ggplot(aes(x, fs3)) +\n  geom_point(alpha = 0.2)\np8 &lt;- tibble(y, fs3 = fa$scores[, 3]) %&gt;% \n  ggplot(aes(y, fs3)) +\n  geom_point(alpha = 0.2)\np9 &lt;- tibble(z, fs3 = fa$scores[, 3]) %&gt;% \n  ggplot(aes(z, fs3)) +\n  geom_point(alpha = 0.2)\n\n\n(p1 | p2 | p3) /\n(p4 | p5 | p6) /\n(p7 | p8 | p9) \n\n\n\n\n\n\n\nI risultati riportati nella figura confermano le aspettative.\nIl metodo della regressione pone il problema della stima dei punteggi fattoriali nei termini di una ideale regressione di ogni fattore rispetto a tutte le variabili osservate. Per il fattore \\(j\\)-esimo, si può scrivere la seguente equazione:\n\\[\n\\begin{aligned}\nF_j =& \\beta_{1j}y_1 + \\dots + \\beta_{pm}y_p + \\varepsilon_j\n\\end{aligned}\n\\]\ndove \\(F_j\\) sono i punteggi fattoriali e \\(y\\) sono le variabili osservate standardizzate \\((Y-\\bar{Y})/s\\). In forma matriciale, il modello diventa\n\\[\n\\textbf{F} = \\textbf{y} \\textbf{B} +\n\\boldsymbol{\\varepsilon}\n\\]\nI coefficienti parziali di regressione B sono ignoti. Tuttavia, possono essere calcolati utilizzando i metodi della regressione lineare. Nel modello di regressione, infatti, i coefficienti dei minimi quadrati possono essere calcolati utilizzando due matrici di correlazioni: la matrice \\(\\textbf{R}_{xx}\\) (le correlazioni tra le variabili \\(X\\)) e la matrice \\(\\textbf{R}_{xy}\\) (le correlazioni tra le variabili \\(X\\) e la variabile \\(Y\\):\n\\[\n\\hat{\\textbf{B}} = \\textbf{R}_{xx}^{-1}\\textbf{R}_{xy}\n\\]\nNel caso dell’analisi fattoriale, \\(\\textbf{R}_{xx}\\) corrisponde alla matrice delle correlazioni tra le variabili osservate e \\(\\textbf{R}_{xy}\\) corrisponde alla matrice di struttura (la matrice delle correlazioni tra le variabili osservate e i fattori). Se i fattori sono ortogonali, la matrice di struttura coincide con la matrice dei pesi fattoriali \\(\\hat{\\boldsymbol{\\Lambda}}\\).\nI coefficienti B dell’equazione precedente possono dunque essere trovati nel modo seguente:\n\\[\n\\begin{equation}\n\\hat{\\textbf{B}} = \\textbf{R}_{yy}^{-1}\\textbf{R}_{xf}=\n\\textbf{R}^{-1}\\hat{\\boldsymbol{\\Lambda}}\n\\end{equation}\n\\]\nUna volta stimati i coefficienti \\(\\hat{\\textbf{B}}\\), i punteggi fattoriali si calcolano allo stesso modo dei punteggi teorici del modello di regressione:\n\\[\n\\begin{equation}\n\\hat{\\textbf{F}} = \\textbf{y} \\hat{\\textbf{B}} = \\textbf{y}\n\\textbf{R}^{-1}\\hat{\\boldsymbol{\\Lambda}},\n\\end{equation}\n\\]\ndove \\(\\textbf{y}\\) è la matrice delle variabili osservate standardizzate \\((Y-\\bar{Y})/s\\).\nEsercizio. Si utilizzino i dati dass21.txt che corrispondono alla somministrazione del test DASS-21 a 334 partecipanti. Lo schema di codifica si può trovare seguendo questo link. Ci si focalizzi sulla sottoscala Stress del DASS-21. Si trovino i punteggi fattoriali usando la funzione factanal() e si replichi il risultato seguendo la procedura delineata sopra.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>I punteggi fattoriali</span>"
    ]
  },
  {
    "objectID": "chapters/fa/05_factor_scores.html#session-info",
    "href": "chapters/fa/05_factor_scores.html#session-info",
    "title": "31  I punteggi fattoriali",
    "section": "\n31.2 Session Info",
    "text": "31.2 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] kableExtra_1.4.0  corrplot_0.95     ggokabeito_0.1.0  see_0.10.0       \n#&gt;  [5] MASS_7.3-65       viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n#&gt;  [9] ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1 \n#&gt; [13] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-19     psych_2.4.12     \n#&gt; [17] scales_1.3.0      markdown_1.13     knitr_1.49        lubridate_1.9.4  \n#&gt; [21] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.4      \n#&gt; [25] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1    \n#&gt; [29] tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.0      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] mime_0.12           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [25] Matrix_1.7-2        R6_2.6.1            fastmap_1.2.0      \n#&gt;  [28] rbibutils_2.3       shiny_1.10.0        digest_0.6.37      \n#&gt;  [31] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n#&gt;  [34] rprojroot_2.0.4     Hmisc_5.2-2         labeling_0.4.3     \n#&gt;  [37] timechange_0.3.0    abind_1.4-8         compiler_4.4.2     \n#&gt;  [40] withr_3.0.2         glasso_1.11         htmlTable_2.4.3    \n#&gt;  [43] backports_1.5.0     carData_3.0-5       ggsignif_0.6.4     \n#&gt;  [46] corpcor_1.6.10      gtools_3.9.5        tools_4.4.2        \n#&gt;  [49] pbivnorm_0.6.0      foreign_0.8-88      zip_2.3.2          \n#&gt;  [52] httpuv_1.6.15       nnet_7.3-20         glue_1.8.0         \n#&gt;  [55] quadprog_1.5-8      nlme_3.1-167        promises_1.3.2     \n#&gt;  [58] lisrelToR_0.3       grid_4.4.2          checkmate_2.3.2    \n#&gt;  [61] cluster_2.1.8       reshape2_1.4.4      generics_0.1.3     \n#&gt;  [64] gtable_0.3.6        tzdb_0.4.0          data.table_1.17.0  \n#&gt;  [67] hms_1.1.3           xml2_1.3.7          car_3.1-3          \n#&gt;  [70] sem_3.1-16          pillar_1.10.1       rockchalk_1.8.157  \n#&gt;  [73] later_1.4.1         splines_4.4.2       lattice_0.22-6     \n#&gt;  [76] survival_3.8-3      kutils_1.73         tidyselect_1.2.1   \n#&gt;  [79] miniUI_0.1.1.1      pbapply_1.7-2       reformulas_0.4.0   \n#&gt;  [82] svglite_2.1.3       stats4_4.4.2        xfun_0.51          \n#&gt;  [85] qgraph_1.9.8        arm_1.14-4          stringi_1.8.4      \n#&gt;  [88] yaml_2.3.10         pacman_0.5.1        boot_1.3-31        \n#&gt;  [91] evaluate_1.0.3      codetools_0.2-20    mi_1.1             \n#&gt;  [94] cli_3.6.4           RcppParallel_5.1.10 rpart_4.1.24       \n#&gt;  [97] systemfonts_1.2.1   xtable_1.8-4        Rdpack_2.6.2       \n#&gt; [100] munsell_0.5.1       Rcpp_1.0.14         coda_0.19-4.1      \n#&gt; [103] png_0.1-8           XML_3.99-0.18       parallel_4.4.2     \n#&gt; [106] jpeg_0.1-10         lme4_1.1-36         mvtnorm_1.3-3      \n#&gt; [109] openxlsx_4.2.8      rlang_1.1.5         multcomp_1.4-28    \n#&gt; [112] mnormt_2.1.1\n\n\n\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>I punteggi fattoriali</span>"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html",
    "href": "chapters/fa/06_constraints_on_parms.html",
    "title": "32  Attendibilità e modello fattoriale",
    "section": "",
    "text": "32.1 Teoria classica dei test e analisi fattoriale\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nIn questo capitolo esamineremo il problema relativo alla valutazione dell’affidabilità di uno strumento mediante l’impiego della tecnica dell’analisi fattoriale. Saranno differenziati tre distinti modelli che delineano le connessioni tra gli indicatori e il sottostante fattore latente comune (modelli congenerico, tau-equivalente, parallelo). Saranno presentati altresì tre diversi indici volti a caratterizzare l’affidabilità, intesa come coerenza interna, in accordo con il modello adottato. Tali indici includono l’indice omega di McDonald, l’indice alpha di Cronbach e l’indice rho, derivato dalla formula “profetica” di Spearman-Brown.\nSarà evidente che l’utilizzo dell’indice alpha di Cronbach è giustificato soltanto se particolari condizioni specifiche vengono soddisfatte, circostanza che si verifica piuttosto raramente nei dati empirici. A causa di tale ragione, in linea generale, risulta più opportuno adottare l’indice omega di McDonald quale misura di coerenza interna.\nMcDonald (2013) illustra come la teoria classica dei test possa essere correlata al modello dell’analisi fattoriale. La figura rappresenta, attraverso i termini del modello fattoriale, la relazione che sussiste tra i punteggi \\(Y\\), derivanti dalla somministrazione di un test composto da cinque item, e i punteggi veri.\nEsistono diverse strategie per stimare l’attendibilità in situazioni in cui viene somministrato un unico test. In questo contesto, analizzeremo tre metodologie che possono essere implementate attraverso l’analisi fattoriale: l’\\(\\alpha\\) di Cronbach, l’\\(\\omega\\) di McDonald e il metodo di Spearman-Brown.\nIl coefficiente \\(\\alpha\\) rappresenta il principale indice utilizzato per quantificare l’attendibilità come misura di coerenza interna o omogeneità. Approfondiremo come questo indice rappresenti il limite inferiore dell’attendibilità di un test, a condizione che siano soddisfatte alcune ipotesi. Tuttavia, se queste assunzioni non vengono rispettate, l’\\(\\alpha\\) si rivela un stimatore distorto dell’attendibilità.\nPrima di esaminare le diverse metodologie per stimare l’attendibilità in termini di coerenza interna, è essenziale distinguere tra le tre diverse forme che il modello unifattoriale può assumere. Queste tre forme corrispondono al modello con indicatori congenerici, al modello \\(\\tau\\)-equivalente e al modello parallelo.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html#teoria-classica-dei-test-e-analisi-fattoriale",
    "href": "chapters/fa/06_constraints_on_parms.html#teoria-classica-dei-test-e-analisi-fattoriale",
    "title": "32  Attendibilità e modello fattoriale",
    "section": "",
    "text": "Figura 32.1: Diagramma di percorso del modello monofattoriale.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html#modello-fattoriale-e-ctt",
    "href": "chapters/fa/06_constraints_on_parms.html#modello-fattoriale-e-ctt",
    "title": "32  Attendibilità e modello fattoriale",
    "section": "\n32.2 Modello fattoriale e CTT",
    "text": "32.2 Modello fattoriale e CTT\nConsiderando un insieme di item osservati \\(X_1, X_2, \\dots, X_p\\), con \\(p&gt;2\\), i punteggi ottenuti da questi item sono composti da due elementi distinti: una componente di punteggio vero e una componente di errore.\n\\[\n\\begin{equation}\n\\begin{aligned}\nX_1 &=T_1+E_1,\\notag\\\\\nX_2 &=T_2+E_2,\\notag\\\\\n&\\dots\\notag\\\\\nX_p &=T_p+E_p.\\notag\n\\end{aligned}\n\\end{equation}\n\\]\nIn linea con l’approccio delineato da McDonald (2013), questa decomposizione tra la componente vera e quella di errore può essere formalizzata mediante l’utilizzo dei parametri del modello fattoriale. L’equazione \\(X_i = T_i + E_i\\) può quindi essere riformulata come segue:\n\\[\nX_i = \\lambda_i \\xi + \\delta_i, \\quad{i=1, \\dots, p},\n\\]\nIn questa equazione, \\(X_i\\) rappresenta il punteggio osservato per l’item \\(i\\)-esimo (espresso in termini di scarti dalla media), \\(\\lambda_i\\) è il carico fattoriale associato all’item \\(i\\)-esimo, \\(\\xi\\) costituisce il fattore comune e \\(\\delta_i\\) è la componente residuale del punteggio osservato per l’item \\(i\\)-esimo. Tale formulazione si basa sulle assunzioni del modello monofattoriale. Nello specifico, si ipotizza che \\(\\xi\\) e \\(\\delta_i\\) siano incorrelati per ogni item \\(i\\), e che \\(\\delta_i\\) e \\(\\delta_k\\) siano incorrelati per ogni coppia \\(i \\neq k\\).",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html#classi-di-modelli",
    "href": "chapters/fa/06_constraints_on_parms.html#classi-di-modelli",
    "title": "32  Attendibilità e modello fattoriale",
    "section": "\n32.3 Classi di modelli",
    "text": "32.3 Classi di modelli\nNell’ambito dei modelli monofattoriali, possiamo distinguere tre scenari principali:\n\nModello con indicatori congenerici: Questo modello rappresenta il caso più generale, in cui non vi sono restrizioni imposte sulla struttura degli indicatori. Gli indicatori sono correlati in quanto riflettono un fattore comune, ma possono avere carichi fattoriali diversi e specificità uniche.\nModello con indicatori \\(\\tau\\)-equivalenti: In questo scenario, tutti gli indicatori hanno lo stesso carico fattoriale, il che implica che misurano il fattore comune con la stessa forza. Tuttavia, possono differire per quanto riguarda la loro varianza e specificità.\nModello con indicatori paralleli: Qui, gli indicatori non solo condividono lo stesso carico fattoriale, ma presentano anche identica varianza degli errori. Questo indica una completa equivalenza tra gli indicatori, mostrando una struttura molto più rigida rispetto al modello \\(\\tau\\)-equivalente.\n\nIl modello con indicatori congenerici funge da base più flessibile, mentre i modelli con indicatori \\(\\tau\\)-equivalenti e paralleli introducono vincoli crescenti che specificano relazioni sempre più strette tra gli indicatori.\n\n32.3.1 Indicatori congenerici\nGli indicatori congenerici rappresentano misure di uno stesso costrutto, ma non è necessario che riflettano tale costrutto con la medesima intensità. Nel contesto degli indicatori congenerici all’interno del modello monofattoriale, non vengono introdotte limitazioni né sulle saturazioni fattoriali né sulle specificità:\n\\[\n\\lambda_1\\neq \\lambda_2 \\neq \\dots\\neq \\lambda_p,\n\\]\n\\[\n\\psi_{11}\\neq \\psi_{22} \\neq \\dots\\neq \\psi_{pp}.\n\\]\nIl modello mono-fattoriale con indicatori congenerici è dunque\n\\[\n\\begin{equation}\nX_i = \\lambda_i \\xi + \\delta_i.\n\\end{equation}\n\\tag{32.1}\\]\nDalle assunzioni precedenti possiamo derivare la matrice \\(\\boldsymbol{\\Sigma}\\) riprodotta in base al modello congenerico la quale risulta essere uguale a\n\\[\n\\boldsymbol{\\Sigma}=\\left[\n      \\begin{array}{ c c c c }\n        \\sigma_{11} & \\sigma_{12} & \\dots & \\sigma_{1p}, \\\\\n        \\sigma_{21} & \\sigma_{22} & \\dots & \\sigma_{2p}. \\\\\n        \\vdots & \\vdots & & \\vdots\\\\\n        \\sigma_{p1} & \\sigma_{p2} & \\dots & \\sigma_{pp}\n      \\end{array}\n    \\right].\n\\]\nSi noti come tutte le varianze e tutte le covarianze siano tra loro diverse.\n\n32.3.2 Indicatori tau-equivalenti\nNel caso di indicatori \\(\\tau\\)-equivalenti, si ha che\n\\[\n\\lambda_1=\\lambda_2=\\dots=\\lambda_p=\\lambda,\n\\]\n\\[\n\\psi_{11}\\neq \\psi_{22} \\neq \\dots\\neq \\psi_{pp}.\n\\]\nIl modello monofattoriale con indicatori \\(\\tau\\)-equivalenti diventa dunque\n\\[\n\\begin{equation}\nX_i = \\lambda \\xi + \\delta_i,\n\\end{equation}\n\\tag{32.2}\\]\novvero\n\\[\n\\begin{equation}\nX_i = \\tau + \\delta_i,\n\\end{equation}\n\\tag{32.3}\\]\ndove \\(\\tau=\\lambda \\xi\\) è l’attributo comune scalato nell’unità di misura dell’indicatore. Secondo il modello dell’Equazione 32.3, tutte le \\(p(p-1)\\) covarianze tra gli item del test devono essere uguali, ovvero\n\\[\n\\begin{equation}\n\\sigma_{ik} = \\lambda^2=\\sigma^2_T,\n\\end{equation}\n\\tag{32.4}\\]\nper \\(i\\neq k\\). Gli elementi sulla diagonale principale della matrice di varianze e covarianze saranno invece\n\\[\n\\begin{equation}\n\\sigma_{ii} = \\lambda^2 + \\psi_{ii} =\\sigma^2_T + \\psi_{ii}.\n\\end{equation}\n\\tag{32.5}\\]\nLa matrice \\(\\boldsymbol{\\Sigma}\\) riprodotta in base al modello \\(\\tau\\)-equivalente è dunque uguale a\n\\[\n\\begin{equation}\n\\boldsymbol{\\Sigma}=\\left[\n      \\begin{array}{ c c c c }\n        \\sigma_{T}^2 + \\psi_{11} & \\sigma_{T}^2 & \\dots & \\sigma_{T}^2 \\\\\n        \\sigma_{T}^2 & \\sigma_{T}^2 + \\psi_{22} & \\dots & \\sigma_{T}^2 \\\\\n        \\vdots & \\vdots & & \\vdots\\\\\n        \\sigma_{T}^2 & \\sigma_{T}^2 & \\dots & \\sigma_{T}^2 + \\psi_{pp}\n      \\end{array}\n    \\right].\n\\end{equation}\n\\tag{32.6}\\]\nTutte le covarianze sono uguali, mentre le varianze sono tra loro diverse.\n\n32.3.3 Indicatori paralleli\nNel caso di indicatori paralleli si ha che\n\\[\n\\lambda_1=\\lambda_2=\\dots=\\lambda_p=\\lambda,\n\\]\n\\[\n\\psi_{11}=\\psi_{22}=\\dots=\\psi_{pp}=\\psi.\n\\]\nIl modello costituito da indicatori paralleli impone dunque un’ulteriore restrizione che riguarda le varianze degli item, ovvero:\n\\[\n\\sigma_{ii} = \\lambda^2 + \\psi =\\sigma^2_T + \\sigma^2.\n\\]\nLa struttura di varianze e covarianze imposta dal modello per indicatori paralleli è dunque tale da richiedere l’uguaglianza tra tutte le covarianze tra gli item e l’uguaglianza tra tutte le varianze degli item. La matrice \\(\\boldsymbol{\\Sigma}\\) riprodotta in base al modello con indicatori paralleli è dunque uguale a\n\\[\n\\boldsymbol{\\Sigma}=\\left[\n      \\begin{array}{ c c c c }\n        \\sigma_{T}^2 + \\sigma^2 & \\sigma_{T}^2 & \\dots & \\sigma_{T}^2 \\\\\n        \\sigma_{T}^2 & \\sigma_{T}^2 + \\sigma^2 & \\dots & \\sigma_{T}^2 \\\\\n        \\vdots & \\vdots & & \\vdots\\\\\n        \\sigma_{T}^2 & \\sigma_{T}^2 & \\dots & \\sigma_{T}^2 +\\sigma^2 \\notag\n      \\end{array}\n    \\right].\n\\]",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html#metodo-dei-minimi-quadrati-non-pesati",
    "href": "chapters/fa/06_constraints_on_parms.html#metodo-dei-minimi-quadrati-non-pesati",
    "title": "32  Attendibilità e modello fattoriale",
    "section": "\n32.4 Metodo dei minimi quadrati non pesati",
    "text": "32.4 Metodo dei minimi quadrati non pesati\nNel contesto del modello unifattoriale, la varianza di ciascun indicatore è decomposta in due componenti: la componente \\(\\sigma^2_T\\), attribuibile all’effetto del fattore latente comune, e la componente \\(\\psi\\), riferita all’influenza del fattore specifico. McDonald (2013) dimostra come sia possibile ottenere stime di tali componenti dai dati osservati. Queste stime vengono successivamente impiegate per calcolare l’affidabilità interna del test mediante le formule degli indici \\(\\alpha\\) di Cronbach e \\(\\omega\\) di McDonald.\nIn precedenza, abbiamo esaminato come la varianza del punteggio vero possa essere equivalente alla covarianza tra due forme parallele dello stesso test: \\(\\sigma^2_T = \\sigma_{XX^\\prime}\\). Nel caso di indicatori \\(\\tau\\)-equivalenti, la matrice \\(\\boldsymbol{\\Sigma}\\) prevista dal modello risulta essere:\n\\[\n\\boldsymbol{\\Sigma}=\\left[\n      \\begin{array}{ c c c c }\n        \\sigma_{T}^2 + \\psi_{11} & \\sigma_{T}^2 & \\dots & \\sigma_{T}^2 \\\\\n        \\sigma_{T}^2 & \\sigma_{T}^2 + \\psi_{22} & \\dots & \\sigma_{T}^2 \\\\\n        \\vdots & \\vdots & & \\vdots\\\\\n        \\sigma_{T}^2 & \\sigma_{T}^2 & \\dots & \\sigma_{T}^2 + \\psi_{pp} \\notag\n      \\end{array}\n    \\right],\n\\]\nossia, tutte le covarianze sono equivalenti tra loro. Nel caso degli indicatori \\(\\tau\\)-equivalenti, dunque, una stima \\(\\hat{\\sigma}^2_T\\) di \\(\\sigma^2_T\\) si ottiene calcolando la media delle covarianze della matrice S:\n\\[\n\\begin{equation}\n\\hat{\\sigma}_T^2 = \\frac{1}{p(p-1)} {\\sum \\sum}_{i \\neq k} s_{ik}.\n\\end{equation}\n\\tag{32.7}\\]\nQuesto metodo di stima di \\(\\sigma^2_T\\) è noto come “metodo dei minimi quadrati non pesati” McDonald (2013).\nInoltre, nel caso di indicatori \\(\\tau\\)-equivalenti, la stima di \\(\\psi_{ii}\\) nell’Equazione 32.5 è calcolata come:\n\\[\n\\hat{\\psi}_{ii }= s_{ii} - \\hat{\\sigma}_T^2,\n\\]\nper ogni item \\(i\\).\nPer quanto riguarda gli indicatori paralleli, la stima di \\(\\sigma^2_T\\) è ancora basata sull’Equazione 32.7, ovvero sulla media delle covarianze della matrice \\(\\boldsymbol{\\Sigma}\\). Tuttavia, la stima del valore costante \\(\\psi\\) è ottenuta tramite l’equazione:\n\\[\n\\begin{equation}\n\\hat{\\psi} = \\frac{1}{p} \\sum_i (s_{ii} - \\hat{\\sigma}_T^2)\n\\end{equation}\n\\tag{32.8}\\]",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html#varianza-del-punteggio-totale-di-un-test",
    "href": "chapters/fa/06_constraints_on_parms.html#varianza-del-punteggio-totale-di-un-test",
    "title": "32  Attendibilità e modello fattoriale",
    "section": "\n32.5 Varianza del punteggio totale di un test",
    "text": "32.5 Varianza del punteggio totale di un test\nConsideriamo un test omogeneo costituito da \\(p\\) item, il cui punteggio totale \\(Y\\) è dato dalla somma dei punteggi individuali degli item, espressi come \\(Y = \\sum_{i=1}^p X_i\\). Analizziamo la varianza di \\(Y\\) utilizzando un modello unifattoriale.\nIn un modello congenerico con un singolo fattore comune, il punteggio di ciascun item \\(i\\), \\(X_i\\), può essere rappresentato dalla seguente equazione:\n\\[\nX_i = \\lambda_i \\xi + \\delta_i,\n\\]\ndove \\(\\lambda_i\\) rappresenta la carica fattoriale dell’item \\(i\\) sul fattore comune \\(\\xi\\), e \\(\\delta_i\\) è l’errore specifico associato all’item. Questa formulazione è analoga all’equazione \\(X_i = T_i + E_i\\) della teoria classica dei test, dove \\(T_i\\) è il vero punteggio e \\(E_i\\) l’errore di misurazione.\nIl punteggio totale, essendo la somma di tutti gli item, si esprime come \\(\\sum_i (\\lambda_i \\xi + \\delta_i)\\). La varianza del punteggio totale può quindi essere calcolata come segue:\n\\[\n\\begin{equation}\n\\begin{aligned}\n  \\mathbb{V}(Y) &= \\mathbb{V}\\left[ \\sum_i  (\\lambda_i \\xi + \\delta_i)  \\right] \\\\\n  &= \\mathbb{V}\\left[ \\left( \\sum_i \\lambda_i\\right) \\xi + \\sum_i \\delta_i\\right] \\\\\n  &=  \\left(\\sum_i \\lambda_i\\right)^2 \\mathbb{V}(\\xi) +  \\sum_i  \\mathbb{V}(\\delta_i) \\\\\n  &= \\left(\\sum_i \\lambda_i\\right)^2 + \\sum_i \\psi_{ii},\n\\end{aligned}\n\\end{equation}\n\\tag{32.9}\\]\ndove \\(\\mathbb{V}(\\xi) = 1\\) per ipotesi. La varianza di \\(Y\\) si decompone in due parti principali: la prima parte, \\((\\sum_i \\lambda_i)^2\\), rappresenta la varianza attribuibile al fattore comune, riflettendo la variazione legata all’attributo misurato dagli item; la seconda parte, \\(\\sum_i \\psi_{ii}\\), corrisponde alla somma delle varianze degli errori specifici di ciascun item, rappresentando la variazione dovuta agli errori di misurazione.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html#stima-dellattendibilità",
    "href": "chapters/fa/06_constraints_on_parms.html#stima-dellattendibilità",
    "title": "32  Attendibilità e modello fattoriale",
    "section": "\n32.6 Stima dell’attendibilità",
    "text": "32.6 Stima dell’attendibilità\n\n32.6.1 Coefficiente Omega\nDopo aver analizzato la varianza del punteggio totale di un test come indicato nella precedente equazione:\n\\[\n\\mathbb{V}(Y) = \\left( \\sum_i \\lambda_i\\right)^2 + \\sum_i \\psi_{ii},\n\\]\nsi introduce il coefficiente di affidabilità \\(\\omega\\). McDonald (2013) definisce \\(\\omega\\) come il rapporto tra la varianza attribuibile al fattore comune e la varianza totale del punteggio. Basandosi sui parametri del modello monofattoriale, il coefficiente \\(\\omega\\) può essere formulato come segue:\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\omega &= \\frac{\\left( \\sum_{i=1}^p \\lambda_i \\right)^2}{\\mathbb{V}(Y)} \\\\\n&= \\frac{\\left( \\sum_{i=1}^p \\lambda_i \\right)^2}{\\left( \\sum_{i=1}^p \\lambda_i \\right)^2  + \\sum_{i=1}^p \\psi_{ii}}\n\\end{aligned}\n\\end{equation}\n\\tag{32.10}\\]\nQuesto coefficiente \\(\\omega\\) offre una stima quantitativa dell’affidabilità di un test, basata sui parametri del modello congenerico e utilizzando i dati raccolti da una singola somministrazione del test. La sua utilità risiede nel quantificare quanto della varianza osservata nel punteggio totale è effettivamente spiegata dal fattore comune misurato dal test.\n\n32.6.1.1 Un esempio concreto\nConsideriamo nuovamente la scala Openness del dataframe bfi discussi nel capitolo (ctt-3-notebook?). Leggiamo i dati in R.\n\ndata(bfi, package = \"psych\")\n\nÈ necessario ricodificare due item.\n\nbfi$O2r &lt;- 7 - bfi$O2\nbfi$O5r &lt;- 7 - bfi$O5\n\n\ncor(\n    bfi[c(\"O1\", \"O2r\", \"O3\", \"O4\", \"O5r\")], \n    use = \"pairwise.complete.obs\"\n) |&gt;\n    round(2)\n#&gt;       O1  O2r   O3   O4  O5r\n#&gt; O1  1.00 0.21 0.40 0.18 0.24\n#&gt; O2r 0.21 1.00 0.26 0.07 0.32\n#&gt; O3  0.40 0.26 1.00 0.19 0.31\n#&gt; O4  0.18 0.07 0.19 1.00 0.18\n#&gt; O5r 0.24 0.32 0.31 0.18 1.00\n\nEseguiamo l’analisi fattoriale confermativa con lavaan.\n\nmod &lt;- \"\n    f =~ NA*O1 + O2r + O3 + O4 + O5r\n    f ~~ 1*f\n\"\n\nfit &lt;- cfa(mod, data = bfi, std.ov = TRUE, std.lv = TRUE)\n\nEstraiamo le saturazioni fattoriali e le specificità dall’oggetto fit.\n\nlambda &lt;- inspect(fit, what = \"std\")$lambda\npsy &lt;- diag(inspect(fit, what = \"est\")$theta)\n\nCalcoliamo il coefficiente \\(\\omega\\)\n\\[\n\\omega = \\frac{\\left( \\sum_{i=1}^p \\lambda_i \\right)^2}{\\left( \\sum_{i=1}^p \\lambda_i \\right)^2  + \\sum_{i=1}^p \\psi_{ii}}\n\\]\nusando i parametri del modello fattoriale.\n\nsum(lambda)^2 / (sum(lambda)^2 + sum(psy)) \n#&gt; [1] 0.618\n\nRipetiamo i calcoli usando la funzione compRelSEM del pacchetto semTools.\n\nsemTools::compRelSEM(fit, tau.eq = FALSE)\n#&gt;     f \n#&gt; 0.618\n\nIl coefficiente \\(\\omega=0.62\\) può essere interpretato dicendo che il 62% della varianza del punteggio totale \\(Y\\) della sottoscala Openness viene spiegato dal fattore comune latente.\n\n32.6.1.2 Coefficiente \\(\\omega\\) e assunzioni della teoria classica dei test\nIl calcolo del coefficiente \\(\\omega\\) si appoggia su un’assunzione fondamentale della teoria classica dei test: che non esistano covarianze tra gli errori specifici degli item, ossia \\(\\psi_{ik}=0\\) per ogni \\(i \\neq k\\). Tuttavia, questa ipotesi potrebbe non reggere in contesti di dati empirici. Bollen (1980) sottolinea che, qualora le covarianze tra errori specifici non siano trascurabili, l’equazione per \\(\\omega\\) dovrebbe essere modificata come segue:\n\\[\n\\begin{equation}\n\\omega = \\frac{\\left( \\sum_{i=1}^p \\lambda_i \\right)^2}{\\left( \\sum_{i=1}^p \\lambda_i \\right)^2  + \\sum_{i=1}^p \\psi_{ii} + \\sum_{i, k, i\\neq k}^p \\psi_{ik}}.\n\\end{equation}\n\\]\nPer verificare la validità dell’assunzione di indipendenza tra gli errori specifici, si può ricorrere a un’analisi fattoriale confermativa. Se l’analisi rivela correlazioni significative tra molti errori specifici, potrebbe essere necessario incorporare ulteriori fattori nel modello per accomodare queste covarianze. Questo può suggerire una struttura non più unidimensionale, indicando la presenza di diverse sottoscale all’interno del test. Tuttavia, anche con l’identificazione di tali sottoscale, le covarianze tra i fattori specifici possono rimanere inesplicate. In tali casi, l’uso dell’equazione modificata per \\(\\omega\\) diventa indispensabile.\n\n32.6.1.3 Interpretazione del Coefficiente \\(\\omega\\)\n\nMcDonald (2013) propone diverse interpretazioni del coefficiente \\(\\omega\\) che aiutano a comprenderne il significato nel contesto della teoria dei test: - \\(\\omega\\) può essere visto come il quadrato della correlazione tra il punteggio totale \\(Y\\) e il fattore comune \\(\\xi\\), che rappresenta anche la correlazione tra \\(Y\\) e il punteggio vero. Questo si allinea alla definizione classica di affidabilità, espressa come \\(\\rho_{XT}^2 = \\sigma^2_{\\tau}/\\sigma^2_X\\), dove \\(\\sigma^2_{\\tau}\\) è la varianza del punteggio vero e \\(\\sigma^2_X\\) quella del punteggio osservato. - \\(\\omega\\) descrive anche la correlazione tra due applicazioni ipotetiche del test, \\(Y\\) e \\(Y'\\), che condividono le stesse somme (o medie) delle cariche fattoriali e delle varianze specifiche nel contesto di un modello a singolo fattore. - \\(\\omega\\) rappresenta il quadrato della correlazione tra il punteggio totale di \\(p\\) item e il punteggio medio di un insieme infinito di item all’interno di un dominio omogeneo, dove i \\(p\\) item analizzati sono un sottoinsieme rappresentativo.\nIn sintesi, il coefficiente \\(\\omega\\) fornisce una misura di quanto il punteggio totale di un test sia rappresentativo del fattore latente che il test intende misurare. Attraverso la correlazione, l’omogeneità e la consistenza osservata tra diverse somministrazioni o versioni di un test, \\(\\omega\\) aiuta a interpretare la qualità e l’affidabilità del test stesso.\n\n32.6.2 Coefficienti \\(\\alpha\\) e \\(\\omega\\) nel modello \\(\\tau\\)-equivalente\nNel contesto dei modelli monofattoriali, i coefficienti \\(\\omega\\) e \\(\\alpha\\) offrono stime dell’attendibilità, ma in contesti distinti. Il coefficiente \\(\\omega\\) è utile per i modelli con indicatori congenerici, mentre il coefficiente \\(\\alpha\\) è specifico per i modelli con indicatori \\(\\tau\\)-equivalenti.\nIn un modello \\(\\tau\\)-equivalente, dove ciascun item ha la stessa carica fattoriale \\(\\lambda\\), la varianza di ogni item si scompone in una parte dovuta al punteggio vero e una parte d’errore, espressa come \\(\\sigma_{ii} = \\lambda^2 + \\psi_{ii} = \\sigma^2_T + \\sigma^2_i\\). In questo scenario, la formula per il coefficiente \\(\\omega\\) si semplifica nel seguente modo:\n\\[\n\\omega = \\frac{\\left( \\sum_i \\lambda_i \\right)^2}{\\left( \\sum_i \\lambda_i \\right)^2  + \\sum_i \\psi_{ii}} = \\frac{p^2 \\lambda^2}{\\sigma^2_Y} = \\frac{p^2 \\sigma_T^2}{\\sigma_Y^2},\n\\]\ndove \\(Y\\) rappresenta il punteggio totale del test.\nApplicando il metodo dei minimi quadrati non pesati, possiamo derivare la stima seguente per \\(\\omega\\):\n\\[\n\\hat{\\omega} = \\frac{p^2 \\hat{\\sigma}_T^2}{s_Y^2},\n\\]\ndove \\(\\hat{\\sigma}_T^2\\) è stimato come:\n\\[\n\\hat{\\sigma}_T^2 = \\frac{1}{p(p-1)} \\sum \\sum_{i \\neq k} s_{ik}.\n\\]\nIntegrando questa stima nella formula precedente, otteniamo:\n\\[\n\\hat{\\omega} = \\frac{p}{p-1}\\frac{\\sum \\sum_{i \\neq k} s_{ik}}{s_Y^2}.\n\\]\nPer gli indicatori \\(\\tau\\)-equivalenti, quindi, \\(\\omega\\) può essere stimato da:\n\\[\n\\hat{\\omega} = \\frac{p}{p-1}\\left(1-\\frac{\\sum_i s_{ii}}{s_Y^2}\\right).\n\\] (eq-alpha-camp)\nQuesta stima di \\(\\omega\\) ha un parallelo nei valori di popolazione definiti da \\(\\alpha\\), che si esprime come:\n\\[\n\\alpha = \\frac{p}{p-1}\\left(1-\\frac{\\sum_{i=1}^p \\sigma_{ii}}{\\sigma_Y^2}\\right) = \\frac{p}{p-1}\\frac{\\sum_{i \\neq k}^p \\text{Cov}(X_i, X_k)}{\\mathbb{V}(Y)}.\n\\tag{32.11}\\]\nIn condizioni ideali del modello \\(\\tau\\)-equivalente, i valori di \\(\\alpha\\) e \\(\\omega\\) convergono. Tuttavia, \\(\\alpha\\) tende a sottostimare \\(\\omega\\), posizionandosi come un limite inferiore per \\(\\omega\\). Data questa natura conservativa di \\(\\alpha\\), alcuni ricercatori lo preferiscono a \\(\\omega\\), sebbene questa proprietà valga solamente quando le assunzioni del modello \\(\\tau\\)-equivalente sono rigorosamente rispettate.\n\n32.6.2.1 Un esempio concreto\nConsideriamo la matrice di varianze e covarianze della sottoscala Openness.\n\nC &lt;- cov(bfi[c(\"O1\", \"O2r\", \"O3\", \"O4\", \"O5r\")], use = \"pairwise.complete.obs\")\nC |&gt; \n    round(2)\n#&gt;       O1  O2r   O3   O4  O5r\n#&gt; O1  1.28 0.38 0.54 0.25 0.36\n#&gt; O2r 0.38 2.45 0.50 0.13 0.67\n#&gt; O3  0.54 0.50 1.49 0.29 0.50\n#&gt; O4  0.25 0.13 0.29 1.49 0.29\n#&gt; O5r 0.36 0.67 0.50 0.29 1.76\n\nCalcoliamo il coefficiente \\(\\alpha\\) usando l’eq. {eq}eq-alpha-camp:\n\np &lt;- 5\nalpha &lt;- (p / (p - 1)) * (1 - tr(C) / sum(C))\nalpha\n#&gt; [1] 0.6\n\n\n32.6.3 La formula “profetica” di Spearman-Brown\nLa formula profetica di Spearman-Brown è impiegata per calcolare l’affidabilità nei modelli di misurazione che utilizzano indicatori paralleli. Supponiamo di avere un test composto da \\(p\\) item paralleli, in cui ogni item ha la stessa carica fattoriale \\(\\lambda\\) e la stessa varianza dell’errore specifico \\(\\psi\\), ovvero \\(\\lambda_1=\\lambda_2=\\dots=\\lambda_p=\\lambda\\) e \\(\\psi_{11}=\\psi_{22}=\\dots=\\psi_{pp}=\\psi\\).\nLa proporzione di varianza nel punteggio totale del test spiegata dalla variabile latente è quindi:\n\\[\n\\left(\\sum_i \\lambda_i \\right)^2 = (p \\lambda)^2 = p^2 \\lambda^2.\n\\]\nDefinendo l’affidabilità di un singolo item, \\(\\rho_1\\), come\n\\[\n\\rho_1 = \\frac{\\lambda^2}{\\lambda^2 + \\psi},\n\\]\nper \\(p\\) item paralleli, l’affidabilità del test, \\(\\rho_p\\), diventa:\n\\[\n\\begin{equation}\n\\begin{aligned}\n  \\rho_p &= \\frac{p^2 \\lambda^2}{p^2 \\lambda^2 + p \\psi} \\\\\n         &= \\frac{p \\lambda^2}{ p \\lambda^2 + \\psi} \\\\\n         &= \\frac{p \\lambda^2}{(p-1) \\lambda^2 + (\\lambda^2 + \\psi)}.\n\\end{aligned}\n\\end{equation}\n\\]\nSfruttando l’affidabilità di un singolo item \\(\\rho_1\\), possiamo riformulare \\(\\rho_p\\) come:\n\\[\n\\begin{equation}\n\\begin{aligned}\n  \\rho_p &= \\frac{p \\rho_1}{(p-1)\\rho_1 + 1}.\n\\end{aligned}\n\\end{equation}\n\\] (eq-spearman-brown-der)\nQuesta espressione, derivata qui sopra, mostra come l’affidabilità \\(\\rho_p\\) di un test composto da \\(p\\) item paralleli possa essere calcolata a partire dall’affidabilità di un singolo item. Tale formula è nota come “formula di predizione” di Spearman-Brown (Spearman-Brown prophecy formula).\nIn contesti con item paralleli, è importante notare che le misure di affidabilità \\(\\omega\\), \\(\\alpha\\), e \\(\\rho_p\\) risultano equivalenti.\n\n32.6.3.1 Un esempio concreto\nPoniamoci il problema di calcolare l’attendibilità della sottoscala Openness utilizzando la formula di Spearman-Brown. Ipotizziamo dunque che gli item della scala Openness siano paralleli. La matrice di correlazione è:\n\nR &lt;- cor(bfi[c(\"O1\", \"O2r\", \"O3\", \"O4\", \"O5r\")], use = \"pairwise.complete.obs\")\nround(R, 3)\n#&gt;        O1   O2r    O3    O4   O5r\n#&gt; O1  1.000 0.214 0.395 0.178 0.239\n#&gt; O2r 0.214 1.000 0.262 0.068 0.325\n#&gt; O3  0.395 0.262 1.000 0.195 0.311\n#&gt; O4  0.178 0.068 0.195 1.000 0.179\n#&gt; O5r 0.239 0.325 0.311 0.179 1.000\n\nSeguendo McDonald (2013), supponiamo di calcolare l’attendibilità di un singolo item (\\(\\rho_1\\)) come la correlazione media tra gli item:\n\nrr &lt;- NULL\np &lt;- 5\nk &lt;- 1\nfor (i in 1:p) {\n  for (j in 1:p) {\n    if (j != i) {\n      rr[k] &lt;- R[i, j]\n    }\n    k &lt;- k + 1\n  }\n}\nro_1 &lt;- mean(rr, na.rm = TRUE)\nprint(ro_1)\n#&gt; [1] 0.237\n\nApplicando la formula di Spearman-Brown, la stima dell’attendibilità del test diventa pari a\n\n(p * ro_1) / ((p - 1) * ro_1 + 1) |&gt;\n    round(3)\n#&gt; [1] 0.608",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html#commenti-e-considerazioni-conclusive",
    "href": "chapters/fa/06_constraints_on_parms.html#commenti-e-considerazioni-conclusive",
    "title": "32  Attendibilità e modello fattoriale",
    "section": "\n32.7 Commenti e considerazioni conclusive",
    "text": "32.7 Commenti e considerazioni conclusive\nIl coefficiente \\(\\alpha\\) di Cronbach è uno degli indici di affidabilità più diffusi in psicometria. Tuttavia, la sua efficacia dipende strettamente dalla \\(\\tau\\)-equivalenza degli item, che presuppongono un tratto latente unidimensionale. Nella pratica, questa condizione è spesso violata: molti test misurano più di un fattore, e le comunalità degli item non sono uniformi, mettendo in discussione la validità dell’ipotesi di \\(\\tau\\)-equivalenza. Se gli errori sono incorrelati, il coefficiente \\(\\alpha\\) può sottostimare l’affidabilità; se invece gli errori sono correlati, può sovrastimarla.\nData questa limitazione, l’utilizzo del coefficiente \\(\\omega\\) di McDonald è generalmente più consigliabile. Il coefficiente \\(\\omega\\) fornisce una stima più robusta dell’affidabilità in vari contesti, inclusi quelli con assunzioni meno restrittive rispetto alla \\(\\tau\\)-equivalenza. Altri indici come il \\(glb\\) (Greatest Lower Bound), discusso da Ten Berge e Sočan (2004), e l’indice \\(\\beta\\) di Revelle (1979), rappresentano alternative valide al coefficiente \\(\\alpha\\), offrendo diversi vantaggi metodologici a seconda delle specifiche esigenze di misurazione e delle caratteristiche dei dati analizzati.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html#session-info",
    "href": "chapters/fa/06_constraints_on_parms.html#session-info",
    "title": "32  Attendibilità e modello fattoriale",
    "section": "\n32.8 Session Info",
    "text": "32.8 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] modelsummary_2.3.0 ggokabeito_0.1.0   see_0.10.0        \n#&gt;  [4] MASS_7.3-65        viridis_0.6.5      viridisLite_0.4.2 \n#&gt;  [7] ggpubr_0.6.0       ggExtra_0.10.1     gridExtra_2.3     \n#&gt; [10] patchwork_1.3.0    bayesplot_1.11.1   semTools_0.5-6    \n#&gt; [13] semPlot_1.1.6      lavaan_0.6-19      psych_2.4.12      \n#&gt; [16] scales_1.3.0       markdown_1.13      knitr_1.49        \n#&gt; [19] lubridate_1.9.4    forcats_1.0.0      stringr_1.5.1     \n#&gt; [22] dplyr_1.1.4        purrr_1.0.4        readr_2.1.5       \n#&gt; [25] tidyr_1.3.1        tibble_3.2.1       ggplot2_3.5.1     \n#&gt; [28] tidyverse_2.0.0    here_1.0.1        \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.0      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] mime_0.12           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [25] Matrix_1.7-2        R6_2.6.1            fastmap_1.2.0      \n#&gt;  [28] rbibutils_2.3       shiny_1.10.0        digest_0.6.37      \n#&gt;  [31] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n#&gt;  [34] rprojroot_2.0.4     Hmisc_5.2-2         timechange_0.3.0   \n#&gt;  [37] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n#&gt;  [40] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n#&gt;  [43] carData_3.0-5       ggsignif_0.6.4      corpcor_1.6.10     \n#&gt;  [46] gtools_3.9.5        tools_4.4.2         pbivnorm_0.6.0     \n#&gt;  [49] foreign_0.8-88      zip_2.3.2           httpuv_1.6.15      \n#&gt;  [52] nnet_7.3-20         glue_1.8.0          quadprog_1.5-8     \n#&gt;  [55] nlme_3.1-167        promises_1.3.2      lisrelToR_0.3      \n#&gt;  [58] grid_4.4.2          checkmate_2.3.2     cluster_2.1.8      \n#&gt;  [61] reshape2_1.4.4      generics_0.1.3      gtable_0.3.6       \n#&gt;  [64] tzdb_0.4.0          data.table_1.17.0   hms_1.1.3          \n#&gt;  [67] car_3.1-3           tables_0.9.31       sem_3.1-16         \n#&gt;  [70] pillar_1.10.1       rockchalk_1.8.157   later_1.4.1        \n#&gt;  [73] splines_4.4.2       lattice_0.22-6      survival_3.8-3     \n#&gt;  [76] kutils_1.73         tidyselect_1.2.1    miniUI_0.1.1.1     \n#&gt;  [79] pbapply_1.7-2       reformulas_0.4.0    stats4_4.4.2       \n#&gt;  [82] xfun_0.51           qgraph_1.9.8        arm_1.14-4         \n#&gt;  [85] stringi_1.8.4       yaml_2.3.10         pacman_0.5.1       \n#&gt;  [88] boot_1.3-31         evaluate_1.0.3      codetools_0.2-20   \n#&gt;  [91] mi_1.1              cli_3.6.4           RcppParallel_5.1.10\n#&gt;  [94] rpart_4.1.24        xtable_1.8-4        Rdpack_2.6.2       \n#&gt;  [97] munsell_0.5.1       Rcpp_1.0.14         coda_0.19-4.1      \n#&gt; [100] png_0.1-8           XML_3.99-0.18       parallel_4.4.2     \n#&gt; [103] jpeg_0.1-10         lme4_1.1-36         mvtnorm_1.3-3      \n#&gt; [106] openxlsx_4.2.8      rlang_1.1.5         multcomp_1.4-28    \n#&gt; [109] mnormt_2.1.1\n\n\n\n\n\nMcDonald, R. P. (2013). Test theory: A unified treatment. Psychology Press.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/07_total_score.html",
    "href": "chapters/fa/07_total_score.html",
    "title": "33  Punteggio totale e modello fattoriale",
    "section": "",
    "text": "33.1 Punteggio totale e modello fattoriale parallelo\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nIn questo capitolo discute l’uso del punteggio totale del test quale misura del costrutto latente. Questa è una pratica largamente usata, ma solo in parte giustificata. Esamineremo a questo proposito le considerazioni di McNeish e Wolf (2020).\nMcNeish e Wolf (2020) richiamano l’attenzione sul fatto che usare il punteggio totale quale misura di un costrutto è possibile solo quando i dati soddisfano i vincoli di un modello fattoriale parallelo.\nConsideriamo l’esempio seguente, nel quale McNeish e Wolf (2020) esaminano i dati “classici” di Holzinger and Swineford (1939), i quali si riferiscono ai seguenti item:\nLeggiamo i dati in R.\nd &lt;- rio::import(\n  here::here(\"data\", \"1_Factor_Parallel.csv\")\n)\nMcNeish e Wolf (2020) sottolineano il fatto che il punteggio totale\n\\[\n\\text{Punteggio totale} = \\text{Item 1 + Item 2 + Item 3 + Item 4 + Item 5 + Item 6}\n\\]\nrappresenta l’idea che ciasun item fornisca la stessa quantità di informazione relativamente alla misura del costrutto. Ciò può essere specificato da un modello fattoriale nel quale le saturazioni fattoriali degli item sono tutte uguali a 1. Questo corrisponde al modello parallelo che abbiamo discusso in precedenza. In tali circostanze, i punteggi fattoriali del test risultano perfettamente associati al punteggio totale (correlazione uguale a 1). Dunque, se tale modello fattoriale è giustificato dai dati, questo giustifica l’uso del punteggio totale del test quale misura del costrutto.\nÈ facile verificare tali affermazioni. Implementiamo il modello parallelo.\nm_parallel &lt;-\n  \"\n  # all loadings are fixed to one\n  f1 =~ 1*X4 + 1*X5 + 1*X6 + 1*X7 + 1*X8 + 1*X9\n  \n  # all residual variances constrained to same value\n  X4 ~~ theta*X4\n  X5 ~~ theta*X5\n  X6 ~~ theta*X6\n  X7 ~~ theta*X7\n  X8 ~~ theta*X8\n  X9 ~~ theta*X9\n\"\nAdattiamo il modello parallelo ai dati forniti dagli autori.\nfit_parallel &lt;- sem(m_parallel, data=d)\nCalcoliamo il punteggio totale.\nd$ts &lt;- with(\n  d,\n  X4 + X5 + X6 + X7 + X8 + X9\n)\nCalcoliamo i punteggi fattoriali.\nscores &lt;- lavPredict(fit_parallel, method=\"regression\")\nd$scores &lt;- as.numeric(scores)\nUn diagramma a dispersione tra il punteggio totale e i punteggi fattoriali conferma che i due sono perfettamente associati. Quindi, usare il punteggio totale o i punteggi fattoriali è equivalente.\nd |&gt; \n  ggplot(aes(x=ts, y=scores)) + \n  geom_point()\nTuttavia, questa conclusione è valida solo se il modello parallelo è giustificato per i dati. Se esaminiamo l’output di lavaan vediamo che, nel caso presente, questo non è vero.\n# report output with fit measures and standardized estimates\nout = summary(fit_parallel, fit.measures = TRUE, standardized = TRUE)\nprint(out)\n#&gt; lavaan 0.6-19 ended normally after 13 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                         7\n#&gt;   Number of equality constraints                     5\n#&gt; \n#&gt;   Number of observations                           301\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                               325.899\n#&gt;   Degrees of freedom                                19\n#&gt;   P-value (Chi-square)                           0.000\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               568.519\n#&gt;   Degrees of freedom                                15\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.446\n#&gt;   Tucker-Lewis Index (TLI)                       0.562\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -2680.931\n#&gt;   Loglikelihood unrestricted model (H1)      -2517.981\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                5365.862\n#&gt;   Bayesian (BIC)                              5373.276\n#&gt;   Sample-size adjusted Bayesian (SABIC)       5366.933\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.232\n#&gt;   90 Percent confidence interval - lower         0.210\n#&gt;   90 Percent confidence interval - upper         0.254\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.000\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    1.000\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.206\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   f1 =~                                                                 \n#&gt;     X4                1.000                               0.633    0.551\n#&gt;     X5                1.000                               0.633    0.551\n#&gt;     X6                1.000                               0.633    0.551\n#&gt;     X7                1.000                               0.633    0.551\n#&gt;     X8                1.000                               0.633    0.551\n#&gt;     X9                1.000                               0.633    0.551\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .X4      (thet)    0.920    0.034   27.432    0.000    0.920    0.697\n#&gt;    .X5      (thet)    0.920    0.034   27.432    0.000    0.920    0.697\n#&gt;    .X6      (thet)    0.920    0.034   27.432    0.000    0.920    0.697\n#&gt;    .X7      (thet)    0.920    0.034   27.432    0.000    0.920    0.697\n#&gt;    .X8      (thet)    0.920    0.034   27.432    0.000    0.920    0.697\n#&gt;    .X9      (thet)    0.920    0.034   27.432    0.000    0.920    0.697\n#&gt;     f1                0.400    0.045    8.803    0.000    1.000    1.000\nDunque, per questi dati, il punteggio totale può ovviamente essere calcolato. Ma non fornisce una misura adeguata del costrutto. Dunque, il punteggio totale non dovrebbe essere usato nel caso dei dati ottenuti con questo test.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Punteggio totale e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/07_total_score.html#punteggio-totale-e-modello-fattoriale-parallelo",
    "href": "chapters/fa/07_total_score.html#punteggio-totale-e-modello-fattoriale-parallelo",
    "title": "33  Punteggio totale e modello fattoriale",
    "section": "",
    "text": "Paragraph comprehension\nSentence completion\nWord definitions\nSpeeded addition\nSpeeded dot counting\nDiscrimination between curved and straight letters",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Punteggio totale e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/07_total_score.html#punteggio-totale-e-modello-fattoriale-congenerico",
    "href": "chapters/fa/07_total_score.html#punteggio-totale-e-modello-fattoriale-congenerico",
    "title": "33  Punteggio totale e modello fattoriale",
    "section": "\n33.2 Punteggio totale e modello fattoriale congenerico",
    "text": "33.2 Punteggio totale e modello fattoriale congenerico\nGli autori adattano ai dati un modello congenerico.\n\nm_congeneric &lt;- \n'\n  #all loadings are uniquely estimated\n  f1 =~ NA*X4 + X5 + X6 + X7 + X8 + X9\n  #constrain factor variance to 1\n  f1 ~~ 1*f1\n'\n\n\n# Fit above model\nfit_congeneric &lt;- sem(m_congeneric, data=d)\n\n\nparameterEstimates(fit_congeneric, standardized = TRUE) %&gt;%\n  dplyr::filter(op == \"=~\") %&gt;%\n  dplyr::select(\n    \"Latent Factor\" = lhs,\n    Indicator = rhs,\n    B = est,\n    SE = se,\n    Z = z,\n    \"p-value\" = pvalue,\n    Beta = std.all\n  ) %&gt;%\n  knitr::kable(\n    digits = 3, booktabs = TRUE, format = \"markdown\",\n    caption = \"Factor Loadings\"\n  )\n\n\nFactor Loadings\n\nLatent Factor\nIndicator\nB\nSE\nZ\np-value\nBeta\n\n\n\nf1\nX4\n0.963\n0.059\n16.27\n0.000\n0.824\n\n\nf1\nX5\n1.121\n0.067\n16.84\n0.000\n0.846\n\n\nf1\nX6\n0.894\n0.058\n15.45\n0.000\n0.792\n\n\nf1\nX7\n0.195\n0.071\n2.77\n0.006\n0.170\n\n\nf1\nX8\n0.185\n0.063\n2.94\n0.003\n0.180\n\n\nf1\nX9\n0.278\n0.065\n4.25\n0.000\n0.258\n\n\n\n\n\nSi noti che le saturazioni fattoriali sono molto diverse tra loro, suggerendo che il punteggio del costrutto si relaziona in modo diverso con ciascun item e che sarebbe inappropriato stimare il punteggio del costrutto assegnando un peso unitario agli item.\nMcNeish e Wolf (2020) calcolano poi i punteggi fattoriali del modello congenerico.\n\nscores_cong &lt;- lavPredict(fit_congeneric, method=\"regression\")\nd$scores_cong &lt;- as.numeric(scores_cong)\n\nIl grafico seguente mostra la relazione tra i punteggi fattoriali e il punteggio totale.\n\nd |&gt; \n  ggplot(aes(x=ts, y=scores_cong)) + \n  geom_point()\n\n\n\n\n\n\n\nNel caso presente, il coefficiente di determinazione tra punteggio totale e punteggi fattoriali è 0.77.\n\ncor(d$ts, d$scores_cong)^2\n#&gt; [1] 0.766\n\nSecondo gli autori, ciò significa che due persone con un punteggio totale identico potrebbero avere punteggi di modello congenerico potenzialmente diversi perché hanno raggiunto il loro particolare punteggio totale approvando item diversi. Poiché il modello congenerico assegna pesi diversi agli item, ciascun item contribuisce in modo diverso al punteggio fattoriale del modello congenerico, il che non è vero per il punteggio totale.\nSi noti che, per i dati di Holzinger and Swineford (1939), neppure un modello congenerico ad un fattore si dimostra adeguato.\n\nout = summary(fit_congeneric, fit.measures = TRUE, standardized = TRUE)\nprint(out)\n#&gt; lavaan 0.6-19 ended normally after 16 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        12\n#&gt; \n#&gt;   Number of observations                           301\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                               115.366\n#&gt;   Degrees of freedom                                 9\n#&gt;   P-value (Chi-square)                           0.000\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               568.519\n#&gt;   Degrees of freedom                                15\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.808\n#&gt;   Tucker-Lewis Index (TLI)                       0.680\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -2575.664\n#&gt;   Loglikelihood unrestricted model (H1)      -2517.981\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                5175.328\n#&gt;   Bayesian (BIC)                              5219.813\n#&gt;   Sample-size adjusted Bayesian (SABIC)       5181.756\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.198\n#&gt;   90 Percent confidence interval - lower         0.167\n#&gt;   90 Percent confidence interval - upper         0.231\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.000\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    1.000\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.129\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   f1 =~                                                                 \n#&gt;     X4                0.963    0.059   16.274    0.000    0.963    0.824\n#&gt;     X5                1.121    0.067   16.835    0.000    1.121    0.846\n#&gt;     X6                0.894    0.058   15.450    0.000    0.894    0.792\n#&gt;     X7                0.195    0.071    2.767    0.006    0.195    0.170\n#&gt;     X8                0.185    0.063    2.938    0.003    0.185    0.180\n#&gt;     X9                0.278    0.065    4.245    0.000    0.278    0.258\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;     f1                1.000                               1.000    1.000\n#&gt;    .X4                0.437    0.056    7.775    0.000    0.437    0.320\n#&gt;    .X5                0.500    0.071    6.998    0.000    0.500    0.285\n#&gt;    .X6                0.474    0.054    8.777    0.000    0.474    0.372\n#&gt;    .X7                1.278    0.105   12.211    0.000    1.278    0.971\n#&gt;    .X8                1.023    0.084   12.204    0.000    1.023    0.967\n#&gt;    .X9                1.080    0.089   12.132    0.000    1.080    0.933\n\nSe trascuriamo le considerazioni sulla struttura fattoriale e esaminiamo (per esempio) unicamente il coefficiente omega, finiamo per trovare una risposta accettabile, ma sbagliata.\n\npsych::omega(d[, 1:6])\n#&gt; Omega \n#&gt; Call: omegah(m = m, nfactors = nfactors, fm = fm, key = key, flip = flip, \n#&gt;     digits = digits, title = title, sl = sl, labels = labels, \n#&gt;     plot = plot, n.obs = n.obs, rotate = rotate, Phi = Phi, option = option, \n#&gt;     covar = covar)\n#&gt; Alpha:                 0.72 \n#&gt; G.6:                   0.76 \n#&gt; Omega Hierarchical:    0.55 \n#&gt; Omega H asymptotic:    0.65 \n#&gt; Omega Total            0.84 \n#&gt; \n#&gt; Schmid Leiman Factor loadings greater than  0.2 \n#&gt;       g  F1*  F2*   F3*   h2   h2   u2   p2  com\n#&gt; X4 0.73            0.68 1.00 1.00 0.00 0.53 1.99\n#&gt; X5 0.96                 0.92 0.92 0.08 1.00 1.01\n#&gt; X6 0.69            0.22 0.54 0.54 0.46 0.90 1.22\n#&gt; X7           0.56       0.33 0.33 0.67 0.03 1.15\n#&gt; X8           0.75       0.59 0.59 0.41 0.05 1.12\n#&gt; X9 0.22      0.49       0.29 0.29 0.71 0.16 1.41\n#&gt; \n#&gt; With Sums of squares  of:\n#&gt;    g  F1*  F2*  F3*   h2 \n#&gt; 2.02 0.00 1.11 0.54 2.67 \n#&gt; \n#&gt; general/max  0.75   max/min =   622\n#&gt; mean percent general =  0.44    with sd =  0.43 and cv of  0.97 \n#&gt; Explained Common Variance of the general factor =  0.55 \n#&gt; \n#&gt; The degrees of freedom are 0  and the fit is  0 \n#&gt; The number of observations was  301  with Chi Square =  0.03  with prob &lt;  NA\n#&gt; The root mean square of the residuals is  0 \n#&gt; The df corrected root mean square of the residuals is  NA\n#&gt; \n#&gt; Compare this with the adequacy of just a general factor and no group factors\n#&gt; The degrees of freedom for just the general factor are 9  and the fit is  0.48 \n#&gt; The number of observations was  301  with Chi Square =  142  with prob &lt;  3.5e-26\n#&gt; The root mean square of the residuals is  0.17 \n#&gt; The df corrected root mean square of the residuals is  0.21 \n#&gt; \n#&gt; RMSEA index =  0.222  and the 10 % confidence intervals are  0.191 0.255\n#&gt; BIC =  90.9 \n#&gt; \n#&gt; Measures of factor score adequacy             \n#&gt;                                                  g   F1*  F2*  F3*\n#&gt; Correlation of scores with factors            0.96  0.08 0.83 0.96\n#&gt; Multiple R square of scores with factors      0.93  0.01 0.68 0.91\n#&gt; Minimum correlation of factor score estimates 0.86 -0.99 0.36 0.83\n#&gt; \n#&gt;  Total, General and Subset omega for each subset\n#&gt;                                                  g  F1*  F2*  F3*\n#&gt; Omega total for total scores and subscales    0.84 0.92 0.66 0.86\n#&gt; Omega general for total scores and subscales  0.55 0.92 0.04 0.61\n#&gt; Omega group for total scores and subscales    0.27 0.00 0.61 0.25\n\n\n\n\n\n\n\nÈ invece necessario ipotizzare un modello congenerico a due fattori.\n\nm2f_cong &lt;- '\n  # all loadings are uniquely estimated on each factor\n  f1 =~ NA*X4 + X5 + X6\n  f2 =~ NA*X7 + X8 + X9\n  \n  # constrain factor variancse to 1\n  f1 ~~ 1*f1\n  f2 ~~ 1*f2\n  \n  # estimate factor covariance\n  f1 ~~ f2\n'\n\n\n# Fit above model\nfit_2f_congeneric &lt;- sem(m2f_cong, data=d)\n\nSolo questo modello fornisce un adattamento adeguato ai dati.\n\nout = summary(fit_2f_congeneric, fit.measures = TRUE, standardized = TRUE)\nprint(out)\n#&gt; lavaan 0.6-19 ended normally after 18 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        13\n#&gt; \n#&gt;   Number of observations                           301\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                14.736\n#&gt;   Degrees of freedom                                 8\n#&gt;   P-value (Chi-square)                           0.064\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               568.519\n#&gt;   Degrees of freedom                                15\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.988\n#&gt;   Tucker-Lewis Index (TLI)                       0.977\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -2525.349\n#&gt;   Loglikelihood unrestricted model (H1)      -2517.981\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                5076.698\n#&gt;   Bayesian (BIC)                              5124.891\n#&gt;   Sample-size adjusted Bayesian (SABIC)       5083.662\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.053\n#&gt;   90 Percent confidence interval - lower         0.000\n#&gt;   90 Percent confidence interval - upper         0.095\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.402\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.159\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.035\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   f1 =~                                                                 \n#&gt;     X4                0.965    0.059   16.296    0.000    0.965    0.826\n#&gt;     X5                1.123    0.067   16.845    0.000    1.123    0.847\n#&gt;     X6                0.895    0.058   15.465    0.000    0.895    0.793\n#&gt;   f2 =~                                                                 \n#&gt;     X7                0.659    0.080    8.218    0.000    0.659    0.575\n#&gt;     X8                0.733    0.077    9.532    0.000    0.733    0.712\n#&gt;     X9                0.599    0.075    8.025    0.000    0.599    0.557\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   f1 ~~                                                                 \n#&gt;     f2                0.275    0.072    3.813    0.000    0.275    0.275\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;     f1                1.000                               1.000    1.000\n#&gt;     f2                1.000                               1.000    1.000\n#&gt;    .X4                0.433    0.056    7.679    0.000    0.433    0.318\n#&gt;    .X5                0.496    0.072    6.892    0.000    0.496    0.282\n#&gt;    .X6                0.472    0.054    8.732    0.000    0.472    0.371\n#&gt;    .X7                0.881    0.100    8.807    0.000    0.881    0.670\n#&gt;    .X8                0.521    0.094    5.534    0.000    0.521    0.492\n#&gt;    .X9                0.798    0.087    9.162    0.000    0.798    0.689\n\nNel contesto di questi dati, l’utilizzo di un modello congenerico non è sufficiente a giustificare l’impiego del punteggio totale, che rappresenta la somma dei punteggi degli item. Questo perché, nel caso specifico, sommando i punteggi di tutti gli item, finiremmo per includere misurazioni di due costrutti distinti.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Punteggio totale e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/07_total_score.html#session-info",
    "href": "chapters/fa/07_total_score.html#session-info",
    "title": "33  Punteggio totale e modello fattoriale",
    "section": "\n33.3 Session Info",
    "text": "33.3 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] modelsummary_2.3.0 ggokabeito_0.1.0   see_0.10.0        \n#&gt;  [4] MASS_7.3-65        viridis_0.6.5      viridisLite_0.4.2 \n#&gt;  [7] ggpubr_0.6.0       ggExtra_0.10.1     gridExtra_2.3     \n#&gt; [10] patchwork_1.3.0    bayesplot_1.11.1   semTools_0.5-6    \n#&gt; [13] semPlot_1.1.6      lavaan_0.6-19      psych_2.4.12      \n#&gt; [16] scales_1.3.0       markdown_1.13      knitr_1.49        \n#&gt; [19] lubridate_1.9.4    forcats_1.0.0      stringr_1.5.1     \n#&gt; [22] dplyr_1.1.4        purrr_1.0.4        readr_2.1.5       \n#&gt; [25] tidyr_1.3.1        tibble_3.2.1       ggplot2_3.5.1     \n#&gt; [28] tidyverse_2.0.0    here_1.0.1        \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1    jsonlite_1.9.0       magrittr_2.0.3      \n#&gt;   [4] TH.data_1.1-3        estimability_1.5.1   farver_2.1.2        \n#&gt;   [7] nloptr_2.1.1         rmarkdown_2.29       vctrs_0.6.5         \n#&gt;  [10] minqa_1.2.8          base64enc_0.1-3      rstatix_0.7.2       \n#&gt;  [13] htmltools_0.5.8.1    broom_1.0.7          Formula_1.2-5       \n#&gt;  [16] htmlwidgets_1.6.4    plyr_1.8.9           sandwich_3.1-1      \n#&gt;  [19] rio_1.2.3            emmeans_1.10.7       zoo_1.8-13          \n#&gt;  [22] igraph_2.1.4         mime_0.12            lifecycle_1.0.4     \n#&gt;  [25] pkgconfig_2.0.3      Matrix_1.7-2         R6_2.6.1            \n#&gt;  [28] fastmap_1.2.0        rbibutils_2.3        shiny_1.10.0        \n#&gt;  [31] digest_0.6.37        OpenMx_2.21.13       fdrtool_1.2.18      \n#&gt;  [34] colorspace_2.1-1     rprojroot_2.0.4      Hmisc_5.2-2         \n#&gt;  [37] labeling_0.4.3       timechange_0.3.0     abind_1.4-8         \n#&gt;  [40] compiler_4.4.2       withr_3.0.2          glasso_1.11         \n#&gt;  [43] htmlTable_2.4.3      backports_1.5.0      carData_3.0-5       \n#&gt;  [46] R.utils_2.13.0       ggsignif_0.6.4       GPArotation_2024.3-1\n#&gt;  [49] corpcor_1.6.10       gtools_3.9.5         tools_4.4.2         \n#&gt;  [52] pbivnorm_0.6.0       foreign_0.8-88       zip_2.3.2           \n#&gt;  [55] httpuv_1.6.15        nnet_7.3-20          R.oo_1.27.0         \n#&gt;  [58] glue_1.8.0           quadprog_1.5-8       nlme_3.1-167        \n#&gt;  [61] promises_1.3.2       lisrelToR_0.3        grid_4.4.2          \n#&gt;  [64] checkmate_2.3.2      cluster_2.1.8        reshape2_1.4.4      \n#&gt;  [67] generics_0.1.3       gtable_0.3.6         tzdb_0.4.0          \n#&gt;  [70] R.methodsS3_1.8.2    data.table_1.17.0    hms_1.1.3           \n#&gt;  [73] car_3.1-3            tables_0.9.31        sem_3.1-16          \n#&gt;  [76] pillar_1.10.1        rockchalk_1.8.157    later_1.4.1         \n#&gt;  [79] splines_4.4.2        lattice_0.22-6       survival_3.8-3      \n#&gt;  [82] kutils_1.73          tidyselect_1.2.1     miniUI_0.1.1.1      \n#&gt;  [85] pbapply_1.7-2        reformulas_0.4.0     stats4_4.4.2        \n#&gt;  [88] xfun_0.51            qgraph_1.9.8         arm_1.14-4          \n#&gt;  [91] stringi_1.8.4        yaml_2.3.10          pacman_0.5.1        \n#&gt;  [94] boot_1.3-31          evaluate_1.0.3       codetools_0.2-20    \n#&gt;  [97] mi_1.1               cli_3.6.4            RcppParallel_5.1.10 \n#&gt; [100] rpart_4.1.24         xtable_1.8-4         Rdpack_2.6.2        \n#&gt; [103] munsell_0.5.1        Rcpp_1.0.14          coda_0.19-4.1       \n#&gt; [106] png_0.1-8            XML_3.99-0.18        parallel_4.4.2      \n#&gt; [109] jpeg_0.1-10          lme4_1.1-36          mvtnorm_1.3-3       \n#&gt; [112] openxlsx_4.2.8       rlang_1.1.5          multcomp_1.4-28     \n#&gt; [115] mnormt_2.1.1\n\n\n\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Punteggio totale e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/01_val_matrici.html",
    "href": "chapters/extraction/01_val_matrici.html",
    "title": "34  Valutazione della matrice di correlazione",
    "section": "",
    "text": "34.1 Introduzione\nPrima di intraprendere un’analisi fattoriale, è essenziale esaminare la matrice di correlazione tra le variabili. Un determinante nullo della matrice di correlazione indica che l’analisi fattoriale non può essere eseguita a causa di collinearità perfetta tra le variabili. Se il determinante è diverso da zero, è comunque necessario valutare se le correlazioni tra le variabili sono sufficientemente elevate da giustificare un’analisi fattoriale. Correlazioni deboli tra gli item possono portare a soluzioni non parsimoniose. Questo può essere valutato sia tramite ispezione visiva della matrice di correlazione sia attraverso due test statistici: il test di sfericità di Bartlett e l’indice Kaiser-Meyer-Olkin (KMO).",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Valutazione della matrice di correlazione</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/01_val_matrici.html#analisi-preliminari",
    "href": "chapters/extraction/01_val_matrici.html#analisi-preliminari",
    "title": "34  Valutazione della matrice di correlazione",
    "section": "\n34.2 Analisi Preliminari",
    "text": "34.2 Analisi Preliminari\nPer illustrare il procedimento di analisi preliminare dei dati, utilizziamo il dataset HolzingerSwineford1939, che contiene 301 osservazioni relative a punteggi di abilità mentale. In questa analisi consideriamo le variabili x1–x9.\nCominciamo con un’anteprima del dataset e una verifica della presenza di dati mancanti.\n\n# Caricamento del dataset e visualizzazione iniziale\ndata(HolzingerSwineford1939)\nglimpse(HolzingerSwineford1939)\n#&gt; Rows: 301\n#&gt; Columns: 15\n#&gt; $ id     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, …\n#&gt; $ sex    &lt;int&gt; 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2,…\n#&gt; $ ageyr  &lt;int&gt; 13, 13, 13, 13, 12, 14, 12, 12, 13, 12, 12, 12, 12, 12, 12,…\n#&gt; $ agemo  &lt;int&gt; 1, 7, 1, 2, 2, 1, 1, 2, 0, 5, 2, 11, 7, 8, 6, 1, 11, 5, 8, …\n#&gt; $ school &lt;fct&gt; Pasteur, Pasteur, Pasteur, Pasteur, Pasteur, Pasteur, Paste…\n#&gt; $ grade  &lt;int&gt; 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,…\n#&gt; $ x1     &lt;dbl&gt; 3.33, 5.33, 4.50, 5.33, 4.83, 5.33, 2.83, 5.67, 4.50, 3.50,…\n#&gt; $ x2     &lt;dbl&gt; 7.75, 5.25, 5.25, 7.75, 4.75, 5.00, 6.00, 6.25, 5.75, 5.25,…\n#&gt; $ x3     &lt;dbl&gt; 0.375, 2.125, 1.875, 3.000, 0.875, 2.250, 1.000, 1.875, 1.5…\n#&gt; $ x4     &lt;dbl&gt; 2.33, 1.67, 1.00, 2.67, 2.67, 1.00, 3.33, 3.67, 2.67, 2.67,…\n#&gt; $ x5     &lt;dbl&gt; 5.75, 3.00, 1.75, 4.50, 4.00, 3.00, 6.00, 4.25, 5.75, 5.00,…\n#&gt; $ x6     &lt;dbl&gt; 1.286, 1.286, 0.429, 2.429, 2.571, 0.857, 2.857, 1.286, 2.7…\n#&gt; $ x7     &lt;dbl&gt; 3.39, 3.78, 3.26, 3.00, 3.70, 4.35, 4.70, 3.39, 4.52, 4.13,…\n#&gt; $ x8     &lt;dbl&gt; 5.75, 6.25, 3.90, 5.30, 6.30, 6.65, 6.20, 5.15, 4.65, 4.55,…\n#&gt; $ x9     &lt;dbl&gt; 6.36, 7.92, 4.42, 4.86, 5.92, 7.50, 4.86, 3.67, 7.36, 4.36,…\n\n# Selezione delle variabili di interesse\nhz &lt;- HolzingerSwineford1939 |&gt;\n  dplyr::select(x1:x9)\n\n# Visualizzazione delle prime 5 righe\nhz |&gt;\n  slice(1:5)\n#&gt;     x1   x2    x3   x4   x5    x6   x7   x8   x9\n#&gt; 1 3.33 7.75 0.375 2.33 5.75 1.286 3.39 5.75 6.36\n#&gt; 2 5.33 5.25 2.125 1.67 3.00 1.286 3.78 6.25 7.92\n#&gt; 3 4.50 5.25 1.875 1.00 1.75 0.429 3.26 3.90 4.42\n#&gt; 4 5.33 7.75 3.000 2.67 4.50 2.429 3.00 5.30 4.86\n#&gt; 5 4.83 4.75 0.875 2.67 4.00 2.571 3.70 6.30 5.92\n\n\n34.2.1 Valutazione dei Dati Mancanti\nPrima di procedere all’ispezione visiva, è importante verificare la presenza di dati mancanti. Eventuali valori mancanti potrebbero influenzare le correlazioni calcolate e, di conseguenza, l’esito dell’analisi fattoriale.\n\n# Controllo dei dati mancanti\nhz |&gt;\n  summarise(across(everything(), ~ sum(is.na(.))))\n#&gt;   x1 x2 x3 x4 x5 x6 x7 x8 x9\n#&gt; 1  0  0  0  0  0  0  0  0  0\n\nIn questo set di dati non ci sono dati mancanti.\nSe dati mancanti sono presenti, sarà necessario applicare un’adeguata tecnica di gestione, come imputazione o rimozione dei casi con dati mancanti, prima di proseguire con l’analisi.\nQuesta fase preliminare è fondamentale per garantire la validità e l’affidabilità delle conclusioni tratte dall’analisi fattoriale.\n\n34.2.2 Distribuzione delle variabili\nEsaminiamo la distribuzione delle variabili.\n\ndescribe(hz)\n#&gt;    vars   n mean   sd median trimmed  mad  min   max range  skew kurtosis\n#&gt; x1    1 301 4.94 1.17   5.00    4.96 1.24 0.67  8.50  7.83 -0.25     0.31\n#&gt; x2    2 301 6.09 1.18   6.00    6.02 1.11 2.25  9.25  7.00  0.47     0.33\n#&gt; x3    3 301 2.25 1.13   2.12    2.20 1.30 0.25  4.50  4.25  0.38    -0.91\n#&gt; x4    4 301 3.06 1.16   3.00    3.02 0.99 0.00  6.33  6.33  0.27     0.08\n#&gt; x5    5 301 4.34 1.29   4.50    4.40 1.48 1.00  7.00  6.00 -0.35    -0.55\n#&gt; x6    6 301 2.19 1.10   2.00    2.09 1.06 0.14  6.14  6.00  0.86     0.82\n#&gt; x7    7 301 4.19 1.09   4.09    4.16 1.10 1.30  7.43  6.13  0.25    -0.31\n#&gt; x8    8 301 5.53 1.01   5.50    5.49 0.96 3.05 10.00  6.95  0.53     1.17\n#&gt; x9    9 301 5.37 1.01   5.42    5.37 0.99 2.78  9.25  6.47  0.20     0.29\n#&gt;      se\n#&gt; x1 0.07\n#&gt; x2 0.07\n#&gt; x3 0.07\n#&gt; x4 0.07\n#&gt; x5 0.07\n#&gt; x6 0.06\n#&gt; x7 0.06\n#&gt; x8 0.06\n#&gt; x9 0.06\n\nI valori di asimmetria e kurosi sono adeguati.\n\n34.2.3 Correlazioni\nUn’ispezione visiva della matrice di correlazione consente di identificare blocchi di variabili con alte correlazioni interne e basse correlazioni con altre variabili. La presenza di tali blocchi suggerisce la possibilità di più fattori comuni sottostanti.\nEsaminiamo le correlazioni tra le variabili usando le funzioni del pacchetto corrr:\n\n\ncorrr::rearrange raggruppa le variabili altamente correlate\n\ncorrr::rplot visualizza il risultato.\n\n\ncor_tb &lt;- correlate(hz)\n\ncor_tb |&gt;\n  rearrange() |&gt;\n  rplot(colors = c(\"red\", \"white\", \"blue\"))\n\n\n\n\n\n\n\nIl grafico suggerisce la presenza di tre gruppi di variabili:\n\nda x4 a x6 (primo gruppo)\nda x1 a x3 (secondo gruppo)\nda x7 a x9 (terzo gruppo).",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Valutazione della matrice di correlazione</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/01_val_matrici.html#sfericità-di-bartlett",
    "href": "chapters/extraction/01_val_matrici.html#sfericità-di-bartlett",
    "title": "34  Valutazione della matrice di correlazione",
    "section": "\n34.3 Sfericità di Bartlett",
    "text": "34.3 Sfericità di Bartlett\nIl test di sfericità di Bartlett verifica l’ipotesi che il campione provenga da una popolazione in cui le variabili non sono correlate. Formalmente, il test della sfericità di Bartlett verifica l’ipotesi \\(H_0 : \\boldsymbol{R} = \\boldsymbol{I}\\) tramite la formula:\n\\[\n\\chi^2 = -\\bigg[n -1 -\\frac{1}{6} (2p +5)\\bigg] \\ln |\\boldsymbol{R}|,\n\\]\nin cui \\(n\\) è il numero dei soggetti, \\(p\\) il numero delle variabili e \\(|\\boldsymbol{R}|\\) il determinante della matrice di correlazione.\nLa statistica del test di sfericità di Bartlett segue una distribuzione chi-quadro con \\(p(p - 1)/2\\) gradi di libertà. Un valore elevato della statistica indica che la matrice di correlazione R contiene valori di correlazione significativamente diversi da 0. Al contrario, un valore basso della statistica indica che le correlazioni sono basse e non si distinguono da 0.\nIl limite di questo test è che dipende dal numero delle variabili e dalla numerosità del campione, quindi tende a rigettare \\(H_0\\) all’aumentare del campione e del numero delle variabili, anche se le correlazioni sono piccole.\nApplichiamo il test di Bartlet per il dati dell’esempio in discussione.\n\ncor_mat &lt;- cor(hz)\n\nout = cortest.bartlett(R = cor_mat, n = 301)\nprint(out)\n#&gt; $chisq\n#&gt; [1] 904\n#&gt; \n#&gt; $p.value\n#&gt; [1] 1.91e-166\n#&gt; \n#&gt; $df\n#&gt; [1] 36\n\nIl risultato del test di Bartlett sui dati HolzingerSwineford1939 indica che esiste una correlazione tra le variabili.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Valutazione della matrice di correlazione</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/01_val_matrici.html#test-di-adeguatezza-campionaria-di-kaiser-meyer-olkin",
    "href": "chapters/extraction/01_val_matrici.html#test-di-adeguatezza-campionaria-di-kaiser-meyer-olkin",
    "title": "34  Valutazione della matrice di correlazione",
    "section": "\n34.4 Test di adeguatezza campionaria di Kaiser-Meyer-Olkin",
    "text": "34.4 Test di adeguatezza campionaria di Kaiser-Meyer-Olkin\nIl test di Kaiser-Meyer-Olkin (KMO) è uno strumento statistico che valuta l’adeguatezza dei dati per l’analisi fattoriale. Esso misura la proporzione di varianza tra le variabili che potrebbe essere attribuita a fattori comuni. Un valore KMO più alto indica una maggiore adattabilità dei dati all’analisi fattoriale.\nLa statistica di adeguatezza campionaria KMO è data da\n\\[\\text{KMO} = \\frac{\\sum_i\\sum_j r^2_{ij}}{\\sum_i\\sum_j r^2_{ij} +\\sum_i\\sum_jp^2_{ij}},\\]\ndove \\(r_{ij}\\) sono le correlazioni osservate e \\(p_{ij}\\) sono le correlazioni parzializzate su tutte le altre. Se le correlazioni parzializzate sono piccole, KMO tende a 1.\nSecondo Kaiser (1970), l’adeguatezza campionaria si valuta nel modo seguente:\n\nda 0.00 a 0.49: inaccettabile\nda 0.50 a 0.59: miserabile\nda 0.60 a 0.69: mediocre\nda 0.70 a 0.79: media\nda 0.80 a 0.89: meritevole\nda 0.90 a 1.00: meravigliosa.\n\nApplichiamo il test KMO ai dati HolzingerSwineford1939.\n\nout = KMO(cor_mat)\nprint(out)\n#&gt; Kaiser-Meyer-Olkin factor adequacy\n#&gt; Call: KMO(r = cor_mat)\n#&gt; Overall MSA =  0.75\n#&gt; MSA for each item = \n#&gt;   x1   x2   x3   x4   x5   x6   x7   x8   x9 \n#&gt; 0.81 0.78 0.73 0.76 0.74 0.81 0.59 0.68 0.79\n\nPer questi dati, il risultato del test KMO indica che l’adeguatezza campionaria è media.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Valutazione della matrice di correlazione</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/01_val_matrici.html#riflessioni-conclusive",
    "href": "chapters/extraction/01_val_matrici.html#riflessioni-conclusive",
    "title": "34  Valutazione della matrice di correlazione",
    "section": "\n34.5 Riflessioni Conclusive",
    "text": "34.5 Riflessioni Conclusive\nL’analisi preliminare dei dati rappresenta un passaggio essenziale per verificare l’adeguatezza di un dataset all’analisi fattoriale. L’ispezione visiva della matrice di correlazione, il test di sfericità di Bartlett e il test di adeguatezza campionaria KMO forniscono indicazioni complementari sull’esistenza di relazioni tra le variabili e sulla qualità dei dati rispetto alla struttura fattoriale. Nel caso del dataset HolzingerSwineford1939, i risultati suggeriscono che le correlazioni tra le variabili giustificano l’analisi fattoriale, mentre il valore medio del KMO evidenzia un margine di miglioramento nella struttura dei dati, come la revisione delle variabili incluse o l’aumento della numerosità campionaria. Questi strumenti aiutano a garantire che l’analisi fattoriale sia basata su una base dati solida, facilitando interpretazioni affidabili dei fattori estratti.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Valutazione della matrice di correlazione</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/01_val_matrici.html#session-info",
    "href": "chapters/extraction/01_val_matrici.html#session-info",
    "title": "34  Valutazione della matrice di correlazione",
    "section": "\n34.6 Session Info",
    "text": "34.6 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] corrr_0.4.4       ggokabeito_0.1.0  see_0.10.0        MASS_7.3-65      \n#&gt;  [5] viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1   \n#&gt;  [9] gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6   \n#&gt; [13] semPlot_1.1.6     lavaan_0.6-19     psych_2.4.12      scales_1.3.0     \n#&gt; [17] markdown_1.13     knitr_1.49        lubridate_1.9.4   forcats_1.0.0    \n#&gt; [21] stringr_1.5.1     dplyr_1.1.4       purrr_1.0.4       readr_2.1.5      \n#&gt; [25] tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0  \n#&gt; [29] here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.0      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] iterators_1.0.14    mime_0.12           lifecycle_1.0.4    \n#&gt;  [25] pkgconfig_2.0.3     Matrix_1.7-2        R6_2.6.1           \n#&gt;  [28] fastmap_1.2.0       rbibutils_2.3       shiny_1.10.0       \n#&gt;  [31] digest_0.6.37       OpenMx_2.21.13      fdrtool_1.2.18     \n#&gt;  [34] colorspace_2.1-1    rprojroot_2.0.4     Hmisc_5.2-2        \n#&gt;  [37] seriation_1.5.7     labeling_0.4.3      timechange_0.3.0   \n#&gt;  [40] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n#&gt;  [43] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n#&gt;  [46] carData_3.0-5       ggsignif_0.6.4      corpcor_1.6.10     \n#&gt;  [49] gtools_3.9.5        tools_4.4.2         pbivnorm_0.6.0     \n#&gt;  [52] foreign_0.8-88      zip_2.3.2           httpuv_1.6.15      \n#&gt;  [55] nnet_7.3-20         glue_1.8.0          quadprog_1.5-8     \n#&gt;  [58] nlme_3.1-167        promises_1.3.2      lisrelToR_0.3      \n#&gt;  [61] grid_4.4.2          checkmate_2.3.2     cluster_2.1.8      \n#&gt;  [64] reshape2_1.4.4      generics_0.1.3      gtable_0.3.6       \n#&gt;  [67] tzdb_0.4.0          ca_0.71.1           data.table_1.17.0  \n#&gt;  [70] hms_1.1.3           car_3.1-3           sem_3.1-16         \n#&gt;  [73] foreach_1.5.2       pillar_1.10.1       rockchalk_1.8.157  \n#&gt;  [76] later_1.4.1         splines_4.4.2       lattice_0.22-6     \n#&gt;  [79] survival_3.8-3      kutils_1.73         tidyselect_1.2.1   \n#&gt;  [82] registry_0.5-1      miniUI_0.1.1.1      pbapply_1.7-2      \n#&gt;  [85] reformulas_0.4.0    stats4_4.4.2        xfun_0.51          \n#&gt;  [88] qgraph_1.9.8        arm_1.14-4          stringi_1.8.4      \n#&gt;  [91] yaml_2.3.10         pacman_0.5.1        boot_1.3-31        \n#&gt;  [94] evaluate_1.0.3      codetools_0.2-20    mi_1.1             \n#&gt;  [97] cli_3.6.4           RcppParallel_5.1.10 rpart_4.1.24       \n#&gt; [100] xtable_1.8-4        Rdpack_2.6.2        munsell_0.5.1      \n#&gt; [103] Rcpp_1.0.14         coda_0.19-4.1       png_0.1-8          \n#&gt; [106] XML_3.99-0.18       parallel_4.4.2      jpeg_0.1-10        \n#&gt; [109] lme4_1.1-36         mvtnorm_1.3-3       openxlsx_4.2.8     \n#&gt; [112] rlang_1.1.5         TSP_1.2-4           multcomp_1.4-28    \n#&gt; [115] mnormt_2.1.1\n\n\n\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Valutazione della matrice di correlazione</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html",
    "href": "chapters/extraction/02_estrazione.html",
    "title": "35  L’estrazione dei fattori",
    "section": "",
    "text": "35.1 Introduzione\nL’analisi fattoriale è una tecnica statistica che semplifica un insieme complesso di variabili osservate identificando un numero ridotto di fattori latenti che spiegano le correlazioni tra queste variabili. L’obiettivo è individuare un insieme più contenuto di variabili non osservabili (fattori) che rappresentino le interrelazioni tra un ampio numero di variabili osservate.\nIl modello statistico dell’analisi fattoriale è rappresentato dalla seguente equazione:\n\\[\n\\boldsymbol{\\Sigma} = \\boldsymbol{\\Lambda} \\boldsymbol{\\Phi} \\boldsymbol{\\Lambda}^{\\mathsf{T}} + \\boldsymbol{\\Psi},\n\\]\ndove:",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html#introduzione",
    "href": "chapters/extraction/02_estrazione.html#introduzione",
    "title": "35  L’estrazione dei fattori",
    "section": "",
    "text": "\\(\\boldsymbol{\\Sigma}\\) è la matrice delle covarianze tra le variabili osservate;\n\n\\(\\boldsymbol{\\Lambda}\\) è la matrice dei carichi fattoriali, che rappresenta la relazione tra le variabili osservate e i fattori latenti;\n\n\\(\\boldsymbol{\\Phi}\\) è la matrice delle correlazioni tra i fattori latenti;\n\n\\(\\boldsymbol{\\Psi}\\) è una matrice diagonale contenente le unicità, ossia la varianza specifica di ciascuna variabile osservata non spiegata dai fattori comuni.\n\n\n35.1.1 Estrazione dei Fattori\nL’estrazione dei fattori è il processo che consente di stimare la matrice dei carichi fattoriali \\(\\boldsymbol{\\Lambda}\\). Esistono diversi metodi per eseguire questa stima, tra cui:\n\nMetodo delle componenti principali\nMassimizza la varianza spiegata dai fattori ma non considera esplicitamente l’errore di misura.\nMetodo dei fattori principali\nVariante del metodo delle componenti principali, tiene conto dell’errore di misura stimando inizialmente le comunalità.\nMetodo dei fattori principali iterato\nVersione iterativa del metodo dei fattori principali in cui le comunalità vengono stimate ripetutamente fino alla convergenza.\nMetodo di massima verosimiglianza (ML)\nStima i parametri del modello assumendo che le variabili osservate seguano una distribuzione normale multivariata. Questo metodo è particolarmente utile per testare ipotesi sui fattori latenti e per confrontare modelli alternativi.\n\nCiascun metodo presenta vantaggi e limitazioni, e la scelta dipende dagli obiettivi dell’analisi e dalle caratteristiche del dataset.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html#metodo-delle-componenti-principali",
    "href": "chapters/extraction/02_estrazione.html#metodo-delle-componenti-principali",
    "title": "35  L’estrazione dei fattori",
    "section": "\n35.2 Metodo delle Componenti Principali",
    "text": "35.2 Metodo delle Componenti Principali\nNonostante il nome, l’analisi fattoriale eseguita con il metodo delle componenti principali non coincide con un’analisi delle componenti principali. Quest’ultima è, invece, un’applicazione del teorema di scomposizione spettrale di una matrice.\nIl teorema spettrale stabilisce che, data una matrice simmetrica \\(\\textbf{S}_{p \\times p}\\), è sempre possibile individuare una matrice ortogonale \\(\\textbf{C}_{p \\times p}\\) tale che:\n\\[\n\\textbf{S} = \\textbf{C}\\textbf{D}\\textbf{C}^{\\mathsf{T}} ,\n\\]\ndove \\(\\textbf{D}\\) è una matrice diagonale. Inoltre, il teorema specifica che gli elementi sulla diagonale di \\(\\textbf{D}\\) sono gli autovalori della matrice \\(\\textbf{S}\\), mentre le colonne di \\(\\textbf{C}\\) rappresentano gli autovettori normalizzati associati a tali autovalori.\n\n35.2.1 Esempio Numerico\nConsideriamo un esempio basato sui dati discussi da Rencher (2002). Una ragazza di 12 anni ha valutato sette persone a lei conosciute rispetto a cinque caratteristiche (kind, intelligent, happy, likeable e just), usando una scala a nove punti. La matrice di correlazione tra queste variabili è la seguente:\n\nR &lt;- matrix(c(\n  1.000, .296, .881, .995, .545,\n  .296, 1.000, -.022, .326, .837,\n  .881, -.022, 1.000, .867, .130,\n  .995, .326, .867, 1.000, .544,\n  .545, .837, .130, .544, 1.000\n),\nncol = 5, byrow = TRUE, dimnames = list(\n  c(\"K\", \"I\", \"H\", \"L\", \"J\"),\n  c(\"K\", \"I\", \"H\", \"L\", \"J\")\n))\nR\n#&gt;       K      I      H     L     J\n#&gt; K 1.000  0.296  0.881 0.995 0.545\n#&gt; I 0.296  1.000 -0.022 0.326 0.837\n#&gt; H 0.881 -0.022  1.000 0.867 0.130\n#&gt; L 0.995  0.326  0.867 1.000 0.544\n#&gt; J 0.545  0.837  0.130 0.544 1.000\n\nGli autovalori e gli autovettori di \\(\\textbf{R}\\) si calcolano con la funzione eigen() in R:\n\ne &lt;- eigen(R)\nprint(e)\n#&gt; eigen() decomposition\n#&gt; $values\n#&gt; [1] 3.263377 1.538382 0.167969 0.030030 0.000242\n#&gt; \n#&gt; $vectors\n#&gt;        [,1]   [,2]    [,3]   [,4]   [,5]\n#&gt; [1,] -0.537 -0.186 -0.1899 -0.125  0.791\n#&gt; [2,] -0.287  0.651  0.6849 -0.120  0.103\n#&gt; [3,] -0.434 -0.474  0.4069  0.614 -0.212\n#&gt; [4,] -0.537 -0.169 -0.0953 -0.629 -0.527\n#&gt; [5,] -0.390  0.538 -0.5658  0.444 -0.204\n\n\n35.2.1.1 Ricostruzione della Matrice\nCome indicato in precedenza, \\(\\textbf{R}\\) può essere scomposta come:\n\\[\n\\textbf{R} = \\textbf{C}\\textbf{D}\\textbf{C}^{\\mathsf{T}} .\n\\]\nEseguendo i calcoli:\n\ne$vectors %*% diag(e$values) %*% t(e$vectors)\n#&gt;       [,1]   [,2]   [,3]  [,4]  [,5]\n#&gt; [1,] 1.000  0.296  0.881 0.995 0.545\n#&gt; [2,] 0.296  1.000 -0.022 0.326 0.837\n#&gt; [3,] 0.881 -0.022  1.000 0.867 0.130\n#&gt; [4,] 0.995  0.326  0.867 1.000 0.544\n#&gt; [5,] 0.545  0.837  0.130 0.544 1.000\n\n\n35.2.1.2 Analisi degli Autovalori\nGli autovalori indicano la quantità di varianza spiegata da ciascuna componente. Nel nostro caso, i primi due autovalori spiegano il 96% della varianza totale:\n\n(e$values[1] + e$values[2]) / 5\n#&gt; [1] 0.96\n\n\n35.2.1.3 Riduzione della Dimensionalità\nPer ridurre la dimensionalità dei dati, usiamo i primi due autovalori e i rispettivi autovettori. Questo ci permette di approssimare \\(\\textbf{R}\\) mediante una matrice di rango ridotto:\n\\[\n\\textbf{R} \\approx \\hat{\\boldsymbol{\\Lambda}} \\hat{\\boldsymbol{\\Lambda}}^{\\mathsf{T}} .\n\\]\n\n35.2.1.4 Fattorizzazione\nScriviamo \\(\\textbf{D}\\) come prodotto di due matrici \\(\\textbf{D}^{1/2}\\):\n\\[\n\\textbf{D} = \\textbf{D}^{1/2} \\textbf{D}^{1/2} ,\n\\]\ncon \\(\\textbf{D}^{1/2}\\) definita come:\n\\[\n\\textbf{D}^{1/2} =\n\\begin{bmatrix}\n\\sqrt{\\theta_1} & 0 & \\dots & 0 \\\\\n0 & \\sqrt{\\theta_2} & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & \\sqrt{\\theta_p}\n\\end{bmatrix}\n\\]\nDefiniamo ora \\(\\textbf{D}_1\\) come la matrice diagonale contenente i primi \\(m\\) autovalori di \\(\\textbf{R}\\) e \\(\\textbf{C}_1\\) come la matrice contenente i corrispondenti \\(m\\) autovettori. Le saturazioni fattoriali stimate sono:\n\\[\n\\hat{\\boldsymbol{\\Lambda}} = \\textbf{C}_1 \\textbf{D}_1^{1/2} .\n\\]\nPer \\(m = 2\\), calcoliamo \\(\\hat{\\boldsymbol{\\Lambda}}\\):\n\nL &lt;- cbind(\n  e$vectors[, 1] * sqrt(e$values[1]),\n  e$vectors[, 2] * sqrt(e$values[2])\n)\n\nround(L, 3)\n#&gt;        [,1]   [,2]\n#&gt; [1,] -0.970 -0.231\n#&gt; [2,] -0.519  0.807\n#&gt; [3,] -0.785 -0.588\n#&gt; [4,] -0.971 -0.210\n#&gt; [5,] -0.704  0.667\n\n\n35.2.1.5 Matrice Riprodotta e Residuale\nLa matrice di correlazione approssimata è:\n\nR_hat &lt;- round(L %*% t(L), 3)\nR_hat\n#&gt;       [,1]   [,2]   [,3]  [,4]  [,5]\n#&gt; [1,] 0.993  0.317  0.896 0.990 0.529\n#&gt; [2,] 0.317  0.921 -0.067 0.335 0.904\n#&gt; [3,] 0.896 -0.067  0.961 0.885 0.160\n#&gt; [4,] 0.990  0.335  0.885 0.987 0.543\n#&gt; [5,] 0.529  0.904  0.160 0.543 0.940\n\nLa matrice residua, con le specificità sulla diagonale principale, è:\n\nR - R_hat\n#&gt;        K      I      H      L      J\n#&gt; K  0.007 -0.021 -0.015  0.005  0.016\n#&gt; I -0.021  0.079  0.045 -0.009 -0.067\n#&gt; H -0.015  0.045  0.039 -0.018 -0.030\n#&gt; L  0.005 -0.009 -0.018  0.013  0.001\n#&gt; J  0.016 -0.067 -0.030  0.001  0.060\n\nIn conclusione, il nome metodo delle componenti principali deriva dal fatto che le saturazioni fattoriali sono proporzionali agli autovettori di \\(\\textbf{R}\\). Tuttavia, l’interpretazione differisce da quella dell’analisi delle componenti principali.\nUn aspetto critico del metodo è la sua non invarianza rispetto ai cambiamenti di scala: le soluzioni ottenute con la matrice \\(\\textbf{S}\\) di varianze-covarianze differiscono da quelle calcolate con la matrice \\(\\textbf{R}\\) di correlazioni. Inoltre, il metodo non prevede un test di bontà di adattamento, disponibile invece con il metodo della massima verosimiglianza.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html#metodo-dei-fattori-principali",
    "href": "chapters/extraction/02_estrazione.html#metodo-dei-fattori-principali",
    "title": "35  L’estrazione dei fattori",
    "section": "\n35.3 Metodo dei Fattori Principali",
    "text": "35.3 Metodo dei Fattori Principali\nIl metodo dei fattori principali (Principal Factor Method o Principal Axis Method) è uno dei metodi più comuni per stimare le saturazioni fattoriali e le comunalità. A differenza del metodo delle componenti principali, che trascura la specificità \\(\\boldsymbol{\\Psi}\\) e si limita a fattorializzare direttamente la matrice delle covarianze \\(\\textbf{S}\\) o delle correlazioni \\(\\textbf{R}\\), il metodo dei fattori principali affronta questo limite introducendo una matrice ridotta di varianze-covarianze o correlazioni.\nQuesta matrice ridotta si ottiene sostituendo una stima delle comunalità alle varianze sulla diagonale principale di \\(\\textbf{S}\\) o \\(\\textbf{R}\\). Questo processo consente di isolare le comunalità dalle specificità e dall’errore.\n\n35.3.1 Stima delle Comunalità\n\n35.3.1.1 Caso della Matrice di Correlazioni \\(\\textbf{R}\\)\n\nPer una matrice ridotta di correlazioni \\(\\textbf{R} - \\hat{\\boldsymbol{\\Psi}}\\), la comunalità \\(i\\)-esima \\(\\hat{h}^2_i\\) può essere stimata come il quadrato del coefficiente di correlazione multipla tra la variabile \\(Y_i\\) e le altre \\(p-1\\) variabili. Questa stima si calcola come:\n\\[\n\\hat{h}^2_i = R^2_i = 1 - \\frac{1}{r^{ii}} ,\n\\]\ndove \\(r^{ii}\\) è l’elemento diagonale \\(i\\)-esimo dell’inversa di \\(\\textbf{R}\\).\n\n35.3.1.2 Caso della Matrice di Varianze-Covarianze \\(\\textbf{S}\\)\n\nSe si utilizza la matrice delle varianze-covarianze \\(\\textbf{S}\\), la comunalità \\(i\\)-esima è stimata come:\n\\[\n\\hat{h}^2_i = s_{ii} - \\frac{1}{r^{ii}} ,\n\\]\ndove \\(s_{ii}\\) è l’elemento diagonale \\(i\\)-esimo di \\(\\textbf{S}\\).\n\n35.3.1.3 Gestione della Singolarità\nSe \\(\\textbf{R}\\) è singolare, la comunalità \\(\\hat{h}^2_i\\) può essere stimata come il valore assoluto del massimo coefficiente di correlazione lineare tra la variabile \\(Y_i\\) e le altre variabili.\n\n35.3.2 Matrice Ridotta\nSostituendo le stime delle comunalità nella diagonale principale di \\(\\textbf{S}\\) o \\(\\textbf{R}\\), otteniamo la matrice ridotta. Ad esempio, per la matrice delle varianze-covarianze:\n\\[\n\\textbf{S} - \\hat{\\boldsymbol{\\Psi}} =\n\\begin{bmatrix}\n\\hat{h}^2_1 & s_{12} & \\dots & s_{1p} \\\\\ns_{21} & \\hat{h}^2_2 & \\dots & s_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\ns_{p1} & s_{p2} & \\dots & \\hat{h}^2_p\n\\end{bmatrix} .\n\\]\nAnalogamente, per la matrice delle correlazioni:\n\\[\n\\textbf{R} - \\hat{\\boldsymbol{\\Psi}} =\n\\begin{bmatrix}\n\\hat{h}^2_1 & r_{12} & \\dots & r_{1p} \\\\\nr_{21} & \\hat{h}^2_2 & \\dots & r_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nr_{p1} & r_{p2} & \\dots & \\hat{h}^2_p\n\\end{bmatrix} .\n\\]\n\n35.3.3 Esempio Numerico\nUsiamo la matrice di correlazione dell’esempio precedente. Per stimare la comunalità \\(i\\)-esima, utilizziamo il valore massimo assoluto nella riga \\(i\\)-esima della matrice \\(\\textbf{R}\\). Le stime delle comunalità sono:\n\\[\n\\hat{h}^2 = \\{0.995, 0.837, 0.881, 0.995, 0.837\\} .\n\\]\nSostituendo queste stime nella diagonale principale, otteniamo la matrice ridotta:\n\nR1 &lt;- R\nh.hat &lt;- c(.995, .837, .881, .995, .837)\nR1[cbind(1:5, 1:5)] &lt;- h.hat\nR1\n#&gt;       K      I      H     L     J\n#&gt; K 0.995  0.296  0.881 0.995 0.545\n#&gt; I 0.296  0.837 -0.022 0.326 0.837\n#&gt; H 0.881 -0.022  0.881 0.867 0.130\n#&gt; L 0.995  0.326  0.867 0.995 0.544\n#&gt; J 0.545  0.837  0.130 0.544 0.837\n\n\n35.3.3.1 Autovalori della Matrice Ridotta\nCalcoliamo gli autovalori della matrice ridotta:\n\nee &lt;- eigen(R1)\nround(ee$values, 3)\n#&gt; [1]  3.202  1.394  0.029  0.000 -0.080\n\nLa somma degli autovalori è:\n\nsum(ee$values)\n#&gt; [1] 4.54\n\n\n35.3.3.2 Stima delle Saturazioni Fattoriali\nI primi due autovalori e i rispettivi autovettori vengono usati per stimare le saturazioni fattoriali. Moltiplichiamo gli autovettori per la radice quadrata dei rispettivi autovalori:\n\nround(ee$vectors[, 1:2] %*% sqrt(diag(ee$values[1:2])), 3)\n#&gt;       [,1]   [,2]\n#&gt; [1,] 0.981 -0.209\n#&gt; [2,] 0.487  0.774\n#&gt; [3,] 0.772 -0.544\n#&gt; [4,] 0.982 -0.187\n#&gt; [5,] 0.667  0.648\n\nIn conclusione, il metodo dei fattori principali consente di stimare le saturazioni fattoriali tenendo conto delle comunalità, a differenza del metodo delle componenti principali che trascura la specificità. Tuttavia, questo metodo richiede che \\(\\textbf{R}\\) non sia singolare. Le soluzioni ottenute sono influenzate dalle scelte iniziali per le comunalità, ma risultano più interpretabili rispetto a quelle del metodo delle componenti principali, poiché cercano di separare la varianza comune dalla specificità e dall’errore.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html#metodo-dei-fattori-principali-iterato",
    "href": "chapters/extraction/02_estrazione.html#metodo-dei-fattori-principali-iterato",
    "title": "35  L’estrazione dei fattori",
    "section": "\n35.4 Metodo dei Fattori Principali Iterato",
    "text": "35.4 Metodo dei Fattori Principali Iterato\nIl metodo dei fattori principali iterato migliora la stima delle comunalità attraverso un processo iterativo che aggiorna progressivamente la diagonale della matrice ridotta \\(\\textbf{S} - \\hat{\\boldsymbol{\\Psi}}\\) o \\(\\textbf{R} - \\hat{\\boldsymbol{\\Psi}}\\). Questo approccio consente di ottenere stime più accurate delle comunalità e, di conseguenza, delle saturazioni fattoriali.\n\n35.4.1 Procedura\n\nStima iniziale delle comunalità: Si parte con una stima iniziale delle comunalità \\(\\hat{h}^2_i\\) per tutte le variabili. Queste stime iniziali possono derivare, ad esempio, dal massimo valore assoluto di correlazione per ciascuna variabile.\nCostruzione della matrice ridotta: Sostituendo le comunalità iniziali nella diagonale principale, si ottiene la matrice ridotta \\(\\textbf{S} - \\hat{\\boldsymbol{\\Psi}}\\) o \\(\\textbf{R} - \\hat{\\boldsymbol{\\Psi}}\\).\nStima delle saturazioni fattoriali: Dalla matrice ridotta, si calcolano gli autovalori e i corrispondenti autovettori, utilizzando i primi \\(m\\) autovalori (e i relativi autovettori) per stimare le saturazioni fattoriali \\(\\hat{\\boldsymbol{\\Lambda}}\\). Le saturazioni fattoriali per la variabile \\(i\\) e il fattore \\(j\\) si indicano con \\(\\hat{\\lambda}_{ij}\\).\n\nAggiornamento delle comunalità: Le comunalità vengono ricalcolate come la somma dei quadrati delle saturazioni fattoriali per ciascuna variabile \\(i\\):\n\\[\n\\hat{h}^2_i = \\sum_{j=1}^m \\hat{\\lambda}_{ij}^2 .\n\\]\n\nSostituzione nella matrice ridotta: I nuovi valori di \\(\\hat{h}^2_i\\) vengono sostituiti nella diagonale principale della matrice ridotta \\(\\textbf{S} - \\hat{\\boldsymbol{\\Psi}}\\) o \\(\\textbf{R} - \\hat{\\boldsymbol{\\Psi}}\\), e il processo viene ripetuto.\nIterazione fino alla convergenza: Questo ciclo iterativo continua finché i valori stimati delle comunalità \\(\\hat{h}^2_i\\) non cambiano significativamente tra un’iterazione e l’altra, indicando la convergenza.\nStima finale dei pesi fattoriali: Una volta raggiunta la convergenza, gli autovalori e gli autovettori della matrice ridotta finale vengono utilizzati per stimare i pesi fattoriali definitivi.\n\n35.4.2 Confronto con il Metodo delle Componenti Principali\n\nSimilitudini:\nQuando il numero di variabili \\(p\\) è grande e le correlazioni tra le variabili sono alte, il metodo dei fattori principali iterato produce risultati simili a quelli del metodo delle componenti principali.\nDifferenze:\nIl metodo dei fattori principali iterato tiene conto delle specificità (diagonale \\(\\boldsymbol{\\Psi}\\)) e fornisce una stima più accurata delle comunalità. Al contrario, il metodo delle componenti principali assume implicitamente che tutta la varianza sia spiegata dai fattori comuni, trascurando la specificità.\n\n35.4.3 Vantaggi del Metodo Iterato\n\nMaggiore precisione:\nL’iterazione migliora progressivamente le stime delle comunalità, riducendo l’impatto di errori iniziali.\nAdattabilità:\nIl metodo può essere applicato sia alla matrice delle covarianze \\(\\textbf{S}\\) sia a quella delle correlazioni \\(\\textbf{R}\\), rendendolo flessibile a diversi contesti analitici.\nConvergenza:\nIl processo iterativo garantisce che le stime finali siano ottimali rispetto al modello specificato.\n\nIn un’applicazione pratica, il metodo dei fattori principali iterato viene solitamente implementato utilizzando funzioni predefinite disponibili in software statistici come R o altre piattaforme dedicate all’analisi dei dati.\nIn conclusione, il metodo dei fattori principali iterato rappresenta un miglioramento rispetto ai metodi non iterativi, poiché tiene conto delle specificità e consente una stima più accurata delle comunalità e dei fattori. Pur essendo più complesso, è particolarmente utile in contesti in cui la precisione delle stime è essenziale.\n\n35.4.4 Casi di Heywood\nUno degli inconvenienti del metodo dei fattori principali iterato è la possibilità di ottenere soluzioni inammissibili, note come casi di Heywood, che si verificano quando la matrice delle correlazioni \\(\\textbf{R}\\) viene fattorizzata e alcune comunalità stimate risultano maggiori di 1.\nSe \\(\\hat{h}^2_i &gt; 1\\) per una comunalità stimata, significa che la specificità \\(\\hat{\\psi}_i\\) è negativa, ossia:\n\\[\n\\hat{\\psi}_i = s_{ii} - \\hat{h}^2_i &lt; 0 .\n\\]\nQuesto è logicamente inaccettabile, poiché una varianza non può assumere valori negativi. Un caso del genere indica un problema nella soluzione iterativa, spesso dovuto a:\n\n\nInadeguatezza del modello fattoriale: il numero di fattori scelto potrebbe non essere sufficiente a rappresentare correttamente i dati.\n\nProblemi di multicollinearità: correlazioni molto alte tra le variabili possono complicare il processo di stima.\n\nRumore nei dati: errori di misura o dati imperfetti possono causare queste anomalie.\n\nQuando si verifica un caso di Heywood, il processo iterativo viene solitamente interrotto dal software, che segnala l’impossibilità di trovare una soluzione ammissibile.\nPer illustrare il problema, possiamo utilizzare la funzione fa() del pacchetto psych in R. Questa funzione implementa il metodo iterativo dei fattori principali.\n\n# Esecuzione del metodo dei fattori principali iterato\npa &lt;- fa(R, nfactors = 2, rotate = \"none\", fm = \"pa\")\npa\n#&gt; Factor Analysis using method =  pa\n#&gt; Call: fa(r = R, nfactors = 2, rotate = \"none\", fm = \"pa\")\n#&gt; Standardized loadings (pattern matrix) based upon correlation matrix\n#&gt;    PA1   PA2   h2     u2 com\n#&gt; K 0.98 -0.21 1.01 -0.008 1.1\n#&gt; I 0.48  0.74 0.77  0.230 1.7\n#&gt; H 0.78 -0.56 0.92  0.085 1.8\n#&gt; L 0.98 -0.19 0.99  0.010 1.1\n#&gt; J 0.69  0.69 0.95  0.049 2.0\n#&gt; \n#&gt;                        PA1  PA2\n#&gt; SS loadings           3.22 1.41\n#&gt; Proportion Var        0.64 0.28\n#&gt; Cumulative Var        0.64 0.93\n#&gt; Proportion Explained  0.70 0.30\n#&gt; Cumulative Proportion 0.70 1.00\n#&gt; \n#&gt; Mean item complexity =  1.5\n#&gt; Test of the hypothesis that 2 factors are sufficient.\n#&gt; \n#&gt; df null model =  10  with the objective function =  12\n#&gt; df of  the model are 1  and the objective function was  5.6 \n#&gt; \n#&gt; The root mean square of the residuals (RMSR) is  0.01 \n#&gt; The df corrected root mean square of the residuals is  0.04 \n#&gt; \n#&gt; Fit based upon off diagonal values = 1\n\nNel risultato ottenuto, è possibile osservare che alcune unicità (specificità, \\(\\hat{\\psi}_i\\)) sono negative. Questo è un chiaro segnale di una soluzione impropria, che si riconosce come un caso di Heywood.\n\n35.4.4.1 Come Gestire i Casi di Heywood\n\nAumentare il numero di fattori: Aggiungere un fattore al modello potrebbe aiutare a catturare una maggiore porzione della varianza totale, riducendo il rischio di comunalità superiori a 1.\n\nControllare la qualità dei dati:\n\nIdentificare ed eliminare eventuali errori di misura.\nValutare la presenza di variabili con correlazioni eccessivamente alte.\n\n\nApplicare regolarizzazioni: Alcuni metodi moderni di analisi fattoriale includono tecniche per evitare soluzioni improprie (ad esempio, vincolando le comunalità o utilizzando approcci bayesiani).\nInterpretare con cautela: In presenza di un caso di Heywood, è importante non accettare automaticamente la soluzione proposta dal modello. Una revisione critica del numero di fattori e della metodologia è essenziale.\n\nIn conclusione, i casi di Heywood rappresentano una limitazione importante del metodo dei fattori principali iterato, specialmente quando applicato a dati complessi o modelli non adeguati. Identificarli e gestirli correttamente è fondamentale per garantire la validità delle conclusioni tratte dall’analisi.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html#metodo-di-massima-verosimiglianza",
    "href": "chapters/extraction/02_estrazione.html#metodo-di-massima-verosimiglianza",
    "title": "35  L’estrazione dei fattori",
    "section": "\n35.5 Metodo di Massima Verosimiglianza",
    "text": "35.5 Metodo di Massima Verosimiglianza\nIl metodo di massima verosimiglianza è particolarmente indicato quando si può ragionevolmente assumere che le variabili manifeste seguano una distribuzione normale multivariata. In tali condizioni, il metodo produce stime dei pesi fattoriali e delle specificità che sono quelle più verosimili date le correlazioni osservate. Questo metodo è spesso preferito rispetto ad altri, a patto che le sue ipotesi di base siano pienamente soddisfatte.\nIl metodo di massima verosimiglianza minimizza una funzione di discrepanza \\(F\\), che misura la distanza tra la matrice di covarianze osservata \\(\\textbf{S}\\) (o la matrice di correlazioni \\(\\textbf{R}\\)) e quella predetta dal modello \\(\\textbf{M}\\). La funzione \\(F\\) può essere espressa come:\n\\[\nF(\\boldsymbol{\\Lambda}, \\boldsymbol{\\Psi}) = \\text{discrepanza tra } \\textbf{S} \\text{ e } \\textbf{M}.\n\\]\nMinimizzando \\(F\\), uguagliando a zero le derivate di \\(F\\) rispetto ai parametri del modello \\(\\boldsymbol{\\Lambda}\\) (pesi fattoriali) e \\(\\boldsymbol{\\Psi}\\) (specificità), si ottengono le equazioni per le stime di massima verosimiglianza:\n\\[\n\\hat{\\boldsymbol{\\Lambda}}, \\hat{\\boldsymbol{\\Psi}} .\n\\]\nQueste equazioni non hanno una soluzione analitica diretta, quindi si ricorre a metodi numerici iterativi per trovare le stime dei parametri. Durante l’iterazione, si cerca di minimizzare la discrepanza tra la matrice osservata e quella predetta dal modello.\n\n35.5.1 Caratteristiche del Metodo\n\nPrecisione delle stime:\nSe le ipotesi sono rispettate, le stime di massima verosimiglianza sono asintoticamente efficienti, ossia hanno la minima varianza possibile tra gli stimatori.\nIndipendenza dall’unità di misura:\nLa soluzione non dipende dall’unità di misura delle variabili manifeste. Questo significa che si ottiene la stessa soluzione analizzando la matrice di covarianze \\(\\textbf{S}\\) o quella di correlazioni \\(\\textbf{R}\\).\nTest di bontà di adattamento:\nLe stime di massima verosimiglianza consentono di eseguire un test chi-quadrato per valutare se la matrice predetta dal modello è coerente con quella osservata. Questo test offre una misura formale della bontà di adattamento del modello ai dati.\n\nLimitazioni:\n\n\nProblemi di convergenza: In alcuni casi, il processo iterativo può non convergere, specialmente con dati problematici o ipotesi non rispettate.\n\nCasi di Heywood: Analogamente al metodo dei fattori principali iterato, possono verificarsi casi di Heywood in cui alcune comunalità stimate risultano maggiori di 1.\n\n\n\n35.5.2 Applicazione Pratica\nPer calcolare le stime di massima verosimiglianza in R, è possibile utilizzare la funzione factanal(), che implementa questo metodo direttamente.\nConsideriamo i dati dell’esempio precedente e calcoliamo i parametri di massima verosimiglianza:\n\nfactanal(\n    covmat = R,        # Matrice di correlazioni\n    factors = 2,       # Numero di fattori\n    rotation = \"none\", # Nessuna rotazione\n    n.obs = 225        # Numero di osservazioni\n)\n#&gt; \n#&gt; Call:\n#&gt; factanal(factors = 2, covmat = R, n.obs = 225, rotation = \"none\")\n#&gt; \n#&gt; Uniquenesses:\n#&gt;     K     I     H     L     J \n#&gt; 0.005 0.268 0.055 0.008 0.005 \n#&gt; \n#&gt; Loadings:\n#&gt;   Factor1 Factor2\n#&gt; K  0.955  -0.289 \n#&gt; I  0.528   0.673 \n#&gt; H  0.720  -0.653 \n#&gt; L  0.954  -0.287 \n#&gt; J  0.764   0.642 \n#&gt; \n#&gt;                Factor1 Factor2\n#&gt; SS loadings      3.203   1.457\n#&gt; Proportion Var   0.641   0.291\n#&gt; Cumulative Var   0.641   0.932\n#&gt; \n#&gt; Test of the hypothesis that 2 factors are sufficient.\n#&gt; The chi square statistic is 648 on 1 degree of freedom.\n#&gt; The p-value is 5.81e-143\n\n\nPesi fattoriali:\nI pesi fattoriali stimati \\(\\hat{\\boldsymbol{\\Lambda}}\\) saranno molto simili a quelli ottenuti con il metodo dei fattori principali iterato.\nSpecificità:\nLe specificità \\(\\hat{\\boldsymbol{\\Psi}}\\) rifletteranno la porzione di varianza non spiegata dai fattori comuni.\nTest di bontà di adattamento:\nIl risultato include un test chi-quadrato che valuta la coerenza tra il modello e i dati osservati. Un valore di p elevato indica un buon adattamento del modello.\n\n35.5.3 Confronto con Altri Metodi\nIl metodo di massima verosimiglianza si distingue per la sua capacità di fornire stime ottimali e una valutazione formale dell’adattamento del modello. Tuttavia, richiede che le variabili manifeste seguano una distribuzione normale multivariata, il che potrebbe non essere sempre soddisfatto nei dati reali. Quando le ipotesi di normalità sono violate, metodi alternativi come la stima mediante minimi quadrati ponderati (WLS) potrebbero essere più appropriati.\nIn conclusione, il metodo di massima verosimiglianza rappresenta uno standard per l’analisi fattoriale, offrendo stime efficienti e strumenti per valutare l’adattamento del modello ai dati. Tuttavia, l’efficacia del metodo dipende dalla validità delle ipotesi di normalità e dalla qualità dei dati utilizzati.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html#riflessioni-conclusive",
    "href": "chapters/extraction/02_estrazione.html#riflessioni-conclusive",
    "title": "35  L’estrazione dei fattori",
    "section": "\n35.6 Riflessioni Conclusive",
    "text": "35.6 Riflessioni Conclusive\nL’analisi fattoriale rappresenta uno strumento fondamentale per la riduzione della dimensionalità e l’identificazione di strutture latenti nei dati. Ogni metodo analizzato, dal metodo delle componenti principali al metodo di massima verosimiglianza, offre vantaggi e limitazioni, risultando più o meno adatto a seconda delle caratteristiche del dataset e degli obiettivi dell’analisi.\nI metodi iterativi, come il metodo dei fattori principali iterato e quello di massima verosimiglianza, si distinguono per la loro capacità di fornire stime più precise, sebbene possano incontrare problemi di convergenza o casi di Heywood. La scelta del metodo più appropriato dipende quindi non solo dai vincoli teorici, come la normalità multivariata, ma anche dalla complessità dei dati e dalla necessità di strumenti formali per testare l’adattamento del modello.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/extraction/02_estrazione.html#informazioni-sullambiente-di-sviluppo",
    "title": "35  L’estrazione dei fattori",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] ggokabeito_0.1.0  see_0.10.0        MASS_7.3-65       viridis_0.6.5    \n#&gt;  [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n#&gt;  [9] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n#&gt; [13] lavaan_0.6-19     psych_2.4.12      scales_1.3.0      markdown_1.13    \n#&gt; [17] knitr_1.49        lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n#&gt; [21] dplyr_1.1.4       purrr_1.0.4       readr_2.1.5       tidyr_1.3.1      \n#&gt; [25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.0      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] mime_0.12           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [25] Matrix_1.7-2        R6_2.6.1            fastmap_1.2.0      \n#&gt;  [28] rbibutils_2.3       shiny_1.10.0        digest_0.6.37      \n#&gt;  [31] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n#&gt;  [34] rprojroot_2.0.4     Hmisc_5.2-2         timechange_0.3.0   \n#&gt;  [37] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n#&gt;  [40] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n#&gt;  [43] carData_3.0-5       ggsignif_0.6.4      corpcor_1.6.10     \n#&gt;  [46] gtools_3.9.5        tools_4.4.2         pbivnorm_0.6.0     \n#&gt;  [49] foreign_0.8-88      zip_2.3.2           httpuv_1.6.15      \n#&gt;  [52] nnet_7.3-20         glue_1.8.0          quadprog_1.5-8     \n#&gt;  [55] nlme_3.1-167        promises_1.3.2      lisrelToR_0.3      \n#&gt;  [58] grid_4.4.2          checkmate_2.3.2     cluster_2.1.8      \n#&gt;  [61] reshape2_1.4.4      generics_0.1.3      gtable_0.3.6       \n#&gt;  [64] tzdb_0.4.0          data.table_1.17.0   hms_1.1.3          \n#&gt;  [67] car_3.1-3           sem_3.1-16          pillar_1.10.1      \n#&gt;  [70] rockchalk_1.8.157   later_1.4.1         splines_4.4.2      \n#&gt;  [73] lattice_0.22-6      survival_3.8-3      kutils_1.73        \n#&gt;  [76] tidyselect_1.2.1    miniUI_0.1.1.1      pbapply_1.7-2      \n#&gt;  [79] reformulas_0.4.0    stats4_4.4.2        xfun_0.51          \n#&gt;  [82] qgraph_1.9.8        arm_1.14-4          stringi_1.8.4      \n#&gt;  [85] pacman_0.5.1        boot_1.3-31         evaluate_1.0.3     \n#&gt;  [88] codetools_0.2-20    mi_1.1              cli_3.6.4          \n#&gt;  [91] RcppParallel_5.1.10 rpart_4.1.24        xtable_1.8-4       \n#&gt;  [94] Rdpack_2.6.2        munsell_0.5.1       Rcpp_1.0.14        \n#&gt;  [97] coda_0.19-4.1       png_0.1-8           XML_3.99-0.18      \n#&gt; [100] parallel_4.4.2      jpeg_0.1-10         lme4_1.1-36        \n#&gt; [103] mvtnorm_1.3-3       openxlsx_4.2.8      rlang_1.1.5        \n#&gt; [106] multcomp_1.4-28     mnormt_2.1.1\n\n\n\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.\n\n\nRencher, A. (2002). Methods of multivariate analysis. 2002. Wiley Publications.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html",
    "href": "chapters/extraction/03_numero_fattori.html",
    "title": "36  Il numero dei fattori",
    "section": "",
    "text": "36.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nL’analisi fattoriale è un potente strumento statistico che ci permette di semplificare la complessità dei dati, identificando un numero ridotto di fattori latenti che spieghino le correlazioni tra un gran numero di variabili osservate. Immaginiamo di avere un questionario con sei aggettivi per misurare la personalità: loquace, assertivo, fantasioso, creativo, estroverso e intellettuale. Ci chiediamo: questi aggettivi misurano un unico tratto della personalità (ad esempio, l’apertura mentale) o più tratti distinti (come estroversione e apertura mentale)?\nDeterminare il numero ottimale di fattori latenti è un quesito fondamentale nell’analisi fattoriale. Aggiungere più fattori migliora sempre l’adattamento del modello ai dati, ma a scapito della parsimonia. Trovare il giusto equilibrio è cruciale per ottenere una soluzione interpretabile e significativa.\nL’analisi fattoriale esplorativa (EFA) è la tecnica statistica più utilizzata per affrontare questo problema. L’EFA permette di esplorare i dati e identificare le strutture latenti sottostanti. Tuttavia, la scelta del numero di fattori da estrarre rimane una decisione complessa, che richiede l’utilizzo di criteri statistici e di un’attenta valutazione teorica.\nMetodi per determinare il numero di fattori:\nLa scelta del numero di fattori ha importanti implicazioni per l’interpretazione dei risultati e per le decisioni successive. Un numero di fattori troppo basso può portare a una perdita di informazioni, mentre un numero troppo alto può rendere l’interpretazione difficile e poco parsimoniosa.\nIn definitiva, la scelta del numero di fattori è un processo iterativo che richiede una combinazione di criteri statistici e considerazioni teoriche. Non esiste una regola ferrea, ma piuttosto una serie di indicatori che, utilizzati congiuntamente, possono guidare il ricercatore verso una soluzione ottimale.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#introduzione",
    "href": "chapters/extraction/03_numero_fattori.html#introduzione",
    "title": "36  Il numero dei fattori",
    "section": "",
    "text": "Criteri basati sugli autovalori: Si basano sull’analisi degli autovalori della matrice di correlazione per identificare i fattori più importanti.\nCriteri informativi: Valutano l’adattamento del modello ai dati, penalizzando modelli troppo complessi.\nMetodi di simulazione: Confronta i risultati ottenuti con i dati reali con quelli ottenuti da dati simulati.\nMetodi esplorativi: Cercano di identificare strutture fattoriali semplici e interpretabili.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#tre-domande-sulla-dimensionalità-del-test",
    "href": "chapters/extraction/03_numero_fattori.html#tre-domande-sulla-dimensionalità-del-test",
    "title": "36  Il numero dei fattori",
    "section": "36.2 Tre domande sulla dimensionalità del test",
    "text": "36.2 Tre domande sulla dimensionalità del test\nSono presenti almeno tre questioni rilevanti riguardo la dimensionalità di un test.\n\nNumero di Dimensioni:\n\nLa prima questione riguarda il numero di dimensioni rifletto dagli item del test. Alcuni test riflettono una sola dimensione, mentre altri ne riflettono due o più. Questa questione è importante poiché ogni dimensione del test è probabile che venga valutata separatamente, necessitando ciascuna una propria analisi psicometrica.\n\nCorrelazione tra Dimensioni:\n\nLa seconda questione indaga se, in un test con più di una dimensione, queste dimensioni siano correlate tra loro. Alcuni test presentano diverse dimensioni che sono in qualche modo correlate, mentre altri hanno dimensioni essenzialmente indipendenti e non correlate. Questa questione è rilevante, in parte, perché la natura delle associazioni tra le dimensioni di un test ha implicazioni per la significatività del “punteggio totale” del test.\n\nNatura delle Dimensioni:\n\nLa terza questione si chiede quali sono le dimensioni in un test con più di una dimensione, ovvero, quali attributi psicologici sono riflessi dalle dimensioni del test? Ad esempio, nel test della personalità con sei aggettivi descritto precedentemente, la prima dimensione riflette l’attributo psicologico dell’estroversione o qualche altro attributo? L’importanza di questa questione è evidente: per valutare ed interpretare efficacemente una dimensione di un test, è necessario comprendere il significato psicologico del punteggio.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#metodi-basati-sugli-autovalori",
    "href": "chapters/extraction/03_numero_fattori.html#metodi-basati-sugli-autovalori",
    "title": "36  Il numero dei fattori",
    "section": "36.3 Metodi basati sugli autovalori",
    "text": "36.3 Metodi basati sugli autovalori\nSono stati proposti quattro criteri basati sugli autovalori per determinare il numero \\(m\\) di fattori da estrarre (Rencher, 2002).\n\nScegliere \\(m\\) tale per cui la varianza spiegata dal modello fattoriale superi una soglia predeterminata, per esempio l’80% della varianza totale, \\(tr(\\textbf{S})\\) o \\(tr(\\textbf{R})\\).\nScegliere \\(m\\) uguale al numero di autovalori aventi un valore maggiore del valore medio degli autovalori. Per R il valore medio degli autovalori è \\(1\\); per S è \\(\\sum_{j=1}^p \\theta_j/p\\).\nUsare lo scree test.\nMediante la statistica \\(\\chi^2\\), valutare l’ipotesi che \\(m\\) sia il numero corretto di fattori, \\(H_0: \\boldsymbol{\\Sigma} =  \\boldsymbol{\\Lambda}\n  \\boldsymbol{\\Lambda}^{\\mathsf{T}} +  \\boldsymbol{\\Psi}\\), dove \\(\\boldsymbol{\\Lambda}\\) è di ordine \\(p \\times m\\).\n\n\n36.3.1 Quota di varianza spiegata\nIl primo criterio si applica soprattutto al metodo delle componenti principali. La proporzione della varianza capionaria spiegata dal fattore \\(j\\)-esimo estratto da S è uguale a\n\\[\\sum_{i=i}^p \\hat{\\lambda}_{ij}^2 / tr(\\textbf{S}).\\]\nNel caso in cui i fattori vengano estratti da R avremo\n\\[\\sum_{i=i}^p \\hat{\\lambda}_{ij}^2 / p.\\]\nNel caso di fattori incorrelati, ciascun fattore contribuisce con una quota complessiva di varianza spiegata pari alla somma dei quadrati delle saturazioni fattoriali contenute nella matrice \\(\\hat{\\boldsymbol{\\Lambda}}\\): \\(\\sum_{i=1}^p\\sum_{j=1}^m\\hat{\\lambda}_{ij}^2\\). Nel caso del metodo delle componenti principali, tale somma è anche uguale alla somma dei primi \\(m\\) autovalori, o alla somma di tutte le \\(p\\) comunalità:\n\\[\\sum_{i=1}^p\\sum_{j=1}^m\\hat{\\lambda}_{ij}^2= \\sum_{i=1}^p \\hat{h}_i^2\n= \\sum_{j=1}^m \\theta_j\\]\nSulla base di queste considerazioni, il numero \\(m\\) di fattori viene scelto in modo da spiegare una quota sufficientemente grande di S o \\(p\\).\nIl numero dei fattori può essere determinato in questo modo anche nel caso in cui l’analisi fattoriale venga eseguita con il metodo dei fattori principali (ovvero, nel caso in cui vengano usate le stime delle comunalità per generare la matrice ridotta \\(\\textbf{S} -\n\\hat{\\boldsymbol{\\Psi}}\\) o \\(\\textbf{R} - \\hat{\\boldsymbol{\\Psi}}\\)). In questo caso, però, è possibile che alcuni autovalori della matrice \\(\\textbf{S} - \\hat{\\boldsymbol{\\Psi}}\\) o \\(\\textbf{R} -\n\\hat{\\boldsymbol{\\Psi}}\\) assumano valore negativo. In tali circostanze, è possibile che la proporzione cumulativa della varianza \\(\\sum_{j=1}^m \\theta_j / \\sum_{j=1}^p \\theta_j\\) assuma un valore maggiore di \\(1.0\\) per \\(j &lt; p\\).\nLa proporzione cumulativa della varianza si riduce poi a \\(1.0\\) quando vengono considerati anche i successivi autovalori negativi. Di conseguenza, può succedere che, utilizzando la matrice \\(\\textbf{S} - \\hat{\\boldsymbol{\\Psi}}\\) o \\(\\textbf{R} - \\hat{\\boldsymbol{\\Psi}}\\), il criterio definito in base alla quota della varianza spiegata venga raggiunto per un valore \\(m\\) minore di quello che verrebbe trovato utilizzando la matrice S o R.\nNel caso del metodo dei fattori principali iterato, \\(m\\) viene specificato precedentemente a ciascuna iterazione e \\(\\sum_{i}\n\\hat{h}^2_i\\) viene ottenuto dopo ciascuna iterazione calcolando \\(\\text{tr}(\\textbf{S} - \\hat{\\boldsymbol{\\Psi}})\\). Per scegliere \\(m\\), come per il metodo delle componenti principali, possono essere usati gli autovalori di S o R.\n\n\n36.3.2 Valore medio degli autovalori\nIl calcolo del valore medio degli autovalori è una procedura euristica implementata in molti software. In una variante di tale metodo, \\(m\\) viene scelto in modo tale da uguagliare il numero degli autovalori positivi della matrice ridotta \\(\\textbf{R} - \\hat{\\boldsymbol{\\Psi}}\\) (in tale matrice vi sono solitamente degli autovalori negativi). Tale variante ha però lo svantaggio di produrre solitamente un numero di fattori troppo grande.\n\n\n36.3.3 Scree test\nLo scree test si basa su un grafico che rappresenta gli autovalori di S o R ordinati in modo decrescente in funzione del numero dei fattori. I punti che rappresentano gli autovalori vengono collegati con una spezzata. Il valore m viene determinato in corrispondenza del fattore oltre il quale il dislivello tra fattori successivi diventa esiguo e la spezzata tende a diventare orizzontale.\n\n\n36.3.4 Parallel analysis\nLa Parallel Analysis si basa sul confronto tra gli autovalori empirici della matrice di correlazione delle variabili originali e quelli generati da un campione casuale di variabili standardizzate. In questo modo si tiene conto delle variazioni dovute agli errori di campionamento. Poiché anche in presenza di variabili incorrelate la matrice di correlazione presenta sempre autovalori maggiori di uno a causa della variabilità campionaria, il confronto tra gli autovalori empirici e quelli generati dalla Parallel Analysis permette di individuare il numero di fattori significativi. Una simulazione di Monte Carlo su una matrice di correlazione di \\(p=10\\) variabili casuali mutuamente indipendenti, ciascuna con \\(n=20\\) osservazioni, può essere utilizzata per illustrare la procedura.\n\nn &lt;- 20\nnsim &lt;- 1000\ne1 &lt;- rep(0, nsim)\nfor (i in 1:nsim) {\n  Y &lt;- cbind(\n    rnorm(n), rnorm(n), rnorm(n), rnorm(n), rnorm(n),\n    rnorm(n), rnorm(n), rnorm(n), rnorm(n), rnorm(n)\n  )\n  e &lt;- eigen(cor(Y))\n  e1[i] &lt;- e$values[1]\n}\nmax(e1)\n\n3.34528401303822\n\n\nPer i dati di questa simulazione, l’autovalore maggiore ha un valore pari a \\(3.35\\), anche se i dati sono del tutto casuali. La Parallel Analysis tiene conto di questo fatto e determina \\(m\\) confrontando gli autovalori empirici con le loro “controparti casuali.” Vanno a determinare \\(m\\) solo gli autovalori empirici che hanno un valore superiore ai corrispondenti autovalori generati da una matrice di dati dello stesso ordine composta da colonne mutualmente incorrelate. Nel caso di questa simulazione di Monte Carlo, se l’autovalore maggiore derivato da una matrice di numeri casuali ha un valore di \\(3.35\\), verranno considerati solo gli autovalori empirici che superano questo valore per determinare il numero di fattori.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#metodi-basati-sul-confronto-tra-modelli",
    "href": "chapters/extraction/03_numero_fattori.html#metodi-basati-sul-confronto-tra-modelli",
    "title": "36  Il numero dei fattori",
    "section": "36.4 Metodi basati sul confronto tra modelli",
    "text": "36.4 Metodi basati sul confronto tra modelli\nIl confronto tra modelli può essere eseguito usando varie statistiche. Una scelta popolare per stimare il numero di fattori nella EFA è il Criterio d’Informazione Bayesiano (BIC; Schwarz, 1978), introdotto come un miglioramento rispetto al Criterio d’Informazione di Akaike (AIC; Akaike, 1973). Un’alternativa è l’indice RMSEA, che può essere considerato come una stima della mancanza di adattamento che tiene in considerazione i gradi di libertà del modello (Browne e Cudeck, 1992). Un altro metodo di questo tipo è il test Minimum Average Partial (MAP), che stima le correlazioni parziali residue medie per diversi numeri di fattori e seleziona quello con il valore più basso (Velicer, 1976).\n\n36.4.1 Test del rapporto di verosimiglianze\nIn questo test si confrontano due ipotesi: l’ipotesi nulla \\(H_0\\) e l’ipotesi alternativa \\(H_1\\), per valutare la bontà di adattamento di un modello fattoriale a una matrice di covarianza delle variabili oggetto di osservazione campionaria \\(Y\\). L’ipotesi nulla postula che la struttura di interdipendenza di \\(Y\\) può essere spiegata da \\(m\\) fattori comuni, mentre l’alternativa postula che i fattori comuni non sono sufficienti per spiegare la matrice di covarianza \\(\\boldsymbol{\\Sigma}\\).\nIl test si basa sulla statistica del chi-quadrato con gradi di libertà pari a \\(\\nu = \\frac{1}{2}[(p-m)^2-(p-m)]\\), dove \\(p\\) è il numero di variabili e \\(m\\) è il numero di fattori. In pratica, si inizia con \\(m^*=1\\) e si valuta l’ipotesi \\(H_0\\) per \\(m^*\\). Se \\(H_0\\) non viene rifiutata, il procedimento si arresta. In caso contrario, si considera \\(m^*+1\\) e si ripete il test finché \\(H_0\\) viene accettata o finché si raggiunge il valore minimo di gradi di libertà pari a zero.\nIl test del rapporto di verosimiglianze è particolarmente indicato quando il numero di osservazioni è grande, ma la sua applicazione è limitata dalle dimensioni del campione. In alternativa, è possibile utilizzare gli indici AIC, BIC e RMSEA per scegliere la soluzione con il valore più piccolo di tali statistiche. Tuttavia, questi indici non forniscono un test statistico per il confronto tra modelli.\nIn pratica, si può considerare il valore \\(m\\) indicato dal test come il limite superiore del numero di fattori che sono importanti dal punto di vista pratico.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#minimizzazione-dellout-of-sample-prediction-error",
    "href": "chapters/extraction/03_numero_fattori.html#minimizzazione-dellout-of-sample-prediction-error",
    "title": "36  Il numero dei fattori",
    "section": "36.5 Minimizzazione dell’out-of-sample prediction error",
    "text": "36.5 Minimizzazione dell’out-of-sample prediction error\nRecentemente è stato proposto un nuovo metodo per stimare il numero di fattori in EFA che affronta il problema come un problema di selezione del modello Haslbeck & Bork (2022). L’obiettivo è confrontare i modelli con 1, 2, …, p fattori, dove \\(p\\) è il numero di variabili, e selezionare il modello con l’errore di previsione atteso più basso nella popolazione. Tuttavia, questo è un compito non banale perché il modello che minimizza l’errore di previsione nel campione non minimizza sempre l’errore di previsione nella popolazione.\nIntuitivamente, questo problema viene affrontato suddividendo il campione di dati in due insiemi: un set di training e un set di test. Il set di training viene utilizzato per stimare i parametri del modello, le cui previsioni vengono poi verificate utilizzando i dati di test (non utilizzati per la stima dei parametri). Questo calcolo dell’errore di previsione fuori campione viene ripetuto diverse volte, suddividendo ogni volta in modo casuale il campione negli insiemi di training e test. Tale metodo per stimare il numero di fattori è implementato nel pacchetto R fspe.\nEsempio. Per confrontare i metodi discussi per la scelta del numero \\(m\\) di fattori usiamo una matrice di correlazioni calcolata sulle sottoscale della WAIS. Le 11 sottoscale del test sono le seguenti:\n\nX1 = Information\nX2 = Comprehension\nX3 = Arithmetic\nX4 = Similarities\nX5 = Digit.span\nX6 = Vocabulary\nX7 = Digit.symbol\nX8 = Picture.completion\nX9 = Block.design\nX10 = Picture.arrangement\nX11 = Object.\n\nI dati sono stati ottenuti dal manuale della III edizione.\n\nvarnames &lt;- c(\n    \"IN\", \"CO\", \"AR\", \"SI\", \"DS\", \"VO\", \"SY\", \"PC\",\n    \"BD\", \"PA\", \"OA\", \"AG\", \"ED\"\n)\ntemp &lt;- matrix(c(\n    1, 0.67, 0.62, 0.66, 0.47, 0.81, 0.47, 0.60, 0.49, 0.51, 0.41,\n    -0.07, 0.66, .67, 1, 0.54, 0.60, 0.39, 0.72, 0.40, 0.54, 0.45,\n    0.49, 0.38, -0.08, 0.52, .62, .54, 1, 0.51, 0.51, 0.58, 0.41,\n    0.46, 0.48, 0.43, 0.37, -0.08, 0.49, .66, .60, .51, 1, 0.41,\n    0.68, 0.49, 0.56, 0.50, 0.50, 0.41, -0.19, 0.55, .47, .39, .51,\n    .41, 1, 0.45, 0.45, 0.42, 0.39, 0.42, 0.31, -0.19, 0.43,\n    .81, .72, .58, .68, .45, 1, 0.49, 0.57, 0.46, 0.52, 0.40, -0.02,\n    0.62, .47, .40, .41, .49, .45, .49, 1, 0.50, 0.50, 0.52, 0.46,\n    -0.46, 0.57, .60, .54, .46, .56, .42, .57, .50, 1, 0.61, 0.59,\n    0.51, -0.28, 0.48, .49, .45, .48, .50, .39, .46, .50, .61, 1,\n    0.54, 0.59, -0.32, 0.44, .51, .49, .43, .50, .42, .52, .52, .59,\n    .54, 1, 0.46, -0.37, 0.49, .41, .38, .37, .41, .31, .40, .46, .51,\n    .59, .46, 1, -0.28, 0.40, -.07, -.08, -.08, -.19, -.19, -.02,\n    -.46, -.28, -.32, -.37, -.28, 1, -0.29, .66, .52, .49, .55, .43,\n    .62, .57, .48, .44, .49, .40, -.29, 1\n), nrow = 13, ncol = 13, byrow = TRUE)\n\ncolnames(temp) &lt;- varnames\nrownames(temp) &lt;- varnames\n\nwais_cor &lt;- temp[1:11, 1:11]\nwais_cor\n\n\nA matrix: 11 x 11 of type dbl\n\n\n\nIN\nCO\nAR\nSI\nDS\nVO\nSY\nPC\nBD\nPA\nOA\n\n\n\n\nIN\n1.00\n0.67\n0.62\n0.66\n0.47\n0.81\n0.47\n0.60\n0.49\n0.51\n0.41\n\n\nCO\n0.67\n1.00\n0.54\n0.60\n0.39\n0.72\n0.40\n0.54\n0.45\n0.49\n0.38\n\n\nAR\n0.62\n0.54\n1.00\n0.51\n0.51\n0.58\n0.41\n0.46\n0.48\n0.43\n0.37\n\n\nSI\n0.66\n0.60\n0.51\n1.00\n0.41\n0.68\n0.49\n0.56\n0.50\n0.50\n0.41\n\n\nDS\n0.47\n0.39\n0.51\n0.41\n1.00\n0.45\n0.45\n0.42\n0.39\n0.42\n0.31\n\n\nVO\n0.81\n0.72\n0.58\n0.68\n0.45\n1.00\n0.49\n0.57\n0.46\n0.52\n0.40\n\n\nSY\n0.47\n0.40\n0.41\n0.49\n0.45\n0.49\n1.00\n0.50\n0.50\n0.52\n0.46\n\n\nPC\n0.60\n0.54\n0.46\n0.56\n0.42\n0.57\n0.50\n1.00\n0.61\n0.59\n0.51\n\n\nBD\n0.49\n0.45\n0.48\n0.50\n0.39\n0.46\n0.50\n0.61\n1.00\n0.54\n0.59\n\n\nPA\n0.51\n0.49\n0.43\n0.50\n0.42\n0.52\n0.52\n0.59\n0.54\n1.00\n0.46\n\n\nOA\n0.41\n0.38\n0.37\n0.41\n0.31\n0.40\n0.46\n0.51\n0.59\n0.46\n1.00\n\n\n\n\n\nIl primo metodo per la determinazione di \\(m\\) richiede di estrarre tanti fattori quanti sono necessari per spiegare una quota predeterminata della varianza totale. Supponiamo di porre il criterio pari all’80% della varianza totale.\n\nout &lt;- eigen(wais_cor)\nsum(out$val[1:4]) / sum(out$val)\nsum(out$val[1:5]) / sum(out$val)\n\n0.765678107076221\n\n\n0.811885250571924\n\n\nLa soluzione ottenuta in questo modo ci porterebbe a mantenere \\(m=5\\) fattori.\nIl secondo metodo suggerisce di mantenere tutti gli autovalori superiori al valore medio degli autovalori (che, nel caso di R è uguale a \\(1\\)).\n\nprint(round(out$values, 3))\n\n [1] 6.074 1.015 0.746 0.587 0.508 0.431 0.423 0.377 0.351 0.310 0.177\n\n\nNel caso presente, \\(m=2\\).\nLo scree test può essere eseguito creando il grafico seguente.\n\nn &lt;- dim(wais_cor)[1]\nscree_tb &lt;- tibble(\n    x = 1:n,\n    y = sort(eigen(wais_cor)$value, decreasing = TRUE)\n)\n\nscree_plot &lt;- scree_tb |&gt;\n  ggplot(aes(x, y)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = 1:n) +\n  ggtitle(\"Scree plot\")\n\nscree_plot\n\n\n\n\n\n\n\n\nLo scree test suggerisce la presenza di un unico fattore comune.\nLa versione della Parallel Analysis può essere eseguita con la funzione paran() contenuta nel pacchetto paran.\n\nparan(wais_cor, graph = TRUE)\n\n\nUsing eigendecomposition of correlation matrix.\nComputing: 10%  20%  30%  40%  50%  60%  70%  80%  90%  100%\n\n\nResults of Horn's Parallel Analysis for component retention\n330 iterations, using the mean estimate\n\n-------------------------------------------------- \nComponent   Adjusted    Unadjusted    Estimated \n            Eigenvalue  Eigenvalue    Bias \n-------------------------------------------------- \n1           1.647667    3.765744      2.118077\n-------------------------------------------------- \n\nAdjusted eigenvalues &gt; 1 indicate dimensions to retain.\n(1 components retained)\n\n\n\n\n\n\n\n\n\n\nLa Parallel Analysis indica una soluzione a \\(m=1\\) fattore.\nIl test inferenziale relativo al numero di fattori basato sulla statistica \\(\\chi^2\\) può essere eseguito nel modo seguente.\n\nfactanal(covmat=wais_cor, factors=4, n.obs=933)\n\n\nCall:\nfactanal(factors = 4, covmat = wais_cor, n.obs = 933)\n\nUniquenesses:\n   IN    CO    AR    SI    DS    VO    SY    PC    BD    PA    OA \n0.229 0.387 0.005 0.416 0.645 0.137 0.005 0.375 0.331 0.492 0.519 \n\nLoadings:\n   Factor1 Factor2 Factor3 Factor4\nIN 0.758   0.306   0.279   0.157  \nCO 0.672   0.312   0.229   0.107  \nAR 0.368   0.247   0.886   0.120  \nSI 0.602   0.376   0.193   0.207  \nDS 0.315   0.288   0.331   0.252  \nVO 0.851   0.242   0.208   0.192  \nSY 0.238   0.359   0.144   0.888  \nPC 0.432   0.623   0.143   0.172  \nBD 0.237   0.733   0.217   0.168  \nPA 0.367   0.539   0.150   0.245  \nOA 0.207   0.620   0.133   0.190  \n\n               Factor1 Factor2 Factor3 Factor4\nSS loadings      2.826   2.264   1.233   1.137\nProportion Var   0.257   0.206   0.112   0.103\nCumulative Var   0.257   0.463   0.575   0.678\n\nTest of the hypothesis that 4 factors are sufficient.\nThe chi square statistic is 35.4 on 17 degrees of freedom.\nThe p-value is 0.00551 \n\n\n\nfactanal(covmat = wais_cor, factors = 5, n.obs = 933)\n\n\nCall:\nfactanal(factors = 5, covmat = wais_cor, n.obs = 933)\n\nUniquenesses:\n   IN    CO    AR    SI    DS    VO    SY    PC    BD    PA    OA \n0.235 0.389 0.117 0.419 0.600 0.109 0.277 0.308 0.334 0.472 0.456 \n\nLoadings:\n   Factor1 Factor2 Factor3 Factor4 Factor5\nIN  0.745   0.264   0.301   0.192   0.118 \nCO  0.667   0.278   0.244   0.129   0.111 \nAR  0.378   0.236   0.814   0.145         \nSI  0.591   0.332   0.207   0.252   0.121 \nDS  0.288   0.208   0.366   0.341   0.155 \nVO  0.865   0.216   0.207   0.229         \nSY  0.251   0.364   0.153   0.708         \nPC  0.425   0.548   0.156   0.216   0.375 \nBD  0.246   0.708   0.230   0.201   0.107 \nPA  0.355   0.457   0.163   0.325   0.245 \nOA  0.211   0.664   0.128   0.205         \n\n               Factor1 Factor2 Factor3 Factor4 Factor5\nSS loadings      2.799   1.986   1.176   1.043   0.280\nProportion Var   0.254   0.181   0.107   0.095   0.025\nCumulative Var   0.254   0.435   0.542   0.637   0.662\n\nTest of the hypothesis that 5 factors are sufficient.\nThe chi square statistic is 12.5 on 10 degrees of freedom.\nThe p-value is 0.256 \n\n\nIl test del \\(\\chi^2\\) indica una soluzione a sei fattori.\nPer concludere, si potrebbe usare il metodo basato sulla minimizzazione dell’errore di previsione. Tuttavia, non possiamo applicare tale metodo ai dati dell’esempio in quanto sarebbe necessario disporre dei dati grezzi (la matrice di correlazioni non è sufficiente). Allo scopo di illustrare la procedura relativa al metodo basato sulla minimizzazione dell’errore di previsione useremo qui un set di dati diverso, ovvero holzinger19.\n\ndata(holzinger19)\n\nsuppressWarnings(\n    fspe_out &lt;- fspe(\n        data = holzinger19,\n        maxK = 10,\n        nfold = 10,\n        rep = 10,\n        method = \"PE\"\n    )\n)\n\n  |                                                                  |   0%\n\n\n  |------------------------------------------------------------------| 100%\n\n\n\npar(mar=c(4,4,1,1))\nplot.new()\nplot.window(xlim=c(1, 10), ylim=c(.6, .8))\naxis(1, 1:10)\naxis(2, las=2)\nabline(h=min(fspe_out$PEs), col=\"grey\")\nlines(fspe_out$PEs, lty=2)\npoints(fspe_out$PEs, pch=20, cex=1.5)\ntitle(xlab=\"Number of Factors\", ylab=\"Prediction Error\")\n\n\n\n\n\n\n\n\nPer i dati holzinger19, il metodo di {cite:t}haslbeck2022estimating produce dunque una soluzione a 4 fattori.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#la-replicabilità-delle-strutture-fattoriali",
    "href": "chapters/extraction/03_numero_fattori.html#la-replicabilità-delle-strutture-fattoriali",
    "title": "36  Il numero dei fattori",
    "section": "36.6 La replicabilità delle strutture fattoriali",
    "text": "36.6 La replicabilità delle strutture fattoriali\nUn aspetto critico dell’analisi fattoriale è la sua capacità di produrre risultati replicabili. Ciò significa che la struttura fattoriale identificata in un campione dovrebbe essere sostanzialmente simile quando replicata in un campione indipendente. Tuttavia, la cross-validazione delle strutture fattoriali spesso presenta difficoltà.\nL’Analisi Fattoriale Confirmatoria (CFA) offre una soluzione a questo problema. La CFA permette di testare una struttura fattoriale predefinita in un nuovo campione, verificando se i dati empirici supportano la struttura teorica ipotizzata.\nLa replicabilità è un indicatore importante della validità di una struttura fattoriale. Essa fornisce evidenze sulla robustezza e sulla generalizzabilità dei risultati ottenuti. Una struttura fattoriale che si replica consistentemente in diversi campioni è più probabile che rifletta una vera struttura sottostante piuttosto che caratteristiche specifiche di un singolo campione.\nCi sono diversi approcci per valutare la replicabilità:\n\nReplicazione in campioni indipendenti: Applicare la stessa analisi fattoriale a campioni diversi e confrontare i risultati.\nValidazione incrociata: Dividere un ampio campione in sottocampioni e confrontare le strutture fattoriali ottenute.\nCFA: Utilizzare i risultati di un’analisi fattoriale esplorativa come base per un modello confermatorio da testare su nuovi dati.\nMetodi di ricampionamento: Tecniche come il bootstrap per valutare la stabilità della soluzione fattoriale.\n\nIn conclusione, la replicabilità è fondamentale per stabilire la validità e l’utilità di una struttura fattoriale. La CFA, insieme ad altri metodi, rappresenta uno strumento essenziale per valutare la replicabilità e la generalizzabilità dei risultati ottenuti con l’analisi fattoriale.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#cosa-fare-con-i-fattori",
    "href": "chapters/extraction/03_numero_fattori.html#cosa-fare-con-i-fattori",
    "title": "36  Il numero dei fattori",
    "section": "36.7 Cosa fare con i fattori",
    "text": "36.7 Cosa fare con i fattori\nUna volta identificati i fattori attraverso l’analisi fattoriale, sorge la domanda su come utilizzarli efficacemente. Questo aspetto è particolarmente rilevante nel contesto dei Modelli di Equazioni Strutturali (SEM), dove i fattori assumono un significato sostanziale e possono essere impiegati in vari ruoli all’interno del modello.\n\n36.7.1 Utilizzo dei fattori nei SEM\nNei Modelli di Equazioni Strutturali, i fattori possono essere utilizzati in diversi modi:\n\nPredittori: I fattori possono fungere da variabili indipendenti per predire altri costrutti nel modello.\nMediatori: Possono essere utilizzati per spiegare il meccanismo attraverso cui una variabile influenza un’altra.\nModeratori: Possono essere impiegati per esaminare come la relazione tra due variabili cambia in funzione del livello del fattore moderatore.\nEsiti: Possono rappresentare le variabili dipendenti del modello, influenzate da altri costrutti.\n\nUn vantaggio significativo dell’utilizzo dei fattori latenti nei SEM è la loro capacità di “disattenuare” le associazioni per l’errore di misurazione. Questo significa che le relazioni stimate tra i costrutti sono più accurate, poiché l’errore di misurazione viene esplicitamente modellato e separato dalla varianza “vera” del costrutto.\n\n\n36.7.2 Sfide nell’uso dei fattori al di fuori dei SEM\nQuando si cerca di utilizzare i fattori al di fuori del contesto dei SEM, emergono alcune problematiche:\n\nSomma o media semplice: Molti ricercatori, dopo aver identificato che tre variabili saturano su un Fattore A, tendono a combinarle semplicemente sommandole o calcolandone la media. Questa pratica, tuttavia, non è accurata poiché ignora i pesi fattoriali (factor loadings) e l’errore di misurazione.\nCompositi lineari ponderati: Un approccio più sofisticato consiste nel creare un composito lineare, sommando le variabili ponderate per i loro pesi fattoriali. Questo metodo preserva le differenze nelle correlazioni tra le variabili e il fattore, ma continua a ignorare l’errore stimato. Di conseguenza, potrebbe non essere pienamente generalizzabile o significativo.\nCompositi a pesi unitari: In alcuni casi, assegnare lo stesso peso a tutte le variabili (compositi a pesi unitari) potrebbe risultare più generalizzabile rispetto ai compositi ponderati. Questo perché parte della variabilità nei pesi fattoriali potrebbe riflettere errori di campionamento piuttosto che differenze reali nell’importanza delle variabili. Si noti che il risultato numerico è identico tra la somma semplice e il composito a pesi unitari – la media semplice differisce solo per una costante (la divisione per il numero di variabili), che non altera le relazioni tra i punteggi. La differenza principale sta nel ragionamento sottostante e nel contesto metodologico: La somma/media semplice è spesso usata senza considerare l’analisi fattoriale. I compositi a pesi unitari sono una scelta consapevole basata sui risultati dell’analisi fattoriale, decidendo di trattare tutte le variabili con uguale importanza.\n\n\n\n36.7.3 Considerazioni aggiuntive\n\nInterpretabilità: I fattori latenti nei SEM offrono una rappresentazione più “pura” del costrutto sottostante, ma possono essere meno intuitivi da interpretare rispetto a punteggi compositi.\nBilanciamento tra precisione e praticità: Mentre l’uso di fattori latenti nei SEM offre maggiore precisione, in alcuni contesti (ad esempio, nella pratica clinica o nella ricerca applicata) potrebbe essere necessario un compromesso tra precisione statistica e facilità d’uso.\nStabilità cross-campione: È importante valutare la stabilità della struttura fattoriale attraverso diversi campioni prima di utilizzare i fattori in analisi successive.\nMetodi avanzati: Tecniche come la regressione con variabili latenti o l’uso di punteggi fattoriali salvati dai SEM possono offrire un compromesso tra la precisione dei modelli SEM completi e la necessità di punteggi compositi.\n\nIn conclusione, mentre i fattori offrono potenti strumenti per la modellazione di costrutti latenti, il loro utilizzo richiede una comprensione approfondita delle loro proprietà statistiche e delle implicazioni delle diverse strategie di operazionalizzazione. La scelta del metodo più appropriato dipenderà dagli obiettivi specifici della ricerca, dal contesto di applicazione e dalle caratteristiche dei dati disponibili.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#riflessioni-conclusive",
    "href": "chapters/extraction/03_numero_fattori.html#riflessioni-conclusive",
    "title": "36  Il numero dei fattori",
    "section": "36.8 Riflessioni Conclusive",
    "text": "36.8 Riflessioni Conclusive\nIn generale, la scelta del numero di fattori \\(m\\) non è sempre ovvia e rappresenta un limite dell’analisi fattoriale. Per affrontare questo problema, tradizionalmente si utilizza uno strumento come lo scree test per valutare la proporzione di varianza spiegata di ciascun item e l’interpretabilità della soluzione ottenuta dopo una rotazione adeguata. Tuttavia, poiché la scelta di \\(m\\) è soggettiva, i limiti della soluzione ottenuta sono evidenti. In alcuni casi, la scelta di \\(m\\) può essere più certa quando tutti i metodi forniscono la stessa risposta. Un’alternativa più moderna potrebbe essere l’uso di un metodo basato sulla minimizzazione dell’errore di previsione come quello descritto da Haslbeck & Bork (2022). In questo modo, si potrebbe ottenere una soluzione più affidabile e oggettiva.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#session-info",
    "href": "chapters/extraction/03_numero_fattori.html#session-info",
    "title": "36  Il numero dei fattori",
    "section": "36.9 Session Info",
    "text": "36.9 Session Info\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] fspe_0.1.2        paran_1.5.3       MASS_7.3-61       viridis_0.6.5    \n [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n [9] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n[13] lavaan_0.6-19     psych_2.4.6.26    scales_1.3.0      markdown_1.13    \n[17] knitr_1.49        lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1    \n[21] dplyr_1.1.4       purrr_1.0.2       readr_2.1.5       tidyr_1.3.1      \n[25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.17.1    jsonlite_1.8.9       magrittr_2.0.3      \n  [4] TH.data_1.1-2        estimability_1.5.1   farver_2.1.2        \n  [7] nloptr_2.1.1         rmarkdown_2.29       vctrs_0.6.5         \n [10] Cairo_1.6-2          minqa_1.2.8          base64enc_0.1-3     \n [13] rstatix_0.7.2        htmltools_0.5.8.1    broom_1.0.7         \n [16] Formula_1.2-5        htmlwidgets_1.6.4    plyr_1.8.9          \n [19] sandwich_3.1-1       emmeans_1.10.5       zoo_1.8-12          \n [22] uuid_1.2-1           igraph_2.1.1         mime_0.12           \n [25] lifecycle_1.0.4      pkgconfig_2.0.3      Matrix_1.7-1        \n [28] R6_2.5.1             fastmap_1.2.0        shiny_1.9.1         \n [31] digest_0.6.37        OpenMx_2.21.13       fdrtool_1.2.18      \n [34] colorspace_2.1-1     rprojroot_2.0.4      Hmisc_5.2-0         \n [37] labeling_0.4.3       fansi_1.0.6          timechange_0.3.0    \n [40] abind_1.4-8          compiler_4.4.2       withr_3.0.2         \n [43] glasso_1.11          htmlTable_2.4.3      backports_1.5.0     \n [46] carData_3.0-5        ggsignif_0.6.4       GPArotation_2024.3-1\n [49] corpcor_1.6.10       gtools_3.9.5         tools_4.4.2         \n [52] pbivnorm_0.6.0       foreign_0.8-87       zip_2.3.1           \n [55] httpuv_1.6.15        nnet_7.3-19          glue_1.8.0          \n [58] quadprog_1.5-8       promises_1.3.0       nlme_3.1-166        \n [61] lisrelToR_0.3        grid_4.4.2           pbdZMQ_0.3-13       \n [64] checkmate_2.3.2      cluster_2.1.6        reshape2_1.4.4      \n [67] generics_0.1.3       gtable_0.3.6         tzdb_0.4.0          \n [70] data.table_1.16.2    hms_1.1.3            car_3.1-3           \n [73] utf8_1.2.4           sem_3.1-16           pillar_1.9.0        \n [76] IRdisplay_1.1        rockchalk_1.8.157    later_1.3.2         \n [79] splines_4.4.2        cherryblossom_0.1.0  lattice_0.22-6      \n [82] survival_3.7-0       kutils_1.73          tidyselect_1.2.1    \n [85] miniUI_0.1.1.1       pbapply_1.7-2        airports_0.1.0      \n [88] stats4_4.4.2         xfun_0.49            qgraph_1.9.8        \n [91] arm_1.14-4           stringi_1.8.4        pacman_0.5.1        \n [94] boot_1.3-31          evaluate_1.0.1       codetools_0.2-20    \n [97] mi_1.1               cli_3.6.3            RcppParallel_5.1.9  \n[100] IRkernel_1.3.2       rpart_4.1.23         xtable_1.8-4        \n[103] repr_1.1.7           munsell_0.5.1        Rcpp_1.0.13-1       \n[106] coda_0.19-4.1        png_0.1-8            XML_3.99-0.17       \n[109] parallel_4.4.2       usdata_0.3.1         jpeg_0.1-10         \n[112] lme4_1.1-35.5        mvtnorm_1.3-2        openxlsx_4.2.7.1    \n[115] crayon_1.5.3         openintro_2.5.0      rlang_1.1.4         \n[118] multcomp_1.4-26      mnormt_2.1.1        \n\n\n\n\n\n\nHaslbeck, J., & Bork, R. van. (2022). Estimating the number of factors in exploratory factor analysis via out-of-sample prediction errors. Psychological Methods.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/04_rotazione.html",
    "href": "chapters/extraction/04_rotazione.html",
    "title": "37  La rotazione fattoriale",
    "section": "",
    "text": "37.1 Indeterminatezza della soluzione fattoriale\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nNel Capitolo 36 abbiamo determinato il numero ottimale di fattori comuni. Tuttavia, la soluzione iniziale, non ruotata, può risultare difficile da interpretare. Per ottenere una struttura fattoriale più semplice e intuitiva, si procede alla rotazione degli assi fattoriali. L’obiettivo della rotazione è quello di identificare gruppi omogenei di variabili che saturano fortemente su un singolo fattore e con saturazioni trascurabili sugli altri.\nLa necessità di effettuare la rotazione deriva dal fatto che la matrice delle saturazioni non ha un’unica soluzione. Attraverso trasformazioni matematiche, è possibile ottenere infinite matrici dello stesso ordine. Questo fenomeno è noto come indeterminatezza della soluzione fattoriale.\nLa matrice delle saturazioni fattoriali \\(\\boldsymbol{\\Lambda}\\) non è univocamente definita poiché non esiste una soluzione unica per determinare le saturazioni fattoriali. Una matrice di correlazioni \\(\\boldsymbol{R}\\) può produrre diverse soluzioni fattoriali, ovvero matrici con lo stesso numero di fattori comuni ma con diverse configurazioni di saturazioni fattoriali, o matrici di saturazioni fattoriali corrispondenti a un diverso numero di fattori comuni.\nEsempio. Siano \\(\\boldsymbol{\\Lambda}_1\\) e \\(\\boldsymbol{\\Lambda}_2\\) due matrici aventi lo stesso numero di righe e colonne, ma contenenti saturazioni fattoriali diverse. \\(\\boldsymbol{\\Lambda}_1\\) è definita dai valori seguenti\nl1 &lt;- matrix(\n  c(\n    0.766,  -0.232,\n    0.670,  -0.203,\n    0.574,  -0.174,\n    0.454,   0.533,\n    0.389,   0.457,\n    0.324,   0.381\n  ),\n  byrow = TRUE, ncol = 2\n)\nmentre per \\(\\boldsymbol{\\Lambda}_2\\) abbiamo\nl2 &lt;- matrix(\n  c(\n    0.783,  0.163,\n    0.685,  0.143,\n    0.587,  0.123,\n    0.143,  0.685,\n    0.123,  0.587,\n    0.102,  0.489\n  ),\n  byrow = TRUE, ncol = 2\n)\nEsaminiamo la matrice delle correlazioni riprodotte dalle due matrici di pesi fattoriali (con le comunalità sulla diagonale di \\(\\boldsymbol{R}\\)):\nl1 %*% t(l1) |&gt; round(2)\n\n\nA matrix: 6 x 6 of type dbl\n\n\n0.64\n0.56\n0.48\n0.22\n0.19\n0.16\n\n\n0.56\n0.49\n0.42\n0.20\n0.17\n0.14\n\n\n0.48\n0.42\n0.36\n0.17\n0.14\n0.12\n\n\n0.22\n0.20\n0.17\n0.49\n0.42\n0.35\n\n\n0.19\n0.17\n0.14\n0.42\n0.36\n0.30\n\n\n0.16\n0.14\n0.12\n0.35\n0.30\n0.25\nl2 %*% t(l2) |&gt; round(2)\n\n\nA matrix: 6 x 6 of type dbl\n\n\n0.64\n0.56\n0.48\n0.22\n0.19\n0.16\n\n\n0.56\n0.49\n0.42\n0.20\n0.17\n0.14\n\n\n0.48\n0.42\n0.36\n0.17\n0.14\n0.12\n\n\n0.22\n0.20\n0.17\n0.49\n0.42\n0.35\n\n\n0.19\n0.17\n0.14\n0.42\n0.36\n0.30\n\n\n0.16\n0.14\n0.12\n0.35\n0.30\n0.25\nCome si vede, viene ottenuto lo stesso risultato utilizzando matrici \\(\\boldsymbol{\\Lambda}\\) con lo stesso numero \\(m\\) di colonne ma saturazioni fattoriali diverse.\nSi consideri ora il caso di matrici \\(\\boldsymbol{\\Lambda}\\) corrispondenti a soluzioni fattoriali con un diverso numero di fattori comuni. Siano \\(\\boldsymbol{\\Lambda}_1\\) e \\(\\boldsymbol{\\Lambda}_2\\) due matrici aventi lo stesso numero di righe ma un numero diverso di colonne:\nl1 &lt;- matrix(\n  c(\n    0.9,\n    0.7,\n    0.5,\n    0.3\n  ),\n  byrow = TRUE, ncol = 1\n)\n\nl2 &lt;- matrix(\n  c(\n    0.78, 0.45,\n    0.61, 0.35,\n    0.43, 0.25,\n    0.25, 0.15\n  ),\n  byrow = TRUE, ncol = 2\n)\nSi noti che la stessa matrice di correlazioni riprodotte (con le comunalità sulla diagonale principale) viene generata dalle saturazioni fattoriali corrispondenti ad un numero diverso di fattori comuni:\nl1 %*% t(l1) |&gt; round(2)\n\n\nA matrix: 4 x 4 of type dbl\n\n\n0.81\n0.63\n0.45\n0.27\n\n\n0.63\n0.49\n0.35\n0.21\n\n\n0.45\n0.35\n0.25\n0.15\n\n\n0.27\n0.21\n0.15\n0.09\nl2 %*% t(l2) |&gt; round(2)\n\n\nA matrix: 4 x 4 of type dbl\n\n\n0.81\n0.63\n0.45\n0.26\n\n\n0.63\n0.49\n0.35\n0.20\n\n\n0.45\n0.35\n0.25\n0.14\n\n\n0.26\n0.20\n0.14\n0.08",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>La rotazione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/04_rotazione.html#parsimonia-e-semplicità",
    "href": "chapters/extraction/04_rotazione.html#parsimonia-e-semplicità",
    "title": "37  La rotazione fattoriale",
    "section": "37.2 Parsimonia e semplicità",
    "text": "37.2 Parsimonia e semplicità\nPer ottenere risultati affidabili dall’analisi fattoriale, si affronta il problema dell’indeterminazione fattoriale scegliendo la soluzione che soddisfa due criteri fondamentali: il criterio della parsimonia e il criterio della semplicità.\nIl criterio della parsimonia richiede di scegliere il modello con il minor numero di fattori comuni che può spiegare la covarianza tra le variabili. In pratica, se ci sono due soluzioni fattoriali con un diverso numero di fattori che riproducono allo stesso modo la matrice di covarianza o di correlazione, si sceglie quella con il minor numero di fattori.\nIn caso invece ci siano diverse soluzioni fattoriali con lo stesso numero m di fattori, il criterio della semplicità guida nella scelta della trasformazione più appropriata della matrice di saturazioni fattoriali \\(\\boldsymbol{\\Lambda}\\). Questa trasformazione, nota come rotazione, cerca di rendere i fattori più interpretabili. Ci sono due tipi di rotazione: ortogonale e obliqua.\nLa rotazione ortogonale assume che i fattori siano incorrelati, mentre la rotazione obliqua consente correlazioni tra i fattori. L’obiettivo della rotazione è di trovare una soluzione che renda i fattori più facilmente interpretabili e, quindi, in grado di spiegare meglio i dati.\n\n37.2.1 Il Criterio della Struttura Semplice nell’Analisi Fattoriale\nL’analisi fattoriale impiega la rotazione degli assi fattoriali per ottenere una “struttura semplice” nella matrice delle saturazioni fattoriali. Questo criterio, proposto originariamente da Thurstone nel 1947, mira a realizzare una matrice caratterizzata da un numero limitato di saturazioni (o carichi fattoriali) significative e diverse da zero, minimizzando al contempo la presenza di variabili influenzate da più di un fattore.\nPer raggiungere una struttura semplice, Thurstone ha delineato specifiche condizioni che la matrice fattoriale ruotata deve soddisfare:\n\nOgni variabile deve presentare saturazioni nulle con la maggior parte dei fattori, escludendo uno o pochi con cui mostra saturazioni significative.\nPer ciascun fattore, devono esistere almeno \\(m\\) saturazioni nulle, dove \\(m\\) è il numero totale di fattori comuni.\n\nL’obiettivo della rotazione è quindi massimizzare il numero di saturazioni nulle o quasi nulle, facilitando l’interpretazione dei fattori. Analizzando la matrice ruotata, è possibile identificare le variabili che sono fortemente associate a specifici fattori e valutare l’intensità di tali associazioni.\nUn fattore si interpreta efficacemente quando i suoi carichi sono elevati e positivi su un gruppo ristretto di variabili; ciò suggerisce che il fattore rappresenta un tratto o una caratteristica comune a tali variabili. Tuttavia, l’interpretazione diventa più complessa quando le variabili presentano saturazioni significative con più di un fattore, poiché indica la presenza di sovrapposizioni nelle influenze fattoriali.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>La rotazione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/04_rotazione.html#rotazione-nello-spazio-geometrico",
    "href": "chapters/extraction/04_rotazione.html#rotazione-nello-spazio-geometrico",
    "title": "37  La rotazione fattoriale",
    "section": "37.3 Rotazione nello Spazio Geometrico",
    "text": "37.3 Rotazione nello Spazio Geometrico\n\n37.3.1 Rotazione Ortogonale\nCome precedentemente osservato, la matrice delle saturazioni fattoriali non è unica, implicando l’esistenza di multiple soluzioni equivalenti per determinare i pesi fattoriali. La rotazione ortogonale è un tipo di trasformazione lineare applicata ai pesi fattoriali per produrre una nuova matrice di saturazioni fattoriali che rispetti criteri specifici di struttura semplice. Questo processo ha lo scopo di rendere i dati più facilmente interpretabili.\nGeometricamente parlando, la rotazione ortogonale è simile a una rotazione rigida degli assi in uno spazio cartesiano che rappresenta i pesi fattoriali. Tale rotazione conserva le distanze tra i punti (che rappresentano le saturazioni fattoriali) ma modifica la loro posizione relativa rispetto ai fattori. Di conseguenza, si ottiene una configurazione dei pesi fattoriali che è più semplice da interpretare.\nLe tecniche di rotazione ortogonale sono tipicamente implementate attraverso metodi come la massima verosimiglianza o l’analisi dei componenti principali, con l’obiettivo di massimizzare il numero di saturazioni nulle o quasi nulle nella matrice delle saturazioni risultante. Questo processo aiuta a chiarire quale variabile è influenzata maggiormente da quali fattori, facilitando l’interpretazione dei risultati dell’analisi fattoriale.\n\n\n37.3.2 Vincoli alla Rotazione dei Fattori\nIl problema della non identificabilità della matrice dei pesi fattoriali, denotata come \\(\\hat{\\boldsymbol{\\Lambda}}\\), indica l’esistenza di molteplici matrici equivalenti che possono produrre identiche correlazioni tra le variabili di un modello. Per affrontare questa questione, è essenziale imporre vincoli sulla rotazione dei fattori. Uno dei criteri fondamentali nella scelta del tipo di rotazione è l’ottenimento di una matrice \\(\\hat{\\boldsymbol{\\Lambda}}\\) semplificata, i cui elementi si avvicinano il più possibile ai valori 0 e 1. Questo facilita l’interpretazione dei fattori come combinazioni lineari delle variabili.\nLe rotazioni ortogonali, utili in presenza di fattori non correlati, mantengono inalterate le comunalità, poiché conservano le distanze geometriche tra i punti rappresentati dai pesi fattoriali. In questo caso, le comunalità sono calcolate come la somma dei quadrati dei pesi fattoriali. Al contrario, le rotazioni non ortogonali modificano la quota di varianza spiegata da ciascun fattore, calcolata dalla somma dei quadrati dei pesi fattoriali divisa per la traccia della matrice di correlazione.\nEsistono vari algoritmi per eseguire la rotazione ortogonale dei fattori, tra cui il metodo grafico, il metodo Quartimax e il metodo Varimax. Ciascuno di questi metodi ha specifiche applicazioni e impatti sulla struttura della matrice risultante, facilitando così l’interpretazione dei dati analizzati.\n\n\n37.3.3 Metodo Grafico per la Rotazione dei Fattori\nQuando si dispone di soli $ m=2 $ fattori, il sistema di coordinate bidimensionale è utilizzato per rappresentare geometricamente i fattori. La visualizzazione grafica delle saturazioni fattoriali permette di determinare visivamente la rotazione più appropriata. Ogni riga della matrice \\(\\hat{\\boldsymbol{\\Lambda}}\\) rappresenta un paio di pesi fattoriali, \\(\\hat{\\lambda}_{i1}, \\hat{\\lambda}_{i2}\\), con \\(i=1, \\dots, p\\), che corrispondono alle coordinate di \\(p\\) punti (equivalenti al numero di variabili manifeste). Per ottimizzare la rappresentazione, gli assi vengono ruotati di un angolo \\(\\phi\\) per avvicinarli il più possibile alla disposizione dei punti sul grafico. Le nuove coordinate \\((\\hat{\\lambda}_{i1}^*, \\hat{\\lambda}_{i2}^*)\\) sono calcolate mediante la trasformazione \\(\\hat{\\boldsymbol{\\Lambda}}^* = \\hat{\\boldsymbol{\\Lambda}} \\textbf{T}\\), dove\n\\[\n\\textbf{T} =\n\\begin{bmatrix}\n\\cos{\\phi} & -\\sin{\\phi}\\\\\n\\sin{\\phi} & \\cos{\\phi}\n\\end{bmatrix}\n\\]\nè una matrice ortogonale \\(2 \\times 2\\).\nEsempio: Consideriamo un caso studiato da Brown, Williams e Barlow (1984), analizzato in Rencher (2002). Ad una ragazza di dodici anni è stato chiesto di valutare sette suoi conoscenti su cinque attributi: gentilezza, intelligenza, felicità, simpatia e giustizia. Per queste variabili, la matrice di correlazione \\(R\\) è stata analizzata per estrarre due fattori mediante il metodo delle componenti principali, senza rotazione iniziale:\n\nR &lt;- matrix(\n  c(\n    1.00, .296, .881, .995, .545,\n    .296, 1.000, -.022, .326, .837,\n    .881, -.022, 1.000, .867, .130,\n    .995, .326, .867, 1.000, .544,\n    .545, .837, .130, .544, 1.00\n  ),\n  ncol = 5, byrow = TRUE, dimnames = list(\n    c(\"K\", \"I\", \"H\", \"L\", \"J\"), c(\"K\", \"I\", \"H\", \"L\", \"J\")\n  )\n)\n\nprint(R)\n\n      K      I      H     L     J\nK 1.000  0.296  0.881 0.995 0.545\nI 0.296  1.000 -0.022 0.326 0.837\nH 0.881 -0.022  1.000 0.867 0.130\nL 0.995  0.326  0.867 1.000 0.544\nJ 0.545  0.837  0.130 0.544 1.000\n\n\nDalla matrice \\(R\\), estraiamo due fattori. Si osserva che i fattori risultano difficili da interpretare: il primo fattore mostra alte saturazioni positive su tutte le variabili manifeste, mentre il secondo fattore si caratterizza per alte saturazioni positive su una variabile e negative sulle altre.\n\nf.pc &lt;- principal(R, 2, rotate = FALSE) \nf.pc\n\nPrincipal Components Analysis\nCall: principal(r = R, nfactors = 2, rotate = FALSE)\nStandardized loadings (pattern matrix) based upon correlation matrix\n   PC1   PC2   h2     u2 com\nK 0.97 -0.23 0.99 0.0067 1.1\nI 0.52  0.81 0.92 0.0792 1.7\nH 0.78 -0.59 0.96 0.0391 1.9\nL 0.97 -0.21 0.99 0.0135 1.1\nJ 0.70  0.67 0.94 0.0597 2.0\n\n                       PC1  PC2\nSS loadings           3.26 1.54\nProportion Var        0.65 0.31\nCumulative Var        0.65 0.96\nProportion Explained  0.68 0.32\nCumulative Proportion 0.68 1.00\n\nMean item complexity =  1.6\nTest of the hypothesis that 2 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.03 \n\nFit based upon off diagonal values = 1\n\n\nIn un grafico delle saturazioni fattoriali, i punti rappresentano le cinque coppie di saturazioni (una per ciascun fattore):\n\nplot(\n  f.pc$load[, 1], f.pc$load[, 2],\n  bty = \"n\", xaxt = \"n\",\n  xlab = \"Primo Fattore\", ylab = \"Secondo Fattore\",\n  ylim = c(-.6, 1), xlim = c(0, 1), pch = 19, asp = 1\n)\naxis(1, pos = c(0, 0))\nabline(0, 0)\n\n\n\n\n\n\n\n\nRencher (2002) suggerisce che una rotazione ortogonale di \\(-35^\\circ\\) avvicinerebbe efficacemente gli assi ai punti nel diagramma di dispersione. Per verificarlo, si può disegnare i nuovi assi nel grafico dopo una rotazione di \\(-35^\\circ\\).\n\nplot(\n  f.pc$load[, 1], f.pc$load[, 2],\n  bty = \"n\", xaxt = \"n\",\n  xlab = \"Primo Fattore\", ylab = \"Secondo Fattore\",\n  ylim = c(-.6, 1), xlim = c(0, 1), pch = 19, asp = 1\n)\naxis(1, pos = c(0, 0))\nabline(0, 0)\n\nar &lt;- matrix(c(\n  0, 0,\n  0, 1,\n  0, 0,\n  1, 0\n), ncol = 2, byrow = TRUE)\n\nangle &lt;- 35\nrad &lt;- angle * pi / 180\nT &lt;- matrix(c(\n  cos(rad), -sin(rad),\n  sin(rad),  cos(rad)\n), ncol = 2, byrow = TRUE)\n\nround(ar %*% T, 3)\n\narrows(0, 0, 0.574, 0.819, lwd = 2)\narrows(0, 0, 0.819, -0.574, lwd = 2)\n\n\nA matrix: 4 x 2 of type dbl\n\n\n0.000\n0.000\n\n\n0.574\n0.819\n\n\n0.000\n0.000\n\n\n0.819\n-0.574\n\n\n\n\n\n\n\n\n\n\n\n\nNella figura, le due frecce rappresentano gli assi ruotati. La rotazione di \\(-35^{\\circ}\\) ha effettivamente avvicinato gli assi ai punti del diagramma. Se usiamo dunque il valore \\(\\phi = -35^{\\circ}\\) nella matrice di rotazione, possiamo calcolare le saturazioni fattoriali della soluzione ruotata \\(\\hat{\\boldsymbol{\\Lambda}}^* = \\hat{\\boldsymbol{\\Lambda}} \\textbf{T}\\).\nLe saturazioni fattoriali ruotate corrispondono alla proiezione ortogonale dei punti sugli assi ruotati:\n\nangle &lt;- -35\nrad &lt;- angle * pi / 180\nT &lt;- matrix(c(\n  cos(rad), -sin(rad),\n  sin(rad),  cos(rad)\n), ncol = 2, byrow = TRUE)\nround(f.pc$load %*% T, 3)\n\n\nA matrix: 5 x 2 of type dbl\n\n\nK\n0.927\n0.367\n\n\nI\n-0.037\n0.959\n\n\nH\n0.980\n-0.031\n\n\nL\n0.916\n0.385\n\n\nJ\n0.194\n0.950\n\n\n\n\n\nLa soluzione ottenuta in questo modo riproduce quanto riportato da {cite:t}rencher10methods.\n\n\n37.3.4 Medodi di rotazione ortogonale\nUn tipo di rotazione ortogonale spesso utilizzata è la rotazione Varimax (Kaiser, 1958). La matrice \\(\\hat{\\boldsymbol{\\Lambda}}\\) è semplificata in modo tale che le varianze dei quadrati degli elementi \\(\\lambda_{ij}\\) appartenenti a colonne diverse di \\(\\hat{\\boldsymbol{\\Lambda}}\\) siano massime. Se le saturazioni fattoriali in una colonna di \\(\\hat{\\boldsymbol{\\Lambda}}\\) sono simili tra loro, la varianza sarà prossima a zero. Tale varianza è tanto più grande quanto più i quadrati degli elementi \\(\\lambda_{ij}\\) assumono valori prossimi a \\(0\\) e \\(1\\). Amplificando le correlazioni più alte e riducendo quelle più basse, la rotazione Varimax agevola l’interpretazione di ciascun fattore.\nUsando la funzione factanal() del modulo R base, la rotazione Varimax può essere applicata alla soluzione ottenuta mediante il metodo di massima verosimiglianza. Usando le funzioni principal() e factor.pa() disponibili nel pacchetto psych, la rotazione Varimax può essere applicata alle soluzioni ottenute mediante il metodo delle componenti principali e il metodo del fattore principale.\nAd esempio, usando il metodo delle componenti principali otteniamo:\n\nf_pc &lt;- principal(R, 2, n.obs = 7, rotate = \"varimax\")\nf_pc\n\nPrincipal Components Analysis\nCall: principal(r = R, nfactors = 2, rotate = \"varimax\", n.obs = 7)\nStandardized loadings (pattern matrix) based upon correlation matrix\n   RC1   RC2   h2     u2 com\nK 0.95  0.30 0.99 0.0067 1.2\nI 0.03  0.96 0.92 0.0792 1.0\nH 0.97 -0.10 0.96 0.0391 1.0\nL 0.94  0.32 0.99 0.0135 1.2\nJ 0.26  0.93 0.94 0.0597 1.2\n\n                       RC1  RC2\nSS loadings           2.81 1.99\nProportion Var        0.56 0.40\nCumulative Var        0.56 0.96\nProportion Explained  0.58 0.42\nCumulative Proportion 0.58 1.00\n\nMean item complexity =  1.1\nTest of the hypothesis that 2 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.03 \n with the empirical chi square  0.12  with prob &lt;  0.73 \n\nFit based upon off diagonal values = 1\n\n\nUn altro metodo di rotazione ortogonale è il metodo Quartimax (Neuhaus e Wringley, 1954), il quale opera una semplificazione della matrice \\(\\hat{\\boldsymbol{\\Lambda}}\\) massimizzando le covarianze tra i quadrati degli elementi \\(\\lambda_{ij}\\) appartenenti a righe diverse, subordinatamente alla condizione che la varianza delle righe rimanga inalterata.\n\n\n37.3.5 Metodi di Rotazione Obliqua\nIl termine “rotazione obliqua” può sembrare inappropriato, in quanto la rotazione implica generalmente una trasformazione ortogonale che preserva le distanze. Tuttavia, come evidenziato da {cite:t}rencher10methods, un’espressione più corretta potrebbe essere “trasformazione obliqua”. Nonostante ciò, l’uso comune ha consolidato il termine “rotazione obliqua”.\nNel contesto della rotazione obliqua, gli assi della soluzione ruotata non sono costretti a rimanere ortogonali tra loro, permettendo così un allineamento più diretto agli agglomerati di punti nello spazio delle saturazioni fattoriali. Questo tipo di trasformazione facilita l’interpretazione dei fattori in presenza di correlazioni tra di essi.\nEsistono diversi approcci analitici per realizzare una rotazione obliqua. Ad esempio, il metodo Direct Oblimin, sviluppato da Jennrich e Sampson nel 1966, utilizza il seguente criterio:\n\\[\n\\sum_{ij} \\left(\\sum_v \\lambda_i^2 \\lambda_j^2 - w \\frac{1}{p} \\sum_v \\lambda_i^2 \\sum_v \\lambda_j^2\\right)\n\\]\nQui, \\(\\sum_{ij}\\) rappresenta la somma su tutte le coppie di fattori \\(ij\\). Il processo prevede una minimizzazione, al contrario della massimizzazione tipica delle rotazioni ortogonali, riflettendo la ricerca di una soluzione che minimizzi la correlazione ridondante tra i fattori, mantenendo al contempo chiarezza interpretativa.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>La rotazione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/04_rotazione.html#matrice-dei-pesi-fattoriali-e-matrice-di-struttura",
    "href": "chapters/extraction/04_rotazione.html#matrice-dei-pesi-fattoriali-e-matrice-di-struttura",
    "title": "37  La rotazione fattoriale",
    "section": "37.4 Matrice dei Pesi Fattoriali e Matrice di Struttura",
    "text": "37.4 Matrice dei Pesi Fattoriali e Matrice di Struttura\n\n37.4.1 Rotazione Ortogonale\nNel contesto della rotazione ortogonale, i fattori rimangono incorrelati tra loro. Consideriamo il caso di due fattori latenti non correlati (\\(\\xi_1\\) e \\(\\xi_2\\)) e quattro variabili manifeste (\\(y_1, y_2, y_3, y_4\\)). I coefficienti \\(\\lambda_{11}, \\lambda_{12}, \\lambda_{13}, \\lambda_{14}\\) rappresentano le saturazioni fattoriali delle variabili nel primo fattore, mentre \\(\\lambda_{21}, \\lambda_{22}, \\lambda_{23}, \\lambda_{24}\\) sono quelle nel secondo fattore. In un modello di percorso, la correlazione tra due variabili è calcolata come la somma di tutti i percorsi validi che le collegano. Se i fattori comuni sono incorrelati, esiste un solo percorso valido che collega ciascuna variabile manifesta a ciascun fattore comune secondo le regole di Wright. Pertanto, le correlazioni tra variabili manifeste e fattori comuni sono direttamente uguali alle saturazioni fattoriali. Queste saturazioni possono essere interpretate come i pesi beta di un modello di regressione multipla, indicando il contributo specifico di ciascun fattore comune nella varianza spiegata degli item (Tabachnick & Fidell, 2001).\n\n\n\n\n\n\nFigura 37.1: Rotazione ortogonale.\n\n\n\n\n\n37.4.2 Rotazione Obliqua\nNel caso della rotazione obliqua, i fattori comuni risultano correlati tra loro, rendendo la soluzione fattoriale più complessa. Pertanto, la matrice delle saturazioni fattoriali non riflette più direttamente le correlazioni tra variabili e fattori. Un modello di percorso in questa configurazione include almeno due percorsi validi che collegano ciascuna variabile manifesta a ciascun fattore comune. È necessario distinguere tra tre matrici diverse:\n\nMatrice Pattern (\\(\\hat{\\boldsymbol{\\Lambda}}\\)): Conosciuta anche come matrice dei modelli, questa matrice rappresenta i coefficienti di regressione parziali delle variabili sulle dimensioni fattoriali, escludendo l’influenza degli altri fattori.\nMatrice di Struttura: Rappresenta le correlazioni complessive tra le variabili manifeste e i fattori, considerando sia gli effetti diretti che quelli indiretti dei fattori correlati.\nMatrice di Intercorrelazione Fattoriale (\\(\\hat{\\boldsymbol{\\Phi}}\\)): Indica le correlazioni tra i fattori stessi.\n\nIn un modello di percorso con rotazione obliqua, gli assi che rappresentano i fattori non sono ortogonali, il che significa che i fattori sono correlati. Le variabili manifeste sono quindi collegate ai fattori attraverso percorsi che includono effetti diretti e indiretti. Ad esempio, per la variabile \\(y_1\\) e il fattore \\(\\xi_1\\), i percorsi includono una freccia causale \\(\\lambda_{11}\\) per l’effetto diretto e un percorso indiretto rappresentato dal prodotto \\(\\lambda_{21}\\phi_{12}\\). L’analisi dei percorsi dimostra che la correlazione tra \\(\\xi_1\\) e \\(y_1\\) è la somma dei valori numerici di questi percorsi validi, ovvero \\(\\lambda_{11} + \\lambda_{21} \\phi_{12}\\).\n\n\n\n\n\n\nFigura 37.2: Rotazione obliqua.\n\n\n\nPer illustrare la rotazione obliqua, utilizziamo i dati discussi da {cite:t}rencher10methods. Si consideri la matrice di correlazione presentata qui sotto.\n\nR &lt;- matrix(\n  c(\n    1.00,  0.735, 0.711, 0.704,\n    0.735, 1.00,  0.693, 0.709,\n    0.711, 0.693, 1.00,  0.839,\n    0.704, 0.709, 0.839, 1.00\n  ),\n  ncol = 4,\n  byrow = TRUE\n)\nR\n\n\nA matrix: 4 x 4 of type dbl\n\n\n1.000\n0.735\n0.711\n0.704\n\n\n0.735\n1.000\n0.693\n0.709\n\n\n0.711\n0.693\n1.000\n0.839\n\n\n0.704\n0.709\n0.839\n1.000\n\n\n\n\n\nIniziamo calcolando la soluzione a due fattori mediante il metodo delle componenti principali e una rotazione Varimax (ovvero, ortogonale). Otteniamo le seguenti saturazioni fattoriali.\n\nf1_pc &lt;- principal(R, 2, rotate = \"varimax\") \nf1_pc\n\nPrincipal Components Analysis\nCall: principal(r = R, nfactors = 2, rotate = \"varimax\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n   RC1  RC2   h2    u2 com\n1 0.50 0.78 0.86 0.140 1.7\n2 0.47 0.81 0.88 0.124 1.6\n3 0.90 0.33 0.92 0.078 1.3\n4 0.89 0.35 0.92 0.083 1.3\n\n                       RC1  RC2\nSS loadings           2.08 1.50\nProportion Var        0.52 0.37\nCumulative Var        0.52 0.89\nProportion Explained  0.58 0.42\nCumulative Proportion 0.58 1.00\n\nMean item complexity =  1.5\nTest of the hypothesis that 2 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.06 \n\nFit based upon off diagonal values = 0.99\n\n\nSi noti che i due fattori non sono molto distinti. Consideriamo dunque la soluzione prodotta da una rotazione obliqua. Usiamo qui l’algoritmo Oblimin.\n\npr_oblimin &lt;- principal(R, 2, rotate = \"oblimin\")\n\nLa matrice \\(\\hat{\\boldsymbol{\\Lambda}}\\) delle saturazioni fattoriali si ricava come indicato di seguito.\n\ncbind(pr_oblimin$load[, 1], pr_oblimin$load[, 2])\n\n\nA matrix: 4 x 2 of type dbl\n\n\n0.0321\n0.9019\n\n\n-0.0254\n0.9556\n\n\n0.9686\n-0.0110\n\n\n0.9473\n0.0133\n\n\n\n\n\nLa matrice \\(\\hat{\\boldsymbol{\\Phi}}\\) di inter-correlazione fattoriale è la seguente.\n\npr_oblimin$Phi\n\n\nA matrix: 2 x 2 of type dbl\n\n\n\nTC1\nTC2\n\n\n\n\nTC1\n1.000\n0.787\n\n\nTC2\n0.787\n1.000\n\n\n\n\n\nLa matrice di struttura, che riporta le correlazioni tra indicatori e fattori comuni, si ottiene pre-moltiplicando la matrice \\(\\boldsymbol{\\Lambda}\\) delle saturazioni fattoriali alla matrice \\(\\boldsymbol{\\Phi}\\) di inter-correlazione fattoriale.\n\\[\n\\text{matrice di struttura} = \\boldsymbol{\\Lambda}\\boldsymbol{\\Phi}.\n\\]\nPer esempio, la correlazione tra la prima variabile manifesta e il primo fattore si ottiene nel modo seguente.\n\npr_oblimin$load[1, 1] + pr_oblimin$load[1, 2] * pr_oblimin$Phi[2, 1]\n\nTC1: 0.741813471502872\n\n\nL’intera matrice di struttura si può trovare eseguendo la moltiplicazione \\(\\boldsymbol{\\Lambda}\\boldsymbol{\\Phi}\\).\n\npr_oblimin$load %*% pr_oblimin$Phi %&gt;% \n  round(3)\n\n\nA matrix: 4 x 2 of type dbl\n\n\nTC1\nTC2\n\n\n\n\n0.742\n0.927\n\n\n0.727\n0.936\n\n\n0.960\n0.751\n\n\n0.958\n0.759",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>La rotazione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/04_rotazione.html#esempio-con-semtools",
    "href": "chapters/extraction/04_rotazione.html#esempio-con-semtools",
    "title": "37  La rotazione fattoriale",
    "section": "37.5 Esempio con semTools",
    "text": "37.5 Esempio con semTools\nPresento qui un esempio di uso di vari metodi di estrazione fattoriale. Tra tali metodi, la rotazione obliqua Geomin è molto popolare ed è il default di M-Plus.\nIniziamo a caricare il pacchetto semTools.\n\nsuppressPackageStartupMessages(library(\"semTools\")) \n\nEseguiamo l’analisi fattoriale esplorativa del classico set di dati di Holzinger e Swineford (1939) il quale è costituito dai punteggi dei test di abilità mentale di bambini di seconda e terza media di due scuole diverse (Pasteur e Grant-White). Nel set di dati originale (disponibile nel pacchetto MBESS), sono forniti i punteggi di 26 test. Tuttavia, un sottoinsieme più piccolo con 9 variabili è più ampiamente utilizzato in letteratura. Questi sono i dati qui usati.\nNel presente esempio, verrà eseguita l’analisi fattoriale esplorativa con l’estrazione di tre fattori. Il metodo di estrazione è mlr:\n\nmaximum likelihood estimation with robust (Huber-White) standard errors and a scaled test statistic that is (asymptotically) equal to the Yuan-Bentler test statistic. For both complete and incomplete data.\n\nLa soluzione iniziale non è ruotata.\n\nunrotated &lt;- efaUnrotate(\n    HolzingerSwineford1939, \n    nf = 3, \n    varList = paste0(\"x\", 1:9), \n    estimator = \"mlr\"\n)\nout &lt;- summary(unrotated)\nprint(out)\n\nlavaan 0.6-19 ended normally after 217 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        36\n\n  Number of observations                           301\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                22.897      23.864\n  Degrees of freedom                                12          12\n  P-value (Chi-square)                           0.029       0.021\n  Scaling correction factor                                  0.959\n    Yuan-Bentler correction (Mplus variant)                       \n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  factor1 =~                                          \n    x1      (l1_1)    0.653    0.083    7.909    0.000\n    x2      (l2_1)    0.353    0.079    4.481    0.000\n    x3      (l3_1)    0.415    0.086    4.832    0.000\n    x4      (l4_1)    0.926    0.067   13.762    0.000\n    x5      (l5_1)    1.014    0.067   15.176    0.000\n    x6      (l6_1)    0.868    0.062   13.887    0.000\n    x7      (l7_1)    0.283    0.091    3.113    0.002\n    x8      (l8_1)    0.340    0.083    4.095    0.000\n    x9      (l9_1)    0.460    0.078    5.881    0.000\n  factor2 =~                                          \n    x1      (l1_2)    0.349    0.124    2.815    0.005\n    x2      (l2_2)    0.242    0.159    1.523    0.128\n    x3      (l3_2)    0.497    0.132    3.767    0.000\n    x4      (l4_2)   -0.337    0.067   -5.058    0.000\n    x5      (l5_2)   -0.461    0.077   -6.009    0.000\n    x6      (l6_2)   -0.280    0.057   -4.908    0.000\n    x7      (l7_2)    0.372    0.188    1.976    0.048\n    x8      (l8_2)    0.510    0.133    3.831    0.000\n    x9      (l9_2)    0.489    0.066    7.416    0.000\n  factor3 =~                                          \n    x1      (l1_3)   -0.338    0.103   -3.275    0.001\n    x2      (l2_3)   -0.405    0.092   -4.401    0.000\n    x3      (l3_3)   -0.404    0.120   -3.355    0.001\n    x4      (l4_3)    0.049    0.098    0.503    0.615\n    x5      (l5_3)    0.122    0.105    1.154    0.248\n    x6      (l6_3)   -0.000    0.076   -0.003    0.998\n    x7      (l7_3)    0.609    0.125    4.863    0.000\n    x8      (l8_3)    0.409    0.143    2.853    0.004\n    x9      (l9_3)    0.112    0.123    0.915    0.360\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  factor1 ~~                                          \n    factor2           0.000                           \n    factor3           0.000                           \n  factor2 ~~                                          \n    factor3           0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    factor1           1.000                           \n    factor2           1.000                           \n    factor3           1.000                           \n   .x1                0.696    0.113    6.184    0.000\n   .x2                1.035    0.106    9.803    0.000\n   .x3                0.692    0.097    7.132    0.000\n   .x4                0.377    0.053    7.170    0.000\n   .x5                0.403    0.064    6.303    0.000\n   .x6                0.365    0.046    7.984    0.000\n   .x7                0.594    0.148    4.014    0.000\n   .x8                0.479    0.099    4.842    0.000\n   .x9                0.551    0.065    8.518    0.000\n\nConstraints:\n                                               |Slack|\n    0-(1_2*1_1+2_2*2_1+3_2*3_1+4_2*4_1+5_2*5_    0.000\n    0-(1_3*1_1+2_3*2_1+3_3*3_1+4_3*4_1+5_3*5_    0.000\n    0-(1_3*1_2+2_3*2_2+3_3*3_2+4_3*4_2+5_3*5_    0.000\n\n\n\nSi noti che, in assenza di rotazione, è impossibile assegnare un significato ai fattori comuni.\n\n37.5.1 Orthogonal varimax\nUtilizziamo ora la rotazione ortogonale Varimax.\n\nout_varimax &lt;- orthRotate(\n    unrotated, \n    method = \"varimax\"\n)\nout &lt;- summary(out_varimax, sort = FALSE, suppress = 0.3)\nprint(out)\n\nStandardized Rotated Factor Loadings\n\n\n   factor1 factor2 factor3\nx1  0.320*  0.607*        \nx2          0.481*        \nx3          0.662*        \nx4  0.838*                \nx5  0.867*                \nx6  0.815*                \nx7                  0.695*\nx8                  0.704*\nx9          0.409*  0.511*\n\nFactor Correlation\n        factor1 factor2 factor3\nfactor1       1       0       0\nfactor2       0       1       0\nfactor3       0       0       1\n\nMethod of rotation: varimax \n\nTest Statistics for Standardized Rotated Factor Loadings\n\n\n       lhs op rhs std.loading    se      z     p ci.lower ci.upper\n1  factor1 =~  x1       0.320 0.055  5.799 0.000    0.212    0.428\n2  factor1 =~  x2       0.135 0.063  2.151 0.031    0.012    0.259\n3  factor1 =~  x3       0.080 0.049  1.622 0.105   -0.017    0.176\n4  factor1 =~  x4       0.838 0.028 30.193 0.000    0.784    0.892\n5  factor1 =~  x5       0.867 0.024 36.189 0.000    0.820    0.914\n6  factor1 =~  x6       0.815 0.024 33.939 0.000    0.768    0.862\n7  factor1 =~  x7       0.102 0.049  2.058 0.040    0.005    0.199\n8  factor1 =~  x8       0.078 0.048  1.617 0.106   -0.016    0.172\n9  factor1 =~  x9       0.170 0.053  3.222 0.001    0.067    0.273\n10 factor2 =~  x1       0.607 0.075  8.138 0.000    0.461    0.753\n11 factor2 =~  x2       0.481 0.067  7.184 0.000    0.350    0.612\n12 factor2 =~  x3       0.662 0.058 11.499 0.000    0.549    0.775\n13 factor2 =~  x4       0.113 0.043  2.615 0.009    0.028    0.198\n14 factor2 =~  x5       0.032 0.040  0.802 0.422   -0.047    0.111\n15 factor2 =~  x6       0.162 0.042  3.855 0.000    0.079    0.244\n16 factor2 =~  x7      -0.062 0.047 -1.341 0.180   -0.154    0.029\n17 factor2 =~  x8       0.174 0.082  2.117 0.034    0.013    0.336\n18 factor2 =~  x9       0.409 0.079  5.173 0.000    0.254    0.564\n19 factor3 =~  x1       0.130 0.066  1.978 0.048    0.001    0.259\n20 factor3 =~  x2      -0.041 0.071 -0.578 0.563   -0.179    0.098\n21 factor3 =~  x3       0.113 0.049  2.324 0.020    0.018    0.209\n22 factor3 =~  x4       0.077 0.040  1.916 0.055   -0.002    0.155\n23 factor3 =~  x5       0.070 0.042  1.669 0.095   -0.012    0.153\n24 factor3 =~  x6       0.066 0.038  1.715 0.086   -0.009    0.141\n25 factor3 =~  x7       0.695 0.092  7.591 0.000    0.516    0.875\n26 factor3 =~  x8       0.704 0.083  8.520 0.000    0.542    0.865\n27 factor3 =~  x9       0.511 0.065  7.885 0.000    0.384    0.638\n       lhs op rhs std.loading    se      z     p ci.lower ci.upper\n1  factor1 =~  x1       0.320 0.055  5.799 0.000    0.212    0.428\n2  factor1 =~  x2       0.135 0.063  2.151 0.031    0.012    0.259\n3  factor1 =~  x3       0.080 0.049  1.622 0.105   -0.017    0.176\n4  factor1 =~  x4       0.838 0.028 30.193 0.000    0.784    0.892\n5  factor1 =~  x5       0.867 0.024 36.189 0.000    0.820    0.914\n6  factor1 =~  x6       0.815 0.024 33.939 0.000    0.768    0.862\n7  factor1 =~  x7       0.102 0.049  2.058 0.040    0.005    0.199\n8  factor1 =~  x8       0.078 0.048  1.617 0.106   -0.016    0.172\n9  factor1 =~  x9       0.170 0.053  3.222 0.001    0.067    0.273\n10 factor2 =~  x1       0.607 0.075  8.138 0.000    0.461    0.753\n11 factor2 =~  x2       0.481 0.067  7.184 0.000    0.350    0.612\n12 factor2 =~  x3       0.662 0.058 11.499 0.000    0.549    0.775\n13 factor2 =~  x4       0.113 0.043  2.615 0.009    0.028    0.198\n14 factor2 =~  x5       0.032 0.040  0.802 0.422   -0.047    0.111\n15 factor2 =~  x6       0.162 0.042  3.855 0.000    0.079    0.244\n16 factor2 =~  x7      -0.062 0.047 -1.341 0.180   -0.154    0.029\n17 factor2 =~  x8       0.174 0.082  2.117 0.034    0.013    0.336\n18 factor2 =~  x9       0.409 0.079  5.173 0.000    0.254    0.564\n19 factor3 =~  x1       0.130 0.066  1.978 0.048    0.001    0.259\n20 factor3 =~  x2      -0.041 0.071 -0.578 0.563   -0.179    0.098\n21 factor3 =~  x3       0.113 0.049  2.324 0.020    0.018    0.209\n22 factor3 =~  x4       0.077 0.040  1.916 0.055   -0.002    0.155\n23 factor3 =~  x5       0.070 0.042  1.669 0.095   -0.012    0.153\n24 factor3 =~  x6       0.066 0.038  1.715 0.086   -0.009    0.141\n25 factor3 =~  x7       0.695 0.092  7.591 0.000    0.516    0.875\n26 factor3 =~  x8       0.704 0.083  8.520 0.000    0.542    0.865\n27 factor3 =~  x9       0.511 0.065  7.885 0.000    0.384    0.638\n\n\n\n\n37.5.2 Orthogonal Quartimin\nUn metodo alternativo per la rotazione ortogonale è Quartimin.\n\nout_quartimin &lt;- orthRotate(\n    unrotated, \n    method = \"quartimin\"\n)\nout &lt;- summary(out_quartimin, sort = FALSE, suppress = 0.3)\nprint(out)\n\nStandardized Rotated Factor Loadings\n\n\n   factor1 factor2 factor3\nx1  0.353*  0.590*        \nx2          0.474*        \nx3          0.657*        \nx4  0.844*                \nx5  0.869*                \nx6  0.823*                \nx7                  0.692*\nx8                  0.702*\nx9          0.397*  0.508*\n\nFactor Correlation\n        factor1 factor2 factor3\nfactor1       1       0       0\nfactor2       0       1       0\nfactor3       0       0       1\n\nMethod of rotation: Quartimin \n\nTest Statistics for Standardized Rotated Factor Loadings\n\n\n       lhs op rhs std.loading    se      z     p ci.lower ci.upper\n1  factor1 =~  x1       0.353 0.062  5.720 0.000    0.232    0.473\n2  factor1 =~  x2       0.158 0.066  2.375 0.018    0.028    0.288\n3  factor1 =~  x3       0.115 0.057  2.015 0.044    0.003    0.226\n4  factor1 =~  x4       0.844 0.027 30.814 0.000    0.790    0.898\n5  factor1 =~  x5       0.869 0.023 37.224 0.000    0.823    0.914\n6  factor1 =~  x6       0.823 0.024 35.026 0.000    0.777    0.869\n7  factor1 =~  x7       0.116 0.054  2.161 0.031    0.011    0.222\n8  factor1 =~  x8       0.104 0.054  1.914 0.056   -0.003    0.210\n9  factor1 =~  x9       0.202 0.059  3.403 0.001    0.086    0.319\n10 factor2 =~  x1       0.590 0.078  7.523 0.000    0.436    0.743\n11 factor2 =~  x2       0.474 0.068  6.935 0.000    0.340    0.608\n12 factor2 =~  x3       0.657 0.059 11.204 0.000    0.542    0.771\n13 factor2 =~  x4       0.072 0.041  1.748 0.080   -0.009    0.152\n14 factor2 =~  x5      -0.010 0.039 -0.263 0.793   -0.087    0.066\n15 factor2 =~  x6       0.122 0.040  3.008 0.003    0.042    0.201\n16 factor2 =~  x7      -0.071 0.048 -1.465 0.143   -0.166    0.024\n17 factor2 =~  x8       0.167 0.090  1.849 0.064   -0.010    0.343\n18 factor2 =~  x9       0.397 0.087  4.581 0.000    0.227    0.567\n19 factor3 =~  x1       0.124 0.071  1.740 0.082   -0.016    0.264\n20 factor3 =~  x2      -0.042 0.074 -0.574 0.566   -0.187    0.102\n21 factor3 =~  x3       0.114 0.054  2.093 0.036    0.007    0.221\n22 factor3 =~  x4       0.056 0.037  1.503 0.133   -0.017    0.128\n23 factor3 =~  x5       0.048 0.038  1.263 0.207   -0.027    0.123\n24 factor3 =~  x6       0.046 0.034  1.347 0.178   -0.021    0.112\n25 factor3 =~  x7       0.692 0.093  7.477 0.000    0.511    0.874\n26 factor3 =~  x8       0.702 0.084  8.331 0.000    0.537    0.867\n27 factor3 =~  x9       0.508 0.070  7.298 0.000    0.371    0.644\n       lhs op rhs std.loading    se      z     p ci.lower ci.upper\n1  factor1 =~  x1       0.353 0.062  5.720 0.000    0.232    0.473\n2  factor1 =~  x2       0.158 0.066  2.375 0.018    0.028    0.288\n3  factor1 =~  x3       0.115 0.057  2.015 0.044    0.003    0.226\n4  factor1 =~  x4       0.844 0.027 30.814 0.000    0.790    0.898\n5  factor1 =~  x5       0.869 0.023 37.224 0.000    0.823    0.914\n6  factor1 =~  x6       0.823 0.024 35.026 0.000    0.777    0.869\n7  factor1 =~  x7       0.116 0.054  2.161 0.031    0.011    0.222\n8  factor1 =~  x8       0.104 0.054  1.914 0.056   -0.003    0.210\n9  factor1 =~  x9       0.202 0.059  3.403 0.001    0.086    0.319\n10 factor2 =~  x1       0.590 0.078  7.523 0.000    0.436    0.743\n11 factor2 =~  x2       0.474 0.068  6.935 0.000    0.340    0.608\n12 factor2 =~  x3       0.657 0.059 11.204 0.000    0.542    0.771\n13 factor2 =~  x4       0.072 0.041  1.748 0.080   -0.009    0.152\n14 factor2 =~  x5      -0.010 0.039 -0.263 0.793   -0.087    0.066\n15 factor2 =~  x6       0.122 0.040  3.008 0.003    0.042    0.201\n16 factor2 =~  x7      -0.071 0.048 -1.465 0.143   -0.166    0.024\n17 factor2 =~  x8       0.167 0.090  1.849 0.064   -0.010    0.343\n18 factor2 =~  x9       0.397 0.087  4.581 0.000    0.227    0.567\n19 factor3 =~  x1       0.124 0.071  1.740 0.082   -0.016    0.264\n20 factor3 =~  x2      -0.042 0.074 -0.574 0.566   -0.187    0.102\n21 factor3 =~  x3       0.114 0.054  2.093 0.036    0.007    0.221\n22 factor3 =~  x4       0.056 0.037  1.503 0.133   -0.017    0.128\n23 factor3 =~  x5       0.048 0.038  1.263 0.207   -0.027    0.123\n24 factor3 =~  x6       0.046 0.034  1.347 0.178   -0.021    0.112\n25 factor3 =~  x7       0.692 0.093  7.477 0.000    0.511    0.874\n26 factor3 =~  x8       0.702 0.084  8.331 0.000    0.537    0.867\n27 factor3 =~  x9       0.508 0.070  7.298 0.000    0.371    0.644\n\n\n\n\n37.5.3 Oblique Quartimin\nL’algoritmo Quartimin può anche essere usato per una soluzione obliqua.\n\nout_oblq &lt;- oblqRotate(\n    unrotated, \n    method = \"quartimin\"\n)\nout &lt;- summary(out_oblq, sort = FALSE, suppress = 0.3)\nprint(out)\n\nStandardized Rotated Factor Loadings\n\n\n   factor1 factor2 factor3\nx1          0.602*        \nx2          0.505*        \nx3          0.689*        \nx4  0.840*                \nx5  0.888*                \nx6  0.808*                \nx7                  0.723*\nx8                  0.702*\nx9          0.366*  0.463*\n\nFactor Correlation\n        factor1 factor2 factor3\nfactor1   1.000   0.326   0.216\nfactor2   0.326   1.000   0.270\nfactor3   0.216   0.270   1.000\n\nMethod of rotation: Quartimin \n\nTest Statistics for Standardized Rotated Factor Loadings\n\n\n       lhs op rhs std.loading    se      z     p ci.lower ci.upper\n1  factor1 =~  x1       0.191 0.064  2.965 0.003    0.065    0.317\n2  factor1 =~  x2       0.044 0.066  0.665 0.506   -0.085    0.172\n3  factor1 =~  x3      -0.070 0.034 -2.031 0.042   -0.137   -0.002\n4  factor1 =~  x4       0.840 0.033 25.622 0.000    0.776    0.905\n5  factor1 =~  x5       0.888 0.027 32.583 0.000    0.835    0.942\n6  factor1 =~  x6       0.808 0.028 28.441 0.000    0.752    0.863\n7  factor1 =~  x7       0.044 0.037  1.179 0.238   -0.029    0.116\n8  factor1 =~  x8      -0.033 0.036 -0.916 0.360   -0.103    0.037\n9  factor1 =~  x9       0.035 0.048  0.728 0.467   -0.059    0.129\n10 factor2 =~  x1       0.602 0.086  7.003 0.000    0.434    0.771\n11 factor2 =~  x2       0.505 0.071  7.163 0.000    0.367    0.644\n12 factor2 =~  x3       0.689 0.056 12.344 0.000    0.580    0.799\n13 factor2 =~  x4       0.022 0.045  0.483 0.629   -0.067    0.110\n14 factor2 =~  x5      -0.067 0.036 -1.890 0.059   -0.137    0.002\n15 factor2 =~  x6       0.078 0.041  1.887 0.059   -0.003    0.158\n16 factor2 =~  x7      -0.152 0.037 -4.059 0.000   -0.225   -0.078\n17 factor2 =~  x8       0.104 0.109  0.960 0.337   -0.109    0.317\n18 factor2 =~  x9       0.366 0.097  3.780 0.000    0.176    0.556\n19 factor3 =~  x1       0.031 0.062  0.500 0.617   -0.090    0.152\n20 factor3 =~  x2      -0.117 0.066 -1.776 0.076   -0.245    0.012\n21 factor3 =~  x3       0.023 0.039  0.587 0.557   -0.054    0.100\n22 factor3 =~  x4       0.005 0.042  0.128 0.898   -0.076    0.087\n23 factor3 =~  x5       0.008 0.035  0.216 0.829   -0.061    0.076\n24 factor3 =~  x6      -0.011 0.030 -0.362 0.717   -0.070    0.048\n25 factor3 =~  x7       0.723 0.087  8.328 0.000    0.553    0.893\n26 factor3 =~  x8       0.702 0.098  7.137 0.000    0.509    0.894\n27 factor3 =~  x9       0.463 0.075  6.211 0.000    0.317    0.609\n       lhs op rhs std.loading    se      z     p ci.lower ci.upper\n1  factor1 =~  x1       0.191 0.064  2.965 0.003    0.065    0.317\n2  factor1 =~  x2       0.044 0.066  0.665 0.506   -0.085    0.172\n3  factor1 =~  x3      -0.070 0.034 -2.031 0.042   -0.137   -0.002\n4  factor1 =~  x4       0.840 0.033 25.622 0.000    0.776    0.905\n5  factor1 =~  x5       0.888 0.027 32.583 0.000    0.835    0.942\n6  factor1 =~  x6       0.808 0.028 28.441 0.000    0.752    0.863\n7  factor1 =~  x7       0.044 0.037  1.179 0.238   -0.029    0.116\n8  factor1 =~  x8      -0.033 0.036 -0.916 0.360   -0.103    0.037\n9  factor1 =~  x9       0.035 0.048  0.728 0.467   -0.059    0.129\n10 factor2 =~  x1       0.602 0.086  7.003 0.000    0.434    0.771\n11 factor2 =~  x2       0.505 0.071  7.163 0.000    0.367    0.644\n12 factor2 =~  x3       0.689 0.056 12.344 0.000    0.580    0.799\n13 factor2 =~  x4       0.022 0.045  0.483 0.629   -0.067    0.110\n14 factor2 =~  x5      -0.067 0.036 -1.890 0.059   -0.137    0.002\n15 factor2 =~  x6       0.078 0.041  1.887 0.059   -0.003    0.158\n16 factor2 =~  x7      -0.152 0.037 -4.059 0.000   -0.225   -0.078\n17 factor2 =~  x8       0.104 0.109  0.960 0.337   -0.109    0.317\n18 factor2 =~  x9       0.366 0.097  3.780 0.000    0.176    0.556\n19 factor3 =~  x1       0.031 0.062  0.500 0.617   -0.090    0.152\n20 factor3 =~  x2      -0.117 0.066 -1.776 0.076   -0.245    0.012\n21 factor3 =~  x3       0.023 0.039  0.587 0.557   -0.054    0.100\n22 factor3 =~  x4       0.005 0.042  0.128 0.898   -0.076    0.087\n23 factor3 =~  x5       0.008 0.035  0.216 0.829   -0.061    0.076\n24 factor3 =~  x6      -0.011 0.030 -0.362 0.717   -0.070    0.048\n25 factor3 =~  x7       0.723 0.087  8.328 0.000    0.553    0.893\n26 factor3 =~  x8       0.702 0.098  7.137 0.000    0.509    0.894\n27 factor3 =~  x9       0.463 0.075  6.211 0.000    0.317    0.609\n\n\n\n\n37.5.4 Orthogonal Geomin\nConsideriamo ora la rotazione Geomin. L’algoritmo Geomin fornisce un metodo di rotazione che riduce al minimo la media geometrica delle saturazioni fattoriali innalzate al quadrato. Qui è usato per ottenere una soluzione ortogonale.\n\nout_geomin_orh &lt;- orthRotate(\n    unrotated, \n    method = \"geomin\"\n)\nout &lt;- summary(out_geomin_orh, sort = FALSE, suppress = 0.3)\nprint(out)\n\nStandardized Rotated Factor Loadings\n\n\n   factor1 factor2 factor3\nx1  0.315*         -0.621*\nx2                 -0.474*\nx3                 -0.671*\nx4  0.838*                \nx5  0.867*                \nx6  0.814*                \nx7          0.696*        \nx8          0.677*        \nx9          0.456* -0.468*\n\nFactor Correlation\n        factor1 factor2 factor3\nfactor1       1       0       0\nfactor2       0       1       0\nfactor3       0       0       1\n\nMethod of rotation: Geomin \n\nTest Statistics for Standardized Rotated Factor Loadings\n\n\n       lhs op rhs std.loading    se       z     p ci.lower ci.upper\n1  factor1 =~  x1       0.315 0.134   2.352 0.019    0.053    0.578\n2  factor1 =~  x2       0.130 0.118   1.102 0.271   -0.101    0.360\n3  factor1 =~  x3       0.074 0.123   0.599 0.549   -0.168    0.316\n4  factor1 =~  x4       0.838 0.036  23.402 0.000    0.767    0.908\n5  factor1 =~  x5       0.867 0.025  35.190 0.000    0.819    0.915\n6  factor1 =~  x6       0.814 0.040  20.427 0.000    0.736    0.892\n7  factor1 =~  x7       0.112 0.068   1.645 0.100   -0.021    0.245\n8  factor1 =~  x8       0.085 0.063   1.341 0.180   -0.039    0.209\n9  factor1 =~  x9       0.172 0.095   1.803 0.071   -0.015    0.359\n10 factor2 =~  x1       0.053 0.061   0.868 0.386   -0.067    0.173\n11 factor2 =~  x2      -0.099 0.070  -1.409 0.159   -0.237    0.039\n12 factor2 =~  x3       0.033 0.048   0.698 0.485   -0.060    0.127\n13 factor2 =~  x4       0.051 0.046   1.115 0.265   -0.039    0.140\n14 factor2 =~  x5       0.054 0.061   0.878 0.380   -0.066    0.173\n15 factor2 =~  x6       0.035 0.037   0.924 0.355   -0.039    0.108\n16 factor2 =~  x7       0.696 0.090   7.702 0.000    0.519    0.874\n17 factor2 =~  x8       0.677 0.088   7.660 0.000    0.504    0.850\n18 factor2 =~  x9       0.456 0.073   6.261 0.000    0.314    0.599\n19 factor3 =~  x1      -0.621 0.101  -6.148 0.000   -0.818   -0.423\n20 factor3 =~  x2      -0.474 0.074  -6.414 0.000   -0.619   -0.329\n21 factor3 =~  x3      -0.671 0.058 -11.658 0.000   -0.784   -0.558\n22 factor3 =~  x4      -0.129 0.161  -0.800 0.424   -0.445    0.187\n23 factor3 =~  x5      -0.048 0.174  -0.277 0.782   -0.389    0.293\n24 factor3 =~  x6      -0.176 0.164  -1.071 0.284   -0.497    0.146\n25 factor3 =~  x7      -0.021 0.059  -0.354 0.724   -0.137    0.095\n26 factor3 =~  x8      -0.257 0.094  -2.720 0.007   -0.442   -0.072\n27 factor3 =~  x9      -0.468 0.105  -4.472 0.000   -0.673   -0.263\n       lhs op rhs std.loading    se       z     p ci.lower ci.upper\n1  factor1 =~  x1       0.315 0.134   2.352 0.019    0.053    0.578\n2  factor1 =~  x2       0.130 0.118   1.102 0.271   -0.101    0.360\n3  factor1 =~  x3       0.074 0.123   0.599 0.549   -0.168    0.316\n4  factor1 =~  x4       0.838 0.036  23.402 0.000    0.767    0.908\n5  factor1 =~  x5       0.867 0.025  35.190 0.000    0.819    0.915\n6  factor1 =~  x6       0.814 0.040  20.427 0.000    0.736    0.892\n7  factor1 =~  x7       0.112 0.068   1.645 0.100   -0.021    0.245\n8  factor1 =~  x8       0.085 0.063   1.341 0.180   -0.039    0.209\n9  factor1 =~  x9       0.172 0.095   1.803 0.071   -0.015    0.359\n10 factor2 =~  x1       0.053 0.061   0.868 0.386   -0.067    0.173\n11 factor2 =~  x2      -0.099 0.070  -1.409 0.159   -0.237    0.039\n12 factor2 =~  x3       0.033 0.048   0.698 0.485   -0.060    0.127\n13 factor2 =~  x4       0.051 0.046   1.115 0.265   -0.039    0.140\n14 factor2 =~  x5       0.054 0.061   0.878 0.380   -0.066    0.173\n15 factor2 =~  x6       0.035 0.037   0.924 0.355   -0.039    0.108\n16 factor2 =~  x7       0.696 0.090   7.702 0.000    0.519    0.874\n17 factor2 =~  x8       0.677 0.088   7.660 0.000    0.504    0.850\n18 factor2 =~  x9       0.456 0.073   6.261 0.000    0.314    0.599\n19 factor3 =~  x1      -0.621 0.101  -6.148 0.000   -0.818   -0.423\n20 factor3 =~  x2      -0.474 0.074  -6.414 0.000   -0.619   -0.329\n21 factor3 =~  x3      -0.671 0.058 -11.658 0.000   -0.784   -0.558\n22 factor3 =~  x4      -0.129 0.161  -0.800 0.424   -0.445    0.187\n23 factor3 =~  x5      -0.048 0.174  -0.277 0.782   -0.389    0.293\n24 factor3 =~  x6      -0.176 0.164  -1.071 0.284   -0.497    0.146\n25 factor3 =~  x7      -0.021 0.059  -0.354 0.724   -0.137    0.095\n26 factor3 =~  x8      -0.257 0.094  -2.720 0.007   -0.442   -0.072\n27 factor3 =~  x9      -0.468 0.105  -4.472 0.000   -0.673   -0.263\n\n\n\n\n37.5.5 Oblique Geomin\nLa rotazione Geomin può anche essere usata per ottenere una soluzione obliqua.\n\nout_geomin_obl &lt;- oblqRotate(\n    unrotated, \n    method = \"geomin\"\n)\nout &lt;- summary(out_geomin_obl, sort = FALSE, suppress = 0.3)\nprint(out)\n\nStandardized Rotated Factor Loadings\n\n\n   factor1 factor2 factor3\nx1                 -0.604*\nx2                 -0.507*\nx3                 -0.691*\nx4  0.839*                \nx5  0.887*                \nx6  0.806*                \nx7          0.726*        \nx8          0.703*        \nx9          0.463* -0.368*\n\nFactor Correlation\n        factor1 factor2 factor3\nfactor1   1.000   0.230  -0.327\nfactor2   0.230   1.000  -0.278\nfactor3  -0.327  -0.278   1.000\n\nMethod of rotation: Geomin \n\nTest Statistics for Standardized Rotated Factor Loadings\n\n\n       lhs op rhs std.loading    se       z     p ci.lower ci.upper\n1  factor1 =~  x1       0.188 0.070   2.670 0.008    0.050    0.326\n2  factor1 =~  x2       0.044 0.054   0.806 0.420   -0.063    0.150\n3  factor1 =~  x3      -0.073 0.049  -1.467 0.142   -0.170    0.024\n4  factor1 =~  x4       0.839 0.032  26.467 0.000    0.777    0.901\n5  factor1 =~  x5       0.887 0.029  30.077 0.000    0.829    0.945\n6  factor1 =~  x6       0.806 0.030  26.717 0.000    0.747    0.865\n7  factor1 =~  x7       0.031 0.034   0.915 0.360   -0.036    0.099\n8  factor1 =~  x8      -0.045 0.048  -0.948 0.343   -0.139    0.048\n9  factor1 =~  x9       0.025 0.034   0.747 0.455   -0.041    0.091\n10 factor2 =~  x1       0.029 0.051   0.567 0.571   -0.071    0.129\n11 factor2 =~  x2      -0.119 0.072  -1.664 0.096   -0.260    0.021\n12 factor2 =~  x3       0.020 0.037   0.538 0.591   -0.053    0.092\n13 factor2 =~  x4       0.007 0.043   0.174 0.862   -0.076    0.091\n14 factor2 =~  x5       0.010 0.036   0.285 0.775   -0.060    0.080\n15 factor2 =~  x6      -0.009 0.030  -0.309 0.758   -0.068    0.049\n16 factor2 =~  x7       0.726 0.072  10.065 0.000    0.585    0.868\n17 factor2 =~  x8       0.703 0.118   5.963 0.000    0.472    0.934\n18 factor2 =~  x9       0.463 0.080   5.810 0.000    0.307    0.619\n19 factor3 =~  x1      -0.604 0.081  -7.438 0.000   -0.763   -0.445\n20 factor3 =~  x2      -0.507 0.073  -6.983 0.000   -0.649   -0.364\n21 factor3 =~  x3      -0.691 0.061 -11.373 0.000   -0.810   -0.572\n22 factor3 =~  x4      -0.024 0.034  -0.702 0.482   -0.091    0.043\n23 factor3 =~  x5       0.065 0.045   1.459 0.144   -0.022    0.153\n24 factor3 =~  x6      -0.080 0.048  -1.679 0.093   -0.173    0.013\n25 factor3 =~  x7       0.150 0.107   1.403 0.161   -0.060    0.360\n26 factor3 =~  x8      -0.106 0.164  -0.645 0.519   -0.428    0.216\n27 factor3 =~  x9      -0.368 0.133  -2.770 0.006   -0.629   -0.108\n       lhs op rhs std.loading    se       z     p ci.lower ci.upper\n1  factor1 =~  x1       0.188 0.070   2.670 0.008    0.050    0.326\n2  factor1 =~  x2       0.044 0.054   0.806 0.420   -0.063    0.150\n3  factor1 =~  x3      -0.073 0.049  -1.467 0.142   -0.170    0.024\n4  factor1 =~  x4       0.839 0.032  26.467 0.000    0.777    0.901\n5  factor1 =~  x5       0.887 0.029  30.077 0.000    0.829    0.945\n6  factor1 =~  x6       0.806 0.030  26.717 0.000    0.747    0.865\n7  factor1 =~  x7       0.031 0.034   0.915 0.360   -0.036    0.099\n8  factor1 =~  x8      -0.045 0.048  -0.948 0.343   -0.139    0.048\n9  factor1 =~  x9       0.025 0.034   0.747 0.455   -0.041    0.091\n10 factor2 =~  x1       0.029 0.051   0.567 0.571   -0.071    0.129\n11 factor2 =~  x2      -0.119 0.072  -1.664 0.096   -0.260    0.021\n12 factor2 =~  x3       0.020 0.037   0.538 0.591   -0.053    0.092\n13 factor2 =~  x4       0.007 0.043   0.174 0.862   -0.076    0.091\n14 factor2 =~  x5       0.010 0.036   0.285 0.775   -0.060    0.080\n15 factor2 =~  x6      -0.009 0.030  -0.309 0.758   -0.068    0.049\n16 factor2 =~  x7       0.726 0.072  10.065 0.000    0.585    0.868\n17 factor2 =~  x8       0.703 0.118   5.963 0.000    0.472    0.934\n18 factor2 =~  x9       0.463 0.080   5.810 0.000    0.307    0.619\n19 factor3 =~  x1      -0.604 0.081  -7.438 0.000   -0.763   -0.445\n20 factor3 =~  x2      -0.507 0.073  -6.983 0.000   -0.649   -0.364\n21 factor3 =~  x3      -0.691 0.061 -11.373 0.000   -0.810   -0.572\n22 factor3 =~  x4      -0.024 0.034  -0.702 0.482   -0.091    0.043\n23 factor3 =~  x5       0.065 0.045   1.459 0.144   -0.022    0.153\n24 factor3 =~  x6      -0.080 0.048  -1.679 0.093   -0.173    0.013\n25 factor3 =~  x7       0.150 0.107   1.403 0.161   -0.060    0.360\n26 factor3 =~  x8      -0.106 0.164  -0.645 0.519   -0.428    0.216\n27 factor3 =~  x9      -0.368 0.133  -2.770 0.006   -0.629   -0.108",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>La rotazione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/04_rotazione.html#interpretazione-dei-fattori-latenti-nellanalisi-fattoriale",
    "href": "chapters/extraction/04_rotazione.html#interpretazione-dei-fattori-latenti-nellanalisi-fattoriale",
    "title": "37  La rotazione fattoriale",
    "section": "37.6 Interpretazione dei fattori latenti nell’analisi fattoriale",
    "text": "37.6 Interpretazione dei fattori latenti nell’analisi fattoriale\nNell’interpretare i fattori comuni latenti in un’analisi fattoriale, è cruciale scegliere tra la matrice pattern e la matrice struttura. Entrambe sono utili per l’interpretazione, ma forniscono informazioni diverse:\n\nMatrice pattern:\n\nMostra le saturazioni fattoriali dirette.\nIndica gli effetti diretti dei fattori latenti sulle variabili manifeste.\nRivela quanto ciascun fattore contribuisce direttamente alla varianza di una variabile osservata.\nFondamentale per comprendere il significato psicologico dei fattori.\n\nMatrice struttura:\n\nRappresenta le correlazioni tra fattori latenti e variabili osservate.\nInclude sia gli effetti diretti che quelli indiretti.\nDescrive la covariazione complessiva tra fattori e variabili manifeste.\nNon distingue tra relazioni dirette e indirette.\n\n\nUn fattore identificato nell’analisi fattoriale rappresenta una variabile latente univariata, ovvero una dimensione sottostante che cattura l’essenza di un fenomeno psicologico. L’interpretazione del fattore emerge dall’intersezione dei significati delle variabili che vi saturano.\nNel caso di rotazioni oblique, dove i fattori sono correlati, è essenziale interpretare ogni fattore come una dimensione psicologica distinta. Ad esempio, l’etichetta assegnata al fattore \\(F_1\\) deve essere concettualmente separata dal fenomeno rappresentato dal fattore \\(F_2\\), nonostante la loro correlazione.\nAdottando questa strategia interpretativa, la matrice pattern diventa lo strumento principale per l’interpretazione. I suoi coefficienti riflettono gli effetti diretti dei fattori latenti sulle variabili manifeste, indicando un’influenza “causale” del fattore su tali variabili. La matrice struttura, invece, descrive le correlazioni complessive tra variabili manifeste e fattori, includendo sia relazioni dirette che indirette.\nIn conclusione, per un’interpretazione più accurata e teoricamente solida dei fattori latenti, è preferibile basarsi sulla matrice pattern, che fornisce informazioni specifiche sugli effetti diretti tra fattori comuni e variabili manifeste.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>La rotazione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/04_rotazione.html#session-info",
    "href": "chapters/extraction/04_rotazione.html#session-info",
    "title": "37  La rotazione fattoriale",
    "section": "37.7 Session Info",
    "text": "37.7 Session Info\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] GPArotation_2024.3-1 MASS_7.3-61          viridis_0.6.5       \n [4] viridisLite_0.4.2    ggpubr_0.6.0         ggExtra_0.10.1      \n [7] gridExtra_2.3        patchwork_1.3.0      bayesplot_1.11.1    \n[10] semTools_0.5-6       semPlot_1.1.6        lavaan_0.6-19       \n[13] psych_2.4.6.26       scales_1.3.0         markdown_1.13       \n[16] knitr_1.49           lubridate_1.9.3      forcats_1.0.0       \n[19] stringr_1.5.1        dplyr_1.1.4          purrr_1.0.2         \n[22] readr_2.1.5          tidyr_1.3.1          tibble_3.2.1        \n[25] ggplot2_3.5.1        tidyverse_2.0.0      here_1.0.1          \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.17.1   jsonlite_1.8.9      magrittr_2.0.3     \n  [4] TH.data_1.1-2       estimability_1.5.1  farver_2.1.2       \n  [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n [10] Cairo_1.6-2         minqa_1.2.8         base64enc_0.1-3    \n [13] rstatix_0.7.2       htmltools_0.5.8.1   broom_1.0.7        \n [16] Formula_1.2-5       htmlwidgets_1.6.4   plyr_1.8.9         \n [19] sandwich_3.1-1      emmeans_1.10.5      zoo_1.8-12         \n [22] uuid_1.2-1          igraph_2.1.1        mime_0.12          \n [25] lifecycle_1.0.4     pkgconfig_2.0.3     Matrix_1.7-1       \n [28] R6_2.5.1            fastmap_1.2.0       shiny_1.9.1        \n [31] numDeriv_2016.8-1.1 digest_0.6.37       OpenMx_2.21.13     \n [34] fdrtool_1.2.18      colorspace_2.1-1    rprojroot_2.0.4    \n [37] Hmisc_5.2-0         fansi_1.0.6         timechange_0.3.0   \n [40] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n [43] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n [46] carData_3.0-5       ggsignif_0.6.4      corpcor_1.6.10     \n [49] gtools_3.9.5        tools_4.4.2         pbivnorm_0.6.0     \n [52] foreign_0.8-87      zip_2.3.1           httpuv_1.6.15      \n [55] nnet_7.3-19         glue_1.8.0          quadprog_1.5-8     \n [58] promises_1.3.0      nlme_3.1-166        lisrelToR_0.3      \n [61] grid_4.4.2          pbdZMQ_0.3-13       checkmate_2.3.2    \n [64] cluster_2.1.6       reshape2_1.4.4      generics_0.1.3     \n [67] gtable_0.3.6        tzdb_0.4.0          data.table_1.16.2  \n [70] hms_1.1.3           car_3.1-3           utf8_1.2.4         \n [73] sem_3.1-16          pillar_1.9.0        IRdisplay_1.1      \n [76] rockchalk_1.8.157   later_1.3.2         splines_4.4.2      \n [79] cherryblossom_0.1.0 lattice_0.22-6      survival_3.7-0     \n [82] kutils_1.73         tidyselect_1.2.1    miniUI_0.1.1.1     \n [85] pbapply_1.7-2       airports_0.1.0      stats4_4.4.2       \n [88] xfun_0.49           qgraph_1.9.8        arm_1.14-4         \n [91] stringi_1.8.4       pacman_0.5.1        boot_1.3-31        \n [94] evaluate_1.0.1      codetools_0.2-20    mi_1.1             \n [97] cli_3.6.3           RcppParallel_5.1.9  IRkernel_1.3.2     \n[100] rpart_4.1.23        xtable_1.8-4        repr_1.1.7         \n[103] munsell_0.5.1       Rcpp_1.0.13-1       coda_0.19-4.1      \n[106] png_0.1-8           XML_3.99-0.17       parallel_4.4.2     \n[109] usdata_0.3.1        jpeg_0.1-10         lme4_1.1-35.5      \n[112] mvtnorm_1.3-2       openxlsx_4.2.7.1    crayon_1.5.3       \n[115] openintro_2.5.0     rlang_1.1.4         multcomp_1.4-26    \n[118] mnormt_2.1.1       \n\n\n\n\n\n\nRencher, A. (2002). Methods of multivariate analysis. 2002. Wiley Publications.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>La rotazione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/05_val_soluzione.html",
    "href": "chapters/extraction/05_val_soluzione.html",
    "title": "38  Valutare e rifinire la soluzione fattoriale",
    "section": "",
    "text": "38.1 Valutazione della matrice pattern\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nLa maggior parte di strumenti usati nell’assessment psicologico e neuropsicologico non valuta una singola dimensione psicologica, ma piuttosto misura molteplici aspetti di un costrutto. Di conseguenza, l’analisi fattoriale produce solitamente una soluzione a più fattori. Idealmente, dopo la rotazione, ciascun item saturerà fortemente su un singolo fattore e debolmente sugli altri. In realtà, anche dopo la rotazione degli assi fattoriali, spesso si presentano item che saturano debolmente su tutti i fattori, oppure item che saturano fortemente su più di un fattore.\nUno dei primi passi da compiere per rifinire la soluzione fattoriale è quello di valutare la matrice struttura e intervenire utilizzando il criterio della “struttura semplice”, per poi valutare gli effetti delle azioni intraprese (es., eliminazione di alcuni item) nella matrice pattern. Ricordiamo che la matrice struttura contiene le correlazioni tra item e fattori, mentre la matrice pattern contiene le saturazioni fattoriali.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Valutare e rifinire la soluzione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/05_val_soluzione.html#valutazione-della-matrice-pattern",
    "href": "chapters/extraction/05_val_soluzione.html#valutazione-della-matrice-pattern",
    "title": "38  Valutare e rifinire la soluzione fattoriale",
    "section": "",
    "text": "38.1.1 Item con basse saturazioni su tutti i fattori\nPrima di procedere con l’analisi fattoriale è auspicabile esaminare la matrice di correlazioni tra gli item ed eliminare quegli item che sono insufficientemente correlati con gli altri item della matrice. Tuttavia, anche dopo questo screening iniziale, è possibile che vi siano item caratterizzati da saturazioni basse su tutti i fattori. Dal punto di vista pratico, si considerano “basse” le saturazioni il cui valore assoluto è minore di 0.30 (Hair et al., 1995). Hair e collaboratori suggeriscono due soluzioni nel caso di item con saturazioni basse su tutti i fattori:\n\neliminare gli item con basse saturazioni,\nvalutare le comunalità degli item problematici e il contributo specifico che forniscono allo strumento.\n\nSe un item ha una bassa comunalità, o se il contributo di un item nei confronti del significato generale dello strumento è di poca importanza, allora l’item dovrebbe essere eliminato. Dopo l’eliminazione degli item critici, si procede calcolando una nuova soluzione fattoriale e si esaminano i risultati ottenuti.\nSe vi sono degli item con basse saturazioni su tutti i fattori che però contribuiscono in maniera importante a determinare il significato della scala nel suo complesso, allora questi item dovrebbero essere mantenuti. Alle volte, per tali item è possibile creare delle sottoscale separate dalle altre.\n\n\n38.1.2 Item con saturazioni evevate su più di un fattore\nÈ comune anche il caso opposto, ovvero quello nel quale ci sono item che saturano su fattori multipli (con saturazioni fattoriali \\(&gt;\\) .30), specialmente nel caso di soluzioni fattoriali ottenutie dopo una rotazione obliqua. Kline (2000) suggerisce di eliminare tali item in quanto rendono difficile da interpretare il significato della scala che così si ottiene. Hair e collaboratori (1995) ritengono invece che tali item dovrebbero essere mantenuti, dato possono chiarire il significato dei fattori che la scala identifica.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Valutare e rifinire la soluzione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/05_val_soluzione.html#valutazione-dellattendibilità",
    "href": "chapters/extraction/05_val_soluzione.html#valutazione-dellattendibilità",
    "title": "38  Valutare e rifinire la soluzione fattoriale",
    "section": "38.2 Valutazione dell’attendibilità",
    "text": "38.2 Valutazione dell’attendibilità\nAll’interno del problema della costruzione di uno strumento vengono esaminati tre aspetti dell’attendibilità: la consistenza interna, la stabilità e l’equivalenza.\n\n38.2.1 Consistenza interna\n\n38.2.1.1 La procedura split-half\nLa consistenza interna misura il grado di coerenza tra gli item che costituiscono lo strumento o le sottoscale dello strumento. Se tutti gli item che costituiscono uno strumento o una sua sottoscala misurano la stessa cosa, allora saranno fortemente associati tra loro.\nÈ possibile misurare la consistenza interna con il metodo dello split-half, ovvero mediante la correlazione di Pearson tra i punteggi ottenuti utilizzando ciascuna delle due metà degli item dello strumento. Usando un software, è meglio trovare la media delle correlazioni inter-item ricavabili a partire da tutte le possibili divisioni a metà dell’insieme di item che costituiscono lo strumento. La correlazione trovata in questo modo viene poi corretta utilizzando la formula “profetica” di Spearman-Brown per tenere in considerazione il fatto che l’attendibilità è stata calcolata utilizzando soltanto metà degli item dello strumento.\nSi noti che la formula di Spearman-Brown è basata sull’assunzione che le due metà dello strumento siano parallele, ovvero che abbiano identici punteggi veri e uguali varianze d’errore (questa assunzione comporta la conseguenza per cui le due metà degli item devono producono punteggi aventi la stessa media e la stessa varianza). Se queste assunzioni molto stringenti non vengono soddisfatte, allora la procedura descritta sopra conduce ad una sovrastima dell’attendibilità quale consisenza interna della scala.\n\n\n38.2.1.2 L’analisi della varianza\nSe tutti gli item di uno strumento o di una sottoscala sono espressione dello stesso costrutto, allora ci dobbiamo aspettare che anche le medie dei punteggi sugli item siano uguali. Come è stato detto sopra, questa è infatti una delle assunzioni delle forme strettamente parallele di un test. È dunque possibile verificare questa assunzione mediante un’ANOVA che sottopone a test l’ipotesi nulla dell’uguaglianza delle medie di gruppi. Nel caso degli item di un test, dato che ciascun soggetto completa tutti gli item che costituiscono lo strumento, è appropriato usare un’ANOVA per misure ripetute che, nella sua declinazione più moderna, corrisponde ad un modello multi-livello (mixed-effect model).\n\n\n38.2.1.3 L’indice \\(\\alpha\\) di Cronbach\nL’indice \\(\\alpha\\) di Cronbach è comunque la misura più utilizzata per valutare l’attendibilità quale consistenza interna di uno strumento. L’\\(\\alpha\\) di Cronbach è stato interpretato come la proporzione di varianza della scala che può essere attribuita al fattore comune (DeVellis, 1991). Può anche essere interpretato come la correlazione stimata tra i punteggi della scala e un’altro strumento della stessa lunghezza tratto dall’universo degli item possibili che costituiscono il dominio del costrutto (Kline, 1986). La radice quadrata del coefficiente \\(\\alpha\\) di Cronbach rappresenta la correlazione stimata tra i punteggi ottenuti tramite lo strumento e i punteggi veri (Nunnally & Bernstein, 1994).\nIn precedenza abbiamo descritto una serie di limiti del coefficiente \\(\\alpha\\) di Cronbach. In generale, molti ricercatori suggeriscono di usare al suo posto l’indice \\(\\omega\\) di McDonald.\n\n\n\n38.2.2 Stabilità temporale\nLa stabilità temporale viene valutata attraverso la procedura di test-retest. La correlazione tra le misure ottenute in due momenti negli stessi rispondenti ci fornisce l’attendibilità di test-retest.\nKline (2000) ha messo in evidenza come l’attendibilità di test-retest sia influenzata da molteplici fattori, tra cui le caratteristiche del campione, la maturità dei rispondenti, i cambiamenti nello stato emozionale, le differenze nelle condizioni di somministrazione del test, la possibilità di ricordare le risposte date in precedenza, la difficoltà degli item, la grandezza del campione e le caratteristiche del costrutto (ad esempio, stato vs. tratto).\nParticolare attenzione deve essere rivolta all’intervallo temporale usato nella procedura di test-retest. Se il periodo di tempo che intercorre tra le due somministrazioni è troppo corto, i risultati possono risultare distorti a causa del fatto che i soggetti si ricordano le risposte date in precedenza. Questo può condurre ad una sovrastima dell’attendibilità test-retest (Pedhazur & Schmelkin, 1991). Un intervallo temporale troppo lungo tra le due somministrazioni ha invece come limite il fatto che, in questo caso, vi è un’alta possibilità che intervengano dei cambiamenti nei rispondenti rispetto al costrutto in esame. Alla luce di queste considerazioni è stato suggerito di utilizzare un intervallo temporale abbastanza breve, ovvero di una o due settimane (Nunnally & Bernstein, 1994; Pedhazur & Schmelkin, 1991). Se è necessario valutare la stabilità temporale nel corso di un lungo arco temporale, Nunnally e Bernstein (1994) suggeriscono di utilizzare un intervallo di sei mesi o maggiore.\n\n\n38.2.3 Equivalenza\nPer cercare di evitare i problemi associati all’attendibilità quale stabilità temporale, alcuni autori si sono posti il problema di esaminare la correlazione tra forme parallele (o equivalenti) dello strumento. La correlazione tra forme parallele di uno strumento va sotto il nome di coefficiente di equivalenza e fornisce una misura alternativa dell’attendibilità dello strumento (Burns & Grove, 2001; Pedhazur & Schmelkin, 1991; Polit & Hungler, 1999).\nNunnally e Bernstein (1994) suggeriscono di confrontare i risultati ottenuti con la somministrazione delle forme parallele lo stesso giorno con quelli ottenuti nel caso di un intervallo temporale di due settimane. Kline (2000) ritiene che l’attendibilità tra due forme parallele debba essere di almeno 0.9 perché, per valori inferiori, sarebbe difficile sostenere che le forme sono veramente parallele.\nÈ tuttavia molto oneroso predisporre due forme parallele di uno strumento. Per questa ragione, il coefficiente di equivalenza viene raramente usato.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Valutare e rifinire la soluzione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/05_val_soluzione.html#selezione-di-un-sottoinsieme-di-item",
    "href": "chapters/extraction/05_val_soluzione.html#selezione-di-un-sottoinsieme-di-item",
    "title": "38  Valutare e rifinire la soluzione fattoriale",
    "section": "38.3 Selezione di un sottoinsieme di item",
    "text": "38.3 Selezione di un sottoinsieme di item\nTipicamente, la costruzione di un test viene realizzata somministrando un grande numero di item per poi selezionare gli item “migliori” che andranno a fare parte del test vero e proprio. Si supponga di somministrare inizialmente \\(m\\) item, quando si desidera che il test finale sia costituito da \\(p &lt; m\\) item. Un modo di affrontare questo problema potrebbe essere quello di calcolare l’attendibilità del test (coefficiente \\(\\omega\\)) per tutti i possibili sottoinsiemi di \\(p\\) item, così da individuare il sottoinsieme migliore. Questo modo di procedere, però, è problematico perché richiede la valutazione di un elevatissimo numero di possibilità. Per esempio, da un insieme iniziale neanche troppo numeroso di 100 item, il numero di sottoinsiemi di 20 item è uguale a\n\\[\n\\binom{100}{20} = 5.36 \\times 10^{20}.\n\\]\nÈ dunque necessario trovare metodi alternativi che evitino una tale esplosione combinatoria. A questo fine, ovvero per procedere alla selezione del sottoinsieme dei “migliori” item, {cite:t}mcdonald2013test suggerisce di calcolare la quantità di informazione di ciascun item. La quantità di informazione di un item è definita come rapporto tra segnale/rumore, in relazione alla scomposizione della varianza dell’item:\n\\[\n\\frac{\\lambda_i^2}{\\psi_{ii}}.\n\\]\n{cite:t}mcdonald2013test mostra come l’omissione di uno o più item produce sempre una riduzione dell’attendibilità del test (ovvero, una riduzione nel valore del coefficiente \\(\\omega\\)). Tuttavia, tale riduzione è tanto più piccola quanto più piccola è la quantità di informazione degli item omessi. Il processo di selezione degli item può dunque essere guidato da un semplice principio: si selezionano gli item aventi la quantità di informazione maggiore. Ovvero, in altre parole, si rimuovono gli item aventi la quantità di informazione più bassa.\nEsempio. Per fare un esempio, consideriamo nuovamente la matrice di varianze e di covarianze della scala SWLS.\n\nvarnames &lt;- c(\"Y1\", \"Y2\", \"Y3\", \"Y4\", \"Y5\")\nSWLS &lt;- matrix(c(\n  2.565, 1.424, 1.481, 1.328, 1.529,\n  1.424, 2.493, 1.267, 1.051, 1.308,\n  1.481, 1.267, 2.462, 1.093, 1.360,\n  1.328, 1.051, 1.093, 2.769, 1.128,\n  1.529, 1.308, 1.360, 1.128, 3.355\n),\nncol = 5, byrow = TRUE,\ndimnames = list(varnames, varnames)\n)\nSWLS\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nY1\nY2\nY3\nY4\nY5\n\n\n\n\nY1\n2.565\n1.424\n1.481\n1.328\n1.529\n\n\nY2\n1.424\n2.493\n1.267\n1.051\n1.308\n\n\nY3\n1.481\n1.267\n2.462\n1.093\n1.360\n\n\nY4\n1.328\n1.051\n1.093\n2.769\n1.128\n\n\nY5\n1.529\n1.308\n1.360\n1.128\n3.355\n\n\n\n\n\nUtilizzando la funzione cfa() contenuta nel pacchetto lavaan, il modello ad un fattore viene definito nel modo seguente.\n\nmod_1 &lt;- \"\n  F =~ Y1 + Y2 + Y3 + Y4 + Y5\n\"\n\nOtteniamo così una stima dei pesi fattoriali e delle unicità.\n\nfit &lt;- lavaan::cfa(\n  mod_1,\n  sample.cov = SWLS,\n  sample.nobs = 215,\n  std.lv = TRUE\n)\n\nCalcoliamo la quantità di informazione fornita da ciascun item. Iniziamo a estrarre dall’oggetto fit la matrice delle saturazioni fattoriali.\n\nlambda &lt;- inspect(fit, what=\"std\")$lambda\nlambda\n\n\nA lavaan.matrix: 5 x 1 of type dbl\n\n\n\nF\n\n\n\n\nY1\n0.8166956\n\n\nY2\n0.6941397\n\n\nY3\n0.7257827\n\n\nY4\n0.5905795\n\n\nY5\n0.6429117\n\n\n\n\n\nEstraiamo da fit le specificità.\n\ntheta &lt;- diag(inspect(fit, what=\"std\")$theta)\ntheta\n\nY10.333008280668232Y20.518170080229262Y30.473239478247548Y40.651215859909304Y50.586664570302436\n\n\nPossiamo ora calcolare quantità di informazione degli item facendo il rapporto tra ciascuna saturazione fattoriale innalzata al quadrato e la corrispondente specificità.\n\nfor (i in 1:5) {\n  print(lambda[i]^2 / theta[i])\n}\n\n      Y1 \n2.002928 \n       Y2 \n0.9298683 \n      Y3 \n1.113095 \n       Y4 \n0.5355891 \n       Y5 \n0.7045515 \n\n\nIl risultato ottenuto indica che il quarto item è il meno informativo e che il quinto item è il secondo meno informativo. Se un solo item deve essere eliminato, dunque, elimineremo il quarto item. Se devono essere eliminati due item, andranno eliminati il quarto e il quinto item.\n\n38.3.1 Attendibilità e numero di item\nDi quanto cambia l’attendibilità di uno strumento se viene variato il numero di item? Una risposta a questa domanda può essere fornita dalla formula profetica di Spearman-Brown. Supponiamo che nella formula di Spearman-Brown,\n\\[\n\\begin{equation}\n  \\rho_p = \\frac{p \\rho_1}{(p-1)\\rho_1 + 1},\n\\end{equation}\n\\](eq-spearman-brown)\n\\(\\rho_1\\) rappresenti l’attendibilità di un test costituito da un certo numero di item. Se poniamo \\(p=2\\), la {eq}eq-spearman-brown ci fornisce una stima dell’attendibilità che si otterrebbe raddoppiando il numero di item nel test. Valori di \\(p\\) minori di \\(1\\), invece, vengono usati per predire la diminuizione dell’attendibilità conseguente ad una diminuzione nel numero degli item del test.\nRicordiamo però che le predizioni della formula di Spearman-Brown sono accurate solo se la forma allungata o accorciata del test è parallela rispetto al test considerato. Per esempio, se ad un test con un coefficiente di attendibilità molto alto vengono aggiunti item aventi una bassa attendibilità, allora l’attendibilità del test allungato sarà minore di quella predetta dalla formula di Spearman-Brown.\nAnche se la formula di Spearman-Brown ha un ruolo centrale nella teoria classica dei test, si tenga conto che non rappresenta l’unico strumento che può essere utilizzato per valutare la relazione tra attendibilità e numero degli item del test. La quantità detta informazione dell’item (item information), formulata dai modelli IRT, consente di predire i cambiamenti nella qualità della misura a seguito dell’aggiunta o della cancellazione di un sottoinsieme di item.\nEsempio. Si consideri la scala SWLS. Chiediamoci come varia l’attendibilità della scala se il numero di item aumenta da 5 a 20. Poniamo che l’attendibilità della scala SWLS costituita da 5 item sia uguale a 0.824. Applicando la formula di Spearman-Brown otteniamo la stima seguente.\n\n(4 * 0.824) / ((4 - 1) * 0.824 + 1)\n\n0.949308755760369\n\n\nEsempio. Possiamo giungere al risultato precedente in un altro modo. Supponiamo che i 15 item aggiuntivi abbiano le stesse saturazioni fattoriali medie (\\(\\bar{\\lambda}\\)) e le stesse varianze specifiche medie (\\(\\bar{\\psi}\\)) rispetto agli item originali. Mediante gli item di cui disponiamo, stimiamo l’attendibilità di un “item medio” nel modo seguente\n\\[\n\\rho_1 = \\frac{\\bar{\\lambda}^2}{\\bar{\\lambda}^2 + \\bar{\\psi}},\n\\]\novvero otteniamo la stima di 0.48:\n\nrho_1 &lt;- mean(lambda)^2 / (mean(lambda)^2 + mean(theta)) \nrho_1\n\n0.484512352433458\n\n\nL’attendibilità predetta di un test costituito da 20 item sarà dunque uguale a\n\n(20 * rho_1) / ((20 - 1) * rho_1 + 1) \n\n0.949490393106468\n\n\nil che replica il risultato ottenuto precedentemente.\nEsempio. Un altro modo ancora per ottenere lo stesso risultato è quello di utilizzare un modello mono-fattoriale per item paralleli.\n\nmod_2 &lt;- \"\n  F =~ a*Y1 + a*Y2 + a*Y3 + a*Y4 + a*Y5\n  Y1 ~~ b*Y1\n  Y2 ~~ b*Y2\n  Y3 ~~ b*Y3\n  Y4 ~~ b*Y4\n  Y5 ~~ b*Y5\n\"\n\nAdattiamo il modello ai dati.\n\nfit2 &lt;- lavaan::cfa(\n  mod_2,\n  sample.cov = SWLS,\n  sample.nobs = 215,\n  std.lv = TRUE\n)\n\nEstraiamo dall’oggetto fit2 le saturazioni fattoriali.\n\nlambda &lt;- inspect(fit2, what=\"std\")$lambda\nlambda\n\n\nA lavaan.matrix: 5 x 1 of type dbl\n\n\n\nF\n\n\n\n\nY1\n0.6893938\n\n\nY2\n0.6893938\n\n\nY3\n0.6893938\n\n\nY4\n0.6893938\n\n\nY5\n0.6893938\n\n\n\n\n\nEstraiamo da fit2 le specificità.\n\ntheta &lt;- diag(inspect(fit2, what=\"std\")$theta)\ntheta\n\nY10.524736147775294Y20.524736147775294Y30.524736147775294Y40.524736147775294Y50.524736147775294\n\n\nCalcoliamo l’attendibilità dell’item “medio” usando \\(\\lambda\\) e \\(\\psi\\) (chiamato theta da lavaan).\n\nrho_1 &lt;- lambda[1]^2 / (lambda[1]^2 + theta[2])\nrho_1 \n\nY2: 0.475263852224706\n\n\nPosso ora applicare la formula di Spearman-Brown.\n\n(20 * rho_1) / ((20 - 1) * rho_1 + 1) \n\nY2: 0.94768340402785\n\n\nIl risultato è praticamente identico a quelli trovati in precedenza.\n\n\n38.3.2 Numero di item e affidabilità\nLa formula di Spearman-Brown può anche essere riarrangiata in maniera tale da consentirci di predire il numero degli item necessari per raggiungere un determinato livello di affidabilità:\n\\[\n\\begin{equation}\np = \\frac{\\rho_p (1-\\rho_1)}{\\rho_1(1-\\rho_p)},\n\\end{equation}\n\\](eq-s-b-inv)\ndove \\(\\rho_1\\) è l’attendibilità stimata di un “item medio,” \\(\\rho_p\\) è il livello desiderato di attendibilità del test allungato e \\(p\\) è il numero di item del test allungato.\nEsempio. L’attendibilità della scala SWLS costituita da 5 item è \\(\\omega = 0.824\\). Quanti item devono essere aggiunti se si vuole raggiungere un livello di attendibilità pari a \\(0.95\\)?\nPonendo \\(\\rho_p = 0.95\\) e \\(\\rho_1= 0.479\\), in base alla {eq}eq-s-b-inv si ottiene che\n\n(.95 * (1 - rho_1)) / (rho_1 * (1 - .95))\n\nY2: 20.9777932006845\n\n\nil test dovrà essere costituito da 21 item.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Valutare e rifinire la soluzione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/05_val_soluzione.html#analisi-degli-item",
    "href": "chapters/extraction/05_val_soluzione.html#analisi-degli-item",
    "title": "38  Valutare e rifinire la soluzione fattoriale",
    "section": "38.4 Analisi degli item",
    "text": "38.4 Analisi degli item\nL’analisi degli item svolge un ruolo importante nello sviluppo e nella revisione dei test psicometrici. L’analisi degli item esamina le risposte fornite ai singoli item del questionario allo scopo di valutare la qualità degli item e del questionario nel suo complesso. Sotto al rubrica di analisi degli item possiamo raggruppare le procedure che possono essere utilizzate per descrivere la difficoltà degli item, le relazioni tra coppie di item, il punteggio totale del test, le relazioni tra gli item e il punteggio totale del test. Tali analisi statistiche vengono usate per la selezione degli item al fine di costruire un questionario omogeneo, attendibile e dotato di validità predittiva.\nLa selezione degli item di un test, però, non può essere svolta in maniera automatica usando soltanto criteri statistici quali quelli elencati sopra. La selezione degli item, invece, deve anche tenere includere considerazioni di ordine teorico basate sulla centralità degli item rispetto alla definizione del costrutto e considerazioni relative agli scopi della misurazione e al modo in cui l’item è stato formulato e costruito. Se alcuni aspetti di un costrutto non vengono rappresentanti da item che soddisfano i criteri statistici descritti sopra, o se c’è un numero insufficiente di item per produrre uno strumento attendibile, allora alcuni item dovranno essere riscritti. Nella riformulazione degli item, risultano utili le intuizioni che si sono guadagnate dalle analisi statistiche degli item che si sono dovuti scartare.\n\n38.4.1 Difficoltà degli item\nUna statistica comune da calcolare durante l’analisi degli item è la proporzione di esaminandi che rispondono correttamente ad ogni item. Questa è nota come difficoltà dell’item, p. La proporzione \\(p_j\\) di partecipanti che rispondono correttamente all’item \\(j\\)-esimo, o proporzione di partecipanti che si dichiarano in accordo con l’affermazione espressa dall’item, se il test non è di prestazione, fornisce una stima del livello di difficoltà \\(\\pi_j\\) dell’item.\nIn realtà, \\(p_j\\) dovrebbe essere chiamato “facilità dell’item” in quanto assume il suo valore maggiore (ovvero \\(1\\)) quando tutti i rispondenti rispondono correttamente all’item e il suo valore minimo (ovvero \\(0\\)) quando le risposte sono tutte sbagliate. Questo valore non va confuso con la difficoltà dell’item nella teoria della risposta agli item o con il valore-\\(p\\) dei test di ipotesi frequentisti.\nI valori \\(p_j\\) giocano un ruolo importante nelle procedure di selezione degli item. La difficoltà degli item deve essere interpretata in riferimento alla probabilità di indovinare la risposta corretta. Si suppone, infatti, che i rispondenti tirino ad indovinare quando non conoscono la risposta alla domanda di un questionario. Nel caso di item dicotomici, per esempio, ci possiamo aspettare un valore \\(p_j\\) pari a \\(0.50\\) sulla base del caso soltanto; nel caso di item a risposta multipla con quattro opzioni di scelta, invece, \\(p_j\\) assume un valore pari a \\(0.25\\) quando i rispondenti tirano ad indovinare.\nSe il test è composto per la maggior parte da item “facili”, allora il test non sarà in grado di discriminare tra rispondenti con diversi livelli di abilità, in quanto quasi tutti i rispondenti saranno in grado di fornire una risposta corretta alla maggioranza degli item. Lo stesso si può dire per un test composto da item “difficili”. Se il test è composto unicamente da item di difficoltà media, non potrà differenziare i rispondenti che hanno un grado di abilità media da quelli con abilità superiori alla media, dato che non ci sono item “difficili”, e neppure da quelli con abilità inferiori alla media, dato che non ci sono item “facili”.\nIn generale, dunque, è buona pratica costruire test composti da item che coprano tutti i livelli di difficoltà. La scelta che viene usualmente fatta è quella di una dispersione moderata e simmetrica del livello di difficoltà attorno ad un valore leggermente superiore al valore che sta a metà tra il livello del caso (\\(1.0\\) diviso per il numero di alternative) e il punteggio pieno (\\(1.0\\)).\nPer item che presentano cinque alternative di risposta, ad esempio, il livello del caso è pari a \\(1.0 / 5 = 0.20\\). Il livello ottimale di difficoltà è uguale a\n\\[\n0.20 + (1.0 - 0.20) / 2 = 0.60.\n\\]\nPer item dicotomici, il livello del caso è \\(1.0 / 2 = 0.50\\) e il livello ottimale di difficoltà è uguale a\n\\[\n0.50 + (1.00 - 0.50) / 2 = 0.75.\n\\]\nIn generale, item con livelli di difficoltà superiore a \\(0.90\\) o inferiore a \\(0.20\\) dovrebbero essere utilizzati con cautela.\nEsempio. Riporto qui sotto le proporzioni di risposte corrette (usando la correzione per il guessing) di 192 studenti di Psicometria nel primo parziale dell’AA 2021/2022. Il test aveva 16 item con 5 alternative di risposta ciascuno. Dunque la difficoltà media ottimale è pari a 0.6.\n\nitem_par_1 &lt;- c(\n  0.54255319, 0.76063830, 0.64361702, 0.65957447, 0.67021277, 0.12234043,\n  0.14361702, 0.18085106, 0.76063830, 0.82978723, 0.81914894, 0.84042553,\n  0.07978723, 0.07978723, 0.76063830, 0.79255319\n)\n\nNel compito, la difficoltà media è risultata essere un po’ inferiore.\n\nmean(item_par_1) %&gt;% \n  round(2)\n\n0.54\n\n\nLa distribuzione dei livelli di difficoltà degli item suggerisce che forse alcuni item “difficili” si sarebbero potuti sostituire con item di difficoltà media.\n\nplot(density(item_par_1))\n\n\n\n\n\n\n\n\n:::\nUn altro esempio riguarda il data set SAPA del pacchetto hemp. Per questi dati possiamo utilizzare la funzione colMeans per calcolare la difficoltà degli item. Poiché abbiamo dei partecipanti che hanno risposte mancanti su alcuni item, dobbiamo passare l’argomento na.rm = TRUE per ignorare i dati mancanti. In caso contrario, la funzione colMeans restituirebbe NA per gli item che hanno almeno un valore mancante. Per rendere più leggibili i valori di difficoltà degli item, arrotondiamo a tre decimali utilizzando la funzione round.\n\nitem_diff &lt;- colMeans(SAPA, na.rm = TRUE)\nround(item_diff, 3)\n\nreason.40.64reason.160.698reason.170.697reason.190.615letter.70.6letter.330.571letter.340.613letter.580.444matrix.450.526matrix.460.55matrix.470.614matrix.550.374rotate.30.194rotate.40.213rotate.60.299rotate.80.185\n\n\nL’output mostra che gli item reason.16 e reason.17 ottengono i livelli di difficoltà più alti, mentre rotate.8 ha il livello di difficoltà più basso. Circa il 70% degli studenti è stato in grado di rispondere correttamente a reason.16 e reason.17, mentre solo il 19% ha risposto correttamente a rotate.8.\n\n\n38.4.2 Correzione per guessing\nAlle volte i valori \\(p_j\\) sono calcolati introducendo una correzione per le risposte fornite casualmente dai soggetti (guessing). Si consideri un test a scelta multipla composto da item aventi ciascuno \\(C\\) alternative di risposta ed una sola risposta corretta. Si supponga che un rispondente risponda correttamente a \\(R\\) item e risponda in maniera sbagliata a \\(W\\) item.\nLa correzione per guessing si ottiene applicando una formula basata sul seguente ragionamento. Se assumiamo che un rispondente si limita a tirare ad indovinare allora, ogni \\(C\\) risposte, ci aspettiamo 1 risposta giusta e \\(C-1\\) risposte sbagliate. Per calcolare il punteggio totale del test in modo da eliminare il numero di risposte corrette ottenute tirando ad indovinare è necessario sottrarre 1 punto per ogni \\(C-1\\) item a cui è stata fornita una risposta corretta. Questo ragionamento conduce alla seguente formula:\n\\[\n\\begin{equation}\nFS = R - \\frac{W}{C - 1},\n\\end{equation}\n\\](eq-guessing)\ncon \\(R\\) = # risposte corrette, \\(W\\) = # risposte sbagliate, \\(C\\) = # alternative di risposta. Per esempio, se \\(C=5\\), allora è necessario sottrarre un punto ogni 4, il che è proprio quello che fa la {eq}eq-guessing.\nLa {eq}eq-guessing produce un punteggio totale corretto per il guessing identico a quello che si otterrebbe assegnando 1 punto a ciascuna risposta corretta e assegnando \\(- \\frac{1}{C-1}\\) punti alle risposte sbagliate; le risposte non date non vengono considerate.\nLa correzione per guessing rappresenta il tentativo di scomporre il numero totale di risposte corrette in due componenti: le risposte corrette dovute alle conoscenze del soggetto, le risposte che risultano corrette come effetto del caso. La stessa formula può anche essere utilizzata per calcolare la difficoltà degli item corretta per il guessing (come è stato fatto nell’esempio del parziale di Psicometria).\n\n\n38.4.3 Discriminatività\nLa discriminatività è una misura di quanto ogni item è in grado di distinguere i soggetti con elevati livelli nel costrutto da quelli con un livello basso. L’indice di discriminatività \\(D\\) per i test di prestazione massima si trova nel modo seguente. Dopo avere calcolato il punteggio totale al test, si dividono i soggetti in due gruppi: soggetti con basso punteggio e soggetti con alto punteggio. Una volta definiti i due gruppi, l’indice di discriminatività \\(D\\) sarà dato da:\n\\[D = P(\\text{alto}) - P(\\text{basso}),\\]\ndove \\(P(\\text{alto}\\) è la proporzione di soggetti che ha risposto correttamente all’item nel gruppo con punteggi alti e \\(P(\\text{basso}\\) è la proporzione di soggetti che ha risposto correttamente all’item nel gruppo con punteggi bassi. Il valore di \\(D\\) può variare da -1 a +1. Nella tabella seguente sono fornite le linee guida per l’interpretazione di questo indice (Ebel, 1965).\n\n\n\nValore di \\(D\\)\nCommento\n\n\n\n\n\\(D \\geq 0.40\\)\nOttima, nessuna revisione\n\n\n\\(0.30 \\leq D &lt; 0.40\\)\nBuona, revisioni minime\n\n\n\\(0.20 \\leq D &lt; 0.30\\)\nSufficiente, revisioni parziali\n\n\n\\(D &lt; 0.20\\)\nInsufficiente, riformulazione o eliminazione\n\n\n\nLa discriminatività degli item di tipo Likert viene valutata con la medesima procedura degli item dei testi di prestazione massima, anche se cambiano le procedure statistiche da utilizzare. Si può dividere la distribuzione dei punteggi totali (o punteggi medi) in quartili e confrontare il punteggio medio o mediano del quartile superiore con quello del quartile inferiore, oppure, se il test è orientato al criterio e lo scopo è selezionare gli item che discriminano meglio due gruppi precostituiti di soggetto, eseguire i medesimi confronti tra il gruppo target (ad esempio, pazienti) e quello “di controllo” (per esempio, popolazione generale).\nÈ consigliabile valutare la dimensione dell’effetto, ad esempio attraverso l’indice \\(d\\) di Cohen. La dimensione dell’effetto dovrebbe essere almeno moderata (\\(d &gt; |0.50|\\)).\nEsempio. Per il primo parziale di Psicometria AA 2021/2022, l’indice \\(d\\) di Cohen calcolato sulla proporzione di risposte corrette per il gruppo di studenti con i punteggi più bassi (primo quartile) e il gruppo di studenti con i punteggi più alti (ultimo quartile) è stato di 4.76, 95% CI [4.0, 5.51]. L’indice complessivo di discriminatività sembra dunque adeguato. Sarebbe però necessario calcolare questo indice item per item.\n\n\n38.4.4 Potere discriminante dell’item e analisi fattoriale\nUn’altra statistica ampiamente utilizzata nell’analisi degli item è il potere discriminante degli item, che si riferisce alla capacità dell’item nel distinguere gli esaminandi con una alta abilità da quelli con una bassa abilità. Sebbene esistano molti modi per calcolare la discriminazione degli item, la forma più comune è la correlazione punto-biseriale tra le risposte degli esaminandi all’item e il loro punteggio totale nel test. Valori grandi e positivi indicano una forte relazione tra il rispondere correttamente all’item e avere un punteggio alto nel test, mentre valori vicini allo zero indicano nessuna relazione e valori negativi indicano che il rispondere correttamente all’item è associato a un punteggio complessivo del test più basso. Valori vicini allo zero o negativi suggeriscono che l’item potrebbe non funzionare correttamente. Alcune delle ragioni per ottenere una discriminazione degli item bassa o negativa potrebbero essere l’utilizzo di una chiave di risposta errata per l’item o l’assenza di risposte corrette. Indipendentemente dalla causa, gli item con correlazioni punto-biseriale basse o negative devono essere modificati, se il test/strumento è in fase di revisione, o rimossi dal test e dal punteggio.\nPer calcolare il potere discriminante dell’item per i dati SAPA, prima calcoliamo il punteggio totale del test utilizzando la funzione rowSums insieme all’opzione na.rm = TRUE e lo salviamo come total_score. Successivamente, correlaziamo gli item in SAPA con il punteggio totale del test utilizzando la funzione cor. Specificamente, usiamo l’argomento use = \"pairwise.complete.obs\" nella funzione cor a causa della presenza di risposte mancanti. Infine, salviamo la matrice di correlazione come item_discr e la stampiamo.\n\ntotal_score &lt;- rowSums(SAPA, na.rm = TRUE)\nitem_discr &lt;- cor(SAPA, total_score, use = \"pairwise.complete.obs\")\nround(item_discr, 2)\n\n\nA matrix: 16 x 1 of type dbl\n\n\nreason.4\n0.59\n\n\nreason.16\n0.53\n\n\nreason.17\n0.59\n\n\nreason.19\n0.56\n\n\nletter.7\n0.58\n\n\nletter.33\n0.56\n\n\nletter.34\n0.59\n\n\nletter.58\n0.58\n\n\nmatrix.45\n0.51\n\n\nmatrix.46\n0.51\n\n\nmatrix.47\n0.55\n\n\nmatrix.55\n0.45\n\n\nrotate.3\n0.51\n\n\nrotate.4\n0.56\n\n\nrotate.6\n0.55\n\n\nrotate.8\n0.48\n\n\n\n\n\nI risultati mostrano che tutti gli item del test SAPA sono moderatamente e positivamente correlati con il punteggio totale del test. Questo indica che tutti gli item funzionano correttamente e non fornisce informazioni salienti su quali item rimuovere o modificare.\nUn altro modo per calcolare il potere discriminante degli item consiste nel dividere i candidati in due gruppi (ad esempio, 1 = alto rendimento e 0 = basso rendimento) in base ai loro punteggi totali nel test e correlare questa variabile di raggruppamento con le risposte agli item. Questo è noto come indice di discriminazione degli item. Un’opzione per creare gruppi di alto e basso rendimento è selezionare il 25% più alto e il 25% più basso dei candidati in base ai loro punteggi totali nel test. Va notato che la decisione di utilizzare il 25% è arbitraria. Potremmo utilizzare un altro valore (ad esempio, il 10% o il 20%) per definire i gruppi di alto e basso rendimento. Dopo aver definito il punto di cut-off per i gruppi, calcoliamo la proporzione di candidati che hanno risposto correttamente all’elemento nei gruppi di alto e basso rendimento.\nNell’esempio seguente, calcoliamo l’indice di discriminazione dell’elemento reason.4 nel set di dati SAPA utilizzando la funzione idi del pacchetto hemp. Per specificare i gruppi di alto e basso rendimento, utilizziamo il valore perc_cut = .25 nella funzione idi.\n\nidi(SAPA, SAPA$reason.4, perc_cut = .25)\n\nUpper 25%0.805135951661631Lower 25%0.194864048338369\n\n\nAbbiamo scoperto che l’81% dei candidati nel gruppo di alto rendimento ha risposto correttamente all’item reason.4, mentre solo il 19% dei candidati nel gruppo di basso rendimento ha risposto correttamente. Questo suggerisce che l’item era più facile per i candidati di alto rendimento e più difficile per quelli di basso rendimento. Pertanto, possiamo dire che questo particolare item risulta utile per differenziare i due gruppi, ma non necessariamente all’interno di ciascun gruppo.\nSecondo McDondald (1999), la nozione di potere discriminante dell’item può essere trattata in maniera più precisa nell’ambito del modello monofattoriale. Se l’insieme di item a disposizione non è eccessivamente grande (200 o meno), infatti, è possibile procedere alla selezione degli item migliori tramite l’analisi fattoriale – ovvero, scegliendo gli item con le saturazioni maggiori.\n\n\n38.4.5 Punteggio sull’item e punteggio totale\nIl grado di associazione tra il punteggio sull’item e il punteggio totale viene considerato dalla teoria classica dei test come un indice che descrive il potere discriminante dell’item. Se il test fornisce una misura attendibile di un unico attributo, e se un item è fortemente associato al punteggio del test, allora l’item sarà in grado di distinguere tra rispondenti che ottengono un punteggio basso nel test e rispondenti che ottengono un punteggio alto nel test.\nNel caso di una forte associazione positiva tra il punteggio sull’item e il punteggio totale, la probabilità di risposta corretta sull’item è alta per rispondenti che ottengono un punteggio totale alto, e bassa per i rispondenti che ottengono un punteggio totale basso. Nel caso di una debole associazione tra il punteggio sull’item e il punteggio totale, invece, la probabilità di risposta corretta all’item non è predittiva del punteggio totale. Gli item con un basso potere discriminante dovrebbero dunque essere rimossi dal reattivo.\nÈ necessario distinguere i casi in cui gli item sono dicotomici dal caso di item continui. Nel caso di item dicotomici e di un test unidimensionale, il potere discriminante viene calcolato mediante la correlazione biseriale o punto-biseriale.\n\n\n38.4.6 Relazioni tra coppie di item\nLe relazioni tra coppie di item sono importanti sia per la costruzione sia per la validazione dei test psicometrici. La teoria classica dei test definisce l’attendibilità di un test (o di un item) come il rapporto tra la varianza del punteggio vero e la varianza del punteggio osservato. Il coefficiente di attendibilità può però essere calcolato anche trovando la correlazione tra due forme parallele di un test (o tra due item). Inoltre, è possibile interpretare la correlazione tra due forme parallele di un test (o tra due item) come il quadrato del coefficiente di correlazione tra i punteggi osservati e i punteggi veri di un test (o di un item).\nMolti indici sono disponibili per misurare il grado di associazione tra item. Per item quantitativi, possiamo usare la correlazione di Pearson o la covarianza. Per item qualitativi politomici ordinali, usiamo la correlazione policorica. Per item ordinali dicotomici, usiamo la correlazione tetracorica. Per item dicotomici usiamo, ad esempio, l’indice \\(\\phi\\).\n\n\n38.4.7 Ridondanza\nNel processo di raffinamento del test occorre anche tenere conto degli item ridondanti, ossia degli item che sono troppo associati tra loro. La ridondanza può essere valutata con indici statistici quali la correlazione: se due o più item hanno tra loro una correlazione maggiore di \\(|0.70|\\) viene mantenuto nell’item pool solo uno di essi, dato che gli altri item forniscono la stessa informazione.\n\n\n38.4.8 Massimizzazione della varianza del punteggio totale\nUno dei criteri che possono essere utilizzati per la selezionare degli item che andranno a costituire la versione finale di un test è la massimizzazione della varianza del punteggio totale. Più in particolare, si vuole massimizzare il rapporto tra la varianza del punteggio totale e la somma delle varianze dei punteggi dei \\(p\\) item. Dato che il coefficiente \\(\\alpha\\) di Cronbach ha la seguente forma:\n\\[\\alpha = \\frac{p}{p-1}\\left[1- \\frac{\\sum \\sigma^2_{Y_i}}{\\sigma^2_T} \\right],\\]\nla scelta di massimizzare il rapporto definito in precedenza avrà anche la conseguenza di massimizzare \\(\\alpha\\).\n{cite:t}mcdonald2013test fa notare che una procedura di selezione degli item basata sul principio della massimizzazione di \\(\\alpha\\) ha però dei limiti. In primo luogo, tale procedura è appropriata solo quando l’insieme di item è troppo grande per selezionare gli item in base all’esame delle saturazioni fattoriali ottenute applicando il modello mono-fattoriale. In secondo luogo, {cite:t}mcdonald2013test nota che la procedura di selezione basata sulla massimizzazione di \\(\\alpha\\) è adeguata solo nel caso di una struttura mono-fattoriale. La selezione degli item basata sulla massimizzazione di \\(\\alpha\\) deve dunque essere accompagnata da considerazione relative al contenuto e alla struttura del costrutto.\n\n\n38.4.9 Indice di affidabilità dell’item\nOltre agli indici di difficoltà e discriminazione degli item, un’altra statistica utile per l’analisi degli item è l’indice di affidabilità dell’item. L’indice di affidabilità dell’item (IRI) è definito come:\n\\[\nIRI = S_i \\cdot r_{i,tt},\n\\]\ndove \\(S_i\\) è la deviazione standard dell’item \\(i\\) e \\(r_{i,tt}\\) è la correlazione tra l’item \\(i\\) e il punteggio totale del test. L’IRI può teoricamente variare tra -0.5 e 0.5, con valori grandi e positivi indicativi di alta affidabilità.\nDi seguito calcoliamo l’IRI per tutti gli item nel set di dati SAPA. Possiamo farlo utilizzando la funzione iri in hemp.\n\niri(SAPA)\n\n\nA matrix: 16 x 1 of type dbl\n\n\nreason.4\n0.2820989\n\n\nreason.16\n0.2451971\n\n\nreason.17\n0.2692675\n\n\nreason.19\n0.2717135\n\n\nletter.7\n0.2865325\n\n\nletter.33\n0.2757209\n\n\nletter.34\n0.2897118\n\n\nletter.58\n0.2863221\n\n\nmatrix.45\n0.2544930\n\n\nmatrix.46\n0.2562540\n\n\nmatrix.47\n0.2668171\n\n\nmatrix.55\n0.2161230\n\n\nrotate.3\n0.2016459\n\n\nrotate.4\n0.2276081\n\n\nrotate.6\n0.2539219\n\n\nrotate.8\n0.1867207\n\n\n\n\n\nI risultati restituiti dalla funzione iri mostrano che l’IRI varia da circa 0.19 a 0.29 per il set di dati SAPA. Tutti questi sono valori ragionevoli per l’IRI (ovvero nessuno è negativo o vicino allo zero).\n\n\n38.4.10 Indice di validità dell’item\nQuando invece del punteggio totale del test viene utilizzato un criterio esterno, questo indice è noto come indice di validità dell’item (IVI). L’IVI può variare anche tra -0.5 e 0.5, con valori elevati (in valore assoluto) che indicano una validità maggiore. Valori negativi elevati indicano una maggiore validità quando ci si aspetta che gli elementi siano correlati in modo negativo con il criterio.\nNell’esempio seguente, utilizziamo la funzione ivi in hemp con “reason.17” come criterio esterno e “reason.4” come elemento di interesse e troviamo che l’IVI è 0.19.\n\nivi(item = SAPA$reason.4, crit = SAPA$reason.17)\n\n0.190321881267833\n\n\n\n\n38.4.11 Distrattori\nUn altro aspetto importante degli elementi che deve essere analizzato sono le opzioni di risposta. Nel contesto dei test a scelta multipla, le opzioni di risposta alternative (cioè sbagliate) vengono definite “distrattori”. I distrattori svolgono un ruolo importante in un elemento a scelta multipla. Per garantire elementi a scelta multipla di alta qualità, è cruciale includere distrattori plausibili e ben funzionanti che siano più probabili di attirare i candidati con conoscenze parziali. I distrattori non plausibili potrebbero dover essere riscritti o sostituiti con un distrattore migliore. La qualità dei distrattori viene tipicamente valutata attraverso l’analisi dei distrattori. L’analisi dei distrattori viene spesso condotta osservando la proporzione di candidati che scelgono un distrattore particolare.\nPer illustrare l’analisi dei distrattori, utilizziamo gli item del data set multiplechoice in hemp. Si tratta di un ipotetico test a scelta multipla composto da 27 item somministrati a 496 candidati. Le quattro opzioni di risposta sono codificate come 1, 2, 3 e 4 nel data set. Utilizziamo la funzione distract in hemp per calcolare la proporzione di candidati che selezionano ciascun distrattore.\n\ndistractors &lt;- distract(multiplechoice)\nhead(distractors)\n\n\nA matrix: 6 x 4 of type dbl\n\n\n\n1\n2\n3\n4\n\n\n\n\nitem1\n0.044\n0.058\n0.052\n0.845\n\n\nitem2\n0.109\n0.069\n0.792\n0.030\n\n\nitem3\n0.188\n0.562\n0.058\n0.192\n\n\nitem4\n0.034\n0.125\n0.742\n0.099\n\n\nitem5\n0.351\n0.254\n0.042\n0.353\n\n\nitem6\n0.081\n0.198\n0.558\n0.163\n\n\n\n\n\nNella tabella sopra, vediamo che molti item avevano distrattori selezionati circa il 5% delle volte o meno. Questi distrattori potrebbero essere candidati per una revisione in quanto sono stati approvati ad un livello così basso da suggerire che la maggior parte degli esaminandi non li ha considerati come opzioni plausibili. Per l’item 1, i distrattori funzionavano tutti più o meno allo stesso modo (ovvero circa il 5% delle volte ogniuno è stato approvato), suggerendo che funzionavano tutti bene rispetto l’uno all’altro, ma che l’item era troppo facile (la risposta corretta era l’opzione 4, selezionata dall’84.5% degli esaminandi). Al contrario, l’item 5 era un item più difficile, con la risposta corretta che ancora una volta era l’opzione 4. Le opzioni 1 e 2 erano molto probabilmente fraintendimenti, mentre l’opzione 3 potrebbe essere rivista o potenzialmente eliminata da questo item a causa del basso tasso di approvazione (solo il 4.2%). Dato l’approvazione molto alta dell’opzione 1 (35.1%), è molto probabile che anche questa opzione fosse corretta. Per ottenere una visione più completa del funzionamento dell’item, sarebbe consigliabile calcolare l’indice di discriminazione specifico per quell’item. Questo ci permetterebbe di ottenere ulteriori informazioni sulla capacità dell’item di distinguere tra candidati di alto e basso livello.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Valutare e rifinire la soluzione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/05_val_soluzione.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/extraction/05_val_soluzione.html#informazioni-sullambiente-di-sviluppo",
    "title": "38  Valutare e rifinire la soluzione fattoriale",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Valutare e rifinire la soluzione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html",
    "href": "chapters/cfa/01_cfa.html",
    "title": "39  Analisti Fattoriale Confermativa",
    "section": "",
    "text": "39.1 Introduzione\nIn questo capitolo esamineremo la CFA per l’analisi dei modelli di misurazione con fattori comuni e indicatori continui. A differenza dell’analisi fattoriale esplorativa (EFA), nella CFA vengono analizzati modelli di misurazione vincolati. Ciò significa che il ricercatore specifica (1) il numero esatto di fattori; (2) il pattern dei carichi fattoriali, ossia la corrispondenza specifica tra i fattori e gli indicatori; e (3) la presenza di errori correlati, se presenti.\nLa seconda caratteristica menzionata sopra implica che un indicatore satura solo sui fattori specificati dal ricercatore, e tutte le saturazioni incrociate di quell’indicatore su altri fattori sono fissate a zero. Sebbene sia possibile specificare un numero esatto di fattori nella EFA, la tecnica analizza modelli di misurazione non restrittivi, in cui ciascun indicatore satura su tutti i fattori (ossia tutte le saturazioni incrociate sono liberamente stimate).\nUn’altra differenza è che i modelli EFA con più fattori sono identificati solo dopo aver specificato un metodo di rotazione dei fattori, come obliqua (i fattori possono covariare) oppure ortogonale (i fattori sono non correlati). Poiché la CFA richiede un modello identificato, non c’è una fase di rotazione e di solito è permesso ai fattori di covariare.\nNell’ambito dei requisiti per l’identificazione, è possibile stimare errori correlati nella CFA, ma è più difficile ottenere questo risultato nella EFA. Pertanto, la tecnica della CFA supporta meglio l’analisi delle strutture di covarianza degli errori rispetto alla EFA.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#limitazioni-dellapproccio-fattoriale",
    "href": "chapters/cfa/01_cfa.html#limitazioni-dellapproccio-fattoriale",
    "title": "39  Analisti Fattoriale Confermativa",
    "section": "\n39.2 Limitazioni dell’approccio fattoriale",
    "text": "39.2 Limitazioni dell’approccio fattoriale\nL’approccio classico dell’analisi fattoriale (EFA più rotazione fattoriale) ha rivelato avere diversi limiti. Nella ricerca iniziale, dibattiti teorici importanti, come il numero di fattori dell’intelligenza o della personalità, erano basati sui risultati di diverse rotazioni fattoriali. Questi dibattiti si sono rivelati essere semplici speculazioni, poiché conclusioni diverse potevano essere supportate a seconda dell’interpretazione dei dati. Per esempio, il dibattito tra Eysenck e Cattell sul numero di fattori della personalità (due o sedici) dipendeva dall’uso di rotazioni ortogonali o oblique sugli stessi dati.\nNella seconda metà del XX secolo, c’era una generale insoddisfazione verso l’analisi fattoriale a causa della sua apparente capacità di adattarsi a quasi qualsiasi soluzione. Furono raccomandati criteri rigorosi per il suo uso, come la necessità di grandi campioni, che spesso rendevano l’analisi impraticabile a quei tempi. Inoltre, furono introdotti vincoli relativi alle ipotesi del modello e al requisito che le variabili nella matrice di correlazione avessero varianze equivalenti, creando problemi pratici significativi, specialmente con dati binari spesso usati nei test psicometrici.\nSolo con l’introduzione di metodi psicometrici moderni, come l’analisi fattoriale confermativa (CFA) discussa in questo capitolo e la Teoria di Risposta all’Item (discussa in una sezione successiva), questi problemi sono stati risolti.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#efa-vs.-cfa-confronto-tra-analisi-fattoriale-esplorativa-e-confermativa",
    "href": "chapters/cfa/01_cfa.html#efa-vs.-cfa-confronto-tra-analisi-fattoriale-esplorativa-e-confermativa",
    "title": "39  Analisti Fattoriale Confermativa",
    "section": "\n39.3 EFA vs. CFA: Confronto tra Analisi Fattoriale Esplorativa e Confermativa",
    "text": "39.3 EFA vs. CFA: Confronto tra Analisi Fattoriale Esplorativa e Confermativa\n\n39.3.1 Fondamenti Comuni e Differenze\nSia l’Analisi Fattoriale Esplorativa (EFA) che quella Confermativa (CFA) si basano sul modello dei fattori comuni. Entrambe le tecniche presuppongono che la varianza degli indicatori osservati possa essere suddivisa in varianza comune e varianza unica. La varianza comune è quella condivisa tra gli indicatori e sottende le covarianze osservate, mentre la varianza unica comprende sia la varianza specifica delle variabili che l’errore di misurazione. I fattori estratti, detti fattori comuni, rappresentano le variabili latenti costruite da questa varianza comune.\nNell’EFA, la struttura dei fattori è indeterminata e viene esplorata senza ipotesi a priori riguardo al numero o alla natura dei fattori. L’EFA è quindi particolarmente utile nelle fasi iniziali di ricerca, quando la teoria è poco sviluppata o si sospetta la presenza di fattori inaspettati.\nAl contrario, la CFA si basa su un modello di fattori predefinito, che specifica a priori quali indicatori sono associati a ciascun fattore, rendendola idonea per confermare teorie esistenti o per validare strutture fattoriali precedentemente esplorate. In CFA, i carichi incrociati (indicatori che caricano su più di un fattore) sono generalmente vincolati a zero, stabilendo una relazione diretta e specifica tra fattori e indicatori.\n\n39.3.2 Indeterminatezza Fattoriale\nUn problema ricorrente in entrambe le tecniche è l’indeterminatezza fattoriale, dove i fattori comuni non possono essere definiti in modo univoco dai loro indicatori a causa della natura approssimativa delle stime. Questo si manifesta sia in EFA, con l’indeterminatezza della rotazione, che in CFA, dove l’analisi potrebbe non replicarsi in nuovi campioni a causa dell’uso dei medesimi dati per verificare il modello.\n\n39.3.3 Indeterminatezza dei Punteggi Fattoriali\nUn’altra complicazione è l’indeterminatezza dei punteggi fattoriali, che si verifica quando esistono infinite soluzioni valide per i punteggi fattoriali a partire dagli indicatori. Questo comporta che diversi metodi possono produrre ordinamenti differenti dei casi, un problema noto come indeterminatezza dei punteggi fattoriali (Grice, 2001).\n\n39.3.4 Rotazione e Specificazione del Modello\nLa EFA può presentare ambiguità a causa dell’infinita quantità di configurazioni dei carichi fattoriali che potrebbero adattarsi ai dati, un fenomeno meno pronunciato nella CFA dove la specifica del modello è più rigida.\n\n39.3.5 Applicazioni Pratiche\nL’EFA è spesso preferita in nuovi ambiti di ricerca, dove i fattori potrebbero non essere ben definiti, mentre la CFA è utilizzata per confermare le strutture fattoriali in studi di validazione o in seguito a revisioni di test esistenti.\n\n39.3.6 Problemi con l’Uso Combinato di EFA e CFA\nSi noti che l’applicazione della CFA immediatamente dopo la EFA nello stesso campione può essere problematica. Talvolta, l’uso congiunto non verifica né conferma i risultati dell’EFA. La restrittività dei modelli CFA, con i carichi incrociati impostati a zero, può portare a risultati che non sono coerenti con i dati analizzati nell’EFA.\n\n39.3.7 Nuove Approcci Intermedi\nMetodi come l’Analisi Strutturale Esplorativa (ESEM) offrono un approccio ibrido che combina la flessibilità dell’EFA con alcuni degli aspetti confirmatori della CFA. Questo permette una maggiore precisione nel testare l’adattamento del modello, pur mantenendo la capacità di esplorare nuove strutture fattoriali.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#raccomandazioni-per-la-selezione-degli-indicatori-nellanalisi-fattoriale",
    "href": "chapters/cfa/01_cfa.html#raccomandazioni-per-la-selezione-degli-indicatori-nellanalisi-fattoriale",
    "title": "39  Analisti Fattoriale Confermativa",
    "section": "\n39.4 Raccomandazioni per la Selezione degli Indicatori nell’Analisi Fattoriale",
    "text": "39.4 Raccomandazioni per la Selezione degli Indicatori nell’Analisi Fattoriale\nLa selezione accurata degli indicatori è cruciale per il successo dell’analisi fattoriale, sia essa Esplorativa (EFA) o Confermativa (CFA). Le linee guida suggerite da Fabrigar e Wegener (2012) e Little et al. (1999), come riassunto in Kline (2023), enfatizzano i seguenti punti chiave:\n\nDefinizione dei Concetti Teorici: È essenziale articolare i concetti teorici in modo dettagliato per delineare chiaramente ogni dominio di interesse. Ad esempio, se lo studio riguarda le dimensioni dell’ansia, è importante riferirsi a letteratura teorica ed empirica che discute vari aspetti come ansia di stato, ansia di tratto e ansia sociale.\nScelta degli Indicatori: Gli indicatori selezionati dovrebbero coprire adeguatamente i domini d’interesse senza affidarsi esclusivamente allo stesso metodo di misurazione, come i questionari di autovalutazione, per ridurre la varianza dovuta a metodi comuni. L’impiego di modelli CFA specializzati può aiutare a stimare questi effetti del metodo.\nGuida Teorica o Empirica Forte: Se esiste una solida base teorica o empirica, gli indicatori omogenei sono preferibili poiché forniscono stime più precise e meno distorte, specialmente in analisi di tipo più confermativo.\nAnalisi di Indicatori Meno Omogenei: Se la guida teorica è debole, può essere vantaggioso esaminare un insieme di indicatori meno omogenei che coprono un’ampia gamma del dominio d’interesse. Ciò evita di basarsi su approssimazioni che potrebbero non riflettere pienamente i concetti chiave.\nUso di Indicatori di Qualità Psicometrica Inferiore: Anche gli indicatori con minore qualità psicometrica possono essere utili se coprono ampiamente il costrutto, generano punteggi che riflettono ampie differenze individuali e sono analizzati attraverso metodi più confermativi.\nProblemi Tecnici: L’analisi potrebbe incontrare problemi come i casi Heywood o la mancata convergenza se alcuni fattori hanno un numero insufficiente di indicatori, specialmente in campioni piccoli. Un numero sicuro minimo di indicatori per ogni fattore previsto è di circa 3-5. Tuttavia, in alcuni casi, potrebbe essere vantaggioso utilizzare meno indicatori per fattore se questi sono psicometricamente solidi.\n\nHayduk e Littvay (2012) hanno sottolineato che non è sempre preferibile avere più indicatori per fattore; in certi contesti, un singolo indicatore ben scelto può essere sufficiente. Se gli indicatori sono altamente ridondanti, non aggiungono informazioni significative. L’idea di una “regola d’oro” di 3-5 indicatori per fattore è una guida generale, ma la scelta dovrebbe essere basata sulle ipotesi specifiche di ricerca piuttosto che su una regola arbitraria.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#fondamenti-dei-modelli-di-base-nella-cfa",
    "href": "chapters/cfa/01_cfa.html#fondamenti-dei-modelli-di-base-nella-cfa",
    "title": "39  Analisti Fattoriale Confermativa",
    "section": "\n39.5 Fondamenti dei Modelli di Base nella CFA",
    "text": "39.5 Fondamenti dei Modelli di Base nella CFA\nI modelli di base nella Confermative Factor Analysis (CFA) con più fattori sono caratterizzati da specifiche fondamentali che garantiscono una misurazione precisa delle variabili latenti. Ecco una sintesi delle caratteristiche principali di tali modelli:\n\nRelazione tra Indicatori e Fattori: Ogni indicatore è una variabile continua influenzata da due principali componenti: un fattore comune, che rappresenta la variabile latente che l’indicatore è inteso a misurare, e la varianza unica. Quest’ultima include sia l’errore di misurazione casuale sia la varianza specifica non spiegata dal fattore, entrambi rappresentati dal termine di errore.\nIndipendenza dei Termini di Errore: I termini di errore sono assunti come indipendenti l’uno dall’altro e dai fattori. Ciò implica l’assenza di confondenti non misurati per qualsiasi coppia di indicatori e l’indipendenza delle cause omesse dai fattori.\nLinearità e Covarianza: Le relazioni all’interno del modello sono lineari e i fattori possono covariare, il che significa che non esistono effetti causali diretti tra i fattori.\n\nQueste caratteristiche definiscono la misurazione unidimensionale, sottolineando che ciascun indicatore è pensato per misurare una sola dimensione e non condivide varianza con altri indicatori una volta controllati i fattori comuni. Tuttavia, è anche possibile specificare modelli CFA multidimensionali, dove alcuni indicatori possono caricare su più di un fattore o dove coppie di termini di errore possono essere correlati.\nInoltre, esistono metodi specializzati per analizzare relazioni non lineari tra fattori e indicatori continui, o tra i fattori stessi, come descritto da Amemiya e Yalcin (2001). Le relazioni tra indicatori categorici e fattori sono intrinsecamente non lineari, e questi scenari sono trattati nel CFA categorico.\nUn esempio di modello CFA di base con due fattori e sei indicatori viene presentato di seguito, dove tutti i carichi incrociati sono fissati a zero, Figura 39.1. Per esempio, il fattore B non ha un effetto causale diretto sull’indicatore X1, il quale è misurato da un altro fattore (A). Tuttavia, ciò non significa che X1 e il fattore B siano completamente scorrelati. La struttura del modello permette a X1 di covariare con B poiché B è correlato con A, che è una causa di X1 (l’altra causa è E1, il termine di errore di X1). In modo simile, si prevede che gli indicatori X1 e X4 covarino poiché sono influenzati dai fattori A e B, rispettivamente, i quali sono correlati.\nLe costanti di scala, indicate come (1) nel modello, definiscono le metriche per le variabili non misurate, inclusi i fattori comuni e i termini di errore degli indicatori, stabilendo così una base uniforme per la misurazione nel modello CFA.\n\n\n\n\n\nFigura 39.1: Modello di analisi fattoriali confermativa con due fattori comuni e sei indicatori. (Figura tratta da Kline (2023))",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#scalatura-dei-fattori-e-inclusione-delle-covariate-nei-modelli-cfa-di-base",
    "href": "chapters/cfa/01_cfa.html#scalatura-dei-fattori-e-inclusione-delle-covariate-nei-modelli-cfa-di-base",
    "title": "39  Analisti Fattoriale Confermativa",
    "section": "\n39.6 Scalatura dei Fattori e Inclusione delle Covariate nei Modelli CFA di Base",
    "text": "39.6 Scalatura dei Fattori e Inclusione delle Covariate nei Modelli CFA di Base\nNella rappresentazione di base dei modelli Confermative Factor Analysis (CFA), la scalatura dei fattori viene spesso eseguita utilizzando il metodo della variabile di riferimento, conosciuto anche come metodo della variabile marker o approccio di identificazione del carico di riferimento (Newsom, 2015). In questo approccio, un vincolo di Unit Loading Identification (ULI) è applicato al carico di un indicatore per ciascun fattore. Per esempio, nel modello illustrato, il carico di \\(X1\\) sul fattore \\(A\\) è fissato a 1.0, stabilendo così la varianza del fattore \\(A\\) sulla base della varianza comune dell’indicatore \\(X1\\), che funge da variabile di riferimento per \\(A\\). Analogamente, la varianza del fattore \\(B\\) è calibrata utilizzando \\(X4\\) come variabile marker.\nQuando più indicatori per lo stesso fattore presentano precisione equivalente e nessuno di essi è considerato particolarmente rappresentativo del concetto sottostante, la scelta dell’indicatore come variabile di riferimento diventa generalmente arbitraria. Questa selezione non influisce solitamente sull’adattamento globale del modello, sulla soluzione standardizzata, o sulle stime delle varianze di errore dell’indicatore nelle soluzioni non standardizzate. Le saturazioni fisse a 1.0 per le variabili di riferimento rimangono invariate nelle soluzioni non standardizzate e non sono soggette a test di significatività, poiché sono considerate costanti.\nUn potenziale svantaggio di questo metodo è l’assenza di test di significatività per le saturazioni fisse, il che può essere limitante se si desidera valutare la significatività di tutte le saturazioni. Metodi alternativi per scalare i fattori, che non richiedono la selezione di variabili di riferimento, saranno discussi nelle sezioni successive.\nNei modelli CFA di base, tutti i fattori sono considerati variabili esogene, il che significa che sono liberi di variare e covariare indipendentemente l’uno dall’altro. Tuttavia, è possibile includere variabili esterne, dette covariate, che si presume possano influenzare i fattori comuni. Ad esempio, l’età dei partecipanti potrebbe essere vista come una covariata che influisce sui fattori comuni in un modello CFA.\nL’inclusione di covariate trasforma i fattori comuni da variabili esogene a endogene, implicando che non sono più completamente liberi di variare in modo indipendente, ma possono essere direttamente influenzati dalle covariate. Questo richiede l’aggiunta di termini di disturbo nei fattori comuni per rappresentare l’effetto diretto delle covariate su di essi, integrando così l’effetto delle variabili esterne nel modello CFA.\n\n39.6.1 Parametri del Modello nella CFA\nIn un modello CFA con indicatori continui, quando le medie delle variabili non sono considerate, i parametri liberi includono varianze, covarianze di variabili esogene e gli effetti diretti (carichi) sulle variabili endogene. Ad esempio, analizzando il modello di base illustrato in figura, i parametri liberi possono essere suddivisi come segue:\n\n\nVarianze: Comprendono le varianze di due fattori e sei termini di errore associati agli indicatori, per un totale di otto varianze.\n\nCovarianza: È presente una covarianza tra i due fattori.\n\nEffetti Diretti (Carichi): Quattro carichi fattoriali rappresentano gli effetti diretti dei fattori sugli indicatori, specificamente per gli indicatori X2, X3, X5 e X6. Questi carichi non sono fissati, a differenza di quelli usati per scalare i fattori.\n\nSommando questi parametri, il totale dei parametri liberi nel modello è 13. Con \\(v = 6\\) variabili osservate, il numero totale di osservazioni statisticamente indipendenti, calcolato come \\(6(7)/2\\), è 21. Di conseguenza, i gradi di libertà per il modello presentato sono calcolati sottraendo i parametri liberi dalle osservazioni indipendenti, risultando in \\(21 - 13 = 8\\) gradi di libertà.\n\n39.6.1.1 Requisiti di Identificazione: Necessari ma Non Sufficienti per i Modelli di CFA\nPer assicurare che un modello di Confermative Factor Analysis (CFA) sia correttamente specificato e possa essere utilizzato per trarre conclusioni valide, è fondamentale soddisfare alcuni requisiti di identificazione essenziali. Questi requisiti sono necessari ma non sempre sufficienti; cioè, la loro soddisfazione non garantisce automaticamente l’identificazione completa del modello.\n\n\nGradi di Libertà (dfM) Maggiori o Uguali a Zero:\n\n\nCalcolo: I gradi di libertà di un modello CFA si determinano sottraendo il numero di parametri liberi (come varianze, covarianze, e carichi fattoriali) dal numero totale di osservazioni indipendenti disponibili, solitamente le varianze e covarianze degli indicatori.\n\nSignificato: Avere gradi di libertà positivi indica che ci sono sufficienti dati per stimare i parametri del modello e verificare il suo adattamento. Un modello con zero gradi di libertà è “saturato” e si adatterà perfettamente ai dati, ma non fornirà validazione ulteriore.\n\nImportanza: Mantenere dfM ≥ 0 è cruciale per evitare la sottospecificazione del modello, che potrebbe condurre a stime inaccurate e conclusioni fuorvianti.\n\n\n\nScalatura Corretta di Ogni Variabile Non Misurata:\n\n\nNecessità: È essenziale scalare ogni variabile latente, come i fattori, per definirne l’unità di misura. Senza una scalatura adeguata, parametri come i carichi fattoriali rimarrebbero indeterminati.\n\nMetodi: La scalatura può essere effettuata in vari modi, come fissando il carico di un indicatore per fattore a 1.0 (metodo della variabile di riferimento), fissando la varianza del fattore a un valore preciso, tipicamente 1.0 (metodo della standardizzazione della varianza), o applicando vincoli alle stime delle saturazioni fattoriali (metodo di codifica degli effetti).\n\n\n\nIn sintesi, pur essendo fondamentale soddisfare questi requisiti per stabilire una base identificabile e interpretabile per un modello CFA, l’identificazione completa del modello può richiedere considerazioni aggiuntive legate alla struttura specifica e alle ipotesi teoriche che sottendono al modello.\n\n\n\n\n\nFigura 39.2: Scalatura dei fattori nel metodo della variabile di riferimento con vincoli di identificazione del carico unitario (ULI) (a), metodo di standardizzazione della variabile con vincoli di identificazione della varianza unitaria (UVI) (b) e metodo di codifica degli effetti con vincoli di identificazione della codifica degli effetti (ECI) (a + b + c)/3 = (d + e + f)/3 = 1.0 (c). (Figura tratta da Kline (2023))\n\n\n\n39.6.1.2 Requisiti Sufficienti Aggiuntivi per l’Identificazione nei Modelli CFA\nOltre ai criteri base, esistono requisiti addizionali che favoriscono l’identificazione adeguata nei modelli di Confermative Factor Analysis (CFA):\n\nRegola dei Tre Indicatori per i Modelli a Singolo Fattore: Affinché un modello CFA con un solo fattore sia pienamente identificabile, è necessario che disponga di almeno tre indicatori. Questo è dovuto al fatto che con solamente due indicatori non si dispone di sufficiente informazione per separare accuratamente la varianza del fattore e i suoi carichi specifici dagli errori di misurazione. Un modello con esattamente tre indicatori ha zero gradi di libertà, il che significa che si adatterà perfettamente ai dati ma non permetterà ulteriori test o validazioni. Per garantire gradi di libertà positivi (dfM &gt; 0) e consentire un’efficace validazione del modello, è consigliabile utilizzare almeno quattro indicatori.\nRegola dei Due Indicatori per i Modelli con Più Fattori: Nei modelli CFA che coinvolgono più di un fattore, è essenziale che ogni fattore sia rappresentato da almeno due indicatori. Questa disposizione aiuta a definire chiaramente ogni fattore e a distinguerlo dagli altri fattori presenti nel modello. Tuttavia, i modelli che si limitano a due indicatori per fattore possono presentare problematiche, specialmente in campioni di dimensioni ridotte, poiché possono emergere instabilità nelle stime e complessità nell’interpretazione dei risultati.\n\nQuesti requisiti aggiuntivi sono fondamentali non solo per garantire che un modello CFA sia teoricamente valido (attraverso una corretta scalatura e definizione delle variabili latenti), ma anche per assicurare la sua utilità pratica, fornendo sufficienti gradi di libertà per consentire validazioni affidabili e interpretazioni significative del modello.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#oltre-i-requisiti-minimi-di-identificazione",
    "href": "chapters/cfa/01_cfa.html#oltre-i-requisiti-minimi-di-identificazione",
    "title": "39  Analisti Fattoriale Confermativa",
    "section": "\n39.7 Oltre i Requisiti Minimi di Identificazione",
    "text": "39.7 Oltre i Requisiti Minimi di Identificazione\nNel contesto della Confermative Factor Analysis (CFA), i requisiti di identificazione sono considerati necessari ma non sufficienti. Questo significa che, anche se il loro soddisfacimento è cruciale per una corretta stima dei parametri del modello, essi non garantiscono da soli che il modello sia il migliore possibile o pienamente identificato in termini di struttura e fondamenti teorici. Questa distinzione sottolinea l’importanza di andare oltre i criteri minimi per esplorare l’adeguatezza complessiva e la validità del modello all’interno del suo contesto teorico e applicativo.\n\n39.7.1 Perché sono Necessari\n\nGradi di Libertà (dfM) Maggiori o Uguali a Zero: Avere gradi di libertà non negativi è essenziale per assicurare che ci siano abbastanza dati per stimare i parametri del modello. Se i gradi di libertà sono negativi, indica che ci sono troppi parametri da stimare rispetto alle informazioni disponibili, il che rende il modello inidentificabile.\nScalatura Corretta di Ogni Variabile Non Misurata: La scalatura delle variabili latenti consente di stabilire un’unità di misura chiara, rendendo possibile l’interpretazione dei parametri come i carichi fattoriali. Senza una scalatura appropriata, i parametri del modello rimarrebbero indeterminati e potrebbero condurre a conclusioni ambigue.\n\n39.7.2 Perché Non Sono Sufficienti\nNonostante la soddisfazione di questi requisiti renda il modello tecnicamente identificabile e stima i parametri, ci sono altre considerazioni che possono influenzare l’adeguatezza del modello:\n\nAdeguamento del Modello: Anche se un modello ha gradi di libertà positivi e le variabili sono correttamente scalate, potrebbe non adattarsi bene ai dati. L’adeguatezza del modello è valutata attraverso statistiche di fit come il Chi-quadrato, RMSEA, CFI, e altri. Un modello può soddisfare i requisiti di identificazione ma avere un cattivo fit.\nValidità Teorica: Un modello può essere tecnicamente corretto ma non catturare accuratamente le relazioni teoriche tra le variabili. La costruzione del modello deve essere guidata da una solida base teorica che giustifica le relazioni tra i fattori e gli indicatori.\n\n39.7.3 Esempio Pratico\nImmaginiamo un modello CFA per misurare due concetti psicologici, come l’ansia e la depressione, con tre indicatori per ciascun fattore. Anche se il modello potrebbe avere gradi di libertà sufficienti e ogni fattore è correttamente scalato con un indicatore con carico fissato a 1.0, potrebbero sorgere problemi:\n\nCross-loadings: Gli indicatori per l’ansia potrebbero anche avere carichi significativi sulla depressione, il che non è catturato nel modello perché ogni indicatore è supposto misurare un solo fattore. Questo problema di validità del modello non è rilevato dai semplici criteri di identificazione.\nAdattamento del Modello: Il modello potrebbe mostrare un cattivo adattamento ai dati, suggerendo che la struttura ipotizzata dei fattori e degli indicatori non riflette accuratamente le relazioni tra le variabili osservate.\n\nIn conclusione, mentre i requisiti di identificazione sono critici per la fattibilità tecnica di un modello CFA, non garantiscono di per sé che il modello sia il migliore possibile o che rifletta accuratamente le dinamiche sottostanti. Ulteriori analisi e valutazioni sono necessarie per assicurare che il modello sia sia identificabile che valido.\n\n39.7.3.1 Altri Metodi per la Scalatura dei Fattori nei Modelli di CFA\nLa scalatura dei fattori è fondamentale per garantire una corretta identificazione e interpretazione dei fattori in un modello di Confermative Factor Analysis (CFA). Oltre al comune metodo della variabile di riferimento, esistono altri due approcci principali:\n\n\nMetodo di Standardizzazione della Varianza (Variance Standardization Method):\n\n\nDescrizione: Questo metodo fissa la varianza di ciascun fattore a 1.0, un approccio noto come unit variance identification (UVI).\n\nImplicazioni: La standardizzazione dei fattori implica che le loro varianze non sono stimate come parametri liberi. Invece, le covarianze tra i fattori sono liberamente stimate e interpretate come correlazioni di Pearson.\n\nCarichi degli Indicatori: Tutti i carichi degli indicatori sono considerati parametri liberi, il che permette di testarne la significatività statistica attraverso i loro errori standard.\n\nVantaggi e Limitazioni: Il principale vantaggio di questo metodo è la sua semplicità e l’assenza di necessità di selezionare una variabile di riferimento. Tuttavia, è generalmente più adatto per modellare fattori esogeni.\n\n\n\nMetodo di Codifica degli Effetti (Effects Coding Method):\n\n\nFunzionamento: A differenza dei metodi precedenti, questo non richiede la selezione di una variabile di riferimento e non implica la standardizzazione dei fattori.\n\nVincolo di Codifica degli Effetti (ECI): Si impone che la media dei carichi fattoriali per gli indicatori di un dato fattore sia uguale a 1.0. Questo obbliga il software SEM a trovare una combinazione ottimale di carichi che, in media, risultino in 1.0.\n\nStima della Varianza del Fattore: La varianza del fattore viene stimata come la varianza comune media calcolata attraverso tutti gli indicatori, considerando il loro contributo individuale alla misurazione del fattore.\n\nVantaggi: Questo metodo consente che tutti gli indicatori contribuiscano equamente alla scalatura del loro fattore comune. È particolarmente utile in studi longitudinali o quando si confrontano gruppi diversi, dato che le varianze dei fattori possono fornire informazioni preziose.\n\n\n\nOgni metodo di scalatura presenta vantaggi specifici e limitazioni, che devono essere considerati in base agli obiettivi della ricerca e alle caratteristiche del modello CFA:\n\nIl Metodo di Standardizzazione della Varianza offre una soluzione semplice e diretta, ma potrebbe non essere sempre il più appropriato, specialmente in contesti dove i fattori sono endogeni.\nIl Metodo di Codifica degli Effetti è vantaggioso per stabilire una scalatura equilibrata e stabile dei fattori, utile soprattutto in studi comparativi o longitudinali.\n\nLa scelta del metodo di scalatura dovrebbe essere guidata dalle necessità specifiche della ricerca, dalla struttura dei dati e dalle ipotesi teoriche del modello di CFA utilizzato.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#esempio-di-cfa-per-un-modello-di-abilità-cognitive-la-kaufman-assessment-battery-for-children",
    "href": "chapters/cfa/01_cfa.html#esempio-di-cfa-per-un-modello-di-abilità-cognitive-la-kaufman-assessment-battery-for-children",
    "title": "39  Analisti Fattoriale Confermativa",
    "section": "\n39.8 Esempio di CFA per un Modello di Abilità Cognitive: La Kaufman Assessment Battery for Children",
    "text": "39.8 Esempio di CFA per un Modello di Abilità Cognitive: La Kaufman Assessment Battery for Children\nLa Kaufman Assessment Battery for Children (KABC-I) è un test di valutazione delle abilità cognitive, somministrato individualmente a bambini dagli 2 anni e mezzo ai 12 anni e mezzo (Kaufman & Kaufman, 1983). Questo strumento è stato progettato per misurare due distinti fattori cognitivi attraverso otto indicatori.\nI primi tre compiti del test sono orientati all’elaborazione sequenziale e richiedono ai bambini di ricordare e ripetere stimoli uditivi (come nel Richiamo Numerico e nell’Ordine delle Parole) o visivi (come nei Movimenti della Mano) in un ordine specifico. Questi compiti sono pensati per riflettere la capacità di memoria a breve termine e di sequenziamento delle informazioni.\nGli altri cinque compiti, che includono la Chiusura Gestaltica e la Serie Fotografica, sono ritenuti misurare un tipo di ragionamento più olistico e meno sequenziale, associato all’elaborazione simultanea. Questi compiti valutano la capacità di integrare e sintetizzare le informazioni visuo-spaziali in un contesto più ampio, spesso indipendentemente dall’ordine in cui le informazioni sono presentate.\nKeith (1985) ha proposto delle denominazioni alternative per i fattori misurati dalla KABC-I, suggerendo i termini “memoria a breve termine” per sostituire “elaborazione sequenziale” e “ragionamento visuo-spaziale” per “elaborazione simultanea”. Queste etichette alternative riflettono una prospettiva leggermente diversa sui tipi di competenze cognitive che i due fattori intendono misurare.\nQuesto modello di CFA, utilizzando i compiti della KABC-I come indicatori, fornisce una struttura utile per comprendere come diversi tipi di elaborazione cognitiva possano essere categorizzati e valutati nei contesti educativi e diagnostici.\n\n# input the correlations in lower diagnonal form\nkabcLower.cor &lt;- \"\n 1.00\n .39 1.00\n .35  .67 1.00\n .21  .11  .16 1.00\n .32  .27  .29  .38 1.00\n .40  .29  .28  .30  .47 1.00\n .39  .32  .30  .31  .42  .41 1.00\n .39  .29  .37  .42  .58  .51  .42 1.00 \"\n\n# name the variables and convert to full correlation matrix\n# hm, hand movements; nr, number recall; wo, word order; gc, gestalt closure;\n# tr, triangles; sm, spatial memory; ma, matrix analogies; ps, photo series\nkabc.cor &lt;- lavaan::getCov(kabcLower.cor, names = c(\n    \"hm\", \"nr\", \"wo\",\n    \"gc\", \"tr\", \"sm\", \"ma\", \"ps\"\n))\n# display correlations\nkabc.cor\n#&gt;      hm   nr   wo   gc   tr   sm   ma   ps\n#&gt; hm 1.00 0.39 0.35 0.21 0.32 0.40 0.39 0.39\n#&gt; nr 0.39 1.00 0.67 0.11 0.27 0.29 0.32 0.29\n#&gt; wo 0.35 0.67 1.00 0.16 0.29 0.28 0.30 0.37\n#&gt; gc 0.21 0.11 0.16 1.00 0.38 0.30 0.31 0.42\n#&gt; tr 0.32 0.27 0.29 0.38 1.00 0.47 0.42 0.58\n#&gt; sm 0.40 0.29 0.28 0.30 0.47 1.00 0.41 0.51\n#&gt; ma 0.39 0.32 0.30 0.31 0.42 0.41 1.00 0.42\n#&gt; ps 0.39 0.29 0.37 0.42 0.58 0.51 0.42 1.00\n\n\n# add the standard deviations and convert to covariances\nkabc.cov &lt;- lavaan::cor2cov(kabc.cor, sds = c(3.40, 2.40, 2.90, 2.70, 2.70, 4.20, 2.80, 3.00))\n\n# display covariances\nkabc.cov\n#&gt;       hm    nr   wo    gc   tr    sm   ma   ps\n#&gt; hm 11.56 3.182 3.45 1.928 2.94  5.71 3.71 3.98\n#&gt; nr  3.18 5.760 4.66 0.713 1.75  2.92 2.15 2.09\n#&gt; wo  3.45 4.663 8.41 1.253 2.27  3.41 2.44 3.22\n#&gt; gc  1.93 0.713 1.25 7.290 2.77  3.40 2.34 3.40\n#&gt; tr  2.94 1.750 2.27 2.770 7.29  5.33 3.18 4.70\n#&gt; sm  5.71 2.923 3.41 3.402 5.33 17.64 4.82 6.43\n#&gt; ma  3.71 2.150 2.44 2.344 3.18  4.82 7.84 3.53\n#&gt; ps  3.98 2.088 3.22 3.402 4.70  6.43 3.53 9.00\n\n\n\n\n\n\nFigura 39.3: Modello CFA per la Kaufman Assessment Battery for Children. (Figura tratta da Kline (2023))\n\n\n\n39.8.0.1 Modello a Fattore Singolo\nNell’ambito della CFA, se il modello bersaglio ha due o più fattori, spesso il primo modello analizzato è un modello a fattore singolo. Se non si può rigettare un modello a fattore singolo, non ha molto senso valutare modelli con più fattori.\nSpecifichiamo il modello ad un fattore comune nella sintassi di lavaan.\n\n# single factor (general ability)\n# indicator hm automatically specified as reference variable\nkabc1.model &lt;- \"\n    General =~ hm + nr + wo + gc + tr + sm + ma + ps \n\"\n\nAdattiamo il modello ai dati.\n\n# variances calculated with N in the denominator, not N - 1\nkabc1 &lt;- lavaan::sem(\n    kabc1.model, \n    sample.cov = kabc.cov, \n    sample.nobs = 200\n)\n\nGeneriamo un modello di percorso.\n\nsemPlot::semPaths(kabc1,\n    what = \"col\", whatLabels = \"par\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\nEsaminiamo la soluzione non standardizzata.\n\nlavaan::parameterEstimates(kabc1) \n#&gt;        lhs op     rhs   est    se    z pvalue ci.lower ci.upper\n#&gt; 1  General =~      hm 1.000 0.000   NA     NA    1.000    1.000\n#&gt; 2  General =~      nr 0.636 0.111 5.71      0    0.418    0.854\n#&gt; 3  General =~      wo 0.805 0.136 5.91      0    0.538    1.072\n#&gt; 4  General =~      gc 0.659 0.123 5.36      0    0.418    0.900\n#&gt; 5  General =~      tr 0.963 0.138 6.98      0    0.693    1.233\n#&gt; 6  General =~      sm 1.433 0.211 6.80      0    1.019    1.846\n#&gt; 7  General =~      ma 0.883 0.137 6.46      0    0.615    1.151\n#&gt; 8  General =~      ps 1.166 0.159 7.32      0    0.854    1.478\n#&gt; 9       hm ~~      hm 7.812 0.863 9.05      0    6.120    9.504\n#&gt; 10      nr ~~      nr 4.240 0.456 9.29      0    3.345    5.134\n#&gt; 11      wo ~~      wo 5.975 0.650 9.20      0    4.702    7.249\n#&gt; 12      gc ~~      gc 5.652 0.599 9.43      0    4.478    6.827\n#&gt; 13      tr ~~      tr 3.831 0.468 8.19      0    2.914    4.748\n#&gt; 14      sm ~~      sm 9.979 1.179 8.46      0    7.668   12.290\n#&gt; 15      ma ~~      ma 4.925 0.558 8.82      0    3.831    6.020\n#&gt; 16      ps ~~      ps 3.936 0.531 7.41      0    2.895    4.977\n#&gt; 17 General ~~ General 3.690 0.921 4.01      0    1.885    5.494\n\nLa saturazione non standardizzata per il compito “Movimenti della Mano” è stato fissato automaticamente a 1.0 per scalare il singolo fattore comune. Le istruzioni seguenti consentono di estrarre dall’output di sem() solo le informazioni relative alle saturazioni fattoriali.\n\nparameterEstimates(kabc1, standardized = TRUE) %&gt;%\n    dplyr::filter(op == \"=~\") %&gt;%\n    dplyr::select(\n        \"Latent Factor\" = lhs,\n        Indicator = rhs,\n        B = est,\n        SE = se,\n        Z = z,\n        \"p-value\" = pvalue,\n        Beta = std.all\n    ) %&gt;%\n    knitr::kable(\n        digits = 3, booktabs = TRUE, format = \"markdown\",\n        caption = \"Factor Loadings\"\n    )\n\n\nFactor Loadings\n\nLatent Factor\nIndicator\nB\nSE\nZ\np-value\nBeta\n\n\n\nGeneral\nhm\n1.000\n0.000\nNA\nNA\n0.566\n\n\nGeneral\nnr\n0.636\n0.111\n5.71\n0\n0.510\n\n\nGeneral\nwo\n0.805\n0.136\n5.91\n0\n0.535\n\n\nGeneral\ngc\n0.659\n0.123\n5.36\n0\n0.470\n\n\nGeneral\ntr\n0.963\n0.138\n6.98\n0\n0.687\n\n\nGeneral\nsm\n1.433\n0.211\n6.80\n0\n0.657\n\n\nGeneral\nma\n0.883\n0.137\n6.46\n0\n0.607\n\n\nGeneral\nps\n1.166\n0.159\n7.32\n0\n0.749\n\n\n\n\n\nEsaminiamo le misure di bontà di adattamento.\n\nfitMeasures(kabc1, c(\"chisq\", \"df\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt; print()\n#&gt;   chisq      df     cfi     tli   rmsea    srmr \n#&gt; 105.427  20.000   0.818   0.746   0.146   0.084\n\nTroviamo i residui grezzi, ovvero la differenza tra la matrice di covarianza osservata e quella predetta dal modello.\n\nlavaan::residuals(kabc1, type = \"raw\") |&gt; print()\n#&gt; $type\n#&gt; [1] \"raw\"\n#&gt; \n#&gt; $cov\n#&gt;        hm     nr     wo     gc     tr     sm     ma     ps\n#&gt; hm  0.000                                                 \n#&gt; nr  0.820  0.000                                          \n#&gt; wo  0.462  2.751  0.000                                   \n#&gt; gc -0.513 -0.836 -0.711  0.000                            \n#&gt; tr -0.631 -0.519 -0.602  0.415  0.000                     \n#&gt; sm  0.397 -0.452 -0.863 -0.097  0.212  0.000              \n#&gt; ma  0.437  0.069 -0.199  0.186  0.022  0.131  0.000       \n#&gt; ps -0.345 -0.659 -0.263  0.550  0.530  0.229 -0.289  0.000\n\nSpecificando type = \"cor.bollen\" o type = \"cor\" otteniamo la differenza tra la matrice di correlazione osservata e quella predetta dal modello.\n\nlavaan::residuals(kabc1, type = \"cor.bollen\") |&gt; print()\n#&gt; $type\n#&gt; [1] \"cor.bollen\"\n#&gt; \n#&gt; $cov\n#&gt;        hm     nr     wo     gc     tr     sm     ma     ps\n#&gt; hm  0.000                                                 \n#&gt; nr  0.101  0.000                                          \n#&gt; wo  0.047  0.397  0.000                                   \n#&gt; gc -0.056 -0.130 -0.091  0.000                            \n#&gt; tr -0.069 -0.080 -0.077  0.057  0.000                     \n#&gt; sm  0.028 -0.045 -0.071 -0.009  0.019  0.000              \n#&gt; ma  0.046  0.010 -0.025  0.025  0.003  0.011  0.000       \n#&gt; ps -0.034 -0.092 -0.030  0.068  0.066  0.018 -0.035  0.000\n\nIn alternativa, possiamo ottenere i residui standardizzati alla maniera di Mplus (standardized.mplus), che vengono calcolati utilizzando la seguente formula:\n\\[\n    \\text{Residuo Standardizzato} = \\frac{\\text{Cov. Osservata} - \\text{Cov. Stimata}}{\\sqrt{\\text{Var. dell'Errore per X} \\times \\text{Var. dell'Errore per Y}}},\n\\]\ndove: - La covarianza osservata è il valore della covarianza tra due variabili nel set di dati. - La covarianza stimata è la covarianza tra le stesse due variabili, come previsto dal modello SEM. - La varianza dell’errore per la variabile X e Y sono le varianze degli errori per le due variabili in questione.\nI residui standardizzati misurano quanto la relazione osservata tra due variabili si discosta da quella prevista dal modello, in unità standardizzate. Un valore vicino a zero indica che il modello si adatta bene ai dati per quella specifica relazione. Valori più grandi in valore assoluto suggeriscono un cattivo adattamento in quella specifica parte del modello.\n\nlavaan::residuals(kabc1, type = \"standardized.mplus\") |&gt; print()\n#&gt; $type\n#&gt; [1] \"standardized.mplus\"\n#&gt; \n#&gt; $cov\n#&gt;        hm     nr     wo     gc     tr     sm     ma     ps\n#&gt; hm  0.000                                                 \n#&gt; nr  2.062  0.000                                          \n#&gt; wo  1.026  6.218  0.000                                   \n#&gt; gc -1.231 -2.727 -1.952  0.000                            \n#&gt; tr -2.200 -2.364 -2.355  1.379  0.000                     \n#&gt; sm  0.723 -1.188 -1.995 -0.210  0.596  0.000              \n#&gt; ma  1.086  0.237 -0.601  0.544  0.089  0.313  0.000       \n#&gt; ps -1.241 -3.422 -1.037  1.833  2.178  0.675 -1.375  0.000\n\nIl modello a fattore singolo mostra un rapporto elevato chi-quadro/df. Inoltre, i residui per questa analisi indicano che l’adattamento locale è scadente. Pertanto, il modello a fattore singolo per la KABC-I è rigettato.\n\n39.8.0.2 Modello a Due Fattori\nIn una seconda analisi, adattiamo ai dati il modello a due fattori rappresentato nella {numref}kline-14-3-fig.\n\nkabc2_model &lt;- \"\n    Sequent =~ hm + nr + wo\n    Simultan =~ gc + tr + sm + ma + ps \n\"\n\n\nkabc2 &lt;- lavaan::sem(kabc2_model, sample.cov = kabc.cov, sample.nobs = 200)\n\n\nsemPlot::semPaths(kabc2,\n    what = \"col\", whatLabels = \"par\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\n\nlavaan::parameterEstimates(kabc2) |&gt; print()\n#&gt;         lhs op      rhs   est    se    z pvalue ci.lower ci.upper\n#&gt; 1   Sequent =~       hm  1.00 0.000   NA     NA    1.000     1.00\n#&gt; 2   Sequent =~       nr  1.15 0.181 6.34  0.000    0.792     1.50\n#&gt; 3   Sequent =~       wo  1.39 0.219 6.34  0.000    0.959     1.82\n#&gt; 4  Simultan =~       gc  1.00 0.000   NA     NA    1.000     1.00\n#&gt; 5  Simultan =~       tr  1.45 0.227 6.35  0.000    0.999     1.89\n#&gt; 6  Simultan =~       sm  2.03 0.335 6.06  0.000    1.373     2.69\n#&gt; 7  Simultan =~       ma  1.21 0.212 5.72  0.000    0.797     1.63\n#&gt; 8  Simultan =~       ps  1.73 0.265 6.52  0.000    1.208     2.25\n#&gt; 9        hm ~~       hm  8.66 0.938 9.24  0.000    6.826    10.50\n#&gt; 10       nr ~~       nr  2.00 0.414 4.83  0.000    1.188     2.81\n#&gt; 11       wo ~~       wo  2.90 0.604 4.80  0.000    1.717     4.09\n#&gt; 12       gc ~~       gc  5.42 0.585 9.26  0.000    4.272     6.57\n#&gt; 13       tr ~~       tr  3.43 0.458 7.48  0.000    2.528     4.32\n#&gt; 14       sm ~~       sm 10.00 1.202 8.32  0.000    7.642    12.35\n#&gt; 15       ma ~~       ma  5.11 0.578 8.84  0.000    3.973     6.24\n#&gt; 16       ps ~~       ps  3.48 0.537 6.48  0.000    2.429     4.54\n#&gt; 17  Sequent ~~  Sequent  2.84 0.838 3.39  0.001    1.197     4.48\n#&gt; 18 Simultan ~~ Simultan  1.83 0.530 3.46  0.001    0.795     2.87\n#&gt; 19  Sequent ~~ Simultan  1.27 0.324 3.92  0.000    0.635     1.91\n\n\nstandardizedSolution(kabc2) |&gt; print()\n#&gt;         lhs op      rhs est.std    se     z pvalue ci.lower ci.upper\n#&gt; 1   Sequent =~       hm   0.497 0.062  8.03      0    0.376    0.618\n#&gt; 2   Sequent =~       nr   0.807 0.046 17.44      0    0.716    0.898\n#&gt; 3   Sequent =~       wo   0.808 0.046 17.48      0    0.718    0.899\n#&gt; 4  Simultan =~       gc   0.503 0.061  8.26      0    0.384    0.622\n#&gt; 5  Simultan =~       tr   0.726 0.044 16.46      0    0.640    0.813\n#&gt; 6  Simultan =~       sm   0.656 0.050 13.23      0    0.559    0.753\n#&gt; 7  Simultan =~       ma   0.588 0.055 10.72      0    0.480    0.695\n#&gt; 8  Simultan =~       ps   0.782 0.040 19.48      0    0.703    0.860\n#&gt; 9        hm ~~       hm   0.753 0.061 12.26      0    0.633    0.874\n#&gt; 10       nr ~~       nr   0.349 0.075  4.67      0    0.202    0.495\n#&gt; 11       wo ~~       wo   0.347 0.075  4.64      0    0.200    0.493\n#&gt; 12       gc ~~       gc   0.747 0.061 12.20      0    0.627    0.867\n#&gt; 13       tr ~~       tr   0.472 0.064  7.37      0    0.347    0.598\n#&gt; 14       sm ~~       sm   0.570 0.065  8.75      0    0.442    0.697\n#&gt; 15       ma ~~       ma   0.654 0.065 10.14      0    0.528    0.781\n#&gt; 16       ps ~~       ps   0.389 0.063  6.20      0    0.266    0.512\n#&gt; 17  Sequent ~~  Sequent   1.000 0.000    NA     NA    1.000    1.000\n#&gt; 18 Simultan ~~ Simultan   1.000 0.000    NA     NA    1.000    1.000\n#&gt; 19  Sequent ~~ Simultan   0.557 0.067  8.35      0    0.426    0.688\n\n\nfitMeasures(kabc2, c(\"chisq\", \"df\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt; print()\n#&gt;  chisq     df    cfi    tli  rmsea   srmr \n#&gt; 38.325 19.000  0.959  0.939  0.071  0.072\n\nUn modello di analisi fattoriale confermativa (CFA) che utilizza un singolo fattore può essere visto come un caso specifico o un “sottoinsieme” di modelli CFA più complessi con due o più fattori che impiegano gli stessi indicatori e lo stesso schema di covarianza degli errori, se presente. Questa struttura gerarchica tra i modelli a singolo fattore e quelli multifattoriali implica che i ricercatori possono applicare il test del chi-quadro per confrontare direttamente l’adattamento di un modello CFA a singolo fattore con quello di modelli CFA a più fattori. In pratica, ciò permette di valutare se l’introduzione di fattori aggiuntivi migliora significativamente l’adattamento del modello ai dati rispetto a un modello più semplice a singolo fattore. Questo tipo di analisi è fondamentale per determinare la complessità ottimale del modello in base alla struttura sottostante dei dati. Sebbene questo argomento verrà approfondito successivamente, è importante anticipare qui l’utilizzo del test del rapporto di verosimiglianza, che consente di confrontare i modelli in maniera quantitativa.\n\nlavTestLRT(kabc1, kabc2) |&gt; print()\n#&gt; \n#&gt; Chi-Squared Difference Test\n#&gt; \n#&gt;       Df  AIC  BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\n#&gt; kabc2 19 7592 7648  38.3                                    \n#&gt; kabc1 20 7657 7710 105.4       67.1 0.575       1    2.6e-16\n\nI risultati del test indicano che l’adattamento del modello con due fattori è statisticamente migliore rispetto a quello del modello a fattore singolo (il modello ad un fattore ha un valore \\(\\chi^2\\) superiore di 67.1 punti, con un grado di libertà).\nAnche se il test del rapporto tra verosimiglianze favorisce il modello a due fattori, possiamo notare che l’esame dei residui mostra un problema con l’indicatore hm.\n\nlavaan::residuals(kabc2, type = \"standardized.mplus\") |&gt; print()\n#&gt; $type\n#&gt; [1] \"standardized.mplus\"\n#&gt; \n#&gt; $cov\n#&gt;        hm     nr     wo     gc     tr     sm     ma     ps\n#&gt; hm  0.000                                                 \n#&gt; nr -0.591  0.000                                          \n#&gt; wo -3.790  1.539  0.000                                   \n#&gt; gc  1.126 -2.329 -1.315     NA                            \n#&gt; tr  2.046 -1.558 -1.001  0.429  0.000                     \n#&gt; sm  3.464 -0.112 -0.355 -0.784 -0.267  0.000              \n#&gt; ma  3.505  1.129  0.727  0.323 -0.245  0.664  0.008       \n#&gt; ps  2.991 -2.002  0.524  0.910  0.677 -0.144 -1.978  0.000\n\nPer affrontare questo problema, calcoliamo i modification indices che ci dicono quale parametro del modello ha l’effetto maggiore sulla misura di fit complessivo.\n\nmodindices(kabc2, sort = TRUE, maximum.number = 5) |&gt; print()\n#&gt;         lhs op rhs    mi   epc sepc.lv sepc.all sepc.nox\n#&gt; 25 Simultan =~  hm 20.10  1.05   1.428    0.421    0.421\n#&gt; 35       nr ~~  wo 20.10  4.74   4.741    1.969    1.969\n#&gt; 26 Simultan =~  nr  7.01 -0.51  -0.691   -0.289   -0.289\n#&gt; 29       hm ~~  wo  7.01 -1.75  -1.746   -0.348   -0.348\n#&gt; 32       hm ~~  sm  4.85  1.61   1.609    0.173    0.173\n\nI risultati degli indici di modifica (MI) indicano che il misfit del modello è principalmente attribuibile alla fissazione a zero del carico tra l’indicatore hm e il fattore comune Simulan, nonché alla fissazione a zero della covarianza tra le componenti residue di nr e wo. Per migliorare l’adattamento del modello, si propone quindi di modificare questi aspetti, iniziando con il primo, ovvero riconsiderando il carico di hm sul fattore Simulan.\n\nkabc3_model &lt;- \"\n    Sequent =~ hm + nr + wo\n    Simultan =~ hm + gc + tr + sm + ma + ps\n\"\n\n\nkabc3 &lt;- lavaan::sem(kabc3_model, sample.cov = kabc.cov, sample.nobs = 200)\n\n\nlavaan::parameterEstimates(kabc3) |&gt; print()\n#&gt;         lhs op      rhs   est    se    z pvalue ci.lower ci.upper\n#&gt; 1   Sequent =~       hm 1.000 0.000   NA     NA    1.000    1.000\n#&gt; 2   Sequent =~       nr 2.285 0.777 2.94  0.003    0.762    3.808\n#&gt; 3   Sequent =~       wo 2.767 0.941 2.94  0.003    0.922    4.612\n#&gt; 4  Simultan =~       hm 1.000 0.000   NA     NA    1.000    1.000\n#&gt; 5  Simultan =~       gc 1.014 0.255 3.98  0.000    0.515    1.514\n#&gt; 6  Simultan =~       tr 1.457 0.329 4.43  0.000    0.812    2.101\n#&gt; 7  Simultan =~       sm 2.103 0.483 4.35  0.000    1.157    3.050\n#&gt; 8  Simultan =~       ma 1.259 0.298 4.23  0.000    0.675    1.842\n#&gt; 9  Simultan =~       ps 1.752 0.391 4.49  0.000    0.987    2.518\n#&gt; 10       hm ~~       hm 7.851 0.845 9.29  0.000    6.195    9.507\n#&gt; 11       nr ~~       nr 1.899 0.487 3.90  0.000    0.944    2.854\n#&gt; 12       wo ~~       wo 2.750 0.713 3.86  0.000    1.352    4.148\n#&gt; 13       gc ~~       gc 5.444 0.585 9.30  0.000    4.296    6.591\n#&gt; 14       tr ~~       tr 3.521 0.457 7.70  0.000    2.625    4.417\n#&gt; 15       sm ~~       sm 9.767 1.179 8.29  0.000    7.457   12.077\n#&gt; 16       ma ~~       ma 5.013 0.569 8.81  0.000    3.898    6.127\n#&gt; 17       ps ~~       ps 3.554 0.529 6.72  0.000    2.517    4.591\n#&gt; 18  Sequent ~~  Sequent 0.734 0.490 1.50  0.134   -0.226    1.693\n#&gt; 19 Simultan ~~ Simultan 1.759 0.760 2.31  0.021    0.269    3.250\n#&gt; 20  Sequent ~~ Simultan 0.579 0.178 3.25  0.001    0.230    0.928\n\nIl modello così modificato fornisce un buon adattamento ai dati.\n\nfitMeasures(kabc3, c(\"chisq\", \"df\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt; print()\n#&gt;  chisq     df    cfi    tli  rmsea   srmr \n#&gt; 18.108 18.000  1.000  1.000  0.005  0.035\n\n\nlavaan::residuals(kabc3, type = \"standardized.mplus\") |&gt; print()\n#&gt; $type\n#&gt; [1] \"standardized.mplus\"\n#&gt; \n#&gt; $cov\n#&gt;        hm     nr     wo     gc     tr     sm     ma     ps\n#&gt; hm  0.000                                                 \n#&gt; nr  1.165  0.000                                          \n#&gt; wo -1.637  0.000  0.000                                   \n#&gt; gc -1.066 -1.919 -0.939  0.000                            \n#&gt; tr -1.710 -0.763 -0.247  0.603  0.000                     \n#&gt; sm  1.325  0.287  0.044 -0.867 -0.304  0.000              \n#&gt; ma  1.730  1.428  1.029  0.258 -0.298  0.338  0.000       \n#&gt; ps -0.512 -1.059  1.285  1.035  1.088 -0.361 -2.181  0.008\n\nEseguiamo il confronto tra questo terzo modello e il secondo.\n\nlavTestLRT(kabc2, kabc3) |&gt; print()\n#&gt; \n#&gt; Chi-Squared Difference Test\n#&gt; \n#&gt;       Df  AIC  BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\n#&gt; kabc3 18 7574 7633  18.1                                    \n#&gt; kabc2 19 7592 7648  38.3       20.2  0.31       1    6.9e-06\n\nIl test del rapporto tra verosimiglianze favorisce il modello nel quale hm satura su entrambi i fattori comuni.\n\nstandardizedSolution(kabc3) |&gt; print()\n#&gt;         lhs op      rhs est.std    se     z pvalue ci.lower ci.upper\n#&gt; 1   Sequent =~       hm   0.253 0.082  3.07  0.002    0.091    0.414\n#&gt; 2   Sequent =~       nr   0.818 0.053 15.34  0.000    0.713    0.922\n#&gt; 3   Sequent =~       wo   0.819 0.053 15.37  0.000    0.715    0.924\n#&gt; 4  Simultan =~       hm   0.391 0.079  4.94  0.000    0.236    0.546\n#&gt; 5  Simultan =~       gc   0.500 0.061  8.21  0.000    0.380    0.619\n#&gt; 6  Simultan =~       tr   0.717 0.044 16.19  0.000    0.631    0.804\n#&gt; 7  Simultan =~       sm   0.666 0.048 13.76  0.000    0.571    0.761\n#&gt; 8  Simultan =~       ma   0.598 0.054 11.11  0.000    0.492    0.703\n#&gt; 9  Simultan =~       ps   0.777 0.040 19.53  0.000    0.699    0.855\n#&gt; 10       hm ~~       hm   0.683 0.061 11.23  0.000    0.563    0.802\n#&gt; 11       nr ~~       nr   0.331 0.087  3.80  0.000    0.160    0.502\n#&gt; 12       wo ~~       wo   0.329 0.087  3.76  0.000    0.157    0.500\n#&gt; 13       gc ~~       gc   0.750 0.061 12.35  0.000    0.631    0.870\n#&gt; 14       tr ~~       tr   0.485 0.064  7.64  0.000    0.361    0.610\n#&gt; 15       sm ~~       sm   0.556 0.064  8.63  0.000    0.430    0.683\n#&gt; 16       ma ~~       ma   0.643 0.064  9.99  0.000    0.517    0.769\n#&gt; 17       ps ~~       ps   0.397 0.062  6.43  0.000    0.276    0.518\n#&gt; 18  Sequent ~~  Sequent   1.000 0.000    NA     NA    1.000    1.000\n#&gt; 19 Simultan ~~ Simultan   1.000 0.000    NA     NA    1.000    1.000\n#&gt; 20  Sequent ~~ Simultan   0.510 0.070  7.25  0.000    0.372    0.647\n\n\nsemPlot::semPaths(kabc3,\n    what = \"col\", whatLabels = \"std\", style = \"mx\",\n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\nNei modelli precedenti, abbiamo adottato un metodo di scalatura dei fattori comuni che fissava la saturazione fattoriale di uno degli indicatori per ciascun fattore comune a 1.0 come riferimento. Ora, esploreremo un diverso approccio di scalatura che prevede la standardizzazione della varianza delle variabili latenti.\nPer attuare questa procedura nel software lavaan, è necessario modificare la configurazione predefinita in cui la saturazione fattoriale del primo indicatore di ogni fattore comune è fissata a 1.0. Per fare ciò, useremo la sintassi NA* per indicare che la saturazione fattoriale del primo indicatore deve essere stimata. Questo si realizza inserendo NA* nell’istruzione che definisce la relazione tra le variabili latenti e gli indicatori (espresso tramite =~). Inoltre, è fondamentale specificare che la varianza delle variabili latenti sia fissata a 1.0, il che si attua mediante la sintassi 1* nell’istruzione che stabilisce la varianza di ciascun fattore comune (~~).\n\nkabc3alt_model &lt;- \"\n    Sequent =~ NA*hm + nr + wo\n    Simultan =~ NA*hm + gc + tr + sm + ma + ps\n\n    Sequent ~~ 1*Sequent\n    Simultan ~~ 1*Simultan\n\"\n\nAdattiamo il modello così parametrizzato ai dati.\n\nkabc3alt &lt;- lavaan::sem(\n    kabc3alt_model, sample.cov = kabc.cov, sample.nobs = 200, std.lv = TRUE\n)\n\nEsaminiamo la soluzione non standardizzata.\n\nsemPlot::semPaths(kabc3alt,\n    what = \"col\", whatLabels = \"par\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\nEsaminiamo la soluzione stanardizzata.\n\nloadings &lt;- standardizedSolution(kabc3alt)\nloadings |&gt; print()\n#&gt;         lhs op      rhs est.std    se     z pvalue ci.lower ci.upper\n#&gt; 1   Sequent =~       hm   0.253 0.082  3.07  0.002    0.091    0.414\n#&gt; 2   Sequent =~       nr   0.818 0.053 15.34  0.000    0.713    0.922\n#&gt; 3   Sequent =~       wo   0.819 0.053 15.37  0.000    0.715    0.924\n#&gt; 4  Simultan =~       hm   0.391 0.079  4.94  0.000    0.236    0.546\n#&gt; 5  Simultan =~       gc   0.500 0.061  8.21  0.000    0.380    0.619\n#&gt; 6  Simultan =~       tr   0.717 0.044 16.19  0.000    0.631    0.804\n#&gt; 7  Simultan =~       sm   0.666 0.048 13.76  0.000    0.571    0.761\n#&gt; 8  Simultan =~       ma   0.598 0.054 11.11  0.000    0.492    0.703\n#&gt; 9  Simultan =~       ps   0.777 0.040 19.53  0.000    0.699    0.855\n#&gt; 10  Sequent ~~  Sequent   1.000 0.000    NA     NA    1.000    1.000\n#&gt; 11 Simultan ~~ Simultan   1.000 0.000    NA     NA    1.000    1.000\n#&gt; 12       hm ~~       hm   0.683 0.061 11.23  0.000    0.563    0.802\n#&gt; 13       nr ~~       nr   0.331 0.087  3.80  0.000    0.160    0.502\n#&gt; 14       wo ~~       wo   0.329 0.087  3.76  0.000    0.157    0.500\n#&gt; 15       gc ~~       gc   0.750 0.061 12.35  0.000    0.631    0.870\n#&gt; 16       tr ~~       tr   0.485 0.064  7.64  0.000    0.361    0.610\n#&gt; 17       sm ~~       sm   0.556 0.064  8.63  0.000    0.430    0.683\n#&gt; 18       ma ~~       ma   0.643 0.064  9.99  0.000    0.517    0.769\n#&gt; 19       ps ~~       ps   0.397 0.062  6.43  0.000    0.276    0.518\n#&gt; 20  Sequent ~~ Simultan   0.510 0.070  7.25  0.000    0.372    0.647\n\n\nrelevant_loadings &lt;- loadings[loadings$op == \"=~\", c(\"lhs\", \"rhs\", \"est.std\")]\nrelevant_loadings |&gt; print()\n#&gt;        lhs rhs est.std\n#&gt; 1  Sequent  hm   0.253\n#&gt; 2  Sequent  nr   0.818\n#&gt; 3  Sequent  wo   0.819\n#&gt; 4 Simultan  hm   0.391\n#&gt; 5 Simultan  gc   0.500\n#&gt; 6 Simultan  tr   0.717\n#&gt; 7 Simultan  sm   0.666\n#&gt; 8 Simultan  ma   0.598\n#&gt; 9 Simultan  ps   0.777\n\nIdealmente, per sostenere l’ipotesi di validità convergente, un fattore dovrebbe spiegare almeno il 50% della varianza in ciascuno dei suoi indicatori continui, come sostengono Bagozzi e Yi (2012). Ciò implica che, per essere considerato adeguatamente rappresentativo del costrutto che intende misurare, tutti gli indicatori di un fattore dovrebbero mostrare che la maggior parte della loro varianza è spiegata dal fattore stesso. Un modo meno rigoroso ma ancora informativo per valutare la validità convergente è attraverso l’uso della Varianza Media Estratta (AVE), calcolata come la media dei quadrati dei carichi fattoriali standardizzati di tutti gli indicatori associati a un particolare fattore. Un valore AVE superiore a 0.50 indica che, in media, il fattore spiega più della metà della varianza degli indicatori rispetto alla varianza residua attribuibile agli errori di misurazione, come indicato da Hair et al. (2022).\nNell’ambito di un modello a due fattori, i risultati ottenuti dall’esempio in esame evidenziano alcune criticità in relazione al criterio più stringente: il modello non riesce a spiegare una variazione significativa (R^2 &gt; 0.50) per quattro dei otto indicatori, ossia la metà di essi. Tuttavia, se consideriamo l’AVE, i risultati migliorano leggermente per il fattore sequenziale, che spiega in media circa il 52% della varianza dei suoi tre indicatori (AVE = 0.517).\nNella pratica analitica reale, valori di R^2 inferiori a 0.50 sono spesso considerati accettabili. Comrey e Lee (1992) hanno proposto una scala di valutazione gradiente in cui un R^2 superiore a 0.50 è classificato come eccellente, mentre valori approssimativamente pari a 0.40, 0.30, 0.20 e 0.10 sono considerati molto buoni, buoni, sufficienti e scarsi, rispettivamente. Secondo queste linee guida più flessibili, i risultati per gli indicatori del modello CFA a due fattori della KABC-I sono classificati come “eccellenti” (R^2 &gt; 0.50) per tre degli otto indicatori, nessuno è giudicato “scarso” (R^2 circa 0.10), e i rimanenti cinque indicatori presentano valori intermedi. È essenziale sottolineare che queste linee guida non dovrebbero essere applicate in modo indiscriminato in tutti i contesti di CFA o con tutti i tipi di indicatori. Gli indicatori continui, come i punteggi totali nell’esempio citato, tendono a mostrare carichi fattoriali più elevati rispetto agli indicatori ordinali, come quelli basati su scale di risposta tipo Likert.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#riflessioni-conclusive",
    "href": "chapters/cfa/01_cfa.html#riflessioni-conclusive",
    "title": "39  Analisti Fattoriale Confermativa",
    "section": "\n39.9 Riflessioni Conclusive",
    "text": "39.9 Riflessioni Conclusive\nL’analisi fattoriale confermativa (CFA) rappresenta uno strumento cruciale nell’ambito delle ricerche psicologiche e sociali, in quanto consente di esaminare modelli di misurazione riflessiva. In questi modelli, i fattori comuni agiscono come proxy per le variabili teoriche. La CFA richiede che il ricercatore definisca preventivamente aspetti critici del modello, come il numero di fattori, l’assegnazione degli indicatori ai fattori e gli schemi di covarianza degli errori.\nNei modelli CFA base, ciascun indicatore continuo è associato a un unico fattore e si presume che gli errori siano indipendenti, formando così una struttura unidimensionale. L’analisi di modelli con più fattori permette di verificare le ipotesi di validità convergente e discriminante.\nÈ anche possibile esplorare modelli CFA che includono covarianze di errore o indicatori correlati a più fattori. Tuttavia, gestire tali modelli è più complesso, specialmente in termini di identificazione del modello. Problemi tecnici come la non convergenza delle soluzioni o risultati inammissibili sono più comuni nei campioni di dimensioni ridotte o quando i fattori sono definiti da soli due indicatori. L’aggiustamento del modello può diventare una sfida, considerata l’ampia varietà di modifiche potenziali.\nUn’altra questione critica è rappresentata dai modelli CFA equivalenti, i quali possono produrre risultati simili nonostante le loro differenze strutturali. Per affrontare queste sfide efficacemente, è essenziale fondare l’analisi più su basi teoriche che su meri calcoli statistici. L’efficacia della CFA, quindi, dipende notevolmente dal contesto teorico e dalla competenza metodologica del ricercatore, essendo cruciale un’approfondita comprensione del dominio di studio per guidare l’analisi.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#session-info",
    "href": "chapters/cfa/01_cfa.html#session-info",
    "title": "39  Analisti Fattoriale Confermativa",
    "section": "\n39.10 Session Info",
    "text": "39.10 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] ggokabeito_0.1.0  see_0.10.0        MASS_7.3-65       viridis_0.6.5    \n#&gt;  [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n#&gt;  [9] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n#&gt; [13] lavaan_0.6-19     psych_2.4.12      scales_1.3.0      markdown_1.13    \n#&gt; [17] knitr_1.49        lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n#&gt; [21] dplyr_1.1.4       purrr_1.0.4       readr_2.1.5       tidyr_1.3.1      \n#&gt; [25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.0      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] mime_0.12           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [25] Matrix_1.7-2        R6_2.6.1            fastmap_1.2.0      \n#&gt;  [28] rbibutils_2.3       shiny_1.10.0        digest_0.6.37      \n#&gt;  [31] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n#&gt;  [34] rprojroot_2.0.4     Hmisc_5.2-2         timechange_0.3.0   \n#&gt;  [37] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n#&gt;  [40] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n#&gt;  [43] carData_3.0-5       ggsignif_0.6.4      corpcor_1.6.10     \n#&gt;  [46] gtools_3.9.5        tools_4.4.2         pbivnorm_0.6.0     \n#&gt;  [49] foreign_0.8-88      zip_2.3.2           httpuv_1.6.15      \n#&gt;  [52] nnet_7.3-20         glue_1.8.0          quadprog_1.5-8     \n#&gt;  [55] nlme_3.1-167        promises_1.3.2      lisrelToR_0.3      \n#&gt;  [58] grid_4.4.2          checkmate_2.3.2     cluster_2.1.8      \n#&gt;  [61] reshape2_1.4.4      generics_0.1.3      gtable_0.3.6       \n#&gt;  [64] tzdb_0.4.0          data.table_1.17.0   hms_1.1.3          \n#&gt;  [67] car_3.1-3           sem_3.1-16          pillar_1.10.1      \n#&gt;  [70] rockchalk_1.8.157   later_1.4.1         splines_4.4.2      \n#&gt;  [73] lattice_0.22-6      survival_3.8-3      kutils_1.73        \n#&gt;  [76] tidyselect_1.2.1    miniUI_0.1.1.1      pbapply_1.7-2      \n#&gt;  [79] reformulas_0.4.0    stats4_4.4.2        xfun_0.51          \n#&gt;  [82] qgraph_1.9.8        arm_1.14-4          stringi_1.8.4      \n#&gt;  [85] yaml_2.3.10         pacman_0.5.1        boot_1.3-31        \n#&gt;  [88] evaluate_1.0.3      codetools_0.2-20    mi_1.1             \n#&gt;  [91] cli_3.6.4           RcppParallel_5.1.10 rpart_4.1.24       \n#&gt;  [94] xtable_1.8-4        Rdpack_2.6.2        munsell_0.5.1      \n#&gt;  [97] Rcpp_1.0.14         coda_0.19-4.1       png_0.1-8          \n#&gt; [100] XML_3.99-0.18       parallel_4.4.2      jpeg_0.1-10        \n#&gt; [103] lme4_1.1-36         mvtnorm_1.3-3       openxlsx_4.2.8     \n#&gt; [106] rlang_1.1.5         multcomp_1.4-28     mnormt_2.1.1\n\n\n\n\n\nKline, R. B. (2023). Principles and practice of structural equation modeling. Guilford publications.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/02_meanstructure.html",
    "href": "chapters/cfa/02_meanstructure.html",
    "title": "40  La struttura delle medie",
    "section": "",
    "text": "40.1 Introduzione\nL’Analisi Fattoriale Confermativa (CFA) condivide con l’analisi fattoriale tradizionale l’obiettivo di esaminare le relazioni di covarianza tra le variabili. Tuttavia, una caratteristica distintiva della CFA è la possibilità di includere nel modello anche le medie, sia delle variabili osservate che di quelle latenti. Questo approccio si rivela particolarmente utile in contesti come l’analisi fattoriale confermativa longitudinale, dove le ipotesi non si limitano alle covarianze, ma riguardano anche i cambiamenti nelle medie dei costrutti analizzati nel tempo.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>La struttura delle medie</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/02_meanstructure.html#interpretazione-delle-intercette-nei-modelli-cfa",
    "href": "chapters/cfa/02_meanstructure.html#interpretazione-delle-intercette-nei-modelli-cfa",
    "title": "40  La struttura delle medie",
    "section": "\n40.2 Interpretazione delle Intercette nei Modelli CFA",
    "text": "40.2 Interpretazione delle Intercette nei Modelli CFA\nNei modelli CFA o nei Modelli di Equazioni Strutturali (SEM), l’intercetta di una variabile indicatore, denotata con \\(\\tau\\), rappresenta la media stimata dell’indicatore quando il fattore latente a cui è associato assume il valore zero. Questo consente di integrare nel modello informazioni sia sulle relazioni tra variabili (covarianze) sia sulle loro medie.\nLa relazione generale per un indicatore \\(y\\) in un modello CFS (o SEM) è espressa dalla seguente equazione:\n\\[\ny = \\tau + \\lambda \\cdot \\text{fattore latente} + \\varepsilon,\n\\]\ndove:\n\n\n\\(y\\): è il punteggio osservato dell’indicatore.\n\n\\(\\tau\\): è l’intercetta, che rappresenta la media stimata dell’indicatore quando il fattore latente è zero.\n\n\\(\\lambda\\): è il carico fattoriale, che misura la relazione tra il fattore latente e l’indicatore, ovvero quanto l’indicatore è influenzato dal fattore latente.\n\n\\(\\varepsilon\\): è l’errore di misura, che cattura la varianza dell’indicatore non spiegata dal fattore latente.\n\n\n40.2.1 Struttura delle Medie nel Modello CFA\nLa struttura delle medie in un modello CFA può essere espressa attraverso la seguente formula:\n\\[\n\\text{media(variabile osservata)} = \\Lambda \\mu_{\\text{lat}} + \\tau,\n\\]\ndove:\n\n\n\\(\\Lambda\\): è la matrice dei carichi fattoriali, che collega i fattori latenti agli indicatori.\n\n\\(\\mu_{\\text{lat}}\\): è il vettore delle medie dei fattori latenti.\n\n\\(\\tau\\): è il vettore delle intercette degli indicatori, che rappresenta le medie degli indicatori indipendentemente dai fattori latenti.\n\n40.2.2 Interpretazione\n\nCosa rappresenta \\(\\tau\\) concretamente?\nL’intercetta \\(\\tau\\) rappresenta il valore atteso di un indicatore quando il fattore latente associato ha un valore pari a zero. Per esempio, immagina un indicatore che misura la performance in un test. Se il fattore latente (ad esempio, “abilità generale”) è zero, \\(\\tau\\) indica la media attesa della performance in quella condizione specifica.\nPerché \\(\\tau\\) è importante?\\(\\tau\\) è fondamentale per interpretare il livello base dell’indicatore, consentendo di separare la varianza spiegata dai fattori latenti da quella attribuibile ad altre cause, come il livello medio dell’indicatore stesso. Questo è particolarmente utile per comprendere i punti di partenza dei partecipanti o i livelli medi degli indicatori in un contesto specifico.\n\n\n40.2.2.1 Esempio Intuitivo\nSupponiamo di analizzare i risultati di un test di matematica. Se il fattore latente rappresenta “abilità matematica” e il carico fattoriale \\(\\lambda\\) è elevato, ciò significa che i punteggi del test sono fortemente influenzati dall’abilità matematica. Tuttavia, \\(\\tau\\) fornisce un’informazione aggiuntiva: indica il punteggio medio nel test per chi ha un’abilità matematica pari a zero.\nSe i dati sono stati centrati, l’intercetta \\(\\tau\\) rappresenta la performance media prevista per i partecipanti con un’abilità matematica media rispetto al gruppo di riferimento.\n\n40.2.2.2 Importanza di \\(\\tau\\) nei Modelli SEM\nL’intercetta \\(\\tau\\) assume particolare rilievo nei modelli SEM applicati a:\n\n\nStudi longitudinali: Le variazioni di \\(\\tau\\) nel tempo possono indicare cambiamenti nei livelli medi degli indicatori, come miglioramenti o peggioramenti in una competenza specifica.\n\nConfronti tra gruppi: Differenze significative nelle intercette tra gruppi possono evidenziare disuguaglianze nei livelli medi di un indicatore, fornendo informazioni utili per analisi comparative.\n\nIn sintesi, l’intercetta \\(\\tau\\) è uno strumento chiave per comprendere e interpretare il comportamento degli indicatori nei modelli SEM, offrendo una visione chiara delle loro relazioni con i fattori latenti e delle differenze a livello di gruppo o temporale.\n\n40.2.3 Utilizzo delle Medie nel Software lavaan\n\nNel software lavaan, utilizzato per l’analisi SEM, è possibile stimare le intercette inserendo l’opzione meanstructure = TRUE nella sintassi del modello. Questo comando permette di includere automaticamente una costante “1” in tutte le equazioni del modello, facilitando così il calcolo delle intercette per le variabili endogene. È necessario fornire i dati originali o una matrice di covarianza, insieme alle medie di tutte le variabili interessate.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>La struttura delle medie</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/02_meanstructure.html#un-esempio-pratico",
    "href": "chapters/cfa/02_meanstructure.html#un-esempio-pratico",
    "title": "40  La struttura delle medie",
    "section": "\n40.3 Un Esempio Pratico",
    "text": "40.3 Un Esempio Pratico\nUtilizziamo il dataset HolzingerSwineford1939 per costruire un modello di misurazione basato su tre costrutti latenti:\n\n\nVisual (visual): rappresenta abilità visive, misurate dagli indicatori x1, x2 e x3.\n\nTextual (textual): rappresenta abilità testuali, misurate dagli indicatori x4, x5 e x6.\n\nSpeed (speed): rappresenta velocità di elaborazione, misurata dagli indicatori x7, x8 e x9.\n\nVisualizziamo una panoramica del dataset:\n\ndata(HolzingerSwineford1939)\nglimpse(HolzingerSwineford1939)\n#&gt; Rows: 301\n#&gt; Columns: 15\n#&gt; $ id     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, …\n#&gt; $ sex    &lt;int&gt; 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2,…\n#&gt; $ ageyr  &lt;int&gt; 13, 13, 13, 13, 12, 14, 12, 12, 13, 12, 12, 12, 12, 12, 12,…\n#&gt; $ agemo  &lt;int&gt; 1, 7, 1, 2, 2, 1, 1, 2, 0, 5, 2, 11, 7, 8, 6, 1, 11, 5, 8, …\n#&gt; $ school &lt;fct&gt; Pasteur, Pasteur, Pasteur, Pasteur, Pasteur, Pasteur, Paste…\n#&gt; $ grade  &lt;int&gt; 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,…\n#&gt; $ x1     &lt;dbl&gt; 3.33, 5.33, 4.50, 5.33, 4.83, 5.33, 2.83, 5.67, 4.50, 3.50,…\n#&gt; $ x2     &lt;dbl&gt; 7.75, 5.25, 5.25, 7.75, 4.75, 5.00, 6.00, 6.25, 5.75, 5.25,…\n#&gt; $ x3     &lt;dbl&gt; 0.375, 2.125, 1.875, 3.000, 0.875, 2.250, 1.000, 1.875, 1.5…\n#&gt; $ x4     &lt;dbl&gt; 2.33, 1.67, 1.00, 2.67, 2.67, 1.00, 3.33, 3.67, 2.67, 2.67,…\n#&gt; $ x5     &lt;dbl&gt; 5.75, 3.00, 1.75, 4.50, 4.00, 3.00, 6.00, 4.25, 5.75, 5.00,…\n#&gt; $ x6     &lt;dbl&gt; 1.286, 1.286, 0.429, 2.429, 2.571, 0.857, 2.857, 1.286, 2.7…\n#&gt; $ x7     &lt;dbl&gt; 3.39, 3.78, 3.26, 3.00, 3.70, 4.35, 4.70, 3.39, 4.52, 4.13,…\n#&gt; $ x8     &lt;dbl&gt; 5.75, 6.25, 3.90, 5.30, 6.30, 6.65, 6.20, 5.15, 4.65, 4.55,…\n#&gt; $ x9     &lt;dbl&gt; 6.36, 7.92, 4.42, 4.86, 5.92, 7.50, 4.86, 3.67, 7.36, 4.36,…\n\n\n40.3.1 Specifica del Modello\nOgni costrutto è definito in relazione ai propri indicatori. Le varianze dei costrutti latenti sono fissate a 1 per garantirne la scalatura, mentre le loro medie sono fissate a 0.\nEcco il modello specificato:\n\nhs_model &lt;- \"\n    visual =~ NA*x1 + x2 + x3\n    textual =~ NA*x4 + x5 + x6\n    speed =~ NA*x7 + x8 + x9\n\n    visual ~~ 1*visual\n    textual ~~ 1*textual\n    speed ~~ 1*speed\n\"\n\n\n40.3.2 Stima del Modello\nRichiediamo la stima delle intercette degli indicatori impostando meanstructure = TRUE. Le intercette (\\(\\tau\\)) rappresentano il valore medio atteso per ciascun indicatore quando il rispettivo fattore latente è pari a zero. Adattiamo il modello ai dati:\n\nfit &lt;- cfa(hs_model,\n    data = HolzingerSwineford1939,\n    meanstructure = TRUE\n)\n\nEsaminiamo un riepilogo dei risultati, inclusi i carichi fattoriali standardizzati:\n\nsummary(fit, standardized = TRUE)\n#&gt; lavaan 0.6-19 ended normally after 20 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        30\n#&gt; \n#&gt;   Number of observations                           301\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                85.306\n#&gt;   Degrees of freedom                                24\n#&gt;   P-value (Chi-square)                           0.000\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   visual =~                                                             \n#&gt;     x1                0.900    0.081   11.128    0.000    0.900    0.772\n#&gt;     x2                0.498    0.077    6.429    0.000    0.498    0.424\n#&gt;     x3                0.656    0.074    8.817    0.000    0.656    0.581\n#&gt;   textual =~                                                            \n#&gt;     x4                0.990    0.057   17.474    0.000    0.990    0.852\n#&gt;     x5                1.102    0.063   17.576    0.000    1.102    0.855\n#&gt;     x6                0.917    0.054   17.082    0.000    0.917    0.838\n#&gt;   speed =~                                                              \n#&gt;     x7                0.619    0.070    8.903    0.000    0.619    0.570\n#&gt;     x8                0.731    0.066   11.090    0.000    0.731    0.723\n#&gt;     x9                0.670    0.065   10.305    0.000    0.670    0.665\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   visual ~~                                                             \n#&gt;     textual           0.459    0.064    7.189    0.000    0.459    0.459\n#&gt;     speed             0.471    0.073    6.461    0.000    0.471    0.471\n#&gt;   textual ~~                                                            \n#&gt;     speed             0.283    0.069    4.117    0.000    0.283    0.283\n#&gt; \n#&gt; Intercepts:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .x1                4.936    0.067   73.473    0.000    4.936    4.235\n#&gt;    .x2                6.088    0.068   89.855    0.000    6.088    5.179\n#&gt;    .x3                2.250    0.065   34.579    0.000    2.250    1.993\n#&gt;    .x4                3.061    0.067   45.694    0.000    3.061    2.634\n#&gt;    .x5                4.341    0.074   58.452    0.000    4.341    3.369\n#&gt;    .x6                2.186    0.063   34.667    0.000    2.186    1.998\n#&gt;    .x7                4.186    0.063   66.766    0.000    4.186    3.848\n#&gt;    .x8                5.527    0.058   94.854    0.000    5.527    5.467\n#&gt;    .x9                5.374    0.058   92.546    0.000    5.374    5.334\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;     visual            1.000                               1.000    1.000\n#&gt;     textual           1.000                               1.000    1.000\n#&gt;     speed             1.000                               1.000    1.000\n#&gt;    .x1                0.549    0.114    4.833    0.000    0.549    0.404\n#&gt;    .x2                1.134    0.102   11.146    0.000    1.134    0.821\n#&gt;    .x3                0.844    0.091    9.317    0.000    0.844    0.662\n#&gt;    .x4                0.371    0.048    7.779    0.000    0.371    0.275\n#&gt;    .x5                0.446    0.058    7.642    0.000    0.446    0.269\n#&gt;    .x6                0.356    0.043    8.277    0.000    0.356    0.298\n#&gt;    .x7                0.799    0.081    9.823    0.000    0.799    0.676\n#&gt;    .x8                0.488    0.074    6.573    0.000    0.488    0.477\n#&gt;    .x9                0.566    0.071    8.003    0.000    0.566    0.558\n\n\n40.3.3 Interpretazione dei Risultati\nCarichi Fattoriali (\\(\\lambda\\))\nI carichi fattoriali indicano quanto fortemente un indicatore è associato al costrutto latente. Ad esempio, per visual, il carico fattoriale di x1 è 0.90, indicando una forte relazione tra il costrutto “Visual” e l’indicatore x1.\nIntercette (\\(\\tau\\))\nLe intercette rappresentano la media predetta degli indicatori quando il fattore latente associato è zero. Nel nostro modello, le medie dei costrutti latenti sono fissate a zero, quindi le intercette corrispondono alle medie predette degli indicatori.\n\n40.3.4 Calcolo delle Medie Osservate e Predette\nPer comprendere meglio il ruolo delle intercette, calcoliamo le medie osservate e predette per gli indicatori x1, x2 e x3.\nEstraiamo le intercette stimate dal modello:\n\nparams &lt;- parameterEstimates(fit)\nintercepts &lt;- params$est[params$op == \"~1\"]\nintercepts\n#&gt;  [1] 4.94 6.09 2.25 3.06 4.34 2.19 4.19 5.53 5.37 0.00 0.00 0.00\n\n\n40.3.5 Calcolo della Media Predetta\nLa media predetta di ciascun indicatore in un modello CFA è fornita direttamente dalle intercette stimate (\\(\\tau\\)). Per gli indicatori x1, x2, x3, estraiamo le intercette dal modello:\n\nmean_predicted_scores &lt;- mean(intercepts[1:3])  # Intercette predette per x1, x2, x3\nmean_predicted_scores\n#&gt; [1] 4.42\n\n\n40.3.6 Calcolo della Media Osservata\nLa media osservata per gli stessi indicatori si ottiene calcolando la media aritmetica dei loro punteggi effettivi, come segue:\n\nmean_observed_scores &lt;- mean(\n  (HolzingerSwineford1939$x1 + HolzingerSwineford1939$x2 + HolzingerSwineford1939$x3) / 3\n)\nmean_observed_scores\n#&gt; [1] 4.42\n\nSe il modello si adatta bene ai dati, le due medie dovrebbero essere molto vicine. Questo riflette l’adeguatezza del modello nel rappresentare i dati. In questo caso, i due valori coincidono o differiscono solo leggermente, confermando che il modello rappresenta fedelmente i dati osservati.\nIn sommario, questo esempio illustra come i carichi fattoriali e le intercette nel modello CFA siano utilizzati per stimare le medie predette degli indicatori. L’allineamento tra medie osservate e predette riflette l’adeguatezza del modello nella rappresentazione dei dati.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>La struttura delle medie</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/02_meanstructure.html#medie-di-costrutti-latenti-non-zero",
    "href": "chapters/cfa/02_meanstructure.html#medie-di-costrutti-latenti-non-zero",
    "title": "40  La struttura delle medie",
    "section": "\n40.4 Medie di Costrutti Latenti Non Zero",
    "text": "40.4 Medie di Costrutti Latenti Non Zero\nQuando le medie dei costrutti latenti (\\(\\mu_{\\text{latente}}\\)) non sono fissate a zero, la media predetta di ciascun indicatore dipende sia dalle intercette (\\(\\tau\\)) che dai carichi fattoriali (\\(\\lambda\\)). In questo caso, l’equazione per la media predetta di un indicatore, ad esempio x1, è:\n\\[\n\\text{media predetta}(x1) = \\mu_{\\text{latente}} \\cdot \\lambda_{x1} + \\tau_{x1},\n\\]\ndove:\n\n\n\\(\\mu_{\\text{latente}}\\): è la media stimata del costrutto latente associato.\n\n\\(\\lambda_{x1}\\): è il carico fattoriale dell’indicatore x1, che misura quanto fortemente il costrutto latente influenza l’indicatore.\n\n\\(\\tau_{x1}\\): è l’intercetta stimata dell’indicatore x1.\n\nQuesta relazione evidenzia che, quando \\(\\mu_{\\text{latente}} \\neq 0\\), la media degli indicatori riflette non solo la loro intercetta, ma anche il contributo del costrutto latente, modulato dai carichi fattoriali.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>La struttura delle medie</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/02_meanstructure.html#modello-con-medie-dei-costrutti-latenti-non-zero",
    "href": "chapters/cfa/02_meanstructure.html#modello-con-medie-dei-costrutti-latenti-non-zero",
    "title": "40  La struttura delle medie",
    "section": "\n40.5 Modello con Medie dei Costrutti Latenti Non Zero",
    "text": "40.5 Modello con Medie dei Costrutti Latenti Non Zero\nPer esplorare questa configurazione, costruiamo un modello SEM in cui le medie dei costrutti latenti sono stimate liberamente. Per garantire l’identificabilità del modello, introduciamo due vincoli fondamentali:\n\n\nIntercette degli indicatori marker fissate a zero: Le variabili osservate x1, x4 e x7, che sono gli indicatori marker per ciascun fattore, avranno intercette impostate a zero.\n\nUna media latente fissata a zero: La media del costrutto latente visual è fissata a zero come riferimento, mentre le medie di textual e speed sono stimate liberamente.\n\n\n40.5.1 Specificazione del Modello\nIl modello viene specificatocome segue:\n\nhs_model &lt;- \"\n    # Definizione dei fattori latenti\n    visual =~ NA*x1 + x2 + x3\n    textual =~ NA*x4 + x5 + x6\n    speed =~ NA*x7 + x8 + x9\n\n    # Standardizzazione delle varianze latenti\n    visual ~~ 1*visual\n    textual ~~ 1*textual\n    speed ~~ 1*speed\n\n    # Vincoli sulle intercette degli indicatori marker\n    x1 ~ 0*1  # Intercetta di x1 fissata a zero\n    x4 ~ 0*1  # Intercetta di x4 fissata a zero\n    x7 ~ 0*1  # Intercetta di x7 fissata a zero\n\n    # Vincoli sulle medie latenti\n    visual ~ 0*1  # Media di visual fissata a zero\n    textual ~ 1   # Media di textual stimata liberamente\n    speed ~ 1     # Media di speed stimata liberamente\n\"\n\n\n40.5.2 Interpretazione delle Medie Predette\nLa caratteristica chiave di questo modello è che le medie predette degli indicatori sono una funzione di tre componenti:\n\nLa media del costrutto latente (\\(\\mu_{\\text{latente}}\\))\nIl carico fattoriale dell’indicatore (\\(\\lambda\\))\nL’intercetta dell’indicatore (\\(\\tau\\))\n\nQuesta relazione è espressa dalla formula:\n\\[\n\\text{media predetta(indicatore)} = \\mu_{\\text{latente}} \\cdot \\lambda + \\tau\n\\]\nPer esempio:\n\nPer l’indicatore marker x1 (fattore visual): \\[\n\\text{media predetta}(x1) = \\mu_{\\text{visual}} \\cdot \\lambda_{x1} + \\tau_{x1} = 0 \\cdot \\lambda_{x1} + 0 = 0\n\\]\nPer l’indicatore marker x4 (fattore textual): \\[\n\\text{media predetta}(x4) = \\mu_{\\text{textual}} \\cdot \\lambda_{x4} + \\tau_{x4}\n\\] dove \\(\\mu_{\\text{textual}}\\) è stimato liberamente e \\(\\tau_{x4} = 0\\)\n\nPer gli altri indicatori (es. x2, x3, x5, x6, x8, x9):\n\nLe intercette sono stimate liberamente\nLe medie predette includono sia il contributo della media latente che dell’intercetta\n\n\n\nQuesta struttura delle medie ci permette di:\n\nconfrontare le medie dei costrutti latenti tra gruppi diversi;\nvalutare i cambiamenti longitudinali nei costrutti latenti;\ninterpretare le differenze nelle medie degli indicatori in termini dei loro componenti strutturali.\n\n40.5.3 Adattamento del Modello\nApplichiamo il modello ai dati e esaminiamo i risultati:\n\nfit &lt;- cfa(hs_model, data = HolzingerSwineford1939, meanstructure = TRUE)\nparams &lt;- parameterEstimates(fit)\nparams\n#&gt;        lhs op     rhs    est    se      z pvalue ci.lower ci.upper\n#&gt; 1   visual =~      x1  4.983 0.212  23.54  0.000    4.568    5.398\n#&gt; 2   visual =~      x2  1.702 0.093  18.25  0.000    1.519    1.885\n#&gt; 3   visual =~      x3  2.243 0.106  21.10  0.000    2.035    2.451\n#&gt; 4  textual =~      x4  1.783 0.081  21.95  0.000    1.624    1.942\n#&gt; 5  textual =~      x5  1.985 0.090  22.01  0.000    1.808    2.162\n#&gt; 6  textual =~      x6  1.652 0.076  21.70  0.000    1.502    1.801\n#&gt; 7    speed =~      x7  1.136 0.072  15.88  0.000    0.996    1.277\n#&gt; 8    speed =~      x8  1.341 0.070  19.13  0.000    1.204    1.478\n#&gt; 9    speed =~      x9  1.229 0.068  17.98  0.000    1.095    1.363\n#&gt; 10  visual ~~  visual  1.000 0.000     NA     NA    1.000    1.000\n#&gt; 11 textual ~~ textual  1.000 0.000     NA     NA    1.000    1.000\n#&gt; 12   speed ~~   speed  1.000 0.000     NA     NA    1.000    1.000\n#&gt; 13      x1 ~1          0.000 0.000     NA     NA    0.000    0.000\n#&gt; 14      x4 ~1          0.000 0.000     NA     NA    0.000    0.000\n#&gt; 15      x7 ~1          0.000 0.000     NA     NA    0.000    0.000\n#&gt; 16  visual ~1          0.000 0.000     NA     NA    0.000    0.000\n#&gt; 17 textual ~1          0.885 0.054  16.41  0.000    0.779    0.990\n#&gt; 18   speed ~1          2.845 0.187  15.21  0.000    2.478    3.211\n#&gt; 19      x1 ~~      x1  0.890 0.257   3.46  0.001    0.385    1.394\n#&gt; 20      x2 ~~      x2  1.134 0.100  11.37  0.000    0.938    1.329\n#&gt; 21      x3 ~~      x3  0.844 0.087   9.72  0.000    0.674    1.015\n#&gt; 22      x4 ~~      x4  0.371 0.045   8.24  0.000    0.283    0.459\n#&gt; 23      x5 ~~      x5  0.446 0.055   8.12  0.000    0.338    0.554\n#&gt; 24      x6 ~~      x6  0.356 0.041   8.68  0.000    0.276    0.437\n#&gt; 25      x7 ~~      x7  0.799 0.077  10.41  0.000    0.649    0.950\n#&gt; 26      x8 ~~      x8  0.488 0.062   7.92  0.000    0.367    0.608\n#&gt; 27      x9 ~~      x9  0.566 0.062   9.13  0.000    0.445    0.688\n#&gt; 28  visual ~~ textual  0.870 0.016  53.41  0.000    0.838    0.902\n#&gt; 29  visual ~~   speed  0.877 0.019  47.02  0.000    0.840    0.913\n#&gt; 30 textual ~~   speed  0.783 0.027  28.49  0.000    0.729    0.837\n#&gt; 31      x2 ~1          4.460 0.064  69.66  0.000    4.335    4.586\n#&gt; 32      x3 ~1          0.106 0.058   1.81  0.070   -0.008    0.220\n#&gt; 33      x5 ~1          0.934 0.075  12.48  0.000    0.787    1.080\n#&gt; 34      x6 ~1         -0.649 0.064 -10.09  0.000   -0.775   -0.523\n#&gt; 35      x8 ~1          0.588 0.236   2.50  0.013    0.126    1.050\n#&gt; 36      x9 ~1          0.847 0.226   3.74  0.000    0.403    1.291\n\n\n40.5.4 Interpretazione dei Risultati\n\nMedie Latenti (\\(\\mu_{\\text{latente}}\\))\nLe medie stimate per i costrutti textual e speed riflettono il loro contributo alle medie predette degli indicatori associati. Per visual, la media latente è fissata a zero per identificare il modello.\nMedia Predetta degli Indicatori\nPer ciascun indicatore, la media predetta include sia l’intercetta (fissata a zero in questo esempio) sia il contributo del costrutto latente moltiplicato per il carico fattoriale.\n\nAd esempio, per x1:\n\\[\n\\text{media predetta}(x1) = \\mu_{\\text{visual}} \\cdot \\lambda_{x1} + \\tau_{x1} = 0 \\cdot \\lambda_{x1} + 0 = 0\n\\]\nMentre per un indicatore di textual (ad esempio x4), la media predetta sarà:\n\\[\n\\text{media predetta}(x4) = \\mu_{\\text{textual}} \\cdot \\lambda_{x4} + \\tau_{x4}.\n\\]\n\n40.5.5 Calcolo delle Medie Predette\nPer calcolare le medie predette:\n\n# Funzione per calcolare le medie predette\ncalc_predicted_means &lt;- function(params) {\n  # Estrai i carichi fattoriali (lambda)\n  lambdas &lt;- params[params$op == \"=~\", c(\"lhs\", \"rhs\", \"est\")]\n  \n  # Estrai le medie latenti (mu)\n  means &lt;- params[params$op == \"~1\" & params$lhs %in% c(\"visual\", \"textual\", \"speed\"), \n                 c(\"lhs\", \"est\")]\n  \n  # Estrai le intercette (tau)\n  intercepts &lt;- params[params$op == \"~1\" & params$lhs %in% paste0(\"x\", 1:9), \n                      c(\"lhs\", \"est\")]\n  \n  # Calcola le medie predette\n  predicted_means &lt;- data.frame(\n    indicator = character(),\n    predicted_mean = numeric(),\n    formula = character(),\n    stringsAsFactors = FALSE\n  )\n  \n  # Per ogni indicatore\n  for(i in 1:9) {\n    indicator &lt;- paste0(\"x\", i)\n    # Trova il fattore latente associato\n    factor &lt;- lambdas$lhs[lambdas$rhs == indicator]\n    lambda &lt;- lambdas$est[lambdas$rhs == indicator]\n    mu &lt;- means$est[means$lhs == factor]\n    tau &lt;- intercepts$est[intercepts$lhs == indicator]\n    \n    # Se l'intercetta non è presente nei risultati, assume 0\n    if(length(tau) == 0) tau &lt;- 0\n    \n    # Calcola la media predetta\n    pred_mean &lt;- mu * lambda + tau\n    \n    # Crea la formula come stringa\n    formula &lt;- sprintf(\"%.3f * %.3f + %.3f = %.3f\", \n                      mu, lambda, tau, pred_mean)\n    \n    # Aggiungi alla tabella dei risultati\n    predicted_means &lt;- rbind(predicted_means,\n                           data.frame(\n                             indicator = indicator,\n                             predicted_mean = pred_mean,\n                             formula = formula,\n                             stringsAsFactors = FALSE\n                           ))\n  }\n  \n  return(predicted_means)\n}\n\n\n# Calcola e mostra le medie predette\nresults &lt;- calc_predicted_means(params)\nprint(results)\n#&gt;   indicator predicted_mean                        formula\n#&gt; 1        x1          0.000  0.000 * 4.983 + 0.000 = 0.000\n#&gt; 2        x2          4.460  0.000 * 1.702 + 4.460 = 4.460\n#&gt; 3        x3          0.106  0.000 * 2.243 + 0.106 = 0.106\n#&gt; 4        x4          1.578  0.885 * 1.783 + 0.000 = 1.578\n#&gt; 5        x5          2.689  0.885 * 1.985 + 0.934 = 2.689\n#&gt; 6        x6          0.812 0.885 * 1.652 + -0.649 = 0.812\n#&gt; 7        x7          3.233  2.845 * 1.136 + 0.000 = 3.233\n#&gt; 8        x8          4.403  2.845 * 1.341 + 0.588 = 4.403\n#&gt; 9        x9          4.344  2.845 * 1.229 + 0.847 = 4.344\n\nLe medie latenti riflettono il contributo di ciascun costrutto alla media predetta dei rispettivi indicatori. La formula \\(\\hat{Y} = \\mu \\cdot \\lambda + \\tau\\) ci consente di distinguere i diversi contributi strutturali alle medie osservate.\nIn sintesi, quando le medie dei costrutti latenti sono diverse da zero, il calcolo delle medie predette degli indicatori diventa più complesso, poiché include il contributo sia delle intercette sia dei carichi fattoriali ponderati dalle medie latenti. Questo approccio è particolarmente utile in contesti in cui è necessario confrontare medie tra gruppi o valutare cambiamenti longitudinali nei costrutti latenti.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>La struttura delle medie</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/02_meanstructure.html#riflessioni-conclusive",
    "href": "chapters/cfa/02_meanstructure.html#riflessioni-conclusive",
    "title": "40  La struttura delle medie",
    "section": "\n40.6 Riflessioni Conclusive",
    "text": "40.6 Riflessioni Conclusive\nIn questo capitolo abbiamo approfondito come l’inclusione delle medie nei modelli CFA consenta di ampliare la comprensione delle relazioni tra variabili, andando oltre le covarianze. Abbiamo evidenziato il ruolo cruciale delle intercette, che rappresentano il valore medio atteso degli indicatori quando i fattori latenti assumono valore zero, e come esse contribuiscano alla struttura delle medie predette degli indicatori. Infine, abbiamo esplorato l’utilità dei modelli con medie dei costrutti latenti non zero, che permettono di analizzare differenze tra gruppi e cambiamenti longitudinali, fornendo un quadro interpretativo più ricco e completo. Questo approccio è particolarmente rilevante per indagini che mirano a comprendere i livelli medi dei costrutti in relazione ai loro indicatori, sia in contesti trasversali che longitudinali.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>La struttura delle medie</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/02_meanstructure.html#session-info",
    "href": "chapters/cfa/02_meanstructure.html#session-info",
    "title": "40  La struttura delle medie",
    "section": "\n40.7 Session Info",
    "text": "40.7 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] ggokabeito_0.1.0  see_0.10.0        MASS_7.3-65       viridis_0.6.5    \n#&gt;  [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n#&gt;  [9] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n#&gt; [13] lavaan_0.6-19     psych_2.4.12      scales_1.3.0      markdown_1.13    \n#&gt; [17] knitr_1.49        lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n#&gt; [21] dplyr_1.1.4       purrr_1.0.4       readr_2.1.5       tidyr_1.3.1      \n#&gt; [25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.0      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] mime_0.12           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [25] Matrix_1.7-2        R6_2.6.1            fastmap_1.2.0      \n#&gt;  [28] rbibutils_2.3       shiny_1.10.0        digest_0.6.37      \n#&gt;  [31] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n#&gt;  [34] rprojroot_2.0.4     Hmisc_5.2-2         timechange_0.3.0   \n#&gt;  [37] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n#&gt;  [40] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n#&gt;  [43] carData_3.0-5       ggsignif_0.6.4      corpcor_1.6.10     \n#&gt;  [46] gtools_3.9.5        tools_4.4.2         pbivnorm_0.6.0     \n#&gt;  [49] foreign_0.8-88      zip_2.3.2           httpuv_1.6.15      \n#&gt;  [52] nnet_7.3-20         glue_1.8.0          quadprog_1.5-8     \n#&gt;  [55] nlme_3.1-167        promises_1.3.2      lisrelToR_0.3      \n#&gt;  [58] grid_4.4.2          checkmate_2.3.2     cluster_2.1.8      \n#&gt;  [61] reshape2_1.4.4      generics_0.1.3      gtable_0.3.6       \n#&gt;  [64] tzdb_0.4.0          data.table_1.17.0   hms_1.1.3          \n#&gt;  [67] car_3.1-3           sem_3.1-16          pillar_1.10.1      \n#&gt;  [70] rockchalk_1.8.157   later_1.4.1         splines_4.4.2      \n#&gt;  [73] lattice_0.22-6      survival_3.8-3      kutils_1.73        \n#&gt;  [76] tidyselect_1.2.1    miniUI_0.1.1.1      pbapply_1.7-2      \n#&gt;  [79] reformulas_0.4.0    stats4_4.4.2        xfun_0.51          \n#&gt;  [82] qgraph_1.9.8        arm_1.14-4          stringi_1.8.4      \n#&gt;  [85] yaml_2.3.10         pacman_0.5.1        boot_1.3-31        \n#&gt;  [88] evaluate_1.0.3      codetools_0.2-20    mi_1.1             \n#&gt;  [91] cli_3.6.4           RcppParallel_5.1.10 rpart_4.1.24       \n#&gt;  [94] xtable_1.8-4        Rdpack_2.6.2        munsell_0.5.1      \n#&gt;  [97] Rcpp_1.0.14         coda_0.19-4.1       png_0.1-8          \n#&gt; [100] XML_3.99-0.18       parallel_4.4.2      jpeg_0.1-10        \n#&gt; [103] lme4_1.1-36         mvtnorm_1.3-3       openxlsx_4.2.8     \n#&gt; [106] rlang_1.1.5         multcomp_1.4-28     mnormt_2.1.1\n\n\n\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>La struttura delle medie</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/03_cat_data.html",
    "href": "chapters/cfa/03_cat_data.html",
    "title": "41  Dati non gaussiani e categoriali",
    "section": "",
    "text": "41.1 Introduzione\nIn questa dispensa abbiamo già discusso l’uso dello stimatore di massima verosimiglianza (ML), largamente utilizzato nell’Analisi Fattoriale Confermativa (CFA) e nei modelli di Structural Equation Modeling (SEM). Questo metodo funziona bene quando i dati sono normalmente distribuiti, ovvero quando la distribuzione delle variabili è simmetrica e segue la classica “curva a campana”.\nTuttavia, se i dati si discostano molto dalla normalità, per esempio con una forte asimmetria o curtosi (distribuzioni molto appuntite o schiacciate), oppure se le variabili non sono su una scala numerica continua (ad esempio, dati binari o ordinali), lo stimatore ML potrebbe non essere adeguato. In questi casi, è meglio usare stimatori alternativi.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Dati non gaussiani e categoriali</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/03_cat_data.html#dati-non-normali-e-alternative-a-ml",
    "href": "chapters/cfa/03_cat_data.html#dati-non-normali-e-alternative-a-ml",
    "title": "41  Dati non gaussiani e categoriali",
    "section": "\n41.2 Dati non normali e alternative a ML",
    "text": "41.2 Dati non normali e alternative a ML\nAnche se ML è robusto a piccole deviazioni dalla normalità, in situazioni di marcata non normalità possono verificarsi i seguenti problemi:\n\n\nSovrastima della statistica chi-quadrato (\\(\\chi^2\\)): i test per valutare l’adattamento del modello ai dati possono risultare troppo severi.\n\nSottostima degli indici di bontà di adattamento: valori come il Tucker-Lewis Index (TLI) e il Comparative Fit Index (CFI) possono sembrare peggiori del reale.\n\nErrori standard sottostimati: le stime dei parametri del modello possono sembrare più precise di quanto non siano.\n\nQuesti problemi diventano più gravi con campioni piccoli. Per evitarli, si possono utilizzare i seguenti stimatori:\n\n41.2.1 1. GLS (Generalized Least Squares)\n\n\n\nQuando usarlo: Adatto se non ci sono dati mancanti.\n\nCome funziona: Valuta quanto la matrice di covarianza stimata dal modello differisce dalla matrice osservata. Più piccola è questa differenza, meglio il modello si adatta ai dati.\n\nInterpretazione: Un valore basso della funzione GLS indica un buon adattamento.\n\n41.2.2 2. WLS (Weighted Least Squares)\n\n\n\nQuando usarlo: Ideale per dati non normali o complessi (detto anche stimatore ADF - Asintoticamente Libero da Distribuzione).\n\nCome funziona: Pesa ogni elemento della matrice di covarianza in modo da considerare l’importanza relativa delle discrepanze.\n\nInterpretazione: Un valore basso indica che il modello si adatta bene, tenendo conto di questi pesi.\n\n41.2.3 3. DWLS (Diagonally Weighted Least Squares)\n\n\n\nQuando usarlo: Una versione semplificata di WLS.\n\nCome funziona: Utilizza pesi solo sugli elementi della diagonale della matrice di covarianza, semplificando i calcoli.\n\nInterpretazione: È più semplice di WLS, ma funziona bene per dati ordinali o binari.\n\n41.2.4 4. ULS (Unweighted Least Squares)\n\n\n\nQuando usarlo: Adatto a situazioni meno complesse.\n\nCome funziona: Tutti gli elementi della matrice hanno lo stesso peso.\n\nInterpretazione: È il metodo più semplice, ma meno sofisticato rispetto agli altri.\n\n41.2.5 ML Robusto: un compromesso per dati non normali\nOltre agli stimatori sopra descritti, esiste una variante di ML chiamata ML Robusto (Robust Maximum Likelihood). Questo metodo è pensato per gestire situazioni di forte non normalità:\n\n\nCorregge la statistica \\(\\chi^2\\): Evita che i risultati sembrino peggiori di quanto siano.\n\nErrori standard più precisi: Migliora la stima della precisione dei parametri.\n\nIndici di adattamento più affidabili: TLI e CFI risultano più accurati.\n\n\nIn sintesi, quando i dati non rispettano le condizioni di normalità, l’uso di stimatori come WLS, DWLS o ML Robusto può garantire risultati più affidabili. Questi metodi considerano le caratteristiche specifiche dei dati, come la distribuzione o la scala, e permettono di valutare meglio l’adattamento del modello. La scelta dello stimatore dipende dal tipo di dati e dal livello di complessità richiesto dall’analisi.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Dati non gaussiani e categoriali</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/03_cat_data.html#un-esempio-concreto",
    "href": "chapters/cfa/03_cat_data.html#un-esempio-concreto",
    "title": "41  Dati non gaussiani e categoriali",
    "section": "\n41.3 Un Esempio Concreto",
    "text": "41.3 Un Esempio Concreto\nPrendiamo in considerazione un caso pratico. Utilizzeremo i dati sintetici presentati da Brown (2015) nelle tabelle 9.5-9.7 come esempio.\n\nd &lt;- readRDS(here::here(\"data\", \"brown_table_9_5_data.RDS\"))\nhead(d)\n#&gt;   x1 x2 x3 x4 x5\n#&gt; 1  0  0  0  0  0\n#&gt; 2  0  0  0  0  0\n#&gt; 3  0  0  0  0  0\n#&gt; 4  4  2  2  1  1\n#&gt; 5  1  0  1  6  0\n#&gt; 6  0  0  0  0  0\n\nLe statistiche descrittive di questo campione di dati mostrano valori eccessivi di asimmetria e di curtosi.\n\npsych::describe(d)\n#&gt;    vars   n mean   sd median trimmed mad min max range skew kurtosis   se\n#&gt; x1    1 870 1.47 2.17      0    1.01   0   0   8     8 1.51     1.25 0.07\n#&gt; x2    2 870 0.82 1.60      0    0.42   0   0   8     8 2.40     5.67 0.05\n#&gt; x3    3 870 1.27 2.07      0    0.78   0   0   8     8 1.80     2.34 0.07\n#&gt; x4    4 870 1.03 1.93      0    0.54   0   0   8     8 2.16     3.98 0.07\n#&gt; x5    5 870 0.61 1.52      0    0.18   0   0   8     8 3.10     9.37 0.05\n\nDefiniamo un modello ad un fattore e, seguendo Brown (2015), aggiungiamo una correlazione residua tra gli indicatori X1 e X3:\n\nmodel &lt;- '\n  f1 =~ x1 + x2 + x3 + x4 + x5\n  x1 ~~ x3 \n'\n\nProcediamo alla stima dei parametri utilizzando uno stimatore di ML robusto. La sintassi lavaan è la seguente:\n\nfit &lt;- cfa(\n  model, \n  data = d, \n  mimic = \"MPLUS\", \n  estimator = \"MLM\"\n)\n\nPer esaminare la soluzione ottenuta ci focalizziamo sulla statistica \\(\\chi^2\\) – si consideri la soluzione robusta fornita nell’output.\n\nsummary(fit)\n#&gt; lavaan 0.6-19 ended normally after 28 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        16\n#&gt; \n#&gt;   Number of observations                           870\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                Standard      Scaled\n#&gt;   Test Statistic                                 25.913      10.356\n#&gt;   Degrees of freedom                                  4           4\n#&gt;   P-value (Chi-square)                            0.000       0.035\n#&gt;   Scaling correction factor                                   2.502\n#&gt;     Satorra-Bentler correction (Mplus variant)                     \n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                           Robust.sem\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;   f1 =~                                               \n#&gt;     x1                1.000                           \n#&gt;     x2                0.703    0.062   11.338    0.000\n#&gt;     x3                1.068    0.044   24.304    0.000\n#&gt;     x4                0.918    0.063   14.638    0.000\n#&gt;     x5                0.748    0.055   13.582    0.000\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;  .x1 ~~                                               \n#&gt;    .x3                0.655    0.143    4.579    0.000\n#&gt; \n#&gt; Intercepts:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;    .x1                1.470    0.074   19.968    0.000\n#&gt;    .x2                0.823    0.054   15.166    0.000\n#&gt;    .x3                1.266    0.070   18.043    0.000\n#&gt;    .x4                1.026    0.065   15.712    0.000\n#&gt;    .x5                0.607    0.051   11.790    0.000\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;    .x1                2.040    0.228    8.952    0.000\n#&gt;    .x2                1.241    0.124   10.019    0.000\n#&gt;    .x3                1.227    0.169    7.255    0.000\n#&gt;    .x4                1.458    0.177    8.233    0.000\n#&gt;    .x5                0.807    0.100    8.063    0.000\n#&gt;     f1                2.675    0.289    9.273    0.000\n\nPer fare un confronto, adattiamo lo stesso modello ai dati usando lo stimatore di ML.\n\nfit2 &lt;- cfa(model, data = d)\n\nNotiamo come il valore della statistica \\(\\chi^2\\) ora ottenuto sia molto maggiore di quello trovato in precedenza.\n\nsummary(fit2)\n#&gt; lavaan 0.6-19 ended normally after 28 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        11\n#&gt; \n#&gt;   Number of observations                           870\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                25.913\n#&gt;   Degrees of freedom                                 4\n#&gt;   P-value (Chi-square)                           0.000\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;   f1 =~                                               \n#&gt;     x1                1.000                           \n#&gt;     x2                0.703    0.035   20.133    0.000\n#&gt;     x3                1.068    0.034   31.730    0.000\n#&gt;     x4                0.918    0.042   21.775    0.000\n#&gt;     x5                0.748    0.033   22.416    0.000\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;  .x1 ~~                                               \n#&gt;    .x3                0.655    0.091    7.213    0.000\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;    .x1                2.040    0.128   15.897    0.000\n#&gt;    .x2                1.241    0.070   17.671    0.000\n#&gt;    .x3                1.227    0.095   12.942    0.000\n#&gt;    .x4                1.458    0.090   16.135    0.000\n#&gt;    .x5                0.807    0.053   15.119    0.000\n#&gt;     f1                2.675    0.220   12.154    0.000\n\n\n41.3.1 Dati Categoriali\nNella discussione precedente, abbiamo esaminato il modello CFA presupponendo che i dati fossero continui e normalmente distribuiti in maniera multivariata. Tuttavia, abbiamo anche trattato la stima robusta per dati non normalmente distribuiti. Ora, è fondamentale riconoscere che molti dei dati utilizzati nelle analisi fattoriali confermative (CFA) o SEM provengono da questionari e scale di tipo Likert, che producono dati categoriali, inclusi formati binari, ordinali e nominali. Questi dati sono di natura ordinale e non sono continui.\nL’uso del metodo di massima verosimiglianza (ML) ordinario non è raccomandato quando si analizzano dati con almeno un indicatore categoriale. Trattare tali variabili come se fossero continue può portare a varie conseguenze indesiderate, tra cui:\n\n\nStime Attenuate delle Relazioni: Le relazioni tra gli indicatori possono risultare attenuate, specialmente se influenzate da effetti di pavimento o soffitto.\n\nEmergenza di “Pseudo-Fattori”: La possibilità di identificare falsi fattori, che non rappresentano veri costrutti ma sono piuttosto artefatti del metodo statistico utilizzato.\n\nDistorsione degli Indici di Bontà di Adattamento e delle Stime degli Errori Standard: Questi indici, che valutano la qualità dell’adattamento del modello, possono essere distorti, così come le stime degli errori standard.\n\nStime Errate dei Parametri: I parametri del modello potrebbero essere stimati in modo inaccurato.\n\nPer mitigare questi problemi, esistono stimatori specifici per i dati categoriali, tra cui:\n\n\nWLS (Weighted Least Squares): Adatto per dati categoriali, considera il peso specifico di ciascuna osservazione.\n\nWLSMV (Weighted Least Squares Mean and Variance Adjusted): Una versione modificata di WLS che si adatta meglio alle peculiarità dei dati categoriali.\n\nULS (Unweighted Least Squares): Questo stimatore non prevede ponderazioni e può essere utile per dati categoriali senza presupporre pesi specifici.\n\nNelle sezioni seguenti, approfondiremo l’approccio CFA per dati categoriali, evidenziando le specificità e le migliori pratiche per gestire questo tipo di dati nelle analisi CFA. Questo ci permetterà di effettuare inferenze più accurate, preservando l’integrità e la validità delle conclusioni derivanti dalle analisi.\n\n41.3.2 Un esempio concreto\nNell’esempio discusso da Brown (2015), i ricercatori desiderano verificare un modello uni-fattoriale di dipendenza da alcol in un campione di 750 pazienti ambulatoriali. Gli indicatori di alcolismo sono item binari che riflettono la presenza/assenza di sei criteri diagnostici per l’alcolismo (0 = criterio non soddisfatto, 1 = criterio soddisfatto). I dati sono i seguenti:\n\nd1 &lt;- readRDS(here::here(\"data\", \"brown_table_9_9_data.RDS\"))\nhead(d1)\n#&gt;   y1 y2 y3 y4 y5 y6\n#&gt; 1  1  1  1  1  1  1\n#&gt; 2  1  1  1  1  1  1\n#&gt; 3  1  1  1  1  1  0\n#&gt; 4  1  1  1  1  1  1\n#&gt; 5  0  0  0  0  0  0\n#&gt; 6  1  1  0  1  1  1\n\nÈ possibile evidenziare la natura ordinale dei dati esaminando le tabelle bivariate che mostrano la frequenza di combinazioni specifiche tra due variabili.\n\nxtabs(~ y1 + y2, d1)\n#&gt;    y2\n#&gt; y1    0   1\n#&gt;   0 103  65\n#&gt;   1 156 426\n\n\nxtabs(~ y3 + y4, d1)\n#&gt;    y4\n#&gt; y3    0   1\n#&gt;   0  41  39\n#&gt;   1 119 551\n\n\nxtabs(~ y5 + y6, d1)\n#&gt;    y6\n#&gt; y5    0   1\n#&gt;   0  95 168\n#&gt;   1  60 427\n\nNelle tabelle precedenti, si osserva una maggiore frequenza di casi in cui entrambe le variabili assumono il valore 1, rispetto ai casi in cui entrambe sono 0 o in cui una è 1 e l’altra è 0. Questo suggerisce l’esistenza di una relazione ordinale tra le coppie di variabili nel dataset.\n\n41.3.3 Il Modello Basato sulle Soglie per Risposte Categoriali Ordinate\nIl modello basato sulle soglie per risposte categoriali ordinate si basa sull’idea che ogni risposta di una variabile categoriale possa essere vista come il risultato di una variabile continua non osservata, che è normalmente distribuita. Questa variabile nascosta, chiamata variabile latente, rappresenta la tendenza di una persona a rispondere in un determinato modo. Le risposte che vediamo, classificate in categorie, sono in realtà approssimazioni di questa variabile latente.\nImmaginiamo di utilizzare un questionario dove le risposte sono su una scala Likert a 7 punti. Questo crea una variabile categoriale con sette categorie ordinate. Se denotiamo con I un particolare item del questionario e con I* la sua corrispondente variabile latente non osservabile, possiamo descrivere il loro legame attraverso le seguenti equazioni, che mappano la variabile latente alle risposte osservabili:\n\\[\n\\begin{align*}\nI &= 1 \\quad \\text{se} \\quad -\\infty &lt; I^* \\leq t_1 \\\\\nI &= 2 \\quad \\text{se} \\quad t_1 &lt; I^* \\leq t_2 \\\\\nI &= 3 \\quad \\text{se} \\quad t_2 &lt; I^* \\leq t_3 \\\\\nI &= 4 \\quad \\text{se} \\quad t_3 &lt; I^* \\leq t_4 \\\\\nI &= 5 \\quad \\text{se} \\quad t_4 &lt; I^* \\leq t_5 \\\\\nI &= 6 \\quad \\text{se} \\quad t_5 &lt; I^* \\leq t_6 \\\\\nI &= 7 \\quad \\text{se} \\quad t_6 &lt; I^* &lt; \\infty\n\\end{align*}\n\\]\nIn queste equazioni, \\(t_i\\) (con i da 1 a 6) rappresenta le soglie che dividono l’intero spettro della variabile latente in sette categorie. Le soglie sono disposte in modo che \\(-\\infty &lt; t_1 &lt; t_2 &lt; t_3 &lt; t_4 &lt; t_5 &lt; t_6 &lt; \\infty\\). È importante notare che il numero di soglie è sempre uno in meno rispetto al numero di categorie, un po’ come il numero di variabili dummy usate nell’analisi di regressione per codificare una variabile categoriale.\nQuesto processo di categorizzazione può essere visualizzato come segue: si immagini una curva normale che rappresenta la distribuzione della variabile latente \\(I*\\). Le sei linee verticali nella figura rappresentano le soglie \\(t_1\\) a \\(t_6\\). Le risposte possibili vanno da I = 1 a I = 7, e la categoria specifica (I) dipende dall’intervallo, definito dalle soglie, in cui il valore di I* si trova.\n\n# Definire le soglie\nthresholds &lt;- c(-3, -2, -1, 0, 1, 2, 3)\n\n# Creare un dataframe per la curva normale\nx_values &lt;- seq(-4, 4, length.out = 300)\ny_values &lt;- dnorm(x_values)\ncurve_data &lt;- data.frame(x = x_values, y = y_values)\n\n# Creare il plot\nggplot(curve_data, aes(x = x, y = y)) +\n    geom_line() +\n    geom_vline(xintercept = thresholds, col = \"red\") +\n    scale_y_continuous(breaks = NULL) +\n    scale_x_continuous(\n      breaks = thresholds, labels = c(\"t1\", \"t2\", \"t3\", \"t4\", \"t5\", \"t6\", \"t7\")\n    ) +\n    labs(\n        title = \"Categorization of Latent Continuous Variable to Categorical Variable\",\n        x = \"Latent Continuous Variable I*\",\n        y = \"\"\n    ) \n\n\n\n\n\n\n\nLa conversione della variabile latente \\(I^*\\) in dati su una scala Likert comporta inevitabilmente degli errori di misurazione e campionamento. Come evidenziato da O’Brien (1985), questo processo di categorizzazione introduce due tipi principali di errore:\n\nErrore di categorizzazione: Questo errore deriva dalla segmentazione di una scala continua in una scala categoriale, dove la variabile latente viene divisa in categorie distinte.\nErrore di trasformazione: Questo errore emerge quando le categorie hanno larghezze disuguali, influenzando la fedeltà della rappresentazione delle misure originali della variabile latente.\n\nDi conseguenza, è fondamentale che le soglie siano stimate contemporaneamente agli altri parametri nel modello di equazioni strutturali per garantire che tali errori siano minimizzati e che l’analisi rifletta accuratamente la realtà sottostante.\n\n41.3.4 Modellazione di Variabili Categoriali nei Modelli CFA\nNell’ambito dei modelli CFA, le variabili categoriali ordinate vengono spesso modellate collegandole a una variabile latente sottostante, denominata \\(I^*\\). Questa variabile latente rappresenta una sorta di “propensione nascosta” che influisce sulle risposte osservate nelle variabili categoriali.\nPer esemplificare, consideriamo il seguente modello che esprime la variabile latente \\(I^*\\) attraverso una serie di predittori (x1, x2, …, xp), ognuno dei quali contribuisce all’esito con un effetto quantificato dai coefficienti \\(\\beta_1, \\beta_2, ..., \\beta_P\\):\n\\[\nI^*_i = \\beta_0 + \\beta_1 x_{1i} + \\dots + \\beta_P x_{Pi} + e_i.\n\\]\nIn questa equazione:\n\n\n\\(I^*_i\\) indica la propensione latente per l’osservatore $ i $.\n\n\\(\\beta_0\\) è un termine costante che agisce come intercetta.\n\n\\(\\beta_1, \\dots, \\beta_P\\) sono i coefficienti che misurano l’impatto di ciascun predittore sulla propensione latente.\n$ e_i$ è il termine di errore che rappresenta le variazioni non spiegate dai predittori.\n\nQuando la variabile categoriale $ I $ funge da indicatore di un fattore latente $ $ in un modello fattoriale confermativo, la formulazione dell’equazione si semplifica a:\n\\[\nI^*_i = \\beta_0 + \\beta_1 \\xi_i + e_i.\n\\]\nIn questa configurazione, \\(\\beta_1\\) rappresenta il carico fattoriale, indicando quanto fortemente il fattore latente \\(\\xi\\) influisce sulla variabile latente \\(I^*\\). Questo schema è analogo a quello usato per modellare indicatori di misurazione continui nei modelli SEM.\nQuesto approccio riflette l’idea che le risposte categoriali osservabili possono essere considerate come manifestazioni esterne di una propensione interna latente. Per la stima di tali modelli, il metodo dei minimi quadrati ponderati (WLS) è generalmente appropriato. Tuttavia, è importante tenere presente che la modellazione di risposte categoriali ordinate può richiedere considerazioni aggiuntive per gestire adeguatamente la loro natura ordinale, dettagli che verranno approfonditi nelle sezioni seguenti.\n\n41.3.5 Adattamento del Modello con lmer\n\nSpecifichiamo il modello nel modo seguente:\n\nmodel3 &lt;- '\n  etoh =~ y1 + y2 + y3 + y4 + y5 + y6\n'\n\nNell’analizzare dati ottenuti da scale ordinali, il software lavaan impiega un metodo specializzato per gestire la natura particolare dei dati categoriali. Questo approccio utilizza lo stimatore WLSMV (Weighted Least Squares Mean and Variance Adjusted). La stima dei parametri avviene tramite il metodo dei minimi quadrati ponderati diagonalmente (DWLS), che si concentra sulle componenti diagonali della matrice di peso. Questa specificità rende lo stimatore WLSMV particolarmente adatto per analizzare dati non normali.\nUna caratteristica importante dello stimatore WLSMV è la sua capacità di calcolare errori standard robusti. Questi sono determinati attraverso un metodo che mantiene l’affidabilità delle stime anche quando i dati non soddisfano le tradizionali assunzioni di normalità. Inoltre, le statistiche di test prodotte da WLSMV sono adeguatamente corrette per tenere conto delle variazioni nella media e nella varianza dei dati. Questo tipo di correzione è cruciale per garantire l’accuratezza e la validità delle statistiche di test, specialmente quando la distribuzione dei dati devia dalla normalità.\nIn conclusione, lavaan offre un approccio avanzato per la modellazione di dati categoriali utilizzando lo stimatore WLSMV, che è ottimizzato per rispondere alle esigenze specifiche di questi tipi di dati. Questo si traduce in stime più precise e statistiche di test affidabili, rendendo lavaan uno strumento molto appropriato per l’analisi di dati categoriali complessi.\n\nfit3 &lt;- cfa(\n  model3, \n  data = d1, \n  ordered = names(d1), \n  estimator = \"WLSMVS\", \n  mimic = \"mplus\"\n)\n\nEsaminiamo la soluzione ottenuta:\n\nsummary(fit3, fit.measures = TRUE)\n#&gt; lavaan 0.6-19 ended normally after 16 iterations\n#&gt; \n#&gt;   Estimator                                       DWLS\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        12\n#&gt; \n#&gt;   Number of observations                           750\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                   Standard      Scaled\n#&gt;   Test Statistic                                     5.651       9.540\n#&gt;   Degrees of freedom                                     9           9\n#&gt;   P-value (Chi-square)                               0.774       0.389\n#&gt;   Scaling correction factor                                      0.592\n#&gt;     mean and variance adjusted correction (WLSMV)                     \n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                              1155.845     694.433\n#&gt;   Degrees of freedom                                15           9\n#&gt;   P-value                                        0.000       0.000\n#&gt;   Scaling correction factor                                  1.664\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    1.000       0.999\n#&gt;   Tucker-Lewis Index (TLI)                       1.005       0.999\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.000       0.009\n#&gt;   90 Percent confidence interval - lower         0.000       0.000\n#&gt;   90 Percent confidence interval - upper         0.028       0.051\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.999       0.944\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.000       0.000\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.031       0.031\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Parameterization                               Delta\n#&gt;   Standard errors                           Robust.sem\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model        Unstructured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;   etoh =~                                             \n#&gt;     y1                1.000                           \n#&gt;     y2                0.822    0.072   11.392    0.000\n#&gt;     y3                0.653    0.092    7.097    0.000\n#&gt;     y4                1.031    0.075   13.703    0.000\n#&gt;     y5                1.002    0.072   13.861    0.000\n#&gt;     y6                0.759    0.076   10.011    0.000\n#&gt; \n#&gt; Thresholds:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;     y1|t1            -0.759    0.051  -14.890    0.000\n#&gt;     y2|t1            -0.398    0.047   -8.437    0.000\n#&gt;     y3|t1            -1.244    0.061  -20.278    0.000\n#&gt;     y4|t1            -0.795    0.051  -15.436    0.000\n#&gt;     y5|t1            -0.384    0.047   -8.148    0.000\n#&gt;     y6|t1            -0.818    0.052  -15.775    0.000\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;    .y1                0.399                           \n#&gt;    .y2                0.594                           \n#&gt;    .y3                0.744                           \n#&gt;    .y4                0.361                           \n#&gt;    .y5                0.397                           \n#&gt;    .y6                0.653                           \n#&gt;     etoh              0.601    0.063    9.596    0.000\n\nSi presti particolare attenzione alla seguente porzione dell’output:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    y1|t1            -0.759    0.051  -14.890    0.000\n    y2|t1            -0.398    0.047   -8.437    0.000\n    y3|t1            -1.244    0.061  -20.278    0.000\n    y4|t1            -0.795    0.051  -15.436    0.000\n    y5|t1            -0.384    0.047   -8.148    0.000\n    y6|t1            -0.818    0.052  -15.775    0.000\nIn questa porzione dell’output di lavaan sono presentati i risultati per le “soglie” (thresholds) relative alle variabili categoriali ordinate utilizzate nel modello SEM. Ecco una spiegazione dettagliata:\n\n\nThresholds (Soglie):\n\nOgni soglia rappresenta un punto di cutoff lungo la variabile continua latente (indicata in precedenza come I*), che determina le categorie della variabile categoriale osservata.\nNell’output, y1|t1, y2|t1, ecc., rappresentano soglie per le variabili rispettive (y1, y2, …, y6). Il termine “t1” si riferisce alla prima soglia per ciascuna di queste variabili.\n\n\n\nEstimate (Stima):\n\nQuesti valori indicano la posizione della soglia sulla scala della variabile continua latente. Per esempio, la soglia per y1 è a -0.759. Questo significa che la divisione tra le prime due categorie di y1 si verifica a -0.759 sulla scala della variabile latente.\n\n\n\nStd.Err (Errore Standard):\n\nL’errore standard della stima di ogni soglia. Ad esempio, per y1, l’errore standard è 0.051. Questo offre un’idea della variabilità o incertezza nella stima della soglia.\n\n\n\nz-value:\n\nIl valore z indica il rapporto tra la stima della soglia e il suo errore standard. Un valore z elevato suggerisce che la stima della soglia è significativamente diversa da zero (ovvero, la soglia è ben definita). Per esempio, per y1, il valore z è -14.890, che è statisticamente significativo.\n\n\n\nP(&gt;|z|):\n\nIl p-value associato al valore z. Un p-value basso (ad esempio, 0.000) indica che la stima della soglia è statisticamente significativa. Questo significa che possiamo essere abbastanza sicuri che la posizione della soglia sulla variabile latente sia accurata e non dovuta al caso.\n\n\n\nIn sintesi, queste soglie consentono di trasformare la variabile latente continua in una variabile categoriale osservata nel modello. La stima di queste soglie e la loro significatività statistica sono cruciali per comprendere come la variabile latente si traduce nelle categorie osservate.\nConfrontiamo ora la soluzione ottenuta con lo stimatore WLSMVS con quella ottenuta mediante lo stimatore ML.\n\nfit4 &lt;- cfa(\n  model3, \n  data = d1\n)\n\n\nsummary(fit4, fit.measures = TRUE)\n#&gt; lavaan 0.6-19 ended normally after 35 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        12\n#&gt; \n#&gt;   Number of observations                           750\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                14.182\n#&gt;   Degrees of freedom                                 9\n#&gt;   P-value (Chi-square)                           0.116\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               614.305\n#&gt;   Degrees of freedom                                15\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.991\n#&gt;   Tucker-Lewis Index (TLI)                       0.986\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -2087.600\n#&gt;   Loglikelihood unrestricted model (H1)      -2080.508\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                4199.199\n#&gt;   Bayesian (BIC)                              4254.640\n#&gt;   Sample-size adjusted Bayesian (SABIC)       4216.535\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.028\n#&gt;   90 Percent confidence interval - lower         0.000\n#&gt;   90 Percent confidence interval - upper         0.054\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.914\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.000\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.021\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;   etoh =~                                             \n#&gt;     y1                1.000                           \n#&gt;     y2                0.934    0.093   10.057    0.000\n#&gt;     y3                0.390    0.055    7.038    0.000\n#&gt;     y4                1.008    0.087   11.541    0.000\n#&gt;     y5                1.158    0.101   11.468    0.000\n#&gt;     y6                0.700    0.077    9.142    0.000\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;    .y1                0.109    0.007   14.692    0.000\n#&gt;    .y2                0.169    0.010   16.781    0.000\n#&gt;    .y3                0.085    0.005   18.483    0.000\n#&gt;    .y4                0.102    0.007   14.285    0.000\n#&gt;    .y5                0.140    0.010   14.506    0.000\n#&gt;    .y6                0.132    0.008   17.514    0.000\n#&gt;     etoh              0.065    0.009    7.664    0.000\n\nSi noti che la soluzione ottenuta mediante lo stimatore WLSMVS produce indici di bontà di adattamento migliori e errori standard dei parametri più piccoli.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Dati non gaussiani e categoriali</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/03_cat_data.html#riflessioni-conclusive",
    "href": "chapters/cfa/03_cat_data.html#riflessioni-conclusive",
    "title": "41  Dati non gaussiani e categoriali",
    "section": "\n41.4 Riflessioni Conclusive",
    "text": "41.4 Riflessioni Conclusive\nIn questo capitolo, abbiamo esplorato la modellazione CFA con dati non normalmente distribuiti. È essenziale riconoscere che, nella pratica analitica, incontrare dati non normalmente distribuiti dovrebbe essere considerato normale. Di conseguenza, si raccomanda l’utilizzo della massima verosimiglianza robusta (ML robusta) ogni volta che sorgono dubbi sulla normalità dei dati.\nCi sono alcune considerazioni importanti da tenere presente:\n\n\nStabilità delle stime di parametro: Anche se le versioni robuste di ML forniscono errori standard robusti e statistiche di test adattate, le stime dei parametri ottenute rimangono quelle della stima ML originale.\n\nRobustezza limitata: Gli aggiustamenti robusti compensano la violazione della normalità, ma non coprono la presenza di valori anomali, che richiedono un’analisi separata.\n\nLimitazioni degli aggiustamenti: Gli aggiustamenti robusti non trattano violazioni delle specifiche del modello, che è un altro argomento di discussione nella letteratura CFA e SEM.\n\nAbbiamo anche discusso l’uso dello stimatore WLSMV per dati categoriali, evidenziando come esso fornisca una stima dell’errore standard più precisa rispetto all’MLE standard e all’MLE robusta.\nVa notato che WLSMV è un metodo generale per dati categoriali nella CFA, ampiamente implementato in software come MPlus. In lavaan, l’uso di WLSMV può essere attivato semplicemente con lavaan(..., estimator = \"WLSMV\"), equivalente a lavaan(..., estimator = \"DWLS\", se = \"robust.sem\", test = \"scaled.shifted\").\nOltre al WLSMV, lavaan offre anche lo stimatore sperimentale di massima verosimiglianza marginale (MML), che, pur essendo preciso, può essere lento e più suscettibile a problemi di convergenza a causa della complessità dell’integrazione numerica. Un altro stimatore è l’ADF (estimator = “WLS”), che non assume specifiche distributive sui dati, ma richiede una dimensione campionaria molto grande (N &gt; 5000) per considerare affidabili le stime dei parametri, gli errori standard e le statistiche di test.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Dati non gaussiani e categoriali</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/03_cat_data.html#session-info",
    "href": "chapters/cfa/03_cat_data.html#session-info",
    "title": "41  Dati non gaussiani e categoriali",
    "section": "\n41.5 Session Info",
    "text": "41.5 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] BifactorIndicesCalculator_0.2.2 ggokabeito_0.1.0               \n#&gt;  [3] see_0.10.0                      MASS_7.3-65                    \n#&gt;  [5] viridis_0.6.5                   viridisLite_0.4.2              \n#&gt;  [7] ggpubr_0.6.0                    ggExtra_0.10.1                 \n#&gt;  [9] gridExtra_2.3                   patchwork_1.3.0                \n#&gt; [11] bayesplot_1.11.1                semTools_0.5-6                 \n#&gt; [13] semPlot_1.1.6                   lavaan_0.6-19                  \n#&gt; [15] psych_2.4.12                    scales_1.3.0                   \n#&gt; [17] markdown_1.13                   knitr_1.49                     \n#&gt; [19] lubridate_1.9.4                 forcats_1.0.0                  \n#&gt; [21] stringr_1.5.1                   dplyr_1.1.4                    \n#&gt; [23] purrr_1.0.4                     readr_2.1.5                    \n#&gt; [25] tidyr_1.3.1                     tibble_3.2.1                   \n#&gt; [27] ggplot2_3.5.1                   tidyverse_2.0.0                \n#&gt; [29] here_1.0.1                     \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.0      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] mime_0.12           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [25] Matrix_1.7-2        R6_2.6.1            fastmap_1.2.0      \n#&gt;  [28] rbibutils_2.3       shiny_1.10.0        digest_0.6.37      \n#&gt;  [31] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n#&gt;  [34] rprojroot_2.0.4     Hmisc_5.2-2         timechange_0.3.0   \n#&gt;  [37] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n#&gt;  [40] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n#&gt;  [43] carData_3.0-5       ggsignif_0.6.4      corpcor_1.6.10     \n#&gt;  [46] gtools_3.9.5        tools_4.4.2         pbivnorm_0.6.0     \n#&gt;  [49] foreign_0.8-88      zip_2.3.2           httpuv_1.6.15      \n#&gt;  [52] nnet_7.3-20         glue_1.8.0          quadprog_1.5-8     \n#&gt;  [55] nlme_3.1-167        promises_1.3.2      lisrelToR_0.3      \n#&gt;  [58] grid_4.4.2          checkmate_2.3.2     cluster_2.1.8      \n#&gt;  [61] reshape2_1.4.4      generics_0.1.3      gtable_0.3.6       \n#&gt;  [64] tzdb_0.4.0          data.table_1.17.0   hms_1.1.3          \n#&gt;  [67] car_3.1-3           sem_3.1-16          pillar_1.10.1      \n#&gt;  [70] rockchalk_1.8.157   later_1.4.1         splines_4.4.2      \n#&gt;  [73] lattice_0.22-6      survival_3.8-3      kutils_1.73        \n#&gt;  [76] tidyselect_1.2.1    miniUI_0.1.1.1      pbapply_1.7-2      \n#&gt;  [79] reformulas_0.4.0    stats4_4.4.2        xfun_0.51          \n#&gt;  [82] qgraph_1.9.8        arm_1.14-4          stringi_1.8.4      \n#&gt;  [85] pacman_0.5.1        boot_1.3-31         evaluate_1.0.3     \n#&gt;  [88] codetools_0.2-20    mi_1.1              cli_3.6.4          \n#&gt;  [91] RcppParallel_5.1.10 rpart_4.1.24        xtable_1.8-4       \n#&gt;  [94] Rdpack_2.6.2        munsell_0.5.1       Rcpp_1.0.14        \n#&gt;  [97] coda_0.19-4.1       png_0.1-8           XML_3.99-0.18      \n#&gt; [100] parallel_4.4.2      jpeg_0.1-10         lme4_1.1-36        \n#&gt; [103] mvtnorm_1.3-3       openxlsx_4.2.8      rlang_1.1.5        \n#&gt; [106] multcomp_1.4-28     mnormt_2.1.1\n\n\n\n\n\nBrown, T. A. (2015). Confirmatory factor analysis for applied research. Guilford publications.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Dati non gaussiani e categoriali</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/04_mmm.html",
    "href": "chapters/cfa/04_mmm.html",
    "title": "42  CFA per matrici multi-tratto multi-metodo",
    "section": "",
    "text": "42.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nLa validità descrive quanto accuratamente un metodo di misurazione riesce a quantificare ciò che è inteso misurare. Esistono diverse categorie di validità, ognuna delle quali si verifica attraverso metodi specifici. Una suddivisione convenzionale delle diverse tipologie di validità, che non riflette necessariamente gli sviluppi più recenti in questo campo, può essere descritta come segue (per ulteriori dettagli si rimanda al capitolo dedicato alla validità nella presente dispena):",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>CFA per matrici multi-tratto multi-metodo</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/04_mmm.html#introduzione",
    "href": "chapters/cfa/04_mmm.html#introduzione",
    "title": "42  CFA per matrici multi-tratto multi-metodo",
    "section": "",
    "text": "Validità di facciata: Valuta se gli item di un test appaiono appropriati e ragionevoli rispetto al costrutto che si intende misurare, sia agli occhi di chi partecipa al test sia di chi lo utilizza. Questo tipo di validità è basato sulla percezione esteriore della misura e si valuta tramite i giudizi di esperti sulla plausibilità delle misure.\nValidità di contenuto: Una misura possiede validità di contenuto quando i suoi indicatori rappresentano in modo esaustivo e accurato l’area di contenuto da misurare. Anche questa validità si basa sui giudizi di esperti.\nValidità di costrutto: Corrisponde alla definizione generale di validità e si riferisce alla capacità di uno strumento di misurare il costrutto che intende misurare. La validità di costrutto si verifica attraverso la correttezza con cui gli indicatori misurano i costrutti teorici di interesse e si convalida attraverso l’analisi delle relazioni tra il costrutto misurato e altri costrutti correlati, secondo modelli teorici specifici.\nValidità di criterio: Indica la capacità di uno strumento di fare previsioni accurate su un criterio esterno, valutando quanto bene la misura predice questo criterio.\nValidità concorrente: Si determina osservando quanto uno strumento di misurazione correla con altri strumenti considerati validi per misurare lo stesso attributo. Una forte correlazione è generalmente vista come una conferma della validità.\nValidità convergente: Si verifica confrontando e correlando i punteggi ottenuti con la misura da validare con quelli ottenuti da un altro costrutto teoricamente relazionato. La verifica di questa validità dipende dall’esistenza di misure valide per costrutti correlati.\nValidità discriminante: È l’opposto della validità convergente e si verifica quando la misura in esame non mostra correlazioni significative con le misure di costrutti teoricamente distinti.\n\n\n42.1.1 MTMM e CFA\nLa Matrice Multi-Tratto Multi-Metodo (MTMM) è un approccio utilizzato per valutare la validità di costrutto, esaminando la correlazione tra diversi costrutti misurati sia con gli stessi metodi sia con metodi differenti. La validità di costrutto è considerata alta quando la misura di un costrutto è indipendente dal metodo di misurazione utilizzato.\n\n\n42.1.2 Un esempio concreto\nNell’esempio discusso da {cite:t}brown2015confirmatory, il ricercatore desidera esaminare la validità del costrutto dei disturbi di personalità del Cluster A del DSM-IV, che sono pattern persistenti di sintomi caratterizzati da comportamenti strani o eccentrici (American Psychiatric Association, 1994). Il cluster A comprende tre costrutti di disturbo della personalità:\n\nparanoico (un pattern duraturo di sfiducia e sospetto tale che le motivazioni degli altri sono interpretate come malevole);\nschizoide (un pattern duraturo di distacco dalle relazioni sociali e una gamma ristretta di espressioni emotive);\nschizotipico (un pattern duraturo di disagio acuto nelle relazioni sociali, distorsioni cognitive e percettive ed eccentricità comportamentali).\n\nIn un campione di 500 pazienti, ciascuno di questi tre tratti è misurato mediante tre metodi di valutazione:\n\nun inventario di autovalutazione dei disturbi di personalità;\nvalutazioni dimensionali da un colloquio clinico strutturato sui disturbi della personalità;\nvalutazioni osservazionali effettuate da psicologi.\n\nI dati sono contenuti in una matrice 3 (T) × 3 (M), organizzata in modo tale che le correlazioni tra i diversi tratti (disturbi della personalità: paranoico, schizotipico, schizoide) siano annidate all’interno di ciascun metodo (tipo di valutazione: inventario, colloquio clinico, valutazioni degli osservatori).\nI dati sono riportati qui sotto.\n\nsds &lt;- c(3.61,  3.66,  3.59,  2.94,  3.03,  2.85,  2.22,  2.42,  2.04)\n\ncors &lt;- '\n  1.000 \n  0.290  1.000 \n  0.372  0.478  1.000 \n  0.587  0.238  0.209  1.000 \n  0.201  0.586  0.126  0.213  1.000 \n  0.218  0.281  0.681  0.195  0.096  1.000 \n  0.557  0.228  0.195  0.664  0.242  0.232  1.000 \n  0.196  0.644  0.146  0.261  0.641  0.248  0.383  1.000 \n  0.219  0.241  0.676  0.290  0.168  0.749  0.361  0.342  1.000'\n\ncovs &lt;- getCov(\n  cors, \n  sds = sds, \n  names = c(\"pari\", \"szti\", \"szdi\", \"parc\", \"sztc\", \"szdc\", \"paro\", \"szto\", \"szdo\")\n  )\n\nLa Matrice Multi-Tratto Multi-Metodo (MTMM) si organizza in due tipi di blocchi di coefficienti:\n\nBlocchi di mono-metodo: contengono le correlazioni tra indicatori che provengono dallo stesso metodo di misurazione. Questi blocchi esaminano come diversi indicatori del medesimo tratto si correlano tra loro quando misurati tramite lo stesso strumento.\nBlocchi di etero-metodo: includono le correlazioni tra indicatori misurati mediante metodi diversi. Particolarmente significativa è la “diagonale di validità” all’interno di questi blocchi, dove le correlazioni rappresentano stime di validità convergente. In altre parole, misure diverse di costrutti teoricamente simili dovrebbero mostrare forti correlazioni.\n\nNell’analisi MTMM, una forte correlazione tra metodi che misurano lo stesso tratto evidenzia la validità convergente. Per esempio, potrebbe risultare che diverse misure della personalità schizotipica mostrino correlazioni elevate, con coefficienti ( r ) che variano da 0.676 a 0.749. Al contrario, elementi al di fuori della diagonale nei blocchi di etero-metodo rivelano la validità discriminante, dove le misure di costrutti teoricamente distinti non dovrebbero essere altamente correlate. Questa validità è confermata quando tali correlazioni sono significativamente più basse rispetto a quelle della diagonale di validità, ad esempio, coefficienti che variano da 0.126 a 0.290.\nInoltre, è possibile rilevare gli effetti del metodo esaminando gli elementi al di fuori della diagonale nei blocchi di mono-metodo. Qui, la varianza nelle correlazioni tra diversi tratti misurati con lo stesso metodo, rispetto alle correlazioni tra gli stessi tratti misurati con metodi diversi, riflette l’entità degli effetti del metodo. Ad esempio, le valutazioni dell’osservatore dei tratti della personalità paranoica e schizotipica potrebbero essere più correlate (r = 0.383) rispetto alle loro misure con metodi diversi (ad esempio, la correlazione tra le misure di personalità paranoide e schizotipica, con l’uso rispettivamente dell’inventario e della valutazione dell’osservatore, è di 0.196).\nLa validità del costrutto è supportata quando i dati indicano alta validità convergente e discriminante con effetti del metodo trascurabili.\nIl modello CFA per analizzare la matrice MTMM può includere correlazioni residue tra le specificità di ciascun metodo, supponendo che ogni fattore comune (come paranoid, schizotypal, schizoid) sia identificato da item misurati con metodi diversi e che le specificità di ciascun metodo siano correlate tra loro.\n{cite:t}brown2015confirmatory mostra come sia possibile analizzare la matrice MTMM con un modello CFA nel quale si ipotizza che vi siano correlazioni residue tra le specificità di ciascun metodo. Il modello è dunque formulato nel modo seguente: ogni fattore comune (paranoid, schizotypal, schizoid) è identificato dagli item corrispondenti definiti da metodi diversi; le specificità di ciascun metodo, inoltre, sono correlate tra loro.\n\nmodel &lt;- '\n  paranoid    =~ pari + parc + paro\n  schizotypal =~ szti + sztc + szto\n  schizoid    =~ szdi + szdc + szdo\n  pari ~~ szti + szdi\n  szti ~~ szdi\n  parc ~~ sztc + szdc\n  sztc ~~ szdc\n  paro ~~ szto + szdo\n  szto ~~ szdo\n'  \n\nAdattiamo il modello ai dati.\n\nfit &lt;- cfa(\n  model, \n  sample.cov = covs, \n  sample.nobs = 500, \n  std.lv = TRUE\n)\n\nEsaminiamo la soluzione ottenuta.\n\nsummary(fit, fit.measures = TRUE, standardized = TRUE) |&gt;\n    print()\n\nlavaan 0.6.17 ended normally after 59 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        30\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                                14.371\n  Degrees of freedom                                15\n  P-value (Chi-square)                           0.498\n\nModel Test Baseline Model:\n\n  Test statistic                              2503.656\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.001\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -9879.996\n  Loglikelihood unrestricted model (H1)      -9872.811\n                                                      \n  Akaike (AIC)                               19819.992\n  Bayesian (BIC)                             19946.430\n  Sample-size adjusted Bayesian (SABIC)      19851.209\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.041\n  P-value H_0: RMSEA &lt;= 0.050                    0.989\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.025\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  paranoid =~                                                           \n    pari              2.588    0.145   17.833    0.000    2.588    0.712\n    parc              2.472    0.121   20.350    0.000    2.472    0.841\n    paro              1.747    0.088   19.946    0.000    1.747    0.788\n  schizotypal =~                                                        \n    szti              2.950    0.132   22.367    0.000    2.950    0.788\n    sztc              2.348    0.123   19.047    0.000    2.348    0.768\n    szto              2.047    0.089   22.905    0.000    2.047    0.843\n  schizoid =~                                                           \n    szdi              2.713    0.120   22.526    0.000    2.713    0.769\n    szdc              2.438    0.107   22.826    0.000    2.438    0.860\n    szdo              1.782    0.073   24.323    0.000    1.782    0.872\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .pari ~~                                                               \n   .szti              1.274    0.338    3.774    0.000    1.274    0.217\n   .szdi              2.537    0.329    7.703    0.000    2.537    0.441\n .szti ~~                                                               \n   .szdi              3.872    0.342   11.329    0.000    3.872    0.746\n .parc ~~                                                               \n   .sztc             -0.335    0.210   -1.597    0.110   -0.335   -0.107\n   .szdc             -0.608    0.176   -3.461    0.001   -0.608   -0.265\n .sztc ~~                                                               \n   .szdc             -0.933    0.188   -4.967    0.000   -0.933   -0.330\n .paro ~~                                                               \n   .szto              0.737    0.118    6.240    0.000    0.737    0.413\n   .szdo              0.505    0.096    5.274    0.000    0.505    0.368\n .szto ~~                                                               \n   .szdo              0.625    0.102    6.158    0.000    0.625    0.478\n  paranoid ~~                                                           \n    schizotypal       0.381    0.046    8.341    0.000    0.381    0.381\n    schizoid          0.359    0.046    7.856    0.000    0.359    0.359\n  schizotypal ~~                                                        \n    schizoid          0.310    0.047    6.666    0.000    0.310    0.310\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .pari              6.514    0.513   12.695    0.000    6.514    0.493\n   .parc              2.529    0.334    7.562    0.000    2.529    0.293\n   .paro              1.867    0.179   10.434    0.000    1.867    0.380\n   .szti              5.309    0.460   11.529    0.000    5.309    0.379\n   .sztc              3.846    0.330   11.654    0.000    3.846    0.411\n   .szto              1.704    0.175    9.742    0.000    1.704    0.289\n   .szdi              5.080    0.386   13.158    0.000    5.080    0.408\n   .szdc              2.085    0.230    9.047    0.000    2.085    0.260\n   .szdo              1.005    0.107    9.351    0.000    1.005    0.240\n    paranoid          1.000                               1.000    1.000\n    schizotypal       1.000                               1.000    1.000\n    schizoid          1.000                               1.000    1.000\n\n\n\n\neffectsize::interpret(fit) |&gt;\n    print()\n\n    Name      Value Threshold Interpretation\n1    GFI 0.99376810      0.95   satisfactory\n2   AGFI 0.98130431      0.90   satisfactory\n3    NFI 0.99425997      0.90   satisfactory\n4   NNFI 1.00061169      0.90   satisfactory\n5    CFI 1.00000000      0.90   satisfactory\n6  RMSEA 0.00000000      0.05   satisfactory\n7   SRMR 0.02482894      0.08   satisfactory\n8    RFI 0.98622392      0.90   satisfactory\n9   PNFI 0.41427499      0.50           poor\n10   IFI 1.00025272      0.90   satisfactory\n\n\nPer i dati considerati da {cite:t}brown2015confirmatory, l’adattamento del modello MTMM è eccellente. Ciò fornisce forti evidenze di validità di costrutto per i fattori Paranoico, Schizoide e Schizotipico che sono stati ipotizzati.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>CFA per matrici multi-tratto multi-metodo</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/04_mmm.html#session-info",
    "href": "chapters/cfa/04_mmm.html#session-info",
    "title": "42  CFA per matrici multi-tratto multi-metodo",
    "section": "42.2 Session Info",
    "text": "42.2 Session Info\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.4.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ggokabeito_0.1.0  viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n [5] ggExtra_0.10.1    bayesplot_1.11.1  gridExtra_2.3     patchwork_1.2.0  \n [9] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-17     psych_2.4.3      \n[13] scales_1.3.0      markdown_1.12     knitr_1.45        lubridate_1.9.3  \n[17] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[21] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.0    \n[25] tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.15.0  jsonlite_1.8.8     datawizard_0.9.1  \n  [4] magrittr_2.0.3     TH.data_1.1-2      estimability_1.5  \n  [7] nloptr_2.0.3       rmarkdown_2.26     vctrs_0.6.5       \n [10] minqa_1.2.6        effectsize_0.8.7   base64enc_0.1-3   \n [13] rstatix_0.7.2      htmltools_0.5.7    broom_1.0.5       \n [16] Formula_1.2-5      htmlwidgets_1.6.4  plyr_1.8.9        \n [19] sandwich_3.1-0     emmeans_1.10.0     zoo_1.8-12        \n [22] uuid_1.2-0         igraph_2.0.2       mime_0.12         \n [25] lifecycle_1.0.4    pkgconfig_2.0.3    Matrix_1.6-5      \n [28] R6_2.5.1           fastmap_1.1.1      shiny_1.8.0       \n [31] digest_0.6.35      OpenMx_2.21.11     fdrtool_1.2.17    \n [34] colorspace_2.1-0   rprojroot_2.0.4    Hmisc_5.1-1       \n [37] fansi_1.0.6        timechange_0.3.0   abind_1.4-5       \n [40] compiler_4.3.3     withr_3.0.0        glasso_1.11       \n [43] htmlTable_2.4.2    backports_1.4.1    carData_3.0-5     \n [46] performance_0.11.0 ggsignif_0.6.4     MASS_7.3-60.0.1   \n [49] corpcor_1.6.10     gtools_3.9.5       tools_4.3.3       \n [52] pbivnorm_0.6.0     foreign_0.8-86     zip_2.3.1         \n [55] httpuv_1.6.14      nnet_7.3-19        glue_1.7.0        \n [58] quadprog_1.5-8     nlme_3.1-164       promises_1.2.1    \n [61] lisrelToR_0.3      grid_4.3.3         pbdZMQ_0.3-11     \n [64] checkmate_2.3.1    cluster_2.1.6      reshape2_1.4.4    \n [67] generics_0.1.3     gtable_0.3.4       tzdb_0.4.0        \n [70] data.table_1.15.2  hms_1.1.3          car_3.1-2         \n [73] utf8_1.2.4         sem_3.1-15         pillar_1.9.0      \n [76] IRdisplay_1.1      rockchalk_1.8.157  later_1.3.2       \n [79] splines_4.3.3      lattice_0.22-5     survival_3.5-8    \n [82] kutils_1.73        tidyselect_1.2.0   miniUI_0.1.1.1    \n [85] pbapply_1.7-2      stats4_4.3.3       xfun_0.42         \n [88] qgraph_1.9.8       arm_1.13-1         stringi_1.8.3     \n [91] boot_1.3-29        evaluate_0.23      codetools_0.2-19  \n [94] mi_1.1             cli_3.6.2          RcppParallel_5.1.7\n [97] IRkernel_1.3.2     rpart_4.1.23       parameters_0.21.6 \n[100] xtable_1.8-4       repr_1.1.6         munsell_0.5.0     \n[103] Rcpp_1.0.12        coda_0.19-4.1      png_0.1-8         \n[106] XML_3.99-0.16.1    parallel_4.3.3     ellipsis_0.3.2    \n[109] bayestestR_0.13.2  jpeg_0.1-10        lme4_1.1-35.1     \n[112] mvtnorm_1.2-4      insight_0.19.10    openxlsx_4.2.5.2  \n[115] crayon_1.5.2       rlang_1.1.3        multcomp_1.4-25   \n[118] mnormt_2.1.1",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>CFA per matrici multi-tratto multi-metodo</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/05_bifactor.html",
    "href": "chapters/cfa/05_bifactor.html",
    "title": "43  Modello bifattoriale",
    "section": "",
    "text": "43.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nNumerose misure psicologiche sono progettate per valutare individui su un singolo costrutto. Tuttavia, caratteristiche psicologiche complesse come depressione e ansia spesso si manifestano in modi vari. Di conseguenza, è consigliabile includere item che coprono diverse aree tematiche per assicurare una validità di contenuto adeguata. Pertanto, molte scale di valutazione comunemente usate producono dati che si prestano a interpretazioni valide sia attraverso un modello unidimensionale, con un forte fattore generale, sia tramite un modello multidimensionale, che comprende due o più fattori correlati.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Modello bifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/05_bifactor.html#struttura-fattoriale",
    "href": "chapters/cfa/05_bifactor.html#struttura-fattoriale",
    "title": "43  Modello bifattoriale",
    "section": "43.2 Struttura Fattoriale",
    "text": "43.2 Struttura Fattoriale\nRecenti studi hanno evidenziato che, di fronte a misure che generano dati multidimensionali a causa di una diversificata struttura di contenuto, l’adozione di un modello di misurazione bifattoriale può essere particolarmente efficace per rappresentare la struttura sottostante. Questo modello suggerisce che le correlazioni tra gli item di un test possono essere spiegate attraverso due tipi di fattori: (a) un fattore generale che riflette la varianza condivisa tra tutti gli item, e (b) una serie di fattori di gruppo che catturano la varianza specifica non spiegata dal fattore generale e che è comune tra item simili in termini di contenuto. Generalmente, si ritiene che il fattore generale e i fattori di gruppo siano indipendenti.\nIl fattore generale rappresenta il costrutto principale che lo strumento si propone di misurare, mentre i fattori di gruppo individuano costrutti più specifici legati a sottodomini. I modelli bifattoriali sono utilizzati per diverse finalità importanti:\n\nAnalizzare la distribuzione della varianza quando si presume che uno strumento misuri sia varianza generale sia specifica di gruppo.\nGestire la multidimensionalità in modo che la misura risulti “essenzialmente unidimensionale”, pur presentando dimensioni secondarie.\nVerificare la presenza di un fattore generale sufficientemente robusto da giustificare l’uso di un modello di misurazione unidimensionale.\nDeterminare l’adeguatezza di un punteggio complessivo e valutare l’utilità di analizzare le sottoscale specifiche.\n\nQuesti approcci permettono una comprensione più profonda e una valutazione più accurata della struttura sottostante dei dati psicologici, offrendo agli specialisti gli strumenti per interpretare con maggiore precisione i risultati dei test psicologici.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Modello bifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/05_bifactor.html#un-esempio-pratico",
    "href": "chapters/cfa/05_bifactor.html#un-esempio-pratico",
    "title": "43  Modello bifattoriale",
    "section": "43.3 Un esempio pratico",
    "text": "43.3 Un esempio pratico\nConsideriamo i dati SRS_data forniti dal pacchetto BifactorIndicesCalculator. Il dataset contiene 500 risposte al test SRS-22r sulla qualità della vita legata alla scoliosi, composta da 20 item. La sottoscala “Function” è composta dagli item 5, 9, 12, 15 e 18. La sottoscala “Pain” è composta dagli item 1, 2, 8, 11 e 17. La sottoscala “SelfImage” è composta dagli item 4, 6, 10, 14 e 19. La sottoscala “MentalHealth” è composta dagli item 3, 7, 13, 16 e 20.\nIniziamo esaminando le statistiche descrittive a livello di item e le correlazioni tra gli item.\n\ndescribe(SRS_data)\n\n\nA psych: 20 x 13\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nSRS_1\n1\n500\n3.69\n1.065\n4\n3.78\n1.48\n1\n5\n4\n-0.5509\n-0.4413\n0.0476\n\n\nSRS_2\n2\n500\n3.81\n1.030\n4\n3.92\n1.48\n1\n5\n4\n-0.7148\n-0.0679\n0.0460\n\n\nSRS_3\n3\n500\n3.90\n1.090\n4\n4.04\n1.48\n1\n5\n4\n-0.8323\n-0.0397\n0.0487\n\n\nSRS_4\n4\n500\n3.23\n1.280\n3\n3.29\n1.48\n1\n5\n4\n-0.1059\n-1.0236\n0.0572\n\n\nSRS_5\n5\n500\n4.20\n0.894\n4\n4.30\n1.48\n2\n5\n3\n-0.7453\n-0.5385\n0.0400\n\n\nSRS_6\n6\n500\n3.91\n0.872\n4\n3.95\n1.48\n1\n5\n4\n-0.3767\n-0.3859\n0.0390\n\n\nSRS_7\n7\n500\n4.24\n1.011\n5\n4.40\n0.00\n1\n5\n4\n-1.2842\n1.0807\n0.0452\n\n\nSRS_8\n8\n500\n3.80\n1.132\n4\n3.91\n1.48\n1\n5\n4\n-0.5140\n-0.6993\n0.0506\n\n\nSRS_9\n9\n500\n4.41\n1.004\n5\n4.63\n0.00\n1\n5\n4\n-1.7706\n2.4517\n0.0449\n\n\nSRS_10\n10\n500\n3.70\n0.855\n4\n3.71\n1.48\n1\n5\n4\n-0.2302\n-0.2097\n0.0382\n\n\nSRS_11\n11\n500\n4.50\n0.839\n5\n4.69\n0.00\n1\n5\n4\n-2.0302\n4.0862\n0.0375\n\n\nSRS_12\n12\n500\n4.26\n1.028\n5\n4.44\n0.00\n1\n5\n4\n-1.3462\n1.1192\n0.0460\n\n\nSRS_13\n13\n500\n3.85\n0.927\n4\n3.94\n1.48\n1\n5\n4\n-0.6457\n0.0497\n0.0414\n\n\nSRS_14\n14\n500\n4.72\n0.676\n5\n4.89\n0.00\n1\n5\n4\n-2.8155\n8.2069\n0.0303\n\n\nSRS_15\n15\n500\n4.81\n0.629\n5\n4.99\n0.00\n1\n5\n4\n-4.1179\n18.0401\n0.0281\n\n\nSRS_16\n16\n500\n4.24\n0.960\n5\n4.40\n0.00\n1\n5\n4\n-1.2300\n0.9875\n0.0429\n\n\nSRS_17\n17\n500\n4.74\n0.860\n5\n4.99\n0.00\n1\n5\n4\n-3.4840\n11.2987\n0.0385\n\n\nSRS_18\n18\n500\n2.92\n0.923\n3\n2.92\n0.00\n1\n5\n4\n0.0515\n0.8364\n0.0413\n\n\nSRS_19\n19\n500\n3.60\n1.107\n4\n3.69\n1.48\n1\n5\n4\n-0.5442\n-0.2818\n0.0495\n\n\nSRS_20\n20\n500\n3.99\n0.900\n4\n4.09\n1.48\n1\n5\n4\n-0.8992\n0.7480\n0.0402\n\n\n\n\n\n\nround(cor(SRS_data, use = \"pairwise.complete.obs\"), 2)\n\n\nA matrix: 20 x 20 of type dbl\n\n\n\nSRS_1\nSRS_2\nSRS_3\nSRS_4\nSRS_5\nSRS_6\nSRS_7\nSRS_8\nSRS_9\nSRS_10\nSRS_11\nSRS_12\nSRS_13\nSRS_14\nSRS_15\nSRS_16\nSRS_17\nSRS_18\nSRS_19\nSRS_20\n\n\n\n\nSRS_1\n1.00\n0.88\n0.43\n0.36\n0.36\n0.39\n0.34\n0.70\n0.31\n0.37\n0.46\n0.59\n0.37\n0.36\n0.19\n0.40\n0.38\n0.30\n0.27\n0.33\n\n\nSRS_2\n0.88\n1.00\n0.40\n0.38\n0.35\n0.42\n0.35\n0.70\n0.34\n0.40\n0.49\n0.58\n0.35\n0.38\n0.20\n0.37\n0.39\n0.29\n0.31\n0.32\n\n\nSRS_3\n0.43\n0.40\n1.00\n0.32\n0.33\n0.39\n0.50\n0.46\n0.30\n0.32\n0.24\n0.46\n0.55\n0.34\n0.19\n0.55\n0.25\n0.25\n0.28\n0.41\n\n\nSRS_4\n0.36\n0.38\n0.32\n1.00\n0.23\n0.43\n0.32\n0.33\n0.19\n0.43\n0.20\n0.30\n0.30\n0.20\n0.23\n0.32\n0.10\n0.21\n0.54\n0.32\n\n\nSRS_5\n0.36\n0.35\n0.33\n0.23\n1.00\n0.33\n0.39\n0.33\n0.47\n0.34\n0.23\n0.52\n0.31\n0.29\n0.20\n0.42\n0.28\n0.31\n0.25\n0.40\n\n\nSRS_6\n0.39\n0.42\n0.39\n0.43\n0.33\n1.00\n0.48\n0.37\n0.25\n0.64\n0.28\n0.37\n0.41\n0.38\n0.22\n0.47\n0.22\n0.31\n0.62\n0.41\n\n\nSRS_7\n0.34\n0.35\n0.50\n0.32\n0.39\n0.48\n1.00\n0.40\n0.24\n0.37\n0.22\n0.42\n0.53\n0.44\n0.23\n0.78\n0.19\n0.39\n0.39\n0.56\n\n\nSRS_8\n0.70\n0.70\n0.46\n0.33\n0.33\n0.37\n0.40\n1.00\n0.30\n0.37\n0.36\n0.52\n0.40\n0.28\n0.14\n0.42\n0.31\n0.32\n0.28\n0.34\n\n\nSRS_9\n0.31\n0.34\n0.30\n0.19\n0.47\n0.25\n0.24\n0.30\n1.00\n0.36\n0.26\n0.49\n0.32\n0.29\n0.22\n0.32\n0.35\n0.27\n0.20\n0.31\n\n\nSRS_10\n0.37\n0.40\n0.32\n0.43\n0.34\n0.64\n0.37\n0.37\n0.36\n1.00\n0.26\n0.37\n0.30\n0.34\n0.22\n0.39\n0.19\n0.28\n0.54\n0.35\n\n\nSRS_11\n0.46\n0.49\n0.24\n0.20\n0.23\n0.28\n0.22\n0.36\n0.26\n0.26\n1.00\n0.42\n0.19\n0.33\n0.20\n0.23\n0.39\n0.18\n0.24\n0.19\n\n\nSRS_12\n0.59\n0.58\n0.46\n0.30\n0.52\n0.37\n0.42\n0.52\n0.49\n0.37\n0.42\n1.00\n0.43\n0.46\n0.23\n0.46\n0.44\n0.33\n0.32\n0.43\n\n\nSRS_13\n0.37\n0.35\n0.55\n0.30\n0.31\n0.41\n0.53\n0.40\n0.32\n0.30\n0.19\n0.43\n1.00\n0.35\n0.17\n0.57\n0.21\n0.30\n0.35\n0.60\n\n\nSRS_14\n0.36\n0.38\n0.34\n0.20\n0.29\n0.38\n0.44\n0.28\n0.29\n0.34\n0.33\n0.46\n0.35\n1.00\n0.33\n0.46\n0.38\n0.30\n0.32\n0.34\n\n\nSRS_15\n0.19\n0.20\n0.19\n0.23\n0.20\n0.22\n0.23\n0.14\n0.22\n0.22\n0.20\n0.23\n0.17\n0.33\n1.00\n0.27\n0.20\n0.17\n0.29\n0.24\n\n\nSRS_16\n0.40\n0.37\n0.55\n0.32\n0.42\n0.47\n0.78\n0.42\n0.32\n0.39\n0.23\n0.46\n0.57\n0.46\n0.27\n1.00\n0.19\n0.38\n0.36\n0.59\n\n\nSRS_17\n0.38\n0.39\n0.25\n0.10\n0.28\n0.22\n0.19\n0.31\n0.35\n0.19\n0.39\n0.44\n0.21\n0.38\n0.20\n0.19\n1.00\n0.25\n0.15\n0.16\n\n\nSRS_18\n0.30\n0.29\n0.25\n0.21\n0.31\n0.31\n0.39\n0.32\n0.27\n0.28\n0.18\n0.33\n0.30\n0.30\n0.17\n0.38\n0.25\n1.00\n0.25\n0.31\n\n\nSRS_19\n0.27\n0.31\n0.28\n0.54\n0.25\n0.62\n0.39\n0.28\n0.20\n0.54\n0.24\n0.32\n0.35\n0.32\n0.29\n0.36\n0.15\n0.25\n1.00\n0.41\n\n\nSRS_20\n0.33\n0.32\n0.41\n0.32\n0.40\n0.41\n0.56\n0.34\n0.31\n0.35\n0.19\n0.43\n0.60\n0.34\n0.24\n0.59\n0.16\n0.31\n0.41\n1.00\n\n\n\n\n\n\nSRS_UnidimensionalModel &lt;-\n\"\n    SRS =~ SRS_1 + SRS_2 + SRS_3 + SRS_4 + SRS_5 +\n    SRS_6 + SRS_7 + SRS_8 + SRS_9 + SRS_10 +\n    SRS_11 + SRS_12 + SRS_13 + SRS_14 + SRS_15 +\n    SRS_16 + SRS_17 + SRS_18 + SRS_19 + SRS_20\n\"\n\nSRS_Unidimensional &lt;- lavaan::cfa(SRS_UnidimensionalModel,\n    SRS_data,\n    ordered = paste0(\"SRS_\", 1:20),\n    orthogonal = TRUE\n)\n\n\nsemPaths(\n    SRS_Unidimensional, \n    intercepts = FALSE\n)\n\n\n\n\n\n\n\n\nEsaminiamo la bontà di adattamento.\n\nfit.subset &lt;- c(\n    \"chisq.scaled\", \"df\", \"pvalue.scaled\",\n    \"rmsea.scaled\", \"rmsea.pvalue.scale\",\n    \"rmsea.ci.lower.scaled\", \"rmsea.ci.upper.scaled\",\n    \"cfi\", \"tli\", \"srmr\"\n)\n\n\nfitmeasures(SRS_Unidimensional, fit.subset) |&gt; print()\n\n         chisq.scaled                    df         pvalue.scaled \n             2087.431               170.000                 0.000 \n         rmsea.scaled rmsea.ci.lower.scaled rmsea.ci.upper.scaled \n                0.150                 0.145                 0.156 \n                  cfi                   tli                  srmr \n                0.961                 0.956                 0.119 \n\n\n\nSRS_BifactorModel &lt;-\n\"\n    SRS =~ SRS_1 + SRS_2 + SRS_3 + SRS_4 + SRS_5 + \n           SRS_6 + SRS_7 + SRS_8 + SRS_9 + SRS_10 +\n           SRS_11 + SRS_12 + SRS_13 + SRS_14 + SRS_15 +\n           SRS_16 + SRS_17 + SRS_18 + SRS_19 + SRS_20\n    Function =~ SRS_5 + SRS_9 + SRS_12 + SRS_15 + SRS_18\n    Pain =~ SRS_1 + SRS_2 + SRS_8 + SRS_11 + SRS_17\n    SelfImage =~ SRS_4 + SRS_6 + SRS_10 + SRS_14 + SRS_19\n    MentalHealth =~ SRS_3 + SRS_7 + SRS_13 + SRS_16 + SRS_20\n\"\n\nSRS_bifactor &lt;- lavaan::cfa(SRS_BifactorModel,\n    SRS_data,\n    ordered = paste0(\"SRS_\", 1:20),\n    orthogonal = TRUE\n)\n\n\nsemPaths(\n    SRS_bifactor,\n    intercepts = FALSE\n)\n\n\n\n\n\n\n\n\nEsaminiamo la bontà di adattamento.\n\nfitmeasures(SRS_bifactor, fit.subset) |&gt; print()\n\n         chisq.scaled                    df         pvalue.scaled \n              468.648               150.000                 0.000 \n         rmsea.scaled rmsea.ci.lower.scaled rmsea.ci.upper.scaled \n                0.065                 0.059                 0.072 \n                  cfi                   tli                  srmr \n                0.997                 0.996                 0.055 \n\n\nConfrontiamo i due modelli.\n\nlavTestLRT(SRS_Unidimensional, SRS_bifactor) |&gt; print()\n\n\nScaled Chi-Squared Difference Test (method = \"satorra.2000\")\n\nlavaan-&gt;lavTestLRT():  \n   lavaan NOTE: The \"Chisq\" column contains standard test statistics, not \n   the robust test that should be reported per model. A robust difference \n   test is a function of two standard (not robust) statistics.\n                    Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq)    \nSRS_bifactor       150           309                                  \nSRS_Unidimensional 170          1965       1007      20     &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nConsideriamo ora gli indici specifici per un modello bifattoriale.\n\nbifactorIndices(SRS_bifactor, UniLambda = SRS_Unidimensional) |&gt; print()\n\n$ModelLevelIndices\n   ECV.SRS        PUC  Omega.SRS OmegaH.SRS       ARPB \n     0.673      0.789      0.942      0.834      0.121 \n\n$FactorLevelIndices\n             ECV_SS ECV_SG ECV_GS Omega OmegaH     H    FD\nSRS           0.673 0.6728  0.673 0.942 0.8338 0.943 0.952\nFunction      0.197 0.0415  0.803 0.799 0.0912 0.403 0.710\nPain          0.412 0.1115  0.588 0.882 0.3427 0.696 0.921\nSelfImage     0.328 0.0818  0.672 0.850 0.2025 0.591 0.830\nMentalHealth  0.342 0.0923  0.658 0.892 0.2930 0.615 0.854\n\n$ItemLevelIndices\n        IECV RelParBias\nSRS_1  0.510     0.3534\nSRS_2  0.498     0.3675\nSRS_3  0.798     0.0366\nSRS_4  0.612     0.0522\nSRS_5  0.635     0.0226\nSRS_6  0.637     0.0881\nSRS_7  0.632     0.1789\nSRS_8  0.682     0.1034\nSRS_9  0.588     0.0192\nSRS_10 0.649     0.0642\nSRS_11 0.633     0.1217\nSRS_12 0.938     0.1001\nSRS_13 0.624     0.1179\nSRS_14 0.999     0.0990\nSRS_15 1.000     0.1011\nSRS_16 0.601     0.1898\nSRS_17 0.750     0.0392\nSRS_18 0.999     0.0933\nSRS_19 0.458     0.1664\nSRS_20 0.691     0.1048\n\n\n\nI ModelLevelIndices possono essere spiegati nel modo seguente:\n\nECV.SRS: Questo indica la proporzione di varianza spiegata dal fattore generale nel modello bifattoriale. ECV sta per “Explained Common Variance” (Varianza Comune Spiegata). Un valore più alto indica che una maggiore parte della varianza totale nei dati è spiegata dal fattore generale.\nPUC: Questo è l’acronimo di “Percentage of Uniqueness in Common” (Percentuale di Unicità nel Comune). Indica quanto della varianza unica (cioè quella non spiegata dal fattore generale) è presente nei fattori di gruppo. Un valore basso indica che i fattori di gruppo spiegano una maggiore parte della varianza unica nei dati.\nOmega.SRS: Questo indice rappresenta il coefficiente di affidabilità del fattore generale del modello bifattoriale. Indica quanto sia affidabile il fattore generale nel catturare la varianza comune tra tutti gli item del test. Un valore più alto indica maggiore affidabilità.\nOmegaH.SRS: Questo indice rappresenta il coefficiente di affidabilità dei fattori di gruppo nel modello bifattoriale. Indica quanto sia affidabile l’insieme dei fattori di gruppo nel catturare la varianza condivisa tra gli item del gruppo specifico. Anche qui, un valore più alto indica maggiore affidabilità.\nARPB: Questo sta per “Average Reproducibility of Parameter Estimates” (Riproducibilità Media delle Stime dei Parametri). Rappresenta la riproducibilità media delle stime dei parametri del modello bifattoriale. In sostanza, valuta quanto le stime dei parametri del modello sono affidabili e riproducibili.\n\nLa sezione dell’output FactorLevelIndices riguarda gli indici a livello di fattore del modello bifattoriale.\n\nECV_SS, ECV_SG, ECV_GS: Questi rappresentano rispettivamente la proporzione di varianza spiegata dal Fattore Generale (GG), dal Fattore Specifico (SS) e dall’Interazione tra Fattore Generale e Fattore Specifico (GS) per ciascun fattore. Indicano quanto ciascun tipo di varianza contribuisce alla spiegazione della varianza totale nell’insieme dei dati del fattore.\nOmega: Questo indice rappresenta il coefficiente di affidabilità dell’estratto del Fattore Generale per ciascun fattore. Indica quanto sia affidabile il Fattore Generale nel catturare la varianza comune tra gli item di quel particolare fattore. Un valore più alto indica maggiore affidabilità.\nOmegaH: Questo indice rappresenta il coefficiente di affidabilità dell’estratto del Fattore Specifico per ciascun fattore. Indica quanto sia affidabile l’insieme dei Fattori Specifici nel catturare la varianza condivisa tra gli item del gruppo specifico. Anche qui, un valore più alto indica maggiore affidabilità.\nH: Questo indice rappresenta la quota della varianza unica spiegata dal Fattore Generale per ciascun fattore. Indica quanto della varianza unica è spiegata dal Fattore Generale piuttosto che da fattori specifici.\nFD: Questo indice rappresenta la distorsione fattoriale, che è una misura di quanto i dati si adattino bene al modello bifattoriale. Valori vicini a 1 indicano un buon adattamento.\n\nInfine, l’output ItemLevelIndices riguarda gli indici a livello di item in un modello bifattoriale.\n\nIECV: Questo indica la proporzione di varianza spiegata dal Fattore Generale per ciascun item. IECV sta per “Item Explained Common Variance” (Varianza Comune Spiegata dell’Item). Indica quanto della varianza totale dell’item può essere spiegata dal Fattore Generale del modello bifattoriale. Valori più alti indicano che il Fattore Generale contribuisce maggiormente a spiegare le variazioni osservate nell’item.\nRelParBias: Questo rappresenta il bias relativo dei parametri dell’item. Indica quanto i parametri dell’item sono influenzati dalla presenza del Fattore Generale e dai fattori specifici nel modello. Valori più alti indicano una maggiore influenza dei fattori specifici rispetto al Fattore Generale nell’item.\n\nPer i dati dell’esempio considerato, di seguito è riportata un’interpretazione succinta dei risultati chiave per ciascun gruppo principale di risultati:\n\n43.3.1 Model-Level Indices\n\nECV.SRS (Explained Common Variance): Il 67.28% della varianza osservata è spiegata dal modello.\nPUC (Percentage of Uncontaminated Correlations): Il 78.95% delle correlazioni tra gli item è “puro”, cioè non contaminato da altri fattori oltre al fattore generale.\nOmega.SRS: La consistenza interna complessiva del test è molto alta (0.942), indicando una buona affidabilità.\nOmegaH.SRS: Il 83.38% della varianza totale standardizzata è attribuibile al fattore generale, confermando che è un fattore dominante nel modello.\nARPB (Average Relative Parameter Bias): Un bias relativo medio basso (0.121) suggerisce che le stime dei parametri sono relativamente poco distorte.\n\n\n\n43.3.2 Factor-Level Indices\n\nECV (Explained Common Variance) per i fattori specifici:\n\nFunzione: 19.73% della varianza è spiegata dal fattore specifico “Funzione”, con l’80.27% attribuibile al fattore generale.\nDolore (Pain): 41.24% della varianza è spiegata dal fattore specifico “Dolore”.\nAutopercezione (SelfImage): 32.80% della varianza è spiegata dal fattore specifico “Autopercezione”.\nSalute Mentale (MentalHealth): 34.24% della varianza è spiegata dal fattore specifico “Salute Mentale”.\n\nOmega e OmegaH per ogni fattore specifico: Le misure Omega indicano la consistenza interna per ciascun sottogruppo di item, mentre OmegaH indica la proporzione della varianza attribuibile ai fattori specifici rispetto al fattore generale.\n\n\n\n43.3.3 Item-Level Indices\n\nIECV (Item Explained Common Variance): Valori come 0.937 per SRS_12 e quasi 1 per SRS_14, SRS_15 e SRS_18 indicano che questi item sono molto influenzati dal fattore generale.\nRelParBias (Relative Parameter Bias): La maggior parte degli item mostra un bias relativo basso, suggerendo che gli effetti dei fattori specifici su questi item sono correttamente rappresentati senza grande distorsione.\n\nIn sintesi, il modello bifattoriale sembra adattarsi bene ai dati, con un forte fattore generale che domina la struttura del test, supportato da alcuni fattori specifici che spiegano porzioni significative della varianza in diverse aree tematiche. Gli item individuati con alti valori di IECV sono particolarmente rappresentativi del fattore generale.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Modello bifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/05_bifactor.html#commenti-e-considerazioni-conclusive",
    "href": "chapters/cfa/05_bifactor.html#commenti-e-considerazioni-conclusive",
    "title": "43  Modello bifattoriale",
    "section": "43.4 Commenti e considerazioni conclusive",
    "text": "43.4 Commenti e considerazioni conclusive\nIn questo capitolo, abbiamo esplorato diversi indici derivati dall’analisi con un modello bifattoriale, ognuno dei quali rivela aspetti specifici delle proprietà psicometriche di uno strumento di misura. Questi indici sono di grande utilità per gli sviluppatori e i valutatori di scale, oltre a essere strumenti preziosi per i ricercatori e i professionisti che le impiegano nella pratica clinica e nella ricerca. Inoltre, contribuiscono allo sviluppo e alla comprensione dei costrutti psicologici che tali strumenti intendono misurare.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Modello bifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/05_bifactor.html#session-info",
    "href": "chapters/cfa/05_bifactor.html#session-info",
    "title": "43  Modello bifattoriale",
    "section": "43.5 Session Info",
    "text": "43.5 Session Info\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] BifactorIndicesCalculator_0.2.2 MASS_7.3-61                    \n [3] viridis_0.6.5                   viridisLite_0.4.2              \n [5] ggpubr_0.6.0                    ggExtra_0.10.1                 \n [7] gridExtra_2.3                   patchwork_1.3.0                \n [9] bayesplot_1.11.1                semTools_0.5-6                 \n[11] semPlot_1.1.6                   lavaan_0.6-19                  \n[13] psych_2.4.6.26                  scales_1.3.0                   \n[15] markdown_1.13                   knitr_1.49                     \n[17] lubridate_1.9.3                 forcats_1.0.0                  \n[19] stringr_1.5.1                   dplyr_1.1.4                    \n[21] purrr_1.0.2                     readr_2.1.5                    \n[23] tidyr_1.3.1                     tibble_3.2.1                   \n[25] ggplot2_3.5.1                   tidyverse_2.0.0                \n[27] here_1.0.1                     \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.17.1   jsonlite_1.8.9      magrittr_2.0.3     \n  [4] TH.data_1.1-2       estimability_1.5.1  farver_2.1.2       \n  [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n [10] Cairo_1.6-2         minqa_1.2.8         base64enc_0.1-3    \n [13] rstatix_0.7.2       htmltools_0.5.8.1   broom_1.0.7        \n [16] Formula_1.2-5       htmlwidgets_1.6.4   plyr_1.8.9         \n [19] sandwich_3.1-1      emmeans_1.10.5      zoo_1.8-12         \n [22] uuid_1.2-1          igraph_2.1.1        mime_0.12          \n [25] lifecycle_1.0.4     pkgconfig_2.0.3     Matrix_1.7-1       \n [28] R6_2.5.1            fastmap_1.2.0       shiny_1.9.1        \n [31] digest_0.6.37       OpenMx_2.21.13      fdrtool_1.2.18     \n [34] colorspace_2.1-1    rprojroot_2.0.4     Hmisc_5.2-0        \n [37] fansi_1.0.6         timechange_0.3.0    abind_1.4-8        \n [40] compiler_4.4.2      withr_3.0.2         glasso_1.11        \n [43] htmlTable_2.4.3     backports_1.5.0     carData_3.0-5      \n [46] ggsignif_0.6.4      corpcor_1.6.10      gtools_3.9.5       \n [49] tools_4.4.2         pbivnorm_0.6.0      foreign_0.8-87     \n [52] zip_2.3.1           httpuv_1.6.15       nnet_7.3-19        \n [55] glue_1.8.0          quadprog_1.5-8      promises_1.3.0     \n [58] nlme_3.1-166        lisrelToR_0.3       grid_4.4.2         \n [61] pbdZMQ_0.3-13       checkmate_2.3.2     cluster_2.1.6      \n [64] reshape2_1.4.4      generics_0.1.3      gtable_0.3.6       \n [67] tzdb_0.4.0          data.table_1.16.2   hms_1.1.3          \n [70] car_3.1-3           utf8_1.2.4          sem_3.1-16         \n [73] pillar_1.9.0        IRdisplay_1.1       rockchalk_1.8.157  \n [76] later_1.3.2         splines_4.4.2       cherryblossom_0.1.0\n [79] lattice_0.22-6      survival_3.7-0      kutils_1.73        \n [82] tidyselect_1.2.1    miniUI_0.1.1.1      pbapply_1.7-2      \n [85] airports_0.1.0      stats4_4.4.2        xfun_0.49          \n [88] qgraph_1.9.8        arm_1.14-4          stringi_1.8.4      \n [91] pacman_0.5.1        boot_1.3-31         evaluate_1.0.1     \n [94] codetools_0.2-20    mi_1.1              cli_3.6.3          \n [97] RcppParallel_5.1.9  IRkernel_1.3.2      rpart_4.1.23       \n[100] xtable_1.8-4        repr_1.1.7          munsell_0.5.1      \n[103] Rcpp_1.0.13-1       coda_0.19-4.1       png_0.1-8          \n[106] XML_3.99-0.17       parallel_4.4.2      usdata_0.3.1       \n[109] jpeg_0.1-10         lme4_1.1-35.5       mvtnorm_1.3-2      \n[112] openxlsx_4.2.7.1    crayon_1.5.3        openintro_2.5.0    \n[115] rlang_1.1.4         multcomp_1.4-26     mnormt_2.1.1",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Modello bifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/06_efa_lavaan.html",
    "href": "chapters/cfa/06_efa_lavaan.html",
    "title": "44  Exploratory Structural Equation Modeling",
    "section": "",
    "text": "44.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nL’Exploratory Structural Equation Modeling (ESEM) rappresenta un framework analitico innovativo che combina i vantaggi dell’Analisi Fattoriale Esplorativa (EFA) con il rigore della Confirmatory Factor Analysis (CFA). Questo approccio integrato consente di mantenere la flessibilità tipica dell’EFA, che permette di considerare le saturazioni incrociate tra i fattori, pur preservando la specificità e il controllo strutturale offerti dalla CFA. In particolare, l’ESEM si distingue per la sua capacità di bilanciare rigore metodologico e adattabilità, rendendolo adatto sia a contesti esplorativi che confermativi.\nUn aspetto chiave dell’ESEM è l’introduzione della rotazione target, una tecnica che facilita la definizione a priori dei carichi fattoriali principali, pur consentendo ai carichi incrociati di rimanere il più possibile vicini a zero, ma senza imporre vincoli rigidi. Questa caratteristica permette di applicare il modello in modo confermativo, basandosi su una struttura fattoriale predefinita, ma con una flessibilità tipicamente associata all’EFA. Di conseguenza, l’ESEM si presta efficacemente a contesti in cui è necessario convalidare ipotesi preesistenti, pur lasciando spazio all’esplorazione di relazioni inattese tra variabili.\nNella tradizionale Confirmatory Factor Analysis (CFA), ampiamente utilizzata in ambito psicologico, la struttura fattoriale è definita a priori: si assume che ogni indicatore carichi esclusivamente sul proprio fattore latente di riferimento, con saturazioni incrociate fissate a zero. Questo approccio, sebbene metodologicamente rigoroso, presenta limitazioni significative. In particolare, i modelli CFA tendono a essere eccessivamente restrittivi, presupponendo “fattori puri” in cui ogni item contribuisce solo al proprio costrutto latente. Tuttavia, nella pratica psicologica, molti item riflettono più di un costrutto, rendendo questa assunzione spesso irrealistica. Ignorare le saturazioni incrociate può portare a una rappresentazione distorta delle relazioni tra item e fattori, con conseguenti sovrastime delle statistiche di adattamento del modello e distorsioni positive nelle correlazioni tra fattori. Studi di simulazione hanno dimostrato che anche piccole saturazioni incrociate, se non considerate, possono alterare significativamente le stime dei parametri.\nUn ulteriore problema della CFA riguarda gli indici di bontà di adattamento, che risultano spesso troppo rigidi per strumenti psicologici multifattoriali. Questa rigidità rende difficile ottenere un adattamento soddisfacente senza apportare modifiche sostanziali ai modelli. Tuttavia, è importante notare che modelli con indici di adattamento non ottimali possono comunque presentare saturazioni ragionevoli e alti livelli di affidabilità quando analizzati a livello di item.\nProprio per superare queste limitazioni, l’ESEM si è affermato come un approccio più flessibile e robusto, in grado di cogliere la complessità delle misure psicologiche senza sacrificare il rigore metodologico. Grazie alla sua capacità di integrare i punti di forza dell’EFA e della CFA, l’ESEM offre un quadro analitico più realistico e adattabile, rendendolo uno strumento prezioso per la ricerca in ambito psicologico.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Exploratory Structural Equation Modeling</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/06_efa_lavaan.html#exploratory-structural-equation-modeling",
    "href": "chapters/cfa/06_efa_lavaan.html#exploratory-structural-equation-modeling",
    "title": "44  Exploratory Structural Equation Modeling",
    "section": "\n44.2 Exploratory Structural Equation Modeling",
    "text": "44.2 Exploratory Structural Equation Modeling\nL’ESEM combina elementi delle CFA e dell’Exploratory Factor Analysis (EFA) all’interno del tradizionale framework delle Equazioni Strutturali (SEM). Questo approccio rappresenta un compromesso tra la ricerca iterativa di soluzioni fattoriali ottimali, tipica dell’EFA, e la modellazione teorica restrittiva delle CFA.\nL’ESEM è essenzialmente un metodo confermativo che permette anche un’esplorazione attraverso l’uso di rotazioni mirate, mantenendo la presenza di caricamenti incrociati, seppur minimizzati. All’interno dell’ESEM, il ricercatore può prevedere a priori una struttura fattoriale, similmente a quanto avviene nelle CFA, ma con una maggiore flessibilità permessa dalla possibilità di modellare saturazioni incrociate.\nNell’ESEM, i fattori generali e specifici devono essere specificati come totalmente indipendenti, e le rotazioni ortogonali sono comuni nei modelli bifattoriali. I metodi di rotazione più usati nell’ESEM includono le rotazioni geomin e target, con rotazioni ortogonali adatte ai modelli più complessi.\nLe analisi di simulazione indicano che le correlazioni tra i fattori latenti ottenute con l’ESEM sono generalmente meno distorte e più vicine alle vere associazioni, rendendo i modelli ESEM più coerenti con le teorie sottostanti e le intenzioni degli strumenti psicometrici misurati.\nQuando un modello ESEM include solo una parte di misurazione, viene definito come “analisi fattoriale esplorativa” o EFA. Se il modello include anche una parte strutturale, come regressioni tra variabili latenti, è classificato come “modello di equazioni strutturali esplorativo” o ESEM.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Exploratory Structural Equation Modeling</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/06_efa_lavaan.html#un-esempio-pratico",
    "href": "chapters/cfa/06_efa_lavaan.html#un-esempio-pratico",
    "title": "44  Exploratory Structural Equation Modeling",
    "section": "\n44.3 Un Esempio Pratico",
    "text": "44.3 Un Esempio Pratico\nIn questo esempio pratico analizzeremo nuovamente i dati di Brown (2015), ovvero otto misure di personalità raccolte su un campione di 250 pazienti che hanno concluso un programma di psicoterapia. Utilizzeremo un’analisi EFA mediante la funzione efa() di lavaan.\nGli item sono i seguenti:\n\nanxiety (N1),\nhostility (N2),\ndepression (N3),\nself-consciousness (N4),\nwarmth (E1),\ngregariousness (E2),\nassertiveness (E3),\npositive emotions (E4).\n\n\nvarnames &lt;- c(\"N1\", \"N2\", \"N3\", \"N4\", \"E1\", \"E2\", \"E3\", \"E4\")\nsds &lt;- \"5.7  5.6  6.4  5.7  6.0  6.2  5.7  5.6\"\n\ncors &lt;- \"\n 1.000\n 0.767  1.000\n 0.731  0.709  1.000\n 0.778  0.738  0.762  1.000\n-0.351  -0.302  -0.356  -0.318  1.000\n-0.316  -0.280  -0.300  -0.267  0.675  1.000\n-0.296  -0.289  -0.297  -0.296  0.634  0.651  1.000\n-0.282  -0.254  -0.292  -0.245  0.534  0.593  0.566  1.000\"\n\npsychot_cor_mat &lt;- getCov(cors, names = varnames)\nn &lt;- 250\n\nDefiniamo un modello ad un solo fattore comune.\n\n# 1-factor model\nf1 &lt;- '\n    efa(\"efa\")*f1 =~ N1 + N2 + N3 + N4 + E1 + E2 + E3 + E4\n'\n\nDefiniamo un modello con due fattori comuni.\n\n# 2-factor model\nf2 &lt;- '\n    efa(\"efa\")*f1 +\n    efa(\"efa\")*f2 =~ N1 + N2 + N3 + N4 + E1 + E2 + E3 + E4\n'\n\nAdattiamo ai dati il modello ad un fattore comune.\n\nefa_f1 &lt;-cfa(\n    model = f1,\n    sample.cov = psychot_cor_mat,\n    sample.nobs = 250,\n    rotation = \"oblimin\"\n)\n\n\nsemPlot::semPaths(\n    efa_f1,\n    what = \"col\", whatLabels = \"no\", style = \"mx\",\n    layout = \"tree\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 5, sizeMan2 = 4\n)\n\n\n\n\n\n\n\nEsaminiamo la soluzione ottenuta.\n\nsummary(\n    efa_f1,\n    fit.measures = TRUE,\n    standardized = TRUE,\n    rsquare = TRUE\n) |&gt; \n    print()\n#&gt; lavaan 0.6-19 ended normally after 2 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        16\n#&gt; \n#&gt;   Rotation method                      OBLIMIN OBLIQUE\n#&gt;   Oblimin gamma                                      0\n#&gt;   Rotation algorithm (rstarts)                GPA (30)\n#&gt;   Standardized metric                             TRUE\n#&gt;   Row weights                                     None\n#&gt; \n#&gt;   Number of observations                           250\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                               375.327\n#&gt;   Degrees of freedom                                20\n#&gt;   P-value (Chi-square)                           0.000\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                              1253.791\n#&gt;   Degrees of freedom                                28\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.710\n#&gt;   Tucker-Lewis Index (TLI)                       0.594\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -2394.637\n#&gt;   Loglikelihood unrestricted model (H1)      -2206.974\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                4821.275\n#&gt;   Bayesian (BIC)                              4877.618\n#&gt;   Sample-size adjusted Bayesian (SABIC)       4826.897\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.267\n#&gt;   90 Percent confidence interval - lower         0.243\n#&gt;   90 Percent confidence interval - upper         0.291\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.000\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    1.000\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.187\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   f1 =~ efa                                                             \n#&gt;     N1                0.879    0.051   17.333    0.000    0.879    0.880\n#&gt;     N2                0.841    0.052   16.154    0.000    0.841    0.842\n#&gt;     N3                0.841    0.052   16.175    0.000    0.841    0.843\n#&gt;     N4                0.870    0.051   17.065    0.000    0.870    0.872\n#&gt;     E1               -0.438    0.062   -7.041    0.000   -0.438   -0.439\n#&gt;     E2               -0.398    0.063   -6.327    0.000   -0.398   -0.398\n#&gt;     E3               -0.398    0.063   -6.342    0.000   -0.398   -0.399\n#&gt;     E4               -0.364    0.063   -5.746    0.000   -0.364   -0.364\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .N1                0.224    0.028    7.915    0.000    0.224    0.225\n#&gt;    .N2                0.289    0.033    8.880    0.000    0.289    0.290\n#&gt;    .N3                0.288    0.032    8.866    0.000    0.288    0.289\n#&gt;    .N4                0.239    0.029    8.174    0.000    0.239    0.240\n#&gt;    .E1                0.804    0.073   10.963    0.000    0.804    0.807\n#&gt;    .E2                0.838    0.076   11.008    0.000    0.838    0.841\n#&gt;    .E3                0.837    0.076   11.007    0.000    0.837    0.841\n#&gt;    .E4                0.864    0.078   11.041    0.000    0.864    0.867\n#&gt;     f1                1.000                               1.000    1.000\n#&gt; \n#&gt; R-Square:\n#&gt;                    Estimate\n#&gt;     N1                0.775\n#&gt;     N2                0.710\n#&gt;     N3                0.711\n#&gt;     N4                0.760\n#&gt;     E1                0.193\n#&gt;     E2                0.159\n#&gt;     E3                0.159\n#&gt;     E4                0.133\n\n\nstandardizedSolution(efa_f1) |&gt; print()\n#&gt;    lhs op rhs est.std    se     z pvalue ci.lower ci.upper\n#&gt; 1   f1 =~  N1   0.880 0.018 48.29      0    0.845    0.916\n#&gt; 2   f1 =~  N2   0.842 0.022 38.61      0    0.800    0.885\n#&gt; 3   f1 =~  N3   0.843 0.022 38.76      0    0.800    0.886\n#&gt; 4   f1 =~  N4   0.872 0.019 45.88      0    0.835    0.909\n#&gt; 5   f1 =~  E1  -0.439 0.054 -8.18      0   -0.544   -0.334\n#&gt; 6   f1 =~  E2  -0.398 0.056 -7.14      0   -0.508   -0.289\n#&gt; 7   f1 =~  E3  -0.399 0.056 -7.16      0   -0.508   -0.290\n#&gt; 8   f1 =~  E4  -0.364 0.057 -6.35      0   -0.477   -0.252\n#&gt; 9   N1 ~~  N1   0.225 0.032  7.01      0    0.162    0.288\n#&gt; 10  N2 ~~  N2   0.290 0.037  7.90      0    0.218    0.362\n#&gt; 11  N3 ~~  N3   0.289 0.037  7.88      0    0.217    0.361\n#&gt; 12  N4 ~~  N4   0.240 0.033  7.23      0    0.175    0.305\n#&gt; 13  E1 ~~  E1   0.807 0.047 17.14      0    0.715    0.900\n#&gt; 14  E2 ~~  E2   0.841 0.044 18.93      0    0.754    0.928\n#&gt; 15  E3 ~~  E3   0.841 0.045 18.89      0    0.753    0.928\n#&gt; 16  E4 ~~  E4   0.867 0.042 20.72      0    0.785    0.949\n#&gt; 17  f1 ~~  f1   1.000 0.000    NA     NA    1.000    1.000\n\n\nlavaan::residuals(efa_f1, type = \"cor\") |&gt; print()\n#&gt; $type\n#&gt; [1] \"cor.bollen\"\n#&gt; \n#&gt; $cov\n#&gt;        N1     N2     N3     N4     E1     E2     E3     E4\n#&gt; N1  0.000                                                 \n#&gt; N2  0.025  0.000                                          \n#&gt; N3 -0.011 -0.001  0.000                                   \n#&gt; N4  0.010  0.003  0.027  0.000                            \n#&gt; E1  0.035  0.068  0.014  0.065  0.000                     \n#&gt; E2  0.035  0.056  0.036  0.080  0.500  0.000              \n#&gt; E3  0.055  0.047  0.040  0.052  0.459  0.492  0.000       \n#&gt; E4  0.039  0.053  0.015  0.073  0.374  0.448  0.421  0.000\n\nAdattiamo ai dati il modello a due fattori comuni.\n\nefa_f2 &lt;- cfa(\n    model = f2,\n    sample.cov = psychot_cor_mat,\n    sample.nobs = 250,\n    rotation = \"oblimin\"\n)\n\n\nsemPlot::semPaths(\n    efa_f2,\n    what = \"col\", whatLabels = \"no\", style = \"mx\",\n    layout = \"tree\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 5, sizeMan2 = 4\n)\n\n\n\n\n\n\n\nEsaminiamo la soluzione ottenuta.\n\nsummary(\n    efa_f2,\n    fit.measures = TRUE,\n    standardized = TRUE,\n    rsquare = TRUE\n) |&gt; print()\n#&gt; lavaan 0.6-19 ended normally after 1 iteration\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        25\n#&gt;   Row rank of the constraints matrix                 2\n#&gt; \n#&gt;   Rotation method                      OBLIMIN OBLIQUE\n#&gt;   Oblimin gamma                                      0\n#&gt;   Rotation algorithm (rstarts)                GPA (30)\n#&gt;   Standardized metric                             TRUE\n#&gt;   Row weights                                     None\n#&gt; \n#&gt;   Number of observations                           250\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                 9.811\n#&gt;   Degrees of freedom                                13\n#&gt;   P-value (Chi-square)                           0.709\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                              1253.791\n#&gt;   Degrees of freedom                                28\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    1.000\n#&gt;   Tucker-Lewis Index (TLI)                       1.006\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -2211.879\n#&gt;   Loglikelihood unrestricted model (H1)      -2206.974\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                4469.758\n#&gt;   Bayesian (BIC)                              4550.752\n#&gt;   Sample-size adjusted Bayesian (SABIC)       4477.840\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.000\n#&gt;   90 Percent confidence interval - lower         0.000\n#&gt;   90 Percent confidence interval - upper         0.048\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.957\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.001\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.010\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   f1 =~ efa                                                             \n#&gt;     N1                0.874    0.053   16.592    0.000    0.874    0.876\n#&gt;     N2                0.851    0.055   15.551    0.000    0.851    0.853\n#&gt;     N3                0.826    0.054   15.179    0.000    0.826    0.828\n#&gt;     N4                0.896    0.053   16.802    0.000    0.896    0.898\n#&gt;     E1               -0.046    0.040   -1.138    0.255   -0.046   -0.046\n#&gt;     E2                0.035    0.034    1.030    0.303    0.035    0.035\n#&gt;     E3                0.000    0.040    0.010    0.992    0.000    0.000\n#&gt;     E4               -0.006    0.049   -0.131    0.896   -0.006   -0.006\n#&gt;   f2 =~ efa                                                             \n#&gt;     N1               -0.017    0.032   -0.539    0.590   -0.017   -0.017\n#&gt;     N2                0.011    0.035    0.322    0.748    0.011    0.011\n#&gt;     N3               -0.035    0.036   -0.949    0.343   -0.035   -0.035\n#&gt;     N4                0.031    0.031    0.994    0.320    0.031    0.031\n#&gt;     E1                0.776    0.059   13.125    0.000    0.776    0.778\n#&gt;     E2                0.854    0.058   14.677    0.000    0.854    0.855\n#&gt;     E3                0.785    0.060   13.106    0.000    0.785    0.787\n#&gt;     E4                0.695    0.063   10.955    0.000    0.695    0.697\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   f1 ~~                                                                 \n#&gt;     f2               -0.432    0.059   -7.345    0.000   -0.432   -0.432\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .N1                0.218    0.028    7.790    0.000    0.218    0.219\n#&gt;    .N2                0.279    0.032    8.693    0.000    0.279    0.280\n#&gt;    .N3                0.287    0.032    8.907    0.000    0.287    0.289\n#&gt;    .N4                0.216    0.029    7.578    0.000    0.216    0.217\n#&gt;    .E1                0.361    0.044    8.226    0.000    0.361    0.362\n#&gt;    .E2                0.292    0.043    6.787    0.000    0.292    0.293\n#&gt;    .E3                0.379    0.046    8.315    0.000    0.379    0.381\n#&gt;    .E4                0.509    0.053    9.554    0.000    0.509    0.511\n#&gt;     f1                1.000                               1.000    1.000\n#&gt;     f2                1.000                               1.000    1.000\n#&gt; \n#&gt; R-Square:\n#&gt;                    Estimate\n#&gt;     N1                0.781\n#&gt;     N2                0.720\n#&gt;     N3                0.711\n#&gt;     N4                0.783\n#&gt;     E1                0.638\n#&gt;     E2                0.707\n#&gt;     E3                0.619\n#&gt;     E4                0.489\n\n\nstandardizedSolution(efa_f2) |&gt; print()\n#&gt;    lhs op rhs est.std    se      z pvalue ci.lower ci.upper\n#&gt; 1   f1 =~  N1   0.876 0.024 36.440  0.000    0.829    0.923\n#&gt; 2   f1 =~  N2   0.853 0.027 31.403  0.000    0.800    0.906\n#&gt; 3   f1 =~  N3   0.828 0.028 29.069  0.000    0.772    0.884\n#&gt; 4   f1 =~  N4   0.898 0.023 38.383  0.000    0.852    0.944\n#&gt; 5   f1 =~  E1  -0.046 0.040 -1.139  0.255   -0.125    0.033\n#&gt; 6   f1 =~  E2   0.035 0.034  1.031  0.303   -0.031    0.101\n#&gt; 7   f1 =~  E3   0.000 0.040  0.010  0.992   -0.078    0.079\n#&gt; 8   f1 =~  E4  -0.006 0.049 -0.131  0.896   -0.103    0.090\n#&gt; 9   f2 =~  N1  -0.017 0.032 -0.539  0.590   -0.079    0.045\n#&gt; 10  f2 =~  N2   0.011 0.035  0.322  0.748   -0.058    0.080\n#&gt; 11  f2 =~  N3  -0.035 0.037 -0.949  0.343   -0.106    0.037\n#&gt; 12  f2 =~  N4   0.031 0.031  0.994  0.320   -0.030    0.092\n#&gt; 13  f2 =~  E1   0.778 0.038 20.654  0.000    0.704    0.852\n#&gt; 14  f2 =~  E2   0.855 0.033 26.036  0.000    0.791    0.920\n#&gt; 15  f2 =~  E3   0.787 0.038 20.886  0.000    0.713    0.861\n#&gt; 16  f2 =~  E4   0.697 0.046 15.282  0.000    0.607    0.786\n#&gt; 17  N1 ~~  N1   0.219 0.032  6.905  0.000    0.157    0.281\n#&gt; 18  N2 ~~  N2   0.280 0.036  7.727  0.000    0.209    0.351\n#&gt; 19  N3 ~~  N3   0.289 0.036  7.909  0.000    0.217    0.360\n#&gt; 20  N4 ~~  N4   0.217 0.032  6.751  0.000    0.154    0.280\n#&gt; 21  E1 ~~  E1   0.362 0.047  7.673  0.000    0.269    0.454\n#&gt; 22  E2 ~~  E2   0.293 0.046  6.322  0.000    0.202    0.384\n#&gt; 23  E3 ~~  E3   0.381 0.049  7.816  0.000    0.285    0.476\n#&gt; 24  E4 ~~  E4   0.511 0.053  9.631  0.000    0.407    0.615\n#&gt; 25  f1 ~~  f1   1.000 0.000     NA     NA    1.000    1.000\n#&gt; 26  f2 ~~  f2   1.000 0.000     NA     NA    1.000    1.000\n#&gt; 27  f1 ~~  f2  -0.432 0.059 -7.345  0.000   -0.547   -0.317\n\nAnche se abbiamo introdotto finora soltanto la misura di bontà di adattamento del chi-quadrato, aggiungiamo qui il calcolo di altre misure di bontà di adattamento che discuteremo in seguito.\n\nfit_measures_robust &lt;- c(\n    \"chisq\", \"df\", \"pvalue\", \"cfi\", \"rmsea\", \"srmr\"\n)\n\nConfrontiamo le misure di bontà di adattamento del modello che ipotizza un solo fattore comune e il modello che ipotizza la presenza di due fattori comuni.\n\n# collect them for each model\nrbind(\n    fitmeasures(efa_f1, fit_measures_robust),\n    fitmeasures(efa_f2, fit_measures_robust)\n) |&gt;\n    # wrangle\n    data.frame() |&gt;\n    mutate(\n        chisq = round(chisq, digits = 0),\n        df = as.integer(df),\n        pvalue = ifelse(pvalue == 0, \"&lt; .001\", pvalue)\n    ) |&gt;\n    mutate_at(vars(cfi:srmr), ~ round(., digits = 3)) |&gt;\n    print()\n#&gt;   chisq df            pvalue  cfi rmsea  srmr\n#&gt; 1   375 20            &lt; .001 0.71 0.267 0.187\n#&gt; 2    10 13 0.709310449320098 1.00 0.000 0.010\n\n\nlavaan::residuals(efa_f2, type = \"cor\") |&gt; print()\n#&gt; $type\n#&gt; [1] \"cor.bollen\"\n#&gt; \n#&gt; $cov\n#&gt;        N1     N2     N3     N4     E1     E2     E3     E4\n#&gt; N1  0.000                                                 \n#&gt; N2  0.018  0.000                                          \n#&gt; N3 -0.014 -0.006  0.000                                   \n#&gt; N4 -0.003 -0.013  0.017  0.000                            \n#&gt; E1 -0.003  0.015 -0.012  0.000  0.000                     \n#&gt; E2 -0.009 -0.004  0.006  0.007  0.006  0.000              \n#&gt; E3  0.015 -0.008  0.011 -0.016  0.006 -0.010  0.000       \n#&gt; E4 -0.001  0.000 -0.013  0.009 -0.024  0.006  0.016  0.000\n\nL’evidenza empirica supporta la superiorità del modello a due fattori rispetto a quello ad un solo fattore comune. In particolare, l’analisi fattoriale esplorativa svolta mediante la funzione efa() evidenzia la capacità del modello a due fattori di fornire una descrizione adeguata della struttura dei dati e di distinguere in modo sensato tra i due fattori ipotizzati.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Exploratory Structural Equation Modeling</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/06_efa_lavaan.html#esem-within-cfa",
    "href": "chapters/cfa/06_efa_lavaan.html#esem-within-cfa",
    "title": "44  Exploratory Structural Equation Modeling",
    "section": "\n44.4 ESEM-within-CFA",
    "text": "44.4 ESEM-within-CFA\nUn approccio alternativo per eseguire un modello ESEM è quello proposto da Marsh et al. (2014), noto come ESEM-within-CFA. Questo metodo prevede di eseguire prima la parte esplorativa (EFA) e poi utilizzare i risultati ottenuti come valori iniziali per un modello CFA. In pratica, si combina la flessibilità dell’EFA con il rigore della CFA, sfruttando i vantaggi di entrambi gli approcci.\nPer illustrare questo metodo, seguiremo il tutorial di esemComp e utilizzeremo il dataset di Holzinger e Swineford (1939), disponibile nel pacchetto lavaan di R. Questo dataset contiene i risultati di 301 bambini in test che misurano tre abilità cognitive:\n\n\nAbilità visiva (item x1-x3),\n\n\nAbilità testuale (item x4-x6),\n\n\nAbilità di velocità (item x7-x9).\n\nPer iniziare, carichiamo il dataset e selezioniamo solo le colonne relative agli item di interesse. Questo ci permetterà di concentrarci sulle variabili rilevanti per l’analisi ESEM.\n\n#load full data\nhw_data &lt;- lavaan::HolzingerSwineford1939\n# keep all rows and only the item-columns\nhw_data &lt;- hw_data[, c(7:15)]\n\n#take a look\nhead(hw_data)\n#&gt;     x1   x2    x3   x4   x5    x6   x7   x8   x9\n#&gt; 1 3.33 7.75 0.375 2.33 5.75 1.286 3.39 5.75 6.36\n#&gt; 2 5.33 5.25 2.125 1.67 3.00 1.286 3.78 6.25 7.92\n#&gt; 3 4.50 5.25 1.875 1.00 1.75 0.429 3.26 3.90 4.42\n#&gt; 4 5.33 7.75 3.000 2.67 4.50 2.429 3.00 5.30 4.86\n#&gt; 5 4.83 4.75 0.875 2.67 4.00 2.571 3.70 6.30 5.92\n#&gt; 6 5.33 5.00 2.250 1.00 3.00 0.857 4.35 6.65 7.50\n\n\n44.4.1 Blocchi EFA (Analisi Fattoriale Esplorativa)\nPer eseguire un’Analisi Fattoriale Esplorativa (EFA) con una rotazione target, prima dobbiamo specificare la matrice di rotazione target. La funzione make_target() semplifica questo processo. Per far funzionare questa funzione, dobbiamo indicare la corrispondenza tra i fattori e i loro principali carichi (loadings), ovvero quali elementi ci aspettiamo che abbiano un carico elevato su ciascun fattore.\nQuesta informazione deve essere contenuta in una lista, dove il nome di ogni elemento è il nome del fattore, e il contenuto è un vettore numerico con il numero di colonna degli elementi che si riferiscono a quel fattore.\nSe controlliamo il dataset e i fattori, vediamo che la corrispondenza tra fattore e numero di colonna dell’elemento è piuttosto chiara in questo dataset. Le prime tre colonne si riferiscono agli elementi del primo fattore, le successive tre colonne sono gli elementi del secondo fattore, e così via. Tuttavia, questo potrebbe non essere il caso nel tuo dataset! Molte scale hanno elementi correlati a diversi fattori alternati, il che porta a elementi non sequenziali che si riferiscono allo stesso fattore.\nÈ importante ricordare che il numero dell’elemento per la matrice di rotazione si riferisce sempre alla posizione della colonna dell’elemento all’interno di un dataframe che contiene solo i dati degli elementi (ricorda che all’inizio di questa guida abbiamo creato un dataset separato contenente solo i dati degli elementi). Il numero più basso sarà sempre uno, e il numero più alto sarà il totale degli elementi.\n\n# list with mapping between factors and items\nmain_loadings_list &lt;- list(visual = c(1:3),\n                           textual = c(4:6),\n                           speed = c(7:9))\ntarget_rot &lt;- make_target(nitems = 9, mainloadings = main_loadings_list)\ntarget_rot\n#&gt;       visual textual speed\n#&gt;  [1,]     NA       0     0\n#&gt;  [2,]     NA       0     0\n#&gt;  [3,]     NA       0     0\n#&gt;  [4,]      0      NA     0\n#&gt;  [5,]      0      NA     0\n#&gt;  [6,]      0      NA     0\n#&gt;  [7,]      0       0    NA\n#&gt;  [8,]      0       0    NA\n#&gt;  [9,]      0       0    NA\n\nNella matrice di rotazione target, i valori NA indicano carichi (loadings) che non devono essere avvicinati a zero durante la procedura di rotazione, mentre gli zeri indicano il contrario, ovvero che quei carichi dovrebbero essere avvicinati a zero.\nÈ altresì possibile realizzare facilmente una rotazione target per un modello bifattoriale.\n\nbifactor_target_rot &lt;- make_target(nitems = 9,\n                                  mainloadings = main_loadings_list,\n                                  bifactor = TRUE)\nbifactor_target_rot\n#&gt;       visual textual speed  G\n#&gt;  [1,]     NA       0     0 NA\n#&gt;  [2,]     NA       0     0 NA\n#&gt;  [3,]     NA       0     0 NA\n#&gt;  [4,]      0      NA     0 NA\n#&gt;  [5,]      0      NA     0 NA\n#&gt;  [6,]      0      NA     0 NA\n#&gt;  [7,]      0       0    NA NA\n#&gt;  [8,]      0       0    NA NA\n#&gt;  [9,]      0       0    NA NA\n\nOra, per l’estrazione dei carichi (loadings) utilizzando la funzione esem_efa(), dobbiamo fornire i dati, il numero di fattori da estrarre e la matrice di rotazione target.\n\n# Specify the efa block.\n# Note that if we continued with the bifactor model 'nfactors' would then be specified as 4 and not 3 due to the G factor being added\n\nefa_block &lt;- esem_efa(data = hw_data,\n                      nfactors = 3,\n                      target = target_rot)\n#&gt; Loading required namespace: GPArotation\nefa_block\n#&gt; Factor Analysis using method =  pa\n#&gt; Call: psych::fa(r = data, nfactors = nfactors, rotate = targetAlgorithm, \n#&gt;     fm = fm, Target = target)\n#&gt; Standardized loadings (pattern matrix) based upon correlation matrix\n#&gt;      PA2   PA3   PA1   h2   u2 com\n#&gt; x1  0.18  0.08  0.58 0.48 0.52 1.2\n#&gt; x2  0.03 -0.09  0.52 0.26 0.74 1.1\n#&gt; x3 -0.08  0.08  0.67 0.45 0.55 1.1\n#&gt; x4  0.85  0.01  0.02 0.73 0.27 1.0\n#&gt; x5  0.89  0.00 -0.06 0.75 0.25 1.0\n#&gt; x6  0.80 -0.01  0.08 0.69 0.31 1.0\n#&gt; x7  0.04  0.75 -0.24 0.51 0.49 1.2\n#&gt; x8 -0.05  0.72  0.03 0.52 0.48 1.0\n#&gt; x9  0.01  0.50  0.32 0.46 0.54 1.7\n#&gt; \n#&gt;                        PA2  PA3  PA1\n#&gt; SS loadings           2.22 1.38 1.26\n#&gt; Proportion Var        0.25 0.15 0.14\n#&gt; Cumulative Var        0.25 0.40 0.54\n#&gt; Proportion Explained  0.46 0.28 0.26\n#&gt; Cumulative Proportion 0.46 0.74 1.00\n#&gt; \n#&gt;  With factor correlations of \n#&gt;      PA2  PA3  PA1\n#&gt; PA2 1.00 0.26 0.34\n#&gt; PA3 0.26 1.00 0.31\n#&gt; PA1 0.34 0.31 1.00\n#&gt; \n#&gt; Mean item complexity =  1.1\n#&gt; Test of the hypothesis that 3 factors are sufficient.\n#&gt; \n#&gt; df null model =  36  with the objective function =  3.05 with Chi Square =  904\n#&gt; df of  the model are 12  and the objective function was  0.08 \n#&gt; \n#&gt; The root mean square of the residuals (RMSR) is  0.02 \n#&gt; The df corrected root mean square of the residuals is  0.03 \n#&gt; \n#&gt; The harmonic n.obs is  301 with the empirical chi square  7.87  with prob &lt;  0.8 \n#&gt; The total n.obs was  301  with Likelihood Chi Square =  22.5  with prob &lt;  0.032 \n#&gt; \n#&gt; Tucker Lewis Index of factoring reliability =  0.963\n#&gt; RMSEA index =  0.054  and the 90 % confidence intervals are  0.016 0.088\n#&gt; BIC =  -46\n#&gt; Fit based upon off diagonal values = 1\n#&gt; Measures of factor score adequacy             \n#&gt;                                                    PA2  PA3  PA1\n#&gt; Correlation of (regression) scores with factors   0.94 0.86 0.84\n#&gt; Multiple R square of scores with factors          0.89 0.74 0.70\n#&gt; Minimum correlation of possible factor scores     0.78 0.48 0.40\n\nLa funzione esem_efa() è in realtà un wrapper intorno alla funzione fa() del pacchetto psych, utilizzata per l’analisi fattoriale esplorativa. Tutti i controlli disponibili in fa() possono essere specificati per una maggiore personalizzazione della procedura di estrazione dei fattori. Assicurati sempre di fornire gli argomenti della funzione originale utilizzando la sintassi nome = valore. Consulta la documentazione di fa() per ulteriori informazioni sui controlli e sui campi presenti nell’oggetto di output.\nPer impostazione predefinita, viene utilizzata una rotazione obliqua come rotazione target. L’utente può scegliere di passare a una rotazione ortogonale impostando il parametro targetAlgorithm su TargetT. Un’altra opzione è quella di eliminare completamente l’utilizzo della rotazione target e optare invece per una rotazione Geomin. In questo caso, basta non specificare il parametro target. Un’ultima possibilità è disponibile per i modelli bifattoriali: in questo caso, basta impostare bifactor = TRUE. Attualmente, i modelli bifattoriali sono supportati solo con la rotazione target.\nIl codice per ciascuno di questi casi è riportato di seguito (commentato).\n\n# geomin rotation\nesem_efa(data = hw_data,\n         nfactors = 3)\n#&gt; Factor Analysis using method =  pa\n#&gt; Call: psych::fa(r = data, nfactors = nfactors, rotate = \"geominQ\", \n#&gt;     fm = fm)\n#&gt; Standardized loadings (pattern matrix) based upon correlation matrix\n#&gt;      PA1   PA3   PA2   h2   u2 com\n#&gt; x1  0.19  0.59  0.03 0.48 0.52 1.2\n#&gt; x2  0.04  0.51 -0.12 0.26 0.74 1.1\n#&gt; x3 -0.07  0.69  0.02 0.45 0.55 1.0\n#&gt; x4  0.84  0.02  0.01 0.73 0.27 1.0\n#&gt; x5  0.88 -0.06  0.01 0.75 0.25 1.0\n#&gt; x6  0.80  0.08 -0.01 0.69 0.31 1.0\n#&gt; x7  0.03 -0.14  0.73 0.51 0.49 1.1\n#&gt; x8 -0.04  0.14  0.69 0.52 0.48 1.1\n#&gt; x9  0.02  0.39  0.45 0.46 0.54 2.0\n#&gt; \n#&gt;                        PA1  PA3  PA2\n#&gt; SS loadings           2.23 1.36 1.27\n#&gt; Proportion Var        0.25 0.15 0.14\n#&gt; Cumulative Var        0.25 0.40 0.54\n#&gt; Proportion Explained  0.46 0.28 0.26\n#&gt; Cumulative Proportion 0.46 0.74 1.00\n#&gt; \n#&gt;  With factor correlations of \n#&gt;      PA1  PA3  PA2\n#&gt; PA1 1.00 0.32 0.22\n#&gt; PA3 0.32 1.00 0.25\n#&gt; PA2 0.22 0.25 1.00\n#&gt; \n#&gt; Mean item complexity =  1.2\n#&gt; Test of the hypothesis that 3 factors are sufficient.\n#&gt; \n#&gt; df null model =  36  with the objective function =  3.05 with Chi Square =  904\n#&gt; df of  the model are 12  and the objective function was  0.08 \n#&gt; \n#&gt; The root mean square of the residuals (RMSR) is  0.02 \n#&gt; The df corrected root mean square of the residuals is  0.03 \n#&gt; \n#&gt; The harmonic n.obs is  301 with the empirical chi square  7.87  with prob &lt;  0.8 \n#&gt; The total n.obs was  301  with Likelihood Chi Square =  22.5  with prob &lt;  0.032 \n#&gt; \n#&gt; Tucker Lewis Index of factoring reliability =  0.963\n#&gt; RMSEA index =  0.054  and the 90 % confidence intervals are  0.016 0.088\n#&gt; BIC =  -46\n#&gt; Fit based upon off diagonal values = 1\n#&gt; Measures of factor score adequacy             \n#&gt;                                                    PA1  PA3  PA2\n#&gt; Correlation of (regression) scores with factors   0.94 0.84 0.85\n#&gt; Multiple R square of scores with factors          0.89 0.71 0.72\n#&gt; Minimum correlation of possible factor scores     0.78 0.42 0.44\n\n\n# orthogonal target rotation\nesem_efa(data = hw_data,\n         nfactors = 3,\n         target = target_rot,\n         targetAlgorithm = \"TargetT\")\n#&gt; Factor Analysis using method =  pa\n#&gt; Call: psych::fa(r = data, nfactors = nfactors, rotate = targetAlgorithm, \n#&gt;     fm = fm, Target = target)\n#&gt; Standardized loadings (pattern matrix) based upon correlation matrix\n#&gt;     PA2   PA3   PA1   h2   u2 com\n#&gt; x1 0.31  0.18  0.59 0.48 0.52 1.7\n#&gt; x2 0.12 -0.01  0.49 0.26 0.74 1.1\n#&gt; x3 0.07  0.16  0.65 0.45 0.55 1.2\n#&gt; x4 0.84  0.09  0.12 0.73 0.27 1.1\n#&gt; x5 0.86  0.08  0.05 0.75 0.25 1.0\n#&gt; x6 0.81  0.08  0.18 0.69 0.31 1.1\n#&gt; x7 0.10  0.70 -0.12 0.51 0.49 1.1\n#&gt; x8 0.07  0.71  0.13 0.52 0.48 1.1\n#&gt; x9 0.16  0.54  0.38 0.46 0.54 2.0\n#&gt; \n#&gt;                        PA2  PA3  PA1\n#&gt; SS loadings           2.26 1.36 1.24\n#&gt; Proportion Var        0.25 0.15 0.14\n#&gt; Cumulative Var        0.25 0.40 0.54\n#&gt; Proportion Explained  0.47 0.28 0.25\n#&gt; Cumulative Proportion 0.47 0.75 1.00\n#&gt; \n#&gt; Mean item complexity =  1.3\n#&gt; Test of the hypothesis that 3 factors are sufficient.\n#&gt; \n#&gt; df null model =  36  with the objective function =  3.05 with Chi Square =  904\n#&gt; df of  the model are 12  and the objective function was  0.08 \n#&gt; \n#&gt; The root mean square of the residuals (RMSR) is  0.02 \n#&gt; The df corrected root mean square of the residuals is  0.03 \n#&gt; \n#&gt; The harmonic n.obs is  301 with the empirical chi square  7.87  with prob &lt;  0.8 \n#&gt; The total n.obs was  301  with Likelihood Chi Square =  22.5  with prob &lt;  0.032 \n#&gt; \n#&gt; Tucker Lewis Index of factoring reliability =  0.963\n#&gt; RMSEA index =  0.054  and the 90 % confidence intervals are  0.016 0.088\n#&gt; BIC =  -46\n#&gt; Fit based upon off diagonal values = 1\n#&gt; Measures of factor score adequacy             \n#&gt;                                                    PA2  PA3  PA1\n#&gt; Correlation of (regression) scores with factors   0.94 0.84 0.81\n#&gt; Multiple R square of scores with factors          0.88 0.71 0.65\n#&gt; Minimum correlation of possible factor scores     0.75 0.42 0.30\n\n\n# bifactor model\nesem_efa(data = hw_data,\n         nfactors = 4,\n         target = bifactor_target_rot,\n         SMC=FALSE,\n         bifactor = TRUE)\n#&gt; Factor Analysis using method =  pa\n#&gt; Call: psych::fa(r = data, nfactors = nfactors, rotate = \"TargetT\", \n#&gt;     SMC = FALSE, fm = fm, Target = target)\n#&gt; Standardized loadings (pattern matrix) based upon correlation matrix\n#&gt;     PA4   PA2   PA3   PA1   h2    u2 com\n#&gt; x1 0.74  0.08 -0.02  0.00 0.55 0.449 1.0\n#&gt; x2 0.41  0.01 -0.06  0.90 0.97 0.029 1.4\n#&gt; x3 0.62 -0.12  0.01  0.10 0.41 0.593 1.1\n#&gt; x4 0.40  0.75  0.02 -0.03 0.73 0.273 1.5\n#&gt; x5 0.32  0.81  0.04  0.01 0.76 0.240 1.3\n#&gt; x6 0.42  0.71  0.02  0.02 0.69 0.312 1.6\n#&gt; x7 0.12  0.09  0.67 -0.10 0.48 0.519 1.1\n#&gt; x8 0.30  0.00  0.67  0.01 0.55 0.453 1.4\n#&gt; x9 0.53  0.01  0.42  0.02 0.46 0.543 1.9\n#&gt; \n#&gt;                        PA4  PA2  PA3  PA1\n#&gt; SS loadings           1.92 1.76 1.09 0.82\n#&gt; Proportion Var        0.21 0.20 0.12 0.09\n#&gt; Cumulative Var        0.21 0.41 0.53 0.62\n#&gt; Proportion Explained  0.34 0.31 0.19 0.15\n#&gt; Cumulative Proportion 0.34 0.66 0.85 1.00\n#&gt; \n#&gt; Mean item complexity =  1.4\n#&gt; Test of the hypothesis that 4 factors are sufficient.\n#&gt; \n#&gt; df null model =  36  with the objective function =  3.05 with Chi Square =  904\n#&gt; df of  the model are 6  and the objective function was  0.06 \n#&gt; \n#&gt; The root mean square of the residuals (RMSR) is  0.02 \n#&gt; The df corrected root mean square of the residuals is  0.04 \n#&gt; \n#&gt; The harmonic n.obs is  301 with the empirical chi square  5.13  with prob &lt;  0.53 \n#&gt; The total n.obs was  301  with Likelihood Chi Square =  18  with prob &lt;  0.0064 \n#&gt; \n#&gt; Tucker Lewis Index of factoring reliability =  0.917\n#&gt; RMSEA index =  0.081  and the 90 % confidence intervals are  0.04 0.126\n#&gt; BIC =  -16.3\n#&gt; Fit based upon off diagonal values = 1\n#&gt; Measures of factor score adequacy             \n#&gt;                                                    PA4  PA2  PA3  PA1\n#&gt; Correlation of (regression) scores with factors   0.85 0.90 0.81 0.95\n#&gt; Multiple R square of scores with factors          0.72 0.82 0.66 0.91\n#&gt; Minimum correlation of possible factor scores     0.45 0.64 0.32 0.81\n\nÈ certo utile poter eseguire queste analisi fattoriali utilizzando l’approccio ESEM (Exploratory Structural Equation Modeling), ma ciò non offre ancora la grande flessibilità ed estensibilità propria dei modelli CFA/SEM (Confirmatory Factor Analysis / Structural Equation Modeling). Per accedere a queste funzionalità, utilizzeremo successivamente l’approccio ESEM-all’interno-del-CFA.\n\n44.4.2 ESEM-all-interno-del-CFA\nUna volta ottenuta un’analisi fattoriale esplorativa (EFA) realizzata con l’approccio ESEM, è sufficiente utilizzare la funzione syntax_composer() per “comporre” il modello ESEM-all-interno-del-CFA utilizzando la sintassi di lavaan. Successivamente, con la sintassi generata, possiamo eseguire il fitting del modello in lavaan.\nLa funzione syntax_composer() richiede come primo argomento una soluzione EFA e come secondo argomento una lista denominata che indica i referenti (indicatori di riferimento) per ciascun fattore. Ogni voce della lista deve avere il formato fattore = \"nome_elemento\". È fondamentale che questa lista rispetti lo stesso ordine in cui i fattori appaiono nella matrice dei carichi fattoriali della soluzione EFA. Di solito, questo ordine non corrisponde a quello utilizzato nella lista per creare la rotazione target, poiché nella matrice EFA i fattori sono ordinati in base alla quantità di varianza spiegata, non in base all’ordine fornito dall’utente.\nPer esempio, controllando i carichi fattoriali, possiamo dedurre che nell’esempio in questione l’ordine nella matrice dei carichi fattoriali è “testuale, velocità, visivo”. Questo ordine non coincide con quello utilizzato in make_target(), dove abbiamo specificato “visivo, testuale, velocità”.\n\nefa_block$loadings\n#&gt; \n#&gt; Loadings:\n#&gt;    PA2    PA3    PA1   \n#&gt; x1  0.179         0.577\n#&gt; x2                0.515\n#&gt; x3                0.670\n#&gt; x4  0.845              \n#&gt; x5  0.886              \n#&gt; x6  0.803              \n#&gt; x7         0.745 -0.240\n#&gt; x8         0.724       \n#&gt; x9         0.504  0.317\n#&gt; \n#&gt;                  PA2   PA3   PA1\n#&gt; SS loadings    2.189 1.354 1.218\n#&gt; Proportion Var 0.243 0.150 0.135\n#&gt; Cumulative Var 0.243 0.394 0.529\n\nQuando esaminiamo la matrice dei carichi fattoriali, possiamo anche scegliere qual è il miglior referente (indicatore di riferimento) per ciascun fattore. Dovrebbe sempre essere un elemento che ha un carico elevato su un fattore e basso sugli altri. Quindi, per il fattore “testuale”, il referente sarà x5, per il fattore “velocità” sarà x8 e per il fattore “visivo” sarà x3. Creeremo la lista con questi elementi in quest’ordine.\n\n# create named character vector of referents\nhw_referents &lt;- list(textual = \"x5\",\n                     speed = \"x8\",\n                     visual = \"x3\")\n\nAlternativamente, è possibile utilizzare la funzione find_referents() per selezionare automaticamente i referenti (indicatori di riferimento). Gli input richiesti sono il risultato della funzione esem_efa() e un vettore di caratteri con i nomi desiderati per i fattori. Ancora una volta, i nomi devono corrispondere all’ordine in cui i fattori appaiono nella soluzione esplorativa.\n\nfind_referents(efa_block, c(\"textual\", \"speed\", \"visual\"))\n#&gt; $textual\n#&gt; [1] \"x5\"\n#&gt; \n#&gt; $speed\n#&gt; [1] \"x7\"\n#&gt; \n#&gt; $visual\n#&gt; [1] \"x3\"\n\nSi dovrebbe notare che i referenti scelti dalla funzione non sono esattamente gli stessi di quelli selezionati manualmente esaminando i carichi fattoriali; il referente per il fattore “velocità” differisce. Ciò accade perché l’attuale implementazione della funzione find_referents() cerca solo l’elemento con il carico più alto per ciascun fattore, senza considerare quanto bene tale elemento carichi su altri fattori.\nInfine, compiliamo la sintassi per lavaan utilizzando la funzione syntax_composer:\n\n# compose lavaan syntax\nmodel_syntax &lt;- syntax_composer(efa_object = efa_block,\n                                referents = hw_referents)\n\n# altenatively, if you plan fit the model with free factor variance parameters\nmodel_syntax_free_var &lt;- syntax_composer(efa_object = efa_block,\n                                referents = hw_referents,\n                                only_fix_crossloadings = FALSE)\n\nwriteLines(model_syntax)\n#&gt; textual =~ start(0.179)*x1+\n#&gt; start(0.03)*x2+\n#&gt; -0.082*x3+\n#&gt; start(0.845)*x4+\n#&gt; start(0.886)*x5+\n#&gt; start(0.803)*x6+\n#&gt; start(0.037)*x7+\n#&gt; -0.049*x8+\n#&gt; start(0.014)*x9 \n#&gt; \n#&gt; speed =~ start(0.081)*x1+\n#&gt; start(-0.085)*x2+\n#&gt; 0.075*x3+\n#&gt; start(0.01)*x4+\n#&gt; 0.003*x5+\n#&gt; start(-0.006)*x6+\n#&gt; start(0.745)*x7+\n#&gt; start(0.724)*x8+\n#&gt; start(0.504)*x9 \n#&gt; \n#&gt; visual =~ start(0.577)*x1+\n#&gt; start(0.515)*x2+\n#&gt; start(0.67)*x3+\n#&gt; start(0.016)*x4+\n#&gt; -0.064*x5+\n#&gt; start(0.08)*x6+\n#&gt; start(-0.24)*x7+\n#&gt; 0.035*x8+\n#&gt; start(0.317)*x9\n\nPossiamo confermare che ogni fattore ha due parametri fissati (i cross-loadings dagli altri fattori) e che tutti gli altri parametri hanno i carichi dell’EFA come punti di partenza.\n\ncfa_fit &lt;- lavaan::cfa(model = model_syntax, data = hw_data, std.lv =TRUE)\nlavaan::summary(cfa_fit, fit.measures = TRUE, std = TRUE)\n#&gt; lavaan 0.6-19 ended normally after 28 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        33\n#&gt; \n#&gt;   Number of observations                           301\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                22.897\n#&gt;   Degrees of freedom                                12\n#&gt;   P-value (Chi-square)                           0.029\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               918.852\n#&gt;   Degrees of freedom                                36\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.988\n#&gt;   Tucker-Lewis Index (TLI)                       0.963\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -3706.541\n#&gt;   Loglikelihood unrestricted model (H1)      -3695.092\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                7479.081\n#&gt;   Bayesian (BIC)                              7601.416\n#&gt;   Sample-size adjusted Bayesian (SABIC)       7496.758\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.055\n#&gt;   90 Percent confidence interval - lower         0.017\n#&gt;   90 Percent confidence interval - upper         0.089\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.365\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.120\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.017\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   textual =~                                                            \n#&gt;     x1                0.218    0.080    2.728    0.006    0.218    0.187\n#&gt;     x2                0.052    0.084    0.611    0.541    0.052    0.044\n#&gt;     x3               -0.082                              -0.082   -0.073\n#&gt;     x4                0.971    0.062   15.657    0.000    0.971    0.836\n#&gt;     x5                1.138    0.063   18.018    0.000    1.138    0.883\n#&gt;     x6                0.878    0.058   15.034    0.000    0.878    0.803\n#&gt;     x7                0.030    0.078    0.391    0.696    0.030    0.028\n#&gt;     x8               -0.049                              -0.049   -0.048\n#&gt;     x9                0.023    0.064    0.359    0.720    0.023    0.023\n#&gt;   speed =~                                                              \n#&gt;     x1                0.080    0.087    0.917    0.359    0.080    0.069\n#&gt;     x2               -0.105    0.093   -1.128    0.259   -0.105   -0.089\n#&gt;     x3                0.075                               0.075    0.066\n#&gt;     x4                0.006    0.062    0.104    0.917    0.006    0.006\n#&gt;     x5                0.003                               0.003    0.002\n#&gt;     x6               -0.008    0.060   -0.139    0.889   -0.008   -0.008\n#&gt;     x7                0.800    0.098    8.140    0.000    0.800    0.736\n#&gt;     x8                0.737    0.069   10.647    0.000    0.737    0.729\n#&gt;     x9                0.503    0.068    7.394    0.000    0.503    0.500\n#&gt;   visual =~                                                             \n#&gt;     x1                0.689    0.088    7.834    0.000    0.689    0.591\n#&gt;     x2                0.597    0.093    6.413    0.000    0.597    0.508\n#&gt;     x3                0.759    0.077    9.829    0.000    0.759    0.672\n#&gt;     x4                0.043    0.067    0.646    0.518    0.043    0.037\n#&gt;     x5               -0.064                              -0.064   -0.050\n#&gt;     x6                0.101    0.063    1.605    0.109    0.101    0.093\n#&gt;     x7               -0.236    0.100   -2.355    0.019   -0.236   -0.217\n#&gt;     x8                0.035                               0.035    0.035\n#&gt;     x9                0.318    0.072    4.384    0.000    0.318    0.315\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   textual ~~                                                            \n#&gt;     speed             0.258    0.087    2.977    0.003    0.258    0.258\n#&gt;     visual            0.303    0.092    3.307    0.001    0.303    0.303\n#&gt;   speed ~~                                                              \n#&gt;     visual            0.307    0.115    2.670    0.008    0.307    0.307\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .x1                0.696    0.087    8.038    0.000    0.696    0.513\n#&gt;    .x2                1.035    0.102   10.151    0.000    1.035    0.749\n#&gt;    .x3                0.692    0.097    7.134    0.000    0.692    0.543\n#&gt;    .x4                0.377    0.048    7.902    0.000    0.377    0.279\n#&gt;    .x5                0.403    0.061    6.590    0.000    0.403    0.243\n#&gt;    .x6                0.365    0.042    8.613    0.000    0.365    0.305\n#&gt;    .x7                0.594    0.106    5.624    0.000    0.594    0.502\n#&gt;    .x8                0.479    0.080    5.958    0.000    0.479    0.469\n#&gt;    .x9                0.551    0.060    9.132    0.000    0.551    0.543\n#&gt;     textual           1.000                               1.000    1.000\n#&gt;     speed             1.000                               1.000    1.000\n#&gt;     visual            1.000                               1.000    1.000\n\nSe hai bisogno di adattare un modello con varianze residue dei fattori libere, dovrai utilizzare la funzione fit_free_factor_var_esem(). Questa funzione è un wrapper intorno alla funzione lavaan(), con gli stessi parametri impostati nella funzione cfa(), eccetto per il fatto che le varianze dei fattori sono libere di essere stimati e i primi indicatori in ciascun fattore non vengono fissati automaticamente. Assumiamo che l’identificazione sia garantita dai referenti fissati nella sintassi del modello, il che dovrebbe essere il caso se hai impostato only_fix_crossloadings = FALSE durante la composizione della sintassi con syntax_composer.\n\ncfa_fit &lt;- fit_free_factor_var_esem(model_syntax_free_var, hw_data)\nlavaan::summary(cfa_fit, fit.measures = TRUE, std = TRUE)\n#&gt; lavaan 0.6-19 ended normally after 45 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        33\n#&gt; \n#&gt;   Number of observations                           301\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                22.897\n#&gt;   Degrees of freedom                                12\n#&gt;   P-value (Chi-square)                           0.029\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               918.852\n#&gt;   Degrees of freedom                                36\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.988\n#&gt;   Tucker-Lewis Index (TLI)                       0.963\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -3706.541\n#&gt;   Loglikelihood unrestricted model (H1)      -3695.092\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                7479.081\n#&gt;   Bayesian (BIC)                              7601.416\n#&gt;   Sample-size adjusted Bayesian (SABIC)       7496.758\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.055\n#&gt;   90 Percent confidence interval - lower         0.017\n#&gt;   90 Percent confidence interval - upper         0.089\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.365\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.120\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.017\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   textual =~                                                            \n#&gt;     x1                0.152    0.064    2.383    0.017    0.196    0.169\n#&gt;     x2                0.028    0.067    0.418    0.676    0.036    0.031\n#&gt;     x3               -0.082                              -0.106   -0.094\n#&gt;     x4                0.754    0.051   14.677    0.000    0.972    0.836\n#&gt;     x5                0.886                               1.143    0.887\n#&gt;     x6                0.680    0.048   14.197    0.000    0.878    0.802\n#&gt;     x7                0.018    0.061    0.293    0.769    0.023    0.021\n#&gt;     x8               -0.049                              -0.063   -0.063\n#&gt;     x9                0.004    0.051    0.073    0.942    0.005    0.005\n#&gt;   speed =~                                                              \n#&gt;     x1                0.080    0.086    0.933    0.351    0.082    0.070\n#&gt;     x2               -0.102    0.092   -1.100    0.271   -0.104   -0.088\n#&gt;     x3                0.075                               0.077    0.068\n#&gt;     x4                0.007    0.061    0.108    0.914    0.007    0.006\n#&gt;     x5                0.003                               0.003    0.002\n#&gt;     x6               -0.008    0.059   -0.134    0.893   -0.008   -0.007\n#&gt;     x7                0.785    0.137    5.739    0.000    0.802    0.737\n#&gt;     x8                0.724                               0.739    0.731\n#&gt;     x9                0.495    0.079    6.281    0.000    0.505    0.502\n#&gt;   visual =~                                                             \n#&gt;     x1                0.606    0.106    5.711    0.000    0.694    0.595\n#&gt;     x2                0.525    0.101    5.210    0.000    0.601    0.511\n#&gt;     x3                0.670                               0.767    0.679\n#&gt;     x4                0.032    0.060    0.527    0.598    0.036    0.031\n#&gt;     x5               -0.064                              -0.073   -0.057\n#&gt;     x6                0.083    0.058    1.442    0.149    0.095    0.087\n#&gt;     x7               -0.204    0.093   -2.196    0.028   -0.233   -0.215\n#&gt;     x8                0.035                               0.040    0.040\n#&gt;     x9                0.283    0.069    4.102    0.000    0.323    0.321\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   textual ~~                                                            \n#&gt;     speed             0.362    0.119    3.029    0.002    0.275    0.275\n#&gt;     visual            0.496    0.147    3.376    0.001    0.336    0.336\n#&gt;   speed ~~                                                              \n#&gt;     visual            0.361    0.133    2.719    0.007    0.309    0.309\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .x1                0.696    0.087    8.038    0.000    0.696    0.513\n#&gt;    .x2                1.035    0.102   10.151    0.000    1.035    0.749\n#&gt;    .x3                0.692    0.097    7.134    0.000    0.692    0.543\n#&gt;    .x4                0.377    0.048    7.902    0.000    0.377    0.279\n#&gt;    .x5                0.403    0.061    6.590    0.000    0.403    0.243\n#&gt;    .x6                0.365    0.042    8.613    0.000    0.365    0.305\n#&gt;    .x7                0.594    0.106    5.624    0.000    0.594    0.502\n#&gt;    .x8                0.479    0.080    5.958    0.000    0.479    0.469\n#&gt;    .x9                0.551    0.060    9.132    0.000    0.551    0.543\n#&gt;     textual           1.663    0.186    8.963    0.000    1.000    1.000\n#&gt;     speed             1.043    0.196    5.319    0.000    1.000    1.000\n#&gt;     visual            1.311    0.267    4.917    0.000    1.000    1.000\n\nUn diagramma di percorso si ottiene con la seguente istruzione:\n\nsemPlot::semPaths(\n    cfa_fit,\n    what = \"col\", whatLabels = \"no\", style = \"mx\",\n    layout = \"tree\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 5, sizeMan2 = 4\n)\n\n\n\n\n\n\n\n\n44.4.3 Omega di McDonald\nÈ possibile calcolare gli omega di McDonald utilizzando il modello adattato e la matrice di rotazione target.\n\nomega_esem(cfa_fit, target_rot)\n#&gt;  visual textual   speed \n#&gt;   0.639   0.885   0.719",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Exploratory Structural Equation Modeling</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/06_efa_lavaan.html#sintassi-esem-in-lavaan",
    "href": "chapters/cfa/06_efa_lavaan.html#sintassi-esem-in-lavaan",
    "title": "44  Exploratory Structural Equation Modeling",
    "section": "\n44.5 Sintassi ESEM in lavaan\n",
    "text": "44.5 Sintassi ESEM in lavaan\n\nSpecifichiamo lo stesso modello descritto in precedenza con la sintassi offerta da lavaan per i modelli ESEM.\n\nmodel &lt;- '\n    # EFA block\n    efa(\"efa1\")*visual + \n    efa(\"efa1\")*textual + \n    efa(\"efa1\")*speed =~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9\n'\n\nAdattiamo il modello ai dati.\n\nfit &lt;- sem(\n    model = model,\n    data = hw_data,\n    rotation = \"geomin\",  \n    rotation.args = list(\n        rstarts = 30,          # Number of random starts for rotation\n        algorithm = \"gpa\",      # Generalized Procrustes Analysis\n        std.ov = TRUE           # Standardize observed variables\n    )\n)\n\nEsaminiamo la soluzione ottenuta.\n\nsummary(fit, standardized = TRUE, fit.measures = TRUE)\n#&gt; lavaan 0.6-19 ended normally after 2 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        39\n#&gt;   Row rank of the constraints matrix                 6\n#&gt; \n#&gt;   Rotation method                       GEOMIN OBLIQUE\n#&gt;   Geomin epsilon                                 0.001\n#&gt;   Rotation algorithm (rstarts)                GPA (30)\n#&gt;   Standardized metric                             TRUE\n#&gt;   Row weights                                     None\n#&gt; \n#&gt;   Number of observations                           301\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                22.897\n#&gt;   Degrees of freedom                                12\n#&gt;   P-value (Chi-square)                           0.029\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               918.852\n#&gt;   Degrees of freedom                                36\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.988\n#&gt;   Tucker-Lewis Index (TLI)                       0.963\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -3706.541\n#&gt;   Loglikelihood unrestricted model (H1)      -3695.092\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                7479.081\n#&gt;   Bayesian (BIC)                              7601.416\n#&gt;   Sample-size adjusted Bayesian (SABIC)       7496.758\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.055\n#&gt;   90 Percent confidence interval - lower         0.017\n#&gt;   90 Percent confidence interval - upper         0.089\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.365\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.120\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.017\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   visual =~ efa1                                                        \n#&gt;     x1                0.712    0.092    7.771    0.000    0.712    0.611\n#&gt;     x2                0.628    0.104    6.063    0.000    0.628    0.534\n#&gt;     x3                0.796    0.096    8.255    0.000    0.796    0.705\n#&gt;     x4                0.011    0.011    0.944    0.345    0.011    0.009\n#&gt;     x5               -0.107    0.089   -1.203    0.229   -0.107   -0.083\n#&gt;     x6                0.076    0.073    1.028    0.304    0.076    0.069\n#&gt;     x7               -0.278    0.109   -2.538    0.011   -0.278   -0.255\n#&gt;     x8                0.012    0.008    1.371    0.170    0.012    0.011\n#&gt;     x9                0.314    0.076    4.142    0.000    0.314    0.312\n#&gt;   textual =~ efa1                                                       \n#&gt;     x1                0.198    0.103    1.917    0.055    0.198    0.170\n#&gt;     x2                0.039    0.092    0.424    0.672    0.039    0.033\n#&gt;     x3               -0.106    0.111   -0.963    0.335   -0.106   -0.094\n#&gt;     x4                0.981    0.058   16.850    0.000    0.981    0.844\n#&gt;     x5                1.153    0.074   15.545    0.000    1.153    0.895\n#&gt;     x6                0.886    0.062   14.338    0.000    0.886    0.810\n#&gt;     x7                0.011    0.012    0.923    0.356    0.011    0.010\n#&gt;     x8               -0.075    0.066   -1.135    0.256   -0.075   -0.074\n#&gt;     x9               -0.002    0.007   -0.315    0.753   -0.002   -0.002\n#&gt;   speed =~ efa1                                                         \n#&gt;     x1                0.015    0.048    0.302    0.762    0.015    0.012\n#&gt;     x2               -0.166    0.092   -1.813    0.070   -0.166   -0.141\n#&gt;     x3                0.002    0.048    0.036    0.971    0.002    0.002\n#&gt;     x4                0.004    0.047    0.091    0.927    0.004    0.004\n#&gt;     x5                0.012    0.036    0.322    0.747    0.012    0.009\n#&gt;     x6               -0.017    0.041   -0.409    0.683   -0.017   -0.015\n#&gt;     x7                0.843    0.105    7.999    0.000    0.843    0.775\n#&gt;     x8                0.752    0.076    9.893    0.000    0.752    0.744\n#&gt;     x9                0.484    0.070    6.954    0.000    0.484    0.481\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   visual ~~                                                             \n#&gt;     textual           0.373    0.118    3.173    0.002    0.373    0.373\n#&gt;     speed             0.432    0.097    4.465    0.000    0.432    0.432\n#&gt;   textual ~~                                                            \n#&gt;     speed             0.306    0.081    3.775    0.000    0.306    0.306\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .x1                0.696    0.087    8.038    0.000    0.696    0.513\n#&gt;    .x2                1.035    0.102   10.151    0.000    1.035    0.749\n#&gt;    .x3                0.692    0.097    7.134    0.000    0.692    0.543\n#&gt;    .x4                0.377    0.048    7.902    0.000    0.377    0.279\n#&gt;    .x5                0.403    0.061    6.590    0.000    0.403    0.243\n#&gt;    .x6                0.365    0.042    8.613    0.000    0.365    0.305\n#&gt;    .x7                0.594    0.106    5.624    0.000    0.594    0.502\n#&gt;    .x8                0.479    0.080    5.958    0.000    0.479    0.469\n#&gt;    .x9                0.551    0.060    9.132    0.000    0.551    0.543\n#&gt;     visual            1.000                               1.000    1.000\n#&gt;     textual           1.000                               1.000    1.000\n#&gt;     speed             1.000                               1.000    1.000\n\nEseguiamo un confronto tra le soluzioni fornite dai due metodi: ESEM specificato in lavaan vs. ESEM-within-CFA.\n\n\n\nIndici di Adattamento del Modello\n\nMetrica\nESEM\nCFA\n\n\n\nCFI\n0.988\n0.988\n\n\nTLI\n0.963\n0.963\n\n\nRMSEA\n0.055 (CI: 0.017–0.089)\n0.055 (CI: 0.017–0.089)\n\n\nSRMR\n0.017\n0.017\n\n\n\n\n\nConclusione: entrambi i modelli mostrano un ottimo adattamento (CFI e TLI &gt; 0.95, RMSEA accettabile).\n\n\n\nLog-Likelihood e Criteri Informativi\n\nMetrica\nESEM\nCFA\n\n\n\nLoglikelihood (H0)\n-3707\n-3707\n\n\nAIC\n7479\n7479\n\n\nBIC\n7601\n7601\n\n\n\n\n\nConclusione: nessuna differenza nei criteri informativi tra i due metodi.\n\n\n\nCaricamenti Fattoriali\n\nItem\nCaricamento_ESEM\nCaricamento_CFA\n\n\n\nx1\n0.712\n0.606\n\n\nx2\n0.628\n0.525\n\n\nx3\n0.796\n0.670\n\n\nx4\n0.011\n0.032\n\n\nx5\n-0.107\n-0.064\n\n\n\n\n\nOsservazione: l’ESEM permette maggior flessibilità nel gestire i caricamenti e le cross-loadings, ottenendo una separazione più chiara dei fattori.\n\n\n\nCovarianze tra i Fattori\n\nCovarianza\nStima_ESEM\nStima_CFA\n\n\n\nVisual ~ Textual\n0.373\n0.496\n\n\nVisual ~ Speed\n0.432\n0.361\n\n\n\n\n\nOsservazione: l’ESEM stima covarianze leggermente diverse, spesso più alte, rispetto alla CFA.\nInterpretazione dei risultati.\n\n\nESEM: offre una maggiore flessibilità e gestione delle cross-loadings, utile quando la struttura fattoriale non è chiara.\n\nCFA: più rigido, adatto quando la struttura dei fattori è ben definita.\n\nIn sintesi, i due metodi producono risultati simili. Tuttavia, il metodo ESEM disponibile nel pacchetto lavaan offre informazioni aggiuntive sulle saturazioni incrociate e sulle relazioni tra i fattori, rendendolo una scelta più solida e completa per analisi di tipo esplorativo.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Exploratory Structural Equation Modeling</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/06_efa_lavaan.html#riflessioni-conclusive",
    "href": "chapters/cfa/06_efa_lavaan.html#riflessioni-conclusive",
    "title": "44  Exploratory Structural Equation Modeling",
    "section": "\n44.6 Riflessioni Conclusive",
    "text": "44.6 Riflessioni Conclusive\nL’ESEM rappresenta un ponte significativo tra i modelli di misurazione tradizionali dell’Exploratory Factor Analysis (EFA) e il più esteso quadro del Confirmatory Factor Analysis/Structural Equation Modeling (CFA/SEM). Grazie a questo, l’ESEM combina i benefici dell’EFA con quelli del CFA/SEM, fornendo un approccio più flessibile e inclusivo nell’analisi dei dati. Tale integrazione ha segnato un progresso notevole nella ricerca statistica, evidenziando l’importanza dell’EFA che precedentemente era sottovalutata.\nL’ESEM e il quadro bifattoriale-ESEM, in particolare, offrono una rappresentazione più fedele e precisa della multidimensionalità dei costrutti psicometrici, che è spesso presente nelle misurazioni. Questo approccio riconosce e gestisce meglio la natura multidimensionale dei costrutti, a differenza dell’approccio tradizionale del CFA, che tende a sovrastimare le correlazioni tra i fattori quando non considera adeguatamente la loro natura gerarchica e interconnessa (Asparouhov et al., 2015; Morin et al., 2020).\nNonostante questi vantaggi, l’ESEM presenta alcune limitazioni che devono essere considerate:\n\n\nComplessità Computazionale: L’ESEM può essere più complesso e richiedere maggiori risorse computazionali rispetto agli approcci tradizionali, soprattutto quando si gestiscono grandi set di dati o modelli con molti fattori.\n\nInterpretazione dei Risultati: A causa della sua flessibilità, l’ESEM può produrre risultati che sono più difficili da interpretare. Ad esempio, la sovrapposizione tra i fattori può complicare l’interpretazione dei costrutti.\n\nRischio di Overfitting: La maggiore flessibilità dell’ESEM può anche portare a un rischio maggiore di overfitting, specialmente in campioni più piccoli o con modelli eccessivamente complessi.\n\nNecessità di Esperienza e Conoscenza: Per utilizzare efficacemente l’ESEM, è richiesta una comprensione approfondita della teoria sottostante e delle tecniche statistiche, che può essere una barriera per alcuni ricercatori.\n\nNonostante queste limitazioni, si prevede che i futuri sviluppi e le applicazioni dell’ESEM conducano a soluzioni più integrate e a un consenso più ampio sulle migliori pratiche nell’utilizzo di questo potente strumento statistico. Nel Capitolo 59 esploreremo il set-ESEM, una recente evoluzione di questa metodologia.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Exploratory Structural Equation Modeling</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/06_efa_lavaan.html#session-info",
    "href": "chapters/cfa/06_efa_lavaan.html#session-info",
    "title": "44  Exploratory Structural Equation Modeling",
    "section": "\n44.7 Session Info",
    "text": "44.7 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] kableExtra_1.4.0                esemComp_0.2                   \n#&gt;  [3] BifactorIndicesCalculator_0.2.2 ggokabeito_0.1.0               \n#&gt;  [5] see_0.10.0                      MASS_7.3-65                    \n#&gt;  [7] viridis_0.6.5                   viridisLite_0.4.2              \n#&gt;  [9] ggpubr_0.6.0                    ggExtra_0.10.1                 \n#&gt; [11] gridExtra_2.3                   patchwork_1.3.0                \n#&gt; [13] bayesplot_1.11.1                semTools_0.5-6                 \n#&gt; [15] semPlot_1.1.6                   lavaan_0.6-19                  \n#&gt; [17] psych_2.4.12                    scales_1.3.0                   \n#&gt; [19] markdown_1.13                   knitr_1.49                     \n#&gt; [21] lubridate_1.9.4                 forcats_1.0.0                  \n#&gt; [23] stringr_1.5.1                   dplyr_1.1.4                    \n#&gt; [25] purrr_1.0.4                     readr_2.1.5                    \n#&gt; [27] tidyr_1.3.1                     tibble_3.2.1                   \n#&gt; [29] ggplot2_3.5.1                   tidyverse_2.0.0                \n#&gt; [31] here_1.0.1                     \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1    jsonlite_1.9.0       magrittr_2.0.3      \n#&gt;   [4] TH.data_1.1-3        estimability_1.5.1   farver_2.1.2        \n#&gt;   [7] nloptr_2.1.1         rmarkdown_2.29       vctrs_0.6.5         \n#&gt;  [10] minqa_1.2.8          base64enc_0.1-3      rstatix_0.7.2       \n#&gt;  [13] htmltools_0.5.8.1    broom_1.0.7          Formula_1.2-5       \n#&gt;  [16] htmlwidgets_1.6.4    plyr_1.8.9           sandwich_3.1-1      \n#&gt;  [19] emmeans_1.10.7       zoo_1.8-13           igraph_2.1.4        \n#&gt;  [22] mime_0.12            lifecycle_1.0.4      pkgconfig_2.0.3     \n#&gt;  [25] Matrix_1.7-2         R6_2.6.1             fastmap_1.2.0       \n#&gt;  [28] rbibutils_2.3        shiny_1.10.0         numDeriv_2016.8-1.1 \n#&gt;  [31] digest_0.6.37        OpenMx_2.21.13       fdrtool_1.2.18      \n#&gt;  [34] colorspace_2.1-1     rprojroot_2.0.4      Hmisc_5.2-2         \n#&gt;  [37] timechange_0.3.0     abind_1.4-8          compiler_4.4.2      \n#&gt;  [40] withr_3.0.2          glasso_1.11          htmlTable_2.4.3     \n#&gt;  [43] backports_1.5.0      carData_3.0-5        ggsignif_0.6.4      \n#&gt;  [46] GPArotation_2024.3-1 corpcor_1.6.10       gtools_3.9.5        \n#&gt;  [49] tools_4.4.2          pbivnorm_0.6.0       foreign_0.8-88      \n#&gt;  [52] zip_2.3.2            httpuv_1.6.15        nnet_7.3-20         \n#&gt;  [55] glue_1.8.0           quadprog_1.5-8       nlme_3.1-167        \n#&gt;  [58] promises_1.3.2       lisrelToR_0.3        grid_4.4.2          \n#&gt;  [61] checkmate_2.3.2      cluster_2.1.8        reshape2_1.4.4      \n#&gt;  [64] generics_0.1.3       gtable_0.3.6         tzdb_0.4.0          \n#&gt;  [67] data.table_1.17.0    hms_1.1.3            xml2_1.3.7          \n#&gt;  [70] car_3.1-3            sem_3.1-16           pillar_1.10.1       \n#&gt;  [73] rockchalk_1.8.157    later_1.4.1          splines_4.4.2       \n#&gt;  [76] lattice_0.22-6       survival_3.8-3       kutils_1.73         \n#&gt;  [79] tidyselect_1.2.1     miniUI_0.1.1.1       pbapply_1.7-2       \n#&gt;  [82] reformulas_0.4.0     svglite_2.1.3        stats4_4.4.2        \n#&gt;  [85] xfun_0.51            qgraph_1.9.8         arm_1.14-4          \n#&gt;  [88] stringi_1.8.4        yaml_2.3.10          pacman_0.5.1        \n#&gt;  [91] boot_1.3-31          evaluate_1.0.3       codetools_0.2-20    \n#&gt;  [94] mi_1.1               cli_3.6.4            RcppParallel_5.1.10 \n#&gt;  [97] rpart_4.1.24         systemfonts_1.2.1    xtable_1.8-4        \n#&gt; [100] Rdpack_2.6.2         munsell_0.5.1        Rcpp_1.0.14         \n#&gt; [103] coda_0.19-4.1        png_0.1-8            XML_3.99-0.18       \n#&gt; [106] parallel_4.4.2       jpeg_0.1-10          lme4_1.1-36         \n#&gt; [109] mvtnorm_1.3-3        openxlsx_4.2.8       rlang_1.1.5         \n#&gt; [112] multcomp_1.4-28      mnormt_2.1.1",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Exploratory Structural Equation Modeling</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/06_efa_lavaan.html#bibliografia",
    "href": "chapters/cfa/06_efa_lavaan.html#bibliografia",
    "title": "44  Exploratory Structural Equation Modeling",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nBrown, T. A. (2015). Confirmatory factor analysis for applied research. Guilford publications.\n\n\nMarsh, H. W., Morin, A. J., Parker, P. D., & Kaur, G. (2014). Exploratory structural equation modeling: An integration of the best features of exploratory and confirmatory factor analysis. Annual Review of Clinical Psychology, 10(1), 85–110.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Exploratory Structural Equation Modeling</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html",
    "href": "chapters/cfa/07_fa_in_r.html",
    "title": "45  Strategia Integrata per un’Analisi Fattoriale",
    "section": "",
    "text": "45.1 Introduzione\nQuesto capitolo propone un tutorial, ispirato al lavoro di Saqr & López-Pernas (2024), su come condurre un’Analisi Fattoriale utilizzando R. Secondo Saqr & López-Pernas (2024), la distinzione tra Analisi Fattoriale Esplorativa (EFA) e Analisi Fattoriale Confermativa (CFA) non è sempre netta. Nella pratica, entrambe le tecniche vengono spesso impiegate all’interno dello stesso studio per ottenere una comprensione più completa dei costrutti latenti.\nIn questa sezione viene presentata una strategia integrata per combinare EFA e CFA, articolata in tre fasi che i ricercatori possono seguire quando i costrutti latenti giocano un ruolo centrale nello studio. Questo approccio è utile sia quando i costrutti latenti sono il fulcro dello strumento in esame, sia quando vengono utilizzati come predittori o esiti nell’analisi.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html#introduzione",
    "href": "chapters/cfa/07_fa_in_r.html#introduzione",
    "title": "45  Strategia Integrata per un’Analisi Fattoriale",
    "section": "",
    "text": "45.1.1 I tre passaggi principali:\n\nEsplorazione della struttura fattoriale\nIdentificare il numero e la natura dei fattori sottostanti attraverso l’EFA, per ottenere un modello iniziale della struttura dei dati.\nCostruzione e valutazione del modello fattoriale\nUtilizzare la CFA per confermare il modello individuato nell’EFA, valutando l’adattamento del modello ai dati raccolti.\nValutazione della generalizzabilità\nVerificare se la struttura fattoriale individuata è replicabile e stabile in campioni diversi o in contesti differenti.\n\nQuesto capitolo assume che il ricercatore abbia già completato una fase preliminare di sviluppo dello strumento, concentrandosi su un costrutto di interesse. Inoltre, si presuppone che i dati utilizzati provengano da un campione rappresentativo della popolazione target.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html#passo-1-esplorazione-della-struttura-fattoriale",
    "href": "chapters/cfa/07_fa_in_r.html#passo-1-esplorazione-della-struttura-fattoriale",
    "title": "45  Strategia Integrata per un’Analisi Fattoriale",
    "section": "\n45.2 Passo 1: Esplorazione della Struttura Fattoriale",
    "text": "45.2 Passo 1: Esplorazione della Struttura Fattoriale\nDopo aver selezionato le variabili di interesse e raccolto i relativi dati, il ricercatore dovrebbe avviare il processo con un’Analisi Fattoriale Esplorativa (EFA). Se si utilizza uno strumento già validato o si dispone di ipotesi solide sulla struttura fattoriale sottostante, l’obiettivo iniziale sarà verificare se il numero di fattori e le saturazioni degli indicatori sui fattori corrispondono ai risultati attesi. In questa fase, alcune domande fondamentali da porsi includono:\n- Le variabili ipotizzate come influenzate da uno stesso fattore caricano effettivamente su un unico fattore?\n- Se si presuppone l’esistenza di un unico fattore sottostante, le variabili mostrano effettivamente carichi elevati su quel fattore?\nNel caso di strumenti nuovi, l’EFA serve a valutare se la struttura fattoriale emergente è interpretabile. In questo caso, è utile chiedersi:\n- Le variabili che saturano principalmente su un fattore condividono effettivamente un contenuto comune?\n- Le variabili che saturano su fattori diversi riflettono differenze qualitative evidenti?\nAd esempio, in un test di matematica, potrebbe emergere che compiti di addizione, sottrazione, divisione e moltiplicazione saturano su quattro fattori distinti, interpretabili rispettivamente come abilità specifiche in ciascuna operazione.\nIn questa fase, potrebbero rendersi necessari aggiustamenti. Per esempio, variabili che non presentano carichi fattoriali sufficientemente elevati (ad esempio inferiori a 0.3) su alcuna dimensione potrebbero essere rimosse, seguite da una nuova esecuzione dell’EFA. Tuttavia, è fondamentale riflettere attentamente sulle ragioni di eventuali carichi fattoriali bassi, che potrebbero dipendere, ad esempio, da una formulazione poco chiara di un item. La rimozione di variabili dovrebbe essere guidata da una motivazione teorica solida, evitando decisioni arbitrarie.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html#passo-2-costruzione-del-modello-fattoriale-e-valutazione-delladattamento",
    "href": "chapters/cfa/07_fa_in_r.html#passo-2-costruzione-del-modello-fattoriale-e-valutazione-delladattamento",
    "title": "45  Strategia Integrata per un’Analisi Fattoriale",
    "section": "\n45.3 Passo 2: Costruzione del Modello Fattoriale e Valutazione dell’Adattamento",
    "text": "45.3 Passo 2: Costruzione del Modello Fattoriale e Valutazione dell’Adattamento\nDopo aver individuato un modello preliminare tramite l’EFA, il passo successivo consiste nel raffinare il modello e applicare la CFA per valutare quanto bene esso si adatti ai dati. Questo significa verificare se le covarianze previste dalla struttura fattoriale corrispondono alle covarianze osservate nel dataset. Nell’EFA, ogni variabile poteva caricare su tutti i fattori, ma con la CFA è possibile limitare i carichi fattoriali sulla base di considerazioni teoriche o empiriche.\nIn questa fase, è importante restringere il modello eliminando i carichi trasversali (cross-loadings) che non sono coerenti con la teoria o che risultano vicini allo zero. I carichi molto bassi possono essere rimossi senza introdurre problemi, ma quelli più alti richiedono una valutazione attenta. Anche se inizialmente sembrano privi di significato, la loro presenza potrebbe suggerire informazioni inattese sui dati. Pertanto, prima di rimuoverli, è fondamentale verificarne la coerenza con la teoria o le ipotesi iniziali. Se, dopo un’analisi approfondita, questi carichi possono essere giustificati teoricamente, è preferibile mantenerli. In caso contrario, si possono eliminare, procedendo poi a valutare l’adattamento del modello modificato ai dati.\nUna volta definite le relazioni tra variabili e fattori, si costruisce il modello CFA e lo si applica al dataset. Se l’adattamento del modello non risulta soddisfacente, è possibile tornare ai risultati dell’EFA per valutare l’inclusione di ulteriori carichi fattoriali o altre modifiche. Tuttavia, qualsiasi aggiunta o cambiamento deve essere giustificato teoricamente, evitando adattamenti puramente empirici.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html#passo-3-valutazione-della-generalizzabilità",
    "href": "chapters/cfa/07_fa_in_r.html#passo-3-valutazione-della-generalizzabilità",
    "title": "45  Strategia Integrata per un’Analisi Fattoriale",
    "section": "\n45.4 Passo 3: Valutazione della Generalizzabilità",
    "text": "45.4 Passo 3: Valutazione della Generalizzabilità\nDopo aver costruito e valutato il modello, l’obiettivo è verificarne la generalizzabilità. Questo passaggio è cruciale per garantire che il modello sia valido non solo per i dati attuali, ma anche per futuri studi sulla stessa popolazione. Tale verifica si effettua tramite la validazione incrociata, che consiste nel testare il modello su un dataset indipendente.\nIdealmente, sarebbe opportuno raccogliere un secondo dataset rappresentativo della stessa popolazione. Tuttavia, nella pratica, questa soluzione è spesso poco realizzabile a causa di limiti di tempo o risorse. Un’alternativa comune è dividere il dataset iniziale in due sottocampioni:\n\n\nCampione di sviluppo: utilizzato per eseguire i Passi 1 (EFA) e 2 (CFA).\n\n\nCampione di validazione: riservato al Passo 3 per testare la generalizzabilità.\n\nSe il modello CFA si adatta bene anche al campione di validazione, si ottiene una maggiore certezza sulla sua applicabilità in futuri studi. Se invece emergono problemi di adattamento, occorre analizzarne le cause, verificare eventuali incoerenze tra teoria e dati, e aggiornare di conseguenza il modello e le ipotesi.\n\n45.4.1 Considerazioni finali\nQuesta strategia, articolata in tre passaggi, rappresenta un approccio sistematico per l’analisi fattoriale in studi che utilizzano strumenti per misurare costrutti latenti. Anche quando si utilizza uno strumento già validato su una popolazione analoga, seguire questa procedura rimane una scelta prudente per evitare possibili distorsioni nei risultati.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html#analisi-fattoriale-in-r",
    "href": "chapters/cfa/07_fa_in_r.html#analisi-fattoriale-in-r",
    "title": "45  Strategia Integrata per un’Analisi Fattoriale",
    "section": "\n45.5 Analisi Fattoriale in R",
    "text": "45.5 Analisi Fattoriale in R\nSaqr & López-Pernas (2024) propone un tutorial dettagliato sui passaggi essenziali per condurre un’Analisi Fattoriale Esplorativa (EFA) e un’Analisi Fattoriale Confermativa (CFA) utilizzando R. Il tutorial affronta i seguenti aspetti chiave:\n\nverifica preliminare delle caratteristiche dei dati per valutarne l’idoneità all’EFA/CFA,\n\nscelta del numero di fattori,\n\nvalutazione dell’adattamento globale e locale del modello,\n\nverifica della generalizzabilità del modello fattoriale finale.\n\n\n45.5.1 Struttura del tutorial\nIl tutorial inizia con la preparazione dei dati: importazione, controllo della loro idoneità per l’analisi fattoriale e suddivisione del dataset per riservare un campione per la validazione incrociata. Successivamente, vengono descritti i passaggi per condurre:\n\nun’EFA per definire una struttura fattoriale preliminare (Passo 1),\n\nuna CFA per affinare e validare il modello (Passo 2),\n\nla verifica della generalizzabilità del modello tramite validazione incrociata (Passo 3).\n\n45.5.2 Preparazione\nIl dataset utilizzato da Saqr & López-Pernas (2024) raccoglie dati di un’indagine sul burnout degli insegnanti in Indonesia, con 876 rispondenti. Le domande sono organizzate in cinque ambiti teorici:\n\n\nConcetto di Sé dell’Insegnante (TSC): 5 item,\n\n\nEfficacia dell’Insegnante (TE): 5 item,\n\n\nEsaurimento Emotivo (EE): 5 item,\n\n\nDepersonalizzazione (DP): 3 item,\n\n\nRiduzione del Senso di Realizzazione Personale (RPA): 7 item.\n\nIn totale, il dataset include 25 variabili, ciascuna valutata su una scala Likert a 5 punti (da 1 = “mai” a 5 = “sempre”). Questa organizzazione rende il dataset ideale per un’analisi fattoriale, consentendo di esplorare la struttura latente delle dimensioni teoriche ipotizzate.\nPrima di procedere con l’EFA e la CFA, è necessario:\n\nverificare la sufficienza del campione (ad esempio, tramite il test di Kaiser-Meyer-Olkin, KMO),\n\ncontrollare la normalità delle distribuzioni o eventuali deviazioni,\n\nsuddividere il dataset in due sottocampioni, uno per lo sviluppo del modello e uno per la validazione.\n\nQuesto approccio organizzato fornisce una base solida per esplorare e confermare la struttura fattoriale, testandone infine la replicabilità su un campione indipendente. La chiarezza dei passaggi rende il tutorial applicabile a una vasta gamma di contesti di ricerca.\nCarichiamo le funzioni di supporto definite da Saqr & López-Pernas (2024):\n\n# Source 'sleasy' functions\nsource(here::here(\"code\", \"sleasy.R\"))\n\nImportiamo i dati:\n\ndataset &lt;- rio::import(\"https://github.com/lamethods/data/raw/main/4_teachersBurnout/2.%20Response.xlsx\")",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html#i-dati-sono-adatti-allanalisi-fattoriale",
    "href": "chapters/cfa/07_fa_in_r.html#i-dati-sono-adatti-allanalisi-fattoriale",
    "title": "45  Strategia Integrata per un’Analisi Fattoriale",
    "section": "\n45.6 I Dati Sono Adatti all’Analisi Fattoriale?",
    "text": "45.6 I Dati Sono Adatti all’Analisi Fattoriale?\nPer condurre un’Analisi Fattoriale Esplorativa (EFA) o una Analisi Fattoriale Confermativa (CFA), è fondamentale assicurarsi che i dati soddisfino determinati requisiti. Di seguito vengono descritti i principali aspetti da considerare.\n\n45.6.1 Variabili continue o categoriche\nIdealmente, le variabili dovrebbero essere continue. Sebbene raramente le variabili siano perfettamente continue, è accettabile trattarle come tali se sono misurate su una scala con almeno cinque categorie di risposta e presentano una distribuzione ragionevolmente simmetrica.\nSe le variabili sono categoriche (ad esempio, binarie o ordinali), è comunque possibile condurre un’analisi fattoriale utilizzando metodi di stima specifici per questo tipo di dati. Inoltre, tutte le variabili dovrebbero preferibilmente essere misurate sulla stessa scala. In caso contrario, oppure se le variabili presentano intervalli di punteggio molto diversi (ad esempio, alcune con valori da 1 a 5 e altre da 2 a 4), è opportuno trasformare le variabili per uniformare le scale prima dell’analisi.\nL’intervallo di ciascuna variabile può essere verificato con il comando seguente:\n\ndescribe(dataset)\n#&gt;      vars   n mean   sd median trimmed  mad min max range  skew kurtosis\n#&gt; TSC1    1 876 3.65 0.68      4    3.62 0.00   1   5     4 -0.09     0.06\n#&gt; TSC2    2 876 3.81 0.64      4    3.78 0.00   2   5     3 -0.07    -0.14\n#&gt; TSC3    3 876 3.73 0.64      4    3.71 0.00   2   5     3 -0.17    -0.02\n#&gt; TSC4    4 876 3.71 0.67      4    3.67 0.00   2   5     3 -0.03    -0.25\n#&gt; TSC5    5 876 3.82 0.65      4    3.79 0.00   2   5     3 -0.10    -0.13\n#&gt; TE1     6 876 4.06 0.71      4    4.10 0.00   1   5     4 -0.47     0.38\n#&gt; TE2     7 876 4.04 0.70      4    4.07 0.00   2   5     3 -0.22    -0.45\n#&gt; TE3     8 876 4.12 0.71      4    4.17 0.00   1   5     4 -0.72     1.60\n#&gt; TE4     9 876 4.11 0.69      4    4.15 0.00   1   5     4 -0.47     0.51\n#&gt; TE5    10 876 3.90 0.75      4    3.92 0.00   1   5     4 -0.41     0.16\n#&gt; EE1    11 876 3.81 0.76      4    3.81 0.00   1   5     4 -0.35     0.23\n#&gt; EE2    12 876 3.73 0.85      4    3.75 1.48   1   5     4 -0.37     0.12\n#&gt; EE3    13 876 3.88 0.83      4    3.91 1.48   1   5     4 -0.31    -0.40\n#&gt; EE4    14 876 3.69 0.80      4    3.67 1.48   1   5     4 -0.03    -0.41\n#&gt; EE5    15 876 3.99 0.81      4    4.03 1.48   1   5     4 -0.43    -0.27\n#&gt; DE1    16 876 3.92 0.68      4    3.93 0.00   1   5     4 -0.53     1.25\n#&gt; DE2    17 876 3.60 0.68      4    3.58 1.48   1   5     4 -0.22     0.64\n#&gt; DE3    18 876 3.82 0.70      4    3.79 0.00   1   5     4 -0.14     0.01\n#&gt; RPA1   19 876 3.93 0.83      4    3.97 1.48   1   5     4 -0.59     0.50\n#&gt; RPA2   20 876 3.94 0.80      4    3.99 0.00   1   5     4 -0.79     1.22\n#&gt; RPA3   21 876 3.88 0.79      4    3.91 0.00   1   5     4 -0.59     0.75\n#&gt; RPA4   22 876 3.87 0.76      4    3.89 0.00   1   5     4 -0.48     0.33\n#&gt; RPA5   23 876 3.84 0.79      4    3.86 0.00   1   5     4 -0.53     0.67\n#&gt;        se\n#&gt; TSC1 0.02\n#&gt; TSC2 0.02\n#&gt; TSC3 0.02\n#&gt; TSC4 0.02\n#&gt; TSC5 0.02\n#&gt; TE1  0.02\n#&gt; TE2  0.02\n#&gt; TE3  0.02\n#&gt; TE4  0.02\n#&gt; TE5  0.03\n#&gt; EE1  0.03\n#&gt; EE2  0.03\n#&gt; EE3  0.03\n#&gt; EE4  0.03\n#&gt; EE5  0.03\n#&gt; DE1  0.02\n#&gt; DE2  0.02\n#&gt; DE3  0.02\n#&gt; RPA1 0.03\n#&gt; RPA2 0.03\n#&gt; RPA3 0.03\n#&gt; RPA4 0.03\n#&gt; RPA5 0.03\n\nNel dataset in esame, le variabili sono misurate su scale Likert a 5 punti con intervalli simili, per cui possono essere trattate come continue senza ulteriori trasformazioni.\n\n45.6.2 Dimensione del campione\nLa dimensione del campione è un aspetto cruciale. Esistono diverse regole empiriche: - Una regola generale suggerisce un campione minimo di 200 osservazioni. - Per modelli semplici (pochi fattori, relazioni forti tra fattori e variabili), campioni più piccoli possono essere sufficienti. Per modelli complessi (molti fattori o relazioni più deboli), è necessario un campione più ampio. - Bentler e Chou raccomandano almeno 5 osservazioni per ogni parametro da stimare, mentre Jackson suggerisce almeno 10, preferibilmente 20 osservazioni per parametro.\nNel dataset di esempio, con 25 variabili che si presume misurino 5 costrutti latenti, i parametri da stimare includono:\n\n\n25 intercetti,\n\n\n25 varianze residue,\n\n\n125 carichi fattoriali (5 fattori × 25 variabili).\n\nIn totale, si devono stimare 175 parametri. La dimensione del campione è verificabile con:\n\nnrow(dataset)\n#&gt; [1] 876\n\nCon 876 osservazioni, il campione è sufficiente secondo Bentler e Chou (5 × 175 = 875) ma non soddisfa il criterio di Jackson per modelli più robusti. Pertanto, non è consigliabile suddividere il dataset per la validazione incrociata. Tuttavia, a scopo didattico, sarà mostrato come creare un campione di riserva.\n\n45.6.3 Correlazioni tra variabili\nUn presupposto fondamentale per l’analisi fattoriale è che le variabili siano correlate. Questo può essere verificato tramite il test di Bartlett, che controlla se la matrice di correlazione è una matrice identità (cioè con elementi fuori diagonale pari a zero). L’ipotesi nulla del test afferma che le variabili non sono correlate. Se l’ipotesi viene rifiutata, è possibile procedere con l’analisi fattoriale. Il comando seguente verifica il p-value del test:\n\nvar_names &lt;- colnames(dataset)\n\n\n(cortest.bartlett(\n    R = cor(dataset[, var_names]), \n    n = nrow(dataset)\n)$p.value) &lt; 0.05\n#&gt; [1] TRUE\n\nNel nostro esempio, il p-value è inferiore a 0,05, indicando che le variabili sono sufficientemente correlate.\n\n45.6.4 Adeguatezza della varianza comune\nUn altro requisito è che le variabili condividano una quantità sufficiente di varianza comune. Questo può essere valutato tramite il test di Kaiser-Meyer-Olkin (KMO), che misura la proporzione di varianza totale attribuibile a varianza comune. Secondo Kaiser, un valore KMO di almeno 0,8 è adeguato, mentre un valore di 0,9 o superiore è eccellente. Per calcolare il valore KMO, si utilizza:\n\nKMO(dataset)\n#&gt; Kaiser-Meyer-Olkin factor adequacy\n#&gt; Call: KMO(r = dataset)\n#&gt; Overall MSA =  0.94\n#&gt; MSA for each item = \n#&gt; TSC1 TSC2 TSC3 TSC4 TSC5  TE1  TE2  TE3  TE4  TE5  EE1  EE2  EE3  EE4  EE5 \n#&gt; 0.96 0.96 0.95 0.94 0.96 0.93 0.96 0.94 0.94 0.96 0.95 0.94 0.95 0.94 0.97 \n#&gt;  DE1  DE2  DE3 RPA1 RPA2 RPA3 RPA4 RPA5 \n#&gt; 0.87 0.86 0.92 0.91 0.91 0.95 0.94 0.96\n\nNel dataset in esame, il valore KMO è pari a 0.94, suggerendo un’eccellente adeguatezza per l’analisi fattoriale.\n\n45.6.5 Normalità e dati mancanti\nLe distribuzioni delle variabili devono essere valutate per verificare la presenza di eventuali deviazioni dalla normalità. Sebbene l’analisi fattoriale possa gestire deviazioni moderate, in caso di non-normalità è necessario utilizzare metodi di stima robusti. La normalità può essere esaminata tramite istogrammi, come nel comando seguente:\n\ndataset |&gt;\n    pivot_longer(2:ncol(dataset),\n        names_to = \"Variable\", values_to = \"Score\"\n    ) |&gt;\n    ggplot(aes(x = Score)) +\n    geom_histogram(bins = 6) +\n    scale_x_continuous(\n        limits = c(0, 6), breaks = c(1, 2, 3, 4, 5)\n    ) +\n    facet_wrap(\"Variable\", ncol = 6, scales = \"free\")\n\n\n\n\n\n\n\nInoltre, è necessario verificare la presenza di dati mancanti. La quantità di valori mancanti per variabile può essere calcolata con:\n\ncolSums(is.na(dataset)) \n#&gt; TSC1 TSC2 TSC3 TSC4 TSC5  TE1  TE2  TE3  TE4  TE5  EE1  EE2  EE3  EE4  EE5 \n#&gt;    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0 \n#&gt;  DE1  DE2  DE3 RPA1 RPA2 RPA3 RPA4 RPA5 \n#&gt;    0    0    0    0    0    0    0    0\n\nSe i dati mancanti sono presenti, è necessario adottare tecniche appropriate per gestirli, come l’imputazione o l’esclusione di osservazioni.\nQuesti controlli preliminari garantiscono che i dati siano adeguati per l’analisi fattoriale e pongono le basi per ottenere risultati affidabili e interpretabili.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html#separare-un-campione-di-riserva",
    "href": "chapters/cfa/07_fa_in_r.html#separare-un-campione-di-riserva",
    "title": "45  Strategia Integrata per un’Analisi Fattoriale",
    "section": "\n45.7 Separare un Campione di Riserva",
    "text": "45.7 Separare un Campione di Riserva\nDopo aver verificato che i dati siano adatti all’analisi fattoriale, è possibile considerare la creazione di un campione di riserva per valutare la generalizzabilità dei risultati. Tuttavia, questa decisione deve tenere conto della dimensione del campione. Come discusso in precedenza, la dimensione minima del campione deve essere almeno 5 volte il numero di parametri da stimare (preferibilmente 10 o 20 volte per modelli più robusti). È importante non suddividere il dataset se il campione disponibile non è sufficientemente ampio da soddisfare i requisiti per entrambe le parti (campione di costruzione e campione di riserva), poiché ciò potrebbe compromettere la qualità del modello. In questi casi, la validazione del modello dovrebbe essere rimandata a studi futuri.\nÈ utile notare che il numero di parametri da stimare in un modello CFA è generalmente inferiore rispetto a un modello EFA. Pertanto, il campione di riserva può essere leggermente più piccolo rispetto a quello utilizzato per costruire il modello.\n\n45.7.1 Considerazioni per il dataset di esempio\nNel nostro esempio, il campione totale di 876 osservazioni non è due volte la dimensione minima richiesta per un modello con 25 variabili e 5 fattori latenti. Tuttavia, a scopo illustrativo, procederemo comunque alla creazione di un campione di riserva. Dividiamo il dataset in due parti uguali:\n- 438 osservazioni per la costruzione del modello (campione di costruzione).\n- 438 osservazioni per la validazione (campione di riserva).\n\n45.7.2 Procedura per la suddivisione\nLa suddivisione avviene in modo casuale attraverso i seguenti passaggi:\n\nImpostazione del seed:\nIl seed viene impostato con set.seed() per garantire che la divisione casuale sia replicabile. Questo è fondamentale per assicurare la coerenza dei risultati.\nCreazione di un vettore di classificazione:\nSi genera un vettore chiamato ind, contenente le etichette “model.building” e “holdout” ripetute 438 volte ciascuna, in ordine casuale. Ogni riga del dataset sarà quindi assegnata a uno dei due gruppi.\nDivisione del dataset:\nUtilizzando la funzione split(), il dataset viene suddiviso in due sottoinsiemi. Le righe vengono assegnate al campione di costruzione o al campione di riserva in base al valore corrispondente nel vettore ind.\nEstrazione dei dataset finali:\nI due nuovi dataset vengono estratti dalla lista creata con split() e memorizzati in due oggetti: model.building e holdout.\n\nEcco il codice per eseguire la suddivisione:\n\n# Imposta il seed per garantire la replicabilità\nset.seed(19)\n\n# Crea il vettore di classificazione\nind &lt;- sample(\n    c(rep(\"model.building\", 438), rep(\"holdout\", 438))\n)\n\n# Suddividi il dataset in base al vettore di classificazione\ntmp &lt;- split(dataset, ind)\n\n# Estrai i due dataset finali\nmodel.building &lt;- tmp$model.building\nholdout &lt;- tmp$holdout\n\n\n45.7.3 Spiegazione dei passaggi\n\nImpostazione del seed:\nLa funzione set.seed(19) garantisce che la suddivisione casuale produca sempre lo stesso risultato, facilitando il controllo e la replicabilità.\nCreazione del vettore ind:\nIl vettore contiene un totale di 876 valori, con 438 assegnati a “model.building” e 438 a “holdout”, in ordine casuale.\nDivisione del dataset:\nLa funzione split() divide il dataset in base ai valori di ind, creando una lista contenente due sottoinsiemi: uno per il modello di costruzione (model.building) e uno per il campione di riserva (holdout).\nEstrazione dei dataset finali:\nI due sottoinsiemi vengono estratti dalla lista tmp e assegnati agli oggetti finali per l’analisi.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html#passo-1-esplorare-la-struttura-fattoriale",
    "href": "chapters/cfa/07_fa_in_r.html#passo-1-esplorare-la-struttura-fattoriale",
    "title": "45  Strategia Integrata per un’Analisi Fattoriale",
    "section": "\n45.8 Passo 1: Esplorare la Struttura Fattoriale",
    "text": "45.8 Passo 1: Esplorare la Struttura Fattoriale\nIl primo passo per esplorare la struttura fattoriale consiste nel determinare il numero di dimensioni sottostanti al costrutto di interesse. Questo processo può essere condotto utilizzando due approcci complementari: l’analisi parallela e il criterio di informazione bayesiano (BIC).\n\nL’analisi parallela fornisce un intervallo plausibile per il numero di dimensioni.\n\nIl BIC aiuta a scegliere il numero specifico di fattori che meglio si adatta ai dati, tenendo conto della parsimonia del modello.\n\n\n45.8.1 Analisi Parallela\nL’analisi parallela è un metodo basato su simulazioni che confronta la varianza spiegata da un certo numero di fattori nei dati reali con la varianza spiegata dagli stessi fattori in dataset simulati (privi di correlazioni tra le variabili, ma con la stessa dimensione e struttura).\nUn fattore viene considerato rilevante se la varianza spiegata nei dati reali supera quella osservata nei dati simulati, indicando che non si tratta di una struttura casuale. Il numero di fattori selezionato è quello in cui i valori osservati nei dati reali superano quelli simulati, fino a un punto in cui non si osserva più questa differenza.\nDettagli tecnici sull’analisi parallela e la sua implementazione sono disponibili nella documentazione della funzione fa.parallel().\n\n45.8.2 Applicazione dell’Analisi Parallela\nPer applicare l’analisi parallela:\n\nSpecificare i dati di costruzione del modello e le colonne corrispondenti alle variabili di interesse.\n\nUtilizzare l’argomento fa = \"fa\" per indicare che si desidera determinare il numero di fattori per l’analisi fattoriale (e non per l’analisi dei componenti principali).\n\nIl risultato include:\n\nUn messaggio che suggerisce il numero plausibile di fattori sottostanti.\nUn grafico che mostra come la varianza spiegata dai dati reali supera quella dei dati simulati fino a un certo numero di fattori.\n\nAd esempio, nel nostro caso, il messaggio indica che sono probabilmente presenti cinque fattori. Nel grafico, si osserva che oltre cinque fattori la varianza spiegata nei dati reali è inferiore a quella dei dati simulati.\nL’analisi parallela è un approccio data-driven: il numero di fattori suggerito è influenzato dal campione analizzato e deve essere considerato un punto di partenza. L’intervallo plausibile può includere più o meno un fattore rispetto a quello suggerito.\n\n45.8.3 Criterio di Informazione Bayesiano (BIC)\nDopo aver determinato un intervallo plausibile di fattori con l’analisi parallela, è necessario scegliere il numero finale utilizzando:\n\n\nL’interpretabilità teorica: valutare se le relazioni tra variabili e fattori sono coerenti con il costrutto di interesse.\n\n\nL’adattamento del modello: confrontare i modelli con diversi numeri di fattori utilizzando il BIC.\n\nIl BIC bilancia l’adattamento del modello ai dati con la semplicità del modello, penalizzando la complessità (cioè l’aggiunta di parametri). Un valore BIC più basso indica un migliore equilibrio tra adattamento e parsimonia.\nAd esempio, se il modello con cinque fattori presenta il BIC più basso, ciò fornisce un supporto per questa soluzione. Tuttavia, la decisione finale dovrebbe integrare il valore del BIC con considerazioni teoriche.\n\n45.8.4 Codice per l’analisi parallela\nDi seguito è riportato il comando per eseguire l’analisi parallela:\n\n# Determinare il numero di fattori con l'analisi parallela\nfa.parallel(x = model.building[, var_names], fa = \"fa\")\n#&gt; Parallel analysis suggests that the number of factors =  5  and the number of components =  NA\n\n\n\n\n\n\n\nQuesto comando genera un grafico e un output testuale, fornendo indicazioni sul numero plausibile di fattori.\nIn sintesi, l’analisi parallela e il BIC sono strumenti potenti e complementari per esplorare la struttura fattoriale. L’analisi parallela suggerisce un intervallo plausibile, mentre il BIC aiuta a identificare la soluzione più parsimoniosa. Integrare questi metodi con considerazioni teoriche è fondamentale per ottenere un modello fattoriale solido e interpretabile.\n\n45.8.5 Analisi Fattoriale Esplorativa\nL’Analisi Fattoriale Esplorativa (EFA) può essere eseguita utilizzando il comando seguente:\n\nEFA &lt;- efa(\n    data = model.building[, var_names],\n    nfactors = 4:6,\n    rotation = \"geomin\", \n    estimator = \"MLR\",\n    meanstructure = TRUE\n)\n\nLa funzione efa() appartiene al pacchetto lavaan e consente di esplorare il numero e la struttura dei fattori latenti nei dati. Di seguito vengono spiegati gli argomenti principali della funzione.\n\n45.8.6 Descrizione degli Argomenti\n\ndata\nSpecifica il dataset su cui eseguire l’EFA. In questo caso, include solo le colonne delle variabili di interesse.\nnfactors\nIndica l’intervallo di numeri di fattori da considerare. Qui, i modelli sono stimati con 4, 5 e 6 fattori.\n\nrotation\nQuesto argomento determina il metodo di rotazione utilizzato per identificare il modello.\n\nLa rotazione è necessaria nell’EFA, poiché, in assenza di restrizioni, esistono infinite soluzioni matematiche equivalenti. Ruotare la matrice dei carichi fattoriali consente di semplificare l’interpretazione, orientando gli assi dei fattori latenti.\n\nIn questo esempio, viene utilizzata la rotazione geomin, che permette ai fattori di essere correlati, una scelta realistica in contesti educativi e psicologici.\n\n\n\nestimator\nSpecifica il metodo di stima.\n\nIl valore predefinito è “ML” (massima verosimiglianza), ma qui viene utilizzato “MLR” (massima verosimiglianza robusta), che gestisce meglio eventuali violazioni della normalità nei dati.\n\n\n\nDati mancanti\n\nSe i dati contengono valori mancanti, è possibile utilizzare l’argomento missing = \"fiml\", che applica il metodo Full Information Maximum Likelihood (FIML). Questo approccio sfrutta tutte le informazioni disponibili ed è appropriato quando i dati mancanti sono MAR (Missing At Random).\n\n\nmeanstructure\nQuando impostato su TRUE, stima anche gli intercetti delle variabili osservate, oltre a varianze e covarianze. Se si utilizza missing = \"fiml\", l’opzione meanstructure è automaticamente attivata.\n\n45.8.7 Interpretazione dei Risultati\nPer identificare il modello migliore, è possibile ordinare i valori del BIC (Criterio di Informazione Bayesiano) in ordine crescente con il comando seguente:\n\nsort(fitMeasures(EFA)[\"bic\", ]) \n#&gt; nfactors = 5 nfactors = 4 nfactors = 6 \n#&gt;        18142        18167        18189\n\nL’output indica che il modello con cinque fattori è quello che ottiene il valore BIC più basso. Questo risultato è in linea sia con l’analisi parallela precedente sia con il numero di fattori atteso in base alla teoria (cinque fattori). Di conseguenza, il modello a cinque fattori è il più adatto per continuare l’analisi.\n\n45.8.8 Estrarre i Carichi Fattoriali\nI carichi fattoriali per il modello a cinque fattori possono essere estratti con il comando seguente:\n\nEFA$nf5\n#&gt; \n#&gt;          f1      f2      f3      f4      f5 \n#&gt; TSC1  0.584*                       *      .*\n#&gt; TSC2  0.487*                       *      .*\n#&gt; TSC3  0.637*                      .*       *\n#&gt; TSC4  0.578*      .*              .*       *\n#&gt; TSC5  0.547*                              . \n#&gt; TE1           0.728*              .         \n#&gt; TE2       .   0.672*                        \n#&gt; TE3           0.708*      .                 \n#&gt; TE4           0.651*              .*        \n#&gt; TE5           0.337*      .*      .*        \n#&gt; EE1               .   0.469*      .         \n#&gt; EE2       .*          0.689*                \n#&gt; EE3                   0.768*                \n#&gt; EE4       .*          0.732*              . \n#&gt; EE5               .   0.479*      .*        \n#&gt; DE1                  -0.353*  0.744*      . \n#&gt; DE2               .*          0.821*        \n#&gt; DE3                       .*  0.755*        \n#&gt; RPA1                                  0.851*\n#&gt; RPA2                                  0.906*\n#&gt; RPA3                                  0.624*\n#&gt; RPA4                      .       .   0.350*\n#&gt; RPA5                      .       .   0.338*\n\nL’output fornisce i carichi standardizzati, che possono essere interpretati come correlazioni tra le variabili osservate e i fattori latenti. Vengono mostrati solo i carichi assoluti superiori a 0.3.\n\n\nOsservazioni sulla struttura fattoriale\nI risultati indicano una struttura semplice, in cui ciascuna variabile carica su un solo fattore, ad eccezione della variabile DE1.\n\n\nDE1 presenta un cross-loading: un carico positivo sul fattore 4 (insieme alle altre variabili DE) e un carico negativo sul fattore 3 (insieme alle variabili EE).\n\nA parte questa eccezione, le variabili TSC, TE, EE, DE e RPA caricano rispettivamente su un unico fattore, confermando la coerenza con il modello teorico.\n\n\n\n45.8.9 Passaggi Successivi\nIl modello può ora essere affinato nella sezione CFA. Poiché la teoria non prevede il cross-loading di DE1, nel modello CFA verrà impostato a zero il carico di questa variabile sul fattore 3. Tuttavia, se il modello CFA non dovesse adattarsi bene, il ripristino di questo cross-loading sarà la prima modifica da considerare.\nQuesta procedura consente di integrare i risultati dell’EFA con la teoria e di preparare il modello per la successiva conferma tramite l’analisi fattoriale confermativa.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html#passo-2-costruire-il-modello-fattoriale-e-valutare-ladattamento",
    "href": "chapters/cfa/07_fa_in_r.html#passo-2-costruire-il-modello-fattoriale-e-valutare-ladattamento",
    "title": "45  Strategia Integrata per un’Analisi Fattoriale",
    "section": "\n45.9 Passo 2: Costruire il Modello Fattoriale e Valutare l’Adattamento",
    "text": "45.9 Passo 2: Costruire il Modello Fattoriale e Valutare l’Adattamento\nIl primo passo per costruire il modello fattoriale è definirlo utilizzando la sintassi di lavaan. Nel modello seguente, vengono specificati i 5 fattori (TSC, TE, EE, DE e RPA) in base alle variabili osservate identificate dall’EFA. Si includono inoltre le correlazioni tra fattori, come indicato dalla teoria e dai risultati precedenti. Gli intercetti non sono esplicitamente definiti, ma possono essere stimati impostando l’argomento meanstructure = TRUE.\n\nCFA_model &lt;- \"\n    # Relazioni tra variabili osservate e fattori\n    TSC =~ TSC1 + TSC2 + TSC3 + TSC5\n    TE =~ TE1 + TE2 + TE3 + TE5\n    EE =~ EE1 + EE2 + EE3 + EE4\n    DE =~ DE1 + DE2 + DE3\n    RPA =~ RPA1 + RPA2 + RPA3 + RPA4\n    # Correlazioni tra fattori\n    TSC ~~ TE + EE + DE + RPA\n    TE ~~ EE + DE + RPA\n    EE ~~ DE + RPA\n    DE ~~ RPA\n\"\n\nIl modello viene stimato con il comando seguente:\n\nCFA &lt;- cfa(\n    model = CFA_model, \n    data = model.building[, var_names],\n    estimator = \"MLR\", \n    std.lv = TRUE, \n    meanstructure = TRUE\n)\n\n\n\nestimator = \"MLR\": utilizza la massima verosimiglianza robusta, che gestisce eventuali deviazioni dalla normalità.\n\nstd.lv = TRUE: standardizza i fattori latenti, rendendo i carichi interpretabili come correlazioni.\n\nmeanstructure = TRUE: stima anche gli intercetti delle variabili osservate.\n\nL’adattamento del modello si valuta attraverso due livelli: adattamento globale e adattamento locale.\n\n45.9.1 Adattamento Globale\nL’adattamento globale verifica quanto bene l’intero modello rappresenti i dati. Le principali misure da considerare sono:\n\n\nTest Chi-quadro: verifica se il modello riproduce perfettamente le relazioni osservate. È sensibile alla dimensione del campione e tende a rifiutare l’adattamento perfetto con campioni ampi.\n\nIndice di adattamento comparativo (CFI): valuta l’adattamento relativo del modello rispetto a un modello nullo (senza correlazioni tra le variabili).\n\nErrore quadratico medio di approssimazione (RMSEA): quantifica l’adattamento approssimativo, penalizzando la complessità del modello.\n\nResiduo quadratico medio standardizzato (SRMR): rappresenta la discrepanza media tra la matrice di covarianza campionaria e quella del modello.\n\nLinee guida per l’interpretazione:\n\n\nChi-quadro: non significativo è preferibile, ma può essere ignorato in campioni ampi.\n\n\nCFI: &gt; 0.90 indica un buon adattamento; &gt; 0.95 è eccellente.\n\n\nRMSEA: &lt; 0.05 è ottimale; &lt; 0.08 è accettabile.\n\n\nSRMR: &lt; 0.08 è raccomandato.\n\nPuoi calcolare queste misure con il comando:\n\nglobalFit(CFA)\n#&gt; Results------------------------------------------------------------------------ \n#&gt;  \n#&gt; Chi-Square (142) = 319 with p-value\n#&gt;           = 1.33e-15\n#&gt; \n#&gt; CFI = 0.948\n#&gt; \n#&gt; RMSEA = 0.0533; lower bound = 0.0459;\n#&gt;       upper bound = 0.0608\n#&gt; \n#&gt; SRMR = 0.0435\n#&gt; \n#&gt; Interpretations--------------------------------------------------------------- \n#&gt;  \n#&gt; The hypothesis of perfect fit *is* rejected according to the Chi-\n#&gt;           Square test statistics because the p-value is smaller than 0.05 \n#&gt;  \n#&gt; The hypothesis of approximate model fit *is not* rejected according\n#&gt;           to the CFI because the value is larger than 0.9. \n#&gt;  \n#&gt; The hypothesis of approximate model fit *is* rejected according\n#&gt;          to the RMSEA because the point estimate is larger or equal to\n#&gt;          0.05. \n#&gt;  \n#&gt; The hypothesis of approximate model fit *is not* rejected according\n#&gt;          to the SRMR because the value is smaller than 0.08. \n#&gt; \n\n\nIl test Chi-quadro rifiuta l’adattamento perfetto, ma le altre misure (CFI, RMSEA, SRMR) indicano un buon adattamento approssimativo.\n\nPoiché almeno tre misure supportano il modello, è possibile procedere senza ulteriori modifiche.\n\n45.9.2 Adattamento Locale\nL’adattamento locale verifica se ogni parte del modello si adatta bene ai dati. Ciò si ottiene confrontando le differenze assolute tra la matrice di covarianza campionaria e quella implicata dal modello. Questo consente di identificare problemi specifici, come variabili mal rappresentate.\nIl comando seguente calcola queste differenze per ogni coppia di variabili:\n\nlocalFit(CFA) \n#&gt; $local_misfit\n#&gt;       TSC1  TSC2  TSC3  TSC5   TE1   TE2   TE3   TE5   EE1   EE2   EE3\n#&gt; TSC1 0.000                                                            \n#&gt; TSC2 0.012 0.000                                                      \n#&gt; TSC3 0.007 0.012 0.000                                                \n#&gt; TSC5 0.007 0.002 0.010 0.000                                          \n#&gt; TE1  0.019 0.000 0.009 0.010 0.000                                    \n#&gt; TE2  0.025 0.014 0.031 0.021 0.011 0.000                              \n#&gt; TE3  0.013 0.010 0.048 0.005 0.003 0.008 0.000                        \n#&gt; TE5  0.025 0.028 0.032 0.022 0.012 0.026 0.005 0.000                  \n#&gt; EE1  0.013 0.010 0.004 0.016 0.042 0.044 0.001 0.072 0.000            \n#&gt; EE2  0.004 0.009 0.025 0.003 0.029 0.050 0.027 0.043 0.002 0.000      \n#&gt; EE3  0.013 0.015 0.039 0.013 0.021 0.042 0.006 0.081 0.012 0.001 0.000\n#&gt; EE4  0.002 0.002 0.000 0.013 0.042 0.021 0.006 0.039 0.017 0.017 0.010\n#&gt; DE1  0.011 0.019 0.015 0.002 0.010 0.026 0.011 0.036 0.010 0.048 0.042\n#&gt; DE2  0.014 0.018 0.030 0.011 0.008 0.025 0.032 0.059 0.058 0.031 0.012\n#&gt; DE3  0.000 0.008 0.041 0.021 0.023 0.006 0.012 0.019 0.048 0.015 0.022\n#&gt; RPA1 0.008 0.015 0.034 0.011 0.013 0.022 0.001 0.012 0.011 0.018 0.019\n#&gt; RPA2 0.006 0.008 0.044 0.007 0.021 0.004 0.009 0.008 0.015 0.016 0.002\n#&gt; RPA3 0.041 0.016 0.012 0.003 0.006 0.010 0.017 0.034 0.035 0.008 0.022\n#&gt; RPA4 0.020 0.000 0.003 0.031 0.001 0.027 0.031 0.039 0.042 0.035 0.031\n#&gt;        EE4   DE1   DE2   DE3  RPA1  RPA2  RPA3  RPA4\n#&gt; TSC1                                                \n#&gt; TSC2                                                \n#&gt; TSC3                                                \n#&gt; TSC5                                                \n#&gt; TE1                                                 \n#&gt; TE2                                                 \n#&gt; TE3                                                 \n#&gt; TE5                                                 \n#&gt; EE1                                                 \n#&gt; EE2                                                 \n#&gt; EE3                                                 \n#&gt; EE4  0.000                                          \n#&gt; DE1  0.040 0.000                                    \n#&gt; DE2  0.052 0.004 0.000                              \n#&gt; DE3  0.012 0.002 0.006 0.000                        \n#&gt; RPA1 0.041 0.008 0.006 0.002 0.000                  \n#&gt; RPA2 0.053 0.010 0.025 0.024 0.024 0.000            \n#&gt; RPA3 0.009 0.002 0.016 0.021 0.009 0.017 0.000      \n#&gt; RPA4 0.053 0.006 0.056 0.074 0.046 0.011 0.052 0.000\n#&gt; \n#&gt; $max_misfit\n#&gt; [1] 0.0805\n\nL’output mostra che la differenza massima tra le due matrici è 0,08, un valore trascurabile rispetto alla scala delle variabili. Non emergono problemi locali degni di nota.\n\n45.9.3 Affinare il Modello\nSe fossero emersi problemi di adattamento locale, si potrebbero apportare modifiche mirate, come aggiungere covarianze tra variabili. Tuttavia, ogni modifica dovrebbe avere una solida giustificazione teorica. Non introdurre parametri aggiuntivi solo per migliorare l’adattamento!\nNel caso specifico, poiché il modello attuale non presenta problemi di adattamento, si può proseguire con la valutazione dei carichi fattoriali.\n\n45.9.4 Esaminare i Carichi Fattoriali\nI carichi fattoriali standardizzati possono essere visualizzati con il seguente comando:\n\ninspect(object = CFA, what = \"std\")$lambda\n#&gt;        TSC    TE    EE    DE   RPA\n#&gt; TSC1 0.657 0.000 0.000 0.000 0.000\n#&gt; TSC2 0.692 0.000 0.000 0.000 0.000\n#&gt; TSC3 0.628 0.000 0.000 0.000 0.000\n#&gt; TSC5 0.726 0.000 0.000 0.000 0.000\n#&gt; TE1  0.000 0.789 0.000 0.000 0.000\n#&gt; TE2  0.000 0.745 0.000 0.000 0.000\n#&gt; TE3  0.000 0.788 0.000 0.000 0.000\n#&gt; TE5  0.000 0.649 0.000 0.000 0.000\n#&gt; EE1  0.000 0.000 0.739 0.000 0.000\n#&gt; EE2  0.000 0.000 0.802 0.000 0.000\n#&gt; EE3  0.000 0.000 0.786 0.000 0.000\n#&gt; EE4  0.000 0.000 0.760 0.000 0.000\n#&gt; DE1  0.000 0.000 0.000 0.665 0.000\n#&gt; DE2  0.000 0.000 0.000 0.640 0.000\n#&gt; DE3  0.000 0.000 0.000 0.738 0.000\n#&gt; RPA1 0.000 0.000 0.000 0.000 0.849\n#&gt; RPA2 0.000 0.000 0.000 0.000 0.854\n#&gt; RPA3 0.000 0.000 0.000 0.000 0.788\n#&gt; RPA4 0.000 0.000 0.000 0.000 0.587\n\nQuesti valori indicano la forza delle relazioni tra variabili osservate e fattori latenti. Carichi superiori a 0.3 (in valore assoluto) sono generalmente considerati rilevanti.\nIn sintesi, in questo passaggio, è stato costruito e valutato un modello CFA basato su teoria e risultati dell’EFA. Il modello presenta un buon adattamento sia globale sia locale, supportando la sua validità per rappresentare i dati. Il prossimo passo sarà interpretare e utilizzare i risultati del modello per ulteriori analisi o decisioni teoriche.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html#passo-3-valutare-la-generalizzabilità",
    "href": "chapters/cfa/07_fa_in_r.html#passo-3-valutare-la-generalizzabilità",
    "title": "45  Strategia Integrata per un’Analisi Fattoriale",
    "section": "\n45.10 Passo 3: Valutare la Generalizzabilità",
    "text": "45.10 Passo 3: Valutare la Generalizzabilità\nL’ultimo passo consiste nel valutare la generalizzabilità del modello CFA definito nel Passo 2, adattandolo al campione di riserva. Questo consente di verificare se il modello è applicabile a dati indipendenti, aumentando la fiducia nella sua capacità di rappresentare in modo affidabile la struttura sottostante del costrutto in studi futuri e in campioni diversi.\n\n45.10.1 Applicazione del Modello al Campione di Riserva\nIl modello viene applicato al campione di riserva utilizzando lo stesso codice del Passo 2, ma specificando il dataset di riserva nell’argomento data:\n\nCFA_holdout &lt;- cfa(\n    model = CFA_model, \n    data = holdout[, var_names],\n    estimator = \"MLR\", \n    std.lv = TRUE, \n    meanstructure = TRUE\n)\n\nCome nel Passo 2, le misure di adattamento globale possono essere calcolate con:\n\nglobalFit(CFA_holdout)\n#&gt; Results------------------------------------------------------------------------ \n#&gt;  \n#&gt; Chi-Square (142) = 340 with p-value\n#&gt;           = 0\n#&gt; \n#&gt; CFI = 0.943\n#&gt; \n#&gt; RMSEA = 0.0564; lower bound = 0.049;\n#&gt;       upper bound = 0.0638\n#&gt; \n#&gt; SRMR = 0.0416\n#&gt; \n#&gt; Interpretations--------------------------------------------------------------- \n#&gt;  \n#&gt; The hypothesis of perfect fit *is* rejected according to the Chi-\n#&gt;           Square test statistics because the p-value is smaller than 0.05 \n#&gt;  \n#&gt; The hypothesis of approximate model fit *is not* rejected according\n#&gt;           to the CFI because the value is larger than 0.9. \n#&gt;  \n#&gt; The hypothesis of approximate model fit *is* rejected according\n#&gt;          to the RMSEA because the point estimate is larger or equal to\n#&gt;          0.05. \n#&gt;  \n#&gt; The hypothesis of approximate model fit *is not* rejected according\n#&gt;          to the SRMR because the value is smaller than 0.08. \n#&gt; \n\nDall’output, emerge che:\n\nIl test Chi-quadro rifiuta l’adattamento perfetto (un risultato atteso con campioni ampi).\n\nLe misure di adattamento approssimativo, come CFI e SRMR, confermano un buon adattamento anche nel campione di riserva.\n\nLe stime del RMSEA rientrano nei valori accettabili, indicando che il modello si adatta sufficientemente bene ai dati di riserva.\n\nQuesti risultati sono comparabili a quelli ottenuti con il campione di costruzione, suggerendo che il modello presenta una generalizzabilità adeguata.\nPer verificare l’adattamento locale, utilizziamo il comando seguente, come fatto nel Passo 2:\n\nlocalFit(CFA_holdout) \n#&gt; $local_misfit\n#&gt;       TSC1  TSC2  TSC3  TSC5   TE1   TE2   TE3   TE5   EE1   EE2   EE3\n#&gt; TSC1 0.000                                                            \n#&gt; TSC2 0.010 0.000                                                      \n#&gt; TSC3 0.012 0.015 0.000                                                \n#&gt; TSC5 0.007 0.007 0.005 0.000                                          \n#&gt; TE1  0.023 0.023 0.003 0.014 0.000                                    \n#&gt; TE2  0.027 0.012 0.019 0.008 0.008 0.000                              \n#&gt; TE3  0.012 0.010 0.024 0.008 0.012 0.002 0.000                        \n#&gt; TE5  0.019 0.014 0.008 0.002 0.046 0.006 0.013 0.000                  \n#&gt; EE1  0.037 0.023 0.009 0.005 0.035 0.009 0.002 0.011 0.000            \n#&gt; EE2  0.028 0.003 0.003 0.019 0.038 0.016 0.069 0.008 0.033 0.000      \n#&gt; EE3  0.032 0.047 0.012 0.017 0.024 0.017 0.004 0.071 0.026 0.019 0.000\n#&gt; EE4  0.006 0.033 0.003 0.002 0.027 0.002 0.002 0.048 0.015 0.020 0.004\n#&gt; DE1  0.056 0.005 0.007 0.007 0.005 0.020 0.003 0.032 0.037 0.072 0.020\n#&gt; DE2  0.005 0.029 0.032 0.061 0.012 0.014 0.046 0.006 0.024 0.038 0.034\n#&gt; DE3  0.012 0.019 0.022 0.002 0.034 0.016 0.014 0.005 0.057 0.032 0.050\n#&gt; RPA1 0.019 0.009 0.012 0.028 0.018 0.001 0.003 0.004 0.030 0.031 0.037\n#&gt; RPA2 0.009 0.020 0.023 0.001 0.017 0.016 0.018 0.004 0.003 0.045 0.008\n#&gt; RPA3 0.000 0.007 0.004 0.009 0.000 0.006 0.000 0.028 0.011 0.021 0.025\n#&gt; RPA4 0.018 0.015 0.006 0.014 0.021 0.019 0.040 0.036 0.049 0.023 0.046\n#&gt;        EE4   DE1   DE2   DE3  RPA1  RPA2  RPA3  RPA4\n#&gt; TSC1                                                \n#&gt; TSC2                                                \n#&gt; TSC3                                                \n#&gt; TSC5                                                \n#&gt; TE1                                                 \n#&gt; TE2                                                 \n#&gt; TE3                                                 \n#&gt; TE5                                                 \n#&gt; EE1                                                 \n#&gt; EE2                                                 \n#&gt; EE3                                                 \n#&gt; EE4  0.000                                          \n#&gt; DE1  0.036 0.000                                    \n#&gt; DE2  0.018 0.020 0.000                              \n#&gt; DE3  0.020 0.019 0.006 0.000                        \n#&gt; RPA1 0.003 0.014 0.029 0.004 0.000                  \n#&gt; RPA2 0.051 0.006 0.010 0.005 0.020 0.000            \n#&gt; RPA3 0.002 0.030 0.006 0.026 0.016 0.017 0.000      \n#&gt; RPA4 0.023 0.008 0.028 0.046 0.057 0.005 0.093 0.000\n#&gt; \n#&gt; $max_misfit\n#&gt; [1] 0.0931\n\nL’analisi mostra che la differenza massima tra la matrice di covarianza campionaria e quella del modello è pari a 0,09, un valore contenuto rispetto alla scala delle variabili osservate. Questo suggerisce che l’adattamento locale è soddisfacente anche per il campione di riserva, in linea con quanto osservato nel campione di costruzione.\n\n45.10.2 Confronto dei Carichi Fattoriali\nI carichi fattoriali del modello adattato al campione di riserva possono essere esaminati con:\n\ninspect(object = CFA_holdout, what = \"std\")$lambda \n#&gt;        TSC    TE    EE    DE   RPA\n#&gt; TSC1 0.679 0.000 0.000 0.000 0.000\n#&gt; TSC2 0.689 0.000 0.000 0.000 0.000\n#&gt; TSC3 0.691 0.000 0.000 0.000 0.000\n#&gt; TSC5 0.702 0.000 0.000 0.000 0.000\n#&gt; TE1  0.000 0.694 0.000 0.000 0.000\n#&gt; TE2  0.000 0.772 0.000 0.000 0.000\n#&gt; TE3  0.000 0.819 0.000 0.000 0.000\n#&gt; TE5  0.000 0.677 0.000 0.000 0.000\n#&gt; EE1  0.000 0.000 0.749 0.000 0.000\n#&gt; EE2  0.000 0.000 0.794 0.000 0.000\n#&gt; EE3  0.000 0.000 0.781 0.000 0.000\n#&gt; EE4  0.000 0.000 0.801 0.000 0.000\n#&gt; DE1  0.000 0.000 0.000 0.677 0.000\n#&gt; DE2  0.000 0.000 0.000 0.659 0.000\n#&gt; DE3  0.000 0.000 0.000 0.766 0.000\n#&gt; RPA1 0.000 0.000 0.000 0.000 0.851\n#&gt; RPA2 0.000 0.000 0.000 0.000 0.867\n#&gt; RPA3 0.000 0.000 0.000 0.000 0.700\n#&gt; RPA4 0.000 0.000 0.000 0.000 0.618\n\nI carichi fattoriali nel campione di riserva sono molto simili a quelli stimati nel campione di costruzione, confermando la stabilità della struttura fattoriale. Le differenze tra i due dataset sono trascurabili, supportando ulteriormente la generalizzabilità del modello.\nIn conclusione, il modello CFA si adatta bene sia al campione di costruzione sia a quello di riserva, con risultati simili in termini di misure di adattamento globale e locale, oltre che di carichi fattoriali. Questo supporta la conclusione che il modello ha una buona generalizzabilità e rappresenta in modo affidabile la struttura sottostante del costrutto analizzato.\nTuttavia, se il modello non si fosse adattato adeguatamente al campione di riserva (ad esempio, con misure di adattamento insufficienti o carichi fattoriali significativamente diversi), sarebbe stato necessario rivedere la struttura fattoriale. In tal caso:\n\nSi dovrebbero identificare le aree problematiche, come indicato dall’analisi dell’adattamento locale.\n\nEventuali modifiche al modello dovrebbero essere teoricamente giustificate.\nUna nuova raccolta di dati sarebbe necessaria per ripetere i tre passaggi, suddividendo i dati in un nuovo campione di costruzione e uno di riserva.\n\n45.10.3 Nota Pratica\nQuesto esempio dimostra l’importanza di suddividere i dati per testare la generalizzabilità. Tuttavia, nella pratica, raccogliere nuovi dati può essere complesso. Pertanto, è fondamentale progettare lo studio con un campione sufficientemente grande da consentire una suddivisione adeguata fin dall’inizio.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html#riflessioni-conclusive",
    "href": "chapters/cfa/07_fa_in_r.html#riflessioni-conclusive",
    "title": "45  Strategia Integrata per un’Analisi Fattoriale",
    "section": "\n45.11 Riflessioni conclusive",
    "text": "45.11 Riflessioni conclusive\nL’analisi fattoriale rappresenta uno strumento cruciale per lo studio e la comprensione di costrutti non direttamente osservabili, offrendo una metodologia rigorosa per identificare e validare strutture latenti nei dati. Questo approccio è particolarmente utile in campi come la psicologia, le scienze sociali, e molte altre discipline in cui i fenomeni di interesse non possono essere misurati direttamente, ma devono essere dedotti attraverso indicatori osservabili.\nLa versatilità dell’analisi fattoriale risiede nella sua capacità di sintetizzare informazioni complesse e di ridurre grandi insiemi di variabili a un numero limitato di fattori interpretabili. Questa caratteristica la rende particolarmente adatta per:\n\n\nCostruzione di strumenti di misura: Identificare e validare le dimensioni sottostanti a questionari e scale psicometriche.\n\nValidazione teorica: Verificare l’esistenza di costrutti teorici definiti o esplorarne di nuovi.\n\nComparazione tra gruppi: Esaminare se i costrutti latenti si manifestano allo stesso modo in diverse popolazioni o contesti.\n\nL’analisi fattoriale si estende oltre la semplice identificazione di fattori, includendo applicazioni avanzate come l’analisi fattoriale multigruppo, che consente di confrontare modelli in sottogruppi della popolazione, verificando l’invarianza di misura. Questo aspetto, fondamentale per garantire la comparabilità delle misurazioni, sarà discusso nel dettaglio nel capitolo successivo.\nNonostante i suoi punti di forza, l’analisi fattoriale presenta alcune sfide che richiedono attenzione:\n\n\nScelta del metodo appropriato: Decidere tra EFA e CFA dipende dal livello di conoscenza teorica del costrutto.\n\nAdeguatezza dei dati: La qualità dell’analisi dipende dalla dimensione del campione, dalla normalità dei dati e dalla presenza di correlazioni sufficienti tra le variabili.\n\nInterpretazione dei fattori: Sebbene i carichi fattoriali forniscano indicazioni sulla struttura latente, l’interpretazione richiede una solida base teorica e non deve essere puramente data-driven.\n\nValidazione incrociata: È essenziale testare la generalizzabilità del modello su campioni indipendenti per evitare di sovradattare il modello ai dati specifici.\n\nQuesto capitolo ha introdotto i concetti fondamentali dell’analisi fattoriale con l’obiettivo di stimolare interesse e fornire le basi per un’applicazione autonoma. Tuttavia, per sfruttare appieno il potenziale di questa metodologia, si suggerisce di approfondire:\n\n\nMetodi avanzati di rotazione: Come la rotazione obliqua o l’approccio procrustes per l’allineamento delle soluzioni.\n\nAnalisi fattoriale esplorativa a livello bayesiano: Un’alternativa moderna che incorpora incertezze nei modelli.\n\nModelli di equazioni strutturali: L’analisi fattoriale rappresenta il nucleo di questi modelli più complessi, che permettono di integrare relazioni causali tra fattori.\n\nIn conclusione, l’analisi fattoriale non si limita a essere un semplice metodo statistico, ma rappresenta un collegamento fondamentale tra la teoria e i dati, indispensabile per comprendere e validare costrutti complessi. Questo capitolo, pur introducendo solo i concetti di base, offre una solida piattaforma per approfondire le numerose applicazioni e potenzialità del metodo.\nUn tema cruciale, che sarà approfondito in un capitolo successivo, riguarda l’analisi fattoriale multigruppo. Questo approccio consente di esaminare l’invarianza di misura, un requisito essenziale per garantire che gli strumenti di misura siano equi e validi in contesti e gruppi diversi. Tale analisi sarà trattata in seguito all’interno del più ampio framework dei modelli di equazioni strutturali (SEM).",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html#session-info",
    "href": "chapters/cfa/07_fa_in_r.html#session-info",
    "title": "45  Strategia Integrata per un’Analisi Fattoriale",
    "section": "\n45.12 Session Info",
    "text": "45.12 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] devtools_2.4.5    usethis_3.1.0     effectsize_1.0.0  ggokabeito_0.1.0 \n#&gt;  [5] see_0.10.0        MASS_7.3-65       viridis_0.6.5     viridisLite_0.4.2\n#&gt;  [9] ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0  \n#&gt; [13] bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-19    \n#&gt; [17] psych_2.4.12      scales_1.3.0      markdown_1.13     knitr_1.49       \n#&gt; [21] lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n#&gt; [25] purrr_1.0.4       readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n#&gt; [29] ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] splines_4.4.2       later_1.4.1         cellranger_1.1.0   \n#&gt;   [4] R.oo_1.27.0         datawizard_1.0.0    XML_3.99-0.18      \n#&gt;   [7] rpart_4.1.24        lifecycle_1.0.4     Rdpack_2.6.2       \n#&gt;  [10] rstatix_0.7.2       rprojroot_2.0.4     lattice_0.22-6     \n#&gt;  [13] insight_1.0.2       rockchalk_1.8.157   backports_1.5.0    \n#&gt;  [16] magrittr_2.0.3      openxlsx_4.2.8      Hmisc_5.2-2        \n#&gt;  [19] rmarkdown_2.29      remotes_2.5.0       httpuv_1.6.15      \n#&gt;  [22] qgraph_1.9.8        zip_2.3.2           sessioninfo_1.2.3  \n#&gt;  [25] pkgbuild_1.4.6      pbapply_1.7-2       minqa_1.2.8        \n#&gt;  [28] multcomp_1.4-28     abind_1.4-8         pkgload_1.4.0      \n#&gt;  [31] quadprog_1.5-8      R.utils_2.13.0      nnet_7.3-20        \n#&gt;  [34] TH.data_1.1-3       sandwich_3.1-1      arm_1.14-4         \n#&gt;  [37] codetools_0.2-20    tidyselect_1.2.1    farver_2.1.2       \n#&gt;  [40] lme4_1.1-36         stats4_4.4.2        base64enc_0.1-3    \n#&gt;  [43] jsonlite_1.9.0      ellipsis_0.3.2      Formula_1.2-5      \n#&gt;  [46] survival_3.8-3      emmeans_1.10.7      tools_4.4.2        \n#&gt;  [49] rio_1.2.3           Rcpp_1.0.14         glue_1.8.0         \n#&gt;  [52] mnormt_2.1.1        xfun_0.51           numDeriv_2016.8-1.1\n#&gt;  [55] withr_3.0.2         fastmap_1.2.0       boot_1.3-31        \n#&gt;  [58] digest_0.6.37       mi_1.1              timechange_0.3.0   \n#&gt;  [61] R6_2.6.1            mime_0.12           estimability_1.5.1 \n#&gt;  [64] colorspace_2.1-1    gtools_3.9.5        jpeg_0.1-10        \n#&gt;  [67] R.methodsS3_1.8.2   generics_0.1.3      data.table_1.17.0  \n#&gt;  [70] corpcor_1.6.10      htmlwidgets_1.6.4   parameters_0.24.1  \n#&gt;  [73] pkgconfig_2.0.3     sem_3.1-16          gtable_0.3.6       \n#&gt;  [76] htmltools_0.5.8.1   carData_3.0-5       profvis_0.4.0      \n#&gt;  [79] png_0.1-8           reformulas_0.4.0    rstudioapi_0.17.1  \n#&gt;  [82] tzdb_0.4.0          reshape2_1.4.4      curl_6.2.1         \n#&gt;  [85] coda_0.19-4.1       checkmate_2.3.2     nlme_3.1-167       \n#&gt;  [88] nloptr_2.1.1        zoo_1.8-13          cachem_1.1.0       \n#&gt;  [91] parallel_4.4.2      miniUI_0.1.1.1      foreign_0.8-88     \n#&gt;  [94] pillar_1.10.1       grid_4.4.2          vctrs_0.6.5        \n#&gt;  [97] urlchecker_1.0.1    promises_1.3.2      car_3.1-3          \n#&gt; [100] OpenMx_2.21.13      xtable_1.8-4        cluster_2.1.8      \n#&gt; [103] htmlTable_2.4.3     evaluate_1.0.3      pbivnorm_0.6.0     \n#&gt; [106] mvtnorm_1.3-3       cli_3.6.4           kutils_1.73        \n#&gt; [109] compiler_4.4.2      rlang_1.1.5         ggsignif_0.6.4     \n#&gt; [112] labeling_0.4.3      fdrtool_1.2.18      plyr_1.8.9         \n#&gt; [115] fs_1.6.5            stringi_1.8.4       munsell_0.5.1      \n#&gt; [118] lisrelToR_0.3       bayestestR_0.15.2   pacman_0.5.1       \n#&gt; [121] Matrix_1.7-2        hms_1.1.3           glasso_1.11        \n#&gt; [124] shiny_1.10.0        rbibutils_2.3       igraph_2.1.4       \n#&gt; [127] broom_1.0.7         memoise_2.0.1       RcppParallel_5.1.10\n#&gt; [130] readxl_1.4.4\n\n\n\n\n\nSaqr, M., & López-Pernas, S. (2024). Learning analytics methods and tutorials: A practical guide using R. Springer Nature.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/E_01.html",
    "href": "chapters/cfa/E_01.html",
    "title": "46  ✏️ Esercizi",
    "section": "",
    "text": "source(\"../_common.R\")\nsuppressPackageStartupMessages({\n    library(\"lavaan\")\n    library(\"semTools\")\n})\nset.seed(42)\n\nE1. Si ripeta l’esercizio che abbiamo svolto in precedenza usando l’analisi fattoriale esplorativa, questa volta usando la CFA in lavaan. I dati sono forniti da Brown (2015) e riguardano a otto misure di personalità raccolte su un campione di 250 pazienti che hanno concluso un programma di psicoterapia:\n\nanxiety (N1),\nhostility (N2),\ndepression (N3),\nself-consciousness (N4),\nwarmth (E1),\ngregariousness (E2),\nassertiveness (E3),\npositive emotions (E4).\n\n\nvarnames &lt;- c(\"N1\", \"N2\", \"N3\", \"N4\", \"E1\", \"E2\", \"E3\", \"E4\")\nsds &lt;- \"5.7  5.6  6.4  5.7  6.0  6.2  5.7  5.6\"\n\ncors &lt;- \"\n 1.000\n 0.767  1.000\n 0.731  0.709  1.000\n 0.778  0.738  0.762  1.000\n-0.351  -0.302  -0.356  -0.318  1.000\n-0.316  -0.280  -0.300  -0.267  0.675  1.000\n-0.296  -0.289  -0.297  -0.296  0.634  0.651  1.000\n-0.282  -0.254  -0.292  -0.245  0.534  0.593  0.566  1.000\"\n\npsychot_cor_mat &lt;- getCov(cors, names = varnames)\nn &lt;- 250\n\nIl modello con due fattori ortogonali può essere adattato ai dati nel modo seguente.\n\ncfa_mod &lt;- \"\n  N =~ N1 + N2 + N3 + N4\n  E =~ E1 + E2 + E3 + E4\n\"\n\n\nfit_cfa &lt;- lavaan::cfa(\n    cfa_mod,\n    sample.cov = psychot_cor_mat,\n    sample.nobs = n,\n    orthogonal = TRUE,\n    std.lv = TRUE\n)\n\n\nsemPlot::semPaths(fit_cfa,\n    what = \"col\", whatLabels = \"par\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\n\nEsaminiamo le saturazioni fattoriali:\n\nparameterEstimates(fit_cfa, standardized = TRUE) %&gt;%\n    dplyr::filter(op == \"=~\") %&gt;%\n    dplyr::select(\n        \"Latent Factor\" = lhs,\n        Indicator = rhs,\n        B = est,\n        SE = se,\n        Z = z,\n        \"p-value\" = pvalue,\n        Beta = std.all\n    ) %&gt;%\n    knitr::kable(\n        digits = 3, booktabs = TRUE, format = \"markdown\",\n        caption = \"Factor Loadings\"\n    )\n\n\n\nTable: Factor Loadings\n\n|Latent Factor |Indicator |     B|    SE|      Z| p-value|  Beta|\n|:-------------|:---------|-----:|-----:|------:|-------:|-----:|\n|N             |N1        | 0.882| 0.051| 17.422|       0| 0.884|\n|N             |N2        | 0.847| 0.052| 16.340|       0| 0.849|\n|N             |N3        | 0.840| 0.052| 16.134|       0| 0.842|\n|N             |N4        | 0.882| 0.051| 17.432|       0| 0.884|\n|E             |E1        | 0.795| 0.056| 14.276|       0| 0.796|\n|E             |E2        | 0.838| 0.054| 15.369|       0| 0.839|\n|E             |E3        | 0.788| 0.056| 14.097|       0| 0.789|\n|E             |E4        | 0.697| 0.058| 11.942|       0| 0.699|\n\n\nIl risultato sembra sensato: le saturazioni su ciascun fattore sono molto alte. Tuttavia, la matrice delle correlazioni residue\n\ncor_table &lt;- residuals(fit_cfa, type = \"cor\")$cov\nknitr::kable(\n    cor_table,\n    digits = 3,\n    format = \"markdown\",\n    booktabs = TRUE\n)\n\n\n\n|   |     N1|     N2|     N3|     N4|     E1|     E2|     E3|     E4|\n|:--|------:|------:|------:|------:|------:|------:|------:|------:|\n|N1 |  0.000|  0.017| -0.013| -0.003| -0.351| -0.316| -0.296| -0.282|\n|N2 |  0.017|  0.000| -0.006| -0.012| -0.302| -0.280| -0.289| -0.254|\n|N3 | -0.013| -0.006|  0.000|  0.018| -0.356| -0.300| -0.297| -0.292|\n|N4 | -0.003| -0.012|  0.018|  0.000| -0.318| -0.267| -0.296| -0.245|\n|E1 | -0.351| -0.302| -0.356| -0.318|  0.000|  0.007|  0.006| -0.022|\n|E2 | -0.316| -0.280| -0.300| -0.267|  0.007|  0.000| -0.011|  0.007|\n|E3 | -0.296| -0.289| -0.297| -0.296|  0.006| -0.011|  0.000|  0.015|\n|E4 | -0.282| -0.254| -0.292| -0.245| -0.022|  0.007|  0.015|  0.000|\n\n\nrivela che il modello ipotizzato dall’analisi fattoriale confermativa non è adeguato.\n\nfit2_cfa &lt;- lavaan::cfa(\n    cfa_mod,\n    sample.cov = psychot_cor_mat,\n    sample.nobs = n,\n    orthogonal = FALSE,\n    std.lv = TRUE\n)\n\n\nsemPlot::semPaths(fit2_cfa,\n    what = \"col\", whatLabels = \"par\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\n\nEsaminiamo le saturazioni fattoriali.\n\nparameterEstimates(fit2_cfa, standardized = TRUE) %&gt;%\n    dplyr::filter(op == \"=~\") %&gt;%\n    dplyr::select(\n        \"Latent Factor\" = lhs,\n        Indicator = rhs,\n        B = est,\n        SE = se,\n        Z = z,\n        \"p-value\" = pvalue,\n        Beta = std.all\n    ) %&gt;%\n    knitr::kable(\n        digits = 3, booktabs = TRUE, format = \"markdown\",\n        caption = \"Factor Loadings\"\n    )\n\n\n\nTable: Factor Loadings\n\n|Latent Factor |Indicator |     B|    SE|      Z| p-value|  Beta|\n|:-------------|:---------|-----:|-----:|------:|-------:|-----:|\n|N             |N1        | 0.883| 0.051| 17.472|       0| 0.885|\n|N             |N2        | 0.847| 0.052| 16.337|       0| 0.849|\n|N             |N3        | 0.842| 0.052| 16.190|       0| 0.844|\n|N             |N4        | 0.880| 0.051| 17.381|       0| 0.882|\n|E             |E1        | 0.800| 0.055| 14.465|       0| 0.802|\n|E             |E2        | 0.832| 0.054| 15.294|       0| 0.834|\n|E             |E3        | 0.788| 0.056| 14.150|       0| 0.789|\n|E             |E4        | 0.698| 0.058| 11.974|       0| 0.699|\n\n\nEsaminiamo i residui.\n\ncor_table &lt;- residuals(fit2_cfa, type = \"cor\")$cov\nknitr::kable(\n    cor_table,\n    digits = 3,\n    format = \"markdown\",\n    booktabs = TRUE\n)\n\n\n\n|   |     N1|     N2|     N3|     N4|     E1|     E2|     E3|     E4|\n|:--|------:|------:|------:|------:|------:|------:|------:|------:|\n|N1 |  0.000|  0.016| -0.015| -0.002| -0.042|  0.005|  0.008| -0.013|\n|N2 |  0.016|  0.000| -0.007| -0.010| -0.006|  0.028|  0.002|  0.004|\n|N3 | -0.015| -0.007|  0.000|  0.018| -0.062|  0.006| -0.007| -0.035|\n|N4 | -0.002| -0.010|  0.018|  0.000| -0.010|  0.053|  0.007|  0.023|\n|E1 | -0.042| -0.006| -0.062| -0.010|  0.000|  0.006|  0.001| -0.027|\n|E2 |  0.005|  0.028|  0.006|  0.053|  0.006|  0.000| -0.007|  0.010|\n|E3 |  0.008|  0.002| -0.007|  0.007|  0.001| -0.007|  0.000|  0.014|\n|E4 | -0.013|  0.004| -0.035|  0.023| -0.027|  0.010|  0.014|  0.000|\n\n\nSistemiamo le saturazioni fattoriali in una matrice 8 \\(\\times\\) 2:\n\nlambda &lt;- inspect(fit2_cfa, what = \"std\")$lambda\nlambda\n\n\nA lavaan.matrix: 8 x 2 of type dbl\n\n\n\nN\nE\n\n\n\n\nN1\n0.8848214\n0.0000000\n\n\nN2\n0.8485128\n0.0000000\n\n\nN3\n0.8436432\n0.0000000\n\n\nN4\n0.8819736\n0.0000000\n\n\nE1\n0.0000000\n0.8018485\n\n\nE2\n0.0000000\n0.8337599\n\n\nE3\n0.0000000\n0.7894530\n\n\nE4\n0.0000000\n0.6990366\n\n\n\n\n\nOtteniamo la matrice di intercorrelazoni fattoriali.\n\nPhi &lt;- inspect(fit2_cfa, what = \"std\")$psi\nPhi\n\n\nA lavaan.matrix.symmetric: 2 x 2 of type dbl\n\n\n\nN\nE\n\n\n\n\nN\n1.000000\n-0.434962\n\n\nE\n-0.434962\n1.000000\n\n\n\n\n\nOtteniamo la matrice di varianze residue.\n\nPsi &lt;- inspect(fit2_cfa, what = \"std\")$theta\nPsi\n\n\nA lavaan.matrix.symmetric: 8 x 8 of type dbl\n\n\n\nN1\nN2\nN3\nN4\nE1\nE2\nE3\nE4\n\n\n\n\nN1\n0.217091\n0.0000000\n0.0000000\n0.0000000\n0.000000\n0.0000000\n0.000000\n0.0000000\n\n\nN2\n0.000000\n0.2800261\n0.0000000\n0.0000000\n0.000000\n0.0000000\n0.000000\n0.0000000\n\n\nN3\n0.000000\n0.0000000\n0.2882661\n0.0000000\n0.000000\n0.0000000\n0.000000\n0.0000000\n\n\nN4\n0.000000\n0.0000000\n0.0000000\n0.2221225\n0.000000\n0.0000000\n0.000000\n0.0000000\n\n\nE1\n0.000000\n0.0000000\n0.0000000\n0.0000000\n0.357039\n0.0000000\n0.000000\n0.0000000\n\n\nE2\n0.000000\n0.0000000\n0.0000000\n0.0000000\n0.000000\n0.3048445\n0.000000\n0.0000000\n\n\nE3\n0.000000\n0.0000000\n0.0000000\n0.0000000\n0.000000\n0.0000000\n0.376764\n0.0000000\n\n\nE4\n0.000000\n0.0000000\n0.0000000\n0.0000000\n0.000000\n0.0000000\n0.000000\n0.5113478\n\n\n\n\n\nMediante i parametri del modello la matrice di correlazione si riproduce nel modo seguente:\n\\[\n\\boldsymbol{\\Sigma} =\\boldsymbol{\\Lambda} \\boldsymbol{\\Phi} \\boldsymbol{\\Lambda}^{\\mathsf{T}} + \\boldsymbol{\\Psi}.\n\\]\nIn \\(\\textsf{R}\\) scriviamo:\n\nR_hat &lt;- lambda %*% Phi %*% t(lambda) + Psi\nR_hat %&gt;%\n    round(3)\n\n\nA lavaan.matrix.symmetric: 8 x 8 of type dbl\n\n\n\nN1\nN2\nN3\nN4\nE1\nE2\nE3\nE4\n\n\n\n\nN1\n1.000\n0.751\n0.746\n0.780\n-0.309\n-0.321\n-0.304\n-0.269\n\n\nN2\n0.751\n1.000\n0.716\n0.748\n-0.296\n-0.308\n-0.291\n-0.258\n\n\nN3\n0.746\n0.716\n1.000\n0.744\n-0.294\n-0.306\n-0.290\n-0.257\n\n\nN4\n0.780\n0.748\n0.744\n1.000\n-0.308\n-0.320\n-0.303\n-0.268\n\n\nE1\n-0.309\n-0.296\n-0.294\n-0.308\n1.000\n0.669\n0.633\n0.561\n\n\nE2\n-0.321\n-0.308\n-0.306\n-0.320\n0.669\n1.000\n0.658\n0.583\n\n\nE3\n-0.304\n-0.291\n-0.290\n-0.303\n0.633\n0.658\n1.000\n0.552\n\n\nE4\n-0.269\n-0.258\n-0.257\n-0.268\n0.561\n0.583\n0.552\n1.000\n\n\n\n\n\nLe correlazioni residue sono:\n\n(psychot_cor_mat - R_hat) %&gt;%\n    round(3)\n\n\nA lavaan.matrix.symmetric: 8 x 8 of type dbl\n\n\n\nN1\nN2\nN3\nN4\nE1\nE2\nE3\nE4\n\n\n\n\nN1\n0.000\n0.016\n-0.015\n-0.002\n-0.042\n0.005\n0.008\n-0.013\n\n\nN2\n0.016\n0.000\n-0.007\n-0.010\n-0.006\n0.028\n0.002\n0.004\n\n\nN3\n-0.015\n-0.007\n0.000\n0.018\n-0.062\n0.006\n-0.007\n-0.035\n\n\nN4\n-0.002\n-0.010\n0.018\n0.000\n-0.010\n0.053\n0.007\n0.023\n\n\nE1\n-0.042\n-0.006\n-0.062\n-0.010\n0.000\n0.006\n0.001\n-0.027\n\n\nE2\n0.005\n0.028\n0.006\n0.053\n0.006\n0.000\n-0.007\n0.010\n\n\nE3\n0.008\n0.002\n-0.007\n0.007\n0.001\n-0.007\n0.000\n0.014\n\n\nE4\n-0.013\n0.004\n-0.035\n0.023\n-0.027\n0.010\n0.014\n0.000\n\n\n\n\n\nCalcoliamo la correlazione predetta dal modello tra le variabili \\(Y_1\\) e \\(Y_2\\):\n\nlambda[1, 1] * lambda[2, 1] + lambda[1, 2] * lambda[2, 2] +\n    lambda[1, 1] * lambda[2, 2] * Phi[1, 2] +\n    lambda[1, 2] * lambda[2, 1] * Phi[1, 2]\n\n0.750782309575684\n\n\nQuesto risultato è molto simile al valore contenuto dell’elemento (1, 2) della matrice di correlazioni osservate:\n\npsychot_cor_mat[1, 2]\n\n0.767\n\n\nUsando le funzonalità di lavaan la matrice di correlazione predetta si ottiene con:\n\nfitted(fit2_cfa)$cov |&gt;\n    print()\n\n       N1     N2     N3     N4     E1     E2     E3     E4\nN1  0.996                                                 \nN2  0.748  0.996                                          \nN3  0.743  0.713  0.996                                   \nN4  0.777  0.745  0.741  0.996                            \nE1 -0.307 -0.295 -0.293 -0.306  0.996                     \nE2 -0.320 -0.306 -0.305 -0.319  0.666  0.996              \nE3 -0.303 -0.290 -0.289 -0.302  0.630  0.656  0.996       \nE4 -0.268 -0.257 -0.255 -0.267  0.558  0.580  0.550  0.996\n\n\nLa matrice dei residui è\n\nresid(fit2_cfa)$cov |&gt;\n    print()\n\n       N1     N2     N3     N4     E1     E2     E3     E4\nN1  0.000                                                 \nN2  0.016  0.000                                          \nN3 -0.015 -0.007  0.000                                   \nN4 -0.002 -0.010  0.018  0.000                            \nE1 -0.042 -0.006 -0.062 -0.010  0.000                     \nE2  0.005  0.028  0.006  0.053  0.006  0.000              \nE3  0.008  0.002 -0.007  0.007  0.001 -0.007  0.000       \nE4 -0.013  0.004 -0.035  0.023 -0.026  0.010  0.014  0.000\n\n\nLa matrice dei residui standardizzati è\n\nresid(fit2_cfa, type = \"standardized\")$cov |&gt;\n    print()\n\n       N1     N2     N3     N4     E1     E2     E3     E4\nN1  0.000                                                 \nN2  1.674  0.000                                          \nN3 -1.769 -0.569  0.000                                   \nN4 -0.350 -1.152  1.746  0.000                            \nE1 -1.214 -0.161 -1.646 -0.294  0.000                     \nE2  0.154  0.794  0.168  1.626  0.637  0.000              \nE3  0.219  0.062 -0.191  0.193  0.075 -0.693  0.000       \nE4 -0.314  0.092 -0.824  0.552 -1.481  0.624  0.690  0.000\n\n\nI valori precedenti possono essere considerati come punti z, dove i valori con un valore assoluto maggiore di 2 possono essere ritenuti problematici. Tuttavia, è importante considerare che in questo modo si stanno eseguendo molteplici confronti, pertanto, si dovrebbe considerare l’opportunità di applicare una qualche forma di correzione per i confronti multipli.\nE2. Si utilizzino i dati dass21.txt che corrispondono alla somministrazione del test DASS-21 a 334 partecipanti. Lo schema di codifica si può trovare seguendo questo link. Si adatti ai dati un modello a tre fattori usando l’analisi fattoriale esplorativa con la funzione lavaan::efa(). Usando le saturazioni fattoriali e la matrice di inter-correlazioni fattoriali, si trovi la matrice di correlazioni riprodotta dal modello. Senza usare l’albebra matriciale, si trovi la correlazione predetta tra gli indicatori DASS-1 e DASS-2.\n\n\n\n\nBrown, T. A. (2015). Confirmatory factor analysis for applied research. Guilford publications.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/E_02_bifactor.html",
    "href": "chapters/cfa/E_02_bifactor.html",
    "title": "47  ✏️ Esercizi",
    "section": "",
    "text": "47.1 Introduzione\nIn questo esercizio replicheremo la validazione della Strengths and Weaknesses of ADHD Symptoms and Normal Behavior Scale descritta nell’articolo di Blume et al. (2020).\nGli adulti con sintomi di disturbo da deficit di attenzione/iperattività (ADHD; American Psychiatric Association [APA], 2013) presentano sintomi di disattenzione (ad esempio, difficoltà a mantenere l’attenzione sul lavoro, durante compiti o attività), iperattività-impulsività (ad esempio, interrompere o intromettersi nelle conversazioni, parlare in modo eccessivo), o una combinazione di entrambi. Questi sintomi sono associati a compromissioni nel funzionamento accademico (ad esempio, tassi più bassi di diploma e laurea), lavorativo (ad esempio, redditi complessivamente inferiori) e sociale (ad esempio, meno amici, tassi di divorzio più alti).\nL’ADHD si manifesta inizialmente durante l’infanzia e persiste nell’età adulta in circa la metà dei casi, con una prevalenza stimata del 2.5% negli adulti. Clinicamente, l’ADHD si presenta in tre modalità principali:\nSwanson e colleghi (2012) hanno introdotto la scala Strengths and Weaknesses of ADHD-Symptoms and Normal-Behavior (SWAN), che valuta i sintomi di disattenzione e iperattività-impulsività nei bambini in età scolare tramite un report di terze parti. La scala, composta da 18 item, si basa sui criteri sintomatici definiti nel Diagnostic and Statistical Manual of Mental Disorders (DSM-IV; APA, 2000) e confermati nel DSM-5 (APA, 2013). La SWAN è stata progettata per valutare il comportamento dei bambini, concentrandosi su situazioni scolastiche, di gioco e domestiche. La scala utilizza un punteggio a 7 punti, con ancore che rappresentano gli estremi negativi (“molto al di sotto della media”) e positivi (“molto al di sopra della media”), confrontando il comportamento del bambino con quello di altri coetanei. La SWAN è stata la prima scala a valutare i sintomi dell’ADHD in modo realmente dimensionale.\nBlume et al. (2020) adattano la versione tedesca esistente, SWAN-DE (Schulz-Zhecheva et al., 2019), in una versione self-report per adulti, denominandola German Strengths and Weaknesses of ADHD and Normal-Behavior Scale Self-Report (SWAN-DE-SB).",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/E_02_bifactor.html#introduzione",
    "href": "chapters/cfa/E_02_bifactor.html#introduzione",
    "title": "47  ✏️ Esercizi",
    "section": "",
    "text": "Presentazione prevalentemente disattenta: predominano i sintomi di disattenzione;\nPresentazione prevalentemente iperattiva-impulsiva: predominano i sintomi di iperattività-impulsività;\nPresentazione combinata: sono presenti livelli significativi di entrambi i tipi di sintomi.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/E_02_bifactor.html#validazione",
    "href": "chapters/cfa/E_02_bifactor.html#validazione",
    "title": "47  ✏️ Esercizi",
    "section": "47.2 Validazione",
    "text": "47.2 Validazione\nDi seguito è fornita una parte dello script R fornito dagli autori per l’analisi statistica dai dati grezzi fino alla formulazione del modello bifattoriale.\n\ndata &lt;- rio::import(here::here(\"data\", \"blume_2024\", \"data_total_OSF.csv\"))\n\ndata$X &lt;- NULL # delete column without information\n\ndata$SW_mean &lt;- as.numeric(data$SW_mean) # convert from character to numeric\ndata$SW_AD_mean &lt;- as.numeric(data$SW_AD_mean)\ndata$SW_HI_mean &lt;- as.numeric(data$SW_HI_mean)\n\ndata$CA_mean &lt;- as.numeric(data$CA_mean)\ndata$CA_AD_mean &lt;- as.numeric(data$CA_AD_mean)\ndata$CA_HI_mean &lt;- as.numeric(data$CA_HI_mean)\n\ndata$HA_mean &lt;- as.numeric(data$HA_mean)\ndata$HA_AD_mean &lt;- as.numeric(data$HA_AD_mean)\ndata$HA_HI_mean &lt;- as.numeric(data$HA_HI_mean)\n\n\ndata_clin &lt;- rio::import(here::here(\"data\", \"blume_2024\", \"data_clinical_OSF.csv\"))\n\ndata_clin$X &lt;- NULL # delete column without information\n\ndata_clin$SW_mean &lt;- as.numeric(data_clin$SW_mean) # convert from character to numeric\ndata_clin$SW_AD_mean &lt;- as.numeric(data_clin$SW_AD_mean)\ndata_clin$SW_HI_mean &lt;- as.numeric(data_clin$SW_HI_mean)\n\ndata_clin$CA_mean &lt;- as.numeric(data_clin$CA_mean)\ndata_clin$CA_AD_mean &lt;- as.numeric(data_clin$CA_AD_mean)\ndata_clin$CA_HI_mean &lt;- as.numeric(data_clin$CA_HI_mean)\n\ndata_clin$HA_mean &lt;- as.numeric(data_clin$HA_mean)\ndata_clin$HA_AD_mean &lt;- as.numeric(data_clin$HA_AD_mean)\ndata_clin$HA_HI_mean &lt;- as.numeric(data_clin$HA_HI_mean)\n\n\n# Information on missing data in the general population sample\nSWAN_vars &lt;- colnames(data)[str_detect(colnames(data), \"SW01\")]\nsum(is.na(data[, SWAN_vars])) # 1 data point missing\nsum(!is.na(data[, SWAN_vars])) # 7163 not missing -&gt; 0.01% missing\n\n1\n\n\n7163\n\n\n\n# age\n\nsem_age1 &lt;- \"\n        SW_GF =~ SW01_01 + SW01_02 + SW01_03 + SW01_04 + SW01_05 + SW01_06\n                           + SW01_07 + SW01_08 + SW01_09 + SW01_10 + SW01_11 + SW01_12\n                           + SW01_13 + SW01_14 + SW01_15 + SW01_16 + SW01_17 + SW01_18;\n        SW_GF ~ age\n\"\nfit_age1 &lt;- sem(sem_age1, data = data)\n\n# Regressions:\n#                   Estimate  Std.Err  z-value  P(&gt;|z|)\n#  SW_GF ~\n#    age               0.001    0.003    0.285    0.775\n\n\nsem_age2 &lt;- \"\n        SW_AD =~ SW01_01 + SW01_02 + SW01_03 + SW01_04 + SW01_05 + SW01_06\n                + SW01_07 + SW01_08 + SW01_09;\n        SW_AD ~ age\n\"\nfit_age2 &lt;- sem(sem_age2, data = data)\n\n# Regressions:\n#                   Estimate  Std.Err  z-value  P(&gt;|z|)\n#  SW_AD ~\n#    age               0.004    0.003    1.018    0.309\n\nsem_age3 &lt;- \"\n        SW_HI =~ SW01_10 + SW01_11 + SW01_12 + SW01_13 + SW01_14 + SW01_15\n                + SW01_16 + SW01_17 + SW01_18;\n        SW_HI ~ age\n\"\nfit_age3 &lt;- sem(sem_age3, data = data)\n\n# Regressions:\n#                   Estimate  Std.Err  z-value  P(&gt;|z|)\n#  SW_HI ~\n#    age              -0.002    0.005   -0.530    0.596\n\n\nglimpse(data)\n\nRows: 398\nColumns: 84\n$ V1             &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, ~\n$ id             &lt;int&gt; 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,~\n$ gender         &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\n$ age            &lt;int&gt; 36, 26, 21, 21, 21, 19, 22, 25, 28, 20, 19, 32, 18,~\n$ diagnosis_ever &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ~\n$ diagnosis_now  &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ~\n$ medication     &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ~\n$ education      &lt;int&gt; 4, 3, 3, 3, 3, 3, 3, 4, 4, 3, 3, 4, 3, 3, 3, 4, 3, ~\n$ SW01_01        &lt;int&gt; 2, 4, 4, 4, 4, 6, 4, 6, 3, 4, 5, 6, 4, 4, 3, 5, 4, ~\n$ SW01_02        &lt;int&gt; 2, 6, 3, 3, 5, 5, 4, 4, 4, 1, 5, 5, 3, 4, 3, 5, 1, ~\n$ SW01_03        &lt;int&gt; 4, 6, 5, 3, 4, 6, 6, 6, 5, 4, 5, 5, 5, 6, 2, 5, 4, ~\n$ SW01_04        &lt;int&gt; 4, 6, 3, 4, 5, 6, 5, 5, 5, 3, 6, 5, 4, 5, 3, 5, 1, ~\n$ SW01_05        &lt;int&gt; 3, 3, 5, 4, 5, 6, 6, 6, 6, 5, 4, 6, 5, 5, 6, 5, 2, ~\n$ SW01_06        &lt;int&gt; 4, 3, 3, 3, 5, 6, 6, 5, 3, 1, 6, 5, 5, 5, 2, 5, 2, ~\n$ SW01_07        &lt;int&gt; 4, 4, 3, 3, 5, 6, 6, 6, 6, 3, 3, 5, 4, 5, 5, 5, 4, ~\n$ SW01_08        &lt;int&gt; 3, 3, 2, 3, 3, 4, 3, 5, 5, 1, 3, 4, 3, 2, 1, 2, 3, ~\n$ SW01_09        &lt;int&gt; 3, 4, 3, 4, 5, 4, 0, 6, 6, 5, 1, 5, 3, 6, 4, 4, 4, ~\n$ SW01_10        &lt;int&gt; 3, 5, 2, 3, 4, 3, 6, 3, 4, 5, 3, 5, 3, 4, 3, 3, 2, ~\n$ SW01_11        &lt;int&gt; 3, 6, 3, 3, 4, 5, 6, 6, 4, 6, 5, 5, 3, 6, 3, 3, 5, ~\n$ SW01_12        &lt;int&gt; 3, 3, 3, 3, 3, 6, 6, 6, 3, 6, 4, 5, 2, 3, 3, 3, 2, ~\n$ SW01_13        &lt;int&gt; 4, 6, 3, 3, 3, 6, 2, 6, 5, 2, 5, 6, 3, 5, 4, 3, 1, ~\n$ SW01_14        &lt;int&gt; 3, 1, 3, 3, 3, 6, 6, 6, 5, 6, 5, 4, 4, 5, 1, 2, 5, ~\n$ SW01_15        &lt;int&gt; 3, 4, 3, 3, 5, 6, 3, 6, 4, 4, 5, 5, 3, 4, 5, 2, 2, ~\n$ SW01_16        &lt;int&gt; 2, 6, 3, 3, 5, 5, 2, 6, 3, 6, 5, 5, 3, 4, 3, 2, 2, ~\n$ SW01_17        &lt;int&gt; 3, 4, 4, 3, 3, 5, 3, 6, 3, 6, 5, 5, 3, 4, 1, 1, 4, ~\n$ SW01_18        &lt;int&gt; 3, 4, 3, 3, 3, 6, 3, 6, 4, 2, 5, 5, 3, 1, 3, 2, 0, ~\n$ HA01_01        &lt;int&gt; 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 2, ~\n$ HA01_02        &lt;int&gt; 2, 1, 0, 1, 0, 0, 1, 0, 0, 2, 0, 2, 1, 0, 0, 0, 2, ~\n$ HA01_03        &lt;int&gt; 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 2, 0, 1, 1, 0, 1, ~\n$ HA01_04        &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, ~\n$ HA01_05        &lt;int&gt; 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, ~\n$ HA01_06        &lt;int&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ~\n$ HA01_07        &lt;int&gt; 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, ~\n$ HA01_08        &lt;int&gt; 1, 0, 1, 1, 0, 0, 1, 1, 0, 2, 1, 1, 1, 1, 2, 1, 2, ~\n$ HA01_09        &lt;int&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, ~\n$ HA01_10        &lt;int&gt; 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, ~\n$ HA01_11        &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\n$ HA01_12        &lt;int&gt; 1, 3, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 2, 0, 1, ~\n$ HA01_13        &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\n$ HA01_14        &lt;int&gt; 1, 2, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 1, 1, 2, 0, ~\n$ HA01_15        &lt;int&gt; 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 2, 2, ~\n$ HA01_16        &lt;int&gt; 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, ~\n$ HA01_17        &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, ~\n$ HA01_18        &lt;int&gt; 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, ~\n$ HA01_19        &lt;int&gt; 0, 1, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 1, 0, 1, 3, 2, ~\n$ HA01_20        &lt;int&gt; 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, ~\n$ HA01_21        &lt;int&gt; 1, 3, 0, 0, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 1, 1, ~\n$ HA01_22        &lt;int&gt; 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ~\n$ CA01_01        &lt;int&gt; 1, 0, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 2, 2, 2, ~\n$ CA01_02        &lt;int&gt; 1, 2, 1, 0, 0, 1, 2, 1, 3, 0, 0, 1, 0, 0, 1, 3, 0, ~\n$ CA01_03        &lt;int&gt; 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 2, ~\n$ CA01_04        &lt;int&gt; 1, 0, 0, 0, 2, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, ~\n$ CA01_05        &lt;int&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, ~\n$ CA01_06        &lt;int&gt; 2, 2, 2, 1, 2, 1, 0, 1, 1, 3, 2, 0, 1, 2, 0, 3, 0, ~\n$ CA01_07        &lt;int&gt; 2, 3, 2, 1, 1, 1, 1, 0, 1, 2, 1, 0, 0, 1, 1, 1, 0, ~\n$ CA01_08        &lt;int&gt; 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, ~\n$ CA01_09        &lt;int&gt; 0, 1, 2, 0, 0, 2, 0, 0, 0, 2, 1, 0, 1, 2, 1, 0, 1, ~\n$ CA01_10        &lt;int&gt; 1, 0, 1, 1, 1, 1, 2, 0, 1, 1, 0, 2, 1, 1, 1, 3, 0, ~\n$ CA01_11        &lt;int&gt; 1, 2, 2, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, ~\n$ CA01_12        &lt;int&gt; 2, 1, 3, 1, 0, 0, 1, 1, 0, 3, 1, 2, 0, 1, 2, 0, 1, ~\n$ CA01_13        &lt;int&gt; 2, 2, 2, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 1, 1, 0, 0, ~\n$ CA01_14        &lt;int&gt; 1, 1, 0, 0, 1, 0, 2, 0, 0, 2, 1, 2, 0, 0, 1, 1, 2, ~\n$ CA01_15        &lt;int&gt; 2, 3, 2, 2, 2, 1, 2, 0, 1, 1, 1, 2, 0, 1, 2, 3, 1, ~\n$ CA01_16        &lt;int&gt; 2, 3, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, ~\n$ CA01_17        &lt;int&gt; 2, 1, 2, 1, 2, 0, 0, 0, 0, 2, 0, 1, 0, 1, 1, 3, 3, ~\n$ CA01_18        &lt;int&gt; 2, 2, 2, 1, 1, 0, 0, 1, 1, 3, 0, 2, 1, 0, 1, 2, 3, ~\n$ CA01_19        &lt;int&gt; 0, 0, 2, 0, 1, 0, 1, 0, 1, 0, 2, 1, 0, 1, 0, 1, 1, ~\n$ CA01_20        &lt;int&gt; 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 2, 1, 0, 1, 1, 0, 1, ~\n$ CA01_21        &lt;int&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 3, 1, 2, 0, 0, 0, 2, 1, ~\n$ CA01_22        &lt;int&gt; 2, 1, 3, 0, 0, 0, 3, 0, 0, 1, 0, 2, 0, 0, 1, 0, 2, ~\n$ CA01_23        &lt;int&gt; 1, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ~\n$ CA01_24        &lt;int&gt; 0, 0, 1, 0, 0, 0, 2, 1, 0, 3, 0, 1, 0, 0, 0, 0, 0, ~\n$ CA01_25        &lt;int&gt; 1, 3, 3, 0, 1, 0, 0, 0, 1, 2, 0, 1, 0, 1, 3, 0, 3, ~\n$ CA01_26        &lt;int&gt; 1, 3, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ~\n$ diagnosis_type &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\n$ SW_mean        &lt;dbl&gt; 3.11, 4.33, 3.22, 3.22, 4.11, 5.39, 4.28, 5.56, 4.3~\n$ SW_AD_mean     &lt;dbl&gt; 3.22, 4.33, 3.44, 3.44, 4.56, 5.44, 4.44, 5.44, 4.7~\n$ SW_HI_mean     &lt;dbl&gt; 3.00, 4.33, 3.00, 3.00, 3.67, 5.33, 4.11, 5.67, 3.8~\n$ CA_mean        &lt;dbl&gt; 1.154, 1.308, 1.423, 0.462, 0.692, 0.538, 0.692, 0.~\n$ CA_AD_mean     &lt;dbl&gt; 1.0, 1.0, 0.8, 0.6, 1.0, 0.0, 0.2, 0.2, 0.2, 2.0, 0~\n$ CA_HI_mean     &lt;dbl&gt; 1.3, 1.1, 1.3, 0.5, 0.7, 0.8, 0.4, 0.1, 0.7, 1.1, 0~\n$ HA_mean        &lt;dbl&gt; 0.5000, 0.9091, 0.1364, 0.3636, 0.1364, 0.1364, 0.5~\n$ HA_AD_mean     &lt;dbl&gt; 0.778, 0.667, 0.111, 0.444, 0.222, 0.000, 0.333, 0.~\n$ HA_HI_mean     &lt;dbl&gt; 0.333, 0.778, 0.222, 0.333, 0.111, 0.333, 0.667, 0.~\n\n\n\n# education\nsem_education1 &lt;- \"\n        SW_GF =~ SW01_01 + SW01_02 + SW01_03 + SW01_04 + SW01_05 + SW01_06\n                           + SW01_07 + SW01_08 + SW01_09 + SW01_10 + SW01_11 + SW01_12\n                           + SW01_13 + SW01_14 + SW01_15 + SW01_16 + SW01_17 + SW01_18;\n        SW_GF ~ education\n\"\nfit_education1 &lt;- sem(sem_education1, data = data)\n\n# Regressions:\n#                   Estimate  Std.Err  z-value  P(&gt;|z|)\n#  SW_GF ~\n#    education         0.170    0.059    2.897    0.004\n\nsem_education2 &lt;- \"\n        SW_AD =~ SW01_01 + SW01_02 + SW01_03 + SW01_04 + SW01_05 + SW01_06\n                + SW01_07 + SW01_08 + SW01_09;\n        SW_AD ~ education\n\"\nfit_education2 &lt;- sem(sem_education2, data = data)\n\n# Regressions:\n#                   Estimate  Std.Err  z-value  P(&gt;|z|)\n#  SW_AD ~\n#    education         0.209    0.066    3.196    0.001\n\nsem_education3 &lt;- \"\n        SW_HI =~ SW01_10 + SW01_11 + SW01_12 + SW01_13 + SW01_14 + SW01_15\n                + SW01_16 + SW01_17 + SW01_18;\n        SW_HI ~ education\n\"\nfit_education3 &lt;- sem(sem_education3, data = data)\n\n# Regressions:\n#                   Estimate  Std.Err  z-value  P(&gt;|z|)\n#  SW_HI ~\n#    education         0.172    0.086    2.016    0.044\n\n# interactions\n\n\nSWAN_vars &lt;- colnames(data)[str_detect(colnames(data), \"SW01\")]\n\nSW_AD &lt;- colnames(data[, c(\n    \"SW01_01\",\n    \"SW01_02\",\n    \"SW01_03\",\n    \"SW01_04\",\n    \"SW01_05\",\n    \"SW01_06\",\n    \"SW01_07\",\n    \"SW01_08\",\n    \"SW01_09\"\n)])\n\nSW_HI &lt;- colnames(data[, c(\n    \"SW01_10\",\n    \"SW01_11\",\n    \"SW01_12\",\n    \"SW01_13\",\n    \"SW01_14\",\n    \"SW01_15\",\n    \"SW01_16\",\n    \"SW01_17\",\n    \"SW01_18\"\n)])\n\n# Cronbachs alphas\npsych::alpha(data[, SWAN_vars]) # 0.90\npsych::alpha(data[, SW_AD]) # 0.85\npsych::alpha(data[, SW_HI]) # 0.87\n\n\nReliability analysis   \nCall: psych::alpha(x = data[, SWAN_vars])\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean   sd median_r\n       0.9       0.9    0.92      0.33 8.8 0.0075  3.8 0.83     0.33\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.88   0.9  0.91\nDuhachek  0.88   0.9  0.91\n\n Reliability if an item is dropped:\n        raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r\nSW01_01      0.89      0.90    0.91      0.33 8.5   0.0078 0.014  0.34\nSW01_02      0.89      0.89    0.91      0.32 8.1   0.0081 0.013  0.32\nSW01_03      0.89      0.89    0.91      0.33 8.3   0.0080 0.014  0.32\nSW01_04      0.89      0.89    0.91      0.33 8.3   0.0080 0.013  0.33\nSW01_05      0.89      0.90    0.91      0.34 8.6   0.0077 0.012  0.33\nSW01_06      0.89      0.89    0.91      0.33 8.5   0.0078 0.014  0.33\nSW01_07      0.89      0.89    0.91      0.33 8.3   0.0080 0.013  0.33\nSW01_08      0.90      0.90    0.91      0.34 8.7   0.0077 0.013  0.34\nSW01_09      0.89      0.89    0.91      0.33 8.3   0.0080 0.014  0.32\nSW01_10      0.89      0.89    0.91      0.32 8.2   0.0081 0.014  0.32\nSW01_11      0.89      0.89    0.91      0.32 8.2   0.0081 0.013  0.32\nSW01_12      0.89      0.89    0.91      0.32 8.0   0.0082 0.012  0.32\nSW01_13      0.89      0.89    0.91      0.32 8.2   0.0081 0.013  0.32\nSW01_14      0.89      0.89    0.91      0.33 8.3   0.0080 0.014  0.33\nSW01_15      0.90      0.90    0.91      0.34 8.7   0.0077 0.012  0.33\nSW01_16      0.89      0.90    0.91      0.34 8.6   0.0077 0.011  0.33\nSW01_17      0.89      0.89    0.91      0.33 8.4   0.0079 0.013  0.33\nSW01_18      0.89      0.89    0.91      0.33 8.3   0.0080 0.013  0.33\n\n Item statistics \n          n raw.r std.r r.cor r.drop mean  sd\nSW01_01 398  0.54  0.54  0.51   0.48  3.9 1.3\nSW01_02 397  0.68  0.68  0.67   0.63  3.5 1.3\nSW01_03 398  0.63  0.64  0.61   0.58  4.4 1.2\nSW01_04 398  0.63  0.63  0.61   0.57  3.9 1.5\nSW01_05 398  0.53  0.52  0.50   0.45  4.1 1.5\nSW01_06 398  0.55  0.55  0.52   0.48  3.9 1.4\nSW01_07 398  0.63  0.63  0.62   0.57  4.1 1.3\nSW01_08 398  0.48  0.48  0.43   0.41  2.6 1.3\nSW01_09 398  0.62  0.61  0.58   0.55  3.6 1.5\nSW01_10 398  0.67  0.67  0.64   0.61  3.5 1.5\nSW01_11 398  0.66  0.67  0.65   0.61  4.4 1.3\nSW01_12 398  0.73  0.73  0.73   0.69  3.7 1.3\nSW01_13 398  0.67  0.67  0.66   0.61  4.0 1.4\nSW01_14 398  0.62  0.62  0.59   0.56  3.4 1.5\nSW01_15 398  0.50  0.50  0.46   0.43  3.8 1.4\nSW01_16 398  0.53  0.53  0.50   0.46  3.9 1.4\nSW01_17 398  0.58  0.58  0.55   0.52  3.5 1.3\nSW01_18 398  0.64  0.64  0.62   0.58  3.6 1.4\n\nNon missing response frequency for each item\n           0    1    2    3    4    5    6 miss\nSW01_01 0.01 0.05 0.10 0.17 0.31 0.31 0.06    0\nSW01_02 0.02 0.07 0.10 0.31 0.22 0.25 0.03    0\nSW01_03 0.01 0.02 0.06 0.17 0.23 0.36 0.17    0\nSW01_04 0.02 0.05 0.08 0.25 0.21 0.25 0.15    0\nSW01_05 0.01 0.06 0.09 0.17 0.21 0.26 0.20    0\nSW01_06 0.02 0.04 0.09 0.23 0.23 0.28 0.13    0\nSW01_07 0.01 0.03 0.07 0.23 0.26 0.27 0.14    0\nSW01_08 0.06 0.13 0.23 0.38 0.12 0.06 0.03    0\nSW01_09 0.03 0.07 0.10 0.28 0.25 0.17 0.11    0\nSW01_10 0.03 0.07 0.14 0.31 0.19 0.18 0.09    0\nSW01_11 0.01 0.02 0.03 0.24 0.22 0.24 0.25    0\nSW01_12 0.00 0.03 0.06 0.42 0.20 0.16 0.12    0\nSW01_13 0.00 0.04 0.08 0.28 0.20 0.23 0.17    0\nSW01_14 0.03 0.09 0.17 0.24 0.21 0.17 0.10    0\nSW01_15 0.01 0.06 0.10 0.23 0.24 0.25 0.12    0\nSW01_16 0.01 0.03 0.13 0.25 0.22 0.21 0.16    0\nSW01_17 0.01 0.04 0.13 0.37 0.21 0.18 0.06    0\nSW01_18 0.01 0.05 0.15 0.30 0.18 0.21 0.10    0\n\n\n\nReliability analysis   \nCall: psych::alpha(x = data[, SW_AD])\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r\n      0.85      0.85    0.85      0.38 5.5 0.012  3.8 0.92     0.38\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.82  0.85  0.87\nDuhachek  0.82  0.85  0.87\n\n Reliability if an item is dropped:\n        raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nSW01_01      0.84      0.84    0.83      0.39 5.1    0.012 0.0107  0.39\nSW01_02      0.82      0.82    0.81      0.36 4.4    0.014 0.0094  0.33\nSW01_03      0.83      0.83    0.83      0.38 5.0    0.013 0.0109  0.38\nSW01_04      0.82      0.82    0.82      0.36 4.6    0.014 0.0092  0.35\nSW01_05      0.83      0.83    0.82      0.38 4.9    0.013 0.0083  0.39\nSW01_06      0.83      0.83    0.83      0.39 5.1    0.012 0.0104  0.39\nSW01_07      0.82      0.82    0.81      0.36 4.5    0.014 0.0087  0.35\nSW01_08      0.84      0.85    0.84      0.41 5.5    0.012 0.0080  0.39\nSW01_09      0.83      0.83    0.83      0.38 5.0    0.013 0.0115  0.38\n\n Item statistics \n          n raw.r std.r r.cor r.drop mean  sd\nSW01_01 398  0.61  0.62  0.55   0.50  3.9 1.3\nSW01_02 397  0.77  0.78  0.76   0.69  3.5 1.3\nSW01_03 398  0.64  0.65  0.58   0.54  4.4 1.2\nSW01_04 398  0.74  0.73  0.70   0.64  3.9 1.5\nSW01_05 398  0.68  0.67  0.63   0.56  4.1 1.5\nSW01_06 398  0.63  0.63  0.56   0.51  3.9 1.4\nSW01_07 398  0.75  0.75  0.73   0.67  4.1 1.3\nSW01_08 398  0.53  0.54  0.44   0.40  2.6 1.3\nSW01_09 398  0.66  0.65  0.58   0.53  3.6 1.5\n\nNon missing response frequency for each item\n           0    1    2    3    4    5    6 miss\nSW01_01 0.01 0.05 0.10 0.17 0.31 0.31 0.06    0\nSW01_02 0.02 0.07 0.10 0.31 0.22 0.25 0.03    0\nSW01_03 0.01 0.02 0.06 0.17 0.23 0.36 0.17    0\nSW01_04 0.02 0.05 0.08 0.25 0.21 0.25 0.15    0\nSW01_05 0.01 0.06 0.09 0.17 0.21 0.26 0.20    0\nSW01_06 0.02 0.04 0.09 0.23 0.23 0.28 0.13    0\nSW01_07 0.01 0.03 0.07 0.23 0.26 0.27 0.14    0\nSW01_08 0.06 0.13 0.23 0.38 0.12 0.06 0.03    0\nSW01_09 0.03 0.07 0.10 0.28 0.25 0.17 0.11    0\n\n\n\nReliability analysis   \nCall: psych::alpha(x = data[, SW_HI])\n\n  raw_alpha std.alpha G6(smc) average_r S/N  ase mean   sd median_r\n      0.87      0.87    0.87      0.43 6.7 0.01  3.8 0.97     0.42\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.85  0.87  0.89\nDuhachek  0.85  0.87  0.89\n\n Reliability if an item is dropped:\n        raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nSW01_10      0.86      0.86    0.86      0.43 6.0    0.011 0.0073  0.41\nSW01_11      0.85      0.85    0.85      0.42 5.7    0.011 0.0059  0.41\nSW01_12      0.85      0.85    0.84      0.41 5.6    0.012 0.0059  0.41\nSW01_13      0.85      0.85    0.85      0.42 5.7    0.012 0.0069  0.41\nSW01_14      0.86      0.86    0.86      0.44 6.3    0.011 0.0079  0.44\nSW01_15      0.86      0.87    0.86      0.45 6.4    0.010 0.0065  0.44\nSW01_16      0.85      0.86    0.85      0.43 6.0    0.011 0.0082  0.41\nSW01_17      0.86      0.86    0.86      0.43 6.1    0.011 0.0083  0.43\nSW01_18      0.85      0.85    0.85      0.42 5.8    0.011 0.0086  0.40\n\n Item statistics \n          n raw.r std.r r.cor r.drop mean  sd\nSW01_10 398  0.69  0.69  0.63   0.59  3.5 1.5\nSW01_11 398  0.73  0.74  0.72   0.65  4.4 1.3\nSW01_12 398  0.77  0.77  0.75   0.70  3.7 1.3\nSW01_13 398  0.74  0.75  0.71   0.66  4.0 1.4\nSW01_14 398  0.66  0.65  0.58   0.54  3.4 1.5\nSW01_15 398  0.62  0.62  0.55   0.51  3.8 1.4\nSW01_16 398  0.70  0.70  0.65   0.60  3.9 1.4\nSW01_17 398  0.66  0.67  0.61   0.56  3.5 1.3\nSW01_18 398  0.73  0.73  0.69   0.64  3.6 1.4\n\nNon missing response frequency for each item\n           0    1    2    3    4    5    6 miss\nSW01_10 0.03 0.07 0.14 0.31 0.19 0.18 0.09    0\nSW01_11 0.01 0.02 0.03 0.24 0.22 0.24 0.25    0\nSW01_12 0.00 0.03 0.06 0.42 0.20 0.16 0.12    0\nSW01_13 0.00 0.04 0.08 0.28 0.20 0.23 0.17    0\nSW01_14 0.03 0.09 0.17 0.24 0.21 0.17 0.10    0\nSW01_15 0.01 0.06 0.10 0.23 0.24 0.25 0.12    0\nSW01_16 0.01 0.03 0.13 0.25 0.22 0.21 0.16    0\nSW01_17 0.01 0.04 0.13 0.37 0.21 0.18 0.06    0\nSW01_18 0.01 0.05 0.15 0.30 0.18 0.21 0.10    0\n\n\n\npsych::omega(data[SWAN_vars], nfactors = 2)\n\nOmega \nCall: omegah(m = m, nfactors = nfactors, fm = fm, key = key, flip = flip, \n    digits = digits, title = title, sl = sl, labels = labels, \n    plot = plot, n.obs = n.obs, rotate = rotate, Phi = Phi, option = option, \n    covar = covar)\nAlpha:                 0.9 \nG.6:                   0.92 \nOmega Hierarchical:    0.58 \nOmega H asymptotic:    0.63 \nOmega Total            0.91 \n\nSchmid Leiman Factor loadings greater than  0.2 \n           g   F1*   F2*   h2   h2   u2   p2  com\nSW01_01 0.40  0.34       0.28 0.28 0.72 0.56 2.06\nSW01_02 0.54  0.50       0.55 0.55 0.45 0.53 2.03\nSW01_03 0.49  0.32  0.20 0.38 0.38 0.62 0.62 2.12\nSW01_04 0.50  0.54       0.54 0.54 0.46 0.46 1.99\nSW01_05 0.40  0.52       0.44 0.44 0.56 0.37 1.95\nSW01_06 0.41  0.36       0.30 0.30 0.70 0.55 2.05\nSW01_07 0.50  0.55       0.55 0.55 0.45 0.45 1.98\nSW01_08 0.34  0.27       0.20 0.20 0.80 0.59 2.08\nSW01_09 0.46  0.35       0.36 0.36 0.64 0.60 2.10\nSW01_10 0.51        0.36 0.43 0.43 0.57 0.61 2.11\nSW01_11 0.52        0.46 0.49 0.49 0.51 0.55 2.05\nSW01_12 0.58        0.44 0.56 0.56 0.44 0.60 2.09\nSW01_13 0.52        0.49 0.51 0.51 0.49 0.53 2.03\nSW01_14 0.46        0.34 0.35 0.35 0.65 0.61 2.10\nSW01_15 0.35        0.44 0.32 0.32 0.68 0.38 1.95\nSW01_16 0.39        0.59 0.53 0.53 0.47 0.29 1.93\nSW01_17 0.44        0.43 0.38 0.38 0.62 0.50 2.01\nSW01_18 0.48        0.50 0.48 0.48 0.52 0.48 2.00\n\nWith Sums of squares  of:\n  g F1* F2*  h2 \n3.9 1.8 2.0 3.5 \n\ngeneral/max  1.13   max/min =   1.91\nmean percent general =  0.52    with sd =  0.09 and cv of  0.18 \nExplained Common Variance of the general factor =  0.51 \n\nThe degrees of freedom are 118  and the fit is  0.99 \nThe number of observations was  398  with Chi Square =  386  with prob &lt;  3.4e-30\nThe root mean square of the residuals is  0.05 \nThe df corrected root mean square of the residuals is  0.05\nRMSEA index =  0.075  and the 10 % confidence intervals are  0.067 0.084\nBIC =  -320\n\nCompare this with the adequacy of just a general factor and no group factors\nThe degrees of freedom for just the general factor are 135  and the fit is  2.52 \nThe number of observations was  398  with Chi Square =  980  with prob &lt;  3.5e-128\nThe root mean square of the residuals is  0.15 \nThe df corrected root mean square of the residuals is  0.16 \n\nRMSEA index =  0.125  and the 10 % confidence intervals are  0.118 0.133\nBIC =  172 \n\nMeasures of factor score adequacy             \n                                                 g  F1*  F2*\nCorrelation of scores with factors            0.76 0.73 0.74\nMultiple R square of scores with factors      0.58 0.54 0.54\nMinimum correlation of factor score estimates 0.17 0.08 0.09\n\n Total, General and Subset omega for each subset\n                                                 g  F1*  F2*\nOmega total for total scores and subscales    0.91 0.84 0.87\nOmega general for total scores and subscales  0.58 0.45 0.46\nOmega group for total scores and subscales    0.26 0.39 0.41\n\n\n\n\n\n\n\n\n\n\n# Correlation Matrix\ncorr.test(data[, c(SW_AD, \"SW_AD_mean\")]) # 0.53 - 0.77\ncorr.test(data[, c(SW_HI, \"SW_HI_mean\")]) # 0.62 - 0.77\ncorr.test(data[, c(SWAN_vars, \"SW_mean\")]) # 0.48 - 0.73\n\nCall:corr.test(x = data[, c(SW_AD, \"SW_AD_mean\")])\nCorrelation matrix \n           SW01_01 SW01_02 SW01_03 SW01_04 SW01_05 SW01_06 SW01_07 SW01_08\nSW01_01       1.00    0.49    0.32    0.31    0.38    0.36    0.32    0.22\nSW01_02       0.49    1.00    0.47    0.53    0.38    0.53    0.46    0.41\nSW01_03       0.32    0.47    1.00    0.44    0.32    0.31    0.40    0.23\nSW01_04       0.31    0.53    0.44    1.00    0.53    0.39    0.54    0.30\nSW01_05       0.38    0.38    0.32    0.53    1.00    0.28    0.61    0.20\nSW01_06       0.36    0.53    0.31    0.39    0.28    1.00    0.40    0.27\nSW01_07       0.32    0.46    0.40    0.54    0.61    0.40    1.00    0.33\nSW01_08       0.22    0.41    0.23    0.30    0.20    0.27    0.33    1.00\nSW01_09       0.33    0.40    0.41    0.39    0.34    0.28    0.48    0.29\nSW_AD_mean    0.61    0.77    0.64    0.74    0.68    0.63    0.75    0.53\n           SW01_09 SW_AD_mean\nSW01_01       0.33       0.61\nSW01_02       0.40       0.77\nSW01_03       0.41       0.64\nSW01_04       0.39       0.74\nSW01_05       0.34       0.68\nSW01_06       0.28       0.63\nSW01_07       0.48       0.75\nSW01_08       0.29       0.53\nSW01_09       1.00       0.66\nSW_AD_mean    0.66       1.00\nSample Size \n           SW01_01 SW01_02 SW01_03 SW01_04 SW01_05 SW01_06 SW01_07 SW01_08\nSW01_01        398     397     398     398     398     398     398     398\nSW01_02        397     397     397     397     397     397     397     397\nSW01_03        398     397     398     398     398     398     398     398\nSW01_04        398     397     398     398     398     398     398     398\nSW01_05        398     397     398     398     398     398     398     398\nSW01_06        398     397     398     398     398     398     398     398\nSW01_07        398     397     398     398     398     398     398     398\nSW01_08        398     397     398     398     398     398     398     398\nSW01_09        398     397     398     398     398     398     398     398\nSW_AD_mean     398     397     398     398     398     398     398     398\n           SW01_09 SW_AD_mean\nSW01_01        398        398\nSW01_02        397        397\nSW01_03        398        398\nSW01_04        398        398\nSW01_05        398        398\nSW01_06        398        398\nSW01_07        398        398\nSW01_08        398        398\nSW01_09        398        398\nSW_AD_mean     398        398\nProbability values (Entries above the diagonal are adjusted for multiple tests.) \n           SW01_01 SW01_02 SW01_03 SW01_04 SW01_05 SW01_06 SW01_07 SW01_08\nSW01_01          0       0       0       0       0       0       0       0\nSW01_02          0       0       0       0       0       0       0       0\nSW01_03          0       0       0       0       0       0       0       0\nSW01_04          0       0       0       0       0       0       0       0\nSW01_05          0       0       0       0       0       0       0       0\nSW01_06          0       0       0       0       0       0       0       0\nSW01_07          0       0       0       0       0       0       0       0\nSW01_08          0       0       0       0       0       0       0       0\nSW01_09          0       0       0       0       0       0       0       0\nSW_AD_mean       0       0       0       0       0       0       0       0\n           SW01_09 SW_AD_mean\nSW01_01          0          0\nSW01_02          0          0\nSW01_03          0          0\nSW01_04          0          0\nSW01_05          0          0\nSW01_06          0          0\nSW01_07          0          0\nSW01_08          0          0\nSW01_09          0          0\nSW_AD_mean       0          0\n\n To see confidence intervals of the correlations, print with the short=FALSE option\n\n\nCall:corr.test(x = data[, c(SW_HI, \"SW_HI_mean\")])\nCorrelation matrix \n           SW01_10 SW01_11 SW01_12 SW01_13 SW01_14 SW01_15 SW01_16 SW01_17\nSW01_10       1.00    0.49    0.58    0.42    0.44    0.30    0.33    0.34\nSW01_11       0.49    1.00    0.61    0.61    0.40    0.30    0.42    0.47\nSW01_12       0.58    0.61    1.00    0.55    0.50    0.34    0.39    0.41\nSW01_13       0.42    0.61    0.55    1.00    0.37    0.38    0.48    0.37\nSW01_14       0.44    0.40    0.50    0.37    1.00    0.32    0.32    0.36\nSW01_15       0.30    0.30    0.34    0.38    0.32    1.00    0.50    0.32\nSW01_16       0.33    0.42    0.39    0.48    0.32    0.50    1.00    0.46\nSW01_17       0.34    0.47    0.41    0.37    0.36    0.32    0.46    1.00\nSW01_18       0.44    0.38    0.49    0.51    0.37    0.44    0.49    0.47\nSW_HI_mean    0.69    0.73    0.77    0.74    0.66    0.62    0.70    0.66\n           SW01_18 SW_HI_mean\nSW01_10       0.44       0.69\nSW01_11       0.38       0.73\nSW01_12       0.49       0.77\nSW01_13       0.51       0.74\nSW01_14       0.37       0.66\nSW01_15       0.44       0.62\nSW01_16       0.49       0.70\nSW01_17       0.47       0.66\nSW01_18       1.00       0.73\nSW_HI_mean    0.73       1.00\nSample Size \n[1] 398\nProbability values (Entries above the diagonal are adjusted for multiple tests.) \n           SW01_10 SW01_11 SW01_12 SW01_13 SW01_14 SW01_15 SW01_16 SW01_17\nSW01_10          0       0       0       0       0       0       0       0\nSW01_11          0       0       0       0       0       0       0       0\nSW01_12          0       0       0       0       0       0       0       0\nSW01_13          0       0       0       0       0       0       0       0\nSW01_14          0       0       0       0       0       0       0       0\nSW01_15          0       0       0       0       0       0       0       0\nSW01_16          0       0       0       0       0       0       0       0\nSW01_17          0       0       0       0       0       0       0       0\nSW01_18          0       0       0       0       0       0       0       0\nSW_HI_mean       0       0       0       0       0       0       0       0\n           SW01_18 SW_HI_mean\nSW01_10          0          0\nSW01_11          0          0\nSW01_12          0          0\nSW01_13          0          0\nSW01_14          0          0\nSW01_15          0          0\nSW01_16          0          0\nSW01_17          0          0\nSW01_18          0          0\nSW_HI_mean       0          0\n\n To see confidence intervals of the correlations, print with the short=FALSE option\n\n\nCall:corr.test(x = data[, c(SWAN_vars, \"SW_mean\")])\nCorrelation matrix \n        SW01_01 SW01_02 SW01_03 SW01_04 SW01_05 SW01_06 SW01_07 SW01_08\nSW01_01    1.00    0.49    0.32    0.31    0.38    0.36    0.32    0.22\nSW01_02    0.49    1.00    0.47    0.53    0.38    0.53    0.46    0.41\nSW01_03    0.32    0.47    1.00    0.44    0.32    0.31    0.40    0.23\nSW01_04    0.31    0.53    0.44    1.00    0.53    0.39    0.54    0.30\nSW01_05    0.38    0.38    0.32    0.53    1.00    0.28    0.61    0.20\nSW01_06    0.36    0.53    0.31    0.39    0.28    1.00    0.40    0.27\nSW01_07    0.32    0.46    0.40    0.54    0.61    0.40    1.00    0.33\nSW01_08    0.22    0.41    0.23    0.30    0.20    0.27    0.33    1.00\nSW01_09    0.33    0.40    0.41    0.39    0.34    0.28    0.48    0.29\nSW01_10    0.34    0.43    0.34    0.38    0.23    0.30    0.29    0.28\nSW01_11    0.22    0.35    0.41    0.35    0.21    0.25    0.29    0.19\nSW01_12    0.30    0.38    0.38    0.42    0.28    0.35    0.37    0.26\nSW01_13    0.22    0.35    0.40    0.29    0.23    0.24    0.33    0.18\nSW01_14    0.24    0.32    0.33    0.31    0.26    0.20    0.29    0.34\nSW01_15    0.29    0.13    0.21    0.12    0.12    0.19    0.15    0.14\nSW01_16    0.17    0.18    0.28    0.09    0.03    0.17    0.09    0.15\nSW01_17    0.17    0.28    0.37    0.24    0.16    0.23    0.28    0.17\nSW01_18    0.25    0.34    0.30    0.21    0.17    0.28    0.25    0.28\nSW_mean    0.54    0.68    0.63    0.63    0.53    0.55    0.63    0.48\n        SW01_09 SW01_10 SW01_11 SW01_12 SW01_13 SW01_14 SW01_15 SW01_16\nSW01_01    0.33    0.34    0.22    0.30    0.22    0.24    0.29    0.17\nSW01_02    0.40    0.43    0.35    0.38    0.35    0.32    0.13    0.18\nSW01_03    0.41    0.34    0.41    0.38    0.40    0.33    0.21    0.28\nSW01_04    0.39    0.38    0.35    0.42    0.29    0.31    0.12    0.09\nSW01_05    0.34    0.23    0.21    0.28    0.23    0.26    0.12    0.03\nSW01_06    0.28    0.30    0.25    0.35    0.24    0.20    0.19    0.17\nSW01_07    0.48    0.29    0.29    0.37    0.33    0.29    0.15    0.09\nSW01_08    0.29    0.28    0.19    0.26    0.18    0.34    0.14    0.15\nSW01_09    1.00    0.35    0.33    0.38    0.36    0.37    0.20    0.23\nSW01_10    0.35    1.00    0.49    0.58    0.42    0.44    0.30    0.33\nSW01_11    0.33    0.49    1.00    0.61    0.61    0.40    0.30    0.42\nSW01_12    0.38    0.58    0.61    1.00    0.55    0.50    0.34    0.39\nSW01_13    0.36    0.42    0.61    0.55    1.00    0.37    0.38    0.48\nSW01_14    0.37    0.44    0.40    0.50    0.37    1.00    0.32    0.32\nSW01_15    0.20    0.30    0.30    0.34    0.38    0.32    1.00    0.50\nSW01_16    0.23    0.33    0.42    0.39    0.48    0.32    0.50    1.00\nSW01_17    0.27    0.34    0.47    0.41    0.37    0.36    0.32    0.46\nSW01_18    0.27    0.44    0.38    0.49    0.51    0.37    0.44    0.49\nSW_mean    0.62    0.67    0.66    0.73    0.67    0.62    0.50    0.53\n        SW01_17 SW01_18 SW_mean\nSW01_01    0.17    0.25    0.54\nSW01_02    0.28    0.34    0.68\nSW01_03    0.37    0.30    0.63\nSW01_04    0.24    0.21    0.63\nSW01_05    0.16    0.17    0.53\nSW01_06    0.23    0.28    0.55\nSW01_07    0.28    0.25    0.63\nSW01_08    0.17    0.28    0.48\nSW01_09    0.27    0.27    0.62\nSW01_10    0.34    0.44    0.67\nSW01_11    0.47    0.38    0.66\nSW01_12    0.41    0.49    0.73\nSW01_13    0.37    0.51    0.67\nSW01_14    0.36    0.37    0.62\nSW01_15    0.32    0.44    0.50\nSW01_16    0.46    0.49    0.53\nSW01_17    1.00    0.47    0.58\nSW01_18    0.47    1.00    0.64\nSW_mean    0.58    0.64    1.00\nSample Size \n        SW01_01 SW01_02 SW01_03 SW01_04 SW01_05 SW01_06 SW01_07 SW01_08\nSW01_01     398     397     398     398     398     398     398     398\nSW01_02     397     397     397     397     397     397     397     397\nSW01_03     398     397     398     398     398     398     398     398\nSW01_04     398     397     398     398     398     398     398     398\nSW01_05     398     397     398     398     398     398     398     398\nSW01_06     398     397     398     398     398     398     398     398\nSW01_07     398     397     398     398     398     398     398     398\nSW01_08     398     397     398     398     398     398     398     398\nSW01_09     398     397     398     398     398     398     398     398\nSW01_10     398     397     398     398     398     398     398     398\nSW01_11     398     397     398     398     398     398     398     398\nSW01_12     398     397     398     398     398     398     398     398\nSW01_13     398     397     398     398     398     398     398     398\nSW01_14     398     397     398     398     398     398     398     398\nSW01_15     398     397     398     398     398     398     398     398\nSW01_16     398     397     398     398     398     398     398     398\nSW01_17     398     397     398     398     398     398     398     398\nSW01_18     398     397     398     398     398     398     398     398\nSW_mean     398     397     398     398     398     398     398     398\n        SW01_09 SW01_10 SW01_11 SW01_12 SW01_13 SW01_14 SW01_15 SW01_16\nSW01_01     398     398     398     398     398     398     398     398\nSW01_02     397     397     397     397     397     397     397     397\nSW01_03     398     398     398     398     398     398     398     398\nSW01_04     398     398     398     398     398     398     398     398\nSW01_05     398     398     398     398     398     398     398     398\nSW01_06     398     398     398     398     398     398     398     398\nSW01_07     398     398     398     398     398     398     398     398\nSW01_08     398     398     398     398     398     398     398     398\nSW01_09     398     398     398     398     398     398     398     398\nSW01_10     398     398     398     398     398     398     398     398\nSW01_11     398     398     398     398     398     398     398     398\nSW01_12     398     398     398     398     398     398     398     398\nSW01_13     398     398     398     398     398     398     398     398\nSW01_14     398     398     398     398     398     398     398     398\nSW01_15     398     398     398     398     398     398     398     398\nSW01_16     398     398     398     398     398     398     398     398\nSW01_17     398     398     398     398     398     398     398     398\nSW01_18     398     398     398     398     398     398     398     398\nSW_mean     398     398     398     398     398     398     398     398\n        SW01_17 SW01_18 SW_mean\nSW01_01     398     398     398\nSW01_02     397     397     397\nSW01_03     398     398     398\nSW01_04     398     398     398\nSW01_05     398     398     398\nSW01_06     398     398     398\nSW01_07     398     398     398\nSW01_08     398     398     398\nSW01_09     398     398     398\nSW01_10     398     398     398\nSW01_11     398     398     398\nSW01_12     398     398     398\nSW01_13     398     398     398\nSW01_14     398     398     398\nSW01_15     398     398     398\nSW01_16     398     398     398\nSW01_17     398     398     398\nSW01_18     398     398     398\nSW_mean     398     398     398\nProbability values (Entries above the diagonal are adjusted for multiple tests.) \n        SW01_01 SW01_02 SW01_03 SW01_04 SW01_05 SW01_06 SW01_07 SW01_08\nSW01_01       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_02       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_03       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_04       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_05       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_06       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_07       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_08       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_09       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_10       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_11       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_12       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_13       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_14       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_15       0    0.01       0    0.02    0.02       0    0.00       0\nSW01_16       0    0.00       0    0.06    0.59       0    0.06       0\nSW01_17       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_18       0    0.00       0    0.00    0.00       0    0.00       0\nSW_mean       0    0.00       0    0.00    0.00       0    0.00       0\n        SW01_09 SW01_10 SW01_11 SW01_12 SW01_13 SW01_14 SW01_15 SW01_16\nSW01_01       0       0       0       0       0       0    0.00    0.01\nSW01_02       0       0       0       0       0       0    0.07    0.01\nSW01_03       0       0       0       0       0       0    0.00    0.00\nSW01_04       0       0       0       0       0       0    0.11    0.18\nSW01_05       0       0       0       0       0       0    0.11    0.59\nSW01_06       0       0       0       0       0       0    0.00    0.01\nSW01_07       0       0       0       0       0       0    0.02    0.18\nSW01_08       0       0       0       0       0       0    0.03    0.02\nSW01_09       0       0       0       0       0       0    0.00    0.00\nSW01_10       0       0       0       0       0       0    0.00    0.00\nSW01_11       0       0       0       0       0       0    0.00    0.00\nSW01_12       0       0       0       0       0       0    0.00    0.00\nSW01_13       0       0       0       0       0       0    0.00    0.00\nSW01_14       0       0       0       0       0       0    0.00    0.00\nSW01_15       0       0       0       0       0       0    0.00    0.00\nSW01_16       0       0       0       0       0       0    0.00    0.00\nSW01_17       0       0       0       0       0       0    0.00    0.00\nSW01_18       0       0       0       0       0       0    0.00    0.00\nSW_mean       0       0       0       0       0       0    0.00    0.00\n        SW01_17 SW01_18 SW_mean\nSW01_01    0.01    0.00       0\nSW01_02    0.00    0.00       0\nSW01_03    0.00    0.00       0\nSW01_04    0.00    0.00       0\nSW01_05    0.02    0.01       0\nSW01_06    0.00    0.00       0\nSW01_07    0.00    0.00       0\nSW01_08    0.01    0.00       0\nSW01_09    0.00    0.00       0\nSW01_10    0.00    0.00       0\nSW01_11    0.00    0.00       0\nSW01_12    0.00    0.00       0\nSW01_13    0.00    0.00       0\nSW01_14    0.00    0.00       0\nSW01_15    0.00    0.00       0\nSW01_16    0.00    0.00       0\nSW01_17    0.00    0.00       0\nSW01_18    0.00    0.00       0\nSW_mean    0.00    0.00       0\n\n To see confidence intervals of the correlations, print with the short=FALSE option\n\n\n\n# Model 1: Bifactor Model\n# Model specification\nswan_model_2 &lt;- \"\nSW_GF =~ NA*SW01_01 + SW01_02 + SW01_03 + SW01_04 + SW01_05 + SW01_06 + SW01_07 +\n    SW01_08 + SW01_09 + SW01_10 + SW01_11 + SW01_12 + SW01_13 + SW01_14 + SW01_15 +\n    SW01_16 + SW01_17 + SW01_18;\nSW_AD =~ NA*SW01_01 + SW01_02 + SW01_03 + SW01_04 + SW01_05 + SW01_06 + SW01_07 +\n    SW01_08 + SW01_09; SW_HI =~ NA*SW01_10 + SW01_11 + SW01_12 + SW01_13 + SW01_14 +\n    SW01_15 + SW01_16 + SW01_17 + SW01_18;\nSW_GF ~~ 1*SW_GF; SW_AD ~~ 1*SW_AD; SW_HI ~~ 1*SW_HI; SW_GF ~~ 0*SW_AD;\nSW_AD ~~ 0*SW_HI; SW_HI ~~ 0*SW_GF\n\"\n\n\n# Model calculation\nswan_m2_cfa &lt;- cfa(swan_model_2,\n    data = data,\n    std.lv = TRUE,\n    missing = \"fiml\",\n    estimator = \"MLR\"\n)\n# Summary\nsummary(swan_m2_cfa, standardized = TRUE, fit = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 66 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        72\n\n  Number of observations                           398\n  Number of missing patterns                         2\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                               352.858     289.028\n  Degrees of freedom                               117         117\n  P-value (Chi-square)                           0.000       0.000\n  Scaling correction factor                                  1.221\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              2928.538    2257.491\n  Degrees of freedom                               153         153\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.297\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.915       0.918\n  Tucker-Lewis Index (TLI)                       0.889       0.893\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.925\n  Robust Tucker-Lewis Index (TLI)                            0.902\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -11152.042  -11152.042\n  Scaling correction factor                                  1.231\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -10975.613  -10975.613\n  Scaling correction factor                                  1.225\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               22448.083   22448.083\n  Bayesian (BIC)                             22735.108   22735.108\n  Sample-size adjusted Bayesian (SABIC)      22506.649   22506.649\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.071       0.061\n  90 Percent confidence interval - lower         0.063       0.053\n  90 Percent confidence interval - upper         0.080       0.069\n  P-value H_0: RMSEA &lt;= 0.050                    0.000       0.014\n  P-value H_0: RMSEA &gt;= 0.080                    0.045       0.000\n                                                                  \n  Robust RMSEA                                               0.066\n  90 Percent confidence interval - lower                     0.056\n  90 Percent confidence interval - upper                     0.076\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.005\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.012\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.042       0.042\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  SW_GF =~                                                              \n    SW01_01           0.511    0.089    5.754    0.000    0.511    0.393\n    SW01_02           0.743    0.121    6.161    0.000    0.743    0.563\n    SW01_03           0.676    0.076    8.842    0.000    0.676    0.543\n    SW01_04           0.747    0.107    6.989    0.000    0.747    0.513\n    SW01_05           0.496    0.095    5.207    0.000    0.496    0.331\n    SW01_06           0.602    0.106    5.673    0.000    0.602    0.427\n    SW01_07           0.587    0.083    7.087    0.000    0.587    0.452\n    SW01_08           0.485    0.102    4.761    0.000    0.485    0.367\n    SW01_09           0.749    0.089    8.409    0.000    0.749    0.507\n    SW01_10           1.033    0.075   13.834    0.000    1.033    0.698\n    SW01_11           0.931    0.095    9.751    0.000    0.931    0.712\n    SW01_12           0.991    0.079   12.618    0.000    0.991    0.793\n    SW01_13           0.900    0.103    8.718    0.000    0.900    0.650\n    SW01_14           0.913    0.082   11.154    0.000    0.913    0.595\n    SW01_15           0.518    0.130    3.975    0.000    0.518    0.361\n    SW01_16           0.587    0.160    3.667    0.000    0.587    0.418\n    SW01_17           0.636    0.084    7.588    0.000    0.636    0.508\n    SW01_18           0.780    0.099    7.865    0.000    0.780    0.554\n  SW_AD =~                                                              \n    SW01_01           0.451    0.106    4.248    0.000    0.451    0.346\n    SW01_02           0.569    0.166    3.420    0.001    0.569    0.431\n    SW01_03           0.346    0.109    3.166    0.002    0.346    0.278\n    SW01_04           0.749    0.098    7.639    0.000    0.749    0.514\n    SW01_05           0.945    0.107    8.810    0.000    0.945    0.630\n    SW01_06           0.487    0.139    3.511    0.000    0.487    0.345\n    SW01_07           0.806    0.083    9.754    0.000    0.806    0.621\n    SW01_08           0.319    0.110    2.902    0.004    0.319    0.241\n    SW01_09           0.470    0.096    4.914    0.000    0.470    0.318\n  SW_HI =~                                                              \n    SW01_10           0.078    0.161    0.482    0.630    0.078    0.053\n    SW01_11           0.217    0.219    0.993    0.321    0.217    0.166\n    SW01_12           0.127    0.192    0.663    0.507    0.127    0.102\n    SW01_13           0.426    0.188    2.261    0.024    0.426    0.308\n    SW01_14           0.171    0.157    1.087    0.277    0.171    0.112\n    SW01_15           0.731    0.120    6.108    0.000    0.731    0.510\n    SW01_16           0.940    0.127    7.401    0.000    0.940    0.669\n    SW01_17           0.432    0.111    3.878    0.000    0.432    0.345\n    SW01_18           0.589    0.126    4.670    0.000    0.589    0.419\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  SW_GF ~~                                                              \n    SW_AD             0.000                               0.000    0.000\n  SW_AD ~~                                                              \n    SW_HI             0.000                               0.000    0.000\n  SW_GF ~~                                                              \n    SW_HI             0.000                               0.000    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .SW01_01           3.889    0.065   59.577    0.000    3.889    2.986\n   .SW01_02           3.529    0.066   53.265    0.000    3.529    2.674\n   .SW01_03           4.354    0.062   69.765    0.000    4.354    3.497\n   .SW01_04           3.920    0.073   53.716    0.000    3.920    2.693\n   .SW01_05           4.088    0.075   54.417    0.000    4.088    2.728\n   .SW01_06           3.942    0.071   55.693    0.000    3.942    2.792\n   .SW01_07           4.093    0.065   62.954    0.000    4.093    3.156\n   .SW01_08           2.643    0.066   39.895    0.000    2.643    2.000\n   .SW01_09           3.578    0.074   48.304    0.000    3.578    2.421\n   .SW01_10           3.457    0.074   46.630    0.000    3.457    2.337\n   .SW01_11           4.372    0.066   66.679    0.000    4.372    3.342\n   .SW01_12           3.741    0.063   59.687    0.000    3.741    2.992\n   .SW01_13           3.982    0.069   57.385    0.000    3.982    2.876\n   .SW01_14           3.440    0.077   44.746    0.000    3.440    2.243\n   .SW01_15           3.814    0.072   53.068    0.000    3.814    2.660\n   .SW01_16           3.897    0.070   55.295    0.000    3.897    2.772\n   .SW01_17           3.515    0.063   56.031    0.000    3.515    2.809\n   .SW01_18           3.606    0.071   51.085    0.000    3.606    2.561\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    SW_GF             1.000                               1.000    1.000\n    SW_AD             1.000                               1.000    1.000\n    SW_HI             1.000                               1.000    1.000\n   .SW01_01           1.232    0.103   11.957    0.000    1.232    0.726\n   .SW01_02           0.866    0.093    9.326    0.000    0.866    0.497\n   .SW01_03           0.974    0.082   11.901    0.000    0.974    0.628\n   .SW01_04           1.000    0.108    9.273    0.000    1.000    0.472\n   .SW01_05           1.107    0.187    5.905    0.000    1.107    0.493\n   .SW01_06           1.394    0.107   13.018    0.000    1.394    0.699\n   .SW01_07           0.689    0.110    6.242    0.000    0.689    0.409\n   .SW01_08           1.410    0.117   12.010    0.000    1.410    0.807\n   .SW01_09           1.401    0.123   11.414    0.000    1.401    0.642\n   .SW01_10           1.115    0.110   10.161    0.000    1.115    0.510\n   .SW01_11           0.798    0.087    9.133    0.000    0.798    0.466\n   .SW01_12           0.565    0.083    6.808    0.000    0.565    0.361\n   .SW01_13           0.925    0.086   10.779    0.000    0.925    0.482\n   .SW01_14           1.490    0.130   11.447    0.000    1.490    0.633\n   .SW01_15           1.254    0.153    8.218    0.000    1.254    0.610\n   .SW01_16           0.748    0.144    5.194    0.000    0.748    0.378\n   .SW01_17           0.976    0.097   10.010    0.000    0.976    0.623\n   .SW01_18           1.026    0.113    9.121    0.000    1.026    0.518\n\n\n\n\nmi &lt;- modindices(swan_m2_cfa, minimum.value = 10, sort = TRUE)\nprint(mi)\n\n        lhs op     rhs   mi    epc sepc.lv sepc.all sepc.nox\n120 SW01_02 ~~ SW01_06 28.3  0.326   0.326    0.297    0.297\n163 SW01_05 ~~ SW01_07 26.2  0.401   0.401    0.460    0.460\n226 SW01_11 ~~ SW01_13 24.3  0.252   0.252    0.293    0.293\n231 SW01_11 ~~ SW01_18 17.9 -0.227  -0.227   -0.251   -0.251\n113 SW01_01 ~~ SW01_15 17.9  0.288   0.288    0.232    0.232\n100 SW01_01 ~~ SW01_02 17.8  0.242   0.242    0.235    0.235\n119 SW01_02 ~~ SW01_05 15.5 -0.258  -0.258   -0.263   -0.263\n121 SW01_02 ~~ SW01_07 13.8 -0.198  -0.198   -0.256   -0.256\n126 SW01_02 ~~ SW01_12 12.6 -0.159  -0.159   -0.228   -0.228\n88    SW_AD =~ SW01_16 12.0 -0.261  -0.261   -0.185   -0.185\n162 SW01_05 ~~ SW01_06 11.8 -0.265  -0.265   -0.213   -0.213\n94    SW_HI =~ SW01_04 10.6 -0.235  -0.235   -0.162   -0.162\n105 SW01_01 ~~ SW01_07 10.3 -0.187  -0.187   -0.203   -0.203\n203 SW01_08 ~~ SW01_14 10.2  0.246   0.246    0.169    0.169\n\n\n\n# Model 2: Bifactor Model with Modification Items 5&7 and 2&6\n# Model specification\nswan_model_3 &lt;- \"\n    SW_GF =~ NA*SW01_01 + SW01_02 + SW01_03 + SW01_04 + SW01_05 + SW01_06 + \n        SW01_07 + SW01_08 + SW01_09 + SW01_10 + SW01_11 + SW01_12 + SW01_13 + \n        SW01_14 + SW01_15 + SW01_16 + SW01_17 + SW01_18; \n    SW_AD =~ NA*SW01_01 + SW01_02 + SW01_03 + SW01_04 + SW01_05 + SW01_06 + \n        SW01_07 + SW01_08 + SW01_09; \n    SW_HI =~ NA*SW01_10 + SW01_11 + SW01_12 + SW01_13 + SW01_14 + SW01_15 + \n        SW01_16 + SW01_17 + SW01_18; SW_GF ~~ 1*SW_GF; \n    SW_AD ~~ 1*SW_AD; \n    SW_HI ~~ 1*SW_HI; \n    SW_GF ~~ 0*SW_AD; \n    SW_AD ~~ 0*SW_HI; \n    SW_HI ~~ 0*SW_GF; \n    SW01_05 ~~ SW01_07; \n    SW01_02 ~~ SW01_06\n\"\n\n\n# Model calculation\nswan_m3_cfa &lt;- cfa(swan_model_3,\n    data = data,\n    std.lv = TRUE,\n    missing = \"fiml\",\n    estimator = \"MLR\"\n)\n\n# Summary\ns &lt;- summary(swan_m3_cfa, standardized = TRUE, fit = TRUE) # standardised factor loading is std.all\ns |&gt; print()\n\nlavaan 0.6-19 ended normally after 63 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        74\n\n  Number of observations                           398\n  Number of missing patterns                         2\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                               308.602     248.615\n  Degrees of freedom                               115         115\n  P-value (Chi-square)                           0.000       0.000\n  Scaling correction factor                                  1.241\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              2928.538    2257.491\n  Degrees of freedom                               153         153\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.297\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.930       0.937\n  Tucker-Lewis Index (TLI)                       0.907       0.916\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.941\n  Robust Tucker-Lewis Index (TLI)                            0.921\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -11129.914  -11129.914\n  Scaling correction factor                                  1.199\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -10975.613  -10975.613\n  Scaling correction factor                                  1.225\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               22407.828   22407.828\n  Bayesian (BIC)                             22702.825   22702.825\n  Sample-size adjusted Bayesian (SABIC)      22468.020   22468.020\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.065       0.054\n  90 Percent confidence interval - lower         0.056       0.046\n  90 Percent confidence interval - upper         0.074       0.062\n  P-value H_0: RMSEA &lt;= 0.050                    0.003       0.204\n  P-value H_0: RMSEA &gt;= 0.080                    0.002       0.000\n                                                                  \n  Robust RMSEA                                               0.059\n  90 Percent confidence interval - lower                     0.049\n  90 Percent confidence interval - upper                     0.070\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.069\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.040       0.040\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  SW_GF =~                                                              \n    SW01_01           0.490    0.081    6.054    0.000    0.490    0.376\n    SW01_02           0.700    0.085    8.216    0.000    0.700    0.531\n    SW01_03           0.661    0.071    9.378    0.000    0.661    0.531\n    SW01_04           0.728    0.094    7.717    0.000    0.728    0.500\n    SW01_05           0.505    0.096    5.254    0.000    0.505    0.337\n    SW01_06           0.572    0.085    6.726    0.000    0.572    0.405\n    SW01_07           0.595    0.082    7.255    0.000    0.595    0.459\n    SW01_08           0.466    0.091    5.122    0.000    0.466    0.353\n    SW01_09           0.741    0.085    8.714    0.000    0.741    0.501\n    SW01_10           1.026    0.075   13.696    0.000    1.026    0.694\n    SW01_11           0.947    0.075   12.631    0.000    0.947    0.724\n    SW01_12           1.009    0.061   16.426    0.000    1.009    0.807\n    SW01_13           0.922    0.079   11.671    0.000    0.922    0.666\n    SW01_14           0.919    0.079   11.582    0.000    0.919    0.599\n    SW01_15           0.542    0.111    4.860    0.000    0.542    0.378\n    SW01_16           0.614    0.138    4.449    0.000    0.614    0.437\n    SW01_17           0.650    0.072    9.054    0.000    0.650    0.519\n    SW01_18           0.794    0.089    8.875    0.000    0.794    0.564\n  SW_AD =~                                                              \n    SW01_01           0.498    0.093    5.342    0.000    0.498    0.383\n    SW01_02           0.656    0.108    6.056    0.000    0.656    0.497\n    SW01_03           0.399    0.091    4.398    0.000    0.399    0.321\n    SW01_04           0.778    0.095    8.199    0.000    0.778    0.535\n    SW01_05           0.803    0.112    7.197    0.000    0.803    0.536\n    SW01_06           0.495    0.098    5.071    0.000    0.495    0.351\n    SW01_07           0.691    0.096    7.156    0.000    0.691    0.532\n    SW01_08           0.368    0.092    4.002    0.000    0.368    0.278\n    SW01_09           0.492    0.091    5.391    0.000    0.492    0.333\n  SW_HI =~                                                              \n    SW01_10           0.055    0.131    0.421    0.674    0.055    0.037\n    SW01_11           0.174    0.167    1.041    0.298    0.174    0.133\n    SW01_12           0.078    0.134    0.582    0.560    0.078    0.063\n    SW01_13           0.385    0.141    2.736    0.006    0.385    0.278\n    SW01_14           0.141    0.127    1.110    0.267    0.141    0.092\n    SW01_15           0.712    0.120    5.959    0.000    0.712    0.497\n    SW01_16           0.929    0.140    6.624    0.000    0.929    0.661\n    SW01_17           0.412    0.098    4.193    0.000    0.412    0.329\n    SW01_18           0.569    0.119    4.768    0.000    0.569    0.404\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  SW_GF ~~                                                              \n    SW_AD             0.000                               0.000    0.000\n  SW_AD ~~                                                              \n    SW_HI             0.000                               0.000    0.000\n  SW_GF ~~                                                              \n    SW_HI             0.000                               0.000    0.000\n .SW01_05 ~~                                                            \n   .SW01_07           0.323    0.102    3.158    0.002    0.323    0.301\n .SW01_02 ~~                                                            \n   .SW01_06           0.263    0.082    3.198    0.001    0.263    0.243\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .SW01_01           3.889    0.065   59.577    0.000    3.889    2.986\n   .SW01_02           3.529    0.066   53.343    0.000    3.529    2.676\n   .SW01_03           4.354    0.062   69.765    0.000    4.354    3.497\n   .SW01_04           3.920    0.073   53.716    0.000    3.920    2.693\n   .SW01_05           4.088    0.075   54.417    0.000    4.088    2.728\n   .SW01_06           3.942    0.071   55.693    0.000    3.942    2.792\n   .SW01_07           4.093    0.065   62.954    0.000    4.093    3.156\n   .SW01_08           2.643    0.066   39.895    0.000    2.643    2.000\n   .SW01_09           3.578    0.074   48.304    0.000    3.578    2.421\n   .SW01_10           3.457    0.074   46.630    0.000    3.457    2.337\n   .SW01_11           4.372    0.066   66.679    0.000    4.372    3.342\n   .SW01_12           3.741    0.063   59.687    0.000    3.741    2.992\n   .SW01_13           3.982    0.069   57.385    0.000    3.982    2.876\n   .SW01_14           3.440    0.077   44.746    0.000    3.440    2.243\n   .SW01_15           3.814    0.072   53.068    0.000    3.814    2.660\n   .SW01_16           3.897    0.070   55.295    0.000    3.897    2.772\n   .SW01_17           3.515    0.063   56.031    0.000    3.515    2.809\n   .SW01_18           3.606    0.071   51.085    0.000    3.606    2.561\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    SW_GF             1.000                               1.000    1.000\n    SW_AD             1.000                               1.000    1.000\n    SW_HI             1.000                               1.000    1.000\n   .SW01_01           1.208    0.104   11.630    0.000    1.208    0.712\n   .SW01_02           0.820    0.104    7.848    0.000    0.820    0.471\n   .SW01_03           0.954    0.081   11.782    0.000    0.954    0.615\n   .SW01_04           0.983    0.121    8.150    0.000    0.983    0.464\n   .SW01_05           1.346    0.156    8.646    0.000    1.346    0.599\n   .SW01_06           1.422    0.107   13.257    0.000    1.422    0.713\n   .SW01_07           0.851    0.106    8.032    0.000    0.851    0.506\n   .SW01_08           1.395    0.117   11.930    0.000    1.395    0.798\n   .SW01_09           1.393    0.123   11.303    0.000    1.393    0.638\n   .SW01_10           1.132    0.112   10.084    0.000    1.132    0.518\n   .SW01_11           0.784    0.084    9.295    0.000    0.784    0.458\n   .SW01_12           0.540    0.069    7.828    0.000    0.540    0.345\n   .SW01_13           0.918    0.086   10.667    0.000    0.918    0.479\n   .SW01_14           1.487    0.130   11.479    0.000    1.487    0.632\n   .SW01_15           1.255    0.156    8.049    0.000    1.255    0.611\n   .SW01_16           0.736    0.162    4.547    0.000    0.736    0.373\n   .SW01_17           0.975    0.098    9.989    0.000    0.975    0.622\n   .SW01_18           1.028    0.117    8.795    0.000    1.028    0.519\n\n\n\n\n# Figure Structural Model (Model 3)\nm &lt;- matrix(nrow = 18, ncol = 3)\nm[, 1] &lt;- c(rep(0, 4), \"SW_A\", rep(0, 8), \"SW_H\", rep(0, 4))\nm[, 2] &lt;- c(SWAN_vars)\nm[, 3] &lt;- c(rep(0, 9), \"SW_G\", rep(0, 8))\n\nstr_model &lt;- semPaths(swan_m3_cfa,\n    layout = m,\n    intercepts = FALSE,\n    what = \"std\",\n    style = \"lisrel\",\n    edge.color = \"grey10\",\n    fade = FALSE,\n    edge.label.cex = 0.6,\n    sizeMan = 6,\n    sizeInt = 5,\n    sizeLat = 8,\n    sizeMan2 = 3,\n    esize = 1,\n    residuals = FALSE,\n    curvePivot = FALSE\n)\n\n\n\n\n\n\n\n\n\n\n\n\nBlume, F., Buhr, L., Kuehnhausen, J., Köpke, R., Weber, L. A., Fallgatter, A. J., Ethofer, T., & Gawrilow, C. (2020). Validation of the Self-Report Version of the German Strengths and Weaknesses of ADHD Symptoms and Normal Behavior Scale (SWAN-DE-SB). Assessment, 10731911241236699.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html",
    "href": "chapters/sem/01_sem_intro.html",
    "title": "48  Introduzione ai Modelli SEM",
    "section": "",
    "text": "48.1 Introduzione\nLa ricerca in psicologia si basa sull’indagine di costrutti teorici che, essendo non direttamente osservabili, possono essere studiati solo in modo indiretto attraverso le risposte dei partecipanti a indicatori osservabili (ad esempio, le risposte a item di un questionario). I capitoli precedenti sull’analisi fattoriale hanno illustrato come valutare la struttura fattoriale di questi costrutti latenti, identificando quali item siano buoni indicatori per misurare tali costrutti. Questo processo è fondamentale per sviluppare strumenti validi e affidabili volti a quantificare i costrutti latenti che caratterizzano la ricerca psicologica. Tuttavia, una buona misurazione non rappresenta quasi mai il fine ultimo: i ricercatori desiderano solitamente esplorare le relazioni tra costrutti, confrontare differenze medie o rispondere a domande del tipo: “La self-compassion è un fattore protettivo contro il burnout?”.\nQuando i ricercatori dispongono di sole variabili osservate come predittori e outcome, e desiderano esaminare gli effetti di uno o più predittori su un singolo outcome, possono utilizzare metodi di analisi familiari, come la regressione multipla. Tuttavia, se le domande di ricerca implicano costrutti latenti o richiedono di testare sistemi complessi di relazioni tra variabili, è necessario ricorrere a tecniche di analisi più flessibili in grado di modellare simultaneamente relazioni tra variabili osservate e latenti: questo è il campo dei Modelli di Equazioni Strutturali (SEM, Structural Equation Modeling).\nAlla base dei SEM si trova una combinazione di analisi fattoriale e analisi dei percorsi (path analysis). L’analisi dei percorsi può essere vista come un’estensione della regressione multipla, poiché consente di stimare e testare effetti diretti tra variabili. Tuttavia, a differenza della regressione multipla, che si concentra sugli effetti diretti di uno o più predittori su un unico outcome, l’analisi dei percorsi consente di analizzare sia effetti diretti sia indiretti tra interi insiemi di variabili predittive e di outcome in modo simultaneo. Questo approccio permette a una variabile di fungere contemporaneamente da predittore e da outcome: una variabile può essere prevista da una o più altre variabili, mentre a sua volta funge da predittore per altre variabili. In altre parole, l’analisi dei percorsi offre la possibilità di costruire modelli complessi, purché tutte le variabili siano osservate.\nL’inclusione di variabili latenti richiede però di andare oltre l’analisi dei percorsi tradizionale. Grazie al lavoro pionieristico di Jöreskog e Van Thillo, è stato possibile integrare variabili latenti nei modelli di percorsi, dando vita a quella che oggi conosciamo come SEM. Questo approccio è stato reso progressivamente più accessibile da software sempre più intuitivi, contribuendo alla diffusione dei SEM nella psicologia e nelle scienze sociali.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#introduzione",
    "href": "chapters/sem/01_sem_intro.html#introduzione",
    "title": "48  Introduzione ai Modelli SEM",
    "section": "",
    "text": "48.1.1 Struttura e Obiettivi dei SEM\nUn modello SEM tipico si compone di due parti principali:\n\n\nLa parte di misurazione, che collega i costrutti latenti a un insieme di variabili osservate o indicatori.\n\n\nLa parte strutturale, che modella le relazioni ipotizzate tra i costrutti latenti.\n\nL’obiettivo principale dei SEM è testare ipotesi teoriche specifiche tramite modelli che rappresentano le previsioni di tali ipotesi, utilizzando costrutti misurati attraverso variabili osservabili appropriate. I SEM fungono così da ponte tra teoria e osservazione, consentendo di tradurre concetti astratti in entità misurabili e di analizzarne le relazioni in modo sistematico e coerente con la teoria.\n\n48.1.2 Considerazioni Critiche sull’Uso dei SEM\nNonostante la loro potenza, i SEM richiedono un uso critico e consapevole. Ogni modello statistico è una semplificazione della realtà, come sottolineato dal celebre aforisma: “Tutti i modelli sono sbagliati, ma alcuni sono utili”. Questo implica che un buon adattamento ai dati non garantisce una rappresentazione accurata della realtà. Modelli intrinsecamente imprecisi possono adattarsi bene ai dati, portando a conclusioni errate. Pertanto, la scelta dei modelli non deve essere un semplice esercizio statistico, ma un processo orientato allo sviluppo e al raffinamento di teorie valide. La revisione critica dei modelli, basata su evidenze empiriche e solidi principi teorici, è essenziale per un progresso scientifico affidabile.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#modelli-di-regressione-e-introduzione-ai-sem",
    "href": "chapters/sem/01_sem_intro.html#modelli-di-regressione-e-introduzione-ai-sem",
    "title": "48  Introduzione ai Modelli SEM",
    "section": "\n48.2 Modelli di Regressione e Introduzione ai SEM",
    "text": "48.2 Modelli di Regressione e Introduzione ai SEM\nIn questo capitolo, i Modelli di Equazioni Strutturali (SEM) vengono introdotti partendo dal caso più semplice: il modello di regressione multipla, reinterpretato e rappresentato all’interno di un framework SEM. L’obiettivo è fornire un ripasso rigoroso del modello di regressione lineare, evidenziando come possa essere formalizzato e implementato come un caso particolare di un modello SEM. Questa rappresentazione permette di estendere il concetto di regressione multipla a modelli più complessi che includono variabili latenti e relazioni strutturali.\nUtilizzeremo dati empirici per illustrare l’approccio, focalizzandoci sulla Self-Sompassion Scale e sulle tre sottoscale del DASS-21: ansia, stress e depressione. Il campione analizzato comprende 526 studenti universitari iscritti a corsi di psicologia.\nLe sottoscale del DASS-21 rappresentano variabili osservate che misurano concetti teorici distinti ma correlati. L’obiettivo è esplorare come il punteggio totale della Self-Compassion possa essere predetto dalle tre sottoscale del DASS-21 utilizzando un approccio di regressione multipla. Successivamente, questo modello sarà riformulato e stimato come un caso specifico di SEM.\nL’implementazione in R mediante il pacchetto lavaan consentirà di confrontare i risultati della regressione tradizionale con quelli ottenuti dalla rappresentazione SEM, illustrando i vantaggi di quest’ultimo approccio, come la maggiore flessibilità e la possibilità di incorporare errori di misura nelle variabili osservate.\n\n48.2.1 Preliminari\nImportiamo i dati:\n\ndat &lt;- read.csv(\n    here::here(\"data\", \"dass_rosenberg_scs.csv\"),\n    header = TRUE\n)\ndat |&gt;\n    head()\n#&gt;   stress anxiety depression rosenberg self_kindness common_humanity\n#&gt; 1      7       6          4        31            17              16\n#&gt; 2      3       2          1        32            14              14\n#&gt; 3      1       0          1        31            20              16\n#&gt; 4     12      11         13        34            12               6\n#&gt; 5     10       6         12        25            16              17\n#&gt; 6      5       1          2        31            14              14\n#&gt;   mindfulness self_judgment isolation over_identification scs_ts\n#&gt; 1          16            11         8                  10     98\n#&gt; 2          16            16        11                  13     82\n#&gt; 3          16            13         6                   9    102\n#&gt; 4           6            10         7                  15     70\n#&gt; 5          13            17        16                  18     73\n#&gt; 6          10            12         8                  11     85\n\n\ndim(dat)\n#&gt; [1] 526  11\n\nSelezioniamo le variabili di interesse:\n\nd_mr &lt;- dat |&gt;\n    dplyr::select(stress, anxiety, depression, scs_ts)\n\nEsaminiamo i diagrammi di dispersione tra le varie misure per verificare che la relazione tra le variabili sia lineare.\n\npairs(d_mr)\n\n\n\n\n\n\n\nConvertiamo i dati in formato matriciale:\n\ny &lt;- d_mr$scs_ts |&gt; as.matrix()\ndim(y)\n#&gt; [1] 526   1",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#modello-di-regressione-lineare-multipla",
    "href": "chapters/sem/01_sem_intro.html#modello-di-regressione-lineare-multipla",
    "title": "48  Introduzione ai Modelli SEM",
    "section": "\n48.3 Modello di Regressione Lineare Multipla",
    "text": "48.3 Modello di Regressione Lineare Multipla\nIl modello generale di regressione lineare multipla (MLR) può essere espresso attraverso la seguente equazione:\n\\[\ny_i = \\beta_0 + \\beta_1 x_{1i} + \\cdots + \\beta_p x_{pi} + \\epsilon_i,\n\\]\ndove:\n\n\n\\(i = 1, \\ldots, N\\) identifica l’\\(i\\)-esima osservazione,\n\n\n\\(\\beta_0\\) è l’intercetta del modello,\n\n\n\\(\\beta_1, \\ldots, \\beta_p\\) sono i coefficienti di regressione associati alle variabili indipendenti,\n\n\n\\(\\epsilon_i\\) è il termine di errore per l’\\(i\\)-esima osservazione,\n\nSi assume che \\(\\epsilon_i\\) sia indipendente dalle variabili esplicative \\(x_{1i}, \\ldots, x_{pi}\\) e distribuito con media zero e varianza costante \\(\\sigma^2\\).\n\nIn questa formulazione, \\(y_i\\) rappresenta il valore della variabile dipendente per l’\\(i\\)-esima osservazione, mentre i coefficienti \\(\\beta\\) quantificano l’effetto delle variabili indipendenti \\(x_{1i}, \\ldots, x_{pi}\\) su \\(y_i\\). Il termine di errore \\(\\epsilon_i\\) cattura la varianza non spiegata dal modello lineare. Questa struttura consente di modellare relazioni lineari tra una variabile dipendente e più variabili indipendenti, fornendo una base per effettuare inferenze sui parametri \\(\\beta\\).\n\n48.3.1 Forma Matriciale del Modello\nIl modello MLR può essere rappresentato in forma matriciale come:\n\\[\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon},\n\\]\ndove:\n\n\n\\(\\mathbf{y}\\) è un vettore \\(N \\times 1\\) contenente i valori osservati della variabile dipendente,\n\n\n\\(\\mathbf{X}\\) è una matrice di progettazione \\(N \\times (p+1)\\) che include le \\(p\\) variabili indipendenti e una colonna di uni per l’intercetta,\n\n\n\\(\\boldsymbol{\\beta}\\) è un vettore \\((p+1) \\times 1\\) dei coefficienti di regressione (inclusa l’intercetta),\n\n\n\\(\\boldsymbol{\\epsilon}\\) è un vettore \\(N \\times 1\\) che rappresenta i termini di errore.\n\nLe componenti del modello sono definite come segue:\n\\[\n\\mathbf{y} =\n\\begin{pmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_N\n\\end{pmatrix}, \\quad\n\\boldsymbol{\\epsilon} =\n\\begin{pmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\vdots \\\\\n\\epsilon_N\n\\end{pmatrix}, \\quad\n\\mathbf{X} =\n\\begin{pmatrix}\n1 & x_{11} & \\cdots & x_{p1} \\\\\n1 & x_{12} & \\cdots & x_{p2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{1N} & \\cdots & x_{pN}\n\\end{pmatrix}.\n\\]\nOgni riga della matrice \\(\\mathbf{X}\\) rappresenta un’osservazione e include i valori delle variabili indipendenti per quella osservazione, oltre a un uno per l’intercetta.\n\n48.3.2 Metodo dei Minimi Quadrati\nIl metodo dei minimi quadrati (Least Squares Estimation, LSE) mira a stimare i parametri \\(\\boldsymbol{\\beta}\\) minimizzando la somma dei quadrati degli errori (SSE), definita come:\n\\[\n\\text{SSE} = \\boldsymbol{\\epsilon}'\\boldsymbol{\\epsilon} = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})'(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}).\n\\]\nEspandendo questa espressione:\n\\[\n\\text{SSE} = \\mathbf{y}'\\mathbf{y} - 2\\boldsymbol{\\beta}'\\mathbf{X}'\\mathbf{y} + \\boldsymbol{\\beta}'\\mathbf{X}'\\mathbf{X}\\boldsymbol{\\beta}.\n\\]\nMinimizzando la SSE rispetto a \\(\\boldsymbol{\\beta}\\) e ponendo la derivata prima pari a zero, si ottiene il sistema normale:\n\\[\n\\mathbf{X}'\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}'\\mathbf{y}.\n\\]\nSe la matrice \\(\\mathbf{X}'\\mathbf{X}\\) è invertibile, la soluzione per i coefficienti stimati è:\n\\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}.\n\\]\nQuesta soluzione fornisce le stime dei coefficienti di regressione che minimizzano la discrepanza tra i valori osservati \\(\\mathbf{y}\\) e quelli predetti \\(\\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\) dal modello. In tal modo, si ottengono le migliori stime lineari e non distorte dei parametri, sotto le ipotesi classiche di regressione.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#regressione-multipla-in-r",
    "href": "chapters/sem/01_sem_intro.html#regressione-multipla-in-r",
    "title": "48  Introduzione ai Modelli SEM",
    "section": "\n48.4 Regressione Multipla in R",
    "text": "48.4 Regressione Multipla in R\nApplichiamo il modello di regressione lineare multipla (MLR) ai dati disponibili, utilizzando sia la formulazione matriciale sia le funzioni predefinite di R. Come esempio, analizziamo le relazioni tra depressione, ansia, stress e una variabile dipendente (scs_ts) nei dati.\n\n48.4.1 Preparazione dei Dati\nSelezioniamo le variabili di interesse dal dataset:\n\ndass &lt;- d_mr |&gt;\n    dplyr::select(depression, anxiety, stress)\n\nCreiamo la matrice di progettazione \\(\\mathbf{X}\\), includendo una colonna di uni per l’intercetta:\n\nX &lt;- model.matrix(~ depression + anxiety + stress, data = dass)\nhead(X)\n#&gt;   (Intercept) depression anxiety stress\n#&gt; 1           1          4       6      7\n#&gt; 2           1          1       2      3\n#&gt; 3           1          1       0      1\n#&gt; 4           1         13      11     12\n#&gt; 5           1         12       6     10\n#&gt; 6           1          2       1      5\n\n\n48.4.2 Stima dei Coefficienti con la Formula Matriciale\nCalcoliamo i coefficienti \\(\\boldsymbol{\\beta}\\) utilizzando la formula dei minimi quadrati:\n\\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}.\n\\]\nIn R, il calcolo viene effettuato come segue:\n\ny &lt;- d_mr$scs_ts\nbeta &lt;- solve(t(X) %*% X) %*% t(X) %*% y\nbeta\n#&gt;               [,1]\n#&gt; (Intercept) 91.361\n#&gt; depression  -1.484\n#&gt; anxiety      1.049\n#&gt; stress      -0.973\n\n\n48.4.3 Verifica dei Risultati con lm()\n\nConfrontiamo i risultati ottenuti con la funzione lm():\n\nfm &lt;- lm(scs_ts ~ depression + anxiety + stress, data = d_mr)\ncoef(fm) \n#&gt; (Intercept)  depression     anxiety      stress \n#&gt;      91.361      -1.484       1.049      -0.973\n\nI coefficienti stimati con il metodo matriciale e quelli calcolati da lm() devono coincidere.\n\n48.4.4 Valori Predetti e Residui\nCalcoliamo i valori predetti \\(\\hat{y}\\) utilizzando i coefficienti stimati:\n\nyhat &lt;- X %*% beta\ncor(yhat, fm$fitted.values) \n#&gt;      [,1]\n#&gt; [1,]    1\n\nCalcoliamo i residui \\(e = \\mathbf{y} - \\hat{\\mathbf{y}}\\):\n\ne &lt;- d_mr$scs_ts - yhat\ncor(e, fm$residuals) \n#&gt;      [,1]\n#&gt; [1,]    1\n\n\n48.4.5 Somma dei Quadrati dei Residui\nLa somma dei quadrati dei residui (RSS) è definita come:\n\\[\n\\text{RSS} = \\mathbf{e}'\\mathbf{e}.\n\\]\nIn R:\n\nRSS &lt;- t(e) %*% e\nRSS \n#&gt;        [,1]\n#&gt; [1,] 128700\n\n\n48.4.6 Stima della Varianza dei Residui\nLa stima della varianza dei residui è data da:\n\\[\n\\hat{\\sigma}^2 = \\frac{\\text{RSS}}{N - (p+1)},\n\\]\ndove \\(N\\) è il numero di osservazioni e \\(p+1\\) è il numero di parametri stimati (inclusa l’intercetta):\n\nvar_e &lt;- RSS / (length(y) - dim(X)[2])\nvar_e \n#&gt;      [,1]\n#&gt; [1,]  247\n\n\n48.4.7 Errore Standard della Regressione\nL’errore standard della regressione, \\(\\hat{\\sigma}\\), è la radice quadrata della varianza dei residui:\n\nsqrt(var_e)\n#&gt;      [,1]\n#&gt; [1,] 15.7\n\nVerifichiamo questi risultati con il sommario del modello lm():\n\nsummary(fm)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = scs_ts ~ depression + anxiety + stress, data = d_mr)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -36.80 -12.00  -0.35  10.74  43.67 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)\n#&gt; (Intercept)   91.361      1.623   56.29  &lt; 2e-16\n#&gt; depression    -1.484      0.238   -6.25  8.7e-10\n#&gt; anxiety        1.049      0.210    4.99  8.2e-07\n#&gt; stress        -0.973      0.255   -3.81  0.00015\n#&gt; \n#&gt; Residual standard error: 15.7 on 522 degrees of freedom\n#&gt; Multiple R-squared:  0.247,  Adjusted R-squared:  0.243 \n#&gt; F-statistic: 57.1 on 3 and 522 DF,  p-value: &lt;2e-16\n\n\n48.4.8 Coefficiente di Determinazione \\(R^2\\)\n\nIl coefficiente di determinazione \\(R^2\\) misura la proporzione della varianza spiegata dal modello rispetto alla varianza totale:\n\\[\nR^2 = \\frac{\\sum (\\hat{y}_i - \\bar{y})^2}{\\sum (y_i - \\bar{y})^2}.\n\\]\nIn R, calcoliamo \\(R^2\\) come segue:\n\nR2 &lt;- (sum((yhat - mean(y))^2)) / (sum((y - mean(y))^2)) \nR2 \n#&gt; [1] 0.247\n\nIn conclusione, questo esempio dimostra come implementare un modello di regressione multipla sia utilizzando la formulazione matriciale sia ricorrendo a funzioni predefinite di R. Il confronto tra i due approcci evidenzia la coerenza dei risultati e offre un’utile comprensione del funzionamento interno dei metodi di regressione.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#modello-di-percorso",
    "href": "chapters/sem/01_sem_intro.html#modello-di-percorso",
    "title": "48  Introduzione ai Modelli SEM",
    "section": "\n48.5 Modello di Percorso",
    "text": "48.5 Modello di Percorso\nPassiamo ora al cuore di questo capitolo: la rappresentazione del modello di regressione multipla come un caso speciale di Modello di Equazioni Strutturali (SEM). In precedenza, il modello di regressione è stato descritto nei termini di un modello statistico stimato mediante il metodo della massima verosimiglianza (ML). Questo metodo, sotto l’ipotesi di normalità multivariata, coincide con il metodo dei minimi quadrati ordinari (OLS). Tuttavia, nei modelli SEM, le relazioni tra le variabili possono essere più complesse rispetto a quelle di un modello di regressione multipla. Di conseguenza, non esistono formule analitiche esplicite per stimare i coefficienti del modello.\n\n48.5.1 Stima nei Modelli SEM\nNei modelli di equazioni strutturali (SEM), la stima dei parametri non avviene attraverso l’applicazione diretta di formule analitiche (come abbiamo visto in precedenza), ma si basa su un processo di ottimizzazione numerica iterativa. È importante comprendere che, quando parliamo di massima verosimiglianza (ML) o minimi quadrati generalizzati (GLS), non ci riferiamo a soluzioni analitiche chiuse, ma a funzioni obiettivo che l’algoritmo cerca di ottimizzare.\nIl processo funziona così:\n\nSi parte da valori iniziali dei parametri (spesso basati su stime preliminari).\nL’algoritmo calcola la discrepanza tra la matrice di covarianza osservata (S) e quella predetta dal modello (Σ) con i parametri correnti.\nBasandosi su questa discrepanza, l’algoritmo aggiusta i parametri in una direzione che dovrebbe ridurre la differenza.\nSi ripetono i passi 2 e 3 finché la discrepanza non può essere ulteriormente ridotta in modo significativo.\n\nLa funzione di discrepanza (o funzione di costo) può essere basata su diversi criteri, come la verosimiglianza o la somma dei quadrati delle differenze, ma in tutti i casi l’obiettivo è trovare i valori dei parametri che la minimizzano attraverso successive approssimazioni. Non esiste una formula diretta per trovare questi valori - l’algoritmo “esplora” iterativamente lo spazio dei parametri cercando il punto di minimo della funzione di costo.\nQuesta natura iterativa del processo di stima ha importanti implicazioni pratiche:\n\nL’algoritmo potrebbe non convergere a una soluzione.\nPotrebbe convergere a un minimo locale invece che globale.\nIl tempo di calcolo aumenta con la complessità del modello.\nLa scelta dei valori iniziali può influenzare il risultato finale.\n\nLa differenza fondamentale rispetto a metodi analitici diretti (come la regressione lineare semplice) è che non esiste una formula chiusa per calcolare i parametri ottimali, ma si procede per successive approssimazioni guidate dalla riduzione di una funzione di costo.\n\n48.5.2 Rappresentazione di un Modello di Percorso con lavaan\n\nPer illustrare l’equivalenza tra un modello di regressione multipla e un SEM, riformuliamo il modello di regressione come un modello di percorso utilizzando la sintassi del pacchetto lavaan in R.\nDefiniamo il modello:\n\nmod_mr &lt;- \"\n  scs_ts ~ anxiety + depression + stress\n\"\n\nIn questo caso:\n\n\nscs_ts rappresenta la variabile dipendente (ad esempio, una misura di self-compassion),\n\n\nanxiety, depression, e stress sono le variabili predittive.\n\nAdattiamo il modello ai dati disponibili:\n\nfit_mr &lt;- lavaan::sem(mod_mr, d_mr)\n\nIl comando lavaan::sem() specifica che vogliamo stimare il modello utilizzando l’approccio SEM.\nEsaminiamo i parametri stimati:\n\nparameterEstimates(fit_mr) \n#&gt;           lhs op        rhs     est     se     z pvalue ci.lower ci.upper\n#&gt; 1      scs_ts  ~    anxiety   1.049  0.209  5.01      0    0.639    1.460\n#&gt; 2      scs_ts  ~ depression  -1.484  0.237 -6.27      0   -1.948   -1.020\n#&gt; 3      scs_ts  ~     stress  -0.973  0.254 -3.83      0   -1.472   -0.475\n#&gt; 4      scs_ts ~~     scs_ts 244.677 15.087 16.22      0  215.106  274.247\n#&gt; 5     anxiety ~~    anxiety  32.082  0.000    NA     NA   32.082   32.082\n#&gt; 6     anxiety ~~ depression  24.546  0.000    NA     NA   24.546   24.546\n#&gt; 7     anxiety ~~     stress  24.538  0.000    NA     NA   24.538   24.538\n#&gt; 8  depression ~~ depression  31.418  0.000    NA     NA   31.418   31.418\n#&gt; 9  depression ~~     stress  25.662  0.000    NA     NA   25.662   25.662\n#&gt; 10     stress ~~     stress  29.714  0.000    NA     NA   29.714   29.714\n\nI parametri stimati includono:\n\nI coefficienti di regressione che rappresentano le relazioni tra la variabile dipendente e i predittori.\n\nL’errore standard associato a ciascun parametro stimato.\n\nIl valore p (che può essere ignorato in un’analisi bayesiana o interpretato con cautela).\n\n48.5.3 Confronto con il Modello di Regressione Multipla\nI parametri stimati da lavaan risultano praticamente identici a quelli ottenuti con il metodo della massima verosimiglianza per il modello di regressione multipla. Questo è coerente con il fatto che il modello di regressione multipla è un caso particolare di SEM in cui tutte le variabili sono osservate e non vi sono variabili latenti o relazioni complesse.\nIn conclusione, rappresentare un modello di regressione multipla come un modello di percorso SEM evidenzia l’equivalenza metodologica tra i due approcci nei casi più semplici. Tuttavia, l’approccio SEM offre maggiore flessibilità, permettendo di includere variabili latenti, relazioni indirette, e modelli più complessi, che non possono essere gestiti con il semplice framework della regressione multipla. Questa flessibilità rende SEM uno strumento indispensabile per analisi avanzate in psicologia e scienze sociali.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#modelli-sem-e-scomposizione-della-covarianza",
    "href": "chapters/sem/01_sem_intro.html#modelli-sem-e-scomposizione-della-covarianza",
    "title": "48  Introduzione ai Modelli SEM",
    "section": "\n48.6 Modelli SEM e Scomposizione della Covarianza",
    "text": "48.6 Modelli SEM e Scomposizione della Covarianza\n\n48.6.1 Principio Fondamentale\nI modelli SEM mirano a spiegare le covarianze osservate nei dati attraverso una rete di relazioni causali dirette e indirette tra variabili. Questa rete è rappresentata da coefficienti di percorso che, opportunamente combinati, permettono di ricostruire la struttura di covarianza dei dati osservati.\n\n48.6.2 Esempio Pratico\nPrendiamo come esempio la relazione tra self-compassion (misurata dal punteggio totale) e ansia (misurata dal DASS-21). La covarianza osservata tra queste due variabili può essere scomposta in diversi percorsi causali:\n\n\nEffetto Diretto:\n\nIl coefficiente che rappresenta l’influenza diretta dell’ansia sulla self-compassion\n\n\n\nEffetti Indiretti:\n\nVia depressione: l’ansia è correlata con la depressione, che a sua volta influenza la self-compassion\nVia stress: l’ansia è correlata con lo stress, che a sua volta influenza la self-compassion\n\n\n\n48.6.3 Calcolo della Covarianza Predetta\nLa covarianza totale tra ansia e self-compassion viene calcolata combinando questi percorsi attraverso la seguente formula:\nCovarianza_Predetta = \n    (Effetto_Diretto × Varianza_Ansia) +\n    (Coefficiente_Depressione × Covarianza_Ansia_Depressione) +\n    (Coefficiente_Stress × Covarianza_Ansia_Stress)\nNel nostro modello specifico:\n\n# Coefficienti di percorso\nbeta_anxiety_scs_ts &lt;- 1.0493140    # Effetto diretto ansia → self-compassion\nbeta_depression_scs_ts &lt;- -1.4841573 # Effetto diretto depressione → self-compassion\nbeta_stress_scs_ts &lt;- -0.9733368    # Effetto diretto stress → self-compassion\n\n# Covarianze tra predittori\ncov_anxiety_depression &lt;- 24.5464225\ncov_anxiety_stress &lt;- 24.5381096\ncov_depression_stress &lt;- 25.6615608\n\n# Varianze dei predittori\nvar_anxiety &lt;- 32.0817418\nvar_depression &lt;- 31.4182365\nvar_stress &lt;- 29.7137880\n\n# Calcolo della covarianza predetta\npredicted_cov_anxiety_scs_ts &lt;- \n    beta_anxiety_scs_ts * var_anxiety +\n    beta_depression_scs_ts * cov_anxiety_depression +\n    beta_stress_scs_ts * cov_anxiety_stress\n\npredicted_cov_anxiety_scs_ts\n#&gt; [1] -26.7\n\n\n48.6.4 Verifica del Modello\nLa bontà del modello può essere verificata confrontando la covarianza predetta con quella osservata nei dati:\n\n# Covarianza osservata nei dati\ncov(d_mr$anxiety, d_mr$scs_ts)\n#&gt; [1] -26.7\n\nQuesto procedimento di scomposizione viene applicato a tutti gli elementi della matrice di varianza/covarianza, permettendo di:\n\nComprendere i meccanismi attraverso cui le variabili si influenzano reciprocamente\nQuantificare l’importanza relativa dei diversi percorsi causali\nValidare la struttura teorica del modello confrontando le covarianze predette con quelle osservate\n\nLa visualizzazione del modello attraverso il grafico semPaths aiuta a rappresentare questa rete di relazioni in modo intuitivo, mostrando i coefficienti di percorso stimati per ogni relazione.\n\nsemPaths(fit_mr,\n    whatLabels = \"est\",\n    sizeMan = 10,\n    edge.label.cex = 1.15,\n    style = \"mx\",\n    nCharNodes = 0, nCharEdges = 0\n)",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#errore-di-specificazione",
    "href": "chapters/sem/01_sem_intro.html#errore-di-specificazione",
    "title": "48  Introduzione ai Modelli SEM",
    "section": "\n48.7 Errore di Specificazione",
    "text": "48.7 Errore di Specificazione\nConosciuto spiritosamente come “heartbreak of L.O.V.E.” (Left-Out Variable Error; Mauro (1990)), l’errore di specificazione rappresenta una problematica fondamentale nei modelli di regressione, che deve essere sempre considerata attentamente durante l’interpretazione dei risultati.\nL’errore di specificazione si verifica quando una variabile viene esclusa dal modello di regressione, e tale variabile soddisfa entrambe le seguenti condizioni:\n\n\nÈ associata ad altre variabili incluse nel modello.\n\n\nHa un effetto diretto sulla variabile dipendente (\\(y\\)).\n\nQuando ciò accade, i coefficienti di regressione stimati per le variabili incluse nel modello risultano distorti in termini sia di intensità sia di segno. Questo fenomeno può portare a conclusioni errate sull’effetto delle variabili indipendenti sulla variabile dipendente.\n\n48.7.1 Un Esempio con Dati Simulati\nConsideriamo un esempio in cui la prestazione (performance) è positivamente associata alla motivazione (motivation) e negativamente all’ansia (anxiety). Inoltre, supponiamo che ansia e motivazione siano positivamente correlate. Vogliamo osservare come il coefficiente della variabile “motivazione” cambi se “ansia” viene esclusa dal modello.\nCreiamo i dati simulati:\n\nset.seed(123)\nn &lt;- 400\n\nanxiety &lt;- rnorm(n, 10, 1.5)\nmotivation &lt;- 4.0 * anxiety + rnorm(n, 0, 3.5)\ncor(anxiety, motivation)\n#&gt; [1] 0.862\n\nLa variabile performance è definita come una combinazione lineare di motivation e anxiety, con un effetto positivo ma piccolo della motivazione e un effetto negativo marcato dell’ansia:\n\nperformance &lt;- 0.5 * motivation - 5.0 * anxiety + rnorm(n, 0, 3)\n\nSalviamo i dati in un data frame:\n\nsim_dat2 &lt;- tibble(performance, motivation, anxiety)\nsim_dat2 |&gt; head() |&gt; print()\n#&gt; # A tibble: 6 × 3\n#&gt;   performance motivation anxiety\n#&gt;         &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1       -26.5       36.4    9.16\n#&gt; 2       -33.0       34.5    9.65\n#&gt; 3       -35.6       47.1   12.3 \n#&gt; 4       -26.9       40.3   10.1 \n#&gt; 5       -28.6       43.1   10.2 \n#&gt; 6       -40.2       44.5   12.6\n\n\n48.7.1.1 Modello corretto\nAdattiamo un modello di regressione che includa entrambi i predittori (motivation e anxiety):\n\nfm1 &lt;- lm(performance ~ motivation + anxiety, sim_dat2)\ncoef(fm1) |&gt; print()\n#&gt; (Intercept)  motivation     anxiety \n#&gt;       1.371       0.495      -5.105\n\nLe stime dei coefficienti di regressione riflettono correttamente i parametri utilizzati per generare i dati.\n\n48.7.1.2 Modello con specificazione errata\nOra escludiamo il predittore anxiety e stimiamo il modello solo con motivation:\n\nfm2 &lt;- lm(performance ~ motivation, sim_dat2)\nsummary(fm2) |&gt; print()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = performance ~ motivation, data = sim_dat2)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -13.501  -3.409   0.005   3.311  12.616 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)\n#&gt; (Intercept) -12.3972     1.4459   -8.57  2.2e-16\n#&gt; motivation   -0.4372     0.0355  -12.31  &lt; 2e-16\n#&gt; \n#&gt; Residual standard error: 4.87 on 398 degrees of freedom\n#&gt; Multiple R-squared:  0.276,  Adjusted R-squared:  0.274 \n#&gt; F-statistic:  151 on 1 and 398 DF,  p-value: &lt;2e-16\n\nIn questo caso, il segno del coefficiente di regressione per motivation è invertito rispetto al modello generatore dei dati. Questo è un tipico esempio di errore di specificazione.\n\n48.7.2 Spiegazione Matematica\nSupponiamo che il vero modello sia:\n\\[\ny = \\alpha + \\beta_1 X_1 + \\beta_2 X_2 + \\varepsilon,\n\\]\nstimato come:\n\\[\ny = a + b_1 X_1 + b_2 X_2 + e.\n\\]\nSe omettiamo erroneamente \\(X_2\\), il modello diventa:\n\\[\ny = a^\\prime + b_1^\\prime X_1 + e^\\prime,\n\\]\ndove \\(b_1^\\prime\\) è dato da:\n\\[\nb_1^\\prime = \\frac{\\text{Cov}(X_1, y)}{\\text{Var}(X_1)}.\n\\]\nSviluppando l’espressione:\n\\[\nb_1^\\prime = b_1 + b_2 \\frac{\\text{Cov}(X_1, X_2)}{\\text{Var}(X_1)}.\n\\]\nPertanto, il coefficiente \\(b_1^\\prime\\) stimato nel modello con specificazione errata è distorto dalla presenza di \\(b_2\\) e dalla correlazione \\(\\text{Cov}(X_1, X_2)\\).\n\n48.7.3 Verifica dell’Errore nei Dati Simulati\nCalcoliamo manualmente il coefficiente distorto \\(b_1^\\prime\\) utilizzando i risultati del modello completo:\n\nfm1$coef[2] + fm1$coef[3] * \n  cov(sim_dat2$motivation, sim_dat2$anxiety) / \n  var(sim_dat2$motivation)\n#&gt; motivation \n#&gt;     -0.437\n\nIl valore calcolato coincide con quello stimato dal modello performance ~ motivation, confermando che il coefficiente è distorto.\nPossiamo trarre le seguenti conclusioni:\n\n\nIl coefficiente stimato \\(b_1^\\prime\\) è distorto se vengono omessi predittori rilevanti (\\(X_2\\)) correlati a quelli inclusi (\\(X_1\\)).\n\n\nLa distorsione è sistematica e non si riduce all’aumentare della numerosità campionaria, rendendo lo stimatore inconsistente.\n\n\nLa causa dell’errore è l’attribuzione degli effetti di \\(X_2\\) al predittore incluso, \\(X_1\\).\n\nL’errore di specificazione può essere evitato solo se:\n\nIl predittore omesso (\\(X_2\\)) non ha un effetto sulla variabile dipendente (\\(\\beta_2 = 0\\)).\n\nIl predittore omesso è incorrelato con i predittori inclusi (\\(\\text{Cov}(X_1, X_2) = 0\\)).\n\nUna corretta specificazione del modello è quindi essenziale per garantire risultati affidabili e interpretazioni corrette.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#errore-di-specificazione-e-modelli-sem",
    "href": "chapters/sem/01_sem_intro.html#errore-di-specificazione-e-modelli-sem",
    "title": "48  Introduzione ai Modelli SEM",
    "section": "\n48.8 Errore di Specificazione e Modelli SEM",
    "text": "48.8 Errore di Specificazione e Modelli SEM\nQui abbiamo descritto l’errore di specificazione in riferimento al modello di regressione. Tuttavia, dato che i Modelli di Equazioni Strutturali (SEM) possono essere considerati un’estensione del modello di regressione, le stesse considerazioni si applicano anche ai SEM.\nAnche se i SEM permettono di rappresentare e testare relazioni più complesse tra variabili osservate e latenti, le loro conclusioni rimangono condizionate a un’importante assunzione: che il modello sia specificato correttamente. In altre parole, le inferenze tratte da un SEM presuppongono che non vi siano variabili rilevanti omesse o erroneamente incluse nel modello. Tuttavia, questa ipotesi è spesso irrealistica. Dato che non possiamo sapere con certezza se esistano altre variabili correlate con quelle del modello e che, in generale, tale eventualità è altamente probabile, è inevitabile che le relazioni descritte da un SEM siano sempre influenzate, almeno in parte, dall’errore di specificazione.\n\n48.8.1 La Natura Condizionale delle Conclusioni nei SEM\nL’interpretazione dei risultati di un SEM è sempre condizionale all’assunzione che il modello rappresenti correttamente le relazioni tra le variabili. Questa condizionalità implica che:\n\nLe stime dei parametri e le inferenze derivate sono valide solo all’interno del contesto teorico e delle specifiche del modello scelto.\n\nLa validità delle conclusioni è compromessa se il modello è distorto da errori di specificazione, come l’omissione di variabili rilevanti o la presenza di relazioni spurie tra le variabili incluse.\n\n48.8.2 La Portata dell’Errore di Specificazione nei SEM\nL’errore di specificazione è intrinseco a tutti i modelli statistici, SEM inclusi. Questo deriva dal fatto che non possiamo conoscere tutte le variabili rilevanti né possiamo verificare in modo definitivo se il modello include tutte le relazioni pertinenti. Ne consegue che:\n\n\nL’errore di specificazione è inevitabile, ma non sempre rilevante. La sua gravità dipende da quanto le variabili omesse o erroneamente incluse distorcono le stime dei parametri del modello.\n\n\nNon è possibile quantificare con precisione la portata dell’errore di specificazione, poiché non possiamo osservare direttamente le variabili omesse.\n\n48.8.3 Utilità del Modello e il Dilemma della Verità\nTornando al celebre aforisma di George Box: “Tutti i modelli sono falsi, ma alcuni sono utili”, l’obiettivo dell’analisi con i SEM non è stabilire se il modello sia “vero”. Questa verifica è impossibile, dato che non possiamo mai essere certi di non aver omesso variabili rilevanti o di aver incluso solo relazioni valide. Il problema non è evitare l’errore di specificazione, bensì valutarne l’impatto e considerare l’utilità del modello nonostante la sua inevitabile falsità.\n\n48.8.4 Cosa Significa “Utilità” in un SEM?\nUn modello è utile se:\n\nConsente di rispondere a specifiche domande di ricerca, anche in presenza di alcune semplificazioni.\n\nFornisce una rappresentazione ragionevolmente coerente delle relazioni teoriche proposte, pur riconoscendo i limiti del contesto empirico.\n\nOffre predizioni interpretabili e comprensibili che possano guidare ulteriori ricerche, lo sviluppo teorico e decisioni pratiche.\n\nIn conclusione, riconoscere l’inevitabilità dell’errore di specificazione è essenziale per un utilizzo critico e consapevole dei SEM. Invece di mirare a modelli “veri” (un obiettivo irrealistico), il nostro scopo dovrebbe essere quello di costruire modelli sufficientemente validi e utili da consentire inferenze significative, pur accettandone le limitazioni. Questo approccio non elimina l’incertezza, ma permette di utilizzare i SEM come strumenti potenti per esplorare e chiarire le relazioni tra costrutti teorici complessi.\n\n48.8.5 Soppressione\nLe conseguenze dell’errore di specificazione possono manifestarsi sotto forma di soppressione (suppression), un fenomeno che si verifica quando le relazioni tra i predittori e la variabile criterio (dipendente) assumono caratteristiche inaspettate o controintuitive nell’analisi di regressione multipla. La soppressione si verifica in due situazioni principali:\n\nIl valore assoluto del peso \\(\\beta\\) di un predittore è maggiore della sua correlazione bivariata con la variabile criterio.\n\nIl peso \\(\\beta\\) e la correlazione bivariata hanno segni opposti.\n\nLa soppressione può essere suddivisa in tre categorie principali:\n\n48.8.5.1 1. Soppressione Negativa\nQuesto fenomeno si verifica quando un predittore ha una correlazione bivariata positiva con il criterio, ma riceve un peso \\(\\beta\\) negativo nell’analisi di regressione multipla.\nL’esempio precedente dell’errore di specificazione illustra proprio un caso di soppressione negativa: la variabile motivazione ha una correlazione bivariata positiva con la prestazione, ma, a causa della specificazione errata del modello (omissione della variabile ansia), il suo coefficiente \\(\\beta\\) diventa negativo. Questo tipo di soppressione è comune nei casi in cui predittori correlati negativamente tra loro competono per spiegare la stessa varianza della variabile criterio.\n\n48.8.5.2 2. Soppressione Classica\nNella soppressione classica, un predittore non ha alcuna correlazione bivariata con il criterio, ma riceve un peso \\(\\beta\\) diverso da zero nell’analisi di regressione multipla. Questo accade quando il predittore in questione contribuisce a ridurre la varianza non spiegata associata agli altri predittori, aumentando così il potere predittivo complessivo del modello.\nAd esempio, una variabile può essere utile per “sopprimere” la varianza irrilevante di un altro predittore, migliorando l’accuratezza delle stime.\n\n48.8.5.3 3. Soppressione Reciproca\nLa soppressione reciproca si verifica quando due predittori sono positivamente correlati con il criterio, ma sono negativamente correlati tra loro. In questi casi, l’inclusione di entrambi i predittori nel modello di regressione può aumentare i pesi \\(\\beta\\) di ciascuno, poiché ciascun predittore riduce la varianza non spiegata dell’altro, migliorando la spiegazione complessiva della varianza del criterio.\n\n48.8.6 Implicazioni della Soppressione\nLa soppressione, come conseguenza dell’errore di specificazione o della struttura dei dati, ha implicazioni significative per l’interpretazione dei modelli di regressione e SEM:\n\n\nDistorsione dei risultati: La soppressione può portare a interpretazioni controintuitive, come l’apparente effetto negativo di un predittore che ha una relazione positiva con il criterio.\n\n\nDipendenza dal modello: I pesi \\(\\beta\\) nei modelli di regressione e SEM sono sempre condizionali alle variabili incluse nel modello. Aggiungere o rimuovere predittori può alterare significativamente i pesi stimati.\n\n\nImportanza del controllo delle variabili: La soppressione evidenzia l’importanza di includere tutti i predittori rilevanti nel modello per evitare distorsioni e garantire stime accurate.\n\nIn conclusione, la soppressione è un fenomeno complesso ma inevitabile in analisi multivariate. Comprendere i meccanismi alla base della soppressione, e come essa può emergere a seguito di errori di specificazione, è cruciale per interpretare correttamente i risultati delle analisi statistiche.\n\n48.8.7 Regressione Stepwise\nNel contesto della regressione, è fondamentale comprendere che i predittori non dovrebbero essere selezionati basandosi esclusivamente sulle loro correlazioni bivariate con la variabile dipendente (criterio). Queste correlazioni, chiamate associazioni di ordine zero, non tengono conto dell’influenza reciproca tra i predittori e, di conseguenza, possono risultare fuorvianti quando si interpretano i coefficienti di regressione parziale.\nLa significatività statistica delle correlazioni bivariate non è un criterio affidabile per la selezione dei predittori, poiché non considera gli effetti congiunti di altri predittori nel modello. Questo punto è particolarmente rilevante nei modelli con più predittori correlati, dove le relazioni tra le variabili sono complesse e non lineari.\n\n48.8.7.1 Criticità delle Procedure Stepwise\nLe procedure automatiche di selezione dei predittori, come la regressione stepwise, possono sembrare attraenti per la loro semplicità ed efficienza. Tuttavia, queste tecniche presentano gravi limitazioni:\n- Sensibilità alla struttura dei dati: Piccole non-linearità o interazioni tra predittori possono alterare in modo significativo i coefficienti di regressione stimati.\n- Esposizione all’errore di specificazione: La rimozione o l’aggiunta automatica di variabili può distorcere il modello, attribuendo erroneamente effetti ad altri predittori.\n- Non replicabilità dei risultati: I modelli generati tramite procedure stepwise sono spesso specifici del campione di dati utilizzato, e i risultati tendono a non essere replicabili in altri campioni.\nPer queste ragioni, molte riviste scientifiche non accettano studi che utilizzano procedure stepwise. I risultati ottenuti con tali tecniche sono considerati inaffidabili e difficilmente generalizzabili.\n\n48.8.7.2 Selezione dei Predittori: Approccio Teorico vs. Statistico\nIn alternativa ai metodi automatici, i predittori dovrebbero essere scelti sulla base di considerazioni teoriche o di risultati empirici consolidati. Questo approccio ponderato aiuta a evitare distorsioni e garantisce che il modello rifletta le ipotesi di ricerca e il contesto teorico di riferimento.\nUna volta selezionati, i predittori possono essere inseriti nell’equazione di regressione con due strategie principali:\n1. Inserimento simultaneo: Tutti i predittori vengono inclusi contemporaneamente nel modello.\n2. Inserimento sequenziale: I predittori vengono aggiunti gradualmente, seguendo un ordine prestabilito, in base a criteri teorici o statistici.\n\n48.8.7.2.1 Regressione Gerarchica\nL’approccio teorico si traduce spesso nella regressione gerarchica, dove l’ordine di inserimento dei predittori è determinato da considerazioni razionali. Ad esempio, è comune includere prima le variabili demografiche e successivamente le variabili psicologiche di interesse. Questo approccio consente di:\n\nControllare gli effetti delle variabili demografiche.\n\nValutare il contributo unico delle variabili psicologiche, misurato tramite l’incremento del coefficiente di determinazione (\\(\\Delta R^2\\)).\n\n48.8.7.2.2 Regressione Stepwise\nL’approccio statistico, invece, è rappresentato dalla regressione stepwise, in cui il computer decide l’ordine di inserimento dei predittori in base ai valori di \\(p\\):\n- Nella stepwise forward inclusion, i predittori vengono aggiunti uno alla volta, scegliendo quello con il \\(p\\) più piccolo a ogni passo. Una volta aggiunti, i predittori rimangono nel modello.\n- Nella stepwise backward elimination, il modello parte con tutti i predittori e li elimina progressivamente in base ai loro valori di \\(p\\), fino a ottenere il modello finale.\n- Nella stepwise bidirezionale, predittori possono essere aggiunti o rimossi a ogni passo.\nQueste varianti si interrompono quando l’aggiunta o la rimozione di ulteriori predittori non migliora significativamente \\(\\Delta R^2\\). Tuttavia, come già sottolineato, tali procedure sono altamente problematiche e non raccomandate per la ricerca scientifica.\n\n48.8.8 La Tentazione di Rimuovere Predittori “Non Significativi”\nUn errore comune è la rimozione dal modello di predittori che non risultano “statisticamente significativi”. Questa pratica è controproducente per diverse ragioni:\n\n\nPotenza statistica insufficiente: In campioni piccoli, i test di significatività possono non rilevare effetti reali. Eliminare variabili potenzialmente rilevanti a causa della mancanza di significatività statistica può compromettere il modello.\n\n\nSpecificazione errata del modello: La rimozione di predittori senza una giustificazione teorica solida può introdurre errori di specificazione, distorcendo i coefficienti di regressione per le variabili rimanenti.\n\nSe esiste una motivazione teorica convincente per includere un predittore, questo dovrebbe essere mantenuto nel modello, indipendentemente dalla sua significatività statistica. Le decisioni relative ai predittori dovrebbero essere guidate da un approccio scientifico, non da criteri meccanici come i valori di \\(p\\).\nIn conclusione, le procedure di selezione dei predittori, in particolare quelle automatiche come la regressione stepwise, presentano gravi rischi di distorsione e non replicabilità. Al contrario, un approccio basato su solide considerazioni teoriche e supportato da prove empiriche garantisce modelli più robusti e utili Infine, è essenziale superare l’ossessione per la significatività statistica e focalizzarsi sulla coerenza teorica e sull’utilità del modello.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#oltre-la-regressione-multipla",
    "href": "chapters/sem/01_sem_intro.html#oltre-la-regressione-multipla",
    "title": "48  Introduzione ai Modelli SEM",
    "section": "\n48.9 Oltre la Regressione Multipla",
    "text": "48.9 Oltre la Regressione Multipla\nUna volta chiarita la relazione tra modello di regressione e modelli SEM, possiamo usare i modelli SEM per analizzare relazioni tra variabili che non possono essere descritte nei termini di un modello di regressione. Per fare un esempio, concludiamo il capitolo utilizzando il modello SEM per descrivere la relazione tra due costrutti:\nAnalizziamo un esempio in cui il modello di Equazioni Strutturali (SEM) viene impiegato per studiare la relazione tra autocompassione e disagio emotivo, utilizzando come indicatori le sotto-scale della DASS-21 (Depressione, Ansia, Stress) e della Self-Compassion Scale. In questo contesto, definiamo due variabili latenti: “disagio emotivo” e “autocompassione”. La variabile latente “disagio emotivo” è composta dalle tre sotto-scale della DASS-21, mentre la variabile “autocompassione” è formata dalle sei sotto-scale della Self-Compassion Scale.\nIl modello strutturale esplora la relazione tra queste due variabili latenti. L’autocompassione è considerata una variabile esogena, ipotizzata come un fattore di protezione che riduce il disagio emotivo, che a sua volta è trattato come variabile endogena. L’ipotesi principale del modello è che esista una relazione di regressione negativa tra autocompassione e disagio emotivo, indicando che livelli più elevati di autocompassione sono associati a minori livelli di disagio emotivo.\nUn elemento chiave dei modelli SEM è la gestione dell’errore di misurazione. Le variabili latenti sono progettate per riflettere il nucleo vero dei costrutti teorici, in questo caso autocompassione e disagio emotivo, isolando gli effetti degli errori di misurazione che possono affliggere gli indicatori osservati. Questo approccio consente di esaminare la “vera” relazione tra i costrutti, eliminando le distorsioni introdotte dagli errori di misurazione nelle misure osservate.\nLa capacità del modello SEM di separare la variabilità attribuibile ai costrutti latenti da quella dovuta agli errori di misurazione aumenta l’accuratezza e l’affidabilità dell’analisi. Questo è particolarmente vantaggioso in campi come la psicologia, dove i costrutti teorici non sono direttamente osservabili e devono essere inferiti attraverso misure potenzialmente errate.\n\nmod_sc &lt;- \"\n  ED =~ anxiety + depression + stress\n  SC =~ self_kindness   + common_humanity   + mindfulness   + \n        self_judgment   + isolation + over_identification\n  ED ~~ SC \n\"\n\nAdattiamo il modello ai dati.\n\nfit_sc &lt;- lavaan::sem(mod_sc, dat, std.lv = TRUE)\n\nEsaminiamo la soluzione ottenuta.\n\nstandardizedSolution(fit_sc) \n#&gt;                    lhs op                 rhs est.std    se      z pvalue\n#&gt; 1                   ED =~             anxiety   0.847 0.014  58.51      0\n#&gt; 2                   ED =~          depression   0.909 0.011  82.50      0\n#&gt; 3                   ED =~              stress   0.929 0.010  91.94      0\n#&gt; 4                   SC =~       self_kindness   0.757 0.022  33.98      0\n#&gt; 5                   SC =~     common_humanity   0.621 0.030  20.70      0\n#&gt; 6                   SC =~         mindfulness   0.689 0.026  26.23      0\n#&gt; 7                   SC =~       self_judgment  -0.770 0.022 -35.80      0\n#&gt; 8                   SC =~           isolation  -0.770 0.022 -35.82      0\n#&gt; 9                   SC =~ over_identification  -0.767 0.022 -35.41      0\n#&gt; 10                  ED ~~                  SC  -0.476 0.038 -12.38      0\n#&gt; 11             anxiety ~~             anxiety   0.282 0.025  11.49      0\n#&gt; 12          depression ~~          depression   0.173 0.020   8.63      0\n#&gt; 13              stress ~~              stress   0.136 0.019   7.25      0\n#&gt; 14       self_kindness ~~       self_kindness   0.427 0.034  12.65      0\n#&gt; 15     common_humanity ~~     common_humanity   0.615 0.037  16.53      0\n#&gt; 16         mindfulness ~~         mindfulness   0.525 0.036  14.52      0\n#&gt; 17       self_judgment ~~       self_judgment   0.407 0.033  12.29      0\n#&gt; 18           isolation ~~           isolation   0.407 0.033  12.28      0\n#&gt; 19 over_identification ~~ over_identification   0.411 0.033  12.36      0\n#&gt; 20                  ED ~~                  ED   1.000 0.000     NA     NA\n#&gt; 21                  SC ~~                  SC   1.000 0.000     NA     NA\n#&gt;    ci.lower ci.upper\n#&gt; 1     0.819    0.876\n#&gt; 2     0.888    0.931\n#&gt; 3     0.910    0.949\n#&gt; 4     0.713    0.801\n#&gt; 5     0.562    0.679\n#&gt; 6     0.637    0.740\n#&gt; 7    -0.812   -0.728\n#&gt; 8    -0.812   -0.728\n#&gt; 9    -0.810   -0.725\n#&gt; 10   -0.551   -0.400\n#&gt; 11    0.234    0.330\n#&gt; 12    0.134    0.212\n#&gt; 13    0.099    0.173\n#&gt; 14    0.361    0.493\n#&gt; 15    0.542    0.688\n#&gt; 16    0.454    0.596\n#&gt; 17    0.342    0.472\n#&gt; 18    0.342    0.472\n#&gt; 19    0.346    0.476\n#&gt; 20    1.000    1.000\n#&gt; 21    1.000    1.000\n\n\n\nSaturazioni Fattoriali (Loadings) per le Variabili Latenti:\n\n\nED: Le variabili osservate “anxiety”, “depression”, e “stress” hanno elevate saturazioni fattoriali sulla variabile latente “ED”. Questo suggerisce che ciascuna di queste misure è un buon indicatore della variabile latente “ED”.\n\nSC: Le variabili “self_kindness”, “common_humanity”, “mindfulness”, “self_judgment”, “isolation”, e “over_identification” hanno anch’esse significative saturazioni sulla variabile latente “SC”. Si noti che “self_judgment”, “isolation”, e “over_identification” hanno saturazioni negative, indicando che queste variabili sono inversamente associate con “SC”.\n\n\n\nCorrelazione tra Variabili Latenti:\n\nLa correlazione tra “ED” e “SC” mostra un coefficiente negativo (-0.476), il che indica una relazione inversa tra queste due variabili latenti. Questo significa che livelli più alti di “SC” sono associati a livelli più bassi di “ED”.\n\n\n\nVarianza delle Variabili Latenti:\n\nLa varianza di “ED” e “SC” indica quanto della variazione nelle variabili latenti è spiegata dai loro rispettivi indicatori. La varianza di “ED” (0.77) è relativamente alta, suggerendo che gli indicatori spiegano una buona parte della varianza in “ED”. La varianza di “SC” è fissata a 1, un approccio comune per identificare il modello.\n\n\n\nVarianze Residue degli Indicatori:\n\nLe varianze residue (ad esempio, “anxiety ~~ anxiety”) rappresentano la varianza non spiegata in ciascun indicatore dalle variabili latenti. Valori più bassi indicano che la variabile latente spiega una maggior parte della varianza dell’indicatore. Ad esempio, “anxiety” ha una varianza residua di 0.28, suggerendo che “ED” spiega una buona parte, ma non tutta, della varianza in “anxiety”.\n\n\n\nGeneriamo una rappresentazione grafica del modello.\n\nsemPaths(fit_sc,\n    whatLabels = \"std\",\n    sizeMan = 10,\n    edge.label.cex = 0.9,\n    style = \"mx\",\n    nCharNodes = 5, nCharEdges = 0, \n    fade=FALSE\n)\n\n\n\n\n\n\n\nI coefficienti stimati nel modello SEM appaiono coerenti e in linea con le aspettative, in particolare il coefficiente che descrive la correlazione tra il fattore dell’autocompassione e il disagio emotivo, che si attesta a -0.48. Questo valore negativo corrobora l’ipotesi secondo cui l’autocompassione svolge un ruolo di fattore protettivo contro il disagio emotivo. Tuttavia, prima di confermare definitivamente questa conclusione, è cruciale esaminare gli indici di bontà di adattamento del modello. Questi indici ci permetteranno di valutare quanto accuratamente il modello SEM si adatta ai dati osservati, fornendo un quadro più chiaro della validità delle nostre inferenze. In altre parole, sebbene il modello suggerisca una relazione negativa tra autocompassione e disagio emotivo, la conferma finale di questa associazione dipenderà dall’adeguatezza complessiva del modello rispetto ai dati. Questo argomento verrà affrontato nei prossimi capitoli.\n\n48.9.1 Vantaggi del Modello SEM\nÈ evidente che avremmo potuto calcolare la correlazione tra disagio emotivo e autocompassione in modo più semplice, utilizzando semplicemente la correlazione di Pearson tra il punteggio totale del DASS-21 (disagio emotivo) e il punteggio totale della Self-Compassion Scale (SCS). Per calcolare il punteggio totale del DASS-21, sommiamo le sottoscale di stress, ansia e depressione:\n\ndat &lt;- dat |&gt; \n  mutate(ed = stress + anxiety + depression)\n\nIl valore della correlazione così ottenuto è pari a 0.405:\n\ncor.test(dat$ed, dat$scs_ts)\n#&gt; \n#&gt;  Pearson's product-moment correlation\n#&gt; \n#&gt; data:  dat$ed and dat$scs_ts\n#&gt; t = -10, df = 524, p-value &lt;2e-16\n#&gt; alternative hypothesis: true correlation is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  -0.474 -0.331\n#&gt; sample estimates:\n#&gt;    cor \n#&gt; -0.405\n\nTuttavia, questo valore differisce in maniera rilevante da quello calcolato mediante il modello SEM, dove la correlazione stimata è \\(r = -0.476\\). Questa differenza evidenzia una distinzione fondamentale tra i due approcci:\n\nCorrelazione di Pearson:\nLa correlazione di Pearson si basa sui punteggi osservati delle variabili, che includono sia il punteggio vero (il costrutto sottostante) sia l’errore di misurazione. Di conseguenza, la correlazione calcolata può essere distorta dall’errore associato alla misurazione di ciascun costrutto.\nModello SEM:\nNei modelli SEM, la correlazione è stimata tra i punteggi veri dei costrutti, al netto dell’errore di misurazione. Questo approccio fornisce una stima più accurata e affidabile della relazione tra i costrutti teorici, eliminando la varianza attribuibile agli errori di misurazione.\n\nIl principale vantaggio dei modelli SEM è la loro capacità di descrivere le associazioni tra le variabili in modo meno influenzato dagli errori di misurazione rispetto alle semplici correlazioni. Questo permette di ottenere stime più vicine alle relazioni reali tra i costrutti, migliorando la validità delle conclusioni.\nLa differenza tra i due valori di correlazione calcolati sottolinea l’importanza di utilizzare modelli SEM quando si vuole ottenere una rappresentazione più precisa delle relazioni tra costrutti latenti, in particolare in presenza di variabili soggette a errori di misurazione.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#impiego-delle-medie-nei-modelli-sem",
    "href": "chapters/sem/01_sem_intro.html#impiego-delle-medie-nei-modelli-sem",
    "title": "48  Introduzione ai Modelli SEM",
    "section": "\n48.10 Impiego delle Medie nei Modelli SEM",
    "text": "48.10 Impiego delle Medie nei Modelli SEM\nUn altro elemento importante da evidenziare nell’introduzione ai modelli SEM è il ruolo delle medie. Nei SEM, l’enfasi è tradizionalmente posta sull’analisi delle covarianze tra variabili. Tuttavia, a differenza dell’analisi fattoriale classica, i SEM permettono di includere anche le medie delle variabili osservate e latenti. Questo arricchisce l’analisi, fornendo informazioni importanti in contesti come i modelli longitudinali di analisi fattoriale confermativa (CFA), dove le ipotesi centrali possono riguardare cambiamenti o differenze nelle medie dei costrutti latenti nel tempo.\n\n48.10.1 Struttura delle Medie nei Modelli SEM\nLa struttura delle medie nei SEM può essere descritta con la seguente equazione:\n\\[\nE(\\mathbf{y}) = \\boldsymbol{\\mu}_y = \\boldsymbol{\\tau} + \\mathbf{\\Lambda} \\boldsymbol{\\alpha},\n\\]\ndove:\n\n\n\\(\\mathbf{y}\\) rappresenta i punteggi degli indicatori osservati,\n\n\n\\(E(\\mathbf{y})\\) è il vettore delle medie attese degli indicatori,\n\n\n\\(\\boldsymbol{\\mu}_y\\) è il vettore delle medie degli indicatori osservati (analogo a \\(\\mathbf{\\Sigma}\\) nelle strutture di covarianza),\n\n\n\\(\\boldsymbol{\\tau}\\) è il vettore delle intercette degli indicatori,\n\n\n\\(\\mathbf{\\Lambda}\\) è la matrice dei carichi fattoriali, che definisce la relazione tra gli indicatori e i costrutti latenti,\n\n\n\\(\\boldsymbol{\\alpha}\\) è il vettore delle medie dei costrutti latenti.\n\nIn un diagramma a percorsi, l’intercetta è rappresentata da un triangolo con il numero 1, che funge da costante di regressione. Questo simbolo indica l’origine della scala per la variabile e permette di stimare la media di una variabile quando viene regredita su questa costante.\n\n48.10.2 Interpretazione e Collegamento tra Medie e Carichi Fattoriali\nNei modelli SEM, gli indicatori con carichi fattoriali più elevati (\\(\\mathbf{\\Lambda}\\)) esercitano un impatto maggiore sulla media stimata del costrutto (\\(\\boldsymbol{\\alpha}\\)). Questo collegamento evidenzia che i carichi fattoriali non solo influenzano la struttura di covarianza, ma giocano un ruolo chiave anche nella determinazione delle medie latenti.\n\n48.10.3 Vincoli e Scalatura delle Medie\nCome per le strutture di covarianza, anche per le strutture delle medie è necessario imporre vincoli per definire la scala del modello. Spesso si utilizza lo zero come riferimento per le medie dei costrutti latenti. Questo vincolo serve a fissare un punto di partenza per le stime e a calcolare le differenze rispetto al riferimento, che possono assumere valori sia positivi sia negativi.\nAd esempio:\n\nNei modelli CFA longitudinali, i vincoli sulle medie possono essere utilizzati per studiare cambiamenti temporali nei costrutti latenti.\n\nNei modelli con gruppi multipli, i vincoli sulle medie permettono di confrontare le medie dei costrutti tra gruppi diversi.\n\n48.10.4 Stima delle Intercette con lavaan\n\nPer stimare le intercette e includere la struttura delle medie in un modello SEM, è necessario disporre non solo della matrice di covarianza, ma anche delle medie delle variabili osservate. Il software lavaan semplifica questo processo attraverso l’opzione meanstructure = TRUE. Quando questa opzione è attivata, lavaan integra automaticamente una costante “1” in tutte le equazioni del modello, consentendo di stimare:\n\nLe intercette per le variabili osservate.\n\nLe medie dei costrutti latenti (\\(\\boldsymbol{\\alpha}\\)).\n\nEsempio di codice in lavaan per includere le medie:\nmodel &lt;- \"\n  y1 + y2 + y3 =~ latent1\n  y4 + y5 + y6 =~ latent2\n\"\nfit &lt;- lavaan::sem(model, data = dat, meanstructure = TRUE)\nsummary(fit, standardize = TRUE)\nIn questo contesto, lavaan stimerà sia le strutture di covarianza sia le medie e le intercette per le variabili osservate e latenti. L’inclusione delle medie è specificata dall’uso dell’argomento meanstructure = TRUE.\n\n48.10.5 Importanza delle Medie nei Modelli SEM\nL’inclusione delle medie nei modelli SEM fornisce un livello aggiuntivo di informazione, permettendo di confrontare:\n\n\nLe medie stimate dai modelli SEM con le medie osservate nei dati.\n\nLe medie di costrutti latenti tra diversi gruppi o in momenti temporali differenti.\n\nIn conclusione, l’integrazione delle medie nei modelli SEM rappresenta un importante arricchimento rispetto all’analisi delle sole covarianze. Questo approccio consente di ottenere una visione più completa delle relazioni tra variabili, al netto dell’errore di misurazione, e di esplorare le differenze nelle medie dei costrutti in contesti complessi come studi longitudinali o analisi multigruppo. Di conseguenza, l’analisi delle medie nei SEM non solo amplia la gamma di domande di ricerca a cui è possibile rispondere, ma migliora anche la validità delle conclusioni tratte dai dati.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#riflessioni-conclusive",
    "href": "chapters/sem/01_sem_intro.html#riflessioni-conclusive",
    "title": "48  Introduzione ai Modelli SEM",
    "section": "\n48.11 Riflessioni Conclusive",
    "text": "48.11 Riflessioni Conclusive\nIn questo capitolo, abbiamo esplorato i Modelli di Equazioni Strutturali (SEM), evidenziando come questi modelli non si limitino a descrivere le correlazioni tra variabili osservabili, ma permettano anche di analizzare le relazioni tra variabili latenti. La forza dei SEM risiede nella loro capacità di integrare il modello di misurazione, che definisce le relazioni tra gli indicatori e le variabili latenti, con il modello strutturale, che esamina le interazioni tra le stesse variabili latenti.\nNei prossimi capitoli, approfondiremo vari aspetti della modellazione SEM. Esamineremo la bontà di adattamento del modello, un criterio fondamentale per verificare la fedeltà con cui il modello riflette la realtà osservata. Analizzeremo anche il confronto tra modelli alternativi, un passaggio cruciale per identificare il modello che migliora l’interpretazione dei dati.\nUn altro tema importante sarà l’analisi dell’applicabilità dei modelli a gruppi diversi, vitale per valutare la loro generalizzabilità e la pertinenza in contesti specifici. Inoltre, discuteremo le sfide metodologiche legate alla gestione di dati categoriali, all’implementazione di modelli SEM multilivello e alla gestione di dati mancanti. Questi approfondimenti ci permetteranno di comprendere meglio come i modelli SEM possono essere adattati e applicati efficacemente in diversi ambiti di ricerca.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#session-info",
    "href": "chapters/sem/01_sem_intro.html#session-info",
    "title": "48  Introduzione ai Modelli SEM",
    "section": "\n48.12 Session Info",
    "text": "48.12 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] effectsize_1.0.0   rsvg_2.6.1         DiagrammeRsvg_0.1 \n#&gt;  [4] mvnormalTest_1.0.0 lavaanExtra_0.2.1  ggokabeito_0.1.0  \n#&gt;  [7] see_0.10.0         MASS_7.3-65        viridis_0.6.5     \n#&gt; [10] viridisLite_0.4.2  ggpubr_0.6.0       ggExtra_0.10.1    \n#&gt; [13] gridExtra_2.3      patchwork_1.3.0    bayesplot_1.11.1  \n#&gt; [16] semTools_0.5-6     semPlot_1.1.6      lavaan_0.6-19     \n#&gt; [19] psych_2.4.12       scales_1.3.0       markdown_1.13     \n#&gt; [22] knitr_1.49         lubridate_1.9.4    forcats_1.0.0     \n#&gt; [25] stringr_1.5.1      dplyr_1.1.4        purrr_1.0.4       \n#&gt; [28] readr_2.1.5        tidyr_1.3.1        tibble_3.2.1      \n#&gt; [31] ggplot2_3.5.1      tidyverse_2.0.0    here_1.0.1        \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.0      datawizard_1.0.0   \n#&gt;   [4] magrittr_2.0.3      TH.data_1.1-3       estimability_1.5.1 \n#&gt;   [7] farver_2.1.2        nloptr_2.1.1        rmarkdown_2.29     \n#&gt;  [10] vctrs_0.6.5         minqa_1.2.8         base64enc_0.1-3    \n#&gt;  [13] rstatix_0.7.2       htmltools_0.5.8.1   curl_6.2.1         \n#&gt;  [16] broom_1.0.7         Formula_1.2-5       htmlwidgets_1.6.4  \n#&gt;  [19] plyr_1.8.9          sandwich_3.1-1      copula_1.1-5       \n#&gt;  [22] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [25] mime_0.12           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [28] Matrix_1.7-2        R6_2.6.1            fastmap_1.2.0      \n#&gt;  [31] rbibutils_2.3       shiny_1.10.0        numDeriv_2016.8-1.1\n#&gt;  [34] digest_0.6.37       OpenMx_2.21.13      fdrtool_1.2.18     \n#&gt;  [37] colorspace_2.1-1    rprojroot_2.0.4     Hmisc_5.2-2        \n#&gt;  [40] pspline_1.0-21      timechange_0.3.0    abind_1.4-8        \n#&gt;  [43] compiler_4.4.2      gsl_2.1-8           withr_3.0.2        \n#&gt;  [46] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n#&gt;  [49] carData_3.0-5       ggsignif_0.6.4      corpcor_1.6.10     \n#&gt;  [52] gtools_3.9.5        tools_4.4.2         pbivnorm_0.6.0     \n#&gt;  [55] foreign_0.8-88      zip_2.3.2           httpuv_1.6.15      \n#&gt;  [58] nnet_7.3-20         glue_1.8.0          quadprog_1.5-8     \n#&gt;  [61] stabledist_0.7-2    nlme_3.1-167        promises_1.3.2     \n#&gt;  [64] lisrelToR_0.3       grid_4.4.2          checkmate_2.3.2    \n#&gt;  [67] cluster_2.1.8       reshape2_1.4.4      generics_0.1.3     \n#&gt;  [70] gtable_0.3.6        nortest_1.0-4       tzdb_0.4.0         \n#&gt;  [73] data.table_1.17.0   hms_1.1.3           car_3.1-3          \n#&gt;  [76] sem_3.1-16          pillar_1.10.1       rockchalk_1.8.157  \n#&gt;  [79] later_1.4.1         splines_4.4.2       moments_0.14.1     \n#&gt;  [82] lattice_0.22-6      survival_3.8-3      kutils_1.73        \n#&gt;  [85] tidyselect_1.2.1    ADGofTest_0.3       miniUI_0.1.1.1     \n#&gt;  [88] pbapply_1.7-2       reformulas_0.4.0    V8_6.0.1           \n#&gt;  [91] stats4_4.4.2        xfun_0.51           qgraph_1.9.8       \n#&gt;  [94] arm_1.14-4          stringi_1.8.4       pacman_0.5.1       \n#&gt;  [97] boot_1.3-31         evaluate_1.0.3      codetools_0.2-20   \n#&gt; [100] mi_1.1              cli_3.6.4           RcppParallel_5.1.10\n#&gt; [103] rpart_4.1.24        parameters_0.24.1   xtable_1.8-4       \n#&gt; [106] Rdpack_2.6.2        munsell_0.5.1       Rcpp_1.0.14        \n#&gt; [109] coda_0.19-4.1       png_0.1-8           XML_3.99-0.18      \n#&gt; [112] parallel_4.4.2      bayestestR_0.15.2   jpeg_0.1-10        \n#&gt; [115] lme4_1.1-36         mvtnorm_1.3-3       insight_1.0.2      \n#&gt; [118] pcaPP_2.0-5         openxlsx_4.2.8      rlang_1.1.5        \n#&gt; [121] multcomp_1.4-28     mnormt_2.1.1\n\n\n\n\n\nMauro, R. (1990). Understanding LOVE (left out variables error): A method for estimating the effects of omitted variables. Psychological Bulletin, 108(2), 314–329.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.\n\n\nSaqr, M., & López-Pernas, S. (2024). Learning analytics methods and tutorials: A practical guide using R. Springer Nature.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/02_data_preparation.html",
    "href": "chapters/sem/02_data_preparation.html",
    "title": "49  Preparazione dei Dati",
    "section": "",
    "text": "49.1 Introduzione\nQuesto capitolo breve ma essenziale esplora diversi aspetti fondamentali della gestione dei dati nell’ambito della modellazione con Modelli di Equazioni Strutturali (SEM).",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Preparazione dei Dati</span>"
    ]
  },
  {
    "objectID": "chapters/sem/02_data_preparation.html#formati-dei-dati-di-input",
    "href": "chapters/sem/02_data_preparation.html#formati-dei-dati-di-input",
    "title": "49  Preparazione dei Dati",
    "section": "49.2 Formati dei Dati di Input",
    "text": "49.2 Formati dei Dati di Input\nI ricercatori spesso analizzano file di dati grezzi. Tuttavia, alcune analisi SEM possono essere eseguite anche con matrici di covarianze e medie. Se si utilizzano dati grezzi, il software SEM crea una propria matrice di covarianza per l’analisi. Talvolta, è necessario usare dati grezzi, come in casi di distribuzioni non normali, dati mancanti o variabili categoriali.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Preparazione dei Dati</span>"
    ]
  },
  {
    "objectID": "chapters/sem/02_data_preparation.html#definitezza-positiva",
    "href": "chapters/sem/02_data_preparation.html#definitezza-positiva",
    "title": "49  Preparazione dei Dati",
    "section": "49.3 Definitezza Positiva",
    "text": "49.3 Definitezza Positiva\nÈ fondamentale che la matrice di dati, sia quella inizialmente fornita come input che quella calcolata dal computer durante l’analisi, soddisfi i criteri di essere positiva definita. Questo concetto implica diverse proprietà chiave: innanzitutto, la matrice deve avere un inverso, il che significa che non è singolare e può essere invertita matematicamente. Inoltre, è necessario che tutti gli autovalori della matrice siano positivi, indicando che non esistono autovalori negativi che potrebbero causare problemi durante l’analisi. Inoltre, la matrice deve essere priva di correlazioni o covarianze al di fuori limite.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Preparazione dei Dati</span>"
    ]
  },
  {
    "objectID": "chapters/sem/02_data_preparation.html#dati-mancanti",
    "href": "chapters/sem/02_data_preparation.html#dati-mancanti",
    "title": "49  Preparazione dei Dati",
    "section": "49.4 Dati Mancanti",
    "text": "49.4 Dati Mancanti\nQuesto è un argomento complesso che richiede l’uso di metodi statistici moderni e sarà approfondito in un capitolo successivo.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Preparazione dei Dati</span>"
    ]
  },
  {
    "objectID": "chapters/sem/02_data_preparation.html#screening-dei-dati",
    "href": "chapters/sem/02_data_preparation.html#screening-dei-dati",
    "title": "49  Preparazione dei Dati",
    "section": "49.5 Screening dei Dati",
    "text": "49.5 Screening dei Dati\n\nCollinearità Estrema, Valori Anomali e Violazioni delle Assunzioni Distribuzionali: È importante gestire questi problemi per assicurare l’affidabilità dei risultati SEM. La collinearità estrema può essere rilevata tramite il fattore di inflazione della varianza (VIF), mentre i valori anomali e le violazioni delle ipotesi distribuzionali richiedono metodi specifici per essere identificati e gestiti.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Preparazione dei Dati</span>"
    ]
  },
  {
    "objectID": "chapters/sem/02_data_preparation.html#varianze-relative",
    "href": "chapters/sem/02_data_preparation.html#varianze-relative",
    "title": "49  Preparazione dei Dati",
    "section": "49.6 Varianze Relative",
    "text": "49.6 Varianze Relative\n\nGestione delle Varianze: La differenza eccessiva tra le varianze può complicare l’iterazione dei metodi di stima in SEM. Per mitigare questo aspetto, i dati con varianze molto basse o alte possono essere riscalati.\n\nIn sintesi, prima di procedere a qualunque analisi statistica è necessario affrontare diversi problemi relativi alla corretta preparazione e gestione dei dati. Questi aspetti sono fondamentali per assicurare l’accuratezza e l’affidabilità dei risultati delle analisi SEM.\n\n\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Preparazione dei Dati</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html",
    "href": "chapters/sem/03_gof.html",
    "title": "50  Test del Modello e Indicizzazione",
    "section": "",
    "text": "50.1 Introduzione\nIn questo capitolo, ci concentriamo sulle due principali categorie di statistiche per valutare l’adattamento globale nei modelli SEM: le statistiche di test del modello e gli indici di adattamento approssimativo. Queste due categorie si riferiscono rispettivamente al test di adattamento del modello e alla misurazione continua della sua bontà.\nUn aspetto critico da considerare è che, benché entrambe le categorie di statistiche valutino la corrispondenza media o generale tra modello e dati, possono non rilevare un cattivo adattamento locale. Questo si riferisce a specifiche coppie di variabili osservate per cui il modello potrebbe non spiegare adeguatamente le associazioni osservate. È fondamentale riconoscere che un modello con adattamento locale inadeguato non dovrebbe essere accettato, indipendentemente dalla sua bontà di adattamento globale.\nLa valutazione completa di un modello SEM segue una sequenza metodica: specificazione del modello, stima dei parametri, verifica dell’adattamento e dei parametri, e, se necessario, modifica del modello. Questo processo iterativo prosegue finché si identifica un modello ritenuto accettabile.\nInoltre, questo capitolo esplora due metodi fondamentali per pianificare la dimensione del campione nei modelli SEM: l’analisi della potenza e la stima della precisione dei parametri (precisione nella pianificazione). Questi approcci sono essenziali per assicurare che lo studio sia adeguatamente dimensionato e che i parametri siano stimati con massima precisione. La valutazione degli indici di bontà dell’adattamento, ampiamente utilizzati nella letteratura, rappresenterà un elemento chiave in questo contesto, fornendo una panoramica completa degli strumenti disponibili per giudicare l’efficacia dei modelli SEM.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#introduzione",
    "href": "chapters/sem/03_gof.html#introduzione",
    "title": "50  Test del Modello e Indicizzazione",
    "section": "",
    "text": "Statistiche di Test del Modello: Queste statistiche prevedono una decisione binaria, ossia stabilire se accettare o respingere le ipotesi nulle riguardanti il modello. La decisione si basa sui valori-p derivati dai test di significatività, con l’obiettivo di verificare se l’intero modello si adatti ai dati osservati.\nIndici di Adattamento Approssimativo: A differenza delle statistiche di test, gli indici di adattamento approssimativo forniscono una misura continua che esprime il grado di adattamento del modello ai dati. Questo approccio ricorda la stima dell’effetto quantitativo più che un test dicotomico, fornendo così una valutazione più dettagliata dell’adattamento e superando la semplice accettazione o rifiuto dell’ipotesi nulla.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#valutazione-della-bontà-di-adattamento-nel-modello-sem",
    "href": "chapters/sem/03_gof.html#valutazione-della-bontà-di-adattamento-nel-modello-sem",
    "title": "50  Test del Modello e Indicizzazione",
    "section": "\n50.2 Valutazione della Bontà di Adattamento nel Modello SEM",
    "text": "50.2 Valutazione della Bontà di Adattamento nel Modello SEM\nNel contesto dei modelli SEM (Structural Equation Modeling), la valutazione dell’adattamento del modello si basa sul confronto tra la matrice di varianze e covarianze stimata dal modello, \\(\\Sigma(\\hat{\\theta})\\), e la matrice di covarianza campionaria, \\(S\\). Il nostro obiettivo è verificare se la discrepanza tra queste due matrici indica possibili inadeguatezze nel modello proposto. Ecco alcuni aspetti rilevanti da considerare:\n\nModelli Saturi vs Modelli Ristretti: Un modello saturo include un numero di parametri in \\(\\theta\\) pari al numero di elementi distinti nella matrice di covarianza. In contrasto, un modello ristretto ha meno parametri rispetto al numero degli elementi distinti nella matrice di covarianza. La differenza tra questi due numeri corrisponde ai gradi di libertà del modello. Per esempio, in un modello saturo, se il numero dei parametri in \\(\\theta\\) e il numero degli elementi distinti nella matrice di covarianza sono entrambi 3, allora il modello ha zero gradi di libertà.\nPerfetto Adattamento dei Modelli Saturi: In un modello saturo, \\(\\Sigma(\\hat{\\theta})\\) coincide sempre con \\(S\\), poiché il modello ha abbastanza parametri per adattarsi perfettamente ai dati del campione. Tuttavia, ciò non implica necessariamente che il modello rappresenti fedelmente la popolazione più ampia. Le stime dei parametri in un modello saturo possono fornire informazioni sui pattern di relazione tra le variabili nel campione specifico, ma è cruciale interpretarle con cautela.\nStima e Identificabilità del Modello: Generalmente, la stima dei parametri non si basa sul semplice risolvere un sistema di equazioni matematiche. Invece, si utilizza una funzione di adattamento o discrepanza tra \\(\\Sigma(\\theta)\\) e \\(S\\), cercando il valore ottimale di \\(\\hat{\\theta}\\) attraverso tecniche di ottimizzazione numerica. Un modello SEM deve essere identificabile, il che significa che deve essere possibile stimare univocamente i parametri del modello. L’identificabilità implica che il numero di unità di informazione, come elementi nella matrice di covarianza, sia maggiore o uguale al numero di parametri da stimare.\n\n\n50.2.1 Gradi di Libertà e Identificabilità del Modello\nI gradi di libertà (dof) in un modello SEM sono calcolati come:\n\\[\ndof = \\# (\\text{unità di informazione}) - \\# (\\text{parametri da stimare})\n\\]\nPer una matrice di covarianza di ordine $ p $, il numero di unità di informazione è $ $. Per garantire l’identificabilità, è necessario soddisfare alcune condizioni:\n\nIn tutti i modelli, l’unità di misura delle variabili latenti deve essere specificata.\nIl numero di unità di informazione deve essere uguale o superiore al numero di parametri da stimare.\nIn modelli ad un fattore, è richiesto un minimo di tre indicatori per una soluzione “appena identificata”.\nIn modelli a più fattori, si raccomanda un minimo di tre indicatori per ogni variabile latente.\n\nUn modello è:\n\n\nNon identificato se \\(dof &lt; 0\\).\n\nAppena identificato o “saturo” se \\(dof = 0\\).\n\nSovra-identificato se \\(dof &gt; 0\\).\n\nÈ importante notare che un’analisi fattoriale con solo due indicatori per un fattore non è possibile, poiché ci sono meno unità di informazione rispetto ai parametri da stimare. Un modello con tre indicatori e un fattore è “appena identificato”, senza gradi di libertà per valutare la bontà dell’adattamento. Per modelli ad un solo fattore comune latente, è quindi necessario disporre di almeno quattro indicatori.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#funzione-di-discrepanza-e-valutazione-della-bontà-di-adattamento",
    "href": "chapters/sem/03_gof.html#funzione-di-discrepanza-e-valutazione-della-bontà-di-adattamento",
    "title": "50  Test del Modello e Indicizzazione",
    "section": "\n50.3 Funzione di Discrepanza e Valutazione della Bontà di Adattamento",
    "text": "50.3 Funzione di Discrepanza e Valutazione della Bontà di Adattamento\nLa funzione di discrepanza tra \\(S\\) (matrice di covarianza osservata) e \\(\\Sigma(\\theta)\\) (matrice di covarianza stimata dal modello in base ai parametri \\(\\theta\\)) misura l’adeguatezza con cui il modello rappresenta i dati. Derivata dalla log-verosimiglianza per una distribuzione normale multivariata, la funzione confronta le strutture di covarianza teoriche e osservate.\nLa formula per la discrepanza ML (Massima Verosimiglianza) è:\n\\[\nFML(S, \\Sigma(\\theta)) = \\log|\\Sigma(\\theta)| - \\log|S| + \\text{traccia}(S\\Sigma(\\theta)^{-1}) - p,\n\\]\ndove \\(|S|\\) e \\(|\\Sigma(\\theta)|\\) indicano i determinanti di \\(S\\) e \\(\\Sigma(\\theta)\\) rispettivamente, e \\(p\\) è la dimensione delle matrici. Vediamo ogni termine per comprenderne il significato.\n\n50.3.1 Componenti della Formula di Discrepanza\n\n\nLogaritmo del determinante della matrice stimata, \\(\\log|\\Sigma(\\theta)|\\):\n\nIl termine \\(\\log|\\Sigma(\\theta)|\\) rappresenta una misura della “dimensione” o “scala” della matrice \\(\\Sigma(\\theta)\\). Più precisamente, il determinante di una matrice di covarianza può essere visto come una misura del volume dello spazio descritto dalle variabili nel modello: maggiore è il determinante, più “ampio” è lo spazio che copre la distribuzione del modello. Il logaritmo del determinante di \\(\\Sigma(\\theta)\\) contribuisce quindi a quantificare la scala complessiva del modello.\n\n\n\nLogaritmo del determinante della matrice osservata, \\(\\log|S|\\):\n\nSimilmente, \\(\\log|S|\\) rappresenta la scala della matrice di covarianza osservata nei dati. Questo termine funge da riferimento per confrontare la scala dei dati con quella stimata dal modello. In altre parole, \\(|S|\\) ci dice quale sarebbe la “dimensione” dei dati se fossero perfettamente rappresentati solo da \\(S\\), la matrice di covarianza empirica.\n\n\n\nTraccia del prodotto \\(S\\Sigma(\\theta)^{-1}\\):\n\nLa traccia, ossia la somma degli elementi diagonali, del prodotto \\(S\\Sigma(\\theta)^{-1}\\) rappresenta la relazione tra \\(S\\) e l’inverso della matrice \\(\\Sigma(\\theta)\\). Se \\(S\\) e \\(\\Sigma(\\theta)\\) fossero perfettamente identiche, questa traccia sarebbe pari a \\(p\\), la dimensione delle matrici, perché il prodotto di una matrice con la propria inversa è la matrice identità, che ha una somma degli elementi diagonali pari alla dimensione. Un valore diverso da \\(p\\) indica discrepanze tra le covarianze osservate e quelle stimate.\n\n\n\nTermine di normalizzazione, \\(-p\\):\n\nSottrarre \\(p\\) serve a normalizzare la traccia in modo che, in assenza di discrepanze (ovvero quando \\(S = \\Sigma(\\theta)\\)), il valore complessivo della funzione di discrepanza sia zero. Questo termine fa sì che la discrepanza sia relativa a quanto \\(S\\) differisca da \\(\\Sigma(\\theta)\\) in una forma più bilanciata.\n\n\n\n50.3.2 Interpretazione Complessiva\nLa funzione di discrepanza combina queste tre componenti per ottenere una misura della distanza o della differenza tra \\(S\\) e \\(\\Sigma(\\theta)\\). Essa confronta sia la “dimensione” complessiva (tramite i termini log-determinante) sia la “forma” (tramite la traccia) delle due matrici. In sintesi, la funzione di discrepanza \\(FML(S, \\Sigma(\\theta))\\) ci indica quanto il modello con parametri \\(\\theta\\) si discosta dai dati osservati e consente di capire se il modello è una buona rappresentazione delle relazioni di covarianza presenti nei dati.\nSe questa funzione di discrepanza risulta elevata, significa che le covarianze stimate dal modello non rispecchiano adeguatamente quelle osservate, indicando una possibile necessità di migliorare il modello o di rivedere i parametri \\(\\theta\\).\n\n50.3.3 Distribuzione e Test di Adattamento\nLa discrepanza calcolata, sotto l’ipotesi di buon adattamento, si distribuisce asintoticamente come una variabile chi-quadrato (χ²), che permette un test statistico. I gradi di libertà sono dati dalla differenza tra il numero di elementi indipendenti nella matrice di covarianza e il numero di parametri del modello.\n\nSe il valore di discrepanza è minore del valore critico χ², l’ipotesi nulla di buon adattamento non viene rifiutata, suggerendo un buon modello.\nSe invece è maggiore, l’ipotesi viene rifiutata, indicando una necessità di revisione del modello.\n\nQuesto test fornisce un’indicazione quantitativa della bontà di adattamento, utile per valutare se le strutture teoriche catturano adeguatamente le relazioni nei dati.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#test-chi2",
    "href": "chapters/sem/03_gof.html#test-chi2",
    "title": "50  Test del Modello e Indicizzazione",
    "section": "\n50.4 Test \\(\\chi^2\\)\n",
    "text": "50.4 Test \\(\\chi^2\\)\n\nIl test del chi quadrato (\\(\\chi^2\\)) è utilizzato per determinare quanto bene un modello teorico si adatta ai dati osservati. La formula per calcolare la statistica \\(\\chi^2\\) è:\n\\[\n\\chi^2 = N \\times F_{\\text{min}},\n\\]\ndove: - \\(N\\) rappresenta la dimensione del campione. - \\(F_{\\text{min}}\\) è il valore minimo della funzione di discrepanza.\nLa funzione di discrepanza, \\(F\\), è una misura di quanto le covarianze (o le varianze) osservate nei dati differiscano da quelle previste dal modello. Durante il processo di stima dei parametri del modello, questa funzione viene minimizzata. Il valore di \\(F\\) al suo minimo, \\(F_{\\text{min}}\\), rappresenta la discrepanza minima tra i dati osservati e quelli previsti dal modello.\nNell’ambito dell’analisi strutturale di covarianza, il valore di \\(F_{\\text{min}}\\) è tipicamente ottenuto attraverso la stima di massima verosimiglianza (Maximum Likelihood, ML). Tuttavia, ci sono due modi comuni per calcolare \\(\\chi^2\\), che possono variare a seconda del software utilizzato:\n\n\\(\\chi^2 = (N - 1) \\times F_{\\text{min}}\\)\n\\(\\chi^2 = N \\times F_{\\text{min}}\\)\n\nLa scelta tra \\(N\\) e \\(N-1\\) dipende da come il software gestisce la normalizzazione e l’adattamento delle strutture di covarianza.\n\n50.4.1 Interpretazione del Test del \\(\\chi^2\\)\n\n\n\nIpotesi Nulla $ H_0 $: Il modello si adatta bene ai dati. Ciò significa che non c’è una differenza significativa tra le covarianze osservate e quelle previste dal modello.\n\nValore p: Un valore p basso (ad esempio, minore di 0.05) suggerisce che dovremmo rifiutare l’ipotesi nulla, indicando che il modello non si adatta bene ai dati.\n\n50.4.2 Limitazioni\nLa statistica \\(\\chi^2\\) è influenzata dalla dimensione del campione: con campioni ampi, anche lievi discrepanze tra il modello e i dati possono portare a un valore di \\(\\chi^2\\) elevato, risultando in un rifiuto ingiustificato di un modello valido. Inoltre, il test del \\(\\chi^2\\) presenta alcune limitazioni importanti:\n\n\nNon fornisce indicazioni sulla direzione o sulla natura della discrepanza: il test non specifica dove il modello si discosta dai dati o in che modo le discrepanze si manifestano.\n\nEfficacia ridotta in modelli complessi: per modelli con molteplici parametri, o in condizioni in cui le ipotesi fondamentali (ad esempio, la normalità multivariata) non sono soddisfatte, il test del \\(\\chi^2\\) potrebbe non essere affidabile.\n\nPer tali ragioni, è comune integrare il test del \\(\\chi^2\\) con altri indici di adattamento, come l’indice di adattamento comparativo (CFI) e la radice dell’errore quadratico medio di approssimazione (RMSEA), per ottenere una valutazione più accurata e robusta dell’adattamento del modello ai dati.\nIl test del \\(\\chi^2\\) resta dunque un utile strumento di valutazione, ma è fondamentale interpretarlo con cautela, tenendo conto delle dimensioni del campione e di altri fattori che possono influire sul risultato. Nonostante le sue limitazioni, la statistica \\(\\chi^2\\) ha un ruolo importante in contesti specifici, come:\n\n\nConfronto tra modelli nidificati: consente di valutare se aggiunte o modifiche migliorano significativamente l’adattamento del modello.\n\nCalcolo di altri indici di adattamento: come l’indice di Tucker-Lewis (TLI).\n\nRapporto tra \\(\\chi^2\\) e gradi di libertà: un rapporto basso è indicativo di un buon adattamento relativo del modello ai dati.\n\nIn conclusione, pur essendo utile, la statistica \\(\\chi^2\\) va affiancata da altri strumenti di valutazione per una comprensione più completa e bilanciata della bontà di adattamento del modello.\n\n50.4.3 Test di rapporto di verosimiglianza\nIl test del \\(\\chi^2\\) può essere impiegato come un test di rapporto di verosimiglianza per confrontare due modelli nidificati. In questo contesto, “nidificati” significa che uno dei modelli (considerato il modello più semplice o ristretto) è un caso speciale dell’altro (il modello più complesso), con meno parametri liberi da stimare. Questo tipo di test è particolarmente utile per valutare se l’aggiunta di parametri supplementari (rendendo il modello più complesso) migliora significativamente l’adattamento del modello ai dati.\nIl processo di confronto tra i due modelli avviene nel seguente modo:\n\nSi stima il modello più semplice e si calcola il suo valore di \\(\\chi^2\\).\nSi stima il modello più complesso e si calcola il suo valore di \\(\\chi^2\\).\nSi confrontano i due valori di \\(\\chi^2\\) per determinare se l’aggiunta di parametri aggiuntivi giustifica un miglioramento dell’adattamento del modello ai dati, dati i gradi di libertà aggiuntivi.\n\nSe il valore p associato al \\(\\chi^2\\) del modello più complesso è significativamente più basso rispetto a quello del modello più semplice, questo suggerisce che l’aggiunta dei parametri fornisce un miglioramento significativo nell’adattamento del modello. Al contrario, se non vi è un miglioramento significativo, si può concludere che il modello più semplice è preferibile in termini di parsimonia e adattamento.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#chi-quadrato-normalizzato-nc",
    "href": "chapters/sem/03_gof.html#chi-quadrato-normalizzato-nc",
    "title": "50  Test del Modello e Indicizzazione",
    "section": "\n50.5 Chi Quadrato Normalizzato (NC)",
    "text": "50.5 Chi Quadrato Normalizzato (NC)\nIl Chi Quadrato Normalizzato (NC) emerge come un tentativo di attenuare l’effetto della dimensione del campione sulla statistica del chi quadrato del modello (\\(\\chi^2\\)). Questa pratica, adottata da alcuni ricercatori, consiste nel dividere \\(\\chi^2\\) per il numero dei gradi di libertà del modello (dfM), risultando nella formula \\(\\frac{\\chi_{ML}}{dfM}\\). Nonostante l’intento di mitigare l’impatto della dimensione del campione (N), l’impiego di NC presenta limitazioni sostanziali:\n\n\nInfluenza di N sui Modelli Erronei: La statistica \\(\\chi_{ML}\\) è sensibile a N esclusivamente per i modelli non corretti. Questo implica che l’uso di NC per modelli veritieri potrebbe essere fuorviante.\n\nIndipendenza di dfM da N: I gradi di libertà del modello (dfM) non sono correlati con la dimensione del campione, rendendo la divisione di \\(\\chi_{ML}\\) per dfM arbitraria e priva di fondamento statistico.\n\nMancanza di Linee Guida: Non esistono criteri consolidati che definiscano i limiti “accettabili” per il valore di NC. Per esempio, non è chiaro se un valore massimo di NC debba essere inferiore a 2.0, 3.0, o altro.\n\nIn conclusione, data la mancanza di una solida giustificazione statistica o logica, Kline (2023) sconsiglia l’utilizzo del Chi Quadrato Normalizzato come strumento di valutazione della bontà di adattamento del modello.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#chi-quadrato-scalato-e-errori-standard-robusti-per-distribuzioni-non-normali",
    "href": "chapters/sem/03_gof.html#chi-quadrato-scalato-e-errori-standard-robusti-per-distribuzioni-non-normali",
    "title": "50  Test del Modello e Indicizzazione",
    "section": "\n50.6 Chi Quadrato Scalato e Errori Standard Robusti per Distribuzioni Non Normali",
    "text": "50.6 Chi Quadrato Scalato e Errori Standard Robusti per Distribuzioni Non Normali\nNell’ambito dell’analisi di massima verosimiglianza (ML), sia l’approccio ML standard che quello robusto forniscono le stesse stime dei parametri. Tuttavia, il ML robusto differisce nell’introduzione di chi quadrati scalati (\\(\\chi^2\\) scalati) e di errori standard robusti, i quali sono adattati per controbilanciare gli effetti della non normalità dei dati.\n\n50.6.1 Chi Quadrato Scalato di Satorra-Bentler\nUn metodo sviluppato da Satorra e Bentler, che si basa sull’utilizzo di dati completi, calcola il chi quadrato scalato (\\(\\chi_{SB}\\)) applicando un fattore di correzione di scala, indicato con \\(c\\), al valore del chi quadrato non scalato del modello (\\(\\chi_{ML}\\)). Questo fattore di scala \\(c\\) è determinato dalla curtosi multivariata media osservata nei dati grezzi. La formula specifica per il calcolo di \\(\\chi_{SB}\\) è:\n\\[\n\\chi_{SB} = \\frac{\\chi_{ML}}{c}.\n\\]\nQuesta formula evidenzia come il chi quadrato scalato di Satorra-Bentler modifica il chi quadrato tradizionale per tenere conto della curtosi nei dati, fornendo così una misura più affidabile della bontà di adattamento del modello in presenza di distribuzioni non normali.\nLe distribuzioni di \\(\\chi_{SB}\\) tendono ad avvicinarsi alle distribuzioni chi quadrato centrali, ma con una caratteristica fondamentale: le loro medie sono asintoticamente corrette. Questo significa che, su larga scala, \\(\\chi_{SB}\\) fornisce una stima media accurata della discrepanza tra i dati osservati e quelli previsti dal modello, correggendo per eventuali distorsioni causate dalla non normalità dei dati.\n\n50.6.2 Chi Quadrato Scalato di Asparouhov e Muthén\nUn altro tipo di chi quadrato, sviluppato da Asparouhov e Muthén, non si basa sul \\(\\chi_{ML}\\) standard. Invece, nei campioni di grandi dimensioni, il loro chi quadrato scalato corrisponde alla statistica T2* di Yuan e Bentler. Questa versione del chi quadrato è particolarmente adatta per gestire dati non normali o con valori mancanti. I gradi di libertà, sia per \\(\\chi_{SB}\\) che per T2*, sono rappresentati da dfM, indicando la flessibilità del modello in termini di numero di parametri stimabili.\n\n50.6.3 Altri Chi Quadrato Corretti\nAl di là di questi, esistono chi quadrati che sono corretti sia per la media che per la varianza. Questi chi quadrati utilizzano fattori di scala diversi e, in genere, seguono distribuzioni chi quadrato centrali con medie e varianze che sono corrette in modo asintotico. Sebbene questi metodi richiedano maggiori risorse computazionali rispetto ai metodi che correggono solo per la media, tendono ad essere più precisi, specialmente in campioni di grandi dimensioni. Questa precisione aggiuntiva è particolarmente utile quando si affrontano set di dati complessi o di ampie dimensioni, permettendo una stima più accurata della bontà di adattamento del modello.\n\n50.6.4 Metodi Robusti con lavaan\n\nIl pacchetto lavaan offre diverse opzioni per implementare metodi robusti di stima basati sulla massima verosimiglianza (ML). Questi metodi sono particolarmente utili in presenza di deviazioni dalle assunzioni di normalità multivariata. Ecco le principali opzioni disponibili:\n\n\nMLM: Utilizzato per dati completi, calcola un chi-quadrato scalato secondo il metodo di Satorra-Bentler basato sulla media.\n\nMLR: Applicabile sia a dati completi che incompleti, genera un chi-quadrato corretto per la media basato sulla statistica T2* di Yuan-Bentler. È particolarmente indicato per analisi con dati mancanti.\n\nMLMV: Per dati completi, produce un chi-quadrato scalato corretto per la media e la varianza.\n\nMLMVS: Adatto a dati completi, utilizza una correzione per eteroschedasticità basata sul metodo di Satterthwaite, calcolando un chi-quadrato corretto per media e varianza.\n\n\n50.6.4.1 La Matrice di Informazione in lavaan\n\nUn concetto centrale per i metodi ML robusti è la matrice di informazione, utilizzata per stimare gli errori standard dei parametri. Questa matrice rappresenta la varianza e la covarianza dei parametri stimati ed è cruciale per testare ipotesi e costruire intervalli di credibilità o confidenza. In lavaan, la matrice di informazione può essere calcolata in due modi:\n\nMatrice di Informazione Attesa: È l’opzione predefinita per il calcolo degli errori standard. Questa matrice si basa sulle aspettative teoriche delle varianze e covarianze dei parametri stimati, derivate dal modello e dai dati. È generalmente utilizzata in condizioni di dati completi e normali.\nMatrice di Informazione Osservata: Viene impiegata quando sono presenti dati mancanti. In questo caso, le varianze e covarianze vengono calcolate utilizzando i dati effettivamente osservati. Questo approccio può fornire stime degli errori standard più affidabili in presenza di incompletezza nei dati.\n\nGli utenti possono scegliere esplicitamente quale matrice utilizzare, ad esempio forzando l’uso della matrice attesa anche con dati incompleti, in base alle esigenze specifiche della loro analisi.\n\n50.6.4.2 Considerazioni Etiche e Metodologiche\nÈ essenziale utilizzare questi strumenti in modo rigoroso e trasparente. Selezionare metodi o combinazioni di chi-quadrati scalati ed errori standard robusti solo per ottenere risultati che meglio supportano le proprie ipotesi compromette l’integrità della ricerca. Per questo motivo, i ricercatori dovrebbero:\n\nDichiarare chiaramente i metodi utilizzati, incluso il tipo di matrice di informazione scelta.\nSegnalare eventuali variazioni nei risultati legate alla scelta del metodo.\nGarantire che le analisi siano guidate da principi metodologici, non da convenienze interpretative.\n\nQueste buone pratiche sono fondamentali per mantenere l’affidabilità e la credibilità delle analisi svolte.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#indicizzazione-delladattamento-del-modello",
    "href": "chapters/sem/03_gof.html#indicizzazione-delladattamento-del-modello",
    "title": "50  Test del Modello e Indicizzazione",
    "section": "\n50.7 Indicizzazione dell’Adattamento del Modello",
    "text": "50.7 Indicizzazione dell’Adattamento del Modello\nL’indicizzazione dell’adattamento del modello si basa sull’uso di indici di adattamento approssimati, i quali si differenziano dai test di significatività tradizionali. Invece di fornire una decisione dicotomica, come il rifiuto o l’accettazione di un’ipotesi nulla, questi indici offrono una misura continua di quanto bene un modello si adatta ai dati osservati. Non essendoci una separazione netta tra i limiti dell’errore di campionamento, gli indici di adattamento forniscono una valutazione più sfumata e graduale della bontà di adattamento.\nQuesti indici possono essere classificati in due categorie principali:\n\n\nStatistiche di Cattivo Adattamento: In questa categoria, valori più elevati indicano un peggior adattamento del modello ai dati. Un esempio tipico di questa categoria è il chi quadrato del modello, dove valori più alti suggeriscono una maggiore discrepanza tra il modello e i dati.\n\nStatistiche di Buon Adattamento: Al contrario, per gli indici in questa categoria, valori più alti segnalano un migliore adattamento del modello ai dati. Molti di questi indici sono normalizzati in modo che il loro intervallo varii da 0 a 1.0, dove 1.0 rappresenta l’adattamento ottimale del modello.\n\nA differenza del test del chi quadrato, che si basa su un framework teorico ben definito, l’interpretazione e l’applicazione degli indici di adattamento approssimati non sono guidate da un unico insieme di principi teorici consolidati. Questa situazione fa sì che la valutazione dell’adattamento del modello si allinei maggiormente a ciò che Little (2013) ha descritto come “scuola di modellazione”. Questo approccio contempla l’analisi di modelli statistici complessi in un contesto in cui le regole decisionali sono meno rigide e più soggette a interpretazione.\nLa natura flessibile di questo approccio rispecchia la varietà e la complessità dei modelli statistici, che devono essere personalizzati per rispondere a specifiche domande di ricerca. Questa flessibilità, tuttavia, porta con sé una certa ambiguità nelle regole di valutazione dei modelli statistici. Pur offrendo la possibilità di adattare l’analisi alle particolarità di ogni studio, questa mancanza di rigore teorico uniforme può talvolta non tradursi in pratiche ottimali di modellazione.\nLa questione filosofica relativa all’adattamento esatto dei modelli statistici solleva dubbi sull’idea di perfezione come standard per questi modelli. In effetti, è ampiamente riconosciuto che tutti i modelli statistici sono in qualche misura imperfetti; sono piuttosto strumenti di approssimazione che aiutano i ricercatori a organizzare e interpretare le loro osservazioni sui fenomeni di interesse. Un modello troppo semplificato, che non cattura la complessità del fenomeno, può essere inadeguato e quindi rifiutato. Allo stesso tempo, un modello eccessivamente complesso, che cerca di replicare fedelmente il fenomeno, può risultare di scarsa utilità scientifica a causa della sua complessità eccessiva.\nGeorge Box, nel suo influente lavoro del 1976, avanzò l’idea che nessun modello statistico potesse essere considerato perfettamente “corretto”. Questa visione nasce dalla consapevolezza che tutti i modelli hanno una certa dose di imperfezione intrinseca. Box suggeriva che lo scopo principale di uno scienziato dovrebbe essere la ricerca di una “descrizione economica” dei fenomeni naturali, cercando cioè di formulare modelli che siano semplici, ma al contempo efficaci, nella rappresentazione della realtà. Egli criticava la tendenza a sovraelaborare o sovraparametrizzare i modelli, considerandola un segno di mediocrità nella pratica scientifica. Box enfatizzava l’importanza di concentrarsi sugli errori sostantivi, o “tigri”, piuttosto che su piccole imperfezioni, o “topi”, affermando:\n\n“Since all models are wrong the scientist must be alert to what is importantly wrong. It is inappropriate to be concerned about mice when there are tigers abroad.”\n\nCiò implica che l’obiettivo nella modellazione statistica non dovrebbe essere una perfezione irraggiungibile, ma piuttosto lo sviluppo di modelli che, pur nella loro semplicità, riescano a cogliere gli aspetti fondamentali dei fenomeni analizzati. Questo richiede un equilibrio tra la complessità necessaria per una descrizione accurata e la semplicità che rende un modello pratico e interpretabile.\nHayduk (2014), nel commentare l’affermazione di Box, si focalizza specificatamente sul contesto della modellizzazione SEM (Structural Equation Modeling). Egli identifica le “tigri”, ovvero gli errori gravi nei modelli, come indicatori di una specificazione errata del modello. Hayduk sottolinea l’importanza critica di riconoscere e correggere gli errori significativi piuttosto che disperdere energie su dettagli minori. In sostanza, Hayduk rafforza l’idea che è essenziale distinguere tra errori minori e maggiori, questi ultimi potendo compromettere seriamente la validità e l’utilità di un modello statistico.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#tipologie-di-indici-di-adattamento-approssimati",
    "href": "chapters/sem/03_gof.html#tipologie-di-indici-di-adattamento-approssimati",
    "title": "50  Test del Modello e Indicizzazione",
    "section": "\n50.8 Tipologie di Indici di Adattamento Approssimati",
    "text": "50.8 Tipologie di Indici di Adattamento Approssimati\nGli indici di adattamento approssimati possono essere classificati in diverse categorie, che riflettono diversi aspetti della bontà di adattamento di un modello statistico ai dati. Sebbene questa classificazione non sia esaustiva né le categorie siano mutualmente esclusive, i tipi principali di indici di adattamento sono i seguenti:\n\nIndici di Adattamento Assoluto: Questi indici, come il GFI (Goodness of Fit Index), misurano quanto bene un modello spiega i dati senza riferimento ad altri modelli. Indicano l’abilità del modello di riprodurre i dati osservati.\nIndici di Adattamento Parsimonioso: Questi indici confrontano i gradi di libertà del modello (dfM) con il numero massimo possibile di gradi di libertà disponibili nei dati. Un esempio è l’AGFI (Adjusted Goodness of Fit Index), che incorpora una penalità per la complessità del modello, benché non sia un indice di adattamento parsimonioso come definito in questa categoria.\nIndici di Adattamento Incrementale (Relativo o Comparativo): Questi indici confrontano l’adattamento del modello del ricercatore con quello di un modello di base, tipicamente un modello di indipendenza che assume covarianze nulle tra le variabili osservate. È possibile scegliere un modello di base diverso, sebbene il calcolo manuale dell’indice possa essere necessario se il modello di base desiderato differisce da quello predefinito nel software.\nIndici di Adattamento Non Centrale: Stimano il grado in cui l’ipotesi di adattamento esatto è falsa, dati il modello e i dati. Questi indici approssimano parametri nelle distribuzioni chi quadrato non centrali, che descrivono anche le distribuzioni campionarie per gli indici di adattamento di questo tipo.\nIndici di Adattamento Predittivo (o basati sulla Teoria dell’Informazione): Derivati dalla teoria dell’informazione, stimano l’adattamento del modello in campioni di replica ipotetici della stessa dimensione, estratti casualmente dalla stessa popolazione del campione originale. Sono utilizzati principalmente per confrontare modelli alternativi basati sulle stesse variabili e adattati agli stessi dati, ma dove i modelli non sono gerarchicamente correlati.\n\nNon tutti gli indici di adattamento approssimati hanno resistito alla prova del tempo. Ad esempio, gli indici di adattamento parsimonioso non hanno mai raggiunto una popolarità significativa tra i ricercatori applicati, restando relativamente oscuri. Altri indici, come il GFI e l’AGFI, sono stati criticati per la loro sensibilità alla dimensione del campione e al numero di indicatori nei modelli di analisi fattoriale.\nI software moderni per la Structural Equation Modeling (SEM) presentano una notevole varietà nel numero di indici di adattamento approssimati forniti nei loro output. Programmi come Amos e LISREL elencano un numero elevato di indici (oltre 12), mentre altri come lavaan e Mplus ne includono un numero più limitato (circa 4-5). Questa abbondanza di indici può portare al rischio di “cherry-picking”, cioè la tendenza a selezionare e riportare solo quegli indici che mostrano risultati favorevoli al modello proposto dal ricercatore. Per mitigare questo rischio, è consigliabile limitarsi a un insieme essenziale di indici e prestare attenzione all’analisi dei residui.\n\n50.8.1 Modello Baseline\nIl modello baseline, noto anche come modello nullo, è un modello in cui tutte le covarianze sono impostate a zero, mentre le varianze sono stimate liberamente. In questo modello, non si stimano i carichi fattoriali; ci si limita invece a stimare le medie e le varianze osservate, eliminando tutte le covarianze tra le variabili.\nÈ utile pensare al modello nullo o baseline come il peggior modello possibile, da confrontare poi con il modello saturato, che rappresenta invece la migliore approssimazione ai dati. Teoricamente, il modello baseline è fondamentale per comprendere come vengono calcolati altri indici di adattamento del modello, in quanto fornisce un punto di riferimento iniziale per la valutazione della bontà di adattamento in un contesto di Modelli di Equazioni Strutturali.\n\n50.8.2 Set di Indici di Adattamento Consigliati\nKline (2023) suggerisce un insieme essenziale di soli tre indici di adattamento approssimati, che sono ampiamente utilizzati nei software SEM e frequentemente presenti negli studi pubblicati. Questi indici sono stati selezionati per le seguenti ragioni:\n\nAmpia Presenza nella Letteratura: Sono ampiamente riportati in numerosi studi SEM, rendendoli familiari sia ai ricercatori che ai revisori.\nStandardizzazione: Le scale di questi indici non dipendono dalle variabili osservate o latenti, fornendo così una misura standardizzata di adattamento.\nValidità Statistica Estesa: Almeno uno di questi indici, l’RMSEA, possiede un solido fondamento statistico e un quadro interpretativo più ampio per la stima degli intervalli, i test delle ipotesi e la pianificazione della dimensione del campione.\n\nNonostante la loro utilità, è fondamentale usare questi indici con attenzione. I ricercatori dovrebbero evitare l’uso acritico di soglie o punti di taglio, sia fissi sia variabili, che si suppone differenzino tra modelli con un buon o cattivo adattamento. L’applicazione di queste soglie può essere problematica, poiché non sono valide universalmente per tutti i tipi di modelli e set di dati. L’uso improprio di tali soglie può portare a decisioni errate, in particolare se si trascura l’analisi dei residui.\nIl gruppo principale di tre indici di adattamento approssimati raccomandato comprende:\n\nRoot Mean Square Error of Approximation (RMSEA) di Steiger-Lind (Steiger, 1990), accompagnato dal suo intervallo di confidenza al 90%. L’RMSEA valuta l’adattamento assoluto del modello, penalizzando la complessità del modello, ma non è un indice di adattamento parsimonioso. È un indice di cattivo adattamento dove il valore zero rappresenta l’adattamento ideale, senza un limite massimo teorico.\nComparative Fit Index (CFI) di Bentler (Bentler, 1990). Il CFI è un indice di adattamento incrementale e valuta la bontà di adattamento relativa del modello rispetto a un modello di base. Si estende su una scala da 0 a 1.0, dove 1.0 indica l’adattamento ottimale, e non impone penalità per la complessità del modello.\nStandardized Root Mean Square Residual (SRMR) (Jöreskog & Sörbom; 1981). L’SRMR è un indice di adattamento assoluto che misura la discrepanza tra le correlazioni osservate e quelle previste dal modello. Un valore di zero indica un adattamento perfetto.\n\nSia l’RMSEA sia il CFI incorporano il chi quadrato del modello e i suoi gradi di libertà nelle loro formule. Questo implica che condividono le stesse assunzioni distributive della corrispondente statistica di test. Se tali assunzioni non sono valide, i valori degli indici e della statistica di test (incluso il valore p) potrebbero non essere accurati. Entrambi gli indici sono stati inizialmente definiti per dati continui con distribuzioni normali analizzati tramite ML standard. Tuttavia, in presenza di dati significativamente non normali, i valori di chiML, RMSEA e CFI possono risultare distorti. Alcuni software SEM implementano correzioni ad hoc per la non normalità.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#misure-di-adeguamento-per-il-confronto",
    "href": "chapters/sem/03_gof.html#misure-di-adeguamento-per-il-confronto",
    "title": "50  Test del Modello e Indicizzazione",
    "section": "\n50.9 Misure di adeguamento per il confronto",
    "text": "50.9 Misure di adeguamento per il confronto\n\n50.9.1 CFI\nGli indici di adattamento comparativo [detti anche indici di adattamento incrementale; ad es. Hu & Bentler (1998)] valutano l’adattamento di una soluzione specificata dall’utente in relazione a un modello di base nidificato più ristretto. Tipicamente, il modello base è un modello “nullo” o “di indipendenza” in cui le covarianze tra tutti gli indicatori di input sono fissate a zero, ma nessun vincolo viene posto sulle varianze degli indicatori. Uno di questi indici, l’indice di adattamento comparativo (comparative fit index, CFI; Bentler, 1990). Il CFI si basa su un confronto relativo, situando il modello di interesse lungo un continuum che va dal modello peggiore (nullo) al modello perfetto (saturo).\nIl CFI valuta la riduzione relativa del parametro di non-centralità (\\(\\lambda\\)) tra il modello di interesse e il modello di riferimento (Bentler, 1990). Il parametro di non-centralità \\(\\lambda_m\\) rappresenta il grado di errore di specificazione del modello \\(m\\) ed è calcolato come:\n\\[\n\\lambda_m = \\chi^2_m - \\text{df}_m,\n\\]\ndove \\(\\chi^2_m\\) è il valore chi-quadro stimato per il modello e \\(\\text{df}_m\\) rappresenta i gradi di libertà. Più alto è \\(\\lambda_m\\), maggiore è la discrepanza tra il modello e i dati osservati. Il valore del CFI si basa sul rapporto tra i parametri di non-centralità del modello di interesse (\\(\\lambda_m\\)) e del modello nullo (\\(\\lambda_b\\)):\n\\[\nCFI(m, b) = 1 - \\frac{\\lambda_m}{\\lambda_b} = 1 - \\frac{\\chi^2_m - \\text{df}_m}{\\chi^2_b - \\text{df}_b}.\n\\]\nIl valore del CFI varia generalmente tra 0 e 1 (anche se in casi particolari può superare 1 o essere negativo), dove un valore vicino a 1 indica un buon adattamento del modello rispetto al modello nullo.\n\n50.9.1.1 Modello nullo come baseline\nIl modello nullo è un modello in cui tutte le variabili osservate sono considerate non correlate. Il CFI misura quindi quanto il modello di interesse riesce a migliorare l’adattamento rispetto a questo modello di riferimento, in modo analogo al concetto di \\(R^2\\) per la regressione lineare.\n\n50.9.1.2 Sensibilità ai dati e alle caratteristiche del modello\nIl comportamento del CFI è influenzato da tre fattori principali:\n\n\nDimensione del campione (\\(n\\)): Campioni più grandi aumentano il parametro di non-centralità del modello nullo (\\(\\lambda_b\\)), migliorando la capacità del CFI di distinguere tra modelli.\n\nNumero di variabili osservate (\\(p\\)): Un numero elevato di variabili può complicare l’interpretazione del CFI, poiché aumenta i gradi di libertà del modello nullo, riducendo la non-centralità \\(\\lambda_b\\).\n\nCorrelazione tra variabili (\\(R\\)): Maggiore è la correlazione tra le variabili, più il modello nullo differisce dai dati, e più il CFI può differenziare tra modelli.\n\n50.9.1.3 Regole empiriche\nValori del CFI superiori a 0.90 erano considerati accettabili in passato (Bentler & Bonett, 1980), mentre valori superiori a 0.95 sono oggi considerati indicativi di un buon adattamento (Hu & Bentler, 1999). Tuttavia, studi di simulazione più recenti, come quelli di Fan e Sivo nel 2005 e di Yuan nel 2005, hanno messo in dubbio l’universalità di un valore soglia specifico per il CFI, evidenziando che l’adeguatezza di tale valore può variare a seconda delle caratteristiche dei modelli e del grado di non normalità nei dati. Di conseguenza, è importante non applicare queste regole in modo meccanico, ma valutare il contesto specifico. Inoltre, Brosseau-Liard e Savalei (2014) hanno descritto delle versioni robuste del CFI adatte per dati non normali. Queste versioni del CFI sono calcolate e fornite dal software lavaan quando si utilizzano metodi di stima Maximum Likelihood (ML) robusti. Questo implica che, quando si lavora con dati che presentano deviazioni dalla normalità, queste versioni robuste del CFI possono offrire una misura più affidabile dell’adattamento del modello.\n\n50.9.1.4 Variabilità campionaria\nA livello di popolazione, un modello corretto dovrebbe avere un valore di CFI pari a 1. Tuttavia, la variabilità campionaria può influenzare il parametro di non-centralità \\(\\lambda_m\\) e \\(\\lambda_b\\), causando deviazioni rispetto alle aspettative teoriche. Questo fenomeno è particolarmente rilevante nei campioni piccoli o in presenza di modelli complessi.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#misure-di-adeguamento-parsimonioso",
    "href": "chapters/sem/03_gof.html#misure-di-adeguamento-parsimonioso",
    "title": "50  Test del Modello e Indicizzazione",
    "section": "\n50.10 Misure di adeguamento parsimonioso",
    "text": "50.10 Misure di adeguamento parsimonioso\n\n50.10.1 TLI\nUn indice che rientra nella degli indici di adeguamento parsimonioso è l’indice Tucker-Lewis (Tucker–Lewis index, TLI, anche chiamato indice di adattamento non normato). Il TLI si pone il problema di penalizzare la complessità del modello, ovvero include una funzione di penalizzazione per l’addizione di parametri che non migliorano in maniera sostanziale l’adattamento del modello. Il TLI è calcolato con la seguente formula:\n\\[\n\\begin{equation}\nTLI = \\frac{(\\chi^2_B / dof_B)–(\\chi^2_T / dof_T)}{(\\chi^2_B / dof_B) – 1},\n\\end{equation}\n\\]\ndove \\(\\chi^2_T\\) è il valore \\(\\chi^2\\) del modello target, \\(dof_T\\) sono i gradi di libertà del modello target, \\(\\chi^2_B\\) è il valore \\(\\chi^2\\) del modello baseline e \\(dof_B\\) sono i gradi di libertà del modello base.\nL’Indice di Tucker-Lewis (TLI) può, in teoria, assumere valori inferiori a zero se il modello di base, ovvero un modello diverso da quello studiato dal ricercatore, mostra un ottimo adattamento ai dati. Tuttavia, questa eventualità è rara nella pratica. Al contrario, il TLI può superare il valore di 1.0 se il modello analizzato dal ricercatore si adatta in modo particolarmente stretto ai dati. Marsh e Balla (1994) hanno evidenziato che la dimensione del campione influenza poco i valori del TLI.\nSecondo quanto osservato da Kenny (2020), si possono trarre due conclusioni importanti:\n\nIl Comparative Fit Index (CFI) e il TLI sono entrambi influenzati dall’entità delle correlazioni tra le variabili misurate. Ciò significa che valori medi di correlazione più elevati risultano in valori più alti sia per il CFI che per il TLI, e il contrario è vero per correlazioni medie più basse.\nI valori del CFI e del TLI mostrano una forte correlazione tra loro. Di conseguenza, è consigliabile riportare solo uno dei due indici per evitare ripetizioni e per mantenere la chiarezza del report. La scelta tra CFI e TLI dovrebbe basarsi su criteri specifici relativi al contesto e agli obiettivi dello studio in questione.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#misure-di-adeguamento-assoluto",
    "href": "chapters/sem/03_gof.html#misure-di-adeguamento-assoluto",
    "title": "50  Test del Modello e Indicizzazione",
    "section": "\n50.11 Misure di Adeguamento Assoluto",
    "text": "50.11 Misure di Adeguamento Assoluto\n\n50.11.1 Root Mean Square Error of Approximation (RMSEA)\nL’Errore Quadratico Medio di Approssimazione (RMSEA) misura quanto bene un modello statistico si adatta ai dati osservati, valutando l’adattamento in termini assoluti, piuttosto che confrontarlo con un modello di riferimento (come fanno indici quali CFI e TLI).\nIl calcolo del RMSEA si basa sul chi-quadrato (\\(\\chi^2\\)), che rappresenta la discrepanza tra la matrice di covarianza osservata e quella stimata dal modello. La discrepanza, indicata con \\(\\delta\\), è definita come:\n\\[\n\\delta = \\chi^2 - df,\n\\]\ndove \\(df\\) rappresenta i gradi di libertà del modello. Valori più alti di \\(\\delta\\) indicano una maggiore discrepanza, ossia un peggior adattamento del modello ai dati.\nLa formula generale per il calcolo del RMSEA è:\n\\[\n\\text{RMSEA} = \\sqrt{\\frac{\\max(0, \\delta)}{df \\cdot (n - 1)}},\n\\]\ndove \\(n\\) è il numero di osservazioni nel campione. Questo indice riflette l’errore di approssimazione del modello rispetto alla matrice di covarianza della popolazione, tenendo conto della parsimonia del modello (ossia del numero di gradi di libertà).\n\n50.11.2 Distribuzione del Chi-Quadrato e Non Centralità\nAssumendo che:\n\ni dati seguano una distribuzione normale multivariata;\nil modello sia corretto;\nil campione sia grande e casuale;\n\nil chi-quadrato del modello (\\(\\chi^2_{\\text{ML}}\\)) segue una distribuzione \\(\\chi^2\\) con \\(df_M\\) gradi di libertà. Tuttavia, se il modello non è corretto, il chi-quadrato segue una distribuzione non centrale \\(\\chi^2(df_M, \\lambda)\\), dove \\(\\lambda\\) rappresenta il parametro di non centralità, che indica il grado di discrepanza tra il modello e i dati.\nIl parametro di non centralità normalizzato è definito come:\n\\[\n\\delta_{\\text{norm}} = \\max(0, \\chi^2_{\\text{ML}} - df_M).\n\\]\nQuesto valore è utilizzato per stimare la discrepanza tra la matrice di covarianza osservata e quella della popolazione sotto il modello.\n\n50.11.3 Formula Finale del RMSEA\nIl valore finale del RMSEA, indicato spesso con \\(\\epsilon\\), è calcolato come:\n\\[\n\\epsilon = \\sqrt{\\frac{\\delta_{\\text{norm}}}{df_M \\cdot (n - 1)}}.\n\\]\nSebbene \\(\\epsilon\\) possa essere una stima distorta a causa della restrizione \\(\\epsilon \\geq 0\\), rappresenta una buona approssimazione dell’errore.\n\n50.11.4 Soglie Interpretative\nBrowne e Cudeck (1993) suggerirono che:\n\n\n\\(\\epsilon \\leq 0.05\\) indica un buon adattamento del modello;\n\n\\(0.05 &lt; \\epsilon \\leq 0.08\\) rappresenta un adattamento accettabile;\n\n\\(\\epsilon &gt; 0.10\\) segnala un cattivo adattamento.\n\nTuttavia, queste soglie non sono universali, e si consiglia di valutare anche il limite superiore dell’intervallo di confidenza di \\(\\epsilon\\) (indicato come \\(\\epsilon_U\\)) per un’interpretazione più accurata.\n\n50.11.5 Considerazioni e Versioni Robuste\n\nInterpretazione: L’interpretazione di \\(\\epsilon\\), \\(\\epsilon_L\\) (limite inferiore) e \\(\\epsilon_U\\) (limite superiore) è appropriata in campioni ampi e con modelli ben specificati. In campioni piccoli o con errori di specificazione significativi, è necessaria maggiore cautela.\nPenalità per Modelli Complessi: Studi di simulazione indicano che il RMSEA tende a penalizzare maggiormente i modelli con pochi gradi di libertà (ad esempio, modelli con poche variabili).\nVersioni Robuste: Versioni robuste del RMSEA, come quella basata sul chi-quadrato scalato di Satorra-Bentler, correggono gli effetti della non normalità e tendono a essere più accurate rispetto alla versione standard, che può sovrastimare l’indice in condizioni di non normalità.\nPotenza Statistica: Esistono metodi per calcolare la potenza statistica associata a ipotesi nulle basate sul RMSEA e per stimare la dimensione minima del campione necessaria a raggiungere determinati livelli di potenza.\n\nConcludendo, il RMSEA è uno strumento potente per valutare l’adattamento assoluto di un modello, ma il suo utilizzo richiede attenzione alle specifiche del modello, alla qualità dei dati e al contesto dell’analisi.\n\n50.11.6 Root Mean Square Residual (RMRS)\nA differenza del chi quadrato del modello e dei gradi di libertà, che valutano la bontà di adattamento di un modello in base a criteri di adattamento globale, l’indice RMRS (Root Mean Square Residual) si concentra esclusivamente sui residui del modello, ovvero le discrepanze tra le correlazioni osservate e quelle previste dal modello.\nLa formula per calcolare l’RMRS è la seguente:\n\\[\nRMRS = \\sqrt{ \\frac{2 \\sum_i\\sum_j(r_{ij} - \\hat{r}_{ij})^2}{p(p+1)}},\n\\]\ndove:\n\n\n\\(p\\) rappresenta il numero di item (variabili) nel modello,\n\n\\(r_{ij}\\) è la correlazione osservata tra le variabili \\(i\\) e \\(j\\),\n\n\\(\\hat{r}_{ij}\\) è la correlazione prevista dal modello tra le variabili \\(i\\) e \\(j\\).\n\nUn valore di RMRS pari a 0 indica un adattamento perfetto del modello, mentre valori crescenti indicano un adattamento meno preciso. In generale, un valore di SRMR inferiore a 0.08 è considerato favorevole (Hu e Bentler, 1999).\nTuttavia, è importante notare che il SRMR è una misura media e può nascondere variazioni significative tra i residui di correlazione individuali. Ad esempio, se il SRMR è 0.03, potrebbe sembrare un buon adattamento. Ma se i residui di correlazione variano da -0.12 a 0.18, con alcuni residui superiori a 0.10, potrebbe indicare problemi di adattamento locali più gravi.\nPertanto, quando si riportano i risultati in un report, per ottenere una comprensione più completa dell’adattamento del modello è consigliabile descrivere i residui di correlazione o, meglio ancora, presentare l’intera matrice dei residui, anziché basarsi esclusivamente su un valore medio come il SRMR.\n\n50.11.7 Interpretazione con lavaan\n\nL’interpretazione degli indici di bontà di adattamento trovati nella CFA o nella modellazione di equazioni strutturali può essere ottenuta usando le funzioni del pacchetto effectsize.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#adattamento-locale",
    "href": "chapters/sem/03_gof.html#adattamento-locale",
    "title": "50  Test del Modello e Indicizzazione",
    "section": "\n50.12 Adattamento Locale",
    "text": "50.12 Adattamento Locale\nI modelli SEM possono teoricamente superare i test di adattamento globale ma fallire nei test di adattamento locale. Questi dettagli, relativi all’adattamento del modello, sono esaminati direttamente nei test di adattamento locale. L’analisi dei residui (sia standardizzati che di correlazione) è quindi cruciale per una valutazione completa del modello (Maydeu-Olivares e Shi, 2017). Le recenti norme di reportistica per il SEM richiedono agli autori di descrivere sia l’adattamento globale che quello locale (Appelbaum et al., 2018); Greiff e Heene, 2017; Vernon e Eysenck, 2007).\n\n50.12.1 Residui di Covarianza, Residui Standardizzati, Residui Normalizzati\n\nResidui di Covarianza: Sono le differenze tra le covarianze osservate e quelle previste dal modello. Questi residui possono essere difficili da interpretare perché non sono standardizzati, ovvero la loro metrica dipende dalle scale delle variabili coinvolte. Pertanto, residui di covarianza per coppie di variabili diverse non sono direttamente confrontabili a meno che tutte le variabili non siano sulla stessa metrica.\nResidui Standardizzati: Sono versioni standardizzate dei residui di covarianza, interpretati come un test z in campioni grandi. Un residuo standardizzato significativamente diverso da zero indica una discrepanza tra modello e dati. Tuttavia, la significatività di questi residui può dipendere dalla dimensione del campione, con residui vicini allo zero che possono essere significativi in campioni grandi, mentre residui relativamente grandi potrebbero non essere significativi in campioni piccoli.\nResidui Normalizzati: Sono i rapporti tra i residui di covarianza e l’errore standard della covarianza campionaria. Sono generalmente più conservativi dei residui standardizzati in termini di test di significatività. In modelli complessi, quando non è possibile calcolare il denominatore di un residuo standardizzato, il residuo normalizzato fornisce un’alternativa più conservativa.\n\nNel software lavaan ci sono due opzioni principali per calcolare i residui di correlazione:\n\n\nOpzione cor.bollen: Questa specifica indica al computer di convertire separatamente le matrici di covarianza del campione e quelle implicite dal modello in matrici di correlazione prima di calcolare i residui. Questo processo comporta la standardizzazione di ciascuna matrice in base alle varianze (deviazioni standard quadrate) presenti nella diagonale principale di ciascuna matrice. Le varianze nella matrice di covarianza del campione sono osservate direttamente, mentre le varianze per le variabili endogene nella matrice di covarianza implicata dal modello sono previste dal modello e possono differire dalle varianze osservate corrispondenti.\n\nOpzione cor.bentler: Questa opzione standardizza sia la matrice di covarianza del campione che quella implicata dal modello basandosi sulle varianze presenti solo nella matrice di covarianza del campione. Poiché non tutti gli elementi della diagonale principale nella matrice di covarianza implicata dal modello sono varianze osservate, alcuni valori dei residui di correlazione del tipo Bentler potrebbero non essere pari a zero. Tuttavia, i valori dei residui fuori diagonale per entrambi i metodi sono generalmente simili.\n\nPer impostazione predefinita, lavaan utilizza il metodo cor.bollen per calcolare i residui di correlazione nelle sue analisi.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#esempio-empirico",
    "href": "chapters/sem/03_gof.html#esempio-empirico",
    "title": "50  Test del Modello e Indicizzazione",
    "section": "\n50.13 Esempio Empirico",
    "text": "50.13 Esempio Empirico\nNel capitolo precedente abbiamo formulato un modello SEM nel quale abbiamo definito una variabile latente con le sei sottoscale della Self-Compassion Scale e una seconda variabile latente con le tre sottoscale della DASS-21. Abbiamo ipotizzato che il fattore dell’autocompassione eserciti un effetto (protettivo) nei confronti del disagio psicologico misurato dal fattore definito dalle sottoscale della DASS-21.\nImportiamo i dati in R.\n\nd_sc &lt;- read.csv(\"../../data/dass_rosenberg_scs.csv\", header = TRUE)\n\nDefiniamo il modello SEM.\n\nmod_sc &lt;- \"\n  F =~ anxiety + depression + stress\n  SC =~ self_kindness   + common_humanity   + mindfulness   + \n        self_judgment   + isolation + over_identification\n  F ~ SC\n\"\n\nAdattiamo il modello.\n\nfit_sc &lt;- lavaan::sem(mod_sc, d_sc)\n\nCreiamo un diagramma di percorso.\n\nsemPlot::semPaths(fit_sc,\n    what = \"col\", whatLabels = \"std\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\nI coefficienti stimati nel modello SEM appaiono coerenti e in linea con le aspettative, in particolare il coefficiente che descrive l’effetto “causale” del fattore dell’autocompassione sul disagio emotivo, che si attesta a -0.48. Questo valore negativo corrobora l’ipotesi secondo cui l’autocompassione svolge un ruolo di fattore protettivo contro il disagio emotivo. Tuttavia, prima di confermare definitivamente questa conclusione, è cruciale esaminare gli indici di bontà di adattamento del modello. Questi indici ci permetteranno di valutare quanto accuratamente il modello SEM si adatta ai dati osservati, fornendo un quadro più chiaro della validità delle nostre inferenze. In altre parole, sebbene il modello suggerisca una relazione negativa tra autocompassione e disagio emotivo, la conferma finale di questa associazione dipenderà dall’adeguatezza complessiva del modello rispetto ai dati.\nCalcoliamo gli indici di bontà di adattamento.\n\nfitMeasures(fit_sc) \n#&gt;                  npar                  fmin                 chisq \n#&gt;              1.90e+01              4.27e-01              4.49e+02 \n#&gt;                    df                pvalue        baseline.chisq \n#&gt;              2.60e+01              0.00e+00              3.13e+03 \n#&gt;           baseline.df       baseline.pvalue                   cfi \n#&gt;              3.60e+01              0.00e+00              8.63e-01 \n#&gt;                   tli                  nnfi                   rfi \n#&gt;              8.11e-01              8.11e-01              8.01e-01 \n#&gt;                   nfi                  pnfi                   ifi \n#&gt;              8.56e-01              6.19e-01              8.64e-01 \n#&gt;                   rni                  logl     unrestricted.logl \n#&gt;              8.63e-01             -1.23e+04             -1.21e+04 \n#&gt;                   aic                   bic                ntotal \n#&gt;              2.47e+04              2.47e+04              5.26e+02 \n#&gt;                  bic2                 rmsea        rmsea.ci.lower \n#&gt;              2.47e+04              1.76e-01              1.62e-01 \n#&gt;        rmsea.ci.upper        rmsea.ci.level          rmsea.pvalue \n#&gt;              1.90e-01              9.00e-01              0.00e+00 \n#&gt;        rmsea.close.h0 rmsea.notclose.pvalue     rmsea.notclose.h0 \n#&gt;              5.00e-02              1.00e+00              8.00e-02 \n#&gt;                   rmr            rmr_nomean                  srmr \n#&gt;              1.20e+00              1.20e+00              7.10e-02 \n#&gt;          srmr_bentler   srmr_bentler_nomean                  crmr \n#&gt;              7.10e-02              7.10e-02              7.90e-02 \n#&gt;           crmr_nomean            srmr_mplus     srmr_mplus_nomean \n#&gt;              7.90e-02              7.10e-02              7.10e-02 \n#&gt;                 cn_05                 cn_01                   gfi \n#&gt;              4.65e+01              5.45e+01              8.46e-01 \n#&gt;                  agfi                  pgfi                   mfi \n#&gt;              7.33e-01              4.89e-01              6.69e-01 \n#&gt;                  ecvi \n#&gt;              9.26e-01\n\nL’analisi degli indici di bontà di adattamento rivela alcune preoccupazioni significative riguardo alla validità del nostro modello SEM. Il rapporto \\(\\chi^2 / df\\) emerge come eccessivamente elevato, segnalando una possibile mancanza di adattamento:\n\n449.141 / 26\n#&gt; [1] 17.3\n\nAnalogamente, i valori di CFI e TLI sono inferiori al livello desiderato, suggerendo che il modello non rappresenta adeguatamente la struttura dei dati. In aggiunta, gli indici RMSEA e SRMR superano le soglie accettabili, indicando ulteriormente un’inadeguata aderenza del modello ai dati.\nDi fronte a questi risultati, è imprudente accettare la conclusione precedentemente formulata secondo cui l’autocompassione agisce come un fattore protettivo contro il disagio emotivo. Questa interpretazione, benché teoricamente fondata, non trova un solido supporto empirico nel contesto del modello attuale.\nIn questa situazione, un percorso costruttivo potrebbe essere quello di rivedere e potenzialmente modificare il modello. L’obiettivo sarebbe quello di esplorare alternative che potrebbero risultare in un migliore adattamento ai dati, mantenendo al contempo l’adeguatezza teorica. Ciò potrebbe includere la revisione delle assunzioni del modello, la riconsiderazione delle variabili incluse o la ristrutturazione delle relazioni ipotizzate tra di esse. Solo attraverso un modello che dimostra una bontà di adattamento adeguata possiamo affermare con maggiore sicurezza che i dati empirici sostengono l’ipotesi dell’effetto protettivo dell’autocompassione sul disagio emotivo.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#potere-statistico-e-precisione",
    "href": "chapters/sem/03_gof.html#potere-statistico-e-precisione",
    "title": "50  Test del Modello e Indicizzazione",
    "section": "\n50.14 Potere Statistico e Precisione",
    "text": "50.14 Potere Statistico e Precisione\nNell’ambito dei modelli di Structural Equation Modeling (SEM), l’analisi della potenza statistica è fondamentale per garantire l’affidabilità e la validità dei risultati. Esistono due approcci principali per quest’analisi: la potenza a priori (prospettica) e la potenza retrospettiva (post hoc, osservata).\n\nPotenza a priori (Prospettica): Questa analisi viene effettuata prima della raccolta dei dati e mira a stimare la probabilità che uno studio identifichi un effetto significativo, se presente nella popolazione. È cruciale nella pianificazione della ricerca per determinare la dimensione del campione necessaria, aumentando così l’efficienza dello studio e prevenendo l’uso di campioni eccessivamente grandi o inadeguati. In SEM, la potenza a priori si stima specificando nel software le caratteristiche del modello di popolazione, ipotesi nulle e alternative, il livello di significatività statistica e la dimensione campionaria prevista.\nPotenza Retrospettiva (Post Hoc, Osservata): A differenza dell’analisi a priori, questa viene condotta dopo la raccolta dei dati. Le statistiche campionarie vengono trattate come parametri reali della popolazione, ma questa pratica presenta limitazioni significative. Le stime possono essere distorte, e una maggiore potenza osservata non implica necessariamente una forte evidenza a favore delle ipotesi nulle non rifiutate. Inoltre, essendo una misura post hoc, non aiuta nella progettazione proattiva della ricerca.\n\nPer l’analisi della potenza in SEM, sono stati sviluppati diversi metodi, tra cui:\n\nIl metodo Satorra–Saris stima la potenza del test del rapporto di verosimiglianza per un singolo parametro.\nIl metodo MacCallum–RMSEA si basa sulla RMSEA di popolazione e sulle distribuzioni chi-quadrato non centrali.\nIl metodo di simulazione Monte Carlo è un’alternativa moderna e flessibile che non presuppone né risultati continui né stima ML predefinita.\n\nCon l’avanzamento degli strumenti informatici, l’analisi della potenza statistica in SEM è diventata più accessibile:\nSoftware SEM con Simulazione Monte Carlo: Software come Mplus e LISREL includono capacità di simulazione Monte Carlo, permettendo di generare dati campionari basati su ipotesi del modello e di valutare la frequenza con cui i risultati significativi vengono ottenuti.\nMetodo Kelley–Lai Precision: Calcola la dimensione campionaria minima necessaria per stimare parametri come l’indice RMSEA entro un margine di errore specificato.\nNel contesto di R, le funzioni semTools::findRMSEApower e semTools::findRMSEAsamplesize del pacchetto semTools facilitano queste analisi:\n\nsemTools::findRMSEApower: Determina la potenza di un test SEM data una dimensione specifica del campione, basandosi sull’RMSEA e altri parametri del test.\nsemTools::findRMSEAsamplesize: Calcola la dimensione del campione necessaria per raggiungere una specifica potenza statistica in un test SEM, considerando l’RMSEA e altri criteri come il livello di significatività e la potenza desiderata.\n\nQuesti strumenti sono importanti per ottimizzare la progettazione della ricerca SEM, garantendo campioni adeguati e potenza statistica sufficiente per rilevare gli effetti di interesse.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#riflessioni-conclusive",
    "href": "chapters/sem/03_gof.html#riflessioni-conclusive",
    "title": "50  Test del Modello e Indicizzazione",
    "section": "\n50.15 Riflessioni Conclusive",
    "text": "50.15 Riflessioni Conclusive\nNella letteratura SEM, sono state avanzate critiche significative all’uso di indici come RMSEA, CFI e TLI e ai loro valori di cutoff convenzionali (si veda, ad esempio, Barrett, 2007). Nonostante queste critiche, tali indici continuano a essere ampiamente utilizzati nella ricerca SEM, in assenza di alternative più accettate e praticabili. Come sottolineato da Xia & Yang (2019), l’attuale prassi considera valori più elevati di RMSEA e valori più bassi di CFI e TLI come indicativi di un peggior adattamento del modello. Questo ha portato molti ricercatori a modificare i loro modelli per ottimizzare tali indici, spesso spingendoli a concentrarsi esclusivamente su questi criteri.\nLa dipendenza eccessiva da RMSEA, CFI e TLI ha condotto a una situazione in cui gli indici di adattamento vengono utilizzati come unico criterio per accettare o rifiutare un modello ipotizzato. Ad esempio, se un modello raggiunge soglie considerate “pubblicabili” (ad es., RMSEA &lt; .06), viene spesso accettato senza ulteriori miglioramenti. Tuttavia, questa pratica è problematica: affermazioni come “poiché i valori di RMSEA, CFI e TLI indicano un buon adattamento, questo modello è stato scelto come modello finale” sono insufficienti e riduttive.\nIl raggiungimento di soglie desiderate per questi indici dovrebbe rappresentare solo uno dei fattori da considerare nel processo di selezione del modello. È essenziale che i ricercatori:\n\nValutino altre opzioni di miglioramento del modello: Analizzando se esistano modifiche che potrebbero migliorare l’adattamento senza compromettere la validità teorica o la parsimonia.\nGiustifichino le scelte adottate o scartate: Spiegando chiaramente perché determinate opzioni di miglioramento non sono state applicate e quali sono le implicazioni di tali decisioni.\nConsiderino le conseguenze scientifiche e pratiche: Valutando l’impatto delle scelte di modellazione sulle conclusioni scientifiche e, quando pertinente, sulle applicazioni cliniche.\n\nIn sintesi, affidarsi esclusivamente a soglie arbitrarie per RMSEA, CFI e TLI non è sufficiente per determinare la qualità di un modello. Un approccio più integrato e critico, che tenga conto di considerazioni teoriche, pratiche e metodologiche, è necessario per garantire che i modelli scelti siano solidi e utili per rispondere alle domande di ricerca.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/sem/03_gof.html#informazioni-sullambiente-di-sviluppo",
    "title": "50  Test del Modello e Indicizzazione",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] effectsize_1.0.0   rsvg_2.6.1         DiagrammeRsvg_0.1 \n#&gt;  [4] lme4_1.1-36        Matrix_1.7-2       mvnormalTest_1.0.0\n#&gt;  [7] lavaanPlot_0.8.1   lavaanExtra_0.2.1  ggokabeito_0.1.0  \n#&gt; [10] see_0.10.0         MASS_7.3-65        viridis_0.6.5     \n#&gt; [13] viridisLite_0.4.2  ggpubr_0.6.0       ggExtra_0.10.1    \n#&gt; [16] gridExtra_2.3      patchwork_1.3.0    bayesplot_1.11.1  \n#&gt; [19] semTools_0.5-6     semPlot_1.1.6      lavaan_0.6-19     \n#&gt; [22] psych_2.4.12       scales_1.3.0       markdown_1.13     \n#&gt; [25] knitr_1.49         lubridate_1.9.4    forcats_1.0.0     \n#&gt; [28] stringr_1.5.1      dplyr_1.1.4        purrr_1.0.4       \n#&gt; [31] readr_2.1.5        tidyr_1.3.1        tibble_3.2.1      \n#&gt; [34] ggplot2_3.5.1      tidyverse_2.0.0    here_1.0.1        \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] splines_4.4.2       later_1.4.1         datawizard_1.0.0   \n#&gt;   [4] XML_3.99-0.18       rpart_4.1.24        lifecycle_1.0.4    \n#&gt;   [7] Rdpack_2.6.2        rstatix_0.7.2       rprojroot_2.0.4    \n#&gt;  [10] lattice_0.22-6      insight_1.0.2       rockchalk_1.8.157  \n#&gt;  [13] backports_1.5.0     magrittr_2.0.3      openxlsx_4.2.8     \n#&gt;  [16] Hmisc_5.2-2         rmarkdown_2.29      httpuv_1.6.15      \n#&gt;  [19] qgraph_1.9.8        zip_2.3.2           pbapply_1.7-2      \n#&gt;  [22] minqa_1.2.8         RColorBrewer_1.1-3  ADGofTest_0.3      \n#&gt;  [25] multcomp_1.4-28     abind_1.4-8         quadprog_1.5-8     \n#&gt;  [28] pspline_1.0-21      nnet_7.3-20         TH.data_1.1-3      \n#&gt;  [31] sandwich_3.1-1      moments_0.14.1      nortest_1.0-4      \n#&gt;  [34] arm_1.14-4          codetools_0.2-20    tidyselect_1.2.1   \n#&gt;  [37] farver_2.1.2        stats4_4.4.2        base64enc_0.1-3    \n#&gt;  [40] jsonlite_1.9.0      Formula_1.2-5       survival_3.8-3     \n#&gt;  [43] emmeans_1.10.7      tools_4.4.2         Rcpp_1.0.14        \n#&gt;  [46] glue_1.8.0          mnormt_2.1.1        xfun_0.51          \n#&gt;  [49] withr_3.0.2         numDeriv_2016.8-1.1 fastmap_1.2.0      \n#&gt;  [52] boot_1.3-31         digest_0.6.37       mi_1.1             \n#&gt;  [55] timechange_0.3.0    R6_2.6.1            mime_0.12          \n#&gt;  [58] estimability_1.5.1  colorspace_2.1-1    gtools_3.9.5       \n#&gt;  [61] jpeg_0.1-10         copula_1.1-5        DiagrammeR_1.0.11  \n#&gt;  [64] generics_0.1.3      data.table_1.17.0   corpcor_1.6.10     \n#&gt;  [67] htmlwidgets_1.6.4   parameters_0.24.1   pkgconfig_2.0.3    \n#&gt;  [70] sem_3.1-16          gtable_0.3.6        pcaPP_2.0-5        \n#&gt;  [73] htmltools_0.5.8.1   carData_3.0-5       png_0.1-8          \n#&gt;  [76] reformulas_0.4.0    rstudioapi_0.17.1   tzdb_0.4.0         \n#&gt;  [79] reshape2_1.4.4      coda_0.19-4.1       visNetwork_2.1.2   \n#&gt;  [82] checkmate_2.3.2     nlme_3.1-167        curl_6.2.1         \n#&gt;  [85] nloptr_2.1.1        zoo_1.8-13          parallel_4.4.2     \n#&gt;  [88] miniUI_0.1.1.1      foreign_0.8-88      pillar_1.10.1      \n#&gt;  [91] grid_4.4.2          vctrs_0.6.5         promises_1.3.2     \n#&gt;  [94] car_3.1-3           OpenMx_2.21.13      xtable_1.8-4       \n#&gt;  [97] cluster_2.1.8       htmlTable_2.4.3     evaluate_1.0.3     \n#&gt; [100] pbivnorm_0.6.0      mvtnorm_1.3-3       cli_3.6.4          \n#&gt; [103] kutils_1.73         compiler_4.4.2      rlang_1.1.5        \n#&gt; [106] ggsignif_0.6.4      fdrtool_1.2.18      plyr_1.8.9         \n#&gt; [109] stringi_1.8.4       munsell_0.5.1       gsl_2.1-8          \n#&gt; [112] lisrelToR_0.3       bayestestR_0.15.2   pacman_0.5.1       \n#&gt; [115] V8_6.0.1            hms_1.1.3           stabledist_0.7-2   \n#&gt; [118] glasso_1.11         shiny_1.10.0        rbibutils_2.3      \n#&gt; [121] igraph_2.1.4        broom_1.0.7         RcppParallel_5.1.10\n\n\n\n\n\nBarrett, P. (2007). Structural equation modelling: Adjudging model fit. Personality and Individual Differences, 42(5), 815–824.\n\n\nHayduk, L. A. (2014). Shame for disrespecting evidence: The personal consequences of insufficient respect for structural equation model testing. BMC Medical Research Methodology, 14, 1–10.\n\n\nHu, L., & Bentler, P. M. (1998). Fit indices in covariance structure modeling: Sensitivity to underparameterized model misspecification. Psychological Methods, 3(4), 424--453.\n\n\nKline, R. B. (2023). Principles and practice of structural equation modeling. Guilford publications.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.\n\n\nXia, Y., & Yang, Y. (2019). RMSEA, CFI, and TLI in structural equation modeling with ordered categorical data: The story they tell depends on the estimation methods. Behavior Research Methods, 51(1), 409–428.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html",
    "href": "chapters/sem/04_mod_comp.html",
    "title": "51  Confronto tra modelli",
    "section": "",
    "text": "51.1 Introduzione\nI ricercatori spesso confrontano modelli alternativi di equazioni strutturali che includono le stesse variabili e sono adattati agli stessi dati. Il contesto più frequente si verifica quando un singolo modello iniziale viene testato attraverso una serie di passaggi. Ad ogni passaggio, il modello iniziale viene ridefinito aggiungendo uno o più parametri liberi, il che generalmente migliora l’adattamento, oppure eliminando (fissando a zero) uno o più parametri liberi, il che generalmente peggiora l’adattamento. Una coppia di modelli alternativi così specificata viene definita “modelli nidificati”, poiché il modello più semplice dei due, o modello vincolato, è un sottoinsieme proprio del modello più complesso, o modello non vincolato. Un contesto diverso si verifica quando ci sono due o più modelli iniziali tali che (1) ogni modello si basa su una teoria diversa e (2) i modelli alternativi non sono nidificati nella loro relazione l’uno con l’altro. In entrambi i contesti, la scelta tra modelli concorrenti dovrebbe essere guidata tanto da basi concettuali quanto da considerazioni statistiche.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html#confrontare-modelli-nel-sem",
    "href": "chapters/sem/04_mod_comp.html#confrontare-modelli-nel-sem",
    "title": "51  Confronto tra modelli",
    "section": "\n51.2 Confrontare Modelli nel SEM",
    "text": "51.2 Confrontare Modelli nel SEM\nNel contesto dei Modelli di Equazioni Strutturali (SEM), un aspetto critico è il confronto tra diversi modelli per determinare quale sia il più adeguato. Questo confronto si presenta frequentemente nella forma di analisi di modelli nidificati. In tale contesto, si confronta un modello considerato “pieno” o “meno restrittivo” con un altro modello che è “ridotto” o “più restrittivo”.\nIl modello pieno include un insieme più ampio di parametri e ipotesi, offrendo una rappresentazione più complessa delle relazioni tra le variabili. Al contrario, il modello ridotto è una versione più semplificata, con meno parametri e ipotesi, risultando in una struttura più contenuta e potenzialmente più parsimoniosa.\nQuesto tipo di confronto è cruciale per valutare l’adeguatezza dei modelli SEM, permettendo ai ricercatori di decidere se la complessità aggiuntiva del modello pieno sia giustificata rispetto al modello ridotto in termini di adattamento ai dati e coerenza con la teoria sottostante.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html#analisi-dei-modelli-nidificati",
    "href": "chapters/sem/04_mod_comp.html#analisi-dei-modelli-nidificati",
    "title": "51  Confronto tra modelli",
    "section": "\n51.3 Analisi dei Modelli Nidificati",
    "text": "51.3 Analisi dei Modelli Nidificati\nNell’ambito dei Modelli di Equazioni Strutturali (SEM), i modelli nidificati occupano un ruolo centrale. Due modelli sono definiti come nidificati quando soddisfano specifici criteri gerarchici, delineati come segue:\n\n\nFormazione del Modello Vincolato:\n\nSi crea un modello vincolato applicando una o più restrizioni a un modello non vincolato esistente. Questo processo aumenta i gradi di libertà del modello vincolato (C) rispetto a quelli del modello non vincolato (U), risultando in \\(\\text{df}_C &gt; \\text{df}_U\\).\n\n\n\nDifferenze nei Gradi di Libertà:\n\nLa differenza \\(\\text{df}_C - \\text{df}_U\\) rappresenta il numero di restrizioni imposte al modello non vincolato per creare il modello vincolato, che equivale alla variazione nel numero di parametri liberi tra i due modelli.\n\n\n\nParametri Liberi e Vincolati:\n\nI parametri liberi nel modello vincolato costituiscono un sottoinsieme di quelli presenti nel modello non vincolato. Allo stesso modo, i parametri fissi nel modello non vincolato formano un sottoinsieme di quelli nel modello vincolato.\n\n\n\nConfronto dei Valori di Chi-Quadro:\n\nTra i due modelli, il valore di \\(\\chi^2\\) è minore o uguale nel modello non vincolato rispetto a quello nel modello vincolato, ovvero \\(\\chi^2_U \\leq \\chi^2_C\\). Questo implica che le distribuzioni di probabilità possibili nel modello vincolato sono comprese anche nel modello non vincolato, che può tuttavia suggerire ulteriori distribuzioni non coerenti con il modello vincolato.\n\n\n\nQuesto tipo di relazione gerarchica, conosciuta come annidamento dei parametri, permette di valutare l’impatto di specifiche restrizioni o aggiunte di parametri. Ad esempio, un parametro libero in un modello non vincolato può essere fissato a zero, eliminando di conseguenza l’effetto corrispondente nel modello vincolato, oppure può essere sottoposto a un vincolo specificato dal ricercatore, riducendo così il numero di parametri liberi ma mantenendo l’effetto nel modello vincolato.\nConsideriamo, per esempio, un modello di percorso non vincolato U con effetti diretti: \\(X \\rightarrow Y_1 \\rightarrow Y_2\\) e \\(X \\rightarrow Y_2\\). Ridefinendo il percorso \\(X \\rightarrow Y_2 = 0\\), eliminiamo questa connessione dal modello U, generando così il modello vincolato C1, nidificato sotto U. Un’alternativa potrebbe essere imporre un vincolo di uguaglianza tra \\(X\\rightarrow Y_2\\) e \\(Y_1 \\rightarrow Y_2\\), indicando che gli effetti diretti non standardizzati di \\(X\\) e \\(Y_1\\) su \\(Y_2\\) sono identici. Questo produce un modello vincolato, C2, con un parametro libero in meno rispetto a U ma che include tutti i percorsi di U.\nNelle prossime sezioni, esamineremo come testare le ipotesi inerenti a questi modelli nidificati e come valutare la loro adeguatezza nel contesto SEM.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html#strategie-di-costruzione-e-potatura-nei-modelli-sem",
    "href": "chapters/sem/04_mod_comp.html#strategie-di-costruzione-e-potatura-nei-modelli-sem",
    "title": "51  Confronto tra modelli",
    "section": "\n51.4 Strategie di Costruzione e Potatura nei Modelli SEM",
    "text": "51.4 Strategie di Costruzione e Potatura nei Modelli SEM\n\n51.4.1 Costruzione Progressiva del Modello\nLa costruzione di modelli SEM inizia tipicamente con un modello iniziale semplice e vincolato, che riflette le ipotesi fondamentali basate su teorie sostanziali. Questo approccio, detto anche ricerca in avanti, implica l’aggiunta progressiva di parametri liberi che rappresentano ipotesi precedentemente escluse, in base alla loro importanza. Sebbene questo processo possa generalmente migliorare l’adattamento del modello (riduzione di chiML), un adattamento migliore non è necessariamente indicativo di una maggiore correttezza del modello. La costruzione del modello può teoricamente proseguire fino a quando non si raggiunge un modello perfettamente adatto ai dati (dfM = 0), ma è essenziale valutare la validità teorica e la parsimonia del modello in ogni passaggio.\n\n51.4.2 Potatura Retrograda del Modello\nIn contrasto, la potatura del modello, o ricerca all’indietro, inizia con un modello più complesso e non vincolato. In questa fase, il ricercatore semplifica il modello eliminando parametri liberi (fissandoli a zero) o imponendo vincoli di stima. Questo processo richiede di dare priorità alle ipotesi in ordine inverso di importanza. Il modello iniziale dovrebbe essere congruente con i dati, altrimenti non ha senso restringerlo ulteriormente. Tipicamente, come si procede con la potatura, l’adattamento complessivo del modello ai dati tende a peggiorare (aumento di chiML). Il criterio per arrestare la potatura si basa sull’adattamento del modello: si ferma quando ulteriori restrizioni peggiorerebbero significativamente l’adattamento ai dati.\n\n51.4.3 Obiettivi e Considerazioni\nL’obiettivo sia nella costruzione che nella potatura di modelli è identificare un modello con una struttura di covarianza (e, se presente, anche di media) correttamente specificata e teoricamente giustificata. Idealmente, entrambi gli approcci dovrebbero convergere verso lo stesso modello ottimale, benché ciò non sia garantito. È importante evitare il rischio di formulare ipotesi post hoc (HARKing), presentando modelli scoperti in modo esplorativo come se fossero stati ipotizzati a priori. Una soluzione a questo problema è la preregistrazione del piano di analisi.\n\n51.4.4 Punti di Forza Relativi\n\n\nCostruzione del Modello: Partire da un modello più semplice può essere vantaggioso, soprattutto per i neofiti del SEM, poiché facilita l’identificazione statistica e riduce il rischio di errori nella specificazione del modello.\n\nPotatura del Modello: Questo approccio può essere particolarmente efficace per i modelli di misurazione, dove le variabili osservate sono usate come indicatori di un numero limitato di fattori comuni. Un modello di misurazione correttamente specificato inizialmente può rendere la potatura più efficace rispetto alla costruzione.\n\nIn entrambi i casi, è cruciale basare le decisioni su solide basi teoriche oltre che su considerazioni statistiche, per assicurare che il modello finale sia non solo adatto ai dati ma anche coerente con il quadro teorico sottostante.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html#strategie-di-ridefinizione-dei-modelli-sem-approcci-teorici-ed-empirici",
    "href": "chapters/sem/04_mod_comp.html#strategie-di-ridefinizione-dei-modelli-sem-approcci-teorici-ed-empirici",
    "title": "51  Confronto tra modelli",
    "section": "\n51.5 Strategie di Ridefinizione dei Modelli SEM: Approcci Teorici ed Empirici",
    "text": "51.5 Strategie di Ridefinizione dei Modelli SEM: Approcci Teorici ed Empirici\n\n51.5.1 Approccio Teorico nella Ridefinizione dei Modelli\nNel processo di costruzione o potatura dei Modelli di Equazioni Strutturali (SEM), l’approccio teorico gioca un ruolo fondamentale. Qui, le modifiche al modello sono guidate da ipotesi teoriche predefinite e specifiche. Ad esempio, considerando un modello di percorso non vincolato U con le relazioni \\(X \\rightarrow Y_1 \\rightarrow Y_2\\) e \\(X \\rightarrow Y_2\\), un ricercatore potrebbe ipotizzare che l’effetto di X su Y2 sia esclusivamente indiretto attraverso Y1. Questa ipotesi può essere testata vincolando il coefficiente di \\(X \\rightarrow Y_2\\) a zero. Se l’adattamento del modello così modificato non è significativamente inferiore rispetto al modello non vincolato, l’ipotesi di un effetto indiretto viene supportata, a patto che la direzionalità delle relazioni sia corretta.\nQuesto approccio enfatizza che le modifiche al modello dovrebbero essere effettuate sulla base di solide basi teoriche e concettuali, piuttosto che su criteri puramente statistici, come evidenziato da Jöreskog (1969): “La decisione di smettere di aggiungere parametri non può basarsi solo su una base statistica; ciò dipende in gran parte dall’interpretazione dei dati da parte del ricercatore, basata su considerazioni teoriche e concettuali sostanziali.”\n\n51.5.2 Approccio Empirico nella Ridefinizione dei Modelli\nAl contrario, l’approccio empirico nella costruzione o potatura dei modelli SEM si basa su criteri statistici. In questo scenario, i parametri liberi vengono aggiunti o eliminati a seconda della loro significatività statistica o di altri indicatori empirici. Per esempio, se i percorsi sono eliminati solo perché i loro coefficienti non sono statisticamente significativi, la ridefinizione del modello è guidata da considerazioni puramente empiriche. Questo approccio è analogo alla tecnica di eliminazione all’indietro nella regressione multipla, dove il software sceglie quali predittori rimuovere in base a criteri di significatività statistica.\n\n51.5.3 Implicazioni per l’Interpretazione dei Modelli\nLa scelta tra un approccio teorico o empirico nella ridefinizione dei modelli SEM ha implicazioni significative per come interpretiamo i risultati. Un modello modificato in base a criteri teorici forti offre una maggiore fiducia nella validità delle sue conclusioni, mentre un modello costruito o potato basandosi principalmente su criteri empirici può essere soggetto a errori di Tipo I o II e può non essere replicabile in campioni diversi.\nÈ fondamentale che i ricercatori si avvicinino alla costruzione e alla potatura dei modelli SEM con un equilibrio tra intuizioni teoriche e risultati empirici, per assicurare che i modelli finali siano non solo statisticamente validi ma anche teoricamente giustificati e interpretativamente significativi.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html#test-della-differenza-chi-quadro-nel-sem",
    "href": "chapters/sem/04_mod_comp.html#test-della-differenza-chi-quadro-nel-sem",
    "title": "51  Confronto tra modelli",
    "section": "\n51.6 Test della Differenza Chi-Quadro nel SEM",
    "text": "51.6 Test della Differenza Chi-Quadro nel SEM\n\n51.6.1 Principi Fondamentali\nIl test della differenza Chi-Quadro (chiD) è una tecnica statistica essenziale nel contesto dei Modelli di Equazioni Strutturali (SEM) per valutare l’effetto delle modifiche ai parametri sui modelli. Questo test viene utilizzato sia nella potatura (restrizione dei parametri) che nella costruzione (aggiunta di parametri) dei modelli. Il valore chiD rappresenta la differenza tra i valori di chi-quadro (chi-quadro massima verosimiglianza, chiML) di due modelli nidificati. I gradi di libertà associati, dfD, sono determinati dalla differenza nei gradi di libertà dei due modelli (\\(\\text{df}_C - \\text{df}_U\\)).\n\n51.6.2 Applicazione del Test\nPer applicare il test della differenza chi-quadro, si seguono questi passaggi:\n\n\nDefinizione dei Modelli: Identificare il modello pieno (con tutti i parametri ritenuti rilevanti) e il modello ridotto (una versione semplificata del modello pieno con alcune restrizioni).\n\nStima dei Modelli: Utilizzare metodi di massima verosimiglianza per stimare entrambi i modelli.\n\nCalcolo del Rapporto di Verosimiglianze: Calcolare la differenza tra i logaritmi delle funzioni di verosimiglianza dei due modelli (\\(D = -2(\\ln(L_r) - \\ln(L_f))\\)).\n\nTest Statistico: Utilizzare la distribuzione chi-quadrato per determinare il p-value. Un p-value basso indica che il modello ridotto non si adatta ai dati così come il modello pieno.\n\n51.6.3 Interpretazione dei Risultati\nUn valore piccolo di chiD suggerisce che non c’è una differenza significativa nell’adattamento tra i due modelli, mentre un valore grande indica una differenza significativa. In termini di potatura, un grande chiD implica che il modello è stato eccessivamente vincolato. Nella costruzione, un grande chiD supporta la conservazione del parametro libero aggiunto. Tuttavia, prima di trarre conclusioni definitive, è cruciale considerare l’adattamento complessivo del modello, sia a livello globale che locale.\n\n51.6.4 Considerazioni nella Stima ML Robusta\nNel caso di stima ML robusta, la differenza tra i chi-quadri scalati non può essere interpretata come un test dell’ipotesi di adattamento uguale a causa delle distribuzioni non centrali in condizioni di non normalità. Sono disponibili metodi specifici per calcolare una statistica di differenza chi-quadro scalata che segue approssimativamente le distribuzioni chi-quadro.\n\n51.6.5 Implicazioni Teoriche ed Empiriche\nLa decisione di modificare un modello basandosi su un approccio teorico o empirico ha implicazioni significative. Ad esempio, l’eliminazione di percorsi non significativi su base puramente statistica può portare a conclusioni errate se non supportate da una solida base teorica. Inoltre, è importante essere consapevoli dei rischi associati alla capitalizzazione sul caso, come errori di Tipo I e II, e del pericolo di seguire “sentieri che si biforcano” nelle decisioni analitiche, che possono rendere i risultati specifici del campione e difficili da replicare. Per mitigare questi rischi, è consigliabile basare le modifiche del modello più su orientamenti teorici che sui risultati dei test di significatività.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html#test-della-differenza-chi-quadro-scalato",
    "href": "chapters/sem/04_mod_comp.html#test-della-differenza-chi-quadro-scalato",
    "title": "51  Confronto tra modelli",
    "section": "\n51.7 Test della Differenza Chi-Quadro Scalato",
    "text": "51.7 Test della Differenza Chi-Quadro Scalato\nIl metodo di Satorra e Bentler (2001) permette di calcolare manualmente una statistica di differenza chi-quadro scalata quando si confrontano due modelli gerarchici nella stima ML robusta. Si presume che il modello 1 sia più vincolato rispetto al modello 2 (cioè, dfM1 &gt; dfM2), i chi-quadri non scalati siano chiML e i chi-quadri scalati siano chiSB. La statistica di test Satorra-Bentler è definita come segue:\n\nCalcolare la Statistica di Differenza Chi-Quadro non Scalata e i suoi Gradi di Libertà:\n\n\nchiD = chiML1 - chiML2 e dfD = dfM1 - dfM2.\n\n\nRecuperare il Fattore di Correzione di Scala, c, per Ogni Modello:\n\n\nc1 = chiML1 / chiSB1 e c2 = chiML2 / chiSB2.\n\n\nCalcolare la Statistica di Differenza Chi-Quadro Scalata, chiSD:\n\n\nchiSD = chiD / ((c1 / dfM1 - c2 / dfM2) / dfD).\nLa probabilità per chiSD (dfD) in una distribuzione chi-quadro centrale rappresenta il p-value per il test di differenza chi-quadro scalato.\n\nIn campioni piccoli o quando il modello più vincolato è molto errato, il denominatore di chiSD può essere &lt; 0, invalidando il test. Questo test è implementato nella funzione lavTestLRT() in lavaan (Rosseel et al., 2023).\n\n51.7.1 Esempio\nConsideriamo nuovamente i dati discussi da Brown (2015) relativi al modello di misurazione per la depressione maggiore così come è definita nel DSM-IV. Ignoriamo qui le differenze di genere. Leggiamo i dati in \\(\\mathsf{R}\\):\n\nd_mdd &lt;- readRDS(here::here(\"data\", \"mdd_sex.RDS\"))\n\nConsideriamo il seguente modello:\n\nmodel_mdd &lt;- \"\n  MDD =~ mdd1 + mdd2 + mdd3 + mdd4 + mdd5 + mdd6 + mdd7 + mdd8 + mdd9\n\"\n\nAdattiamo il modello ai dati.\n\nfit_mdd &lt;- cfa(\n    model_mdd,\n    data = d_mdd\n)\n\n\nsemPaths(fit_mdd,\n    whatLabels = \"std\",\n    sizeMan = 8,\n    edge.label.cex = 0.7,\n    style = \"mx\",\n    nCharNodes = 0, nCharEdges = 0\n)\n\n\n\n\n\n\n\n\nfitMeasures(fit_mdd) \n#&gt;                  npar                  fmin                 chisq \n#&gt;              1.80e+01              7.40e-02              1.10e+02 \n#&gt;                    df                pvalue        baseline.chisq \n#&gt;              2.70e+01              0.00e+00              1.30e+03 \n#&gt;           baseline.df       baseline.pvalue                   cfi \n#&gt;              3.60e+01              0.00e+00              9.34e-01 \n#&gt;                   tli                  nnfi                   rfi \n#&gt;              9.12e-01              9.12e-01              8.87e-01 \n#&gt;                   nfi                  pnfi                   ifi \n#&gt;              9.15e-01              6.86e-01              9.34e-01 \n#&gt;                   rni                  logl     unrestricted.logl \n#&gt;              9.34e-01             -1.37e+04             -1.37e+04 \n#&gt;                   aic                   bic                ntotal \n#&gt;              2.75e+04              2.76e+04              7.50e+02 \n#&gt;                  bic2                 rmsea        rmsea.ci.lower \n#&gt;              2.76e+04              6.40e-02              5.20e-02 \n#&gt;        rmsea.ci.upper        rmsea.ci.level          rmsea.pvalue \n#&gt;              7.70e-02              9.00e-01              2.90e-02 \n#&gt;        rmsea.close.h0 rmsea.notclose.pvalue     rmsea.notclose.h0 \n#&gt;              5.00e-02              1.90e-02              8.00e-02 \n#&gt;                   rmr            rmr_nomean                  srmr \n#&gt;              1.91e-01              1.91e-01              4.40e-02 \n#&gt;          srmr_bentler   srmr_bentler_nomean                  crmr \n#&gt;              4.40e-02              4.40e-02              5.00e-02 \n#&gt;           crmr_nomean            srmr_mplus     srmr_mplus_nomean \n#&gt;              5.00e-02              4.40e-02              4.40e-02 \n#&gt;                 cn_05                 cn_01                   gfi \n#&gt;              2.74e+02              3.20e+02              9.64e-01 \n#&gt;                  agfi                  pgfi                   mfi \n#&gt;              9.40e-01              5.78e-01              9.46e-01 \n#&gt;                  ecvi \n#&gt;              1.95e-01\n\nGli indici Comparative Fit Index (CFI) = 0.934 e Tucker-Lewis Index (TLI) = 0.912 sono superiori a 0.9, dunque sono almeno sufficienti per gli standard correnti. L’indice RMSEA = 0.064 è appena superiore alla soglia di 0.06. L’indice SRMR = 0.044 è inferiore alla soglia 0.05. Dunque, complessivamente, il modello sembra adeguato.\nAdattiamo ora il modello con la modifica proposta da {cite:t}brown2015confirmatory, ovvero\n\nmodel2_mdd &lt;- \"\n  MDD =~ mdd1 + mdd2 + mdd3 + mdd4 + mdd5 + mdd6 + mdd7 + mdd8 +  mdd9\n  mdd1 ~~ mdd2\n\"\n\nfit2_mdd &lt;- cfa(\n    model2_mdd,\n    data = d_mdd\n)\n\nEseguiamo il test del rapporto di verosimiglianze:\n\nlavTestLRT(fit_mdd, fit2_mdd)\n#&gt; \n#&gt; Chi-Squared Difference Test\n#&gt; \n#&gt;          Df   AIC   BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\n#&gt; fit2_mdd 26 27490 27577  67.6                                    \n#&gt; fit_mdd  27 27530 27614 110.3       42.7 0.236       1    6.3e-11\n\nIl test indica che il modello alternativo si adatta meglio ai dati del modello originale.\nEsaminiamo gli indici di bontà di adattamento.\n\neffectsize::interpret(fit2_mdd)\n#&gt;     Name  Value Threshold Interpretation\n#&gt; 1    GFI 0.9781      0.95   satisfactory\n#&gt; 2   AGFI 0.9620      0.90   satisfactory\n#&gt; 3    NFI 0.9479      0.90   satisfactory\n#&gt; 4   NNFI 0.9544      0.90   satisfactory\n#&gt; 5    CFI 0.9671      0.90   satisfactory\n#&gt; 6  RMSEA 0.0462      0.05   satisfactory\n#&gt; 7   SRMR 0.0368      0.08   satisfactory\n#&gt; 8    RFI 0.9279      0.90   satisfactory\n#&gt; 9   PNFI 0.6846      0.50   satisfactory\n#&gt; 10   IFI 0.9673      0.90   satisfactory\n\nGli indici Comparative Fit Index (CFI) = 0.967 e Tucker-Lewis Index (TLI) = 0.954 sono superiori a 0.95. L’indice RMSEA = 0.046. L’indice SRMR = 0.037.\nIl “costo” che si paga per questo miglioramento dell’adattamento è che indici di adattamento così buoni, probabilmente, non si replicheranno in un altro campione di dati, a meno che venga introdotto un qualche altro aggiustamento che, sicuramente, sarà diverso da quello usato nel campione corrente. Personalmente, non avrei introdotto il “miglioramento” proposto da {cite:t}brown2015confirmatory in quanto, anche senza un tale aggiustamento post-hoc, il modello produce un adattamento accettabile.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html#confronto-di-modelli-non-annidati",
    "href": "chapters/sem/04_mod_comp.html#confronto-di-modelli-non-annidati",
    "title": "51  Confronto tra modelli",
    "section": "\n51.8 Confronto di Modelli Non Annidati",
    "text": "51.8 Confronto di Modelli Non Annidati\nProseguendo, analizzeremo l’adattamento di due modelli distinti, entrambi costituiti dalle stesse variabili e applicati agli stessi dati. Tuttavia, a differenza di quanto precedentemente delineato, questi modelli non sono collegati gerarchicamente, ma si configurano come modelli non annidati. Questa situazione si presenta comunemente quando i ricercatori mettono a confronto modelli basati su teorie divergenti. È possibile effettuare un confronto informale dei valori del chi-quadrato derivanti da modelli non annidati, ma la loro differenza non va interpretata come una statistica di test valida. In altre parole, i test di differenza del chi-quadrato, sia in forma scalata che non, non sono appropriati in questo contesto. La ragione risiede nel fatto che la differenza tra le statistiche di test di modelli non annidati non segue una distribuzione chi-quadrato centrale. Sebbene siano stati compiuti sforzi per elaborare test di significatività adatti al confronto di modelli non annidati, questi metodi non hanno trovato un ampio utilizzo e spesso portano a complicazioni interpretative (Levy & Hancock, 2007).\nUna soluzione più pragmatica è rappresentata dalla famiglia degli indici di adattamento predittivo, conosciuti anche come criteri teorico-informativi. Questi indici non sono test di significatività, poiché le loro distribuzioni di probabilità variano ampiamente a seconda del tipo di modello e dei dati considerati e, pertanto, rimangono generalmente ignote. Piuttosto, essi riflettono sia la qualità dell’adattamento del modello sia la sua complessità, bilanciando questi due aspetti. Ciò implica l’applicazione di una penalità per la complessità del modello, che consente di regolare l’adattamento in funzione del numero di parametri liberi. Per esempio, nel caso di due modelli non annidati che mostrano un adattamento simile agli stessi dati, verrà privilegiato il modello meno complesso, in quanto considerato più probabile nella generalizzazione su campioni replicati. In questo scenario, il valore del criterio informativo sarà inferiore per il modello più semplice, dato che una penalità maggiore per la complessità viene applicata all’adattamento del modello più complesso. Di conseguenza, il modello con il criterio informativo più basso è da preferire. In questo capitolo, dopo aver introdotto un problema di ricerca, esploreremo due indici di adattamento predittivo ampiamente usati (AIC e BIC).\n\n\n\n\n\nFigura 51.1: Modelli alternativi non annidati di percorso ricorsivo per l’adattamento dopo un intervento chirurgico cardiaco. (Figura tratta da Kline (2023).)\n\n\nNella figura sono presentati due modelli di percorso che descrivono il recupero dei pazienti dopo un intervento chirurgico cardiaco (Romney et al., 1992) – si veda Kline (2023), cap. 11. Il modello psicosomatico rappresenta le ipotesi che il morale del paziente trasmetta gli effetti della disfunzione neurologica e dello stato socioeconomico ridotto (SES) su sintomi della malattia e scarse relazioni sociali. Il modello medico rappresenta un diverso schema di relazioni causali tra le stesse variabili. In particolare, sia i sintomi della malattia sia la disfunzione neurologica sono specificati come variabili esogene con effetti diretti sul SES ridotto, basso morale e scarse relazioni. Tra queste tre variabili endogene, si ipotizza che il SES ridotto influenzi indirettamente le scarse relazioni attraverso il suo impatto precedente sul basso morale. Ci sono ulteriori effetti indiretti nel modello medico convenzionale dalle variabili esogene a quelle endogene. I due modelli nella figura non sono annidati, quindi il test della differenza del chi-quadro non può essere utilizzato per confrontarli direttamente. È dunque necessario seguire un altro approccio.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html#aic-e-bic",
    "href": "chapters/sem/04_mod_comp.html#aic-e-bic",
    "title": "51  Confronto tra modelli",
    "section": "\n51.9 AIC e BIC",
    "text": "51.9 AIC e BIC\nUno degli indici di adattamento predittivo più noti basato sulla stima di massima verosimiglianza (ML) è il Criterio di Informazione di Akaike (AIC), che prende il nome dallo statistico Hirotugu Akaike. La formula per l’AIC di Akaike (1974, p. 719) è:\n\\[ \\text{AIC} = -2 \\ln L_0 + 2q \\]\ndove \\(L_0\\) è la funzione di verosimiglianza massimizzata nella stima ML per il modello del ricercatore e \\(q\\) è il numero di parametri liberi del modello. Si noti che la penalità per la complessità nell’equazione precedente, \\(2q\\), diventa relativamente più piccola all’aumentare della dimensione del campione (Mulaik, 2009b).\nUn diverso indice teorico-informativo che tiene direttamente conto della dimensione del campione è il Criterio di Informazione Bayesiano (BIC) (Raftery, 1993; Schwarz, 1978). La formula è\n\\[ \\text{BIC} = -2 \\ln L_0 + q \\ln N \\]\nConfrontato con l’AIC, il BIC impone una penalità relativa maggiore per la complessità del modello.\nSupponiamo che il numero di parametri stimati liberamente sia \\(q = 10\\) e che \\(N = 300\\). La penalità AIC equivale a \\(2(10)\\), ovvero 20.000 (Equazione 11.4), ma la penalità BIC per lo stesso modello è \\(10 (\\ln 300)\\), ovvero 50.038, più del doppio rispetto all’AIC. I valori relativi delle penalità BIC aumentano più lentamente all’aumentare della dimensione del campione; in altre parole, la sua penalità è asintotica su campioni sempre più grandi (Mulaik, 2009b).\nIn sostanza, sia l’AIC che il BIC sono strumenti per bilanciare l’adattamento del modello con la sua complessità, ma differiscono nel modo in cui valutano e penalizzano questa complessità, soprattutto in relazione alla dimensione del campione.\nUtilizzando lo script fornito da Kline (2023), iniziamo a importare i dati in R:\n\n# input the correlations in lower diagnonal form\nromneyLower.cor &lt;- \"\n 1.00\n  .53 1.00\n  .15  .18 1.00\n  .52  .29 -.05 1.00\n  .30  .34  .23  .09 1.00 \"\n\n# name the variables and convert to full correlation matrix\nromney.cor &lt;- lavaan::getCov(romneyLower.cor, names = c(\n    \"morale\", \"symptoms\",\n    \"dysfunction\", \"relations\", \"ses\"\n))\n\nEsaminiamo le matrici di correlazioni e covarianze:\n\n# display the correlations\nromney.cor\n#&gt;             morale symptoms dysfunction relations  ses\n#&gt; morale        1.00     0.53        0.15      0.52 0.30\n#&gt; symptoms      0.53     1.00        0.18      0.29 0.34\n#&gt; dysfunction   0.15     0.18        1.00     -0.05 0.23\n#&gt; relations     0.52     0.29       -0.05      1.00 0.09\n#&gt; ses           0.30     0.34        0.23      0.09 1.00\n\n\n# add the standard deviations and convert to covariances\nromney.cov &lt;- lavaan::cor2cov(romney.cor, sds = c(\n    3.75, 17.00, 19.50,\n    3.50, 24.70\n))\n\n# display the covariances\nromney.cov\n#&gt;             morale symptoms dysfunction relations    ses\n#&gt; morale       14.06     33.8       10.97      6.83  27.79\n#&gt; symptoms     33.79    289.0       59.67     17.25 142.77\n#&gt; dysfunction  10.97     59.7      380.25     -3.41 110.78\n#&gt; relations     6.83     17.3       -3.41     12.25   7.78\n#&gt; ses          27.79    142.8      110.78      7.78 610.09\n\nSpecifichiamo il modello psicosomatico.\n\nsomatic.model &lt;- \"\n    # regressions\n    morale ~ ses + dysfunction\n    relations ~ morale\n    symptoms ~ morale\n    # without the zero constraint listed next,\n    # lavaan automatically specifies correlated\n    # disturbances for symptoms and relations,\n    # but their disturbances are independent in\n    # figure 11.1\n    symptoms ~~ 0*relations\n    # unanalyzed association between ses and dysfunction\n    # automatically specified \n\"\n\nSpecifichiamo il modello medico convenzionale.\n\nmedical.model &lt;- \"\n    # regressions\n    ses ~ symptoms + dysfunction\n    morale ~ symptoms + ses\n    relations ~ dysfunction + morale\n    # unanalyzed association between symptoms and dysfunction\n    # automatically specified \n\"\n\nAdattiamo ai dati il modello psicosomatico.\n\nsomatic &lt;- lavaan::sem(somatic.model,\n    sample.cov = romney.cov,\n    sample.nobs = 469, fixed.x = FALSE, sample.cov.rescale = FALSE\n)\n\n\nsemPaths(somatic,\n    whatLabels = \"std\",\n    sizeMan = 10,\n    edge.label.cex = 1.0,\n    style = \"mx\",\n    nCharNodes = 0, nCharEdges = 0\n)\n\n\n\n\n\n\n\n\nlavaan::summary(somatic, fit.measures = TRUE, rsquare = TRUE) \n#&gt; lavaan 0.6-19 ended normally after 1 iteration\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        10\n#&gt; \n#&gt;   Number of observations                           469\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                40.488\n#&gt;   Degrees of freedom                                 5\n#&gt;   P-value (Chi-square)                           0.000\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               390.816\n#&gt;   Degrees of freedom                                 9\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.907\n#&gt;   Tucker-Lewis Index (TLI)                       0.833\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -8572.844\n#&gt;   Loglikelihood unrestricted model (H1)      -8552.599\n#&gt;                                                       \n#&gt;   Akaike (AIC)                               17165.687\n#&gt;   Bayesian (BIC)                             17207.193\n#&gt;   Sample-size adjusted Bayesian (SABIC)      17175.455\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.123\n#&gt;   90 Percent confidence interval - lower         0.090\n#&gt;   90 Percent confidence interval - upper         0.159\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.000\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.982\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.065\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Regressions:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;   morale ~                                            \n#&gt;     ses               0.043    0.007    6.217    0.000\n#&gt;     dysfunction       0.016    0.009    1.897    0.058\n#&gt;   relations ~                                         \n#&gt;     morale            0.485    0.037   13.184    0.000\n#&gt;   symptoms ~                                          \n#&gt;     morale            2.403    0.178   13.535    0.000\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;  .relations ~~                                        \n#&gt;    .symptoms          0.000                           \n#&gt;   ses ~~                                              \n#&gt;     dysfunction     110.779   22.821    4.854    0.000\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;    .morale           12.699    0.829   15.313    0.000\n#&gt;    .relations         8.938    0.584   15.313    0.000\n#&gt;    .symptoms        207.820   13.571   15.313    0.000\n#&gt;     ses             610.090   39.840   15.313    0.000\n#&gt;     dysfunction     380.250   24.831   15.313    0.000\n#&gt; \n#&gt; R-Square:\n#&gt;                    Estimate\n#&gt;     morale            0.097\n#&gt;     relations         0.270\n#&gt;     symptoms          0.281\n\nOra approfondiamo l’analisi dei residui. Nel contesto di un modello SEM, i residui sono derivati dalle differenze tra la matrice di correlazioni (o covarianze) osservata e quella prevista dal modello. Queste differenze sono elaborate attraverso specifiche funzioni per generare i residui. Utilizzando il pacchetto lavaan in R, possiamo accedere a tre principali tipi di residui: standardized.mplus, normalized, e cor.bollen.\n\nResidui Standardizzati (Standardized.mplus): Questo tipo di residuo è una versione standardizzata dei residui. I residui standardizzati sono ottenuti calcolando la differenza tra i valori osservati e quelli previsti dal modello, e dividendo questa differenza per uno stimatore della deviazione standard del residuo. Questo processo trasforma i residui in una scala in cui hanno una varianza approssimativamente uguale. I residui standardizzati sono utili per identificare punti dati che il modello non riesce a spiegare bene. In lavaan, type = \"standardized.mplus\" si riferisce a una particolare forma di standardizzazione dei residui, simile a quella utilizzata nel software Mplus.\nResidui Normalizzati (Normalized): I residui normalizzati sono un altro tipo di residui standardizzati. Sono calcolati come i residui standardizzati ma poi vengono normalizzati. La normalizzazione qui significa che i residui vengono ulteriormente trasformati in modo che la loro distribuzione si avvicini a una distribuzione normale. Questo è utile per verificare se i residui seguono una distribuzione normale, il che è un’assunzione comune in molti modelli statistici, inclusi quelli SEM.\nCorrelazione dei Residui secondo Bollen (Cor.bollen): Questo tipo di residuo si riferisce alla correlazione tra i residui di due variabili diverse nel modello. Il metodo cor.bollen calcola la correlazione tra i residui dopo che il modello è stato adattato ai dati. Questo tipo di analisi è utile per rilevare se ci sono correlazioni non modellate tra le variabili che potrebbero influenzare la validità del modello.\n\n\nlavaan::residuals(somatic, type = \"standardized.mplus\") \n#&gt; $type\n#&gt; [1] \"standardized.mplus\"\n#&gt; \n#&gt; $cov\n#&gt;             morale reltns symptm    ses dysfnc\n#&gt; morale       0.000                            \n#&gt; relations    0.000  0.000                     \n#&gt; symptoms     0.000  0.427  0.000              \n#&gt; ses          0.000 -1.776  4.542  0.000       \n#&gt; dysfunction  0.000 -3.291  2.549  0.000  0.000\n\n\nlavaan::residuals(somatic, type = \"normalized\") \n#&gt; $type\n#&gt; [1] \"normalized\"\n#&gt; \n#&gt; $cov\n#&gt;             morale reltns symptm   ses dysfnc\n#&gt; morale        0.00                           \n#&gt; relations     0.00   0.00                    \n#&gt; symptoms      0.00   0.30   0.00             \n#&gt; ses           0.00  -1.42   3.71  0.00       \n#&gt; dysfunction   0.00  -2.77   2.14  0.00   0.00\n\n\nlavaan::residuals(somatic, type = \"cor.bollen\")\n#&gt; $type\n#&gt; [1] \"cor.bollen\"\n#&gt; \n#&gt; $cov\n#&gt;             morale reltns symptm    ses dysfnc\n#&gt; morale       0.000                            \n#&gt; relations    0.000  0.000                     \n#&gt; symptoms     0.000  0.014  0.000              \n#&gt; ses          0.000 -0.066  0.181  0.000       \n#&gt; dysfunction  0.000 -0.128  0.100  0.000  0.000\n\nAdattiamo ai dati il modello medico convenzionale.\n\nmedical &lt;- lavaan::sem(medical.model,\n    sample.cov = romney.cov,\n    sample.nobs = 469, fixed.x = FALSE, sample.cov.rescale = FALSE\n)\n\n\nsemPaths(medical,\n    whatLabels = \"std\",\n    sizeMan = 10,\n    edge.label.cex = 1.15,\n    style = \"mx\",\n    nCharNodes = 0, nCharEdges = 0\n)\n\n\n\n\n\n\n\n\nlavaan::summary(medical, fit.measures = TRUE, rsquare = TRUE)  \n#&gt; lavaan 0.6-19 ended normally after 1 iteration\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        12\n#&gt; \n#&gt;   Number of observations                           469\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                 3.245\n#&gt;   Degrees of freedom                                 3\n#&gt;   P-value (Chi-square)                           0.355\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               400.859\n#&gt;   Degrees of freedom                                 9\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.999\n#&gt;   Tucker-Lewis Index (TLI)                       0.998\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -8554.222\n#&gt;   Loglikelihood unrestricted model (H1)      -8552.599\n#&gt;                                                       \n#&gt;   Akaike (AIC)                               17132.444\n#&gt;   Bayesian (BIC)                             17182.251\n#&gt;   Sample-size adjusted Bayesian (SABIC)      17144.166\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.013\n#&gt;   90 Percent confidence interval - lower         0.000\n#&gt;   90 Percent confidence interval - upper         0.080\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.742\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.050\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.016\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Regressions:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;   ses ~                                               \n#&gt;     symptoms          0.448    0.063    7.110    0.000\n#&gt;     dysfunction       0.221    0.055    4.019    0.000\n#&gt;   morale ~                                            \n#&gt;     symptoms          0.107    0.009   11.756    0.000\n#&gt;     ses               0.021    0.006    3.291    0.001\n#&gt;   relations ~                                         \n#&gt;     dysfunction      -0.024    0.007   -3.335    0.001\n#&gt;     morale            0.504    0.037   13.745    0.000\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;   symptoms ~~                                         \n#&gt;     dysfunction      59.670   15.553    3.836    0.000\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;    .ses             521.598   34.062   15.313    0.000\n#&gt;    .morale            9.884    0.645   15.313    0.000\n#&gt;    .relations         8.732    0.570   15.313    0.000\n#&gt;     symptoms        289.000   18.872   15.313    0.000\n#&gt;     dysfunction     380.250   24.831   15.313    0.000\n#&gt; \n#&gt; R-Square:\n#&gt;                    Estimate\n#&gt;     ses               0.145\n#&gt;     morale            0.297\n#&gt;     relations         0.290\n\n\nlavaan::residuals(medical, type = \"standardized.mplus\")\n#&gt; $type\n#&gt; [1] \"standardized.mplus\"\n#&gt; \n#&gt; $cov\n#&gt;                ses morale reltns symptm dysfnc\n#&gt; ses          0.000                            \n#&gt; morale       0.000  0.000                     \n#&gt; relations   -1.161 -1.818     NA              \n#&gt; symptoms     0.000  0.000  0.833  0.000       \n#&gt; dysfunction  0.000  0.842  0.862  0.000  0.000\n\n\nlavaan::residuals(medical, type = \"normalized\") \n#&gt; $type\n#&gt; [1] \"normalized\"\n#&gt; \n#&gt; $cov\n#&gt;                ses morale reltns symptm dysfnc\n#&gt; ses          0.000                            \n#&gt; morale       0.000  0.000                     \n#&gt; relations   -0.901 -0.080 -0.069              \n#&gt; symptoms     0.000  0.000  0.573  0.000       \n#&gt; dysfunction  0.000  0.680  0.370  0.000  0.000\n\n\nlavaan::residuals(medical, type = \"cor.bollen\") \n#&gt; $type\n#&gt; [1] \"cor.bollen\"\n#&gt; \n#&gt; $cov\n#&gt;                ses morale reltns symptm dysfnc\n#&gt; ses          0.000                            \n#&gt; morale       0.000  0.000                     \n#&gt; relations   -0.041 -0.003  0.000              \n#&gt; symptoms     0.000  0.000  0.028  0.000       \n#&gt; dysfunction  0.000  0.032  0.017  0.000  0.000\n\nIn conclusione, i valori i valori degli indici di adattamento predittivo per i due modelli alternativi di Romney et al. (1992) sono stati generati seguendo le istruzioni precedentemente descritte. Non sorprende che l’adattamento globale del modello medico convenzionale, più complesso (con $ dfM = 3 $), sia migliore rispetto a quello del modello psicosomatico, più semplice (con $ dfM = 5 $). Nonostante il modello medico convenzionale sia più complesso e quindi soggetto a una penalità maggiore per il minor numero di gradi di libertà, i valori ottenuti sia nell’AIC che nel BIC sono inferiori rispetto a quelli del modello psicosomatico. Questo indica che il vantaggio in termini di adattamento del modello medico convenzionale è sufficiente a superare la penalità per la sua maggiore complessità. In base a queste analisi, il modello medico convenzionale è considerato più adatto rispetto al modello psicosomatico, come evidenziato dai valori più bassi nei criteri AIC e BIC.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html#indici-di-modifica-e-statistiche-correlate-nel-sem",
    "href": "chapters/sem/04_mod_comp.html#indici-di-modifica-e-statistiche-correlate-nel-sem",
    "title": "51  Confronto tra modelli",
    "section": "\n51.10 Indici di Modifica e Statistiche Correlate nel SEM",
    "text": "51.10 Indici di Modifica e Statistiche Correlate nel SEM\nNell’elaborazione di modelli SEM (Structural Equation Modeling), esiste la possibilità di implementare modifiche attraverso processi automatizzati. Questi interventi, di carattere esplorativo, si basano sull’aggiunta o sulla rimozione di parametri seguendo criteri empirici, come la significatività statistica di un indice di modifica (MI) o di un test di punteggio. Gli indici di modifica, in particolare, sono calcolati per quei parametri che nel modello sono stati inizialmente vincolati e servono a stimare quanto il chi-quadrato del modello di massima verosimiglianza (chiML) si ridurrebbe se un dato parametro vincolato venisse liberato.\nIl meccanismo di modifica automatica opera liberando, ad ogni iterazione, il parametro vincolato che presenta il valore di MI più alto. Questo processo continua finché non si raggiunge un MI che è statisticamente significativo secondo i criteri stabiliti dal ricercatore. È cruciale, tuttavia, riconoscere che l’uso di questa metodologia, specialmente in campioni di piccole dimensioni, può portare alla formulazione di modelli che si basano eccessivamente sul caso. Di conseguenza, questi modelli potrebbero risultare poco robusti e difficilmente replicabili in studi successivi. L’automazione nel processo di ottimizzazione dei modelli SEM richiede dunque un’attenta valutazione del contesto e della dimensione del campione per garantire l’affidabilità e la validità dei risultati ottenuti.\nUn altro strumento di uso frequente nei SEM è il test di Wald, basato sulla statistica W. Questo test è progettato per valutare l’impatto che avrebbe la fissazione a zero di un parametro precedentemente stimato liberamente nel modello. In termini più tecnici, il test di Wald stima l’incremento che si verificherebbe nel chi-quadrato del modello di massima verosimiglianza (chiML) se tale parametro fosse “potato”, ovvero escluso dal modello.\nSimilmente ai processi di modifica automatica, l’efficacia del test di Wald può essere influenzata dalla casualità. Un aspetto cruciale da considerare è la sensibilità di questi test alla dimensione del campione. Infatti, anche piccole modifiche nella bontà di adattamento del modello possono assumere una significatività statistica notevole in campioni di ampie dimensioni.\nDi conseguenza, quando si valutano gli indici di modifica come il MI (Modifica Index), è essenziale che il ricercatore non si limiti a considerarne la sola significatività statistica. È importante anche valutare l’entità del cambiamento che si verificherebbe nel coefficiente del parametro se fosse liberato. Se il cambiamento previsto è minimo, la significatività statistica dell’indice di modifica potrebbe essere più indicativa della dimensione del campione che non della sostanziale rilevanza dell’effetto analizzato. Questa considerazione sottolinea l’importanza di un approccio olistico e critico nell’interpretazione dei risultati dei test diagnostici in SEM, specialmente in contesti dove la dimensione del campione può influenzare significativamente i risultati.\n\nfit_mdd &lt;- cfa(\n    model_mdd,\n    data = d_mdd\n)\n\nCalcoliamo gli indici di modifica.\n\nmodification_indices &lt;- lavInspect(fit_mdd, \"mi\")\nmodification_indices \n#&gt;     lhs op  rhs     mi    epc sepc.lv sepc.all sepc.nox\n#&gt; 20 mdd1 ~~ mdd2 46.877  0.732   0.732    0.560    0.560\n#&gt; 21 mdd1 ~~ mdd3  4.996 -0.211  -0.211   -0.108   -0.108\n#&gt; 22 mdd1 ~~ mdd4  4.625 -0.215  -0.215   -0.108   -0.108\n#&gt; 23 mdd1 ~~ mdd5  7.797 -0.266  -0.266   -0.137   -0.137\n#&gt; 24 mdd1 ~~ mdd6  6.434 -0.236  -0.236   -0.132   -0.132\n#&gt; 25 mdd1 ~~ mdd7  0.293  0.053   0.053    0.026    0.026\n#&gt; 26 mdd1 ~~ mdd8  5.046 -0.219  -0.219   -0.116   -0.116\n#&gt; 27 mdd1 ~~ mdd9  5.385  0.187   0.187    0.111    0.111\n#&gt; 28 mdd2 ~~ mdd3  1.435 -0.138  -0.138   -0.055   -0.055\n#&gt; 29 mdd2 ~~ mdd4 10.931 -0.402  -0.402   -0.158   -0.158\n#&gt; 30 mdd2 ~~ mdd5  5.835 -0.281  -0.281   -0.113   -0.113\n#&gt; 31 mdd2 ~~ mdd6  2.235 -0.169  -0.169   -0.074   -0.074\n#&gt; 32 mdd2 ~~ mdd7  0.394 -0.076  -0.076   -0.029   -0.029\n#&gt; 33 mdd2 ~~ mdd8  0.027 -0.019  -0.019   -0.008   -0.008\n#&gt; 34 mdd2 ~~ mdd9  2.070 -0.142  -0.142   -0.066   -0.066\n#&gt; 35 mdd3 ~~ mdd4 16.199  0.591   0.591    0.155    0.155\n#&gt; 36 mdd3 ~~ mdd5  3.298  0.258   0.258    0.069    0.069\n#&gt; 37 mdd3 ~~ mdd6  6.361  0.337   0.337    0.098    0.098\n#&gt; 38 mdd3 ~~ mdd7  0.499 -0.106  -0.106   -0.027   -0.027\n#&gt; 39 mdd3 ~~ mdd8  0.014 -0.017  -0.017   -0.005   -0.005\n#&gt; 40 mdd3 ~~ mdd9  2.860 -0.208  -0.208   -0.064   -0.064\n#&gt; 41 mdd4 ~~ mdd5 12.198  0.509   0.509    0.136    0.136\n#&gt; 42 mdd4 ~~ mdd6 17.435  0.573   0.573    0.165    0.165\n#&gt; 43 mdd4 ~~ mdd7  1.272 -0.174  -0.174   -0.043   -0.043\n#&gt; 44 mdd4 ~~ mdd8  0.975  0.143   0.143    0.039    0.039\n#&gt; 45 mdd4 ~~ mdd9  1.879 -0.173  -0.173   -0.053   -0.053\n#&gt; 46 mdd5 ~~ mdd6  7.502  0.364   0.364    0.107    0.107\n#&gt; 47 mdd5 ~~ mdd7  0.096  0.046   0.046    0.012    0.012\n#&gt; 48 mdd5 ~~ mdd8  4.217  0.288   0.288    0.080    0.080\n#&gt; 49 mdd5 ~~ mdd9  0.544 -0.090  -0.090   -0.028   -0.028\n#&gt; 50 mdd6 ~~ mdd7  2.046 -0.201  -0.201   -0.055   -0.055\n#&gt; 51 mdd6 ~~ mdd8  0.877  0.124   0.124    0.037    0.037\n#&gt; 52 mdd6 ~~ mdd9  2.479 -0.180  -0.180   -0.061   -0.061\n#&gt; 53 mdd7 ~~ mdd8  0.188  0.064   0.064    0.017    0.017\n#&gt; 54 mdd7 ~~ mdd9 13.527  0.474   0.474    0.139    0.139\n#&gt; 55 mdd8 ~~ mdd9  0.322  0.069   0.069    0.022    0.022\n\nNel modello che stiamo analizzando, l’indice di modifica (MI) più elevato si riferisce alla possibile modifica del parametro che governa la correlazione tra i residui degli indicatori mm1 e mm2. Nel modello model_mdd questa correlazione residua è impostata a zero, indicando l’assenza di una correlazione diretta tra questi residui. Tuttavia, l’indice di modifica suggerisce che se permettessimo a questa correlazione di essere stimata liberamente dal modello (anziché tenerla fissa a zero), si verificherebbe un miglioramento dell’adattamento del modello ai dati osservati. Questa osservazione è in linea con il modello alternativo che è stato proposto per questi dati proposto da Brown (2015).\nIncorporare una correlazione residua tra mm1 e mm2 significa riconoscere che, oltre alla variazione spiegata dalle variabili latenti comuni, esiste una relazione unica tra questi due indicatori che non è catturata dal modello. Tale relazione potrebbe essere dovuta a fattori specifici relativi a questi indicatori o a una misurazione comune non prevista dal modello originale.\nÈ importante sottolineare che ogni modifica al modello basata sugli indici di modifica dovrebbe essere attentamente valutata per assicurarsi che sia supportata sia da giustificazioni teoriche che empiriche. Aggiungere correlazioni residue può migliorare l’adattamento del modello, ma dovrebbe essere fatto con cautela per evitare di creare un modello eccessivamente complesso che potrebbe non essere generalizzabile al di fuori del campione di dati specifico utilizzato.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html#riflessioni-conclusive",
    "href": "chapters/sem/04_mod_comp.html#riflessioni-conclusive",
    "title": "51  Confronto tra modelli",
    "section": "\n51.11 Riflessioni Conclusive",
    "text": "51.11 Riflessioni Conclusive\nNel campo dei modelli SEM, è pratica comune selezionare il modello più appropriato da un insieme di alternative, tutte calibrate sugli stessi dati. È frequente il confronto tra modelli gerarchicamente collegati, in cui il modello più restrittivo è incluso, o annidato, in quello meno restrittivo. In queste situazioni, il test di differenza del chi-quadrato viene impiegato per valutare se i modelli hanno un adattamento equivalente. Utilizzare questo test e le statistiche diagnostiche correlate, come gli indici di modifica, richiede un approccio guidato dalla teoria, non solo da criteri empirici. Un eccessivo affidamento su criteri puramente empirici, come la significatività statistica, può portare a una dipendenza eccessiva dal caso.\nPer confrontare modelli non annidati, il test di differenza del chi-quadrato non è idoneo, ma si possono adottare gli indici di adattamento predittivo, basati sulla teoria dell’informazione, per la loro valutazione. Quando si decide di mantenere un modello, è cruciale prendere in considerazione anche altri modelli alternativi potenzialmente equivalenti. È importante fornire motivazioni solide su perché il modello selezionato dal ricercatore sia da preferire rispetto a queste alternative equivalenti.\nQuesto approccio consente una comprensione più profonda e una scelta più informata del modello, assicurando che la selezione sia fondata su basi teoriche solide e non solamente su risultati statistici.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html#session-info",
    "href": "chapters/sem/04_mod_comp.html#session-info",
    "title": "51  Confronto tra modelli",
    "section": "\n51.12 Session Info",
    "text": "51.12 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] effectsize_1.0.0   rsvg_2.6.1         DiagrammeRsvg_0.1 \n#&gt;  [4] mvnormalTest_1.0.0 lavaanPlot_0.8.1   lavaanExtra_0.2.1 \n#&gt;  [7] ggokabeito_0.1.0   see_0.10.0         MASS_7.3-65       \n#&gt; [10] viridis_0.6.5      viridisLite_0.4.2  ggpubr_0.6.0      \n#&gt; [13] ggExtra_0.10.1     gridExtra_2.3      patchwork_1.3.0   \n#&gt; [16] bayesplot_1.11.1   semTools_0.5-6     semPlot_1.1.6     \n#&gt; [19] lavaan_0.6-19      psych_2.4.12       scales_1.3.0      \n#&gt; [22] markdown_1.13      knitr_1.49         lubridate_1.9.4   \n#&gt; [25] forcats_1.0.0      stringr_1.5.1      dplyr_1.1.4       \n#&gt; [28] purrr_1.0.4        readr_2.1.5        tidyr_1.3.1       \n#&gt; [31] tibble_3.2.1       ggplot2_3.5.1      tidyverse_2.0.0   \n#&gt; [34] here_1.0.1        \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] splines_4.4.2       later_1.4.1         datawizard_1.0.0   \n#&gt;   [4] XML_3.99-0.18       rpart_4.1.24        lifecycle_1.0.4    \n#&gt;   [7] Rdpack_2.6.2        rstatix_0.7.2       rprojroot_2.0.4    \n#&gt;  [10] lattice_0.22-6      insight_1.0.2       rockchalk_1.8.157  \n#&gt;  [13] backports_1.5.0     magrittr_2.0.3      openxlsx_4.2.8     \n#&gt;  [16] Hmisc_5.2-2         rmarkdown_2.29      httpuv_1.6.15      \n#&gt;  [19] qgraph_1.9.8        zip_2.3.2           pbapply_1.7-2      \n#&gt;  [22] minqa_1.2.8         RColorBrewer_1.1-3  ADGofTest_0.3      \n#&gt;  [25] multcomp_1.4-28     abind_1.4-8         quadprog_1.5-8     \n#&gt;  [28] pspline_1.0-21      nnet_7.3-20         TH.data_1.1-3      \n#&gt;  [31] sandwich_3.1-1      moments_0.14.1      nortest_1.0-4      \n#&gt;  [34] arm_1.14-4          performance_0.13.0  codetools_0.2-20   \n#&gt;  [37] tidyselect_1.2.1    farver_2.1.2        lme4_1.1-36        \n#&gt;  [40] stats4_4.4.2        base64enc_0.1-3     jsonlite_1.9.0     \n#&gt;  [43] Formula_1.2-5       survival_3.8-3      emmeans_1.10.7     \n#&gt;  [46] tools_4.4.2         Rcpp_1.0.14         glue_1.8.0         \n#&gt;  [49] mnormt_2.1.1        xfun_0.51           withr_3.0.2        \n#&gt;  [52] numDeriv_2016.8-1.1 fastmap_1.2.0       boot_1.3-31        \n#&gt;  [55] digest_0.6.37       mi_1.1              timechange_0.3.0   \n#&gt;  [58] R6_2.6.1            mime_0.12           estimability_1.5.1 \n#&gt;  [61] colorspace_2.1-1    gtools_3.9.5        jpeg_0.1-10        \n#&gt;  [64] copula_1.1-5        DiagrammeR_1.0.11   generics_0.1.3     \n#&gt;  [67] data.table_1.17.0   corpcor_1.6.10      htmlwidgets_1.6.4  \n#&gt;  [70] parameters_0.24.1   pkgconfig_2.0.3     sem_3.1-16         \n#&gt;  [73] gtable_0.3.6        pcaPP_2.0-5         htmltools_0.5.8.1  \n#&gt;  [76] carData_3.0-5       png_0.1-8           reformulas_0.4.0   \n#&gt;  [79] rstudioapi_0.17.1   tzdb_0.4.0          reshape2_1.4.4     \n#&gt;  [82] coda_0.19-4.1       visNetwork_2.1.2    checkmate_2.3.2    \n#&gt;  [85] nlme_3.1-167        curl_6.2.1          nloptr_2.1.1       \n#&gt;  [88] zoo_1.8-13          parallel_4.4.2      miniUI_0.1.1.1     \n#&gt;  [91] foreign_0.8-88      pillar_1.10.1       grid_4.4.2         \n#&gt;  [94] vctrs_0.6.5         promises_1.3.2      car_3.1-3          \n#&gt;  [97] OpenMx_2.21.13      xtable_1.8-4        cluster_2.1.8      \n#&gt; [100] htmlTable_2.4.3     evaluate_1.0.3      pbivnorm_0.6.0     \n#&gt; [103] mvtnorm_1.3-3       cli_3.6.4           kutils_1.73        \n#&gt; [106] compiler_4.4.2      rlang_1.1.5         ggsignif_0.6.4     \n#&gt; [109] fdrtool_1.2.18      plyr_1.8.9          stringi_1.8.4      \n#&gt; [112] munsell_0.5.1       gsl_2.1-8           lisrelToR_0.3      \n#&gt; [115] bayestestR_0.15.2   pacman_0.5.1        V8_6.0.1           \n#&gt; [118] Matrix_1.7-2        hms_1.1.3           stabledist_0.7-2   \n#&gt; [121] glasso_1.11         shiny_1.10.0        rbibutils_2.3      \n#&gt; [124] igraph_2.1.4        broom_1.0.7         RcppParallel_5.1.10\n\n\n\n\n\nBrown, T. A. (2015). Confirmatory factor analysis for applied research. Guilford publications.\n\n\nKline, R. B. (2023). Principles and practice of structural equation modeling. Guilford publications.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/05_cfa_mod_comp.html",
    "href": "chapters/sem/05_cfa_mod_comp.html",
    "title": "52  CFA: confronto tra modelli",
    "section": "",
    "text": "52.1 Introduzione\nIn un modello CFA, i parametri possono essere stimati senza vincoli, possono essere fissi o possono essre stimati sulla base di alcuni vincoli. Un parametro libero è sconosciuto e il ricercatore consente all’algoritmo di stima di trovare il suo valore ottimale che, insime agli altri parametri del modello, riduce al minimo le differenze tra le matrici di varianze-covarianze osservate e quelle predette dal modello. Un parametro fisso è pre-specificato dal ricercatore ad un valore specifico, più comunemente 1.0 (ad esempio, per definire la metrica di una variabile latente) o 0 (ad esempio, l’assenza di saturazionoi fattoriali o di covarianze di errore). Come per un parametro libero, anche un parametro vincolato è sconosciuto; tuttavia, un tale parametro non può assumere un valore qualsiasi, ma deve rispettare le restrizioni su suoi valori che il ricercatore ha imposto. I vincoli più comuni sono i vincoli di uguaglianza, in cui i parametri non standardizzati devono assumere valori uguali (ad esempio, in diversi gruppi).\nConsideriamo un esempio discusso da Brown (2015). Viene qui esaminato un set di dati in cui le prime tre misure osservate (X1, X2, X3) sono indicatori di un costrutto latente corrispondente alla Memoria uditiva e il secondo insieme di misure (X4, X5, X6) sono indicatori di un altro costrutto latente, Memoria visiva. Le tre misure usate quali indicatori del costrutto di memoria uditiva sono:\nle tre misure usate come indicatori del costrutto di memoria visiva sono:\nI dati sono i seguenti:\nsds &lt;- '2.610  2.660  2.590  1.940  2.030  2.050'\n\ncors &lt;-'\n  1.000\n  0.661  1.000\n  0.630  0.643  1.000\n  0.270  0.300  0.268  1.000\n  0.297  0.265  0.225  0.805  1.000\n  0.290  0.287  0.248  0.796  0.779  1.000'\n\ncovs &lt;- getCov(cors, sds = sds, names = paste(\"x\", 1:6, sep = \"\"))\nprint(covs)\n#&gt;      x1   x2   x3   x4   x5   x6\n#&gt; x1 6.81 4.59 4.26 1.37 1.57 1.55\n#&gt; x2 4.59 7.08 4.43 1.55 1.43 1.57\n#&gt; x3 4.26 4.43 6.71 1.35 1.18 1.32\n#&gt; x4 1.37 1.55 1.35 3.76 3.17 3.17\n#&gt; x5 1.57 1.43 1.18 3.17 4.12 3.24\n#&gt; x6 1.55 1.57 1.32 3.17 3.24 4.20\nAdattiamo i cinque modelli discussi da Brown (2015).",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>CFA: confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/05_cfa_mod_comp.html#introduzione",
    "href": "chapters/sem/05_cfa_mod_comp.html#introduzione",
    "title": "52  CFA: confronto tra modelli",
    "section": "",
    "text": "X1 = memoria logica,\nX2 = associazione verbale a coppie,\nX3 = liste di parole;\n\n\n\nX4 = immagini di facce,\nX5 = foto di famiglia,\nX6 = generiche riproduzioni visive.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>CFA: confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/05_cfa_mod_comp.html#modello-congenerico",
    "href": "chapters/sem/05_cfa_mod_comp.html#modello-congenerico",
    "title": "52  CFA: confronto tra modelli",
    "section": "\n52.2 Modello congenerico",
    "text": "52.2 Modello congenerico\n\nmodel.congeneric &lt;- '\n  auditorymemory =~ x1 + x2 + x3\n  visualmemory   =~ x4 + x5 + x6\n'\n\n\nfit.congeneric &lt;- cfa(\n  model.congeneric, \n  sample.cov = covs, \n  sample.nobs = 200, \n  std.lv = TRUE\n)\n\nL’output si ottiene con:\n\nsummary(\n  fit.congeneric, \n  fit.measures = TRUE, \n  standardized = TRUE, \n  rsquare = TRUE\n)\n#&gt; lavaan 0.6-19 ended normally after 21 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        13\n#&gt; \n#&gt;   Number of observations                           200\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                 4.877\n#&gt;   Degrees of freedom                                 8\n#&gt;   P-value (Chi-square)                           0.771\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               719.515\n#&gt;   Degrees of freedom                                15\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    1.000\n#&gt;   Tucker-Lewis Index (TLI)                       1.008\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -2337.980\n#&gt;   Loglikelihood unrestricted model (H1)      -2335.541\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                4701.959\n#&gt;   Bayesian (BIC)                              4744.837\n#&gt;   Sample-size adjusted Bayesian (SABIC)       4703.652\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.000\n#&gt;   90 Percent confidence interval - lower         0.000\n#&gt;   90 Percent confidence interval - upper         0.057\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.929\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.010\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.012\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Expected\n#&gt;   Information saturated (h1) model          Structured\n#&gt; \n#&gt; Latent Variables:\n#&gt;                     Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   auditorymemory =~                                                      \n#&gt;     x1                 2.101    0.166   12.663    0.000    2.101    0.807\n#&gt;     x2                 2.182    0.168   12.976    0.000    2.182    0.823\n#&gt;     x3                 2.013    0.166   12.124    0.000    2.013    0.779\n#&gt;   visualmemory =~                                                        \n#&gt;     x4                 1.756    0.108   16.183    0.000    1.756    0.907\n#&gt;     x5                 1.795    0.115   15.608    0.000    1.795    0.887\n#&gt;     x6                 1.796    0.117   15.378    0.000    1.796    0.878\n#&gt; \n#&gt; Covariances:\n#&gt;                     Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   auditorymemory ~~                                                      \n#&gt;     visualmemory       0.382    0.070    5.463    0.000    0.382    0.382\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .x1                2.366    0.372    6.365    0.000    2.366    0.349\n#&gt;    .x2                2.277    0.383    5.940    0.000    2.277    0.323\n#&gt;    .x3                2.621    0.373    7.027    0.000    2.621    0.393\n#&gt;    .x4                0.662    0.117    5.668    0.000    0.662    0.177\n#&gt;    .x5                0.877    0.134    6.554    0.000    0.877    0.214\n#&gt;    .x6                0.956    0.139    6.866    0.000    0.956    0.229\n#&gt;     auditorymemory    1.000                               1.000    1.000\n#&gt;     visualmemory      1.000                               1.000    1.000\n#&gt; \n#&gt; R-Square:\n#&gt;                    Estimate\n#&gt;     x1                0.651\n#&gt;     x2                0.677\n#&gt;     x3                0.607\n#&gt;     x4                0.823\n#&gt;     x5                0.786\n#&gt;     x6                0.771\n\nIl diagramma di percorso del modello è il seguente.\n\nsemPaths(\n  fit.congeneric,\n  \"std\",\n  posCol = c(\"black\"),\n  edge.label.cex = 1.2,\n  sizeMan = 7,\n  edge.width = 0.4, # Set a fixed width for all arrows\n  fade = FALSE # Disable fading of the arrows\n)",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>CFA: confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/05_cfa_mod_comp.html#modello-tau-equivalente",
    "href": "chapters/sem/05_cfa_mod_comp.html#modello-tau-equivalente",
    "title": "52  CFA: confronto tra modelli",
    "section": "\n52.3 Modello tau-equivalente",
    "text": "52.3 Modello tau-equivalente\nSolo memoria auditiva:\n\nmodel.tau.a &lt;- '\n  auditorymemory =~ x1 + v1*x1 + v1*x2 + v1*x3\n  visualmemory   =~ x4 + x5 + x6\n'\n\n\nfit.tau.a &lt;- cfa(\n  model.tau.a, \n  sample.cov = covs, \n  sample.nobs = 200, \n  std.lv = TRUE\n)\n\nMemoria auditiva e visiva:\n\nmodel.tau.av &lt;- '\n  auditorymemory =~ NA*x1 + v1*x1 + v1*x2 + v1*x3\n  visualmemory   =~ NA*x4 + v2*x4 + v2*x5 + v2*x6\n'\n\n\nfit.tau.av &lt;- cfa(\n  model.tau.av, \n  sample.cov = covs, \n  sample.nobs = 200, \n  std.lv = TRUE\n)\n\n\nsemPaths(\n  fit.tau.av,\n  \"std\",\n  posCol = c(\"black\"),\n  edge.label.cex = 1.2,\n  sizeMan = 7,\n  edge.width = 0.4, # Set a fixed width for all arrows\n  fade = FALSE # Disable fading of the arrows\n)",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>CFA: confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/05_cfa_mod_comp.html#modello-parallelo",
    "href": "chapters/sem/05_cfa_mod_comp.html#modello-parallelo",
    "title": "52  CFA: confronto tra modelli",
    "section": "\n52.4 Modello parallelo",
    "text": "52.4 Modello parallelo\nSolo memoria auditiva:\n\nmodel.parallel.a &lt;- '\n  auditorymemory =~ x1 + v1*x1 + v1*x2 + v1*x3\n  visualmemory   =~ x4 + v2*x4 + v2*x5 + v2*x6\n  x1 ~~ v3 * x1\n  x2 ~~ v3 * x2\n  x3 ~~ v3 * x3\n'\n\n\nfit.parallel.a &lt;- cfa(\n  model.parallel.a, \n  sample.cov = covs, \n  sample.nobs = 200, \n  std.lv = TRUE\n)\n\nMemoria auditiva e visiva:\n\nmodel.parallel.av &lt;- '\n  auditorymemory =~ x1 + v1*x1 + v1*x2 + v1*x3\n  visualmemory   =~ x4 + v2*x4 + v2*x5 + v2*x6\n  x1 ~~ v3 * x1\n  x2 ~~ v3 * x2\n  x3 ~~ v3 * x3\n \n  x4 ~~ v4 * x4\n  x5 ~~ v4 * x5\n  x6 ~~ v4 * x6\n'\n\n\nfit.parallel.av &lt;- cfa(\n  model.parallel.av, \n  sample.cov = covs, \n  sample.nobs = 200, \n  std.lv = TRUE\n)\n\n\nsemPaths(\n    fit.parallel.av,\n    \"std\",\n    posCol = c(\"black\"),\n    edge.label.cex = 1.2,\n    sizeMan = 7,\n    edge.width = 0.4, # Set a fixed width for all arrows\n    fade = FALSE # Disable fading of the arrows\n)",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>CFA: confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/05_cfa_mod_comp.html#il-test-del-chi2",
    "href": "chapters/sem/05_cfa_mod_comp.html#il-test-del-chi2",
    "title": "52  CFA: confronto tra modelli",
    "section": "\n52.5 Il test del \\(\\chi^2\\)\n",
    "text": "52.5 Il test del \\(\\chi^2\\)\n\nIl confronto tra modelli nidificati procede attraverso il test \\(\\chi^2\\). Tale test si basa su una proprietà delle variabili casuali distribuite come \\(\\chi^2\\): la differenza tra due v.c. \\(X_1\\) e \\(X_2\\) che seguono la distribuzione \\(\\chi^2\\), rispettivamente con \\(\\nu_1\\) e \\(\\nu_2\\), con \\(\\nu_1 &gt; \\nu_2\\), è una variabile causale che segue la distribuzione \\(\\chi^2\\) con gradi di libertà pari a \\(\\nu_1 - \\nu_2\\).\nUn modello nidificato è un modello che impone dei vincoli sui parametri del modello di partenza. L’imposizione di vincoli sui parametri ha la conseguenza che vi sarà un numero minore di parametri da stimare. Il confronto tra i modelli si esegue valutando in maniera relativa la bontà di adattamento di ciascun modello per mezzo della statistica chi-quadrato. La statistica così calcolata avrà un numero di gradi di libertà uguale alla differenza tra i gradi di libertà dei due modelli.\nNel caso dell’esempio in dicussione, abbiamo\n\nout = anova(\n  fit.congeneric, \n  fit.tau.a, \n  fit.tau.av, \n  fit.parallel.a, \n  fit.parallel.av, \n  test = \"chisq\"\n)\nprint(out)\n#&gt; \n#&gt; Chi-Squared Difference Test\n#&gt; \n#&gt;                 Df  AIC  BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\n#&gt; fit.congeneric   8 4702 4745  4.88                                    \n#&gt; fit.tau.a       10 4699 4735  5.66       0.78 0.000       2       0.68\n#&gt; fit.tau.av      12 4695 4725  5.88       0.22 0.000       2       0.90\n#&gt; fit.parallel.a  14 4691 4714  5.98       0.10 0.000       2       0.95\n#&gt; fit.parallel.av 16 4690 4707  9.28       3.30 0.057       2       0.19\n\nI test precedenti indicano come non vi sia una perdita di adattamento passando dal modello congenerico al modello più restrittivo (ovvero, il modello parallelo per entrambi i fattori). Per questi dati, dunque, può essere adottato il modello più semplice, cioè il modello parallelo.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>CFA: confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/05_cfa_mod_comp.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/sem/05_cfa_mod_comp.html#informazioni-sullambiente-di-sviluppo",
    "title": "52  CFA: confronto tra modelli",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] ggokabeito_0.1.0  see_0.10.0        MASS_7.3-65       viridis_0.6.5    \n#&gt;  [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n#&gt;  [9] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n#&gt; [13] lavaan_0.6-19     psych_2.4.12      scales_1.3.0      markdown_1.13    \n#&gt; [17] knitr_1.49        lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n#&gt; [21] dplyr_1.1.4       purrr_1.0.4       readr_2.1.5       tidyr_1.3.1      \n#&gt; [25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.0      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] mime_0.12           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [25] Matrix_1.7-2        R6_2.6.1            fastmap_1.2.0      \n#&gt;  [28] rbibutils_2.3       shiny_1.10.0        digest_0.6.37      \n#&gt;  [31] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n#&gt;  [34] rprojroot_2.0.4     Hmisc_5.2-2         timechange_0.3.0   \n#&gt;  [37] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n#&gt;  [40] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n#&gt;  [43] carData_3.0-5       ggsignif_0.6.4      corpcor_1.6.10     \n#&gt;  [46] gtools_3.9.5        tools_4.4.2         pbivnorm_0.6.0     \n#&gt;  [49] foreign_0.8-88      zip_2.3.2           httpuv_1.6.15      \n#&gt;  [52] nnet_7.3-20         glue_1.8.0          quadprog_1.5-8     \n#&gt;  [55] nlme_3.1-167        promises_1.3.2      lisrelToR_0.3      \n#&gt;  [58] grid_4.4.2          checkmate_2.3.2     cluster_2.1.8      \n#&gt;  [61] reshape2_1.4.4      generics_0.1.3      gtable_0.3.6       \n#&gt;  [64] tzdb_0.4.0          data.table_1.17.0   hms_1.1.3          \n#&gt;  [67] car_3.1-3           sem_3.1-16          pillar_1.10.1      \n#&gt;  [70] rockchalk_1.8.157   later_1.4.1         splines_4.4.2      \n#&gt;  [73] lattice_0.22-6      survival_3.8-3      kutils_1.73        \n#&gt;  [76] tidyselect_1.2.1    miniUI_0.1.1.1      pbapply_1.7-2      \n#&gt;  [79] reformulas_0.4.0    stats4_4.4.2        xfun_0.51          \n#&gt;  [82] qgraph_1.9.8        arm_1.14-4          stringi_1.8.4      \n#&gt;  [85] pacman_0.5.1        boot_1.3-31         evaluate_1.0.3     \n#&gt;  [88] codetools_0.2-20    mi_1.1              cli_3.6.4          \n#&gt;  [91] RcppParallel_5.1.10 rpart_4.1.24        xtable_1.8-4       \n#&gt;  [94] Rdpack_2.6.2        munsell_0.5.1       Rcpp_1.0.14        \n#&gt;  [97] coda_0.19-4.1       png_0.1-8           XML_3.99-0.18      \n#&gt; [100] parallel_4.4.2      jpeg_0.1-10         lme4_1.1-36        \n#&gt; [103] mvtnorm_1.3-3       openxlsx_4.2.8      rlang_1.1.5        \n#&gt; [106] multcomp_1.4-28     mnormt_2.1.1\n\n\n\n\n\nBrown, T. A. (2015). Confirmatory factor analysis for applied research. Guilford publications.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>CFA: confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html",
    "href": "chapters/sem/06_refine_solution.html",
    "title": "53  La revisione del modello",
    "section": "",
    "text": "53.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nI passi principali nella CFA e nei modelli SEM comprendono la specificazione del modello, la stima dei parametri, la valutazione del modello e dei parametri e la modificazione del modello. Questa sequenza può essere ripetuta molte volte fino a quando non si trovi un modello considerato accettabile.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html#stima-del-modello",
    "href": "chapters/sem/06_refine_solution.html#stima-del-modello",
    "title": "53  La revisione del modello",
    "section": "53.2 Stima del modello",
    "text": "53.2 Stima del modello\nConsideriamo qui un modello SEM con una sola variabile latente identificata da un insieme di indicatori, ovvero un modello CFA. L’obiettivo della CFA è ottenere stime per i parametro del modello (vale a dire, saturazioni fattoriali, varianze e covarianze fattoriali, varianze residue ed eventualmente covarianze degli errori) che sono in grado di produrre una matrice di covarianza prevista (denotata da \\(\\boldsymbol{\\Sigma}\\)) la quale è il più possibile simile alla matrice di covarianze campionarie (denotata da \\(\\boldsymbol{S}\\)). Questo processo di stima è basato sulla minimizzazione di una funzione che descrive la differenza tra \\(\\boldsymbol{\\Sigma}\\) e \\(\\boldsymbol{S}\\). Il metodo di stima più utilizzato nella CFA (e, in generale, nei modelli SEM) è la massima verosimiglianza (ML).",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html#massima-verosimiglianza",
    "href": "chapters/sem/06_refine_solution.html#massima-verosimiglianza",
    "title": "53  La revisione del modello",
    "section": "53.3 Massima verosimiglianza",
    "text": "53.3 Massima verosimiglianza\nL’equazione fondamentale dell’analisi fattoriale è\n\\[\n\\boldsymbol y = \\boldsymbol \\Lambda  \\boldsymbol x  + \\boldsymbol z,\n\\]\ndove \\(\\boldsymbol{y}\\) è un vettore di \\(p\\) componenti (i punteggi osservati nel del test), \\(\\boldsymbol{x}\\) è un vettore di \\(k &lt; p\\) componenti (i punteggi fattoriali), \\(\\boldsymbol{\\Lambda}\\) è una \\(p \\cdot k\\) matrice (di saturazioni fattoriali), e \\(\\boldsymbol{z}\\) è un vettore di \\(p\\) componenti (la componenti dei punteggi del test non dovute all’effetto causale delle variabili comuni latenti). Per l’item \\(i\\)-esimo, in precedenza abbiamo scritto l’equazione precedente come\n\\[\ny_i = \\lambda_{i1} \\xi_1 + \\dots + \\lambda_{ik} \\xi_k + \\delta_i.\n\\]\nDalle assunzioni del modello fattoriale deriva che\n\\[\n\\boldsymbol{\\Sigma} = \\boldsymbol{\\Lambda}\\boldsymbol{\\Phi}\\boldsymbol{\\Lambda}^\\prime + \\Psi,\n\\]\ndove \\(\\boldsymbol{\\Phi}\\) è la matrice delle inter-correlazioni fattoriali.\nSi assume che il vettore casuale \\(\\boldsymbol{y}\\) abbia una distribuzione normale multivariata con matrice di covarianza \\(\\boldsymbol{\\Sigma}\\) e che da tale distribuzione sia stato estratto un campione casuale di \\(n\\) osservazioni \\(y_l, y_2, \\dots, y_n\\). Il logaritmo della funzione di verosimiglianza per il campione è dato da\n\\[\n\\log L = \\frac{1}{2}n [\\log | \\boldsymbol{\\Sigma}| + tr(\\boldsymbol{\\boldsymbol{S} \\Sigma}^{-1})].\n\\]\nL’equazione precedente viene vista come funzione di \\(\\Lambda\\) e \\(\\Psi\\). Anziché massimizzare \\(\\log L\\), è equivalente e più conveniente minimizzare\n\\[\nF_{k}(\\Lambda, \\Psi) = \\log |\\boldsymbol{\\Sigma}| + tr[\\boldsymbol{S}\\boldsymbol{\\Sigma}^{-1}]  - \\log|\\boldsymbol{S}| – p,\n\\]\ndove \\(|\\boldsymbol{S}|\\) è il determinante della matrice di covarianza tra le variabili osservate, \\(|\\boldsymbol{\\Sigma}|\\) è il determinante della matrice di covarianza prevista e \\(p\\) è il numero di indicatori.\nL’obiettivo della stima di massima verosimiglianza della CFA è trovare le stime dei parametri che rendono più verosimili i dati osservati (o, al contrario, massimizzano la verosimiglianza dei parametri dati i dati). Le stime dei parametri in un modello CFA si ottengono con una procedura iterativa. Cioè, l’algoritmo inizia con una serie iniziale di stime dei parametri (denominate valori iniziali o stime iniziali, che possono essere generate automaticamente dal software o specificate dall’utente) e raffina ripetutamente queste stime nel tentativo di minimizzare la differenza tra \\(\\boldsymbol{\\Sigma}\\) e \\(\\boldsymbol{S}\\). Il programma effettua controlli interni per valutare i suoi progressi nell’ottenere stime dei parametri che al meglio riproducono \\(\\boldsymbol{S}\\). Si raggiunge la convergenza quando l’algoritmo produce una serie di stime dei parametri che non possono essere ulteriormente migliorate per ridurre la differenza tra \\(\\boldsymbol{\\Sigma}\\) e \\(\\boldsymbol{S}\\).",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html#identificabilità-del-modello",
    "href": "chapters/sem/06_refine_solution.html#identificabilità-del-modello",
    "title": "53  La revisione del modello",
    "section": "53.4 Identificabilità del modello",
    "text": "53.4 Identificabilità del modello\nUn modello CFA deve essere formulato in modo tale da garantire la risolvibilità matematica dello stesso, ovvero deve essere tale da consentire una stima univoca dei parametri del modello. Detto in altre parole, la specificazione del modello ne deve garantire l’dentificabilità.\nIl problema dell’identificazione richiede, innanzitutto, di chiarire il concetto di gradi di libertà (degrees of freedom). Nel presente contesto, per gradi di libertà (\\(dof\\)) intendiamo\n\\[\ndof = \\# (\\text{unità di informazione}) - \\# (\\text{parametri da stimare}).\n\\]\nI dati che vengono analizzati da un modello CFA sono contenuti in una matrice di covarianza. Per una matrice di covarianza di ordine \\(p\\), il numero di unità di informazione è\n\\[\n\\frac{p (p+1)}{2}.\n\\]\nAffinché il modello sia identificabile, devono essere soddisfatte le seguenti condizioni.\n\nIndipendentemente dalla complessità del modello (ad es. modelli ad un fattore rispetto a più fattori), l’unità di misura delle variabili latenti deve essere specificata (di solito fissandola a un valore di 1);\nIndipendentemente dalla complessità del modello, il numero di unità di informazione (es. la matrice di covarianza degli indicatori) deve essere uguale o superiore al numero di parametri da stimare (es. saturazioni fattoriali, specificità, covarianze degli errori dell’indicatore, covarianze tra i fattori);\nNel caso di modelli ad un fattore è richiesto un minimo di tre indicatori. Quando vengono utilizzati tre indicatori, la soluzione a un fattore si dice “appena identificata” (just-identified); in tali condizioni non è possibile valutare la bontà dell’adattamento.\nNel caso di modelli a due o più fattori e due indicatori per costrutto latente, la soluzione è sovraidentificata, a condizione che ogni variabile latente sia correlata con almeno un’altra variabile latente e gli errori tra gli indicatori siano tra loro incorrelati. Tuttavia, poiché tali soluzioni sono suscettibili di scarsa identificazione empirica, viene raccomandato un minimo di tre indicatori per variabile latente.\n\nIn conclusione, una semplice e necessaria condizione per l’identificazione di un modello CFA è che vi siano più unità di informazione che parametri da stimare. Dunque, abbiamo che:\n\nse \\(dof &lt; 0\\), il modello non è identificato e, in questo caso, non è possibile stimare i parametri;\nse \\(dof = 0\\), il modello è appena identificato o “saturo”; in questo caso, la matrice di covarianza riprodotta coincide con la matrice di covarianza delle variabili osservate e, di conseguenza, non esiste un residuo attraverso cui valutare la bontà dell’adattamento del modello;\nse \\(dof &gt; 0\\), il modello è sovra-identificato ed esistono le condizioni per valutare la bontà dell’adattamento.\n\nLe considerazioni precedenti ci fanno capire perché non si può fare un’analisi fattoriale con solo due indicatori e un fattore; in tali circostanze, infatti, ci sono \\((2 \\cdot 3)/2 = 3\\) gradi di libertà, ma 4 parametri da stimare (due saturazioni fattoriali e due specificità). Il caso di tre item e un fattore definisce un modello “appena identificato”, ovvero, il caso in cui ci sono zero gradi di libertà. In tali circostanze è possibile stimare i parametri (ricordiamo il metodo dell’annullamento della tetrade), ma non è possibile un test di bontà dell’adattamento. Questo vuol dire, in pratica, che per un modello SEM ad un solo fattore comune latente è necessario disporre di almeno quattro indicatori.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html#un-esempio-concreto",
    "href": "chapters/sem/06_refine_solution.html#un-esempio-concreto",
    "title": "53  La revisione del modello",
    "section": "53.5 Un Esempio Concreto",
    "text": "53.5 Un Esempio Concreto\nNell’approfondire la tematica dei Modelli di Equazioni Strutturali, è utile considerare alcune problematiche comuni che possono emergere nella fase di adattamento del modello ai dati. Facendo riferimento agli esempi discussi da Brown (2015) nel contesto dell’analisi fattoriale confermativa (CFA), possiamo identificare diverse potenziali cause di inadeguato adattamento. Queste cause possono essere di natura sia teorica che tecnica e spesso richiedono un’attenta riflessione e analisi per essere risolte. Esaminiamo alcune delle questioni più rilevanti:\n\nNumero Errato di Fattori Comuni Latenti:\n\nUno degli errori più comuni è ipotizzare un numero di fattori latenti che non riflette adeguatamente la struttura sottostante dei dati. Un numero insufficiente di fattori può portare a un modello semplificato eccessivamente, mentre un numero eccessivo può causare sovra-aggiustamento e complessità non necessaria.\n\nItem che Saturano su Fattori Multipli:\n\nIn alcuni casi, un item può essere erroneamente ipotizzato per saturare su un singolo fattore comune, mentre in realtà ha relazioni significative con più fattori. Questo errore nella specificazione del modello può portare a stime imprecise e a un adattamento inadeguato.\n\nAssegnazione Errata degli Item ai Fattori:\n\nUn’altra possibile causa di inadeguato adattamento riguarda l’errata assegnazione di un item al fattore comune sbagliato. Tale errore può derivare da una comprensione insufficiente delle dimensioni teoriche che si stanno misurando o da una cattiva interpretazione dei dati empirici.\n\nCorrelazioni Residue Non Considerate:\n\nInfine, le correlazioni residue non incorporate nel modello possono giocare un ruolo significativo nell’adattamento del modello. Queste correlazioni possono indicare relazioni non catturate dai fattori comuni, suggerendo la necessità di rivedere l’ipotesi di base del modello o di aggiungere percorsi specifici per accomodare queste correlazioni.\n\n\nIn sintesi, l’adattamento del modello SEM ai dati è un processo complesso che richiede una profonda comprensione sia della teoria sottostante che della natura dei dati. Ogni volta che un modello non si adatta adeguatamente, è essenziale esaminare criticamente questi e altri potenziali fattori per identificare e correggere le cause alla base di tale inadeguatezza. Questo processo non solo migliora l’adattamento del modello, ma può anche fornire intuizioni preziose sulla struttura dei dati e sulla validità delle teorie sottostanti.\nBrown (2015) mostra come il ricercatore possa usare i Modification Indices per valutare le cause del mancato adattamento del modello ai dati. I Modification Indices sono una misura utilizzata per identificare le covariate tra le variabili del modello che potrebbero migliorare l’aderenza del modello ai dati. I modification indices indicano quale sarebbe il miglioramento nell’aderenza del modello, ad esempio, se venisse permessa la correlazione tra due variabili che attualmente non sono considerate correlate. Ciò consente di identificare le relazioni nascoste tra le variabili e può aiutare a migliorare la precisione e l’accuratezza del modello.\nTuttavia, è importante tenere presente che i modification indices da soli non dovrebbero essere usati per prendere decisioni definitive sulle modifiche del modello. Invece, dovrebbero essere considerati insieme ad altre informazioni, come la conoscenza teorica, l’esperienza e altre tecniche di analisi dei dati per determinare se una modifica del modello è giustificata e in che modo.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html#un-numero-di-fattori-troppo-piccolo",
    "href": "chapters/sem/06_refine_solution.html#un-numero-di-fattori-troppo-piccolo",
    "title": "53  La revisione del modello",
    "section": "53.6 Un numero di fattori troppo piccolo",
    "text": "53.6 Un numero di fattori troppo piccolo\nUna delle possibili fonti di mancanza di adattamento del modello può dipendere dal fatto che è stato ipotizzato un numero insufficiente di fattori latenti comuni. Brown (2015) discute il caso nel quale si confrontano gli indici di bontà di adattamento di un modello ad un solo fattore comune e un modello a due fattori comuni. L’esempio riguarda i dati già in precedenza discussi e relativi relativi a otto misure di personalità raccolte su un campione di 250 pazienti che hanno concluso un programma di psicoterapia. Le scale sono le seguenti:\n\nanxiety (N1),\nhostility (N2),\ndepression (N3),\nself-consciousness (N4),\nwarmth (E1),\ngregariousness (E2),\nassertiveness (E3),\npositive emotions (E4).\n\nLeggiamo i dati in \\(\\mathsf{R}\\).\n\nvarnames &lt;- c(\"N1\", \"N2\", \"N3\", \"N4\", \"E1\", \"E2\", \"E3\", \"E4\")\n\nsds &lt;- c(5.7,  5.6,  6.4,  5.7,  6.0,  6.2,  5.7,  5.6)\n\ncors &lt;- '\n 1.000\n 0.767  1.000 \n 0.731  0.709  1.000 \n 0.778  0.738  0.762  1.000 \n-0.351  -0.302  -0.356  -0.318  1.000 \n-0.316  -0.280  -0.300  -0.267  0.675  1.000 \n-0.296  -0.289  -0.297  -0.296  0.634  0.651  1.000 \n-0.282  -0.254  -0.292  -0.245  0.534  0.593  0.566  1.000'\n\npsychot_cor_mat &lt;- getCov(cors, names = varnames)\n\nn &lt;- 250\n\nSupponiamo di adattare ai dati il modello “sbagliato” che include un unico fattore comune. Svolgiamo qui l’analisi fattoriale esplorativa usando la funzione sperimentale efa() di lavaan.\n\n# 1-factor model\nf1 &lt;- '\n  efa(\"efa\")*f1 =~ N1 + N2 + N3 + N4 + E1 + E2 + E3 + E4\n'\n\nAdattiamo il modello ai dati.\n\nefa_f1 &lt;-\n  cfa(\n    model = f1,\n    sample.cov = psychot_cor_mat,\n    sample.nobs = 250,\n    rotation = \"oblimin\"\n  )\n\nConsideriamo ora un modello a due fattori.\n\nf2 &lt;- '\n  efa(\"efa\")*f1 +\n  efa(\"efa\")*f2 =~ N1 + N2 + N3 + N4 + E1 + E2 + E3 + E4\n'\n\nAdattiamo il modello ai dati.\n\nefa_f2 &lt;-\n  cfa(\n    model = f2,\n    sample.cov = psychot_cor_mat,\n    sample.nobs = 250,\n    rotation = \"oblimin\"\n  )\n\nEsaminiamo gli indici di bontà di adattamento.\n\n# define the fit measures\nfit_measures_robust &lt;- c(\"chisq\", \"df\", \"pvalue\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")\n\n# collect them for each model\nrbind(\n  fitmeasures(efa_f1, fit_measures_robust),\n  fitmeasures(efa_f2, fit_measures_robust)\n) %&gt;%\n  # wrangle\n  data.frame() %&gt;%\n  mutate(\n    chisq = round(chisq, digits = 0),\n    df = as.integer(df),\n    pvalue = ifelse(pvalue == 0, \"&lt; .001\", pvalue)\n  ) %&gt;%\n  mutate_at(vars(cfi:srmr), ~ round(., digits = 3)) |&gt; print()\n\n  chisq df            pvalue  cfi   tli rmsea  srmr\n1   375 20            &lt; .001 0.71 0.594 0.267 0.187\n2    10 13 0.709310449320098 1.00 1.006 0.000 0.010\n\n\n\neffectsize::interpret(efa_f1) |&gt; print()\n\n    Name Value Threshold Interpretation\n1    GFI 0.671      0.95           poor\n2   AGFI 0.408      0.90           poor\n3    NFI 0.701      0.90           poor\n4   NNFI 0.594      0.90           poor\n5    CFI 0.710      0.90           poor\n6  RMSEA 0.267      0.05           poor\n7   SRMR 0.187      0.08           poor\n8    RFI 0.581      0.90           poor\n9   PNFI 0.500      0.50   satisfactory\n10   IFI 0.712      0.90           poor\n\n\n\neffectsize::interpret(efa_f2) |&gt; print()\n\n    Name   Value Threshold Interpretation\n1    GFI 0.99055      0.95   satisfactory\n2   AGFI 0.97384      0.90   satisfactory\n3    NFI 0.99217      0.90   satisfactory\n4   NNFI 1.00560      0.90   satisfactory\n5    CFI 1.00000      0.90   satisfactory\n6  RMSEA 0.00000      0.05   satisfactory\n7   SRMR 0.00991      0.08   satisfactory\n8    RFI 0.98315      0.90   satisfactory\n9   PNFI 0.46065      0.50           poor\n10   IFI 1.00257      0.90   satisfactory\n\n\nI risultati mostrano come, in un modello EFA, una soluzione a due fattori produca un adattamento adeguato, mentre ciò non si verifica con un modello ad un solo fattore.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html#specificazione-errata-delle-relazioni-tra-indicatori-e-fattori-latenti",
    "href": "chapters/sem/06_refine_solution.html#specificazione-errata-delle-relazioni-tra-indicatori-e-fattori-latenti",
    "title": "53  La revisione del modello",
    "section": "53.7 Specificazione errata delle relazioni tra indicatori e fattori latenti",
    "text": "53.7 Specificazione errata delle relazioni tra indicatori e fattori latenti\nUn’altra potenziale fonte di errata specificazione del modello CFA è una designazione errata delle relazioni tra indicatori e fattori latenti.\nIn questo esempio, un ricercatore ha sviluppato un questionario di 12 item (gli item sono valutati su scale da 0 a 8) progettato per valutare le motivazioni dei giovani adulti a consumare bevande alcoliche (Cooper, 1994). La misura aveva lo scopo di valutare tre aspetti di questo costrutto (4 item ciascuno): (1) motivazioni di coping (item 1–4), (2) motivazioni sociali (item 5–8) e (3) motivazioni di miglioramento (item 9 –12). I dati sono i seguenti.\n\nsds &lt;- c(2.06, 1.52, 1.92, 1.41, 1.73, 1.77, 2.49, 2.27, 2.68, 1.75, 2.57, 2.66)\n\ncors &lt;- '\n  1.000 \n  0.300  1.000 \n  0.229  0.261  1.000 \n  0.411  0.406  0.429  1.000 \n  0.172  0.252  0.218  0.481  1.000 \n  0.214  0.268  0.267  0.579  0.484  1.000 \n  0.200  0.214  0.241  0.543  0.426  0.492  1.000 \n  0.185  0.230  0.185  0.545  0.463  0.548  0.522  1.000 \n  0.134  0.146  0.108  0.186  0.122  0.131  0.108  0.151  1.000 \n  0.134  0.099  0.061  0.223  0.133  0.188  0.105  0.170  0.448  1.000 \n  0.160  0.131  0.158  0.161  0.044  0.124  0.066  0.061  0.370  0.350  1.000 \n  0.087  0.088  0.101  0.198  0.077  0.177  0.128  0.112  0.356  0.359  0.507  1.000'\n\ncovs &lt;- getCov(cors, sds = sds, names = paste(\"x\", 1:12, sep = \"\"))\n\nIniziamo con un modello che ipotizza tre fattori comuni latenti correlati, coerentemente con la motivazione che stava alla base della costruzione dello strumento.\n\nmodel1 &lt;- '\n  copingm  =~ x1 + x2 + x3 + x4\n  socialm  =~ x5 + x6 + x7 + x8\n  enhancem =~ x9 + x10 + x11 + x12\n'\n\nAdattiamo il modello ai dati.\n\nfit1 &lt;- cfa(\n  model1, \n  sample.cov = covs, \n  sample.nobs = 500, \n  mimic = \"mplus\"\n)\n\nEsaminando le misure di adattamento potremmo concludere che il modello è adeguato.\n\neffectsize::interpret(fit1) |&gt; print()\n\n    Name  Value Threshold Interpretation\n1    GFI 0.9701      0.95   satisfactory\n2   AGFI 0.9472      0.90   satisfactory\n3    NFI 0.9479      0.90   satisfactory\n4   NNFI 0.9710      0.90   satisfactory\n5    CFI 0.9776      0.90   satisfactory\n6  RMSEA 0.0375      0.05   satisfactory\n7   SRMR 0.0344      0.08   satisfactory\n8    RFI 0.9325      0.90   satisfactory\n9   PNFI 0.7324      0.50   satisfactory\n10   IFI 0.9778      0.90   satisfactory\n\n\nTuttavia, un esame più attento mette in evidenza un comportamento anomalo dell’item x4 e alcune caratteristiche anomale del modello in generale.\n\nstandardizedSolution(fit1) |&gt; print()\n\n        lhs op      rhs est.std    se     z pvalue ci.lower ci.upper\n1   copingm =~       x1   0.432 0.039 11.03   0.00    0.355    0.508\n2   copingm =~       x2   0.436 0.039 11.17   0.00    0.359    0.512\n3   copingm =~       x3   0.451 0.038 11.73   0.00    0.376    0.527\n4   copingm =~       x4   0.953 0.024 38.97   0.00    0.905    1.001\n5   socialm =~       x5   0.633 0.032 20.06   0.00    0.571    0.695\n6   socialm =~       x6   0.748 0.025 29.36   0.00    0.698    0.798\n7   socialm =~       x7   0.690 0.029 24.15   0.00    0.634    0.746\n8   socialm =~       x8   0.729 0.026 27.52   0.00    0.677    0.781\n9  enhancem =~       x9   0.602 0.039 15.58   0.00    0.526    0.678\n10 enhancem =~      x10   0.597 0.039 15.40   0.00    0.521    0.673\n11 enhancem =~      x11   0.661 0.037 17.98   0.00    0.589    0.733\n12 enhancem =~      x12   0.665 0.037 18.17   0.00    0.593    0.737\n13       x1 ~~       x1   0.814 0.034 24.09   0.00    0.747    0.880\n14       x2 ~~       x2   0.810 0.034 23.84   0.00    0.744    0.877\n15       x3 ~~       x3   0.796 0.035 22.94   0.00    0.728    0.864\n16       x4 ~~       x4   0.091 0.047  1.96   0.05    0.000    0.183\n17       x5 ~~       x5   0.599 0.040 14.98   0.00    0.521    0.677\n18       x6 ~~       x6   0.441 0.038 11.57   0.00    0.366    0.515\n19       x7 ~~       x7   0.524 0.039 13.29   0.00    0.447    0.601\n20       x8 ~~       x8   0.469 0.039 12.15   0.00    0.393    0.545\n21       x9 ~~       x9   0.638 0.047 13.71   0.00    0.546    0.729\n22      x10 ~~      x10   0.643 0.046 13.88   0.00    0.552    0.734\n23      x11 ~~      x11   0.563 0.049 11.61   0.00    0.468    0.659\n24      x12 ~~      x12   0.558 0.049 11.45   0.00    0.462    0.653\n25  copingm ~~  copingm   1.000 0.000    NA     NA    1.000    1.000\n26  socialm ~~  socialm   1.000 0.000    NA     NA    1.000    1.000\n27 enhancem ~~ enhancem   1.000 0.000    NA     NA    1.000    1.000\n28  copingm ~~  socialm   0.799 0.031 26.15   0.00    0.739    0.859\n29  copingm ~~ enhancem   0.322 0.051  6.34   0.00    0.222    0.422\n30  socialm ~~ enhancem   0.268 0.056  4.82   0.00    0.159    0.377\n31       x1 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n32       x2 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n33       x3 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n34       x4 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n35       x5 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n36       x6 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n37       x7 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n38       x8 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n39       x9 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n40      x10 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n41      x11 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n42      x12 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n43  copingm ~1            0.000 0.000    NA     NA    0.000    0.000\n44  socialm ~1            0.000 0.000    NA     NA    0.000    0.000\n45 enhancem ~1            0.000 0.000    NA     NA    0.000    0.000\n\n\nIn particolare, l’item x4 mostra una saturazione molto forte sul fattore Motivi di coping (.955) ed emerge una correlazione molto alta tra i fattori Motivi di coping e Motivi sociali (.798).\nBrown (2015) suggerisce di esaminare i Modification Indices. Tale esame mostra che il MI associato a x4 è molto alto, 18.916.\n\nmodindices(fit1) |&gt; print()\n\n         lhs op rhs     mi    epc sepc.lv sepc.all sepc.nox\n46   copingm =~  x5  0.030 -0.030  -0.027   -0.015   -0.015\n47   copingm =~  x6  0.484  0.127   0.113    0.064    0.064\n48   copingm =~  x7  0.780  0.220   0.196    0.079    0.079\n49   copingm =~  x8  1.962 -0.323  -0.287   -0.127   -0.127\n50   copingm =~  x9  0.101  0.044   0.039    0.015    0.015\n51   copingm =~ x10  2.016  0.129   0.114    0.065    0.065\n52   copingm =~ x11  1.870 -0.181  -0.161   -0.063   -0.063\n53   copingm =~ x12  0.040 -0.027  -0.024   -0.009   -0.009\n54   socialm =~  x1  6.927 -0.520  -0.569   -0.277   -0.277\n55   socialm =~  x2  0.052 -0.033  -0.036   -0.024   -0.024\n56   socialm =~  x3  2.058 -0.267  -0.292   -0.152   -0.152\n57   socialm =~  x4 18.916  1.300   1.423    1.010    1.010\n58   socialm =~  x9  0.338  0.067   0.073    0.027    0.027\n59   socialm =~ x10  2.884  0.128   0.140    0.080    0.080\n60   socialm =~ x11  4.357 -0.229  -0.251   -0.098   -0.098\n61   socialm =~ x12  0.001  0.004   0.004    0.002    0.002\n62  enhancem =~  x1  1.954  0.093   0.149    0.072    0.072\n63  enhancem =~  x2  0.863  0.045   0.073    0.048    0.048\n64  enhancem =~  x3  0.380  0.038   0.061    0.032    0.032\n65  enhancem =~  x4  3.102 -0.104  -0.168   -0.119   -0.119\n66  enhancem =~  x5  0.596 -0.039  -0.063   -0.036   -0.036\n67  enhancem =~  x6  2.495  0.078   0.125    0.071    0.071\n68  enhancem =~  x7  0.539 -0.052  -0.084   -0.034   -0.034\n69  enhancem =~  x8  0.093 -0.019  -0.031   -0.014   -0.014\n70        x1 ~~  x2 10.299  0.379   0.379    0.149    0.149\n71        x1 ~~  x3  0.986  0.147   0.147    0.046    0.046\n72        x1 ~~  x4  0.016 -0.015  -0.015   -0.019   -0.019\n73        x1 ~~  x5  0.452 -0.080  -0.080   -0.032   -0.032\n74        x1 ~~  x6  0.484 -0.078  -0.078   -0.036   -0.036\n75        x1 ~~  x7  0.290 -0.089  -0.089   -0.027   -0.027\n76        x1 ~~  x8  1.535 -0.181  -0.181   -0.063   -0.063\n77        x1 ~~  x9  0.468  0.133   0.133    0.034    0.034\n78        x1 ~~ x10  0.067  0.033   0.033    0.013    0.013\n79        x1 ~~ x11  4.030  0.364   0.364    0.102    0.102\n80        x1 ~~ x12  1.504 -0.229  -0.229   -0.062   -0.062\n81        x2 ~~  x3  3.508  0.205   0.205    0.088    0.088\n82        x2 ~~  x4  6.780 -0.229  -0.229   -0.393   -0.393\n83        x2 ~~  x5  1.449  0.106   0.106    0.058    0.058\n84        x2 ~~  x6  0.102  0.026   0.026    0.016    0.016\n85        x2 ~~  x7  1.144 -0.130  -0.130   -0.053   -0.053\n86        x2 ~~  x8  0.366 -0.065  -0.065   -0.031   -0.031\n87        x2 ~~  x9  1.877  0.196   0.196    0.067    0.067\n88        x2 ~~ x10  0.434 -0.062  -0.062   -0.032   -0.032\n89        x2 ~~ x11  1.599  0.169   0.169    0.064    0.064\n90        x2 ~~ x12  0.726 -0.117  -0.117   -0.043   -0.043\n91        x3 ~~  x4  0.107 -0.037  -0.037   -0.051   -0.051\n92        x3 ~~  x5  0.024  0.017   0.017    0.008    0.008\n93        x3 ~~  x6  0.211  0.048   0.048    0.024    0.024\n94        x3 ~~  x7  0.009  0.015   0.015    0.005    0.005\n95        x3 ~~  x8  5.281 -0.310  -0.310   -0.117   -0.117\n96        x3 ~~  x9  0.031  0.031   0.031    0.009    0.009\n97        x3 ~~ x10  3.545 -0.221  -0.221   -0.092   -0.092\n98        x3 ~~ x11  5.967  0.408   0.408    0.124    0.124\n99        x3 ~~ x12  0.055 -0.040  -0.040   -0.012   -0.012\n100       x4 ~~  x5  0.063 -0.016  -0.016   -0.028   -0.028\n101       x4 ~~  x6  0.052  0.015   0.015    0.029    0.029\n102       x4 ~~  x7  2.114  0.131   0.131    0.170    0.170\n103       x4 ~~  x8  0.208  0.037   0.037    0.057    0.057\n104       x4 ~~  x9  0.887 -0.091  -0.091   -0.100   -0.100\n105       x4 ~~ x10  1.063  0.065   0.065    0.109    0.109\n106       x4 ~~ x11  2.637 -0.149  -0.149   -0.181   -0.181\n107       x4 ~~ x12  0.169  0.039   0.039    0.046    0.046\n108       x5 ~~  x6  0.370  0.057   0.057    0.036    0.036\n109       x5 ~~  x7  0.292 -0.072  -0.072   -0.030   -0.030\n110       x5 ~~  x8  0.007  0.010   0.010    0.005    0.005\n111       x5 ~~  x9  0.822  0.133   0.133    0.047    0.047\n112       x5 ~~ x10  0.339  0.056   0.056    0.030    0.030\n113       x5 ~~ x11  1.126 -0.145  -0.145   -0.056   -0.056\n114       x5 ~~ x12  1.143 -0.151  -0.151   -0.057   -0.057\n115       x6 ~~  x7  2.528 -0.215  -0.215   -0.101   -0.101\n116       x6 ~~  x8  0.053  0.029   0.029    0.016    0.016\n117       x6 ~~  x9  1.056 -0.141  -0.141   -0.056   -0.056\n118       x6 ~~ x10  0.598  0.069   0.069    0.042    0.042\n119       x6 ~~ x11  0.248  0.064   0.064    0.028    0.028\n120       x6 ~~ x12  1.667  0.170   0.170    0.073    0.073\n121       x7 ~~  x8  1.431  0.206   0.206    0.074    0.074\n122       x7 ~~  x9  0.032 -0.036  -0.036   -0.009   -0.009\n123       x7 ~~ x10  1.521 -0.163  -0.163   -0.065   -0.065\n124       x7 ~~ x11  0.263 -0.097  -0.097   -0.028   -0.028\n125       x7 ~~ x12  0.637  0.156   0.156    0.044    0.044\n126       x8 ~~  x9  1.621  0.227   0.227    0.068    0.068\n127       x8 ~~ x10  1.311  0.134   0.134    0.061    0.061\n128       x8 ~~ x11  2.144 -0.244  -0.244   -0.081   -0.081\n129       x8 ~~ x12  0.591 -0.132  -0.132   -0.043   -0.043\n130       x9 ~~ x10 19.846  0.862   0.862    0.288    0.288\n131       x9 ~~ x11  2.908 -0.518  -0.518   -0.126   -0.126\n132       x9 ~~ x12  7.696 -0.876  -0.876   -0.207   -0.207\n133      x10 ~~ x11  7.331 -0.534  -0.534   -0.197   -0.197\n134      x10 ~~ x12  5.572 -0.484  -0.484   -0.174   -0.174\n135      x11 ~~ x12 26.947  1.711   1.711    0.447    0.447\n\n\nLe considerazioni precedenti, dunque, suggeriscono che il modello potrebbe non avere descritto in maniera adeguata le relazioni tra x4 e i fattori comuni latenti. In base a considerazioni teoriche, supponiamo che abbia senso pensare che x4 saturi non solo sul fattore Motivi di coping ma anche sul fattore di Motivi Sociali. Specifichiamo dunque un nuovo modello nel modo seguente.\n\nmodel2 &lt;- '\n  copingm  =~ x1 + x2 + x3 + x4\n  socialm  =~ x4 + x5 + x6 + x7 + x8\n  enhancem =~ x9 + x10 + x11 + x12\n'\n\nAdattiamo il modello.\n\nfit2 &lt;- cfa(\n  model2, \n  sample.cov = covs, \n  sample.nobs = 500, \n  mimic = \"mplus\"\n)\n\nEsaminiamo gli indici di bontà di adattamento.\n\neffectsize::interpret(fit2) |&gt; print()\n\n    Name  Value Threshold Interpretation\n1    GFI 0.9768      0.95   satisfactory\n2   AGFI 0.9583      0.90   satisfactory\n3    NFI 0.9583      0.90   satisfactory\n4   NNFI 0.9839      0.90   satisfactory\n5    CFI 0.9878      0.90   satisfactory\n6  RMSEA 0.0279      0.05   satisfactory\n7   SRMR 0.0289      0.08   satisfactory\n8    RFI 0.9449      0.90   satisfactory\n9   PNFI 0.7260      0.50   satisfactory\n10   IFI 0.9880      0.90   satisfactory\n\n\nLa bontà di adattamento è migliorata.\nEsaminiamo la soluzione standardizzata. Vediamo ora che sono scomparse le due anomalie trovate in precedenza.\n\nstandardizedSolution(fit2) |&gt; print()\n\n        lhs op      rhs est.std    se     z pvalue ci.lower ci.upper\n1   copingm =~       x1   0.514 0.043 12.03      0    0.430    0.597\n2   copingm =~       x2   0.515 0.043 12.07      0    0.431    0.599\n3   copingm =~       x3   0.516 0.043 12.11      0    0.432    0.600\n4   copingm =~       x4   0.538 0.062  8.66      0    0.416    0.660\n5   socialm =~       x4   0.439 0.061  7.20      0    0.320    0.558\n6   socialm =~       x5   0.632 0.032 20.00      0    0.570    0.694\n7   socialm =~       x6   0.746 0.025 29.28      0    0.696    0.796\n8   socialm =~       x7   0.691 0.028 24.23      0    0.635    0.746\n9   socialm =~       x8   0.731 0.026 27.76      0    0.679    0.782\n10 enhancem =~       x9   0.603 0.039 15.62      0    0.527    0.678\n11 enhancem =~      x10   0.595 0.039 15.31      0    0.519    0.671\n12 enhancem =~      x11   0.665 0.037 18.19      0    0.593    0.737\n13 enhancem =~      x12   0.663 0.037 18.10      0    0.591    0.735\n14       x1 ~~       x1   0.736 0.044 16.79      0    0.650    0.822\n15       x2 ~~       x2   0.735 0.044 16.73      0    0.649    0.821\n16       x3 ~~       x3   0.734 0.044 16.68      0    0.647    0.820\n17       x4 ~~       x4   0.230 0.037  6.29      0    0.158    0.301\n18       x5 ~~       x5   0.601 0.040 15.04      0    0.522    0.679\n19       x6 ~~       x6   0.443 0.038 11.63      0    0.368    0.517\n20       x7 ~~       x7   0.523 0.039 13.29      0    0.446    0.600\n21       x8 ~~       x8   0.466 0.038 12.11      0    0.390    0.541\n22       x9 ~~       x9   0.637 0.046 13.70      0    0.546    0.728\n23      x10 ~~      x10   0.646 0.046 13.99      0    0.556    0.737\n24      x11 ~~      x11   0.558 0.049 11.47      0    0.463    0.653\n25      x12 ~~      x12   0.561 0.049 11.55      0    0.465    0.656\n26  copingm ~~  copingm   1.000 0.000    NA     NA    1.000    1.000\n27  socialm ~~  socialm   1.000 0.000    NA     NA    1.000    1.000\n28 enhancem ~~ enhancem   1.000 0.000    NA     NA    1.000    1.000\n29  copingm ~~  socialm   0.610 0.057 10.74      0    0.498    0.721\n30  copingm ~~ enhancem   0.350 0.059  5.96      0    0.235    0.465\n31  socialm ~~ enhancem   0.265 0.055  4.79      0    0.156    0.373\n32       x1 ~1            0.000 0.045  0.00      1   -0.088    0.088\n33       x2 ~1            0.000 0.045  0.00      1   -0.088    0.088\n34       x3 ~1            0.000 0.045  0.00      1   -0.088    0.088\n35       x4 ~1            0.000 0.045  0.00      1   -0.088    0.088\n36       x5 ~1            0.000 0.045  0.00      1   -0.088    0.088\n37       x6 ~1            0.000 0.045  0.00      1   -0.088    0.088\n38       x7 ~1            0.000 0.045  0.00      1   -0.088    0.088\n39       x8 ~1            0.000 0.045  0.00      1   -0.088    0.088\n40       x9 ~1            0.000 0.045  0.00      1   -0.088    0.088\n41      x10 ~1            0.000 0.045  0.00      1   -0.088    0.088\n42      x11 ~1            0.000 0.045  0.00      1   -0.088    0.088\n43      x12 ~1            0.000 0.045  0.00      1   -0.088    0.088\n44  copingm ~1            0.000 0.000    NA     NA    0.000    0.000\n45  socialm ~1            0.000 0.000    NA     NA    0.000    0.000\n46 enhancem ~1            0.000 0.000    NA     NA    0.000    0.000\n\n\nEsaminando i MI, notiamo che il modello potrebbe migliorare se introduciamo una correlazione tra le specificità x11 e x12.\n\nmodindices(fit2) |&gt; print()\n\n         lhs op rhs     mi    epc sepc.lv sepc.all sepc.nox\n47   copingm =~  x5  0.076  0.032   0.034    0.020    0.020\n48   copingm =~  x6  1.413  0.143   0.151    0.086    0.086\n49   copingm =~  x7  0.245  0.083   0.088    0.035    0.035\n50   copingm =~  x8  3.668 -0.295  -0.311   -0.137   -0.137\n51   copingm =~  x9  0.243  0.066   0.069    0.026    0.026\n52   copingm =~ x10  0.566  0.065   0.069    0.040    0.040\n53   copingm =~ x11  0.119 -0.044  -0.046   -0.018   -0.018\n54   copingm =~ x12  0.598 -0.102  -0.108   -0.041   -0.041\n55   socialm =~  x1  1.948 -0.396  -0.245   -0.119   -0.119\n56   socialm =~  x2  0.718  0.177   0.110    0.072    0.072\n57   socialm =~  x3  0.298  0.144   0.089    0.047    0.047\n58   socialm =~  x9  0.316  0.114   0.071    0.026    0.026\n59   socialm =~ x10  3.169  0.236   0.146    0.084    0.084\n60   socialm =~ x11  4.927 -0.430  -0.266   -0.104   -0.104\n61   socialm =~ x12  0.017  0.026   0.016    0.006    0.006\n62  enhancem =~  x1  0.314  0.040   0.064    0.031    0.031\n63  enhancem =~  x2  0.003  0.003   0.004    0.003    0.003\n64  enhancem =~  x3  0.037 -0.013  -0.020   -0.011   -0.011\n65  enhancem =~  x4  0.106 -0.013  -0.021   -0.015   -0.015\n66  enhancem =~  x5  0.464 -0.034  -0.055   -0.032   -0.032\n67  enhancem =~  x6  2.703  0.079   0.128    0.072    0.072\n68  enhancem =~  x7  0.467 -0.048  -0.077   -0.031   -0.031\n69  enhancem =~  x8  0.095 -0.019  -0.031   -0.014   -0.014\n70        x1 ~~  x2  1.966  0.187   0.187    0.081    0.081\n71        x1 ~~  x3  2.042 -0.241  -0.241   -0.083   -0.083\n72        x1 ~~  x4  0.775  0.098   0.098    0.082    0.082\n73        x1 ~~  x5  0.238 -0.058  -0.058   -0.024   -0.024\n74        x1 ~~  x6  0.187 -0.048  -0.048   -0.023   -0.023\n75        x1 ~~  x7  0.019 -0.022  -0.022   -0.007   -0.007\n76        x1 ~~  x8  0.366 -0.087  -0.087   -0.032   -0.032\n77        x1 ~~  x9  0.155  0.076   0.076    0.020    0.020\n78        x1 ~~ x10  0.104  0.041   0.041    0.016    0.016\n79        x1 ~~ x11  2.019  0.255   0.255    0.075    0.075\n80        x1 ~~ x12  1.911 -0.257  -0.257   -0.073   -0.073\n81        x2 ~~  x3  0.035 -0.023  -0.023   -0.011   -0.011\n82        x2 ~~  x4  3.029 -0.144  -0.144   -0.163   -0.163\n83        x2 ~~  x5  2.503  0.138   0.138    0.079    0.079\n84        x2 ~~  x6  0.509  0.058   0.058    0.038    0.038\n85        x2 ~~  x7  0.471 -0.082  -0.082   -0.035   -0.035\n86        x2 ~~  x8  0.015  0.013   0.013    0.006    0.006\n87        x2 ~~  x9  1.289  0.161   0.161    0.058    0.058\n88        x2 ~~ x10  0.467 -0.064  -0.064   -0.035   -0.035\n89        x2 ~~ x11  0.338  0.077   0.077    0.031    0.031\n90        x2 ~~ x12  0.970 -0.135  -0.135   -0.052   -0.052\n91        x3 ~~  x4  1.095  0.109   0.109    0.098    0.098\n92        x3 ~~  x5  0.169  0.045   0.045    0.021    0.021\n93        x3 ~~  x6  0.681  0.085   0.085    0.044    0.044\n94        x3 ~~  x7  0.315  0.085   0.085    0.029    0.029\n95        x3 ~~  x8  3.075 -0.235  -0.235   -0.092   -0.092\n96        x3 ~~  x9  0.022 -0.026  -0.026   -0.008   -0.008\n97        x3 ~~ x10  3.825 -0.230  -0.230   -0.100   -0.100\n98        x3 ~~ x11  3.498  0.313   0.313    0.099    0.099\n99        x3 ~~ x12  0.079 -0.049  -0.049   -0.015   -0.015\n100       x4 ~~  x5  0.337 -0.037  -0.037   -0.041   -0.041\n101       x4 ~~  x6  0.033 -0.012  -0.012   -0.015   -0.015\n102       x4 ~~  x7  1.053  0.094   0.094    0.077    0.077\n103       x4 ~~  x8  0.071 -0.022  -0.022   -0.021   -0.021\n104       x4 ~~  x9  0.541 -0.070  -0.070   -0.048   -0.048\n105       x4 ~~ x10  1.128  0.066   0.066    0.070    0.070\n106       x4 ~~ x11  1.313 -0.102  -0.102   -0.079   -0.079\n107       x4 ~~ x12  0.322  0.052   0.052    0.039    0.039\n108       x5 ~~  x6  0.504  0.066   0.066    0.042    0.042\n109       x5 ~~  x7  0.262 -0.068  -0.068   -0.028   -0.028\n110       x5 ~~  x8  0.004  0.008   0.008    0.004    0.004\n111       x5 ~~  x9  0.850  0.135   0.135    0.047    0.047\n112       x5 ~~ x10  0.288  0.052   0.052    0.027    0.027\n113       x5 ~~ x11  1.019 -0.138  -0.138   -0.054   -0.054\n114       x5 ~~ x12  1.224 -0.157  -0.157   -0.059   -0.059\n115       x6 ~~  x7  2.404 -0.209  -0.209   -0.099   -0.099\n116       x6 ~~  x8  0.034  0.023   0.023    0.012    0.012\n117       x6 ~~  x9  0.978 -0.135  -0.135   -0.054   -0.054\n118       x6 ~~ x10  0.524  0.065   0.065    0.039    0.039\n119       x6 ~~ x11  0.341  0.074   0.074    0.033    0.033\n120       x6 ~~ x12  1.520  0.163   0.163    0.069    0.069\n121       x7 ~~  x8  1.171  0.186   0.186    0.067    0.067\n122       x7 ~~  x9  0.020 -0.028  -0.028   -0.007   -0.007\n123       x7 ~~ x10  1.593 -0.167  -0.167   -0.066   -0.066\n124       x7 ~~ x11  0.175 -0.079  -0.079   -0.023   -0.023\n125       x7 ~~ x12  0.586  0.149   0.149    0.042    0.042\n126       x8 ~~  x9  1.808  0.239   0.239    0.072    0.072\n127       x8 ~~ x10  1.267  0.131   0.131    0.060    0.060\n128       x8 ~~ x11  1.791 -0.222  -0.222   -0.075   -0.075\n129       x8 ~~ x12  0.595 -0.132  -0.132   -0.043   -0.043\n130       x9 ~~ x10 20.103  0.864   0.864    0.288    0.288\n131       x9 ~~ x11  3.658 -0.582  -0.582   -0.142   -0.142\n132       x9 ~~ x12  7.229 -0.845  -0.845   -0.199   -0.199\n133      x10 ~~ x11  7.617 -0.543  -0.543   -0.201   -0.201\n134      x10 ~~ x12  4.512 -0.431  -0.431   -0.154   -0.154\n135      x11 ~~ x12 26.071  1.680   1.680    0.440    0.440\n\n\nIl nuovo modello diventa dunque il seguente.\n\nmodel3 &lt;- '\n  copingm  =~ x1 + x2 + x3 + x4\n  socialm  =~ x4 + x5 + x6 + x7 + x8\n  enhancem =~ x9 + x10 + x11 + x12\n  x11 ~~ x12\n'\n\nAdattiamo il modello.\n\nfit3 &lt;- cfa(\n  model3, \n  sample.cov = covs, \n  sample.nobs = 500, \n  mimic = \"mplus\"\n)\n\nUn test basato sul rapporto di verosimiglianze conferma che il miglioramento di adattamento è sostanziale.\n\nlavTestLRT(fit2, fit3) |&gt; print()\n\n\nChi-Squared Difference Test\n\n     Df   AIC   BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)    \nfit3 49 23934 24107  45.0                                        \nfit2 50 23957 24125  69.4       24.5 0.217       1    7.5e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nEsaminiamo gli indici di bontà di adattamento.\n\nsummary(fit3, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 61 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        41\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                                44.955\n  Degrees of freedom                                49\n  P-value (Chi-square)                           0.638\n\nModel Test Baseline Model:\n\n  Test statistic                              1664.026\n  Degrees of freedom                                66\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.003\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -11926.170\n  Loglikelihood unrestricted model (H1)     -11903.692\n                                                      \n  Akaike (AIC)                               23934.339\n  Bayesian (BIC)                             24107.138\n  Sample-size adjusted Bayesian (SABIC)      23977.002\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.025\n  P-value H_0: RMSEA &lt;= 0.050                    1.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.023\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  copingm =~                                          \n    x1                1.000                           \n    x2                0.740    0.094    7.909    0.000\n    x3                0.933    0.118    7.903    0.000\n    x4                0.719    0.118    6.070    0.000\n  socialm =~                                          \n    x4                1.000                           \n    x5                1.771    0.273    6.485    0.000\n    x6                2.141    0.319    6.703    0.000\n    x7                2.784    0.421    6.611    0.000\n    x8                2.689    0.402    6.681    0.000\n  enhancem =~                                         \n    x9                1.000                           \n    x10               0.648    0.070    9.293    0.000\n    x11               0.776    0.093    8.340    0.000\n    x12               0.802    0.096    8.327    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n .x11 ~~                                              \n   .x12               1.460    0.300    4.873    0.000\n  copingm ~~                                          \n    socialm           0.398    0.071    5.603    0.000\n    enhancem          0.669    0.145    4.613    0.000\n  socialm ~~                                          \n    enhancem          0.320    0.084    3.783    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                0.000    0.092    0.000    1.000\n   .x2                0.000    0.068    0.000    1.000\n   .x3                0.000    0.086    0.000    1.000\n   .x4                0.000    0.063    0.000    1.000\n   .x5                0.000    0.077    0.000    1.000\n   .x6                0.000    0.079    0.000    1.000\n   .x7                0.000    0.111    0.000    1.000\n   .x8                0.000    0.101    0.000    1.000\n   .x9                0.000    0.120    0.000    1.000\n   .x10               0.000    0.078    0.000    1.000\n   .x11               0.000    0.115    0.000    1.000\n   .x12               0.000    0.119    0.000    1.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                3.117    0.230   13.546    0.000\n   .x2                1.694    0.125   13.527    0.000\n   .x3                2.705    0.200   13.536    0.000\n   .x4                0.454    0.070    6.502    0.000\n   .x5                1.794    0.130   13.835    0.000\n   .x6                1.384    0.115   12.015    0.000\n   .x7                3.240    0.248   13.089    0.000\n   .x8                2.393    0.194   12.352    0.000\n   .x9                3.958    0.400    9.895    0.000\n   .x10               1.710    0.170   10.063    0.000\n   .x11               4.657    0.371   12.545    0.000\n   .x12               4.997    0.398   12.561    0.000\n    copingm           1.118    0.217    5.158    0.000\n    socialm           0.380    0.110    3.469    0.001\n    enhancem          3.210    0.490    6.550    0.000\n\n\n\nGli indici di fit sono migliorati.\nEsaminiamo la soluzione standardizzata.\n\nstandardizedSolution(fit3) |&gt; print()\n\n        lhs op      rhs est.std    se     z pvalue ci.lower ci.upper\n1   copingm =~       x1   0.514 0.043 12.02      0    0.430    0.598\n2   copingm =~       x2   0.515 0.043 12.05      0    0.431    0.599\n3   copingm =~       x3   0.514 0.043 12.04      0    0.431    0.598\n4   copingm =~       x4   0.540 0.063  8.61      0    0.417    0.663\n5   socialm =~       x4   0.438 0.061  7.13      0    0.317    0.558\n6   socialm =~       x5   0.632 0.032 20.00      0    0.570    0.694\n7   socialm =~       x6   0.746 0.025 29.29      0    0.697    0.796\n8   socialm =~       x7   0.690 0.029 24.21      0    0.634    0.746\n9   socialm =~       x8   0.731 0.026 27.80      0    0.680    0.783\n10 enhancem =~       x9   0.669 0.041 16.39      0    0.589    0.749\n11 enhancem =~      x10   0.664 0.041 16.24      0    0.584    0.744\n12 enhancem =~      x11   0.542 0.045 12.12      0    0.454    0.629\n13 enhancem =~      x12   0.541 0.045 12.08      0    0.453    0.628\n14      x11 ~~      x12   0.303 0.050  6.10      0    0.205    0.400\n15       x1 ~~       x1   0.736 0.044 16.76      0    0.650    0.822\n16       x2 ~~       x2   0.735 0.044 16.70      0    0.649    0.821\n17       x3 ~~       x3   0.735 0.044 16.73      0    0.649    0.822\n18       x4 ~~       x4   0.229 0.037  6.22      0    0.157    0.301\n19       x5 ~~       x5   0.601 0.040 15.04      0    0.522    0.679\n20       x6 ~~       x6   0.443 0.038 11.64      0    0.368    0.517\n21       x7 ~~       x7   0.524 0.039 13.31      0    0.447    0.601\n22       x8 ~~       x8   0.465 0.038 12.10      0    0.390    0.541\n23       x9 ~~       x9   0.552 0.055 10.10      0    0.445    0.659\n24      x10 ~~      x10   0.559 0.054 10.31      0    0.453    0.666\n25      x11 ~~      x11   0.706 0.048 14.58      0    0.611    0.801\n26      x12 ~~      x12   0.708 0.048 14.62      0    0.613    0.802\n27  copingm ~~  copingm   1.000 0.000    NA     NA    1.000    1.000\n28  socialm ~~  socialm   1.000 0.000    NA     NA    1.000    1.000\n29 enhancem ~~ enhancem   1.000 0.000    NA     NA    1.000    1.000\n30  copingm ~~  socialm   0.610 0.057 10.73      0    0.499    0.721\n31  copingm ~~ enhancem   0.353 0.060  5.84      0    0.235    0.472\n32  socialm ~~ enhancem   0.289 0.056  5.14      0    0.179    0.399\n33       x1 ~1            0.000 0.045  0.00      1   -0.088    0.088\n34       x2 ~1            0.000 0.045  0.00      1   -0.088    0.088\n35       x3 ~1            0.000 0.045  0.00      1   -0.088    0.088\n36       x4 ~1            0.000 0.045  0.00      1   -0.088    0.088\n37       x5 ~1            0.000 0.045  0.00      1   -0.088    0.088\n38       x6 ~1            0.000 0.045  0.00      1   -0.088    0.088\n39       x7 ~1            0.000 0.045  0.00      1   -0.088    0.088\n40       x8 ~1            0.000 0.045  0.00      1   -0.088    0.088\n41       x9 ~1            0.000 0.045  0.00      1   -0.088    0.088\n42      x10 ~1            0.000 0.045  0.00      1   -0.088    0.088\n43      x11 ~1            0.000 0.045  0.00      1   -0.088    0.088\n44      x12 ~1            0.000 0.045  0.00      1   -0.088    0.088\n45  copingm ~1            0.000 0.000    NA     NA    0.000    0.000\n46  socialm ~1            0.000 0.000    NA     NA    0.000    0.000\n47 enhancem ~1            0.000 0.000    NA     NA    0.000    0.000\n\n\nNon ci sono ulteriori motivi di preoccupazione. Brown (2015) conclude che il modello più adeguato sia model3.\nNel caso presente, a mio parare, l’introduzione della correlazione residua tra x11 e x12 si sarebbe anche potuta evitare, dato che il modello model3 (con meno idiosincrasie legate al campione) si era già dimostrato adeguato.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html#saturazione-sul-fattore-sbagliato",
    "href": "chapters/sem/06_refine_solution.html#saturazione-sul-fattore-sbagliato",
    "title": "53  La revisione del modello",
    "section": "53.8 Saturazione sul fattore sbagliato",
    "text": "53.8 Saturazione sul fattore sbagliato\nBrown (2015) considera anche il caso opposto, ovvero quello nel quale il ricercatore ipotizza una saturazione spuria. Per i dati in discussione, si può avere la situazione presente.\n\nmodel4 &lt;- '\n  copingm  =~ x1 + x2 + x3 + x4\n  socialm  =~ x4 +x5 + x6 + x7 + x8 + x12\n  enhancem =~ x9 + x10 + x11\n'\n\nAdattiamo il modello ai dati.\n\nfit4 &lt;- cfa(\n  model4, \n  sample.cov = covs, \n  sample.nobs = 500, \n  mimic = \"mplus\"\n)\n\nEsaminiamo la soluzione ottenuta.\n\nsummary(fit4, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 59 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        40\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                               212.717\n  Degrees of freedom                                50\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              1664.026\n  Degrees of freedom                                66\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.898\n  Tucker-Lewis Index (TLI)                       0.866\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -12010.051\n  Loglikelihood unrestricted model (H1)     -11903.692\n                                                      \n  Akaike (AIC)                               24100.101\n  Bayesian (BIC)                             24268.685\n  Sample-size adjusted Bayesian (SABIC)      24141.723\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.081\n  90 Percent confidence interval - lower         0.070\n  90 Percent confidence interval - upper         0.092\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.554\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.073\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  copingm =~                                          \n    x1                1.000                           \n    x2                0.741    0.093    7.925    0.000\n    x3                0.932    0.118    7.906    0.000\n    x4                0.699    0.117    5.995    0.000\n  socialm =~                                          \n    x4                1.000                           \n    x5                1.725    0.260    6.634    0.000\n    x6                2.098    0.305    6.879    0.000\n    x7                2.717    0.401    6.775    0.000\n    x8                2.619    0.382    6.848    0.000\n    x12               0.900    0.236    3.818    0.000\n  enhancem =~                                         \n    x9                1.000                           \n    x10               0.638    0.076    8.408    0.000\n    x11               0.767    0.094    8.153    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  copingm ~~                                          \n    socialm           0.410    0.072    5.663    0.000\n    enhancem          0.661    0.148    4.456    0.000\n  socialm ~~                                          \n    enhancem          0.347    0.089    3.902    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                0.000    0.092    0.000    1.000\n   .x2                0.000    0.068    0.000    1.000\n   .x3                0.000    0.086    0.000    1.000\n   .x4                0.000    0.063    0.000    1.000\n   .x5                0.000    0.077    0.000    1.000\n   .x6                0.000    0.079    0.000    1.000\n   .x7                0.000    0.111    0.000    1.000\n   .x8                0.000    0.101    0.000    1.000\n   .x12               0.000    0.119    0.000    1.000\n   .x9                0.000    0.120    0.000    1.000\n   .x10               0.000    0.078    0.000    1.000\n   .x11               0.000    0.115    0.000    1.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                3.106    0.230   13.478    0.000\n   .x2                1.686    0.125   13.449    0.000\n   .x3                2.698    0.200   13.477    0.000\n   .x4                0.463    0.069    6.719    0.000\n   .x5                1.805    0.130   13.886    0.000\n   .x6                1.378    0.115   12.022    0.000\n   .x7                3.255    0.248   13.143    0.000\n   .x8                2.418    0.194   12.455    0.000\n   .x12               6.740    0.430   15.673    0.000\n   .x9                3.891    0.436    8.933    0.000\n   .x10               1.724    0.183    9.435    0.000\n   .x11               4.662    0.371   12.579    0.000\n    copingm           1.129    0.218    5.170    0.000\n    socialm           0.397    0.111    3.566    0.000\n    enhancem          3.277    0.524    6.258    0.000\n\n\n\nÈ chiaro che il modello model4 è inadeguato. Il problema emerge chiaramente anche esaminando i MI.\n\nmodindices(fit4) |&gt; print()\n\n         lhs op rhs      mi    epc sepc.lv sepc.all sepc.nox\n47   copingm =~  x5   0.090  0.036   0.038    0.022    0.022\n48   copingm =~  x6   0.554  0.090   0.096    0.054    0.054\n49   copingm =~  x7   0.107  0.055   0.059    0.024    0.024\n50   copingm =~  x8   3.919 -0.306  -0.325   -0.143   -0.143\n51   copingm =~ x12   6.109  0.499   0.530    0.199    0.199\n52   copingm =~  x9   0.390 -0.096  -0.102   -0.038   -0.038\n53   copingm =~ x10   0.027 -0.016  -0.017   -0.010   -0.010\n54   copingm =~ x11   0.823  0.123   0.131    0.051    0.051\n55   socialm =~  x1   1.990 -0.398  -0.251   -0.122   -0.122\n56   socialm =~  x2   0.638  0.166   0.105    0.069    0.069\n57   socialm =~  x3   0.372  0.160   0.101    0.053    0.053\n58   socialm =~  x9   0.315 -0.130  -0.082   -0.031   -0.031\n59   socialm =~ x10   1.423  0.179   0.113    0.064    0.064\n60   socialm =~ x11   0.520 -0.150  -0.094   -0.037   -0.037\n61  enhancem =~  x1   1.029  0.067   0.121    0.059    0.059\n62  enhancem =~  x2   0.232  0.023   0.042    0.028    0.028\n63  enhancem =~  x3   0.153 -0.024  -0.043   -0.023   -0.023\n64  enhancem =~  x4   0.745 -0.031  -0.056   -0.040   -0.040\n65  enhancem =~  x5   0.343 -0.028  -0.050   -0.029   -0.029\n66  enhancem =~  x6   0.103  0.015   0.027    0.015    0.015\n67  enhancem =~  x7   2.752 -0.110  -0.198   -0.080   -0.080\n68  enhancem =~  x8   0.129 -0.021  -0.038   -0.017   -0.017\n69  enhancem =~ x12 116.781  0.916   1.658    0.624    0.624\n70        x1 ~~  x2   1.709  0.177   0.177    0.077    0.077\n71        x1 ~~  x3   2.273 -0.257  -0.257   -0.089   -0.089\n72        x1 ~~  x4   0.850  0.103   0.103    0.086    0.086\n73        x1 ~~  x5   0.292 -0.064  -0.064   -0.027   -0.027\n74        x1 ~~  x6   0.188 -0.048  -0.048   -0.023   -0.023\n75        x1 ~~  x7   0.023 -0.025  -0.025   -0.008   -0.008\n76        x1 ~~  x8   0.419 -0.093  -0.093   -0.034   -0.034\n77        x1 ~~ x12   0.025 -0.034  -0.034   -0.007   -0.007\n78        x1 ~~  x9   0.011  0.020   0.020    0.006    0.006\n79        x1 ~~ x10   0.004  0.008   0.008    0.003    0.003\n80        x1 ~~ x11   1.804  0.259   0.259    0.068    0.068\n81        x2 ~~  x3   0.071 -0.034  -0.034   -0.016   -0.016\n82        x2 ~~  x4   2.979 -0.143  -0.143   -0.162   -0.162\n83        x2 ~~  x5   2.403  0.135   0.135    0.077    0.077\n84        x2 ~~  x6   0.551  0.060   0.060    0.040    0.040\n85        x2 ~~  x7   0.457 -0.081  -0.081   -0.035   -0.035\n86        x2 ~~  x8   0.012  0.011   0.011    0.006    0.006\n87        x2 ~~ x12   0.134 -0.058  -0.058   -0.017   -0.017\n88        x2 ~~  x9   1.033  0.145   0.145    0.056    0.056\n89        x2 ~~ x10   1.140 -0.100  -0.100   -0.058   -0.058\n90        x2 ~~ x11   0.323  0.081   0.081    0.029    0.029\n91        x3 ~~  x4   1.472  0.127   0.127    0.113    0.113\n92        x3 ~~  x5   0.140  0.041   0.041    0.019    0.019\n93        x3 ~~  x6   0.717  0.087   0.087    0.045    0.045\n94        x3 ~~  x7   0.317  0.086   0.086    0.029    0.029\n95        x3 ~~  x8   3.121 -0.237  -0.237   -0.093   -0.093\n96        x3 ~~ x12   0.001  0.006   0.006    0.001    0.001\n97        x3 ~~  x9   0.000  0.003   0.003    0.001    0.001\n98        x3 ~~ x10   4.165 -0.241  -0.241   -0.111   -0.111\n99        x3 ~~ x11   3.806  0.350   0.350    0.099    0.099\n100       x4 ~~  x5   0.316 -0.036  -0.036   -0.039   -0.039\n101       x4 ~~  x6   0.052 -0.015  -0.015   -0.019   -0.019\n102       x4 ~~  x7   1.182  0.099   0.099    0.081    0.081\n103       x4 ~~  x8   0.062 -0.021  -0.021   -0.020   -0.020\n104       x4 ~~ x12   0.033  0.020   0.020    0.011    0.011\n105       x4 ~~  x9   1.418 -0.115  -0.115   -0.086   -0.086\n106       x4 ~~ x10   0.914  0.061   0.061    0.068    0.068\n107       x4 ~~ x11   0.517 -0.068  -0.068   -0.047   -0.047\n108       x5 ~~  x6   0.611  0.073   0.073    0.046    0.046\n109       x5 ~~  x7   0.115 -0.045  -0.045   -0.019   -0.019\n110       x5 ~~  x8   0.079  0.034   0.034    0.016    0.016\n111       x5 ~~ x12   3.265 -0.302  -0.302   -0.087   -0.087\n112       x5 ~~  x9   0.203  0.066   0.066    0.025    0.025\n113       x5 ~~ x10   0.000  0.002   0.002    0.001    0.001\n114       x5 ~~ x11   2.312 -0.224  -0.224   -0.077   -0.077\n115       x6 ~~  x7   2.239 -0.200  -0.200   -0.094   -0.094\n116       x6 ~~  x8   0.073  0.033   0.033    0.018    0.018\n117       x6 ~~ x12   0.478  0.109   0.109    0.036    0.036\n118       x6 ~~  x9   1.251 -0.153  -0.153   -0.066   -0.066\n119       x6 ~~ x10   0.784  0.079   0.079    0.051    0.051\n120       x6 ~~ x11   0.370  0.083   0.083    0.033    0.033\n121       x7 ~~  x8   1.644  0.219   0.219    0.078    0.078\n122       x7 ~~ x12   0.433 -0.152  -0.152   -0.032   -0.032\n123       x7 ~~  x9   0.005 -0.015  -0.015   -0.004   -0.004\n124       x7 ~~ x10   1.836 -0.179  -0.179   -0.076   -0.076\n125       x7 ~~ x11   0.348 -0.119  -0.119   -0.031   -0.031\n126       x8 ~~ x12   2.680 -0.335  -0.335   -0.083   -0.083\n127       x8 ~~  x9   0.676  0.147   0.147    0.048    0.048\n128       x8 ~~ x10   0.337  0.068   0.068    0.033    0.033\n129       x8 ~~ x11   3.437 -0.330  -0.330   -0.098   -0.098\n130      x12 ~~  x9   7.051  0.713   0.713    0.139    0.139\n131      x12 ~~ x10   6.960  0.465   0.465    0.136    0.136\n132      x12 ~~ x11  68.717  2.238   2.238    0.399    0.399\n133       x9 ~~ x10   0.081  0.138   0.138    0.053    0.053\n134       x9 ~~ x11   0.166  0.209   0.209    0.049    0.049\n135      x10 ~~ x11   0.423 -0.211  -0.211   -0.075   -0.075\n\n\nIl MI relativo alla saturazione di x12 su enhancem è uguale a 116.781. Chiaramente, in una revisione del modello, questo problema dovrebbe essere affrontato.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html#riflessioni-conclusive",
    "href": "chapters/sem/06_refine_solution.html#riflessioni-conclusive",
    "title": "53  La revisione del modello",
    "section": "53.9 Riflessioni Conclusive",
    "text": "53.9 Riflessioni Conclusive\nGli esempi presentati da Brown (2015) mostrano come l’applicazione dei MI, combinata con l’esame delle soluzioni fattoriali, rappresenti un approccio utile per ottimizzare e perfezionare il modello proposto.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/sem/06_refine_solution.html#informazioni-sullambiente-di-sviluppo",
    "title": "53  La revisione del modello",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] effectsize_0.8.9  MASS_7.3-61       viridis_0.6.5     viridisLite_0.4.2\n [5] ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0  \n [9] bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-19    \n[13] psych_2.4.6.26    scales_1.3.0      markdown_1.13     knitr_1.49       \n[17] lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n[21] purrr_1.0.2       readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n[25] ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.17.1   jsonlite_1.8.9      datawizard_0.13.0  \n  [4] magrittr_2.0.3      TH.data_1.1-2       estimability_1.5.1 \n  [7] farver_2.1.2        nloptr_2.1.1        rmarkdown_2.29     \n [10] vctrs_0.6.5         minqa_1.2.8         base64enc_0.1-3    \n [13] rstatix_0.7.2       htmltools_0.5.8.1   broom_1.0.7        \n [16] Formula_1.2-5       htmlwidgets_1.6.4   plyr_1.8.9         \n [19] sandwich_3.1-1      emmeans_1.10.5      zoo_1.8-12         \n [22] uuid_1.2-1          igraph_2.1.1        mime_0.12          \n [25] lifecycle_1.0.4     pkgconfig_2.0.3     Matrix_1.7-1       \n [28] R6_2.5.1            fastmap_1.2.0       shiny_1.9.1        \n [31] numDeriv_2016.8-1.1 digest_0.6.37       OpenMx_2.21.13     \n [34] fdrtool_1.2.18      colorspace_2.1-1    rprojroot_2.0.4    \n [37] Hmisc_5.2-0         fansi_1.0.6         timechange_0.3.0   \n [40] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n [43] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n [46] carData_3.0-5       performance_0.12.4  ggsignif_0.6.4     \n [49] corpcor_1.6.10      gtools_3.9.5        tools_4.4.2        \n [52] pbivnorm_0.6.0      foreign_0.8-87      zip_2.3.1          \n [55] httpuv_1.6.15       nnet_7.3-19         glue_1.8.0         \n [58] quadprog_1.5-8      promises_1.3.0      nlme_3.1-166       \n [61] lisrelToR_0.3       grid_4.4.2          pbdZMQ_0.3-13      \n [64] checkmate_2.3.2     cluster_2.1.6       reshape2_1.4.4     \n [67] generics_0.1.3      gtable_0.3.6        tzdb_0.4.0         \n [70] data.table_1.16.2   hms_1.1.3           car_3.1-3          \n [73] utf8_1.2.4          sem_3.1-16          pillar_1.9.0       \n [76] IRdisplay_1.1       rockchalk_1.8.157   later_1.3.2        \n [79] splines_4.4.2       cherryblossom_0.1.0 lattice_0.22-6     \n [82] survival_3.7-0      kutils_1.73         tidyselect_1.2.1   \n [85] miniUI_0.1.1.1      pbapply_1.7-2       airports_0.1.0     \n [88] stats4_4.4.2        xfun_0.49           qgraph_1.9.8       \n [91] arm_1.14-4          stringi_1.8.4       pacman_0.5.1       \n [94] boot_1.3-31         evaluate_1.0.1      codetools_0.2-20   \n [97] mi_1.1              cli_3.6.3           RcppParallel_5.1.9 \n[100] IRkernel_1.3.2      rpart_4.1.23        parameters_0.23.0  \n[103] xtable_1.8-4        repr_1.1.7          munsell_0.5.1      \n[106] Rcpp_1.0.13-1       coda_0.19-4.1       png_0.1-8          \n[109] XML_3.99-0.17       parallel_4.4.2      usdata_0.3.1       \n[112] bayestestR_0.15.0   jpeg_0.1-10         lme4_1.1-35.5      \n[115] mvtnorm_1.3-2       insight_0.20.5      openxlsx_4.2.7.1   \n[118] crayon_1.5.3        openintro_2.5.0     rlang_1.1.4        \n[121] multcomp_1.4-26     mnormt_2.1.1       \n\n\n\n\n\n\nBrown, T. A. (2015). Confirmatory factor analysis for applied research. Guilford publications.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/07_group_invariance.html",
    "href": "chapters/sem/07_group_invariance.html",
    "title": "54  Invarianza di misura",
    "section": "",
    "text": "54.1 Introduzione\nNel campo della psicologia, non è sufficiente ottenere una buona misura delle variabili latenti. Molte ricerche mirano a rispondere a domande riguardanti relazioni o differenze tra più costrutti latenti. Per rispondere a queste domande, è fondamentale verificare preliminarmente l’invarianza di misura. Tale verifica garantisce che il modello fattoriale sottostante uno strumento sia applicabile in modo equivalente ai sottogruppi rilevanti del campione.\nL’invarianza di misura implica che, se si analizzano separatamente i sottogruppi con un’analisi fattoriale, si ottiene lo stesso modello in ognuno di essi. Questo aspetto è cruciale poiché l’interpretazione delle variabili latenti dipende dal modello fattoriale. Se il modello differisce, ad esempio, tra maschi e femmine, un’analisi combinata di entrambi i gruppi risulterebbe confusa e poco interpretabile. In altre parole, per confrontare gruppi, è necessario che il modello fattoriale abbia lo stesso numero di fattori e lo stesso schema di relazioni tra fattori e variabili osservate per tutti i gruppi. Questo livello minimo è chiamato invarianza configurale.\nTuttavia, molte domande di ricerca richiedono livelli più elevati di invarianza, in cui non solo la struttura del modello è uguale tra i gruppi, ma anche i valori di alcuni parametri, come carichi fattoriali e intercette. Solo dopo aver verificato l’invarianza di misura, è possibile confrontare in modo attendibile le medie o le relazioni tra costrutti nei sottogruppi.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Invarianza di misura</span>"
    ]
  },
  {
    "objectID": "chapters/sem/07_group_invariance.html#introduzione",
    "href": "chapters/sem/07_group_invariance.html#introduzione",
    "title": "54  Invarianza di misura",
    "section": "",
    "text": "54.1.1 Livelli di Invarianza Fattoriale\nL’invarianza di misura può essere suddivisa in tre livelli principali:\n\nInvarianza configurale: verifica se il modello fattoriale ha la stessa struttura di base (numero di fattori e pattern di saturazioni) tra i gruppi. È il livello minimo richiesto per confrontare i gruppi.\nInvarianza metrica: verifica se i carichi fattoriali (o saturazioni) sono uguali tra i gruppi. Questo livello è necessario per confrontare le relazioni tra i costrutti latenti nei gruppi.\nInvarianza scalare: verifica se le intercette delle variabili osservate sono uguali tra i gruppi. Questo livello è essenziale per confrontare le medie dei costrutti latenti.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Invarianza di misura</span>"
    ]
  },
  {
    "objectID": "chapters/sem/07_group_invariance.html#invarianza-nei-modelli-sem-multi-gruppo",
    "href": "chapters/sem/07_group_invariance.html#invarianza-nei-modelli-sem-multi-gruppo",
    "title": "54  Invarianza di misura",
    "section": "\n54.2 Invarianza nei Modelli SEM Multi-Gruppo",
    "text": "54.2 Invarianza nei Modelli SEM Multi-Gruppo\nL’analisi multi-gruppo in un modello di equazioni strutturali (SEM) estende le analisi condotte su un singolo gruppo, permettendo di verificare l’equivalenza del modello tra sottogruppi. I modelli SEM multi-gruppo richiedono di considerare due componenti principali:\n\nInvarianza nei modelli di misura: verifica se le relazioni tra i costrutti latenti e le variabili osservate sono equivalenti tra i gruppi. Questa analisi include la verifica dei carichi fattoriali, delle intercette e delle varianze residue.\nInvarianza nei modelli strutturali: verifica se le relazioni tra i costrutti latenti (ad esempio, i coefficienti di percorso, le medie e le covarianze) sono equivalenti tra i gruppi.\n\nIn generale, l’obiettivo è ottenere modelli parsimoniosi che garantiscano un buon adattamento ai dati. Tuttavia, modelli con livelli elevati di invarianza possono essere restrittivi, e il compromesso tra parsimonia e adattamento richiede decisioni basate sul contesto di ricerca.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Invarianza di misura</span>"
    ]
  },
  {
    "objectID": "chapters/sem/07_group_invariance.html#test-dellinvarianza-fattoriale",
    "href": "chapters/sem/07_group_invariance.html#test-dellinvarianza-fattoriale",
    "title": "54  Invarianza di misura",
    "section": "\n54.3 Test dell’Invarianza Fattoriale",
    "text": "54.3 Test dell’Invarianza Fattoriale\nPer verificare l’invarianza di misura, si utilizza l’Analisi Fattoriale Confermativa a Gruppi Multipli (MG-CFA), che permette di determinare se un modello fattoriale descrive le stesse relazioni tra variabili osservate e latenti in diversi gruppi. Ad esempio, in uno studio che confronta gruppi basati sul genere, è essenziale verificare se un test, come quello sull’autostima, misura lo stesso costrutto con la stessa struttura fattoriale in entrambi i gruppi. Solo se questa condizione è soddisfatta, è possibile confrontare validamente le medie o le relazioni tra le variabili nei due gruppi.\nSe l’invarianza non viene verificata, qualsiasi confronto tra gruppi può risultare fuorviante, in quanto si rischia di confrontare costrutti che non sono concettualmente equivalenti. In questo capitolo, analizzeremo l’invarianza di misura considerando sia variabili osservate continue che categoriali. Tale analisi rappresenta una base fondamentale per garantire validità e coerenza nelle analisi successive, come il confronto delle medie dei costrutti latenti o la stima di relazioni strutturali.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Invarianza di misura</span>"
    ]
  },
  {
    "objectID": "chapters/sem/07_group_invariance.html#indicatori-continui",
    "href": "chapters/sem/07_group_invariance.html#indicatori-continui",
    "title": "54  Invarianza di misura",
    "section": "\n54.4 Indicatori Continui",
    "text": "54.4 Indicatori Continui\n\n54.4.1 Intercette degli Item\nNei modelli di equazioni strutturali, si analizza tradizionalmente la matrice di covarianza delle variabili osservate. In precedenza, abbiamo introdotto il modello dell’analisi fattoriale nel seguente modo:\n\\[\ny_i = \\mu + \\lambda_j \\xi_k + \\delta_i ,\n\\]\ndove \\(y_i\\) rappresenta il valore osservato, \\(\\mu\\) è la media, \\(\\lambda_j\\) è il carico fattoriale, \\(\\xi_k\\) è il fattore latente, e \\(\\delta_i\\) è l’errore. Per semplicità, nelle analisi iniziali si esclude spesso la media \\(\\mu\\), considerando gli scarti dalla media (\\(y_i - \\mu\\)), poiché ciò non modifica la struttura delle covarianze.\nTuttavia, in alcune applicazioni, come la verifica dell’invarianza fattoriale, è cruciale includere anche le medie delle variabili osservate. In questo caso, le medie sono rappresentate dalle intercette degli item nel modello fattoriale.\n\n54.4.2 Specificare le Intercette con lavaan\n\nIn lavaan, le intercette possono essere incluse specificandole esplicitamente per ciascun item. Ad esempio, per un singolo indicatore, si utilizza la sintassi seguente:\n# Specifica di un'intercetta per una variabile manifesta\nmy_item ~ 1\nQui, my_item rappresenta il nome della variabile manifesta, e ~ 1 indica la presenza di un’intercetta nel modello.\n\n54.4.2.1 Modello a Due Fattori con Intercette\nPer un modello con due fattori e sei indicatori, è possibile includere esplicitamente le intercette per ogni variabile osservata:\n# Modello con intercette specificate\nmod1 &lt;- \"\n  # Modello a due fattori\n  f1 =~ x1 + x2 + x3\n  f2 =~ x4 + x5 + x6\n  # Intercette\n  x1 ~ 1\n  x2 ~ 1\n  x3 ~ 1\n  x4 ~ 1\n  x5 ~ 1\n  x6 ~ 1\n\"\nTuttavia, un approccio più efficiente consiste nell’omettere le intercette nella sintassi del modello e includerle automaticamente utilizzando l’argomento meanstructure = TRUE nella funzione cfa():\n# Modello senza intercette esplicite\nmod2 &lt;- \"\n  f1 =~ x1 + x2 + x3\n  f2 =~ x4 + x5 + x6\n\"\n\n# Stima del modello con struttura delle medie\nfit &lt;- cfa(\n  mod2,\n  data = d,\n  meanstructure = TRUE\n)\n\n54.4.3 Effetti della Specificazione delle Intercette\nL’aggiunta delle intercette non modifica l’adattamento complessivo del modello. Infatti, includendo la struttura delle medie, si aggiungono \\(p\\) dati (le medie degli \\(p\\) indicatori) ma si stimano anche \\(p\\) nuovi parametri (le intercette), lasciando invariata la bontà dell’adattamento in termini di chi-quadrato e gradi di libertà.\nL’unico motivo per specificare esplicitamente le intercette è la necessità di imporre vincoli nella stima di questi parametri. Ad esempio, in un’analisi multi-gruppo, si potrebbe voler confrontare le medie delle variabili latenti tra gruppi, imponendo vincoli di uguaglianza sulle intercette degli item.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Invarianza di misura</span>"
    ]
  },
  {
    "objectID": "chapters/sem/07_group_invariance.html#tipologie-di-invarianza",
    "href": "chapters/sem/07_group_invariance.html#tipologie-di-invarianza",
    "title": "54  Invarianza di misura",
    "section": "\n54.5 Tipologie di Invarianza",
    "text": "54.5 Tipologie di Invarianza\nNel contesto dell’analisi fattoriale confermativa (CFA) multigruppo, l’invarianza viene testata a diversi livelli, ciascuno con specifici vincoli sui parametri del modello. Ogni livello successivo rappresenta un passo avanti nella verifica dell’equivalenza delle misurazioni tra gruppi, aggiungendo nuovi vincoli ai parametri e includendo quelli stabiliti dai livelli precedenti. Ecco una descrizione dettagliata di ciascun livello:\n\nInvarianza Configurale\nL’invarianza configurale rappresenta il livello più basilare. Si verifica quando la stessa struttura fattoriale si adatta a tutti i gruppi. Questo implica che gli stessi fattori sono presenti in ogni gruppo e che gli stessi item misurano tali fattori in modo consistente. Tuttavia, non si assumono ancora uguaglianze nei carichi fattoriali, nelle intercette o in altri parametri. Questo livello garantisce che i gruppi condividano lo stesso modello concettuale, ma non permette ancora confronti quantitativi tra di essi.\nInvarianza Metrica\nL’invarianza metrica aggiunge un vincolo di uguaglianza ai carichi fattoriali (factor loadings) tra i gruppi, mantenendo la struttura configurale. Questo livello verifica se gli item hanno la stessa relazione con i fattori latenti in tutti i gruppi. L’invarianza metrica è essenziale per confrontare le relazioni tra fattori latenti e variabili osservate, poiché garantisce che la scala del fattore latente sia equivalente nei gruppi.\nInvarianza Scalare\nL’invarianza scalare introduce un ulteriore vincolo: le intercette degli item devono essere uguali tra i gruppi. Insieme all’invarianza metrica, questo livello permette di confrontare le medie dei fattori latenti tra gruppi. L’uguaglianza delle intercette implica che i gruppi interpretano i punteggi degli item in modo equivalente e che eventuali differenze nelle medie dei fattori latenti riflettono effettive differenze nei costrutti, non nelle modalità di risposta.\nInvarianza Residuale\nL’invarianza residuale richiede che le varianze degli errori delle variabili osservate siano uguali tra i gruppi. Questo implica che l’affidabilità degli item (in termini di errore di misurazione) sia la stessa in tutti i gruppi. È un livello più stringente e meno frequentemente richiesto per la maggior parte delle applicazioni, ma utile in contesti dove si vuole garantire una misura estremamente robusta.\nInvarianza delle Varianze dei Fattori Latenti\nA questo livello, si testa se le varianze dei fattori latenti sono uguali tra i gruppi. L’uguaglianza delle varianze dei fattori implica che la variabilità del costrutto latente è comparabile nei diversi gruppi, consentendo ulteriori confronti sui pattern di dispersione.\nInvarianza delle Medie dei Fattori Latenti\nQuesto è il livello più stringente di invarianza. Si verifica se le medie dei fattori latenti sono uguali tra i gruppi. L’invarianza delle medie dei fattori, combinata con i livelli precedenti, implica che i gruppi condividano non solo la struttura del modello, ma anche valori medi simili nei costrutti latenti.\n\n\n54.5.1 Confronto dei Modelli\nOgni livello successivo di invarianza viene valutato confrontando gli indici di adattamento del modello corrente con quelli del modello precedente. Questo confronto aiuta a stabilire se l’aggiunta di nuovi vincoli compromette l’adattamento del modello. Tra i metodi di confronto più comuni vi sono:\n\n\nChi-quadrato (\\(\\Delta \\chi^2\\)): Confronta direttamente i modelli vincolati e non vincolati. È sensibile alla dimensione del campione, per cui può risultare poco informativo con campioni ampi.\n\nCriteri di adattamento alternativi: Cambiamenti negli indici di adattamento, come CFI (\\(\\Delta CFI\\)), TLI, o RMSEA, sono spesso utilizzati per determinare se un modello con più vincoli mantiene un buon adattamento.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Invarianza di misura</span>"
    ]
  },
  {
    "objectID": "chapters/sem/07_group_invariance.html#un-esempio-concreto",
    "href": "chapters/sem/07_group_invariance.html#un-esempio-concreto",
    "title": "54  Invarianza di misura",
    "section": "\n54.6 Un esempio Concreto",
    "text": "54.6 Un esempio Concreto\nConsideriamo un esempio tratto da Brown (2015), che illustra un’analisi fattoriale confermativa (CFA) applicata alla misurazione della depressione maggiore, così come definita nel DSM-IV.\nIl modello CFA in questo caso è progettato per valutare un costrutto latente, la depressione maggiore, utilizzando un set di nove indicatori osservabili che rappresentano i criteri diagnostici del DSM-IV. Gli indicatori includono:\n\n\nMDD1: Umore depresso\n\n\nMDD2: Perdita di interesse nelle attività usuali\n\n\nMDD3: Cambiamenti di peso o appetito\n\n\nMDD4: Disturbi del sonno\n\n\nMDD5: Agitazione o rallentamento psicomotorio\n\n\nMDD6: Affaticamento o perdita di energia\n\n\nMDD7: Sentimenti di inutilità o senso di colpa\n\n\nMDD8: Difficoltà di concentrazione\n\n\nMDD9: Pensieri di morte o suicidari\n\nQuesto esempio illustra come il modello CFA possa essere utilizzato per verificare se i nove indicatori forniscono una rappresentazione valida e coerente del costrutto latente “depressione maggiore”. Inoltre, l’approccio permette di valutare se la struttura del modello è applicabile a diversi sottogruppi, ad esempio in base al genere, all’età o al contesto culturale, verificando così l’invarianza di misura.\nImportiamo i dati in \\(\\mathsf{R}\\):\n\nd &lt;- readRDS(\n  here::here(\"data\", \"mdd_sex.RDS\")\n)\n\n\nhead(d)\n#&gt;      sex mdd1 mdd2 mdd3 mdd4 mdd5 mdd6 mdd7 mdd8 mdd9\n#&gt; 1 female    5    4    1    6    5    6    5    4    2\n#&gt; 2 female    5    5    5    5    4    5    4    5    4\n#&gt; 3 female    4    5    4    2    6    6    0    0    0\n#&gt; 4 female    5    5    3    3    5    5    6    4    0\n#&gt; 5 female    5    5    0    5    0    4    6    0    0\n#&gt; 6 female    6    6    4    6    4    6    5    6    2\n\nNel caso presente, i gruppi corrispondono al genere. Confrontiamo le distribuzioni di densità empirica degli item tra i due gruppi.\n\nd_long &lt;- d |&gt;\n    pivot_longer(!sex, names_to = \"item\", values_to = \"value\")\n\nd_long |&gt;\n    ggplot(aes(value, col=sex)) +\n    geom_density(linewidth=1.5) +\n    facet_wrap(~item, nrow=3, scales=\"free\") +\n    labs(x=\" \", y=\"Density\")\n\n\n\n\n\n\n\nIn questo tutorial, affrontiamo il problema di verificare l’invarianza fattoriale rispetto al genere utilizzando un modello CFA multi-gruppo. Consideriamo il seguente modello, basato sugli indicatori della depressione maggiore:\n\nmodel_mdd &lt;- \"\n  MDD =~ mdd1 + mdd2 + mdd3 + mdd4 + mdd5 + mdd6 + mdd7 + mdd8 +\n         mdd9\n  mdd1 ~~ mdd2\n\"\n\nIn questa specificazione, il fattore latente MDD (depressione maggiore) è misurato da nove indicatori osservabili (mdd1–mdd9), e viene aggiunta una correlazione residua tra gli indicatori mdd1 (umore depresso) e mdd2 (perdita di interesse). Questo vincolo riflette un’eventuale sovrapposizione concettuale o statistica tra questi due item (Brown, 2015).\n\n54.6.1 Stima dei Parametri nel Contesto Multi-Gruppo\nIn precedenza, abbiamo analizzato i modelli CFA a singolo gruppo, in cui i parametri vengono stimati minimizzando una funzione di discrepanza. Questa funzione confronta la matrice di covarianza osservata con quella prevista dal modello, cercando il miglior insieme di stime che riduca al minimo tale differenza.\nQuando si passa all’analisi multi-gruppo, il metodo di stima si estende in modo naturale:\n\nDefinizione delle Funzioni di Discrepanza per Gruppo\nPer ogni gruppo (ad esempio, uomini e donne), si calcola una funzione di discrepanza separata. Questa funzione misura la discrepanza tra le covarianze osservate e quelle stimate per ciascun gruppo.\nFunzione di Discrepanza Complessiva\nLa funzione complessiva viene definita come la somma ponderata delle funzioni di discrepanza di ciascun gruppo. In altre parole, l’adattamento complessivo del modello tiene conto simultaneamente di tutti i gruppi.\nStima dei Parametri\nI parametri del modello vengono stimati minimizzando questa funzione di discrepanza complessiva. Questo approccio consente di ottenere stime congiunte che rappresentano il miglior adattamento possibile per tutti i gruppi, rispettando i vincoli imposti dal modello.\n\n54.6.2 Applicazione dell’Invarianza Fattoriale al Confronto di Genere\nL’analisi multi-gruppo permette di verificare se il costrutto latente MDD (depressione maggiore) è misurato in modo equivalente tra uomini e donne, attraverso una sequenza di modelli con vincoli progressivi. Ogni livello di invarianza aggiunge nuovi vincoli al modello precedente, garantendo un confronto sempre più dettagliato e rigoroso tra i gruppi.\nDi seguito, definiamo i modelli per ciascun livello di invarianza, utilizzando la sintassi del pacchetto lavaan, ampiamente utilizzato per l’analisi SEM. I vincoli vengono impostati attraverso l’argomento group.equal, che consente di specificare quali parametri devono essere uguali tra i gruppi.\n\n54.6.2.1 Modelli di Invarianza\n\n\nInvarianza Configurale\nQuesto modello verifica se la stessa struttura fattoriale è valida per entrambi i gruppi, senza imporre vincoli aggiuntivi sui parametri.\n\n\nfit_ef &lt;- cfa(\n  model_mdd,\n  data = d,\n  group = \"sex\",\n  meanstructure = TRUE\n)\n\n\n\nInvarianza Metrica\nQui si aggiunge il vincolo di uguaglianza dei carichi fattoriali (loadings), per verificare se gli item si relazionano ai fattori latenti nello stesso modo nei due gruppi.\n\n\nfit_efl &lt;- update(\n  fit_ef,\n  group.equal = c(\"loadings\")\n)\n\n\n\nInvarianza Scalare\nOltre ai carichi fattoriali, si vincolano le intercette degli item per garantire che i punteggi medi degli item siano interpretati allo stesso modo nei due gruppi.\n\n\nfit_eii &lt;- update(\n  fit_efl,\n  group.equal = c(\"loadings\", \"intercepts\")\n)\n\n\n\nInvarianza Residuale\nQuesto modello aggiunge il vincolo di uguaglianza delle varianze residue degli indicatori tra i gruppi, assicurando che la precisione di misurazione sia equivalente.\n\n\nfit_eir &lt;- update(\n  fit_eii,\n  group.equal = c(\"loadings\", \"intercepts\", \"residuals\")\n)\n\n\n\nInvarianza delle Varianze Latenti\nSi aggiunge il vincolo di uguaglianza delle varianze dei fattori latenti, verificando se la variabilità del costrutto latente è simile tra i gruppi.\n\n\nfit_fv &lt;- update(\n  fit_eir,\n  group.equal = c(\n    \"loadings\", \"intercepts\", \"residuals\",\n    \"lv.variances\"\n  )\n)\n\n\n\nInvarianza delle Medie Latenti\nInfine, si vincolano le medie latenti dei fattori, per verificare se i gruppi hanno medie equivalenti nel costrutto latente.\n\n\nfit_fm &lt;- update(\n  fit_fv,\n  group.equal = c(\n    \"loadings\", \"intercepts\", \"residuals\",\n    \"lv.variances\", \"means\"\n  )\n)\n\n\n54.6.2.2 Interpretazione dei Risultati\nOgni modello rappresenta un passo successivo nella verifica dell’invarianza fattoriale. Dopo aver adattato ciascun modello ai dati, è essenziale confrontare gli indici di adattamento—come il \\(\\chi^2\\), il CFI e il RMSEA—tra un modello e il successivo. Questo confronto consente di determinare se i vincoli introdotti a ciascun livello di invarianza sono coerenti con i dati e non compromettono l’adattamento complessivo del modello.\nPer eseguire il confronto tra modelli nidificati, utilizziamo il test del rapporto di verosimiglianze, che confronta direttamente la discrepanza tra i modelli:\n\nout &lt;- lavTestLRT(fit_ef, fit_efl, fit_eii, fit_eir, fit_fv, fit_fm)\nprint(out)\n#&gt; \n#&gt; Chi-Squared Difference Test\n#&gt; \n#&gt;         Df   AIC   BIC Chisq Chisq diff  RMSEA Df diff Pr(&gt;Chisq)\n#&gt; fit_ef  52 27526 27785  98.9                                     \n#&gt; fit_efl 60 27514 27735 102.8       3.93 0.0000       8       0.86\n#&gt; fit_eii 68 27510 27695 115.3      12.47 0.0386       8       0.13\n#&gt; fit_eir 77 27502 27645 125.0       9.71 0.0145       9       0.37\n#&gt; fit_fv  78 27501 27639 125.8       0.79 0.0000       1       0.37\n#&gt; fit_fm  79 27501 27635 127.7       1.92 0.0495       1       0.17\n\nIl confronto tra i modelli nidificati mostra che l’introduzione di vincoli sempre più stringenti—uguaglianza dei carichi fattoriali, delle intercette, delle varianze residue, delle varianze dei fattori latenti e delle medie latenti—non porta a una sostanziale perdita di adattamento del modello ai dati. In particolare:\n\nIl valore del test del rapporto di verosimiglianze (\\(\\Delta \\chi^2\\)) non risulta incompatibile con i dati, indicando che i vincoli aggiunti sono accettabili.\nGli indici di adattamento (ad esempio RMSEA) restano a livelli appropriati per ciascun modello.\n\nPer i dati esaminati da Brown (2015), possiamo concludere che vi sono forti evidenze di invarianza fattoriale tra uomini e donne rispetto al costrutto di depressione maggiore (MDD). Questo implica che:\n\nGli indicatori misurano il costrutto in modo equivalente nei due gruppi.\nLe differenze osservate tra i gruppi nei punteggi totali del test possono essere interpretate come reali differenze nel costrutto latente, e non come artefatti di misurazione.\n\nL’invarianza fattoriale stabilita giustifica il confronto tra le medie dei punteggi latenti nei due gruppi, permettendo un’interpretazione solida e valida dei risultati.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Invarianza di misura</span>"
    ]
  },
  {
    "objectID": "chapters/sem/07_group_invariance.html#analisi-multi-gruppo-con-indicatori-ordinali",
    "href": "chapters/sem/07_group_invariance.html#analisi-multi-gruppo-con-indicatori-ordinali",
    "title": "54  Invarianza di misura",
    "section": "\n54.7 Analisi Multi-Gruppo con Indicatori Ordinali",
    "text": "54.7 Analisi Multi-Gruppo con Indicatori Ordinali\nL’analisi multi-gruppo per dati ordinali presenta sfide specifiche legate alla natura di questo tipo di variabili. A differenza delle variabili continue, le variabili ordinali richiedono approcci particolari sia per il calcolo delle saturazioni fattoriali che per l’analisi statistica.\nLe variabili ordinali rappresentano categorie con un ordine intrinseco, ma senza garanzia di spaziature uniformi tra le categorie. Esempi comuni includono scale di risposta come:\n\n\n“Fortemente in disaccordo” - “Fortemente d’accordo”\n\n“Mai” - “Sempre”\n\nSebbene queste categorie abbiano un ordine logico, le distanze tra di esse non sono necessariamente equivalenti e non possono essere interpretate come grandezze quantitative. L’assegnazione di valori numerici è arbitraria e non rappresenta differenze reali di intensità o grandezza.\n\n54.7.1 Problematiche nell’Analisi di Dati Ordinali\n\n54.7.1.1 Correlazioni Policoriche\nUno degli aspetti chiave nell’analisi di dati ordinali è il calcolo delle correlazioni. Le correlazioni policoriche rappresentano una soluzione efficace, basandosi sull’ipotesi che le risposte ordinali derivino da una variabile latente continua e normalmente distribuita. In questo approccio:\n\nOgni categoria di risposta è associata a un intervallo specifico sulla variabile latente sottostante.\nI punti di taglio (o soglie, \\(\\tau_1, \\tau_2, \\dots, \\tau_k\\)) suddividono la distribuzione normale in sezioni che corrispondono alle frequenze osservate per ciascuna categoria.\n\nAd esempio, per una scala a quattro punti, le soglie dividono la distribuzione in quattro intervalli, ciascuno associato a una categoria di risposta (ad esempio, “Mai”, “A volte”, “Spesso”, “Sempre”).\n\n54.7.1.2 Invarianza delle Soglie\nNell’analisi multi-gruppo, un aspetto fondamentale è l’invarianza delle soglie (threshold invariance). Questo concetto presuppone che i punti di taglio che definiscono le categorie di risposta rimangano stabili tra i gruppi. La stabilità delle soglie è essenziale per garantire che le relazioni tra le categorie di risposta siano comparabili nei diversi gruppi. In assenza di invarianza delle soglie, eventuali differenze tra gruppi potrebbero riflettere variazioni nella struttura della misurazione, piuttosto che vere differenze nel costrutto latente.\n\n54.7.2 Stima delle Saturazioni Fattoriali\nPer i dati ordinali, lo stimatore dei Minimi Quadrati Ponderati (Weighted Least Squares, WLS) è la scelta preferita. Questo metodo è particolarmente adatto ai dati ordinali poiché:\n\nTiene conto della natura categoriale delle risposte.\nFornisce stime più affidabili delle saturazioni fattoriali rispetto agli stimatori progettati per dati continui, come il massimo di verosimiglianza.\n\nIn alternativa, si può utilizzare una variante ponderata del WLS, nota come WLSMV (Weighted Least Squares Mean and Variance adjusted), che introduce correzioni per migliorare la robustezza delle stime.\nIn sintesi, l’analisi di invarianza fattoriale con dati ordinali richiede un’attenzione particolare a due aspetti principali:\n\n\nUso di correlazioni policoriche, che consentono di modellare la relazione tra categorie ordinali assumendo una variabile latente continua.\n\nScelta di stimatori adeguati, come il WLS o il WLSMV, che rispettano la natura dei dati ordinali e garantiscono stime affidabili.\n\nAdottando questi approcci, è possibile ottenere risultati validi e interpretabili, garantendo che eventuali confronti tra gruppi riflettano differenze reali nei costrutti latenti e non artefatti derivanti dalla misurazione.\n\n54.7.3 Un Esempio Concreto\nWu & Estabrook (2016) evidenziano che la metodologia tradizionale per valutare l’invarianza fattoriale con dati continui richiede adattamenti significativi quando applicata a indicatori categoriali. Solitamente, l’analisi dell’invarianza fattoriale segue una sequenza standard: si parte dalla definizione di un modello configurale e si introducono gradualmente vincoli ai parametri del modello, come carichi fattoriali, intercette e varianze residue. Tuttavia, per i dati categoriali, questa procedura presenta criticità specifiche, in particolare a causa della dipendenza dalle soglie utilizzate per definire le correlazioni policoriche.\n\n54.7.3.1 La Proposta di Wu & Estabrook (2016)\n\nWu & Estabrook (2016) sottolineano l’importanza di valutare preliminarmente l’equivalenza delle soglie tra gruppi, proponendo un approccio alternativo che enfatizza la costruzione di un modello di soglia (threshold model). Questo modello serve a verificare se le soglie, che definiscono gli intervalli della variabile latente continua sottostante, siano stabili e comparabili tra i gruppi. Solo una volta stabilita questa coerenza, è opportuno procedere con la valutazione dell’invarianza delle saturazioni fattoriali e degli altri parametri.\nTale approccio si basa sull’assunto che, nei dati categoriali, le soglie sono un elemento cruciale per interpretare correttamente le relazioni tra indicatori e fattori latenti. L’ordine delle analisi suggerito da Wu & Estabrook (2016) garantisce che eventuali differenze osservate tra gruppi siano attribuibili a reali variazioni nei costrutti latenti, e non a discrepanze nella definizione delle categorie di risposta.\n\n54.7.3.2 Esempio Applicativo: Scala sul Bullismo\nPer illustrare l’approccio, riprendiamo l’esempio proposto da Svetina et al. (2020). I dati analizzati provengono da una scala Likert a 4 punti (da 0 = “mai” a 3 = “almeno una volta alla settimana”) che misura la frequenza di episodi di bullismo. La scala include quattro item, come ad esempio: “Mi prendevano in giro o mi insultavano”. I dati sono stati raccolti in tre paesi: Azerbaigian, Austria e Finlandia, coinvolgendo i seguenti campioni:\n\n\nAzerbaigian: 31 scuole, 3.808 partecipanti.\n\n\nAustria: 40 scuole, 4.457 partecipanti.\n\n\nFinlandia: 246 scuole, 4.520 partecipanti.\n\nOgni partecipante ha risposto agli item della scala valutando la frequenza di episodi di bullismo subiti. L’obiettivo è verificare se la scala misura il costrutto di bullismo in modo equivalente nei tre paesi.\nImportiamo i dati in R:\n\ndat &lt;- read.table(\"../../data/BULLY.dat\", header = FALSE)\nnames(dat) &lt;- c(\"IDCNTRY\", \"R09A\", \"R09B\", \"R09C\", \"R09D\")\nhead(dat)\n#&gt;   IDCNTRY R09A R09B R09C R09D\n#&gt; 1      31    3    3    0    0\n#&gt; 2      31    0    0    0    0\n#&gt; 3      31    3    2    1    3\n#&gt; 4      31    0    0    3    0\n#&gt; 5      31    0    0    0    0\n#&gt; 6      31    0    0    0    0\n\nLa matrice all.results viene creata per raccogliere e organizzare i risultati dei diversi modelli sottoposti a confronto. I modelli analizzati includono:\n\n\nBaseline: nessun vincolo tra i gruppi (invarianza configurale).\n\nProposition 4: vincolo di equivalenza delle soglie tra i gruppi (invarianza delle soglie).\n\nProposition 7: vincolo di equivalenza sia delle soglie che delle saturazioni fattoriali tra i gruppi (invarianza delle soglie e dei carichi fattoriali).\n\nGli indici di bontà di adattamento che saranno registrati per ciascun modello comprendono: il chi-quadrato (\\(\\chi^2\\)), i gradi di libertà (\\(df\\)), il valore \\(p\\), il RMSEA, il CFI e il TLI. Questi indici permettono di valutare la qualità dell’adattamento dei modelli ai dati e di confrontare l’effetto dei vincoli imposti tra i gruppi.\n\nall.results &lt;- matrix(NA, ncol = 6, nrow = 3)\n\n\n54.7.3.3 Procedura Analitica\n1. Modello Configurale (Baseline)\nNel modello configurale, non si impongono vincoli sui parametri tra gruppi. Questo rappresenta il livello più basilare di invarianza.\nUtilizziamo la funzione measEq.syntax per creare il modello di partenza:\n\nmod.cat &lt;- \"F1 =~ R09A + R09B + R09C + R09D\"\n\nbaseline &lt;- measEq.syntax(\n  configural.model = mod.cat,\n  data = dat,\n  ordered = c(\"R09A\", \"R09B\", \"R09C\", \"R09D\"),\n  parameterization = \"delta\",\n  ID.fac = \"std.lv\",\n  ID.cat = \"Wu.Estabrook.2016\",\n  group = \"IDCNTRY\",\n  group.equal = \"configural\"\n)\n\nIl modello viene adattato ai dati:\n\nmodel.baseline &lt;- as.character(baseline)\nfit.baseline &lt;- cfa(\n  model.baseline, \n  data = dat, \n  group = \"IDCNTRY\", \n  ordered = c(\"R09A\", \"R09B\", \"R09C\", \"R09D\")\n)\n\nI risultati di bontà di adattamento vengono salvati per il confronto con modelli successivi:\n\nall.results[1, ] &lt;- round(\n  fitmeasures(fit.baseline, c(\n    \"chisq.scaled\", \"df.scaled\", \"pvalue.scaled\", \n    \"rmsea.scaled\", \"cfi.scaled\", \"tli.scaled\"\n  )), 3\n)\n\n2. Modello con Invarianza delle Soglie (Threshold Invariance)\nPer testare l’invarianza delle soglie, aggiungiamo il vincolo di uguaglianza delle soglie tra i gruppi:\n\nprop4 &lt;- measEq.syntax(\n  configural.model = mod.cat,\n  data = dat,\n  ordered = c(\"R09A\", \"R09B\", \"R09C\", \"R09D\"),\n  parameterization = \"delta\",\n  ID.fac = \"std.lv\",\n  ID.cat = \"Wu.Estabrook.2016\",\n  group = \"IDCNTRY\",\n  group.equal = c(\"thresholds\")\n)\n\nAdattiamo il modello e salviamo i risultati:\n\nmodel.prop4 &lt;- as.character(prop4)\nfit.prop4 &lt;- cfa(\n  model.prop4,\n  data = dat,\n  group = \"IDCNTRY\",\n  ordered = c(\"R09A\", \"R09B\", \"R09C\", \"R09D\")\n)\n\nall.results[2, ] &lt;- round(\n  fitmeasures(fit.prop4, c(\n    \"chisq.scaled\", \"df.scaled\", \"pvalue.scaled\", \n    \"rmsea.scaled\", \"cfi.scaled\", \"tli.scaled\"\n  )), 3\n)\n\nEseguiamo il confronto tra il modello configurale e il modello con invarianza delle soglie:\n\nlavTestLRT(fit.baseline, fit.prop4)\n#&gt; \n#&gt; Scaled Chi-Squared Difference Test (method = \"satorra.2000\")\n#&gt; \n#&gt; lavaan-&gt;lavTestLRT():  \n#&gt;    lavaan NOTE: The \"Chisq\" column contains standard test statistics, not \n#&gt;    the robust test that should be reported per model. A robust difference \n#&gt;    test is a function of two standard (not robust) statistics.\n#&gt;              Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq)\n#&gt; fit.baseline  6          26.9                              \n#&gt; fit.prop4    14          42.2         61       8      3e-10\n\n3. Modello con Invarianza delle Soglie e dei Carichi Fattoriali (Threshold and Loading Invariance)\nAggiungiamo ora i vincoli di uguaglianza sia per le soglie che per i carichi fattoriali:\n\nprop7 &lt;- measEq.syntax(\n  configural.model = mod.cat,\n  data = dat,\n  ordered = c(\"R09A\", \"R09B\", \"R09C\", \"R09D\"),\n  parameterization = \"delta\",\n  ID.fac = \"std.lv\",\n  ID.cat = \"Wu.Estabrook.2016\",\n  group = \"IDCNTRY\",\n  group.equal = c(\"thresholds\", \"loadings\")\n)\n\nmodel.prop7 &lt;- as.character(prop7)\nfit.prop7 &lt;- cfa(\n  model.prop7, \n  data = dat, \n  group = \"IDCNTRY\", \n  ordered = c(\"R09A\", \"R09B\", \"R09C\", \"R09D\")\n)\n\nall.results[3, ] &lt;- round(\n  fitmeasures(fit.prop7, c(\n    \"chisq.scaled\", \"df.scaled\", \"pvalue.scaled\", \n    \"rmsea.scaled\", \"cfi.scaled\", \"tli.scaled\"\n  )), 3\n)\n\nEseguiamo il confronto tra i modelli:\n\nlavTestLRT(fit.prop4, fit.prop7)\n#&gt; \n#&gt; Scaled Chi-Squared Difference Test (method = \"satorra.2000\")\n#&gt; \n#&gt; lavaan-&gt;lavTestLRT():  \n#&gt;    lavaan NOTE: The \"Chisq\" column contains standard test statistics, not \n#&gt;    the robust test that should be reported per model. A robust difference \n#&gt;    test is a function of two standard (not robust) statistics.\n#&gt;           Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq)\n#&gt; fit.prop4 14          42.2                              \n#&gt; fit.prop7 20          93.1       73.7       6    7.1e-14\n\nRisultati Finali\nI risultati dei tre modelli vengono confrontati utilizzando gli indici di bontà di adattamento:\n\n\n\n\n\n\n\n\n\n\n\nModello\n\\(\\chi^2\\)\ndf\n\n\\(p\\)-value\nRMSEA\nCFI\nTLI\n\n\n\nBaseline\n50.9\n6\n0.000\n0.042\n0.997\n0.991\n\n\nProp4\n107.8\n14\n0.000\n0.040\n0.994\n0.992\n\n\nProp7\n186.5\n20\n0.000\n0.044\n0.989\n0.990\n\n\n\nNel caso presente, i risultati del test del rapporto di verosimiglianza indicano che l’invarianza delle soglie non viene rispettata. Di conseguenza, ulteriori confronti sui carichi fattoriali sono superflui, ma sono stati illustrati per completezza della procedura.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Invarianza di misura</span>"
    ]
  },
  {
    "objectID": "chapters/sem/07_group_invariance.html#vincoli-inter-gruppi",
    "href": "chapters/sem/07_group_invariance.html#vincoli-inter-gruppi",
    "title": "54  Invarianza di misura",
    "section": "\n54.8 Vincoli Inter-Gruppi",
    "text": "54.8 Vincoli Inter-Gruppi\nQuando si adatta un modello di equazioni strutturali (SEM) ai dati provenienti da più gruppi, è spesso utile imporre vincoli di uguaglianza inter-gruppi su alcuni parametri. Tali parametri possono includere effetti causali (diretti, indiretti o totali), varianze, covarianze, medie o intercette. La scelta dei vincoli da applicare dipende dalle ipotesi teoriche specifiche riguardo alle somiglianze o differenze tra i gruppi.\nSe un modello con vincoli inter-gruppi mostra un peggioramento significativo nell’adattamento rispetto a un modello senza vincoli, e quest’ultimo si adatta bene ai dati, si può concludere che i gruppi differiscono per i parametri oggetto del vincolo.\nUn’alternativa all’approccio multi-gruppo consiste nel rappresentare l’appartenenza al gruppo in un modello per un singolo gruppo che include variabili di interazione. In tale modello, variabili endogene vengono regresse su termini prodotto tra l’appartenenza al gruppo e altre variabili. Questo approccio consente di stimare effetti interattivi tra il gruppo e altre variabili causali, che possono essere interpretati come effetti indiretti condizionali. Tuttavia, un limite di questo approccio è l’assunzione di omoscedasticità tra i gruppi, che, se violata, può portare a stime inesatte. In contrasto, l’approccio multi-gruppo consente di testare esplicitamente ipotesi di omogeneità.\n\n54.8.1 Un Esempio Concreto: Studio di Lynam et al. (1993)\nLynam e colleghi (1993) hanno esaminato il ruolo dello status socioeconomico familiare (SES), del quoziente intellettivo verbale (QI verbale), della motivazione durante il test del QI, del rendimento scolastico e dei comportamenti delinquenziali in due gruppi di adolescenti maschi (bianchi e neri) di età compresa tra 12 e 13 anni. I partecipanti provenivano da scuole pubbliche urbane degli Stati Uniti e facevano parte di uno studio longitudinale su individui a rischio elevato di comportamenti delinquenziali.\n\n54.8.1.1 Il Modello Teorico\nIl modello teorico proposto da Lynam et al. è illustrato nella figura seguente:\n\n\n\n\n\nFigura 54.1: Diagramma di percorso proposto da Lynam et al. (1993).\n\n\nSecondo il modello, SES, motivazione e QI verbale influenzano i comportamenti delinquenziali sia direttamente che indirettamente, attraverso il rendimento scolastico. Ad esempio, abilità verbali limitate possono condurre all’abbandono scolastico, aumentando così la delinquenza a causa di opportunità di impiego ridotte o maggiore tempo libero non supervisionato.\n\n54.8.1.2 Critiche al Modello\nIl modello proposto è trasversale e non include una sequenza temporale chiara nelle misurazioni, il che rende discutibile la direzionalità ipotizzata. Block (1995) ha criticato questo approccio, suggerendo che l’impulsività potrebbe mediare l’effetto del QI verbale sulla delinquenza, sottolineando che non è la scarsa abilità verbale di per sé a determinare direttamente i comportamenti delinquenziali.\n\n54.8.2 Specificazione dei Dati\nLe correlazioni, deviazioni standard e medie per ciascun gruppo sono riportate nei seguenti blocchi di codice:\n\n# Correlazioni in forma diagonale inferiore\nblackLower.cor &lt;- \"\n 1.00\n  .08 1.00\n  .28  .30 1.00\n  .05  .21  .50 1.00\n -.11 -.17 -.26 -.33 1.00 \"\n\nwhiteLower.cor &lt;- \"\n 1.00\n  .25 1.00\n  .37  .40 1.00\n  .27  .28  .61 1.00\n -.11 -.20 -.31 -.21 1.00 \"\n\n# Covarianze e deviazioni standard\nblack.cor &lt;- lavaan::getCov(blackLower.cor, names = c(\n    \"ses\", \"effort\", \"viq\", \"achieve\", \"delinq\"\n))\nwhite.cor &lt;- lavaan::getCov(whiteLower.cor, names = c(\n    \"ses\", \"effort\", \"viq\", \"achieve\", \"delinq\"\n))\n\n# Deviazioni standard\nblack.cov &lt;- lavaan::cor2cov(black.cor, sds = c(10.58, 1.35, 13.62, .79, 1.63))\nwhite.cov &lt;- lavaan::cor2cov(white.cor, sds = c(11.53, 1.32, 16.32, .96, 1.45))\n\n# Medie di ciascun gruppo\nblack.mean &lt;- c(31.96, -.01, 93.76, 2.51, 1.40)\nwhite.mean &lt;- c(34.64, .05, 104.18, 2.88, 1.22)\n\n# Creazione degli oggetti per l'analisi\ncombined.cov &lt;- list(black = black.cov, white = white.cov)\ncombined.mean &lt;- list(black = black.mean, white = white.mean)\ncombined.n &lt;- list(black = 214, white = 181)\n\n\n54.8.3 Specificazione del Modello e Adattamento\nIl modello teorico è stato specificato come segue:\n\nlynam.model &lt;- \"\n achieve ~ ses + effort + viq\n delinq ~ achieve + ses + effort + viq \n\"\n\nL’adattamento dei modelli ai dati è stato eseguito con i seguenti vincoli:\n\n# Modello 1: Vincoli completi\nlynam1 &lt;- lavaan::sem(\n    lynam.model,\n    sample.cov = combined.cov,\n    sample.mean = combined.mean,\n    sample.nobs = combined.n,\n    group.equal = c(\"regressions\"),\n    fixed.x = FALSE, meanstructure = TRUE\n)\n\n# Modello 2: Vincoli parziali (achievement -&gt; delinquency libero)\nlynam2 &lt;- lavaan::sem(\n    lynam.model,\n    sample.cov = combined.cov,\n    sample.mean = combined.mean,\n    sample.nobs = combined.n,\n    group.equal = c(\"regressions\"),\n    group.partial = c(\"delinq ~ achieve\"),\n    fixed.x = FALSE, meanstructure = TRUE\n)\n\n# Modello 3: Vincoli sulle intercette del rendimento\nlynam3 &lt;- lavaan::sem(\n    lynam.model,\n    sample.cov = combined.cov,\n    sample.mean = combined.mean,\n    sample.nobs = combined.n,\n    group.equal = c(\"regressions\", \"intercepts\"),\n    group.partial = c(\"delinq ~ achieve\", \"delinq ~ 1\"),\n    fixed.x = FALSE, meanstructure = TRUE\n)\n\n# Modello 4: Vincoli sulle intercette della delinquenza\nlynam4 &lt;- lavaan::sem(\n    lynam.model,\n    sample.cov = combined.cov,\n    sample.mean = combined.mean,\n    sample.nobs = combined.n,\n    group.equal = c(\"regressions\", \"intercepts\"),\n    group.partial = c(\"delinq ~ achieve\", \"achieve ~ 1\"),\n    fixed.x = FALSE, meanstructure = TRUE\n)\n\n\n54.8.4 Valutazione dei Modelli\nLe statistiche di adattamento sono state calcolate per ciascun modello:\n\n# Specificare le statistiche di adattamento globale\nfit.stats &lt;- c(\n    \"chisq\", \"df\", \"pvalue\",\n    \"rmsea\", \"rmsea.ci.lower\", \"rmsea.ci.upper\", \"cfi\", \"srmr\"\n)\n\n# Stampare le statistiche per ogni modello\nlavaan::fitMeasures(lynam1, fit.stats) |&gt; print()\n#&gt;          chisq             df         pvalue          rmsea rmsea.ci.lower \n#&gt;         11.736          7.000          0.110          0.059          0.000 \n#&gt; rmsea.ci.upper            cfi           srmr \n#&gt;          0.115          0.975          0.036\nlavaan::fitMeasures(lynam2, fit.stats) |&gt; print()\n#&gt;          chisq             df         pvalue          rmsea rmsea.ci.lower \n#&gt;          6.107          6.000          0.411          0.010          0.000 \n#&gt; rmsea.ci.upper            cfi           srmr \n#&gt;          0.093          0.999          0.029\nlavaan::fitMeasures(lynam3, fit.stats) |&gt; print()\n#&gt;          chisq             df         pvalue          rmsea rmsea.ci.lower \n#&gt;          6.409          7.000          0.493          0.000          0.000 \n#&gt; rmsea.ci.upper            cfi           srmr \n#&gt;          0.083          1.000          0.030\nlavaan::fitMeasures(lynam4, fit.stats) |&gt; print()\n#&gt;          chisq             df         pvalue          rmsea rmsea.ci.lower \n#&gt;         10.237          7.000          0.176          0.048          0.000 \n#&gt; rmsea.ci.upper            cfi           srmr \n#&gt;          0.107          0.983          0.033\n\nI confronti tra modelli sono stati effettuati con il test del rapporto di verosimiglianza:\n\n# Confronti tra modelli\nlavaan::anova(lynam1, lynam2)\n#&gt; \n#&gt; Chi-Squared Difference Test\n#&gt; \n#&gt;        Df  AIC  BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\n#&gt; lynam2  6 9850 9985  6.11                                    \n#&gt; lynam1  7 9854 9985 11.74       5.63 0.153       1      0.018\n\n\n# Confronti tra modelli\nlavaan::anova(lynam2, lynam3)\n#&gt; \n#&gt; Chi-Squared Difference Test\n#&gt; \n#&gt;        Df  AIC  BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\n#&gt; lynam2  6 9850 9985  6.11                                    \n#&gt; lynam3  7 9848 9980  6.41      0.302     0       1       0.58\n\n\n# Confronti tra modelli\nlavaan::anova(lynam2, lynam4)\n#&gt; \n#&gt; Chi-Squared Difference Test\n#&gt; \n#&gt;        Df  AIC  BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\n#&gt; lynam2  6 9850 9985  6.11                                    \n#&gt; lynam4  7 9852 9984 10.24       4.13 0.126       1      0.042\n\n\n54.8.5 Interpretazione dei Risultati\n\n\nModello 1: I vincoli completi non si adattano ai dati, suggerendo differenze rilevanti tra i gruppi.\n\nModello 2: Rilasciare il vincolo sull’effetto del rendimento migliora in maniera notevole l’adattamento.\n\nModello 3: Vincolando l’intercetta del rendimento, l’adattamento peggiora leggermente rispetto al Modello 2, ma rimane coerente con i dati.\n\nModello 4: Vincolando anche l’intercetta della delinquenza, l’adattamento peggiora in maniera rilevante, indicando che questo vincolo non è supportato.\n\nIn conclusione, l’analisi conferma che il rendimento scolastico ha un effetto protettivo più forte contro la delinquenza nei giovani neri rispetto ai bianchi. Tuttavia, l’introduzione di vincoli aggiuntivi sulle intercette compromette l’adattamento, sottolineando la necessità di considerare le differenze di gruppo nel rappresentare accuratamente il fenomeno.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Invarianza di misura</span>"
    ]
  },
  {
    "objectID": "chapters/sem/07_group_invariance.html#riflessioni-conclusive",
    "href": "chapters/sem/07_group_invariance.html#riflessioni-conclusive",
    "title": "54  Invarianza di misura",
    "section": "\n54.9 Riflessioni Conclusive",
    "text": "54.9 Riflessioni Conclusive\nL’analisi dell’invarianza di misura rappresenta un pilastro fondamentale nella ricerca psicometrica contemporanea, andando oltre la semplice verifica statistica per toccare questioni di equità e validità nelle comparazioni tra gruppi. La sua implementazione sistematica permette di stabilire se le differenze osservate riflettono reali variazioni nel costrutto sottostante o sono invece artefatti dello strumento di misura.\nLa complessità dei moderni contesti di ricerca, caratterizzati da popolazioni sempre più diversificate, rende questa verifica particolarmente rilevante. L’invarianza configura infatti un prerequisito metodologico per qualsiasi studio comparativo, che si tratti di ricerche transculturali, analisi longitudinali o confronti tra sottogruppi demografici.\nUn aspetto spesso sottovalutato riguarda l’impatto dell’invarianza parziale: mentre tradizionalmente si cercava un’invarianza completa, la ricerca recente ha evidenziato come anche livelli parziali di invarianza possano supportare confronti significativi, purché adeguatamente modellati e interpretati. Questo approccio più sfumato riflette una maggiore consapevolezza della complessità dei costrutti psicologici e della loro manifestazione in diversi contesti.\nLa distinzione tra dati continui e ordinali nell’analisi dell’invarianza sottolinea inoltre l’importanza di adattare le procedure statistiche alla natura effettiva delle misurazioni psicologiche, evitando l’applicazione acritica di metodi sviluppati per variabili continue a scale Likert o altri formati di risposta ordinale.\nLe implicazioni di questi concetti si estendono oltre l’ambito accademico, influenzando la pratica clinica, la selezione del personale e la valutazione educativa. La garanzia di equità nelle misurazioni psicologiche diventa particolarmente cruciale in contesti decisionali ad alto impatto, dove interpretazioni non corrette potrebbero portare a decisioni discriminatorie o inappropriate.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Invarianza di misura</span>"
    ]
  },
  {
    "objectID": "chapters/sem/07_group_invariance.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/sem/07_group_invariance.html#informazioni-sullambiente-di-sviluppo",
    "title": "54  Invarianza di misura",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] ggokabeito_0.1.0  see_0.10.0        MASS_7.3-65       viridis_0.6.5    \n#&gt;  [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n#&gt;  [9] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n#&gt; [13] lavaan_0.6-19     psych_2.4.12      scales_1.3.0      markdown_1.13    \n#&gt; [17] knitr_1.49        lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n#&gt; [21] dplyr_1.1.4       purrr_1.0.4       readr_2.1.5       tidyr_1.3.1      \n#&gt; [25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.0      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] mime_0.12           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [25] Matrix_1.7-2        R6_2.6.1            fastmap_1.2.0      \n#&gt;  [28] rbibutils_2.3       shiny_1.10.0        digest_0.6.37      \n#&gt;  [31] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n#&gt;  [34] rprojroot_2.0.4     Hmisc_5.2-2         labeling_0.4.3     \n#&gt;  [37] timechange_0.3.0    abind_1.4-8         compiler_4.4.2     \n#&gt;  [40] withr_3.0.2         glasso_1.11         htmlTable_2.4.3    \n#&gt;  [43] backports_1.5.0     carData_3.0-5       ggsignif_0.6.4     \n#&gt;  [46] corpcor_1.6.10      gtools_3.9.5        tools_4.4.2        \n#&gt;  [49] pbivnorm_0.6.0      foreign_0.8-88      zip_2.3.2          \n#&gt;  [52] httpuv_1.6.15       nnet_7.3-20         glue_1.8.0         \n#&gt;  [55] quadprog_1.5-8      nlme_3.1-167        promises_1.3.2     \n#&gt;  [58] lisrelToR_0.3       grid_4.4.2          checkmate_2.3.2    \n#&gt;  [61] cluster_2.1.8       reshape2_1.4.4      generics_0.1.3     \n#&gt;  [64] gtable_0.3.6        tzdb_0.4.0          data.table_1.17.0  \n#&gt;  [67] hms_1.1.3           car_3.1-3           sem_3.1-16         \n#&gt;  [70] pillar_1.10.1       rockchalk_1.8.157   later_1.4.1        \n#&gt;  [73] splines_4.4.2       lattice_0.22-6      survival_3.8-3     \n#&gt;  [76] kutils_1.73         tidyselect_1.2.1    miniUI_0.1.1.1     \n#&gt;  [79] pbapply_1.7-2       reformulas_0.4.0    stats4_4.4.2       \n#&gt;  [82] xfun_0.51           qgraph_1.9.8        arm_1.14-4         \n#&gt;  [85] stringi_1.8.4       pacman_0.5.1        boot_1.3-31        \n#&gt;  [88] evaluate_1.0.3      codetools_0.2-20    mi_1.1             \n#&gt;  [91] cli_3.6.4           RcppParallel_5.1.10 rpart_4.1.24       \n#&gt;  [94] xtable_1.8-4        Rdpack_2.6.2        munsell_0.5.1      \n#&gt;  [97] Rcpp_1.0.14         coda_0.19-4.1       png_0.1-8          \n#&gt; [100] XML_3.99-0.18       parallel_4.4.2      jpeg_0.1-10        \n#&gt; [103] lme4_1.1-36         mvtnorm_1.3-3       openxlsx_4.2.8     \n#&gt; [106] rlang_1.1.5         multcomp_1.4-28     mnormt_2.1.1\n\n\n\n\n\nBrown, T. A. (2015). Confirmatory factor analysis for applied research. Guilford publications.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.\n\n\nSaqr, M., & López-Pernas, S. (2024). Learning analytics methods and tutorials: A practical guide using R. Springer Nature.\n\n\nSvetina, D., Rutkowski, L., & Rutkowski, D. (2020). Multiple-group invariance with categorical outcomes using updated guidelines: an illustration using M plus and the lavaan/semtools packages. Structural Equation Modeling: A Multidisciplinary Journal, 27(1), 111–130.\n\n\nWu, H., & Estabrook, R. (2016). Identification of confirmatory factor analysis models of different levels of invariance for ordered categorical outcomes. Psychometrika, 81(4), 1014–1045.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Invarianza di misura</span>"
    ]
  },
  {
    "objectID": "chapters/sem/08_multilevel_sem.html",
    "href": "chapters/sem/08_multilevel_sem.html",
    "title": "55  Modelli di Equazioni Strutturali Multilivello",
    "section": "",
    "text": "55.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nNella ricerca psicologica e nelle scienze sociali, i dati raccolti spesso mostrano strutture complesse a più livelli, in cui le informazioni sono organizzate in gruppi o cluster, e le osservazioni all’interno di ogni cluster tendono a essere correlate tra loro. Questo fenomeno rappresenta un aspetto cruciale poiché, in molte situazioni, i modelli classici di equazioni strutturali (SEM) non sono adatti per analizzare dati di questo tipo. La principale limitazione dei modelli SEM tradizionali risiede nella loro incapacità di tenere conto delle correlazioni intrinseche che caratterizzano i dati strutturati su più livelli, il che può portare a stime distorte e inefficaci.\nDi conseguenza, per analizzare i dati a struttura multilivello, è necessario estendere l’approccio SEM classico integrando una modellazione adatta a tale struttura. La modellazione delle equazioni strutturali multilivello (multilevel SEM) introduce variabili latenti progettate per catturare sia la variazione tra i cluster sia quella all’interno dei cluster. In questo modo, le variabili osservate sono influenzate da fattori latenti che operano sia a livello individuale che a livello di gruppo.\nQuesta strategia consente di distinguere tra la variazione sistematica tra i gruppi (variazione tra i cluster) e la variazione individuale all’interno dei gruppi (variazione intra-cluster), permettendo un’analisi più precisa e rappresentativa dei dati complessi tipici delle scienze sociali e psicologiche. Adottando questo approccio, si ottiene una visione più completa e informativa delle dinamiche che sottostanno ai fenomeni studiati, in quanto il modello multilevel SEM è in grado di cogliere sia le differenze tra gruppi sia le specificità individuali all’interno dei gruppi.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Modelli di Equazioni Strutturali Multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/08_multilevel_sem.html#concetto-generale-per-la-modellazione-a-equazioni-strutturali-multilivello",
    "href": "chapters/sem/08_multilevel_sem.html#concetto-generale-per-la-modellazione-a-equazioni-strutturali-multilivello",
    "title": "55  Modelli di Equazioni Strutturali Multilivello",
    "section": "55.2 Concetto Generale per la Modellazione a Equazioni Strutturali Multilivello",
    "text": "55.2 Concetto Generale per la Modellazione a Equazioni Strutturali Multilivello\nLa tipica implementazione della modellazione a equazioni strutturali multilivello (SEM) prevede la scomposizione dell’outcome osservato in due componenti, una per descrivere la varianza a livello “within” (all’interno) e l’altra a livello “between” (tra gruppi), come segue:\n\\[\n\\bar{y}_{ij} - \\bar{y}_{..} = (\\bar{y}_{ij} - \\bar{y}_{.j}) + (\\bar{y}_{.j} - \\bar{y}_{..}),\n\\]\ndove \\(j\\) è l’indicatore del cluster \\(j\\)-esimo (ad esempio, una scuola, come nell’esempio sopra), con \\(j = 1, \\dots, J\\) e \\(i\\) rappresenta l’indicatore dell’individuo \\(i\\)-esimo all’interno del cluster, con \\(i = 1, \\dots, n_j\\). Il termine \\(\\bar{y}_{.j}\\) indica la media a livello di cluster per il cluster \\(j\\), mentre \\(\\bar{y}_{..}\\) rappresenta la media complessiva.\nQuesta equazione corrisponde alla scomposizione della matrice di covarianza della popolazione in componenti “within” e “between”:\n\\[\n\\text{Cov}(y) = \\Sigma_T = \\Sigma_W + \\Sigma_B.\n\\]\nBasandosi su questa scomposizione, è possibile costruire una funzione di verosimiglianza per stimare i parametri associati ai pesi fattoriali, ai coefficienti di percorso e alle varianze residue nei modelli di equazioni strutturali. Gli errori standard e gli intervalli di credibilità si possono ottenere grazie alla teoria della stima di massima verosimiglianza per l’inferenza statistica. Questa stima viene implementata in lavaan basandosi sui metodi descritti da McDonald e Goldstein (1989).",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Modelli di Equazioni Strutturali Multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/08_multilevel_sem.html#un-esempio-pratico",
    "href": "chapters/sem/08_multilevel_sem.html#un-esempio-pratico",
    "title": "55  Modelli di Equazioni Strutturali Multilivello",
    "section": "55.3 Un Esempio Pratico",
    "text": "55.3 Un Esempio Pratico\nIn questo capitolo introdurremo l’implementazione in lavaan per l’analisi SEM multilivello seguendo il tutorial fornito sul sito di lavaan.org.\n\nUtilizzeremo un set di dati artificiali ricavato dal sito di MPlus.\n\n\ndat &lt;- read.table(\"http://statmodel.com/usersguide/chap9/ex9.6.dat\")\nnames(dat) &lt;- c(\"y1\", \"y2\", \"y3\", \"y4\", \"x1\", \"x2\", \"w\", \"clus\")\nhead(dat)\n\n\nA data.frame: 6 x 8\n\n\n\ny1\ny2\ny3\ny4\nx1\nx2\nw\nclus\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n\n\n\n\n1\n2.2033\n1.859\n1.7385\n2.245\n1.143\n-0.797\n-0.150\n1\n\n\n2\n1.9349\n2.128\n0.0831\n2.509\n1.949\n-0.123\n-0.150\n1\n\n\n3\n0.3220\n0.977\n-0.8354\n0.558\n-0.716\n-0.767\n-0.150\n1\n\n\n4\n0.0732\n-1.743\n-2.3103\n-1.514\n-2.649\n0.638\n-0.150\n1\n\n\n5\n-1.2149\n0.453\n0.3726\n-1.790\n-0.263\n0.303\n-0.150\n1\n\n\n6\n0.2983\n-1.820\n0.5613\n-2.091\n-0.945\n1.363\n0.319\n2\n\n\n\n\n\nIl data frame è costituito da 1000 righe:\n\ndim(dat) |&gt; print()\n\n[1] 1000    8\n\n\n\nCi sono 110 cluster (clus), il che significa che ci sono misure ripetute per ciascun cluster (possiamo immaginare che i cluster corrispondano ai soggetti):\n\nlength(unique(dat$clus))\n\n110\n\n\n\nAnalizzeremo questi dati mediante un modello di equazioni strutturali multilivello. Iniziamo a definire il modello SEM appropriato per questi dati.\n\nmodel &lt;- \"\n    level: 1\n        fw =~ y1 + y2 + y3 + y4\n        fw ~ x1 + x2\n\n    level: 2\n        fb =~ y1 + y2 + y3 + y4\n\n    # optional\n    y1 ~~ 0*y1\n    y2 ~~ 0*y2\n    y3 ~~ 0*y3\n    y4 ~~ 0*y4\n    fb ~ w\n\"\n\nQuesta sintassi del modello è strutturata su due livelli, uno per il livello 1 (intra-cluster) e uno per il livello 2 (inter-cluster). All’interno di ciascun livello, è possibile specificare un modello come nel caso a livello singolo, ma con una distinzione importante: ogni livello rappresenta fonti di variabilità differenti, in questo caso tra le misurazioni individuali (livello 1) e le differenze tra gruppi o cluster (livello 2).\n\n55.3.1 Spiegazione dei livelli\n\nLivello 1 (Intra-cluster):\n\nFattore latente fw: Al livello individuale, fw è un fattore latente che riflette la variazione tra le quattro variabili osservate y1, y2, y3 e y4. La sintassi fw =~ y1 + y2 + y3 + y4 indica che fw è il costrutto latente che sottende questi indicatori osservabili.\nEffetto dei predittori x1 e x2: Al livello individuale, fw è modellato in funzione dei predittori x1 e x2 (fw ~ x1 + x2), che rappresentano variabili a livello intra-cluster che possono influenzare il fattore latente fw. Questo permette di catturare come i predittori influenzano la variabilità nelle risposte individuali.\n\nLivello 2 (Inter-cluster):\n\nFattore latente fb: Al livello del cluster, fb rappresenta un secondo fattore latente che viene definito anch’esso dalle variabili y1, y2, y3 e y4, ma con una prospettiva inter-cluster. Questo livello considera quindi la variazione nei punteggi medi del cluster piuttosto che nelle risposte individuali.\nEffetto del predittore w: Al livello di cluster, fb è modellato come funzione del predittore w (fb ~ w), che rappresenta una variabile esplicativa per le differenze tra cluster.\n\n\n\n\n55.3.2 Parte opzionale\nLa sezione opzionale, che include espressioni come y1 ~~ 0*y1, specifica la varianza residua delle variabili osservate y1, y2, y3 e y4 al livello di cluster. L’uso di 0*y1, 0*y2, etc., indica che la varianza residua a livello inter-cluster viene fissata a zero, assumendo che tutta la varianza tra cluster sia spiegata da fb.\nIn sintesi, questo modello permette di distinguere come i fattori latenti (fw e fb) siano influenzati rispettivamente da variabili a livello individuale e cluster, consentendo di modellare simultaneamente la variabilità intra- e inter-cluster.\n\n\n55.3.3 Coefficiente di Correlazione Intraclasse (ICC)\nL’analisi SEM Multilivello permette di calcolare il Coefficiente di Correlazione Intraclasse (ICC), una misura statistica utile in studi dove i dati sono raggruppati in cluster o gruppi (come ad esempio soggetti all’interno di classi o pazienti all’interno di ospedali). L’ICC valuta il grado di somiglianza o omogeneità delle misurazioni all’interno di ciascun gruppo rispetto alla variazione totale nei dati.\nPiù precisamente, l’ICC quantifica la proporzione della varianza totale che può essere attribuita alle differenze tra i gruppi piuttosto che a quelle all’interno dei gruppi. Quando l’ICC è elevato, significa che una parte rilevante della varianza osservata nei dati deriva dalle differenze tra i gruppi. In questo caso, le misurazioni all’interno dello stesso gruppo tendono a essere più simili tra loro rispetto a quelle di gruppi differenti. Al contrario, un ICC basso indica che la varianza è in gran parte dovuta alle differenze individuali all’interno dei gruppi, suggerendo una scarsa influenza del raggruppamento.\nIn sintesi, l’ICC è un indice di quanto “forte” sia l’effetto del raggruppamento sulle misurazioni, informando sulla necessità di considerare la struttura multilivello dei dati nell’analisi.\n\n\n55.3.4 ICC nei Modelli SEM Multilivello\nIl calcolo dell’ICC (Intra-Class Correlation) per ciascuna variabile osservata in un modello SEM multilivello è essenziale per comprendere la struttura gerarchica dei dati e la variabilità tra i gruppi. L’ICC quantifica la proporzione della varianza totale di una variabile che può essere attribuita a differenze tra gruppi piuttosto che a variazioni individuali all’interno dei gruppi. Un ICC elevato per una variabile suggerisce che l’appartenenza a un determinato gruppo (come una scuola, una famiglia o un ospedale) ha un’influenza rilevante su quella variabile, indicando che una parte consistente della variazione osservata è dovuta alle differenze tra gruppi piuttosto che alle differenze tra individui all’interno di ciascun gruppo.\nIn pratica, l’ICC è un criterio utile per determinare l’adeguatezza di un modello multilivello. Un ICC basso suggerisce che la variabilità tra i gruppi è limitata e che potrebbe non essere necessario utilizzare un modello multilivello complesso, in quanto le differenze individuali rappresentano la maggior parte della variabilità. Al contrario, un ICC alto indica che la struttura a cluster dei dati è rilevante e che ignorarla potrebbe portare a stime distorte e a conclusioni potenzialmente errate.\nUtilizzando l’ICC come guida, i ricercatori possono decidere se e in che misura adottare un approccio multilivello per rappresentare in modo accurato e affidabile la varianza attribuibile a fattori tra e intra-gruppo, ottenendo così una comprensione più precisa dei fenomeni studiati.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Modelli di Equazioni Strutturali Multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/08_multilevel_sem.html#calcolo-dellicc-con-lmer",
    "href": "chapters/sem/08_multilevel_sem.html#calcolo-dellicc-con-lmer",
    "title": "55  Modelli di Equazioni Strutturali Multilivello",
    "section": "55.4 Calcolo dell’ICC con lmer",
    "text": "55.4 Calcolo dell’ICC con lmer\nPer iniziare, poniamoci il problema di calcolare l’ICC mediante un modello lineare multilivello. Svolgeremo questi calcoli con lmer. Il modello lmer considera ogni variabile osservata separatamente, fornendo un’analisi indipendente per ciascuna. Per y1, per esempio, abbiamo:\n\nmodel_lmer &lt;- lmer(y1 ~ 1 + (1 | clus), data = dat)\n\nCalcoliamo l’ICC:\n\n varianza_cluster &lt;- VarCorr(model_lmer)$clus[1]\n varianza_residua &lt;- attr(VarCorr(model_lmer), \"sc\")^2\n ICC &lt;- varianza_cluster / (varianza_cluster + varianza_residua)\n ICC\n\n0.129536196128802\n\n\n\nIl Coefficiente di Correlazione Intraclass (ICC) di 0.129 per la variabile y1 significa che circa il 12.9% della variazione totale in y1 è ascrivibile alle differenze tra gli studenti, considerati come unità separate o cluster individuali. Questa percentuale relativamente modesta della variazione totale suggerisce che le caratteristiche o i comportamenti individuali degli studenti spiegano solo una piccola parte della variazione osservata in y1.\nUn ICC di questo livello, che si può considerare relativamente basso, implica che la maggior parte della variazione nella variabile non è legata in modo sostanziale alle differenze tra gli studenti. Questo può essere indicativo del fatto che altri fattori, esterni alle caratteristiche individuali degli studenti, giocano un ruolo più determinante. In un contesto educativo, ad esempio, questo potrebbe suggerire che elementi come l’ambiente scolastico, le metodologie didattiche impiegate, o le specificità del programma di studi, hanno un impatto maggiore sulla variazione di y1 rispetto alle differenze individuali tra gli studenti.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Modelli di Equazioni Strutturali Multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/08_multilevel_sem.html#calcolo-dellicc-con-sem",
    "href": "chapters/sem/08_multilevel_sem.html#calcolo-dellicc-con-sem",
    "title": "55  Modelli di Equazioni Strutturali Multilivello",
    "section": "55.5 Calcolo dell’ICC con sem",
    "text": "55.5 Calcolo dell’ICC con sem\nOra calcoliamo l’ICC utilizzando un modello SEM multilivello. Per adattare questo modello, è necessario aggiungere l’argomento cluster= nella chiamata alla funzione sem di lavaan. Questo argomento specifica la variabile di raggruppamento, permettendo al modello di tenere conto della struttura a cluster dei dati e di stimare l’ICC per ciascuna variabile osservata.\n\n fit &lt;- sem(model,\n     data = dat,\n     cluster = \"clus\",\n     fixed.x = FALSE\n )\n\nL’argomento fixed.x controlla come vengono trattate le variabili predittive (o esogene) all’interno del modello.\nQuando fixed.x = FALSE, si sta indicando a lavaan di trattare le variabili esogene non come valori fissi, ma come variabili aleatorie con una propria varianza e covarianza da stimare nel modello. Questo significa che le variabili esogene non sono considerate “date” o senza errore, ma il modello tiene conto della loro variabilità.\n\nfixed.x = TRUE (impostazione predefinita in lavaan):\n\nLe variabili esogene sono considerate senza errore (cioè come dati “fissi”).\nNon si stima la varianza delle variabili esogene.\nQuesto approccio è tipico nei modelli di regressione classici, dove le variabili predittive sono trattate come note e prive di errore.\n\nfixed.x = FALSE:\n\nLe variabili esogene sono considerate come variabili aleatorie con errori di misurazione, quindi la loro varianza e covarianza vengono stimate.\nQuesto approccio è più realistico in molti contesti psicologici e sociali, dove è ragionevole assumere che anche le variabili esogene possano contenere errori.\n\n\nIn molte situazioni di ricerca, le variabili esogene (come i punteggi dei questionari o le misure di osservazione) non sono perfettamente accurate e possono contenere errore. Impostare fixed.x = FALSE consente di modellare questa incertezza, offrendo una rappresentazione più realistica dei dati.\n\nsummary(fit) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 27 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        26\n\n  Number of observations                          1000\n  Number of clusters [clus]                        110\n\nModel Test User Model:\n                                                      \n  Test statistic                                 3.863\n  Degrees of freedom                                17\n  P-value (Chi-square)                           1.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\n\nLevel 1 [within]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  fw =~                                               \n    y1                1.000                           \n    y2                0.999    0.033   30.735    0.000\n    y3                0.995    0.033   29.804    0.000\n    y4                1.017    0.033   30.364    0.000\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  fw ~                                                \n    x1                0.973    0.042   23.287    0.000\n    x2                0.510    0.038   13.422    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  x1 ~~                                               \n    x2                0.032    0.032    1.014    0.311\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    x1                0.007    0.031    0.222    0.825\n    x2                0.014    0.032    0.440    0.660\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .y1                0.981    0.057   17.151    0.000\n   .y2                0.948    0.056   17.015    0.000\n   .y3                1.070    0.060   17.700    0.000\n   .y4                1.014    0.059   17.182    0.000\n   .fw                0.980    0.071   13.888    0.000\n    x1                0.985    0.044   22.361    0.000\n    x2                1.017    0.045   22.361    0.000\n\n\nLevel 2 [clus]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  fb =~                                               \n    y1                1.000                           \n    y2                0.960    0.073   13.078    0.000\n    y3                0.924    0.074   12.452    0.000\n    y4                0.949    0.075   12.631    0.000\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  fb ~                                                \n    w                 0.344    0.078    4.429    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .y1               -0.083    0.074   -1.128    0.259\n   .y2               -0.077    0.071   -1.081    0.280\n   .y3               -0.045    0.071   -0.637    0.524\n   .y4               -0.030    0.072   -0.418    0.676\n    w                 0.006    0.086    0.070    0.944\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .y1                0.000                           \n   .y2                0.000                           \n   .y3                0.000                           \n   .y4                0.000                           \n   .fb                0.361    0.078    4.643    0.000\n    w                 0.815    0.110    7.416    0.000\n\n\n\n\nfitMeasures(fit) |&gt;\n    print()\n\n                 npar                  fmin                 chisq \n               26.000                 3.913                 3.863 \n                   df                pvalue        baseline.chisq \n               17.000                 1.000              3283.563 \n          baseline.df       baseline.pvalue                   cfi \n               24.000                 0.000                 1.000 \n                  tli                  nnfi                   rfi \n                1.006                 1.006                 0.998 \n                  nfi                  pnfi                   ifi \n                0.999                 0.707                 1.004 \n                  rni                  logl     unrestricted.logl \n                1.004             -9527.429             -9525.497 \n                  aic                   bic                ntotal \n            19106.857             19234.459              1000.000 \n                 bic2                 rmsea        rmsea.ci.lower \n            19151.882                 0.000                 0.000 \n       rmsea.ci.upper        rmsea.ci.level          rmsea.pvalue \n                0.000                 0.900                 1.000 \n       rmsea.close.h0 rmsea.notclose.pvalue     rmsea.notclose.h0 \n                0.050                 0.000                 0.080 \n                 srmr           srmr_within          srmr_between \n                0.022                 0.004                 0.017 \n\n\nQuando calcoliamo l’ICC utilizzando lavaan, otteniamo valori distinti per ciascuna variabile osservata all’interno del modello multilivello. Questi valori rappresentano la proporzione di varianza in ogni variabile che è attribuibile alle differenze tra i gruppi, considerando le relazioni specificate nel modello SEM. In altre parole, l’ICC di ciascun item riflette quanto le differenze tra i gruppi influenzano quella particolare variabile, nel contesto delle dipendenze definite dal modello.\nPer ottenere l’ICC per ciascuno dei quattro item, è possibile utilizzare il comando:\n\nlavInspect(fit, \"icc\") |&gt;\n    print()\n\n   y1    y2    y3    y4    x1    x2 \n0.125 0.121 0.106 0.115 0.000 0.000 \n\n\n\nNel caso di y1, la stima di ICC fornita dal modello SEM multilivello è molto simile al risultato ottenuto con lmer.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Modelli di Equazioni Strutturali Multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/08_multilevel_sem.html#riflessioni-conclusive",
    "href": "chapters/sem/08_multilevel_sem.html#riflessioni-conclusive",
    "title": "55  Modelli di Equazioni Strutturali Multilivello",
    "section": "55.6 Riflessioni Conclusive",
    "text": "55.6 Riflessioni Conclusive\nIn questo capitolo, abbiamo illustrato il modello di equazioni strutturali multilivello utilizzando lavaan. Come evidenziato dall’esempio, l’implementazione in lavaan è molto diretta, richiedendo solo l’inclusione dell’opzione cluster nella funzione sem. È importante sottolineare che, al momento, lavaan supporta solo modelli SEM a due livelli.\nNell’ambito dei modelli SEM multilivello, abbiamo visto come l’interpretazione dei coefficienti di correlazione intra-classe (ICC) possa fornire intuizioni significative sulla variazione dei dati all’interno di gruppi o cluster. Un ICC basso, come quello osservato nell’esempio (0.129), indica che una porzione minore della variazione totale è attribuibile alle differenze tra i cluster. Nel contesto specifico dei nostri dati, dove ogni studente è considerato un cluster individuale, ciò suggerisce che fattori esterni agli studenti stessi potrebbero giocare un ruolo più significativo nella variazione osservata rispetto alle caratteristiche individuali degli studenti.\nIn conclusione, la modellazione di equazioni strutturali multilivello è uno strumento potente e flessibile nell’analisi di dati strutturati gerarchicamente. lavaan, sebbene limitato ai modelli a due livelli, fornisce un approccio accessibile e diretto per questi tipi di analisi. Per modelli più complessi e a più livelli, Mplus offre soluzioni alternative che possono gestire una gamma più ampia di esigenze analitiche.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Modelli di Equazioni Strutturali Multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/08_multilevel_sem.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/sem/08_multilevel_sem.html#informazioni-sullambiente-di-sviluppo",
    "title": "55  Modelli di Equazioni Strutturali Multilivello",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lme4_1.1-35.5     Matrix_1.7-1      MASS_7.3-61       viridis_0.6.5    \n [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n [9] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n[13] lavaan_0.6-19     psych_2.4.6.26    scales_1.3.0      markdown_1.13    \n[17] knitr_1.49        lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1    \n[21] dplyr_1.1.4       purrr_1.0.2       readr_2.1.5       tidyr_1.3.1      \n[25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.17.1   jsonlite_1.8.9      magrittr_2.0.3     \n  [4] TH.data_1.1-2       estimability_1.5.1  farver_2.1.2       \n  [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n [19] emmeans_1.10.5      zoo_1.8-12          uuid_1.2-1         \n [22] igraph_2.1.1        mime_0.12           lifecycle_1.0.4    \n [25] pkgconfig_2.0.3     R6_2.5.1            fastmap_1.2.0      \n [28] shiny_1.9.1         digest_0.6.37       OpenMx_2.21.13     \n [31] fdrtool_1.2.18      colorspace_2.1-1    rprojroot_2.0.4    \n [34] Hmisc_5.2-0         fansi_1.0.6         timechange_0.3.0   \n [37] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n [40] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n [43] carData_3.0-5       ggsignif_0.6.4      corpcor_1.6.10     \n [46] gtools_3.9.5        tools_4.4.2         pbivnorm_0.6.0     \n [49] foreign_0.8-87      zip_2.3.1           httpuv_1.6.15      \n [52] nnet_7.3-19         glue_1.8.0          quadprog_1.5-8     \n [55] promises_1.3.0      nlme_3.1-166        lisrelToR_0.3      \n [58] grid_4.4.2          pbdZMQ_0.3-13       checkmate_2.3.2    \n [61] cluster_2.1.6       reshape2_1.4.4      generics_0.1.3     \n [64] gtable_0.3.6        tzdb_0.4.0          data.table_1.16.2  \n [67] hms_1.1.3           car_3.1-3           utf8_1.2.4         \n [70] sem_3.1-16          pillar_1.9.0        IRdisplay_1.1      \n [73] rockchalk_1.8.157   later_1.3.2         splines_4.4.2      \n [76] cherryblossom_0.1.0 lattice_0.22-6      survival_3.7-0     \n [79] kutils_1.73         tidyselect_1.2.1    miniUI_0.1.1.1     \n [82] pbapply_1.7-2       airports_0.1.0      stats4_4.4.2       \n [85] xfun_0.49           qgraph_1.9.8        arm_1.14-4         \n [88] stringi_1.8.4       pacman_0.5.1        boot_1.3-31        \n [91] evaluate_1.0.1      codetools_0.2-20    mi_1.1             \n [94] cli_3.6.3           RcppParallel_5.1.9  IRkernel_1.3.2     \n [97] rpart_4.1.23        xtable_1.8-4        repr_1.1.7         \n[100] munsell_0.5.1       Rcpp_1.0.13-1       coda_0.19-4.1      \n[103] png_0.1-8           XML_3.99-0.17       parallel_4.4.2     \n[106] usdata_0.3.1        jpeg_0.1-10         mvtnorm_1.3-2      \n[109] openxlsx_4.2.7.1    crayon_1.5.3        openintro_2.5.0    \n[112] rlang_1.1.4         multcomp_1.4-26     mnormt_2.1.1",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Modelli di Equazioni Strutturali Multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/09_structural_regr.html",
    "href": "chapters/sem/09_structural_regr.html",
    "title": "56  Modelli di Regressione Strutturale",
    "section": "",
    "text": "56.1 Introduzione\nUn modello comunemente utilizzato nell’analisi delle equazioni strutturali (SEM) è il modello di regressione strutturale (Sr), noto anche come modello di percorso con variabili latenti o modello LISREL completo. Questo tipo di modello combina due componenti principali:\nIl capitolo inizia affrontando la specificazione dei modelli Sr con indicatori continui, analizzando i requisiti per garantirne l’identificazione. In particolare, viene discusso come stabilire condizioni che consentano una stima coerente e interpretabile dei parametri del modello.\nSuccessivamente, vengono presentate due strategie distinte per analizzare modelli Sr completi, in cui tutte le variabili nel modello strutturale sono fattori comuni con molteplici indicatori. Queste strategie si concentrano su:\nInoltre, il capitolo esplora i modelli Sr parziali, in cui alcune variabili nella parte strutturale del modello sono rappresentate da indicatori singoli anziché da fattori comuni con molteplici indicatori. Viene illustrato un metodo specifico per gestire questo tipo di indicatori, permettendo di controllare esplicitamente gli errori di misurazione associati senza influire negativamente sull’adattamento globale del modello.\nIl capitolo evidenzia l’importanza di distinguere chiaramente tra le componenti di misurazione e strutturali nei modelli Sr. Questo approccio non solo migliora la comprensione delle relazioni tra variabili latenti, ma aiuta anche a mitigare il rischio di errori di specificazione, aumentando l’affidabilità dei risultati ottenuti dall’analisi SEM.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Modelli di Regressione Strutturale</span>"
    ]
  },
  {
    "objectID": "chapters/sem/09_structural_regr.html#introduzione",
    "href": "chapters/sem/09_structural_regr.html#introduzione",
    "title": "56  Modelli di Regressione Strutturale",
    "section": "",
    "text": "Parte strutturale: rappresenta le ipotesi sugli effetti diretti e indiretti tra le variabili latenti o osservate. Questa sezione descrive le relazioni causali tra i fattori comuni.\n\nParte di misurazione: rappresenta la relazione tra i fattori latenti e i loro indicatori osservati, definendo come i fattori comuni si manifestano attraverso le variabili misurate.\n\n\n\n\n\nIdentificazione delle fonti di errore di specificazione: analisi che separa la valutazione della parte di misurazione (validità degli indicatori) dall’analisi della parte strutturale (relazioni tra variabili latenti).\n\nValutazione del modello: tecniche per comprendere come gli errori nella parte di misurazione influenzino le inferenze strutturali.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Modelli di Regressione Strutturale</span>"
    ]
  },
  {
    "objectID": "chapters/sem/09_structural_regr.html#modello-di-regressione-strutturale-completo",
    "href": "chapters/sem/09_structural_regr.html#modello-di-regressione-strutturale-completo",
    "title": "56  Modelli di Regressione Strutturale",
    "section": "\n56.2 Modello di Regressione Strutturale Completo",
    "text": "56.2 Modello di Regressione Strutturale Completo\nNella Figura 56.1 (a) è rappresentato un modello di percorso con variabili manifeste. Viene assunto che la variabile esogena X1 sia misurata senza errore, sebbene questa assunzione sia spesso violata nella pratica. Le variabili endogene nel modello, come Y1 e Y4, possono avere errori casuali che si manifestano nelle loro perturbazioni.\n\n\n\n\n\nFigura 56.1: Esempi di un modello di percorso con variabili manifeste (a) e di un corrispondente modello di regressione strutturale completo con indicatori multipli per ogni fattore comune nella parte strutturale (b) (Figura tratta da Kline, 2023).\n\n\nLa Figura 56.1 (b) illustra un modello di Regressione Strutturale (SR) completo, che integra sia componenti strutturali sia di misurazione. In questo modello SR, a differenza del modello di percorso, ciascun indicatore (X1, Y1, Y4) è definito come uno tra numerosi indicatori associati a un fattore comune. Di conseguenza, tutte le variabili osservabili in questa figura includono termini di errore.\nNella parte strutturale del modello, presentata nella Figura 56.1 (b), si osserva la rappresentazione degli stessi schemi di effetti causali diretti e indiretti trovati nel modello di percorso mostrato nella 56.1, ma applicati ai fattori comuni.\nPer quanto riguarda l’analisi delle medie, le osservazioni e i parametri nei modelli SR sono trattati allo stesso modo di quelli nei modelli di percorso e nei modelli di Analisi Fattoriale Confermativa (CFA), conformemente alle regole precedentemente stabilite.\nL’identificazione di un modello SR completo avviene quando sia la sua componente di misurazione, riformulata come un modello CFA, sia la parte strutturale risultano identificate. La regola di identificazione in due fasi implica che, per determinare se un modello SR completo sia identificato, è necessario esaminare separatamente ciascuna delle sue parti, ovvero quelle di misurazione e strutturale.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Modelli di Regressione Strutturale</span>"
    ]
  },
  {
    "objectID": "chapters/sem/09_structural_regr.html#modellazione-in-due-fasi",
    "href": "chapters/sem/09_structural_regr.html#modellazione-in-due-fasi",
    "title": "56  Modelli di Regressione Strutturale",
    "section": "\n56.3 Modellazione in Due Fasi",
    "text": "56.3 Modellazione in Due Fasi\nImmaginiamo che un ricercatore abbia definito un modello di Regressione Strutturale (SR) completo, come mostrato nella Figura 56.1(a). Dopo aver raccolto i dati, il ricercatore adotta un approccio monofase per analizzare il modello, eseguendo una stima simultanea delle componenti di misurazione e strutturali. Tuttavia, i risultati rivelano che il modello non si adatta bene ai dati. Ciò solleva interrogativi sulla localizzazione del problema: è nella parte di misurazione, nella parte strutturale, o in entrambe? Identificare la fonte del problema con precisione può essere complesso usando un approccio monofase.\n\n\n\n\n\nFigura 56.2: Valutazione della regola in due fasi per l’identificazione di un modello di regressione strutturale completo presentato con simbolismo grafico compatto per i termini di errore degli indicatori nella parte di misurazione e le perturbazioni nella parte strutturale (Figura tratta da Kline, 2023).\n\n\nLa modellazione in due fasi, proposta da Anderson e Gerbing (1988), affronta questa difficoltà separando l’analisi in due momenti distinti.\nNel primo passaggio, il modello SR viene riformulato in un modello CFA per testare esclusivamente le relazioni tra i costrutti latenti e i loro indicatori. Un cattivo adattamento del modello CFA indica problemi nelle ipotesi sulla misurazione (ad esempio, carichi fattoriali errati o struttura del modello incoerente). Solo se il modello CFA è valido si passa al secondo passaggio, in cui vengono analizzate le relazioni strutturali tra i costrutti latenti. In questa fase, si confrontano il modello SR originale e varianti alternative per identificare la struttura più adatta.\n\n56.3.1 Limiti e Sfide della Modellazione in Due Fasi\nLa modellazione in due fasi presenta alcuni limiti. Il processo richiede numerose decisioni, come la riformulazione dei modelli CFA e SR, che possono generare un “giardino dei sentieri che si biforcano”, aumentando il rischio di interpretazioni errate. Inoltre, test ripetuti sullo stesso dataset possono portare a risultati influenzati da variazioni casuali.\nUn altro problema riguarda i modelli equivalenti. Ad esempio, un modello CFA e un modello SR con una parte strutturale appena identificata possono risultare indistinguibili statisticamente. In questi casi, la scelta tra i modelli deve basarsi su criteri teorici o sul design dello studio.\nInfine, se i carichi fattoriali cambiano tra modelli strutturali differenti, ciò indica che il modello di misurazione non è stabile, complicando ulteriormente l’interpretazione.\n\n56.3.2 Considerazioni Finali\nLa modellazione in due fasi offre un metodo utile per diagnosticare e risolvere problemi nei modelli SR, ma va utilizzata con attenzione. Le statistiche di adattamento, come il chi-quadro, il CFI o il RMSEA, sono spesso più sensibili alla parte di misurazione rispetto a quella strutturale. Inoltre, soglie standard per questi indici non sono sempre applicabili universalmente, e l’interpretazione dipende dal tipo di modello e dai dati.\nL’approccio bifase permette di isolare e analizzare separatamente i problemi nelle componenti di misurazione e strutturali, ma richiede un’analisi teoricamente solida per evitare di incorrere in errori metodologici o interpretativi.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Modelli di Regressione Strutturale</span>"
    ]
  },
  {
    "objectID": "chapters/sem/09_structural_regr.html#una-applicazione-concreta",
    "href": "chapters/sem/09_structural_regr.html#una-applicazione-concreta",
    "title": "56  Modelli di Regressione Strutturale",
    "section": "\n56.4 Una Applicazione Concreta",
    "text": "56.4 Una Applicazione Concreta\nLa Figura 56.3 illustra un modello SR (Regressione Strutturale) iniziale che esplora il rendimento scolastico e l’adattamento in aula di studenti di età media corrispondente ai gradi 7-8. Il modello considera l’influenza dell’abilità cognitiva generale e del livello di rischio di disturbi psicopatologici. Uno degli indicatori di rischio deriva dalla diagnosi di disturbi psichiatrici maggiori nei genitori, mentre il secondo è basato sul livello socio-economico (SES) della famiglia, con punteggi più alti che indicano un SES inferiore. Le abilità cognitive sono valutate tramite i punteggi in ragionamento verbale, analisi visivo-spaziale e memoria, ottenuti da un test di QI somministrato individualmente.\n\n\n\n\n\nFigura 56.3: Modello iniziale completo di regressione strutturale del rendimento scolastico e dell’adattamento in classe come funzione dell’abilità cognitiva e del rischio di psicopatologia (Figura tratta da Kline, 2023).\n\n\nIl modello comprende due fattori endogeni: il rendimento scolastico, valutato attraverso test standardizzati di lettura, aritmetica e ortografia, e l’adattamento in classe, misurato con tre indicatori forniti dagli insegnanti riguardo alla motivazione, stabilità emotiva e qualità delle relazioni sociali degli studenti. In questo modello strutturale, sia il rendimento scolastico sia l’adattamento in classe sono influenzati dall’abilità cognitiva e dal rischio, ma non vi è un effetto diretto o una covarianza delle perturbazioni tra questi due fattori endogeni, indicando che eventuali associazioni tra di essi sono attribuibili alle loro cause comuni, i fattori esogeni.\n\n# input the correlations in lower diagnonal form\nworlandLower.cor &lt;- \"\n1.00\n .70 1.00\n .65  .60 1.00\n .55  .50  .45 1.00\n .50  .45  .40  .70 1.00\n .35  .35  .30  .55  .50 1.00\n .30  .30  .30  .50  .45  .44 1.00\n .25  .20  .22  .41  .28  .34  .40 1.00\n .35  .32  .32  .48  .45  .42  .60  .45 1.00\n-.25 -.24 -.22 -.21 -.18 -.15 -.15 -.12 -.17 1.00\n-.22 -.26 -.30 -.25 -.22 -.18 -.17 -.14 -.20  .42 1.00 \"\n\n# name the variables and convert to full correlation matrix\nworland.cor &lt;- lavaan::getCov(worlandLower.cor, names = c(\n    \"verbal\", \"visual\",\n    \"memory\", \"read\", \"math\", \"spell\", \"motive\", \"harmony\", \"stable\", \"parent\", \"ses\"\n))\n\n# add the standard deviations and convert to covariances\nworland.cov &lt;- lavaan::cor2cov(worland.cor,\n    sds = c(\n        13.75, 14.80, 12.60, 14.90, 15.25, 13.85, 9.50, 11.10, 8.70,\n        12.00, 8.50\n    )\n)\n\nPrimo Passaggio nella modellazione in due fasi per un modello SR completo di rendimento scolastico e adattamento in classe\n\n# 4-factor CFA\nworlandCFA.model &lt;- \"\n    Cognitive =~ verbal + visual + memory\n    Achieve =~ read + math + spell\n    Adjust =~ motive + harmony + stable\n    Risk =~ parent + ses \n \"\n\n\nworlandCFA &lt;- lavaan::cfa(worlandCFA.model,\n    sample.cov = worland.cov,\n    sample.nobs = 158\n)\n\n\nsemPlot::semPaths(worlandCFA,\n    what = \"col\", whatLabels = \"par\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\n\nfitMeasures(worlandCFA, c(\"chisq\", \"df\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt; print()\n#&gt;  chisq     df    cfi    tli  rmsea   srmr \n#&gt; 16.212 38.000  1.000  1.049  0.000  0.023\n\n\n# predicted correlation matrix for factors\nlavaan::lavInspect(worlandCFA, \"cor.lv\") |&gt; print()\n#&gt;           Cogntv Achiev Adjust   Risk\n#&gt; Cognitive  1.000                     \n#&gt; Achieve    0.703  1.000              \n#&gt; Adjust     0.500  0.751  1.000       \n#&gt; Risk      -0.459 -0.401 -0.349  1.000\n\n\nlavaan::residuals(worlandCFA, type = \"standardized.mplus\") |&gt; print()\n#&gt; $type\n#&gt; [1] \"standardized.mplus\"\n#&gt; \n#&gt; $cov\n#&gt;         verbal visual memory   read   math  spell motive harmny stable\n#&gt; verbal   0.000                                                        \n#&gt; visual   0.000  0.000                                                 \n#&gt; memory   0.123 -0.130  0.000                                          \n#&gt; read     0.598  0.113 -0.285  0.000                                   \n#&gt; math     0.505  0.038 -0.377  0.597  0.000                            \n#&gt; spell   -0.952 -0.255 -0.704 -0.667 -0.206  0.000                     \n#&gt; motive  -0.821 -0.157  0.310 -0.042 -0.078  1.484  0.000              \n#&gt; harmony  0.117 -0.453  0.171  0.942 -1.179  1.196 -1.010  0.000       \n#&gt; stable   0.285  0.112  0.566 -1.893 -0.467  0.942  0.422  0.436  0.000\n#&gt; parent  -0.092 -0.260 -0.157  0.234  0.379  0.157  0.301  0.020 -0.022\n#&gt; ses      1.890 -0.247 -1.417 -0.370 -0.114 -0.112  0.213 -0.125 -0.362\n#&gt;         parent    ses\n#&gt; verbal               \n#&gt; visual               \n#&gt; memory               \n#&gt; read                 \n#&gt; math                 \n#&gt; spell                \n#&gt; motive               \n#&gt; harmony              \n#&gt; stable               \n#&gt; parent   0.000       \n#&gt; ses      0.000  0.000\n\n\nlavaan::residuals(worlandCFA, type = \"cor.bollen\") |&gt; print()\n#&gt; $type\n#&gt; [1] \"cor.bollen\"\n#&gt; \n#&gt; $cov\n#&gt;         verbal visual memory   read   math  spell motive harmny stable\n#&gt; verbal   0.000                                                        \n#&gt; visual   0.000  0.000                                                 \n#&gt; memory   0.001 -0.002  0.000                                          \n#&gt; read     0.015  0.003 -0.010  0.000                                   \n#&gt; math     0.017  0.001 -0.016  0.007  0.000                            \n#&gt; spell   -0.040 -0.012 -0.035 -0.010 -0.006  0.000                     \n#&gt; motive  -0.031 -0.007  0.016 -0.001 -0.003  0.075  0.000              \n#&gt; harmony  0.006 -0.026  0.010  0.041 -0.053  0.071 -0.027  0.000       \n#&gt; stable   0.011  0.005  0.028 -0.034 -0.015  0.045  0.005  0.012  0.000\n#&gt; parent  -0.003 -0.010 -0.007  0.008  0.017  0.009  0.015  0.001 -0.001\n#&gt; ses      0.050 -0.010 -0.068 -0.012 -0.005 -0.006  0.010 -0.008 -0.016\n#&gt;         parent    ses\n#&gt; verbal               \n#&gt; visual               \n#&gt; memory               \n#&gt; read                 \n#&gt; math                 \n#&gt; spell                \n#&gt; motive               \n#&gt; harmony              \n#&gt; stable               \n#&gt; parent   0.000       \n#&gt; ses      0.000  0.000\n\n\n# calculate factor reliability coefficients (semTools)\nsemTools::reliability(worlandCFA) |&gt; print()\n#&gt;        Cognitive Achieve Adjust  Risk\n#&gt; alpha      0.846   0.809  0.725 0.568\n#&gt; omega      0.851   0.821  0.730 0.577\n#&gt; omega2     0.851   0.821  0.730 0.577\n#&gt; omega3     0.852   0.823  0.733 0.577\n#&gt; avevar     0.659   0.610  0.474 0.409\n\nSecondo Passaggio nella modellazione in due fasi per un modello SR completo di rendimento scolastico e adattamento in classe\nI risultati del Passaggio 1 del metodo in due fasi, che si concentrava sul modello di misurazione, consentono di procedere all’analisi del modello SR originale, che prevede cinque percorsi nella Figura 56.3, nel Passaggio 2 del metodo. Anche questa seconda analisi ha portato a una soluzione ammissibile.\n\n# step 2a\n# 4-factor SR model with 5 paths among factors\n\n# by default, lavaan frees the disturbance covariance\n# between a pair of outcomes in a structural model\n# when there is no direct effect between them\n# thus, this parameter is explicitly fixed to zero\n# in this analysis\n\nworlandSRa_model &lt;- \"\n    # measurement part\n    Cognitive =~ verbal + visual + memory\n    Achieve =~ read + math + spell\n    Adjust =~ motive + harmony + stable\n    Risk =~ parent + ses\n    # structural part (5 paths)\n    Achieve ~ Cognitive + Risk\n    Adjust ~ Cognitive + Risk\n    # constrain disturbance covariance to zero\n    Adjust ~~ 0*Achieve \n\"\n\n\nworlandSRa &lt;- lavaan::sem(worlandSRa_model,\n    sample.cov = worland.cov,\n    sample.nobs = 158\n)\n\n\nsemPaths(\n    worlandSRa,\n    \"std\",\n    posCol = c(\"black\"),\n    edge.label.cex = 1.2,\n    sizeMan = 7,\n    edge.width = 0.4, # Set a fixed width for all arrows\n    fade = FALSE # Disable fading of the arrows\n)\n\n\n\n\n\n\n\n\nfitMeasures(worlandSRa, c(\"chisq\", \"df\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt; print()\n#&gt;  chisq     df    cfi    tli  rmsea   srmr \n#&gt; 37.320 39.000  1.000  1.004  0.000  0.045\n\n\nlavaan::residuals(worlandSRa, type = \"standardized.mplus\") |&gt; print()\n#&gt; $type\n#&gt; [1] \"standardized.mplus\"\n#&gt; \n#&gt; $cov\n#&gt;         verbal visual memory   read   math  spell motive harmny stable\n#&gt; verbal   0.002                                                        \n#&gt; visual   0.209  0.002                                                 \n#&gt; memory   0.204 -0.221  0.002                                          \n#&gt; read     0.671  0.115 -0.298  0.004                                   \n#&gt; math     0.562  0.043 -0.384  0.596  0.002                            \n#&gt; spell   -0.902 -0.252 -0.709 -0.670 -0.201  0.000                     \n#&gt; motive  -0.777 -0.158  0.304  0.030 -0.026  1.506  0.000              \n#&gt; harmony  0.132 -0.456  0.165  0.962 -1.151  1.208 -0.997  0.000       \n#&gt; stable   0.307  0.102  0.554 -1.797 -0.427  0.959  0.421  0.426  0.000\n#&gt; parent   0.229  0.003  0.057  0.544  0.602  0.308  0.446  0.118  0.158\n#&gt; ses      2.082  0.015 -1.204  0.009  0.133  0.043  0.362 -0.024 -0.143\n#&gt;         parent    ses\n#&gt; verbal               \n#&gt; visual               \n#&gt; memory               \n#&gt; read                 \n#&gt; math                 \n#&gt; spell                \n#&gt; motive               \n#&gt; harmony              \n#&gt; stable               \n#&gt; parent      NA       \n#&gt; ses      4.056  0.003\n\n\nlavaan::residuals(worlandSRa, type = \"cor.bollen\") |&gt; print()\n#&gt; $type\n#&gt; [1] \"cor.bollen\"\n#&gt; \n#&gt; $cov\n#&gt;         verbal visual memory   read   math  spell motive harmny stable\n#&gt; verbal   0.000                                                        \n#&gt; visual   0.002  0.000                                                 \n#&gt; memory   0.003 -0.004  0.000                                          \n#&gt; read     0.018  0.003 -0.010  0.000                                   \n#&gt; math     0.019  0.002 -0.016  0.007  0.000                            \n#&gt; spell   -0.038 -0.012 -0.036 -0.010 -0.006  0.000                     \n#&gt; motive  -0.029 -0.007  0.015  0.001 -0.001  0.076  0.000              \n#&gt; harmony  0.007 -0.026  0.010  0.042 -0.052  0.072 -0.026  0.000       \n#&gt; stable   0.012  0.004  0.027 -0.033 -0.013  0.046  0.005  0.012  0.000\n#&gt; parent   0.007  0.000  0.003  0.021  0.028  0.018  0.023  0.008  0.008\n#&gt; ses      0.059  0.001 -0.058  0.000  0.006  0.003  0.018 -0.002 -0.007\n#&gt;         parent    ses\n#&gt; verbal               \n#&gt; visual               \n#&gt; memory               \n#&gt; read                 \n#&gt; math                 \n#&gt; spell                \n#&gt; motive               \n#&gt; harmony              \n#&gt; stable               \n#&gt; parent   0.000       \n#&gt; ses      0.312  0.000\n\nSebbene gli indici di fit siano buoni, l’adattamento locale del modello con cinque percorsi tra i fattori è scarso. Ad esempio, i residui standardizzati per diverse coppie di indicatori dei fattori di rendimento e adattamento hanno spesso un valore maggiore di 2:\n\nLettura, Motivazione, 3.466\nOrtografia, Motivazione, 3.348\nLettura, Armonia, 2.903\n\nBasandosi su tutti questi risultati relativi all’adattamento globale e locale, il modello SR iniziale nella Figura 56.3 con cinque percorsi tra i fattori è rifiutato.\nEsaminiamo i modification indices.\n\nmodificationIndices(worlandSRa, sort = TRUE, minimum.value = 5)\n#&gt;         lhs op     rhs mi   epc sepc.lv sepc.all sepc.nox\n#&gt; 125 Achieve  ~  Adjust 20 115.2    63.7   63.746   63.746\n#&gt; 120  parent ~~     ses 20  32.6    32.6    0.361    0.361\n#&gt; 126  Adjust  ~ Achieve 20  26.1    47.2   47.200   47.200\n\nI risultati dei modification indices mostrano che l’assenza di un percorso tra i fattori di rendimento e adattamento nella Figura 56.3 è chiaramente incoerente con i dati. Per aggiungere una covariazione tra i fattori di rendimento e adattamento abbiamo due opzioni: o aggiungere un effetto diretto tra i fattori o permettere alle loro perturbazioni di covariare. Ma sarebbe difficile giustificare un effetto diretto rispetto all’altro: scarse abilità scolastiche potrebbero peggiorare l’adattamento in classe tanto quanto i problemi comportamentali a scuola potrebbero influire negativamente sul rendimento. La specificazione di una causalità reciproca tra Rendimento e Adattamento renderebbe il modello strutturale non ricorsivo, ma il modello non sarebbe identificato senza imporre vincoli irrealistici. Riformuliamo dunque il modello della fig-kline-15-3 permettendo alle perturbazioni tra i fattori di rendimento e adattamento di covariare.\n\n# step 2b\n# 4-factor SR model with 6 paths among factors\n# this model is equivalent to the basic 4-factor\n# CFA measurement model analyzed in step 1\n\nworlandSRb_model &lt;- \"\n    # measurement part\n    Cognitive =~ verbal + visual + memory\n    Achieve =~ read + math + spell\n    Adjust =~ motive + harmony + stable\n    Risk =~ parent + ses\n    # structural part (6 paths)\n    Achieve ~ Cognitive + Risk\n    Adjust ~ Cognitive + Risk\n    Adjust ~~ Achieve \n\"\n\n\nworlandSRb &lt;- lavaan::sem(worlandSRb_model,\n    sample.cov = worland.cov,\n    sample.nobs = 158\n)\n\n\nsemPlot::semPaths(worlandSRb,\n    what = \"col\", whatLabels = \"par\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\n\nparameterEstimates(worlandSRb) |&gt; print()\n#&gt;          lhs op       rhs     est     se      z pvalue ci.lower ci.upper\n#&gt; 1  Cognitive =~    verbal   1.000  0.000     NA     NA    1.000    1.000\n#&gt; 2  Cognitive =~    visual   1.000  0.090 11.144  0.000    0.824    1.175\n#&gt; 3  Cognitive =~    memory   0.788  0.077 10.217  0.000    0.637    0.940\n#&gt; 4    Achieve =~      read   1.000  0.000     NA     NA    1.000    1.000\n#&gt; 5    Achieve =~      math   0.925  0.083 11.100  0.000    0.761    1.088\n#&gt; 6    Achieve =~     spell   0.678  0.080  8.480  0.000    0.521    0.835\n#&gt; 7     Adjust =~    motive   1.000  0.000     NA     NA    1.000    1.000\n#&gt; 8     Adjust =~   harmony   0.861  0.136  6.311  0.000    0.593    1.128\n#&gt; 9     Adjust =~    stable   0.940  0.114  8.231  0.000    0.716    1.164\n#&gt; 10      Risk =~    parent   1.000  0.000     NA     NA    1.000    1.000\n#&gt; 11      Risk =~       ses   0.773  0.224  3.445  0.001    0.333    1.212\n#&gt; 12   Achieve  ~ Cognitive   0.719  0.109  6.574  0.000    0.504    0.933\n#&gt; 13   Achieve  ~      Risk  -0.175  0.190 -0.922  0.357   -0.548    0.198\n#&gt; 14    Adjust  ~ Cognitive   0.261  0.070  3.743  0.000    0.124    0.398\n#&gt; 15    Adjust  ~      Risk  -0.146  0.127 -1.153  0.249   -0.395    0.102\n#&gt; 16   Achieve ~~    Adjust  36.374  7.967  4.565  0.000   20.758   51.989\n#&gt; 17    verbal ~~    verbal  46.253  9.782  4.728  0.000   27.080   65.426\n#&gt; 18    visual ~~    visual  76.171 12.153  6.268  0.000   52.351   99.991\n#&gt; 19    memory ~~    memory  69.732  9.717  7.177  0.000   50.688   88.776\n#&gt; 20      read ~~      read  51.273 11.238  4.562  0.000   29.246   73.299\n#&gt; 21      math ~~      math  86.341 13.049  6.616  0.000   60.764  111.917\n#&gt; 22     spell ~~     spell 112.797 14.078  8.012  0.000   85.205  140.389\n#&gt; 23    motive ~~    motive  37.737  6.463  5.839  0.000   25.071   50.404\n#&gt; 24   harmony ~~   harmony  83.953 10.590  7.927  0.000   63.197  104.709\n#&gt; 25    stable ~~    stable  29.306  5.386  5.441  0.000   18.749   39.863\n#&gt; 26    parent ~~    parent  88.005 18.343  4.798  0.000   52.053  123.958\n#&gt; 27       ses ~~       ses  38.896 10.208  3.810  0.000   18.889   58.904\n#&gt; 28 Cognitive ~~ Cognitive 141.617 22.098  6.409  0.000   98.307  184.928\n#&gt; 29   Achieve ~~   Achieve  84.317 16.324  5.165  0.000   52.322  116.312\n#&gt; 30    Adjust ~~    Adjust  38.008  8.188  4.642  0.000   21.960   54.055\n#&gt; 31      Risk ~~      Risk  55.079 19.989  2.755  0.006   15.902   94.257\n#&gt; 32 Cognitive ~~      Risk -40.517 12.448 -3.255  0.001  -64.914  -16.119\n\n\nfitMeasures(worlandSRb, c(\"chisq\", \"df\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt; print()\n#&gt;  chisq     df    cfi    tli  rmsea   srmr \n#&gt; 16.212 38.000  1.000  1.049  0.000  0.023\n\n\n# predicted correlation matrix for factors\nlavaan::lavInspect(worlandSRb, \"cor.lv\") |&gt; print()\n#&gt;           Cogntv Achiev Adjust   Risk\n#&gt; Cognitive  1.000                     \n#&gt; Achieve    0.703  1.000              \n#&gt; Adjust     0.500  0.751  1.000       \n#&gt; Risk      -0.459 -0.401 -0.349  1.000\n\n\nlavaan::residuals(worlandSRb, type = \"standardized.mplus\") |&gt; print()\n#&gt; $type\n#&gt; [1] \"standardized.mplus\"\n#&gt; \n#&gt; $cov\n#&gt;         verbal visual memory   read   math  spell motive harmny stable\n#&gt; verbal      NA                                                        \n#&gt; visual  -0.002     NA                                                 \n#&gt; memory   0.122 -0.131     NA                                          \n#&gt; read     0.597  0.113 -0.285     NA                                   \n#&gt; math     0.504  0.038 -0.377  0.597     NA                            \n#&gt; spell   -0.952 -0.255 -0.704 -0.667 -0.206     NA                     \n#&gt; motive  -0.821 -0.157  0.309 -0.042 -0.078  1.484     NA              \n#&gt; harmony  0.117 -0.453  0.171  0.942 -1.179  1.196 -1.010     NA       \n#&gt; stable   0.284  0.112  0.566 -1.893 -0.467  0.942  0.422  0.436     NA\n#&gt; parent  -0.093 -0.261 -0.157  0.234  0.378  0.157  0.300  0.019 -0.022\n#&gt; ses      1.891 -0.247 -1.417 -0.370 -0.114 -0.112  0.213 -0.125 -0.362\n#&gt;         parent    ses\n#&gt; verbal               \n#&gt; visual               \n#&gt; memory               \n#&gt; read                 \n#&gt; math                 \n#&gt; spell                \n#&gt; motive               \n#&gt; harmony              \n#&gt; stable               \n#&gt; parent   0.034       \n#&gt; ses      0.043  0.017\n\n\nlavaan::residuals(worlandSRb, type = \"cor.bollen\") |&gt; print()\n#&gt; $type\n#&gt; [1] \"cor.bollen\"\n#&gt; \n#&gt; $cov\n#&gt;         verbal visual memory   read   math  spell motive harmny stable\n#&gt; verbal   0.000                                                        \n#&gt; visual   0.000  0.000                                                 \n#&gt; memory   0.001 -0.002  0.000                                          \n#&gt; read     0.015  0.003 -0.010  0.000                                   \n#&gt; math     0.017  0.001 -0.016  0.007  0.000                            \n#&gt; spell   -0.040 -0.012 -0.036 -0.010 -0.006  0.000                     \n#&gt; motive  -0.031 -0.007  0.016 -0.001 -0.003  0.075  0.000              \n#&gt; harmony  0.006 -0.026  0.010  0.041 -0.053  0.071 -0.027  0.000       \n#&gt; stable   0.011  0.005  0.028 -0.034 -0.015  0.045  0.005  0.012  0.000\n#&gt; parent  -0.003 -0.011 -0.007  0.008  0.017  0.009  0.015  0.001 -0.001\n#&gt; ses      0.050 -0.010 -0.068 -0.012 -0.005 -0.006  0.010 -0.008 -0.016\n#&gt;         parent    ses\n#&gt; verbal               \n#&gt; visual               \n#&gt; memory               \n#&gt; read                 \n#&gt; math                 \n#&gt; spell                \n#&gt; motive               \n#&gt; harmony              \n#&gt; stable               \n#&gt; parent   0.000       \n#&gt; ses      0.000  0.000\n\nConfrontiamo i due modelli con il test del rapporto di verosimiglianza.\n\nlavTestLRT(worlandSRa, worlandSRb)\n#&gt; \n#&gt; Chi-Squared Difference Test\n#&gt; \n#&gt;            Df   AIC   BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\n#&gt; worlandSRb 38 12938 13024  16.2                                    \n#&gt; worlandSRa 39 12957 13040  37.3       21.1 0.357       1    4.3e-06\n\nL’adattamento del modello SR con 5 percorsi tra i fattori è significativamente peggiore rispetto a quello del modello CFA con 6 percorsi. Gli indici di fit del modello con 6 percorsi sono buoni così come il suo adattamento locale.\n\n# standardized estimates with standard errors\nlavaan::standardizedSolution(worlandSRb) |&gt; print()\n#&gt;          lhs op       rhs est.std    se      z pvalue ci.lower ci.upper\n#&gt; 1  Cognitive =~    verbal   0.868 0.032 27.075  0.000    0.805    0.931\n#&gt; 2  Cognitive =~    visual   0.806 0.037 21.720  0.000    0.733    0.879\n#&gt; 3  Cognitive =~    memory   0.747 0.043 17.470  0.000    0.663    0.831\n#&gt; 4    Achieve =~      read   0.876 0.031 28.212  0.000    0.815    0.937\n#&gt; 5    Achieve =~      math   0.791 0.038 20.777  0.000    0.717    0.866\n#&gt; 6    Achieve =~     spell   0.639 0.053 11.984  0.000    0.534    0.743\n#&gt; 7     Adjust =~    motive   0.761 0.049 15.552  0.000    0.665    0.857\n#&gt; 8     Adjust =~   harmony   0.561 0.065  8.657  0.000    0.434    0.688\n#&gt; 9     Adjust =~    stable   0.781 0.048 16.381  0.000    0.688    0.875\n#&gt; 10      Risk =~    parent   0.620 0.100  6.217  0.000    0.425    0.816\n#&gt; 11      Risk =~       ses   0.677 0.104  6.495  0.000    0.473    0.881\n#&gt; 12   Achieve  ~ Cognitive   0.657 0.079  8.357  0.000    0.503    0.811\n#&gt; 13   Achieve  ~      Risk  -0.100 0.107 -0.935  0.350   -0.310    0.110\n#&gt; 14    Adjust  ~ Cognitive   0.431 0.103  4.179  0.000    0.229    0.633\n#&gt; 15    Adjust  ~      Risk  -0.151 0.127 -1.186  0.236   -0.400    0.098\n#&gt; 16   Achieve ~~    Adjust   0.643 0.085  7.592  0.000    0.477    0.808\n#&gt; 17    verbal ~~    verbal   0.246 0.056  4.421  0.000    0.137    0.355\n#&gt; 18    visual ~~    visual   0.350 0.060  5.847  0.000    0.233    0.467\n#&gt; 19    memory ~~    memory   0.442 0.064  6.920  0.000    0.317    0.567\n#&gt; 20      read ~~      read   0.232 0.054  4.271  0.000    0.126    0.339\n#&gt; 21      math ~~      math   0.374 0.060  6.197  0.000    0.255    0.492\n#&gt; 22     spell ~~     spell   0.592 0.068  8.686  0.000    0.458    0.725\n#&gt; 23    motive ~~    motive   0.421 0.074  5.649  0.000    0.275    0.567\n#&gt; 24   harmony ~~   harmony   0.686 0.073  9.445  0.000    0.543    0.828\n#&gt; 25    stable ~~    stable   0.390 0.075  5.229  0.000    0.244    0.536\n#&gt; 26    parent ~~    parent   0.615 0.124  4.967  0.000    0.372    0.858\n#&gt; 27       ses ~~       ses   0.542 0.141  3.840  0.000    0.265    0.818\n#&gt; 28 Cognitive ~~ Cognitive   1.000 0.000     NA     NA    1.000    1.000\n#&gt; 29   Achieve ~~   Achieve   0.498 0.077  6.470  0.000    0.347    0.649\n#&gt; 30    Adjust ~~    Adjust   0.732 0.080  9.108  0.000    0.574    0.889\n#&gt; 31      Risk ~~      Risk   1.000 0.000     NA     NA    1.000    1.000\n#&gt; 32 Cognitive ~~      Risk  -0.459 0.098 -4.686  0.000   -0.651   -0.267\n\nLa correlazione stimata tra i fattori esogeni abilità cognitiva e rischio, –.459, è sensata: è negativa (più alto il rischio, minore l’abilità cognitiva). Questa correlazione non è prossima a -1.0, il che suggerisce che i due fattori sono distinti e non quasi identici, confermando così l’ipotesi di validità discriminante.\nAnalizzando gli impatti specifici, un incremento di un punto nel fattore cognitivo (misurato come varianza comune del ragionamento verbale) prevede un aumento di .719 punti nel rendimento scolastico (misurato come varianza comune dell’abilità di lettura), tenendo conto del fattore di rischio. In termini standardizzati, un aumento di una deviazione standard nell’abilità cognitiva si traduce in un aumento di .657 deviazioni standard nel rendimento scolastico, sempre controllando per il rischio.\nL’influenza del rischio sul rendimento scolastico è meno marcata: un incremento di un punto nel rischio (misurato come varianza comune del disturbo genitoriale) prevede una diminuzione di .175 punti nel rendimento scolastico. Standardizzando, un aumento di una deviazione standard nel rischio si associa a una diminuzione di .100 deviazioni standard nel rendimento, controllando per l’abilità cognitiva.\nLa correlazione di perturbazione di .643 misura la relazione tra il rendimento scolastico e l’adattamento in classe, dopo aver escluso l’influenza di altri fattori noti, in questo caso l’abilità cognitiva e il rischio di psicopatologia. In termini più semplici, la correlazione di perturbazione ci dice quanto sono correlati il rendimento scolastico e l’adattamento in classe quando si tiene conto (o si “controlla”) dell’effetto dell’abilità cognitiva e del rischio. Un valore di .643 indica una correlazione moderatamente forte, suggerendo che quando il rendimento scolastico di uno studente migliora (o peggiora), anche il suo adattamento in classe tende a migliorare (o peggiorare) in modo simile, indipendentemente dal suo livello di abilità cognitiva o dal grado di rischio di psicopatologia.\nLa presenza di questa correlazione parziale sostanziale implica che ci sono fattori non misurati nel modello che influenzano sia il rendimento scolastico sia l’adattamento in classe. Questi fattori non misurati potrebbero includere variabili come il sostegno familiare, la qualità dell’insegnamento, fattori ambientali o personalità dello studente. Importante è che questi fattori non misurati sono distinti sia dall’abilità cognitiva dello studente sia dal suo rischio di psicopatologia. In conclusione, il valore di .643 non solo mette in luce l’interdipendenza tra rendimento scolastico e adattamento in classe, ma suggerisce anche l’esistenza di altre variabili influenti che non sono state direttamente misurate o incluse nel modello. Questa informazione può essere preziosa per indirizzare ulteriori ricerche o interventi educativi.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Modelli di Regressione Strutturale</span>"
    ]
  },
  {
    "objectID": "chapters/sem/09_structural_regr.html#strategie-avanzate-di-modellazione-per-i-modelli-di-regressione-strutturale",
    "href": "chapters/sem/09_structural_regr.html#strategie-avanzate-di-modellazione-per-i-modelli-di-regressione-strutturale",
    "title": "56  Modelli di Regressione Strutturale",
    "section": "\n56.5 Strategie Avanzate di Modellazione per i Modelli di Regressione Strutturale",
    "text": "56.5 Strategie Avanzate di Modellazione per i Modelli di Regressione Strutturale\nOltre all’approccio tradizionale di modellazione in due fasi, esiste un metodo più complesso a quattro fasi per analizzare i modelli SR completi. Questa strategia, introdotta da Mulaik e Millsap nel 2000, amplia la modellazione bifase aggiungendo ulteriori analisi esplorative che possono portare a conclusioni più definitive in una serie più estesa di studi. Questo metodo prevede che ogni fattore comune abbia almeno quattro indicatori, numero ritenuto sufficiente per testare l’unidimensionalità con il test dell’annullamento della tetrade. I quattro indicatori rappresentano anche il numero minimo perché un modello CFA a singolo fattore sia considerato sovraidentificato. Il ricercatore testa quindi una serie di almeno quattro modelli gerarchicamente correlati, seguendo questi passaggi:\n\nPrimo Passaggio: Il modello iniziale meno restrittivo è un modello EFA, dove ogni indicatore satura su tutti i fattori. Il numero di fattori è lo stesso dei modelli analizzati nei passaggi successivi. Questo modello viene analizzato con lo stesso metodo di stima utilizzato nei passaggi successivi, ad esempio il metodo ML per indicatori continui e normalmente distribuiti. Alternativamente, si possono usare tecniche come ESEM o E/CFA al posto dell’EFA. Questo passaggio serve a testare la correttezza provvisoria delle ipotesi riguardo al numero di fattori.\nSecondo Passaggio: Corrisponde al Primo Passaggio della modellazione bifase. Qui, si specifica un modello CFA con alcuni carichi incrociati fissati a zero, identificando gli indicatori che non dipendono da certi fattori comuni. Se l’adattamento del modello CFA è ragionevole, si può procedere al test del modello SR; in caso contrario, il modello di misurazione va rivisto.\nTerzo Passaggio: Si specifica il modello SR target con lo stesso schema di carichi incrociati fissati a zero del modello CFA del Secondo Passaggio. Tipicamente, la parte strutturale del modello SR include meno effetti diretti rispetto al totale delle covarianze tra fattori nel modello CFA. Se la parte strutturale del modello SR ha tanti percorsi quanti il modello CFA, i due modelli saranno equivalenti e questo passaggio può essere omesso.\nQuarto Passaggio: Coinvolge test su ipotesi specifiche sui parametri definiti dall’inizio del processo. Questi test possono comportare l’applicazione di vincoli zero o altri, aumentando di uno dfM. I Passaggi 3 e 4 della modellazione a quattro fasi rappresentano una precisazione delle attività generali del Secondo Passaggio della modellazione bifase.\n\nUna delle critiche alla modellazione a quattro fasi riguarda la necessità di avere almeno quattro indicatori per fattore, condizione non sempre pratica o desiderabile, specialmente quando pochi indicatori, o anche un singolo indicatore ottimale, presentano migliori caratteristiche psicometriche rispetto a quattro. Tuttavia, Mulaik e Millsap hanno osservato che avere almeno quattro indicatori può compensare, in parte, le limitazioni di un campione più piccolo incrementando dfM.\nEntrambi gli approcci, bifase e quattro fasi, sfruttano la variazione casuale quando i modelli vengono testati e riformulati utilizzando gli stessi dati, e sono considerati migliori della modellazione monofase, dove non esiste una distinzione tra questioni di misurazione e struttura.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Modelli di Regressione Strutturale</span>"
    ]
  },
  {
    "objectID": "chapters/sem/09_structural_regr.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/sem/09_structural_regr.html#informazioni-sullambiente-di-sviluppo",
    "title": "56  Modelli di Regressione Strutturale",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] lme4_1.1-36       Matrix_1.7-2      ggokabeito_0.1.0  see_0.10.0       \n#&gt;  [5] MASS_7.3-65       viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n#&gt;  [9] ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1 \n#&gt; [13] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-19     psych_2.4.12     \n#&gt; [17] scales_1.3.0      markdown_1.13     knitr_1.49        lubridate_1.9.4  \n#&gt; [21] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.4      \n#&gt; [25] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1    \n#&gt; [29] tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.0      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [22] mime_0.12           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [25] R6_2.6.1            fastmap_1.2.0       rbibutils_2.3      \n#&gt;  [28] shiny_1.10.0        digest_0.6.37       OpenMx_2.21.13     \n#&gt;  [31] fdrtool_1.2.18      colorspace_2.1-1    rprojroot_2.0.4    \n#&gt;  [34] Hmisc_5.2-2         timechange_0.3.0    abind_1.4-8        \n#&gt;  [37] compiler_4.4.2      withr_3.0.2         glasso_1.11        \n#&gt;  [40] htmlTable_2.4.3     backports_1.5.0     carData_3.0-5      \n#&gt;  [43] ggsignif_0.6.4      corpcor_1.6.10      gtools_3.9.5       \n#&gt;  [46] tools_4.4.2         pbivnorm_0.6.0      foreign_0.8-88     \n#&gt;  [49] zip_2.3.2           httpuv_1.6.15       nnet_7.3-20        \n#&gt;  [52] glue_1.8.0          quadprog_1.5-8      nlme_3.1-167       \n#&gt;  [55] promises_1.3.2      lisrelToR_0.3       grid_4.4.2         \n#&gt;  [58] checkmate_2.3.2     cluster_2.1.8       reshape2_1.4.4     \n#&gt;  [61] generics_0.1.3      gtable_0.3.6        tzdb_0.4.0         \n#&gt;  [64] data.table_1.17.0   hms_1.1.3           car_3.1-3          \n#&gt;  [67] sem_3.1-16          pillar_1.10.1       rockchalk_1.8.157  \n#&gt;  [70] later_1.4.1         splines_4.4.2       lattice_0.22-6     \n#&gt;  [73] survival_3.8-3      kutils_1.73         tidyselect_1.2.1   \n#&gt;  [76] miniUI_0.1.1.1      pbapply_1.7-2       reformulas_0.4.0   \n#&gt;  [79] stats4_4.4.2        xfun_0.51           qgraph_1.9.8       \n#&gt;  [82] arm_1.14-4          stringi_1.8.4       yaml_2.3.10        \n#&gt;  [85] pacman_0.5.1        boot_1.3-31         evaluate_1.0.3     \n#&gt;  [88] codetools_0.2-20    mi_1.1              cli_3.6.4          \n#&gt;  [91] RcppParallel_5.1.10 rpart_4.1.24        xtable_1.8-4       \n#&gt;  [94] Rdpack_2.6.2        munsell_0.5.1       Rcpp_1.0.14        \n#&gt;  [97] coda_0.19-4.1       png_0.1-8           XML_3.99-0.18      \n#&gt; [100] parallel_4.4.2      jpeg_0.1-10         mvtnorm_1.3-3      \n#&gt; [103] openxlsx_4.2.8      rlang_1.1.5         multcomp_1.4-28    \n#&gt; [106] mnormt_2.1.1\n\n\n\n\n\nKline, R. B. (2023). Principles and practice of structural equation modeling. Guilford publications.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Modelli di Regressione Strutturale</span>"
    ]
  },
  {
    "objectID": "chapters/sem/10_missing_data.html",
    "href": "chapters/sem/10_missing_data.html",
    "title": "57  Dati mancanti",
    "section": "",
    "text": "57.1 Introduzione\nRaramente un ricercatore si trova nella situazione fortunata nella quale un’analisi statistica (di tipo CFA/SEM o altro) può essere condotta utilizzando un set di dati in cui tutte le variabili sono state osservate su tutte le unità statistiche: nella pratica ricerca i dati mancanti sono la norma piuttosto che l’eccezione.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Dati mancanti</span>"
    ]
  },
  {
    "objectID": "chapters/sem/10_missing_data.html#tipologie-di-dati-mancanti",
    "href": "chapters/sem/10_missing_data.html#tipologie-di-dati-mancanti",
    "title": "57  Dati mancanti",
    "section": "\n57.2 Tipologie di dati mancanti",
    "text": "57.2 Tipologie di dati mancanti\nCi sono molti motivi che possono stare alla base dei dati mancanti. Ad esempio, i dati possono mancare per disegno dello studio (“mancanza pianificata”), come ad esempio nei progetti di ricerca in cui i partecipanti al campione vengono selezionati casualmente per completare sottoinsiemi diversi della batteria di valutazione (una scelta di questo tipo viene motivata, ad esempio, a causa di considerazioni pratiche come i vincoli di tempo). In tali condizioni, si presume che i dati mancanti si distribuiscano in un modo completamente casuale rispetto a tutte le altre variabili nello studio.\nIn generale, i meccanismi che determinano la presenza di dati mancanti possono essere classificati in tre categorie:\n\n\nvalori mancanti completamente casuali (Missing Completely At Random, MCAR). La probabilità di dati mancanti su una variabile non è collegata né al valore mancante sulla variabile, né al valore di ogni altra variabile presente nella matrice dati che si sta analizzando;\n\nvalori mancanti casuali (Missing At Random, MAR). I valori mancanti sono indipendenti dal valore che viene a mancare, ma dipendono da altre variabili, cioè i dati sulla variabile sono mancanti per categorie di partecipanti che potrebbero essere identificati dai valori assunti dalle altre variabili presenti nello studio;\n\nvalori mancanti non ignorabili (Missing Not At Random, MNAR). La mancanza di un dato può dipendere sia dal valore del dato stesso che dalle altre variabili. Per esempio, se si studia la salute mentale e le persone depresse riferiscono meno volentieri informazioni riguardanti il loro stato di salute, allora i dati non sono mancanti per caso.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Dati mancanti</span>"
    ]
  },
  {
    "objectID": "chapters/sem/10_missing_data.html#la-gestione-dei-dati-mancanti",
    "href": "chapters/sem/10_missing_data.html#la-gestione-dei-dati-mancanti",
    "title": "57  Dati mancanti",
    "section": "\n57.3 La gestione dei dati mancanti",
    "text": "57.3 La gestione dei dati mancanti\nIl passo successivo dopo la definizione dei meccanismi è quello della gestione dei dati mancanti. Sostanzialmente le scelte possibili sono due: l’eliminazione dei casi o la sostituzione dei dati mancanti. Un metodo semplice, indicato solo nel caso in cui l’ammontare dei dati mancanti è limitato e questi sono mancanti completamente a caso (MCAR), è quello di rimuovere i casi con dati mancanti (case deletion).\nCi sono due metodi per eliminare le osservazioni con valori mancanti: listwise deletion e pairwise deletion. Nel primo caso si elimina dal campione ogni osservazione che contiene dati mancanti. Le analisi avverranno quindi solo sui casi che hanno valori validi su tutte le variabili in esame. In questo modo si ottiene una maggiore semplicità di trattazione nell’analisi statistica, tuttavia non si utilizza tutta l’informazione osservata (si riduce la numerosità campionaria e, quindi, l’informazione). Il secondo metodo è la pairwise deletion, che utilizza tutti i casi che hanno i dati validi su due variabili volta per volta. In questo modo si riesce a massimizzare la numerosità del campione da utilizzare, ma si tratta comunque di un metodo che presenta dei problemi, per esempio il fatto che con questo approccio i parametri del modello saranno basati su differenti insiemi di dati, con differenti numerosità campionarie e differenti errori standard.\nQuando i dati non sono MNAR è opportuno sostituirli con appropriate funzioni dei dati effettivamente osservati. Questa procedura è chiamata imputazione (imputation). Di seguito sono indicati alcuni metodi.\n\n\nMean Imputation. Il dato mancante viene sostituito con la media della variabile. Questo metodo, utilizzato troppo spesso per la sua semplicità, riducendo la variabilità dei dati, ha effetti importanti su molte analisi dei dati e, in generale, dovrebbe essere evitato.\n\nRegression Imputation. Si tratta di un approccio basato sulle informazioni disponibili sulle altre variabili. Si stima una equazione di regressione lineare per ogni variabile utilizzando le altre variabili come predittori. Questo metodo offre il vantaggio di poter utilizzare i rapporti esistenti tra le variabili per effettuare le valutazioni dei dati mancanti; tuttavia esso è usato raramente, in quanto amplifica le correlazioni tra le variabili; quindi, se le analisi si basano su regressioni o modelli SEM, questo metodo è sconsigliato.\n\nMultiple Imputation. La tecnica di multiple imputation, applicabile in caso di MAR, prevede che un dato mancante su una variabile sia sostituito, sulla base dei dati esistenti sulle altre variabili, con un valore che però comprende anche una componente di errore ricavata dalla distribuzione dei residui della variabile.\n\nExpectation-Maximization. Un altro approccio moderno del trattamento dei dati mancanti è l’applicazione dell’algoritmo Expectation Maximization (EM). La tecnica è quella di stimare i parametri sulla base dei dati osservati, e di stimare poi i dati mancanti sulla base di questi parametri (fase E). Poi i parametri vengono nuovamente stimati sulla base della nuova matrice di dati (fase M), e così via. Questo processo viene iterato fino a quando i valori stimati convergono. Tuttavia, una limitazione fondamentale dell’utilizzo dell’algoritmo EM per calcolare le matrici di input per le analisi CFA/SEM è che gli errori standard risultanti delle stime dei parametri non sono consistenti. Pertanto, gli intervalli di confidenza e i test di significatività possono risultare compromessi.\n\n\n57.3.1 Metodo Direct ML\nBenché i metodi precedenti vengano spesso usati, nella pratica concreta è preferibile usare il metodo Direct ML, conosciuto anche come “raw ML” o “full information ML” (FIML), in quanto è generalmente considerano come il metodo migliore per gestire i dati mancanti nella maggior parte delle applicazioni CFA e SEM. Il metodo full information ML è esente dai problemi associati all’utilizzo dell’algoritmo EM e produce stime consistenti sotto l’ipotesi di normalità multivariata per dati mancanti MAR.\nIntuitivamente, l’approccio utilizza la relazione tra le variabili per dedurre quali siano i valori mancanti con maggiore probabilità. Ad esempio, se due variabili, \\(X\\) e \\(Y\\), sono correlate positivamente, allora se, per alcune osservazioni \\(i\\), \\(X_i\\) è il valore più alto nella variabile, è probabile che anche il valore mancante \\(Y_i\\) sia un valore alto. FIML utilizza queste informazioni senza procedere all’imputazione dei valori mancanti, ma invece basandosi sulle stime più verosimili dei parametri della popolazione, ovvero massimizzando direttamente la verosimiglianza del modello specificato. Sotto l’assunzione di normalità multivariata, la funzione di verosimiglianza diventa\n\\[\nL(\\mu, \\Sigma) = \\prod_i f(y_i \\mid \\mu_i, \\Sigma_i),\n\\]\ndove \\(y_i\\) sono i dati, \\(\\mu_i\\) e \\(\\Sigma_i\\) sono i parametri della popolazione se gli elementi mancanti in \\(y_i\\) vengono rimossi. Si cercano i valori \\(\\mu\\) e \\(\\Sigma\\) che massimizzano la verosimiglianza.\nIn lavaan l’applicazione di tale metodo si ottiene specificando l’argomento missing = \"ml\".\n\n57.3.2 Un esempio concreto\nPer applicare il metodo direct ML, Brown (2015) prende in esame i dati reali di un questionario (un singolo fattore, quattro item, una covarianza di errore) caratterizzato dalla presenza di dati mancanti. Importiamo i dati in R:\n\nd &lt;- rio::import(here::here(\"data\", \"brown_table_9_1.csv\"))\nhead(d)\n#&gt;   subject s1 s2 s3 s4\n#&gt; 1    5760  2  0  1 NA\n#&gt; 2    5761  3  3  3 NA\n#&gt; 3    5763  2  4  4 NA\n#&gt; 4    5761  2  0  0 NA\n#&gt; 5    5769  2  1  1 NA\n#&gt; 6    5771  4  3  3 NA\n\nAbbiamo 650 osservazioni:\n\ndim(d)\n#&gt; [1] 650   5\n\nLe frequenze di dati mancanti vengono ottentute mediante la funzione summary()\n\nsummary(d)\n#&gt;     subject           s1             s2             s3             s4     \n#&gt;  Min.   :5756   Min.   :0.00   Min.   :0.00   Min.   :0.00   Min.   :0.0  \n#&gt;  1st Qu.:5934   1st Qu.:2.00   1st Qu.:2.00   1st Qu.:1.00   1st Qu.:2.0  \n#&gt;  Median :6102   Median :3.00   Median :3.00   Median :2.00   Median :3.0  \n#&gt;  Mean   :6104   Mean   :2.93   Mean   :2.56   Mean   :2.21   Mean   :2.4  \n#&gt;  3rd Qu.:6275   3rd Qu.:4.00   3rd Qu.:4.00   3rd Qu.:4.00   3rd Qu.:3.0  \n#&gt;  Max.   :6451   Max.   :4.00   Max.   :4.00   Max.   :4.00   Max.   :4.0  \n#&gt;                 NA's   :25     NA's   :25     NA's   :25     NA's   :190\n\nIl modello viene specificato come segue Brown (2015):\n\nmodel &lt;- '\n  esteem =~ s1 + s2 + s3 + s4\n  s2 ~~ s4\n'\n\nAdattiamo il modello ai dati specificanto l’utilizzo del metodo full information ML per la gestione dei dati mancanti:\n\nfit &lt;- cfa(model, data = d, missing = \"fiml\")\n\nÈ possibile identificare le configurazioni di risposte agli item che contengono dati mancanti:\n\nfit@Data@Mp[[1]]$npatterns\n#&gt; [1] 5\n\n\npats &lt;- fit@Data@Mp[[1]]$pat * 1L\ncolnames(pats) &lt;- fit@Data@ov.names[[1]]\nprint(pats)\n#&gt;      s1 s2 s3 s4\n#&gt; [1,]  1  1  1  1\n#&gt; [2,]  1  1  1  0\n#&gt; [3,]  0  1  1  1\n#&gt; [4,]  1  0  1  1\n#&gt; [5,]  1  1  0  1\n\nPossiamo esaminare la proporzione di dati disponibili per ciascun indicatore e per ciascuna coppia di indicatori:\n\ncoverage &lt;- fit@Data@Mp[[1]]$coverage\ncolnames(coverage) &lt;- rownames(coverage) &lt;- fit@Data@ov.names[[1]]\nprint(coverage)\n#&gt;       s1    s2    s3    s4\n#&gt; s1 0.962 0.923 0.923 0.669\n#&gt; s2 0.923 0.962 0.923 0.669\n#&gt; s3 0.923 0.923 0.962 0.669\n#&gt; s4 0.669 0.669 0.669 0.708\n\nAd esempio, consideriamo l’item s1; se moltiplichiamo la copertura di questo elemento per la numerosità campionaria possiamo concludere che questa variabile contiene 25 osservazioni mancanti; e così via.\n\n650 * 0.9615385\n#&gt; [1] 625\n\nProcediamo poi come sempre per esaminare la soluzione ottenuta.\n\neffectsize::interpret(fit)\n#&gt;     Name   Value Threshold Interpretation\n#&gt; 1    GFI 0.99945      0.95   satisfactory\n#&gt; 2   AGFI 0.99229      0.90   satisfactory\n#&gt; 3    NFI 0.99919      0.90   satisfactory\n#&gt; 4   NNFI 0.99898      0.90   satisfactory\n#&gt; 5    CFI 0.99983      0.90   satisfactory\n#&gt; 6  RMSEA 0.02024      0.05   satisfactory\n#&gt; 7   SRMR 0.00485      0.08   satisfactory\n#&gt; 8    RFI 0.99516      0.90   satisfactory\n#&gt; 9   PNFI 0.16653      0.50           poor\n#&gt; 10   IFI 0.99983      0.90   satisfactory\n\n\nstandardizedSolution(fit)\n#&gt;       lhs op    rhs est.std    se     z pvalue ci.lower ci.upper\n#&gt; 1  esteem =~     s1   0.737 0.020 37.09      0    0.698    0.776\n#&gt; 2  esteem =~     s2   0.920 0.013 68.65      0    0.894    0.947\n#&gt; 3  esteem =~     s3   0.880 0.013 66.43      0    0.854    0.906\n#&gt; 4  esteem =~     s4   0.905 0.016 55.40      0    0.873    0.937\n#&gt; 5      s2 ~~     s4  -0.886 0.216 -4.11      0   -1.309   -0.463\n#&gt; 6      s1 ~~     s1   0.456 0.029 15.55      0    0.399    0.514\n#&gt; 7      s2 ~~     s2   0.153 0.025  6.19      0    0.104    0.201\n#&gt; 8      s3 ~~     s3   0.225 0.023  9.64      0    0.179    0.271\n#&gt; 9      s4 ~~     s4   0.182 0.030  6.15      0    0.124    0.240\n#&gt; 10 esteem ~~ esteem   1.000 0.000    NA     NA    1.000    1.000\n#&gt; 11     s1 ~1          2.375 0.078 30.61      0    2.223    2.527\n#&gt; 12     s2 ~1          1.881 0.066 28.59      0    1.752    2.010\n#&gt; 13     s3 ~1          1.584 0.059 26.78      0    1.468    1.700\n#&gt; 14     s4 ~1          1.850 0.071 26.05      0    1.710    1.989\n#&gt; 15 esteem ~1          0.000 0.000    NA     NA    0.000    0.000",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Dati mancanti</span>"
    ]
  },
  {
    "objectID": "chapters/sem/10_missing_data.html#dati-mancanti-in-r",
    "href": "chapters/sem/10_missing_data.html#dati-mancanti-in-r",
    "title": "57  Dati mancanti",
    "section": "\n57.4 Dati mancanti in R",
    "text": "57.4 Dati mancanti in R\nPer completezza, aggiungiamo qualche breve accenno alla gestione dei dati mancanti in R.\nIn R, i valori mancanti vengono indicati dal codice NA, che significa not available — non disponibile.\nSe una variabile contiene valori mancanti, R non è in grado di applicare ad essa alcune funzioni, come ad esempio la media. Per questa ragione, la gran parte delle funzioni di R prevedono modi specifici per trattare i valori mancanti.\nCi sono diversi tipi di dati “mancanti” in R;\n\n\nNA - generico dato mancante;\n\nNaN - il codice NaN (Not a Number) indica i valori numerici impossibili, quali ad esempio un valore 0/0;\n\nInf e -Inf - Infinity, si verifca, ad esempio, quando si divide un numero per 0.\n\nLa funzione is.na() ritorna un output che indica con TRUE le celle che contengono NA o NaN.\nSi noti che\n\nse is.na(x) è TRUE, allora !is.na(x) è FALSE;\n\nall(!is.na(x)) ritorna TRUE se tutti i valori x sono NOT NA;\n\nany(is.na(x)) risponde alla domanda: c’è qualche valore NA (almeno uno) in x?;\n\ncomplete.cases(x) ritorna TRUE se ciascun elemento di x è is NOT NA; ritorna FALSE se almeno un elemento di x è NA;\n\nLe funzioni R is.nan() e is.infinite() si applicano ai tipi di dati NaN e Inf.\nPer esempio, consideriamo il seguente data.frame:\n\nd &lt;- tibble(\n  w = c(1, 2, NA, 3, NA), \n  x = 1:5, \n  y = 1, \n  z = x ^ 2 + y,\n  q = c(3, NA, 5, 1, 4)\n)\nd\n#&gt; # A tibble: 5 × 5\n#&gt;       w     x     y     z     q\n#&gt;   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1     1     1     2     3\n#&gt; 2     2     2     1     5    NA\n#&gt; 3    NA     3     1    10     5\n#&gt; 4     3     4     1    17     1\n#&gt; 5    NA     5     1    26     4\n\n\nis.na(d$w)\n#&gt; [1] FALSE FALSE  TRUE FALSE  TRUE\nis.na(d$x)\n#&gt; [1] FALSE FALSE FALSE FALSE FALSE\n\nPer creare un nuovo Dataframe senza valori mancanti:\n\nd_clean &lt;- d[complete.cases(d), ]\nd_clean\n#&gt; # A tibble: 2 × 5\n#&gt;       w     x     y     z     q\n#&gt;   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1     1     1     2     3\n#&gt; 2     3     4     1    17     1\n\nOppure, se vogliamo eliminare le righe con NA solo in una variabile:\n\nd1 &lt;- d[!is.na(d$q), ]\nd1\n#&gt; # A tibble: 4 × 5\n#&gt;       w     x     y     z     q\n#&gt;   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1     1     1     2     3\n#&gt; 2    NA     3     1    10     5\n#&gt; 3     3     4     1    17     1\n#&gt; 4    NA     5     1    26     4\n\nSe vogliamo esaminare le righe con i dati mancanti in qualunque colonna:\n\nd_na &lt;- d[!complete.cases(d), ]\nd_na\n#&gt; # A tibble: 3 × 5\n#&gt;       w     x     y     z     q\n#&gt;   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     2     2     1     5    NA\n#&gt; 2    NA     3     1    10     5\n#&gt; 3    NA     5     1    26     4",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Dati mancanti</span>"
    ]
  },
  {
    "objectID": "chapters/sem/10_missing_data.html#riflessioni-conclusive",
    "href": "chapters/sem/10_missing_data.html#riflessioni-conclusive",
    "title": "57  Dati mancanti",
    "section": "\n57.5 Riflessioni Conclusive",
    "text": "57.5 Riflessioni Conclusive\nIn conclusione, questo capitolo ha fornito una panoramica completa sui dati mancanti, affrontando le loro tipologie, le possibili cause e le strategie di gestione più appropriate in base al contesto e al meccanismo sottostante. Abbiamo esplorato tecniche tradizionali come la listwise e pairwise deletion, metodi di imputazione semplici e avanzati, e approcci più robusti come l’algoritmo Expectation-Maximization e il Full Information Maximum Likelihood (FIML), evidenziandone vantaggi e limiti.\nIn particolare, è stato sottolineato come il metodo FIML rappresenti una soluzione ottimale per molte applicazioni in CFA e SEM, grazie alla sua capacità di sfruttare tutte le informazioni disponibili senza introdurre i bias tipici dell’imputazione. Esempi pratici e codice R hanno illustrato come implementare queste tecniche, rendendo il capitolo una risorsa preziosa sia per comprendere i fondamenti teorici sia per affrontare casi applicativi concreti. La gestione adeguata dei dati mancanti non solo migliora l’affidabilità delle analisi, ma contribuisce a una migliore interpretazione dei risultati, garantendo robustezza e validità nelle conclusioni di ricerca.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Dati mancanti</span>"
    ]
  },
  {
    "objectID": "chapters/sem/10_missing_data.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/sem/10_missing_data.html#informazioni-sullambiente-di-sviluppo",
    "title": "57  Dati mancanti",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] ggokabeito_0.1.0  see_0.10.0        MASS_7.3-65       viridis_0.6.5    \n#&gt;  [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n#&gt;  [9] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n#&gt; [13] lavaan_0.6-19     psych_2.4.12      scales_1.3.0      markdown_1.13    \n#&gt; [17] knitr_1.49        lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n#&gt; [21] dplyr_1.1.4       purrr_1.0.4       readr_2.1.5       tidyr_1.3.1      \n#&gt; [25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.0      datawizard_1.0.0   \n#&gt;   [4] magrittr_2.0.3      TH.data_1.1-3       estimability_1.5.1 \n#&gt;   [7] farver_2.1.2        nloptr_2.1.1        rmarkdown_2.29     \n#&gt;  [10] vctrs_0.6.5         minqa_1.2.8         effectsize_1.0.0   \n#&gt;  [13] base64enc_0.1-3     rstatix_0.7.2       htmltools_0.5.8.1  \n#&gt;  [16] broom_1.0.7         Formula_1.2-5       htmlwidgets_1.6.4  \n#&gt;  [19] plyr_1.8.9          sandwich_3.1-1      rio_1.2.3          \n#&gt;  [22] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#&gt;  [25] mime_0.12           lifecycle_1.0.4     pkgconfig_2.0.3    \n#&gt;  [28] Matrix_1.7-2        R6_2.6.1            fastmap_1.2.0      \n#&gt;  [31] rbibutils_2.3       shiny_1.10.0        digest_0.6.37      \n#&gt;  [34] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n#&gt;  [37] rprojroot_2.0.4     Hmisc_5.2-2         timechange_0.3.0   \n#&gt;  [40] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n#&gt;  [43] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n#&gt;  [46] carData_3.0-5       performance_0.13.0  R.utils_2.13.0     \n#&gt;  [49] ggsignif_0.6.4      corpcor_1.6.10      gtools_3.9.5       \n#&gt;  [52] tools_4.4.2         pbivnorm_0.6.0      foreign_0.8-88     \n#&gt;  [55] zip_2.3.2           httpuv_1.6.15       nnet_7.3-20        \n#&gt;  [58] R.oo_1.27.0         glue_1.8.0          quadprog_1.5-8     \n#&gt;  [61] nlme_3.1-167        promises_1.3.2      lisrelToR_0.3      \n#&gt;  [64] grid_4.4.2          checkmate_2.3.2     cluster_2.1.8      \n#&gt;  [67] reshape2_1.4.4      generics_0.1.3      gtable_0.3.6       \n#&gt;  [70] tzdb_0.4.0          R.methodsS3_1.8.2   data.table_1.17.0  \n#&gt;  [73] hms_1.1.3           car_3.1-3           sem_3.1-16         \n#&gt;  [76] pillar_1.10.1       rockchalk_1.8.157   later_1.4.1        \n#&gt;  [79] splines_4.4.2       lattice_0.22-6      survival_3.8-3     \n#&gt;  [82] kutils_1.73         tidyselect_1.2.1    miniUI_0.1.1.1     \n#&gt;  [85] pbapply_1.7-2       reformulas_0.4.0    stats4_4.4.2       \n#&gt;  [88] xfun_0.51           qgraph_1.9.8        arm_1.14-4         \n#&gt;  [91] stringi_1.8.4       pacman_0.5.1        boot_1.3-31        \n#&gt;  [94] evaluate_1.0.3      codetools_0.2-20    mi_1.1             \n#&gt;  [97] cli_3.6.4           RcppParallel_5.1.10 rpart_4.1.24       \n#&gt; [100] parameters_0.24.1   xtable_1.8-4        Rdpack_2.6.2       \n#&gt; [103] munsell_0.5.1       Rcpp_1.0.14         coda_0.19-4.1      \n#&gt; [106] png_0.1-8           XML_3.99-0.18       parallel_4.4.2     \n#&gt; [109] bayestestR_0.15.2   jpeg_0.1-10         lme4_1.1-36        \n#&gt; [112] mvtnorm_1.3-3       insight_1.0.2       openxlsx_4.2.8     \n#&gt; [115] rlang_1.1.5         multcomp_1.4-28     mnormt_2.1.1\n\n\n\n\n\nBrown, T. A. (2015). Confirmatory factor analysis for applied research. Guilford publications.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.\n\n\nSaqr, M., & López-Pernas, S. (2024). Learning analytics methods and tutorials: A practical guide using R. Springer Nature.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Dati mancanti</span>"
    ]
  },
  {
    "objectID": "chapters/sem/11_small_samples.html",
    "href": "chapters/sem/11_small_samples.html",
    "title": "58  Modellizzazione SEM in Piccoli Campioni",
    "section": "",
    "text": "58.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nQuesto capitolo si concentra sull’applicazione dei modelli SEM in contesti caratterizzati da campioni di piccole dimensioni e sintetizza l’esposizione di Rosseel (2020) su questo argomento, mentre si avvale di esempi numerici tratti dai principi di Kline (2023).\nÈ risaputo che la maggior parte dei metodi di stima e inferenza in SEM si basa su presupposti asintotici, presupponendo la presenza di campioni casuali di grandi dimensioni. Tuttavia, nei casi di campioni più limitati, come quelli con N &lt; 200, emergono specifiche problematiche: i metodi iterativi possono non convergere, si possono verificare soluzioni non valide a causa dei casi di Heywood o di altri risultati anomali difficili da interpretare, e le stime dei parametri possono risultare fortemente distorte.\nInnanzitutto, i modelli strutturali possono diventare molto complessi, coinvolgendo numerose variabili (sia osservate che latenti), rendendo necessaria l’analisi di numerosi parametri e richiedendo un adeguato volume di dati per ottenere stime accurate. Inoltre, il framework statistico alla base della SEM tradizionale si fonda sulla teoria dei grandi campioni, suggerendo che una buona precisione nelle stime dei parametri e nell’inferenza sia garantita solo con campioni di dimensioni considerevoli. Alcuni studi di simulazione hanno addirittura suggerito che dimensioni del campione enormi siano necessarie per risultati affidabili, sebbene tali conclusioni siano rilevanti solo in specifici contesti e abbiano contribuito alla convinzione generalizzata che la SEM sia applicabile solo con campioni di dimensioni considerevoli (ad es. n &gt; 500) o addirittura molto grandi (n &gt; 2000).\nTuttavia, la realtà delle dimensioni ridotte dei campioni è una situazione comune per molte ragioni. In tali casi, molti ricercatori esitano ad utilizzare la SEM e si affidano a metodologie subottimali, come l’analisi di regressione o l’analisi di percorso basate su punteggi sommati. Tuttavia, è importante notare che il bias associato alle dimensioni ridotte del campione può essere ancora più accentuato in tecniche come la regressione multipla o l’analisi di percorso con variabili manifeste, soprattutto in assenza di considerazioni sull’errore di misurazione. Una strategia più efficace potrebbe essere quella di adottare l’approccio della SEM, pur cercando soluzioni per affrontare le sfide poste dalle dimensioni ridotte del campione.\nQuesto capitolo si propone di esplorare diverse strategie per superare tali sfide nell’utilizzo della SEM con campioni di piccole dimensioni. Sarà organizzato in tre sezioni: innanzitutto, verranno esaminate le problematiche comuni associate alle dimensioni ridotte del campione nella SEM. Successivamente, saranno presentati quattro approcci alternativi di stima che possono essere impiegati quando le dimensioni del campione sono limitate, anziché ricorrere alla SEM tradizionale. Infine, saranno discussi alcuni possibili correttivi per le statistiche di test e gli errori standard nelle situazioni di piccoli campioni. L’efficacia di alcune di queste tecniche sarà illustrata tramite l’analisi di un modello di fattore comune applicato a un campione di dimensioni ridotte.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Modellizzazione SEM in Piccoli Campioni</span>"
    ]
  },
  {
    "objectID": "chapters/sem/11_small_samples.html#problemi-con-le-piccole-dimensioni-del-campione-nella-sem",
    "href": "chapters/sem/11_small_samples.html#problemi-con-le-piccole-dimensioni-del-campione-nella-sem",
    "title": "58  Modellizzazione SEM in Piccoli Campioni",
    "section": "58.2 Problemi con le Piccole Dimensioni del Campione nella SEM",
    "text": "58.2 Problemi con le Piccole Dimensioni del Campione nella SEM\nConsideriamo un modello SEM con almeno 4 indicatori continui per ciascuna variabile latente. Se tutte le variabili osservate sono continue, lo stimatore di default nella maggior parte (se non in tutti) dei pacchetti software SEM è il metodo della massima verosimiglianza. Generalmente, lo stimatore della massima verosimiglianza è una buona scelta perché presenta molte proprietà statistiche desiderabili. Inoltre, l’approccio della massima verosimiglianza può essere adattato per gestire dati mancanti (sotto l’assunzione che i dati siano mancanti casualmente) e sono stati sviluppati errori standard e statistiche di test “robusti” per trattare dati non normali e modelli mal specificati.\nTuttavia, se la dimensione del campione è relativamente piccola (ad esempio, n &lt; 200), possono sorgere diversi problemi. Innanzitutto, il modello potrebbe non convergere, il che significa che l’ottimizzatore (l’algoritmo che cerca di trovare i valori dei parametri del modello che massimizzano la verosimiglianza dei dati) non è riuscito a trovare una soluzione che soddisfi uno o più criteri di convergenza. In rare occasioni, l’ottimizzatore potrebbe semplicemente sbagliare. In questo caso, modificare i criteri di convergenza, passare a un altro algoritmo di ottimizzazione o fornire valori iniziali migliori potrebbe risolvere il problema. Ma se la dimensione del campione è piccola, potrebbe benissimo essere che il set di dati non contenga informazioni sufficienti per trovare una soluzione unica per il modello.\nUn secondo problema potrebbe essere che il modello ottenga la convergenza ma produca una soluzione non ammissibile. Ciò significa che alcuni parametri assumono valori inamissibili. L’esempio più comune è una varianza negativa. Un altro esempio è un valore di correlazione che supera 1 (in valore assoluto). È importante rendersi conto che alcuni approcci di stima (sia frequentisti che bayesiani) potrebbero, per progettazione, non produrre mai soluzioni fuori gamma. Sebbene ciò possa sembrare una caratteristica desiderabile, maschera potenziali problemi con il modello o i dati. È importante che gli utenti notino varianze negative (o altri parametri fuori gamma). Le varianze negative sono spesso innocue, ma possono essere un sintomo di una cattiva specificazione strutturale.\nUn terzo problema riguarda il fatto che la massima verosimiglianza è una tecnica per grandi campioni. Questo implica che lavorare con piccole dimensioni del campione può portare a stime puntuali distorte, errori standard troppo piccoli, intervalli di confidenza non sufficientemente ampi e p-valori per test di ipotesi non affidabili.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Modellizzazione SEM in Piccoli Campioni</span>"
    ]
  },
  {
    "objectID": "chapters/sem/11_small_samples.html#soluzioni-possibili-per-la-stima-dei-parametri",
    "href": "chapters/sem/11_small_samples.html#soluzioni-possibili-per-la-stima-dei-parametri",
    "title": "58  Modellizzazione SEM in Piccoli Campioni",
    "section": "58.3 Soluzioni Possibili per la Stima dei Parametri",
    "text": "58.3 Soluzioni Possibili per la Stima dei Parametri\nIn questa sezione, consideriamo brevemente quattro approcci alternativi per stimare i parametri in un contesto SEM con dimensioni di campione piccole. Ci limitiamo ai metodi frequentisti e alle soluzioni disponibili in software gratuiti e open-source.\n\n58.3.1 Stima di Verosimiglianza Penalizzata\nI metodi di stima di verosimiglianza penalizzata (o metodi di regolarizzazione) sono stati sviluppati nella letteratura del machine learning statistico e sono particolarmente utili quando la dimensione del campione è piccola rispetto al numero di variabili nel modello. Questi metodi sono simili ai metodi di verosimiglianza ordinari (come la massima verosimiglianza) ma includono un termine di penalità aggiuntivo per controllare la complessità del modello. Il termine di penalità può essere formulato per incorporare conoscenze pregresse sui parametri o per scoraggiare valori dei parametri meno realistici (ad esempio, lontani da zero). Due termini di penalità popolari sono la penalità ridge e la penalità lasso (least absolute shrinkage and selection operator).\nPer illustrare come funziona questa penalizzazione, immaginate un modello di regressione univariata con un gran numero di predittori. Senza penalizzazione, tutti i coefficienti di regressione sono calcolati nel modo usuale. Tuttavia, il termine di penalità ridge ridurrà tutti i coefficienti verso zero, mentre la penalità lasso ridurrà ulteriormente i piccoli coefficienti fino a zero. In quest’ultimo approccio, sopravvivono solo i predittori “forti” (per i quali c’è un forte supporto nei dati), mentre i predittori “deboli” che possono essere difficilmente distinti dal rumore vengono eliminati. In generale, l’aggiunta di termini di penalità porta a modelli meno complessi, il che è particolarmente vantaggioso se la dimensione del campione è piccola.\nSebbene queste approcci di penalizzazione siano esistiti da alcuni decenni, sono stati applicati solo di recente alla SEM. Due esempi nel software R sono il pacchetto regsem (Jacobucci, Grimm, Brandmaier, Serang e Kievit, 2018) e il pacchetto lslx (Huang e Hu, 2018).\nUno svantaggio di questi metodi di penalizzazione è che l’utente deve indicare quali parametri richiedono la penalizzazione e in che misura. In un’analisi esplorativa, può essere utile e persino vantaggioso penalizzare i parametri verso zero se nel dati non si trova un forte supporto per essi. Tuttavia, la SEM è di solito un approccio confermativo, e l’utente deve assicurarsi che tutti i parametri inizialmente postulati nel modello non vengano rimossi dalla penalizzazione.\n\n\n58.3.2 Model-implied instrumental variables\nBollen (1996) ha proposto un approccio alternativo di stima per i modelli SEM basato sull’utilizzo di variabili strumentali implicite nel modello in combinazione con il metodo dei minimi quadrati a due stadi (MIIV-2SLS). In questo approccio, il modello viene tradotto in un insieme di equazioni (di regressione). Successivamente, ogni variabile latente in queste equazioni viene sostituita con il suo indicatore principale (solitamente il primo indicatore, dove il carico fattoriale è fissato a uno e l’intercetta a zero) meno il suo termine di errore residuo. Le equazioni risultanti non contengono più variabili latenti ma hanno una struttura dell’errore più complessa. È importante notare che la stima dei minimi quadrati ordinari non è più adatta per risolvere queste equazioni poiché alcuni predittori sono ora correlati con il termine di errore nell’equazione. Qui entrano in gioco le variabili strumentali (anche chiamate strumenti). Per ogni equazione, è necessario trovare un insieme di variabili strumentali. Una variabile strumentale deve essere non correlata con il termine di errore dell’equazione ma fortemente correlata con il predittore problematico. Di solito, le variabili strumentali sono ricercate al di fuori del modello, ma nell’approccio di Bollen, le variabili strumentali sono selezionate tra le variabili osservate che fanno parte del modello. Sono stati sviluppati diversi procedimenti automatizzati per trovare queste variabili strumentali all’interno del modello. Una volta selezionati gli strumenti, è necessaria una procedura di stima per stimare tutti i coefficienti delle equazioni, come il metodo dei minimi quadrati a due stadi (2SLS).\nUna motivazione principale per MIIV-2SLS è che è robusto: non si basa sulla normalità ed è meno probabile che diffonda il bias (che può derivare da errate specificazioni strutturali) in una parte del modello ad altre parti del modello. Un’altra caratteristica attraente di MIIV-2SLS è che non è iterativo. Ciò significa che non possono esserci problemi di convergenza e MIIV-2SLS può fornire una soluzione ragionevole per modelli in cui il massimo verosimigliante fallisce nella convergenza.\nSono necessarie ulteriori ricerche per valutare le prestazioni di questo stimatore in contesti in cui la dimensione del campione è (molto) piccola. L’approccio MIIV-2SLS è disponibile nel pacchetto R MIIVsem (Fisher, Bollen, Gates, & Rönkkö, 2017).\n\n\n58.3.3 Stima a Due Fasi\nNel metodo di stima a due fasi, si effettua una distinzione tra la parte di misurazione e la parte strutturale (di regressione) del modello, e la stima avviene in due passaggi distinti. Nel primo passo, vengono adattati uno per uno tutti i modelli di misurazione. Nel secondo passo, viene adattato il modello completo, inclusa la parte strutturale, ma i parametri dei modelli di misurazione vengono mantenuti fissi ai valori trovati nel primo passo. La principale motivazione per l’approccio a due fasi è quella di separare il modello (o i modelli) di misurazione dalla parte strutturale durante la stima in modo che non possano influenzarsi reciprocamente. Nel tradizionale framework della massima verosimiglianza, invece, tutti i parametri vengono adattati simultaneamente. Di conseguenza, errori nella specificazione del modello strutturale possono influenzare i pesi fattoriali stimati di uno o più modelli di misurazione, e ciò può causare problemi di interpretazione per le variabili latenti.\nL’approccio a due fasi è stato recentemente implementato nel pacchetto R lavaan (Rosseel, 2012).\n\n\n58.3.4 Regressione del Punteggio dei Fattori\nL’idea fondamentale della regressione del punteggio dei fattori è quella di sostituire tutte le variabili latenti con i loro punteggi. Questo processo è simile al metodo in due fasi, dove ciascun modello di misurazione viene adattato individualmente. Successivamente, si calcolano i punteggi dei fattori per tutte le variabili latenti nel modo consueto. Una volta che le variabili latenti sono sostituite dai loro punteggi, tutte le variabili diventano osservabili. In un passaggio finale, si stima la parte strutturale del modello. Questa stima può consistere in un’analisi di regressione o in un’analisi dei percorsi. Il termine “regressione del punteggio dei fattori” si riferisce a entrambi gli scenari.\nSe usata in modo ingenuo, questa regressione potrebbe portare a un notevole bias nelle stime dei parametri della parte strutturale, anche con campioni di grandi dimensioni. Questo si verifica perché i punteggi dei fattori vengono trattati come se fossero osservati senza errore di misurazione. Esistono però diversi metodi per correggere questo bias. Ad esempio, il metodo di Croon (2002) procede come segue: prima, si calcola la matrice di varianza-covarianza dei punteggi dei fattori. Poi, sulla base delle informazioni dei modelli di misurazione, gli elementi di questa matrice vengono corretti per approssimare le varianze e covarianze implicite dal modello delle variabili latenti. Questa matrice di varianza-covarianza corretta diventa poi l’input per un’analisi di regressione o dei percorsi regolare.\nSimile al metodo in due fasi, la regressione del punteggio dei fattori (combinata con la correzione di Croon) può essere un’alternativa utile per modelli piuttosto grandi in combinazione con una dimensione campionaria relativamente piccola. Inoltre, è possibile adattare i modelli di misurazione utilizzando un stimatore non iterativo, evitando problemi di convergenza. Tuttavia, la correzione di Croon può produrre una matrice di varianza-covarianza (per le variabili appartenenti alla parte strutturale) che non è definita positiva, specialmente se l’errore di misurazione è sostanziale. Pertanto, la correzione di Croon non è esente da problemi di stima. In questo caso, l’unica soluzione potrebbe essere quella di creare un punteggio somma per ogni variabile latente e stimare un modello in cui ogni variabile latente ha un unico indicatore (il punteggio somma) con la sua affidabilità fissata a un valore realistico fornito dall’utente.\n\n\n58.3.5 Discussione\nTutti i metodi descritti in questa sezione hanno vantaggi e svantaggi. L’approccio della verosimiglianza penalizzata è forse l’unico metodo specificamente progettato per gestire campioni (molto) piccoli. Gli altri tre metodi utilizzano un approccio di “divide et impera”; scompongono il modello completo in parti più piccole e stimano i parametri di ciascuna parte in successione. Oltre a ridurre la complessità e a essere meno vulnerabili a problemi di convergenza, gli ultimi tre metodi hanno il vantaggio di essere efficaci nel localizzare le parti problematiche all’interno di un modello ampio.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Modellizzazione SEM in Piccoli Campioni</span>"
    ]
  },
  {
    "objectID": "chapters/sem/11_small_samples.html#inferenze-per-modelli-sem-in-piccoli-campioni",
    "href": "chapters/sem/11_small_samples.html#inferenze-per-modelli-sem-in-piccoli-campioni",
    "title": "58  Modellizzazione SEM in Piccoli Campioni",
    "section": "58.4 Inferenze per Modelli SEM in Piccoli Campioni",
    "text": "58.4 Inferenze per Modelli SEM in Piccoli Campioni\nMolti autori hanno documentato che quando la dimensione del campione è piccola, il test del chi-quadrato porta ad un inflezione degli errori di Tipo I anche nelle circostanze ideali (cioè, modello correttamente specificato, dati normali). Allo stesso modo, gli errori standard sono spesso attenuati (troppo piccoli) e gli intervalli di confidenza non sono sufficientemente ampi.\nNelle due sottosezioni successive, {cite:t}rosseel2020small discute brevemente alcuni tentativi per affrontare questi problemi di inferenza su campioni piccoli nella modellizzazione SEM.\n\n58.4.1 Migliorare la statistica test del chi-quadrato\nSono state suggerite diverse correzioni per migliorare le prestazioni della statistica del test del chi-quadrato, come la correzione di Bartlett. I risultati di studi di simulazione su questo tema, però, non sono coerenti e, secondo {cite:t}rosseel2020small, per valutare i modelli quando la dimensione del campione è piccola, potrebbe essere opportuno abbandonare del tutto il test del chi-quadrato e esplorare approcci alternativi.\nUn approccio è quello di considerare gli intervalli di confidenza e i test di aderenza basati sull’indice SRMR (standardized root mean square residuals; Maydeu-Olivares, Shi, & Rosseel, 2018). Questi test sembrano funzionare bene anche quando n = 100 (la dimensione del campione più piccola considerata in Maydeu-Olivares et al., 2018) e il modello non è troppo grande. Questi test sono stati implementati come parte della funzione lavResiduals() del pacchetto lavaan.\n\n\n58.4.2 Una migliore stima degli Errori Standard e degli Intervalli di Confidenza\nIn generale, è ben noto che se si utilizza la teoria dei grandi campioni per costruire espressioni analitiche al fine di calcolare gli errori standard, questi possono avere prestazioni scadenti in campioni di piccole dimensioni.\nQuando le assunzioni alla base degli errori standard analitici non sono soddisfatte, spesso si suggerisce di utilizzare un approccio di resampling. Un metodo popolare è il bootstrap (Efron & Tibshirani, 1993): viene generato un campione bootstrap (o campione di replica) e si stima un nuovo set di parametri per questo campione bootstrap. Questo processo viene ripetuto un gran numero di volte (ad esempio, 1.000), e la deviazione standard di un parametro su tutti i campioni bootstrap replicati viene utilizzata come stima dell’errore standard per quel parametro. Purtroppo, nonostante molti altri vantaggi, sembra che il bootstrap non sia una soluzione affidabile quando la dimensione del campione è (molto) piccola (Yung & Bentler, 1996).\nIn alternativa, sono state sviluppate correzioni per dimensioni di campione piccole sugli errori standard (robusti) e sono state recentemente adattate al contesto SEM. Tuttavia, questa tecnologia non è ancora disponibile nei software SEM.\n\n\n58.4.3 Strategie Alternative\nOltre ai metodi precedenti suggeriti da {cite:t}rosseel2020small, altre strategie possibili sono stati indicate da {cite:t}kline2023principles.\n\nSelezione di Indicatori con Elevate Caratteristiche Psicometriche: {cite:t}kline2023principles consiglia di utilizzare indicatori che mostrino eccellenti proprietà psicometriche, idealmente con carichi standardizzati superiori a .70 per gli indicatori continui. Questa pratica riduce il rischio di incorrere in casi di Heywood.\nApplicazione di Restrizioni di Uguaglianza sui Carichi Non Standardizzati: Imponendo vincoli di uguaglianza sulle saturazioni degli indicatori relativi allo stesso fattore si possono evitare soluzioni inammissibili. Questo approccio è particolarmente valido quando gli indicatori sono sulla stessa scala. Un’alternativa valida consiste nel fissare le saturazioni degli indicatori dello stesso fattore a valori costanti non nulli, che riflettano le variazioni nelle loro deviazioni standard.\nSEM Basato su Compositi per Modelli Complessi in Campioni Piccoli: Questo approccio implica l’utilizzo di metodi basati su compositi, che sono combinazioni lineari di variabili osservate. In pratica, le variabili osservate vengono combinate in modi specifici per formare indicatori composti che rappresentano i costrutti latenti nel modello. Questi indicatori composti vengono quindi utilizzati per stimare i parametri del modello SEM. L’obiettivo è ottenere risultati per modelli complessi che richiederebbero campioni molto più ampi con il tradizionale approccio SEM. Tuttavia, è importante notare che i risultati del SEM basato su compositi possono essere soggetti a distorsioni anche nei campioni di piccole dimensioni.\nParceling: Questa strategia coinvolge la creazione di “parcel” o aggregati di due o più indicatori a livello di item attraverso la media dei singoli item. In altre parole, gli indicatori originali sono raggruppati in insiemi più piccoli e i loro valori sono combinati per creare nuovi indicatori aggregati. Questi nuovi indicatori vengono quindi utilizzati per rappresentare i costrutti latenti nel modello SEM. Sebbene il parceling possa ridurre la varianza dell’errore e migliorare la stabilità dei modelli, è importante considerare le sue limitazioni, tra cui la possibile perdita di informazioni e la sensibilità alle scelte fatte durante il processo di parceling. I risultati ottenuti con il parceling possono variare notevolmente in base alle decisioni prese dal ricercatore durante l’analisi.\n\n\n58.4.3.1 Parceling\nApprofondiamo brevemente la strategia del parceling. Il parceling è una strategia che comporta la suddivisione di un set di item in gruppi più piccoli, o “parcel”, per semplificare i modelli e migliorare la loro stima e adattamento.\nUn esempio, citato in {cite:t}kline2023principles, illustra come il parceling possa essere utilizzato in una CFA per ridurre il numero di indicatori e semplificare il modello. {cite:t}kline2023principles considera un questionario di 120 item diviso in tre gruppi distinti di 40 item ciascuno, ognuno mirato a misurare un dominio specifico di un costrutto. In un campione di 150 partecipanti, un’analisi fattoriale confermativa (CFA) con tre fattori e 40 indicatori per fattore, con 120 indicatori totali, può presentare sfide notevoli nella stima del modello a causa della ridotta dimensione del campione. Per affrontare questi problemi, il ricercatore può suddividere ogni gruppo di 40 item in 4 gruppi minori (o “parcel”) di 10 item ciascuno, sommando i punteggi all’interno di ogni “parcel”. Questi punteggi aggregati sostituiscono poi gli item singoli come indicatori in un modello CFA a 3 fattori che avrà quindi solo 12 indicatori in totale (4 indicatori parcellizzati per fattore). Se gli indicatori parcellizzati hanno una distribuzione normale, per la stima si può ricorrere al metodo dei minimi quadrati (ML); altrimenti, si può utilizzare un estimatore ML robusto.\nQuesto metodo è particolarmente utile in situazioni dove si hanno molti item e campioni di dimensioni ridotte, e offre diversi benefici, tra cui:\n\nMaggiore Affidabilità: Il parceling può aumentare l’affidabilità di una scala psicometrica, poiché gli item aggregati tendono ad avere maggior coerenza interna rispetto agli item singoli.\nRapporto Varianza Comune/Varianza Unica: Utilizzando il parceling, si può ottenere un rapporto più favorevole tra la varianza comune (quella spiegata dai fattori comuni) e la varianza unica (quella non spiegata).\nMinore Probabilità di Violazioni Distribuzionali: La pratica del parceling riduce la probabilità che le assunzioni distribuzionali siano violate, il che è importante per l’applicazione di certe tecniche statistiche.\n\nNonostante i suoi benefici, il parceling ha anche limitazioni. La metodologia utilizzata per formare i parcel può influenzare i risultati. Inoltre, il parceling non è consigliabile quando gli item all’interno di un parcel non sono unidimensionali, poiché ciò può distorcere i risultati. È fondamentale verificare l’unidimensionalità prima di procedere con il parceling.\nIn studi con campioni di piccole dimensioni, il parceling ha dimostrato diversi vantaggi, come mostrato nello studio di simulazione di Orçan e Yanyun (2016). Questi includono una riduzione della complessità del modello, tassi di errore di Tipo I più ragionevoli e tassi di errore di Tipo I più bassi quando si utilizza il metodo di stima della massima verosimiglianza con errori standard robusti (MLR) a livello di parcel.\nIn conclusione, il parceling è una tecnica utile che può migliorare l’affidabilità e la validità dei modelli psicometrici, specialmente in presenza di grandi set di item e campioni di piccole dimensioni. Tuttavia, è essenziale valutare attentamente la sua applicabilità e procedere con cautela, specialmente per quanto riguarda l’unidimensionalità dei parcel.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Modellizzazione SEM in Piccoli Campioni</span>"
    ]
  },
  {
    "objectID": "chapters/sem/11_small_samples.html#sem-in-un-piccolo-campione",
    "href": "chapters/sem/11_small_samples.html#sem-in-un-piccolo-campione",
    "title": "58  Modellizzazione SEM in Piccoli Campioni",
    "section": "58.5 SEM in un Piccolo Campione",
    "text": "58.5 SEM in un Piccolo Campione\n{cite:t}kline2023principles discute uno studio in cui è stato applicato un modello CFA a due fattori ad un campione di 103 donne, le quali hanno compilato questionari su esperienze di origine familiare e adattamento coniugale {cite:p}sabatelli2003family.\n\n# input the correlations in lower diagnonal form\nsabatelliLower.cor &lt;- \"\n 1.000\n  .740 1.000\n  .265  .422 1.000\n  .305  .401  .791 1.000\n  .315  .351  .662  .587 1.000 \"\n\n# name the variables and convert to full correlation matrix\nsabatelli.cor &lt;- lavaan::getCov(sabatelliLower.cor, names = c(\n    \"problems\", \"intimacy\", \"father\", \"mother\", \"both\"\n    )\n)\n\n# add the standard deviations and convert to covariances\nsabatelli.cov &lt;- lavaan::cor2cov(sabatelli.cor, sds = c(\n    32.936, 22.749, 13.390, 13.679, 14.382\n    )\n)\n\nIl modello proposto dagli autori è specificato di seguito:\n\nsabatelli_model &lt;- \"\n    # common factors\n    # loading for intimacy constrained to equal .691\n    Marital =~ problems + intimacy\n    FOE =~ father + mother + both\n\"\n\nIn riferimento al modello specificato sopra, la soluzione fornita da lavaan risulta inammissibile a causa di un caso di Heywood, evidenziato da una varianza d’errore negativa per la variabile “intimità”.\n\noriginal &lt;- lavaan::sem(sabatelli_model,\n    sample.cov = sabatelli.cov,\n    sample.nobs = 103\n)\n\n\nlavaan::summary(original,\n    fit.measures = TRUE, standardized = TRUE,\n    rsquare = TRUE\n) |&gt; print()\n\nlavaan 0.6.17 ended normally after 141 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        11\n\n  Number of observations                           103\n\nModel Test User Model:\n                                                      \n  Test statistic                                 4.688\n  Degrees of freedom                                 4\n  P-value (Chi-square)                           0.321\n\nModel Test Baseline Model:\n\n  Test statistic                               271.302\n  Degrees of freedom                                10\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.997\n  Tucker-Lewis Index (TLI)                       0.993\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -2087.964\n  Loglikelihood unrestricted model (H1)      -2085.620\n                                                      \n  Akaike (AIC)                                4197.928\n  Bayesian (BIC)                              4226.910\n  Sample-size adjusted Bayesian (SABIC)       4192.163\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.041\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.159\n  P-value H_0: RMSEA &lt;= 0.050                    0.448\n  P-value H_0: RMSEA &gt;= 0.080                    0.387\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.028\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Marital =~                                                            \n    problems          1.000                              23.362    0.713\n    intimacy          1.006    0.221    4.547    0.000   23.503    1.038\n  FOE =~                                                                \n    father            1.000                              12.488    0.937\n    mother            0.919    0.089   10.320    0.000   11.480    0.843\n    both              0.808    0.098    8.206    0.000   10.088    0.705\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Marital ~~                                                            \n    FOE             129.409   44.216    2.927    0.003    0.444    0.444\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .problems        528.472  130.514    4.049    0.000  528.472    0.492\n   .intimacy        -39.892  109.200   -0.365    0.715  -39.892   -0.078\n   .father           21.613   10.983    1.968    0.049   21.613    0.122\n   .mother           53.509   11.710    4.570    0.000   53.509    0.289\n   .both            103.075   16.168    6.375    0.000  103.075    0.503\n    Marital         545.776  169.103    3.227    0.001    1.000    1.000\n    FOE             155.939   26.732    5.833    0.000    1.000    1.000\n\nR-Square:\n                   Estimate\n    problems          0.508\n    intimacy             NA\n    father            0.878\n    mother            0.711\n    both              0.497\n\n\n\nPer ovviare ad un tale problema, in una nuova analisi del modello di adattamento coniugale è stato applicato un vincolo specifico ai carichi non standardizzati degli indicatori. A causa delle differenze sostanziali nelle metriche tra le due variabili, ovvero “intimacy” (con una deviazione standard di 22.749) e “problems” (con una deviazione standard di 32.936), sono state fissate le seguenti saturazioni fattoriali: il carico per la variabile “problemi” è stato fissato a 1, mentre il carico per la variabile “intimità” è stato fissato a 0.691. Questi valori sono stati calcolati in modo da riflettere proporzionalmente la differenza nelle deviazioni standard tra le due variabili.\n\n# analysis with constrained loadings for indicators of marital adjustment\n# model df = 5\n\n# standard deviations for both indicators\n# of the marital factor are listed next\n# intimacy, sd = 22.749\n# problems, sd = 32.936\n# ratio = 22.749/32.936 = .691\n\n# specify model with constrained loadings for problems, intimacy\n\nproportional.model &lt;- \"\n    # common factors\n    # loading for intimacy constrained to equal .691\n    Marital =~ 1*problems + .691*intimacy\n    FOE =~ father + mother + both \n\"\n\n\nproportional &lt;- lavaan::sem(proportional.model,\n    sample.cov = sabatelli.cov,\n    sample.nobs = 103\n)\n\n\nsemPlot::semPaths(proportional,\n    what = \"col\", whatLabels = \"par\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\n\n\nlavaan::summary(proportional,\n    fit.measures = TRUE, standardized = TRUE,\n    rsquare = TRUE\n) |&gt; print()\n\nlavaan 0.6.17 ended normally after 110 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n\n  Number of observations                           103\n\nModel Test User Model:\n                                                      \n  Test statistic                                 8.449\n  Degrees of freedom                                 5\n  P-value (Chi-square)                           0.133\n\nModel Test Baseline Model:\n\n  Test statistic                               271.302\n  Degrees of freedom                                10\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.987\n  Tucker-Lewis Index (TLI)                       0.974\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -2089.845\n  Loglikelihood unrestricted model (H1)      -2085.620\n                                                      \n  Akaike (AIC)                                4199.690\n  Bayesian (BIC)                              4226.037\n  Sample-size adjusted Bayesian (SABIC)       4194.449\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.082\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.174\n  P-value H_0: RMSEA &lt;= 0.050                    0.242\n  P-value H_0: RMSEA &gt;= 0.080                    0.584\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.045\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Marital =~                                                            \n    problems          1.000                              28.391    0.840\n    intimacy          0.691                              19.618    0.885\n  FOE =~                                                                \n    father            1.000                              12.373    0.929\n    mother            0.935    0.091   10.279    0.000   11.568    0.850\n    both              0.821    0.100    8.235    0.000   10.155    0.710\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Marital ~~                                                            \n    FOE             164.822   42.788    3.852    0.000    0.469    0.469\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .problems        335.923   80.080    4.195    0.000  335.923    0.294\n   .intimacy        106.214   34.373    3.090    0.002  106.214    0.216\n   .father           24.457   11.060    2.211    0.027   24.457    0.138\n   .mother           51.489   11.730    4.389    0.000   51.489    0.278\n   .both            101.712   16.070    6.329    0.000  101.712    0.497\n    Marital         806.040  132.946    6.063    0.000    1.000    1.000\n    FOE             153.095   26.669    5.741    0.000    1.000    1.000\n\nR-Square:\n                   Estimate\n    problems          0.706\n    intimacy          0.784\n    father            0.862\n    mother            0.722\n    both              0.503\n\n\n\n\nfitMeasures(proportional, c(\"chisq\", \"df\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt;\n    print()\n\nchisq    df   cfi   tli rmsea  srmr \n8.449 5.000 0.987 0.974 0.082 0.045 \n\n\n\nlavaan::parameterEstimates(proportional) |&gt; print()\n\n        lhs op      rhs     est      se      z pvalue ci.lower ci.upper\n1   Marital =~ problems   1.000   0.000     NA     NA    1.000    1.000\n2   Marital =~ intimacy   0.691   0.000     NA     NA    0.691    0.691\n3       FOE =~   father   1.000   0.000     NA     NA    1.000    1.000\n4       FOE =~   mother   0.935   0.091 10.279  0.000    0.757    1.113\n5       FOE =~     both   0.821   0.100  8.235  0.000    0.625    1.016\n6  problems ~~ problems 335.923  80.080  4.195  0.000  178.970  492.877\n7  intimacy ~~ intimacy 106.214  34.373  3.090  0.002   38.843  173.584\n8    father ~~   father  24.457  11.060  2.211  0.027    2.779   46.134\n9    mother ~~   mother  51.489  11.730  4.389  0.000   28.498   74.481\n10     both ~~     both 101.712  16.070  6.329  0.000   70.216  133.208\n11  Marital ~~  Marital 806.040 132.946  6.063  0.000  545.471 1066.608\n12      FOE ~~      FOE 153.095  26.669  5.741  0.000  100.825  205.365\n13  Marital ~~      FOE 164.822  42.788  3.852  0.000   80.959  248.684\n\n\n\nlavaan::standardizedSolution(proportional) |&gt; print()\n\n        lhs op      rhs est.std    se      z pvalue ci.lower ci.upper\n1   Marital =~ problems   0.840 0.036 23.439  0.000    0.770    0.910\n2   Marital =~ intimacy   0.885 0.037 24.040  0.000    0.813    0.957\n3       FOE =~   father   0.929 0.035 26.779  0.000    0.861    0.997\n4       FOE =~   mother   0.850 0.040 21.126  0.000    0.771    0.929\n5       FOE =~     both   0.710 0.055 12.800  0.000    0.601    0.818\n6  problems ~~ problems   0.294 0.060  4.884  0.000    0.176    0.412\n7  intimacy ~~ intimacy   0.216 0.065  3.317  0.001    0.088    0.344\n8    father ~~   father   0.138 0.064  2.139  0.032    0.012    0.264\n9    mother ~~   mother   0.278 0.068  4.065  0.000    0.144    0.412\n10     both ~~     both   0.497 0.079  6.313  0.000    0.342    0.651\n11  Marital ~~  Marital   1.000 0.000     NA     NA    1.000    1.000\n12      FOE ~~      FOE   1.000 0.000     NA     NA    1.000    1.000\n13  Marital ~~      FOE   0.469 0.091  5.177  0.000    0.292    0.647\n\n\nNonostante gli indici di bontà di adattamento siano eccellenti, la potenza di questa analisi statistica risulta estremamente limitata. Per valutare questa limitazione, è possibile utilizzare la funzione semTools::findRMSEAsamplesize(). Questa funzione calcola la dimensione del campione necessaria per rilevare una differenza significativa tra RMSEA_0 e RMSEA_A, considerando un modello con df gradi di libertà.\nPer esempio, se desideriamo distinguere tra RMSEA_0=0.05 e RMSEA_A=0.10 utilizzando il modello attuale con 5 gradi di libertà, la funzione ci indica che sono necessarie 561 osservazioni per ottenere una potenza statistica di 0.8:\n\nsemTools::findRMSEAsamplesize(0.05, .10, 5, .80, .05, 1)\n\n561\n\n\nPer creare un grafico che rappresenti la potenza statistica per rilevare la differenza tra RMSEA_0=0.05 e RMSEA_A=0.10 (utilizzati qui come esempio) al variare della dimensione del campione, è possibile seguire la seguente procedura:\n\nsemTools::plotRMSEApower(rmsea0 = .05, rmseaA = .10, df = 5, 50, 1000)\n\n\n\n\n\n\n\n\nQuesta analisi di potenza indica che la dimensione del campione utilizzato (\\(n\\) = 103) è del tutto inadeguata.\nPer migliorare il nostro giudizio sull’adattamento del modello consideriamo l’analisi dei residui.\n\nlavaan::residuals(proportional, type = \"standardized.mplus\") |&gt; print()\n\n$type\n[1] \"standardized.mplus\"\n\n$cov\n         prblms intmcy father mother   both\nproblems     NA                            \nintimacy     NA  0.918                     \nfather   -3.994  1.039  0.002              \nmother   -0.871  1.049  0.328  0.000       \nboth      0.407  0.930  0.352 -2.776     NA\n\n\n\n\nlavaan::lavResiduals(proportional, type = \"cor.bollen\", summary = TRUE) |&gt; print()\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n         prblms intmcy father mother   both\nproblems  0.000                            \nintimacy -0.004  0.000                     \nfather   -0.101  0.036  0.000              \nmother   -0.030  0.048  0.002  0.000       \nboth      0.035  0.056  0.003 -0.016  0.000\n\n$cov.z\n         prblms intmcy father mother   both\nproblems  0.000                            \nintimacy -2.524  0.000                     \nfather   -2.386  1.245  0.000              \nmother   -0.586  1.134  0.881  0.000       \nboth      0.523  0.936  0.507 -1.143  0.000\n\n$summary\n                           cov\ncrmr                     0.044\ncrmr.se                  0.015\ncrmr.exactfit.z          0.504\ncrmr.exactfit.pvalue     0.307\nucrmr                    0.023\nucrmr.se                 0.029\nucrmr.ci.lower          -0.024\nucrmr.cilupper           0.071\nucrmr.closefit.h0.value  0.050\nucrmr.closefit.z        -0.928\nucrmr.closefit.pvalue    0.823\n\n\n\nQuesti sono risultati relativamente scarsi per un modello così piccolo. Il computer non è stato in grado di calcolare tutti i residui standardizzati possibili, il che non è sorprendente in un campione così ridotto.\n\n58.5.1 Stimatore MIIV-2SLS\nUna seconda analisi viene condotta utilizzando lo stimatore MIIV-2SLS. Il pacchetto MIIVsem non calcola statistiche globali di bontà di adattamento. Al contrario, calcola il test di Sargan per ciascun indicatore previsto dal modello. Le statistiche del test di Sargan approssimano distribuzioni chi-quadro centrali con gradi di libertà equivalenti al numero di item meno uno, quindi df = 2. L’ipotesi nulla è che ogni insieme di strumenti multipli sia incorrelato con il termine di errore per l’equazione. Il mancato rifiuto dell’ipotesi nulla per il test di Sargan suggerisce una buona corrispondenza del modello con i dati.\n\nMIIVsem::miivs(sabatelli_model)\n\nModel Equation Information \n\n LHS        RHS        MIIVs                     \n intimacy   problems   father, mother, both      \n mother     father     problems, intimacy, both  \n both       father     problems, intimacy, mother\n\n\n\n\nsabatelli &lt;- MIIVsem::miive(sabatelli_model,\n    sample.cov = sabatelli.cov,\n    sample.nobs = 103, var.cov = TRUE\n)\n\nlavaan::summary(sabatelli, rsquare = TRUE) |&gt; print()\n\nMIIVsem (0.5.8) results \n\nNumber of observations                                                    103\nNumber of equations                                                         3\nEstimator                                                           MIIV-2SLS\nStandard Errors                                                      standard\nMissing                                                              listwise\n\n\nParameter Estimates:\n\n\nSTRUCTURAL COEFFICIENTS:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Sargan   df   P(Chi)\n  FOE =~                                                                     \n    father            1.000                                                  \n    mother            0.899    0.089   10.149    0.000    1.763    2    0.414\n    both              0.787    0.099    7.935    0.000    3.590    2    0.166\n  Marital =~                                                                 \n    problems          1.000                                                  \n    intimacy          0.805    0.155    5.195    0.000    4.980    2    0.083\n\nINTERCEPTS:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   \n    both              0.000                              \n    father            0.000                              \n    intimacy          0.000                              \n    mother            0.000                              \n    problems          0.000                              \n\nVARIANCES:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   \n    FOE             158.501                              \n    Marital         702.393                              \n    both            103.301                              \n    father           20.856                              \n    intimacy         50.871                              \n    mother           54.195                              \n    problems        422.427                              \n\nCOVARIANCES:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   \n  Marital ~~                                             \n    FOE             157.495                              \n\nR-SQUARE:\n                   Estimate\n    problems          0.624\n    intimacy          0.900\n    father            0.884\n    mother            0.703\n    both              0.487\nNULL\n\n\nSi noti che le stime standardizzate non sono calcolate nella versione del pacchetto MIIVsem utilizzata in questa analisi. I valori non standardizzati delle saturazioni fattoriali sono simili a quelli ottenuti in precedenza.\nIl pacchetto MIIVsem non fornisce né le correlazioni previste dal modello per gli indicatori né i residui di correlazione. Per ottenere i residui di correlazione per l’estimatore 2SLS, è possibile utilizzare il pacchetto lavaan per specificare nuovamente il modello precedentemente adattato, ma con l’importante modifica di fissare tutti i parametri non standardizzati in modo che siano identici alle loro controparti 2SLS. Successivamente, è possibile adattare nuovamente il modello con questi parametri fissati alla matrice di covarianza. La matrice di correlazione prevista in questa analisi si basa sulle stime dei parametri 2SLS, consentendo così di ottenere i residui di correlazione desiderati.\n\nsabatelliFixed.model &lt;- \"\n    # common factors\n    Marital =~ 1.0*problems + .805*intimacy\n    FOE =~ 1.0*father + .899*mother + .787*both\n    # factor variances, covariances\n    FOE ~~ 158.501*FOE\n    Marital ~~ 157.495*FOE\n    Marital ~~ 702.393*Marital\n    # indicator error variances\n    father ~~ 20.856*father\n    mother ~~ 54.195*mother\n    both ~~ 103.301*both\n    problems ~~ 422.427*problems\n    intimacy ~~ 50.781*intimacy \n \"\n\n\nsabatelliFixed &lt;- lavaan::sem(sabatelliFixed.model,\n    sample.cov = sabatelli.cov,\n    sample.nobs = 103\n)\n\n\n# standardized parameter \"estimates\" listed\n# next are fixed to nonzero constants, and\n# standard errors are undefined\nlavaan::parameterEstimates(sabatelliFixed)\n\n\nA lavaan.data.frame: 13 x 9\n\n\nlhs\nop\nrhs\nest\nse\nz\npvalue\nci.lower\nci.upper\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nMarital\n=~\nproblems\n1.000\n0\nNA\nNA\n1.000\n1.000\n\n\nMarital\n=~\nintimacy\n0.805\n0\nNA\nNA\n0.805\n0.805\n\n\nFOE\n=~\nfather\n1.000\n0\nNA\nNA\n1.000\n1.000\n\n\nFOE\n=~\nmother\n0.899\n0\nNA\nNA\n0.899\n0.899\n\n\nFOE\n=~\nboth\n0.787\n0\nNA\nNA\n0.787\n0.787\n\n\nFOE\n~~\nFOE\n158.501\n0\nNA\nNA\n158.501\n158.501\n\n\nMarital\n~~\nFOE\n157.495\n0\nNA\nNA\n157.495\n157.495\n\n\nMarital\n~~\nMarital\n702.393\n0\nNA\nNA\n702.393\n702.393\n\n\nfather\n~~\nfather\n20.856\n0\nNA\nNA\n20.856\n20.856\n\n\nmother\n~~\nmother\n54.195\n0\nNA\nNA\n54.195\n54.195\n\n\nboth\n~~\nboth\n103.301\n0\nNA\nNA\n103.301\n103.301\n\n\nproblems\n~~\nproblems\n422.427\n0\nNA\nNA\n422.427\n422.427\n\n\nintimacy\n~~\nintimacy\n50.781\n0\nNA\nNA\n50.781\n50.781\n\n\n\n\n\n\nlavaan::residuals(sabatelliFixed, type = \"standardized.mplus\") |&gt; print()\n\n$type\n[1] \"standardized.mplus\"\n\n$cov\n         prblms intmcy father mother   both\nproblems -0.338                            \nintimacy -0.180  0.092                     \nfather   -0.938  0.016 -0.073              \nmother   -0.120  0.293  0.043  0.116       \nboth      0.491  0.412  0.067  0.100  0.118\n\n\n\n\nlavaan::residuals(sabatelliFixed, type = \"cor.bollen\") |&gt; print()\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n         prblms intmcy father mother   both\nproblems  0.000                            \nintimacy -0.010  0.000                     \nfather   -0.086  0.001  0.000              \nmother   -0.008  0.026  0.003  0.000       \nboth      0.055  0.038  0.006  0.002  0.000\n\n\n\nSi noti che nessuno dei residui di correlazione assoluti basati sui risultati 2SLS supera lo 0.10, compreso il residuo per la coppia di indicatori “problems” e “father”. In termini di adattamento locale, dunque, in questo esempio i risultati dello stimatore 2SLS sono da preferire rispetto a quelli dello stimatore ML.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Modellizzazione SEM in Piccoli Campioni</span>"
    ]
  },
  {
    "objectID": "chapters/sem/11_small_samples.html#considerazioni-conclusive",
    "href": "chapters/sem/11_small_samples.html#considerazioni-conclusive",
    "title": "58  Modellizzazione SEM in Piccoli Campioni",
    "section": "58.6 Considerazioni Conclusive",
    "text": "58.6 Considerazioni Conclusive\nIn questo capitolo abbiamo discusso diversi problemi nel contesto della SEM quando le dimensioni del campione sono ridotte e vengono utilizzati metodi di stima standard (massima verosimiglianza), come la mancata convergenza, le soluzioni non ammissibili, il bias, le statistiche di test poco performanti e gli intervalli di confidenza e gli errori standard inaccurati. Come possibili soluzioni per ottenere stime puntuali migliori, {cite:t}rosseel2020small presenta quattro approcci alternativi alla stima: la stima della verosimiglianza penalizzata, le variabili strumentali derivanti dal modello, la stima a due fasi e la regressione dei punteggi fattoriali. Solo il primo metodo è stato specificamente progettato per gestire campioni ridotti. Gli altri approcci sono stati sviluppati con altre preoccupazioni in mente, ma potrebbero essere alternative valide per la stima quando le dimensioni del campione sono ridotte.\nPer quanto riguarda l’inferenza, {cite:t}rosseel2020small discute vari tentativi per migliorare le prestazioni della statistica del chi-quadro per valutare l’adattamento globale in presenza di campioni ridotti. Per quanto riguarda gli errori standard, sottolinea che il bootstrapping potrebbe non essere la soluzione che stiamo cercando. Per ottenere errori standard (e intervalli di confidenza) migliori nel contesto di campioni ridotti, {cite:t}rosseel2020small ritiene che sia necessario aspettare fino a quando nuove tecnologie saranno disponibili. Altri suggerimenti sono stati forniti da {cite:t}kline2023principles. La tecnica del “parceling” è stata presentata in relazione alla discussione fornita da {cite:t}rioux2020item.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Modellizzazione SEM in Piccoli Campioni</span>"
    ]
  },
  {
    "objectID": "chapters/sem/11_small_samples.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/sem/11_small_samples.html#informazioni-sullambiente-di-sviluppo",
    "title": "58  Modellizzazione SEM in Piccoli Campioni",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] MIIVsem_0.5.8     lme4_1.1-35.5     Matrix_1.7-0      ggokabeito_0.1.0 \n [5] viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1   \n [9] gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6   \n[13] semPlot_1.1.6     lavaan_0.6-18     psych_2.4.6.26    scales_1.3.0     \n[17] markdown_1.13     knitr_1.48        lubridate_1.9.3   forcats_1.0.0    \n[21] stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2       readr_2.1.5      \n[25] tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0  \n[29] here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.16.0  jsonlite_1.8.9     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5.1 farver_2.1.2      \n  [7] nloptr_2.1.1       rmarkdown_2.28     vctrs_0.6.5       \n [10] minqa_1.2.8        base64enc_0.1-3    rstatix_0.7.2     \n [13] htmltools_0.5.8.1  broom_1.0.6        Formula_1.2-5     \n [16] htmlwidgets_1.6.4  plyr_1.8.9         sandwich_3.1-1    \n [19] emmeans_1.10.4     zoo_1.8-12         uuid_1.2-1        \n [22] igraph_2.0.3       mime_0.12          lifecycle_1.0.4   \n [25] pkgconfig_2.0.3    R6_2.5.1           fastmap_1.2.0     \n [28] shiny_1.9.1        digest_0.6.37      OpenMx_2.21.12    \n [31] fdrtool_1.2.18     colorspace_2.1-1   rprojroot_2.0.4   \n [34] Hmisc_5.1-3        fansi_1.0.6        timechange_0.3.0  \n [37] abind_1.4-8        compiler_4.4.1     withr_3.0.1       \n [40] glasso_1.11        htmlTable_2.4.3    backports_1.5.0   \n [43] carData_3.0-5      ggsignif_0.6.4     MASS_7.3-61       \n [46] corpcor_1.6.10     gtools_3.9.5       tools_4.4.1       \n [49] pbivnorm_0.6.0     foreign_0.8-87     zip_2.3.1         \n [52] httpuv_1.6.15      nnet_7.3-19        glue_1.7.0        \n [55] quadprog_1.5-8     promises_1.3.0     nlme_3.1-166      \n [58] lisrelToR_0.3      grid_4.4.1         pbdZMQ_0.3-13     \n [61] checkmate_2.3.2    cluster_2.1.6      reshape2_1.4.4    \n [64] generics_0.1.3     gtable_0.3.5       tzdb_0.4.0        \n [67] data.table_1.16.0  hms_1.1.3          car_3.1-2         \n [70] utf8_1.2.4         sem_3.1-16         pillar_1.9.0      \n [73] IRdisplay_1.1      rockchalk_1.8.157  later_1.3.2       \n [76] splines_4.4.1      lattice_0.22-6     survival_3.7-0    \n [79] kutils_1.73        tidyselect_1.2.1   miniUI_0.1.1.1    \n [82] pbapply_1.7-2      stats4_4.4.1       xfun_0.47         \n [85] qgraph_1.9.8       arm_1.14-4         stringi_1.8.4     \n [88] pacman_0.5.1       boot_1.3-31        evaluate_1.0.0    \n [91] codetools_0.2-20   mi_1.1             cli_3.6.3         \n [94] RcppParallel_5.1.9 IRkernel_1.3.2     rpart_4.1.23      \n [97] xtable_1.8-4       repr_1.1.7         munsell_0.5.1     \n[100] Rcpp_1.0.13        coda_0.19-4.1      png_0.1-8         \n[103] XML_3.99-0.17      parallel_4.4.1     jpeg_0.1-10       \n[106] mvtnorm_1.3-1      openxlsx_4.2.7.1   crayon_1.5.3      \n[109] rlang_1.1.4        multcomp_1.4-26    mnormt_2.1.1      \n\n\n\n\n\n\nKline, R. B. (2023). Principles and practice of structural equation modeling. Guilford publications.\n\n\nRosseel, Y. (2020). Small sample solutions for structural equation modeling. In Small sample size solutions: A guide for applied researchers and practitioners (pp. 226–238). Routledge.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Modellizzazione SEM in Piccoli Campioni</span>"
    ]
  },
  {
    "objectID": "chapters/sem/13_esem.html",
    "href": "chapters/sem/13_esem.html",
    "title": "59  Exploratory structural equation modelling",
    "section": "",
    "text": "59.1 Introduzione\nL’Exploratory Structural Equation Modeling (ESEM) è una tecnica statistica innovativa che combina i punti di forza dell’analisi fattoriale esplorativa (EFA) e dell’analisi fattoriale confermativa (CFA). Introdotta da Asparouhov e Muthén nel 2009 e successivamente sviluppata da Marsh et al. (2009, 2014), l’ESEM consente di modellare strutture fattoriali complesse mantenendo una flessibilità analitica che supera i limiti tradizionali della CFA. Questa tecnica si rivela particolarmente vantaggiosa in presenza di item con fonti di varianza multiple, come sottolineato da Morin et al. (2013), grazie alla sua capacità di migliorare l’adattamento del modello, ridurre le correlazioni spurie tra fattori e rappresentare in modo realistico le saturazioni fattoriali incrociate.\nL’efficacia dell’ESEM è stata dimostrata in numerosi ambiti della psicologia, tra cui la psicologia clinica, educativa, industriale e della salute, dove spesso supera il CFA in termini di adattamento e interpretabilità del modello. Tuttavia, in contesti specifici, potrebbe essere necessario introdurre restrizioni al modello ESEM completamente libero. Questo ha portato all’evoluzione del set-ESEM (Marsh et al., 2020), una tecnica che integra in modo strategico elementi di ESEM e CFA in un quadro analitico unificato.\nIn questo capitolo, esploreremo i fondamenti e le applicazioni dell’Exploratory Structural Equation Modeling (ESEM), seguendo il tutorial proposto da Marsh & Alamer (2024).",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>Exploratory structural equation modelling</span>"
    ]
  },
  {
    "objectID": "chapters/sem/13_esem.html#efa-cfa-esem-e-set-esem",
    "href": "chapters/sem/13_esem.html#efa-cfa-esem-e-set-esem",
    "title": "59  Exploratory structural equation modelling",
    "section": "\n59.2 EFA, CFA, ESEM e Set-ESEM",
    "text": "59.2 EFA, CFA, ESEM e Set-ESEM\nL’analisi fattoriale esplorativa (EFA) e l’analisi fattoriale confermativa (CFA) rappresentano i due approcci principali per indagare le strutture latenti nei dati. L’EFA, introdotta da Spearman (1904) e sviluppata ulteriormente da Thurstone (1935, 1947), era inizialmente conosciuta semplicemente come “analisi fattoriale.” Solo con l’introduzione della CFA si è stabilita una distinzione tra l’approccio esplorativo (EFA) e quello confermativo (CFA). La CFA è diventata uno strumento centrale nella psicometria grazie alla sua capacità di valutare l’adattamento del modello, gestire dati mancanti con metodi avanzati e confrontare modelli teorici alternativi. Tuttavia, uno dei suoi principali limiti è l’ipotesi rigida che ogni item carichi su un solo fattore, ignorando potenziali carichi incrociati.\nQuesto limite ha portato allo sviluppo dell’Exploratory Structural Equation Modeling (ESEM) da parte di Asparouhov e Muthén (2009). L’ESEM combina la flessibilità dell’EFA con la potenza analitica della SEM, consentendo carichi incrociati tra i fattori e offrendo un migliore adattamento ai dati psicometrici complessi. Questo approccio si è dimostrato superiore alla CFA in numerosi studi, migliorando l’adattamento del modello e sostenendo la validità discriminante tra i fattori. Una meta-analisi recente condotta da Gegenfurtner (2022) su 158 studi ha confermato che l’ESEM supera la CFA sia per bontà di adattamento sia per validità discriminante.\nL’ESEM è ormai uno strumento consolidato nella psicometria moderna, dimostrando la sua efficacia nel modellare strutture fattoriali complesse. Grazie alla sua capacità di rappresentare accuratamente le correlazioni e le regressioni tra fattori e di utilizzare tutte le informazioni disponibili a livello degli indicatori, si è affermato come un’alternativa valida e spesso preferibile alla CFA.\nTuttavia, l’ESEM, pur essendo più flessibile, può risultare meno parsimonioso rispetto alla CFA in alcune situazioni. Per questo motivo è stato sviluppato il set-ESEM (Marsh et al., 2020), un’evoluzione che bilancia la flessibilità dell’ESEM con una struttura più rigorosa, tipica della CFA. Il set-ESEM utilizza tecniche di rotazione come la geomin rotation o il target rotation per limitare i carichi incrociati non essenziali, rendendo il modello più parsimonioso e adatto a specifiche esigenze empiriche.\n\n\n\n\n\nFigura 59.1: CFA, ESEM completo e set-ESEM. Nota: le linee tratteggiate indicano i carichi incrociati non target. [Figura tratta da Marsh et al., 2024]",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>Exploratory structural equation modelling</span>"
    ]
  },
  {
    "objectID": "chapters/sem/13_esem.html#situazioni-in-cui-il-set-esem-è-preferibile-allesem-completo",
    "href": "chapters/sem/13_esem.html#situazioni-in-cui-il-set-esem-è-preferibile-allesem-completo",
    "title": "59  Exploratory structural equation modelling",
    "section": "\n59.3 Situazioni in cui il Set-ESEM è Preferibile all’ESEM Completo",
    "text": "59.3 Situazioni in cui il Set-ESEM è Preferibile all’ESEM Completo\nIn alcune situazioni, l’ESEM completo potrebbe non essere l’approccio ottimale per un’analisi. Questo accade quando è necessario garantire che specifici insiemi di fattori e item siano distinti da altri insiemi non correlati. Per affrontare tali scenari, si può utilizzare il set-ESEM, un approccio introdotto da Marsh et al. (2020) che permette di creare sottoinsiemi di ESEM all’interno di un modello più ampio. Il set-ESEM bilancia la flessibilità dell’ESEM completo con la struttura più rigorosa della CFA, offrendo un compromesso ideale in termini di adattamento del modello, parsimonia e definizione chiara dei modelli di misurazione. Di seguito, descriviamo due situazioni reali in cui il set-ESEM può risultare preferibile, basandoci su dati empirici.\n\n59.3.1 1. Item relativi a costrutti teoricamente distinti\nLa prima situazione riguarda dataset che includono item derivati da costrutti concettualmente distinti o appartenenti a teorie differenti. Ad esempio, consideriamo un dataset che misura le tre necessità psicologiche di base—autonomia, competenza e relazionalità—utilizzando la scala BPN-L2 (Alamer, 2022), insieme a due costrutti di perseveranza nello sforzo e coerenza dell’interesse, derivati dalla teoria del grit (Duckworth et al., 2007) e misurati con la scala L2-grit (Alamer, 2021b). Poiché le necessità psicologiche di base e il grit si fondano su teorie con obiettivi e funzioni differenti, stimare carichi incrociati tra i loro item risulterebbe inappropriato. Per esempio, i fattori delle necessità psicologiche sono influenzati dal contesto sociale, mentre il grit è considerato un tratto stabile della personalità.\nIn queste circostanze, il set-ESEM consente di suddividere il modello in due blocchi: uno dedicato ai tre fattori delle necessità psicologiche, con carichi incrociati tra loro ma non con gli item del grit, e un secondo blocco per i due fattori del grit, senza carichi incrociati con le necessità psicologiche. Questo approccio mantiene una maggiore parsimonia, preservando sia la coerenza teorica sia l’accuratezza empirica.\n\n59.3.2 2. Costrutti rilevanti misurati in più momenti temporali\nIl secondo scenario in cui il set-ESEM è consigliato riguarda analisi longitudinali, in cui i dati provengono da costrutti misurati in più momenti temporali. In questi casi, i carichi incrociati dovrebbero essere stimati solo tra item relativi allo stesso momento temporale. Ad esempio, consideriamo un dataset che misura passione armoniosa, passione ossessiva e autonomia in due momenti distinti. Questi costrutti sono correlati concettualmente, rendendo ragionevoli i carichi incrociati all’interno dello stesso momento. Tuttavia, permettere carichi incrociati tra item di momenti diversi sarebbe teoricamente inappropriato e tecnicamente problematico, introducendo effetti di confondimento.\nInoltre, nelle analisi longitudinali SEM è consuetudine correlare i residui degli stessi item nel tempo (Marsh & Hau, 1996). Utilizzando il set-ESEM, è possibile preservare la flessibilità analitica dell’ESEM mantenendo il rigore strutturale necessario per evitare interpretazioni distorte.\n\n\n\n\n\nFigura 59.2: CFA, ESEM completo e set-ESEM. Nota: le linee tratteggiate indicano i carichi incrociati non target. (Figura tratta da Marsh & Alamer, 2024)",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>Exploratory structural equation modelling</span>"
    ]
  },
  {
    "objectID": "chapters/sem/13_esem.html#necessità-psicologiche-di-base-e-percezione-del-sé",
    "href": "chapters/sem/13_esem.html#necessità-psicologiche-di-base-e-percezione-del-sé",
    "title": "59  Exploratory structural equation modelling",
    "section": "\n59.4 Necessità Psicologiche di Base e Percezione del Sé",
    "text": "59.4 Necessità Psicologiche di Base e Percezione del Sé\nPer illustrare il primo scenario descritto in precedenza, esaminiamo uno studio condotto su 269 studenti sauditi che imparano l’inglese come seconda lingua (L2) in un’università pubblica saudita (Marsh & Alamer, 2024). I partecipanti, di età compresa tra 18 e 20 anni (M = 18.5), parlavano arabo come lingua madre e hanno completato un questionario online. Lo studio utilizza il set-ESEM per analizzare i dati, dimostrando come questo approccio possa superare sia l’ESEM completo che la CFA in termini di adattamento del modello e precisione nelle stime.\n\n59.4.1 Struttura del Modello\nLo studio si basa su due blocchi teorici di costrutti:\n\nNecessità Psicologiche di Base (BPN):\nQuesto blocco include autonomia, competenza e relazionalità, tre fattori derivati dalla teoria delle necessità psicologiche di base (Ryan & Deci, 2017; Noels, 2023). Gli item valutano la percezione degli studenti sull’insegnante come promotore di questi tre fattori.\nPercezione del Sé:\nQuesto blocco comprende senso di significato, senso di sicurezza e motivazione intrinseca, costrutti associati ai risultati positivi delle BPN. La letteratura suggerisce che quando gli studenti percepiscono l’insegnante come un promotore delle BPN, si osserva un aumento della motivazione intrinseca, del senso di significato e della sicurezza (Alamer, 2022; Alamer & Al Khateeb, 2023; Guay et al., 2015).\n\n59.4.2 Distinzione Concettuale\nAi partecipanti è stato chiesto di valutare sia la percezione dell’insegnante come promotore delle BPN, sia la loro percezione personale in termini di senso di significato, sicurezza e motivazione intrinseca. Questa distinzione concettuale giustifica l’assenza di carichi incrociati tra i due blocchi di item:\n\nGli item relativi alle BPN si concentrano sull’insegnante e riflettono l’interazione sociale.\n\nGli item relativi alla percezione di sé misurano costrutti soggettivi e individuali.\n\nConsentire carichi incrociati tra questi due domini sarebbe teoricamente ingiustificato, data la loro natura distinta.\n\n59.4.3 Variabile di Esito\nLo studio include anche l’intenzione di abbandonare il corso come variabile di esito. Questo costrutto rappresenta un indicatore pratico delle implicazioni educative delle percezioni relative all’insegnante e al sé.\n\n59.4.4 Analisi dei Modelli\nI modelli alternativi analizzati nello studio—uno basato su CFA e l’altro su set-ESEM—sono illustrati nella Figura 59.3. L’analisi dimostra che il set-ESEM bilancia efficacemente rigore teorico e flessibilità empirica, fornendo stime più affidabili delle relazioni tra variabili latenti e risultati migliori rispetto agli approcci tradizionali.\n\n\n\n\n\nFigura 59.3: Set-ESEM (modello A) e CFA (modello B). Nota: le linee tratteggiate indicano i carichi incrociati non target. (Figura tratta da Marsh & Alamer, 2024)\n\n\nStrumenti di Misura\nPer valutare i costrutti oggetto di studio, sono state utilizzate diverse scale validate, ognuna composta da specifici item rappresentativi.\n\n\nBPN-L2 (Alamer, 2022):\nQuesta scala misura le tre necessità psicologiche di base — autonomia, competenza e relazionalità — ciascuna con tre item. Esempi:\n\n\nAutonomia: “Il mio insegnante ci permette di scegliere i compiti di apprendimento linguistico” (ω = .75).\n\n\nCompetenza: “Il mio insegnante ci dice che siamo capaci di imparare l’inglese” (ω = .75).\n\n\nRelazionalità: “Il mio insegnante di inglese è amichevole e cordiale con noi” (ω = .91).\n\n\n\nMotivazione intrinseca (SDT-L2; Alamer, 2022):\nQuesto costrutto è stato misurato tramite tre item, come:\n\n\n“Imparo l’inglese perché mi piace” (ω = .91).\n\n\n\nSenso di sicurezza e senso di significato (Dörnyei & Ushioda, 2021; Dörnyei & Ryan, 2015):\n\n\nSicurezza: Tre item, ad esempio: “Credo nelle mie capacità di fare bene nel corso” (ω = .74).\n\n\nSignificato: Tre item, come: “So perché mi sono iscritto a questo corso” (ω = .91).\n\n\n\nIntenzione di abbandonare il corso (Lounsbury et al., 2004):\nQuesto costrutto è stato misurato con cinque item, ad esempio:\n\n\n“Non ho intenzione di continuare a studiare in questo settore” (ω = .90).\nTutte le misure adottano una scala Likert a cinque punti, con risposte che vanno da 1 (fortemente in disaccordo) a 5 (fortemente d’accordo), per valutare l’accordo o il disaccordo dei partecipanti con ciascun item.\n\n\n\nDati\nImportiamo i dati e esaminiamo le variabili.\n\nstudy1_dat &lt;- rio::import(\n    here::here(\n        \"data\", \"marsh_alamer\", \"Study_1_data.csv\"\n    )\n)\n\nglimpse(study1_dat)\n#&gt; Rows: 269\n#&gt; Columns: 23\n#&gt; $ Intent_to_withdraw1 &lt;int&gt; 2, 2, 4, 4, 5, 1, 4, 2, 1, 5, 2, 4, 5, 5, 3, 5…\n#&gt; $ Intent_to_withdraw2 &lt;int&gt; 2, 3, 4, 5, 5, 1, 3, 4, 1, 5, 2, 5, 5, 5, 4, 5…\n#&gt; $ Intent_to_withdraw3 &lt;int&gt; 1, 2, 1, 4, 5, 1, 3, 1, 1, 5, 2, 3, 4, 4, 3, 4…\n#&gt; $ Intent_to_withdraw4 &lt;int&gt; 2, 2, 3, 5, 5, 1, 3, 2, 2, 4, 2, 4, 4, 4, 5, 4…\n#&gt; $ Intent_to_withdraw5 &lt;int&gt; 3, 3, 4, 4, 4, 1, 4, 2, 1, 5, 2, 4, 4, 4, 4, 4…\n#&gt; $ T_relatedness1      &lt;int&gt; 4, 4, 2, 2, 2, 5, 1, 4, 4, 4, 4, 4, 1, 1, 1, 1…\n#&gt; $ T_relatedness2      &lt;int&gt; 3, 4, 2, 2, 1, 5, 1, 4, 4, 2, 4, 3, 1, 1, 1, 1…\n#&gt; $ T_relatedness3      &lt;int&gt; 3, 5, 2, 2, 2, 5, 1, 4, 4, 4, 4, 4, 1, 1, 3, 1…\n#&gt; $ T_competence1       &lt;int&gt; 4, 5, 2, 4, 3, 5, 5, 4, 2, 4, 3, 4, 2, 2, 3, 2…\n#&gt; $ T_competence2       &lt;int&gt; 4, 4, 2, 3, 3, 5, 5, 5, 2, 4, 4, 4, 2, 2, 4, 2…\n#&gt; $ T_competence3       &lt;int&gt; 4, 5, 1, 4, 3, 5, 4, 4, 2, 4, 3, 3, 2, 2, 4, 2…\n#&gt; $ T_autonomy1         &lt;int&gt; 4, 5, 2, 2, 4, 5, 3, 4, 4, 4, 4, 4, 2, 2, 2, 2…\n#&gt; $ T_autonomy2         &lt;int&gt; 3, 5, 2, 3, 3, 5, 1, 4, 4, 4, 4, 3, 3, 3, 2, 3…\n#&gt; $ T_autonomy3         &lt;int&gt; 3, 5, 2, 2, 3, 5, 1, 4, 4, 3, 3, 3, 1, 1, 4, 1…\n#&gt; $ S_meaning1          &lt;int&gt; 4, 5, 4, 1, 2, 5, 3, 4, 4, 3, 5, 2, 5, 5, 2, 5…\n#&gt; $ S_meaning2          &lt;int&gt; 3, 5, 4, 2, 2, 5, 1, 4, 4, 4, 5, 2, 5, 5, 1, 5…\n#&gt; $ S_meaning3          &lt;int&gt; 5, 4, 4, 2, 2, 5, 1, 4, 4, 4, 5, 2, 5, 5, 2, 5…\n#&gt; $ S_confidence1       &lt;int&gt; 4, 5, 5, 4, 4, 5, 5, 4, 2, 5, 5, 3, 5, 5, 4, 5…\n#&gt; $ S_confidence2       &lt;int&gt; 4, 5, 5, 4, 4, 5, 5, 4, 2, 5, 5, 4, 5, 5, 4, 5…\n#&gt; $ S_confidence3       &lt;int&gt; 4, 5, 5, 5, 5, 5, 5, 4, 1, 5, 5, 3, 5, 5, 4, 5…\n#&gt; $ S_Intrinsic1        &lt;int&gt; 4, 4, 2, 2, 2, 5, 3, 4, 2, 5, 3, 3, 2, 2, 1, 2…\n#&gt; $ S_Intrinsic2        &lt;int&gt; 4, 5, 2, 3, 3, 5, 4, 4, 2, 5, 5, 4, 1, 1, 2, 1…\n#&gt; $ S_Intrinsic3        &lt;int&gt; 4, 5, 1, 2, 3, 5, 4, 4, 2, 5, 4, 4, 1, 1, 2, 1…\n\nCodice lavaan per il modello ESEM\nDefiniamo il modello ESEM.\n\nesem1 &lt;- '\n\n  # the long format (more flexible) each factor is defined separately\n  efa(\"teacher\")*Teacher_autonomy =~ T_autonomy1 + T_autonomy2 + T_autonomy3 + T_competence1 + T_competence2 + T_competence3 + T_relatedness1 + T_relatedness2 + T_relatedness3\n  efa(\"teacher\")*Teacher_competence =~ T_autonomy1 + T_autonomy2 + T_autonomy3 + T_competence1 + T_competence2 + T_competence3 + T_relatedness1 + T_relatedness2 + T_relatedness3\n  efa(\"teacher\")*Teacher_relatedness =~ T_autonomy1 + T_autonomy2 + T_autonomy3 + T_competence1 + T_competence2 + T_competence3 + T_relatedness1 + T_relatedness2 + T_relatedness3\n\n  # the short format (less flexible) all factors defined in one instance (remove ”##” if you want to use this)\n  # efa(\"teacher\")*Teacher_autonomy +\n  # efa(\"teacher\")*Teacher_competence +\n  # efa(\"teacher\")*Teacher_relatedness =~ T_autonomy1 + T_autonomy2 + T_autonomy3 + T_competence1 + T_competence2 + T_competence3 + T_relatedness1 + T_relatedness2 + T_relatedness3\n\n  # defining the second ESEM block\n  efa(\"self\")*Self_Meaning =~ S_meaning1 + S_meaning2 + S_meaning3 + S_confidence1 + S_confidence2 + S_confidence3 + S_Intrinsic1 + S_Intrinsic2+S_Intrinsic3\n  efa(\"self\")*Self_Confidence =~ S_meaning1 + S_meaning2 + S_meaning3 + S_confidence1 + S_confidence2 + S_confidence3 + S_Intrinsic1 + S_Intrinsic2+S_Intrinsic3\n  efa(\"self\")*Intrinsic_Motivation =~ S_meaning1 + S_meaning2 + S_meaning3 + S_confidence1 + S_confidence2 + S_confidence3 + S_Intrinsic1 + S_Intrinsic2+S_Intrinsic3\n\n  # defining the outcome variable\n  Intent_to_Quit =~ Intent_to_withdraw1 + Intent_to_withdraw2 + Intent_to_withdraw3 + Intent_to_withdraw4 + Intent_to_withdraw5\n\n  # defining the structural part\n  Self_Meaning ~ Teacher_autonomy + Teacher_competence + Teacher_relatedness\n  Self_Confidence ~ Teacher_autonomy + Teacher_competence + Teacher_relatedness\n  Intrinsic_Motivation ~ Teacher_autonomy + Teacher_competence + Teacher_relatedness\n  Intent_to_Quit ~ Self_Meaning + Self_Confidence + Intrinsic_Motivation +\n                   Teacher_autonomy + Teacher_competence + Teacher_relatedness\n'\n\nAdattiamo il modello ai dati.\n\nout1 &lt;- sem(\n    model = esem1,\n    data = study1_dat,\n    estimator = \"MLR\", # verbose = TRUE, test = \"yuan.bentler\",\n    rotation = \"geomin\",\n    rotation.args = list(geomin.epsilon = 0.005)\n)\n\nCreiamo il diagramma di percorso.\n\nsemPlot::semPaths(\n    out1,\n    what = \"col\", whatLabels = \"no\", style = \"mx\",\n    layout = \"tree\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 5, sizeMan2 = 4\n)\n\n\n\n\n\n\n\nEsaminiamo la soluzione fattoriale.\n\nsummary(out1, standardized = TRUE, fit.measures = TRUE) \n#&gt; lavaan 0.6-19 ended normally after 67 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                       103\n#&gt;   Row rank of the constraints matrix                12\n#&gt; \n#&gt;   Rotation method                       GEOMIN OBLIQUE\n#&gt;   Geomin epsilon                                 0.005\n#&gt;   Rotation algorithm (rstarts)                GPA (30)\n#&gt;   Standardized metric                             TRUE\n#&gt;   Row weights                                     None\n#&gt; \n#&gt;   Number of observations                           269\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                               Standard      Scaled\n#&gt;   Test Statistic                               396.932     367.121\n#&gt;   Degrees of freedom                               185         185\n#&gt;   P-value (Chi-square)                           0.000       0.000\n#&gt;   Scaling correction factor                                  1.081\n#&gt;     Yuan-Bentler correction (Mplus variant)                       \n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                              4271.294    3518.139\n#&gt;   Degrees of freedom                               253         253\n#&gt;   P-value                                        0.000       0.000\n#&gt;   Scaling correction factor                                  1.214\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.947       0.944\n#&gt;   Tucker-Lewis Index (TLI)                       0.928       0.924\n#&gt;                                                                   \n#&gt;   Robust Comparative Fit Index (CFI)                         0.950\n#&gt;   Robust Tucker-Lewis Index (TLI)                            0.932\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -7894.627   -7894.627\n#&gt;   Scaling correction factor                                  1.378\n#&gt;       for the MLR correction                                      \n#&gt;   Loglikelihood unrestricted model (H1)      -7696.161   -7696.161\n#&gt;   Scaling correction factor                                  1.179\n#&gt;       for the MLR correction                                      \n#&gt;                                                                   \n#&gt;   Akaike (AIC)                               15971.254   15971.254\n#&gt;   Bayesian (BIC)                             16298.373   16298.373\n#&gt;   Sample-size adjusted Bayesian (SABIC)      16009.844   16009.844\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.065       0.060\n#&gt;   90 Percent confidence interval - lower         0.056       0.052\n#&gt;   90 Percent confidence interval - upper         0.074       0.069\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.003       0.025\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.003       0.000\n#&gt;                                                                   \n#&gt;   Robust RMSEA                                               0.063\n#&gt;   90 Percent confidence interval - lower                     0.053\n#&gt;   90 Percent confidence interval - upper                     0.072\n#&gt;   P-value H_0: Robust RMSEA &lt;= 0.050                         0.013\n#&gt;   P-value H_0: Robust RMSEA &gt;= 0.080                         0.001\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.045       0.045\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Sandwich\n#&gt;   Information bread                           Observed\n#&gt;   Observed information based on                Hessian\n#&gt; \n#&gt; Latent Variables:\n#&gt;                                  Estimate  Std.Err  z-value  P(&gt;|z|)\n#&gt;   Teacher_autonomy =~ teacher                                       \n#&gt;     T_autonomy1                     0.787    0.128    6.159    0.000\n#&gt;     T_autonomy2                     1.156    0.077   14.918    0.000\n#&gt;     T_autonomy3                     0.971    0.104    9.323    0.000\n#&gt;     T_competence1                  -0.058    0.072   -0.806    0.420\n#&gt;     T_competence2                   0.288    0.164    1.753    0.080\n#&gt;     T_competence3                   0.123    0.219    0.564    0.573\n#&gt;     T_relatedness1                  0.423    0.149    2.845    0.004\n#&gt;     T_relatedness2                 -0.011    0.029   -0.391    0.696\n#&gt;     T_relatedness3                  0.332    0.246    1.348    0.178\n#&gt;   Teacher_competence =~ teacher                                     \n#&gt;     T_autonomy1                     0.261    0.119    2.184    0.029\n#&gt;     T_autonomy2                    -0.035    0.039   -0.901    0.368\n#&gt;     T_autonomy3                     0.087    0.085    1.022    0.307\n#&gt;     T_competence1                   1.220    0.073   16.735    0.000\n#&gt;     T_competence2                   0.943    0.147    6.433    0.000\n#&gt;     T_competence3                   0.622    0.169    3.678    0.000\n#&gt;     T_relatedness1                 -0.024    0.020   -1.247    0.212\n#&gt;     T_relatedness2                  0.049    0.053    0.919    0.358\n#&gt;     T_relatedness3                  0.061    0.151    0.404    0.686\n#&gt;   Teacher_relatedness =~ teacher                                    \n#&gt;     T_autonomy1                     0.042    0.061    0.684    0.494\n#&gt;     T_autonomy2                    -0.048    0.066   -0.729    0.466\n#&gt;     T_autonomy3                     0.078    0.104    0.752    0.452\n#&gt;     T_competence1                   0.029    0.054    0.536    0.592\n#&gt;     T_competence2                  -0.041    0.040   -1.043    0.297\n#&gt;     T_competence3                   0.179    0.111    1.608    0.108\n#&gt;     T_relatedness1                  0.832    0.156    5.346    0.000\n#&gt;     T_relatedness2                  1.129    0.086   13.160    0.000\n#&gt;     T_relatedness3                  0.316    0.198    1.593    0.111\n#&gt;   Self_Meaning =~ self                                              \n#&gt;     S_meaning1                      0.808    0.065   12.388    0.000\n#&gt;     S_meaning2                      1.065    0.060   17.818    0.000\n#&gt;     S_meaning3                      1.040    0.056   18.600    0.000\n#&gt;     S_confidence1                  -0.028    0.040   -0.697    0.486\n#&gt;     S_confidence2                   0.098    0.038    2.599    0.009\n#&gt;     S_confidence3                  -0.016    0.017   -0.900    0.368\n#&gt;     S_Intrinsic1                   -0.008    0.053   -0.157    0.875\n#&gt;     S_Intrinsic2                   -0.002    0.044   -0.057    0.955\n#&gt;     S_Intrinsic3                    0.009    0.037    0.231    0.818\n#&gt;   Self_Confidence =~ self                                           \n#&gt;     S_meaning1                      0.052    0.060    0.875    0.382\n#&gt;     S_meaning2                     -0.027    0.027   -0.996    0.319\n#&gt;     S_meaning3                     -0.002    0.026   -0.069    0.945\n#&gt;     S_confidence1                   0.609    0.074    8.255    0.000\n#&gt;     S_confidence2                   0.560    0.059    9.441    0.000\n#&gt;     S_confidence3                   0.553    0.065    8.445    0.000\n#&gt;     S_Intrinsic1                   -0.027    0.073   -0.374    0.708\n#&gt;     S_Intrinsic2                    0.107    0.062    1.737    0.082\n#&gt;     S_Intrinsic3                   -0.011    0.031   -0.354    0.723\n#&gt;   Intrinsic_Motivation =~ self                                      \n#&gt;     S_meaning1                      0.043    0.030    1.461    0.144\n#&gt;     S_meaning2                     -0.011    0.016   -0.665    0.506\n#&gt;     S_meaning3                     -0.014    0.016   -0.915    0.360\n#&gt;     S_confidence1                  -0.026    0.029   -0.905    0.366\n#&gt;     S_confidence2                  -0.004    0.010   -0.396    0.692\n#&gt;     S_confidence3                   0.028    0.024    1.194    0.232\n#&gt;     S_Intrinsic1                    0.449    0.047    9.469    0.000\n#&gt;     S_Intrinsic2                    0.498    0.076    6.581    0.000\n#&gt;     S_Intrinsic3                    0.634    0.078    8.120    0.000\n#&gt;   Intent_to_Quit =~                                                 \n#&gt;     Intnt_t_wthdr1                  1.000                           \n#&gt;     Intnt_t_wthdr2                  0.946    0.033   28.953    0.000\n#&gt;     Intnt_t_wthdr3                  1.017    0.031   32.987    0.000\n#&gt;     Intnt_t_wthdr4                  0.683    0.074    9.170    0.000\n#&gt;     Intnt_t_wthdr5                  0.665    0.053   12.648    0.000\n#&gt;    Std.lv  Std.all\n#&gt;                   \n#&gt;     0.787    0.612\n#&gt;     1.156    0.922\n#&gt;     0.971    0.774\n#&gt;    -0.058   -0.046\n#&gt;     0.288    0.227\n#&gt;     0.123    0.102\n#&gt;     0.423    0.324\n#&gt;    -0.011   -0.009\n#&gt;     0.332    0.250\n#&gt;                   \n#&gt;     0.261    0.203\n#&gt;    -0.035   -0.028\n#&gt;     0.087    0.069\n#&gt;     1.220    0.972\n#&gt;     0.943    0.744\n#&gt;     0.622    0.516\n#&gt;    -0.024   -0.019\n#&gt;     0.049    0.039\n#&gt;     0.061    0.046\n#&gt;                   \n#&gt;     0.042    0.033\n#&gt;    -0.048   -0.038\n#&gt;     0.078    0.062\n#&gt;     0.029    0.023\n#&gt;    -0.041   -0.033\n#&gt;     0.179    0.149\n#&gt;     0.832    0.636\n#&gt;     1.129    0.900\n#&gt;     0.316    0.238\n#&gt;                   \n#&gt;     0.890    0.738\n#&gt;     1.173    0.919\n#&gt;     1.146    0.927\n#&gt;    -0.031   -0.031\n#&gt;     0.107    0.140\n#&gt;    -0.017   -0.023\n#&gt;    -0.009   -0.008\n#&gt;    -0.003   -0.002\n#&gt;     0.009    0.007\n#&gt;                   \n#&gt;     0.055    0.045\n#&gt;    -0.028   -0.022\n#&gt;    -0.002   -0.002\n#&gt;     0.637    0.639\n#&gt;     0.586    0.763\n#&gt;     0.578    0.766\n#&gt;    -0.028   -0.024\n#&gt;     0.112    0.093\n#&gt;    -0.012   -0.009\n#&gt;                   \n#&gt;     0.070    0.058\n#&gt;    -0.018   -0.014\n#&gt;    -0.023   -0.019\n#&gt;    -0.043   -0.043\n#&gt;    -0.006   -0.008\n#&gt;     0.046    0.061\n#&gt;     0.725    0.619\n#&gt;     0.805    0.669\n#&gt;     1.025    0.815\n#&gt;                   \n#&gt;     1.170    0.955\n#&gt;     1.107    0.911\n#&gt;     1.190    0.875\n#&gt;     0.799    0.637\n#&gt;     0.778    0.648\n#&gt; \n#&gt; Regressions:\n#&gt;                           Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n#&gt;   Self_Meaning ~                                                      \n#&gt;     Teacher_autnmy          -0.040    0.139   -0.287    0.774   -0.036\n#&gt;     Teacher_cmptnc           0.271    0.125    2.172    0.030    0.246\n#&gt;     Teachr_rltdnss           0.287    0.120    2.387    0.017    0.260\n#&gt;   Self_Confidence ~                                                   \n#&gt;     Teacher_autnmy          -0.096    0.152   -0.630    0.529   -0.092\n#&gt;     Teacher_cmptnc           0.299    0.146    2.048    0.041    0.286\n#&gt;     Teachr_rltdnss          -0.306    0.159   -1.925    0.054   -0.293\n#&gt;   Intrinsic_Motivation ~                                              \n#&gt;     Teacher_autnmy           0.862    0.333    2.588    0.010    0.533\n#&gt;     Teacher_cmptnc           0.217    0.198    1.096    0.273    0.134\n#&gt;     Teachr_rltdnss           0.339    0.227    1.494    0.135    0.210\n#&gt;   Intent_to_Quit ~                                                    \n#&gt;     Self_Meaning            -0.123    0.078   -1.580    0.114   -0.115\n#&gt;     Self_Confidenc           0.056    0.072    0.774    0.439    0.050\n#&gt;     Intrinsc_Mtvtn           0.169    0.096    1.764    0.078    0.234\n#&gt;     Teacher_autnmy          -0.790    0.211   -3.752    0.000   -0.676\n#&gt;     Teacher_cmptnc           0.078    0.115    0.676    0.499    0.066\n#&gt;     Teachr_rltdnss          -0.208    0.156   -1.337    0.181   -0.178\n#&gt;   Std.all\n#&gt;          \n#&gt;    -0.036\n#&gt;     0.246\n#&gt;     0.260\n#&gt;          \n#&gt;    -0.092\n#&gt;     0.286\n#&gt;    -0.293\n#&gt;          \n#&gt;     0.533\n#&gt;     0.134\n#&gt;     0.210\n#&gt;          \n#&gt;    -0.115\n#&gt;     0.050\n#&gt;     0.234\n#&gt;    -0.676\n#&gt;     0.066\n#&gt;    -0.178\n#&gt; \n#&gt; Covariances:\n#&gt;                          Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n#&gt;   Teacher_autonomy ~~                                                \n#&gt;     Teacher_cmptnc          0.652    0.067    9.730    0.000    0.652\n#&gt;     Teachr_rltdnss          0.659    0.064   10.287    0.000    0.659\n#&gt;   Teacher_competence ~~                                              \n#&gt;     Teachr_rltdnss          0.545    0.084    6.495    0.000    0.545\n#&gt;  .Self_Meaning ~~                                                    \n#&gt;    .Self_Confidenc          0.195    0.083    2.346    0.019    0.195\n#&gt;    .Intrinsc_Mtvtn          0.071    0.105    0.678    0.498    0.071\n#&gt;  .Self_Confidence ~~                                                 \n#&gt;    .Intrinsc_Mtvtn          0.281    0.116    2.431    0.015    0.281\n#&gt;   Std.all\n#&gt;          \n#&gt;     0.652\n#&gt;     0.659\n#&gt;          \n#&gt;     0.545\n#&gt;          \n#&gt;     0.195\n#&gt;     0.071\n#&gt;          \n#&gt;     0.281\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .T_autonomy1       0.642    0.085    7.529    0.000    0.642    0.388\n#&gt;    .T_autonomy2       0.356    0.093    3.822    0.000    0.356    0.226\n#&gt;    .T_autonomy3       0.399    0.062    6.482    0.000    0.399    0.254\n#&gt;    .T_competence1     0.138    0.115    1.209    0.227    0.138    0.088\n#&gt;    .T_competence2     0.338    0.072    4.723    0.000    0.338    0.210\n#&gt;    .T_competence3     0.766    0.106    7.238    0.000    0.766    0.528\n#&gt;    .T_relatedness1    0.409    0.080    5.113    0.000    0.409    0.239\n#&gt;    .T_relatedness2    0.254    0.161    1.579    0.114    0.254    0.161\n#&gt;    .T_relatedness3    1.363    0.119   11.408    0.000    1.363    0.773\n#&gt;    .S_meaning1        0.600    0.084    7.099    0.000    0.600    0.412\n#&gt;    .S_meaning2        0.274    0.089    3.093    0.002    0.274    0.168\n#&gt;    .S_meaning3        0.231    0.052    4.420    0.000    0.231    0.151\n#&gt;    .S_confidence1     0.593    0.136    4.354    0.000    0.593    0.598\n#&gt;    .S_confidence2     0.218    0.053    4.138    0.000    0.218    0.370\n#&gt;    .S_confidence3     0.232    0.046    4.988    0.000    0.232    0.407\n#&gt;    .S_Intrinsic1      0.855    0.105    8.179    0.000    0.855    0.622\n#&gt;    .S_Intrinsic2      0.772    0.101    7.655    0.000    0.772    0.534\n#&gt;    .S_Intrinsic3      0.528    0.098    5.370    0.000    0.528    0.334\n#&gt;    .Intnt_t_wthdr1    0.133    0.031    4.277    0.000    0.133    0.088\n#&gt;    .Intnt_t_wthdr2    0.253    0.038    6.622    0.000    0.253    0.171\n#&gt;    .Intnt_t_wthdr3    0.432    0.057    7.549    0.000    0.432    0.234\n#&gt;    .Intnt_t_wthdr4    0.936    0.137    6.820    0.000    0.936    0.595\n#&gt;    .Intnt_t_wthdr5    0.836    0.104    8.004    0.000    0.836    0.580\n#&gt;     Teacher_autnmy    1.000                               1.000    1.000\n#&gt;     Teacher_cmptnc    1.000                               1.000    1.000\n#&gt;     Teachr_rltdnss    1.000                               1.000    1.000\n#&gt;    .Self_Meaning      1.000                               0.824    0.824\n#&gt;    .Self_Confidenc    1.000                               0.914    0.914\n#&gt;    .Intrinsc_Mtvtn    1.000                               0.383    0.383\n#&gt;    .Intent_to_Quit    0.786    0.093    8.477    0.000    0.574    0.574\n\nCodice lavaan per il modello CFA\nDefiniamo ora il modello CFA.\n\ncfa1 &lt;- ' ## Specify the measurement model\n\n  # \"teacher\" factors\n  Teacher_autonomy    =~    T_autonomy1 +    T_autonomy2 +    T_autonomy3\n  Teacher_competence  =~  T_competence1 +  T_competence2 +  T_competence3\n  Teacher_relatedness =~ T_relatedness1 + T_relatedness2 + T_relatedness3\n\n  # \"self\" factors\n  Self_Meaning         =~    S_meaning1 +    S_meaning2 +    S_meaning3\n  Self_Confidence      =~ S_confidence1 + S_confidence2 + S_confidence3\n  Intrinsic_Motivation =~  S_Intrinsic1 +  S_Intrinsic2 +  S_Intrinsic3\n\n  # defining the outcome variable\n  Intent_to_Quit =~ Intent_to_withdraw1 + Intent_to_withdraw2 + Intent_to_withdraw3 + Intent_to_withdraw4 + Intent_to_withdraw5\n\n  # specify the structural model\n  Self_Meaning ~ Teacher_autonomy + Teacher_competence + Teacher_relatedness\n  Self_Confidence ~ Teacher_autonomy + Teacher_competence + Teacher_relatedness\n  Intrinsic_Motivation ~ Teacher_autonomy + Teacher_competence + Teacher_relatedness\n  Intent_to_Quit ~ Self_Meaning + Self_Confidence + Intrinsic_Motivation +\n                   Teacher_autonomy + Teacher_competence + Teacher_relatedness\n\n  # residual covariances among mediating factors in Block 2 (\"self\")\n  # (not automatically estimated due to being predictors as well,\n  #  but ESEM rotation allows their covariances to be nonzero)\n  Self_Meaning    ~~ Self_Confidence + Intrinsic_Motivation\n  Self_Confidence ~~ Intrinsic_Motivation\n'\n\nAdattiamo il modello.\n\nfit1 &lt;- sem(\n    model = cfa1, data = study1_dat,\n    estimator = \"MLR\", std.lv = TRUE\n)\n\nCreiamo il diagramma di percorso.\n\nsemPlot::semPaths(\n    fit1,\n    what = \"col\", whatLabels = \"no\", style = \"mx\",\n    layout = \"tree\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 6, sizeMan2 = 4\n)\n\n\n\n\n\n\n\nEsaminiamo la soluzione fattoriale.\n\nsummary(fit1, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n#&gt; lavaan 0.6-19 ended normally after 53 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        67\n#&gt; \n#&gt;   Number of observations                           269\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                               Standard      Scaled\n#&gt;   Test Statistic                               459.107     419.040\n#&gt;   Degrees of freedom                               209         209\n#&gt;   P-value (Chi-square)                           0.000       0.000\n#&gt;   Scaling correction factor                                  1.096\n#&gt;     Yuan-Bentler correction (Mplus variant)                       \n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                              4271.294    3518.139\n#&gt;   Degrees of freedom                               253         253\n#&gt;   P-value                                        0.000       0.000\n#&gt;   Scaling correction factor                                  1.214\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.938       0.936\n#&gt;   Tucker-Lewis Index (TLI)                       0.925       0.922\n#&gt;                                                                   \n#&gt;   Robust Comparative Fit Index (CFI)                         0.942\n#&gt;   Robust Tucker-Lewis Index (TLI)                            0.930\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -7925.715   -7925.715\n#&gt;   Scaling correction factor                                  1.439\n#&gt;       for the MLR correction                                      \n#&gt;   Loglikelihood unrestricted model (H1)      -7696.161   -7696.161\n#&gt;   Scaling correction factor                                  1.179\n#&gt;       for the MLR correction                                      \n#&gt;                                                                   \n#&gt;   Akaike (AIC)                               15985.429   15985.429\n#&gt;   Bayesian (BIC)                             16226.275   16226.275\n#&gt;   Sample-size adjusted Bayesian (SABIC)      16013.842   16013.842\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.067       0.061\n#&gt;   90 Percent confidence interval - lower         0.058       0.053\n#&gt;   90 Percent confidence interval - upper         0.075       0.069\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.001       0.013\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.004       0.000\n#&gt;                                                                   \n#&gt;   Robust RMSEA                                               0.064\n#&gt;   90 Percent confidence interval - lower                     0.055\n#&gt;   90 Percent confidence interval - upper                     0.073\n#&gt;   P-value H_0: Robust RMSEA &lt;= 0.050                         0.006\n#&gt;   P-value H_0: Robust RMSEA &gt;= 0.080                         0.001\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.053       0.053\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Sandwich\n#&gt;   Information bread                           Observed\n#&gt;   Observed information based on                Hessian\n#&gt; \n#&gt; Latent Variables:\n#&gt;                           Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n#&gt;   Teacher_autonomy =~                                                 \n#&gt;     T_autonomy1              1.016    0.059   17.270    0.000    1.016\n#&gt;     T_autonomy2              1.066    0.050   21.119    0.000    1.066\n#&gt;     T_autonomy3              1.093    0.046   23.977    0.000    1.093\n#&gt;   Teacher_competence =~                                               \n#&gt;     T_competence1            1.129    0.053   21.180    0.000    1.129\n#&gt;     T_competence2            1.164    0.050   23.337    0.000    1.164\n#&gt;     T_competence3            0.819    0.074   11.027    0.000    0.819\n#&gt;   Teacher_relatedness =~                                              \n#&gt;     T_relatedness1           1.191    0.048   24.632    0.000    1.191\n#&gt;     T_relatedness2           1.053    0.052   20.124    0.000    1.053\n#&gt;     T_relatedness3           0.628    0.089    7.015    0.000    0.628\n#&gt;   Self_Meaning =~                                                     \n#&gt;     S_meaning1               0.839    0.061   13.822    0.000    0.920\n#&gt;     S_meaning2               1.061    0.060   17.603    0.000    1.163\n#&gt;     S_meaning3               1.039    0.058   17.821    0.000    1.139\n#&gt;   Self_Confidence =~                                                  \n#&gt;     S_confidence1            0.582    0.071    8.139    0.000    0.617\n#&gt;     S_confidence2            0.578    0.057   10.172    0.000    0.612\n#&gt;     S_confidence3            0.538    0.066    8.172    0.000    0.570\n#&gt;   Intrinsic_Motivation =~                                             \n#&gt;     S_Intrinsic1             0.443    0.050    8.881    0.000    0.719\n#&gt;     S_Intrinsic2             0.501    0.074    6.773    0.000    0.812\n#&gt;     S_Intrinsic3             0.634    0.069    9.250    0.000    1.028\n#&gt;   Intent_to_Quit =~                                                   \n#&gt;     Intnt_t_wthdr1           0.893    0.050   17.918    0.000    1.171\n#&gt;     Intnt_t_wthdr2           0.844    0.048   17.608    0.000    1.107\n#&gt;     Intnt_t_wthdr3           0.907    0.053   17.267    0.000    1.189\n#&gt;     Intnt_t_wthdr4           0.608    0.069    8.834    0.000    0.798\n#&gt;     Intnt_t_wthdr5           0.593    0.055   10.801    0.000    0.778\n#&gt;   Std.all\n#&gt;          \n#&gt;     0.790\n#&gt;     0.850\n#&gt;     0.871\n#&gt;          \n#&gt;     0.900\n#&gt;     0.918\n#&gt;     0.681\n#&gt;          \n#&gt;     0.911\n#&gt;     0.840\n#&gt;     0.473\n#&gt;          \n#&gt;     0.763\n#&gt;     0.911\n#&gt;     0.922\n#&gt;          \n#&gt;     0.619\n#&gt;     0.798\n#&gt;     0.756\n#&gt;          \n#&gt;     0.614\n#&gt;     0.675\n#&gt;     0.817\n#&gt;          \n#&gt;     0.956\n#&gt;     0.910\n#&gt;     0.875\n#&gt;     0.636\n#&gt;     0.648\n#&gt; \n#&gt; Regressions:\n#&gt;                          Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n#&gt;   Self_Meaning ~                                                     \n#&gt;     Teacher_autnmy         -0.170    0.192   -0.882    0.378   -0.155\n#&gt;     Teacher_cmptnc          0.335    0.160    2.096    0.036    0.305\n#&gt;     Teachr_rltdnss          0.308    0.157    1.963    0.050    0.281\n#&gt;   Self_Confidence ~                                                  \n#&gt;     Teacher_autnmy         -0.080    0.158   -0.511    0.610   -0.076\n#&gt;     Teacher_cmptnc          0.437    0.134    3.251    0.001    0.412\n#&gt;     Teachr_rltdnss         -0.403    0.160   -2.526    0.012   -0.380\n#&gt;   Intrinsic_Motivation ~                                             \n#&gt;     Teacher_autnmy          0.886    0.283    3.131    0.002    0.546\n#&gt;     Teacher_cmptnc          0.128    0.192    0.666    0.505    0.079\n#&gt;     Teachr_rltdnss          0.342    0.213    1.606    0.108    0.211\n#&gt;   Intent_to_Quit ~                                                   \n#&gt;     Self_Meaning           -0.151    0.087   -1.731    0.083   -0.126\n#&gt;     Self_Confidenc          0.051    0.085    0.596    0.551    0.041\n#&gt;     Intrinsc_Mtvtn          0.197    0.109    1.804    0.071    0.244\n#&gt;     Teacher_autnmy         -0.899    0.267   -3.368    0.001   -0.686\n#&gt;     Teacher_cmptnc          0.155    0.143    1.083    0.279    0.118\n#&gt;     Teachr_rltdnss         -0.259    0.191   -1.356    0.175   -0.197\n#&gt;   Std.all\n#&gt;          \n#&gt;    -0.155\n#&gt;     0.305\n#&gt;     0.281\n#&gt;          \n#&gt;    -0.076\n#&gt;     0.412\n#&gt;    -0.380\n#&gt;          \n#&gt;     0.546\n#&gt;     0.079\n#&gt;     0.211\n#&gt;          \n#&gt;    -0.126\n#&gt;     0.041\n#&gt;     0.244\n#&gt;    -0.686\n#&gt;     0.118\n#&gt;    -0.197\n#&gt; \n#&gt; Covariances:\n#&gt;                         Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n#&gt;  .Self_Meaning ~~                                                   \n#&gt;    .Self_Confidenc         0.241    0.081    2.970    0.003    0.241\n#&gt;    .Intrinsc_Mtvtn         0.093    0.106    0.876    0.381    0.093\n#&gt;  .Self_Confidence ~~                                                \n#&gt;    .Intrinsc_Mtvtn         0.334    0.119    2.801    0.005    0.334\n#&gt;   Teacher_autonomy ~~                                               \n#&gt;     Teacher_cmptnc         0.765    0.044   17.327    0.000    0.765\n#&gt;     Teachr_rltdnss         0.798    0.042   18.935    0.000    0.798\n#&gt;   Teacher_competence ~~                                             \n#&gt;     Teachr_rltdnss         0.662    0.049   13.510    0.000    0.662\n#&gt;   Std.all\n#&gt;          \n#&gt;     0.241\n#&gt;     0.093\n#&gt;          \n#&gt;     0.334\n#&gt;          \n#&gt;     0.765\n#&gt;     0.798\n#&gt;          \n#&gt;     0.662\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .T_autonomy1       0.623    0.086    7.243    0.000    0.623    0.376\n#&gt;    .T_autonomy2       0.435    0.063    6.871    0.000    0.435    0.277\n#&gt;    .T_autonomy3       0.380    0.062    6.153    0.000    0.380    0.241\n#&gt;    .T_competence1     0.301    0.063    4.791    0.000    0.301    0.191\n#&gt;    .T_competence2     0.254    0.058    4.380    0.000    0.254    0.158\n#&gt;    .T_competence3     0.778    0.116    6.726    0.000    0.778    0.537\n#&gt;    .T_relatedness1    0.291    0.077    3.752    0.000    0.291    0.170\n#&gt;    .T_relatedness2    0.464    0.066    7.012    0.000    0.464    0.295\n#&gt;    .T_relatedness3    1.369    0.121   11.283    0.000    1.369    0.777\n#&gt;    .S_meaning1        0.608    0.088    6.906    0.000    0.608    0.418\n#&gt;    .S_meaning2        0.278    0.089    3.115    0.002    0.278    0.171\n#&gt;    .S_meaning3        0.230    0.052    4.434    0.000    0.230    0.151\n#&gt;    .S_confidence1     0.612    0.139    4.413    0.000    0.612    0.616\n#&gt;    .S_confidence2     0.214    0.055    3.893    0.000    0.214    0.363\n#&gt;    .S_confidence3     0.244    0.049    4.982    0.000    0.244    0.429\n#&gt;    .S_Intrinsic1      0.857    0.105    8.146    0.000    0.857    0.624\n#&gt;    .S_Intrinsic2      0.786    0.108    7.292    0.000    0.786    0.544\n#&gt;    .S_Intrinsic3      0.528    0.090    5.836    0.000    0.528    0.333\n#&gt;    .Intnt_t_wthdr1    0.130    0.031    4.223    0.000    0.130    0.087\n#&gt;    .Intnt_t_wthdr2    0.254    0.038    6.685    0.000    0.254    0.172\n#&gt;    .Intnt_t_wthdr3    0.433    0.057    7.569    0.000    0.433    0.235\n#&gt;    .Intnt_t_wthdr4    0.937    0.137    6.830    0.000    0.937    0.595\n#&gt;    .Intnt_t_wthdr5    0.836    0.104    8.005    0.000    0.836    0.580\n#&gt;     Teacher_autnmy    1.000                               1.000    1.000\n#&gt;     Teacher_cmptnc    1.000                               1.000    1.000\n#&gt;     Teachr_rltdnss    1.000                               1.000    1.000\n#&gt;    .Self_Meaning      1.000                               0.832    0.832\n#&gt;    .Self_Confidenc    1.000                               0.889    0.889\n#&gt;    .Intrinsc_Mtvtn    1.000                               0.380    0.380\n#&gt;    .Intent_to_Quit    1.000                               0.582    0.582\n\nConfronto tra modelli\n\n# Extract model fit statistics from out1 and fit1\nfit_stats_out1 &lt;- fitMeasures(out1, c(\"chisq\", \"df\", \"rmsea\", \"rmsea.ci.lower\", \"rmsea.ci.upper\", \"cfi\", \"tli\"))\nfit_stats_fit1 &lt;- fitMeasures(fit1, c(\"chisq\", \"df\", \"rmsea\", \"rmsea.ci.lower\", \"rmsea.ci.upper\", \"cfi\", \"tli\"))\n\n# Create a tibble with the extracted fit statistics\nfit_table &lt;- tibble(\n    Model = c(\"CFA-based model\", \"Set-ESEM-based model\"),\n    chisq = c(fit_stats_fit1[\"chisq\"], fit_stats_out1[\"chisq\"]),\n    df = c(fit_stats_fit1[\"df\"], fit_stats_out1[\"df\"]),\n    RMSEA = c(fit_stats_fit1[\"rmsea\"], fit_stats_out1[\"rmsea\"]),\n    `RMSEA 90% CI` = c(\n        sprintf(\"(%.3f, %.3f)\", fit_stats_fit1[\"rmsea.ci.lower\"], fit_stats_fit1[\"rmsea.ci.upper\"]),\n        sprintf(\"(%.3f, %.3f)\", fit_stats_out1[\"rmsea.ci.lower\"], fit_stats_out1[\"rmsea.ci.upper\"])\n    ),\n    CFI = c(fit_stats_fit1[\"cfi\"], fit_stats_out1[\"cfi\"]),\n    TLI = c(fit_stats_fit1[\"tli\"], fit_stats_out1[\"tli\"])\n)\n\n# Convert numeric columns to formatted strings with three decimal places\nfit_table &lt;- fit_table %&gt;%\n    mutate(\n        across(where(is.numeric), ~ sprintf(\"%.3f\", .)),\n        chisq = sprintf(\"%.3f\", as.numeric(chisq)), # Ensure chisq is formatted correctly\n        df = as.character(df) # Convert df to character for consistent formatting\n    )\n\n# Calculate column widths for alignment\ncol_widths &lt;- fit_table %&gt;%\n    summarise(across(everything(), ~ max(nchar(.), na.rm = TRUE)))\n\n# Create text-based table output\nheader &lt;- paste(\n    str_pad(\"Model\", col_widths$Model, side = \"right\"),\n    str_pad(\"chisq\", col_widths$chisq, side = \"right\"),\n    str_pad(\"df\", col_widths$df, side = \"right\"),\n    str_pad(\"RMSEA\", col_widths$RMSEA, side = \"right\"),\n    str_pad(\"RMSEA 90% CI\", col_widths$`RMSEA 90% CI`, side = \"right\"),\n    str_pad(\"CFI\", col_widths$CFI, side = \"right\"),\n    str_pad(\"TLI\", col_widths$TLI, side = \"right\"),\n    sep = \" | \"\n)\nseparator &lt;- strrep(\"-\", nchar(header))\n\n# Print header and separator\ncat(header, \"\\n\")\n#&gt; Model                | chisq   | df      | RMSEA | RMSEA 90% CI   | CFI   | TLI\ncat(separator, \"\\n\")\n#&gt; ---------------------------------------------------------------------------------\n\n# Print each row formatted with aligned columns\nfit_table %&gt;%\n    mutate(\n        Model = str_pad(Model, col_widths$Model, side = \"right\"),\n        chisq = str_pad(chisq, col_widths$chisq, side = \"right\"),\n        df = str_pad(df, col_widths$df, side = \"right\"),\n        RMSEA = str_pad(RMSEA, col_widths$RMSEA, side = \"right\"),\n        `RMSEA 90% CI` = str_pad(`RMSEA 90% CI`, col_widths$`RMSEA 90% CI`, side = \"right\"),\n        CFI = str_pad(CFI, col_widths$CFI, side = \"right\"),\n        TLI = str_pad(TLI, col_widths$TLI, side = \"right\")\n    ) %&gt;%\n    rowwise() %&gt;%\n    mutate(row_text = paste(Model, chisq, df, RMSEA, `RMSEA 90% CI`, CFI, TLI, sep = \" | \")) %&gt;%\n    pull(row_text) %&gt;%\n    cat(sep = \"\\n\")\n#&gt; CFA-based model      | 459.107 | 209.000 | 0.067 | (0.058, 0.075) | 0.938 | 0.925\n#&gt; Set-ESEM-based model | 396.932 | 185.000 | 0.065 | (0.056, 0.074) | 0.947 | 0.928\n\nPer stimare i modelli, Marsh & Alamer (2024) utilizzano la versione robusta della massima verosimiglianza (MLR), che garantisce stime robuste rispetto a deviazioni dalla normalità multivariata (Yuan & Bentler, 2000). Per valutare la qualità dei modelli, sono presi in considerazione diversi indicatori di adattamento: il chi-quadro robusto (χ²) con i relativi gradi di libertà e valore p, il Comparative Fit Index (CFI), il Tucker-Lewis Index (TLI), e il Root Mean Square Error of Approximation (RMSEA) con il suo intervallo di confidenza al 90%. Coerentemente con l’approccio MLR, i valori di CFI, TLI e RMSEA riportati nei due esempi sono calcolati nella loro versione robusta.\nI risultati presentati nella tabella precedente indicano che sia il modello strutturale basato su CFA sia quello basato su set-ESEM mostrano un buon adattamento ai dati. Tuttavia, Marsh & Alamer (2024) si concentrano principalmente sulle differenze nelle relazioni strutturali tra i due modelli, tralasciando un’analisi dettagliata delle specifiche del modello di misura.\n\n59.4.5 Miglior adattamento del set-ESEM\nLa Tabella 2 dell’articolo di Marsh & Alamer (2024) riporta i coefficienti di percorso per entrambi i modelli. Sebbene entrambi mostrino un adattamento accettabile, il modello set-ESEM si distingue per un adattamento superiore. Questo è evidenziato da un incremento di +0.01 nei valori di TLI e CFI rispetto al modello CFA. Anche i criteri informativi confermano il vantaggio del set-ESEM:\n\n\nCFA: AIC = 15985.43, BIC = 16226.27, BIC corretto = 16013.84.\n\nSet-ESEM: AIC = 15971.25, BIC = 16298.37, BIC corretto = 16009.84.\n\nValori più bassi di AIC e BIC indicano un miglior adattamento complessivo per il modello set-ESEM.\n\n59.4.6 Riduzione delle correlazioni sovrastimate\nUn noto limite del CFA è la tendenza a sovrastimare le correlazioni tra variabili latenti esogene, il che può aumentare il rischio di collinearità e compromettere l’accuratezza dei parametri stimati (Shao et al., 2022). Il set-ESEM riduce notevolmente questo problema. Ad esempio:\n\nLa correlazione tra Autonomia_Insegnante e Relazionalità_Insegnante nel modello CFA è di .80, mentre nel set-ESEM scende a .51 (Δr = .29).\n\nQuesta riduzione migliora la validità discriminante e garantisce stime più attendibili.\n\n59.4.7 Sensibilità ai percorsi significativi\nIl set-ESEM dimostra una maggiore capacità di rilevare percorsi significativi rispetto al CFA. Due esempi illustrativi includono:\n\nIl percorso Competenza_Insegnante → Motivazione_Intrinseca è non significativo nel CFA (β = .08, p = .51), ma diventa significativo nel set-ESEM (β = .19, p = .03).\nIl percorso Relazionalità_Insegnante → Intenzione_di_Ritiro è non significativo nel CFA (β = −.20, p = .18), ma significativo nel set-ESEM (β = −.28, p = .01).\n\nQuesti risultati dimostrano come il set-ESEM possa rivelare relazioni importanti tra variabili latenti che il CFA potrebbe non individuare.\nIn conclusione, i risultati evidenziano che i modelli CFA e set-ESEM possono portare a interpretazioni diverse sulle relazioni tra variabili latenti. Tuttavia, il miglior adattamento del set-ESEM, unito alla riduzione delle correlazioni spurie e a una maggiore sensibilità ai percorsi significativi, suggerisce che i coefficienti stimati con questo approccio siano più affidabili. In contesti analitici complessi, il set-ESEM si dimostra un’opzione preferibile, garantendo un equilibrio ottimale tra flessibilità, parsimonia e validità delle stime.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>Exploratory structural equation modelling</span>"
    ]
  },
  {
    "objectID": "chapters/sem/13_esem.html#riflessioni-conclusive",
    "href": "chapters/sem/13_esem.html#riflessioni-conclusive",
    "title": "59  Exploratory structural equation modelling",
    "section": "\n59.5 Riflessioni Conclusive",
    "text": "59.5 Riflessioni Conclusive\nIn questo tutorial, Marsh & Alamer (2024) presentano un’introduzione approfondita all’ESEM, con particolare attenzione al set-ESEM. Sebbene questa tecnica sia spesso utilizzata per i modelli di misurazione, gli autori ne esplorano l’applicazione nei modelli strutturali, evidenziando i vantaggi concettuali ed empirici rispetto al CFA e all’ESEM completamente rilassato. Il set-ESEM, grazie alla possibilità di specificare “mini-set” indipendenti di ESEM all’interno di un unico modello (Marsh et al., 2020), rappresenta un compromesso ottimale tra flessibilità e parsimonia. Questo approccio è particolarmente utile quando l’ESEM completo risulta tecnicamente impraticabile o teoricamente non giustificato.\nGli esempi discussi dagli autori dimostrano che il set-ESEM offre una rappresentazione più accurata dei dati rispetto al CFA. Sebbene entrambi i modelli possano mostrare un adattamento accettabile, le correlazioni tra fattori esogeni nel CFA tendono a essere sistematicamente sovrastimate, come evidenziato in entrambi gli studi presentati. Questo porta a un rischio maggiore di multicollinearità, che può influenzare negativamente la stima dei coefficienti di percorso (Mai et al., 2018; Morin, 2023). Ad esempio, nello Studio 1, alcune relazioni tra variabili latenti risultano significative nel set-ESEM, ma non nel CFA. In particolare:\n\nIl percorso Competenza_Insegnante → Motivazione_Intrinseca, non significativo nel CFA (β = .08, p = .51), diventa significativo nel set-ESEM (β = .19, p = .03).\nIl percorso Relazionalità_Insegnante → Intenzione_di_Ritiro, non significativo nel CFA (β = −.20, p = .18), risulta significativo nel set-ESEM (β = −.28, p = .01).\n\nQueste differenze sottolineano come il set-ESEM riesca a identificare relazioni importanti tra variabili che il CFA potrebbe non rilevare. Simili variazioni emergono nello Studio 2 (non trattato qui), dove le differenze tra i due approcci influenzano le conclusioni sulle relazioni longitudinali.\nMarsh & Alamer (2024) identificano i principali vantaggi del set-ESEM rispetto a CFA ed ESEM completo:\n\n\nEquilibrio tra parsimonia e adattamento: Il set-ESEM è più parsimonioso dell’ESEM completo e si adatta spesso meglio del CFA.\n\nSeparazione dei costrutti teorici: Permette di includere costrutti teoricamente distinti in un unico modello, evitando carichi incrociati non giustificati.\n\nRotazione target: Consente un approccio confermativo, superando le limitazioni delle rotazioni meccaniche (es. geomin).\n\nGestione di modelli strutturali complessi: Rende possibile testare modelli strutturali che l’ESEM completo non può trattare.\n\nMigliore validità discriminante: Le correlazioni tra fattori risultano più realistiche rispetto al CFA.\n\nPrecisione dei coefficienti di percorso: Riduce l’attenuazione degli effetti, migliorando l’accuratezza delle stime e diminuendo i tassi di errore di tipo II.\n\nNonostante i suoi vantaggi, il set-ESEM rimane meno parsimonioso rispetto al CFA. Pertanto, quando i modelli CFA e set-ESEM mostrano correlazioni tra fattori e indici di adattamento simili, il CFA dovrebbe essere preferito per la sua semplicità. Tuttavia, in presenza di cross-loading teoricamente giustificati, il set-ESEM rappresenta l’opzione migliore.\nIn sintesi, il set-ESEM offre una soluzione metodologica efficace per superare i limiti del CFA e dell’ESEM completo. Gli esempi empirici mostrano come l’adozione del set-ESEM possa migliorare l’adattamento del modello e la precisione delle stime, evitando problemi di collinearità comuni nel CFA. L’analisi dei dati con il CFA, senza considerare il set-ESEM, potrebbe portare a interpretazioni parziali o errate delle relazioni tra variabili, con implicazioni importanti per la teoria e la pratica (Shao et al., 2022; Tabachnick & Fidell, 2023).\nIn definitiva, Marsh & Alamer (2024) raccomandano l’uso del set-ESEM per analisi strutturali e di misurazione, specialmente quando l’ESEM completo è troppo flessibile o il CFA troppo restrittivo. La loro analisi evidenzia che, in situazioni in cui i coefficienti di percorso differiscono significativamente tra CFA e set-ESEM, quest’ultimo fornisce risultati più affidabili e interpretabili.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>Exploratory structural equation modelling</span>"
    ]
  },
  {
    "objectID": "chapters/sem/13_esem.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/sem/13_esem.html#informazioni-sullambiente-di-sviluppo",
    "title": "59  Exploratory structural equation modelling",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] kableExtra_1.4.0  ggokabeito_0.1.0  see_0.10.0        MASS_7.3-65      \n#&gt;  [5] viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1   \n#&gt;  [9] gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6   \n#&gt; [13] semPlot_1.1.6     lavaan_0.6-19     psych_2.4.12      scales_1.3.0     \n#&gt; [17] markdown_1.13     knitr_1.49        lubridate_1.9.4   forcats_1.0.0    \n#&gt; [21] stringr_1.5.1     dplyr_1.1.4       purrr_1.0.4       readr_2.1.5      \n#&gt; [25] tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0  \n#&gt; [29] here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] rstudioapi_0.17.1   jsonlite_1.9.0      magrittr_2.0.3     \n#&gt;   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#&gt;   [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n#&gt;  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#&gt;  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#&gt;  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#&gt;  [19] rio_1.2.3           emmeans_1.10.7      zoo_1.8-13         \n#&gt;  [22] igraph_2.1.4        mime_0.12           lifecycle_1.0.4    \n#&gt;  [25] pkgconfig_2.0.3     Matrix_1.7-2        R6_2.6.1           \n#&gt;  [28] fastmap_1.2.0       rbibutils_2.3       shiny_1.10.0       \n#&gt;  [31] numDeriv_2016.8-1.1 digest_0.6.37       OpenMx_2.21.13     \n#&gt;  [34] fdrtool_1.2.18      colorspace_2.1-1    rprojroot_2.0.4    \n#&gt;  [37] Hmisc_5.2-2         timechange_0.3.0    abind_1.4-8        \n#&gt;  [40] compiler_4.4.2      withr_3.0.2         glasso_1.11        \n#&gt;  [43] htmlTable_2.4.3     backports_1.5.0     carData_3.0-5      \n#&gt;  [46] R.utils_2.13.0      ggsignif_0.6.4      corpcor_1.6.10     \n#&gt;  [49] gtools_3.9.5        tools_4.4.2         pbivnorm_0.6.0     \n#&gt;  [52] foreign_0.8-88      zip_2.3.2           httpuv_1.6.15      \n#&gt;  [55] nnet_7.3-20         R.oo_1.27.0         glue_1.8.0         \n#&gt;  [58] quadprog_1.5-8      nlme_3.1-167        promises_1.3.2     \n#&gt;  [61] lisrelToR_0.3       grid_4.4.2          checkmate_2.3.2    \n#&gt;  [64] cluster_2.1.8       reshape2_1.4.4      generics_0.1.3     \n#&gt;  [67] gtable_0.3.6        tzdb_0.4.0          R.methodsS3_1.8.2  \n#&gt;  [70] data.table_1.17.0   hms_1.1.3           xml2_1.3.7         \n#&gt;  [73] car_3.1-3           sem_3.1-16          pillar_1.10.1      \n#&gt;  [76] rockchalk_1.8.157   later_1.4.1         splines_4.4.2      \n#&gt;  [79] lattice_0.22-6      survival_3.8-3      kutils_1.73        \n#&gt;  [82] tidyselect_1.2.1    miniUI_0.1.1.1      pbapply_1.7-2      \n#&gt;  [85] reformulas_0.4.0    svglite_2.1.3       stats4_4.4.2       \n#&gt;  [88] xfun_0.51           qgraph_1.9.8        arm_1.14-4         \n#&gt;  [91] stringi_1.8.4       yaml_2.3.10         pacman_0.5.1       \n#&gt;  [94] boot_1.3-31         evaluate_1.0.3      codetools_0.2-20   \n#&gt;  [97] mi_1.1              cli_3.6.4           RcppParallel_5.1.10\n#&gt; [100] rpart_4.1.24        systemfonts_1.2.1   xtable_1.8-4       \n#&gt; [103] Rdpack_2.6.2        munsell_0.5.1       Rcpp_1.0.14        \n#&gt; [106] coda_0.19-4.1       png_0.1-8           XML_3.99-0.18      \n#&gt; [109] parallel_4.4.2      jpeg_0.1-10         lme4_1.1-36        \n#&gt; [112] mvtnorm_1.3-3       openxlsx_4.2.8      rlang_1.1.5        \n#&gt; [115] multcomp_1.4-28     mnormt_2.1.1\n\n\n\n\n\nMarsh, H., & Alamer, A. (2024). When and how to use set-exploratory structural equation modelling to test structural models: A tutorial using the R package lavaan. British Journal of Mathematical and Statistical Psychology.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>Exploratory structural equation modelling</span>"
    ]
  },
  {
    "objectID": "chapters/sem/14_sem_power.html",
    "href": "chapters/sem/14_sem_power.html",
    "title": "60  Dimensione Campionaria e Analisi della Potenza",
    "section": "",
    "text": "60.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nBuchberger et al. (2024) osservano che, nell’ultimo decennio, la questione della bassa potenza statistica e delle sue conseguenze sull’interpretazione dei risultati scientifici ha attirato un’attenzione crescente nella ricerca psicologica. La potenza statistica rappresenta la capacità di rilevare effetti reali, ovvero la probabilità di individuare correttamente un effetto quando è effettivamente presente. Questo valore dipende strettamente dalla dimensione campionaria, dall’entità dell’effetto che si intende rilevare e dall’affidabilità delle misurazioni.\nLa bassa potenza rappresenta un problema in quanto può limitare l’utilità scientifica dei risultati; inoltre, anche se si rilevano effetti significativi, questi potrebbero non riflettere associazioni reali. Per garantire che gli studi empirici siano adeguatamente progettati per rilevare gli effetti nei campioni esaminati, è cruciale calcolare in anticipo la potenza e stabilire la dimensione campionaria necessaria, evitando studi sotto-dimensionati.\nIn genere, in psicologia, si considera adeguata una potenza intorno a 0.80, anche se alcuni autori suggeriscono di puntare a valori più elevati, come 0.95, per bilanciare meglio gli errori di tipo I e tipo II.\nDiversi autori hanno proposto regole pratiche per stabilire la dimensione campionaria nei modelli SEM, come un numero minimo di osservazioni o un rapporto tra osservazioni e parametri da stimare. Tuttavia, queste regole empiriche possono essere fuorvianti, poiché la potenza nei SEM dipende anche da fattori quali l’entità dei carichi fattoriali e la complessità del modello. Inoltre, la dimensione campionaria necessaria dipende dalla specifica domanda di ricerca.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Dimensione Campionaria e Analisi della Potenza</span>"
    ]
  },
  {
    "objectID": "chapters/sem/14_sem_power.html#obiettivi-principali-dei-modelli-sem",
    "href": "chapters/sem/14_sem_power.html#obiettivi-principali-dei-modelli-sem",
    "title": "60  Dimensione Campionaria e Analisi della Potenza",
    "section": "60.2 Obiettivi Principali dei Modelli SEM",
    "text": "60.2 Obiettivi Principali dei Modelli SEM\nBuchberger et al. (2024) fanno notare che i due obiettivi principali dei SEM sono:\n\nRilevare un effetto specifico, come determinare se la correlazione tra due fattori latenti è diversa da zero.\nConfrontare modelli, verificando quale tra due o più modelli spiega meglio i dati.\n\nQuesti due obiettivi richiedono approcci diversi: il primo riguarda la potenza per rilevare un effetto mirato, mentre il secondo si concentra sulla potenza per individuare errori di specificazione del modello. Nei confronti tra modelli non nidificati (non derivabili l’uno dall’altro tramite restrizioni parametriche), i test basati sul \\(\\chi^2\\) non sono utilizzabili; è quindi necessario ricorrere a metodi alternativi, come le simulazioni Monte Carlo.\nMolti studi SEM mirano a determinare se un parametro specifico differisca da un valore atteso. In tal caso, si confronta un modello in cui il parametro è stimato liberamente con uno in cui è fissato a un valore specifico. Se il confronto tra i modelli suggerisce una differenza significativa, si può concludere che il parametro differisce effettivamente dal valore atteso.\nPer molti ricercatori, è rilevante individuare quale tra modelli teorici concorrenti descriva meglio i dati. Quando i modelli sono nidificati, è possibile usare metodi analitici basati sul \\(\\chi^2\\), mentre per modelli non nidificati si ricorre a tecniche di randomizzazione, come le simulazioni Monte Carlo. Questo approccio consente di stimare la potenza per confrontare modelli non nidificati senza compromettere le assunzioni teoriche del modello.\nIn conclusione, la determinazione della dimensione campionaria e l’analisi di potenza per i SEM richiedono una valutazione accurata della specificità della domanda di ricerca e delle ipotesi del modello, utilizzando approcci sia analitici che di simulazione per ottenere stime affidabili.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Dimensione Campionaria e Analisi della Potenza</span>"
    ]
  },
  {
    "objectID": "chapters/sem/14_sem_power.html#dimensione-campionaria-per-confronti-tra-gruppi",
    "href": "chapters/sem/14_sem_power.html#dimensione-campionaria-per-confronti-tra-gruppi",
    "title": "60  Dimensione Campionaria e Analisi della Potenza",
    "section": "60.3 Dimensione Campionaria per Confronti tra Gruppi",
    "text": "60.3 Dimensione Campionaria per Confronti tra Gruppi\nPer introdurre il metodo della simulazione Monte Carlo applicato all’analisi di potenza nei modelli SEM, seguiremo il tutorial di Chen & Yung (2023). Prima di esplorare l’uso della simulazione Monte Carlo per stimare la potenza in contesti SEM, è utile applicare questo metodo a un caso più semplice: il calcolo della dimensione campionaria necessaria per rilevare una differenza clinicamente rilevante tra due gruppi. Questo esempio introduttivo aiuterà a chiarire il funzionamento della simulazione Monte Carlo. Successivamente, applicheremo lo stesso approccio alla determinazione della potenza per i modelli SEM, un caso più complesso che richiede tecniche avanzate di simulazione per stimare accuratamente la potenza.\n\n60.3.1 Formula per la Dimensione Campionaria per Gruppo\nLa formula per determinare la dimensione campionaria per ciascun gruppo, per rilevare una differenza clinicamente significativa tra due gruppi, è:\n\\[\nn \\geq 2 \\left(\\frac{s^2}{\\delta^2}\\right) \\left[z_{1 - \\alpha/2} + z_{1 - \\beta}\\right]^2,\n\\tag{60.1}\\]\ndove:\n\n\\(s\\) è la stima della deviazione standard (assumendo varianze omogenee tra i gruppi);\n\\(\\delta\\) è la differenza clinicamente rilevante tra i gruppi, cioè l’effetto minimo che si desidera rilevare;\n\\(z_{1 - \\alpha/2}\\) e \\(z_{1 - \\beta}\\) sono i valori critici della distribuzione normale standard per i limiti degli errori di tipo I e tipo II.\n\n\n\n60.3.2 Derivazione\nPer calcolare la dimensione campionaria necessaria a confrontare le medie di due gruppi indipendenti, seguiamo i passaggi teorici necessari per derivare la formula finale. Il nostro interesse è quantificare la differenza tra le medie dei due gruppi, indicata come \\(\\mu_1 - \\mu_2\\). Supponiamo che le due medie siano stimate da campioni con deviazione standard comune \\(\\sigma\\).\nPoiché ciascuna media di campione (\\(\\bar{X}_1\\) per il gruppo 1 e \\(\\bar{X}_2\\) per il gruppo 2) ha varianza \\(\\frac{\\sigma^2}{n}\\) e i campioni sono indipendenti, la varianza della differenza \\(\\bar{X}_1 - \\bar{X}_2\\) è data dalla somma delle varianze:\n\\[\n\\text{Var}(\\bar{X}_1 - \\bar{X}_2) = \\frac{\\sigma^2}{n} + \\frac{\\sigma^2}{n} = \\frac{2\\sigma^2}{n}.\n\\]\nDa qui, la deviazione standard della differenza tra le medie è quindi:\n\\[\n\\text{Deviazione standard} = \\sqrt{\\frac{2\\sigma^2}{n}}.\n\\]\nPer il teorema del limite centrale, questa differenza tra le medie segue una distribuzione normale, dato che \\(n\\) è sufficientemente grande.\nPer determinare la potenza del test e la dimensione campionaria necessaria, definiamo:\n\nIpotesi nulla \\(H_0\\): \\(\\mu_1 - \\mu_2 = 0\\) (assenza di differenza significativa).\nIpotesi alternativa \\(H_1\\): \\(\\mu_1 - \\mu_2 = \\delta\\), dove \\(\\delta\\) rappresenta una differenza significativa che vogliamo rilevare.\nLivello di significatività \\(\\alpha\\): probabilità di rifiutare \\(H_0\\) quando è vera (errore di Tipo I).\nPotenza desiderata \\(1-\\beta\\): probabilità di rilevare una vera differenza \\(\\delta\\) (cioè non commettere un errore di Tipo II).\n\nSotto l’ipotesi nulla, la differenza tra le medie campionarie, \\(\\bar{X}_1 - \\bar{X}_2\\), ha media zero e deviazione standard \\(\\sqrt{\\frac{2\\sigma^2}{n}}\\). Possiamo quindi utilizzare la statistica \\(Z\\) per standardizzare questa differenza:\n\\[\nZ = \\frac{(\\bar{X}_1 - \\bar{X}_2) - 0}{\\sqrt{\\frac{2\\sigma^2}{n}}}.\n\\]\nIn un test a due code con livello di significatività \\(\\alpha\\), rifiutiamo \\(H_0\\) se il valore assoluto di \\(Z\\) supera \\(z_{1-\\alpha/2}\\), ovvero se:\n\\[\n|\\bar{X}_1 - \\bar{X}_2| &gt; z_{1-\\alpha/2} \\cdot \\sqrt{\\frac{2\\sigma^2}{n}}.\n\\]\nQuesto ci dice che il valore critico per rifiutare \\(H_0\\) è pari a \\(z_{1-\\alpha/2} \\cdot \\sqrt{\\frac{2\\sigma^2}{n}}\\). Tuttavia, poiché vogliamo assicurare una potenza del test pari a \\(1-\\beta\\), la differenza \\(\\delta\\) deve superare questo valore critico con probabilità \\(1 - \\beta\\).\nSotto l’ipotesi alternativa \\(H_1\\), la differenza attesa tra le medie campionarie è \\(\\delta\\), e quindi standardizziamo rispetto alla deviazione standard della differenza:\n\\[\nZ = \\frac{\\delta}{\\sqrt{\\frac{2\\sigma^2}{n}}}.\n\\]\nImponiamo ora che la somma dei valori critici \\(z_{1-\\alpha/2}\\) e \\(z_{1-\\beta}\\) sia uguale a questa statistica test, ottenendo:\n\\[\n\\delta = \\sqrt{\\frac{2\\sigma^2}{n}} (z_{1-\\alpha/2} + z_{1-\\beta}).\n\\]\nRisolviamo questa equazione per \\(n\\) al fine di ottenere la dimensione campionaria necessaria:\n\\[\nn \\geq \\frac{2\\sigma^2}{\\delta^2} (z_{1-\\alpha/2} + z_{1-\\beta})^2.\n\\]\nNella pratica, non sempre conosciamo \\(\\sigma^2\\); pertanto, lo sostituiamo con la stima \\(s^2\\), ottenendo:\n\\[\nn \\geq 2 \\left(\\frac{s^2}{\\delta^2}\\right) [z_{1-\\alpha/2} + z_{1-\\beta}]^2.\n\\]\nQuesta formula fornisce un modo pratico per calcolare la dimensione campionaria minima necessaria per garantire che il test abbia la potenza desiderata, tenendo conto della variabilità dei dati (\\(s^2\\)), dell’errore di Tipo I (\\(\\alpha\\)), dell’errore di Tipo II (\\(\\beta\\)), e della differenza minima rilevante \\(\\delta\\).\nIn conclusione, questa derivazione della dimensione campionaria è utile poiché:\n\nConsente di specificare il livello di controllo sugli errori di Tipo I e Tipo II.\nTiene conto della variabilità campionaria attraverso la stima \\(s^2\\).\nPermette di impostare una differenza minima rilevante da rilevare tra i gruppi.\nFornisce una stima della dimensione campionaria necessaria per garantire la potenza statistica richiesta.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Dimensione Campionaria e Analisi della Potenza</span>"
    ]
  },
  {
    "objectID": "chapters/sem/14_sem_power.html#esempio-pratico",
    "href": "chapters/sem/14_sem_power.html#esempio-pratico",
    "title": "60  Dimensione Campionaria e Analisi della Potenza",
    "section": "60.4 Esempio Pratico",
    "text": "60.4 Esempio Pratico\nConsideriamo i dati seguenti, ponendo \\(\\alpha\\) = 0.05 e la potenza \\(1-\\beta\\) = 0.80:\n\nmu2 &lt;- 1.2\nmu1 &lt;- 1\nsd &lt;- 0.5 # SD of each group\n\nalpha &lt;- 0.05\nbeta &lt;- 0.2\n\nCalcoliamo la dimensione minima campionaria per ciascun gruppo:\n\n# Mean difference\ndelta &lt;- mu2 - mu1\n\n# Required sample size\nn &lt;- 2 * sd^2 / delta^2 * (qnorm(1 - alpha / 2) + qnorm(1 - beta))^2\nn\n\n98.1109966793636",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Dimensione Campionaria e Analisi della Potenza</span>"
    ]
  },
  {
    "objectID": "chapters/sem/14_sem_power.html#collegamento-con-la-modellazione-a-equazioni-strutturali",
    "href": "chapters/sem/14_sem_power.html#collegamento-con-la-modellazione-a-equazioni-strutturali",
    "title": "60  Dimensione Campionaria e Analisi della Potenza",
    "section": "60.5 Collegamento con la Modellazione a Equazioni Strutturali",
    "text": "60.5 Collegamento con la Modellazione a Equazioni Strutturali\nIl confronto tra due gruppi è un caso relativamente semplice rispetto alla complessità dei modelli a equazioni strutturali (SEM), i quali spesso richiedono approcci avanzati per il calcolo della potenza, come il Monte Carlo Simulation-Based Approach (MCSB). Per comprendere meglio questo metodo, è utile applicare inizialmente il MCSB a un caso base come il confronto tra le medie di due gruppi. Successivamente, estenderemo questo approccio per includere i SEM, dove la complessità delle relazioni e dei parametri richiede strumenti di simulazione più sofisticati.\n\n60.5.1 Applicazione del Metodo MCSB\nPer implementare il metodo MCSB, dobbiamo definire due componenti principali:\n\nIl modello generativo dei dati, che descrive i parametri noti e serve per simulare i dati, come le differenze attese tra i gruppi e la variabilità delle misure.\nIl modello di stima dei parametri, che specifica come i parametri saranno stimati dai dati simulati, utilizzando un software di modellazione SEM come lavaan o simsem.\n\n\n\n60.5.2 Riformulazione del Confronto tra Trattamenti come Modello di Regressione\nPossiamo rappresentare il confronto tra gruppi anche come modello di regressione lineare:\n\\[\ny = \\beta_0 + \\beta_1 \\cdot TRT + \\epsilon,\n\\]\ndove:\n\n\\(y\\) è la variabile di esito,\n\\(TRT\\) è una variabile indicatrice che differenzia i gruppi (1 per il gruppo trattato, 0 per il gruppo di controllo),\n\\(\\beta_0\\) rappresenta la media del gruppo di controllo,\n\\(\\beta_1\\) rappresenta la differenza media tra i gruppi,\n\\(\\epsilon\\) è il termine di errore, distribuito normalmente con media zero.\n\n\n\n60.5.3 Implementazione in R dell’Approccio MCSB con simsem\nPer calcolare la potenza e stimare correttamente i parametri, è necessario determinare la dimensione dell’effetto e la varianza residua dell’esito \\(y\\) in questa equazione. Di seguito, i passaggi per implementare il metodo MCSB:\n\nCalcolo del \\(d\\) di Cohen:\nSupponiamo di avere una differenza clinicamente rilevante \\(\\delta = 0.2\\) e una deviazione standard \\(sd = 0.5\\). La dimensione dell’effetto standardizzata, o \\(d\\) di Cohen, è calcolata come:\n\\[\nd = \\frac{\\delta}{sd} = \\frac{0.2}{0.5} = 0.4.\n\\]\nConversione di \\(d\\) in Coefficiente di Correlazione \\(r\\):\nSpesso è utile esprimere la dimensione dell’effetto in termini di coefficiente di correlazione \\(r\\), che riflette la forza dell’associazione tra trattamento e esito. Il pacchetto compute.es in R include la funzione des() per convertire \\(d\\) in un valore di correlazione equivalente \\(r\\).\nUsando des(d, n.1 = n, n.2 = n), otteniamo un valore approssimativo di \\(r \\approx 0.2\\).\nCalcolo della Varianza di \\(y\\):\nIn questo contesto, il coefficiente \\(r = 0.2\\) rappresenta l’associazione tra il trattamento (TRT) e l’esito \\(y\\). La varianza totale di \\(y\\) può essere separata in una parte spiegata (dall’effetto di \\(TRT\\)) e una parte non spiegata. La varianza residua di \\(y\\), dopo aver tenuto conto del predittore, è data da \\(1 - r^2\\), poiché \\(r^2\\) rappresenta la proporzione della varianza di \\(y\\) spiegata dal trattamento:\n\\[\n\\text{var}(y) = 1 - r^2.\n\\]\nSostituendo \\(r = 0.2\\):\n\\[\n\\text{var}(y) = 1 - (0.2)^2 = 1 - 0.04 = 0.96.\n\\]\n\nQuesti passaggi possono essere implementati in R utilizzando il pacchetto compute.es, facilitando il calcolo e la configurazione della simulazione per l’analisi di potenza con simsem.\n\n# Calcolo dell'ES in termini di d\nd &lt;- delta / sd\n# print(d) # E.g., [1] 0.4\n\n# Conversione in altre misure di ES\nd2ES &lt;- des(d, n.1 = n, n.2 = n, verbose = FALSE)\n\n# Estrazione di r\nr &lt;- d2ES$r\nprint(r) # E.g., [1] 0.2\n\n# Calcolo della varianza di y\nvar.y &lt;- 1 - r**2\nprint(var.y) # E.g., [1] 0.96\n\n[1] 0.2\n[1] 0.96\n\n\nPer utilizzare simsem, è necessario specificare il processo di generazione dei dati. Per il caso presente abbiamo:\n\n# Modello di generazione dei dati\ndatMod &lt;- \"\n    # Regressione con correlazione nota di 0.2\n    y ~ 0.2*TRT\n    # Varianza dell'errore\n    y ~~ 0.96*y\n\"\n\nIl valore \\(0.2\\) in y ~ 0.2*TRT rappresenta tecnicamente un coefficiente di regressione. Tuttavia, se le variabili sono standardizzate, questo valore coincide con la correlazione tra la variabile predittore \\(TRT\\) e la variabile di risposta \\(y\\).\nIn questo contesto, considerare il coefficiente di correlazione è utile poiché ci permette di calcolare facilmente la varianza residua di \\(y\\). Conoscendo \\(r\\), infatti, possiamo stabilire la varianza residua come \\(1 - r^2\\), semplificando la specifica del modello.\nIl modello di stima è il seguente:\n\n# Modello di stima\nestMod &lt;- \"\n    # Regressione\n    y ~ TRT\n\"\n\nPossiamo ora eseguire la simulazione MCSB:\n\nsimOut &lt;- sim(\n    nRep = 1000, \n    generate = datMod, \n    model = estMod, \n    n = 198,\n    lavaanfun = \"sem\", \n    seed = 123, \n    silent = TRUE\n)\n\nNella simulazione eseguita con simOut, il parametro nRep indica il numero di repliche, che è stato impostato a 1000 per ridurre i tempi di esecuzione. Idealmente, sarebbe preferibile un numero più elevato (ad esempio &gt;10.000) per ottenere risultati più precisi. Il parametro generate specifica il modello di generazione dei dati (qui datMod), mentre model collega al modello di stima (qui estMod) utilizzando la funzione lavaanfun = \"sem\". La dimensione del campione, n, è impostata a 198 per il calcolo della potenza statistica, e seed consente di riprodurre la simulazione con gli stessi risultati. L’opzione silent = TRUE sopprime l’output intermedio per semplificare la visualizzazione.\nEsaminiamo l’output della simulazione.\n\nsummaryParam(simOut)\n\n\nA data.frame: 2 x 10\n\n\n\nEstimate Average\nEstimate SD\nAverage SE\nPower (Not equal 0)\nStd Est\nStd Est SD\nStd Ave SE\nAverage Param\nAverage Bias\nCoverage\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\ny~TRT\n0.202\n0.0693\n0.0698\n0.816\n0.201\n0.0675\n0.0671\n0.20\n0.00219\n0.953\n\n\ny~~y\n0.956\n0.0969\n0.0960\n1.000\n0.955\n0.0271\n0.0267\n0.96\n-0.00433\n0.942\n\n\n\n\n\nL’output mostra diverse colonne relative ai parametri stimati con due righe: la prima riga è per lo stimatore della correlazione tra \\(y\\) e TRT (cioè il coefficiente di regressione), mentre la seconda riga si riferisce alla varianza residua di \\(y\\) in y ~~ y.\n\nEstimate Average: È la media delle stime dei parametri nelle 1000 repliche. I valori ottenuti (0.202 per y ~ TRT e 0.956 per y ~~ y) sono molto vicini ai valori simulati, cioè \\(r = 0.2\\) e \\(\\text{var}(y) = 0.96\\), indicando che le stime sono coerenti con i parametri di partenza.\nEstimate SD: È la deviazione standard delle stime dei parametri tra le repliche. Rappresenta la variabilità delle stime nei vari set di dati simulati, e aiuta a capire la precisione delle stime.\nAverage SE: È la media degli errori standard delle stime in tutte le repliche, un’indicazione della variabilità stimata del parametro per ogni replica.\nPower (Not equal 0): Questa colonna indica la proporzione di repliche in cui i parametri sono risultati significativamente diversi da zero. Qui la potenza statistica per y ~ TRT è 0.816, vicina al valore target di 0.80. La lieve differenza potrebbe essere ridotta aumentando nRep.\nAverage Param: Rappresenta i valori medi dei parametri simulati. In questo caso, sono i valori di partenza utilizzati nel modello di generazione dei dati, cioè 0.20 per y ~ TRT e 0.96 per y ~~ y.\nAverage Bias: È la differenza tra i valori medi stimati e i parametri di partenza. Nel nostro caso, il bias è molto piccolo, indicando che le stime sono ben centrate attorno ai parametri di partenza.\nCoverage: Indica la percentuale di intervalli di confidenza che contengono i valori veri dei parametri. In generale, ci si aspetta una copertura intorno al 95% per un intervallo di confidenza al 95%, qui abbiamo coperture di 95.3% e 94.2%, il che è in linea con le aspettative.\n\nIn sintesi, i risultati della simulazione mostrano stime accurate e, con una numerosità campionaria totale di \\(n = 198\\), raggiungono una potenza statistica vicina all’obiettivo di 0.80, accompagnata da un bias minimo e una buona copertura.\nIn altre parole, questa dimostrazione evidenzia come la tecnica di simulazione MCSB riesca, nel caso del confronto tra le medie di due gruppi, a collegare la dimensione del campione alla potenza del test, tenendo conto dell’ampiezza dell’effetto e della variabilità campionaria, in linea con le aspettative teoriche.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Dimensione Campionaria e Analisi della Potenza</span>"
    ]
  },
  {
    "objectID": "chapters/sem/14_sem_power.html#analisi-di-potenza-nei-modelli-sem",
    "href": "chapters/sem/14_sem_power.html#analisi-di-potenza-nei-modelli-sem",
    "title": "60  Dimensione Campionaria e Analisi della Potenza",
    "section": "60.6 Analisi di Potenza nei Modelli SEM",
    "text": "60.6 Analisi di Potenza nei Modelli SEM\nDopo aver compreso l’approccio MCSB e la sua implementazione in simsem, possiamo ora illustrare come calcolare la potenza per modelli di equazioni strutturali (SEM) generali. Un esempio è offerto da Chen & Yung (2023), che utilizzano un modello di crescita latente (LGC) per analizzare i dati di Byrne (2013).\n\nIl dataset raccoglie i dati longitudinali raccolti da 405 donne di Hong Kong, sottoposte a valutazione post-chirurgica per il tumore al seno (Byrne, 2013).\n\nI dati, disponibili nel file hkcancer_red2.dat, includono 10 variabili:\n\nID: identificatore di ciascuna partecipante.\nMOOD1, MOOD4, MOOD8: valutazioni soggettive dello stato d’animo a 1, 4 e 8 mesi dall’intervento; punteggi più alti indicano un umore peggiore.\nSOCADJ1, SOCADJ4, SOCADJ8: misure di adattamento sociale a 1, 4 e 8 mesi; punteggi più alti indicano un miglior adattamento sociale.\nAge: età della partecipante al momento dell’intervento.\nAgeGrp: categoria di età (dichotomizzata) con ‘Younger’ (&lt; 50 anni) e ‘Older’ (&gt; 50 anni).\nSurgTX: tipo di intervento chirurgico, distinguendo tra lumpectomia e mastectomia.\n\n\n60.6.1 Modello di Crescita Latente (LGC)\nIl modello di crescita latente consente di modellare le traiettorie di crescita nel tempo e di confrontare differenze tra gruppi (ad esempio, tra gruppi di età o di intervento). Questo modello permette di analizzare sia le variazioni intra-individuali che quelle inter-individuali nel contesto delle traiettorie longitudinali. Nel caso del dataset sul tumore al seno, possiamo modellare sia il cambiamento longitudinale dello stato d’animo (MOOD) che dell’adattamento sociale (SOCADJ) lungo un periodo di 8 mesi, includendo le variazioni tra le partecipanti nelle loro traiettorie.\nChen & Yung (2023) affrontano la questione della determinazione della dimensione campionaria necessaria per rilevare un effetto significativo su “MOOD” con una potenza statistica pari a 0.80.\n\n\n60.6.2 Calcolo della Dimensione Campionaria\nPer determinare la dimensione campionaria necessaria a rilevare l’effetto dell’intervento chirurgico su “MOOD”, è necessario specificare il modello generativo dei dati, che comprende tutti i parametri del modello. Questi parametri descrivono il cambiamento atteso dello stato d’animo nel tempo in relazione al tipo di intervento.\nPrima di procedere con l’analisi di potenza, carichiamo i dati in R per adattare il modello in base alle osservazioni empiriche. Ecco come eseguire il caricamento dei dati:\n\ndCancer &lt;- rio::import(here::here(\"data\", \"hkcancer_red2.dat\"))\n\n\n# Replace all \"*\" with NA in dCancer\ndCancer[dCancer == \"*\"] &lt;- NA\nhead(dCancer)\n\n\nA data.frame: 6 x 11\n\n\n\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\nV10\nV11\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n3\n2\n15.000000\nNA\nNA\n95.906250\nNA\nNA\n70\n1\n1\n\n\n2\n2\n3\n16.000000\n25.000000\n22.000000\n114.888889\n105.111111\n90.444444\n47\n0\n1\n\n\n3\n2\n2\n37.000000\n26.000000\n25.000000\n80.666667\n95.333333\n95.333333\n47\n0\n1\n\n\n4\n2\n2\n19.000000\n16.000000\n15.000000\n112.838710\n108.580645\n99.000000\n52\n1\n1\n\n\n5\n1\n2\n13.000000\n16.000000\n14.000000\n115.000000\n105.000000\n101.000000\n43\n0\n1\n\n\n6\n1\n1\n21.000000\n28.000000\n19.000000\n106.451613\n114.967742\n107.516129\n34\n0\n0\n\n\n\n\n\n\nvar_names &lt;- c(\n\"X1\", \"X2\", \"MOOD1\", \"MOOD4\", \"MOOD8\", \"SOCADJ1\", \"SOCADJ4\", \"SOCADJ8\", \"Age\",  \n\"AgeGrp\", \"SurgTx\")\n\nnames(dCancer) &lt;- var_names\nhead(dCancer)\n\n\nA data.frame: 6 x 11\n\n\n\nX1\nX2\nMOOD1\nMOOD4\nMOOD8\nSOCADJ1\nSOCADJ4\nSOCADJ8\nAge\nAgeGrp\nSurgTx\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n3\n2\n15.000000\nNA\nNA\n95.906250\nNA\nNA\n70\n1\n1\n\n\n2\n2\n3\n16.000000\n25.000000\n22.000000\n114.888889\n105.111111\n90.444444\n47\n0\n1\n\n\n3\n2\n2\n37.000000\n26.000000\n25.000000\n80.666667\n95.333333\n95.333333\n47\n0\n1\n\n\n4\n2\n2\n19.000000\n16.000000\n15.000000\n112.838710\n108.580645\n99.000000\n52\n1\n1\n\n\n5\n1\n2\n13.000000\n16.000000\n14.000000\n115.000000\n105.000000\n101.000000\n43\n0\n1\n\n\n6\n1\n1\n21.000000\n28.000000\n19.000000\n106.451613\n114.967742\n107.516129\n34\n0\n0\n\n\n\n\n\nQuesta preparazione ci consentirà di configurare il modello LGC con simsem, impostando i parametri di interesse in modo da simulare la potenza con il metodo MCSB e verificare la capacità del modello di rilevare gli effetti desiderati su “MOOD”.\nDefiniamo il modello a crescita latente.\n\nmod4MOOD &lt;- \"\n    # Intercept and Slope with fixed-coefficients\n    iMOOD =~ 1*MOOD1 + 1*MOOD4 + 1*MOOD8\n    sMOOD =~ 0*MOOD1 + 1*MOOD4 + 2.33*MOOD8\n    \n    # Regression with a labeled for simulation\n    iMOOD ~ SurgTx\n    sMOOD ~ a*SurgTx\n\"\n\nQuesto modello di crescita latente è utilizzato per analizzare l’evoluzione dello stato d’animo (MOOD) di un gruppo di persone a tre momenti distinti nel tempo (1, 4 e 8 mesi dopo l’intervento chirurgico).\nIl modello ha due componenti principali: intercetta e pendenza. Queste rappresentano rispettivamente il valore iniziale e la velocità di cambiamento dello stato d’animo nel tempo.\n\nIntercetta (iMOOD): l’intercetta rappresenta il punto di partenza o il livello iniziale dello stato d’animo (MOOD). Nel modello, è definita come:\niMOOD =~ 1*MOOD1 + 1*MOOD4 + 1*MOOD8\nQui, iMOOD è una variabile latente che cattura il livello medio dello stato d’animo in ciascun momento, usando coefficienti fissati a 1 per indicare che ogni misura contribuisce allo stesso modo alla stima dell’intercetta. Questo implica che l’intercetta è il valore di base dello stato d’animo comune a tutti i partecipanti nei tre punti temporali.\nPendenza (sMOOD): la pendenza rappresenta la velocità e la direzione del cambiamento dello stato d’animo nel tempo.\nsMOOD =~ 0*MOOD1 + 1*MOOD4 + 2.33*MOOD8\nQui, sMOOD è una variabile latente che descrive come cambia lo stato d’animo nei diversi momenti temporali. I coefficienti (0, 1, 2.33) riflettono l’intervallo di tempo tra le misurazioni (ad esempio, da 1 a 8 mesi). Il coefficiente 2.33 per MOOD8 indica che l’effetto temporale si accumula, essendo il punto finale del periodo di osservazione.\n\nIl modello include anche delle regressioni che collegano l’intercetta e la pendenza a una variabile predittiva, il tipo di intervento chirurgico (SurgTx), che distingue tra due gruppi (lumpectomia e mastectomia).\n\niMOOD ~ SurgTx: questa regressione rappresenta l’effetto del tipo di intervento sul livello iniziale dello stato d’animo. In altre parole, cerca di vedere se esiste una differenza nello stato d’animo iniziale in base al tipo di intervento ricevuto.\nsMOOD ~ a*SurgTx: questa regressione, con un coefficiente etichettato a, rappresenta l’effetto del tipo di intervento sul tasso di cambiamento dello stato d’animo nel tempo. Il coefficiente a mostra se il tipo di intervento influisce sulla velocità di miglioramento o peggioramento dello stato d’animo lungo i mesi.\n\nIn sintesi, questo modello di crescita latente analizza sia il livello iniziale dello stato d’animo (intercetta) sia il cambiamento nel tempo (pendenza) e verifica se il tipo di intervento chirurgico influenza questi due aspetti.\nAdattiamo il modello ai dati.\n\n# Call growth function to fit the LGC\nfitMOOD &lt;- growth(mod4MOOD,\n    data = dCancer, \n    estimator = \"MLR\",\n    missing = \"fiml\"\n)\n\n\nsummary(fitMOOD) |&gt; print()\n\nlavaan 0.6-19 ended normally after 75 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n\n  Number of observations                           405\n  Number of missing patterns                         8\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                 2.022       2.077\n  Degrees of freedom                                 2           2\n  P-value (Chi-square)                           0.364       0.354\n  Scaling correction factor                                  0.974\n    Yuan-Bentler correction (Mplus variant)                       \n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  iMOOD =~                                            \n    MOOD1             1.000                           \n    MOOD4             1.000                           \n    MOOD8             1.000                           \n  sMOOD =~                                            \n    MOOD1             0.000                           \n    MOOD4             1.000                           \n    MOOD8             2.330                           \n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  iMOOD ~                                             \n    SurgTx           -0.118    0.758   -0.155    0.876\n  sMOOD ~                                             \n    SurgTx     (a)   -0.282    0.332   -0.850    0.395\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n .iMOOD ~~                                            \n   .sMOOD            -2.650    1.581   -1.676    0.094\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .iMOOD            10.550    0.671   15.715    0.000\n   .sMOOD            -0.389    0.298   -1.304    0.192\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .MOOD1            13.492    3.384    3.987    0.000\n   .MOOD4            18.792    2.415    7.782    0.000\n   .MOOD8             5.398    3.880    1.391    0.164\n   .iMOOD            25.827    3.574    7.226    0.000\n   .sMOOD             2.563    1.259    2.036    0.042\n\n\n\n\nsemPlot::semPaths(fitMOOD,\n    what = \"col\", \n    whatLabels = \"par\", \n    nCharNodes = 8,\n    shapeMan = \"rectangle\", \n    sizeMan = 8, \n    sizeMan2 = 7\n)\n\n\n\n\n\n\n\n\nUna volta ottenute le stime dei parametri del modello, possiamo utilizzarle per costruire la componente di generazione dei dati come segue (nel codice seguente, utilizzo i valori usati da Chen & Yung (2023) per riprodurre il loro risultato).\n\ndat.mod4MOOD &lt;- \"\n    # Intercept and Slope with MOOD\n    iMOOD =~ 1*MOOD1 + 1*MOOD4 + 1*MOOD8\n    sMOOD =~ 0*MOOD1 + 1*MOOD4 + 2.33*MOOD8\n    # residual variances for observed\n    MOOD1 ~~ 14.307*MOOD1\n    MOOD4 ~~ 18.637*MOOD4\n    MOOD8 ~~ 6.745*MOOD8\n    # Regression paths to covariates\n    iMOOD ~ (-0.116)*SurgTx\n    sMOOD ~ a*SurgTx + (-0.332)*SurgTx\n    # latent Intercepts\n    iMOOD ~ 21.683*1\n    sMOOD ~ 0.004*1\n    # latent variances/coVariances\n    iMOOD ~~ 25.826*iMOOD\n    sMOOD ~~ 2.272*sMOOD\n    iMOOD ~~ (-2.135)*sMOOD\n    # mean and variance for SurgTX\n    SurgTx ~ 0.5*1\n    SurgTx ~~ 0.25*SurgTx\n\"\n\nPossiamo ora usare la funzione sim() di simsem. Seguendo il tutorial di Chen & Yung (2023), fissiamo la dimensione campionaria complesiva a n = 405:\n\nsimOut1 &lt;- sim(\n    nRep = 1000, \n    generate = dat.mod4MOOD,\n    model = mod4MOOD, \n    n = 405, \n    lavaanfun = \"growth\",\n    seed = 123, \n    silent = TRUE\n)\n\n\n# simulation output\nsummary(simOut1)\n\nRESULT OBJECT\nModel Type\n[1] \"lavaan\"\n========= Fit Indices Cutoffs ============\n           Alpha\nFit Indices      0.1     0.05     0.01    0.001     Mean     SD\n      chisq    4.455    5.665    8.871   11.378    1.980  1.888\n      aic   7571.694 7588.105 7624.726 7667.762 7508.592 49.361\n      bic   7611.733 7628.144 7664.765 7707.801 7548.631 49.361\n      rmsea    0.055    0.067    0.092    0.108    0.016  0.025\n      cfi      0.994    0.991    0.983    0.975    0.998  0.004\n      tli      0.982    0.972    0.948    0.926    1.000  0.014\n      srmr     0.018    0.020    0.026    0.028    0.011  0.005\n========= Parameter Estimates and Standard Errors ============\n             Estimate Average Estimate SD Average SE Power (Not equal 0)\niMOOD~SurgTx           -0.088       0.614      0.607               0.056\na                      -0.337       0.244      0.242               0.287\nMOOD1~~MOOD1           14.312       2.882      2.816               1.000\nMOOD4~~MOOD4           18.563       1.831      1.846               1.000\nMOOD8~~MOOD8            6.843       3.477      3.401               0.536\niMOOD~~iMOOD           25.691       3.168      3.189               1.000\nsMOOD~~sMOOD            2.211       1.124      1.100               0.512\niMOOD~~sMOOD           -2.072       1.278      1.284               0.359\niMOOD~1                21.664       0.440      0.429               1.000\nsMOOD~1                 0.007       0.167      0.171               0.043\n             Std Est Std Est SD Std Ave SE Average Param Average Bias\niMOOD~SurgTx  -0.009      0.061      0.060        -0.116        0.028\na             -0.126      0.129      0.195        -0.332       -0.005\nMOOD1~~MOOD1   0.357      0.067      0.066        14.307        0.005\nMOOD4~~MOOD4   0.437      0.031      0.031        18.637       -0.074\nMOOD8~~MOOD8   0.195      0.099      0.096         6.745        0.098\niMOOD~~iMOOD   0.996      0.005      0.006        25.826       -0.135\nsMOOD~~sMOOD   0.968      0.147      0.311         2.272       -0.061\niMOOD~~sMOOD  -0.272      0.459     13.990        -2.135        0.063\niMOOD~1        4.291      0.282      0.282        21.683       -0.019\nsMOOD~1        0.005      0.146      0.194         0.004        0.003\n             Coverage\niMOOD~SurgTx    0.949\na               0.955\nMOOD1~~MOOD1    0.946\nMOOD4~~MOOD4    0.947\nMOOD8~~MOOD8    0.949\niMOOD~~iMOOD    0.956\nsMOOD~~sMOOD    0.940\niMOOD~~sMOOD    0.949\niMOOD~1         0.938\nsMOOD~1         0.956\n========= Correlation between Fit Indices ============\n       chisq    aic    bic  rmsea    cfi    tli   srmr\nchisq  1.000 -0.012 -0.012  0.955 -0.939 -0.995  0.957\naic   -0.012  1.000  1.000 -0.002 -0.014  0.016 -0.040\nbic   -0.012  1.000  1.000 -0.002 -0.014  0.016 -0.040\nrmsea  0.955 -0.002 -0.002  1.000 -0.932 -0.949  0.890\ncfi   -0.939 -0.014 -0.014 -0.932  1.000  0.941 -0.818\ntli   -0.995  0.016  0.016 -0.949  0.941  1.000 -0.957\nsrmr   0.957 -0.040 -0.040  0.890 -0.818 -0.957  1.000\n================== Replications =====================\nNumber of replications = 1000 \nNumber of converged replications = 947 \nNumber of nonconverged replications: \n   1. Nonconvergent Results = 0 \n   2. Nonconvergent results from multiple imputation = 0 \n   3. At least one SE were negative or NA = 0 \n   4. Nonpositive-definite latent or observed (residual) covariance matrix \n      (e.g., Heywood case or linear dependency) = 53 \n\n\nPossiamo osservare che la potenza associata al parametro \\(a\\) (cioè, l’effetto dell’intervento chirurgico) è 0.287, il che non sorprende, dato che sapevamo già che questo parametro non è statisticamente significativo con la dimensione campionaria attuale di 405.\nLa domanda successiva è quindi quale dimensione campionaria sia necessaria per ottenere una potenza pari a 0.80. A questo fine, è necessario applicare l’approccio MCSB per una serie di dimensioni campionarie, in modo da calcolare le rispettive potenze.\nChen & Yung (2023) costruiscono una curva che mostra la relazione tra dimensione campionaria e potenza, così da identificare la dimensione campionaria necessaria per raggiungere una potenza di 0.80. Per questo scopo, Chen & Yung (2023) utilizzano n = rep(seq(400, 2000, by = 200), 500), per eseguire l’MCSB su una sequenza di dimensioni campionarie da 400 a 2000, con incrementi di 200 (ovvero: 400, 600, 800, 1000, 1200, 1400, 1600, 1800, e 2000), ciascuna delle quali sarà utilizzata per 500 simulazioni.\n\n# Simulation for sequential sample size\nsimAll &lt;- sim(\n    nRep = NULL, \n    generate = dat.mod4MOOD,\n    model = mod4MOOD, \n    n = rep(seq(400, 2000, 200), 500),\n    lavaanfun = \"growth\", \n    seed = 123, \n    silent = TRUE\n    ) \n\n\n# Print the simulations\nsummary(simAll)\n\nRESULT OBJECT\nModel Type\n[1] \"lavaan\"\n========= Fit Indices Cutoffs ============\n     N chisq   aic   bic rmsea   cfi   tli  srmr\n1  400  6.19  7503  7545 0.061 0.993 0.978 0.019\n2  800  6.04 14933 14979 0.053 0.994 0.983 0.016\n3 1200  5.88 22363 22413 0.044 0.996 0.987 0.013\n4 1600  5.73 29793 29847 0.036 0.997 0.992 0.011\n5 2000  5.57 37223 37281 0.027 0.999 0.996 0.008\n========= Parameter Estimates and Standard Errors ============\n             Estimate Average Estimate SD Average SE Power (Not equal 0)\niMOOD~SurgTx           -0.122       0.400      0.387               0.066\na                      -0.327       0.163      0.154               0.607\nMOOD1~~MOOD1           14.298       1.831      1.794               1.000\nMOOD4~~MOOD4           18.662       1.230      1.178               1.000\nMOOD8~~MOOD8            6.700       2.223      2.163               0.855\niMOOD~~iMOOD           25.761       2.094      2.033               1.000\nsMOOD~~sMOOD            2.265       0.722      0.700               0.877\niMOOD~~sMOOD           -2.130       0.855      0.818               0.761\niMOOD~1                21.687       0.284      0.273               1.000\nsMOOD~1                 0.002       0.116      0.109               0.058\n             Std Est Std Est SD Std Ave SE Average Param Average Bias\niMOOD~SurgTx  -0.012      0.039      0.038        -0.116       -0.006\na             -0.113      0.079      0.329        -0.332        0.005\nMOOD1~~MOOD1   0.357      0.043      0.042        14.307       -0.009\nMOOD4~~MOOD4   0.439      0.020      0.020        18.637        0.025\nMOOD8~~MOOD8   0.191      0.063      0.061         6.745       -0.045\niMOOD~~iMOOD   0.998      0.003      0.003        25.826       -0.065\nsMOOD~~sMOOD   0.981      0.164      1.757         2.272       -0.007\niMOOD~~sMOOD  -0.272      0.097      0.134        -2.135        0.005\niMOOD~1        4.280      0.186      0.178        21.683        0.004\nsMOOD~1        0.000      0.086      0.169         0.004       -0.002\n             Coverage r_coef.n r_se.n\niMOOD~SurgTx    0.949   -0.008 -0.933\na               0.943   -0.022 -0.933\nMOOD1~~MOOD1    0.953   -0.007 -0.931\nMOOD4~~MOOD4    0.950   -0.022 -0.925\nMOOD8~~MOOD8    0.952    0.016 -0.933\niMOOD~~iMOOD    0.951   -0.001 -0.929\nsMOOD~~sMOOD    0.955   -0.004 -0.935\niMOOD~~sMOOD    0.948    0.012 -0.934\niMOOD~1         0.948   -0.003 -0.935\nsMOOD~1         0.942    0.012 -0.935\n========= Correlation between Fit Indices ============\n       chisq    aic    bic  rmsea    cfi    tli   srmr      n\nchisq  1.000 -0.007 -0.007  0.911 -0.780 -0.854  0.825 -0.007\naic   -0.007  1.000  1.000 -0.158  0.204  0.015 -0.427  1.000\nbic   -0.007  1.000  1.000 -0.158  0.204  0.015 -0.427  1.000\nrmsea  0.911 -0.158 -0.158  1.000 -0.875 -0.897  0.866 -0.158\ncfi   -0.780  0.204  0.204 -0.875  1.000  0.930 -0.776  0.204\ntli   -0.854  0.015  0.015 -0.897  0.930  1.000 -0.818  0.015\nsrmr   0.825 -0.427 -0.427  0.866 -0.776 -0.818  1.000 -0.427\nn     -0.007  1.000  1.000 -0.158  0.204  0.015 -0.427  1.000\n================== Replications =====================\nNumber of replications = 4500 \nNumber of converged replications = 4455 \nNumber of nonconverged replications: \n   1. Nonconvergent Results = 0 \n   2. Nonconvergent results from multiple imputation = 0 \n   3. At least one SE were negative or NA = 0 \n   4. Nonpositive-definite latent or observed (residual) covariance matrix \n      (e.g., Heywood case or linear dependency) = 45 \nNOTE: The sample size is varying.\n\n\nUtilizzando la funzione getPower, è quindi ottenere le stime di potenza per ciascuna dimensione campionaria come segue:\n\nLGC.N &lt;- getPower(simAll)\n# Find the samplesize for 80% power\nfindPower(LGC.N, \"N\", 0.8) |&gt; print()\n\niMOOD~SurgTx            a MOOD1~~MOOD1 MOOD4~~MOOD4 MOOD8~~MOOD8 \n          NA         1710          Inf          Inf          816 \niMOOD~~iMOOD sMOOD~~sMOOD iMOOD~~sMOOD      iMOOD~1      sMOOD~1 \n         Inf          748         1161          Inf           NA \n\n\nPertanto, in base alla simulazione di Chen & Yung (2023), sarebbe necessaria una dimensione campionaria di 1710 per ottenere una potenza di 0.80.\nQuesto risultato può essere mostrato graficamente nella figura seguente. In questa figura, la linea tratteggiata orizzontale indica la potenza a 0.80, mentre la linea con la freccia che va da questa linea orizzontale a \\(N\\) = 1710 indica la dimensione campionaria determinata dalla curva di potenza.\n\n# Call plotPower to plot the power to sample size\nplotPower(simAll, powerParam = \"a\")\n# Add a horizontal line of 0.80\nabline(h = 0.8, lwd = 2, lty = 8)\narrows(1710, 0.8, 1710, 0, lwd = 2)\ntext(\n    1710,\n    -0.018, \"N = 1710\"\n)",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Dimensione Campionaria e Analisi della Potenza</span>"
    ]
  },
  {
    "objectID": "chapters/sem/14_sem_power.html#riflessioni-conclusive",
    "href": "chapters/sem/14_sem_power.html#riflessioni-conclusive",
    "title": "60  Dimensione Campionaria e Analisi della Potenza",
    "section": "60.7 Riflessioni Conclusive",
    "text": "60.7 Riflessioni Conclusive\nIn questo capitolo, abbiamo illustrato come determinare la dimensione campionaria e calcolare la potenza statistica utilizzando l’approccio basato sulla simulazione Monte Carlo. Questo metodo rappresenta l’approccio più generale e flessibile per ottenere stime affidabili della dimensione campionaria e della potenza necessarie alla progettazione di studi in vari contesti di ricerca, inclusi i modelli a equazioni strutturali (SEM).\nLa simulazione Monte Carlo consente di affrontare situazioni complesse e realistiche che spesso non sono gestibili con metodi analitici tradizionali. Essa permette infatti di modellare vari scenari, includendo la variabilità dei parametri e le incertezze che caratterizzano i dati empirici. Inoltre, il metodo può essere applicato a una vasta gamma di modelli statistici, garantendo flessibilità nell’adattamento a diversi contesti di studio e domande di ricerca.\nIn conclusione, il metodo basato sulla simulazione Monte Carlo offre un potente strumento per pianificare studi empirici robusti e ben fondati, permettendo ai ricercatori di prendere decisioni informate riguardo alla dimensione del campione e alla configurazione del modello in relazione agli obiettivi specifici del loro studio.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Dimensione Campionaria e Analisi della Potenza</span>"
    ]
  },
  {
    "objectID": "chapters/sem/14_sem_power.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/sem/14_sem_power.html#informazioni-sullambiente-di-sviluppo",
    "title": "60  Dimensione Campionaria e Analisi della Potenza",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nRandom number generation:\n RNG:     L'Ecuyer-CMRG \n Normal:  Inversion \n Sample:  Rejection \n \nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] quantreg_5.99     compute.es_0.2-5  simsem_0.5-16     kableExtra_1.4.0 \n [5] MASS_7.3-61       viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n [9] ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1 \n[13] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-19     psych_2.4.6.26   \n[17] scales_1.3.0      markdown_1.13     knitr_1.49        lubridate_1.9.3  \n[21] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[25] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1    \n[29] tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] splines_4.4.2       later_1.3.2         pbdZMQ_0.3-13      \n  [4] R.oo_1.27.0         XML_3.99-0.17       rpart_4.1.23       \n  [7] lifecycle_1.0.4     rstatix_0.7.2       rprojroot_2.0.4    \n [10] lattice_0.22-6      rockchalk_1.8.157   backports_1.5.0    \n [13] magrittr_2.0.3      openxlsx_4.2.7.1    Hmisc_5.2-0        \n [16] rmarkdown_2.29      httpuv_1.6.15       qgraph_1.9.8       \n [19] zip_2.3.1           pbapply_1.7-2       minqa_1.2.8        \n [22] multcomp_1.4-26     abind_1.4-8         quadprog_1.5-8     \n [25] R.utils_2.12.3      nnet_7.3-19         TH.data_1.1-2      \n [28] sandwich_3.1-1      openintro_2.5.0     arm_1.14-4         \n [31] MatrixModels_0.5-3  airports_0.1.0      svglite_2.1.3      \n [34] codetools_0.2-20    xml2_1.3.6          tidyselect_1.2.1   \n [37] farver_2.1.2        lme4_1.1-35.5       stats4_4.4.2       \n [40] base64enc_0.1-3     jsonlite_1.8.9      Formula_1.2-5      \n [43] survival_3.7-0      emmeans_1.10.5      systemfonts_1.1.0  \n [46] tools_4.4.2         rio_1.2.3           Rcpp_1.0.13-1      \n [49] glue_1.8.0          mnormt_2.1.1        xfun_0.49          \n [52] IRdisplay_1.1       withr_3.0.2         fastmap_1.2.0      \n [55] boot_1.3-31         fansi_1.0.6         SparseM_1.84-2     \n [58] digest_0.6.37       mi_1.1              timechange_0.3.0   \n [61] R6_2.5.1            mime_0.12           estimability_1.5.1 \n [64] colorspace_2.1-1    Cairo_1.6-2         gtools_3.9.5       \n [67] jpeg_0.1-10         R.methodsS3_1.8.2   utf8_1.2.4         \n [70] generics_0.1.3      data.table_1.16.2   corpcor_1.6.10     \n [73] usdata_0.3.1        htmlwidgets_1.6.4   pkgconfig_2.0.3    \n [76] sem_3.1-16          gtable_0.3.6        htmltools_0.5.8.1  \n [79] carData_3.0-5       png_0.1-8           rstudioapi_0.17.1  \n [82] tzdb_0.4.0          reshape2_1.4.4      uuid_1.2-1         \n [85] coda_0.19-4.1       checkmate_2.3.2     nlme_3.1-166       \n [88] nloptr_2.1.1        repr_1.1.7          zoo_1.8-12         \n [91] parallel_4.4.2      miniUI_0.1.1.1      foreign_0.8-87     \n [94] pillar_1.9.0        grid_4.4.2          vctrs_0.6.5        \n [97] promises_1.3.0      car_3.1-3           OpenMx_2.21.13     \n[100] xtable_1.8-4        cluster_2.1.6       htmlTable_2.4.3    \n[103] evaluate_1.0.1      pbivnorm_0.6.0      mvtnorm_1.3-2      \n[106] cli_3.6.3           kutils_1.73         compiler_4.4.2     \n[109] rlang_1.1.4         crayon_1.5.3        ggsignif_0.6.4     \n[112] fdrtool_1.2.18      plyr_1.8.9          stringi_1.8.4      \n[115] munsell_0.5.1       lisrelToR_0.3       pacman_0.5.1       \n[118] Matrix_1.7-1        IRkernel_1.3.2      hms_1.1.3          \n[121] glasso_1.11         shiny_1.9.1         igraph_2.1.1       \n[124] broom_1.0.7         RcppParallel_5.1.9  cherryblossom_0.1.0\n\n\n\n\n\n\nBuchberger, E. S., Ngo, C. T., Peikert, A., Brandmaier, A. M., & Werkle-Bergner, M. (2024). Estimating statistical power for structural equation models in developmental cognitive science: A tutorial in R: Power simulation for SEMs. Behavior Research Methods, 1–18.\n\n\nByrne, B. M. (2013). Structural equation modeling with Mplus: Basic concepts, applications, and programming. routledge.\n\n\nChen, D.-G., & Yung, Y.-F. (2023). Structural Equation Modeling Using R/SAS: A Step-by-step Approach with Real Data Analysis. CRC Press.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Dimensione Campionaria e Analisi della Potenza</span>"
    ]
  },
  {
    "objectID": "chapters/sem/E_01.html",
    "href": "chapters/sem/E_01.html",
    "title": "61  ✏️ Esercizi",
    "section": "",
    "text": "Nello studio di {cite:t}weiss2018difficulties viene esaminata la relazione tra la difficiltà di regolare le emozioni positive e l’abuso di alcol e di sostanze. Gli autori propongono due modelli SEM. Si riproduca l’analisi svolta da {cite:t}weiss2018difficulties usando lavaan.\n\nsource(\"../_common.R\")\n\nsuppressPackageStartupMessages({\n    library(\"lavaan\")\n    library(\"lavaanExtra\")\n    library(\"lavaanPlot\")\n    library(\"psych\")\n    library(\"dplyr\") \n    library(\"tidyr\")\n    library(\"knitr\")\n    library(\"mvnormalTest\")\n    library(\"semPlot\")\n    library(\"DiagrammeRsvg\")\n    library(\"rsvg\")\n    library(\"effectsize\")\n})\nset.seed(42)\n\nNello studio di {cite:t}weiss2018difficulties\n\nLa difficoltà di regolare le emozioni positive viene misurata con la Difficulties in Emotion Regulation Scale – Positive (DERS-P; Weiss, Gratz, & Lavender, 2015), che comprende le sottoscale di Acceptance, Impulse, e Goals.\nL’abuso di sostanze viene misurato con la Drug Abuse Screening Test (DAST; Skinner, 1982).\nL’abuso di alcol viene misurato con la Alcohol Use Disorder Identification Test (AUDIT; Saunders, Aasland, Babor, De la Fuente, & Grant, 1993), con le sottoscale di Hazardous Consumption, Dependence, e Consequences.\n\nI dati di un campione di 284 partecipanti sono riportati nella forma di una matrice di correlazione.\n\nlower &lt;- \"\n   1\n   .38 1\n   .41 .64 1\n   .34 .44 .30 1\n   .29 .12 .27 .06 1\n   .29 .22 .20 .17 .54 1\n   .30 .15 .23 .09 .73 .69 1\n\"\n\n\ndat_cov &lt;- lavaan::getCov(\n    lower,\n    names = c(\"dmis\", \"con\", \"dep\", \"consu\", \"acc\", \"goal\", \"imp\")\n)\nprint(dat_cov)\n\n      dmis  con  dep consu  acc goal  imp\ndmis  1.00 0.38 0.41  0.34 0.29 0.29 0.30\ncon   0.38 1.00 0.64  0.44 0.12 0.22 0.15\ndep   0.41 0.64 1.00  0.30 0.27 0.20 0.23\nconsu 0.34 0.44 0.30  1.00 0.06 0.17 0.09\nacc   0.29 0.12 0.27  0.06 1.00 0.54 0.73\ngoal  0.29 0.22 0.20  0.17 0.54 1.00 0.69\nimp   0.30 0.15 0.23  0.09 0.73 0.69 1.00\n\n\nIn questo studio, gli autori adottano due modelli SEM distinti per analizzare i dati. Nel primo modello, si postula che la difficoltà nella regolazione delle emozioni positive funzioni come variabile esogena, influenzando sia l’abuso di sostanze sia l’abuso di alcol. Inoltre, si ipotizza una correlazione tra abuso di sostanze e abuso di alcol, suggerendo una possibile interdipendenza tra questi due comportamenti problematici.\nPer quanto riguarda le variabili latenti specifiche, la difficoltà di regolare le emozioni positive, indicata come drpe, è rappresentata da una variabile latente che si basa su tre indicatori.Parallelamente, l’abuso di alcol, etichettato come amis, è concepito come una seconda variabile latente, anch’essa identificata tramite tre indicatori distinti.\n\nmod &lt;- \"\n  drpe =~ NA*acc + goal + imp\n  amis =~ NA*con + dep + consu\n  amis ~ drpe\n  dmis ~ drpe\n  dmis ~~ amis\n  drpe ~~ 1*drpe\n  amis ~~ 1*amis\n\"\n\nAdattiamo il modello ai dati con sem().\n\nfit &lt;- lavaan::sem(mod, sample.cov = dat_cov, sample.nobs = 284)\n\nEsaminiamo i risultati.\n\nstandardizedSolution(fit) |&gt; print()\n\n     lhs op   rhs est.std    se      z pvalue ci.lower ci.upper\n1   drpe =~   acc   0.770 0.031 24.982  0.000    0.710    0.830\n2   drpe =~  goal   0.728 0.033 21.849  0.000    0.663    0.794\n3   drpe =~   imp   0.945 0.024 39.322  0.000    0.898    0.992\n4   amis =~   con   0.837 0.039 21.217  0.000    0.759    0.914\n5   amis =~   dep   0.756 0.041 18.420  0.000    0.676    0.837\n6   amis =~ consu   0.494 0.052  9.439  0.000    0.392    0.597\n7   amis  ~  drpe   0.254 0.066  3.863  0.000    0.125    0.383\n8   dmis  ~  drpe   0.334 0.056  6.001  0.000    0.225    0.443\n9   amis ~~  dmis   0.458 0.055  8.303  0.000    0.350    0.567\n10  drpe ~~  drpe   1.000 0.000     NA     NA    1.000    1.000\n11  amis ~~  amis   0.936 0.033 28.023  0.000    0.870    1.001\n12   acc ~~   acc   0.407 0.047  8.575  0.000    0.314    0.500\n13  goal ~~  goal   0.470 0.049  9.677  0.000    0.375    0.565\n14   imp ~~   imp   0.107 0.045  2.349  0.019    0.018    0.196\n15   con ~~   con   0.300 0.066  4.551  0.000    0.171    0.430\n16   dep ~~   dep   0.428 0.062  6.900  0.000    0.307    0.550\n17 consu ~~ consu   0.756 0.052 14.595  0.000    0.654    0.857\n18  dmis ~~  dmis   0.889 0.037 23.960  0.000    0.816    0.961\n\n\nCreiamo un path diagram.\n\nsemPaths(fit,\n    whatLabels = \"std\",\n    sizeMan = 10,\n    edge.label.cex = 0.9,\n    style = \"ram\",\n    nCharNodes = 0, nCharEdges = 0\n)\n\n\n\n\n\n\n\n\nGli autori esplorano un modello alternativo nel quale le relazioni causali vengono rovesciate: in questo caso è la difficoltà di regolazione delle emozioni positive ad essere la variabile esogena, e l’abuso di sostanze e l’abuso di alcol sono le variabili esogene.\n\nmod_alt &lt;- \"\n  drpe =~ NA*acc + goal + imp\n  amis =~ NA*con + dep + consu\n  drpe ~ amis + dmis\n  dmis ~~ amis\n  drpe ~~ 1*drpe\n  amis ~~ 1*amis\n\"\n\n\nfit_alt &lt;- sem(mod_alt, sample.cov = dat_cov, sample.nobs = 311)\n\n\nstandardizedSolution(fit_alt) |&gt; print()\n\n     lhs op   rhs est.std    se      z pvalue ci.lower ci.upper\n1   drpe =~   acc   0.770 0.029 26.143  0.000    0.712    0.828\n2   drpe =~  goal   0.728 0.032 22.864  0.000    0.666    0.791\n3   drpe =~   imp   0.945 0.023 41.149  0.000    0.900    0.990\n4   amis =~   con   0.837 0.038 22.203  0.000    0.763    0.910\n5   amis =~   dep   0.756 0.039 19.276  0.000    0.679    0.833\n6   amis =~ consu   0.494 0.050  9.877  0.000    0.396    0.592\n7   drpe  ~  amis   0.115 0.075  1.549  0.121   -0.031    0.261\n8   drpe  ~  dmis   0.276 0.066  4.189  0.000    0.147    0.405\n9   amis ~~  dmis   0.503 0.050 10.122  0.000    0.405    0.600\n10  drpe ~~  drpe   0.879 0.037 23.633  0.000    0.806    0.952\n11  amis ~~  amis   1.000 0.000     NA     NA    1.000    1.000\n12   acc ~~   acc   0.407 0.045  8.973  0.000    0.318    0.496\n13  goal ~~  goal   0.470 0.046 10.126  0.000    0.379    0.561\n14   imp ~~   imp   0.107 0.043  2.458  0.014    0.022    0.192\n15   con ~~   con   0.300 0.063  4.763  0.000    0.177    0.424\n16   dep ~~   dep   0.428 0.059  7.221  0.000    0.312    0.545\n17 consu ~~ consu   0.756 0.049 15.273  0.000    0.659    0.853\n18  dmis ~~  dmis   1.000 0.000     NA     NA    1.000    1.000\n\n\n\nsemPaths(fit_alt,\n    whatLabels = \"std\",\n    sizeMan = 10,\n    edge.label.cex = 0.9,\n    style = \"ram\",\n    nCharNodes = 0, nCharEdges = 0\n)\n\n\n\n\n\n\n\n\nVedremo in seguito come sia possibile eseguire un test statistico per stabilire quale di due modelli sia più appropriato. Anticipando qui tale discussione, applichiamo il test del rapporto di verosimiglianze.\n\nlavTestLRT(fit, fit_alt) |&gt; print()\n\n\nChi-Squared Difference Test\n\n        Df    AIC    BIC  Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\nfit     12 4963.6 5022.0 38.211                                    \nfit_alt 12 5433.1 5492.9 41.844     3.6327     0       0           \n\n\nI risultati di questo test suggeriscono che il primo modello è maggiormente appropriato per descrivere i dati raccolti da {cite:t}weiss2018difficulties.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/sem/15_prior_pred_mod_check.html",
    "href": "chapters/sem/15_prior_pred_mod_check.html",
    "title": "62  Prior Predictive Model Checking",
    "section": "",
    "text": "62.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nIn questo capitolo verrà discusso l’approccio del Bayesian prior predictive similarity checking proposto da Bonifay et al. (2024).",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Prior Predictive Model Checking</span>"
    ]
  },
  {
    "objectID": "chapters/sem/15_prior_pred_mod_check.html#replicabilità-e-gof",
    "href": "chapters/sem/15_prior_pred_mod_check.html#replicabilità-e-gof",
    "title": "62  Prior Predictive Model Checking",
    "section": "62.2 Replicabilità e GOF",
    "text": "62.2 Replicabilità e GOF\nIl progresso scientifico empirico si basa sulla replicabilità, ossia la capacità di riprodurre i risultati di uno studio precedente seguendo le stesse procedure con nuovi dati (Bollen et al., 2015). Le ricerche sulla replicabilità in psicologia si sono spesso concentrate sugli effetti sperimentali (e.g., Klein, 2014; Open Science Collaboration, 2015; Youyou et al., 2023), ma molti ambiti della disciplina si fondano sull’uso di modelli statistici piuttosto che su un disegno sperimentale. Anche in questi contesti, è essenziale verificare il grado di replicabilità dei modelli statistici utilizzati.\nUn modo per quantificare la replicabilità dei modelli (sia nell’analisi di regressione, nei modelli a equazioni strutturali (SEM), nella teoria della risposta al item, nei modelli di rete o in altri contesti di modellizzazione) è valutare la bontà di adattamento (GOF, goodness of fit) del modello ai dati osservati. Storicamente, molti ricercatori in psicologia hanno considerato la replicabilità di un modello principalmente come la capacità di riprodurre la bontà di adattamento di uno studio precedente: «[il] miglior adattamento del modello… ha replicato i risultati» di ricerche precedenti (Whiteman et al., 2022, p. 132), «il miglior adattamento… ha replicato i risultati precedenti» (Giuntoli et al., 2021, p. 1668), «un adattamento sostanzialmente migliore… ha replicato l’approccio classico» (Fernández de la Cruz et al., 2018, p. 608). Tuttavia, Bonifay et al. (2024) fanno notare come questa pratica meriti un’attenta considerazione, in quanto la mera replicazione di una buona bontà di adattamento non è sufficiente per confermare la validità del modello statistico originale e della teoria sottostante.\nBonifay et al. (2024) propongono il seguente esempio. Si considerino le matrici di covarianza simulate mostrate nella riga superiore della Figura 1. La matrice a sinistra (Pannello B) rappresenta le covarianze tra le variabili di uno studio originale, mentre le altre due (Pannelli C e D) rappresentano le covarianze delle stesse variabili in due dataset di replicazione. Questo scenario illustra il tipico caso di replicazione del modello, in cui la stessa struttura viene adattata a dataset differenti, lasciando liberi i parametri. Sebbene le differenze nei dati siano evidenti, un modello con due fattori correlati si adatta bene a ciascuna matrice di covarianza (indice di adattamento comparativo [CFI] elevato, i.e., ≥ 0.95). Una bontà di adattamento elevata indica che il modello rappresenta adeguatamente le covarianze all’interno del dataset originale e delle repliche, ma non informa sul fatto che il modello rifletta le stesse relazioni tra le variabili. Come mostrato in Figura 1, affidarsi esclusivamente alla bontà di adattamento può portare a ignorare differenze significative nei pattern di dati.\n\n# Load Correlation Matrices ----\ncor_mats &lt;- readRDS(\n    here::here(\"data\", \"Bonifay\", \"figure1_cormat.RDS\")\n)\n\n# Create and Save Correlation Plots ----\ncorrplot(cor_mats[[1]],\n    method = \"circle\", type = \"full\",\n    tl.col = \"black\", tl.cex = 2.5, cl.pos = \"n\"\n)\n\ncorrplot(cor_mats[[2]],\n    method = \"circle\", type = \"full\",\n    tl.col = \"black\", tl.cex = 2.5, cl.pos = \"n\"\n)\n\ncorrplot(cor_mats[[3]],\n    method = \"circle\", type = \"full\",\n    tl.col = \"black\", tl.cex = 2.5, cl.pos = \"n\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncor_mats\n\n\n    $original\n        \n\nA matrix: 6 x 6 of type dbl\n\n\n\ny1\ny2\ny3\ny4\ny5\ny6\n\n\n\n\ny1\n1.000\n0.682\n0.638\n0.185\n0.507\n0.571\n\n\ny2\n0.682\n1.000\n0.644\n0.275\n0.661\n0.705\n\n\ny3\n0.638\n0.644\n1.000\n0.189\n0.473\n0.532\n\n\ny4\n0.185\n0.275\n0.189\n1.000\n0.468\n0.477\n\n\ny5\n0.507\n0.661\n0.473\n0.468\n1.000\n0.730\n\n\ny6\n0.571\n0.705\n0.532\n0.477\n0.730\n1.000\n\n\n\n\n\n    $replication1\n        \n\nA matrix: 6 x 6 of type dbl\n\n\n\ny1\ny2\ny3\ny4\ny5\ny6\n\n\n\n\ny1\n1.000\n0.389\n0.503\n0.527\n0.501\n0.391\n\n\ny2\n0.389\n1.000\n0.536\n0.432\n0.420\n0.423\n\n\ny3\n0.503\n0.536\n1.000\n0.782\n0.707\n0.493\n\n\ny4\n0.527\n0.432\n0.782\n1.000\n0.766\n0.452\n\n\ny5\n0.501\n0.420\n0.707\n0.766\n1.000\n0.380\n\n\ny6\n0.391\n0.423\n0.493\n0.452\n0.380\n1.000\n\n\n\n\n\n    $replication2\n        \n\nA matrix: 6 x 6 of type dbl\n\n\n\ny1\ny2\ny3\ny4\ny5\ny6\n\n\n\n\ny1\n1.0000\n-0.2736\n-0.3580\n0.0224\n0.1361\n0.0264\n\n\ny2\n-0.2736\n1.0000\n0.5371\n0.0917\n0.0198\n-0.0611\n\n\ny3\n-0.3580\n0.5371\n1.0000\n-0.0743\n-0.0419\n-0.0680\n\n\ny4\n0.0224\n0.0917\n-0.0743\n1.0000\n0.3464\n0.4743\n\n\ny5\n0.1361\n0.0198\n-0.0419\n0.3464\n1.0000\n0.3296\n\n\ny6\n0.0264\n-0.0611\n-0.0680\n0.4743\n0.3296\n1.0000\n\n\n\n\n\n\n\n\n\nM &lt;- '\n    F1 =~ NA*y1 + y2 + y3\n    F2 =~ NA*y4 + y5 + y6\n    F1 ~~ 1*F1\n    F2 ~~ 1 * F2\n'\n\nfit_original &lt;- cfa(model = M, sample.cov = cor_mats$original, sample.nobs = 1000)\nparameterEstimates(fit_original)\n\n\nA lavaan.data.frame: 15 x 9\n\n\nlhs\nop\nrhs\nest\nse\nz\npvalue\nci.lower\nci.upper\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nF1\n=~\ny1\n0.772\n0.0278\n27.73\n0\n0.718\n0.827\n\n\nF1\n=~\ny2\n0.899\n0.0260\n34.56\n0\n0.848\n0.949\n\n\nF1\n=~\ny3\n0.732\n0.0284\n25.77\n0\n0.677\n0.788\n\n\nF2\n=~\ny4\n0.502\n0.0311\n16.11\n0\n0.441\n0.563\n\n\nF2\n=~\ny5\n0.823\n0.0272\n30.22\n0\n0.770\n0.876\n\n\nF2\n=~\ny6\n0.903\n0.0262\n34.53\n0\n0.852\n0.954\n\n\nF1\n~~\nF1\n1.000\n0.0000\nNA\nNA\n1.000\n1.000\n\n\nF2\n~~\nF2\n1.000\n0.0000\nNA\nNA\n1.000\n1.000\n\n\ny1\n~~\ny1\n0.403\n0.0224\n18.00\n0\n0.359\n0.447\n\n\ny2\n~~\ny2\n0.192\n0.0183\n10.49\n0\n0.156\n0.227\n\n\ny3\n~~\ny3\n0.463\n0.0243\n19.03\n0\n0.415\n0.511\n\n\ny4\n~~\ny4\n0.747\n0.0348\n21.45\n0\n0.679\n0.816\n\n\ny5\n~~\ny5\n0.322\n0.0206\n15.59\n0\n0.281\n0.362\n\n\ny6\n~~\ny6\n0.183\n0.0192\n9.53\n0\n0.146\n0.221\n\n\nF1\n~~\nF2\n0.835\n0.0156\n53.59\n0\n0.804\n0.865\n\n\n\n\n\n\nfitMeasures(fit_original, \"cfi\") |&gt; round(2)\n\ncfi: 0.96\n\n\n\nsemPaths(fit_original,\n    whatLabels = \"std\",\n    sizeMan = 8,\n    edge.label.cex = 0.7,\n    style = \"mx\",\n    nCharNodes = 0, nCharEdges = 0\n)\n\n\n\n\n\n\n\n\n\nfit_rep1 &lt;- cfa(model = M, sample.cov = cor_mats$replication1, sample.nobs = 1000)\nparameterEstimates(fit_rep1)\n\n\nA lavaan.data.frame: 15 x 9\n\n\nlhs\nop\nrhs\nest\nse\nz\npvalue\nci.lower\nci.upper\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nF1\n=~\ny1\n0.596\n0.0300\n19.88\n0\n0.537\n0.655\n\n\nF1\n=~\ny2\n0.566\n0.0303\n18.69\n0\n0.507\n0.626\n\n\nF1\n=~\ny3\n0.899\n0.0265\n33.95\n0\n0.847\n0.951\n\n\nF2\n=~\ny4\n0.904\n0.0254\n35.67\n0\n0.855\n0.954\n\n\nF2\n=~\ny5\n0.830\n0.0265\n31.34\n0\n0.778\n0.882\n\n\nF2\n=~\ny6\n0.524\n0.0306\n17.14\n0\n0.464\n0.584\n\n\nF1\n~~\nF1\n1.000\n0.0000\nNA\nNA\n1.000\n1.000\n\n\nF2\n~~\nF2\n1.000\n0.0000\nNA\nNA\n1.000\n1.000\n\n\ny1\n~~\ny1\n0.644\n0.0306\n21.02\n0\n0.584\n0.704\n\n\ny2\n~~\ny2\n0.678\n0.0320\n21.21\n0\n0.616\n0.741\n\n\ny3\n~~\ny3\n0.191\n0.0203\n9.41\n0\n0.151\n0.231\n\n\ny4\n~~\ny4\n0.181\n0.0154\n11.73\n0\n0.151\n0.211\n\n\ny5\n~~\ny5\n0.310\n0.0180\n17.23\n0\n0.274\n0.345\n\n\ny6\n~~\ny6\n0.724\n0.0336\n21.54\n0\n0.658\n0.790\n\n\nF1\n~~\nF2\n0.959\n0.0127\n75.76\n0\n0.934\n0.984\n\n\n\n\n\n\nfitMeasures(fit_rep1, \"cfi\") |&gt; round(2)\n\ncfi: 0.96\n\n\n\nfit_rep2 &lt;- cfa(model = M, sample.cov = cor_mats$replication2, sample.nobs = 1000)\nparameterEstimates(fit_rep2)\n\n\nA lavaan.data.frame: 15 x 9\n\n\nlhs\nop\nrhs\nest\nse\nz\npvalue\nci.lower\nci.upper\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nF1\n=~\ny1\n0.422\n0.0363\n11.62\n0.000000\n0.35098\n0.493\n\n\nF1\n=~\ny2\n-0.627\n0.0412\n-15.23\n0.000000\n-0.70783\n-0.546\n\n\nF1\n=~\ny3\n-0.855\n0.0479\n-17.86\n0.000000\n-0.94899\n-0.761\n\n\nF2\n=~\ny4\n0.701\n0.0418\n16.78\n0.000000\n0.61899\n0.783\n\n\nF2\n=~\ny5\n0.491\n0.0370\n13.26\n0.000000\n0.41836\n0.563\n\n\nF2\n=~\ny6\n0.676\n0.0411\n16.43\n0.000000\n0.59526\n0.757\n\n\nF1\n~~\nF1\n1.000\n0.0000\nNA\nNA\n1.00000\n1.000\n\n\nF2\n~~\nF2\n1.000\n0.0000\nNA\nNA\n1.00000\n1.000\n\n\ny1\n~~\ny1\n0.821\n0.0405\n20.27\n0.000000\n0.74139\n0.900\n\n\ny2\n~~\ny2\n0.606\n0.0463\n13.09\n0.000000\n0.51506\n0.696\n\n\ny3\n~~\ny3\n0.268\n0.0707\n3.79\n0.000152\n0.12918\n0.406\n\n\ny4\n~~\ny4\n0.508\n0.0496\n10.23\n0.000000\n0.41057\n0.605\n\n\ny5\n~~\ny5\n0.758\n0.0403\n18.81\n0.000000\n0.67902\n0.837\n\n\ny6\n~~\ny6\n0.542\n0.0477\n11.37\n0.000000\n0.44876\n0.636\n\n\nF1\n~~\nF2\n0.091\n0.0432\n2.11\n0.035024\n0.00639\n0.176\n\n\n\n\n\n\nfitMeasures(fit_rep2, \"cfi\") |&gt; round(2)\n\ncfi: 0.94\n\n\nA complicare ulteriormente le cose, l’adattamento dello stesso modello a ciascuna matrice di dati produce stime dei parametri molto variabili. Ad esempio, il fattore di carico standardizzato l21 (secondo indicatore sul primo fattore) è stimato a 0.90, 0.57 e -2.63 nelle tre matrici, mentre la correlazione tra i fattori (c21) varia da quasi indipendenza (0.09) a una sovrapposizione quasi totale (0.96).\nIn sintesi, la bontà di adattamento non ci fornisce alcuna indicazione sul grado di somiglianza tra il dataset di replicazione, i parametri del modello e quelli dello studio originale, mentre una forte somiglianza tra questi elementi è cruciale per valutare il successo della replicazione.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Prior Predictive Model Checking</span>"
    ]
  },
  {
    "objectID": "chapters/sem/15_prior_pred_mod_check.html#bontà-di-adattamento-e-replicazione",
    "href": "chapters/sem/15_prior_pred_mod_check.html#bontà-di-adattamento-e-replicazione",
    "title": "62  Prior Predictive Model Checking",
    "section": "62.3 Bontà di Adattamento e Replicazione",
    "text": "62.3 Bontà di Adattamento e Replicazione\nNel loro approfondimento critico sull’uso della bontà di adattamento come strumento per testare le teorie, Roberts & Pashler (2000) hanno sostenuto che «dimostrare che una teoria si adatta ai dati… è quasi privo di significato» (p. 361; vedi anche Vanpaemel, 2020). In particolare, hanno individuato tre limiti del GOF che ne impediscono l’uso come supporto teorico solido:\n\nNon chiarisce cosa predice una teoria.\n\nNon spiega la variabilità dei dati.\n\nNon considera la probabilità a priori che la teoria possa adattarsi a qualsiasi insieme di dati plausibili.\n\nDi conseguenza, Roberts & Pashler (2000) hanno concluso che il GOF fornisce un supporto convincente a una teoria solo quando sia i dati che la teoria sono ben vincolati, ovvero quando i dati non sono troppo variabili e la teoria non è troppo flessibile. Tuttavia, in un singolo studio, tali vincoli possono essere difficili da definire e applicare, anche per la mancanza di criteri di riferimento (ad esempio, cosa significa dire che i dati «non sono troppo variabili»? Variabili rispetto a cosa? E in che misura?).\nNel contesto delle repliche, però, il confronto con lo studio originale offre un chiaro riferimento per caratterizzare la variabilità dei dati e delle stime dei parametri del modello. Questo consente di estendere naturalmente le tre critiche di Roberts & Pashler (2000) al tema della replicazione:\n\nPrevisione limitata del risultato della replica\nLa bontà di adattamento dello studio originale non fornisce alcuna informazione sostanziale sull’esito della replica. Il fatto che un modello si sia adattato bene nello studio originale non implica che si replichino aspetti inferenziali più importanti, come i pattern dei dati o le stime dei parametri.\nAssenza di indicazioni sulla somiglianza tra i dati originali e quelli della replica\nDue set di dati possono presentare pattern nettamente distinti, potenzialmente derivanti da meccanismi di generazione diversi. Tuttavia, il modello potrebbe mascherare queste differenze, compromettendo l’accuratezza delle inferenze.\nTendenza intrinseca del modello ad adattarsi bene\nSe un modello possiede una forte predisposizione ad adattarsi bene ai dati (Bonifay & Cai, 2017; Falk & Muthukrishna, 2023), una buona bontà di adattamento per i dati originali e di replica non rappresenta una sorpresa né un valore scientifico. In tali casi, il GOF può replicarsi indipendentemente dai pattern specifici dei dati che il modello intende rappresentare.\n\nIdealmente, i ricercatori possono essere fiduciosi che i loro risultati offrano un supporto alla teoria alla base del modello statistico solo se dimostrano che i dati della replica non sono più variabili rispetto ai dati originali, e che le stime dei parametri nella replica non riflettono una maggiore flessibilità rispetto a quelle originali.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Prior Predictive Model Checking</span>"
    ]
  },
  {
    "objectID": "chapters/sem/15_prior_pred_mod_check.html#definire-lobiettivo-della-replica",
    "href": "chapters/sem/15_prior_pred_mod_check.html#definire-lobiettivo-della-replica",
    "title": "62  Prior Predictive Model Checking",
    "section": "62.4 Definire l’Obiettivo della Replica",
    "text": "62.4 Definire l’Obiettivo della Replica\nIn psicologia, ottenere una somiglianza perfetta tra studi è poco pratico e probabilmente non necessario, anche a causa dell’eterogeneità intrinseca della popolazione (McShane et al., 2019). Piuttosto, i ricercatori dovrebbero focalizzarsi sugli aspetti specifici dello studio originale che intendono replicare. A tal proposito, la Figura 2 illustra un approccio più ragionevole per definire obiettivi di replicazione.\n\n\n\n\n\n\nFigura 62.1: Definire Obiettivi Chiari è Essenziale per Indagare la Replicazione dei Modelli Statistici. Nota: Per andare oltre la semplice replicazione della bontà di adattamento (Goodness of Fit), i ricercatori devono mirare alle aree più significative e centrali del bersaglio. Questo richiede test più rigorosi dei dati e/o dei parametri del modello, che possono essere condotti utilizzando tecniche bayesiane di verifica della similarità predittiva a priori, con l’impiego di distribuzioni a priori sempre più informative, come illustrato nella parte inferiore della figura. La replicazione informata dalla teoria richiede, come minimo, di verificare ipotesi specifiche sui dati e/o sui parametri del modello. La replicazione empirica, sia approssimativa che ravvicinata, implica invece il controllo che i dati replicati e/o i parametri del modello siano rispettivamente approssimativamente o strettamente simili a quelli dello studio originale. (Figura tratta da Bonifay et al., 2024)\n\n\n\n\nCerchio esterno del bersaglio: rappresenta la pratica attuale nelle scienze sociali, spesso limitata a verificare che il modello originale abbia una buona bontà di adattamento ai dati della replica, senza considerare le caratteristiche empiriche dello studio originale. Questa pratica offre il supporto più debole alla teoria originale.\nCerchi interni: rappresentano obiettivi progressivamente più ambiziosi.\n\n\n62.4.1 Obiettivi di replica:\n\nReplicazione informata dalla teoria: un ricercatore interessato alle implicazioni teoriche più ampie dello studio originale può puntare alla teoria sottostante, formulando ipotesi specifiche sui dati o sulle stime dei parametri (es. “Per supportare l’associazione positiva tra x e y, il coefficiente di replica b deve avere un valore positivo”). Questo approccio supera la semplice verifica del GOF e merita studi dedicati.\nReplicazione empirica approssimativa: un ricercatore che desidera replicare direttamente i risultati empirici può puntare allo studio originale, testando la somiglianza approssimativa tra i dati e il modello della replica rispetto a quelli originali. Ad esempio, si può verificare se le covarianze tra i dati di replica riflettono quelle dello studio originale o se le stime dei parametri sono simili (es. “Per una replica approssimativa, b1 deve essere tra 0.4 e 0.7”).\nReplicazione empirica ravvicinata: è il test più rigoroso, in cui le stime devono essere estremamente simili a quelle originali (es. “Per una replica ravvicinata, b1 deve essere tra 0.52 e 0.58”). Riuscire a soddisfare tali criteri fornisce prove solide che il modello cattura lo stesso segnale in entrambi gli studi.\n\n\n\n62.4.2 Test progressivi e rischiosi\nLa struttura a cerchi concentrici del bersaglio rappresenta una sequenza di test sempre più stringenti. Come osservato da Roberts & Pashler (2000), i test di bontà di adattamento sono spesso troppo facili da superare, rendendoli deboli come prova di replica. La replicazione basata sul GOF è l’obiettivo più facile (e per alcuni modelli può comportare un rischio di fallimento praticamente nullo), offrendo il supporto più debole ai risultati originali.\n\nCerchi interni del bersaglio: man mano che ci si avvicina al centro, il rischio di fallimento aumenta, ma aumenta anche la forza delle prove a favore della replica. La replicazione informata dalla teoria è più rigorosa rispetto al GOF, offrendo supporto alla teoria sottostante. La replicazione empirica approssimativa è ancora più rischiosa, ma fornisce prove solide di somiglianza tra dati e parametri. Infine, la replicazione empirica ravvicinata è il test più rischioso, ma il suo successo rappresenta una prova molto forte della replica dei risultati originali.\n\nCome sottolineato da Waller & Meehl (2002), «i test rischiosi sono i mezzi più efficienti per valutare la solidità di una teoria».\nNella seconda parte dell’articolo, Bonifay et al. (2024) presentano un metodo statistico per quantificare la somiglianza tra i dati originali e replicati, oltre che tra le stime dei parametri. Discutono un esempio concreto nel contesto della modellizzazione della struttura latente della psicopatologia e forniscono raccomandazioni per futuri studi di replicazione.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Prior Predictive Model Checking</span>"
    ]
  },
  {
    "objectID": "chapters/sem/15_prior_pred_mod_check.html#verifica-predittiva-a-priori-dei-modelli",
    "href": "chapters/sem/15_prior_pred_mod_check.html#verifica-predittiva-a-priori-dei-modelli",
    "title": "62  Prior Predictive Model Checking",
    "section": "62.5 Verifica Predittiva a Priori dei Modelli",
    "text": "62.5 Verifica Predittiva a Priori dei Modelli\nPer indagare formalmente la somiglianza tra dati originali e di replica, nonché tra le stime dei parametri, Bonifay et al. (2024) propongono di utilizzare la verifica predittiva a priori bayesiana (Prior Predictive Model Checking, PrPMC; Box, 1980; Evans & Moshonov, 2006; Gelman et al., 2017). Questa tecnica sfrutta le distribuzioni a priori per valutare le implicazioni del modello prima di includere i dati osservati nell’analisi.\nIn un’analisi bayesiana, l’obiettivo principale è calcolare la distribuzione a posteriori \\(p(\\theta \\mid y)\\), combinando le informazioni sui dati osservati \\(y\\) e sul parametro sconosciuto \\(\\theta\\) tramite il teorema di Bayes:\n\\[\np(\\theta \\mid y) \\propto p(y \\mid \\theta)p(\\theta),\n\\]\ndove:\n\n\\(p(\\theta)\\) è la distribuzione a priori dei parametri.\n\\(p(y \\mid \\theta)\\) è la funzione di verosimiglianza dei dati dato il modello.\n\nLa PrPMC consiste nel generare campioni predittivi ipotetici per ciascuna variabile osservata, utilizzando esclusivamente le distribuzioni a priori definite sui parametri del modello. Questi campioni rappresentano scenari plausibili in base alle aspettative incorporate nei priori, permettendo una valutazione preliminare del modello.\nSupponiamo di voler stimare l’altezza media degli scalatori negli Stati Uniti, ipotizzando che sia vicina alla media nazionale di 168 cm (Fryar et al., 2021). Possiamo rappresentare questa aspettativa con una distribuzione normale \\(N(\\mu = 168, \\sigma = 10)\\). Generando campioni predittivi da questa distribuzione, otteniamo valori che variano plausibilmente attorno a questa media. Se la conoscenza sugli scalatori suggerisse un valore maggiore o minore, potremmo affinare i priori (ad esempio, aumentando \\(\\mu\\) o riducendo \\(\\sigma\\)) prima di raccogliere i dati.\nLa PrPMC può essere estesa per confrontare i dati osservati con i campioni predittivi. Questo confronto utilizza una statistica di test o una quantità di test per valutare la somiglianza tra i dati osservati e le aspettative predittive.\n\nStatistica di test: una proprietà statistica dei dati (es. mediana, range).\nQuantità di test: una proprietà dipendente dai dati e dal modello (es. stime dei parametri o indici di bontà di adattamento).\n\nAd esempio, per gli scalatori, potremmo confrontare la media delle altezze osservate con le medie dei campioni predittivi. Se la media osservata si trovasse agli estremi della distribuzione predittiva (ad esempio, \\(prpp \\leq 0.05\\) o \\(prpp \\geq 0.95\\)), ciò indicherebbe una discrepanza sistematica tra i dati osservati e le aspettative a priori.\nBonifay et al. (2024) propongono l’uso della PrPMC per verificare la somiglianza tra dati originali e di replica, così come tra le stime dei parametri del modello. Questo approccio, chiamato verifica di similarità predittiva a priori, consente di valutare:\n\nSomiglianza dei dati: confrontando la distribuzione dei dati replicati con le aspettative dei dati originali (es. intercorrelazioni tra item).\nSomiglianza dei parametri: confrontando le stime dei parametri derivanti dai dati di replica con quelle predette dal modello originale. Ad esempio, i caricamenti fattoriali stimati dal modello originale possono essere confrontati con quelli derivati da campioni predittivi a priori.\n\nCome illustrato nella Figura 62.1, i cerchi concentrici rappresentano diversi livelli di rischio e severità nei test di replica:\n\nCerchio esterno: distribuzioni a priori diffuse, che riflettono una bassa restrizione sui dati e sui parametri, portando a test meno rigorosi e meno significativi.\nCentro del bersaglio: distribuzioni a priori altamente informative, con restrizioni strette sui dati e sui parametri, producendo test più rigorosi e significativi.\n\nSe il valore \\(prpp\\) risultante si trova tra \\(0.05\\) e \\(0.95\\), possiamo concludere che i dati e/o i parametri replicati sono coerenti con le aspettative a priori. Questo approccio consente di condurre test progressivamente più stringenti e di acquisire prove più solide del successo della replica.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Prior Predictive Model Checking</span>"
    ]
  },
  {
    "objectID": "chapters/sem/15_prior_pred_mod_check.html#riflessioni-conclusive",
    "href": "chapters/sem/15_prior_pred_mod_check.html#riflessioni-conclusive",
    "title": "62  Prior Predictive Model Checking",
    "section": "62.6 Riflessioni Conclusive",
    "text": "62.6 Riflessioni Conclusive\nIn conclusione, la verifica predittiva a priori offre un metodo formale per quantificare la somiglianza tra studi originali e repliche, sia a livello di dati che di parametri. Implementando questa metodologia, i ricercatori possono definire obiettivi chiari e condurre analisi rigorose per valutare il successo della replicazione, migliorando così la robustezza delle conclusioni dello studio.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Prior Predictive Model Checking</span>"
    ]
  },
  {
    "objectID": "chapters/sem/15_prior_pred_mod_check.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/sem/15_prior_pred_mod_check.html#informazioni-sullambiente-di-sviluppo",
    "title": "62  Prior Predictive Model Checking",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nRandom number generation:\n RNG:     L'Ecuyer-CMRG \n Normal:  Inversion \n Sample:  Rejection \n \nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] quantreg_5.99     compute.es_0.2-5  simsem_0.5-16     kableExtra_1.4.0 \n [5] MASS_7.3-61       viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n [9] ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1 \n[13] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-19     psych_2.4.6.26   \n[17] scales_1.3.0      markdown_1.13     knitr_1.49        lubridate_1.9.3  \n[21] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[25] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1    \n[29] tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] splines_4.4.2       later_1.3.2         pbdZMQ_0.3-13      \n  [4] R.oo_1.27.0         XML_3.99-0.17       rpart_4.1.23       \n  [7] lifecycle_1.0.4     rstatix_0.7.2       rprojroot_2.0.4    \n [10] lattice_0.22-6      rockchalk_1.8.157   backports_1.5.0    \n [13] magrittr_2.0.3      openxlsx_4.2.7.1    Hmisc_5.2-0        \n [16] rmarkdown_2.29      httpuv_1.6.15       qgraph_1.9.8       \n [19] zip_2.3.1           pbapply_1.7-2       minqa_1.2.8        \n [22] multcomp_1.4-26     abind_1.4-8         quadprog_1.5-8     \n [25] R.utils_2.12.3      nnet_7.3-19         TH.data_1.1-2      \n [28] sandwich_3.1-1      openintro_2.5.0     arm_1.14-4         \n [31] MatrixModels_0.5-3  airports_0.1.0      svglite_2.1.3      \n [34] codetools_0.2-20    xml2_1.3.6          tidyselect_1.2.1   \n [37] farver_2.1.2        lme4_1.1-35.5       stats4_4.4.2       \n [40] base64enc_0.1-3     jsonlite_1.8.9      Formula_1.2-5      \n [43] survival_3.7-0      emmeans_1.10.5      systemfonts_1.1.0  \n [46] tools_4.4.2         rio_1.2.3           Rcpp_1.0.13-1      \n [49] glue_1.8.0          mnormt_2.1.1        xfun_0.49          \n [52] IRdisplay_1.1       withr_3.0.2         fastmap_1.2.0      \n [55] boot_1.3-31         fansi_1.0.6         SparseM_1.84-2     \n [58] digest_0.6.37       mi_1.1              timechange_0.3.0   \n [61] R6_2.5.1            mime_0.12           estimability_1.5.1 \n [64] colorspace_2.1-1    Cairo_1.6-2         gtools_3.9.5       \n [67] jpeg_0.1-10         R.methodsS3_1.8.2   utf8_1.2.4         \n [70] generics_0.1.3      data.table_1.16.2   corpcor_1.6.10     \n [73] usdata_0.3.1        htmlwidgets_1.6.4   pkgconfig_2.0.3    \n [76] sem_3.1-16          gtable_0.3.6        htmltools_0.5.8.1  \n [79] carData_3.0-5       png_0.1-8           rstudioapi_0.17.1  \n [82] tzdb_0.4.0          reshape2_1.4.4      uuid_1.2-1         \n [85] coda_0.19-4.1       checkmate_2.3.2     nlme_3.1-166       \n [88] nloptr_2.1.1        repr_1.1.7          zoo_1.8-12         \n [91] parallel_4.4.2      miniUI_0.1.1.1      foreign_0.8-87     \n [94] pillar_1.9.0        grid_4.4.2          vctrs_0.6.5        \n [97] promises_1.3.0      car_3.1-3           OpenMx_2.21.13     \n[100] xtable_1.8-4        cluster_2.1.6       htmlTable_2.4.3    \n[103] evaluate_1.0.1      pbivnorm_0.6.0      mvtnorm_1.3-2      \n[106] cli_3.6.3           kutils_1.73         compiler_4.4.2     \n[109] rlang_1.1.4         crayon_1.5.3        ggsignif_0.6.4     \n[112] fdrtool_1.2.18      plyr_1.8.9          stringi_1.8.4      \n[115] munsell_0.5.1       lisrelToR_0.3       pacman_0.5.1       \n[118] Matrix_1.7-1        IRkernel_1.3.2      hms_1.1.3          \n[121] glasso_1.11         shiny_1.9.1         igraph_2.1.1       \n[124] broom_1.0.7         RcppParallel_5.1.9  cherryblossom_0.1.0\n\n\n\n\n\n\nBonifay, W., Winter, S. D., Skoblow, H. F., & Watts, A. L. (2024). Good fit is weak evidence of replication: increasing rigor through prior predictive similarity checking. Assessment, 10731911241234118.\n\n\nRoberts, S., & Pashler, H. (2000). How persuasive is a good fit? A comment on theory testing. Psychological Review, 107(2), 358–367.\n\n\nWaller, N. G., & Meehl, P. E. (2002). Risky tests, verisimilitude, and path analysis. Psychological Methods, 7(3), 323–337. https://doi.org/10.1037/1082-989X.7.3.323",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Prior Predictive Model Checking</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_logistic_regr.html",
    "href": "chapters/mokken/01_logistic_regr.html",
    "title": "63  Modello di Regressione Logistica",
    "section": "",
    "text": "63.1 Introduzione\nPrima di affrontare i modelli parametrici e non parametrici della Teoria della Risposta all’Item (IRT), è fondamentale acquisire una solida comprensione del modello di regressione logistica. Questo modello, ampiamente utilizzato per l’analisi di dati categorici, rappresenta un punto di riferimento cruciale per comprendere i principi che i modelli IRT sviluppano ed estendono.\nLa regressione logistica consente di stimare la probabilità di un evento in funzione di una o più variabili predittive, fornendo una base teorica utile per interpretare la probabilità che un esaminando risponda correttamente a un item. Tuttavia, mentre il modello di regressione logistica si limita a considerare la relazione tra le variabili predittive e l’evento, i modelli IRT aggiungono un livello di complessità: integrano l’abilità individuale dell’esaminando e le caratteristiche specifiche degli item, permettendo un’analisi più articolata.\nNonostante alcune similitudini, i modelli IRT si differenziano profondamente dalla regressione logistica. Essi non solo modellano simultaneamente le proprietà degli item e le abilità degli individui, ma considerano anche le interdipendenze tra le risposte agli item e tra gli item stessi, superando l’assunzione di indipendenza tra le osservazioni tipica della regressione logistica.\nIn questo capitolo, esploreremo il modello di regressione logistica come punto di partenza per comprendere le basi teoriche e applicative dei modelli IRT, fornendo una transizione chiara e graduale verso l’approfondimento di questa teoria fondamentale.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_logistic_regr.html#modello-di-regressione-logistica-per-variabili-binarie",
    "href": "chapters/mokken/01_logistic_regr.html#modello-di-regressione-logistica-per-variabili-binarie",
    "title": "63  Modello di Regressione Logistica",
    "section": "\n63.2 Modello di Regressione Logistica per Variabili Binarie",
    "text": "63.2 Modello di Regressione Logistica per Variabili Binarie\nLa regressione logistica è uno strumento utile per analizzare la relazione tra una variabile dipendente dicotomica (che assume due valori, ad esempio “successo” e “fallimento”) e una o più variabili indipendenti (che possono essere sia quantitative che qualitative). È particolarmente utile per modellare situazioni in cui vogliamo stimare la probabilità che un evento si verifichi in base alle caratteristiche di un individuo o di una situazione.\n\n63.2.1 Definizione del Modello\nConsideriamo \\(n\\) osservazioni indipendenti, dove \\(Y_i\\) rappresenta l’osservazione \\(i\\)-esima della variabile dipendente (ad esempio, “successo” o “fallimento”), con \\(i = 1, \\dots, n\\). Ogni osservazione è associata a un insieme di variabili esplicative \\((x_{1}, \\dots, x_{p})\\). L’obiettivo è stimare la probabilità di successo, denotata con \\(\\pi_i\\), data la combinazione delle variabili esplicative:\n\\[\nP(Y = 1 \\mid X = x_i) = \\pi_i.\n\\]\nIn questo contesto, \\(Y\\) segue una distribuzione di Bernoulli, quindi può assumere solo due valori:\n\\[\ny_i =\n\\begin{cases}\n1 & \\text{se si verifica un successo (osservazione $i$)}, \\\\\n0 & \\text{se si verifica un fallimento.}\n\\end{cases}\n\\]\nLe probabilità associate sono:\n\n\n\\(\\pi_i\\) per il successo,\n\n\\(1 - \\pi_i\\) per il fallimento.\n\nLa media condizionata \\(\\mathbb{E}(Y \\mid X = x)\\) rappresenta la probabilità attesa di successo per un dato valore \\(x\\) delle variabili esplicative:\n\\[\n\\mathbb{E}(Y \\mid X = x) = \\pi_i.\n\\]\n\n63.2.2 Esempio Pratico\nImmaginiamo un dataset con 100 soggetti, in cui:\n\n\nage è una variabile esplicativa che indica l’età,\n\nchd è la variabile dipendente che indica la presenza (chd = 1) o l’assenza (chd = 0) di disturbi cardiaci.\n\nLa probabilità condizionata \\(\\pi_i\\) indica la probabilità di osservare disturbi cardiaci in un certo gruppo d’età:\n\\[\n\\pi_i = P(Y = 1 \\mid X = x).\n\\]\nPer valori discreti della variabile age, possiamo calcolare la proporzione di individui con \\(Y = 1\\) (cioè con disturbi cardiaci) per ogni gruppo di età. Queste proporzioni rappresentano una stima non parametrica della funzione di regressione tra chd e age.\n\n63.2.3 Visualizzazione dei Dati\nCon il dataset, possiamo calcolare queste proporzioni e rappresentarle graficamente:\n\nchdage &lt;- rio::import(\n    here::here(\"data\", \"logistic_reg\", \"chdage_dat.txt\")\n)\n\n# Calcolo delle proporzioni di Y = 1 (chd) per età\nprop_data &lt;- chdage %&gt;%\n    group_by(age) %&gt;%\n    summarise(prop_chd = mean(chd))\n\n# Creazione del grafico con smoothing\nggplot(prop_data, aes(x = age, y = prop_chd)) +\n    geom_point() + # Punti proporzione\n    geom_smooth(method = \"loess\", span = 0.7) + # Smoothing LOESS\n    labs(\n        title = \"Relazione tra Età e Disturbi Cardiaci\",\n        x = \"Età\",\n        y = \"Proporzione di CHD = 1\"\n    )\n\n\n\n\n\n\n\n\n63.2.4 Interpretazione della Relazione\nOsservando il grafico:\n\nPer valori bassi di age, la proporzione di Y = 1 (presenza di disturbi cardiaci) è vicina a 0.\nPer valori elevati di age, la proporzione di Y = 1 tende a 1.\nPer valori intermedi di age, la proporzione aumenta gradualmente, seguendo un andamento sigmoidale.\n\nQuesto andamento riflette la natura probabilistica del fenomeno: la probabilità di disturbi cardiaci cresce con l’età, ma non è una crescita lineare.\n\n63.2.5 Vantaggio del Modello Logistico\nSebbene la stima non parametrica (ad esempio LOESS) possa fornire un quadro generale, la regressione logistica permette di modellare questa relazione utilizzando una semplice funzione. Questo approccio è particolarmente vantaggioso quando ci sono più variabili esplicative, consentendo di quantificare come ciascuna contribuisce alla probabilità di un evento.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_logistic_regr.html#modello-lineare-nelle-probabilità",
    "href": "chapters/mokken/01_logistic_regr.html#modello-lineare-nelle-probabilità",
    "title": "63  Modello di Regressione Logistica",
    "section": "\n63.3 Modello Lineare nelle Probabilità",
    "text": "63.3 Modello Lineare nelle Probabilità\nIl modello lineare nelle probabilità rappresenta un primo approccio per descrivere la relazione tra una variabile dipendente binaria (ad esempio “successo” o “fallimento”) e una variabile indipendente continua o categoriale. La sua formulazione è data da:\n\\[\nY_i = \\alpha + \\beta X_i + \\varepsilon_i,\n\\]\ndove:\n\n\n\\(\\alpha\\) è l’intercetta,\n\n\\(\\beta\\) è il coefficiente di regressione,\n\n\\(\\varepsilon_i\\) rappresenta l’errore, che si assume distribuito normalmente con media 0 e varianza costante (\\(\\varepsilon_i \\sim \\mathcal{N}(0, 1)\\)).\n\nIl valore atteso di \\(Y_i\\) è quindi\n\\[\\mathbb{E}(Y_i) = \\alpha + \\beta X_i,\\]\nche porta alla stima della probabilità di successo \\(\\pi_i\\) come:\n\\[\n\\pi_i = \\alpha + \\beta X_i.\n\\]\n\n63.3.1 Limiti del Modello Lineare nelle Probabilità\nNonostante la semplicità, il modello lineare nelle probabilità presenta alcune problematiche.\n\nValori predetti fuori dall’intervallo [0,1]. Poiché \\(\\pi_i = \\alpha + \\beta X_i\\) è una funzione lineare, i valori predetti di \\(\\pi_i\\) possono essere negativi o superiori a 1, il che è incompatibile con l’interpretazione di \\(\\pi_i\\) come probabilità.\nAssunzione di normalità degli errori. La variabile dipendente \\(Y_i\\) è binaria (0 o 1), ma l’errore \\(\\varepsilon_i\\) non segue una distribuzione normale. Ad esempio, se \\(Y_i = 1\\), l’errore sarà:\n\n\\[\n\\varepsilon_i = 1 - \\mathbb{E}(Y_i) = 1 - (\\alpha + \\beta X_i) = 1 - \\pi_i.\n\\]\nAnalogamente, se \\(Y_i = 0\\), l’errore sarà:\n\\[\n\\varepsilon_i = 0 - \\mathbb{E}(Y_i) = 0 - (\\alpha + \\beta X_i) = - \\pi_i.\n\\]\nPertanto, gli errori sono dicotomici e non normali.\n\nProblemi di omoschedasticità.\n\nNel modello lineare nelle probabilità, la varianza degli errori dipende dalla media \\(\\pi_i\\), quindi non è costante. La varianza degli errori si calcola come:\n\\[\n\\mathbb{V}(\\varepsilon_i) = (1-\\pi_i)\\pi_i.\n\\] dove \\(\\pi_i\\) varia in funzione di \\(X_i\\). Dato che \\(\\pi_i\\) dipende da \\(x\\), ciò significa che la varianza non è costante in funzione di \\(x\\). Questa eteroschedasticità viola una delle assunzioni fondamentali del metodo dei minimi quadrati.\n\n\nLinearità irrealistica. La relazione tra \\(X_i\\) e la probabilità di successo non è sempre lineare nella realtà. Ad esempio, per valori estremi di \\(X_i\\), una relazione lineare può portare a predizioni improbabili (valori negativi o superiori a 1) e non cattura l’andamento sigmoidale tipico di molti fenomeni.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_logistic_regr.html#modello-lineare-nelle-probabilità-vincolato",
    "href": "chapters/mokken/01_logistic_regr.html#modello-lineare-nelle-probabilità-vincolato",
    "title": "63  Modello di Regressione Logistica",
    "section": "\n63.4 Modello Lineare nelle Probabilità Vincolato",
    "text": "63.4 Modello Lineare nelle Probabilità Vincolato\nUn tentativo di risolvere il problema dei valori predetti fuori dall’intervallo [0,1] consiste nell’introdurre vincoli:\n\\[\n\\pi=\n\\begin{cases}\n  0                           &\\text{se $\\alpha + \\beta X &lt; 0$},\\\\\n  \\alpha + \\beta X           &\\text{se $0 \\leq \\alpha + \\beta X \\leq 1$},\\\\\n  1 &\\text{se $\\alpha + \\beta X &gt; 1$}.\n\\end{cases}\n\\]\nTuttavia, questo approccio presenta diversi limiti:\n\n\nDipendenza critica dai valori estremi di \\(\\pi\\): I valori di \\(\\pi = 0\\) e \\(\\pi = 1\\) dipendono fortemente dai valori più bassi e più alti di \\(X_i\\), che possono variare tra campioni.\n\nCambiamenti bruschi nella pendenza: La curva di regressione subisce variazioni improvvise vicino agli estremi, risultando poco realistica.\n\nComplicazioni con più variabili esplicative: Quando il numero di variabili indipendenti aumenta, il modello diventa instabile e difficile da interpretare.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_logistic_regr.html#regressione-logistica",
    "href": "chapters/mokken/01_logistic_regr.html#regressione-logistica",
    "title": "63  Modello di Regressione Logistica",
    "section": "\n63.5 Regressione Logistica",
    "text": "63.5 Regressione Logistica\nLa regressione logistica offre una soluzione efficace per modellare probabilità, garantendo che i valori previsti siano sempre compresi nell’intervallo \\([0,1]\\). Invece di specificare un modello direttamente per le probabilità condizionate \\(\\pi_i\\), si definisce un modello lineare per una loro trasformazione: il logaritmo degli odds, noto come logit. Questa trasformazione risolve il problema del vincolo imposto dall’intervallo delle probabilità.\n\n63.5.1 Modello Logistico e Logit\nIl logit, definito come il logaritmo naturale del rapporto tra probabilità di successo e probabilità di fallimento, è dato da:\n\\[\n\\eta_i = \\log_e \\frac{\\pi_i}{1-\\pi_i} = \\alpha + \\beta x_i,\n\\]\ndove:\n\n\n\\(\\eta_i\\) è il logit, sempre un numero reale,\n\n\\(\\alpha\\) e \\(\\beta\\) sono i parametri del modello,\n\n\\(x_i\\) è la variabile esplicativa.\n\nQuesto approccio consente di modellare \\(\\pi_i\\) come una funzione non lineare di \\(x_i\\), espressa dalla funzione logistica:\n\\[\n\\pi_i = \\frac{1}{1 + e^{-(\\alpha + \\beta x_i)}}.\n\\]\n\n63.5.2 Caratteristiche del Modello Logistico\nIl modello logistico presenta i seguenti vantaggi:\n\n\nIntervallo limitato: garantisce che \\(\\pi_i\\) sia sempre compreso tra 0 e 1.\n\nRelazione sigmoidale: rappresenta una transizione fluida tra probabilità basse e alte in funzione di \\(x_i\\).\n\nAdatto a variabili dicotomiche: rispetta la natura della variabile dipendente.\n\n63.5.3 Relazione tra Probabilità, Odds e Logit\nLa relazione tra probabilità (\\(P\\)), odds (\\(O\\)) e logit (\\(L\\)) è illustrata nella tabella seguente:\n\n\n\n\n\n\n\nProbabilità (\\(P\\))\nOdds (\\(O = P / (1-P)\\))\nLogit (\\(L = \\ln(O)\\))\n\n\n\n0.01\n0.01 / 0.99 = 0.0101\n\\(-4.60\\)\n\n\n0.50\n0.50 / 0.50 = 1.0000\n\\(0.00\\)\n\n\n0.99\n0.99 / 0.01 = 99.0000\n\\(4.60\\)\n\n\n\nIl logit trasforma l’intervallo \\([0,1]\\) della probabilità in tutta la linea reale, semplificando l’uso di modelli lineari.\n\n63.5.4 Logit Empirici e Relazione Lineare\nPer visualizzare la relazione tra variabili trasformate, è possibile calcolare i logit empirici. Consideriamo un esempio con 8 intervalli della variabile age, calcolando il logit degli odds per ciascun gruppo. La relazione risultante è lineare, come mostrato dal seguente codice:\n\ndat1 &lt;- chdage %&gt;%\n    mutate(age_c = ntile(age, 8)) %&gt;%\n    group_by(age_c) %&gt;%\n    summarise(\n        age_bin_center = (min(age) + max(age)) / 2,\n        proportion_heart_disease = mean(chd)\n    )\n\nxc &lt;- dat1$age_bin_center\nyc &lt;- dat1$proportion_heart_disease\nlogit_y &lt;- log(yc / (1 - yc))\nfit &lt;- lm(logit_y ~ xc)\n\nplot(\n    xc, logit_y,\n    xlab = \"Età\", ylab = \"Logit(Y)\",\n    main = \"Relazione Lineare tra Logit e Età\", type = \"n\"\n)\npoints(xc, logit_y, cex = 2)\nabline(fit)\n\n\n\n\n\n\n\n\n63.5.5 Modello Logistico Applicato\nUtilizzando un modello logistico, possiamo rappresentare l’andamento sigmoidale della probabilità condizionata:\n\\[\n\\pi_i = \\frac{e^{\\alpha + \\beta x_i}}{1 + e^{\\alpha + \\beta x_i}}.\n\\]\nIn R, il modello può essere stimato come segue:\n\nfm &lt;- glm(chd ~ age, family = binomial(link = \"logit\"), data = chdage)\nlogit_hat &lt;- fm$coef[1] + fm$coef[2] * chdage$age\npi_hat &lt;- exp(logit_hat) / (1 + exp(logit_hat))\n\nplot(chdage$age, pi_hat,\n    xlab = \"Età\",\n    ylab = \"P(CHD)\",\n    main = \"Probabilità di Malattia Cardiaca\", type = \"n\"\n)\nlines(chdage$age, pi_hat)\npoints(dat1$age_bin_center, dat1$proportion_heart_disease, cex = 2)\n\n\n\n\n\n\n\nUn’alternativa per visualizzare i risultati è l’uso del pacchetto sjPlot:\n\nplot_model(fm, type = \"pred\", terms = \"age\") +\n  labs(y = \"Probabilità di Malattia Cardiaca\")\n\n\n\n\n\n\n\nIn conclusione, la regressione logistica rappresenta un metodo robusto e flessibile per modellare probabilità, superando i limiti del modello lineare nelle probabilità. La sua capacità di rappresentare relazioni non lineari e rispettare i vincoli probabilistici la rende ideale per l’analisi di variabili dipendenti binarie.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_logistic_regr.html#modelli-lineari-generalizzati-glm",
    "href": "chapters/mokken/01_logistic_regr.html#modelli-lineari-generalizzati-glm",
    "title": "63  Modello di Regressione Logistica",
    "section": "\n63.6 Modelli Lineari Generalizzati (GLM)",
    "text": "63.6 Modelli Lineari Generalizzati (GLM)\nI Modelli Lineari Generalizzati (GLM) rappresentano una potente estensione del modello lineare classico, progettata per affrontare i limiti che emergono con variabili risposta di natura non continua o con varianze non costanti. Nel caso di una variabile risposta binaria, come nel modello di regressione logistica, il modello lineare classico incontra diverse difficoltà:\n\n\nDistribuzione Binomiale: La variabile risposta \\(Y_i\\) segue una distribuzione binomiale (con parametro \\(n_i\\), tipicamente \\(n_i = 1\\) per dati individuali), incompatibile con l’assunzione di normalità.\n\nVincoli sulle Probabilità: Specificare un modello lineare come \\(\\pi_i = \\beta_0 + \\beta_1 x_i\\) può portare a stime di probabilità fuori dall’intervallo \\([0,1]\\).\n\nVarianze Non Costanti: La varianza dei residui, calcolata come \\(V(\\varepsilon_i) = \\pi_i (1 - \\pi_i)\\), varia in funzione di \\(\\pi_i\\).\n\nI GLM affrontano queste sfide consentendo di specificare una relazione tra la media attesa della variabile risposta e le variabili esplicative attraverso una funzione di legame. Questi modelli includono varianti come:\n\n\nRegressione Lineare: Per variabili dipendenti continue.\n\nRegressione Logistica: Per variabili risposta binarie.\n\nModello Loglineare di Poisson: Per conteggi o frequenze in tabelle di contingenza.\n\n\n63.6.1 Struttura dei GLM\nUn GLM si compone di tre elementi principali:\n\n\nComponente Aleatoria: Specifica la distribuzione della variabile risposta \\(Y_i\\), ad esempio:\n\nNormale per variabili continue,\nBinomiale per variabili binarie,\nPoisson per conteggi.\n\n\n\nComponente Sistematica: Definisce la relazione lineare tra le variabili esplicative e una trasformazione della media attesa della variabile risposta. È rappresentata dal predittore lineare:\n\\[\n\\eta_i = \\alpha + \\sum_{j} \\beta_j X_{ij}.\n\\]\n\nFunzione di Legame: Trasforma la media attesa \\(\\mathbb{E}(Y_i)\\) in modo che sia modellata linearmente rispetto a \\(\\eta_i\\). Ad esempio, nella regressione logistica, il legame è dato dal logit.\n\n\n\n\n\n\n\n\nComponente Aleatoria\nFunzione di Legame\nApplicazione\n\n\n\nGaussiana\nIdentità\nRegressione lineare\n\n\nBinomiale\nLogit\nRegressione logistica\n\n\nPoisson\nLogaritmo\nModello loglineare\n\n\n\n63.6.2 Componente Sistematica\nLa componente sistematica descrive come le variabili esplicative (\\(X_{ij}\\)) influenzano il predittore lineare \\(\\eta_i\\). Per \\(k\\) osservazioni e \\(p\\) variabili esplicative, il predittore lineare è definito come:\n\\[\n\\eta_i = \\alpha + \\sum_{j} \\beta_j X_{ij},\n\\]\ndove:\n\n\n\\(\\alpha\\) è l’intercetta,\n\n\\(\\beta_j\\) sono i coefficienti delle variabili esplicative.\n\n63.6.3 Componente Aleatoria\nLa componente aleatoria assume che le osservazioni \\(Y_i\\) siano realizzazioni indipendenti di una variabile casuale. Per una variabile risposta binaria:\n\\[\nY_i \\sim \\text{Bin}(n_i, \\pi_i),\n\\]\ndove \\(n_i = 1\\) per dati individuali.\n\n63.6.4 Funzione di Legame\nLa funzione di legame \\(g(\\cdot)\\) connette la media attesa \\(\\mathbb{E}(Y_i) = \\pi_i\\) alla componente sistematica \\(\\eta_i\\). Per la regressione logistica, il legame è dato dal logit:\n\\[\n\\eta_i = g(\\pi_i) = \\ln\\left(\\frac{\\pi_i}{1-\\pi_i}\\right).\n\\]\nLa funzione legame è invertibile, consentendo di esprimere la probabilità \\(\\pi_i\\) come funzione del predittore lineare:\n\\[\n\\pi_i = \\frac{e^{\\eta_i}}{1 + e^{\\eta_i}} = \\frac{e^{\\alpha + \\sum_j \\beta_j X_{ij}}}{1 + e^{\\alpha + \\sum_j \\beta_j X_{ij}}}.\n\\]\nQuesta relazione permette di ottenere un modello non lineare per le probabilità \\(\\pi_i\\).\n\n63.6.5 Visualizzazione della Funzione Logistica\nLa funzione logistica, che rappresenta il legame tra il predittore lineare \\(\\eta_i\\) e la probabilità \\(\\pi_i\\), ha un andamento sigmoidale:\n\nx &lt;- seq(-5, 5, length.out = 100)\nprob &lt;- plogis(x)  # Inversa del logit\nplot(x, prob, type = \"l\", \n     main = \"Funzione Logistica\", \n     ylab = \"Probabilità (\\u03c0)\", \n     xlab = \"Valori di \\u03b7\")\n\n\n\n\n\n\n\n\n63.6.6 Applicazioni dei GLM\nI GLM sono particolarmente utili in contesti in cui:\n\nLa variabile risposta non è continua (es. binaria o discreta),\nLe varianze non sono costanti,\nLa relazione tra media e predittore è non lineare.\n\nNella regressione logistica, la combinazione di queste componenti consente di descrivere in modo accurato la probabilità di successo in funzione delle variabili esplicative.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_logistic_regr.html#regressione-logistica-con-r",
    "href": "chapters/mokken/01_logistic_regr.html#regressione-logistica-con-r",
    "title": "63  Modello di Regressione Logistica",
    "section": "\n63.7 Regressione Logistica con R",
    "text": "63.7 Regressione Logistica con R\nLa regressione logistica può essere implementata in R utilizzando la funzione glm() (Generalized Linear Model). Questo metodo consente di stimare i parametri del modello, tenendo conto della distribuzione della variabile risposta e della funzione di legame appropriata.\nPer stimare i parametri del modello sui dati dell’esempio, si utilizza il seguente codice:\n\nfm &lt;- glm(chd ~ age,\n    family = binomial(link = \"logit\"),\n    data = chdage\n)\n\n\n\nfamily = binomial: Specifica che la variabile risposta segue una distribuzione binomiale (necessaria per una variabile binaria come chd).\n\nlink = \"logit\": Indica che la funzione di legame utilizzata è il logit.\n\nL’output del modello può essere visualizzato con la funzione summary():\n\nsummary(fm)\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = chd ~ age, family = binomial(link = \"logit\"), data = chdage)\n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)\n#&gt; (Intercept)  -5.3095     1.1337   -4.68  2.8e-06\n#&gt; age           0.1109     0.0241    4.61  4.0e-06\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 136.66  on 99  degrees of freedom\n#&gt; Residual deviance: 107.35  on 98  degrees of freedom\n#&gt; AIC: 111.4\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\nL’output mostra i coefficienti stimati (\\(\\alpha\\) e \\(\\beta\\)), i loro errori standard e altre statistiche. Per i dati dell’esempio, i risultati principali sono:\n\nIntercetta (\\(\\alpha = -5.309\\)): Indica il log-odds di sviluppare CHD a età 0.\nCoefficiente di Età (\\(\\beta = 0.111\\)): Indica come i log-odds di CHD cambiano per ogni anno aggiuntivo di età.\n\nLe equazioni risultanti sono:\n\nLogit stimato:\\[\n\\hat{\\eta}(x) = -5.309 + 0.111 \\cdot \\text{age}.\n\\]\nProbabilità stimata:\\[\n\\hat{\\pi}(x) = \\frac{e^{-5.309 + 0.111 \\cdot \\text{age}}}{1 + e^{-5.309 + 0.111 \\cdot \\text{age}}}.\n\\]\n\n\n63.7.1 Interpretazione dei Coefficienti\nLa comprensione dei coefficienti del modello di regressione logistica può essere suddivisa in tre livelli: log-odds, odds ratio e probabilità predette.\n\n63.7.2 Interpretazione Basata sui Log-Odds\n\n\nIntercetta (\\(-5.309\\)):\n\nRappresenta i log-odds di sviluppare CHD quando l’età è 0.\nUn valore negativo suggerisce che la probabilità di CHD è molto bassa a età 0.\n\n\n\nCoefficiente di Età (\\(0.111\\)):\n\nOgni anno aggiuntivo di età aumenta i log-odds di CHD di 0.111.\n\nUn coefficiente positivo indica che il rischio di CHD aumenta con l’età.\n\n\n\n63.7.3 Interpretazione Attraverso l’Odds Ratio\nPer una comprensione più intuitiva, il coefficiente di età può essere trasformato in un odds ratio esponenziando il valore del coefficiente:\n\\[\n\\text{Odds Ratio per Età} = e^{0.111} \\approx 1.12.\n\\]\n\n\nSignificato:\n\nUn odds ratio di 1.12 implica che per ogni anno di età in più, gli odds di sviluppare CHD aumentano del 12%.\nSe l’odds ratio fosse pari a 1, ciò indicherebbe che l’età non influisce sul rischio di CHD.\n\n\n\n63.7.4 Interpretazione Basata sulle Probabilità Predette\nIl modo più diretto per interpretare l’impatto delle variabili esplicative è attraverso le probabilità predette. Le probabilità mostrano come il rischio di CHD varia con l’età.\nPossiamo calcolare e visualizzare le probabilità predette per diverse età utilizzando il pacchetto jtools con la funzione effect_plot():\n\neffect_plot(fm,\n    pred = age, interval = TRUE, plot.points = TRUE,\n    jitter = 0.05\n)\n\n\n\n\n\n\n\n\n\npred = age: Indica che vogliamo calcolare le probabilità predette in funzione di age.\n\ninterval = TRUE: Aggiunge intervalli di confidenza per le stime.\n\nplot.points = TRUE: Mostra i punti osservati sui dati originali.\n\nQuesto grafico rappresenta la relazione sigmoidale tra età e probabilità di CHD, fornendo una rappresentazione intuitiva e accessibile anche a chi non ha una formazione avanzata in statistica.\nIn conclusione, la regressione logistica in R, tramite glm(), è uno strumento versatile per analizzare variabili binarie. L’interpretazione dei coefficienti attraverso log-odds, odds ratio e probabilità predette offre molteplici prospettive utili per comprendere l’effetto delle variabili esplicative e comunicare i risultati in modo chiaro ed efficace.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_logistic_regr.html#riflessioni-conclusive",
    "href": "chapters/mokken/01_logistic_regr.html#riflessioni-conclusive",
    "title": "63  Modello di Regressione Logistica",
    "section": "\n63.8 Riflessioni Conclusive",
    "text": "63.8 Riflessioni Conclusive\nNel caso di una variabile dipendente binaria \\(Y_i\\), il tradizionale modello di regressione lineare risulta inadatto, principalmente a causa della natura discreta di \\(Y_i\\), della varianza non costante e della necessità di vincolare i valori predetti all’intervallo \\([0, 1]\\). Queste limitazioni vengono superate applicando un modello lineare non direttamente alla probabilità \\(\\pi_i\\) (il valore atteso di \\(Y_i\\)), ma a una sua trasformazione: il logit.\nNel modello di regressione logistica, la componente sistematica esprime i logit, definiti come il logaritmo naturale degli odds, come una funzione lineare dei predittori:\n\\[\n\\ln \\frac{\\pi_i}{1-\\pi_i} = \\alpha + \\beta X_i.\n\\]\nQuesto rende il modello lineare nei logit, semplificando la relazione tra variabili esplicative e odds. La funzione logit è invertibile, e la trasformazione inversa (antilogit) consente di esprimere le probabilità \\(\\pi_i\\) in funzione del predittore lineare \\(\\eta_i = \\alpha + \\beta X_i\\):\n\\[\n\\pi_i = \\frac{\\exp(\\alpha + \\beta X_i)}{1 + \\exp(\\alpha + \\beta X_i)}.\n\\]\nQuesta relazione rende il modello non lineare rispetto alle probabilità, ma garantisce che i valori predetti rimangano nell’intervallo \\([0, 1]\\).\nNel contesto della regressione logistica, il valore atteso della variabile dipendente \\(Y_i\\), condizionato ai valori dei predittori, rappresenta la probabilità che \\(Y_i\\) assuma il valore 1:\n\\[\n\\mathbb{E}(Y \\mid x_i) = Pr(Y = 1 \\mid X = x_i) \\equiv \\pi_i.\n\\]\nQuesto valore può essere interpretato come la proporzione di individui nella popolazione con \\(Y = 1\\) per una data combinazione di valori \\(X = x_i\\).\nLa componente aleatoria del modello considera \\(Y_i\\) come una variabile aleatoria binomiale, con due scenari principali:\n\n\nDati raggruppati: Quando le osservazioni sono aggregate, la variabile risposta segue una distribuzione binomiale con parametro \\(n_i\\), dove \\(n_i\\) rappresenta il numero di osservazioni per ogni gruppo omogeneo di predittori.\n\nDati individuali: Quando ogni osservazione è indipendente, \\(n_i = 1\\) per tutte le unità.\n\nLa funzione logistica:\n\\[\n\\Lambda(\\eta) = \\frac{\\exp(\\eta)}{1 + \\exp(\\eta)},\n\\]\nè stata scelta come funzione di legame per trasformare il predittore lineare \\(\\eta_i = \\alpha + \\beta X_i\\) nelle probabilità \\(\\pi_i\\). Questa funzione è preferita per la sua semplicità interpretativa e per il fatto che produce un andamento sigmoidale, che descrive bene molte relazioni probabilistiche.\nIn conclusione, il modello di regressione logistica risolve elegantemente le limitazioni del modello lineare applicato a variabili binarie, fornendo un approccio flessibile e interpretabile:\n\n\nLineare nei logit: Il modello sfrutta la semplicità di una relazione lineare per descrivere log-odds.\n\nNon lineare nelle probabilità: La funzione logistica garantisce che le probabilità predette siano sempre comprese nell’intervallo \\([0, 1]\\).\n\nAdatto a variabili binarie: La componente aleatoria binomiale riflette la natura discreta della variabile dipendente.\n\nQuesto modello si dimostra particolarmente utile in ambiti dove le variabili risposta sono dicotomiche, offrendo interpretazioni intuitive tramite logit, odds e probabilità. La sua flessibilità consente di essere applicato sia a dati individuali sia a dati raggruppati, rendendolo uno strumento fondamentale per analisi statistiche moderne.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_logistic_regr.html#session-info",
    "href": "chapters/mokken/01_logistic_regr.html#session-info",
    "title": "63  Modello di Regressione Logistica",
    "section": "\n63.9 Session Info",
    "text": "63.9 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] sjPlot_2.8.17        jtools_2.3.0         effects_4.2-2       \n#&gt;  [4] gmodels_2.19.1       LaplacesDemon_16.1.6 car_3.1-3           \n#&gt;  [7] carData_3.0-5        ggokabeito_0.1.0     see_0.10.0          \n#&gt; [10] MASS_7.3-65          viridis_0.6.5        viridisLite_0.4.2   \n#&gt; [13] ggpubr_0.6.0         ggExtra_0.10.1       gridExtra_2.3       \n#&gt; [16] patchwork_1.3.0      bayesplot_1.11.1     semTools_0.5-6      \n#&gt; [19] semPlot_1.1.6        lavaan_0.6-19        psych_2.4.12        \n#&gt; [22] scales_1.3.0         markdown_1.13        knitr_1.49          \n#&gt; [25] lubridate_1.9.4      forcats_1.0.0        stringr_1.5.1       \n#&gt; [28] dplyr_1.1.4          purrr_1.0.4          readr_2.1.5         \n#&gt; [31] tidyr_1.3.1          tibble_3.2.1         ggplot2_3.5.1       \n#&gt; [34] tidyverse_2.0.0      here_1.0.1          \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] splines_4.4.2       later_1.4.1         R.oo_1.27.0        \n#&gt;   [4] datawizard_1.0.0    XML_3.99-0.18       rpart_4.1.24       \n#&gt;   [7] lifecycle_1.0.4     Rdpack_2.6.2        rstatix_0.7.2      \n#&gt;  [10] rprojroot_2.0.4     globals_0.16.3      lattice_0.22-6     \n#&gt;  [13] insight_1.0.2       rockchalk_1.8.157   backports_1.5.0    \n#&gt;  [16] survey_4.4-2        magrittr_2.0.3      openxlsx_4.2.8     \n#&gt;  [19] Hmisc_5.2-2         rmarkdown_2.29      httpuv_1.6.15      \n#&gt;  [22] qgraph_1.9.8        zip_2.3.2           RColorBrewer_1.1-3 \n#&gt;  [25] pbapply_1.7-2       DBI_1.2.3           minqa_1.2.8        \n#&gt;  [28] multcomp_1.4-28     abind_1.4-8         quadprog_1.5-8     \n#&gt;  [31] R.utils_2.13.0      nnet_7.3-20         TH.data_1.1-3      \n#&gt;  [34] sandwich_3.1-1      listenv_0.9.1       gdata_3.0.1        \n#&gt;  [37] arm_1.14-4          performance_0.13.0  parallelly_1.42.0  \n#&gt;  [40] codetools_0.2-20    tidyselect_1.2.1    ggeffects_2.2.0    \n#&gt;  [43] farver_2.1.2        lme4_1.1-36         broom.mixed_0.2.9.6\n#&gt;  [46] stats4_4.4.2        base64enc_0.1-3     jsonlite_1.9.0     \n#&gt;  [49] Formula_1.2-5       survival_3.8-3      emmeans_1.10.7     \n#&gt;  [52] tools_4.4.2         rio_1.2.3           Rcpp_1.0.14        \n#&gt;  [55] glue_1.8.0          mnormt_2.1.1        mgcv_1.9-1         \n#&gt;  [58] xfun_0.51           withr_3.0.2         fastmap_1.2.0      \n#&gt;  [61] mitools_2.4         boot_1.3-31         digest_0.6.37      \n#&gt;  [64] mi_1.1              timechange_0.3.0    R6_2.6.1           \n#&gt;  [67] mime_0.12           estimability_1.5.1  colorspace_2.1-1   \n#&gt;  [70] gtools_3.9.5        jpeg_0.1-10         R.methodsS3_1.8.2  \n#&gt;  [73] generics_0.1.3      data.table_1.17.0   corpcor_1.6.10     \n#&gt;  [76] htmlwidgets_1.6.4   pkgconfig_2.0.3     sem_3.1-16         \n#&gt;  [79] gtable_0.3.6        furrr_0.3.1         htmltools_0.5.8.1  \n#&gt;  [82] png_0.1-8           snakecase_0.11.1    reformulas_0.4.0   \n#&gt;  [85] rstudioapi_0.17.1   tzdb_0.4.0          reshape2_1.4.4     \n#&gt;  [88] coda_0.19-4.1       checkmate_2.3.2     nlme_3.1-167       \n#&gt;  [91] nloptr_2.1.1        zoo_1.8-13          sjlabelled_1.2.0   \n#&gt;  [94] parallel_4.4.2      miniUI_0.1.1.1      foreign_0.8-88     \n#&gt;  [97] pillar_1.10.1       grid_4.4.2          vctrs_0.6.5        \n#&gt; [100] promises_1.3.2      OpenMx_2.21.13      xtable_1.8-4       \n#&gt; [103] cluster_2.1.8       htmlTable_2.4.3     evaluate_1.0.3     \n#&gt; [106] pbivnorm_0.6.0      mvtnorm_1.3-3       cli_3.6.4          \n#&gt; [109] kutils_1.73         compiler_4.4.2      rlang_1.1.5        \n#&gt; [112] ggsignif_0.6.4      labeling_0.4.3      fdrtool_1.2.18     \n#&gt; [115] plyr_1.8.9          sjmisc_2.8.10       stringi_1.8.4      \n#&gt; [118] pander_0.6.6        munsell_0.5.1       lisrelToR_0.3      \n#&gt; [121] pacman_0.5.1        Matrix_1.7-2        sjstats_0.19.0     \n#&gt; [124] hms_1.1.3           glasso_1.11         future_1.34.0      \n#&gt; [127] shiny_1.10.0        haven_2.5.4         rbibutils_2.3      \n#&gt; [130] igraph_2.1.4        broom_1.0.7         RcppParallel_5.1.10",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_core_issues.html",
    "href": "chapters/mokken/02_core_issues.html",
    "title": "65  Analisi della Scala di Mokken",
    "section": "",
    "text": "65.1 Introduzione\nL’Analisi delle Scale di Mokken (MSA), sviluppata dal ricercatore olandese Robert J. Mokken, è un metodo psicometrico utilizzato per verificare se un test misura in modo coerente un costrutto psicologico latente, come l’ansia o l’autostima. Si basa sulla Teoria Non Parametrica della Risposta agli Item (NIRT), una branca della psicometria che analizza le risposte delle persone agli item di un test per dedurre caratteristiche non direttamente osservabili, chiamate costrutti latenti.\nUn costrutto latente, come l’ansia, non può essere osservato direttamente, ma viene stimato attraverso le risposte degli individui agli item di un test. L’idea principale è che queste risposte riflettano la posizione dell’individuo lungo un continuum (ad esempio, da “poco ansioso” a “molto ansioso”). Tuttavia, nella pratica, la relazione tra le risposte agli item e il costrutto latente può essere complessa e non seguire un andamento matematico preciso.\nLa MSA consente di verificare:\nA differenza dei modelli parametrici della Teoria della Risposta agli Item (IRT), la MSA adotta un approccio non parametrico, che offre maggiore flessibilità. Non richiede di assumere una relazione matematica specifica (ad esempio, una curva logistica) tra il costrutto latente e le risposte agli item. Questa caratteristica rende la MSA particolarmente utile quando:\nLa MSA amplia il modello di Guttman, basato sull’idea di perfetta cumulatività: una persona che risponde correttamente a un item più difficile dovrebbe rispondere correttamente anche a tutti gli item più semplici. Sebbene utile per creare scale gerarchiche, questo principio risulta spesso troppo rigido nella pratica, poiché non tiene conto delle variazioni naturali nei dati reali.\nLa MSA introduce un approccio probabilistico per affrontare queste complessità. Consente alcune deviazioni dalla perfetta cumulatività, rendendo il modello più realistico e adattabile a situazioni in cui le risposte ai test sono influenzate da fattori esterni, come la motivazione, la comprensione degli item o la stanchezza.\nUna caratteristica distintiva della MSA è che la forma della relazione tra il livello del tratto latente (\\(\\theta\\)) e la probabilità di rispondere correttamente a un item (nota come funzione di risposta all’item, IRF) non deve seguire una forma matematica specifica, come avviene nei modelli parametrici della Teoria della Risposta agli Item (IRT). Tuttavia, pur non imponendo vincoli rigidi alla forma delle IRF, la MSA introduce requisiti di ordinamento per item e rispondenti, che impongono alcune restrizioni sulle caratteristiche delle risposte agli item. Questi requisiti permettono di mantenere la coerenza nell’interpretazione del costrutto latente, pur lasciando spazio per variazioni nei dati reali.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_core_issues.html#introduzione",
    "href": "chapters/mokken/02_core_issues.html#introduzione",
    "title": "65  Analisi della Scala di Mokken",
    "section": "",
    "text": "Se gli item di un test sono coerenti tra loro e misurano lo stesso costrutto psicologico (concetto di omogeneità).\n\nSe è possibile ordinare sia le persone sia gli item lungo una scala, dalla persona con il punteggio più basso a quella con il punteggio più alto.\n\n\n\ni dati non soddisfano le ipotesi richieste dai modelli parametrici;\n\nsi preferisce un’analisi più semplice e robusta, senza ricorrere a formule matematiche complesse.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_core_issues.html#assunzioni-dellanalisi-delle-scale-di-mokken",
    "href": "chapters/mokken/02_core_issues.html#assunzioni-dellanalisi-delle-scale-di-mokken",
    "title": "65  Analisi della Scala di Mokken",
    "section": "\n65.2 Assunzioni dell’Analisi delle Scale di Mokken",
    "text": "65.2 Assunzioni dell’Analisi delle Scale di Mokken\nLe principali assunzioni della MSA sono descritte di seguito.\n\n65.2.1 Assunzione di Unidimensionalità\nL’assunzione di unidimensionalità richiede che le risposte agli item siano governate da un unico tratto latente, ovvero che tutti gli item della scala misurino lo stesso costrutto psicologico. Questo tratto rappresenta una variabile sottostante non osservabile che determina le risposte ai singoli item. Nella pratica, progettare scale che misurano un unico tratto latente facilita l’interpretazione dei punteggi, riduce la complessità analitica e aumenta la validità delle conclusioni.\n\n65.2.2 Assunzione di Indipendenza Locale\nL’indipendenza locale è un concetto fondamentale nella MSA e nella IRT. In parole semplici, questa assunzione dice che, una volta noto il livello del tratto latente \\(\\theta\\) (ad esempio, un’abilità o un tratto psicologico), le risposte di una persona ai diversi item di un test non devono influenzarsi a vicenda.\nImmagina un test composto da più domande, tutte progettate per misurare lo stesso tratto latente \\(\\theta\\), come la competenza matematica. L’indipendenza locale afferma che:\n\nle risposte alle singole domande dipendono solo dal livello di \\(\\theta\\) della persona;\nle risposte a una domanda non sono condizionate da come la persona ha risposto alle altre domande.\n\nIn pratica, ogni domanda misura esclusivamente \\(\\theta\\) senza essere influenzata dal contesto o da altre risposte.\nQuesta proprietà può essere descritta con una formula:\n\\[\nP(X = x \\mid \\theta) = \\prod_{i=1}^k P(X_i = x_i \\mid \\theta),\n\\]\ndove:\n\n\n\\(P(X = x \\mid \\theta)\\) è la probabilità che una persona risponda in un certo modo a tutte le domande, dato il suo livello di \\(\\theta\\).\n\n\\(P(X_i = x_i \\mid \\theta)\\) è la probabilità di rispondere a una singola domanda \\(i\\), dato il livello di \\(\\theta\\).\n\n\\(\\prod\\) indica che moltiplichiamo insieme le probabilità per tutte le domande del test.\n\nQuesta formula mostra che, dato \\(\\theta\\), le risposte sono indipendenti tra loro.\nL’indipendenza locale ha due implicazioni fondamentali:\n\nLe risposte non sono correlate (dato \\(\\theta\\)):\nUna volta noto il livello di \\(\\theta\\), non esiste relazione tra le risposte a domande diverse. Se osserviamo delle correlazioni, sono dovute esclusivamente alla variabilità di \\(\\theta\\).\n\nLe risposte riflettono solo \\(\\theta\\):\nLe risposte non devono essere influenzate da altri fattori, come:\n\nsimilarità tra i contenuti delle domande;\neffetti dell’ordine delle domande o stanchezza;\nindizi presenti in alcune domande che aiutano a rispondere ad altre.\n\n\n\nSe questa assunzione viene violata, il test potrebbe non rappresentare accuratamente il tratto latente \\(\\theta\\). Alcuni esempi comuni di violazione includono:\n\nDomande troppo simili possono creare correlazioni non legate a \\(\\theta\\). Ad esempio, se due item riguardano frazioni e decimali, la conoscenza di uno potrebbe influenzare l’altro.\nStanchezza, distrazione o l’ordine delle domande potrebbero modificare il modo in cui una persona risponde.\nRispondere a una domanda potrebbe fornire informazioni utili per rispondere a un’altra.\n\nL’indipendenza locale garantisce che il test misuri in modo accurato il tratto latente \\(\\theta\\). Quando questa assunzione è rispettata, possiamo essere certi che le risposte riflettano solo \\(\\theta\\) e non altri fattori esterni. Tuttavia, nella pratica, l’indipendenza locale può essere compromessa. Per questo motivo, è essenziale verificare che sia rispettata, così da assicurare la validità del modello psicometrico (sia in MSA che in IRT).\n\n65.2.3 Assunzione di Monotonicità Latente\nL’assunzione di monotonicità latente garantisce che, all’aumentare del tratto latente (\\(\\theta\\)), la probabilità di rispondere correttamente a un item (per domande dicotomiche) o di scegliere una categoria di risposta più alta (per domande politomiche) aumenti o rimanga costante, senza mai diminuire.\nMatematicamente, si esprime così:\n\\[\nP_i(\\theta_a) \\leq P_i(\\theta_b) \\quad \\text{se} \\quad \\theta_a \\leq \\theta_b ,\n\\]\ndove:\n\n\n\\(P_i(\\theta)\\) è la probabilità di rispondere correttamente o di scegliere una risposta più alta per l’item \\(i\\);\n\n\\(\\theta_a\\) e \\(\\theta_b\\) rappresentano due livelli del tratto latente.\n\nIn altre parole, questa proprietà assicura che individui con livelli più alti di \\(\\theta\\) abbiano una probabilità maggiore o uguale di ottenere punteggi migliori rispetto a quelli con livelli più bassi. Ciò garantisce una relazione coerente tra il tratto latente e le risposte al test, rendendo la scala interpretabile e robusta.\n\n65.2.3.1 Come verificare la monotonicità\nPer valutare la monotonicità si analizza la relazione tra il tratto latente (o una sua stima) e la probabilità di rispondere correttamente a un item. Un approccio comune utilizza il restscore, che rappresenta il punteggio totale di un partecipante sulle risposte agli altri item del test, escludendo l’item che si sta analizzando. Il restscore fornisce una stima indiretta del livello del tratto latente.\n\n65.2.3.2 Esempio di calcolo del restscore\nSupponiamo che un test abbia 10 item e che un partecipante abbia risposto così:\n\n\nItem\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nRisposta\n1\n1\n0\n1\n0\n1\n1\n1\n0\n1\n\n\nSe vogliamo calcolare il restscore per l’item 10, sommiamo le risposte agli altri 9 item:\n\\[\n\\text{Restscore} = 1 + 1 + 0 + 1 + 0 + 1 + 1 + 1 + 0 = 6.\n\\]\nIl restscore indica la prestazione complessiva del partecipante senza considerare l’item analizzato.\n\n65.2.3.3 Verifica della monotonicità con le Funzioni di Risposta agli Item (IRF)\nPer confermare la monotonicità, si costruiscono le Funzioni di Risposta agli Item (IRF), che mostrano graficamente come varia la probabilità di rispondere correttamente a un item al crescere del restscore. Una funzione monotona è non decrescente, cioè la probabilità aumenta o rimane costante.\n\n65.2.3.4 Esempio\nConsideriamo un test con 5 item e i seguenti dati relativi all’item 5:\n\n\n\n\n\n\n\n\nRestscore\nFrequenza Totale\nRisposte Corrette\nProbabilità Correttezza\n\n\n\n0\n5\n1\n\\(1/5 = 0.20\\)\n\n\n1\n10\n3\n\\(3/10 = 0.30\\)\n\n\n2\n15\n6\n\\(6/15 = 0.40\\)\n\n\n3\n10\n5\n\\(5/10 = 0.50\\)\n\n\n4\n5\n4\n\\(4/5 = 0.80\\)\n\n\n\nIn questo caso, la probabilità di risposta corretta aumenta (0.20, 0.30, 0.40, 0.50, 0.80), confermando la monotonicità. Questa relazione può essere visualizzata graficamente:\n\nrestscore &lt;- c(0, 1, 2, 3, 4)\nprob_correct &lt;- c(0.20, 0.30, 0.40, 0.50, 0.80)\n\n# Grafico\nplot(restscore, prob_correct,\n    type = \"b\", pch = 16, col = \"blue\",\n    xlab = \"Restscore\", ylab = \"Probabilità di Risposta Corretta\",\n    main = \"Funzione di Risposta all'Item (IRF)\"\n)\n\n\n\n\n\n\n\nIl grafico dovrebbe mostrare una curva crescente o piatta, segnalando che la monotonicità è rispettata.\n\n65.2.3.5 Aggregazione dei restscore\nSe il numero di partecipanti con un determinato restscore è troppo basso, le probabilità calcolate potrebbero essere instabili. In questi casi, è utile aggregare i restscore in gruppi.\nEsempio:\n\n\n\n\n\n\n\n\nGruppo Restscore\nFrequenza Totale\nRisposte Corrette\nProbabilità Correttezza\n\n\n\n0-2\n30\n10\n\\(10/30 = 0.33\\)\n\n\n3-4\n15\n9\n\\(9/15 = 0.60\\)\n\n\n\nQuesta aggregazione migliora l’affidabilità delle stime mantenendo il focus sulla relazione tra restscore e probabilità.\n\n65.2.3.6 Monotonicità e coefficienti di scalabilità\nNella MSA, la monotonicità viene analizzata anche tramite i coefficienti di scalabilità (\\(H_i\\) per i singoli item e \\(H_{ij}\\) per le coppie di item). Questi coefficienti verificano se gli item rispettano le assunzioni del modello, come l’omogeneità e, indirettamente, la monotonicità.\nNella MSA esistono tre principali tipi di coefficienti di scalabilità:\n\n\n\\(H_i\\): Coefficiente di scalabilità per un singolo item\nMisura quanto un item specifico contribuisce alla scala totale. Indica se le risposte a un dato item sono coerenti con il costrutto latente misurato dalla scala.\n\n\nInterpretazione:\n\n\n\\(H_i &gt; 0.30\\): l’item è considerato accettabile.\n\n\n\\(H_i &gt; 0.40\\): l’item è considerato buono.\n\n\n\\(H_i &gt; 0.50\\): l’item è considerato molto buono.\n\n\n\n\n\\(H_{ij}\\): Coefficiente di scalabilità per una coppia di item\nMisura la relazione tra due item specifici in termini di coerenza con il costrutto latente. Questo coefficiente verifica se le risposte a una coppia di item mostrano una relazione coerente con il tratto latente misurato dalla scala.\n\n\\(H\\): Coefficiente di scalabilità per l’intera scala\nÈ la media dei coefficienti di scalabilità di tutti gli item e misura quanto bene la scala complessiva rappresenta il costrutto latente.\n\n\nInterpretazione:\n\n\n\\(H &gt; 0.30\\): la scala è accettabile.\n\n\n\\(H &gt; 0.40\\): la scala è buona.\n\n\n\\(H &gt; 0.50\\): la scala è molto buona.\n\n\n\n\n\n65.2.3.7 Formula del coefficiente di scalabilità\nIl coefficiente di scalabilità per un item \\(i\\) (\\(H_i\\)) è calcolato come:\n\\[\nH_i = \\frac{\\sum_{j \\neq i} \\text{Cov}(X_i, X_j)}{\\sum_{j \\neq i} \\text{Var}(X_i)},\n\\]\ndove:\n\n\n\\(\\text{Cov}(X_i, X_j)\\) è la covarianza tra le risposte agli item \\(i\\) e \\(j\\);\n\n\\(\\text{Var}(X_i)\\) è la varianza delle risposte all’item \\(i\\);\nla sommatoria \\(\\sum_{j \\neq i}\\) indica che vengono considerati tutti gli item tranne \\(i\\).\n\nIn termini semplici, \\(H_i\\) misura quanto le risposte a un item sono correlate con le risposte agli altri item, rispetto alla variabilità interna dell’item stesso.\n\n65.2.3.8 Relazione tra monotonicità e coefficienti di scalabilità\nI coefficienti di scalabilità verificano l’omogeneità degli item (cioè, se misurano lo stesso costrutto latente), ma non garantiscono automaticamente la monotonicità. Ad esempio:\n\n\n\\(H_i &gt; 0.30\\) indica che un item contribuisce in modo rilevante alla scala, ma non assicura che la probabilità di rispondere correttamente all’item cresca monotonamente con il livello del tratto latente (\\(\\theta\\)).\n\n\nViolazioni della monotonicità possono verificarsi anche in presenza di coefficienti di scalabilità elevati. Pertanto, è necessario analizzare le Funzioni di Risposta agli Item (IRF) per verificare che la probabilità di risposte corrette o di scelte superiori aumenti in modo non decrescente con il tratto latente.\n\n65.2.3.9 Esempio pratico\nSupponiamo di avere una scala con tre item (\\(X_1\\), \\(X_2\\), \\(X_3\\)) e i seguenti coefficienti di scalabilità calcolati:\n\n\nItem\n\\(H_i\\)\nNote\n\n\n\n\\(X_1\\)\n0.45\nL’item contribuisce bene alla scala.\n\n\n\\(X_2\\)\n0.32\nL’item è accettabile.\n\n\n\\(X_3\\)\n0.28\nL’item potrebbe non essere adeguato.\n\n\n\nIn questo caso:\n\n\n\\(X_1\\) e \\(X_2\\) sono sufficientemente scalabili, mentre \\(X_3\\) potrebbe essere problematico.\n\nLa monotonicità deve comunque essere verificata per ciascun item, anche per \\(X_1\\), che ha un valore elevato di \\(H_i\\).\n\nIn conclusione, i coefficienti di scalabilità sono strumenti essenziali per valutare la qualità degli item in una scala, misurando la loro capacità di rappresentare il costrutto latente. Tuttavia, non garantiscono che la monotonicità sia rispettata. Per costruire una scala psicometrica valida e robusta, è necessario combinare l’analisi dei coefficienti di scalabilità con la verifica delle Funzioni di Risposta agli Item (IRF).\n\n65.2.4 Assunzione di Non-Intersezione delle Funzioni di Risposta\nL’assunzione di non-intersezione delle funzioni di risposta (IRF) prevede che le probabilità di successo su item più difficili non superino mai quelle relative a item più facili, per ogni livello del tratto latente. In altre parole, le IRF devono essere ordinate in modo che la probabilità di rispondere correttamente a un item più difficile sia sempre inferiore o uguale rispetto a un item meno difficile. Formalmente, questa proprietà può essere espressa come:\n\\[\nP_1(\\theta) \\leq P_2(\\theta) \\leq ... \\leq P_k(\\theta) \\quad \\text{per ogni} \\ \\theta .\n\\]\nL’intersezione delle IRF comporterebbe una violazione dell’ordinamento degli item, il che renderebbe difficile interpretare i risultati della scala.\nIn sintesi, l’Analisi delle Scale di Mokken si basa su assunzioni chiave simili a quelle dell’IRT, ma le implementa in un contesto non parametrico. La verifica di queste assunzioni garantisce la validità delle scale costruite e la corretta interpretazione dei risultati, rendendo l’MSA uno strumento potente per la costruzione di scale psicometriche robuste.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_core_issues.html#modelli-della-mokken-scale-analysis",
    "href": "chapters/mokken/02_core_issues.html#modelli-della-mokken-scale-analysis",
    "title": "65  Analisi della Scala di Mokken",
    "section": "\n65.3 Modelli della Mokken Scale Analysis",
    "text": "65.3 Modelli della Mokken Scale Analysis\nDalle suddette assunzioni derivano due modelli della MSA:\n\nModello di Monotonicità Omogenea (Mokken, 1971): rispetta le prime tre assunzioni (unidimensionalità, indipendenza locale e monotonicità latente). Questo modello permette di ordinare i rispondenti in base al tratto latente.\nModello di Doppia Monotonicità: rispetta tutte e quattro le assunzioni (unidimensionalità, indipendenza locale, monotonicità latente e non-intersezione). Consente di ordinare non solo i rispondenti, ma anche gli item in termini di difficoltà.\n\n\n65.3.1 Errori Standard nei Coefficienti di Scalabilità\nGli errori standard (SE) sono essenziali per interpretare i coefficienti di scalabilità (\\(H\\), \\(H_j\\), \\(H_{ij}\\)) nell’Analisi delle Scale di Mokken (MSA). Forniscono una misura dell’incertezza associata alle stime e aiutano a valutare quanto il valore stimato rappresenti accuratamente il coefficiente reale nella popolazione.\n\n65.3.1.1 Importanza degli Errori Standard\nUn errore standard elevato rispetto al coefficiente stimato indica maggiore incertezza. Ad esempio, se \\(H_j = 0.30\\) e \\(SE = 0.08\\), il coefficiente stimato potrebbe essere inferiore alla soglia accettabile di 0.30 nella popolazione, sollevando dubbi sulla scalabilità dell’item.\nGrazie agli errori standard, è possibile calcolare intervalli di confidenza (CI) per quantificare la precisione della stima. Per un livello di confidenza del 95%, la formula è:\n\\[\n\\text{95\\% CI} = H_j \\pm (1.96 \\times SE),\n\\]\ndove:\n\n\n\\(H_j\\) è il coefficiente stimato.\n\n\\(1.96\\) è il valore critico associato al 95% di confidenza.\n\n\\(SE\\) è l’errore standard.\n\nL’intervallo di confidenza descrive l’incertezza della stima e indica che, su un gran numero di campioni estratti dalla stessa popolazione, il 95% degli intervalli calcolati includerà il vero valore del coefficiente.\n\n65.3.1.2 Esempio di Calcolo del CI\nSupponiamo che un coefficiente di scalabilità abbia \\(H_j = 0.30\\) e un errore standard di \\(SE = 0.10\\). Il 95% CI è:\n\\[\n\\text{95\\% CI} = 0.30 \\pm (1.96 \\times 0.10) = [0.10, 0.50].\n\\]\nQuesto intervallo mostra che, se ripetessimo il campionamento molte volte, il valore reale di \\(H_j\\) sarebbe compreso tra 0.10 e 0.50 nel 95% dei casi. Tuttavia, un intervallo così ampio riflette una stima relativamente incerta, e la possibilità che \\(H_j\\) sia sotto la soglia accettabile di 0.30 richiede ulteriori considerazioni.\n\n65.3.2 Fattori che Influenzano l’Errore Standard\nDiversi elementi influenzano la dimensione dell’errore standard, tra cui:\n\n\nDimensione del campione:\n\nCampioni più grandi riducono l’errore standard, migliorando la precisione delle stime.\n\nNei campioni piccoli, l’incertezza è maggiore, rendendo difficile interpretare \\(H_j\\) con precisione.\n\n\n\nDistribuzione dei punteggi degli item:\n\nDistribuzioni sbilanciate (ad esempio, molte risposte estreme come tutto 0 o tutto 1) aumentano l’errore standard.\n\nUna distribuzione più uniforme dei punteggi garantisce una stima più stabile.\n\n\n\nEterogeneità degli item:\n\nSe gli item non misurano lo stesso costrutto o presentano deviazioni significative dal modello, i coefficienti di scalabilità diventano meno affidabili e i SE aumentano.\n\n\n\n65.3.3 Utilizzo Pratico degli Errori Standard\nGli errori standard offrono informazioni cruciali per decidere se un item contribuisce in modo adeguato alla scala. Se \\(H_j\\) è basso o il suo intervallo di confidenza include valori inferiori a 0.30, l’item potrebbe non essere idoneo.\n\n65.3.3.1 Interpretazione del CI\n\nSe il CI è interamente sopra 0.30:\nL’item è probabilmente scalabile e contribuisce in modo adeguato alla scala.\nSe il CI include valori sotto 0.30:\nC’è incertezza sulla scalabilità dell’item, e potrebbe essere necessaria una revisione.\nSe il CI è interamente sotto 0.30:\nL’item non è scalabile e dovrebbe essere considerato per la rimozione, a meno che non misuri aspetti unici e rilevanti del costrutto.\n\n65.3.4 Eliminazione degli Item con Bassi Coefficienti di Scalabilità\nMokken (1971) ha sottolineato che item con bassi coefficienti di scalabilità (\\(H_j &lt; 0.30\\)) spesso introducono errori nel modello, riducendo la coerenza della scala. Tuttavia, rimuovere automaticamente questi item può avere conseguenze sia positive sia negative:\n\n\nPro:\n\nMigliora la coerenza interna della scala.\n\nAumenta l’affidabilità psicometrica.\n\n\n\nContro:\n\nRiduce la copertura del costrutto.\n\nPuò eliminare aspetti teoricamente rilevanti del costrutto latente.\n\n\n\nPer evitare problemi, la decisione di eliminare un item dovrebbe considerare non solo i coefficienti di scalabilità e i SE, ma anche la validità teorica dell’item.\nIn conclusione, gli errori standard nei coefficienti di scalabilità sono strumenti essenziali per valutare l’incertezza delle stime e la qualità degli item in una scala psicometrica. Attraverso l’analisi di \\(SE\\), CI e coefficienti di scalabilità, i ricercatori possono prendere decisioni più informate sulla costruzione e sulla revisione delle scale, bilanciando le esigenze psicometriche con quelle teoriche.\n\nEsempio 65.1 Item con basso \\(H_j\\): se \\(H_j = 0.20\\) e il 95% CI è \\([0.05, 0.35]\\), il coefficiente è inferiore alla soglia accettabile di 0.30. Tuttavia, prima di eliminare l’item, si dovrebbe verificare se:\n\nl’item copre un aspetto importante del costrutto;\nla sua rimozione influisce sull’affidabilità della scala.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_core_issues.html#estensione-della-msa-agli-item-politomici",
    "href": "chapters/mokken/02_core_issues.html#estensione-della-msa-agli-item-politomici",
    "title": "65  Analisi della Scala di Mokken",
    "section": "\n65.4 Estensione della MSA agli Item Politomici",
    "text": "65.4 Estensione della MSA agli Item Politomici\nLa MSA, inizialmente sviluppata per item dicotomici (es. risposte “sì/no”), è stata ampliata da Molenaar (1982a, 1997) per includere item politomici, come quelli utilizzati nelle scale Likert. Questa estensione mantiene i principi di base della MSA ma li adatta per gestire la maggiore complessità degli item con più categorie di risposta.\nGli item politomici sono domande che offrono più di due opzioni di risposta ordinate, come una scala Likert a 5 punti (da “fortemente in disaccordo” a “fortemente d’accordo”). A differenza degli item dicotomici, gli item politomici introducono una struttura più articolata, dove le risposte non sono semplicemente giuste o sbagliate, ma riflettono livelli crescenti di accordo, preferenza o intensità.\nNel caso degli item politomici, la MSA non valuta solo l’item nel suo complesso, ma analizza anche i “passaggi” tra categorie di risposta consecutive. Questi passaggi rappresentano le transizioni da una categoria a quella successiva lungo la scala di risposta.\nAd esempio, in un item Likert a 5 punti (“fortemente in disaccordo”, “in disaccordo”, “neutrale”, “d’accordo”, “fortemente d’accordo”), ci sono 4 passaggi distinti:\n\nDa “fortemente in disaccordo” a “in disaccordo o superiore”.\nDa “in disaccordo” a “neutrale o superiore”.\nDa “neutrale” a “d’accordo o superiore”.\nDa “d’accordo” a “fortemente d’accordo”.\n\nOgni passaggio riflette una progressione lungo il continuum latente del tratto psicologico che si sta misurando.\n\n65.4.1 Funzioni di Risposta del Passaggio dell’Item\nPer analizzare i passaggi tra categorie, la MSA utilizza le Funzioni di Risposta del Passaggio dell’Item (Item Step Response Functions, ISRF). Le ISRF descrivono la probabilità che una persona con un certo livello di tratto latente (\\(\\theta\\)) scelga una determinata categoria o una categoria superiore.\n\n65.4.1.1 Caratteristiche delle ISRF:\n\nProbabilità di transizione:\nLe ISRF collegano la probabilità di scegliere una categoria (o una superiore) al livello di \\(\\theta\\) dell’individuo. Ad esempio, per un partecipante con \\(\\theta\\) medio-alto, la probabilità di passare a “d’accordo o superiore” dovrebbe essere maggiore rispetto a un partecipante con \\(\\theta\\) basso.\nContributo di ogni passaggio:\nOgni passaggio tra categorie viene analizzato separatamente, per verificare quanto contribuisce al progresso lungo il continuum latente. Questo permette di identificare eventuali problemi in specifiche transizioni.\n\n65.4.1.2 Requisito di monotonicità per le ISRF\nLe probabilità rappresentate dalle ISRF devono essere monotone crescenti. Ciò significa che, all’aumentare del livello di \\(\\theta\\), la probabilità di scegliere una categoria più alta (o una superiore) deve aumentare o rimanere costante.\n\n65.4.1.3 Esempio di Applicazione\nImmaginiamo un item Likert a 5 punti in una scala che misura l’autostima. Per un individuo con un basso livello di autostima (\\(\\theta\\) basso), le ISRF potrebbero mostrare una probabilità più alta di rimanere nelle categorie più basse (“fortemente in disaccordo”, “in disaccordo”). Al contrario, per un individuo con un alto livello di autostima (\\(\\theta\\) alto), le ISRF indicherebbero una maggiore probabilità di scegliere categorie superiori (“d’accordo”, “fortemente d’accordo”).\n\n65.4.1.4 Vantaggi dell’estensione agli item politomici\n\nMaggiore flessibilità:\nLa MSA può essere applicata a una gamma più ampia di test psicologici, inclusi quelli con risposte più sfumate e dettagliate.\nAnalisi dettagliata:\nEsaminando i passaggi tra categorie, è possibile individuare problemi specifici in alcune transizioni (ad esempio, una mancanza di progressione coerente tra categorie adiacenti).\nAdattamento alle scale ordinarie:\nGli item politomici sono comuni nei test psicologici e sociali; questa estensione rende la MSA più applicabile ai dati reali.\n\nIn conclusione, l’estensione della MSA agli item politomici rappresenta un importante avanzamento nell’analisi delle scale psicometriche. Analizzando non solo gli item nel loro complesso ma anche i singoli passaggi tra categorie, la MSA offre una visione più dettagliata di come gli item riflettono il tratto latente. Tuttavia, è fondamentale verificare che le probabilità rappresentate dalle ISRF rispettino la monotonicità, per garantire la validità e l’affidabilità della scala.\n\n65.4.2 Assunzioni Fondamentali per la MSA con Item Politomici\nL’estensione della MSA agli item politomici mantiene i principi fondamentali del modello originariamente sviluppato per gli item dicotomici, adattandoli ai singoli passaggi tra categorie. Ecco le principali assunzioni:\n\n\nMonotonicità delle ISRF:\n\nLa probabilità di scegliere una categoria \\(k\\) o superiore deve aumentare o rimanere costante all’aumentare del tratto latente \\(\\theta\\).\n\nQuesto garantisce che le categorie siano ordinate in modo coerente e riflettano livelli crescenti di \\(\\theta\\). Ad esempio, un individuo con un tratto latente più alto dovrebbe avere una maggiore probabilità di selezionare categorie superiori rispetto a chi ha un tratto latente più basso.\n\n\n\nOrdinamento coerente delle categorie:\n\nOgni categoria di risposta deve rappresentare un livello progressivamente più alto del tratto latente.\n\nAd esempio, il passaggio da “neutrale” a “d’accordo” dovrebbe indicare un incremento tangibile lungo il continuum di \\(\\theta\\), garantendo che le categorie siano interpretabili e coerenti.\n\n\n\nValidità del modello di omogeneità monotona:\n\nCome per gli item dicotomici, il modello presuppone che tutti gli item misurino lo stesso tratto latente in modo coerente e monotono.\n\nQuesta coerenza implica che le risposte degli individui siano influenzate principalmente dal livello di \\(\\theta\\), con minime interferenze da altri fattori.\n\n\n\n65.4.3 Vantaggi dell’Analisi Politomica con la MSA\nL’estensione della MSA agli item politomici offre strumenti utili per affrontare la complessità di scale con più categorie di risposta, come quelle basate su scale Likert. I principali vantaggi includono:\n\n\nMaggiore precisione nella misurazione:\n\nLa presenza di più categorie di risposta consente di catturare differenze più sottili tra i rispondenti rispetto agli item dicotomici.\n\nQuesto è particolarmente utile per misurare tratti psicologici che variano su un continuum, come atteggiamenti, opinioni o livelli di motivazione. Le categorie multiple offrono un modo per rappresentare una gamma più ampia di livelli del tratto latente.\n\n\n\nValutazione approfondita delle transizioni tra categorie:\n\nLa MSA politomica analizza le transizioni tra categorie consecutive (passaggi), permettendo di verificare se ciascun passaggio è coerente con un progresso lungo il tratto latente.\n\nQuesta analisi consente di identificare problemi come:\n\nCategorie che non rappresentano un aumento coerente del tratto latente.\n\nPassaggi con probabilità di transizione non monotone, che potrebbero indicare una cattiva formulazione dell’item o una scarsa capacità discriminativa.\n\n\n\n\n\nApplicabilità a scale con risposte articolate:\n\nLa MSA per item politomici è particolarmente adatta per scale frequentemente utilizzate in ambiti psicologici, educativi e sociali, dove le risposte graduali (ad esempio, “in disaccordo” a “d’accordo”) sono comuni.\n\nQuesto approccio consente di valutare con precisione la coerenza delle categorie di risposta rispetto al tratto latente e di identificare eventuali categorie ridondanti o poco utili.\n\n\n\n65.4.4 Implicazioni per la costruzione di scale\nL’analisi politomica con la MSA non si limita a verificare se gli item misurano un tratto latente, ma permette anche di ottimizzare la struttura della scala. Attraverso l’analisi dei passaggi, è possibile:\n\n\nGarantire che ogni categoria rappresenti un livello del tratto latente:\n\nOgni passaggio deve riflettere un progresso chiaro lungo il continuum latente, senza sovrapposizioni tra categorie.\n\n\n\nIndividuare anomalie nelle categorie:\n\nCategorie che non seguono un ordine coerente possono essere riformulate o combinate con altre per migliorare la qualità della scala.\n\n\n\nValutare l’utilità di ciascun passaggio:\n\nSe un passaggio tra categorie ha una bassa capacità discriminativa o non rispecchia adeguatamente il progresso lungo \\(\\theta\\), potrebbe essere necessario modificarlo o eliminarlo.\n\n\n\nIn conclusione, l’estensione della MSA agli item politomici offre strumenti analitici per verificare la coerenza e l’utilità delle categorie di risposta. Attraverso l’analisi dei passaggi tra categorie, si può ottenere una misurazione più dettagliata e precisa dei tratti latenti, migliorando la capacità delle scale di rappresentare accuratamente il costrutto d’interesse. Questo approccio è particolarmente rilevante in ambiti in cui le risposte articolate sono indispensabili per catturare la complessità dei fenomeni misurati.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_core_issues.html#mokken-reliability-coefficient",
    "href": "chapters/mokken/02_core_issues.html#mokken-reliability-coefficient",
    "title": "65  Analisi della Scala di Mokken",
    "section": "\n65.5 Mokken Reliability Coefficient",
    "text": "65.5 Mokken Reliability Coefficient\nL’affidabilità di un test psicometrico si riferisce alla sua capacità di fornire risultati coerenti nel tempo o attraverso somministrazioni ripetute. Sebbene l’alfa di Cronbach sia l’indicatore di affidabilità più diffuso, presenta alcune limitazioni, soprattutto quando gli item non sono omogenei o quando si lavora con scale ordinali. In questi casi, il coefficiente \\(\\rho\\) di Mokken (Mokken reliability coefficient o \\(\\rho_M\\)) rappresenta un’alternativa valida, basandosi su assunzioni meno restrittive e adattandosi meglio ai dati ordinali.\nIl coefficiente \\(\\rho\\) di Mokken è una misura di consistenza interna che deriva dalla teoria della scala non parametrica di Mokken, utilizzata principalmente per dati ordinali. A differenza dell’alfa di Cronbach, \\(\\rho\\) non richiede che gli item soddisfino rigorosi criteri di omogeneità o equivalenza, rendendolo particolarmente utile in contesti in cui i dati non rispettano pienamente i presupposti della teoria classica dei test (CTT).\nIl coefficiente \\(\\rho\\) di Mokken misura il rapporto tra la varianza spiegata dal punteggio totale (\\(S^2_T\\)) e la varianza totale osservata. La formula è la seguente:\n\\[\n\\rho = 1 - \\frac{\\sum_{i=1}^k S^2_i}{S^2_T},\n\\]\ndove:\n\n\n\\(k\\): numero di item del test,\n\n\\(S^2_i\\): varianza di ciascun item,\n\n\\(S^2_T\\): varianza totale del punteggio del test (somma dei punteggi degli item).\n\nInterpretazione di \\(\\rho\\):\n\n\n\\(\\rho\\) varia tra 0 e 1.\n\nvalori vicini a 1 indicano maggiore affidabilità.\n\n\n\\(\\rho &gt; 0.70\\): accettabile per la ricerca esplorativa.\n\n\n\\(\\rho &gt; 0.80\\): preferibile per applicazioni pratiche.\n\n\n\nVantaggi di \\(\\rho\\) di Mokken:\n\na differenza dell’alfa di Cronbach, che assume una scala intervallare, \\(\\rho\\) può essere utilizzato con dati ordinali come le scale Likert.\nnon richiede che tutti gli item abbiano correlazioni simili (omogeneità), né che riflettano lo stesso grado di relazione con la variabile latente.\npuò essere applicato a test che non rispettano i presupposti della teoria classica dei test o di modelli parametrici più rigidi.\n\nLimitazioni di \\(\\rho\\) di Mokken:\n\nse gli item non rispettano l’assunzione di monotonicità, l’interpretazione del coefficiente può risultare ambigua.\nil coefficiente è appropriato per scale unidimensionali; in presenza di più dimensioni, è necessario analizzare ogni sottoscala separatamente.\n\nIn conclusione, il coefficiente \\(\\rho\\) di Mokken valuta l’affidabilità di scale ordinali, offrendo un’alternativa all’alfa di Cronbach quando le assunzioni della teoria classica dei test non sono soddisfatte. Tuttavia, è essenziale verificare la monotonicità degli item e considerare la dimensionalità della scala per un’interpretazione accurata.\n\n65.5.1 Procedura di Selezione Automatica degli Item\nLa Procedura di Selezione Automatica degli Item (AISP) è una metodologia impiegata nella MSA per selezionare insiemi di item che rispettino le assunzioni del Modello di Mokken (MHM). A differenza di tecniche più comuni come l’analisi fattoriale o l’analisi parallela, l’AISP non determina esplicitamente la dimensionalità dei dati. Piuttosto, si basa sui coefficienti di scalabilità per identificare gruppi di item che misurano lo stesso costrutto latente, formando così una o più scale.\n\n65.5.1.1 Come Funziona l’AISP\nL’AISP segue un approccio iterativo che:\n\nSeleziona l’item iniziale, valutato come più rappresentativo di una dimensione, utilizzando il coefficiente di scalabilità individuale (\\(H_i\\)).\nSuccessivamente, analizza le coppie di item (\\(H_{ij}\\)) per identificare un insieme scalabile di item che misurano lo stesso costrutto.\n\nUn item viene incluso in una scala se:\n\nIl suo coefficiente di scalabilità individuale (\\(H_i\\)) supera una soglia predefinita (\\(c\\), solitamente pari a 0.30);\nLa covarianza tra ogni coppia di item (\\(H_{ij}\\)) è positiva e superiore alla stessa soglia.\n\nSe un item non soddisfa questi criteri, la procedura tenta di assegnarlo a una nuova scala. Questo processo iterativo continua finché tutti gli item non sono stati assegnati a una scala o esclusi come non scalabili.\n\n65.5.1.2 Soglia (\\(c\\)) e Implicazioni\nLa scelta della soglia \\(c\\) è cruciale:\n\n\nValore alto di \\(c\\): Maggiore precisione nella selezione degli item, ma rischio di escluderne troppi, riducendo la lunghezza della scala e la copertura del costrutto.\n\nValore basso di \\(c\\): Aumenta il numero di item inclusi, ma rischia di compromettere la coerenza e la validità della scala.\n\nUn valore comunemente utilizzato è \\(c = 0.30\\), che rappresenta un buon equilibrio tra inclusività e rigore. Tuttavia, il valore ottimale dipende dagli obiettivi dello studio e dalla natura dei dati.\n\n65.5.1.3 Limitazioni dell’AISP\n\n\nRelazioni tra dimensioni: L’AISP può risultare meno efficace in presenza di dimensioni fortemente correlate o di item che saturano su più dimensioni, poiché non distingue esplicitamente tali situazioni.\n\nDipendenza dai dati statistici: L’AISP si basa esclusivamente sui coefficienti di scalabilità e non considera le relazioni teoriche tra gli item. Questo può portare all’inclusione di item con scarso valore teorico o alla loro esclusione inappropriata.\n\nUnidimensionalità richiesta: Gli item selezionati da una scala devono riflettere un unico costrutto. In caso di multidimensionalità, occorre considerare scale separate.\n\nIn conclusione, l’AISP offre un’alternativa flessibile all’analisi fattoriale per la costruzione di scale psicometriche, particolarmente utile quando si lavora con dati ordinali. Sebbene non forzi soluzioni e tenga conto della scalabilità degli item, è fondamentale integrarla con considerazioni teoriche per garantire che le scale siano valide e utili nel contesto di applicazione.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_core_issues.html#ordinamento-invariante-degli-item-iio",
    "href": "chapters/mokken/02_core_issues.html#ordinamento-invariante-degli-item-iio",
    "title": "65  Analisi della Scala di Mokken",
    "section": "\n65.6 Ordinamento Invariante degli Item (IIO)",
    "text": "65.6 Ordinamento Invariante degli Item (IIO)\nL’Ordinamento Invariante degli Item (Invariant Item Ordering, IIO) è un concetto fondamentale nella MSA. Garantisce che l’ordine di difficoltà degli item rimanga costante per tutti i partecipanti, indipendentemente dal loro livello sul costrutto latente misurato (ad esempio, ansia o autostima).\nIn pratica, l’IIO richiede che:\n\nGli item abbiano sempre lo stesso ordine di difficoltà:\nUn item considerato “più difficile” deve essere percepito come tale da tutti i partecipanti, indipendentemente dal loro livello del tratto latente.\nAd esempio, in un test di matematica, un problema complesso deve risultare più difficile di uno semplice per tutti gli studenti, sia quelli più abili sia quelli meno abili.\nI punteggi totali siano interpretabili:\nSe un partecipante ha un punteggio totale più alto, deve avere una probabilità maggiore di rispondere positivamente (o correttamente) a ogni item rispetto a un partecipante con un punteggio più basso.\n\nL’IIO garantisce che le differenze nei punteggi totali riflettano effettivamente differenze nel costrutto latente, senza essere influenzate da variabili estranee come età, genere o interpretazioni diverse degli item.\n\n65.6.1 Perché l’IIO è Importante?\n\nUniformità nella misurazione:\nL’IIO assicura che gli item misurino lo stesso tratto latente per tutti i partecipanti.\nAd esempio, in una scala di depressione, un punteggio totale più alto deve indicare una maggiore gravità del disturbo, con una progressione coerente dagli item più “facili” (come stanchezza) ai più “difficili” (come pensieri suicidari).\nValidità dei punteggi:\nSe l’ordine degli item cambia tra sottogruppi di partecipanti, i punteggi totali potrebbero non rappresentare correttamente il costrutto latente, invalidando i confronti tra gruppi.\nIdentificazione di bias:\nLa verifica dell’IIO aiuta a individuare problemi come la Funzione Differenziale degli Item (Differential Item Functioning, DIF), che si verifica quando un item funziona diversamente per sottogruppi specifici (ad esempio, uomini e donne).\n\n65.6.2 Come Verificare l’IIO?\nUn metodo semplice per verificare l’IIO è il metodo dei gruppi di restscore. Ecco come funziona:\n\nCalcolo del restscore:\nPer ciascun partecipante, calcola il punteggio totale sugli item, escludendo l’item che stai analizzando. Questo punteggio si chiama restscore.\nCreazione di gruppi basati sul restscore:\nDividi i partecipanti in gruppi (ad esempio, punteggi bassi, medi e alti) in base al loro restscore.\n\nConfronto dell’ordine degli item:\nVerifica se l’ordine di difficoltà degli item rimane lo stesso tra i gruppi.\n\nAd esempio, se l’ordine teorico degli item è \\(A &lt; B &lt; C\\) (da più facile a più difficile), questo ordine deve essere confermato nei gruppi con restscore basso, medio e alto.\n\n\n\nSe l’ordine degli item cambia tra i gruppi, l’IIO non è rispettato. Questo potrebbe indicare che alcuni item non misurano il costrutto latente in modo coerente.\n\n65.6.3 Cosa Succede se l’IIO Non È Rispettato?\nSe l’IIO viene violato, si possono riscontrare i seguenti problemi:\n\nBias negli item:\nAlcuni item potrebbero misurare qualcosa di diverso dal costrutto latente (ad esempio, un item di depressione potrebbe essere influenzato dal genere o dalla cultura).\nPunteggi non interpretabili:\nI punteggi totali non riflettono accuratamente il livello del tratto latente, compromettendo l’utilità del test.\nInvalidità della scala:\nLa scala potrebbe non misurare un unico costrutto latente, rendendola poco affidabile per applicazioni pratiche.\n\nIn conclusione, l’IIO è essenziale per garantire che una scala misuri in modo coerente un costrutto latente. Verificare l’IIO consente di assicurarsi che:\n\ngli item mantengano un ordine di difficoltà stabile per tutti i partecipanti;\ni punteggi totali siano validi e rappresentino accuratamente il tratto latente;\n\nla scala sia priva di bias o funzionamenti differenziali tra sottogruppi.\n\nIl metodo dei gruppi di restscore offre un approccio intuitivo per valutare l’IIO e identificare eventuali problematiche nella costruzione o interpretazione della scala.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_core_issues.html#dimensione-del-campione-per-lmsa",
    "href": "chapters/mokken/02_core_issues.html#dimensione-del-campione-per-lmsa",
    "title": "65  Analisi della Scala di Mokken",
    "section": "\n65.7 Dimensione del Campione per l’MSA",
    "text": "65.7 Dimensione del Campione per l’MSA\nLa determinazione della dimensione del campione è essenziale per garantire l’affidabilità dei risultati nella MSA, ma non esistono linee guida univoche. Campioni troppo piccoli possono portare a:\n\n\nFalsi positivi, con scale erroneamente ritenute valide.\n\nFalsi negativi, con scale esistenti che non vengono rilevate.\n\nPrincipali Fattori che Influenzano la Dimensione del Campione\n\n\nCoefficiente di scalabilità (\\(H_i\\)):\n\nPer \\(H_i\\) basso (\\(\\approx 0.22\\)): sono necessari campioni molto grandi (750-2500 partecipanti).\n\nPer \\(H_i\\) moderato o alto (\\(\\approx 0.42\\)): campioni più piccoli (50-250 partecipanti) sono sufficienti.\n\n\nLunghezza del test:\nLa lunghezza del test ha un impatto minore, poiché test più lunghi non richiedono necessariamente campioni più grandi.\nCorrelazione tra dimensioni:\nScale multidimensionali possono necessitare di campioni più grandi a causa della complessità del modello.\n\nLinee Guida Generali\n\nPer $H_i : almeno 1000-2500 partecipanti per ottenere risultati accurati.\nPer $H_i : campioni di 50-250 partecipanti possono essere adeguati.\n\nStudi come quelli di Watson et al. (2018) suggeriscono che, sebbene i valori medi di \\(H\\) e \\(H_i\\) siano stabili anche con campioni più piccoli, i loro intervalli di confidenza sono molto ampi, aumentando il rischio di errori nelle decisioni sugli item.\nIn conclusione, campioni più grandi migliorano la precisione delle stime e la stabilità dei coefficienti di scalabilità. Tuttavia, per scale con \\(H_i\\) elevato, campioni moderati possono essere sufficienti per un’analisi accurata.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_core_issues.html#confronto-tra-la-ctt-e-la-msa",
    "href": "chapters/mokken/02_core_issues.html#confronto-tra-la-ctt-e-la-msa",
    "title": "65  Analisi della Scala di Mokken",
    "section": "\n65.8 Confronto tra la CTT e la MSA",
    "text": "65.8 Confronto tra la CTT e la MSA\nLa Classical Test Theory (CTT) e la MSA rappresentano due approcci distinti alla misurazione psicometrica, ognuno con vantaggi e limiti. Pur condividendo alcuni concetti di base, si differenziano per assunzioni, flessibilità e modalità di analisi dei dati.\n\n65.8.1 Somiglianze tra CTT e MSA\nEntrambi gli approcci analizzano le relazioni tra item e punteggi totali per valutare la qualità di un test. Alcuni indicatori delle due teorie sono concettualmente simili:\n\nIl coefficiente di scalabilità degli item (\\(H_i\\)) della MSA è paragonabile alla correlazione tra un item e il punteggio totale nella CTT.\nIl coefficiente di scalabilità tra coppie di item (\\(H_{ij}\\)) della MSA si avvicina alla correlazione tra coppie di item nella CTT.\nIl coefficiente complessivo di scalabilità (\\(H\\)) nella MSA è analogo all’indice medio di discriminazione degli item nella CTT.\n\nQueste somiglianze rendono entrambi gli approcci utili per descrivere le caratteristiche degli item e dei test.\n\n65.8.2 Differenze tra CTT e MSA\n\n\nAssunzioni e loro verifica:\n\nLa CTT si basa su assunzioni teoriche non direttamente verificabili dai dati, come l’indipendenza degli errori di misura e l’unidimensionalità del costrutto.\n\nLa MSA, invece, consente di testare empiricamente assunzioni fondamentali come la monotonicità, l’indipendenza locale e l’unidimensionalità. Per esempio, un coefficiente di scalabilità negativo (\\(H_i &lt; 0\\)) indicherebbe che i dati non sono compatibili con il modello.\n\n\n\nLivello di misurazione:\n\nLa CTT richiede dati a livello di scala ad intervalli e considera i punteggi totali come un’approssimazione diretta del costrutto latente.\n\nLa MSA è più adatta per dati ordinali (come risposte su una scala Likert) e non presuppone una relazione specifica tra il costrutto latente e le risposte agli item.\n\n\n\nIn conclusione, la CTT e la MSA non si escludono a vicenda, ma possono essere viste come approcci complementari. La CTT fornisce una base teorica solida e intuitiva per la misurazione, mentre la MSA permette un’analisi più rigorosa e adattabile alle caratteristiche dei dati. Utilizzare entrambi gli approcci in modo integrato può offrire una visione più completa della qualità e della validità di una scala psicometrica.\n\n65.8.3 Riflessioni Conclusive\nQuesto capitolo ha evidenziato l’importanza di non dare per scontata l’assunzione, tipica della Classical Test Theory (CTT), che i punteggi grezzi rappresentino dati ordinali validi. Tale presupposto, spesso implicito, necessita di una verifica empirica per garantire che le interpretazioni siano accurate e affidabili. In questo contesto, l’Analisi delle Scale di Mokken (MSA) offre strumenti efficaci per valutare empiricamente questa ipotesi.\nAttraverso i coefficienti di scalabilità (\\(H\\), \\(H_i\\), \\(H_{ij}\\)), la MSA permette di verificare se un test misura un costrutto unidimensionale in modo coerente, mentre il principio dell’Ordinamento Invariante degli Item (IIO) garantisce che l’ordine di difficoltà degli item sia stabile per tutti i livelli di abilità dei rispondenti. Questi strumenti forniscono un quadro analitico robusto per validare la qualità di una scala psicometrica, superando alcune limitazioni della CTT.\nLa MSA rappresenta un’evoluzione rispetto al modello di Guttman, introducendo maggiore flessibilità e adattabilità nella gestione delle deviazioni dalla cumulatività perfetta. Questo la rende particolarmente utile per l’analisi esplorativa di scale psicologiche, consentendo di indagare la struttura delle risposte agli item e la loro relazione con il costrutto latente.\nIn sintesi, l’MSA non solo integra ma amplia i tradizionali approcci della psicometria, offrendo strumenti analitici utili per costruire e validare scale che misurano costrutti complessi in modo accurato e interpretabile.\n\n\n\n\nWind, S. A. (2017). An instructional module on Mokken scale analysis. Educational Measurement: Issues and Practice, 36(2), 50–66.\n\n\nWind, S. A. (2024). Item-Explanatory Mokken Scale Analysis: Using Nonparametric Item Response Theory to Explore Item Attributes. The Journal of Experimental Education, 1–21.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/03_applications.html",
    "href": "chapters/mokken/03_applications.html",
    "title": "66  Applicazione Pratica",
    "section": "",
    "text": "66.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nIn questo capitolo, esamineremo l’applicazione pratica dei concetti e delle metodologie esplorate nel precedente capitolo, affrontando un’analisi dettagliata di un set di dati concreti. Il nostro focus è un caso di studio di grande rilevanza psicologica: l’indagine condotta dai ricercatori dell’ospedale Meyer, mirata a comprendere la capacità di coping delle famiglie di fronte alla diagnosi di una grave malattia in un figlio.\nQuesto lavoro non solo rappresenta un’opportunità per mettere in pratica le teorie e i metodi discussi, ma offre anche una finestra su questioni di vitale importanza nel campo della psicologia. Affrontare tematiche così delicate ci permette di esplorare le dinamiche familiari in situazioni di stress estremo, fornendo intuizioni preziose che possono guidare interventi psicosociali efficaci.\nPer garantire la massima accuratezza e rilevanza dei nostri risultati, iniziamo con un’attenta preparazione e pulizia dei dati. Questo passo ci assicura che l’analisi sia condotta su informazioni ben distribuite e rappresentative, eliminando gli item con eccessiva asimmetria e curtosi.\nAttraverso questa esplorazione approfondita, miriamo a dimostrare come le competenze metodologiche e analitiche possano essere efficacemente applicate a questioni di profondo impatto psicologico, evidenziando il potere dell’analisi statistica nel trasformare set di dati complessi in comprensioni approfondite e applicabili.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/03_applications.html#importazione-dei-dati",
    "href": "chapters/mokken/03_applications.html#importazione-dei-dati",
    "title": "66  Applicazione Pratica",
    "section": "66.2 Importazione dei dati",
    "text": "66.2 Importazione dei dati\n\ndf_tot &lt;- readRDS(\"../../data/fai_2022_11_20.rds\")\n\ntemp &lt;- df_tot |&gt; \n  dplyr::filter(FLAG == \"keep\")\ntemp$FLAG &lt;- NULL\n\nPer migliorare la qualità del nostro set di dati, rimuoveremo gli item che presentano livelli eccessivi di asimmetria (skewness) e curtosi. Questo passaggio è fondamentale per garantire che i nostri dati siano ben distribuiti e rappresentativi, migliorando così l’affidabilità e la validità delle nostre analisi. Gli item con asimmetria e curtosi estreme possono infatti distorcere i risultati degli analisi statistiche e influenzare negativamente le conclusioni tratte dallo studio.\n\nitems_stats &lt;- psych::describe(temp)\n\nitems_skew_kurt_bad &lt;- items_stats |&gt;\n    dplyr::filter(skew &gt; 2.5 | kurtosis &gt; 7.5) |&gt;\n    row.names()\nprint(items_skew_kurt_bad)\n\n [1] \"other\"                  \"child_birth_place\"      \"has_chronic_disease\"   \n [4] \"hospitalization_number\" \"emergency_care_number\"  \"divorce\"               \n [7] \"low_income\"             \"change_address\"         \"change_city\"           \n[10] \"is_mother_italian\"      \"is_father_italian\"      \"is_father_working\"     \n[13] \"is_child_italian\"       \"FAI_24\"                 \"FAI_32\"                \n[16] \"FAI_52\"                 \"FAI_53\"                 \"FAI_61\"                \n[19] \"FAI_74\"                 \"FAI_76\"                 \"FAI_77\"                \n[22] \"FAI_138\"                \"FAI_152\"                \"FAI_174\"               \n[25] \"FAI_175\"                \"FAI_182\"                \"FAI_193\"               \n\n\n\n# Select the strings starting with \"FAI_\"\nbad_fai_items &lt;- grep(\"^FAI_\", items_skew_kurt_bad, value = TRUE)\n\ndf &lt;- temp |&gt;\n    dplyr::select(!any_of(bad_fai_items))\n\nCi concentreremo qui su un sottoinsieme di item, ovvero quelli che riguardano l’area delle caratteristiche del bambino.\n\n# First subscale: items names.\nitem_subscale &lt;- c(\n    \"FAI_49\", \"FAI_106\", \"FAI_60\", \"FAI_124\", \"FAI_86\",\n    \"FAI_47\", \"FAI_121\", \"FAI_167\", \"FAI_99\",\n    \"FAI_63\", \"FAI_168\", \"FAI_5\", \"FAI_132\", \"FAI_85\", \"FAI_81\",\n    \"FAI_83\",\n    # \"FAI_152\",  \"FAI_175\",\n    \"FAI_57\", \"FAI_91\", \"FAI_135\", \"FAI_1\"\n)\n\n# Select only the items of this subscale.\nsubscale_data &lt;- df %&gt;%\n    dplyr::select(all_of(item_subscale))\ndim(subscale_data)\n\n\n45320",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/03_applications.html#statistiche-descrittive",
    "href": "chapters/mokken/03_applications.html#statistiche-descrittive",
    "title": "66  Applicazione Pratica",
    "section": "66.3 Statistiche descrittive",
    "text": "66.3 Statistiche descrittive\nEsaminiamo le statistiche descrittive degli item.\n\npsych::describe(subscale_data)\n\n\nA psych: 20 x 13\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nFAI_49\n1\n453\n0.5629139\n1.0320709\n0\n0.3085399\n0.0000\n0\n4\n4\n2.0665609\n3.6052375\n0.04849092\n\n\nFAI_106\n2\n453\n0.6710817\n0.9911011\n0\n0.4710744\n0.0000\n0\n4\n4\n1.6036473\n2.0835785\n0.04656599\n\n\nFAI_60\n3\n453\n0.9403974\n1.2169441\n0\n0.7272727\n0.0000\n0\n4\n4\n1.1792903\n0.2955897\n0.05717702\n\n\nFAI_124\n4\n453\n1.0463576\n1.1955175\n1\n0.8429752\n1.4826\n0\n4\n4\n1.1667416\n0.4658086\n0.05617031\n\n\nFAI_86\n5\n453\n1.1898455\n1.0171900\n1\n1.0606061\n1.4826\n0\n4\n4\n0.9491960\n0.6428214\n0.04779175\n\n\nFAI_47\n6\n453\n1.6247241\n1.1132778\n2\n1.5950413\n1.4826\n0\n4\n4\n0.1994007\n-0.7340689\n0.05230635\n\n\nFAI_121\n7\n453\n0.4900662\n0.7121035\n0\n0.3746556\n0.0000\n0\n4\n4\n1.8321794\n4.5725145\n0.03345754\n\n\nFAI_167\n8\n453\n1.9072848\n1.0306218\n2\n1.8815427\n1.4826\n0\n4\n4\n0.1974188\n-0.2695266\n0.04842284\n\n\nFAI_99\n9\n453\n2.0618102\n1.1543198\n2\n2.0220386\n1.4826\n0\n4\n4\n0.3447637\n-0.7883135\n0.05423468\n\n\nFAI_63\n10\n453\n1.1501104\n1.1615070\n1\n1.0192837\n1.4826\n0\n4\n4\n0.7295382\n-0.4985964\n0.05457236\n\n\nFAI_168\n11\n453\n0.7748344\n1.1298770\n0\n0.5399449\n0.0000\n0\n4\n4\n1.4492317\n1.1504289\n0.05308625\n\n\nFAI_5\n12\n453\n1.0132450\n1.1805209\n1\n0.8264463\n1.4826\n0\n4\n4\n1.0371531\n0.1238107\n0.05546571\n\n\nFAI_132\n13\n453\n1.4238411\n1.2360930\n1\n1.3002755\n1.4826\n0\n4\n4\n0.5953586\n-0.6157883\n0.05807672\n\n\nFAI_85\n14\n453\n1.3311258\n0.9027136\n1\n1.3002755\n1.4826\n0\n4\n4\n0.2930726\n-0.1115428\n0.04241319\n\n\nFAI_81\n15\n453\n1.7969095\n1.2685052\n2\n1.7465565\n1.4826\n0\n4\n4\n0.2930069\n-1.0004309\n0.05959957\n\n\nFAI_83\n16\n453\n1.1986755\n1.1825793\n1\n1.0468320\n1.4826\n0\n4\n4\n0.8458362\n-0.1601818\n0.05556242\n\n\nFAI_57\n17\n453\n0.3863135\n0.7123983\n0\n0.2341598\n0.0000\n0\n4\n4\n2.2165283\n5.6437160\n0.03347139\n\n\nFAI_91\n18\n453\n0.5320088\n0.9298228\n0\n0.3057851\n0.0000\n0\n4\n4\n2.1393688\n4.4338138\n0.04368689\n\n\nFAI_135\n19\n453\n0.4039735\n0.7686801\n0\n0.2231405\n0.0000\n0\n4\n4\n2.2914674\n5.8874641\n0.03611574\n\n\nFAI_1\n20\n453\n0.4547461\n0.7530503\n0\n0.2892562\n0.0000\n0\n4\n4\n1.8594704\n3.7153232\n0.03538139\n\n\n\n\n\nEsaminiamo la distribuzione del punteggio totale.\n\nscores &lt;- apply(subscale_data, 1, sum)\nhist(scores)",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/03_applications.html#automated-item-selection-procedure-aisp",
    "href": "chapters/mokken/03_applications.html#automated-item-selection-procedure-aisp",
    "title": "66  Applicazione Pratica",
    "section": "66.4 Automated Item Selection Procedure (AISP)",
    "text": "66.4 Automated Item Selection Procedure (AISP)\nIl primo passo nell’Analisi delle Scale Mokken (MSA) consiste nell’esecuzione dell’Automated Item Selection Procedure (AISP). Come precedentemente discusso, questa analisi ricerca insiemi di item (scale) che si conformano al modello di omogeneità monotona. Similmente all’analisi fattoriale esplorativa, l’AISP è un metodo per suddividere i dati in diverse sottoscale che soddisfano i criteri della MSA, includendo possibilmente anche l’individuazione di eventuali item non scalabili. Per eseguire la AISP, è necessario eseguire il codice seguente.\n\nsubscale_data &lt;- as.data.frame(subscale_data)\nscale &lt;- aisp(subscale_data, verbose = FALSE)\nprint(scale)\n\n        0.3\nFAI_49    1\nFAI_106   1\nFAI_60    1\nFAI_124   1\nFAI_86    0\nFAI_47    2\nFAI_121   0\nFAI_167   0\nFAI_99    0\nFAI_63    2\nFAI_168   0\nFAI_5     1\nFAI_132   2\nFAI_85    2\nFAI_81    1\nFAI_83    1\nFAI_57    2\nFAI_91    0\nFAI_135   0\nFAI_1     0\n\n\nNell’output precedente, FAI_* sono le etichette degli item considerati. Il valore .30 in alto indica il limite inferiore del coefficiente di scalabilità per la costruzione delle scale. ‘1’ indica che l’item appartiene alla scala 1 e 2 significa che l’item appartiene alla scala 2. ‘0’ indica che l’item non è scalabile. Dei 20 item di questa scala, sette item formano la scala 1 mentre altri cinque item formano la scala 2. Sette item risultano non scalabili.\nÈ possibile modificare sia il limite inferiore c (il limite inferiore predefinito è .30) sia il livello di alpha, che di default è .05. Per esempio:\n\nscale &lt;- aisp(subscale_data, lowerbound = 0.4, alpha = 0.1)\nprint(scale)\n\n        0.4\nFAI_49    1\nFAI_106   1\nFAI_60    1\nFAI_124   1\nFAI_86    0\nFAI_47    0\nFAI_121   0\nFAI_167   0\nFAI_99    0\nFAI_63    3\nFAI_168   0\nFAI_5     2\nFAI_132   0\nFAI_85    0\nFAI_81    2\nFAI_83    2\nFAI_57    3\nFAI_91    0\nFAI_135   0\nFAI_1     0\n\n\nSi noti che modificare il valore predefinito di c cambia la struttura della scala. Sijtsma e van der Ark (2017) hanno mostrato che il valore predefinito di .30 è quello che si dimostra più utile nella maggior parte delle applicazioni pratiche. Tuttavia, raccomandano di eseguire l’AISP 12 volte con c=0, .05, .10, .15, .20, .25, .30, .35, .40, .45, .50 e .55, così da potere esaminare le seguenti condizioni:\n\nNei dati unidimensionali, all’aumentare di c, si trova successivamente (a) la maggior parte o tutti gli item in una scala; (b) una scala più piccola; e (c) una o poche scale piccole e diversi item non scalabili. Viene raccomandato di prendere il risultato della fase (a) come finale.\nNei dati multidimensionali, all’aumentare di c, si trova successivamente (a) la maggior parte o tutti gli item in una scala; (b) due o più scale; e (c) due o più scale più piccole e diversi item non scalabili. Prendere il risultato della fase (b) come finale.\n\nLa decisione finale sulla struttura dei dati dovrebbe essere presa dal ricercatore sulla base di considerazioni teoriche e non sono statistiche.\nPer eseguire l’AISP con molteplici limiti inferiori, possiamo usare l’istruzione seguente:\n\naisp(\n    subscale_data, \n    lowerbound = c(.05, .10, .15, .20, .25, .30, .35, .40, .45, .50, .55), \n    verbose = FALSE\n)\n\n\nA matrix: 20 x 11 of type dbl\n\n\n\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n0.55\n\n\n\n\nFAI_49\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nFAI_106\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nFAI_60\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nFAI_124\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nFAI_86\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nFAI_47\n1\n1\n1\n1\n2\n2\n2\n0\n0\n0\n0\n\n\nFAI_121\n1\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n\n\nFAI_167\n1\n1\n1\n2\n0\n0\n0\n0\n0\n0\n0\n\n\nFAI_99\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nFAI_63\n1\n1\n1\n1\n2\n2\n2\n3\n3\n3\n0\n\n\nFAI_168\n1\n1\n1\n1\n3\n0\n0\n0\n0\n0\n0\n\n\nFAI_5\n1\n1\n1\n1\n1\n1\n0\n2\n0\n0\n0\n\n\nFAI_132\n1\n1\n1\n1\n2\n2\n0\n0\n0\n0\n0\n\n\nFAI_85\n2\n2\n0\n2\n2\n2\n2\n0\n0\n0\n0\n\n\nFAI_81\n1\n1\n1\n1\n1\n1\n1\n2\n2\n2\n2\n\n\nFAI_83\n1\n1\n1\n1\n1\n1\n1\n2\n2\n2\n2\n\n\nFAI_57\n1\n1\n1\n1\n2\n2\n2\n3\n3\n3\n0\n\n\nFAI_91\n1\n1\n1\n1\n3\n0\n0\n0\n0\n0\n0\n\n\nFAI_135\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nFAI_1\n2\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\nPer eseguire un’analisi della dimensionalità utilizzando un metodo diverso, ovvero l’algoritmo genetico (Straat, van der Ark & Sijtsma, 2013), si può utilizzare il seguente codice:\n\nscale &lt;- aisp(subscale_data, search = \"ga\")\nprint(scale)\n\n        0.3\nFAI_49    1\nFAI_106   1\nFAI_60    1\nFAI_124   1\nFAI_86    0\nFAI_47    2\nFAI_121   0\nFAI_167   0\nFAI_99    0\nFAI_63    2\nFAI_168   0\nFAI_5     1\nFAI_132   2\nFAI_85    2\nFAI_81    1\nFAI_83    1\nFAI_57    2\nFAI_91    0\nFAI_135   0\nFAI_1     0\n\n\nI risultati dell’algoritmo genetico sono equivalenti a quelli ottenuti utilizzando il limite inferiore raccomandato di 0.3.\nI risultati dell’analisi della dimensionalità ottenuti tramite l’AISP e l’algoritmo genetico (GA) dovrebbero essere replicabili in altri campioni. Pertanto, nelle procedure AISP e GA, la dimensione del campione è un fattore critico. Sijtsma e Molenaar (2002) affermano che l’AISP richiede almeno 100 partecipanti. Tuttavia, in studi basati su simulazioni di Monte Carlo, Straat, van der Ark e Sijtsma (2014) hanno dimostrato che sia l’AISP sia il GA necessitano di un campione compreso tra 250 e 500 partecipanti quando la qualità degli item (ovvero la loro capacità discriminante) è elevata, e tra 1250 e 1750 partecipanti quando la qualità degli item è scarsa.\nPossiamo selezionare gli item della scala 1 individuata dalla AISP nel modo seguente.\n\naisp.lb &lt;- aisp(subscale_data, lowerbound = .3)\ngood_items &lt;- subscale_data[, aisp.lb == 1]\nnames(good_items) |&gt; print()\n\n[1] \"FAI_49\"  \"FAI_106\" \"FAI_60\"  \"FAI_124\" \"FAI_5\"   \"FAI_81\"  \"FAI_83\"",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/03_applications.html#scalability-coefficients",
    "href": "chapters/mokken/03_applications.html#scalability-coefficients",
    "title": "66  Applicazione Pratica",
    "section": "66.5 Scalability Coefficients",
    "text": "66.5 Scalability Coefficients\nIl codice seguente ritorna i valori \\(H_{ij}\\), \\(H_j\\), e \\(H\\). Nelle parentesi tonde sono riportati gli errori standard.\n\nscal_coef &lt;- coefH(good_items, se = TRUE)\nscal_coef |&gt; print()\n\n$Hij\n        FAI_49  se      FAI_106 se      FAI_60  se      FAI_124 se      FAI_5  \nFAI_49                   0.561  (0.057)  0.608  (0.054)  0.601  (0.048)  0.212 \nFAI_106  0.561  (0.057)                  0.617  (0.042)  0.722  (0.039)  0.317 \nFAI_60   0.608  (0.054)  0.617  (0.042)                  0.688  (0.037)  0.280 \nFAI_124  0.601  (0.048)  0.722  (0.039)  0.688  (0.037)                  0.288 \nFAI_5    0.212  (0.056)  0.317  (0.055)  0.280  (0.052)  0.288  (0.052)        \nFAI_81   0.254  (0.060)  0.354  (0.056)  0.324  (0.050)  0.381  (0.053)  0.413 \nFAI_83   0.223  (0.057)  0.381  (0.056)  0.430  (0.047)  0.387  (0.048)  0.409 \n        se      FAI_81  se      FAI_83  se     \nFAI_49  (0.056)  0.254  (0.060)  0.223  (0.057)\nFAI_106 (0.055)  0.354  (0.056)  0.381  (0.056)\nFAI_60  (0.052)  0.324  (0.050)  0.430  (0.047)\nFAI_124 (0.052)  0.381  (0.053)  0.387  (0.048)\nFAI_5            0.413  (0.049)  0.409  (0.048)\nFAI_81  (0.049)                  0.551  (0.041)\nFAI_83  (0.048)  0.551  (0.041)                \n\n$Hi\n        Item H  se     \nFAI_49    0.410 (0.039)\nFAI_106   0.493 (0.032)\nFAI_60    0.484 (0.029)\nFAI_124   0.502 (0.029)\nFAI_5     0.323 (0.037)\nFAI_81    0.386 (0.034)\nFAI_83    0.404 (0.032)\n\n$H\nScale H      se \n  0.428 (0.026) \n\n$Hij\n        FAI_49  se      FAI_106 se      FAI_60  se      FAI_124 se      FAI_5  \nFAI_49                   0.561  (0.057)  0.608  (0.054)  0.601  (0.048)  0.212 \nFAI_106  0.561  (0.057)                  0.617  (0.042)  0.722  (0.039)  0.317 \nFAI_60   0.608  (0.054)  0.617  (0.042)                  0.688  (0.037)  0.280 \nFAI_124  0.601  (0.048)  0.722  (0.039)  0.688  (0.037)                  0.288 \nFAI_5    0.212  (0.056)  0.317  (0.055)  0.280  (0.052)  0.288  (0.052)        \nFAI_81   0.254  (0.060)  0.354  (0.056)  0.324  (0.050)  0.381  (0.053)  0.413 \nFAI_83   0.223  (0.057)  0.381  (0.056)  0.430  (0.047)  0.387  (0.048)  0.409 \n        se      FAI_81  se      FAI_83  se     \nFAI_49  (0.056)  0.254  (0.060)  0.223  (0.057)\nFAI_106 (0.055)  0.354  (0.056)  0.381  (0.056)\nFAI_60  (0.052)  0.324  (0.050)  0.430  (0.047)\nFAI_124 (0.052)  0.381  (0.053)  0.387  (0.048)\nFAI_5            0.413  (0.049)  0.409  (0.048)\nFAI_81  (0.049)                  0.551  (0.041)\nFAI_83  (0.048)  0.551  (0.041)                \n\n$Hi\n        Item H  se     \nFAI_49    0.410 (0.039)\nFAI_106   0.493 (0.032)\nFAI_60    0.484 (0.029)\nFAI_124   0.502 (0.029)\nFAI_5     0.323 (0.037)\nFAI_81    0.386 (0.034)\nFAI_83    0.404 (0.032)\n\n$H\nScale H      se \n  0.428 (0.026) \n\n$covHij\n               [,1]          [,2]          [,3]          [,4]          [,5]\n [1,]  3.256522e-03  1.972567e-03  1.673569e-03  6.850269e-04  0.0009303758\n [2,]  1.972567e-03  2.872632e-03  1.379662e-03  5.619181e-04  0.0007468355\n [3,]  1.673569e-03  1.379662e-03  2.324218e-03  7.130034e-04  0.0009755748\n [4,]  6.850269e-04  5.619181e-04  7.130034e-04  3.132843e-03  0.0014577910\n [5,]  9.303758e-04  7.468355e-04  9.755748e-04  1.457791e-03  0.0035999264\n [6,]  1.176059e-03  1.372448e-03  1.121228e-03  1.001618e-03  0.0007258314\n [7,]  1.143505e-03  6.796210e-04  6.075535e-04  3.611057e-04  0.0005325189\n [8,]  9.205140e-04  4.625925e-04  3.783489e-04  2.484545e-04  0.0003248843\n [9,]  1.281966e-04  2.342448e-04  1.519768e-04  1.410060e-03  0.0006882034\n[10,]  4.801610e-05  3.000776e-04  1.972389e-04  6.442391e-04  0.0016398978\n[11,]  6.330714e-05  1.877564e-04  2.994296e-04  4.332023e-04  0.0002887708\n[12,]  7.230668e-04  6.943860e-04  7.140702e-04  2.950943e-04  0.0003590450\n[13,]  3.194478e-04  4.391644e-04  3.080059e-04  1.808926e-03  0.0006665414\n[14,]  3.310593e-04  2.500548e-04  2.591961e-04  5.841647e-04  0.0016242307\n[15,] -2.626538e-05  1.143234e-04  3.461109e-04  5.949934e-04  0.0002896028\n[16,]  1.885976e-04  2.599218e-04  3.321093e-04  1.573800e-03  0.0006905796\n[17,]  1.132431e-04  1.511628e-04  5.240330e-05  5.614512e-04  0.0016082483\n[18,]  2.425335e-04  4.564482e-04  2.708591e-04  4.202745e-04  0.0002271944\n[19,]  6.016676e-05 -5.677038e-08  6.439974e-05  4.080167e-04  0.0003000318\n[20,] -6.323510e-05  2.770648e-04  9.115472e-05  4.564269e-04 -0.0000318649\n[21,] -1.452389e-04  4.045822e-05 -1.410212e-04 -8.502939e-05  0.0001437967\n               [,6]          [,7]          [,8]         [,9]        [,10]\n [1,]  1.176059e-03  1.143505e-03  9.205140e-04 1.281966e-04 0.0000480161\n [2,]  1.372448e-03  6.796210e-04  4.625925e-04 2.342448e-04 0.0003000776\n [3,]  1.121228e-03  6.075535e-04  3.783489e-04 1.519768e-04 0.0001972389\n [4,]  1.001618e-03  3.611057e-04  2.484545e-04 1.410060e-03 0.0006442391\n [5,]  7.258314e-04  5.325189e-04  3.248843e-04 6.882034e-04 0.0016398978\n [6,]  3.235792e-03  3.934074e-04  4.943276e-04 4.792224e-04 0.0002711309\n [7,]  3.934074e-04  1.765623e-03  7.886910e-04 3.994963e-04 0.0003576934\n [8,]  4.943276e-04  7.886910e-04  1.518517e-03 3.437684e-04 0.0004289051\n [9,]  4.792224e-04  3.994963e-04  3.437684e-04 3.075515e-03 0.0011512604\n[10,]  2.711309e-04  3.576934e-04  4.289051e-04 1.151260e-03 0.0030881212\n[11,]  1.580928e-03  7.872979e-04  6.366403e-04 1.439675e-03 0.0011535909\n[12,]  5.881401e-04  7.514867e-04  4.972229e-04 5.983421e-05 0.0002224072\n[13,]  7.426056e-04  4.490709e-04  9.903078e-05 1.401373e-03 0.0005654204\n[14,]  3.670699e-04  4.022907e-04  2.546547e-04 5.617317e-04 0.0015767924\n[15,]  1.394333e-03  3.075009e-04  2.545034e-04 4.954782e-04 0.0003710759\n[16,]  5.291520e-04  1.750076e-04  4.106679e-04 1.618941e-03 0.0007001615\n[17,]  2.446268e-04  2.201295e-04  2.059815e-04 6.070280e-04 0.0017308946\n[18,]  1.440363e-03  3.706182e-04  4.282926e-04 4.721104e-04 0.0005205689\n[19,] -2.408574e-05  7.725707e-05  8.859144e-05 7.086189e-04 0.0004992864\n[20,]  6.059513e-04  6.986203e-05 -5.566603e-05 8.917753e-04 0.0004276745\n[21,]  2.480873e-04 -1.299639e-05 -3.392847e-05 3.442378e-04 0.0005256114\n             [,11]         [,12]        [,13]        [,14]         [,15]\n [1,] 6.330714e-05  7.230668e-04 3.194478e-04 0.0003310593 -2.626538e-05\n [2,] 1.877564e-04  6.943860e-04 4.391644e-04 0.0002500548  1.143234e-04\n [3,] 2.994296e-04  7.140702e-04 3.080059e-04 0.0002591961  3.461109e-04\n [4,] 4.332023e-04  2.950943e-04 1.808926e-03 0.0005841647  5.949934e-04\n [5,] 2.887708e-04  3.590450e-04 6.665414e-04 0.0016242307  2.896028e-04\n [6,] 1.580928e-03  5.881401e-04 7.426056e-04 0.0003670699  1.394333e-03\n [7,] 7.872979e-04  7.514867e-04 4.490709e-04 0.0004022907  3.075009e-04\n [8,] 6.366403e-04  4.972229e-04 9.903078e-05 0.0002546547  2.545034e-04\n [9,] 1.439675e-03  5.983421e-05 1.401373e-03 0.0005617317  4.954782e-04\n[10,] 1.153591e-03  2.224072e-04 5.654204e-04 0.0015767924  3.710759e-04\n[11,] 3.100653e-03  3.797460e-04 5.558660e-04 0.0004499206  1.388343e-03\n[12,] 3.797460e-04  1.353425e-03 3.361125e-04 0.0003599263  3.711701e-04\n[13,] 5.558660e-04  3.361125e-04 2.679508e-03 0.0010149171  9.133291e-04\n[14,] 4.499206e-04  3.599263e-04 1.014917e-03 0.0025074061  7.814797e-04\n[15,] 1.388343e-03  3.711701e-04 9.133291e-04 0.0007814797  2.239613e-03\n[16,] 5.671980e-04  3.549025e-04 1.809069e-03 0.0006726168  7.152364e-04\n[17,] 5.230923e-04  8.662901e-05 5.994210e-04 0.0016280518  4.484829e-04\n[18,] 1.453794e-03  3.694086e-04 6.818643e-04 0.0005001348  1.390561e-03\n[19,] 3.721344e-04  8.121422e-05 6.065452e-04 0.0004767595  4.240890e-05\n[20,] 6.949490e-04  2.048871e-04 8.608315e-04 0.0001955560  4.525888e-04\n[21,] 3.341346e-04 -7.180751e-05 6.989798e-05 0.0004952616  1.117222e-04\n             [,16]        [,17]        [,18]         [,19]         [,20]\n [1,] 1.885976e-04 1.132431e-04 0.0002425335  6.016676e-05 -6.323510e-05\n [2,] 2.599218e-04 1.511628e-04 0.0004564482 -5.677038e-08  2.770648e-04\n [3,] 3.321093e-04 5.240330e-05 0.0002708591  6.439974e-05  9.115472e-05\n [4,] 1.573800e-03 5.614512e-04 0.0004202745  4.080167e-04  4.564269e-04\n [5,] 6.905796e-04 1.608248e-03 0.0002271944  3.000318e-04 -3.186490e-05\n [6,] 5.291520e-04 2.446268e-04 0.0014403628 -2.408574e-05  6.059513e-04\n [7,] 1.750076e-04 2.201295e-04 0.0003706182  7.725707e-05  6.986203e-05\n [8,] 4.106679e-04 2.059815e-04 0.0004282926  8.859144e-05 -5.566603e-05\n [9,] 1.618941e-03 6.070280e-04 0.0004721104  7.086189e-04  8.917753e-04\n[10,] 7.001615e-04 1.730895e-03 0.0005205689  4.992864e-04  4.276745e-04\n[11,] 5.671980e-04 5.230923e-04 0.0014537943  3.721344e-04  6.949490e-04\n[12,] 3.549025e-04 8.662901e-05 0.0003694086  8.121422e-05  2.048871e-04\n[13,] 1.809069e-03 5.994210e-04 0.0006818643  6.065452e-04  8.608315e-04\n[14,] 6.726168e-04 1.628052e-03 0.0005001348  4.767595e-04  1.955560e-04\n[15,] 7.152364e-04 4.484829e-04 0.0013905614  4.240890e-05  4.525888e-04\n[16,] 2.714269e-03 9.497205e-04 0.0009504702  8.252786e-04  9.374942e-04\n[17,] 9.497205e-04 2.781060e-03 0.0011345975  3.571314e-04  1.293712e-04\n[18,] 9.504702e-04 1.134597e-03 0.0023072463  5.416910e-05  4.234757e-04\n[19,] 8.252786e-04 3.571314e-04 0.0000541691  2.416128e-03  1.043338e-03\n[20,] 9.374942e-04 1.293712e-04 0.0004234757  1.043338e-03  2.288321e-03\n[21,] 8.680063e-05 4.749021e-04 0.0002621942  4.096861e-04  5.441687e-04\n              [,21]\n [1,] -1.452389e-04\n [2,]  4.045822e-05\n [3,] -1.410212e-04\n [4,] -8.502939e-05\n [5,]  1.437967e-04\n [6,]  2.480873e-04\n [7,] -1.299639e-05\n [8,] -3.392847e-05\n [9,]  3.442378e-04\n[10,]  5.256114e-04\n[11,]  3.341346e-04\n[12,] -7.180751e-05\n[13,]  6.989798e-05\n[14,]  4.952616e-04\n[15,]  1.117222e-04\n[16,]  8.680063e-05\n[17,]  4.749021e-04\n[18,]  2.621942e-04\n[19,]  4.096861e-04\n[20,]  5.441687e-04\n[21,]  1.663587e-03\n\n$covHi\n             [,1]         [,2]         [,3]         [,4]         [,5]\n[1,] 0.0014980127 0.0007465455 0.0007793789 0.0007134071 0.0005453263\n[2,] 0.0007465455 0.0010331381 0.0006387837 0.0006398164 0.0005589013\n[3,] 0.0007793789 0.0006387837 0.0008679826 0.0006400363 0.0005827344\n[4,] 0.0007134071 0.0006398164 0.0006400363 0.0008379947 0.0005613310\n[5,] 0.0005453263 0.0005589013 0.0005827344 0.0005613310 0.0013482139\n[6,] 0.0004596982 0.0005289703 0.0005019538 0.0005248655 0.0006078101\n[7,] 0.0004856933 0.0005415021 0.0005393324 0.0005410202 0.0005874255\n             [,6]         [,7]\n[1,] 0.0004596982 0.0004856933\n[2,] 0.0005289703 0.0005415021\n[3,] 0.0005019538 0.0005393324\n[4,] 0.0005248655 0.0005410202\n[5,] 0.0006078101 0.0005874255\n[6,] 0.0011488908 0.0004461349\n[7,] 0.0004461349 0.0010515910\n\n$covH\n             [,1]\n[1,] 0.0006624019\n\n\n\nPossiamo interpretare i coefficienti di scalabilità nel modo seguente.\n\nCoefficiente di Scalabilità tra Coppie di Item (Hij): Per ogni coppia di item (i, j), il coefficiente \\(H_{ij}\\) valuta l’efficacia con cui questi due item riflettono la variabile latente. Un coefficiente \\(H_{ij}\\) positivo per coppie di item appartenenti alla stessa scala di Mokken indica che questi item sono coerenti e misurano efficacemente la stessa variabile latente. Matematicamente, \\(H_{ij}\\) è definito per ogni coppia di item i e j, dove i, j = 1, …, J.\nCoeffiente di Scalabilità dell’Item (Hj): Il coefficiente \\(H_{j}\\) di un singolo item è analogo ai parametri di discriminazione nei modelli IRT parametrici. Esprime l’efficacia con cui un item distingue tra individui a diversi livelli della variabile latente. Per essere considerato efficace, \\(H_{j}\\) dovrebbe superare un certo limite inferiore, generalmente stabilito a c &gt; 0.3.\nCoefficiente di Scalabilità del Test (H): H rappresenta la scalabilità complessiva dell’intero insieme di item. L’interpretazione di H segue le seguenti soglie euristiche:\n\nDebole: se 0.3 ≤ H &lt; 0.4.\nModerato: se 0.4 ≤ H &lt; 0.5.\nForte: se H &gt; 0.5.\n\nQuesti valori indicano la forza con cui l’insieme di item misura la variabile latente. I coefficienti di scalabilità degli item forniscono indicazioni sulla discriminazione degli item e sulla loro aderenza al modello di omogeneità monotona. Item con bassa discriminazione non contribuiscono a un ordinamento affidabile degli esaminandi e dovrebbero essere scartati.\n\nSecondo Sijtsma e Molenaar (2002), le assunzioni di unidimensionalità, indipendenza locale e monotonicità implicano le seguenti restrizioni sui coefficienti di scalabilità: - 0 ≤ \\(H_{ij}\\) ≤ 1, per tutte le coppie di item i ≠ j. - 0 ≤ \\(H_{j}\\) ≤ 1, per tutti gli item j. - 0 ≤ \\(H\\) ≤ 1, per l’intero insieme di item.\nI coefficienti di scalabilità sono fondamentali per valutare quanto efficacemente un insieme di item lavori insieme per misurare una variabile latente. Valori alti di \\(H\\) suggeriscono che l’insieme di item è fortemente correlato e misura in modo affidabile la variabile latente, garantendo che l’analisi con la MSA sia valida e affidabile.\nÈ possibile ottenere il numero di valori negativi \\(H_{ij}\\) per ciascun item usando il codice seguente.\n\nscal_coef &lt;- coefH(good_items, se = FALSE)$Hij\napply(scal_coef, 1, function(x) sum(x &lt; 0)) |&gt; print()\n\n$Hij\n           FAI_49   FAI_106    FAI_60   FAI_124     FAI_5    FAI_81    FAI_83\nFAI_49  1.0000000 0.5613569 0.6075009 0.6005396 0.2118286 0.2539023 0.2229846\nFAI_106 0.5613569 1.0000000 0.6172766 0.7223965 0.3168352 0.3539449 0.3812326\nFAI_60  0.6075009 0.6172766 1.0000000 0.6879216 0.2795178 0.3243635 0.4303254\nFAI_124 0.6005396 0.7223965 0.6879216 1.0000000 0.2879511 0.3813744 0.3866023\nFAI_5   0.2118286 0.3168352 0.2795178 0.2879511 1.0000000 0.4132827 0.4085199\nFAI_81  0.2539023 0.3539449 0.3243635 0.3813744 0.4132827 1.0000000 0.5506000\nFAI_83  0.2229846 0.3812326 0.4303254 0.3866023 0.4085199 0.5506000 1.0000000\n\n$Hi\n   FAI_49   FAI_106    FAI_60   FAI_124     FAI_5    FAI_81    FAI_83 \n0.4096841 0.4929127 0.4838063 0.5019022 0.3231299 0.3862103 0.4040312 \n\n$H\n[1] 0.4279251\n\n FAI_49 FAI_106  FAI_60 FAI_124   FAI_5  FAI_81  FAI_83 \n      0       0       0       0       0       0       0 \n\n\nL’istruzione seguente esegue la procedura frequentista del test di ipotesi, con l’ipotesi alternativa che i coefficienti di scalabilità sono maggiori di zero. Il test è unidirezionale, dunque il valore soglia della statistica \\(z\\) è 1.65.\n\ncoefZ(good_items) |&gt; print()\n\n$Zij\n           FAI_49   FAI_106    FAI_60   FAI_124    FAI_5    FAI_81    FAI_83\nFAI_49   0.000000 11.122254 11.766708 11.519386 4.077595  4.303706  4.213676\nFAI_106 11.122254  0.000000 12.328013 14.049032 6.189922  6.313023  7.368202\nFAI_60  11.766708 12.328013  0.000000 14.039308 5.741312  6.231554  8.542324\nFAI_124 11.519386 14.049032 14.039308  0.000000 5.889676  7.265907  7.779304\nFAI_5    4.077595  6.189922  5.741312  5.889676 0.000000  8.141997  8.214378\nFAI_81   4.303706  6.313023  6.231554  7.265907 8.141997  0.000000 10.794877\nFAI_83   4.213676  7.368202  8.542324  7.779304 8.214378 10.794877  0.000000\n\n$Zi\n  FAI_49  FAI_106   FAI_60  FAI_124    FAI_5   FAI_81   FAI_83 \n18.87934 23.25525 23.49005 24.24297 15.73678 17.72590 19.40083 \n\n$Z\n[1] 37.97973\n\n\n\nNel caso presente, il test indica che i coefficienti di scalabilità di tutti gli item, sia considerati singolarmente sia considerati a coppie, sono maggiori di zero. Lo stesso si può dire per il coefficiente di scalabilità della scala nel suo complesso.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/03_applications.html#monotonicità",
    "href": "chapters/mokken/03_applications.html#monotonicità",
    "title": "66  Applicazione Pratica",
    "section": "66.6 Monotonicità",
    "text": "66.6 Monotonicità\nCome spiegato in precedenza, la monotonicità è un’importante assunzione della MSA. La probabilità di una risposta corretta dovrebbe aumentare con θ. La monotonicità può essere esaminata con il seguente codice:\n\nmonoton &lt;- check.monotonicity(good_items)\nsummary(monoton) |&gt; print()\n\n        ItemH #ac #vi #vi/#ac maxvi sum sum/#ac zmax #zsig crit\nFAI_49   0.41  20   0       0     0   0       0    0     0    0\nFAI_106  0.49  12   0       0     0   0       0    0     0    0\nFAI_60   0.48  18   0       0     0   0       0    0     0    0\nFAI_124  0.50  21   0       0     0   0       0    0     0    0\nFAI_5    0.32  24   0       0     0   0       0    0     0    0\nFAI_81   0.39  24   0       0     0   0       0    0     0    0\nFAI_83   0.40  21   0       0     0   0       0    0     0    0\n\n\nOgni riga dell’output rappresenta un item (ad esempio, FAI_49, FAI_106, ecc.). La spiegazione delle colonne è la seguente:\n\nItemH: Il coefficiente H per ogni item, che misura l’omogeneità dell’item. Un valore più alto indica una maggiore omogeneità. Nelle scale di Mokken, si cercano generalmente valori superiori a 0.3.\n#ac (Active pairs): Il numero di coppie attive, ovvero coppie di risposte che contribuiscono alla stima dell’H.\n#vi (Violations): Il numero di violazioni della monotonicità. La monotonicità implica che, man mano che aumenta il punteggio totale del test, la probabilità di una risposta positiva all’item non diminuisce.\n#vi/#ac: Il rapporto tra il numero di violazioni e il numero di coppie attive.\nmaxvi (Maximum violation): La massima violazione osservata.\nsum (Sum of violation size): La somma delle dimensioni delle violazioni.\nsum/#ac: Il rapporto tra la somma delle dimensioni delle violazioni e il numero di coppie attive.\nzmax: Il valore massimo della statistica Z per le violazioni.\n#zsig (Number of significant Z): Numero di statistiche Z significative.\ncrit: Un criterio per giudicare se le violazioni sono problematiche.\n\nNel caso presente, sembra che non ci siano violazioni della monotonicità per nessuno degli item elencati. Questo significa che per questi item, all’aumentare del punteggio totale, non si osserva una diminuzione della probabilità di una risposta positiva, mantenendo quindi una buona coerenza interna e validità per la scala.\nUn grafico della monotonicità per una coppia di item si ottiene nel modo seguente.\n\nplot(check.monotonicity(good_items), item = c(1, 2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa figura di questo esempio illustra i grafici di monotonicità per gli Item 49 e 106. Il grafico è diviso in due pannelli. Il pannello sul lato sinistro mostra le Funzioni di Risposta del Passaggio dell’Item (ISRFs), mentre il pannello sul lato destro mostra la Funzione di Risposta all’Item (IRF) complessiva per ciascun item. Il grafico evidenzia che sia l’IRF sia le ISRFs sono sempre non decrescenti per l’item 49; per l’item 106, invece, si osserva una piccola violazione della monotonicità.\nSe esaminiamo tutti gli item dell’area relativa alle caratteristiche del bambino (non solo quelli selezionati dalla procedura AISP) notiamo come, per alcuni item, si osserva un numero di violazioni maggiore di zero.\n\nmonoton2 &lt;- check.monotonicity(subscale_data)\nsummary(monoton2) |&gt; print()\n\n        ItemH #ac #vi #vi/#ac maxvi  sum sum/#ac zmax #zsig crit\nFAI_49   0.25  24   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_106  0.30  15   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_60   0.30  18   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_124  0.32  24   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_86   0.16  21   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_47   0.23  24   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_121  0.18  23   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_167  0.18  24   2    0.08  0.04 0.07  0.0031 1.07     0   31\nFAI_99   0.12  24   1    0.04  0.05 0.05  0.0023 1.58     0   31\nFAI_63   0.23  24   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_168  0.23  18   1    0.06  0.06 0.06  0.0032 0.73     0   25\nFAI_5    0.23  24   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_132  0.21  24   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_85   0.17  19   1    0.05  0.07 0.07  0.0035 1.08     0   30\nFAI_81   0.24  24   1    0.04  0.04 0.04  0.0015 0.52     0   17\nFAI_83   0.24  24   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_57   0.21  15   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_91   0.18  21   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_135  0.15  21   1    0.05  0.03 0.03  0.0016 0.39     0   22\nFAI_1    0.10  21   0    0.00  0.00 0.00  0.0000 0.00     0    0",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/03_applications.html#ordinamento-invariante-degli-item",
    "href": "chapters/mokken/03_applications.html#ordinamento-invariante-degli-item",
    "title": "66  Applicazione Pratica",
    "section": "66.7 Ordinamento Invariante degli Item",
    "text": "66.7 Ordinamento Invariante degli Item\nIl passo successivo nella MSA è indagare l’ordinamento invariante degli item (IIO) o la non intersezione delle Funzioni di Risposta all’Item (IRFs). È fondamentale determinare se l’ordine degli item sia lo stesso per tutti i rispondenti con diversi livelli del tratto. Esistono diversi metodi per esaminare l’IIO. Il metodo predefinito nel pacchetto R mokken è l’IIO manifesto o MIIO (Manifest IIO) (Ligtvoet, Van der Ark, Te Marvelde & Sijtsma, 2010). Per esaminare l’ordinamento invariante degli item, eseguiamo il seguente codice.\n\niio &lt;- check.iio(good_items)\nsummary(iio) |&gt; print()\n\n$method\n[1] \"MIIO\"\n\n$item.summary\n        ItemH #ac #vi #vi/#ac maxvi  sum sum/#ac tmax #tsig crit\nFAI_81   0.39  18   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_83   0.40  18   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_124  0.50  18   1    0.06  0.31 0.31  0.0171 3.50     1   83\nFAI_5    0.32  18   2    0.11  0.42 0.73  0.0407 3.50     2  145\nFAI_60   0.48  18   1    0.06  0.42 0.42  0.0236 2.49     1   97\nFAI_106  0.49  18   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_49   0.41  18   0    0.00  0.00 0.00  0.0000 0.00     0    0\n\n$backward.selection\n        step 1 step 2\nFAI_81       0      0\nFAI_83       0      0\nFAI_124      1      0\nFAI_5        2     NA\nFAI_60       1      0\nFAI_106      0      0\nFAI_49       0      0\n\n$HT\n[1] 0.3358101\n\n\n\nL’output include due tabelle principali. La prima tabella contiene le seguenti colonne. La prima colonna, “ItemH”, mostra il coefficiente di scalabilità Hi per ciascun item, ‘#ac’ indica il numero totale di coppie attive, ‘#vi’ segnala il numero totale di violazioni, ‘#vi/#ac’ mostra il numero medio di violazioni per coppia attiva, ‘maxvi’ indica la massima violazione, ‘sum’ rappresenta la somma di tutte le violazioni, ‘sum/#ac’ mostra la media delle violazioni per coppia attiva, ‘tmax’ indica la statistica di test massima, ‘#tsig’ il numero di violazioni significative, e il valore ‘crit’ è una somma ponderata di altri elementi come ‘itemH’, ‘#ac’, ecc. Valori elevati di ‘crit’ indicano item di scarsa qualità (il valore 0 è perfetto, valori più alti sono peggiori).\nL’output precedente mostra che l’Item FAI_5 ha 2 violazioni. In altre parole, la IRF per questo item si interseca con la IRF di altri due item, ed entrambe queste violazioni sono significative (#tsig per questo item è 2). Poiché ha il numero più alto di violazioni, è un buon candidato per essere rimosso dal test. Rimuovere questo item risolve l’intersezione degli altri item con questo item.\nPer esaminare graficamente queste intersezione, usiamo il codice seguente.\n\nplot(iio)",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/03_applications.html#non-intersezione-delle-funzioni-di-risposta-degli-step-degli-item",
    "href": "chapters/mokken/03_applications.html#non-intersezione-delle-funzioni-di-risposta-degli-step-degli-item",
    "title": "66  Applicazione Pratica",
    "section": "66.8 Non intersezione delle funzioni di risposta degli step degli item",
    "text": "66.8 Non intersezione delle funzioni di risposta degli step degli item\nPer indagare sulla non intersezione delle funzioni di risposta degli step degli item si possono impiegare le matrici P++ e P– (Molenaar & Sijtsma, 2000).\n\npmatrix &lt;- check.pmatrix(good_items)\nsummary(pmatrix) |&gt; print()\n\n        ItemH  #ac #vi #vi/#ac maxvi  sum sum/#ac zmax #zsig crit\nFAI_49   0.41 1920  17    0.01  0.05 0.65   3e-04 2.93    15   67\nFAI_106  0.49 1920  10    0.01  0.06 0.39   2e-04 4.21    10   59\nFAI_60   0.48 1920  23    0.01  0.06 0.94   5e-04 3.88    22   81\nFAI_124  0.50 1920  20    0.01  0.06 0.84   4e-04 4.60    20   80\nFAI_5    0.32 1920  25    0.01  0.06 1.07   6e-04 3.88    24   92\nFAI_81   0.39 1920  28    0.01  0.05 1.09   6e-04 3.60    27   89\nFAI_83   0.40 1920  17    0.01  0.06 0.68   4e-04 4.60    16   78\n\n\nCome spiegato nelle sezioni precedenti, la colonna ‘#vi’ indica il numero di violazioni delle ISRF. La colonna ‘#zsig’ mostra il numero di violazioni statisticamente significative. Le violazioni possono anche essere controllate graficamente utilizzando il seguente codice:\n\nplot(check.pmatrix(good_items), item = 5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa non intersezione delle ISRF può essere esaminata anche con il metodo del punteggio residuo.\n\nrestscore &lt;- check.restscore(good_items) \nsummary(restscore) |&gt; print()\n\n        ItemH #ac #vi #vi/#ac maxvi  sum sum/#ac zmax #zsig crit\nFAI_49   0.41 288   5    0.02  0.08 0.23  0.0008 1.19     0   18\nFAI_106  0.49 288   7    0.02  0.07 0.33  0.0011 2.16     2   34\nFAI_60   0.48 288   6    0.02  0.14 0.47  0.0016 2.19     4   50\nFAI_124  0.50 288   9    0.03  0.15 0.56  0.0019 2.78     1   44\nFAI_5    0.32 288  14    0.05  0.15 1.01  0.0035 2.78     5   74\nFAI_81   0.39 288   8    0.03  0.10 0.44  0.0015 1.70     1   37\nFAI_83   0.40 288   9    0.03  0.07 0.40  0.0014 2.07     1   36\n\n\nI risultati del metodo del punteggio residuo sono diversi da quelli delle matrici P++ e P–. L’output viene interpretato come abbiamo fatto in precedenza. La colonna ‘#vi’ indica il numero di violazioni delle ISRF. La colonna ‘#zsig’ mostra il numero di violazioni statisticamente significative. Questa colonna mostra che il numero di violazioni statisticamente significative è molto inferiore rispetto a quelle riportate dalle matrici P++ e P–. La violazione della non intersezione può essere esaminata graficamente nel modo seguente. Per esempio, consideriamo gli item FAI_49 e FAI_5.\n\nplot(restscore, item.pairs = c(4, 5))",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/03_applications.html#affidabilità",
    "href": "chapters/mokken/03_applications.html#affidabilità",
    "title": "66  Applicazione Pratica",
    "section": "66.9 Affidabilità",
    "text": "66.9 Affidabilità\nIl pacchetto Mokken calcola quattro diversi coefficienti di affidabilità: l’affidabilità della scala Mokken (MS) ρ, (Mokken, 1971), Lambda-2 (Guttman, 1945), l’alpha di Cronbach (Cronbach, 1951), e il coefficiente di affidabilità della classe latente (LCRC, van der Ark, van der Palm, & Sijtsma, 2011).\n\nMS (Molenaar-Sijtsma Method): Questo indice è basato sul metodo Molenaar-Sijtsma di stima dell’affidabilità per scale non parametriche, come quelle analizzate con l’analisi Mokken. Questo metodo considera la varianza tra gli item e la varianza totale per stimare l’affidabilità.\nAlpha (Cronbach’s Alpha): L’alpha di Cronbach è forse il più noto indice di affidabilità, utilizzato per valutare la consistenza interna degli item di un test. Misura fino a che punto gli item di un test sono correlati tra loro.\nLambda-2: Un altro indice di affidabilità, simile all’alpha di Cronbach, ma talvolta considerato più robusto poiché tiene conto delle correlazioni medie tra gli item.\nLCRC (Latent Class Reliability Coefficient): Questo è un indice di affidabilità che tiene conto dell’approccio delle classi latenti. È particolarmente utile quando gli item possono essere raggruppati in sottoscale che riflettono diversi costrutti o dimensioni.\n\nPer ottenere queste stime dell’affidabilità è possibile eseguire il seguente codice.\n\ncheck.reliability(good_items, LCRC = TRUE)\n\n\n    $MS\n        0.819110476790128\n    $alpha\n        0.81675073603741\n    $lambda.2\n        0.824148383038108\n    $LCRC\n        0.843559865692261\n\n\n\nIn generale, tutti i valori ottenuti indicano che la scala in considerazione ha un’alta affidabilità. Questo significa che è probabile che produca risultati coerenti nel tempo e che gli item che la compongono siano correlati tra loro in modo significativo, contribuendo tutti a misurare lo stesso costrutto o costrutti correlati.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/03_applications.html#identificazione-degli-outlier",
    "href": "chapters/mokken/03_applications.html#identificazione-degli-outlier",
    "title": "66  Applicazione Pratica",
    "section": "66.10 Identificazione degli Outlier",
    "text": "66.10 Identificazione degli Outlier\nGli outlier sono persone con modelli di risposta aberranti o molti errori di Guttman. Sijtsma e van der Ark (2017) raccomandano di rimuovere gli outlier e di rieseguire l’analisi senza di essi. Se c’è una differenza notevole nei risultati, allora la rimozione degli outlier è giustificata. Per ottenere il numero di errori di Guttman per ogni rispondente eseguiamo il seguente codice:\n\ngutt &lt;- check.errors(good_items)\nprint(gutt)\n\n$Gplus\n  [1]  2  6  0  7  2  4 23 30  2  7 15  0  1  9  2  0  5 24  3  3  0  0 11 51  0\n [26]  4  2 19  9  2 60  2  9 35  2 17 39  2  4  3  3 12  3  0 10  2  0  0 37  4\n [51] 68  0  2 31  2  0 12 36  1 70 12  1 28 27 25  3  4 10  9 16 28 22 81 26  1\n [76]  0 65 28  0 25  3 59 60 40  0  5  0  0  1  0  8  3  8  6 27 12 22 16 12  4\n[101] 12  9  0  0 20 25 25  6  4  6  0 47 20 18  4  7  6 33 14 25 12  4  0  1 65\n[126] 17 83 26 65  5  5 30  2  9 42  7 13  8 16  3 41  0 12 21  4  8 26 22  3 46\n[151]  7 14 38 12 12  1  6  4 32 11  2  6 17  1  8  8 27  8  3  5 58  2 62  3  0\n[176] 12 10 45 21  0  2 10 31 19 46 11 21  2 25  9 17  2  4 20 21 49 18 19 14 16\n[201] 44  5  7 14 41 24 44  0 18  9 31 12  9 46  8  6 11 14 23 41 24  1  7 39 23\n[226]  0 23  3  6  2  2  4  0  0  8 24 38  0 16 62  2  2 21  7 25  9  0  0  8 14\n[251] 10  4  1  0  2 18  0 21 19  0 12  0 41  8  7 16 25 20  3 41  3  0  6 18  0\n[276]  4  2  4  4 12  5 13  6  9  7  6  0  6  7  2  0  0 13 26  8 41 30  2  0  6\n[301]  6  0  5  2  0  0 39 48  0 43  8 15 10  0  4 25 16  9  0  5  2  2  8  2  8\n[326]  7  1  0 53 32  9  3  0  4 13  3  6 35 30  2 34  8 10 10  5 23 22 24 12  3\n[351]  0  9 16 21 39 27 47 19 28 32  7  2 16  0  3 26 34  6  4 11  7 25  1 15  6\n[376] 10 38 17  0  6  3 38  4  0 52  5  2  3  0  0 14  0 27 22 32  8 32 14  2 31\n[401] 14 29 52 12 11 32  2  3 17  2  0  5 62 86 34  5 24 39  7 10 31  0  0 25 10\n[426] 19 31 13  7 31 36  8  0  0 28  6 23 15 52  3  1  7  4 33 24 11 24  0  2  3\n[451]  0  1  0\n\n$UGplus\n$UGplus$U1Gplus\n[1] 54.5\n\n$UGplus$U2Gplus\n[1] 354.5259\n\n\n\n\nIl vettore $Gplus mostra il numero di errori di Guttman G+ per ogni persona. I valori 54.5 e 354.5259 sono le Tukey fences per il rilevamento degli outlier. Essi segnalano i casi in cui il numero di errori di Guttman G+ è oltre il Tukey fence della distribuzione di G+.\nIl primo valore, 54.5, è il limite per il numero di errori di Guttman se la distribuzione è approssimativamente normale. I rispondenti con un numero di errori di Guttman superiore al limite possono essere considerati sospetti. Il valore 354.5259 è un limite di soglia per il numero di errori di Guttman se la distribuzione è asimmetrica.\n\nhist(gutt$Gplus)\n\n\n\n\n\n\n\n\nPer il caso presente, l’istogramma del numero di errori di Gutmann mostra che la loro distribuzione è asimmetrica positiva. Pertanto, possiamo scegliere il secondo limite soglia. Per trovare le persone con valori G+ sopra il Tucky fence superiore, eseguiamo il seguente codice.\n\nerr &lt;- gutt$Gplus \nwhich(err &gt; 354.5259)\n\n\n\n\nI risultati indicano che, in base a questo secondo criterio, non ci sono modelli di risposta sospetti nei dati.\nSe questa procedura producesse invece come risultato l’individuazione di alcuni outlier, potremmo rimuoverli dai dati nel modo seguente:\ngood_items_clean &lt;- good_items[-which(err &gt; 354.5259), ]",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/03_applications.html#considerazioni-conclusive",
    "href": "chapters/mokken/03_applications.html#considerazioni-conclusive",
    "title": "66  Applicazione Pratica",
    "section": "66.11 Considerazioni conclusive",
    "text": "66.11 Considerazioni conclusive\nIn questo capitolo, abbiamo esplorato un approccio pratico all’analisi di un set di dati reali, utilizzando il caso di studio sull’indagine della capacità di coping delle famiglie di fronte alla diagnosi di una grave malattia in un figlio, condotta dai ricercatori del Meyer. Attraverso una serie di passaggi metodici e analitici rigorosi, siamo riusciti a trasformare un complesso insieme di dati in informazioni comprensibili e significative.\nAbbiamo iniziato importando i dati e conducendo un’accurata pulizia per rimuovere gli item con eccessiva asimmetria e curtosi. Successivamente, abbiamo implementato la Procedura di Selezione Automatica degli Item (AISP) nell’ambito dell’Analisi delle Scale Mokken (MSA) per identificare e selezionare scale omogenee e coerenti.\nUna volta stabilite le scale, abbiamo approfondito le caratteristiche degli item, esaminando le loro statistiche descrittive e la loro distribuzione. Abbiamo anche valutato la loro affidabilità attraverso vari coefficienti, tra cui l’Alpha di Cronbach e il Coefficiente di Affidabilità della Classe Latente (LCRC), rilevando un’alta coerenza interna.\nUlteriori analisi hanno incluso la verifica della monotonicità degli item e dell’ordinamento invariante degli item (IIO), fondamentali per garantire che la scala rispettasse i principi teorici sottostanti la MSA. Infine, abbiamo esaminato la presenza di outlier, utilizzando i Tukey fences per identificare e gestire i modelli di risposta aberranti.\nIn conclusione, questo capitolo dimostra come l’applicazione metodica e sistematica delle tecniche di analisi statistica possa fornire intuizioni preziose e comprensibili da un set di dati complesso. Ciò non solo rafforza la validità e l’affidabilità della ricerca, ma fornisce anche una base solida per ulteriori indagini e interpretazioni.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/03_applications.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/mokken/03_applications.html#informazioni-sullambiente-di-sviluppo",
    "title": "66  Applicazione Pratica",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats4    stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] mokken_3.1.2         poLCA_1.6.0.1        MASS_7.3-61         \n [4] scatterplot3d_0.3-44 mirt_1.42            lattice_0.22-6      \n [7] TAM_4.2-21           CDM_8.2-6            mvtnorm_1.3-1       \n[10] ggokabeito_0.1.0     viridis_0.6.5        viridisLite_0.4.2   \n[13] ggpubr_0.6.0         ggExtra_0.10.1       bayesplot_1.11.1    \n[16] gridExtra_2.3        patchwork_1.3.0      semTools_0.5-6      \n[19] semPlot_1.1.6        lavaan_0.6-18        psych_2.4.6.26      \n[22] scales_1.3.0         markdown_1.13        knitr_1.48          \n[25] lubridate_1.9.3      forcats_1.0.0        stringr_1.5.1       \n[28] dplyr_1.1.4          purrr_1.0.2          readr_2.1.5         \n[31] tidyr_1.3.1          tibble_3.2.1         ggplot2_3.5.1       \n[34] tidyverse_2.0.0      here_1.0.1          \n\nloaded via a namespace (and not attached):\n  [1] splines_4.4.1        later_1.3.2          pbdZMQ_0.3-13       \n  [4] R.oo_1.26.0          XML_3.99-0.17        rpart_4.1.23        \n  [7] lifecycle_1.0.4      rstatix_0.7.2        rprojroot_2.0.4     \n [10] globals_0.16.3       rockchalk_1.8.157    backports_1.5.0     \n [13] magrittr_2.0.3       openxlsx_4.2.7.1     Hmisc_5.1-3         \n [16] rmarkdown_2.28       httpuv_1.6.15        qgraph_1.9.8        \n [19] zip_2.3.1            sessioninfo_1.2.2    pbapply_1.7-2       \n [22] minqa_1.2.8          audio_0.1-11         multcomp_1.4-26     \n [25] abind_1.4-8          quadprog_1.5-8       R.utils_2.12.3      \n [28] nnet_7.3-19          TH.data_1.1-2        sandwich_3.1-1      \n [31] listenv_0.9.1        testthat_3.2.1.1     RPushbullet_0.3.4   \n [34] vegan_2.6-8          arm_1.14-4           parallelly_1.38.0   \n [37] permute_0.9-7        codetools_0.2-20     tidyselect_1.2.1    \n [40] farver_2.1.2         lme4_1.1-35.5        base64enc_0.1-3     \n [43] jsonlite_1.8.9       polycor_0.8-1        progressr_0.14.0    \n [46] Formula_1.2-5        survival_3.7-0       emmeans_1.10.4      \n [49] tools_4.4.1          snow_0.4-4           Rcpp_1.0.13         \n [52] glue_1.7.0           mnormt_2.1.1         xfun_0.47           \n [55] mgcv_1.9-1           admisc_0.36          IRdisplay_1.1       \n [58] withr_3.0.1          beepr_2.0            fastmap_1.2.0       \n [61] boot_1.3-31          fansi_1.0.6          digest_0.6.37       \n [64] mi_1.1               timechange_0.3.0     R6_2.5.1            \n [67] mime_0.12            estimability_1.5.1   colorspace_2.1-1    \n [70] gtools_3.9.5         jpeg_0.1-10          R.methodsS3_1.8.2   \n [73] utf8_1.2.4           generics_0.1.3       data.table_1.16.0   \n [76] corpcor_1.6.10       SimDesign_2.17.1     htmlwidgets_1.6.4   \n [79] pkgconfig_2.0.3      sem_3.1-16           gtable_0.3.5        \n [82] brio_1.1.5           htmltools_0.5.8.1    carData_3.0-5       \n [85] png_0.1-8            rstudioapi_0.16.0    tzdb_0.4.0          \n [88] reshape2_1.4.4       uuid_1.2-1           curl_5.2.3          \n [91] coda_0.19-4.1        checkmate_2.3.2      nlme_3.1-166        \n [94] nloptr_2.1.1         repr_1.1.7           zoo_1.8-12          \n [97] parallel_4.4.1       miniUI_0.1.1.1       foreign_0.8-87      \n[100] pillar_1.9.0         grid_4.4.1           vctrs_0.6.5         \n[103] promises_1.3.0       car_3.1-2            OpenMx_2.21.12      \n[106] xtable_1.8-4         Deriv_4.1.6          cluster_2.1.6       \n[109] dcurver_0.9.2        GPArotation_2024.3-1 htmlTable_2.4.3     \n[112] evaluate_1.0.0       pbivnorm_0.6.0       cli_3.6.3           \n[115] kutils_1.73          compiler_4.4.1       rlang_1.1.4         \n[118] crayon_1.5.3         future.apply_1.11.2  ggsignif_0.6.4      \n[121] fdrtool_1.2.18       plyr_1.8.9           stringi_1.8.4       \n[124] munsell_0.5.1        lisrelToR_0.3        pacman_0.5.1        \n[127] Matrix_1.7-0         IRkernel_1.3.2       hms_1.1.3           \n[130] glasso_1.11          future_1.34.0        shiny_1.9.1         \n[133] igraph_2.0.3         broom_1.0.6          RcppParallel_5.1.9",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_rasch_model.html",
    "href": "chapters/irt/01_rasch_model.html",
    "title": "67  Modello di Rasch",
    "section": "",
    "text": "67.1 Introduzione\nLa psicometria ha compiuto notevoli progressi con l’introduzione della Teoria della Risposta all’Item (IRT), un approccio innovativo che supera i limiti della Classica Teoria dei Test (CTT), particolarmente nella gestione e nella concettualizzazione degli errori di misurazione. A differenza della CTT, che si concentra sull’analisi del punteggio totale di un test, l’IRT esamina le risposte ai singoli item, fornendo una visione più dettagliata delle capacità individuali. Questa analisi si fonda su due elementi fondamentali: le caratteristiche degli item, che includono parametri come la difficoltà e la capacità discriminativa, e il livello di abilità del rispondente, che rappresenta la sua posizione su un continuum latente della capacità.\nIl nucleo dell’IRT è la modellizzazione della probabilità che un individuo risponda correttamente a un determinato item, espressa come funzione del livello di abilità del rispondente e delle caratteristiche dell’item stesso. Questa relazione viene rappresentata graficamente attraverso le Curve Caratteristiche degli Item, che illustrano come la probabilità di risposta corretta varia in base al livello di abilità.\nL’IRT presenta numerosi vantaggi. Offre una notevole flessibilità, consentendo di modellare sia risposte dicotomiche che categoriali o ordinali, molto comuni nei test psicometrici. L’analisi a livello di item permette di identificare item problematici, come quelli troppo facili o incapaci di discriminare tra rispondenti con diversi livelli di abilità. Inoltre, le stime di abilità non dipendono dagli item specifici somministrati, rendendo i test basati sull’IRT particolarmente adatti per applicazioni adattive. L’IRT fornisce anche una funzione di informazione del test che permette di valutare la precisione della misurazione per diversi livelli di abilità.\nTuttavia, l’IRT presenta anche alcune sfide. La stima affidabile dei parametri richiede campioni di dimensioni maggiori rispetto alla CTT, e i modelli, soprattutto quelli multidimensionali, necessitano di una padronanza di tecniche statistiche avanzate.\nNel contesto della psicometria moderna, è importante considerare anche la Mokken Scale Analysis (MSA), un approccio non parametrico per l’analisi di scale psicometriche. A differenza dell’IRT, la MSA non presuppone una forma specifica per le curve caratteristiche degli item, basandosi invece su criteri meno restrittivi come la monotonicità e l’indipendenza locale. La MSA risulta particolarmente utile per verificare la struttura della scala e individuare item problematici, rappresentando spesso un ottimo punto di partenza per analisi preliminari, specialmente con campioni di dimensioni ridotte.\nUn approccio efficace all’analisi psicometrica può prevedere l’integrazione di MSA e IRT. Si può iniziare con una valutazione preliminare degli item e della scala attraverso la MSA, per poi passare a un’analisi più approfondita mediante l’IRT. Questa combinazione permette di sfruttare i punti di forza di entrambi gli approcci: la MSA offre una valutazione iniziale della scalabilità e della coerenza interna degli item, mentre l’IRT fornisce una modellizzazione dettagliata degli item e delle abilità latenti, con una precisa valutazione della scala.\nIn conclusione, l’IRT rappresenta una pietra miliare nella psicometria moderna, offrendo strumenti sofisticati per l’analisi di item e abilità latenti. Tuttavia, la Mokken Scale Analysis mantiene la sua utilità, particolarmente in fasi esplorative o in contesti con dati limitati. Una comprensione integrata di entrambi gli approcci consente una valutazione più completa e robusta delle scale psicometriche, permettendo ai ricercatori di scegliere gli strumenti più appropriati in base alle specifiche esigenze del loro studio.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_rasch_model.html#curve-caratteristiche-degli-item",
    "href": "chapters/irt/01_rasch_model.html#curve-caratteristiche-degli-item",
    "title": "67  Modello di Rasch",
    "section": "\n67.2 Curve Caratteristiche degli Item",
    "text": "67.2 Curve Caratteristiche degli Item\nUn concetto fondamentale nell’IRT è la Curva Caratteristica dell’Item (Item Characteristic Curve, ICC), che rappresenta graficamente la relazione tra il livello di abilità latente di un individuo e la sua probabilità di rispondere correttamente a un determinato item o di manifestare un comportamento specifico. Questa relazione viene tipicamente modellata attraverso una funzione logistica che genera una curva a forma di S (sigmoide), dove la probabilità di risposta corretta varia in modo sistematico con il livello di abilità: è molto bassa per individui con scarse capacità, aumenta rapidamente nella fascia intermedia e tende asintoticamente al massimo per i livelli più elevati di abilità, senza mai raggiungerlo completamente.\nLe ICC rappresentano uno strumento prezioso per valutare la qualità degli item di un test, fornendo informazioni cruciali su diversi aspetti. Attraverso l’analisi della forma e dei parametri di una ICC, è possibile determinare la difficoltà dell’item, misurata dal livello di abilità necessario per ottenere una probabilità del 50% di risposta corretta. Si può inoltre valutare la discriminatività dell’item, ovvero la sua capacità di distinguere efficacemente tra individui con diversi livelli di abilità. Nei modelli più sofisticati, è anche possibile stimare la probabilità che un rispondente con bassa abilità fornisca una risposta corretta per caso.\nL’analisi delle ICC permette inoltre di identificare diverse problematiche comuni negli item. L’effetto soffitto si manifesta quando un item risulta troppo facile, portando quasi tutti i partecipanti a rispondere correttamente indipendentemente dal loro livello di abilità, rendendo così l’item poco utile per la misurazione del costrutto. Al contrario, l’effetto pavimento si verifica quando un item è talmente difficile che solo gli individui con abilità molto elevate riescono a rispondere correttamente. Un altro problema frequente è la bassa discriminatività, che si presenta quando l’item non riesce a differenziare efficacemente tra individui con diversi livelli di abilità.\nLe Curve Caratteristiche degli Item si rivelano quindi uno strumento essenziale nell’ambito dell’IRT, offrendo una rappresentazione chiara e dettagliata della relazione tra le abilità latenti e le prestazioni sugli item. La loro capacità di evidenziare le caratteristiche specifiche di ciascun item le rende fondamentali per il miglioramento della qualità e dell’affidabilità dei test psicometrici. Attraverso l’analisi delle ICC, è possibile ottimizzare la misurazione, assicurando che ogni item contribuisca in modo efficace alla valutazione del costrutto di interesse. Quando vengono identificati problemi rilevanti attraverso l’analisi delle ICC, gli item possono essere sottoposti a revisione o, se necessario, eliminati dal test, garantendo così la massima precisione nella misurazione dei costrutti psicologici.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_rasch_model.html#la-scala-di-guttman",
    "href": "chapters/irt/01_rasch_model.html#la-scala-di-guttman",
    "title": "67  Modello di Rasch",
    "section": "\n67.3 La Scala di Guttman",
    "text": "67.3 La Scala di Guttman\nPer comprendere meglio la teoria alla base delle ICC, è utile iniziare dalla Scala di Guttman, che stabilisce una relazione gerarchica tra la difficoltà degli item e le abilità degli individui. In una Scala di Guttman ideale, si assume che una persona con un determinato livello di abilità risponda correttamente a tutti gli item meno difficili e sbagli quelli più difficili. Questo modello si può rappresentare attraverso una matrice di risposte dove 1 indica una risposta corretta e 0 una risposta errata: in una scala perfetta, le risposte corrette si accumulano progressivamente man mano che il livello di abilità dell’individuo aumenta.\nLa seguente tabella mostra un esempio di una Scala di Guttman perfetta per cinque item.\n\n\n\n\n\n\n\n\n\n\nPattern di risposta\nItem 1\nItem 2\nItem 3\nItem 4\nItem 5\n\n\n\n1\n0\n0\n0\n0\n0\n\n\n2\n1\n0\n0\n0\n0\n\n\n3\n1\n1\n0\n0\n0\n\n\n4\n1\n1\n1\n0\n0\n\n\n5\n1\n1\n1\n1\n0\n\n\n6\n1\n1\n1\n1\n1\n\n\n\nIn questo modello ideale, le risposte corrette si accumulano man mano che il livello di abilità dell’individuo aumenta.\nGraficamente, la Scala di Guttman può essere rappresentata tramite curve caratteristiche degli item che mostrano, sull’asse verticale, la probabilità di rispondere correttamente a un item (che nel modello ideale è binaria: 1 o 0) e sull’asse orizzontale il livello di abilità dell’individuo. In questo modello ideale, un individuo risponde correttamente a tutti gli item con difficoltà inferiore o uguale al proprio livello di abilità, mentre sbaglia quelli con difficoltà superiore, creando così un modello di risposta perfettamente prevedibile.\nLe frecce nel grafico seguente rappresentano cinque individui con diversi livelli di abilità. Ogni freccia indica il punto in cui l’abilità di una persona interseca le curve caratteristiche degli item. Secondo il modello ideale, ogni persona dovrebbe rispondere correttamente a tutti gli item posizionati a sinistra della propria abilità sul grafico (item meno difficili) e sbagliare quelli a destra (item più difficili).\n\n\n\n\n\n\n\n\nTuttavia, questo modello deterministico raramente si osserva nei dati reali, dove intervengono numerosi fattori che introducono variabilità nelle risposte. Le persone con lo stesso livello di abilità possono rispondere diversamente agli stessi item, gli item possono presentare ambiguità nella loro formulazione, e fattori come motivazione, attenzione e contesto possono influenzare significativamente le risposte.\nÈ proprio da queste limitazioni del modello di Guttman che emerge la necessità di un approccio probabilistico, che viene sviluppato nel modello di Rasch. Quest’ultimo mantiene l’idea fondamentale della relazione gerarchica tra abilità e difficoltà, ma introduce una componente probabilistica che permette di gestire le deviazioni dal modello ideale, offrendo così una rappresentazione più realistica del processo di risposta agli item.\nIl modello di Rasch, che verrà approfondito nella prossima sezione, può essere visto come un’evoluzione naturale della Scala di Guttman, dove le transizioni nette tra risposta corretta e incorretta vengono sostituite da una curva logistica che descrive la probabilità di risposta corretta come una funzione continua dell’abilità. Questo passaggio da un modello deterministico a uno probabilistico rappresenta un avanzamento fondamentale nella teoria psicometrica, permettendo una modellizzazione più accurata e flessibile dei dati reali.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_rasch_model.html#il-modello-di-rasch",
    "href": "chapters/irt/01_rasch_model.html#il-modello-di-rasch",
    "title": "67  Modello di Rasch",
    "section": "\n67.4 Il Modello di Rasch",
    "text": "67.4 Il Modello di Rasch\nIl modello di Rasch rappresenta un’importante evoluzione nella psicometria, consentendo di superare i limiti dello scaling di Guttman. Questo modello si basa sul concetto di variabile latente, una caratteristica non direttamente osservabile ma inferita attraverso comportamenti misurabili, come le risposte a un test. Ad esempio, la competenza matematica può essere vista come una variabile latente stimata analizzando le risposte corrette e errate di un individuo in un test composto da più domande.\nSecondo il modello di Rasch, sia la competenza degli individui sia la difficoltà degli item sono rappresentate lungo un continuum latente. Gli individui con abilità più elevate si trovano verso l’estremità superiore del continuum, mentre quelli con abilità inferiori si collocano verso l’estremità inferiore. Analogamente, le domande del test (gli item) sono posizionate lungo il continuum in base alla loro difficoltà, indicata dal parametro \\(\\beta_i\\) per ciascun item \\(i\\). La posizione di una persona sul continuum è rappresentata dal parametro \\(\\theta_p\\), che riflette il livello di abilità latente dell’individuo \\(p\\).\nLa probabilità che un partecipante risponda correttamente a un determinato item dipende dalla differenza tra l’abilità del partecipante (\\(\\theta_p\\)) e la difficoltà dell’item (\\(\\beta_i\\)). Se \\(\\theta_p\\) è molto maggiore di \\(\\beta_i\\), la probabilità di una risposta corretta è alta; se \\(\\theta_p\\) è inferiore a \\(\\beta_i\\), la probabilità è bassa. Quando \\(\\theta_p\\) è circa uguale a \\(\\beta_i\\), la probabilità si avvicina al 50%, riflettendo incertezza nella risposta.\n\nEsercizio 67.1 Consideriamo un test con cinque domande (item) utilizzato per valutare la competenza matematica. Le risposte dei partecipanti (corrette o errate) costituiscono i dati osservabili. La difficoltà di ciascun item può essere stimata calcolando la proporzione di risposte corrette per ogni domanda.\nImportiamo i dati in R.\n\n# Importazione dei dati\nmath_dat &lt;- rio::import(here::here(\"data\", \"deAyala\", \"Math.txt\"))\nhead(math_dat)\n#&gt;   V1 V2 V3 V4 V5\n#&gt; 1  1  1  0  0  0\n#&gt; 2  1  1  1  0  0\n#&gt; 3  1  0  0  0  0\n#&gt; 4  1  1  1  0  0\n#&gt; 5  1  0  1  1  0\n#&gt; 6  1  1  1  0  0\n\n# Calcolo della difficoltà degli item\ncolMeans(math_dat)\n#&gt;    V1    V2    V3    V4    V5 \n#&gt; 0.887 0.644 0.566 0.427 0.387\n\nGli item sono ordinati per difficoltà crescente: il primo è il più facile, mentre l’ultimo è il più difficile.\nPer analizzare la relazione tra il punteggio totale di ciascun partecipante e la proporzione di risposte corrette per ogni item, possiamo generare un grafico.\n\n# Calcolo dei punteggi totali\nmath_dat2 &lt;- math_dat\nmath_dat2$total_score &lt;- rowSums(math_dat2[, -1])\n\n# Preparazione dati per il grafico\nplot_data &lt;- lapply(names(math_dat2)[1:5], function(item) {\n    math_dat2 %&gt;%\n        group_by(total_score) %&gt;%\n        summarise(proportion = mean(get(item) == 1)) %&gt;%\n        mutate(item = item)\n})\n\nplot_data &lt;- do.call(rbind, plot_data)\n\n# Creazione del grafico\nggplot(\n  plot_data, \n  aes(x = total_score, y = proportion, group = item, color = item)\n  ) +\n    geom_line(linewidth = 1.5) +\n    labs(\n        x = \"Punteggio Totale\",\n        y = \"Proporzione di Risposte Corrette\",\n        title = \"Proporzione di Risposte Corrette\\nin Base al Punteggio Totale\"\n    )\n\n\n\n\n\n\n\n\n\n67.4.1 Curve Caratteristiche degli Item (ICC)\nLe curve caratteristiche degli item (ICC) forniscono una rappresentazione grafica della probabilità di risposta corretta in funzione del livello di abilità latente. Nel modello di Rasch, questa relazione è descritta da una funzione logistica:\n\\[\nP(X_{pi} = 1 | \\theta_p, \\beta_i) = \\frac{1}{1 + e^{-(\\theta_p - \\beta_i)}}.\n\\tag{67.1}\\]\nQuesta equazione mostra che la probabilità di una risposta corretta è determinata esclusivamente dalla differenza tra \\(\\theta_p\\) (abilità del partecipante) e \\(\\beta_i\\) (difficoltà dell’item). La forma sigmoide della curva riflette tre situazioni:\n\nquando \\(\\theta_p\\) è molto maggiore di \\(\\beta_i\\), la probabilità è vicina a 1;\nquando \\(\\theta_p\\) è molto minore di \\(\\beta_i\\), la probabilità è vicina a 0;\nquando \\(\\theta_p\\) è circa uguale a \\(\\beta_i\\), la probabilità è prossima a 0.5.\n\nNel grafico delle ICC, gli item facili (\\(\\beta_i &lt; 0\\)) mostrano alte probabilità di risposta corretta anche per partecipanti con abilità modeste, mentre gli item difficili (\\(\\beta_i &gt; 0\\)) richiedono abilità elevate per ottenere una risposta corretta.\n\n# Creazione del modello di Rasch e grafico delle ICC\nrasch_model &lt;- rasch(math_dat)\nplot(rasch_model, type = \"ICC\")\n\n\n\n\n\n\n\nLe curve mostrano che, nel modello di Rasch, la difficoltà degli item è il parametro principale che varia, mentre la pendenza delle curve rimane costante, confermando l’assunto fondamentale del modello.\n\n67.4.2 Interpretazione Pratica\nNel contesto dei test psicometrici, il modello di Rasch dell’Equazione 67.1 offre un approccio rigoroso per interpretare i dati dei test psicometrici. La stima delle difficoltà degli item (\\(\\beta_i\\)) e delle abilità dei partecipanti (\\(\\theta_p\\)) permette di verificare la coerenza tra gli item e di identificare eventuali problematiche, come:\n\n\nItem troppo facili: risolti correttamente da quasi tutti i partecipanti, indipendentemente dal livello di abilità.\n\nItem troppo difficili: risolti solo dai partecipanti con abilità molto elevate, contribuendo poco alla misurazione complessiva.\n\nLa relazione tra abilità e difficoltà è descritta dall’Equazione 67.1. Questa equazione trasforma la differenza \\(\\theta_p - \\beta_i\\), che teoricamente può variare da \\(-\\infty\\) a \\(+\\infty\\), in una probabilità compresa tra 0 e 1. In termini pratici:\n\nQuando \\(\\theta_p\\) è molto maggiore di \\(\\beta_i\\), la probabilità di una risposta corretta è vicina a 1.\nQuando \\(\\theta_p\\) è molto minore di \\(\\beta_i\\), la probabilità si avvicina a 0.\nQuando \\(\\theta_p \\approx \\beta_i\\), la probabilità è circa 0.5, indicando incertezza.\n\nSebbene i parametri \\(\\theta_p\\) (abilità) e \\(\\beta_i\\) (difficoltà) possano teoricamente assumere qualsiasi valore, nella pratica si collocano generalmente tra -3 e +3. Questo intervallo rappresenta una scala standardizzata, utile per interpretare i livelli di abilità e difficoltà:\n\n\nItem facili (\\(\\beta_i &lt; 0\\)): sono generalmente risolti correttamente anche da persone con abilità modeste.\n\nItem difficili (\\(\\beta_i &gt; 0\\)): richiedono abilità elevate per essere superati.\n\nItem intermedi (\\(\\beta_i \\approx 0\\)): massima capacità discriminativa vicino alla media.\n\nQuesta rappresentazione permette di identificare con precisione le caratteristiche di ciascun item e di valutare se contribuisce efficacemente alla misurazione del tratto latente. Ad esempio, item che risultano troppo facili o troppo difficili forniscono meno informazioni utili rispetto a quelli con difficoltà intermedia.\nIn sintesi, il modello di Rasch è uno strumento fondamentale per costruire test psicometrici affidabili, in grado di misurare con precisione abilità, atteggiamenti e tratti di personalità.\n\n67.4.3 Rappresentazioni Alternative della Funzione Logistica\nLa funzione logistica utilizzata nel modello di Rasch può essere scritta in due modi: con la funzione esponenziale sia al numeratore sia al denominatore (a sinistra), oppure equivalentemente con la funzione esponenziale solo al denominatore, seguita dal suo argomento negativo (a destra):\n\\[\n\\frac{\\exp(\\theta_p - \\beta_i)}{1 + \\exp(\\theta_p - \\beta_i)} = \\frac{1}{1 + \\exp(-(\\theta_p - \\beta_i))}\n\\]\nPer dimostrare l’equivalenza delle due espressioni della funzione logistica nel modello di Rasch, seguiamo i seguenti passaggi algebrici. Per semplificare il lato destro, utilizziamo la proprietà dell’esponenziale che afferma \\(e^{-x} = \\frac{1}{e^x}\\). Quindi, riscriviamo \\(\\exp(-(\\theta_p - \\beta_i))\\) come \\(\\frac{1}{\\exp(\\theta_p - \\beta_i)}\\):\n\\[ \\frac{1}{1 + \\frac{1}{\\exp(\\theta_p - \\beta_i)}} \\]\nIl denominatore del lato destro diventa \\(1 + \\frac{1}{\\exp(\\theta_p - \\beta_i)}\\). Per combinare i termini nel denominatore, otteniamo un denominatore comune:\n\\[ \\frac{1}{\\frac{\\exp(\\theta_p - \\beta_i) + 1}{\\exp(\\theta_p - \\beta_i)}} \\]\nSimplificando ulteriormente, il denominatore diventa \\(\\exp(\\theta_p - \\beta_i) + 1\\), quindi l’intera espressione diventa:\n\\[ \\frac{1}{\\exp(\\theta_p - \\beta_i) + 1} \\]\nPossiamo ora invertire la frazione per ottenere il lato sinistro dell’equazione originale:\n\\[ \\frac{\\exp(\\theta_p - \\beta_i)}{\\exp(\\theta_p - \\beta_i) + 1} \\]\nQuindi, abbiamo dimostrato che il lato sinistro e il lato destro dell’equazione originale sono effettivamente equivalenti.\n\nEsercizio 67.2 Per illustrare come il modello di Rasch venga utilizzato per calcolare i punti su una curva caratteristica dell’item, consideriamo il seguente problema. I valori dei parametri dell’item sono:\n\na = 1 è il parametro di discriminazione dell’item,\nb = -0.5 è il parametro di difficoltà dell’item.\n\nTroviamo la probabilità di rispondere correttamente a questo item al livello di abilità theta = 1.5.\n\nicc &lt;- function(a, b, theta) {\n    1 / (1 + exp(-a * (theta - b)))\n}\n\na = 1\nb = -0.5\ntheta = 1.5\nicc(a, b, theta)\n#&gt; [1] 0.881\n\n\ntheta_range &lt;- seq(-3, 3, .1)\nplot(theta_range, icc(a, b, theta_range),\n    type = \"l\", xlim = c(-3, 3), ylim = c(0, 1),\n    xlab = \"Ability\", ylab = \"Probability of Correct Response\"\n)\npoints(theta, icc(a, b, theta), cex=2)\nsegments(-3, icc(a, b, theta), theta, icc(a, b, theta), lty = \"dashed\")\nsegments(theta, icc(a, b, theta), theta, 0, lty = \"dashed\")",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_rasch_model.html#la-proprietà-di-oggettività-specifica",
    "href": "chapters/irt/01_rasch_model.html#la-proprietà-di-oggettività-specifica",
    "title": "67  Modello di Rasch",
    "section": "\n67.5 La Proprietà di Oggettività Specifica",
    "text": "67.5 La Proprietà di Oggettività Specifica\nUna caratteristica distintiva del modello di Rasch è la oggettività specifica, che garantisce che la differenza tra i logit delle probabilità di rispondere correttamente a due item \\(i\\) e \\(j\\) sia costante per qualsiasi livello di abilità \\(\\theta\\). In altre parole, il confronto tra due item dipende esclusivamente dalla loro difficoltà e non dall’abilità del rispondente, realizzando così un principio fondamentale di misurazione oggettiva.\nNel modello di Rasch, la probabilità di rispondere correttamente a un item viene trasformata in logit, cioè il logaritmo delle quote tra la probabilità di una risposta corretta e quella di una risposta errata. Il logit è definito come:\n\\[\n\\log \\left( \\frac{\\text{Pr}(U_{pi} = 1 \\mid \\theta_p, \\beta_i)}{\\text{Pr}(U_{pi} = 0 \\mid \\theta_p, \\beta_i)} \\right) = \\theta_p - \\beta_i,\n\\]\ndove:\n\n\n\\(\\theta_p\\) è l’abilità latente del partecipante \\(p\\),\n\n\\(\\beta_i\\) è la difficoltà dell’item \\(i\\).\n\nLa probabilità \\(\\pi\\) di rispondere correttamente a un item è calcolata attraverso la funzione logistica:\n\\[\n\\pi = \\frac{\\exp(\\theta_p - \\beta_i)}{1 + \\exp(\\theta_p - \\beta_i)}.\n\\]\nIl complemento della probabilità, ossia la probabilità di rispondere in modo errato, è:\n\\[\n1 - \\pi = \\frac{1}{1 + \\exp(\\theta_p - \\beta_i)}.\n\\]\nLe quote \\(O\\) sono definite come il rapporto tra la probabilità di successo e quella di insuccesso:\n\\[\nO = \\exp(\\theta_p - \\beta_i).\n\\]\nIl logaritmo delle quote corrisponde direttamente al logit:\n\\[\n\\log(O) = \\theta_p - \\beta_i.\n\\]\nQuesta relazione evidenzia che i logit sono proporzionali alla differenza tra l’abilità del partecipante e la difficoltà dell’item.\nUn aumento della differenza \\(\\theta_p - \\beta_i\\) si traduce in una maggiore probabilità di successo (\\(\\pi\\)):\n\n\nValori alti di \\(\\theta_p - \\beta_i\\): indicano che l’abilità supera la difficoltà, con una probabilità di successo vicina a 1.\n\nValori bassi di \\(\\theta_p - \\beta_i\\): indicano che la difficoltà supera l’abilità, con una probabilità di successo vicina a 0.\n\nIn sintesi, la proprietà di oggettività specifica assicura che il modello di Rasch fornisca una misurazione consistente e comparabile degli item e delle abilità, indipendentemente dal contesto o dai partecipanti coinvolti.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_rasch_model.html#implicazioni-della-proprietà-di-oggettività-specifica",
    "href": "chapters/irt/01_rasch_model.html#implicazioni-della-proprietà-di-oggettività-specifica",
    "title": "67  Modello di Rasch",
    "section": "\n67.6 Implicazioni della Proprietà di Oggettività Specifica",
    "text": "67.6 Implicazioni della Proprietà di Oggettività Specifica\nL’oggettività specifica nel modello di Rasch significa che il confronto tra due item è indipendente dall’abilità dei rispondenti. Nella pratica, ciò si traduce nel fatto che le curve caratteristiche degli item (ICC) per diversi item sono parallele lungo la scala dei logit. Questa parallelismo deriva dal fatto che la differenza tra i logit di due item, \\(\\beta_i\\) e \\(\\beta_j\\), è costante per tutti i valori di abilità \\(\\theta_p\\).\nLe curve per item con diverse difficoltà\n\nhanno pendenze identiche, riflettendo che il tasso di variazione della probabilità rispetto a \\(\\theta_p\\) è uguale per tutti gli item;\nnon si intersecano mai lungo l’asse delle abilità, poiché ogni differenza tra le probabilità di risposta corretta è attribuibile esclusivamente alla differenza di difficoltà tra gli item (\\(\\beta_j - \\beta_i\\));\nsi spostano verticalmente lungo l’asse delle probabilità in base alla difficoltà dell’item, mantenendo una rappresentazione coerente del rapporto tra abilità e probabilità di risposta corretta.\n\nLa rappresentazione logit\n\nconsente il calcolo della probabilità di risposta corretta indipendentemente dal set di item somministrati, garantendo comparabilità tra diversi test.\nrende le differenze tra item facilmente interpretabili, attribuendo ogni variazione a una caratteristica misurabile (la difficoltà) e non a fattori confondenti come il livello di abilità dei partecipanti.\n\nQuesta struttura garantisce misure eque e precise, promuovendo un’interpretazione robusta dei dati raccolti.\n\n# Creazione di un dataframe con i valori di abilità (theta_p) e le difficoltà degli item (beta)\ntheta_p &lt;- seq(-3, 3, length.out = 100)\nbeta_i &lt;- -1\nbeta_j &lt;- 1\n\n# Calcolo dei logit per gli item i e j\nlogit_i &lt;- theta_p - beta_i\nlogit_j &lt;- theta_p - beta_j\n\ndata &lt;- data.frame(\n    Ability = c(theta_p, theta_p),\n    Logit = c(logit_i, logit_j),\n    Item = factor(c(rep(\"Item i (beta_i = -1)\", length(theta_p)), rep(\"Item j (beta_j = 1)\", length(theta_p))))\n)\n\nggplot(data, aes(x = Ability, y = Logit, color = Item)) +\n    geom_line() +\n    scale_color_manual(values = c(\"blue\", \"red\")) +\n    ggtitle(TeX(\"Parallel Lines for Item i and j in the Rasch Model\")) +\n    xlab(TeX(\"Ability ($\\\\theta_p$)\")) +\n    ylab(TeX(\"Logit Probability\"))",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_rasch_model.html#il-modello-di-rasch-e-lanalisi-fattoriale",
    "href": "chapters/irt/01_rasch_model.html#il-modello-di-rasch-e-lanalisi-fattoriale",
    "title": "67  Modello di Rasch",
    "section": "\n67.7 Il Modello di Rasch e l’Analisi Fattoriale",
    "text": "67.7 Il Modello di Rasch e l’Analisi Fattoriale\nPer comprendere meglio il Modello di Rasch, può essere utile confrontarlo con l’Analisi Fattoriale. Sebbene si basino su approcci metodologici differenti, entrambi condividono l’obiettivo di individuare le dimensioni latenti che influenzano le risposte osservate nei dati. Questo confronto evidenzia somiglianze e differenze, fornendo una prospettiva più chiara sui due metodi.\nNell’Analisi Fattoriale, il modello tipico è espresso come \\(Y_i = \\lambda_i \\xi + \\delta_i\\), dove \\(Y_i\\) è il punteggio osservato per l’item i-esimo, \\(\\lambda_i\\) rappresenta la saturazione fattoriale che indica quanto l’item è influenzato dal fattore latente \\(\\xi\\), e \\(\\delta_i\\) è il termine di errore specifico per quell’item. L’idea centrale è che, controllando per \\(\\xi\\), le correlazioni tra gli item \\(Y_i\\) diventano nulle, poiché qualsiasi associazione comune è spiegata dal fattore latente.\nIl Modello di Rasch, pur perseguendo lo stesso obiettivo generale, adotta un approccio diverso. Esso si concentra sull’analisi di risposte dicotomiche (0 o 1) e presuppone che la probabilità di una risposta corretta sia una funzione logistica dell’abilità del rispondente \\(\\theta\\) e della difficoltà dell’item \\(\\delta_i\\).\nLa differenza cruciale tra il Modello di Rasch e l’Analisi Fattoriale risiede nella trattazione dei parametri degli item:\n\n\nPotere discriminante:\n\nNell’Analisi Fattoriale, le saturazioni fattoriali (\\(\\lambda_i\\)) variano tra gli item, riflettendo differenze nella capacità degli item di rappresentare la dimensione latente.\nNel Modello di Rasch, tutti gli item hanno lo stesso potere discriminante. Si presume che siano ugualmente efficaci nel distinguere tra rispondenti con abilità diverse.\n\n\n\nFocus sui parametri:\n\nL’Analisi Fattoriale si concentra sull’identificazione delle saturazioni fattoriali (\\(\\lambda_i\\)) per ciascun item e sull’individuazione dei fattori latenti comuni.\nIl Modello di Rasch stima l’abilità dei rispondenti (\\(\\theta\\)) e la difficoltà degli item (\\(\\delta_i\\)), presupponendo un’equivalenza nella discriminazione tra gli item.\n\n\n\nIn conclusione, sia il Modello di Rasch che l’Analisi Fattoriale mirano a spiegare le risposte osservate attraverso una dimensione latente. Tuttavia:\n\nL’Analisi Fattoriale enfatizza le relazioni tra gli item, stimando saturazioni fattoriali per descrivere come ogni item contribuisce alla dimensione latente comune.\nIl Modello di Rasch si focalizza sull’interazione tra l’abilità del rispondente e la difficoltà dell’item, fornendo una rappresentazione dettagliata delle dinamiche che influenzano le risposte dicotomiche.\n\nQuesti due approcci, pur perseguendo finalità analoghe, si applicano a contesti e dati differenti, completandosi a vicenda nel panorama della psicometria.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_rasch_model.html#riflessioni-conclusive",
    "href": "chapters/irt/01_rasch_model.html#riflessioni-conclusive",
    "title": "67  Modello di Rasch",
    "section": "\n67.8 Riflessioni Conclusive",
    "text": "67.8 Riflessioni Conclusive\nIl modello di Rasch si differenzia notevolmente dalla Teoria Classica dei Test (CTT) grazie a una serie di caratteristiche fondamentali che ne fanno uno strumento di analisi psicometrica più sofisticato e dettagliato.\n\n67.8.1 Principali Differenze tra Rasch e CTT\n\n\nLivello di Analisi\n\nLa CTT si concentra sull’analisi aggregata dell’intero test, descrivendo il punteggio totale come \\(X = T + E\\), dove \\(T\\) rappresenta il punteggio vero e \\(E\\) l’errore di misurazione. Questo approccio si limita a una visione complessiva delle misure, senza approfondire le dinamiche specifiche degli item.\n\nAl contrario, il modello di Rasch analizza la probabilità di una risposta corretta per ciascun item, offrendo un’analisi dettagliata a livello di singolo item e consentendo una comprensione approfondita del funzionamento di ogni elemento del test.\n\n\n\nApproccio Focalizzato sugli Item\n\nMentre la CTT utilizza un approccio basato sulla somma delle risposte, il modello di Rasch, seguendo il paradigma proposto da Guttman, analizza direttamente le risposte osservate, modellandone le probabilità. Questo cambio di prospettiva migliora la precisione e riduce gli errori di misurazione.\n\n\n\n67.8.2 Vantaggi del Modello di Rasch\n\n\nPrecisione e Dettaglio\n\nL’analisi item per item permette di identificare specifiche aree di forza o debolezza sia nei rispondenti sia nei test stessi. Ciò consente di migliorare gli strumenti di misura e garantisce stime più accurate delle abilità individuali.\n\n\n\nSeparazione tra Attributi della Persona e Caratteristiche dell’Item\n\nIl modello di Rasch distingue nettamente tra l’abilità del rispondente (\\(\\theta_p\\)) e la difficoltà dell’item (\\(\\beta_i\\)). Questa separazione assicura che la difficoltà di un item sia una proprietà intrinseca e stabile, indipendente dal campione di rispondenti, migliorando la coerenza e la generalizzabilità delle misure.\n\n\n\nFlessibilità nei Modelli di Risposta\n\nIl modello di Rasch può essere applicato a una vasta gamma di formati di domanda, come domande dicotomiche, scale Likert e risposte aperte. Questa versatilità lo rende adatto a misurare una varietà di costrutti psicologici.\n\n\n\nValutazione Adattiva\n\nIntegrato nell’ambito dell’Item Response Theory (IRT), il modello di Rasch supporta valutazioni adattive, in cui gli item somministrati variano in base al livello di abilità del rispondente. Ciò riduce gli errori di misurazione e fornisce stime più precise e mirate.\n\n\n\nAnalisi Approfondita degli Item\n\nIl modello consente di valutare in dettaglio le caratteristiche degli item, come difficoltà, discriminazione e parametri di indovinamento, fornendo informazioni utili per migliorare la qualità dei test.\n\n\n\n67.8.3 Limiti e Critiche\nNonostante i numerosi vantaggi, il modello di Rasch è talvolta criticato per le sue assunzioni restrittive, che includono:\n\nLa presunzione di un potere discriminante uguale per tutti gli item.\nLa dipendenza da una funzione logistica semplice per modellare le risposte.\n\nQueste assunzioni possono ridurre la capacità del modello di catturare la complessità delle risposte in scenari reali. Tuttavia, le sue basi teoriche solide garantiscono un’analisi robusta, in grado di mantenere l’invarianza delle proprietà degli item e delle abilità dei rispondenti attraverso diversi contesti e campioni.\nIn conclusione, il modello di Rasch rappresenta un notevole passo avanti rispetto alla CTT, offrendo una misurazione più precisa e flessibile, capace di adattarsi a molteplici contesti psicometrici. Nonostante le critiche, rimane uno strumento essenziale per la costruzione, la valutazione e il miglioramento dei test psicologici, consentendo un’analisi dettagliata e affidabile delle risposte osservate.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_rasch_model.html#session-info",
    "href": "chapters/irt/01_rasch_model.html#session-info",
    "title": "67  Modello di Rasch",
    "section": "\n67.9 Session Info",
    "text": "67.9 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] grid      stats4    stats     graphics  grDevices utils     datasets \n#&gt; [8] methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] latex2exp_0.9.6   ggmirt_0.1.0      TAM_4.2-21        CDM_8.2-6        \n#&gt;  [5] mvtnorm_1.3-3     ltm_1.2-0         polycor_0.8-1     msm_1.8.2        \n#&gt;  [9] mirt_1.44.0       lattice_0.22-6    ggokabeito_0.1.0  see_0.10.0       \n#&gt; [13] MASS_7.3-65       viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n#&gt; [17] ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1 \n#&gt; [21] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-19     psych_2.4.12     \n#&gt; [25] scales_1.3.0      markdown_1.13     knitr_1.49        lubridate_1.9.4  \n#&gt; [29] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.4      \n#&gt; [33] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1    \n#&gt; [37] tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] splines_4.4.2        later_1.4.1          R.oo_1.27.0         \n#&gt;   [4] XML_3.99-0.18        rpart_4.1.24         lifecycle_1.0.4     \n#&gt;   [7] Rdpack_2.6.2         rstatix_0.7.2        rprojroot_2.0.4     \n#&gt;  [10] globals_0.16.3       rockchalk_1.8.157    backports_1.5.0     \n#&gt;  [13] magrittr_2.0.3       openxlsx_4.2.8       Hmisc_5.2-2         \n#&gt;  [16] rmarkdown_2.29       yaml_2.3.10          httpuv_1.6.15       \n#&gt;  [19] qgraph_1.9.8         zip_2.3.2            sessioninfo_1.2.3   \n#&gt;  [22] pbapply_1.7-2        minqa_1.2.8          multcomp_1.4-28     \n#&gt;  [25] abind_1.4-8          audio_0.1-11         expm_1.0-0          \n#&gt;  [28] quadprog_1.5-8       R.utils_2.13.0       nnet_7.3-20         \n#&gt;  [31] TH.data_1.1-3        sandwich_3.1-1       listenv_0.9.1       \n#&gt;  [34] testthat_3.2.3       RPushbullet_0.3.4    vegan_2.6-10        \n#&gt;  [37] arm_1.14-4           parallelly_1.42.0    permute_0.9-7       \n#&gt;  [40] codetools_0.2-20     tidyselect_1.2.1     farver_2.1.2        \n#&gt;  [43] lme4_1.1-36          base64enc_0.1-3      jsonlite_1.9.0      \n#&gt;  [46] progressr_0.15.1     Formula_1.2-5        survival_3.8-3      \n#&gt;  [49] emmeans_1.10.7       tools_4.4.2          rio_1.2.3           \n#&gt;  [52] snow_0.4-4           Rcpp_1.0.14          glue_1.8.0          \n#&gt;  [55] mnormt_2.1.1         admisc_0.37          xfun_0.51           \n#&gt;  [58] mgcv_1.9-1           withr_3.0.2          beepr_2.0           \n#&gt;  [61] fastmap_1.2.0        boot_1.3-31          digest_0.6.37       \n#&gt;  [64] mi_1.1               timechange_0.3.0     R6_2.6.1            \n#&gt;  [67] mime_0.12            estimability_1.5.1   colorspace_2.1-1    \n#&gt;  [70] gtools_3.9.5         jpeg_0.1-10          R.methodsS3_1.8.2   \n#&gt;  [73] generics_0.1.3       data.table_1.17.0    corpcor_1.6.10      \n#&gt;  [76] SimDesign_2.18       htmlwidgets_1.6.4    pkgconfig_2.0.3     \n#&gt;  [79] sem_3.1-16           gtable_0.3.6         brio_1.1.5          \n#&gt;  [82] htmltools_0.5.8.1    carData_3.0-5        png_0.1-8           \n#&gt;  [85] reformulas_0.4.0     rstudioapi_0.17.1    tzdb_0.4.0          \n#&gt;  [88] reshape2_1.4.4       coda_0.19-4.1        checkmate_2.3.2     \n#&gt;  [91] nlme_3.1-167         curl_6.2.1           nloptr_2.1.1        \n#&gt;  [94] zoo_1.8-13           parallel_4.4.2       miniUI_0.1.1.1      \n#&gt;  [97] foreign_0.8-88       pillar_1.10.1        vctrs_0.6.5         \n#&gt; [100] promises_1.3.2       car_3.1-3            OpenMx_2.21.13      \n#&gt; [103] xtable_1.8-4         Deriv_4.1.6          cluster_2.1.8       \n#&gt; [106] dcurver_0.9.2        GPArotation_2024.3-1 htmlTable_2.4.3     \n#&gt; [109] evaluate_1.0.3       pbivnorm_0.6.0       cli_3.6.4           \n#&gt; [112] kutils_1.73          compiler_4.4.2       rlang_1.1.5         \n#&gt; [115] future.apply_1.11.3  ggsignif_0.6.4       labeling_0.4.3      \n#&gt; [118] fdrtool_1.2.18       plyr_1.8.9           stringi_1.8.4       \n#&gt; [121] munsell_0.5.1        lisrelToR_0.3        pacman_0.5.1        \n#&gt; [124] Matrix_1.7-2         hms_1.1.3            glasso_1.11         \n#&gt; [127] future_1.34.0        shiny_1.10.0         rbibutils_2.3       \n#&gt; [130] igraph_2.1.4         broom_1.0.7          RcppParallel_5.1.10\n\n\n\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/02_assumptions.html",
    "href": "chapters/irt/02_assumptions.html",
    "title": "68  Assunzioni e Proprietà del Modello di Rasch",
    "section": "",
    "text": "68.1 Introduzione\nQuesto capitolo esamina le proprietà distintive del modello di Rasch, che lo rendono uno strumento fondamentale nella misurazione psicometrica. Tra queste proprietà, l’oggettività specifica consente di confrontare abilità individuali e difficoltà degli item in modo indipendente dal campione e dalla selezione degli item, garantendo misurazioni stabili e affidabili. Inoltre, il modello sfrutta il concetto di statistiche sufficienti, che permettono di stimare abilità e difficoltà utilizzando informazioni aggregate, riducendo la necessità di analizzare ogni singola risposta. Un altro aspetto cruciale è la rappresentazione dei punteggi su una scala di intervallo, che consente confronti significativi tra le differenze di abilità e difficoltà, pur richiedendo convenzioni per definire il punto zero e l’unità di misura.\nIn questo capitolo analizzeremo come le tre assunzioni fondamentali del modello di Rasch — unidimensionalità, monotonicità e indipendenza locale — diano origine alle sue proprietà distintive. Approfondiremo l’applicazione del modello nella progettazione di test psicometrici equi e accurati, evidenziandone i punti di forza e discutendo le situazioni in cui le sue limitazioni rendono necessario il ricorso a estensioni multidimensionali o modelli alternativi.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>Assunzioni e Proprietà del Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/02_assumptions.html#statistiche-sufficienti",
    "href": "chapters/irt/02_assumptions.html#statistiche-sufficienti",
    "title": "68  Assunzioni e Proprietà del Modello di Rasch",
    "section": "68.2 Statistiche Sufficienti",
    "text": "68.2 Statistiche Sufficienti\nIniziamo a chiarire il concetto di “statistica sufficiente”. Una statistica è una funzione dei dati osservati, utilizzata per riassumere caratteristiche rilevanti di un insieme di dati. Ad esempio, la media campionaria è una statistica comunemente calcolata come:\n\\[ \\bar{x} = \\frac{1}{P} \\sum_{p=1}^{P} x_p, \\]\ndove \\(\\bar{x}\\) rappresenta la media dei valori \\(x_p\\) osservati per \\(P\\) individui. Questa statistica è spesso impiegata per stimare il valore atteso di una popolazione, in quanto fornisce una sintesi delle informazioni relative alla media del campione.\nOltre alla media campionaria, è possibile definire altre statistiche. Ad esempio, si potrebbe calcolare la media di un sottoinsieme di valori del campione, come:\n\\[ x^* = \\frac{1}{3} (x_1 + x_3 + x_5). \\]\nSebbene \\(x^*\\) sia un valido stimatore, risulta generalmente meno efficace di \\(\\bar{x}\\) perché utilizza solo una parte del campione (escludendo, ad esempio, \\(x_2, x_4\\), ecc.), riducendo la quantità di informazioni sfruttate.\nIl concetto di statistica sufficiente si applica quando una statistica, come \\(\\bar{x}\\), contiene tutte le informazioni necessarie per stimare il parametro di interesse (ad esempio, la media della popolazione) che sono presenti nei dati campionari. In altre parole, una statistica sufficiente cattura completamente l’informazione sui parametri senza richiedere ulteriori dettagli dai dati individuali.\nLe statistiche sufficienti sono particolarmente utili nelle analisi inferenziali perché, una volta calcolate, rendono superflua la conoscenza dei dati grezzi ai fini della stima del parametro.\n\n68.2.1 Applicazioni nel Modello di Rasch\nIl modello di Rasch permette di identificare statistiche sufficienti per i principali parametri:\n\nStatistica sufficiente per \\(\\theta_p\\) (abilità della persona):\nPer il parametro di abilità \\(\\theta_p\\) di una persona \\(p\\), la statistica sufficiente è il punteggio totale \\(r_p\\), calcolato sommando tutte le risposte corrette fornite dalla persona ai diversi item. Questo punteggio sintetizza l’informazione fondamentale sull’abilità di \\(p\\), senza richiedere un’analisi dettagliata delle singole risposte.\nStatistica sufficiente per \\(\\beta_i\\) (difficoltà dell’item):\nPer il parametro di difficoltà di un item \\(\\beta_i\\), la statistica sufficiente è il numero totale di risposte corrette \\(c_i\\) fornite da tutte le persone per quell’item. Questo valore concentra l’informazione necessaria per descrivere la difficoltà dell’item.\n\nLa probabilità che una persona \\(p\\) risponda correttamente all’item \\(i\\) è definita da una funzione logistica che dipende dalla differenza tra abilità e difficoltà:\n\\[\nP(Y_{pi} = 1 \\mid \\theta_p, \\beta_i) = \\frac{e^{\\theta_p - \\beta_i}}{1 + e^{\\theta_p - \\beta_i}}.\n\\]\nIn questo schema:\n\nIl punteggio totale \\(r_p\\) e il numero totale di risposte corrette \\(c_i\\) sono statistiche sufficienti, poiché contengono tutte le informazioni utili per stimare i parametri \\(\\theta_p\\) e \\(\\beta_i\\).\n\nLa indipendenza condizionale delle risposte, dato \\(\\theta_p\\) o \\(\\beta_i\\), permette di sintetizzare i dati attraverso i punteggi totali senza perdere informazioni rilevanti per l’inferenza.\n\nQuesta caratteristica del modello di Rasch facilita l’analisi statistica, poiché consente di stimare i parametri senza dover considerare l’intera matrice delle risposte individuali. Inoltre, l’uso di statistiche sufficienti migliora l’efficienza computazionale e rende i risultati più facilmente interpretabili.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>Assunzioni e Proprietà del Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/02_assumptions.html#assunzioni-del-modello-di-rasch",
    "href": "chapters/irt/02_assumptions.html#assunzioni-del-modello-di-rasch",
    "title": "68  Assunzioni e Proprietà del Modello di Rasch",
    "section": "68.3 Assunzioni del Modello di Rasch",
    "text": "68.3 Assunzioni del Modello di Rasch\nIl modello di Rasch si basa su tre fondamentali assunzioni che ne garantiscono la validità e l’applicabilità: unidimensionalità, monotonicità e indipendenza locale.\n\n68.3.1 Unidimensionalità nel Modello di Rasch\nL’assunzione di unidimensionalità è centrale nel modello di Rasch e implica che le risposte agli item di un test siano determinate prevalentemente da un unico tratto latente o dimensione. Questo tratto, definito come la dimensione target del test, ordina le persone secondo le loro abilità, escludendo l’influenza significativa di altre caratteristiche. In altre parole, un test unidimensionale misura esclusivamente una specifica abilità o attributo.\nAd esempio, un test di matematica ideale dovrebbe valutare unicamente la competenza matematica. Al contrario, test complessi come il SAT, che valutano sia abilità matematiche che verbali, non possono essere considerati unidimensionali perché ogni sezione misura una dimensione distinta.\n\n68.3.1.1 Unidimensionalità e Funzionamento Ideale\nUn test unidimensionale ben progettato assegna a ciascun partecipante un unico valore di abilità, riflettendo esclusivamente la competenza nella dimensione target. Tuttavia, se più dimensioni latenti influenzano le risposte, si possono verificare problematiche come il Funzionamento Differenziale degli Item (DIF). Il DIF emerge quando la difficoltà di specifici item varia tra gruppi di candidati non per differenze nella dimensione target, ma per l’influenza di dimensioni secondarie.\nAd esempio, in un test di matematica, la competenza linguistica potrebbe rappresentare una dimensione secondaria che influenza le risposte. Se un gruppo di candidati ha abilità linguistiche significativamente diverse rispetto a un altro, alcuni item potrebbero risultare più facili o difficili in modo sistematico, distorcendo così i risultati del test. Questo evidenzia un problema di parzialità, poiché il test non misura equamente la competenza matematica per tutti i partecipanti.\n\n\n68.3.1.2 Identificazione del DIF e Multidimensionalità\nPer rilevare il DIF e verificare l’unidimensionalità, si utilizzano test statistici specifici che esplorano la possibile influenza di dimensioni non previste dal modello. La presenza di DIF o di influenze multidimensionali può compromettere la validità del test, portando a valutazioni ingiuste o inaffidabili.\nMolti costrutti psicologici, tuttavia, sono intrinsecamente multidimensionali. Ad esempio, il modello dell’intelligenza proposto da Carroll (1993) descrive una struttura gerarchica che comprende dimensioni come l’intelligenza fluida e cristallizzata, dimostrando la complessità di tali costrutti.\n\n\n68.3.1.3 Estensioni del Modello di Rasch per Multidimensionalità\nIl modello di Rasch classico si basa sull’unidimensionalità, limitandosi alla misurazione di un’unica abilità. Tuttavia, per affrontare la complessità dei costrutti psicologici, sono state sviluppate estensioni multidimensionali del modello di Rasch. Queste versioni permettono di valutare simultaneamente più dimensioni, offrendo una rappresentazione più accurata e completa delle abilità o caratteristiche misurate.\nLe estensioni multidimensionali consentono di:\n\nIsolare dimensioni distinte: Identificare e misurare separatamente tratti diversi influenti sulle risposte.\nGestire costrutti complessi: Analizzare costrutti psicologici multidimensionali come l’intelligenza o la personalità.\nMigliorare l’equità del test: Ridurre la parzialità e il DIF, garantendo una valutazione più giusta per tutti i partecipanti.\n\nIn conclusione, l’unidimensionalità è un pilastro fondamentale per il modello di Rasch classico, cruciale per garantire la validità e l’affidabilità dei test psicometrici. Tuttavia, la realtà dei costrutti psicologici richiede spesso un approccio più flessibile che consideri la loro natura multidimensionale. Le estensioni multidimensionali del modello di Rasch rappresentano una risposta essenziale a questa sfida, migliorando la precisione delle misurazioni e l’equità delle valutazioni in contesti complessi.\n\n\n\n68.3.2 Monotonicità\nL’assunzione di monotonicità stabilisce che con l’incremento del tratto latente (\\(\\theta\\)), aumenta anche la probabilità di una risposta corretta. Ciò si allinea con l’intuizione generale nella misurazione: individui con un livello più elevato del tratto latente tendono a ottenere punteggi migliori nei test.\n\n\n68.3.3 Indipendenza Locale nel Modello di Rasch\nL’indipendenza locale è un’assunzione fondamentale nel modello di Rasch, secondo cui, una volta controllato il tratto latente (ad esempio, l’abilità di una persona), le risposte a due item distinti devono essere indipendenti. In altre parole, eventuali correlazioni tra risposte a diversi item sono interamente attribuibili al tratto latente, senza che una risposta influenzi o sia influenzata da un’altra.\n\n68.3.3.1 Il Concetto di Indipendenza Stocastica\nIn statistica, l’indipendenza stocastica implica che la probabilità di un evento non dipenda dall’esito di un altro. Questo principio semplifica il calcolo delle probabilità congiunte. Ad esempio, nel caso di due lanci di una moneta equilibrata, la probabilità di ottenere “testa” in entrambi i lanci si calcola come il prodotto delle probabilità individuali:\n\\[\n\\text{Pr}(\\text{testa, testa}) = \\text{Pr}(\\text{testa}) \\times \\text{Pr}(\\text{testa}) = 0.5 \\times 0.5 = 0.25.\n\\]\nNel modello di Rasch, questo principio si traduce nell’indipendenza condizionale delle risposte agli item, dato il parametro di abilità \\(\\theta_p\\).\n\n\n68.3.3.2 Applicazione dell’Indipendenza Locale nel Modello di Rasch\nL’indipendenza locale consente di calcolare la probabilità congiunta delle risposte a un test come il prodotto delle probabilità individuali. Per due item \\(i\\) e \\(j\\), la probabilità congiunta delle risposte è data da:\n\\[\n\\text{Pr}(U_{pi} = u_{pi}, U_{pj} = u_{pj} \\mid \\theta_p, \\beta_i, \\beta_j) = \\text{Pr}(U_{pi} = u_{pi} \\mid \\theta_p, \\beta_i) \\times \\text{Pr}(U_{pj} = u_{pj} \\mid \\theta_p, \\beta_j).\n\\]\nGeneralizzando a un test con \\(I\\) item, possiamo rappresentare la probabilità congiunta come:\n\\[\n\\text{Pr}(U_{p\\cdot} = u_{p\\cdot} \\mid \\theta_p, \\beta) = \\prod_{i=1}^{I} \\text{Pr}(U_{pi} = u_{pi} \\mid \\theta_p, \\beta_i),\n\\]\ndove \\(\\beta = (\\beta_1, \\dots, \\beta_I)\\) rappresenta il vettore dei parametri di difficoltà degli item e \\(U_{p\\cdot}\\) è il vettore delle risposte della persona \\(p\\). Questa formulazione semplifica notevolmente i calcoli e l’analisi statistica.\n\n\n68.3.3.3 Limitazioni dell’Indipendenza Locale\nNonostante la sua utilità, l’assunzione di indipendenza locale può essere violata in alcune situazioni, come:\n\nTest di matematica: La soluzione di un problema può dipendere dalla comprensione di item precedenti.\nTestlet: Gruppi di item che condividono un tema comune possono introdurre correlazioni tra le risposte.\n\nIn questi casi, le risposte non sono condizionatamente indipendenti, e l’applicazione del modello di Rasch può risultare inappropriata. Tali situazioni richiedono modelli alternativi, come quelli che incorporano dipendenze strutturali tra item.\nIn conclusione, l’indipendenza locale è una proprietà chiave che consente al modello di Rasch di calcolare in modo efficiente le probabilità congiunte delle risposte e di eseguire inferenze robuste. Tuttavia, è fondamentale valutare attentamente il contesto del test per verificare se questa assunzione sia valida. Nei casi in cui l’indipendenza locale non è rispettata, l’adozione di modelli più complessi può essere necessaria per garantire una valutazione accurata e priva di bias.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>Assunzioni e Proprietà del Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/02_assumptions.html#scala-di-misurazione-nel-modello-di-rasch",
    "href": "chapters/irt/02_assumptions.html#scala-di-misurazione-nel-modello-di-rasch",
    "title": "68  Assunzioni e Proprietà del Modello di Rasch",
    "section": "68.4 Scala di Misurazione nel Modello di Rasch",
    "text": "68.4 Scala di Misurazione nel Modello di Rasch\nEsaminiamo ora livello di misurazione dei punteggi ottenuti mediante il modello di Rash. Un aspetto cruciale è che i parametri del modello, come l’abilità delle persone (\\(\\theta_p\\)) e la difficoltà degli item (\\(\\beta_i\\)), sono rappresentati su una scala di intervallo. Questo livello di misurazione consente di confrontare differenze tra abilità e difficoltà, ma non definisce un punto zero assoluto né un’unità di misura intrinseca.\n\n68.4.1 Misurazione su una Scala di Intervallo\nNel modello di Rasch, i punteggi ottenuti non sono meri numeri ordinali (che stabiliscono solo un ordine), ma rappresentano intervalli misurabili. Questo significa che:\n\nLe differenze tra abilità o difficoltà hanno un significato costante e interpretabile.\nTuttavia, la scala non ha un punto zero assoluto; il valore “zero” è arbitrario e dipende dalla convenzione adottata.\n\nUn’analogia utile è il confronto con le scale di temperatura in gradi Celsius o Fahrenheit: mentre le differenze (ad esempio, 10°C contro 20°C) hanno un significato consistente, lo zero non rappresenta un’assenza di temperatura, ma è definito convenzionalmente.\n\n\n68.4.2 Trasformazioni e Ricalibrazione della Scala\nLe misure nel modello di Rasch possono essere trasformate senza alterare le probabilità di risposta corrette:\n\nTraslazione: Le abilità e le difficoltà possono essere traslate sottraendo un valore costante (\\(b\\)): \\[\n\\theta_p' = \\theta_p - b, \\quad \\beta_i' = \\beta_i - b.\n\\] Questa operazione mantiene invariata la funzione logistica che definisce le probabilità di risposta corretta.\nRiscalatura: La scala può essere modificata mediante una moltiplicazione o divisione per un fattore costante (\\(a\\)): \\[\n\\theta_p'' = \\frac{\\theta_p}{a}, \\quad \\beta_i'' = \\frac{\\beta_i}{a}.\n\\] Anche in questo caso, le probabilità restano invariate se la funzione logistica è adattata al nuovo fattore di scala.\n\nQueste trasformazioni mostrano che la scala è relativa: ciò che conta non è il valore assoluto delle misure, ma le differenze e il rapporto tra i parametri.\n\n\n68.4.3 Implicazioni per la Misurazione\nPoiché i parametri del modello di Rasch sono su una scala di intervallo, è necessario stabilire convenzioni per definire un punto zero e un’unità di misura. Comunemente, si adottano le seguenti strategie:\n\nFissare un riferimento: Ad esempio, assegnare la difficoltà di un item a zero.\nNormalizzazione: Imporre che la somma delle difficoltà degli item o delle abilità dei partecipanti sia pari a zero.\nStandardizzazione della pendenza: Impostare la scala della funzione logistica a 1, garantendo una coerenza nelle unità di misura.\n\nQueste scelte non influenzano la validità delle misure, ma permettono di ancorare i parametri a una scala interpretabile.\nIn conclusione, il modello di Rasch fornisce misurazioni robuste su una scala di intervallo, permettendo analisi precise delle differenze tra abilità e difficoltà. Tuttavia, la mancanza di un punto zero intrinseco e di un’unità di misura assoluta richiede la definizione di convenzioni per la calibratura della scala. Questo aspetto, sebbene tecnico, è cruciale per garantire la coerenza e l’interpretabilità dei risultati ottenuti nei contesti psicometrici.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>Assunzioni e Proprietà del Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/02_assumptions.html#oggettività-specifica",
    "href": "chapters/irt/02_assumptions.html#oggettività-specifica",
    "title": "68  Assunzioni e Proprietà del Modello di Rasch",
    "section": "68.5 Oggettività Specifica",
    "text": "68.5 Oggettività Specifica\nL’oggettività specifica è uno dei principi cardine del modello di Rasch e garantisce che i confronti tra individui siano equi e indipendenti dagli item specifici utilizzati nel test. Questo concetto assicura che le differenze tra le abilità delle persone si riflettano in modo coerente, senza essere influenzate da caratteristiche particolari degli item.\nL’oggettività specifica implica che:\n\nConfronti tra individui: Se una persona ha una probabilità maggiore di rispondere correttamente rispetto a un’altra, questa superiorità si manifesta uniformemente su tutti gli item.\nConfronti tra item: Se un item è più facile per una persona, sarà più facile per chiunque altro, indipendentemente dalle abilità specifiche.\n\nIn altre parole, il modello di Rasch garantisce che le probabilità di risposta corretta dipendano solo dalla differenza tra l’abilità della persona (\\(\\theta_p\\)) e la difficoltà dell’item (\\(\\beta_i\\)), preservando la coerenza nei confronti.\n\n68.5.1 Verifica dell’Oggettività Specifica\nUn indicatore chiave dell’oggettività specifica è rappresentato dalle Curve di Caratteristica dell’Item (ICC). Nel modello di Rasch, le ICC per diversi item non si incrociano: ciò indica che la relazione tra abilità e probabilità di risposta corretta rimane consistente per tutti gli item, rispettando l’oggettività specifica.\nL’oggettività specifica può essere descritta algebricamente attraverso i rapporti di probabilità. Per due persone \\(p\\) e \\(q\\) e un item \\(i\\), sia \\(P_{pi}\\) la probabilità che la persona \\(p\\) risponda correttamente all’item \\(i\\). L’oggettività specifica richiede che il rapporto di probabilità tra le due persone sia costante per tutti gli item:\n\\[\n\\frac{P_{pi}}{P_{qi}} = \\frac{P_{pj}}{P_{qj}}, \\quad \\forall i, j.\n\\]\nQuesto implica che:\n\\[\nP_{pi} \\cdot P_{qj} = P_{pj} \\cdot P_{qi}.\n\\]\nTale proprietà garantisce che le differenze tra individui siano indipendenti dagli specifici item somministrati.\nConsideriamo il seguente esempio pratico. Supponiamo che Marco e Cora affrontino un test composto da venti item. Se le probabilità di rispondere correttamente al primo item sono 20% per Marco e 80% per Cora, il rapporto tra le loro probabilità (4:1) deve rimanere lo stesso per tutti gli altri item del test. Questo equilibrio assicura che il confronto tra Marco e Cora non dipenda dagli item specifici ma esclusivamente dalle loro abilità relative.\n\n\n68.5.2 Limitazioni e Considerazioni\nSebbene l’oggettività specifica sia una proprietà potente, essa è soggetta a limitazioni:\n\nDipendenza dal contesto: La trasposizione di un test tra gruppi con caratteristiche culturali o professionali diverse (es. banchieri e ingegneri) potrebbe invalidare l’oggettività specifica se gli item vengono interpretati in modi differenti.\nNecessità di verifica empirica: L’oggettività specifica deve essere testata con i dati per confermare che le proprietà del modello siano rispettate nel contesto specifico.\n\nRasch stesso sottolineava l’importanza di validare questa proprietà ogni volta che si raccolgono nuovi dati.\nIn conclusione, l’oggettività specifica rappresenta il cuore del modello di Rasch, garantendo confronti equi e coerenti tra individui e item. Tuttavia, non è una proprietà intrinseca e universale, ma un’ipotesi di lavoro che deve essere verificata empiricamente in ogni applicazione. Questo principio, se rispettato, assicura che i test psicometrici siano strumenti affidabili per misurare abilità e caratteristiche, indipendentemente dal contesto o dagli item utilizzati.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>Assunzioni e Proprietà del Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/02_assumptions.html#riflessioni-conclusive",
    "href": "chapters/irt/02_assumptions.html#riflessioni-conclusive",
    "title": "68  Assunzioni e Proprietà del Modello di Rasch",
    "section": "68.6 Riflessioni Conclusive",
    "text": "68.6 Riflessioni Conclusive\nIl modello di Rasch rappresenta un approccio rigoroso alla misurazione psicometrica, basato su tre assunzioni fondamentali: unidimensionalità, monotonicità e indipendenza locale. Queste assunzioni costituiscono il fondamento del modello, garantendone validità e applicabilità. Tuttavia, la loro violazione può richiedere l’adozione di metodologie più avanzate o modelli alternativi per affrontare la complessità dei dati. In tali situazioni, un’analisi più approfondita dei dati o l’impiego di strumenti statistici sofisticati diventa indispensabile.\nUno degli aspetti distintivi del modello di Rasch è il principio di oggettività specifica, che consente di stimare la difficoltà degli item in modo indipendente dalle abilità dei partecipanti. Questo è reso possibile dall’uso della stima di massima verosimiglianza condizionale, che isola la difficoltà degli item basandosi unicamente sulle risposte specifiche a ciascun item, senza essere influenzata dal livello complessivo di abilità del campione.\nL’oggettività specifica assicura che i parametri di difficoltà degli item siano stabili e affidabili, indipendentemente dalla composizione del campione. Questo è analogo al concetto di invarianza in regressione lineare, dove i parametri della retta di regressione, come pendenza e intercetta, rimangono invariati rispetto al campione utilizzato per l’analisi. Nel modello di Rasch, l’invarianza dei parametri garantisce che la difficoltà degli item resti costante, anche quando i partecipanti hanno livelli di abilità diversi.\nUn aspetto particolarmente vantaggioso del modello di Rasch è che l’oggettività specifica elimina la necessità di utilizzare campioni normati o rappresentativi per calibrare gli item. Qualsiasi gruppo di partecipanti, purché presenti una sufficiente varietà nelle risposte, può essere impiegato per stimare i parametri di difficoltà. Questo contrasta con i metodi tradizionali, che spesso richiedono campioni rappresentativi per sviluppare tabelle normative basate su percentuali di risposte corrette.\nIl modello di Rasch offre un framework solido e trasparente per la misurazione psicometrica, distinguendosi per la precisione e la generalizzabilità delle sue stime. Tuttavia, il rispetto delle sue assunzioni fondamentali è cruciale per garantire risultati accurati ed equi. Verificare l’unidimensionalità, la monotonicità e l’indipendenza locale è essenziale per evitare bias e preservare l’integrità delle misurazioni.\nIn conclusione, il modello di Rasch si configura come uno strumento potente per sviluppare e validare strumenti di misura, combinando semplicità teorica con robustezza operativa. La sua capacità di produrre risultati indipendenti dalle caratteristiche del campione lo rende una scelta ideale per molte applicazioni psicometriche, purché le sue assunzioni siano rigorosamente testate e rispettate.\n\n\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>Assunzioni e Proprietà del Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/03_estimation.html",
    "href": "chapters/irt/03_estimation.html",
    "title": "69  Stima",
    "section": "",
    "text": "69.0.1 Introduzione\nPer utilizzare il modello di Rasch nella ricerca pratica, è essenziale comprendere come stimare i suoi parametri a partire dai dati osservati. In questa sezione verranno illustrati diversi metodi di stima, ognuno dei quali consente di calcolare sia i parametri degli item che quelli delle persone, differenziandosi però per l’approccio utilizzato.\nAlcuni metodi, come la massima verosimiglianza congiunta e l’inferenza bayesiana, stimano simultaneamente i parametri degli item e delle persone. Altri, come la massima verosimiglianza condizionale e la massima verosimiglianza marginale, separano il processo di stima: i parametri degli item vengono stimati per primi, seguiti dalla stima dei parametri delle persone in una fase successiva. Questa distinzione tra approcci permette di scegliere la metodologia più adatta al contesto e alle caratteristiche dei dati analizzati.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>Stima</span>"
    ]
  },
  {
    "objectID": "chapters/irt/03_estimation.html#la-funzione-di-verosimiglianza",
    "href": "chapters/irt/03_estimation.html#la-funzione-di-verosimiglianza",
    "title": "69  Stima",
    "section": "\n69.1 La Funzione di Verosimiglianza",
    "text": "69.1 La Funzione di Verosimiglianza\nLa stima dei parametri nel modello di Rasch si basa sulla funzione di verosimiglianza, che rappresenta la probabilità di osservare i dati disponibili dato un insieme di parametri sconosciuti. Nel contesto del modello, \\(U_{pi}\\) denota la risposta (corretta o errata) fornita dalla persona \\(p\\) all’item \\(i\\), dove una risposta corretta è codificata come 1 e una errata come 0. La probabilità condizionale che una persona con abilità \\(\\theta_p\\) risponda specificamente \\(u_{pi}\\) all’item \\(i\\), la cui difficoltà è \\(\\beta_i\\), è definita dalla formula:\n\\[\n\\text{Pr}(U_{pi} = u_{pi} | \\theta_p, \\beta_i) = \\frac{\\exp\\{u_{pi} \\cdot (\\theta_p - \\beta_i)\\}}{1 + \\exp(\\theta_p - \\beta_i)}.\n\\]\nQuesta equazione calcola la probabilità della risposta osservata, basandosi sulla differenza tra l’abilità della persona (\\(\\theta_p\\)) e la difficoltà dell’item (\\(\\beta_i\\)). Se l’abilità \\(\\theta_p\\) supera la difficoltà \\(\\beta_i\\), la probabilità di una risposta corretta (\\(u_{pi} = 1\\)) è elevata; al contrario, se \\(\\theta_p\\) è inferiore a \\(\\beta_i\\), tale probabilità sarà ridotta.\n\n69.1.1 Verosimiglianza Complessiva per una Persona\nLa verosimiglianza complessiva per una persona \\(p\\) rispetto a tutte le sue risposte agli item del test (\\(i = 1, \\dots, I\\)) si ottiene moltiplicando le probabilità condizionali di ciascuna risposta. La funzione di verosimiglianza totale è quindi espressa come:\n\\[\nL_{up}(\\theta_p, \\beta) = \\prod_{i=1}^{I} \\frac{\\exp\\{u_{pi} \\cdot (\\theta_p - \\beta_i)\\}}{1 + \\exp(\\theta_p - \\beta_i)}.\n\\]\nRiorganizzando per maggiore chiarezza, questa può essere riscritta come:\n\\[\nL_{up}(\\theta_p, \\beta) = \\frac{\\exp(r_p \\cdot \\theta_p - \\sum_{i=1}^{I} u_{pi} \\cdot \\beta_i)}{\\prod_{i=1}^{I} [1 + \\exp(\\theta_p - \\beta_i)]}, \\tag{1}\n\\]\ndove:\n\n\n\\(r_p = \\sum_{i=1}^{I} u_{pi}\\) rappresenta il punteggio grezzo della persona \\(p\\), ovvero il numero totale di risposte corrette.\n\nQuesta formulazione sintetizza come la funzione di verosimiglianza dipenda non solo dal parametro di abilità \\(\\theta_p\\) della persona, ma anche dai parametri di difficoltà \\(\\beta_i\\) degli item. La funzione è essenziale per stimare questi parametri e per interpretare la relazione tra abilità e difficoltà nel contesto del test.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>Stima</span>"
    ]
  },
  {
    "objectID": "chapters/irt/03_estimation.html#stima-dei-parametri-nel-modello-di-rasch",
    "href": "chapters/irt/03_estimation.html#stima-dei-parametri-nel-modello-di-rasch",
    "title": "69  Stima",
    "section": "\n69.2 Stima dei Parametri nel Modello di Rasch",
    "text": "69.2 Stima dei Parametri nel Modello di Rasch\nL’?eq-rasch-likelihood rappresenta la base comune per tutti i metodi di stima dei parametri nel modello di Rasch. Tuttavia, il metodo scelto per stimare i parametri influenzerà il modo in cui l’abilità delle persone (\\(\\theta_p\\)) e la difficoltà degli item (\\(\\beta_i\\)) vengono calcolate e interpretate:\n\n\nStima simultanea: Alcuni metodi, come la massima verosimiglianza congiunta e l’inferenza bayesiana, stimano simultaneamente \\(\\theta_p\\) e \\(\\beta_i\\).\n\nStima separata: Altri metodi, come la massima verosimiglianza condizionale e la massima verosimiglianza marginale, stimano \\(\\beta_i\\) in un primo passaggio, per poi derivare \\(\\theta_p\\).\n\nOgni approccio introduce assunzioni specifiche che influenzano le proprietà delle stime e la loro applicabilità in diversi contesti.\n\n69.2.1 Stima della Massima Verosimiglianza Congiunta\nLa stima della massima verosimiglianza congiunta (JML) mira a determinare simultaneamente i parametri delle persone (\\(\\theta_p\\)) e degli item (\\(\\beta_i\\)) che massimizzano la probabilità complessiva dei dati osservati, come descritto nella funzione di verosimiglianza del modello di Rasch. Questo approccio identifica il set di parametri più probabili che potrebbero aver generato il dataset analizzato.\nPunti di forza:\n\nMetodo diretto e intuitivo, che utilizza tutta l’informazione disponibile nei dati osservati.\n\nLimitazioni:\n\n\nStime inconsistenti: Nonostante la semplicità del metodo, JML non garantisce stime consistenti dei parametri degli item, anche con campioni di grandi dimensioni. Questo limita la sua affidabilità, specialmente in contesti che richiedono alta precisione e robustezza nelle stime.\n\nBias intrinseco: Le stime delle abilità (\\(\\theta_p\\)) e delle difficoltà (\\(\\beta_i\\)) possono essere influenzate l’una dall’altra, causando errori sistematici.\n\nImplementazione in R:\n\nJML è implementato nel pacchetto TAM, attraverso la funzione tam.jml(). Sebbene disponibile, il suo utilizzo è sconsigliato in analisi avanzate o quando la consistenza delle stime è critica.\n\n69.2.2 Stima della Massima Verosimiglianza Condizionale\nLa stima della massima verosimiglianza condizionale (CML) affronta le limitazioni della JML separando la stima dei parametri degli item da quella delle persone. Questo approccio procede in due fasi:\n\n\nStima dei parametri degli item:\n\nLa CML utilizza le statistiche sufficienti delle persone (ad esempio, i punteggi grezzi \\(r_p = \\sum u_{pi}\\)) per isolare i parametri degli item. In questa fase, le abilità delle persone (\\(\\theta_p\\)) non sono direttamente considerate, evitando il bias congiunto.\n\n\n\nStima dei parametri delle persone:\n\nUna volta stimati i parametri degli item (\\(\\beta_i\\)), si procede alla stima delle abilità (\\(\\theta_p\\)) basandosi sui dati individuali e sulle difficoltà stimate.\n\n\n\nVantaggi:\n\nFornisce stime consistenti dei parametri degli item.\nEvita il problema del bias associato alla stima simultanea di JML.\n\nLimitazioni:\n\nL’accuratezza dei parametri delle persone dipende dalla precisione delle stime degli item nella prima fase.\n\nImplementazione in R:\n\nLa CML è implementata nel pacchetto eRm tramite la funzione RM(), che consente di stimare i parametri degli item in modo robusto e separato.\n\n69.2.3 Stima della Massima Verosimiglianza Marginale\nLa stima della massima verosimiglianza marginale (MML) rappresenta un approccio avanzato che considera le abilità delle persone come una variabile casuale seguendo una distribuzione ipotizzata, tipicamente normale. Questo metodo differisce dalla CML trattando i parametri delle abilità (\\(\\theta_p\\)) come effetti casuali anziché fissi, e li integra nella funzione di verosimiglianza complessiva.\nCome funziona:\n\n\nDistribuzione marginale delle abilità:\n\nLa MML assume che le abilità (\\(\\theta_p\\)) siano distribuite nella popolazione secondo una distribuzione nota (ad esempio, una normale standard). Invece di stimare direttamente \\(\\theta_p\\), il metodo stima i parametri degli item (\\(\\beta_i\\)) tenendo conto di questa distribuzione.\n\n\n\nScoring individuale:\n\nDopo aver stimato i parametri degli item, si calcolano i punteggi individuali (\\(\\theta_p\\)) basandosi sulle risposte e sui parametri stimati.\n\n\n\nVantaggi:\n\nProduce stime più precise e realistiche dei parametri degli item rispetto alla JML.\nÈ particolarmente utile quando le abilità nella popolazione seguono una distribuzione continua e ipotizzabile.\n\nLimitazioni:\n\nLa validità delle stime dipende dalla correttezza dell’assunzione sulla distribuzione delle abilità (\\(\\theta_p\\)).\n\nImplementazione in R:\n\nLa MML è supportata dai pacchetti mirt e TAM. Ad esempio:\n\nFunzioni come mirt() in mirt permettono stime flessibili con distribuzioni marginali specificabili.\nAnche ltm (sebbene non più attivamente sviluppato) offre strumenti per la stima marginale.\n\n\n\n69.2.4 Confronto tra i Metodi\n\n\n\n\n\n\n\n\nMetodo\nCaratteristiche principali\nPro\nContro\n\n\n\nJML (Massima Verosimiglianza Congiunta)\nStima simultanea di \\(\\theta_p\\) e \\(\\beta_i\\).\nIntuitivo e diretto.\nStime inconsistenti; bias congiunto.\n\n\nCML (Massima Verosimiglianza Condizionale)\nStima separata in due fasi: prima \\(\\beta_i\\), poi \\(\\theta_p\\).\nStime consistenti per \\(\\beta_i\\); evita il bias.\nDipende dall’accuratezza delle stime iniziali degli item.\n\n\nMML (Massima Verosimiglianza Marginale)\nIntegra una distribuzione marginale per \\(\\theta_p\\); tratta \\(\\theta_p\\) come effetti casuali.\nStime realistiche e robuste; considera la distribuzione della popolazione.\nDipende dall’assunzione sulla distribuzione delle abilità.\n\n\n\nIn conclusione, ogni metodo presenta vantaggi e svantaggi che lo rendono più o meno adatto a specifici contesti di analisi. La JML è utile per analisi preliminari o semplici, ma è limitata dalla mancanza di consistenza. La CML e la MML offrono stime più robuste e realistiche, con la MML che si distingue per la sua flessibilità nell’incorporare distribuzioni di popolazione.\n\nEsercizio 69.1 Consideriamo ora la procedura di stima del livello di abilità \\(\\theta\\) di un individuo nel modello di Rasch attraverso l’uso della massima verosimiglianza marginale. La procedura per stimare la posizione di un individuo, dato un particolare pattern di risposte, può essere formulata con i seguenti passaggi.\n\nConsideriamo un determinato pattern di risposta. Per esempio, il pattern “11000” indica che un particolare individuo ha fornito due risposte corrette seguite da tre errate a cinque item, con un totale di \\(X = 2\\) risposte corrette.\nCalcoliamo le probabilità per ogni risposta. Utilizziamo l’Equazione 67.1 per calcolare la probabilità di ciascuna risposta nel pattern, in base a un dato livello di abilità \\(\\theta\\).\nDeterminiamo la probabilità del pattern di risposta. Questo passaggio si basa sull’assunzione di indipendenza condizionale (ovvero, per un dato \\(\\theta\\), le risposte sono indipendenti l’una dall’altra). Questa assunzione ci permette di applicare la regola di moltiplicazione per eventi indipendenti alle probabilità degli item per ottenere la probabilità complessiva del pattern di risposta per un dato \\(\\theta\\).\nRipetiamo i calcoli per diversi valori di \\(\\theta\\). Ripetiamo i passaggi 1 e 2 per una serie di valori di \\(\\theta\\). Nel nostro esempio, il range di \\(\\theta\\) va da \\(-3\\) a \\(3\\).\nDeterminiamo il valore di \\(\\theta\\) con la massima verosimiglianza. L’ultimo passaggio consiste nel determinare quale valore di \\(\\theta\\) tra quelli calcolati nel passaggio 3 abbia la più alta verosimiglianza di produrre il pattern “11000”. Per fare questo scegliamo il valore \\(\\theta\\) per cui la verosimiglianza è massima.\n\nDi seguito, esaminiamo uno script in R che implementa questa procedura.\n\n# Definiamo il pattern di risposta\nresponse_pattern &lt;- c(1, 1, 0, 0, 0)\n\n# Range di valori di theta da esplorare\ntheta_values &lt;- seq(-3, 3, by = 0.01)\n\n# Funzione per calcolare la probabilità di un singolo pattern di risposta\ncalculate_probability &lt;- function(theta, pattern) {\n    correct_probs &lt;- exp(theta) / (1 + exp(theta))\n    item_probs &lt;- ifelse(pattern == 1, correct_probs, 1 - correct_probs)\n    prod(item_probs)\n}\n# Per semplicità, assumiamo che il parametro di difficoltà (beta) sia zero per tutti gli item.\n\n# Calcoliamo le probabilità per ogni valore di theta. Usiamo sapply per applicare \n# la funzione calculate_probability a ciascun valore di theta nel range specificato.\nprobabilities &lt;- sapply(theta_values, calculate_probability, pattern = response_pattern)\n\n# Identifichiamo il valore di theta con la massima verosimiglianza\nbest_theta &lt;- theta_values[which.max(probabilities)]\n\nprint(paste(\"Valore di theta calcolato con la massima verosimiglianza:\", best_theta))\n#&gt; [1] \"Valore di theta calcolato con la massima verosimiglianza: -0.41\"\n\nQuesto script calcola la probabilità di ottenere il pattern di risposta “11000” per cinque item per un dato intervallo di valori di \\(\\theta\\) e identifica il valore di \\(\\theta\\) che massimizza questa probabilità. Si noti che il modello di Rasch prevede che tutti gli item abbiano la stessa discriminazione, quindi non è necessario specificare un parametro di discriminazione per ogni item. Abbiamo assunto inoltre che la difficoltà di tutti gli item sia uguale a zero.\nLa verosimiglianza di un pattern di risposta di un singolo rispondente a diversi item può essere rappresentata simbolicamente nel modo seguente. Se consideriamo \\(x\\) come il pattern di risposta di un rispondente (ad esempio, \\(x = 11000\\) indica che il rispondente ha risposto correttamente ai primi due item e ha dato risposte sbagliate agli ultimi tre), la verosimiglianza del vettore di risposta \\(x_i\\) della persona \\(i\\) è espressa come:\n\\[\n\\begin{equation}\nL(x_i) = \\prod_{j=1}^{L} p_{ij},\n\\end{equation}\n\\]\ndove \\(p_{ij} = p(x_{ij} = 1 \\mid \\theta_i, \\alpha_j, \\delta_j)\\) rappresenta la probabilità che la persona \\(i\\), con un livello di abilità \\(\\theta_i\\), risponda correttamente all’item \\(j\\). In questa formula, \\(\\alpha_j\\) è il parametro di discriminazione dell’item \\(j\\) e \\(\\delta_j\\) è il suo parametro di difficoltà. Il parametro \\(\\alpha_j\\) indica quanto bene l’item \\(j\\) è in grado di discriminare tra rispondenti di diversi livelli di abilità, mentre \\(\\delta_j\\) rappresenta il livello di abilità per cui la probabilità di una risposta corretta è del 50%. Il prodotto è calcolato su tutti gli \\(L\\) item a cui il rispondente ha risposto, e il simbolo \\(\\prod\\) rappresenta il prodotto di tutte queste probabilità individuali.\nIl calcolo diretto della verosimiglianza può diventare problematico all’aumentare del numero di item, poiché il prodotto di molteplici probabilità può risultare in valori molto piccoli, difficili da gestire con precisione in calcoli numerici. Pertanto, è spesso più pratico lavorare con la trasformazione logaritmica naturale della verosimiglianza, ovvero \\(\\log_e(L(x_i))\\) o \\(\\ln(L(x_i))\\). Questa trasformazione converte il prodotto in una somma, come segue:\n\\[\n\\begin{equation}\n\\ln L(x_i) = \\sum_{j=1}^{L} \\ln(p_{ij}).\n\\end{equation}\n\\]\nL’uso del logaritmo naturale trasforma quindi la verosimiglianza in una somma di logaritmi, semplificando il calcolo e riducendo i problemi di rappresentazione numerica nei calcoli complessi.\n\n# Definizione del pattern di risposta\nresponse_pattern &lt;- c(1, 1, 0, 0, 0)\n\n# Range di valori di theta da esplorare\ntheta_values &lt;- seq(-3, 3, by = 0.1)\n\n# Calcolo della log-verosimiglianza per ogni valore di theta\nlog_likelihoods &lt;- numeric(length(theta_values))\nfor (i in seq_along(theta_values)) {\n    theta &lt;- theta_values[i]\n    log_item_probs &lt;- numeric(length(response_pattern))\n\n    # Calcolo delle probabilità logaritmiche individuali per ogni item nel pattern\n    for (j in seq_along(response_pattern)) {\n        prob_correct &lt;- exp(theta) / (1 + exp(theta))\n        prob &lt;- ifelse(response_pattern[j] == 1, prob_correct, 1 - prob_correct)\n        log_item_probs[j] &lt;- log(prob)\n    }\n\n    # Calcolo della log-verosimiglianza\n    log_likelihoods[i] &lt;- sum(log_item_probs)\n}\n\n# Creazione di un dataframe per il plotting\nplot_data &lt;- data.frame(theta = theta_values, log_likelihood = log_likelihoods)\n\n# Rappresentazione grafica della log-verosimiglianza\nggplot(plot_data, aes(x = theta, y = log_likelihood)) +\n    geom_line() +\n    labs(\n        x = expression(theta), y = \"Log-likelihood\",\n        title = \"Log-likelihood Function for Response Pattern 11000\"\n    )",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>Stima</span>"
    ]
  },
  {
    "objectID": "chapters/irt/03_estimation.html#errore-standard-della-stima-e-informazione-dellitem",
    "href": "chapters/irt/03_estimation.html#errore-standard-della-stima-e-informazione-dellitem",
    "title": "69  Stima",
    "section": "\n69.3 Errore Standard della Stima e Informazione dell’Item",
    "text": "69.3 Errore Standard della Stima e Informazione dell’Item\nNel modello di Rasch, l’Errore Standard della Stima (EES) è un indicatore chiave che quantifica l’incertezza associata alla stima del livello di abilità di un individuo (\\(\\theta\\)). L’EES è fondamentale perché fornisce una misura della precisione con cui la stima di \\(\\theta\\) riflette l’abilità reale del rispondente. Un EES più basso indica una stima più precisa, mentre un EES più alto segnala una maggiore incertezza.\n\n69.3.1 Calcolo dell’EES\nL’EES è determinato dall’informazione totale dell’item a un dato livello di abilità \\(\\theta\\), indicata con \\(I(\\theta)\\). L’EES è definito come l’inverso della radice quadrata di \\(I(\\theta)\\):\n\\[\n\\text{EES}(\\theta) = \\frac{1}{\\sqrt{I(\\theta)}},\n\\]\ndove \\(I(\\theta)\\) rappresenta l’informazione totale accumulata dagli item del test a quel livello di abilità.\n\n69.3.2 Informazione dell’Item\nL’informazione dell’item misura il contributo di ciascun item alla precisione della stima di \\(\\theta\\). Per un dato livello di abilità, l’informazione fornita da un singolo item dipende dalla probabilità che il rispondente dia una risposta corretta (\\(p_{ij}\\)) e dalla probabilità di una risposta errata (\\(1 - p_{ij}\\)). La formula per calcolare l’informazione totale degli item è:\n\\[\nI(\\theta) = \\sum_{j=1}^{L} p_{ij}(1 - p_{ij}),\n\\]\ndove:\n\n\n\\(L\\) è il numero totale di item del test.\n\n\\(p_{ij}\\) è la probabilità che una persona con abilità \\(\\theta\\) risponda correttamente all’item \\(j\\).\n\nL’informazione fornita da un singolo item raggiunge il suo massimo quando la difficoltà dell’item (\\(\\delta_j\\)) è uguale al livello di abilità del rispondente (\\(\\theta\\)). In questa condizione, l’item discrimina al meglio tra rispondenti con livelli di abilità leggermente superiori o inferiori a \\(\\delta_j\\).\n\n69.3.3 Relazione tra Informazione e Precisione\n\n\nMassima informazione, minima incertezza: Quando \\(I(\\theta)\\) è alta, l’EES (\\(\\text{EES}(\\theta)\\)) è basso, indicando una stima precisa.\n\nBassa informazione, alta incertezza: Quando \\(I(\\theta)\\) è bassa, l’EES è alto, segnalando una maggiore incertezza nella stima di \\(\\theta\\).\n\nQuesta relazione evidenzia l’importanza di progettare test con item che siano informativi per il range di abilità di interesse.\n\n69.3.4 Curva di Informazione dell’Item\nL’informazione dell’item varia a seconda del livello di abilità del rispondente. Per visualizzare questa relazione, si traccia la curva di informazione dell’item, che rappresenta l’informazione fornita da un singolo item in funzione di \\(\\theta\\). Alcune caratteristiche della curva:\n\nHa una forma a campana.\nRaggiunge il picco quando \\(\\theta = \\delta_j\\), ossia quando l’abilità del rispondente corrisponde alla difficoltà dell’item.\nLarghezza e altezza della curva dipendono dalla discriminazione dell’item (nel modello Rasch, fissata a 1).\n\nLa somma delle curve di informazione dei singoli item produce la curva di informazione totale del test, che mostra la precisione complessiva del test a diversi livelli di abilità.\n\n69.3.5 Applicazioni pratiche\n\nProgettazione del test: La conoscenza dell’informazione degli item aiuta a creare test che siano più informativi per specifici livelli di abilità, riducendo l’EES per i range di interesse.\n\nInterpretazione dei risultati: L’EES permette di stimare intervalli di confidenza per \\(\\theta\\), fornendo una misura della precisione della stima:\n\\[\n\\text{Intervallo di confidenza per } \\theta = \\theta \\pm 1.96 \\cdot \\text{EES}(\\theta).\n\\]\n\n\nL’analisi dell’informazione dell’item e del test è quindi essenziale per garantire che le misurazioni ottenute siano affidabili e utili per l’interpretazione e il confronto delle abilità.\n\nEsercizio 69.2 Utilizzando il modello di Rasch, possiamo calcolare le probabilità di risposta corretta per diversi valori di abilità e, di conseguenza, la Funzione Informativa dell’Item (Item Information Function, IIF):\n\n# Definizione di un range di abilità\ntheta &lt;- seq(-4, 4, by = 0.1)\n\n# Definizione di un parametro di difficoltà dell'item\nbeta &lt;- 0\n\n# Calcolo delle probabilità di risposta corretta per ciascun valore di abilità usando la funzione logistica\nprob_correct &lt;- exp(theta - beta) / (1 + exp(theta - beta))\n\n# Calcolo dell'informazione dell'item\nitem_info &lt;- prob_correct * (1 - prob_correct)\n\n# Creazione della prima grafica (ICC)\nplot(theta, prob_correct,\n    type = \"l\", col = \"blue\", lwd = 2,\n    xlab = \"Abilita' theta\", ylab = \"Probabilita' di Risposta Corretta\",\n    main = \"Curva Caratteristica dell'Item (ICC) e Informazione dell'Item\"\n)\n\n# Aggiunta di un secondo asse y per l'informazione\npar(new = TRUE)\nplot(theta, item_info,\n    type = \"l\", col = \"red\", lwd = 2,\n    xlab = \"\", ylab = \"\", axes = FALSE, ann = FALSE\n)\n\n# Aggiungere l'asse y di destra per l'informazione\naxis(side = 4, at = pretty(range(item_info)))\nmtext(\"Informazione\", side = 4, line = 3)\n\n# Aggiunta della legenda\nlegend(\"topright\",\n    legend = c(\"ICC\", \"Informazione\"),\n    col = c(\"blue\", \"red\"), lty = 1, cex = 0.8\n)\n\n\n\n\n\n\n\nQuesta rappresentazione grafica in R mostra come l’informazione vari in funzione del livello di abilità. In generale, l’informazione è massima quando l’abilità dell’esaminando è vicina alla difficoltà dell’item e diminuisce man mano che ci si allontana da questo punto.\nIl concetto di informazione in IRT è fondamentale sia per la costruzione del test sia per la sua interpretazione. Indica quanto efficacemente ciascun item misura l’abilità a vari livelli e aiuta a determinare quali item sono più informativi per la stima dell’abilità degli esaminandi. Inoltre, fornisce indicazioni sulla precisione con cui l’abilità degli esaminandi può essere stimata a vari punti lungo la scala di abilità.\n\n\nEsercizio 69.3 Per dimostrare come calcolare la TIF in \\(\\mathsf{R}\\), possiamo estendere l’esempio precedente includendo più item e sommando le loro informazioni:\n\n# Definizione di parametri di difficoltà per diversi item\nbeta_items &lt;- c(-1, 0, 1) # Esempio di tre item con difficoltà diverse\n\n# Calcolo dell'informazione per ogni item e somma per ottenere la TIF\ntest_info &lt;- rep(0, length(theta))\nfor (beta in beta_items) {\n    prob_correct &lt;- exp(theta - beta) / (1 + exp(theta - beta))\n    item_info &lt;- prob_correct * (1 - prob_correct)\n    test_info &lt;- test_info + item_info\n}\n\n# Creazione del grafico della TIF\nplot(theta, test_info,\n    type = \"l\", col = \"blue\", lwd = 2,\n    xlab = \"Abilità theta\", ylab = \"Informazione del Test\",\n    main = \"Funzione di Informazione del Test (TIF)\"\n)\n\n\n\n\n\n\n\nIn questo esempio, calcoliamo e sommiamo le informazioni di tre item con diverse difficoltà per visualizzare la TIF di un test ipotetico. La TIF mostra in modo chiaro come il test nel suo insieme stima l’abilità degli esaminandi a vari livelli, fornendo così indicazioni preziose sulla costruzione e sull’utilizzo ottimale del test in diversi contesti.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>Stima</span>"
    ]
  },
  {
    "objectID": "chapters/irt/03_estimation.html#stima-dellabilità",
    "href": "chapters/irt/03_estimation.html#stima-dellabilità",
    "title": "69  Stima",
    "section": "\n69.4 Stima dell’Abilità",
    "text": "69.4 Stima dell’Abilità\nNel contesto dell’IRT, la stima dell’abilità di un esaminando (\\(\\theta\\)) viene effettuata utilizzando metodi iterativi, come la massima verosimiglianza, che sfruttano i dati del test e i parametri degli item. Questo processo consente di stimare il livello di abilità in modo personalizzato, tenendo conto del pattern di risposte specifico di ciascun esaminando.\n\n69.4.1 Procedura di Stima dell’Abilità\n\n\nPunto di partenza:\n\nLa stima inizia con un’ipotesi iniziale o un valore a priori per l’abilità dell’esaminando. Questo valore può essere scelto in base a considerazioni teoriche (ad esempio, \\(\\theta = 0\\), corrispondente alla media presunta dell’abilità) o determinato da informazioni preliminari.\n\n\n\nUtilizzo dei parametri degli item:\n\nI parametri noti degli item (ad esempio, difficoltà (_i) e discriminazione (a_i)) vengono utilizzati per calcolare la probabilità che l’esaminando risponda correttamente a ciascun item in base al livello di abilità iniziale ipotizzato. Questa probabilità è calcolata attraverso la funzione di risposta dell’item (IRF).\n\n\n\nIterazione per aggiustare la stima:\n\nIl livello di abilità viene aggiornato iterativamente. L’obiettivo di ogni iterazione è migliorare la corrispondenza tra le probabilità previste di risposta corretta (basate sul livello di abilità stimato) e il pattern effettivo di risposte fornite dall’esaminando.\nQuesto processo continua fino a quando le modifiche alla stima di \\(\\theta\\) diventano trascurabili, indicando che è stato raggiunto un punto di convergenza. Il risultato finale è una stima stabile e affidabile dell’abilità.\n\n\n\nStima personalizzata:\n\nIl processo viene ripetuto per ciascun esaminando, assicurando che ogni stima di \\(\\theta\\) sia basata esclusivamente sulle sue risposte.\n\n\n\n69.4.2 Metodi alternativi di stima\n\n\nStima simultanea:\n\nIn alternativa alla stima iterativa individuale, esistono approcci che stimano simultaneamente i livelli di abilità di tutti gli esaminandi. Questi metodi sono particolarmente utili in presenza di un ampio campione, ottimizzando il processo di calcolo.\n\n\n\nStima Bayesiana:\n\nLa stima bayesiana combina i dati del test con una distribuzione a priori sull’abilità (\\(\\theta\\)) per ottenere una stima posteriore. Questo approccio è particolarmente utile quando il numero di item è limitato o le risposte sono incomplete.\n\n\n\n69.4.3 Importanza della Stima dell’Abilità\nLa stima dell’abilità in IRT è fondamentale per due motivi principali:\n\n\nValutazione personalizzata:\n\nPermette di misurare l’abilità di ciascun esaminando in maniera individualizzata, considerando le interazioni specifiche tra il rispondente e gli item. Questa personalizzazione rende la stima più accurata rispetto ai punteggi grezzi, che non tengono conto delle caratteristiche degli item.\n\n\n\nAnalisi mirate:\n\nPoiché la stima dell’abilità è direttamente legata ai parametri degli item, consente di condurre analisi dettagliate sull’efficacia del test (ad esempio, quali item sono più informativi per specifici livelli di abilità) e sulle caratteristiche dei rispondenti.\n\n\n\nIn conclusione, la stima dell’abilità in IRT è un processo iterativo che utilizza i parametri degli item e il pattern di risposte individuali per fornire stime accurate e personalizzate del livello di abilità di ciascun esaminando. Grazie alla sua precisione, questa metodologia rappresenta una componente essenziale dell’IRT, sia per la valutazione degli esaminandi sia per l’ottimizzazione dei test.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>Stima</span>"
    ]
  },
  {
    "objectID": "chapters/irt/03_estimation.html#stima-bayesiana",
    "href": "chapters/irt/03_estimation.html#stima-bayesiana",
    "title": "69  Stima",
    "section": "\n69.5 Stima Bayesiana",
    "text": "69.5 Stima Bayesiana\nLa stima bayesiana sta diventando un metodo sempre più popolare per stimare i parametri del modello di Rasch. Come la stima della massima verosimiglianza congiunta, la stima bayesiana stima simultaneamente sia i parametri delle persone che quelli degli item. Tuttavia, mentre la stima della massima verosimiglianza congiunta trova i valori di \\(\\theta\\) e \\(\\beta\\) massimizzando la verosimiglianza congiunta, la stima bayesiana utilizza la regola di Bayes per trovare la densità a posteriori, \\(f(\\theta,\\beta \\mid u)\\).\nNel modello di Rasch, la regola di Bayes afferma che:\n\\[\nf(\\theta,\\beta \\mid u) = \\frac{\\text{Pr}(u \\mid \\theta,\\beta)f(\\theta,\\beta)}{\\text{Pr}(u)}.\n\\]\nIl primo termine nel numeratore, \\(\\text{Pr}(u \\mid \\theta, \\beta)\\), è la verosimiglianza congiunta. Il secondo è la distribuzione a priori congiunta per \\(\\theta\\) e \\(\\beta\\). Il denominatore è la probabilità media dei dati osservati rispetto alla distribuzione a priori congiunta.\nA differenza della stima della massima verosimiglianza, che si concentra sulla massimizzazione della verosimiglianza, la stima bayesiana integra le informazioni a priori con i dati osservati. La regola di Bayes combina la verosimiglianza dei dati osservati (la probabilità di osservare i dati dati i parametri) con la distribuzione a priori (le nostre credenze sui parametri prima di osservare i dati) per produrre una distribuzione a posteriori (le nostre credenze aggiornate sui parametri dopo aver osservato i dati). La densità a posteriori \\(f(\\theta,\\beta \\mid u)\\) ci fornisce una stima completa dei parametri, considerando sia i dati osservati sia le informazioni a priori.\nIn pratica, la stima bayesiana fornisce un approccio flessibile e informativo alla stima dei parametri nel modello di Rasch, consentendo l’integrazione di conoscenze pregresse e osservazioni attuali.\n\n69.5.1 Implementazione\nEsaminiamo un’applicazione della stima Bayesiana usando il linguaggio probabilistico Stan. Il modello di Rasch è implementato nel file rasch_model.stan utilizzando le distribuzioni a priori specificate da Debelak et al. (2022).\n\nstan_file &lt;- \"../../code/rasch_model.stan\"\nmod &lt;- cmdstan_model(stan_file)\nmod$print()\n#&gt; data {\n#&gt;   int&lt;lower=1&gt; num_person;\n#&gt;   int&lt;lower=1&gt; num_item;\n#&gt;   array[num_person, num_item] int&lt;lower=0, upper=1&gt; U;\n#&gt; }\n#&gt; parameters {\n#&gt;   vector[num_person] theta;\n#&gt;   vector[num_item] beta;\n#&gt;   real mu_beta;\n#&gt;   real&lt;lower=0&gt; sigma2_theta;\n#&gt;   real&lt;lower=0&gt; sigma2_beta;\n#&gt; }\n#&gt; transformed parameters {\n#&gt;   array[num_person, num_item] real&lt;lower=0, upper=1&gt; prob_solve;\n#&gt;   for (p in 1:num_person) \n#&gt;     for (i in 1:num_item) \n#&gt;       prob_solve[p, i] = inv_logit(theta[p] - beta[i]);\n#&gt; }\n#&gt; model {\n#&gt;   for (p in 1:num_person) \n#&gt;     for (i in 1:num_item) \n#&gt;       U[p, i] ~ bernoulli(prob_solve[p, i]);\n#&gt;   theta ~ normal(0, sqrt(sigma2_theta));\n#&gt;   beta ~ normal(mu_beta, sqrt(sigma2_beta));\n#&gt;   sigma2_theta ~ inv_chi_square(0.5);\n#&gt;   sigma2_beta ~ inv_chi_square(0.5);\n#&gt; }\n\nNella presente implementazione bayesiana del modello di Rasch, le sezioni “transformed parameters” e “model” hanno un ruolo centrale nel definire come i dati vengono processati e come il modello viene applicato. Vediamo dettagliatamente ciascuna sezione:\n\n69.5.1.1 Sezione Transformed Parameters\nNella sezione transformed parameters, viene definita la trasformazione dei parametri di base (i parametri theta per le abilità delle persone e beta per la difficoltà degli item) in una probabilità di risposta corretta per ogni coppia persona-item. Qui viene usata la funzione logistica inversa per convertire la differenza tra l’abilità della persona e la difficoltà dell’item in una probabilità:\ntransformed parameters {\n  array[num_person, num_item] real&lt;lower=0, upper=1&gt; prob_solve;\n  for (p in 1:num_person) \n    for (i in 1:num_item) \n      prob_solve[p, i] = inv_logit(theta[p] - beta[i]);\n}\nQuesta trasformazione serve a mappare la differenza tra l’abilità della persona (theta[p]) e la difficoltà dell’item (beta[i]) in un intervallo di probabilità tra 0 e 1. La funzione inv_logit è comunemente usata per questo scopo, essendo la funzione logistica inversa.\n\n69.5.1.2 Sezione Model\nNella sezione model, vengono definite le distribuzioni di probabilità per i dati osservati e i parametri del modello, che sono essenziali per la stima bayesiana. Questa parte del codice descrive come i dati sono generati, supponendo il modello di Rasch:\nmodel {\n  for (p in 1:num_person) \n    for (i in 1:num_item) \n      U[p, i] ~ bernoulli(prob_solve[p, i]);\n  theta ~ normal(0, sqrt(sigma2_theta));\n  beta ~ normal(mu_beta, sqrt(sigma2_beta));\n  sigma2_theta ~ inv_chi_square(0.5);\n  sigma2_beta ~ inv_chi_square(0.5);\n}\n\n\nU[p, i] ~ bernoulli(prob_solve[p, i]): ogni risposta U[p, i], che indica se la persona p ha risposto correttamente all’item i, segue una distribuzione di Bernoulli dove la probabilità di successo è data da prob_solve[p, i]. Questa è la vera verosimiglianza del modello, che collega i dati osservati alle probabilità calcolate tramite il modello logistico.\n\ntheta ~ normal(0, sqrt(sigma2_theta)) e beta ~ normal(mu_beta, sqrt(sigma2_beta)): le distribuzioni a priori per i parametri theta e beta sono normali. Questo significa che, in assenza di dati, si assume che queste variabili si distribuiscano normalmente con una media di 0 per theta e mu_beta per beta, e una deviazione standard derivata dai parametri di varianza sigma2_theta e sigma2_beta.\n\nsigma2_theta ~ inv_chi_square(0.5) e sigma2_beta ~ inv_chi_square(0.5): le varianze sigma2_theta e sigma2_beta hanno distribuzioni a priori che seguono una distribuzione chi quadrato inversa con parametro di forma 0.5. Questa è una scelta comune per imporre una distribuzione non informativa (vaga) sui parametri di scala.\n\nIn conclusione, la sezione transformed parameters calcola le probabilità di risposta corretta basate sui parametri di abilità e difficoltà, mentre la sezione model specifica come questi parametri e le risposte osservate interagiscono secondo il modello di Rasch, definendo così la struttura della verosimiglianza e delle priorità nel contesto bayesiano.\nCompiliamo il modello usando CmdStan:\n\nmod$compile()\n\nDefiniamo i dati nel formato appropriato per Stan:\n\ndata(data.fims.Aus.Jpn.scored, package = \"TAM\")\npeople &lt;- 1:400\nresponses &lt;- data.fims.Aus.Jpn.scored[people, 2:15]\nresponses &lt;- as.matrix(sapply(responses, as.integer))\ncolnames(responses) &lt;- gsub(\"M1PTI\", \"I\", colnames(responses))\n\nstan_data &lt;- list(\n    num_person = nrow(responses),\n    num_item = ncol(responses),\n    U = responses\n)\n\nEseguiamo il campionamento MCMC per ottenere la distribuzione a posteriori dei parametri.\n\nfit &lt;- mod$sample(\n    data = stan_data,\n    chains = 4, # Number of MCMC chains\n    parallel_chains = 2, # Number of chains to run in parallel \n    iter_warmup = 2000, # Number of warmup iterations per chain\n    iter_sampling = 2000, # Number of sampling iterations per chain\n    seed = 1234 # Set a seed for reproducibility\n)\n#&gt; Running MCMC with 4 chains, at most 2 in parallel...\n#&gt; \n#&gt; Chain 1 Iteration:    1 / 4000 [  0%]  (Warmup)\n#&gt; Chain 2 Iteration:    1 / 4000 [  0%]  (Warmup)\n#&gt; Chain 1 Iteration:  100 / 4000 [  2%]  (Warmup) \n#&gt; Chain 2 Iteration:  100 / 4000 [  2%]  (Warmup) \n#&gt; Chain 1 Iteration:  200 / 4000 [  5%]  (Warmup) \n#&gt; Chain 2 Iteration:  200 / 4000 [  5%]  (Warmup) \n#&gt; Chain 1 Iteration:  300 / 4000 [  7%]  (Warmup) \n#&gt; Chain 2 Iteration:  300 / 4000 [  7%]  (Warmup) \n#&gt; Chain 1 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#&gt; Chain 2 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#&gt; Chain 1 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#&gt; Chain 2 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#&gt; Chain 1 Iteration:  600 / 4000 [ 15%]  (Warmup) \n#&gt; Chain 2 Iteration:  600 / 4000 [ 15%]  (Warmup) \n#&gt; Chain 1 Iteration:  700 / 4000 [ 17%]  (Warmup) \n#&gt; Chain 2 Iteration:  700 / 4000 [ 17%]  (Warmup) \n#&gt; Chain 1 Iteration:  800 / 4000 [ 20%]  (Warmup) \n#&gt; Chain 2 Iteration:  800 / 4000 [ 20%]  (Warmup) \n#&gt; Chain 1 Iteration:  900 / 4000 [ 22%]  (Warmup) \n#&gt; Chain 2 Iteration:  900 / 4000 [ 22%]  (Warmup) \n#&gt; Chain 1 Iteration: 1000 / 4000 [ 25%]  (Warmup) \n#&gt; Chain 2 Iteration: 1000 / 4000 [ 25%]  (Warmup) \n#&gt; Chain 1 Iteration: 1100 / 4000 [ 27%]  (Warmup) \n#&gt; Chain 2 Iteration: 1100 / 4000 [ 27%]  (Warmup) \n#&gt; Chain 1 Iteration: 1200 / 4000 [ 30%]  (Warmup) \n#&gt; Chain 2 Iteration: 1200 / 4000 [ 30%]  (Warmup) \n#&gt; Chain 1 Iteration: 1300 / 4000 [ 32%]  (Warmup) \n#&gt; Chain 2 Iteration: 1300 / 4000 [ 32%]  (Warmup) \n#&gt; Chain 1 Iteration: 1400 / 4000 [ 35%]  (Warmup) \n#&gt; Chain 2 Iteration: 1400 / 4000 [ 35%]  (Warmup) \n#&gt; Chain 1 Iteration: 1500 / 4000 [ 37%]  (Warmup) \n#&gt; Chain 2 Iteration: 1500 / 4000 [ 37%]  (Warmup) \n#&gt; Chain 1 Iteration: 1600 / 4000 [ 40%]  (Warmup) \n#&gt; Chain 2 Iteration: 1600 / 4000 [ 40%]  (Warmup) \n#&gt; Chain 1 Iteration: 1700 / 4000 [ 42%]  (Warmup) \n#&gt; Chain 2 Iteration: 1700 / 4000 [ 42%]  (Warmup) \n#&gt; Chain 1 Iteration: 1800 / 4000 [ 45%]  (Warmup) \n#&gt; Chain 2 Iteration: 1800 / 4000 [ 45%]  (Warmup) \n#&gt; Chain 1 Iteration: 1900 / 4000 [ 47%]  (Warmup) \n#&gt; Chain 2 Iteration: 1900 / 4000 [ 47%]  (Warmup) \n#&gt; Chain 1 Iteration: 2000 / 4000 [ 50%]  (Warmup) \n#&gt; Chain 1 Iteration: 2001 / 4000 [ 50%]  (Sampling) \n#&gt; Chain 2 Iteration: 2000 / 4000 [ 50%]  (Warmup) \n#&gt; Chain 2 Iteration: 2001 / 4000 [ 50%]  (Sampling) \n#&gt; Chain 1 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#&gt; Chain 2 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#&gt; Chain 1 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#&gt; Chain 2 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#&gt; Chain 1 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#&gt; Chain 2 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#&gt; Chain 1 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#&gt; Chain 2 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#&gt; Chain 1 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#&gt; Chain 2 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#&gt; Chain 1 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#&gt; Chain 2 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#&gt; Chain 1 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#&gt; Chain 2 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#&gt; Chain 1 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#&gt; Chain 2 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#&gt; Chain 1 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#&gt; Chain 2 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#&gt; Chain 1 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#&gt; Chain 2 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#&gt; Chain 1 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#&gt; Chain 2 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#&gt; Chain 1 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#&gt; Chain 2 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#&gt; Chain 1 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#&gt; Chain 2 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#&gt; Chain 1 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#&gt; Chain 2 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#&gt; Chain 1 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#&gt; Chain 2 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#&gt; Chain 1 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#&gt; Chain 2 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#&gt; Chain 1 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#&gt; Chain 2 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#&gt; Chain 1 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#&gt; Chain 2 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#&gt; Chain 1 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#&gt; Chain 2 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#&gt; Chain 1 Iteration: 4000 / 4000 [100%]  (Sampling) \n#&gt; Chain 1 finished in 14.5 seconds.\n#&gt; Chain 3 Iteration:    1 / 4000 [  0%]  (Warmup)\n#&gt; Chain 2 Iteration: 4000 / 4000 [100%]  (Sampling) \n#&gt; Chain 2 finished in 14.7 seconds.\n#&gt; Chain 4 Iteration:    1 / 4000 [  0%]  (Warmup) \n#&gt; Chain 3 Iteration:  100 / 4000 [  2%]  (Warmup) \n#&gt; Chain 3 Iteration:  200 / 4000 [  5%]  (Warmup) \n#&gt; Chain 4 Iteration:  100 / 4000 [  2%]  (Warmup) \n#&gt; Chain 3 Iteration:  300 / 4000 [  7%]  (Warmup) \n#&gt; Chain 4 Iteration:  200 / 4000 [  5%]  (Warmup) \n#&gt; Chain 3 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#&gt; Chain 4 Iteration:  300 / 4000 [  7%]  (Warmup) \n#&gt; Chain 3 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#&gt; Chain 4 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#&gt; Chain 3 Iteration:  600 / 4000 [ 15%]  (Warmup) \n#&gt; Chain 4 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#&gt; Chain 3 Iteration:  700 / 4000 [ 17%]  (Warmup) \n#&gt; Chain 4 Iteration:  600 / 4000 [ 15%]  (Warmup) \n#&gt; Chain 3 Iteration:  800 / 4000 [ 20%]  (Warmup) \n#&gt; Chain 4 Iteration:  700 / 4000 [ 17%]  (Warmup) \n#&gt; Chain 3 Iteration:  900 / 4000 [ 22%]  (Warmup) \n#&gt; Chain 4 Iteration:  800 / 4000 [ 20%]  (Warmup) \n#&gt; Chain 3 Iteration: 1000 / 4000 [ 25%]  (Warmup) \n#&gt; Chain 4 Iteration:  900 / 4000 [ 22%]  (Warmup) \n#&gt; Chain 3 Iteration: 1100 / 4000 [ 27%]  (Warmup) \n#&gt; Chain 4 Iteration: 1000 / 4000 [ 25%]  (Warmup) \n#&gt; Chain 3 Iteration: 1200 / 4000 [ 30%]  (Warmup) \n#&gt; Chain 4 Iteration: 1100 / 4000 [ 27%]  (Warmup) \n#&gt; Chain 3 Iteration: 1300 / 4000 [ 32%]  (Warmup) \n#&gt; Chain 4 Iteration: 1200 / 4000 [ 30%]  (Warmup) \n#&gt; Chain 3 Iteration: 1400 / 4000 [ 35%]  (Warmup) \n#&gt; Chain 4 Iteration: 1300 / 4000 [ 32%]  (Warmup) \n#&gt; Chain 3 Iteration: 1500 / 4000 [ 37%]  (Warmup) \n#&gt; Chain 4 Iteration: 1400 / 4000 [ 35%]  (Warmup) \n#&gt; Chain 3 Iteration: 1600 / 4000 [ 40%]  (Warmup) \n#&gt; Chain 4 Iteration: 1500 / 4000 [ 37%]  (Warmup) \n#&gt; Chain 3 Iteration: 1700 / 4000 [ 42%]  (Warmup) \n#&gt; Chain 4 Iteration: 1600 / 4000 [ 40%]  (Warmup) \n#&gt; Chain 3 Iteration: 1800 / 4000 [ 45%]  (Warmup) \n#&gt; Chain 4 Iteration: 1700 / 4000 [ 42%]  (Warmup) \n#&gt; Chain 3 Iteration: 1900 / 4000 [ 47%]  (Warmup) \n#&gt; Chain 4 Iteration: 1800 / 4000 [ 45%]  (Warmup) \n#&gt; Chain 3 Iteration: 2000 / 4000 [ 50%]  (Warmup) \n#&gt; Chain 3 Iteration: 2001 / 4000 [ 50%]  (Sampling) \n#&gt; Chain 4 Iteration: 1900 / 4000 [ 47%]  (Warmup) \n#&gt; Chain 3 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#&gt; Chain 4 Iteration: 2000 / 4000 [ 50%]  (Warmup) \n#&gt; Chain 4 Iteration: 2001 / 4000 [ 50%]  (Sampling) \n#&gt; Chain 3 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#&gt; Chain 4 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#&gt; Chain 3 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#&gt; Chain 4 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#&gt; Chain 3 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#&gt; Chain 4 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#&gt; Chain 3 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#&gt; Chain 4 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#&gt; Chain 3 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#&gt; Chain 4 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#&gt; Chain 3 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#&gt; Chain 4 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#&gt; Chain 3 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#&gt; Chain 4 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#&gt; Chain 3 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#&gt; Chain 4 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#&gt; Chain 3 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#&gt; Chain 4 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#&gt; Chain 3 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#&gt; Chain 4 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#&gt; Chain 3 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#&gt; Chain 4 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#&gt; Chain 3 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#&gt; Chain 4 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#&gt; Chain 3 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#&gt; Chain 4 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#&gt; Chain 3 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#&gt; Chain 4 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#&gt; Chain 3 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#&gt; Chain 4 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#&gt; Chain 3 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#&gt; Chain 4 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#&gt; Chain 3 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#&gt; Chain 4 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#&gt; Chain 3 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#&gt; Chain 4 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#&gt; Chain 3 Iteration: 4000 / 4000 [100%]  (Sampling) \n#&gt; Chain 3 finished in 14.5 seconds.\n#&gt; Chain 4 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#&gt; Chain 4 Iteration: 4000 / 4000 [100%]  (Sampling) \n#&gt; Chain 4 finished in 14.8 seconds.\n#&gt; \n#&gt; All 4 chains finished successfully.\n#&gt; Mean chain execution time: 14.6 seconds.\n#&gt; Total execution time: 29.8 seconds.\n\nEsaminiamo le tracce per due parametri.\n\nfit_draws &lt;- fit$draws() # extract the posterior draws\nmcmc_trace(fit_draws, pars = c(\"beta[1]\"))\n\n\n\n\n\n\n\n\nmcmc_trace(fit_draws, pars = c(\"theta[1]\"))\n\n\n\n\n\n\n\nFocalizziamoci sulla stima dei parametri degli item.\n\nparameters &lt;- c(\n    \"beta[1]\", \"beta[2]\", \"beta[3]\", \"beta[4]\", \"beta[5]\",\n    \"beta[6]\", \"beta[7]\", \"beta[8]\", \"beta[9]\",\"beta[10]\",\n    \"beta[11]\", \"beta[12]\", \"beta[13]\", \"beta[14]\"\n)\n\nEsaminiamo la statistica rhat.\n\nrhats &lt;- rhat(fit_draws, pars = parameters)\nmcmc_rhat(rhats)\n\n\n\n\n\n\n\nEsaminiamo l’effect ratio:\n\neff_ratio &lt;- neff_ratio(fit, pars = parameters)\neff_ratio \n#&gt;  beta[1]  beta[2]  beta[3]  beta[4]  beta[5]  beta[6]  beta[7]  beta[8] \n#&gt;     1.13     1.19     1.16     1.10     1.36     1.16     1.18     1.08 \n#&gt;  beta[9] beta[10] beta[11] beta[12] beta[13] beta[14] \n#&gt;     1.30     1.21     1.26     1.23     1.39     1.31\n\n\nmcmc_neff(eff_ratio)\n\n\n\n\n\n\n\nEsaminiamo l’autocorrelazione.\n\nmcmc_acf(fit_draws, pars = parameters)\n\n\n\n\n\n\n\nOtteniamo le statistiche riassuntive delle distribuzioni a posteriori dei parametri degli item.\n\nfit$summary(\n    variables = parameters,\n    posterior::default_summary_measures(),\n    extra_quantiles = ~ posterior::quantile2(., probs = c(.0275, .975))\n)\n#&gt; # A tibble: 14 × 9\n#&gt;   variable    mean  median    sd   mad     q5    q95  q2.75  q97.5\n#&gt;   &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 beta[1]  -1.10   -1.10   0.129 0.126 -1.31  -0.892 -1.35  -0.849\n#&gt; 2 beta[2]  -1.24   -1.24   0.130 0.130 -1.46  -1.03  -1.50  -0.997\n#&gt; 3 beta[3]  -2.03   -2.02   0.158 0.156 -2.30  -1.77  -2.34  -1.73 \n#&gt; 4 beta[4]  -0.0478 -0.0489 0.120 0.121 -0.241  0.149 -0.274  0.191\n#&gt; 5 beta[5]   2.51    2.51   0.183 0.185  2.22   2.82   2.18   2.88 \n#&gt; 6 beta[6]  -1.24   -1.24   0.132 0.131 -1.46  -1.03  -1.50  -0.989\n#&gt; # ℹ 8 more rows\n\nI risultati ottenuti replicano quelli riportati da Debelak et al. (2022).",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>Stima</span>"
    ]
  },
  {
    "objectID": "chapters/irt/03_estimation.html#grandezza-del-campione",
    "href": "chapters/irt/03_estimation.html#grandezza-del-campione",
    "title": "69  Stima",
    "section": "\n69.6 Grandezza del Campione",
    "text": "69.6 Grandezza del Campione\nLa stima dei parametri degli item basata su un campione osservato di risposte è spesso definita come la calibrazione degli item. Generalmente, un campione di calibrazione più ampio consente una stima più accurata dei parametri degli item, sebbene altri fattori influenzino anch’essi l’accuratezza della stima. Ad esempio, la difficoltà di un item può essere stimata con maggiore precisione se l’item non è né troppo facile né troppo difficile per il campione di partecipanti al test. Pertanto, i fattori che influenzano l’accuratezza della stima includono l’allineamento e la forma delle distribuzioni dei parametri degli item e delle persone, il numero di item e la tecnica di stima utilizzata.\nDiverse pubblicazioni hanno affrontato la questione della dimensione del campione tipicamente necessaria per lavorare con il modello di Rasch e come questa sia influenzata da questi e altri fattori. Ad esempio, De Ayala (2009) fornisce la linea guida generale che un campione di calibrazione dovrebbe contenere almeno diverse centinaia di rispondenti e cita, tra le altre referenze, un articolo precedente di Wright (1977) che afferma che un campione di calibrazione di 500 sarebbe più che adeguato. De Ayala (2009) suggerisce anche che 250 o più rispondenti sono necessari per adattare un modello di Partial Credit. Poiché il modello di Partial Credit è una generalizzazione del modello di Rasch con più parametri degli item, ciò implica che la dimensione del campione suggerita di 250 dovrebbe essere sufficiente anche per adattare un modello di Rasch. Studi più recenti hanno indagato l’applicazione del modello di Rasch con dimensioni del campione di soli 100 rispondenti (ad esempio, Steinfeld & Robitzsch, 2021; Suárez-Falcón & Glas, 2003). Tali linee guida non devono essere interpretate come regole fisse, ma solo come indicazioni generali in quanto una dimensione del campione adeguata dipende dalle condizioni e dagli obiettivi dell’analisi.\nUn metodo più elaborato per determinare la dimensione del campione necessaria è l’analisi della potenza statistica. Qui, l’accuratezza della stima desiderata o il rischio di falsi positivi e falsi negativi devono essere formalizzati prima dell’analisi. La dimensione del campione necessaria viene quindi determinata in base a queste considerazioni.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>Stima</span>"
    ]
  },
  {
    "objectID": "chapters/irt/03_estimation.html#session-info",
    "href": "chapters/irt/03_estimation.html#session-info",
    "title": "69  Stima",
    "section": "\n69.7 Session Info",
    "text": "69.7 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats4    grid      stats     graphics  grDevices utils     datasets \n#&gt; [8] methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] effectsize_1.0.0    rsvg_2.6.1          rstan_2.32.6       \n#&gt;  [4] StanHeaders_2.32.10 posterior_1.6.1     cmdstanr_0.8.1     \n#&gt;  [7] ggmirt_0.1.0        TAM_4.2-21          CDM_8.2-6          \n#&gt; [10] mvtnorm_1.3-3       mirt_1.44.0         lattice_0.22-6     \n#&gt; [13] latex2exp_0.9.6     ggokabeito_0.1.0    see_0.10.0         \n#&gt; [16] MASS_7.3-65         viridis_0.6.5       viridisLite_0.4.2  \n#&gt; [19] ggpubr_0.6.0        ggExtra_0.10.1      gridExtra_2.3      \n#&gt; [22] patchwork_1.3.0     bayesplot_1.11.1    semTools_0.5-6     \n#&gt; [25] semPlot_1.1.6       lavaan_0.6-19       psych_2.4.12       \n#&gt; [28] scales_1.3.0        markdown_1.13       knitr_1.49         \n#&gt; [31] lubridate_1.9.4     forcats_1.0.0       stringr_1.5.1      \n#&gt; [34] dplyr_1.1.4         purrr_1.0.4         readr_2.1.5        \n#&gt; [37] tidyr_1.3.1         tibble_3.2.1        ggplot2_3.5.1      \n#&gt; [40] tidyverse_2.0.0     here_1.0.1         \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] splines_4.4.2        later_1.4.1          R.oo_1.27.0         \n#&gt;   [4] datawizard_1.0.0     XML_3.99-0.18        rpart_4.1.24        \n#&gt;   [7] lifecycle_1.0.4      Rdpack_2.6.2         rstatix_0.7.2       \n#&gt;  [10] rprojroot_2.0.4      processx_3.8.6       globals_0.16.3      \n#&gt;  [13] insight_1.0.2        rockchalk_1.8.157    backports_1.5.0     \n#&gt;  [16] magrittr_2.0.3       openxlsx_4.2.8       Hmisc_5.2-2         \n#&gt;  [19] rmarkdown_2.29       yaml_2.3.10          httpuv_1.6.15       \n#&gt;  [22] qgraph_1.9.8         zip_2.3.2            pkgbuild_1.4.6      \n#&gt;  [25] sessioninfo_1.2.3    pbapply_1.7-2        minqa_1.2.8         \n#&gt;  [28] multcomp_1.4-28      abind_1.4-8          audio_0.1-11        \n#&gt;  [31] quadprog_1.5-8       R.utils_2.13.0       tensorA_0.36.2.1    \n#&gt;  [34] nnet_7.3-20          TH.data_1.1-3        sandwich_3.1-1      \n#&gt;  [37] inline_0.3.21        listenv_0.9.1        testthat_3.2.3      \n#&gt;  [40] RPushbullet_0.3.4    vegan_2.6-10         arm_1.14-4          \n#&gt;  [43] parallelly_1.42.0    permute_0.9-7        codetools_0.2-20    \n#&gt;  [46] tidyselect_1.2.1     farver_2.1.2         lme4_1.1-36         \n#&gt;  [49] matrixStats_1.5.0    base64enc_0.1-3      jsonlite_1.9.0      \n#&gt;  [52] polycor_0.8-1        progressr_0.15.1     Formula_1.2-5       \n#&gt;  [55] survival_3.8-3       emmeans_1.10.7       tools_4.4.2         \n#&gt;  [58] snow_0.4-4           Rcpp_1.0.14          glue_1.8.0          \n#&gt;  [61] mnormt_2.1.1         admisc_0.37          xfun_0.51           \n#&gt;  [64] mgcv_1.9-1           distributional_0.5.0 loo_2.8.0           \n#&gt;  [67] withr_3.0.2          beepr_2.0            fastmap_1.2.0       \n#&gt;  [70] boot_1.3-31          digest_0.6.37        mi_1.1              \n#&gt;  [73] timechange_0.3.0     R6_2.6.1             mime_0.12           \n#&gt;  [76] estimability_1.5.1   colorspace_2.1-1     gtools_3.9.5        \n#&gt;  [79] jpeg_0.1-10          R.methodsS3_1.8.2    utf8_1.2.4          \n#&gt;  [82] generics_0.1.3       data.table_1.17.0    corpcor_1.6.10      \n#&gt;  [85] SimDesign_2.18       htmlwidgets_1.6.4    parameters_0.24.1   \n#&gt;  [88] pkgconfig_2.0.3      sem_3.1-16           gtable_0.3.6        \n#&gt;  [91] brio_1.1.5           htmltools_0.5.8.1    carData_3.0-5       \n#&gt;  [94] png_0.1-8            reformulas_0.4.0     rstudioapi_0.17.1   \n#&gt;  [97] tzdb_0.4.0           reshape2_1.4.4       coda_0.19-4.1       \n#&gt; [100] checkmate_2.3.2      nlme_3.1-167         curl_6.2.1          \n#&gt; [103] nloptr_2.1.1         zoo_1.8-13           parallel_4.4.2      \n#&gt; [106] miniUI_0.1.1.1       foreign_0.8-88       pillar_1.10.1       \n#&gt; [109] vctrs_0.6.5          promises_1.3.2       car_3.1-3           \n#&gt; [112] OpenMx_2.21.13       xtable_1.8-4         Deriv_4.1.6         \n#&gt; [115] cluster_2.1.8        dcurver_0.9.2        GPArotation_2024.3-1\n#&gt; [118] htmlTable_2.4.3      evaluate_1.0.3       pbivnorm_0.6.0      \n#&gt; [121] cli_3.6.4            kutils_1.73          compiler_4.4.2      \n#&gt; [124] rlang_1.1.5          future.apply_1.11.3  ggsignif_0.6.4      \n#&gt; [127] labeling_0.4.3       fdrtool_1.2.18       ps_1.9.0            \n#&gt; [130] plyr_1.8.9           stringi_1.8.4        QuickJSR_1.6.0      \n#&gt; [133] munsell_0.5.1        lisrelToR_0.3        bayestestR_0.15.2   \n#&gt; [136] V8_6.0.1             pacman_0.5.1         Matrix_1.7-2        \n#&gt; [139] hms_1.1.3            glasso_1.11          future_1.34.0       \n#&gt; [142] shiny_1.10.0         rbibutils_2.3        igraph_2.1.4        \n#&gt; [145] broom_1.0.7          RcppParallel_5.1.10\n\n\n\n\n\nDebelak, R., Strobl, C., & Zeigenfuse, M. D. (2022). An introduction to the rasch model with examples in r. CRC Press.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>Stima</span>"
    ]
  },
  {
    "objectID": "chapters/irt/04_1pl_2pl_3pl.html",
    "href": "chapters/irt/04_1pl_2pl_3pl.html",
    "title": "70  Modelli 1PL, 2PL e 3PL",
    "section": "",
    "text": "70.1 Introduzione\nAll’interno della teoria della risposta agli item (IRT), il modello di Rasch rappresenta l’approccio più restrittivo, poiché impone vincoli stringenti sulle relazioni tra abilità delle persone e difficoltà degli item. Questi vincoli garantiscono semplicità e proprietà matematiche utili, ma limitano la flessibilità del modello nel rappresentare dati complessi.\nProgressivamente, tali restrizioni possono essere allentate per definire modelli più flessibili:\nQuesta progressione da Rasch a 1PL, 2PL e 3PL permette una maggiore adattabilità del modello IRT, bilanciando semplicità e flessibilità a seconda delle esigenze specifiche dei dati e dell’analisi.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>Modelli 1PL, 2PL e 3PL</span>"
    ]
  },
  {
    "objectID": "chapters/irt/04_1pl_2pl_3pl.html#introduzione",
    "href": "chapters/irt/04_1pl_2pl_3pl.html#introduzione",
    "title": "70  Modelli 1PL, 2PL e 3PL",
    "section": "",
    "text": "Il modello 1PL (One-Parameter Logistic), che conserva l’assunzione di uguale discriminazione tra gli item ma non richiede tutte le proprietà rigorose del modello di Rasch.\n\nIl modello 2PL (Two-Parameter Logistic), che introduce un parametro aggiuntivo per descrivere la capacità discriminante degli item, consentendo una rappresentazione più accurata delle risposte.\n\nIl modello 3PL (Three-Parameter Logistic), che aggiunge un terzo parametro per tenere conto della probabilità di risposta corretta casuale (detta anche “guessing”).",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>Modelli 1PL, 2PL e 3PL</span>"
    ]
  },
  {
    "objectID": "chapters/irt/04_1pl_2pl_3pl.html#un-esempio-pratico",
    "href": "chapters/irt/04_1pl_2pl_3pl.html#un-esempio-pratico",
    "title": "70  Modelli 1PL, 2PL e 3PL",
    "section": "\n70.2 Un Esempio Pratico",
    "text": "70.2 Un Esempio Pratico\nIn questo capitolo, utilizzeremo nuovamente i dati che abbiamo esaminato in precedenza nel Capitolo 69.\n\ndata(data.fims.Aus.Jpn.scored, package = \"TAM\")\nfims &lt;- data.fims.Aus.Jpn.scored\n\nIl data set include 400 partecipanti. Per facilitare la manipolazione dei dati, cambiamo il nome delle colonne.\n\nresponses &lt;- fims[1:400, 2:15]\ncolnames(responses) &lt;- gsub(\"M1PTI\", \"I\", colnames(responses))\nglimpse(responses)\n#&gt; Rows: 400\n#&gt; Columns: 14\n#&gt; $ I1  &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,…\n#&gt; $ I2  &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,…\n#&gt; $ I3  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,…\n#&gt; $ I6  &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,…\n#&gt; $ I7  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n#&gt; $ I11 &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,…\n#&gt; $ I12 &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n#&gt; $ I14 &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,…\n#&gt; $ I17 &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,…\n#&gt; $ I18 &lt;dbl&gt; 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,…\n#&gt; $ I19 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n#&gt; $ I21 &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,…\n#&gt; $ I22 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,…\n#&gt; $ I23 &lt;dbl&gt; 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,…\n\nDefiniamo il fattore gender:\n\ngender &lt;- as.factor(fims$SEX[1:400])\nlevels(gender) &lt;- c(\"male\", \"female\")\n\ngender |&gt; table()\n#&gt; gender\n#&gt;   male female \n#&gt;    246    154",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>Modelli 1PL, 2PL e 3PL</span>"
    ]
  },
  {
    "objectID": "chapters/irt/04_1pl_2pl_3pl.html#modello-1pl",
    "href": "chapters/irt/04_1pl_2pl_3pl.html#modello-1pl",
    "title": "70  Modelli 1PL, 2PL e 3PL",
    "section": "\n70.3 Modello 1PL",
    "text": "70.3 Modello 1PL\nIl modello ad un parametro logistico (1PL) descrive la probabilità che un rispondente con un certo livello di abilità dia una risposta corretta a un item specifico. La formula del modello è:\n\\[\nP(X_i = 1 \\mid \\theta_v, \\alpha, \\delta_i) = \\frac{\\exp(\\alpha(\\theta_v - \\delta_i))}{1 + \\exp(\\alpha(\\theta_v - \\delta_i))} = \\frac{1}{1 + \\exp(-\\alpha(\\theta_v - \\delta_i))}, \\tag{1}\n\\]\ndove:\n\n\n\\(\\theta_v\\) è il livello di abilità del rispondente \\(v\\),\n\n\\(\\delta_i\\) è il parametro di difficoltà dell’item \\(i\\),\n\n\\(\\alpha\\) è il parametro di discriminazione dell’item, fissato e uguale per tutti gli item nel modello 1PL.\n\nL’assunzione fondamentale del modello 1PL è che \\(\\alpha\\) sia costante per tutti gli item, indicando che tutti gli item hanno la stessa capacità di discriminazione tra rispondenti con abilità diverse.\n\n70.3.1 Il Ruolo del Parametro \\(\\alpha\\)\n\nIl parametro \\(\\alpha\\) definisce la pendenza della curva caratteristica dell’item (ICC). Maggiore è \\(\\alpha\\), più ripida è la curva ICC, e maggiore è la capacità dell’item di discriminare tra rispondenti con abilità vicine alla difficoltà dell’item (\\(\\delta_i\\)).\n\n\n\\(\\alpha = 0\\): L’item non discrimina affatto; la probabilità di risposta corretta è costante e indipendente dal livello di abilità.\n\n\\(\\alpha &gt; 0\\): L’item discrimina, e la sua capacità discriminatoria cresce con l’aumento di \\(\\alpha\\).\n\n70.3.2 Esempio Pratico\nConsideriamo tre item (\\(i_1\\), \\(i_2\\), \\(i_3\\)) con la stessa difficoltà \\(\\delta = 0\\) e tre valori di discriminazione: \\(\\alpha_1 = 0.0\\), \\(\\alpha_2 = 1.0\\) e \\(\\alpha_3 = 2.0\\). Esaminiamo due rispondenti con livelli di abilità:\n\nRispondente A con \\(\\theta_A = -1\\),\nRispondente B con \\(\\theta_B = 1\\).\n\nItem con $= 0.0\nCon \\(\\alpha = 0\\), la probabilità di risposta corretta è costante, pari a 0.5 per tutti i livelli di abilità:\n\\[\nP(X_i = 1 \\mid \\theta, \\delta) = 0.5.\n\\]\nQuesto item non discrimina tra rispondenti con abilità diverse e non aggiunge alcuna informazione utile.\nItem con \\(\\alpha\\) = 1.0\nCon \\(\\alpha = 1.0\\), la probabilità di risposta corretta dipende dal livello di abilità:\n\\[\nP(X_i = 1 \\mid \\theta, \\delta) = \\frac{1}{1 + \\exp(-(\\theta - \\delta))}.\n\\]\n\nPer \\(\\theta_A = -1\\): \\(P(X_i = 1) \\approx 0.269\\),\nPer \\(\\theta_B = 1\\): \\(P(X_i = 1) \\approx 0.731\\).\n\nLa curva ICC è moderatamente ripida e l’item discrimina tra rispondenti con abilità diverse.\nItem con \\(\\alpha\\) = 2.0\nCon \\(\\alpha = 2.0\\), l’ICC diventa più ripida:\n\\[\nP(X_i = 1 \\mid \\theta, \\delta) = \\frac{1}{1 + \\exp(-2(\\theta - \\delta))}.\n\\]\n\nPer \\(\\theta_A = -1\\): \\(P(X_i = 1) \\approx 0.119\\),\nPer \\(\\theta_B = 1\\): \\(P(X_i = 1) \\approx 0.881\\).\n\nLa maggiore ripidità riflette una capacità discriminatoria più alta, permettendo di distinguere con maggiore precisione i rispondenti in base alle loro abilità.\nIn conclusione, nel modello 1PL, il parametro \\(\\alpha\\) controlla la capacità degli item di discriminare tra rispondenti con abilità diverse. Un aumento di \\(\\alpha\\) rende la curva ICC più ripida, migliorando la discriminazione, mentre un \\(\\alpha\\) più basso rende la curva piatta e riduce la capacità informativa dell’item. Tuttavia, nel 1PL, l’assunzione che \\(\\alpha\\) sia costante per tutti gli item rappresenta un limite rispetto ai modelli più complessi, come il 2PL, che permettono a ogni item di avere una discriminazione diversa.\n\n70.3.3 Modello di Rasch e Modello 1PL: Confronto e Differenze\nIl modello di Rasch e il modello 1PL (One-Parameter Logistic) sono due approcci alla misurazione che condividono una struttura matematica simile. Entrambi utilizzano un parametro di discriminazione (α) costante per tutti gli item, pur permettendo variazioni nei parametri di difficoltà (δᵢ).\nLa differenza tecnica principale sta nel valore del parametro α:\n\nNel modello di Rasch, α è sempre fissato a 1.0\nNel modello 1PL, α può assumere qualsiasi valore costante, anche diverso da 1.0\n\nMatematicamente, i due modelli sono equivalenti: è possibile convertire i parametri da un modello all’altro attraverso una semplice riscalatura, moltiplicando o dividendo θᵥ e δᵢ per α, mantenendo invariate le probabilità di risposta corretta.\nNonostante la loro equivalenza matematica, i due modelli si distinguono per filosofia e obiettivi:\nIl modello 1PL si concentra sull’adattamento ai dati empirici:\n\nMira a descrivere al meglio i dati osservati\nOffre flessibilità nella scelta del parametro α\nSi adatta ai dati esistenti\n\nIl modello di Rasch privilegia la misurazione oggettiva:\n\nPone l’enfasi sulla costruzione di misure stabili e generalizzabili\nConsidera il modello come uno standard di riferimento\nRichiede che i dati si conformino al modello, non viceversa\nSi pone come strumento per sviluppare misurazioni valide e oggettive\n\nIn sintesi, mentre il modello 1PL è più orientato alla descrizione statistica dei dati, il modello di Rasch si propone come standard per la costruzione di strumenti di misurazione oggettivi e universalmente applicabili.\n\nEsercizio 70.1 In \\(\\mathsf{R}\\), il modello di Rasch si implementa nel modo seguente:\n\nmirt_rm &lt;- mirt(responses, 1, \"Rasch\", verbose = FALSE)\n\nIl modello 1PL si implementa nel modo seguente:\n\nmirt_1pl &lt;- mirt(responses, 1, \"1PL\", verbose = FALSE)\n\nConfrontiamo i due modelli:\n\nanova(mirt_rm, mirt_1pl)\n#&gt;           AIC SABIC   HQ  BIC logLik     X2 df   p\n#&gt; mirt_rm  5663  5675 5687 5723  -2816              \n#&gt; mirt_1pl 5662  5673 5684 5718  -2817 -0.705 -1 NaN\n\nI modelli mirt_rm e mirt_1pl sono praticamente equivalenti in termini di adattamento ai dati. Il modello mirt_1pl mostra lievi miglioramenti nei criteri di informazione (AIC e BIC), ma la differenza è minima.\n\n\n70.3.4 Modello 2PL\nIl modello 2PL (Modello IRT a due parametri) rappresenta un’estensione del modello 1PL che consente una maggiore flessibilità, poiché permette alle Curve Caratteristiche degli Item (ICC) di avere pendenze diverse. Questo significa che, a differenza del modello 1PL e del modello di Rasch, le ICC degli item non sono necessariamente parallele. Nel modello 2PL, ogni item è descritto da due parametri fondamentali:\n\n\nParametro di difficoltà (\\(b\\)): Indica il livello di abilità (\\(\\theta\\)) a cui la probabilità di risposta corretta è del 50%. Determina il posizionamento della curva ICC lungo l’asse delle abilità.\n\nParametro di discriminazione (\\(a\\)): Regola la pendenza della curva ICC, rappresentando la capacità dell’item di distinguere tra rispondenti con abilità simili. Un valore più alto di \\(a\\) indica una maggiore sensibilità dell’item alle variazioni di abilità.\n\nLa formula generale per le ICC nel modello 2PL è:\n\\[\nP(X_i = 1 \\mid \\theta, a_i, b_i) = \\frac{1}{1 + \\exp(-a_i (\\theta - b_i))},\n\\]\ndove:\n\n\n\\(\\theta\\) rappresenta l’abilità del rispondente,\n\n\\(a_i\\) è il parametro di discriminazione per l’item \\(i\\),\n\n\\(b_i\\) è il parametro di difficoltà per l’item \\(i\\).\n\n70.3.5 Implementazione in R con il Pacchetto mirt\n\nUtilizziamo il pacchetto mirt per adattare il modello 2PL ai dati. Il comando mirt() permette di stimare i parametri specificando il modello 2PL:\n\nmirt_2pl &lt;- mirt(responses, 1, \"2PL\")\n#&gt; \nIteration: 1, Log-Lik: -2818.855, Max-Change: 0.66304\nIteration: 2, Log-Lik: -2772.597, Max-Change: 0.26013\nIteration: 3, Log-Lik: -2762.517, Max-Change: 0.12884\nIteration: 4, Log-Lik: -2760.353, Max-Change: 0.06587\nIteration: 5, Log-Lik: -2759.824, Max-Change: 0.03961\nIteration: 6, Log-Lik: -2759.675, Max-Change: 0.02169\nIteration: 7, Log-Lik: -2759.622, Max-Change: 0.01040\nIteration: 8, Log-Lik: -2759.609, Max-Change: 0.00642\nIteration: 9, Log-Lik: -2759.605, Max-Change: 0.00396\nIteration: 10, Log-Lik: -2759.602, Max-Change: 0.00160\nIteration: 11, Log-Lik: -2759.602, Max-Change: 0.00089\nIteration: 12, Log-Lik: -2759.602, Max-Change: 0.00064\nIteration: 13, Log-Lik: -2759.601, Max-Change: 0.00020\nIteration: 14, Log-Lik: -2759.601, Max-Change: 0.00009\n\nPer analizzare graficamente le Curve Caratteristiche degli Item, usiamo la funzione plot():\n\nplot(mirt_2pl, type = \"trace\")\n\n\n\n\n\n\n\nSe desideriamo visualizzare tutte le ICC in un unico grafico, senza separarle per item, aggiungiamo l’opzione facet_items = FALSE:\n\nplot(mirt_2pl, type = \"trace\", facet_items = FALSE)\n\n\n\n\n\n\n\nLa funzione coef() consente di ottenere le stime dei parametri degli item:\n\ncoef(mirt_2pl, IRTpars = TRUE, simplify = TRUE)\n#&gt; $items\n#&gt;          a      b g u\n#&gt; I1   1.147 -1.022 0 1\n#&gt; I2   1.769 -0.911 0 1\n#&gt; I3   1.372 -1.680 0 1\n#&gt; I6   1.479 -0.043 0 1\n#&gt; I7   1.071  2.441 0 1\n#&gt; I11  1.594 -0.957 0 1\n#&gt; I12  0.703  1.079 0 1\n#&gt; I14  0.771 -0.612 0 1\n#&gt; I17  0.707  1.758 0 1\n#&gt; I18  0.750 -0.502 0 1\n#&gt; I19  1.831  1.459 0 1\n#&gt; I21 -0.214 -7.076 0 1\n#&gt; I22  0.277  7.657 0 1\n#&gt; I23  1.521 -1.503 0 1\n#&gt; \n#&gt; $means\n#&gt; F1 \n#&gt;  0 \n#&gt; \n#&gt; $cov\n#&gt;    F1\n#&gt; F1  1\n\nQueste stime includono:\n\n\n\\(a\\), parametro di discriminazione,\n\n\\(b\\), parametro di difficoltà.\n\n70.3.6 Confronto tra Modello 1PL e Modello 2PL\nPer valutare quale modello si adatta meglio ai dati, confrontiamo il modello 1PL (discriminazione fissa) con il modello 2PL (discriminazione variabile) utilizzando la funzione anova():\n\nanova(mirt_rm, mirt_2pl)\n#&gt;           AIC SABIC   HQ  BIC logLik     X2 df p\n#&gt; mirt_rm  5663  5675 5687 5723  -2816            \n#&gt; mirt_2pl 5575  5598 5619 5687  -2760 113.77 13 0\n\nInterpretazione dei Risultati\n\n\nCriteri di Informazione (AIC e BIC): Il modello 2PL tipicamente mostra valori di AIC e BIC più bassi rispetto al modello 1PL, indicando un miglior adattamento ai dati.\n\nLog-Likelihood: Il modello 2PL presenta un log-likelihood superiore rispetto al modello 1PL, a indicare una maggiore probabilità di osservare i dati sotto il modello 2PL.\n\nTest di \\(X^2\\): Se il p-value associato è significativo (\\(p &lt; 0.05\\)), ciò suggerisce che il modello 2PL spiega significativamente più variazione rispetto al modello 1PL.\n\n70.3.7 Differenze Chiave tra Modello 1PL e Modello 2PL\n\n\n\n\n\n\n\nCaratteristica\nModello 1PL\nModello 2PL\n\n\n\nParametro di discriminazione (\\(a\\))\nFisso per tutti gli item (\\(a = \\alpha\\) costante)\nVariabile tra gli item (\\(a_i\\) specifico)\n\n\nCurva ICC\nTutte le curve ICC sono parallele\nLe curve ICC possono avere pendenze diverse\n\n\nAdattamento ai dati\nMeno flessibile, buono per dati uniformi\nPiù flessibile, cattura differenze di discriminazione\n\n\n\nIn conclusione, il modello 2PL è particolarmente utile quando gli item differiscono nella loro capacità di discriminare tra rispondenti con abilità simili. Questo lo rende una scelta preferibile rispetto al modello 1PL in situazioni in cui gli item non sono omogenei in termini di discriminazione. Tuttavia, la maggiore flessibilità del modello 2PL comporta una maggiore complessità e richiede un dataset con sufficiente variabilità per stimare accuratamente i parametri \\(a_i\\) e \\(b_i\\).",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>Modelli 1PL, 2PL e 3PL</span>"
    ]
  },
  {
    "objectID": "chapters/irt/04_1pl_2pl_3pl.html#modello-3pl",
    "href": "chapters/irt/04_1pl_2pl_3pl.html#modello-3pl",
    "title": "70  Modelli 1PL, 2PL e 3PL",
    "section": "\n70.4 Modello 3PL",
    "text": "70.4 Modello 3PL\nIl modello IRT a tre parametri (3PL) è un’estensione del modello 2PL che aggiunge un terzo parametro, il guessing (\\(g\\)), per tenere conto della probabilità di rispondere correttamente a un item semplicemente per caso. Questo parametro è particolarmente utile nei test a scelta multipla, dove i rispondenti con abilità molto bassa possono comunque selezionare la risposta corretta in modo casuale.\nLa probabilità di risposta corretta nel modello 3PL è espressa come:\n\\[\nP(X_i = 1 \\mid \\theta, a_i, b_i, g_i) = g_i + (1 - g_i) \\cdot \\frac{1}{1 + \\exp(-a_i (\\theta - b_i))},\n\\]\ndove:\n\n\n\\(\\theta\\): abilità latente del rispondente,\n\n\\(a_i\\): parametro di discriminazione dell’item \\(i\\) (controlla la pendenza della curva ICC),\n\n\\(b_i\\): parametro di difficoltà dell’item \\(i\\) (indica il livello di abilità richiesto per una probabilità del 50% di risposta corretta, escludendo il guessing),\n\n\\(g_i\\): parametro di guessing (probabilità minima di rispondere correttamente a un item, anche per rispondenti con abilità molto bassa).\n\n\n70.4.1 Caratteristiche del Modello 3PL\n\n\nParametro di Guessing (\\(g\\)):\n\nIntroduce un asintoto inferiore maggiore di zero nella curva caratteristica dell’item (ICC).\nAd esempio, un valore \\(g_i = 0.25\\) indica che, anche per abilità molto basse (\\(\\theta \\to -\\infty\\)), la probabilità di rispondere correttamente all’item è almeno del 25%. Questo valore è tipico per test a scelta multipla con quattro opzioni, dove c’è il 25% di probabilità di indovinare.\n\n\n\nRelazione con il Modello 2PL:\n\nIl modello 3PL generalizza il modello 2PL aggiungendo il parametro \\(g\\), che aumenta la flessibilità per rappresentare meglio il comportamento degli item in situazioni reali.\nMentre nel modello 2PL la probabilità di risposta corretta può scendere a zero per abilità molto basse, nel modello 3PL la probabilità minima è definita da \\(g\\).\n\n\n\nCurve Caratteristiche degli Item (ICC):\n\nLa presenza del parametro \\(g\\) modifica la forma della curva ICC, che non tocca mai lo zero ma si avvicina asintoticamente al valore di \\(g\\) per \\(\\theta\\) molto basso.\n\n\n\nComplessità del Modello:\n\nL’aggiunta del parametro \\(g\\) rende il modello più complesso rispetto al 2PL, aumentando il numero di parametri da stimare.\nPer ottenere stime affidabili, è necessario disporre di un dataset con un numero sufficiente di item e rispondenti.\n\n\n\n\nEsercizio 70.2 Utilizziamo il pacchetto mirt per stimare i parametri del modello 3PL:\n\nmirt_3pl &lt;- mirt(responses, 1, \"3PL\")\n\nLe curve ICC possono essere visualizzate con il comando:\n\nplot(mirt_3pl, type = \"trace\", facet_items = TRUE)\n\n\n\n\n\n\n\nUtilizziamo la funzione coef() per ottenere le stime dei parametri degli item (\\(a\\), \\(b\\), \\(g\\)):\n\ncoef(mirt_3pl, IRTpars = TRUE, simplify = TRUE)\n#&gt; $items\n#&gt;          a      b     g u\n#&gt; I1   1.410 -0.493 0.244 1\n#&gt; I2   2.667 -0.404 0.291 1\n#&gt; I3   1.445 -1.633 0.000 1\n#&gt; I6   1.514  0.009 0.019 1\n#&gt; I7   1.163  2.297 0.000 1\n#&gt; I11  1.487 -0.984 0.000 1\n#&gt; I12  2.131  1.275 0.207 1\n#&gt; I14  0.805 -0.586 0.000 1\n#&gt; I17  3.076  1.320 0.138 1\n#&gt; I18  0.724 -0.510 0.000 1\n#&gt; I19  1.844  1.449 0.000 1\n#&gt; I21 -5.264 -2.271 0.167 1\n#&gt; I22  9.179  1.990 0.090 1\n#&gt; I23  1.625 -1.452 0.000 1\n#&gt; \n#&gt; $means\n#&gt; F1 \n#&gt;  0 \n#&gt; \n#&gt; $cov\n#&gt;    F1\n#&gt; F1  1\n\nConfronto tra modelli 2PL e 3PL\n\nanova(mirt_2pl, mirt_3pl)\n#&gt;           AIC SABIC   HQ  BIC logLik     X2 df p\n#&gt; mirt_2pl 5575  5598 5619 5687  -2760            \n#&gt; mirt_3pl 5564  5599 5631 5732  -2740 38.784 14 0\n\n\nIl modello 3PL presenta un AIC inferiore rispetto al modello 2PL, suggerendo un miglior adattamento ai dati.\nTuttavia, il BIC penalizza maggiormente la complessità del modello, favorendo leggermente il modello 2PL.\nLa significatività del test \\(X^2\\) (\\(p = 0.0004\\)) indica che il modello 3PL offre un miglioramento significativo rispetto al modello 2PL.\n\nValutazione della bontà dell’adattamento\nPer verificare se il modello 3PL rappresenta adeguatamente i dati, utilizziamo la statistica \\(M2\\):\n\nM2(mirt_3pl)\n#&gt;         M2 df     p  RMSEA RMSEA_5 RMSEA_95  SRMSR   TLI   CFI\n#&gt; stats 76.1 63 0.125 0.0228       0   0.0394 0.0453 0.975 0.983\n\n\nIl valore \\(p = 0.125\\) indica che il modello 3PL non può essere rifiutato come rappresentazione adeguata dei dati.\nIl RMSEA inferiore a 0.05 (limite superiore: 0.039) suggerisce un buon adattamento.\n\nAdattamento degli item\nIl comando itemfit() calcola le statistiche di adattamento (fit) per ciascun item del modello 3PL, fornendo i valori di infit e outfit insieme ai relativi z-score che indicano quanto questi valori si discostano da quelli attesi secondo una distribuzione normale standardizzata.\n\nL’infit (Information-weighted fit) si concentra principalmente sui rispondenti con un livello di abilità simile alla difficoltà dell’item, ed è quindi particolarmente sensibile alle discrepanze nella “zona di interesse” dell’item, dove la probabilità di risposta corretta si aggira intorno al 50%.\nL’outfit (Outlier-sensitive fit) invece considera tutti i rispondenti, inclusi quelli con abilità molto diverse dalla difficoltà dell’item, risultando più sensibile a risposte inaspettate o estreme.\n\nPer entrambe le statistiche, valori compresi tra 0.7 e 1.3 indicano un buon adattamento dell’item al modello. Valori inferiori a 0.7 suggeriscono che l’item è troppo prevedibile o ridondante, mentre valori superiori a 1.3 indicano la presenza di risposte inaspettate. Per quanto riguarda gli z-score, valori con modulo inferiore a 2 sono considerati accettabili, mentre valori superiori potrebbero indicare problemi di adattamento.\n\nitemfit(mirt_3pl, \"infit\", method = \"ML\") # infit and outfit stats\n#&gt;    item outfit z.outfit infit z.infit\n#&gt; 1    I1  1.005    0.084 0.983  -0.273\n#&gt; 2    I2  0.865   -0.367 0.873  -2.016\n#&gt; 3    I3  0.876   -0.404 0.909  -0.940\n#&gt; 4    I6  0.981   -0.145 0.916  -1.503\n#&gt; 5    I7  0.783   -0.797 0.896  -0.899\n#&gt; 6   I11  1.462    2.383 0.886  -1.567\n#&gt; 7   I12  0.945   -0.929 0.942  -1.157\n#&gt; 8   I14  0.967   -0.565 1.009   0.223\n#&gt; 9   I17  0.784   -2.457 0.807  -2.709\n#&gt; 10  I18  1.025    0.539 1.013   0.349\n#&gt; 11  I19  0.590   -1.393 0.816  -2.165\n#&gt; 12  I21  0.915   -0.983 0.917  -0.963\n#&gt; 13  I22  0.860   -0.978 0.844  -1.160\n#&gt; 14  I23  0.778   -0.736 0.872  -1.370\n\nDall’analisi dei risultati emerge che la maggior parte degli item mostra un buon adattamento al modello. In particolare, l’item I1 presenta valori ottimali sia per outfit (1.005) che per infit (0.983), con z-score molto contenuti (0.084 e -0.273 rispettivamente). Anche gli item I3, I6, I14, I18 e I21 mostrano valori di adattamento soddisfacenti, rientrando negli intervalli di accettabilità sia per le statistiche di fit che per gli z-score.\nTuttavia, alcuni item presentano aspetti critici che meritano attenzione. L’item I11 mostra un outfit elevato (1.462) con uno z-score significativo (2.383), suggerendo la presenza di risposte anomale da parte di soggetti con livelli di abilità distanti dalla difficoltà dell’item. L’item I17, pur avendo valori di fit accettabili (outfit = 0.784, infit = 0.807), presenta uno z-score problematico per l’infit (-2.709), indicando possibili discrepanze significative per i rispondenti con abilità vicine alla difficoltà dell’item.\nUn caso particolare è rappresentato dall’item I19, che mostra un outfit inferiore alla soglia minima (0.590) e uno z-score dell’infit significativo (-2.165). Questi valori potrebbero indicare che l’item è troppo prevedibile o eccessivamente facile rispetto al livello atteso dal modello.\nNel complesso, sebbene la maggior parte degli item mostri un adattamento soddisfacente, potrebbe essere opportuno rivedere gli item I11, I17 e I19 per migliorare la qualità complessiva dello strumento di misura.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>Modelli 1PL, 2PL e 3PL</span>"
    ]
  },
  {
    "objectID": "chapters/irt/04_1pl_2pl_3pl.html#invarianza-di-gruppo-nella-item-response-theory",
    "href": "chapters/irt/04_1pl_2pl_3pl.html#invarianza-di-gruppo-nella-item-response-theory",
    "title": "70  Modelli 1PL, 2PL e 3PL",
    "section": "\n70.5 Invarianza di Gruppo nella Item Response Theory",
    "text": "70.5 Invarianza di Gruppo nella Item Response Theory\nL’invarianza di gruppo dei parametri degli item rappresenta una delle caratteristiche più importanti della IRT. Questo principio afferma che le proprietà misurate di un item - come la sua difficoltà, discriminazione e probabilità di indovinare la risposta corretta - sono caratteristiche intrinseche dell’item stesso e rimangono stabili indipendentemente dalla popolazione di riferimento.\nPer comprendere meglio questo concetto, consideriamo un esempio concreto. Immaginiamo di somministrare lo stesso test a due gruppi distinti di esaminandi:\n\nil primo gruppo è composto da individui con abilità relativamente bassa, con punteggi che variano tra -3 e -1 sulla scala di abilità (con una media di -2);\nil secondo gruppo invece include individui con abilità più elevata, con punteggi tra +1 e +3 (media +2).\n\nQuando analizziamo le risposte utilizzando il metodo della massima verosimiglianza, osserviamo un fenomeno notevole: per ogni item, otteniamo gli stessi parametri indipendentemente dal gruppo analizzato. Per esempio, se per un determinato item otteniamo un parametro di discriminazione a = 1.27 e un parametro di difficoltà b = 0.39 analizzando l’intero campione, ritroveremo sostanzialmente gli stessi valori anche analizzando separatamente il gruppo con abilità bassa o quello con abilità alta.\nQuesto risultato ha implicazioni pratiche molto importanti. Significa che:\n\nle caratteristiche dell’item rimangono stabili anche quando il test viene somministrato a popolazioni diverse;\npossiamo confrontare in modo valido le prestazioni di gruppi diversi sullo stesso item;\nle stime dei parametri dell’item sono robuste e generalizzabili;\nla calibrazione degli item può essere effettuata su un campione e poi applicata con fiducia a popolazioni diverse.\n\nL’invarianza di gruppo rappresenta quindi una proprietà fondamentale che distingue i modelli IRT dai modelli classici della teoria dei test, permettendo confronti più equi e interpretazioni più affidabili dei risultati dei test tra diverse popolazioni.\nQuesta proprietà è particolarmente utile in contesti pratici, come quando si devono confrontare gruppi culturali diversi, classi scolastiche di diverso livello, o quando si vuole verificare se un test funziona allo stesso modo per popolazioni diverse. L’invarianza garantisce che le differenze osservate riflettano reali differenze nelle abilità misurate, piuttosto che artefatti dovuti alle caratteristiche del campione utilizzato per la calibrazione.\n\nEsercizio 70.3 Questo esercizio utilizza una simulazione in R per dimostrare visivamente il principio di invarianza dei parametri degli item nella IRT. La funzione groupinv() simula le risposte di due gruppi distinti di esaminandi e visualizza le loro curve caratteristiche dell’item (ICC).\n\ngroupinv &lt;- function(mdl, t1l, t1u, t2l, t2u) {\n    if (missing(t1l)) t1l &lt;- -3\n    if (missing(t1u)) t1u &lt;- -1\n    if (missing(t2l)) t2l &lt;- 1\n    if (missing(t2u)) t2u &lt;- 3\n    theta &lt;- seq(-3, 3, .1875)\n    f &lt;- rep(21, length(theta))\n    wb &lt;- round(runif(1, -3, 3), 2)\n    wa &lt;- round(runif(1, 0.2, 2.8), 2)\n    wc &lt;- round(runif(1, 0, .35), 2)\n    if (mdl == 1 | mdl == 2) {\n        wc &lt;- 0\n    }\n    if (mdl == 1) {\n        wa &lt;- 1\n    }\n    for (g in 1:length(theta)) {\n        P &lt;- wc + (1 - wc) / (1 + exp(-wa * (theta - wb)))\n    }\n    p &lt;- rbinom(length(theta), f, P) / f\n    lowerg1 &lt;- 0\n    for (g in 1:length(theta)) {\n        if (theta[g] &lt;= t1l) {\n            lowerg1 &lt;- lowerg1 + 1\n        }\n    }\n    upperg1 &lt;- 0\n    for (g in 1:length(theta)) {\n        if (theta[g] &lt;= t1u) {\n            upperg1 &lt;- upperg1 + 1\n        }\n    }\n    theta1 &lt;- theta[lowerg1:upperg1]\n    p1 &lt;- p[lowerg1:upperg1]\n    lowerg2 &lt;- 0\n    for (g in 1:length(theta)) {\n        if (theta[g] &lt;= t2l) {\n            lowerg2 &lt;- lowerg2 + 1\n        }\n    }\n    upperg2 &lt;- 0\n    for (g in 1:length(theta)) {\n        if (theta[g] &lt;= t2u) {\n            upperg2 &lt;- upperg2 + 1\n        }\n    }\n    theta2 &lt;- theta[lowerg2:upperg2]\n    p2 &lt;- p[lowerg2:upperg2]\n    theta12 &lt;- c(theta1, theta2)\n    p12 &lt;- c(p1, p2)\n    par(lab = c(7, 5, 3))\n    plot(theta12, p12,\n        xlim = c(-3, 3), ylim = c(0, 1),\n        xlab = \"Ability\", ylab = \"Probability of Correct Response\"\n    )\n    if (mdl == 1) {\n        maintext &lt;- paste(\"Pooled Groups\", \"\\n\", \"b=\", wb)\n    }\n    if (mdl == 2) {\n        maintext &lt;- paste(\"Pooled Groups\", \"\\n\", \"a=\", wa, \"b=\", wb)\n    }\n    if (mdl == 3) {\n        maintext &lt;- paste(\n            \"Pooled Groups\", \"\\n\",\n            \"a=\", wa, \"b=\", wb, \"c=\", wc\n        )\n    }\n    par(new = \"T\")\n    plot(theta, P,\n        xlim = c(-3, 3), ylim = c(0, 1), type = \"l\",\n        xlab = \"\", ylab = \"\", main = maintext\n    )\n}\n\nset.seed(1)\ngroupinv(1, -3, -1, 1, 3)\n\n\n\n\n\n\n\nNel grafico risultante, osserviamo due segmenti di punti che rappresentano le risposte dei due gruppi. La linea continua mostra la curva ICC stimata utilizzando i dati di entrambi i gruppi. Il fatto che questa curva si adatti bene ai punti di entrambi i gruppi, nonostante la loro diversa distribuzione di abilità, dimostra visivamente il principio di invarianza: i parametri dell’item rimangono stabili indipendentemente dal gruppo considerato.\nQuesta visualizzazione è particolarmente efficace perché:\n\nmostra chiaramente la separazione tra i due gruppi di abilità;\npermette di verificare che la stessa curva ICC si adatta bene a entrambi i gruppi;\nconferma che le stime dei parametri (riportate nel titolo del grafico) sono valide per l’intero range di abilità.\n\nL’esercizio fornisce quindi una dimostrazione empirica dell’invarianza di gruppo, una delle proprietà fondamentali che rendono la IRT uno strumento robusto per la misurazione psicometrica.\n\n\n70.5.1 Confronto con la Teoria Classica dei Test\nLa differenza più notevole tra IRT e CTT riguarda proprio il modo in cui viene trattata l’invarianza dei parametri degli item rispetto ai gruppi esaminati. Questo aspetto emerge chiaramente analizzando come le due teorie definiscono e misurano la difficoltà degli item.\nNella CTT, la difficoltà di un item è definita come la proporzione di risposte corrette nel campione. Questa definizione rende il parametro di difficoltà intrinsecamente dipendente dalla popolazione esaminata: lo stesso item mostrerà una “difficoltà” diversa se somministrato a gruppi con differenti livelli di abilità. Per esempio, se somministriamo un test a due classi di livello diverso, nella CTT otterremo due stime di difficoltà diverse per lo stesso item, rendendo problematico qualsiasi confronto diretto tra i gruppi.\nL’IRT risolve questa limitazione fondamentale introducendo parametri che sono teoricamente invarianti rispetto alla popolazione. Il parametro di difficoltà (\\(\\beta\\)) in particolare rappresenta una proprietà intrinseca dell’item che rimane costante indipendentemente dal gruppo esaminato. Questo significa che, a differenza della CTT, l’IRT può fornire stime comparabili della difficoltà dell’item anche quando viene somministrato a popolazioni con distribuzioni di abilità molto diverse.\nL’invarianza nella IRT si manifesta nella curva caratteristica dell’item (ICC): la relazione tra abilità e probabilità di risposta corretta mantiene la stessa forma matematica indipendentemente dal gruppo considerato (come abbiamo osservato nell’esempio precedente). Questa proprietà ha importanti implicazioni pratiche:\n\npossiamo stimare i parametri dell’item utilizzando qualsiasi sottogruppo della popolazione e ottenere risultati coerenti;\nè possibile confrontare direttamente le prestazioni di gruppi diversi sullo stesso item;\nla calibrazione degli item può essere effettuata su un campione e poi applicata con fiducia ad altri gruppi.\n\nTuttavia, è importante notare che mentre nella CTT la dipendenza dalla popolazione è una limitazione intrinseca del modello, nell’IRT l’invarianza è una proprietà teorica che nella pratica può essere influenzata da vari fattori. Le stime empiriche dei parametri possono mostrare alcune variazioni dovute all’errore campionario, e l’invarianza è garantita solo quando l’item misura effettivamente lo stesso costrutto in tutti i gruppi considerati.\nQuesta differenza fondamentale tra CTT e IRT nell’approccio all’invarianza di gruppo rende l’IRT particolarmente adatta per applicazioni che richiedono confronti affidabili tra popolazioni diverse, come nel testing adattivo, negli studi longitudinali e nelle comparazioni tra gruppi culturali diversi.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>Modelli 1PL, 2PL e 3PL</span>"
    ]
  },
  {
    "objectID": "chapters/irt/04_1pl_2pl_3pl.html#riflessioni-conclusive",
    "href": "chapters/irt/04_1pl_2pl_3pl.html#riflessioni-conclusive",
    "title": "70  Modelli 1PL, 2PL e 3PL",
    "section": "\n70.6 Riflessioni Conclusive",
    "text": "70.6 Riflessioni Conclusive\nIn questo capitolo, abbiamo esplorato la progressione dei modelli IRT, partendo dal modello di Rasch fino al più flessibile modello 3PL. Ogni modello offre un equilibrio unico tra semplicità e adattabilità, consentendo di rispondere a esigenze diverse nell’ambito della misurazione psicometrica.\n\nIl modello di Rasch, con i suoi vincoli rigorosi, si distingue per la sua capacità di fornire misurazioni oggettive e stabili, risultando particolarmente utile nella costruzione di strumenti di misurazione.\nIl modello 1PL mantiene la semplicità del modello di Rasch ma introduce una maggiore flessibilità consentendo di variare il parametro di discriminazione a livello teorico.\nIl modello 2PL aggiunge un ulteriore livello di complessità, permettendo a ogni item di avere una discriminazione specifica, migliorando l’adattamento ai dati reali.\nIl modello 3PL completa questa progressione introducendo il parametro di guessing, necessario per tenere conto delle risposte corrette casuali, tipiche nei test a scelta multipla.\n\nQuesta evoluzione riflette l’importanza di adattare il modello alle caratteristiche dei dati e alle finalità dell’analisi. Abbiamo anche sottolineato l’importanza dell’invarianza di gruppo, una proprietà chiave che consente confronti equi tra popolazioni diverse, distinguendo l’IRT dalla Teoria Classica dei Test (CTT).\nIn definitiva, la scelta del modello dipende dall’obiettivo specifico dello studio e dalla complessità dei dati osservati. Il modello di Rasch offre rigore e semplicità, mentre i modelli 2PL e 3PL offrono flessibilità e precisione. Questa progressione dimostra la versatilità della IRT come framework per la misurazione psicometrica, supportando applicazioni che spaziano dalla ricerca accademica allo sviluppo di test standardizzati.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>Modelli 1PL, 2PL e 3PL</span>"
    ]
  },
  {
    "objectID": "chapters/irt/04_1pl_2pl_3pl.html#session-info",
    "href": "chapters/irt/04_1pl_2pl_3pl.html#session-info",
    "title": "70  Modelli 1PL, 2PL e 3PL",
    "section": "\n70.7 Session Info",
    "text": "70.7 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] grid      stats4    stats     graphics  grDevices utils     datasets \n#&gt; [8] methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] latex2exp_0.9.6   psychotools_0.7-4 ggmirt_0.1.0      TAM_4.2-21       \n#&gt;  [5] CDM_8.2-6         mvtnorm_1.3-3     mirt_1.44.0       lattice_0.22-6   \n#&gt;  [9] eRm_1.0-6         ggokabeito_0.1.0  see_0.10.0        MASS_7.3-65      \n#&gt; [13] viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1   \n#&gt; [17] gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6   \n#&gt; [21] semPlot_1.1.6     lavaan_0.6-19     psych_2.4.12      scales_1.3.0     \n#&gt; [25] markdown_1.13     knitr_1.49        lubridate_1.9.4   forcats_1.0.0    \n#&gt; [29] stringr_1.5.1     dplyr_1.1.4       purrr_1.0.4       readr_2.1.5      \n#&gt; [33] tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0  \n#&gt; [37] here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] splines_4.4.2        later_1.4.1          R.oo_1.27.0         \n#&gt;   [4] XML_3.99-0.18        rpart_4.1.24         lifecycle_1.0.4     \n#&gt;   [7] Rdpack_2.6.2         rstatix_0.7.2        rprojroot_2.0.4     \n#&gt;  [10] globals_0.16.3       rockchalk_1.8.157    backports_1.5.0     \n#&gt;  [13] magrittr_2.0.3       openxlsx_4.2.8       Hmisc_5.2-2         \n#&gt;  [16] rmarkdown_2.29       yaml_2.3.10          httpuv_1.6.15       \n#&gt;  [19] qgraph_1.9.8         zip_2.3.2            sessioninfo_1.2.3   \n#&gt;  [22] pbapply_1.7-2        minqa_1.2.8          multcomp_1.4-28     \n#&gt;  [25] abind_1.4-8          audio_0.1-11         quadprog_1.5-8      \n#&gt;  [28] R.utils_2.13.0       nnet_7.3-20          TH.data_1.1-3       \n#&gt;  [31] sandwich_3.1-1       listenv_0.9.1        testthat_3.2.3      \n#&gt;  [34] RPushbullet_0.3.4    vegan_2.6-10         arm_1.14-4          \n#&gt;  [37] parallelly_1.42.0    permute_0.9-7        codetools_0.2-20    \n#&gt;  [40] tidyselect_1.2.1     farver_2.1.2         lme4_1.1-36         \n#&gt;  [43] base64enc_0.1-3      jsonlite_1.9.0       polycor_0.8-1       \n#&gt;  [46] progressr_0.15.1     Formula_1.2-5        survival_3.8-3      \n#&gt;  [49] emmeans_1.10.7       tools_4.4.2          snow_0.4-4          \n#&gt;  [52] Rcpp_1.0.14          glue_1.8.0           mnormt_2.1.1        \n#&gt;  [55] admisc_0.37          xfun_0.51            mgcv_1.9-1          \n#&gt;  [58] withr_3.0.2          beepr_2.0            fastmap_1.2.0       \n#&gt;  [61] boot_1.3-31          digest_0.6.37        mi_1.1              \n#&gt;  [64] timechange_0.3.0     R6_2.6.1             mime_0.12           \n#&gt;  [67] estimability_1.5.1   colorspace_2.1-1     gtools_3.9.5        \n#&gt;  [70] jpeg_0.1-10          R.methodsS3_1.8.2    generics_0.1.3      \n#&gt;  [73] data.table_1.17.0    corpcor_1.6.10       SimDesign_2.18      \n#&gt;  [76] htmlwidgets_1.6.4    pkgconfig_2.0.3      sem_3.1-16          \n#&gt;  [79] gtable_0.3.6         brio_1.1.5           htmltools_0.5.8.1   \n#&gt;  [82] carData_3.0-5        png_0.1-8            reformulas_0.4.0    \n#&gt;  [85] rstudioapi_0.17.1    tzdb_0.4.0           reshape2_1.4.4      \n#&gt;  [88] coda_0.19-4.1        checkmate_2.3.2      nlme_3.1-167        \n#&gt;  [91] curl_6.2.1           nloptr_2.1.1         zoo_1.8-13          \n#&gt;  [94] parallel_4.4.2       miniUI_0.1.1.1       foreign_0.8-88      \n#&gt;  [97] pillar_1.10.1        vctrs_0.6.5          promises_1.3.2      \n#&gt; [100] car_3.1-3            OpenMx_2.21.13       xtable_1.8-4        \n#&gt; [103] Deriv_4.1.6          cluster_2.1.8        dcurver_0.9.2       \n#&gt; [106] GPArotation_2024.3-1 htmlTable_2.4.3      evaluate_1.0.3      \n#&gt; [109] pbivnorm_0.6.0       cli_3.6.4            kutils_1.73         \n#&gt; [112] compiler_4.4.2       rlang_1.1.5          future.apply_1.11.3 \n#&gt; [115] ggsignif_0.6.4       fdrtool_1.2.18       plyr_1.8.9          \n#&gt; [118] stringi_1.8.4        munsell_0.5.1        lisrelToR_0.3       \n#&gt; [121] pacman_0.5.1         Matrix_1.7-2         hms_1.1.3           \n#&gt; [124] glasso_1.11          future_1.34.0        shiny_1.10.0        \n#&gt; [127] rbibutils_2.3        igraph_2.1.4         broom_1.0.7         \n#&gt; [130] RcppParallel_5.1.10\n\n\n\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>Modelli 1PL, 2PL e 3PL</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_implementation.html",
    "href": "chapters/irt/05_implementation.html",
    "title": "71  Implementazione",
    "section": "",
    "text": "71.1 Introduzione\nIn questo capitolo esamineremo il tutorial di Debelak et al. (2022).",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_implementation.html#un-esempio-pratico",
    "href": "chapters/irt/05_implementation.html#un-esempio-pratico",
    "title": "71  Implementazione",
    "section": "\n71.2 Un esempio pratico",
    "text": "71.2 Un esempio pratico\nIl set di dati data.fims.Aus.Jpn.scored contiene le risposte valutate per un sottoinsieme di item da parte di studenti australiani e giapponesi nello studio “First International Mathematics Study” (FIMS, Husén, 1967).\n\ndata(data.fims.Aus.Jpn.scored, package = \"TAM\")\nfims &lt;- data.fims.Aus.Jpn.scored\n\n\nglimpse(fims)\n#&gt; Rows: 6,371\n#&gt; Columns: 16\n#&gt; $ SEX     &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n#&gt; $ M1PTI1  &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1…\n#&gt; $ M1PTI2  &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1…\n#&gt; $ M1PTI3  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1…\n#&gt; $ M1PTI6  &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0…\n#&gt; $ M1PTI7  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#&gt; $ M1PTI11 &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1…\n#&gt; $ M1PTI12 &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#&gt; $ M1PTI14 &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1…\n#&gt; $ M1PTI17 &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0…\n#&gt; $ M1PTI18 &lt;dbl&gt; 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1…\n#&gt; $ M1PTI19 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#&gt; $ M1PTI21 &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0…\n#&gt; $ M1PTI22 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#&gt; $ M1PTI23 &lt;dbl&gt; 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1…\n#&gt; $ country &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n\nOltre alle risposte sui 14 item di matematica, il data set contiene anche informazioni sul genere del partecipate e sul paese d’origine.\n\nfims$SEX &lt;- as.factor(fims$SEX)\nlevels(fims$SEX) &lt;- c(\"male\", \"female\")\nfims$country &lt;- as.factor(fims$country)\nlevels(fims$country) &lt;- c(\"Australia\", \"Japan\")\n\n\nsummary(fims[, c(\"SEX\", \"country\")])\n#&gt;      SEX            country    \n#&gt;  male  :3319   Australia:4320  \n#&gt;  female:3052   Japan    :2051\n\nEsaminiamo le risposte dei primi 400 partecipanti. Con le seguenti istruzioni, per facilitare la manipolazione dei dati, cambiamo il nome delle colonne.\n\nresponses &lt;- fims[1:400, 2:15]\ncolnames(responses) &lt;- gsub(\"M1PTI\", \"I\", colnames(responses))\nglimpse(responses)\n#&gt; Rows: 400\n#&gt; Columns: 14\n#&gt; $ I1  &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,…\n#&gt; $ I2  &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,…\n#&gt; $ I3  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,…\n#&gt; $ I6  &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,…\n#&gt; $ I7  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n#&gt; $ I11 &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,…\n#&gt; $ I12 &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n#&gt; $ I14 &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,…\n#&gt; $ I17 &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,…\n#&gt; $ I18 &lt;dbl&gt; 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,…\n#&gt; $ I19 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n#&gt; $ I21 &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,…\n#&gt; $ I22 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,…\n#&gt; $ I23 &lt;dbl&gt; 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,…\n\n\ngender &lt;- as.factor(fims$SEX[1:400])\nlevels(gender) &lt;- c(\"male\", \"female\")",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_implementation.html#modello-di-rasch",
    "href": "chapters/irt/05_implementation.html#modello-di-rasch",
    "title": "71  Implementazione",
    "section": "\n71.3 Modello di Rasch",
    "text": "71.3 Modello di Rasch\nUn’analisi IRT può essere paragonata a un’analisi fattoriale. Dopo avere adattato il modello di Rasch ai dati usando mirt(), possiamo usare la funzione summary() per ottenere quella che viene definita “soluzione fattoriale”, che include i carichi fattoriali (F1) e le comunalità (h2). Le comunalità, essendo carichi fattoriali al quadrato, sono interpretate come la varianza spiegata in un item dal tratto latente. Nel caso presente, tutti gli item hanno una relazione sostanziale (saturazioni \\(\\approx\\) .50) con il tratto latente, indicando che il tratto latente è un buon indicatore della varianza osservata in quegli item. Questo suggerisce che il tratto latente è in grado di spiegare una porzione almento moderata della varianza nei punteggi degli item.\n\nmirt_rm &lt;- mirt(responses, 1, itemtype = \"Rasch\", verbose = FALSE)\nsummary(mirt_rm)\n#&gt;     F1    h2\n#&gt; I1     0.238\n#&gt; I2     0.238\n#&gt; I3     0.238\n#&gt; I6     0.238\n#&gt; I7     0.238\n#&gt; I11    0.238\n#&gt; I12    0.238\n#&gt; I14    0.238\n#&gt; I17    0.238\n#&gt; I18    0.238\n#&gt; I19    0.238\n#&gt; I21    0.238\n#&gt; I22    0.238\n#&gt; I23    0.238\n#&gt; \n#&gt; SS loadings:  0 \n#&gt; Proportion Var:  0 \n#&gt; \n#&gt; Factor correlations: \n#&gt; \n#&gt;    F1\n#&gt; F1  1\n\nNell’IRT, tuttavia, siamo generalmente più interessati ai parametri di discriminazione e difficoltà. Questi parametri possono essere estratti dall’oggetto creato da mirt() nel seguente modo:\n\nparams_rm &lt;- coef(mirt_rm, IRTpars = TRUE, simplify = TRUE)\nround(params_rm$items, 2) # g = c = guessing parameter\n#&gt;     a     b g u\n#&gt; I1  1 -1.10 0 1\n#&gt; I2  1 -1.25 0 1\n#&gt; I3  1 -2.04 0 1\n#&gt; I6  1 -0.05 0 1\n#&gt; I7  1  2.53 0 1\n#&gt; I11 1 -1.25 0 1\n#&gt; I12 1  0.81 0 1\n#&gt; I14 1 -0.50 0 1\n#&gt; I17 1  1.33 0 1\n#&gt; I18 1 -0.40 0 1\n#&gt; I19 1  2.06 0 1\n#&gt; I21 1  1.75 0 1\n#&gt; I22 1  2.41 0 1\n#&gt; I23 1 -1.93 0 1\n\n\n\n\\(a\\) (Discriminazione): Il parametro \\(a\\) (discriminazione) rappresenta la pendenza delle curve caratteristiche degli item (ICC - Item Characteristic Curves). Una pendenza elevata (valore alto di \\(a\\)) indica che l’item è molto efficace nel distinguere tra individui con livelli diversi del tratto latente (ad esempio, abilità). Questo significa che piccole variazioni nel tratto latente portano a grandi cambiamenti nella probabilità di rispondere correttamente all’item. Una pendenza bassa (valore basso di \\(a\\)) suggerisce che l’item non è altrettanto efficace nel discriminare tra livelli diversi del tratto latente. In questo caso, anche ampie variazioni nel tratto latente comportano solo piccoli cambiamenti nella probabilità di risposta corretta. Nel modello di Rasch si assume che tutti gli item abbiano la stessa pendenza (o potere discriminante), e quindi tutti i valori di \\(a\\) sono fissati allo stesso valore (ovvero 1).\n\n\\(b\\) (Difficoltà): Rappresenta il livello di abilità a cui un rispondente ha il 50% di probabilità di rispondere correttamente all’item. Un valore positivo indica un item più difficile (richiede un livello di abilità superiore per rispondere correttamente), mentre un valore negativo indica un item più facile. Ad esempio, I7 ha un valore di difficoltà di 2.53, il che significa che è relativamente difficile, mentre I3, con un valore di -2.04, è relativamente facile.\n\n\\(g\\) (Probabilità di Indovinare): In questo modello, la probabilità di indovinare è impostata a zero per tutti gli item, il che è coerente con il modello di Rasch, dove non si considera la possibilità di indovinare correttamente un item per caso.\n\nAdattiamo ora ai dati il modello di Rasch con la funzione eRm::RM()\n\nrm_sum0 &lt;- eRm::RM(responses)\n\n\nsummary(rm_sum0)\n#&gt; \n#&gt; Results of RM estimation: \n#&gt; \n#&gt; Call:  eRm::RM(X = responses) \n#&gt; \n#&gt; Conditional log-likelihood: -1887 \n#&gt; Number of iterations: 23 \n#&gt; Number of parameters: 13 \n#&gt; \n#&gt; Item (Category) Difficulty Parameters (eta): with 0.95 CI:\n#&gt;     Estimate Std. Error lower CI upper CI\n#&gt; I2    -1.420      0.121   -1.658   -1.183\n#&gt; I3    -2.210      0.145   -2.494   -1.926\n#&gt; I6    -0.215      0.108   -0.426   -0.004\n#&gt; I7     2.364      0.170    2.031    2.697\n#&gt; I11   -1.420      0.121   -1.658   -1.183\n#&gt; I12    0.642      0.113    0.422    0.863\n#&gt; I14   -0.663      0.110   -0.879   -0.448\n#&gt; I17    1.152      0.122    0.913    1.391\n#&gt; I18   -0.565      0.109   -0.778   -0.351\n#&gt; I19    1.889      0.146    1.602    2.175\n#&gt; I21    1.578      0.134    1.315    1.841\n#&gt; I22    2.244      0.163    1.925    2.564\n#&gt; I23   -2.103      0.141   -2.379   -1.827\n#&gt; \n#&gt; Item Easiness Parameters (beta) with 0.95 CI:\n#&gt;          Estimate Std. Error lower CI upper CI\n#&gt; beta I1     1.273      0.118    1.041    1.504\n#&gt; beta I2     1.420      0.121    1.183    1.658\n#&gt; beta I3     2.210      0.145    1.926    2.494\n#&gt; beta I6     0.215      0.108    0.004    0.426\n#&gt; beta I7    -2.364      0.170   -2.697   -2.031\n#&gt; beta I11    1.420      0.121    1.183    1.658\n#&gt; beta I12   -0.642      0.113   -0.863   -0.422\n#&gt; beta I14    0.663      0.110    0.448    0.879\n#&gt; beta I17   -1.152      0.122   -1.391   -0.913\n#&gt; beta I18    0.565      0.109    0.351    0.778\n#&gt; beta I19   -1.889      0.146   -2.175   -1.602\n#&gt; beta I21   -1.578      0.134   -1.841   -1.315\n#&gt; beta I22   -2.244      0.163   -2.564   -1.925\n#&gt; beta I23    2.103      0.141    1.827    2.379\n\nLa funzione RM() impone un vincolo sulle stime dei parametri di difficoltà degli item. Questo vincolo è che la media di questi parametri (beta) sia zero. Questo approccio è noto come “parametrizzazione ancorata” o “centrata”. Il vantaggio di questa parametrizzazione è che posiziona la scala di difficoltà degli item in un punto di riferimento fisso, facilitando il confronto tra diversi set di item o tra diverse applicazioni dello stesso test.\nVerifichiamo.\n\ncoef(rm_sum0) \n#&gt;  beta I1  beta I2  beta I3  beta I6  beta I7 beta I11 beta I12 beta I14 \n#&gt;    1.273    1.420    2.210    0.215   -2.364    1.420   -0.642    0.663 \n#&gt; beta I17 beta I18 beta I19 beta I21 beta I22 beta I23 \n#&gt;   -1.152    0.565   -1.889   -1.578   -2.244    2.103\n\n\nsum(rm_sum0$betapar)\n#&gt; [1] 8.88e-16\n\nNella parametrizzazione utilizzata da mirt(), i parametri di difficoltà vengono invece stimati senza un vincolo sulla loro media. Questo può portare a stime dei parametri di difficoltà che differiscono da quelle ottenute tramite RM(). Questa libertà nella stima dei parametri permette una flessibilità maggiore, specialmente in modelli IRT complessi o multidimensionali, ma può rendere più complesso il confronto diretto tra set di item o test differenti.\nDalla soluzione prodotta da eRm::RM() possiamo estrarre le stime sia nei termini della facilità che della difficoltà degli item.\n\ntab &lt;- data.frame(\n    item_score = colSums(responses),\n    easiness = coef(rm_sum0),\n    difficulty = -coef(rm_sum0)\n)\ntab[order(tab$item_score), ]\n#&gt;     item_score easiness difficulty\n#&gt; I7          40   -2.364      2.364\n#&gt; I22         44   -2.244      2.244\n#&gt; I19         58   -1.889      1.889\n#&gt; I21         73   -1.578      1.578\n#&gt; I17         98   -1.152      1.152\n#&gt; I12        134   -0.642      0.642\n#&gt; I6         204    0.215     -0.215\n#&gt; I18        233    0.565     -0.565\n#&gt; I14        241    0.663     -0.663\n#&gt; I1         287    1.273     -1.273\n#&gt; I2         297    1.420     -1.420\n#&gt; I11        297    1.420     -1.420\n#&gt; I23        336    2.103     -2.103\n#&gt; I3         341    2.210     -2.210\n\nIn alterativa, possiamo usare il pacchetto TAM. Come nel caso di mirt, anche in questo caso viene usata una procedura di stima di massima verosimiglianza marginale.\n\ntam_rm &lt;- tam.mml(responses)\n#&gt; ....................................................\n#&gt; Processing Data      2025-03-03 16:21:19.426311 \n#&gt;     * Response Data: 400 Persons and  14 Items \n#&gt;     * Numerical integration with 21 nodes\n#&gt;     * Created Design Matrices   ( 2025-03-03 16:21:19.427326 )\n#&gt;     * Calculated Sufficient Statistics   ( 2025-03-03 16:21:19.42854 )\n#&gt; ....................................................\n#&gt; Iteration 1     2025-03-03 16:21:19.429614\n#&gt; E Step\n#&gt; M Step Intercepts   |----\n#&gt;   Deviance = 5667.9246\n#&gt;   Maximum item intercept parameter change: 0.315\n#&gt;   Maximum item slope parameter change: 0\n#&gt;   Maximum regression parameter change: 0\n#&gt;   Maximum variance parameter change: 0.123\n#&gt; ....................................................\n#&gt; Iteration 2     2025-03-03 16:21:19.430778\n#&gt; E Step\n#&gt; M Step Intercepts   |--\n#&gt;   Deviance = 5633.0417 | Absolute change: 34.9 | Relative change: 0.00619\n#&gt;   Maximum item intercept parameter change: 0.00328\n#&gt;   Maximum item slope parameter change: 0\n#&gt;   Maximum regression parameter change: 0\n#&gt;   Maximum variance parameter change: 0.00796\n#&gt; ....................................................\n#&gt; Iteration 3     2025-03-03 16:21:19.434852\n#&gt; E Step\n#&gt; M Step Intercepts   |--\n#&gt;   Deviance = 5633.0057 | Absolute change: 0.036 | Relative change: 6.39e-06\n#&gt;   Maximum item intercept parameter change: 0.0022\n#&gt;   Maximum item slope parameter change: 0\n#&gt;   Maximum regression parameter change: 0\n#&gt;   Maximum variance parameter change: 0.00584\n#&gt; ....................................................\n#&gt; Iteration 4     2025-03-03 16:21:19.435331\n#&gt; E Step\n#&gt; M Step Intercepts   |--\n#&gt;   Deviance = 5632.9887 | Absolute change: 0.017 | Relative change: 3.02e-06\n#&gt;   Maximum item intercept parameter change: 0.00151\n#&gt;   Maximum item slope parameter change: 0\n#&gt;   Maximum regression parameter change: 0\n#&gt;   Maximum variance parameter change: 0.00406\n#&gt; ....................................................\n#&gt; Iteration 5     2025-03-03 16:21:19.435636\n#&gt; E Step\n#&gt; M Step Intercepts   |--\n#&gt;   Deviance = 5632.9805 | Absolute change: 0.0081 | Relative change: 1.44e-06\n#&gt;   Maximum item intercept parameter change: 0.00104\n#&gt;   Maximum item slope parameter change: 0\n#&gt;   Maximum regression parameter change: 0\n#&gt;   Maximum variance parameter change: 0.00281\n#&gt; ....................................................\n#&gt; Iteration 6     2025-03-03 16:21:19.43595\n#&gt; E Step\n#&gt; M Step Intercepts   |--\n#&gt;   Deviance = 5632.9767 | Absolute change: 0.0039 | Relative change: 6.8e-07\n#&gt;   Maximum item intercept parameter change: 0.00071\n#&gt;   Maximum item slope parameter change: 0\n#&gt;   Maximum regression parameter change: 0\n#&gt;   Maximum variance parameter change: 0.00194\n#&gt; ....................................................\n#&gt; Iteration 7     2025-03-03 16:21:19.436259\n#&gt; E Step\n#&gt; M Step Intercepts   |--\n#&gt;   Deviance = 5632.9749 | Absolute change: 0.0018 | Relative change: 3.2e-07\n#&gt;   Maximum item intercept parameter change: 0.000486\n#&gt;   Maximum item slope parameter change: 0\n#&gt;   Maximum regression parameter change: 0\n#&gt;   Maximum variance parameter change: 0.00134\n#&gt; ....................................................\n#&gt; Iteration 8     2025-03-03 16:21:19.436569\n#&gt; E Step\n#&gt; M Step Intercepts   |--\n#&gt;   Deviance = 5632.974 | Absolute change: 9e-04 | Relative change: 1.5e-07\n#&gt;   Maximum item intercept parameter change: 0.000333\n#&gt;   Maximum item slope parameter change: 0\n#&gt;   Maximum regression parameter change: 0\n#&gt;   Maximum variance parameter change: 0.000925\n#&gt; ....................................................\n#&gt; Iteration 9     2025-03-03 16:21:19.436876\n#&gt; E Step\n#&gt; M Step Intercepts   |--\n#&gt;   Deviance = 5632.9736 | Absolute change: 4e-04 | Relative change: 7e-08\n#&gt;   Maximum item intercept parameter change: 0.000228\n#&gt;   Maximum item slope parameter change: 0\n#&gt;   Maximum regression parameter change: 0\n#&gt;   Maximum variance parameter change: 0.000637\n#&gt; ....................................................\n#&gt; Iteration 10     2025-03-03 16:21:19.437188\n#&gt; E Step\n#&gt; M Step Intercepts   |--\n#&gt;   Deviance = 5632.9734 | Absolute change: 2e-04 | Relative change: 3e-08\n#&gt;   Maximum item intercept parameter change: 0.000156\n#&gt;   Maximum item slope parameter change: 0\n#&gt;   Maximum regression parameter change: 0\n#&gt;   Maximum variance parameter change: 0.000439\n#&gt; ....................................................\n#&gt; Iteration 11     2025-03-03 16:21:19.437502\n#&gt; E Step\n#&gt; M Step Intercepts   |--\n#&gt;   Deviance = 5632.9733 | Absolute change: 1e-04 | Relative change: 2e-08\n#&gt;   Maximum item intercept parameter change: 0.000107\n#&gt;   Maximum item slope parameter change: 0\n#&gt;   Maximum regression parameter change: 0\n#&gt;   Maximum variance parameter change: 0.000302\n#&gt; ....................................................\n#&gt; Iteration 12     2025-03-03 16:21:19.437807\n#&gt; E Step\n#&gt; M Step Intercepts   |-\n#&gt;   Deviance = 5632.9733 | Absolute change: 0 | Relative change: 1e-08\n#&gt;   Maximum item intercept parameter change: 7.3e-05\n#&gt;   Maximum item slope parameter change: 0\n#&gt;   Maximum regression parameter change: 0\n#&gt;   Maximum variance parameter change: 0.000208\n#&gt; ....................................................\n#&gt; Iteration 13     2025-03-03 16:21:19.438079\n#&gt; E Step\n#&gt; M Step Intercepts   |-\n#&gt;   Deviance = 5632.9732 | Absolute change: 0 | Relative change: 0\n#&gt;   Maximum item intercept parameter change: 5e-05\n#&gt;   Maximum item slope parameter change: 0\n#&gt;   Maximum regression parameter change: 0\n#&gt;   Maximum variance parameter change: 0.000143\n#&gt; ....................................................\n#&gt; Iteration 14     2025-03-03 16:21:19.438348\n#&gt; E Step\n#&gt; M Step Intercepts   |-\n#&gt;   Deviance = 5632.9732 | Absolute change: 0 | Relative change: 0\n#&gt;   Maximum item intercept parameter change: 3.4e-05\n#&gt;   Maximum item slope parameter change: 0\n#&gt;   Maximum regression parameter change: 0\n#&gt;   Maximum variance parameter change: 9.8e-05\n#&gt; ....................................................\n#&gt; Item Parameters\n#&gt;    xsi.index xsi.label    est\n#&gt; 1          1        I1 -1.103\n#&gt; 2          2        I2 -1.250\n#&gt; 3          3        I3 -2.042\n#&gt; 4          4        I6 -0.048\n#&gt; 5          5        I7  2.531\n#&gt; 6          6       I11 -1.250\n#&gt; 7          7       I12  0.813\n#&gt; 8          8       I14 -0.495\n#&gt; 9          9       I17  1.326\n#&gt; 10        10       I18 -0.397\n#&gt; 11        11       I19  2.064\n#&gt; 12        12       I21  1.754\n#&gt; 13        13       I22  2.414\n#&gt; 14        14       I23 -1.935\n#&gt; ...................................\n#&gt; Regression Coefficients\n#&gt;      [,1]\n#&gt; [1,]    0\n#&gt; \n#&gt; Variance:\n#&gt;        [,1]\n#&gt; [1,] 0.9037\n#&gt; \n#&gt; \n#&gt; EAP Reliability:\n#&gt; [1] 0.656\n#&gt; \n#&gt; -----------------------------\n#&gt; Start:  2025-03-03 16:21:19.425439\n#&gt; End:  2025-03-03 16:21:19.44105 \n#&gt; Time difference of 0.0156 secs\n\nPossiamo ispezionare le stime dei parametri con\n\ntam_rm$item\n#&gt;     item   N     M xsi.item AXsi_.Cat1 B.Cat1.Dim1\n#&gt; I1    I1 400 0.718   -1.103     -1.103           1\n#&gt; I2    I2 400 0.743   -1.250     -1.250           1\n#&gt; I3    I3 400 0.853   -2.042     -2.042           1\n#&gt; I6    I6 400 0.510   -0.048     -0.048           1\n#&gt; I7    I7 400 0.100    2.531      2.531           1\n#&gt; I11  I11 400 0.743   -1.250     -1.250           1\n#&gt; I12  I12 400 0.335    0.813      0.813           1\n#&gt; I14  I14 400 0.603   -0.495     -0.495           1\n#&gt; I17  I17 400 0.245    1.326      1.326           1\n#&gt; I18  I18 400 0.583   -0.397     -0.397           1\n#&gt; I19  I19 400 0.145    2.064      2.064           1\n#&gt; I21  I21 400 0.182    1.755      1.755           1\n#&gt; I22  I22 400 0.110    2.415      2.415           1\n#&gt; I23  I23 400 0.840   -1.935     -1.935           1\n\nLe colonne di questo output possono essere interpretate come segue:\n\n\nitem indica il nome dell’item.\n\nN indica il numero di candidati che hanno risposto a ciascun item. In questo caso, tutti i 400 candidati hanno risposto a ogni item.\n\nM è la media delle risposte a ciascun item. Nel caso di un item con una media alta ciò significa che a tale item è stata fornita uba risposta corretta da una alta percentuale di candidati.\n\nxsi.item per il modello di Rasch è il parametro di difficoltà dell’item. Gli item con valori alti tendono ad essere più difficili.\n\nAXsi.Cat1 ripete la difficoltà dell’item per il modello di Rasch, ma permetterebbe l’inclusione di una matrice di design A, che non abbiamo usato qui. Per i modelli politomici, l’output includerà parametri dell’item per più di una categoria.\n\nB.Cat1.Dim1 è il parametro di discriminazione o pendenza dell’item. Per il modello di Rasch, la pendenza è 1 per ogni item.\n\nPossiamo mostrare solo la difficoltà e l’errore standard con:\n\ntam_rm$xsi\n#&gt;        xsi se.xsi\n#&gt; I1  -1.103  0.120\n#&gt; I2  -1.250  0.123\n#&gt; I3  -2.042  0.149\n#&gt; I6  -0.048  0.109\n#&gt; I7   2.531  0.174\n#&gt; I11 -1.250  0.123\n#&gt; I12  0.813  0.115\n#&gt; I14 -0.495  0.111\n#&gt; I17  1.326  0.125\n#&gt; I18 -0.397  0.111\n#&gt; I19  2.064  0.150\n#&gt; I21  1.755  0.138\n#&gt; I22  2.415  0.168\n#&gt; I23 -1.935  0.145\n\nLa parametrizzazione classica IRT si ottiene con:\n\ntam_rm$item_irt\n#&gt;    item alpha   beta\n#&gt; 1    I1     1 -1.103\n#&gt; 2    I2     1 -1.250\n#&gt; 3    I3     1 -2.042\n#&gt; 4    I6     1 -0.048\n#&gt; 5    I7     1  2.531\n#&gt; 6   I11     1 -1.250\n#&gt; 7   I12     1  0.813\n#&gt; 8   I14     1 -0.495\n#&gt; 9   I17     1  1.326\n#&gt; 10  I18     1 -0.397\n#&gt; 11  I19     1  2.064\n#&gt; 12  I21     1  1.755\n#&gt; 13  I22     1  2.415\n#&gt; 14  I23     1 -1.935",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_implementation.html#valutazione-del-test",
    "href": "chapters/irt/05_implementation.html#valutazione-del-test",
    "title": "71  Implementazione",
    "section": "\n71.4 Valutazione del Test",
    "text": "71.4 Valutazione del Test\nIl primo approccio per valutare i test consiste nell’utilizzo di metodi grafici. Tra questi, il primo strumento che esaminiamo è la mappa persona-item, utile per verificare se il campione di partecipanti copre l’intera gamma di difficoltà degli item e se, viceversa, gli item coprono l’intero spettro delle abilità del campione.\nIl secondo metodo riguarda il confronto tra le Curve Caratteristiche degli Item (ICC) teoriche ed empiriche, che consente di identificare eventuali item con scarsa aderenza al modello.\nInfine, il terzo approccio è il test grafico per il Funzionamento Differenziale degli Item (DIF), che offre un metodo visivo per rilevare discrepanze nel comportamento degli item tra gruppi di rispondenti.\n\n71.4.1 Mappa Persona-Item\nLa Mappa Persona-Item (nota anche come Wright Map o person-item map) è uno strumento grafico utile per valutare quanto efficacemente gli item coprono l’intervallo delle abilità latenti nel campione studiato. Questo strumento consente di rispondere alla domanda: Quanto bene gli item riflettono la gamma delle abilità latenti presenti nel campione?\nLa costruzione della mappa avviene in due fasi:\n\n\nDistribuzione delle abilità latenti: Si rappresenta graficamente la distribuzione delle abilità latenti (\\(\\theta\\)) del campione di persone.\n\nDifficoltà degli item: Si sovrappone la difficoltà di ciascun item sulla stessa scala di \\(\\theta\\) utilizzata per le abilità.\n\nL’allineamento di queste due rappresentazioni permette di valutare visivamente la corrispondenza tra le abilità del campione e le difficoltà degli item. Idealmente, le difficoltà degli item dovrebbero coprire l’intera gamma delle abilità delle persone, e viceversa. Questa corrispondenza è essenziale per garantire una stima precisa dei parametri degli item e delle abilità dei rispondenti.\nLa mappa persona-item fornisce quindi una panoramica intuitiva e immediata dell’adeguatezza del test rispetto al campione studiato, evidenziando eventuali gap nella copertura delle abilità o difficoltà non equilibrate.\nPer generare una mappa persona-item, è possibile utilizzare la funzione:\n\nitempersonMap(mirt_rm)\n\n\n\n\n\n\n\nQuesta funzione consente di visualizzare facilmente le distribuzioni e di valutare il bilanciamento tra le abilità delle persone e le difficoltà degli item.\nLa parte superiore della mappa persona-item mostra un istogramma delle stime dei parametri di abilità, mentre la parte inferiore mostra le stime delle difficoltà per ciascun item del test. Per ogni item, la stima della difficoltà è indicata dalla posizione del punto sulla linea tratteggiata corrispondente a quell’item. Ad esempio, la difficoltà stimata per l’item 1 corrisponde alla posizione del punto sulla linea tratteggiata più in alto. La mappa persona-item offre un controllo visivo di coerenza per le stime del nostro modello IRT (Teoria della Risposta all’Item). Le stime delle abilità sono più accurate quando cadono nel mezzo della distribuzione dei parametri degli item e viceversa. Pertanto, idealmente, l’istogramma delle abilità e le stime delle difficoltà dovrebbero essere centrate sullo stesso punto e mostrare un’ampia sovrapposizione. Nel nostro test, sembra essere questo il caso.\nIn alternativa, possiamo usare la funzione plotPImap() di eRm.\n\n71.4.2 ICC Empiriche\nLe Curve Caratteristiche degli Item (ICC) descrivono la relazione teorica tra l’abilità dei partecipanti al test e la probabilità di una risposta corretta che ci aspettiamo sotto il modello di Rasch per una data difficoltà. La ICC attesa per un item può essere tracciata dopo che la sua difficoltà è stata stimata. Oltre alle probabilità attese di una risposta corretta illustrate dall’ICC, possiamo anche tracciare le frequenze relative empiriche di una risposta corretta. Queste frequenze relative empiriche sono indicate nella figura come punti e vengono chiamate ICC empiriche.\nUsando eRm possimo generare le ECC empiriche nel modo seguente.\n\nplotICC(\n    rm_sum0, \n    item.subset = \"all\",\n    empICC = list(\"raw\"), \n    empCI = list()\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLe ICC empiriche sono rappresentate dai singoli punti, mentre la ICC attesa sotto il modello di Rasch è indicata dalla linea liscia. Dalle figure precedenti, per gli item 12 e 14 notiamo che in generale la forma dell’ICC empirica è molto ben allineata con l’ICC attesa, ma per l’item 12 l’ICC empirica mostra valori sopra zero anche per le abilità più basse a sinistra della dimensione latente. Questo potrebbe indicare una tendenza al tentativo di indovinare (guessing). Per l’item 19, l’ICC empirica appare più ripida dell’ICC attesa sotto il modello di Rasch. Mostra un salto molto più pronunciato tra la prima metà approssimativa dei punti e i punti rimanenti. Per l’item 21, al contrario, l’ICC empirica è molto più piatta rispetto a quella attesa. Confronteremo la nostra impressione visiva con le statistiche di adattamento degli item per questi item di seguito.\nÈ possibile visualizzare le ICC attese di tutti gli item del test in un unico grafico utilizzando la funzione plotjointICC(). Questo grafico ci permette di esaminare come la difficoltà influenzi la probabilità che un candidato risponda correttamente a un item. Ricordiamo che la difficoltà di un item è definita come il livello di abilità in cui una persona ha una probabilità del 50% di rispondere correttamente all’item. Abbiamo aggiunto una linea tratteggiata orizzontale alla probabilità di 0.5 usando il comando segments. Il punto in cui un’ICC interseca questa linea rappresenta la sua difficoltà. Questo ci permette di leggere facilmente le difficoltà relative degli item dal grafico. Spostandosi da sinistra a destra, il primo ICC intersecato dalla linea orizzontale corrisponde all’item meno difficile (in questo caso l’item 3, seguito da vicino dall’item 23, come indicato nell’ordine degli item nella legenda), e l’ultimo ICC intersecato dalla linea orizzontale è per l’item più difficile (in questo caso l’item 7).\nDa notare che le ICC attese nella figura sono parallele per definizione. Il modello di Rasch assume che le ICC siano parallele, quindi produrrà sempre ICC teoriche o attese parallele, anche quando gli item hanno in realtà pendenze o tassi di guessing diversi, come abbiamo visto in precedenza per le ICC empiriche.\n\neRm::plotjointICC(rm_sum0, cex = 0.7)\nsegments(-2.5, 0.5, 4, 0.5, lty = 2)\n\n\n\n\n\n\n\nIn alternativa, possiamo generare le ICC usando il pacchetto mirt:\n\nmirt_rm &lt;- mirt(responses, 1, \"Rasch\")\n#&gt; \nIteration: 1, Log-Lik: -2816.924, Max-Change: 0.03346\nIteration: 2, Log-Lik: -2816.650, Max-Change: 0.02041\nIteration: 3, Log-Lik: -2816.562, Max-Change: 0.01357\nIteration: 4, Log-Lik: -2816.520, Max-Change: 0.01027\nIteration: 5, Log-Lik: -2816.501, Max-Change: 0.00584\nIteration: 6, Log-Lik: -2816.493, Max-Change: 0.00395\nIteration: 7, Log-Lik: -2816.490, Max-Change: 0.00307\nIteration: 8, Log-Lik: -2816.488, Max-Change: 0.00174\nIteration: 9, Log-Lik: -2816.487, Max-Change: 0.00118\nIteration: 10, Log-Lik: -2816.487, Max-Change: 0.00094\nIteration: 11, Log-Lik: -2816.487, Max-Change: 0.00054\nIteration: 12, Log-Lik: -2816.487, Max-Change: 0.00036\nIteration: 13, Log-Lik: -2816.487, Max-Change: 0.00028\nIteration: 14, Log-Lik: -2816.487, Max-Change: 0.00016\nIteration: 15, Log-Lik: -2816.487, Max-Change: 0.00011\nIteration: 16, Log-Lik: -2816.487, Max-Change: 0.00008\nplot(mirt_rm, type = \"trace\")\n\n\n\n\n\n\n\nLe curve caratteristiche degli item offrono un quadro dettagliato e visuale di come ciascun item del test si comporta attraverso diversi livelli dell’abilità latente. Per esempio:\n\n\nVisualizzazione della Difficoltà e della Discriminazione:\n\nSupponiamo di avere un item che mostra una curva con una ripida salita in un punto specifico della scala di abilità. Questo indica che l’item ha una difficoltà concentrata attorno a quel punto e che discrimina efficacemente tra rispondenti con abilità appena al di sotto e al di sopra di quel livello.\nAl contrario, una curva più graduale suggerisce che l’item è meno discriminante, con una variazione più ampia nella probabilità di risposta corretta a seconda del livello di abilità.\n\n\n\nIdentificazione di Lacune nella Valutazione:\n\nVisualizzando le curve di più item, possiamo identificare se ci sono lacune nella copertura dell’abilità latente. Ad esempio, se tutti gli item hanno curve che si concentrano su livelli di abilità bassi, potrebbe esserci una mancanza di item difficili per misurare l’abilità ad alti livelli.\nInoltre, se le curve degli item si sovrappongono eccessivamente, potrebbe indicare ridondanza tra gli item, suggerendo che alcuni di essi non aggiungono informazioni uniche alla valutazione.\n\n\n\nConfronto tra Diversi Tipi di Item:\n\nPer esempio, gli item progettati per misurare concetti di base potrebbero avere curve che mostrano alta probabilità di risposta corretta anche a livelli di abilità bassi.\nAl contrario, item progettati per essere più impegnativi potrebbero mostrare probabilità elevate di risposta corretta solo a livelli di abilità più alti.\n\n\n\n71.4.3 Test Grafico\nIl test grafico del modello, basato sui principi di Rasch (1960), è un metodo intuitivo per valutare l’invarianza degli item in un test, confrontando i parametri degli item stimati per due gruppi di persone. Affinché il modello di Rasch sia considerato valido, è necessario che le stime dei parametri degli item per i diversi gruppi concordino, fino a una trasformazione lineare. In termini pratici, ciò si traduce nel fatto che, quando visualizzate in un grafico, le stime dei parametri degli item dei due gruppi dovrebbero allinearsi lungo una linea retta.\nPer complementare questa analisi, possiamo ricorrere al test del rapporto di verosimiglianza di Andersen (1973), un approccio ben consolidato per verificare l’adeguatezza del modello di Rasch nel rappresentare il comportamento dei partecipanti ai test. Il test di Andersen valuta se le stime dei parametri degli item rimangono consistenti tra diversi gruppi di partecipanti. Se i parametri degli item stimati individualmente per ciascun gruppo differiscono significativamente, ciò indica che il modello di Rasch potrebbe non essere un’adeguata rappresentazione del comportamento osservato nei test.\nA differenza del test grafico, il test del rapporto di verosimiglianza confronta il massimo della verosimiglianza condizionata sotto il modello di Rasch con il massimo della verosimiglianza condizionata quando i parametri degli item possono variare tra i gruppi. Questa metodologia offre un’indicazione di quanto efficacemente ciascun modello rappresenti il comportamento dei partecipanti.\nIl test del rapporto di verosimiglianza utilizza la statistica di test \\(T = −2 \\cdot log(LR)\\), che ha una distribuzione campionaria approssimativamente \\(\\chi^2\\) per campioni grandi. Valori del rapporto di verosimiglianza inferiori a 1, o valori elevati di T, suggeriscono una violazione del modello di Rasch.\nIl test di Andersen è implementato nel pacchetto eRm in R, offrendo uno strumento utile per l’analisi. Tuttavia, è importante notare che un risultato non significativo in questo test non può essere interpretato automaticamente come supporto per il modello di Rasch, specialmente se il modello più generale non descrive adeguatamente i dati. Inoltre, la capacità di rilevare differenze tra i gruppi specificati dipende dall’effettiva diversità dei parametri del modello tra questi gruppi. Sono stati messi a punto approcci più flessibili per rilevare le differenze nei parametri.\n\nlrt_mean_split &lt;- LRtest(rm_sum0, splitcr = \"mean\")\nlrt_mean_split\n#&gt; \n#&gt; Andersen LR-test: \n#&gt; LR-value: 79.7 \n#&gt; Chi-square df: 13 \n#&gt; p-value:  0\n\nL’output di questo test mostra una violazione significativa del modello di Rasch al livello \\(\\alpha\\) = 0.05.\n\nplotGOF(\n    lrt_mean_split,\n    tlab = \"item\", pos = 1,\n    main = \"Difficulty by Score (with Item Names)\",\n    conf = list(gamma = 0.95, col = 1)\n)\n\n\n\n\n\n\n\nOra possiamo tracciare le stime delle difficoltà di ciascun gruppo utilizzando la funzione plotGOF() per creare il test grafico. La funzione plotGOF() prende il risultato di LRtest() e traccia le stime dei parametri degli item per i due gruppi. Per facilitare la valutazione visiva, plotGOF() può opzionalmente etichettare gli item e aggiungere ellissi di confidenza.\nPer creare il grafico per il test grafico basato sulla divisione media, possiamo procedere in questo modo: ogni piccolo cerchio nella Figura mostra le stime delle difficoltà per un singolo item. La coordinata x di un cerchio indica la sua stima di difficoltà per i partecipanti al test con punteggi sotto la media e la sua coordinata y indica la stima di difficoltà per i partecipanti al test con punteggi sopra la media. La linea y = x è fornita come riferimento, poiché i punti che cadono su questa linea avrebbero la stessa stima in entrambi i gruppi. La distanza tra qualsiasi punto e la linea di riferimento y = x indica quanto le stime differiscono tra i due gruppi. Indica anche la direzione di questa differenza. Gli item sotto la linea sono più difficili per i partecipanti al test con punteggi sotto la media, mentre gli item sopra la linea sono più difficili per i partecipanti al test con punteggi sopra la media.\nGli assi orizzontali e verticali mostrano intervalli di confidenza per le stime per ciascun gruppo di partecipanti al test. La larghezza di ciascun intervallo di confidenza è determinata dall’elemento gamma della lista fornita a conf. L’impostazione predefinita gamma = .95 produce intervalli di confidenza al 95% per ciascun asse dell’ellisse. Quando un’ellisse di confidenza non incrocia la linea di riferimento, l’item rispettivo è diagnosticato come mostrante un significativo DIF.\nLa figura indica che gli item 2, 6, 21 e 22 differiscono significativamente tra le persone con punteggi sopra e sotto la media, poiché le loro ellissi di confidenza non incrociano la linea di riferimento. Gli item 21 e 22 sono più difficili per le persone con punteggi pari o superiori alla media, mentre gli item 2 e 6 sono più difficili per le persone con punteggi sotto la media. Tali violazioni del modello possono verificarsi quando le ICC osservate differiscono dalle ICC attese sotto il modello di Rasch per i partecipanti al test con abilità basse e alte. Questo può accadere, ad esempio, se è presente il tentativo di indovinare (guessing), o se la pendenza è più ripida o meno ripida di quanto previsto dal modello di Rasch.\nPossiamo anche fornire all’argomento splitcr una variabile che divide i partecipanti al test in gruppi. Ad esempio, possiamo testare se i parametri degli item differiscono in base al genere passando un vettore contenente le appartenenze di gruppo come argomento splitcr.\n\nlrt_gender &lt;- LRtest(rm_sum0, splitcr = gender)\nlrt_gender\n#&gt; \n#&gt; Andersen LR-test: \n#&gt; LR-value: 33 \n#&gt; Chi-square df: 13 \n#&gt; p-value:  0.002\n\nCome nel test precedente, anche il Test del Rapporto di Verosimiglianza (LRT) per il genere indica una violazione significativa del modello di Rasch al livello α = 0.05.\n\nplotGOF(\n    lrt_gender,\n    tlab = \"item\", pos = 1,\n    main = \"Difficulty by Score (with Item Names)\",\n    conf = list(gamma = 0.95, col = 1)\n)\n\n\n\n\n\n\n\nLa figura indica che gli item 2, 7 e 21 differiscono tra partecipanti al test femminili e maschili. Gli item 2 e 7 sono più difficili per i partecipanti femminili, mentre l’item 21 è più difficile per i partecipanti maschili.\n\n71.4.4 Test di Wald\nLe impostazioni del test del rapporto di verosimiglianza di Andersen (1973) e del test di Wald sono molto simili. Entrambi i test si basano sull’idea che il modello di Rasch sia un modello ragionevole per i dati dei test solo se i parametri degli item stimati non variano sistematicamente tra gruppi di persone. In entrambi i test, consideriamo le stime dei parametri degli item per ciascun gruppo di persone. A differenza del test del rapporto di verosimiglianza, tuttavia, il test di Wald confronta direttamente le stime dei parametri degli item dei gruppi. In sostanza, il test di Wald calcola la differenza tra la stima del primo gruppo della difficoltà dell’item i, β̂(1)i, e quella del secondo gruppo, β̂(2)i. Questa differenza viene divisa per il suo errore standard per tenere conto del fatto che tutte le stime sono soggette a rumore. Questo porta alla statistica di test per l’item i:\n\\[\nT_i = \\frac{\\hat{\\beta}^{(1)}_i - \\hat{\\beta}^{(2)}_i}{\\sqrt{se(\\hat{\\beta}^{(1)}_i)^2 + se(\\hat{\\beta}^{(2)}_i)^2}},\n\\]\ndove $ se(^{(1)}_i) $ e $ se(^{(2)}_i) $ indicano rispettivamente gli errori standard di $ ^{(1)}_i $ e $ ^{(2)}_i $.\nPer campioni di grandi dimensioni, $ T_i $ approssimativamente segue una distribuzione normale standard sotto l’ipotesi nulla che il vero parametro dell’item sia lo stesso per entrambi i gruppi. Valori estremi di $ T_i $ sono improbabili sotto la distribuzione normale. Quindi, un valore estremo di $ T_i $, con un piccolo valore p, indica che l’item i viola il modello di Rasch.\nEseguiamo il test con R:\n\nWaldtest(rm_sum0, splitcr = \"mean\")\n#&gt; \n#&gt; Wald test on item level (z-values):\n#&gt; \n#&gt;          z-statistic p-value\n#&gt; beta I1       -0.514   0.607\n#&gt; beta I2       -3.328   0.001\n#&gt; beta I3       -0.838   0.402\n#&gt; beta I6       -2.555   0.011\n#&gt; beta I7        0.210   0.834\n#&gt; beta I11      -1.773   0.076\n#&gt; beta I12       1.562   0.118\n#&gt; beta I14       1.821   0.069\n#&gt; beta I17       1.550   0.121\n#&gt; beta I18       0.333   0.739\n#&gt; beta I19      -1.827   0.068\n#&gt; beta I21       5.768   0.000\n#&gt; beta I22       4.106   0.000\n#&gt; beta I23      -1.560   0.119\n\nQuesti test indicano nuovamente che gli item 2, 6, 21 e 22 differiscono significativamente tra i partecipanti al test con punteggi sopra e sotto la media.\nPossiamo anche eseguire il test per la differenza tra maschi e femmine:\n\nWaldtest(rm_sum0, splitcr = gender)\n#&gt; \n#&gt; Wald test on item level (z-values):\n#&gt; \n#&gt;          z-statistic p-value\n#&gt; beta I1       -1.727   0.084\n#&gt; beta I2        2.543   0.011\n#&gt; beta I3       -1.020   0.308\n#&gt; beta I6        0.067   0.946\n#&gt; beta I7        3.089   0.002\n#&gt; beta I11      -1.978   0.048\n#&gt; beta I12      -0.861   0.389\n#&gt; beta I14      -0.673   0.501\n#&gt; beta I17       0.815   0.415\n#&gt; beta I18      -0.493   0.622\n#&gt; beta I19       0.583   0.560\n#&gt; beta I21      -2.305   0.021\n#&gt; beta I22      -0.030   0.976\n#&gt; beta I23      -1.019   0.308\n\nI risultati qui concordano in gran parte anche con la figura precedente. In linea con il test grafico, il test di Wald indica che gli item 2, 7 e 21 differiscono tra i gruppi.\n\n71.4.5 Ancoraggio\nL’ancoraggio è una procedura cruciale quando si confrontano le stime dei parametri degli item tra diversi gruppi, un passo fondamentale in test come il Wald e in metodi grafici. Tale processo necessita di particolare attenzione perché implica la restrizione di alcuni parametri degli item per allineare le scale latenti tra i gruppi. Ad esempio, fissare il parametro del primo item a zero in entrambi i gruppi crea un punto di riferimento comune, ma anche limitazioni.\nLa scelta degli item di ancoraggio è delicata: fissare un parametro in entrambi i gruppi significa non poter più valutare la differenza per quell’item specifico. La selezione dovrebbe essere guidata da un’attenta analisi dei dati e da considerazioni teoriche. Approcci guidati dai dati sono stati proposti per identificare item invarianti o escludere quelli con DIF, processo noto come purificazione. Tuttavia, occorre cautela: anche metodi ben progettati possono portare a conclusioni errate se gli item di ancoraggio scelti sono inappropriati.\nIn pratica, spesso si adotta una restrizione in cui la somma dei parametri degli item è zero per tutti i gruppi. Questo approccio, adottato da pacchetti software come eRm e difR in R, si basa sull’assunzione che eventuali DIF si annullino su tutti gli item. Ma se questa assunzione non è valida, o se l’ancoraggio include item con DIF, potremmo incorrere in errori interpretativi.\nIn sintesi, l’ancoraggio è una strategia potente ma che richiede un’attenta considerazione e un’analisi critica. È fondamentale non solo selezionare gli item di ancoraggio adeguati ma anche interpretare i risultati con una comprensione chiara delle ipotesi e delle potenziali limitazioni del metodo scelto.\n\nresp &lt;- as.matrix(responses)\nanchortest(\n    resp ~ gender,\n    class = \"constant\",\n    select = \"MPT\"\n)\n#&gt; Anchor items:\n#&gt; respI23, respI3, respI22, respI18\n#&gt; \n#&gt; Final DIF tests:\n#&gt; \n#&gt;   Simultaneous Tests for General Linear Hypotheses\n#&gt; \n#&gt; Linear Hypotheses:\n#&gt;              Estimate Std. Error z value Pr(&gt;|z|)\n#&gt; respI1 == 0    0.2422     0.2900    0.84   0.4037\n#&gt; respI2 == 0   -0.8110     0.2923   -2.77   0.0055\n#&gt; respI3 == 0    0.1253     0.2667    0.47   0.6384\n#&gt; respI6 == 0   -0.1964     0.2709   -0.72   0.4685\n#&gt; respI7 == 0   -1.7745     0.5717   -3.10   0.0019\n#&gt; respI11 == 0   0.3185     0.2971    1.07   0.2837\n#&gt; respI12 == 0   0.0195     0.2821    0.07   0.9449\n#&gt; respI14 == 0  -0.0289     0.2734   -0.11   0.9160\n#&gt; respI17 == 0  -0.3926     0.3080   -1.27   0.2025\n#&gt; respI18 == 0  -0.0705     0.2203   -0.32   0.7492\n#&gt; respI19 == 0  -0.3641     0.3621   -1.01   0.3147\n#&gt; respI21 == 0   0.4477     0.3216    1.39   0.1639\n#&gt; respI22 == 0  -0.1710     0.3019   -0.57   0.5711\n#&gt; (Univariate p values reported)\n\n\nanchortest(\n    resp ~ gender,\n    class = \"forward\",\n    select = \"MTT\"\n)\n#&gt; Anchor items:\n#&gt; respI23, respI12, respI18, respI14, respI6, respI1, respI11,\n#&gt; respI19, respI17\n#&gt; \n#&gt; Final DIF tests:\n#&gt; \n#&gt;   Simultaneous Tests for General Linear Hypotheses\n#&gt; \n#&gt; Linear Hypotheses:\n#&gt;              Estimate Std. Error z value Pr(&gt;|z|)\n#&gt; respI1 == 0    0.2818     0.2362    1.19   0.2328\n#&gt; respI2 == 0   -0.7714     0.2660   -2.90   0.0037\n#&gt; respI3 == 0    0.1649     0.3229    0.51   0.6097\n#&gt; respI6 == 0   -0.1568     0.2157   -0.73   0.4671\n#&gt; respI7 == 0   -1.7349     0.5574   -3.11   0.0019\n#&gt; respI11 == 0   0.3580     0.2432    1.47   0.1410\n#&gt; respI12 == 0   0.0591     0.2262    0.26   0.7940\n#&gt; respI14 == 0   0.0107     0.2188    0.05   0.9610\n#&gt; respI17 == 0  -0.3530     0.2514   -1.40   0.1603\n#&gt; respI18 == 0  -0.0309     0.2175   -0.14   0.8871\n#&gt; respI19 == 0  -0.3245     0.3030   -1.07   0.2841\n#&gt; respI21 == 0   0.4872     0.2952    1.65   0.0989\n#&gt; respI22 == 0  -0.1314     0.3717   -0.35   0.7237\n#&gt; (Univariate p values reported)\n\n\nanchortest(\n    resp ~ gender,\n    select = \"Gini\"\n)\n#&gt; Anchor items:\n#&gt; respI23\n#&gt; \n#&gt; Final DIF tests:\n#&gt; \n#&gt;   Simultaneous Tests for General Linear Hypotheses\n#&gt; \n#&gt; Linear Hypotheses:\n#&gt;              Estimate Std. Error z value Pr(&gt;|z|)\n#&gt; respI1 == 0   0.12610    0.38737    0.33   0.7448\n#&gt; respI2 == 0  -0.92706    0.38904   -2.38   0.0172\n#&gt; respI3 == 0   0.00921    0.42718    0.02   0.9828\n#&gt; respI6 == 0  -0.31249    0.37656   -0.83   0.4066\n#&gt; respI7 == 0  -1.89057    0.63210   -2.99   0.0028\n#&gt; respI11 == 0  0.20238    0.39228    0.52   0.6059\n#&gt; respI12 == 0 -0.09661    0.38679   -0.25   0.8028\n#&gt; respI14 == 0 -0.14496    0.37695   -0.38   0.7006\n#&gt; respI17 == 0 -0.50866    0.40738   -1.25   0.2118\n#&gt; respI18 == 0 -0.18656    0.37641   -0.50   0.6202\n#&gt; respI19 == 0 -0.48019    0.45080   -1.07   0.2868\n#&gt; respI21 == 0  0.33158    0.41809    0.79   0.4277\n#&gt; respI22 == 0 -0.28708    0.47621   -0.60   0.5466\n#&gt; (Univariate p values reported)\n\nGli output di R della funzione anchortest() elencano gli item di ancoraggio selezionati dai rispettivi approcci di selezione dell’ancoraggio, oltre ai risultati del test di Wald basati su questi item di ancoraggio. Tutti e tre gli approcci portano a risultati in cui solo gli item 2 e 7 mostrano DIF per genere, mentre il test grafico e il test di Wald in eRm hanno identificato anche l’item 21 e l’item 11 (al limite) come aventi DIF.\nRiesaminando il test grafico nella figura precedente, notiamo che gli item 2 e 7 mostrano DIF nella stessa direzione (sopra la diagonale), mentre gli item 21 e 11 sono orientati nella direzione opposta (sotto la diagonale) e in misura minore.\nConsiderando questi risultati nel loro insieme, si può concludere che potrebbe essere presente un DIF non bilanciato e che la diagonale usata nella Figura 6.4 non è ideale per valutare gli item. Per illustrare ciò, tracciamo manualmente una linea di riferimento alternativa attraverso la posizione dell’item 23, che è stato selezionato come item di ancoraggio (primario) dai tre approcci presentati in psychotools, utilizzando il comando abline.\n\n plotGOF(\n    lrt_gender,\n    tlab = \"item\", pos = 1,\n    main = \"Difficulty by Gender (with Item Names)\",\n    conf = list(gamma = 0.95, col = 1)\n)\nabline(-0.3, 1, lty=2)\n\n\n\n\n\n\n\nCome si può vedere nella figura risultante, basandoci sulla linea di riferimento alternativa, non troviamo più DIF negli item 11 e 21, ma gli item 2 e 7 mostrano ancora più chiaramente un DIF.\nPer questo set di dati, la stessa conclusione viene raggiunta in eRm quando si utilizza la funzione stepwiseIt(), che esegue diversi test di Wald e ad ogni passo esclude l’item singolo con la statistica di test più grande.\n\nstepwiseIt(rm_sum0, criterion = list(\"Waldtest\", gender))\n#&gt; Eliminated item - Step 1: I7\n#&gt; Eliminated item - Step 2: I2\n#&gt; \n#&gt; Results for stepwise item elimination:\n#&gt; Number of steps: 2 \n#&gt; Criterion: Waldtest\n#&gt; \n#&gt;            z-statistic p-value\n#&gt; Step 1: I7        3.09   0.002\n#&gt; Step 2: I2        3.06   0.002\n\nUtilizzando questo metodo, dopo l’esclusione degli item 7 e 2, che presentavano il DIF più marcato, non si rilevano più differenze significative nei test degli item rimanenti. Per visualizzare meglio questo processo, immaginiamo la figura precedente: inizialmente, la linea di riferimento corrisponde alla diagonale solida. Tuttavia, dopo aver eliminato l’item 7, questa linea si sposta verso quella tratteggiata nel secondo passaggio e, rimuovendo poi l’item 2, si allinea o si avvicina molto alla linea tratteggiata nel terzo passaggio. Di conseguenza, gli item restanti non mostrano più un DIF significativo.\nIn sintesi, mentre i test grafici e di Wald basati sulla restrizione della somma zero possono risultare ingannevoli in presenza di un DIF non bilanciato, l’impiego di metodi di ancoraggio avanzati e l’approccio di eliminazione graduale degli item possono offrire una visione più accurata e dettagliata della situazione.\n\n71.4.6 Rimozione di item\nSe questa analisi facesse parte della costruzione di un test reale, gli item che mostrano DIF (o altre anomalie nelle analisi successive) dovrebbero essere attentamente esaminati da esperti di contenuto per decidere se modificarli o rimuoverli dal test. Nella discussione seguente, tuttavia, non rimuoveremo gli item perché desideriamo mantenere il set di dati completo. Tuttavia, se si desiderasse rimuovere alcuni item (ovvero colonne) dal set di dati, ciò potrebbe essere fatto con i seguenti comandi.\n\nresponses_removeDIFitems &lt;- \n  responses[, -which(colnames(responses) %in% c(\"I2\", \"I7\"))]\ncolnames(responses_removeDIFitems)\n#&gt;  [1] \"I1\"  \"I3\"  \"I6\"  \"I11\" \"I12\" \"I14\" \"I17\" \"I18\" \"I19\" \"I21\" \"I22\" \"I23\"\n\nDopo aver rimosso degli item, l’intero processo dovrebbe ricominciare da capo, rifacendo il modello di Rasch e indagando sugli item rimanenti.\n\n71.4.7 Test di Martin-Löf\nNella sezione precedente, abbiamo visto che il test del rapporto di verosimiglianza di Andersen (1973) verifica l’ipotesi che i parametri degli item siano invarianti per vari gruppi di persone. Una ipotesi correlata riguarda l’invarianza dei parametri delle persone per diversi gruppi di item.\nQui, la domanda fondamentale è se diversi gruppi di item misurino tratti latenti differenti. Ciò rappresenterebbe una violazione del modello di Rasch, il quale implica un singolo tratto latente alla base di tutti gli item. Se questo tipo di violazione del modello viene rilevato, un modello IRT multidimensionale potrebbe essere più appropriato.\nUn metodo comune per valutare la dimensionalità in generale è l’analisi fattoriale esplorativa. Qui invece descriveremo il test di Martin-Löf che affronta l’ipotesi alternativa secondo cui gruppi di item misurano tratti latenti differenti ed è disponibile nel pacchetto eRm. Come il test del rapporto di verosimiglianza di Andersen, questo test si basa sul confronto di due verosimiglianze condizionate. La prima verosimiglianza condizionata Lu(r,β) è quella del modello di Rasch. La seconda verosimiglianza condizionata Lu(r1, r2, β) è nuovamente quella di un modello più generale che ora permette diversi parametri di persona per specifici gruppi di item. I gruppi di item devono essere definiti prima dell’analisi, il che può essere fatto in base alle loro difficoltà (cioè, testiamo item facili contro difficili) o in base a diverse dimensioni latenti che si sospetta siano misurate dai gruppi di item (cioè, il gruppo di item 1 è sospettato di misurare una dimensione latente diversa rispetto al gruppo di item 2). Se la seconda verosimiglianza è maggiore, ciò indica una violazione del modello di Rasch (analogamente al test del rapporto di verosimiglianza di Andersen).\nIl test di Martin-Löf è spesso descritto come un test per la unidimensionalità. Certi tipi di multidimensionalità possono anche manifestarsi come DIF. Per questa ragione, i test che mirano a rilevare il DIF, possono anche essere sensibili a certe violazioni della unidimensionalità.\n\nmloef_median &lt;- MLoef(rm_sum0, splitcr = \"median\")\nmloef_median\n#&gt; \n#&gt; Martin-Loef-Test (split criterion: median)\n#&gt; LR-value: 67.083 \n#&gt; Chi-square df: 48 \n#&gt; p-value: 0.036\n\nOtteniamo un valore p inferiore a 0.05. Ciò indica che le stime dei parametri delle persone ottenute dagli item facili e difficili differiscono in modo significativo, ovvero, una violazione del modello di Rasch.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_implementation.html#item-e-person-fit",
    "href": "chapters/irt/05_implementation.html#item-e-person-fit",
    "title": "71  Implementazione",
    "section": "\n71.5 Item e Person Fit",
    "text": "71.5 Item e Person Fit\n\n71.5.1 Tests e Statistiche di Bontà di Adattamento\nIn questa sezione esaminiamo una varietà di metodi per valutare l’adattamento dei dati di risposta agli item e al modello di Rasch. Alcuni di questi metodi sono test statistici formali, mentre altri sono statistiche descrittive per le quali sono stati suggeriti nella letteratura dei limiti critici empirici. Vedremo anche che esistono approcci per valutare l’adattamento a livello dell’intero test psicologico, così come approcci focalizzati sulla valutazione dell’adattamento di singoli item o individui.\n\n71.5.2 Test di Bontà di Adattamento χ2 e G2\nNella valutazione del modello di Rasch, esaminiamo due classi principali di test di bontà di adattamento: il test χ2 e il test G2, entrambi noti nell’analisi delle tabelle di contingenza. A differenza dei test del rapporto di verosimiglianza o dei test di Martin-Löf, il test χ2 non confronta l’adattamento relativo di due modelli. Piuttosto, esso valuta quanto accuratamente i modelli di risposta previsti dal modello di Rasch corrispondano ai modelli di risposta osservati. Questo avviene attraverso il confronto tra il numero di partecipanti che mostrano ciascun modello di risposta osservato e il numero previsto dal modello di Rasch.\nIl principio dei test di bontà di adattamento χ2 per il modello di Rasch è basato sull’analisi di tutti i possibili modelli di risposta (combinazioni di 0 e 1 per risposte errate e corrette). Definiamo Ou come il numero osservato di partecipanti con il modello di risposta u e Eu come il numero previsto sotto il modello di Rasch. La statistica del test χ2 è data da:\n\\[ T = \\sum_{u} \\frac{(O_u - E_u)^2}{E_u} \\]\nIn questa formula, le differenze tra osservazioni e previsioni sono elevate al quadrato e poi ponderate inversamente rispetto alla frequenza attesa. In campioni di grandi dimensioni, T segue approssimativamente una distribuzione χ2, se il modello di Rasch è appropriato. Valori alti di T indicano una cattiva adattazione del modello.\nTuttavia, il test χ2 richiede che ogni modello di risposta abbia una frequenza attesa sufficientemente alta, una condizione spesso non soddisfatta in test con molti item. In questi casi, il test χ2 non segue una distribuzione χ2 sotto l’ipotesi nulla, rendendolo poco pratico. Una soluzione potrebbe essere quella di raggruppare i modelli di risposta per aumentare le frequenze attese.\nParallelamente, la statistica del rapporto di verosimiglianza G2, anch’essa derivante dall’analisi dei dati categoriali, è calcolata come:\n\\[ G^2 = 2 \\sum_{u} O_u \\log \\left( \\frac{O_u}{E_u} \\right) \\]\nG2 confronta le frequenze osservate con quelle attese, anziché le verosimiglianze di due modelli. Se le frequenze attese sono vicine a quelle osservate, il rapporto \\(\\frac{O_u}{E_u}\\) si avvicina a 1, rendendo il logaritmo naturale \\(\\log\\left(\\frac{O_u}{E_u}\\right)\\) vicino a 0 e la statistica G2 tende a 0, indicando un buon adattamento. Anche G2 segue una distribuzione χ2 se il modello di Rasch è appropriato. Tuttavia, proprio come per il test χ2, G2 è praticabile solo con grandi frequenze attese, limitandone l’uso effettivo. Nonostante ciò, G2 è importante da comprendere poiché molte altre statistiche di test si basano su di esso.\n\n71.5.3 Statistica M2\nLa statistica M2, sviluppata da Maydeu-Olivares e Joe (2006), affronta il problema dei modelli di risposta rari che possono complicare i test χ2. Invece di confrontare le frequenze di interi modelli di risposta, la statistica M2 utilizza le informazioni provenienti dagli item individuali e dalle coppie di item. Specificatamente, confronta: 1. Le frequenze attese e osservate delle risposte corrette agli item individuali. 2. Le frequenze attese e osservate delle risposte corrette a entrambi gli item in una coppia di item.\nPer esempio, con due item, confronterebbe le frequenze osservate e attese per una risposta corretta al primo item, al secondo item e ad entrambi gli item insieme. Questo approccio è simile all’analisi delle tabelle di frequenza per le coppie di item. La statistica M2, come il test di bontà di adattamento χ2, implica un cattivo adattamento tra i dati e il modello di Rasch se produce un valore elevato o, equivalentemente, un valore p piccolo. Senza violazione del modello, la statistica M2 segue approssimativamente una distribuzione χ2 con gradi di libertà calcolati come $ k - d $, dove $ k $ è il numero di frequenze confrontate e $ d $ è il numero di parametri liberi del modello.\n\n71.5.4 Errore Quadratico Medio di Approssimazione (RMSEA)\nIl RMSEA deriva dalla statistica M2. Utilizza i gradi di libertà (nuovamente $ k - d $) e la dimensione del campione $ P $ per calcolare il valore RMSEA. La formula per il RMSEA è:\n\\[ \\text{RMSEA} = \\sqrt{\\frac{M2 - df}{P \\cdot df}} \\]\nValori di RMSEA vicini a 0 generalmente indicano un buon adattamento del modello ai dati. Sebbene non esistano linee guida universalmente accettate per interpretare il RMSEA, un valore intorno a 0,05 è spesso considerato indicativo di un buon adattamento del modello.\n\n71.5.5 Residuo Quadratico Medio Standardizzato (SRMSR)\nSRMSR è un’altra statistica di adattamento complessivo che confronta le correlazioni o le covarianze osservate tra tutte le coppie di item con quelle previste sotto il modello di Rasch (o un altro modello della teoria della risposta agli item). Valori vicini a 0 suggeriscono un buon adattamento del modello. Maydeu-Olivares (2013) raccomanda l’uso di un valore di soglia di 0.05 per SRMSR, simile al RMSEA.\nNel complesso, queste statistiche (M2, RMSEA e SRMSR) sono utili per valutare l’adattamento di un modello, come il modello di Rasch, a un dato insieme di dati di risposta agli item. Forniscono diverse prospettive attraverso le quali la congruenza tra i dati e il modello teorico può essere valutata, ognuna con il suo focus unico e metodo di calcolo.\n\nfit_rasch &lt;- mirt(responses, 1, itemtype = \"Rasch\", verbose = FALSE)\nfit_rasch\n#&gt; \n#&gt; Call:\n#&gt; mirt(data = responses, model = 1, itemtype = \"Rasch\", verbose = FALSE)\n#&gt; \n#&gt; Full-information item factor analysis with 1 factor(s).\n#&gt; Converged within 1e-04 tolerance after 16 EM iterations.\n#&gt; mirt version: 1.44.0 \n#&gt; M-step optimizer: nlminb \n#&gt; EM acceleration: Ramsay \n#&gt; Number of rectangular quadrature: 61\n#&gt; Latent density type: Gaussian \n#&gt; \n#&gt; Log-likelihood = -2816\n#&gt; Estimated parameters: 15 \n#&gt; AIC = 5663\n#&gt; BIC = 5723; SABIC = 5675\n#&gt; G2 (16368) = 1319, p = 1\n#&gt; RMSEA = 0, CFI = NaN, TLI = NaN\n\n\nM2(fit_rasch)\n#&gt;        M2 df p  RMSEA RMSEA_5 RMSEA_95  SRMSR   TLI   CFI\n#&gt; stats 278 90 0 0.0723  0.0626   0.0819 0.0941 0.749 0.752\n\nLa statistica M2 è alta e significativa, indicando che ci sono differenze preoccupanti tra il modello e i dati. Questo è ulteriormente supportato da un RMSEA troppo alto e da un CFA e TLI lontani da 1.\nRicordiamo il significato degli indici RMSEA, CFA e TLI.\nRMSEA (Root Mean Square Error of Approximation): - Il RMSEA è una misura di adattamento che valuta quanto bene un modello si adatta ai dati a livello di popolazione. - Un valore basso di RMSEA indica un buon adattamento, suggerendo che il modello approssima bene la realtà. - Generalmente, un RMSEA inferiore a 0.05 o 0.06 è considerato indicativo di un ottimo adattamento del modello.\nCFA (Comparative Fit Index): - Il CFA è un indice relativo di bontà di adattamento che confronta il modello specificato con un modello nullo o di base. - Valori più vicini a 1 indicano un adattamento migliore. Un CFA superiore a 0.90 o 0.95 è spesso considerato indicativo di un buon adattamento.\nTLI (Tucker-Lewis Index): - Simile al CFA, il TLI è un altro indice relativo di adattamento che tiene conto della complessità del modello. - Anche per il TLI, valori più vicini a 1 indicano un adattamento migliore. Valori superiori a 0.90 o 0.95 sono generalmente considerati buoni.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_implementation.html#valutare-ladattamento-degli-item",
    "href": "chapters/irt/05_implementation.html#valutare-ladattamento-degli-item",
    "title": "71  Implementazione",
    "section": "\n71.6 Valutare l’Adattamento degli Item",
    "text": "71.6 Valutare l’Adattamento degli Item\nTuttavia, nell’IRT, ci interessiamo maggiormente agli indici di adattamento degli item e delle persone. L’IRT ci consente di valutare quanto bene ogni item si adatti al modello e se i pattern di risposta individuali sono allineati con il modello.\nIniziamo con l’addattamento agli item. Sono stati proposti diversi indici per valutare l’adattamento degli item e possiamo utilizzare la funzione itemfit() per ottenere una varietà di questi indici. Di default, riceviamo l’S_X2 di Orlando e Thissen (2000) con i corrispondenti gradi di libertà (dfs), RMSEA e valori p. Questo test dovrebbe risultare non significativo per indicare un buon adattamento dell’item. Come vediamo qui sotto, diversi item mostra un cattivo adattamento.\n\nitemfit(fit_rasch)\n#&gt;    item  S_X2 df.S_X2 RMSEA.S_X2 p.S_X2\n#&gt; 1    I1  4.15       7      0.000  0.762\n#&gt; 2    I2 19.33       7      0.066  0.007\n#&gt; 3    I3  5.53       7      0.000  0.596\n#&gt; 4    I6 14.09       8      0.044  0.080\n#&gt; 5    I7  8.98       7      0.027  0.254\n#&gt; 6   I11 23.36       7      0.077  0.001\n#&gt; 7   I12 17.67       7      0.062  0.014\n#&gt; 8   I14  9.79       7      0.032  0.201\n#&gt; 9   I17 35.13       7      0.100  0.000\n#&gt; 10  I18  3.70       7      0.000  0.814\n#&gt; 11  I19 20.93       7      0.071  0.004\n#&gt; 12  I21 86.54       7      0.169  0.000\n#&gt; 13  I22 73.46       7      0.154  0.000\n#&gt; 14  I23  7.61       7      0.015  0.368\n\n\n71.6.1 Statistiche di Infit e Outfit\nNella sezione precedente abbiamo discusso i test χ2 e M2, basati sul confronto tra le frequenze osservate e quelle attese secondo il modello di Rasch. Le statistiche di adattamento presentate di seguito si basano su un approccio simile, utilizzando i residui di Rasch. Questi sono le differenze tra le risposte osservate (vale a dire, le risposte 0 o 1 per gli item dicotomici) e i loro valori attesi (cioè, le probabilità predette di una risposta corretta secondo il modello di Rasch). Tipicamente, questi valori attesi vengono calcolati in base alle stime dei parametri degli item e delle persone.\nGeneralmente, quando c’è un buon adattamento tra i dati e il modello, si può prevedere che i residui siano piccoli. Pertanto, è naturale che i residui di Rasch possano essere utilizzati per valutare l’adattamento del modello di Rasch. Vedremo che nell’analisi di Rasch non solo i casi in cui i residui sono più grandi del previsto possono essere motivo di preoccupazione, ma anche quelli in cui i residui sono più piccoli del previsto.\nUn approccio comune per verificare l’adattamento di singoli item usando i residui di Rasch consiste nel calcolare le statistiche di infit e outfit. Descriveremo i passaggi per calcolare queste statistiche prima di affrontarne l’interpretazione. Ci concentreremo sul caso in cui queste statistiche vengono calcolate per singoli item.\n\n71.6.1.1 Outfit\nLa funzione principale della statistica di outfit è quella di quantificare in che misura le risposte dei partecipanti si allontanano dalle previsioni del modello. Questo indice si calcola attraverso diversi passaggi, che mirano a stabilire la misura in cui le risposte individuali si discostano dalle aspettative teoriche.\n1. Definizione dei Residui di Rasch: Inizialmente, per ogni partecipante e per ciascun item del test, si calcola il residuo di Rasch. Un residuo è essenzialmente la differenza tra la risposta osservata di un individuo a un determinato item e la risposta prevista da quel partecipante per lo stesso item. La risposta prevista è calcolata sulla base della probabilità, fornita dal modello di Rasch, che il partecipante risponda correttamente all’item. Ad esempio, se il modello prevede che un partecipante abbia il 40% di probabilità di rispondere correttamente a un item e il partecipante risponde effettivamente correttamente, il residuo corrispondente sarà $ 1 - 0.40 = 0.60 $.\n2. Standardizzazione dei Residui di Rasch: Successivamente, questi residui vengono standardizzati. La standardizzazione implica l’adeguamento dei residui in modo che abbiano una media di zero e una varianza di uno. Ciò permette di confrontare i residui in maniera uniforme, indipendentemente dalle caratteristiche specifiche degli item o dei partecipanti.\n3. Calcolo dello Z-Score: Per ciascun residuo, si calcola lo z-score standardizzato, $ Z_{si} $, utilizzando la formula:\n\\[\n   Z_{si} = \\frac{X_{si} - E(X_{si})}{\\sqrt{Var(X_{si})}},\n   \\]\ndove $ Z_{si} $ rappresenta lo z-score del residuo per il partecipante $ s $ all’item $ i $, $ X_{si} $ è la risposta osservata, $ E(X_{si}) $ è la risposta attesa (basata sulla probabilità di una risposta corretta secondo il modello di Rasch), e $ Var(X_{si}) $ è la varianza della risposta attesa.\n4. Calcolo della Statistica di Outfit: Per calcolare la statistica di outfit mean square (MSQ) per un specifico item, si seguono questi passaggi: - Si elevano al quadrato gli z-score standardizzati di ogni partecipante per l’item in questione. - Si sommano tutti questi valori quadrati. - Si divide la somma ottenuta per il numero totale dei partecipanti.\nLa formula risultante per la statistica di outfit MSQ per l’item $ i $ è la seguente:\n\\[\n   \\text{Outfit MSQ}_i = \\frac{\\sum_{p=1}^{P} Z_{pi}^2}{P}.\n   \\]\nQuesta procedura fornisce una misura dell’adattamento delle risposte degli individui all’item specifico, rispetto alle previsioni del modello di Rasch. Un valore di MSQ significativamente alto o basso può indicare potenziali discrepanze tra le risposte osservate e quelle previste, suggerendo la necessità di ulteriori analisi o revisioni del modello o degli item del test.\nSecondo Wright e Masters (1990), questa statistica ha un valore atteso di 1 sotto il modello di Rasch. Valori superiori a 1 indicano residui di Rasch più grandi del previsto secondo il modello di Rasch, e quindi una possibile violazione del modello. Tali item vengono anche detti mostrare un underfit. Valori inferiori a 1 indicano che i residui sono inferiori al previsto. Ciò è considerato indicare un overfit delle risposte al modello di Rasch. In questo contesto, overfit significa che la deviazione tra i valori attesi e i dati empirici è minore del previsto.\nPossiamo inoltre ottenere una statistica di mean square pesata e standardizzata per ciascun item, tipicamente denotata da ti. Siano \\(\\sqrt[3]{\\text{MSQ}_i}\\) e sd(MSQ_i) il cubo radice e la deviazione standard attesa di Outfit MSQ_i, rispettivamente. Allora la statistica standardizzata ti è\n\\[\n\\text{Outfit ti} = \\left( \\sqrt[3]{\\text{MSQ}_i} - 1 \\right) \\left( \\frac{3}{\\text{sd(MSQ}_i)} \\right) + \\left( \\frac{\\text{sd(MSQ}_i)}{3} \\right).\n\\]\nQuesta statistica standardizzata ti è spesso presentata nei risultati del software in aggiunta alla statistica MSQ.\nItem che mostrano underfit e overfit possono anche essere identificati approssimativamente usando le loro ICC empiriche, come abbiamo già visto in precedenza. Gli item che mostrano underfit hanno ICC empiriche più piatte di quelle previste sotto il modello di Rasch. Gli item che mostrano overfit hanno ICC empiriche più ripide del previsto.\n\n71.6.1.2 Infit\nL’indice di infit è un altro indice critico nel modello di Rasch. A differenza dell’outfit, che è più influenzato da risposte casuali o outlier, l’infit è più sensibile alle risposte che sono incoerenti con il pattern generale del modello. L’infit è calcolato come una media ponderata dei residui standardizzati, dove i pesi sono inversamente proporzionali alla varianza degli item. Questo rende l’infit particolarmente utile per identificare problemi di adattamento del modello legati alla consistenza interna delle risposte.\nLa statistica di infit MSQ, come quella di outfit, serve a valutare l’adattamento delle risposte individuali rispetto alle aspettative teoriche del modello. Tuttavia, la statistica di infit differisce dall’outfit per il modo in cui tratta i residui.\n1. Ponderazione dei Residui di Rasch: Nella statistica di infit, i residui di Rasch delle risposte individuali vengono ponderati in base alla loro varianza attesa sotto il modello di Rasch. Ciò significa che i residui con varianze minori (che tendono a verificarsi quando c’è una grande distanza tra le abilità dei rispondenti e la difficoltà degli item) hanno un impatto relativamente minore sulla statistica di infit rispetto a quelli con varianze maggiori.\n2. Riduzione dell’Impatto degli Outlier: Questo approccio di ponderazione rende la statistica di infit meno sensibile agli outlier rispetto all’outfit. In altre parole, mentre la statistica di outfit è influenzata in maniera più uniforme da tutte le deviazioni dalle aspettative del modello, l’infit dà maggiore peso alle deviazioni che sono meno estreme o più prevedibili data la struttura del modello.\n3. Formula per la Statistica di Infit MSQ: La formula per calcolare l’Infit MSQ per un dato item $ i $ è la seguente:\n\\[\n   \\text{Infit MSQ}_i = \\frac{\\sum_{p=1}^{P} W_{pi} Z_{pi}^2}{\\sum_{p=1}^{P} W_{pi}},\n   \\]\ndove: - $ Z_{pi} $ rappresenta il residuo di Rasch standardizzato per il rispondente $ p $ all’item $ i $. - $ W_{pi} $ è la varianza attesa del residuo $ Z_{pi} $ sotto il modello di Rasch. - $ P $ è il numero totale dei rispondenti.\n4. Standardizzazione della Statistica Infit: Come per l’outfit, è anche possibile calcolare una versione standardizzata dell’Infit MSQ per ogni item. Questa versione standardizzata, nota come statistica Infit t, consente di confrontare più facilmente l’adattamento degli item in diverse situazioni o in diversi test, normalizzando i valori su una scala comune.\nIn sintesi, la statistica di infit MSQ offre un modo ponderato per valutare l’adattamento delle risposte ai singoli item in un test basato sul modello di Rasch, tenendo conto della varianza attesa delle risposte. Questo la rende particolarmente utile per identificare i casi in cui le risposte si discostano dalle previsioni del modello in modi meno estremi o più in linea con la struttura del modello stesso.\n\n71.6.1.3 Soglie\nPer entrambi i valori MSQ e t delle statistiche di infit e outfit, sono stati proposti vari valori di soglia. Bond e Fox (2007) e Engelhard (2013) menzionano valori di soglia di -2 e 2 per le statistiche t, mentre Paek e Cole (2020) suggeriscono -3 e 3. Analogamente, Bond e Fox (2007) danno 0.75 e 1.3 come valori di soglia per le statistiche MSQ, mentre DeMars (2010) menziona 0.6 e 1.5 come possibili alternative. Desjardins e Bulut (2018), d’altra parte, si oppongono all’uso di valori di soglia specifici per queste statistiche.\nPossiamo calcolare le statistiche infit e oputfit degli item usando il pacchetto eRm:\n\nrm_sum0 &lt;- RM(responses)\neRm::itemfit(person.parameter(rm_sum0))\n#&gt; \n#&gt; Itemfit Statistics: \n#&gt;     Chisq  df p-value Outfit MSQ Infit MSQ Outfit t Infit t Discrim\n#&gt; I1    325 397   0.996      0.818     0.904   -1.750  -1.666   0.418\n#&gt; I2    274 397   1.000      0.688     0.809   -2.928  -3.271   0.520\n#&gt; I3    289 397   1.000      0.727     0.869   -1.574  -1.505   0.371\n#&gt; I6    334 397   0.991      0.838     0.860   -2.317  -3.257   0.505\n#&gt; I7    273 397   1.000      0.686     0.838   -1.328  -1.423   0.279\n#&gt; I11   333 397   0.992      0.836     0.816   -1.432  -3.143   0.473\n#&gt; I12   459 397   0.018      1.152     0.972    1.538  -0.538   0.321\n#&gt; I14   396 397   0.508      0.994     1.023   -0.043   0.492   0.320\n#&gt; I17   524 397   0.000      1.317     0.936    2.290  -1.031   0.280\n#&gt; I18   433 397   0.104      1.088     1.019    1.121   0.424   0.314\n#&gt; I19   227 397   1.000      0.569     0.750   -2.579  -2.999   0.453\n#&gt; I21   906 397   0.000      2.276     1.246    5.846   2.986  -0.108\n#&gt; I22   728 397   0.000      1.829     0.985    2.918  -0.105   0.059\n#&gt; I23   276 397   1.000      0.693     0.852   -1.918  -1.812   0.412\n\nIn alternativa, è possibile usare la funzione mirt del pacchetto mirt:\n\nmirt_rm &lt;- mirt(responses, 1, \"Rasch\")\n#&gt; \nIteration: 1, Log-Lik: -2816.924, Max-Change: 0.03346\nIteration: 2, Log-Lik: -2816.650, Max-Change: 0.02041\nIteration: 3, Log-Lik: -2816.562, Max-Change: 0.01357\nIteration: 4, Log-Lik: -2816.520, Max-Change: 0.01027\nIteration: 5, Log-Lik: -2816.501, Max-Change: 0.00584\nIteration: 6, Log-Lik: -2816.493, Max-Change: 0.00395\nIteration: 7, Log-Lik: -2816.490, Max-Change: 0.00307\nIteration: 8, Log-Lik: -2816.488, Max-Change: 0.00174\nIteration: 9, Log-Lik: -2816.487, Max-Change: 0.00118\nIteration: 10, Log-Lik: -2816.487, Max-Change: 0.00094\nIteration: 11, Log-Lik: -2816.487, Max-Change: 0.00054\nIteration: 12, Log-Lik: -2816.487, Max-Change: 0.00036\nIteration: 13, Log-Lik: -2816.487, Max-Change: 0.00028\nIteration: 14, Log-Lik: -2816.487, Max-Change: 0.00016\nIteration: 15, Log-Lik: -2816.487, Max-Change: 0.00011\nIteration: 16, Log-Lik: -2816.487, Max-Change: 0.00008\nmirt::itemfit(mirt_rm, fit_stats = \"infit\", method = \"ML\")\n#&gt;    item outfit z.outfit infit z.infit\n#&gt; 1    I1  0.814   -1.683 0.904  -1.665\n#&gt; 2    I2  0.685   -2.793 0.809  -3.271\n#&gt; 3    I3  0.724   -1.510 0.870  -1.493\n#&gt; 4    I6  0.834   -2.272 0.860  -3.258\n#&gt; 5    I7  0.682   -1.357 0.831  -1.504\n#&gt; 6   I11  0.832   -1.381 0.816  -3.143\n#&gt; 7   I12  1.148    1.484 0.971  -0.554\n#&gt; 8   I14  0.990   -0.086 1.023   0.494\n#&gt; 9   I17  1.315    2.276 0.935  -1.040\n#&gt; 10  I18  1.083    1.011 1.019   0.426\n#&gt; 11  I19  0.569   -2.585 0.748  -3.014\n#&gt; 12  I21  2.279    5.852 1.244   2.960\n#&gt; 13  I22  1.823    2.911 0.978  -0.165\n#&gt; 14  I23  0.690   -1.831 0.853  -1.804\n\nLa tabella risultante inizia con le statistiche del test di adattamento χ2 approssimativo, i suoi gradi di libertà e i valori di p risultanti. Se il modello di Rasch è valido, la statistica di test risultante può essere approssimativamente descritta da una distribuzione χ2, il che porta ai valori di p presentati.\nLe colonne seguenti presentano le statistiche MSQ e t di infit e outfit. Per le statistiche MSQ di infit e outfit, valori vicini a 1 indicano un buon adattamento del modello, mentre per le statistiche t di infit e outfit, valori vicini a 0 indicano un buon adattamento. Valori più alti indicano che le risposte sono più casuali di quanto previsto dal modello di Rasch, segnalando un sottoadattamento (underfit); valori più bassi indicano che le risposte sono meno casuali del previsto, segnalando un sovradattamento (overfit).\nSeguendo una delle linee guida proposte, esamineremo ulteriormente quegli item i cui valori t di infit o outfit sono inferiori a -2 o superiori a 2 (ma esistono linee guida alternative). Troviamo che per gli item 2, 6, 11 e 19, almeno un valore t è inferiore a -2, indicando un sovradattamento. Per l’item 19 ciò è supportato dal fatto che la ICC empirica ha una pendenza più ripida rispetto alla ICC attesa.\nPer gli item 17, 21 e 22, invece, almeno un valore t per le statistiche di infit e outfit è superiore a 2, indicando un sottoadattamento. Questo è nuovamente in linea con l’esame delle ICC, dove abbiamo riscontrato che la ICC empirica per l’item 21 ha una pendenza inferiore rispetto alla ICC attesa.\n\nitemfitPlot(mirt_rm)\n\n\n\n\n\n\n\n\n71.6.2 Valutare l’Adattamento delle Persone\nPossiamo generare le stesse misure di adattamento per ogni persona per valutare quanto bene i pattern di risposta di ciascuno si allineano con il modello. Ragioniamo in questo modo: se una persona con un alto valore di \\(\\theta\\) (cioè alta abilità latente) non risponde correttamente a un item facile, questa persona non si adatta bene al modello. Al contrario, se una persona con bassa abilità risponde correttamente a una domanda molto difficile, anche questo non è conforme al modello. Nella pratica, è probabile che ci saranno alcune persone che non si adattano bene al modello. Tuttavia, finché il numero di rispondenti non conformi è basso, la situazione è accettabile. Di solito, ci concentriamo nuovamente sulle statistiche di infit e outfit. Se meno del 5% dei rispondenti presenta valori di infit e outfit superiori o inferiori a 1.96 e -1.96, possiamo considerare il modello adeguato.\nStimiamo gli indici infit e outfit delle persone usando eRm:\n\neRm::personfit(person.parameter(rm_sum0)) \n#&gt; \n#&gt; Personfit Statistics: \n#&gt;      Chisq df p-value Outfit MSQ Infit MSQ Outfit t Infit t\n#&gt; 1    13.19 13   0.433      0.942     1.080     0.07    0.35\n#&gt; 2     7.00 13   0.902      0.500     0.744    -0.44   -0.85\n#&gt; 3     7.56 13   0.871      0.540     0.799    -0.37   -0.62\n#&gt; 4    13.65 13   0.399      0.975     1.112     0.14    0.44\n#&gt; 5     4.02 13   0.991      0.287     0.354    -1.81   -2.20\n#&gt; 6    22.05 13   0.055      1.575     1.304     0.96    1.01\n#&gt; 7    22.82 13   0.044      1.630     1.623     1.02    1.83\n#&gt; 8    12.94 13   0.453      0.924     0.746     0.00   -0.66\n#&gt; 9    71.92 13   0.000      5.137     1.376     2.32    1.11\n#&gt; 10   22.51 13   0.048      1.608     1.065     1.12    0.30\n#&gt; 11    8.24 13   0.827      0.589     0.786    -0.83   -0.52\n#&gt; 12   17.93 13   0.160      1.280     0.916     0.67   -0.10\n#&gt; 13    5.63 13   0.959      0.402     0.667    -0.33   -1.01\n#&gt; 14   18.08 13   0.154      1.291     0.950     0.62   -0.07\n#&gt; 15    9.43 13   0.740      0.673     0.890    -0.50   -0.24\n#&gt; 16   49.15 13   0.000      3.511     1.562     1.53    1.20\n#&gt; 17   16.14 13   0.242      1.153     1.094     0.46    0.37\n#&gt; 18    7.40 13   0.880      0.529     0.785    -0.39   -0.68\n#&gt; 19    4.70 13   0.981      0.336     0.424    -1.70   -1.96\n#&gt; 20    4.70 13   0.981      0.336     0.424    -1.70   -1.96\n#&gt; 21   45.74 13   0.000      3.267     1.221     1.99    0.79\n#&gt; 22    5.44 13   0.964      0.389     0.501    -1.49   -1.60\n#&gt; 23    6.69 13   0.917      0.478     0.812    -0.21   -0.49\n#&gt; 24    9.04 13   0.770      0.646     0.957    -0.19   -0.05\n#&gt; 25   11.94 13   0.533      0.853     1.032    -0.01    0.21\n#&gt; 26    8.35 13   0.820      0.596     0.851    -0.49   -0.42\n#&gt; 27   28.14 13   0.009      2.010     1.853     1.40    2.35\n#&gt; 28    7.92 13   0.849      0.566     0.731    -0.84   -0.66\n#&gt; 29   65.84 13   0.000      4.703     1.677     1.83    1.38\n#&gt; 30    8.30 13   0.824      0.593     0.752    -0.70   -0.71\n#&gt; 31    6.58 13   0.922      0.470     0.660    -0.78   -1.18\n#&gt; 32   11.04 13   0.607      0.789     1.055    -0.12    0.28\n#&gt; 33   10.08 13   0.687      0.720     0.908    -0.39   -0.18\n#&gt; 34   13.55 13   0.406      0.968     1.375     0.25    1.23\n#&gt; 35   23.62 13   0.035      1.687     1.170     1.23    0.60\n#&gt; 36   10.89 13   0.620      0.778     0.863    -0.33   -0.28\n#&gt; 37    6.06 13   0.944      0.433     0.585    -1.15   -1.38\n#&gt; 38   28.19 13   0.009      2.014     1.700     1.77    1.73\n#&gt; 39   13.75 13   0.392      0.982     1.271     0.36    0.85\n#&gt; 40   34.16 13   0.001      2.440     1.211     1.50    0.76\n#&gt; 41   71.92 13   0.000      5.137     1.376     2.32    1.11\n#&gt; 42   10.85 13   0.624      0.775     0.840    -0.31   -0.32\n#&gt; 43    4.02 13   0.991      0.287     0.354    -1.81   -2.20\n#&gt; 44    6.98 13   0.903      0.499     0.661    -1.04   -0.90\n#&gt; 45   68.33 13   0.000      4.881     1.024     2.24    0.18\n#&gt; 46   15.79 13   0.260      1.128     1.141     0.41    0.55\n#&gt; 47   21.71 13   0.060      1.551     1.756     0.86    1.83\n#&gt; 48   13.78 13   0.390      0.984     1.094     0.25    0.37\n#&gt; 49   15.99 13   0.250      1.142     1.334     0.60    0.81\n#&gt; 50   10.99 13   0.612      0.785     1.008    -0.24    0.13\n#&gt; 51   12.66 13   0.475      0.904     1.053    -0.04    0.27\n#&gt; 52    4.17 13   0.989      0.298     0.804     0.40    0.00\n#&gt; 53    7.53 13   0.873      0.538     0.972    -0.17    0.04\n#&gt; 54    5.03 13   0.975      0.359     0.491    -1.08   -1.99\n#&gt; 55   11.32 13   0.584      0.809     1.061    -0.08    0.30\n#&gt; 56   42.85 13   0.000      3.061     1.148     1.39    0.46\n#&gt; 57   30.25 13   0.004      2.161     1.909     1.95    2.13\n#&gt; 58   17.43 13   0.180      1.245     1.137     0.57    0.47\n#&gt; 59    5.67 13   0.957      0.405     0.521    -1.43   -1.51\n#&gt; 60    4.09 13   0.990      0.292     0.390    -1.43   -2.02\n#&gt; 61   16.04 13   0.247      1.145     1.125     0.43    0.44\n#&gt; 62   18.20 13   0.150      1.300     1.095     0.65    0.37\n#&gt; 63   19.94 13   0.097      1.425     1.438     0.90    1.15\n#&gt; 64   37.69 13   0.000      2.692     2.222     1.74    2.65\n#&gt; 65   24.53 13   0.027      1.752     1.618     1.21    1.51\n#&gt; 66   12.17 13   0.513      0.870     0.973     0.09    0.04\n#&gt; 67   19.62 13   0.105      1.402     1.199     0.84    0.68\n#&gt; 68   12.57 13   0.482      0.898     0.877    -0.05   -0.24\n#&gt; 69   18.67 13   0.134      1.334     1.365     0.74    1.10\n#&gt; 70    7.60 13   0.868      0.543     0.755    -0.60   -0.78\n#&gt; 71    9.72 13   0.716      0.695     0.926    -0.50   -0.08\n#&gt; 72    5.03 13   0.975      0.359     0.491    -1.08   -1.99\n#&gt; 73    7.08 13   0.898      0.506     0.637    -1.02   -0.98\n#&gt; 74    7.60 13   0.868      0.543     0.755    -0.60   -0.78\n#&gt; 75    4.22 13   0.989      0.301     0.456    -0.98   -1.79\n#&gt; 76    5.67 13   0.957      0.405     0.521    -1.43   -1.51\n#&gt; 77    9.06 13   0.768      0.647     0.829    -0.38   -0.50\n#&gt; 78    8.27 13   0.826      0.591     0.796    -0.58   -0.46\n#&gt; 79   11.04 13   0.607      0.789     1.055    -0.12    0.28\n#&gt; 80    5.83 13   0.952      0.416     0.549    -1.31   -1.31\n#&gt; 81   10.48 13   0.654      0.749     0.917    -0.24   -0.10\n#&gt; 82   10.89 13   0.620      0.778     1.038    -0.04    0.22\n#&gt; 83   10.09 13   0.687      0.721     0.868    -0.30   -0.24\n#&gt; 84    6.77 13   0.914      0.483     0.619    -1.09   -1.05\n#&gt; 85    4.02 13   0.991      0.287     0.354    -1.81   -2.20\n#&gt; 86    6.47 13   0.927      0.462     0.649    -0.80   -1.22\n#&gt; 87    8.13 13   0.835      0.581     0.875    -0.36   -0.24\n#&gt; 88   12.39 13   0.496      0.885     0.903    -0.06   -0.14\n#&gt; 89    5.67 13   0.957      0.405     0.521    -1.43   -1.51\n#&gt; 90    5.12 13   0.972      0.366     0.513    -1.18   -1.46\n#&gt; 91    5.36 13   0.966      0.383     0.507    -1.32   -1.73\n#&gt; 92    4.09 13   0.990      0.292     0.390    -1.43   -2.02\n#&gt; 93    6.58 13   0.922      0.470     0.660    -0.78   -1.18\n#&gt; 94   12.09 13   0.520      0.864     0.995    -0.08    0.09\n#&gt; 95    5.63 13   0.959      0.402     0.667    -0.33   -1.01\n#&gt; 96   24.69 13   0.025      1.764     1.076     1.33    0.34\n#&gt; 97   15.12 13   0.300      1.080     1.025     0.32    0.19\n#&gt; 98    9.84 13   0.707      0.703     0.918    -0.43   -0.15\n#&gt; 99   10.89 13   0.620      0.778     0.863    -0.33   -0.28\n#&gt; 100   7.27 13   0.888      0.519     0.764    -0.41   -0.76\n#&gt; 101   8.27 13   0.826      0.591     0.796    -0.58   -0.46\n#&gt; 102  13.28 13   0.427      0.948     0.998     0.06    0.11\n#&gt; 103  20.67 13   0.080      1.476     1.181     0.98    0.58\n#&gt; 104  24.87 13   0.024      1.777     1.370     1.40    1.00\n#&gt; 105   6.58 13   0.922      0.470     0.660    -0.78   -1.18\n#&gt; 106   5.03 13   0.975      0.359     0.491    -1.08   -1.99\n#&gt; 107   5.16 13   0.972      0.368     0.485    -1.37   -1.84\n#&gt; 108  22.38 13   0.050      1.599     1.205     1.16    0.64\n#&gt; 109   4.02 13   0.991      0.287     0.354    -1.81   -2.20\n#&gt; 110   4.70 13   0.981      0.336     0.424    -1.70   -1.96\n#&gt; 111   5.03 13   0.975      0.359     0.491    -1.08   -1.99\n#&gt; 112   6.94 13   0.905      0.496     0.675    -0.81   -0.85\n#&gt; 113   5.36 13   0.966      0.383     0.507    -1.32   -1.73\n#&gt; 114   9.88 13   0.703      0.706     0.844    -0.51   -0.34\n#&gt; 115  18.18 13   0.151      1.298     1.534     0.63    1.62\n#&gt; 116   4.35 13   0.987      0.310     0.634    -0.11   -0.77\n#&gt; 117  10.40 13   0.661      0.743     1.095    -0.05    0.41\n#&gt; 118  26.27 13   0.016      1.876     1.226     1.46    0.75\n#&gt; 119   5.83 13   0.952      0.416     0.549    -1.31   -1.31\n#&gt; 120  40.54 13   0.000      2.896     1.718     1.51    1.87\n#&gt; 121   5.44 13   0.964      0.389     0.501    -1.49   -1.60\n#&gt; 122  28.23 13   0.008      2.016     1.666     1.41    1.93\n#&gt; 123   5.64 13   0.958      0.403     0.735    -0.38   -0.69\n#&gt; 124   5.67 13   0.957      0.405     0.521    -1.43   -1.51\n#&gt; 125  38.77 13   0.000      2.769     1.219     1.46    0.72\n#&gt; 126  45.62 13   0.000      3.258     1.202     1.98    0.73\n#&gt; 127 102.53 13   0.000      7.324     1.711     2.35    1.44\n#&gt; 128  40.52 13   0.000      2.895     1.439     1.51    1.26\n#&gt; 129   7.30 13   0.886      0.521     0.779    -0.40   -0.70\n#&gt; 130  26.98 13   0.013      1.927     1.304     1.52    0.95\n#&gt; 131   8.84 13   0.785      0.632     0.892    -0.42   -0.27\n#&gt; 132   9.43 13   0.740      0.673     0.890    -0.50   -0.24\n#&gt; 133  27.41 13   0.011      1.957     1.442     1.35    1.38\n#&gt; 134  19.83 13   0.099      1.417     1.647     0.72    1.92\n#&gt; 135  25.52 13   0.020      1.823     1.546     1.22    1.64\n#&gt; 136  11.12 13   0.601      0.794     1.081    -0.15    0.33\n#&gt; 137  17.81 13   0.165      1.272     1.014     0.67    0.16\n#&gt; 138  16.06 13   0.246      1.147     1.220     0.45    0.67\n#&gt; 139   5.74 13   0.955      0.410     0.644    -0.70   -1.01\n#&gt; 140   5.12 13   0.972      0.366     0.513    -1.18   -1.46\n#&gt; 141   7.08 13   0.898      0.506     0.637    -1.02   -0.98\n#&gt; 142  11.50 13   0.569      0.822     1.160     0.06    0.61\n#&gt; 143  10.95 13   0.615      0.782     0.932    -0.18   -0.06\n#&gt; 144   6.06 13   0.944      0.433     0.585    -1.15   -1.38\n#&gt; 145   4.92 13   0.977      0.351     0.633    -0.47   -1.06\n#&gt; 146   5.28 13   0.968      0.377     0.630    -0.37   -1.16\n#&gt; 147  24.05 13   0.031      1.718     1.470     1.37    1.26\n#&gt; 148  15.12 13   0.300      1.080     1.025     0.32    0.19\n#&gt; 149   5.44 13   0.964      0.389     0.501    -1.49   -1.60\n#&gt; 150   5.16 13   0.972      0.368     0.485    -1.37   -1.84\n#&gt; 151  11.64 13   0.558      0.831     0.940    -0.20   -0.05\n#&gt; 152   8.67 13   0.797      0.619     0.812    -0.44   -0.56\n#&gt; 153   6.26 13   0.936      0.447     0.628    -0.83   -1.32\n#&gt; 154  15.00 13   0.307      1.071     1.263     0.31    0.77\n#&gt; 155  17.01 13   0.199      1.215     1.418     0.55    1.23\n#&gt; 156   7.27 13   0.888      0.519     0.764    -0.41   -0.76\n#&gt; 157  12.37 13   0.498      0.883     1.136    -0.04    0.50\n#&gt; 158   8.92 13   0.779      0.637     0.938    -0.21   -0.11\n#&gt; 159   9.20 13   0.758      0.657     0.810    -0.59   -0.41\n#&gt; 160   5.44 13   0.964      0.389     0.501    -1.49   -1.60\n#&gt; 161  27.20 13   0.012      1.943     1.221     1.16    0.79\n#&gt; 162   5.12 13   0.972      0.366     0.513    -1.18   -1.46\n#&gt; 163  58.61 13   0.000      4.186     1.621     2.44    1.86\n#&gt; 164  10.70 13   0.636      0.764     0.896    -0.16   -0.26\n#&gt; 165  41.12 13   0.000      2.937     1.378     1.53    1.12\n#&gt; 166   5.16 13   0.972      0.368     0.485    -1.37   -1.84\n#&gt; 167  58.08 13   0.000      4.149     1.313     1.56    0.62\n#&gt; 168  17.86 13   0.163      1.276     1.502     0.60    1.54\n#&gt; 169 146.26 13   0.000     10.447     1.338     2.27    0.65\n#&gt; 170  21.25 13   0.068      1.518     1.420     1.00    1.24\n#&gt; 171  26.86 13   0.013      1.919     1.962     1.31    2.59\n#&gt; 172   4.22 13   0.989      0.301     0.456    -0.98   -1.79\n#&gt; 173   9.22 13   0.756      0.659     1.019     0.02    0.17\n#&gt; 174   7.86 13   0.853      0.561     0.817    -0.10   -0.47\n#&gt; 175  11.45 13   0.573      0.818     0.834    -0.21   -0.34\n#&gt; 176   7.47 13   0.877      0.533     0.799    -0.13   -0.53\n#&gt; 177   4.22 13   0.989      0.301     0.456    -0.98   -1.79\n#&gt; 178  10.70 13   0.636      0.764     0.896    -0.16   -0.26\n#&gt; 179  20.35 13   0.087      1.453     1.320     0.91    0.99\n#&gt; 180 123.64 13   0.000      8.832     2.158     3.23    2.72\n#&gt; 181   8.28 13   0.825      0.592     1.068     0.61    0.35\n#&gt; 182   8.86 13   0.784      0.633     0.808    -0.41   -0.58\n#&gt; 183  48.85 13   0.000      3.489     1.539     2.10    1.65\n#&gt; 184   4.09 13   0.990      0.292     0.390    -1.43   -2.02\n#&gt; 185  10.98 13   0.613      0.784     1.153     0.01    0.59\n#&gt; 186   4.22 13   0.989      0.301     0.456    -0.98   -1.79\n#&gt; 187   6.77 13   0.914      0.483     0.619    -1.09   -1.05\n#&gt; 188  46.01 13   0.000      3.287     1.243     2.00    0.86\n#&gt; 189  36.36 13   0.001      2.597     1.751     1.60    2.16\n#&gt; 190  20.04 13   0.094      1.432     1.448     0.88    1.30\n#&gt; 191   5.16 13   0.972      0.368     0.485    -1.37   -1.84\n#&gt; 192  58.33 13   0.000      4.167     1.599     2.43    1.80\n#&gt; 193   7.14 13   0.895      0.510     0.663    -0.92   -1.06\n#&gt; 194   5.67 13   0.957      0.405     0.521    -1.43   -1.51\n#&gt; 195  11.73 13   0.550      0.838     0.857    -0.16   -0.27\n#&gt; 196  13.87 13   0.383      0.991     0.924     0.14   -0.10\n#&gt; 197   9.06 13   0.768      0.647     0.829    -0.38   -0.50\n#&gt; 198   4.28 13   0.988      0.306     0.544    -0.56   -1.41\n#&gt; 199   5.74 13   0.955      0.410     0.644    -0.70   -1.01\n#&gt; 200  10.58 13   0.646      0.755     0.746    -0.35   -0.61\n#&gt; 201  17.32 13   0.185      1.237     1.268     0.60    0.78\n#&gt; 202   6.26 13   0.936      0.447     0.628    -0.83   -1.32\n#&gt; 203   9.81 13   0.710      0.700     0.767    -0.43   -0.66\n#&gt; 204   6.26 13   0.936      0.447     0.628    -0.83   -1.32\n#&gt; 205  10.48 13   0.654      0.749     0.917    -0.24   -0.10\n#&gt; 206   9.17 13   0.760      0.655     1.086    -0.01    0.35\n#&gt; 207  16.66 13   0.215      1.190     1.409     0.52    1.13\n#&gt; 208  11.76 13   0.548      0.840     0.980    -0.07    0.07\n#&gt; 209   5.83 13   0.952      0.416     0.549    -1.31   -1.31\n#&gt; 210  14.94 13   0.311      1.067     1.217     0.35    0.68\n#&gt; 211   5.35 13   0.967      0.382     0.570    -0.68   -1.62\n#&gt; 212   5.44 13   0.964      0.389     0.501    -1.49   -1.60\n#&gt; 213   5.67 13   0.957      0.405     0.521    -1.43   -1.51\n#&gt; 214   5.12 13   0.972      0.366     0.513    -1.18   -1.46\n#&gt; 215   5.64 13   0.958      0.403     0.735    -0.38   -0.69\n#&gt; 216   6.77 13   0.914      0.483     0.619    -1.09   -1.05\n#&gt; 217   5.78 13   0.954      0.413     0.860    -0.01   -0.18\n#&gt; 218  12.70 13   0.471      0.907     0.921    -0.01   -0.09\n#&gt; 219   5.92 13   0.949      0.423     0.747    -0.34   -0.65\n#&gt; 220   4.70 13   0.981      0.336     0.424    -1.70   -1.96\n#&gt; 221  27.07 13   0.012      1.934     1.200     1.15    0.73\n#&gt; 222   7.44 13   0.878      0.532     0.713    -0.86   -0.86\n#&gt; 223   5.63 13   0.959      0.402     0.667    -0.33   -1.01\n#&gt; 224   4.22 13   0.989      0.301     0.456    -0.98   -1.79\n#&gt; 225   9.13 13   0.763      0.652     0.859    -0.44   -0.27\n#&gt; 226  14.02 13   0.372      1.001     1.074     0.17    0.32\n#&gt; 227  10.95 13   0.615      0.782     0.932    -0.18   -0.06\n#&gt; 228  13.78 13   0.390      0.984     1.094     0.25    0.37\n#&gt; 229   5.67 13   0.957      0.405     0.521    -1.43   -1.51\n#&gt; 230   8.26 13   0.826      0.590     0.741    -0.77   -0.63\n#&gt; 231   6.77 13   0.914      0.483     0.619    -1.09   -1.05\n#&gt; 232  10.66 13   0.639      0.762     0.843    -0.37   -0.34\n#&gt; 233  62.46 13   0.000      4.461     2.648     3.13    3.89\n#&gt; 234   8.74 13   0.793      0.624     0.755    -0.50   -0.59\n#&gt; 235   7.86 13   0.853      0.561     0.817    -0.10   -0.47\n#&gt; 236   7.92 13   0.849      0.566     0.731    -0.84   -0.66\n#&gt; 237   7.92 13   0.849      0.566     0.731    -0.84   -0.66\n#&gt; 238   4.37 13   0.987      0.312     0.865     0.40    0.07\n#&gt; 239  18.15 13   0.152      1.296     1.223     0.92    0.53\n#&gt; 240  33.12 13   0.002      2.366     1.889     1.72    2.43\n#&gt; 241   4.02 13   0.991      0.287     0.354    -1.81   -2.20\n#&gt; 242   8.79 13   0.788      0.628     0.782    -0.71   -0.54\n#&gt; 243   7.44 13   0.878      0.532     0.713    -0.86   -0.86\n#&gt; 244   8.23 13   0.828      0.588     0.749    -0.78   -0.60\n#&gt; 245   5.63 13   0.959      0.402     0.667    -0.33   -1.01\n#&gt; 246   4.22 13   0.989      0.301     0.456    -0.98   -1.79\n#&gt; 247  22.64 13   0.046      1.617     0.967     1.00   -0.01\n#&gt; 248  16.01 13   0.249      1.144     1.334     0.44    0.96\n#&gt; 249   4.02 13   0.991      0.287     0.354    -1.81   -2.20\n#&gt; 250  18.84 13   0.128      1.346     1.182     0.65    0.62\n#&gt; 251   4.35 13   0.987      0.310     0.634    -0.11   -0.77\n#&gt; 252   9.20 13   0.758      0.657     0.810    -0.59   -0.41\n#&gt; 253   6.47 13   0.927      0.462     0.649    -0.80   -1.22\n#&gt; 254   9.30 13   0.750      0.664     0.822    -0.62   -0.41\n#&gt; 255   5.67 13   0.957      0.405     0.521    -1.43   -1.51\n#&gt; 256  18.16 13   0.152      1.297     1.362     0.70    0.99\n#&gt; 257  12.84 13   0.460      0.917     1.162     0.07    0.53\n#&gt; 258   5.36 13   0.966      0.383     0.507    -1.32   -1.73\n#&gt; 259   6.91 13   0.907      0.494     0.910     0.11   -0.05\n#&gt; 260   7.50 13   0.875      0.536     0.710    -0.98   -0.78\n#&gt; 261   6.98 13   0.903      0.499     0.661    -1.04   -0.90\n#&gt; 262   4.02 13   0.991      0.287     0.354    -1.81   -2.20\n#&gt; 263   7.78 13   0.857      0.556     1.094     0.58    0.37\n#&gt; 264   8.74 13   0.793      0.624     0.755    -0.50   -0.59\n#&gt; 265   9.30 13   0.750      0.664     1.038    -0.21    0.23\n#&gt; 266   8.48 13   0.811      0.606     0.901    -0.26   -0.24\n#&gt; 267   4.22 13   0.989      0.301     0.456    -0.98   -1.79\n#&gt; 268  20.66 13   0.080      1.476     1.214     0.84    0.76\n#&gt; 269   6.47 13   0.927      0.462     0.649    -0.80   -1.22\n#&gt; 270   8.37 13   0.819      0.598     0.829    -0.27   -0.51\n#&gt; 271  23.03 13   0.041      1.645     1.150     0.95    0.52\n#&gt; 272   8.67 13   0.798      0.619     0.756    -0.51   -0.58\n#&gt; 273   7.85 13   0.853      0.561     0.740    -0.78   -0.76\n#&gt; 274  19.06 13   0.121      1.362     1.210     0.66    0.70\n#&gt; 275   8.23 13   0.828      0.588     0.749    -0.78   -0.60\n#&gt; 276  14.98 13   0.309      1.070     1.236     0.31    0.78\n#&gt; 277  12.38 13   0.497      0.884     1.034    -0.08    0.21\n#&gt; 278   4.92 13   0.977      0.351     0.633    -0.47   -1.06\n#&gt; 279   5.12 13   0.972      0.366     0.513    -1.18   -1.46\n#&gt; 280  11.27 13   0.589      0.805     1.035     0.00    0.22\n#&gt; 281  30.64 13   0.004      2.189     1.642     1.39    1.61\n#&gt; 282  16.13 13   0.242      1.152     1.025     0.45    0.19\n#&gt; 283  10.80 13   0.628      0.771     1.136    -0.01    0.54\n#&gt; 284   5.44 13   0.964      0.389     0.501    -1.49   -1.60\n#&gt; 285  17.34 13   0.184      1.238     1.623     0.57    1.67\n#&gt; 286   5.89 13   0.950      0.421     0.769    -0.35   -0.58\n#&gt; 287  32.26 13   0.002      2.304     1.363     1.77    0.99\n#&gt; 288  23.82 13   0.033      1.701     1.277     1.00    0.82\n#&gt; 289   4.02 13   0.991      0.287     0.354    -1.81   -2.20\n#&gt; 290   9.69 13   0.719      0.692     0.728    -0.50   -0.67\n#&gt; 291  11.57 13   0.563      0.826     0.991    -0.21    0.09\n#&gt; 292  15.01 13   0.307      1.072     1.001     0.32    0.13\n#&gt; 293  12.04 13   0.524      0.860     0.736    -0.14   -0.69\n#&gt; 294   8.46 13   0.812      0.605     0.790    -0.47   -0.64\n#&gt; 296   5.74 13   0.955      0.410     0.644    -0.70   -1.01\n#&gt; 297  15.67 13   0.267      1.119     1.279     0.41    0.83\n#&gt; 298   7.78 13   0.857      0.556     1.094     0.58    0.37\n#&gt; 299  15.52 13   0.276      1.109     1.298     0.38    0.85\n#&gt; 300  32.03 13   0.002      2.288     1.795     2.03    1.84\n#&gt; 301   5.03 13   0.975      0.359     0.491    -1.08   -1.99\n#&gt; 302   5.36 13   0.966      0.383     0.507    -1.32   -1.73\n#&gt; 303  10.58 13   0.646      0.755     0.746    -0.35   -0.61\n#&gt; 304  12.08 13   0.521      0.863     0.975    -0.08    0.03\n#&gt; 305  14.77 13   0.322      1.055     0.934     0.27   -0.07\n#&gt; 306  18.87 13   0.127      1.348     1.290     0.78    0.83\n#&gt; 308  14.15 13   0.363      1.011     1.091     0.28    0.37\n#&gt; 309   5.74 13   0.955      0.410     0.644    -0.70   -1.01\n#&gt; 310  18.16 13   0.152      1.297     1.087     0.72    0.36\n#&gt; 311   5.35 13   0.967      0.382     0.570    -0.68   -1.62\n#&gt; 312  16.04 13   0.247      1.145     1.125     0.43    0.44\n#&gt; 313  23.03 13   0.041      1.645     1.150     0.95    0.52\n#&gt; 314   6.94 13   0.905      0.496     0.675    -0.81   -0.85\n#&gt; 315  11.31 13   0.584      0.808     1.024    -0.23    0.19\n#&gt; 316   7.47 13   0.876      0.534     0.633    -0.99   -1.06\n#&gt; 317  24.89 13   0.024      1.778     1.098     1.34    0.40\n#&gt; 318  18.18 13   0.151      1.298     0.811     0.70   -0.41\n#&gt; 319   6.02 13   0.945      0.430     0.903     0.01   -0.08\n#&gt; 320  13.67 13   0.398      0.976     1.072     0.11    0.32\n#&gt; 321   7.50 13   0.875      0.536     0.710    -0.98   -0.78\n#&gt; 322   8.74 13   0.793      0.624     0.755    -0.50   -0.59\n#&gt; 323  13.75 13   0.392      0.982     1.201     0.18    0.63\n#&gt; 324  28.43 13   0.008      2.031     1.448     1.03    1.04\n#&gt; 325   6.90 13   0.907      0.493     0.768    -0.53   -0.58\n#&gt; 326  15.66 13   0.268      1.119     1.228     0.39    0.69\n#&gt; 327   8.74 13   0.793      0.624     0.755    -0.50   -0.59\n#&gt; 328   4.02 13   0.991      0.287     0.354    -1.81   -2.20\n#&gt; 329   9.69 13   0.719      0.692     0.728    -0.50   -0.67\n#&gt; 330   5.44 13   0.964      0.389     0.501    -1.49   -1.60\n#&gt; 331   4.09 13   0.990      0.292     0.390    -1.43   -2.02\n#&gt; 332   7.44 13   0.878      0.532     0.713    -0.86   -0.86\n#&gt; 333   5.12 13   0.972      0.366     0.513    -1.18   -1.46\n#&gt; 334   8.05 13   0.840      0.575     0.762    -0.74   -0.68\n#&gt; 335  31.38 13   0.003      2.242     1.672     1.97    1.61\n#&gt; 336   4.70 13   0.981      0.336     0.424    -1.70   -1.96\n#&gt; 337  10.03 13   0.691      0.717     0.928    -0.13   -0.08\n#&gt; 338  11.78 13   0.546      0.841     1.183     0.09    0.68\n#&gt; 339  11.27 13   0.589      0.805     1.035     0.00    0.22\n#&gt; 340  30.47 13   0.004      2.176     1.461     1.65    1.20\n#&gt; 341   4.09 13   0.990      0.292     0.390    -1.43   -2.02\n#&gt; 342  17.01 13   0.199      1.215     1.125     0.54    0.45\n#&gt; 343   5.12 13   0.972      0.366     0.513    -1.18   -1.46\n#&gt; 344  12.13 13   0.517      0.866     0.920    -0.10   -0.09\n#&gt; 345  12.26 13   0.507      0.876     0.905     0.10   -0.15\n#&gt; 346  12.94 13   0.453      0.924     0.746     0.00   -0.66\n#&gt; 347  21.89 13   0.057      1.564     1.668     0.81    1.67\n#&gt; 348  13.52 13   0.409      0.965     0.934     0.10   -0.05\n#&gt; 349   5.74 13   0.955      0.410     0.644    -0.70   -1.01\n#&gt; 350  12.70 13   0.471      0.907     0.921    -0.01   -0.09\n#&gt; 351  23.70 13   0.034      1.693     1.714     1.14    1.69\n#&gt; 352   7.47 13   0.876      0.534     0.633    -0.99   -1.06\n#&gt; 353  26.39 13   0.015      1.885     1.588     1.60    1.51\n#&gt; 354  15.31 13   0.288      1.094     1.203     0.35    0.69\n#&gt; 355   4.02 13   0.991      0.287     0.354    -1.81   -2.20\n#&gt; 356   7.14 13   0.895      0.510     0.663    -0.92   -1.06\n#&gt; 357  13.63 13   0.401      0.973     1.200     0.11    0.64\n#&gt; 358   5.44 13   0.964      0.389     0.501    -1.49   -1.60\n#&gt; 359  18.07 13   0.155      1.291     1.289     0.62    0.97\n#&gt; 360  11.94 13   0.533      0.853     1.032    -0.01    0.21\n#&gt; 361   8.30 13   0.824      0.593     0.752    -0.70   -0.71\n#&gt; 362  21.69 13   0.060      1.549     1.504     1.09    1.28\n#&gt; 363   8.67 13   0.797      0.619     0.812    -0.44   -0.56\n#&gt; 364   8.35 13   0.820      0.596     0.851    -0.49   -0.42\n#&gt; 365  17.60 13   0.173      1.257     1.041     0.65    0.23\n#&gt; 366  20.61 13   0.081      1.472     1.003     0.97    0.13\n#&gt; 367   8.28 13   0.825      0.592     1.068     0.61    0.35\n#&gt; 368   4.57 13   0.984      0.326     0.848     0.42    0.06\n#&gt; 369  63.80 13   0.000      4.557     1.160     1.79    0.49\n#&gt; 370  38.91 13   0.000      2.779     1.507     1.46    1.42\n#&gt; 371  20.97 13   0.073      1.498     1.488     0.91    1.25\n#&gt; 372  31.49 13   0.003      2.249     1.150     1.62    0.57\n#&gt; 373   5.16 13   0.972      0.368     0.485    -1.37   -1.84\n#&gt; 374   7.47 13   0.877      0.533     0.799    -0.13   -0.53\n#&gt; 375  26.27 13   0.016      1.876     1.226     1.46    0.75\n#&gt; 376   7.27 13   0.888      0.519     0.764    -0.41   -0.76\n#&gt; 377   8.75 13   0.792      0.625     0.921    -0.23   -0.17\n#&gt; 378   6.39 13   0.931      0.456     0.873     0.07   -0.14\n#&gt; 379  19.96 13   0.096      1.426     1.151     0.79    0.58\n#&gt; 380  11.67 13   0.555      0.833     1.198     0.21    0.66\n#&gt; 381   9.06 13   0.769      0.647     0.959    -0.19   -0.04\n#&gt; 382   6.74 13   0.915      0.481     0.635    -1.01   -1.17\n#&gt; 383   5.67 13   0.957      0.405     0.521    -1.43   -1.51\n#&gt; 384  46.62 13   0.000      3.330     1.814     1.70    2.07\n#&gt; 385  27.07 13   0.012      1.934     1.200     1.15    0.73\n#&gt; 386   9.88 13   0.703      0.706     0.844    -0.51   -0.34\n#&gt; 387   9.51 13   0.733      0.679     0.828    -0.53   -0.36\n#&gt; 388  20.99 13   0.073      1.499     1.349     1.05    1.00\n#&gt; 389   8.21 13   0.829      0.587     0.737    -0.59   -0.64\n#&gt; 390   5.67 13   0.957      0.405     0.521    -1.43   -1.51\n#&gt; 391   7.40 13   0.880      0.529     0.785    -0.39   -0.68\n#&gt; 392   4.02 13   0.991      0.287     0.354    -1.81   -2.20\n#&gt; 393   9.02 13   0.771      0.645     0.802    -0.67   -0.47\n#&gt; 394   8.23 13   0.828      0.588     0.749    -0.78   -0.60\n#&gt; 395  19.67 13   0.104      1.405     1.465     0.90    1.25\n#&gt; 396   8.66 13   0.798      0.619     0.865    -0.24   -0.38\n#&gt; 397   5.36 13   0.966      0.383     0.507    -1.32   -1.73\n#&gt; 398   4.70 13   0.981      0.336     0.424    -1.70   -1.96\n#&gt; 399  14.35 13   0.350      1.025     1.217     0.26    0.77\n#&gt; 400   5.28 13   0.968      0.377     0.630    -0.37   -1.16\n\nCome per le statistiche di infit e outfit per i singoli item, individui con valori di t superiori a 2 mostrano un comportamento di risposta più casuale rispetto a quanto previsto dal modello di Rasch. Questo può indicare, ad esempio, comportamenti di risposta basati su supposizioni o scarsa attenzione. I modelli di risposta che portano a valori di t inferiori a -2 indicano un comportamento di risposta più deterministico rispetto a quello atteso. In questo esempio, le persone identificate con il numero 5 e 43 mostrano questo comportamento.\nOtteniamo le stime infit e outfit per le persone con mirt:\n\nhead(personfit(mirt_rm))\n#&gt;   outfit z.outfit infit z.infit     Zh\n#&gt; 1  0.931   0.0419 1.090   0.372 -0.142\n#&gt; 2  0.510  -0.6496 0.728  -0.898  0.889\n#&gt; 3  0.548  -0.5660 0.782  -0.682  0.751\n#&gt; 4  0.996   0.1686 1.111   0.430 -0.232\n#&gt; 5  0.289  -1.8874 0.352  -2.239  1.654\n#&gt; 6  1.465   0.8848 1.315   1.010 -1.002\n\n\npersonfit(mirt_rm) %&gt;%\n    summarize(\n        infit.outside = prop.table(table(z.infit &gt; 1.96 | z.infit &lt; -1.96)),\n        outfit.outside = prop.table(table(z.outfit &gt; 1.96 | z.outfit &lt; -1.96))\n    ) # lower row = non-fitting people\n#&gt;   infit.outside outfit.outside\n#&gt; 1        0.9175           0.98\n#&gt; 2        0.0825           0.02\n\n\npersonfitPlot(mirt_rm)\n\n\n\n\n\n\n\nIn conclusione, nel caso dei dati in esame, meno del 5% dei rispondenti mostra valori di outfit che eccedono la soglia di 1.96 o che sono inferiori a -1.96. Invece, l’8% dei rispondenti mostra valori di infit che eccedono la soglia di 1.96 o che sono inferiori a -1.96. Questi risultati suggeriscono che il modello di Rasch non è del tutto coerente con i dati esaminati.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_implementation.html#curva-di-informazione-dellitem",
    "href": "chapters/irt/05_implementation.html#curva-di-informazione-dellitem",
    "title": "71  Implementazione",
    "section": "\n71.7 Curva di Informazione dell’Item",
    "text": "71.7 Curva di Informazione dell’Item\nUn altro modo per valutare la qualità di ciascun item è tramite la creazione delle cosiddette curve di informazione degli item. L’informazione è un concetto statistico che si riferisce alla capacità di un item di stimare con precisione i punteggi su theta. L’informazione a livello di item chiarisce quanto bene ogni item contribuisca alla precisione nella stima dei punteggi, con livelli più elevati di informazione che portano a stime dei punteggi più accurate.\nPer esempio:\n\nUn item con un’elevata informazione sarà molto utile per discriminare tra rispondenti con diversi livelli di abilità latente attorno a un certo punto della scala di theta. Questo significa che l’item fornisce dati affidabili e significativi sulla capacità o conoscenza che si sta misurando.\nAl contrario, un item con bassa informazione non aggiunge molto alla precisione della stima del punteggio. Questo potrebbe accadere se l’item è troppo facile o troppo difficile per la maggior parte dei rispondenti, o se non è strettamente correlato al tratto latente che si sta cercando di misurare.\n\nLa posizione delle Curve Caratteristiche degli Item (ICC) determina le regioni sul tratto latente dove ciascun item fornisce il massimo di informazione. Questo viene illustrato tramite il grafico dell’informazione dell’item.\n\nplotINFO(rm_sum0, type = \"item\", legpos = FALSE)\n\n\n\n\n\n\n\nQui vediamo che alcuni item forniscono maggiori informazioni sui livelli più bassi di \\(\\theta\\), altri a livelli medi di \\(\\theta\\) e altri ancora ai livelli alti di \\(\\theta\\).\n\n71.7.1 Informazione del Test\nIl concetto di “informazione” può essere applicato anche all’intera scala del test. La Test Information Curve (TIC) è una rappresentazione grafica che mostra quanta informazione un test fornisce a diversi livelli di abilità latente (\\(\\theta\\)). L’informazione è una misura della precisione con cui il test stima l’abilità di un individuo\nIn questo caso, osserviamo che la scala è molto efficace nel stimare i punteggi di theta tra -2 e 3, ma presenta una minore precisione nella stima dei punteggi di theta agli estremi. In altre parole, il test fornisce stime accurate per una vasta gamma di abilità medie e leggermente superiori alla media, ma diventa meno affidabile per valutare abilità molto basse o molto elevate.\nQuesta osservazione ha importanti implicazioni pratiche:\n\n\nValutazione Ottimale per la Maggior Parte dei Rispondenti: La scala è particolarmente adatta per valutare rispondenti il cui livello di abilità si trova all’interno dell’intervallo in cui il test è più informativo (-2 a 4).\n\nLimiti nella Valutazione degli Estremi: Per rispondenti con abilità molto al di sotto di -2 o molto al di sopra di 4, il test potrebbe non fornire stime di abilità così precise. Questo significa che per questi individui, il test potrebbe non essere in grado di discriminare efficacemente tra diversi livelli di abilità.\n\nLe curve di informazione del test aiutano a identificare dove il test è più efficace e dove potrebbe aver bisogno di miglioramenti o aggiustamenti, come l’aggiunta di item più difficili o più facili per estendere la sua precisione ai livelli estremi di abilità. Questa analisi consente di ottimizzare il test per una valutazione più accurata su tutta la gamma di abilità latente che si intende misurare.\nIl grafico dell’informazione del test può essere generato utilizzando eRm::plotINFO:\n\neRm::plotINFO(rm_sum0, type = \"test\")\n\n\n\n\n\n\n\nOppure possiamo usare l’output di mirt:\n\nplot(mirt_rm, type = \"info\")",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_implementation.html#errore-standard-di-misurazione-del-test",
    "href": "chapters/irt/05_implementation.html#errore-standard-di-misurazione-del-test",
    "title": "71  Implementazione",
    "section": "\n71.8 Errore Standard di Misurazione del Test",
    "text": "71.8 Errore Standard di Misurazione del Test\nIl Test Standard Error of Measurement (SEM) è una misura della precisione con cui un test stima il livello di abilità latente (\\(\\theta\\)) per una persona. In altre parole, rappresenta l’incertezza associata alla stima di \\(\\theta\\). Il grafico prodotto da plot(raschModel, type = \"SE\") mostra come il SEM varia in funzione del livello di abilità latente (\\(\\theta\\)).\n\nIl SEM è una stima dell’errore standard nella misurazione di \\(\\theta\\).\nÈ inversamente proporzionale alla quantità di informazione fornita dal test a un dato livello di \\(\\theta\\):\n\n\\[\nSEM(\\theta) = \\frac{1}{\\sqrt{\\text{Informazione}(\\theta)}}\n\\]\n\nIl SEM è espresso nella stessa scala di \\(\\theta\\).\nUn valore più basso del SEM implica una stima più precisa di \\(\\theta\\).\nIl SEM non è costante: varia in base al livello di \\(\\theta\\), riflettendo il fatto che il test è più informativo per alcune abilità rispetto ad altre.\n\n\nplot(mirt_rm, type = \"SE\")\n\n\n\n\n\n\n\nNel modello Rasch, il SEM dipende dalla distribuzione dei parametri di difficoltà (\\(b\\)) degli item:\n\nIl SEM minimo si verifica intorno ai valori di \\(\\theta\\) che corrispondono ai parametri di difficoltà (\\(b\\)) degli item.\nUn SEM alto si verifica a valori di \\(\\theta\\) lontani dal range dei parametri di difficoltà (\\(b\\)), poiché il test non discrimina bene a quei livelli di abilità.\n\nIn conclusione, il grafico del SEM ci aiuta a identificare i punti di forza e di debolezza del test in termini di precisione della stima di \\(\\theta\\). Il SEM ci permette di capire in quali range di \\(\\theta\\) il test fornisce stime più affidabili.\nLa funzione testInfoPlot() fornisce il grafico del SEM insieme alla curva di informazione del test:\n\ntestInfoPlot(mirt_rm, adj_factor = 2)\n\n\n\n\n\n\n\nL’informazione del test è maggiore attorno allo zero e, di conseguenza, gli errori standard aumentano allontanandosi dallo zero.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_implementation.html#stima-dei-parametri-delle-persone",
    "href": "chapters/irt/05_implementation.html#stima-dei-parametri-delle-persone",
    "title": "71  Implementazione",
    "section": "\n71.9 Stima dei Parametri delle Persone",
    "text": "71.9 Stima dei Parametri delle Persone\nLa stima dei parametri delle persone ottenuta con il metodo di massima verosimiglianza si ottiene nel modo seguente:\n\ntheta &lt;- eRm::person.parameter(rm_sum0)\ntheta\n#&gt; \n#&gt; Person Parameters:\n#&gt; \n#&gt;  Raw Score Estimate Std.Error\n#&gt;          1  -3.3992     1.085\n#&gt;          2  -2.5218     0.832\n#&gt;          3  -1.9160     0.737\n#&gt;          4  -1.4093     0.692\n#&gt;          5  -0.9452     0.673\n#&gt;          6  -0.4965     0.668\n#&gt;          7  -0.0474     0.673\n#&gt;          8   0.4118     0.683\n#&gt;          9   0.8878     0.698\n#&gt;         10   1.3892     0.720\n#&gt;         11   1.9351     0.762\n#&gt;         12   2.5766     0.851\n#&gt;         13   3.4832     1.097\n#&gt;         14   4.4549        NA\n\nDa notare che questa tabella non mostra una stima per ogni persona. La stima dell’abilità di una persona dipende unicamente dal numero di item a cui ha risposto correttamente. Questo significa che dobbiamo calcolare una stima dell’abilità per ogni possibile punteggio totale (indicato come “punteggi grezzi” nella tabella) e possiamo assegnare tale stima a ciascuna persona che ottiene quel punteggio. Ad esempio, stimiamo che l’abilità di una persona che risponde correttamente a dieci item sia circa 1.39.\nVediamo che le stime dell’abilità aumentano con il punteggio grezzo. Questo ha senso, poiché un candidato ha maggiori probabilità di rispondere correttamente a un item se la sua abilità supera la difficoltà di quell’item. Più item vengono risposti correttamente, più è probabile che l’abilità del candidato sia elevata. Inoltre, vediamo che l’errore standard aumenta con la distanza da zero, come era prevedibile dalla mappa persona-item o dalle curve di informazione degli item e del test, dove abbiamo visto che la maggior parte degli item si trova intorno allo zero.\nLa mancanza di un errore standard per i candidati che rispondono correttamente a tutti i 14 item potrebbe lasciarci perplessi. La ragione di tale mancanza è che non esiste una stima di massima verosimiglianza per questo punteggio perfetto. Per gestire questo, la funzione person.parameter() utilizza un metodo chiamato interpolazione spline per produrre una stima dell’abilità, ma la procedura non fornisce stime dell’errore. Lo stesso sarebbe vero per i candidati che risolvono correttamente 0 item, ma in questo campione non si è verificato un punteggio di zero.\nPossiamo ottenere informazioni sulle stime dell’abilità dei singoli candidati utilizzando la funzione summary(), cioè,\n\nsummary(theta)\n#&gt; \n#&gt; Estimation of Ability Parameters\n#&gt; \n#&gt; Collapsed log-likelihood: -76.3 \n#&gt; Number of iterations: 10 \n#&gt; Number of parameters: 13 \n#&gt; \n#&gt; ML estimated ability parameters (without spline interpolated values): \n#&gt;           Estimate Std. Err.   2.5 %  97.5 %\n#&gt; theta 1    -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 2    -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 3    -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 4    -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 5     0.4118     0.683 -0.9270  1.7506\n#&gt; theta 6    -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 7    -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 8    -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 9    -1.9160     0.737 -3.3596 -0.4724\n#&gt; theta 10   -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 11   -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 12    0.4118     0.683 -0.9270  1.7506\n#&gt; theta 13   -1.9160     0.737 -3.3596 -0.4724\n#&gt; theta 14   -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 15   -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 16   -2.5218     0.832 -4.1521 -0.8915\n#&gt; theta 17   -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 18   -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 19   -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 20   -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 21   -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 22   -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 23   -1.9160     0.737 -3.3596 -0.4724\n#&gt; theta 24   -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 25   -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 26   -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 27   -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 28    0.4118     0.683 -0.9270  1.7506\n#&gt; theta 29   -2.5218     0.832 -4.1521 -0.8915\n#&gt; theta 30   -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 31   -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 32   -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 33   -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 34   -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 35   -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 36   -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 37   -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 38   -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 39   -1.9160     0.737 -3.3596 -0.4724\n#&gt; theta 40   -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 41   -1.9160     0.737 -3.3596 -0.4724\n#&gt; theta 42    0.4118     0.683 -0.9270  1.7506\n#&gt; theta 43    0.4118     0.683 -0.9270  1.7506\n#&gt; theta 44    0.4118     0.683 -0.9270  1.7506\n#&gt; theta 45   -1.9160     0.737 -3.3596 -0.4724\n#&gt; theta 46   -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 47    1.3892     0.720 -0.0226  2.8010\n#&gt; theta 48    1.3892     0.720 -0.0226  2.8010\n#&gt; theta 49   -2.5218     0.832 -4.1521 -0.8915\n#&gt; theta 50   -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 51   -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 52   -3.3992     1.085 -5.5254 -1.2730\n#&gt; theta 53    1.9351     0.762  0.4415  3.4287\n#&gt; theta 54   -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 55   -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 56   -2.5218     0.832 -4.1521 -0.8915\n#&gt; theta 57   -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 58    0.8878     0.698 -0.4796  2.2552\n#&gt; theta 59   -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 60    0.8878     0.698 -0.4796  2.2552\n#&gt; theta 61    0.8878     0.698 -0.4796  2.2552\n#&gt; theta 62    0.8878     0.698 -0.4796  2.2552\n#&gt; theta 63    0.4118     0.683 -0.9270  1.7506\n#&gt; theta 64    1.3892     0.720 -0.0226  2.8010\n#&gt; theta 65    0.8878     0.698 -0.4796  2.2552\n#&gt; theta 66    1.3892     0.720 -0.0226  2.8010\n#&gt; theta 67   -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 68   -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 69   -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 70   -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 71    0.4118     0.683 -0.9270  1.7506\n#&gt; theta 72   -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 73    0.4118     0.683 -0.9270  1.7506\n#&gt; theta 74   -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 75    1.3892     0.720 -0.0226  2.8010\n#&gt; theta 76   -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 77   -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 78    0.8878     0.698 -0.4796  2.2552\n#&gt; theta 79   -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 80    0.4118     0.683 -0.9270  1.7506\n#&gt; theta 81    0.8878     0.698 -0.4796  2.2552\n#&gt; theta 82    1.3892     0.720 -0.0226  2.8010\n#&gt; theta 83    0.8878     0.698 -0.4796  2.2552\n#&gt; theta 84    0.4118     0.683 -0.9270  1.7506\n#&gt; theta 85    0.4118     0.683 -0.9270  1.7506\n#&gt; theta 86   -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 87    1.3892     0.720 -0.0226  2.8010\n#&gt; theta 88    0.4118     0.683 -0.9270  1.7506\n#&gt; theta 89   -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 90    0.8878     0.698 -0.4796  2.2552\n#&gt; theta 91   -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 92    0.8878     0.698 -0.4796  2.2552\n#&gt; theta 93   -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 94   -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 95   -1.9160     0.737 -3.3596 -0.4724\n#&gt; theta 96   -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 97   -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 98   -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 99   -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 100  -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 101   0.8878     0.698 -0.4796  2.2552\n#&gt; theta 102  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 103   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 104   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 105  -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 106  -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 107  -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 108   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 109   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 110  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 111  -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 112   0.8878     0.698 -0.4796  2.2552\n#&gt; theta 113  -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 114  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 115  -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 116  -2.5218     0.832 -4.1521 -0.8915\n#&gt; theta 117  -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 118  -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 119   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 120  -1.9160     0.737 -3.3596 -0.4724\n#&gt; theta 121  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 122  -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 123   1.9351     0.762  0.4415  3.4287\n#&gt; theta 124  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 125  -1.9160     0.737 -3.3596 -0.4724\n#&gt; theta 126  -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 127  -2.5218     0.832 -4.1521 -0.8915\n#&gt; theta 128  -1.9160     0.737 -3.3596 -0.4724\n#&gt; theta 129  -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 130  -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 131  -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 132  -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 133  -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 134  -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 135  -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 136   0.8878     0.698 -0.4796  2.2552\n#&gt; theta 137  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 138   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 139   1.3892     0.720 -0.0226  2.8010\n#&gt; theta 140   0.8878     0.698 -0.4796  2.2552\n#&gt; theta 141   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 142  -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 143   0.8878     0.698 -0.4796  2.2552\n#&gt; theta 144  -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 145   1.9351     0.762  0.4415  3.4287\n#&gt; theta 146  -1.9160     0.737 -3.3596 -0.4724\n#&gt; theta 147  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 148  -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 149  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 150  -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 151  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 152  -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 153  -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 154   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 155  -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 156  -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 157  -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 158  -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 159   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 160  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 161  -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 162   0.8878     0.698 -0.4796  2.2552\n#&gt; theta 163  -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 164  -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 165  -1.9160     0.737 -3.3596 -0.4724\n#&gt; theta 166  -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 167  -3.3992     1.085 -5.5254 -1.2730\n#&gt; theta 168  -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 169  -3.3992     1.085 -5.5254 -1.2730\n#&gt; theta 170  -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 171  -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 172   1.3892     0.720 -0.0226  2.8010\n#&gt; theta 173  -1.9160     0.737 -3.3596 -0.4724\n#&gt; theta 174  -1.9160     0.737 -3.3596 -0.4724\n#&gt; theta 175   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 176  -1.9160     0.737 -3.3596 -0.4724\n#&gt; theta 177   1.3892     0.720 -0.0226  2.8010\n#&gt; theta 178  -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 179  -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 180  -1.9160     0.737 -3.3596 -0.4724\n#&gt; theta 181  -3.3992     1.085 -5.5254 -1.2730\n#&gt; theta 182  -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 183  -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 184   0.8878     0.698 -0.4796  2.2552\n#&gt; theta 185  -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 186   1.3892     0.720 -0.0226  2.8010\n#&gt; theta 187   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 188  -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 189  -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 190  -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 191  -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 192  -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 193  -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 194  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 195   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 196  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 197  -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 198   1.9351     0.762  0.4415  3.4287\n#&gt; theta 199   1.3892     0.720 -0.0226  2.8010\n#&gt; theta 200   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 201   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 202  -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 203  -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 204  -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 205   0.8878     0.698 -0.4796  2.2552\n#&gt; theta 206   1.9351     0.762  0.4415  3.4287\n#&gt; theta 207   1.9351     0.762  0.4415  3.4287\n#&gt; theta 208   0.8878     0.698 -0.4796  2.2552\n#&gt; theta 209   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 210   1.3892     0.720 -0.0226  2.8010\n#&gt; theta 211  -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 212  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 213  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 214   0.8878     0.698 -0.4796  2.2552\n#&gt; theta 215   1.9351     0.762  0.4415  3.4287\n#&gt; theta 216   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 217   2.5766     0.851  0.9081  4.2451\n#&gt; theta 218   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 219   1.9351     0.762  0.4415  3.4287\n#&gt; theta 220  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 221  -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 222  -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 223  -1.9160     0.737 -3.3596 -0.4724\n#&gt; theta 224   1.3892     0.720 -0.0226  2.8010\n#&gt; theta 225   0.8878     0.698 -0.4796  2.2552\n#&gt; theta 226  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 227   0.8878     0.698 -0.4796  2.2552\n#&gt; theta 228   1.3892     0.720 -0.0226  2.8010\n#&gt; theta 229  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 230   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 231   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 232  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 233  -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 234   0.8878     0.698 -0.4796  2.2552\n#&gt; theta 235  -1.9160     0.737 -3.3596 -0.4724\n#&gt; theta 236   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 237   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 238   3.4832     1.097  1.3336  5.6329\n#&gt; theta 239  -3.3992     1.085 -5.5254 -1.2730\n#&gt; theta 240  -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 241   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 242  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 243  -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 244   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 245  -1.9160     0.737 -3.3596 -0.4724\n#&gt; theta 246   1.3892     0.720 -0.0226  2.8010\n#&gt; theta 247  -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 248  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 249   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 250  -1.9160     0.737 -3.3596 -0.4724\n#&gt; theta 251  -2.5218     0.832 -4.1521 -0.8915\n#&gt; theta 252   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 253  -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 254  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 255  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 256   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 257   0.8878     0.698 -0.4796  2.2552\n#&gt; theta 258  -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 259  -2.5218     0.832 -4.1521 -0.8915\n#&gt; theta 260  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 261   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 262   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 263   3.4832     1.097  1.3336  5.6329\n#&gt; theta 264   0.8878     0.698 -0.4796  2.2552\n#&gt; theta 265   1.3892     0.720 -0.0226  2.8010\n#&gt; theta 266  -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 267   1.3892     0.720 -0.0226  2.8010\n#&gt; theta 268  -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 269  -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 270  -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 271   1.3892     0.720 -0.0226  2.8010\n#&gt; theta 272   0.8878     0.698 -0.4796  2.2552\n#&gt; theta 273  -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 274  -1.9160     0.737 -3.3596 -0.4724\n#&gt; theta 275   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 276  -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 277  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 278   1.9351     0.762  0.4415  3.4287\n#&gt; theta 279   0.8878     0.698 -0.4796  2.2552\n#&gt; theta 280   1.3892     0.720 -0.0226  2.8010\n#&gt; theta 281   1.3892     0.720 -0.0226  2.8010\n#&gt; theta 282   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 283  -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 284  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 285  -1.9160     0.737 -3.3596 -0.4724\n#&gt; theta 286   1.9351     0.762  0.4415  3.4287\n#&gt; theta 287   0.8878     0.698 -0.4796  2.2552\n#&gt; theta 288   1.3892     0.720 -0.0226  2.8010\n#&gt; theta 289   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 290   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 291  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 292   0.8878     0.698 -0.4796  2.2552\n#&gt; theta 293  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 294  -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 296   1.3892     0.720 -0.0226  2.8010\n#&gt; theta 297   1.3892     0.720 -0.0226  2.8010\n#&gt; theta 298   3.4832     1.097  1.3336  5.6329\n#&gt; theta 299   0.8878     0.698 -0.4796  2.2552\n#&gt; theta 300   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 301  -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 302  -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 303   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 304  -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 305  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 306   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 308   1.3892     0.720 -0.0226  2.8010\n#&gt; theta 309   1.3892     0.720 -0.0226  2.8010\n#&gt; theta 310  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 311  -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 312   0.8878     0.698 -0.4796  2.2552\n#&gt; theta 313   1.3892     0.720 -0.0226  2.8010\n#&gt; theta 314   0.8878     0.698 -0.4796  2.2552\n#&gt; theta 315   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 316  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 317  -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 318   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 319   2.5766     0.851  0.9081  4.2451\n#&gt; theta 320  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 321  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 322   0.8878     0.698 -0.4796  2.2552\n#&gt; theta 323   0.8878     0.698 -0.4796  2.2552\n#&gt; theta 324   2.5766     0.851  0.9081  4.2451\n#&gt; theta 325   1.3892     0.720 -0.0226  2.8010\n#&gt; theta 326   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 327   0.8878     0.698 -0.4796  2.2552\n#&gt; theta 328   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 329   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 330  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 331   0.8878     0.698 -0.4796  2.2552\n#&gt; theta 332  -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 333   0.8878     0.698 -0.4796  2.2552\n#&gt; theta 334  -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 335   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 336  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 337   1.3892     0.720 -0.0226  2.8010\n#&gt; theta 338  -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 339   1.3892     0.720 -0.0226  2.8010\n#&gt; theta 340   0.8878     0.698 -0.4796  2.2552\n#&gt; theta 341   0.8878     0.698 -0.4796  2.2552\n#&gt; theta 342   1.9351     0.762  0.4415  3.4287\n#&gt; theta 343   0.8878     0.698 -0.4796  2.2552\n#&gt; theta 344   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 345   1.3892     0.720 -0.0226  2.8010\n#&gt; theta 346  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 347   1.9351     0.762  0.4415  3.4287\n#&gt; theta 348   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 349   1.3892     0.720 -0.0226  2.8010\n#&gt; theta 350   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 351   0.8878     0.698 -0.4796  2.2552\n#&gt; theta 352  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 353  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 354  -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 355   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 356  -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 357  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 358  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 359  -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 360  -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 361  -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 362   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 363  -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 364  -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 365  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 366   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 367  -3.3992     1.085 -5.5254 -1.2730\n#&gt; theta 368  -3.3992     1.085 -5.5254 -1.2730\n#&gt; theta 369  -2.5218     0.832 -4.1521 -0.8915\n#&gt; theta 370  -1.9160     0.737 -3.3596 -0.4724\n#&gt; theta 371   0.8878     0.698 -0.4796  2.2552\n#&gt; theta 372  -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 373  -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 374  -1.9160     0.737 -3.3596 -0.4724\n#&gt; theta 375  -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 376  -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 377  -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 378  -2.5218     0.832 -4.1521 -0.8915\n#&gt; theta 379  -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 380  -1.9160     0.737 -3.3596 -0.4724\n#&gt; theta 381  -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 382  -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 383  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 384  -1.9160     0.737 -3.3596 -0.4724\n#&gt; theta 385  -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 386  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 387   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 388  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 389   0.8878     0.698 -0.4796  2.2552\n#&gt; theta 390  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 391  -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 392   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 393  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 394   0.4118     0.683 -0.9270  1.7506\n#&gt; theta 395  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 396  -1.4093     0.692 -2.7663 -0.0522\n#&gt; theta 397  -0.4965     0.668 -1.8067  0.8137\n#&gt; theta 398  -0.0474     0.673 -1.3664  1.2716\n#&gt; theta 399  -0.9452     0.673 -2.2645  0.3741\n#&gt; theta 400  -1.9160     0.737 -3.3596 -0.4724\n\nL’output – che qui abbiamo di nuovo troncato per risparmiare spazio – contiene stime ed errori standard, insieme ai limiti inferiori (2.5%) e superiori (97.5%) degli intervalli di confidenza al 95%, per tutti i candidati che hanno risposto correttamente fino a 13 item. I candidati che hanno risposto correttamente a tutti i 14 item (o a nessun item) sono omessi da questo output. Possiamo anche notare che alcuni candidati, ad esempio il secondo e il terzo, ricevono esattamente la stessa stima di abilità ed errore standard. Questo è dovuto al fatto che hanno lavorato sullo stesso set di item e hanno ottenuto lo stesso punteggio totale. In alternativa, possiamo utilizzare il comando coef(theta) per ottenere solo la stima dell’abilità per ciascun candidato.\nPossiamo anche stimare l’abilità dei candidati utilizzando la funzione mirt::fscores(). Per i modelli unidimensionali, gli argomenti più importanti di fscores() sono object e method. L’argomento object accetta il risultato della funzione mirt(). L’argomento method indica quale metodo utilizzare per stimare i parametri della persona. Per impostazione predefinita, method=\"EAP\", il che indica che il parametro della persona dovrebbe essere stimato utilizzando il metodo expected a posteriori (EAP). Possiamo calcolare le stime EAP per il modello di Rasch e stampare le sue prime sei voci inserendo:\n\ntheta_eap &lt;- fscores(mirt_rm)\nhead(theta_eap)\n#&gt;          F1\n#&gt; [1,] -0.218\n#&gt; [2,] -0.828\n#&gt; [3,] -0.828\n#&gt; [4,] -0.218\n#&gt; [5,]  0.391\n#&gt; [6,] -0.521\n\nPer impostazione predefinita mirt mostra solo le stime puntuali, ma è possibile aggiungere gli errori standard tramite l’opzione full.scores.SE = TRUE alla funzione fscores(). Gli errori standard dovrebbero essere esaminati prima di interpretare o riportare le stime dei parametri della persona.\nLa funzione fscores() fornisce anche stimatori di massima verosimiglianza (ML), massimo a posteriori (MAP) e likelihood ponderata (WLE). Ora confrontiamo i quattro tipi di stime dei parametri della persona fornite da mirt. Gli stimatori ML, MAP e WLE possono essere calcolati inserendo\n\ntheta_ml &lt;- fscores(mirt_rm, method = \"ML\", max_theta = 30)\ntheta_map &lt;- fscores(mirt_rm, method = \"MAP\")\ntheta_wle &lt;- fscores(mirt_rm, method = \"WLE\")\n\n\nests &lt;- cbind(theta_eap, theta_ml, theta_map, theta_wle)\ncolnames(ests) &lt;- c(\"EAP\", \"ML\", \"MAP\", \"WLE\")\npairs(ests, xlim = c(-3, 3), ylim = c(-3, 3))",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_implementation.html#affidabilità-condizionale",
    "href": "chapters/irt/05_implementation.html#affidabilità-condizionale",
    "title": "71  Implementazione",
    "section": "\n71.10 Affidabilità Condizionale",
    "text": "71.10 Affidabilità Condizionale\nIl concetto di affidabilità varia tra la CTT e la IRT. Nell’IRT, possiamo calcolare l’affidabilità condizionale, ossia l’affidabilità della scala a diversi livelli di theta.\n\nNella CTT, l’affidabilità è solitamente considerata come una proprietà fissa del test, indipendentemente dal livello di abilità dei rispondenti. Si misura spesso attraverso il coefficiente alfa di Cronbach o metodi simili.\nNell’IRT, invece, l’affidabilità è vista come una proprietà variabile che dipende dal livello di theta del rispondente. A diversi livelli di theta, la precisione con cui il test misura l’abilità può variare significativamente.\n\nL’affidabilità condizionale fornisce una misura più specifica e dettagliata di quanto affidabilmente un test misura l’abilità a diversi livelli di \\(\\theta\\).\n\nconRelPlot(mirt_rm)\n\n\n\n\n\n\n\n\nplot(mirt_rm, type = \"rxx\")\n\n\n\n\n\n\n\nNel caso presente,\n\na livelli medi di \\(\\theta\\): Il test mostra una buona affidabilità, indicando che è in grado di distinguere con precisione tra rispondenti con abilità medie.\nagli estremi di \\(\\theta\\): Il test mostra un’affidabilità più bassa, suggerendo che non è altrettanto efficace nel distinguere tra livelli di abilità molto alti o molto bassi.\n\nIn sostanza, l’affidabilità condizionale nell’IRT ci fornisce una comprensione più dettagliata di dove il test funziona bene e dove potrebbe richiedere miglioramenti per valutare con precisione l’abilità su tutta la gamma di theta.\nÈ comunque possibile calcolare un singolo valore di attendibilità:\n\nmarginal_rxx(mirt_rm)\n#&gt; [1] 0.698\n\n\nIl valore riportato (\\(r_{xx} = 0.698\\)) indica che circa il 70% della varianza osservata nei punteggi stimati è attribuibile al punteggio vero (\\(\\theta\\)), mentre il restante 30% è dovuto all’errore di misura. Questo valore suggerisce che il test stima l’abilità latente \\(\\theta\\) con una precisione accettabile.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_implementation.html#curva-caratteristica-del-test",
    "href": "chapters/irt/05_implementation.html#curva-caratteristica-del-test",
    "title": "71  Implementazione",
    "section": "\n71.11 Curva Caratteristica del Test",
    "text": "71.11 Curva Caratteristica del Test\nUna proprietà aggiuntiva di un modello IRT è che il punteggio complessivo delle risposte corrette (la somma dei punteggi per le risposte corrette) risulta essere una stima efficace del tratto latente sottostante. Un grafico della cosiddetta curva caratteristica della scala (scale characteristic curve) permette di valutare visivamente questo aspetto tracciando la relazione tra theta e il punteggio di risposte corrette.\n\nQuesto tipo di grafico mostra come il punteggio totale delle risposte corrette si correla con il livello di abilità latente (theta) stimato dal modello IRT.\nAd esempio, se la curva mostra che punteggi più alti di risposte corrette corrispondono sistematicamente a livelli più alti di theta e viceversa, ciò indica che il punteggio totale è un buon indicatore del tratto latente.\nAl contrario, se la curva non mostra una relazione chiara o lineare tra punteggio totale e theta, ciò potrebbe suggerire che il punteggio totale non cattura completamente la complessità o le sfumature del tratto latente.\n\nIn sintesi, la curva caratteristica della scala fornisce una rappresentazione visiva di come il punteggio totale di risposte corrette rifletta l’abilità latente misurata dal test, offrendo una visione utile per valutare l’efficacia del punteggio totale come indicatore del tratto latente in questione.\n\nscaleCharPlot(mirt_rm)\n\n\n\n\n\n\n\n\nplot(mirt_rm, type = \"score\")\n\n\n\n\n\n\n\nQuesta curva di solito assume la forma di una S, poiché la relazione è più forte nel range medio di theta e meno precisa agli estremi (come già visto nella curva di informazione del test).\nPossiamo ovviamente testare anche questo con una semplice correlazione. Per prima cosa, estraiamo il punteggio latente IRT utilizzando la funzione fscores(). Quindi lo correliamo con il punteggio di risposte corrette.\n\nscore &lt;- fscores(mirt_rm)\nsumscore &lt;- rowSums(responses)\ncor.test(score, sumscore)\n#&gt; \n#&gt;  Pearson's product-moment correlation\n#&gt; \n#&gt; data:  score and sumscore\n#&gt; t = 1097, df = 398, p-value &lt;2e-16\n#&gt; alternative hypothesis: true correlation is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  1 1\n#&gt; sample estimates:\n#&gt; cor \n#&gt;   1\n\nNel caso presente, la correlazione è quasi perfetta.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_implementation.html#riflessioni-conclusive",
    "href": "chapters/irt/05_implementation.html#riflessioni-conclusive",
    "title": "71  Implementazione",
    "section": "\n71.12 Riflessioni Conclusive",
    "text": "71.12 Riflessioni Conclusive\nTradizionalmente, il punteggio totale ottenuto in un test psicologico è stato considerato come la misura più efficace dell’abilità o della predisposizione di una persona rispetto a un certo tratto di personalità. Tuttavia, la dipendenza del punteggio totale dalla difficoltà degli item presenta limitazioni significative. Ad esempio, due persone possono ottenere lo stesso punteggio totale rispondendo in modo diverso a item di varia difficoltà, il che non riflette accuratamente le loro abilità reali.\nNella Teoria Classica dei Test (CTT), l’enfasi è posta sul punteggio totale, ma questa prospettiva ignora le variazioni nella difficoltà degli item e assume che gli errori di misurazione si annullino reciprocamente attraverso la procedura di sommazione. Tuttavia, la CTT è limitata dalla sua assunzione di varianze di errore uniformi per tutti i rispondenti, dall’aspettativa di errori di misurazione nulli e dalla focalizzazione esclusiva sui punteggi totali, senza considerare l’adattamento di item e persone.\nAl contrario, la Teoria della Risposta all’Item (IRT) cambia il focus dai punteggi totali alle risposte a ciascun item, sfruttando le caratteristiche degli item. L’IRT descrive come attributi come abilità, atteggiamento o personalità, insieme alle caratteristiche degli item, influenzino la probabilità di fornire una risposta. Il Modello di Rasch, una forma semplice di IRT per risposte binarie, stabilisce una relazione diretta tra la probabilità di una risposta corretta e il livello di abilità del rispondente.\nLa stima dell’abilità in IRT non dipende dagli specifici item somministrati, permettendo di confrontare i risultati tra gruppi diversi con lo stesso set di item. Inoltre, la qualità degli item è valutata indipendentemente dal campione di rispondenti, rendendo le proprietà degli item costanti tra diversi gruppi con varie abilità.\nL’IRT supera i limiti della CTT stimando congiuntamente le proprietà degli item e il livello di abilità dei rispondenti. Le caratteristiche degli item diventano indipendenti dal campione di individui utilizzato per costruire il test, permettendo la creazione di insiemi di item equivalenti per misurare abilità latenti. Questo approccio offre maggiore precisione e affidabilità nelle misurazioni, assicurando la comparabilità tra diversi gruppi di individui. In conclusione, l’IRT rappresenta un metodo statistico avanzato e versatile per una valutazione più accurata e affidabile di tratti e abilità in contesti psicometrici.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_implementation.html#session-info",
    "href": "chapters/irt/05_implementation.html#session-info",
    "title": "71  Implementazione",
    "section": "\n71.13 Session Info",
    "text": "71.13 Session Info\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] grid      stats4    stats     graphics  grDevices utils     datasets \n#&gt; [8] methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] psychotools_0.7-4 ggmirt_0.1.0      TAM_4.2-21        CDM_8.2-6        \n#&gt;  [5] mvtnorm_1.3-3     mirt_1.44.0       lattice_0.22-6    eRm_1.0-6        \n#&gt;  [9] ggokabeito_0.1.0  see_0.10.0        MASS_7.3-65       viridis_0.6.5    \n#&gt; [13] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n#&gt; [17] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n#&gt; [21] lavaan_0.6-19     psych_2.4.12      scales_1.3.0      markdown_1.13    \n#&gt; [25] knitr_1.49        lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n#&gt; [29] dplyr_1.1.4       purrr_1.0.4       readr_2.1.5       tidyr_1.3.1      \n#&gt; [33] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] splines_4.4.2        later_1.4.1          R.oo_1.27.0         \n#&gt;   [4] XML_3.99-0.18        rpart_4.1.24         lifecycle_1.0.4     \n#&gt;   [7] Rdpack_2.6.2         rstatix_0.7.2        rprojroot_2.0.4     \n#&gt;  [10] globals_0.16.3       rockchalk_1.8.157    backports_1.5.0     \n#&gt;  [13] magrittr_2.0.3       openxlsx_4.2.8       Hmisc_5.2-2         \n#&gt;  [16] rmarkdown_2.29       yaml_2.3.10          httpuv_1.6.15       \n#&gt;  [19] qgraph_1.9.8         zip_2.3.2            sessioninfo_1.2.3   \n#&gt;  [22] cowplot_1.1.3        pbapply_1.7-2        minqa_1.2.8         \n#&gt;  [25] multcomp_1.4-28      abind_1.4-8          audio_0.1-11        \n#&gt;  [28] quadprog_1.5-8       R.utils_2.13.0       nnet_7.3-20         \n#&gt;  [31] TH.data_1.1-3        sandwich_3.1-1       listenv_0.9.1       \n#&gt;  [34] testthat_3.2.3       RPushbullet_0.3.4    vegan_2.6-10        \n#&gt;  [37] arm_1.14-4           parallelly_1.42.0    permute_0.9-7       \n#&gt;  [40] codetools_0.2-20     tidyselect_1.2.1     farver_2.1.2        \n#&gt;  [43] lme4_1.1-36          base64enc_0.1-3      jsonlite_1.9.0      \n#&gt;  [46] polycor_0.8-1        progressr_0.15.1     Formula_1.2-5       \n#&gt;  [49] survival_3.8-3       emmeans_1.10.7       tools_4.4.2         \n#&gt;  [52] snow_0.4-4           Rcpp_1.0.14          glue_1.8.0          \n#&gt;  [55] mnormt_2.1.1         admisc_0.37          xfun_0.51           \n#&gt;  [58] mgcv_1.9-1           withr_3.0.2          beepr_2.0           \n#&gt;  [61] fastmap_1.2.0        boot_1.3-31          digest_0.6.37       \n#&gt;  [64] mi_1.1               timechange_0.3.0     R6_2.6.1            \n#&gt;  [67] mime_0.12            estimability_1.5.1   colorspace_2.1-1    \n#&gt;  [70] gtools_3.9.5         jpeg_0.1-10          R.methodsS3_1.8.2   \n#&gt;  [73] generics_0.1.3       data.table_1.17.0    corpcor_1.6.10      \n#&gt;  [76] SimDesign_2.18       htmlwidgets_1.6.4    pkgconfig_2.0.3     \n#&gt;  [79] sem_3.1-16           gtable_0.3.6         brio_1.1.5          \n#&gt;  [82] htmltools_0.5.8.1    carData_3.0-5        png_0.1-8           \n#&gt;  [85] reformulas_0.4.0     rstudioapi_0.17.1    tzdb_0.4.0          \n#&gt;  [88] reshape2_1.4.4       coda_0.19-4.1        checkmate_2.3.2     \n#&gt;  [91] nlme_3.1-167         curl_6.2.1           nloptr_2.1.1        \n#&gt;  [94] zoo_1.8-13           parallel_4.4.2       miniUI_0.1.1.1      \n#&gt;  [97] foreign_0.8-88       pillar_1.10.1        vctrs_0.6.5         \n#&gt; [100] promises_1.3.2       car_3.1-3            OpenMx_2.21.13      \n#&gt; [103] xtable_1.8-4         Deriv_4.1.6          cluster_2.1.8       \n#&gt; [106] dcurver_0.9.2        GPArotation_2024.3-1 htmlTable_2.4.3     \n#&gt; [109] evaluate_1.0.3       pbivnorm_0.6.0       cli_3.6.4           \n#&gt; [112] kutils_1.73          compiler_4.4.2       rlang_1.1.5         \n#&gt; [115] future.apply_1.11.3  ggsignif_0.6.4       labeling_0.4.3      \n#&gt; [118] fdrtool_1.2.18       plyr_1.8.9           stringi_1.8.4       \n#&gt; [121] munsell_0.5.1        lisrelToR_0.3        pacman_0.5.1        \n#&gt; [124] Matrix_1.7-2         hms_1.1.3            glasso_1.11         \n#&gt; [127] future_1.34.0        shiny_1.10.0         rbibutils_2.3       \n#&gt; [130] igraph_2.1.4         broom_1.0.7          RcppParallel_5.1.10\n\n\n\n\n\nDebelak, R., Strobl, C., & Zeigenfuse, M. D. (2022). An introduction to the rasch model with examples in r. CRC Press.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/01_lgm_intro.html",
    "href": "chapters/lgm/01_lgm_intro.html",
    "title": "72  Curve di crescita latente",
    "section": "",
    "text": "72.1 Introduzione\nQuando si vuole studiare come le persone cambiano nel tempo, è necessario raccogliere dati longitudinali, ovvero misurazioni ripetute sullo stesso gruppo di individui. A differenza delle analisi tradizionali che confrontano le persone in un unico momento, i dati longitudinali permettono di tracciare le traiettorie individuali di cambiamento.\nLe caratteristiche distintive di questi dati, come la presenza di variazione sia tra gli individui sia all’interno degli stessi e la dipendenza tra le osservazioni ripetute, richiedono l’utilizzo di modelli statistici specifici.\nI modelli a curve di crescita latente (LGCM) sono stati sviluppati proprio per affrontare queste complessità. Questi modelli consentono di modellare la crescita e il cambiamento nel tempo, tenendo conto sia delle differenze individuali nelle traiettorie di sviluppo sia degli effetti di variabili esterne. Ad esempio, in uno studio sullo sviluppo cognitivo, un LGCM può essere utilizzato per analizzare come il quoziente intellettivo cambia dall’infanzia all’età adulta, tenendo conto di fattori come l’ambiente familiare e l’istruzione.\nUno dei principali vantaggi degli LGCM è la loro flessibilità. Possono essere applicati a una vasta gamma di dati e possono essere utilizzati per rispondere a diverse domande di ricerca. Inoltre, gli LGCM permettono di identificare i periodi di vita in cui il cambiamento è più rapido o più lento, e di valutare l’impatto di interventi specifici sulle traiettorie di sviluppo.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>72</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/01_lgm_intro.html#concettualizzazioni-del-tempo-in-studi-longitudinali",
    "href": "chapters/lgm/01_lgm_intro.html#concettualizzazioni-del-tempo-in-studi-longitudinali",
    "title": "72  Curve di crescita latente",
    "section": "72.2 Concettualizzazioni del Tempo in Studi Longitudinali",
    "text": "72.2 Concettualizzazioni del Tempo in Studi Longitudinali\nNegli studi longitudinali, ci sono principalmente cinque modalità di concettualizzare e analizzare il trascorrere del tempo:\n\nDisegno Trasversale (Cross-Sectional Design):\n\nQuesto metodo studia gruppi di persone di età diverse, ma in un unico momento temporale, senza misurazioni ripetute sugli stessi individui.\nAd esempio, si potrebbe confrontare le capacità cognitive di bambini di 6, 8 e 10 anni in un unico momento.\nQuesto approccio permette di ottenere dati preliminari e valutare le relazioni tra variabili, ma non può descrivere i processi evolutivi nel tempo, poiché le differenze tra gruppi di età potrebbero dipendere sia da fattori di sviluppo che da differenze tra coorti.\n\nDisegno Longitudinale di Singola Coorte:\n\nPrevede misurazioni ripetute sugli stessi individui nel tempo.\nAd esempio, si potrebbero valutare le stesse persone a 6, 8 e 10 anni per studiare il loro sviluppo cognitivo.\nQuesto permette di analizzare i cambiamenti intra-individuali nel tempo utilizzando modelli di panel, modelli di curva di crescita o altri modelli di cambiamento.\nI modelli di panel esaminano variazioni in sequenze di misurazioni, mentre i modelli di curva di crescita analizzano la variabilità nel cambiamento individuale.\n\nDisegno Cross-Sequenziale:\n\nCombina un disegno trasversale iniziale con una successione di misurazioni longitudinali.\nAd esempio, si potrebbero valutare gruppi di bambini di 6, 8 e 10 anni, e poi seguirli nel tempo con misurazioni successive.\nQuesto approccio permette di studiare sia gli effetti legati all’età che le differenze tra coorti, anche se può essere più complesso separare questi fattori.\n\nDisegno Sequenziale di Coorte:\n\nAvvia uno studio longitudinale con gruppi (coorti) di partecipanti della stessa età.\nOgni nuova coorte attraversa la stessa fascia di età nel tempo.\nAd esempio, si potrebbero valutare gruppi di bambini di 6 anni, 8 anni e 10 anni, seguendoli negli anni successivi.\nQuesto design aiuta a distinguere gli effetti legati all’età da quelli dovuti alle differenze tra coorti.\n\nDisegno Sequenziale Temporale:\n\nMeno comune, ma utile per separare gli effetti legati all’età da quelli legati al tempo di misurazione.\nMantiene invariata la fascia di età dei partecipanti, ma valuta nuove e vecchie coorti in diversi momenti temporali.\nAd esempio, si potrebbero valutare gruppi di bambini di 6-8 anni in diversi anni, per distinguere i cambiamenti legati all’età da quelli legati al passare del tempo.\nQuesto disegno però non permette di separare gli effetti di coorte dall’interazione tra età e tempo di misurazione.\n\n\nIn sintesi, ciascuno di questi approcci offre vantaggi e svantaggi nel comprendere l’impatto dell’età, delle coorti e del tempo di misurazione sui fenomeni di interesse negli studi longitudinali. La scelta del design dipende dagli obiettivi specifici della ricerca.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>72</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/01_lgm_intro.html#validità",
    "href": "chapters/lgm/01_lgm_intro.html#validità",
    "title": "72  Curve di crescita latente",
    "section": "72.3 Validità",
    "text": "72.3 Validità\nRischi per la validità includono la regressione verso la media, gli effetti del retest, effetti di selezione, attrito selettivo e effetti di strumentazione.\n\nLa regressione verso la media indica che i punteggi estremi tendono a spostarsi verso la media nelle misurazioni successive. È un fenomeno di inaffidabilità nelle misure ripetute e può essere mitigato utilizzando modelli SEM a variabili latenti.\nGli effetti del retest emergono quando una misura è sensibile all’esposizione ripetuta. Questi effetti possono essere stimati e corretti assegnando casualmente ai partecipanti la ricezione o meno di una misurazione, o utilizzando protocolli di mancata risposta pianificata.\nGli effetti di selezione si verificano quando il piano di campionamento non fornisce un campione rappresentativo della popolazione di interesse. L’attrito selettivo si riferisce alla perdita di partecipanti correlata a specifiche caratteristiche del campione.\nGli effetti di strumentazione possono alterare le proprietà di misurazione del fenomeno studiato. Misure sensibili al cambiamento sono cruciali per rilevare i processi di cambiamento.\n\nIn sintesi, gli studi longitudinali affrontano diverse sfide di validità, che richiedono metodi sofisticati per la misurazione e l’analisi dei dati. È fondamentale considerare come varie forze, come la regressione verso la media e gli effetti di retest, possano influenzare i risultati, e come strumenti di misurazione adeguati possano catturare in modo efficace i cambiamenti nel tempo.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>72</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/01_lgm_intro.html#dati-mancanti",
    "href": "chapters/lgm/01_lgm_intro.html#dati-mancanti",
    "title": "72  Curve di crescita latente",
    "section": "72.4 Dati Mancanti",
    "text": "72.4 Dati Mancanti\nIl problema dei dati mancanti nei disegni longitudinali rappresenta una sfida cruciale nella ricerca, specialmente nei campi della psicologia e delle scienze sociali. I dati mancanti portano a due conseguenze principali: la perdita di potenza statistica e l’introduzione di bias.\nLa perdita di potenza si verifica perché la riduzione dei dati comporta una minore capacità di rilevare effetti reali. Nei disegni longitudinali, questa perdita è particolarmente critica, poiché le misure ripetute nel tempo sono essenziali per comprendere le dinamiche e i cambiamenti. Una riduzione nel numero di osservazioni può rendere difficile individuare tendenze significative o effetti degli interventi.\nIl bias si introduce quando i dati mancanti non sono distribuiti casualmente. Se la mancanza di dati è legata a caratteristiche specifiche dei soggetti, i risultati possono non essere più rappresentativi della popolazione originale, portando a conclusioni errate o fuorvianti.\nI metodi tradizionali per gestire i dati mancanti, come l’eliminazione dei casi o l’utilizzo dell’ultimo punto disponibile, possono peggiorare la situazione, aumentando sia la perdita di potenza sia il bias. Al contrario, le tecniche moderne mirano a preservare il più possibile la potenza del dataset originale e, se usate correttamente, possono ridurre il bias selettivo.\nTra le tecniche moderne per la gestione dei dati mancanti troviamo l’imputazione multipla, che crea più set completi di dati imputando i valori mancanti in modo da riflettere l’incertezza associata a tali valori. L’uso di variabili ausiliarie appropriate nel modello di analisi e nel processo di imputazione contribuisce a minimizzare il bias.\nLe variabili ausiliarie sono fondamentali: se scelte correttamente, possono spiegare il meccanismo dei dati mancanti e ridurre il bias. Se assenti o selezionate in modo inappropriato, i risultati dell’analisi possono rimanere distorti e le conclusioni dello studio risultare compromesse.\nUn approccio basato sui dati, come l’uso dell’imputazione multipla con MICE (Multiple Imputation by Chained Equations) o missForest, e l’inclusione di tutte le variabili disponibili (comprese informazioni potenzialmente non lineari), permette di rappresentare al meglio il meccanismo di mancanza dei dati. Questo approccio presuppone generalmente che le relazioni tra variabili siano lineari, ma consente anche di includere informazioni non lineari rilevanti. Successivamente, quando viene selezionato un sottoinsieme di variabili per l’analisi, l’effetto della gestione dei dati mancanti è mantenuto, aumentando la generalizzabilità delle analisi considerate le variabili incluse nel protocollo.\nIn sintesi, la gestione dei dati mancanti nei disegni longitudinali richiede un’attenta considerazione del meccanismo di mancanza e l’applicazione di tecniche moderne che possano mitigare la perdita di potenza e il bias. Questo è essenziale per garantire l’affidabilità e la validità dei risultati della ricerca.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>72</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/01_lgm_intro.html#domande-della-ricerca",
    "href": "chapters/lgm/01_lgm_intro.html#domande-della-ricerca",
    "title": "72  Curve di crescita latente",
    "section": "72.5 Domande della ricerca",
    "text": "72.5 Domande della ricerca\nL’analisi di dati longitudinali nella ricerca psicologica è particolarmente interessata a valutare quanto le persone cambino in un particolare aspetto o tratto nel tempo (cioè la crescita media), quanto questa crescita varia rispetto agli altri individui (cioè la varianza della crescita), e come le persone cambiano rispetto a se stesse nel tempo (cioè i modelli di crescita intra-individuale). I modelli di crescita media (LGM) descrivono i primi due di questi elementi chiave, mentre i modelli di punteggio di cambiamento latente (LCSM) descrivono il terzo (come i valori precedenti prevedono i valori successivi nel tempo all’interno della stessa persona).\nIn ambito di ricerca psicologica, l’analisi di dati longitudinali si concentra sullo studio di come le persone cambiano in un particolare tratto o aspetto nel tempo, sulla variazione di questo cambiamento rispetto ad altre persone e su come le persone cambiano rispetto a se stesse nel tempo. I modelli di crescita media (LGM) e quelli di punteggio di cambiamento latente (LCSM) vengono utilizzati per descrivere questi aspetti.\nGrimm et al. (2016) identificano cinque motivi principali per cui questi modelli vengono utilizzati.\n\nIn primo luogo, l’analisi longitudinale consente di identificare direttamente il cambiamento e la stabilità intra-individuale. Ciò significa che è possibile valutare in che modo specifici attributi dell’individuo cambiano o rimangono gli stessi nel tempo, attraverso la misurazione ripetuta della stessa persona.\nIn secondo luogo, l’analisi longitudinale consente di identificare le differenze interindividuali nel cambiamento intra-individuale, ovvero se diversi individui cambiano in modi diversi, in quantità o direzioni diverse o se passano da uno stadio all’altro in momenti diversi.\nIn terzo luogo, l’analisi longitudinale consente di analizzare le interrelazioni nel cambiamento comportamentale, ovvero come i cambiamenti in una variabile influenzino i cambiamenti in un’altra variabile.\nIn quarto luogo, l’analisi longitudinale consente di analizzare le cause del cambiamento intra-individuale, ovvero di identificare i fattori e/o i meccanismi variabili nel tempo che influenzano i cambiamenti intra-individuali.\nInfine, in quinto luogo, l’analisi longitudinale consente di analizzare le cause delle differenze interindividuali nel cambiamento intra-individuale, ovvero di identificare le variabili invarianti nel tempo che sono correlate a specifici aspetti del cambiamento all’interno della persona, come le caratteristiche demografiche, gli interventi sperimentali e le caratteristiche dei contesti degli individui.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>72</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/01_lgm_intro.html#riflessioni-conclusive",
    "href": "chapters/lgm/01_lgm_intro.html#riflessioni-conclusive",
    "title": "72  Curve di crescita latente",
    "section": "72.6 Riflessioni Conclusive",
    "text": "72.6 Riflessioni Conclusive\nQuesto capitolo ha introdotto le peculiarità dei dati longitudinali e l’approccio necessario per studiare il cambiamento umano nel tempo. L’uso di disegni e modelli specifici, dalle curve di crescita latente alle tecniche di imputazione per i dati mancanti, è cruciale per comprendere come individui e gruppi evolvano nel tempo.\nGli studi longitudinali richiedono una gestione attenta delle complessità statistiche per evitare che i risultati siano distorti o di difficile generalizzazione. In particolare, il controllo di variabili di confondimento e la corretta modellazione della crescita intra- e interindividuale evidenziano quanto la qualità dei risultati dipenda dalla precisione nella progettazione dello studio e nella selezione delle metodologie analitiche.\nLa metodologia longitudinale ci ricorda che il cambiamento umano è complesso e multiforme. Sebbene ogni modello rappresenti un tentativo di cogliere questa complessità, nessun approccio è esaustivo: per cogliere appieno le sfumature del cambiamento, potrebbe essere necessario combinare più metodologie, integrando i punti di forza di ciascun approccio.\n\n\n\n\nGrimm, K. J., Ram, N., & Estabrook, R. (2016). Growth modeling: Structural equation and multilevel modeling approaches. Guilford Publications.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>72</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/02_lgm_prelims.html",
    "href": "chapters/lgm/02_lgm_prelims.html",
    "title": "73  Considerazioni Preliminari",
    "section": "",
    "text": "73.1 Strutture dei dati\nIn questo capitolo esamineremo alcune questioni pratiche e preliminari relative all’organizzazione di dati che provengono da studi longitudinali\nTradizionalmente, gli studi longitudinali venivano condotti con un numero relativamente basso di valutazioni ripetute (meno di 8) e un numero elevato di individui (più di 200). Tuttavia, i progressi nelle teorie statistica, che includono l’utilizzo di modelli non lineari, e nella tecnologia di raccolta dati, come i sondaggi basati sul web e gli smartphone, hanno notevolmente ampliato le possibilità di raccogliere e analizzare dati longitudinali. In particolare, Grimm et al. (2016) hanno discusso l’applicazione di modelli di crescita latente a dati longitudinali di grandi dimensioni, comprendenti fino a 50,000 individui e 1,000 valutazioni ripetute.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>73</span>  <span class='chapter-title'>Considerazioni Preliminari</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/02_lgm_prelims.html#strutture-dei-dati",
    "href": "chapters/lgm/02_lgm_prelims.html#strutture-dei-dati",
    "title": "73  Considerazioni Preliminari",
    "section": "",
    "text": "73.1.1 Formato Long e Wide\nI dati longitudinali tipicamente si presentano in due forme: long e wide. Nel formato long, la descrizione del tempo è sulle righe; nel formato wide le variabili relative ad ogni occasione temporale sono organizzate in colonne. È possibile trasformere i dati dal formato long in formato wide e viceversa usando le funzioni R pivot_wider() e pivot_longer(). La sintassi è spiegata nella pagina web tidyr.\nPer fare un esempio, esaminiamo il cambiamento nel rendimento in matematica dei bambini durante la scuola elementare e media utilizzando il set di dati NLSY-CYA. Questi dati sono stati analizzati da Grimm et al. (2016) e possono essere utilizzati per illustrare i concetti relativi alle analisi di cambiamento longitudinale.\nIniziamo a leggere i dati in R.\n\n# set filepath for data file\nfilepath &lt;- \"https://raw.githubusercontent.com/LRI-2/Data/main/GrowthModeling/nlsy_math_long_R.dat\"\n\n# read in the text data file using the url() function\ndat &lt;- read.table(\n  file = url(filepath),\n  na.strings = \".\"\n) # indicates the missing data designator\n# copy data with new name\nnlsy_math_long &lt;- dat\n\n# Add names the columns of the data set\nnames(nlsy_math_long) &lt;- c(\n  \"id\", \"female\", \"lb_wght\",\n  \"anti_k1\", \"math\", \"grade\",\n  \"occ\", \"age\", \"men\",\n  \"spring\", \"anti\"\n)\n\n# view the first few observations in the data set\nhead(nlsy_math_long) |&gt; \nprint()\n\n    id female lb_wght anti_k1 math grade occ age men spring anti\n1  201      1       0       0   38     3   2 111   0      1    0\n2  201      1       0       0   55     5   3 135   1      1    0\n3  303      1       0       1   26     2   2 121   0      1    2\n4  303      1       0       1   33     5   3 145   0      1    2\n5 2702      0       0       0   56     2   2 100  NA      1    0\n6 2702      0       0       0   58     4   3 125  NA      1    2\n\n\nI dati sono qui forniti nel formato long.\nContiamo il numero di partecipanti.\n\nnlsy_math_long |&gt;\n  distinct(id) |&gt;\n  count() |&gt; \n  print()\n\n    n\n1 932\n\n\nCon pivot_wider possiamo trasformare i dati in formato wide.\n\nnlsy_math_wide &lt;- nlsy_math_long |&gt; \n  pivot_wider(names_from = grade, values_from = math)\n\nnlsy_math_wide |&gt;\n  head() |&gt; \n  print()\n\n# A tibble: 6 x 16\n     id female lb_wght anti_k1   occ   age   men spring  anti   `3`\n  &lt;int&gt;  &lt;int&gt;   &lt;int&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1   201      1       0       0     2   111     0      1     0    38\n2   201      1       0       0     3   135     1      1     0    NA\n3   303      1       0       1     2   121     0      1     2    NA\n4   303      1       0       1     3   145     0      1     2    NA\n5  2702      0       0       0     2   100    NA      1     0    NA\n6  2702      0       0       0     3   125    NA      1     2    NA\n# i 6 more variables: `5` &lt;int&gt;, `2` &lt;int&gt;, `4` &lt;int&gt;, `8` &lt;int&gt;,\n#   `6` &lt;int&gt;, `7` &lt;int&gt;",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>73</span>  <span class='chapter-title'>Considerazioni Preliminari</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/02_lgm_prelims.html#visualizzazione-dei-dati-longitudinali",
    "href": "chapters/lgm/02_lgm_prelims.html#visualizzazione-dei-dati-longitudinali",
    "title": "73  Considerazioni Preliminari",
    "section": "73.2 Visualizzazione dei dati longitudinali",
    "text": "73.2 Visualizzazione dei dati longitudinali\nCome in qualsiasi analisi statistica, è importante esaminare attentamente i dati. Ciò include la produzione di sia riepiloghi quantitativi che visualizzazioni. Per fare un esempio di visualizzazione di dati longitudinali, esaminiamo il cambiamento nel rendimento in matematica dei bambini durante la scuola elementare e media utilizzando il set di dati NLSY-CYA (si veda Grimm et al., 2016). Da questi dati, selezioniamo solo il grado scolastico, il codice identificativo e il punteggio di matematica.\n\nnlsy_math_only_long &lt;- nlsy_math_long |&gt;\n    dplyr::select(id, grade, math)\n\nLe traiettorie di cambiamento intra-individuale possono essere prodotte nel modo seguente.\n\nnlsy_math_long |&gt; # data set\n  ggplot(aes(x = grade, y = math, group = id)) + # setting variables\n  geom_point(size = .5) + # adding points to plot\n  geom_line(alpha=0.2) + # adding lines to plot\n  # setting the x-axis with breaks and labels\n  scale_x_continuous(\n    limits = c(2, 8),\n    breaks = c(2, 3, 4, 5, 6, 7, 8),\n    name = \"Grade at Testing\"\n  ) +\n  # setting the y-axis with limits breaks and labels\n  scale_y_continuous(\n    limits = c(10, 90),\n    breaks = c(10, 30, 50, 70, 90),\n    name = \"PIAT Mathematics\"\n  )\n\n\n\n\n\n\n\n\nEsaminiamo le traiettorie di sviluppo dei primi 9 partecipanti.\n\nsubset_it &lt;- c(201, 303, 2702, 4303, 5002, 5005, 5701, 6102, 6801)\ntemp &lt;- nlsy_math_long[nlsy_math_long$id %in% subset_it, ]\n\ntemp |&gt;\n  ggplot(aes(x = grade, y = math)) +\n  geom_point() +\n  geom_line() +\n  # coord_cartesian(ylim = c(1, 4)) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~id)",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>73</span>  <span class='chapter-title'>Considerazioni Preliminari</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/02_lgm_prelims.html#data-screening",
    "href": "chapters/lgm/02_lgm_prelims.html#data-screening",
    "title": "73  Considerazioni Preliminari",
    "section": "73.3 Data screening",
    "text": "73.3 Data screening\nPrima di adattare i modelli di crescita ai dati longitudinali, è essenziale effettuare una valutazione preliminare dei dati e acquisire informazioni di base sulle variabili da utilizzare nell’analisi. Una delle prime fasi di questa valutazione preliminare è l’ispezione della distribuzione dei punteggi per ogni variabile, utilizzando le principali statistiche descrittive univariate, come la media, la mediana, la varianza (deviazione standard), l’asimmetria, la curtosi, il minimo, il massimo, l’intervallo e il numero di osservazioni per ogni variabile, in base alla metrica del tempo scelta. Le statistiche descrittive bivariate, come le correlazioni/covarianze e le tabelle di frequenza bivariate per variabili nominali o ordinali, possono fornire informazioni sui possibili schemi e relazioni non lineari, nonché sui potenziali valori anomali e codici errati.\nI dati longitudinali sono caratterizzati dall’ordinamento dei dati lungo una o più metriche del tempo (ad esempio, l’occasione di misurazione, l’età, la data, il tempo dall’evento, il numero di esposizioni, ecc.). È importante esaminare come la media, la varianza e il numero di casi disponibili cambiano attraverso le misure ripetute (ad esempio wght5, wght6, wght7). Va notato che la selezione della metrica del tempo influisce notevolmente sulla capacità di interpretare i risultati di qualsiasi modello di crescita specifico. Pertanto, durante la fase di selezione dei dati, è necessario considerare attentamente come varie proprietà dei dati longitudinali cambiano quando i dati sono organizzati in relazione a diverse metriche del tempo.\nPer i dati dell’esempio, le statistiche descrittive possono essere ottenute nel modo seguente.\n\ndescribe(nlsy_math_long) |&gt;\n    print()\n\n        vars    n      mean        sd median   trimmed       mad min\nid         1 2221 528449.15 327303.70 497403 515466.90 384144.63 201\nfemale     2 2221      0.49      0.50      0      0.49      0.00   0\nlb_wght    3 2221      0.08      0.27      0      0.00      0.00   0\nanti_k1    4 2221      1.42      1.50      1      1.19      1.48   0\nmath       5 2221     46.12     12.80     46     46.22     11.86  12\ngrade      6 2221      4.51      1.77      4      4.44      1.48   2\nocc        7 2221      2.84      0.79      3      2.77      1.48   2\nage        8 2221    126.90     22.06    126    126.28     25.20  82\nmen        9 1074      0.19      0.40      0      0.12      0.00   0\nspring    10 2221      0.65      0.48      1      0.69      0.00   0\nanti      11 2170      1.58      1.54      1      1.38      1.48   0\n            max   range  skew kurtosis      se\nid      1256601 1256400  0.30    -0.90 6945.07\nfemale        1       1  0.03    -2.00    0.01\nlb_wght       1       1  3.10     7.63    0.01\nanti_k1       8       8  1.14     1.14    0.03\nmath         81      69 -0.03    -0.18    0.27\ngrade         8       6  0.26    -0.92    0.04\nocc           5       3  0.55    -0.48    0.02\nage         175      93  0.19    -0.91    0.47\nmen           1       1  1.54     0.37    0.01\nspring        1       1 -0.63    -1.61    0.01\nanti          8       8  0.98     0.64    0.03\n\n\nEsaminiamo le statistiche descrittive bivariate.\n\n# Calcola la matrice di correlazione e arrotondala a 2 decimali\ncor_matrix &lt;- cor(nlsy_math_long, use = \"pairwise.complete.obs\") |&gt; round(2)\n\n# Imposta i valori al di sopra della diagonale principale a NA\ncor_matrix[!lower.tri(cor_matrix, diag = TRUE)] &lt;- NA\n\n# Stampa solo la matrice triangolare inferiore\nprint(cor_matrix, na.print = \"\")\n\n           id female lb_wght anti_k1  math grade  occ  age  men spring anti\nid       1.00                                                              \nfemale  -0.01   1.00                                                       \nlb_wght -0.01   0.06    1.00                                               \nanti_k1 -0.02  -0.09    0.03    1.00                                       \nmath    -0.22  -0.05   -0.03   -0.08  1.00                                 \ngrade   -0.01   0.00   -0.02   -0.03  0.59  1.00                           \nocc      0.01  -0.02   -0.03   -0.04  0.53  0.87 1.00                      \nage     -0.01  -0.04    0.01   -0.01  0.58  0.95 0.86 1.00                 \nmen     -0.02   0.02    0.04    0.01  0.30  0.62 0.57 0.64 1.00            \nspring  -0.11   0.04    0.03   -0.01  0.29  0.12 0.17 0.21 0.16   1.00     \nanti     0.01  -0.07    0.02    0.52 -0.05  0.04 0.04 0.06 0.13  -0.01    1\n\n\nScomponiamo i punteggi nelle componenti tra i soggetti ed entro i soggetti.\nEsaminiamo la distribuzione delle medie dei punteggi tra i soggetti.\n\ntmp &lt;- meanDecompose(math ~ id, data = nlsy_math_long)\n\n\nplot(\n    testDistribution(\n        tmp[[\"math by id\"]]$X,\n        extremevalues = \"theoretical\", ev.perc = .001\n    ),\n    varlab = \"Between Person Math Scores\"\n)\n\n\n\n\n\n\n\n\nEsaminiamo la distribuzione dei punteggi entro i soggetti.\n\nplot(\n    testDistribution(\n        tmp[[\"math by residual\"]]$X,\n        extremevalues = \"theoretical\", ev.perc = .001\n    ),\n    varlab = \"Within Person Math Scores\"\n)",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>73</span>  <span class='chapter-title'>Considerazioni Preliminari</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/02_lgm_prelims.html#attendibilità",
    "href": "chapters/lgm/02_lgm_prelims.html#attendibilità",
    "title": "73  Considerazioni Preliminari",
    "section": "73.4 Attendibilità",
    "text": "73.4 Attendibilità\nPer garantire la validità delle analisi, è necessario valutare l’affidabilità degli strumenti di misurazione utilizzati. Ciò è particolarmente importante in analisi longitudinali, in cui si studiano i cambiamenti all’interno di ciascun individuo nel tempo. Una delle metodologie utilizzate per valutare l’affidabilità degli strumenti di misurazione è il calcolo dell’indice di affidabilità \\(\\omega\\) di McDonald per ogni momento temporale in cui si effettua la misurazione. Tuttavia, è importante sottolineare che questo indice non è equivalente alle misure di affidabilità degli indici di cambiamento longitudinale. Quest’ultima è una problematica complessa, soprattutto in disegni longitudinali intensivi, e costituisce un tema di discussione nella letteratura metodologica contemporanea.\nMentre l’affidabilità dei cambiamenti intra-individuali è difficile da stimare, l’affidabilità delle differenze inter-individuali può essere trovata facilmente. L’affidabilità del cambiamento inter-individuale è la proporzione di varianza nei punteggi osservati tra gli individui che può essere attribuita alla varianza nei punteggi veri piuttosto che alla varianza dell’errore:\n\\[\n\\frac{\\text{varianza di interesse}}{\\text{varianza di interesse} + \\text{varianza d'errore}},\n\\]\nNel caso delle misure di differenza individuale ottenute tramite Intensive Longitudinal Designs (ILD), per valutare l’affidabilità delle medie intra-individuali \\(\\bar{y}\\) (cioè la varianza “vera” delle medie tra i soggetti) si può utilizzare il coefficiente di correlazione intraclasse (ICC). In sostanza, l’ICC indica la proporzione di varianza totale che è attribuibile alle differenze tra gli individui rispetto alla varianza totale, che comprende sia le differenze tra gli individui che le differenze all’interno di ciascun individuo nel tempo. L’ICC viene calcolato come un rapporto tra varianze:\n\\[\n\\rho^2_{\\bar{y}} = \\frac{\\hat{\\tau}^2_{\\mu}}{\\hat{\\tau}^2_{\\mu} + \\hat{\\tau}^2_{\\varepsilon}}.\n\\]\nIl coefficiente di correlazione intraclasse (ICC) può essere stimato utilizzano un modello misto lineare con intercetta casuale che tiene conto della clusterizzazione dei dati, cioè del fatto che le osservazioni sono raggruppate in base ai soggetti. L’ICC viene calcolato come il rapporto tra la varianza tra le medie dei cluster di raggruppamento dei dati e la varianza totale, che comprende la varianza tra i cluster e la varianza all’interno dei cluster. In altre parole, l’ICC rappresenta la proporzione di varianza totale che è dovuta alle differenze tra i soggetti rispetto alla varianza totale delle misure ripetute. Questo indice è utile per valutare l’affidabilità delle misure ripetute e la loro utilità per lo studio delle differenze individuali.\nConsideriamo nuovamente i dati nlsy_math_long, che rappresentano il cambiamento nel rendimento in matematica dei bambini tra i gradi scolastici 2 e 8.\nSelezioniamo solo le variabili di interesse.\n\nnlsy_math_only_long &lt;- nlsy_math_long %&gt;%\n    dplyr::select(id, grade, math)\n\nnlsy_math_only_long |&gt;\n    head()|&gt; \n    print()\n\n    id grade math\n1  201     3   38\n2  201     5   55\n3  303     2   26\n4  303     5   33\n5 2702     2   56\n6 2702     4   58\n\n\nIl coefficiente ICC può essere trovato, ad esempio, mediante la funzione iccMixed specificando un raggruppamento dei dati nei termini dei soggetti.\n\niccMixed(\n  dv = \"math\",\n  id = c(\"id\"),\n  data = nlsy_math_long\n) |&gt;\n  print()\n\n        Var     Sigma       ICC\n     &lt;char&gt;     &lt;num&gt;     &lt;num&gt;\n1:       id  47.02036 0.2872289\n2: Residual 116.68307 0.7127711\n\n\nPer capire meglio il significato dell’ICC calcolato in precedenza per i dati nlsy_math_long, possiamo replicare lo stesso risultato utilizzando il modello misto lineare lmer, che tiene conto dell’effetto casuale del soggetto.\n\nm &lt;- lmer(math ~ 1 + (1 | id), data = nlsy_math_only_long)\n\nIl modello misto decompone la varianza totale in due componenti: la varianza che dipende dalla differenze tra le medie di ciascun soggetto (tra i soggetti, o varianza delle intercette individuali, \\(\\tau_{00}\\)) e la varianza che dipende dalle variazioni di ciascun soggetto attorno alla sua media.\nCalcoliamo la varianza totale dei punteggi di matematica.\n\nvar(nlsy_math_only_long$math) |&gt;\n    print()\n\n[1] 163.8379\n\n\nEsaminiamo ora la scomposizione della varianza eseguita dal modello misto. Si noti che la somma delle due componenti è uguale alla varianza totale.\n\nVarCorr(m) |&gt;\n    print()\n\n Groups   Name        Std.Dev.\n id       (Intercept)  6.8571 \n Residual             10.8020 \n\n\n\n6.8571^2 + 10.8020^2\n\n163.70302441\n\n\nIl coefficiente ICC è data dal rapporto tra la varianza attribuibile alla variazione tra le medie dei soggetti e la varianza totale.\n\n6.8571^2 / (6.8571^2 + 10.8020^2)\n\n0.287226339155697\n\n\nNel contesto dei modelli di crescita latente (LGM), l’ICC può essere usato per stimare l’affidabilità delle medie intra-individuali dei fattori latenti, ma non è adatto a valutare l’affidabilità delle variazioni intra-individuali nelle traiettorie di sviluppo. Per queste ultime, è necessario ricorrere a diverse misure di affidabilità.\nAd esempio, il coefficiente di affidabilità test-retest può essere utilizzato per stimare l’affidabilità intra-individuale delle traiettorie di sviluppo calcolate in due momenti distinti. Tuttavia, questo richiede l’assunzione che i punteggi veri restino invariati nel tempo. Un’alternativa è l’uso di forme parallele di test per misurare l’affidabilità delle traiettorie intra-individuali, anche se tali forme sono raramente disponibili.\nPer stimare l’affidabilità della componente sistematica della variazione intra-individuale nelle traiettorie di sviluppo, sono necessari metodi di stima specifici, ancora oggetto di dibattito nella letteratura metodologica.\nPrima di approfondire i modelli di crescita latente, è utile considerare l’analisi dei dati longitudinali tramite modelli misti. In R, queste analisi possono essere condotte con la funzione lmer del pacchetto lme4.\n\n73.4.1 Analisi con lmer\nLa funzione lmer() accetta i seguenti argomenti:\n\nformula: una formula lineare a due lati che descrive sia gli effetti fissi che gli effetti casuali del modello, con la risposta a sinistra dell’operatore ~ e i predittori e gli effetti casuali sulla destra dell’operatore ~.\ndata: Un data.frame, che deve essere nel cosiddetto formato “lungo”, con una singola riga per osservazione.\n\nIniziamo a descrivere la sintassi che consente la specificazione di un modello misto. Gli effetti fissi sono specificati come segue.\n\n\n\n\n\n\n\nFormula\nDescription\n\n\n\n\na + b\nmain effects of a and b (and no interaction)\n\n\na:b\nonly interaction of a and b (and no main effects)\n\n\na * b\nmain effects and interaction of a and b (expands to: a + b + a:b)\n\n\n(a+b+c)^2\nmain effects and two-way interactions, but no three-way interaction (expands to: a + b + c + a:b + b:c + a:c)\n\n\n(a+b)*c\nall main effects and pairwise interactions between c and a or b (expands to: a + b + c + a:c + b:c)\n\n\n0 + a\n0 suppresses the intercept resulting in a model that has one parameter per level of a (identical to: a - 1)\n\n\n\nGli effetti random vengono aggiunti alla formula tra parentesi (). All’interno di queste parentesi si fornisce sul lato sinistro di un segno condizionale | la specifica degli effetti casuali relativi alle pendenze individuali da includere nel modello. Sul lato destro di questo segno condizionale, si specifica il fattore di raggruppamento o i fattori di raggruppamento da cui dipendono questi effetti casuali. I fattori di raggruppamento devono essere di classe factor (cioè non possono essere variabili numeriche).\nGli effetti random vengono specificati come segue.\n\n\n\n\n\n\n\nFormula\nDescription\n\n\n\n\n(1\\|s)\nrandom intercepts for unique level of the factor s\n\n\n(1\\|s) + (1\\|i)\nrandom intercepts for each unique level of s and for each unique level of i\n\n\n(1\\|s/i)\nrandom intercepts for factor s and i, where the random effects for i are nested in s. This expands to (1\\|s) + (1\\|s:i) , i.e. a random intercept for each level of s, and each unique combination of the levels of s and i. Nested random effects are used in so-called multilevel models. For example, s might refer to schools, and i to classrooms within those schools.\n\n\n(a\\|s)\nrandom intercepts and random slopes for a, for each level of s. Correlations between the intercept and slope effects are also estimated – identical to (a*b\\|s))\n\n\n(a*b\\|s)\nrandom intercepts and slopes for a, b, and the a:b interaction, for each level of s. Correlations between all the random effects are estimated.\n\n\n(0+a\\|s)\nrandom slopes for a for each level of s, but no random intercepts\n\n\n(a\\|\\|s)\nrandom intercepts and random slopes for a, for each level of s, but no correlations between the random effects (i.e. they are set to 0). This expands to: (0+a\\|s) + (1\\|s)\n\n\n\n\n\n73.4.2 Formulazione del modello\nIn precedenza abbiamo descritto la variazione media tra gli individui mediante un modello misto ad intercetta casuale. È possibile estendere questo modello a casi più complessi, per esempio quello che assume una retta di regressione con pendenza ed intercetta diversa per ciascun soggetto. Per i dati nlsy_math_long possiamo specificare un tale modello in lmer usando la sintassi seguente.\n\nm &lt;- lmer(math ~ grade + (1 + grade | id), data = nlsy_math_long)\n\nIn un modello misto, i coefficienti delle rette di regressione di ciascun soggetto sono considerati come componenti casuali di una distribuzione di coefficienti relativi all’intercetta e alla pendenza complessive del gruppo. Questi coefficienti casuali possono essere modellati specificando una componente casuale (1 + grado | id). In questo tipo di modello, esiste una correlazione tra i parametri delle intercette e quelli delle pendenze individuali. Ciò significa che le componenti di varianza attribuibili ai vari effetti del modello (fissi e casuali) non sono più indipendenti e la varianza totale non può essere scomposta in componenti indipendenti.\nPer estrarre le componenti di varianza di un modello misto, è possibile utilizzare le funzioni fornite dal pacchetto insight. Ad esempio, nel caso dell’esempio presentato, i risultati possono essere ottenuti attraverso l’oggetto creato dalla funzione lmer.\n\ninsight::get_variance(m) |&gt;\n    print()\n\n$var.fixed\n[1] 58.94602\n\n$var.random\n[1] 70.66767\n\n$var.residual\n[1] 36.23643\n\n$var.distribution\n[1] 36.23643\n\n$var.dispersion\n[1] 0\n\n$var.intercept\n      id \n68.40554 \n\n$var.slope\n id.grade \n0.7391598 \n\n$cor.slope_intercept\n        id \n-0.2353175 \n\n\n\nUna descrizione visiva della varianza delle varie componenti del modello può essere ottenuta mediante la funzione modelDiagnostics del pacchetto JWileymisc.\n\nmd &lt;- JWileymisc::modelDiagnostics(m, ev.perc = .001)\nplot(md, ask = FALSE, ncol = 2, nrow = 3)\n\n\n\n\n\n\n\n\nUna descrizione dei parametri del modello può essere ottenuta nel modo seguente.\n\nmt &lt;- modelTest(m)\nnames(mt)\nAPAStyler(mt) |&gt;\n    print()\n\n\n'FixedEffects''RandomEffects''EffectSizes''OverallModel'\n\n\n                        Term                     Est           Type\n                      &lt;char&gt;                  &lt;char&gt;         &lt;char&gt;\n 1:              (Intercept) 26.59*** [25.66, 27.51]  Fixed Effects\n 2:                    grade  4.34*** [ 4.17,  4.51]  Fixed Effects\n 3: cor_grade.(Intercept)|id                   -0.24 Random Effects\n 4:        sd_(Intercept)|id                    8.27 Random Effects\n 5:              sd_grade|id                    0.86 Random Effects\n 6:                    sigma                    6.02 Random Effects\n 7:                 Model DF                       6  Overall Model\n 8:               N (Groups)                id (932)  Overall Model\n 9:         N (Observations)                    2221  Overall Model\n10:                   logLik                -7968.69  Overall Model\n11:                      AIC                15949.39  Overall Model\n12:                      BIC                15983.62  Overall Model\n13:              Marginal R2                    0.36  Overall Model\n14:              Marginal F2                    0.55  Overall Model\n15:           Conditional R2                    0.78  Overall Model\n16:           Conditional F2                    3.57  Overall Model\n17:   grade (Fixed + Random)     0.55/2.26, p &lt; .001   Effect Sizes\n18:           grade (Random)     0.01/0.09, p = .002   Effect Sizes\n\n\nLa varianza spiegata dal modello viene ottenuta nel modo seguente.\n\nmodelPerformance(m) |&gt;\n    print()\n\n$Performance\n    Model Estimator N_Obs N_Groups      AIC      BIC        LL  LLDF\n   &lt;char&gt;    &lt;char&gt; &lt;num&gt;   &lt;char&gt;    &lt;num&gt;    &lt;num&gt;     &lt;num&gt; &lt;num&gt;\n1: merMod      REML  2221 id (932) 15952.99 15987.22 -7970.494     6\n     Sigma MarginalR2 ConditionalR2 MarginalF2 ConditionalF2\n     &lt;num&gt;      &lt;num&gt;         &lt;num&gt;      &lt;num&gt;         &lt;num&gt;\n1: 6.01967  0.3553143      0.781476  0.5511433      3.576157\n\nattr(,\"class\")\n[1] \"modelPerformance.merMod\" \"modelPerformance\"       \n\n\n\n\n\n\nGrimm, K. J., Ram, N., & Estabrook, R. (2016). Growth modeling: Structural equation and multilevel modeling approaches. Guilford Publications.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>73</span>  <span class='chapter-title'>Considerazioni Preliminari</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/06_lgm_mixed.html",
    "href": "chapters/lgm/06_lgm_mixed.html",
    "title": "74  LGM e modelli misti",
    "section": "",
    "text": "74.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nI modelli di crescita latente (LGM, Latent Growth Models) rappresentano una forma specializzata di analisi fattoriale confermativa (CFA) che si focalizza sull’evoluzione dei costrutti nel tempo. Questi modelli si distinguono per la loro capacità di fissare le saturazioni fattoriali a valori predefiniti. Questo significa che si imposta in anticipo come ciascun fattore contribuisca alla varianza osservata nei dati raccolti in diversi momenti. In molti casi, la traiettoria di crescita nel tempo può essere descritta utilizzando una funzione lineare o quadratica, permettendo di modellare diversi tipi di evoluzioni, come un aumento costante o un cambiamento accelerato.\nUn aspetto centrale dei LGM è il concetto di fattori di crescita, che rappresentano le differenze individuali all’interno dei dati longitudinali. Questi fattori di crescita sono rappresentati da variabili latenti continue, denominate growth factors. In pratica, permettono di catturare e quantificare variazioni individuali nel modo in cui i soggetti cambiano nel tempo, ad esempio, in termini di sviluppo delle competenze o dell’andamento di un sintomo.\nPer facilitare una comprensione più approfondita dei modelli LGM, nel presente capitolo si propone un confronto con i modelli misti {cite:p}hoffman2022catching. Questo confronto mira a evidenziare le differenze e le somiglianze tra i due approcci, aiutando a discernere quando e perché scegliere un modello rispetto all’altro. Mentre i modelli misti possono essere utilizzati per analizzare dati gerarchici o nidificati, i modelli LGM si concentrano specificamente sull’analisi della traiettoria di crescita nel tempo, rendendoli particolarmente utili in studi longitudinali dove l’interesse primario è capire come un costrutto si sviluppa o cambia nel corso del tempo.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>74</span>  <span class='chapter-title'>LGM e modelli misti</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/06_lgm_mixed.html#modelli-misti",
    "href": "chapters/lgm/06_lgm_mixed.html#modelli-misti",
    "title": "74  LGM e modelli misti",
    "section": "74.2 Modelli misti",
    "text": "74.2 Modelli misti\nI modelli lineari generalizzati classici presuppongono l’indipendenza delle osservazioni, un’ipotesi che può non essere valida in contesti dove si effettuano misurazioni ripetute sullo stesso soggetto nel tempo, portando a osservazioni correlate. In questi scenari, si ricorre all’uso di modelli lineari a effetti misti (o modelli gerarchici lineari), che consentono di gestire adeguatamente la correlazione intragruppo.\nNei modelli a effetti misti, si realizza un equilibrio tra i due approcci estremi di modellazione: la modellazione aggregata (o pooled) e la modellazione separata (o per gruppo). La modellazione aggregata tratta l’insieme dei dati come un unico gruppo, ignorando le differenze intergruppo e potenzialmente perdendo informazioni rilevanti sulla variabilità tra i gruppi. La modellazione separata, d’altra parte, adatta un modello distinto per ciascun gruppo, potendo portare a sovrapparametrizzazione e ridotta capacità di generalizzazione.\nI modelli a effetti misti superano queste limitazioni integrando elementi sia della modellazione aggregata sia della modellazione separata. Ciò consente di considerare sia le differenze tra gruppi (variabilità intergruppo) sia l’informazione comune (variabilità intragruppo), migliorando la precisione delle stime dei parametri. Tradizionalmente, i dati rilevati all’interno di un soggetto sono classificati come dati di Livello 1, mentre i dati raccolti tra soggetti diversi sono definiti dati di Livello 2.\nAnalogamente ai modelli di regressione lineari tradizionali, i modelli a effetti misti includono un’intercetta fissa, coefficienti fissi per i predittori e un termine di errore per la deviazione tra i valori osservati e quelli predetti dal modello. Tuttavia, a differenza dei modelli lineari standard, in un modello a effetti misti, l’intercetta e i coefficienti dei predittori possono variare tra le unità di analisi, permettendo una maggiore flessibilità e adattabilità nel rappresentare la struttura dei dati. Questo approccio rende i modelli a effetti misti particolarmente adatti per l’analisi di dati longitudinali o gerarchici, dove è necessario tener conto della correlazione tra osservazioni all’interno dello stesso gruppo o soggetto.\nIl modello lineare multilivello, applicato all’analisi di dati strutturati in gruppi o cluster, permette una comprensione dettagliata della variazione sia all’interno dei gruppi (within-group variation) sia tra i gruppi (between-group variation). Formalmente, consideriamo un’unità statistica \\(i\\) all’interno di un gruppo \\(j\\) (dove \\(i = 1, ..., n_j\\) e \\(j = 1, ..., N\\)), per un totale di \\(N\\) gruppi ciascuno con numerosità \\(n_j\\).\nPer una variabile dipendente \\(Y\\), una variabile indipendente a livello individuale \\(x\\) e una variabile di gruppo \\(z\\), il modello si articola su due livelli. Il primo livello è rappresentato dalla seguente equazione lineare:\n\\[\nY_{ij} = \\beta_{0j} + \\beta_{1j}x_{ij} + \\varepsilon_{ij}.\n\\]\nQuesta equazione descrive la relazione tra \\(Y\\) e \\(x\\) per ogni unità \\(i\\) nel gruppo \\(j\\), dove \\(\\beta_{0j}\\) è l’intercetta e \\(\\beta_{1j}\\) la pendenza, specifiche per ciascun gruppo \\(j\\). Il termine d’errore \\(\\varepsilon_{ij}\\) è assunto normalmente distribuito con media zero e varianza costante \\(\\sigma^2\\).\nIl secondo livello del modello esplicita come l’intercetta \\(\\beta_{0j}\\) e la pendenza \\(\\beta_{1j}\\) varino tra i gruppi in relazione alla variabile di gruppo \\(z\\)\n\\[\n\\beta_{0j} = \\gamma_{00} + \\gamma_{01}z_j + U_{0j}\n\\]\n\\[\n\\beta_{1j} = \\gamma_{10} + \\gamma_{11}z_j + U_{1j}\n\\]\ndove:\n\n\\(\\gamma_{00}\\) rappresenta l’intercetta media a livello di gruppo. In altre parole, è il valore previsto di \\(Y\\) quando tutte le variabili indipendenti (\\(x\\) e \\(z\\)) sono pari a zero. È una sorta di “punto di partenza” medio per i vari gruppi nel modello.\n\\(\\gamma_{01}\\) è il coefficiente di pendenza che descrive come l’intercetta varia tra i gruppi in relazione alla variabile di gruppo \\(z\\). In pratica, \\(\\gamma_{01}\\) moltiplica la variabile di gruppo \\(z_j\\) (per ogni gruppo \\(j\\)) per quantificare l’influenza di questa variabile sulla variazione dell’intercetta tra i gruppi. Un valore positivo di \\(\\gamma_{01}\\) indica che un aumento in \\(z_j\\) è associato a un aumento dell’intercetta di \\(Y\\), mentre un valore negativo indica il contrario.\n\\(\\gamma_{10}\\) rappresenta il valore medio della pendenza della relazione tra \\(Y\\) e \\(x\\) attraverso tutti i gruppi, quando la variabile di gruppo \\(z\\) è zero. Indica come, in media, la variabile indipendente a livello individuale \\(x\\) si relaziona con \\(Y\\) nei diversi gruppi.\n\\(\\gamma_{11}\\) modella come la relazione (pendenza) tra \\(Y\\) e \\(x\\) varia tra i gruppi in funzione della variabile di gruppo \\(z\\). Analogamente a \\(\\gamma_{01}\\), questo coefficiente moltiplica \\(z_j\\) per mostrare l’effetto di \\(z\\) sulla pendenza di \\(Y\\) rispetto a \\(x\\) tra i gruppi. Se \\(\\gamma_{11}\\) è significativo, indica che l’effetto di \\(x\\) su \\(Y\\) non è costante tra i gruppi, ma varia in base al valore di \\(z\\).\n\nIn sintesi, questi coefficienti permettono di comprendere non solo come varia la relazione tra \\(Y\\) e \\(x\\) all’interno di ciascun gruppo (grazie a \\(\\gamma_{10}\\)), ma anche come questa relazione sia influenzata dalla variabile di gruppo \\(z\\) (mediante \\(\\gamma_{11}\\)). Allo stesso modo, essi illustrano come l’intercetta di \\(Y\\) varia tra i gruppi in base a \\(z\\) (\\(\\gamma_{01}\\)), oltre a fornire un valore di intercetta medio (\\(\\gamma_{00}\\)).\nQueste equazioni legano le variazioni di \\(\\beta_{0j}\\) e \\(\\beta_{1j}\\) tra i gruppi alla variabile \\(z\\). I termini \\(U_{0j}\\) e \\(U_{1j}\\) rappresentano l’errore a livello di gruppo, anch’essi assunti normalmente distribuiti con media zero e varianze costanti \\(\\tau_0^2\\) e \\(\\tau_1^2\\), rispettivamente, e indipendenti dall’errore a livello individuale \\(\\varepsilon_{ij}\\).\nIl modello multilivello permette così di analizzare come le caratteristiche di gruppo (come \\(z\\)) influenzano non solo l’intercetta (il livello di base di \\(Y\\)) ma anche la relazione tra \\(Y\\) e \\(x\\) (la pendenza). In altre parole, consente di esplorare come la relazione tra una variabile dipendente e indipendente possa cambiare da un gruppo all’altro.\nUn elemento chiave di questo approccio è il coefficiente di correlazione intragruppo \\(\\rho(Y \\mid x)\\), definito come:\n\\[\n\\rho(Y \\mid x) = \\frac{\\tau_0^2}{\\tau_0^2 + \\sigma^2}\n\\]\nIl coefficiente di correlazione intragruppo misura la proporzione della varianza totale di \\(Y\\) attribuibile alle differenze tra i gruppi. Un valore di \\(\\rho\\) vicino a 1 indica che la maggior parte della varianza di \\(Y\\) è spiegata dalle differenze tra i gruppi, mentre un valore vicino a 0 suggerisce che la varianza è prevalentemente dovuta a differenze all’interno dei singoli gruppi. Questo coefficiente fornisce quindi una misura quantitativa dell’importanza relativa delle variazioni tra e all’interno dei gruppi nel modello.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>74</span>  <span class='chapter-title'>LGM e modelli misti</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/06_lgm_mixed.html#simulare-effetti-casuali",
    "href": "chapters/lgm/06_lgm_mixed.html#simulare-effetti-casuali",
    "title": "74  LGM e modelli misti",
    "section": "74.3 Simulare effetti casuali",
    "text": "74.3 Simulare effetti casuali\nEsaminiamo con una simulazione una dimostrazione del funzionamento dei modelli misti. Ciò ci permetterà di meglio comprendere i modelli a crescita latente. Simuleremo dei dati bilanciati, con punteggi su quattro rilevazioni temporali per 500 individui (soggetti). Esamineremo il tasso di crescita (‘growth’) e consentiremo la presenza di intercette e pendenze specifiche per i diversi soggetti.\nLe istruzioni seguenti generano i dati (per i nostri scopi, non è importante capire i dettagli di questa porzione di codice).\n\nset.seed(12345)\nn &lt;- 500\ntimepoints &lt;- 4\ntime &lt;- rep(0:3, times = n)\nsubject &lt;- rep(1:n, each = 4)\n\nintercept &lt;- .5\nslope &lt;- .25\nrandomEffectsCorr &lt;- matrix(c(1, .2, .2, 1), ncol = 2)\n\nrandomEffects &lt;- MASS::mvrnorm(\n  n,\n  mu = c(0, 0), Sigma = randomEffectsCorr, empirical = T\n) %&gt;%\n  data.frame()\n\ncolnames(randomEffects) &lt;- c(\"Int\", \"Slope\")\n\nNella simulazione, abbiamo impostato gli effetti fissi, che comprendono l’intercetta e la pendenza della regressione lineare standard, ai valori di 0.5 e 0.25 rispettivamente. Inoltre, è stata simulata una correlazione di 0.2 tra l’intercetta e la pendenza che sono specifiche per ogni singolo soggetto. A causa di questa correlazione, i dati sono stati generati utilizzando una distribuzione normale multivariata. In questo contesto, abbiamo assegnato una varianza di 1 sia per l’intercetta sia per la pendenza.\nProcediamo ora con l’analisi dei dati risultanti dalla simulazione. Questo passaggio è fondamentale per comprendere le implicazioni dei parametri scelti nella simulazione e per verificare se i dati generati rispecchiano le aspettative teoriche stabilite inizialmente.\nI dati prodotti fino ad ora sono i seguenti:\n\ndata.frame(\n    Subject = subject, \n    time = time, \n    randomEffects[subject, ]\n) |&gt;\n    head(10)\n\n\nA data.frame: 10 x 4\n\n\n\nSubject\ntime\nInt\nSlope\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1\n0\n-1.3322902\n-0.9548087\n\n\n1.1\n1\n1\n-1.3322902\n-0.9548087\n\n\n1.2\n1\n2\n-1.3322902\n-0.9548087\n\n\n1.3\n1\n3\n-1.3322902\n-0.9548087\n\n\n2\n2\n0\n-2.1261548\n-1.7813625\n\n\n2.1\n2\n1\n-2.1261548\n-1.7813625\n\n\n2.2\n2\n2\n-2.1261548\n-1.7813625\n\n\n2.3\n2\n3\n-2.1261548\n-1.7813625\n\n\n3\n3\n0\n0.4606242\n0.3039838\n\n\n3.1\n3\n1\n0.4606242\n0.3039838\n\n\n\n\n\nPer generare la variabile target, procediamo sommando gli effetti casuali, precedentemente calcolati, all’intercetta globale e applichiamo un analogo procedimento alle pendenze. In aggiunta, introduciamo un rumore gaussiano ai dati, caratterizzato da una deviazione standard \\(\\sigma\\) pari a 0.5. Questa operazione ha lo scopo di aggiungere un livello di variabilità realistica e di incertezza ai dati, rendendo la simulazione più vicina a scenari osservati nella realtà pratica.\n\nset.seed(12345)\nsigma &lt;- .5\ny1 &lt;- \n  (intercept + randomEffects$Int[subject]) + # random intercepts\n  (slope + randomEffects$Slope[subject]) * time + # random slopes\n  rnorm(n * timepoints, mean = 0, sd = sigma) # noise\n\nd &lt;- data.frame(subject, time, y1)\n\n\nd |&gt;\n  head(10) \n\n\nA data.frame: 10 x 3\n\n\n\nsubject\ntime\ny1\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1\n0\n-0.5395258\n\n\n2\n1\n1\n-1.1823659\n\n\n3\n1\n2\n-2.2965593\n\n\n4\n1\n3\n-3.1734649\n\n\n5\n2\n0\n-1.3232110\n\n\n6\n2\n1\n-4.0664952\n\n\n7\n2\n2\n-4.3738304\n\n\n8\n2\n3\n-6.3583342\n\n\n9\n3\n0\n0.8185443\n\n\n10\n3\n1\n1.0549470\n\n\n\n\n\nIl grafico seguente mostra le rette di regressione per ciascuno dei 500 soggetti.\n\nggplot(d, aes(x = time, y = y1)) +\n  geom_path(aes(group = subject), alpha = .1) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nAdattiamo ai dati un modello misto utilizzando la funzione lmer del pacchetto lme4. Si noti che questo è un modello in cui solo le intercette sono consentite variare tra i cluster.\n\nm0 &lt;- lmer(y1 ~ 1 + (1 | subject), data = d)\n\nLe componenti di varianza del modello si estraggono con la funzione VarCorr().\n\nVarCorr(m0)\n\n Groups   Name        Std.Dev.\n subject  (Intercept) 1.8306  \n Residual             1.4352  \n\n\nCalcoliamo il coefficiente di correlazione intraclasse.\n\n1.8306^2 / (1.8306^2 + 1.4352^2)\n\n0.619323810990691\n\n\nLaddove\n\n(1.8306^2 + 1.4352^2)\n\n5.4108954\n\n\nè uguale alla varianza della variabile risposta\n\nvar(d$y1)\n\n5.40584388269803\n\n\nLo stesso risultato si ottiene utilizzando una funzione R per il calcolo della correlazione intraclasse.\n\nmultilevelTools::iccMixed(\n  dv = \"y1\",\n  id = c(\"subject\"),\n  data = d\n) |&gt;\n  print()\n\n        Var    Sigma       ICC\n     &lt;char&gt;    &lt;num&gt;     &lt;num&gt;\n1:  subject 3.350987 0.6193062\n2: Residual 2.059886 0.3806938\n\n\n\n0.6193062 + 0.3806938\n\n1\n\n\nEsaminiamo ora un modello in cui sia le intercette sia le pendenze variano tra i cluster.\n\nmix_mod &lt;- lmer(y1 ~ time + (1 + time | subject), data = d)\nsummary(mix_mod) |&gt;\n    print()\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y1 ~ time + (1 + time | subject)\n   Data: d\n\nREML criterion at convergence: 5881.3\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.03499 -0.46249  0.00414  0.48241  2.74992 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n subject  (Intercept) 1.0245   1.0122       \n          time        1.0301   1.0149   0.15\n Residual             0.2412   0.4911       \nNumber of obs: 2000, groups:  subject, 500\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  0.50159    0.04885  10.267\ntime         0.25157    0.04644   5.417\n\nCorrelation of Fixed Effects:\n     (Intr)\ntime 0.072 \n\n\nGli effetti fissi che abbiamo ottenuto (\\(\\alpha\\) = 0.50159, \\(\\beta\\) = 0.25157) sono simili ai valori che abbiamo impostato nella simulazione per l’intercetta e la pendenza globale.\n\nVarCorr(mix_mod)\n\n Groups   Name        Std.Dev. Corr \n subject  (Intercept) 1.01217       \n          time        1.01494  0.150\n Residual             0.49108       \n\n\nLe varianze degli effetti casuali stimati (\\(1.0122^2\\), \\(1.0149^2\\)) sono molto simili al valore impostato di 1 nella simulazione, la correlazione (0.15) è simile al valore impostato di 0.2 e la deviazione standard dei residui (0.4911) è simile al valore impostato di 0.5.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>74</span>  <span class='chapter-title'>LGM e modelli misti</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/06_lgm_mixed.html#modello-di-crescita-latente",
    "href": "chapters/lgm/06_lgm_mixed.html#modello-di-crescita-latente",
    "title": "74  LGM e modelli misti",
    "section": "74.4 Modello di crescita latente",
    "text": "74.4 Modello di crescita latente\n\nEsploriamo ora l’analisi degli stessi dati tramite un modello di crescita latente (LGM). I modelli LGM possono essere considerati come un’estensione dei modelli CFA, in cui si ipotizzano due fattori latenti principali: il primo è una variabile latente associata alle intercette casuali, cioè rappresenta la variazione delle intercette individuali dei partecipanti; il secondo è una variabile latente relativa alle pendenze casuali, che descrive la variazione delle pendenze individuali dei partecipanti. Queste intercette e pendenze si riferiscono alla linea di regressione che descrive per ciascun partecipante la relazione tra la variabile in esame e il tempo.\nIn quanto il modello mira a spiegare la relazione tra le medie dei punteggi dei partecipanti nel tempo, è necessario analizzare i dati grezzi piuttosto che la matrice di covarianza campionaria. Ciò significa utilizzare le osservazioni individuali per ciascun partecipante come input.\nPer l’analisi, utilizzeremo nuovamente il software lavaan, ma con una sintassi differente per poter fissare le saturazioni fattoriali a valori specifici, come richiesto dai vincoli del modello LGM. Di conseguenza, l’output che otterremo sarà diverso da quello dei modelli SEM standard, poiché i parametri relativi alle saturazioni fattoriali sono fissi e non stimati.\nPer il fattore che rappresenta le intercette, le saturazioni fattoriali sono impostate a 1. Questo valore può essere interpretato come l’equivalente della colonna dell’intercetta nella matrice \\(\\boldsymbol{X}\\) di un modello di regressione multipla.\nLe saturazioni per il fattore che definisce le pendenze casuali sono stabilite in base alla sequenza temporale delle misurazioni \\(y\\), con valori \\(\\lambda\\) che vanno da 0 a 3. Questi riflettono gli intervalli temporali delle misurazioni. Iniziare la codifica da 0 conferisce un significato interpretabile allo zero, analogamente ai valori che, nella matrice \\(\\boldsymbol{X}\\) di un modello di regressione multipla, corrisponderebbero alla colonna della pendenza.\nIl modello di crescita latente (LGM) espresso dalla formula\n\\[\ny_j = \\alpha_0 + \\alpha_1 \\lambda_j + \\zeta_{00} + \\zeta_{11} \\lambda_j + \\epsilon_j\n\\]\npuò essere messo in relazione con il modello lineare ad effetti misti precedentemente descritto attraverso la comprensione della struttura dei due modelli e delle loro componenti.\n\nInterpretazione dei Componenti del Modello LGM:\n\n$ _0 $: Rappresenta l’intercetta media del modello di crescita.\n$ _1 _j $: Rappresenta il tasso medio di crescita nel tempo, dove $ _j $ sono i valori che descrivono l’intervallo temporale delle misurazioni.\n$ _{00} $: Indica la variazione delle intercette individuali tra i soggetti rispetto all’intercetta media $ _0 $.\n$ _{11} _j $: Esprime la variazione nelle pendenze individuali (tassi di crescita) tra i soggetti rispetto al tasso medio di crescita $ _1 $.\n$ _j $: Rappresenta l’errore di misurazione per ogni singolo soggetto.\n\nConfronto con il Modello Lineare ad Effetti Misti:\n\nNel modello lineare ad effetti misti, si considerano sia effetti fissi (come l’intercetta e la pendenza media del modello) sia effetti casuali (variazione delle intercette e delle pendenze tra i soggetti). In maniera simile, il modello LGM considera l’intercetta media e il tasso medio di crescita (effetti fissi) e permette la variazione individuale in queste componenti (effetti casuali).\nLa componente $ {00} $ nel modello LGM è analoga alla variazione casuale delle intercette nel modello ad effetti misti, mentre $ {11} $ corrisponde alla variazione casuale delle pendenze.\nEntrambi i modelli permettono di analizzare dati strutturati longitudinalmente, offrendo la flessibilità di modellare non solo la tendenza generale (effetti fissi) ma anche la variazione individuale intorno a questa tendenza (effetti casuali).\n\n\nIn sintesi, il modello LGM può essere visto come un caso speciale o un’estensione del modello lineare ad effetti misti, con un’enfasi particolare sulla modellazione del cambiamento nel tempo e sulla relazione di questa dinamica con variabili latenti. Entrambi i modelli sono strumenti potenti nell’analisi di dati longitudinali, permettendo di esaminare sia la tendenza centrale sia la variabilità individuale all’interno dei dati.\nUn requisito degli LGM è che i dati devono essere forniti del formato wide (mentre per il precedente modello misto abbiamo usato il formato long), il che significa che ogni colonna rappresenta la variabile di esito in un diverso momento nel tempo. Si presume che ogni osservazione o riga sia indipendente dalle altre; le colonne mostrano invece una dipendenza temporale. Trasformiamo dunque i dati nel formato richiesto.\n\ndwide &lt;- d %&gt;%\n  spread(time, y1) %&gt;%\n  rename_at(vars(-subject), function(x) paste0(\"y\", x))\nhead(dwide)\n\n\nA data.frame: 6 x 5\n\n\n\nsubject\ny0\ny1\ny2\ny3\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1\n-0.5395258\n-1.1823659\n-2.2965593\n-3.173465\n\n\n2\n2\n-1.3232110\n-4.0664952\n-4.3738304\n-6.358334\n\n\n3\n3\n0.8185443\n1.0549470\n2.0104678\n3.531232\n\n\n4\n4\n0.4469440\n-0.3162615\n-1.7896354\n-1.843919\n\n\n5\n5\n1.8959902\n5.5259110\n9.6045869\n12.546123\n\n\n6\n6\n2.1829579\n1.6287374\n-0.3136214\n-1.660328\n\n\n\n\n\nIl modello misto che abbiamo descritto in precedenza corrisponde dunque ad un modello fattoriale con due variabili latenti: un fattore (\\(\\eta_0\\)) che rappresenta il “punteggio vero” delle intercette individuali e un fattore (\\(\\eta_1\\)) che rappresenta il “punteggio vero” delle pendenze delle rette di regressione per i singoli individui.\nNella sintassi di lavaan il modello diventa:\n\nmodel &lt;- \"\n    i =~ 1*y0 + 1*y1 + 1*y2 + 1*y3\n    s =~ 0*y0 + 1*y1 + 2*y2 + 3*y3\n\"\n\nPossiamo adattare il modello ai dati usando una funzione specifica di lavaan, ovvero growth, che può essere usata per questa classe di modelli.\n\ngrowth_curve_model &lt;- growth(model, data = dwide)\n\n\nsummary(growth_curve_model) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 41 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         9\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                                 4.212\n  Degrees of freedom                                 5\n  P-value (Chi-square)                           0.519\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  i =~                                                \n    y0                1.000                           \n    y1                1.000                           \n    y2                1.000                           \n    y3                1.000                           \n  s =~                                                \n    y0                0.000                           \n    y1                1.000                           \n    y2                2.000                           \n    y3                3.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  i ~~                                                \n    s                 0.162    0.051    3.137    0.002\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    i                 0.501    0.049   10.263    0.000\n    s                 0.252    0.046    5.428    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .y0                0.268    0.042    6.356    0.000\n   .y1                0.237    0.022   10.713    0.000\n   .y2                0.209    0.029    7.262    0.000\n   .y3                0.299    0.066    4.556    0.000\n    i                 1.007    0.078   12.996    0.000\n    s                 1.021    0.068   14.953    0.000\n\n\n\n\ngrowth_curve_model |&gt;\n    semPaths(\n        style = \"ram\",\n        whatLabels = \"par\", edge.label.cex = .6,\n        label.prop = 0.9, edge.label.color = \"black\", rotation = 4,\n        equalizeManifests = FALSE, optimizeLatRes = TRUE,\n        node.width = 1.5,\n        edge.width = 0.5, shapeMan = \"rectangle\", shapeLat = \"ellipse\",\n        shapeInt = \"triangle\", sizeMan = 4, sizeInt = 2, sizeLat = 4,\n        curve = 2, unCol = \"#070b8c\"\n    )\n\n\n\n\n\n\n\n\nUsiamo l’oggetto creato da growth per creare un diagramma di percorso.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>74</span>  <span class='chapter-title'>LGM e modelli misti</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/06_lgm_mixed.html#comparazione-tra-modelli-a-effetti-misti-e-modelli-di-crescita-latente",
    "href": "chapters/lgm/06_lgm_mixed.html#comparazione-tra-modelli-a-effetti-misti-e-modelli-di-crescita-latente",
    "title": "74  LGM e modelli misti",
    "section": "74.5 Comparazione tra Modelli a Effetti Misti e Modelli di Crescita Latente",
    "text": "74.5 Comparazione tra Modelli a Effetti Misti e Modelli di Crescita Latente\nNell’output del metodo growth(), la sezione denominata Intercepts rappresenta in realtà gli effetti fissi all’interno del contesto di un modello a effetti misti:\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    i                 0.510    0.048   10.542    0.000\n    s                 0.234    0.046    5.133    0.000\nPuò apparire inizialmente non intuitivo riferirsi agli effetti fissi come ‘intercette’ in un modello a effetti misti. Tuttavia, questa terminologia diventa più chiara quando consideriamo la parametrizzazione del modello di crescita latente (LGM). In un modello LGM, ‘i’ rappresenta l’intercetta generale del modello (cioè, il punto di partenza medio per tutti i soggetti), mentre ‘s’ indica la pendenza media, ovvero il tasso di crescita o di cambiamento nel tempo.\nÈ interessante notare come le stime riportate qui siano molto vicine a quelle che si ottengono in un modello a effetti misti. Questa similitudine dimostra l’affinità tra i due approcci di modellazione: entrambi mirano a comprendere e quantificare sia gli effetti generali (come la tendenza media di crescita) sia le variazioni individuali all’interno di un insieme di dati longitudinali. In entrambi i casi, l’intercetta e la pendenza giocano ruoli cruciali nell’interpretazione dei modelli e nella comprensione di come i valori della variabile dipendente evolvano nel tempo.\n\nprint(fixef(mix_mod))\n\n(Intercept)        time \n  0.5015932   0.2515722 \n\n\nSi noti inoltre che le stime degli effetti fissi del modello misto sono identiche a quelle che vengono trovate usando un modello di regressione standard:\n\nlm(y1 ~ time, data = d)\n\n\nCall:\nlm(formula = y1 ~ time, data = d)\n\nCoefficients:\n(Intercept)         time  \n     0.5016       0.2516  \n\n\nConsideriamo ora le stime della varianza nel modello a crescita latente.\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  i ~~                                                \n    s                 0.220    0.050    4.371    0.000\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .y0                0.310    0.042    7.308    0.000\n   .y1                0.220    0.021   10.338    0.000\n   .y2                0.230    0.029    7.935    0.000\n   .y3                0.275    0.064    4.295    0.000\n    i                 0.973    0.076   12.854    0.000\n    s                 0.986    0.066   14.889    0.000\nConfrontiamo questi valori con quelli ottenuti dal modello misto.\n\nVarCorr(mix_mod) |&gt;\n    print()\n\n Groups   Name        Std.Dev. Corr \n subject  (Intercept) 1.01217       \n          time        1.01494  0.150\n Residual             0.49108       \n\n\nSi noti che il modello a crescita latente, per impostazione predefinita, assume una varianza eterogenea per ogni rilevazione temporale. I modelli misti, invece, per impostazione predefinita assumono la stessa varianza per ogni punto temporale. È però possibile specificare una stima separata della varianza nelle diverse rilevazioni temporali.\nSe vincoliamo le varianze ad essere uguali per ciascuna rilevazione temporale nel modello LGM, i due modelli producono delle stime identiche. La sintassi seguente viene utilizzata per forzare l’uguaglianza delle varianze in ciascuna rilevazione temporale.\n\nmodel &lt;- \"\n    # intercept and slope with fixed coefficients\n    i =~ 1*y0 + 1*y1 + 1*y2 + 1*y3\n    s =~ 0*y0 + 1*y1 + 2*y2 + 3*y3\n    y0 ~~ resvar*y0\n    y1 ~~ resvar*y1\n    y2 ~~ resvar*y2\n    y3 ~~ resvar*y3\n\"\n\nAdattiamo il nuovo modello ai dati.\n\ngrowth_curve_model &lt;- growth(model, data = dwide)\n\nEsaminiamo i risultati.\n\nsummary(growth_curve_model) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 27 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         9\n  Number of equality constraints                     3\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                                 6.180\n  Degrees of freedom                                 8\n  P-value (Chi-square)                           0.627\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  i =~                                                \n    y0                1.000                           \n    y1                1.000                           \n    y2                1.000                           \n    y3                1.000                           \n  s =~                                                \n    y0                0.000                           \n    y1                1.000                           \n    y2                2.000                           \n    y3                3.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  i ~~                                                \n    s                 0.154    0.051    3.034    0.002\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    i                 0.502    0.049   10.278    0.000\n    s                 0.252    0.046    5.423    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .y0      (rsvr)    0.241    0.011   22.361    0.000\n   .y1      (rsvr)    0.241    0.011   22.361    0.000\n   .y2      (rsvr)    0.241    0.011   22.361    0.000\n   .y3      (rsvr)    0.241    0.011   22.361    0.000\n    i                 1.022    0.076   13.502    0.000\n    s                 1.028    0.068   15.095    0.000\n\n\n\nPer lme4 abbiamo:\n\nprint(VarCorr(mix_mod), comp = \"Var\")\n\n Groups   Name        Variance Cov  \n subject  (Intercept) 1.02448       \n          time        1.03011  0.154\n Residual             0.24116       \n\n\nIn entrambi i casi, la varianza residua è uguale a 0.241 e la correlazione tra intercette e pendenze casuali è uguale a 0.154.\nInoltre, le stime dei coefficienti casuali del modello misto sono identiche a quelle delle variabili latenti.\n\ncoef(mix_mod)[[1]] |&gt; \n    head()\n\n\nA data.frame: 6 x 2\n\n\n\n(Intercept)\ntime\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n-0.3967486\n-0.9050565\n\n\n2\n-1.5328622\n-1.5945199\n\n\n3\n0.5429873\n0.8759706\n\n\n4\n0.3095727\n-0.7887484\n\n\n5\n2.0327226\n3.5319068\n\n\n6\n2.0645454\n-1.1411935\n\n\n\n\n\n\nlavPredict(growth_curve_model) |&gt;\n    head()\n\n\nA matrix: 6 x 2 of type dbl\n\n\ni\ns\n\n\n\n\n-0.3966515\n-0.9050631\n\n\n-1.5324914\n-1.5946260\n\n\n0.5430942\n0.8759036\n\n\n0.3094388\n-0.7886563\n\n\n2.0328124\n3.5317637\n\n\n2.0637121\n-1.1407804",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>74</span>  <span class='chapter-title'>LGM e modelli misti</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/06_lgm_mixed.html#riflessioni-conclusive",
    "href": "chapters/lgm/06_lgm_mixed.html#riflessioni-conclusive",
    "title": "74  LGM e modelli misti",
    "section": "74.6 Riflessioni Conclusive",
    "text": "74.6 Riflessioni Conclusive\nIn conclusione, abbiamo visto che, nel caso più semplice in cui viene assunta la stessa varianza per ogni punto temporale, i modelli LGM producono risultati identici ai modelli misti. Tuttavia, la concettualizzazione del cambiamento nei termini di un modello a crescita latente offre molti vantaggi rispetto alla descrizione dei dati nei termini dei modelli misti in quanto i modelli LGM sono più flessibili e consentono la verifica di ipotesi statistiche che non possono essere esaminate nel contesto dei modelli misti.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>74</span>  <span class='chapter-title'>LGM e modelli misti</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/03_time_effects.html",
    "href": "chapters/lgm/03_time_effects.html",
    "title": "75  Dati longitudinali",
    "section": "",
    "text": "75.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nL’obiettivo di questo capitolo è esaminare come è possibile estendere i modelli SEM per adattarli alle particolarità dei dati longitudinali. Per semplificare, cominciamo concentrandoci su due misurazioni temporali consecutive.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>75</span>  <span class='chapter-title'>Dati longitudinali</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/03_time_effects.html#misurare-il-cambiamento",
    "href": "chapters/lgm/03_time_effects.html#misurare-il-cambiamento",
    "title": "75  Dati longitudinali",
    "section": "75.2 Misurare il Cambiamento",
    "text": "75.2 Misurare il Cambiamento\nIl modo più semplice per valutare il cambiamento individuale tra due momenti temporali consiste nel calcolare la differenza tra i punteggi ottenuti nelle due occasioni. Tuttavia, questa strategia ha un limite significativo: non disponiamo del punteggio “vero” dell’individuo in ciascun momento, ma solo di una misura influenzata dall’errore di misurazione. L’errore di misurazione può ridurre notevolmente la precisione delle stime sulle differenze individuali, compromettendo l’interpretazione del cambiamento.\nPer superare questo problema negli studi longitudinali, vengono impiegati i modelli di crescita latente (Latent Growth Models, LGM), appartenenti alla famiglia dei modelli a equazioni strutturali (SEM). Questi modelli permettono di stimare traiettorie di cambiamento per ciascun individuo, separando le componenti latenti dal rumore delle misurazioni.\n\n75.2.1 Componenti del Modello di Crescita Latente\nNei LGM, si assume che ogni individuo segua una propria traiettoria di cambiamento nel tempo. I dati osservati possono essere scomposti in tre componenti principali:\n\nPunteggi latenti: rappresentano il livello individuale del costrutto in un dato momento.\nPunteggi di cambiamento latenti: indicano il cambiamento individuale nel tempo.\nCaratteristiche uniche non osservate: includono gli errori di misurazione specifici per ogni momento.\n\nL’equazione generale del modello SEM è espressa come:\n\\[\n\\Sigma = \\Lambda \\Psi \\Lambda' + \\Theta,\n\\]\ndove:\n\n$ $ rappresenta la matrice delle varianze e covarianze teoriche.\n$ $ è la matrice dei carichi fattoriali, che descrive le relazioni tra indicatori e costrutti latenti.\n$ $ indica le varianze e covarianze tra i fattori latenti.\n$ $ rappresenta le varianze residue e covarianze tra gli errori di misura.\n\n\n\n75.2.2 Struttura del Modello di Misurazione Longitudinale\nIn un modello longitudinale, si definiscono tre fattori latenti principali:\n\nUn fattore che rappresenta il livello di base del costrutto in un dato momento.\nUn fattore che rappresenta il cambiamento nel costrutto tra momenti temporali.\nUn fattore che rappresenta l’errore di misurazione specifico per ciascun momento.\n\nPer illustrare come funziona questo modello, consideriamo un costrutto misurato in due occasioni. Il punteggio osservato di un individuo in un tempo specifico può essere descritto dalla formula:\n\\[\nx_{it} = \\tau_i + (1)\\xi_1 + (t)\\xi_2 + \\delta_{it},\n\\]\ndove:\n\n\\(\\tau_i\\) è il livello iniziale dell’individuo \\(i\\),\n\\(\\xi_1\\) rappresenta il livello latente al tempo \\(t_1\\),\n\\(\\xi_2\\) rappresenta il cambiamento latente tra i due momenti,\n\\(\\delta_{it}\\) è l’errore di misurazione specifico per l’individuo \\(i\\) al tempo \\(t\\).\n\n\n\n75.2.3 Modello per Più Occasioni di Misurazione\nQuando vengono utilizzati più indicatori in ciascun momento, la struttura del modello può essere rappresentata come segue:\n\\[\n\\begin{align}\nx_{1} &= 0 + (1)\\xi_{1} + (0)\\xi_{2} + \\delta_{1} \\notag\\\\\nx_{2} &= 0 + (1)\\xi_{1} + (1)\\xi_{2} + \\delta_{2} \\notag\\\\\nx_{3} &= 0 + (1)\\xi_{1} + (2)\\xi_{2} + \\delta_{3} \\notag\\\\\nx_{4} &= 0 + (1)\\xi_{1} + (4)\\xi_{2} + \\delta_{4} \\notag\\\\\nx_{5} &= 0 + (1)\\xi_{1} + (5)\\xi_{2} + \\delta_{5} \\notag\n\\end{align}\n\\]\nInoltre, nel modello si ipotizza una correlazione tra \\(\\xi_1\\) e \\(\\xi_2\\), rappresentata dalla matrice di intercorrelazione dei fattori:\n\\[\n\\mathbf{\\Phi} =\n\\begin{bmatrix}\n\\phi_{11} & \\\\\n\\phi_{21} & \\phi_{22}\n\\end{bmatrix}\n\\]\ndove:\n\n\\(\\phi_{11}\\) è la varianza dell’intercetta latente (livello di base),\n\\(\\phi_{22}\\) è la varianza della pendenza latente (cambiamento nel tempo),\n\\(\\phi_{21}\\) è la covarianza tra intercetta e pendenza, utile per comprendere come il livello iniziale sia associato alla velocità di cambiamento.\n\n\n\n75.2.4 Correlazioni tra Varianze Residue\nUn aspetto distintivo dei modelli longitudinali è la possibilità di correlare le varianze residue degli stessi indicatori misurati in momenti diversi (ad esempio, la correlazione tra X1 al Tempo 1 e X1 al Tempo 2). Questo consente di distinguere tra le informazioni stabili del costrutto nel tempo e le variazioni specifiche di ciascun indicatore a ogni misurazione.\n\n\n75.2.5 Interpretazione e Utilità\nQuesto approccio permette di esaminare in modo approfondito lo sviluppo o il cambiamento di un costrutto latente e dei suoi indicatori nel tempo, fornendo un quadro dettagliato delle dinamiche individuali e collettive di cambiamento.\nIl modello di crescita latente definito da queste equazioni produce previsioni sulla struttura delle medie e delle covarianze dei dati osservati. Queste previsioni sono utilizzate nel contesto della modellizzazione delle equazioni strutturali per stimare i parametri e valutare l’adattamento del modello ai dati. La struttura delle covarianze prevista dal modello è:\n\\[\n\\boldsymbol{\\Sigma} = \\boldsymbol{\\Lambda} \\boldsymbol{\\Phi} \\boldsymbol{\\Lambda}' + \\boldsymbol{\\Theta}.\n\\]\nLa figura Figura 75.1 rappresenta graficamente il percorso del modello di crescita latente (LGM) che stiamo analizzando.\n\n\n\n\n\n\nFigura 75.1: Modello di crescita latente.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>75</span>  <span class='chapter-title'>Dati longitudinali</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/03_time_effects.html#la-variazione-temporale-di-positive-affect",
    "href": "chapters/lgm/03_time_effects.html#la-variazione-temporale-di-positive-affect",
    "title": "75  Dati longitudinali",
    "section": "75.3 La Variazione Temporale di Positive Affect",
    "text": "75.3 La Variazione Temporale di Positive Affect\nApplichiamo questo modello al caso in cui tre indicatori di Positive Affect (Glad, Cheerful, Happy) vengono misurati in due momenti del tempo (si veda Little (2023)).\nImportiamo i dati.\n\ndat &lt;- read.table(\n    file = \"../../data/grade7and8.long.823.dat\",\n    col.names = c(\n        \"PAT1P1\", \"PAT1P2\", \"PAT1P3\", \"NAT1P1\", \"NAT1P2\", \"NAT1P3\",\n        \"PAT2P1\", \"PAT2P2\", \"PAT2P3\", \"NAT2P1\", \"NAT2P2\", \"NAT2P3\",\n        \"PAT3P1\", \"PAT3P2\", \"PAT3P3\", \"NAT3P1\", \"NAT3P2\", \"NAT3P3\",\n        \"grade\", \"female\", \"black\", \"hispanic\", \"other\"\n    )\n)\nglimpse(dat)\n\nRows: 823\nColumns: 23\n$ PAT1P1   &lt;dbl&gt; 1.50000, 2.98116, 3.50000, 3.00000, 3.00000, 3.00000, 3.0~\n$ PAT1P2   &lt;dbl&gt; 1.50000, 2.98284, 4.00000, 3.50000, 2.50000, 2.50000, 2.5~\n$ PAT1P3   &lt;dbl&gt; 2.00000, 2.98883, 4.00000, 2.50000, 3.00000, 3.00000, 4.0~\n$ NAT1P1   &lt;dbl&gt; 2.50000, 1.56218, 1.50000, 1.50000, 1.00000, 1.50000, 1.0~\n$ NAT1P2   &lt;dbl&gt; 3.50000, 1.45688, 1.00000, 2.00000, 1.00000, 2.50000, 1.0~\n$ NAT1P3   &lt;dbl&gt; 3.00000, 1.65477, 1.00000, 1.50000, 1.00000, 2.50000, 1.0~\n$ PAT2P1   &lt;dbl&gt; 4.00000, 4.00000, 4.00000, 2.95942, 3.17170, 2.00000, 3.0~\n$ PAT2P2   &lt;dbl&gt; 4.00000, 4.00000, 2.50000, 2.99083, 2.87806, 2.00000, 3.0~\n$ PAT2P3   &lt;dbl&gt; 4.00000, 4.00000, 4.00000, 3.06670, 3.11031, 3.00000, 4.0~\n$ NAT2P1   &lt;dbl&gt; 2.00000, 1.00000, 1.00000, 1.65159, 1.65777, 2.00000, 1.0~\n$ NAT2P2   &lt;dbl&gt; 1.00000, 1.00000, 1.00000, 1.42599, 1.44804, 2.00000, 1.0~\n$ NAT2P3   &lt;dbl&gt; 2.00000, 1.00000, 1.00000, 1.67184, 1.56296, 2.00000, 1.0~\n$ PAT3P1   &lt;dbl&gt; 4.00000, 4.00000, 4.00000, 4.00000, 2.67109, 3.00000, 2.5~\n$ PAT3P2   &lt;dbl&gt; 4.00000, 4.00000, 4.00000, 3.50000, 2.85851, 2.00000, 2.0~\n$ PAT3P3   &lt;dbl&gt; 4.00000, 4.00000, 3.48114, 3.50000, 3.28099, 2.50000, 3.5~\n$ NAT3P1   &lt;dbl&gt; 1.00000, 1.00000, 1.18056, 1.00000, 1.19869, 2.00000, 1.0~\n$ NAT3P2   &lt;dbl&gt; 1.00000, 1.00000, 1.00000, 1.50000, 1.00000, 2.00000, 1.0~\n$ NAT3P3   &lt;dbl&gt; 2.50000, 1.00000, 1.62051, 1.00000, 1.00000, 3.00000, 1.0~\n$ grade    &lt;int&gt; 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ~\n$ female   &lt;int&gt; 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, ~\n$ black    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ~\n$ hispanic &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ~\n$ other    &lt;int&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ~\n\n\nLa specificazione del modello SEM longitudinale per questi dati in lavaan può essere formulata in modo simile a un modello CFA per un singolo momento del tempo. In questo caso, ci sono due fattori comuni, che chiameremo Fattore_T1 e Fattore_T2, che vengono identificati dagli indicatori misurati nei due momenti del tempo. Questi due fattori comuni sono correlati tra loro.\nTuttavia, la differenza chiave rispetto ai casi precedenti è che i fattori specifici di ciascun indicatore nei due momenti del tempo sono anche correlati tra loro. Questo significa che, oltre alla correlazione tra i fattori comuni Fattore_T1 e Fattore_T2, dobbiamo anche specificare la correlazione tra i fattori specifici dei singoli indicatori nei due momenti del tempo.\n\nmod_1 &lt;- \"\n  # Definizione dei fattori latenti al tempo T1\n  Fattore_T1 =~ NA*PAT1P1 + PAT1P2 + PAT1P3\n\n  # Definizione dei fattori latenti al tempo T2\n  Fattore_T2 =~ NA*PAT2P1 + PAT2P2 + PAT2P3\n\n  # Varianza dei fattori latenti\n  Fattore_T1 ~~ 1*Fattore_T1\n  Fattore_T2 ~~ 1*Fattore_T2\n\n  # Covarianza tra i fattori latenti\n  Fattore_T1 ~~ Fattore_T2\n\n  # Definizione degli errori di misurazione per gli indicatori al tempo T1\n  PAT1P1 ~~ PAT1P1\n  PAT1P2 ~~ PAT1P2\n  PAT1P3 ~~ PAT1P3\n\n  # Definizione degli errori di misurazione per gli indicatori al tempo T2\n  PAT2P1 ~~ PAT2P1\n  PAT2P2 ~~ PAT2P2\n  PAT2P3 ~~ PAT2P3\n\n  # Covarianze tra i residui degli item tra T1 e T2\n  PAT1P1 ~~ PAT2P1\n  PAT1P2 ~~ PAT2P2\n  PAT1P3 ~~ PAT2P3\n\n  # Opzionale: Specifica delle medie degli indicatori (intercette)\n  PAT1P1 ~ 1\n  PAT1P2 ~ 1\n  PAT1P3 ~ 1\n  PAT2P1 ~ 1\n  PAT2P2 ~ 1\n  PAT2P3 ~ 1\n\"\n\nLe covarianze tra gli errori degli indicatori corrispondenti tra T1 e T2 sono stimate, indicando potenziali correlazioni tra gli errori degli stessi indicatori nei due momenti temporali.\nIn questo modello, i carichi fattoriali e le intercettazioni non sono ancora eguagliati nel tempo, il che significa che ogni set di indicatori è libero di avere relazioni uniche con il proprio fattore latente in ciascun momento temporale.\nQuesto modello è definito “configural-invariant” perché mantiene la stessa struttura fattoriale (o configurazione) nel tempo, ma non impone ancora l’equivalenza dei parametri tra i due momenti temporali.\nIl modello configural-invariant è spesso il punto di partenza per testare l’invarianza longitudinale in SEM, poiché stabilisce una base di confronto prima di imporre vincoli più stringenti come l’invarianza dei carichi fattoriali o delle intercette nei modelli successivi.\nAdattiamo il modello ai dati.\n\nfit_1 &lt;- lavaan::sem(mod_1, data = dat, meanstructure = TRUE)\n\n\nparameterEstimates(fit_1) |&gt; print()\n\n          lhs op        rhs   est    se       z pvalue ci.lower ci.upper\n1  Fattore_T1 =~     PAT1P1 0.670 0.022  30.862  0.000    0.628    0.713\n2  Fattore_T1 =~     PAT1P2 0.661 0.021  31.241  0.000    0.619    0.702\n3  Fattore_T1 =~     PAT1P3 0.643 0.021  29.979  0.000    0.601    0.685\n4  Fattore_T2 =~     PAT2P1 0.689 0.021  32.994  0.000    0.648    0.730\n5  Fattore_T2 =~     PAT2P2 0.680 0.021  33.049  0.000    0.639    0.720\n6  Fattore_T2 =~     PAT2P3 0.639 0.021  31.155  0.000    0.598    0.679\n7  Fattore_T1 ~~ Fattore_T1 1.000 0.000      NA     NA    1.000    1.000\n8  Fattore_T2 ~~ Fattore_T2 1.000 0.000      NA     NA    1.000    1.000\n9  Fattore_T1 ~~ Fattore_T2 0.552 0.027  20.141  0.000    0.498    0.606\n10     PAT1P1 ~~     PAT1P1 0.135 0.010  12.919  0.000    0.114    0.155\n11     PAT1P2 ~~     PAT1P2 0.121 0.010  12.308  0.000    0.102    0.141\n12     PAT1P3 ~~     PAT1P3 0.145 0.010  14.046  0.000    0.125    0.165\n13     PAT2P1 ~~     PAT2P1 0.102 0.008  12.160  0.000    0.086    0.119\n14     PAT2P2 ~~     PAT2P2 0.098 0.008  11.997  0.000    0.082    0.114\n15     PAT2P3 ~~     PAT2P3 0.125 0.009  14.711  0.000    0.108    0.142\n16     PAT1P1 ~~     PAT2P1 0.012 0.006   1.946  0.052    0.000    0.025\n17     PAT1P2 ~~     PAT2P2 0.005 0.006   0.884  0.377   -0.006    0.017\n18     PAT1P3 ~~     PAT2P3 0.011 0.006   1.781  0.075   -0.001    0.024\n19     PAT1P1 ~1            2.992 0.027 112.316  0.000    2.940    3.044\n20     PAT1P2 ~1            2.896 0.026 111.210  0.000    2.845    2.947\n21     PAT1P3 ~1            3.112 0.026 119.527  0.000    3.061    3.163\n22     PAT2P1 ~1            3.002 0.026 113.400  0.000    2.950    3.054\n23     PAT2P2 ~1            2.909 0.026 111.532  0.000    2.858    2.960\n24     PAT2P3 ~1            3.127 0.025 122.862  0.000    3.077    3.177\n25 Fattore_T1 ~1            0.000 0.000      NA     NA    0.000    0.000\n26 Fattore_T2 ~1            0.000 0.000      NA     NA    0.000    0.000\n\n\n\nsemPaths(fit_1,\n    whatLabels = \"std\",\n    sizeMan = 10,\n    edge.label.cex = 0.9,\n    style = \"mx\",\n    nCharNodes = 0, nCharEdges = 0\n)\n\n\n\n\n\n\n\n\n\nfitMeasures(fit_1, c(\"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt; print()\n\n  cfi   tli rmsea  srmr \n1.000 0.999 0.016 0.010 \n\n\nPotremmo pensare che modello di baseline (con cui possono essere confrontati i modelli che descrivono il cambiamento temporale) sia semplicemente il modello in cui non sono permesse covarianze.\n\nmod_2 &lt;- \"\n  # Definizione dei fattori latenti al tempo T1\n  Fattore_T1 =~ NA*PAT1P1 + PAT1P2 + PAT1P3\n\n  # Definizione dei fattori latenti al tempo T2\n  Fattore_T2 =~ NA*PAT2P1 + PAT2P2 + PAT2P3\n\n  # Varianza dei fattori latenti\n  Fattore_T1 ~~ 1*Fattore_T1\n  Fattore_T2 ~~ 1*Fattore_T2\n\n  # Covarianza tra i fattori latenti\n  Fattore_T1 ~~ 0*Fattore_T2\n\n  # Definizione degli errori di misurazione per gli indicatori al tempo T1\n  PAT1P1 ~~ PAT1P1\n  PAT1P2 ~~ PAT1P2\n  PAT1P3 ~~ PAT1P3\n\n  # Definizione degli errori di misurazione per gli indicatori al tempo T2\n  PAT2P1 ~~ PAT2P1\n  PAT2P2 ~~ PAT2P2\n  PAT2P3 ~~ PAT2P3\n\n  # Covarianze tra i residui degli item tra T1 e T2\n  # PAT1P1 ~~ PAT2P1\n  # PAT1P2 ~~ PAT2P2\n  # PAT1P3 ~~ PAT2P3\n\n  # Opzionale: Specifica delle medie degli indicatori (intercette)\n  PAT1P1 ~ 1\n  PAT1P2 ~ 1\n  PAT1P3 ~ 1\n  PAT2P1 ~ 1\n  PAT2P2 ~ 1\n  PAT2P3 ~ 1\n\"\n\n\nfit_2 &lt;- lavaan::sem(mod_2, data = dat, meanstructure = TRUE)\n\n\nsemPaths(fit_2,\n    whatLabels = \"std\",\n    sizeMan = 10,\n    edge.label.cex = 0.9,\n    style = \"mx\",\n    nCharNodes = 0, nCharEdges = 0\n)\n\n\n\n\n\n\n\n\n\nparameterEstimates(fit_2) |&gt; print()\n\n          lhs op        rhs   est    se       z pvalue ci.lower ci.upper\n1  Fattore_T1 =~     PAT1P1 0.670 0.022  30.720      0    0.627    0.713\n2  Fattore_T1 =~     PAT1P2 0.658 0.021  30.943      0    0.616    0.700\n3  Fattore_T1 =~     PAT1P3 0.647 0.021  30.105      0    0.605    0.689\n4  Fattore_T2 =~     PAT2P1 0.689 0.021  32.917      0    0.648    0.730\n5  Fattore_T2 =~     PAT2P2 0.681 0.021  33.073      0    0.641    0.722\n6  Fattore_T2 =~     PAT2P3 0.636 0.021  30.961      0    0.596    0.676\n7  Fattore_T1 ~~ Fattore_T1 1.000 0.000      NA     NA    1.000    1.000\n8  Fattore_T2 ~~ Fattore_T2 1.000 0.000      NA     NA    1.000    1.000\n9  Fattore_T1 ~~ Fattore_T2 0.000 0.000      NA     NA    0.000    0.000\n10     PAT1P1 ~~     PAT1P1 0.135 0.011  12.641      0    0.114    0.156\n11     PAT1P2 ~~     PAT1P2 0.125 0.010  12.310      0    0.105    0.145\n12     PAT1P3 ~~     PAT1P3 0.141 0.010  13.503      0    0.120    0.161\n13     PAT2P1 ~~     PAT2P1 0.102 0.009  11.872      0    0.085    0.119\n14     PAT2P2 ~~     PAT2P2 0.097 0.008  11.609      0    0.080    0.113\n15     PAT2P3 ~~     PAT2P3 0.127 0.009  14.716      0    0.110    0.144\n16     PAT1P1 ~1            2.992 0.027 112.344      0    2.940    3.044\n17     PAT1P2 ~1            2.896 0.026 111.246      0    2.845    2.947\n18     PAT1P3 ~1            3.112 0.026 119.412      0    3.061    3.163\n19     PAT2P1 ~1            3.002 0.026 113.387      0    2.950    3.054\n20     PAT2P2 ~1            2.909 0.026 111.468      0    2.858    2.960\n21     PAT2P3 ~1            3.127 0.025 123.031      0    3.077    3.177\n22 Fattore_T1 ~1            0.000 0.000      NA     NA    0.000    0.000\n23 Fattore_T2 ~1            0.000 0.000      NA     NA    0.000    0.000\n\n\n\nfitMeasures(fit_2, c(\"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt; print()\n\n  cfi   tli rmsea  srmr \n0.931 0.886 0.187 0.253 \n\n\nTuttavia, Little (2023) fa notare che, nel contesto dei disegni longitudinali, il modello di base adeguato prevede che vengano aggiunte al modello nullo delle aspettative aggiuntive, specificatamente che le medie e le varianze rimangano invariate nel tempo. Questa specificazione ampliata del modello nullo fornisce il confronto appropriato per analizzare e interpretare i dati longitudinali.\n\nmod_3 &lt;- \"\n  # Definizione dei fattori latenti al tempo T1\n  Fattore_T1 =~ NA*PAT1P1 + b1*PAT1P2 + b2*PAT1P3\n\n  # Definizione dei fattori latenti al tempo T2\n  Fattore_T2 =~ NA*PAT2P1 + b1*PAT2P2 + b2*PAT2P3\n\n  # Varianza dei fattori latenti\n  Fattore_T1 ~~ c1*Fattore_T1\n  Fattore_T2 ~~ c1*Fattore_T2\n\n  # Covarianza tra i fattori latenti (assumendo che sia 0)\n  Fattore_T1 ~~ 0*Fattore_T2\n\n  # Definizione degli errori di misurazione per gli indicatori al tempo T1\n  PAT1P1 ~~ a1*PAT1P1\n  PAT1P2 ~~ a2*PAT1P2\n  PAT1P3 ~~ a3*PAT1P3\n\n  # Definizione degli errori di misurazione per gli indicatori al tempo T2\n  PAT2P1 ~~ a1*PAT2P1\n  PAT2P2 ~~ a2*PAT2P2\n  PAT2P3 ~~ a3*PAT2P3\n\n  # Specifica delle medie degli indicatori (intercettazioni) uguali tra i due tempi\n  # PAT1P1 ~ m1\n  # PAT1P2 ~ m2\n  # PAT1P3 ~ m3\n  # PAT2P1 ~ m1\n  # PAT2P2 ~ m2\n  # PAT2P3 ~ m3\n\"\n\n\nfit_3 &lt;- lavaan::sem(mod_3, data = dat, meanstructure = TRUE)\n\n\nsemPaths(fit_3,\n    whatLabels = \"std\",\n    sizeMan = 10,\n    edge.label.cex = 0.9,\n    style = \"mx\",\n    nCharNodes = 0, nCharEdges = 0\n)\n\n\n\n\n\n\n\n\n\nparameterEstimates(fit_3) |&gt; print()\n\n          lhs op        rhs label   est    se       z pvalue ci.lower\n1  Fattore_T1 =~     PAT1P1       0.863 0.018  47.730      0    0.828\n2  Fattore_T1 =~     PAT1P2    b1 0.854 0.012  71.968      0    0.831\n3  Fattore_T1 =~     PAT1P3    b2 0.818 0.012  66.282      0    0.794\n4  Fattore_T2 =~     PAT2P1       0.871 0.018  48.118      0    0.835\n5  Fattore_T2 =~     PAT2P2    b1 0.854 0.012  71.968      0    0.831\n6  Fattore_T2 =~     PAT2P3    b2 0.818 0.012  66.282      0    0.794\n7  Fattore_T1 ~~ Fattore_T1    c1 0.614 0.014  44.667      0    0.587\n8  Fattore_T2 ~~ Fattore_T2    c1 0.614 0.014  44.667      0    0.587\n9  Fattore_T1 ~~ Fattore_T2       0.000 0.000      NA     NA    0.000\n10     PAT1P1 ~~     PAT1P1    a1 0.119 0.007  17.380      0    0.105\n11     PAT1P2 ~~     PAT1P2    a2 0.111 0.007  16.933      0    0.098\n12     PAT1P3 ~~     PAT1P3    a3 0.134 0.007  19.941      0    0.121\n13     PAT2P1 ~~     PAT2P1    a1 0.119 0.007  17.380      0    0.105\n14     PAT2P2 ~~     PAT2P2    a2 0.111 0.007  16.933      0    0.098\n15     PAT2P3 ~~     PAT2P3    a3 0.134 0.007  19.941      0    0.121\n16     PAT1P1 ~1                  2.992 0.026 113.076      0    2.940\n17     PAT1P2 ~1                  2.896 0.026 111.098      0    2.844\n18     PAT1P3 ~1                  3.112 0.026 120.907      0    3.062\n19     PAT2P1 ~1                  3.002 0.027 112.656      0    2.949\n20     PAT2P2 ~1                  2.909 0.026 111.616      0    2.858\n21     PAT2P3 ~1                  3.127 0.026 121.471      0    3.076\n22 Fattore_T1 ~1                  0.000 0.000      NA     NA    0.000\n23 Fattore_T2 ~1                  0.000 0.000      NA     NA    0.000\n   ci.upper\n1     0.898\n2     0.878\n3     0.842\n4     0.906\n5     0.878\n6     0.842\n7     0.641\n8     0.641\n9     0.000\n10    0.132\n11    0.123\n12    0.147\n13    0.132\n14    0.123\n15    0.147\n16    3.044\n17    2.947\n18    3.163\n19    3.054\n20    2.960\n21    3.177\n22    0.000\n23    0.000\n\n\n\nfitMeasures(fit_3, c(\"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt; print()\n\n  cfi   tli rmsea  srmr \n0.927 0.916 0.160 0.254 \n\n\nPossiamo ora fare il confronto tra il modello di cambiamento latente e l’appropriato modello di confronto.\n\nlavTestLRT(fit_1, fit_3) |&gt; print()\n\n\nChi-Squared Difference Test\n\n      Df    AIC    BIC    Chisq Chisq diff  RMSEA Df diff Pr(&gt;Chisq)    \nfit_1  5 7427.8 7531.4   6.0645                                         \nfit_3 13 7693.5 7759.5 287.8078     281.74 0.2039       8  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nÈ evidente che, nel contesto di questi dati, un modello che presuma l’assenza di qualsiasi cambiamento è completamente inadeguato.\n\n\n\n\nLittle, T. D. (2023). Longitudinal structural equation modeling. Guilford Press.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>75</span>  <span class='chapter-title'>Dati longitudinali</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/05_intro_panel.html",
    "href": "chapters/lgm/05_intro_panel.html",
    "title": "76  Specificare e Interpretare un Modello Longitudinale",
    "section": "",
    "text": "76.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nNel presente capitolo esploreremo i modelli panel longitudinali tradizionali, con un’attenzione particolare alla struttura simplex e alle tecniche per ottimizzare, interpretare e arricchire questi modelli con covariate e analisi degli effetti indiretti.\nI modelli panel e i modelli SEM longitudinali si concentrano sulle relazioni predittive tra variabili latenti (ad esempio, atteggiamenti o abilità) e sulle loro variazioni nel tempo. Sebbene spesso usati come sinonimi, i modelli panel differiscono dai modelli di Confirmatory Factor Analysis (CFA) longitudinali: mentre i CFA analizzano la stabilità dei livelli medi dei costrutti, i modelli panel esplorano le interazioni dinamiche tra variabili nel corso del tempo. Un’importante distinzione va fatta anche con i modelli di crescita latente (LGM), che hanno l’obiettivo di mappare l’evoluzione temporale dei livelli medi di un costrutto, come il monitoraggio dello sviluppo di una competenza specifica.\nInoltre, le relazioni di regressione nei modelli panel suggeriscono un’interpretazione causale, ma questa deve essere affrontata con cautela. La causalità in questi modelli è implicata quando si osservano effetti predittivi coerenti nel tempo, ma è essenziale che i dati siano raccolti con rigore per permettere inferenze causali. Un elemento fondamentale è il controllo delle variabili confondenti, che permette di ridurre il rischio di bias e di migliorare la robustezza delle inferenze.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>76</span>  <span class='chapter-title'>Specificare e Interpretare un Modello Longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/05_intro_panel.html#il-processo-di-cambiamento-simplex",
    "href": "chapters/lgm/05_intro_panel.html#il-processo-di-cambiamento-simplex",
    "title": "76  Specificare e Interpretare un Modello Longitudinale",
    "section": "76.2 Il Processo di Cambiamento Simplex",
    "text": "76.2 Il Processo di Cambiamento Simplex\nUn modello efficace per rappresentare il cambiamento continuo e graduale nel tempo è la struttura simplex. Questa struttura si basa sull’assunto che gli individui cambino a un ritmo stabile, con influenze esterne minime. Nel modello simplex, la correlazione tra punti temporali decresce in modo prevedibile, secondo una progressione graduale. La Tabella 1 illustra una struttura di correlazione simplex in cui la stabilità decresce col passare del tempo.\nTabella 1.  Esempi di Strutture di Correlazione Simplex.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nT1\nT2\nT3\nT4\nT5\nT6\nT7\nT8\n\n\n\n\nT1\n–\n.800\n.640\n.512\n.410\n.328\n.262\n.210\n\n\nT2\n.528\n–\n.800\n.640\n.512\n.410\n.328\n.262\n\n\nT3\n.279\n.528\n–\n.800\n.640\n.512\n.410\n.328\n\n\nT4\n.147\n.279\n.528\n–\n.800\n.640\n.512\n.410\n\n\nT5\n.078\n.147\n.279\n.528\n–\n.800\n.640\n.512\n\n\nT6\n.041\n.078\n.147\n.279\n.528\n–\n.800\n.640\n\n\nT7\n.022\n.041\n.078\n.147\n.279\n.528\n–\n.800\n\n\nT8\n.011\n.022\n.041\n.078\n.147\n.279\n.528\n–\n\n\n\nNota: Le correlazioni sopra la diagonale sono basate sull’analogia del mescolamento delle carte con una correlazione autoregressiva di .528. Le correlazioni sotto la diagonale si basano su una stabilità iniziale più elevata (.80), indicando una persistenza più forte nel tempo.\nNel modello simplex, il cambiamento graduale e stabile è rappresentato da un coefficiente di stabilità costante tra punti temporali consecutivi, simile alla correlazione tra l’ordine delle carte in un mazzo dopo diverse mescolate.\nL’esempio del mazzo di carte mescolato è una metafora utile per comprendere come funziona una struttura di correlazione simplex e il concetto di autoregressività nei modelli longitudinali. Immaginiamo di avere un mazzo di carte perfettamente ordinato, in cui ogni carta ha una posizione specifica. Ogni volta che mescoliamo il mazzo, l’ordine cambia, ma non in modo totalmente casuale: la disposizione iniziale ha ancora una certa influenza sull’ordine risultante dopo la mescolata.\n\n76.2.1 Mescolamento e Correlazione\nSe consideriamo l’ordine delle carte prima e dopo una singola mescolata, possiamo calcolare la correlazione tra la posizione delle carte iniziale e quella dopo il mescolamento. Una sola mescolata modifica la disposizione delle carte, ma mantiene una certa somiglianza con l’ordine iniziale: diciamo, per esempio, che la correlazione è di 0.528. Questa correlazione rappresenta la stabilità del cambiamento: dopo una singola mescolata, le carte non sono ancora completamente in un ordine casuale.\nOgni successiva mescolata riduce ulteriormente questa correlazione. Dopo una seconda mescolata, la correlazione tra l’ordine originale e il nuovo ordine sarà inferiore, ad esempio 0.279. Con il terzo mescolamento, la correlazione continua a decrescere, e così via. Dopo circa sette mescolate perfette, l’ordine diventa quasi del tutto casuale, con una correlazione vicino a 0 rispetto all’ordine iniziale.\n\n\n76.2.2 Cosa Rappresenta nel Contesto dei Modelli Longitudinali\nIn un modello longitudinale con struttura simplex, ogni “mescolata” rappresenta un passaggio temporale in cui un fenomeno cambia gradualmente ma in modo prevedibile. La correlazione tra i punti temporali successivi diminuisce man mano che ci si allontana dal punto di partenza, proprio come la correlazione dell’ordine delle carte diminuisce con ogni mescolata.\n\nCorrelazione tra punti temporali consecutivi: rappresenta la stabilità immediata del costrutto. Più è alta la correlazione tra misurazioni consecutive, maggiore è la stabilità del fenomeno nel tempo.\nCorrelazione tra punti temporali distanti: rappresenta quanto il fenomeno rimanga stabile su periodi più lunghi. Una diminuzione graduale della correlazione, come nell’esempio del mazzo di carte, è tipica di processi che cambiano in modo costante ma senza grandi sconvolgimenti improvvisi.\n\nIn conclusione, l’esempio del mazzo di carte ci aiuta a visualizzare come un modello simplex cattura il cambiamento graduale e prevedibile in un processo. Ogni passaggio temporale influenza il successivo, ma con il tempo questa influenza diminuisce, portando a una correlazione minore tra i punti temporali distanti.\nQuesta struttura è utile nei modelli panel longitudinali perché descrive una dinamica di cambiamento continua e coerente, tipica di molti fenomeni psicologici e sociali che evolvono in modo graduale e prevedibile nel tempo.\n\n\n\n\n\n\nFigura 76.1: Stime dei parametri standardizzati dal modello simplex di mazzi di carte mescolati consecutivamente. Nota. Queste stime dei parametri provengono da un modello adattato ai dati nella tabella precedente. Questo modello ha 21 gradi di libertà e un adattamento perfetto del modello. Le correlazioni tra i mazzi separati da più di una mescolata sono riprodotte tracciando i percorsi di regressione tra ciascun mazzo consecutivo. Le linee tratteggiate mostrano le correlazioni riprodotte con l’ordine iniziale del mazzo. (Figura tratta da Little, 2023)\n\n\n\n\n\n76.2.3 Modelli Simplex e Modelli Autoregressivi (AR1 e AR2)\nIn termini formali, possiamo dire che la struttura simplex può essere vista come un’istanza di un modello autoregressivo di primo ordine (AR1), in cui ogni punto temporale è correlato solo con il precedente. Nei modelli AR1, l’effetto di ogni variabile dipende unicamente dalla sua osservazione immediatamente precedente. Per processi che mostrano maggiore persistenza nel tempo, si può invece adottare un modello autoregressivo di secondo ordine (AR2), dove ogni punto è influenzato non solo dal precedente, ma anche dal punto ancora precedente.\nIl modello AR2 suggerisce che l’influenza persiste per due passaggi temporali, implicando una stabilità più duratura rispetto al modello AR1. Questo approccio è utile per rappresentare processi in cui l’effetto di un evento non si dissipa immediatamente, ma ha un’influenza estesa nel tempo.\n\n\n76.2.4 Applicazioni della Struttura Simplex nella Ricerca Psicologica\nIn psicologia e scienze sociali, il modello simplex è frequentemente usato per studiare processi di cambiamento in campioni longitudinali. La semplicità di questa struttura la rende una scelta ideale per rappresentare fenomeni evolutivi graduali, come lo sviluppo di competenze o il cambiamento di atteggiamenti. La struttura simplex può anche essere estesa con l’inclusione di variabili contestuali, il che ne aumenta la flessibilità senza compromettere la chiarezza.\nPer comprendere la natura di un processo di cambiamento, è essenziale che il modello predittivo catturi correttamente il ritmo del cambiamento stesso. Una frequenza di misurazione adeguata permette di rilevare con precisione la velocità e la consistenza delle variazioni, aumentando la validità delle inferenze che si possono trarre.\nIn sintesi, i modelli panel longitudinali e la struttura simplex offrono potenti strumenti per analizzare il cambiamento e le relazioni temporali nei dati longitudinali. La struttura simplex, in particolare, è una rappresentazione versatile ed efficace dei processi di cambiamento graduale, utile per studiare fenomeni psicologici ed evolutivi in modo teoricamente informato e statisticamente robusto.\n\n\n76.2.5 Modello Simplex per il Mescolamento di Carte\nEsaminiamo qui di seguto l’implementazione del modello Simplex proposta da Little (2023) per i dati artificiali relativi all’esempio del mazzo di carte discusso in precedenza.\n\ntri_corr &lt;- c(\n    1, rep(0, 7),\n    0.523, 1, rep(0, 6),\n    0.279, 0.523, 1, rep(0, 5),\n    0.147, 0.279, 0.523, 1, rep(0, 4),\n    0.078, 0.147, 0.279, 0.523, 1, rep(0, 3),\n    0.041, 0.078, 0.147, 0.279, 0.523, 1, rep(0, 2),\n    0.022, 0.041, 0.078, 0.147, 0.279, 0.523, 1, 0,\n    0.011, 0.022, 0.041, 0.078, 0.147, 0.279, 0.523, 1\n)\nupper &lt;- matrix(tri_corr, 8, byrow = FALSE)\nlower &lt;- matrix(tri_corr, 8, byrow = TRUE)\nmycorr &lt;- upper + lower - diag(8)\n\nrownames(mycorr) &lt;- c(\"Time1\", \"Time2\", \"Time3\", \"Time4\", \"Time5\", \"Time6\", \"Time7\", \"Time8\")\ncolnames(mycorr) &lt;- c(\"Time1\", \"Time2\", \"Time3\", \"Time4\", \"Time5\", \"Time6\", \"Time7\", \"Time8\")\nmynob &lt;- 166\n\nmycorr |&gt; print()\n\n      Time1 Time2 Time3 Time4 Time5 Time6 Time7 Time8\nTime1 1.000 0.523 0.279 0.147 0.078 0.041 0.022 0.011\nTime2 0.523 1.000 0.523 0.279 0.147 0.078 0.041 0.022\nTime3 0.279 0.523 1.000 0.523 0.279 0.147 0.078 0.041\nTime4 0.147 0.279 0.523 1.000 0.523 0.279 0.147 0.078\nTime5 0.078 0.147 0.279 0.523 1.000 0.523 0.279 0.147\nTime6 0.041 0.078 0.147 0.279 0.523 1.000 0.523 0.279\nTime7 0.022 0.041 0.078 0.147 0.279 0.523 1.000 0.523\nTime8 0.011 0.022 0.041 0.078 0.147 0.279 0.523 1.000\n\n\n\nmod6_2 &lt;- \"\n    Time2 ~ Time1\n    Time3 ~ Time2\n    Time4 ~ Time3\n    Time5 ~ Time4\n    Time6 ~ Time5\n    Time7 ~ Time6\n    Time8 ~ Time7\n\n    Time1 ~~ 1*Time1\n    Time2 ~~ Time2\n    Time3 ~~ Time3\n    Time4 ~~ Time4\n    Time5 ~~ Time5\n    Time6 ~~ Time6\n    Time7 ~~ Time7\n    Time8 ~~ Time8\n\"\n\n\nfit6_2 &lt;- lavaan(mod6_2, sample.cov = mycorr, sample.nobs = mynob, fixed.x = FALSE)\n\n\nparameterEstimates(fit6_2) |&gt; print()\n\n     lhs op   rhs   est    se     z pvalue ci.lower ci.upper\n1  Time2  ~ Time1 0.523 0.066 7.930      0    0.394    0.652\n2  Time3  ~ Time2 0.523 0.066 7.912      0    0.393    0.653\n3  Time4  ~ Time3 0.523 0.066 7.908      0    0.393    0.653\n4  Time5  ~ Time4 0.523 0.066 7.906      0    0.393    0.653\n5  Time6  ~ Time5 0.523 0.066 7.906      0    0.393    0.653\n6  Time7  ~ Time6 0.523 0.066 7.906      0    0.393    0.653\n7  Time8  ~ Time7 0.523 0.066 7.906      0    0.393    0.653\n8  Time1 ~~ Time1 1.000 0.000    NA     NA    1.000    1.000\n9  Time2 ~~ Time2 0.722 0.079 9.110      0    0.567    0.877\n10 Time3 ~~ Time3 0.722 0.079 9.110      0    0.567    0.877\n11 Time4 ~~ Time4 0.722 0.079 9.110      0    0.567    0.877\n12 Time5 ~~ Time5 0.722 0.079 9.110      0    0.567    0.877\n13 Time6 ~~ Time6 0.722 0.079 9.110      0    0.567    0.877\n14 Time7 ~~ Time7 0.722 0.079 9.110      0    0.567    0.877\n15 Time8 ~~ Time8 0.722 0.079 9.110      0    0.567    0.877\n\n\nNel commentare il modello Simplex specificato, si può osservare che, per i dati artificiali in questione, la stima della correlazione tra costrutti latenti in momenti successivi risulta costante, con un valore di 0.523. Questo dato è in linea con i risultati ottenuti da Little (2023), che riporta una correlazione di 0.528. È importante notare la consistenza in queste stime, indicativa di una relazione stabile nel tempo tra i costrutti.\nInoltre, il modello mostra che la varianza delle variabili latenti rimane relativamente costante nel tempo, con un valore di 0.722. Questo suggerisce che, nonostante il passare del tempo e i possibili cambiamenti nei costrutti, la quantità di varianza che essi spiegano rimane simile. Un’eccezione a questo schema si trova nella varianza al Tempo 1, che è stata fissata a 1. Questa scelta metodologica è comune in molti modelli di serie temporali per stabilire un punto di riferimento o una scala di misurazione per le varianze nei tempi successivi.\nRifocalizziamoci sulle correlazioni nella parte inferiore della diagonale della Tabella 1, dove possiamo osservare una stabilità piuttosto elevata tra punti temporali successivi, con una correlazione di 0.80 tra ciascun punto temporale e il successivo. Tuttavia, all’aumentare dell’intervallo tra le misurazioni, la correlazione tra punti temporali distanti diminuisce in modo graduale e prevedibile. Ad esempio, nella tabella, la correlazione tra Tempo 1 e Tempo 3 è di 0.64, lo stesso valore che troviamo tra Tempo 2 e Tempo 4, tra Tempo 5 e Tempo 7, e così via per ogni coppia di punti temporali separati da uno spazio temporale intermedio. Alla massima distanza, la correlazione tra Tempo 1 e Tempo 8 è ancora leggermente positiva, pari a 0.210; con un numero crescente di punti temporali, questa correlazione si avvicinerebbe gradualmente a zero.\nQuesto schema di correlazioni evidenzia che, mentre la stabilità a breve termine (tra punti temporali adiacenti) è elevata, essa diminuisce all’aumentare della distanza tra le misurazioni. Questo riflette una riduzione dell’influenza o della connessione tra i costrutti latenti misurati a intervalli temporali più lunghi.\nNel triangolo superiore della tabella, la stabilità tra punti temporali adiacenti è più bassa, con una correlazione di 0.528. Qui, i punti temporali bi-contigui (separati da un intervallo intermedio) si correlano a 0.279 e la correlazione tra i punti più distanti, da Tempo 1 a Tempo 8, scende a 0.011, praticamente nulla. Entrambi questi schemi riflettono un tasso costante di cambiamento e sono ben rappresentati da un modello autoregressivo simplex.\nUn modello autoregressivo simplex è in grado di riprodurre tutte queste correlazioni attraverso effetti indiretti: nel modello, l’influenza del Tempo 1 sul Tempo 8 viene trasmessa indirettamente attraverso una sequenza di influenze dirette da un punto temporale al successivo (es., da Tempo 1 a Tempo 2, da Tempo 2 a Tempo 3, e così via fino a Tempo 8). Questo passaggio continuo di influenze permette di riprodurre lo schema di correlazioni osservato nella Tabella 1. Utilizzando le regole di tracciamento dei percorsi in un modello autoregressivo, possiamo osservare come questo schema di correlazioni diminuisca progressivamente, evidenziando il modo in cui il modello rappresenta il declino della connessione tra i punti temporali man mano che aumenta la distanza tra essi.\n\n0.523^{2:7} |&gt;\n    round(3) |&gt;\n    print()\n\n[1] 0.274 0.143 0.075 0.039 0.020 0.011\n\n\nIl modello presentato nella (little-fig-simplex?) è un modello Simplex univariato perché include un solo costrutto, rappresentato in più punti temporali. È importante notare che, in un modello come questo, si verifica e si garantisce la forte invarianza fattoriale del modello di misurazione. Inoltre, i residui corrispondenti presentano unicità correlate nel tempo per ogni occorrenza dello stesso indicatore. I coefficienti di percorso in questo modello riproducono perfettamente le correlazioni nella parte superiore della Tabella 1.\nLo stesso modello, quando applicato all’altro insieme di correlazioni nella Tabella 1, riprodurrebbe altrettanto perfettamente il pattern di correlazione. Gli effetti diretti in ciascun punto temporale adiacente sarebbero di .8, mentre l’effetto indiretto sarebbe il prodotto multiplo dei coefficienti di percorso diretti. Questo approccio mette in evidenza come le correlazioni tra punti temporali più lontani siano il risultato di una serie di influenze dirette che si susseguono nel tempo.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>76</span>  <span class='chapter-title'>Specificare e Interpretare un Modello Longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/05_intro_panel.html#basi-di-un-modello-panel",
    "href": "chapters/lgm/05_intro_panel.html#basi-di-un-modello-panel",
    "title": "76  Specificare e Interpretare un Modello Longitudinale",
    "section": "76.3 Basi di un Modello Panel",
    "text": "76.3 Basi di un Modello Panel\nConsideriamo ora il modello panel. Nella costruzione di un modello panel, le associazioni tra variabili osservate in momenti diversi sono spesso trasformate in percorsi di regressione direzionali. Le covarianze tra variabili al Tempo 1 sono generalmente viste come associazioni “a ordine zero,” ovvero associazioni non controllate per altre variabili. Nei momenti successivi, le covarianze tra variabili rappresentano invece varianze residue, o “fattori di disturbo,” ossia componenti di varianza non spiegate dagli effetti temporali precedenti.\nUn concetto chiave nei modelli panel è il percorso autoregressivo, che rappresenta la relazione predittiva tra lo stesso costrutto in momenti successivi. In aggiunta, i modelli panel possono includere effetti incrociati ritardati (o cross-lagged), dove una variabile predice un’altra variabile in un momento futuro. Entrambi questi tipi di percorso permettono di osservare le dinamiche temporali e la persistenza di influenze tra variabili.\n\n\n\n\n\n\nFigura 76.2: Etichette dei parametri per tre punti temporali con affetto positivo e affetto negativo: Un’analisi di base del modello panel direzionale. Nota. Si consente l’associazione delle varianze residue tra gli indicatori corrispondenti nel tempo. (Figura adattata da Little, 2023)\n\n\n\n\n76.3.1 Modello Panel per Affetto Positivo e Negativo\nLa figura (little-fig-simplex?) approfondisce l’uso dei modelli longitudinali tramite un esempio di modello CFA per studiare l’affetto positivo e negativo negli adolescenti. A differenza di un’analisi limitata a soli due punti temporali, questo esempio illustra una configurazione più complessa, in cui i due costrutti (affetto positivo e negativo) vengono misurati in tre momenti distinti. Questa struttura a più punti temporali permette di esaminare come i livelli di affetto positivo e negativo cambiano nel tempo e di osservare le interazioni tra i costrutti lungo diverse fasi della misurazione.\nI dati sono i seguenti.\n\ndat &lt;- read.table(\n    file = \"../../data/grade7and8.long.823.dat\",\n    col.names = c(\n        \"PAT1P1\", \"PAT1P2\", \"PAT1P3\", \"NAT1P1\", \"NAT1P2\", \"NAT1P3\",\n        \"PAT2P1\", \"PAT2P2\", \"PAT2P3\", \"NAT2P1\", \"NAT2P2\", \"NAT2P3\",\n        \"PAT3P1\", \"PAT3P2\", \"PAT3P3\", \"NAT3P1\", \"NAT3P2\", \"NAT3P3\",\n        \"grade\", \"female\", \"black\", \"hispanic\", \"other\"\n    )\n)\n\n\npsych::describe(dat[, 1:18])\n\n\nA psych: 18 x 13\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nPAT1P1\n1\n823\n2.991885\n0.7644692\n3.00000\n3.042751\n0.7413000\n1\n4\n3\n-0.4591316\n-0.3486015\n0.02664772\n\n\nPAT1P2\n2\n823\n2.895543\n0.7471531\n3.00000\n2.919860\n0.7413000\n1\n4\n3\n-0.2986814\n-0.3732379\n0.02604412\n\n\nPAT1P3\n3\n823\n3.112289\n0.7481646\n3.00000\n3.175932\n0.7413000\n1\n4\n3\n-0.5761073\n-0.2344880\n0.02607938\n\n\nNAT1P1\n4\n823\n1.706311\n0.7057569\n1.50000\n1.596773\n0.7413000\n1\n4\n3\n1.2112142\n1.2584152\n0.02460114\n\n\nNAT1P2\n5\n823\n1.450148\n0.6576346\n1.00000\n1.312657\n0.0000000\n1\n4\n3\n1.7700996\n3.1319596\n0.02292370\n\n\nNAT1P3\n6\n823\n1.453063\n0.6678318\n1.00000\n1.311948\n0.0000000\n1\n4\n3\n1.8185647\n3.3558790\n0.02327915\n\n\nPAT2P1\n7\n823\n3.001628\n0.7599034\n3.00000\n3.044683\n0.7413000\n1\n4\n3\n-0.4055187\n-0.5083138\n0.02648857\n\n\nPAT2P2\n8\n823\n2.909043\n0.7491414\n3.00000\n2.936239\n0.7413000\n1\n4\n3\n-0.2667719\n-0.5077469\n0.02611343\n\n\nPAT2P3\n9\n823\n3.126799\n0.7295410\n3.09211\n3.189561\n0.8778623\n1\n4\n3\n-0.6282730\n-0.1499079\n0.02543020\n\n\nNAT2P1\n10\n823\n1.695210\n0.6614440\n1.50000\n1.606124\n0.7413000\n1\n4\n3\n1.1219909\n1.2251472\n0.02305649\n\n\nNAT2P2\n11\n823\n1.537798\n0.6225102\n1.50000\n1.429653\n0.7413000\n1\n4\n3\n1.3883623\n1.9372133\n0.02169934\n\n\nNAT2P3\n12\n823\n1.580027\n0.6499109\n1.50000\n1.471439\n0.7413000\n1\n4\n3\n1.3401267\n1.8529497\n0.02265447\n\n\nPAT3P1\n13\n823\n2.886528\n0.7823545\n3.00000\n2.917427\n0.7413000\n1\n4\n3\n-0.2178390\n-0.7223282\n0.02727116\n\n\nPAT3P2\n14\n823\n2.849560\n0.7624570\n3.00000\n2.868214\n0.7413000\n1\n4\n3\n-0.1687695\n-0.6306593\n0.02657758\n\n\nPAT3P3\n15\n823\n3.056508\n0.7484883\n3.00000\n3.107101\n0.7413000\n1\n4\n3\n-0.4511290\n-0.5120333\n0.02609066\n\n\nNAT3P1\n16\n823\n1.723787\n0.6912895\n1.50000\n1.623666\n0.7413000\n1\n4\n3\n1.2328507\n1.5654947\n0.02409684\n\n\nNAT3P2\n17\n823\n1.575689\n0.6600865\n1.50000\n1.469134\n0.7413000\n1\n4\n3\n1.3962112\n2.0916581\n0.02300917\n\n\nNAT3P3\n18\n823\n1.641652\n0.6980201\n1.50000\n1.533856\n0.7413000\n1\n4\n3\n1.1666457\n1.0690468\n0.02433145\n\n\n\n\n\n\nplots_list &lt;- list()\n\n# Creazione della lista di grafici con i pannelli più grandi\nplots_list &lt;- list()\n\nfor (i in 1:16) {\n    col_name &lt;- names(dat)[i]\n    p &lt;- ggplot(dat, aes(x = !!sym(col_name))) +\n        geom_density(fill = \"blue\", color = \"black\", alpha = 0.5) +\n        ggtitle(col_name)\n    plots_list[[i]] &lt;- p\n}\n\n# Organizza e visualizza i grafici con pannelli più grandi\ndo.call(grid.arrange, c(plots_list, ncol = 4)) \n\n\n\n\n\n\n\n\nIniziamo a specificare il modello nullo.\n\nmod_null &lt;- \"\n    PAT1P1 ~~ V1*PAT1P1\n    PAT1P2 ~~ V2*PAT1P2\n    PAT1P3 ~~ V3*PAT1P3\n    NAT1P1 ~~ V4*NAT1P1\n    NAT1P2 ~~ V5*NAT1P2\n    NAT1P3 ~~ V6*NAT1P3\n\n    PAT2P1 ~~ V1*PAT2P1\n    PAT2P2 ~~ V2*PAT2P2\n    PAT2P3 ~~ V3*PAT2P3\n    NAT2P1 ~~ V4*NAT2P1\n    NAT2P2 ~~ V5*NAT2P2\n    NAT2P3 ~~ V6*NAT2P3\n\n    PAT3P1 ~~ V1*PAT3P1\n    PAT3P2 ~~ V2*PAT3P2\n    PAT3P3 ~~ V3*PAT3P3\n    NAT3P1 ~~ V4*NAT3P1\n    NAT3P2 ~~ V5*NAT3P2\n    NAT3P3 ~~ V6*NAT3P3\n\n    PAT1P1 ~ T1*1\n    PAT1P2 ~ T2*1\n    PAT1P3 ~ T3*1\n    NAT1P1 ~ T4*1\n    NAT1P2 ~ T5*1\n    NAT1P3 ~ T6*1\n\n    PAT2P1 ~ T1*1\n    PAT2P2 ~ T2*1\n    PAT2P3 ~ T3*1\n    NAT2P1 ~ T4*1\n    NAT2P2 ~ T5*1\n    NAT2P3 ~ T6*1\n\n    PAT3P1 ~ T1*1\n    PAT3P2 ~ T2*1\n    PAT3P3 ~ T3*1\n    NAT3P1 ~ T4*1\n    NAT3P2 ~ T5*1\n    NAT3P3 ~ T6*1\n\"\n\nIl modello nullo (baseline) è usato da Little (2023) come punto di partenza nell’analisi SEM e per i confronti con modelli più complessi. Il modello nullo specifica sei variabili osservate (PAT1P1, PAT1P2, PAT1P3, NAT1P1, NAT1P2, NAT1P3) misurate in tre punti temporali distinti. Questo implica che ci sono 18 variabili osservate in totale. Ogni variabile osservata ha la propria varianza unica che è stimata nel modello. Le medie delle 6 variabili misurate in ciascuno dei tre punti temporali sono assunte non variare in funzione del tempo. Nonostante il modello prenda in considerazione misurazioni ripetute nel tempo, non vi è alcuna specificazione di correlazioni o percorsi causali tra queste misure nel tempo, come sarebbe tipico per i modelli longitudinali. Essendo un modello nullo, non vengono specificate relazioni tra le variabili (varianze e medie) diverse dai loro effetti unici.\n\nfit_null &lt;- lavaan(mod_null, data = dat, orthogonal = TRUE)\n\n\nsummary(fit_null, standardized = T, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 32 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        36\n  Number of equality constraints                    24\n\n  Number of observations                           823\n\nModel Test User Model:\n                                                       \n  Test statistic                              11213.103\n  Degrees of freedom                                177\n  P-value (Chi-square)                            0.000\n\nModel Test Baseline Model:\n\n  Test statistic                             11131.067\n  Degrees of freedom                               153\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.000\n  Tucker-Lewis Index (TLI)                       0.131\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -15975.145\n  Loglikelihood unrestricted model (H1)     -10368.594\n                                                      \n  Akaike (AIC)                               31974.291\n  Bayesian (BIC)                             32030.846\n  Sample-size adjusted Bayesian (SABIC)      31992.739\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.275\n  90 Percent confidence interval - lower         0.271\n  90 Percent confidence interval - upper         0.280\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.328\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    PAT1P1    (T1)    2.960    0.016  190.946    0.000    2.960    3.843\n    PAT1P2    (T2)    2.885    0.015  190.376    0.000    2.885    3.831\n    PAT1P3    (T3)    3.099    0.015  207.418    0.000    3.099    4.174\n    NAT1P1    (T4)    1.708    0.014  123.730    0.000    1.708    2.490\n    NAT1P2    (T5)    1.521    0.013  116.519    0.000    1.521    2.345\n    NAT1P3    (T6)    1.558    0.014  114.474    0.000    1.558    2.304\n    PAT2P1    (T1)    2.960    0.016  190.946    0.000    2.960    3.843\n    PAT2P2    (T2)    2.885    0.015  190.376    0.000    2.885    3.831\n    PAT2P3    (T3)    3.099    0.015  207.418    0.000    3.099    4.174\n    NAT2P1    (T4)    1.708    0.014  123.730    0.000    1.708    2.490\n    NAT2P2    (T5)    1.521    0.013  116.519    0.000    1.521    2.345\n    NAT2P3    (T6)    1.558    0.014  114.474    0.000    1.558    2.304\n    PAT3P1    (T1)    2.960    0.016  190.946    0.000    2.960    3.843\n    PAT3P2    (T2)    2.885    0.015  190.376    0.000    2.885    3.831\n    PAT3P3    (T3)    3.099    0.015  207.418    0.000    3.099    4.174\n    NAT3P1    (T4)    1.708    0.014  123.730    0.000    1.708    2.490\n    NAT3P2    (T5)    1.521    0.013  116.519    0.000    1.521    2.345\n    NAT3P3    (T6)    1.558    0.014  114.474    0.000    1.558    2.304\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    PAT1P1    (V1)    0.593    0.017   35.135    0.000    0.593    1.000\n    PAT1P2    (V2)    0.567    0.016   35.135    0.000    0.567    1.000\n    PAT1P3    (V3)    0.551    0.016   35.135    0.000    0.551    1.000\n    NAT1P1    (V4)    0.471    0.013   35.135    0.000    0.471    1.000\n    NAT1P2    (V5)    0.421    0.012   35.135    0.000    0.421    1.000\n    NAT1P3    (V6)    0.457    0.013   35.135    0.000    0.457    1.000\n    PAT2P1    (V1)    0.593    0.017   35.135    0.000    0.593    1.000\n    PAT2P2    (V2)    0.567    0.016   35.135    0.000    0.567    1.000\n    PAT2P3    (V3)    0.551    0.016   35.135    0.000    0.551    1.000\n    NAT2P1    (V4)    0.471    0.013   35.135    0.000    0.471    1.000\n    NAT2P2    (V5)    0.421    0.012   35.135    0.000    0.421    1.000\n    NAT2P3    (V6)    0.457    0.013   35.135    0.000    0.457    1.000\n    PAT3P1    (V1)    0.593    0.017   35.135    0.000    0.593    1.000\n    PAT3P2    (V2)    0.567    0.016   35.135    0.000    0.567    1.000\n    PAT3P3    (V3)    0.551    0.016   35.135    0.000    0.551    1.000\n    NAT3P1    (V4)    0.471    0.013   35.135    0.000    0.471    1.000\n    NAT3P2    (V5)    0.421    0.012   35.135    0.000    0.421    1.000\n    NAT3P3    (V6)    0.457    0.013   35.135    0.000    0.457    1.000\n\n\n\n\n\n76.3.2 Modello SEM Iniziale\nNel modello SEM iniziale, Little (2023) definisce sei variabili latenti (Pos1, Pos2, Pos3, Neg1, Neg2, Neg3) che rappresentano costrutti psicologici positivi e negativi misurati in tre diversi momenti temporali. Ogni variabile latente è identificato da tre indicatori (per esempio, Pos1 è identificato da PAT1P1, PAT1P2, PAT1P3), con saturazioni fattoriali L1, L2, L3 che quantificano la relazione tra la variabile latenti e i suoi indicatori. Il modello stima la varianza di ciascuna variabile latente e la covarianza tra variabili latenti diverse. Le medie delle variabili latenti sono impostate a 1, indicando che sono considerate fisse. Il modello include stime per la varianza e la covarianza degli indicatori attraverso il tempo, suggerendo l’esistenza di correlazioni temporali tra gli stessi indicatori misurati in momenti diversi. Ci sono percorsi di regressione che collegano le variabili latenti nel tempo (ad esempio, Pos2 è influenzata da Pos1). Il modello impone alcuni vincoli sulle saturazioni fattoriali e sulle intercette degli indicatori.\nQuesto modello mira a esplorare le relazioni dinamiche e temporali tra variabili latenti, diversamente da un modello di invarianza configurale, che è più orientato alla valutazione della costanza della struttura fattoriale.\n\nSEMmod &lt;- \"\n    Pos1 =~ L1*PAT1P1 + L2*PAT1P2 + L3*PAT1P3\n    Neg1 =~ L4*NAT1P1 + L5*NAT1P2 + L6*NAT1P3\n    Pos2 =~ L1*PAT2P1 + L2*PAT2P2 + L3*PAT2P3\n    Neg2 =~ L4*NAT2P1 + L5*NAT2P2 + L6*NAT2P3\n    Pos3 =~ L1*PAT3P1 + L2*PAT3P2 + L3*PAT3P3\n    Neg3 =~ L4*NAT3P1 + L5*NAT3P2 + L6*NAT3P3\n\n    ## Latent factor variance and covariance (Psi matrix)\n    Pos1 ~~ Pos1 ##Psi 1,1\n    Pos1 ~~ Neg1 ##Psi 1,2\n    Neg1 ~~ Neg1 ##Psi 2,2\n\n    Pos2 ~~ Pos2 ##Psi 3,3\n    Pos2 ~~ Neg2 ##Psi 3,4\n    Neg2 ~~ Neg2 ##Psi 4,4\n\n    Pos3 ~~ Pos3  ##Psi 5,5\n    Pos3 ~~ Neg3  ##Psi 5,6\n    Neg3 ~~ Neg3  ##Psi 6,6\n\n    ## Latent means (Alpha matrix)\n    Pos1 ~ 1\n    Neg1 ~ 1\n    Pos2 ~ 1\n    Neg2 ~ 1\n    Pos3 ~ 1\n    Neg3 ~ 1\n\n    ## indicator resduals var-covar (Theta-Epsilon matrix)\n    ## Time1\n    PAT1P1 ~~ PAT1P1\n    PAT1P1 ~~ PAT2P1\n    PAT1P1 ~~ PAT3P1\n\n    PAT1P2 ~~ PAT1P2\n    PAT1P2 ~~ PAT2P2\n    PAT1P2 ~~ PAT3P2\n\n    PAT1P3 ~~ PAT1P3\n    PAT1P3 ~~ PAT2P3\n    PAT1P3 ~~ PAT3P3\n\n    NAT1P1 ~~ NAT1P1\n    NAT1P1 ~~ NAT2P1\n    NAT1P1 ~~ NAT3P1\n\n    NAT1P2 ~~ NAT1P2\n    NAT1P2 ~~ NAT2P2\n    NAT1P2 ~~ NAT3P2\n\n    NAT1P3 ~~ NAT1P3\n    NAT1P3 ~~ NAT2P3\n    NAT1P3 ~~ NAT3P3\n\n    #Time2\n    PAT2P1 ~~ PAT2P1\n    PAT2P1 ~~ PAT3P1\n\n    PAT2P2 ~~ PAT2P2\n    PAT2P2 ~~ PAT3P2\n\n    PAT2P3 ~~ PAT2P3\n    PAT2P3 ~~ PAT3P3\n\n    NAT2P1 ~~ NAT2P1\n    NAT2P1 ~~ NAT3P1\n\n    NAT2P2 ~~ NAT2P2\n    NAT2P2 ~~ NAT3P2\n\n    NAT2P3 ~~ NAT2P3\n    NAT2P3 ~~ NAT3P3\n\n    ## Time3\n    PAT3P1  ~~ PAT3P1\n    PAT3P2 ~~ PAT3P2\n    PAT3P3 ~~ PAT3P3\n    NAT3P1 ~~ NAT3P1\n    NAT3P2 ~~ NAT3P2\n    NAT3P3 ~~ NAT3P3\n\n    ##Indicator means/intercepts (Tau vector)\n    PAT1P1 ~ t1*1\n    PAT1P2 ~ t2*1\n    PAT1P3 ~ t3*1\n    NAT1P1 ~ t4*1\n    NAT1P2 ~ t5*1\n    NAT1P3 ~ t6*1\n\n    PAT2P1 ~ t1*1\n    PAT2P2 ~ t2*1\n    PAT2P3 ~ t3*1\n    NAT2P1 ~ t4*1\n    NAT2P2 ~ t5*1\n    NAT2P3 ~ t6*1\n\n    PAT3P1 ~ t1*1\n    PAT3P2 ~ t2*1\n    PAT3P3 ~ t3*1\n    NAT3P1 ~ t4*1\n    NAT3P2 ~ t5*1\n    NAT3P3 ~ t6*1\n\n    ##Regression paths here\n    Pos2 ~ Pos1\n    Pos3 ~ Pos1 + Pos2\n    Neg2 ~ Neg1\n    Neg3 ~ Neg1 + Neg2\n\n    ## Constraints\n    L1 == 3 - L2 - L3\n    L4 == 3 - L5 - L6\n    t1 == 0 - t2 - t3\n    t4 == 0 - t5 - t6\n\"\n\n\nfitSEM &lt;- lavaan(SEMmod, data = dat, meanstructure = TRUE)\n\n\nsummary(fitSEM, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 129 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        93\n  Number of equality constraints                    28\n\n  Number of observations                           823\n\nModel Test User Model:\n                                                      \n  Test statistic                               441.520\n  Degrees of freedom                               124\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                             11131.067\n  Degrees of freedom                               153\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.971\n  Tucker-Lewis Index (TLI)                       0.964\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -10589.354\n  Loglikelihood unrestricted model (H1)     -10368.594\n                                                      \n  Akaike (AIC)                               21308.709\n  Bayesian (BIC)                             21615.051\n  Sample-size adjusted Bayesian (SABIC)      21408.635\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.056\n  90 Percent confidence interval - lower         0.050\n  90 Percent confidence interval - upper         0.061\n  P-value H_0: RMSEA &lt;= 0.050                    0.044\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.045\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Pos1 =~                                                               \n    PAT1P1    (L1)    1.040    0.010  103.232    0.000    0.684    0.884\n    PAT1P2    (L2)    0.997    0.010   98.450    0.000    0.655    0.881\n    PAT1P3    (L3)    0.963    0.010   93.311    0.000    0.633    0.856\n  Neg1 =~                                                               \n    NAT1P1    (L4)    0.957    0.012   78.547    0.000    0.553    0.757\n    NAT1P2    (L5)    0.999    0.011   90.776    0.000    0.578    0.891\n    NAT1P3    (L6)    1.044    0.011   94.041    0.000    0.604    0.906\n  Pos2 =~                                                               \n    PAT2P1    (L1)    1.040    0.010  103.232    0.000    0.692    0.910\n    PAT2P2    (L2)    0.997    0.010   98.450    0.000    0.663    0.900\n    PAT2P3    (L3)    0.963    0.010   93.311    0.000    0.641    0.877\n  Neg2 =~                                                               \n    NAT2P1    (L4)    0.957    0.012   78.547    0.000    0.525    0.816\n    NAT2P2    (L5)    0.999    0.011   90.776    0.000    0.548    0.873\n    NAT2P3    (L6)    1.044    0.011   94.041    0.000    0.573    0.890\n  Pos3 =~                                                               \n    PAT3P1    (L1)    1.040    0.010  103.232    0.000    0.699    0.900\n    PAT3P2    (L2)    0.997    0.010   98.450    0.000    0.671    0.864\n    PAT3P3    (L3)    0.963    0.010   93.311    0.000    0.648    0.856\n  Neg3 =~                                                               \n    NAT3P1    (L4)    0.957    0.012   78.547    0.000    0.566    0.837\n    NAT3P2    (L5)    0.999    0.011   90.776    0.000    0.591    0.888\n    NAT3P3    (L6)    1.044    0.011   94.041    0.000    0.617    0.869\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Pos2 ~                                                                \n    Pos1              0.550    0.033   16.667    0.000    0.544    0.544\n  Pos3 ~                                                                \n    Pos1              0.340    0.039    8.822    0.000    0.333    0.333\n    Pos2              0.372    0.038    9.848    0.000    0.368    0.368\n  Neg2 ~                                                                \n    Neg1              0.445    0.033   13.494    0.000    0.468    0.468\n  Neg3 ~                                                                \n    Neg1              0.285    0.038    7.470    0.000    0.279    0.279\n    Neg2              0.408    0.040   10.113    0.000    0.379    0.379\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Pos1 ~~                                                               \n    Neg1             -0.063    0.015   -4.293    0.000   -0.166   -0.166\n .Pos2 ~~                                                               \n   .Neg2             -0.050    0.011   -4.503    0.000   -0.183   -0.183\n .Pos3 ~~                                                               \n   .Neg3             -0.074    0.011   -6.758    0.000   -0.288   -0.288\n .PAT1P1 ~~                                                             \n   .PAT2P1            0.010    0.006    1.630    0.103    0.010    0.089\n   .PAT3P1            0.007    0.007    1.039    0.299    0.007    0.058\n .PAT1P2 ~~                                                             \n   .PAT2P2            0.007    0.006    1.236    0.217    0.007    0.065\n   .PAT3P2            0.013    0.007    1.922    0.055    0.013    0.097\n .PAT1P3 ~~                                                             \n   .PAT2P3            0.012    0.006    1.866    0.062    0.012    0.088\n   .PAT3P3            0.012    0.007    1.723    0.085    0.012    0.081\n .NAT1P1 ~~                                                             \n   .NAT2P1            0.027    0.007    3.603    0.000    0.027    0.151\n   .NAT3P1            0.009    0.007    1.230    0.219    0.009    0.052\n .NAT1P2 ~~                                                             \n   .NAT2P2            0.001    0.005    0.105    0.916    0.001    0.006\n   .NAT3P2            0.006    0.005    1.169    0.242    0.006    0.066\n .NAT1P3 ~~                                                             \n   .NAT2P3           -0.001    0.005   -0.178    0.859   -0.001   -0.011\n   .NAT3P3           -0.008    0.006   -1.410    0.159   -0.008   -0.081\n .PAT2P1 ~~                                                             \n   .PAT3P1           -0.000    0.006   -0.068    0.946   -0.000   -0.004\n .PAT2P2 ~~                                                             \n   .PAT3P2            0.005    0.006    0.776    0.438    0.005    0.039\n .PAT2P3 ~~                                                             \n   .PAT3P3            0.010    0.006    1.538    0.124    0.010    0.072\n .NAT2P1 ~~                                                             \n   .NAT3P1            0.011    0.006    1.810    0.070    0.011    0.081\n .NAT2P2 ~~                                                             \n   .NAT3P2            0.008    0.005    1.621    0.105    0.008    0.088\n .NAT2P3 ~~                                                             \n   .NAT3P3           -0.004    0.006   -0.674    0.500   -0.004   -0.037\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    Pos1              3.000    0.024  124.676    0.000    4.564    4.564\n    Neg1              1.520    0.021   71.493    0.000    2.629    2.629\n   .Pos2              1.361    0.101   13.447    0.000    2.047    2.047\n   .Neg2              0.929    0.053   17.429    0.000    1.693    1.693\n   .Pos3              0.787    0.107    7.373    0.000    1.171    1.171\n   .Neg3              0.558    0.064    8.746    0.000    0.944    0.944\n   .PAT1P1    (t1)   -0.140    0.031   -4.557    0.000   -0.140   -0.181\n   .PAT1P2    (t2)   -0.088    0.031   -2.855    0.004   -0.088   -0.118\n   .PAT1P3    (t3)    0.228    0.031    7.238    0.000    0.228    0.308\n   .NAT1P1    (t4)    0.174    0.021    8.467    0.000    0.174    0.239\n   .NAT1P2    (t5)   -0.068    0.018   -3.706    0.000   -0.068   -0.105\n   .NAT1P3    (t6)   -0.106    0.018   -5.760    0.000   -0.106   -0.159\n   .PAT2P1    (t1)   -0.140    0.031   -4.557    0.000   -0.140   -0.184\n   .PAT2P2    (t2)   -0.088    0.031   -2.855    0.004   -0.088   -0.120\n   .PAT2P3    (t3)    0.228    0.031    7.238    0.000    0.228    0.312\n   .NAT2P1    (t4)    0.174    0.021    8.467    0.000    0.174    0.271\n   .NAT2P2    (t5)   -0.068    0.018   -3.706    0.000   -0.068   -0.108\n   .NAT2P3    (t6)   -0.106    0.018   -5.760    0.000   -0.106   -0.165\n   .PAT3P1    (t1)   -0.140    0.031   -4.557    0.000   -0.140   -0.180\n   .PAT3P2    (t2)   -0.088    0.031   -2.855    0.004   -0.088   -0.114\n   .PAT3P3    (t3)    0.228    0.031    7.238    0.000    0.228    0.301\n   .NAT3P1    (t4)    0.174    0.021    8.467    0.000    0.174    0.258\n   .NAT3P2    (t5)   -0.068    0.018   -3.706    0.000   -0.068   -0.102\n   .NAT3P3    (t6)   -0.106    0.018   -5.760    0.000   -0.106   -0.150\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    Pos1              0.432    0.024   18.356    0.000    1.000    1.000\n    Neg1              0.334    0.018   18.207    0.000    1.000    1.000\n   .Pos2              0.311    0.018   17.618    0.000    0.704    0.704\n   .Neg2              0.235    0.014   17.253    0.000    0.781    0.781\n   .Pos3              0.281    0.017   16.941    0.000    0.621    0.621\n   .Neg3              0.237    0.014   16.908    0.000    0.679    0.679\n   .PAT1P1            0.131    0.010   13.048    0.000    0.131    0.219\n   .PAT1P2            0.124    0.009   13.290    0.000    0.124    0.224\n   .PAT1P3            0.147    0.010   14.824    0.000    0.147    0.268\n   .NAT1P1            0.228    0.013   17.665    0.000    0.228    0.427\n   .NAT1P2            0.086    0.007   11.609    0.000    0.086    0.206\n   .NAT1P3            0.080    0.008   10.338    0.000    0.080    0.180\n   .PAT2P1            0.099    0.008   12.300    0.000    0.099    0.171\n   .PAT2P2            0.104    0.008   13.261    0.000    0.104    0.191\n   .PAT2P3            0.124    0.008   14.911    0.000    0.124    0.231\n   .NAT2P1            0.139    0.009   16.035    0.000    0.139    0.335\n   .NAT2P2            0.094    0.007   13.123    0.000    0.094    0.238\n   .NAT2P3            0.086    0.007   11.805    0.000    0.086    0.208\n   .PAT3P1            0.115    0.010   11.900    0.000    0.115    0.190\n   .PAT3P2            0.153    0.011   14.459    0.000    0.153    0.253\n   .PAT3P3            0.153    0.010   14.904    0.000    0.153    0.267\n   .NAT3P1            0.136    0.009   15.347    0.000    0.136    0.299\n   .NAT3P2            0.094    0.008   12.323    0.000    0.094    0.212\n   .NAT3P3            0.123    0.009   13.525    0.000    0.123    0.244\n\nConstraints:\n                                               |Slack|\n    L1 - (3-L2-L3)                               0.000\n    L4 - (3-L5-L6)                               0.000\n    t1 - (0-t2-t3)                               0.000\n    t4 - (0-t5-t6)                               0.000",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>76</span>  <span class='chapter-title'>Specificare e Interpretare un Modello Longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/05_intro_panel.html#invarianza",
    "href": "chapters/lgm/05_intro_panel.html#invarianza",
    "title": "76  Specificare e Interpretare un Modello Longitudinale",
    "section": "76.4 Invarianza",
    "text": "76.4 Invarianza\nL’invarianza nei modelli SEM panel è fondamentale per garantire che le misurazioni di un costrutto siano comparabili nel tempo. Nei modelli longitudinali, l’invarianza implica che il significato e la struttura di un costrutto rimangano stabili attraverso diverse occasioni di misurazione. Senza l’invarianza, qualsiasi cambiamento osservato potrebbe riflettere variazioni nella misurazione stessa, piuttosto che un cambiamento reale nel costrutto. Testare l’invarianza nei modelli SEM panel consente quindi di distinguere tra cambiamenti reali e artefatti della misurazione, supportando un’interpretazione valida delle traiettorie di sviluppo.\n\n76.4.1 Modello di Invarianza Configurale\nIn un modello di invarianza configurale, ci si aspetta che la struttura fattoriale, cioè il numero di fattori e il pattern di carichi fattoriali, sia la stessa in tutti i gruppi o momenti temporali considerati.\nOgni variabile latente (Pos1, Pos2, Pos3, Neg1, Neg2, Neg3) è misurata da un set specifico di indicatori in ciascuno dei tre momenti temporali. Ad esempio, Pos1 è misurata da PAT1P1, PAT1P2, e PAT1P3. I carichi fattoriali (L1, L2, L3, ecc.) sono specificati separatamente per ogni momento temporale. I vincoli imposti (ad esempio, L1 == 3 - L2 - L3) indicano che ci sono alcune restrizioni nella relazione tra i carichi fattoriali. Questi vincoli sono utilizzati per testare l’uguaglianza dei carichi attraverso i diversi tempi.\nIl modello stima separatamente la varianza di ciascun indicatore e di ciascuna variabile latente in ogni momento temporale. Il modello include covarianze sia tra gli indicatori in diversi momenti temporali sia tra le variabili latenti, suggerendo che esiste una correlazione tra le misurazioni nel tempo. Le medie delle variabili latenti e degli indicatori sono specificate imponendo alcuni vincoli (ad esempio, t1 == 0 - t2 - t3).\nI vincoli imposti sui carichi fattoriali e sulle medie degli indicatori permettono di testare se la struttura fattoriale è consistente nel tempo, che è l’essenza dell’invarianza configurale.\n\nmod_config &lt;- \"\n    Pos1 =~ L1*PAT1P1 + L2*PAT1P2 + L3*PAT1P3\n    Neg1 =~ L4*NAT1P1 + L5*NAT1P2 + L6*NAT1P3\n    Pos2 =~ L7*PAT2P1 + L8*PAT2P2 + L9*PAT2P3\n    Neg2 =~ L10*NAT2P1 + L11*NAT2P2 + L12*NAT2P3\n    Pos3 =~ L13*PAT3P1 + L14*PAT3P2 + L15*PAT3P3\n    Neg3 =~ L16*NAT3P1 + L17*NAT3P2 + L18*NAT3P3\n    L1 == 3 - L2 - L3\n    L4 == 3 - L5 - L6\n    L7 == 3 - L8 - L9\n    L10== 3 - L11- L12\n    L13== 3 - L14- L15\n    L16== 3 - L17- L18\n\n    PAT1P1~~PAT1P1\n    PAT1P2~~PAT1P2\n    PAT1P3~~PAT1P3\n    NAT1P1~~NAT1P1\n    NAT1P2~~NAT1P2\n    NAT1P3~~NAT1P3\n    PAT2P1~~PAT2P1\n    PAT2P2~~PAT2P2\n    PAT2P3~~PAT2P3\n    NAT2P1~~NAT2P1\n    NAT2P2~~NAT2P2\n    NAT2P3~~NAT2P3\n    PAT3P1~~PAT3P1\n    PAT3P2~~PAT3P2\n    PAT3P3~~PAT3P3\n    NAT3P1~~NAT3P1\n    NAT3P2~~NAT3P2\n    NAT3P3~~NAT3P3\n\n    Pos1~~Pos1\n    Neg1~~Neg1\n    Pos2~~Pos2\n    Neg2~~Neg2\n    Pos3~~Pos3\n    Neg3~~Neg3\n\n    PAT1P1~~PAT2P1 + PAT3P1\n    PAT2P1~~PAT3P1\n    PAT1P2~~PAT2P2 + PAT3P2\n    PAT2P2~~PAT3P2\n    PAT1P3~~PAT2P3 + PAT3P3\n    PAT2P3~~PAT3P3\n    NAT1P1~~NAT2P1 + NAT3P1\n    NAT2P1~~NAT3P1\n    NAT1P2~~NAT2P2 + NAT3P2\n    NAT2P2~~NAT3P2\n    NAT1P3~~NAT2P3 + NAT3P3\n    NAT2P3~~NAT3P3\n\n    Pos1~~Pos2 + Pos3 + Neg1 + Neg2 + Neg3\n    Pos2~~Pos3 + Neg1 + Neg2 + Neg3\n    Pos3~~Neg1 + Neg2 + Neg3\n    Neg1~~Neg2 + Neg3\n    Neg2~~Neg3\n\n    Pos1~NA*1\n    Neg1~NA*1\n    Pos2~NA*1\n    Neg2~NA*1\n    Pos3~NA*1\n    Neg3~NA*1\n\n    PAT1P1~t1*1\n    PAT1P2~t2*1\n    PAT1P3~t3*1\n    NAT1P1~t4*1\n    NAT1P2~t5*1\n    NAT1P3~t6*1\n    PAT2P1~t7*1\n    PAT2P2~t8*1\n    PAT2P3~t9*1\n    NAT2P1~t10*1\n    NAT2P2~t11*1\n    NAT2P3~t12*1\n    PAT3P1~t13*1\n    PAT3P2~t14*1\n    PAT3P3~t15*1\n    NAT3P1~t16*1\n    NAT3P2~t17*1\n    NAT3P3~t18*1\n\n    t1 == 0 - t2 - t3\n    t4 == 0 - t5 - t6\n    t7 == 0 - t8 - t9\n    t10== 0 - t11- t12\n    t13== 0 - t14- t15\n    t16== 0 - t17- t18\n\"\n\n\nfit_config &lt;- lavaan(mod_config, data = dat, meanstructure = TRUE)\n\n\nsummary(fit_config, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 160 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        99\n  Number of equality constraints                    12\n\n  Number of observations                           823\n\nModel Test User Model:\n                                                      \n  Test statistic                               352.232\n  Degrees of freedom                               102\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                             11131.067\n  Degrees of freedom                               153\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.977\n  Tucker-Lewis Index (TLI)                       0.966\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -10544.710\n  Loglikelihood unrestricted model (H1)     -10368.594\n                                                      \n  Akaike (AIC)                               21263.420\n  Bayesian (BIC)                             21673.447\n  Sample-size adjusted Bayesian (SABIC)      21397.168\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.055\n  90 Percent confidence interval - lower         0.048\n  90 Percent confidence interval - upper         0.061\n  P-value H_0: RMSEA &lt;= 0.050                    0.108\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.035\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Pos1 =~                                                               \n    PAT1P1    (L1)    1.021    0.018   57.311    0.000    0.672    0.878\n    PAT1P2    (L2)    0.999    0.018   57.022    0.000    0.658    0.882\n    PAT1P3    (L3)    0.980    0.018   54.726    0.000    0.644    0.862\n  Neg1 =~                                                               \n    NAT1P1    (L4)    0.911    0.023   40.011    0.000    0.521    0.740\n    NAT1P2    (L5)    1.032    0.019   53.312    0.000    0.591    0.899\n    NAT1P3    (L6)    1.057    0.020   54.058    0.000    0.605    0.907\n  Pos2 =~                                                               \n    PAT2P1    (L7)    1.031    0.015   67.338    0.000    0.690    0.909\n    PAT2P2    (L8)    1.010    0.015   66.147    0.000    0.676    0.905\n    PAT2P3    (L9)    0.958    0.016   60.988    0.000    0.641    0.877\n  Neg2 =~                                                               \n    NAT2P1   (L10)    0.973    0.020   49.201    0.000    0.537    0.824\n    NAT2P2   (L11)    0.976    0.019   52.312    0.000    0.538    0.866\n    NAT2P3   (L12)    1.052    0.019   55.900    0.000    0.581    0.894\n  Pos3 =~                                                               \n    PAT3P1   (L13)    1.065    0.018   60.101    0.000    0.709    0.907\n    PAT3P2   (L14)    0.981    0.018   53.971    0.000    0.653    0.857\n    PAT3P3   (L15)    0.954    0.018   52.524    0.000    0.636    0.849\n  Neg3 =~                                                               \n    NAT3P1   (L16)    0.994    0.019   52.539    0.000    0.586    0.852\n    NAT3P2   (L17)    0.989    0.018   54.815    0.000    0.583    0.884\n    NAT3P3   (L18)    1.017    0.019   53.509    0.000    0.600    0.861\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .PAT1P1 ~~                                                             \n   .PAT2P1            0.011    0.006    1.701    0.089    0.011    0.091\n   .PAT3P1            0.008    0.007    1.133    0.257    0.008    0.064\n .PAT2P1 ~~                                                             \n   .PAT3P1           -0.000    0.006   -0.039    0.969   -0.000   -0.002\n .PAT1P2 ~~                                                             \n   .PAT2P2            0.007    0.006    1.251    0.211    0.007    0.066\n   .PAT3P2            0.012    0.007    1.782    0.075    0.012    0.089\n .PAT2P2 ~~                                                             \n   .PAT3P2            0.005    0.006    0.732    0.464    0.005    0.037\n .PAT1P3 ~~                                                             \n   .PAT2P3            0.011    0.006    1.740    0.082    0.011    0.083\n   .PAT3P3            0.013    0.007    1.851    0.064    0.013    0.087\n .PAT2P3 ~~                                                             \n   .PAT3P3            0.010    0.006    1.524    0.127    0.010    0.071\n .NAT1P1 ~~                                                             \n   .NAT2P1            0.029    0.007    3.893    0.000    0.029    0.163\n   .NAT3P1            0.010    0.007    1.412    0.158    0.010    0.061\n .NAT2P1 ~~                                                             \n   .NAT3P1            0.009    0.006    1.506    0.132    0.009    0.069\n .NAT1P2 ~~                                                             \n   .NAT2P2            0.000    0.005    0.006    0.996    0.000    0.000\n   .NAT3P2            0.005    0.005    1.009    0.313    0.005    0.058\n .NAT2P2 ~~                                                             \n   .NAT3P2            0.008    0.005    1.660    0.097    0.008    0.087\n .NAT1P3 ~~                                                             \n   .NAT2P3            0.000    0.005    0.002    0.998    0.000    0.000\n   .NAT3P3           -0.006    0.006   -1.016    0.310   -0.006   -0.057\n .NAT2P3 ~~                                                             \n   .NAT3P3           -0.004    0.006   -0.729    0.466   -0.004   -0.039\n  Pos1 ~~                                                               \n    Pos2              0.243    0.019   12.892    0.000    0.552    0.552\n    Pos3              0.230    0.019   12.278    0.000    0.525    0.525\n    Neg1             -0.062    0.015   -4.217    0.000   -0.164   -0.164\n    Neg2             -0.059    0.014   -4.149    0.000   -0.162   -0.162\n    Neg3             -0.014    0.015   -0.913    0.361   -0.035   -0.035\n  Pos2 ~~                                                               \n    Pos3              0.241    0.019   12.725    0.000    0.542    0.542\n  Neg1 ~~                                                               \n    Pos2             -0.058    0.015   -3.963    0.000   -0.152   -0.152\n  Pos2 ~~                                                               \n    Neg2             -0.090    0.014   -6.254    0.000   -0.244   -0.244\n    Neg3             -0.028    0.015   -1.851    0.064   -0.071   -0.071\n  Neg1 ~~                                                               \n    Pos3             -0.010    0.015   -0.717    0.473   -0.027   -0.027\n  Neg2 ~~                                                               \n    Pos3             -0.047    0.014   -3.296    0.001   -0.128   -0.128\n  Pos3 ~~                                                               \n    Neg3             -0.078    0.015   -5.087    0.000   -0.199   -0.199\n  Neg1 ~~                                                               \n    Neg2              0.149    0.013   11.184    0.000    0.472    0.472\n    Neg3              0.149    0.014   10.586    0.000    0.441    0.441\n  Neg2 ~~                                                               \n    Neg3              0.167    0.014   11.998    0.000    0.514    0.514\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    Pos1              3.000    0.024  124.563    0.000    4.560    4.560\n    Neg1              1.537    0.021   72.416    0.000    2.684    2.684\n    Pos2              3.012    0.024  124.258    0.000    4.503    4.503\n    Neg2              1.604    0.020   78.945    0.000    2.907    2.907\n    Pos3              2.931    0.024  120.101    0.000    4.401    4.401\n    Neg3              1.647    0.022   75.997    0.000    2.793    2.793\n   .PAT1P1    (t1)   -0.071    0.054   -1.303    0.192   -0.071   -0.093\n   .PAT1P2    (t2)   -0.103    0.054   -1.916    0.055   -0.103   -0.138\n   .PAT1P3    (t3)    0.174    0.055    3.171    0.002    0.174    0.232\n   .NAT1P1    (t4)    0.307    0.037    8.291    0.000    0.307    0.436\n   .NAT1P2    (t5)   -0.136    0.031   -4.349    0.000   -0.136   -0.206\n   .NAT1P3    (t6)   -0.172    0.031   -5.455    0.000   -0.172   -0.257\n   .PAT2P1    (t7)   -0.106    0.047   -2.243    0.025   -0.106   -0.139\n   .PAT2P2    (t8)   -0.134    0.047   -2.860    0.004   -0.134   -0.180\n   .PAT2P3    (t9)    0.240    0.048    4.960    0.000    0.240    0.328\n   .NAT2P1   (t10)    0.135    0.033    4.058    0.000    0.135    0.207\n   .NAT2P2   (t11)   -0.027    0.031   -0.874    0.382   -0.027   -0.044\n   .NAT2P3   (t12)   -0.108    0.031   -3.419    0.001   -0.108   -0.166\n   .PAT3P1   (t13)   -0.234    0.053   -4.418    0.000   -0.234   -0.299\n   .PAT3P2   (t14)   -0.026    0.054   -0.481    0.631   -0.026   -0.034\n   .PAT3P3   (t15)    0.260    0.054    4.779    0.000    0.260    0.347\n   .NAT3P1   (t16)    0.087    0.033    2.663    0.008    0.087    0.127\n   .NAT3P2   (t17)   -0.054    0.031   -1.724    0.085   -0.054   -0.081\n   .NAT3P3   (t18)   -0.033    0.033   -1.019    0.308   -0.033   -0.048\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .PAT1P1            0.133    0.010   12.941    0.000    0.133    0.228\n   .PAT1P2            0.123    0.010   12.691    0.000    0.123    0.222\n   .PAT1P3            0.144    0.010   14.093    0.000    0.144    0.257\n   .NAT1P1            0.224    0.013   17.831    0.000    0.224    0.452\n   .NAT1P2            0.083    0.008   10.067    0.000    0.083    0.193\n   .NAT1P3            0.078    0.008    9.243    0.000    0.078    0.176\n   .PAT2P1            0.100    0.008   12.156    0.000    0.100    0.174\n   .PAT2P2            0.102    0.008   12.582    0.000    0.102    0.182\n   .PAT2P3            0.124    0.008   14.727    0.000    0.124    0.231\n   .NAT2P1            0.137    0.009   15.465    0.000    0.137    0.322\n   .NAT2P2            0.097    0.007   13.132    0.000    0.097    0.251\n   .NAT2P3            0.084    0.008   10.906    0.000    0.084    0.200\n   .PAT3P1            0.109    0.010   10.536    0.000    0.109    0.178\n   .PAT3P2            0.154    0.011   14.284    0.000    0.154    0.265\n   .PAT3P3            0.156    0.011   14.776    0.000    0.156    0.279\n   .NAT3P1            0.129    0.009   14.221    0.000    0.129    0.274\n   .NAT3P2            0.095    0.008   12.052    0.000    0.095    0.218\n   .NAT3P3            0.125    0.009   13.536    0.000    0.125    0.258\n    Pos1              0.433    0.024   18.354    0.000    1.000    1.000\n    Neg1              0.328    0.018   17.933    0.000    1.000    1.000\n    Pos2              0.448    0.024   18.745    0.000    1.000    1.000\n    Neg2              0.305    0.017   18.153    0.000    1.000    1.000\n    Pos3              0.444    0.024   18.323    0.000    1.000    1.000\n    Neg3              0.348    0.019   18.211    0.000    1.000    1.000\n\nConstraints:\n                                               |Slack|\n    L1 - (3-L2-L3)                               0.000\n    L4 - (3-L5-L6)                               0.000\n    L7 - (3-L8-L9)                               0.000\n    L10 - (3-L11-L12)                            0.000\n    L13 - (3-L14-L15)                            0.000\n    L16 - (3-L17-L18)                            0.000\n    t1 - (0-t2-t3)                               0.000\n    t4 - (0-t5-t6)                               0.000\n    t7 - (0-t8-t9)                               0.000\n    t10 - (0-t11-t12)                            0.000\n    t13 - (0-t14-t15)                            0.000\n    t16 - (0-t17-t18)                            0.000\n\n\n\n\n\n76.4.2 Modello di Invarianza Debole\nIl modello di invarianza debole, o invarianza metrica, è un passo oltre l’invarianza configurale nella SEM per testare l’uguaglianza di costrutti psicologici nel tempo. Mentre l’invarianza configurale si concentra sulla struttura fattoriale (cioè, la presenza e il pattern dei carichi fattoriali), l’invarianza debole agginge il vincolo dell’uguaglianza dei carichi fattoriali nei diversi momenti temporali.\nNel modello successivo, i carichi fattoriali per gli indicatori corrispondenti sono mantenuti costanti nelle tre rilevazioni temporali. Ad esempio, lo stesso valore per L1 è utilizzato per PAT1P1, PAT2P1 e PAT3P1 in tutti e tre i momenti temporali. Questo significa che questo modello verifica se la relazione tra le variabili latenti (Pos e Neg) e i loro indicatori (PAT e NAT) è la stessa nel tempo.\nIl modello stima separatamente la varianza di ciascun indicatore e di ciascuna variabile latente in ogni momento temporale. Questo è simile all’invarianza configurale.\nIl modello include covarianze sia tra gli indicatori in diversi momenti temporali sia tra le variabili latenti, permettendo di catturare la correlazione tra le misurazioni nel tempo.\nViene fatta un’ipotesi sulle medie degli indicatori, come mostrato nelle equazioni PAT1P1 ~ t1*1, ecc. I vincoli sulle medie degli indicatori (ad esempio, t1 == 0 - t2 - t3) suggeriscono che ci sono alcune restrizioni matematiche imposte sulle medie degli indicatori. Prendendo l’equazione t1 == 0 - t2 - t3 come esempio, questa impone una relazione diretta tra tre medie degli indicatori. In pratica, afferma che la media di un indicatore (rappresentata da t1) è definita come l’opposto della somma delle medie di altri due indicatori (t2 e t3). Questo tipo di vincolo può essere interpretato come un meccanismo di bilanciamento. Se t2 e t3 aumentano, allora t1 diminuisce di conseguenza, mantenendo una relazione bilanciata tra queste tre medie.\nMentre l’invarianza configurale richiede solo che la stessa struttura fattoriale sia presente attraverso i gruppi o nel tempo (ad esempio, gli stessi fattori con gli stessi indicatori), l’invarianza debole richiede anche che i carichi fattoriali siano gli stessi. Questo è un test più rigoroso dell’invarianza poiché non solo assume che le stesse variabili latenti siano misurate, ma anche che la forza della relazione tra le variabili latenti e i loro indicatori sia costante.\n\nmod_weak &lt;- \"\n    Pos1 =~ L1*PAT1P1 + L2*PAT1P2 + L3*PAT1P3\n    Neg1 =~ L4*NAT1P1 + L5*NAT1P2 + L6*NAT1P3\n    Pos2 =~ L1*PAT2P1 + L2*PAT2P2 + L3*PAT2P3\n    Neg2 =~ L4*NAT2P1 + L5*NAT2P2 + L6*NAT2P3\n    Pos3 =~ L1*PAT3P1 + L2*PAT3P2 + L3*PAT3P3\n    Neg3 =~ L4*NAT3P1 + L5*NAT3P2 + L6*NAT3P3\n\n    ## indicator residual variances (Theta-Epsilon matrix)\n    PAT1P1 ~~ PAT1P1\n    PAT1P2 ~~ PAT1P2\n    PAT1P3 ~~ PAT1P3\n    NAT1P1 ~~ NAT1P1\n    NAT1P2 ~~ NAT1P2\n    NAT1P3 ~~ NAT1P3\n    PAT2P1 ~~ PAT2P1\n    PAT2P2 ~~ PAT2P2\n    PAT2P3 ~~ PAT2P3\n    NAT2P1 ~~ NAT2P1\n    NAT2P2 ~~ NAT2P2\n    NAT2P3 ~~ NAT2P3\n    PAT3P1 ~~ PAT3P1\n    PAT3P2 ~~ PAT3P2\n    PAT3P3 ~~ PAT3P3\n    NAT3P1 ~~ NAT3P1\n    NAT3P2 ~~ NAT3P2\n    NAT3P3 ~~ NAT3P3\n\n    ### cross-time correlated residuals\n    ## correlate residuals of indicators with themselves across time\n    PAT1P1 ~~ PAT2P1 + PAT3P1\n    PAT2P1 ~~ PAT3P1\n    PAT1P2 ~~ PAT2P2 + PAT3P2\n    PAT2P2 ~~ PAT3P2\n    PAT1P3 ~~ PAT2P3 + PAT3P3\n    PAT2P3 ~~ PAT3P3\n    NAT1P1 ~~ NAT2P1 + NAT3P1\n    NAT2P1 ~~ NAT3P1\n    NAT1P2 ~~ NAT2P2 + NAT3P2\n    NAT2P2 ~~ NAT3P2\n    NAT1P3 ~~ NAT2P3 + NAT3P3\n    NAT2P3 ~~ NAT3P3\n\n    ## indicator intercepts (Tau vector), include labels for model constraints\n    PAT1P1 ~ t1*1\n    PAT1P2 ~ t2*1\n    PAT1P3 ~ t3*1\n    NAT1P1 ~ t4*1\n    NAT1P2 ~ t5*1\n    NAT1P3 ~ t6*1\n\n    PAT2P1 ~ t7*1\n    PAT2P2 ~ t8*1\n    PAT2P3 ~ t9*1\n    NAT2P1 ~ t10*1\n    NAT2P2 ~ t11*1\n    NAT2P3 ~ t12*1\n\n    PAT3P1 ~ t13*1\n    PAT3P2 ~ t14*1\n    PAT3P3 ~ t15*1\n    NAT3P1 ~ t16*1\n    NAT3P2 ~ t17*1\n    NAT3P3 ~ t18*1\n\n    ### latent factor variance (Psi matrix)\n    Pos1 ~~ Pos1\n    Neg1 ~~ Neg1\n    Pos2 ~~ Pos2\n    Neg2 ~~ Neg2\n    Pos3 ~~ Pos3\n    Neg3 ~~ Neg3\n\n    ### factor covariance\n    Pos1 ~~ Pos2 + Pos3 + Neg1 + Neg2 + Neg3\n    Pos2 ~~ Pos3 + Neg1 + Neg2 + Neg3\n    Pos3 ~~ Neg1 + Neg2 + Neg3\n    Neg1 ~~ Neg2 + Neg3\n    Neg2 ~~ Neg3\n\n    ## latent means (Alpha matrix)\n    Pos1 ~ 1\n    Neg1 ~ 1\n    Pos2 ~ 1\n    Neg2 ~ 1\n    Pos3 ~ 1\n    Neg3 ~ 1\n\n    ## model constraints\n    L1 == 3 - L2 - L3\n    L4 == 3 - L5 - L6\n    t1 == 0 - t2 - t3\n    t4 == 0 - t5 - t6\n    t7 == 0 - t8 - t9\n    t10 == 0 - t11 - t12\n    t13 == 0 - t14 - t15\n    t16 == 0 - t17 - t18\n\"\n\n\nfit_wk &lt;- lavaan(mod_weak, data = dat, meanstructure = TRUE)\n#### Did not converge on first run, used final estimates on starting values for next run\nfit_weak &lt;- lavaan(mod_weak, data = dat, meanstructure = TRUE, start = fit_wk)\n\n\nsummary(fit_weak, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 4 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        99\n  Number of equality constraints                    20\n\n  Number of observations                           823\n\nModel Test User Model:\n                                                      \n  Test statistic                               366.562\n  Degrees of freedom                               110\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                             11131.067\n  Degrees of freedom                               153\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.977\n  Tucker-Lewis Index (TLI)                       0.967\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -10551.875\n  Loglikelihood unrestricted model (H1)     -10368.594\n                                                      \n  Akaike (AIC)                               21261.750\n  Bayesian (BIC)                             21634.074\n  Sample-size adjusted Bayesian (SABIC)      21383.200\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.053\n  90 Percent confidence interval - lower         0.047\n  90 Percent confidence interval - upper         0.059\n  P-value H_0: RMSEA &lt;= 0.050                    0.182\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.035\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Pos1 =~                                                               \n    PAT1P1    (L1)    1.038    0.010  102.905    0.000    0.683    0.884\n    PAT1P2    (L2)    0.998    0.010   98.574    0.000    0.656    0.881\n    PAT1P3    (L3)    0.963    0.010   93.200    0.000    0.633    0.856\n  Neg1 =~                                                               \n    NAT1P1    (L4)    0.964    0.012   79.273    0.000    0.558    0.766\n    NAT1P2    (L5)    0.997    0.011   90.664    0.000    0.578    0.891\n    NAT1P3    (L6)    1.039    0.011   93.757    0.000    0.602    0.906\n  Pos2 =~                                                               \n    PAT2P1    (L1)    1.038    0.010  102.905    0.000    0.695    0.911\n    PAT2P2    (L2)    0.998    0.010   98.574    0.000    0.668    0.901\n    PAT2P3    (L3)    0.963    0.010   93.200    0.000    0.645    0.878\n  Neg2 =~                                                               \n    NAT2P1    (L4)    0.964    0.012   79.273    0.000    0.532    0.820\n    NAT2P2    (L5)    0.997    0.011   90.664    0.000    0.550    0.873\n    NAT2P3    (L6)    1.039    0.011   93.757    0.000    0.574    0.889\n  Pos3 =~                                                               \n    PAT3P1    (L1)    1.038    0.010  102.905    0.000    0.693    0.898\n    PAT3P2    (L2)    0.998    0.010   98.574    0.000    0.666    0.864\n    PAT3P3    (L3)    0.963    0.010   93.200    0.000    0.643    0.853\n  Neg3 =~                                                               \n    NAT3P1    (L4)    0.964    0.012   79.273    0.000    0.567    0.840\n    NAT3P2    (L5)    0.997    0.011   90.664    0.000    0.587    0.886\n    NAT3P3    (L6)    1.039    0.011   93.757    0.000    0.612    0.868\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .PAT1P1 ~~                                                             \n   .PAT2P1            0.010    0.006    1.643    0.100    0.010    0.090\n   .PAT3P1            0.008    0.007    1.171    0.242    0.008    0.065\n .PAT2P1 ~~                                                             \n   .PAT3P1            0.000    0.006    0.046    0.963    0.000    0.003\n .PAT1P2 ~~                                                             \n   .PAT2P2            0.008    0.006    1.304    0.192    0.008    0.068\n   .PAT3P2            0.012    0.007    1.761    0.078    0.012    0.089\n .PAT2P2 ~~                                                             \n   .PAT3P2            0.005    0.006    0.796    0.426    0.005    0.040\n .PAT1P3 ~~                                                             \n   .PAT2P3            0.011    0.006    1.784    0.074    0.011    0.084\n   .PAT3P3            0.013    0.007    1.780    0.075    0.013    0.084\n .PAT2P3 ~~                                                             \n   .PAT3P3            0.010    0.006    1.482    0.138    0.010    0.069\n .NAT1P1 ~~                                                             \n   .NAT2P1            0.029    0.007    3.931    0.000    0.029    0.166\n   .NAT3P1            0.011    0.007    1.474    0.141    0.011    0.063\n .NAT2P1 ~~                                                             \n   .NAT3P1            0.010    0.006    1.625    0.104    0.010    0.073\n .NAT1P2 ~~                                                             \n   .NAT2P2           -0.000    0.005   -0.038    0.970   -0.000   -0.002\n   .NAT3P2            0.005    0.005    1.052    0.293    0.005    0.059\n .NAT2P2 ~~                                                             \n   .NAT3P2            0.008    0.005    1.654    0.098    0.008    0.089\n .NAT1P3 ~~                                                             \n   .NAT2P3           -0.000    0.005   -0.079    0.937   -0.000   -0.005\n   .NAT3P3           -0.006    0.006   -1.103    0.270   -0.006   -0.063\n .NAT2P3 ~~                                                             \n   .NAT3P3           -0.004    0.006   -0.780    0.435   -0.004   -0.042\n  Pos1 ~~                                                               \n    Pos2              0.243    0.019   12.903    0.000    0.552    0.552\n    Pos3              0.231    0.019   12.304    0.000    0.526    0.526\n    Neg1             -0.062    0.015   -4.210    0.000   -0.163   -0.163\n    Neg2             -0.059    0.014   -4.163    0.000   -0.162   -0.162\n    Neg3             -0.014    0.015   -0.921    0.357   -0.036   -0.036\n  Pos2 ~~                                                               \n    Pos3              0.243    0.019   12.757    0.000    0.544    0.544\n  Neg1 ~~                                                               \n    Pos2             -0.059    0.015   -3.979    0.000   -0.153   -0.153\n  Pos2 ~~                                                               \n    Neg2             -0.091    0.014   -6.280    0.000   -0.246   -0.246\n    Neg3             -0.028    0.015   -1.875    0.061   -0.072   -0.072\n  Neg1 ~~                                                               \n    Pos3             -0.011    0.015   -0.731    0.465   -0.028   -0.028\n  Neg2 ~~                                                               \n    Pos3             -0.047    0.014   -3.305    0.001   -0.128   -0.128\n  Pos3 ~~                                                               \n    Neg3             -0.079    0.015   -5.118    0.000   -0.200   -0.200\n  Neg1 ~~                                                               \n    Neg2              0.152    0.014   11.288    0.000    0.477    0.477\n    Neg3              0.151    0.014   10.634    0.000    0.443    0.443\n  Neg2 ~~                                                               \n    Neg3              0.167    0.014   12.005    0.000    0.514    0.514\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .PAT1P1    (t1)   -0.123    0.032   -3.855    0.000   -0.123   -0.160\n   .PAT1P2    (t2)   -0.100    0.032   -3.107    0.002   -0.100   -0.134\n   .PAT1P3    (t3)    0.223    0.033    6.799    0.000    0.223    0.301\n   .NAT1P1    (t4)    0.225    0.022   10.149    0.000    0.225    0.309\n   .NAT1P2    (t5)   -0.081    0.019   -4.220    0.000   -0.081   -0.126\n   .NAT1P3    (t6)   -0.144    0.019   -7.423    0.000   -0.144   -0.217\n   .PAT2P1    (t7)   -0.127    0.032   -3.989    0.000   -0.127   -0.166\n   .PAT2P2    (t8)   -0.099    0.032   -3.094    0.002   -0.099   -0.133\n   .PAT2P3    (t9)    0.225    0.033    6.910    0.000    0.225    0.307\n   .NAT2P1   (t10)    0.149    0.022    6.793    0.000    0.149    0.230\n   .NAT2P2   (t11)   -0.061    0.020   -3.101    0.002   -0.061   -0.098\n   .NAT2P3   (t12)   -0.087    0.020   -4.399    0.000   -0.087   -0.136\n   .PAT3P1   (t13)   -0.157    0.031   -5.018    0.000   -0.157   -0.203\n   .PAT3P2   (t14)   -0.077    0.032   -2.426    0.015   -0.077   -0.099\n   .PAT3P3   (t15)    0.234    0.032    7.256    0.000    0.234    0.310\n   .NAT3P1   (t16)    0.136    0.022    6.076    0.000    0.136    0.202\n   .NAT3P2   (t17)   -0.066    0.020   -3.251    0.001   -0.066   -0.100\n   .NAT3P3   (t18)   -0.070    0.021   -3.384    0.001   -0.070   -0.100\n    Pos1              3.000    0.024  124.639    0.000    4.563    4.563\n    Neg1              1.537    0.021   71.637    0.000    2.652    2.652\n    Pos2              3.012    0.024  124.219    0.000    4.502    4.502\n    Neg2              1.604    0.020   78.947    0.000    2.908    2.908\n    Pos3              2.931    0.024  119.853    0.000    4.391    4.391\n    Neg3              1.647    0.022   76.092    0.000    2.797    2.797\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .PAT1P1            0.130    0.010   13.072    0.000    0.130    0.219\n   .PAT1P2            0.124    0.009   13.267    0.000    0.124    0.223\n   .PAT1P3            0.147    0.010   14.837    0.000    0.147    0.268\n   .NAT1P1            0.220    0.013   17.540    0.000    0.220    0.414\n   .NAT1P2            0.087    0.007   11.767    0.000    0.087    0.207\n   .NAT1P3            0.079    0.008   10.385    0.000    0.079    0.179\n   .PAT2P1            0.099    0.008   12.350    0.000    0.099    0.170\n   .PAT2P2            0.104    0.008   13.257    0.000    0.104    0.188\n   .PAT2P3            0.123    0.008   14.898    0.000    0.123    0.229\n   .NAT2P1            0.138    0.009   15.933    0.000    0.138    0.327\n   .NAT2P2            0.094    0.007   13.203    0.000    0.094    0.237\n   .NAT2P3            0.087    0.007   11.961    0.000    0.087    0.209\n   .PAT3P1            0.115    0.010   11.939    0.000    0.115    0.193\n   .PAT3P2            0.150    0.010   14.356    0.000    0.150    0.253\n   .PAT3P3            0.154    0.010   14.973    0.000    0.154    0.272\n   .NAT3P1            0.134    0.009   15.181    0.000    0.134    0.294\n   .NAT3P2            0.094    0.008   12.427    0.000    0.094    0.215\n   .NAT3P3            0.123    0.009   13.604    0.000    0.123    0.247\n    Pos1              0.432    0.024   18.358    0.000    1.000    1.000\n    Neg1              0.336    0.018   18.230    0.000    1.000    1.000\n    Pos2              0.448    0.024   18.754    0.000    1.000    1.000\n    Neg2              0.304    0.017   18.188    0.000    1.000    1.000\n    Pos3              0.446    0.024   18.349    0.000    1.000    1.000\n    Neg3              0.347    0.019   18.230    0.000    1.000    1.000\n\nConstraints:\n                                               |Slack|\n    L1 - (3-L2-L3)                               0.000\n    L4 - (3-L5-L6)                               0.000\n    t1 - (0-t2-t3)                               0.000\n    t4 - (0-t5-t6)                               0.000\n    t7 - (0-t8-t9)                               0.000\n    t10 - (0-t11-t12)                            0.000\n    t13 - (0-t14-t15)                            0.000\n    t16 - (0-t17-t18)                            0.000\n\n\n\n\n\n76.4.3 Modello di Invarianza Forte\nIl modello di invarianza forte è un passaggio ulteriore nell’analisi dell’invarianza in un contesto di modellazione SEM longitudinale. Mentre l’invarianza configurale si concentra sulla struttura fattoriale e l’invarianza debole aggiunge l’uguaglianza dei carichi fattoriali, l’invarianza forte va oltre per includere anche l’uguaglianza delle medie degli indicatori.\nCome nei modelli di invarianza debole, i carichi fattoriali (L1, L2, L3, L4, L5, L6) sono mantenuti uguali attraverso i diversi momenti temporali, indicando che la forza della relazione tra le variabili latenti e i loro indicatori è costante.\nIl modello impone che le medie degli indicatori siano uguali attraverso i diversi momenti temporali. Questo è indicato dalle equazioni come PAT1P1 ~ t1*1, PAT2P1 ~ t1*1, e PAT3P1 ~ t1*1, dove t1 è lo stesso in tutti e tre i momenti temporali.\nIl modello continua a stimare separatamente la varianza degli indicatori e la covarianza sia tra gli indicatori in diversi momenti temporali sia tra le variabili latenti.\nSono imposti alcuni vincoli, come t1 == 0 - t2 - t3, che servono a identificare il modello e riflettono ipotesi teoriche sulle relazioni tra gli indicatori.\nL’invarianza forte è fondamentale per garantire che le misure di un costrutto siano completamente comparabili nel tempo o tra i gruppi. Se un modello dimostra invarianza forte, significa che non solo la relazione tra le variabili latenti e i loro indicatori è costante, ma anche che il livello di base di ciascun indicatore è lo stesso. Questo è cruciale per confronti delle medie latenti o per esaminare i cambiamenti nel tempo.\n\nmod_strong &lt;- \"\n    ### loadings\n    Pos1 =~ L1*PAT1P1 + L2*PAT1P2 + L3*PAT1P3\n    Neg1 =~ L4*NAT1P1 + L5*NAT1P2 + L6*NAT1P3\n    Pos2 =~ L1*PAT2P1 + L2*PAT2P2 + L3*PAT2P3\n    Neg2 =~ L4*NAT2P1 + L5*NAT2P2 + L6*NAT2P3\n    Pos3 =~ L1*PAT3P1 + L2*PAT3P2 + L3*PAT3P3\n    Neg3 =~ L4*NAT3P1 + L5*NAT3P2 + L6*NAT3P3\n    L1 == 3 - L2 - L3\n    L4 == 3 - L5 - L6\n\n    ### factor variance\n    Pos1 ~~ Pos1\n    Neg1 ~~ Neg1\n    Pos2 ~~ Pos2\n    Neg2 ~~ Neg2\n    Pos3 ~~ Pos3\n    Neg3 ~~ Neg3\n\n    ### factor covariance\n    Pos1 ~~ Pos2 + Pos3 + Neg1 + Neg2 + Neg3\n    Pos2 ~~ Pos3 + Neg1 + Neg2 + Neg3\n    Pos3 ~~ Neg1 + Neg2 + Neg3\n    Neg1 ~~ Neg2 + Neg3\n    Neg2 ~~ Neg3\n\n    ### residual variance\n    PAT1P1 ~~ PAT1P1\n    PAT1P2 ~~ PAT1P2\n    PAT1P3 ~~ PAT1P3\n    NAT1P1 ~~ NAT1P1\n    NAT1P2 ~~ NAT1P2\n    NAT1P3 ~~ NAT1P3\n    PAT2P1 ~~ PAT2P1\n    PAT2P2 ~~ PAT2P2\n    PAT2P3 ~~ PAT2P3\n    NAT2P1 ~~ NAT2P1\n    NAT2P2 ~~ NAT2P2\n    NAT2P3 ~~ NAT2P3\n    PAT3P1 ~~ PAT3P1\n    PAT3P2 ~~ PAT3P2\n    PAT3P3 ~~ PAT3P3\n    NAT3P1 ~~ NAT3P1\n    NAT3P2 ~~ NAT3P2\n    NAT3P3 ~~ NAT3P3\n\n    ### cross-time correlated residuals\n    PAT1P1 ~~ PAT2P1 + PAT3P1\n    PAT2P1 ~~ PAT3P1\n    PAT1P2 ~~ PAT2P2 + PAT3P2\n    PAT2P2 ~~ PAT3P2\n    PAT1P3 ~~ PAT2P3 + PAT3P3\n    PAT2P3 ~~ PAT3P3\n    NAT1P1 ~~ NAT2P1 + NAT3P1\n    NAT2P1 ~~ NAT3P1\n    NAT1P2 ~~ NAT2P2 + NAT3P2\n    NAT2P2 ~~ NAT3P2\n    NAT1P3 ~~ NAT2P3 + NAT3P3\n    NAT2P3 ~~ NAT3P3\n\n    ## latent mean\n    Pos1 ~ 1\n    Neg1 ~ 1\n    Pos2 ~ 1\n    Neg2 ~ 1\n    Pos3 ~ 1\n    Neg3 ~ 1\n\n    ## intercept\n    PAT1P1 ~ t1*1\n    PAT1P2 ~ t2*1\n    PAT1P3 ~ t3*1\n    NAT1P1 ~ t4*1\n    NAT1P2 ~ t5*1\n    NAT1P3 ~ t6*1\n    PAT2P1 ~ t1*1\n    PAT2P2 ~ t2*1\n    PAT2P3 ~ t3*1\n    NAT2P1 ~ t4*1\n    NAT2P2 ~ t5*1\n    NAT2P3 ~ t6*1\n    PAT3P1 ~ t1*1\n    PAT3P2 ~ t2*1\n    PAT3P3 ~ t3*1\n    NAT3P1 ~ t4*1\n    NAT3P2 ~ t5*1\n    NAT3P3 ~ t6*1\n\n    t1 == 0 - t2 - t3\n    t4 == 0 - t5 - t6\n    \"\n\n\nfit_strong &lt;- lavaan(mod_strong, data = dat, meanstructure = TRUE)\n\n\nsummary(fit_strong, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 148 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        99\n  Number of equality constraints                    28\n\n  Number of observations                           823\n\nModel Test User Model:\n                                                      \n  Test statistic                               418.737\n  Degrees of freedom                               118\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                             11131.067\n  Degrees of freedom                               153\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.973\n  Tucker-Lewis Index (TLI)                       0.964\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -10577.963\n  Loglikelihood unrestricted model (H1)     -10368.594\n                                                      \n  Akaike (AIC)                               21297.925\n  Bayesian (BIC)                             21632.545\n  Sample-size adjusted Bayesian (SABIC)      21407.076\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.056\n  90 Percent confidence interval - lower         0.050\n  90 Percent confidence interval - upper         0.061\n  P-value H_0: RMSEA &lt;= 0.050                    0.052\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.037\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Pos1 =~                                                               \n    PAT1P1    (L1)    1.040    0.010  103.200    0.000    0.684    0.884\n    PAT1P2    (L2)    0.997    0.010   98.587    0.000    0.656    0.881\n    PAT1P3    (L3)    0.963    0.010   93.301    0.000    0.633    0.856\n  Neg1 =~                                                               \n    NAT1P1    (L4)    0.957    0.012   78.488    0.000    0.553    0.757\n    NAT1P2    (L5)    0.999    0.011   90.830    0.000    0.578    0.891\n    NAT1P3    (L6)    1.044    0.011   94.108    0.000    0.604    0.906\n  Pos2 =~                                                               \n    PAT2P1    (L1)    1.040    0.010  103.200    0.000    0.696    0.911\n    PAT2P2    (L2)    0.997    0.010   98.587    0.000    0.667    0.900\n    PAT2P3    (L3)    0.963    0.010   93.301    0.000    0.644    0.878\n  Neg2 =~                                                               \n    NAT2P1    (L4)    0.957    0.012   78.488    0.000    0.527    0.816\n    NAT2P2    (L5)    0.999    0.011   90.830    0.000    0.550    0.873\n    NAT2P3    (L6)    1.044    0.011   94.108    0.000    0.576    0.891\n  Pos3 =~                                                               \n    PAT3P1    (L1)    1.040    0.010  103.200    0.000    0.694    0.898\n    PAT3P2    (L2)    0.997    0.010   98.587    0.000    0.665    0.864\n    PAT3P3    (L3)    0.963    0.010   93.301    0.000    0.642    0.853\n  Neg3 =~                                                               \n    NAT3P1    (L4)    0.957    0.012   78.488    0.000    0.563    0.836\n    NAT3P2    (L5)    0.999    0.011   90.830    0.000    0.588    0.887\n    NAT3P3    (L6)    1.044    0.011   94.108    0.000    0.614    0.868\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Pos1 ~~                                                               \n    Pos2              0.243    0.019   12.901    0.000    0.552    0.552\n    Pos3              0.231    0.019   12.306    0.000    0.527    0.527\n    Neg1             -0.062    0.015   -4.219    0.000   -0.164   -0.164\n    Neg2             -0.059    0.014   -4.163    0.000   -0.162   -0.162\n    Neg3             -0.014    0.015   -0.919    0.358   -0.035   -0.035\n  Pos2 ~~                                                               \n    Pos3              0.243    0.019   12.758    0.000    0.544    0.544\n  Neg1 ~~                                                               \n    Pos2             -0.059    0.015   -3.971    0.000   -0.152   -0.152\n  Pos2 ~~                                                               \n    Neg2             -0.091    0.014   -6.291    0.000   -0.246   -0.246\n    Neg3             -0.028    0.015   -1.882    0.060   -0.072   -0.072\n  Neg1 ~~                                                               \n    Pos3             -0.011    0.015   -0.722    0.470   -0.028   -0.028\n  Neg2 ~~                                                               \n    Pos3             -0.047    0.014   -3.309    0.001   -0.128   -0.128\n  Pos3 ~~                                                               \n    Neg3             -0.079    0.015   -5.140    0.000   -0.201   -0.201\n  Neg1 ~~                                                               \n    Neg2              0.152    0.013   11.258    0.000    0.475    0.475\n    Neg3              0.150    0.014   10.606    0.000    0.442    0.442\n  Neg2 ~~                                                               \n    Neg3              0.166    0.014   11.985    0.000    0.513    0.513\n .PAT1P1 ~~                                                             \n   .PAT2P1            0.010    0.006    1.662    0.096    0.010    0.091\n   .PAT3P1            0.007    0.007    1.089    0.276    0.007    0.061\n .PAT2P1 ~~                                                             \n   .PAT3P1           -0.000    0.006   -0.017    0.986   -0.000   -0.001\n .PAT1P2 ~~                                                             \n   .PAT2P2            0.008    0.006    1.325    0.185    0.008    0.069\n   .PAT3P2            0.012    0.007    1.749    0.080    0.012    0.088\n .PAT2P2 ~~                                                             \n   .PAT3P2            0.005    0.006    0.780    0.435    0.005    0.039\n .PAT1P3 ~~                                                             \n   .PAT2P3            0.011    0.006    1.785    0.074    0.011    0.084\n   .PAT3P3            0.013    0.007    1.782    0.075    0.013    0.084\n .PAT2P3 ~~                                                             \n   .PAT3P3            0.010    0.006    1.486    0.137    0.010    0.070\n .NAT1P1 ~~                                                             \n   .NAT2P1            0.028    0.007    3.711    0.000    0.028    0.156\n   .NAT3P1            0.009    0.007    1.149    0.251    0.009    0.049\n .NAT2P1 ~~                                                             \n   .NAT3P1            0.011    0.006    1.798    0.072    0.011    0.080\n .NAT1P2 ~~                                                             \n   .NAT2P2            0.000    0.005    0.038    0.970    0.000    0.002\n   .NAT3P2            0.006    0.005    1.169    0.243    0.006    0.066\n .NAT2P2 ~~                                                             \n   .NAT3P2            0.008    0.005    1.642    0.101    0.008    0.089\n .NAT1P3 ~~                                                             \n   .NAT2P3           -0.001    0.005   -0.176    0.861   -0.001   -0.011\n   .NAT3P3           -0.008    0.006   -1.350    0.177   -0.008   -0.077\n .NAT2P3 ~~                                                             \n   .NAT3P3           -0.004    0.006   -0.713    0.476   -0.004   -0.039\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    Pos1              3.000    0.024  124.671    0.000    4.563    4.563\n    Neg1              1.520    0.021   71.482    0.000    2.629    2.629\n    Pos2              3.012    0.024  124.262    0.000    4.502    4.502\n    Neg2              1.605    0.020   79.147    0.000    2.912    2.912\n    Pos3              2.929    0.024  119.856    0.000    4.389    4.389\n    Neg3              1.647    0.022   76.221    0.000    2.800    2.800\n   .PAT1P1    (t1)   -0.140    0.031   -4.557    0.000   -0.140   -0.181\n   .PAT1P2    (t2)   -0.089    0.031   -2.892    0.004   -0.089   -0.120\n   .PAT1P3    (t3)    0.229    0.031    7.275    0.000    0.229    0.309\n   .NAT1P1    (t4)    0.174    0.021    8.461    0.000    0.174    0.238\n   .NAT1P2    (t5)   -0.068    0.018   -3.691    0.000   -0.068   -0.104\n   .NAT1P3    (t6)   -0.107    0.018   -5.781    0.000   -0.107   -0.160\n   .PAT2P1    (t1)   -0.140    0.031   -4.557    0.000   -0.140   -0.183\n   .PAT2P2    (t2)   -0.089    0.031   -2.892    0.004   -0.089   -0.120\n   .PAT2P3    (t3)    0.229    0.031    7.275    0.000    0.229    0.312\n   .NAT2P1    (t4)    0.174    0.021    8.461    0.000    0.174    0.270\n   .NAT2P2    (t5)   -0.068    0.018   -3.691    0.000   -0.068   -0.107\n   .NAT2P3    (t6)   -0.107    0.018   -5.781    0.000   -0.107   -0.165\n   .PAT3P1    (t1)   -0.140    0.031   -4.557    0.000   -0.140   -0.181\n   .PAT3P2    (t2)   -0.089    0.031   -2.892    0.004   -0.089   -0.116\n   .PAT3P3    (t3)    0.229    0.031    7.275    0.000    0.229    0.304\n   .NAT3P1    (t4)    0.174    0.021    8.461    0.000    0.174    0.259\n   .NAT3P2    (t5)   -0.068    0.018   -3.691    0.000   -0.068   -0.102\n   .NAT3P3    (t6)   -0.107    0.018   -5.781    0.000   -0.107   -0.151\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    Pos1              0.432    0.024   18.357    0.000    1.000    1.000\n    Neg1              0.334    0.018   18.210    0.000    1.000    1.000\n    Pos2              0.448    0.024   18.753    0.000    1.000    1.000\n    Neg2              0.304    0.017   18.183    0.000    1.000    1.000\n    Pos3              0.445    0.024   18.343    0.000    1.000    1.000\n    Neg3              0.346    0.019   18.219    0.000    1.000    1.000\n   .PAT1P1            0.130    0.010   13.046    0.000    0.130    0.218\n   .PAT1P2            0.124    0.009   13.297    0.000    0.124    0.224\n   .PAT1P3            0.147    0.010   14.841    0.000    0.147    0.268\n   .NAT1P1            0.228    0.013   17.679    0.000    0.228    0.427\n   .NAT1P2            0.086    0.007   11.639    0.000    0.086    0.206\n   .NAT1P3            0.080    0.008   10.321    0.000    0.080    0.179\n   .PAT2P1            0.099    0.008   12.317    0.000    0.099    0.170\n   .PAT2P2            0.104    0.008   13.290    0.000    0.104    0.189\n   .PAT2P3            0.123    0.008   14.903    0.000    0.123    0.229\n   .NAT2P1            0.139    0.009   16.054    0.000    0.139    0.333\n   .NAT2P2            0.094    0.007   13.168    0.000    0.094    0.237\n   .NAT2P3            0.086    0.007   11.823    0.000    0.086    0.206\n   .PAT3P1            0.115    0.010   11.933    0.000    0.115    0.193\n   .PAT3P2            0.151    0.010   14.387    0.000    0.151    0.254\n   .PAT3P3            0.154    0.010   14.968    0.000    0.154    0.272\n   .NAT3P1            0.136    0.009   15.342    0.000    0.136    0.301\n   .NAT3P2            0.094    0.008   12.327    0.000    0.094    0.213\n   .NAT3P3            0.123    0.009   13.539    0.000    0.123    0.246\n\nConstraints:\n                                               |Slack|\n    L1 - (3-L2-L3)                               0.000\n    L4 - (3-L5-L6)                               0.000\n    t1 - (0-t2-t3)                               0.000\n    t4 - (0-t5-t6)                               0.000\n\n\n\n\n\n76.4.4 Confronto tra Modelli\nUn confronto tra i modelli precedenti può essere eseguito mediante il test del rapporto tra verosimiglianze.\n\nout &lt;- compareFit(fit_null, fitSEM, fit_config, fit_weak, fit_strong)\nsummary(out) |&gt; \n    print()\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n            Df   AIC   BIC    Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)\nfit_config 102 21263 21673   352.23                                      \nfit_weak   110 21262 21634   366.56       14.3 0.03101       8  0.0735478\nfit_strong 118 21298 21633   418.74       52.2 0.08191       8  1.557e-08\nfitSEM     124 21309 21615   441.52       22.8 0.05830       6  0.0008723\nfit_null   177 31974 32031 11213.10    10771.6 0.49571      53  &lt; 2.2e-16\n              \nfit_config    \nfit_weak   .  \nfit_strong ***\nfitSEM     ***\nfit_null   ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n####################### Model Fit Indices ###########################\n                     chisq  df pvalue        rmsea          cfi\nfit_config 352.232&lt;U+2020&gt; 102   .000        .055  .977&lt;U+2020&gt;\nfit_weak          366.562  110   .000 .053&lt;U+2020&gt;        .977 \nfit_strong        418.737  118   .000        .056         .973 \nfitSEM            441.520  124   .000        .056         .971 \nfit_null        11213.103  177   .000        .275         .000 \n                    tli         srmr               aic               bic\nfit_config        .966         .035         21263.420         21673.447 \nfit_weak   .967&lt;U+2020&gt; .035&lt;U+2020&gt; 21261.750&lt;U+2020&gt;        21634.074 \nfit_strong        .964         .037         21297.925         21632.545 \nfitSEM            .964         .045         21308.709  21615.051&lt;U+2020&gt;\nfit_null          .131         .328         31974.291         32030.846 \n\n################## Differences in Fit Indices #######################\n                      df  rmsea    cfi    tli  srmr       aic       bic\nfit_weak - fit_config  8 -0.001 -0.001  0.002 0.000    -1.670   -39.373\nfit_strong - fit_weak  8  0.002 -0.004 -0.003 0.002    36.175    -1.529\nfitSEM - fit_strong    6  0.000 -0.002  0.000 0.008    10.783   -17.494\nfit_null - fitSEM     53  0.219 -0.971 -0.833 0.282 10665.582 10415.795\n\nThe following lavaan models were compared:\n    fit_config\n    fit_weak\n    fit_strong\n    fitSEM\n    fit_null\nTo view results, assign the compareFit() output to an object and  use the summary() method; see the class?FitDiff help page.\n\n\nI valori di Chisq indicano il grado di adattamento dei modelli ai dati. Valori più bassi indicano un migliore adattamento. Dall’elenco, fit_config ha il valore di Chi-square più basso, suggerendo il miglior adattamento tra i modelli confrontati. Un valore di Chisq diff significativo (basso valore p) suggerisce che il modello più vincolato ha un adattamento significativamente peggiore.Chisq diff di 14.3 con un valore p di 0.0735 indica che non c’è una differenza statisticamente significativa nel fit tra i modelli configurale e debole. Questo suggerisce che l’aggiunta dell’invarianza debole (uguaglianza dei carichi fattoriali) non peggiora significativamente il fit. Chisq diff è 52.2 con un valore p molto basso (1.557e-08), indica che l’aggiunta dell’invarianza forte (uguaglianza delle medie) peggiora significativamente il fit rispetto al modello debole. Una differenza di 22.8 nel Chi-square e un valore p basso (0.0008723) suggeriscono che il modello forte ha un fit significativamente peggiore rispetto al modello SEM base. Il modello nullo ha un valore molto alto di Chi-square, indicando, come previsto, un adattamento molto scarso. Questo è normale per i modelli nulli e serve come riferimento estremo.\nIl RMSEA è un indice di bontà di adattamento che considera la complessità del modello. Valori inferiori a 0.05 indicano un buon adattamento, valori tra 0.05 e 0.08 indicano un adattamento accettabile, e valori superiori a 0.10 sono generalmente considerati inaccettabili. In questo caso, il RMSEA aumenta da fit_config a fit_strong, suggerendo un peggioramento dell’adattamento con l’aggiunta di vincoli più forti.\nIn conclusione, i risultati indicano che l’aggiunta di vincoli di invarianza debole non peggiora significativamente il fit, mentre l’aggiunta di vincoli di invarianza forte riduce in modo significativo la bontà di adattamento del modello. Questo suggerisce che, mentre i carichi fattoriali possono essere considerati invarianti tra i gruppi o nel tempo, le medie degli indicatori potrebbero non esserlo.\nLittle (2023) nota che, con un campione così grande, disponiamo di un livello di potere statistico sufficiente anche per rilevare differenze minuscole. Quindi, i risultati dei test statistici precedenti vanno presi con un grano di sale. In particolare, Little (2023) nota che il modello di invarianza forte fornisce evidenze di un adattamento soddisfacente e che il peggioramento dell’adattamento rispetto al modello di invarianza debole è, quantitativamente, estremamente piccolo se esaminato rispetto alle dimensioni di CFI, TLI, RMSEA, e SRMR. Per queste ragioni, Little (2023) conclude affermando che il modello di invarianza forte risulta giustificato da questi dati. I criteri per determinare una perdita eccessiva dell’adattamento, data la potenza della dimensione del campione, sono un valore p inferiore a .001, un cambiamento nel CFI superiore a .002, o una stima puntuale dell’RMSEA che cade al di fuori dell’intervallo di confidenza del modello di invarianza forte.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>76</span>  <span class='chapter-title'>Specificare e Interpretare un Modello Longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/05_intro_panel.html#riflessioni-conclusive",
    "href": "chapters/lgm/05_intro_panel.html#riflessioni-conclusive",
    "title": "76  Specificare e Interpretare un Modello Longitudinale",
    "section": "76.5 Riflessioni Conclusive",
    "text": "76.5 Riflessioni Conclusive\nIn questo capitolo, abbiamo affrontato i passaggi e molte delle problematiche associate all’adattamento di un modello panel standard ai dati longitudinali. Nei dati longitudinali, il continuum lungo il quale sono ordinati i costrutti è rappresentato dal tempo.\nI modelli panel di base che abbiamo esaminato qui sono solo alcuni tra i vari tipi di modelli che possono essere adattati ai dati panel. In questo contesto, l’analisi dei dati longitudinali implica un approccio sistematico per esaminare come determinati costrutti o variabili cambiano nel corso del tempo. Questo può includere l’analisi di tendenze, cicli o pattern nei dati raccolti in diversi momenti.\nAdattare un modello panel a dati longitudinali richiede una comprensione approfondita sia della natura dei dati sia delle tecniche statistiche utilizzate. Questo processo può comportare sfide specifiche, come la gestione di dati mancanti, l’accounting per la variabilità sia tra i soggetti che all’interno dello stesso soggetto nel tempo, e la scelta del modello statistico più appropriato in base alla struttura dei dati e agli obiettivi della ricerca.\nI modelli panel di base, come quelli discussi in questo capitolo, sono un punto di partenza fondamentale. Tuttavia, esistono molte altre varianti e approfondimenti di questi modelli che possono essere esplorati per adattarsi meglio a scenari complessi o per rispondere a specifiche domande di ricerca. Questi includono modelli panel più avanzati che possono tener conto di effetti casuali, effetti fissi, o che possono essere usati per analizzare le interazioni tra variabili nel tempo.\nL’obiettivo finale di questi modelli è di fornire una rappresentazione accurata di come i costrutti si evolvono nel tempo, permettendo ai ricercatori di trarre conclusioni affidabili dai loro dati.\n\n\n\n\nLittle, T. D. (2023). Longitudinal structural equation modeling. Guilford Press.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>76</span>  <span class='chapter-title'>Specificare e Interpretare un Modello Longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/07_growth_1.html",
    "href": "chapters/lgm/07_growth_1.html",
    "title": "77  Curve di crescita latente",
    "section": "",
    "text": "77.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nNel capitolo precedente, abbiamo esplorato come i modelli di Crescita Latente (LGM) possano essere correlati e confrontati con i modelli lineari ad effetti misti attraverso l’uso di dati simulati. In particolare, abbiamo osservato come sia possibile strutturare un modello LGM che incorpori un fattore latente per la variazione delle intercette individuali, il quale cattura le dinamiche del cambiamento delle medie nel tempo, e un secondo fattore latente che riflette le variazioni individuali nelle pendenze delle rette di regressione.\nAbbiamo esaminato in dettaglio il processo di definizione di questi fattori latenti, mettendo in atto una serie di vincoli sugli indicatori che identificano le variabili latenti. In particolare, abbiamo visto come l’applicazione dei vincoli 0, 1, 2, 3 alle saturazioni fattoriali per il fattore “pendenza” determini una relazione lineare tra la media del costrutto e il tempo. Questo approccio è particolarmente efficace quando le misurazioni del costrutto sono state effettuate a intervalli regolari.\nIn questo capitolo, ci dedicheremo all’approfondimento di questo argomento, spostando la nostra attenzione dall’ambito teorico e simulato all’analisi di un set di dati reali. Questo passaggio ci fornirà una visione più chiara e concreta di come questi modelli possano essere impiegati nell’analisi di dati longitudinali reali, con tutti le loro sfaccettature e sfide.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>77</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/07_growth_1.html#una-applicazione-concreta",
    "href": "chapters/lgm/07_growth_1.html#una-applicazione-concreta",
    "title": "77  Curve di crescita latente",
    "section": "77.2 Una applicazione concreta",
    "text": "77.2 Una applicazione concreta\nEsaminiamo l’adattamento di un modello LGM ad un campione di dati reali. In questo tutorial, considereremo il cambiamento nel rendimento in matematica dei bambini durante la scuola elementare e media utilizzando il set di dati NLSY-CYA {cite:p}grimm2016growth. Iniziamo a leggere i dati.\n\n#set filepath for data file\nfilepath &lt;- \"https://raw.githubusercontent.com/LRI-2/Data/main/GrowthModeling/nlsy_math_wide_R.dat\"\n\n# read in the text data file using the url() function\ndat &lt;- read.table(\n  file = url(filepath),\n  na.strings = \".\"\n) # indicates the missing data designator\n\n# copy data with new name\nnlsy_math_wide &lt;- dat\n\n# Give the variable names\nnames(nlsy_math_wide) &lt;- c(\n  \"id\", \"female\", \"lb_wght\", \"anti_k1\",\n  \"math2\", \"math3\", \"math4\", \"math5\", \"math6\", \"math7\", \"math8\",\n  \"age2\", \"age3\", \"age4\", \"age5\", \"age6\", \"age7\", \"age8\",\n  \"men2\", \"men3\", \"men4\", \"men5\", \"men6\", \"men7\", \"men8\",\n  \"spring2\", \"spring3\", \"spring4\", \"spring5\", \"spring6\", \"spring7\", \"spring8\",\n  \"anti2\", \"anti3\", \"anti4\", \"anti5\", \"anti6\", \"anti7\", \"anti8\"\n)\n\n# view the first few observations (and columns) in the data set\nhead(nlsy_math_wide[, 1:11], 10)\n\n\nA data.frame: 10 x 11\n\n\n\nid\nfemale\nlb_wght\nanti_k1\nmath2\nmath3\nmath4\nmath5\nmath6\nmath7\nmath8\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n201\n1\n0\n0\nNA\n38\nNA\n55\nNA\nNA\nNA\n\n\n2\n303\n1\n0\n1\n26\nNA\nNA\n33\nNA\nNA\nNA\n\n\n3\n2702\n0\n0\n0\n56\nNA\n58\nNA\nNA\nNA\n80\n\n\n4\n4303\n1\n0\n0\nNA\n41\n58\nNA\nNA\nNA\nNA\n\n\n5\n5002\n0\n0\n4\nNA\nNA\n46\nNA\n54\nNA\n66\n\n\n6\n5005\n1\n0\n0\n35\nNA\n50\nNA\n60\nNA\n59\n\n\n7\n5701\n0\n0\n2\nNA\n62\n61\nNA\nNA\nNA\nNA\n\n\n8\n6102\n0\n0\n0\nNA\nNA\n55\n67\nNA\n81\nNA\n\n\n9\n6801\n1\n0\n0\nNA\n54\nNA\n62\nNA\n66\nNA\n\n\n10\n6802\n0\n0\n0\nNA\n55\nNA\n66\nNA\n68\nNA\n\n\n\n\n\nIl nostro interesse specifico riguarda il cambiamento relativo alle misure ripetute di matematica, da math2 a math8. Selezioniamo dunque le variabili di interesse.\n\nnlsy_math_sub &lt;- nlsy_math_wide |&gt;\n    dplyr::select(\"id\", \"math2\", \"math3\", \"math4\", \"math5\", \"math6\", \"math7\", \"math8\")\n\nTrasformiamo i dati in formato long.\n\nnlsy_math_long &lt;- reshape(\n  data = nlsy_math_sub,\n  timevar = c(\"grade\"),\n  idvar = \"id\",\n  varying = c(\n    \"math2\", \"math3\", \"math4\",\n    \"math5\", \"math6\", \"math7\", \"math8\"\n  ),\n  direction = \"long\", sep = \"\"\n)\n\nOrdiniamo i dati in base alle variabili id e grade.\n\nnlsy_math_long &lt;- nlsy_math_long[order(nlsy_math_long$id, nlsy_math_long$grade), ]\n\nRimuoviamo gli NA dalla variabile math per potere generare il grafico con le traiettorie individuali di sviluppo.\n\nnlsy_math_long &lt;- nlsy_math_long[which(is.na(nlsy_math_long$math) == FALSE), ]\n\nEsaminiamo i dati grezzi.\n\nnlsy_math_long |&gt;\n  ggplot(aes(x = grade, y = math)) +\n  geom_point(\n    size = 1.2,\n    alpha = .8,\n    # to add some random noise for plotting purposes\n    position = \"jitter\"\n  ) +\n  labs(title = \"PAT Mathematics as a function of Grade\")\n\n\n\n\n\n\n\n\nAggiungiamo al grafico le retta dei minimi quadrati calcolata su tutti i dati (ignorando il ragruppamento dei dati in funzione dei partecipanti).\n\nnlsy_math_long |&gt;\n    ggplot(aes(x = grade, y = math)) +\n    geom_point(\n        size = 1.2,\n        alpha = .8,\n        # to add some random noise for plotting purposes\n        position = \"jitter\"\n    ) +\n    geom_smooth(\n        method = lm,\n        se = FALSE,\n        col = \"blue\",\n        linewidth = 1.5,\n        alpha = .8\n    ) + # to add regression line\n    labs(title = \"PAT Mathematics as a function of Grade\")\n\n\n\n\n\n\n\n\nEsaminiamo le traiettorie di cambiamento intra-individuale.\n\n# intraindividual change trajetories\nnlsy_math_long |&gt;\n  ggplot(\n    aes(x = grade, y = math, group = id)\n  ) + # setting variables\n  geom_point(size = .5) + # adding points to plot\n  geom_line(alpha = 0.3) + # adding lines to plot\n  # setting the x-axis with breaks and labels\n  scale_x_continuous(\n    limits = c(2, 8),\n    breaks = c(2, 3, 4, 5, 6, 7, 8),\n    name = \"Grade at Testing\"\n  ) +\n  # setting the y-axis with limits breaks and labels\n  scale_y_continuous(\n    limits = c(10, 90),\n    breaks = c(10, 30, 50, 70, 90),\n    name = \"PIAT Mathematics\"\n  )",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>77</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/07_growth_1.html#modello-di-assenza-di-crescita",
    "href": "chapters/lgm/07_growth_1.html#modello-di-assenza-di-crescita",
    "title": "77  Curve di crescita latente",
    "section": "77.3 Modello di assenza di crescita",
    "text": "77.3 Modello di assenza di crescita\nDall’analisi dei grafici precedenti, si osserva che i punteggi di matematica mostrano un incremento sistematico nel tempo. Per iniziare l’analisi, adotteremo un modello di assenza di crescita come benchmark di base per il confronto con modelli più complessi successivi.\nIn questo modello si assume che i punteggi di matematica degli studenti rimangano invariati nel corso del tempo. Esso mira a stimare, per ogni studente, il “valore vero” dei loro punteggi in matematica, senza prendere in considerazione eventuali variazioni nel tempo. Poiché non contempla la dinamica temporale dei punteggi, questo modello rappresenta una situazione di stallo o assenza di sviluppo, risultando spesso di limitato interesse e pertanto generalmente non viene adottato in analisi più approfondite.\nIl modello di assenza della crescita è caratterizzato dalla presenza di una variabile latente e di un’intercetta, la quale rappresenta un livello medio di performance che si mantiene costante nel tempo. Questa configurazione del modello permette di stabilire un punto di partenza per comprendere se e in che misura i punteggi di matematica variano effettivamente nel corso del tempo, quando confrontati con modelli che considerano la crescita o l’evoluzione dei punteggi.\nPer definire il modello di assenza di crescita, utilizziamo la seguente sintassi di lavaan.\n\nng_math_lavaan_model &lt;- ' \n  # latent variable definitions\n      #intercept\n      eta_1 =~ 1*math2\n      eta_1 =~ 1*math3\n      eta_1 =~ 1*math4\n      eta_1 =~ 1*math5\n      eta_1 =~ 1*math6\n      eta_1 =~ 1*math7\n      eta_1 =~ 1*math8\n\n  # factor variances\n      eta_1 ~~ eta_1\n\n  # covariances among factors \n      #none (only 1 factor)\n\n  # factor means \n      eta_1 ~ start(30)*1\n\n  # manifest variances (made equivalent by naming theta)\n      math2 ~~ theta*math2\n      math3 ~~ theta*math3\n      math4 ~~ theta*math4\n      math5 ~~ theta*math5\n      math6 ~~ theta*math6\n      math7 ~~ theta*math7\n      math8 ~~ theta*math8\n  # manifest means (fixed at zero)\n      math2 ~ 0*1\n      math3 ~ 0*1\n      math4 ~ 0*1\n      math5 ~ 0*1\n      math6 ~ 0*1\n      math7 ~ 0*1\n      math8 ~ 0*1\n' #end of model definition\n\nAdattiamo il modello ai dati.\n\nng_math_lavaan_fit &lt;- sem(ng_math_lavaan_model,\n  data = nlsy_math_wide,\n  meanstructure = TRUE,\n  estimator = \"ML\",\n  missing = \"fiml\"\n)\n\nNel codice fornito, l’opzione missing = \"fiml\" utilizzata nella funzione sem() specifica il metodo “Full Information Maximum Likelihood” (FIML) per gestire i dati mancanti nel dataset. FIML è un approccio sofisticato per la gestione dei dati mancanti in analisi statistiche complesse come i modelli SEM. A differenza di metodi più semplici come l’eliminazione lista per lista o l’imputazione media, FIML utilizza tutte le informazioni disponibili nel dataset, inclusi i pattern dei dati mancanti, per produrre stime dei parametri. Questo metodo è particolarmente utile quando si lavora con dataset longitudinali o complessi dove i dati mancanti sono comuni. FIML è considerato un approccio più accurato e meno distorto rispetto ad altri metodi, in quanto non si limita a utilizzare solo i casi completi, ma incorpora l’intero insieme di dati disponibili, comprese le osservazioni parziali.\nEsaminiamo la soluzione.\n\nsummary(ng_math_lavaan_fit, fit.measures = TRUE, standardized = TRUE) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 18 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         9\n  Number of equality constraints                     6\n\n                                                  Used       Total\n  Number of observations                           932         933\n  Number of missing patterns                        60            \n\nModel Test User Model:\n                                                      \n  Test statistic                              1759.002\n  Degrees of freedom                                32\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               862.334\n  Degrees of freedom                                21\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.000\n  Tucker-Lewis Index (TLI)                      -0.347\n                                                      \n  Robust Comparative Fit Index (CFI)             0.000\n  Robust Tucker-Lewis Index (TLI)                0.093\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -8745.952\n  Loglikelihood unrestricted model (H1)      -7866.451\n                                                      \n  Akaike (AIC)                               17497.903\n  Bayesian (BIC)                             17512.415\n  Sample-size adjusted Bayesian (SABIC)      17502.888\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.241\n  90 Percent confidence interval - lower         0.231\n  90 Percent confidence interval - upper         0.250\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n                                                      \n  Robust RMSEA                                   0.467\n  90 Percent confidence interval - lower         0.402\n  90 Percent confidence interval - upper         0.534\n  P-value H_0: Robust RMSEA &lt;= 0.050             0.000\n  P-value H_0: Robust RMSEA &gt;= 0.080             1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.480\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  eta_1 =~                                                              \n    math2             1.000                               6.850    0.536\n    math3             1.000                               6.850    0.536\n    math4             1.000                               6.850    0.536\n    math5             1.000                               6.850    0.536\n    math6             1.000                               6.850    0.536\n    math7             1.000                               6.850    0.536\n    math8             1.000                               6.850    0.536\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    eta_1            45.915    0.324  141.721    0.000    6.703    6.703\n   .math2             0.000                               0.000    0.000\n   .math3             0.000                               0.000    0.000\n   .math4             0.000                               0.000    0.000\n   .math5             0.000                               0.000    0.000\n   .math6             0.000                               0.000    0.000\n   .math7             0.000                               0.000    0.000\n   .math8             0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    eta_1            46.917    4.832    9.709    0.000    1.000    1.000\n   .math2   (thet)  116.682    4.548   25.656    0.000  116.682    0.713\n   .math3   (thet)  116.682    4.548   25.656    0.000  116.682    0.713\n   .math4   (thet)  116.682    4.548   25.656    0.000  116.682    0.713\n   .math5   (thet)  116.682    4.548   25.656    0.000  116.682    0.713\n   .math6   (thet)  116.682    4.548   25.656    0.000  116.682    0.713\n   .math7   (thet)  116.682    4.548   25.656    0.000  116.682    0.713\n   .math8   (thet)  116.682    4.548   25.656    0.000  116.682    0.713\n\n\n\nGeneriamo il diagramma di percorso.\n\nsemPaths(ng_math_lavaan_fit, what = \"path\", whatLabels = \"par\")\n\n\n\n\n\n\n\n\nCalcoliamo le traiettorie predette.\n\n#obtaining predicted factor scores for individuals\nnlsy_math_predicted &lt;- as.data.frame(cbind(nlsy_math_wide$id,lavPredict(ng_math_lavaan_fit)))\n\n#naming columns\nnames(nlsy_math_predicted) &lt;- c(\"id\", \"eta_1\")\n\n#looking at data\nhead(nlsy_math_predicted) \n\n\nA data.frame: 6 x 2\n\n\n\nid\neta_1\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n201\n46.17558\n\n\n2\n303\n38.59816\n\n\n3\n2702\n56.16725\n\n\n4\n4303\n47.51278\n\n\n5\n5002\n51.06429\n\n\n6\n5005\n49.05038\n\n\n\n\n\n\n# calculating implied manifest scores\nnlsy_math_predicted$math2 &lt;- 1 * nlsy_math_predicted$eta_1\nnlsy_math_predicted$math3 &lt;- 1 * nlsy_math_predicted$eta_1\nnlsy_math_predicted$math4 &lt;- 1 * nlsy_math_predicted$eta_1\nnlsy_math_predicted$math5 &lt;- 1 * nlsy_math_predicted$eta_1\nnlsy_math_predicted$math6 &lt;- 1 * nlsy_math_predicted$eta_1\nnlsy_math_predicted$math7 &lt;- 1 * nlsy_math_predicted$eta_1\nnlsy_math_predicted$math8 &lt;- 1 * nlsy_math_predicted$eta_1\n\n# reshaping wide to long\nnlsy_math_predicted_long &lt;- reshape(\n  data = nlsy_math_predicted,\n  timevar = c(\"grade\"),\n  idvar = \"id\",\n  varying = c(\n    \"math2\", \"math3\", \"math4\",\n    \"math5\", \"math6\", \"math7\", \"math8\"\n  ),\n  direction = \"long\", sep = \"\"\n)\n# sorting for easy viewing\n# order by id and time\nnlsy_math_predicted_long &lt;- nlsy_math_predicted_long[order(nlsy_math_predicted_long$id, nlsy_math_predicted_long$grade), ]\n\n# intraindividual change trajetories\nggplot(\n  data = nlsy_math_predicted_long, # data set\n  aes(x = grade, y = math, group = id)\n) + # setting variables\n  # geom_point(size=.5) + #adding points to plot\n  geom_line(alpha = 0.1) + # adding lines to plot\n  # setting the x-axis with breaks and labels\n  scale_x_continuous(\n    limits = c(2, 8),\n    breaks = c(2, 3, 4, 5, 6, 7, 8),\n    name = \"Grade at Testing\"\n  ) +\n  # setting the y-axis with limits breaks and labels\n  scale_y_continuous(\n    limits = c(10, 90),\n    breaks = c(10, 30, 50, 70, 90),\n    name = \"Predicted PIAT Mathematics\"\n  )\n\n\n\n\n\n\n\n\nDal grafico risulta evidente che il modello impiegato genera una serie di linee orizzontali, ognuna rappresentante la traiettoria statica dell’abilità matematica per ogni individuo. In questo modello, l’intercetta associata a ciascuna di queste linee orizzontali corrisponde al “valore vero” dell’abilità matematica di ogni bambino. Conformemente alle ipotesi del modello, questo valore si mantiene invariato nel corso del tempo, suggerendo che, secondo il modello, l’abilità matematica di ciascun individuo non subisce variazioni o sviluppi significativi nel periodo osservato.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>77</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/07_growth_1.html#modello-di-crescita-lineare",
    "href": "chapters/lgm/07_growth_1.html#modello-di-crescita-lineare",
    "title": "77  Curve di crescita latente",
    "section": "77.4 Modello di crescita lineare",
    "text": "77.4 Modello di crescita lineare\nNella discussione dei modelli di crescita, il modello di assenza di crescita viene sempre seguito dall’esame del modello di crescita lineare. Infatti, i modelli di crescita lineare rappresentano spesso il punto di partenza quando si cerca di comprendere il cambiamento all’interno dell’individuo. Successivamente, possono essere considerati anche modelli di crescita non lineare. Procediamo dunque all’implementazione di un modello di crescita latente lineare.\n\nlg_math_lavaan_model &lt;- '\n  # latent variable definitions\n      #intercept (note intercept is a reserved term)\n      eta_1 =~ 1*math2\n      eta_1 =~ 1*math3\n      eta_1 =~ 1*math4\n      eta_1 =~ 1*math5\n      eta_1 =~ 1*math6\n      eta_1 =~ 1*math7\n      eta_1 =~ 1*math8\n\n      #linear slope \n      eta_2 =~ 0*math2\n      eta_2 =~ 1*math3\n      eta_2 =~ 2*math4\n      eta_2 =~ 3*math5\n      eta_2 =~ 4*math6\n      eta_2 =~ 5*math7\n      eta_2 =~ 6*math8\n\n  # factor variances\n      eta_1 ~~ eta_1\n      eta_2 ~~ eta_2\n\n  # covariances among factors \n      eta_1 ~~ eta_2\n\n  # factor means \n      eta_1 ~ 1\n      eta_2 ~ 1\n\n  # manifest variances (made equivalent by naming theta)\n      math2 ~~ theta*math2\n      math3 ~~ theta*math3\n      math4 ~~ theta*math4\n      math5 ~~ theta*math5\n      math6 ~~ theta*math6\n      math7 ~~ theta*math7\n      math8 ~~ theta*math8\n  # manifest means (fixed at zero)\n      math2 ~ 0*1\n      math3 ~ 0*1\n      math4 ~ 0*1\n      math5 ~ 0*1\n      math6 ~ 0*1\n      math7 ~ 0*1\n      math8 ~ 0*1\n' #end of model definition\n\nAdattiamo il modello ai dati.\n\nlg_math_lavaan_fit &lt;- sem(lg_math_lavaan_model,\n  data = nlsy_math_wide,\n  meanstructure = TRUE,\n  estimator = \"ML\",\n  missing = \"fiml\"\n)\n\nEsaminiamo il risultato ottenuto.\n\nsummary(lg_math_lavaan_fit, fit.measures = TRUE, standardized = TRUE) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 38 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        12\n  Number of equality constraints                     6\n\n                                                  Used       Total\n  Number of observations                           932         933\n  Number of missing patterns                        60            \n\nModel Test User Model:\n                                                      \n  Test statistic                               204.484\n  Degrees of freedom                                29\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               862.334\n  Degrees of freedom                                21\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.791\n  Tucker-Lewis Index (TLI)                       0.849\n                                                      \n  Robust Comparative Fit Index (CFI)             0.896\n  Robust Tucker-Lewis Index (TLI)                0.925\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -7968.693\n  Loglikelihood unrestricted model (H1)      -7866.451\n                                                      \n  Akaike (AIC)                               15949.386\n  Bayesian (BIC)                             15978.410\n  Sample-size adjusted Bayesian (SABIC)      15959.354\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.081\n  90 Percent confidence interval - lower         0.070\n  90 Percent confidence interval - upper         0.091\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.550\n                                                      \n  Robust RMSEA                                   0.134\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.233\n  P-value H_0: Robust RMSEA &lt;= 0.050             0.136\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.792\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.121\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  eta_1 =~                                                              \n    math2             1.000                               8.035    0.800\n    math3             1.000                               8.035    0.799\n    math4             1.000                               8.035    0.792\n    math5             1.000                               8.035    0.779\n    math6             1.000                               8.035    0.762\n    math7             1.000                               8.035    0.742\n    math8             1.000                               8.035    0.719\n  eta_2 =~                                                              \n    math2             0.000                               0.000    0.000\n    math3             1.000                               0.856    0.085\n    math4             2.000                               1.712    0.169\n    math5             3.000                               2.568    0.249\n    math6             4.000                               3.424    0.325\n    math7             5.000                               4.279    0.395\n    math8             6.000                               5.135    0.459\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  eta_1 ~~                                                              \n    eta_2            -0.181    1.150   -0.158    0.875   -0.026   -0.026\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    eta_1            35.267    0.355   99.229    0.000    4.389    4.389\n    eta_2             4.339    0.088   49.136    0.000    5.070    5.070\n   .math2             0.000                               0.000    0.000\n   .math3             0.000                               0.000    0.000\n   .math4             0.000                               0.000    0.000\n   .math5             0.000                               0.000    0.000\n   .math6             0.000                               0.000    0.000\n   .math7             0.000                               0.000    0.000\n   .math8             0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    eta_1            64.562    5.659   11.408    0.000    1.000    1.000\n    eta_2             0.733    0.327    2.238    0.025    1.000    1.000\n   .math2   (thet)   36.230    1.867   19.410    0.000   36.230    0.359\n   .math3   (thet)   36.230    1.867   19.410    0.000   36.230    0.358\n   .math4   (thet)   36.230    1.867   19.410    0.000   36.230    0.352\n   .math5   (thet)   36.230    1.867   19.410    0.000   36.230    0.341\n   .math6   (thet)   36.230    1.867   19.410    0.000   36.230    0.326\n   .math7   (thet)   36.230    1.867   19.410    0.000   36.230    0.309\n   .math8   (thet)   36.230    1.867   19.410    0.000   36.230    0.290\n\n\n\n\nprint(fitMeasures(lg_math_lavaan_fit, c(\"chisq\", \"df\", \"pvalue\", \"cfi\", \"rmsea\")))\n\n  chisq      df  pvalue     cfi   rmsea \n204.484  29.000   0.000   0.791   0.081 \n\n\nGeneriamo un diagramma di percorso.\n\nsemPaths(lg_math_lavaan_fit, what = \"path\", whatLabels = \"par\")\n\n\n\n\n\n\n\n\nEsaminiamo le traiettorie di crescita.\n\nnlsy_math_predicted &lt;- as.data.frame(\n    cbind(nlsy_math_wide$id, lavPredict(lg_math_lavaan_fit))\n)\n\n#naming columns\nnames(nlsy_math_predicted) &lt;- c(\"id\", \"eta_1\", \"eta_2\")\n\nhead(nlsy_math_predicted)\n\n\nA data.frame: 6 x 3\n\n\n\nid\neta_1\neta_2\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n201\n36.94675\n4.534084\n\n\n2\n303\n26.03589\n4.050780\n\n\n3\n2702\n49.70187\n4.594149\n\n\n4\n4303\n41.04200\n4.548064\n\n\n5\n5002\n37.01240\n4.496746\n\n\n6\n5005\n37.68809\n4.324198\n\n\n\n\n\n\n#calculating implied manifest scores\nnlsy_math_predicted$math2 &lt;- 1 * nlsy_math_predicted$eta_1 + 0 * nlsy_math_predicted$eta_2\nnlsy_math_predicted$math3 &lt;- 1 * nlsy_math_predicted$eta_1 + 1 * nlsy_math_predicted$eta_2\nnlsy_math_predicted$math4 &lt;- 1 * nlsy_math_predicted$eta_1 + 2 * nlsy_math_predicted$eta_2\nnlsy_math_predicted$math5 &lt;- 1 * nlsy_math_predicted$eta_1 + 3 * nlsy_math_predicted$eta_2\nnlsy_math_predicted$math6 &lt;- 1 * nlsy_math_predicted$eta_1 + 4 * nlsy_math_predicted$eta_2\nnlsy_math_predicted$math7 &lt;- 1 * nlsy_math_predicted$eta_1 + 5 * nlsy_math_predicted$eta_2\nnlsy_math_predicted$math8 &lt;- 1 * nlsy_math_predicted$eta_1 + 6 * nlsy_math_predicted$eta_2\n\n\n# reshaping wide to long\nnlsy_math_predicted_long &lt;- reshape(\n  data = nlsy_math_predicted,\n  timevar = c(\"grade\"),\n  idvar = \"id\",\n  varying = c(\n    \"math2\", \"math3\", \"math4\", \"math5\", \"math6\", \"math7\", \"math8\"\n  ),\n  direction = \"long\", sep = \"\"\n)\n\n\nhead(nlsy_math_predicted_long)\n\n\nA data.frame: 6 x 5\n\n\n\nid\neta_1\neta_2\ngrade\nmath\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n201.2\n201\n36.94675\n4.534084\n2\n36.94675\n\n\n303.2\n303\n26.03589\n4.050780\n2\n26.03589\n\n\n2702.2\n2702\n49.70187\n4.594149\n2\n49.70187\n\n\n4303.2\n4303\n41.04200\n4.548064\n2\n41.04200\n\n\n5002.2\n5002\n37.01240\n4.496746\n2\n37.01240\n\n\n5005.2\n5005\n37.68809\n4.324198\n2\n37.68809\n\n\n\n\n\n\n# sorting for easy viewing\n# order by id and time\nnlsy_math_predicted_long &lt;-\n  nlsy_math_predicted_long[order(nlsy_math_predicted_long$id, nlsy_math_predicted_long$grade), ]\n\n\nhead(nlsy_math_predicted_long)\n\n\nA data.frame: 6 x 5\n\n\n\nid\neta_1\neta_2\ngrade\nmath\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n201.2\n201\n36.94675\n4.534084\n2\n36.94675\n\n\n201.3\n201\n36.94675\n4.534084\n3\n41.48083\n\n\n201.4\n201\n36.94675\n4.534084\n4\n46.01492\n\n\n201.5\n201\n36.94675\n4.534084\n5\n50.54900\n\n\n201.6\n201\n36.94675\n4.534084\n6\n55.08309\n\n\n201.7\n201\n36.94675\n4.534084\n7\n59.61717\n\n\n\n\n\n\nggplot(\n  data = nlsy_math_predicted_long, # data set\n  aes(x = grade, y = math, group = id)\n) + # setting variables\n  # geom_point(size=.5) + #adding points to plot\n  geom_line(alpha = 0.15) + # adding lines to plot\n  # setting the x-axis with breaks and labels\n  scale_x_continuous(\n    limits = c(2, 8),\n    breaks = c(2, 3, 4, 5, 6, 7, 8),\n    name = \"Grade at Testing\"\n  ) +\n  # setting the y-axis with limits breaks and labels\n  scale_y_continuous(\n    limits = c(10, 90),\n    breaks = c(10, 30, 50, 70, 90),\n    name = \"Predicted PIAT Mathematics\"\n  )\n\n\n\n\n\n\n\n\nIl modello di crescita latente lineare descrive la traiettoria di sviluppo di ogni bambino attraverso una linea retta, mettendo in luce le variazioni individuali nelle competenze matematiche nel corso del tempo. Il grafico illustra che, per ciascun bambino, si registra un incremento “reale” di circa 5 punti nell’abilità matematica per ogni anno scolastico. Questo modello, quindi, non solo traccia la progressione lineare delle competenze matematiche, ma rivela anche un pattern di crescita coerente e uniforme tra i bambini nel periodo considerato.\n\n77.4.1 Sintassi alternativa\nPer semplificare la scrittura del modello possiamo usare la funzione growth. Tuttavia, per il modello discusso in precedenza, è necessario specificare un parametro aggiuntivo rispetto ai default di growth: vogliamo che le varianze residue di math siano costanti nel tempo.\n\nm1 &lt;-   '\n  i =~ 1*math2 + 1*math3 + 1*math4 + 1*math5 + 1*math6 + 1*math7 + 1*math8  \n  s =~ 0 * math2 + 1 * math3 + 2 * math4 + 3 * math5 + 4 * math6 + 5 * math7 + 6 * math8\n  \n  # manifest variances (made equivalent by naming theta)\n  math2 ~~ theta*math2\n  math3 ~~ theta*math3\n  math4 ~~ theta*math4\n  math5 ~~ theta*math5\n  math6 ~~ theta*math6\n  math7 ~~ theta*math7\n  math8 ~~ theta*math8\n'\n\nAdattiamo il modello.\n\nfit_m1 &lt;- growth(\n  m1,\n  data = nlsy_math_wide,\n  estimator = \"ML\",\n  missing = \"fiml\"\n)\n\nOtteniamo in questo modo lo stesso risultato trovato con la precedente specificazione del modello.\n\nprint(fitMeasures(fit_m1, c(\"chisq\", \"df\", \"pvalue\", \"cfi\", \"rmsea\")))\n\n  chisq      df  pvalue     cfi   rmsea \n204.484  29.000   0.000   0.791   0.081 \n\n\n\n\n77.4.2 Interpretazione dei Parametri del Modello\nNell’output relativo alla sezione Intercepts, il parametro eta_1, con un valore di 35.267, rappresenta la previsione del punteggio in matematica al tempo iniziale $ t_0 $. Questo valore indica la media iniziale dei punteggi in matematica per gli studenti. Per quanto riguarda il parametro eta_2, il suo valore di 4.339 suggerisce che, ad ogni incremento unitario nell’arco temporale considerato, ci si aspetta un aumento medio di 4.339 punti nel punteggio predetto di matematica.\nPassando alla sezione Variances, il valore di eta_1 pari a 64.562 indica la varianza tra gli studenti nelle intercette, cioè la variabilità dei valori iniziali di matematica tra i diversi studenti. Il valore di eta_2, pari a 0.733, rappresenta invece la varianza tra gli studenti nelle pendenze, ossia la variabilità dei tassi di crescita dei punteggi in matematica tra gli studenti. Calcolando l’intervallo $ 35.267 $ e assumendo una distribuzione normale, otteniamo una stima dell’intervallo al 95% per i valori plausibili delle medie dei punteggi in matematica tra gli studenti. Questo intervallo non rappresenta un intervallo di fiducia frequentista, ma piuttosto un intervallo attorno alla stima del valore vero. Analogamente, l’intervallo $ 4.339 $ fornisce una stima dell’intervallo al 95% per i valori plausibili delle pendenze dei punteggi in matematica tra gli studenti.\nLa covarianza stimata di -0.181 (con SE = 1.150) suggerisce che non vi è una relazione significativa tra intercette e pendenze. Se la covarianza fosse stata positiva, avremmo potuto interpretarla come un’indicazione che studenti con un punteggio iniziale più alto in matematica tendono a mostrare un maggiore incremento dei punteggi nel tempo. Al contrario, una covarianza negativa tra intercetta e pendenza implicherebbe che studenti con punteggi iniziali più alti tendono a mostrare un aumento meno marcato dei punteggi nel tempo.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>77</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/07_growth_1.html#confronto-con-il-modello-a-effetti-misti",
    "href": "chapters/lgm/07_growth_1.html#confronto-con-il-modello-a-effetti-misti",
    "title": "77  Curve di crescita latente",
    "section": "77.5 Confronto con il Modello a Effetti Misti",
    "text": "77.5 Confronto con il Modello a Effetti Misti\nProcediamo ora all’analisi degli stessi dati impiegando un modello a effetti misti. Dobbiamo però tenere presente che, in questo contesto, non saremo in grado di replicare esattamente gli stessi risultati ottenuti con il modello di crescita latente (LGM), a causa della presenza di dati mancanti. Nel modello LGM, abbiamo adottato l’approccio della massima verosimiglianza (ML) per la stima dei parametri, gestendo i dati mancanti attraverso l’uso del metodo fiml (Full Information Maximum Likelihood) implementato nel software lavaan. Questo metodo non comporta l’imputazione dei dati mancanti, ma sfrutta le informazioni disponibili in ciascun caso per stimare i parametri secondo il criterio della massima verosimiglianza.\nTuttavia, quando si tratta di modelli a effetti misti, il metodo FIML non è generalmente una strategia applicabile. Di conseguenza, per procedere con l’analisi in questo contesto, adotteremo una soluzione alternativa, consistente nell’eliminazione dei casi che presentano dati mancanti. Questo approccio, sebbene meno sofisticato rispetto al FIML, ci permetterà di procedere con l’analisi del modello a effetti misti, pur con una certa limitazione dovuta alla riduzione del campione di dati disponibili.\nIn formato long, i dati sono i seguenti.\n\nnlsy_math_long |&gt;\n    head()\n\n\nA data.frame: 6 x 3\n\n\n\nid\ngrade\nmath\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n\n\n\n\n201.3\n201\n3\n38\n\n\n201.5\n201\n5\n55\n\n\n303.2\n303\n2\n26\n\n\n303.5\n303\n5\n33\n\n\n2702.2\n2702\n2\n56\n\n\n2702.4\n2702\n4\n58\n\n\n\n\n\nSottraiamo 2 dalla variabile grade in modo che il valore 0 corrisponda alla prima rilevazione temporale. In questo modo, l’intercetta rappresenterà il valore atteso del punteggio di matematica per la prima rilevazione temporale (quando grade è pari a 2).\n\nnlsy_math_long$grade_c2 &lt;- nlsy_math_long$grade - 2\n\nNel contesto del modello a effetti misti, utilizziamo la funzione lmer per adattare il modello. In questa configurazione ((1 | id)), adottiamo un modello con intercette casuali che prevede una pendenza uniforme per tutti gli individui, implicando un tasso di crescita costante per ciascuno. Questa scelta è coerente con le traiettorie di crescita illustrate nella figura precedente.\nL’utilizzo dell’opzione REML = FALSE nel modello specifica che stiamo applicando il metodo della massima verosimiglianza (ML) per la stima dei parametri, anziché l’approccio REML (Restricted Maximum Likelihood), che è il metodo predefinito nella funzione lmer.\nIn aggiunta, l’opzione na.action = na.exclude viene utilizzata per indicare che le osservazioni contenenti dati mancanti saranno escluse dall’analisi. Questo significa che tali osservazioni non contribuiranno alla stima dei parametri del modello, permettendoci di procedere con l’analisi nonostante la presenza di dati incompleti. Questo approccio, benché pratico, può avere implicazioni sulla rappresentatività e sulla generalizzabilità dei risultati, specialmente se la quantità di dati mancanti è sostanziale.\n\nfit_lmer &lt;- lmer(\n    math ~ 1 + grade_c2 + (1 | id),\n    data = nlsy_math_long,\n    REML = FALSE,\n    na.action = na.exclude\n)\n\n\nsummary(fit_lmer) |&gt;\n    print()\n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: math ~ 1 + grade_c2 + (1 | id)\n   Data: nlsy_math_long\n\n     AIC      BIC   logLik deviance df.resid \n 15957.7  15980.5  -7974.8  15949.7     2217 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2082 -0.5265  0.0081  0.5456  2.5651 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n id       (Intercept) 67.30    8.204   \n Residual             39.31    6.270   \nNumber of obs: 2221, groups:  id, 932\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 35.33081    0.36264   97.43\ngrade_c2     4.29352    0.08266   51.94\n\nCorrelation of Fixed Effects:\n         (Intr)\ngrade_c2 -0.555\n\n\nDall’output vediamo che il punteggio di matematica in corrispondenza del secondo grado scolastico (codificato qui con 0) è uguale a 35.33 (0.36). Il tasso di crescita, ovvero l’aumento atteso dei punteggi di matematica per ciascun grado scolastico è uguale a 4.29 (0.08).\nUna rappresentazione grafica dei punteggi predetti dal modello misto può essere ottenuta nel modo seguente.\n\ngr &lt;- emmeans::ref_grid(fit_lmer, cov.keep= c('grade_c2'))\nemm &lt;- emmeans(gr, spec= c('grade_c2'), level= 0.95)\n\n\nnlsy_math_long |&gt;\n    ggplot(aes(x= grade_c2, y= math)) +\n        geom_ribbon(\n            data= data.frame(emm), \n            aes(ymin= lower.CL, ymax= upper.CL, y= NULL), fill= 'grey80'\n        ) +\n        geom_line(data= data.frame(emm), aes(y= emmean)) +\n        geom_point() \n\n\n\n\n\n\n\n\nQuesti risultati, ottenuti escludendo tutte le osservazioni con dati mancanti, sono comunque molto simili ai risultati ottenuti usando lavaan (si veda la figura con le traiettorie di crescita del modello LGM).",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>77</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/07_growth_1.html#modello-di-crescita-non-lineare",
    "href": "chapters/lgm/07_growth_1.html#modello-di-crescita-non-lineare",
    "title": "77  Curve di crescita latente",
    "section": "77.6 Modello di Crescita Non Lineare",
    "text": "77.6 Modello di Crescita Non Lineare\nIn alcuni casi, può essere utile esplorare la possibilità che il cambiamento osservato segua una traiettoria non lineare. Il metodo adottato per costruire un modello di crescita non lineare può essere paragonato all’utilizzo di variabili dummy in un modello di regressione lineare. Tuttavia, in questo contesto, apportiamo una modifica specifica ai carichi fattoriali associati alla variabile latente che rappresenta la pendenza.\nNel modello di crescita non lineare, fissiamo il primo carico fattoriale a 0 e l’ultimo a 1. Questa configurazione implica che il primo punto temporale rappresenta il punto di partenza, mentre l’ultimo indica la conclusione dell’intervallo temporale considerato. I carichi fattoriali intermedi, invece, non sono fissi e vengono stimati liberamente dal modello. Questa impostazione permette di interpretare la pendenza come l’entità complessiva del cambiamento che si verifica tra l’inizio e la fine dell’intervallo temporale considerato.\nI carichi fattoriali che vengono stimati rappresentano la proporzione del cambiamento complessivo che si è verificato fino a quel particolare punto temporale, rispetto al cambiamento totale osservato durante l’intero intervallo. In altre parole, questi carichi fattoriali intermedi offrono una misura di quanto il cambiamento si sia sviluppato a ogni punto temporale intermedio, in rapporto al cambiamento totale che si è verificato dall’inizio alla fine del periodo considerato.\nAttraverso questo approccio, il modello di crescita non lineare fornisce una comprensione più dettagliata e flessibile della dinamica del cambiamento, permettendo di catturare traiettorie che potrebbero non essere adeguatamente descritte da un modello lineare.\n\nmod_nl &lt;- \"\n    i =~ 1*math2 + 1*math3 + 1*math4 + 1*math5 + 1*math6 + 1*math7 + 1*math8\n    s = ~ 0 * math2 + math3 + math4 + math5 + math6 + math7 + 1*math8\n    math2 ~~ theta*math2\n    math3 ~~ theta*math3\n    math4 ~~ theta*math4\n    math5 ~~ theta*math5\n    math6 ~~ theta*math6\n    math7 ~~ theta*math7\n    math8 ~~ theta*math8\n\"\n\nAdattiamo il modello ai dati.\n\nfit_nl &lt;- growth(\n  mod_nl,\n  data = nlsy_math_wide, \n  estimator = \"ML\",\n  missing = \"fiml\"\n)\n\nEsaminiamo la soluzione.\n\nsummary(fit_nl, fit.measures = TRUE, standardized = TRUE) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 128 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        17\n  Number of equality constraints                     6\n\n                                                  Used       Total\n  Number of observations                           932         933\n  Number of missing patterns                        60            \n\nModel Test User Model:\n                                                      \n  Test statistic                                52.947\n  Degrees of freedom                                24\n  P-value (Chi-square)                           0.001\n\nModel Test Baseline Model:\n\n  Test statistic                               862.334\n  Degrees of freedom                                21\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.966\n  Tucker-Lewis Index (TLI)                       0.970\n                                                      \n  Robust Comparative Fit Index (CFI)             1.000\n  Robust Tucker-Lewis Index (TLI)                1.023\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -7892.924\n  Loglikelihood unrestricted model (H1)      -7866.451\n                                                      \n  Akaike (AIC)                               15807.848\n  Bayesian (BIC)                             15861.059\n  Sample-size adjusted Bayesian (SABIC)      15826.124\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.036\n  90 Percent confidence interval - lower         0.023\n  90 Percent confidence interval - upper         0.049\n  P-value H_0: RMSEA &lt;= 0.050                    0.961\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n                                                      \n  Robust RMSEA                                   0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.177\n  P-value H_0: Robust RMSEA &lt;= 0.050             0.644\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.284\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.094\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i =~                                                                  \n    math2             1.000                               8.509    0.839\n    math3             1.000                               8.509    0.857\n    math4             1.000                               8.509    0.851\n    math5             1.000                               8.509    0.840\n    math6             1.000                               8.509    0.823\n    math7             1.000                               8.509    0.808\n    math8             1.000                               8.509    0.791\n  s =~                                                                  \n    math2             0.000                               0.000    0.000\n    math3             0.295    0.019   15.783    0.000    1.849    0.186\n    math4             0.533    0.019   28.588    0.000    3.346    0.335\n    math5             0.664    0.021   31.083    0.000    4.167    0.411\n    math6             0.799    0.022   36.470    0.000    5.016    0.485\n    math7             0.901    0.030   30.314    0.000    5.656    0.537\n    math8             1.000                               6.276    0.583\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i ~~                                                                  \n    s               -13.303    7.281   -1.827    0.068   -0.249   -0.249\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    i                32.400    0.474   68.399    0.000    3.808    3.808\n    s                25.539    0.731   34.916    0.000    4.070    4.070\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .math2   (thet)   30.502    1.678   18.182    0.000   30.502    0.296\n   .math3   (thet)   30.502    1.678   18.182    0.000   30.502    0.310\n   .math4   (thet)   30.502    1.678   18.182    0.000   30.502    0.305\n   .math5   (thet)   30.502    1.678   18.182    0.000   30.502    0.297\n   .math6   (thet)   30.502    1.678   18.182    0.000   30.502    0.286\n   .math7   (thet)   30.502    1.678   18.182    0.000   30.502    0.275\n   .math8   (thet)   30.502    1.678   18.182    0.000   30.502    0.264\n    i                72.408    6.590   10.988    0.000    1.000    1.000\n    s                39.385   11.371    3.464    0.001    1.000    1.000\n\n\n\nEffettuiamo il test del rapporto di verosimiglianze per confrontare il modello di crescita lineare con quello che assume una crescita non lineare.\n\nlavTestLRT(fit_m1, fit_nl) |&gt;\n    print()\n\n\nChi-Squared Difference Test\n\n       Df   AIC   BIC   Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)    \nfit_nl 24 15808 15861  52.947                                          \nfit_m1 29 15949 15978 204.484     151.54 0.17733       5  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIl risultato indica che il modello che assume un cambiamento non lineare si adatta meglio ai dati. Possiamo visualizzare il cambiamento nel modo seguente.\n\n# extract just th eloadings of the slopes\nloadings &lt;- parameterestimates(fit_nl) %&gt;% # get estimates\n  filter(lhs == \"s\", op == \"=~\") %&gt;% # filter the rows we want\n  .[[\"est\"]] # extract \"est\" variable\n# print result\nprint(loadings)\n\n[1] 0.0000000 0.2946469 0.5331544 0.6640287 0.7992433 0.9012769 1.0000000\n\n\n\n# predict scores\npred_lgm3 &lt;- predict(fit_nl)\n# create long data for each individual\npred_lgm3_long &lt;- map(loadings, # loop over time\n                      function(x) pred_lgm3[, 1] + \n                        x * pred_lgm3[, 2]) %&gt;% \n  reduce(cbind) %&gt;% # bring together the wave predictions \n  as.data.frame()\n\n\n# predict scores\npred_lgm3 &lt;- predict(fit_nl)\n# create long data for each individual\npred_lgm3_long &lt;- map(loadings, # loop over time\n                      function(x) pred_lgm3[, 1] + \n                        x * pred_lgm3[, 2]) %&gt;% \n  reduce(cbind) %&gt;% # bring together the wave predictions \n  as.data.frame() %&gt;% # make data frame\n  setNames(str_c(\"Grade \", 1:7)) %&gt;% # give names to variables\n  mutate(id = row_number()) %&gt;% # make unique id\n  gather(-id, key = grade, value = pred) # make long format\npred_lgm3_long %&gt;% \n  ggplot(aes(grade, pred, group = id)) + # what variables to plot?\n  geom_line(alpha = 0.05) + # add a transparent line for each person\n  stat_summary( # add average line\n    aes(group = 1),\n    fun = mean,\n    geom = \"line\",\n    size = 1.5,\n    color = \"green\"\n  ) + \n  stat_summary(data = pred_lgm3_long, # add average from linear model\n               aes(group = 1),\n               fun = mean,\n               geom = \"line\",\n               size = 1.5,\n               color = \"red\",\n               alpha = 0.5\n  ) +\n  stat_summary(data = pred_lgm3_long, # add average from squared model\n               aes(group = 1),\n               fun = mean,\n               geom = \"line\",\n               size = 1.5,\n               color = \"blue\",\n               alpha = 0.5\n  ) +\n  labs(y = \"Predicted PIAT Mathematics\", # labels\n       x = \"Grade at Testing\")",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>77</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/07_growth_1.html#riflessioni-finali",
    "href": "chapters/lgm/07_growth_1.html#riflessioni-finali",
    "title": "77  Curve di crescita latente",
    "section": "77.7 Riflessioni Finali",
    "text": "77.7 Riflessioni Finali\nQuesto capitolo ha esplorato l’implementazione e l’adattamento dei modelli di crescita lineare all’interno del framework della modellizzazione delle equazioni strutturali (SEM), utilizzando il pacchetto lavaan in R. Abbiamo illustrato come calcolare e visualizzare graficamente le traiettorie di crescita predette da questi modelli.\nI modelli di crescita lineare rappresentano un punto di partenza essenziale per analizzare il cambiamento individuale nel tempo. Tuttavia, possono non essere sempre in grado di descrivere accuratamente il processo di cambiamento. Per questa ragione, è opportuno valutare anche altri modelli e, eventualmente, esaminare le variazioni tra diversi gruppi. L’impiego dei modelli di crescita all’interno dei framework SEM e dei modelli a effetti misti presenta sia vantaggi sia limitazioni. Ad esempio, i modelli SEM offrono indici di adattamento globale quali RMSEA, CFI e TLI, che non sono disponibili nell’approccio dei modelli a effetti misti, i quali si basano piuttosto su criteri come AIC e BIC e su strumenti diagnostici quali i grafici dei residui.\nUn aspetto cruciale nell’adattamento dei modelli di crescita lineare è la scelta della metrica temporale. Nel nostro esempio, abbiamo utilizzato il grado scolastico come indicatore temporale, ma esistono altre opzioni possibili. Ad esempio, l’età al momento del test potrebbe essere una metrica più appropriata, in quanto potrebbe riflettere più accuratamente gli intervalli tra le misurazioni. Si deve inoltre considerare che l’utilizzo del grado scolastico può avere delle limitazioni, ad esempio in casi di studenti che ripetono o saltano un anno.\nLa posizione dell’intercetta può essere scelta in qualsiasi punto del continuum temporale. Nel nostro esempio, abbiamo centrato l’intercetta sulla valutazione della seconda elementare, in quanto era il primo dato disponibile. Tuttavia, è importante selezionare un punto di origine che sia significativo per lo studio specifico. Per esempio, posizionare l’intercetta alla fine dell’ottava elementare potrebbe essere rilevante per studi che mirano a valutare la preparazione degli studenti per la scuola superiore.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>77</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/08_growth_cont.html",
    "href": "chapters/lgm/08_growth_cont.html",
    "title": "78  Il tempo su una metrica continua",
    "section": "",
    "text": "78.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nQuando si affronta lo studio del cambiamento in un individuo, un aspetto cruciale è la selezione di una scala temporale adeguata per osservare questo cambiamento. Nel capitolo precedente, ad esempio, abbiamo adottato il grado scolastico come nostra scala temporale di riferimento, strutturando le osservazioni su questa base. Tuttavia, il grado scolastico non è l’unica scala temporale applicabile a tali dati. Altre scale significative potrebbero essere l’età o le specifiche occasioni in cui si effettuano le misurazioni.\nCi sono scale temporali che rappresentano intervalli discreti, come le occasioni di misurazione, dove i valori assunti sono specifici e comuni tra i partecipanti. In questo contesto, però, potrebbe non essere possibile valutare ogni partecipante ad ogni occasione di misurazione. D’altra parte, esistono scale temporali più fluide, come l’età, dove i valori sono unici per ciascun partecipante e non condivisi.\nInteressante è notare come la stessa scala temporale possa essere impiegata sia in un contesto discreto che continuo. Ad esempio, l’età può essere approssimata all’anno più vicino, mentre il grado scolastico può essere definito più precisamente, considerando l’anno scolastico e il numero di giorni trascorsi dall’inizio dell’anno scolastico.\nIn questo capitolo, ci concentreremo sulle tecniche per modellare la crescita individuale utilizzando una scala temporale continua, esplorando come questa possa fornire una comprensione più dettagliata e sfumata del cambiamento all’interno della persona.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>78</span>  <span class='chapter-title'>Il tempo su una metrica continua</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/08_growth_cont.html#lapplicazione-della-finestra-temporale",
    "href": "chapters/lgm/08_growth_cont.html#lapplicazione-della-finestra-temporale",
    "title": "78  Il tempo su una metrica continua",
    "section": "78.2 L’Applicazione della Finestra Temporale",
    "text": "78.2 L’Applicazione della Finestra Temporale\nProseguendo nell’analisi delle metriche del tempo, un approccio interessante è quello della finestra temporale, particolarmente utile per dati che presentano occasioni di misurazione variabili individualmente. Questa metodologia cerca di standardizzare la variabilità temporale individuale su una scala temporale discreta. Un esempio pratico di questo può essere visto nell’arrotondamento dell’età o del tempo al semestre o al quarto d’anno più vicino.\nQuesto metodo, benché utile, rappresenta ancora un’approssimazione della realtà temporale. Riducendo la dimensione delle finestre temporali si può aumentare la precisione, ma questo può comportare una maggiore dispersione dei dati, rendendo così più complessa l’accuratezza delle stime.\nNell’applicazione pratica di questo esempio, definiamo le finestre temporali in termini di semestri. Pertanto, lavoriamo con i dati in formato long, arrotondando l’età al semestre più vicino, e successivamente convertiamo questi dati in formato wide. Questo consente la loro integrazione nel framework SEM, facilitando l’analisi e l’interpretazione dei cambiamenti individuali nel tempo.\nPer questo esempio considereremo i dati di abilità matematica NLSY-CYA Long Data [si veda {cite:t}grimm2016growth]. Iniziamo a leggere i dati.\n\n#set filepath for data file\nfilepath &lt;- \"https://raw.githubusercontent.com/LRI-2/Data/main/GrowthModeling/nlsy_math_long_R.dat\"\n#read in the text data file using the url() function\ndat &lt;- read.table(file=url(filepath),\n                  na.strings = \".\")  #indicates the missing data designator\n#copy data with new name \nnlsy_math_long &lt;- dat  \n\n#Add names the columns of the data set\nnames(nlsy_math_long) = c('id'     , 'female', 'lb_wght', \n                          'anti_k1', 'math'  , 'grade'  ,\n                          'occ'    , 'age'   , 'men'    ,\n                          'spring' , 'anti')\n\n#subset to the variables of interest\nnlsy_math_long &lt;- nlsy_math_long[ ,c(\"id\", \"math\", \"grade\", \"age\")]\n#view the first few observations in the data set \nhead(nlsy_math_long, 10)\n\n\nA data.frame: 10 x 4\n\n\n\nid\nmath\ngrade\nage\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n201\n38\n3\n111\n\n\n2\n201\n55\n5\n135\n\n\n3\n303\n26\n2\n121\n\n\n4\n303\n33\n5\n145\n\n\n5\n2702\n56\n2\n100\n\n\n6\n2702\n58\n4\n125\n\n\n7\n2702\n80\n8\n173\n\n\n8\n4303\n41\n3\n115\n\n\n9\n4303\n58\n4\n135\n\n\n10\n5002\n46\n4\n117\n\n\n\n\n\n\n#intraindividual change trajetories\nggplot(data=nlsy_math_long,                    #data set\n       aes(x = age, y = math, group = id)) + #setting variables\n  geom_point(size=.5) + #adding points to plot\n  geom_line(alpha = 0.5) +  #adding lines to plot\n  #setting the x-axis with breaks and labels\n  scale_x_continuous(#limits=c(2,8),\n                     #breaks = c(2,3,4,5,6,7,8), \n                     name = \"Age at Testing\") +    \n  #setting the y-axis with limits breaks and labels\n  scale_y_continuous(limits=c(10,90), \n                     breaks = c(10,30,50,70,90), \n                     name = \"PIAT Mathematics\")\n\n\n\n\n\n\n\n\nImplementiamo il metodo della finestra temporale e ricodifichiamo i dati in formato wide.\n\n# creating new age variable scaled in years\nnlsy_math_long$ageyr &lt;- (nlsy_math_long$age / 12)\nhead(nlsy_math_long)\n\n\nA data.frame: 6 x 5\n\n\n\nid\nmath\ngrade\nage\nageyr\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n201\n38\n3\n111\n9.250000\n\n\n2\n201\n55\n5\n135\n11.250000\n\n\n3\n303\n26\n2\n121\n10.083333\n\n\n4\n303\n33\n5\n145\n12.083333\n\n\n5\n2702\n56\n2\n100\n8.333333\n\n\n6\n2702\n58\n4\n125\n10.416667\n\n\n\n\n\n\n# rounding to nearest half-year\n# multiplied by 10 to remove decimal for easy conversion to wide\nnlsy_math_long$agewindow &lt;- plyr::round_any(nlsy_math_long$ageyr * 10, 5)\nhead(nlsy_math_long)\n\n\nA data.frame: 6 x 6\n\n\n\nid\nmath\ngrade\nage\nageyr\nagewindow\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n201\n38\n3\n111\n9.250000\n90\n\n\n2\n201\n55\n5\n135\n11.250000\n110\n\n\n3\n303\n26\n2\n121\n10.083333\n100\n\n\n4\n303\n33\n5\n145\n12.083333\n120\n\n\n5\n2702\n56\n2\n100\n8.333333\n85\n\n\n6\n2702\n58\n4\n125\n10.416667\n105\n\n\n\n\n\n\n# reshaping long to wide (just variables of interest)\nnlsy_math_wide &lt;- reshape(\n  data = nlsy_math_long[, c(\"id\", \"math\", \"agewindow\")],\n  timevar = c(\"agewindow\"),\n  idvar = c(\"id\"),\n  v.names = c(\"math\"),\n  direction = \"wide\", sep = \"\"\n)\n\n# reordering columns for easy viewing\nnlsy_math_wide &lt;- nlsy_math_wide[, c(\n  \"id\", \"math70\", \"math75\", \"math80\", \"math85\", \"math90\", \"math95\", \"math100\", \"math105\", \"math110\", \"math115\", \"math120\", \"math125\", \"math130\", \"math135\", \"math140\", \"math145\"\n)]\n# looking at the data\nhead(nlsy_math_wide)\n\n\nA data.frame: 6 x 17\n\n\n\nid\nmath70\nmath75\nmath80\nmath85\nmath90\nmath95\nmath100\nmath105\nmath110\nmath115\nmath120\nmath125\nmath130\nmath135\nmath140\nmath145\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n201\nNA\nNA\nNA\nNA\n38\nNA\nNA\nNA\n55\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n3\n303\nNA\nNA\nNA\nNA\nNA\nNA\n26\nNA\nNA\nNA\n33\nNA\nNA\nNA\nNA\nNA\n\n\n5\n2702\nNA\nNA\nNA\n56\nNA\nNA\nNA\n58\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n80\n\n\n8\n4303\nNA\nNA\nNA\nNA\nNA\n41\nNA\nNA\n58\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n10\n5002\nNA\nNA\nNA\nNA\nNA\nNA\n46\nNA\nNA\nNA\n54\nNA\nNA\nNA\n66\nNA\n\n\n13\n5005\nNA\nNA\n35\nNA\nNA\n50\nNA\nNA\nNA\n60\nNA\nNA\nNA\n59\nNA\nNA\n\n\n\n\n\nSpecifichiamo il modello SEM.\n\nlg_math_age_lavaan_model &lt;- '\n  # latent variable definitions\n      #intercept (note intercept is a reserved term)\n      eta_1 =~  1*math70 +\n                1*math75 +\n                1*math80 +\n                1*math85 +\n                1*math90 +\n                1*math95 +\n                1*math100 +\n                1*math105 +\n                1*math110 +\n                1*math115 +\n                1*math120 +\n                1*math125 +\n                1*math130 +\n                1*math135 +\n                1*math140 +\n                1*math145 \n\n      #linear slope (note intercept is a reserved term)\n      eta_2 =~ -1*math70 +\n               -0.5*math75 +\n                0*math80 +\n                0.5*math85 +\n                1*math90 +\n                1.5*math95 +\n                2*math100 +\n                2.5*math105 +\n                3*math110 +\n                3.5*math115 +\n                4*math120 +\n                4.5*math125 +\n                5*math130 +\n                5.5*math135 +\n                6*math140 +\n                6.5*math145\n\n  # factor variances\n      eta_1 ~~ start(65)*eta_1\n      eta_2 ~~ start(.75)*eta_2\n\n  # covariances among factors \n      eta_1 ~~ start(1.2)*eta_2\n\n  # manifest variances (made equivalent by naming theta)\n      math70 ~~ start(35)*theta*math70\n      math75 ~~ theta*math75\n      math80 ~~ theta*math80\n      math85 ~~ theta*math85\n      math90 ~~ theta*math90\n      math95 ~~ theta*math95\n      math100 ~~ theta*math100\n      math105 ~~ theta*math105\n      math110 ~~ theta*math110\n      math115 ~~ theta*math115\n      math120 ~~ theta*math120\n      math125 ~~ theta*math125\n      math130 ~~ theta*math130\n      math135 ~~ theta*math135\n      math140 ~~ theta*math140\n      math145 ~~ theta*math145\n      \n  # manifest means (fixed at zero)\n      math70 ~ 0*1\n      math75 ~ 0*1\n      math80 ~ 0*1\n      math85 ~ 0*1\n      math90 ~ 0*1\n      math95 ~ 0*1\n      math100 ~ 0*1\n      math105 ~ 0*1\n      math110 ~ 0*1\n      math115 ~ 0*1\n      math120 ~ 0*1\n      math125 ~ 0*1\n      math130 ~ 0*1\n      math135 ~ 0*1\n      math140 ~ 0*1\n      math145 ~ 0*1\n\n  # factor means (estimated freely)\n      eta_1 ~ start(35)*1\n      eta_2 ~ start(4)*1\n' #end of model definition\n\nIn questo modello, si definiscono due variabili latenti: l’intercetta latente (eta_1) e la pendenza lineare (eta_2). La scelta dei coefficienti per eta_2 consente di modellare una traiettoria di crescita lineare nel tempo. Ogni coefficiente corrisponde al peso assegnato a ciascuna misura di matematica (math70, math75, …, math145) nell’espressione della pendenza lineare.\n\nI coefficienti vanno da -1 a 6.5, aumentando di 0.5 ad ogni passaggio. Questa progressione rappresenta l’aumento lineare nel tempo. Ad esempio, math70 ha un coefficiente di -1, math75 ha un coefficiente di -0.5, e così via fino a math145, che ha un coefficiente di 6.5.\nI coefficienti sono scelti per mantenere una distanza temporale costante tra ogni punto di misurazione. Ad esempio, la differenza di 0.5 tra i coefficienti di math70 e math75 implica che il lasso di tempo tra queste due misurazioni è costante rispetto alle altre misurazioni.\nÈ interessante notare che il coefficiente per math80 è 0. Questo implica che math80 è stato scelto come punto di riferimento o centro per la pendenza lineare. I valori negativi e positivi dei coefficienti rappresentano misurazioni prima e dopo questo punto di riferimento, rispettivamente.\n\nAdattiamo il modello ai dati.\n\n#estimating the model using sem() function\nlg_math_age_lavaan_fit &lt;- sem(lg_math_age_lavaan_model,\n  data = nlsy_math_wide,\n  meanstructure = TRUE,\n  estimator = \"ML\",\n  missing = \"fiml\"\n)\n\nEsaminiamo la soluzione.\n\nsummary(lg_math_age_lavaan_fit, fit.measures = TRUE) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 40 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        21\n  Number of equality constraints                    15\n\n  Number of observations                           932\n  Number of missing patterns                       139\n\nModel Test User Model:\n                                                      \n  Test statistic                               295.028\n  Degrees of freedom                               146\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              1053.342\n  Degrees of freedom                               120\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.840\n  Tucker-Lewis Index (TLI)                       0.869\n                                                      \n  Robust Comparative Fit Index (CFI)             0.003\n  Robust Tucker-Lewis Index (TLI)                0.181\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -7928.559\n  Loglikelihood unrestricted model (H1)      -7781.045\n                                                      \n  Akaike (AIC)                               15869.117\n  Bayesian (BIC)                             15898.141\n  Sample-size adjusted Bayesian (SABIC)      15879.086\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.033\n  90 Percent confidence interval - lower         0.028\n  90 Percent confidence interval - upper         0.039\n  P-value H_0: RMSEA &lt;= 0.050                    1.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n                                                      \n  Robust RMSEA                                   4.193\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.000\n  P-value H_0: Robust RMSEA &lt;= 0.050               NaN\n  P-value H_0: Robust RMSEA &gt;= 0.080               NaN\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.314\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 =~                                            \n    math70            1.000                           \n    math75            1.000                           \n    math80            1.000                           \n    math85            1.000                           \n    math90            1.000                           \n    math95            1.000                           \n    math100           1.000                           \n    math105           1.000                           \n    math110           1.000                           \n    math115           1.000                           \n    math120           1.000                           \n    math125           1.000                           \n    math130           1.000                           \n    math135           1.000                           \n    math140           1.000                           \n    math145           1.000                           \n  eta_2 =~                                            \n    math70           -1.000                           \n    math75           -0.500                           \n    math80            0.000                           \n    math85            0.500                           \n    math90            1.000                           \n    math95            1.500                           \n    math100           2.000                           \n    math105           2.500                           \n    math110           3.000                           \n    math115           3.500                           \n    math120           4.000                           \n    math125           4.500                           \n    math130           5.000                           \n    math135           5.500                           \n    math140           6.000                           \n    math145           6.500                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 ~~                                            \n    eta_2             1.157    1.010    1.146    0.252\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .math70            0.000                           \n   .math75            0.000                           \n   .math80            0.000                           \n   .math85            0.000                           \n   .math90            0.000                           \n   .math95            0.000                           \n   .math100           0.000                           \n   .math105           0.000                           \n   .math110           0.000                           \n   .math115           0.000                           \n   .math120           0.000                           \n   .math125           0.000                           \n   .math130           0.000                           \n   .math135           0.000                           \n   .math140           0.000                           \n   .math145           0.000                           \n    eta_1            35.236    0.347  101.512    0.000\n    eta_2             4.229    0.081   51.910    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1            65.063    5.503   11.824    0.000\n    eta_2             0.725    0.277    2.616    0.009\n   .math70  (thet)   32.337    1.695   19.083    0.000\n   .math75  (thet)   32.337    1.695   19.083    0.000\n   .math80  (thet)   32.337    1.695   19.083    0.000\n   .math85  (thet)   32.337    1.695   19.083    0.000\n   .math90  (thet)   32.337    1.695   19.083    0.000\n   .math95  (thet)   32.337    1.695   19.083    0.000\n   .math100 (thet)   32.337    1.695   19.083    0.000\n   .math105 (thet)   32.337    1.695   19.083    0.000\n   .math110 (thet)   32.337    1.695   19.083    0.000\n   .math115 (thet)   32.337    1.695   19.083    0.000\n   .math120 (thet)   32.337    1.695   19.083    0.000\n   .math125 (thet)   32.337    1.695   19.083    0.000\n   .math130 (thet)   32.337    1.695   19.083    0.000\n   .math135 (thet)   32.337    1.695   19.083    0.000\n   .math140 (thet)   32.337    1.695   19.083    0.000\n   .math145 (thet)   32.337    1.695   19.083    0.000\n\n\n\n\nparameterEstimates(lg_math_age_lavaan_fit) |&gt;\n    print()\n\n       lhs op     rhs label    est    se       z pvalue ci.lower ci.upper\n1    eta_1 =~  math70        1.000 0.000      NA     NA    1.000    1.000\n2    eta_1 =~  math75        1.000 0.000      NA     NA    1.000    1.000\n3    eta_1 =~  math80        1.000 0.000      NA     NA    1.000    1.000\n4    eta_1 =~  math85        1.000 0.000      NA     NA    1.000    1.000\n5    eta_1 =~  math90        1.000 0.000      NA     NA    1.000    1.000\n6    eta_1 =~  math95        1.000 0.000      NA     NA    1.000    1.000\n7    eta_1 =~ math100        1.000 0.000      NA     NA    1.000    1.000\n8    eta_1 =~ math105        1.000 0.000      NA     NA    1.000    1.000\n9    eta_1 =~ math110        1.000 0.000      NA     NA    1.000    1.000\n10   eta_1 =~ math115        1.000 0.000      NA     NA    1.000    1.000\n11   eta_1 =~ math120        1.000 0.000      NA     NA    1.000    1.000\n12   eta_1 =~ math125        1.000 0.000      NA     NA    1.000    1.000\n13   eta_1 =~ math130        1.000 0.000      NA     NA    1.000    1.000\n14   eta_1 =~ math135        1.000 0.000      NA     NA    1.000    1.000\n15   eta_1 =~ math140        1.000 0.000      NA     NA    1.000    1.000\n16   eta_1 =~ math145        1.000 0.000      NA     NA    1.000    1.000\n17   eta_2 =~  math70       -1.000 0.000      NA     NA   -1.000   -1.000\n18   eta_2 =~  math75       -0.500 0.000      NA     NA   -0.500   -0.500\n19   eta_2 =~  math80        0.000 0.000      NA     NA    0.000    0.000\n20   eta_2 =~  math85        0.500 0.000      NA     NA    0.500    0.500\n21   eta_2 =~  math90        1.000 0.000      NA     NA    1.000    1.000\n22   eta_2 =~  math95        1.500 0.000      NA     NA    1.500    1.500\n23   eta_2 =~ math100        2.000 0.000      NA     NA    2.000    2.000\n24   eta_2 =~ math105        2.500 0.000      NA     NA    2.500    2.500\n25   eta_2 =~ math110        3.000 0.000      NA     NA    3.000    3.000\n26   eta_2 =~ math115        3.500 0.000      NA     NA    3.500    3.500\n27   eta_2 =~ math120        4.000 0.000      NA     NA    4.000    4.000\n28   eta_2 =~ math125        4.500 0.000      NA     NA    4.500    4.500\n29   eta_2 =~ math130        5.000 0.000      NA     NA    5.000    5.000\n30   eta_2 =~ math135        5.500 0.000      NA     NA    5.500    5.500\n31   eta_2 =~ math140        6.000 0.000      NA     NA    6.000    6.000\n32   eta_2 =~ math145        6.500 0.000      NA     NA    6.500    6.500\n33   eta_1 ~~   eta_1       65.063 5.503  11.824  0.000   54.278   75.849\n34   eta_2 ~~   eta_2        0.725 0.277   2.616  0.009    0.182    1.268\n35   eta_1 ~~   eta_2        1.157 1.010   1.146  0.252   -0.822    3.136\n36  math70 ~~  math70 theta 32.337 1.695  19.083  0.000   29.016   35.658\n37  math75 ~~  math75 theta 32.337 1.695  19.083  0.000   29.016   35.658\n38  math80 ~~  math80 theta 32.337 1.695  19.083  0.000   29.016   35.658\n39  math85 ~~  math85 theta 32.337 1.695  19.083  0.000   29.016   35.658\n40  math90 ~~  math90 theta 32.337 1.695  19.083  0.000   29.016   35.658\n41  math95 ~~  math95 theta 32.337 1.695  19.083  0.000   29.016   35.658\n42 math100 ~~ math100 theta 32.337 1.695  19.083  0.000   29.016   35.658\n43 math105 ~~ math105 theta 32.337 1.695  19.083  0.000   29.016   35.658\n44 math110 ~~ math110 theta 32.337 1.695  19.083  0.000   29.016   35.658\n45 math115 ~~ math115 theta 32.337 1.695  19.083  0.000   29.016   35.658\n46 math120 ~~ math120 theta 32.337 1.695  19.083  0.000   29.016   35.658\n47 math125 ~~ math125 theta 32.337 1.695  19.083  0.000   29.016   35.658\n48 math130 ~~ math130 theta 32.337 1.695  19.083  0.000   29.016   35.658\n49 math135 ~~ math135 theta 32.337 1.695  19.083  0.000   29.016   35.658\n50 math140 ~~ math140 theta 32.337 1.695  19.083  0.000   29.016   35.658\n51 math145 ~~ math145 theta 32.337 1.695  19.083  0.000   29.016   35.658\n52  math70 ~1                0.000 0.000      NA     NA    0.000    0.000\n53  math75 ~1                0.000 0.000      NA     NA    0.000    0.000\n54  math80 ~1                0.000 0.000      NA     NA    0.000    0.000\n55  math85 ~1                0.000 0.000      NA     NA    0.000    0.000\n56  math90 ~1                0.000 0.000      NA     NA    0.000    0.000\n57  math95 ~1                0.000 0.000      NA     NA    0.000    0.000\n58 math100 ~1                0.000 0.000      NA     NA    0.000    0.000\n59 math105 ~1                0.000 0.000      NA     NA    0.000    0.000\n60 math110 ~1                0.000 0.000      NA     NA    0.000    0.000\n61 math115 ~1                0.000 0.000      NA     NA    0.000    0.000\n62 math120 ~1                0.000 0.000      NA     NA    0.000    0.000\n63 math125 ~1                0.000 0.000      NA     NA    0.000    0.000\n64 math130 ~1                0.000 0.000      NA     NA    0.000    0.000\n65 math135 ~1                0.000 0.000      NA     NA    0.000    0.000\n66 math140 ~1                0.000 0.000      NA     NA    0.000    0.000\n67 math145 ~1                0.000 0.000      NA     NA    0.000    0.000\n68   eta_1 ~1               35.236 0.347 101.512  0.000   34.556   35.917\n69   eta_2 ~1                4.229 0.081  51.910  0.000    4.069    4.389\n\n\n\ninspect(lg_math_age_lavaan_fit, what=\"est\") |&gt;\n    print()\n\n$lambda\n        eta_1 eta_2\nmath70      1  -1.0\nmath75      1  -0.5\nmath80      1   0.0\nmath85      1   0.5\nmath90      1   1.0\nmath95      1   1.5\nmath100     1   2.0\nmath105     1   2.5\nmath110     1   3.0\nmath115     1   3.5\nmath120     1   4.0\nmath125     1   4.5\nmath130     1   5.0\nmath135     1   5.5\nmath140     1   6.0\nmath145     1   6.5\n\n$theta\n        math70 math75 math80 math85 math90 math95 mth100 mth105 mth110\nmath70  32.337                                                        \nmath75   0.000 32.337                                                 \nmath80   0.000  0.000 32.337                                          \nmath85   0.000  0.000  0.000 32.337                                   \nmath90   0.000  0.000  0.000  0.000 32.337                            \nmath95   0.000  0.000  0.000  0.000  0.000 32.337                     \nmath100  0.000  0.000  0.000  0.000  0.000  0.000 32.337              \nmath105  0.000  0.000  0.000  0.000  0.000  0.000  0.000 32.337       \nmath110  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 32.337\nmath115  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\nmath120  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\nmath125  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\nmath130  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\nmath135  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\nmath140  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\nmath145  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n        mth115 mth120 mth125 mth130 mth135 mth140 mth145\nmath70                                                  \nmath75                                                  \nmath80                                                  \nmath85                                                  \nmath90                                                  \nmath95                                                  \nmath100                                                 \nmath105                                                 \nmath110                                                 \nmath115 32.337                                          \nmath120  0.000 32.337                                   \nmath125  0.000  0.000 32.337                            \nmath130  0.000  0.000  0.000 32.337                     \nmath135  0.000  0.000  0.000  0.000 32.337              \nmath140  0.000  0.000  0.000  0.000  0.000 32.337       \nmath145  0.000  0.000  0.000  0.000  0.000  0.000 32.337\n\n$psi\n       eta_1  eta_2\neta_1 65.063       \neta_2  1.157  0.725\n\n$nu\n        intrcp\nmath70       0\nmath75       0\nmath80       0\nmath85       0\nmath90       0\nmath95       0\nmath100      0\nmath105      0\nmath110      0\nmath115      0\nmath120      0\nmath125      0\nmath130      0\nmath135      0\nmath140      0\nmath145      0\n\n$alpha\n      intrcp\neta_1 35.236\neta_2  4.229\n\n\n\nCreiamo un diagramma di percorso.\n\nlg_math_age_lavaan_fit |&gt;\n    semPaths(\n        style = \"ram\",\n        whatLabels = \"par\", edge.label.cex = .6,\n        label.prop = 0.9, edge.label.color = \"black\", rotation = 4,\n        equalizeManifests = FALSE, optimizeLatRes = TRUE, node.width = 1.5,\n        edge.width = 0.5, shapeMan = \"rectangle\", shapeLat = \"ellipse\",\n        shapeInt = \"triangle\", sizeMan = 4, sizeInt = 2, sizeLat = 4,\n        curve = 2, unCol = \"#070b8c\"\n    )",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>78</span>  <span class='chapter-title'>Il tempo su una metrica continua</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/09_time_inv_cov.html",
    "href": "chapters/lgm/09_time_inv_cov.html",
    "title": "79  Covariate indipendenti dal tempo",
    "section": "",
    "text": "79.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nNell’ambito degli studi sui modelli di crescita lineare, una questione rilevante è capire in che modo le differenze individuali nelle traiettorie di cambiamento siano influenzate da altre variabili. Il presente capitolo si dedica all’integrazione di covariate invarianti nel tempo in questi modelli di crescita.\nLe covariate invarianti nel tempo sono quelle variabili che rimangono costanti per ogni individuo durante il periodo di studio. Esempi tipici includono il genere, le condizioni sperimentali, lo stato socio-economico, e altri attributi che non subiscono modifiche nel tempo. In termini di modellazione, queste variabili vengono trattate come fattori indipendenti in un modello di regressione multipla, dove l’intercetta e la pendenza del modello di crescita lineare fungono da variabili dipendenti. Le covariate possono assumere vari formati: possono essere continue (es. età), ordinali (es. livelli di istruzione) o categoriche (es. genere).\nIncludere covariate invarianti nel tempo permette di esplorare come le differenze individuali nella traiettoria di crescita (sia in termini di intercetta che di pendenza) siano associate a queste variabili. Questo approccio offre la possibilità di indagare le ragioni sottostanti le diverse modalità di cambiamento tra gli individui.\nIn sintesi, l’integrazione di covariate invarianti nel tempo nei modelli di crescita lineare fornisce una visione più dettagliata delle dinamiche individuali e del modo in cui vari fattori possono influenzare le traiettorie di crescita. Questo approccio arricchisce la comprensione dei fenomeni studiati, pur richiedendo un’interpretazione cauta e informata dei risultati ottenuti.\nPer questo esempio considereremo i dati di prestazione matematica dal data set NLSY-CYA Long Data [si veda {cite:t}grimm2016growth]. Iniziamo a leggere i dati.\n# set filepath for data file\nfilepath &lt;- \"https://raw.githubusercontent.com/LRI-2/Data/main/GrowthModeling/nlsy_math_long_R.dat\"\n# read in the text data file using the url() function\ndat &lt;- read.table(\n  file = url(filepath),\n  na.strings = \".\"\n) # indicates the missing data designator\n# copy data with new name\nnlsy_math_long &lt;- dat\n\n# Add names the columns of the data set\nnames(nlsy_math_long) &lt;- c(\n  \"id\", \"female\", \"lb_wght\",\n  \"anti_k1\", \"math\", \"grade\",\n  \"occ\", \"age\", \"men\",\n  \"spring\", \"anti\"\n)\n\n# view the first few observations in the data set\nhead(nlsy_math_long, 10)\n\n\nA data.frame: 10 x 11\n\n\n\nid\nfemale\nlb_wght\nanti_k1\nmath\ngrade\nocc\nage\nmen\nspring\nanti\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n201\n1\n0\n0\n38\n3\n2\n111\n0\n1\n0\n\n\n2\n201\n1\n0\n0\n55\n5\n3\n135\n1\n1\n0\n\n\n3\n303\n1\n0\n1\n26\n2\n2\n121\n0\n1\n2\n\n\n4\n303\n1\n0\n1\n33\n5\n3\n145\n0\n1\n2\n\n\n5\n2702\n0\n0\n0\n56\n2\n2\n100\nNA\n1\n0\n\n\n6\n2702\n0\n0\n0\n58\n4\n3\n125\nNA\n1\n2\n\n\n7\n2702\n0\n0\n0\n80\n8\n4\n173\nNA\n1\n2\n\n\n8\n4303\n1\n0\n0\n41\n3\n2\n115\n0\n0\n1\n\n\n9\n4303\n1\n0\n0\n58\n4\n3\n135\n0\n1\n2\n\n\n10\n5002\n0\n0\n4\n46\n4\n2\n117\nNA\n1\n4\nnlsy_math_long |&gt;\n  ggplot(\n    aes(grade, math, group = id)\n  ) +\n  geom_line(alpha = 0.3) + # add individual line with transparency\n  stat_summary( # add average line\n    aes(group = 1),\n    fun = mean,\n    geom = \"line\",\n    linewidth = 1.5,\n    color = \"blue\"\n  ) +\n  labs(x = \"Grade at testing\", y = \"PAT Mathematics\")\nPer ottenere una visione più dettagliata dei cambiamenti a livello individuale, possiamo selezionare casualmente un campione di 20 individui e registrare, per ciascuno di essi, l’evoluzione dei loro punteggi in matematica nel tempo.\n# sample 20 ids\npeople &lt;- unique(nlsy_math_long$id) %&gt;% sample(20)\n# do separate graph for each individual\nnlsy_math_long %&gt;% \n  filter(id %in% people) %&gt;%  # filter only sampled cases\n  ggplot(aes(grade, math, group = 1)) +\n  geom_line() +\n  facet_wrap(~id) + # a graph for each individual\n  labs(x = \"Grade at testing\", y = \"PAT Mathematics\")\nPer semplicità, leggiamo gli stessi dati in formato wide da un file.\n# set filepath for data file\nfilepath &lt;- \"https://raw.githubusercontent.com/LRI-2/Data/main/GrowthModeling/nlsy_math_wide_R.dat\"\n\n# read in the text data file using the url() function\ndat &lt;- read.table(\n  file = url(filepath),\n  na.strings = \".\"\n) # indicates the missing data designator\n\n# copy data with new name\nnlsy_math_wide &lt;- dat\n\n# Give the variable names\nnames(nlsy_math_wide) &lt;- c(\n  \"id\", \"female\", \"lb_wght\", \"anti_k1\",\n  \"math2\", \"math3\", \"math4\", \"math5\", \"math6\", \"math7\", \"math8\",\n  \"age2\", \"age3\", \"age4\", \"age5\", \"age6\", \"age7\", \"age8\",\n  \"men2\", \"men3\", \"men4\", \"men5\", \"men6\", \"men7\", \"men8\",\n  \"spring2\", \"spring3\", \"spring4\", \"spring5\", \"spring6\", \"spring7\", \"spring8\",\n  \"anti2\", \"anti3\", \"anti4\", \"anti5\", \"anti6\", \"anti7\", \"anti8\"\n)\n\n\n# view the first few observations (and columns) in the data set\nhead(nlsy_math_wide[, 1:11], 10)\n\n\nA data.frame: 10 x 11\n\n\n\nid\nfemale\nlb_wght\nanti_k1\nmath2\nmath3\nmath4\nmath5\nmath6\nmath7\nmath8\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n201\n1\n0\n0\nNA\n38\nNA\n55\nNA\nNA\nNA\n\n\n2\n303\n1\n0\n1\n26\nNA\nNA\n33\nNA\nNA\nNA\n\n\n3\n2702\n0\n0\n0\n56\nNA\n58\nNA\nNA\nNA\n80\n\n\n4\n4303\n1\n0\n0\nNA\n41\n58\nNA\nNA\nNA\nNA\n\n\n5\n5002\n0\n0\n4\nNA\nNA\n46\nNA\n54\nNA\n66\n\n\n6\n5005\n1\n0\n0\n35\nNA\n50\nNA\n60\nNA\n59\n\n\n7\n5701\n0\n0\n2\nNA\n62\n61\nNA\nNA\nNA\nNA\n\n\n8\n6102\n0\n0\n0\nNA\nNA\n55\n67\nNA\n81\nNA\n\n\n9\n6801\n1\n0\n0\nNA\n54\nNA\n62\nNA\n66\nNA\n\n\n10\n6802\n0\n0\n0\nNA\n55\nNA\n66\nNA\n68\nNA\nSpecifichiamo il modello SEM (si noti che, anche in questo caso, la scrittura del modello può essere semplificata usando la funzione growth).\nI covarianti invarianti nel tempo valutati qui includono lb_wght, una variabile dicotomica codificata come dummy che indica se il bambino aveva un peso alla nascita normale (codificato 0) o basso (codificato 1), e anti_k1, una variabile continua con valori che variano da 0 a 8 indicando il grado in cui il bambino manifestava comportamenti antisociali all’asilo o in prima elementare (punteggi più alti indicano un comportamento più antisociale).\n#writing out linear growth model with tic in full SEM way \nlg_math_tic_lavaan_model &lt;- '\n    #latent variable definitions\n            #intercept\n              eta1 =~ 1*math2+\n                      1*math3+\n                      1*math4+\n                      1*math5+\n                      1*math6+\n                      1*math7+\n                      1*math8\n            #linear slope\n              eta2 =~ 0*math2+\n                      1*math3+\n                      2*math4+\n                      3*math5+\n                      4*math6+\n                      5*math7+\n                      6*math8\n\n          #factor variances\n            eta1 ~~ eta1\n            eta2 ~~ eta2\n\n          #factor covariance\n            eta1 ~~ eta2\n\n          #manifest variances (set equal by naming theta)\n            math2 ~~ theta*math2\n            math3 ~~ theta*math3\n            math4 ~~ theta*math4\n            math5 ~~ theta*math5\n            math6 ~~ theta*math6\n            math7 ~~ theta*math7\n            math8 ~~ theta*math8\n\n          #latent means (freely estimated)\n            eta1 ~ 1\n            eta2 ~ 1\n\n          #manifest means (fixed to zero)\n            math2 ~ 0*1\n            math3 ~ 0*1\n            math4 ~ 0*1\n            math5 ~ 0*1\n            math6 ~ 0*1\n            math7 ~ 0*1\n            math8 ~ 0*1\n\n        #Time invariant covaraite\n        #regression of time-invariant covariate on intercept and slope factors\n            eta1 ~ lb_wght + anti_k1\n            eta2 ~ lb_wght + anti_k1\n\n        #variance of TIV covariates\n            lb_wght ~~ lb_wght\n            anti_k1 ~~ anti_k1\n\n        #covariance of TIV covariates\n            lb_wght ~~ anti_k1\n\n        #means of TIV covariates (freely estimated)\n            lb_wght ~ 1\n            anti_k1 ~ 1\n' #end of model definition\nAdattiamo il modello ai dati.\nlg_math_tic_lavaan_fit &lt;- sem(lg_math_tic_lavaan_model,\n  data = nlsy_math_wide,\n  meanstructure = TRUE,\n  estimator = \"ML\",\n  missing = \"fiml\"\n)\nEsaminiamo la soluzione ottenuta.\nsummary(lg_math_tic_lavaan_fit, fit.measures = TRUE) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 110 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        21\n  Number of equality constraints                     6\n\n  Number of observations                           933\n  Number of missing patterns                        61\n\nModel Test User Model:\n                                                      \n  Test statistic                               220.221\n  Degrees of freedom                                39\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               892.616\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.788\n  Tucker-Lewis Index (TLI)                       0.805\n                                                      \n  Robust Comparative Fit Index (CFI)             0.920\n  Robust Tucker-Lewis Index (TLI)                0.926\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -9785.085\n  Loglikelihood unrestricted model (H1)      -9674.975\n                                                      \n  Akaike (AIC)                               19600.171\n  Bayesian (BIC)                             19672.747\n  Sample-size adjusted Bayesian (SABIC)      19625.108\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.071\n  90 Percent confidence interval - lower         0.062\n  90 Percent confidence interval - upper         0.080\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.046\n                                                      \n  Robust RMSEA                                   0.100\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.183\n  P-value H_0: Robust RMSEA &lt;= 0.050             0.218\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.654\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.097\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta1 =~                                             \n    math2             1.000                           \n    math3             1.000                           \n    math4             1.000                           \n    math5             1.000                           \n    math6             1.000                           \n    math7             1.000                           \n    math8             1.000                           \n  eta2 =~                                             \n    math2             0.000                           \n    math3             1.000                           \n    math4             2.000                           \n    math5             3.000                           \n    math6             4.000                           \n    math7             5.000                           \n    math8             6.000                           \n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta1 ~                                              \n    lb_wght          -2.716    1.294   -2.099    0.036\n    anti_k1          -0.551    0.232   -2.369    0.018\n  eta2 ~                                              \n    lb_wght           0.625    0.333    1.873    0.061\n    anti_k1          -0.019    0.059   -0.327    0.743\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n .eta1 ~~                                             \n   .eta2             -0.078    1.145   -0.068    0.945\n  lb_wght ~~                                          \n    anti_k1           0.007    0.014    0.548    0.584\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .eta1             36.290    0.497   73.052    0.000\n   .eta2              4.315    0.122   35.420    0.000\n   .math2             0.000                           \n   .math3             0.000                           \n   .math4             0.000                           \n   .math5             0.000                           \n   .math6             0.000                           \n   .math7             0.000                           \n   .math8             0.000                           \n    lb_wght           0.080    0.009    9.031    0.000\n    anti_k1           1.454    0.050   29.216    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .eta1             63.064    5.609   11.242    0.000\n   .eta2              0.713    0.326    2.185    0.029\n   .math2   (thet)   36.257    1.868   19.406    0.000\n   .math3   (thet)   36.257    1.868   19.406    0.000\n   .math4   (thet)   36.257    1.868   19.406    0.000\n   .math5   (thet)   36.257    1.868   19.406    0.000\n   .math6   (thet)   36.257    1.868   19.406    0.000\n   .math7   (thet)   36.257    1.868   19.406    0.000\n   .math8   (thet)   36.257    1.868   19.406    0.000\n    lb_wght           0.074    0.003   21.599    0.000\n    anti_k1           2.312    0.107   21.599    0.000\nInterpretazione dei risultati per eta1 (Intercetta).\nInterpretazione dei risultati per eta2 (Pendenza).\nIn sintesi, il peso alla nascita basso e i comportamenti antisociali sembrano influenzare negativamente il valore iniziale (intercetta) del costrutto misurato. Il peso alla nascita e i comportamenti antisociali non sembrano avere un impatto significativo sulla pendenza.\nCreiamo un diagramma di percorso.\nlg_math_tic_lavaan_fit |&gt;\n    semPaths(\n        style = \"lisrel\",\n        whatLabels = \"std\", edge.label.cex = .6,\n        label.prop = 0.9, edge.label.color = \"black\", rotation = 4,\n        equalizeManifests = FALSE, optimizeLatRes = TRUE, node.width = 1.5,\n        edge.width = 0.5, shapeMan = \"rectangle\", shapeLat = \"ellipse\",\n        shapeInt = \"triangle\", sizeMan = 4, sizeInt = 2, sizeLat = 4,\n        curve = 2, unCol = \"#070b8c\"\n    )",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>79</span>  <span class='chapter-title'>Covariate indipendenti dal tempo</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/09_time_inv_cov.html#introduzione",
    "href": "chapters/lgm/09_time_inv_cov.html#introduzione",
    "title": "79  Covariate indipendenti dal tempo",
    "section": "",
    "text": "È importante sottolineare che, pur fornendo insights significativi, i risultati ottenuti da questi modelli non implicano relazioni causali. Questi modelli hanno limitazioni simili a quelle dei modelli di regressione standard in termini di inferenza causale.\nÈ cruciale considerare il contesto nel quale le covariate invarianti nel tempo sono state raccolte. Se queste provengono da un contesto sperimentale con assegnazione casuale, le inferenze potrebbero essere più robuste. Tuttavia, nel caso di dati osservazionali, è necessario un’attenta considerazione per evitare interpretazioni errate o eccessivamente assertive riguardo la causalità.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlb_wght: L’effetto stimato di lb_wght sull’intercetta (eta1) è di -2.716 con uno standard error di 1.294. Questo valore di -2.716 indica che avere un peso alla nascita basso (codificato come 1) è associato ad una riduzione media di 2.716 unità nel valore iniziale di eta1, rispetto a un peso alla nascita normale (codificato come 0). Questo risultato è statisticamente significativo, come indicato dal valore di P (0.036), che è inferiore a 0.05.\nanti_k1: Per anti_k1, l’effetto stimato sull’intercetta è di -0.551 con uno standard error di 0.232. Questo suggerisce che per ogni unità di aumento nel punteggio di comportamento antisociale (anti_k1), il valore iniziale di eta1 diminuisce in media di 0.551 unità. Anche questo risultato è statisticamente significativo, con un valore di P di 0.018.\n\n\n\nlb_wght: Per la pendenza (eta2), l’effetto stimato di lb_wght è di 0.625 con uno standard error di 0.333. Ciò indica che avere un peso alla nascita basso è associato ad un aumento medio di 0.625 unità nella pendenza di eta2, rispetto a un peso normale alla nascita. Tuttavia, questo risultato non è statisticamente significativo al livello del 5%, dato che il valore di P è 0.061, che è leggermente superiore a 0.05.\nanti_k1: L’effetto stimato di anti_k1 sulla pendenza è molto piccolo (-0.019) e non è statisticamente significativo (valore di P = 0.743), suggerendo che non c’è una relazione chiara tra il comportamento antisociale in età precoce e il cambiamento nel tempo di eta2.\n\n\n\n\n\n79.1.1 Valutare il contributo delle covariate\nUna domanda comune in questo approccio per comprendere le associazioni tra covarianti invarianti nel tempo e traiettorie individuali è se l’aggiunta dei covarianti invarianti nel tempo sia stata utile. Nel framework di modellazione multilivello, i -2LL ottenuti quando si adattano modelli con e senza i covarianti invarianti nel tempo possono essere confrontati direttamente (se nessun partecipante è stato escluso dall’analisi a causa di dati incompleti sui covarianti invarianti nel tempo), fornendo un modo per valutare l’adattamento relativo del modello. Specificamente, possiamo esaminare la differenza tra i -2LL rispetto alla differenza nel numero di parametri stimati (o differenza nei gradi di libertà).\n\n#writing out linear growth model with tic in full SEM way \nlg_math_ticZERO_lavaan_model &lt;- '\n    #latent variable definitions\n            #intercept\n              eta1 =~ 1*math2+\n                      1*math3+\n                      1*math4+\n                      1*math5+\n                      1*math6+\n                      1*math7+\n                      1*math8\n            #linear slope\n              eta2 =~ 0*math2+\n                      1*math3+\n                      2*math4+\n                      3*math5+\n                      4*math6+\n                      5*math7+\n                      6*math8\n\n          #factor variances\n            eta1 ~~ eta1\n            eta2 ~~ eta2\n\n          #factor covariance\n            eta1 ~~ eta2\n\n          #manifest variances (set equal by naming theta)\n            math2 ~~ theta*math2\n            math3 ~~ theta*math3\n            math4 ~~ theta*math4\n            math5 ~~ theta*math5\n            math6 ~~ theta*math6\n            math7 ~~ theta*math7\n            math8 ~~ theta*math8\n\n          #latent means (freely estimated)\n            eta1 ~ 1\n            eta2 ~ 1\n\n          #manifest means (fixed to zero)\n            math2 ~ 0*1\n            math3 ~ 0*1\n            math4 ~ 0*1\n            math5 ~ 0*1\n            math6 ~ 0*1\n            math7 ~ 0*1\n            math8 ~ 0*1\n\n        #Time invariant covaraite\n          #regression of time-invariant covariate on intercept and slope factors\n          #FIXED to 0\n            eta1 ~ 0*lb_wght + 0*anti_k1\n            eta2 ~ 0*lb_wght + 0*anti_k1\n\n        #variance of TIV covariates\n            lb_wght ~~ lb_wght\n            anti_k1 ~~ anti_k1\n\n        #covariance of TIV covaraites\n            lb_wght ~~ anti_k1\n\n        #means of TIV covariates (freely estimated)\n            lb_wght ~ 1\n            anti_k1 ~ 1\n' #end of model definition\n\nAdattiamo il modello ai dati.\n\nlg_math_ticZERO_lavaan_fit &lt;- sem(lg_math_ticZERO_lavaan_model,\n  data = nlsy_math_wide,\n  meanstructure = TRUE,\n  estimator = \"ML\",\n  missing = \"fiml\"\n)\n\nEsaminiamo il risultato ottenuto.\n\nsummary(lg_math_ticZERO_lavaan_fit, fit.measures = TRUE) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 85 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        17\n  Number of equality constraints                     6\n\n  Number of observations                           933\n  Number of missing patterns                        61\n\nModel Test User Model:\n                                                      \n  Test statistic                               234.467\n  Degrees of freedom                                43\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               892.616\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.776\n  Tucker-Lewis Index (TLI)                       0.813\n                                                      \n  Robust Comparative Fit Index (CFI)             0.917\n  Robust Tucker-Lewis Index (TLI)                0.931\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -9792.208\n  Loglikelihood unrestricted model (H1)      -9674.975\n                                                      \n  Akaike (AIC)                               19606.416\n  Bayesian (BIC)                             19659.638\n  Sample-size adjusted Bayesian (SABIC)      19624.703\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.069\n  90 Percent confidence interval - lower         0.061\n  90 Percent confidence interval - upper         0.078\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.020\n                                                      \n  Robust RMSEA                                   0.097\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.173\n  P-value H_0: Robust RMSEA &lt;= 0.050             0.208\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.646\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.103\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta1 =~                                             \n    math2             1.000                           \n    math3             1.000                           \n    math4             1.000                           \n    math5             1.000                           \n    math6             1.000                           \n    math7             1.000                           \n    math8             1.000                           \n  eta2 =~                                             \n    math2             0.000                           \n    math3             1.000                           \n    math4             2.000                           \n    math5             3.000                           \n    math6             4.000                           \n    math7             5.000                           \n    math8             6.000                           \n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta1 ~                                              \n    lb_wght           0.000                           \n    anti_k1           0.000                           \n  eta2 ~                                              \n    lb_wght           0.000                           \n    anti_k1           0.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n .eta1 ~~                                             \n   .eta2             -0.181    1.150   -0.158    0.875\n  lb_wght ~~                                          \n    anti_k1           0.007    0.014    0.548    0.584\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .eta1             35.267    0.355   99.229    0.000\n   .eta2              4.339    0.088   49.136    0.000\n   .math2             0.000                           \n   .math3             0.000                           \n   .math4             0.000                           \n   .math5             0.000                           \n   .math6             0.000                           \n   .math7             0.000                           \n   .math8             0.000                           \n    lb_wght           0.080    0.009    9.031    0.000\n    anti_k1           1.454    0.050   29.216    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .eta1             64.562    5.659   11.408    0.000\n   .eta2              0.733    0.327    2.238    0.025\n   .math2   (thet)   36.230    1.867   19.410    0.000\n   .math3   (thet)   36.230    1.867   19.410    0.000\n   .math4   (thet)   36.230    1.867   19.410    0.000\n   .math5   (thet)   36.230    1.867   19.410    0.000\n   .math6   (thet)   36.230    1.867   19.410    0.000\n   .math7   (thet)   36.230    1.867   19.410    0.000\n   .math8   (thet)   36.230    1.867   19.410    0.000\n    lb_wght           0.074    0.003   21.599    0.000\n    anti_k1           2.312    0.107   21.599    0.000\n\n\n\nGeneriamo il diagramma di percorso.\n\nlg_math_ticZERO_lavaan_fit |&gt;\n    semPaths(\n        style = \"lisrel\",\n        whatLabels = \"std\", edge.label.cex = .6,\n        label.prop = 0.9, edge.label.color = \"black\", rotation = 4,\n        equalizeManifests = FALSE, optimizeLatRes = TRUE, node.width = 1.5,\n        edge.width = 0.5, shapeMan = \"rectangle\", shapeLat = \"ellipse\",\n        shapeInt = \"triangle\", sizeMan = 4, sizeInt = 2, sizeLat = 4,\n        curve = 2, unCol = \"#070b8c\"\n    )\n\n\n\n\n\n\n\n\nEseguiamo il confronto tra i due modelli mediante il test del rapporto tra verosimiglianze.\n\nlavTestLRT(lg_math_tic_lavaan_fit, lg_math_ticZERO_lavaan_fit) |&gt;\n    print()\n\n\nChi-Squared Difference Test\n\n                           Df   AIC   BIC  Chisq Chisq diff    RMSEA\nlg_math_tic_lavaan_fit     39 19600 19673 220.22                    \nlg_math_ticZERO_lavaan_fit 43 19606 19660 234.47     14.245 0.052395\n                           Df diff Pr(&gt;Chisq)   \nlg_math_tic_lavaan_fit                          \nlg_math_ticZERO_lavaan_fit       4   0.006552 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNel nostro esempio, il -2LL per il modello di crescita lineare era 15.937 e il -2LL per il modello di crescita lineare con due predittori invarianti nel tempo (basso peso alla nascita e comportamenti antisociali) dell’intercetta e della pendenza era 15.923, una differenza di 14. La differenza nel numero di parametri stimati era (19 – 15) = 4. Quindi, il miglioramento dell’adattamento era significativo (χ2(4) = 14, p &lt; .01), indicando che basso peso alla nascita e comportamenti antisociali erano predittori utili. Parallelamente alle differenze nei -2LL, le differenze in AIC e BIC hanno anche indicato un miglioramento nell’adattamento del modello (criteri di informazione più bassi indicano un migliore adattamento) quando i covarianti invarianti nel tempo erano inclusi nel modello.\nNel framework di modellazione delle equazioni strutturali, è tipico valutare l’adattamento globale (ad esempio, RMSEA, CFI, TLI); tuttavia, raramente l’aggiunta di covarianti invarianti nel tempo cambia significativamente questi indici. L’adattamento relativo dei modelli può essere informativo come nel framework di modellazione multilivello. Come sopra, le differenze nei -2LL (o χ2) possono essere calcolate per testare se l’inclusione dei covarianti invarianti nel tempo ha migliorato significativamente l’adattamento del modello. Si noti che, nel framework di modellazione delle equazioni strutturali, il modello di confronto (baseline) non è semplicemente un modello senza i covarianti invarianti nel tempo. Piuttosto, è un modello che include i covarianti invarianti nel tempo ma vincola i loro effetti sull’intercetta e sulla pendenza a 0.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>79</span>  <span class='chapter-title'>Covariate indipendenti dal tempo</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/09_time_inv_cov.html#confronto-con-il-modello-misto",
    "href": "chapters/lgm/09_time_inv_cov.html#confronto-con-il-modello-misto",
    "title": "79  Covariate indipendenti dal tempo",
    "section": "79.2 Confronto con il modello misto",
    "text": "79.2 Confronto con il modello misto\nEseguiamo ora l’analisi statistica utilizzando un modello misto con intercetta e pendenza casuale. Confronteremo un modello ridotto, che include solo l’effetto del tempo, con un modello completo che include le covariate esaminate in precedenza. Il modello completo include gli effetti principali delle covariate e l’interazione tra le covariate e il tempo.\nAdattiamo il modello “completo”.\n\nnlsy_math_long$grade_c2 &lt;- nlsy_math_long$grade-2\n\nfit2_lmer &lt;- lmer(\n    math ~ 1 + grade_c2 + lb_wght + anti_k1 + I(grade_c2 * lb_wght) + I(grade_c2 * anti_k1) +\n        (1 + grade_c2 | id),\n    data = nlsy_math_long,\n    REML = FALSE,\n    na.action = na.exclude\n)\n\nEsaminiamo i risultati ottenuti.\n\nsummary(fit2_lmer)\n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: math ~ 1 + grade_c2 + lb_wght + anti_k1 + I(grade_c2 * lb_wght) +  \n    I(grade_c2 * anti_k1) + (1 + grade_c2 | id)\n   Data: nlsy_math_long\n\n     AIC      BIC   logLik deviance df.resid \n 15943.1  16000.2  -7961.6  15923.1     2211 \n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.07986 -0.52517 -0.00867  0.53079  2.53455 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n id       (Intercept) 63.0653  7.941         \n          grade_c2     0.7141  0.845    -0.01\n Residual             36.2543  6.021         \nNumber of obs: 2221, groups:  id, 932\n\nFixed effects:\n                      Estimate Std. Error t value\n(Intercept)           36.28983    0.49630  73.120\ngrade_c2               4.31521    0.12060  35.782\nlb_wght               -2.71621    1.29359  -2.100\nanti_k1               -0.55087    0.23246  -2.370\nI(grade_c2 * lb_wght)  0.62463    0.33314   1.875\nI(grade_c2 * anti_k1) -0.01930    0.05886  -0.328\n\nCorrelation of Fixed Effects:\n            (Intr) grd_c2 lb_wgh ant_k1 I(_2*l\ngrade_c2    -0.529                            \nlb_wght     -0.194  0.096                     \nanti_k1     -0.671  0.358 -0.025              \nI(grd_2*l_)  0.091 -0.168 -0.532  0.026       \nI(gr_2*a_1)  0.343 -0.660  0.026 -0.529 -0.055\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.00577269 (tol = 0.002, component 1)\n\n\nAdattiamo un modello misto vincolato senza covariate, utilizzando un modello con intercetta e pendenza casuale.\n\nfit3_lmer &lt;- lmer(\n    math ~ 1 + grade_c2 + (1 + grade_c2 | id),\n    data = nlsy_math_long,\n    REML = FALSE,\n    na.action = na.exclude\n)\n\nConfrontiamo i due modelli utilizzando il test del rapporto di verosimiglianza.\n\nanova(fit2_lmer, fit3_lmer) |&gt; \n    print()\n\nData: nlsy_math_long\nModels:\nfit3_lmer: math ~ 1 + grade_c2 + (1 + grade_c2 | id)\nfit2_lmer: math ~ 1 + grade_c2 + lb_wght + anti_k1 + I(grade_c2 * lb_wght) + I(grade_c2 * anti_k1) + (1 + grade_c2 | id)\n          npar   AIC   BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)   \nfit3_lmer    6 15949 15984 -7968.7    15937                        \nfit2_lmer   10 15943 16000 -7961.6    15923 14.245  4   0.006552 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLa differenza nei -2LL tra questi due modelli era 14, con una differenza di 4 gradi di libertà, χ2(4) = 14, p &lt; .01. Questa differenza è identica a quella ottenuta confrontando i modelli nel framework di modellazione LGM. Giungiamo alla stessa conclusione riguardo l’importanza del basso peso alla nascita e dei comportamenti antisociali nel framework di modellazione multilivello e quando esaminiamo le differenze nelle traiettorie matematiche dei bambini.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>79</span>  <span class='chapter-title'>Covariate indipendenti dal tempo</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/09_time_inv_cov.html#considerazioni-importanti",
    "href": "chapters/lgm/09_time_inv_cov.html#considerazioni-importanti",
    "title": "79  Covariate indipendenti dal tempo",
    "section": "79.3 Considerazioni Importanti",
    "text": "79.3 Considerazioni Importanti\nLa maggior parte delle ricerche nel modello di crescita cerca di comprendere come le caratteristiche interpersonali (ovvero, i covarianti invarianti nel tempo) siano associate a differenze interpersonali nei cambiamenti intrapersonali catturati dai dati longitudinali. Nel nostro esempio illustrativo, ci siamo limitati a due covarianti invarianti nel tempo per semplicità. Tuttavia, è possibile includere contemporaneamente nel modello diversi covarianti invarianti nel tempo e le interazioni tra di essi. Come in tutte le analisi di regressione, è essenziale un’adeguata scala e centratura dei covarianti invarianti nel tempo per ottenere stime di parametri sostanzialmente significative. Tutte le pratiche comuni nella regressione, come l’esame delle interazioni (moderazione) tra i covarianti invarianti nel tempo, relazioni non lineari e test di mediazione, sono possibili e implementate in modi tipici. Ad esempio, è possibile calcolare variabili prodotto, includerle nel dataset e inserirle come predittori aggiuntivi per esaminare effetti interattivi. Inoltre, gli effetti dei covarianti invarianti nel tempo possono essere aggiunti al modello in modo gerarchico per isolare se il loro inserimento ha migliorato significativamente l’adattamento del modello (simile all’esame del cambiamento significativo in R^2).\n\n79.3.1 Varianza Spiegata\nOltre a valutare l’importanza dei covarianti invarianti nel tempo, i ricercatori vogliono anche sapere quanto della varianza nell’intercetta e nella pendenza sia stata spiegata dai covarianti invarianti nel tempo. In altre parole, quale proporzione delle differenze interpersonali nell’intercetta e nelle pendenze è stata spiegata dai covarianti invarianti nel tempo. Nei framework di modellazione multilivello e di equazioni strutturali, è possibile confrontare le stime di varianza dell’intercetta e della pendenza ottenute in modelli con e senza covarianti invarianti nel tempo. Nel nostro esempio, ad esempio, la stima della varianza dell’intercetta era 64.562 per il modello di crescita lineare senza covarianti invarianti nel tempo e 63.064 quando il basso peso alla nascita e i comportamenti antisociali erano inclusi come covarianti invarianti nel tempo. La differenza tra le varianze stimate era di 1.498. Convertendo questa differenza in una proporzione della varianza originale, troviamo che i covarianti invarianti nel tempo hanno spiegato lo 0.023 (1.498/64.562) ovvero il 2.3% delle differenze interpersonali nell’intercetta. Calcoli simili per la pendenza hanno prodotto una varianza spiegata dello 0.027 (2.7%).\n\n\n79.3.2 Coefficienti Standardizzati\nNell’ambito della ricerca, oltre alla valutazione della varianza spiegata, può essere utile calcolare i coefficienti standardizzati. Questi aiutano a determinare l’importanza di ciascun predittore e funzionano come una misura della grandezza dell’effetto. I coefficienti di regressione di secondo livello (percorsi), che partono dai covarianti invarianti nel tempo verso l’intercetta e la pendenza, sono inizialmente non standardizzati. Per ottenere i coefficienti standardizzati, è necessario moltiplicare il coefficiente non standardizzato per il rapporto tra la deviazione standard del predittore (cioè, il covariante invariante nel tempo) e quella del risultato (cioè, l’intercetta o la pendenza).\nLa formula generale per il calcolo di un coefficiente standardizzato è:\n\\[ \\beta^* = \\frac{b \\times \\sigma_{\\text{predittore}}}{\\sigma_{\\text{risultato}}}, \\]\ndove:\n\n\\(\\beta^*\\) rappresenta il coefficiente standardizzato.\n\\(b\\) è il coefficiente non standardizzato.\n\\(\\sigma_{\\text{predittore}}\\) è la deviazione standard del predittore, come i comportamenti antisociali nel nostro caso.\n\\(\\sigma_{\\text{risultato}}\\) è la deviazione standard del risultato, come l’intercetta nel nostro caso.\n\nApplicando questa formula, il coefficiente standardizzato per l’effetto dei comportamenti antisociali sull’intercetta è dato da:\n\\[\n\\beta^* = \\frac{-0.551 \\times \\sigma_{\\text{anti\\_k1}}}{\\sqrt{63.065 + (0.080 \\times \\sigma_{\\text{X1}} \\times 0.080) + (-0.551 \\times \\sigma_{\\text{anti\\_k1}} \\times -0.551) + 2 \\times (0.080 \\times \\sigma_{\\text{X1,anti\\_k1}} \\times -0.551)}}\n\\]\nQui, \\(\\sigma_{\\text{anti\\_k1}}\\) rappresenta la deviazione standard dei comportamenti antisociali, \\(\\sigma_{\\text{X1}}\\) è la deviazione standard di un altro predittore, se presente, e \\(\\sigma_{\\text{X1,anti\\_k1}}\\) è la covarianza tra i due predittori.\nIn conclusione, il calcolo produce:\n\\[\n-0.551 \\times 2.312 / \\sqrt{63.065 + (0.080 \\times 0.074 \\times 0.080) + (-0.551 \\times 2.312 \\times -0.551) + 2 \\times (0.080 \\times 0.007 \\times -0.551)} = -0.105\n\\]\nQuindi, l’effetto dei comportamenti antisociali sui punteggi di matematica di seconda elementare è risultato essere di piccola entità.\n\n\n79.3.3 Riflessioni Conclusive\nIn questo capitolo, abbiamo esaminato il modello di crescita lineare con covarianti invarianti nel tempo, un modello spesso utilizzato per esaminare le differenze individuali nella crescita e nel cambiamento. L’uso di questo modello implica una serie di assunzioni. Innanzitutto, il modello presume l’invarianza della struttura del cambiamento per tutte le persone. Ciò significa che si assume che tutti i bambini, indipendentemente dai loro punteggi sui covarianti invarianti nel tempo, seguano una traiettoria di crescita lineare. Inoltre, abbiamo ipotizzato che la grandezza della varianza residua nell’intercetta e nella pendenza, così come la covarianza residua tra l’intercetta e la pendenza, siano le stesse per i bambini con valori diversi sui covarianti invarianti nel tempo.\nAssumiamo anche che la varianza residua dei punteggi osservati sia equivalente per tutti i bambini. In altre parole, indipendentemente dai valori dei covarianti invarianti nel tempo, l’inadeguatezza del modello lineare è identica. Nel nostro esempio, abbiamo ipotizzato che la grandezza delle fluttuazioni annuali nelle prestazioni matematiche dei bambini con livelli inferiori o superiori di comportamento antisociale fosse equivalente. Dato che queste assunzioni potrebbero essere vere o meno, esse dovrebbero essere attentamente considerate prima di intraprendere tali analisi.\nNel prossimo capitolo, discuteremo i modelli di crescita per gruppi multipli che facilitano un esame approfondito di queste assunzioni per determinati tipi di covarianti invarianti nel tempo, in particolare quelli che sono variabili categoriche, ordinali o variabili continue che sono state categorizzate (ad esempio, tramite uno split mediano). Questo approccio permette una verifica più accurata e specifica delle ipotesi del modello in contesti diversi e con differenti tipologie di dati.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>79</span>  <span class='chapter-title'>Covariate indipendenti dal tempo</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/10_growth_groups.html",
    "href": "chapters/lgm/10_growth_groups.html",
    "title": "80  Modelli di crescita latenti a gruppi multipli",
    "section": "",
    "text": "80.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nNel capitolo precedente, abbiamo approfondito i modelli di crescita con covarianti invarianti nel tempo. In questo capitolo, esploreremo un approccio alternativo per studiare le differenze individuali nel cambiamento: l’analisi comparativa tra gruppi (McArdle, 1989; McArdle & Hamagami, 1996). Sebbene i modelli di crescita con covarianti invarianti nel tempo siano efficaci nell’analizzare le differenze nelle traiettorie medie di crescita, questi modelli presentano limitazioni nell’indagare altri aspetti dei cambiamenti intrapersonali e delle differenze interpersonali in tali processi.\nSenza adeguati ampliamenti, i modelli basati esclusivamente su covarianti invarianti nel tempo non forniscono informazioni sulle variazioni nelle varianze e covarianze tra i fattori di crescita, né sulla variabilità residua e sulla dinamica dei cambiamenti intrapersonali. Nel presente capitolo, dimostreremo come l’approccio di confronto tra gruppi possa essere impiegato per esaminare le differenze in qualsiasi aspetto del modello di crescita. Questa flessibilità metodologica ci permette di acquisire una comprensione più profonda su come e perché gli individui mostrino percorsi di sviluppo diversificati.\nPer i nostri esempi, utilizziamo i punteggi di rendimento in matematica dai dati NLSY-CYA [si veda {cite:t}grimm2016growth]. Importiamo i dati.\n# set filepath for data file\nfilepath &lt;- \"https://raw.githubusercontent.com/LRI-2/Data/main/GrowthModeling/nlsy_math_long_R.dat\"\n\n# read in the text data file using the url() function\ndat &lt;- read.table(\n  file = url(filepath),\n  na.strings = \".\"\n) # indicates the missing data designator\n\n# copy data with new name\nnlsy_math_long &lt;- dat\n\n# Add names the columns of the data set\nnames(nlsy_math_long) &lt;- c(\n  \"id\", \"female\", \"lb_wght\",\n  \"anti_k1\", \"math\", \"grade\",\n  \"occ\", \"age\", \"men\",\n  \"spring\", \"anti\"\n)\n\n# reducing to variables of interest\nnlsy_math_long &lt;- nlsy_math_long[, c(\"id\", \"grade\", \"math\", \"lb_wght\")]\n\n# adding another dummy code variable for normal birth weight that coded the opposite of the low brithweight variable.\nnlsy_math_long$nb_wght &lt;- 1 - nlsy_math_long$lb_wght\n\n# view the first few observations in the data set\nhead(nlsy_math_long, 10) |&gt;\n  print()\n\n     id grade math lb_wght nb_wght\n1   201     3   38       0       1\n2   201     5   55       0       1\n3   303     2   26       0       1\n4   303     5   33       0       1\n5  2702     2   56       0       1\n6  2702     4   58       0       1\n7  2702     8   80       0       1\n8  4303     3   41       0       1\n9  4303     4   58       0       1\n10 5002     4   46       0       1\nEsaminiamo le curve di crescita nei due gruppi.\n# intraindividual change trajetories\nggplot(\n  data = nlsy_math_long, # data set\n  aes(x = grade, y = math, group = id)\n) + # setting variables\n  geom_point(size = .5) + # adding points to plot\n  geom_line(alpha=0.3) + # adding lines to plot\n  # setting the x-axis with breaks and labels\n  scale_x_continuous(\n    limits = c(2, 8),\n    breaks = c(2, 3, 4, 5, 6, 7, 8),\n    name = \"Grade at Testing\"\n  ) +\n  # setting the y-axis with limits breaks and labels\n  scale_y_continuous(\n    limits = c(10, 90),\n    breaks = c(10, 30, 50, 70, 90),\n    name = \"PIAT Mathematics\"\n  ) +\n  facet_wrap(~lb_wght)\nPer semplicità, carichiamo di nuovo i dati già trasformati in formato wide.\n# set filepath for data file\nfilepath &lt;- \"https://raw.githubusercontent.com/LRI-2/Data/main/GrowthModeling/nlsy_math_wide_R.dat\"\n# read in the text data file using the url() function\ndat &lt;- read.table(\n  file = url(filepath),\n  na.strings = \".\"\n) # indicates the missing data designator\n# copy data with new name\nnlsy_math_wide &lt;- dat\n\n# Give the variable names\nnames(nlsy_math_wide) &lt;- c(\n  \"id\", \"female\", \"lb_wght\", \"anti_k1\",\n  \"math2\", \"math3\", \"math4\", \"math5\", \"math6\", \"math7\", \"math8\",\n  \"age2\", \"age3\", \"age4\", \"age5\", \"age6\", \"age7\", \"age8\",\n  \"men2\", \"men3\", \"men4\", \"men5\", \"men6\", \"men7\", \"men8\",\n  \"spring2\", \"spring3\", \"spring4\", \"spring5\", \"spring6\", \"spring7\", \"spring8\",\n  \"anti2\", \"anti3\", \"anti4\", \"anti5\", \"anti6\", \"anti7\", \"anti8\"\n)\n\n# view the first few observations (and columns) in the data set\nhead(nlsy_math_wide[, 1:11], 10)\n\n\nA data.frame: 10 x 11\n\n\n\nid\nfemale\nlb_wght\nanti_k1\nmath2\nmath3\nmath4\nmath5\nmath6\nmath7\nmath8\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n201\n1\n0\n0\nNA\n38\nNA\n55\nNA\nNA\nNA\n\n\n2\n303\n1\n0\n1\n26\nNA\nNA\n33\nNA\nNA\nNA\n\n\n3\n2702\n0\n0\n0\n56\nNA\n58\nNA\nNA\nNA\n80\n\n\n4\n4303\n1\n0\n0\nNA\n41\n58\nNA\nNA\nNA\nNA\n\n\n5\n5002\n0\n0\n4\nNA\nNA\n46\nNA\n54\nNA\n66\n\n\n6\n5005\n1\n0\n0\n35\nNA\n50\nNA\n60\nNA\n59\n\n\n7\n5701\n0\n0\n2\nNA\n62\n61\nNA\nNA\nNA\nNA\n\n\n8\n6102\n0\n0\n0\nNA\nNA\n55\n67\nNA\n81\nNA\n\n\n9\n6801\n1\n0\n0\nNA\n54\nNA\n62\nNA\n66\nNA\n\n\n10\n6802\n0\n0\n0\nNA\n55\nNA\n66\nNA\n68\nNA",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>80</span>  <span class='chapter-title'>Modelli di crescita latenti a gruppi multipli</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/10_growth_groups.html#invarianza-tra-gruppi",
    "href": "chapters/lgm/10_growth_groups.html#invarianza-tra-gruppi",
    "title": "80  Modelli di crescita latenti a gruppi multipli",
    "section": "80.2 Invarianza tra gruppi",
    "text": "80.2 Invarianza tra gruppi\nDefiniamo il modello di crescita latente per i due gruppi.\n\n# writing out linear growth model in full SEM way\nmg_math_lavaan_model &lt;- \"\n  # latent variable definitions\n      #intercept (note intercept is a reserved term)\n      eta_1 =~ 1*math2\n      eta_1 =~ 1*math3\n      eta_1 =~ 1*math4\n      eta_1 =~ 1*math5\n      eta_1 =~ 1*math6\n      eta_1 =~ 1*math7\n      eta_1 =~ 1*math8\n\n      #linear slope\n      eta_2 =~ 0*math2\n      eta_2 =~ 1*math3\n      eta_2 =~ 2*math4\n      eta_2 =~ 3*math5\n      eta_2 =~ 4*math6\n      eta_2 =~ 5*math7\n      eta_2 =~ 6*math8\n\n  # factor variances\n      eta_1 ~~ eta_1\n      eta_2 ~~ eta_2\n\n  # covariances among factors\n      eta_1 ~~ eta_2\n\n  # factor means\n      eta_1 ~ start(35)*1\n      eta_2 ~ start(4)*1\n\n  # manifest variances (made equivalent by naming theta)\n      math2 ~~ theta*math2\n      math3 ~~ theta*math3\n      math4 ~~ theta*math4\n      math5 ~~ theta*math5\n      math6 ~~ theta*math6\n      math7 ~~ theta*math7\n      math8 ~~ theta*math8\n  # manifest means (fixed at zero)\n      math2 ~ 0*1\n      math3 ~ 0*1\n      math4 ~ 0*1\n      math5 ~ 0*1\n      math6 ~ 0*1\n      math7 ~ 0*1\n      math8 ~ 0*1\n\" # end of model definition\n\nAdattiamo il modello ai dati specificando la separazione delle osservazioni in due gruppi e introducendo i vincoli di eguaglianza tra gruppi sulle saturazioni fattoriali, le medie, le varianze, le covarianze, e i residui. In questo modello, sostanzialmente, non c’è alcune differenza tra gruppi.\n\nmg_math_lavaan_fitM1 &lt;- sem(mg_math_lavaan_model,\n  data = nlsy_math_wide,\n  meanstructure = TRUE,\n  estimator = \"ML\",\n  missing = \"fiml\",\n  group = \"lb_wght\", # to separate groups\n  group.equal = c(\n    \"loadings\", # for constraints\n    \"means\",\n    \"lv.variances\",\n    \"lv.covariances\",\n    \"residuals\"\n  )\n)\n\nEsaminiamo i risultati.\n\nsummary(mg_math_lavaan_fitM1, fit.measures = TRUE) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 24 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        24\n  Number of equality constraints                    18\n\n  Number of observations per group:               Used       Total\n    0                                              857         858\n    1                                               75          75\n  Number of missing patterns per group:                           \n    0                                               60            \n    1                                               25            \n\nModel Test User Model:\n                                                      \n  Test statistic                               249.111\n  Degrees of freedom                                64\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    0                                          191.954\n    1                                           57.156\n\nModel Test Baseline Model:\n\n  Test statistic                               887.887\n  Degrees of freedom                                42\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.781\n  Tucker-Lewis Index (TLI)                       0.856\n                                                      \n  Robust Comparative Fit Index (CFI)             1.000\n  Robust Tucker-Lewis Index (TLI)                0.346\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -7968.693\n  Loglikelihood unrestricted model (H1)      -7844.138\n                                                      \n  Akaike (AIC)                               15949.386\n  Bayesian (BIC)                             15978.410\n  Sample-size adjusted Bayesian (SABIC)      15959.354\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.079\n  90 Percent confidence interval - lower         0.069\n  90 Percent confidence interval - upper         0.089\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.436\n                                                      \n  Robust RMSEA                                   0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.000\n  P-value H_0: Robust RMSEA &lt;= 0.050             1.000\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.128\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\n\nGroup 1 [0]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 =~                                            \n    math2             1.000                           \n    math3             1.000                           \n    math4             1.000                           \n    math5             1.000                           \n    math6             1.000                           \n    math7             1.000                           \n    math8             1.000                           \n  eta_2 =~                                            \n    math2             0.000                           \n    math3             1.000                           \n    math4             2.000                           \n    math5             3.000                           \n    math6             4.000                           \n    math7             5.000                           \n    math8             6.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 ~~                                            \n    eta_2   (.17.)   -0.181    1.150   -0.158    0.875\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1   (.18.)   35.267    0.355   99.229    0.000\n    eta_2   (.19.)    4.339    0.088   49.136    0.000\n   .math2             0.000                           \n   .math3             0.000                           \n   .math4             0.000                           \n   .math5             0.000                           \n   .math6             0.000                           \n   .math7             0.000                           \n   .math8             0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1   (.15.)   64.562    5.659   11.408    0.000\n    eta_2   (.16.)    0.733    0.327    2.238    0.025\n   .math2   (thet)   36.230    1.867   19.410    0.000\n   .math3   (thet)   36.230    1.867   19.410    0.000\n   .math4   (thet)   36.230    1.867   19.410    0.000\n   .math5   (thet)   36.230    1.867   19.410    0.000\n   .math6   (thet)   36.230    1.867   19.410    0.000\n   .math7   (thet)   36.230    1.867   19.410    0.000\n   .math8   (thet)   36.230    1.867   19.410    0.000\n\n\nGroup 2 [1]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 =~                                            \n    math2             1.000                           \n    math3             1.000                           \n    math4             1.000                           \n    math5             1.000                           \n    math6             1.000                           \n    math7             1.000                           \n    math8             1.000                           \n  eta_2 =~                                            \n    math2             0.000                           \n    math3             1.000                           \n    math4             2.000                           \n    math5             3.000                           \n    math6             4.000                           \n    math7             5.000                           \n    math8             6.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 ~~                                            \n    eta_2   (.17.)   -0.181    1.150   -0.158    0.875\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1   (.18.)   35.267    0.355   99.229    0.000\n    eta_2   (.19.)    4.339    0.088   49.136    0.000\n   .math2             0.000                           \n   .math3             0.000                           \n   .math4             0.000                           \n   .math5             0.000                           \n   .math6             0.000                           \n   .math7             0.000                           \n   .math8             0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1   (.15.)   64.562    5.659   11.408    0.000\n    eta_2   (.16.)    0.733    0.327    2.238    0.025\n   .math2   (thet)   36.230    1.867   19.410    0.000\n   .math3   (thet)   36.230    1.867   19.410    0.000\n   .math4   (thet)   36.230    1.867   19.410    0.000\n   .math5   (thet)   36.230    1.867   19.410    0.000\n   .math6   (thet)   36.230    1.867   19.410    0.000\n   .math7   (thet)   36.230    1.867   19.410    0.000\n   .math8   (thet)   36.230    1.867   19.410    0.000\n\n\n\nCreiamo il diagramma di percorso.\n\nsemPaths(mg_math_lavaan_fitM1, what = \"path\", whatLabels = \"par\")",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>80</span>  <span class='chapter-title'>Modelli di crescita latenti a gruppi multipli</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/10_growth_groups.html#vincoli-sulle-medie",
    "href": "chapters/lgm/10_growth_groups.html#vincoli-sulle-medie",
    "title": "80  Modelli di crescita latenti a gruppi multipli",
    "section": "80.3 Vincoli sulle medie",
    "text": "80.3 Vincoli sulle medie\nTrasformiamo ora il modello restrittivo specificato in precedenza allentando via via i vincoli che abbiamo introdotto. In questo modello rendiamo possibile la differenza tra le medie nei due gruppi.\n\nmg_math_lavaan_fitM2 &lt;- sem(mg_math_lavaan_model,\n  data = nlsy_math_wide,\n  meanstructure = TRUE,\n  estimator = \"ML\",\n  missing = \"fiml\",\n  group = \"lb_wght\", # to separate groups\n  group.equal = c(\n    \"loadings\", # for constraints\n    # \"means\", commented out so can differ\n    \"lv.variances\",\n    \"lv.covariances\",\n    \"residuals\"\n  )\n)\n\nEsaminiamo i risultati.\n\nsummary(mg_math_lavaan_fitM2, fit.measures = TRUE) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 31 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        24\n  Number of equality constraints                    16\n\n  Number of observations per group:               Used       Total\n    0                                              857         858\n    1                                               75          75\n  Number of missing patterns per group:                           \n    0                                               60            \n    1                                               25            \n\nModel Test User Model:\n                                                      \n  Test statistic                               243.910\n  Degrees of freedom                                62\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    0                                          191.440\n    1                                           52.470\n\nModel Test Baseline Model:\n\n  Test statistic                               887.887\n  Degrees of freedom                                42\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.785\n  Tucker-Lewis Index (TLI)                       0.854\n                                                      \n  Robust Comparative Fit Index (CFI)             1.000\n  Robust Tucker-Lewis Index (TLI)                0.326\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -7966.093\n  Loglikelihood unrestricted model (H1)      -7844.138\n                                                      \n  Akaike (AIC)                               15948.185\n  Bayesian (BIC)                             15986.884\n  Sample-size adjusted Bayesian (SABIC)      15961.477\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.079\n  90 Percent confidence interval - lower         0.069\n  90 Percent confidence interval - upper         0.090\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.472\n                                                      \n  Robust RMSEA                                   0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.000\n  P-value H_0: Robust RMSEA &lt;= 0.050             1.000\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.127\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\n\nGroup 1 [0]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 =~                                            \n    math2             1.000                           \n    math3             1.000                           \n    math4             1.000                           \n    math5             1.000                           \n    math6             1.000                           \n    math7             1.000                           \n    math8             1.000                           \n  eta_2 =~                                            \n    math2             0.000                           \n    math3             1.000                           \n    math4             2.000                           \n    math5             3.000                           \n    math6             4.000                           \n    math7             5.000                           \n    math8             6.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 ~~                                            \n    eta_2   (.17.)   -0.035    1.144   -0.031    0.975\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1            35.488    0.369   96.080    0.000\n    eta_2             4.292    0.092   46.898    0.000\n   .math2             0.000                           \n   .math3             0.000                           \n   .math4             0.000                           \n   .math5             0.000                           \n   .math6             0.000                           \n   .math7             0.000                           \n   .math8             0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1   (.15.)   63.704    5.637   11.301    0.000\n    eta_2   (.16.)    0.699    0.325    2.147    0.032\n   .math2   (thet)   36.321    1.871   19.413    0.000\n   .math3   (thet)   36.321    1.871   19.413    0.000\n   .math4   (thet)   36.321    1.871   19.413    0.000\n   .math5   (thet)   36.321    1.871   19.413    0.000\n   .math6   (thet)   36.321    1.871   19.413    0.000\n   .math7   (thet)   36.321    1.871   19.413    0.000\n   .math8   (thet)   36.321    1.871   19.413    0.000\n\n\nGroup 2 [1]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 =~                                            \n    math2             1.000                           \n    math3             1.000                           \n    math4             1.000                           \n    math5             1.000                           \n    math6             1.000                           \n    math7             1.000                           \n    math8             1.000                           \n  eta_2 =~                                            \n    math2             0.000                           \n    math3             1.000                           \n    math4             2.000                           \n    math5             3.000                           \n    math6             4.000                           \n    math7             5.000                           \n    math8             6.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 ~~                                            \n    eta_2   (.17.)   -0.035    1.144   -0.031    0.975\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1            32.733    1.244   26.314    0.000\n    eta_2             4.905    0.320   15.320    0.000\n   .math2             0.000                           \n   .math3             0.000                           \n   .math4             0.000                           \n   .math5             0.000                           \n   .math6             0.000                           \n   .math7             0.000                           \n   .math8             0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1   (.15.)   63.704    5.637   11.301    0.000\n    eta_2   (.16.)    0.699    0.325    2.147    0.032\n   .math2   (thet)   36.321    1.871   19.413    0.000\n   .math3   (thet)   36.321    1.871   19.413    0.000\n   .math4   (thet)   36.321    1.871   19.413    0.000\n   .math5   (thet)   36.321    1.871   19.413    0.000\n   .math6   (thet)   36.321    1.871   19.413    0.000\n   .math7   (thet)   36.321    1.871   19.413    0.000\n   .math8   (thet)   36.321    1.871   19.413    0.000\n\n\n\nEseguiamo il confronto tra i due modelli.\n\nlavTestLRT(mg_math_lavaan_fitM1, mg_math_lavaan_fitM2) |&gt;\n    print()\n\n\nChi-Squared Difference Test\n\n                     Df   AIC   BIC  Chisq Chisq diff    RMSEA Df diff\nmg_math_lavaan_fitM2 62 15948 15987 243.91                            \nmg_math_lavaan_fitM1 64 15949 15978 249.11     5.2005 0.058601       2\n                     Pr(&gt;Chisq)  \nmg_math_lavaan_fitM2             \nmg_math_lavaan_fitM1    0.07425 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNon vi è evidenza che consentire una differenza tra medie tra gruppi migliori l’adattamento del modello.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>80</span>  <span class='chapter-title'>Modelli di crescita latenti a gruppi multipli</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/10_growth_groups.html#vincoli-sulle-varianzecovarianze",
    "href": "chapters/lgm/10_growth_groups.html#vincoli-sulle-varianzecovarianze",
    "title": "80  Modelli di crescita latenti a gruppi multipli",
    "section": "80.4 Vincoli sulle varianze/covarianze",
    "text": "80.4 Vincoli sulle varianze/covarianze\nNel modello M3 consentiamo che anche le varianza e le covarianza differiscano tra gruppi, oltre alle medie.\n\nmg_math_lavaan_fitM3 &lt;- sem(mg_math_lavaan_model,\n  data = nlsy_math_wide,\n  meanstructure = TRUE,\n  estimator = \"ML\",\n  missing = \"fiml\",\n  group = \"lb_wght\", # to separate groups\n  group.equal = c(\n    \"loadings\", # for constraints\n    # \"means\", commented out so can differ\n    # \"lv.variances\",\n    # \"lv.covariances\",\n    \"residuals\"\n  )\n)\n\nEsaminiamo i risultati.\n\nsummary(mg_math_lavaan_fitM3, fit.measures = TRUE) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 57 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        24\n  Number of equality constraints                    13\n\n  Number of observations per group:               Used       Total\n    0                                              857         858\n    1                                               75          75\n  Number of missing patterns per group:                           \n    0                                               60            \n    1                                               25            \n\nModel Test User Model:\n                                                      \n  Test statistic                               241.182\n  Degrees of freedom                                59\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    0                                          191.157\n    1                                           50.024\n\nModel Test Baseline Model:\n\n  Test statistic                               887.887\n  Degrees of freedom                                42\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.785\n  Tucker-Lewis Index (TLI)                       0.847\n                                                      \n  Robust Comparative Fit Index (CFI)             1.000\n  Robust Tucker-Lewis Index (TLI)                0.320\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -7964.728\n  Loglikelihood unrestricted model (H1)      -7844.138\n                                                      \n  Akaike (AIC)                               15951.457\n  Bayesian (BIC)                             16004.668\n  Sample-size adjusted Bayesian (SABIC)      15969.732\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.081\n  90 Percent confidence interval - lower         0.071\n  90 Percent confidence interval - upper         0.092\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.598\n                                                      \n  Robust RMSEA                                   0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.000\n  P-value H_0: Robust RMSEA &lt;= 0.050             1.000\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.124\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\n\nGroup 1 [0]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 =~                                            \n    math2             1.000                           \n    math3             1.000                           \n    math4             1.000                           \n    math5             1.000                           \n    math6             1.000                           \n    math7             1.000                           \n    math8             1.000                           \n  eta_2 =~                                            \n    math2             0.000                           \n    math3             1.000                           \n    math4             2.000                           \n    math5             3.000                           \n    math6             4.000                           \n    math7             5.000                           \n    math8             6.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 ~~                                            \n    eta_2             0.243    1.147    0.212    0.832\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1            35.485    0.365   97.271    0.000\n    eta_2             4.293    0.091   47.089    0.000\n   .math2             0.000                           \n   .math3             0.000                           \n   .math4             0.000                           \n   .math5             0.000                           \n   .math6             0.000                           \n   .math7             0.000                           \n   .math8             0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1            61.062    5.692   10.727    0.000\n    eta_2             0.663    0.326    2.033    0.042\n   .math2   (thet)   36.276    1.870   19.402    0.000\n   .math3   (thet)   36.276    1.870   19.402    0.000\n   .math4   (thet)   36.276    1.870   19.402    0.000\n   .math5   (thet)   36.276    1.870   19.402    0.000\n   .math6   (thet)   36.276    1.870   19.402    0.000\n   .math7   (thet)   36.276    1.870   19.402    0.000\n   .math8   (thet)   36.276    1.870   19.402    0.000\n\n\nGroup 2 [1]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 =~                                            \n    math2             1.000                           \n    math3             1.000                           \n    math4             1.000                           \n    math5             1.000                           \n    math6             1.000                           \n    math7             1.000                           \n    math8             1.000                           \n  eta_2 =~                                            \n    math2             0.000                           \n    math3             1.000                           \n    math4             2.000                           \n    math5             3.000                           \n    math6             4.000                           \n    math7             5.000                           \n    math8             6.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 ~~                                            \n    eta_2            -3.801    4.912   -0.774    0.439\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1            32.850    1.413   23.241    0.000\n    eta_2             4.881    0.341   14.332    0.000\n   .math2             0.000                           \n   .math3             0.000                           \n   .math4             0.000                           \n   .math5             0.000                           \n   .math6             0.000                           \n   .math7             0.000                           \n   .math8             0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1            95.283   24.221    3.934    0.000\n    eta_2             1.297    1.315    0.986    0.324\n   .math2   (thet)   36.276    1.870   19.402    0.000\n   .math3   (thet)   36.276    1.870   19.402    0.000\n   .math4   (thet)   36.276    1.870   19.402    0.000\n   .math5   (thet)   36.276    1.870   19.402    0.000\n   .math6   (thet)   36.276    1.870   19.402    0.000\n   .math7   (thet)   36.276    1.870   19.402    0.000\n   .math8   (thet)   36.276    1.870   19.402    0.000\n\n\n\nConfrontiamo il modello M2 con il modello M3.\n\nlavTestLRT(mg_math_lavaan_fitM2, mg_math_lavaan_fitM3) |&gt;\n    print()\n\n\nChi-Squared Difference Test\n\n                     Df   AIC   BIC  Chisq Chisq diff RMSEA Df diff\nmg_math_lavaan_fitM3 59 15952 16005 241.18                         \nmg_math_lavaan_fitM2 62 15948 15987 243.91     2.7283     0       3\n                     Pr(&gt;Chisq)\nmg_math_lavaan_fitM3           \nmg_math_lavaan_fitM2     0.4354\n\n\nNon ci sono evidenze che una differenza nelle varianze e nelle covarianze tra gruppi migliori la bontà dell’adattamento del modello ai dati.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>80</span>  <span class='chapter-title'>Modelli di crescita latenti a gruppi multipli</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/10_growth_groups.html#vincoli-sui-residui",
    "href": "chapters/lgm/10_growth_groups.html#vincoli-sui-residui",
    "title": "80  Modelli di crescita latenti a gruppi multipli",
    "section": "80.5 Vincoli sui residui",
    "text": "80.5 Vincoli sui residui\nEsaminiamo ora il vincolo sulle covarianze residue. Iniziamo a specificare il modello in una nuova forma.\n\nmg_math_lavaan_model4 &lt;- \"\n  # latent variable definitions\n      #intercept (note intercept is a reserved term)\n      eta_1 =~ 1*math2\n      eta_1 =~ 1*math3\n      eta_1 =~ 1*math4\n      eta_1 =~ 1*math5\n      eta_1 =~ 1*math6\n      eta_1 =~ 1*math7\n      eta_1 =~ 1*math8\n\n      #linear slope\n      eta_2 =~ 0*math2\n      eta_2 =~ 1*math3\n      eta_2 =~ 2*math4\n      eta_2 =~ 3*math5\n      eta_2 =~ 4*math6\n      eta_2 =~ 5*math7\n      eta_2 =~ 6*math8\n\n  # factor variances\n      eta_1 ~~ start(60)*eta_1\n      eta_2 ~~ start(.75)*eta_2\n\n  # covariances among factors\n      eta_1 ~~ eta_2\n\n  # factor means\n      eta_1 ~ start(35)*1\n      eta_2 ~ start(4)*1\n\n  # manifest variances (made equivalent by naming theta)\n      math2 ~~ c(theta1,theta2)*math2\n      math3 ~~ c(theta1,theta2)*math3\n      math4 ~~ c(theta1,theta2)*math4\n      math5 ~~ c(theta1,theta2)*math5\n      math6 ~~ c(theta1,theta2)*math6\n      math7 ~~ c(theta1,theta2)*math7\n      math8 ~~ c(theta1,theta2)*math8\n  # manifest means (fixed at zero)\n      math2 ~ 0*1\n      math3 ~ 0*1\n      math4 ~ 0*1\n      math5 ~ 0*1\n      math6 ~ 0*1\n      math7 ~ 0*1\n      math8 ~ 0*1\n\" # end of model definition\n\nAdattiamo il modello ai dati.\n\nmg_math_lavaan_fitM4 &lt;- sem(mg_math_lavaan_model4,\n  data = nlsy_math_wide,\n  meanstructure = TRUE,\n  estimator = \"ML\",\n  missing = \"fiml\",\n  group = \"lb_wght\", # to separate groups\n  group.equal = c(\"loadings\")\n) # for constraints\n\nEsaminiamo i risulati.\n\nsummary(mg_math_lavaan_fitM4, fit.measures = TRUE) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 62 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        24\n  Number of equality constraints                    12\n\n  Number of observations per group:               Used       Total\n    0                                              857         858\n    1                                               75          75\n  Number of missing patterns per group:                           \n    0                                               60            \n    1                                               25            \n\nModel Test User Model:\n                                                      \n  Test statistic                               237.836\n  Degrees of freedom                                58\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    0                                          190.833\n    1                                           47.004\n\nModel Test Baseline Model:\n\n  Test statistic                               887.887\n  Degrees of freedom                                42\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.787\n  Tucker-Lewis Index (TLI)                       0.846\n                                                      \n  Robust Comparative Fit Index (CFI)             1.000\n  Robust Tucker-Lewis Index (TLI)                0.294\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -7963.056\n  Loglikelihood unrestricted model (H1)      -7844.138\n                                                      \n  Akaike (AIC)                               15950.111\n  Bayesian (BIC)                             16008.159\n  Sample-size adjusted Bayesian (SABIC)      15970.048\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.082\n  90 Percent confidence interval - lower         0.071\n  90 Percent confidence interval - upper         0.092\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.607\n                                                      \n  Robust RMSEA                                   0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.000\n  P-value H_0: Robust RMSEA &lt;= 0.050             1.000\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.124\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\n\nGroup 1 [0]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 =~                                            \n    math2             1.000                           \n    math3             1.000                           \n    math4             1.000                           \n    math5             1.000                           \n    math6             1.000                           \n    math7             1.000                           \n    math8             1.000                           \n  eta_2 =~                                            \n    math2             0.000                           \n    math3             1.000                           \n    math4             2.000                           \n    math5             3.000                           \n    math6             4.000                           \n    math7             5.000                           \n    math8             6.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 ~~                                            \n    eta_2            -0.063    1.161   -0.054    0.957\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1            35.481    0.365   97.257    0.000\n    eta_2             4.297    0.091   47.145    0.000\n   .math2             0.000                           \n   .math3             0.000                           \n   .math4             0.000                           \n   .math5             0.000                           \n   .math6             0.000                           \n   .math7             0.000                           \n   .math8             0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1            62.287    5.728   10.873    0.000\n    eta_2             0.774    0.334    2.314    0.021\n   .math2   (tht1)   35.172    1.904   18.473    0.000\n   .math3   (tht1)   35.172    1.904   18.473    0.000\n   .math4   (tht1)   35.172    1.904   18.473    0.000\n   .math5   (tht1)   35.172    1.904   18.473    0.000\n   .math6   (tht1)   35.172    1.904   18.473    0.000\n   .math7   (tht1)   35.172    1.904   18.473    0.000\n   .math8   (tht1)   35.172    1.904   18.473    0.000\n\n\nGroup 2 [1]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 =~                                            \n    math2             1.000                           \n    math3             1.000                           \n    math4             1.000                           \n    math5             1.000                           \n    math6             1.000                           \n    math7             1.000                           \n    math8             1.000                           \n  eta_2 =~                                            \n    math2             0.000                           \n    math3             1.000                           \n    math4             2.000                           \n    math5             3.000                           \n    math6             4.000                           \n    math7             5.000                           \n    math8             6.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 ~~                                            \n    eta_2             0.745    5.522    0.135    0.893\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1            32.800    1.407   23.314    0.000\n    eta_2             4.873    0.341   14.298    0.000\n   .math2             0.000                           \n   .math3             0.000                           \n   .math4             0.000                           \n   .math5             0.000                           \n   .math6             0.000                           \n   .math7             0.000                           \n   .math8             0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1            79.627   25.629    3.107    0.002\n    eta_2            -0.157    1.477   -0.106    0.915\n   .math2   (tht2)   48.686    8.444    5.766    0.000\n   .math3   (tht2)   48.686    8.444    5.766    0.000\n   .math4   (tht2)   48.686    8.444    5.766    0.000\n   .math5   (tht2)   48.686    8.444    5.766    0.000\n   .math6   (tht2)   48.686    8.444    5.766    0.000\n   .math7   (tht2)   48.686    8.444    5.766    0.000\n   .math8   (tht2)   48.686    8.444    5.766    0.000\n\n\n\nFacciamo un confronto tra la bontà di adattamento del modello M3 e del modello M4.\n\nlavTestLRT(mg_math_lavaan_fitM3, mg_math_lavaan_fitM4) |&gt;\n    print()\n\n\nChi-Squared Difference Test\n\n                     Df   AIC   BIC  Chisq Chisq diff    RMSEA Df diff\nmg_math_lavaan_fitM4 58 15950 16008 237.84                            \nmg_math_lavaan_fitM3 59 15952 16005 241.18     3.3457 0.070948       1\n                     Pr(&gt;Chisq)  \nmg_math_lavaan_fitM4             \nmg_math_lavaan_fitM3    0.06738 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnche in questo caso non otteniamo un risultato che fornisce evidenza di differenze tra i due gruppi.\nIn sintesi, possiamo dire che le evidenze presenti suggeriscono che i modelli di crescita latente per di due gruppi hanno parametri uguali per ciò che concerne le saturazioni fattoriali, le medie, le varianze, le covarianze e i residui.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>80</span>  <span class='chapter-title'>Modelli di crescita latenti a gruppi multipli</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/10_growth_groups.html#riflessioni-conclusive",
    "href": "chapters/lgm/10_growth_groups.html#riflessioni-conclusive",
    "title": "80  Modelli di crescita latenti a gruppi multipli",
    "section": "80.6 Riflessioni Conclusive",
    "text": "80.6 Riflessioni Conclusive\nL’approccio a gruppi multipli, utilizzato per studiare le differenze interpersonali nei cambiamenti intrapersonali, è estremamente efficace. Qui ci siamo concentrati sui modelli di crescita lineare per descrivere i cambiamenti intrapersonali e le differenze interpersonali in questi cambiamenti, ma è anche possibile considerare modelli non lineari più complessi. Per esempio, in alcune situazioni, un gruppo potrebbe seguire una traiettoria di crescita lineare (ad esempio, un gruppo di controllo), mentre un altro potrebbe seguire una crescita esponenziale (ad esempio, un gruppo di intervento).\nQuando consideriamo l’ipotesi che gruppi diversi di individui possano seguire traiettorie di cambiamento intrapersonale differenti, l’utilità del framework a gruppi multipli diventa ancora più evidente. Abbiamo presentato il framework a gruppi multipli come alternativa all’approccio con covarianti invarianti nel tempo, tuttavia i due approcci possono essere integrati. Come descritto nel capitolo precedente, i covarianti invarianti nel tempo sono utilizzati per spiegare le differenze interpersonali nell’intercetta e nella pendenza. Includendo i covarianti invarianti nel tempo nel framework a gruppi multipli, possiamo spiegare la variabilità nell’intercetta e nella pendenza all’interno di ciascun gruppo e testare se le relazioni tra i covarianti invarianti nel tempo e l’intercetta e la pendenza differiscono tra i gruppi.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>80</span>  <span class='chapter-title'>Modelli di crescita latenti a gruppi multipli</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/11_lgm_wais.html",
    "href": "chapters/lgm/11_lgm_wais.html",
    "title": "\n81  Lo Sviluppo dell’Intelligenza\n",
    "section": "",
    "text": "81.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nQuesta discussione riproduce il tutorial presentato nel Workshop on Latent Growth Modeling in Lavaan tenuto al Donders Institute nel novembre 2024. Questo tutorial riprende in un unico studio i concetti che avevamo esaminato nei capitoli precedenti. Verranno utilizzati dei dati longitudinali relativi al WISC-V forniti dagli autori a 6, 7, 9 e 11 anni.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>81</span>  <span class='chapter-title'>Lo Sviluppo dell'Intelligenza</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/11_lgm_wais.html#dati",
    "href": "chapters/lgm/11_lgm_wais.html#dati",
    "title": "\n81  Lo Sviluppo dell’Intelligenza\n",
    "section": "\n81.2 Dati",
    "text": "81.2 Dati\nIl WISC-V Test (Wechsler Intelligence Scale for Children) è un test del QI somministrato a bambini di età compresa tra 6 e 16 anni. Fornisce cinque punteggi indici principali, ovvero Indice di Comprensione Verbale, Indice Visuo-Spaziale, Indice di Ragionamento Fluido, Indice di Memoria di Lavoro e Indice di Velocità di Elaborazione. Nel workshop gli autori discutono su un sottoinsieme contenente: Indice di Comprensione Verbale, Indice di Velocità di Elaborazione e il totale.\n\nwisc &lt;- rio::import(\n  here::here(\n    \"data\", \"wisc.csv\"\n  )\n)[,-1]\n\nhead(wisc)         #first 6 rows\n#&gt;   ID Verbal_T6 Verbal_T7 Verbal_T9 Verbal_T11 Pspeed_T6 Pspeed_T7 Pspeed_T9\n#&gt; 1  0      24.4      27.0      39.6       55.6     19.84      23.0      43.9\n#&gt; 2  1      12.4      14.4      21.9       37.8      5.90      13.4      18.3\n#&gt; 3  2      32.4      33.5      34.3       50.2     27.64      45.0      47.0\n#&gt; 4  3      22.7      28.4      42.2       44.7     33.16      29.7      46.0\n#&gt; 5  4      28.2      37.8      41.1       71.0     27.64      44.4      65.5\n#&gt; 6  5      16.1      20.1      38.0       39.9      8.45      15.8      27.0\n#&gt;   Pspeed_T11 Total_6 Total_7 Total_9 Total_11 age_T6 sex race mo_edu\n#&gt; 1       44.2   22.13    25.0    41.8     49.9   5.83   1    1      4\n#&gt; 2       40.4    9.17    13.9    20.1     39.1   5.92   2    2      6\n#&gt; 3       77.7   30.03    39.3    40.6     63.9   6.33   1    1      2\n#&gt; 4       61.7   27.93    29.0    44.1     53.2   6.33   2    1      2\n#&gt; 5       64.2   27.93    41.1    53.3     67.6   6.17   1    1      3\n#&gt; 6       39.1   12.25    17.9    32.5     39.5   5.67   1    1      2\n#&gt;   mo_educat fa_edu fa_educat\n#&gt; 1         0      4         0\n#&gt; 2         0      5         0\n#&gt; 3         2      3         1\n#&gt; 4         2      2         2\n#&gt; 5         1      3         1\n#&gt; 6         2      2         2\n\n\ndim(wisc)          #number of rows and columns\n#&gt; [1] 204  20\n\nGli autori si concentrano sull’analisi dei dati del subtest verbale.\n\nwisc_verbal &lt;- wisc[,c(\"ID\",\"Verbal_T6\",\"Verbal_T7\",\"Verbal_T9\",\"Verbal_T11\")]\nglimpse(wisc_verbal)\n#&gt; Rows: 204\n#&gt; Columns: 5\n#&gt; $ ID         &lt;int&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 14, 15, 16, 17, 21, 2…\n#&gt; $ Verbal_T6  &lt;dbl&gt; 24.4, 12.4, 32.4, 22.7, 28.2, 16.1, 8.5, 14.1, 15.5, 20…\n#&gt; $ Verbal_T7  &lt;dbl&gt; 27.0, 14.4, 33.5, 28.4, 37.8, 20.1, 16.5, 20.9, 23.4, 3…\n#&gt; $ Verbal_T9  &lt;dbl&gt; 39.6, 21.9, 34.3, 42.2, 41.1, 38.0, 28.7, 21.5, 37.4, 3…\n#&gt; $ Verbal_T11 &lt;dbl&gt; 55.6, 37.8, 50.2, 44.7, 71.0, 39.9, 40.8, 25.7, 45.5, 4…\n\nI dati vanno trasformati nel formato long.\n\nwisc_verbal_long &lt;- wisc_verbal %&gt;% \n  pivot_longer(!ID, names_to = \"wave\", values_to = \"verbal\")\n\nwisc_verbal_long |&gt; head()\n#&gt; # A tibble: 6 × 3\n#&gt;      ID wave       verbal\n#&gt;   &lt;int&gt; &lt;chr&gt;       &lt;dbl&gt;\n#&gt; 1     0 Verbal_T6    24.4\n#&gt; 2     0 Verbal_T7    27.0\n#&gt; 3     0 Verbal_T9    39.6\n#&gt; 4     0 Verbal_T11   55.6\n#&gt; 5     1 Verbal_T6    12.4\n#&gt; 6     1 Verbal_T7    14.4\n\nUn grafico dei dati si ottiene nel modo seguente.\n\nwisc_verbal_long$wave = factor(wisc_verbal_long$wave, levels=c(\"Verbal_T6\",\"Verbal_T7\",\"Verbal_T9\",\"Verbal_T11\"))\n\nggplot(wisc_verbal_long, aes(wave, verbal, group=ID, fill=ID, color=ID)) +\n  geom_point() + \n  geom_line() +\n  theme_classic(base_size = 15) + # adding a classic theme; https://ggplot2.tidyverse.org/reference/ggtheme.html\n  theme(legend.position = \"none\") + # getting rid of legend\n  labs(x = \"Wave\", y = \"Score on Verbal Subtest\")",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>81</span>  <span class='chapter-title'>Lo Sviluppo dell'Intelligenza</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/11_lgm_wais.html#modello-lineare",
    "href": "chapters/lgm/11_lgm_wais.html#modello-lineare",
    "title": "\n81  Lo Sviluppo dell’Intelligenza\n",
    "section": "\n81.3 Modello lineare",
    "text": "81.3 Modello lineare\nIl modello più semplice è quello di crescita lineare.\n\n# Create LGM\nlinear_growth_model &lt;- '\n  i =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11\n  s =~ 0*Verbal_T6 + 1*Verbal_T7 + 2*Verbal_T9 + 3*Verbal_T11'\n\nAdattiamo il modello ai dati ed esaminiamo i risultati.\n\n# Fit LGM\nfit_linear_growth_model &lt;- growth(linear_growth_model, data=wisc_verbal,missing='fiml')\n# Output results\nsummary(fit_linear_growth_model, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)\n#&gt; lavaan 0.6-19 ended normally after 65 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                         9\n#&gt; \n#&gt;   Number of observations                           204\n#&gt;   Number of missing patterns                         1\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                               100.756\n#&gt;   Degrees of freedom                                 5\n#&gt;   P-value (Chi-square)                           0.000\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               585.906\n#&gt;   Degrees of freedom                                 6\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.835\n#&gt;   Tucker-Lewis Index (TLI)                       0.802\n#&gt;                                                       \n#&gt;   Robust Comparative Fit Index (CFI)             0.835\n#&gt;   Robust Tucker-Lewis Index (TLI)                0.802\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -2530.194\n#&gt;   Loglikelihood unrestricted model (H1)      -2479.816\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                5078.388\n#&gt;   Bayesian (BIC)                              5108.251\n#&gt;   Sample-size adjusted Bayesian (SABIC)       5079.736\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.306\n#&gt;   90 Percent confidence interval - lower         0.256\n#&gt;   90 Percent confidence interval - upper         0.360\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.000\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    1.000\n#&gt;                                                       \n#&gt;   Robust RMSEA                                   0.306\n#&gt;   90 Percent confidence interval - lower         0.256\n#&gt;   90 Percent confidence interval - upper         0.360\n#&gt;   P-value H_0: Robust RMSEA &lt;= 0.050             0.000\n#&gt;   P-value H_0: Robust RMSEA &gt;= 0.080             1.000\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.113\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Observed\n#&gt;   Observed information based on                Hessian\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   i =~                                                                  \n#&gt;     Verbal_T6         1.000                               4.354    0.775\n#&gt;     Verbal_T7         1.000                               4.354    0.681\n#&gt;     Verbal_T9         1.000                               4.354    0.583\n#&gt;     Verbal_T11        1.000                               4.354    0.417\n#&gt;   s =~                                                                  \n#&gt;     Verbal_T6         0.000                               0.000    0.000\n#&gt;     Verbal_T7         1.000                               1.251    0.196\n#&gt;     Verbal_T9         2.000                               2.502    0.335\n#&gt;     Verbal_T11        3.000                               3.752    0.360\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   i ~~                                                                  \n#&gt;     s                 5.081    1.079    4.709    0.000    0.933    0.933\n#&gt; \n#&gt; Intercepts:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;     i                18.760    0.391   48.006    0.000    4.308    4.308\n#&gt;     s                 7.291    0.192   38.007    0.000    5.829    5.829\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .Verbal_T6        12.600    2.175    5.793    0.000   12.600    0.399\n#&gt;    .Verbal_T7        10.213    1.463    6.982    0.000   10.213    0.250\n#&gt;    .Verbal_T9        10.243    1.941    5.277    0.000   10.243    0.184\n#&gt;    .Verbal_T11       45.410    5.781    7.855    0.000   45.410    0.417\n#&gt;     i                18.961    3.154    6.012    0.000    1.000    1.000\n#&gt;     s                 1.565    0.658    2.379    0.017    1.000    1.000\n#&gt; \n#&gt; R-Square:\n#&gt;                    Estimate\n#&gt;     Verbal_T6         0.601\n#&gt;     Verbal_T7         0.750\n#&gt;     Verbal_T9         0.816\n#&gt;     Verbal_T11        0.583\n\nIl modello non si adatta bene ai dati.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>81</span>  <span class='chapter-title'>Lo Sviluppo dell'Intelligenza</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/11_lgm_wais.html#crescita-non-lineare",
    "href": "chapters/lgm/11_lgm_wais.html#crescita-non-lineare",
    "title": "\n81  Lo Sviluppo dell’Intelligenza\n",
    "section": "\n81.4 Crescita non lineare",
    "text": "81.4 Crescita non lineare\nNell’analisi precedente, abbiamo modellato un modello di crescita lineare. Tuttavia, è anche possibile modellare una crescita non lineare in lavaan come una traiettoria quadratica. Per fare ciò, è necessario aggiungere un terzo parametro chiamato termine quadratico che avrà gli stessi loadings del coefficiente angolare, ma al quadrato.\nPer fare questo, è necessario specificare un’altra variabile latente nel modello chiamata termine quadratico. Al termine quadratico vengono assegnati loadings che sono i quadrati dei loadings del coefficiente angolare.\n\n\n# Create quadratic growth model\nquad_growth_model &lt;- 'i =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11\n                      s =~ 0*Verbal_T6 + 1*Verbal_T7 + 2*Verbal_T9 + 3*Verbal_T11\n                      q =~ 0*Verbal_T6 + 1*Verbal_T7 + 4*Verbal_T9 + 9*Verbal_T11'\n# Fit model\nfit_quad_growth_model &lt;- growth(quad_growth_model, data=wisc_verbal,missing='fiml')\n# Output results\nsummary(fit_quad_growth_model, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)\n#&gt; lavaan 0.6-19 ended normally after 99 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        13\n#&gt; \n#&gt;   Number of observations                           204\n#&gt;   Number of missing patterns                         1\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                 6.176\n#&gt;   Degrees of freedom                                 1\n#&gt;   P-value (Chi-square)                           0.013\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               585.906\n#&gt;   Degrees of freedom                                 6\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.991\n#&gt;   Tucker-Lewis Index (TLI)                       0.946\n#&gt;                                                       \n#&gt;   Robust Comparative Fit Index (CFI)             0.991\n#&gt;   Robust Tucker-Lewis Index (TLI)                0.946\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -2482.904\n#&gt;   Loglikelihood unrestricted model (H1)      -2479.816\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                4991.808\n#&gt;   Bayesian (BIC)                              5034.943\n#&gt;   Sample-size adjusted Bayesian (SABIC)       4993.755\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.159\n#&gt;   90 Percent confidence interval - lower         0.059\n#&gt;   90 Percent confidence interval - upper         0.289\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.039\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.910\n#&gt;                                                       \n#&gt;   Robust RMSEA                                   0.159\n#&gt;   90 Percent confidence interval - lower         0.059\n#&gt;   90 Percent confidence interval - upper         0.289\n#&gt;   P-value H_0: Robust RMSEA &lt;= 0.050             0.039\n#&gt;   P-value H_0: Robust RMSEA &gt;= 0.080             0.910\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.023\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Observed\n#&gt;   Observed information based on                Hessian\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   i =~                                                                  \n#&gt;     Verbal_T6         1.000                               4.883    0.843\n#&gt;     Verbal_T7         1.000                               4.883    0.800\n#&gt;     Verbal_T9         1.000                               4.883    0.668\n#&gt;     Verbal_T11        1.000                               4.883    0.459\n#&gt;   s =~                                                                  \n#&gt;     Verbal_T6         0.000                                  NA       NA\n#&gt;     Verbal_T7         1.000                                  NA       NA\n#&gt;     Verbal_T9         2.000                                  NA       NA\n#&gt;     Verbal_T11        3.000                                  NA       NA\n#&gt;   q =~                                                                  \n#&gt;     Verbal_T6         0.000                                  NA       NA\n#&gt;     Verbal_T7         1.000                                  NA       NA\n#&gt;     Verbal_T9         4.000                                  NA       NA\n#&gt;     Verbal_T11        9.000                                  NA       NA\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   i ~~                                                                  \n#&gt;     s                -0.564    6.937   -0.081    0.935   -0.064   -0.064\n#&gt;     q                 2.014    1.811    1.112    0.266    0.738    0.738\n#&gt;   s ~~                                                                  \n#&gt;     q                 1.518    1.719    0.883    0.377    1.500    1.500\n#&gt; \n#&gt; Intercepts:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;     i                19.697    0.409   48.124    0.000    4.033    4.033\n#&gt;     s                 4.051    0.354   11.439    0.000       NA       NA\n#&gt;     q                 1.284    0.130    9.861    0.000       NA       NA\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .Verbal_T6         9.730    6.209    1.567    0.117    9.730    0.290\n#&gt;    .Verbal_T7        11.059    2.297    4.814    0.000   11.059    0.297\n#&gt;    .Verbal_T9         9.542    2.677    3.564    0.000    9.542    0.179\n#&gt;    .Verbal_T11       29.417   11.518    2.554    0.011   29.417    0.260\n#&gt;     i                23.848    6.558    3.636    0.000    1.000    1.000\n#&gt;     s                -3.277    7.053   -0.465    0.642       NA       NA\n#&gt;     q                -0.312    0.656   -0.476    0.634       NA       NA\n#&gt; \n#&gt; R-Square:\n#&gt;                    Estimate\n#&gt;     Verbal_T6         0.710\n#&gt;     Verbal_T7         0.703\n#&gt;     Verbal_T9         0.821\n#&gt;     Verbal_T11        0.740\n\nÈ anche possibile modellare una crescita non lineare in lavaan senza alcuna ipotesi sulla forma. Per farlo, si fissano i loadings della prima e dell’ultima misurazione, ma si stimano liberamente quelli intermedi.\n\n# Create non-linear growth model\nbasis_growth_model &lt;- 'i =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11\n                       s =~ 0*Verbal_T6 + Verbal_T7 + Verbal_T9 + 1*Verbal_T11'\n# Fit model\nfit_basis_growth_model &lt;- growth(basis_growth_model, data=wisc_verbal,missing='fiml')\n# Output results\nsummary(fit_basis_growth_model, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)\n#&gt; lavaan 0.6-19 ended normally after 109 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        11\n#&gt; \n#&gt;   Number of observations                           204\n#&gt;   Number of missing patterns                         1\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                 5.893\n#&gt;   Degrees of freedom                                 3\n#&gt;   P-value (Chi-square)                           0.117\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               585.906\n#&gt;   Degrees of freedom                                 6\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.995\n#&gt;   Tucker-Lewis Index (TLI)                       0.990\n#&gt;                                                       \n#&gt;   Robust Comparative Fit Index (CFI)             0.995\n#&gt;   Robust Tucker-Lewis Index (TLI)                0.990\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -2482.763\n#&gt;   Loglikelihood unrestricted model (H1)      -2479.816\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                4987.525\n#&gt;   Bayesian (BIC)                              5024.024\n#&gt;   Sample-size adjusted Bayesian (SABIC)       4989.173\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.069\n#&gt;   90 Percent confidence interval - lower         0.000\n#&gt;   90 Percent confidence interval - upper         0.151\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.275\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.491\n#&gt;                                                       \n#&gt;   Robust RMSEA                                   0.069\n#&gt;   90 Percent confidence interval - lower         0.000\n#&gt;   90 Percent confidence interval - upper         0.151\n#&gt;   P-value H_0: Robust RMSEA &lt;= 0.050             0.275\n#&gt;   P-value H_0: Robust RMSEA &gt;= 0.080             0.491\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.043\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Observed\n#&gt;   Observed information based on                Hessian\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   i =~                                                                  \n#&gt;     Verbal_T6         1.000                               4.610    0.825\n#&gt;     Verbal_T7         1.000                               4.610    0.731\n#&gt;     Verbal_T9         1.000                               4.610    0.620\n#&gt;     Verbal_T11        1.000                               4.610    0.446\n#&gt;   s =~                                                                  \n#&gt;     Verbal_T6         0.000                               0.000    0.000\n#&gt;     Verbal_T7         0.237    0.012   20.223    0.000    1.300    0.206\n#&gt;     Verbal_T9         0.536    0.012   43.295    0.000    2.937    0.395\n#&gt;     Verbal_T11        1.000                               5.484    0.531\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   i ~~                                                                  \n#&gt;     s                15.072    3.133    4.811    0.000    0.596    0.596\n#&gt; \n#&gt; Intercepts:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;     i                19.634    0.391   50.221    0.000    4.259    4.259\n#&gt;     s                24.180    0.565   42.773    0.000    4.409    4.409\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .Verbal_T6         9.974    1.645    6.063    0.000    9.974    0.319\n#&gt;    .Verbal_T7         9.704    1.307    7.422    0.000    9.704    0.244\n#&gt;    .Verbal_T9         9.217    1.513    6.093    0.000    9.217    0.167\n#&gt;    .Verbal_T11       25.340    4.317    5.870    0.000   25.340    0.237\n#&gt;     i                21.255    2.898    7.335    0.000    1.000    1.000\n#&gt;     s                30.076    6.788    4.430    0.000    1.000    1.000\n#&gt; \n#&gt; R-Square:\n#&gt;                    Estimate\n#&gt;     Verbal_T6         0.681\n#&gt;     Verbal_T7         0.756\n#&gt;     Verbal_T9         0.833\n#&gt;     Verbal_T11        0.763\n\n\n# Compare model fit\nanova(fit_linear_growth_model, fit_quad_growth_model)\n#&gt; \n#&gt; Chi-Squared Difference Test\n#&gt; \n#&gt;                         Df  AIC  BIC  Chisq Chisq diff RMSEA Df diff\n#&gt; fit_quad_growth_model    1 4992 5035   6.18                         \n#&gt; fit_linear_growth_model  5 5078 5108 100.76       94.6 0.333       4\n#&gt;                         Pr(&gt;Chisq)\n#&gt; fit_quad_growth_model             \n#&gt; fit_linear_growth_model     &lt;2e-16\n\nIl modello non lineare in lavaan senza alcuna ipotesi sulla forma e il modello quadratico non sono annidati. Pertanto un test del rapporto di verosimiglianza non è possibile. Tuttavia, gli indici di bontà di adattamento del modello senza ipotesi sulla forma sono migliori del modello quadratico, per cui sarà quello il modello prescelto.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>81</span>  <span class='chapter-title'>Lo Sviluppo dell'Intelligenza</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/11_lgm_wais.html#predizioni",
    "href": "chapters/lgm/11_lgm_wais.html#predizioni",
    "title": "\n81  Lo Sviluppo dell’Intelligenza\n",
    "section": "\n81.5 Predizioni",
    "text": "81.5 Predizioni\nSi potrebbe essere interessati a ciò che predice i punteggi di base e/o il cambiamento. Per valutare questo, si possono aggiungere predittori nel modello di crescita. Un’ipotesi potrebbe essere che il livello di istruzione della madre predica lo sviluppo della comprensione verbale.\n\n# Specify model\nbasis_growth_model_cov &lt;- ' \n  i =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11\n  s =~ 0*Verbal_T6 + Verbal_T7 + Verbal_T9 + 1*Verbal_T11 \n  s~mo_edu\n  i~mo_edu\n  '\n\n\n# Fit model\nfit_basis_growth_model_cov &lt;- growth(basis_growth_model_cov, data=wisc,missing='fiml')\n# Output results\nsummary(fit_basis_growth_model_cov, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)\n#&gt; lavaan 0.6-19 ended normally after 118 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        13\n#&gt; \n#&gt;   Number of observations                           204\n#&gt;   Number of missing patterns                         1\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                 6.498\n#&gt;   Degrees of freedom                                 5\n#&gt;   P-value (Chi-square)                           0.261\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               650.266\n#&gt;   Degrees of freedom                                10\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.998\n#&gt;   Tucker-Lewis Index (TLI)                       0.995\n#&gt;                                                       \n#&gt;   Robust Comparative Fit Index (CFI)             0.998\n#&gt;   Robust Tucker-Lewis Index (TLI)                0.995\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -2450.885\n#&gt;   Loglikelihood unrestricted model (H1)      -2447.636\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                4927.770\n#&gt;   Bayesian (BIC)                              4970.906\n#&gt;   Sample-size adjusted Bayesian (SABIC)       4929.718\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.038\n#&gt;   90 Percent confidence interval - lower         0.000\n#&gt;   90 Percent confidence interval - upper         0.110\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.520\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.210\n#&gt;                                                       \n#&gt;   Robust RMSEA                                   0.038\n#&gt;   90 Percent confidence interval - lower         0.000\n#&gt;   90 Percent confidence interval - upper         0.110\n#&gt;   P-value H_0: Robust RMSEA &lt;= 0.050             0.520\n#&gt;   P-value H_0: Robust RMSEA &gt;= 0.080             0.210\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.038\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Observed\n#&gt;   Observed information based on                Hessian\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   i =~                                                                  \n#&gt;     Verbal_T6         1.000                               4.615    0.826\n#&gt;     Verbal_T7         1.000                               4.615    0.732\n#&gt;     Verbal_T9         1.000                               4.615    0.619\n#&gt;     Verbal_T11        1.000                               4.615    0.447\n#&gt;   s =~                                                                  \n#&gt;     Verbal_T6         0.000                               0.000    0.000\n#&gt;     Verbal_T7         0.237    0.012   20.312    0.000    1.306    0.207\n#&gt;     Verbal_T9         0.535    0.012   43.171    0.000    2.949    0.396\n#&gt;     Verbal_T11        1.000                               5.508    0.534\n#&gt; \n#&gt; Regressions:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   s ~                                                                   \n#&gt;     mo_edu           -1.724    0.414   -4.165    0.000   -0.313   -0.394\n#&gt;   i ~                                                                   \n#&gt;     mo_edu           -1.943    0.259   -7.503    0.000   -0.421   -0.531\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;  .i ~~                                                                  \n#&gt;    .s                 9.676    2.721    3.556    0.000    0.489    0.489\n#&gt; \n#&gt; Intercepts:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .i                26.302    0.958   27.446    0.000    5.700    5.700\n#&gt;    .s                30.098    1.527   19.710    0.000    5.464    5.464\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .Verbal_T6         9.923    1.622    6.117    0.000    9.923    0.318\n#&gt;    .Verbal_T7         9.607    1.281    7.500    0.000    9.607    0.242\n#&gt;    .Verbal_T9         9.443    1.501    6.291    0.000    9.443    0.170\n#&gt;    .Verbal_T11       24.956    4.288    5.820    0.000   24.956    0.234\n#&gt;    .i                15.298    2.309    6.624    0.000    0.718    0.718\n#&gt;    .s                25.619    6.352    4.033    0.000    0.844    0.844\n#&gt; \n#&gt; R-Square:\n#&gt;                    Estimate\n#&gt;     Verbal_T6         0.682\n#&gt;     Verbal_T7         0.758\n#&gt;     Verbal_T9         0.830\n#&gt;     Verbal_T11        0.766\n#&gt;     i                 0.282\n#&gt;     s                 0.156\n\nI risultati indicano come il livello di educazione della madre influenza sia il valore di base delle abilità verbali del bambino, sia il tasso di crescita.\nAggiungiamo ora la velocità di elaborazione a 11 anni come esito dei cambiamenti nella comprensione verbale. In altre parole, verifichiamo se le pendenze del cambiamento verbale predicono il livello di velocità di elaborazione a 11.\n\n# Specify model\nbasis_growth_model_covO &lt;- ' \n  i =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11\n  s =~ 0*Verbal_T6 + Verbal_T7 + Verbal_T9 + 1*Verbal_T11 \n  Pspeed_T11~s\n  Pspeed_T11~1\n'\n\n# Fit model\nfit_basis_growth_model_covO &lt;- growth(basis_growth_model_covO, data=wisc,missing='fiml')\n# Output results\nsummary(fit_basis_growth_model_covO, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)\n#&gt; lavaan 0.6-19 ended normally after 142 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        14\n#&gt; \n#&gt;   Number of observations                           204\n#&gt;   Number of missing patterns                         1\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                14.016\n#&gt;   Degrees of freedom                                 6\n#&gt;   P-value (Chi-square)                           0.029\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               685.769\n#&gt;   Degrees of freedom                                10\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.988\n#&gt;   Tucker-Lewis Index (TLI)                       0.980\n#&gt;                                                       \n#&gt;   Robust Comparative Fit Index (CFI)             0.988\n#&gt;   Robust Tucker-Lewis Index (TLI)                0.980\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -3240.780\n#&gt;   Loglikelihood unrestricted model (H1)      -3233.772\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                6509.560\n#&gt;   Bayesian (BIC)                              6556.014\n#&gt;   Sample-size adjusted Bayesian (SABIC)       6511.657\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.081\n#&gt;   90 Percent confidence interval - lower         0.024\n#&gt;   90 Percent confidence interval - upper         0.137\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.151\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.566\n#&gt;                                                       \n#&gt;   Robust RMSEA                                   0.081\n#&gt;   90 Percent confidence interval - lower         0.024\n#&gt;   90 Percent confidence interval - upper         0.137\n#&gt;   P-value H_0: Robust RMSEA &lt;= 0.050             0.151\n#&gt;   P-value H_0: Robust RMSEA &gt;= 0.080             0.566\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.043\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Observed\n#&gt;   Observed information based on                Hessian\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   i =~                                                                  \n#&gt;     Verbal_T6         1.000                               4.476    0.804\n#&gt;     Verbal_T7         1.000                               4.476    0.714\n#&gt;     Verbal_T9         1.000                               4.476    0.598\n#&gt;     Verbal_T11        1.000                               4.476    0.434\n#&gt;   s =~                                                                  \n#&gt;     Verbal_T6         0.000                               0.000    0.000\n#&gt;     Verbal_T7         0.237    0.012   19.886    0.000    1.220    0.195\n#&gt;     Verbal_T9         0.534    0.013   42.125    0.000    2.752    0.367\n#&gt;     Verbal_T11        1.000                               5.157    0.500\n#&gt; \n#&gt; Regressions:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   Pspeed_T11 ~                                                          \n#&gt;     s                 1.683    0.219    7.690    0.000    8.680    0.697\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   i ~~                                                                  \n#&gt;     s                17.195    2.513    6.842    0.000    0.745    0.745\n#&gt; \n#&gt; Intercepts:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .Pspeed_T11       10.214    5.368    1.903    0.057   10.214    0.820\n#&gt;     i                19.648    0.390   50.431    0.000    4.389    4.389\n#&gt;     s                24.194    0.554   43.672    0.000    4.691    4.691\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .Verbal_T6        10.997    1.451    7.578    0.000   10.997    0.354\n#&gt;    .Verbal_T7         9.658    1.299    7.436    0.000    9.658    0.246\n#&gt;    .Verbal_T9        10.154    1.529    6.640    0.000   10.154    0.181\n#&gt;    .Verbal_T11       25.254    3.694    6.836    0.000   25.254    0.238\n#&gt;    .Pspeed_T11       79.657   10.996    7.244    0.000   79.657    0.514\n#&gt;     i                20.036    2.643    7.580    0.000    1.000    1.000\n#&gt;     s                26.598    5.701    4.666    0.000    1.000    1.000\n#&gt; \n#&gt; R-Square:\n#&gt;                    Estimate\n#&gt;     Verbal_T6         0.646\n#&gt;     Verbal_T7         0.754\n#&gt;     Verbal_T9         0.819\n#&gt;     Verbal_T11        0.762\n#&gt;     Pspeed_T11        0.486\n\nI dati mostrano come le pendenze del cambiamento verbale effettivamente predicono il livello di velocità di elaborazione a 11 anni.\nI predittori tempo-invarianti sono predittori delle differenze individuali nelle intercette e nelle pendenze. Sono spesso misurati al basale (ad esempio, reddito familiare) o sono caratteristiche specifiche della persona il cui valore è costante nel tempo (ad esempio, sesso biologico, paese di origine). Ad esempio, nelle analisi precedenti, il livello di istruzione della madre e la velocità di elaborazione a 6 anni sono predittori tempo-invarianti.\nI predittori tempo-varianti sono predittori dell’esito in ogni punto temporale. Nel nostro esempio, ad esempio, avremmo bisogno di misurazioni a T6, T7, T9 e T11.\nIn questo ultimo modello useremo la velocità di elaborazione come predittore tempo-variante della misurazione verbale in ogni punto temporale. Ci chiediamo le seguenti domande. Come sono l’intercetta e la pendenza delle misure verbali? La velocità di elaborazione predice le misure verbali allo stesso modo in tutti i punti temporali?\n\n# Specify model\nbasis_growth_model_tvp &lt;- ' \n  i =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11\n  s =~ 0*Verbal_T6 + Verbal_T7 + Verbal_T9 + 1*Verbal_T11 \n  Verbal_T6~Pspeed_T6\n  Verbal_T7~Pspeed_T7\n  Verbal_T9~Pspeed_T9\n  Verbal_T11~Pspeed_T11\n  '\n# Fit LGM\nfit_basis_growth_model_tvp &lt;- growth(basis_growth_model_tvp, data=wisc,missing='fiml')\n# Output results\nsummary(fit_basis_growth_model_tvp, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)\n#&gt; lavaan 0.6-19 ended normally after 96 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        15\n#&gt; \n#&gt;   Number of observations                           204\n#&gt;   Number of missing patterns                         1\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                90.277\n#&gt;   Degrees of freedom                                15\n#&gt;   P-value (Chi-square)                           0.000\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                               754.793\n#&gt;   Degrees of freedom                                22\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.897\n#&gt;   Tucker-Lewis Index (TLI)                       0.849\n#&gt;                                                       \n#&gt;   Robust Comparative Fit Index (CFI)             0.897\n#&gt;   Robust Tucker-Lewis Index (TLI)                0.849\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -2440.511\n#&gt;   Loglikelihood unrestricted model (H1)      -2395.373\n#&gt;                                                       \n#&gt;   Akaike (AIC)                                4911.022\n#&gt;   Bayesian (BIC)                              4960.794\n#&gt;   Sample-size adjusted Bayesian (SABIC)       4913.269\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.157\n#&gt;   90 Percent confidence interval - lower         0.127\n#&gt;   90 Percent confidence interval - upper         0.189\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.000\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    1.000\n#&gt;                                                       \n#&gt;   Robust RMSEA                                   0.157\n#&gt;   90 Percent confidence interval - lower         0.127\n#&gt;   90 Percent confidence interval - upper         0.189\n#&gt;   P-value H_0: Robust RMSEA &lt;= 0.050             0.000\n#&gt;   P-value H_0: Robust RMSEA &gt;= 0.080             1.000\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.194\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Observed\n#&gt;   Observed information based on                Hessian\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   i =~                                                                  \n#&gt;     Verbal_T6         1.000                               3.628    0.710\n#&gt;     Verbal_T7         1.000                               3.628    0.627\n#&gt;     Verbal_T9         1.000                               3.628    0.535\n#&gt;     Verbal_T11        1.000                               3.628    0.386\n#&gt;   s =~                                                                  \n#&gt;     Verbal_T6         0.000                               0.000    0.000\n#&gt;     Verbal_T7         0.297    0.071    4.209    0.000    1.337    0.231\n#&gt;     Verbal_T9         0.703    0.108    6.531    0.000    3.161    0.466\n#&gt;     Verbal_T11        1.000                               4.498    0.479\n#&gt; \n#&gt; Regressions:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   Verbal_T6 ~                                                           \n#&gt;     Pspeed_T6         0.243    0.038    6.467    0.000    0.243    0.397\n#&gt;   Verbal_T7 ~                                                           \n#&gt;     Pspeed_T7         0.230    0.034    6.853    0.000    0.230    0.397\n#&gt;   Verbal_T9 ~                                                           \n#&gt;     Pspeed_T9         0.220    0.036    6.130    0.000    0.220    0.332\n#&gt;   Verbal_T11 ~                                                          \n#&gt;     Pspeed_T11        0.319    0.039    8.233    0.000    0.319    0.423\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   i ~~                                                                  \n#&gt;     s                 5.873    2.723    2.157    0.031    0.360    0.360\n#&gt; \n#&gt; Intercepts:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;     i                15.271    0.754   20.245    0.000    4.209    4.209\n#&gt;     s                12.341    1.994    6.190    0.000    2.744    2.744\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .Verbal_T6         8.814    1.705    5.170    0.000    8.814    0.338\n#&gt;    .Verbal_T7         9.806    1.311    7.478    0.000    9.806    0.292\n#&gt;    .Verbal_T9         9.583    1.978    4.846    0.000    9.583    0.208\n#&gt;    .Verbal_T11       27.351    4.282    6.387    0.000   27.351    0.310\n#&gt;     i                13.165    2.347    5.609    0.000    1.000    1.000\n#&gt;     s                20.235    6.197    3.265    0.001    1.000    1.000\n#&gt; \n#&gt; R-Square:\n#&gt;                    Estimate\n#&gt;     Verbal_T6         0.662\n#&gt;     Verbal_T7         0.708\n#&gt;     Verbal_T9         0.792\n#&gt;     Verbal_T11        0.690",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>81</span>  <span class='chapter-title'>Lo Sviluppo dell'Intelligenza</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/11_lgm_wais.html#interazione-tra-pendenza-e-intercetta",
    "href": "chapters/lgm/11_lgm_wais.html#interazione-tra-pendenza-e-intercetta",
    "title": "\n81  Lo Sviluppo dell’Intelligenza\n",
    "section": "\n81.6 Interazione tra pendenza e intercetta",
    "text": "81.6 Interazione tra pendenza e intercetta\nOra che sappiamo come stimare la traiettoria di una variabile, siamo in grado di stimare la traiettoria di due variabili e vedere come interagiscono.\nNell’analisi successiva, creiamo due modelli di crescita non lineari, uno per la comprensione verbale e uno per la velocità di elaborazione. Correliamo i cambiamenti delle due metriche e ci chiediamo se loro pendenze sono correlate.\n\n# Specify model\nbasis_growth_model_cor_ver_pro &lt;- ' \n  i_verbal =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11\n  s_verbal =~ 0*Verbal_T6 + Verbal_T7 + Verbal_T9 + 1*Verbal_T11 \n  i_processpeed =~ 1*Pspeed_T6 + 1*Pspeed_T7 + 1*Pspeed_T9 + 1*Pspeed_T11\n  s_processpeed =~ 0*Pspeed_T6 + Pspeed_T7 + Pspeed_T9 + 1*Pspeed_T11 \n  s_verbal ~~ s_processpeed\n'\n\n# Fit LGM\nfit_basis_growth_model_cor_ver_pro &lt;- growth(basis_growth_model_cor_ver_pro, data=wisc,missing='fiml')\n# Output results\nsummary(fit_basis_growth_model_cor_ver_pro, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)\n#&gt; lavaan 0.6-19 ended normally after 211 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        26\n#&gt; \n#&gt;   Number of observations                           204\n#&gt;   Number of missing patterns                         1\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                29.305\n#&gt;   Degrees of freedom                                18\n#&gt;   P-value (Chi-square)                           0.045\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                              1423.083\n#&gt;   Degrees of freedom                                28\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.992\n#&gt;   Tucker-Lewis Index (TLI)                       0.987\n#&gt;                                                       \n#&gt;   Robust Comparative Fit Index (CFI)             0.992\n#&gt;   Robust Tucker-Lewis Index (TLI)                0.987\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -5124.285\n#&gt;   Loglikelihood unrestricted model (H1)      -5109.632\n#&gt;                                                       \n#&gt;   Akaike (AIC)                               10300.570\n#&gt;   Bayesian (BIC)                             10386.841\n#&gt;   Sample-size adjusted Bayesian (SABIC)      10304.465\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.055\n#&gt;   90 Percent confidence interval - lower         0.009\n#&gt;   90 Percent confidence interval - upper         0.091\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.367\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.137\n#&gt;                                                       \n#&gt;   Robust RMSEA                                   0.055\n#&gt;   90 Percent confidence interval - lower         0.009\n#&gt;   90 Percent confidence interval - upper         0.091\n#&gt;   P-value H_0: Robust RMSEA &lt;= 0.050             0.367\n#&gt;   P-value H_0: Robust RMSEA &gt;= 0.080             0.137\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.048\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Observed\n#&gt;   Observed information based on                Hessian\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   i_verbal =~                                                           \n#&gt;     Verbal_T6         1.000                               4.637    0.832\n#&gt;     Verbal_T7         1.000                               4.637    0.734\n#&gt;     Verbal_T9         1.000                               4.637    0.617\n#&gt;     Verbal_T11        1.000                               4.637    0.451\n#&gt;   s_verbal =~                                                           \n#&gt;     Verbal_T6         0.000                               0.000    0.000\n#&gt;     Verbal_T7         0.237    0.012   20.495    0.000    1.349    0.213\n#&gt;     Verbal_T9         0.533    0.012   43.283    0.000    3.037    0.404\n#&gt;     Verbal_T11        1.000                               5.694    0.554\n#&gt;   i_processpeed =~                                                      \n#&gt;     Pspeed_T6         1.000                               7.604    0.902\n#&gt;     Pspeed_T7         1.000                               7.604    0.799\n#&gt;     Pspeed_T9         1.000                               7.604    0.713\n#&gt;     Pspeed_T11        1.000                               7.604    0.617\n#&gt;   s_processpeed =~                                                      \n#&gt;     Pspeed_T6         0.000                               0.000    0.000\n#&gt;     Pspeed_T7         0.298    0.011   26.220    0.000    1.841    0.194\n#&gt;     Pspeed_T9         0.648    0.012   53.375    0.000    4.005    0.376\n#&gt;     Pspeed_T11        1.000                               6.183    0.502\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   s_verbal ~~                                                           \n#&gt;     s_processpeed    16.813    4.806    3.499    0.000    0.478    0.478\n#&gt;   i_verbal ~~                                                           \n#&gt;     s_verbal         14.606    3.112    4.694    0.000    0.553    0.553\n#&gt;     i_processpeed    26.537    3.572    7.430    0.000    0.753    0.753\n#&gt;     s_processpeed     2.520    3.154    0.799    0.424    0.088    0.088\n#&gt;   s_verbal ~~                                                           \n#&gt;     i_processpeed    25.796    4.875    5.291    0.000    0.596    0.596\n#&gt;   i_processpeed ~~                                                      \n#&gt;     s_processpeed    14.974    5.451    2.747    0.006    0.319    0.319\n#&gt; \n#&gt; Intercepts:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;     i_verbal         19.642    0.390   50.360    0.000    4.236    4.236\n#&gt;     s_verbal         24.200    0.561   43.138    0.000    4.250    4.250\n#&gt;     i_processpeed    17.949    0.590   30.419    0.000    2.360    2.360\n#&gt;     s_processpeed    32.986    0.615   53.609    0.000    5.335    5.335\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .Verbal_T6         9.574    1.581    6.055    0.000    9.574    0.308\n#&gt;    .Verbal_T7         9.698    1.269    7.643    0.000    9.698    0.243\n#&gt;    .Verbal_T9        10.149    1.491    6.806    0.000   10.149    0.180\n#&gt;    .Verbal_T11       22.419    4.039    5.551    0.000   22.419    0.212\n#&gt;    .Pspeed_T6        13.286    2.911    4.565    0.000   13.286    0.187\n#&gt;    .Pspeed_T7        20.338    2.534    8.026    0.000   20.338    0.225\n#&gt;    .Pspeed_T9        20.430    2.945    6.937    0.000   20.430    0.180\n#&gt;    .Pspeed_T11       25.840    5.200    4.969    0.000   25.840    0.170\n#&gt;     i_verbal         21.502    2.893    7.432    0.000    1.000    1.000\n#&gt;     s_verbal         32.425    6.990    4.639    0.000    1.000    1.000\n#&gt;     i_processpeed    57.821    6.889    8.393    0.000    1.000    1.000\n#&gt;     s_processpeed    38.226    8.968    4.263    0.000    1.000    1.000\n#&gt; \n#&gt; R-Square:\n#&gt;                    Estimate\n#&gt;     Verbal_T6         0.692\n#&gt;     Verbal_T7         0.757\n#&gt;     Verbal_T9         0.820\n#&gt;     Verbal_T11        0.788\n#&gt;     Pspeed_T6         0.813\n#&gt;     Pspeed_T7         0.775\n#&gt;     Pspeed_T9         0.820\n#&gt;     Pspeed_T11        0.830\n\n\n# Specify model\nbasis_growth_model_pred_ver_pro &lt;- ' \n  i_verbal =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11\n  s_verbal =~ 0*Verbal_T6 + Verbal_T7 + Verbal_T9 + 1*Verbal_T11 \n  i_processpeed =~ 1*Pspeed_T6 + 1*Pspeed_T7 + 1*Pspeed_T9 + 1*Pspeed_T11\n  s_processpeed =~ 0*Pspeed_T6 + Pspeed_T7 + Pspeed_T9 + 1*Pspeed_T11 \n  s_verbal ~ i_processpeed\n  s_processpeed ~ i_verbal'\n\n# Fit LGM\nfit_basis_growth_model_pred_ver_pro &lt;- growth(basis_growth_model_pred_ver_pro, data=wisc,missing='fiml')\n# Output results\nsummary(fit_basis_growth_model_pred_ver_pro, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)\n#&gt; lavaan 0.6-19 ended normally after 175 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        24\n#&gt; \n#&gt;   Number of observations                           204\n#&gt;   Number of missing patterns                         1\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                                38.158\n#&gt;   Degrees of freedom                                20\n#&gt;   P-value (Chi-square)                           0.008\n#&gt; \n#&gt; Model Test Baseline Model:\n#&gt; \n#&gt;   Test statistic                              1423.083\n#&gt;   Degrees of freedom                                28\n#&gt;   P-value                                        0.000\n#&gt; \n#&gt; User Model versus Baseline Model:\n#&gt; \n#&gt;   Comparative Fit Index (CFI)                    0.987\n#&gt;   Tucker-Lewis Index (TLI)                       0.982\n#&gt;                                                       \n#&gt;   Robust Comparative Fit Index (CFI)             0.987\n#&gt;   Robust Tucker-Lewis Index (TLI)                0.982\n#&gt; \n#&gt; Loglikelihood and Information Criteria:\n#&gt; \n#&gt;   Loglikelihood user model (H0)              -5128.711\n#&gt;   Loglikelihood unrestricted model (H1)      -5109.632\n#&gt;                                                       \n#&gt;   Akaike (AIC)                               10305.422\n#&gt;   Bayesian (BIC)                             10385.057\n#&gt;   Sample-size adjusted Bayesian (SABIC)      10309.018\n#&gt; \n#&gt; Root Mean Square Error of Approximation:\n#&gt; \n#&gt;   RMSEA                                          0.067\n#&gt;   90 Percent confidence interval - lower         0.033\n#&gt;   90 Percent confidence interval - upper         0.099\n#&gt;   P-value H_0: RMSEA &lt;= 0.050                    0.181\n#&gt;   P-value H_0: RMSEA &gt;= 0.080                    0.268\n#&gt;                                                       \n#&gt;   Robust RMSEA                                   0.067\n#&gt;   90 Percent confidence interval - lower         0.033\n#&gt;   90 Percent confidence interval - upper         0.099\n#&gt;   P-value H_0: Robust RMSEA &lt;= 0.050             0.181\n#&gt;   P-value H_0: Robust RMSEA &gt;= 0.080             0.268\n#&gt; \n#&gt; Standardized Root Mean Square Residual:\n#&gt; \n#&gt;   SRMR                                           0.055\n#&gt; \n#&gt; Parameter Estimates:\n#&gt; \n#&gt;   Standard errors                             Standard\n#&gt;   Information                                 Observed\n#&gt;   Observed information based on                Hessian\n#&gt; \n#&gt; Latent Variables:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   i_verbal =~                                                           \n#&gt;     Verbal_T6         1.000                               4.809    0.856\n#&gt;     Verbal_T7         1.000                               4.809    0.760\n#&gt;     Verbal_T9         1.000                               4.809    0.647\n#&gt;     Verbal_T11        1.000                               4.809    0.477\n#&gt;   s_verbal =~                                                           \n#&gt;     Verbal_T6         0.000                               0.000    0.000\n#&gt;     Verbal_T7         0.238    0.011   21.065    0.000    1.421    0.225\n#&gt;     Verbal_T9         0.534    0.012   43.708    0.000    3.189    0.429\n#&gt;     Verbal_T11        1.000                               5.977    0.593\n#&gt;   i_processpeed =~                                                      \n#&gt;     Pspeed_T6         1.000                               7.861    0.929\n#&gt;     Pspeed_T7         1.000                               7.861    0.831\n#&gt;     Pspeed_T9         1.000                               7.861    0.756\n#&gt;     Pspeed_T11        1.000                               7.861    0.662\n#&gt;   s_processpeed =~                                                      \n#&gt;     Pspeed_T6         0.000                               0.000    0.000\n#&gt;     Pspeed_T7         0.299    0.011   26.953    0.000    2.078    0.220\n#&gt;     Pspeed_T9         0.648    0.012   53.898    0.000    4.495    0.432\n#&gt;     Pspeed_T11        1.000                               6.940    0.585\n#&gt; \n#&gt; Regressions:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   s_verbal ~                                                            \n#&gt;     i_processpeed     0.408    0.071    5.782    0.000    0.537    0.537\n#&gt;   s_processpeed ~                                                       \n#&gt;     i_verbal          0.143    0.142    1.013    0.311    0.099    0.099\n#&gt; \n#&gt; Covariances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;   i_verbal ~~                                                           \n#&gt;     i_processpeed    26.674    3.649    7.310    0.000    0.706    0.706\n#&gt;  .s_verbal ~~                                                           \n#&gt;    .s_processpeed    10.954    5.052    2.168    0.030    0.315    0.315\n#&gt; \n#&gt; Intercepts:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;     i_verbal         19.630    0.393   49.926    0.000    4.082    4.082\n#&gt;    .s_verbal         16.877    1.368   12.333    0.000    2.823    2.823\n#&gt;     i_processpeed    17.944    0.592   30.305    0.000    2.283    2.283\n#&gt;    .s_processpeed    30.167    2.843   10.612    0.000    4.347    4.347\n#&gt; \n#&gt; Variances:\n#&gt;                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n#&gt;    .Verbal_T6         8.445    1.407    6.000    0.000    8.445    0.267\n#&gt;    .Verbal_T7         9.715    1.287    7.547    0.000    9.715    0.243\n#&gt;    .Verbal_T9        10.375    1.493    6.951    0.000   10.375    0.188\n#&gt;    .Verbal_T11       21.001    3.922    5.355    0.000   21.001    0.207\n#&gt;    .Pspeed_T6         9.777    2.479    3.943    0.000    9.777    0.137\n#&gt;    .Pspeed_T7        21.117    2.640    8.000    0.000   21.117    0.236\n#&gt;    .Pspeed_T9        21.159    3.017    7.014    0.000   21.159    0.196\n#&gt;    .Pspeed_T11       23.241    5.106    4.552    0.000   23.241    0.165\n#&gt;     i_verbal         23.129    2.847    8.125    0.000    1.000    1.000\n#&gt;    .s_verbal         25.426    5.629    4.517    0.000    0.712    0.712\n#&gt;     i_processpeed    61.801    6.889    8.971    0.000    1.000    1.000\n#&gt;    .s_processpeed    47.685    8.194    5.820    0.000    0.990    0.990\n#&gt; \n#&gt; R-Square:\n#&gt;                    Estimate\n#&gt;     Verbal_T6         0.733\n#&gt;     Verbal_T7         0.757\n#&gt;     Verbal_T9         0.812\n#&gt;     Verbal_T11        0.793\n#&gt;     Pspeed_T6         0.863\n#&gt;     Pspeed_T7         0.764\n#&gt;     Pspeed_T9         0.804\n#&gt;     Pspeed_T11        0.835\n#&gt;     s_verbal          0.288\n#&gt;     s_processpeed     0.010",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>81</span>  <span class='chapter-title'>Lo Sviluppo dell'Intelligenza</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/12_temp_reliability.html",
    "href": "chapters/lgm/12_temp_reliability.html",
    "title": "82  Affidabilità longitudinale",
    "section": "",
    "text": "82.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nNel capitolo (interrater-reliability?), abbiamo discusso il calcolo dell’affidabilità delle misure in disegni longitudinali utilizzando la teoria della generalizzabilità. In questo capitolo, affronteremo lo stesso problema attraverso i modelli di equazioni strutturali (SEM).\nNegli ultimi anni, i progressi tecnologici hanno trasformato i metodi di raccolta dei dati longitudinali intensivi, consentendo la raccolta di informazioni in modo meno invasivo e riducendo le difficoltà per i partecipanti. Tradizionalmente, i dati longitudinali venivano raccolti con un numero limitato di misurazioni ripetute e intervalli di tempo lunghi tra una misurazione e l’altra. Oggi, invece, è possibile ottenere dati con un numero elevato di misurazioni ravvicinate nel tempo, grazie all’uso di strumenti come applicazioni per smartphone e tablet. Questi dati longitudinali intensivi permettono di esaminare la dinamica di processi psicologici che variano nel tempo, come i cambiamenti giornalieri negli stati psicologici.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>82</span>  <span class='chapter-title'>Affidabilità longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/12_temp_reliability.html#la-struttura-annidata-dei-dati-longitudinali",
    "href": "chapters/lgm/12_temp_reliability.html#la-struttura-annidata-dei-dati-longitudinali",
    "title": "82  Affidabilità longitudinale",
    "section": "82.2 La Struttura Annidata dei Dati Longitudinali",
    "text": "82.2 La Struttura Annidata dei Dati Longitudinali\nI dati raccolti con misure quotidiane presentano una struttura annidata, poiché le varie occasioni di misurazione sono raggruppate all’interno dello stesso individuo. Per analizzare l’affidabilità in questo contesto, si ricorre comunemente a due approcci: la teoria della generalizzabilità e l’approccio fattoriale.\n\nTeoria della Generalizzabilità: Questo approccio scompone la varianza totale in componenti di tempo, item e persona, permettendo di valutare l’affidabilità dei cambiamenti nel tempo a livello individuale. Tuttavia, la teoria della generalizzabilità si basa su alcune assunzioni che possono non adattarsi completamente ai dati raccolti.\nApproccio Fattoriale: Questo metodo è più flessibile e consente di modellare le associazioni tra gli item e il punteggio vero, oltre a gestire le varianze degli errori. Nelle sezioni precedenti abbiamo esaminato come l’analisi fattoriale confermativa multilivello (MCFA) possa stimare l’ICC delle singole variabili in contesti con dati annidati (cioè con misurazioni multiple per lo stesso partecipante). Ora, utilizzeremo la MCFA per determinare l’affidabilità sia a livello intra-individuale che inter-individuale nei casi di misure ripetute nel tempo.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>82</span>  <span class='chapter-title'>Affidabilità longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/12_temp_reliability.html#studio-di-caso-affidabilità-con-dati-longitudinali-intensivi",
    "href": "chapters/lgm/12_temp_reliability.html#studio-di-caso-affidabilità-con-dati-longitudinali-intensivi",
    "title": "82  Affidabilità longitudinale",
    "section": "82.3 Studio di Caso: Affidabilità con Dati Longitudinali Intensivi",
    "text": "82.3 Studio di Caso: Affidabilità con Dati Longitudinali Intensivi\nPer guidare la discussione, ci baseremo sull’articolo di Alphen et al. (2022), che offre un tutorial per valutare l’affidabilità dei dati longitudinali intensivi raccolti quotidianamente. In questo studio, gli autori utilizzano dati empirici relativi al livello di stress lavorativo giornaliero tra insegnanti di scuola secondaria, mostrando come calcolare l’affidabilità tramite MCFA e confrontandola con gli indici di affidabilità derivati dalla teoria della generalizzabilità.\nGrazie a questa comparazione, è possibile comprendere i vantaggi e le differenze tra i due approcci, evidenziando come l’analisi fattoriale multilivello consenta una rappresentazione più dettagliata delle variazioni intra-individuali e inter-individuali nei dati longitudinali.\n\n82.3.1 Affidabilità nei Modelli Fattoriali a Livello Singolo\nIn psicologia, la Confermatory Factor Analysis (CFA) è ormai lo standard per valutare la dimensionalità e l’affidabilità dei punteggi. Quando si lavora con dati a livello singolo, l’affidabilità può essere misurata attraverso diversi indici. Tra questi, l’indice \\(\\omega\\) offre un’alternativa al tradizionale coefficiente di consistenza interna \\(\\alpha\\), poiché non richiede che i carichi fattoriali degli item contribuiscano in egual misura al costrutto latente.\nI valori di \\(\\omega\\) spaziano tra zero e uno, con valori prossimi a uno che indicano una maggiore affidabilità della scala. Questo indice rappresenta la proporzione di varianza nei punteggi della scala spiegata dal fattore latente comune a tutti gli indicatori.\n\n\n82.3.2 Definizione dell’Affidabilità Composita \\(\\omega\\)\nL’affidabilità composita \\(\\omega\\) per un costrutto misurato con \\(p\\) item è calcolata come segue:\n\\[\n\\omega = \\frac{\\sum_{i=1}^{p} \\lambda_i^2 \\Phi}{\\sum_{i=1}^{p} \\lambda_i^2 \\Phi + \\sum_{i=1}^{p} \\theta_i},\n\\]\ndove:\n\n\\(i\\) indica ciascun item,\n\\(\\lambda\\) rappresenta il carico fattoriale dell’item sul costrutto latente,\n\\(\\Phi\\) è la varianza del fattore latente,\n\\(\\theta\\) è la varianza residua dell’item.\n\nQuesto indice considera i diversi contributi degli item al costrutto latente, fornendo una stima dell’affidabilità che riflette meglio la struttura fattoriale del punteggio rispetto agli approcci che assumono contributi uniformi.\nPer chiarire, Alphen et al. (2022) propongono un esempio concreto. Supponiamo di avere un modello a singolo fattore, in cui la varianza del fattore è fissata a 1 per l’identificazione del modello, e i carichi fattoriali sui tre indicatori sono pari a 0.7, 0.8 e 0.9. Di conseguenza, le specificità degli item saranno rispettivamente 0.51, 0.36 e 0.19.\nInserendo questi valori nell’equazione per \\(\\omega\\), otteniamo un’affidabilità composita della scala pari a 0.84:\n\\[\n\\omega = \\frac{\\left(0.70 + 0.80 + 0.90\\right)^{2} \\cdot 1}{\\left(0.70 + 0.80 + 0.90\\right)^{2} \\cdot 1 + \\left(0.51 + 0.36 + 0.19\\right)} = 0.84.\n\\]\nQuesto risultato indica che l’84% della varianza totale nei punteggi della scala è attribuibile al fattore comune, il che riflette un’elevata affidabilità della misura.\n\n\n82.3.3 Affidabilità nei Modelli Fattoriali Multilivello\nIn psicologia, i dati spesso presentano una struttura annidata, in cui le unità a un livello inferiore sono raggruppate in unità di livello superiore. Ad esempio, gli studenti sono annidati nelle classi, e i pazienti negli ospedali. Con misurazioni ripetute sugli stessi individui, come nei dati raccolti giornalmente, le occasioni di misurazione sono annidate negli individui. Nel caso illustrato qui, i dati empirici provengono da misurazioni ripetute su insegnanti durante 15 occasioni di raccolta, creando una struttura annidata in cui le occasioni sono raggruppate per ogni insegnante.\nL’analisi fattoriale multilivello consente di rappresentare varianze e covarianze distinte per le differenze intra-individuali e inter-individuali (Muthén, 1994). Nell’esempio discusso, Alphen et al. (2022) si focalizzano su strutture a due livelli: le occasioni di misurazione (Livello 1, o livello intra-individuale) e gli individui (Livello 2, o livello inter-individuale).\nLa Figura 82.1 illustra un modello fattoriale multilivello. In una confermatory factor analysis (CFA) a due livelli, i punteggi degli item vengono suddivisi in componenti latenti intra- e inter-individuali. La componente inter-individuale modella la struttura di covarianza tra individui, spiegando le differenze tra di essi e fornendo un’interpretazione simile a quella di una CFA a livello singolo. La componente intra-individuale modella la covarianza tra le misurazioni ripetute per ciascun individuo, riflettendo le variazioni all’interno degli individui nei diversi momenti temporali. In questo contesto, il livello intra-individuale rappresenta caratteristiche di stato (condizioni fluttuanti nel tempo), mentre il livello inter-individuale riflette caratteristiche di tratto, offrendo una misura aggregata più stabile nel tempo, simile a un’indicazione della personalità.\n\n\n\n\n\n\nFigura 82.1: Un modello configurale multilivello con i carichi fattoriali, varianze residuali e varianza del fattoriali dell’esempio discusso da Alphen et al. (2022). (Figura tratta da Alphen et al., 2022)\n\n\n\nGeldhof et al. (2014) hanno ampliato il metodo per calcolare \\(\\omega\\) adattandolo ai modelli a due livelli, ottenendo così indici di affidabilità distinti per il livello intra-individuale (\\(\\omega_w\\)) e inter-individuale (\\(\\omega_b\\)). Questo approccio è stato successivamente sviluppato ulteriormente da Lai (2021).",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>82</span>  <span class='chapter-title'>Affidabilità longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/12_temp_reliability.html#calcolo-dellaffidabilità-della-scala-con-dati-giornalieri",
    "href": "chapters/lgm/12_temp_reliability.html#calcolo-dellaffidabilità-della-scala-con-dati-giornalieri",
    "title": "82  Affidabilità longitudinale",
    "section": "82.4 Calcolo dell’Affidabilità della Scala con Dati Giornalieri",
    "text": "82.4 Calcolo dell’Affidabilità della Scala con Dati Giornalieri\nAlphen et al. (2022) illustrano l’analisi dell’affidabilità utilizzando un dataset con misure longitudinali intensive giornaliere sullo stress degli insegnanti. In questo contesto, il fattore comune a livello esterno può essere interpretato come la componente stabile dello stress, mentre il fattore a livello interno rappresenta la variabilità dello stress nel tempo. Quando si modellano componenti interne ed esterne dello stesso fattore, il modello fattoriale multilivello prende il nome di modello configurale (Stapleton et al., 2016).\nNel modello configurale multilivello, i fattori a livello interno ed esterno riflettono componenti diverse della stessa variabile latente, con una struttura fattoriale identica per entrambi i livelli e carichi fattoriali uguali (Asparouhov & Muthen, 2012). Lai (2021) ha fornito le formule per calcolare gli indici di affidabilità a livello intra-individuale (\\(\\omega_w\\)) e inter-individuale (\\(\\omega_b\\)) in questi modelli configurali. Alphen et al. (2022) illustrano come calcolare questi indici di affidabilità.\n\n82.4.1 Affidabilità a Livello Intra-Individuale\nPer determinare l’affidabilità a livello intra-individuale, utilizziamo la seguente formula:\n\\[\n\\omega_w = \\frac{\\sum (\\lambda_i^2 \\Phi_w)}{\\sum (\\lambda_i^2 \\Phi_w) + \\sum (\\theta_w)},\n\\]\ndove il pedice \\(w\\) si riferisce al livello intra-individuale. I carichi fattoriali (\\(\\lambda\\)) non hanno un pedice di livello specifico poiché sono vincolati ad essere identici a entrambi i livelli. In questo contesto, \\(\\Phi_w\\) rappresenta la varianza del fattore a livello intra-individuale e \\(\\theta_w\\) rappresenta la varianza residua al livello interno.\nInserendo i valori di esempio dalla Figura 82.1, otteniamo un’affidabilità intra-individuale di 0.84:\n\\[\n\\omega_w = \\frac{(0.70 + 0.80 + 0.90)^2}{(0.70 + 0.80 + 0.90)^2 + (0.51 + 0.36 + 0.19)} = 0.84.\n\\]\nQuesto valore indica che il fattore comune a livello interno spiega l’84% della varianza nelle deviazioni a livello intra-individuale nei punteggi della scala.\n\n\n82.4.2 Affidabilità a Livello Inter-Individuale\nPer il calcolo dell’affidabilità a livello inter-individuale, l’equazione è la seguente:\n\\[\n\\omega_b = \\frac{\\sum (\\lambda_i^2 \\Phi_b)}{\\sum (\\lambda_i^2 (\\Phi_b + \\Phi_w/n)) + \\sum (\\theta_b + \\theta_w/n)},\n\\]\ndove \\(n\\) è il numero di occasioni di misurazione (in questo caso, 15). La presenza di \\(\\Phi_w/n\\) e \\(\\theta_w/n\\) tiene conto della varianza dell’errore di campionamento delle medie osservate a livello di persona.\nInserendo i valori di esempio di Alphen et al. (2022) e impostando \\(n = 15\\), otteniamo un’affidabilità inter-individuale di 0.90:\n\\[\n\\omega_b = \\frac{(0.70 + 0.80 + 0.90)^2}{\n    (0.70 + 0.80 + 0.90)^2(0.90 + \\frac{1}{15}) + (0.05 + 0.05 + 0.05) \\\\\n    + \\frac{(0.51 + 0.36 + 0.19)}{15}\n} = 0.90\n\\]\nQuesto valore indica che il fattore comune a livello esterno spiega il 90% della varianza nelle medie osservate a livello di persona.\n\n\n82.4.3 Interpretazione degli Indici di Affidabilità\nQueste formule permettono di calcolare l’affidabilità delle componenti intra- e inter-individuali di una scala in studi longitudinali intensivi. I risultati consentono ai ricercatori di distinguere tra variazioni stabili (livello inter-individuale) e temporanee (livello intra-individuale), offrendo una visione dettagliata della dinamica dei fenomeni psicologici misurati.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>82</span>  <span class='chapter-title'>Affidabilità longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/12_temp_reliability.html#confronto-con-la-teoria-della-generalizzabilità",
    "href": "chapters/lgm/12_temp_reliability.html#confronto-con-la-teoria-della-generalizzabilità",
    "title": "82  Affidabilità longitudinale",
    "section": "82.5 Confronto con la Teoria della Generalizzabilità",
    "text": "82.5 Confronto con la Teoria della Generalizzabilità\nAlphen et al. (2022) hanno anche derivato le componenti di varianza per il calcolo del punteggio di affidabilità a livello interno e a livello esterno utilizzando la teoria della generalizzabilità. Per questi dati, Alphen et al. (2022) trovano che la stima dell’affidabilità a livello interno è .87, molto simile alla stima ottenuta con l’approccio CFA multilivello. Tuttavia, la stima a livello esterno ottenuta con il metodo della generalizzabilità è 0.99, che è .11 più alta rispetto all’approccio analitico fattoriale. Questa differenza potrebbe essere causata dalle assunzioni più rigide fatte dal metodo della teoria della generalizzabilità. Tuttavia, Alphen et al. (2022) notano che questi risultati sono specifici al dataset utilizzato e sarebbe necessario uno studio di simulazione per valutare, in generale, quali sono le differenze sistematiche tra le stime di affidabilità ottenute con i due diversi metodi.\nQui sotto viene presentato il metodo SEM per il calcolo dell’affidabilità inter- e intra-persona usando gli script R forniti da Alphen et al. (2022).",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>82</span>  <span class='chapter-title'>Affidabilità longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/12_temp_reliability.html#metodo-a-cinque-passi",
    "href": "chapters/lgm/12_temp_reliability.html#metodo-a-cinque-passi",
    "title": "82  Affidabilità longitudinale",
    "section": "82.6 Metodo a Cinque Passi",
    "text": "82.6 Metodo a Cinque Passi\nPer studiare l’affidabilità delle misure longitudinali intensive, Alphen et al. (2022) propongono una procedura in cinque passi. Questi passaggi sono stati ideati per evitare bias e problemi di specificazione del modello e comprendono:\n\nIspezione delle Correlazioni Intraclasse: Questo primo passo verifica la proporzione di varianza attribuibile alle differenze tra i gruppi, utile per comprendere la struttura annidata dei dati.\nVerifica della Varianza e Covarianza al Livello tra-Persone: Si testa la presenza di varianza e covarianza significative tra le persone per valutare la necessità di un approccio multilivello, dove le differenze tra persone giocano un ruolo importante.\nSpecifica Progressiva del Modello di Misura a ciascun Livello: Il modello di misura viene definito gradualmente per ciascun livello, assicurando che la struttura del modello rappresenti adeguatamente le relazioni tra variabili.\nVerifica della Invarianza di Misura tra Livelli: Si testa l’invarianza della misura per assicurarsi che la struttura del modello sia simile nei vari livelli, il che è fondamentale per confrontare interpretazioni tra livelli.\nCalcolo degli Indici di Affidabilità a Livello Intra-Personale (ωw) e Inter-Personale (ωb): Infine, si calcolano gli indici di affidabilità a livello intra-personale (ωw) e inter-personale (ωb) per quantificare la stabilità delle misure rispettivamente entro e tra persone.\n\nQuesta procedura strutturata permette di valutare l’affidabilità delle misure in studi longitudinali intensivi, garantendo che il modello rispetti le caratteristiche dei dati e fornisca stime affidabili di variabilità e stabilità a livello intra- e inter-personale.\nIniziamo ad importare i dati dell’esempio di Alphen et al. (2022).\n\nvan_alphen &lt;- read.table(\"../../data/van_alphen.dat\", na.strings = \"9999\")\ncolnames(van_alphen) &lt;- c(\"day\", \"school\", \"ID\", \"str1\", \"str2\", \"str3\", \"str4\")\nvan_alphen |&gt; head()\n\n\nA data.frame: 6 x 7\n\n\n\nday\nschool\nID\nstr1\nstr2\nstr3\nstr4\n\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n1\n1\n1\n26\nNA\n24\n78\n\n\n2\n1\n1\n30\nNA\n24\n24\n50\n\n\n3\n1\n1\n55\nNA\n36\n70\n72\n\n\n4\n1\n1\n92\nNA\n37\n34\n41\n\n\n5\n1\n2\n20\n24\nNA\n24\n36\n\n\n6\n1\n2\n22\n12\nNA\n18\n39\n\n\n\n\n\n\nvan_alphen |&gt; tail()\n\n\nA data.frame: 6 x 7\n\n\n\nday\nschool\nID\nstr1\nstr2\nstr3\nstr4\n\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1264\n15\n6\n62\nNA\n66\n57\n61\n\n\n1265\n15\n6\n87\nNA\n42\n59\n83\n\n\n1266\n15\n6\n115\n53\nNA\n53\n45\n\n\n1267\n15\n6\n118\nNA\n16\n32\n16\n\n\n1268\n15\n6\n123\n23\n22\n23\nNA\n\n\n1269\n15\n6\n143\n64\n64\n65\nNA\n\n\n\n\n\nEseguiamo una procedura di imputazione multipla per gestire il problema dei dati mancanti. In Mplus, questo passaggio può essere eseguito direttamente durante la procedura di fit, ma in R non è possibile. Pertanto, utilizziamo missRanger per imputare i dati prima di adattare il modello.\n\nimp &lt;- missRanger(van_alphen, num.trees = 100)\n\n\nVariables to impute:        str3, str4, str1, str2\nVariables used to impute:   day, school, ID, str1, str2, str3, str4\n\niter 1 \n  |============================================================| 100%\niter 2 \n  |============================================================| 100%\niter 3 \n  |============================================================| 100%\niter 4 \n  |============================================================| 100%\niter 5 \n  |============================================================| 100%\niter 6 \n  |============================================================| 100%\n\n\n\nimp |&gt; summary()\n\n     day                school            ID              str1       \n Length:1269        Min.   :1.000   Min.   :  1.00   Min.   :  0.00  \n Class :character   1st Qu.:2.000   1st Qu.: 43.00   1st Qu.:  7.00  \n Mode  :character   Median :4.000   Median : 81.00   Median : 28.09  \n                    Mean   :3.779   Mean   : 78.53   Mean   : 33.63  \n                    3rd Qu.:5.000   3rd Qu.:115.00   3rd Qu.: 57.05  \n                    Max.   :6.000   Max.   :151.00   Max.   :100.00  \n      str2               str3             str4       \n Min.   :  0.0000   Min.   :  0.00   Min.   :  0.00  \n 1st Qu.:  0.7818   1st Qu.:  7.00   1st Qu.:  7.00  \n Median : 21.0000   Median : 30.00   Median : 34.00  \n Mean   : 26.6310   Mean   : 34.71   Mean   : 34.99  \n 3rd Qu.: 45.0000   3rd Qu.: 61.00   3rd Qu.: 59.00  \n Max.   :100.0000   Max.   :100.00   Max.   :100.00  \n\n\n\n82.6.1 Passo 1: Ispezione delle Correlazioni Intraclasse\nIl modello multilivello è appropriato se una quota rilevante della varianza può essere attribuita al livello tra-persone. Il coefficiente di correlazione intraclasse (ICC) di una variabile permette di quantificare l’entità di questa proporzione (Snijders & Bosker, 2012). Pertanto, il primo passo consiste nel verificare se è presente una varianza significativa al livello tra-persone attraverso l’ispezione dell’ICC, calcolato come:\n\\[\n\\text{ICC} = \\frac{\\sigma_b}{\\sigma_b + \\sigma_w},\n\\]\ndove \\(\\sigma_b\\) e \\(\\sigma_w\\) rappresentano, rispettivamente, la varianza dell’indicatore al livello tra-persone e al livello entro-persona, ottenute adattando modelli saturi a entrambi i livelli.\n\nmodel1 &lt;- \"\n        level: 1\n            str1 ~~ str2 + str3 + str4\n            str2 ~~ str3 + str4\n            str3 ~~ str4\n\n        level: 2\n            str1 ~~ str2 + str3 + str4\n            str2 ~~ str3 + str4\n            str3 ~~ str4\n\n          # means\n            str1 + str2 + str3 + str4  ~ 1\n    \"\n\n\nfit.step1 &lt;- lavaan(\n    model = model1, data = imp, cluster = \"ID\",\n    auto.var = TRUE\n)\n\nsummary(fit.step1) |&gt; print()\n\nlavaan 0.6-19 ended normally after 363 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        24\n\n  Number of observations                          1269\n  Number of clusters [ID]                          151\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\n\nLevel 1 [within]:\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  str1 ~~                                             \n    str2            342.261   16.268   21.039    0.000\n    str3            406.537   19.297   21.067    0.000\n    str4            359.195   17.888   20.081    0.000\n  str2 ~~                                             \n    str3            305.064   16.314   18.699    0.000\n    str4            307.366   15.847   19.396    0.000\n  str3 ~~                                             \n    str4            347.263   18.496   18.775    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    str1            471.690   19.872   23.737    0.000\n    str2            383.080   16.154   23.714    0.000\n    str3            538.863   22.705   23.733    0.000\n    str4            489.494   20.666   23.686    0.000\n\n\nLevel 2 [ID]:\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  str1 ~~                                             \n    str2            275.295   38.963    7.066    0.000\n    str3            278.127   40.489    6.869    0.000\n    str4            292.286   41.776    6.996    0.000\n  str2 ~~                                             \n    str3            251.097   37.105    6.767    0.000\n    str4            271.965   39.062    6.962    0.000\n  str3 ~~                                             \n    str4            282.024   40.920    6.892    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    str1             34.296    1.579   21.727    0.000\n    str2             27.161    1.492   18.207    0.000\n    str3             35.551    1.561   22.779    0.000\n    str4             35.744    1.611   22.186    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    str1            298.145   42.327    7.044    0.000\n    str2            270.910   37.919    7.144    0.000\n    str3            281.366   41.412    6.794    0.000\n    str4            310.574   44.373    6.999    0.000\n\n\n\n\n# see ICC\nlavInspect(fit.step1, \"icc\") |&gt; print()\n\n str1  str2  str3  str4 \n0.387 0.414 0.343 0.388 \n\n\n\n\n82.6.2 Passo 2: Verifica della Varianza e Covarianza al Livello tra-Persone\nIn questo secondo passaggio, verifichiamo se: a) la varianza al livello tra-persone è significativa e b) se esistono covarianze significative a questo livello (Hox, Moerbeek, & van der Schoot, 2017). Il test delle covarianze ci permette di valutare se esistono effettivamente relazioni che potrebbero essere modellate con un fattore comune al livello tra-persone.\nPer prima cosa, per testare la significatività della varianza al livello tra-persone (passo 2a), adattiamo ai dati un modello nullo per questo livello. Un modello nullo è un modello in cui tutte le varianze (e le covarianze) sono fissate a zero. Al livello entro-persona specifichiamo invece un modello saturo, in cui tutti gli item sono correlati, assicurando così una perfetta aderenza ai dati. In questo modo, ogni eventuale discrepanza nel fit del modello deriva esclusivamente dal livello tra-persone, dove le varianze sono fissate a zero.\nQuando si testa il fit del modello, un test χ² significativo indica che il modello differisce in modo significativo rispetto a un modello che si adatterebbe perfettamente ai dati (Kline, 2011). Pertanto, se il test χ² respinge il modello nullo, possiamo concludere che è presente una varianza significativa al livello tra-persone.\nSuccessivamente, per verificare se esiste una covarianza significativa al livello tra-persone (passo 2b), rilasciamo il vincolo sulle varianze in modo da stimarle liberamente, mantenendo però le covarianze fissate a zero. La specificazione del modello saturo al livello entro-persona rimane invariata. Anche in questo caso, un test χ² significativo di fit del modello indica che questo modello differisce in modo significativo rispetto a un modello che si adatterebbe perfettamente ai dati. Poiché non abbiamo modellato alcuna relazione tra gli item a livello tra-persone, un test χ² significativo indica che le covarianze dovrebbero essere prese in considerazione. In altre parole, un test χ² significativo suggerisce la presenza di covarianze significative, che potrebbero essere spiegate con un modello fattoriale nei passaggi successivi.\n\nmodel2a &lt;- \"\n        level: 1\n            str1 ~~ str2 + str3 + str4\n            str2 ~~ str3 + str4\n            str3 ~~ str4\n\n        level: 2\n            str1 ~~ 0*str2 + 0*str3 + 0*str4\n            str2 ~~ 0*str3 + 0*str4\n            str3 ~~ 0*str4\n\n            str1 ~~ 0*str1\n            str2 ~~ 0*str2\n            str3 ~~ 0*str3\n            str4 ~~ 0*str4\n\n          # means\n            str1 + str2 + str3 + str4  ~ 1\n    \"\n\n\nfit.step2a &lt;- lavaan(\n    model = model2a, data = imp, cluster = \"ID\",\n    auto.var = TRUE\n)\n\nsummary(fit.step2a) |&gt; print()\n\nlavaan 0.6-19 ended normally after 118 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        14\n\n  Number of observations                          1269\n  Number of clusters [ID]                          151\n\nModel Test User Model:\n                                                      \n  Test statistic                               565.401\n  Degrees of freedom                                10\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\n\nLevel 1 [within]:\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  str1 ~~                                             \n    str2            623.313   26.624   23.412    0.000\n    str3            685.932   29.343   23.376    0.000\n    str4            659.716   29.075   22.690    0.000\n  str2 ~~                                             \n    str3            558.874   25.832   21.635    0.000\n    str4            588.081   26.523   22.172    0.000\n  str3 ~~                                             \n    str4            633.186   28.998   21.835    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    str1            774.610   30.685   25.244    0.000\n    str2            660.240   26.257   25.145    0.000\n    str3            817.829   32.239   25.368    0.000\n    str4            809.702   32.482   24.928    0.000\n\n\nLevel 2 [ID]:\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  str1 ~~                                             \n    str2              0.000                           \n    str3              0.000                           \n    str4              0.000                           \n  str2 ~~                                             \n    str3              0.000                           \n    str4              0.000                           \n  str3 ~~                                             \n    str4              0.000                           \n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    str1             33.560    0.781   42.954    0.000\n    str2             26.659    0.721   36.960    0.000\n    str3             34.732    0.803   43.264    0.000\n    str4             34.988    0.799   43.802    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    str1              0.000                           \n    str2              0.000                           \n    str3              0.000                           \n    str4              0.000                           \n\n\n\n\nmodel2b &lt;- \"\n        level: 1\n            str1 ~~ str2 + str3 + str4\n            str2 ~~ str3 + str4\n            str3 ~~ str4\n\n        level: 2\n            str1 ~~ 0*str2 + 0*str3 + 0*str4\n            str2 ~~ 0*str3 + 0*str4\n            str3 ~~ 0*str4\n\n          # means\n            str1 + str2 + str3 + str4  ~ 1\n    \"\n\n\nfit.step2b &lt;- lavaan(\n    model = model2b, data = imp, cluster = \"ID\",\n    auto.var = TRUE\n)\n\nsummary(fit.step2b) |&gt; print()\n\nlavaan 0.6-19 ended normally after 240 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        18\n\n  Number of observations                          1269\n  Number of clusters [ID]                          151\n\nModel Test User Model:\n                                                      \n  Test statistic                               425.971\n  Degrees of freedom                                 6\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\n\nLevel 1 [within]:\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  str1 ~~                                             \n    str2            583.494   26.975   21.631    0.000\n    str3            671.583   30.182   22.251    0.000\n    str4            623.698   29.159   21.390    0.000\n  str2 ~~                                             \n    str3            532.195   25.884   20.561    0.000\n    str4            534.900   26.062   20.524    0.000\n  str3 ~~                                             \n    str4            598.950   29.051   20.617    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    str1            750.987   32.247   23.288    0.000\n    str2            589.951   26.541   22.228    0.000\n    str3            790.466   33.639   23.498    0.000\n    str4            738.518   32.940   22.420    0.000\n\n\nLevel 2 [ID]:\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  str1 ~~                                             \n    str2              0.000                           \n    str3              0.000                           \n    str4              0.000                           \n  str2 ~~                                             \n    str3              0.000                           \n    str4              0.000                           \n  str3 ~~                                             \n    str4              0.000                           \n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    str1             33.600    0.803   41.857    0.000\n    str2             26.507    0.803   32.993    0.000\n    str3             34.830    0.874   39.851    0.000\n    str4             35.056    0.878   39.911    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    str1              6.520    3.333    1.956    0.050\n    str2             23.657    5.371    4.405    0.000\n    str3             17.924    5.397    3.321    0.001\n    str4             24.375    6.655    3.663    0.000\n\n\n\n\nanova(fit.step2a, fit.step2b)\n\n\nA anova: 2 x 8\n\n\n\nDf\nAIC\nBIC\nChisq\nChisq diff\nRMSEA\nDf diff\nPr(&gt;Chisq)\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nfit.step2b\n6\n42786.09\n42878.72\n425.9705\nNA\nNA\nNA\nNA\n\n\nfit.step2a\n10\n42917.52\n42989.56\n565.4010\n139.4305\n0.1633418\n4\n3.737341e-29\n\n\n\n\n\n\n\n82.6.3 Passo 3: Definizione di un Modello di Misura al Livello Entro-Persona\nIn questo terzo passo, esaminiamo se gli item possono essere rappresentati da un unico fattore al livello entro-persona. Per farlo, definiamo un modello di misura per il livello entro-persona, mantenendo invece un modello saturo a livello tra-persone. La bontà di adattamento di questo modello, e dei modelli successivi, può essere valutata utilizzando il test χ². Se il test χ² risulta significativo, dobbiamo rifiutare l’adattamento perfetto del modello. Con campioni di grandi dimensioni, anche piccole discrepanze nel modello possono portare a rifiutarlo (Marsh, Balla, & McDonald, 1988).\nPer questo motivo, oltre al test χ², consideriamo anche indici di adattamento approssimato: un RMSEA inferiore a 0.05 e un CFI superiore a 0.95 indicano un buon adattamento (Browne & Cudeck, 1992), mentre un RMSEA inferiore a 0.08 e un CFI superiore a 0.90 indicano un adattamento accettabile (Hu & Bentler, 1999).\nSe il modello non si adatta adeguatamente ai dati, possono essere intrapresi passi aggiuntivi per affrontare le cause di tale discrepanza prima di procedere. In questi casi, l’ispezione degli indici di modifica o dei residui di correlazione può fornire informazioni preziose su eventuali discrepanze locali del modello. È importante che le modifiche al modello siano sempre guidate da considerazioni teoriche, poiché seguire esclusivamente i risultati statistici può portare a modelli che non si generalizzano ad altri campioni (MacCallum, 1986).\nSolo quando il modello si adatta adeguatamente ai dati e ha senso teorico, è opportuno passare al passaggio successivo.\n\nmodel3 &lt;- \"\n        level: 1\n            stress =~ str1 + str2 + str3 + str4\n            stress ~~ 1*stress\n\n        level: 2\n            str1 ~~ str2 + str3 + str4\n            str2 ~~ str3 + str4\n            str3 ~~ str4\n\n          # means\n            str1 + str2 + str3 + str4  ~ 1\n    \"\n\n\nfit.step3 &lt;- lavaan(\n    model = model3, data = imp, cluster = \"ID\",\n    auto.var = TRUE\n)\n\n\nfitMeasures(fit.step3) |&gt; print()\n\n                 npar                  fmin                 chisq \n               22.000                13.036                50.423 \n                   df                pvalue        baseline.chisq \n                2.000                 0.000              4286.653 \n          baseline.df       baseline.pvalue                   cfi \n               12.000                 0.000                 0.989 \n                  tli                  nnfi                   rfi \n                0.932                 0.932                 0.929 \n                  nfi                  pnfi                   ifi \n                0.988                 0.165                 0.989 \n                  rni                  logl     unrestricted.logl \n                0.989            -21207.330            -21182.119 \n                  aic                   bic                ntotal \n            42458.661             42571.872              1269.000 \n                 bic2                 rmsea        rmsea.ci.lower \n            42501.990                 0.138                 0.107 \n       rmsea.ci.upper        rmsea.ci.level          rmsea.pvalue \n                0.172                 0.900                 0.000 \n       rmsea.close.h0 rmsea.notclose.pvalue     rmsea.notclose.h0 \n                0.050                 0.999                 0.080 \n                 srmr           srmr_within          srmr_between \n                0.023                 0.019                 0.003 \n\n\n\n\n82.6.4 Passo 4: Adattamento di un Modello a Due Livelli con Vincoli tra Livelli\nNel modello configurale, desideriamo che il costrutto abbia un significato comparabile a entrambi i livelli. Ad esempio, vorremmo che la “nervosità” rappresenti il sentimento generale degli individui come indicatore di stress al livello tra-persone e che, al livello entro-persona, esprima le variazioni quotidiane di quella stessa emozione per indicare le fluttuazioni giornaliere dello stress. Per permettere questa interpretazione, è necessario vincolare i carichi fattoriali affinché siano uguali tra il livello entro-persona e tra-persona.\nIn questo modello, la varianza del fattore al livello tra-persone deve essere stimata liberamente, poiché il vincolo sui carichi fattoriali già identifica la scala del fattore a livello tra-persona quando la varianza del fattore al livello entro-persona è fissata (Jak et al., 2014).\n\nmodel4 &lt;- \"\n        level: 1\n            stress =~ L1*str1 + L2*str2 + L3*str3 + L4*str4\n            stress ~~ 1*stress\n\n        level: 2\n            stress =~ L1*str1 + L2*str2 + L3*str3 + L4*str4\n            stress ~~ fb*stress\n\n          # means\n            str1 + str2 + str3 + str4  ~ 1\n    \"\n\n\nfit.step4 &lt;- lavaan(\n    model = model4, data = imp, cluster = \"ID\",\n    auto.var = TRUE\n)\n\n\nfitMeasures(fit.step4) |&gt; print()\n\n                 npar                  fmin                 chisq \n               17.000                13.049                83.701 \n                   df                pvalue        baseline.chisq \n                7.000                 0.000              4286.653 \n          baseline.df       baseline.pvalue                   cfi \n               12.000                 0.000                 0.982 \n                  tli                  nnfi                   rfi \n                0.969                 0.969                 0.967 \n                  nfi                  pnfi                   ifi \n                0.980                 0.572                 0.982 \n                  rni                  logl     unrestricted.logl \n                0.982            -21223.969            -21182.119 \n                  aic                   bic                ntotal \n            42481.938             42569.420              1269.000 \n                 bic2                 rmsea        rmsea.ci.lower \n            42515.419                 0.093                 0.076 \n       rmsea.ci.upper        rmsea.ci.level          rmsea.pvalue \n                0.111                 0.900                 0.000 \n       rmsea.close.h0 rmsea.notclose.pvalue     rmsea.notclose.h0 \n                0.050                 0.894                 0.080 \n                 srmr           srmr_within          srmr_between \n                0.032                 0.018                 0.014 \n\n\n\n\n82.6.5 Passo 5: Calcolo degli Indici di Affidabilità\nSe il modello ottenuto al Passo 4 presenta un buon adattamento, l’ultimo passo consiste nel calcolare gli indici di affidabilità. Utilizziamo le stime dei parametri ottenute per calcolare ωb (affidabilità a livello tra-persone) e ωw (affidabilità a livello entro-persona) seguendo le formule presentate.\nQuesti indici quantificano la stabilità e la coerenza delle misure rispettivamente tra e entro persone, offrendo una valutazione completa dell’affidabilità delle misure a ciascun livello del modello.\n\nmodel5 &lt;- \"\n        level: 1\n            stress =~ L1*str1 + L2*str2 + L3*str3 + L4*str4\n            stress ~~ 1*stress\n\n            str1 ~~ tw1*str1\n            str2 ~~ tw2*str2\n            str3 ~~ tw3*str3\n            str4 ~~ tw4*str4\n\n        level: 2\n            stress =~ L1*str1 + L2*str2 + L3*str3 + L4*str4\n            stress ~~ fb*stress\n\n            str1 ~~ tb1*str1\n            str2 ~~ tb2*str2\n            str3 ~~ tb3*str3\n            str4 ~~ tb4*str4\n\n          # means\n            str1 + str2 + str3 + str4  ~ 1\n\n          # reliability calculations\n          lambda := L1 + L2 + L3 + L4\n          thetaw := tw1 + tw2 + tw3 + tw4\n          thetab := tb1 + tb2 + tb3 + tb4\n          omega_w := lambda^2 / (lambda^2 + thetaw)\n          omega_b := (lambda^2 * fb) / (lambda^2 * (1/15 + fb) + fb + thetaw/15)\n    \"\n\n\nfit.step5 &lt;- lavaan(\n    model = model5, data = imp, cluster = \"ID\",\n    auto.var = TRUE\n)\n\n\nsummary(fit.step5) |&gt; print()\n\nlavaan 0.6-19 ended normally after 93 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        21\n  Number of equality constraints                     4\n\n  Number of observations                          1269\n  Number of clusters [ID]                          151\n\nModel Test User Model:\n                                                      \n  Test statistic                                83.701\n  Degrees of freedom                                 7\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\n\nLevel 1 [within]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  stress =~                                           \n    str1      (L1)   20.206    0.482   41.904    0.000\n    str2      (L2)   17.076    0.458   37.289    0.000\n    str3      (L3)   19.223    0.526   36.513    0.000\n    str4      (L4)   18.338    0.520   35.246    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    stress            1.000                           \n   .str1     (tw1)   53.776    5.062   10.623    0.000\n   .str2     (tw2)  110.015    5.823   18.895    0.000\n   .str3     (tw3)  160.900    8.107   19.847    0.000\n   .str4     (tw4)  172.111    8.652   19.894    0.000\n\n\nLevel 2 [ID]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  stress =~                                           \n    str1      (L1)   20.206    0.482   41.904    0.000\n    str2      (L2)   17.076    0.458   37.289    0.000\n    str3      (L3)   19.223    0.526   36.513    0.000\n    str4      (L4)   18.338    0.520   35.246    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .str1             34.417    1.589   21.659    0.000\n   .str2             27.133    1.415   19.179    0.000\n   .str3             35.533    1.591   22.335    0.000\n   .str4             35.740    1.537   23.251    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    stress    (fb)    0.745    0.112    6.644    0.000\n   .str1     (tb1)   -0.800    2.973   -0.269    0.788\n   .str2     (tb2)   18.555    4.380    4.236    0.000\n   .str3     (tb3)   20.302    5.575    3.641    0.000\n   .str4     (tb4)   23.640    6.723    3.516    0.000\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    lambda           74.843    1.708   43.824    0.000\n    thetaw          496.802   12.485   39.790    0.000\n    thetab           61.697    9.488    6.502    0.000\n    omega_w           0.919    0.004  232.932    0.000\n    omega_b           0.911    0.012   75.486    0.000\n\n\n\nI risultati sono simili a quelli riportati da Alphen et al. (2022), sebbene nel loro caso sia stata utilizzata una diversa procedura di imputazione e il modello sia stato adattato con Mplus.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>82</span>  <span class='chapter-title'>Affidabilità longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/12_temp_reliability.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/lgm/12_temp_reliability.html#informazioni-sullambiente-di-sviluppo",
    "title": "82  Affidabilità longitudinale",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lme4_1.1-35.5     Matrix_1.7-0      ggokabeito_0.1.0  viridis_0.6.5    \n [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n [9] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n[13] lavaan_0.6-18     psych_2.4.6.26    scales_1.3.0      markdown_1.13    \n[17] knitr_1.48        lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1    \n[21] dplyr_1.1.4       purrr_1.0.2       readr_2.1.5       tidyr_1.3.1      \n[25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.16.0  jsonlite_1.8.9     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5.1 farver_2.1.2      \n  [7] nloptr_2.1.1       rmarkdown_2.28     vctrs_0.6.5       \n [10] minqa_1.2.8        base64enc_0.1-3    rstatix_0.7.2     \n [13] htmltools_0.5.8.1  broom_1.0.6        Formula_1.2-5     \n [16] htmlwidgets_1.6.4  plyr_1.8.9         sandwich_3.1-1    \n [19] emmeans_1.10.4     zoo_1.8-12         uuid_1.2-1        \n [22] igraph_2.0.3       mime_0.12          lifecycle_1.0.4   \n [25] pkgconfig_2.0.3    R6_2.5.1           fastmap_1.2.0     \n [28] shiny_1.9.1        digest_0.6.37      OpenMx_2.21.12    \n [31] fdrtool_1.2.18     colorspace_2.1-1   rprojroot_2.0.4   \n [34] Hmisc_5.1-3        fansi_1.0.6        timechange_0.3.0  \n [37] abind_1.4-8        compiler_4.4.1     withr_3.0.1       \n [40] glasso_1.11        htmlTable_2.4.3    backports_1.5.0   \n [43] carData_3.0-5      ggsignif_0.6.4     MASS_7.3-61       \n [46] corpcor_1.6.10     gtools_3.9.5       tools_4.4.1       \n [49] pbivnorm_0.6.0     foreign_0.8-87     zip_2.3.1         \n [52] httpuv_1.6.15      nnet_7.3-19        glue_1.7.0        \n [55] quadprog_1.5-8     promises_1.3.0     nlme_3.1-166      \n [58] lisrelToR_0.3      grid_4.4.1         pbdZMQ_0.3-13     \n [61] checkmate_2.3.2    cluster_2.1.6      reshape2_1.4.4    \n [64] generics_0.1.3     gtable_0.3.5       tzdb_0.4.0        \n [67] data.table_1.16.0  hms_1.1.3          car_3.1-2         \n [70] utf8_1.2.4         sem_3.1-16         pillar_1.9.0      \n [73] IRdisplay_1.1      rockchalk_1.8.157  later_1.3.2       \n [76] splines_4.4.1      lattice_0.22-6     survival_3.7-0    \n [79] kutils_1.73        tidyselect_1.2.1   miniUI_0.1.1.1    \n [82] pbapply_1.7-2      stats4_4.4.1       xfun_0.47         \n [85] qgraph_1.9.8       arm_1.14-4         stringi_1.8.4     \n [88] pacman_0.5.1       boot_1.3-31        evaluate_1.0.0    \n [91] codetools_0.2-20   mi_1.1             cli_3.6.3         \n [94] RcppParallel_5.1.9 IRkernel_1.3.2     rpart_4.1.23      \n [97] xtable_1.8-4       repr_1.1.7         munsell_0.5.1     \n[100] Rcpp_1.0.13        coda_0.19-4.1      png_0.1-8         \n[103] XML_3.99-0.17      parallel_4.4.1     jpeg_0.1-10       \n[106] mvtnorm_1.3-1      openxlsx_4.2.7.1   crayon_1.5.3      \n[109] rlang_1.1.4        multcomp_1.4-26    mnormt_2.1.1      \n\n\n\n\n\n\nAlphen, T. van, Jak, S., Jansen in de Wal, J., Schuitema, J., & Peetsma, T. (2022). Determining reliability of daily measures: An illustration with data on teacher stress. Applied Measurement in Education, 35(1), 63–79.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>82</span>  <span class='chapter-title'>Affidabilità longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/prediction/01_prediction.html",
    "href": "chapters/prediction/01_prediction.html",
    "title": "83  Predizione",
    "section": "",
    "text": "83.1 Introduzione\nLe predizioni rappresentano un aspetto cruciale in numerosi ambiti. Possono riguardare sia dati categoriali, valutabili attraverso strumenti come matrici di confusione e modelli di regressione logistica, sia dati continui, analizzabili tramite regressioni multiple o modelli più complessi come quelli misti o a equazioni strutturali.\nIl capitolo si concentra sulla valutazione delle predizioni in contesti categoriali, approfondendo l’utilizzo della curva ROC e dell’AUC (Area Under the Curve) per misurare la qualità dei modelli predittivi.",
    "crumbs": [
      "Predizione",
      "<span class='chapter-number'>83</span>  <span class='chapter-title'>Predizione</span>"
    ]
  },
  {
    "objectID": "chapters/prediction/01_prediction.html#probabilità-di-una-condizione-medica",
    "href": "chapters/prediction/01_prediction.html#probabilità-di-una-condizione-medica",
    "title": "83  Predizione",
    "section": "\n83.2 Probabilità di una condizione medica",
    "text": "83.2 Probabilità di una condizione medica\nUn esempio classico, discusso da Petersen (2024), illustra il problema di calcolare la probabilità di avere l’HIV (\\(P(HIV \\mid Test+)\\)) partendo da un risultato positivo a un test diagnostico. Le informazioni di base sono:\n\n\nTasso di base dell’HIV (\\(P(HIV)\\)): 0.3% (0.003), ovvero la probabilità che una persona nella popolazione generale abbia l’HIV.\n\nSensibilità del test (\\(P(Test+ \\mid HIV)\\)): 95% (0.95), ovvero la probabilità che il test sia positivo quando la persona ha l’HIV.\n\nSpecificità del test (\\(P(Test- \\mid \\neg HIV)\\)): 99.28% (0.9928), ovvero la probabilità che il test sia negativo quando la persona non ha l’HIV.\n\nUtilizziamo il teorema di Bayes per calcolare \\(P(HIV \\mid Test+)\\):\n\\[\nP(HIV \\mid Test+) = \\frac{P(Test+ \\mid HIV) \\cdot P(HIV)}{P(Test+)},\n\\]\ndove il denominatore \\(P(Test+)\\) rappresenta la probabilità complessiva di un test positivo, somma di:\n\n\nVeri positivi: \\(P(Test+ \\mid HIV) \\cdot P(HIV)\\),\n\nFalsi positivi: \\(P(Test+ \\mid \\neg HIV) \\cdot P(\\neg HIV)\\).\n\nCon \\(P(Test+ \\mid \\neg HIV) = 1 - P(Test- \\mid \\neg HIV) = 1 - 0.9928 = 0.0072\\), e \\(P(\\neg HIV) = 1 - P(HIV) = 0.997\\), possiamo calcolare:\n\\[\nP(Test+) = (0.95 \\cdot 0.003) + (0.0072 \\cdot 0.997) \\approx 0.010027.\n\\]\nInserendo i valori nella formula di Bayes:\n\\[\nP(HIV \\mid Test+) = \\frac{0.95 \\cdot 0.003}{0.010027} \\approx 0.2844 \\quad (28.44\\%).\n\\]\nUn primo test positivo aumenta quindi la probabilità di avere l’HIV al 28.44%.\nDopo un primo test positivo, la probabilità di avere l’HIV è aumentata al 28.44%. Consideriamo ora l’effetto di un secondo test positivo e calcoliamo la probabilità aggiornata di avere l’HIV.\nLa probabilità di ottenere un secondo test positivo (\\(P(\\text{Secondo Test+})\\)) si calcola considerando due scenari:\n\n\nLa persona ha l’HIV:\n\nProbabilità: \\(P(HIV \\mid Test+) = 0.2844\\),\nSensibilità del test: \\(P(Test+ \\mid HIV) = 0.95\\).\n\n\n\nLa persona non ha l’HIV:\n\nProbabilità: \\(P(\\neg HIV \\mid Test+) = 1 - P(HIV \\mid Test+) = 0.7156\\),\nTasso di falsi positivi: \\(P(Test+ \\mid \\neg HIV) = 0.0072\\).\n\n\n\nLa probabilità totale è data da:\n\\[\nP(\\text{Secondo Test+}) = P(Test+ \\mid HIV) \\cdot P(HIV \\mid Test+) + P(Test+ \\mid \\neg HIV) \\cdot P(\\neg HIV \\mid Test+).\n\\]\nSostituendo i valori:\n\\[\nP(\\text{Secondo Test+}) = (0.95 \\cdot 0.2844) + (0.0072 \\cdot 0.7156) \\approx 0.2753.\n\\]\nAggiorniamo la probabilità di avere l’HIV dopo un secondo test positivo usando nuovamente il teorema di Bayes:\n\\[\nP(HIV \\mid \\text{Secondo Test+}) = \\frac{P(Test+ \\mid HIV) \\cdot P(HIV \\mid Test+)}{P(\\text{Secondo Test+})}.\n\\]\nSostituendo i valori:\n\\[\nP(HIV \\mid \\text{Secondo Test+}) = \\frac{0.95 \\cdot 0.2844}{0.2753} \\approx 0.981 \\quad (98.1\\%).\n\\]\nL’esempio presentato da Petersen (2024) è rilevante per il problema generale della predizione, in quanto illustra come il ragionamento bayesiano consenta di integrare informazioni iniziali e successive per migliorare la precisione delle stime. In particolare, i risultati evidenziano tre aspetti fondamentali:\n\nTasso di base come punto di partenza cruciale: La probabilità iniziale (prior) di avere l’HIV, pari allo 0.3%, sottolinea quanto sia importante considerare il contesto epidemiologico e demografico nella fase iniziale della predizione. Questo valore guida l’intero processo di aggiornamento e mostra che una condizione rara richiede prove forti per modificarne la probabilità.\nAggiornamento incrementale delle probabilità: Il passaggio da una probabilità del 28.44% dopo un primo test positivo a una probabilità del 98.1% dopo un secondo test positivo evidenzia la potenza del teorema di Bayes nel combinare evidenze successive. Ogni risultato positivo aggiunge informazioni che riducono l’incertezza iniziale, migliorando progressivamente la qualità della predizione.\nValore aggiunto dei test ripetuti: L’analisi dimostra che l’efficacia diagnostica cresce con l’accumularsi di evidenze. Test ripetuti consentono di discriminare meglio tra casi veri positivi e falsi positivi, fornendo stime più affidabili e utili per decisioni cliniche.\n\nQuesti risultati mettono in luce l’importanza del ragionamento bayesiano non solo per valutare la probabilità di una condizione medica, ma anche per affrontare una vasta gamma di problemi di predizione in cui l’incertezza iniziale può essere ridotta integrando dati nuovi. L’approccio evidenzia come sia possibile arrivare a conclusioni robuste anche in contesti caratterizzati da bassi tassi di base e test diagnostici non perfetti.",
    "crumbs": [
      "Predizione",
      "<span class='chapter-number'>83</span>  <span class='chapter-title'>Predizione</span>"
    ]
  },
  {
    "objectID": "chapters/prediction/01_prediction.html#accuratezza-delle-predizioni",
    "href": "chapters/prediction/01_prediction.html#accuratezza-delle-predizioni",
    "title": "83  Predizione",
    "section": "\n83.3 Accuratezza delle Predizioni",
    "text": "83.3 Accuratezza delle Predizioni\nDopo l’introduzione sull’uso del teorema di Bayes per la predizione, Petersen (2024) affronta il tema delle predizioni con una tabella 2 × 2. Per spiegare questo caso, Petersen (2024) presenta un esempio adattato da Meehl & Rosen (1955).\nImmaginiamo che l’esercito americano utilizzi un test per escludere i candidati con basse probabilità di completare l’addestramento di base. Per analizzare l’accuratezza delle predizioni effettuate, possiamo utilizzare una matrice di confusione, che confronta le predizioni del test con i risultati reali.\n\n\n\n\n\n\n\n\n\nDecisione (Predizione)\nAdattamento Effettivo (Scarso)\nAdattamento Effettivo (Buono)\nTotale Predetto\nRapporto di Selezione (SR)\n\n\n\nEscludere\nTP = 86 (0.043)\nFP = 422 (0.211)\n508\nSR = 0.254\n\n\nTrattenere\nFN = 14 (0.007)\nTN = 1.478 (0.739)\n1,492\n1 − SR = 0.746\n\n\nTotale Effettivo\n100\n1,900\nN = 2,000\n\n\n\nTasso di Base (BR)\nBR = 0.05\n1 − BR = 0.95\n\n\n\n\n\n\n83.3.1 La Matrice di Confusione\nLa matrice di confusione è uno strumento che mette in relazione le predizioni di un modello con i risultati osservati. Nel caso di una predizione binaria (es. sì/no o positivo/negativo), la matrice è organizzata in quattro categorie:\n\n\nVero Positivo (TP): La predizione corretta che identifica una persona con la caratteristica (es. cattivo adattamento).\n\nVero Negativo (TN): La predizione corretta che identifica una persona senza la caratteristica.\n\nFalso Positivo (FP): L’errore in cui si predice la presenza della caratteristica quando in realtà non è presente.\n\nFalso Negativo (FN): L’errore in cui si predice l’assenza della caratteristica quando in realtà è presente.\n\nQuesti quattro risultati consentono di valutare l’accuratezza di un modello.\n\n83.3.2 Tassi Marginali e Indicatori Chiave\nDalla matrice di confusione possiamo calcolare alcuni tassi e indicatori utili:\n\nTasso di Base (BR): La probabilità che una persona abbia la caratteristica di interesse. Ad esempio: \\[\nBR = \\frac{FN + TP}{N} = \\frac{100}{2000} = 0.05\n\\] Ciò significa che solo il 5% dei candidati ha un cattivo adattamento.\nRapporto di Selezione (SR): La probabilità di essere esclusi dal programma: \\[\nSR = \\frac{TP + FP}{N} = \\frac{508}{2000} = 0.254\n\\] In questo caso, il 25,4% dei candidati è stato escluso.\nPercentuale di Accuratezza: Rappresenta la proporzione di predizioni corrette sul totale: \\[\n\\text{Accuratezza} = 100 \\times \\frac{TP + TN}{N} = 100 \\times \\frac{86 + 1478}{2000} = 78\\%\n\\]\nAccuratezza per Caso: Misura la precisione che si otterrebbe effettuando predizioni casuali basate solo sulle probabilità marginali (BR e SR). Per esempio: \\[\nP(TP) = BR \\times SR = 0.05 \\times 0.254 = 0.0127\n\\] \\[\nP(TN) = (1 − BR) \\times (1 − SR) = 0.95 \\times 0.746 = 0.7087\n\\] \\[\n\\text{Accuratezza per Caso} = P(TP) + P(TN) = 0.0127 + 0.7087 = 0.7214 \\, (72,14\\%)\n\\]\n\nConfrontando il 78% di accuratezza del modello con il 72.14% ottenibile per caso, il modello fornisce un miglioramento del 6%.\n\n83.3.3 L’Importanza del Tasso di Base\nQuando il tasso di base è molto basso (come in questo caso, BR = 0.05), l’accuratezza complessiva può essere ingannevole. Se predicessimo che nessuno ha un cattivo adattamento, otterremmo un’accuratezza del 95%, ma il modello non identificherebbe alcun caso di cattivo adattamento.\n\n\n\n\n\n\n\n\nDecisione (Predizione)\nAdattamento Effettivo (Scarso)\nAdattamento Effettivo (Buono)\nTotale Predetto\n\n\n\nEscludere\nTP = 0\nFP = 0\n0\n\n\nTrattenere\nFN = 100\nTN = 1,900\n2,000\n\n\nTotale Effettivo\n100\n1,900\nN = 2,000\n\n\n\nIn questo caso, l’accuratezza complessiva sarebbe: \\[\nP(\\text{Accuratezza}) = \\frac{TP + TN}{N} = \\frac{0 + 1900}{2000} = 95\\%.\n\\]\nQuesto esempio evidenzia che un’elevata accuratezza complessiva non garantisce un buon modello, specialmente quando il tasso di base è sbilanciato.\nIn conclusione, l’analisi di una matrice di confusione richiede attenzione ai tassi di base e agli errori, poiché l’accuratezza globale può essere fuorviante. È essenziale confrontare il valore del modello con ciò che si otterrebbe per caso o con strategie alternative, come basarsi solo sul tasso di base. Inoltre, occorre considerare il peso relativo degli errori (falsi positivi e falsi negativi) in base al contesto applicativo. Questi aspetti saranno discussi nel prossimo paragrafo.\n\n83.3.4 Diversi Tipi di Errori e i loro Costi\nIn un processo di classificazione, non tutti gli errori hanno lo stesso costo. Esistono due tipi principali di errori: i falsi positivi e i falsi negativi, ciascuno con implicazioni diverse che dipendono dal contesto della predizione.\nSpesso, l’accuratezza complessiva può essere aumentata affidandosi semplicemente al tasso di base, ma in molte situazioni può essere preferibile utilizzare uno strumento di screening, anche a costo di una minore accuratezza complessiva, se ciò consente di minimizzare errori specifici che hanno costi elevati. Ad esempio:\n\nScreening medico: Consideriamo uno strumento di screening per l’HIV. I falsi positivi (classificare erroneamente una persona come a rischio) comportano costi come la necessità di test di conferma e, talvolta, ansia temporanea per l’individuo. Tuttavia, un falso negativo (non identificare una persona effettivamente a rischio) ha costi molto più alti, poiché potrebbe portare a un mancato intervento precoce, con conseguenze gravi per la salute. In questo caso, i costi associati ai falsi negativi superano di gran lunga quelli dei falsi positivi, rendendo lo screening preferibile nonostante una diminuzione dell’accuratezza complessiva.\nSelezione del personale in situazioni di rischio: La CIA, ad esempio, ha utilizzato strumenti di selezione per identificare potenziali spie durante periodi di guerra. Un falso positivo in questo contesto (considerare erroneamente una persona come una spia) potrebbe risultare nell’esclusione di un candidato innocente. Un falso negativo (assumere una persona che è effettivamente una spia) comporta rischi molto più gravi, rendendo cruciale l’identificazione corretta delle spie, anche a costo di più falsi positivi.\n\nIl modo in cui i costi degli errori vengono valutati dipende fortemente dal contesto. Alcuni potenziali costi dei falsi positivi includono trattamenti medici non necessari o il rischio di incarcerare una persona innocente. Al contrario, i falsi negativi possono portare al rilascio di una persona pericolosa, alla mancata individuazione di una malattia grave, o al mancato riconoscimento di un rischio imminente.\n\n83.3.5 Importanza del Rapporto di Selezione e del Tasso di Base\nIl costo degli errori può variare a seconda di come si imposta il rapporto di selezione (cioè, quanto rigorosamente si applica il criterio per accettare o escludere un individuo). La scelta di un rapporto di selezione meno restrittivo o più restrittivo influisce sulla probabilità di incorrere in falsi positivi e falsi negativi e può dipendere dal contesto e dai costi associati agli errori.\n\n\nCriterio meno rigido: Se escludere candidati è costoso, ad esempio quando si ha la necessità di assumere molte persone, potrebbe essere più utile un criterio di selezione permissivo, che accetta anche persone con un rischio potenziale.\n\nCriterio più rigido: In contesti in cui non è necessario accettare molti individui, si può adottare un criterio di selezione più rigido per ridurre i rischi, scartando un numero maggiore di candidati sospetti.\n\nQuando il rapporto di selezione differisce dal tasso di base degli esiti negativi effettivi, inevitabilmente si generano errori:\n\nSe, ad esempio, il rapporto di selezione prevede di escludere il 25% dei candidati, ma solo il 5% risulta effettivamente “non idoneo,” il risultato sarà un numero elevato di falsi positivi.\nD’altro canto, se si esclude solo l’1% dei candidati mentre il tasso di non idoneità è del 5%, si finirà per includere molti falsi negativi.\n\n83.3.6 Predizioni e Affidabilità in Condizioni di Basso Tasso di Base\nFare predizioni accurate diventa particolarmente complesso quando il tasso di base è basso, come nel caso di eventi rari (ad esempio, il suicidio). In questi casi, il numero di casi positivi reali è molto ridotto, rendendo difficile identificare correttamente i pochi eventi positivi senza generare numerosi falsi positivi o falsi negativi.\nQuesta difficoltà può essere compresa in relazione alla teoria classica dei test, che definisce l’affidabilità come il rapporto tra la varianza del punteggio vero e la varianza del punteggio osservato. Con un tasso di base molto basso, la varianza del punteggio vero è ridotta, il che abbassa l’affidabilità della misura e rende più complessa una predizione accurata.\n\n83.3.7 Sensibilità, Specificità, PPV e NPV\nCome abbiamo visto, la percentuale di accuratezza da sola non è sufficiente per valutare l’efficacia di un modello, poiché è molto influenzata dai tassi di base. Ad esempio, se il tasso di base è basso, potremmo ottenere un’alta percentuale di accuratezza semplicemente affermando che nessuno ha la condizione; se è alto, affermando che tutti ce l’hanno. Perciò, è essenziale considerare altre metriche di accuratezza, come sensibilità (SN), specificità (SP), valore predittivo positivo (PPV) e valore predittivo negativo (NPV).\nQueste metriche, che si possono calcolare dalla matrice di confusione, ci aiutano a valutare se il modello è efficace nel rilevare la condizione senza includere erroneamente i casi negativi. Analizziamole in dettaglio:\n\n\nSensibilità (SN): indica la capacità del test di identificare correttamente i veri positivi, cioè le persone con la condizione. Si calcola come la proporzione di veri positivi (\\(\\text{TP}\\)) rispetto al totale di persone con la condizione (\\(\\text{TP} + \\text{FN}\\)):\n\\[\n\\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{86}{86 + 14} = 0.86\n\\]\n\n\nSpecificità (SP): misura la capacità del test di identificare correttamente i veri negativi, ossia le persone senza la condizione. Si calcola come la proporzione di veri negativi (\\(\\text{TN}\\)) rispetto al totale di persone senza la condizione (\\(\\text{TN} + \\text{FP}\\)):\n\\[\n\\frac{\\text{TN}}{\\text{TN} + \\text{FP}} = \\frac{1,478}{1,478 + 422} = 0.78\n\\]\n\n\nValore Predittivo Positivo (PPV): indica la probabilità che una persona classificata come positiva abbia effettivamente la condizione. Si calcola come la proporzione di veri positivi (\\(\\text{TP}\\)) sul totale dei positivi stimati (\\(\\text{TP} + \\text{FP}\\)):\n\\[\n\\frac{\\text{TP}}{\\text{TP} + \\text{FP}} = \\frac{86}{86 + 422} = 0.17\n\\]\n\n\nValore Predittivo Negativo (NPV): rappresenta la probabilità che una persona classificata come negativa non abbia effettivamente la condizione. Si calcola come la proporzione di veri negativi (\\(\\text{TN}\\)) sul totale dei negativi stimati (\\(\\text{TN} + \\text{FN}\\)):\n\\[\n\\frac{\\text{TN}}{\\text{TN} + \\text{FN}} = \\frac{1,478}{1,478 + 14} = 0.99\n\\]\n\n\nOgni misura è espressa come una proporzione, variando da 0 a 1, dove valori più alti indicano una maggiore accuratezza per ciascun aspetto specifico. Usando queste metriche otteniamo un quadro dettagliato dell’efficacia dello strumento a un determinato cutoff.\nIn questo caso, il nostro strumento mostra:\n\n\nAlta sensibilità (0.86): è efficace nel rilevare chi ha la condizione.\n\nBassa specificità (0.78): classifica erroneamente come positivi molti casi che non hanno la condizione.\n\nBasso PPV (0.17): la maggior parte dei casi classificati come positivi sono in realtà negativi, indicando una frequenza elevata di falsi positivi.\n\nAlto NPV (0.99): quasi tutti i casi classificati come negativi non hanno la condizione.\n\nQuindi, pur avendo una buona capacità di rilevare i positivi (alta sensibilità), il modello è meno efficace nel limitare i falsi positivi (basso PPV). Questo potrebbe essere accettabile se l’obiettivo è identificare tutti i potenziali casi positivi, anche a costo di includere molti falsi positivi, ma potrebbe non essere ideale se il costo degli errori di falsa positività è elevato.",
    "crumbs": [
      "Predizione",
      "<span class='chapter-number'>83</span>  <span class='chapter-title'>Predizione</span>"
    ]
  },
  {
    "objectID": "chapters/prediction/01_prediction.html#stime-di-accuratezza-e-cutoff",
    "href": "chapters/prediction/01_prediction.html#stime-di-accuratezza-e-cutoff",
    "title": "83  Predizione",
    "section": "\n83.4 Stime di Accuratezza e Cutoff",
    "text": "83.4 Stime di Accuratezza e Cutoff\nSensibilità, specificità, PPV e NPV variano in base al cutoff (ovvero, la soglia) per la classificazione. Consideriamo il seguente esempio. Degli alieni visitano la Terra e sviluppano un test per determinare se una bacca è commestibile o non commestibile.\n\nsampleSize &lt;- 1000\n\nedibleScores &lt;- rnorm(sampleSize, 50, 15)\ninedibleScores &lt;- rnorm(sampleSize, 100, 15)\n\nedibleData &lt;- data.frame(score = c(edibleScores, inedibleScores), type = c(rep(\"edible\", sampleSize), rep(\"inedible\", sampleSize)))\n\ncutoff &lt;- 75\n\nhist_edible &lt;- density(edibleScores, from = 0, to = 150) %$%\n    data.frame(x = x, y = y) %&gt;%\n    mutate(area = x &gt;= cutoff)\n\nhist_edible$type[hist_edible$area == TRUE] &lt;- \"edible_FP\"\nhist_edible$type[hist_edible$area == FALSE] &lt;- \"edible_TN\"\n\nhist_inedible &lt;- density(inedibleScores, from = 0, to = 150) %$%\n    data.frame(x = x, y = y) %&gt;%\n    mutate(area = x &lt; cutoff)\n\nhist_inedible$type[hist_inedible$area == TRUE] &lt;- \"inedible_FN\"\nhist_inedible$type[hist_inedible$area == FALSE] &lt;- \"inedible_TP\"\n\ndensity_data &lt;- bind_rows(hist_edible, hist_inedible)\n\ndensity_data$type &lt;- factor(density_data$type, levels = c(\"edible_TN\", \"inedible_TP\", \"edible_FP\", \"inedible_FN\"))\n\nLa figura successiva mostra le distribuzioni dei punteggi in base al tipo di bacca. Si può notare come ci sono due distribuzioni distinte, ma con una certa sovrapposizione. Pertanto, qualsiasi cutoff selezionato comporterà almeno alcune classificazioni errate. L’entità della sovrapposizione delle distribuzioni riflette la quantità di errore di misurazione dello strumento rispetto alla caratteristica di interesse.\n\nggplot(data = edibleData, aes(x = score, ymin = 0, fill = type)) +\n    geom_density(alpha = .5) +\n    scale_fill_manual(\n      name = \"Tipo di Bacca\", values = c(viridis(2)[1], viridis(2)[2])\n    ) +\n    scale_y_continuous(name = \"Frequenza\") \n\n\n\n\n\n\n\nLa figura successiva mostra le distribuzioni dei punteggi in base al tipo di bacca con un cutoff. La linea rossa indica il cutoff: il livello al di sopra del quale le bacche vengono classificate come non commestibili. Ci sono errori su entrambi i lati del cutoff. Sotto il cutoff, ci sono dei falsi negativi (blu): bacche non commestibili erroneamente classificate come commestibili. Sopra il cutoff, ci sono dei falsi positivi (verde): bacche commestibili erroneamente classificate come non commestibili. I costi dei falsi negativi potrebbero includere malattia o morte derivanti dal consumo di bacche non commestibili, mentre i costi dei falsi positivi potrebbero includere maggiore tempo per trovare cibo, insufficienza di cibo e fame.\n\nggplot(data = density_data, aes(x = x, ymin = 0, ymax = y, fill = type)) +\n    geom_ribbon(alpha = 1) +\n    scale_fill_manual(\n        name = \"Tipo di Bacca\",\n        values = c(viridis(4)[4], viridis(4)[1], viridis(4)[3], viridis(4)[2]),\n        breaks = c(\"edible_TN\", \"inedible_TP\", \"edible_FP\", \"inedible_FN\"),\n        labels = c(\n          \"Commestibile: TN\", \"Non Commestibile: TP\", \n          \"Commestibile: FP\", \"Non Commestibile: FN\")\n    ) +\n    geom_line(aes(y = y)) +\n    geom_vline(xintercept = cutoff, color = \"red\", linewidth = 2) +\n    scale_x_continuous(name = \"Punteggio\") +\n    scale_y_continuous(name = \"Frequenza\") \n\n\n\n\n\n\n\nA seconda dei nostri obiettivi di valutazione, potremmo voler usare un diverso rapporto di selezione modificando il cutoff. La Figura mostra le distribuzioni dei punteggi quando si aumenta il cutoff. Ora ci sono più falsi negativi (blu) e meno falsi positivi (verde). Se alziamo il cutoff per essere più conservativi, il numero di falsi negativi aumenta, mentre il numero di falsi positivi diminuisce. Di conseguenza, aumentando il cutoff, la sensibilità e il valore predittivo negativo (NPV) diminuiscono, mentre la specificità e il valore predittivo positivo (PPV) aumentano. Un cutoff più alto potrebbe essere ottimale se i costi dei falsi positivi sono considerati superiori a quelli dei falsi negativi. Ad esempio, se gli alieni non possono rischiare di mangiare bacche non commestibili perché sono fatali, e ci sono abbastanza bacche commestibili per nutrire la colonia aliena.\n\n# Raise the cutoff\ncutoff &lt;- 85\n\nggplot(data = density_data, aes(x = x, ymin = 0, ymax = y, fill = type)) +\n    geom_ribbon(alpha = 1) +\n    scale_fill_manual(\n        name = \"Tipo di Bacca\",\n        values = c(viridis(4)[4], viridis(4)[1], viridis(4)[3], viridis(4)[2]),\n        breaks = c(\"edible_TN\", \"inedible_TP\", \"edible_FP\", \"inedible_FN\"),\n        labels = c(\n          \"Commestibile: TN\", \"Non Commestibile: TP\", \n          \"Commestibile: FP\", \"Non Commestibile: FN\")\n    ) +\n    geom_line(aes(y = y)) +\n    geom_vline(xintercept = cutoff, color = \"red\", linewidth = 2) +\n    scale_x_continuous(name = \"Punteggio\") +\n    scale_y_continuous(name = \"Frequenza\") +\n    theme(\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank()\n    )\n\n\n\n\n\n\n\nIn alternativa, possiamo abbassare il cutoff per essere più liberali. La Figura seguente mostra le distribuzioni dei punteggi quando abbassiamo il cutoff. Ora ci sono meno falsi negativi (blu) e più falsi positivi (verde). Abbassando il cutoff, la sensibilità e il NPV aumentano, mentre la specificità e il PPV diminuiscono. Un cutoff più basso potrebbe essere ottimale se i costi dei falsi negativi sono considerati superiori a quelli dei falsi positivi. Ad esempio, se gli alieni non possono rischiare di perdere bacche commestibili perché sono scarse, e mangiare bacche non commestibili comporta solo disagi temporanei.\n\n# Lower the cutoff\ncutoff &lt;- 65\n\nggplot(data = density_data, aes(x = x, ymin = 0, ymax = y, fill = type)) +\n    geom_ribbon(alpha = 1) +\n    scale_fill_manual(\n        name = \"Tipo di Bacca\",\n        values = c(viridis(4)[4], viridis(4)[1], viridis(4)[3], viridis(4)[2]),\n        breaks = c(\"edible_TN\", \"inedible_TP\", \"edible_FP\", \"inedible_FN\"),\n        labels = c(\n          \"Commestibile: TN\", \"Non Commestibile: TP\", \n          \"Commestibile: FP\", \"Non Commestibile: FN\")\n    ) +\n    geom_line(aes(y = y)) +\n    geom_vline(xintercept = cutoff, color = \"red\", linewidth = 2) +\n    scale_x_continuous(name = \"Punteggio\") +\n    scale_y_continuous(name = \"Frequenza\") +\n    theme(\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank()\n    )\n\n\n\n\n\n\n\nIn sintesi, sensibilità e specificità variano in base al cutoff utilizzato per la classificazione. Se aumentiamo il cutoff, la specificità e il PPV aumentano, mentre la sensibilità e il NPV diminuiscono. Se abbassiamo il cutoff, la sensibilità e il NPV aumentano, mentre la specificità e il PPV diminuiscono. Pertanto, il cutoff ottimale dipende dai costi associati ai falsi negativi e ai falsi positivi. Se i falsi negativi sono più costosi, dovremmo impostare un cutoff basso; se i falsi positivi sono più costosi, dovremmo impostare un cutoff alto.",
    "crumbs": [
      "Predizione",
      "<span class='chapter-number'>83</span>  <span class='chapter-title'>Predizione</span>"
    ]
  },
  {
    "objectID": "chapters/prediction/01_prediction.html#teoria-della-detezione-del-segnale",
    "href": "chapters/prediction/01_prediction.html#teoria-della-detezione-del-segnale",
    "title": "83  Predizione",
    "section": "\n83.5 Teoria della Detezione del Segnale",
    "text": "83.5 Teoria della Detezione del Segnale\nLa Teoria della Detezione del Segnale (Signal Detection Theory, SDT) è un approccio probabilistico che descrive la capacità di rilevare uno stimolo target (segnale) in un contesto di stimoli non target (rumore). Sviluppata durante la Seconda Guerra Mondiale per ottimizzare le prestazioni di radar e sonar, questa teoria distingue tra due aspetti fondamentali: sensibilità e bias.\n\nSensibilità\nLa sensibilità quantifica l’abilità di discriminare tra segnale e rumore, indipendentemente dalle tendenze decisionali. È una misura della qualità del sistema di rilevamento.\nBias\nIl bias riflette la tendenza dell’osservatore a rispondere in modo conservativo o liberale, cioè a sovrastimare o sottostimare la presenza del segnale, influenzando la soglia decisionale.\n\nOriginariamente impiegata per la selezione e l’addestramento degli operatori radar, la SDT ha trovato applicazioni in numerosi settori moderni. In medicina, ad esempio, viene utilizzata per valutare la capacità di diagnosticare patologie come tumori o infezioni. In psicologia, la SDT ha contribuito a studiare processi percettivi e decisionali, come la percezione sociale, evidenziando differenze sistematiche nella sensibilità e nel bias tra individui o gruppi.\n\n83.5.1 Metriche Principali\nLa SDT introduce metriche specifiche per quantificare sensibilità e bias:\n\n\n\\(d'\\): misura la sensibilità, rappresentando la separazione statistica tra distribuzioni di segnale e rumore.\n\n\n\\(\\beta\\), \\(c\\), \\(b\\): diverse misure del bias decisionale, che descrivono l’inclinazione del soggetto verso risposte più conservative o liberali.",
    "crumbs": [
      "Predizione",
      "<span class='chapter-number'>83</span>  <span class='chapter-title'>Predizione</span>"
    ]
  },
  {
    "objectID": "chapters/prediction/01_prediction.html#curva-roc-e-auc",
    "href": "chapters/prediction/01_prediction.html#curva-roc-e-auc",
    "title": "83  Predizione",
    "section": "\n83.6 Curva ROC e AUC",
    "text": "83.6 Curva ROC e AUC\nLa curva ROC (Receiver Operating Characteristic) è uno strumento essenziale per l’analisi delle prestazioni di modelli di classificazione binaria, particolarmente utile per visualizzare il compromesso tra sensibilità e specificità. Questa curva è strettamente connessa alla Signal Detection Theory (SDT), che fornisce una base teorica per comprendere i processi decisionali sotto incertezza.\nMentre la SDT descrive i principi fondamentali del processo decisionale, la curva ROC rappresenta un’applicazione pratica che consente di analizzare come la variazione della soglia decisionale influenzi le prestazioni del modello. Questa analisi è particolarmente utile per identificare soglie che ottimizzino il bilanciamento tra errori di classificazione (falsi positivi e falsi negativi).\n\n83.6.1 Definizione e Significato\nLa curva ROC visualizza il compromesso tra sensibilità (True Positive Rate, TPR) e 1-specificità (False Positive Rate, FPR) al variare della soglia decisionale. Ogni punto della curva rappresenta una combinazione specifica di questi due indicatori.\n\n\nSensibilità (TPR): proporzione di casi positivi correttamente classificati rispetto al totale dei veri positivi.\n\n1-Specificità (FPR): proporzione di falsi positivi rispetto al totale dei veri negativi.\n\n\n83.6.1.1 Costruzione della Curva ROC\n\n\nCalcolo di TPR e FPR: Si definiscono diverse soglie per il modello e, per ciascuna, si calcolano i valori di sensibilità e 1-specificità.\n\nGrafico bidimensionale: La sensibilità è rappresentata sull’asse y, mentre 1-specificità è riportata sull’asse x.\n\nInterpretazione: Una curva che si avvicina all’angolo superiore sinistro indica prestazioni elevate, con alta sensibilità e specificità. Una curva vicina alla diagonale principale rappresenta invece un modello privo di capacità discriminativa.\n\n83.6.2 Area Sotto la Curva (AUC)\nL’Area Under the Curve (AUC) è una misura aggregata della capacità del modello di distinguere tra classi. È definita come l’area sotto la curva ROC e può assumere valori compresi tra 0.5 (discriminazione casuale) e 1 (classificazione perfetta).\n\n83.6.2.1 Interpretazione dei Valori di AUC\n\n\nAUC = 1: Il modello distingue perfettamente tra classi positive e negative.\n\n0.9 ≤ AUC &lt; 1: Prestazioni eccellenti.\n\n0.75 ≤ AUC &lt; 0.9: Buone prestazioni.\n\n0.5 ≤ AUC &lt; 0.75: Prestazioni moderate.\n\nAUC = 0.5: Nessuna capacità discriminativa (classificazione casuale).\n\n83.6.3 Limiti della Curva ROC e dell’AUC\nSebbene la curva ROC e l’AUC siano strumenti utili, presentano alcune limitazioni che devono essere considerate per una corretta interpretazione.\n\n83.6.3.1 Problema dei Base Rate\nI base rate rappresentano la prevalenza di ciascuna classe nella popolazione. In dataset sbilanciati, l’AUC può essere fuorviante, poiché misura le prestazioni globali senza distinguere tra le difficoltà nella classificazione delle diverse classi.\nAd esempio, un modello potrebbe ottenere un AUC elevato classificando correttamente quasi tutti i casi della classe più frequente, trascurando però le prestazioni sulla classe meno rappresentata. Questo problema è particolarmente rilevante in applicazioni critiche, come la diagnosi di malattie rare, dove l’accuratezza della classe minoritaria è cruciale.\n\n83.6.4 La Base Rate Fallacy e i Limiti dell’AUC\nLa Base Rate Fallacy si manifesta quando i tassi di prevalenza di un evento (i cosiddetti base rate) vengono ignorati nell’interpretazione delle probabilità o nella valutazione delle prestazioni di un modello. Questo errore è particolarmente rilevante nei contesti in cui una delle due classi è molto meno frequente dell’altra.\nAd esempio, consideriamo un test diagnostico con un’accuratezza del 90% per una malattia rara che colpisce solo l’1% della popolazione. Anche se il test sembra altamente affidabile, il numero di falsi positivi potrebbe essere molto elevato rispetto ai veri positivi, a causa della bassa prevalenza della malattia. Di conseguenza, il valore predittivo positivo (probabilità che un individuo con test positivo sia realmente malato) sarebbe molto basso, rendendo il test di scarsa utilità pratica nonostante un AUC apparentemente elevato.\n\n83.6.4.1 Limiti dell’AUC\nPur essendo una misura aggregata utile per valutare le prestazioni complessive di un modello, l’AUC presenta alcune importanti limitazioni:\n\nInsensibilità ai Base Rate\nL’AUC non tiene conto della distribuzione delle classi, il che può portare a valutazioni fuorvianti in presenza di classi sbilanciate. Un modello potrebbe ottenere un AUC elevato classificando correttamente quasi tutti i casi della classe dominante, ignorando però quelli della classe meno rappresentata.\nMancanza di Dettagli sulle Soglie\nEssendo una misura aggregata, l’AUC non fornisce informazioni sulle prestazioni del modello a una soglia decisionale specifica. Questo può rappresentare un limite pratico quando è necessario scegliere un cutoff ottimale per massimizzare le prestazioni in un determinato contesto applicativo.\nTrattamento Simmetrico degli Errori\nL’AUC considera falsi positivi e falsi negativi come equivalenti. Tuttavia, in molti contesti reali (ad esempio, in ambito medico), questi errori hanno conseguenze molto diverse, e il loro bilanciamento può essere cruciale.\nSovrastima delle Prestazioni in Dataset Sbilanciati\nNei dataset con classi fortemente sbilanciate, l’AUC può mascherare le difficoltà del modello nel trattare correttamente la classe meno rappresentata, portando a una sopravvalutazione delle sue prestazioni globali.\n\nIn sintesi, sebbene la curva ROC e l’AUC siano strumenti fondamentali per l’analisi delle prestazioni dei modelli di classificazione, devono essere utilizzati con consapevolezza dei loro limiti. In contesti caratterizzati da base rate sbilanciati, è importante integrare queste metriche con altre, come i valori predittivi positivo e negativo, o analisi mirate delle soglie decisionali.\nUna valutazione completa delle prestazioni di un modello richiede di considerare il contesto applicativo, i costi relativi degli errori di classificazione e le implicazioni pratiche delle decisioni prese sulla base dei risultati del modello. Solo così è possibile ottimizzare il processo decisionale e garantire l’utilità del modello nelle applicazioni reali.\n\n83.6.5 Esempio in R\nIn questo esempio, utilizziamo un dataset simulato che rappresenta i punteggi ottenuti da un questionario psicologico sullo stress percepito e la loro associazione con l’appartenenza a due gruppi: basso stress e alto stress.\nCreiamo un dataset con 200 partecipanti, suddivisi equamente tra i due gruppi:\n\n# Generazione del dataset simulato\nset.seed(123)\nn &lt;- 200\n# Creazione di variabili\ngroup &lt;- factor(\n  rep(c(\"Basso\", \"Alto\"), each = n / 2), \n  levels = c(\"Basso\", \"Alto\")\n)\nstress_score &lt;- c(\n  rnorm(n / 2, mean = 50, sd = 10), # Gruppo basso stress\n  rnorm(n / 2, mean = 70, sd = 10)  # Gruppo alto stress\n)\n# Creazione del dataframe\ndata_stress &lt;- data.frame(\n  group = group,\n  stress_score = stress_score\n)\n# Visualizzazione delle prime righe\nhead(data_stress)\n#&gt;   group stress_score\n#&gt; 1 Basso         44.4\n#&gt; 2 Basso         47.7\n#&gt; 3 Basso         65.6\n#&gt; 4 Basso         50.7\n#&gt; 5 Basso         51.3\n#&gt; 6 Basso         67.2\n\nIl dataset contiene:\n\n\ngroup: Gruppo di appartenenza (Basso o Alto stress percepito).\n\nstress_score: Punteggio del questionario sullo stress percepito (variabile continua).\n\n\n83.6.5.1 Generazione della Curva ROC\nUtilizziamo la curva ROC per valutare la capacità del questionario di distinguere tra i due gruppi.\n\n# Generazione della curva ROC\nrocCurve &lt;- roc(data_stress$group, data_stress$stress_score)\n\nVisualizziamo il grafico della curva ROC e calcoliamo l’AUC:\n# Visualizzazione della curva ROC\nplot(rocCurve, legacy.axes = TRUE, print.auc = TRUE)\n\n83.6.5.2 Calcolo di Punti sulla Curva ROC per Soglie Specifiche\nFissiamo una serie di soglie per classificare i partecipanti in base al punteggio di stress percepito. Classifichiamo i partecipanti come “Alto stress” se il loro punteggio è superiore o uguale alla soglia, altrimenti come “Basso stress”.\n\n# Definiamo diverse soglie\nthresholds &lt;- seq(0, 100, by = 5)\n\n# Funzione per calcolare sensibilità e specificità\ncalc_metrics &lt;- function(threshold, data) {\n  # Predizione basata sulla soglia\n  predicted &lt;- factor(\n    ifelse(data$stress_score &lt;= threshold, \"Alto\", \"Basso\"),\n    levels = c(\"Alto\", \"Basso\")\n  )\n  \n  # Gruppo osservato\n  actual &lt;- factor(data$group, levels = c(\"Alto\", \"Basso\"))\n  \n  # Matrice di confusione\n  conf_matrix &lt;- table(Predicted = predicted, Actual = actual)\n  \n  # Estrarre metriche dalla matrice di confusione\n  TP &lt;- conf_matrix[\"Alto\", \"Alto\"]\n  FP &lt;- conf_matrix[\"Alto\", \"Basso\"]\n  TN &lt;- conf_matrix[\"Basso\", \"Basso\"]\n  FN &lt;- conf_matrix[\"Basso\", \"Alto\"]\n  \n  # Calcolo di sensibilità e specificità\n  sensitivity &lt;- TP / (TP + FN)\n  specificity &lt;- TN / (TN + FP)\n  \n  # Restituzione dei risultati\n  return(list(\n    threshold = threshold,\n    sensitivity = sensitivity,\n    specificity = specificity,\n    fpr = 1 - specificity\n  ))\n}\n\n# Calcolo metriche per ogni soglia\nresults &lt;- lapply(thresholds, function(t) calc_metrics(t, data_stress))\n\n# Estrazione dei valori per il plotting\nfpr_values &lt;- sapply(results, function(x) x$fpr)\nsens_values &lt;- sapply(results, function(x) x$sensitivity)\n\n# Invertire y per calcolare i punti richiesti\npoints_inverted &lt;- list(x = fpr_values, y = 1 - sens_values)\n\n# Visualizzazione della curva ROC\nplot(rocCurve, legacy.axes = TRUE, print.auc = TRUE)\n\n# Plot dei punti con colori appropriati\npoints(points_inverted$x, points_inverted$y, col = \"green\", pch = 19, cex = 0.8)\n\n# Aggiunta di etichette per soglie selezionate\nselected_thresholds &lt;- seq(0, 100, by = 20)\nfor (threshold in selected_thresholds) {\n  idx &lt;- which(thresholds == threshold)\n  if (length(idx) &gt; 0) {\n    text(\n      points_inverted$x[idx],\n      points_inverted$y[idx],\n      labels = threshold,\n      pos = 3,\n      cex = 0.7\n    )\n  }\n}\n\n\n\n\n\n\n\nLa curva ROC permette di valutare l’abilità di un modello di distinguere tra due gruppi (ad esempio, “Basso stress” e “Alto stress”). Ogni punto sulla curva ROC corrisponde a una soglia specifica utilizzata per classificare i partecipanti. Per calcolare questi punti, seguiamo i seguenti passaggi:\n1. Definizione delle Soglie. Le soglie sono valori specifici del punteggio continuo (stress_score) che separano i gruppi. Per ogni soglia:\n\nI partecipanti con punteggio ≥ soglia vengono classificati come “Alto stress”.\nI partecipanti con punteggio &lt; soglia vengono classificati come “Basso stress”.\n\n2. Sensibilità e Specificità. Per ogni soglia, calcoliamo due metriche fondamentali:\n\n\nSensibilità (True Positive Rate, TPR): proporzione di partecipanti del gruppo “Alto stress” classificati correttamente.\n\nSpecificità: proporzione di partecipanti del gruppo “Basso stress” classificati correttamente. La False Positive Rate (FPR) è complementare alla specificità: \\(\\text{FPR} = 1 - \\text{Specificità}\\).\n\n3. Logica della Funzione calc_metrics. La funzione prende una soglia e il dataset come input. I passaggi principali sono:\n\n\nClassificazione: per ogni partecipante, confrontiamo il punteggio con la soglia per assegnare la classe predetta.\n\nMatrice di Confusione: costruiamo una tabella che confronta le classificazioni predette con quelle osservate per calcolare i seguenti valori:\n\n\nTP (True Positives): numero di “Alto stress” predetti correttamente.\n\nFP (False Positives): numero di “Basso stress” classificati erroneamente come “Alto stress”.\n\nTN (True Negatives): numero di “Basso stress” classificati correttamente.\n\nFN (False Negatives): numero di “Alto stress” classificati erroneamente come “Basso stress”.\n\n\n\nMetriche: calcoliamo sensibilità, specificità e FPR.\n\n4. Calcolo e Visualizzazione dei Punti.\n\nPer ciascuna soglia nella sequenza definita (seq(0, 100, by = 5)), la funzione calcola le metriche.\nI valori di FPR (asse \\(x\\)) e sensibilità (asse \\(y\\)) vengono estratti per costruire i punti ROC.\nLa curva ROC viene tracciata utilizzando la funzione roc, mentre i punti calcolati manualmente vengono aggiunti al grafico per confrontarli.\n\nQuesta parte del codice calcola e traccia i punti ROC per ogni soglia:\n# Calcolo delle metriche per ogni soglia\nresults &lt;- lapply(thresholds, function(t) calc_metrics(t, data_stress))\n\n# Estrazione di FPR e sensibilità per il plotting\nfpr_values &lt;- sapply(results, function(x) x$fpr)\nsens_values &lt;- sapply(results, function(x) x$sensitivity)\n\n# Visualizzazione dei punti ROC sul grafico\npoints(fpr_values, 1 - sens_values, col = \"green\", pch = 19, cex = 0.8)\n\n# Aggiunta di etichette alle soglie selezionate\nselected_thresholds &lt;- seq(0, 100, by = 20)\nfor (threshold in selected_thresholds) {\n  idx &lt;- which(thresholds == threshold)\n  if (length(idx) &gt; 0) {\n    text(\n      fpr_values[idx], \n      1 - sens_values[idx], \n      labels = threshold, \n      pos = 3, \n      cex = 0.7\n    )\n  }\n}\n\n\nInversione di \\(y\\): per allineare i punti calcolati alla curva ROC, si usa \\(1 - \\text{Sensibilità}\\) sull’asse \\(y\\). Questo perché la curva ROC calcola direttamente la sensibilità, mentre i punti manuali la confrontano come \\(1 - \\text{Specificità}\\) sull’asse \\(x\\).\n\nPunti e Etichette: i punti calcolati vengono evidenziati in verde e annotati con i valori delle soglie.\n\nIn sintesi, il grafico finale mostra la curva ROC completa generata automaticamente e i punti calcolati manualmente. Le etichette sulle soglie forniscono informazioni su come il comportamento del modello varia al variare della soglia. Questo approccio permette di comprendere i concetti dietro la costruzione della curva ROC e come ogni soglia influenzi le metriche di classificazione.\nPer una rappresentazione più fluida, applichiamo una lisciatura:\n\n# Curva ROC lisciata\nplot(\n  roc(data_stress$group, data_stress$stress_score, smooth = TRUE),\n  legacy.axes = TRUE, print.auc = TRUE\n)\n\n\n\n\n\n\n\nNel nostro esempio, il valore di AUC ≈ 0.92 indica che il questionario sullo stress percepito ha un’ottima capacità di distinguere tra i due gruppi, essendo molto vicino al valore massimo di 1. Questo risultato evidenzia che il questionario possiede una combinazione di elevata sensibilità e specificità, rendendolo uno strumento efficace per discriminare tra persone con basso e alto stress percepito.\nIn sintesi, l’analisi dimostra che:\n\nil questionario è un valido strumento diagnostico;\n\nl’approccio ROC è utile per valutare l’efficacia di un test psicologico nella distinzione tra gruppi con caratteristiche diverse, come livelli di stress, depressione o ansia.\n\nQuesto esempio illustra l’applicazione pratica della curva ROC nel contesto psicologico, sottolineando il suo valore nell’analisi dell’accuratezza di strumenti diagnostici e questionari.",
    "crumbs": [
      "Predizione",
      "<span class='chapter-number'>83</span>  <span class='chapter-title'>Predizione</span>"
    ]
  },
  {
    "objectID": "chapters/prediction/01_prediction.html#riflessioni-conclusive",
    "href": "chapters/prediction/01_prediction.html#riflessioni-conclusive",
    "title": "83  Predizione",
    "section": "\n83.7 Riflessioni Conclusive",
    "text": "83.7 Riflessioni Conclusive\nL’analisi delle prestazioni dei modelli predittivi è un aspetto centrale in numerosi ambiti applicativi, dalla psicologia fino all’intelligenza artificiale. Questo capitolo ha esplorato strumenti e concetti fondamentali per valutare l’accuratezza e l’affidabilità di modelli di classificazione, sottolineando come ciascuno di essi possa contribuire a migliorare la qualità delle decisioni in condizioni di incertezza.\nUno dei temi chiave affrontati riguarda l’importanza di metriche come sensibilità, specificità, valore predittivo positivo (PPV) e valore predittivo negativo (NPV), che offrono una descrizione dettagliata delle prestazioni di un modello in relazione ai suoi errori (falsi positivi e falsi negativi). Queste metriche, derivate dalla matrice di confusione, non solo consentono di misurare l’accuratezza complessiva, ma permettono anche di adattare l’interpretazione del modello alle esigenze specifiche del contesto applicativo:\n\n\nSensibilità e specificità descrivono rispettivamente la capacità del modello di rilevare correttamente i positivi e di escludere correttamente i negativi.\n\nPPV e NPV forniscono informazioni sulla probabilità che una predizione del modello corrisponda alla realtà, tenendo conto della prevalenza delle classi (tasso di base).\n\nLa matrice di confusione rappresenta una base analitica essenziale, da cui derivare tassi marginali e indicatori chiave. Tuttavia, è stato evidenziato come l’accuratezza globale possa risultare fuorviante in presenza di tassi di base sbilanciati, poiché essa può essere dominata dalla classe più frequente. Per questo motivo, l’interpretazione delle prestazioni di un modello richiede sempre un’analisi approfondita, che includa sia gli errori commessi che i costi associati a tali errori.\nLa curva ROC (Receiver Operating Characteristic) e l’AUC (Area Under the Curve) costituiscono strumenti avanzati per analizzare le prestazioni dei modelli predittivi, offrendo un quadro completo della capacità discriminativa del modello al variare delle soglie decisionali. L’AUC, in particolare, sintetizza in un singolo valore la qualità del modello, rendendo possibile il confronto tra diversi approcci anche in presenza di tassi di base molto diversi. Tuttavia, è stato sottolineato come il valore aggregato dell’AUC debba essere interpretato con cautela, in quanto non fornisce informazioni dettagliate sulle prestazioni a soglie specifiche.\nUn altro tema affrontato riguarda il teorema di Bayes, applicato a contesti diagnostici per aggiornare la probabilità di una condizione sulla base di nuove evidenze. Questo approccio ha evidenziato l’importanza di integrare informazioni iniziali (priori) con nuove osservazioni (dati) per ridurre l’incertezza e migliorare la precisione delle stime. L’esempio del test diagnostico per l’HIV ha illustrato come l’aggiornamento progressivo delle probabilità consenta di ottenere stime più affidabili anche in contesti caratterizzati da bassi tassi di base e test diagnostici imperfetti.\nInfine, sono state discusse le implicazioni pratiche legate alla scelta delle soglie decisionali, al bilanciamento tra diversi tipi di errori e alla gestione delle distribuzioni sbilanciate. In particolare, è stato evidenziato come la selezione del cutoff ottimale debba essere guidata non solo dalle prestazioni globali del modello, ma anche dai costi associati agli errori di classificazione in un dato contesto.\nIn sintesi, questo capitolo ha fornito una panoramica delle principali metodologie per la valutazione delle predizioni, combinando strumenti tradizionali come la matrice di confusione e il teorema di Bayes con approcci avanzati come la curva ROC e l’AUC. Questi strumenti, utilizzati in modo complementare, consentono di affrontare con rigore e flessibilità le sfide della classificazione, guidando verso decisioni informate e ottimizzate in base alle esigenze specifiche di ciascun contesto applicativo.",
    "crumbs": [
      "Predizione",
      "<span class='chapter-number'>83</span>  <span class='chapter-title'>Predizione</span>"
    ]
  },
  {
    "objectID": "chapters/prediction/01_prediction.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/prediction/01_prediction.html#informazioni-sullambiente-di-sviluppo",
    "title": "83  Predizione",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.3.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] grid      stats     graphics  grDevices utils     datasets  methods  \n#&gt; [8] base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] tinytex_0.56            MOTE_1.0.2              ggrepel_0.9.6          \n#&gt;  [4] car_3.1-3               carData_3.0-5           msir_1.3.3             \n#&gt;  [7] ResourceSelection_0.3-6 rms_7.0-0               Hmisc_5.2-2            \n#&gt; [10] ROCR_1.0-11             pROC_1.18.5             magrittr_2.0.3         \n#&gt; [13] petersenlab_1.1.0       ggokabeito_0.1.0        see_0.10.0             \n#&gt; [16] MASS_7.3-65             viridis_0.6.5           viridisLite_0.4.2      \n#&gt; [19] ggpubr_0.6.0            ggExtra_0.10.1          gridExtra_2.3          \n#&gt; [22] patchwork_1.3.0         bayesplot_1.11.1        semTools_0.5-6         \n#&gt; [25] semPlot_1.1.6           lavaan_0.6-19           psych_2.4.12           \n#&gt; [28] scales_1.3.0            markdown_1.13           knitr_1.49             \n#&gt; [31] lubridate_1.9.4         forcats_1.0.0           stringr_1.5.1          \n#&gt; [34] dplyr_1.1.4             purrr_1.0.4             readr_2.1.5            \n#&gt; [37] tidyr_1.3.1             tibble_3.2.1            ggplot2_3.5.1          \n#&gt; [40] tidyverse_2.0.0         here_1.0.1             \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] splines_4.4.2       later_1.4.1         polspline_1.1.25   \n#&gt;   [4] XML_3.99-0.18       rpart_4.1.24        lifecycle_1.0.4    \n#&gt;   [7] Rdpack_2.6.2        rstatix_0.7.2       rprojroot_2.0.4    \n#&gt;  [10] lattice_0.22-6      rockchalk_1.8.157   backports_1.5.0    \n#&gt;  [13] openxlsx_4.2.8      rmarkdown_2.29      yaml_2.3.10        \n#&gt;  [16] httpuv_1.6.15       qgraph_1.9.8        zip_2.3.2          \n#&gt;  [19] pbapply_1.7-2       DBI_1.2.3           minqa_1.2.8        \n#&gt;  [22] RColorBrewer_1.1-3  multcomp_1.4-28     abind_1.4-8        \n#&gt;  [25] quadprog_1.5-8      nnet_7.3-20         TH.data_1.1-3      \n#&gt;  [28] sandwich_3.1-1      arm_1.14-4          MatrixModels_0.5-3 \n#&gt;  [31] codetools_0.2-20    tidyselect_1.2.1    farver_2.1.2       \n#&gt;  [34] lme4_1.1-36         stats4_4.4.2        base64enc_0.1-3    \n#&gt;  [37] jsonlite_1.9.0      Formula_1.2-5       survival_3.8-3     \n#&gt;  [40] emmeans_1.10.7      tools_4.4.2         Rcpp_1.0.14        \n#&gt;  [43] glue_1.8.0          mnormt_2.1.1        xfun_0.51          \n#&gt;  [46] mgcv_1.9-1          withr_3.0.2         fastmap_1.2.0      \n#&gt;  [49] mitools_2.4         boot_1.3-31         SparseM_1.84-2     \n#&gt;  [52] digest_0.6.37       mi_1.1              timechange_0.3.0   \n#&gt;  [55] R6_2.6.1            mime_0.12           estimability_1.5.1 \n#&gt;  [58] colorspace_2.1-1    mix_1.0-13          gtools_3.9.5       \n#&gt;  [61] jpeg_0.1-10         generics_0.1.3      data.table_1.17.0  \n#&gt;  [64] corpcor_1.6.10      htmlwidgets_1.6.4   pkgconfig_2.0.3    \n#&gt;  [67] sem_3.1-16          gtable_0.3.6        htmltools_0.5.8.1  \n#&gt;  [70] png_0.1-8           reformulas_0.4.0    rstudioapi_0.17.1  \n#&gt;  [73] tzdb_0.4.0          reshape2_1.4.4      coda_0.19-4.1      \n#&gt;  [76] checkmate_2.3.2     nlme_3.1-167        nloptr_2.1.1       \n#&gt;  [79] zoo_1.8-13          parallel_4.4.2      miniUI_0.1.1.1     \n#&gt;  [82] foreign_0.8-88      pillar_1.10.1       reshape_0.8.9      \n#&gt;  [85] vctrs_0.6.5         promises_1.3.2      OpenMx_2.21.13     \n#&gt;  [88] xtable_1.8-4        cluster_2.1.8       htmlTable_2.4.3    \n#&gt;  [91] evaluate_1.0.3      pbivnorm_0.6.0      ez_4.4-0           \n#&gt;  [94] mvtnorm_1.3-3       cli_3.6.4           kutils_1.73        \n#&gt;  [97] compiler_4.4.2      rlang_1.1.5         ggsignif_0.6.4     \n#&gt; [100] labeling_0.4.3      fdrtool_1.2.18      mclust_6.1.1       \n#&gt; [103] plyr_1.8.9          stringi_1.8.4       munsell_0.5.1      \n#&gt; [106] MBESS_4.9.3         lisrelToR_0.3       pacman_0.5.1       \n#&gt; [109] quantreg_6.00       Matrix_1.7-2        hms_1.1.3          \n#&gt; [112] glasso_1.11         shiny_1.10.0        rbibutils_2.3      \n#&gt; [115] igraph_2.1.4        broom_1.0.7         RcppParallel_5.1.10\n\n\n\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "Predizione",
      "<span class='chapter-number'>83</span>  <span class='chapter-title'>Predizione</span>"
    ]
  },
  {
    "objectID": "99-references.html",
    "href": "99-references.html",
    "title": "Bibliografia",
    "section": "",
    "text": "Allen, M. J., & Yen, W. M. (2001). Introduction to measurement\ntheory. Waveland Press.\n\n\nAlphen, T. van, Jak, S., Jansen in de Wal, J., Schuitema, J., &\nPeetsma, T. (2022). Determining reliability of daily measures: An\nillustration with data on teacher stress. Applied Measurement in\nEducation, 35(1), 63–79.\n\n\nAmerican Educational Research Association, American Psychological\nAssociation, & National Council on Measurement in Education. (2014).\nStandards for educational and psychological testing. American\nEducational Research Association.\n\n\nArias, A. (2024). A short tutorial on validation in educational and\npsychological assessment. Teaching Quantitative Methods\nVignettes, 20(3).\n\n\nBandalos, D. L. (2018). Measurement theory and applications for the\nsocial sciences. Guilford Publications.\n\n\nBarbeau, K., Boileau, K., Sarr, F., & Smith, K. (2019). Path\nanalysis in mplus: A tutorial using a conceptual model of psychological\nand behavioral antecedents of bulimic symptoms in young adults. The\nQuantitative Methods for Psychology, 15(1), 38–53.\n\n\nBarrett, P. (2007). Structural equation modelling: Adjudging model fit.\nPersonality and Individual Differences, 42(5),\n815–824.\n\n\nBates, D., Mächler, M., Bolker, B., & Walker, S. (2014). Fitting\nlinear mixed-effects models using lme4. arXiv Preprint\narXiv:1406.5823.\n\n\nBlampied, N. M. (2022). Reliable change and the reliable change index:\nStill useful after all these years? The Cognitive Behaviour\nTherapist, 15, e50.\n\n\nBlume, F., Buhr, L., Kuehnhausen, J., Köpke, R., Weber, L. A.,\nFallgatter, A. J., Ethofer, T., & Gawrilow, C. (2020). Validation of\nthe self-report version of the german strengths and weaknesses of ADHD\nsymptoms and normal behavior scale (SWAN-DE-SB). Assessment,\n10731911241236699.\n\n\nBollen, K., & Lennox, R. (1991). Conventional wisdom on measurement:\nA structural equation perspective. Psychological Bulletin,\n110(2), 305–314.\n\n\nBonifay, W., Winter, S. D., Skoblow, H. F., & Watts, A. L. (2024).\nGood fit is weak evidence of replication: Increasing rigor through prior\npredictive similarity checking. Assessment, 10731911241234118.\n\n\nBrown, A. (2023). Psychometrics in exercises using r and RStudio:\nTextbook and data resource. https://bookdown.org/annabrown/psychometricsR\n\n\nBrown, T. A. (2015). Confirmatory factor analysis for applied\nresearch. Guilford publications.\n\n\nBuchberger, E. S., Ngo, C. T., Peikert, A., Brandmaier, A. M., &\nWerkle-Bergner, M. (2024). Estimating statistical power for structural\nequation models in developmental cognitive science: A tutorial in r:\nPower simulation for SEMs. Behavior Research Methods, 1–18.\n\n\nByrne, B. M. (2013). Structural equation modeling with mplus: Basic\nconcepts, applications, and programming. routledge.\n\n\nCaudek, C., & Luccio, R. (2001). Statistica per psicologi\n(III rist. 2023, Vol. 11, p. 320). Laterza.\n\n\nCharter, R. A. (1996). Revisiting the standard errors of measurement,\nestimate, and prediction and their application to test scores.\nPerceptual and Motor Skills, 82(3), 1139–1144.\n\n\nChen, D.-G., & Yung, Y.-F. (2023). Structural equation modeling\nusing r/SAS: A step-by-step approach with real data analysis. CRC\nPress.\n\n\nClement, L. M., & Bradley-Garcia, M. (2022). A step-by-step tutorial\nfor performing a moderated mediation analysis using PROCESS. The\nQuantitative Methods for Psychology, 18(3), 258–271.\n\n\nDebelak, R., Strobl, C., & Zeigenfuse, M. D. (2022). An\nintroduction to the rasch model with examples in r. CRC Press.\n\n\nDudek, F. J. (1979). The continuing misinterpretation of the standard\nerror of measurement. Psychological Bulletin, 86(2),\n335--337.\n\n\nGrimm, K. J., Ram, N., & Estabrook, R. (2016). Growth modeling:\nStructural equation and multilevel modeling approaches. Guilford\nPublications.\n\n\nHargrave, T. D., & Hammer, M. Y. (2016). Restoration of\nrelationships after affairs. In Techniques for the couple\ntherapist (pp. 190–193). Routledge.\n\n\nHaslbeck, J., & Bork, R. van. (2022). Estimating the number of\nfactors in exploratory factor analysis via out-of-sample prediction\nerrors. Psychological Methods.\n\n\nHayduk, L. A. (2014). Shame for disrespecting evidence: The personal\nconsequences of insufficient respect for structural equation model\ntesting. BMC Medical Research Methodology, 14, 1–10.\n\n\nHu, L., & Bentler, P. M. (1998). Fit indices in covariance structure\nmodeling: Sensitivity to underparameterized model misspecification.\nPsychological Methods, 3(4), 424--453.\n\n\nJohn, O. P., & Benet-Martinez, V. (2014). Measurement: Reliability,\nconstruct validation, and scale construction. In H. T. Reis & C. M.\nJudd (Eds.), Handbook of research methods in social and personality\npsychology (2nd ed., pp. 473–503). Cambridge University Press.\n\n\nKan, K.-J., Maas, H. L. van der, & Levine, S. Z. (2019). Extending\npsychometric network analysis: Empirical evidence against g in favor of\nmutualism? Intelligence, 73, 52–62.\n\n\nKline, P. (2013). Handbook of psychological testing. Routledge.\n\n\nKline, R. B. (2023). Principles and practice of structural equation\nmodeling. Guilford publications.\n\n\nKomaroff, E. (1997). Effect of simultaneous violations of essential/g=\nt/-equivalence and uncorrelated error on coefficient/g= a. Applied\nPsychological Measurement, 21(4), 337–348.\n\n\nLittle, T. D. (2023). Longitudinal structural equation\nmodeling. Guilford Press.\n\n\nLord, F. M., & Novick, M. R. (1968). Statistical theories of\nmental test scores. Addison-Wesley.\n\n\nMarsh, H. W., Morin, A. J., Parker, P. D., & Kaur, G. (2014).\nExploratory structural equation modeling: An integration of the best\nfeatures of exploratory and confirmatory factor analysis. Annual\nReview of Clinical Psychology, 10(1), 85–110.\n\n\nMarsh, H., & Alamer, A. (2024). When and how to use set-exploratory\nstructural equation modelling to test structural models: A tutorial\nusing the r package lavaan. British Journal of Mathematical and\nStatistical Psychology.\n\n\nMauro, R. (1990). Understanding LOVE (left out\nvariables error): A method for estimating the effects of omitted\nvariables. Psychological Bulletin, 108(2),\n314–329.\n\n\nMcDonald, R. P. (2013). Test theory: A unified treatment.\nPsychology Press.\n\n\nNunnally, J. C. (1994). Psychometric theory. McGraw-Hill.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With\napplied examples in r. CRC Press.\n\n\nRandall, J. (2021). “Color-neutral” is not a thing:\nRedefining construct definition and representation through a\njustice-oriented critical antiracist lens. Educational Measurement:\nIssues and Practice, 40(4), 82–90.\n\n\nRandall, J., Slomp, D., Poe, M., & Oliveri, E. (2023). Disrupting\nwhite supremacy in assessment: Toward a justice-oriented, antiracist\nvalidity framework. In Twin pandemics (pp. 78–86). Routledge.\n\n\nRencher, A. (2002). Methods of multivariate analysis. 2002.\nWiley Publications.\n\n\nReynolds, C. R., & Livingston, R. (2021). Mastering modern\npsychological testing. Springer.\n\n\nRoberts, S., & Pashler, H. (2000). How persuasive is a good fit? A\ncomment on theory testing. Psychological Review,\n107(2), 358–367.\n\n\nRosseel, Y. (2020). Small sample solutions for structural equation\nmodeling. In Small sample size solutions: A guide for applied\nresearchers and practitioners (pp. 226–238). Routledge.\n\n\nSaqr, M., & López-Pernas, S. (2024). Learning analytics methods\nand tutorials: A practical guide using r. Springer Nature.\n\n\nSpearman, C. (1904). General intelligence objectively determined and\nmeasured. American Journal of Psychology, 15, 201–293.\n\n\nSvetina, D., Rutkowski, L., & Rutkowski, D. (2020). Multiple-group\ninvariance with categorical outcomes using updated guidelines: An\nillustration using m plus and the lavaan/semtools packages.\nStructural Equation Modeling: A Multidisciplinary Journal,\n27(1), 111–130.\n\n\nTimmerman, M. E., Voncken, L., & Albers, C. J. (2021). A tutorial on\nregression-based norming of psychological tests with GAMLSS.\nPsychological Methods, 26(3), 357–373.\n\n\nWaller, N. G., & Meehl, P. E. (2002). Risky tests, verisimilitude,\nand path analysis. Psychological Methods, 7(3),\n323–337. https://doi.org/10.1037/1082-989X.7.3.323\n\n\nWechsler, D. (2008). Wechsler adult intelligence scale–fourth edition\n(WAIS–IV). San Antonio, TX: NCS Pearson, 22(498),\n816–827.\n\n\nWind, S. A. (2017). An instructional module on mokken scale analysis.\nEducational Measurement: Issues and Practice, 36(2),\n50–66.\n\n\nWind, S. A. (2024). Item-explanatory mokken scale analysis: Using\nnonparametric item response theory to explore item attributes. The\nJournal of Experimental Education, 1–21.\n\n\nWu, H., & Estabrook, R. (2016). Identification of confirmatory\nfactor analysis models of different levels of invariance for ordered\ncategorical outcomes. Psychometrika, 81(4), 1014–1045.\n\n\nXia, Y., & Yang, Y. (2019). RMSEA, CFI, and TLI in structural\nequation modeling with ordered categorical data: The story they tell\ndepends on the estimation methods. Behavior Research Methods,\n51(1), 409–428.",
    "crumbs": [
      "Bibliografia"
    ]
  },
  {
    "objectID": "chapters/appendix/a1_intro_r.html",
    "href": "chapters/appendix/a1_intro_r.html",
    "title": "Appendice A — Linguaggio R",
    "section": "",
    "text": "\\(\\mathsf{R}\\) è un linguaggio di programmazione per l’analisi dei dati, il calcolo e la visualizzazione grafica. È open source ed estensibile, il che significa che il codice sorgente è disponibile per essere esaminato e riutilizzato. Può essere scaricato gratuitamente dal sito web del Comprehensive R Archive Network (CRAN) ed è disponibile per PC, MacOS e sistemi operativi Linux/Unix. Gran parte del core-R è scritto in Fortran o C++, ma molti pacchetti per \\(\\mathsf{R}\\) sono scritti in \\(\\mathsf{R}\\) stesso. Chiunque può aggiungere pacchetti al CRAN o ad altri repository come GitHub o BioConductor. CRAN ha test di garanzia della qualità per garantire che i programmi contribuiti abbiano una documentazione coerente e non falliscano durante l’esecuzione degli esempi forniti. Al momento ci sono migliaia di pacchetti disponibili per \\(\\mathsf{R}\\) e questo numero aumenta quotidianamente.\nPer programmare in \\(\\mathsf{R}\\), è importante seguire le regole sintattiche del linguaggio. Se una riga di codice non è scritta correttamente, l’interprete di \\(\\mathsf{R}\\) segnalerà un errore. Questo può essere difficile per i principianti, ma ci sono due vantaggi nell’atto di scrivere codice.\n\nPrima di poter risolvere un problema con il codice, è necessario comprenderlo e analizzarlo in modo preciso. Questo aiuta a sviluppare una comprensione più profonda del problema e a identificare soluzioni efficaci.\nInoltre, se un programma non funziona correttamente, il programmatore che lo ha scritto è l’unico responsabile. Questo aiuta i programmatori a sviluppare una maggiore autoconsapevolezza e responsabilità nei confronti del proprio lavoro.\n\nInoltre, su Internet è disponibile una vasta gamma di materiali utili per avvicinarsi all’ambiente \\(\\mathsf{R}\\) e aiutare l’utente nell’apprendimento di questo software statistico. Tra le tante introduzioni al linguaggio \\(\\mathsf{R}\\), si veda ad esempio, Introduction to R di Venables, Smith, and the R development core team (2023).",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Linguaggio R</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a2_sums.html",
    "href": "chapters/appendix/a2_sums.html",
    "title": "Appendice B — Simbolo di somma",
    "section": "",
    "text": "B.1 Manipolazione di somme\nLe somme si incontrano costantemente in svariati contesti matematici e statistici quindi abbiamo bisogno di una notazione adeguata che ci consenta di gestirle. La somma dei primi \\(n\\) numeri interi può essere scritta come \\(1+2+\\dots+(n-1)+n\\), dove `\\(\\dots\\)’ ci dice di completare la sequenza definita dai termini che vengono prima e dopo. Ovviamente, una notazione come \\(1+7+\\dots+73.6\\) non avrebbe alcun senso senza qualche altro tipo di precisazione. In generale, nel seguito incontreremo delle somme nella forma\n\\[\nx_1+x_2+\\dots+x_n,\n\\]\ndove \\(x_n\\) è un numero che è stato definito altrove. La notazione precedente, che fa uso dei tre puntini di sospensione, è utile in alcuni contesti ma in altri risulta ambigua. Pertanto la notazione di uso corrente è del tipo\n\\[\n\\sum_{i=1}^n x_i\n\\] e si legge “sommatoria per \\(i\\) che va da \\(1\\) a \\(n\\) di \\(x_i\\)”. Il simbolo \\(\\sum\\) (lettera sigma maiuscola dell’alfabeto greco) indica l’operazione di somma, il simbolo \\(x_i\\) indica il generico addendo della sommatoria, le lettere \\(1\\) ed \\(n\\) indicano i cosiddetti estremi della sommatoria, ovvero l’intervallo (da \\(1\\) fino a \\(n\\) estremi inclusi) in cui deve variare l’indice \\(i\\) allorché si sommano gli addendi \\(x_i\\). Solitamente l’estremo inferiore è \\(1\\) ma potrebbe essere qualsiasi altri numero \\(m &lt; n\\). Quindi\n\\[\n\\sum_{i=1}^n x_i = x_1 + x_{2} + \\dots + x_{n}.\n\\]\nPer esempio, se i valori \\(x\\) sono \\(\\{3, 11, 4, 7\\}\\), si avrà\n\\[\n\\sum_{i=1}^4 x_i = 3+11+4+7 = 25\n\\]\nladdove \\(x_1 = 3\\), \\(x_2 = 11\\), eccetera. La quantità \\(x_i\\) nella formula precedente si dice l’argomento della sommatoria, mentre la variabile \\(i\\), che prende i valori naturali successivi indicati nel simbolo, si dice indice della sommatoria.\nLa notazione di sommatoria può anche essere fornita nella forma seguente\n\\[\n\\sum_{P(i)} x_i\n\\]\ndove \\(P(i)\\) è qualsiasi proposizione riguardante \\(i\\) che può essere vera o falsa. Quando è ovvio che si vogliono sommare tutti i valori di \\(n\\) osservazioni, la notazione può essere semplificata nel modo seguente: \\(\\sum_{i} x_i\\) oppure \\(\\sum x_i\\). Al posto di \\(i\\) si possono trovare altre lettere: \\(k, j, l, \\dots\\),.\nÈ conveniente utilizzare le seguenti regole per semplificare i calcoli che coinvolgono l’operatore della sommatoria.",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Simbolo di somma</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a2_sums.html#manipolazione-di-somme",
    "href": "chapters/appendix/a2_sums.html#manipolazione-di-somme",
    "title": "Appendice B — Simbolo di somma",
    "section": "",
    "text": "B.1.1 Proprietà 1\nLa sommatoria di \\(n\\) valori tutti pari alla stessa costante \\(a\\) è pari a \\(n\\) volte la costante stessa:\n\\[\n\\sum_{i=1}^{n} a = \\underbrace{a + a + \\dots + a} = {n\\text{ volte } a} = n a.\n\\]\n\n\nB.1.2 Proprietà 2 (proprietà distributiva)\nNel caso in cui l’argomento contenga una costante, è possibile riscrivere la sommatoria. Ad esempio con\n\\[\n\\sum_{i=1}^{n} a x_i = a x_1 + a x_2 + \\dots + a x_n\n\\]\nè possibile raccogliere la costante \\(a\\) e fare \\(a(x_1 +x_2 + \\dots + x_n)\\). Quindi possiamo scrivere\n\\[\n\\sum_{i=1}^{n} a x_i = a \\sum_{i=1}^{n} x_i.\n\\]\n\n\nB.1.3 Proprietà 3 (proprietà associativa)\nNel caso in cui\n\\[\n\\sum_{i=1}^{n} (a + x_i) = (a + x_1) + (a + x_1) + \\dots  (a + x_n)\n\\]\nsi ha che\n\\[\n\\sum_{i=1}^{n} (a + x_i) = n a + \\sum_{i=1}^{n} x_i.\n\\]\nÈ dunque chiaro che in generale possiamo scrivere\n\\[\n\\sum_{i=1}^{n} (x_i + y_i) = \\sum_{i=1}^{n} x_i + \\sum_{i=1}^{n} y_i.\n\\]\n\n\nB.1.4 Proprietà 4\nSe deve essere eseguita un’operazione algebrica (innalzamento a potenza, logaritmo, ecc.) sull’argomento della sommatoria, allora tale operazione algebrica deve essere eseguita prima della somma. Per esempio,\n\\[\n\\sum_{i=1}^{n} x_i^2 = x_1^2 + x_2^2 + \\dots + x_n^2 \\neq \\left(\\sum_{i=1}^{n} x_i \\right)^2.\n\\]\n\n\nB.1.5 Proprietà 5\nNel caso si voglia calcolare \\(\\sum_{i=1}^{n} x_i y_i\\), il prodotto tra i punteggi appaiati deve essere eseguito prima e la somma dopo:\n\\[\n\\sum_{i=1}^{n} x_i y_i = x_1 y_1 + x_2 y_2 + \\dots + x_n y_n,\n\\]\ninfatti, \\(a_1 b_1 + a_2 b_2 \\neq (a_1 + a_2)(b_1 + b_2)\\).",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Simbolo di somma</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a2_sums.html#doppia-sommatoria",
    "href": "chapters/appendix/a2_sums.html#doppia-sommatoria",
    "title": "Appendice B — Simbolo di somma",
    "section": "B.2 Doppia sommatoria",
    "text": "B.2 Doppia sommatoria\nÈ possibile incontrare la seguente espressione in cui figurano una doppia sommatoria e un doppio indice:\n\\[\n\\sum_{i=1}^{n}\\sum_{j=1}^{m} x_{ij}.\n\\]\nLa doppia sommatoria comporta che per ogni valore dell’indice esterno, \\(i\\) da \\(1\\) ad \\(n\\), occorre sviluppare la seconda sommatoria per \\(j\\) da \\(1\\) ad \\(m\\). Quindi,\n\\[\n\\sum_{i=1}^{3}\\sum_{j=4}^{6} x_{ij} = (x_{1, 4} + x_{1, 5} + x_{1, 6}) + (x_{2, 4} + x_{2, 5} + x_{2, 6}) + (x_{3, 4} + x_{3, 5} + x_{3, 6}).\n\\]\nUn caso particolare interessante di doppia sommatoria è il seguente:\n\\[\n\\sum_{i=1}^{n}\\sum_{j=1}^{n} x_i y_j\n\\]\nSi può osservare che nella sommatoria interna (quella che dipende dall’indice \\(j\\)), la quantità \\(x_i\\) è costante, ovvero non dipende dall’indice (che è \\(j\\)). Allora possiamo estrarre \\(x_i\\) dall’operatore di sommatoria interna e scrivere\n\\[\n\\sum_{i=1}^{n} \\left( x_i \\sum_{j=1}^{n} y_j \\right).\n\\]\nAllo stesso modo si può osservare che nell’argomento della sommatoria esterna la quantità costituita dalla sommatoria in \\(j\\) non dipende dall’indice \\(i\\) e quindi questa quantità può essere estratta dalla sommatoria esterna. Si ottiene quindi\n\\[\n\\sum_{i=1}^{n}\\sum_{j=1}^{n} x_i y_j = \\sum_{i=1}^{n} \\left( x_i \\sum_{j=1}^{n} y_j \\right) = \\sum_{i=1}^{n} x_i \\sum_{j=1}^{n} y_j.\n\\]\nFacciamo un esercizio. Verifichiamo quanto detto sopra nel caso particolare di \\(x = \\{2, 3, 1\\}\\) e \\(y = \\{1, 4, 9\\}\\), svolgendo prima la doppia sommatoria per poi verificare che quanto così ottenuto sia uguale al prodotto delle due sommatorie.\n\\[\n\\begin{align}\n\\sum_{i=1}^3 \\sum_{j=1}^3 x_i y_j &= x_1y_1 + x_1y_2 + x_1y_3 +\nx_2y_1 + x_2y_2 + x_2y_3 +\nx_3y_1 + x_3y_2 + x_3y_3 \\notag\\\\\n&= 2 \\times (1+4+9) + 3 \\times (1+4+9) + 2 \\times (1+4+9) = 84,\\notag\n\\end{align}\n\\]\novvero\n\\[\n(2 + 3 + 1) \\times (1+4+9) = 84.\n\\]",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Simbolo di somma</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a3_calculus.html",
    "href": "chapters/appendix/a3_calculus.html",
    "title": "Appendice C — Per liberarvi dai terrori preliminari",
    "section": "",
    "text": "C.1 Introduzione ai logaritmi\nFornisco qui la traduzione del primo capitolo di Calculus made easy.\nIl terrore preliminare, che impedisce alla maggior parte dei ragazzi di quinta anche solo di tentare di imparare l’analisi, può essere abolito una volta per tutte semplicemente affermando qual è il significato – in termini di buon senso – dei due simboli principali che sono usati nell’analisi matematica.\nQuesti terribili simboli sono:\nÈ tutto.\nIl logaritmo è una funzione matematica che risponde alla domanda: “quante volte devo moltiplicare un certo numero (chiamato”base”) per ottenere un altro numero?” Matematicamente, questo è espresso come:\n\\[\n\\log_b(a) = x \\iff b^x = a\n\\]\nAd esempio, \\(\\log_2(8) = 3\\) perché \\(2^3 = 8\\).\nNel contesto dei logaritmi, i valori molto piccoli (compresi tra 0 e 1) diventano più grandi (in termini assoluti) e negativi quando applichiamo una funzione logaritmica. Questo è utile per stabilizzare i calcoli, specialmente quando lavoriamo con prodotti di numeri molto piccoli che potrebbero portare a problemi di underflow.\nPer esempio: - \\(\\log(1) = 0\\) - \\(\\log(0.1) = -1\\) - \\(\\log(0.01) = -2\\) - \\(\\log(0.001) = -3\\)\nCome si può vedere, i valori assoluti dei logaritmi crescono man mano che il numero originale si avvicina a zero.\nUna delle proprietà più utili dei logaritmi è che consentono di trasformare un prodotto in una somma:\n\\[\n\\log_b(a \\times c) = \\log_b(a) + \\log_b(c)\n\\]\nQuesta proprietà è estremamente utile in calcoli complessi, come nella statistica bayesiana, dove il prodotto di molte probabilità potrebbe diventare un numero molto piccolo e causare problemi numerici.\nUn’altra proprietà utile dei logaritmi è che un rapporto tra due numeri diventa la differenza dei loro logaritmi:\n\\[\n\\log_b\\left(\\frac{a}{c}\\right) = \\log_b(a) - \\log_b(c)\n\\]\nAnche questa proprietà è molto utilizzata in matematica, specialmente in situazioni in cui è necessario normalizzare i dati.\nIn sintesi, i logaritmi sono strumenti potenti per semplificare e stabilizzare i calcoli matematici. Essi consentono di lavorare più agevolmente con numeri molto grandi o molto piccoli e di trasformare operazioni complesse come prodotti e divisioni in somme e differenze, rendendo i calcoli più gestibili e meno inclini a errori numerici.",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Per liberarvi dai terrori preliminari</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/solutions_probability.html",
    "href": "chapters/appendix/solutions_probability.html",
    "title": "Appendice D — Probabilità",
    "section": "",
    "text": "# Standard library imports\nimport os\n\n# Third-party imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport arviz as az\nimport scipy.stats as stats\nfrom scipy.special import expit  # Funzione logistica\nimport math\nfrom cmdstanpy import cmdstan_path, CmdStanModel\n\n# Configuration\nseed = sum(map(ord, \"stan_poisson_regression\"))\nrng = np.random.default_rng(seed=seed)\naz.style.use(\"arviz-darkgrid\")\n%config InlineBackend.figure_format = \"retina\"\n\n# Define directories\nhome_directory = os.path.expanduser(\"~\")\nproject_directory = f\"{home_directory}/_repositories/psicometria\"\n\n# Print project directory to verify\nprint(f\"Project directory: {project_directory}\")\n\nProject directory: /Users/corradocaudek/_repositories/psicometria\n\n\n\n?sec-prob-on-general-spaces\n?exr-prob-on-general-spaces-1\nPer calcolare questa probabilità in maniera analitica, utilizziamo la seguente uguaglianza:\n\\[\nP(\\text{almeno 2 psicologi clinici}) = 1 - P(\\text{nessun psicologo clinico}) - P(\\text{1 psicologo clinico}).\n\\]\nIl numero totale di modi per selezionare 5 persone dal gruppo di 20 è dato da:\n\\[\n\\binom{20}{5} = \\frac{20!}{5!(15!)} = 15,504.\n\\]\nIl numero di modi per avere nessun psicologo clinico nella commissione (ovvero, selezionare solo psicologi del lavoro) è:\n\\[\n\\binom{10}{0} \\times \\binom{10}{5} = 1 \\times 252 = 252.\n\\]\nQuindi, la probabilità di avere nessun psicologo clinico è:\n\\[\nP(\\text{nessun psicologo clinico}) = \\frac{252}{15,504} \\approx 0.016.\n\\]\nIl numero di modi per avere esattamente 1 psicologo clinico nella commissione è:\n\\[\n\\binom{10}{1} \\times \\binom{10}{4} = 10 \\times 210 = 2,100.\n\\]\nQuindi, la probabilità di avere esattamente 1 psicologo clinico è:\n\\[\nP(\\text{1 psicologo clinico}) = \\frac{2,100}{15,504} \\approx 0.135.\n\\]\nLa probabilità di avere almeno 2 psicologi clinici nella commissione è quindi:\n\\[\n\\begin{align}\nP(\\text{almeno 2 psicologi clinici}) &= 1 - P(\\text{nessun psicologo clinico}) - P(\\text{1 psicologo clinico}) \\notag\\\\\n&= 1 - 0.016 - 0.135 \\notag\\\\\n&= 0.848.\\notag\n\\end{align}\n\\]\nQuindi, la probabilità che almeno 2 psicologi clinici siano nella commissione è circa 0.848.\n\n# Funzione per calcolare le combinazioni\ndef nCk(n, k):\n    return math.factorial(n) // (math.factorial(k) * math.factorial(n - k))\n\n\n# Calcolo delle probabilità per il problema della commissione\ntotal_ways = nCk(20, 5)\nno_clinical = nCk(10, 0) * nCk(10, 5)\none_clinical = nCk(10, 1) * nCk(10, 4)\n\np_no_clinical = no_clinical / total_ways\np_one_clinical = one_clinical / total_ways\n\np_at_least_two_clinical = 1 - p_no_clinical - p_one_clinical\n\nprint(f\"Probabilità di almeno 2 psicologi clinici: {p_at_least_two_clinical:.3f}\")\n\nProbabilità di almeno 2 psicologi clinici: 0.848\n\n\nIn maniera più intuitiva, possiamo risolvere il problema con una simulazione Monte Carlo.\n\nimport random\n\n# Numero di simulazioni\nsimulations = 1000000\n\n# Numero di successi (almeno 2 psicologi clinici nella commissione)\nsuccess_count = 0\n\n# Creiamo una lista che rappresenta il gruppo di 20 persone\n# 1 rappresenta un psicologo clinico, 0 rappresenta un psicologo del lavoro\ngroup = [1] * 10 + [0] * 10\n\n# Simulazione Monte Carlo\nfor _ in range(simulations):\n    # Estrai casualmente 5 persone dal gruppo\n    committee = random.sample(group, 5)\n\n    # Conta quanti psicologi clinici ci sono nella commissione\n    num_clinical_psychologists = sum(committee)\n\n    # Verifica se ci sono almeno 2 psicologi clinici\n    if num_clinical_psychologists &gt;= 2:\n        success_count += 1\n\n# Calcola la probabilità\nprobability = success_count / simulations\n\n# Mostra il risultato\nprint(\n    f\"La probabilità che almeno 2 psicologi clinici siano nella commissione è: {probability:.4f}\"\n)\n\nLa probabilità che almeno 2 psicologi clinici siano nella commissione è: 0.8482\n\n\n\n\n?sec-simulations\n?exr-prob-simulation-1\nPer calcolare le deviazioni standard delle distribuzioni gaussiane date le percentuali di studenti che ottengono meno di 18, possiamo utilizzare le proprietà della distribuzione normale e i quantili della distribuzione normale standard (distribuzione normale con media 0 e deviazione standard 1).\nLe distribuzioni normali hanno la proprietà che possiamo trasformare qualsiasi valore \\(X\\) della distribuzione \\(N(\\mu, \\sigma)\\) nella distribuzione normale standard \\(N(0, 1)\\) tramite la formula:\n\\[ Z = \\frac{X - \\mu}{\\sigma}, \\]\ndove \\(Z\\) è il quantile standardizzato.\nPer trovare il valore di \\(\\sigma\\) dato un certo percentile, utilizziamo l’inverso della funzione di distribuzione cumulativa (CDF) della distribuzione normale standard. Per un dato percentile \\(p\\), \\(z_p\\) è tale che:\n\\[ p = P(Z \\leq z_p) \\]\nQuindi possiamo trovare \\(\\sigma\\) risolvendo per \\(\\sigma\\) nella formula:\n\\[ z_p = \\frac{X - \\mu}{\\sigma}, \\]\n\\[ \\sigma = \\frac{X - \\mu}{z_p}, \\]\ndove:\n\n\\(X\\) è il punteggio di soglia (18 in questo caso).\n\\(\\mu\\) è la media della distribuzione.\n\\(z_p\\) è il quantile della distribuzione normale standard per il percentile \\(p\\).\n\nI quantili della distribuzione normale standard per i percentili desiderati sono:\n\nPer il 15%, il quantile è \\(z_{0.15} \\approx -1.036\\).\nPer il 10%, il quantile è \\(z_{0.10} \\approx -1.281\\).\nPer il 5%, il quantile è \\(z_{0.05} \\approx -1.645\\).\n\nPrima Prova\n\nMedia: \\(\\mu = 24\\)\nPercentuale che ottiene meno di 18: 15%\nQuantile: \\(z_{0.15} = -1.036\\)\nSoglia: \\(X = 18\\)\n\n\\[ \\sigma_1 = \\frac{24 - 18}{1.036} \\approx 5.79 \\]\nSeconda Prova\n\nMedia: \\(\\mu = 25\\)\nPercentuale che ottiene meno di 18: 10%\nQuantile: \\(z_{0.10} = -1.281\\)\nSoglia: \\(X = 18\\)\n\n\\[ \\sigma_2 = \\frac{25 - 18}{1.281} \\approx 5.46 \\]\nTerza Prova\n\nMedia: \\(\\mu = 26\\)\nPercentuale che ottiene meno di 18: 5%\nQuantile: \\(z_{0.05} = -1.645\\)\nSoglia: \\(X = 18\\)\n\n\\[ \\sigma_3 = \\frac{26 - 18}{1.645} \\approx 4.86 \\]\n\n# Funzione per calcolare la deviazione standard data la media, la soglia e il quantile\ndef calculate_std(mean, threshold, quantile):\n    return abs((mean - threshold) / quantile)\n\n\n# Parametri delle distribuzioni gaussiane per le tre prove\nmean_test1 = 24\nstd_test1 = calculate_std(mean_test1, 18, -1.036)\nmean_test2 = 25\nstd_test2 = calculate_std(mean_test2, 18, -1.281)\nmean_test3 = 26\nstd_test3 = calculate_std(mean_test3, 18, -1.645)\n\n# Numero di studenti\nn_students = 220\n\n# Percentuale di studenti che non fa le prove\ndrop_test1 = 0.10\ndrop_test2 = 0.05\n\n# Seed per il generatore di numeri casuali basato sulla stringa \"simulation\"\nseed = sum(map(ord, \"simulation\"))\nrng = np.random.default_rng(seed=seed)\n\n# Generazione dei voti per le tre prove\n# Genera i voti solo per gli studenti che partecipano alla prova\ntest1_scores = np.where(\n    rng.random(n_students) &gt; drop_test1,\n    rng.normal(mean_test1, std_test1, n_students),\n    np.nan,\n)\ntest2_scores = np.where(\n    rng.random(n_students) &gt; drop_test2,\n    rng.normal(mean_test2, std_test2, n_students),\n    np.nan,\n)\ntest3_scores = rng.normal(mean_test3, std_test3, n_students)\n\n# Calcola il voto finale solo per gli studenti che hanno partecipato a tutte e tre le prove\nfinal_scores = np.nanmean(\n    np.column_stack((test1_scores, test2_scores, test3_scores)), axis=1\n)\n\n# Filtra gli studenti che non hanno partecipato a tutte e tre le prove\nvalid_final_scores = final_scores[~np.isnan(final_scores)]\n\n# Visualizzazione della distribuzione finale dei voti\nplt.hist(valid_final_scores, bins=30, edgecolor=\"black\")\nplt.title(\"Distribuzione dei voti finali\")\nplt.xlabel(\"Voto finale\")\nplt.ylabel(\"Frequenza\")\nplt.show()\n\n# Statistiche descrittive dei voti finali\nmean_final_score = np.mean(valid_final_scores)\nmedian_final_score = np.median(valid_final_scores)\nstd_final_score = np.std(valid_final_scores)\n\nprint(f\"Media dei voti finali: {mean_final_score:.2f}\")\nprint(f\"Mediana dei voti finali: {median_final_score:.2f}\")\nprint(f\"Deviazione standard dei voti finali: {std_final_score:.2f}\")",
    "crumbs": [
      "Appendici",
      "Soluzioni degli esercizi",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Probabilità</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E3_thurstone.html#esempio",
    "href": "chapters/measurement/E3_thurstone.html#esempio",
    "title": "\n4  ✏ Esercizi\n",
    "section": "\n4.2 Esempio",
    "text": "4.2 Esempio\nImmaginiamo di avere tre oggetti – A, B e C – per cui abbiamo raccolto preferenze pairwise (cioè a coppie). Ipotizziamo di aver rilevato le seguenti percentuali di preferenza:\n\n\nA è preferito a B dal 70% dei rispondenti.\n\nA è preferito a C dall’80% dei rispondenti.\n\nB è preferito a C dal 60% dei rispondenti.\n\nQuindi, se chiediamo ad un gruppo di persone di scegliere tra A e B, il 70% sceglie A, mentre se chiediamo di scegliere tra A e C, l’80% sceglie A; tra B e C, il 60% sceglie B.\n\n4.2.1 Creazione del data frame delle preferenze\nIn R, possiamo creare un piccolo data frame (chiamato preferences) che contiene queste informazioni. Ciascuna riga riporta:\n\nLa prima colonna (obj1): l’oggetto preferito dalla proporzione p.\nLa seconda colonna (obj2): l’altro oggetto del confronto.\nLa terza colonna (p): la proporzione di rispondenti che preferisce obj1 rispetto a obj2.\n\n\n# Definiamo le preferenze pairwise\npreferences &lt;- data.frame(\n  obj1 = c(\"A\", \"A\", \"B\"),\n  obj2 = c(\"B\", \"C\", \"C\"),\n  p    = c(0.70, 0.80, 0.60)\n)\npreferences\n#&gt;   obj1 obj2   p\n#&gt; 1    A    B 0.7\n#&gt; 2    A    C 0.8\n#&gt; 3    B    C 0.6\n\n\nPrima riga: “A vs. B, p=0.70” significa che il 70% preferisce A rispetto a B.\nSeconda riga: “A vs. C, p=0.80” significa che l’80% preferisce A rispetto a C.\nTerza riga: “B vs. C, p=0.60” significa che il 60% preferisce B rispetto a C.\n\n4.2.2 Conversione delle percentuali in z-score (qnorm)\nIl modello di Thurstone (caso V) prevede che queste proporzioni siano interpretabili come aree cumulative di una distribuzione normale standard. Se ad esempio il 70% preferisce A a B, questo 0.70 è la probabilità cumulata (fino a uno z-score positivo). Per convertire una proporzione in uno z-score, utilizziamo la funzione qnorm().\n\n# Aggiungiamo al data frame la colonna z, che contiene la trasformazione delle \n# proporzioni in z-score\npreferences &lt;- preferences %&gt;%\n  mutate(z = qnorm(p))\n\npreferences\n#&gt;   obj1 obj2   p     z\n#&gt; 1    A    B 0.7 0.524\n#&gt; 2    A    C 0.8 0.842\n#&gt; 3    B    C 0.6 0.253\n\n\n4.2.3 Creazione di una matrice delle preferenze (z-score matrix)\nOra vogliamo creare una matrice (che chiameremo z_matrix) contenente gli z-score di tutte le coppie (A-B, A-C, B-C, e anche B-A, C-A, C-B, che saranno i corrispettivi negativi). In altre parole:\n\nNelle celle della matrice in cui la riga è obj1 e la colonna è obj2, mettiamo lo z-score positivo (es. 0.52 se il 70% preferisce A a B).\nNella cella opposta (dove la riga è obj2 e la colonna è obj1), mettiamo lo z-score negativo (es. -0.52, perché se 70% preferisce A a B, allora il 30% preferisce B a A).\n\nCosì otteniamo un quadro simmetrico del grado di preferenza fra tutti gli oggetti.\n\n# Ricaviamo l’elenco degli oggetti presenti\nobjects &lt;- unique(c(preferences$obj1, preferences$obj2))\n\n# Creiamo una matrice quadrata vuota in cui inserire gli z-score\nz_matrix &lt;- matrix(0, \n                   nrow = length(objects), \n                   ncol = length(objects),\n                   dimnames = list(objects, objects))\n\n# Compiliamo la matrice: \n# per ogni riga del data frame `preferences`,\n# inseriamo lo z-score positivo nella posizione [obj1, obj2] e\n# quello negativo nella posizione [obj2, obj1].\nfor (i in 1:nrow(preferences)) {\n  riga    &lt;- preferences$obj1[i]\n  colonna &lt;- preferences$obj2[i]\n  valore  &lt;- preferences$z[i]\n  \n  z_matrix[riga, colonna] &lt;- valore\n  z_matrix[colonna, riga] &lt;- -valore\n}\n\nz_matrix\n#&gt;        A      B     C\n#&gt; A  0.000  0.524 0.842\n#&gt; B -0.524  0.000 0.253\n#&gt; C -0.842 -0.253 0.000\n\n\nLa diagonale è 0 (ovviamente un oggetto confrontato con sé stesso non dà informazioni).\nL’elemento [A, B] è \\(+0.524\\) (z-score corrispondente a 70%), mentre l’elemento [B, A] è \\(-0.524\\).\n\n4.2.4 Calcolo dei punteggi latenti medi\nSecondo il modello di Thurstone, uno dei modi più semplici per posizionare ogni oggetto sulla scala latente è calcolare, per ogni riga, la media degli z-score rispetto agli altri oggetti. L’idea è che questa media costituisca una stima della “posizione” dell’oggetto sulla scala latente di preferenza.\n\n# Calcoliamo la media degli z-score per ogni riga (oggetto)\nlatent_scores &lt;- rowMeans(z_matrix)\nlatent_scores\n#&gt;       A       B       C \n#&gt;  0.4553 -0.0904 -0.3650\n\nPer esempio:\n\nc(0.000,  0.524, 0.842) |&gt; mean()\n#&gt; [1] 0.455\n\nSe latent_scores[\"A\"] è più alto degli altri, significa che A si colloca in media “più a destra” (o più in alto) sulla scala latente rispetto a B e C (cioè, è mediamente più preferito).\n\n4.2.5 Creazione del ranking finale\nInfine, ordiniamo gli oggetti in base a questi punteggi latenti. Il risultato è un data frame con:\n\nIl nome dell’oggetto.\nIl suo punteggio latente medio.\nL’ordine di ranking (da più alto a più basso).\n\n\nranking &lt;- data.frame(\n  Oggetto          = names(latent_scores),\n  Punteggio_Latente = latent_scores\n) %&gt;%\n  arrange(desc(Punteggio_Latente)) %&gt;%\n  mutate(Ranking = row_number())\n\nranking\n#&gt;   Oggetto Punteggio_Latente Ranking\n#&gt; A       A            0.4553       1\n#&gt; B       B           -0.0904       2\n#&gt; C       C           -0.3650       3\n\n\n4.2.6 Interpretazione\n\n\nOggetti con punteggi latenti più alti: sono quelli preferiti più spesso dagli intervistati.\n\n\nOggetti con punteggi latenti più bassi: sono quelli meno preferiti.\n\nSe ad esempio A ha un punteggio latente di 0.5, B di 0.1 e C di -0.6, allora l’ordine di preferenza sarà A &gt; B &gt; C.\nIn conclusione, questa è la logica di base dello Scaling di Thurstone (caso V), che assume varianza costante degli errori di giudizio e distribuzioni normali per i punteggi latenti. Nella pratica, se si hanno molti oggetti (non solo 3), si procede alla raccolta di tutte le percentuali pairwise e poi si utilizza un procedimento analogo (eventualmente affinandolo con tecniche di stima più complesse).",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>✏ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E3_thurstone.html#scaling-fechneriano",
    "href": "chapters/measurement/E3_thurstone.html#scaling-fechneriano",
    "title": "\n4  ✏ Esercizi\n",
    "section": "\n4.3 Scaling Fechneriano",
    "text": "4.3 Scaling Fechneriano\nUn altro esempio di scaling psicologico riguarda lo scaling fechneriano. Faccio qui riferimento ad uno studio condotto un po’ di tempo fa nell’ambito delle Vision Sciences.\nNello studio di Domini & Caudek (2009), abbiamo utilizzato il modello Intrinsic Constraints per correlare la profondità visiva percepita di un oggetto tridimensionale con i principi teorici dello scaling sensoriale Fechneriano. Quest’ultimo, basato sulle teorie psicofisiche di Fechner, prevede la costruzione di una scala psicofisica per attributi sensoriali mediante l’integrazione cumulativa di incrementi psicometrici, in particolare delle Just Noticeable Differences (JNDs). L’obiettivo della ricerca era progettare stimoli visivi caratterizzati da diversi indizi di profondità (stereopsi, parallasse di movimento, ecc.) in modo da indurre una percezione soggettiva equivalente della profondità 3D. Secondo questa logica, oggetti definiti da cue visivi distinti dovrebbero risultare percettivamente equiparabili in termini di profondità qualora fossero associati a un numero identico di JNDs. In altre parole, l’uniformità percettiva emerge quando gli stimoli, pur differendo negli indizi di profondità, raggiungono la stessa soglia di discriminabilità psicofisica.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>✏ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E3_thurstone.html#il-modello-thurstoniano",
    "href": "chapters/measurement/E3_thurstone.html#il-modello-thurstoniano",
    "title": "\n4  ✏ Esercizi\n",
    "section": "",
    "text": "Le utilità (o punteggi) latenti siano distribuite normalmente. Ogni oggetto possiede una distribuzione gaussiana dei propri punteggi latenti nella popolazione.\nLe distribuzioni differiscano nelle medie, ma abbiano la stessa varianza. Nel Caso V di Thurstone, tutte le distribuzioni hanno una varianza comune, mentre le medie possono variare da un oggetto all’altro (alcuni oggetti sono mediamente più apprezzati di altri).\n\n\n\nSi calcola, per ciascuna coppia di oggetti, la proporzione di individui che preferisce un oggetto all’altro.\nTali proporzioni vengono convertite in z-score (percentili della distribuzione normale cumulativa) per stimare quanto un oggetto supera un altro in termini di deviazioni standard.\nSommando o mediando opportunamente questi z-score, si ottiene una stima dell’utilità media (ovvero la media della distribuzione normale latente) di ogni oggetto. Di solito, per identificare il modello, si fissa a zero la media di un oggetto (ad esempio, quello meno desiderato).",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>✏ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E3_thurstone.html#tutorial",
    "href": "chapters/measurement/E3_thurstone.html#tutorial",
    "title": "\n4  ✏ Esercizi\n",
    "section": "\n4.2 Tutorial",
    "text": "4.2 Tutorial\nNel seguente esercizio vedremo come eseguire uno scaling thurstoniano su dati di preferenze (o scelte) e ottenere delle stime delle utilità psicologiche di una serie di oggetti o caratteristiche confrontate fra loro. In altre parole, date le classifiche (rank) osservate, vogliamo inferire quali possano essere le distribuzioni di “valore psicologico” che hanno generato tali preferenze. Per stimare le utilità non osservate a partire dalle preferenze osservate, si ipotizza che le utilità siano distribuite normalmente con la stessa varianza (ipotesi del “Caso V” di Thurstone).\n\n4.2.1 Studio delle preferenze per alcune caratteristiche lavorative\nI dati di questo esempio provengono (in forma semplificata) da uno studio sulla motivazione al lavoro, condotto su \\(N=1100\\) partecipanti. A ogni partecipante è stato chiesto di ordinare in base all’importanza nove possibili caratteristiche di un lavoro ideale (assegnando il rank 1 alla più importante e il rank 9 alla meno importante). Le nove caratteristiche, qui elencate con i nomi delle colonne corrispondenti, sono:\n\nAmbiente di Sostegno (Support)\nLavoro Stimolante (Challenge)\nProspettive di Carriera (Career)\nIntegrità Etica (Ethics)\nAutonomia e Impatto Personale (Autonomy)\nOpportunità di Crescita (Development)\nInterazione Sociale (Interaction)\nCompetizione (Competition)\nAmbiente e Sicurezza (Safety)\n\nDi seguito, illustriamo passo dopo passo l’analisi con il pacchetto psych di R.\n\n4.2.2 Passo 1. Lettura e ispezione dei dati\nIl file di dati (ad esempio, JobFeatures.txt) contiene le classifiche di 1100 partecipanti, ciascuno con i ranghi per le 9 caratteristiche. Per leggere il file in RStudio si può usare la funzione read.delim():\n\nJobFeatures &lt;- rio::import(\"../../data/JobFeatures.txt\")\nglimpse(JobFeatures)\n#&gt; Rows: 1,079\n#&gt; Columns: 9\n#&gt; $ Support     &lt;int&gt; 8, 7, 5, 7, 1, 6, 5, 1, 1, 7, 6, 8, 5, 9, 8, 1, 6, 7, …\n#&gt; $ Challenge   &lt;int&gt; 3, 5, 8, 6, 4, 1, 4, 9, 3, 4, 2, 1, 4, 8, 6, 7, 4, 4, …\n#&gt; $ Career      &lt;int&gt; 4, 1, 1, 8, 8, 3, 7, 2, 7, 6, 3, 4, 6, 1, 3, 5, 8, 3, …\n#&gt; $ Ethics      &lt;int&gt; 5, 6, 9, 9, 3, 7, 2, 8, 4, 1, 9, 3, 7, 5, 9, 6, 7, 5, …\n#&gt; $ Autonomy    &lt;int&gt; 2, 2, 6, 3, 9, 8, 3, 7, 9, 8, 4, 6, 3, 7, 5, 2, 3, 8, …\n#&gt; $ Development &lt;int&gt; 6, 8, 2, 4, 2, 5, 6, 5, 2, 5, 1, 2, 2, 6, 1, 3, 1, 2, …\n#&gt; $ Interaction &lt;int&gt; 1, 3, 3, 2, 6, 2, 1, 4, 6, 9, 5, 5, 1, 4, 2, 8, 2, 6, …\n#&gt; $ Competition &lt;int&gt; 7, 9, 4, 5, 7, 4, 9, 6, 8, 3, 7, 7, 9, 2, 7, 9, 9, 1, …\n#&gt; $ Safety      &lt;int&gt; 9, 4, 7, 1, 5, 9, 8, 3, 5, 2, 8, 9, 8, 3, 4, 4, 5, 9, …\n\nPossiamo controllare i nomi delle variabili con:\n\nnames(JobFeatures)\n#&gt; [1] \"Support\"     \"Challenge\"   \"Career\"      \"Ethics\"      \"Autonomy\"   \n#&gt; [6] \"Development\" \"Interaction\" \"Competition\" \"Safety\"\n\nLa funzione head(JobFeatures) mostra le prime righe del data frame:\n\nhead(JobFeatures)\n#&gt;   Support Challenge Career Ethics Autonomy Development Interaction\n#&gt; 1       8         3      4      5        2           6           1\n#&gt; 2       7         5      1      6        2           8           3\n#&gt; 3       5         8      1      9        6           2           3\n#&gt; 4       7         6      8      9        3           4           2\n#&gt; 5       1         4      8      3        9           2           6\n#&gt; 6       6         1      3      7        8           5           2\n#&gt;   Competition Safety\n#&gt; 1           7      9\n#&gt; 2           9      4\n#&gt; 3           4      7\n#&gt; 4           5      1\n#&gt; 5           7      5\n#&gt; 6           4      9\n\n\n4.2.3 Passo 2. Esecuzione dello scaling thurstoniano\nPer poter eseguire l’analisi, caricando il pacchetto psych. La funzione che ci interessa è thurstone(). Se cerchiamo informazioni:\nhelp(\"thurstone\")\nscopriamo che la funzione ha la forma generale:\nthurstone(x, ranks = FALSE, digits = 2)\n\n\nx: il set di dati su cui eseguire lo scaling.\n\nranks: se TRUE, i dati forniti sono ranghi raw; se FALSE, ci si aspetta una matrice di proporzioni di scelta (di default è FALSE).\n\ndigits: numero di cifre decimali per l’indice di bontà di adattamento (default 2).\n\nDal momento che abbiamo dati grezzi di ranking, impostiamo ranks = TRUE. Il comando:\n\nthurstone(JobFeatures, ranks = TRUE)\n#&gt; Thurstonian scale (case 5) scale values \n#&gt; Call: thurstone(x = JobFeatures, ranks = TRUE)\n#&gt; [1] 0.97 0.93 0.91 0.92 0.60 1.04 0.63 0.00 0.23\n#&gt; \n#&gt;  Goodness of fit of model   1\n\nrestituirà in Console un output simile al seguente:\nThurstonian scale (case 5) scale values \nCall: thurstone(x = JobFeatures, ranks = TRUE)\n[1] 0.97 0.93 0.91 0.92 0.60 1.04 0.63 0.00 0.23\n\nGoodness of fit of model   1\n\nI valori dopo la riga Call: sono le medie stimate delle utilità psicologiche.\n\nSono ordinate nello stesso modo in cui le caratteristiche compaiono nel data frame.\n\nUna di queste medie è fissata a 0 (tipicamente, la caratteristica meno preferita viene posta a 0 per identificare il modello).\n\n4.2.4 Passo 3. Calcolo delle proporzioni di preferenza a coppie\nSe assegniamo i risultati della funzione a un oggetto, ad esempio:\n\nscaling &lt;- thurstone(JobFeatures, ranks = TRUE)\n\nnon vediamo nulla stampato in Console, ma l’analisi è stata eseguita. Ora ls(scaling) mostra i componenti:\n\nls(scaling)\n#&gt; [1] \"Call\"     \"GF\"       \"choice\"   \"residual\" \"scale\"\n\n\n\nscaling$scale contiene i valori medi di utilità, già visti.\n\n\nscaling$choice contiene la matrice delle proporzioni di scelta (9×9). Ogni cella \\((i, j)\\) indica la proporzione di partecipanti che preferiscono la caratteristica j rispetto alla caratteristica i.\n\n\nscaling$choice |&gt; \n  round(2)\n#&gt;       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\n#&gt;  [1,] 0.50 0.47 0.47 0.47 0.36 0.51 0.36 0.20 0.23\n#&gt;  [2,] 0.53 0.50 0.47 0.49 0.38 0.53 0.36 0.17 0.28\n#&gt;  [3,] 0.53 0.53 0.50 0.50 0.38 0.52 0.39 0.19 0.26\n#&gt;  [4,] 0.53 0.51 0.50 0.50 0.36 0.52 0.38 0.19 0.26\n#&gt;  [5,] 0.64 0.62 0.62 0.64 0.50 0.67 0.51 0.29 0.34\n#&gt;  [6,] 0.49 0.47 0.48 0.48 0.33 0.50 0.29 0.15 0.20\n#&gt;  [7,] 0.64 0.64 0.61 0.62 0.49 0.71 0.50 0.22 0.30\n#&gt;  [8,] 0.80 0.83 0.81 0.81 0.71 0.85 0.78 0.50 0.61\n#&gt;  [9,] 0.77 0.72 0.74 0.74 0.66 0.80 0.70 0.39 0.50\n\nSe, per esempio, la cella \\((8,6)\\) vale 0.853, significa che l’8ª caratteristica è preferita alla 6ª dall’85.3% del campione (o dei ranghi aggregati).\n\n4.2.5 Passo 4. Valutazione dell’adattamento del modello\nPossiamo esaminare gli scarti (residui) confrontando le proporzioni osservate con quelle predette dal modello:\n\nscaling$residual |&gt; \n  round(3)\n#&gt;         [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]\n#&gt;  [1,]  0.000  0.013  0.002  0.008 -0.006  0.020  0.008 -0.037  0.007\n#&gt;  [2,] -0.013  0.000  0.022  0.000 -0.006  0.011  0.019  0.010 -0.040\n#&gt;  [3,] -0.002 -0.022  0.000  0.007  0.005  0.028  0.001 -0.005 -0.009\n#&gt;  [4,] -0.008  0.000 -0.007  0.000  0.011  0.028  0.007 -0.012 -0.011\n#&gt;  [5,]  0.006  0.006 -0.005 -0.011  0.000 -0.005  0.007 -0.013  0.017\n#&gt;  [6,] -0.020 -0.011 -0.028 -0.028  0.005  0.000  0.049  0.003  0.016\n#&gt;  [7,] -0.008 -0.019 -0.001 -0.007 -0.007 -0.049  0.000  0.042  0.041\n#&gt;  [8,]  0.037 -0.010  0.005  0.012  0.013 -0.003 -0.042  0.000 -0.021\n#&gt;  [9,] -0.007  0.040  0.009  0.011 -0.017 -0.016 -0.041  0.021  0.000\n\n\nResidui vicini a zero indicano che il modello di Thurstone (utilità normali) riproduce bene i dati osservati.\n\nInfine, con:\n\nscaling$GF\n#&gt; [1] 0.999\n\notteniamo un indice di bontà di adattamento prossimo a 1, segno che il modello fornisce una buona descrizione dei dati (più il valore è vicino a 1, meglio il modello si adatta).\nIn sintesi\n- Thurstonian scaling (Caso V): presuppone che le utilità latenti degli oggetti abbiano distribuzioni normali con varianza uguale.\n- Procedura:\n1. Lettura dei ranking e ispezione dei dati.\n2. Calcolo dello scaling con la funzione thurstone().\n3. Interpretazione delle scale (medie di utilità).\n4. Analisi delle proporzioni di preferenza a coppie.\n5. Verifica dei residui e dell’indice Goodness of Fit (GF).\n- Conclusione interpretativa:\n- Un oggetto con media più alta risulta, in media, più desiderato.\n- L’oggetto con media zero (fissata) è quello meno preferito o funge da riferimento.\n- Residui bassi e un GF vicino a 1 indicano che il modello spiega bene i dati.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>✏ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E3_thurstone.html#riflessioni-conclusive",
    "href": "chapters/measurement/E3_thurstone.html#riflessioni-conclusive",
    "title": "\n4  ✏ Esercizi\n",
    "section": "\n4.4 Riflessioni Conclusive",
    "text": "4.4 Riflessioni Conclusive\nLo scaling thurstoniano (Caso V) trasforma i ranking discreti in stime continue di utilità, consentendo di evidenziare differenze relative fra le caratteristiche analizzate e di quantificare la “distanza” tra gli oggetti su una scala latente comune. Quando il numero di oggetti è elevato, si procede al calcolo di tutte le proporzioni di preferenza pairwise, applicando la stessa logica, eventualmente integrata con metodi di stima più sofisticati.\nQuesta metodologia, introdotta da Louis Leon Thurstone negli anni Venti, è una delle prime forme di scaling psicologico. Con il termine scaling si indica il processo di collocare stimoli (o concetti) su un continuum, in base al grado in cui essi possiedono una certa proprietà psicologica. Nel Caso V, si assume che tutte le distribuzioni di punteggi latenti abbiano la stessa varianza, ipotesi che semplifica il modello ma può risultare limitante nella pratica, poiché la coerenza dei giudizi e la natura degli stimoli possono variare notevolmente.\nPrima di Thurstone, Gustav Theodor Fechner (1801-1887) pose le basi della psicofisica proponendo l’idea che la relazione fra l’intensità fisica di uno stimolo e la sua percezione psicologica seguisse una legge logaritmica. Al centro del metodo di Fechner vi è il concetto di JND (Just Noticeable Difference, o “differenza appena percepibile”): la più piccola variazione dello stimolo che un individuo è in grado di distinguere da un riferimento. Accumulando queste unità di percezione – le JND – Fechner ipotizzò che si potesse costruire una scala psicologica che misurasse l’intensità soggettiva delle sensazioni in funzione dello stimolo fisico.\nDal punto di vista concettuale, lo scaling di Fechner precede e ispira quello di Thurstone in quanto entrambi i metodi assumono l’esistenza di variabili latenti alla base dei giudizi individuali. Nel caso di Fechner, l’obiettivo principale era correlare la misura fisica di uno stimolo (ad esempio, il peso o la luminosità) alla sensazione percepita dal soggetto, segmentando la scala fisica in unità psicologicamente equipollenti (le JND). Thurstone, invece, ha esteso questo paradigma al contesto dei giudizi di preferenza e classificazione, introducendo la nozione di distribuzioni normali per i punteggi latenti e di varianza costante tra gli oggetti.\n\n4.4.1 Limiti e prospettive\nUna criticità rilevante nello scaling thurstoniano è la mancanza di procedure consolidate per la falsificazione interna del modello: a differenza, ad esempio, di quanto avviene nello scaling di Mokken, dove si può testare la monotonicità delle risposte. Se i dati violano l’assunzione di varianza costante o di un unico continuum psicologico condiviso, il modello di Thurstone non fornisce criteri immediati per identificare o risolvere tali problemi.\nNel panorama odierno, molti studiosi preferiscono approcci basati sulla Teoria della Risposta all’Item (IRT), che offre una rappresentazione più flessibile: la probabilità di risposta a un item dipende sia dalle sue caratteristiche, sia dal livello latente dell’individuo. L’IRT permette di testare in modo più rigoroso le assunzioni del modello (p. es. la monotonicità e l’indipendenza locale) e di stimare con maggiore precisione sia le proprietà degli item, sia i parametri delle persone.\nIn conclusione, lo scaling thurstoniano è stato un passo fondamentale nello sviluppo degli strumenti di misurazione in psicologia, ereditando la tensione di Fechner nel voler “misurare l’immisurabile”. Nonostante la sua eleganza e semplicità introduttiva, diverse questioni tecniche (come l’ipotesi di varianza costante) e l’assenza di metodi di falsificazione robusti hanno portato, nel tempo, allo sviluppo di approcci più potenti e flessibili, tra cui la Teoria della Risposta all’Item. Rimane comunque un importante riferimento storico e didattico per comprendere la logica dello scaling psicologico e il percorso che ha condotto agli strumenti sofisticati oggi disponibili.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>✏ Esercizi</span>"
    ]
  }
]